%===============================================================================
% ifacconf.tex 2022-02-11 jpuente  
% Template for IFAC meeting papers
% Copyright (c) 2023 International Federation of Automatic Control
%===============================================================================
\documentclass{ifacconf}

\makeatletter
\let\old@ssect\@ssect % Store how ifacconf defines \@ssect
\makeatother

\usepackage{natbib}        % required for bibliography
\usepackage{hyperref}

\makeatletter
\def\@ssect#1#2#3#4#5#6{%
  \NR@gettitle{#6}% Insert key \nameref title grab
  \old@ssect{#1}{#2}{#3}{#4}{#5}{#6}% Restore ifacconf's \@ssect
}
\makeatother

\usepackage{url}
\usepackage{graphicx}      % include this line if your document contains figures
\usepackage[export]{adjustbox}
\usepackage{comment}
\usepackage{color}
\usepackage{latexsym}
\usepackage{xurl}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{pifont}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{balance}

\def\rot{\rotatebox} % Rotate table cells (e.g., vertical headings)

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmin}{\arg\!\min} % argmin operator
\DeclareMathOperator*{\argmax}{\arg\!\max} % argmax operator

\newcommand{\cmark}{\ding{51}} % Check-mark
\newcommand{\xmark}{\ding{55}} % Cross-mark
\newcommand{\smark}{\ding{72}} % Star-mark
\newcommand{\bs}{\boldsymbol}
\newcommand{\mb}{\mathbf}
\newcommand{\La}{\mathcal{L}}
\newcommand{\Lb}{\pazocal{L}}

%===============================================================================

\begin{document}
\begin{frontmatter}

\title{Towards Sim2Real Transfer of Autonomy Algorithms using AutoDRIVE Ecosystem}
% Title, preferably not more than 10 words.

\author[First]{Chinmay Samak}
\author[First]{Tanmay Samak}
\author[First]{Venkat Krovi}
% \thanks[footnoteinfo]{Corresponding Author}

\address[First]{Department of Automotive Engineering, Clemson University International Center for Automotive Research, Greenville, SC 29607, USA. {\tt {\{\href{mailto:csamak@clemson.edu}{csamak}, \href{mailto:tsamak@clemson.edu}{tsamak}, \href{mailto:vkrovi@clemson.edu}{vkrovi}\}@clemson.edu}}}

\begin{abstract} % 50-100 words
The engineering community currently encounters significant challenges in the development of intelligent transportation algorithms that can be transferred from simulation to reality with minimal effort. This can be achieved by robustifying the algorithms using domain adaptation methods and/or by adopting cutting-edge tools that help support this objective seamlessly. This work presents AutoDRIVE, an openly accessible digital twin ecosystem designed to facilitate synergistic development, simulation and deployment of cyber-physical solutions pertaining to autonomous driving technology; and focuses on bridging the autonomy-oriented simulation-to-reality (sim2real) gap using the proposed ecosystem. In this paper, we extensively explore the modeling and simulation aspects of the ecosystem and substantiate its efficacy by demonstrating the successful transition of two candidate autonomy algorithms from simulation to reality to help support our claims: (i) autonomous parking using probabilistic robotics approach; (ii) behavioral cloning using deep imitation learning. The outcomes of these case studies further strengthen the credibility of AutoDRIVE as an invaluable tool for advancing the state-of-the-art in autonomous driving technology.
% The engineering community currently faces several challenges in prototyping intelligent transportation algorithms, which can be transferred from simulation to reality with minimal effort. This work presents AutoDRIVE, an openly accessible ecosystem for synergistically developing, simulating and deploying cyber-physical solutions concerned with autonomous driving technology. This work focuses on bridging the autonomy-oriented simulation-to-reality (sim2real) gap using the proposed ecosystem. We discuss the modeling and simulation aspects of the ecosystem and demonstrate the transition of two candidate autonomy algorithms from simulation to reality to help support our claims: (i) autonomous parking using probabilistic robotics approach; (ii) behavioral cloning using deep imitation learning.
\end{abstract}

\begin{keyword} % 5-10 keywords
Autonomous Vehicles; Mobile Robots; Digital Twins; Sim2Real; Real2Sim
\end{keyword}

\end{frontmatter}

%===============================================================================

\section{Introduction}
\label{Section: Introduction}

% Advancing the field of connected autonomous vehicles (CAVs) requires both cutting-edge research as well as comprehensive education. While research makes sure that the field sees some advancement in the near future, education takes care of preparing the next generation to drive this field even further in the distant future. Today, most researchers, educators and students tend to exploit some form of simulation tool and/or scaled testbed to alleviate the monetary, spatial, temporal and safety constraints associated with rapid-prototyping of CAV solutions. In a research setting, such platforms accelerate the process of designing experiments, recording datasets as well as re-iteratively prototyping and validating autonomy solutions. In an education setting, such platforms aid in designing interactive demonstrations, hands-on assignments, projects and competitions.

The progression of connected autonomous vehicles (CAVs) necessitates a dual approach of cutting-edge research and comprehensive education. Research endeavors propel immediate advancements in the field, while education plays a pivotal role in equipping the next generation with the necessary skills and knowledge to propel the field even further in the long term. Recently, majority of researchers, educators and students rely on simulation tools and/or a scaled testbeds to to alleviate the monetary, spatial, temporal and safety constraints associated with rapid-prototyping of CAV solutions. In research settings, these platforms accelerate the process of designing experiments, recording datasets as well as re-iteratively prototyping and validating autonomy solutions. In educational contexts, such platforms facilitate the creation of interactive demonstrations, hands-on assignments, projects, and competitions centered around CAV technology.

% Figure environment removed

% However, existing platforms for this purpose are observed to limit the throughput of developing and validating connected autonomy solutions. Firstly, most of these platforms lack the integrity required to promote hardware-software co-development; some only offer software simulation tools, while others only provide physical scaled vehicles to test autonomy algorithms. Such isolated platforms not only decelerate the prototyping phase due to compatibility issues, but also adversely affect the validation phase involving simulation to real-world (sim2real) deployments. Secondly, most of these platforms focus specifically on vehicles rather than a holistic intelligent transportation ecosystem involving infrastructure, traffic elements and peer agents, which limits their applications. Thirdly, some of these platforms are domain or application specific with limited sensing modalities, stringent design requirements and/or fixed development frameworks; some even lack an on-board computation unit and are merely teleoperated from a remote server to execute the intended mission.

However, existing platforms catering to the development and validation of connected autonomy solutions have been observed to impose limitations on throughput. Firstly, a significant portion of these platforms lacks the essential integrity necessary to foster hardware-software co-development. Some platforms solely offer software simulators, while others merely provide scaled vehicles for testing autonomy algorithms. Such isolated platforms not only impede the prototyping phase due to compatibility issues but also hinder the validation phase involving the transition from simulation to real-world. Secondly, a majority of these platforms concentrate exclusively on vehicles, neglecting the holistic integration of an intelligent transportation ecosystem encompassing infrastructure, traffic elements, and peer agents. Consequently, their applications remain limited in scope. Thirdly, certain platforms are confined to specific domains or applications, featuring restricted sensing modalities, stringent design requirements, and fixed development frameworks. Such constraints further inhibit the versatility and adaptability of these platforms for broader use cases in connected autonomy research and education.

% This work presents AutoDRIVE\footnote{\url{https://autodrive-ecosystem.github.io}}, an openly accessible ecosystem for synergistically developing, simulating and deploying cyber-physical solutions concerned with autonomous driving technology. This seamless workflow is enabled through the use of a tightly integrated trio comprising an algorithm development kit for designing autonomy solutions, a software simulator to virtually prototype and test them and a hardware testbed to deploy and validate them (refer Fig. \ref{fig1}). The harmony among these three sub-systems not only enhances the hardware-software co-development of autonomy solutions, but also helps seamlessly bridge the gap between software simulation and hardware deployment.

% This work presents AutoDRIVE\footnote{\url{https://autodrive-ecosystem.github.io}}, an openly accessible ecosystem for synergistically developing, simulating and deploying cyber-physical solutions concerned with autonomous driving technology. This seamless workflow is enabled through the use of a tightly integrated trio comprising an algorithm development kit for designing autonomy solutions, a software simulator to virtually prototype and test them and a hardware testbed to deploy and validate them (refer Fig. \ref{fig1}). The harmony among these three sub-systems not only enhances the hardware-software co-development of autonomy solutions, but also helps seamlessly bridge the gap between software simulation and hardware deployment. This work particularly focuses on the challenges associated with bridging the simulation-to-reality (sim2real) gap for autonomy-oriented applications using the proposed ecosystem. In this context, we discuss the percepto-dynamics modeling and graphics rendering aspects of a scaled vehicle and infrastructure using the proposed ecosystem. Finally, we also demonstrate the transition of two candidate autonomy algorithms from simulation to reality to help support our claims: (i) autonomous parking using probabilistic robotics approach for mapping, localization, path planning and control; (ii) behavioral cloning using computer vision and end-to-end deep imitation learning.

This research presents AutoDRIVE\footnote{\url{https://autodrive-ecosystem.github.io}} \cite{AutoDRIVEEcosystem2023}, a publicly accessible ecosystem specifically designed to facilitate the integrated development, simulation, and deployment of cyber-physical solutions pertaining to autonomous driving technology. This seamless workflow is made possible by a tightly integrated trio, consisting of an algorithm development kit for designing autonomy solutions, a software simulator for virtual prototyping and testing them under a variety of conditions and edge-cases, and a hardware testbed for physical deployment and validation (refer Fig. \ref{fig1}). The synergy among these three sub-systems not only enhances the hardware-software co-development of autonomy solutions but also effectively bridges the gap between simulation and reality. This work places particular emphasis on the challenges associated with bridging the sim2real gap for autonomy-oriented applications using the proposed ecosystem. In this context, we delve into the percepto-dynamics modeling and simulation aspects of a scaled vehicle and infrastructure using the proposed ecosystem. Furthermore, we substantiate our claims by showcasing the successful transition of two candidate autonomy algorithms from simulation to reality to help support our claims: (i) autonomous parking using probabilistic robotics approach for mapping, localization, path planning and control; (ii) behavioral cloning using computer vision and end-to-end deep imitation learning.

%===============================================================================

\section{State of the Art}
\label{Section: State of the Art}

\subsection{Software Simulators}
\label{Sub-Section: Software Simulators}

% For several years, the automotive industry has made use of simulators like Ansys Automotive \cite{AnsysAutomotive2021} and Adams Car \cite{AdamsCar2021} to simulate vehicular dynamics at different levels, thereby accelerating the development of its end-products. Since the past few years, however, owing to the increasing popularity of advanced driver-assistance systems (ADAS) and autonomous vehicles, most of the traditional automotive simulators, such as Ansys Autonomy \cite{AnsysAutonomy2021}, CarSim \cite{CarSim2021} and CarMaker \cite{CarMaker2021}, have started releasing vehicular autonomy features in their updated versions.

% Apart from these, there are several commercial simulators that specifically target the field of autonomous driving. These include NVIDIA's Drive Constellation \cite{DRIVEConstellation2021}, Cognata \cite{Cognata2021}, rFpro \cite{rFpro2021}, dSPACE \cite{dSPACE2021} and PreScan \cite{PreScan2021}, to name a few. In the recent past, several research projects have also tried adopting computer games like GTA V \cite{Richter2016, Richter2017, Johnson-Roberson2017} in order to virtually simulate self-driving cars, but they were quickly shut down by the game's publisher.

% Nevertheless, the open-source community has also developed several simulators for such applications. Gazebo \cite{Gazebo2004}, a simulator natively adopted by Robot Operating System (ROS) \cite{ROS2009}, has been used extensively by most of the scaled autonomous vehicles described earlier in Section \ref{Sub-Section: Hardware Testbeds}. TORCS \cite{TORCS2021}, another open-source simulator widely known in the self-driving community, is probably one of the earliest to specifically target manual and autonomous racing problems. Other prominent examples include CARLA \cite{CARLA2017}, AirSim \cite{AirSim2018} and Deepdrive \cite{Deepdrive2021} developed using the Unreal \cite{Unreal2021} game engine along with Apollo GameSim \cite{ApolloGameSim2021} and LGSVL Simulator \cite{LGSVLSimulator2020} developed using the Unity \cite{Unity2021} game engine.

% Over the years, automotive industry has extensively utilized simulators like CarSim \cite{CarSim2021}, Ansys Automotive \cite{AnsysAutomotive2021} and Adams Car \cite{AdamsCar2021} to accelerate the development of vehicular dynamics in their end-products. With the rising popularity of CAVs, traditional automotive simulators like CarMaker \cite{CarMaker2021} and Ansys Autonomy \cite{AnsysAutonomy2021} have integrated vehicular autonomy features in their upgraded versions. Furthermore, various commercial simulators are dedicated to autonomous driving, including NVIDIA's Drive Constellation \cite{DRIVEConstellation2021}, Cognata \cite{Cognata2021}, rFpro \cite{rFpro2021}, dSPACE \cite{dSPACE2021}, and PreScan \cite{PreScan2021}. Some researchers have also tried using video games like GTA V \cite{Richter2016, Richter2017, Johnson-Roberson2017} to simulate self-driving cars, but faced limitations due to game publisher restrictions.

Over the years, open-source community has contributed several simulators for autonomous driving applications. Gazebo \cite{Gazebo2004}, natively integrated with Robot Operating System (ROS) \cite{ROS2009}, is commonly used for scaled autonomous robots. TORCS \cite{TORCS2021} has been an early focus in the self-driving community, particularly for manual and autonomous racing. Other notable examples include CARLA \cite{CARLA2017}, AirSim \cite{AirSim2018}, and Deepdrive \cite{Deepdrive2021}, developed using the Unreal %\cite{Unreal2021}
game engine, as well as Apollo GameSim \cite{ApolloGameSim2021}, LGSVL Simulator \cite{LGSVLSimulator2020} and AutoRACE Simulator \cite{AutoRACE2021}, created using the Unity %\cite{Unity2021}
game engine.

However, these simulators pose 3 major limitations:

\begin{itemize}
    \item Firstly, some simulation tools prioritize photorealism over physical accuracy, while others prioritize physical accuracy over graphics quality. In contrast, AutoDRIVE Simulator strikes a balance between physics and graphics, providing a range of configurations to accommodate varying compute capabilities.
    \item Secondly, the dynamics and perception systems of scaled vehicles and environments differ significantly from their full-scale counterparts. Existing simulation tools do not adequately support scaled ecosystems to their full capacity. Consequently, transitioning from full-scale simulation to scaled real-world deployment necessitates substantial additional effort to re-tune the autonomy algorithms.
    \item Thirdly, the existing simulators may lack precise real-world counterparts, rendering them unsuitable for ``digital twin'' applications, involving synthetic data generation, variability testing, reinforcement learning, real2sim and sim2real transfer, among others.
\end{itemize}

% However, the existing simulators have 3 major limitations as follows:

% \begin{itemize}
%     \item Firstly, some of the available simulation tools for autonomous vehicles focus mainly on photorealism as opposed to physical (kinematic/dynamic) accuracy. Others outweigh physical accuracy over graphics quality. AutoDRIVE Simulator weighs both physics and graphics equally, and offers a set of configurations to choose from depending on the compute capability.
%     \item Secondly, the dynamics and perception systems of scaled vehicles and environments are significantly different as compared to full-scale counterparts. The existing simulation tools do not support scaled ecosystems to full-capacity. As a result, transitioning from full-scale simulation to scaled real-world deployment (which is exercised by several technical workshops, coursework lab assignments and research projects, especially in academia) calls for significant additional effort in re-tuning the autonomy algorithms.
%     \item Thirdly, the existing simulators may not have exact real-world counterparts and hence cannot be used for “digital twin” applications such as synthetic data generation, variability testing, reinforcement learning, real2sim and sim2real transfer, etc.
% \end{itemize}

\subsection{Hardware Testbeds}
\label{Sub-Section: Hardware Testbeds}

% In the recent past, many educational and research institutes have started developing scaled autonomous vehicles. A few of these include the MIT Racecar \cite{MIT-Racecar2017}, F1TENTH \cite{F1TENTH2019} and AutoRally \cite{AutoRally2021}. HyphaROS RaceCar \cite{HyphaROS-Racecar2021} and Donkey Car \cite{DonkeyCar2021} are among some of the other community-driven platforms for autonomous driving; both of these were developed with specific applications in mind - the former for map-based navigation, and the later for vision-aided imitation learning. Apart from these, commercial products such as QCar \cite{QCar2021} and DeepRacer \cite{DeepRacer2021} are now surfacing the market. However, the fact that such products are over-charged for profitability and employ some form of proprietary hardware and/or software components restrict their openness and flexibility to the community; not to mention potential issues like warranty-voids and vendor lock-ins. Another scaled platform for autonomy research and education, called Duckietown \cite{Duckietown2017}, was primarily designed to be openly accessible and inexpensive. However, the fact that it utilizes differential-drive robots, which in no way account for the kinodynamic constraints of a steered car-like vehicle, does not meet the community requirements in terms of a ``self-driving car'' platform. Nevertheless, it is a really good platform for teaching fundamentals of autonomy, much like the TurtleBot3 \cite{Turtlebot2021}.

In recent times, numerous educational institutions have embarked on the development of scaled autonomous vehicles. Popular examples include the MIT Racecar \cite{MIT-Racecar2017}, F1TENTH \cite{F1TENTH2019}, and AutoRally \cite{AutoRally2021}. Additionally, community-driven platforms like HyphaROS RaceCar \cite{HyphaROS-Racecar2021} and Donkey Car \cite{DonkeyCar2021} have emerged, tailored to specific applications like map-based navigation and vision-aided imitation learning, respectively.
%Commercial products like QCar \cite{QCar2021} and DeepRacer \cite{DeepRacer2021} have also entered the market, but their proprietary nature and high costs limit their accessibility and flexibility for the wider community.
Duckietown \cite{Duckietown2017} is another platform, which employs differential-drive robots instead of kinodynamically constrained car-like vehicles, thereby falling short of the community's requirements. Nevertheless, it remains a popular platform for teaching autonomy fundamentals, much like TurtleBot3 \cite{Turtlebot2021}.

However, these platforms pose 3 major limitations:

\begin{itemize}
    \item Firstly, some of these platforms lack diverse sensing modalities, sufficient computational power, constrained actuation mechanisms, and active or passive infrastructural elements.
    \item Secondly, most platforms utilize commercial radio-controlled (RC) cars as their base-chassis, which are expensive, may not be readily available worldwide, and limit exploration in the realm of mechatronics engineering. Additionally, many platforms are confined to specific software frameworks like ROS, creating a skill-set dependency for end-users.
    \item Thirdly, some platforms lack any form of simulation support, some support simulation using RViz \cite{RViz2021} or Gazebo, while others provide task-specific Gym \cite{OpenAIGym2016} environments for machine learning; none of which is ideal.
\end{itemize}

% However, these platforms exhibit 3 major limitations:

% \begin{itemize}
%     \item In terms of comprehensiveness, some of these platforms lack diverse sensing modalities, some lack adequate computational power, some lack Ackermann steering mechanism, and most lack active or passive infrastructural elements. Only a few satisfy the prominent community requirements, but can prove to be prohibitively expensive for university programs.
%     \item In terms of flexibility, most of these platforms, if not all, use commercial-off-the-shelf (COTS) radio-controlled (RC) cars as their base-chassis, which: (a) are quite expensive; (b) may not be available all around the world; (c) limit research on the ``mechatronics engineering'' front, which is equally important for cyber-physical systems such as CAVs. Additionally, most of these platforms only support a specific software framework such as ROS, which inherently creates a skillset-dependency for the end-users. Furthermore, providing assets and plugins for pre-packaged simulators like Gazebo \cite{Gazebo2004} or OpenAI Gym \cite{OpenAIGym2016} environments offers only so much flexibility to the users in terms of designing and running the simulated scenarios.
%     \item In terms of integrity, some of these platforms do not support simulation in any form, some ROS-based ones support kinematic/dynamic simulation using RViz \cite{RViz2021} and/or Gazebo, while others offer task-specific Gym environments for imitation/reinforcement learning; none of which is ideal.
% \end{itemize}

%===============================================================================

\section{AutoDRIVE Ecosystem}
\label{Section: AutoDRIVE Ecosystem}

This section primarily focuses on digital-twin capabilities of AutoDRIVE Ecosystem, highlighting the strong resemblance between AutoDRIVE Simulator \cite{AutoDRIVESimulator2021, AutoDRIVESimulatorReport2020} and AutoDRIVE Testbed for seamless sim2real transfer of autonomy algorithms developed using AutoDRIVE Devkit.

% AutoDRIVE Simulator \cite{AutoDRIVESimulator2021, AutoDRIVESimulatorReport2020} is developed atop the Unity %\cite{Unity2021}
% game engine, which employs PhysX %\cite{PhysX2021}
% for simulating multi-threaded framerate-independent kinematics and dynamics of all the physical entities, and exploits High Definition Render Pipeline (HDRP) %\cite{HDRP2021}
% along with Post-Processing Stack %\cite{PPS2021}
% for rendering photorealistic graphics. It acts as the digital twin of AutoDRIVE Testbed and is primarily targeted towards virtual prototyping of autonomy solutions, but can also be used for synthetic data generation.

\subsection{Physical Vehicle}
\label{Sub-Section: Physical  Vehicle}

% Figure environment removed

AutoDRIVE's native vehicle, named Nigel (refer Fig. \ref{fig2}), offers realistic driving and steering actuation, comprehensive sensor suite, high-performance computational resources, and a standard vehicular lighting system.

\subsubsection{Chassis}
\label{Sub-Sub-Section: Chassis}

Nigel is a 1:14 scale vehicle, which adopts front/rear/all-wheel-drive Ackermann-steered mechanism, thereby resembling car-like kinodynamic constraints.

\subsubsection{Power Electronics}
\label{Sub-Sub-Section: Power Electronics}

An 11.1 V 5200 mAh lithium-polymer (LiPo) battery acts as the power-source for the vehicle. Other components such as the buck converter, motor driver, switch and voltage checker help route the raw power to appropriate sub-systems of the vehicle.

\subsubsection{Sensor Suite}
\label{Sub-Sub-Section: Sensor Suite}

Nigel hosts a comprehensive sensor suite comprising throttle and steering feedbacks, 1920 CPR incremental encoders, 3-axis IPS, 9-axis IMU, two 62.2$^\circ$ FOV cameras with 3.04 mm focal length and a 7-10 Hz, 360$^\circ$ FOV LIDAR with 0.15-12 m range and 1$^\circ$ resolution.

\subsubsection{Computation, Communication and Software}
\label{Sub-Sub-Section: Computation, Communication and Software}

% Nigel adopts Jetson Nano Developer Kit - B01 for most of its high-level computation (autonomy algorithms), communication (V2V and V2I) and software installation (JetPack SDK, ROS and AutoDRIVE Devkit). Additionally, it also hosts Arduino Nano (running the vehicle firmware) for acquiring and filtering raw sensor data and controlling actuators/lights.

Nigel relies on Jetson Nano Developer Kit - B01 for high-level computation (autonomy algorithms), communication (V2V and V2I), and software installation (JetPack SDK and AutoDRIVE Devkit). It also utilizes Arduino Nano (running the vehicle firmware) for sensor data acquisition and filtering, and actuators/lights control.

\subsubsection{Actuators}
\label{Sub-Sub-Section: Actuators}

Nigel is equipped with two 6V 160 RPM rated DC geared motors to drive its rear wheels, and a 9.4 kgf.cm servo motor (saturated at $\pm$ 30$^\circ$ w.r.t. zero-steer value) to steer its front wheels. All the actuators are operated at 5V, providing a top speed of $\sim$0.26 m/s for driving and $\sim$0.42 rad/s for steering.

\subsubsection{Lights and Indicators}
\label{Sub-Sub-Section: Lights and Indicators}

Nigel's lighting system comprises of dual-mode headlights, triple-mode turning indicators, and automated taillights and reverse indicators.

\subsection{Virtual Vehicle}
\label{Sub-Section: Virtual Vehicle}

% Figure environment removed

The vehicle is jointly modelled as a rigid-body and a collection of sprung masses with inherent damping (refer Fig. \ref{fig3}). The ``sprung-mass'' representation computes the suspension forces, which, aggregated with the tire forces, are applied to the ``rigid-body'' representation that exactly mimics the mass, center of mass and moment of inertia of the ``sprung-mass'' representation. Needless to say, the two representations are related through the rigid-body center of mass equation: $X_{COM} = \frac{\sum{{^iM}*{^iX}}}{\sum{^iM}}$; where, $X_{COM}$ is the rigid-body center of mass offset, $^iM$ are the sprung masses such that $M=\sum{^iM}$ is the total mass of rigid-body, and $^iX$ are the sprung mass coordinates w.r.t. $^iM$.

\subsubsection{Suspension Dynamics}
\label{Sub-Sub-Section: Suspension Dynamics}

The vehicle is modeled with a rather stiff suspension system to simulate the selective passive compliance between wheel mounts and vehicle chassis to account for losses at these interfaces due to vibration, friction, wear, damping, loosening, deformation, fatigue and fretting. The suspension force acting on each of the sprung masses can be then computed using a second-order dynamic model: ${^iM} * {^i{\ddot{Z}}} + {^iB} * ({^i{\dot{Z}}}-{^i{\dot{z}}}) + {^iK} * ({^i{Z}}-{^i{z}})$; where, $^iB$ and $^iK$ are the damping and spring coefficients of $i$-th suspension, respectively. The computed suspension forces jointly affect the rigid-body dynamics of the vehicle as well as the tire forces being computed at that time instant (since it affects the load bearing down on the tires).

\subsubsection{Wheel Dynamics}
\label{Sub-Sub-Section: Wheel Dynamics}

The wheels of the vehicle are also modelled as rigid bodies of mass $m$, acted upon by gravitational and suspension forces: ${^im} * {^i{\ddot{z}}} + {^iB} * ({^i{\dot{z}}}-{^i{\dot{Z}}}) + {^iK} * ({^i{z}}-{^i{Z}})$; and the wheel rotations are damped to mimic rotational losses due to rolling resistance.

\subsubsection{Tire Dynamics}
\label{Sub-Sub-Section: Tire Dynamics}

The tire forces are broken down into longitudinal $F_{t_x}$ and lateral  $F_{t_y}$ components, and are computed based on the respective friction curve for each tire: $\left\{\begin{matrix} {^iF_{t_x}} = F(^iS_x) \\{^iF_{t_y}} = F(^iS_y) \\ \end{matrix}\right.$; where, $^iS_x$ and $^iS_y$  are the longitudinal and lateral slips of $i$-th tire, respectively. Here, the friction curve is approximated as a two-piece spline $F(S)$; one from zero $(S_0,F_0)$ to extremum point $(S_e,F_e)$, and other from extremum point $(S_e,F_e)$ to asymptote point $(S_a,F_a)$ (refer right inset in Fig. \ref{fig3}): $F(S) = \left\{\begin{matrix} f_0(S); \;\; S_0 \leq S < S_e \\ f_1(S); \;\; S_e \leq S < S_a \\ \end{matrix}\right.$; where, $f_k(S) = a_k*S^3+b_k*S^2+c_k*S+d_k$ is a cubic polynomial function, and $F(S)$ is saturated after the asymptote point $(S_a,F_a)$.

Now, slip is in-turn dependent on the various factors like tire stiffness, steering angle, wheel speeds, suspension forces and rigid-body momentum. Longitudinal slip $S_x$ is determined based on the difference between the longitudinal components of surface velocity of the wheel compared to the angular velocity of the wheel: ${^iS_x} = \frac{{^ir}*{^i\omega}-v_x}{v_x}$; where, $v_x$ is the longitudinal linear velocity of the vehicle (i.e., surface velocity of the wheel), $^ir$ is the radius of $i$-th wheel, and $^i\omega$ is the angular velocity of $i$-th wheel. Lateral slip $S_y$ is determined by the angle (commonly denoted as $\alpha$) between the direction the tire is moving in and the direction the tire is pointing in: ${^iS_y} = \frac{v_y}{\left| v_x \right|}$; where, $v_x$ is the longitudinal linear velocity of the vehicle, and $v_y$ is the lateral linear velocity of the vehicle (a.k.a. sideslip velocity).	

\subsubsection{Sensor Simulation}
\label{Sub-Sub-Section: Sensor Simulation}

As described earlier, the vehicle is provided with an abundance of sensing modalities, all of which are modeled and simulated close to their real-world counterparts.

\begin{itemize}
    \item \textit{Throttle Feedback:} Instantaneous feedback of throttle command $[-1,1]$, where positive values indicate driving forward and negative values indicate driving reverse.
    \item \textit{Steering Feedback:} Instantaneous feedback of steering command $[-1,1]$, where positive values indicate left turns and negative values indicate right turns.
    \item \textit{Incremental Encoders:} These are simulated by measuring the rotation of each of the rear wheels (based on their rigid-body transform update) and factoring in the resolution of the encoders.
    \item \textit{IPS:} Position of the vehicle is measured based on its rigid-body transform update. The values are converted from Unity's left-handed coordinate system to the right-handed coordinate system widely adopted for robotics applications. This mimics the AprilTag-based fiducial localization system on the physical vehicle.
    \item \textit{IMU:} Orientation of the vehicle is measured based on its rigid-body transform update. Additionally, the linear acceleration and angular velocity of the vehicle are measured based on temporally-coherent rigid-body transformations, using rigid-body equations of motion. This mimics the MPU-9250 on the physical vehicle.
    \item \textit{LIDAR:} Planar laser scan is recorded by 360$^\circ$ iterative ray-casting at 1$^\circ$ resolution and 7 Hz update rate. The raycast hits are recorded between the minimum (0.15 m) and maximum (12.0 m) ranges of the LIDAR, respectively. This mimics the RPLIDAR A1 on the physical vehicle.
    \item \textit{Cameras:} Physical cameras are simulated based on their focal length (3.04 mm), field of view (62.2$^\circ$), sensor size (4.6 mm) and target resolution (720p). Additionally, lens and film effects are simulated by post-processing the raw frames. This mimics the two PiCamera V2.1 modules on the physical vehicle.
\end{itemize}

\subsubsection{Actuator Simulation}
\label{Sub-Sub-Section: Actuator Simulation}

As described earlier, the vehicle has driving and steering actuators, the response delays and saturation limits of which are matched with their real-world counterparts by tuning their torque profiles and actuation limits, respectively (refer left inset in Fig. \ref{fig3}).

\begin{itemize}
    \item \textit{Driving Actuators:} Each of the rear wheels is driven using a rotary motor, which applies a torque to it: ${^i\tau_{drive}} = {^iI_w}*{^i\dot{\omega}_w}$; where, ${^iI_w} = \frac{1}{2}*{^im_w}*{^i{r_w}^2}$ is the moment of inertia of $i$-th wheel, and $^i\dot{\omega}_w$ is the angular acceleration of $i$-th wheel. Additionally, the holding torque of the driving actuators is simulated by applying an idle motor torque equivalent to the braking torque: ${^i\tau_{idle}} = {^i\tau_{brake}}$.
    \item \textit{Steering Actuators:} The front wheels are steered using a steering actuator coupled to the steering mechanism of the vehicle. The steering actuator also produces a torque proportional to the moment of inertia of the steering mechanism: $\tau_{steer} = I_{steer}*\dot{\omega}_{steer}$. The individual turning angles, $\delta_l$ and $\delta_r$, for left and right wheels, respectively, are calculated based on the commanded steering angle $\delta$, using the Ackermann steering geometry defined by wheelbase $l$ and track width $w$, as follows: $\left\{\begin{matrix} \delta_l = \textup{tan}^{-1}\left(\frac{2*l*\textup{tan}(\delta)}{2*l+w*\textup{tan}(\delta)}\right) \\ \delta_r = \textup{tan}^{-1}\left(\frac{2*l*\textup{tan}(\delta)}{2*l-w*\textup{tan}(\delta)}\right) \\ \end{matrix}\right.$
\end{itemize}

\subsection{Physical Infrastructure}
\label{Sub-Section: Physical Infrastructure}

AutoDRIVE provides a modular and reconfigurable infrastructure development kit, enabling swift design and construction of customized driving scenarios.

% Figure environment removed

\subsubsection{Environment Modules}
\label{Sub-Sub-Section: Environment Modules}

Environment modules include static terrain and road layouts as well as obstruction objects for rapidly designing custom scenarios. Apart from these, experts may also choose to design scaled real-world or imaginary scenarios using third-party tools, and import them into AutoDRIVE Ecosystem.

\subsubsection{Traffic Elements}
\label{Sub-Sub-Section: Traffic Elements}

Traffic signs and lights define traffic laws within a particular driving scenario, thereby governing the traffic flow. These modules support IoT and V2I communication technologies, and can be therefore integrated with AutoDRIVE Smart City Manager (SCM).

\subsubsection{Surveillance Elements}
\label{Sub-Sub-Section: Surveillance Elements}

AutoDRIVE features a surveillance element called AutoDRIVE Eye to view the scene from a bird's eye view. The said element is also integrated with AutoDRIVE SCM, and is capable of estimating vehicle's 2D pose within the map by detecting and tracking the AprilTag markers attached to each of them.

\subsubsection{Preconfigured Maps}
\label{Sub-Sub-Section: Preconfigured Maps}

This work focuses on two of the several preconfigured maps offered by AutoDRIVE Ecosystem. Parking School (refer Fig. \ref{fig4a}, \ref{fig4b}) is designed specifically for autonomous parking applications, wherein construction boxes define static obstacles and all the available free-space is drivable. On the other hand, Driving School (refer Fig. \ref{fig4c}, \ref{fig4d}) covers driving over straight roads, curved roads and crossing an intersection.


\subsection{Virtual Infrastructure}
\label{Section: Virtual Infrastructure}

At every time step, the simulator performs mesh-mesh collision/interference detection and accordingly computes the contact forces, frictional forces and momentum transfer, along with linear and angular drag acting over each of the rigid bodies (vehicle/infrastructure) present in the scene. This closely emulates the interactions among vehicle(s), infrastructure elements and the environment.
%\footnote{Note the subtle distinction between ``infrastructure'' and ``environment'', in that the former comprises artificial 2D/3D rigid-body components, while the latter additionally comprises natural elements like air and gravity. In other words, $\textup{infrastructure} \subset \textup{environment}$.}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Case Studies}
\label{Section: Case Studies}

This work showcases sim2real capability of AutoDRIVE Ecosystem through two different case-studies. Although this paper cannot furnish exhaustive details pertaining to either case study, we recommend interested readers to peruse this technical report \cite{AutoDRIVEReport2021}.

% Figure environment removed

\subsection{Autonomous Parking}
\label{Sub-Section: Autonomous Parking}

% This case study leveraged (and thereby demonstrated) AutoDRIVE's ROS-enabled capabilities to exhibit autonomous parking (refer Fig. \ref{fig5a}). First, the vehicle mapped its surroundings using Hector SLAM algorithm \cite{HectorSLAM2011}. It could then localize itself against this known static map using range-flow-based odometry \cite{RF2O2016} and adaptive particle filter algorithm \cite{AMCL2001}. For autonomous navigation, the vehicle planned a feasible global path from its current pose to parking pose using A* algorithm \cite{AStar1968}, while also re-planning its local trajectory for dynamic collision avoidance using timed-elastic-band approach \cite{TEBPlanner2017}. A proportional controller generated driving (throttle/brake) and steering commands for the vehicle to track the local trajectory.

This case study was implemented using the probabilistic robotics approach comprising 5 different stages. Firstly, the vehicle mapped its surroundings using the Hector SLAM algorithm \cite{HectorSLAM2011}. It then localized itself against this known static map using range-flow-based odometry \cite{RF2O2016} and an adaptive particle filter algorithm \cite{AMCL2001}. For autonomous navigation, the vehicle planned a feasible global path from its current pose to the parking pose using the A* algorithm \cite{AStar1968}. Simultaneously, it re-planned its local trajectory for dynamic collision avoidance, leveraging the timed-elastic-band approach \cite{TEBPlanner2017}. A proportional controller generated driving (throttle/brake) and steering commands to enable the vehicle to follow the local trajectory accurately.

% % Figure environment removed

% Figure environment removed

% % Figure environment removed

% % Figure environment removed

During simulation-based testing (refer Fig. \ref{fig6a}), parameter variations such as infusion of Gaussian noise in LIDAR measurements [$n_{lidar} \sim N\left ( 0, 0.025 \right )$ m] and actuator commands [$n_{drive} \sim N\left ( 0, 0.013 \right )$ m/s and $n_{steer} \sim N\left ( 0, 0.018 \right )$ rad/s]. Furthermore, perturbation of wall modules' poses [$n_{x,y} \sim N\left ( 0, 0.01 \right )$ m and $n_{\theta} \sim N\left ( 0, 0.087 \right )$ rad] as well as introduction of unmapped obstacles were performed. Going forward, the same pipeline was deployed on the real vehicle (refer Fig. \ref{fig6b}) using AutoDRIVE Testbed to validate seamless sim2real transfer. The pipeline performed flawlessly owing to realistic LIDAR and vehicle dynamics models as well as variability analysis during simulation.

\subsection{Behavioral Cloning}
\label{Sub-Section: Behavioral Cloning}

% This case study was based on \cite{RBC2021}, wherein the objective was to employ a convolutional neural network (CNN) %\cite{Krizhevsky2012}
% for cloning the end-to-end driving behavior of a human (refer Fig. \ref{fig5b}). As such, AutoDRIVE Simulator was exploited to record 5-laps worth of temporally-coherent labeled manual driving data, which was balanced, augmented and pre-processed using standard computer vision techniques, to train a 6-layer deep CNN. Post training for 4 epochs, with a learning rate of 1e-3 using the Adam optimizer \cite{Kingma2014}, the model was deployed back into AutoDRIVE Simulator to validate its performance (refer Fig. \ref{fig7a}).

This case study was based on \cite{RBC2021}, wherein the objective was to utilize a convolutional neural network (CNN) for cloning the end-to-end driving behavior of a human (refer to Fig. \ref{fig5b}). To achieve this, AutoDRIVE Simulator was employed to record 5-laps of temporally-coherent labeled manual driving data. This data was then balanced, augmented, and pre-processed using standard computer vision techniques to train a 6-layer deep CNN model for 4 epochs with a learning rate of 1e-3 using the Adam optimizer \cite{Kingma2014}. Following training, the model was deployed back into AutoDRIVE Simulator to evaluate its performance (refer to Fig. \ref{fig7a}).

% % Figure environment removed

% Figure environment removed

% % Figure environment removed

% % Figure environment removed

Variability testing concerning light intensity and direction as well as vehicle's initial conditions and velocity limit was performed to ensure algorithm robustness. Further, the same CNN model was deployed onto AutoDRIVE Testbed for validating the sim2real transition of this vision-based algorithm (refer Fig. \ref{fig7b}). Despite subtle variations in the environmental conditions, the model generalized well and the vehicle was able to complete several laps along the track without major deviation and/or collision.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{Section: Conclusion}

In this paper, we presented AutoDRIVE, a publicly accessible digital twin ecosystem for CAVs developed with an aim of tightly integrating reality and simulation into a unified toolchain, without compromising on the comprehensiveness, flexibility and accessibility required for prototyping and validating autonomy solutions. This work focused on bridging the autonomy-oriented sim2real gap using the proposed ecosystem. Furthermore, we extensively discussed the modeling and simulation aspects of the ecosystem and substantiated its efficacy by demonstrating the successful transition of two candidate autonomy algorithms including autonomous parking and behavioral cloning from simulation to reality to help support our claims. Further research will delve into the investigation of handling real and virtual world uncertainties, formulating qualitative/quantitative evaluation metrics and benchmarks, as well as improving the robustness and generalization of sim2real frameworks for autonomous vehicles. %Further investigation will be done to comment on and improve the robustness of sim2real transfer explicitly considering sensor simulation, vehicle modeling and% scenario representation. Potential enhancements to the ecosystem include supporting heterogeneous vehicles and robotic pedestrians, full-scale vehicles and environments, expanding API support and adding extended reality capabilities, to name a few.
% We hope that the community benefits from adopting this ecosystem, may it be for education, research or anything in between.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\balance
\bibliography{ifacconf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
