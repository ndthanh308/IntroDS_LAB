\section{Sequential Analysis}
Temporal association rule mining explores temporal relationships between symbolic data points. 
TITARL (Temporal Interval Tree Association Rule Learner) is a sequence mining algorithm that enables discovering the sequential patterns between different symbolic events. This algorithm has been applied for dyadic rapport prediction \citep{madaio2017using}, and also social stance synthesis \citep{janssoone2016using}. I conducted one basic sequential analysis with TIARL with the following annotated variables:

\begin{itemize}
    \item Hedges (HD): Tutee's hedges. 739 cases found.
    \item Rapport: Annotated rapport level from 1 to 7.
    \item Knowledge Telling (KT): if one utterance simple states variables, processes or numbers. 2126 cases found for tutor, and 2220 cases found for tutee.
    \item Knowledge Building (KB): if one utterance explains the underlying ideas or reasoning process. 352 cases found for tutor, and 223 found for tutee.
    \item Shallow Question (SQ): if one utterance asks for a definition, answer etc. 714 cases found for tutor, and 883 cases found for tutee.
    \item Deep Question (DQ): if one utterance asks for questions that rely on inferences or analogies. 108 cases found for tutor, and 75 cases found for tutee.
    \item Meta-cognitive (MC): utterances that refer to the process of tutoring, or the tutor/tuteeâ€™s self-evaluation of their own knowledge. 67 cases found for tutor, and 89 cases found for tutee.
    \item Self-disclosure (SD): 406 cases found for tutor, and 495 cases found for tutee. 
    \item Praise (PR): 74 cases found for tutor, and 41 cases found for tutee.
    \item Violation of Social norm (SV): 1127 cases found for tutor, and 1115 cases found for tutee.
    
\end{itemize}




With maximum 5 conditions, 1119 rules are extracted in which contains: 1) 26 (2\%), 2) 54 (4\%) 3) 193 (17\%) 4) 429 (38\%) 5) 417 (37\%). Figure \ref{fig:titarl} shows one typical rule found by TITARL algorithm with 100\% confidence, 5\% support, and 31 uses. Confidence is the probability of a rule to be true. Support is the percentage of sequential events that can apply one particular rule. We prune the rules with 50\% confidence and also 5\% support. The number of rules reduces down to 81. It contains 14 rules found with 2 conditions; 51 rules found with 3 conditions; And 2 rules found with 4 conditions. All rules are presented in a PDF file. 

% Figure environment removed

% Confidence, Support, and Conditions.
The importance of each variable can be determined by the time used in the rules. We thus calculate the time used for each variable and post one typical rule that contains one specific variable. Table \ref{tab:time_used}
shows the number of rules that contains one specific variable.


\begin{table*}[!ht]
    \centering
    \begin{tabular}{l|llll}
    \hline
        \diagbox[font=\footnotesize\itshape, width=2.1cm]{Varibles}{Condition} & 2 & 3 & 4 & 5 \\ \hline
        HD Tutee & 9 (13\%) & 53 (76\%) &  6 (8\%) & 1 (1\%) \\ \hline
        KT Tutee & - & 28 (84\%) &  4 (12\%) & 1 (3\%) \\ \hline
        KT Tutor & 2 (5\%) & 29 (80\%) &  4 (11\%) & 1 (2\%) \\ \hline
        KB Tutee & 1 (100\%) & - & - & - \\ \hline
        KB Tutor & 2 (50\%) & 2 (50\%) & - & - \\ \hline
        SQ Tutee & 4 (44\%) &  5 (55\%) & - & - \\ \hline
        SQ Tutor & 2 (66\%) & 1 (33\%) & - & - \\ \hline
        DQ Tutee & - & - & - & - \\ \hline
        DQ Tutor & - & - & - & - \\ \hline
        MC Tutee & - & - & - & - \\ \hline
        MC Tutor & - & - & - & - \\ \hline
        SD Tutee & 3 (42\%) & 4 (57\%) & - & - \\ \hline
        SD Tutor & - & 2 (100\%) & - & - \\ \hline
        PR Tutee & - & - & - & - \\ \hline
        PR Tutor & - & - & - & - \\ \hline
        SV Tutee & - & - & - & - \\ \hline
        SV Tutor & - &  2 (100\%) & - & - \\ \hline
        Rapport 2 & - & 7 (77\%) & 2 (22\%) & - \\ \hline
        Rapport 3 & 1 (16\%) & 5 (83\%) & - & - \\ \hline
    \end{tabular}
    \caption{Number of rules containing one specific variable, 50\% confidence, 5\% support}
    \label{tab:time_used}
\end{table*}

In general, first, there is no rule for just one condition, and most rules are with 3 conditions. It means that it is difficult to conclude a direct path (i.e., one condition rule) between one variable and the tutor's hedge. Therefore, it is a multi-variable prediction problem.  Second, most rules contain HD and KT. There are two possible reasons: 1) HD and KT are the most frequent variables, and 2) KT and the tutee's HD have a high correlation with Tutor's hedge. Third, PR, MC, and DQ have limited contributions to predicting the tutor's hedge. Lastly, low rapport (2 or 3) has a certain contribution to predicting the tutor's hedge. 

Examples \ref{fig:ex_1, fig:ex_2, fig:ex_3, fig:ex_4, fig:ex_5} are most representative with relative high confidence and support.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed




\section{Reinforcement Learning Setting}
% description of the method
% We expect that the agent can \textit{chose} to hedge or non-hedge according to the interactional state, and the chosen action is also beneficial to the tutee's ultimate learning. We, therefore, consider this task as a sequential decision-making problem as well as a Reinforcement Learning (RL) problem. The main goal of a typical RL task is to maximize the cumulative \textbf{reward} by performing a series of \textbf{actions} in a \textbf{environment} (figure \ref{fig:rl}). 

% % Figure environment removed

% State, Action, Reward

%% general RL framework
The RL task often refers to a Markov decision process (MDP) described as a set of $(S, A, P, R)$. 
\begin{itemize}
    \item S: MDP is composed of different states $s\in S$. A state contains a set of information retrieved/interpreted from the current environment.
    \item A: actions $a\in A$ can be performed by an agent.
    \item P: transition probability function $P(s'|s, a)$, it outputs the probability from state $s$ to $s'$ given action $a$.
    \item R: reward function $r=R(s, a)$, it outputs the current reward given state $s$ and action $a$.
\end{itemize}

We can frame the predicting hedges tasks as an MDP, agent chose different hedges strategies according to the current state and potential reward. So we next introduce the actions, rewards, and states for this task as follows.

%% Action
\paragraph{Action}
Two different action spaces can be applied to this problem. First, the agent just chooses from hedge or non-hedge, $A=(h, nh)$. Second, the agent can choose from sub-categories of hedges (subjectivizer, apologizer, propositional hedge, extender) and the non-hedge, $A=(s, a, p, e, nh)$. 

%% Reward
\paragraph{Reward}
Hedge's ultimate goal is to promote the tutee to solve the problem. Therefore, the tutee's learning performance is the key to defining our reward function. In this proposal, we first introduce the rule-governed sparse rewards. It can also be adapted to a more fine-grained continuous-fashion function in the future. We define our reward as follows:
\begin{itemize}
    \item 1. If the tutee successfully solves the current problem, 1 is assigned, otherwise, 0 is assigned as a reward.
    \item 2. If the tutee's session-wise learning gain is positive, 10 is assigned, otherwise, -10 is assigned as a reward.
    \item 3. We want to not only reward the agent by the tutee's concrete learning performance but also the math-solving attempts. We define the attempts as the \textit{questions} asked by the tutee after the agent's actions. Two types of questions are annotated: \textit{shallow} and \textit{deep} questions. A shallow question is asking to be told, and a deep question is asking to be explained. We consider deep questions are more helpful for solving problems. Therefore, if one shallow question is posed, 1 is assigned, and 2 is assigned for the deep question as a reward, otherwise, 0 is assigned, if no question after the agent's actions. 
    \item 4. \textit{Alignment signals} can be an effective predictor in the collaborative learning setup \citep{norman2022studying}. We consider also these signals after the hedges as an ``potential" reward.
\end{itemize}

\paragraph{State}
%% State
We, as the ``interpreter" in the RL framework, can retrieve what kind of information can be observed from the environment. So we choose the following variables: 
\begin{itemize}
    \item Tutoring moves: The tutoring move (TM) that the tutee just used. It includes explanation, question, and meta-communication.
    \item Conversational strategies: The conversational strategies (CS) that the tutee just used. It includes self-disclosure, praise, violation of social norms, and hedge. 
    \item Rapport/utopy: The current rapport (RP) level (1-7), or the trend of rapport for the current interaction --- utopy \citep{madaio2017impact}.
    \item Dialogue acts: The dialogue act (DA) that the tutee just used. We use an automatic DA tagger to classify the tutee's turn. However, for simplicity, we choose to tag the 10 most frequent DAs and group the remaining categories into one category.
    \item Self-efficacy: The confident tutor with greater self-efficacy uses more hedges during the low rapport interactions \citep{madaio2017think}, so we add the self-efficacy level into the state. (?)
    \item Pre-test: Tutee's pre-test score reflects the pre-existing proficiency in solving math problems.
    \item LIWC: Linguistic Inquiry and Word Count (LIWC) \citep{pennebaker2015a} extracts the count of words belonging to specific psycho-social categories. It has been shown to be effective in hedge classification \citep{raphalen-etal-2022-might}, and we wonder if it can play a role in deciding whether to use a hedge. 
    \item Nonverbal and paraverbal behavior: Nonverbals like smile and laughter can mitigate the face-threatening act \citep{warner2014laughing}. We include also head nods and gaze shifts.
\end{itemize}

Too many variables can cause the complexity of the construction of the state machine. Here we do not suggest choosing all state variables, we would be able to judge the effectiveness of different combinations based on different experiments. 

% %% State transition
% \paragraph{State transition}
% After the state variables are determined, we can convert the dialogue data into different states. We then define the transition function based on the transition distribution of the different dialogue states. For example (figure \ref{fig:state_transition}), the first conversation starts from state $A$. With a hedge, it leads to state $B$, then $B$ to $end$. We consider the probability from $A$ to $B$ with action $a=hedge$ is $P(B|A,a=hedge)=1$. A non-hedge leads to state $end$ directly in second conversation, so the probability from $A$ to $end$ with action $a=non-hedge$ is $P(end|A,a=non-hedge)=1$, and $P(B|A,a=hedge)$ remains 1. In the third conversation, the same hedge leads to state $C$ rather than $B$, so the probabilities varies accordingly, where $P(B|A,a=hedge)=0.5$ and $P(C|A,a=hedge)=0.5$. In this way, we can calculate the transition probability function across all the existing dialogue states.

% % Figure environment removed

% Baselines
% \subsection{Models}
% In this section, we present several RL models that can fit this task. 

% %% MDP 
% \subsubsection{MDP}
% At the end of the last section, we introduce the state transition function. We can just use this model as our baseline model to compare with other models to be introduced. 
% %% RL-LSTM 
% \subsubsection{RL-LSTM}
% \citet{zhao2018socially} use RL-LSTM \citep{bakker2001reinforcement} model to choose the best tutoring move. They incorporate conversational strategies and tutoring moves as the state variables. LSTM outputs the rewards directly with the current state as input.  In this work, they use tutoring moves, conversational strategies, and rapport as the state variable reaching the most reward and the great similarity in tutoring strategies policy compared with humans. LSTM adds past information to the reasoning, but memorizing the longer dependant information might be less adequate.

% %% Decision Transformer
% \subsubsection{Decision Transformer}
% The self-attention mechanism helps the modern language models (e.g., Transformer-based models) to efficiently process long texts (articles, stories, and dialogues). While \citet{chen2021decision} explores the self-attention mechanism's potential ability in the sequential decision-making process with a causal masked transformer. This work, by using the self-attention mechanism, reduces the slow and erratic propagation effect caused by Bellman's equation \citep{sutton2018reinforcement}. Additionally, Decision Transformer performs better in long sequences and also sparse rewards. Figure \ref{fig:hedge_dt} shows the proposed model with Decision Transformer. Return $R_t$ (accumulated reward), state $S_t$ and action $A_t$ are fed into the model sequentially. The transformer outputs the action autogressively. 

% % Figure environment removed