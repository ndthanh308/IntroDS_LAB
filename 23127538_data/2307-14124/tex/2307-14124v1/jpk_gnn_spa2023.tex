\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%\title{Memory-Efficient Graph Convolutional Networks for Event Processing}
\title{Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras
\thanks{The work presented in this paper was supported by the programme ``Excellence Initiative -- Research University'' for the AGH University of Krakow and partly by Sorbonne University}
}

%\author{\IEEEauthorblockN{Kamil Jeziorek}
%\IEEEauthorblockA{\textit{Embedded Vision Systems Group,} \\
%\textit{Computer Vision Laboratory,} \\
%\textit{Department of Automatic Control and Robotics,} \\
%\textit{AGH University of Krakow, Poland}\\
%\textit{\href{mailto:kjeziorek@student.agh.edu.pl}{kjeziorek@student.agh.edu.pl}}}
%\and
%\IEEEauthorblockN{Tomasz Kryjak, Senior Member, IEEE}
%\IEEEauthorblockA{\textit{Embedded Vision Systems Group,} \\
%\textit{Computer Vision Laboratory,} \\
%\textit{Department of Automatic Control and Robotics,} \\
%\textit{AGH University of Krakow, Poland}\\
%\textit{\href{mailto:tomasz.kryjak@agh.edu.pl}{tomasz.kryjak@agh.edu.pl}}}
%}

\author{
\IEEEauthorblockN{Kamil Jeziorek$^*$, Andrea Pinna $^\dag$, Tomasz Kryjak $^*$$^\dag$}
\IEEEauthorblockA{$^*$\textit{Embedded Vision Systems Group,} 
\textit{Department of Automatic Control and Robotics,} 
\textit{AGH University of Krakow, Poland}}
\IEEEauthorblockA{$^\dag$\textit{Sorbonne Universite, CNRS, LIP6, F-75005 Paris, France}}
\textit{\href{mailto:kjeziorek@student.agh.edu.pl}{kjeziorek@student.agh.edu.pl}, \href{mailto:andrea.pinna@lip6.fr}{andrea.pinna@lip6.fr}, \href{mailto:tomasz.kryjak@agh.edu.pl}{tomasz.kryjak@agh.edu.pl}}
}


\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
W\begin{abstract}
% Ostatnie postępy w badaniach nad kamerami zdarzeniowymi kładą nacisk na przetwarzanie danych w ich oryginalnej, rzadkiej formie, co pozwala na wykorzystanie unikalnych cech, takich jak wysoka rozdzielczość czasowa, szeroki zakres dynamiki, niskie opóźnienia i odporność na rozmycie obrazu. 
% Jedną z obiecujących metod analizy danych zdarzeniowych są grafowe sieci konwolucyjne (GNC).
% %Grafowe sieci konwolucyjne (GCN) pojawiły się jako obiecująca metoda przetwarzania danych zdarzeniowych w formacie grafu. 
% Jednak istniejące badania w tej dziedzinie koncentrują się głównie na optymalizacji kosztów obliczeniowych, pomijając koszty pamięci.
% W nieniejszej pracy rozważamy łącznie oba czynniki, tak aby uzsykać statysfakconującą skuteczność oraz wględnie niewielką złożoność modelu.  
% % W tej pracy skupiamy się na obu czynnikach, aby osiągnąć kompleksowe rozwiązanie. - powtórzenie
% W tym celu przeprowadziliśmy analizę porównawczą różnych operacji splotu grafów, biorąc pod uwagę czas wykonania, liczbę parametrów uczących modelu, wymagania dotyczące formatu danych i wyniki uczenia. 
% Nasze rezultaty wskazują na 450-krotne zmniejszenie liczby parametrów uczenia dla modułu ekstrakcji cech i 4.5-krotne zmniejszenie rozmiaru reprezentacji danych, przy jednoczesnym zachowaniu dokładności klasyfikacji na poziomie 52.3\%, o 6.3\% więcej w porównaniu z operacją splotu wykorzystywaną w najnowocześniejszych podejściach. %TODO Doprecyzować w senie, że zachowując względem czego >
% W celu dalszej oceny wydajności zaimplementowaliśmy architekturę wykrywania obiektów i oceniliśmy jej wydajność na zbiorze danych N-Caltech101. 
% Wyniki wykazały dokładność na poziomie 53.7\% mAP@0.5 i osiągnięciu szybkości wykonywania równą 82 grafów na sekundę.
% %TODO Wynik z porówaniem (czy to lepiej, gorzej, czy porówynalnie z SOTA)
Recent advances in event camera research emphasize processing data in its original sparse form, which allows the use of its unique features such as high temporal resolution, high dynamic range, low latency, and resistance to image blur. One promising approach for analyzing event data is through graph convolutional networks (GCNs). However, current research in this domain primarily focuses on optimizing computational costs, neglecting the associated memory costs.
In this paper, we consider both factors together in order to achieve satisfying results and relatively low model complexity. For this purpose, we performed a comparative analysis of different graph convolution operations, considering factors such as execution time, the number of trainable model parameters, data format requirements, and training outcomes. Our results show a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3\%, which is 6.3\% higher compared to the operation used in state-of-the-art approaches.
To further evaluate performance, we implemented the object detection architecture and evaluated its performance on the N-Caltech101 dataset. The results showed an accuracy of 53.7\% mAP@0.5 and reached an execution rate of 82 graphs per second.
\end{abstract}
\begin{IEEEkeywords}
\textit{event camera, dynamic vision sensors, event data processing, graph convolutional networks}
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{introduction}

% Kamery zdarzeniowe to nowoczesne czujniki wizyjne, których zasada działania inspirowana jest ludzkim okiem.
% W przeciwieństwie do tradycyjnych kamer, które rejestrują klatki w stałych odstępach czasu, kamery zdarzeniowe wykrywają zmiany natężenia światła w poszczególnych pikselach, co skutkuje generowaniem asynchronicznego strumienia informacji, w postaci tzw. zdarzeń. 
% Ta unikalna konstrukcja oferuje kilka zalet, w tym szeroki zakres tonalny, wysoką rozdzielczość czasową i dużą odporność na efekt rozmycia związany z ruchem. 
% W konsekwencji kamery zdarzeniowe zyskały dużą uwagę i znalazły zastosowanie w różnych trudnych scenariuszach, w których konwencjonalne kamery wizyjne napotykają ograniczenia, w szczególności w zaawansowanej robotyce mobilnej (pojazdy autonomiczne: drony, samochody) oraz do szeroko rozumianej poprawy rejestracji materiału wideo (poprawa w warunkach nierównomiernego oświetlenia - HDS, zwiększenie liczby klatek na sekundę) \cite{PAMI}. %TODO dodać cite PAMI Davide - OK

% Pomimo ich wyraźnych zalet, przetwarzanie danych z kamer zdarzeniowych jest trudne ze względu na ich rzadki i przestrzenno-czasowy charakter, który wymaga podejść różniących się od tych opracowanych dla tradycyjnych systemów wizyjnych. 
% Wczesne podejścia obejmowały rzutowanie zdarzeń na gęste dwuwymiarowe pseudo-obrazy (reprezentacje), takie jak np. ramki zdarzeń, lub ich rekonstrukcję na ramki w skali szarości z wykorzystaniem głębokich sieci neuronowych. 
% Chociaż metody te pozwoliły wykorzystać potencjał konwolucyjnych sieci neuronowych, mają one wady. 
% Przede wszystkim proces rzutowania lub rekonstrukcji powoduje utratę kluczowych cech kamer zdarzeniowych, a mianowicie wysokiej rozdzielczości czasowej i zalet wynikających z rzadkości danych (efektywności obliczeniowej i energetycznej).
% Ponadto proces rekonstrukcji jest dość złożony obliczeniowe, co zostało przedstawione w pracy \cite{reconstruction_e2vid}. %TODO Cytat do pracy Davide - OK 
% Skłoniło to badaczy do poszukiwania alternatywnych rozwiązań w zakresie przetwarzania danych zdarzeniowych przy jednoczesnym zachowaniu ich rzadkości.

% Pierwsze próby rozwiązania tego problemu polegały na wykorzystaniu metod opartych na filtrach oraz sieciach neuronowych impulsowych (SNN). Jednak metody oparte na filtrach wymagają ręcznego definiowania równań, co uniemożliwia osiągnięcie dobrych rezultatów dla bardziej skomplikowanych zadań. Natomiast metody oparte na SNN mają jeszcze słabo rozwinięte reguły uczenia się, a ich implementacja może być bardziej skomplikowana w porównaniu do tradycyjnych sieci konwolucyjnych.

% Inną, stosunkową nową propozycją jest wykorzystanie grafowych sieci neuronowych.
% Ostatnie postępy w tej dziedzinie pokazały, że przetwarzanie zdarzeń za pomocą grafowych sieci konwolucyjnych jest możliwe. Niewątpliwą zaletą takiego podejścia jest przetwarzanie danych zdarzeniowych w postaci oryginalnej chmury punktów przy jednoczesnym wykorzystaniu potencjału operacji konwolucji. %TODO ew. wymienić zalety - OK
% Dostrzegliśmy jednak, że istniejące podejścia koncentrowały się przede wszystkim na minimalizacji złożoności obliczeniowej, przykładając mniejszą wagę do efektywności wykorzystania pamięci. 

%TODO Przerobić Niniejszy artykuł przedstawia nowe spojrzenie na optymalizację pamięci w przetwarzaniu danych zdarzeń, biorąc pod uwagę zarówno wydajność, jak i złożoność obliczeniową. 
%TODO  Outlook

% Pozostała część artykułu została zorganizowana w następujący sposób. Rozdział 2 prezentuje kamerę zdarzeniową wraz z jej zaletami oraz mechanizmem generacji informacji. Rozdział 3 opisuje grafowe sieci konwolucyjne oraz zasadę działania operacji konwolucji oraz poolingu. W rozdziale 4 omawiamy istniejące prace dotyczące przetwarzania zdarzeń przy wykorzystaniu grafowych sieci konwolucyjnych oraz ich problemy związane z wykorzystaniem pamięci. Rozdział 5 prezentuje eksperymenty, które przeprowadziliśmy w celu wykazania poprawy wykorzystania pamięci. Na końcu podsumowujemy nasze rezultaty w rozdziale 6 oraz przedstawiamy przyszłe plany badawcze.

%TODO Graphical abstraact - opisać jak go widzę

Event cameras are modern vision sensors whose operating principle is inspired by the human eye.
Unlike traditional cameras, which record frames at fixed intervals, event cameras detect changes in light intensity in individual pixels, resulting in the generation of an asynchronous stream of information, into so-called events. 
This unique design offers several advantages, including a high tonal range, high temporal resolution and high resistance to motion blur effects. 
As a result, event cameras have gained a lot of attention and have found applications in a variety of challenging scenarios where conventional video cameras face limitations, particularly in advanced mobile robotics (autonomous vehicles: drones, cars) and for broadly improving video capture (improvement in uneven lighting conditions, increasing frame rates) \cite{PAMI}. 

Despite their clear advantages, event camera data processing is difficult due to its sparse and spatial-temporal nature, which requires approaches that differ from those developed for traditional vision systems. 
Early approaches involved projecting events into dense two-dimensional pseudo-representations, such as event frames, or reconstructing them into greyscale frames using deep neural networks. 
Although these methods have made it possible to exploit the potential of convolutional neural networks, they have drawbacks. 
First of all, the projection or reconstruction process loses the key features of event cameras, specifically the high temporal resolution and the advantages of data sparsity (computational and energy efficiency).
In addition, the reconstruction process is quite computationally complex, as shown in the work \cite{reconstruction_e2vid}.

This has prompted researchers to look for alternative solutions for processing event data while keeping it in a sparse form.
The first attempts to solve this problem involved filter-based methods \cite{filters} and spiking neural networks (SNNs) \cite{snn}. However, filter-based methods require manual definition of equations, which makes it difficult to achieve good results for more complex tasks. SNN-based methods, on the other hand, still have underdeveloped learning rules, and their implementation can be more complicated compared to traditional convolutional networks.
Another relatively new proposal is the use of graph neural networks.
Recent advances in this field have shown that event processing using graph convolutional networks is possible \cite{b1} \cite{b2} \cite{b3}. The undoubted advantage of this approach is to process event data in the form of the original point cloud while exploiting the potential of convolution operations.
However, we saw that existing approaches focused primarily on reducing computational complexity, placing less importance on memory efficiency. 

% W niniejszym artykule postawiliśmy sobie za cel przeanalizowanie wpływu wykorzystania sieci grafowych na złożoność pamięciową. W tym celu porównaliśmy różne operacje konwolucji grafowych pod względem wykorzystywanych danych do przetwarzania oraz rozmiaru sieci. Wyniki pokazały, że istnieje możliwość redukcji obu współczynników przy zachowaniu stosunkowo dobrych wyników w porównaniu do najnowszych badań. Ta praca stanowi uzupełnienie istniejącej wiedzy, ponieważ dotychczasowi autorzy skupiali się głównie na optymalizacji złożoności obliczeniowej.

In this paper, our objective was to analyse the impact of employing graph networks on memory complexity. To achieve this, we conducted a comparison of various graph convolution operations based on their data requirements and network size. The findings demonstrated the potential to decrease both data and network size while still achieving satisfactory performance in comparison to state-of-the-art results. This study serves as a valuable addition to current knowledge since previous researchers have primarily concentrated on only optimising computational complexity.

%TODO Przerobić Niniejszy artykuł przedstawia nowe spojrzenie na optymalizację pamięci w przetwarzaniu danych zdarzeń, biorąc pod uwagę zarówno wydajność, jak i złożoność obliczeniową. 


%TODO Graphical abstraact - opisać jak go widzę

The remainder of the paper is organised as follows. Section \ref{event_cameras} presents event cameras, their advantages and the mechanism of information generation. Section \ref{gnn} describes graph convolutional networks and the principle of convolution and pooling operations. Section \ref{related} discusses previous work on event processing using graph convolutional networks and their memory usage problems. Section \ref{experience} presents the experiments we conducted to demonstrate improvements in memory usage. Finally, we summarise our results and present future research plans.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Event Cameras}\label{event_cameras}

% Kamery zdarzeniowe, znane również jako Dynamic Vision Sensors (DVS), to kamery inspirowane biologią. Ich unikalna zasada działania oferuje szereg korzyści w porównaniu z tradycyjnymi kamerami, zmieniając sposób przechwytywania i przetwarzania informacji wizualnych. %TODO bez rewolucji może. - OK

% Jedną z charakterystycznych cech kamer zdarzeniowych jest ich asynchroniczny charakter. 
% W przeciwieństwie do konwencjonalnych kamer, które rejestrują poziom jasności globalnie dla wszystkich pikseli w ustalonych odstępach czasu, kamery zdarzeniowe rejestrują zmiany jasności dla pojedynczych pikseli. 
% Ta niezależność na poziomie pikseli umożliwia kamerom zdarzeniowym przechwytywanie precyzyjnych informacji czasowych, ponieważ zdarzenia są generowane tylko wtedy, gdy wystąpi znacząca zmiana jasności danego piksela (spowodowana ruchem na scenie, ruchem własnym kamery lub zmianą oświetlenia). 
% W konsekwencji, kamery zdarzeniowe wyróżniają się w rejestrowaniu dynamicznych scen z minimalnym rozmyciem ruchu. 
% Proces generowania zdarzeń w DVS jest kontrolowany przez mechanizm progowy. Gdy logarytmiczna zmiana natężenia światła \(\Delta L\) w określonym pikselu \((x_i, y_i)\) przekroczy wstępnie zdefiniowaną wartość progową \(C\), generowane jest zdarzenie. Można to wyrazić jako:

% \begin{equation}
% \Delta L(x_i,y_i,t_i) = L(x_i,y_i,t_i) - L(x_i, y_i, t_i - t_i^*) \geq p_i C \label{generate}
% \end{equation}

% gdzie \(t_i\) reprezentuje czas wystąpienia aktualnego zdarzenia, a \(t_i^*\) zdarzenia poprzedniego. Równanie to określa podstawową zasadę generowania zdarzeń w kamerach zdarzeniowych, zapewniając, że rejestrowane są tylko istotne i znaczące zmiany. Każde wygenerowane zdarzenie zawiera cztery informacje: współrzędne piksela \((x_i, y_i)\), dokładny znacznik czasu \(t_i\) pochodzący z wewnętrznego zegara oraz polaryzację \(p_i\) wskazującą, czy zmiana natężenia światła była dodatnia bądź ujemna. Zdarzenia te łącznie tworzą przestrzenno-czasowy ciąg asynchronicznych danych:

% \begin{equation}
% E = \{e_i\}_{i=1}^N, e_i = (x_i, y_i, t_i, p_i) \label{events}
% \end{equation}

% Co więcej, dzięki wysokiemu zakresowi dynamiki (ponad 120 dB), kamery zdarzeniowe są w stanie wiernie odtwarzać sceny zawierające zarówno jasne, jak i ciemne obszary, rejestrując szeroki zakres intensywności światła. Te cechy sprawiają, że kamery zdarzeniowe szczególnie dobrze sprawdzają się w trudnych warunkach oświetleniowych, takich jak słabo oświetlone środowiska lub sceny o wysokim kontraście (nierównomiernym oświetleniu).

% % Przetwarzanie danych z kamer zdarzeniowych stanowi wyzwanie, szczególnie jeśli celem jest wykorzystaniu ich pełnego potencjału. %TODO ref davide PAMI
% %TODO Tu już było we wstępnie - w jednym miejscy wystarczy. - ok
% Tradycyjne algorytmy widzenia komputerowego, zaprojektowane głównie do przetwarzania danych opartych na klatkach, nie mogą być bezpośrednio stosowane do asynchronicznego charakteru danych zdarzeń. 
% Naukowcy zbadali różne techniki, aby rozwiązać ten problem, takie jak konstruowanie ramek zdarzeń lub konwertowanie zdarzeń na obrazy w skali szarości. 
% Podejścia te mają na celu wypełnienie luki między zdarzeniami a istniejącymi algorytmami wizyjnymi. Wiążą się one jednak z pewnymi kompromisami, skutkującymi utratą wysokiej rozdzielczości czasowej charakterystycznej dla DVS. 
% Co więcej, konwersja na ramki lub obrazy może nie w pełni uchwycić bogactwa i dynamiki oryginalnego strumienia zdarzeń. 
% Dodatkowo, wynikowe ramki mogą zawierać piksele z wartościami zerowymi, co wymaga dodatkowej obsługi przy użyciu tradycyjnych konwolucyjnych sieci neuronowych (CNN).
% % Dodatkowo, wynikowe ramki mogą zawierać piksele z wartościami zerowymi, co wymaga dodatkowych obliczeń przy użyciu tradycyjnych konwolucyjnych sieci neuronowych (CNN).

% % ew. obsługi tego. - OK

% Aby sprostać tym wyzwaniom, naukowcy zwrócili uwagę na przetwarzanie zdarzeń bezpośrednio w oryginalnej przestrzenno-czasowej chmurze punktów. Jednym z obiecujących rozwiązań jest wykorzystanie grafowych sieci konwolucyjnych (GCN) do przetwarzania zdarzeń.

Event cameras, also known as Dynamic Vision Sensors (DVS), are bio-inspired cameras. Their unique operating principle offers a number of advantages over traditional cameras, changing the way visual information is captured and processed.

One of the distinctive features of event cameras is their asynchronous nature. 
Unlike conventional cameras, which record brightness levels globally for all pixels at fixed intervals, event cameras record brightness changes for individual pixels. 
This independence at the pixel level enables event cameras to capture precise temporal information, as events are only generated when a significant change in the brightness of a given pixel occurs (due to movement in the scene, the cameras' own movement or a change in lighting). 
Consequently, event cameras excel at capturing dynamic scenes with minimal motion blur. 
The event generation process in DVS is controlled by a threshold mechanism. When the logarithmic change in light intensity \(\Delta L\) at a specific pixel \((x_i, y_i)\) exceeds a predefined threshold value \(C\), an event is generated. This can be expressed as:

\begin{equation}
\Delta L(x_i,y_i,t_i) = L(x_i,y_i,t_i) - L(x_i, y_i, t_i - t_i^*) \geq p_i C \label{generate}
\end{equation}

where \(t_i\) represents the time of occurrence of the current event and \(t_i^*\) the previous event. This equation defines the basic principle of event generation in event cameras, ensuring that only significant and meaningful changes are recorded. Each generated event includes four items of information: the pixel coordinates \((x_i, y_i)\), the exact timestamp \(t_i\) derived from the internal clock, and the polarity \(p_i\) indicating whether the change in light intensity was positive or negative. These events together form a spatial-temporal sequence of asynchronous data:

\begin{equation}
E = \{e_i\}_{i=1}^N, e_i = (x_i, y_i, t_i, p_i) 
\label{events}
\end{equation}

Moreover, thanks to their high dynamic range (over 120 dB), event cameras are able to faithfully reproduce scenes containing both bright and dark areas, capturing a wide range of light intensities. These features make event cameras particularly well suited to difficult lighting conditions, such as poorly lit environments or scenes with high contrast (uneven lighting).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Graph Convolutional Networks}\label{gnn}

% Grafowe sieci konwolucyjne (GCN) to rodzaj modeli uczenia maszynowego, które operują na danych o strukturze grafowej. W odróżnieniu od tradycyjnych sieci neuronowych, które są przeznaczone głównie do danych siatkowych, GCN są w stanie wydobywać złożone relacje i zależności obecne w danych grafowych. Graf składa się z wierzchołków \(V\) oraz krawędzi \(\mathcal{E}\), które reprezentują relacje między dwoma wierzchołkami. GCN wykorzystuje tę strukturę grafu do nauki i propagacji informacji pomiędzy węzłami, co umożliwia im przewidywanie lub wykonanie zadań na podstawie istniejących wzorców połączeń.

% %TODO co to jest węzeł i jak się ma do wierzchołka - to trzeva łopatologicznie - OK, akurat węzeł==krawędź, w literaturze naprzemiennie używane, dodałem natomiast do krawędzi, że określają relację pomiędzy dwoma wierzchołkami

% \subsection{Graph Convolution}\label{conv}

% Operacja konwolucji jest kluczowym elementem grafowych sieci neuronowych. Pozwala na propagację informacji między wierzchołkami w celu aktualizacji ich reprezentacji oraz uzyskania kluczowych informacji o grafie. Ogólna zasada tej operacji określana jest jako agregacja sąsiedztwa \(N(i)\) i realizowana jest dla każdego wierzchołka \(x_i\) w trzech krokach: message (wiadomość), aggregation (agregacja) oraz update (aktualizacja) function.

% Funkcja message jest odpowiedzialna za generowanie informacji przekazywanych między wierzchołkami. Definiowana jest na poziomie pojedynczej krawędzi pomiędzy wierzchołkami \(x_i\) i \(x_j\) i może uwzględniać atrybuty krawędzi \( e_{i,j}\), jeśli są zdefiniowane:

% \begin{equation}
% msg_{i,j} = \phi (x_i, x_j, e_{i,j}) \label{events}
% \end{equation}

% Funkcja aggregation zbiera przekazane wiadomości z sąsiadujących wierzchołków \(x_j\), które zostały uzyskane za pomocą funkcji message, i łączy je na poziomie pojedynczego wierzchołka \(x_i\):

% \begin{equation}
% agr_i = \bigoplus_{j \in N(i)} msg_{i, j} \label{events}
% \end{equation}

% Na koniec wykonuje się funkcję aktualizacji, która odpowiedzialna jest za aktualizację reprezentacji wierzchołka \(x_i\) na podstawie zgromadzonych informacji z operacji agregacji. W tej fazie reprezentacja wierzchołka jest aktualizowana na podstawie informacji z sąsiadów.

% \begin{equation}
% x_i' = \gamma (agr_i)
% \end{equation}

% gdzie funkcje \(\phi\) oraz \(\gamma\) mogą być reprezentowane jako funkcje liniowe lub MLP, natomiast operacja \(\bigoplus\) określona może być funkcją sumy, wartości średniej lub maksymalnej. Warto zaznaczyć, że funkcje message, aggregation i update mogą być definiowane na różne sposoby w zależności od konkretnego modelu grafowych sieci konwolucyjnych. Różne warianty GCN mogą wykorzystywać różne operacje i przekształcenia w tych funkcjach, dostosowane do konkretnych wymagań i struktury danych grafowych. Wpływ różnych wariantów konwolucji dla danych zdarzeniowych jest tematem tego artykułu. %TODO zdanie, że eksperymenty takie sa tematem artykułu - OK

% \subsection{Graph Pooling}\label{pool}

% Warstwy łączące w sieciach GCN służą podobnemu celowi, co ich odpowiedniki w konwolucyjnych sieciach neuronowych: zmniejszeniu wymiarowości danych przy jednoczesnym zachowaniu kluczowych informacji oraz generalizacji.
% %TODO - też generalizacji? Tak
% W sieciach GCN, warstwa poolingu polega na grupowaniu wierzchołków na podstawie regularnej siatki o określonym wymiarze lub za pomocą metod klasteryzacji punktów w przestrzeni. %TODO nieco niejasne - OK
% Proces poolingu obejmuje wybór reprezentatywnych wierzchołków lub agregację informacji wewnątrz każdej grupy lub klastra. Wybór ten może być dokonywany na podstawie różnych kryteriów, takich jak znaczenie wierzchołka, maksymalna wartość lub średnia wśród wszystkich wierzchołków. Wynikiem tej operacji jest nowy wierzchołek, który reprezentuje grupę.
% Poprzez tworzenie gęstszych reprezentacji poprzez pooling, GCN zmniejsza złożoność obliczeniową w kolejnych warstwach. To umożliwia bardziej efektywne przetwarzanie danych o dużej skali, jednocześnie zachowując istotne cechy i przepływ informacji. Ponadto, pozwala to również na zastosowanie warstw w pełni połączonych do klasyfikacji danych. %TODO nie grafu tylko ... może klasyfikacji danych - OK 

Graph convolutional networks (GCNs) \cite{gcns} are a type of machine learning models that operate on graph-structured data. Unlike traditional neural networks, which are mainly designed for grid data, GCNs are able to extract the complex relationships and dependencies present in graph data. A graph is made up of vertices \(V\) and edges \(\mathcal{E}\), which represent the relationships between two vertices. GCN uses this graph structure to learn and propagate information between nodes, enabling them to predict or perform tasks based on existing connection patterns.

\subsection{Graph Convolution}\label{conv}

The convolution operation is a key element of graph neural networks. It allows the propagation of information between vertices in order to update their representation and obtain key information about the graph. The general principle of this operation is referred to as neighbourhood \(N(i)\) aggregation and is performed for each vertex \(x_i\) in three steps: message, aggregation and update function.

The message function is responsible for generating the information passed between vertices. It is defined at single edge level between vertices \(x_i\) and \(x_j\) and can take into account edge attributes \( e_{i,j}\) if they are defined:

\begin{equation}
msg_{i,j} = \phi (x_i, x_j, e_{i,j}) \label{msg}
\end{equation}

The aggregation function collects forwarded messages from neighbouring vertices \(x_j\), which were obtained by the message function, and combines them at the level of a single vertex \(x_i\):

\begin{equation}
agr_i = \bigoplus_{j \in N(i)} msg_{i, j} \label{agr}
\end{equation}

Finally, the update function is executed, which is responsible for updating the vertex representation \(x_i\). In this phase, the vertex representation is updated based on the aggregated information from its neighbours.

\begin{equation}
x_i' = \gamma (agr_i) \label{update}
\end{equation}

where the functions \(\phi\) and \(\gamma\) can be represented as linear or MLP functions, while the operation \(\bigoplus\) can be specified as a sum, mean or maximum value function. It is worth noting that message, aggregation and update functions can be defined in different ways depending on the specific model of graph convolutional networks. Different GCN variants may use different operations and transformations in these functions, adjusted to the specific requirements and structure of the graph data. The impact of different convolution variants for event data processing is the topic of this article. 

\subsection{Graph Pooling}\label{pool}

Pooling layers in GCNs serve a similar purpose to their equivalents in convolutional neural networks: reducing the dimensionality of the data while retaining key information and generalisation.

In GCNs, the pooling layer involves grouping vertices based on a regular grid of a specific dimension \cite{voxel_grid} or using methods to cluster points in space \cite{cluster}. 
The pooling process involves selecting representative vertices or aggregating information within each group or cluster. This selection can be based on different criteria, such as the importance of a vertex, the maximum value or the average among all vertices. The result of this operation is a new vertex that represents the group.
By creating denser representations through pooling, GCN reduces the computational complexity in subsequent layers. This allows for more efficient processing of large-scale data, while preserving essential features and information flow. Furthermore, it also allows the use of fully connected layers for data classification.

%TODO: Przydałby się rysunek do tego... takich schemacik ilustrujący grafy, konwolucje i pooling

\section{Previous works}\label{related}

% Niniejszy przegląd skupia się wyłącznie na grafowych sieciach konwolucyjnych stosowanych do przetwarzania danych zdarzeniowych. Ze względu na stosunkowo nowe podejście, ilość prac dotyczących tego zagadnienia jest jeszcze niewielka. 
% %TODO Jakieś zdanie wstępu ,,, że przegląd tylko do GNN dla DVS i że prac jest mało. - ok?

% Początkowe prace \cite{standard1} \cite{standard2} koncentrowały się na konstrukcji grafu z przychodzących zdarzeń i jego przetwarzaniu w całości za pomocą grafowych sieci konwolucyjnych. 
% Choć te operacje są wykonywane tylko na istotnych informacjach związanych ze zdarzeniami, co redukuje koszty obliczeniowe związane z gęstymi reprezentacjami o zerowej wartości (opisanymi w sekcji \ref{event_cameras}), wymagają one powtarzania operacji na wcześniejszych zdarzeniach, które nie mają wpływu na nowe zdarzenia.
% W praktyce oznacza to, że konieczne jest przetwarzanie całego grafu od początku dla nowych zdarzeń, nawet jeśli większość informacji jest niezmieniona. Taka redundancja obliczeniowa może prowadzić do nieefektywnego wykorzystania zasobów obliczeniowych i ograniczeń czasowych. %TODO A można to nieco rozwinąć ? - ok

% Ostatnie badania skupiły się na zastosowaniu grafowych sieci neuronowych do zdarzeń, aktualizując sieć asynchronicznie zdarzenie po zdarzeniu. Celem takiej operacji jest wykonywanie obliczeń tylko na zmieniających się zdarzeniach, co zwiększa wydajność obliczeniową. W serii publikacji \cite{b1}, \cite{b2}, \cite{b3} zaproponowano technikę, w której propagacja jest wykonywana dla pojedynczego zdarzenia. Nowe zdarzenie jest łączone z istniejącym grafem, a sąsiadujące z nim wierzchołki są aktualizowane. Pozostałe wierzchołki, które nie mają nowego sąsiada, nie wymagają aktualizacji, co prowadzi do redukcji ilości operacji. %TODO serii publikacji i czy ew. coś więcej > - OK i OK
% % Ta aktualizacja jest propagowana do kolejnych warstw, stopniowo aktualizując więcej sąsiadów. Takie podejście zapewnia, że operacje są wykonywane wyłącznie na zmieniających się wierzchołkach, pomijając niepotrzebne obliczenia.

% Jednakże, aby przeprowadzić taką operację, warstwy w tych sieciach muszą posiadać informacje o ich aktualnym stanie. 
% Na przykładzie opisanym w publikacji \cite{b1}, z całego zbioru zdarzeń jest wybierany określony podzbiór, który następnie przechodzi przez sieć jako zwykły graf w celu inicjalizacji wewnętrznych warstw. %TODO styl. personifkiacja - ok
% Następnie, dla każdego nowego przychodzącego zdarzenia, określane są wierzchołki wymagające aktualizacji, a wynik tej operacji jest propagowany do kolejnych warstw. %TODO styl. -ok?
% Ponieważ każda warstwa ma własną kopię grafu, wymagana pamięć do jego przechowywania jest znacząca. 
% Zapotrzebowanie na pamięć wzrasta wraz z liczbą warstw, rozmiarami obrazu z kamer (obecnie istnieją kamery o rozdzielczości 1 megapiksela tj. HD 1280 x 720 pikseli \cite{b4}) oraz liczbą wierzchołków na graf.
% Łącznie pamięć potrzebna do przechowywania wszystkich kopii może przekroczyć możliwości systemów wbudowanych.

% Z drugiej strony, zdarzenia w swojej oryginalnej formie składają się z czterech wartości (\(x, y, t, p\)), jak opisano w rozdziale \ref{event_cameras}. %TODO w PL nie ma słowa sekcja (tylko zwłok)
% Podczas generowania grafu, każde zdarzenie jest reprezentowane jako wierzchołek o lokalizacji przestrzennej 3D (\(x, y, t \)) i wartości \(p\), a określanie krawędzi wymaga ustalenia indeksów dwóch wierzchołków (\(e_i, e_j\)). %TODO powt. i x2 podczas i określić - ok
% Jak wspomniano w sekcji \ref{conv}, istnieją różne wersje operacji splotu grafu, z których niektóre uwzględniają tylko informacje o położeniu wierzchołków, wartościach i indeksach krawędzi, podczas gdy inne uwzględniają również atrybuty specyficzne dla krawędzi. 
% W publikacjach \cite{b1}, \cite{b2}, \cite{b3} zastosowana funkcja splotu wymaga uwzględnienia tych wartości. %TODO zastosowana funkcja splotu czy jakoś tak - ok
% Osiąga się to za pomocą miary kartezjańskiej, w której wynikowa wartość zawiera dodatkowe informacje. 
% Wraz ze wzrostem liczby krawędzi wzrasta również liczba ich atrybutów, co prowadzi do większego zapotrzebowania na pamięć do przechowywania i przetwarzania danych.
 
% Podczas gdy wszystkie prace koncentrują się na minimalizacji kosztów obliczeniowych, naszym celem jest przedstawienie innego aspektu, który obejmuje wykorzystanie zasobów pamięci. 
% Obejmuje to zarówno dane generowane podczas budowy grafu, jak i architekturę sieci, przy jednoczesnym dążeniu do osiągnięcia dobrych wyników i krótkiego czasu przetwarzania. %TODO jak wiadomo nie używamy słowa optymalne - ok



This review focuses exclusively on graph convolutional networks applied to event data processing. Due to the relatively new approach, the amount of work on this topic is still small. 

The initial works \cite{standard1} \cite{standard2} focused on constructing a graph from incoming events and processing it in its entirety using graph convolutional networks. 
Although these operations are only performed on the relevant event information, which reduces the computational costs associated with dense zero-valued representations (as described in the \ref{event_cameras} section), they require repeated operations on previous events that are not affected by new events.
In practice, this means that it is necessary to process the entire graph from the beginning for new events, even if most of the information is unchanged. Such computational redundancy can lead to inefficient use of computing resources and time constraints.

Recent researches have focused on applying graph neural networks to events, updating the network asynchronously event by event. The aim of such an operation is to perform computations only on changing events, thus increasing computational efficiency. In a series of publications \cite{b1}, \cite{b2}, \cite{b3}, a technique has been proposed in which propagation is performed for a single event. A new event is merged with an existing graph and the neighbouring vertices are updated. The remaining vertices that do not have a new neighbour do not need to be updated, leading to a reduction in the number of operations.

However, in order to perform this operation, the layers in these networks require information about their current state. 
In the example described in the paper \cite{b1}, a particular subset of events is selected from the entire set of events, which is then passed through the network as a simple graph to initialise the internal layers.
Then, for each new incoming event, the neighbouring vertices that need to be updated are identified, and the result of this operation is propagated to the next layers. 
As each layer has its own copy of the graph, the memory required to store it is considerable. 
The memory requirement increases with the number of layers, the size of the camera images (there are currently cameras with a resolution of 1 megapixel, i.e. HD 1280 x 720 pixels \cite{b4}) and the number of vertices per graph.
The total memory required to store all copies may exceed the capacity of embedded systems.

On the other hand, events in their original form consist of four values (\(x, y, t, p\)), as described in \ref{event_cameras}. When the graph is created, each event is represented as a vertex with a 3D spatio-temporal location (\(x, y, t \)) and an attribute \(p\), while edge definition requires the indexes of two vertices to be determined.
As mentioned in the \ref{conv} section, there are different versions of the graph convolution operation, some of which only consider vertex attribute information, and edge indices, while others also consider edge-specific attributes. 
In the publications \cite{b1}, \cite{b2}, \cite{b3} applied convolution function requires these values to be taken into account.
This is achieved using a Cartesian coordinates of linked nodes in which the resulting value contains additional information. 
As the number of edges increases, the number of their attributes also increases, leading to a larger memory requirement for data storage and processing.

While all work focuses on minimising computational costs, our aim is to present another aspect that involves the usage of memory resources. 
This includes both the data generated during graph construction and the network architecture, while also aiming for good performance and low processing times.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}\label{experience}

% Wszystkie przedstawione eksperymenty zostały przeprowadzone na zbiorze danych N-Caltech101 \cite{b5}, który składa się ze 100 klas (oryginalna wersja ma 101 klas, w tym "Faces" i "Faces Easy", które zostały połączone w wersji neuromorficznej). 
% Obejmuje on 8246 próbek, przy czym wymiary danych nie przekraczają 240 x 180. 
% Dane te pozwalają na zastosowanie ich zarówno w zadaniach klasyfikacji, jak i detekcji, ponieważ każda próbka zawiera pojedynczy obiekt. %TODO ja bym użył klasyfikacji i detekcji - ok
% Podczas procesu uczenia dane zostały losowo podzielone na 80\% zbioru treningowego i 20\% zbioru testowego, zapewniając równy rozkład próbek z każdej klasy.

% Do przeprowadzenia eksperymentów wykorzystaliśmy bibliotekę PyTorch Geometric \cite{b6}, która zapewnia wygodne narzędzia i funkcje do pracy z danymi o strukturze grafu i umożliwia płynną integrację z architekturami grafowych sieci konwolucyjnych.

% All the presented experiments were conducted on the N-Caltech101 \cite{b5} dataset, which consists of 100 classes (the original version has 101 classes, including 'Faces' and 'Faces Easy', which were combined in the neuromorphic version). 

All the presented experiments were conducted on the N-Caltech101 dataset \cite{b5}, which was created by using a~moving event camera to capture the standard Caltech101 dataset displayed on a monitor. The N-Caltech101 dataset consists of 100 classes, where the original version of the dataset includes 101 classes, including 'Faces' and 'Faces Easy', which were merged in the neuromorphic version.
It includes 8246 samples, with data dimensions not exceeding 240 x 180. 
This data allows it to be used in both classification and detection tasks, as each sample contains a single object.
During the training process, the data was randomly split into an 80\% training set and a 20\% test set, ensuring an equal distribution of samples from each class.

For the experiments, we used the PyTorch Geometric library \cite{b6}, which provides convenient tools and features for working with graph-structured data and allows seamless integration into graph convolutional network architectures.

\subsection{Graph size}\label{graph_size}

% Aby ocenić wymiarowość danych wymaganych przez graf, przeprowadziliśmy eksperyment na całym zbiorze danych. 
% Zastosowaliśmy metodę przedstawioną w \cite{b2} do tworzenia grafu dla każdej próbki. 
% Ustaliliśmy maksymalną liczbę zdarzeń na graf równą 25 000, przy czym wybierane były zdarzenia o największym zagęszczeniu w określonym oknie czasowym. %TODO to by trzeba objaśnić - ok?
% Promień sąsiedztwa względem którego wierzchołki były łączone krawędziami ustawiliśmy na 5, a żeby ograniczyć liczbę wygenerowanych krawędzi, określiliśmy limit sąsiadów na 32 dla wierzchołka. %TODO jw. to nie będzie jasne dla postronnego czytelnika - ok?
% Przebadaliśmy dwa różne podejścia: w pierwszym znormalizowaliśmy czasy wystąpienia zdarzeń do wartości zbliżonych do rozdzielczości danych (w zakresie od 0 do 100), a w drugim nie znormalizowaliśmy, gdzie wartości czasu były w zakresie mikrosekund. Normalizacja czasu ma istotny wpływ na wyniki uczenia, ale także wpływa na rozmiar danych. W przypadku zwiększenia wartości czasu, wierzchołki znajdują się dalej od siebie, co skutkuje lokalnym łączeniem zdarzeń na podstawie ustalonego promienia sąsiedztwa i redukcją ilości wygenerowanych krawędzi. %TODO przebadaliśmy > pierwsza część jakoś inaczej L w pierwszym znornalizowaliśmy czasy wystąpienia zdarzeń ? - ok?
% Rysunek \ref{fig} ilustruje różnice między tymi dwoma podejściami.

% % Figure environment removed

% Dla całego zbioru danych N-Caltech101, średnia liczba zdarzeń wynosi 24 457, co daje 24 457 wartości wierzchołków (p) i przestrzenno-czasowych (x, y, t). 
% Natomiast średnia liczba krawędzi dla danych nieznormalizowanych wynosi 756 744, podczas gdy dla danych znormalizowanych wynosi 381 563.

% Biblioteka PyTorch Geometric reprezentuje wartości wierzchołków jako int32, a pozycje przestrzenno-czasowe jako wektory float32. %TODO badania nad optymalizacją w przyszłości ? Ew. do future work
% Każda krawędź jest reprezentowana jako wektor dwuelementowy z indeksami wierzchołków jako wartościami int32. Ostatecznie, wartość atrybutu krawędzi składa się z trzech wartości float32. 
% Łącznie każdy wierzchołek wymaga 16 bajtów, podczas gdy każda krawędź, wraz z atrybutem, wymaga 40 bajtów. %TODO ale to nie wynika z ograniczenia na liczbę sąsiadóW ? bo coś mie to niepasuje skąd tyle bajtów... 
% Średnia ilość pamięci wykorzystywanej przez graf wynosi około 15 MB dla danych znormalizowanych, w porównaniu do 30.5 MB dla danych nieznormalizowanych. %TODO tylko MB styknie - ok

% Jeśli pominąć wartości atrybutów krawędzi w grafie, rozmiar danych dla znormalizowanych grafów zmniejsza się do 3.5 MB, podczas gdy dla danych nieznormalizowanych wynosi 6.5 MB. %TODO jw. - ok
% Zatem pominięcie wartości atrybutów krawędzi zmniejsza rozmiar danych średnio ponad 4.5-krotnie. Dla porównania, surowy obraz o rozmiarze 240 na 180 pikseli i pojedynczym kanale, wykorzystujący typ danych uint8 zajmuje 0.04 MB, podczas gdy obraz 3-kanałowy zajmuje 0.12 MB. To pokazuje, jak dużo pamięci wymagają takie reprezentacje danych i jak istotna jest ich redukcja. %TODO j.w.  + może słowo komentarza, że ta reprezentajca jest jednak "ciężka'. Inna sprawa, czy tego grafu nie można jakoś sprytniej opicać, jakieś tablice sąsiedztwa itp ? - ok

% Należy zauważyć, że powyższe wartości są określone dla niskiej rozdzielczości i ograniczonej liczby zdarzeń w grafie. W przypadku większych rozmiarów danych, które zostały przetworzone przy użyciu kamery o rozdzielczości 1 megapiksela \cite{b4}, liczba zdarzeń na grafie, a tym samym liczba krawędzi, znacznie wzrośnie. Aby temu zaradzić, można przeskalować rozmiar danych wejściowych, podobnie jak jest to często praktykowane w przypadku klasycznych sieci konwolucyjnych (CNN). 

% W tym eksperymencie skupiliśmy się tylko na pominięciu atrybutów krawędzi w celu redukcji pamięci. Jednak istnieje również możliwość redukcji pamięci poprzez wykorzystanie mniejszych reprezentacji niż te stosowane w bibliotece PyTorch Geometric. Przykładowo, dla grafu o maksymalnej liczbie zdarzeń wynoszącej 25 000, wartości indeksów wierzchołków mogą być reprezentowane jako liczby 15-bitowe, co może być tematem przyszłych badań.

To assess the size of the data required by the graph, we conducted an experiment on the entire dataset. We used the method outlined in \cite{b2} to create a graph for each sample. We set the maximum number of events per graph equal to 25 000, where the events with the highest density within a certain time window were selected. We set the neighbourhood radius relative to which vertices were connected by edges to 5, and to limit the number of edges generated, we set the neighbour limit to 32 per vertex. We tested two different approaches: in the first, we normalised the occurrence times of the events to values close to the resolution of the data (in the range of 0 to 100), and in the second we did not normalise, where the time values were in the range of microseconds. Normalising the time has a significant impact on the training results, but also affects the size of the data. If the time values are increased, the vertices are further apart, resulting in local linking of events based on a defined neighbourhood radius and a reduction in the number of generated edges. 
Figure \ref{fig} illustrates the differences between the two approaches.

% Figure environment removed

For the entire N-Caltech101 dataset, the average number of events is 24,457 (the lower value is due to samples with event counts not exceeding 25,000 events), resulting in 24,457 vertex \(p\) and spatial-temporal (\(x, y, t\)) values. The average number of edges for the unnormalised data is 756,744, while for the normalised data it is 381,563.

The PyTorch Geometric library represents vertex values as \textit{int32} and spatial-temporal positions as \textit{float32} vectors. Each edge is represented as a two-element vector with the vertex indices as \textit{int32} values. Finally, the edge attribute value consists of three \textit{float32} values. 
In total, each vertex requires 16 bytes, while each edge, including the attribute, requires 40 bytes. 
The average amount of memory used by the graph is about 15 MB for normalised data, compared to 30.5 MB for unnormalised data. 

If the edge attribute values in the graph are excluded, the data size for normalised graphs decreases to 3.5 MB, while for unnormalised data it is 6.5 MB. Thus, omitting the edge attribute values reduces the data size by an average of more than 4.5 times. In comparison, a 240 by 180 pixel raw image with a single channel using the \textit{uint8} data type takes up 0.04 MB, while a 3 channel image takes up 0.12 MB. This shows how much memory such data representations require and how important it is to reduce them. 

It is important to note that the above values are specified for low resolution and a limited number of events in the graph. For larger data sizes generated with, for example, a 1 megapixel camera \cite{b4}, the number of events in the graph, and thus the number of edges, will increase significantly. To remedy this, the size of the input data can be rescaled, as is often practised with classical convolutional networks (CNNs). 

In this experiment, we only focused on omitting edge attributes to reduce memory. However, it is also possible to reduce memory by using smaller data types than those used in the PyTorch Geometric library. For example, for a graph with a maximum number of events of 25,000, vertex index values can be represented as 15-bit numbers in FPGA, which may be a topic for future research.


\subsection{Convolution operations comparison}\label{classification}

% W celu zbadania wpływu pomijania cech krawędzi na wyniki uczenia, przeprowadziliśmy porównanie kilku operacji konwolucyjnych. %TODO raczej klasyfikacji i detekcji
% Jako punkt odniesienia wybraliśmy operację konwolucji SplineConv \cite{spline}, która jest użyta w najnowszych publikacjach w tej dziedzinie \cite{b2} \cite{b3}. %TODO ref i użyta chyba - ok
% Wykorzystuje ona zarówno cechy wierzchołków, jak i cechy krawędzi. 
% Jako modele, które nie uwzględniają cech krawędzi, wybraliśmy dwie standardowe operacje: GCNConv \cite{gcn} i SAGEConv \cite{sage}, które wymagają jedynie indeksów krawędzi i cech wierzchołków, oraz dwie operacje konwolucyjne, których głównym celem jest przetwarzanie danych w postaci chmury punktów -- EdgeConv \cite{edge} oraz PointNetConv \cite{pointnet}.

% Funkcja EdgeConv wykorzystuje tylko informacje o cechach wierzchołków i indeksach krawędzi, a operacja konwolucji jest zdefiniowana jako:

% \begin{equation}
% x_i' = \sum_{j\in N(i)} \phi (x_i || x_j - x_i) \label{edgeconv}
% \end{equation}

% Operator PointNetConv wykorzystuje cechy wierzchołków, indeksy krawędzi ale również pozycje wierzchołków w przestrzeni:

% \begin{equation}
% x_i' = \gamma(\underset{j \in N(i)}{max} \phi (x_j, p_j - p_i)) \label{pointconv}
% \end{equation}

% Warto zauważyć, że PointNetConv wykorzystuje pozycje wierzchołków \(p_i\) oraz \(p_j\) w swojej funkcji wiadomości (message) do określenia odległości między nimi, bez konieczności wcześniejszego obliczania wartości atrybutu krawędzi na podstawie metryki kartezjańskiej i zapisywaniu go. %TODO nie bardzo rozumiem.

% Biblioteka PyTorch Geometric umożliwia zdefiniowanie własnej funkcji \(\phi\) dla modelu EdgeConv oraz funkcji \(\phi\) i \(\gamma\) (opcjonalnej) dla modelu PointNetConv. W naszym przypadku zdecydowaliśmy się na zdefiniowanie pojedynczych warstw liniowych o minimalnym wymaganym rozmiarze. Funkcji \(\gamma\) dla modelu PointNetConv nie wykorzystaliśmy. %TODO funkcji ? - ok

% Porównaliśmy modele pod kątem trzech kluczowych aspektów: liczby parametrów uczących, osiągniętej dokładności i czasu wykonania. 
% Zastosowaliśmy dla każdego modelu identyczną architekturę, jak zaproponowano w \cite{b2}. 
% Składała się ona z siedmiu warstw konwolucyjnych o kolejnych wyjściowych kanałach (8, 16, 32, 32, 32, 128, 128), a po piątej warstwie konwolucji zastosowano warstwę MaxPooling o rozmiarze okna (16, 12). 
% Wyjście ekstraktora cech zostało spłaszczone do rozmiaru 2048, a klasyfikator składał się z pojedynczej warstwy liniowej o wyjściu o rozmiarze 100, odpowiadającym liczbie klas.

% W pierwszym kroku sprawdziliśmy liczbę parametrów uczących dla każdej warstwy konwolucyjnej oraz dla warstwy fully connected -- wyniki prezentujemy w tabeli \ref{parametry}. 
% Dla modelu SplineConv liczba parametrów potrzebnych do ekstraktora cech wynosi około 20.2 miliona, podczas gdy dla pozostałych modeli wartości te wynoszą około 41 tysięcy dla modeli GCNConv i SAGEConv, 43 tysiące dla modelu PointNetConv oraz 81 tysięcy dla modelu EdgeConv. 
% Można zauważyć, że redukcja liczby parametrów wynosi od 249 do 492 razy w porównaniu z modelem SplineConv.

% \begin{table}[htbp]
% \caption{The number of learning parameters for each layer in the model and for each convolution operation. For Spline convolution operations, the number of parameters reaches millions, while for other convolutions the number of parameters is in the tens of thousands.} %TODO ew. komentarz - ok
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% \hline
% Layer & Conv1 & Conv2 & Conv3 & Conv4 & Conv5 & Conv6 & Conv7 & MLP & Sum \\ \hline
% SplineConv & 8.2 K & 262 K & 525 K & 525 K & 2.1 M & 8.4 M & 8.4 M & 204 K & 20.4 M \\ \hline
% EdgeConv & 80 & 1.1 K & 2.1 K & 2.1 K & 8.6 K & 33.2 K & 33.2 K & 204 K & 285 K \\ \hline
% GCNConv & 64 & 608 & 1.1 K & 1.1 K & 4.5 K & 16.8 K & 16.8 K & 204 K & 245 K \\ \hline
% SAGEConv & 64 & 608 & 1.1 K & 1.1 K & 4.5 K & 16.8 K & 16.8 K & 204 K & 245 K \\ \hline
% PointNetConv & 112 & 704 & 1.2 K & 1.2 K & 4.9 K & 17.2 K & 17.2 K & 204 K & 247 K \\ \hline
% \end{tabular}%
% }
% \label{parametry}
% \end{table}

% Następnie dokonaliśmy pomiaru czasu wykonania operacji. 
% Wykorzystaliśmy kartę graficzną NVIDIA GeForce RTX 3060. 
% Czas zmierzyliśmy dla samej operacji przetwarzania sieci, a średnia wartość na całym zbiorze walidacyjnym przedstawiona jest w tabeli \ref{czasy}. 
% Najlepsze rezultaty osiągnął model SAGEConv, którego średni czas przetwarzania wynosił 6.819 ms, co przekłada się na 146 grafów na sekundę (GPS). 
% Z kolei najgorsze wyniki uzyskał model SplineConv z czasem 92.762 ms (tj. 10.78 GPS). %TODO 

% \begin{table}[htbp]
% \caption{Operation computation times and number of graphs per second for each model. The Spline convolution achieves the longest time, while the Edge and PointNet convolutions have values three times lower. The most efficient operation is the SAGE convolution.}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|c|c|}
% \hline
% Layer & Computation Time {[}ms{]} & Graphs per second \\ \hline
% SplineConv & 92.762 & 10.78 \\ \hline
% EdgeConv & 37.159 & 26.91 \\ \hline
% GCNConv & 14.869 & 67.25 \\ \hline
% SAGEConv & 6.819 & 146.65 \\ \hline
% PointNetConv & 33.421 & 29.92 \\ \hline
% \end{tabular}%
% }
% \label{czasy}
% \end{table}

% Ostatnim testem było sprawdzenie wyników uzyskanych przez poszczególne modele. %TODO Jak bym się zastanowił czy jednak tego nie odwrócić i zacząć od wyników a potem pozostałych atrybutów.
% Do uczenia wykorzystano optymalizator Adam z parametrem uczącym 1e-3 oraz wartością weight\_decay równą 
% 5e−3. 
% Liczba epok została ograniczona maksymalnie do 150, a liczba batchy wynosiła 8. 
% Wyniki uczenia dla każdej epoki przedstawiono na rysunku \ref{acc_fig}. %TODO ref !!! - ok
% Zaskakująco najlepsze rezultaty uzyskał model PointNetConv, osiągając maksymalną dokładność wynoszącą 52.3\%. 
% Model SplineConv poradził sobie nieco gorzej, uzyskując dokładność na poziomie 46\%. 
% Natomiast model EdgeConv, który nie wykorzystuje informacji o pozycjach wierzchołków, osiągnął najniższą dokładność spośród tej trójki, wynoszącą jedynie 33.41\%. 
% Proces uczenia dla modelu GCNConv i SAGEConv nie prowadził do zbieżnych wyników. %TODO niezbyt jasne, raczej porec uczenia nie prowadził do zbieżnych wyników ? - ok

% % Figure environment removed

% Na podstawie wszystkich wyników można stwierdzić, że najlepiej poradził sobie model PointNetConv, osiągając najlepsze wyniki w zadaniu klasyfikacji oraz zapewniając dobrą liczba parametrów uczących i krótki czas wykonania operacji. %TODO co to jest ilość -> liczba parametrów uczących ? - tak, podmieniłem
% Natomiast niskie wyniki modelu EdgeConv pokazują, że informacja o wzajemnym położeniu wierzchołków wydaje się kluczowa w rozważnym zagadnieniu. %TODO raczej wydaje się kluczowa w rozważnym zagadnieniu. - ok


In order to study the effect of skipping edge features on training results, we conducted a comparison of several convolution operations.
As a reference, we chose the SplineConv convolution operation \cite{spline}, which is used in recent publications  \cite{b2} \cite{b3}. It uses both vertex and edge features. 
We chose two standard operations as models that do not consider edge features: GCNConv \cite{gcn} and SAGEConv \cite{sage}, which only require edge indices and vertex features, and two convolutional operations whose main purpose is to process point cloud data -- EdgeConv \cite{edge} and PointNetConv \cite{pointnet}.

The EdgeConv function uses only information about vertex features and edge indices, and the convolution operation is defined as:

\begin{equation}
x_i' = \sum_{j\in N(i)} \phi (x_i || x_j - x_i) \label{edgeconv}
\end{equation}

The PointNetConv operator uses vertex features, edge indices but also vertex positions in space:

\begin{equation}
x_i' = \gamma(\underset{j \in N(i)}{max} \phi (x_j, p_j - p_i)) \label{pointconv}
\end{equation}

It is worth noting that PointNetConv uses the positions of vertices \(p_i\) and \(p_j\) in its message function, explained in Section \ref{conv}, to determine the distance between them, without having to first calculate the value of the edge attribute based on the Cartesian metric and storing it. 

The PyTorch Geometric library allows to define a custom function \(\phi\) for the EdgeConv model, and functions \(\phi\) and \(\gamma\) (optional) for the PointNetConv model. In our case, we decided to define single linear layers with the minimum required size. We did not use the \(\gamma\) function for the PointNetConv model.

We compared the models in terms of three key aspects: number of trainable parameters, execution time  and achieved accuracy. 
We used an identical architecture for each model, as proposed in \cite{b2}. 
It consisted of seven convolution layers with following output channels (8, 16, 32, 32, 32, 128, 128), and the fifth convolution layer was followed by a MaxPooling layer with a window size of (16, 12). The output of the feature extractor was flattened to 2048, and the classifier consisted of a single linear layer with an output size of 100, corresponding to the number of classes.

In the first step, we checked the number of parameters for each feature extraction layer and for the fully connected layer -- the results are presented in the table \ref{parameters}. 
For the SplineConv, the number of parameters needed for the feature extractor is about 20.2 million, while for the other models the values are about 41k for the GCNConv and SAGEConv models, 43k for the PointNetConv model, and 81k for the EdgeConv model. Since the number of outputs from the feature extractor is the same for all models, the fully connected layer has an identical number of parameters for every model.
This means that the reduction in the number of parameters ranges from 249 to 492 times compared to the SplineConv model.

\begin{table}[!t]
\caption{The number of learning parameters in the feature extraction layer and the fully connected layer of the models.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|}
\hline
Layer & Feature Extraction Layers & Fully Connected Layers & Sum \\ \hline
SplineConv & 20.2 M & 204 K & 20.4 M \\ \hline
EdgeConv & 81 K & 204 K & 285 K \\ \hline
GCNConv & 41 K & 204 K & 245 K \\ \hline
SAGEConv & 41 K & 204 K & 245 K \\ \hline
PointNetConv & 43 K & 204 K & 247 K \\ \hline
\end{tabular}%
}
\label{parameters}
\end{table}

% \begin{table}[!t]
% \caption{The number of learning parameters for each layer in the model and for each convolution operation. For Spline convolution operations, the number of parameters reaches millions, while for other convolutions the number of parameters is in the tens of thousands.}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% \hline
% Layer & Conv1 & Conv2 & Conv3 & Conv4 & Conv5 & Conv6 & Conv7 & MLP & Sum \\ \hline
% SplineConv & 8.2 K & 262 K & 525 K & 525 K & 2.1 M & 8.4 M & 8.4 M & 204 K & 20.4 M \\ \hline
% EdgeConv & 80 & 1.1 K & 2.1 K & 2.1 K & 8.6 K & 33.2 K & 33.2 K & 204 K & 285 K \\ \hline
% GCNConv & 64 & 608 & 1.1 K & 1.1 K & 4.5 K & 16.8 K & 16.8 K & 204 K & 245 K \\ \hline
% SAGEConv & 64 & 608 & 1.1 K & 1.1 K & 4.5 K & 16.8 K & 16.8 K & 204 K & 245 K \\ \hline
% PointNetConv & 112 & 704 & 1.2 K & 1.2 K & 4.9 K & 17.2 K & 17.2 K & 204 K & 247 K \\ \hline
% \end{tabular}%
% }
% \label{parameters}
% \end{table}

We then measured the execution time.  We used an NVIDIA GeForce RTX 3060 graphics card for the computation. 
We measured the time for the network processing operation alone, and the average value over the entire validation set is shown in the table \ref{times}. 
The best results were achieved by the SAGEConv model, with an average processing time of 6.819 ms, which is equivalent to 146 graphs per second (GPS). 
In contrast, the worst results were achieved by the SplineConv model with a time of 92.762 ms (i.e., 10.78 GPS). 

\begin{table}[!t]
\caption{Operation computation times and number of graphs per second for each model.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|}
\hline
Layer & Computation Time {[}ms{]} & Graphs per second \\ \hline
SplineConv & 92.762 & 10.78 \\ \hline
EdgeConv & 37.159 & 26.91 \\ \hline
GCNConv & 14.869 & 67.25 \\ \hline
SAGEConv & 6.819 & 146.65 \\ \hline
PointNetConv & 33.421 & 29.92 \\ \hline
\end{tabular}%
}
\label{times}
\end{table}

% \begin{table}[!t]
% \caption{Operation computation times and number of graphs per second for each model. The Spline convolution achieves the longest time, while the Edge and PointNet convolutions have values three times lower. The most efficient operation is the SAGE convolution.}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|c|c|}
% \hline
% Layer & Computation Time {[}ms{]} & Graphs per second \\ \hline
% SplineConv & 92.762 & 10.78 \\ \hline
% EdgeConv & 37.159 & 26.91 \\ \hline
% GCNConv & 14.869 & 67.25 \\ \hline
% SAGEConv & 6.819 & 146.65 \\ \hline
% PointNetConv & 33.421 & 29.92 \\ \hline
% \end{tabular}%
% }
% \label{times}
% \end{table}

The final test examined the results obtained by each model. 
For learning, the \textit{Adam} optimizer was used, with a \textit{learning\_rate} parameter of 1e-3 and a \textit{weight\_decay} value of 5e-3.  The number of epochs was limited to a maximum of 150, and the number of batches was 8. 
The learning results for each epoch are shown in Figure \ref{acc_fig}.
Surprisingly, the PointNetConv model had the best results, achieving a maximum accuracy of 52.3\%. 
The SplineConv model performed slightly worse, achieving an accuracy of 46\%. 
Meanwhile, the EdgeConv model, which does not use vertex position information, achieved the lowest accuracy of the three, at only 33.41\%. 
The learning process for the GCNConv and SAGEConv models did not lead to convergent results.

% Figure environment removed

Based on all the results, it can be concluded that the PointNetConv model performed best, achieving the best results in the classification task and providing a good number of trainable parameters and a low execution time. 
On the other hand, the low results of the EdgeConv model show that information about the mutual position of vertices seems to be crucial in this problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Detection model}\label{detection}

% W celu przeprowadzenia ostatecznej analizy przetestowaliśmy operację PointNet w zadaniu detekcji. Stworzyliśmy własny model do ekstrakcji cech, który różni się od modelu używanego w zadaniu klasyfikacji. 
% % W przypadku detekcji zdecydowaliśmy się na większą liczbę warstw poolujących z mniejszym oknem. %TODO dlaczego ???
% Architekturę tego modelu przedstawiono na rysunku \ref{det_model}.

% % Figure environment removed

% Model składa się z dwóch rodzajów bloków. 
% Pierwszy blok wejściowy zawiera dwie konwolucje, a na końcu jest stosowany MaxPooling. 
% Pozostałe trzy bloki składają się z trzech konwolucji, a ponieważ operacje splotu nie wpływają na strukturę grafu, zastosowano połączenie resztkowe między wyjściem pierwszej konwolucji, a wyjściem trzeciej konwolucji. 
% Na końcu każdego z tych bloków również stosowany jest MaxPooling o rozmiarze okna 4x4. 
% W ten sposób chcieliśmy odtworzyć tradycyjne modele CNN, które wykorzystują wiele warstw poolujących o rozmiarze okna 2x2, oraz poprzedzające je kilka warstw konwolucyjnych. %TODO coś o tym residual ?

% Podczas procesu uczenia ponownie użyliśmy optymalizatora Adam z parametrem uczenia równym 1e-3 i współczynnikiem weight\_decay równym 1e-4. Określiliśmy maksymalną liczbę epok na 1000, a rozmiar (batch size) ustaliliśmy na 16. Wyniki uczenia zostały przedstawione na wykresie \ref{mAP}.

% % Figure environment removed

% Podczas testów udało nam się osiągnąć dokładność mAP@0.5 na poziomie 53.7\%, w porównaniu gdy model z pracy \cite{b2} osiąga dokładność 59.5\%, a bardziej skomplikowane architektur przedstawione w pracy \cite{b3} osiągają dokładności w zakresie 62.9-73.2\%.
% Ten wynik został osiągnięty przy wykorzystaniu modelu, który ma mniej niż 100 tysięcy parametrów uczących w części ekstrakcji cech. %TODO powt. - można to to mAP odnieść do literatury / - ok
% Analiza wydajności czasowej przeprowadzona na całym zbiorze walidacyjnym pokazała, że ten model osiąga średni czas przetwarzania wynoszący 0.012186 s na wywołanie, co przekłada się średnio na 82 grafy na sekundę.

% Przeprowadzone testy umożliwiły nam stwierdzenie, że istnieje możliwość usprawnienia obecnych podejść pod względem zużycia pamięci bez znaczącej straty w osiąganych wynikach i zadowalających czasach przetwarzania.
% Wykorzystanie konwolucji PointNet przyniosło nie tylko redukcję liczby parametrów uczących, ale także zmniejszenie zapotrzebowania na pamięć ze względu na brak konieczności przechowywania informacji o wartości krawędzi. %TODO Brak knieczności przechowywanie inf o kraWedziech - ok
% Dodatkowe korzyści można również zaobserwować podczas generacji grafu, gdzie dla każdego nowego zdarzenia wystarcza operacja wyznaczania sąsiadów, co przekłada się również na dodatkowe obniżenie obciążenia obliczeniowego. Co więcej, wyniki wydajnościowe pokazują, że grafowe sieci konwolucyjne można z powodzeniem wdrążyć w aplikacjach czasu rzeczywistego.


For the final analysis, we tested the PointNet operation in a detection task. We created our own model for feature extraction, which differs from the model used in the classification task. 
The architecture of this model is shown in Figure \ref{det_model}.

% Figure environment removed


The model consists of two types of blocks. 
The first input block contains two convolutions, and MaxPooling with 4x4 window is used at the end. 
The other three blocks consist of three convolutions, and since the convolution operations do not affect the graph structure, a residual connection is used between the output of the first convolution and the output of the third convolution. 
MaxPooling is also used at the end of the first two blocks. In each subsequent block, the pooling window size is twice as large as the previous one.
In this way, we wanted to replicate traditional CNN models, which use many pooling layers with a 2x2 window size, and several convolution layers preceding them.

During the training process, we again used the \textit{Adam} optimizer with a \textit{learning\_rate} parameter of 1e-3 and a \textit{weight\_decay} factor of 1e-4. We set the maximum number of epochs to 1000, and set the batch size to 16. The learning results are shown in the Figure \ref{mAP}.

% Figure environment removed

During the tests, we were able to achieve an accuracy of mAP@0.5 of 53.7 \%, as compared to the model in the \cite{b2} paper that achieves an accuracy of 59.5 \%, and the more complex architectures presented in the \cite{b3} paper achieve accuracies in the range of 62.9-73.2 \%.
This result was achieved using a model that has less than 100k trainable parameters in the feature extraction part. 
A time performance analysis performed on the entire validation set showed that this model achieves an average processing time of 12.186 milliseconds, which translates to an average of 82 graphs per second.

The tests performed allowed us to conclude that there is potential to improve current approaches in terms of memory consumption without significant loss in performance and satisfactory processing times.
The use of PointNet convolution brought not only a reduction in the number of model parameters, but also a reduction in memory requirements due to the fact that edge value information does not need to be stored. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary} \label{summary}

% W ramach naszej pracy przedstawiliśmy nową perspektywę przetwarzania danych za pomocą grafowych sieci konwolucyjnych. W eksperymentach przeprowadziliśmy analizę różnych operatorów konwolucji na grafie, uwzględniając zapotrzebowanie na pamięć i liczbę parametrów uczących, przy jednoczesnym osiąganiu wysokich wyników i efektywnych operacji. Nasze badania wykazały, że model PointNet umożliwia redukcję liczby parametrów uczących o 450 razy oraz zmniejszenie zużycia pamięci do reprezentacji danych o 4.5 razy w porównaniu do najnowszych prac. Osiągnęliśmy również relatywnie dobre wyniki klasyfikacji na poziomie 52.3\% dokładności oraz detekcji 53.7\% mAP@0.5 podczas treningu sieci, które znacząco nie odbiegają od istniejących osiągnięć.

% Aktualne badania wskazują na duży potencjał grafowych sieci konwolucyjnych w przetwarzaniu danych zdarzeniowych. Jednak nasze wyniki pokazują, że koncentracja jedynie na kosztach obliczeniowych może prowadzić do znacznego wzrostu zużycia pamięci. Nasza praca dąży do udoskonalenia istniejących rozwiązań oraz zmiany podejścia w przyszłych badaniach, uwzględniając istotny czynnik jakim jest wykorzystanie pamięci.

% W dalszych badaniach planujemy kontynuować redukcję zużycia pamięci poprzez modyfikację opisu grafów oraz zastosowanie bardziej efektywnej reprezentacji danych. Badania będą również obejmować analizę innych operatorów konwolucji oraz próbę opracowania własnego operatora, dostosowanego specjalnie do danych zdarzeniowych. Dodatkowo, zamierzamy porównać korzyści uzyskane dla znacznie większych zbiorów danych. W naszych planach jest także implementacja grafowych sieci konwolucyjnych na platformach sprzętowych, takich jak SoC FPGA lub Jetson Nano, w celu praktycznego zastosowania modelu w rzeczywistych problemach.

In our work, we presented a new perspective on event data processing using graph convolutional networks. In the experiments, we conducted an analysis of different convolution operators for graphs, taking into account the memory requirements and the number of trainable parameters of the models, while maintaining high performance and efficient operations. Our research has shown that the PointNet model allows a reduction in the number of trainable parameters by 450 times and a reduction in memory consumption for data representation by 4.5 times compared to state-of-the-art work. We also achieved relatively good classification results of 52.3\% accuracy and 53.7\% detection mAP@0.5 during network training, which are not significantly different from existing achievements.

Current research shows the great potential of graph convolutional networks in event data processing. However, our results show that focusing only on computational costs can lead to a significant increase in memory consumption. Thus, our work seeks to improve existing solutions and change the approach in future research, taking into account the important factor of memory usage.

In further research, we plan to continue to reduce memory consumption by modifying the graph description and using a more efficient data representation. The research will also include an analysis of other convolution operators and an attempt to develop our own operator, tailored specifically for event data. In addition, we intend to compare the benefits obtained for much larger data sets. Our plans also include the implementation of graph convolutional networks on hardware platforms, such as SoC FPGAs or Jetson Nano, in order to practically apply the model to real-world tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{00}
\bibitem{PAMI} G. Gallego et al., “Event-Based Vision: A Survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 1, pp. 154–180, 2022, doi: 10.1109/TPAMI.2020.3008413.

\bibitem{reconstruction_e2vid} H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, “High Speed and High Dynamic Range Video with an Event Camera,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 6, pp. 1964–1980, 2021, doi: 10.1109/TPAMI.2019.2963386.

\bibitem{filters} G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. V. Thakor, and R. B. Benosman, “HFirst: A Temporal Approach to Object Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, pp. 2028–2040, 2015.

\bibitem{snn} M. Gehrig, S. Shrestha, D. Mouritzen, and D. Scaramuzza, “Event-Based Angular Velocity Regression with Spiking Networks,” 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 4195–4202, 2020.

\bibitem{b1} Y. Li et al., “Graph-based Asynchronous Event Processing for Rapid Object Recognition,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 914–923. doi: 10.1109/ICCV48922.2021.00097.

\bibitem{b2} S. Schaefer, D. Gehrig, and D. Scaramuzza, “AEGNN: Asynchronous Event-based Graph Neural Networks,” in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 12361–12371. doi: 10.1109/CVPR52688.2022.01205.

\bibitem{b3} D. Gehrig and D. Scaramuzza, “Pushing the Limits of Asynchronous Graph-based Object Detection with Event Cameras,” ArXiv, vol. abs/2211.12324, 2022.

\bibitem{gcns} Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive survey on graph neural networks,” IEEE transactions on neural networks and learning systems, vol. 32, no. 1, pp. 4–24, 2020.

\bibitem{voxel_grid} M. Simonovsky and N. Komodakis, “Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs,” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 29–38, 2017.

\bibitem{cluster} I. S. Dhillon, Y. Guan, and B. Kulis, “Weighted Graph Cuts without Eigenvectors A Multilevel Approach,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 11, pp. 1944–1957, 2007, doi: 10.1109/TPAMI.2007.1115.

\bibitem{standard1} Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-Based Spatio-Temporal Feature Learning for Neuromorphic Vision Sensing,” IEEE Transactions on Image Processing, vol. 29, pp. 9084–9098, 2020, doi: 10.1109/TIP.2020.3023597.

\bibitem{standard2} Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-Based Object Classification for Neuromorphic Vision Sensing,” 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 491–501, 2019.

\bibitem{b4} E. Perot, P. de Tournemire, D. O. Nitti, J. Masci, and A. Sironi, “Learning to Detect Objects with a 1 Megapixel Event Camera,” ArXiv, vol. abs/2009.13436, 2020.

\bibitem{b5} G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, “Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades,” Frontiers in Neuroscience, vol. 9, 2015, doi: 10.3389/fnins.2015.00437.

\bibitem{b6} M. Fey and J. E. Lenssen, “Fast Graph Representation Learning with PyTorch Geometric,” ArXiv, vol. abs/1903.02428, 2019.

\bibitem{spline} M. Fey, J. E. Lenssen, F. Weichert, and H. Müller, “SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels,” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 869–877, 2017.

\bibitem{gcn} T. Kipf and M. Welling, “Semi-Supervised Classification with Graph Convolutional Networks,” ArXiv, vol. abs/1609.02907, 2016.

\bibitem{sage} W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive Representation Learning on Large Graphs,” NIPS, 2017.

\bibitem{edge} Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, “Dynamic Graph CNN for Learning on Point Clouds,” ACM Transactions on Graphics (TOG), vol. 38, pp. 1–12, 2018.

\bibitem{pointnet} C. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 77–85, 2016.

\end{thebibliography}
\end{document}

% \begin{thebibliography}{00}

% \bibitem{PAMI} G. Gallego et al., “Event-Based Vision: A Survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 1, pp. 154–180, 2022, doi: 10.1109/TPAMI.2020.3008413.

% \bibitem{reconstruction_e2vid} H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, “High Speed and High Dynamic Range Video with an Event Camera,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 6, pp. 1964–1980, 2021, doi: 10.1109/TPAMI.2019.2963386.

% \bibitem{standard1} Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-Based Spatio-Temporal Feature Learning for Neuromorphic Vision Sensing,” IEEE Transactions on Image Processing, vol. 29, pp. 9084–9098, 2020, doi: 10.1109/TIP.2020.3023597.

% \bibitem{standard2} Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, “Graph-Based Object Classification for Neuromorphic Vision Sensing,” 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 491–501, 2019.

% \bibitem{b1} Y. Li et al., “Graph-based Asynchronous Event Processing for Rapid Object Recognition,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 914–923. doi: 10.1109/ICCV48922.2021.00097.

% \bibitem{b2} S. Schaefer, D. Gehrig, and D. Scaramuzza, “AEGNN: Asynchronous Event-based Graph Neural Networks,” in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 12361–12371. doi: 10.1109/CVPR52688.2022.01205.

% \bibitem{b3} D. Gehrig and D. Scaramuzza, “Pushing the Limits of Asynchronous Graph-based Object Detection with Event Cameras,” ArXiv, vol. abs/2211.12324, 2022.

% \bibitem{b4} E. Perot, P. de Tournemire, D. O. Nitti, J. Masci, and A. Sironi, “Learning to Detect Objects with a 1 Megapixel Event Camera,” ArXiv, vol. abs/2009.13436, 2020.

% \bibitem{b5} G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, “Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades,” Frontiers in Neuroscience, vol. 9, 2015, doi: 10.3389/fnins.2015.00437.

% \bibitem{b6} M. Fey and J. E. Lenssen, “Fast Graph Representation Learning with PyTorch Geometric,” ArXiv, vol. abs/1903.02428, 2019.

% \bibitem{spline} M. Fey, J. E. Lenssen, F. Weichert, and H. Müller, “SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels,” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 869–877, 2017.

% \bibitem{gcn} T. Kipf and M. Welling, “Semi-Supervised Classification with Graph Convolutional Networks,” ArXiv, vol. abs/1609.02907, 2016.

% \bibitem{sage} W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive Representation Learning on Large Graphs,” NIPS, 2017.

% \bibitem{edge} Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, “Dynamic Graph CNN for Learning on Point Clouds,” ACM Transactions on Graphics (TOG), vol. 38, pp. 1–12, 2018.

% \bibitem{pointnet} C. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 77–85, 2016.


% \bibitem{voxel_grid} M. Simonovsky and N. Komodakis, “Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs,” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 29–38, 2017.

% \bibitem{cluster} I. S. Dhillon, Y. Guan, and B. Kulis, “Weighted Graph Cuts without Eigenvectors A Multilevel Approach,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 11, pp. 1944–1957, 2007, doi: 10.1109/TPAMI.2007.1115.

% \bibitem{filters} G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. V. Thakor, and R. B. Benosman, “HFirst: A Temporal Approach to Object Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, pp. 2028–2040, 2015.

% \bibitem{snn} M. Gehrig, S. Shrestha, D. Mouritzen, and D. Scaramuzza, “Event-Based Angular Velocity Regression with Spiking Networks,” 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 4195–4202, 2020.

% \bibitem{gcns} Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive survey on graph neural networks,” IEEE transactions on neural networks and learning systems, vol. 32, no. 1, pp. 4–24, 2020.

% \end{thebibliography}
% \end{document}


% \begin{thebibliography}{00}

% \bibitem{PAMI} G. Gallego et al., "Event-Based Vision: A Survey," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 1, pp. 154-180, 1 Jan. 2022, doi: 10.1109/TPAMI.2020.3008413.

% \bibitem{reconstruction_e2vid} Rebecq, H., Ranftl, R., Koltun, V., \& Scaramuzza, D. (2019). High Speed and High Dynamic Range Video with an Event Camera. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43, 1964-1980.

% \bibitem{standard1} Bi, Y., Chadha, A., Abbas, A., Bourtsoulatze, E., \& Andreopoulos, Y. (2020). Graph-Based Spatio-Temporal Feature Learning for Neuromorphic Vision Sensing. IEEE Transactions on Image Processing, 29, 9084-9098.

% \bibitem{standard2} Bi, Y., Chadha, A., Abbas, A., Bourtsoulatze, E., \& Andreopoulos, Y. (2019). Graph-Based Object Classification for Neuromorphic Vision Sensing. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 491-501.

% \bibitem{b1} Y. Li, et al., "Graph-based Asynchronous Event Processing for Rapid Object Recognition," in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), Montreal, QC, Canada, 2021 pp. 914-923.
% doi: 10.1109/ICCV48922.2021.00097

% \bibitem{b2} Schaefer, S.T., Gehrig, D., \& Scaramuzza, D. (2022). AEGNN: Asynchronous Event-based Graph Neural Networks. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12361-12371.

% \bibitem{b3} Gehrig, D., \& Scaramuzza, D. (2022). Pushing the Limits of Asynchronous Graph-based Object Detection with Event Cameras. ArXiv, abs/2211.12324.

% \bibitem{b4} Perot, E., Tournemire, P.D., Nitti, D.O., Masci, J., \& Sironi, A. (2020). Learning to Detect Objects with a 1 Megapixel Event Camera. ArXiv, abs/2009.13436.

% \bibitem{b5} Orchard, G.; Cohen, G.; Jayawant, A.; and Thakor, N.  “Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades", Frontiers in Neuroscience, vol.9, no.437, Oct. 2015

% \bibitem{b6} Fey, M., \& Lenssen, J.E. (2019). Fast Graph Representation Learning with PyTorch Geometric. ArXiv, abs/1903.02428.

% \bibitem{spline} Fey, M., Lenssen, J.E., Weichert, F., \& Müller, H. (2017). SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 869-877.

% \bibitem{gcn} Kipf, T., \& Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks. ArXiv, abs/1609.02907.

% \bibitem{sage} Hamilton, W.L., Ying, Z., \& Leskovec, J. (2017). Inductive Representation Learning on Large Graphs. NIPS.

% \bibitem{edge} Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., \& Solomon, J.M. (2018). Dynamic Graph CNN for Learning on Point Clouds. ACM Transactions on Graphics (TOG), 38, 1 - 12.

% \bibitem{pointnet} Qi, C., Su, H., Mo, K., \& Guibas, L.J. (2016). PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 77-85.

% \end{thebibliography}

% \end{document}
