\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

% ready for submission
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2023}

%this line below need to go
\usepackage[preprint]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{pst-all}
\usepackage{multirow}
\usepackage{comment}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{xr}
\usepackage{bm}
\usepackage{color, colortbl}
\usepackage{thmtools,thm-restate}

% save space
\usepackage[subtle]{savetrees}


\definecolor{yellow}{rgb}{0.91, 0.84, 0.42}
\definecolor{blue}{rgb}{0.6, 0.73, 0.89}
\definecolor{red}{rgb}{0.8, 0.25, 0.33}
\definecolor{green}{rgb}{0.7, 0.75, 0.71}
\definecolor{orange}{rgb}{1.0, 0.6, 0.4}
\definecolor{cream}{rgb}{1.0, 0.99, 0.82}
\definecolor{Gray}{gray}{0.9}

%% Self-defined macros
\newcommand{\cut}[1]{}
\newcommand{\swap}[3][-]{#3#1#2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\supp}{supp}

\title{Label Noise: Correcting a Correction}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  William Toner\\
  University of Edinburgh\\
  \texttt{w.j.toner@sms.ed.ac.uk} \\
  % examples of more authors
   \And
   Amos Storkey \\
   %Affiliation \\
   University of Edinburgh\\
   \texttt{a.storkey@ed.ac.uk} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\Experiments
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
\end{abstract}
\vspace{-2mm}

\section{Introduction}
Over the last decade, we have seen an enormous improvement in the efficacy of machine learning methods for classification. Correspondingly, there has been an increased need for large labelled datasets to train these models. However, obtaining cleanly labelled datasets at the scale and quantity needed for industrial machine learning can be prohibitively expensive. For this reason, practitioners commonly rely on approaches which yield large datasets but contain high-label noise. Examples include web querying or crowd-sourcing systems. Even standard dataset collection methods are susceptible to noise introduced by fallible human labellers. This is especially true when data are hard to label or labelling requires a specialist background (e.g.\ medical imaging). Such issues have led to immense interest in designing machine learning methods which can learn within the regime of noisy labels. 

%An example of label noise in this setting would be images of golfer Tiger Woods being miscategorised as `Tiger'\todo{cite}.

Most approaches for addressing the label noise problem consist of a mechanism for either removing or compensating for it. Unfortunately, many of these methods are elaborate or require pipelines involving multiple networks and stages \citep{dividemix, coteaching, mentornet, decoupling, meta_dynamic, meta_gradient}. This complexity damages their applicability in settings with user limitations on time, technical expertise or computational resources. 

A simpler style of approach designs methods to be inherently resilient in the face of corrupted labels. The most prominent family of such methods are robust loss functions. Here the goal is to choose an objective function which allows training in the presence of noise without harming the generality of the learned classifier. An advantage of these methods is that they are simple; a robust loss can be easily implemented with minimal computational overhead. Regularisation and consistency-based approaches modify a loss with model-dependent terms or data cross-terms to restrict the network and thus avoid overfitting \citep{mixup, elr, gjs, bootstrap}. Other methods alter the cross-entropy objective to be less inclined to fit to noise \citep{sce, GCE_Loss, normalised_losses}. Losses of this type are usually empirically rather than theoretically motivated, meaning the reasons for their robustness are rarely fully understood. 
%As such, using these losses can result in poor performance if settings deviate from those used in their evaluation.
 
A more principled class of robust losses is \emph{loss correction methods}. Here one uses the data to infer the noising distribution so that its impact may be subtracted from the training objective. The problem with such methods is that they rely on having detailed knowledge of the noising process, thereby damaging their applicability \citep{fprop, fprop_old, noiseAdapt, larsen, mnihHinton}. Regardless, such losses are still susceptible to overfitting to datasets with label noise.

%The greedy nature of empirical risk minimisation means 
In this paper, we tackle the challenge of overfitting in popular robust losses by introducing a principled solution: bounding the allowable loss during training by recognising that the presence of label noise means the generalised noisy risk is lower bounded. The critical contribution of this paper is explicitly deriving these bounds and showing that their implementation improves robustness. In addition, we provide a deeper understanding of existing robust loss functions, unifying correction losses with several other popular heuristic losses into a single family.

%We give strong theoretical motivation for this solution, showing how
% In this paper we (i) unify popular robust losses into a generalised family of correction-based methods, (ii) show how, despite some benefits, all these methods suffer from overfitting and (iii) introduce a simple and theoretically justified family of methods which overcomes this. We also (iv) instantiate several versions within this family called budget losses and (v) empirically verify their effectiveness, showing consistent improvements over non-budgeted counterparts.

%\subsection{Paper Summary and Outline}
\noindent\fbox{%
    \parbox{0.98\textwidth}{%
     \textbf{Key Idea:} When a distribution contains label noise, this implies there is a minimum achievable risk. Current methods do not respect this bound, targeting a zero training loss instead. It is this which causes overfitting. By bounding the loss below during training, one may prevent this. Crucially, these bounds are explicitly derivable.
    }%
}

The paper is structured as follows. Following a summary of the literature and definition of terminology (Sections~\ref{sec:related_work},\ref{sec:probform}), we define in Section~\ref{sec:robustloss} a class of generalised correction losses, unifying a number of robust losses into one family, noting they still have the propensity to overfit. We then observe that the presence of label noise implies a lower bound on the achievable risk. We leverage this by introducing a \emph{budget-corrected loss}, which lower bounds the loss during training. Later, in Section~\ref{sec:riskbounds}, we present a formula for generating these budgets. Finally, in Section~\ref{sec:experiments}, we experimentally test these budgets showing that they improve robustness across datasets and noise types. \vspace{-0.2mm}

\section{Related Work}\label{sec:related_work}
\vspace{-0.2mm}

%We categorise the existing literature into two broad classes: i) corruption identification methods and ii) robust loss-based methods.

\textbf{Corruption identification methods:}  Methods in this class first identify
different types of corrupted samples and then re-weight, refine or remove these from the dataset. Many such methods rely on the heuristic that noisy samples have higher losses, especially earlier in training. This is based upon the well-known observation that complex models generally learn to classify easier data points before over-fitting on noise \citep{memorisation}. \citet{sefie} use the entropy of the historical prediction distribution to identify refurbishable samples. \citet{bmm} deploy a beta mixture model in the loss space and use the posterior probabilities that a sample is corrupted in the parameters of a bootstrapping loss. \citet{curiculum} define a loss which ignores samples that incur a higher loss value. Other approaches include a two-network model \citep{dividemix} in which a Gaussian mixture model selects clean samples based on their loss values. These samples are then taken and used to train the other network. A number of other two-network models work on similar lines. Co-teaching \citep{coteaching} trains two networks, one on the outputs of the other with the lowest loss values. Decoupling \citep{decoupling} has the two networks update on the basis of disagreement with each other. Mentor-Net \citep{mentornet} harnesses a teacher network for training a student network by re-weighting probably correct samples.

%\cite{metaLabelCorrect} is a meta-learning approach in which a label correction network corrects labels and feeds them to a classifier to train. This is done so that the performance of the classifier is optimised on a held-out validation set. Other meta-learning approaches include \cite{meta_gradient} in which samples are re-weighted so that the learned classifier generalises better to a held-out clean meta-set. Similarly, in \cite{metaLabels} (soft), labels are treated as learnable parameters and learned to maximise the performance on the meta-set. \\ 

%Other corruption identification methods expect that noisily labelled data lie heavily out of class distribution under an appropriate metric. In FINE \citep{eigenfine}, noisy samples are detected and removed using an eigendecomposition in the latent space. Alternatively, a KNN \cite{knn} in the latent space can identify and select samples based on their coherence to their neighbours' classes. Related to this, cross-data loss terms can encourage consistency between different views of a data sample. \citet{gjs} achieve data consistency by applying a generalised Jensen-Shannon divergence to different augmentations of a datapoint. In Mix-Up \citep{mixup}, consistency comes from training on data-label pairs obtained by convex combinations of those from the training set.

 Other corruption identification methods may use the latent space to identify out-of-distribution data using eigendecomposition \citep{eigenfine} or KNN \citep{knn}. Alternatives achieve consistency between different views of a data sample by minimising a Jensen-Shannon divergence between different augmentations \citep{gjs} or convex combinations of data-label pairs \citep{mixup}.


\textbf{Correction-Based Loss Methods:} Methods in this class achieve robustness by altering the loss function. We further subdivide these into loss correction-based and robust loss methods. The former corrects the loss to compensate for the noising procedure \citep{larsen}. This procedure involves using noisy \citep{fprop} or clean data \citep{trustedData} to infer the noise transition matrix. A downside of these methods is the difficulty in estimating a noising matrix for a large number of classes. Moreover, they typically assume that this matrix has minimal dependence on the datapoints, which impacts generality \citep{noiseAdapt, fprop_old}.

\textbf{Robust Loss Methods:} An alternative set of methods consists of looking for innately robust loss functions. One of the advantages here is the simplicity of these methods, as they do not require multiple networks or complex noise detection pipelines. This makes them suitable for plug-and-play use in any setting. These methods are based on the observation that cross-entropy results in overfitting in the presence of label noise \citep{janocha2017loss}. \citet{sce} propose a solution to this by adding a `reverse cross-entropy' (RCE) term to the usual cross-entropy (CE) term. \citet{janocha2017loss} observe that $L^p$-losses typically used for regression show good robustness in a classification setting. This is particularly true for the MAE loss (Mean Absolute Error), which exhibits good robustness albeit with a tendency to under-fit and train slowly \citep{normalised_losses}. Following this observation, Generalised Cross-Entropy \citep{GCE_Loss} construct a family of losses which interpolate between CE and $L^1$ in order to get the best of both. \citet{robust_theory} offer some theoretical insights, suggesting robustness may be obtained by choosing losses which are bounded or satisfy a `symmetry' property. On the back of this \citet{normalised_losses} show that one can take unbounded loss functions and re-normalise them to achieve this objective. Other methods soften of mix labels to avoid overfitting \citep{bootstrap, labelSmoothing, soft_labels}, or use regularisers \citep{elr,tanaka}. \citet{flooding}, in-line with our work, bound the loss to prevent overfitting. However, their method is only briefly discussed in relation to label noise and provides no mechanism for selecting a budget. We ground this work firmly in the context of label noise and provide theoretical results for producing bounds on the loss.

\section{Problem Formulation}\label{sec:probform}
\subsection{Classification via Empirical Risk Minimisation -- Formalisation and Notations}
Given a dataset of data-label pairs, the goal of discriminative classification is to learn a mapping from data to label space which predicts the labels of unseen datapoints with high accuracy. One way in which this classifier can be learned is through a process known as Empirical Risk Mimimisation (ERM) on a surrogate loss function. Here one defines a (parametric) family of models and selects the parameter which minimises a quantity called the `empirical risk' defined using the dataset. %For purposes of clarity let us formalise this process explicitly. 
%The definitions and notation introduced in this section will be used extensively in the subsequent sections.

Let $\mathcal{X}\subset \mathbb{R}^d$ be some data domain and let $\mathcal{Y}=\{1,2,3,\ldots c\}$ be some label domain. Suppose there exists some latent distribution $p(x,y)$ over data-label space and we have a dataset $\{(x_i, y_i)\}_{i=1}^N$ of samples drawn independent and identically distributed (iid) from it.
\begin{definition}[Probability Estimator]
    Let $\Delta^{c-1}$ denote the \textbf{probability simplex} of $c-$dimensional non-negative vectors which sum to one. A \textbf{probability estimator} $\vec{q}: \mathcal{X}\rightarrow \Delta^{c-1}$ is a model which takes a point in dataspace and outputs a distribution over labels. We will denote this as $\vec{q}(x) := (q(y=1 \vert x),  q(y=2\vert x), \ldots, q(y=c \vert x))$. 
\end{definition}
%definition os classifier has been removed at the moment
\begin{definition}[Elementwise/Batchwise Losses]\label{def:elementwise}
    An \textbf{elementwise} loss function is a function which takes a predicted distribution over labels and evaluates it against an observed label. That is $\mathcal{L}:\Delta^{c-1}\times \mathcal{Y} \rightarrow \mathbb{R}$. A \textbf{batchwise} loss evaluates a batch of predictions against a corresponding batch of labels $\mathcal{L}:(\Delta^{c-1}\times \mathcal{Y})^N \rightarrow \mathbb{R}$. 
\end{definition}


\begin{definition}[Proper]\label{def:proper}
Let $\mathcal{L}$ be an elementwise loss. We define $\mathcal{L}$ as \textbf{proper} if, for any given $\vec{p}$, the expected loss $L_{\mathcal{L}}(\Vec{q}, \vec{p}):= \sum_{i=1}^c p_i\mathcal{L}(\vec{q}, y=i)$ is minimised by setting $\vec{q}=\vec{p}$. If $\vec{p}$ is the unique minimising point, we refer to $\mathcal{L}$ as \textbf{strictly proper}.
\end{definition}
 

%Two important examples of elementwise losses are the 0-1 loss and cross-entropy losses respectively. These are defined  $\mathcal{L}_{0-1}(\vec{q},y=k)=1$ if $k\neq \argmax_i q_i$ and is zero otherwise. $\mathcal{L}_{CE}(\vec{q}, k) := -log(q_k)$

\begin{definition}[Pointwise Risk] \label{def:risk}
  Given some parametric probability estimator $\vec{q}_{\theta}(x)$, a distribution $p(x,y)$ and an elementwise loss $\mathcal{L}$, we define the \textbf{Pointwise $\mathcal{L}-$Risk} of $q_{\theta}$ at $x$ to be the expected loss of $\vec{q}_{\theta}(x)$ under the label distribution at $p(y\vert x)$. Formally;
\begin{align*}
    R_{\mathcal{L}}(q_{\theta})(x) := \mathbb{E}_{y\sim p(y\vert x)}[\mathcal{L}(q_{\theta}(x), y)] := \sum_{i=1}^c p(y=i \vert x)\mathcal{L}(q_{\theta}(y=i \vert x), i)
\end{align*}
\end{definition}
We then define the \textbf{Generalised $\mathcal{L}$-Risk} of $q_{\theta}$ with respect to $p(x,y)$ as the expectation of the pointwise risk with respect to $p(x)$, $R_{\mathcal{L}}(q_{\theta}) := \mathbb{E}_{x\sim p(x)}[R_{\mathcal{L}}(q_{\theta})(x)]$. The \textbf{Empirical $\mathcal{L}$-Risk} is defined by approximating this expression on a dataset of samples drawn from $p$. We distinguish this from the former using a hat $\hat{R}_{\mathcal{L}}(q_{\theta}) := \frac{1}{N}\sum_{i=1}^N \mathcal{L}(\vec{q}_{\theta}(x_i), y_i)$  



Our aim, with a given parametric family of probability estimators, is to select $\theta^*$ that minimises the expected misclassification rate on a randomly sampled test set. Typically, \textit{surrogate losses} are minimised during training, in part to overcome the challenge of optimising $0-1$ loss \footnote{The $0-1$ loss is 1 if a sample is misclassified and 0 otherwise}. By far, the most commonly used surrogate is the cross-entropy loss (CE). \vspace{-0.2mm}


\subsection{Label Noise}
The standard classification approaches are practical on datasets with clean labels. Unfortunately, datasets are frequently contaminated with labelling errors (also referred to as label noise) in real-world scenarios, which can significantly degrade performance. Formally,

\begin{definition}[Label Noise] \label{def:label_noise}
    Label noise refers to any process that randomly modifies the labels of samples drawn from a distribution over data-label space, denoted as $p(x,y)$. In particular, we consider label noise that can be modelled using a noising distribution $p(\tilde{y} | y, x)$, which specifies the transition probabilities between the clean label $y$ and the noisy label $\tilde{y}$ for each data point $x$. The resulting noisy distribution is denoted as $\tilde{p}(x,\tilde{y}) = \sum_{y}p(\tilde{y} | y, x)p(x, y)$.
    
    For each data point $x$, the noising distribution can be represented by a transition matrix $T(x)$, where $T(x)_{ij} := p(\tilde{y}=j | y=i, x)$. When the noising distribution is independent of $x$, we refer to the label noise as \textbf{uniform}. We say that the label noise is \textbf{symmetric} at $x$ if $T(x)_{ii} = 1-\eta(x)$ and $T_{ij}=\frac{\eta(x)}{c-1}, i\neq j$, where $\eta(x)$ is the noise rate at $x$ and $c$ is the number of distinct labels. More generally, the noise rate at $x$ is defined as the probability that a label is corrupted at that point: $\eta(x) := p(\tilde{y}\neq y \vert x) = 1- \sum_{i=1}^c T_{ii}(x)p(y=i \vert x)$.

    We use the term \textbf{noisy risk} when our risks (Defn ~\ref{def:risk}) are evaluated with respect to the noisy distribution $\tilde{p}(x,\tilde{y})$. We denote these as $R^\eta, \hat{R}^{\eta}$ respectively. This distinguishes them from the \textbf{clean risks} evaluated with respect to the un-noised distribution $p(x,y)$.
    %We also define a class of noise called \textbf{conservative}, when $\forall x\in \supp(p(x)), \argmax_{i\in \{1,2,\ldots ,c \}} \tilde{p}(\tilde{y}=i \vert x) = \argmax_{i\in \{1,2,\ldots ,c \}} p(y=i \vert x)$. Put simply, conservative noise \textit{conserves} whichever class has the highest probability. This definition is used implicitly in each of the lemmas in the \citet{robust_theory} and we believe it is instructive to explicitly define this as a class of noise. All noise we consider is assumed to satisfy this condition. A discussion of this assumption and its generality may be found in the appendices.
\end{definition}
%    Let $p(x,y)$ be a distribution over data-label space. \textbf{Label noise} is any procedure which randomly alters the labels of samples drawn from $p$. We limit our attention to label noise which may be expressed via a noising distribution $r(\Tilde{y}\vert y,x)$ which defines the transition probabilities between the clean ($y$) and noisy ($\tilde{y}$) labels for each $x$. Specifically we have $\Tilde{p}(x,\Tilde{y}) = \sum_{y}r(\Tilde{y}\vert y,x)p(x, y)$. We call $p$ and $\Tilde{p}$ the clean and noisy distributions respectively. For each $x$, $r(\Tilde{y}\vert y,x)$ may be identified with a transition matrix $T(x)$ where $T(x)_{ij}:= p(\tilde{y}=j\vert y=i, x)$. When the noising distribution has no dependence on $x$, ($T(x)=T$) we say the label noise is \textbf{uniform}.  We say that the label noise is \textbf{symmetric} at $x$ when we can express $T(x)$ in terms of the noise rate so that $T(x)_{ii} = 1-\eta(x)$ and $T_{ij}=\frac{\eta(x)}{c-1}, i\neq j$.\\

%For each $x$ we define the \textbf{noise rate} $\eta(x)$ to be the probability that a label drawn from $p(y\vert x)$ is altered by $T$. The \textbf{mean noise rate} is defined as the average of $\eta(x)$ with respect to $p(x)$. i.e. the average proportion of corrupted labels for a dataset drawn from the noised distribution. 

\section{Robust Losses}\label{sec:robustloss}
%What do we want to say? Label noise causes overfitting. This isn't a problem normally as the empirical risk should be near zero anyway. We can avoid this by correcting this loss although this has similar problems. We can trade-off fitting power but this can lead to poorer fits and may still underfit. We think we should try to avoid overfitting by using flooding. We introduce the loss and make the point that in the noisy case this should be bounded below.

%When one trains with a cross-entropy loss, the network fits the corrupted labels as well as the uncorrupted labels and this damages generalisation.

%The detrimental impact of label noise arises from its tendency to induce overfitting. A common way to mitigate this issue is by modifying the standard cross-entropy loss used in training, for example by switching cross-entropy for losses which fit less aggressively. As a result, our network may ignore the corrupted labels, fitting the remainder. Another approach is to correct the loss by incorporating the noise model. In this section, we show how these two types of losses may be partially unified by generalising correction-based losses to allow non-linear noise models. 
In this section, we show how two types of losses may be partially unified by generalising correction-based losses to allow non-linear noise models. We call this family \emph{$f$-proper losses}. This unification will aid in our subsequent analysis where we: (a) remark that despite enjoying certain theoretical guarantees, these losses are still prone to overfitting, (b) argue that the presence of label noise implies a lower bound on the achievable risk, and (c) hypothesise that utilising this bound may mitigate overfitting. This leads us to propose bounding the empirical risk during training. We call this a \emph{budget-corrected loss}.

\textbf{Robust losses} are a popular approach to tackling label noise by selecting losses less prone to fit the entire training set. In essence, one trades off fitting power to gain improved robustness. The canonical example of such a loss is a mean absolute error (MAE) $(\mathcal{L}_{MAE}(\vec{q}, y=k) = 1-q_k)$.  MAE will typically ignore the harder-to-fit samples; on noisy datasets, this often corresponds to those with corrupted labels. The downside is that MAE dramatically underfits on datasets with many classes. Alternative losses mitigate this underfitting by interpolating between CE and MAE to avoid both of their pitfalls. Two well-known examples are the Generalised Cross-Entropy (GCE) and Symmetric Cross-Entropy (SCE) defined $\mathcal{L}_{GCE}(\vec{q}, y=k) := \frac{1-q_k^a}{a} \label{eqn:gce}$ and $\mathcal{L}_{SCE}(\vec{q}, y=k) = -log(q_k) +A(1-q_k)$ respectively. By varying the parameters $a,A$, we can alter the losses' behaviour from being more like CE to MAE. 

\textbf{Correction-based losses} are an alternative motivated by the observation that, under label noise, the empirical risk is no longer a suitable proxy for the generalised clean risk. However, by altering the loss through incorporating the noise model, one may ensure $R^{\eta}_{\mathcal{L}_{\textit{Corr.}}}(q) = R_{\mathcal{L}}(q)$. The most effective method is the \textit{forward-corrected} loss \citep{fprop}. Given a base loss $\mathcal{L}$, the forward-corrected loss is defined $\mathcal{L}^{\mapsto}(T^{-1}\vec{q}(x), \tilde{y}=k) := \mathcal{L}(\vec{q}(x), \tilde{y}=k)$ where $T$ is a stochastic matrix approximating $p(\Tilde{y}\vert y)$. 

\textbf{Unification.} For forward-corrected losses, the loss and corrected loss are related by a linear transform $T$. We now show that generalising this, to allow $T$ to be a non-linear transformation, we construct a family which unifies the forward-corrected loss with losses such as SCE and GCE. We call the resulting class of losses \textit{$f$-proper}. This name reflects our requirement that the base loss be proper.

%This definition is broad, containing within it all losses whose expected loss function (\ref{def:expected_loss}) is bounded. This generality proves crucial in deriving general results in section 5. 

\begin{restatable}{definition}{definitionFourPointOne}[$f-$proper Losses]\label{def:f-proper}
Let $\mathcal{L}$ be an elementwise loss and $f:\Delta^{c-1}\rightarrow \Delta^{c-1}$ be an bijective function. We define $\mathcal{L}$ as (strictly) \textbf{$f$-proper} if there exists a (strictly) proper loss $\mathcal{L}_{\text{proper}}$ (as defined in \ref{def:proper}) such that for all $\vec{q}\in \Delta^{c-1}$, $\mathcal{L}(f(\vec{q}), i) = \mathcal{L}_{\text{proper}}(\vec{q}, i)$. We refer to $\mathcal{L}_{\text{proper}}$ as the \textbf{base loss}.
\end{restatable}

This definition describes that a loss is a proper loss under a choice of an appropriate transformation. E.g., in the context of label noise, this transformation can be a noise model. We can therefore interpret \emph{$f-$proper losses} as a generalised class of correction losses where we permit non-linear noise models. This definition is broad, trivially including all proper losses such as CE when $f=id$. We now demonstrate that, in addition, the GCE, SCE and forward-corrected CE (FCE) losses mentioned above are all $f$-proper, deriving expressions for the transformations $f$. For the plots of the respective functions and a discussion of Definition~\ref{def:f-proper}, we refer to Appendix~\ref{sec:AddTheory}.
\begin{restatable}{lemma}{lemmaFourPointTwo}\label{lemma:semiproper_gce}
    The GCE, SCE and FCE losses are all strictly $f-$proper where $f_{GCE}(\vec{p})_i = \frac{p_i^{\frac{1}{1-a}}}{\sum_{i=1}^c p_i^{\frac{1}{1-a}}}$, $f_{SCE}(\Vec{p})_i = \frac{p_i}{\lambda-Ap_i}$ and $f_{FCE}(\vec{p}) = T^{-1}\vec{p}$. Here $T$ is the invertible stochastic matrix used to define the correction, and $\lambda$ is a constant selected to ensure the correct normalisation.
\end{restatable}

Lemma \ref{lemma:semiproper_gce} demonstrates that GCE and SCE are non-linear correction-based losses; the noise model is represented by the function $f^{-1}$. We stress that these are by no means the only robust losses which adhere to definition \ref{def:f-proper}. However, these losses permit us to compute $f$ explicitly. For this reason, they provide useful examples when empirically demonstrating the results of Section~\ref{sec:riskbounds}.

\subsection{The Problem of Overfitting}
%In Definition~\ref{def:f-proper}, we defined a family of generalised correction losses. We showed that, among others, this family includes forward-correction losses as well as the GCE and SCE losses. This unification 
The unification established by Definition~\ref{def:f-proper} provides two main benefits. It improves our understanding of losses like GCE since we show that they encode an implicit noise model. More importantly, this grants a common framework for analysing the failure modes of these losses. In this section, our analysis demonstrates that correcting for the noise model alone is insufficient. To achieve robustness, we must additionally correct our loss by imposing a lower bound on the training loss to account for the randomness introduced by label noise. 

The original motivation for noise-corrected losses recognises that the empirical risk no longer adequately approximates the generalised clean risk in the presence of label noise. Therefore, by modifying the loss as in Defn.~\ref{def:f-proper} by defining $\mathcal{L}(f(\vec{q}),i):= \mathcal{L}_{pr.}(\vec{q}, i)$, one can ensure that $R^{\eta}_{\mathcal{L}}(q) = R_{\mathcal{L}_{pr.}}(q)$. This holds when $f$ adequately models the true noising process. Despite this correction, the empirical risk is minimised by precisely fitting the noisy labels. Consequently, training with a highly expressive neural network still incurs overfitting, similar to that observed with the uncorrected losses. %i'm aware this is still very sub-par 
 
We want to train our model to fit the clean labels without overfitting the noisy ones. Our key insight is realising that we can do this in a principled way. When a label distribution contains noise, there is a lower bound on the optimal noisy risk a model can achieve. An analogy to this is that no forecaster can predict the outcome of a biased coin flip 100\% of the time. We can't expect any model to be better than this over a large number of flips. Simply put, even if an optimal model $q^*(x)$ exists, which minimises the noisy risk, we \emph{still} expect it to incur a non-zero loss on a randomly sampled noisy training set.

We propose, therefore, that the principled way to handle label noise is to limit the minimum allowable risk on the training set. Specifically, we define a budget $B$ and train so that our loss does not go below this value. Explicitly we define this as follows:

\begin{definition}[Budget Loss]\label{def:budget_loss}
    Let $\mathcal{L}$ be an elementwise loss. Let $\mathcal{D}$ be a batch of $N$ data-label pairs $(x_i, y_i)$. Define the \textbf{$B-$corrected} (batchwise) loss $\mathcal{L}_{B}$ as follows:
\begin{align}
    \boxed{\mathcal{L}_{B}(\vec{q}(x), \mathcal{D}) := \vert  B - \frac{1}{N}\sum_{i=1}^N\mathcal{L}(\Vec{q}(x_i), y_i)\vert} \label{eqn:budget}
\end{align}
\end{definition}

Previously, \citet{flooding} have explored the idea of targeting a specific non-zero loss value. We want to remark our contribution goes beyond that, providing a theoretically justified toolset for selecting the budget for a broad family of loss functions. We achieve this by constructing bounds for the minimum achievable noisy risk of $f-$ proper losses.

\section{Risk Bounds}\label{sec:riskbounds}
In the last section, we remarked that despite theoretical motivation, $f-$proper losses are still prone to overfitting. We proposed training against a budget to prevent this, noting that the optimal generalised noisy risk is non-zero. In this section, we make this more concrete, giving bounds for the generalised noisy $\mathcal{L}$-risk of the optimal probability estimator for $f-$proper losses. We conclude by presenting a formula for choosing a budget $B$ to train against in Defn.~\ref{def:budget_loss}. We call this the \textit{noise-corrected budget}. Throughout this section, we adhere to the notation and definitions introduced in Section~\ref{sec:probform}.
%In practice this gives us an effective way of avoiding overfitting since such bounds hold with high probability on the training set. We propose using this bound to train against a budget rather than simply minimising the empirical risk.

The goal of this section is to provide bounds which are practically useful methods for deciding on a budget for our budget-corrected loss, as in Defn~\ref{def:budget_loss}. Correspondingly we have two main desiderata: (a) Bounds should be easily computable; depending only on the loss, the aggregate noise rate $\eta$ and the number of classes $c$. (b) Bounds should be insensitive to the specific structure of the noising distribution.

We begin by presenting our bound in its most general form. Following this, we make a simplifying assumption, which allows us to derive more useable bounds. The proof is given in Appendix~\ref{sec:proofs}. 
\begin{restatable}{lemma}{LemmaFivePointOne}\label{lemma:general}.
    Let $\mathcal{L}$ be an elementwise, strictly $f-$proper loss. For any probability estimator $\vec{q}(x)$, we may derive a bound for the noisy pointwise risk that is tight if and only if $\vec{q}(x)=\Tilde{p}(\tilde{y}\vert x)$. Specifically, for all $x\in supp(p(x))$ we have; 
    \begin{align}
        R^{\eta}_{\mathcal{L}}(\vec{q})(x) \geq R^{\eta}_{\mathcal{L}}(f(\Tilde{p}(\tilde{y}\vert x)))
    \end{align}
\end{restatable}

The key assumption which we employ to simplify Lemma~\ref{lemma:general} is that the clean label distribution is (approximately) deterministic. We define this as follows.
\begin{definition}[Deterministic]\label{def:determinism}
    We say that $p(y\vert x)$ is \textbf{deterministic} when, for each $x\in supp(p(x))$, there exists $k(x)\in \mathcal{Y}$ such that $p(y=k(x)\vert x) = 1$. 
\end{definition}

Definition~\ref{def:determinism} describes a scenario where the label is considered a deterministic function of the input. i.e. there is no randomness in the label distribution. While an idealised assumption, it is a reasonable approximation for many real-world image classification tasks where clear images of a single subject dominate the dataset. In domains with high inherent randomness such as medical diagnostics, this assumption is less suitable. In the following we leverage this assumption to construct a number of bounds. Later we discuss the validity of this assumption and the impact on the derived bounds.

\subsection{Entropy Bounds}
The following famous characterisation of proper losses will be indispensable in statements and proofs of the following results \cite{savagen}.
\begin{theorem}[Savage]\label{prop:savage}
    Let $\mathcal{L}$ be some elementwise loss. $\mathcal{L}$ is proper if and only if there exists a concave function $J:\Delta^{c-1}\rightarrow\mathbb{R}$ such that $\mathcal{L}(\vec{q}, i) := J(\vec{q}) + (\vec{e}_i-\vec{q})\cdot\nabla J(\vec{q})$. We call $J$ the \textbf{entropy function} of $\mathcal{L}$ where $J(\vec{q}) = \sum_{i=1}^cq_i \mathcal{L}(\vec{q}, i)$. Examples: When $J(\vec{q}) := -\sum_{i=1}^c q_i log(q_i)$ (Shannon Entropy) one recovers the cross-entropy loss. When $J(\vec{q}):= \vert\vert \vec{q}\vert\vert_2^2$ one recovers the L2 loss. 
\end{theorem}

%Recall our definition of a proper loss \ref{def:proper}. Proper loss functions may be constructed from a concave function $J$ defined on the probability $c$-simplex, using the expression $\mathcal{L}(\vec{q}, i) := J(\vec{q}) + (\vec{e}_i-\vec{q})\cdot\nabla J(\vec{q})$. For instance, when $J(\vec{q}) := \sum_{i=1}^c -q_i log(q_i)$, we obtain the cross-entropy loss. Conversely, for any proper loss function $\mathcal{L}$, there exists a unique concave function $J$ satisfying this relation. One may easily show this function is precisely the $\mathcal{L}$-entropy, defined in \ref{def:entropy} as $\sum_i q_i \mathcal{L}(\vec{q}, i) = J(\vec{q})$.

\begin{definition}\label{def:symmetric_loss}
   We call a loss \textbf{symmetric} if it is invariant under a permutation of the labels. In practice this means the loss has no inherent bias toward any particular class. We call a proper loss symmetric if the entropy $J$ is a symmetric function of its variables eg $J(a, b) = J(b, a)$ for all $a,b$. 
\end{definition}

\begin{restatable}{lemma}{lemmaFivePointFive}\label{prop:det_proper}
      Let $p(x,y)$ be a distribution where $p(y\vert x)$ is deterministic, and let $\Tilde{p}(x,\Tilde{y})$ be a noisy distribution obtained by applying label noise to $p(x,y)$. Assume that $\mathcal{L}$ is a symmetric $f$-proper loss and let $J$ denote the entropy function of its base loss (\ref{def:f-proper}). For any probability estimator $q$, we can derive a bound for the noisy risk that is tight if and only if the label noise is symmetric and $q(x) = \tilde{p}(\tilde{y}\vert x)$. Specifically, for each $x$, we have the following lower bound, where $\eta(x)$ denotes the noise rate at $x$ (\ref{def:label_noise}):
  \begin{align*}
     R^{\eta}_{\mathcal{L}}(q) \geq \mathbb{E}_{x\sim p(x)}[J\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)]
 \end{align*}
  \end{restatable}

\begin{restatable}{corollary}{corrFivePointSix}\label{cor:uniform_symmetric}
    When the label noise is uniform and symmetric with rate $\eta$, the following bound on the risk of any probability estimator is tight and achieved only when $q(x) = \tilde{p}(\tilde{y} \mid x)$.
      \begin{align}\label{cor:symmetric_eqn}
     R^{\eta}_{\mathcal{L}}(q) \geq J\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big)
 \end{align}
\end{restatable}

\begin{restatable}{corollary}{corrFivePointSeven}
    \label{cor:fprop}
    Let $\mathcal{L}$ be a symmetric $f$-proper loss and let $\Vec{u}(\eta, c) := (1-\eta, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$. We have the following bound on the noisy risk which is tight if the label noise is symmetric and uniform.
\begin{align*}
           R^{\eta}_{\mathcal{L}}(q) \geq  (1-\eta)\mathcal{L}(f(\vec{u}(\eta,c)), 1) + \eta \mathcal{L}(f(\vec{u}(\eta,c)), i\neq 1)
    \end{align*}
\end{restatable}

Proofs of Corollary \ref{cor:uniform_symmetric}, Corollary \ref{cor:fprop} and Lemma \ref{prop:det_proper} are given in Appendix~\ref{sec:proofs}. Corollary \ref{cor:uniform_symmetric} gives a bound on the noisy risk in terms only of $c$ and $\eta$. Specifically, we bound the risk below by the mean entropy of the noisy label distribution. As remarked, this bound is only tight if the noise is symmetric and uniform. When noise deviates from these idealised assumptions, our bound becomes less optimal. Ideally, we'd like this deviation to be small so our bound is insensitive to the precise structure of the noising distribution. This would make it suitable for use as a budget. The level of insensitivity depends on the entropy function itself. Shannon-Entropy is relatively sensitive to the noising structure. Conversely, losses like GCE and SCE induce insensitive entropy functions. This topic is discussed further in Appendix~\ref{sec:AddTheory}.

Using these results, we can calculate the budget for our budget-corrected loss (Defn.~\ref{def:budget_loss}).
\begin{definition}[Noise-Corrected Budget]
Let $\mathcal{L}$ be a symmetric $f-$proper loss whose base loss has entropy function $J$. Let $\Vec{u}(\eta, c) := (1-\eta, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$. We define the \textbf{noise-corrected budget} as:
\begin{align}\label{budgets}
     \boxed{B(\eta, c) := J(\vec{u}(\eta, c)) = (1-\eta)\mathcal{L}(f(\vec{u}(\eta,c)), 1) + \eta \mathcal{L}(f(\vec{u}(\eta,c)), i\neq 1)}
\end{align}
\end{definition}

This leads us to the main proposal of this paper. When our dataset has label noise we propose using the budget loss (Eqn.~\ref{eqn:budget}) with $B$ set to $B(\eta,c)$, the noise-corrected budget from Eqn.~\ref{budgets}. We call this the \textbf{noise-corrected budget loss}. For CE and FCE the noise-corrected budget corresponds to the Shannon Entropy of the distribution $(1-\eta, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$. For SCE and GCE we substitute expressions for $f$ derived in Lemma~\ref{lemma:semiproper_gce} into Eqn.~\ref{budgets} to generate budgets. These are given explicitly in Appendix~\ref{sec:explicitBudgets}.
%We recall that for $f$-proper losses these bounds are only tight if the noise is symmetric. Nevertheless, our goal is to define easily usable bounds, so we employ these bounds in all situations.

\section{Experiments}\label{sec:experiments}

\subsection{Losses}
In this section, we empirically investigate the effectiveness of the noise-corrected budget loss (Eqn.~\ref{eqn:budget} with Eqn.~\ref{budgets}) for improving robustness to label noise. We consider several loss functions: CE, SCE, F-corrected CE (FCE), and GCE. Additionally, we explore a variant of CE that includes a prior on the model probabilities (CEP). Our experiments all follow a similar structure. We use a dataset containing intrinsic or synthetic label noise in the training set. We train models using each loss on this noisy training set and evaluate their performance on a clean test set. We compare the results obtained without budgeting and with noise-corrected budgets. The latter are denoted by a '+B' after the loss name (e.g., CE+B). We also explore treating the budget as a hyperparameter to assess its proximity to optimality, indicated by an asterisk (e.g., CE+B*). 

Our results are compared against other standard robust losses, including mean squared error (MSE), mean absolute error (MAE), NCE-MAE, ELR, Curriculum loss (CL), Bootstrapping loss (Boot.), Spherical loss (Sph.), Mix-up, and a version of GCE that incorporates the additional tricks outlined by \citet{GCE_Loss}. To differentiate this version of GCE from our simplified GCE, we refer to it as `Truncated loss' (Trunc.) due to its use of truncation.

\subsection{Datasets}
We evaluate each loss on various datasets with different label noise types. We consider versions of EMNIST, FashionMNIST, Cifar10, Cifar100 corrupted by symmetric label noise at rates of 0.2 and 0.4 and MNIST with rates of 0.4 and 0.6. Additionally, we explore more sophisticated noise types. In the case of `Asym-Cifar100,' we introduce asymmetric noise by randomly transitioning labels within the 20 superclasses of CIFAR100. For example, within the superclass `fish' (comprised of aquarium-fish, flatfish, ray, shark, trout), 
we change training labels to other members of the set with a probability of $\eta \in {0.2, 0.4}$ (e.g., flatfish $\rightarrow$ trout). For `Non-Uniform EMNIST,' we investigate the impact of using non-uniform noise. We train a linear classifier on EMNIST and, with a probability of 0.6, modify the label of a data point in our training set to match the output of this classifier. Since the performance of the classifier varies across data-space, this creates noise with an $x$-dependence. Further experiments on the TinyImageNet and Animals-10N datasets, which contain real, intrinsic open-set noise, are given in Appendix~\ref{sec:FurtherExp}. 

For the Animals and TinyImageNet experiments, we use a ResNet-34 to parameterise our model. For the other datasets, we use a ResNet-18. For each experiment, the number of epochs is kept consistent across losses. The budgets we employ in each experiment are obtained by substituting the relevant number of classes $c$ and the noise rate $\eta$ into \ref{budgets}. An exception is the case of Non-uniform EMNIST, where we use a class number of $c=2$ to reflect that label is a mixture of the clean label and classifier labels. The results where we vary the budget are obtained by doing a small grid search on either side of our noise-corrected budget value. Additional precise experimental details may be found in Appendix~\ref{sec:FurtherExp}  


\subsection{Results}
The results of our experiments are presented in two tables. Table \ref{table:MNIST} includes the simpler datasets of MNIST, FashionMNIST, EMNIST, and CIFAR10, while table \ref{table:CIFAR} displays CIFAR100, Asym-CIFAR100, and Non-uniform-EMNIST. The results for TinyImageNet and Animals are presented in a third table in Appendix~\ref{sec:FurtherExp}. Each table follows a similar structure, with losses listed in rows and datasets in columns. The baselines are grouped together at the top. Our main losses are organised into triplets, such as CE, CE+B, and CE+B*. The rows that use the noise-corrected budget (e.g., GCE+B) are highlighted in grey to enhance readability. If using our noise-corrected budget leads to higher mean accuracy compared to training without the budget, this is indicated by a box. The best overall model for each dataset is highlighted by colouring the given cell light yellow.

With few exceptions, our budget leads to improved performance compared to the standard version of each loss. For the Asym-Cifar100 and Non-Uniform-EMNIST datasets, our CE+B loss performs worse than regular CE. This outcome was expected since our derived budgets are only optimal for symmetric noise and may be suboptimal for non-symmetric noise. This discrepancy is especially pronounced for losses based on Shannon-Entropy like CE (Appendix~\ref{sec:sens}). In contrast, the other $f$-proper losses, as we had anticipated, exhibit greater resilience to the precise noise structure and consistently outperform the baseline across different types of noise.

Our optimal budget is attained by doing a grid search in a small vicinity of the noise-corrected budget. When this doesn't yield an improvement, the starred and unstarred accuracy values are the same. In slightly over half of our experiments, we find that we may achieve an improvement by perturbing the budget. This improvement is generally minor. Our assumption that the underlying clean dataset is deterministic (\ref{def:determinism}) means one should be able to improve performance by raising the budget to account for the additional randomness in the label distributions. Generally, we find this to be so. An exception to this are the non-uniform and asymmetric datasets. In these cases, one typically benefits from marginally lowering the budget. These observations are consistent with our expectations, as the bounds are tight only for symmetric noise and will otherwise be overly strict. The values of the optimal budgets may be found in a table in Appendix~\ref{sec:FurtherExp}. 

Figure \ref{fig:entropy_graphs} presents a visualisation of how test accuracy changes with the budget used during training on noisy CIFAR10 (left) and EMNIST (right) datasets. For CIFAR10 we train using an SCE loss. For the EMNIST dataset, we use a CE loss. The training set of each dataset has been corrupted with symmetric noise at a rate of $\eta=0.4$. We employ a budget with an offset from the noise-corrected budget: $B(\epsilon) := B(\eta=0.4, c=10)+\epsilon$, and plot the clean test performance against the value of $\epsilon$. Thus $\epsilon=0$ corresponds to the budget given in \ref{budgets}. The graphs in Figure \ref{fig:entropy_graphs} exhibit similar patterns. As $\epsilon$ increases, the test performance improves due to the budget preventing overfitting. A peak is reached at around $\epsilon=0.35, 0.15$ respectively, followed by a decline in performance as the model starts to underfit the noisy data. The presence of a prominent bump and its proximity to the prescribed value provide empirical evidence supporting our theoretical framework. Notably, the optimal budget seems to lie at $\epsilon > 0$, which we attribute to the intrinsic entropy present in the dataset. This deviation originates from our simplifying assumption, which modelled the underlying distributions as deterministic \ref{def:determinism}.  \vspace{-5mm}

\section{Conclusion, Limitations and Further Work}
\vspace{-1mm}
In this work, we have looked at mitigating the impact of label noise by training subject to a budget. We motivated this by noting that label noise implies a minimum achievable risk. We then concretely constructed these lower bounds for various losses. Later we empirically showed that the derived budgets indeed substantially improved the performance. 

While our method proves successful in the settings we looked at, its applicability has a few limitations. The derived bounds are reliable if the dataset is well approximated as deterministic. For datasets with high inherent randomness, such as in the medical domain, the proposed budgets might not suffice. An avenue for future work is extending our results to a more noisy data environment. Another area for improvement is that our method still relies on approximating the aggregate noise rate. However, it is still a substantial improvement on existing approaches which require modelling the entire noise structure but limits applicability to settings where this number is approximately known
%We have taken a very frequentist view in motivating our choice of budget. An interesting perspective on this work would be to take a more Bayesian viewpoint. In particular, the FEC loss admits an interpretation as MLE with respect to a corrected noise model. This raises the question of whether one might conceptualise budgeting as a way to target a different and more suitable candidate from the posterior.
\pagebreak
% Figure environment removed
\begin{table*}[h!]
    \centering
    \begin{adjustbox}{max width=1.00\textwidth}
    \begin{tabular}{l|cc|cc|cc|cc|cc}
    \textbf{} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{FashionMNIST} & \multicolumn{4}{c|}{EMNIST} & \multicolumn{2}{c}{CIFAR10} \vspace{-2mm}\\
\multirow{2}{*}{Losses} & \multicolumn{1}{c}{\multirow{2}{*}{0.4}} & \multicolumn{1}{c|}{\multirow{2}{*}{0.6}} & \multicolumn{1}{c}{\multirow{2}{*}{0.2}} & \multicolumn{1}{c|}{\multirow{2}{*}{0.4}} & \multicolumn{2}{c}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{1}{c}{\multirow{2}{*}{0.2}} & \multicolumn{1}{c}{\multirow{2}{*}{0.4}} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c|}{Top 5} & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c|}{Top 5} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{}  \\ \hline
MSE  & $93.3_{\pm 0.47 }$& $85.8_{\pm 0.95 }$& $84.8_{\pm 0.22} $& $80.6_{\pm 0.84 }$& $82.9_{\pm 0.29}$ & $98.1_{\pm 0.04}$ & $80.2_{\pm 0.19}$ & $97.1_{\pm 0.07}$ & $78.7_{\pm 1.51}$ &   $56.4_{\pm 0.11}$ \\
MAE & $97.9_{\pm 0.08} $& $96.4_{\pm 0.08} $& $83.2_{\pm 0.10} $& $82.2_{\pm 0.37} $& $49.8_{\pm 2.83}$ & $52.2_{\pm 0.10}$ & $50.4_{\pm 1.14}$ & $51.4_{\pm 0.96}$ & $88.6_{\pm 1.34}$ &   $78.9_{\pm 5.95}$\\
NCE & $97.8_{\pm 0.06} $ & $96.0_{\pm 0.25} $ & $87.7_{\pm 0.26} $ & $86.3_{\pm 0.14} $ & $84.5_{\pm 0.25}$ & $97.9_{\pm 0.05}$ & $82.6_{\pm 0.81}$ & $96.7_{\pm 0.03}$ & \cellcolor{cream} $\textcolor{black}{\bm{89.3}_{\pm 0.40}}$ &   \cellcolor{cream}$\textcolor{black}{\bm{86.0}_{\pm 0.81}}$ \\
MixUp & $95.8_{\pm 1.24} $& $86.8_{\pm 0.85} $& $86.9_{\pm 0.10} $& $82.3_{\pm 0.54} $& $84.3_{\pm 0.08}$ & $98.1_{\pm 0.04}$ & $81.6_{\pm 0.48}$ & $97.1_{\pm 0.08}$ & $86.0_{\pm 0.46}$ &  $77.9_{\pm 0.49}$ \\
Sph. & $95.0_{\pm 0.41}$ & $88.1_{\pm 0.82}$ & $87.2_{\pm 0.04}$ & $84.1_{\pm 0.75}$ & $84.6_{\pm 0.12}$ & $98.3_{\pm 0.05}$ & $83.2_{\pm 0.29}$ & $98.1_{\pm 0.58}$ & $86.6_{\pm 0.01}$  & $72.1_{\pm 0.80}$\\
Boot. & $86.6_{\pm 0.56} $& $71.2_{\pm 1.17} $& $82.0_{\pm 0.61}$ & $73.4_{\pm 1.06} $ & $80.5_{\pm 0.24}$ & $96.7_{\pm 0.06}$ & $77.3_{\pm 0.98}$ & $95.0_{\pm 0.25}$ & $77.0_{\pm 1.57}$ &  $58.2_{\pm 2.99}$\\
Trunc. & $97.1_{\pm 0.12} $& $94.2_{\pm 0.39} $& $87.8_{\pm 0.29} $& $85.3_{\pm 0.77} $& $84.1_{\pm 0.53}$ & $97.4_{\pm 1.03}$ & $83.1_{\pm 0.55}$ & $97.2_{\pm 1.00}$ & $88.3_{\pm 0.56}$ &   $84.2_{\pm 0.69}$ \\ 
CL  & $82.7_{\pm 0.57}$ & $67.5_{\pm 1.83} $& $81.2_{\pm 0.34}$ & $73.1_{\pm 0.66} $ & $79.6_{\pm 0.17}$ & $96.4_{\pm 0.05}$ & $75.1_{\pm 0.67}$ & $94.2_{\pm 0.24}$ & $76.0_{\pm 2.16}$ &   $59.4_{\pm 4.20}$\\ 
ELR & $98.1_{\pm 0.04}$ & $97.8_{\pm 0.07} $& $85.3_{\pm 0.23}$ & $83.4_{\pm 0.02} $ & $81.8_{\pm 0.26}$ & $97.5_{\pm 0.21}$ & $76.6_{\pm 0.10}$ & $96.5_{\pm 0.11}$ & $88.1_{\pm 0.82}$ &   $85.7_{\pm 0.06}$\\ \hline
FCE. & $95.4_{\pm 0.25} $ & $92.3_{\pm 0.13} $ & $83.6_{\pm 0.11} $ & $79.9_{\pm 0.78} $ & $83.1_{\pm 0.12}$ & $98.4_{\pm 0.20}$ & $80.6_{\pm 0.12}$ & $98.0_{\pm 0.03}$ & $84.7_{\pm 0.40}$ &  $75.1_{\pm 0.04}$ \\
\rowcolor{green}
FCE+B & $\boxed{95.7_{\pm{0.18}}}$ & $\boxed{92.7_{\pm{0.74}}}$ & $\boxed{84.8_{\pm{0.26}}}$ & $\boxed{81.7_{\pm{0.27}}}$  & $\boxed{83.4_{\pm{0.09}}}$ & $\boxed{98.5_{\pm{0.03}}}$ & $\boxed{81.6_{\pm{0.51}}}$ & $\boxed{98.1_{\pm{0.15}}}$ & $\boxed{86.7_{\pm_{0.21}}}$ & $\boxed{82.2_{\pm_{0.06}}}$\\ 
FCE+B* & $96.7_{\pm 0.17}$ & $94.3_{\pm 0.50}$ & $84.8_{\pm{0.26}}$ & $83.3_{\pm 0.22}$ & $84.4_{\pm 0.06}$ & \cellcolor{cream}$\bm{98.6}_{\pm{0.13}}$ & $83.1_{\pm 0.42}$ & $98.1_{\pm{0.10}}$ & $87.2_{\pm{0.20}}$ & $82.2_{\pm_{0.06}}$\\\hline
GCE  & $94.4_{\pm 0.36}$ & $83.8_{\pm 1.14} $ &  $86.4_{\pm 0.24}$ & $81.6_{\pm 0.37}$ & $84.3_{\pm 0.13}$ & $98.4_{\pm 0.08}$ & $82.7_{\pm 0.07}$ &   $97.9_{\pm 0.02}$ & $81.1_{\pm 0.72}$ & $60.0_{\pm 1.31} $ \\
\rowcolor{green}
GCE+B & $\boxed{96.6_{\pm 0.22}}$ & $\boxed{94.0_{\pm 0.13}} $ & $\boxed{86.5_{\pm 0.56}}$ & $\boxed{85.5_{\pm 0.13}}$ & $84.1_{\pm 0.29}$ & $\boxed{98.4_{\pm 0.04}}$ & $\boxed{82.8_{\pm 0.28}}$ &  $\boxed{98.0_{\pm 0.06}}$ & $\boxed{86.1_{\pm 0.22}}$ & $\boxed{79.0_{\pm 1.17}}$ \\
GCE+B* & $96.6_{\pm 0.22}$ & $94.0_{\pm 0.13} $ & $87.0_{\pm 0.04}$ & $85.5_{\pm 0.13}$ & $84.3_{\pm 0.09}$ & $98.4_{\pm 0.06}$ & $83.6_{\pm 0.25}$ &   \cellcolor{cream} $\bm{98.2}_{\pm 0.03}$ & $86.7_{\pm 0.07}$ & $80.2_{\pm 0.83}$ \\ \hline
SCE & $89.5_{\pm 5.29} $& $70.2_{\pm 0.69} $& $82.7_{\pm 0.64} $& $74.4_{\pm 0.37} $& $82.1_{\pm 0.33}$ & $96.8_{\pm 0.10}$ & $79.6_{\pm 0.61}$ & $95.4_{\pm 0.15}$ & $78.2_{\pm 0.42}$ &   $59.0_{\pm 4.43}$\\
\rowcolor{green}
SCE+B & $\boxed{97.0_{\pm 0.16}}$ & $\boxed{93.4_{\pm 0.29}} $& $\boxed{87.5_{\pm 0.22}}$ & $\boxed{85.2_{\pm 0.98}} $ & $\boxed{83.5_{\pm 0.29}}$ & $\boxed{97.3_{\pm 0.14}}$ & $\boxed{81.8_{\pm 0.52}}$ & $\boxed{96.4_{\pm 0.20}}$ & $\boxed{88.9_{\pm 0.44}}$ &   $\boxed{84.7_{\pm 0.37}}$\\
SCE+B* & $97.0_{\pm 0.16}$ & $93.7_{\pm 0.52} $& $87.5_{\pm 0.22}$ & $85.8_{\pm 0.67} $ & $83.6_{\pm 0.03}$ & $97.4_{\pm 0.02}$ & $81.8_{\pm 0.52}$ & $96.5_{\pm 0.26}$ & $88.9_{\pm 0.44}$ &   $84.9_{\pm 0.20}$\\ \hline
CE & $80.8_{\pm 2.31}$ & $67.3_{\pm 0.80} $& $80.9_{\pm 1.11} $& $72.1_{\pm 2.16} $& $79.9_{\pm 0.28}$ & $96.4_{\pm 0.08}$ & $75.6_{\pm 0.20}$ & $94.2_{\pm 0.24}$ & $76.9_{\pm 1.22}$  &   $59.9_{\pm 2.15}$ \\
\rowcolor{green}
CE+B & $\boxed{96.2_{\pm 0.32}}$ & $\boxed{93.0_{\pm 0.09}}$& $\boxed{87.9_{\pm 0.10}}$ & $\boxed{84.7_{\pm 0.37}} $ & $\boxed{80.8_{\pm 0.08}}$ & $\boxed{97.0_{\pm 0.04}}$ & $\boxed{78.9_{\pm 0.12}}$ & $\boxed{96.1_{\pm 0.26}}$ & $\boxed{84.5_{\pm 0.73}}$ &   $\boxed{76.0_{\pm 1.13}}$\\ 
CE+B* & $96.2_{\pm 0.32}$ & $93.0_{\pm 0.09}$ & $87.9_{\pm 0.10}$ & $84.7_{\pm 0.37} $ & $81.5_{\pm 0.11}$ & $97.3_{\pm 0.02}$ & $79.0_{\pm 0.09}$ &  $96.2_{\pm 0.01}$ & $84.8_{\pm 0.55}$ &   $78.6_{\pm 1.28}$\\ \hline
CEP & $97.5_{\pm 0.08 }$ & $92.1_{\pm 0.44} $ & $87.8_{\pm 0.12 }$ & $84.8_{\pm 0.23}$ & $85.5_{\pm 0.10}$ & $98.1_{\pm 0.07}$ & $84.3_{\pm 0.22}$ & $97.6_{\pm 0.14}$ & $84.2_{\pm 0.51}$ &  $58.2_{\pm 2.94}$\\
\rowcolor{green}
CEP+B & $95.6_{\pm 0.32}$ & $85.5_{\pm 0.77} $& $\boxed{88.1_{\pm 0.31}}$ & $84.2_{\pm 0.33} $ & \cellcolor{cream} $\boxed{\bm{85.8}_{\pm 0.12}}$ & $\boxed{98.3_{\pm 0.02}}$ & \cellcolor{cream} $\boxed{\bm{84.8}_{\pm 0.10}}$ & $\boxed{98.0_{\pm 0.04}}$ & $\boxed{88.5_{\pm 0.32}}$ &   $\boxed{85.1_{\pm 0.20}}$\\ 
CEP+B* & \cellcolor{cream} $\bm{98.5}_{\pm 0.05}$ & \cellcolor{cream} $\bm{97.9}_{\pm 0.11}$ & \cellcolor{cream} $\bm{88.4}_{\pm 0.04}$ &  \cellcolor{cream}$\bm{87.2}_{\pm 0.21} $ & $85.8_{\pm 0.12}$ & $98.3_{\pm 0.02}$ & $84.8_{\pm 0.10}$ & $98.0_{\pm 0.16}$ & $88.5_{\pm 0.32}$ &   $85.1_{\pm 0.20}$\\
\end{tabular}
\end{adjustbox}
\vspace{-3mm}
    \caption{Test accuracies obtained by using different losses on the noisy MNIST/ FashionMNIST/EMNIST/CIFAR10 datasets. Losses implementing the Noise-Corrected Budget shaded in grey. When using this budget provides benefit, the corresponding value is boxed. Overall top values in yellow.\label{table:MNIST}}
    \vspace{2mm}
    \centering
    \begin{adjustbox}{max width=0.98\textwidth}
    \begin{tabular}{l|cccc|cccc|cc}
    \textbf{}  & \multicolumn{4}{c|}{CIFAR100} & \multicolumn{4}{c|}{ASYM-CIFAR100} & \multicolumn{2}{c}{Non-Uniform-EMNIST} \\
\multirow{2}{*}{Losses} & \multicolumn{2}{c|}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{2}{c|}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{2}{c}{0.6} \vspace{-1.5mm}\\
  \multicolumn{1}{c|}{} & Top1 & \multicolumn{1}{c|}{Top5} & Top1 & Top5 & Top1 & \multicolumn{1}{c|}{Top5} & Top1 & Top5 & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c}{Top 5} \\ \hline
 MSE & $57.2_{\pm 0.93}$ & $78.6_{\pm 0.25}$ & $40.6_{\pm 0.38}$ & $63.0_{\pm 0.24}$ & $56.3_{\pm 0.11}$  & $82.6_{\pm 0.22}$  & $40.7_{\pm 0.12}$ & $74.4_{\pm 0.25}$ & $44.7_{\pm 2.66}$ & $86.7_{\pm 3.10}$ \\
MAE & $10.0_{\pm 0.11}$  & $13.8_{\pm 0.28}$ & $7.6_{\pm 1.89}$  & $11.6_{\pm 1.25}$ & $7.1_{\pm 6.02}$  & $11.1_{\pm 6.6}$  & $11.1_{\pm 5.43}$  & $25.1_{\pm 5.76}$ &  $9.8_{\pm 1.74}$ & $23.1_{\pm 1.80}$\\
NCE  & $38.7_{\pm 3.13}$ & $51.8_{\pm 3.77}$ & $19.1_{\pm 0.20}$ & $28.8_{\pm 0.15}$ & $16.3_{\pm 1.24}$ & $25.4_{\pm 1.80}$  & $21.8_{\pm 1.24}$ & $37.2_{\pm 1.80}$  & $18.0_{\pm 1.17}$ &  $38.8_{\pm 1.93}$ \\
MixUp  & $59.6_{\pm 0.31}$ & $81.5_{\pm 0.39}$ & $51.3_{\pm 8.63}$ & $75.8_{\pm 8.09}$ & $61.2_{\pm 0.88}$ & $86.0_{\pm 1.12}$  & $47.2_{\pm 0.60}$ & $81.3_{\pm 0.23}$ &  \cellcolor{cream} $\bm{52.4}_{\pm 0.80}$ & $95.5_{\pm 0.08}$ \\
Sph.   & $57.7_{\pm 0.18}$ & $82.9_{\pm 0.54}$ & $48.8_{\pm 0.51}$ & $74.3_{\pm 0.73}$ & $54.2_{\pm 0.32}$ & $81.2_{\pm 0.29}$  & $39.2_{\pm 0.31}$ & $72.1_{\pm 0.15}$ & $41.9_{\pm 0.10}$ &  $94.4_{\pm 0.04}$ \\
Boot.  & $54.0_{\pm 0.37}$ & $76.4_{\pm 0.39}$ & $37.7_{\pm 0.89}$ & $60.9_{\pm 1.52}$ & $56.0_{\pm 0.34}$ & $83.8_{\pm 0.03}$  & $43.2_{\pm 0.35}$ & $78.3_{\pm 0.20}$ &  $49.1_{\pm 0.29}$ & $95.3_{\pm 0.42}$ \\
Trunc. & $58.1_{\pm 0.36}$ & $82.7_{\pm 0.37}$ & $50.9_{\pm 1.17}$ & $77.2_{\pm 0.59}$ & $56.3_{\pm 0.62}$ & $82.3_{\pm 0.61}$  & $45.2_{\pm 0.81}$ & $75.6_{\pm 0.29}$ &  $23.7_{\pm 0.98}$ & $40.1_{\pm 1.24}$ \\
CL & $53.0_{\pm 0.21}$ & $76.3_{\pm 0.19}$ & $36.3_{\pm 0.77}$ & $60.1_{\pm 0.66}$ & $55.3_{\pm 0.48}$ & $83.5_{\pm 0.28}$  & $42.4_{\pm 0.45}$ & $78.1_{\pm 0.14}$ &  $48.2_{\pm 0.45}$ & $95.0_{\pm 0.04}$ \\
ELR & $10.4_{\pm 0.24}$ & $31.7_{\pm 0.44}$ & $10.0_{\pm 0.64}$ & $30.1_{\pm 0.88}$ & $10.8_{\pm 0.21}$ & $32.7_{\pm 0.53}$  & $10.3_{\pm 0.39}$ & $30.8_{\pm 0.35}$ &  $40.3_{\pm 0.39}$ & $93.0_{\pm 0.24}$ \\\hline
FCE & $56.9_{\pm 0.58}$ & $79.2_{\pm 0.14}$ & $43.7_{\pm 0.15}$ & $66.2_{\pm{0.19}}$ & $55.3_{\pm 0.54}$ & $83.5_{\pm 0.24}$  & $41.4_{\pm 0.55}$ & $77.3_{\pm 0.75}$ &  $39.0_{\pm 0.05}$ & $67.8_{\pm 0.47}$ \\
\rowcolor{green}
FCE+B & $56.1_{\pm 2.22}$ & $\boxed{81.8_{\pm 1.37}}$ & $\boxed{50.2_{\pm 0.02}}$ & $\boxed{77.2_{\pm 0.19}}$ & $54.2_{\pm 0.44}$ & $83.3_{\pm 0.43}$  & $\boxed{43.8_{\pm 0.02}}$ & $\boxed{77.5_{\pm 0.13}}$ &  $\boxed{40.0_{\pm 0.35}}$ & $\boxed{73.2_{\pm 0.08}}$ \\
FCE+B* & $56.1_{\pm 2.22}$ & $82.2_{\pm 0.39}$ & $50.2_{\pm 0.02}$ & $77.2_{\pm 0.19}$ & $54.2_{\pm 0.44}$ & $83.4_{\pm 0.24}$  & $45.1_{\pm 0.37}$ & $79.9_{\pm 0.24}$ &  $43.1_{\pm 0.40}$ & $79.4_{\pm 0.12}$ \\\hline
GCE  & $60.0_{\pm 0.13}$ & $82.6_{\pm 0.63}$ & $44.9_{\pm 0.07}$ & $67.2_{\pm 0.34}$ & $53.8_{\pm 0.55}$ & $81.6_{\pm 0.14}$  & $39.4_{\pm 0.44}$ & $74.0_{\pm 0.36}$ &  $44.8_{\pm 0.62}$ & $91.2_{\pm 0.70}$ \\
\rowcolor{green}
 GCE+B & $59.4_{\pm 0.02}$ & $\boxed{83.5_{\pm 0.24}}$  & $\boxed{50.3_{\pm 0.11}}$ & $\boxed{75.3_{\pm 0.64}}$ & $\boxed{55.4_{\pm 0.55}}$ & $\boxed{83.0_{\pm 0.35}}$  & $\boxed{46.5_{\pm 1.44}}$ & $\boxed{77.7_{\pm 0.35}}$ &  $\boxed{47.1_{\pm 0.20}}$ & $\boxed{93.5_{\pm 0.43}}$ \\
GCE+B* & $61.0_{\pm 1.33}$ & $83.9_{\pm 0.74}$  & $50.3_{\pm 0.11}$ & $75.3_{\pm 0.64}$ & $56.6_{\pm 0.10}$ & $83.8_{\pm 0.88}$  & $47.7_{\pm 0.35}$ & $77.9_{\pm 0.03}$ &   $47.1_{\pm 0.20}$ & $93.5_{\pm 0.43}$ \\\hline
SCE  & $55.9_{\pm 0.53}$ & $76.5_{\pm 0.15}$ & $38.7_{\pm 0.60}$ & $60.9_{\pm 0.41}$ & $57.5_{\pm 0.19}$ & $83.7_{\pm 0.17}$  & $43.3_{\pm 0.87}$ & $77.5_{\pm 0.75}$ &  $47.2_{\pm 0.33}$ & $92.5_{\pm 0.01}$\\
\rowcolor{green}
SCE+B & $55.5_{\pm 0.90}$ & $\boxed{77.4_{\pm 0.84}}$ & $\boxed{47.1_{\pm 1.32}}$ & $\boxed{69.2_{\pm 1.18}}$ & $\boxed{57.9_{\pm 0.83}}$ & $83.7_{\pm 0.41}$  & $\boxed{50.0_{\pm 1.62}}$ & $\boxed{80.4_{\pm 0.65}}$ &  $\boxed{47.9_{\pm 0.80}}$ & $\boxed{93.8_{\pm 0.05}}$ \\
SCE+B* & $56.6_{\pm 1.07}$ & $78.5_{\pm 0.88}$ & $47.3_{\pm 1.16}$ & $69.6_{\pm 0.90}$ & $57.9_{\pm 0.83}$ & $83.7_{\pm 0.41}$  & $50.0_{\pm 1.62}$ & $80.4_{\pm 0.65}$ &  $47.9_{\pm 0.80}$ & $93.8_{\pm 0.05}$ \\\hline
CE & $52.3_{\pm 1.35}$ & $75.6_{\pm 0.93}$ & $35.3_{\pm 1.14}$ & $59.3_{\pm 0.81}$ & $54.9_{\pm 0.12}$ & $83.3_{\pm 0.25}$ & $42.4_{\pm 0.16}$ & $78.9_{\pm 0.56}$ &  $48.6_{\pm 0.11}$ & $95.3_{\pm 0.10}$ \\
\rowcolor{green}
CE+B & $\boxed{50.9_{\pm 1.01}}$ & $\boxed{76.5_{\pm 0.86}}$ & $\boxed{39.9_{\pm 1.02}}$ & $\boxed{65.8_{\pm 1.19}}$ & $52.9_{\pm 1.86}$ & $83.2_{\pm 0.88}$  & $34.7_{\pm 2.51}$ & $73.4_{\pm 1.50}$  &  $45.5_{\pm 5.11}$ & $93.0_{\pm 0.16}$ \\
CE+B* & $50.9_{\pm 1.01}$ & $78.2_{\pm 1.16}$ & $39.9_{\pm 1.02}$ & $68.1_{\pm 0.63}$ & $53.3_{\pm 0.89}$ & $83.2_{\pm 0.88}$  & $45.9_{\pm 0.40}$ & $79.7_{\pm 0.29}$  & $50.2_{\pm 0.35}$ & \cellcolor{cream} $\bm{95.9}_{\pm 0.14}$ \\ \hline
CEP  & $58.8_{\pm 0.87}$ & $78.6_{\pm 0.38}$ & $43.5_{\pm 0.24}$ & $65.1_{\pm 1.27}$ & $59.4_{\pm 0.08}$ & $82.2_{\pm 0.03}$  & $46.5_{\pm 0.17}$ & $76.4_{\pm 0.25}$ &  $48.2_{\pm 0.05}$ & $95.4_{\pm 0.07}$ \\
\rowcolor{green}
CEP+B & \cellcolor{cream} $\boxed{\bm{62.3}_{\pm 0.87}}$ & \cellcolor{cream} $\boxed{\bm{85.1}_{\pm 0.46}}$ & $\boxed{54.3_{\pm 0.86}}$ & $\boxed{79.2_{\pm 0.93}}$ & $\boxed{63.0_{\pm 0.92}}$ & \cellcolor{cream} $\boxed{\bm{87.5}_{\pm 0.32}}$  & $\boxed{53.0_{\pm 0.28}}$ & $\boxed{82.8_{\pm 0.13}}$ &  $45.0_{\pm 0.48}$ & $95.0_{\pm 0.08}$\\
CEP+B* & $62.9_{\pm 0.79}$ & $85.1_{\pm 0.46}$ & \cellcolor{cream} $\bm{55.3}_{\pm 0.37}$ & \cellcolor{cream} $\bm{79.8}_{\pm 0.08}$ & \cellcolor{cream} $\bm{63.0}_{\pm 0.14}$ & $87.5_{\pm 0.32}$  & \cellcolor{cream} $\bm{55.6}_{\pm 0.66}$ & \cellcolor{cream} $\bm{83.8}_{\pm 0.11}$ &   $47.7_{\pm 0.19}$ & $95.9_{\pm 0.23}$
    \end{tabular}
    \end{adjustbox}
    \vspace{-3mm}
    \caption{Test accuracies for different losses on the noisy CIFAR100/Asym-CIFAR100/Non-Uniform EMNIST datasets. Losses implementing the Noise-Corrected Budget shaded in grey. When using this budget provides benefit, the corresponding value is boxed. Overall top values in yellow. \vspace{-2mm}}
    \label{table:CIFAR}
\end{table*}



%\bibliographystyle{plain}
\bibliographystyle{apalike}
\bibliography{neurips_2023}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\onecolumn

\section{Proofs}\label{sec:proofs}

\lemmaFourPointTwo*

\begin{proof}
We begin by introducing the following notation: Let $\mathcal{L}$ be an elementwise loss and let $\vec{p}, \vec{q}$ be two distributions, we denote the expected loss of $\vec{q}$ with respect to $\vec{p}$ to be $L_{\mathcal{L}}(\vec{q}, \Vec{p}) := \sum_{i=1}^c p_i\mathcal{L}(\vec{q}, i)$.

Let us begin by considering GCE. The expected loss may be written $L_{GCE}(\vec{q}, \vec{p}) := \sum_{i=1}^c p_i\mathcal{L}_{GCE}(\vec{q}, i) := \sum_{i=1}^c p_i\frac{1-q_i^a}{a}$. We find the minima by constructing the Langrangian $A(\vec{q},\lambda) := \sum_{i=1}^c p_i\frac{1-q_i^a}{a} +\lambda (\sum_{i=1}^c q_i - 1)$. By taking partials and equating to zero, we obtain $q_i^{1-a} = \frac{ap_i}{\lambda}, \forall i$. Using the fact that $\sum_{i=1}^c q_i = 1$ one may find the value of $\lambda$. Specifically, $\lambda = a(\sum_{i=1}^c p_i^{\frac{1}{1-a}})^{1-a}$. Thus overall one has $q^*_i = (\frac{ap_i}{\lambda})^{\frac{1}{1-a}} = \frac{p_i^{\frac{1}{1-a}}}{\sum_{i=1}^c p_i^{\frac{1}{1-a}}}$. Let us repeat this for the SCE loss. The expected loss may be written $L_{SCE}(\vec{q}, \vec{p}):= \sum_{i=1}^c p_i\mathcal{L}_{SCE}(\vec{q}, i) := \sum_{i=1}^c p_i(A(1-q_i)-log(q_i))$. As before, we construct the relevant Lagrangian and find the stationary points: $B(\vec{q}, \lambda) := \sum_{i=1}^c p_i(A(1-q_i)-log(q_i)) + \lambda (\sum_{i=1}^c q_i - 1)$. Taking partials and equating to zero we obtain $p_i(A+\frac{1}{q_i}) = \lambda \implies q_i^* = \frac{p_i}{\lambda-Ap_i}$. Here the value of the normalisation constant $\lambda$ cannot be found in closed form for high values of $c$ and must be computed numerically. Finally, we consider the F-Corrected CE loss. We assume that the loss is corrected by some invertible stochastic matrix $T$. $L_{FCorr}(\vec{q}, \vec{p}):= \sum_{i=1}^c p_i\mathcal{L}_{FCorr}(\vec{q}, i) := \sum_{i=1}^c -p_ilog((T\vec{q})_i)$. We remark that since CE is proper that this is minimised on the simplex by $\vec{p} = T\vec{q}^*\iff \vec{q}^* = T^{-1}\vec{p}$. For each loss, the function $f$ obtained is bijective as desired.
\end{proof}


\LemmaFivePointOne*
\begin{proof}
    Recollect that since $\mathcal{L}$ is a strictly $f-$proper loss, this implies that there exists a strictly proper loss $\tilde{\mathcal{L}}$ such that, for all $\vec{q}$, $\mathcal{L}(f(\vec{q}), i) = \tilde{\mathcal{L}}(\vec{q}, i)$. Now, let $x$ be some arbitrary point in the support of $p(x)$ and let $\vec{q}(x)$ be some probability estimator. The pointwise noisy risk of $\vec{q}$ at $x$ may be written as $R^{\eta}_{\mathcal{L}}(\vec{q})(x) := \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\mathcal{L}(\vec{q}(x), i) = \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\tilde{\mathcal{L}}(f^{-1}(\vec{q}(x)), i) \geq \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\tilde{\mathcal{L}}(\tilde{p}(\tilde{y}\vert x), i) =  \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\mathcal{L}(f(\tilde{p}(\tilde{y}\vert x)), i) =: R^{\eta}_{\mathcal{L}}(f(\Tilde{p}(\tilde{y}\vert x)))$. The inequality follows from the definition of the properness of $\tilde{\mathcal{L}}$ while the following equality follows from the definition of $f-$properness. It remains to show that this is attained uniquely by $\vec{q}(x) = f(\tilde{p}(\tilde{y}\vert x))$. Since $\tilde{\mathcal{L}}$ is strictly proper, we know that the inequality is only obtained by $f^{-1}(\vec{q}(x)) = \tilde{p}(\tilde{y}=i\vert x)$. The injectivity of $f$ (as specified in the definition of $f-$proper) means this occurs uniquely at $\vec{q}(x) = f(\tilde{p}(\tilde{y}=i\vert x))$ as desired.
\end{proof}

 \lemmaFivePointFive*

  \begin{proof}
  Our proof follows similar lines to Lemma 5.1 with the additional application of Theorem 5.3.
  
      Recollect that since $\mathcal{L}$ is a (strictly) $f-$proper loss, this implies that there exists a (strictly) proper loss $\tilde{\mathcal{L}}$ such that, for all $\vec{q}$, $\mathcal{L}(f(\vec{q}), i) = \tilde{\mathcal{L}}(\vec{q}, i)$. Now, let $x$ be some arbitrary point in the support of $p(x)$ and let $\vec{q}(x)$ be some probability estimator. The pointwise noisy risk of $\vec{q}$ at $x$ may be written:
      \begin{align}
          R^{\eta}_{\mathcal{L}}(\vec{q})(x) := & \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\mathcal{L}(\vec{q}(x), i) \\
          = & \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\tilde{\mathcal{L}}(f^{-1}(\vec{q}(x)), i) \\
          \geq & \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\tilde{\mathcal{L}}(\tilde{p}(\tilde{y}\vert x), i) \label{eqn:inequality2}\\ = &  \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\mathcal{L}(f(\tilde{p}(\tilde{y}\vert x)), i) \label{eqn:equality2}
          =: R^{\eta}_{\mathcal{L}}(f(\Tilde{p}(\tilde{y}\vert x)))
      \end{align}
     The inequality (Eqn~\ref{eqn:inequality2}) follows from the definition of the properness of $\tilde{\mathcal{L}}$ while the following equality (Eqn.~\ref{eqn:equality2}) follows from the definition of $f-$properness. Note that Equation~\ref{eqn:equality2} is the definition of the entropy of $\Tilde{p}(\Tilde{y}\vert x)$ with respect to the entropy function $J$ associated with $\Tilde{\mathcal{L}}$. Thus for all $x\in \supp(p(x))$, we have $R^{\eta}_{\mathcal{L}}(\vec{q})(x) \geq J(\Tilde{p}(\Tilde{y}\vert x))$. As in the proof of Lemma 5.1, note that when $\mathcal{L}$ is (\textit{strictly}) $f-$proper, this is attained (uniquely) by setting $\vec{q}(x) = f(\tilde{p}(\tilde{y}=i\vert x))$. It remains to show that $ J(\Tilde{p}(\Tilde{y}\vert x)) = J(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1})$ when the noise is symmetric. This follows from the determinism assumption.
  
Suppose that, for all $x\in \supp (p(x))$ that the label noise is symmetric with rate $\eta(x)$. Let $T(x)$ denote the noising transition matrix at $x$, that is $T_{ij}:= p(\tilde{y}=j\vert y=i, x)$. By the deterministic assumption, we have some $k$ such that $p(y=k \vert x)=1$ and $p(y=i \vert x)=0$ otherwise. Thus $\Tilde{p}(\Tilde{y}\vert x) = \sum_{y=1}^c\Tilde{p}(\tilde{y}\vert y,x)p(y \vert x) = \Tilde{p}(\tilde{y}\vert y=k,x) = (T_{1k}, T_{2k}, \ldots, T_{ck})$. The noise rate is defined to be the probability that the label is altered by our noise and may thus be expressed as $\eta(x) = 1-T_{kk}$. By the definition of symmetric noise (Definition 3.5), we have $T_{ik} = \frac{\eta(x)}{1-c}$ for $i \neq k$. Hence $ J(\Tilde{p}(\Tilde{y}\vert x)) = J(T_{1k}, T_{2k}, \ldots, T_{ck}) = J(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1})$ as desired.
  \end{proof}

\corrFivePointSix*
\begin{proof}
    Uniform label noise means that for all $x$, $\eta(x) = \eta$. Thus from Lemma 5.5 we have  $R^{\eta}_{\mathcal{L}}(q) \geq \mathbb{E}_{x\sim p(x)}[J\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)] = \mathbb{E}_{x\sim p(x)}[J\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big)] = J\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big)$. This bound is obtained by setting $\vec{q}(x) = \Tilde{p}(\Tilde{y}\vert x)$ for all $x\in \supp(p(x))$. This is unique if $\mathcal{L}$ is \textit{strictly} $f-$proper.  
\end{proof}

\corrFivePointSeven*
\begin{proof}
    Lemma 5.1 states that, for all $x\in supp(p(x))$ we have,  $R^{\eta}_{\mathcal{L}}(\vec{q})(x) \geq R^{\eta}_{\mathcal{L}}(f(\Tilde{p}(\tilde{y}\vert x)))$. Since $p(y\vert x)$ is deterministic it follows we have some $k$ such that $p(y=k \vert x)=1$ and $p(y=i \vert x)=0$ otherwise. Since $\mathcal{L}$ is symmetric, we may, without loss of generality, let $k=1$. Thus $\Tilde{p}(\Tilde{y}\vert x) = \sum_{y=1}^c\Tilde{p}(\tilde{y}\vert y,x)p(y \vert x) = \Tilde{p}(\tilde{y}\vert y=k,x) = (1-\eta, \frac{\eta}{c-1}, \frac{\eta}{c-1}, \ldots ,  \frac{\eta}{c-1}) =: \vec{u}(\eta, c)$. The second-to-last equality follows from our assumption that our noise is symmetric and uniform. Putting these together we have, for all $x\in supp(p(x))$ we have,  $R^{\eta}_{\mathcal{L}}(\vec{q})(x) \geq R^{\eta}_{\mathcal{L}}(f(\vec{u}(\eta, c))) := (1-\eta)\mathcal{L}(f(\vec{u}(\eta, c)), 1) + \eta \mathcal{L}(f(\vec{u}(\eta, c)), i\neq 1)$. Our result follows by taking expectations with respect to $p(x)$ on both sides. By Lemma 5.1, equality is obtained by setting  $\vec{q}(x) = \Tilde{p}(\Tilde{y}\vert x)$ for all $x\in \supp(p(x))$. This is unique if $\mathcal{L}$ is \textit{strictly} $f-$proper. 

\end{proof}

 \begin{lemma}
      Let $p(x,y)$ be a distribution where $p(y\vert x)$ is deterministic, and let $\Tilde{p}(x,\Tilde{y})$ be a noisy distribution obtained by applying label noise to $p(x,y)$. Assume that $\mathcal{L}$ is a symmetric (strictly) $f$-proper loss and let $J$ denote the entropy function of its base loss. For any probability estimator $q$, we may lower bound the generalised noisy risk of $\vec{q}$ in terms of a quantity $A(\eta(x), c)$ where $\eta(x)$ denotes the noise rate at $x$ and $c$ the number of classes. This bound is achieved (uniquely) by $q(x) = \tilde{p}(\tilde{y}\vert x)$. Specifically,
  \begin{align*}
     R^{\eta}_{\mathcal{L}}(q) \geq \mathbb{E}_{x\sim p(x)}[A(\eta(x), c)]\\
 \end{align*}
 Where $A(\eta(x), c)$ lies in the following interval:
 \begin{align*}
          A(\eta(x), c) \in \big[J(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ), J\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\big]
 \end{align*}
  \end{lemma}
  \begin{proof}
      Let $\vec{q}(x)$ be a probability estimator and let $x$ be some point in the support of $p(x)$. We established in the proof of Lemma 5.5 that $R^{\eta}_{\mathcal{L}}(\vec{q})(x) \geq J(\tilde{p}(\tilde{y}\vert x))$. We have equality (uniquely) when $\vec{q}(x) = \tilde{p}(\tilde{y}\vert x)$. Let $T(x)$ denote the noising transition matrix at $x$, that is $T_{ij}(x):= p(\tilde{y}=j\vert y=i, x)$. By the determinism assumption, we have some $k$ such that $p(y=k \vert x)=1$ and $p(y=i \vert x)=0$ otherwise. Thus $\Tilde{p}(\Tilde{y}\vert x) = \sum_{y=1}^c\Tilde{p}(\tilde{y}\vert y,x)p(y \vert x) = \Tilde{p}(\tilde{y}\vert y=k,x) = (T_{1k}(x), T_{2k}(x), \ldots, T_{ck}(x))$. Let $A(\eta(x), c) := J(T_{1k}(x), T_{2k}(x), \ldots, T_{ck}(x))$ where $\eta(x):= 1- T_{kk}$ is the noise rate at $x$. The symmetry of $J$ means that, without loss of generality, we may let $k=1$. It remains to show that $A(\eta(x), c) \in \big[J(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ), J\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\big]$.  
      
      \textbf{Upper Limit:} We begin by demonstrating that $A(\eta(x), c)$ is upper bounded by $J(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1})$. Let $\Delta(\eta(x))$ denote the set of non-negative vectors $(a_1, a_2, \ldots, a_{c-1})$ such that $a_i \leq 1$ and $\sum_{i=1}^{c-1}a_i = \eta(x)$. We wish to show the supremum of $J(1-\eta(x), a_1, a_2, \ldots, a_{c-1})$ is attained on $\Delta(\eta(x))$ by setting $a_i = \frac{\eta(x)}{c-1}$ for all $i$. This corresponds to the label noise being symmetric at $x$. By Theorem 5.3, $J$ is a (strictly) concave function. Moreover, the symmetry assumption implies that $J$ is a symmetric function of its variables. Define the function $g(a_1, a_2, \ldots, a_{c-1}) := J(1-\eta(x), a_1, a_2, \ldots,  a_{c-1})$. We wish to show that $g$ attains its maximum on the relevant domain when $a_i=a_j$ for all $i,j$. We begin by noting that the (strict) concavity of $J$ implies the (strict) concavity of $g$. To see this consider two arbitrary vectors $\vec{x}=(x_1, x_2, \ldots x_{c-1}), \vec{y}=(y_1, y_2,\ldots y_{c-1})$. Now $g(\lambda \Vec{x} + (1-\lambda) \vec{y}) = J(\lambda \vec{x}^{\prime} + (1-\lambda) \vec{y}^{\prime})$ where $\vec{x}^{\prime} := (1-\eta(x), x_1, x_2, \ldots ,x_{c-1})$ and $\vec{y}^{\prime} := (1-\eta(x), y_1, y_2, \ldots ,y_{c-1})$. Thus the concavity of $J$ implies  $g(\lambda \Vec{x} + (1-\lambda) \vec{y}) := J(\lambda \vec{x}^{\prime} + (1-\lambda) \vec{y}^{\prime})  \geq \lambda J(\vec{x}^{\prime}) + (1-\lambda)J(\vec{y}^{\prime}) = \lambda g(\vec{x}) + (1-\lambda)g(\vec{y}) $ as desired. Thus $g$ is a symmetric (strictly) concave function of its variables. 

      Let $\vec{a}^*$ denote a maxima of $g$ on $\Delta(\eta(x))$. Let $\sigma$ denote the cyclic permutation of the components of $\vec{a}$. That is $\sigma(a_1, a_2, \ldots, a_{c-1}) := (a_{c-1}, a_1, a_2, \ldots, a_{c-2})$. By the symmetry of $g$, we know that if $\vec{a}^*$ is a maxima then so is $\sigma^i(\vec{a}^*)$ for all $i$. Hence by the (strict) concavity of $g$, we have: 
      \begin{align}
         g\big(\frac{\eta(x)}{c-1}, \frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1}\big) &= g\big(\frac{1}{c-1}(\vec{a}^* + \sigma(\vec{a}^*) + \sigma^2(\vec{a}^*) + \ldots + \sigma^{c-2}(\vec{a}^*))\big) \\
         &\geq \frac{1}{c-1}g\big(\vec{a}^*\big) + \frac{1}{c-1}g\big(\sigma(\vec{a}^*)\big) + \ldots + \frac{1}{c-1}g\big(\sigma^{c-1}(\vec{a}^*)\big) \\
         &= g(\vec{a}^*)
      \end{align}
      Hence $g$ is maximised by setting $a_i = \frac{\eta(x)}{c-1}$ for all $i$ as desired. This is the unique maxima when $\mathcal{L}$ is strictly $f-$proper.

      \textbf{Lower Limit:} It now remains to show that the lower bound on $A(\eta(x),c)$ holds. The (strict) concavity means that $g$ attains it minima on the vertices of $\Delta(\eta(x))$ (eg $(\eta(x),0,\ldots, 0)$. To see this let $\vec{a}^* = (a^*_1, a^*_2, \ldots, a^*_{c-1})$ denote a minima of $g$ on $\Delta(\eta(x))$. Then we have, 
      \begin{align}
          g(a^*_1, a^*_2, \ldots, a^*_{c-1}) &= g(a^*_1 \vec{e_1} + a^*_2 \vec{e_2} + \ldots + a^*_{c-1} \vec{e}_{c-1})\\
          &\geq \sum_{i=1}^{c-1}\frac{a^*_i}{\eta(x)} g(\eta(x)\vec{e_i}) \\
          &= g(\eta(x), 0, \ldots, 0)\label{eqn:gsym}\\ &= J(1-\eta(x), \eta(x), 0, 0, \ldots, 0 )
      \end{align}
      $\vec{e}_i$ denotes the coordinate vector with 1 in the $i$th position and zeros elsewhere. Equation~\ref{eqn:gsym} holds by the symmetry of $g$ and since $\sum a_i^* = \eta(x)$. Thus we have shown that $g$ is lower bounded by $J(1-\eta(x), \eta(x), 0, 0, \ldots, 0 )$ as desired. Moreover, this infimum is obtained on the vertices of $\Delta(\eta(x))$.
  \end{proof}


\section{Additional Theory and Discussion }\label{sec:AddTheory}
\subsection{Sensitivity of Bounds}\label{sec:sens}
The Noised-Corrected Budget defined in Definition 5.8 was established on the basis of Lemma 5.5 and Corollary 5.6, which assume noise is symmetric and/or uniform. When we deviate from these noise conditions, we generally find that this budget is too high in that an optimal probability estimator could achieve a (noisy) risk lower than this value without overfitting. Since we use this budget in all noise conditions, it is essential to get an idea of the size of the gap between our budget and the minimum achievable risk. Ideally we want this gap to be small. In this section, we look briefly at this topic, noting that this gap is smaller for GCE and SCE than CE. This implies that the noise-corrected budget is more suitably used with SCE and GCE than with CE when noise deviates from idealised assumptions.

In Lemma 5.5 we derived a lower bound on the pointwise noisy risk of a probability estimator when $\mathcal{L}$ is $f-$proper. This bound holds when the label noise is symmetric. When label noise deviates from this condition, our bounds no longer necessarily hold. More generally, we have the following bound:
 \begin{lemma}\label{lemma:sens}
      Let $p(x,y)$ be a distribution where $p(y\vert x)$ is deterministic, and let $\Tilde{p}(x,\Tilde{y})$ be a noisy distribution obtained by applying label noise to $p(x,y)$. Assume that $\mathcal{L}$ is a symmetric (strictly) $f$-proper loss and let $J$ denote the entropy function of its base loss. For any probability estimator $q$, we may lower bound the generalised noisy risk of $\vec{q}$ in terms of a quantity $A(\eta(x), c)$ where $\eta(x)$ denotes the noise rate at $x$ and $c$ the number of classes. This bound is achieved (uniquely) by $q(x) = \tilde{p}(\tilde{y}\vert x)$. Specifically,
  \begin{align*}
     R^{\eta}_{\mathcal{L}}(q) \geq \mathbb{E}_{x\sim p(x)}[A(\eta(x), c)]\\
 \end{align*}
 Where $A(\eta(x), c)$ lies in the following interval:
 \begin{align*}
          A(\eta(x), c) \in \big[J(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ), J\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\big]
 \end{align*}
  \end{lemma}
The proof is given at the end of Appendix~\ref{sec:proofs}.


  Lemma 5.1 tells us that $A(\eta(x),c)$ attains this upper limit of $J(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} )$ only if our label noise is symmetric at $x$. Conversely, as indicated in our proof (Appendix~\ref{sec:proofs}), the lower limit is obtained when the label noise flips labels to only one other class. 

\begin{corollary}\label{cor:uniform_interval}
        When the label noise is uniform with rate $\eta$, one may lower bound the noisy risk of any probability estimator $\vec{q}$ in terms of a quantity $A(\eta, c)$. Moreover, this bound is tight and achieved only when $q(x) = \tilde{p}(\tilde{y} \mid x)$. 
        \begin{align*}
           R^{\eta}_{\mathcal{L}}(q) \geq A(\eta, c)
        \end{align*}
        Where $A(\eta, c)$ lies in the following interval, achieving the upper limit only if the label noise is symmetric for all $x\in \supp(p(x))$:
      \begin{align}
               A(\eta, c) \in \big[J(1-\eta, \eta, 0, 0, \ldots, 0 ), J\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big)\big]
     \end{align}
\end{corollary}
\begin{proof}
    This follows immediately from Lemma~\ref{lemma:sens} when $\eta(x)$ has no dependence on $x$ ($\eta(x)=\eta$).
\end{proof}
  Corollary~\ref{cor:uniform_interval} indicates that when noise is uniform but not symmetric, our Noise-Corrected Budget (Definition 5.8) of $J(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$ is too high since the true minimum achievable risk is lower than this budget. In other words, there exists a probability estimator which attains a risk lower than our budget. This non-optimality is the cost we incur as a result of requiring a simple, easily computable budget. Importantly, this corollary gives us a rough way to quantify this non-optimality, using the difference between the upper and lower limits of the interval $\big[J(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ), J\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\big]$. When this difference is large, one could construct two types of label noise with the same rate $\eta$, such that the difference in the minimum achievable risks between these noise types is significant. Conversely, when this gap is small, the minimum achievable risk for any type of label noise at a fixed rate $\eta$ is similar. This is a desirable property and suggests that simply setting our budget to our noise-corrected budget is probably suitable regardless of the specifics of the noising process.

   % Figure environment removed
  
  On the top row of Figure~\ref{fig:sensitivity}, we give a plot of the upper and lower limits of $A(\eta, c)$ (Corollary~\ref{cor:uniform_interval}) for $\eta\in (0, 0.5]$ for $c=10$ (left) and $c=200$ (right) for GCE, SCE and CE. The upper limit is given by a dotted line, while the lower limit is given by a filled line in the same colour. Each loss is scaled so they may be more easily compared. Similarly, in the row below, we plot the ratios of the upper and lower limits of $A(\eta, c)$ for each loss. These graphs show that the difference between the upper and lower limits is much greater for CE than for SCE and GCE. This difference is more pronounced when the number of classes is greater. The result is that on non-symmetric noise, our noise-corrected budget (Definition 5.8) will generally be less suitable when used in conjunction with CE than when used with GCE or SCE.

\subsection{$f-$Proper Losses: A Discussion}
Recall Definition~\ref{def:f-proper} below. 

\definitionFourPointOne*

It is instructive to consider how broad this family of losses is since the results in Section 5 hold for all losses which satisfy this definition. We mentioned in Section 4 that this definition trivially contains all proper losses by letting $f=id.$. Below we give a sufficient condition for a loss to be $f-$proper.

\begin{proposition}
    Let $\mathcal{L}$ be some elementwise loss function. Let $L_{\mathcal{L}}:\Delta^{c-1}\times \Delta^{c-1}\rightarrow \mathbb{R}$ denote its expected loss function $L_{\mathcal{L}}(\vec{q}, \Vec{p}) := \sum_{i=1}^c p_i\mathcal{L}(\vec{q}, i)$. Define $g(\vec{p}):= \argmin_{\vec{q}}L_{\mathcal{L}}(\vec{q}, \Vec{p})$, if $g$ is surjective then $\mathcal{L}$ is $f-$proper. 
\end{proposition}
\begin{proof}
 Let $\mathcal{L}$ be some elementwise loss such that $g$ (as defined above) is surjective for some loss $\mathcal{L}$. By the definition of $g$ we have $L_{\mathcal{L}}(\vec{q}, \vec{p}) \geq L_{\mathcal{L}}(g(\vec{p}), \vec{p}) := \sum_{i=1}^c p_i\mathcal{L}(g(\vec{p}), i)$. Now, define the elementwise loss $\tilde{\mathcal{L}}(\vec{q}, i):= \mathcal{L}(g(\vec{q}), i)$. We claim that $\tilde{\mathcal{L}}$ is proper. Let $\vec{p}\in \Delta^{c-1}$ then $L_{\tilde{\mathcal{L}}}(\vec{q}, \vec{p}) := \sum_{i=1}^c p_i\tilde{\mathcal{L}}(\vec{q}, i) := \sum_{i=1}^c p_i\mathcal{L}(g(\vec{q}), i) = L_{\mathcal{L}}(g(\vec{q}), \vec{p})$. This is minimised by setting $g(\vec{q}) = g(\vec{p})$ which occurs at $\vec{p}=\vec{q}$. (If $g$ is injective this occurs uniquely when $\vec{p}=\vec{q}$ although this is not required). Thus it follows that $\Tilde{\mathcal{L}}$ is proper. Since $g$ is surjective then one may define an injective inverse function $f:= g^{-1}$ on $\Delta^{c-1}$.
 Thus we have $\mathcal{L}(\vec{q}, i) = \tilde{\mathcal{L}}(f(\vec{q}), i)$ with $\Tilde{\mathcal{L}}$ proper and $f$ injective as desired. Hence $\mathcal{L}$ is $f-$proper. 
\end{proof}


\subsection{Noise Model Plots}
In Lemma 4.2 we showed that the SCE, GCE and FCE losses are $f-$proper and derived the corresponding functions $f$. As discussed, these functions can be interpreted as denoising models i.e. $p(y\vert x) \approx f(\tilde{p}(\tilde{y}\vert x))$. In Figure~\ref{fig:semiProper}, we give plots of $f$ for SCE, GCE and FCE. The $x-$axis is the true probability $p$ of an event occurring.
On the $y-$axis we plot $f(p)$ against $p$.
%and the $y-$axis is the probability $q$, which minimises the expected loss under a given loss function. For each loss, we plot $f(p)$ against $p$. 
For proper losses, one sets $q=p$, which corresponds to no noise model. The graphs for GCE and SCE are remarkably similar. One can interpret their graphs as a noise model where labels which are intrinsically uncertain are more likely to incur label noise than those which are less ambiguous. FCE requires a noise model in order to be fully specified; we assume symmetric label noise at $\eta=0.4$. Varying $\eta$ will change the steepness of the respective $f$. Finally, we plot MAE. This loss function is not $f-$proper; however, it's useful as a reference. We see that the expected loss is minimised by letting $q=0$ if $p<0.5$ and $q=1$ otherwise. The graphs of SCE and GCE lie between those of MAE and CE. By varying the parameters of these losses, we can interpolate between them.
% Figure environment removed



\subsection{Explicit Budgets}\label{sec:explicitBudgets}
From Lemma 4.2 and Corollary 5.7, we can produce the noise-corrected budgets (Definition 5.8) for GCE and SCE. The budget for GCE is given below.
\begin{align*}
    B_{GCE}(\eta,c) :=\frac{(1-\eta)}{a}\bigg(1-\bigg(\frac{(1-\eta)^{\frac{1}{1-a}}}{(1-\eta)^{\frac{1}{1-a}} + (c-1)(\frac{\eta}{c-1})^{\frac{1}{1-a}}}\bigg)^a\bigg) +\\ \frac{\eta}{a}\bigg(1-\bigg(\frac{\frac{\eta}{c-1}^{\frac{1}{1-a}}}{(1-\eta)^{\frac{1}{1-a}} + (c-1)(\frac{\eta}{c-1})^{\frac{1}{1-a}}}\bigg)^a\bigg)
\end{align*}
The noise-corrected budget for SCE is
\begin{align}
    B_{SCE}(\eta,c) := (1-\eta)\left( -log\bigg(\frac{1-\eta}{\lambda - A(1-\eta)}\bigg) + A \bigg(1-  \frac{1-\eta}{\lambda - A(1-\eta)}\bigg)\right) \\+ \eta \left(-log\bigg(\frac{\eta}{\lambda(c-1) - A\eta}\bigg) + A \bigg(1-  \frac{\eta}{\lambda(c-1) - A\eta}\bigg) \right)
\end{align}
Recollect that $\lambda$ is chosen so that the resulting distribution normalises: $\frac{1-\eta}{\lambda - A(1-\eta)} + \frac{\eta(c-1)}{\lambda(c-1) - A\eta} = 1$ and may be computed numerically or by solving the resulting quadratic.

\section{Further Experiments}\label{sec:FurtherExp}

\subsection{Experimental Details}
The number of training epochs was the same for each loss. For MNIST, FashionMNIST, TinyImageNet and Animals10N, we used 100 epochs; for all other datasets, we used 120 epochs. Each experiment in Tables 1 and 2 (Section 6) was run three times, and the mean and unbiased estimate of the standard deviation is given. We used a ResNet18 architecture for all experiments except TinyImageNet and Animals10N, where a ResNet34 was used. Each experiment is carried out on a single GeForce GTX Titan X. We used a batch size of 300 in all experiments except TinyImageNet and Animals10N, where this is reduced to 200. A learning rate of 0.0001 was used for all losses except MAE (lr = 0.001) and ELR where we used their recommended learning rate of 0.01. We use a learning rate scheduler which scales our learning rate by 0.6 at epoch 60. Our implementation of the \textit{Truncated Loss} comes from the official github implementation of GCE. Likewise, we use the official codebase for our implementation of ELR. Other losses are re-implementations based on details given in the respective papers. Our SCE loss used the recommended hyperparameter of $A=8$. Our GCE loss used $a=0.4$. FCE requires one to define a noise model. In each case, we assume noise is symmetric at the relevant rate. For Animals10N, this rate is set to 11\%, which is the estimated noise rate.

\subsubsection{CE with Prior}
One of the losses used in our experiments is cross-entropy with a `prior' term (CEP). We give an explanation of the motivation for this additional loss term and details of how it's implemented. 

 In Section 5 we assumed that the un-noised distribution $p(y\vert x)$ is deterministic for each $x$ (i.e. $p(k\vert x) = 1, p(i\neq k\vert x)=0)$. Thus, in the case of symmetric noise with a known noise rate $\eta$, the noisy label distribution $\tilde{p}(\tilde{y}\vert x)$ is of the form for each $x$:
 \begin{align}\label{eqn:prior_dist}
     \tilde{p}(\tilde{y}\vert x) = \left(\frac{\eta}{c-1}, \frac{\eta}{c-1}, \ldots, \underbrace{1-\eta}_{\text{kth position}}, \ldots, \frac{\eta}{c-1}\right)
 \end{align}
We argue, therefore, that it is reasonable to introduce a term to penalise our model when its outputs deviate from this distribution. This is achieved through a regularisation term which measures the KL-divergence between our model probabilities and the desired distribution (Eqn~\ref{eqn:prior_dist}). Let $\vec{p}_{\eta} := (p_1, p_2, \ldots, p_c) := (1-\eta, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$ and let $q_1,q_2, \ldots, q_c$ denote the probabilities output by our model. We sort the $q_i$ into descending order (which we denote as $q_{\sigma(i)}$) and define our prior term as:
\begin{align}
    \mathcal{L}_{prior}(\vec{q}, \vec{p}_{\eta}) := -\sum_{i=1}^c p_{i}\log(q_{\sigma(i)}) \label{eqn:prior}
\end{align}
 Thus, overall we have $\mathcal{L}_{CEP}(\vec{q}, i) := \mathcal{L}_{CE}(\vec{q}, i) + \mathcal{L}_{prior}(\vec{q}, \vec{p}_{\eta})$. Tables 1 and 2 in Section 6 show that this additional term generally results in additional improvement over using the noise-corrected budget alone. This prior acts as a method of feasible set reduction: There are many different probability estimators which achieve a training error equal to our noise-corrected budget. Therefore, by introducing a prior term (Eqn.\ref{eqn:prior}) we can further restrict the set of admissible models.
  
\subsubsection{Optimal Budgets}
In our experiment tables in Section 6, we give results using our noise-corrected budgets. We additionally give results where the budget is treated as a hyperparameter. We do not search over the entire space; rather, we do a grid search near the noise-corrected budget. For MNIST, FashionMNIST, EMNIST, CIFAR10 and CIFAR100, we search over $\{-0.2, -0.15, -0.1, \ldots,  0.15, 0.2\}$ where e.g. $0.2$ means that we add $0.2$ onto our noise-corrected budget. For Asymmetric CIFAR100 (ACIFAR100) and Non-uniform EMNIST (NU-EMNIST), this range is broadened to $\{-0.6, -0.55, \ldots  0.55, 0.6\}$. The budgets which give the best results are given in Table~\ref{table:offsets}. When the optimal budget is higher than the noise-corrected budget, this is highlighted in blue. Otherwise, the cell is indicated in red. In our original table, we have columns for Top1 and Top5 accuracy which often have slightly different optimal budgets. For brevity, we combine these by taking a mean of these values.

\begin{table}[h!]\label{table:offsets}
\begin{adjustbox}{max width=1.00\textwidth}
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|c}
 & \multicolumn{2}{c}{MNIST} & \multicolumn{2}{c}{Fashion} & \multicolumn{2}{c}{EMNIST} & \multicolumn{2}{c}{CIFAR10} & \multicolumn{2}{c}{CIFAR100} & \multicolumn{2}{c}{ACIFAR100} & NU-EMNIST \\
 & 0.4 & 0.6 & 0.2 & 0.4 & 0.2 & 0.4 & 0.2 & 0.4 & 0.2 & 0.4 & 0.2 & 0.4 & 0.6 \\
FCE & \cellcolor[HTML]{FFCCC9}-0.05 & \cellcolor[HTML]{FFCCC9}-0.05 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{FFCCC9}-0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.1 & \cellcolor[HTML]{CBCEFB}0.03 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{FFCCC9}-0.1 & \cellcolor[HTML]{FFCCC9}-0.35 & \cellcolor[HTML]{FFCCC9}-0.2 \\
GCE & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.03 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{EFEFEF}0.0 \\
SCE & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{CBCEFB}0.1 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 \\
CEB & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.1 & \cellcolor[HTML]{CBCEFB}0.1 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{FFCCC9}-0.6 \\
CEP & \cellcolor[HTML]{FFCCC9}-0.15 & \cellcolor[HTML]{FFCCC9}-0.15 & \cellcolor[HTML]{FFCCC9}-0.15 & \cellcolor[HTML]{FFCCC9}-0.15 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.02 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{FFCCC9}-0.08 & \cellcolor[HTML]{FFCCC9}-0.08 & \cellcolor[HTML]{FFCCC9}-0.08 & \cellcolor[HTML]{FFCCC9}-0.08 & \cellcolor[HTML]{FFCCC9}-0.1
\end{tabular}
\end{adjustbox}
\caption{table giving the offset of the `optimal' budget from the noise-corrected budget. Here a negative (blue) number means that the budget is greater than the noise-corrected budget. Positive (red) means the optimal budget is lower. Grey means that the optimal budget is zero, i.e no offset.}
\end{table}


\begin{table}[h!]
\centering
\begin{adjustbox}{max width=0.8\textwidth}
\begin{tabular}{c|lllll}
\multicolumn{1}{l}{} & \multicolumn{2}{c}{TinyImageNet (0.2)}  & \multicolumn{2}{c}{TinyImageNet (0.4)} & Animals \\\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Losses & \multicolumn{1}{l}{Top 1} & \multicolumn{1}{l}{Top 5} & \multicolumn{1}{l}{Top 1} & \multicolumn{1}{l}{Top 5} & \multicolumn{1}{l}{} \\\hline
L2 (MSE) & $42.91$ & $67.02$ & $29.42$ & $53.13$ & $80.97$ \\
MAE & $3.86$ & $5.58$ & $3.94$ & $5.54$ & $54.67$ \\
NCE-MAE & $7.63$ & $10.24$ & $6.29$ & $10.70$ & $80.85$ \\
Mix-Up & $47.13$ & $70.08$ & $31.05$ & $58.96$ & \cellcolor{cream}$83.76$ \\
Bootstrap & $40.04$ & $61.94$ & $25.69$ & $46.65$ & $82.11$ \\
Truncated & $43.35$ & $63.67$ & $38.14$ & $59.99$ & $81.69$ \\
Mix-Up & $47.13$ & $70.08$ & $31.05$ & $58.96$ & $83.10$ \\ 
Curriculum & $41.81$ & $64.53$ & $27.57$ & $48.84$ & $81.68$  \\
ELR & $44.95$ & $66.65$ & $34.66$ & $55.72$ & $82.62$\\\hline
FCE & $43.81$ & $64.97$ & \cellcolor{cream}$48.85$ & $29.92$ & $81.82$ \\\rowcolor{green}
FCE+B & \cellcolor{cream} $\boxed{51.18}$ & \cellcolor{cream} $\boxed{73.79}$ & $46.34$ & \cellcolor{cream}$\boxed{69.92}$  &  $\boxed{82.40}$ \\\hline
GCE & $39.81$ & $60.51$ & $26.93$ & $45.17$ & $81.13$  \\\rowcolor{green}
GCE+B & $\boxed{47.40}$ & $\boxed{71.37}$ & $\boxed{39.13}$ & $\boxed{63.75}$ &  $\boxed{81.37}$ \\\hline
SCE & $39.81$ & $60.51$ & $26.93$ & $45.17$ & $82.59$  \\\rowcolor{green}
SCE+B & $\boxed{41.02}$ & $\boxed{63.06}$  & $\boxed{32.02}$ & $\boxed{52.44}$  &$81.25$   \\\hline
CE & $39.34$ & $61.82$ & $25.84$ & $46.08$ & $81.45$ \\\rowcolor{green}
CE+B & $38.47$ & $\boxed{61.85}$ & $\boxed{30.00}$ & $\boxed{52.61}$ & $80.72$ \\ \hline
CEP & $44.39$ & $64.56$ & $33.33$ & $51.45$ & $82.06$  \\\rowcolor{green}
CEP+B & $\boxed{47.85}$ & $\boxed{71.00}$ & $\boxed{40.56}$ & $\boxed{65.15}$  & $81.79$ \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Test accuracies obtained by using different losses on the noisy TinyImageNet and Animals10N datasets. Losses implementing the Noise-Corrected Budget are shaded in grey. When using this budget provides benefit, the corresponding value is boxed. Overall top values are in yellow.}
\label{table:imagenet}
\end{table}



\end{document}