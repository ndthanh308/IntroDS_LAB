\begin{thebibliography}{}

\bibitem[Arazo et~al., 2019]{bmm}
Arazo, E., Ortego, D., Albert, P., Oâ€™Connor, N., and McGuinness, K. (2019).
\newblock Unsupervised label noise modeling and loss correction.
\newblock In {\em International conference on machine learning}, pages
  312--321. PMLR.

\bibitem[Arpit et~al., 2017]{memorisation}
Arpit, D., Jastrz{\k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al. (2017).
\newblock A closer look at memorization in deep networks.
\newblock In {\em International conference on machine learning}, pages
  233--242. PMLR.

\bibitem[Englesson and Azizpour, 2021]{gjs}
Englesson, E. and Azizpour, H. (2021).
\newblock {Generalized Jensen-Shannon divergence loss for learning with noisy
  labels}.
\newblock {\em Advances in Neural Information Processing Systems},
  34:30284--30297.

\bibitem[Feng et~al., 2021]{knn}
Feng, C., Tzimiropoulos, G., and Patras, I. (2021).
\newblock {S3:} supervised self-supervised learning under label noise.
\newblock {\em CoRR}, abs/2111.11288.

\bibitem[Ghosh and Kumar, 2017]{robust_theory}
Ghosh, A. and Kumar, H. (2017).
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~31.

\bibitem[Goldberger and Ben-Reuven, 2016]{noiseAdapt}
Goldberger, J. and Ben-Reuven, E. (2016).
\newblock Training deep neural-networks using a noise adaptation layer.

\bibitem[Han et~al., 2018]{coteaching}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
  (2018).
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Hendrycks et~al., 2018]{trustedData}
Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. (2018).
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Ishida et~al., 2020]{flooding}
Ishida, T., Yamane, I., Sakai, T., Niu, G., and Sugiyama, M. (2020).
\newblock Do we need zero training loss after achieving zero training error?
\newblock In {\em International Conference on Machine Learning}, pages
  4604--4614. PMLR.

\bibitem[Janocha and Czarnecki, 2016]{janocha2017loss}
Janocha, K. and Czarnecki, W.~M. (2016).
\newblock On loss functions for deep neural networks in classification.
\newblock {\em Schedae Informaticae}, 25:49.

\bibitem[Jiang et~al., 2018]{mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L. (2018).
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In {\em International conference on machine learning}, pages
  2304--2313. PMLR.

\bibitem[Kim et~al., 2021]{eigenfine}
Kim, T., Ko, J., Choi, J., Yun, S.-Y., et~al. (2021).
\newblock Fine samples for learning with noisy labels.
\newblock {\em Advances in Neural Information Processing Systems},
  34:24137--24149.

\bibitem[Larsen et~al., 1998]{larsen}
Larsen, J., Nonboe, L., Hintz-Madsen, M., and Hansen, L. (1998).
\newblock Design of robust neural network classifiers.
\newblock In {\em Proceedings of the 1998 IEEE International Conference on
  Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)},
  volume~2, pages 1205--1208 vol.2.

\bibitem[Li et~al., 2020]{dividemix}
Li, J., Socher, R., and Hoi, S.~C. (2020).
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock {\em arXiv preprint arXiv:2002.07394}.

\bibitem[Li et~al., 2023]{meta_dynamic}
Li, X.-C., Xia, X., Zhu, F., Liu, T., yao Zhang, X., and lin Liu, C. (2023).
\newblock Dynamic loss for learning with label noise.

\bibitem[Liu et~al., 2020]{elr}
Liu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C. (2020).
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock {\em Advances in neural information processing systems},
  33:20331--20342.

\bibitem[Ma et~al., 2020]{normalised_losses}
Ma, X., Huang, H., Wang, Y., Erfani, S. R.~S., and Bailey, J. (2020).
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, ICML'20. JMLR.org.

\bibitem[Malach and Shalev-Shwartz, 2017]{decoupling}
Malach, E. and Shalev-Shwartz, S. (2017).
\newblock Decoupling" when to update" from" how to update".
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Mnih and Hinton, 2012]{mnihHinton}
Mnih, V. and Hinton, G.~E. (2012).
\newblock Learning to label aerial images from noisy data.
\newblock In {\em Proceedings of the 29th International conference on machine
  learning (ICML-12)}, pages 567--574.

\bibitem[Patrini et~al., 2017]{fprop}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L. (2017).
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1944--1952.

\bibitem[Reed et~al., 2014]{bootstrap}
Reed, S.~E., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
  (2014).
\newblock Training deep neural networks on noisy labels with bootstrapping.

\bibitem[Ren et~al., 2018]{meta_gradient}
Ren, M., Zeng, W., Yang, B., and Urtasun, R. (2018).
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em International conference on machine learning}, pages
  4334--4343. PMLR.

\bibitem[Savage, 1971]{savagen}
Savage, L.~J. (1971).
\newblock Elicitation of personal probabilities and expectations.
\newblock {\em Journal of the American Statistical Association},
  66(336):783--801.

\bibitem[Song et~al., 2019]{sefie}
Song, H., Kim, M., and Lee, J.-G. (2019).
\newblock Selfie: Refurbishing unclean samples for robust deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5907--5915. PMLR.

\bibitem[Sukhbaatar et~al., 2015]{fprop_old}
Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., and Fergus, R. (2015).
\newblock Training convolutional networks with noisy labels.
\newblock In {\em 3rd International Conference on Learning Representations,
  ICLR 2015}.

\bibitem[Szegedy et~al., 2016]{labelSmoothing}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016).
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 2818--2826.

\bibitem[Tanaka et~al., 2018]{tanaka}
Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K. (2018).
\newblock Joint optimization framework for learning with noisy labels.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5552--5560.

\bibitem[Thiel, 2008]{soft_labels}
Thiel, C. (2008).
\newblock Classification on soft labels is robust against label noise.
\newblock In {\em International Conference on Knowledge-Based and Intelligent
  Information and Engineering Systems}, pages 65--73. Springer.

\bibitem[Wang et~al., 2019]{sce}
Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., and Bailey, J. (2019).
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In {\em 2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 322--330, Los Alamitos, CA, USA. IEEE Computer Society.

\bibitem[Zhang et~al., 2017]{mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D. (2017).
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv e-prints}, pages arXiv--1710.

\bibitem[Zhang and Sabuncu, 2018]{GCE_Loss}
Zhang, Z. and Sabuncu, M. (2018).
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Zhou et~al., 2020]{curiculum}
Zhou, T., Wang, S., and Bilmes, J. (2020).
\newblock Robust curriculum learning: from clean label detection to noisy label
  self-correction.
\newblock In {\em International Conference on Learning Representations}.

\end{thebibliography}
