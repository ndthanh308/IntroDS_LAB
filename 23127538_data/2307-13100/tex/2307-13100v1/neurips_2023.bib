
@inproceedings{fprop,
  title={Making Deep Neural Networks Robust To Label Noise: A Loss Correction Approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1944--1952},
  year={2017}
}



@article{trustedData,
  title={Using Trusted Data To Train Deep Networks On Labels Corrupted By Severe Noise},
  author={Hendrycks, Dan and Mazeika, Mantas and Wilson, Duncan and Gimpel, Kevin},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{mnihHinton,
  title={Learning To Label Aerial Images from Noisy Data},
  author={Mnih, Volodymyr and Hinton, Geoffrey E},
  booktitle={Proceedings of the 29th International conference on machine learning (ICML-12)},
  pages={567--574},
  year={2012}
}

@article{noiseAdapt,
  title={Training Deep Neural-Networks Using A Noise Adaptation Layer},
  author={Goldberger, Jacob and Ben-Reuven, Ehud},
  year={2016}
}

@INPROCEEDINGS{larsen,

  author={Larsen, J. and Nonboe, L. and Hintz-Madsen, M. and Hansen, L.K.},

  booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)}, 

  title={Design of Robust Neural Network Classifiers}, 

  year={1998},

  volume={2},

  number={},

  pages={1205-1208 vol.2},

  doi={10.1109/ICASSP.1998.675487}}



@article{bitempered,
  title={Robust Bi-Tempered Logistic Loss Based on Bregman Divergences},
  author={Amid, Ehsan and Warmuth, Manfred KK and Anil, Rohan and Koren, Tomer},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{properBregman,
  title={Proper Scoring Rules and Bregman Divergences},
  author={Ovcharov, Evgeni Y},
  journal={Bernoulli},
  volume={24},
  number={1},
  pages={53--79},
  year={2018},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{decoupling,
  title={Decoupling" When to Update" From" How to Update"},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@INPROCEEDINGS{labelSmoothing,

  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Rethinking the Inception Architecture for Computer Vision}, 

  year={2016},

  volume={},

  number={},

  pages={2818-2826},

  doi={10.1109/CVPR.2016.308}}

@article{eigenfine,
  title={Fine samples for Learning with Noisy Labels},
  author={Kim, Taehyeon and Ko, Jongwoo and Choi, JinHwan and Yun, Se-Young and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24137--24149},
  year={2021}
}

@incollection{survey2020,
  title={Impact of Noisy Labels in Learning Techniques: a Survey},
  author={Nigam, Nitika and Dutta, Tanima and Gupta, Hari Prabhat},
  booktitle={Advances in data and information sciences},
  pages={403--411},
  year={2020},
  publisher={Springer}
}

@inproceedings{soft_labels,
  title={Classification on Soft Labels is Robust Against Label Noise},
  author={Thiel, Christian},
  booktitle={International Conference on Knowledge-Based and Intelligent Information and Engineering Systems},
  pages={65--73},
  year={2008},
  organization={Springer}
}

@article{GCE_Loss,
  title={Generalized Cross Entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{CalibrationSurvey,
  author    = {Telmo de Menezes e Silva Filho and
               Hao Song and
               Miquel Perell{\'{o}}{-}Nieto and
               Ra{\'{u}}l Santos{-}Rodr{\'{\i}}guez and
               Meelis Kull and
               Peter A. Flach},
  title     = {Classifier Calibration: How to Assess and Improve Predicted Class
               Probabilities: a Survey},
  journal   = {CoRR},
  volume    = {abs/2112.10327},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.10327},
  eprinttype = {arXiv},
  eprint    = {2112.10327},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-10327.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{universalKernels,
author = {Micchelli, Charles and Xu, Yuesheng and Zhang, Haizhang},
year = {2006},
month = {12},
pages = {},
title = {Universal Kernels},
volume = {7},
journal = {Mathematics}
}


@article{gjs,
  title={{Generalized Jensen-Shannon divergence loss for learning with noisy labels}},
  author={Englesson, Erik and Azizpour, Hossein},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30284--30297},
  year={2021}
}


@inproceedings{robust_theory,
  title={Robust loss functions under label noise for deep neural networks},
  author={Ghosh, Aritra and Kumar, Himanshu},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{normalised_losses,
author = {Ma, Xingjun and Huang, Hanxun and Wang, Yisen and Erfani, Simone Romano Sarah and Bailey, James},
title = {Normalized Loss Functions for Deep Learning with Noisy Labels},
year = {2020},
publisher = {JMLR.org},
abstract = {Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {607},
numpages = {11},
series = {ICML'20}
}

@INPROCEEDINGS {sce,
author = {Y. Wang and X. Ma and Z. Chen and Y. Luo and J. Yi and J. Bailey},
booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
title = {Symmetric Cross Entropy for Robust Learning With Noisy Labels},
year = {2019},
volume = {},
issn = {},
pages = {322-330},
abstract = {Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (&quot;easy&quot; classes), but more surprisingly, it also suffers from significant under learning on some other classes (&quot;hard&quot; classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.},
keywords = {noise measurement;training;entropy;neural networks;robustness;artificial intelligence;task analysis},
doi = {10.1109/ICCV.2019.00041},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00041},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov}
}

@article{cifar,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@article{deng2012mnist,
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [best of the web]},
  author={Deng, Li},
  journal={IEEE signal processing magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@inproceedings{flooding,
  title={Do We Need Zero Training Loss After Achieving Zero Training Error?},
  author={Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={4604--4614},
  year={2020},
  organization={PMLR}
}


@inproceedings{Taylor_c,
author = {Feng, Lei and Shu, Senlin and Lin, Zhuoyi and Lv, Fengmao and Li, Li and An, Bo},
title = {Can Cross Entropy Loss Be Robust to Label Noise?},
year = {2021},
isbn = {9780999241165},
abstract = {Trained with the standard cross entropy loss, deep neural networks can achieve great performance on correctly labeled data. However, if the training data is corrupted with label noise, deep models tend to overfit the noisy labels, thereby achieving poor generation performance. To remedy this issue, several loss functions have been proposed and demonstrated to be robust to label noise. Although most of the robust loss functions stem from Categorical Cross Entropy (CCE) loss, they fail to embody the intrinsic relationships between CCE and other loss functions. In this paper, we propose a general framework dubbed Taylor cross entropy loss to train deep models in the presence of label noise. Specifically, our framework enables to weight the extent of fitting the training labels by controlling the order of Taylor Series for CCE, hence it can be robust to label noise. In addition, our framework clearly reveals the intrinsic relationships between CCE and other loss functions, such as Mean Absolute Error (MAE) and Mean Squared Error (MSE). Moreover, we present a detailed theoretical analysis to certify the robustness of this framework. Extensive experimental results on benchmark datasets demonstrate that our proposed approach significantly outperforms the state-of-the-art counterparts.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {305},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{xiao2017fashion,
  title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@inproceedings{cohen2017emnist,
  title={EMNIST: Extending MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}

@article{active,
  title={Active bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples},
  author={Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{elr,
  title={Early-learning Regularization Prevents Memorization of Noisy Labels},
  author={Liu, Sheng and Niles-Weed, Jonathan and Razavian, Narges and Fernandez-Granda, Carlos},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20331--20342},
  year={2020}
}

@inproceedings{sefie,
  title={Selfie: Refurbishing Unclean Samples for Robust Deep Learning},
  author={Song, Hwanjun and Kim, Minseok and Lee, Jae-Gil},
  booktitle={International Conference on Machine Learning},
  pages={5907--5915},
  year={2019},
  organization={PMLR}
}

@misc{meta_dynamic,
title={Dynamic Loss for Learning with Label Noise},
author={Xiu-Chuan Li and Xiaobo Xia and Fei Zhu and Tongliang Liu and Xu-yao Zhang and Cheng-lin Liu},
year={2023},
url={https://openreview.net/forum?id=J_kUIC1DNHJ}
}

@article{pred_var,
  title={Active bias: Training more Accurate Neural Networks by Emphasizing High Variance Samples},
  author={Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{secost,
  title={Secost: Sequential Co-Supervision for Weakly Labeled Audio Event Detection},
  author={Kumar, Anurag and Ithapu, Vamsi Krishna},
  year={2020}
}

@inproceedings{memorisation,
  title={A Closer Look At Memorization in Deep Networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}

@inproceedings{meta_gradient,
  title={Learning to Reweight Examples for Robust Deep Learning},
  author={Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  booktitle={International conference on machine learning},
  pages={4334--4343},
  year={2018},
  organization={PMLR}
}

@inproceedings{curiculum,
  title={Robust Curriculum Learning: from Clean Label Detection to Noisy Label Self-Correction},
  author={Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{cleanLab,
  title={Confident Learning: Estimating Uncertainty in Dataset Labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}

@article{savagen,
  title={Elicitation of Personal Probabilities and Expectations},
  author={Savage, Leonard J},
  journal={Journal of the American Statistical Association},
  volume={66},
  number={336},
  pages={783--801},
  year={1971},
  publisher={Taylor \& Francis}
}

@inproceedings{neighbour,
  title={Learning with Neighbor Consistency for Noisy Labels},
  author={Iscen, Ahmet and Valmadre, Jack and Arnab, Anurag and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4672--4681},
  year={2022}
}




@article{KNN,
  author    = {Chen Feng and
               Georgios Tzimiropoulos and
               Ioannis Patras},
  title     = {{S3:} Supervised Self-supervised Learning under Label Noise},
  journal   = {CoRR},
  volume    = {abs/2111.11288},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.11288},
  eprinttype = {arXiv},
  eprint    = {2111.11288},
  timestamp = {Fri, 26 Nov 2021 13:48:43 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-11288.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tanaka,
  title={Joint Optimization Framework for Learning with Noisy Labels},
  author={Tanaka, Daiki and Ikami, Daiki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5552--5560},
  year={2018}
}


@inproceedings{bmm,
  title={Unsupervised Label Noise Modeling and Loss Correction},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and Oâ€™Connor, Noel and McGuinness, Kevin},
  booktitle={International conference on machine learning},
  pages={312--321},
  year={2019},
  organization={PMLR}
}

@inproceedings{mentornet,
  title={Mentornet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International conference on machine learning},
  pages={2304--2313},
  year={2018},
  organization={PMLR}
}
@article{coteaching,
  title={Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels},
  author={Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{dividemix,
  title={Dividemix: Learning with Noisy Labels as Semi-Supervised Learning},
  author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2002.07394},
  year={2020}
}


@inproceedings{splitSelect,
  title={Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019},
  organization={PMLR}
}

@misc{truncated_github,
  author = {Alan Chou},
  title = {Truncated Losses},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AlanChou/Truncated-Loss/}},
  commit = {93a2464}
}



@misc{gjs_github,
  author = {Erik Englesson},
  title = {Truncated Losses},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/ErikEnglesson/GJS/}},
  commit = {}
} 



@article{metaLabels,
  title={Learning Soft Labels Via Meta Learning},
  author={Vyas, Nidhi and Saxena, Shreyas and Voice, Thomas},
  journal={arXiv preprint arXiv:2009.09496},
  year={2020}
}

@inproceedings{fprop_old,
  title={Training convolutional networks with noisy labels},
  author={Sukhbaatar, Sainbayar and Bruna, Joan and Paluri, Manohar and Bourdev, Lubomir and Fergus, Rob},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015}
}


@article{metaLabelCorrect,
  author    = {Guoqing Zheng and
               Ahmed Hassan Awadallah and
               Susan T. Dumais},
  title     = {Meta Label Correction for Learning with Weak Supervision},
  journal   = {CoRR},
  volume    = {abs/1911.03809},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.03809},
  eprinttype = {arXiv},
  eprint    = {1911.03809},
  timestamp = {Sun, 01 Dec 2019 20:31:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-03809.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{janocha2017loss,
  title={On Loss Functions for Deep Neural Networks in Classification},
  author={Janocha, Katarzyna and Czarnecki, Wojciech Marian},
  journal={Schedae Informaticae},
  volume={25},
  pages={49},
  year={2016},
  publisher={Jagiellonian University-Jagiellonian University Press}
}

@article{robustLR,
  title={Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning with Label Noise},
  author={Chen, Mingcai and Cheng, Hao and Du, Yuntao and Xu, Ming and Jiang, Wenyu and Wang, Chongjun},
  journal={arXiv preprint arXiv:2112.02960},
  year={2021}
}


@article{entropyReg,
  title={Regularizing Neural Networks by Penalizing Confident Output Distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}

@article{mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv e-prints},
  pages={arXiv--1710},
  year={2017}
}

@article{bootstrap,
  title={TRAINING DEEP NEURAL NETWORKS ON NOISY LABELS WITH BOOTSTRAPPING},
  author={Reed, Scott E and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
  publisher={Citeseer},
  year={2014}
}



