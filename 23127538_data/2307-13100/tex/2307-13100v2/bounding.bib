@ARTICLE{bigOlderSurvey,

  author={Frenay, Benoit and Verleysen, Michel},

  journal={IEEE Transactions on Neural Networks and Learning Systems}, 

  title={Classification in the Presence of Label Noise: A Survey}, 

  year={2014},

  volume={25},

  number={5},

  pages={845-869},

  keywords={Noise;Labeling;Training;Noise measurement;Taxonomy;Reliability;Context;Class noise;classification;label noise;mislabeling;robust methods;survey.;Class noise;classification;label noise;mislabeling;robust methods;survey},

  doi={10.1109/TNNLS.2013.2292894}
}

@article{algan2021survey,
  title={Image classification with deep learning in the presence of noisy labels: A survey},
  author={Algan, G{\"o}rkem and Ulusoy, Ilkay},
  journal={Knowledge-Based Systems},
  volume={215},
  pages={106771},
  year={2021},
  publisher={Elsevier}
}


@article{Sugiyama2020survey,
  title={A survey of label-noise representation learning: Past, present and future},
  author={Han, Bo and Yao, Quanming and Liu, Tongliang and Niu, Gang and Tsang, Ivor W and Kwok, James T and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2011.04406},
  year={2020}
}


@article{johnson2022survey,
  title={A survey on classifying big data with label noise},
  author={Johnson, Justin M and Khoshgoftaar, Taghi M},
  journal={ACM Journal of Data and Information Quality},
  volume={14},
  number={4},
  pages={1--43},
  year={2022},
  publisher={ACM New York, NY}
}


@inproceedings{annotateNoise,
  title={Learning with annotation noise},
  author={Beigman, Eyal and Klebanov, Beata Beigman},
  booktitle={Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP},
  pages={280--287},
  year={2009}
}


@inproceedings{fprop,
  title={Making Deep Neural Networks Robust To Label Noise: A Loss Correction Approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1944--1952},
  year={2017}
}

@article{wei2021open,
  title={Open-set label noise can improve robustness against inherent label noise},
  author={Wei, Hongxin and Tao, Lue and Xie, Renchunzi and An, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7978--7992},
  year={2021}
}


@inproceedings{multi-label,
  title={Re-labeling imagenet: from single to multi-labels, from global to localized labels},
  author={Yun, Sangdoo and Oh, Seong Joon and Heo, Byeongho and Han, Dongyoon and Choe, Junsuk and Chun, Sanghyuk},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2340--2350},
  year={2021}
}

@ARTICLE{ExtendedT,

  author={Xia, Xiaobo and Han, Bo and Wang, Nannan and Deng, Jiankang and Li, Jiatong and Mao, Yinian and Liu, Tongliang},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Extended $T$T: Learning With Mixed Closed-Set and Open-Set Noisy Labels}, 

  year={2023},

  volume={45},

  number={3},

  pages={3047-3058},

  keywords={Noise measurement;Training;Training data;Data models;Search engines;Face recognition;Computer science;Noise transition matrix;mixed noisy labels;instance-dependent label noise;deep clustering;robustness},

  doi={10.1109/TPAMI.2022.3180545}}


@ARTICLE{tigerWoods,
  author={Feng, Chen and Tzimiropoulos, Georgios and Patras, Ioannis},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={NoiseBox: Towards More Efficient and Effective Learning with Noisy Labels}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  keywords={Noise;Noise measurement;Training;Computational modeling;Predictive models;Supervised learning;Entropy;Noisy labels;Sample selection;K-nearest neighbours;Feature consistency;Class imbalance},
  doi={10.1109/TCSVT.2024.3426994}}



@InProceedings{open_set,
  title = 	 {Robust Inference via Generative Classifiers for Handling Noisy Labels},
  author =       {Lee, Kimin and Yun, Sukmin and Lee, Kibok and Lee, Honglak and Li, Bo and Shin, Jinwoo},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3763--3772},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/lee19f/lee19f.pdf},
  url = 	 {https://proceedings.mlr.press/v97/lee19f.html},
  abstract = 	 {Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy training datasets. To mitigate the issue, we propose a novel inference method, termed Robust Generative classifier (RoG), applicable to any discriminative (e.g., softmax) neural classifier pre-trained on noisy datasets. In particular, we induce a generative classifier on top of hidden feature spaces of the pre-trained DNNs, for obtaining a more robust decision boundary. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy with neither re-training of the deep model nor changing its architectures. With the assumption of Gaussian distribution for features, we prove that RoG generalizes better than baselines under noisy labels. Finally, we propose the ensemble version of RoG to improve its performance by investigating the layer-wise characteristics of DNNs. Our extensive experimental results demonstrate the superiority of RoG given different learning models optimized by several training techniques to handle diverse scenarios of noisy labels.}
}


@InProceedings{MassiveData,
author = {Xiao, Tong and Xia, Tian and Yang, Yi and Huang, Chang and Wang, Xiaogang},
title = {Learning From Massive Noisy Labeled Data for Image Classification},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@InProceedings{dimDriven,
  title = 	 {Dimensionality-Driven Learning with Noisy Labels},
  author =       {Ma, Xingjun and Wang, Yisen and Houle, Michael E. and Zhou, Shuo and Erfani, Sarah and Xia, Shutao and Wijewickrema, Sudanthi and Bailey, James},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3355--3364},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/ma18d/ma18d.pdf},
  url = 	 {https://proceedings.mlr.press/v80/ma18d.html}
}

@InProceedings{AugmentNLN,
    author    = {Nishi, Kento and Ding, Yi and Rich, Alex and Hollerer, Tobias},
    title     = {Augmentation Strategies for Learning With Noisy Labels},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8022-8031}
}

@InProceedings{EvidenceMix,
    author    = {Sachdeva, Ragav and Cordeiro, Filipe R. and Belagiannis, Vasileios and Reid, Ian and Carneiro, Gustavo},
    title     = {EvidentialMix: Learning With Combined Open-Set and Closed-Set Noisy Labels},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2021},
    pages     = {3607-3615}
}

@inproceedings{cubuk2019autoaugment,
  title={Autoaugment: Learning augmentation strategies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={113--123},
  year={2019}
}


@INPROCEEDINGS{noisyAugmentStudy,

  author={Pereira, Emeson and Carneiro, Gustavo and Cordeiro, Filipe R.},

  booktitle={2022 35th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)}, 

  title={A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels}, 

  year={2022},

  volume={1},

  number={},

  pages={25-30},

  keywords={Training;Deep learning;Graphics;Analytical models;Neural networks;Robustness;Data models;label noise;deep learning;classification},

  doi={10.1109/SIBGRAPI55357.2022.9991791}}



@InProceedings{identTransition,
  title = 	 {Identifiability of Label Noise Transition Matrix},
  author =       {Liu, Yang and Cheng, Hao and Zhang, Kun},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {21475--21496},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/liu23g/liu23g.pdf},
  url = 	 {https://proceedings.mlr.press/v202/liu23g.html},
  abstract = 	 {The noise transition matrix plays a central role in the problem of learning with noisy labels. Among many other reasons, a large number of existing solutions rely on the knowledge of it. Identifying and estimating the transition matrix without ground truth labels is a critical and challenging task. When label noise transition depends on each instance, the problem of identifying the instance-dependent noise transition matrix becomes substantially more challenging. Despite recently proposed solutions for learning from instance-dependent noisy labels, the literature lacks a unified understanding of when such a problem remains identifiable. The goal of this paper is to characterize the identifiability of the label noise transition matrix. Building on Kruskal’s identifiability results, we are able to show the necessity of multiple noisy labels in identifying the noise transition matrix at the instance level. We further instantiate the results to explain the successes of the state-of-the-art solutions and how additional assumptions alleviated the requirement of multiple noisy labels. Our result reveals that disentangled features improve identification. This discovery led us to an approach that improves the estimation of the transition matrix using properly disentangled features. Code is available at https://github.com/UCSC-REAL/Identifiability.}
}


@INPROCEEDINGS{unknownClassCleaning,

  author={Yu, Qing and Aizawa, Kiyoharu},

  booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, 

  title={Unknown Class Label Cleaning For Learning With Open-Set Noisy Labels}, 

  year={2020},

  volume={},

  number={},

  pages={1731-1735},

  keywords={Noise measurement;Training;Training data;Cleaning;Optimization;Neural networks;Robustness;Noisy label;label cleaning;open-set image classification},

  doi={10.1109/ICIP40778.2020.9190652}}

@InProceedings{webly,
author = {Chen, Xinlei and Gupta, Abhinav},
title = {Webly Supervised Learning of Convolutional Networks},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
} 

@InProceedings{IterativeOpenSet,
author = {Wang, Yisen and Liu, Weiyang and Ma, Xingjun and Bailey, James and Zha, Hongyuan and Song, Le and Xia, Shu-Tao},
title = {Iterative Learning With Open-Set Noisy Labels},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@article{jost2006entropy,
  title={Entropy and diversity},
  author={Jost, Lou},
  journal={Oikos},
  volume={113},
  number={2},
  pages={363--375},
  year={2006},
  publisher={Wiley Online Library}
}

@article{xu2019l_dmi,
  title={L\_dmi: A novel information-theoretic loss function for training deep nets robust to label noise},
  author={Xu, Yilun and Cao, Peng and Kong, Yuqing and Wang, Yizhou},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}



@article{botta2005rao,
  title={Rao's quadratic entropy as a measure of functional diversity based on multiple traits},
  author={Botta-Duk{\'a}t, Zolt{\'a}n},
  journal={Journal of vegetation science},
  volume={16},
  number={5},
  pages={533--540},
  year={2005},
  publisher={Wiley Online Library}
}



@article{trustedData,
  title={Using Trusted Data To Train Deep Networks On Labels Corrupted By Severe Noise},
  author={Hendrycks, Dan and Mazeika, Mantas and Wilson, Duncan and Gimpel, Kevin},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{wolpert1997noFreeLunch,
  author={Wolpert, D.H. and Macready, W.G.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={No free lunch theorems for optimization}, 
  year={1997},
  volume={1},
  number={1},
  pages={67-82},
  keywords={Iron;Evolutionary computation;Information theory;Minimax techniques;Simulated annealing;Algorithm design and analysis;Performance analysis;Probability distribution;Bayesian methods},
  doi={10.1109/4235.585893}}



@inproceedings{mnihHinton,
  title={Learning To Label Aerial Images from Noisy Data},
  author={Mnih, Volodymyr and Hinton, Geoffrey E},
  booktitle={Proceedings of the 29th International conference on machine learning (ICML-12)},
  pages={567--574},
  year={2012}
}

@article{guo2022learning,
  title={Learning to re-weight examples with optimal transport for imbalanced classification},
  author={Guo, Dandan and Li, Zhuo and Zhao, He and Zhou, Mingyuan and Zha, Hongyuan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25517--25530},
  year={2022}
}

@article{bar2021multiplicative,
  title={Multiplicative reweighting for robust neural network optimization},
  author={Bar, Noga and Koren, Tomer and Giryes, Raja},
  journal={arXiv preprint arXiv:2102.12192},
  year={2021}
}

@article{noiseAdapt2,
  title={Learning from noisy labels with deep neural networks},
  author={Sukhbaatar, Sainbayar and Fergus, Rob},
  journal={arXiv preprint arXiv:1406.2080},
  volume={2},
  number={3},
  pages={4},
  year={2014},
  publisher={Citeseer}
}

@inproceedings{noiseAdapt,
  title={Training Deep Neural-Networks Using a Noise Adaptation Layer},
  author={Goldberger, Jacob and Ben-Reuven, Ehud},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2022},
  organization={ICLR},
  url={https://openreview.net/forum?id=H12GRgcxg}
}


@INPROCEEDINGS{larsen,

  author={Larsen, J. and Nonboe, L. and Hintz-Madsen, M. and Hansen, L.K.},

  booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)}, 

  title={Design of Robust Neural Network Classifiers}, 

  year={1998},

  volume={2},

  number={},

  pages={1205-1208 vol.2},

  doi={10.1109/ICASSP.1998.675487}}



@article{bitempered,
  title={Robust Bi-Tempered Logistic Loss Based on Bregman Divergences},
  author={Amid, Ehsan and Warmuth, Manfred KK and Anil, Rohan and Koren, Tomer},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{englesson2021consistency,
  title={Consistency regularization can improve robustness to label noise},
  author={Englesson, Erik and Azizpour, Hossein},
  journal={arXiv preprint arXiv:2110.01242},
  year={2021}
}



@InProceedings{peer,
  title = 	 {Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates},
  author =       {Liu, Yang and Guo, Hongyi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6226--6236},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/liu20e/liu20e.pdf},
  url = 	 {https://proceedings.mlr.press/v119/liu20e.html},
  abstract = 	 {Learning with noisy labels is a common challenge in supervised learning. Existing approaches often require practitioners to specify noise rates, i.e., a set of parameters controlling the severity of label noises in the problem, and the specifications are either assumed to be given or estimated using additional steps. In this work, we introduce a new family of loss functions that we name as peer loss functions, which enables learning from noisy labels and does not require a priori specification of the noise rates. Peer loss functions work within the standard empirical risk minimization (ERM) framework. We show that, under mild conditions, performing ERM with peer loss functions on the noisy data leads to the optimal or a near-optimal classifier as if performing ERM over the clean training data, which we do not have access to. We pair our results with an extensive set of experiments. Peer loss provides a way to simplify model development when facing potentially noisy training labels, and can be promoted as a robust candidate loss function in such situations.}
}



@article{properBregman,
  title={Proper Scoring Rules and Bregman Divergences},
  author={Ovcharov, Evgeni Y},
  journal={Bernoulli},
  volume={24},
  number={1},
  pages={53--79},
  year={2018},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{decoupling,
  title={Decoupling" When to Update" From" How to Update"},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@INPROCEEDINGS{labelSmoothing,

  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Rethinking the Inception Architecture for Computer Vision}, 

  year={2016},

  volume={},

  number={},

  pages={2818-2826},

  doi={10.1109/CVPR.2016.308}}

@article{eigenfine,
  title={Fine samples for Learning with Noisy Labels},
  author={Kim, Taehyeon and Ko, Jongwoo and Choi, JinHwan and Yun, Se-Young and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24137--24149},
  year={2021}
}



@inproceedings{soft_labels,
  title={Classification on Soft Labels is Robust Against Label Noise},
  author={Thiel, Christian},
  booktitle={International Conference on Knowledge-Based and Intelligent Information and Engineering Systems},
  pages={65--73},
  year={2008},
  organization={Springer}
}

@article{GCE_Loss,
  title={Generalized Cross Entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}



@article{universalKernels,
author = {Micchelli, Charles and Xu, Yuesheng and Zhang, Haizhang},
year = {2006},
month = {12},
pages = {},
title = {Universal Kernels},
volume = {7},
journal = {Mathematics}
}


@article{gjs,
  title={{Generalized Jensen-Shannon divergence loss for learning with noisy labels}},
  author={Englesson, Erik and Azizpour, Hossein},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30284--30297},
  year={2021}
}

@article{ghosh2015making,
  title={Making risk minimization tolerant to label noise},
  author={Ghosh, Aritra and Manwani, Naresh and Sastry, PS},
  journal={Neurocomputing},
  volume={160},
  pages={93--107},
  year={2015},
  publisher={Elsevier}
}


@inproceedings{robust_theory,
  title={Robust loss functions under label noise for deep neural networks},
  author={Ghosh, Aritra and Kumar, Himanshu},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  year={2017}
}

@inproceedings{normalised_losses,
author = {Ma, Xingjun and Huang, Hanxun and Wang, Yisen and Erfani, Simone Romano Sarah and Bailey, James},
title = {Normalized Loss Functions for Deep Learning with Noisy Labels},
year = {2020},
publisher = {JMLR.org},
abstract = {Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {607},
numpages = {11},
series = {ICML'20}
}

@INPROCEEDINGS {sce,
author = {Y. Wang and X. Ma and Z. Chen and Y. Luo and J. Yi and J. Bailey},
booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
title = {Symmetric Cross Entropy for Robust Learning With Noisy Labels},
year = {2019},
volume = {},
issn = {},
pages = {322-330},
abstract = {Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (&quot;easy&quot; classes), but more surprisingly, it also suffers from significant under learning on some other classes (&quot;hard&quot; classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.},
keywords = {noise measurement;training;entropy;neural networks;robustness;artificial intelligence;task analysis},
doi = {10.1109/ICCV.2019.00041},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00041},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov}
}

@article{cifar,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@article{deng2012mnist,
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [best of the web]},
  author={Deng, Li},
  journal={IEEE signal processing magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@inproceedings{flooding,
  title={Do We Need Zero Training Loss After Achieving Zero Training Error?},
  author={Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={4604--4614},
  year={2020},
  organization={PMLR}
}



@article{xiao2017fashion,
  title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@inproceedings{cohen2017emnist,
  title={EMNIST: Extending MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}


@inproceedings{sefie,
  title={Selfie: Refurbishing Unclean Samples for Robust Deep Learning},
  author={Song, Hwanjun and Kim, Minseok and Lee, Jae-Gil},
  booktitle={International Conference on Machine Learning},
  pages={5907--5915},
  year={2019},
  organization={PMLR}
}

@misc{meta_dynamic,
title={Dynamic Loss for Learning with Label Noise},
author={Xiu-Chuan Li and Xiaobo Xia and Fei Zhu and Tongliang Liu and Xu-yao Zhang and Cheng-lin Liu},
year={2023},
url={https://openreview.net/forum?id=J_kUIC1DNHJ}
}

@article{secost,
  title={Secost: Sequential Co-Supervision for Weakly Labeled Audio Event Detection},
  author={Kumar, Anurag and Ithapu, Vamsi Krishna},
  year={2020}
}

@inproceedings{memorisation,
  title={A Closer Look At Memorization in Deep Networks},
  author={Arpit, Devansh and Jastrz\k{e}bski, Stanis\l{}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  year={2017},
  organization={PMLR}
}

@article{dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}



@inproceedings{meta_gradient,
  title={Learning to Reweight Examples for Robust Deep Learning},
  author={Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  booktitle={International conference on machine learning},
  pages={4334--4343},
  year={2018},
  organization={PMLR}
}

@inproceedings{curiculum,
  title={Robust Curriculum Learning: from Clean Label Detection to Noisy Label Self-Correction},
  author={Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{cleanLab,
  title={Confident Learning: Estimating Uncertainty in Dataset Labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}

@article{savagen,
  title={Elicitation of Personal Probabilities and Expectations},
  author={Savage, Leonard J},
  journal={Journal of the American Statistical Association},
  volume={66},
  number={336},
  pages={783--801},
  year={1971},
  publisher={Taylor \& Francis}
}



@article{KNN,
  author    = {Chen Feng and
               Georgios Tzimiropoulos and
               Ioannis Patras},
  title     = {{S3:} Supervised Self-supervised Learning under Label Noise},
  journal   = {CoRR},
  volume    = {abs/2111.11288},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.11288},
  eprinttype = {arXiv},
  eprint    = {2111.11288},
  timestamp = {Fri, 26 Nov 2021 13:48:43 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-11288.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tanaka,
  title={Joint Optimization Framework for Learning with Noisy Labels},
  author={Tanaka, Daiki and Ikami, Daiki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5552--5560},
  year={2018}
}


@inproceedings{bmm,
  title={Unsupervised Label Noise Modeling and Loss Correction},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and O’Connor, Noel and McGuinness, Kevin},
  booktitle={International conference on machine learning},
  pages={312--321},
  year={2019},
  organization={PMLR}
}

@inproceedings{mentornet,
  title={Mentornet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International conference on machine learning},
  pages={2304--2313},
  year={2018},
  organization={PMLR}
}
@article{coteaching,
  title={Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels},
  author={Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{dividemix,
  title={Dividemix: Learning with Noisy Labels as Semi-Supervised Learning},
  author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2002.07394},
  year={2020}
}


@inproceedings{splitSelect,
  title={Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019},
  organization={PMLR}
}

@misc{truncated_github,
  author = {Alan Chou},
  title = {Truncated Losses},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AlanChou/Truncated-Loss/}},
  commit = {93a2464}
}



@inproceedings{fprop_old,
  title={Training convolutional networks with noisy labels},
  author={Sukhbaatar, Sainbayar and Bruna, Joan and Paluri, Manohar and Bourdev, Lubomir and Fergus, Rob},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015}
}


@article{metaLabelCorrect,
  author    = {Guoqing Zheng and
               Ahmed Hassan Awadallah and
               Susan T. Dumais},
  title     = {Meta Label Correction for Learning with Weak Supervision},
  journal   = {CoRR},
  volume    = {abs/1911.03809},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.03809},
  eprinttype = {arXiv},
  eprint    = {1911.03809},
  timestamp = {Sun, 01 Dec 2019 20:31:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-03809.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{janocha2017loss,
  title={On Loss Functions for Deep Neural Networks in Classification},
  author={Janocha, Katarzyna and Czarnecki, Wojciech Marian},
  journal={Schedae Informaticae},
  volume={25},
  pages={49},
  year={2016},
  publisher={Jagiellonian University-Jagiellonian University Press}
}

@article{robustLR,
  title={Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning with Label Noise},
  author={Chen, Mingcai and Cheng, Hao and Du, Yuntao and Xu, Ming and Jiang, Wenyu and Wang, Chongjun},
  journal={arXiv preprint arXiv:2112.02960},
  year={2021}
}


@article{entropyReg,
  title={Regularizing Neural Networks by Penalizing Confident Output Distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}

@article{mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv e-prints},
  pages={arXiv--1710},
  year={2017}
}

@article{bootstrap,
  title={TRAINING DEEP NEURAL NETWORKS ON NOISY LABELS WITH BOOTSTRAPPING},
  author={Reed, Scott E and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
  publisher={Citeseer},
  year={2014}
}



@misc{characteristic,
      title={A Characteristic Function Approach to Deep Implicit Generative Modeling}, 
      author={Abdul Fatir Ansari and Jonathan Scarlett and Harold Soh},
      year={2020},
      eprint={1909.07425},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{IKL,
      title={Implicit Kernel Learning}, 
      author={Chun-Liang Li and Wei-Cheng Chang and Youssef Mroueh and Yiming Yang and Barnabás Póczos},
      year={2019},
      eprint={1902.10214},
      archivePrefix={arXiv},
      primaryClass={stat.ML}


}


@InProceedings{scott13,
  title = 	 {Classification with Asymmetric Label Noise: Consistency and Maximal Denoising},
  author = 	 {Scott, Clayton and Blanchard, Gilles and Handy, Gregory},
  booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
  pages = 	 {489--511},
  year = 	 {2013},
  editor = 	 {Shalev-Shwartz, Shai and Steinwart, Ingo},
  volume = 	 {30},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Princeton, NJ, USA},
  month = 	 {12--14 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v30/Scott13.pdf},
  url = 	 {https://proceedings.mlr.press/v30/Scott13.html},
  abstract = 	 {In many real-world classification problems, the labels of training examples are randomly corrupted. Thus, the set of training examples for each class is contaminated by examples of the other class. Previous theoretical work on this problem assumes that the two classes are separable, that the label noise is independent of the true class label, or that the noise proportions for each class are known. We introduce a general framework for classification with label noise that eliminates these assumptions. Instead, we give assumptions ensuring identifiability and the existence of a consistent estimator of the optimal risk, with associated estimation strategies. For any arbitrary pair of contaminated distributions, there is a unique pair of non-contaminated distributions satisfying the proposed assumptions, and we argue that this solution corresponds in a certain sense to maximal denoising. In particular, we find that learning in the presence of label noise is possible even when the class-conditional distributions overlap and the label noise is not symmetric. A key to our approach is a universally consistent estimator of the maximal proportion of one distribution that is present in another, a problem we refer to as“mixture proportion estimation. This work is motivated by a problem in nuclear particle classification.}
}



@InProceedings{menon,
  title = 	 {Learning from Corrupted Binary Labels via Class-Probability Estimation},
  author = 	 {Menon, Aditya and Rooyen, Brendan Van and Ong, Cheng Soon and Williamson, Bob},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {125--134},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/menon15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/menon15.html},
  abstract = 	 {Many supervised learning problems involve learning from samples whose labels are corrupted in some way. For example, each sample may have some constant probability of being incorrectly labelled (learning with label noise), or one may have a pool of unlabelled samples in lieu of negative samples (learning from positive and unlabelled data). This paper uses class-probability estimation to study these and other corruption processes belonging to the mutually contaminated distributions framework (Scott et al., 2013), with three conclusions. First, one can optimise balanced error and AUC without knowledge of the corruption process parameters. Second, given estimates of the corruption parameters, one can minimise a range of classification risks. Third, one can estimate the corruption parameters using only corrupted data. Experiments confirm the efficacy of class-probability estimation in learning from corrupted labels.}
}


@INPROCEEDINGS{polyPerceptron,

  author={Cohen, E.},

  booktitle={Proceedings 38th Annual Symposium on Foundations of Computer Science}, 

  title={Learning noisy perceptrons by a perceptron in polynomial time}, 

  year={1997},

  volume={},

  number={},

  pages={514-523},

  keywords={Polynomials;Machine learning;Vectors;Probability distribution;Ear;Zinc;Linear programming;Noise level;Noise generators},

  doi={10.1109/SFCS.1997.646140}}

@inproceedings{bylander1994learning,
  title={Learning linear threshold functions in the presence of classification noise},
  author={Bylander, Tom},
  booktitle={Proceedings of the seventh annual conference on Computational learning theory},
  pages={340--347},
  year={1994}
}


@article{multiple_disc,
  author    = {Corentin Hardy and
               Erwan Le Merrer and
               Bruno Sericola},
  title     = {{MD-GAN:} Multi-Discriminator Generative Adversarial Networks for
               Distributed Datasets},
  journal   = {CoRR},
  volume    = {abs/1811.03850},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.03850},
  archivePrefix = {arXiv},
  eprint    = {1811.03850},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-03850.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{dropout-gan,
  author    = {Gon{\c{c}}alo Mordido and
               Haojin Yang and
               Christoph Meinel},
  title     = {Dropout-GAN: Learning from a Dynamic Ensemble of Discriminators},
  journal   = {CoRR},
  volume    = {abs/1807.11346},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.11346},
  archivePrefix = {arXiv},
  eprint    = {1807.11346},
  timestamp = {Mon, 13 Aug 2018 16:48:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-11346.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{random_proj,
  author    = {Behnam Neyshabur and
               Srinadh Bhojanapalli and
               Ayan Chakrabarti},
  title     = {Stabilizing {GAN} Training with Multiple Random Projections},
  journal   = {CoRR},
  volume    = {abs/1705.07831},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07831},
  archivePrefix = {arXiv},
  eprint    = {1705.07831},
  timestamp = {Mon, 13 Aug 2018 16:46:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NeyshaburBC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{cohen2005feature,
 author = {Cohen, Shay and Ruppin, Eytan and Dror, Gideon},
 title = {Feature Selection Based on the Shapley Value},
 booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
 series = {IJCAI'05},
 year = {2005},
 location = {Edinburgh, Scotland},
 pages = {665--670},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1642293.1642400},
 acmid = {1642400},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}

@inproceedings{louppe2013understanding,
  title={Understanding variable importances in forests of randomized trees},
  author={Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
  booktitle={Advances in Neural Information Processing Systems},
  pages={431--439},
  year={2013}
}

@book{murphy2012machine,
	address = {London},
	title = {Machine {Learning}: {A} {Probabilistic} {Perspective}},
	isbn = {978-0-262-01802-0},
	url = {http://dl.acm.org/citation.cfm?id=2380985},
	publisher = {The MIT Press},
	author = {Murphy, Kevin P.},
	year = {2012}
}

@unpublished{goodfellow2016deep,
    title={Deep Learning},
    author={Ian Goodfellow Yoshua Bengio and Aaron Courville},
    note={Book in preparation for MIT Press},
    url={http://www.deeplearningbook.org},
    year={2016}
}

@misc{lichman2013uci,
	author = "M. Lichman",
	year = "2013",
	title = "{UCI} Machine Learning Repository",
	url = "http://archive.ics.uci.edu/ml",
	institution = "University of California, Irvine, School of Information and Computer Sciences"
} 

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{zheng2020error,
  title={Error-bounded correction of noisy labels},
  author={Zheng, Songzhu and Wu, Pengxiang and Goswami, Aman and Goswami, Mayank and Metaxas, Dimitris and Chen, Chao},
  booktitle={International Conference on Machine Learning},
  pages={11447--11457},
  year={2020},
  organization={PMLR}
}


@article{kim2024learning,
  title={Learning with Noisy Labels: Interconnection of Two Expectation-Maximizations},
  author={Kim, Heewon and Chang, Hyun Sung and Cho, Kiho and Lee, Jaeyun and Han, Bohyung},
  journal={arXiv preprint arXiv:2401.04390},
  year={2024}
}


@inproceedings{wang2021learning,
  title={Learning from noisy labels with complementary loss functions},
  author={Wang, Deng-Bao and Wen, Yong and Pan, Lujia and Zhang, Min-Ling},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={11},
  pages={10111--10119},
  year={2021}
}


@inproceedings{guyon2003design,
  title={Design of experiments of the NIPS 2003 variable selection benchmark},
  author={Guyon, Isabelle},
  booktitle={NIPS 2003 workshop on feature extraction and feature selection},
  year={2003}
}

@software{autograd,
  author = {Dougal Maclaurin and David Duvenaud and Matthew Johnson and Ryan P. Adams},
  title = {Autograd: Reverse-mode differentiation of native {P}ython},
  url = {http://github.com/HIPS/autograd},
  version = {1.1.2},
  year = {2015},
}

@inproceedings{bergstra2010,
    author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
    month = jun,
    title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
    booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
    year = {2010},
    location = {Austin, TX},
    note = {Oral Presentation}
}

@inproceedings{stevens2015holoviews,
  title={HoloViews: Building Complex Visualizations Easily for Reproducible Science},
  author={Stevens, Jean-Luc R and Rudiger, Philipp and Bednar, James A},
  booktitle={SciPy Conference Proceedings},
  year={2015}
}

@book{van2000asymptotic,
  title={Asymptotic Statistics},
  author={van der Vaart, A.W.},
  isbn={9780521784504},
  lccn={98015176},
  series={Cambridge Series in Statistical and Probabilistic Mathematics},
  url={https://books.google.co.uk/books?id=UEuQEM5RjWgC},
  year={2000},
  publisher={Cambridge University Press}
}

@article{keinan2004fair,
  title={Fair attribution of functional contribution in artificial and biological networks},
  author={Keinan, Alon and Sandbank, Ben and Hilgetag, Claus C and Meilijson, Isaac and Ruppin, Eytan},
  journal={Neural Computation},
  volume={16},
  number={9},
  pages={1887--1915},
  year={2004},
  publisher={MIT Press}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@phdthesis{neal1995bayesian,
    title = {Bayesian learning for neural networks},
    url = {http://www.db.toronto.edu/~radford/ftp/thesis.pdf},
    urldate = {2014-11-21},
    school = {University of Toronto},
    author = {Neal, Radford M.},
    year = {1995}
}

@article{kingma2015variational,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	url = {http://arxiv.org/abs/1506.02557},
	urldate = {2015-06-10},
	journal = {arXiv:1506.02557 [cs, stat]},
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02557},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning}
}

@article{ahmed1989entropy,
	title = {Entropy expressions and their estimators for multivariate distributions},
	volume = {35},
	issn = {0018-9448},
	doi = {10.1109/18.30996},
	abstract = {Entropy expressions for several continuous multivariate distributions are derived. Point estimation of entropy for the multinormal distribution and for the distribution of order statistics from D.G. Weinman's (Ph.D dissertation, Ariz. State Univ., Tempe, AZ, 1966) exponential distribution is considered. The asymptotic distribution of the uniformly minimum variance unbiased estimator for multinormal entropy is obtained. Simulation results on convergence of the means and variances of these estimators are provided},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Ahmed, N. A. and Gokhale, D. V.},
	month = may,
	year = {1989},
	keywords = {asymptotic distribution, continuous multivariate distributions, convergence, Density functional theory, Density measurement, entropy, exponential distribution, Gaussian distribution, information theory, multinormal distribution, multinormal entropy, order statistics distribution, parameter estimation, point estimation, Random variables, statistical analysis, Statistical distributions, statistical inference, Testing, uniformly minimum variance unbiased estimator},
	pages = {688--692}
}

@article{xu2014classifier,
  title={Classifier cascades and trees for minimizing feature evaluation cost},
  author={Xu, Zhixiang and Kusner, Matt J and Weinberger, Kilian Q and Chen, Minmin and Chapelle, Olivier},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2113--2144},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{kusner2014feature,
  title={Feature-Cost Sensitive Learning with Submodular Trees of Classifiers.},
  author={Kusner, Matt J and Chen, Wenlin and Zhou, Quan and Xu, Zhixiang Eddie and Weinberger, Kilian Q and Chen, Yixin},
  booktitle={AAAI},
  pages={1939--1945},
  year={2014}
}

@article{krause2012near,
  title={Near-optimal nonmyopic value of information in graphical models},
  author={Krause, Andreas and Guestrin, Carlos E},
  journal={arXiv preprint arXiv:1207.1394},
  year={2012}
}

@article{lucas2015designing,
  title={Designing optimal greenhouse gas observing networks that consider performance and cost},
  author={Lucas, DD and Kwok, C Yver and Cameron-Smith, P and Graven, H and Bergmann, D and Guilderson, TP and Weiss, R and Keeling, R},
  journal={Geoscientific Instrumentation, Methods and Data Systems},
  volume={4},
  number={1},
  pages={121},
  year={2015},
  publisher={Copernicus GmbH}
}

@article{mackay1992bayesian,
  title={Bayesian interpolation},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={3},
  pages={415--447},
  year={1992},
  publisher={MIT Press}
}

@Article{vgg,
    author       = "Simonyan, K. and Zisserman, A.",
    title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    journal      = "CoRR",
    volume       = "abs/1409.1556",
    year         = "2014"
}

@inproceedings{schulman2015gradient,
  title={Gradient estimation using stochastic computation graphs},
  author={Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3528--3536},
  year={2015}
}

@article{tsne,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{ixmas,
  author = {Weinland, Daniel and Ronfard, Remi and Boyer, Edmond},
  title = {Free Viewpoint Action Recognition using Motion History Volumes},
  journal = {Computer Vision and Image Understanding},
  year = {2006},
  volume = {104},
  pages = {249--257},
  number = {2-3}
}

@article{richman2016allocate,
  title={How to Allocate Resources For Features Acquisition?},
  author={Richman, Oran and Mannor, Shie},
  journal={arXiv preprint arXiv:1607.02763},
  year={2016}
}

@misc{lasagne,
  author       = {Sander Dieleman and
                  Jan Schlüter and
                  Colin Raffel and
                  Eben Olson and
                  Søren Kaae Sønderby and
                  Daniel Nouri and
                  Daniel Maturana and
                  Martin Thoma and
                  Eric Battenberg and
                  Jack Kelly and
                  Jeffrey De Fauw and
                  Michael Heilman and
                  Diogo Moitinho de Almeida and
                  Brian McFee and
                  Hendrik Weideman and
                  Gábor Takács and
                  Peter de Rivaz and
                  Jon Crall and
                  Gregory Sanders and
                  Kashif Rasul and
                  Cong Liu and
                  Geoffrey French and
                  Jonas Degrave},
  title        = {Lasagne: First release.},
  month        = aug,
  year         = 2015,
  doi          = {10.5281/zenodo.27878},
  url          = {http://dx.doi.org/10.5281/zenodo.27878}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	urldate = {2015-05-25},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@ARTICLE{maddison_concrete_2016,
   author = {{Maddison}, C.~J. and {Mnih}, A. and {Whye Teh}, Y.},
    title = "{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.00712},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Statistics - Machine Learning},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161100712M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{jang_categorical_2016,
   author = {{Jang}, E. and {Gu}, S. and {Poole}, B.},
    title = "{Categorical Reparameterization with Gumbel-Softmax}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.01144},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161101144J},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{gao_variational_2016,
   author = {{Gao}, S. and {Ver Steeg}, G. and {Galstyan}, A.},
    title = "{Variational Information Maximization for Feature Selection}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.02827},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160602827G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{gentle2004random,
  title={Random Number Generation and Monte Carlo Methods},
  author={Gentle, J.E.},
  isbn={9780387001784},
  lccn={2003042437},
  series={Statistics and Computing},
  url={https://books.google.co.uk/books?id=8sV\_nuXolycC},
  year={2004},
  publisher={Springer New York}
}

@book{hájek1981sampling,
  title={Sampling from a finite population},
  author={H{\'a}jek, J. and Dupa{\v{c}}, V.},
  isbn={9780824712914},
  lccn={81007835},
  series={Statistics, textbooks and monographs},
  url={https://books.google.co.uk/books?id=DxHvAAAAMAAJ},
  year={1981},
  publisher={M. Dekker}
}

@Article{Kochar2001,
author="Kochar, Subhash C.
and Korwar, Ramesh",
title="On Random Sampling Without Replacement from a Finite Population",
journal="Annals of the Institute of Statistical Mathematics",
year="2001",
volume="53",
number="3",
pages="631--646",
issn="1572-9052",
doi="10.1023/A:1014693702392",
url="http://dx.doi.org/10.1023/A:1014693702392"
}

@article{Traat2004395,
title = "Sampling design and sample selection through distribution theory ",
journal = "Journal of Statistical Planning and Inference ",
volume = "123",
number = "2",
pages = "395 - 413",
year = "2004",
note = "",
issn = "0378-3758",
doi = "http://dx.doi.org/10.1016/S0378-3758(03)00150-2",
url = "//www.sciencedirect.com/science/article/pii/S0378375803001502",
author = "Imbi Traat and Lennart Bondesson and Kadri Meister",
keywords = "Multivariate Bernoulli design",
keywords = "Multinomial design",
keywords = "Hypergeometric design",
keywords = "Conditional Poisson design",
keywords = "Sampford design",
keywords = "Order sampling design",
keywords = "List-sequential sampling",
keywords = "Markov chain Monte Carlo",
keywords = "Gibbs sampling "
}

@incollection{swersky2012nchoosek,
title = {Probabilistic n-Choose-k Models for Classification and Ranking},
author = {Swersky, Kevin and Frey, Brendan J and Tarlow, Daniel and Richard S. Zemel and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {3050--3058},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4702-probabilistic-n-choose-k-models-for-classification-and-ranking.pdf}
}

@inproceedings{tarlow2012fast,
 author = {Tarlow, Daniel and Swersky, Kevin and Zemel, Richard S. and Adams, Ryan P. and Frey, Brendan J.},
 title = {Fast Exact Inference for Recursive Cardinality Models},
 booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'12},
 year = {2012},
 isbn = {978-0-9749039-8-9},
 location = {Catalina Island, CA},
 pages = {825--834},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3020652.3020738},
 acmid = {3020738},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
}

@incollection{Ghahramani2006Infinite,
title = {Infinite latent feature models and the Indian buffet process},
author = {Ghahramani, Zoubin and Thomas L. Griffiths},
booktitle = {Advances in Neural Information Processing Systems 18},
editor = {Y. Weiss and P. B. Sch\"{o}lkopf and J. C. Platt},
pages = {475--482},
year = {2006},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2882-infinite-latent-feature-models-and-the-indian-buffet-process.pdf}
}

@article{Kulesza2012Determinantal,
url = {http://dx.doi.org/10.1561/2200000044},
year = {2012},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Determinantal Point Processes for Machine Learning},
doi = {10.1561/2200000044},
issn = {1935-8237},
number = {2–3},
pages = {123-286},
author = {Alex Kulesza and Ben Taskar}
}

@ARTICLE{molchanov2017variational,
   author = {{Molchanov}, D. and {Ashukha}, A. and {Vetrov}, D.},
    title = "{Variational Dropout Sparsifies Deep Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.05369},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170105369M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{srinivas2016generalized,
   author = {{Srinivas}, S. and {Venkatesh Babu}, R.},
    title = "{Generalized Dropout}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.06791},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161106791S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{gumbel1954statistical,
  title={Statistical theory of extreme values and some practical applications: a series of lectures},
  author={Gumbel, E.J.},
  lccn={54061179},
  issn={1049-4685},
  series={Applied mathematics series},
  url={https://books.google.co.uk/books?id=SNpJAAAAMAAJ},
  year={1954},
  publisher={U. S. Govt. Print. Office}
}

@ARTICLE{shazeer2017outrageously,
   author = {{Shazeer}, N. and {Mirhoseini}, A. and {Maziarz}, K. and {Davis}, A. and 
	{Le}, Q. and {Hinton}, G. and {Dean}, J.},
    title = "{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.06538},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170106538S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{jacobs1991moe,
 author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
 title = {Adaptive Mixtures of Local Experts},
 journal = {Neural Comput.},
 issue_date = {Spring 1991},
 volume = {3},
 number = {1},
 month = mar,
 year = {1991},
 issn = {0899-7667},
 pages = {79--87},
 numpages = {9},
 url = {http://dx.doi.org/10.1162/neco.1991.3.1.79},
 doi = {10.1162/neco.1991.3.1.79},
 acmid = {1351018},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{jordan1994hierarchical,
 author = {Jordan, Michael I. and Jacobs, Robert A.},
 title = {Hierarchical Mixtures of Experts and the EM Algorithm},
 journal = {Neural Comput.},
 issue_date = {March 1994},
 volume = {6},
 number = {2},
 month = mar,
 year = {1994},
 issn = {0899-7667},
 pages = {181--214},
 numpages = {34},
 url = {http://dx.doi.org/10.1162/neco.1994.6.2.181},
 doi = {10.1162/neco.1994.6.2.181},
 acmid = {188106},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{rosen1997asymptotic,
title = "Asymptotic theory for order sampling",
journal = "Journal of Statistical Planning and Inference",
volume = "62",
number = "2",
pages = "135 - 158",
year = "1997",
note = "",
issn = "0378-3758",
doi = "http://dx.doi.org/10.1016/S0378-3758(96)00185-1",
url = "http://www.sciencedirect.com/science/article/pii/S0378375896001851",
author = "Bengt Rosén",
keywords = "Order sampling",
keywords = "Successive sampling",
keywords = "Sequential Poisson sampling",
keywords = "Linear statistics",
keywords = "Asymptotic results"
}

@article{lees2010theoretical,
  title={Theoretical maximum capacity as benchmark for empty vehicle redistribution in personal rapid transit},
  author={Lees-Miller, John and Hammersley, John and Wilson, R},
  journal={Transportation Research Record: Journal of the Transportation Research Board},
  number={2146},
  pages={76--83},
  year={2010},
  publisher={Transportation Research Board of the National Academies}
}

@misc{dziugaite,
      title={Training generative neural networks via Maximum Mean Discrepancy optimization}, 
      author={Gintare Karolina Dziugaite and Daniel M. Roy and Zoubin Ghahramani},
      year={2015},
      eprint={1505.03906},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{GaussianRKHS,
title={Some Properties of Gaussian Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory},
  author={Minh, Ha Quang},
  journal={Constructive Approximation},
  number={32},
  pages={307–338},
  year={2010}
}
  
  @article{GMMN,
  author    = {Yujia Li and
               Kevin Swersky and
               Richard S. Zemel},
  title     = {Generative Moment Matching Networks},
  journal   = {CoRR},
  volume    = {abs/1502.02761},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.02761},
  archivePrefix = {arXiv},
  eprint    = {1502.02761},
  timestamp = {Mon, 13 Aug 2018 16:46:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LiSZ15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{MMD_Net,
    title={Training generative neural networks via Maximum Mean Discrepancy optimization},
    author={Gintare Karolina Dziugaite and Daniel M. Roy and Zoubin Ghahramani},
    year={2015},
    eprint={1505.03906},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@article{MMD_GAN,
  author    = {Chun{-}Liang Li and
               Wei{-}Cheng Chang and
               Yu Cheng and
               Yiming Yang and
               Barnab{\'{a}}s P{\'{o}}czos},
  title     = {{MMD} {GAN:} Towards Deeper Understanding of Moment Matching Network},
  journal   = {CoRR},
  volume    = {abs/1705.08584},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.08584},
  archivePrefix = {arXiv},
  eprint    = {1705.08584},
  timestamp = {Mon, 13 Aug 2018 16:46:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LiCCYP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wgan,
    title={Wasserstein GAN},
    author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
    year={2017},
    eprint={1701.07875},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}



@article{kernel_two_sample_gretton,
  author    = {Arthur Gretton and
               Karsten M. Borgwardt and
               Malte J. Rasch and
               Bernhard Sch{\"{o}}lkopf and
               Alexander J. Smola},
  title     = {A Kernel Method for the Two-Sample Problem},
  journal   = {CoRR},
  volume    = {abs/0805.2368},
  year      = {2008},
  url       = {http://arxiv.org/abs/0805.2368},
  archivePrefix = {arXiv},
  eprint    = {0805.2368},
  timestamp = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-0805-2368.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{visual_chiral,
   title={Visual Chirality},
   ISBN={9781728171685},
   url={http://dx.doi.org/10.1109/CVPR42600.2020.01231},
   DOI={10.1109/cvpr42600.2020.01231},
   journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Lin, Zhiqiu and Sun, Jin and Davis, Abe and Snavely, Noah},
   year={2020},
   month={Jun}
}

@article{DeepKernelLearning,
  author    = {Andrew Gordon Wilson and
               Zhiting Hu and
               Ruslan Salakhutdinov and
               Eric P. Xing},
  title     = {Deep Kernel Learning},
  journal   = {CoRR},
  volume    = {abs/1511.02222},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.02222},
  archivePrefix = {arXiv},
  eprint    = {1511.02222},
  timestamp = {Mon, 13 Aug 2018 16:47:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WilsonHSX15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{demystifying,
    title={Demystifying MMD GANs},
    author={Mikołaj Bińkowski and Dougal J. Sutherland and Michael Arbel and Arthur Gretton},
    year={2018},
    eprint={1801.01401},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{mcgan,
  author    = {Youssef Mroueh and
               Tom Sercu and
               Vaibhava Goel},
  title     = {McGan: Mean and Covariance Feature Matching {GAN}},
  journal   = {CoRR},
  volume    = {abs/1702.08398},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.08398},
  archivePrefix = {arXiv},
  eprint    = {1702.08398},
  timestamp = {Mon, 13 Aug 2018 16:47:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MrouehSG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{muller,
  author    = {Müller, Alfred},
  title     = {Integral Probability Metrics and Their Generating Classes of Functions},
  journal   = {Advances in Applied Probability pp. 429–443},
  volume    = {vol. 29, no. 2},
  year      = {1997},
  url       = {www.jstor.org/stable/1428011},
  archivePrefix = {jstor},
  timestamp = {Accessed 10 Sept. 2020},
}

@article{GAN_orig,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Y.},
year = {2014},
month = {06},
pages = {},
title = {Generative Adversarial Nets},
journal = {ArXiv}
}

@misc{kingmaVAE,
    title={Auto-Encoding Variational Bayes},
    author={Diederik P Kingma and Max Welling},
    year={2013},
    eprint={1312.6114},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{DenoisingDP,
  title={Denoising Diffusion Probabilistic Models},
  author={Jonathan Ho and Ajay Jain and P. Abbeel},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.11239}
}

@article{likelhood_gan,
  author    = {Hamid Eghbal{-}zadeh and
               Gerhard Widmer},
  title     = {Likelihood Estimation for Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1707.07530},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.07530},
  archivePrefix = {arXiv},
  eprint    = {1707.07530},
  timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Eghbal-zadehW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{flowGAN,
  author    = {Aditya Grover and
               Manik Dhar and
               Stefano Ermon},
  title     = {Flow-GAN: Bridging implicit and prescribed learning in generative
               models},
  journal   = {CoRR},
  volume    = {abs/1705.08868},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.08868},
  archivePrefix = {arXiv},
  eprint    = {1705.08868},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GroverDE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{parzen1962,
author = "Parzen, Emanuel",
doi = "10.1214/aoms/1177704472",
fjournal = "Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "09",
number = "3",
pages = "1065--1076",
publisher = "The Institute of Mathematical Statistics",
title = "On Estimation of a Probability Density Function and Mode",
url = "https://doi.org/10.1214/aoms/1177704472",
volume = "33",
year = "1962"
}

@article{lsGAN,
  author    = {Xudong Mao and
               Qing Li and
               Haoran Xie and
               Raymond Y. K. Lau and
               Zhen Wang},
  title     = {Multi-class Generative Adversarial Networks with the {L2} Loss Function},
  journal   = {CoRR},
  volume    = {abs/1611.04076},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.04076},
  archivePrefix = {arXiv},
  eprint    = {1611.04076},
  timestamp = {Wed, 13 Nov 2019 15:48:57 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/MaoLXLW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{dcgan,
    title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
    author={Alec Radford and Luke Metz and Soumith Chintala},
    year={2015},
    eprint={1511.06434},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
year = {2012},
month = {01},
pages = {},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
volume = {25},
journal = {Neural Information Processing Systems},
doi = {10.1145/3065386}
}

@misc{MMDgradient,
    title={On gradient regularizers for MMD GANs},
    author={Michael Arbel and Dougal J. Sutherland and Mikołaj Bińkowski and Arthur Gretton},
    year={2018},
    eprint={1805.11565},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{phi-div,
    title={On integral probability metrics, φ-divergences and binary classification},
    author={Bharath K. Sriperumbudur and Kenji Fukumizu and Arthur Gretton and Bernhard Schölkopf and Gert R. G. Lanckriet},
    year={2009},
    eprint={0901.2698},
    archivePrefix={arXiv},
    primaryClass={cs.IT}
}

@misc{wgan-gp,
    title={Improved Training of Wasserstein GANs},
    author={Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
    year={2017},
    eprint={1704.00028},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{cramer,
title={The Cramer Distance as a Solution to Biased Wasserstein Gradients},
author={Marc G. Bellemare and Ivo Danihelka and Will Dabney and Shakir Mohamed and Balaji Lakshminarayanan and Stephan Hoyer and Remi Munos},
year={2018},
url={https://openreview.net/forum?id=S1m6h21Cb},
}

@article{variationBlur,
  author    = {Shengjia Zhao and
               Jiaming Song and
               Stefano Ermon},
  title     = {Towards Deeper Understanding of Variational Autoencoding Models},
  journal   = {CoRR},
  volume    = {abs/1702.08658},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.08658},
  archivePrefix = {arXiv},
  eprint    = {1702.08658},
  timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhaoSE17a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{arjovsky2017principled,
      title={Towards Principled Methods for Training Generative Adversarial Networks}, 
      author={Martin Arjovsky and Léon Bottou},
      year={2017},
      eprint={1701.04862},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{song2,
author="Smola, Alex
and Gretton, Arthur
and Song, Le
and Sch{\"o}lkopf, Bernhard",
editor="Hutter, Marcus
and Servedio, Rocco A.
and Takimoto, Eiji",
title="A Hilbert Space Embedding for Distributions",
booktitle="Algorithmic Learning Theory",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="13--31",
abstract="We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a reproducing kernel Hilbert space. Applications of this technique can be found in two-sample tests, which are used for determining whether two sets of observations arise from the same distribution, covariate shift correction, local learning, measures of independence, and density estimation.",
isbn="978-3-540-75225-7"
}

@Misc{songthesis,
title = { Learning via Hilbert space embedding of distributions },
author = { Song Lê and University of Sydney. School of Information Technologies },
year = { 2008 },
type = { Thesis },
subjects = { Hilbert space; Probabilities },
language = { English },
note = { Includes graphs and tables },
catalogue-url = { https://trove.nla.gov.au/work/27382824 }
}

@Misc{bochner_notes,
title = {Gaussian measures and Bochner’s theorem},
author = { Jordan Bell, Department of Mathematics, University of Toronto
},
year = { 2015 },
url = {http://individual.utoronto.ca/jordanbell/notes/bochnertheorem.pdf}
}

@article{randomFastMMD,
  author    = {Ji Zhao and
               Deyu Meng},
  title     = {FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample
               Test},
  journal   = {CoRR},
  volume    = {abs/1405.2664},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.2664},
  archivePrefix = {arXiv},
  eprint    = {1405.2664},
  timestamp = {Mon, 13 Aug 2018 16:48:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/0001M14a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{blockVar,
title = {B-test: A Non-parametric, Low Variance Kernel Two-sample Test},
author = {Zaremba, Wojciech and Gretton, Arthur and Blaschko, Matthew},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {755--763},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5081-b-test-a-non-parametric-low-variance-kernel-two-sample-test.pdf}
}

@article{unhinged,
  title={Learning with symmetric label noise: The importance of being unhinged},
  author={Van Rooyen, Brendan and Menon, Aditya and Williamson, Robert C},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@article{memory_gan,
  author    = {Youngjin Kim and
               Minjung Kim and
               Gunhee Kim},
  title     = {Memorization Precedes Generation: Learning Unsupervised GANs with
               Memory Networks},
  journal   = {CoRR},
  volume    = {abs/1803.01500},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.01500},
  archivePrefix = {arXiv},
  eprint    = {1803.01500},
  timestamp = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-01500.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{norm_flow,

  author={I. {Kobyzev} and S. {Prince} and M. {Brubaker}},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Normalizing Flows: An Introduction and Review of Current Methods}, 

  year={2020},

  volume={},

  number={},

  pages={1-1},}
  
 @inproceedings{vaegan,
  title     = {VAEGAN: A Collaborative Filtering Framework based on Adversarial Variational Autoencoders},
  author    = {Yu, Xianwen and Zhang, Xiaoning and Cao, Yang and Xia, Min},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {4206--4212},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/584},
  url       = {https://doi.org/10.24963/ijcai.2019/584},
}

@article{mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
} 

@article{univ-and-char,
  author  = {Bharath K. Sriperumbudur and Kenji Fukumizu and Gert R.G. Lanckriet},
  title   = {Universality, Characteristic Kernels and RKHS Embedding of Measures},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {70},
  pages   = {2389-2410},
  url     = {http://jmlr.org/papers/v12/sriperumbudur11a.html}
}

@misc{learningImplicit,
      title={Learning in Implicit Generative Models}, 
      author={Shakir Mohamed and Balaji Lakshminarayanan},
      year={2017},
      eprint={1610.03483},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@TechReport{LFW,
  author =       {Gary B. Huang and Manu Ramesh and Tamara Berg and 
                  Erik Learned-Miller},
  title =        {Labeled Faces in the Wild: A Database for Studying 
                  Face Recognition in Unconstrained Environments},
  institution =  {University of Massachusetts, Amherst},
  year =         2007,
  number =       {07-49},
  month =        {October}}

@misc{xiao2017fashionmnist,
      title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}, 
      author={Han Xiao and Kashif Rasul and Roland Vollgraf},
      year={2017},
      eprint={1708.07747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{heterogeneous,
  title={How does heterogeneous label noise impact generalization in neural nets?},
  author={Khanal, Bidur and Kanan, Christopher},
  booktitle={Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part II},
  pages={229--241},
  year={2021},
  organization={Springer}
}

@article{cheng2024dynamic,
  title={Dynamic training for handling textual label noise},
  author={Cheng, Shaohuan and Chen, Wenyu and Liu, Wanlong and Zhou, Li and Zhao, Honglin and Kong, Weishan and Qu, Hong and Fu, Mingsheng},
  journal={Applied Intelligence},
  pages={1--16},
  year={2024},
  publisher={Springer}
}

@ARTICLE{robustRegressionFeature,
  author={Guo, Yaqing and Wang, Wenjian and Wang, Xuejun},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Robust Linear Regression Feature Selection Method for Data Sets With Unknown Noise}, 
  year={2023},
  volume={35},
  number={1},
  pages={31-44},
  keywords={Feature extraction;Face recognition;Linear regression;Estimation;Training;Data models;Task analysis;Regression;feature selection;unknown noise;robust;face recognition},
  doi={10.1109/TKDE.2021.3076891}}

@inproceedings{gradientClip,
  title={Can gradient clipping mitigate label noise?},
  author={Menon, Aditya Krishna and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{FID,
  author    = {Martin Heusel and
               Hubert Ramsauer and
               Thomas Unterthiner and
               Bernhard Nessler and
               G{\"{u}}nter Klambauer and
               Sepp Hochreiter},
  title     = {GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium},
  journal   = {CoRR},
  volume    = {abs/1706.08500},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.08500},
  archivePrefix = {arXiv},
  eprint    = {1706.08500},
  timestamp = {Sat, 23 Jan 2021 01:20:58 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/HeuselRUNKH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{adomaityte2023high,
  title={High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality},
  author={Adomaityte, Urte and Defilippis, Leonardo and Loureiro, Bruno and Sicuro, Gabriele},
  journal={arXiv preprint arXiv:2309.16476},
  year={2023}
}

@article{huber1973robust,
  title={Robust regression: asymptotics, conjectures and Monte Carlo},
  author={Huber, Peter J},
  journal={The annals of statistics},
  pages={799--821},
  year={1973},
  publisher={JSTOR}
}



@misc{IS,
      title={Improved Techniques for Training GANs}, 
      author={Tim Salimans and Ian Goodfellow and Wojciech Zaremba and Vicki Cheung and Alec Radford and Xi Chen},
      year={2016},
      eprint={1606.03498},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{flow++images,
  author    = {Jonathan Ho and
               Xi Chen and
               Aravind Srinivas and
               Yan Duan and
               Pieter Abbeel},
  title     = {Flow++: Improving Flow-Based Generative Models with Variational Dequantization
               and Architecture Design},
  journal   = {CoRR},
  volume    = {abs/1902.00275},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.00275},
  archivePrefix = {arXiv},
  eprint    = {1902.00275},
  timestamp = {Tue, 21 May 2019 18:03:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-00275.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ganReviewCreswell,
  author    = {Antonia Creswell and
               Tom White and
               Vincent Dumoulin and
               Kai Arulkumaran and
               Biswa Sengupta and
               Anil A. Bharath},
  title     = {Generative Adversarial Networks: An Overview},
  journal   = {CoRR},
  volume    = {abs/1710.07035},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.07035},
  archivePrefix = {arXiv},
  eprint    = {1710.07035},
  timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-07035.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{ganReviewWang,

  author={K. {Wang} and C. {Gou} and Y. {Duan} and Y. {Lin} and X. {Zheng} and F. {Wang}},

  journal={IEEE/CAA Journal of Automatica Sinica}, 

  title={Generative adversarial networks: introduction and outlook}, 

  year={2017},

  volume={4},

  number={4},

  pages={588-598},}

@article{stylegan,
  author    = {Tero Karras and
               Samuli Laine and
               Timo Aila},
  title     = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1812.04948},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.04948},
  archivePrefix = {arXiv},
  eprint    = {1812.04948},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-04948.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{progan,
  author    = {Tero Karras and
               Timo Aila and
               Samuli Laine and
               Jaakko Lehtinen},
  title     = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  journal   = {CoRR},
  volume    = {abs/1710.10196},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10196},
  archivePrefix = {arXiv},
  eprint    = {1710.10196},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-10196.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cogan,
  author    = {Ming{-}Yu Liu and
               Oncel Tuzel},
  title     = {Coupled Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1606.07536},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.07536},
  archivePrefix = {arXiv},
  eprint    = {1606.07536},
  timestamp = {Mon, 13 Aug 2018 16:46:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/0001T16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cyclegan,
  author    = {Jun{-}Yan Zhu and
               Taesung Park and
               Phillip Isola and
               Alexei A. Efros},
  title     = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial
               Networks},
  journal   = {CoRR},
  volume    = {abs/1703.10593},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.10593},
  archivePrefix = {arXiv},
  eprint    = {1703.10593},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhuPIE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cgan,
  author    = {Mehdi Mirza and
               Simon Osindero},
  title     = {Conditional Generative Adversarial Nets},
  journal   = {CoRR},
  volume    = {abs/1411.1784},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.1784},
  archivePrefix = {arXiv},
  eprint    = {1411.1784},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MirzaO14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{dagan,
title={Data Augmentation Generative Adversarial Networks},
author={Anthreas Antoniou and Amos Storkey and Harrison Edwards},
year={2018},
url={https://openreview.net/forum?id=S1Auv-WRZ},
}

@misc{kantorovich,
title={A Hitchhiker’s guide to Wasserstein distances},
author={Giuliano Basso},
year={2015},
url={http://n.ethz.ch/~gbasso/download/A\%20Hitchhikers\%20guide\%20to\%20Wasserstein/A\%20Hitchhikers\%20guide\%20to\%20Wasserstein.pdf},
}

@misc{huffman,
title={Entropy and Huffman Codes},
author={Todd Ebert},
year={2015},
url={https://web.csulb.edu/~tebert/teaching/lectures/528/huffman/huffman.pdf},
}

@inbook{rasmussen_williams_2008, place={Cambridge, MA}, title={Relationship between GPs and other Models}, booktitle={Gaussian processes for machine learning}, publisher={MIT Press}, author={Rasmussen, Carl Edward and Williams, Christopher K. I.}, year={2008}} 
 
@misc{rachevMetrics,
title={Lecture 3: Probability metrics},
author={Prof. Dr. Svetlozar Rachev},
year={2008},
url={https://statistik.econ.kit.edu/download/doc_secure1/3_StochModels.pdf},
}

 @inbook{wavelets, place={Boca Raton, FL}, title={Chapter 1}, booktitle={A first course on wavelets}, publisher={CRC Press}, author={Hernández Eugenio and Weiss, Guido}, year={1996}} 

@misc{2020review,
      title={A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications}, 
      author={Jie Gui and Zhenan Sun and Yonggang Wen and Dacheng Tao and Jieping Ye},
      year={2020},
      eprint={2001.06937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lle,
title={Nonlinear dimensionality reduction by locally linear embedding},
author={Sam Roweis},
year={2000}
}

@misc{manifoldhypothesis,
      title={Testing the Manifold Hypothesis}, 
      author={Charles Fefferman and Sanjoy Mitter and Hariharan Narayanan},
      year={2013},
      eprint={1310.0425},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}


@article{princpalCurves,
author = { Trevor   Hastie  and  Werner   Stuetzle },
title = {Principal Curves},
journal = {Journal of the American Statistical Association},
volume = {84},
number = {406},
pages = {502-516},
year  = {1989},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1989.10478797},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478797
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1989.10478797
    
}

}

@article{nlpca,
    author = {Scholz, Matthias and Kaplan, Fatma and Guy, Charles L. and Kopka, Joachim and Selbig, Joachim},
    title = "{Non-linear PCA: a missing data approach}",
    journal = {Bioinformatics},
    volume = {21},
    number = {20},
    pages = {3887-3895},
    year = {2005},
    month = {08},
    abstract = "{Motivation: Visualizing and analysing the potential non-linear structure of a dataset is becoming an important task in molecular biology. This is even more challenging when the data have missing values.Results: Here, we propose an inverse model that performs non-linear principal component analysis (NLPCA) from incomplete datasets. Missing values are ignored while optimizing the model, but can be estimated afterwards. Results are shown for both artificial and experimental datasets. In contrast to linear methods, non-linear methods were able to give better missing value estimations for non-linear structured data.Application: We applied this technique to a time course of metabolite data from a cold stress experiment on the model plant Arabidopsis thaliana, and could approximate the mapping function from any time point to the metabolite responses. Thus, the inverse NLPCA provides greatly improved information for better understanding the complex response to cold stress.Contact:scholz@mpimp-golm.mpg.de}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bti634},
    url = {https://doi.org/10.1093/bioinformatics/bti634},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/21/20/3887/524944/bti634.pdf},
}

@article{KernelPCA,
author = {Schölkopf, Bernhard and Smola, Alex and Müller, Klaus-Robert},
year = {1998},
month = {07},
pages = {1299-1319},
title = {Nonlinear Component Analysis as a Kernel Eigenvalue Problem},
volume = {10},
journal = {Neural Computation},
doi = {10.1162/089976698300017467}
}

@misc{gansEqual,
      title={Are GANs Created Equal? A Large-Scale Study}, 
      author={Mario Lucic and Karol Kurach and Marcin Michalski and Sylvain Gelly and Olivier Bousquet},
      year={2018},
      eprint={1711.10337},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{fisher,
      title={Fisher GAN}, 
      author={Youssef Mroueh and Tom Sercu},
      year={2017},
      eprint={1705.09675},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{f-div,
author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
title = {F-GAN: Training Generative Neural Samplers Using Variational Divergence Minimization},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {271–279},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

  
@ARTICLE{div-functionals,

  author={X. {Nguyen} and M. J. {Wainwright} and M. I. {Jordan}},

  journal={IEEE Transactions on Information Theory}, 

  title={Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization}, 

  year={2010},

  volume={56},

  number={11},

  pages={5847-5861},

  doi={10.1109/TIT.2010.2068870}}

@ARTICLE{relative,
       author = {{Jolicoeur-Martineau}, Alexia},
        title = "{The relativistic discriminator: a key element missing from standard GAN}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Statistics - Machine Learning},
         year = 2018,
        month = jul,
          eid = {arXiv:1807.00734},
        pages = {arXiv:1807.00734},
archivePrefix = {arXiv},
       eprint = {1807.00734},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180700734J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{sobolev,
  author    = {Youssef Mroueh and
               Chun{-}Liang Li and
               Tom Sercu and
               Anant Raj and
               Yu Cheng},
  title     = {Sobolev {GAN}},
  journal   = {CoRR},
  volume    = {abs/1711.04894},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.04894},
  archivePrefix = {arXiv},
  eprint    = {1711.04894},
  timestamp = {Mon, 13 Aug 2018 16:48:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-04894.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{optimalKernel,
 author = {Gretton, Arthur and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Fukumizu, Kenji and Sriperumbudur, Bharath K.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {1205--1213},
 publisher = {Curran Associates, Inc.},
 title = {Optimal kernel choice for large-scale two-sample tests},
 url = {https://proceedings.neurips.cc/paper/2012/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf},
 volume = {25},
 year = {2012}
}

@misc{GRAM_MMD_ratio,
      title={Generative Ratio Matching Networks}, 
      author={Akash Srivastava and Kai Xu and Michael U. Gutmann and Charles Sutton},
      year={2020},
      eprint={1806.00101},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{deepkernel-nastygretton,
      title={Learning Deep Kernels for Non-Parametric Two-Sample Tests}, 
      author={Feng Liu and Wenkai Xu and Jie Lu and Guangquan Zhang and Arthur Gretton and D. J. Sutherland},
      year={2020},
      eprint={2002.09116},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{li2019implicit,
      title={Implicit Kernel Learning}, 
      author={Chun-Liang Li and Wei-Cheng Chang and Youssef Mroueh and Yiming Yang and Barnabás Póczos},
      year={2019},
      eprint={1902.10214},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{noise-contrastive-est, title = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models}, author = {Michael Gutmann and Aapo Hyvärinen}, booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = {297--304}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf}, url = {http://proceedings.mlr.press/v9/gutmann10a.html}, abstract = {} }

@misc{gui2020review,
      title={A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications}, 
      author={Jie Gui and Zhenan Sun and Yonggang Wen and Dacheng Tao and Jieping Ye},
      year={2020},
      eprint={2001.06937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{onlineKernelGAN,
      title={Online Kernel based Generative Adversarial Networks}, 
      author={Yeojoon Youn and Neil Thistlethwaite and Sang Keun Choe and Jacob Abernethy},
      year={2020},
      eprint={2006.11432},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{EBGAN,
  author    = {Junbo Jake Zhao and
               Micha{\"{e}}l Mathieu and
               Yann LeCun},
  title     = {Energy-based Generative Adversarial Network},
  journal   = {CoRR},
  volume    = {abs/1609.03126},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.03126},
  archivePrefix = {arXiv},
  eprint    = {1609.03126},
  timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhaoML16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{EnergyStats,
  author    = {Gábor J. Székely, Maria L. Rizzo},
  title     = {Energy statistics: A class of statistics based on distances},
  journal   = {Journal of Statistical Planning and Inference},
  volume    = {143},
  year      = {2002},
  url       = {https://www.sciencedirect.com/science/article/pii/S0378375813000633},
}



@article{L2,
  title={On loss functions for deep neural networks in classification},
  author={Janocha, Katarzyna and Czarnecki, Wojciech Marian},
  journal={arXiv preprint arXiv:1702.05659},
  year={2017}
}

@article{robustLR,
  title={Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning with Label Noise},
  author={Chen, Mingcai and Cheng, Hao and Du, Yuntao and Xu, Ming and Jiang, Wenyu and Wang, Chongjun},
  journal={arXiv preprint arXiv:2112.02960},
  year={2021}
}


@article{entropyReg,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}




@article{universalKernels,
author = {Micchelli, Charles and Xu, Yuesheng and Zhang, Haizhang},
year = {2006},
month = {12},
pages = {},
title = {Universal Kernels},
volume = {7},
journal = {Mathematics}
}



@article{secost,
  title={Secost: Sequential co-supervision for weakly labeled audio event detection},
  author={Kumar, Anurag and Ithapu, Vamsi Krishna},
  year={2020}
}

@inproceedings{charoenphakdee2019symmetric,
  title={On symmetric losses for learning from corrupted labels},
  author={Charoenphakdee, Nontawat and Lee, Jongyeong and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={961--970},
  organization={PMLR}
}

@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society: series B (methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

@article{zhang2021learning,
  title={Learning with feature-dependent label noise: A progressive approach},
  author={Zhang, Yikai and Zheng, Songzhu and Wu, Pengxiang and Goswami, Mayank and Chen, Chao},
  journal={arXiv preprint arXiv:2103.07756},
  year={2021}
}

@article{WAR,
  title={Wasserstein adversarial regularization (WAR) on label noise},
  author={Damodaran, Bharath and Fatras, Kilian and Lobry, Sylvain and Flamary, R{\'e}mi and Tuia, Devis and Courty, Nicolas},
  year={2019}
}


@InProceedings{AsymmetricLoss,
  title = 	 {Asymmetric Loss Functions for Learning with Noisy Labels},
  author =       {Zhou, Xiong and Liu, Xianming and Jiang, Junjun and Gao, Xin and Ji, Xiangyang},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12846--12856},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhou21f/zhou21f.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zhou21f.html},
  abstract = 	 {Robust loss functions are essential for training deep neural networks with better generalization power in the presence of noisy labels. Symmetric loss functions are confirmed to be robust to label noise. However, the symmetric condition is overly restrictive. In this work, we propose a new class of loss functions, namely asymmetric loss functions, which are robust to learning from noisy labels for arbitrary noise type. Subsequently, we investigate general theoretical properties of asymmetric loss functions, including classification-calibration, excess risk bound, and noise-tolerance. Meanwhile, we introduce the asymmetry ratio to measure the asymmetry of a loss function, and the empirical results show that a higher ratio will provide better robustness. Moreover, we modify several common loss functions, and establish the necessary and sufficient conditions for them to be asymmetric. Experiments on benchmark datasets demonstrate that asymmetric loss functions can outperform state-of-the-art methods.}
}


@article{sun2024prediction,
  title={Prediction Consistency Regularization for Learning with Noise Labels Based on Contrastive Clustering},
  author={Sun, Xinkai and Zhang, Sanguo and Ma, Shuangge},
  journal={Entropy},
  volume={26},
  number={4},
  pages={308},
  year={2024},
  publisher={MDPI}
}



@article{cleanLab,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}


@inproceedings{neighbour,
  title={Learning with Neighbor Consistency for Noisy Labels},
  author={Iscen, Ahmet and Valmadre, Jack and Arnab, Anurag and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4672--4681},
  year={2022}
}

@inproceedings{splitSelect,
  title={Understanding and utilizing deep neural networks trained with noisy labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019},
  organization={PMLR}
}


%Supp bib

@misc{mmdganGITHUB,
  author = {Chun-Liang Li and Wei-Cheng Chang and Yu Cheng and Yiming Yang and Barnabás Póczos},
  title = {MMD-GAN},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/OctoberChang/MMD-GAN}},
}


@misc{fidGithub,
  author = {M. Seitzer},
  title = {pytorch-fid},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/amjltc295/pytorch-fid}},
}

@misc{Seitzer2020FID,
  author={Maximilian Seitzer},
  title={{pytorch-fid: FID Score for PyTorch}},
  month={August},
  year={2020},
  note={Version 0.1.1},
  howpublished={\url{https://github.com/mseitzer/pytorch-fid}},
}

@misc{wgan-ganGITHUB,
  author = {Martin Arjovsky},
  title = {WassersteinGAN},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/martinarjovsky/WassersteinGAN}},
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{cohen2005feature,
 author = {Cohen, Shay and Ruppin, Eytan and Dror, Gideon},
 title = {Feature Selection Based on the Shapley Value},
 booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
 series = {IJCAI'05},
 year = {2005},
 location = {Edinburgh, Scotland},
 pages = {665--670},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1642293.1642400},
 acmid = {1642400},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}

@inproceedings{louppe2013understanding,
  title={Understanding variable importances in forests of randomized trees},
  author={Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
  booktitle={Advances in Neural Information Processing Systems},
  pages={431--439},
  year={2013}
}

@book{murphy2012machine,
	address = {London},
	title = {Machine {Learning}: {A} {Probabilistic} {Perspective}},
	isbn = {978-0-262-01802-0},
	url = {http://dl.acm.org/citation.cfm?id=2380985},
	publisher = {The MIT Press},
	author = {Murphy, Kevin P.},
	year = {2012}
}

@misc{lichman2013uci,
	author = "M. Lichman",
	year = "2013",
	title = "{UCI} Machine Learning Repository",
	url = "http://archive.ics.uci.edu/ml",
	institution = "University of California, Irvine, School of Information and Computer Sciences"
} 

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{guyon2003design,
  title={Design of experiments of the NIPS 2003 variable selection benchmark},
  author={Guyon, Isabelle},
  booktitle={NIPS 2003 workshop on feature extraction and feature selection},
  year={2003}
}

@software{autograd,
  author = {Dougal Maclaurin and David Duvenaud and Matthew Johnson and Ryan P. Adams},
  title = {Autograd: Reverse-mode differentiation of native {P}ython},
  url = {http://github.com/HIPS/autograd},
  version = {1.1.2},
  year = {2015},
}

@inproceedings{bergstra2010,
    author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
    month = jun,
    title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
    booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
    year = {2010},
    location = {Austin, TX},
    note = {Oral Presentation}
}

@inproceedings{stevens2015holoviews,
  title={HoloViews: Building Complex Visualizations Easily for Reproducible Science},
  author={Stevens, Jean-Luc R and Rudiger, Philipp and Bednar, James A},
  booktitle={SciPy Conference Proceedings},
  year={2015}
}

@book{van2000asymptotic,
  title={Asymptotic Statistics},
  author={van der Vaart, A.W.},
  isbn={9780521784504},
  lccn={98015176},
  series={Cambridge Series in Statistical and Probabilistic Mathematics},
  url={https://books.google.co.uk/books?id=UEuQEM5RjWgC},
  year={2000},
  publisher={Cambridge University Press}
}

@article{keinan2004fair,
  title={Fair attribution of functional contribution in artificial and biological networks},
  author={Keinan, Alon and Sandbank, Ben and Hilgetag, Claus C and Meilijson, Isaac and Ruppin, Eytan},
  journal={Neural Computation},
  volume={16},
  number={9},
  pages={1887--1915},
  year={2004},
  publisher={MIT Press}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@phdthesis{neal1995bayesian,
    title = {Bayesian learning for neural networks},
    url = {http://www.db.toronto.edu/~radford/ftp/thesis.pdf},
    urldate = {2014-11-21},
    school = {University of Toronto},
    author = {Neal, Radford M.},
    year = {1995}
}

@article{kingma2015variational,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	url = {http://arxiv.org/abs/1506.02557},
	urldate = {2015-06-10},
	journal = {arXiv:1506.02557 [cs, stat]},
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02557},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning}
}

@article{ahmed1989entropy,
	title = {Entropy expressions and their estimators for multivariate distributions},
	volume = {35},
	issn = {0018-9448},
	doi = {10.1109/18.30996},
	abstract = {Entropy expressions for several continuous multivariate distributions are derived. Point estimation of entropy for the multinormal distribution and for the distribution of order statistics from D.G. Weinman's (Ph.D dissertation, Ariz. State Univ., Tempe, AZ, 1966) exponential distribution is considered. The asymptotic distribution of the uniformly minimum variance unbiased estimator for multinormal entropy is obtained. Simulation results on convergence of the means and variances of these estimators are provided},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Ahmed, N. A. and Gokhale, D. V.},
	month = may,
	year = {1989},
	keywords = {asymptotic distribution, continuous multivariate distributions, convergence, Density functional theory, Density measurement, entropy, exponential distribution, Gaussian distribution, information theory, multinormal distribution, multinormal entropy, order statistics distribution, parameter estimation, point estimation, Random variables, statistical analysis, Statistical distributions, statistical inference, Testing, uniformly minimum variance unbiased estimator},
	pages = {688--692}
}

@article{xu2014classifier,
  title={Classifier cascades and trees for minimizing feature evaluation cost},
  author={Xu, Zhixiang and Kusner, Matt J and Weinberger, Kilian Q and Chen, Minmin and Chapelle, Olivier},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2113--2144},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{kusner2014feature,
  title={Feature-Cost Sensitive Learning with Submodular Trees of Classifiers.},
  author={Kusner, Matt J and Chen, Wenlin and Zhou, Quan and Xu, Zhixiang Eddie and Weinberger, Kilian Q and Chen, Yixin},
  booktitle={AAAI},
  pages={1939--1945},
  year={2014}
}

@article{krause2012near,
  title={Near-optimal nonmyopic value of information in graphical models},
  author={Krause, Andreas and Guestrin, Carlos E},
  journal={arXiv preprint arXiv:1207.1394},
  year={2012}
}

@article{lucas2015designing,
  title={Designing optimal greenhouse gas observing networks that consider performance and cost},
  author={Lucas, DD and Kwok, C Yver and Cameron-Smith, P and Graven, H and Bergmann, D and Guilderson, TP and Weiss, R and Keeling, R},
  journal={Geoscientific Instrumentation, Methods and Data Systems},
  volume={4},
  number={1},
  pages={121},
  year={2015},
  publisher={Copernicus GmbH}
}

@article{mackay1992bayesian,
  title={Bayesian interpolation},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={3},
  pages={415--447},
  year={1992},
  publisher={MIT Press}
}

@Article{vgg,
    author       = "Simonyan, K. and Zisserman, A.",
    title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    journal      = "CoRR",
    volume       = "abs/1409.1556",
    year         = "2014"
}

@inproceedings{schulman2015gradient,
  title={Gradient estimation using stochastic computation graphs},
  author={Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3528--3536},
  year={2015}
}

@article{tsne,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{ixmas,
  author = {Weinland, Daniel and Ronfard, Remi and Boyer, Edmond},
  title = {Free Viewpoint Action Recognition using Motion History Volumes},
  journal = {Computer Vision and Image Understanding},
  year = {2006},
  volume = {104},
  pages = {249--257},
  number = {2-3}
}

@article{richman2016allocate,
  title={How to Allocate Resources For Features Acquisition?},
  author={Richman, Oran and Mannor, Shie},
  journal={arXiv preprint arXiv:1607.02763},
  year={2016}
}

@misc{lasagne,
  author       = {Sander Dieleman and
                  Jan Schlüter and
                  Colin Raffel and
                  Eben Olson and
                  Søren Kaae Sønderby and
                  Daniel Nouri and
                  Daniel Maturana and
                  Martin Thoma and
                  Eric Battenberg and
                  Jack Kelly and
                  Jeffrey De Fauw and
                  Michael Heilman and
                  Diogo Moitinho de Almeida and
                  Brian McFee and
                  Hendrik Weideman and
                  Gábor Takács and
                  Peter de Rivaz and
                  Jon Crall and
                  Gregory Sanders and
                  Kashif Rasul and
                  Cong Liu and
                  Geoffrey French and
                  Jonas Degrave},
  title        = {Lasagne: First release.},
  month        = aug,
  year         = 2015,
  doi          = {10.5281/zenodo.27878},
  url          = {http://dx.doi.org/10.5281/zenodo.27878}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	urldate = {2015-05-25},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@ARTICLE{maddison_concrete_2016,
   author = {{Maddison}, C.~J. and {Mnih}, A. and {Whye Teh}, Y.},
    title = "{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.00712},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Statistics - Machine Learning},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161100712M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{gneiting2007strictly,
  title={Strictly proper scoring rules, prediction, and estimation},
  author={Gneiting, Tilmann and Raftery, Adrian E},
  journal={Journal of the American statistical Association},
  volume={102},
  number={477},
  pages={359--378},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{brier1950verification,
  title={Verification of forecasts expressed in terms of probability},
  author={Brier, Glenn W},
  journal={Monthly weather review},
  volume={78},
  number={1},
  pages={1--3},
  year={1950},
  publisher={American Meteorological Society}
}

@ARTICLE{jang_categorical_2016,
   author = {{Jang}, E. and {Gu}, S. and {Poole}, B.},
    title = "{Categorical Reparameterization with Gumbel-Softmax}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.01144},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161101144J},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{gao_variational_2016,
   author = {{Gao}, S. and {Ver Steeg}, G. and {Galstyan}, A.},
    title = "{Variational Information Maximization for Feature Selection}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.02827},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160602827G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{gentle2004random,
  title={Random Number Generation and Monte Carlo Methods},
  author={Gentle, J.E.},
  isbn={9780387001784},
  lccn={2003042437},
  series={Statistics and Computing},
  url={https://books.google.co.uk/books?id=8sV\_nuXolycC},
  year={2004},
  publisher={Springer New York}
}

@book{hájek1981sampling,
  title={Sampling from a finite population},
  author={H{\'a}jek, J. and Dupa{\v{c}}, V.},
  isbn={9780824712914},
  lccn={81007835},
  series={Statistics, textbooks and monographs},
  url={https://books.google.co.uk/books?id=DxHvAAAAMAAJ},
  year={1981},
  publisher={M. Dekker}
}

@Article{Kochar2001,
author="Kochar, Subhash C.
and Korwar, Ramesh",
title="On Random Sampling Without Replacement from a Finite Population",
journal="Annals of the Institute of Statistical Mathematics",
year="2001",
volume="53",
number="3",
pages="631--646",
issn="1572-9052",
doi="10.1023/A:1014693702392",
url="http://dx.doi.org/10.1023/A:1014693702392"
}

@article{Traat2004395,
title = "Sampling design and sample selection through distribution theory ",
journal = "Journal of Statistical Planning and Inference ",
volume = "123",
number = "2",
pages = "395 - 413",
year = "2004",
note = "",
issn = "0378-3758",
doi = "http://dx.doi.org/10.1016/S0378-3758(03)00150-2",
url = "//www.sciencedirect.com/science/article/pii/S0378375803001502",
author = "Imbi Traat and Lennart Bondesson and Kadri Meister",
keywords = "Multivariate Bernoulli design",
keywords = "Multinomial design",
keywords = "Hypergeometric design",
keywords = "Conditional Poisson design",
keywords = "Sampford design",
keywords = "Order sampling design",
keywords = "List-sequential sampling",
keywords = "Markov chain Monte Carlo",
keywords = "Gibbs sampling "
}

@incollection{swersky2012nchoosek,
title = {Probabilistic n-Choose-k Models for Classification and Ranking},
author = {Swersky, Kevin and Frey, Brendan J and Tarlow, Daniel and Richard S. Zemel and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {3050--3058},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4702-probabilistic-n-choose-k-models-for-classification-and-ranking.pdf}
}

@inproceedings{tarlow2012fast,
 author = {Tarlow, Daniel and Swersky, Kevin and Zemel, Richard S. and Adams, Ryan P. and Frey, Brendan J.},
 title = {Fast Exact Inference for Recursive Cardinality Models},
 booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'12},
 year = {2012},
 isbn = {978-0-9749039-8-9},
 location = {Catalina Island, CA},
 pages = {825--834},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3020652.3020738},
 acmid = {3020738},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
}

@incollection{Ghahramani2006Infinite,
title = {Infinite latent feature models and the Indian buffet process},
author = {Ghahramani, Zoubin and Thomas L. Griffiths},
booktitle = {Advances in Neural Information Processing Systems 18},
editor = {Y. Weiss and P. B. Sch\"{o}lkopf and J. C. Platt},
pages = {475--482},
year = {2006},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2882-infinite-latent-feature-models-and-the-indian-buffet-process.pdf}
}

@article{Kulesza2012Determinantal,
url = {http://dx.doi.org/10.1561/2200000044},
year = {2012},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Determinantal Point Processes for Machine Learning},
doi = {10.1561/2200000044},
issn = {1935-8237},
number = {2–3},
pages = {123-286},
author = {Alex Kulesza and Ben Taskar}
}

@ARTICLE{molchanov2017variational,
   author = {{Molchanov}, D. and {Ashukha}, A. and {Vetrov}, D.},
    title = "{Variational Dropout Sparsifies Deep Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.05369},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170105369M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{srinivas2016generalized,
   author = {{Srinivas}, S. and {Venkatesh Babu}, R.},
    title = "{Generalized Dropout}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.06791},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161106791S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{gumbel1954statistical,
  title={Statistical theory of extreme values and some practical applications: a series of lectures},
  author={Gumbel, E.J.},
  lccn={54061179},
  issn={1049-4685},
  series={Applied mathematics series},
  url={https://books.google.co.uk/books?id=SNpJAAAAMAAJ},
  year={1954},
  publisher={U. S. Govt. Print. Office}
}

@ARTICLE{shazeer2017outrageously,
   author = {{Shazeer}, N. and {Mirhoseini}, A. and {Maziarz}, K. and {Davis}, A. and 
	{Le}, Q. and {Hinton}, G. and {Dean}, J.},
    title = "{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.06538},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170106538S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{jacobs1991moe,
 author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
 title = {Adaptive Mixtures of Local Experts},
 journal = {Neural Comput.},
 issue_date = {Spring 1991},
 volume = {3},
 number = {1},
 month = mar,
 year = {1991},
 issn = {0899-7667},
 pages = {79--87},
 numpages = {9},
 url = {http://dx.doi.org/10.1162/neco.1991.3.1.79},
 doi = {10.1162/neco.1991.3.1.79},
 acmid = {1351018},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{jordan1994hierarchical,
 author = {Jordan, Michael I. and Jacobs, Robert A.},
 title = {Hierarchical Mixtures of Experts and the EM Algorithm},
 journal = {Neural Comput.},
 issue_date = {March 1994},
 volume = {6},
 number = {2},
 month = mar,
 year = {1994},
 issn = {0899-7667},
 pages = {181--214},
 numpages = {34},
 url = {http://dx.doi.org/10.1162/neco.1994.6.2.181},
 doi = {10.1162/neco.1994.6.2.181},
 acmid = {188106},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{rosen1997asymptotic,
title = "Asymptotic theory for order sampling",
journal = "Journal of Statistical Planning and Inference",
volume = "62",
number = "2",
pages = "135 - 158",
year = "1997",
note = "",
issn = "0378-3758",
doi = "http://dx.doi.org/10.1016/S0378-3758(96)00185-1",
url = "http://www.sciencedirect.com/science/article/pii/S0378375896001851",
author = "Bengt Rosén",
keywords = "Order sampling",
keywords = "Successive sampling",
keywords = "Sequential Poisson sampling",
keywords = "Linear statistics",
keywords = "Asymptotic results"
}

@article{lees2010theoretical,
  title={Theoretical maximum capacity as benchmark for empty vehicle redistribution in personal rapid transit},
  author={Lees-Miller, John and Hammersley, John and Wilson, R},
  journal={Transportation Research Record: Journal of the Transportation Research Board},
  number={2146},
  pages={76--83},
  year={2010},
  publisher={Transportation Research Board of the National Academies}
}

@misc{dziugaite,
      title={Training generative neural networks via Maximum Mean Discrepancy optimization}, 
      author={Gintare Karolina Dziugaite and Daniel M. Roy and Zoubin Ghahramani},
      year={2015},
      eprint={1505.03906},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{GaussianRKHS,
title={Some Properties of Gaussian Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory},
  author={Minh, Ha Quang},
  journal={Constructive Approximation},
  number={32},
  pages={307–338},
  year={2010}
}


@article{visual_chiral,
   title={Visual Chirality},
   ISBN={9781728171685},
   url={http://dx.doi.org/10.1109/CVPR42600.2020.01231},
   DOI={10.1109/cvpr42600.2020.01231},
   journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Lin, Zhiqiu and Sun, Jin and Davis, Abe and Snavely, Noah},
   year={2020},
   month={Jun}
}

@article{DeepKernelLearning,
  author    = {Andrew Gordon Wilson and
               Zhiting Hu and
               Ruslan Salakhutdinov and
               Eric P. Xing},
  title     = {Deep Kernel Learning},
  journal   = {CoRR},
  volume    = {abs/1511.02222},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.02222},
  archivePrefix = {arXiv},
  eprint    = {1511.02222},
  timestamp = {Mon, 13 Aug 2018 16:47:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WilsonHSX15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mcgan,
  author    = {Youssef Mroueh and
               Tom Sercu and
               Vaibhava Goel},
  title     = {McGan: Mean and Covariance Feature Matching {GAN}},
  journal   = {CoRR},
  volume    = {abs/1702.08398},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.08398},
  archivePrefix = {arXiv},
  eprint    = {1702.08398},
  timestamp = {Mon, 13 Aug 2018 16:47:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MrouehSG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{kingmaVAE,
    title={Auto-Encoding Variational Bayes},
    author={Diederik P Kingma and Max Welling},
    year={2013},
    eprint={1312.6114},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{DenoisingDP,
  title={Denoising Diffusion Probabilistic Models},
  author={Jonathan Ho and Ajay Jain and P. Abbeel},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.11239}
}

@article{likelhood_gan,
  author    = {Hamid Eghbal{-}zadeh and
               Gerhard Widmer},
  title     = {Likelihood Estimation for Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1707.07530},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.07530},
  archivePrefix = {arXiv},
  eprint    = {1707.07530},
  timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Eghbal-zadehW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{flowGAN,
  author    = {Aditya Grover and
               Manik Dhar and
               Stefano Ermon},
  title     = {Flow-GAN: Bridging implicit and prescribed learning in generative
               models},
  journal   = {CoRR},
  volume    = {abs/1705.08868},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.08868},
  archivePrefix = {arXiv},
  eprint    = {1705.08868},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GroverDE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{parzen1962,
author = "Parzen, Emanuel",
doi = "10.1214/aoms/1177704472",
fjournal = "Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "09",
number = "3",
pages = "1065--1076",
publisher = "The Institute of Mathematical Statistics",
title = "On Estimation of a Probability Density Function and Mode",
url = "https://doi.org/10.1214/aoms/1177704472",
volume = "33",
year = "1962"
}


@article{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
year = {2012},
month = {01},
pages = {},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
volume = {25},
journal = {Neural Information Processing Systems},
doi = {10.1145/3065386}
}

@misc{MMDgradient,
    title={On gradient regularizers for MMD GANs},
    author={Michael Arbel and Dougal J. Sutherland and Mikołaj Bińkowski and Arthur Gretton},
    year={2018},
    eprint={1805.11565},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{wgan-gp,
    title={Improved Training of Wasserstein GANs},
    author={Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
    year={2017},
    eprint={1704.00028},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{arjovsky2017principled,
      title={Towards Principled Methods for Training Generative Adversarial Networks}, 
      author={Martin Arjovsky and Léon Bottou},
      year={2017},
      eprint={1701.04862},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{song2,
author="Smola, Alex
and Gretton, Arthur
and Song, Le
and Sch{\"o}lkopf, Bernhard",
editor="Hutter, Marcus
and Servedio, Rocco A.
and Takimoto, Eiji",
title="A Hilbert Space Embedding for Distributions",
booktitle="Algorithmic Learning Theory",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="13--31",
abstract="We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a reproducing kernel Hilbert space. Applications of this technique can be found in two-sample tests, which are used for determining whether two sets of observations arise from the same distribution, covariate shift correction, local learning, measures of independence, and density estimation.",
isbn="978-3-540-75225-7"
}

@Misc{songthesis,
title = { Learning via Hilbert space embedding of distributions },
author = { Song Lê and University of Sydney. School of Information Technologies },
year = { 2008 },
type = { Thesis },
subjects = { Hilbert space; Probabilities },
language = { English },
note = { Includes graphs and tables },
catalogue-url = { https://trove.nla.gov.au/work/27382824 }
}

@Misc{bochner_notes,
title = {Gaussian measures and Bochner’s theorem},
author = { Jordan Bell, Department of Mathematics, University of Toronto
},
year = { 2015 },
url = {http://individual.utoronto.ca/jordanbell/notes/bochnertheorem.pdf}
}

@incollection{blockVar,
title = {B-test: A Non-parametric, Low Variance Kernel Two-sample Test},
author = {Zaremba, Wojciech and Gretton, Arthur and Blaschko, Matthew},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {755--763},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5081-b-test-a-non-parametric-low-variance-kernel-two-sample-test.pdf}
}

@article{memory_gan,
  author    = {Youngjin Kim and
               Minjung Kim and
               Gunhee Kim},
  title     = {Memorization Precedes Generation: Learning Unsupervised GANs with
               Memory Networks},
  journal   = {CoRR},
  volume    = {abs/1803.01500},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.01500},
  archivePrefix = {arXiv},
  eprint    = {1803.01500},
  timestamp = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-01500.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{norm_flow,

  author={I. {Kobyzev} and S. {Prince} and M. {Brubaker}},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Normalizing Flows: An Introduction and Review of Current Methods}, 

  year={2020},

  volume={},

  number={},

  pages={1-1},}
  
 @inproceedings{vaegan,
  title     = {VAEGAN: A Collaborative Filtering Framework based on Adversarial Variational Autoencoders},
  author    = {Yu, Xianwen and Zhang, Xiaoning and Cao, Yang and Xia, Min},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {4206--4212},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/584},
  url       = {https://doi.org/10.24963/ijcai.2019/584},
}


@article{flow++images,
  author    = {Jonathan Ho and
               Xi Chen and
               Aravind Srinivas and
               Yan Duan and
               Pieter Abbeel},
  title     = {Flow++: Improving Flow-Based Generative Models with Variational Dequantization
               and Architecture Design},
  journal   = {CoRR},
  volume    = {abs/1902.00275},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.00275},
  archivePrefix = {arXiv},
  eprint    = {1902.00275},
  timestamp = {Tue, 21 May 2019 18:03:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-00275.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ganReviewCreswell,
  author    = {Antonia Creswell and
               Tom White and
               Vincent Dumoulin and
               Kai Arulkumaran and
               Biswa Sengupta and
               Anil A. Bharath},
  title     = {Generative Adversarial Networks: An Overview},
  journal   = {CoRR},
  volume    = {abs/1710.07035},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.07035},
  archivePrefix = {arXiv},
  eprint    = {1710.07035},
  timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-07035.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{progan,
  author    = {Tero Karras and
               Timo Aila and
               Samuli Laine and
               Jaakko Lehtinen},
  title     = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  journal   = {CoRR},
  volume    = {abs/1710.10196},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10196},
  archivePrefix = {arXiv},
  eprint    = {1710.10196},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-10196.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cogan,
  author    = {Ming{-}Yu Liu and
               Oncel Tuzel},
  title     = {Coupled Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1606.07536},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.07536},
  archivePrefix = {arXiv},
  eprint    = {1606.07536},
  timestamp = {Mon, 13 Aug 2018 16:46:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/0001T16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cyclegan,
  author    = {Jun{-}Yan Zhu and
               Taesung Park and
               Phillip Isola and
               Alexei A. Efros},
  title     = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial
               Networks},
  journal   = {CoRR},
  volume    = {abs/1703.10593},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.10593},
  archivePrefix = {arXiv},
  eprint    = {1703.10593},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhuPIE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cgan,
  author    = {Mehdi Mirza and
               Simon Osindero},
  title     = {Conditional Generative Adversarial Nets},
  journal   = {CoRR},
  volume    = {abs/1411.1784},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.1784},
  archivePrefix = {arXiv},
  eprint    = {1411.1784},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MirzaO14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{dagan,
title={Data Augmentation Generative Adversarial Networks},
author={Anthreas Antoniou and Amos Storkey and Harrison Edwards},
year={2018},
url={https://openreview.net/forum?id=S1Auv-WRZ},
}

@misc{kantorovich,
title={A Hitchhiker’s guide to Wasserstein distances},
author={Giuliano Basso},
year={2015},
url={http://n.ethz.ch/~gbasso/download/A\%20Hitchhikers\%20guide\%20to\%20Wasserstein/A\%20Hitchhikers\%20guide\%20to\%20Wasserstein.pdf},
}

@misc{huffman,
title={Entropy and Huffman Codes},
author={Todd Ebert},
year={2015},
url={https://web.csulb.edu/~tebert/teaching/lectures/528/huffman/huffman.pdf},
}

@inbook{rasmussen_williams_2008, place={Cambridge, MA}, title={Relationship between GPs and other Models}, booktitle={Gaussian processes for machine learning}, publisher={MIT Press}, author={Rasmussen, Carl Edward and Williams, Christopher K. I.}, year={2008}} 

 @inbook{wavelets, place={Boca Raton, FL}, title={Chapter 1}, booktitle={A first course on wavelets}, publisher={CRC Press}, author={Hernández Eugenio and Weiss, Guido}, year={1996}} 

@misc{2020review,
      title={A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications}, 
      author={Jie Gui and Zhenan Sun and Yonggang Wen and Dacheng Tao and Jieping Ye},
      year={2020},
      eprint={2001.06937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lle,
title={Nonlinear dimensionality reduction by locally linear embedding},
author={Sam Roweis},
year={2000}
}

@misc{manifoldhypothesis,
      title={Testing the Manifold Hypothesis}, 
      author={Charles Fefferman and Sanjoy Mitter and Hariharan Narayanan},
      year={2013},
      eprint={1310.0425},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}


@article{princpalCurves,
author = { Trevor   Hastie  and  Werner   Stuetzle },
title = {Principal Curves},
journal = {Journal of the American Statistical Association},
volume = {84},
number = {406},
pages = {502-516},
year  = {1989},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1989.10478797},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478797
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1989.10478797
    
}

}

@article{nlpca,
    author = {Scholz, Matthias and Kaplan, Fatma and Guy, Charles L. and Kopka, Joachim and Selbig, Joachim},
    title = "{Non-linear PCA: a missing data approach}",
    journal = {Bioinformatics},
    volume = {21},
    number = {20},
    pages = {3887-3895},
    year = {2005},
    month = {08},
    abstract = "{Motivation: Visualizing and analysing the potential non-linear structure of a dataset is becoming an important task in molecular biology. This is even more challenging when the data have missing values.Results: Here, we propose an inverse model that performs non-linear principal component analysis (NLPCA) from incomplete datasets. Missing values are ignored while optimizing the model, but can be estimated afterwards. Results are shown for both artificial and experimental datasets. In contrast to linear methods, non-linear methods were able to give better missing value estimations for non-linear structured data.Application: We applied this technique to a time course of metabolite data from a cold stress experiment on the model plant Arabidopsis thaliana, and could approximate the mapping function from any time point to the metabolite responses. Thus, the inverse NLPCA provides greatly improved information for better understanding the complex response to cold stress.Contact:scholz@mpimp-golm.mpg.de}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bti634},
    url = {https://doi.org/10.1093/bioinformatics/bti634},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/21/20/3887/524944/bti634.pdf},
}

@article{KernelPCA,
author = {Schölkopf, Bernhard and Smola, Alex and Müller, Klaus-Robert},
year = {1998},
month = {07},
pages = {1299-1319},
title = {Nonlinear Component Analysis as a Kernel Eigenvalue Problem},
volume = {10},
journal = {Neural Computation},
doi = {10.1162/089976698300017467}
}

@misc{gansEqual,
      title={Are GANs Created Equal? A Large-Scale Study}, 
      author={Mario Lucic and Karol Kurach and Marcin Michalski and Sylvain Gelly and Olivier Bousquet},
      year={2018},
      eprint={1711.10337},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

  
@ARTICLE{div-functionals,

  author={X. {Nguyen} and M. J. {Wainwright} and M. I. {Jordan}},

  journal={IEEE Transactions on Information Theory}, 

  title={Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization}, 

  year={2010},

  volume={56},

  number={11},

  pages={5847-5861},

  doi={10.1109/TIT.2010.2068870}}

@ARTICLE{relative,
       author = {{Jolicoeur-Martineau}, Alexia},
        title = "{The relativistic discriminator: a key element missing from standard GAN}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Statistics - Machine Learning},
         year = 2018,
        month = jul,
          eid = {arXiv:1807.00734},
        pages = {arXiv:1807.00734},
archivePrefix = {arXiv},
       eprint = {1807.00734},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180700734J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{optimalKernel,
 author = {Gretton, Arthur and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Fukumizu, Kenji and Sriperumbudur, Bharath K.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {1205--1213},
 publisher = {Curran Associates, Inc.},
 title = {Optimal kernel choice for large-scale two-sample tests},
 url = {https://proceedings.neurips.cc/paper/2012/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf},
 volume = {25},
 year = {2012}
}


@misc{deepkernel-nastygretton,
      title={Learning Deep Kernels for Non-Parametric Two-Sample Tests}, 
      author={Feng Liu and Wenkai Xu and Jie Lu and Guangquan Zhang and Arthur Gretton and D. J. Sutherland},
      year={2020},
      eprint={2002.09116},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{li2019implicit,
      title={Implicit Kernel Learning}, 
      author={Chun-Liang Li and Wei-Cheng Chang and Youssef Mroueh and Yiming Yang and Barnabás Póczos},
      year={2019},
      eprint={1902.10214},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{noise-contrastive-est, title = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models}, author = {Michael Gutmann and Aapo Hyvärinen}, booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = {297--304}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf}, url = {http://proceedings.mlr.press/v9/gutmann10a.html}, abstract = {} }

@misc{gui2020review,
      title={A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications}, 
      author={Jie Gui and Zhenan Sun and Yonggang Wen and Dacheng Tao and Jieping Ye},
      year={2020},
      eprint={2001.06937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{onlineKernelGAN,
      title={Online Kernel based Generative Adversarial Networks}, 
      author={Yeojoon Youn and Neil Thistlethwaite and Sang Keun Choe and Jacob Abernethy},
      year={2020},
      eprint={2006.11432},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{EnergyStats,
  author    = {Gábor J. Székely, Maria L. Rizzo},
  title     = {Energy statistics: A class of statistics based on distances},
  journal   = {Journal of Statistical Planning and Inference},
  volume    = {143},
  year      = {2002},
  url       = {https://www.sciencedirect.com/science/article/pii/S0378375813000633},
}



@article{CalibrationSurvey,
  author    = {Telmo de Menezes e Silva Filho and
               Hao Song and
               Miquel Perell{\'{o}}{-}Nieto and
               Ra{\'{u}}l Santos{-}Rodr{\'{\i}}guez and
               Meelis Kull and
               Peter A. Flach},
  title     = {Classifier Calibration: How to Assess and Improve Predicted Class
               Probabilities: a Survey},
  journal   = {CoRR},
  volume    = {abs/2112.10327},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.10327},
  eprinttype = {arXiv},
  eprint    = {2112.10327},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-10327.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{universalKernels,
author = {Micchelli, Charles and Xu, Yuesheng and Zhang, Haizhang},
year = {2006},
month = {12},
pages = {},
title = {Universal Kernels},
volume = {7},
journal = {Mathematics}
}


@article{deng2012mnist,
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [best of the web]},
  author={Deng, Li},
  journal={IEEE signal processing magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}


@inproceedings{Taylor_c,
author = {Feng, Lei and Shu, Senlin and Lin, Zhuoyi and Lv, Fengmao and Li, Li and An, Bo},
title = {Can Cross Entropy Loss Be Robust to Label Noise?},
year = {2021},
isbn = {9780999241165},
abstract = {Trained with the standard cross entropy loss, deep neural networks can achieve great performance on correctly labeled data. However, if the training data is corrupted with label noise, deep models tend to overfit the noisy labels, thereby achieving poor generation performance. To remedy this issue, several loss functions have been proposed and demonstrated to be robust to label noise. Although most of the robust loss functions stem from Categorical Cross Entropy (CCE) loss, they fail to embody the intrinsic relationships between CCE and other loss functions. In this paper, we propose a general framework dubbed Taylor cross entropy loss to train deep models in the presence of label noise. Specifically, our framework enables to weight the extent of fitting the training labels by controlling the order of Taylor Series for CCE, hence it can be robust to label noise. In addition, our framework clearly reveals the intrinsic relationships between CCE and other loss functions, such as Mean Absolute Error (MAE) and Mean Squared Error (MSE). Moreover, we present a detailed theoretical analysis to certify the robustness of this framework. Extensive experimental results on benchmark datasets demonstrate that our proposed approach significantly outperforms the state-of-the-art counterparts.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {305},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{xiao2017fashion,
  title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}




@article{elr,
  title={Early-learning Regularization Prevents Memorization of Noisy Labels},
  author={Liu, Sheng and Niles-Weed, Jonathan and Razavian, Narges and Fernandez-Granda, Carlos},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20331--20342},
  year={2020}
}


@article{secost,
  title={Secost: Sequential Co-Supervision for Weakly Labeled Audio Event Detection},
  author={Kumar, Anurag and Ithapu, Vamsi Krishna},
  year={2020}
}


@article{cleanLab,
  title={Confident Learning: Estimating Uncertainty in Dataset Labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}


@inproceedings{splitSelect,
  title={Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019},
  organization={PMLR}
}

@misc{truncated_github,
  author = {Alan Chou},
  title = {Truncated Losses},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AlanChou/Truncated-Loss/}},
  commit = {93a2464}
}



@misc{gjs_github,
  author = {Erik Englesson},
  title = {Truncated Losses},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/ErikEnglesson/GJS/}}
} 



@article{robustLR,
  title={Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning with Label Noise},
  author={Chen, Mingcai and Cheng, Hao and Du, Yuntao and Xu, Ming and Jiang, Wenyu and Wang, Chongjun},
  journal={arXiv preprint arXiv:2112.02960},
  year={2021}
}


@article{entropyReg,
  title={Regularizing Neural Networks by Penalizing Confident Output Distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}


@article{robustLR,
  title={Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning with Label Noise},
  author={Chen, Mingcai and Cheng, Hao and Du, Yuntao and Xu, Ming and Jiang, Wenyu and Wang, Chongjun},
  journal={arXiv preprint arXiv:2112.02960},
  year={2021}
}



@article{entropyReg,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}

@incollection{survey2020,
  title={Impact of noisy labels in learning techniques: a survey},
  author={Nigam, Nitika and Dutta, Tanima and Gupta, Hari Prabhat},
  booktitle={Advances in data and information sciences},
  pages={403--411},
  year={2020},
  publisher={Springer}
}



@article{universalKernels,
author = {Micchelli, Charles and Xu, Yuesheng and Zhang, Haizhang},
year = {2006},
month = {12},
pages = {},
title = {Universal Kernels},
volume = {7},
journal = {Mathematics}
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research [best of the web]},
  author={Deng, Li},
  journal={IEEE signal processing magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}


@article{active,
  title={Active bias: Training more accurate neural networks by emphasizing high variance samples},
  author={Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@article{pred_var,
  title={Active bias: Training more accurate neural networks by emphasizing high variance samples},
  author={Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@article{secost,
  title={Secost: Sequential co-supervision for weakly labeled audio event detection},
  author={Kumar, Anurag and Ithapu, Vamsi Krishna},
  year={2020}
}

@article{cleanLab,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}


@inproceedings{splitSelect,
  title={Understanding and utilizing deep neural networks trained with noisy labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019},
  organization={PMLR}
}

@misc{truncated_github,
  author = {Alan Chou},
  title = {Truncated Losses},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AlanChou/Truncated-Loss/}},
  commit = {93a2464}
}



@article{yao2020dual,
  title={Dual t: Reducing estimation error for transition matrix in label-noise learning},
  author={Yao, Yu and Liu, Tongliang and Han, Bo and Gong, Mingming and Deng, Jiankang and Niu, Gang and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7260--7271},
  year={2020}
}


@inproceedings{labelnoiseES,
  title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
  author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  booktitle={International conference on artificial intelligence and statistics},
  pages={4313--4324},
  year={2020},
  organization={PMLR}
}

@article{epochwise,
  title={When and how epochwise double descent happens},
  author={Stephenson, Cory and Lee, Tyler},
  journal={arXiv preprint arXiv:2108.12006},
  year={2021}
}

@article{nakkiran2021deep_epochwise_orig,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}

@article{SVM_orig,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@inproceedings{votedPerceptron,
  author    = {Yoav Freund and Robert E. Schapire},
  title     = {Large Margin Classification Using the Perceptron Algorithm},
  booktitle = {Machine Learning: Proceedings of the Eleventh Annual Conference},
  year      = {1999},
  pages     = {209--217},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address   = {San Francisco, CA, USA}
}


@article{gouk2021regularisation,
  title={Regularisation of neural networks by enforcing lipschitz continuity},
  author={Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael J},
  journal={Machine Learning},
  volume={110},
  pages={393--416},
  year={2021},
  publisher={Springer}
}

@inproceedings{bootkrajang2012label,
  title={Label-noise robust logistic regression and its applications},
  author={Bootkrajang, Jakramate and Kab{\'a}n, Ata},
  booktitle={Joint European conference on machine learning and knowledge discovery in databases},
  pages={143--158},
  year={2012},
  organization={Springer}
}


@InProceedings{trimmedLoss,
  title = 	 {Learning with Bad Training Data via Iterative Trimmed Loss Minimization},
  author =       {Shen, Yanyao and Sanghavi, Sujay},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5739--5748},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/shen19e/shen19e.pdf},
  url = 	 {https://proceedings.mlr.press/v97/shen19e.html},
  abstract = 	 {In this paper, we study a simple and generic framework to tackle the problem of learning model parameters when a fraction of the training samples are corrupted. Our approach is motivated by a simple observation: in a variety of such settings, the evolution of training accuracy (as a function of training epochs) is different for clean samples and bad samples. We propose to iteratively minimize the trimmed loss, by alternating between (a) selecting samples with lowest current loss, and (b) retraining a model on only these samples. Analytically, we characterize the statistical performance and convergence rate of the algorithm for simple and natural linear and non-linear models. Experimentally, we demonstrate its effectiveness in three settings: (a) deep image classifiers with errors only in labels, (b) generative adversarial networks with bad training images, and (c) deep image classifiers with adversarial (image, label) pairs (i.e., backdoor attacks). For the well-studied setting of random label noise, our algorithm achieves state-of-the-art performance without having access to any a-priori guaranteed clean samples.}
}



@InProceedings{pmlr-v119-harutyunyan20a,
  title = 	 {Improving generalization by controlling label-noise information in neural network weights},
  author =       {Harutyunyan, Hrayr and Reing, Kyle and Steeg, Greg Ver and Galstyan, Aram},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4071--4081},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/harutyunyan20a/harutyunyan20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/harutyunyan20a.html},
  abstract = 	 {In the presence of noisy or incorrect labels, neural networks have the undesirable tendency to memorize information about the noise. Standard regularization techniques such as dropout, weight decay or data augmentation sometimes help, but do not prevent this behavior. If one considers neural network weights as random variables that depend on the data and stochasticity of training, the amount of memorized information can be quantified with the Shannon mutual information between weights and the vector of all training labels given inputs, $I(w; \mathbf{y} \mid \mathbf{x})$. We show that for any training algorithm, low values of this term correspond to reduction in memorization of label-noise and better generalization bounds. To obtain these low values, we propose training algorithms that employ an auxiliary network that predicts gradients in the final layers of a classifier without accessing labels. We illustrate the effectiveness of our approach on versions of MNIST, CIFAR-10, and CIFAR-100 corrupted with various noise models, and on a large-scale dataset Clothing1M that has noisy labels.}
}

@article{shanthini2019taxonomy,
  title={A taxonomy on impact of label noise and feature noise using machine learning techniques},
  author={Shanthini, A and Vinodhini, G and Chandrasekaran, RM and Supraja, P},
  journal={Soft Computing},
  volume={23},
  pages={8597--8607},
  year={2019},
  publisher={Springer}
}

@article{prestopping,
  title={How does Early Stopping Help Generalization against Label Noise? arXiv 2019},
  author={Song, H and Kim, M and Park, D and Lee, JG},
year = {2019},
  journal={arXiv preprint arXiv:1911.08059}
}

@article{robust_for_small_noise,
  title={Deep learning is robust to massive label noise},
  author={Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
  journal={arXiv preprint arXiv:1705.10694},
  year={2017}
}

@inproceedings{taylorce,
  title={Can cross entropy loss be robust to label noise?},
  author={Feng, Lei and Shu, Senlin and Lin, Zhuoyi and Lv, Fengmao and Li, Li and An, Bo},
  booktitle={Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence},
  pages={2206--2212},
  year={2021}
}

@ARTICLE{surveyDeep,
  author={Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Learning From Noisy Labels With Deep Neural Networks: A Survey}, 
  year={2023},
  volume={34},
  number={11},
  pages={8135-8153},
  keywords={Noise measurement;Training;Deep learning;Training data;Task analysis;Taxonomy;Data models;Classification;deep learning;label noise;noisy label;robust deep learning;robust optimization;survey},
  doi={10.1109/TNNLS.2022.3152527}}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}


@InProceedings{ES_AdvRobust,
  title = 	 {Overfitting in adversarially robust deep learning},
  author =       {Rice, Leslie and Wong, Eric and Kolter, Zico},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8093--8104},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/rice20a/rice20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/rice20a.html},
  abstract = 	 {It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models (L-infinity and L-2). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting. Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at https://github.com/ locuslab/robust_overfitting.}
}

@article{angluin1988,
  title={Learning from noisy examples},
  author={Angluin, Dana and Laird, Philip},
  journal={Machine learning},
  volume={2},
  pages={343--370},
  year={1988},
  publisher={Springer}
}

@inproceedings{hard_sample2,
  title={Multiple instance learning framework with masked hard instance mining for whole slide image classification},
  author={Tang, Wenhao and Huang, Sheng and Zhang, Xiaoxian and Zhou, Fengtao and Zhang, Yi and Liu, Bo},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4078--4087},
  year={2023}
}


@InProceedings{hard_sample,
author = {Suh, Yumin and Han, Bohyung and Kim, Wonsik and Lee, Kyoung Mu},
title = {Stochastic Class-Based Hard Example Mining for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
} 

@inproceedings{old_backward,
  title={Learning SVMs from sloppily labeled data},
  author={Stempfel, Guillaume and Ralaivola, Liva},
  booktitle={Artificial Neural Networks--ICANN 2009: 19th International Conference, Limassol, Cyprus, September 14-17, 2009, Proceedings, Part I 19},
  pages={884--893},
  year={2009},
  organization={Springer}
}

@article{labelFlipNaive,
  title={Label flipping attacks against Naive Bayes on spam filtering systems},
  author={Zhang, Hongpo and Cheng, Ning and Zhang, Yang and Li, Zhanbo},
  journal={Applied Intelligence},
  volume={51},
  number={7},
  pages={4503--4514},
  year={2021},
  publisher={Springer}
}


@INPROCEEDINGS{noiseFE,

  author={Pechenizkiy, M. and Tsymbal, A. and Puuronen, S. and Pechenizkiy, O.},

  booktitle={19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06)}, 

  title={Class Noise and Supervised Learning in Medical Domains: The Effect of Feature Extraction}, 

  year={2006},

  volume={},

  number={},

  pages={708-713},

  keywords={Supervised learning;Feature extraction;Medical diagnostic imaging;Learning systems;Iron;Delta modulation;Computer errors;Educational institutions;Data mining;Breast neoplasms},

  doi={10.1109/CBMS.2006.65}}


@inproceedings{noiseTolerantWindowing,
  title={Noise-tolerant windowing},
  author={F{\"u}rnkranz, Johannes},
  booktitle={IJCAI (2)},
  pages={852--859},
  year={1997},
  organization={Citeseer}
}

@article{nettleton2010study,
  title={A study of the effect of different types of noise on the precision of supervised learning techniques},
  author={Nettleton, David F and Orriols-Puig, Albert and Fornells, Albert},
  journal={Artificial intelligence review},
  volume={33},
  pages={275--306},
  year={2010},
  publisher={Springer}
}



@article{natarajan2013learning,
  title={Learning with noisy labels},
  author={Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@incollection{prechelt2002early,
  title={Early stopping-but when?},
  author={Prechelt, Lutz},
  booktitle={Neural Networks: Tricks of the trade},
  pages={55--69},
  year={2002},
  publisher={Springer}
}



@article{self_ensemble,
  title={Self: Learning to filter noisy labels with self-ensembling},
  author={Nguyen, Duc Tam and Mummadi, Chaithanya Kumar and Ngo, Thi Phuong Nhung and Nguyen, Thi Hoai Phuong and Beggel, Laura and Brox, Thomas},
  journal={arXiv preprint arXiv:1910.01842},
  year={2019}
}

@article{xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{FineTuneDARTS,
  title={Fine-tuning darts for image classification},
  author={Tanveer, Muhammad Suhaib and Khan, Muhammad Umar Karim and Kyung, Chong-Min},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={4789--4796},
  year={2021},
  organization={IEEE}
}

@inproceedings{selfPacedBoost,
  title={Self-paced boost learning for classification.},
  author={Pi, Te and Li, Xi and Zhang, Zhongfei and Meng, Deyu and Wu, Fei and Xiao, Jun and Zhuang, Yueting and others},
  booktitle={IJCAI},
  pages={1932--1938},
  year={2016}
}

@article{meng2015objective,
  title={What objective does self-paced learning indeed optimize?},
  author={Meng, Deyu and Zhao, Qian and Jiang, Lu},
  journal={arXiv preprint arXiv:1511.06049},
  year={2015}
}


@article{metaWeightNet,
  title={Meta-weight-net: Learning an explicit mapping for sample weighting},
  author={Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{ViT,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{cifar10-testloss,
author = {Jie, Renlong and Gao, Junbin and Vasnev, Andrey and Tran, Minh-Ngoc},
year = {2022},
month = {08},
pages = {},
title = {Adaptive hierarchical hyper-gradient descent},
volume = {13},
journal = {International Journal of Machine Learning and Cybernetics},
doi = {10.1007/s13042-022-01625-4}
}

@article{majidi2021exponentiated,
  title={Exponentiated gradient reweighting for robust training under label noise and beyond},
  author={Majidi, Negin and Amid, Ehsan and Talebi, Hossein and Warmuth, Manfred K},
  journal={arXiv preprint arXiv:2104.01493},
  year={2021}
}

@article{kumar2021constrained,
  title={Constrained instance and class reweighting for robust learning under label noise},
  author={Kumar, Abhishek and Amid, Ehsan},
  journal={arXiv preprint arXiv:2111.05428},
  year={2021}
}

@inproceedings{anchor,
  title={Provably end-to-end label-noise learning without anchor points},
  author={Li, Xuefeng and Liu, Tongliang and Han, Bo and Niu, Gang and Sugiyama, Masashi},
  booktitle={International conference on machine learning},
  pages={6403--6413},
  year={2021},
  organization={PMLR}
}

@article{importance_reweighting,
  title={Classification with noisy labels by importance reweighting},
  author={Liu, Tongliang and Tao, Dacheng},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={38},
  number={3},
  pages={447--461},
  year={2015},
  publisher={IEEE}
}

@article{early_stop_without_val,
  title={Early stopping without a validation set},
  author={Mahsereci, Maren and Balles, Lukas and Lassner, Christoph and Hennig, Philipp},
  journal={arXiv preprint arXiv:1703.09580},
  year={2017}
}

@article{fit_random_labels,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{gretton2008covariate,
  title={Covariate shift by kernel mean matching},
  author={Gretton, Arthur and Smola, Alex and Huang, Jiayuan and Schmittfull, Marcel and Borgwardt, Karsten and Sch{\"o}lkopf, Bernhard},
  year={2008}
}


@inproceedings{classicationVIAregression,
  title={Robust classification via regression for learning with noisy labels},
  author={Englesson, Erik and Azizpour, Hossein},
  booktitle={ICLR 2024-The Twelfth International Conference on Learning Representations, Messe Wien Exhibition and Congress Center, Vienna, Austria, May 7-11t, 2024},
  year={2024}
}


@inproceedings{group_noise,
  title={Learning with group noise},
  author={Wang, Qizhou and Yao, Jiangchao and Gong, Chen and Liu, Tongliang and Gong, Mingming and Yang, Hongxia and Han, Bo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={10192--10200},
  year={2021}
}


@inproceedings{robust_early_learning,
  title={Robust early-learning: hindering the memorization of noisy labels},
  author={Xia, Xiaobo and Liu, Tongliang and Han, Bo and Gong, Chen and Wang, Nannan and Ge, Zongyuan and Chang, Yi},
  booktitle={International Conference on Learning Representations 2021},
  pages={1--15},
  year={2021},
  organization={International Conference on Learning Representations (ICLR)}
}


@inproceedings{EarlyStoppingNoisy,
 author = {Bai, Yingbin and Yang, Erkun and Han, Bo and Yang, Yanhua and Li, Jiatong and Mao, Yinian and Niu, Gang and Liu, Tongliang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24392--24403},
 publisher = {Curran Associates, Inc.},
 title = {Understanding and Improving Early Stopping for Learning with Noisy Labels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf},
 volume = {34},
 year = {2021}
}


@ARTICLE{ident_matrix,
  author={Fu, Xiao and Huang, Kejun and Sidiropoulos, Nicholas D.},
  journal={IEEE Signal Processing Letters}, 
  title={On Identifiability of Nonnegative Matrix Factorization}, 
  year={2018},
  volume={25},
  number={3},
  pages={328-332},
  keywords={Matrix decomposition;Indexes;Data models;Sensors;Hyperspectral sensors;US Government;Science - general;Convex analysis;identifiability;nonnegative matrix factorization (NMF);sufficiently scattered},
  doi={10.1109/LSP.2018.2789405}
}


@inproceedings{l1RegTransition,
  title={Learning noise transition matrix from only noisy labels via total variation regularization},
  author={Zhang, Yivan and Niu, Gang and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={12501--12512},
  year={2021},
  organization={PMLR}
}

@article{imae,
  title={IMAE for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude's variance matters},
  author={Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Clifton, David A and Robertson, Neil M},
  journal={arXiv preprint arXiv:1903.12141},
  year={2019}
}

@book{norris1998markov,
  title={Markov Chains},
  author={Norris, J. R.},
  year={1998},
  publisher={Cambridge University Press},
  address={Cambridge}
}


@article{NTK,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}



@article{gold,
  title={Using trusted data to train deep networks on labels corrupted by severe noise},
  author={Hendrycks, Dan and Mazeika, Mantas and Wilson, Duncan and Gimpel, Kevin},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{metaLabels,
  title={Learning soft labels via meta learning},
  author={Vyas, Nidhi and Saxena, Shreyas and Voice, Thomas},
  journal={arXiv preprint arXiv:2009.09496},
  year={2020}
}

@article{T-revision,
  title={Are anchor points really indispensable in label-noise learning?},
  author={Xia, Xiaobo and Liu, Tongliang and Wang, Nannan and Han, Bo and Gong, Chen and Niu, Gang and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{capNetNoRouting,
  title={No routing needed between capsules},
  author={Byerly, Adam and Kalganova, Tatiana and Dear, Ian},
  journal={Neurocomputing},
  volume={463},
  pages={545--553},
  year={2021},
  publisher={Elsevier}
}


@article{robustLR,
  title={Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning with Label Noise},
  author={Chen, Mingcai and Cheng, Hao and Du, Yuntao and Xu, Ming and Jiang, Wenyu and Wang, Chongjun},
  journal={arXiv preprint arXiv:2112.02960},
  year={2021}
}




@article{entropyReg,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}



@article{universalKernels,
author = {Micchelli, Charles and Xu, Yuesheng and Zhang, Haizhang},
year = {2006},
month = {12},
pages = {},
title = {Universal Kernels},
volume = {7},
journal = {Mathematics}
}



@book{hastie2009elements,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  edition={2},
  publisher={Springer Series in Statistics},
  address={New York, NY, USA},
  year={2009},
  isbn={978-0-387-84857-0},
  note={Chapter 13 discusses the notion of dominant classifiers}
}

@inproceedings{KDA_noise,
  title={Estimating a kernel fisher discriminant in the presence of label noise},
  author={Lawrence, Neil and Sch{\"o}lkopf, Bernhard},
  booktitle={18th international conference on machine learning (ICML 2001)},
  pages={306--306},
  year={2001},
  organization={Morgan Kaufmann}
}



@ARTICLE{robust_theory_earlier_binary,
  author={Manwani, Naresh and Sastry, P. S.},
  journal={IEEE Transactions on Cybernetics}, 
  title={Noise Tolerance Under Risk Minimization}, 
  year={2013},
  volume={43},
  number={3},
  pages={1146-1151},
  keywords={Noise;Risk management;Noise measurement;Training data;Training;Vectors;Fasteners;Label noise;loss functions;noise tolerance;risk minimization},
  doi={10.1109/TSMCB.2012.2223460}}



@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research [best of the web]},
  author={Deng, Li},
  journal={IEEE signal processing magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}



@article{secost,
  title={Secost: Sequential co-supervision for weakly labeled audio event detection},
  author={Kumar, Anurag and Ithapu, Vamsi Krishna},
  year={2020}
}

@article{cleanLab,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}



@inproceedings{splitSelect,
  title={Understanding and utilizing deep neural networks trained with noisy labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019},
  organization={PMLR}
}

@misc{truncated_github,
  author = {Alan Chou},
  title = {Truncated Losses},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AlanChou/Truncated-Loss/}},
  commit = {93a2464}
}













