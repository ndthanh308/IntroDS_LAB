\documentclass[twoside]{article}
\usepackage[preprint]{aistats2024}
\usepackage{natbib}
\usepackage{empheq}
% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

% ready for submission
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2023}
\usepackage{xcolor}         
%this line below need to go
%\usepackage[preprint]{neurips_2023}
\definecolor{indigo}{rgb}{0.2, 0.0, 0.45}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true, allcolors=indigo]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% colors
\usepackage{adjustbox}
\usepackage{pst-all}
\usepackage{multirow}
\usepackage{comment}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{xr}
\usepackage{bm}
\usepackage{color, colortbl}
\usepackage{dashrule}

\usepackage[normalem]{ulem}


\usepackage{algorithm}      % algorithm floating environment
\usepackage{algpseudocode}  % algorithmicx pseudocode


%\setcounter{page}{12}

% save space
%\usepackage[subtle]{savetrees}


\definecolor{yellow}{rgb}{0.91, 0.84, 0.42}
\definecolor{blue}{rgb}{0.6, 0.73, 0.89}
\definecolor{red}{rgb}{0.8, 0.25, 0.33}
\definecolor{green}{rgb}{0.7, 0.75, 0.71}
\definecolor{orange}{rgb}{1.0, 0.6, 0.4}
\definecolor{cream}{rgb}{1.0, 0.99, 0.82}
\definecolor{Gray}{gray}{0.9}
\definecolor{green}{rgb}{0.6, 0.73, 0.89}


%% Self-defined macros
\newcommand{\cut}[1]{}
\newcommand{\swap}[3][-]{#3#1#2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\supp}{supp}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  William Toner\\
  University of Edinburgh\\
  \texttt{w.j.toner@sms.ed.ac.uk} \\
  % examples of more authors
   \And
   Amos Storkey \\
   %Affiliation \\
   University of Edinburgh\\
   \texttt{a.storkey@ed.ac.uk} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\Experiments
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}
\begin{document}
\pagestyle{plain}
\twocolumn[
\aistatstitle{Label Noise: Correcting the Forward-Correction}
\aistatsauthor{ William Toner
\And Amos Storkey}
\aistatsaddress{University of Edinburgh \\
  \texttt{w.j.toner@sms.ed.ac.uk}  \And  University of Edinburgh\\
   \texttt{a.storkey@ed.ac.uk}}]
\begin{abstract}
Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. The `forward-correction' is a popular approach wherein the model outputs are noised before being evaluated against noisy data. When the true noise model is known, applying the forward-correction guarantees consistency of the learning algorithm. While providing some benefit, the correction is insufficient to prevent overfitting to finite noisy datasets. In this work, we propose an approach to tackling overfitting caused by label noise.
%for forward-corrected loss functions. 
We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Motivated by this observation, we propose imposing a lower bound on the training loss to mitigate overfitting. %effectively `correcting' the already corrected loss.
Our main contribution is providing theoretical insights that allow us to approximate the lower bound given only an estimate of the average noise rate. We empirically demonstrate that using this bound significantly enhances robustness in various settings, with virtually no additional computational cost.
\end{abstract}
\vspace{-2mm}
% yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions

%\tableofcontents

\section{Introduction}
Over the last decade, we have seen an enormous improvement in the efficacy of machine learning methods for classification. Correspondingly, there has been an increased need for large labelled datasets to train these models. However, obtaining cleanly labelled datasets at the scale and quantity needed for industrial machine learning can be prohibitively expensive. For this reason, practitioners commonly rely on approaches which yield large datasets but contain high label noise. Examples include web querying or crowd-sourcing systems. Even standard dataset collection methods are susceptible to noise introduced by fallible human labellers. This is especially true when data are hard to label or labelling requires a specialist background (e.g., medical imaging). Such issues have led to immense interest in designing machine learning methods which can learn within environments characterised by noisy labels. 

Most approaches for addressing the label noise problem consist of a mechanism for either removing or compensating for it. Unfortunately, many of these methods are elaborate or require pipelines involving multiple networks and stages \citep{dividemix, coteaching, mentornet, decoupling, meta_dynamic, meta_gradient,EvidenceMix}. This complexity damages their applicability in settings with user limitations on time, technical expertise or computational resources. 

A simpler style of approach designs methods to be inherently resilient in the face of corrupted labels. The most prominent family of such methods is \textit{robust loss functions}. Here the goal is to choose an objective function which allows training in the presence of noise without harming the generality of the learned classifier. An advantage of these methods is their simplicity, as a robust loss can be easily implemented with minimal computational overhead. Regularisation and consistency-based approaches modify a loss with model-dependent terms or data cross-terms, to restrict the network and thus avoid overfitting \citep{mixup, elr, gjs, bootstrap}. Other methods alter the cross-entropy objective to be less inclined to fit to noise \citep{sce, GCE_Loss, normalised_losses}. Losses of this type are usually empirically rather than theoretically motivated, meaning the reasons for their robustness are rarely fully understood. 
 
% A more principled class of robust losses is \emph{loss correction methods}. Here one uses the data to infer the label noise model so that the impact of label noise may be subtracted from the training objective.
\emph{Loss correction methods} represent a more principled class of robust losses, using data to infer and subtract the impact of label noise from the training objective. Forward-corrections apply noise to the model predictions before evaluating these noised predictions against noisy data, whereas backward-corrections de-noise the noisy labels \citep{fprop}. Despite satisfying slightly weaker theoretical properties than the backward-correction \citep{unhinged}, forward-corrections generally perform slightly better, are easier to implement, and are more frequently used \citep{fprop, fprop_old, noiseAdapt, larsen, mnihHinton, anchor}. Nevertheless forward-corrections are still susceptible to overfitting on small noisy datasets.

In this paper, we tackle the challenge of overfitting in popular robust losses, taking a particular focus on forward-corrected loss functions due to their prevalence and importance. We introduce a principled solution: bounding the allowable loss during training by recognising that the presence of label noise means the generalised noisy risk is lower bounded. The critical contribution of this paper is explicitly deriving these bounds and showing that their implementation improves robustness. In addition, we provide a deeper understanding of existing robust loss functions, unifying correction losses with several other popular heuristic losses into a single family.

\paragraph{Key Idea:} When a distribution contains label noise, this implies there is a minimum achievable (noisy) risk. Current methods do not respect this bound, targeting a minimal training loss instead, causing overfitting. By bounding the loss below during training, one may prevent this. 
%Crucially, these bounds are explicitly derivable.

\subsection{Contributions} This work comprises two contributions:
\textbf{(a)} a smaller contribution is to \textbf{generalise the forward-correction} to include non-linear models, demonstrating that some popular heuristic robust loss functions (such as GCE and SCE) are, in fact, disguised forward-correction loss functions. This improves the generality of the main result where we: \textbf{(b)} show how overfitting can be avoided by ensuring the training loss remains above a certain threshold. We refer to augmenting a loss function in this manner as a \textbf{`bounded-loss'}. The crucial insight of this work is that, when labels are noisy, no model can achieve a loss below the average entropy of the noisy class posterior distribution. Under a separability assumption, the noisy entropy depends only on the noise model, and can thus be crudely estimated when only the average noise rate is known. This estimate, called the `noise-bound', is chosen as our threshold for the bounded loss. We demonstrate empirically that this significantly enhances robustness across a broad range of settings.

\subsection{Notation and Preliminaries}  
%\subsection{} what's an appropriate heading?
We adhere to standard notational and terminological conventions consistent with related work. Key notations are defined below, while a full summary together with a comprehensive review of \textbf{related work} may be found in the Appendix.

\paragraph*{Domains} $\mathcal{X} \subset \mathbb{R}^d$ represents the dataspace, and $\mathcal{Y} \coloneqq \{1,2,\ldots, c\}$ denotes the label space with $c$ the total number of classes. The probability simplex, denoted by $\Delta$, consists of $c$-dimensional vectors whose non-negative components sum to one. Vector quantities are presented in \textbf{bold}.

\paragraph*{Loss Function} A loss function $L: \Delta \times \mathcal{Y} \rightarrow \mathbb{R}$ measures the discrepancy between predicted and actual label distributions, resulting in a loss value. The loss across all classes for a prediction $\bm{q}$ is expressed as:
\begin{align*}
    \bm{L}(\bm{q}) = (L(\bm{q},1), L(\bm{q},2), \ldots, L(\bm{q},c)).
\end{align*}


\paragraph*{Expected Loss} The \emph{expected loss} for a forecast $\bm{q}$ relative to the true class distribution $\bm{p}$ is the average loss over labels sampled from $\bm{p}$, denoted by $H_{L}(\bm{p}, \bm{q})$ and calculated via:
\[
    H_{L}(\bm{p}, \bm{q}) = \bm{p}^T \bm{L}(\bm{q}).
\]

\paragraph*{Proper Losses} A loss $L$ is considered (strictly) proper if the expected loss is (uniquely) minimised by the true label distribution, i.e., $\bm{q} \coloneqq \bm{p}$.


\paragraph*{$L$-Risk} The (generalised) $L$-risk, representing the expected loss over the \textit{entire} data distribution $p(x,y)$, evaluates the overall efficacy of $\bm{q}(x)$:
\begin{align*}
    R_{L}(\bm{q}) \coloneqq \mathbb{E}_{x \sim p(x)}[H_{L}(\bm{p}(y | x), \bm{q}(x))].
\end{align*}

We call the risk computed with respect to an i.i.d dataset drawn from $p(x,y)$, the \emph{empirical} risk which we denote $\widehat{R}_L$.

\paragraph{Label Noise} Label noise encompasses any random process that modifies the labels from the data-label distribution $p(x,y)$. To differentiate between the original (clean) and altered (noisy) labels, we use a tilde notation, such as $\widetilde{p}(x,\widetilde{y})$. A detailed taxonomy of label noise types is provided in the Appendix.

\paragraph{Noisy Risk} Label noise introduces the concept of \emph{noisy} generalised risk, denoted by $R^{\eta}_{L}$. This risk is computed with respect to the noisy label distribution.

\paragraph*{Noise Rate} The \emph{noise rate} at any $x \in \mathcal{X}$, defined as $\eta(x) \coloneqq p(\widetilde{y} \neq y \mid x)$, reflects the likelihood of label alteration by noise at that point. The \emph{average noise rate} across the dataspace is $\eta \coloneqq p(\widetilde{y} \neq y)$.

\paragraph{Problem Statement} The objective of robust loss methods is to construct a loss function $L$ such that minimising the noisy empirical $L$-risk leads to effective generalisation to the clean data distribution.


\section{Generalised Forward-Corrections}
\subsection{Robust Loss Functions}\label{sec:robustloss}
Label noise robust loss functions can be categorised into two broad sets: regularisation-based robust losses and correction-based losses.
\paragraph*{Regularisation-Based Losses} A popular approach to tackling label noise by selecting losses less prone to fit the entire training set than the standard cross-entropy (CE). An archetypal example of such a loss is a mean absolute error (MAE) $(L_{MAE}(\bm{q}, y=k) = 1-q_k)$.  MAE will typically ignore the harder-to-fit samples; on noisy datasets, this often corresponds to those with corrupted labels. The downside is that MAE dramatically underfits on datasets with many classes \citep{normalised_losses}. Alternative losses mitigate this underfitting by interpolating between CE and MAE to avoid both of their pitfalls. Two well-known examples are the Generalised Cross-Entropy (GCE) and Symmetric Cross-Entropy (SCE) defined $L_{GCE}(\bm{q}, y=k) := \frac{1-q_k^a}{a} \label{Equation:gce}$ and $L_{SCE}(\bm{q}, y=k) = -log(q_k) +A(1-q_k)$ respectively \citep{GCE_Loss, sce}. By varying the parameters $a,A$, we can alter the losses' behaviour from being more like CE to MAE.


\paragraph*{Correction-based Losses} Correction-based loss functions arise as an alternative, motivated by the observation that under label noise, the empirical risk ceases to be an effective proxy for the generalised clean risk \citep{old_backward}. By altering the loss through incorporating the noise model, one may ensure the corrective property that
\begin{align*}
    \argmin_{\bm{q}} R^{\eta}_{L_{F}}(\bm{q}) = \argmin_{\bm{q}} R_{L}(\bm{q}).
\end{align*}
Thus ensuring that minimising the noisy generalised risk aligns with mnimising the generalised clean risk. 
A popular and effective method is the \textit{forward-correction} \citep{fprop}. Given a \emph{base loss} $L$, the forward-correction of $L$ is defined 
\begin{align}\label{def:f_corr}
    L_{F}(\bm{q}, k) \coloneqq L(\widehat{T}\bm{q}, k),
\end{align}
 where $\widehat{T}$ is a column stochastic matrix approximating the true transition matrix $T\coloneq p(\widetilde{y}\vert y)$ for class-conditional label noise.
 Conversely, we say that a loss function $L$ is a \emph{forward-corrected loss} if it is the forward-correction of some other loss function. 

\subsection{Non-Linear Noise Models}\label{sec:non-linear-noise-models}
In the class-conditional label noise framework \citep{angluin1988}, a label noise model is defined by a column stochastic matrix $T$, which represents the transition probabilities $p(\widetilde{y}\mid y)$. This traditional formulation assumes that the noisy label depends on an unobserved clean label, which might not reflect real-world scenarios where the noisy annotator never sees the clean label.

An alternative conceptualisation of class-conditional label noise considers the transition matrix as merely a tool to link noisy and clean class distributions. For example, if the true class posterior at $x$ is $\bm{p}(y\mid x)$, then according to a class-conditional label noise model, the distribution observed by the noisy labeller is given by $T\bm{p}(y\mid x)$. This approach does not presume dependence on a specific true label, but rather describes how noisy and clean label distributions are related.
 
However, this perspective raises questions about the necessity of assuming a linear relationship between these distributions. It is plausible that a labeller might make fewer errors on instances that are clearly representative of their class, and many more errors when the class distribution $\bm{p}(y\mid x)$ is more evenly distributed. This scenario suggests a non-linear noise model:
\begin{align*}
    \widetilde{\bm{p}}(y\mid x) \coloneq f(\bm{p}(y\mid x)),
\end{align*}
where $f:\Delta\rightarrow\Delta$ is some (possibly non-linear) transformation on this simplex - which for simplicity we will limit to being injective.
 
It is straightforward to generalise the forward-correction to allow for $\widehat{T}$ in Equation~\ref{def:f_corr} being a non-linear transformation $f$. 
\begin{definition}[Generalised Forward-Correction]\label{def:gen_corr}
 Let $L_f$ be a loss function and $f:\Delta\rightarrow \Delta$ be an injective function. We say $L_f$ is a `generalised forward-corrected loss' if there exists a loss function $L$ such that for all $\bm{q}\in \Delta$, $k\in \{1,2,\ldots,c\}$
 \begin{align*}
     L_f(\bm{q}, k) = L(f(\bm{q}), k).
 \end{align*}
 We refer to $L$ as the \textbf{base loss}. $f$ can be thought of as a label noise model. 
\end{definition}

The forward-correction is trivially an example of a generalised forward-correction loss obtained by setting $f\coloneq\widehat{T}$. We now demonstrate that the GCE and SCE losses previously discussed are generalised forward-correction losses, deriving expressions for the underlying (non-linear) noise models $f$. This derivation relies on the assumption that the base losses which generate these loss functions is proper - i.e we decompose both SCE and GCE into a proper loss corrected by a non-linear noise model. This decomposition is unique.

\begin{lemma}\label{lemma:semiproper_gce}
    The GCE, SCE and forward-corrected CE (denoted FCE) loss functions can be formulated as generalised forward-corrected losses with a proper base loss. The noise models $f_{GCE}, f_{SCE}, f_{FCE}$ satisfy
    \begin{align*}
        (f^{-1}_{GCE}(\bm{p}))_i &= \frac{p_i^{\frac{1}{1-a}}}{\sum_{i=1}^c p_i^{\frac{1}{1-a}}},\\
        (f^{-1}_{SCE}(\bm{p}))_i &= \frac{p_i}{\lambda-Ap_i},\\
        f_{FCE}(\bm{p}) &= \widehat{T}\bm{p},
    \end{align*}
     where $\widehat{T}$ is the invertible stochastic matrix used to define the correction, and $\lambda$ is a constant selected to ensure the correct normalisation.
\end{lemma}
For an interpretation and plots of the noise models; $f_{GCE}, f_{SCE}$, see Appendix~\ref{sec:additional_theory}.

Lemma~\ref{lemma:semiproper_gce} demonstrates that GCE and SCE can be conceptualised as non-linear forward-corrected losses; the noise model is represented by the function $f$. We stress that these are by no means the only robust losses which adhere to Definition~\ref{def:gen_corr}. However, they provide useful examples when empirically demonstrating the results of Section~\ref{sec:riskbounds}.

The generalisation established by Definition~\ref{def:gen_corr} offers three advantages. i) It enhances our understanding of losses like GCE, demonstrating that they implicitly incorporate a noise model. ii) Partially unifies correction losses with other robust loss functions. iii) Ensures that theoretical results derived for generalised forward-correction losses are widely applicable, encompassing both traditional, linear correction losses and many other robust loss functions.

\section{Loss Bounding}
 Our analysis in this section reveals that merely correcting for the noise model (as in Definition~\ref{def:gen_corr}) is inadequate for achieving robustness. We must also adjust our loss function to incorporate a lower bound to account for the randomness introduced by label noise.

 \paragraph*{Key Observation} 
When a data distribution contains label noise, there is a lower bound on the optimal noisy risk a model can achieve. An analogy to this is that no forecaster can predict the outcome of a biased coin flip 100\% of the time - e.g. if a coin comes up heads 60\% of the time we cannot expect a forecaster to predict more accurately than 60\% over a large number of flips. Similarly, \uline{even an optimal model, which minimises the noisy risk, will \emph{still} incur a non-zero loss on a randomly sampled noisy dataset.}

\subsection{Overfitting to Label Noise} 
If one trains a classifier on a noisy dataset using the cross-entropy loss function, the classifier learns to fit all labels in the training set  - including those which have been corrupted by label noise \citep{memorisation} - damaging generality.
Ideally, when training a model on a noisy dataset we wish to fit the clean labels without overfitting the noisy ones.
A major obstacle to achieving this desire is the difficulty in determining whether a specific label is clean or corrupted. 
However, while it is difficult to determine whether a model has overfit to a \emph{specific} label it is often obvious when a model has overfit to a dataset. For example, if a dataset is known to contain label noise, obtaining a training loss of zero heavily implies overfitting has occurred.

\paragraph{Forward-Corrections} When learning a classifier in the presence of noisy labels it is therefore inappropriate to target a zero training loss. Utilising the forward-correction partly addresses this issue. The forward-correction works by noising our model predictions $\bm{q}\mapsto \widehat{T}\bm{q}$ before applying the loss. This guarantees that a zero loss is no longer possible, since our noised model never predicts any label with full confidence. Despite possessing this desirable property, the forward-correction does not go far enough in that the lower bound it imposes is still too high to prevent overfitting. An illustrative example for a dataset polluted by $40\%$ symmetric label noise is presented in Table~\ref{ch6:tab:bound_comparison}. 
The table gives the lowest attainable training loss for a model trained on this noisy dataset versus the `optimal training loss' i.e the loss which would be obtained by an optimal model, possessing full knowledge of how the dataset was generated but has not been permitted to observe the dataset labels. Obtaining a training loss lower than this optimal value would suggest overfitting has occurred. We see that while FCE imposes a bound (unlike CE), this bound is still too low. 

Further discussion of this example is given in Appendix~\ref{ch6:example:fce}.

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Loss }} & \multicolumn{1}{c|}{\textbf{Lowest Attainable}} & \multicolumn{1}{c|}{\textbf{Optimal }}\\  
\multicolumn{1}{|c|}{\textbf{Function}}                       & \textbf{Training Loss} & \textbf{Training Loss} \\ \hline
CE                                           & 0 & 0.673 \\ \hline
FCE                                          & 0.511 & 0.673 \\ \hline
FCE$+$B                                        & 0.673 & 0.673 \\ \hline
\end{tabular}
\caption{Bounds imposed on the attainable training loss by different cross-entropy variants versus the \emph{optimal} training loss: This table compares the minimum training loss achievable by a model on a large noisy dataset using different loss functions, distinguishing between scenarios where the model is allowed or not allowed to observe the dataset labels. We assume a separable binary-label dataset corrupted by 40\% symmetric label noise. When the dataset labels are not observable, the loss on the dataset is minimised (in expectation) by an optimal model - a minimiser of the (noisy) generalised risk. This model will obtain a loss of $0.673$ on this dataset with high probability. In contrast, using CE or FCE loss functions can result in training losses below this optimal figure, making overfitting likely While FCE introduces an inherent loss bound, improving robustness, it may still permit overfitting. Our bounded variant, FCE$+$B, is designed to better mitigate overfitting.}
\label{ch6:tab:bound_comparison}
\end{table}


\subsection{Bounded Loss}
We propose, therefore, that the principled way to handle label noise is to limit the minimum allowable risk on the training set. Specifically, we define a lower bound `$B$' and train - preventing the training loss from going below this value. Explicitly, we augment our loss as follows:

\begin{definition}[Bounded Loss]\label{def:budget_loss}
    Let $L$ be a loss function. Let $\mathcal{D}$ be a batch of $N$ data-label pairs $(x_i, y_i)$. Given a lower bound, $B\in\mathbb{R}$, we define the $B$-\textbf{bounded} loss $L_{\stackrel{B}{\rule{0.21cm}{0.4pt}}}$ obtained from $L$ as follows:
\begin{align}
    \boxed{
    \vspace{5pt}
        \scalebox{1.2}{ 
            $L_{\stackrel{B}{\rule{0.21cm}{0.4pt}}}(\bm{q}(x), \mathcal{D}) \coloneqq \left| \frac{1}{N}\sum_{i=1}^N L(\bm{q}(x_i), y_i) - B\right|$
        }
    \vspace{5pt}
    } \label{ch6:Equation:budget}
\end{align}

\end{definition}
Essentially when the average loss on a batch of samples is above our bound $B$, training proceeds as normal. However, if the training loss on a batch dips below $B$, the learning rate effectively becomes negative resulting in `untraining' which proceeds until the average loss is back above $B$. Bounding the loss in this way has been previously explored by \citet{flooding}. We extend upon this work by grounding loss bounding within the context of loss corrections and by providing a theoretically justified method for selecting the loss bound.

\section{Risk Bounds}\label{sec:riskbounds}
In the last section, we remarked that despite theoretical motivation, correction losses are still prone to overfitting. We proposed training against a lower bound to prevent this, noting that the minimal achievable generalised noisy risk is non-zero. In this section, we explicitly derive lower bounds on the generalised noisy $L$-risk for generalised forward-corrected losses. We conclude by presenting a formula for choosing a bound $B$ to use in Definition~\ref{def:budget_loss}. We call this the \textit{noise-bound}. 

\paragraph*{Assumptions} Throughout this section we suppose that all loss functions are generalised forward-corrected losses, which we denote $L_f$, furthermore we suppose that their base loss functions $L$, are proper. We assume also that the loss function has no inherent bias toward any particular class; i.e the loss is unaffected by a random permutation of the label set. Examples of such losses includes, CE, MSE, FCE, GCE, SCE and many others.


\subsection{Entropy As Lower Bound}\label{sec:entropy_bounding}
In Lemma~\ref{lemma:general} we establish a general lower bound on noisy risk in terms of the average entropy of the noisy label distribution. Precisely stating Lemma~\ref{lemma:general} requires us to define the `entropy function' of a proper loss.

\begin{definition}[Entropy Function]
Given a proper loss function \(L\), define its entropy function \citep{properBregman} as the expected loss incurred when the forecast equals the true distribution over classes: \(\mathcal{H}:\Delta \rightarrow \mathbb{R}\) by:
\[
\mathcal{H}(\bm{p}) \coloneqq H_{L}(\bm{p}, \bm{p}) = \bm{p}^T \bm{L}(\bm{p}),
\]
where \(\bm{p}\) is a probability distribution over the classes and \(H_{L}\) denotes the expected loss.
\end{definition}

The entropy function for a (strictly) proper loss function is (strictly) concave and, by the definition of properness, satisfies $\mathcal{H}(\bm{p}) \leq H(\bm{p},\bm{q})$ for all $\bm{p},\bm{q}\in\Delta$. This leads immediately to the following lemma.

%\vspace{-2mm}
%\hdashrule{8.62cm}{0.4pt}{3mm 2pt}
\begin{lemma}\label{lemma:general}
    Let $L_f$ be a generalised forward-corrected loss whose `base-loss' $L$ is strictly proper (Recall the definition of `base-loss' from Definition~\ref{def:gen_corr}). The noisy risk of any probability estimator $\bm{q}$ is lower bounded:
        \begin{align}\label{Equation:general_entropy_bound}
        R^{\eta}_{L_f}(\bm{q}) \geq \mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{\bm{p}}(\tilde{y}\vert x))],
    \end{align}
    where $\mathcal{H}$ is the entropy function of the base-loss. This bound is tight when $f$ is equal to the true noise model. Equality is attained by setting $\bm{q}(x)=f^{-1}\left(\widetilde{\bm{p}}(\widetilde{y}\vert x)\right)$.\footnote{Note that if $f$ is the true noise model then $\widetilde{\bm{p}}(\widetilde{y}\mid x) \in f(\Delta)$ and the inverse is unique by injectivity.}
\end{lemma}


 Lemma~\ref{lemma:general} establishes that, when using a generalised forward-corrected loss (with proper base loss) the average entropy of the noisy distribution provides a lower bound on the noisy risk. In an ideal world one would calculate the entropy expression in Equation~\ref{Equation:general_entropy_bound} wherein it could be used as the lower bound `$B$' in Equation~\ref{ch6:Equation:budget}. However, in most settings Equation~\ref{Equation:general_entropy_bound} will be nearly impossible to precisely calculate due to lack of knowledge about the data distribution and noise model. In such settings we are required therefore to make some major simplifying assumptions in order to make an approximation. 

 \paragraph*{Separability} The key simplifying assumption which we employ is assuming that the clean label distribution is (approximately) separable. i.e. there is no randomness in the label distribution. While an idealised assumption, it is a reasonable approximation for many real-world image classification tasks where clear images of a single subject dominate the dataset. In domains with high inherent randomness such as medical diagnostics, this assumption is less suitable. In the following we leverage this assumption to construct a few bounds. 

\subsection{Estimating The Entropy}\label{sec:estimating_entropy}
Given a datapoint $x$, the entropy of the noisy class posterior $\widetilde{\bm{p}}(\widetilde{y}\mid x)$ will depend both of the noise rate and the \emph{type} of label noise. For example, symmetric label noise will result in a different entropy than pairwise label noise, even given the same noise rate. The following Lemma gives a range on the possible entropies of $\widetilde{\bm{p}}(\widetilde{y}\mid x)$ given that the noise rate at $x$ is equal to $\eta(x)$. For brevity we introduce the following notation:
\begin{align}
    \bm{u}_{\text{pair}}(\eta,c) &\coloneq (1-\eta(x), \eta(x), 0,  \ldots, 0 )\\
     \bm{u}_{\text{sym}}(\eta,c) &\coloneq \Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big) \label{notation:u_sym}
\end{align}
\hdashrule{8.62cm}{0.4pt}{3mm 2pt}

 \begin{lemma}\label{lemma:entropy_interval}
 Let $L_f$ be a generalised forward-corrected loss function whose base-loss $L$ has entropy function $\mathcal{H}$.
  Suppose that label noise is applied to a separable data-label distribution and let $x\sim p(x)$.
 Given that the noise rate at $x$ is $\eta(x)$, the entropy of the noisy label distribution at $x$, $\mathcal{H}(\widetilde{\bm{p}}(\widetilde{y}\mid x))$ must lie in the following interval:
 \begin{align*}
          %\mathcal{H}(\widetilde{p}(\widetilde{y}\mid x) \in \\
          \left[\mathcal{H}(\bm{u}_{\text{pair}}(\eta(x),c)),  \mathcal{H}(\bm{u}_{\text{sym}}(\eta(x),c))\right].
 \end{align*}
In particular, for a fixed noise rate $\eta(x)$, the highest entropy occurs under symmetric label noise at $x$, while the lowest entropy is observed with pairwise label noise.
  \end{lemma}
\vspace{-2mm}
\hdashrule{8.62cm}{0.4pt}{3mm 2pt}
\begin{corollary}\label{cor:worst_case_entropy}
       Given an average noise rate $\eta\coloneq \mathbb{E}_{x\sim p(x)}[\eta(x)]$, the greatest possible value of $\mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{\bm{p}}(\widetilde{y}\mid x)]$ occurs when $\eta(x)$ is constant:
       \begin{align*}
           \sup_{p(\widetilde{y}\mid x,y)}\left( \mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{\bm{p}}(\widetilde{y}\mid x))]\right) = \mathcal{H}\left(\bm{u}_{\text{sym}}(\eta,c)\right),
       \end{align*}
    where the supremum is taken over all noise models such that $\mathbb{E}_{x\sim p(x)}[\eta(x)] = \eta$.
\end{corollary}
\vspace{-2mm}
\hdashrule{8.62cm}{0.4pt}{3mm 2pt}
\paragraph*{Worst-Case Entropy} Corollary~\ref{cor:worst_case_entropy} establishes a worst-cast entropy given a specified average noise rate $\eta$. Simply put, the Corollary tells us \emph{`given that the average noise rate does not exceed $\eta$, the noise model with the highest entropy is uniform symmetric label noise'.}

\subsection{Main Proposal: Noise-Bounded Loss}
\paragraph*{Discussion} The goal of this section is to derive lower bounds on the noisy risk which can be used as `$B$' in our $B$-bounded loss (Definition~\ref{def:budget_loss}). Ideally, with precise knowledge of the noise model, the lower bound would be set equal to the average entropy of the noisy label distribution. However, the label noise model is typically unknown, and we might only have access to an approximate noise rate.

 \paragraph*{What bound do we choose when we only know the noise rate?} 
Corollary~\ref{cor:worst_case_entropy} establishes that, given a known noise rate $\eta$, a `worst-case' entropy occurs when the label noise is symmetric and uniform. This means that if we set our bound `$B$' under the assumption of symmetric-uniform label noise at rate $\eta$, $B$ can never be lower than the true noisy entropy - making overfitting unlikely. We call this the `noise-bound'. 

\begin{definition}[Noise-Bound]\label{def:noise-bound}
Let $L_f$ be a generalised forward-corrected loss whose base loss $L$ has entropy function $\mathcal{H}$. Using the notation $\bm{u}_{\text{sym}}(\eta, c)$ from Equation~\ref{notation:u_sym}, we define the \textbf{noise-bound} as:
\begin{align}\label{eqn:noise_bound}
     \boxed{B(\eta, c) := \mathcal{H}(\bm{u}_{\text{sym}}(\eta, c)) = \bm{u}_{\text{sym}}(\eta,c)\cdot \bm{L}(\bm{u}_{\text{sym}}(\eta,c)).}
\end{align}
\end{definition}

\paragraph*{Examples} For CE and FCE the noise-bound corresponds to the Shannon Entropy of the distribution $\bm{u}_{\text{sym}}(\eta,c) = (1-\eta, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$. For SCE and GCE we remark that 
\begin{align*}
    B(\eta, c) &= \bm{u}_{\text{sym}}(\eta,c)\cdot \bm{L}(\bm{u}_{\text{sym}}(\eta,c))\\ &= \bm{u}_{\text{sym}}(\eta,c)\cdot \bm{L}_f(f^{-1}(\bm{u}_{\text{sym}}(\eta,c)))
\end{align*}
allowing us to compute $B(\eta, c)$ by substituting expressions for $f^{-1}$ derived in Lemma~\ref{lemma:semiproper_gce}. These are given explicitly in Appendix~\ref{sec:explicit_bounds}.

This leads us to the main proposal of this paper. When our dataset has label noise we propose using the bounded loss (Equation~\ref{ch6:Equation:budget}) with $B$ set to $B(\eta,c)$, the `noise-bound' from Equation~\ref{eqn:noise_bound}. We call this the \textbf{noise-bounded loss}: 

\begin{definition}[Noise-Bounded Loss]\label{def:noise_bounded_loss}
    Let $L$ be a loss function. Let $\mathcal{D}$ be a batch of $N$ data-label pairs $(x_i, y_i)$. Given a noise rate $\eta$, we define the \textbf{noise-bounded} loss $L_{\stackrel{B(\eta, c)}{\rule{0.41cm}{0.4pt}}}$ obtained from $L$ as follows:
\begin{align}
    \boxed{L_{\stackrel{B(\eta, c)}{\rule{0.41cm}{0.4pt}}}(\bm{q}(x), \mathcal{D}) \coloneqq \left|  B(\eta, c) - \frac{1}{N}\sum_{i=1}^NL(\bm{q}(x_i), y_i)\right|} \label{ch6:Equation:noise_budget} 
\end{align}
where $B(\eta, c)$ is as given in Equation~\ref{eqn:noise_bound}.
\end{definition}

\paragraph*{Example FCE:} The noise-bounded variant of FCE (which we denote FCE$+$B) is given in Equation~\ref{eq:three_ce_variants}.


% Figure environment removed

\paragraph*{Bound Optimality} 
By construction, the noise-bound is only equal to the average entropy of the noisy label distribution if the label noise is symmetric and uniform. In all other cases the noise-bound will be higher than strictly necessary. Ideally, we'd like the gap between the noise-bound and the true noisy entropy to be small in a typical setting: If the gap is large, the noise-bounded loss will cease training long before overfitting occurs and possibly when there is still signal to be learned. A small gap occurs when all distributions of the form $(1-\eta, \eta_2, \ldots, \eta_c)$ (where $\eta\coloneq\sum_i \eta_i$) have roughly the same entropy - i.e. the entropy is `insensitive' to the \emph{structure} of the noise model, depending mostly on the noise \emph{rate}. The level of insensitivity depends on the entropy function itself. Shannon-Entropy is relatively sensitive to the structure of the noise model. In contrast, losses like GCE and SCE induce insensitive entropy functions. This topic is discussed further in Appendix~\ref{sec:sensitivity_of_bounds}.

\paragraph*{Empirical and Generalised Risk} Our analysis in Section~\ref{sec:entropy_bounding} demonstrates that an estimator cannot achieve a noisy risk below the mean noisy label entropy. However, in a small finite dataset of i.i.d. samples, it is possible for an estimator to achieve a noisy \emph{empirical} risk that is lower than the noisy entropy, and this can occur with non-zero probability, even without access to the actual dataset labels. However, as the size of the dataset increases, the likelihood of an estimator achieving a loss significantly lower than the noisy entropy rapidly diminishes. According to the Central Limit Theorem, the probability of obtaining a loss more than \(\delta\) below the noisy label entropy diminishes at the rate of \(O\left(\frac{1}{\sqrt{N}}\right)\), where \(N\) is the dataset size. In practical terms this means obtaining a training loss below the the noise-bound is \emph{almost impossible} unless one has overfit to the noisy labels. 

\begin{algorithm}
\caption{Training with Noise-Bounded Loss}
\label{alg:noise_aware_training}
\begin{algorithmic}[1]
\State \textbf{Input:} Noisy dataset $\mathcal{D} = \{ (x_i, \widetilde{y}_i) \}_{i=1}^N$, estimated noise rate $\eta$, number of classes $c$, epochs $T$
\State \textbf{Output:} Trained model parameters $\Theta$

\Function{ComputeNoiseBound}{$\eta$, $c$}
    \State $\bm{u}_{\text{sym}}(\eta, c) \gets \left(1-\eta, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1}\right)$
    \State \textbf{return} $\bm{u}_{\text{sym}}(\eta, c) \cdot L(\bm{u}_{\text{sym}}(\eta, c))$  %\Comment{Computing $\mathcal{H}(\bm{u}_{\text{sym}}(\eta, c))$}
\EndFunction

\Procedure{TrainModel}{$\mathcal{D}$, $\eta$, $c$, $T$}
    \State $B(\eta, c) \gets \Call{ComputeNoiseBound}{\eta, c}$
    \For{$epoch = 1$ to $T$}
        \For{each $(x_i, y_i)$ in $\mathcal{D}$}
            \State $\bm{q}(x_i) \gets \text{ModelPrediction}(x_i; \Theta)$
            \State $loss \gets \left| B(\eta, c) - \frac{1}{N}\sum_{j=1}^N L(\bm{q}(x_j), y_j)\right|$
            \State $\Theta \gets \text{UpdateModel}(\Theta, loss)$ %\Comment{Gradient descent update}
        \EndFor
    \EndFor
    \State \textbf{return} $\Theta$
\EndProcedure

\end{algorithmic}
\end{algorithm}

\section{Experiments}\label{sec:experiments}

\subsection{Loss Functions}
In this section, we empirically investigate the effectiveness of the noise-bounded loss (Equation~\ref{ch6:Equation:noise_budget}) for improving robustness to label noise. We consider several loss functions: CE, SCE, forward-corrected CE (FCE), and GCE. Additionally, we explore a variant of CE that includes a prior on the model probabilities (CEP). Our experiments all follow a similar structure. We use a dataset containing intrinsic or synthetic label noise in the training set. We train neural network models using each loss on this noisy training set and evaluate their performance on a clean test set. We then compare results from models trained without noise-bounds to those trained with noise-bounds, denoted by a `$+$B' suffix in the loss name (e.g., CE$+$B indicates the use of a noise-bounded cross-entropy loss).

\paragraph*{Baseline Loss Functions} Our results are benchmarked against other standard robust loss functions, including mean squared error (MSE) \citep{L2}, mean absolute error (MAE), NCE-MAE \citep{normalised_losses}, ELR \citep{elr}, Curriculum loss (CL) \citep{curiculum}, Bootstrapping loss (Boot.) \citep{bootstrap}, Spherical loss (Sph.), Mix-up \citep{mixup}, and a version of GCE that incorporates the additional tricks outlined by \citet{GCE_Loss}. To differentiate this version of GCE from our simplified GCE, we refer to it as `Truncated loss' (Trunc.) due to its use of truncation.


\subsection{Datasets}
We evaluate each loss on various datasets with different label noise types. We consider versions of EMNIST, FashionMNIST, CIFAR10, CIFAR100 corrupted by symmetric label noise at rates of 0.2 and 0.4 and MNIST with rates of 0.4 and 0.6. Additionally, we explore more sophisticated noise types. In the case of `Asym-CIFAR100,' we introduce asymmetric noise by randomly transitioning labels within the 20 superclasses of CIFAR100. For example, within the superclass `fish' (comprised of aquarium-fish, flatfish, ray, shark, trout), 
we change training labels to other members of the set with a probability of $\eta \in \{0.2, 0.4\}$ (e.g., flatfish $\rightarrow$ trout). For `Non-Uniform EMNIST,' we investigate the impact of using non-uniform noise. We train a linear classifier on EMNIST and, with a probability of 0.6, modify the label of a data point in our training set to match the output of this classifier. Since the performance of the classifier varies across data-space, this creates label noise with an $x$-dependence. Further experiments on the TinyImageNet and Animals-10N datasets, which contain real, intrinsic open-set noise, are given in Appendix~\ref{sec:further_experiments}. 

\paragraph*{Hyperparameters} For the Animals and TinyImageNet experiments, we use a ResNet-34 to parameterise our model. For the other datasets, we use a ResNet-18. For each experiment, the number of epochs is kept consistent across losses. The bounds we employ in each experiment are obtained by substituting the relevant number of classes $c$ and the noise rate $\eta$ into Equation~\ref{eqn:noise_bound}. An exception is the case of Non-uniform EMNIST, where we use a class number of $c=2$ to reflect that label is a mixture of the clean label and classifier labels. The results where we vary the bound `$B$' are obtained by doing a small grid search on either side of our noise-bound value. Additional precise experimental details may be found in Appendix~\ref{sec:further_experiments}

\subsection{Results}
% Figure environment removed

The results of our experiments are presented in two tables. Table \ref{table:MNIST} includes the simpler datasets of MNIST, FashionMNIST, EMNIST, and CIFAR10, while table \ref{table:CIFAR} displays CIFAR100, Asym-CIFAR100, and Non-uniform-EMNIST. The results for TinyImageNet and Animals are presented in a third table in Appendix~\ref{sec:further_experiments}. 
Each table follows a similar structure, with losses listed in rows and datasets in columns. The baselines are grouped together at the top. Our main losses are organised into pairs, such as CE, CE$+$B.
%, and CE$+$B*.
The rows that use the noise-bound (e.g., GCE$+$B) are highlighted in blue to enhance readability. If using our noise-bound leads to higher mean accuracy compared to training without the bound, this is indicated by a \boxed{box}. The best overall model for each dataset is highlighted in \textbf{bold}. \uline{In $\bm{82\%}$ of cases utilising the noise-bound improves performance relative to the unbounded loss variant. }

\paragraph*{Exceptions} With few exceptions, our bound leads to improved performance compared to the standard, unbounded version of each loss. For the Asym-CIFAR100 and Non-Uniform-EMNIST datasets, our CE$+$B loss performs worse than regular CE. This outcome was expected since our derived bounds are optimal for symmetric noise and may be suboptimal for non-symmetric noise - This discrepancy is especially pronounced for losses based on Shannon-Entropy like CE. In contrast, the other generalised forward-corrected losses, as we had anticipated, exhibit greater resilience to the precise noise structure and consistently outperform the baseline across different types of noise.

\paragraph*{Impact of Estimated Noise Rate} Figure~\ref{fig:entropy_graph2s} illustrates how the clean test accuracy varies with the estimated noise rate, $\widehat{\eta}$, for training on noisy CIFAR10 (using SCE loss) and EMNIST (using CE loss) datasets. Both datasets were corrupted with symmetric noise at a rate of $\eta=0.4$. We trained models using noise-bounds calculated from $\widehat{\eta}$, substituting it into Equation~\ref{eqn:noise_bound} to compute $B(\widehat{\eta}, c=10)$, and plotted the clean test performance against $\widehat{\eta}$. Initially, at $\widehat{\eta}\approx 0$, where no effective bound constrains the model, overfitting is common. As $\widehat{\eta}$ increases, the bound restricts overfitting, enhancing test accuracy. Optimal performance is observed near the true noise rate at $\widehat{\eta}\approx 0.4$, marked by a vertical green dotted line. Beyond this point, performance declines as the model underfits the noisy data. The prominent peak in performance near this green line empirically validates our theoretical approach. Interestingly, slight overestimations of the noise rate often yield marginal improvements in performance, likely originating from our simplifying assumption, which modelled the underlying distributions as separable. Appendix~\ref{sec:further_experiments} explores this in more detail, treating the bound `$B$' as a hyperparameter.


\begin{table*}[!htb]
    \centering
    \begin{adjustbox}{max width=1.00\textwidth}
    \begin{tabular}{l|cc|cc|cc|cc|cc}
    \textbf{} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{FashionMNIST} & \multicolumn{4}{c|}{EMNIST} & \multicolumn{2}{c}{CIFAR10} \\
\multirow{2}{*}{Losses} & \multicolumn{1}{c}{\multirow{2}{*}{0.4}} & \multicolumn{1}{c|}{\multirow{2}{*}{0.6}} & \multicolumn{1}{c}{\multirow{2}{*}{0.2}} & \multicolumn{1}{c|}{\multirow{2}{*}{0.4}} & \multicolumn{2}{c}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{1}{c}{\multirow{2}{*}{0.2}} & \multicolumn{1}{c}{\multirow{2}{*}{0.4}} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c|}{Top 5} & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c|}{Top 5} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{}  \\ \hline
MSE  & $93.3_{\pm 0.47 }$& $85.8_{\pm 0.95 }$& $84.8_{\pm 0.22} $& $80.6_{\pm 0.84 }$& $82.9_{\pm 0.29}$ & $98.1_{\pm 0.04}$ & $80.2_{\pm 0.19}$ & $97.1_{\pm 0.07}$ & $78.7_{\pm 1.51}$ &   $56.4_{\pm 0.11}$ \\
MAE & $97.9_{\pm 0.08} $& $96.4_{\pm 0.08} $& $83.2_{\pm 0.10} $& $82.2_{\pm 0.37} $& $49.8_{\pm 2.83}$ & $52.2_{\pm 0.10}$ & $50.4_{\pm 1.14}$ & $51.4_{\pm 0.96}$ & $88.6_{\pm 1.34}$ &   $78.9_{\pm 5.95}$\\
NCE & $97.8_{\pm 0.06} $ & $96.0_{\pm 0.25} $ & $87.7_{\pm 0.26} $ & $86.3_{\pm 0.14} $ & $84.5_{\pm 0.25}$ & $97.9_{\pm 0.05}$ & $82.6_{\pm 0.81}$ & $96.7_{\pm 0.03}$ & $\textcolor{black}{\bm{89.3}_{\pm 0.40}}$ & $\textcolor{black}{\bm{86.0}_{\pm 0.81}}$ \\
MixUp & $95.8_{\pm 1.24} $& $86.8_{\pm 0.85} $& $86.9_{\pm 0.10} $& $82.3_{\pm 0.54} $& $84.3_{\pm 0.08}$ & $98.1_{\pm 0.04}$ & $81.6_{\pm 0.48}$ & $97.1_{\pm 0.08}$ & $86.0_{\pm 0.46}$ &  $77.9_{\pm 0.49}$ \\
Sph. & $95.0_{\pm 0.41}$ & $88.1_{\pm 0.82}$ & $87.2_{\pm 0.04}$ & $84.1_{\pm 0.75}$ & $84.6_{\pm 0.12}$ & $98.3_{\pm 0.05}$ & $83.2_{\pm 0.29}$ & $98.1_{\pm 0.58}$ & $86.6_{\pm 0.01}$  & $72.1_{\pm 0.80}$\\
Boot. & $86.6_{\pm 0.56} $& $71.2_{\pm 1.17} $& $82.0_{\pm 0.61}$ & $73.4_{\pm 1.06} $ & $80.5_{\pm 0.24}$ & $96.7_{\pm 0.06}$ & $77.3_{\pm 0.98}$ & $95.0_{\pm 0.25}$ & $77.0_{\pm 1.57}$ &  $58.2_{\pm 2.99}$\\
Trunc. & $97.1_{\pm 0.12} $& $94.2_{\pm 0.39} $& $87.8_{\pm 0.29} $& $85.3_{\pm 0.77} $& $84.1_{\pm 0.53}$ & $97.4_{\pm 1.03}$ & $83.1_{\pm 0.55}$ & $97.2_{\pm 1.00}$ & $88.3_{\pm 0.56}$ &   $84.2_{\pm 0.69}$ \\ 
CL  & $82.7_{\pm 0.57}$ & $67.5_{\pm 1.83} $& $81.2_{\pm 0.34}$ & $73.1_{\pm 0.66} $ & $79.6_{\pm 0.17}$ & $96.4_{\pm 0.05}$ & $75.1_{\pm 0.67}$ & $94.2_{\pm 0.24}$ & $76.0_{\pm 2.16}$ &   $59.4_{\pm 4.20}$\\ 
ELR & $\bm{98.1}_{\pm 0.04}$ & $\bm{97.8}_{\pm 0.07} $& $85.3_{\pm 0.23}$ & $83.4_{\pm 0.02} $ & $81.8_{\pm 0.26}$ & $97.5_{\pm 0.21}$ & $76.6_{\pm 0.10}$ & $96.5_{\pm 0.11}$ & $88.1_{\pm 0.82}$ &   $85.7_{\pm 0.06}$\\ \hline
FCE. & $95.4_{\pm 0.25} $ & $92.3_{\pm 0.13} $ & $83.6_{\pm 0.11} $ & $79.9_{\pm 0.78} $ & $83.1_{\pm 0.12}$ & $98.4_{\pm 0.20}$ & $80.6_{\pm 0.12}$ & $98.0_{\pm 0.03}$ & $84.7_{\pm 0.40}$ &  $75.1_{\pm 0.04}$ \\
\rowcolor{green}
FCE$+$B & $\boxed{95.7_{\pm{0.18}}}$ & $\boxed{92.7_{\pm{0.74}}}$ & $\boxed{84.8_{\pm{0.26}}}$ & $\boxed{81.7_{\pm{0.27}}}$  & $\boxed{83.4_{\pm{0.09}}}$ & $\boxed{\bm{98.5}_{\pm{0.03}}}$ & $\boxed{81.6_{\pm{0.51}}}$ & $\boxed{\bm{98.1}_{\pm{0.15}}}$ & $\boxed{86.7_{\pm_{0.21}}}$ & $\boxed{82.2_{\pm_{0.06}}}$\\ 
\hline
GCE  & $94.4_{\pm 0.36}$ & $83.8_{\pm 1.14} $ &  $86.4_{\pm 0.24}$ & $81.6_{\pm 0.37}$ & $84.3_{\pm 0.13}$ & $98.4_{\pm 0.08}$ & $82.7_{\pm 0.07}$ &   $97.9_{\pm 0.02}$ & $81.1_{\pm 0.72}$ & $60.0_{\pm 1.31} $ \\
\rowcolor{green}
GCE$+$B & $\boxed{96.6_{\pm 0.22}}$ & $\boxed{94.0_{\pm 0.13}} $ & $\boxed{86.5_{\pm 0.56}}$ & $\boxed{85.5_{\pm 0.13}}$ & $84.1_{\pm 0.29}$ & $\boxed{98.4_{\pm 0.04}}$ & $\boxed{82.8_{\pm 0.28}}$ &  $\boxed{98.0_{\pm 0.06}}$ & $\boxed{86.1_{\pm 0.22}}$ & $\boxed{79.0_{\pm 1.17}}$ \\
\hline
SCE & $89.5_{\pm 5.29} $& $70.2_{\pm 0.69} $& $82.7_{\pm 0.64} $& $74.4_{\pm 0.37} $& $82.1_{\pm 0.33}$ & $96.8_{\pm 0.10}$ & $79.6_{\pm 0.61}$ & $95.4_{\pm 0.15}$ & $78.2_{\pm 0.42}$ &   $59.0_{\pm 4.43}$\\
\rowcolor{green}
SCE$+$B & $\boxed{97.0_{\pm 0.16}}$ & $\boxed{93.4_{\pm 0.29}} $& $\boxed{87.5_{\pm 0.22}}$ & $\boxed{85.2_{\pm 0.98}} $ & $\boxed{83.5_{\pm 0.29}}$ & $\boxed{97.3_{\pm 0.14}}$ & $\boxed{81.8_{\pm 0.52}}$ & $\boxed{96.4_{\pm 0.20}}$ & $\boxed{88.9_{\pm 0.44}}$ &   $\boxed{84.7_{\pm 0.37}}$\\
\hline
CE & $80.8_{\pm 2.31}$ & $67.3_{\pm 0.80} $& $80.9_{\pm 1.11} $& $72.1_{\pm 2.16} $& $79.9_{\pm 0.28}$ & $96.4_{\pm 0.08}$ & $75.6_{\pm 0.20}$ & $94.2_{\pm 0.24}$ & $76.9_{\pm 1.22}$  &   $59.9_{\pm 2.15}$ \\
\rowcolor{green}
CE$+$B & $\boxed{96.2_{\pm 0.32}}$ & $\boxed{93.0_{\pm 0.09}}$& $\boxed{87.9_{\pm 0.10}}$ & $\boxed{84.7_{\pm 0.37}} $ & $\boxed{80.8_{\pm 0.08}}$ & $\boxed{97.0_{\pm 0.04}}$ & $\boxed{78.9_{\pm 0.12}}$ & $\boxed{96.1_{\pm 0.26}}$ & $\boxed{84.5_{\pm 0.73}}$ &   $\boxed{76.0_{\pm 1.13}}$\\ 
\hline
CEP & $97.5_{\pm 0.08 }$ & $92.1_{\pm 0.44} $ & $87.8_{\pm 0.12 }$ & $84.8_{\pm 0.23}$ & $85.5_{\pm 0.10}$ & $98.1_{\pm 0.07}$ & $84.3_{\pm 0.22}$ & $97.6_{\pm 0.14}$ & $84.2_{\pm 0.51}$ &  $58.2_{\pm 2.94}$\\
\rowcolor{green}
CEP$+$B & $95.6_{\pm 0.32}$ & $85.5_{\pm 0.77} $& $\boxed{\bm{88.1}_{\pm 0.31}}$ & $84.2_{\pm 0.33} $ &  $\boxed{\bm{85.8}_{\pm 0.12}}$ & $\boxed{98.3_{\pm 0.02}}$ &  $\boxed{\bm{84.8}_{\pm 0.10}}$ & $\boxed{98.0_{\pm 0.04}}$ & $\boxed{88.5_{\pm 0.32}}$ &   $\boxed{85.1_{\pm 0.20}}$\\ 
\end{tabular}
\end{adjustbox}
    \caption{Test accuracies obtained by using different losses on the noisy MNIST/ FashionMNIST/EMNIST/CIFAR10 datasets. Losses implementing the noise-bound shaded in blue. When using this bound provides benefit, the corresponding value is \boxed{boxed}. Overall top values in \textbf{bold}.}\label{table:MNIST}
    \centering
\end{table*}

\begin{table*}[!htb]
    \centering
    \begin{adjustbox}{max width=0.98\textwidth}
    \begin{tabular}{l|cccc|cccc|cc}
    \textbf{}  & \multicolumn{4}{c|}{CIFAR100} & \multicolumn{4}{c|}{ASYM-CIFAR100} & \multicolumn{2}{c}{Non-Uniform-EMNIST} \\
\multirow{2}{*}{Losses} & \multicolumn{2}{c|}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{2}{c|}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{2}{c}{0.6} \\
  \multicolumn{1}{c|}{} & Top1 & \multicolumn{1}{c|}{Top5} & Top1 & Top5 & Top1 & \multicolumn{1}{c|}{Top5} & Top1 & Top5 & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c}{Top 5} \\ \hline
 MSE & $57.2_{\pm 0.93}$ & $78.6_{\pm 0.25}$ & $40.6_{\pm 0.38}$ & $63.0_{\pm 0.24}$ & $56.3_{\pm 0.11}$  & $82.6_{\pm 0.22}$  & $40.7_{\pm 0.12}$ & $74.4_{\pm 0.25}$ & $44.7_{\pm 2.66}$ & $86.7_{\pm 3.10}$ \\
MAE & $10.0_{\pm 0.11}$  & $13.8_{\pm 0.28}$ & $7.6_{\pm 1.89}$  & $11.6_{\pm 1.25}$ & $7.1_{\pm 6.02}$  & $11.1_{\pm 6.6}$  & $11.1_{\pm 5.43}$  & $25.1_{\pm 5.76}$ &  $9.8_{\pm 1.74}$ & $23.1_{\pm 1.80}$\\
NCE  & $38.7_{\pm 3.13}$ & $51.8_{\pm 3.77}$ & $19.1_{\pm 0.20}$ & $28.8_{\pm 0.15}$ & $16.3_{\pm 1.24}$ & $25.4_{\pm 1.80}$  & $21.8_{\pm 1.24}$ & $37.2_{\pm 1.80}$  & $18.0_{\pm 1.17}$ &  $38.8_{\pm 1.93}$ \\
MixUp  & $59.6_{\pm 0.31}$ & $81.5_{\pm 0.39}$ & $51.3_{\pm 8.63}$ & $75.8_{\pm 8.09}$ & $61.2_{\pm 0.88}$ & $86.0_{\pm 1.12}$  & $47.2_{\pm 0.60}$ & $81.3_{\pm 0.23}$ &  $\bm{52.4}_{\pm 0.80}$ & $\bm{95.5}_{\pm 0.08}$ \\
Sph.   & $57.7_{\pm 0.18}$ & $82.9_{\pm 0.54}$ & $48.8_{\pm 0.51}$ & $74.3_{\pm 0.73}$ & $54.2_{\pm 0.32}$ & $81.2_{\pm 0.29}$  & $39.2_{\pm 0.31}$ & $72.1_{\pm 0.15}$ & $41.9_{\pm 0.10}$ &  $94.4_{\pm 0.04}$ \\
Boot.  & $54.0_{\pm 0.37}$ & $76.4_{\pm 0.39}$ & $37.7_{\pm 0.89}$ & $60.9_{\pm 1.52}$ & $56.0_{\pm 0.34}$ & $83.8_{\pm 0.03}$  & $43.2_{\pm 0.35}$ & $78.3_{\pm 0.20}$ &  $49.1_{\pm 0.29}$ & $95.3_{\pm 0.42}$ \\
Trunc. & $58.1_{\pm 0.36}$ & $82.7_{\pm 0.37}$ & $50.9_{\pm 1.17}$ & $77.2_{\pm 0.59}$ & $56.3_{\pm 0.62}$ & $82.3_{\pm 0.61}$  & $45.2_{\pm 0.81}$ & $75.6_{\pm 0.29}$ &  $23.7_{\pm 0.98}$ & $40.1_{\pm 1.24}$ \\
CL & $53.0_{\pm 0.21}$ & $76.3_{\pm 0.19}$ & $36.3_{\pm 0.77}$ & $60.1_{\pm 0.66}$ & $55.3_{\pm 0.48}$ & $83.5_{\pm 0.28}$  & $42.4_{\pm 0.45}$ & $78.1_{\pm 0.14}$ &  $48.2_{\pm 0.45}$ & $95.0_{\pm 0.04}$ \\
ELR & $10.4_{\pm 0.24}$ & $31.7_{\pm 0.44}$ & $10.0_{\pm 0.64}$ & $30.1_{\pm 0.88}$ & $10.8_{\pm 0.21}$ & $32.7_{\pm 0.53}$  & $10.3_{\pm 0.39}$ & $30.8_{\pm 0.35}$ &  $40.3_{\pm 0.39}$ & $93.0_{\pm 0.24}$ \\\hline
FCE & $56.9_{\pm 0.58}$ & $79.2_{\pm 0.14}$ & $43.7_{\pm 0.15}$ & $66.2_{\pm{0.19}}$ & $55.3_{\pm 0.54}$ & $83.5_{\pm 0.24}$  & $41.4_{\pm 0.55}$ & $77.3_{\pm 0.75}$ &  $39.0_{\pm 0.05}$ & $67.8_{\pm 0.47}$ \\
\rowcolor{green}
FCE$+$B & $56.1_{\pm 2.22}$ & $\boxed{81.8_{\pm 1.37}}$ & $\boxed{50.2_{\pm 0.02}}$ & $\boxed{77.2_{\pm 0.19}}$ & $54.2_{\pm 0.44}$ & $83.3_{\pm 0.43}$  & $\boxed{43.8_{\pm 0.02}}$ & $\boxed{77.5_{\pm 0.13}}$ &  $\boxed{40.0_{\pm 0.35}}$ & $\boxed{73.2_{\pm 0.08}}$ \\
\hline
GCE  & $60.0_{\pm 0.13}$ & $82.6_{\pm 0.63}$ & $44.9_{\pm 0.07}$ & $67.2_{\pm 0.34}$ & $53.8_{\pm 0.55}$ & $81.6_{\pm 0.14}$  & $39.4_{\pm 0.44}$ & $74.0_{\pm 0.36}$ &  $44.8_{\pm 0.62}$ & $91.2_{\pm 0.70}$ \\
\rowcolor{green}
 GCE$+$B & $59.4_{\pm 0.02}$ & $\boxed{83.5_{\pm 0.24}}$  & $\boxed{50.3_{\pm 0.11}}$ & $\boxed{75.3_{\pm 0.64}}$ & $\boxed{55.4_{\pm 0.55}}$ & $\boxed{83.0_{\pm 0.35}}$  & $\boxed{46.5_{\pm 1.44}}$ & $\boxed{77.7_{\pm 0.35}}$ &  $\boxed{47.1_{\pm 0.20}}$ & $\boxed{93.5_{\pm 0.43}}$ \\
\hline
SCE  & $55.9_{\pm 0.53}$ & $76.5_{\pm 0.15}$ & $38.7_{\pm 0.60}$ & $60.9_{\pm 0.41}$ & $57.5_{\pm 0.19}$ & $83.7_{\pm 0.17}$  & $43.3_{\pm 0.87}$ & $77.5_{\pm 0.75}$ &  $47.2_{\pm 0.33}$ & $92.5_{\pm 0.01}$\\
\rowcolor{green}
SCE$+$B & $55.5_{\pm 0.90}$ & $\boxed{77.4_{\pm 0.84}}$ & $\boxed{47.1_{\pm 1.32}}$ & $\boxed{69.2_{\pm 1.18}}$ & $\boxed{57.9_{\pm 0.83}}$ & $83.7_{\pm 0.41}$  & $\boxed{50.0_{\pm 1.62}}$ & $\boxed{80.4_{\pm 0.65}}$ &  $\boxed{47.9_{\pm 0.80}}$ & $\boxed{93.8_{\pm 0.05}}$ \\
\hline
CE & $52.3_{\pm 1.35}$ & $75.6_{\pm 0.93}$ & $35.3_{\pm 1.14}$ & $59.3_{\pm 0.81}$ & $54.9_{\pm 0.12}$ & $83.3_{\pm 0.25}$ & $42.4_{\pm 0.16}$ & $78.9_{\pm 0.56}$ &  $48.6_{\pm 0.11}$ & $95.3_{\pm 0.10}$ \\
\rowcolor{green}
CE$+$B & $\boxed{50.9_{\pm 1.01}}$ & $\boxed{76.5_{\pm 0.86}}$ & $\boxed{39.9_{\pm 1.02}}$ & $\boxed{65.8_{\pm 1.19}}$ & $52.9_{\pm 1.86}$ & $83.2_{\pm 0.88}$  & $34.7_{\pm 2.51}$ & $73.4_{\pm 1.50}$  &  $45.5_{\pm 5.11}$ & $93.0_{\pm 0.16}$ \\
 \hline
CEP  & $58.8_{\pm 0.87}$ & $78.6_{\pm 0.38}$ & $43.5_{\pm 0.24}$ & $65.1_{\pm 1.27}$ & $59.4_{\pm 0.08}$ & $82.2_{\pm 0.03}$  & $46.5_{\pm 0.17}$ & $76.4_{\pm 0.25}$ &  $48.2_{\pm 0.05}$ & $95.4_{\pm 0.07}$ \\
\rowcolor{green}
CEP$+$B &  $\boxed{\bm{62.3}_{\pm 0.87}}$ & $\boxed{\bm{85.1}_{\pm 0.46}}$ & $\boxed{\bm{54.3}_{\pm 0.86}}$ & $\boxed{\bm{79.2}_{\pm 0.93}}$ & $\boxed{\bm{63.0}_{\pm 0.92}}$ &  $\boxed{\bm{87.5}_{\pm 0.32}}$  & $\boxed{\bm{53.0}_{\pm 0.28}}$ & $\boxed{\bm{82.8}_{\pm 0.13}}$ &  $45.0_{\pm 0.48}$ & $95.0_{\pm 0.08}$\\
    \end{tabular}
    \end{adjustbox}
    \caption{Test accuracies for different losses on the noisy CIFAR100/Asym-CIFAR100/Non-Uniform EMNIST datasets. Losses implementing the noise-bound shaded in blue. When using this bound provides benefit, the corresponding value is \boxed{boxed}. Overall top values in \textbf{bold}. }
    \label{table:CIFAR}
\end{table*}

\section{Conclusion, Limitations and Further Work}
\paragraph*{} In this work, we have looked at mitigating the impact of label noise in forward-corrected losses by training subject to a bound, motivated by our observation that label noise implies a minimum achievable risk. 

\paragraph*{Summary} We began by defining a family of loss functions which we called `generalised forward-corrected losses' since they contain correction losses as a strict subset. We showed how some popular existing robust losses can be formulated as generalised forward-corrected loss functions. We explained how label noise implies the existence of a lower bound on the achievable risk. We proposed training a model and preventing the training loss going below a given threshold - we called this a `bounded loss'. We derived this lower bound for generalised forward-corrected losses showing it is the average entropy of the noisy label distribution (with respect to the entropy function of the base loss). We showed that uniform symmetric label noise is a `worst-case' noise meaning that it has the highest entropy for a given noise rate $\eta$. When the label noise rate is known and but the noise model is otherwise unknown, we proposed using this worst-case entropy as a bound for our bounded-loss. Finally, we empirically showed that training using this `noise-bound' improves the performance.   

\subsection{Limitations and Future Work} 
While our method has shown success in the settings we examined, its applicability does have limitations. The reliability of the derived bounds depends on the dataset being well-approximated as separable. For datasets with high inherent randomness, such as those in the medical domain, the proposed bounds may not be sufficient. Future work could aim to extend our results to environments with more prevalent noise. Additionally, our approach relies on approximating the average noise rate, which limits its applicability to scenarios where the noise rate is approximately known. Nonetheless, this represents a substantial improvement over existing methods that require modeling the entire noise structure.

Although our proposed method generally offers benefits, there are still observable differences in performance between different loss functions. Understanding why these differences occur is a crucial direction for future research. Another promising area of future work involves extending these ideas to backward-corrections \citep{fprop}. The results in Section~\ref{sec:entropy_bounding} and the loss functions used in our experiments focus on forward-corrections. Backward-corrections, which are more prone to overfitting due to their tendency to generate large gradients, present an interesting challenge. It would be worthwhile to explore how loss bounding affects the robustness of these loss functions.


%\bibliographystyle{plain}
\bibliographystyle{apalike}
\bibliography{Bounding/bounding}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\onecolumn


\section{Notation and Terminology}\label{sec:notation}
\begin{longtable}{>{\raggedright\arraybackslash}p{2cm} p{12cm}}
\caption{Notation Table: Table summarising the notation used in this study.}\label{ch1:tab:notation} \\
\hline
\textbf{Symbol} & \textbf{Description} \\
\hline
\endfirsthead

\hline
\textbf{Symbol} & \textbf{Description} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

$c$ & Number of classes/labels. \\
$\mathcal{X}$ & The data domain, a subset of $\mathbb{R}^d$. \\
$\mathcal{Y}$ & The label space, defined as ${1, 2, 3, \ldots, c}$. \\
$\Delta$ & Probability simplex: The set of vectors $(p_1, p_2, \ldots, p_c)$ where each $p_i \geq 0$ and $\sum p_i = 1$. \\
$\bm{q}$ & A probability vector representing a forecast. \\
$\bm{p}$ & A probability vector representing ground-truth probabilities. \\
$\bm{q}:\mathcal{X}\rightarrow \Delta$ & A probability estimator model producing a forecast at each point in $\mathcal{X}$.\\
$\bm{p}(y\mid x)$ & The vector representing the class posterior probabilities at $x$, expressed as $\bm{p}(y\mid x) = (p(y=1\mid x), p(y=2\mid x), \ldots, p(y=c\mid x))$. \\
$f$ & A classifier function mapping each point in $\mathcal{X}$ to a label in $\mathcal{Y}$. \\
$L$ & The loss function used to evaluate the accuracy of predictions against actual labels. \\
$\bm{L}$ & The vector-valued function of the loss function $L$, where $\bm{L}(\bm{q}) = (L(\bm{q},1), \ldots, L(\bm{q},c))$. \\
$R_{L}(\bm{q})$ & The $L$-risk of an estimator $\bm{q}$. \\
$R_{L}(\bm{q})(x)$ & The \emph{pointwise} $L$-risk of an estimator $\bm{q}$ at $x$. \\
$R^{\eta}_{L}(\bm{q})$ & The noisy $L$-risk of an estimator $\bm{q}$. \\
$\mathcal{H}$ or $\mathcal{H}_{L}$ & The entropy function corresponding to the loss function $L$. \\
$H_{L}(\bm{p},\bm{q})$ & The expected $L$-loss for a forecast $\bm{q}$ given the true label distribution $\bm{p}$. \\
$\eta$ & The noise rate of the label noise model. \\
$y, \widetilde{y}$ & The actual label and the noisy label, respectively. \\
$p(x,y)$ & The joint distribution of data and labels. \\
$\widetilde{p}(x,y)$ & The joint distribution of data and labels after corruption by label noise. \\
$p(\widetilde{y}\mid y, x)$ & The noise model generating noisy labels $\widetilde{y}$ from clean labels $y$ given $x$. \\
$T$ & The label noise transition matrix describing the probabilities of transforming a true label into a noisy label. \\
$\bm{e}_k$ & The standard basis vector in $\mathbb{R}^c$ where only the $k$\textsuperscript{th} element is 1, and all others are 0.
\end{longtable}

\subsection{Terminology}
The majority of terminology which we adopt is the same as that found in any other contemporary machine learning paper on label noise. Label noise is categorised into two primary types: closed-set and open-set. We specifically address \emph{closed-set label noise}, wherein the original label set and the noisy label set are identical. This is in contrast to open-set noise where the true label may not be included in the established label set \citep{wei2021open}. For instance, in a web-scraped dataset of animal images, a photograph of Tiger Woods' might be erroneously labeled as Tiger', even though the correct label, `golfer', is absent from the set of labels When a label noise model has no dependence on the datapoint $p(\widetilde{y}\mid y,x)=p(\widetilde{y}\mid y)$ we say that it is `uniform' or `class-conditional', otherwise we sat that the label noise model is non-uniform. A uniform label noise model is called `symmetric' where the transition probability between any distinct classes is the same and `asymmetric' otherwise. Pairwise label noise is a subset of asymmetric label noise where mislabelling occurs between specific class pairs. %Other terminology is defined 

% We use $c$ for the number of classes, with $\mathcal{X} \subset \mathbb{R}^d$ representing the dataspace and $\mathcal{Y} \coloneq \{1, 2, \ldots, c\}$ the label space. The probability simplex $\Delta$ includes probability vectors with $c$ non-negative components that sum to one. A forecast, denoted $\bm{q} \in \Delta$, predicts a distribution over labels, distinguishing it from the true label distribution $\bm{p} \in \Delta$. A probability estimator model $\bm{q}: \mathcal{X} \rightarrow \Delta$ outputs forecasts for each dataspace location, which induces a plug-in classifier $f$ that assigns labels. More broadly we use \textbf{bold} to distinguish vector quantities from scalars. The loss function $L$ evaluates forecasts against actual labels, with $H_L(\bm{p}, \bm{q})$ denoting the expected loss for forecast $\bm{q}$ under the true distribution $\bm{p}$. 

% Risk is denoted by $R_L(\bm{q})$ for (clean) generalised risk, (expected loss averaged over dataspace) and $\widehat{R}_L$ for (clean) empirical risk, calculated over a dataset. $R^{\eta}_L$ denotes the noisy generalised risk.  A tilde is used to indicate noise-affected quantities so that, for example, $\widetilde{y}$ is a noisy label and $\widetilde{p}(\widetilde{y}\mid x)$ denotes a noisy posterior over labels.

% $\eta(x)\coloneq p(\widetilde{y}\neq y\mid x)$ denotes the noise rate at $x$, $\eta\coloneq p(\widetilde{y}\neq y)$ denotes the average noise rate across dataspace. $T$ denotes the transition matrix, detailing the probabilities of moving between clean and noisy labels in the case of class-conditional label noise.


\section{Related Work}\label{sec:related_work}

\textbf{Corruption identification methods:}  Methods in this class first identify different types of corrupted samples and then re-weight, refine or remove these from the dataset. Many such methods rely on the heuristic that noisy samples have higher losses, especially earlier in training. This is based upon the well-known observation that complex models generally learn to classify easier data points before over-fitting on noise \citep{memorisation}. \citet{sefie} use the entropy of the historical prediction distribution to identify refurbishable samples. \citet{bmm} deploy a beta mixture model in the loss space and use the posterior probabilities that a sample is corrupted in the parameters of a bootstrapping loss. \citet{curiculum} define a loss which ignores samples that incur a higher loss value. Other approaches include a two-network model \citep{dividemix} in which a Gaussian mixture model selects clean samples based on their loss values. These samples are then taken and used to train the other network. A number of other two-network models work on similar lines. Co-teaching \citep{coteaching} trains two networks, one on the outputs of the other with the lowest loss values. Decoupling \citep{decoupling} has the two networks update on the basis of disagreement with each other. Mentor-Net \citep{mentornet} harnesses a teacher network for training a student network by re-weighting probably correct samples.

\cite{metaLabelCorrect} is a meta-learning approach in which a label correction network corrects labels and feeds them to a classifier to train. This is done so that the performance of the classifier is optimised on a held-out validation set. Other meta-learning approaches include \cite{meta_gradient} in which samples are re-weighted so that the learned classifier generalises better to a held-out clean meta-set. Similarly, in \cite{metaLabels} (soft), labels are treated as learnable parameters and learned to maximise the performance on the meta-set. Other corruption identification methods expect that noisily labelled data lie heavily out of class distribution under an appropriate metric. In FINE \citep{eigenfine}, noisy samples are detected and removed using an eigendecomposition in the latent space. Alternatively, a KNN \cite{knn} in the latent space can identify and select samples based on their coherence to their neighbours' classes.

%Related to this, cross-data loss terms can encourage consistency between different views of a data sample. \citet{gjs} achieve data consistency by applying a generalised Jensen-Shannon divergence to different augmentations of a datapoint. In Mix-Up \citep{mixup}, consistency comes from training on data-label pairs obtained by convex combinations of those from the training set.

 % Other corruption identification methods may use the latent space to identify out-of-distribution data using eigendecomposition \citep{eigenfine} or KNN \citep{knn}. Alternatives achieve consistency between different views of a data sample by minimising a Jensen-Shannon divergence between different augmentations \citep{gjs} or convex combinations of data-label pairs \citep{mixup}.

\subsection{Robust Loss Functions}
An important set of methods for learning in the presence of noisy labels are robust loss methods. These methods work by substituting the cross-entropy objective for a loss which is less prone to inducing overfitting in the presence of label noise.  An advantage here is the simplicity of these methods, as they do not require multiple networks or complex noise detection pipelines. This makes them suitable for plug-and-play use in any setting. These approaches may, crudely, be broken down into two classes; regularisation-based loss and correction-based losses. 

\textbf{Correction-Based Losses:} Correction-based loss functions are motivated by the observation that label noise causes a distortion of the risk objective which can prevent consistency/Bayes-consistency. Methods in this class achieve robustness by altering the loss function to correct this distortion. We further subdivide these into correction-based losses and noise-tolerant losses. The former corrects the loss to compensate for the noising procedure \citep{larsen, mnihHinton, gold, old_backward}. This procedure involves using noisy \citep{fprop} or clean data \citep{trustedData} to infer the noise transition matrix. The estimated noise model is then used to noise the model outputs, in the case of the `forward-correction', or denoise the labels, in the case of the `backward-correction' \citep{old_backward, fprop}. A downside of these methods is the difficulty in estimating a noising matrix for a large number of classes and difficulty handling non-uniform noise models which impacts generality \citep{noiseAdapt, fprop_old}. Noise-tolerant loss functions \citep{robust_theory, robust_theory_earlier_binary, unhinged, normalised_losses} use loss functions which ensure Bayes-consistency despite the presence of label noise, without the need to apply a correction to the loss function. 

\textbf{Regularisation-Based Robust Losses:} Regularisation-based robust loss functions regularise the loss so that it less susceptible to overfitting to noise than the commonly used cross-entropy \citep{janocha2017loss}. Regularisation-based loss functions are varied. Many methods apply regularisation to ensure consistency between the predicted labels of nearby datapoints \citep{mixup, englesson2021consistency, gjs, neighbour} or consistency of model predictions over time \citep{elr, cheng2024dynamic}. \citet{janocha2017loss} observe that $L^p$-losses typically used for regression show good robustness in a classification setting. This is particularly true for the MAE loss (Mean Absolute Error), which exhibits good robustness albeit with a tendency to under-fit and train slowly \citep{normalised_losses}. This observation motivates a set of methods which combine or interpolate between CE and MAE to obtain the best of both.
%Other methods make alterations to the MAE loss which is known to be robust but with a tendency to overfit. 
\citet{sce} propose a solution to this by adding a `reverse cross-entropy' (RCE) term to the usual cross-entropy (CE) term. \citet{taylorce} curtail the Taylor expansion of cross-entropy making it perform more like MAE. Generalised Cross-Entropy \citep{GCE_Loss} construct a family of losses which interpolate between CE and MAE using a Box-Cox transformation in order to get the best of both. Other methods soften of mix labels to avoid overfitting \citep{bootstrap, labelSmoothing, soft_labels}.
%, or use regularisers \citep{elr,tanaka}. 
\citet{flooding}, in-line with our work, bound the loss to prevent overfitting. However, their method is only briefly discussed in relation to label noise and provides no mechanism for selecting a loss bound. We ground this work firmly in the context of label noise and provide theoretical results for producing bounds on the loss.

% \citet{robust_theory} offer some theoretical insights, suggesting robustness may be obtained by choosing losses which are bounded or satisfy a `symmetry' property. On the back of this \citet{normalised_losses} show that one can take unbounded loss functions and re-normalise them to achieve this objective. 

\section{Proofs}\label{sec:bounding_proofs}
The following standard result regarding proper losses due to Savage \citep{savagen, gneiting2007strictly} is indispensable in subsequent proofs.
\begin{theorem}
    [Savage's Theorem]\label{thm:savages}
A differentiable loss function $L$ is (strictly) proper if and only if there exists a (strictly) concave function $\mathcal{J}: \mathbb{R}^c \rightarrow \mathbb{R}$ such that for each $\bm{q} \in \Delta$ and $k \in \mathcal{Y}$,
\begin{align*}
    L(\bm{q}, k) = \nabla \mathcal{J}(\bm{q})(\bm{e}_k - \bm{q}) + \mathcal{J}(\bm{q}).
\end{align*}
Moreover, $\mathcal{J}$ is precisely the entropy function for $L$. Thus, in particular, a loss is (strictly) proper if and only if its associated entropy function is (strictly) concave.
\end{theorem}

\begin{definition}[Generalised Forward-Correction]
 Let $L_f$ be a loss function and $f:\Delta\rightarrow \Delta$ be an injective function. We say $L_f$ is a `generalised forward-corrected loss' if there exists a loss function $L$ such that for all $\bm{q}\in \Delta$, $k\in \{1,2,\ldots,c\}$
 \begin{align*}
     L_f(\bm{q}, k) = L(f(\bm{q}), k)
 \end{align*}
 We refer to $L$ as the \textbf{base loss}. $f$ can be thought of as a label noise model. 
\end{definition}


\begin{lemma}
    The GCE, SCE and FCE losses can be formulated as generalised forward-correction losses with a proper base loss. The noise models $f_{GCE}, f_{SCE}, f_{FCE}$ satisfy
    \begin{align*}
        (f^{-1}_{GCE}(\bm{p}))_i &= \frac{p_i^{\frac{1}{1-a}}}{\sum_{i=1}^c p_i^{\frac{1}{1-a}}},\\
        (f^{-1}_{SCE}(\bm{p}))_i &= \frac{p_i}{\lambda-Ap_i},\\
        f_{FCE}(\bm{p}) &= T^{-1}\bm{p},
    \end{align*}
     where $T$ is the invertible stochastic matrix used to define the correction, and $\lambda$ is a constant selected to ensure the correct normalisation.
\end{lemma}
\textbf{Proof Idea:} Suppose that $L$ is a proper loss, let $f:\Delta\rightarrow\Delta$ be injective noise-model, and consider the minimiser of the expected loss defined $L_f(\bm{q},k )\coloneq L(f(\bm{q}),k)$ at $\bm{p}\in\Delta$;
\begin{align*}
    \argmin_{\bm{q}\in\Delta} H(\bm{p},\bm{q}) &= \argmin_{\bm{q}\in\Delta}\sum_{i=1}^c p_iL_f(\bm{q},i)\\
    &= \argmin_{\bm{q}\in\Delta}\sum_{i=1}^c p_iL(f(\bm{q}),i).
\end{align*}
Since $L$ is proper then we know this is minimised by $\bm{q}$ such that $f(\bm{q}) = \bm{p}$. In other words \emph{we can uncover the noise model $f$ by finding the minimiser of the expected loss}. This is how we find $f$ for each of the loss functions. The core idea of the following proof is to write out the expected loss for each loss function and, for each $\bm{p}\in\Delta$, to find $\argmin_{\bm{q}\in\Delta} H(\bm{p},\bm{q})$. Assuming that this $\argmin$ consists of a single point then this induces a map $f(\bm{p}) \coloneq \argmin_{\bm{q}\in\Delta} H(\bm{p},\bm{q})$ which, for the reasons given, can be identified with the noise model. 
%We are assuming that each loss function can be formulated as a correction applied to a proper loss e.g. $L_{GCE}(\bm{q},k) = L(f_{GCE}(\bm{q}),k)$, where $L$ is proper. Thus if the expected GCE loss is minimised by some function $q=f(p)$, then the properness of $L$ implies that $f=f_{GCE}$. 


\begin{proof}

We begin by introducing the following notation: Let $L$ be an elementwise loss and let $\bm{p}, \bm{q}$ be two distributions, we denote the expected loss of $\bm{q}$ with respect to $\bm{p}$ to be $H_{L}(\bm{q}, \bm{p}) := \sum_{i=1}^c p_iL(\bm{q}, i)$.

Let us begin by considering GCE. The expected loss may be written $L_{GCE}(\bm{q}, \bm{p}) := \sum_{i=1}^c p_iL_{GCE}(\bm{q}, i) := \sum_{i=1}^c p_i\frac{1-q_i^a}{a}$. We find the minima by constructing the Langrangian $A(\bm{q},\lambda) := \sum_{i=1}^c p_i\frac{1-q_i^a}{a} +\lambda (\sum_{i=1}^c q_i - 1)$. By taking partials and equating to zero, we obtain $q_i^{1-a} = \frac{ap_i}{\lambda}, \forall i$. Using the fact that $\sum_{i=1}^c q_i = 1$ one may find the value of $\lambda$. Specifically, $\lambda = a(\sum_{i=1}^c p_i^{\frac{1}{1-a}})^{1-a}$. Thus overall one has $q^*_i = (\frac{ap_i}{\lambda})^{\frac{1}{1-a}} = \frac{p_i^{\frac{1}{1-a}}}{\sum_{i=1}^c p_i^{\frac{1}{1-a}}}$. Let us repeat this for the SCE loss. The expected loss may be written $L_{SCE}(\bm{q}, \bm{p}):= \sum_{i=1}^c p_iL_{SCE}(\bm{q}, i) := \sum_{i=1}^c p_i(A(1-q_i)-log(q_i))$. As before, we construct the relevant Lagrangian and find the stationary points: $B(\bm{q}, \lambda) := \sum_{i=1}^c p_i(A(1-q_i)-log(q_i)) + \lambda (\sum_{i=1}^c q_i - 1)$. Taking partials and equating to zero we obtain $p_i(A+\frac{1}{q_i}) = \lambda \implies q_i^* = \frac{p_i}{\lambda-Ap_i}$. Here the value of the normalisation constant $\lambda$ cannot be found in closed form for high values of $c$ and must be computed numerically. Finally, we consider the forward-corrected CE loss. We assume that the loss is corrected by some invertible stochastic matrix $T$. $L_{F}(\bm{q}, \bm{p}):= \sum_{i=1}^c p_iL_{F}(\bm{q}, i) := \sum_{i=1}^c -p_ilog((T\bm{q})_i)$. We remark that since CE is proper that this is minimised on the simplex by $\bm{p} = T\bm{q}^*\iff \bm{q}^* = T^{-1}\bm{p}$. For each loss, the function $f$ obtained is injective as desired.
\end{proof}

\subsection{Entropy Bounds}
% \textbf{Lemma 5.1}
%     Let $L$ be an elementwise, strictly $f-$proper loss. For any probability estimator $\bm{q}(x)$, we may derive a bound for the noisy pointwise risk that is tight and obtained when $\bm{q}(x)=\Tilde{p}(\tilde{y}\vert x)$. Specifically, for all $x\in supp(p(x))$ we have; 
%     \begin{align}
%         R^{\eta}_{L}(\bm{q})(x) \geq R^{\eta}_{L}(f(\Tilde{p}(\tilde{y}\vert x)))
%     \end{align}


\begin{lemma}
    Let $L_f$ be a generalised-correction-loss whose `base-loss' $L$ is strictly proper (Recall the definition of `base-loss' from Definition~\ref{def:gen_corr}). The noisy risk of any probability estimator $\bm{q}$ is lower bounded:
        \begin{align}
        R^{\eta}_{L_f}(\bm{q}) \geq \mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{\bm{p}}(\tilde{y}\vert x))],
    \end{align}
    where $\mathcal{H}$ is the entropy function of the base-loss. This bound is tight when $f$ is equal to the true noise model. Equality is attained by setting $\bm{q}(x)=f^{-1}\left(\widetilde{\bm{p}}(\widetilde{y}\vert x)\right)$.\footnote{Note that if $f$ is the true noise model then $\widetilde{\bm{p}}(\widetilde{y}\mid x) \in f(\Delta)$ and the inverse is unique by injectivity.}
\end{lemma}
\begin{proof}
    Recollect that $L_f$ is a generalised forward-correction loss with (strictly) proper base loss $L$: $L_f(\bm{q},k)=L(f(\bm{q}, k))$. Let $x$ be some arbitrary point in the support of $p(x)$ and let $\bm{q}(x)$ be some probability estimator. The pointwise noisy risk of $\bm{q}$ at $x$ may be written as
    \begin{align*}
        R^{\eta}_{L_f}(\bm{q})(x) &\coloneq \sum_{i=1}^c \widetilde{p}(\widetilde{y}=i\vert x)L_f(\bm{q}(x), i) \\
        &= \sum_{i=1}^c \widetilde{p}(\widetilde{y}=i\vert x)L(f(\bm{q}(x)), i) \\
        &\geq \sum_{i=1}^c \widetilde{p}(\widetilde{y}=i\vert x)L(\widetilde{\bm{p}}(\widetilde{y}\vert x), i)\\
        &\eqcolon \mathcal{H}(\widetilde{\bm{p}}(\widetilde{y}\vert x))
    \end{align*}
    The inequality follows from the definition of the properness of $L$. Inequality~\ref{Equation:general_entropy_bound} follows by taking expectation with respect to $p(x)$ on both sides. Equality is attained setting by $f(\bm{q}(x)) = \widetilde{\bm{p}}(\widetilde{y}\vert x))$ for each $x$, (equivalently $f^{-1}(\bm{q}(x)) = \widetilde{\bm{p}}(\widetilde{y}\vert x)$) which is possible when $f$ is the true noise model as then $\widetilde{\bm{p}} \in f(\Delta)$. The injectivity of $f$ (as specified in the definition of $f-$proper) means this occurs uniquely at $\bm{q}(x) = f(\widetilde{\bm{p}}(\widetilde{y}\vert x))$ as desired.
\end{proof}

\begin{lemma}[Class-Conditional Label Noise]
    When the classes are balanced and label noise is asymmetric and given by transition matrix $T$, the noisy risk of a probability estimator $\bm{q}$ may be lower bounded as follows,
\begin{align}
     R^{\eta}_{L_f}(\bm{q}) \geq \frac{1}{c}\sum_{i=1}^c \mathcal{H}(\bm{T}_{\cdot,i} ),
 \end{align}
 where $\bm{T}_{\cdot, i}$ denotes the $i$\textsuperscript{th} column of the matrix $T$.
 \end{lemma}
 \begin{proof}
     The right-hand side of Inequality~\ref{Equation:general_entropy_bound} can be written
     \begin{align*}
         \mathbb{E}_{x\sim p(x)}\left[\mathcal{H}(\widetilde{\bm{p}}(\tilde{y}\vert x))\right] &= \mathbb{E}_{x\sim p(x)}\left[\sum_{i=1}^c \widetilde{p}(\widetilde{y}=i\vert x)L(\widetilde{\bm{p}}(\widetilde{y}\vert x), i)\right]\\
          &= \mathbb{E}_{x\sim p(x)}\left[T\bm{p}(y\vert x)\cdot \bm{L}(T\bm{p}(y\vert x))\right]\\
          &= \frac{1}{c}\sum_{k=1}^c (T\bm{e}_k)\cdot \bm{L}(T\bm{e}_k),
     \end{align*}
     where the final equality comes from using the fact that classes are balanced and all points are anchor points. 
     This is equal to 
     \begin{align*}
          \frac{1}{c}\sum_{k=1}^c T_{\cdot, k}\cdot \bm{L}(T_{\cdot, k}) = \frac{1}{c}\sum_{k=1}^c \mathcal{H}(\bm{T}_{\cdot,k} ), 
     \end{align*}
     as desired.
 \end{proof}

 \begin{corollary}[Uniform Symmetric Label Noise]\label{cor:uniform_symmetric}
    Given uniform, symmetric label noise at rate $\eta$, the risk associated with any probability estimator can be bounded as follows:
\begin{align}
     R^{\eta}_{L_f}(\bm{q}) \geq \mathcal{H}\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big).
 \end{align}
 This can be written equivalently as 
\begin{align*}
    R^{\eta}_{L_f}(\bm{q}) \geq  \bm{u}_{\text{sym}}(\eta,c)\cdot \bm{L}(\bm{u}_{\text{sym}}(\eta,c)).
\end{align*}
where
\begin{align}\label{Equation:u_notation}
    \bm{u}_{\text{sym}}(\eta, c) := \left(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1}\right).
\end{align}
\end{corollary}
\begin{proof}
    When label is symmetric every column of the matrix $T$ is a permutation of $\left(1-\eta, \frac{\eta}{c-1}, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1}\right)$. The result follows immediately from the symmetry assumption on the entropy. 
\end{proof}


 \begin{lemma}[Non-Uniform Symmetric Label Noise]\label{prop:det_proper}
      Let $p(x,y)$ be a separable distribution, and let $\widetilde{p}(x,\widetilde{y})$ be a noisy distribution obtained by applying non-uniform symmetric label noise to $p(x,y)$. Assume that $L$ is a generalised forward-correction losses loss and let $\mathcal{H}$ denote the (symmetric) entropy function of its base loss. For any probability estimator $\bm{q}$, we have the following lower bound on its noisy risk,
  \begin{align*}
     R^{\eta}_{L_f}(\bm{q}) \geq \mathbb{E}_{x\sim p(x)}\left[\mathcal{H}\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\right],
 \end{align*}
    where $\eta(x)$ denotes the noise rate at $x$. This inequality is strict and may be obtained by setting $\bm{q}(x) = f^{-1}(\widetilde{\bm{p}}(y\mid x))$, if $\widetilde{\bm{p}}(y\mid x)\in f(\Delta)$. 
\end{lemma}
\begin{proof}
     The right-hand side of Inequality~\ref{Equation:general_entropy_bound} can be written
     \begin{align*}
         \mathbb{E}_{x\sim p(x)}\left[\mathcal{H}(\widetilde{\bm{p}}(\tilde{y}\vert x))\right] &= \mathbb{E}_{x\sim p(x)}\left[\sum_{i=1}^c \widetilde{p}(\widetilde{y}=i\vert x)L(\widetilde{\bm{p}}(\widetilde{y}\vert x), i)\right]\\
          &= \mathbb{E}_{x\sim p(x)}\left[T(x)\bm{p}(y\vert x)\cdot \bm{L}(T(x)\bm{p}(y\vert x))\right].
     \end{align*}
Our separability assumption means that $\bm{p}(y\vert x) = \bm{e}_k$ for some $k$. For each $x$ it follows that $T(x)\bm{p}(y\mid x)$ is some rearrangement of the vector $(1-\eta(x), \frac{\eta(x)}{c-1}, \frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1})$. By the assumption that the entropy function is symmetric we may conclude that 
\begin{align*}
    \mathbb{E}_{x\sim p(x)}\left[\mathcal{H}(\widetilde{\bm{p}}(\tilde{y}\vert x))\right] =  \mathbb{E}_{x\sim p(x)}\left[\mathcal{H}\left(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1}\right) \right].
\end{align*}
     %&= \frac{1}{c}\sum_{k=1}^c (T\bm{e}_k)\cdot \bm{L}(T\bm{e}_k),
\end{proof}


\subsubsection{The General Case}
%  \begin{lemma}
%       Let $p(x,y)$ be a distribution where $p(y\vert x)$ is separable, and let $\Tilde{p}(x,\Tilde{y})$ be a noisy distribution obtained by applying label noise to $p(x,y)$. Assume that $L$ is a symmetric (strictly) generalised forward-correction losses loss and let $\mathcal{H}$ denote the entropy function of its base loss. For any probability estimator $q$, we can derive a bound for the noisy risk when the label noise is symmetric that is achieved (uniquely) if $q(x) = \tilde{p}(\tilde{y}\vert x)$. Specifically, for each $x$, we have the following lower bound, where $\eta(x)$ denotes the noise rate at $x$:
%   \begin{align*}
%      R^{\eta}_{L}(q) \geq \mathbb{E}_{x\sim p(x)}[\mathcal{H}\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)]
%  \end{align*}
% \end{lemma}
%   \begin{proof}
%   Our proof follows similar lines to Lemma 5.1 with the additional application of Theorem 5.3.
  
%       Recollect that since $L$ is a (strictly) $f-$proper loss, this implies that there exists a (strictly) proper loss $\tilde{L}$ such that, for all $\bm{q}$, $L(f(\bm{q}), i) = \tilde{L}(\bm{q}, i)$. Now, let $x$ be some arbitrary point in the support of $p(x)$ and let $\bm{q}(x)$ be some probability estimator. The pointwise noisy risk of $\bm{q}$ at $x$ may be written:
%       \begin{align}
%           R^{\eta}_{L}(\bm{q})(x) := & \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)L(\bm{q}(x), i) \\
%           = & \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\tilde{L}(f^{-1}(\bm{q}(x)), i) \\
%           \geq & \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)\tilde{L}(\tilde{p}(\tilde{y}\vert x), i) \label{Equation:inequality2}\\ = &  \sum_{i=1}^c \tilde{p}(\tilde{y}=i\vert x)L(f(\tilde{p}(\tilde{y}\vert x)), i) \label{Equation:equality2}
%           =: R^{\eta}_{L}(f(\Tilde{p}(\tilde{y}\vert x)))
%       \end{align}
%      The inequality (Equation~\ref{Equation:inequality2}) follows from the definition of the properness of $\tilde{L}$ while the following equality (Equation~\ref{Equation:equality2}) follows from the definition of $f-$properness. Note that Equation~\ref{Equation:equality2} is the definition of the entropy of $\Tilde{p}(\Tilde{y}\vert x)$ with respect to the entropy function $\mathcal{H}$ associated with $\Tilde{L}$. Thus for all $x\in \supp(p(x))$, we have $R^{\eta}_{L}(\bm{q})(x) \geq \mathcal{H}(\Tilde{p}(\Tilde{y}\vert x))$. As in the proof of Lemma 5.1, note that when $L$ is (\textit{strictly}) $f-$proper, this is attained (uniquely) by setting $\bm{q}(x) = f(\tilde{p}(\tilde{y}=i\vert x))$. It remains to show that $ \mathcal{H}(\Tilde{p}(\Tilde{y}\vert x)) = \mathcal{H}(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1})$ when the noise is symmetric. This follows from the determinism assumption.
  
% Suppose that, for all $x\in \supp (p(x))$ that the label noise is symmetric with rate $\eta(x)$. Let $T(x)$ denote the noising transition matrix at $x$, that is $T_{ij}:= p(\tilde{y}=j\vert y=i, x)$. By the separable assumption, we have some $k$ such that $p(y=k \vert x)=1$ and $p(y=i \vert x)=0$ otherwise. Thus $\Tilde{p}(\Tilde{y}\vert x) = \sum_{y=1}^c\Tilde{p}(\tilde{y}\vert y,x)p(y \vert x) = \Tilde{p}(\tilde{y}\vert y=k,x) = (T_{1k}, T_{2k}, \ldots, T_{ck})$. The noise rate is defined to be the probability that the label is altered by our noise and may thus be expressed as $\eta(x) = 1-T_{kk}$. By the definition of symmetric noise (Definition 3.5), we have $T_{ik} = \frac{\eta(x)}{1-c}$ for $i \neq k$. Hence $ \mathcal{H}(\Tilde{p}(\Tilde{y}\vert x)) = \mathcal{H}(T_{1k}, T_{2k}, \ldots, T_{ck}) = \mathcal{H}(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1})$ as desired.
%   \end{proof}


% \begin{corollary}
%     When the label noise is uniform and symmetric with rate $\eta$, the following bound on the risk of any probability estimator is tight and achieved only when $q(x) = \tilde{p}(\tilde{y} \mid x)$.
%       \begin{align}
%      R^{\eta}_{L}(q) \geq \mathcal{H}\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big)
%  \end{align}
% \end{corollary}
% \begin{proof}
%     Uniform label noise means that for all $x$, $\eta(x) = \eta$. Thus from Lemma 5.5 we have  $R^{\eta}_{L}(q) \geq \mathbb{E}_{x\sim p(x)}[\mathcal{H}\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)] = \mathbb{E}_{x\sim p(x)}[\mathcal{H}\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big)] = \mathcal{H}\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big)$. This bound is obtained by setting $\bm{q}(x) = \Tilde{p}(\Tilde{y}\vert x)$ for all $x\in \supp(p(x))$. This is unique if $L$ is \textit{strictly} $f-$proper.  
% \end{proof}


% \begin{corollary}
%     Let $L$ be a symmetric (strictly) generalised forward-correction losses loss and let $\bm{u}_{\text{sym}}(\eta, c) := (1-\eta, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$. We have the following bound on the noisy risk if the label noise is symmetric and uniform, which is obtained (uniquely) by setting $\bm{q}(x) = \Tilde{p}(\Tilde{y}\vert x)$ for all $x\in \supp(p(x))$.
% \begin{align*}
%            R^{\eta}_{L}(q) \geq  (1-\eta)L(f(\bm{u}_{\text{sym}}(\eta,c)), 1) + \eta L(f(\bm{u}_{\text{sym}}(\eta,c)), i\neq 1)
% \end{align*}
% \end{corollary}
% \begin{proof}
%     Lemma 5.1 states that, for all $x\in supp(p(x))$ we have,  $R^{\eta}_{L}(\bm{q})(x) \geq R^{\eta}_{L}(f(\Tilde{p}(\tilde{y}\vert x)))$. Since $p(y\vert x)$ is separable it follows we have some $k$ such that $p(y=k \vert x)=1$ and $p(y=i \vert x)=0$ otherwise. Since $L$ is symmetric, we may, without loss of generality, let $k=1$. Thus $\Tilde{p}(\Tilde{y}\vert x) = \sum_{y=1}^c\Tilde{p}(\tilde{y}\vert y,x)p(y \vert x) = \Tilde{p}(\tilde{y}\vert y=k,x) = (1-\eta, \frac{\eta}{c-1}, \frac{\eta}{c-1}, \ldots ,  \frac{\eta}{c-1}) =: \bm{u}_{\text{sym}}(\eta, c)$. The second-to-last equality follows from our assumption that our noise is symmetric and uniform. Putting these together we have, for all $x\in supp(p(x))$ we have,  $R^{\eta}_{L}(\bm{q})(x) \geq R^{\eta}_{L}(f(\bm{u}_{\text{sym}}(\eta, c))) := (1-\eta)L(f(\bm{u}_{\text{sym}}(\eta, c)), 1) + \eta L(f(\bm{u}_{\text{sym}}(\eta, c)), i\neq 1)$. Our result follows by taking expectations with respect to $p(x)$ on both sides. By Lemma 5.1, equality is obtained by setting  $\bm{q}(x) = \Tilde{p}(\Tilde{y}\vert x)$ for all $x\in \supp(p(x))$. This is unique if $L$ is \textit{strictly} $f-$proper. 

% \end{proof}

 \begin{lemma}
       Lemma~\ref{lemma:general} establishes that we can lower bound the noisy risk of an estimator by the average entropy of the noisy class posteriors
  \begin{align*}
     R^{\eta}_{L_f}(\bm{q}) \geq \mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{p}(\widetilde{y}\mid x)].
 \end{align*}
 Given that the noise rate at $x$ is $\eta(x)$, this Lemma establishes that $\mathcal{H}(\widetilde{p}(\widetilde{y}\mid x)$ must lie in the following interval:
 \begin{align*}
          \mathcal{H}(\widetilde{p}(\widetilde{y}\mid x) \in \big[\mathcal{H}(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ), \mathcal{H}\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\big].
 \end{align*}
 In particular, given a fixed noise rate $\eta(x)$ at $x$, the highest possible entropy occurs when label noise is symmetric at $x$. The lowest entropy occurs when the label noise is pairwise.
  \end{lemma}
  \begin{proof}
Let $\bm{q}(x)$ be a probability estimator and let $x$ be some point in the support of $p(x)$. We established in the proof of Lemma~\ref{lemma:general} that $R^{\eta}_{L}(\bm{q})(x) \geq \mathcal{H}(\widetilde{\bm{p}}(\widetilde{y}\vert x))$. We have equality (uniquely) when $\bm{q}(x) = \widetilde{\bm{p}}(\widetilde{y}\vert x)$. Let $T(x)$ denote the noising transition matrix at $x$. By the separability assumption, we have some $k$ such that $p(y=k \vert x)=1$ and $p(y=i \vert x)=0$ otherwise. Thus $\Tilde{p}(\Tilde{y}\vert x) = \sum_{y=1}^c\Tilde{p}(\tilde{y}\vert y,x)p(y \vert x) = \Tilde{p}(\tilde{y}\vert y=k,x) = (T_{1k}(x), T_{2k}(x), \ldots, T_{ck}(x))$. Let $A(\eta(x), c) := \mathcal{H}(T_{1k}(x), T_{2k}(x), \ldots, T_{ck}(x))$ where $\eta(x):= 1- T_{kk}$ is the noise rate at $x$. The symmetry of $\mathcal{H}$ means that, without loss of generality, we may let $k=1$. It remains to show that $A(\eta(x), c) \in \big[\mathcal{H}(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ), \mathcal{H}\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\big]$.  
      
\textbf{Upper Limit:} We begin by demonstrating that $A(\eta(x), c)$ is upper bounded by $\mathcal{H}(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1})$. Let $\Delta(\eta(x))$ denote the set of non-negative vectors $(a_1, a_2, \ldots, a_{c-1})$ such that $a_i \leq 1$ and $\sum_{i=1}^{c-1}a_i = \eta(x)$. We wish to show the supremum of $\mathcal{H}(1-\eta(x), a_1, a_2, \ldots, a_{c-1})$ is attained on $\Delta(\eta(x))$ by setting $a_i = \frac{\eta(x)}{c-1}$ for all $i$. This corresponds to the label noise being symmetric at $x$. By Theorem~\ref{thm:savages} $\mathcal{H}$ is a (strictly) concave function. Moreover, the symmetry assumption implies that $\mathcal{H}$ is a symmetric function of its variables. Define the function $g(a_1, a_2, \ldots, a_{c-1}) := \mathcal{H}(1-\eta(x), a_1, a_2, \ldots,  a_{c-1})$. We wish to show that $g$ attains its maximum on $\Delta(\eta(x))$ when $a_i=a_j$ for all $i,j$. We begin by noting that the (strict) concavity of $\mathcal{H}$ implies the (strict) concavity of $g$. To see this consider two arbitrary vectors $\bm{x}=(x_1, x_2, \ldots x_{c-1}), \bm{y}=(y_1, y_2,\ldots y_{c-1})$. Now $g(\lambda \bm{x} + (1-\lambda) \bm{y}) = \mathcal{H}(\lambda \bm{x}^{\prime} + (1-\lambda) \bm{y}^{\prime})$ where $\bm{x}^{\prime} := (1-\eta(x), x_1, x_2, \ldots ,x_{c-1})$ and $\bm{y}^{\prime} := (1-\eta(x), y_1, y_2, \ldots ,y_{c-1})$. Thus the concavity of $\mathcal{H}$ implies  $g(\lambda \bm{x} + (1-\lambda) \bm{y}) := \mathcal{H}(\lambda \bm{x}^{\prime} + (1-\lambda) \bm{y}^{\prime})  \geq \lambda \mathcal{H}(\bm{x}^{\prime}) + (1-\lambda)\mathcal{H}(\bm{y}^{\prime}) = \lambda g(\bm{x}) + (1-\lambda)g(\bm{y}) $ as desired. Thus $g$ is a symmetric (strictly) concave function of its variables. 

      Let $\bm{a}^*$ denote a maxima of $g$ on $\Delta(\eta(x))$. Let $\sigma$ denote the cyclic permutation of the components of $\bm{a}$. That is $\sigma(a_1, a_2, \ldots, a_{c-1}) := (a_{c-1}, a_1, a_2, \ldots, a_{c-2})$. By the symmetry of $g$, we know that if $\bm{a}^*$ is a maxima then so is $\sigma^i(\bm{a}^*)$ for all $i$: $g(\bm{a}^*) = g(\sigma^i(\bm{a}^*))$ for all $i\in\mathbb{N}$. The defining property of a concave function is that
      \begin{align*}
          g(\lambda_1 \bm{v}_1 + \lambda_2 \bm{v}_2 + \ldots + \lambda_d, \bm{v}_d) &\geq \sum_{i=1}^d \lambda_i g(\bm{x}_i)\\
          \text{where } \sum_i \lambda_i &= 1.
      \end{align*}
      Hence by the (strict) concavity of $g$, setting $\lambda_i \coloneq \frac{1}{c-1}$;
      \begin{align*}
         g\left(\frac{\eta(x)}{c-1}, \frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1}\right) &= g\left(\frac{1}{c-1}(\bm{a}^* + \sigma(\bm{a}^*) + \sigma^2(\bm{a}^*) + \ldots + \sigma^{c-2}(\bm{a}^*))\right) \\
         &\geq \frac{1}{c-1}g\left(\bm{a}^*\right) + \frac{1}{c-1}g\left(\sigma(\bm{a}^*)\right) + \ldots + \frac{1}{c-1}g\left(\sigma^{c-2}(\bm{a}^*)\right) \\
         &= g(\bm{a}^*)
      \end{align*}
      Hence $g$ is maximised by setting $a_i = \frac{\eta(x)}{c-1}$ for all $i$ as desired. This is the unique maxima when the base loss strictly proper.

      \textbf{Lower Limit:} It now remains to show that the lower bound on $A(\eta(x),c)$ holds. The (strict) concavity means that $g$ attains it minima on the vertices of $\Delta(\eta(x))$ (eg $(\eta(x),0,\ldots, 0)$. To see this let $\bm{a}^* = (a^*_1, a^*_2, \ldots, a^*_{c-1})$ denote a minima of $g$ on $\Delta(\eta(x))$. Then we have, 
      \begin{align}
          g(a^*_1, a^*_2, \ldots, a^*_{c-1}) &= g(a^*_1 \bm{e_1} + a^*_2 \bm{e_2} + \ldots + a^*_{c-1} \bm{e}_{c-1}) \notag \\
          &\geq \sum_{i=1}^{c-1}\frac{a^*_i}{\eta(x)} g(\eta(x)\bm{e_i})\notag \\
          &= g(\eta(x), 0, \ldots, 0)\label{Equation:gsym}\\ &= \mathcal{H}(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ) \notag
      \end{align}
      $\bm{e}_i$ denotes the coordinate vector with 1 in the $i$th position and zeros elsewhere. Equation~\ref{Equation:gsym} holds by the symmetry of $g$ and since $\sum a_i^* = \eta(x)$. Thus we have shown that $g$ is lower bounded by $\mathcal{H}(1-\eta(x), \eta(x), 0, 0, \ldots, 0 )$ as desired. Moreover, this infimum is obtained on the vertices of $\Delta(\eta(x))$.
  \end{proof}

    \begin{corollary}
       Given an average noise rate $\eta\coloneq \mathbb{E}_{x\sim p(x)}[\eta(x)]$, the greatest possible value of $\mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{p}(\widetilde{y}\mid x)]$ occurs when $\eta(x)$ is constant:
       \begin{align*}
           \sup_{p(\widetilde{y}\mid x,y)}\left( \mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{p}(\widetilde{y}\mid x))]\right) = \mathcal{H}\left(1-\eta, \frac{\eta}{c-1},  \frac{\eta}{c-1}, \ldots,  \frac{\eta}{c-1}\right),
       \end{align*}
    where the supremum is taken over all noise models such that $\mathbb{E}_{x\sim p(x)}[\eta(x)] = \eta$.
  \end{corollary}
  \begin{proof}
      We established in the proof of Lemma~\ref{lemma:entropy_interval} that, given that the noise rate at $x$ is $\eta(x)$,
       \begin{align*}
          \mathcal{H}(\widetilde{p}(\widetilde{y}\mid x) \in \big[\mathcal{H}(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ), \mathcal{H}\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\big].
 \end{align*}
 Thus, given a fixed average noise rate $\eta$ we maximise the expected entropy when the noise model describes symmetric label noise at each point in dataspace. We now wish to demonstrate that
 \begin{align*}
     \mathbb{E}_{x\sim p(x)}\left[\mathcal{H}\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\right] \leq \mathcal{H}\left(1-\eta, \frac{\eta}{c-1},  \frac{\eta}{c-1}, \ldots,  \frac{\eta}{c-1}\right),
 \end{align*}
 which is to say that we maximise the entropy of symmetric noise by setting $\eta(x)=const$. $\mathcal{H}$ is concave (strictly concave if the base loss is strictly proper) thus we can use Jensen's Inequality which tells us that
 \begin{align*}
     f(\mathbb{E}[X]) \geq \mathbb{E}[f(X)],
 \end{align*}
 if $f$ is concave. Hence, by setting our random variable $X\coloneq (1-\eta(x), \eta(x)/(c-1), \ldots, \eta(x)/(c-1))$, and $f=\mathcal{H}$ yields the desired result.
  \end{proof}


\section{Additional Theory and Discussion }\label{sec:additional_theory}

\subsection{Example: Forward-Corrected CE}\label{ch6:example:fce}
This section gives a breakdown of the example from Table~\ref{ch6:tab:bound_comparison}. We demonstrate how a neural network classifier can achieve a training loss lower than any model that has not accessed the training labels, indicating that it must have overfit.

Consider a separable distribution \( p(x, y) \) corrupted by uniform, symmetric label noise at a rate of 40\%. Assume this noise model is known and corrected for in the loss calculation. We utilise a cross-entropy loss, hence the forward-corrected loss is defined:
\begin{align*}
    L_F(q, 1) &= -\log(0.6q + 0.4(1 - q)) = -\log(0.4 + 0.2q),\\
    L_F(q, 0) &= -\log(0.4q + 0.6(1 - q)) = -\log(0.6 - 0.2q).
\end{align*}

We generate a large noisy dataset from \( p(x, y) \), denoted \( \widetilde{D} = \{(x_i, \widetilde{y}_i)\}_{i=1}^N \). Our objective is to construct a model that attains minimal loss on \( \widetilde{D} \).

\paragraph*{Scenario One: No Label Peeking}
Without peeking at the dataset labels, we minimise the expected loss on the noisy dataset by setting our model predictions \( q(x) \) such that, for every \( x \sim p(x) \), \( q(x) \) minimises the noisy expected loss with respect to \( L_F \). For example, consider a point \( x_0 \sim p(x) \) with a clean label \( y_0=1 \), implying \( p(y_0 = 1 \mid x_0) = 1 \). The noisy class posteriors at \( x_0 \) is \( \widetilde{p}(\widetilde{y}_0 \mid x_0) = 0.6 \). The noisy expected loss at \( x_0 \) is:
\begin{align*}
    H_{L_F}(\widetilde{p}, q) &= 0.6 L_F(q, 1) + 0.4 L_F(q, 0)\\
    &= -0.6\log(0.4 + 0.2q) - 0.4\log(0.6 - 0.2q).
\end{align*}
This loss is minimised by setting \( q = 1 \), yielding an expected noisy loss of \( 0.673 \) at \( x_0 \). Similarly, for \( y = 0 \), setting \( q(x) = 0 \) achieves the same loss.

\paragraph*{Scenario Two: Label Peeking}
Conversely, if a model can peek at the noisy dataset labels before forecasting, it minimises the empirical risk by setting \( q(x) = 1 \) if \( \widetilde{y}_0 = 1 \) and \( q(x) = 0 \) if \( \widetilde{y}_0 = 0 \), attaining a noisy risk of:
\begin{align*}
  L_F(q = 1, 1) &= L_F(q = 0, 0) = -\log(0.6) = 0.511.
\end{align*}

Therefore, while an over-parameterised neural network model trained for sufficient epochs may achieve a training loss of \( 0.511 \), the lowest possible training loss for an optimal model without label access is \( 0.673 \).

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Loss Function}} & \multicolumn{2}{c|}{\textbf{Optimal Training Loss}} \\ \cline{2-3} 
\multicolumn{1}{|c|}{}                       & \textbf{Label Peeking} & \textbf{No Peeking} \\ \hline
CE                                           & 0 & 0.673 \\ \hline
FCE                                          & 0.511 & 0.673 \\ \hline
FCE$+$B                                        & 0.673 & 0.673 \\ \hline
\end{tabular}
\caption{Comparison of the minimum loss which may be achieved by a model on a (large) dataset when using different loss function both \emph{allowing} and \emph{not-allowing} peeking at the dataset labels assuming $40\%$ symmetric label noise on a separable binary label dataset. The results highlight a significant overfitting potential in cross-entropy, as indicated by the large gap between the peeking and non-peeking scenarios. While FCE introduces an inherent loss bound, improving robustness, it may still permit overfitting. Our bounded variant, FCE$+$B, is designed to better align with the dataset and mitigate overfitting.}
\end{table}



\subsection{Sensitivity of Bounds}\label{sec:sensitivity_of_bounds}
The noise-bound is equal to the average entropy of the noisy label distribution when label noise is uniform and symmetric. When we deviate from these noise conditions, this bound is too high in that an optimal probability estimator could achieve a (noisy) risk lower than this value without overfitting. Since we use this bound in all noise conditions, it is essential to get an idea of the size of the gap between our bound and the minimum achievable risk. Ideally we want this gap to be small. In this section, we look briefly at this topic, noting that this gap is usually smaller for GCE and SCE than CE. This implies that the noise-bound is more suitably used with SCE and GCE than with CE when noise deviates from idealised assumptions. Given a noise rate $\eta$, the following Lemma gives the worst-case gap between the true average entropy of the noisy distribution and the noise-bound, asusming uniform label noise. 

% When label noise deviates from this condition, our bounds no longer necessarily hold. More generally, we have the following bound:
%  \begin{lemma}\label{lemma:sens}
%       Let $p(x,y)$ be a distribution where $p(y\vert x)$ is separable, and let $\Tilde{p}(x,\Tilde{y})$ be a noisy distribution obtained by applying label noise to $p(x,y)$. Assume that $L$ is a symmetric (strictly) generalised forward-correction losses loss and let $\mathcal{H}$ denote the entropy function of its base loss. For any probability estimator $q$, we may lower bound the generalised noisy risk of $\bm{q}$ in terms of a quantity $A(\eta(x), c)$ where $\eta(x)$ denotes the noise rate at $x$ and $c$ the number of classes. This bound is achieved (uniquely) by $q(x) = \tilde{p}(\tilde{y}\vert x)$. Specifically,
%   \begin{align*}
%      R^{\eta}_{L}(q) \geq \mathbb{E}_{x\sim p(x)}[A(\eta(x), c)]\\
%  \end{align*}
%  Where $A(\eta(x), c)$ lies in the following interval:
%  \begin{align*}
%           A(\eta(x), c) \in \big[\mathcal{H}(1-\eta(x), \eta(x), 0, 0, \ldots, 0 ), \mathcal{H}\Big(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} \Big)\big]
%  \end{align*}
%   \end{lemma}
% The proof is given at the end of Section~\ref{sec:bounding_proofs}.

% Corollary~\ref{cor:worst_case_entropy} states that given a noise rate of $\eta$ the highest entropy occurs when label noise is uniform and symmtruc.
%        \begin{align*}
%            \sup_{p(\widetilde{y}\mid x,y)}\left( \mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{p}(\widetilde{y}\mid x))]\right) = \mathcal{H}\left(1-\eta, \frac{\eta}{c-1},  \frac{\eta}{c-1}, \ldots,  \frac{\eta}{c-1}\right),
%        \end{align*}
       
%   Lemma 5.1 tells us that $A(\eta(x),c)$ attains this upper limit of $\mathcal{H}(1-\eta(x), \frac{\eta(x)}{c-1},\frac{\eta(x)}{c-1}, \ldots, \frac{\eta(x)}{c-1} )$ only if our label noise is symmetric at $x$. Conversely, as indicated in our proof (Section~\ref{sec:bounding_proofs}), the lower limit is obtained when the label noise flips labels to only one other class. 

\begin{corollary}\label{cor:uniform_interval}
        Suppose we have some uniform label noise at noise rate $\eta$. Let $\mathcal{H}$ denote the average entropy of the noisy label distribution, that is 
        \begin{align*}
            \mathcal{H} \coloneq \mathbb{E}_{x\sim p(x)}[\mathcal{H}(\widetilde{\bm{p}}(y\mid x))].
        \end{align*}
        Let $B(\eta,c)$ denote the noise-bound defined in Definition~\ref{def:noise-bound}. Then
        \begin{align*}
            \vert B(\eta,c) - \mathcal{H}\vert \leq  \mathcal{H}\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big) - \mathcal{H}(1-\eta, \eta, 0, 0, \ldots, 0 )
        \end{align*}
\end{corollary}
\begin{proof}
    This follows immediately from Lemma~\ref{lemma:entropy_interval} when $\eta(x)$ has no dependence on $x$ ($\eta(x)=\eta$).
\end{proof}
  As discussed previously, when noise is uniform but not symmetric, our noise-bound (Definition~\ref{def:noise-bound}) of $\mathcal{H}(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$ is too high since the true minimum achievable risk is lower than this bound. In other words, there exists a probability estimator which attains a risk lower than our bound. This non-optimality is the cost we incur as a result of requiring a simple, easily computable bound depending on only on the noise rate. Importantly, Corollary~\ref{cor:uniform_interval} gives us a rough way to quantify this non-optimality, using the difference between the upper and lower entropy limits
    \begin{align}\label{Equation:limits}
       \left[ \mathcal{H}\Big(1-\eta, \frac{\eta}{c-1},\frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1} \Big), \mathcal{H}(1-\eta, \eta, 0, 0, \ldots, 0 )\right]
    \end{align}
  When this difference is large, one can construct two types of label noise with the same rate $\eta$, such that the difference in the minimum achievable risks between these noise types is significant. Conversely, when this gap is small, the minimum achievable risk for any type of label noise at a fixed rate $\eta$ is similar. This is a desirable property and suggests that simply setting our bound to our noise-bound is probably suitable regardless of the specifics of the noising process.

   % Figure environment removed
  
  On the top row of Figure~\ref{fig:sensitivity}, we give a plot of the upper and lower limits of Equation~\ref{Equation:limits} for $\eta\in (0, 0.5]$ for $c=10$ (left) and $c=200$ (right) for GCE, SCE and CE. The upper limit is given by a dotted line, while the lower limit is given by a filled line in the same colour. Each loss is scaled so they may be more easily compared. Similarly, in the row below, we plot the ratios of the upper and lower limits of Equation~\ref{Equation:limits} for each loss. These graphs show that the difference between the upper and lower limits is much greater for CE than for SCE and GCE. This difference is more pronounced when the number of classes is greater. The result is that on non-symmetric noise, our noise-bound (Definition~\ref{def:noise-bound}) will generally be less suitable when used in conjunction with CE than when used with GCE or SCE.

% \subsection{Generalised Correction Losses: A Discussion}
% Recall Definition~\ref{def:gen_corr} below. 

% \begin{definition}[Generalised Correction Losses]\label{def:gen_corr}
%  Let $L_f$ be a loss function and $f:\Delta\rightarrow \Delta$ be an injective function. We say $L_f$ is a `generalised forward-correction loss' if there exists a $L$ (as defined in Section~\ref{ch0:sec:proper_scoring_list}) such that for all $\bm{q}\in \Delta$,
%  \begin{align*}
%      L_f(\bm{q}, k) = L(f(\bm{q}), k)
%  \end{align*}
%  We refer to $L$ as the \textbf{base loss}. $f$ can be thought of as a label noise model. 
% \end{definition}

% It is instructive to consider how broad this family of losses is since the results in Section 5 hold for all losses which satisfy this definition. We mentioned in Section 4 that this definition trivially contains all proper losses by letting $f=id.$. Below we give a sufficient condition for a loss to be $f-$proper.


% \begin{proposition}
%     Let $L$ be some elementwise loss function. Let $H_{L}:\Delta\times \Delta\rightarrow \mathbb{R}$ denote its expected loss function $H_{L}(\bm{q}, \bm{p}) := \sum_{i=1}^c p_iL(\bm{q}, i)$. Define $g(\bm{p}):= \argmin_{\bm{q}}H_{L}(\bm{q}, \bm{p})$, if $g$ is surjective then $L$ is $f-$proper. 
% \end{proposition}
% \begin{proof}
%  Let $L$ be some elementwise loss such that $g$ (as defined above) is surjective for some loss $L$. By the definition of $g$ we have $H_{L}(\bm{q}, \bm{p}) \geq H_{L}(g(\bm{p}), \bm{p}) := \sum_{i=1}^c p_iL(g(\bm{p}), i)$. Now, define the elementwise loss $\tilde{L}(\bm{q}, i):= L(g(\bm{q}), i)$. We claim that $\tilde{L}$ is proper. Let $\bm{p}\in \Delta$ then $H_{\tilde{L}}(\bm{q}, \bm{p}) := \sum_{i=1}^c p_i\tilde{L}(\bm{q}, i) := \sum_{i=1}^c p_iL(g(\bm{q}), i) = H_{L}(g(\bm{q}), \bm{p})$. This is minimised by setting $g(\bm{q}) = g(\bm{p})$ which occurs at $\bm{p}=\bm{q}$. (If $g$ is injective this occurs uniquely when $\bm{p}=\bm{q}$ although this is not required). Thus it follows that $\Tilde{L}$ is proper. Since $g$ is surjective then one may define an injective inverse function $f:= g^{-1}$ on $\Delta$.
%  Thus we have $L(\bm{q}, i) = \tilde{L}(f(\bm{q}), i)$ with $\Tilde{L}$ proper and $f$ injective as desired. Hence $L$ is $f-$proper. 
% \end{proof}

%WHY IS MAE NOT? The work below is BS
%An example of a loss which is not $f-$proper is MAE. Suppose conversely that there exist $f$ injective and $L_{pr.}$ proper so that $L_{MAE}(f(\bm{q}), i) = L_{pr.}(\bm{q}, i)$.  The expected loss for MAE is maximised by setting $\bm{q}=\bm{e}_k$ where $k:=\argmax_{i}p_i$. Hence 


\subsection{Noise Model Plots}
In Lemma~\ref{lemma:semiproper_gce} we showed that the SCE, GCE and FCE losses are generalised forward-correction losses and derived the corresponding functions $f$. (In fact we derived $f^{-1}$ as this turned out to be easier.) As discussed, these functions can be interpreted as noise models; $f(\bm{p}(y\vert x)) \approx \widetilde{\bm{p}}(\widetilde{y}\vert x)$. In section we provide some plots of these noise models. 

\paragraph*{Properness} While Definition~\ref{def:gen_corr} does not require the so-called base loss to be proper, Lemma~\ref{lemma:semiproper_gce} shows that GCE and SCE can be obtained by applying a non-linear correction to a proper loss. The defining characteristic of a proper loss is that the expected loss is minimised by setting $\bm{p}=\bm{q}$. Therefore,
\begin{align*}
    H_{L_f}(\bm{\tilde{p}},\bm{q}) \coloneq \bm{\tilde{p}}\cdot L_f (\bm{q}) = \bm{\tilde{p}}\cdot L(f(\bm{q})).
\end{align*}
 is minimised by setting $\bm{\tilde{p}} = f(\bm{q}) \iff \bm{q} = f^{-1}(\bm{\tilde{p}})$. We make this point because plotting $f^{-1}$ as a function of $\bm{p}$ (which we do below) is the same as plotting $\argmin_q H(\bm{p},\bm{q})$ - this allows us to include the MAE loss on this plot even though it isn't a generalised forward-correction loss. 
 
 % $f^{-1}$ therefore is the denoising model. Given a noisy class posterior $\bm{\widetilde{p}}\in\Delta$, $f^{-1}(\bm{\widetilde{p}})$ gives the clean class posterior. 

 %This loss function is not a generalised-correction loss however, it's useful as a reference. We see that the expected loss is minimised by letting $q=0$ if $p<0.5$ and $q=1$ otherwise.

\paragraph*{Plots} In Figure~\ref{fig:semiProper}, we present plots of $f^{-1}$ for the SCE, GCE and FCE loss functions in the binary setting.  The $x-$axis gives probability of a noisy label being equal to one $\widetilde{p}(\widetilde{y}=1\mid x)$. On the $y-$axis we plot $p(y=1\mid x)$ where $\bm{p} = f^{-1}(\widetilde{\bm{p}})$. For proper losses, $f=id$, reflecting the fact that they contain encode no noise model. The graphs for GCE and SCE are remarkably similar. Their graphs portray a noise model where labels noise occurs more frequently at points where $\bm{p}$ contains higher intrinsic uncertainty. Conversely no label noise occurs at anchor points at all. FCE requires a noise model in order to be fully specified; we assume symmetric label noise at $\eta=0.4$. Varying $\eta$ will change the steepness of the respective $f^{-1}$. Finally, we plot MAE The graphs of SCE and GCE lie between those of MAE and CE. By varying the parameters of these losses, we can interpolate between them.
% Figure environment removed



\subsection{Explicit Bounds}\label{sec:explicit_bounds}
Using Lemma~\ref{lemma:semiproper_gce} we can produce the noise-bounds (Definition~\ref{def:noise-bound}) for GCE and SCE. The bound for GCE is given below.
\begin{align*}
    B_{GCE}(\eta,c) :=\frac{(1-\eta)}{a}\left(1-\left(\frac{(1-\eta)^{\frac{1}{1-a}}}{(1-\eta)^{\frac{1}{1-a}} + (c-1)(\frac{\eta}{c-1})^{\frac{1}{1-a}}}\right)^a\right) +\\ \frac{\eta}{a}\left(1-\left(\frac{\frac{\eta}{c-1}^{\frac{1}{1-a}}}{(1-\eta)^{\frac{1}{1-a}} + (c-1)(\frac{\eta}{c-1})^{\frac{1}{1-a}}}\right)^a\right).
\end{align*}
The noise-bound for SCE is
\begin{align*}
    B_{SCE}(\eta,c) := (1-\eta)\left( -log\left(\frac{1-\eta}{\lambda - A(1-\eta)}\right) + A \left(1-  \frac{1-\eta}{\lambda - A(1-\eta)}\right)\right) \\+ \eta \left(-log\left(\frac{\eta}{\lambda(c-1) - A\eta}\right) + A \left(1-  \frac{\eta}{\lambda(c-1) - A\eta}\right) \right).
\end{align*}
Recollect that $\lambda$ is chosen so that the resulting distribution normalises: $\frac{1-\eta}{\lambda - A(1-\eta)} + \frac{\eta(c-1)}{\lambda(c-1) - A\eta} = 1$ and may be computed numerically or by solving the resulting quadratic.

\section{Further Experiments}\label{sec:further_experiments}

\subsection{Experiment Details}
The number of training epochs was the same for each loss. For MNIST, FashionMNIST, TinyImageNet and Animals10N, we used 100 epochs; for all other datasets, we used 120 epochs. Each experiment in Tables 1 and 2 (Section 6) was run three times, and the mean and unbiased estimate of the standard deviation is given. We used a ResNet18 architecture for all experiments except TinyImageNet and Animals10N, where a ResNet34 was used. Each experiment is carried out on a single GeForce GTX Titan X. We used a batch size of 300 in all experiments except TinyImageNet and Animals10N, where this is reduced to 200. A learning rate of 0.0001 was used for all losses except MAE (lr = 0.001) and ELR where we used their recommended learning rate of 0.01. We use a learning rate scheduler which scales our learning rate by 0.6 at epoch 60. Our implementation of the \textit{Truncated Loss} comes from the official github implementation of GCE. Likewise, we use the official codebase for our implementation of ELR. Other losses are re-implementations based on details given in the respective papers. Our SCE loss used the recommended hyperparameter of $A=8$. Our GCE loss used $a=0.4$. FCE requires one to define a noise model. In each case, we assume noise is symmetric at the relevant rate. For Animals10N, this rate is set to 11\%, which is the estimated noise rate.

\subsubsection{CE with Prior}
One of the losses used in our experiments is cross-entropy with a `prior' term (CEP). We give an explanation of the motivation for this additional loss term and details of how it's implemented. 

 In Section~\ref{sec:estimating_entropy} we assumed that the un-noised distribution $p(x,y)$ is separable (i.e. for each $x$, $p(y=k\mid x) = 1, p(i\neq k\mid x)=0)$ for some $k\in\mathcal{Y}$. Thus, in the case of symmetric noise with a known noise rate $\eta$, the noisy label distribution $\widetilde{\bm{p}}(\widetilde{y}\vert x)$ is of the form for each $x$:
 \begin{align}\label{Equation:prior_dist}
     \widetilde{\bm{p}}(\widetilde{y}\mid x) = \left(\frac{\eta}{c-1}, \frac{\eta}{c-1}, \ldots, \underbrace{1-\eta}_{\text{k\textsuperscript{th} position}}, \ldots, \frac{\eta}{c-1}\right)
 \end{align}
We argue, therefore, that it is reasonable to introduce a term to penalise our model when its outputs deviate from this distribution. This is achieved through a regularisation term which measures the KL-divergence between our model probabilities and the desired distribution (Equation~\ref{Equation:prior_dist}). Let $\bm{p}_{\eta} := (p_1, p_2, \ldots, p_c) := (1-\eta, \frac{\eta}{c-1}, \ldots, \frac{\eta}{c-1})$ and let $q_1,q_2, \ldots, q_c$ denote the probabilities output by our model. We sort the $q_i$ into descending order (which we denote as $q_{\sigma(i)}$) and define our prior term as:
\begin{align}
    L_{prior}(\bm{q}, \bm{p}_{\eta}) := -\sum_{i=1}^c p_{i}\log(q_{\sigma(i)}) \label{Equation:prior}
\end{align}
 Thus, overall we have $L_{CEP}(\bm{q}, i) := L_{CE}(\bm{q}, i) + L_{prior}(\bm{q}, \bm{p}_{\eta})$. Tables 1 and 2 in Section 6 show that this additional term generally results in additional improvement over using the noise-bound alone. This prior acts as a method of feasible set reduction: There are many different probability estimators which achieve a training error equal to our noise-bound. Therefore, by introducing a prior term (Equation~\ref{Equation:prior}) we can further restrict the set of admissible models.

\subsection{Varying The Bound}\label{sec:vary_bounds}
 We explore treating the bound `$B$' as a hyperparameter to assess the proximity of the noise-bound to optimality. This consists of doing a grid search in a small vicinity of the noise-bound for each loss function and recording how this impacts clean test performance. Tables~\ref{tab:full_table1},~\ref{tab:full_table2} include the result of these experiments, indicated with a star (e.g. CE$+$B*) together with the results of the other loss functions.  When varying the bound from the noise-bound doesn't yield an improvement, the starred and unstarred accuracy values are the same. In slightly over half of our experiments, we find that we may achieve an improvement by perturbing the bound. This improvement is generally minor. Our assumption that the underlying clean dataset is separable means one should be able to improve performance by raising the bound to account for the additional randomness in the label distributions. Generally, we find this to be so. An exception to this pattern are the non-uniform and asymmetric datasets. In these cases, one typically benefits from marginally \textit{lowering} the bound. This observation is consistent with our expectation; the noise-bound is a `worst-case' entropy, attained only by uniform symmetric label noise. For other noise models the noise-bound will be higher than strictly necessary to prevent overfitting and may benefit from being slightly decreased. The values of the optimal bounds may be found in a table in Appendix~\ref{sec:further_experiments}. 


\begin{table}[h!]
\begin{adjustbox}{max width=1.00\textwidth}
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|c}
 & \multicolumn{2}{c}{MNIST} & \multicolumn{2}{c}{Fashion} & \multicolumn{2}{c}{EMNIST} & \multicolumn{2}{c}{CIFAR10} & \multicolumn{2}{c}{CIFAR100} & \multicolumn{2}{c}{ACIFAR100} & NU-EMNIST \\
 & 0.4 & 0.6 & 0.2 & 0.4 & 0.2 & 0.4 & 0.2 & 0.4 & 0.2 & 0.4 & 0.2 & 0.4 & 0.6 \\
FCE & \cellcolor[HTML]{FFCCC9}-0.05 & \cellcolor[HTML]{FFCCC9}-0.05 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{FFCCC9}-0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.1 & \cellcolor[HTML]{CBCEFB}0.03 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{FFCCC9}-0.1 & \cellcolor[HTML]{FFCCC9}-0.35 & \cellcolor[HTML]{FFCCC9}-0.2 \\
GCE & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.03 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{EFEFEF}0.0 \\
SCE & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{CBCEFB}0.1 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 \\
CEB & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.05 & \cellcolor[HTML]{CBCEFB}0.1 & \cellcolor[HTML]{CBCEFB}0.1 & \cellcolor[HTML]{CBCEFB}0.2 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{FFCCC9}-0.6 \\
CEP & \cellcolor[HTML]{FFCCC9}-0.15 & \cellcolor[HTML]{FFCCC9}-0.15 & \cellcolor[HTML]{FFCCC9}-0.15 & \cellcolor[HTML]{FFCCC9}-0.15 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{CBCEFB}0.02 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{EFEFEF}0.0 & \cellcolor[HTML]{FFCCC9}-0.08 & \cellcolor[HTML]{FFCCC9}-0.08 & \cellcolor[HTML]{FFCCC9}-0.08 & \cellcolor[HTML]{FFCCC9}-0.08 & \cellcolor[HTML]{FFCCC9}-0.1
\end{tabular}
\end{adjustbox}
\caption{table giving the offset of the `optimal' bound from the noise-bound. Here a negative (blue) number means that the bound is greater than the noise-bound. Positive (red) means the optimal bound is lower. Grey means that the optimal bound is zero, i.e. no offset.}\label{table:offsets}
\end{table}

\begin{table*}[!htb]
    \centering
    \begin{adjustbox}{max width=1.00\textwidth}
    \begin{tabular}{l|cc|cc|cc|cc|cc}
    \textbf{} & \multicolumn{2}{c|}{MNIST} & \multicolumn{2}{c|}{FashionMNIST} & \multicolumn{4}{c|}{EMNIST} & \multicolumn{2}{c}{CIFAR10} \\
\multirow{2}{*}{Losses} & \multicolumn{1}{c}{\multirow{2}{*}{0.4}} & \multicolumn{1}{c|}{\multirow{2}{*}{0.6}} & \multicolumn{1}{c}{\multirow{2}{*}{0.2}} & \multicolumn{1}{c|}{\multirow{2}{*}{0.4}} & \multicolumn{2}{c}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{1}{c}{\multirow{2}{*}{0.2}} & \multicolumn{1}{c}{\multirow{2}{*}{0.4}} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c|}{Top 5} & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c|}{Top 5} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{}  \\ \hline
MSE  & $93.3_{\pm 0.47 }$& $85.8_{\pm 0.95 }$& $84.8_{\pm 0.22} $& $80.6_{\pm 0.84 }$& $82.9_{\pm 0.29}$ & $98.1_{\pm 0.04}$ & $80.2_{\pm 0.19}$ & $97.1_{\pm 0.07}$ & $78.7_{\pm 1.51}$ &   $56.4_{\pm 0.11}$ \\
MAE & $97.9_{\pm 0.08} $& $96.4_{\pm 0.08} $& $83.2_{\pm 0.10} $& $82.2_{\pm 0.37} $& $49.8_{\pm 2.83}$ & $52.2_{\pm 0.10}$ & $50.4_{\pm 1.14}$ & $51.4_{\pm 0.96}$ & $88.6_{\pm 1.34}$ &   $78.9_{\pm 5.95}$\\
NCE & $97.8_{\pm 0.06} $ & $96.0_{\pm 0.25} $ & $87.7_{\pm 0.26} $ & $\bm{86.3}_{\pm 0.14} $ & $84.5_{\pm 0.25}$ & $97.9_{\pm 0.05}$ & $82.6_{\pm 0.81}$ & $96.7_{\pm 0.03}$ &  $\textcolor{black}{\bm{89.3}_{\pm 0.40}}$ &   $\textcolor{black}{\bm{86.0}_{\pm 0.81}}$ \\
MixUp & $95.8_{\pm 1.24} $& $86.8_{\pm 0.85} $& $86.9_{\pm 0.10} $& $82.3_{\pm 0.54} $& $84.3_{\pm 0.08}$ & $98.1_{\pm 0.04}$ & $81.6_{\pm 0.48}$ & $97.1_{\pm 0.08}$ & $86.0_{\pm 0.46}$ &  $77.9_{\pm 0.49}$ \\
Sph. & $95.0_{\pm 0.41}$ & $88.1_{\pm 0.82}$ & $87.2_{\pm 0.04}$ & $84.1_{\pm 0.75}$ & $84.6_{\pm 0.12}$ & $98.3_{\pm 0.05}$ & $83.2_{\pm 0.29}$ & $98.1_{\pm 0.58}$ & $86.6_{\pm 0.01}$  & $72.1_{\pm 0.80}$\\
Boot. & $86.6_{\pm 0.56} $& $71.2_{\pm 1.17} $& $82.0_{\pm 0.61}$ & $73.4_{\pm 1.06} $ & $80.5_{\pm 0.24}$ & $96.7_{\pm 0.06}$ & $77.3_{\pm 0.98}$ & $95.0_{\pm 0.25}$ & $77.0_{\pm 1.57}$ &  $58.2_{\pm 2.99}$\\
Trunc. & $97.1_{\pm 0.12} $& $94.2_{\pm 0.39} $& $87.8_{\pm 0.29} $& $85.3_{\pm 0.77} $& $84.1_{\pm 0.53}$ & $97.4_{\pm 1.03}$ & $83.1_{\pm 0.55}$ & $97.2_{\pm 1.00}$ & $88.3_{\pm 0.56}$ &   $84.2_{\pm 0.69}$ \\ 
CL  & $82.7_{\pm 0.57}$ & $67.5_{\pm 1.83} $& $81.2_{\pm 0.34}$ & $73.1_{\pm 0.66} $ & $79.6_{\pm 0.17}$ & $96.4_{\pm 0.05}$ & $75.1_{\pm 0.67}$ & $94.2_{\pm 0.24}$ & $76.0_{\pm 2.16}$ &   $59.4_{\pm 4.20}$\\ 
ELR & $98.1_{\pm 0.04}$ & $97.8_{\pm 0.07} $& $85.3_{\pm 0.23}$ & $83.4_{\pm 0.02} $ & $81.8_{\pm 0.26}$ & $97.5_{\pm 0.21}$ & $76.6_{\pm 0.10}$ & $96.5_{\pm 0.11}$ & $88.1_{\pm 0.82}$ &   $85.7_{\pm 0.06}$\\ \hline
FCE. & $95.4_{\pm 0.25} $ & $92.3_{\pm 0.13} $ & $83.6_{\pm 0.11} $ & $79.9_{\pm 0.78} $ & $83.1_{\pm 0.12}$ & $98.4_{\pm 0.20}$ & $80.6_{\pm 0.12}$ & $98.0_{\pm 0.03}$ & $84.7_{\pm 0.40}$ &  $75.1_{\pm 0.04}$ \\
\rowcolor{green}
FCE$+$B & $\boxed{95.7_{\pm{0.18}}}$ & $\boxed{92.7_{\pm{0.74}}}$ & $\boxed{84.8_{\pm{0.26}}}$ & $\boxed{81.7_{\pm{0.27}}}$  & $\boxed{83.4_{\pm{0.09}}}$ & $\boxed{98.5_{\pm{0.03}}}$ & $\boxed{81.6_{\pm{0.51}}}$ & $\boxed{\bm{98.1}_{\pm{0.15}}}$ & $\boxed{86.7_{\pm_{0.21}}}$ & $\boxed{82.2_{\pm_{0.06}}}$\\ 
FCE$+$B* & $96.7_{\pm 0.17}$ & $94.3_{\pm 0.50}$ & $84.8_{\pm{0.26}}$ & $83.3_{\pm 0.22}$ & $84.4_{\pm 0.06}$ & $\bm{98.6}_{\pm{0.13}}$ & $83.1_{\pm 0.42}$ & $98.1_{\pm{0.10}}$ & $87.2_{\pm{0.20}}$ & $82.2_{\pm_{0.06}}$\\\hline
GCE  & $94.4_{\pm 0.36}$ & $83.8_{\pm 1.14} $ &  $86.4_{\pm 0.24}$ & $81.6_{\pm 0.37}$ & $84.3_{\pm 0.13}$ & $98.4_{\pm 0.08}$ & $82.7_{\pm 0.07}$ &   $97.9_{\pm 0.02}$ & $81.1_{\pm 0.72}$ & $60.0_{\pm 1.31} $ \\
\rowcolor{green}
GCE$+$B & $\boxed{96.6_{\pm 0.22}}$ & $\boxed{94.0_{\pm 0.13}} $ & $\boxed{86.5_{\pm 0.56}}$ & $\boxed{85.5_{\pm 0.13}}$ & $84.1_{\pm 0.29}$ & $\boxed{98.4_{\pm 0.04}}$ & $\boxed{82.8_{\pm 0.28}}$ &  $\boxed{98.0_{\pm 0.06}}$ & $\boxed{86.1_{\pm 0.22}}$ & $\boxed{79.0_{\pm 1.17}}$ \\
GCE$+$B* & $96.6_{\pm 0.22}$ & $94.0_{\pm 0.13} $ & $87.0_{\pm 0.04}$ & $85.5_{\pm 0.13}$ & $84.3_{\pm 0.09}$ & $98.4_{\pm 0.06}$ & $83.6_{\pm 0.25}$ &    $\bm{98.2}_{\pm 0.03}$ & $86.7_{\pm 0.07}$ & $80.2_{\pm 0.83}$ \\ \hline
SCE & $89.5_{\pm 5.29} $& $70.2_{\pm 0.69} $& $82.7_{\pm 0.64} $& $74.4_{\pm 0.37} $& $82.1_{\pm 0.33}$ & $96.8_{\pm 0.10}$ & $79.6_{\pm 0.61}$ & $95.4_{\pm 0.15}$ & $78.2_{\pm 0.42}$ &   $59.0_{\pm 4.43}$\\
\rowcolor{green}
SCE$+$B & $\boxed{97.0_{\pm 0.16}}$ & $\boxed{93.4_{\pm 0.29}} $& $\boxed{87.5_{\pm 0.22}}$ & $\boxed{85.2_{\pm 0.98}} $ & $\boxed{83.5_{\pm 0.29}}$ & $\boxed{97.3_{\pm 0.14}}$ & $\boxed{81.8_{\pm 0.52}}$ & $\boxed{96.4_{\pm 0.20}}$ & $\boxed{88.9_{\pm 0.44}}$ &   $\boxed{84.7_{\pm 0.37}}$\\
SCE$+$B* & $97.0_{\pm 0.16}$ & $93.7_{\pm 0.52} $& $87.5_{\pm 0.22}$ & $85.8_{\pm 0.67} $ & $83.6_{\pm 0.03}$ & $97.4_{\pm 0.02}$ & $81.8_{\pm 0.52}$ & $96.5_{\pm 0.26}$ & $88.9_{\pm 0.44}$ &   $84.9_{\pm 0.20}$\\ \hline
CE & $80.8_{\pm 2.31}$ & $67.3_{\pm 0.80} $& $80.9_{\pm 1.11} $& $72.1_{\pm 2.16} $& $79.9_{\pm 0.28}$ & $96.4_{\pm 0.08}$ & $75.6_{\pm 0.20}$ & $94.2_{\pm 0.24}$ & $76.9_{\pm 1.22}$  &   $59.9_{\pm 2.15}$ \\
\rowcolor{green}
CE$+$B & $\boxed{96.2_{\pm 0.32}}$ & $\boxed{93.0_{\pm 0.09}}$& $\boxed{87.9_{\pm 0.10}}$ & $\boxed{84.7_{\pm 0.37}} $ & $\boxed{80.8_{\pm 0.08}}$ & $\boxed{97.0_{\pm 0.04}}$ & $\boxed{78.9_{\pm 0.12}}$ & $\boxed{96.1_{\pm 0.26}}$ & $\boxed{84.5_{\pm 0.73}}$ &   $\boxed{76.0_{\pm 1.13}}$\\ 
CE$+$B* & $96.2_{\pm 0.32}$ & $93.0_{\pm 0.09}$ & $87.9_{\pm 0.10}$ & $84.7_{\pm 0.37} $ & $81.5_{\pm 0.11}$ & $97.3_{\pm 0.02}$ & $79.0_{\pm 0.09}$ &  $96.2_{\pm 0.01}$ & $84.8_{\pm 0.55}$ &   $78.6_{\pm 1.28}$\\ \hline
CEP & $97.5_{\pm 0.08 }$ & $92.1_{\pm 0.44} $ & $87.8_{\pm 0.12 }$ & $84.8_{\pm 0.23}$ & $85.5_{\pm 0.10}$ & $98.1_{\pm 0.07}$ & $84.3_{\pm 0.22}$ & $97.6_{\pm 0.14}$ & $84.2_{\pm 0.51}$ &  $58.2_{\pm 2.94}$\\
\rowcolor{green}
CEP$+$B & $95.6_{\pm 0.32}$ & $85.5_{\pm 0.77} $& $\boxed{88.1_{\pm 0.31}}$ & $84.2_{\pm 0.33} $ &  $\boxed{\bm{85.8}_{\pm 0.12}}$ & $\boxed{98.3_{\pm 0.02}}$ &  $\boxed{\bm{84.8}_{\pm 0.10}}$ & $\boxed{98.0_{\pm 0.04}}$ & $\boxed{88.5_{\pm 0.32}}$ &   $\boxed{85.1_{\pm 0.20}}$\\ 
CEP$+$B* &  $\bm{98.5}_{\pm 0.05}$ &  $\bm{97.9}_{\pm 0.11}$ &  $\bm{88.4}_{\pm 0.04}$ &  $\bm{87.2}_{\pm 0.21} $ & $85.8_{\pm 0.12}$ & $98.3_{\pm 0.02}$ & $84.8_{\pm 0.10}$ & $98.0_{\pm 0.16}$ & $88.5_{\pm 0.32}$ &   $85.1_{\pm 0.20}$\\
\end{tabular}
\end{adjustbox}
    \caption{Test accuracies obtained by using different losses on the noisy MNIST/ FashionMNIST/EMNIST/CIFAR10 datasets. Losses implementing the noise-bound shaded in blue. When using this bound provides benefit, the corresponding value is \boxed{boxed}. Overall top values in \textbf{bold}.}\label{tab:full_table1}
    \centering
\end{table*}

\begin{table*}[!htb]
    \centering
    \begin{adjustbox}{max width=0.98\textwidth}
    \begin{tabular}{l|cccc|cccc|cc}
    \textbf{}  & \multicolumn{4}{c|}{CIFAR100} & \multicolumn{4}{c|}{ASYM-CIFAR100} & \multicolumn{2}{c}{Non-Uniform-EMNIST} \\
\multirow{2}{*}{Losses} & \multicolumn{2}{c|}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{2}{c|}{0.2} & \multicolumn{2}{c|}{0.4} & \multicolumn{2}{c}{0.6} \\
  \multicolumn{1}{c|}{} & Top1 & \multicolumn{1}{c|}{Top5} & Top1 & Top5 & Top1 & \multicolumn{1}{c|}{Top5} & Top1 & Top5 & \multicolumn{1}{c}{Top 1} & \multicolumn{1}{c}{Top 5} \\ \hline
 MSE & $57.2_{\pm 0.93}$ & $78.6_{\pm 0.25}$ & $40.6_{\pm 0.38}$ & $63.0_{\pm 0.24}$ & $56.3_{\pm 0.11}$  & $82.6_{\pm 0.22}$  & $40.7_{\pm 0.12}$ & $74.4_{\pm 0.25}$ & $44.7_{\pm 2.66}$ & $86.7_{\pm 3.10}$ \\
MAE & $10.0_{\pm 0.11}$  & $13.8_{\pm 0.28}$ & $7.6_{\pm 1.89}$  & $11.6_{\pm 1.25}$ & $7.1_{\pm 6.02}$  & $11.1_{\pm 6.6}$  & $11.1_{\pm 5.43}$  & $25.1_{\pm 5.76}$ &  $9.8_{\pm 1.74}$ & $23.1_{\pm 1.80}$\\
NCE  & $38.7_{\pm 3.13}$ & $51.8_{\pm 3.77}$ & $19.1_{\pm 0.20}$ & $28.8_{\pm 0.15}$ & $16.3_{\pm 1.24}$ & $25.4_{\pm 1.80}$  & $21.8_{\pm 1.24}$ & $37.2_{\pm 1.80}$  & $18.0_{\pm 1.17}$ &  $38.8_{\pm 1.93}$ \\
MixUp  & $59.6_{\pm 0.31}$ & $81.5_{\pm 0.39}$ & $51.3_{\pm 8.63}$ & $75.8_{\pm 8.09}$ & $61.2_{\pm 0.88}$ & $86.0_{\pm 1.12}$  & $47.2_{\pm 0.60}$ & $81.3_{\pm 0.23}$ &   $\bm{52.4}_{\pm 0.80}$ & $95.5_{\pm 0.08}$ \\
Sph.   & $57.7_{\pm 0.18}$ & $82.9_{\pm 0.54}$ & $48.8_{\pm 0.51}$ & $74.3_{\pm 0.73}$ & $54.2_{\pm 0.32}$ & $81.2_{\pm 0.29}$  & $39.2_{\pm 0.31}$ & $72.1_{\pm 0.15}$ & $41.9_{\pm 0.10}$ &  $94.4_{\pm 0.04}$ \\
Boot.  & $54.0_{\pm 0.37}$ & $76.4_{\pm 0.39}$ & $37.7_{\pm 0.89}$ & $60.9_{\pm 1.52}$ & $56.0_{\pm 0.34}$ & $83.8_{\pm 0.03}$  & $43.2_{\pm 0.35}$ & $78.3_{\pm 0.20}$ &  $49.1_{\pm 0.29}$ & $95.3_{\pm 0.42}$ \\
Trunc. & $58.1_{\pm 0.36}$ & $82.7_{\pm 0.37}$ & $50.9_{\pm 1.17}$ & $77.2_{\pm 0.59}$ & $56.3_{\pm 0.62}$ & $82.3_{\pm 0.61}$  & $45.2_{\pm 0.81}$ & $75.6_{\pm 0.29}$ &  $23.7_{\pm 0.98}$ & $40.1_{\pm 1.24}$ \\
CL & $53.0_{\pm 0.21}$ & $76.3_{\pm 0.19}$ & $36.3_{\pm 0.77}$ & $60.1_{\pm 0.66}$ & $55.3_{\pm 0.48}$ & $83.5_{\pm 0.28}$  & $42.4_{\pm 0.45}$ & $78.1_{\pm 0.14}$ &  $48.2_{\pm 0.45}$ & $95.0_{\pm 0.04}$ \\
ELR & $10.4_{\pm 0.24}$ & $31.7_{\pm 0.44}$ & $10.0_{\pm 0.64}$ & $30.1_{\pm 0.88}$ & $10.8_{\pm 0.21}$ & $32.7_{\pm 0.53}$  & $10.3_{\pm 0.39}$ & $30.8_{\pm 0.35}$ &  $40.3_{\pm 0.39}$ & $93.0_{\pm 0.24}$ \\\hline
FCE & $56.9_{\pm 0.58}$ & $79.2_{\pm 0.14}$ & $43.7_{\pm 0.15}$ & $66.2_{\pm{0.19}}$ & $55.3_{\pm 0.54}$ & $83.5_{\pm 0.24}$  & $41.4_{\pm 0.55}$ & $77.3_{\pm 0.75}$ &  $39.0_{\pm 0.05}$ & $67.8_{\pm 0.47}$ \\
\rowcolor{green}
FCE$+$B & $56.1_{\pm 2.22}$ & $\boxed{81.8_{\pm 1.37}}$ & $\boxed{50.2_{\pm 0.02}}$ & $\boxed{77.2_{\pm 0.19}}$ & $54.2_{\pm 0.44}$ & $83.3_{\pm 0.43}$  & $\boxed{43.8_{\pm 0.02}}$ & $\boxed{77.5_{\pm 0.13}}$ &  $\boxed{40.0_{\pm 0.35}}$ & $\boxed{73.2_{\pm 0.08}}$ \\
FCE$+$B* & $56.1_{\pm 2.22}$ & $82.2_{\pm 0.39}$ & $50.2_{\pm 0.02}$ & $77.2_{\pm 0.19}$ & $54.2_{\pm 0.44}$ & $83.4_{\pm 0.24}$  & $45.1_{\pm 0.37}$ & $79.9_{\pm 0.24}$ &  $43.1_{\pm 0.40}$ & $79.4_{\pm 0.12}$ \\\hline
GCE  & $60.0_{\pm 0.13}$ & $82.6_{\pm 0.63}$ & $44.9_{\pm 0.07}$ & $67.2_{\pm 0.34}$ & $53.8_{\pm 0.55}$ & $81.6_{\pm 0.14}$  & $39.4_{\pm 0.44}$ & $74.0_{\pm 0.36}$ &  $44.8_{\pm 0.62}$ & $91.2_{\pm 0.70}$ \\
\rowcolor{green}
 GCE$+$B & $59.4_{\pm 0.02}$ & $\boxed{83.5_{\pm 0.24}}$  & $\boxed{50.3_{\pm 0.11}}$ & $\boxed{75.3_{\pm 0.64}}$ & $\boxed{55.4_{\pm 0.55}}$ & $\boxed{83.0_{\pm 0.35}}$  & $\boxed{46.5_{\pm 1.44}}$ & $\boxed{77.7_{\pm 0.35}}$ &  $\boxed{47.1_{\pm 0.20}}$ & $\boxed{93.5_{\pm 0.43}}$ \\
GCE$+$B* & $61.0_{\pm 1.33}$ & $83.9_{\pm 0.74}$  & $50.3_{\pm 0.11}$ & $75.3_{\pm 0.64}$ & $56.6_{\pm 0.10}$ & $83.8_{\pm 0.88}$  & $47.7_{\pm 0.35}$ & $77.9_{\pm 0.03}$ &   $47.1_{\pm 0.20}$ & $93.5_{\pm 0.43}$ \\\hline
SCE  & $55.9_{\pm 0.53}$ & $76.5_{\pm 0.15}$ & $38.7_{\pm 0.60}$ & $60.9_{\pm 0.41}$ & $57.5_{\pm 0.19}$ & $83.7_{\pm 0.17}$  & $43.3_{\pm 0.87}$ & $77.5_{\pm 0.75}$ &  $47.2_{\pm 0.33}$ & $92.5_{\pm 0.01}$\\
\rowcolor{green}
SCE$+$B & $55.5_{\pm 0.90}$ & $\boxed{77.4_{\pm 0.84}}$ & $\boxed{47.1_{\pm 1.32}}$ & $\boxed{69.2_{\pm 1.18}}$ & $\boxed{57.9_{\pm 0.83}}$ & $83.7_{\pm 0.41}$  & $\boxed{50.0_{\pm 1.62}}$ & $\boxed{80.4_{\pm 0.65}}$ &  $\boxed{47.9_{\pm 0.80}}$ & $\boxed{93.8_{\pm 0.05}}$ \\
SCE$+$B* & $56.6_{\pm 1.07}$ & $78.5_{\pm 0.88}$ & $47.3_{\pm 1.16}$ & $69.6_{\pm 0.90}$ & $57.9_{\pm 0.83}$ & $83.7_{\pm 0.41}$  & $50.0_{\pm 1.62}$ & $80.4_{\pm 0.65}$ &  $47.9_{\pm 0.80}$ & $93.8_{\pm 0.05}$ \\\hline
CE & $52.3_{\pm 1.35}$ & $75.6_{\pm 0.93}$ & $35.3_{\pm 1.14}$ & $59.3_{\pm 0.81}$ & $54.9_{\pm 0.12}$ & $83.3_{\pm 0.25}$ & $42.4_{\pm 0.16}$ & $78.9_{\pm 0.56}$ &  $48.6_{\pm 0.11}$ & $95.3_{\pm 0.10}$ \\
\rowcolor{green}
CE$+$B & $\boxed{50.9_{\pm 1.01}}$ & $\boxed{76.5_{\pm 0.86}}$ & $\boxed{39.9_{\pm 1.02}}$ & $\boxed{65.8_{\pm 1.19}}$ & $52.9_{\pm 1.86}$ & $83.2_{\pm 0.88}$  & $34.7_{\pm 2.51}$ & $73.4_{\pm 1.50}$  &  $45.5_{\pm 5.11}$ & $93.0_{\pm 0.16}$ \\
CE$+$B* & $50.9_{\pm 1.01}$ & $78.2_{\pm 1.16}$ & $39.9_{\pm 1.02}$ & $68.1_{\pm 0.63}$ & $53.3_{\pm 0.89}$ & $83.2_{\pm 0.88}$  & $45.9_{\pm 0.40}$ & $79.7_{\pm 0.29}$  & $50.2_{\pm 0.35}$ &  $\bm{95.9}_{\pm 0.14}$ \\ \hline
CEP  & $58.8_{\pm 0.87}$ & $78.6_{\pm 0.38}$ & $43.5_{\pm 0.24}$ & $65.1_{\pm 1.27}$ & $59.4_{\pm 0.08}$ & $82.2_{\pm 0.03}$  & $46.5_{\pm 0.17}$ & $76.4_{\pm 0.25}$ &  $48.2_{\pm 0.05}$ & $95.4_{\pm 0.07}$ \\
\rowcolor{green}
CEP$+$B &  $\boxed{\bm{62.3}_{\pm 0.87}}$ &  $\boxed{\bm{85.1}_{\pm 0.46}}$ & $\boxed{54.3_{\pm 0.86}}$ & $\boxed{79.2_{\pm 0.93}}$ & $\boxed{63.0_{\pm 0.92}}$ &  $\boxed{\bm{87.5}_{\pm 0.32}}$  & $\boxed{53.0_{\pm 0.28}}$ & $\boxed{82.8_{\pm 0.13}}$ &  $45.0_{\pm 0.48}$ & $95.0_{\pm 0.08}$\\
CEP$+$B* & $62.9_{\pm 0.79}$ & $85.1_{\pm 0.46}$ &  $\bm{55.3}_{\pm 0.37}$ &  $\bm{79.8}_{\pm 0.08}$ &  $\bm{63.0}_{\pm 0.14}$ & $87.5_{\pm 0.32}$  &  $\bm{55.6}_{\pm 0.66}$ &  $\bm{83.8}_{\pm 0.11}$ &   $47.7_{\pm 0.19}$ & $95.9_{\pm 0.23}$
    \end{tabular}
    \end{adjustbox}
    \caption{Test accuracies for different losses on the noisy CIFAR100/Asym-CIFAR100/Non-Uniform EMNIST datasets. Losses implementing the noise-bound shaded in blue. When using this bound provides benefit, the corresponding value is \boxed{boxed}. Overall top values in \textbf{bold}. }\label{tab:full_table2}
\end{table*}


\begin{table}[h!]
\centering
\begin{adjustbox}{max width=0.8\textwidth}
\begin{tabular}{c|lllll}
\multicolumn{1}{l}{} & \multicolumn{2}{c}{TinyImageNet (0.2)}  & \multicolumn{2}{c}{TinyImageNet (0.4)} & Animals \\\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Losses & \multicolumn{1}{l}{Top 1} & \multicolumn{1}{l}{Top 5} & \multicolumn{1}{l}{Top 1} & \multicolumn{1}{l}{Top 5} & \multicolumn{1}{l}{} \\\hline
L2 (MSE) & $42.91$ & $67.02$ & $29.42$ & $53.13$ & $80.97$ \\
MAE & $3.86$ & $5.58$ & $3.94$ & $5.54$ & $54.67$ \\
NCE-MAE & $7.63$ & $10.24$ & $6.29$ & $10.70$ & $80.85$ \\
Mix-Up & $47.13$ & $70.08$ & $31.05$ & $58.96$ & $83.76$ \\
Bootstrap & $40.04$ & $61.94$ & $25.69$ & $46.65$ & $82.11$ \\
Truncated & $43.35$ & $63.67$ & $38.14$ & $59.99$ & $81.69$ \\
Mix-Up & $47.13$ & $70.08$ & $31.05$ & $58.96$ & $83.10$ \\ 
Curriculum & $41.81$ & $64.53$ & $27.57$ & $48.84$ & $81.68$  \\
ELR & $44.95$ & $66.65$ & $34.66$ & $55.72$ & $82.62$\\\hline
FCE & $43.81$ & $64.97$ & $48.85$ & $29.92$ & $81.82$ \\\rowcolor{green}
FCE$+$B &  $\boxed{51.18}$ &  $\boxed{73.79}$ & $46.34$ & $\boxed{69.92}$  &  $\boxed{82.40}$ \\\hline
GCE & $39.81$ & $60.51$ & $26.93$ & $45.17$ & $81.13$  \\\rowcolor{green}
GCE$+$B & $\boxed{47.40}$ & $\boxed{71.37}$ & $\boxed{39.13}$ & $\boxed{63.75}$ &  $\boxed{81.37}$ \\\hline
SCE & $39.81$ & $60.51$ & $26.93$ & $45.17$ & $82.59$  \\\rowcolor{green}
CE & $39.34$ & $61.82$ & $25.84$ & $46.08$ & $81.45$ \\\rowcolor{green}
CE$+$B & $38.47$ & $\boxed{61.85}$ & $\boxed{30.00}$ & $\boxed{52.61}$ & $80.72$ \\ \hline
CEP & $44.39$ & $64.56$ & $33.33$ & $51.45$ & $82.06$  \\\rowcolor{green}
CEP$+$B & $\boxed{47.85}$ & $\boxed{71.00}$ & $\boxed{40.56}$ & $\boxed{65.15}$  & $81.79$ \\ \hline
\end{tabular}
\end{adjustbox}
\vspace{1mm}
\caption{Test accuracies obtained by using different losses on the noisy TinyImageNet and Animals10N datasets. Losses implementing the noise-bound are shaded in blue. When using this bound provides benefit, the corresponding value is \boxed{boxed}. Overall top values are in \textbf{bold}.}
\label{table:imagenet}
\end{table}

\subsubsection{Optimal Bounds}
In our experiment tables in Section~\ref{sec:vary_bounds}, we give results using our noise-bounds. We additionally give results where the bound is treated as a hyperparameter. We do not search over the entire space; rather, we do a grid search near the noise-bound. For MNIST, FashionMNIST, EMNIST, CIFAR10 and CIFAR100, we search over $\{-0.2, -0.15, -0.1, \ldots,  0.15, 0.2\}$ where e.g. $0.2$ means that we add $0.2$ onto our noise-bound ($B(\eta,c)\mapsto B(\eta,c) + 0.2$). For Asymmetric CIFAR100 (ACIFAR100) and Non-uniform EMNIST (NU-EMNIST), this range is broadened to $\{-0.6, -0.55, \ldots  0.55, 0.6\}$. The bounds which give the best results are given in Table~\ref{table:offsets}. When the optimal bound is higher than the noise-bound, this is highlighted in blue. Otherwise, the cell is indicated in red. In our original table, we have columns for Top1 and Top5 accuracy which often have slightly different optimal bounds. For brevity, we combine these by taking a mean of these values.


\end{document}



