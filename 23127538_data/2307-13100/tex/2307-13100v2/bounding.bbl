\begin{thebibliography}{}

\bibitem[Angluin and Laird, 1988]{angluin1988}
Angluin, D. and Laird, P. (1988).
\newblock Learning from noisy examples.
\newblock {\em Machine learning}, 2:343--370.

\bibitem[Arazo et~al., 2019]{bmm}
Arazo, E., Ortego, D., Albert, P., Oâ€™Connor, N., and McGuinness, K. (2019).
\newblock Unsupervised label noise modeling and loss correction.
\newblock In {\em International conference on machine learning}, pages 312--321. PMLR.

\bibitem[Arpit et~al., 2017]{memorisation}
Arpit, D., Jastrz\k{e}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al. (2017).
\newblock A closer look at memorization in deep networks.
\newblock In {\em International conference on machine learning}. PMLR.

\bibitem[Cheng et~al., 2024]{cheng2024dynamic}
Cheng, S., Chen, W., Liu, W., Zhou, L., Zhao, H., Kong, W., Qu, H., and Fu, M. (2024).
\newblock Dynamic training for handling textual label noise.
\newblock {\em Applied Intelligence}, pages 1--16.

\bibitem[Englesson and Azizpour, 2021a]{englesson2021consistency}
Englesson, E. and Azizpour, H. (2021a).
\newblock Consistency regularization can improve robustness to label noise.
\newblock {\em arXiv preprint arXiv:2110.01242}.

\bibitem[Englesson and Azizpour, 2021b]{gjs}
Englesson, E. and Azizpour, H. (2021b).
\newblock {Generalized Jensen-Shannon divergence loss for learning with noisy labels}.
\newblock {\em Advances in Neural Information Processing Systems}, 34:30284--30297.

\bibitem[Feng et~al., 2021a]{knn}
Feng, C., Tzimiropoulos, G., and Patras, I. (2021a).
\newblock {S3:} supervised self-supervised learning under label noise.
\newblock {\em CoRR}, abs/2111.11288.

\bibitem[Feng et~al., 2021b]{taylorce}
Feng, L., Shu, S., Lin, Z., Lv, F., Li, L., and An, B. (2021b).
\newblock Can cross entropy loss be robust to label noise?
\newblock In {\em Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence}, pages 2206--2212.

\bibitem[Ghosh and Kumar, 2017]{robust_theory}
Ghosh, A. and Kumar, H. (2017).
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~31.

\bibitem[Gneiting and Raftery, 2007]{gneiting2007strictly}
Gneiting, T. and Raftery, A.~E. (2007).
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock {\em Journal of the American statistical Association}, 102(477):359--378.

\bibitem[Goldberger and Ben-Reuven, 2022]{noiseAdapt}
Goldberger, J. and Ben-Reuven, E. (2022).
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In {\em Proceedings of the International Conference on Learning Representations}. ICLR.

\bibitem[Han et~al., 2018]{coteaching}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M. (2018).
\newblock Co-teaching: Robust training of deep neural networks with extremely noisy labels.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Hendrycks et~al., 2018a]{gold}
Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. (2018a).
\newblock Using trusted data to train deep networks on labels corrupted by severe noise.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Hendrycks et~al., 2018b]{trustedData}
Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. (2018b).
\newblock Using trusted data to train deep networks on labels corrupted by severe noise.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Iscen et~al., 2022]{neighbour}
Iscen, A., Valmadre, J., Arnab, A., and Schmid, C. (2022).
\newblock Learning with neighbor consistency for noisy labels.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 4672--4681.

\bibitem[Ishida et~al., 2020]{flooding}
Ishida, T., Yamane, I., Sakai, T., Niu, G., and Sugiyama, M. (2020).
\newblock Do we need zero training loss after achieving zero training error?
\newblock In {\em International Conference on Machine Learning}, pages 4604--4614. PMLR.

\bibitem[Janocha and Czarnecki, 2016]{janocha2017loss}
Janocha, K. and Czarnecki, W.~M. (2016).
\newblock On loss functions for deep neural networks in classification.
\newblock {\em Schedae Informaticae}, 25:49.

\bibitem[Janocha and Czarnecki, 2017]{L2}
Janocha, K. and Czarnecki, W.~M. (2017).
\newblock On loss functions for deep neural networks in classification.
\newblock {\em arXiv preprint arXiv:1702.05659}.

\bibitem[Jiang et~al., 2018]{mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L. (2018).
\newblock Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels.
\newblock In {\em International conference on machine learning}, pages 2304--2313. PMLR.

\bibitem[Kim et~al., 2021]{eigenfine}
Kim, T., Ko, J., Choi, J., Yun, S.-Y., et~al. (2021).
\newblock Fine samples for learning with noisy labels.
\newblock {\em Advances in Neural Information Processing Systems}, 34:24137--24149.

\bibitem[Larsen et~al., 1998]{larsen}
Larsen, J., Nonboe, L., Hintz-Madsen, M., and Hansen, L. (1998).
\newblock Design of robust neural network classifiers.
\newblock In {\em Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)}, volume~2, pages 1205--1208 vol.2.

\bibitem[Li et~al., 2020]{dividemix}
Li, J., Socher, R., and Hoi, S.~C. (2020).
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock {\em arXiv preprint arXiv:2002.07394}.

\bibitem[Li et~al., 2021]{anchor}
Li, X., Liu, T., Han, B., Niu, G., and Sugiyama, M. (2021).
\newblock Provably end-to-end label-noise learning without anchor points.
\newblock In {\em International conference on machine learning}, pages 6403--6413. PMLR.

\bibitem[Li et~al., 2023]{meta_dynamic}
Li, X.-C., Xia, X., Zhu, F., Liu, T., yao Zhang, X., and lin Liu, C. (2023).
\newblock Dynamic loss for learning with label noise.

\bibitem[Liu et~al., 2020]{elr}
Liu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C. (2020).
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock {\em Advances in neural information processing systems}, 33:20331--20342.

\bibitem[Ma et~al., 2020]{normalised_losses}
Ma, X., Huang, H., Wang, Y., Erfani, S. R.~S., and Bailey, J. (2020).
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In {\em Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org.

\bibitem[Malach and Shalev-Shwartz, 2017]{decoupling}
Malach, E. and Shalev-Shwartz, S. (2017).
\newblock Decoupling" when to update" from" how to update".
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Manwani and Sastry, 2013]{robust_theory_earlier_binary}
Manwani, N. and Sastry, P.~S. (2013).
\newblock Noise tolerance under risk minimization.
\newblock {\em IEEE Transactions on Cybernetics}, 43(3):1146--1151.

\bibitem[Mnih and Hinton, 2012]{mnihHinton}
Mnih, V. and Hinton, G.~E. (2012).
\newblock Learning to label aerial images from noisy data.
\newblock In {\em Proceedings of the 29th International conference on machine learning (ICML-12)}, pages 567--574.

\bibitem[Ovcharov, 2018]{properBregman}
Ovcharov, E.~Y. (2018).
\newblock Proper scoring rules and bregman divergences.
\newblock {\em Bernoulli}, 24(1):53--79.

\bibitem[Patrini et~al., 2017]{fprop}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L. (2017).
\newblock Making deep neural networks robust to label noise: A loss correction approach.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 1944--1952.

\bibitem[Reed et~al., 2014]{bootstrap}
Reed, S.~E., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A. (2014).
\newblock Training deep neural networks on noisy labels with bootstrapping.

\bibitem[Ren et~al., 2018]{meta_gradient}
Ren, M., Zeng, W., Yang, B., and Urtasun, R. (2018).
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em International conference on machine learning}, pages 4334--4343. PMLR.

\bibitem[Sachdeva et~al., 2021]{EvidenceMix}
Sachdeva, R., Cordeiro, F.~R., Belagiannis, V., Reid, I., and Carneiro, G. (2021).
\newblock Evidentialmix: Learning with combined open-set and closed-set noisy labels.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, pages 3607--3615.

\bibitem[Savage, 1971]{savagen}
Savage, L.~J. (1971).
\newblock Elicitation of personal probabilities and expectations.
\newblock {\em Journal of the American Statistical Association}, 66(336):783--801.

\bibitem[Song et~al., 2019]{sefie}
Song, H., Kim, M., and Lee, J.-G. (2019).
\newblock Selfie: Refurbishing unclean samples for robust deep learning.
\newblock In {\em International Conference on Machine Learning}, pages 5907--5915. PMLR.

\bibitem[Stempfel and Ralaivola, 2009]{old_backward}
Stempfel, G. and Ralaivola, L. (2009).
\newblock Learning svms from sloppily labeled data.
\newblock In {\em Artificial Neural Networks--ICANN 2009: 19th International Conference, Limassol, Cyprus, September 14-17, 2009, Proceedings, Part I 19}, pages 884--893. Springer.

\bibitem[Sukhbaatar et~al., 2015]{fprop_old}
Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., and Fergus, R. (2015).
\newblock Training convolutional networks with noisy labels.
\newblock In {\em 3rd International Conference on Learning Representations, ICLR 2015}.

\bibitem[Szegedy et~al., 2016]{labelSmoothing}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016).
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 2818--2826.

\bibitem[Thiel, 2008]{soft_labels}
Thiel, C. (2008).
\newblock Classification on soft labels is robust against label noise.
\newblock In {\em International Conference on Knowledge-Based and Intelligent Information and Engineering Systems}, pages 65--73. Springer.

\bibitem[Van~Rooyen et~al., 2015]{unhinged}
Van~Rooyen, B., Menon, A., and Williamson, R.~C. (2015).
\newblock Learning with symmetric label noise: The importance of being unhinged.
\newblock {\em Advances in neural information processing systems}, 28.

\bibitem[Vyas et~al., 2020]{metaLabels}
Vyas, N., Saxena, S., and Voice, T. (2020).
\newblock Learning soft labels via meta learning.
\newblock {\em arXiv preprint arXiv:2009.09496}.

\bibitem[Wang et~al., 2019]{sce}
Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., and Bailey, J. (2019).
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In {\em 2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 322--330, Los Alamitos, CA, USA. IEEE Computer Society.

\bibitem[Wei et~al., 2021]{wei2021open}
Wei, H., Tao, L., Xie, R., and An, B. (2021).
\newblock Open-set label noise can improve robustness against inherent label noise.
\newblock {\em Advances in Neural Information Processing Systems}, 34:7978--7992.

\bibitem[Zhang et~al., 2017]{mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D. (2017).
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv e-prints}, pages arXiv--1710.

\bibitem[Zhang and Sabuncu, 2018]{GCE_Loss}
Zhang, Z. and Sabuncu, M. (2018).
\newblock Generalized cross entropy loss for training deep neural networks with noisy labels.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Zheng et~al., 2019]{metaLabelCorrect}
Zheng, G., Awadallah, A.~H., and Dumais, S.~T. (2019).
\newblock Meta label correction for learning with weak supervision.
\newblock {\em CoRR}, abs/1911.03809.

\bibitem[Zhou et~al., 2020]{curiculum}
Zhou, T., Wang, S., and Bilmes, J. (2020).
\newblock Robust curriculum learning: from clean label detection to noisy label self-correction.
\newblock In {\em International Conference on Learning Representations}.

\end{thebibliography}
