@article{luo1991convergence,
  title={On the convergence of the LMS algorithm with adaptive learning rate for linear feedforward networks},
  author={Luo, Zhi-Quan},
  journal={Neural Computation},
  volume={3},
  number={2},
  pages={226--245},
  year={1991},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{ahn2020sgd,
  title={SGD with shuffling: optimal rates without component convexity and large epoch requirements},
  author={Ahn, Kwangjun and Yun, Chulhee and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17526--17535},
  year={2020}
}

@article{liuzzi2022convergence,
  title={Convergence under Lipschitz smoothness of ease-controlled Random Reshuffling gradient Algorithms},
  author={Liuzzi, Giampaolo and Palagi, Laura and Seccia, Ruggiero},
  journal={arXiv preprint arXiv:2212.01848},
  year={2022}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}

@incollection{bengio2012practical,
  title={Practical recommendations for gradient-based training of deep architectures},
  author={Bengio, Yoshua},
  booktitle={Neural networks: Tricks of the trade},
  pages={437--478},
  year={2012},
  publisher={Springer}
}

@article{ding2022suboptimal,
  title={Suboptimal Local Minima Exist for Wide Neural Networks with Smooth Activations},
  author={Ding, Tian and Li, Dawei and Sun, Ruoyu},
  journal={Mathematics of Operations Research},
  year={2022},
  publisher={INFORMS}
}



@article{palagi,
  title={Global optimization issues in deep network regression: an overview},
  author={Palagi, Laura},
  journal={Journal of Global Optimization},
  pages={1--39},
  year={2018},
  publisher={Springer}
}

@INPROCEEDINGS{8862686,
  author={Vani, S. and Rao, T. V. Madhusudhana},
  booktitle={2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)}, 
  title={An Experimental Approach towards the Performance Assessment of Various Optimizers on Convolutional Neural Network}, 
  year={2019},
  volume={},
  number={},
  pages={331-336},
  doi={10.1109/ICOEI.2019.8862686}}

@article{BRAIEK2020110542,
title = {On testing machine learning programs},
journal = {Journal of Systems and Software},
volume = {164},
pages = {110542},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110542},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300248},
author = {Houssem Ben Braiek and Foutse Khomh},
keywords = {Machine learning, Data cleaning, Feature engineering testing, Model testing, Implementation testing}
}

@inproceedings{xu2020second,
  title={Second-order optimization for non-convex machine learning: An empirical study},
  author={Xu, Peng and Roosta, Fred and Mahoney, Michael W},
  booktitle={Proceedings of the 2020 SIAM International Conference on Data Mining},
  pages={199--207},
  year={2020},
  organization={SIAM}
}

@article{wang2023deep,
  title={Deep learning-based EEG emotion recognition: Current trends and future perspectives},
  author={Wang, Xiaohu and Ren, Yongmei and Luo, Ze and He, Wei and Hong, Jun and Huang, Yinzhen},
  journal={Frontiers in Psychology},
  volume={14},
  pages={213},
  year={2023},
  publisher={Frontiers}
}

@article{morris2023deep,
  title={Deep learning applications in surgery: Current uses and future directions},
  author={Morris, Miranda X and Rajesh, Aashish and Asaad, Malke and Hassan, Abbas and Saadoun, Rakan and Butler, Charles E},
  journal={The American Surgeon},
  volume={89},
  number={1},
  pages={36--42},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{yin2023deep,
  title={Deep learning for pancreatic diseases based on endoscopic ultrasound: A systematic review},
  author={Yin, Minyue and Liu, Lu and Gao, Jingwen and Lin, Jiaxi and Qu, Shuting and Xu, Wei and Liu, Xiaolin and Xu, Chunfang and Zhu, Jinzhou},
  journal={International Journal of Medical Informatics},
  pages={105044},
  year={2023},
  publisher={Elsevier}
}
@article{gupta2023recognition,
  title={Recognition of Suspicious Human Activity in Video Surveillance: A Review},
  author={Gupta, Neha and Agarwal, Bharat Bhushan},
  journal={Engineering, Technology \& Applied Science Research},
  volume={13},
  number={2},
  pages={10529--10534},
  year={2023}
}

@book{lan2020first,
  title={First-order and stochastic optimization methods for machine learning},
  address={New York},
  author={Lan, Guanghui},
  year={2020},
  publisher={Springer}
}

@book{bengio2017deep,
  title={Deep learning},
  author={Bengio, Yoshua and Goodfellow, Ian and Courville, Aaron},
  volume={1},
  year={2017},
  publisher={MIT press Cambridge},
  address={MA, USA}
}

@article{im2016empirical,
  title={An empirical analysis of the optimization of deep network loss surfaces},
  author={Im, Daniel Jiwoong and Tao, Michael and Branson, Kristin},
  journal={arXiv preprint arXiv:1612.04010},
  year={2016}
}

@inproceedings{mercioni2020p,
  title={P-swish: Activation function with learnable parameters based on swish activation function in deep learning},
  author={Mercioni, Marina Adriana and Holban, Stefan},
  booktitle={2020 International Symposium on Electronics and Telecommunications (ISETC)},
  pages={1--4},
  year={2020},
  organization={IEEE}
}

@article{ramachandran2017searching,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@article{sun2019survey,
  title={A survey of optimization methods from a machine learning perspective},
  author={Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
  journal={IEEE transactions on cybernetics},
  volume={50},
  number={8},
  pages={3668--3681},
  year={2019},
  publisher={IEEE}
}

@article{pouyanfar2018survey,
  title={A survey on deep learning: Algorithms, techniques, and applications},
  author={Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja S},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={5},
  pages={1--36},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{jais2019adam,
  title={Adam optimization algorithm for wide and deep neural network},
  author={Jais, Imran Khan Mohd and Ismail, Amelia Ritahani and Nisa, Syed Qamrun},
  journal={Knowledge Engineering and Data Science},
  volume={2},
  number={1},
  pages={41--46},
  year={2019}
}

@article{baumann2019comparative,
  title={A comparative study of the leading machine learning techniques and two new optimization algorithms},
  author={Baumann, Philipp and Hochbaum, Dorit S and Yang, Yan T},
  journal={European journal of operational research},
  volume={272},
  number={3},
  pages={1041--1057},
  year={2019},
  publisher={Elsevier}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{kovacs2011analysis,
  title={On the analysis and design of software for reinforcement learning, with a survey of existing systems},
  author={Kovacs, Tim and Egginton, Robert},
  journal={Machine learning},
  volume={84},
  number={1},
  pages={7--49},
  year={2011},
  publisher={Springer}
}

@article{lim2000comparison,
  title={A comparison of prediction accuracy, complexity, and training time of thirty-three old and new classification algorithms},
  author={Lim, Tjen-Sien and Loh, Wei-Yin and Shih, Yu-Shan},
  journal={Machine learning},
  volume={40},
  number={3},
  pages={203--228},
  year={2000},
  publisher={Springer}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{mcmahan2013ad,
  title={Ad click prediction: a view from the trenches},
  author={McMahan, H Brendan and Holt, Gary and Sculley, David and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and others},
  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1222--1230},
  year={2013}
}

@article{Howard2017MobileNetsEC,
  title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.04861}
}

@article{Huang2017DenselyCC,
  title={Densely Connected Convolutional Networks},
  author={Gao Huang and Zhuang Liu and Kilian Q. Weinberger},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={2261-2269}
}

@article{He2016DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}

@inproceedings{uc_merced,
    author = {Yang, Yi and Newsam, Shawn},
    title = {Bag-of-Visual-Words and Spatial Extensions for Land-Use Classification},
    year = {2010},
    isbn = {9781450304283},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1869790.1869829},
    doi = {10.1145/1869790.1869829},
    booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
    pages = {270–279},
    numpages = {10},
    location = {San Jose, California},
    series = {GIS '10}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@inproceedings{Dozat2016IncorporatingNM,
  title={Incorporating Nesterov Momentum into Adam},
  author={Timothy Dozat},
  year={2016},
  booktitle={ICLR Workshop}
}

@inproceedings{Sutskever2013OnTI,
  title={On the importance of initialization and momentum in deep learning},
  author={Ilya Sutskever and James Martens and George E. Dahl and Geoffrey E. Hinton},
  booktitle={ICML},
  year={2013}
}
@article{yun2018small,
  title={Small nonlinearities in activation functions create bad local minima in neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1802.03487},
  year={2018}
}

@article{berahas2020robust,
  title={A robust multi-batch L-BFGS method for machine learning},
  author={Berahas, Albert S and Tak{\'a}{\v{c}}, Martin},
  journal={Optimization Methods and Software},
  volume={35},
  number={1},
  pages={191--219},
  year={2020},
  publisher={Taylor \& Francis}
}

@inproceedings{bollapragada2018progressive,
  title={A progressive batching L-BFGS method for machine learning},
  author={Bollapragada, Raghu and Nocedal, Jorge and Mudigere, Dheevatsa and Shi, Hao-Jun and Tang, Ping Tak Peter},
  booktitle={International Conference on Machine Learning},
  pages={620--629},
  year={2018},
  organization={PMLR}
}

@article{berahas2016multi,
  title={A multi-batch L-BFGS method for machine learning},
  author={Berahas, Albert S and Nocedal, Jorge and Tak{\'a}c, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{palagi2019global,
  title={Global optimization issues in deep network regression: an overview},
  author={Palagi, Laura},
  journal={Journal of Global Optimization},
  volume={73},
  number={2},
  pages={239--277},
  year={2019},
  publisher={Springer}
}

@article{AGGARWAL2021100004,
title = {Generative adversarial network: An overview of theory and applications},
journal = {International Journal of Information Management Data Insights},
volume = {1},
number = {1},
pages = {100004},
year = {2021},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2020.100004},
url = {https://www.sciencedirect.com/science/article/pii/S2667096820300045},
author = {Alankrita Aggarwal and Mamta Mittal and Gopi Battineni},
keywords = {GAN, Deep learning, Image mining, Big data, Literature review, Neural networks},
abstract = {In recent times, image segmentation has been involving everywhere including disease diagnosis to autonomous vehicle driving. In computer vision, this image segmentation is one of the vital works and it is relatively complicated than other vision undertakings as it needs low-level spatial data. Especially, Deep Learning has impacted the field of segmentation incredibly and gave us today different successful models. The deep learning associated Generated Adversarial Networks (GAN) has presenting remarkable outcomes on image segmentation. In this study, the authors have presented a systematic review analysis on recent publications of GAN models and their applications. Three libraries such as Embase (Scopus), WoS, and PubMed have been considered for searching the relevant papers available in this area. Search outcomes have identified 2084 documents, after two-phase screening 52 potential records are included for final review. The following applications of GAN have been emerged: 3D object generation, medicine, pandemics, image processing, face detection, texture transfer, and traffic controlling. Before 2016, research in this field was limited and thereafter its practical usage came into existence worldwide. The present study also envisions the challenges associated with GAN and paves the path for future research in this realm.}
}

@article{KOLAGATI2022100054,
title = {Exposing deepfakes using a deep multilayer perceptron – convolutional neural network model},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {1},
pages = {100054},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2021.100054},
url = {https://www.sciencedirect.com/science/article/pii/S2667096821000471},
author = {Santosh Kolagati and Thenuga Priyadharshini and V. {Mary Anita Rajam}},
keywords = {CNN, Multilayer perceptron, Deepfake detection, Mixed data, classification},
abstract = {Creating deepfakes has rapidly become easier and more accessible due to advancements in hardware and computing. The harmful nature of deepfakes urges immediate action to improve detection of such doctored videos. In this work, we build a deep hybrid neural network model to detect deepfake videos. Using facial landmarks detection, we extract data pertaining to various facial attributes from the videos. This data is passed to a multilayer perceptron to learn differences in real and deepfake videos. Simultaneously, we use a convolutional neural network to extract features and train on the videos. We combine these two models to build a multi-input deepfake detector. A subset of the Deepfake Detection Challenge Dataset along with the Dessa Dataset is used to train the model. The proposed model provides good classification results with an accuracy of 84% and an AuC score of 0.87.}
}

@article{ZUTSHI2021100042,
title = {Systematic review and exploration of new avenues for sorting algorithm},
journal = {International Journal of Information Management Data Insights},
volume = {1},
number = {2},
pages = {100042},
year = {2021},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2021.100042},
url = {https://www.sciencedirect.com/science/article/pii/S2667096821000355},
author = {Anand Zutshi and Dipanjan Goswami},
abstract = {Sorting plays a crucial role in almost all algorithms that support data science applications. A quadratic sorting algorithm is one in which the time taken for the algorithm to sort a dataset grows as the square of the size of the dataset. This paper aims at developing a new quadratic sorting algorithm by addressing the limitations in existing sorting algorithms. We use the idea that an unsorted data sequence can be thought of as a set of disjoint sorted sequences of data items. This paper describes a novel approach to provide a solution towards an unsorted sequence of data items. We have compared our method with existing quadratic sorting algorithms to demonstrate its efficiency over other techniques.}
}

@article{CHAUHAN2021100020,
title = {Optimization and fine-tuning of DenseNet model for classification of COVID-19 cases in medical imaging},
journal = {International Journal of Information Management Data Insights},
volume = {1},
number = {2},
pages = {100020},
year = {2021},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2021.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2667096821000136},
author = {Tavishee Chauhan and Hemant Palivela and Sarveshmani Tiwari},
keywords = {COVID-19 diagnosis, Convolution neural networks, DenseNet, Transfer learning, Fine tuning, Radiology images, Early stopping},
abstract = {It’s been more than a year that the entire world is fighting against COVID-19 pandemic. Starting from the Wuhan city in China, COVID-19 has conquered the entire world with its rapid progression. But seeking the importance towards the human situation, it has become essential to build such an automated model to diagnose COVID-19 within less computational time easily. As the disease has spread, there is not enough data to implement an accurate COVID-19 predicting model. But technology is a boon, which makes it possible. Effective techniques based on medical imaging using artificial intelligence have approached to assist humans in needful time. It has become very essential to detect COVID-19 in humans at an early stage to prevent it from becoming more infectious. The neural networks have shown promising results in medical imaging. In this research, a deep learning-based approach is used for image classification to detect COVID-19 using chest X-ray images (CXR). A CNN classifier have been used to classify the normal-healthy images from the COVID-19 images, using transfer learning. The concept of early stopping is used to enhance the accuracy of the proposed DenseNet model. The results of the system have been evaluated using accuracy, precision, recall and F1-score metrics. An automated comparative analysis among multiple optimizers, LR Scheduler and Loss Function is performed to get the highest accuracy suitable for the proposed system. The Adamax optimizer with Cross Entropy loss function and StepLR scheduler have outperformed with 98.45\% accuracy for normal-healthy CXR images and 98.32\% accuracy for COVID-19 images.}
}
@article{YOUNG2022100070,
title = {Empirical evaluation of performance degradation of machine learning-based predictive models – A case study in healthcare information systems},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {1},
pages = {100070},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000143},
author = {Zachary Young and Robert Steele},
keywords = {Health predictive model, Performance degradation, Machine learning, Health information systems},
abstract = {While there have been a very large number of academic studies of proposed machine learning-based health predictive models, it is widely recognized that machine learning-based models in all domains typically degrade in performance over time, post training. This known characteristic of machine learning-based models could present significant risks in the healthcare setting to patient quality of care or safety. Nevertheless, there has been little study of the performance degradation of such models on real-world data. In this article, we empirically measure performance degradation of predictive models that predict at time of admission, emergency patient mortality, drawing upon a large dataset of over 1.83 million patient discharge records. We demonstrate important empirical results including both relatively slow performance degradation over two and a half years, but also significant differences in the rate and extent of performance degradation between different machine learning model types and time period of the training set.}
}

@inproceedings{li2007optimizing,
  title={Optimizing sorting with machine learning algorithms},
  author={Li, Xiaoming and Garzaran, Maria Jesus and Padua, David},
  booktitle={2007 IEEE International Parallel and Distributed Processing Symposium},
  pages={1--6},
  year={2007},
  organization={IEEE}
}

@article{CALDELLI202131,
title = {Optical Flow based CNN for detection of unlearnt deepfake manipulations},
journal = {Pattern Recognition Letters},
volume = {146},
pages = {31-37},
year = {2021},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2021.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167865521000842},
author = {Roberto Caldelli and Leonardo Galteri and Irene Amerini and Alberto {Del Bimbo}},
keywords = {Deepfake manipulations, Optical Flow, Video forensics, CNN},
abstract = {A new phenomenon named Deepfakes constitutes a serious threat in video manipulation. AI-based technologies have provided easy-to-use methods to create extremely realistic videos. On the side of multimedia forensics, being able to individuate this kind of fake contents becomes ever more crucial. In this work, a new forensic technique able to detect fake and original video sequences is proposed; it is based on the use of CNNs trained to distinguish possible motion dissimilarities in the temporal structure of a video sequence by exploiting optical flow fields. The results obtained highlight comparable performances with the state-of-the-art methods which, in general, only resort to single video frames. Furthermore, the proposed optical flow based detection scheme also provides a superior robustness in the more realistic cross-forgery operative scenario and can even be combined with frame-based approaches to improve their global effectiveness.}
}

@article{creswell2018generative,
  title={Generative adversarial networks: An overview},
  author={Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A},
  journal={IEEE signal processing magazine},
  volume={35},
  number={1},
  pages={53--65},
  year={2018},
  publisher={IEEE}
}

@article{lecun1995convolutional,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995},
  publisher={Cambridge, MA USA}
}

@article{hijazi2015using,
  title={Using convolutional neural networks for image recognition},
  author={Hijazi, Samer and Kumar, Rishi and Rowen, Chris and others},
  journal={Cadence Design Systems Inc.: San Jose, CA, USA},
  volume={9},
  year={2015}
}

@ARTICLE{9762984,
  author={Papa, Lorenzo and Alati, Edoardo and Russo, Paolo and Amerini, Irene},
  journal={IEEE Access}, 
  title={SPEED: Separable Pyramidal Pooling EncodEr-Decoder for Real-Time Monocular Depth Estimation on Low-Resource Settings}, 
  year={2022},
  volume={10},
  number={},
  pages={44881-44890},
  doi={10.1109/ACCESS.2022.3170425}}


  
  @article{guo2018review,
  title={A review of semantic segmentation using deep neural networks},
  author={Guo, Yanming and Liu, Yu and Georgiou, Theodoros and Lew, Michael S},
  journal={International journal of multimedia information retrieval},
  volume={7},
  number={2},
  pages={87--93},
  year={2018},
  publisher={Springer}
}

@article{dolan2002benchmarking,
  title={Benchmarking optimization software with performance profiles},
  author={Dolan, Elizabeth D and Mor{\'e}, Jorge J},
  journal={Mathematical programming},
  volume={91},
  number={2},
  pages={201--213},
  year={2002},
  publisher={Springer}
}

@article{ding2017trunk,
  title={Trunk-branch ensemble convolutional neural networks for video-based face recognition},
  author={Ding, Changxing and Tao, Dacheng},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={4},
  pages={1002--1014},
  year={2017},
  publisher={IEEE}
}

@article{wang2017generative,
  title={Generative adversarial networks: introduction and outlook},
  author={Wang, Kunfeng and Gou, Chao and Duan, Yanjie and Lin, Yilun and Zheng, Xinhu and Wang, Fei-Yue},
  journal={IEEE/CAA Journal of Automatica Sinica},
  volume={4},
  number={4},
  pages={588--598},
  year={2017},
  publisher={IEEE}
}

@article{sun2019optimization,
  title={Optimization for deep learning: theory and algorithms},
  author={Sun, Ruoyu},
  journal={arXiv preprint arXiv:1912.08957},
  year={2019}
}

@article{yang2015non,
  title={Non-rigid multi-modal medical image registration by combining L-BFGS-B with cat swarm optimization},
  author={Yang, Feng and Ding, Mingyue and Zhang, Xuming and Hou, Wenguang and Zhong, Cheng},
  journal={Information sciences},
  volume={316},
  pages={440--456},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{wang2019accelerating,
  title={Accelerating image reconstruction in ultrasound transmission tomography using L-BFGS algorithm},
  author={Wang, Hongjian and Gemmeke, Hartmut and Hopp, Torsten and Hesser, J{\"u}rgen},
  booktitle={Medical Imaging 2019: Ultrasonic Imaging and Tomography},
  volume={10955},
  pages={67--76},
  year={2019},
  organization={SPIE}
}

@article{swirszcz2016local,
  title={Local minima in training of neural networks},
  author={Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  journal={arXiv preprint arXiv:1611.06310},
  year={2016}
}

@article{McMahan2013AdCP,
  title={Ad click prediction: a view from the trenches},
  author={H. B. McMahan and Gary Holt and D. Sculley and Michael Young and Dietmar Ebner and Julian Grady and Lan Nie and Todd Phillips and Eugene Davydov and Daniel Golovin and Sharat Chikkerur and Dan Liu and Martin Wattenberg and Arnar Mar Hrafnkelsson and Tom Boulos and Jeremy Kubica},
  journal={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  year={2013}
}



@inproceedings{mcmahan2011follow,
  title={Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization},
  author={McMahan, Brendan},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={525--533},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}}

@inproceedings{Duchi2010AdaptiveSM,
  title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author={John C. Duchi and Elad Hazan and Yoram Singer},
  booktitle={J. Mach. Learn. Res.},
  year={2010}
}

@article{Zeiler2012ADADELTAAA,
  title={ADADELTA: An Adaptive Learning Rate Method},
  author={Matthew D. Zeiler},
  journal={ArXiv},
  year={2012},
  volume={abs/1212.5701}
}



@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@article{Abbaschian2021DeepLT,
  title={Deep Learning Techniques for Speech Emotion Recognition, from Databases to Models},
  author={Babak Joze Abbaschian and Daniel Sierra-Sosa and Adel Said Elmaghraby},
  journal={Sensors (Basel, Switzerland)},
  year={2021},
  volume={21}
}

@article{Kuutti2021ASO,
  title={A Survey of Deep Learning Applications to Autonomous Vehicle Control},
  author={Sampo Kuutti and R. Bowden and Yaochu Jin and Phil Barber and Saber Fallah},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  year={2021},
  volume={22},
  pages={712-733}
}

@article{Shorten2021DeepLA,
  title={Deep Learning applications for COVID-19},
  author={Connor Shorten and Taghi M. Khoshgoftaar and Borko Furht},
  journal={Journal of Big Data},
  year={2021},
  volume={8}
}

@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  pages={2},
  year={2012}
}


@book{nocedal1999numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen J},
  year={1999},
  publisher={Springer},
  address={New York}
}

@article{liu1989limited,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer}
}

@article{grippo1994class,
  title={A class of unconstrained minimization methods for neural network training},
  author={Grippo, Luigi},
  journal={Optimization Methods and Software},
  volume={4},
  number={2},
  pages={135--150},
  year={1994},
  publisher={Taylor \& Francis}
}

@incollection{bengio2012practical,
  title={Practical recommendations for gradient-based training of deep architectures},
  author={Bengio, Yoshua},
  booktitle={Neural networks: Tricks of the trade},
  pages={437--478},
  year={2012},
  publisher={Springer}
}

@article{malinovsky2022federated,
  title={Federated Random Reshuffling with Compression and Variance Reduction},
  author={Malinovsky, Grigory and Richt{\'a}rik, Peter},
  year={2022},
  journal={arXiv preprint arXiv:2205.03914}
}

@article{sadiev2022federated,
  title={Federated Optimization Algorithms with Random Reshuffling and Gradient Compression},
  author={Sadiev, Abdurakhmon and Malinovsky, Grigory and Gorbunov, Eduard and Sokolov, Igor and Khaled, Ahmed and Burlachenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2206.07021},
  year={2022}
}


@article{malinovsky2021random,
  title={Random reshuffling with variance reduction: New analysis and better rates},
  author={Malinovsky, Grigory and Sailanbayev, Alibek and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2104.09342},
  year={2021}
}

@inproceedings{mishchenko2022proximal,
  title={Proximal and federated random reshuffling},
  author={Mishchenko, Konstantin and Khaled, Ahmed and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={15718--15749},
  year={2022},
  organization={PMLR}
}

@inproceedings{NEURIPS2020_c8cc6e90,
 author = {Mishchenko, Konstantin and Khaled, Ahmed and Richtarik, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17309--17320},
 publisher = {Curran Associates, Inc.},
 title = {Random Reshuffling: Simple Analysis with Vast Improvements},
 url = {https://proceedings.neurips.cc/paper/2020/file/c8cc6e90ccbff44c9cee23611711cdc4-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{cannelli2020asynchronous,
  title={Asynchronous parallel algorithms for nonconvex optimization},
  author={Cannelli, Loris and Facchinei, Francisco and Kungurtsev, Vyacheslav and Scutari, Gesualdo},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={121--154},
  year={2020},
  publisher={Springer}
}

@phdthesis{secciaPhD2020,
  author =       {Ruggiero Seccia},
  title =        {Block Coordinate Incremental Methods for training Deep Neural Networks},
  school =       {Sapienza, University of Rome},
  year =         {2020}
}



@article{malinovsky2022server,
  title={Server-Side Stepsizes and Sampling Without Replacement Provably Help in Federated Optimization},
  author={Malinovsky, Grigory and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2201.11066},
  year={2022}
}

@article{lei2019stochastic,
  title={Stochastic gradient descent for nonconvex learning without bounded gradient assumptions},
  author={Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={10},
  pages={4394--4400},
  year={2019},
  publisher={IEEE}
}

@CONFERENCE{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016},
  note =         {ICLR 2017}
  }


@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@inproceedings{gorbunov2020unified,
  title={A unified theory of SGD: Variance reduction, sampling, quantization and coordinate descent},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={680--690},
  year={2020},
  organization={PMLR}
}

@article{gower2021stochastic,
  title={Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching},
  author={Gower, Robert M and Richt{\'a}rik, Peter and Bach, Francis},
  journal={Mathematical Programming},
  volume={188},
  number={1},
  pages={135--192},
  year={2021},
  publisher={Springer}
}

@article{nguyen2022finite,
  title={Finite-sum smooth optimization with SARAH},
  author={Nguyen, Lam M and van Dijk, Marten and Phan, Dzung T and Nguyen, Phuong Ha and Weng, Tsui-Wei and Kalagnanam, Jayant R},
  journal={Computational Optimization and Applications},
  pages={1--33},
  year={2022},
  publisher={Springer}
}

@article{sun2020optimization,
  title={Optimization for deep learning: An overview},
  author={Sun, Ruo-Yu},
  journal={Journal of the Operations Research Society of China},
  volume={8},
  number={2},
  pages={249--294},
  year={2020},
  publisher={Springer}
}


@article{levy2017online,
  title={Online to offline conversions, universality and adaptive minibatch sizes},
  author={Levy, Kfir},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{nesterov1998introductory,
  title={Introductory lectures on convex programming volume i: Basic course},
  author={Nesterov, Yurii},
  journal={Lecture notes},
  volume={3},
  number={4},
  pages={5},
  year={1998}
}

@article{ward2020adagrad,
  title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={9047--9076},
  year={2020},
  publisher={JMLRORG}
}


@article{shamir2016without,
  title={Without-replacement sampling for stochastic gradient methods},
  author={Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{li2021convergence,
  title={Convergence of random reshuffling under the {K}urdyka-{$\L$}ojasiewicz inequality},
  author={Li, Xiao and Milzarek, Andre and Qiu, Junwen},
  journal={arXiv preprint arXiv:2110.04926},
  year={2021}
}



@inproceedings{sankararaman2020impact,
  title={The impact of neural network overparameterization on gradient confusion and stochastic gradient descent},
  author={Sankararaman, Karthik Abinav and De, Soham and Xu, Zheng and Huang, W Ronny and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={8469--8479},
  year={2020},
  organization={PMLR}
}

@article{khaled2020better,
  title={Better theory for {SGD} in the nonconvex world},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.03329},
  year={2020}
}

@inproceedings{gower2021sgd,
  title={{SGD} for structured nonconvex functions: Learning rates, minibatching and interpolation},
  author={Gower, Robert and Sebbouh, Othmane and Loizou, Nicolas},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1315--1323},
  year={2021},
  organization={PMLR}
}

@article{patel2021stochastic,
  title={Stochastic Gradient Descent on Nonconvex Functions with General Noise Models},
  author={Patel, Vivak and Zhang, Shushu},
  journal={arXiv preprint arXiv:2104.00423},
  year={2021}
}

@article{duchi2018stochastic,
  title={Stochastic methods for composite and weakly convex optimization problems},
  author={Duchi, John C and Ruan, Feng},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={4},
  pages={3229--3259},
  year={2018},
  publisher={SIAM}
}

@article{asi2019stochastic,
  title={Stochastic (approximate) proximal point methods: Convergence, optimality, and adaptivity},
  author={Asi, Hilal and Duchi, John C},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={3},
  pages={2257--2290},
  year={2019},
  publisher={SIAM}
}

@article{gurbuzbalaban2015globally,
  title={A globally convergent incremental {N}ewton method},
  author={G{\"u}rb{\"u}zbalaban, Mert and Ozdaglar, Asuman and Parrilo, Pablo},
  journal={Mathematical Programming},
  volume={151},
  number={1},
  pages={283--313},
  year={2015},
  publisher={Springer}
}

@article{chen2018lag,
  title={LAG: Lazily aggregated gradient for communication-efficient distributed learning},
  author={Chen, Tianyi and Giannakis, Georgios and Sun, Tao and Yin, Wotao},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@article{mayne1981solving,
  title={Solving nonlinear inequalities in a finite number of iterations},
  author={Mayne, David Q and Polak, Elijah and Heunis, AJ},
  journal={Journal of Optimization Theory and Applications},
  volume={33},
  number={2},
  pages={207--221},
  year={1981},
  publisher={Springer}
}

@article{zhang2010nonmonotone,
  title={A nonmonotone smoothing-type algorithm for solving a system of equalities and inequalities},
  author={Zhang, Ying and Huang, Zheng-Hai},
  journal={Journal of Computational and Applied Mathematics},
  volume={233},
  number={9},
  pages={2312--2321},
  year={2010},
  publisher={Elsevier}
}

@article{more2009benchmarking,
  title={Benchmarking derivative-free optimization algorithms},
  author={Mor{\'e}, Jorge J. and Wild, Stefan M.},
  journal={SIAM Journal on Optimization},
  volume={20},
  number={1},
  pages={172--191},
  year={2009},
  publisher={SIAM}
}

@article{palagisecciaSOCO2021,
  title={On the convergence of a Block-Coordinate Incremental Gradient method},
  author={Palagi, Laura and Seccia, Ruggiero},
  journal={Soft Computing},
  pages={1--12},
  year={2021},
  publisher={Springer}
}

@misc{SKDkaggle,
  title = {{CIFAR 10 Pytorch} },
  howpublished = {\url{https://github.com/kuangliu/pytorch-cifar/tree/master}}
}




@article{van2001art,
  title={The art of data augmentation},
  author={Van Dyk, David A and Meng, Xiao-Li},
  journal={Journal of Computational and Graphical Statistics},
  volume={10},
  number={1},
  pages={1--50},
  year={2001},
  publisher={Taylor \& Francis}
}

@incollection{lemarechal1981view,
  title={A view of line-searches},
  author={Lemar{\'e}chal, Claude},
  booktitle={Optimization and Optimal Control},
  pages={59--78},
  year={1981},
  publisher={Springer}
}

@article{grippo1988global,
  title={Global convergence and stabilization of unconstrained minimization methods without derivatives},
  author={Grippo, Luigi and Lampariello, Francesco and Lucidi, Stefano},
  journal={Journal of Optimization Theory and Applications},
  volume={56},
  number={3},
  pages={385--406},
  year={1988},
  publisher={Springer}
}

@article{grippo1986nonmonotone,
  title={A nonmonotone line search technique for {N}ewton’s method},
  author={Grippo, Luigi and Lampariello, Francesco and Lucidi, Stephano},
  journal={SIAM journal on Numerical Analysis},
  volume={23},
  number={4},
  pages={707--716},
  year={1986},
  publisher={SIAM}
}

@incollection{chamberlain1982watchdog,
  title={The watchdog technique for forcing convergence in algorithms for constrained optimization},
  author={Chamberlain, RM and Powell, MJD and Lemarechal, Claude and Pedersen, HC},
  booktitle={Algorithms for Constrained Minimization of Smooth Nonlinear Functions},
  pages={1--17},
  year={1982},
  publisher={Springer}
}

@article{gurbuzbalaban2021random,
  title={Why random reshuffling beats stochastic gradient descent},
  author={G{\"u}rb{\"u}zbalaban, Mert and Ozdaglar, Asu and Parrilo, Pablo A},
  journal={Mathematical Programming},
  volume={186},
  number={1},
  pages={49--84},
  year={2021},
  publisher={Springer}
}


@article{keel,
  title={Keel data-mining software tool: data set repository, integration of algorithms and experimental analysis framework.},
  author={Alcal{\'a}-Fdez, Jes{\'u}s and Fern{\'a}ndez, Alberto and Luengo, Juli{\'a}n and Derrac, Joaqu{\'\i}n and Garc{\'\i}a, Salvador and S{\'a}nchez, Luciano and Herrera, Francisco},
  journal={Journal of Multiple-Valued Logic \& Soft Computing},
  volume={17},
  year={2011},
  publisher={Citeseer}
}


@misc{sido,
  author={Causality Challenge #1: Causation and Prediction},
  title={\url{http://www.causality.inf.ethz.ch/data/SIDO.html}} ,
  year={2008}
}





@misc{UCI,
  title={UCI machine learning repository},
  author={Asuncion, Arthur and Newman, David},
  year={2007}
}

@book{numpy,
  title={A guide to NumPy},
  author={Oliphant, Travis E},
  volume={1},
  year={2006},
  publisher={Trelgol Publishing USA}
}

@article{kolda2003optimization,
  title={Optimization by direct search: New perspectives on some classical and modern methods},
  author={Kolda, Tamara G and Lewis, Robert Michael and Torczon, Virginia},
  journal={SIAM review},
  volume={45},
  number={3},
  pages={385--482},
  year={2003},
  publisher={SIAM}
}
@article{byrd2012sample,
  title={Sample size selection in optimization methods for machine learning},
  author={Byrd, Richard H and Chin, Gillian M and Nocedal, Jorge and Wu, Yuchen},
  journal={Mathematical programming},
  volume={134},
  number={1},
  pages={127--155},
  year={2012},
  publisher={Springer}
}

@article{friedlander2012hybrid,
  title={Hybrid deterministic-stochastic methods for data fitting},
  author={Friedlander, Michael P and Schmidt, Mark},
  journal={SIAM Journal on Scientific Computing},
  volume={34},
  number={3},
  pages={A1380--A1405},
  year={2012},
  publisher={SIAM}
}

@inproceedings{de2017automated,
  title={Automated inference with adaptive batches},
  author={De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  booktitle={Artificial Intelligence and Statistics},
  pages={1504--1513},
  year={2017}
}

@article{bollapragada2018adaptive,
  title={Adaptive sampling strategies for stochastic optimization},
  author={Bollapragada, Raghu and Byrd, Richard and Nocedal, Jorge},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={4},
  pages={3312--3343},
  year={2018},
  publisher={SIAM}
}

@article{lucidi2002global,
  title={On the global convergence of derivative-free methods for unconstrained optimization},
  author={Lucidi, Stefano and Sciandrone, Marco},
  journal={SIAM Journal on Optimization},
  volume={13},
  number={1},
  pages={97--116},
  year={2002},
  publisher={SIAM}
}

@article{grippo2007,
  title={Nonmonotone derivative-free methods for nonlinear equations},
  author={Grippo, Luigi and Sciandrone, Marco},
  journal={Computational Optimization and applications},
  volume={37},
  number={},
  pages={297--328},
  year={2007},
  publisher={Springer}
}

@article{lucidi2002derivative,
  title={A derivative-free algorithm for bound constrained optimization},
  author={Lucidi, Stefano and Sciandrone, Marco},
  journal={Computational Optimization and applications},
  volume={21},
  number={2},
  pages={119--142},
  year={2002},
  publisher={Springer}
}

@book{shawe2004kernel,
  title={Kernel methods for pattern analysis},
  author={Shawe-Taylor, John and Cristianini, Nello and others},
  year={2004},
  publisher={Cambridge university press}
}
@article{collins2002logistic,
  title={Logistic regression, AdaBoost and Bregman distances},
  author={Collins, Michael and Schapire, Robert E and Singer, Yoram},
  journal={Machine Learning},
  volume={48},
  number={1-3},
  pages={253--285},
  year={2002},
  publisher={Springer}
}
@article{zhi1994analysis,
  title={Analysis of an approximate gradient projection method with applications to the backpropagation algorithm},
  author={Zhi-Quan, Luo and Paul, Tseng},
  journal={Optimization Methods and Software},
  volume={4},
  number={2},
  pages={85--101},
  year={1994},
  publisher={Taylor \& Francis}
}

@book{bertsekas1989parallel,
  title={Parallel and distributed computation: numerical methods},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  volume={23},
  year={1989},
  publisher={Prentice hall Englewood Cliffs, NJ}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}
@article{powell1973search,
  title={On search directions for minimization algorithms},
  author={Powell, Michael JD},
  journal={Mathematical programming},
  volume={4},
  number={1},
  pages={193--201},
  year={1973},
  publisher={Springer}
}

@book{dennis1996numerical,
  title={Numerical methods for unconstrained optimization and nonlinear equations},
  author={Dennis Jr, John E and Schnabel, Robert B},
  year={1996},
  publisher={SIAM}
}
@article{hestenes1952methods,
  title={Methods of conjugate gradients for solving linear systems},
  author={Hestenes, Magnus R and Stiefel, Eduard and others},
  journal={Journal of research of the National Bureau of Standards},
  volume={49},
  number={6},
  pages={409--436},
  year={1952}
}
@inproceedings{penrose1955generalized,
  title={A generalized inverse for matrices},
  author={Penrose, Roger},
  booktitle={Mathematical proceedings of the Cambridge philosophical society},
  volume={51},
  number={3},
  pages={406--413},
  year={1955},
  organization={Cambridge University Press}
}

@article{moore1920reciprocal,
  title={On the reciprocal of the general algebraic matrix},
  author={Moore, Eliakim H},
  journal={Bull. Am. Math. Soc.},
  volume={26},
  pages={394--395},
  year={1920}
}

@article{nedic2001incremental,
  title={Incremental subgradient methods for nondifferentiable optimization},
  author={Nedic, Angelia and Bertsekas, Dimitri P},
  journal={SIAM Journal on Optimization},
  volume={12},
  number={1},
  pages={109--138},
  year={2001},
  publisher={SIAM}
}

@incollection{nedic2001convergence,
  title={Convergence rate of incremental subgradient algorithms},
  author={Nedi{\'c}, Angelia and Bertsekas, Dimitri},
  booktitle={Stochastic optimization: algorithms and applications},
  pages={223--264},
  year={2001},
  publisher={Springer}
}

@article{gurbuzbalaban2017convergence,
  title={On the convergence rate of incremental aggregated gradient algorithms},
  author={Gurbuzbalaban, Mert and Ozdaglar, Asuman and Parrilo, Pablo A},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={2},
  pages={1035--1048},
  year={2017},
  publisher={SIAM}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@inproceedings{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
  author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
  booktitle={Advances in neural information processing systems},
  pages={2663--2671},
  year={2012}
}

@article{tseng2014incrementally,
  title={Incrementally updated gradient methods for constrained and regularized optimization},
  author={Tseng, Paul and Yun, Sangwoon},
  journal={Journal of Optimization Theory and Applications},
  volume={160},
  number={3},
  pages={832--853},
  year={2014},
  publisher={Springer}
}

@article{blatt2007convergent,
  title={A convergent incremental gradient method with a constant step size},
  author={Blatt, Doron and Hero, Alfred O and Gauchman, Hillel},
  journal={SIAM Journal on Optimization},
  volume={18},
  number={1},
  pages={29--51},
  year={2007},
  publisher={SIAM}
}

@article{tseng1998incremental,
  title={An incremental gradient (-projection) method with momentum term and adaptive stepsize rule},
  author={Tseng, Paul},
  journal={SIAM Journal on Optimization},
  volume={8},
  number={2},
  pages={506--531},
  year={1998},
  publisher={SIAM}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@article{kohonen1974adaptive,
  title={An adaptive associative memory principle},
  author={Kohonen, Teuvo},
  journal={IEEE Transactions on Computers},
  volume={100},
  number={4},
  pages={444--445},
  year={1974},
  publisher={IEEE}
}

@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@incollection{grippo2023basic,
  title={Basic Concepts on Optimization Algorithms},
  author={Grippo, Luigi and Sciandrone, Marco},
  booktitle={Introduction to Methods for Nonlinear Optimization},
  pages={167--175},
  year={2023},
  publisher={Springer}
}

@article{lee2017big,
  title={Big data: Dimensions, evolution, impacts, and challenges},
  author={Lee, In},
  journal={Business horizons},
  volume={60},
  number={3},
  pages={293--303},
  year={2017},
  publisher={Elsevier}
}

@article{croitoru2023diffusion,
  title={Diffusion models in vision: A survey},
  author={Croitoru, Florinel-Alin and Hondru, Vlad and Ionescu, Radu Tudor and Shah, Mubarak},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{polyak1987introduction,
  title={Introduction to optimization. optimization software},
  author={Polyak, Boris T},
  journal={Inc., Publications Division, New York},
  volume={1},
  year={1987}
}

@article{solodov1998incremental,
  title={Incremental gradient algorithms with stepsizes bounded away from zero},
  author={Solodov, Mikhail V},
  journal={Computational Optimization and Applications},
  volume={11},
  number={1},
  pages={23--35},
  year={1998},
  publisher={Springer}
}
@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{bertsekas2011incremental,
  title={Incremental gradient, subgradient, and proximal methods for convex optimization: A survey},
  author={Bertsekas, Dimitri P},
  journal={Optimization for Machine Learning},
  volume={2010},
  number={1-38},
  pages={3},
  year={2011},
  publisher={MIT press}
}
@article{lu2015complexity,
  title={On the complexity analysis of randomized block-coordinate descent methods},
  author={Lu, Zhaosong and Xiao, Lin},
  journal={Mathematical Programming},
  volume={152},
  number={1-2},
  pages={615--642},
  year={2015},
  publisher={Springer}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013}
}

@article{mikolov2012statistical,
  title={Statistical language models based on neural networks},
  author={Mikolov, Tom{\'a}{\v{s}} and others},
  journal={Presentation at Google, Mountain View, 2nd April},
  volume={80},
  pages={26},
  year={2012}
}

@inproceedings{dauphin2014identifying,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2933--2941},
  year={2014}
}

@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}

@inproceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6389--6399},
  year={2018}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{swirszcz2016local,
  title={Local minima in training of neural networks},
  author={Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  journal={arXiv preprint arXiv:1611.06310},
  year={2016}
}

@inproceedings{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in neural information processing systems},
  pages={6158--6169},
  year={2019}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}

@article{brady1989back,
  title={Back propagation fails to separate where perceptrons succeed},
  author={Brady, Martin L and Raghavan, Raghu and Slawny, Joseph},
  journal={IEEE Transactions on Circuits and Systems},
  volume={36},
  number={5},
  pages={665--674},
  year={1989},
  publisher={IEEE}
}

@article{sontag1989backpropagation,
  title={Backpropagation can give rise to spurious local minima even for networks without hidden layers},
  author={Sontag, Eduardo D and Sussmann, H{\'e}ctor J},
  journal={Complex Systems},
  volume={3},
  number={1},
  pages={91--106},
  year={1989}
}

@article{gori1992problem,
  title={On the problem of local minima in backpropagation},
  author={Gori, Marco and Tesi, Alberto},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={14},
  number={1},
  pages={76--86},
  year={1992}
}

@misc{Bigdatastats,
  title = {77+ Big Data Stats for the Big Future Ahead | Updated 2020},
  editor = {Hosting tribunal},
  howpublished = {https://hostingtribunal.com/blog/big-data-stats/\#gref},
  year = {2020}
}

@misc{Forbes15,
  title = {Big Data: 20 Mind-Boggling Facts Everyone Must Read},
  author = {Bernard Marr},
  editor = {Forbes},
  howpublished = {https://www.forbes.com/sites/bernardmarr/2015/09/30/big-data-20-mind-boggling-facts-everyone-must-read/\#7f48bd617b1e},
  year = {2015}
}



% ========================================================================================
% Chapter 2
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}




% ========================================================================================


@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, Yurii E},
  booktitle={Dokl. akad. nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}

@inproceedings{vanish,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={url{http://www.deeplearningbook.org}},
    year={2016}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  url={https://keras.io}},
}

@Misc{scipy,
  author =    {Eric Jones and Travis Oliphant and Pearu Peterson and others},
  title =     {{SciPy}: Open source scientific tools for {Python}},
  year =      {2001--},
  url = "http:\/\/www.scipy.org/",
  note = {[Online; accessed <today>]}
}

@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@article{bravi2014incremental,
  title={An incremental decomposition method for unconstrained optimization},
  author={Bravi, Luca and Sciandrone, Marco},
  journal={Applied Mathematics and Computation},
  volume={235},
  pages={80--86},
  year={2014},
  publisher={Elsevier}
}

@Article{Huang2011,
author="Huang, Guang-Bin
and Wang, Dian Hui
and Lan, Yuan",
title="Extreme learning machines: a survey",
journal="International Journal of Machine Learning and Cybernetics",
year="2011",
month="Jun",
day="01",
volume="2",
number="2",
pages="107--122",
doi="10.1007/s13042-011-0019-y",
}


@inproceedings{Zhao2014,
 author = {Zhao, Tuo and Yu, Mo and Wang, Yiming and Arora, Raman and Liu, Han},
 title = {Accelerated Mini-batch Randomized Block Coordinate Descent Method},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {3329--3337},
 numpages = {9},
 url = {http:\/\/dl.acm.org\/citation.cfm?id=2969033.2969198},
 acmid = {2969198},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@incollection{Zhang2017,
title = {Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks},
author = {Zhang, Ziming and Brand, Matthew},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {1721--1730},
year = {2017},
url = {http://papers.nips.cc/paper/6769-convergent-block-coordinate-descent-for-training-tikhonov-regularized-deep-neural-networks.pdf}
}


@article{Yu2017a,
author    = {Yu, Adams Wei and
			 Huang, Lei and 
			 Lin, Qihang and
			 Salakhutdinov, Ruslan and
			 Carbonell, Jaime},
  title     = {Normalized Gradient with Adaptive Stepsize Method for Deep Neural
               Network Training},
  journal   = {CoRR},
  volume    = {abs/1707.04822},
  year      = {2017}
}

@article{Wu2008,
    author = {Wu, Tong T. and Lange, Kenneth},
    citeulike-article-id = {9641309},
    journal = {The Annals of Applied Statistics},
    keywords = {cj-bib},
    number = {1},
    pages = {224--244},
    posted-at = {2011-08-11 01:40:39},
    priority = {2},
    title = {{Coordinate Descent Algorithms for Lasso Penalized Regression}},
    volume = {2},
    year = {2008}
}

@incollection{fisher1992statistical,
  title={Statistical methods for research workers},
  author={Fisher, Ronald Aylmer},
  booktitle={Breakthroughs in statistics},
  pages={66--70},
  year={1992},
  publisher={Springer}
}

@article{Wright2015,
    title = {{Coordinate descent algorithms}},
    year = {2015},
    journal = {Mathematical Programming},
    author = {Wright, Stephen J.},
    number = {1},
    pages = {3--34},
    volume = {151},
    publisher = {Springer Berlin Heidelberg},
    url = {http://dx.doi.org/10.1007/s10107-015-0892-3},
    isbn = {0025-5610},
    doi = {10.1007/s10107-015-0892-3},
    issn = {14364646},
    pmid = {22081779},
    arxivId = {1502.04759},
    keywords = {Coordinate descent, Parallel numerical computing, Randomized algorithms}
}

@article{Duchi:2011:ASM:1953048.2021068,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
 journal = {J. Mach. Learn. Res.},
 issue_date = {2/1/2011},
 volume = {12},
 month = jul,
 year = {2011},
 issn = {1532-4435},
 pages = {2121--2159},
 numpages = {39},
 url = {http:\/\/dl.acm.org\/citation.cfm?id=1953048.2021068},
 acmid = {2021068},
 publisher = {JMLR.org},
} 


@article{Konur2013,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980}
}

@article{Tang2015,
    title = {{Compressed-Domain Ship Detection on Spaceborne Optical Image Using Deep Neural Network and Extreme Learning Machine}},
    year = {2015},
    journal = {IEEE Transactions on Geoscience and Remote Sensing},
    author = {Tang, J X and Deng, C W and Huang, G B and Zhao, B J},
    pages = {1--12},
    isbn = {0196-2892},
    doi = {10.1109/tgrs.2014.2335751},
    issn = {0196-2892},
}


@article{Robbins&Monro:1951,
  added-at = {2008-10-07T16:03:39.000+0200},
  author = {Robbins, H. and Monro, S.},
  biburl = {https://www.bibsonomy.org/bibtex/2cc1b9aa8927ac4952e93f34094a3eaaf/brefeld},
  interhash = {93d54534a08c30eda9e34d1def03ffa3},
  intrahash = {cc1b9aa8927ac4952e93f34094a3eaaf},
  journal = {Annals of Mathematical Statistics},
  keywords = {imported},
  pages = {400-407},
  timestamp = {2008-10-07T16:03:40.000+0200},
  title = {A stochastic approximation method},
  volume = 22,
  year = 1951
}



@article{BCD_for_LASSO,
  title={Efficient block-coordinate descent algorithms for the group lasso},
  author={Qin, Zhiwei and Scheinberg, Katya and Goldfarb, Donald},
  journal={Mathematical Programming Computation},
  volume={5},
  number={2},
  pages={143--169},
  year={2013},
  publisher={Springer}
}

@MISC{Penrose54,
    author = {Penrose, R. },
    title = {[ 406] A GENERALIZED INVERSE FOR MATRICES},
    year = {1954}
}

@Book{NoceWrig06,
  Title = {Numerical Optimization},
  Author = {Jorge Nocedal and Stephen J. Wright},
  Publisher = {Springer},
  Year  = {2006},

  Address = {New York, NY, USA},
  Edition = {second}
}


@Book{bertsekasbook2016,
  Title = {Nonlinear Programming},
  Author = {Bertsekas, Dimitri P},
  Publisher = {Scientific, Athena},
  Year  = {2016},
  Edition = {third edition}
}


@article{Nesterov2017,
  author    = {Yurii Nesterov},
  title     = {Efficiency of Coordinate Descent Methods on Huge-Scale Optimization
               Problems},
  journal   = {{SIAM} Journal on Optimization},
  volume    = {22},
  number    = {2},
  pages     = {341--362},
  year      = {2012}
}

@article{Nakamura2017,
  author    = {Kensuke Nakamura and
               Stefano Soatto and
               Byung{-}Woo Hong},
  title     = {Block-Cyclic Stochastic Coordinate Descent for Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1711.07190},
  year      = {2017}
}

@misc{
  lau2018a,
  title={A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training},
  author={Tim Tsz-Kit Lau and Jinshan Zeng and Baoyuan Wu and Yuan Yao},
  year={2018},
  url={https://openreview.net/forum?id=HycIjFkPM}
}


@inproceedings{NIPS2013_4937,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@Inproceedings{Han2014,
author = {Han, Kun and Yu, Dong and Tashev, Ivan},
title = {Speech Emotion Recognition Using Deep Neural Network and Extreme Learning Machine},
year = {2014},
month = {September},
url = {https://www.microsoft.com/en-us/research/publication/speech-emotion-recognition-using-deep-neural-network-and-extreme-learning-machine/},
}

@article{grippo2015class,
  title={A class of derivative-free nonmonotone optimization algorithms employing coordinate rotations and gradient approximations},
  author={Grippo, Luigi and Rinaldi, F},
  journal={Computational Optimization and Applications},
  volume={60},
  number={1},
  pages={1--33},
  year={2015},
  publisher={Springer}
}

@article{grippof1999globally,
  title={Globally convergent block-coordinate techniques for unconstrained optimization},
  author={Grippo, Luigi and Sciandrone, Marco},
  journal={Optimization methods and software},
  volume={10},
  number={4},
  pages={587--637},
  year={1999},
  publisher={Taylor \& Francis}
}

@article{Grippo2016,
    title = {{Decomposition Techniques for Multilayer Perceptron Training}},
    year = {2016},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    author = {Grippo, Luigi and Manno, Andrea and Sciandrone, Marco},
    number = {11},
    pages = {2146--2159},
    volume = {27},
    doi = {10.1109/TNNLS.2015.2475621},
}



@article{Ding2015,
    title = {{Deep Extreme Learning Machine and Its Application in EEG Classification}},
    year = {2015},
    journal = {Mathematical Problems in Engineering},
    author = {Ding, Shifei and Zhang, Nan and Xu, Xinzheng and Guo, Lili and Zhang, Jian},
    volume = {2015},
    isbn = {1388-2457 (Print){\textbackslash}r1388-2457 (Linking)},
    doi = {10.1155/2015/129021},
    issn = {15635147},
    pmid = {11275538}
}

@inproceedings{NIPS2014_5258,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}

@inproceedings{saddle,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2933--2941},
  year={2014}
}

@InProceedings{chauhan17a,
  title = 	 {Mini-batch Block-coordinate based Stochastic Average Adjusted Gradient Methods to Solve Big Data Problems},
  author = 	 {Vinod Kumar Chauhan and Kalpana Dahiya and Anuj Sharma},
  booktitle = 	 {Proceedings of the Ninth Asian Conference on Machine Learning},
  pages = 	 {49--64},
  year = 	 {2017},
  volume = 	 {77},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {15--17 Nov},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v77/chauhan17a/chauhan17a.pdf},
  url = 	 {http://proceedings.mlr.press/v77/chauhan17a.html}
  }

@InProceedings{Carreira-Perpinan2012,
  title = 	 {{Distributed optimization of deeply nested systems}},
  author = 	 {Carreira-Perpinan , Miguel and Wang, Weiran },
  booktitle = 	 {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {10--19},
  year = 	 {2014},
  volume = 	 {33},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Reykjavik, Iceland},
  month = 	 {22--25 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v33/carreira-perpinan14.pdf},
  url = 	 {http://proceedings.mlr.press/v33/carreira-perpinan14.html},
}


@article{Buzzi2001,
  title={Convergent decomposition techniques for training RBF neural networks},
  author={Buzzi, C. and Grippo, L. and Sciandrone, M.},
  journal={Neural Computation},
  volume={13},
  number={8},
  pages={1891--1920},
  year={2001},
  publisher={MITP}
}

@article{Nocedal,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  journal={SIAM Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@INPROCEEDINGS{Bottou10large-scalemachine,
    author = {Bottou, L{\'e}on },
    title = {Large-scale machine learning with stochastic gradient descent},
    booktitle = {in COMPSTAT},
    year = {2010}
}

@MISC{Huang2006,
    author = {Guang-bin, Huang and Qin-yu, Zhu and Chee-kheong, Siew},
    title = { Extreme learning machine: Theory and applications},
    year = {2006}
}

@article{bertsekas2018nonlinear,
  title={Nonlinear programming},
  author={Bertsekas, Dimitri P},
  journal={Journal of the Operational Research Society},
  volume={48},
  number={3},
  pages={334--334},
  year={1997},
  publisher={Taylor \& Francis}
}

@article{Bertsekas2000,
    title = {{Gradient Convergence in Gradient methods with Errors}},
    year = {2000},
    journal = {SIAM Journal on Optimization},
    author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
    number = {3},
    pages = {627--642},
    volume = {10},
    url = {http://epubs.siam.org/doi/10.1137/S1052623497331063},
    isbn = {3642027245},
    doi = {10.1137/S1052623497331063},
    issn = {1052-6234},
    keywords = {gra-, gradient methods, incremental gradient methods, stochastic approximation}
}


@article{DBLP:journals/corr/Bertsekas15c,
  author    = {Bertsekas, Dimitri P.},
  title     = {Incremental Gradient, Subgradient, and Proximal Methods for Convex
               Optimization: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/1507.01030},
  year      = {2015},
  url       = {http://arxiv.org/abs/1507.01030},
  archivePrefix = {arXiv},
  eprint    = {1507.01030},
  timestamp = {Mon, 13 Aug 2018 16:47:43 +0200},
}

@article{Bertsekas:1996:ILS:588880.588920,
 author = {Bertsekas, Dimitri P.},
 title = {Incremental Least Squares Methods and the Extended Kalman Filter},
 journal = {SIAM J. on Optimization},
 issue_date = {1996},
 volume = {6},
 number = {3},
 month = mar,
 year = {1996},
 issn = {1052-6234},
 pages = {807--822},
 numpages = {16},
 url = {http://dx.doi.org/10.1137/S1052623494268522},
 doi = {10.1137/S1052623494268522},
 acmid = {588920},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {Kalman filter, least squares, optimization},
} 



@article{Beck2013,
  author = {Beck, Amir and Tetruashvili, Luba},
  biburl = {https://www.bibsonomy.org/bibtex/22cc56361f53a1d1dfb8f781a84074bfb/dblp},
  ee = {http://dx.doi.org/10.1137/120887679},
  interhash = {a38c7758926030b0f882c026c777251e},
  intrahash = {2cc56361f53a1d1dfb8f781a84074bfb},
  journal = {SIAM Journal on Optimization},
  keywords = {dblp},
  number = 4,
  pages = {2037-2060},
  timestamp = {2014-01-07T11:32:40.000+0100},
  title = {On the Convergence of Block Coordinate Descent Type Methods.},
  url = {http://dblp.uni-trier.de/db/journals/siamjo/siamjo23.html#BeckT13},
  volume = 23,
  year = 2013
}

@article{palagi,
  title={Global optimization issues in deep network regression: an overview},
  author={Palagi, Laura},
  journal={Journal of Global Optimization},
  pages={1--39},
  year={2018},
  publisher={Springer}
}

@article{WangB14a,
  title={Randomized block coordinate descent for online and stochastic optimization},
  author={Wang, Huahua and Banerjee, Arindam},
  journal={arXiv preprint arXiv:1407.0107},
  year={2014}
}

@inproceedings{zhao,
  title={Accelerated mini-batch randomized block coordinate descent method},
  author={Zhao, Tuo and Yu, Mo and Wang, Yiming and Arora, Raman and Liu, Han},
  booktitle={Advances in neural information processing systems},
  pages={3329--3337},
  year={2014}
}

@article{qian99,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier}
}

@book{leung2004handbook,
  title={Handbook of scheduling: algorithms, models, and performance analysis},
  author={Leung, Joseph YT},
  year={2004},
  publisher={CRC press}
}

@inproceedings{manno2016convergent,
  title={A convergent and fully distributable SVMs training algorithm},
  author={Manno, Andrea and Sagratella, Simone and Livi, Lorenzo},
  booktitle={2016 International Joint Conference on Neural Networks (IJCNN)},
  pages={3076--3080},
  year={2016},
  organization={IEEE}
}

@article{manno2018parallel,
  title={Parallel decomposition methods for linearly constrained problems subject to simple bound with application to the SVMs training},
  author={Manno, Andrea and Palagi, Laura and Sagratella, Simone},
  journal={Computational Optimization and Applications},
  volume={71},
  number={1},
  pages={115--145},
  year={2018},
  publisher={Springer}
}

@article{palagi2019block,
  title={Block layer decomposition schemes for training deep neural networks},
  author={Palagi, Laura and Seccia, Ruggiero},
  journal={Journal of Global Optimization},
  pages={1--28},
  year={2019},
  publisher={Springer}
}


@inproceedings{foglino2019gray,
  title={A gray-box approach for curriculum learning},
  author={Foglino, Francesco and Leonetti, Matteo and Sagratella, Simone and Seccia, Ruggiero},
  booktitle={World Congress on Global Optimization},
  pages={720--729},
  year={2019},
  organization={Springer}
}

@inproceedings{narvekar2017autonomous,
  title={Autonomous Task Sequencing for Customized Curriculum Design in Reinforcement Learning.},
  author={Narvekar, Sanmit and Sinapov, Jivko and Stone, Peter},
  booktitle={IJCAI},
  pages={2536--2542},
  year={2017}
}

@inproceedings{peng2016empirical,
  title={An empirical study of non-expert curriculum design for machine learners},
  author={Peng, Bei and MacGlashan, James and Loftin, Robert and Littman, Michael L and Roberts, David L and Taylor, Matthew E},
  booktitle={In Proceedings of the IJCAI Interactive Machine Learning Workshop},
  year={2016}
}

@inproceedings{narvekar2016source,
  title={Source task creation for curriculum learning},
  author={Narvekar, Sanmit and Sinapov, Jivko and Leonetti, Matteo and Stone, Peter},
  booktitle={Proceedings of the 2016 International Conference on Autonomous Agents \& Multiagent Systems},
  pages={566--574},
  year={2016},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@article{taylor2009transfer,
  title={Transfer learning for reinforcement learning domains: A survey},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Jul},
  pages={1633--1685},
  year={2009}
}

@software{ibm_docplex,
  author = {{IBM}},
  title= {{IBM Decision Optimization}} ,
  version = {IBM ILOG CPLEX 12.9},
  url = {http://ibmdecisionoptimization.github.io/docplex-doc/mp/refman.html},
  year         = 2019
}

@inproceedings{bengio,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  booktitle={Advances in neural information processing systems},
  pages={2546--2554},
  year={2011}
}
@incollection{rasmussen2004gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Advanced lectures on machine learning},
  pages={63--71},
  year={2004},
  publisher={Springer}
}

@inproceedings{Larochelle,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  pages={2951--2959},
  year={2012}
}

@article{Outoftheloop,
  title={Taking the human out of the loop: A review of bayesian optimization},
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and De Freitas, Nando},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={148--175},
  year={2016},
  publisher={IEEE}
}

@article{frazier2018tutorial,
  title={A tutorial on bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}

@article{hyperopt,
  title={Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures},
  author={Bergstra, James and Yamins, Daniel and Cox, David Daniel},
  year={2013},
  publisher={JMLR}
}

@misc{hyperopt2,
  title={Hyperopt: Distributed asynchronous hyperparameter optimization in Python},
  author={Bergstra, J},
  year={2013}
}
@misc{gpyopt2016,
  title =    {GPyOpt: A Bayesian Optimization framework in Python},
  howpublished = {\url{http://github.com/SheffieldML/GPyOpt}},
  year = {2016}
}

@inproceedings{foglino2019optimization,
  title={An optimization framework for task sequencing in curriculum learning},
  author={Foglino, Francesco and Christakou, Christiano Coletto and Leonetti, Matteo},
  booktitle={2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  pages={207--214},
  year={2019},
  organization={IEEE}
}

@inproceedings{svetlik2017automatic,
  title={Automatic Curriculum Graph Generation for Reinforcement Learning Agents.},
  author={Svetlik, Maxwell and Leonetti, Matteo and Sinapov, Jivko and Shah, Rishi and Walker, Nick and Stone, Peter},
  booktitle={AAAI},
  pages={2590--2596},
  year={2017}
}

@article{belotti2013mixed,
  title={Mixed-integer nonlinear optimization},
  author={P. Belotti and C. Kirches and S. Leyffer and J. Linderoth and J. Luedtke and A. Mahajan},
  journal={Acta Numer.},
  volume={22},
  pages={1--131},
  year={2013},
  publisher={Cambridge Univ Press}
}

@article{di2016direct,
  title={A {DIRECT}-type approach for derivative-free constrained global optimization},
  author={Di Pillo, Gianni and Liuzzi, Giampaolo and Lucidi, Stefano and Piccialli, Veronica and Rinaldi, Francesco},
  journal={Computational Optimization and Applications},
  volume={65},
  number={2},
  pages={361--397},
  year={2016},
  publisher={Springer}
}

@article{custodio2017methodologies,
  title={Methodologies and software for derivative-free optimization},
  author={Cust{\'o}dio, Ana Lu{\'\i}sa and Scheinberg, Katya and Nunes Vicente, Lu{\'\i}s},
  journal={Advances and Trends in Optimization with Engineering Applications},
  pages={495--506},
  year={2017},
  publisher={Society for Industrial and Applied Mathematics (SIAM)}
}

@article{boukouvala2016global,
  title={Global optimization advances in mixed-integer nonlinear programming, {MINLP}, and constrained derivative-free optimization, {CDFO}},
  author={Boukouvala, Fani and Misener, Ruth and Floudas, Christodoulos A},
  journal={European Journal of Operational Research},
  volume={252},
  number={3},
  pages={701--727},
  year={2016},
  publisher={Elsevier}
}

@article{silver2016mastering,
  title={Mastering the game of {G}o with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{leonetti2012combining,
  title={Combining local and global direct derivative-free optimization for reinforcement learning},
  author={Leonetti, Matteo and Kormushev, Petar and Sagratella, Simone},
  journal={Cybernetics and Information Technologies},
  volume={12},
  number={3},
  pages={53--65},
  year={2012},
  publisher={Versita}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Optimal promo scheduling
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Goodfellow,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{Fontes19,
author = {Fontes, Dalila and Pereira, Paulo and Fontes, Fernando},
year = {2019},
month = {04},
pages = {134-139},
title = {A Decision Support System for TV self-promotion Scheduling},
volume = {8},
journal = {International Journal of Advanced Trends in Computer Science and Engineering},
doi = {10.30534/ijatcse/2019/06822019}
}



@article{panaggio2016prediction,
  title={Prediction and Optimal Scheduling of Advertisements in Linear Television},
  author={Panaggio, Mark J and Fok, Pak-Wing and Bhatt, Ghan S and Burhoe, Simon and Capps, Michael and Edholm, Christina J and Moustaid, Fadoua El and Emerson, Tegan and Estock, Star-Lena and Gold, Nathan and others},
  journal={arXiv preprint arXiv:1608.07305},
  year={2016}
}

@article{zhang2006mathematical,
  title={Mathematical models for the television advertising allocation problem},
  author={Zhang, Xinhui},
  journal={International Journal of Operational Research},
  volume={1},
  number={3},
  pages={302--322},
  year={2006},
  publisher={Inderscience Publishers}
}

@article{reddy1998spot,
  title={SPOT: Scheduling programs optimally for television},
  author={Reddy, Srinivas K and Aronson, Jay E and Stam, Antonie},
  journal={Management Science},
  volume={44},
  number={1},
  pages={83--102},
  year={1998},
  publisher={INFORMS}
}
@article{ghassemi2013scheduling,
  title={Scheduling TV commercials using genetic algorithms},
  author={Ghassemi Tari, Farhad and Alaei, Reza},
  journal={International Journal of Production Research},
  volume={51},
  number={16},
  pages={4921--4929},
  year={2013},
  publisher={Taylor \& Francis}
}

@incollection{pereira2008decision,
  title={A decision support system for planning promotion time slots},
  author={Pereira, Paulo A and Fontes, Fernando ACC and Fontes, Dalila BMM},
  booktitle={Operations Research Proceedings 2007},
  pages={147--152},
  year={2008},
  publisher={Springer}
}

@article{dalila2019,
  title={A Decision Support System for TV self-promotion Scheduling},
  author={Dalila, B.M.M. Fontes Paulo A., Pereira and Fernando A.C.C. Fontes},
  journal={International Journal of Advanced Trends in Computer Science and Engineering},
  volume={8},
  number={2},
  pages={134--139},
  year={2019},
  doi = { 10.30534/ijatcse/2019/06822019}
}

@incollection{singh2018advertisement,
  title={Advertisement Scheduling Models in Television Media: A Review},
  author={Singh, Meenu and Pant, Millie and Kaul, Arshia and Jha, PC},
  booktitle={Soft Computing: Theories and Applications},
  pages={505--514},
  year={2018},
  publisher={Springer}
}

@article{danaher2011forecasting,
  title={Forecasting television ratings},
  author={Danaher, Peter J and Dagger, Tracey S and Smith, Michael S},
  journal={International Journal of Forecasting},
  volume={27},
  number={4},
  pages={1215--1240},
  year={2011},
  publisher={Elsevier}
}

@article{danaher2012using,
  title={Using a nested logit model to forecast television ratings},
  author={Danaher, Peter and Dagger, Tracey},
  journal={International Journal of Forecasting},
  volume={28},
  number={3},
  pages={607--622},
  year={2012},
  publisher={Elsevier}
}

@article{seshadri2015scheduling,
  title={Scheduling spots on television},
  author={Seshadri, Sridhar and Subramanian, Sriram and Souyris, Sebastian},
  year={2015},
  publisher={Citeseer}
}

@article{bollapragada2004schedulingvideotapes,
  title={Scheduling commercial videotapes in broadcast television},
  author={Bollapragada, Srinivas and Bussieck, Michael R and Mallik, Suman},
  journal={Operations Research},
  volume={52},
  number={5},
  pages={679--689},
  year={2004},
  publisher={INFORMS}
}

@article{bollapragada2004scheduling,
  title={Scheduling commercials on broadcast television},
  author={Bollapragada, Srinivas and Garbiras, Marc},
  journal={Operations Research},
  volume={52},
  number={3},
  pages={337--345},
  year={2004},
  publisher={INFORMS}
}

@article{brown1969selling,
  title={Selling television time: an optimisation problem},
  author={Brown, AR},
  journal={The Computer Journal},
  volume={12},
  number={3},
  pages={201--207},
  year={1969},
  publisher={The British Computer Society}
}

@article{velusamy2008efficient,
  title={An efficient ad recommendation system for TV programs},
  author={Velusamy, S. and Gopal, L. and Bhatnagar, S. and Varadarajan, S.},
  journal={Multimedia Systems},
  volume={14},
  number={2},
  pages={73--87},
  year={2008},
  publisher={Springer}
}

@incollection{dudek2015short,
  title={Short-term load forecasting using random forests},
  author={Dudek, G.},
  booktitle={Intelligent Systems' 2014},
  pages={821--828},
  year={2015},
  publisher={Springer}
}

@article{kane2014comparison,
  title={Comparison of ARIMA and Random Forest time series models for prediction of avian influenza H5N1 outbreaks},
  author={Kane, M. J. and Price, N. and Scotch, M. and Rabinowitz, P.},
  journal={BMC bioinformatics},
  volume={15},
  number={1},
  pages={276},
  year={2014},
  publisher={BioMed Central}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, T. and Guestrin, C.},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016},
  organization={ACM}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, L.},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{wager2014confidence,
  title={Confidence intervals for random forests: The jackknife and the infinitesimal jackknife},
  author={Wager, S. and Hastie, T. and Efron, B.},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1625--1651},
  year={2014},
  publisher={JMLR. org}
}

%%%%%%%%%%%%%%%%%%%%%%%
% Contribution in volume
@incollection{science-contrib,
  author       = {M. Broy}, 
  title        = {Software engineering --- from auxiliary to key technologies},
  booktitle    = {Software Pioneers},
  publisher    = {Springer},
  year         = 2002,
  editor       = {M. Broy and  E. Dener},
  pages        = {10-13},
  address      = {Heidelberg}
}


%
% Online Document
@misc{science-online,
  author       = {Dod, J.}, 
  title        = {Effective substances},
  howpublished = {In: The Dictionary of Substances and Their Effects. Royal Society of Chemistry. Available via DIALOG.},
  year         = 1999,
  note         = {\url{http://www.rsc.org/dose/title of subordinate document}}
}


%
% Monograph
@book{science-mono,
  author    = {Geddes, K.O. and Czapor, S.R. and Labahn, G.}, 
  title     = {Algorithms for Computer Algebra},
  publisher = {Kluwer},
  year      = 1992,
  address   = {Boston}
}

%
% Journal article
@article{science-journal,
  author  = {Hamburger, C.}, 
  title   = {Quasimonotonicity, regularity and duality for nonlinear systems of partial differential equations},
  journal = {Ann. Mat. Pura. Appl.},
  year    = 1995,
  pages   = {321-354},
  volume  = 169
}

%
% Journal article by DOI
@article{science-DOI,
  author  = {Slifka, M.K. and Whitton, J.L.}, 
  title   = {Clinical implications of dysregulated cytokine production},
  journal = {J. Mol. Med.},
  year    = 2000,
  note    = {DOI: 10.1007/s001090000086}
}

%
% Monograph
@book{phys-mono,
  author    = {H. Ibach and H. L\"uth}, 
  title     = {Solid-State Physics},
  publisher = {Springer},
  year      = 1996,
  edition   = 2,
  address   = {New York}
}

%
%
% Contribution in proceedings
\bibitem{} , in \textit{Neuromuscular Junction}, ed. by E. Zaimis. Handbook of Experimental Pharmacology, vol 42 (Springer, Heidelberg, 1976), p. 593
@inproceedings{phys-conf,
  author       = {S.E. Smith}, 
  title        = {Neuromuscolar junction --- from auxiliary to key technologies},
  booktitle    = {New Technologies for Bioengineering and Medicine},
  publisher    = {Springer},
  year         = 2002,
  editor       = {M. Broy and  E. Dener},
  organization = {MEDENG conference, New York, 22-25 June 2001},
  pages        = {10-13},
  address      = {Heidelberg}
}

%
%PhDthesis
@phdthesis{phdthesis,
  author       = {Peter Joslin}, 
  title        = {The title of the work},
  school       = {The school of the thesis},
  year         = 1993,
  address      = {The address of the publisher},
  month        = 7,
  note         = {An optional note}
}

%
%Technical report
@techreport{techreport,
  author       = {Peter Lambert}, 
  title        = {The title of the work},
  institution  = {The institution that published},
  year         = 1993,
  number       = 2,
  address      = {The address of the publisher},
  month        = 7,
  note         = {An optional note}
}

@misc{ibm-docplexcloud,
  author = {{IBM}},
  title= {{IBM Decision Optimization on Cloud}} ,
  version = {IBM ILOG CPLEX 12.9},
  url = {https://developer.ibm.com/docloud/documentation/decision-optimization-on-cloud/},
  year         = 2019
}

%  url = {https://www.ibm.com/support/producthub/icpdata/docs/}
@misc{ibm-cp4d,
  author = {{IBM}},
    title = {{IBM Cloud Pak for Data}}, 
  version = {2.5},
  url = {https://www.ibm.com/support/producthub/icpdata/docs/},
  year = 2019
}

%  url = {https://www.ibm.com/support/producthub/icpdata/docs/}
@misc{ibm-cplex,
  author = {{IBM}},
    title = {{IBM ILOG CPLEX Optimization Studio V12.9.0 documentation}}, 
  version = {12.9.0},
  url = {https://www.ibm.com/support/knowledgecenter/SSSA5P_12.9.0},
  year = 2019
}

@Inbook{Bottou2012,
author="Bottou, L{\'e}on",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Stochastic Gradient Descent Tricks",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="421--436",
abstract="Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_25",
url="https://doi.org/10.1007/978-3-642-35289-8_25"
}
