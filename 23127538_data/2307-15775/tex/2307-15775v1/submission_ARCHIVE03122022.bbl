\begin{thebibliography}{10}

\bibitem{SKDkaggle}
{\em {CIFAR 10 Pytorch}}.
\newblock \url{https://github.com/kuangliu/pytorch-cifar/tree/master}.

\bibitem{keel}
{\sc J.~Alcal{\'a}-Fdez, A.~Fern{\'a}ndez, J.~Luengo, J.~Derrac,
  S.~Garc{\'\i}a, L.~S{\'a}nchez, and F.~Herrera}, {\em Keel data-mining
  software tool: data set repository, integration of algorithms and
  experimental analysis framework.}, Journal of Multiple-Valued Logic \& Soft
  Computing, 17 (2011).

\bibitem{asi2019stochastic}
{\sc H.~Asi and J.~C. Duchi}, {\em Stochastic (approximate) proximal point
  methods: Convergence, optimality, and adaptivity}, SIAM Journal on
  Optimization, 29 (2019), pp.~2257--2290.

\bibitem{UCI}
{\sc A.~Asuncion and D.~Newman}, {\em Uci machine learning repository}, 2007.

\bibitem{bertsekas2011incremental}
{\sc D.~P. Bertsekas}, {\em Incremental gradient, subgradient, and proximal
  methods for convex optimization: A survey}, Optimization for Machine
  Learning, 2010 (2011), p.~3.

\bibitem{Bertsekas2000}
{\sc D.~P. Bertsekas and J.~N. Tsitsiklis}, {\em {Gradient Convergence in
  Gradient methods with Errors}}, SIAM Journal on Optimization, 10 (2000),
  pp.~627--642, \url{https://doi.org/10.1137/S1052623497331063},
  \url{http://epubs.siam.org/doi/10.1137/S1052623497331063}.

\bibitem{blatt2007convergent}
{\sc D.~Blatt, A.~O. Hero, and H.~Gauchman}, {\em A convergent incremental
  gradient method with a constant step size}, SIAM Journal on Optimization, 18
  (2007), pp.~29--51.

\bibitem{bollapragada2018adaptive}
{\sc R.~Bollapragada, R.~Byrd, and J.~Nocedal}, {\em Adaptive sampling
  strategies for stochastic optimization}, SIAM Journal on Optimization, 28
  (2018), pp.~3312--3343.

\bibitem{Bottou10large-scalemachine}
{\sc L.~Bottou}, {\em Large-scale machine learning with stochastic gradient
  descent}, in in COMPSTAT, 2010.

\bibitem{bottou2018optimization}
{\sc L.~Bottou, F.~E. Curtis, and J.~Nocedal}, {\em Optimization methods for
  large-scale machine learning}, Siam Review, 60 (2018), pp.~223--311.

\bibitem{sido}
{\sc C.~C.~. Causation and Prediction}, {\em
  \url{http://www.causality.inf.ethz.ch/data/SIDO.html}}, 2008.

\bibitem{chen2018lag}
{\sc T.~Chen, G.~Giannakis, T.~Sun, and W.~Yin}, {\em Lag: Lazily aggregated
  gradient for communication-efficient distributed learning}, Advances in
  Neural Information Processing Systems, 31 (2018).

\bibitem{croitoru2023diffusion}
{\sc F.-A. Croitoru, V.~Hondru, R.~T. Ionescu, and M.~Shah}, {\em Diffusion
  models in vision: A survey}, IEEE Transactions on Pattern Analysis and
  Machine Intelligence,  (2023).

\bibitem{NIPS2014_5258}
{\sc A.~Defazio, F.~Bach, and S.~Lacoste-Julien}, {\em Saga: A fast incremental
  gradient method with support for non-strongly convex composite objectives},
  in Advances in neural information processing systems, 2014, pp.~1646--1654.

\bibitem{dolan2002benchmarking}
{\sc E.~D. Dolan and J.~J. Mor{\'e}}, {\em Benchmarking optimization software
  with performance profiles}, Mathematical programming, 91 (2002),
  pp.~201--213.

\bibitem{Dozat2016IncorporatingNM}
{\sc T.~Dozat}, {\em Incorporating nesterov momentum into adam}, in ICLR
  Workshop, 2016.

\bibitem{Duchi:2011:ASM:1953048.2021068}
{\sc J.~Duchi, E.~Hazan, and Y.~Singer}, {\em Adaptive subgradient methods for
  online learning and stochastic optimization}, J. Mach. Learn. Res., 12
  (2011), pp.~2121--2159,
  \url{http:\/\/dl.acm.org\/citation.cfm?id=1953048.2021068}.

\bibitem{Duchi2010AdaptiveSM}
{\sc J.~C. Duchi, E.~Hazan, and Y.~Singer}, {\em Adaptive subgradient methods
  for online learning and stochastic optimization}, in J. Mach. Learn. Res.,
  2010.

\bibitem{duchi2018stochastic}
{\sc J.~C. Duchi and F.~Ruan}, {\em Stochastic methods for composite and weakly
  convex optimization problems}, SIAM Journal on Optimization, 28 (2018),
  pp.~3229--3259.

\bibitem{ghadimi2013stochastic}
{\sc S.~Ghadimi and G.~Lan}, {\em Stochastic first-and zeroth-order methods for
  nonconvex stochastic programming}, SIAM Journal on Optimization, 23 (2013),
  pp.~2341--2368.

\bibitem{gower2021sgd}
{\sc R.~Gower, O.~Sebbouh, and N.~Loizou}, {\em {SGD} for structured nonconvex
  functions: Learning rates, minibatching and interpolation}, in International
  Conference on Artificial Intelligence and Statistics, PMLR, 2021,
  pp.~1315--1323.

\bibitem{gower2021stochastic}
{\sc R.~M. Gower, P.~Richt{\'a}rik, and F.~Bach}, {\em Stochastic
  quasi-gradient methods: Variance reduction via jacobian sketching},
  Mathematical Programming, 188 (2021), pp.~135--192.

\bibitem{grippo2023basic}
{\sc L.~Grippo and M.~Sciandrone}, {\em Basic concepts on optimization
  algorithms}, in Introduction to Methods for Nonlinear Optimization, Springer,
  2023, pp.~167--175.

\bibitem{gurbuzbalaban2021random}
{\sc M.~G{\"u}rb{\"u}zbalaban, A.~Ozdaglar, and P.~A. Parrilo}, {\em Why random
  reshuffling beats stochastic gradient descent}, Mathematical Programming, 186
  (2021), pp.~49--84.

\bibitem{he2016deep}
{\sc K.~He, X.~Zhang, S.~Ren, and J.~Sun}, {\em Deep residual learning for
  image recognition}, in Proceedings of the IEEE conference on computer vision
  and pattern recognition, 2016, pp.~770--778.

\bibitem{khan2022transformers}
{\sc S.~Khan, M.~Naseer, M.~Hayat, S.~W. Zamir, F.~S. Khan, and M.~Shah}, {\em
  Transformers in vision: A survey}, ACM computing surveys (CSUR), 54 (2022),
  pp.~1--41.

\bibitem{Kingma2015AdamAM}
{\sc D.~P. Kingma and J.~Ba}, {\em Adam: A method for stochastic optimization},
  CoRR, abs/1412.6980 (2015).

\bibitem{lee2017big}
{\sc I.~Lee}, {\em Big data: Dimensions, evolution, impacts, and challenges},
  Business horizons, 60 (2017), pp.~293--303.

\bibitem{lei2019stochastic}
{\sc Y.~Lei, T.~Hu, G.~Li, and K.~Tang}, {\em Stochastic gradient descent for
  nonconvex learning without bounded gradient assumptions}, IEEE transactions
  on neural networks and learning systems, 31 (2019), pp.~4394--4400.

\bibitem{liuzzi2022convergence}
{\sc G.~Liuzzi, L.~Palagi, and R.~Seccia}, {\em Convergence under lipschitz
  smoothness of ease-controlled random reshuffling gradient algorithms}, arXiv
  preprint arXiv:2212.01848,  (2022).

\bibitem{mayne1981solving}
{\sc D.~Q. Mayne, E.~Polak, and A.~Heunis}, {\em Solving nonlinear inequalities
  in a finite number of iterations}, Journal of Optimization Theory and
  Applications, 33 (1981), pp.~207--221.

\bibitem{nguyen2022finite}
{\sc L.~M. Nguyen, M.~van Dijk, D.~T. Phan, P.~H. Nguyen, T.-W. Weng, and J.~R.
  Kalagnanam}, {\em Finite-sum smooth optimization with sarah}, Computational
  Optimization and Applications,  (2022), pp.~1--33.

\bibitem{robbins1951stochastic}
{\sc H.~Robbins and S.~Monro}, {\em A stochastic approximation method}, The
  annals of mathematical statistics,  (1951), pp.~400--407.

\bibitem{Robbins&Monro:1951}
{\sc H.~Robbins and S.~Monro}, {\em A stochastic approximation method}, Annals
  of Mathematical Statistics, 22 (1951), pp.~400--407.

\bibitem{ruder2016overview}
{\sc S.~Ruder}, {\em An overview of gradient descent optimization algorithms},
  arXiv preprint arXiv:1609.04747,  (2016).

\bibitem{schmidt2017minimizing}
{\sc M.~Schmidt, N.~Le~Roux, and F.~Bach}, {\em Minimizing finite sums with the
  stochastic average gradient}, Mathematical Programming, 162 (2017),
  pp.~83--112.

\bibitem{shalev2014understanding}
{\sc S.~Shalev-Shwartz and S.~Ben-David}, {\em Understanding machine learning:
  From theory to algorithms}, Cambridge university press, 2014.

\bibitem{shamir2016without}
{\sc O.~Shamir}, {\em Without-replacement sampling for stochastic gradient
  methods}, Advances in neural information processing systems, 29 (2016).

\bibitem{Sutskever2013OnTI}
{\sc I.~Sutskever, J.~Martens, G.~E. Dahl, and G.~E. Hinton}, {\em On the
  importance of initialization and momentum in deep learning}, in ICML, 2013.

\bibitem{van2001art}
{\sc D.~A. Van~Dyk and X.-L. Meng}, {\em The art of data augmentation}, Journal
  of Computational and Graphical Statistics, 10 (2001), pp.~1--50.

\bibitem{Zeiler2012ADADELTAAA}
{\sc M.~D. Zeiler}, {\em Adadelta: An adaptive learning rate method}, ArXiv,
  abs/1212.5701 (2012).

\bibitem{zhang2010nonmonotone}
{\sc Y.~Zhang and Z.-H. Huang}, {\em A nonmonotone smoothing-type algorithm for
  solving a system of equalities and inequalities}, Journal of Computational
  and Applied Mathematics, 233 (2010), pp.~2312--2321.

\end{thebibliography}
