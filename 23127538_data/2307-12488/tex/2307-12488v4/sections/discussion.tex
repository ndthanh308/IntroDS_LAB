\section{Discussion and Conclusion}
This study unveils the strengths and limitations of LLMs when applied to security-oriented code analysis tasks.
Notably, we demonstrate that ChatGPT holds promise as a valuable tool for security-oriented source code analysis, effectively learning high-level semantics.
However, it is imperative to admit certain limitations when employing ChatGPT for security-oriented program analysis.
Users and researchers should be aware that ChatGPT's performance may be hindered when analyzing complex source code with insufficient information on variables, methods, and function names.
Additionally, our focused experiments on CodeBert and GraphCodeBert shed light on the impact of literal and logic features on their performance, indicating the importance of considering the reliability of features in the analyzed code.

We firmly believe that our findings and analysis will furnish valuable insights for future researchers in this domain, facilitating a deeper understanding of LLMs' potential and limitations in software security.
As researchers venture further into the challenges we have identified, they can actively work towards maximizing the potential of ChatGPT and other LLMs in security-oriented code analysis.
By addressing these limitations and capitalizing on the model's strengths, we can pave the way for the development of more robust and accurate analysis tools, significantly contributing to enhanced software security practices.
%Our hope is that this collective effort will lead to more secure software systems, benefiting users and the broader cybersecurity community alike.