% \vspace{-0.5em}
\section{Quantitative Analysis on CodeBert}
% The previous section has explored the ChatGPT's strengths, potential, and limitation in solve some code analysis tasks.
% In this section, we shift our attention from the ChatGPT to the CodeBert. 
CodeBert and GraphCodeBERT~\cite{feng2020codebert,guo2020graphcodebert} are based on Transformer architecture and learn code representations through self-supervised training tasks, such as masked language modeling and structure-aware tasks, using a large-scale unlabeled corpus.
Specifically, CodeBERT is pre-trained over six programming languages (Python, Java, JavaScript, PHP, Ruby, Go) and is designed to accomplish three main tasks: masked language modeling, code structure edge prediction, and representation alignment.
By mastering these tasks, CodeBERT can comprehend and represent the underlying semantics of code, empowering security researchers with a powerful tool for source code analysis and protection.

Since the pre-trained CodeBert and GraphCodeBert and datasets~\cite{trainedcodebert} for the downstream tasks are publicly available for us to download and use, we are able to conduct focused experiments to evaluate them. 
Currently, GraphCodeBert takes code snippets of functions or class methods as data samples. 
It tokenized keywords, operators, and developer-defined names from the code pieces. It utilizes Transformer to learn the data flow of the source code in the pre-training stage.
Inside a function or a method, we can group the developer-defined names into three categories: 1) variable name. 2) method name. 3) method invocation name. 
Program logic is not affected if we map these developer-defined names with other strings in the same namespace.
To evaluate whether the model could learn the code semantics, we design experiments to anonymize three types of literal features: variable names, method names, and method/function invocation names.
For each group of experiments, we anonymize certain categories of developer-defined names. The last group anonymizes all three kinds of literal features.

%\begin{enumerate}
%    \item In the first group of experiments, we anonymize the variable names. %An example is the change from \texttt{it\_end} to \texttt{var3} and \texttt{finished} to \texttt{var4} between \autoref{code:goodname} and \autoref{code:badname}. 
%    \item In the second group of experiments, we anonymize the method names. %An example is the change from \texttt{bubble\_sort} to \texttt{fun1} between \autoref{code:goodname} and \autoref{code:badname}. 
%    \item In the third group of experiments, we anonymize the method/function invocation names. %An example is the change from \texttt{swap} to \texttt{fun2} between \autoref{code:goodname} and \autoref{code:badname}. 
%    \item The last group of experiments are a combination of the first three experiments, which anonymize all three kinds of developer-defined names. 
%\end{enumerate}

\begin{table}[t!]%htpb!
    \setlength{\tabcolsep}{4pt}
  \centering
  \captionsetup{justification=centering}
  \scriptsize
  \caption{Results on Code Search.}
  \label{tab:search}
  \begin{tabular}{lcccccc}
  \toprule
  {\bf Lang-} & \multirow{2}{*}{\bf Orig.}  & \multirow{2}{*}{\bf Anon.} &  {\bf w/o } & {\bf w/o }  & {\bf w/o} & \multirow{2}{*}{\bf All} \\
  {\bf uage}& & & {\bf Variable} & {\bf Method Def.} & {\bf Method Inv.} & \\
  \midrule
  \multirow{2}{0.5cm}{Java}  & \multirow{2}{0.5cm}{70.36\%} & Random  & 67.73\% & 60.89\% & 69.84\% & 17.42\% \\
  & &  Meaningful & 67.14\%	& 58.36\% &	69.84\% &	17.03\% \\
  \midrule
  \multirow{2}{0.5cm}{Python}  & \multirow{2}{0.5cm}{68.17\%}  & Random  &  59.8\% & 55.43\% & 65.61\% & 24.09\% \\
  & & Meaningful& 59.78\% &	55.65\% &	65.61\% &	23.73\%  \\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}[t!]%htpb!
  \setlength{\tabcolsep}{4pt}
  \centering
  \captionsetup{justification=centering}
  \scriptsize
  \caption{Results on Clone Detection.}
  \label{tab:clone}
  \begin{tabular}{ccccccc}
  \toprule
  {\bf Lang-} & \multirow{2}{*}{\bf Orig.}  & \multirow{2}{*}{\bf Anon.} &  {\bf w/o } & {\bf w/o }  & {\bf w/o} & \multirow{2}{*}{\bf All} \\
  {\bf uage}& & & {\bf Variable} & {\bf Method Def.} & {\bf Method Inv.} & \\
  \midrule
  \multirow{2}{0.5cm}{Java}  & \multirow{2}{0.5cm}{94.87\%} & Random & 92.64\% & 93.97\% & 94.72\% & 86.77\% \\
  & & Meaningful & 92.52\% &	94.27\% &	93.67\% &	84.76\% \\
  \bottomrule
\end{tabular}
\end{table}
Besides, we adopt two strategies to anonymize the name:
The first strategy called ``randomly-generated'' randomly generates strings (\eg, ``oe4yqk4cit2maq7t'') without any literal meaning. 
The second strategy called ``meaningfully-generated'' generates strings with a literal meaning.
However, the literal meaning does not reflect the intention of the variable/function/invocation.
For example, this strategy could replace ``bubble\_sort'' with ``aes\_encryption''.

% (replacing a name with a meaningful-string or randomly-generated-sting without any literal meaning)

Based on the four types of name sets to replace and two replacing strategies, we eventually generated 8 variants of the original dataset from \cite{guo2020graphcodebert}. 
Then, we retrain the existing models and evaluate their performance on the existing 2 downstream tasks: natural language code search, and clone detection. 
%We did not choose the other downstream tasks because code refinement adopts an abstraction representation and code translation involves two programming languages, which introduce challenges to uniformly anonymized user-defined names. 
%Since our experiments are focusing on the learning ability of pre-trained models on literal features, and other semantic features. And the downstream tasks are only used to evaluate the effectiveness of the pre-trained model. Therefore, we believe that any reasonable downstream should be fine. 


\subsection{Experiment Results}
Table~\ref{tab:search} and Table~\ref{tab:clone} show experiment results (accuracy) on the downstream task of code search and code clone detection, respectively.
The second column shows the module performance reported by the original paper~\cite{guo2020graphcodebert}.
The fourth, fifth, and sixth columns show the module performance when we anonymize the variable name, method definition name, and method invocation name, respectively.
The last column shows the model performance after we remove all three developer-defined names.





The results show that the anonymization of the variable names, method definition names, and method invocation names will result in a huge downgrade in model performance no matter if we replace developer-defined names with ``randomly-generated" strings or ``meaningfully-generated" strings.
Also, on average the dataset with ``meaningfully-generated" strings performs worse than the dataset with randomly-generated strings, which indicates that  ``meaningfully-generated" strings could mislead the models.

Overall, our experiments reflect that current source-code level representation learning methods still largely rely on the \textit{literal feature} and ignore the \textit{logic feature}.
However, the \textit{literal feature} is not always reliable as mentioned in \autoref{sec:intro}.
The current model still cannot effectively learn the logic feature in the source code.
Attackers can simply randomize the \textit{literal feature} to mislead the CodeBert and GraphCodeBERT models.
%An adversarial machine learning could be trained to further exploit the weakness of the CodeBert. %\fixme{(How? more details?)}
%\fixme{}

%The \textit{logic feature} hidden in the source code contains all the information of a piece of the program. But current models still cannot effectively learn them.  


%\subsection{Observations}
%\fixme{(remove this section?)}
%Through a set of experiments and empirical analysis, this section tries to explain the learning ability of current BERT-based source code representation learning schemes.
%The results show that CodeBERT and GraphCodeBERT are efficient to learn \textit{literal features} but less efficient to learn \textit{logic features}. 