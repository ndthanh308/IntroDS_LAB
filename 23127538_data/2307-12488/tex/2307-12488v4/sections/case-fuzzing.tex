\vspace{-0.5em}\subsection{Input Generation}
Input generation is wide used in software testing, which aim to achieve a high coverage by triggering different program pathes.
This section, we aim to answer \ding{188} whether ChatGPT can offer valuable and effective seed inputs after understanding the source code.
Specifically, we test the ChatGPT's input generatation ability to achieve code coverage, input mutations, and  exeuction simulation.

Upon analyzing the responses ({\bf \ref{appendix:Q11}, \ref{appendix:Q12}, \ref{appendix:Q13}}) from ChatGPT, we observed that the provided seeds were comparable to those generated by AFL (American Fuzzy Lop) \cite{aflrefurl} using seed mutation strategies. However, it became apparent that ChatGPT did not propose highly effective seeds (capable of triggering the vulnerability). This observation suggests that ChatGPT may face challenges in leveraging the program semantics it learned from the code to generate such effective seeds.
There could be two possible reasons for this:

\begin{enumerate}
\item The prompts we use may not have been designed to specifically encourage ChatGPT to produce the most useful and targeted solutions.
\item ChatGPT's responses might be too general, primarily relying on the information present in the corpus it was trained on, rather than effectively applying the learned program semantics to new situations.
\end{enumerate}

%Understanding these limitations is crucial for utilizing ChatGPT effectively in vulnerability discovery and other security-oriented tasks. Further research is needed to refine the prompts and explore strategies to enhance ChatGPT's ability to provide more precise and context-aware responses in security-related scenarios.

