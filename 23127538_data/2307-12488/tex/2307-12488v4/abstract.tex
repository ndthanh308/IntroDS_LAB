
\begin{abstract}
% Large Language Models (LLMs), such as GPT and BERT, have demonstrated remarkable capabilities in addressing neural language process tasks.
% %
% Recently, the release of ChatGPT has garnered significant attention due to its ability to analyze, comprehend, and synthesize information from user inputs.
% %
% Therefore, these LLMs were adopt by researchers in many different domains.
LLMs can be used on code analysis tasks like code review, vulnerabilities analysis and etc.
%
However, the strengths and limitations of adopting these LLMs to the code analysis are still unclear. 
%
% Besides, the potentials of these LLMs in solving many other security-oriented code analysis tasks are under investigation.
% 
In this paper, we delve into LLMs' capabilities in security-oriented program analysis, considering perspectives from both attackers and security analysts.
%
We focus on two representative LLMs, ChatGPT and CodeBert, and evaluate their performance in solving typical analytic tasks with varying levels of difficulty.
%
% Given the different nature of ChatGPT and CodeBERT, we conduct a qualitative analysis of the model's output for ChatGPT and a quantitative analysis for CodeBERT, respectively.
% %
% For ChatGPT, we present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess its responses.
%
% Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in security-oriented program analysis.
% % 
% On the other hand, for CodeBERT, we systematically analyze and classify the features in code, quantitatively evaluating the impact of these features on the model's performance.
%
Our study demonstrates the LLM's efficiency in learning high-level semantics from code, positioning ChatGPT as a potential asset in security-oriented contexts.
% 
However, it is essential to acknowledge certain limitations, such as the heavy reliance on well-defined variable and function names, making them unable to learn from anonymized code. 
%
For example, the performance of these LLMs heavily relies on the well-defined variable and function names, therefore, will not be able to learn anonymized code.
%
We believe that the concerns raised in this case study deserve in-depth investigation in the future.
%
% We hope that our findings and analysis will offer valuable insights for future researchers in this domain, contributing to a better understanding of LLM's potential and limitations in software security.
\end{abstract}
