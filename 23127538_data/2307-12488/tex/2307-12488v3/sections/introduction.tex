\section{Introduction}
\label{sec:intro}
Large pre-trained language models (LLMs), such as BERT~\cite{devlin2018bert} (Bidirectional Encoder Representations from Transformers) by~\citeauthor{devlin2018bert} and GPT~\cite{radford2018improving} (Generative Pre-trained Transformer) by~\citeauthor{radford2018improving}, have revolutionized the field of Natural Language Processing (NLP) with their exceptional capabilities, setting new standards in performance across various tasks.
These pre-trained models are usually first learned on a large unsupervised text corpus and then fine-tuned on different downstream tasks. 

Recently, those techniques have been deployed on the program analysis, then applied to security-related downstream tasks, such as code review and vulnerabilities discovery.
%
In 2020, \citeauthor{feng2020codebert} proposed a pre-trained model called CodeBERT~\cite{feng2020codebert} based on Bidirectional Encoder Representations from Transformers (BERT) that learns general-purpose representations to support downstream NL (Natural Language)-PL (Program Language) applications such as natural language code search and code documentation generation.
%
In 2021, a new pre-trained model called GraphCodeBERT~\cite{guo2020graphcodebert} proposed by \citeauthor{guo2020graphcodebert} improves the CodeBERT by enabling the model to capture more program semantic information (e.g. data flow).

In the meanwhile, ChatGPT~\cite{chatgptlimitation}, an artificial intelligence chatbot developed by OpenAI and launched in November 2022, has garnered significant attention for its impressive capabilities.
With the ability to respond to diverse prompts, ranging from answering questions in various domains to generating specific essays, people are truly astonished by its prowess.
Users are intrigued by ChatGPT's potential to address questions across multiple domains, and they are keen to observe how its application may evolve in different areas. 

However, while CodeBert and ChatGPT shows strengths in analyzing, comprehending, and synthesizing information, its weaknesses and potential risks remain less understood.
Users who are unaware of these limitations may be misled by the outputs.
For instance, in the case of requesting paper recommendations on a specific topic, we have encountered instances where some suggested papers turned out to be fake or unreliable.
The following example shows a conversation with ChatGPT, and papers colored in red are not existed.
\colorlet{shadecolor}{red!5}
\begin{shaded*}
\noindent{\bf Q1:} \textit{Please provide some papers that discuss the limitations of ChatGPT.} 
\end{shaded*}
\vspace{-2.5em}
\colorlet{shadecolor}{green!5}
\begin{shaded*}
\setlength{\parindent}{0em}
{\footnotesize 
  \noindent
{\bf Reply from ChatGPT:}  Certainly! Here are a few papers that discuss the limitations of language models like ChatGPT:
    \begin{enumerate}
    \item ``Language Models are Few-Shot Learners" by Tom B. Brown et al. (NeurIPS 2020)
    
    \item ``On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?" by Emily M. Bender et al. (2021)
    
    \item {\color{bittersweet} ``Bad Chatbot! Exploring the Limitations of Seq2Seq Models with the Help of Regular Expressions'' by Mateusz Fedoryszak (ACL 2018)}
 
    \item {\color{bittersweet} ``The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics'' by Mathias Müller et al. (EMNLP 2020)}
    
    \item {\color{bittersweet} ``The Unreliability of GPT-3 for Answering Science Questions'' by Vedanuj Goswami et al. (EMNLP 2021)}
    \end{enumerate}
  }
\end{shaded*} 
This example underscores the issues when using ChatGPT and the significance of comprehending both the capabilities and limitations of ChatGPT to employ it responsibly.
% \noindent{\bf Q1:} \textit{Please provide some papers that discuss the limitations of ChatGPT.}
% \begin{shaded*}
%     \footnotesize
%     \noindent{\bf Reply from ChatGPT:}  Certainly! Here are a few papers that discuss the limitations of language models like ChatGPT:

%     \begin{enumerate}
%     \item ``Language Models are Few-Shot Learners" by Tom B. Brown et al. (NeurIPS 2020)
    
%     \item ``On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?" by Emily M. Bender et al. (2021)
    
%     \item {\color{bittersweet}``Bad Chatbot! Exploring the Limitations of Seq2Seq Models with the Help of Regular Expressions" by Mateusz Fedoryszak (ACL 2018)}
 
%     \item {\color{bittersweet}``The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics" by Mathias Müller et al. (EMNLP 2020)}
    
%     \item {\color{bittersweet}``The Unreliability of GPT-3 for Answering Science Questions" by Vedanuj Goswami et al. (EMNLP 2021)}
%     \end{enumerate}
% \end{shaded*}



% Some research has been conducted to analyze the learning ability of GPT and BERT on various domains.
Recent papers have analyzed ChatGPT's strengths and failures in various domains.
\cite{azaria2022chatgpt} and \cite{frieder2023mathematical} explored ChatGPT's mathematical capabilities, while \cite{borji2023categorical} presented a categorical archive of ChatGPT failures, including reasoning, logic, math, factual errors, wit and humor, coding, syntactic structure, spelling, grammar, self-awareness, ethics, and morality. Furthermore, Microsoft conducted preliminary experiments to evaluate ChatGPT's proficiency in coding and program comprehension~\cite{bubeck2023sparks}. However, these experiments yielded limited suggestions for security analysts and attackers.
OpenAI's release of the "GPT-4 Technical Report~\cite{openai2023gpt4}" shows GPT-4's remarkable performance on various professional and academic benchmarks, demonstrating human-level proficiency in certain domains. 
% Notably, GPT-4 successfully passed a simulated bar exam with a score ranking around the top 10\% of test takers. 
However, despite these achievements, OpenAI remains cognizant of ChatGPT's limitations and challenges of solving them~\cite{chatgptlimitation}: ``ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Fixing this issue is challenging, as: (1) during reinforcement learning training, there's currently no source of truth; (2) training the model to be more cautious causes it to decline questions that it can answer correctly; and (3) supervised training misleads the model because the ideal answer depends on what the model knows, rather than what the human demonstrator knows.''

% Besides, the recently works \cite{zhong2023can} has evaluated and compared the learning ability of ChatGPT and BERT on the NLP and NLU (Natural language understanding) tasks. They found that with the help of advanced prompting strategies, ChatGPT shows better understanding ability, and even outperforms the powerful BERT-based model (\ie, RoBERTa-large~\cite{liu2019roberta}) on some NLU tasks. However, there is still a performance gap between ChatGPT and fine-tuned RoBERTa-large in terms of average performance. That said, while ChatGPT could solve many NLP problems quite well, it still fails to beat the current SOTA models.
% \textcolor{red}{(Link between two paragraphs: above and following)}
However, we observe that no existing work has investigated the use of ChatGPT in security domains. 
As security researchers, our primary focus centers on exploring the potential and limitations of LLMs (i.e., ChatGPT and CodeBERT) in program analysis tasks, with a particular emphasis on addressing software security challenges. 
We have identified notable issues that arise when applying language models originally designed for natural languages to program analysis tasks. For instance, semantic inference is a critical step in source code analysis and directly appying language models on this task may not have a satisfying result. This is because, in natural language, the meaning of a word is deterministic within a specific context, but in programming language, the name of a variable, function or class may contain any string, which is not deterministic.
This inherent variability in naming conventions poses unique challenges for language models that are trained to interpret natural languages, as it requires them to adapt to the fluid and context-dependent nature of code syntax and semantics. As a result, these models may encounter difficulties in accurately understanding and processing code-related information, leading to limitations in their feasibility on program analysis tasks.
However, to the best of our knowledge, no existing papers have specifically delved into these issues.

In this paper, our primary objective is to explore strengths, potentials, and limitations  
of LLMs (Large Language Models) in security-oriented program analysis tasks.
% To this end, we present a series of experiments and empirical analyses aiming to unveil the capabilities of LLMs in the context of security-oriented program analysis.
Given the different nature of ChatGPT and CodeBert/GraphCodeBert, we conduct distinct experiments to analyze them respectively.
Firstly, as ChatGPT is a conversational large language model, the use of prompt style~\footnote{A prompt is a set of instructions provided to an LLM that programs the LLM by customizing it and/or enhancing or refining its capabilities~\cite{white2023prompt}.} plays a crucial role in effectively conversing with it.
Therefore, we have opted for a case study approach.
We meticulously design a set of program analysis tasks with varying levels of challenge to thoroughly evaluate the quality of its responses.
Secondly, considering that CodeBert is a pre-trained model applicable to downstream tasks, we conduct a focused set of experiments to quantitatively assess its performance using a group of datasets specially created for this purpose.

Specifically, our case study on the ChatGPT focus on 5 problems: code semantic inference, vulnerability analysis, generalization ability in code review, dynamic vulnerability discovery, and assemble code analysis. 
Through empirical analysis, we demonstrate that ChatGPT shows promise as a valuable tool for security-oriented source code analysis, effectively learning high-level semantics. Surprisingly, ChatGPT shows the ability to perform inter-procedural analysis and infer the high-level semantics of the source code. These remarkable learning capabilities position ChatGPT as a potential asset in security-oriented contexts.

Despite these strengths, it is crucial to acknowledge certain limitations when utilizing ChatGPT for security-oriented program analysis. It should be clear that ChatGPT's performance may downgrade a lot when analyzing complex source code with insufficient information in variable, function or class names. 
%
To make it worse, instances such as code generated from decompilation~\cite{katz2018using} or non-conventional code naming~\cite{butler2015investigating} might yield reduced accuracy, as ChatGPT's generalization ability is limited to the patterns and examples present in its training data. 
%
Furthermore, the focused experiments on CodeBert and GraphCodeBert reveal the extent to which literal features and logic features impact their performance. While these models exhibit remarkable capabilities, their effectiveness can be influenced by unreliable features in the analyzed code.
%
Additionally, for very specific questions involving implementation-level details, ChatGPT's proposed solutions may possess high-level conceptual correctness but may lack precision in fully addressing the problem. These limitations underscore the need of further investigation and refinement to enhance ChatGPT's performance in such scenarios. 
%

We anticipate that our findings and analysis will provide valuable insights for future researchers in this domain, fostering a deeper understanding of the potential and limitations of LLMs in software security.
As researchers delve deeper into the challenges we have identified, they can actively work towards maximizing the potential of ChatGPT and other LLMs in security-oriented program analysis. 
By addressing these limitations and building upon the model's strengths, we can pave a way for more robust and more accurate analysis tools that contribute to enhanced software security practices.








