% \vspace{-0.5em}
\section{Case Study}
To verify our findings, we conducted case studies on ChatGPT to further investigate the details. ChatGPT is a state-of-the-art LLM designed to understand and generate human-like text based on the input it receives. Its ability to answer questions, provide explanations, and engage in conversations is helpful for us to understand the nuances of success or failure in our case study. Similar to CodeBERT, ChatGPT has also been adopted to perform source code level analysis and address security-related issues, such as vulnerability discovery and fixing~\cite{surameery2023use,cheshkov2023evaluation}.

In this section, we conduct case studies on ChatGPT with a series of analytical tasks. Specifically, we first assess ChatGPT's capability to comprehend source code. Subsequently, we evaluate ChatGPT's proficiency in tackling specific security challenges and its generalization ability.

\subsection{Semantic Comprehension}


\label{sec:gene}
To evaluate whether ChatGPT has a better ability to understand the logical flows in the code without the influence of naming, we randomly select several code snippets from LeetCode~\footnote{The source code is available at: \url{https://github.com/Moirai7/ChatGPTforSourceCode}}, representing varying levels of difficulty, and assess ChatGPT's performance on the original code snippets and the corresponding obfuscated code snippets.





Our experiments show that ChatGPT can correctly understand the original code snippets.
Taking the Supper Egg Drop problem ({\bf \ref{appendix:Q1}}) as an example, it explains the purpose of the code snippet, the logic and complexity of the algorithm, and provides suggestions to improve the code. 
Then, we manually obfuscated the code snippets by renaming the variables and function names and insert dummy code that will not have an impact on its results.
The obfuscated code is shown in~\autoref{code:obfuscated}.
ChatGPT can not provide useful review of the obfuscated code ({\bf \ref{appendix:Q2}}).

Based on the responses from ChatGPT, we can conjecture that ChatGPT learns the literal features of code snippets that exist within its training data and generates responses based on statistical patterns involving characters, keywords, and syntactic structures. This knowledge encompasses various aspects such as indentation, syntax, and common coding conventions. However, it lacks a profound comprehension of code semantics, logic, or context beyond the patterns and examples present in its training data. Consequently, it struggles to comprehend obfuscated code, as it relies heavily on literal features. This issue is exacerbated by the fact that ChatGPT is trained on a general corpus rather than a code-focused corpus like CodeBERT, making it more susceptible to misinterpretation due to name changes.

\subsection{Vulnerabilities Analysis}
\label{sec:smart}
Though GPT relies on literal features to understand code logic, several works demonstrate its ability to understand normal code (without obfuscation)~\cite{sun2024gptscan,sagodi2024reality,xiao2023supporting}.
The following section tests its ability in software testing, including vulnerability identification, exploitation development, remediation, and real-world security audits.

\subsubsection{Synthetic Programs and Classic Vulnerability}
We adopt a piece of code ({\bf \ref{appendix:Q3}}) that contains buffer overflow vulnerability and then task ChatGPT to find the vulnerabilities.
Then, we replace the \texttt{strcpy} function with a homemade string copy function ({\bf \ref{appendix:Q4}}) and challenge ChatGPT with modified code.
The analysis conducted by ChatGPT on the vulnerability in the code was impressively accurate, showing its potential to identify vulnerabilities in the source code effectively.
However, in an effort to explore its understanding of potential security risks, we posed another question:
\ding{182} Whether ChatGPT can exploit a vulnerability to launch an attack.

Through experiment ({\bf \ref{appendix:Q5}}), we observed that the exploit proposed by ChatGPT, although not directly usable to launch attacks, was conceptually correct.
This suggests that ChatGPT possesses the ability to assist attackers in generating potential exploits.

Then, we investigate \ding{183} whether ChatGPT can also provide solutions to fix the identified vulnerability.
Our evaluation ({\bf \ref{appendix:Q6}}) has demonstrated that the proposed patching effectively addresses the vulnerability in the code, providing an effective fix. As a result, we believe that ChatGPT holds the potential to assist developers and security analysts in identifying and resolving vulnerabilities in normal programs.

Finally, we assess \ding{184} whether it can suggest protection schemes that thwart potential exploits by attackers.
By leveraging ChatGPT's analysis, we aim to explore its ability to propose robust security measures that can safeguard against known vulnerabilities and potential attack vectors.

In the experiment (({\bf \ref{appendix:Q7}})), the proposed protection scheme suggested by ChatGPT demonstrates a conceptually correct approach to detect buffer overflows through the use of a ``canary''.
However, upon closer inspection, we identified some implementation errors that may impact its effectiveness.
The placement of the canary at the beginning of the buffer, instead of at the end, renders it unable to detect buffer overflows that overwrite adjacent variables located at higher memory addresses.
Additionally, the revised code introduces the ``canary'' into the buffer without correspondingly increasing the buffer size, limiting its capacity to handle strings as long as the original implementation.

While ChatGPT appears to understand the concept of using a ``canary'' for buffer overflow detection, it seems to struggle in correctly applying this knowledge to unseen code snippets. These issues raise concerns regarding ChatGPT's soundness.
That means GPT can provide answer/solution that are correct in the high level, but incorrect in some details. 
As the software protection require higher soundness, which poses challenges to adopt GPT to protect software. 

\subsubsection{Real-world Program and Logic Bugs}
The last subsection discussed GPT's ability with synthetic programs containing popular classic bugs. In this section, we test its ability to reason about logic bugs (cross-function vulnerabilities) in real-world applications. Specifically, our objective is to investigate the capabilities of ChatGPT to analyze and audit smart contracts, which are commonly deployed in the Web3 ecosystem.

First, we tested different types of vulnerabilities (({\bf \ref{appendix:Q8}})) that happened before ChatGPT launched so that they could potentially manifest within the ChatGPT system (\ie, seen programs).
According to the answer, ChatGPT adeptly identifies potential vulnerabilities and offers appropriate patches. Furthermore, after selecting a full list of smart contracts \cite{Crytic,runtimeverified} across various categories including reentrancy attack and variable shadowing, ChatGPT consistently discerns vulnerabilities and provides accurate solutions.

Secondly, we conducted analyses on a selection of recently emerged vulnerable smart contracts (({\bf \ref{appendix:Q9}})), occurring within the temporal scope of 2023, to evaluate their efficacy in terms of generalization. This examination was predicated upon a real-world incursion targeting Midas Capital (as detailed in \cite{midascapital}) in June 2023.
The actual root cause of this attack is due to round issues, however, based on the answer that ChatGPT gives us, it is not aware of the risk in function \texttt{redeemFresh}.
We also test several recent attack cases \cite{Web3Sec}, and ChatGPT can correctly identify none of them.

Third, we additionally assess the efficacy of ChatGPT in the realm of cross-contract vulnerability detection (({\bf \ref{appendix:Q10}})). 
To this end, we present a real-world attack scenario exemplified by an incident occurring within the Euler Finance ecosystem \cite{eulerdefi}.
According to the findings of the analysis, ChatGPT is unable to accurately discern vulnerabilities, opting instead to provide seemingly correct responses arbitrarily, devoid of substantive remedial recommendations. Consequently, it is still difficult for chatGPT to grapple with the intricacies in the analysis of cross-contract vulnerabilities.

% \input{sections/case-fuzzing}

\subsection{Observations}
% This section presents a series of experiments and empirical analyses aimed at elucidating ChatGPT's capacity for solving security-oriented program analysis.
% Firstly, it demonstrates that ChatGPT, along with other large language models, opens up a novel avenue for security-oriented source code analysis, proving to be an efficient method for learning high-level semantics from well-named source code. 



The conclusion of this case study aligns with the findings from our main experiments: users should expect degraded model performance when applying LLMs to analyze source code lacking sufficient information in variable, function, or class names. For instance, code generated from decompilation~\cite{katz2018using} or code that does not adhere to standard naming conventions~\cite{butler2015investigating} may yield lower accuracy, as demonstrated in questions {\bf \ref{appendix:Q1}} and {\bf \ref{appendix:Q2}}.
Furthermore, additional findings from the case study indicate that for very specific questions involving implementation-level details, logical reasoning, or systematic solutions, ChatGPT's proposed solutions may be conceptually correct at a high level but may not fully address the problem.