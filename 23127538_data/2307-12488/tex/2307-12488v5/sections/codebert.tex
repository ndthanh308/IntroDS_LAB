% \vspace{-0.5em}
% \section{Quantitative Analysis on CodeBert}
% The previous section has explored the ChatGPT's strengths, potential, and limitation in solve some code analysis tasks.
% In this section, we shift our attention from the ChatGPT to the CodeBert. 

% {\color{red} Code Search: Given a natural language as the input, the objec- tive of code search is to find the most semantically related code from a collection of codes. We conduct experiments on the CodeSearchNet corpus (Husain et al., 2019) }


% {\color{blue} \url{https://arxiv.org/pdf/2002.08155} \url{https://arxiv.org/pdf/2009.08366}}

% \url{https://arxiv.org/pdf/1909.09436}
% \url{https://arxiv.org/pdf/2301.08427v2}

% {\color{yellow} https://aisec.cc/}


\section{Terminology: Literal Features and Logic Features}

A source code file of a program consists of a sequence of tokens. These tokens can be grouped into three categories: keywords, operators (both defined in the programming language specification), and user-defined names.

Keywords are reserved words that have special meanings and purposes and can only be used for specific purposes. For example, for, if, and break are widely known keywords used in many programming languages. They are used in a program to change the control flow. A programming language usually contains only a limited number of keywords. For example, the C programming language contains 32 keywords, and Python 3.7 contains 35 keywords.

In addition to keywords, a programming language needs to define a set of operators. For example, arithmetic operators (e.g., +, -, and *) and logical operators (e.g., and, or, and not) are two of the most important categories. Keywords and operators are defined by a programming language.

In addition, a programmer needs to define some tokens (i.e., names) to represent variables, structures, functions, methods, classes, and packages. When programmers write a code snippet, they can randomly choose any string to name these elements as long as the string does not conflict with reserved keywords and operators and follows the programming language specification. However, programmers have limited flexibility in choosing the keywords and operators. Only some keywords (such as for and while) and operators (such as ++, +1) are interchangeable. Thus, the program logic is fixed when using a set of keywords and operators in a program. In contrast, the names of variables, functions, or classes have no impact on the logic of the program.

\begin{definition}(Literal Features and Logical Features). 
    Given a piece of code, the \textbf{literal features} are features of the literal meaning of the variable names, function names, and programmer-readable annotation. These features could be removed without changing the functionality of the code. The \textbf{logical features} are features that control the program logic. The keywords and operators defined by a programming language specification are logical features.
\end{definition}

\section{Methodology}
Currently, GraphCodeBERT takes code snippets of functions or class methods as data samples. It tokenizes keywords, operators, and developer-defined names from the code snippets and utilizes a Transformer to learn the data flow of the source code in the pre-training stage. Within a function or a method, developer-defined names can be grouped into three categories: 1) variable names, 2) method names, and 3) method invocation names. The program logic is not affected if we map these developer-defined names to other strings in the same namespace.

Since the pre-trained CodeBERT and GraphCodeBERT models, along with the datasets~\cite{trainedcodebert} for downstream tasks, are publicly available for download and use, we are able to conduct focused experiments to evaluate them. Here, we will conduct the evaluation based on two downstream tasks: natural language code search and clone detection.

The goal of code search is to find the code snippet that is most semantically relevant to a given natural language query from a collection of code. This reflects an LLM's ability to understand a specific function and map its intended purpose to the relevant code portion within the entire program.

Code clone detection aims to identify and measure the similarity between different code fragments, determining if they produce the same results when given the same input. This indicates whether an LLM's understanding has reached a level where it can mimic the code execution process and comprehend the logic flow.

We did not choose other downstream tasks because code refinement adopts an abstract representation and code translation involves two programming languages, which introduces challenges in uniformly anonymizing user-defined names. Since our experiments focus on the learning ability of pre-trained models on literal and other semantic features, and the downstream tasks are only used to evaluate the effectiveness of the pre-trained model, we believe that any reasonable downstream task should be appropriate.


\subsection{Dataset}

\begin{table}[t!]%htpb!
    \setlength{\tabcolsep}{4pt}
  \centering
  \begin{threeparttable}
  \captionsetup{justification=centering}
  \scriptsize
  \caption{Newly Created Dataset for Our Experiments.}
  \label{tab:anonymize}
  \begin{tabular}{cccp{0.3cm}<{\centering}p{0.3cm}<{\centering}p{0.3cm}<{\centering}p{0.6cm}<{\centering}c}
  \toprule
%   {\bf Lang-} & \multirow{2}{*}{\bf Orig.}  & \multirow{2}{*}{\bf Anon.} &  {\bf w/o } & {\bf w/o }  & {\bf w/o} & \multirow{2}{*}{\bf All} \\
   & {\bf Task} & {\bf Language} & {\bf Var.}~\tnote{1} & {\bf Def.} & {\bf Inv.} & {\bf Anon.}~\tnote{2} & {\bf Size} \\
  \midrule
  {$\mathcal{D}$1} &  CodeSearch & Java\&Python &  \ding{51} & \ding{55} &  \ding{55} & Rand.  & 175,878 \& 266,738~\tnote{3} \\
  \midrule
  {$\mathcal{D}$2} &  CodeSearch & Java\&Python &  \ding{51} & \ding{55} &  \ding{55} & Shuff. & 175,878 \& 266,738 \\
  \midrule
  {$\mathcal{D}$3} & CodeSearch & Java\&Python&  \ding{55} & \ding{51} &  \ding{55} & Rand. & 175,878 \& 266,738 \\
  \midrule
  {$\mathcal{D}$4} & CodeSearch & Java\&Python&  \ding{55} & \ding{51} &  \ding{55} & Shuff. & 175,878 \& 266,738 \\
  \midrule
  {$\mathcal{D}$5} & CodeSearch & Java\&Python&  \ding{55} & \ding{55} &  \ding{51} & Rand. & 175,878 \& 266,738 \\
  \midrule
  {$\mathcal{D}$6} & CodeSearch & Java\&Python&  \ding{55} & \ding{55} &  \ding{51} & Shuff. & 175,878 \& 266,738 \\
  \midrule
  {$\mathcal{D}$7} & CodeSearch & Java\&Python&  \ding{51} & \ding{51} &  \ding{51} & Rand. & 175,878 \& 266,738 \\
  \midrule
  {$\mathcal{D}$8} & CodeSearch & Java\&Python&  \ding{51} & \ding{51} &  \ding{51} & Shuff. & 175,878 \& 266,738 \\
  \midrule
  {$\mathcal{D}$9} & CloneDete. & Java&  \ding{51} & \ding{55} &  \ding{55} & Rand. & 1316,444 \\
  \midrule
  {$\mathcal{D}$10} & CloneDete. & Java&  \ding{51} & \ding{55} &  \ding{55} & Shuff. & 1316,444 \\
  \midrule
  {$\mathcal{D}$11} & CloneDete. & Java&  \ding{55} & \ding{51} &  \ding{55} & Rand. & 1316,444 \\
  \midrule
  {$\mathcal{D}$12} & CloneDete. & Java&  \ding{55} & \ding{51} &  \ding{55} & Shuff. & 1316,444 \\
  \midrule
  {$\mathcal{D}$13} & CloneDete. & Java&  \ding{55} & \ding{55} &  \ding{51} & Rand. & 1316,444 \\
  \midrule
  {$\mathcal{D}$14} & CloneDete. & Java&  \ding{55} & \ding{55} &  \ding{51} & Shuff. & 1316,444 \\
  \midrule
  {$\mathcal{D}$15} & CloneDete. & Java&  \ding{51} & \ding{51} &  \ding{51} & Rand. & 1316,444 \\
  \midrule
  {$\mathcal{D}$16} & CloneDete. & Java&  \ding{51} & \ding{51} &  \ding{51} & Shuff. & 1316,444 \\
  \midrule
%   \multirow{2}{0.5cm}{Java}  & \multirow{2}{0.5cm}{70.36\%} & Random  & 67.73\% & 60.89\% & 69.84\% & 17.42\% \\
%   & &  Meaningful & 67.14\%	& 58.36\% &	69.84\% &	17.03\% \\
%   \midrule
%   \multirow{2}{0.5cm}{Python}  & \multirow{2}{0.5cm}{68.17\%}  & Random  &  59.8\% & 55.43\% & 65.61\% & 24.09\% \\
%   & & Meaningful& 59.78\% &	55.65\% &	65.61\% &	23.73\%  \\
  \bottomrule
\end{tabular}
  \begin{tablenotes}
    \scriptsize
    \item[1] {\bf Var.} {\bf Def.}, and {\bf Inv.} represent that the variable names, method/function definition names, and the method/function invocation names are anonymized. 
    \item[2] Anonymized methods: randomly-generated or shuffling. 
    \item[3] Two numbers are the sizes of data samples corresponding two languages. 
  \end{tablenotes}
\end{threeparttable}
\end{table}

Then, we retrain the existing models and evaluate their performance on the two downstream tasks: natural language code search and clone detection. For the code search task, we used the test portion from the CodeSearchNet corpus collection~\cite{husain2019codesearchnet}. This dataset, derived from publicly available open-source non-fork GitHub repositories and their associated documentation, provides code-text pairs for various code search challenges in both Java and Python. The testing dataset for the clone detection task is based on the BigCloneBench dataset~\cite{svajlenko2014towards}, which provides a benchmark built from verified clones of ten common functionalities in different inter-project Java repositories.

Based on the datasets for these two tasks, we generate three types of datasets that anonymize three types of literal features in the code: variable names, method/function definition names, and method/function invocation names. Additionally, we create another type of dataset that anonymizes all three types of features together.

\begin{enumerate}
    \item In the first type of datasets, we anonymize the variable names. An example is the change from \texttt{it\_end} to \texttt{var3} and \texttt{finished} to \texttt{var4} between \autoref{code:goodname} and \autoref{code:badname}. 
    \item In the second type of datasets, we anonymize the method/function names. An example is the change from \texttt{bubble\_sort} to \texttt{fun1} between \autoref{code:goodname} and \autoref{code:badname}. 
    \item In the third type of datasets, we anonymize the method/function invocation names. An example is the change from \texttt{pred} to \texttt{fun2} between \autoref{code:goodname} and \autoref{code:badname}. 
    \item The last type of datasets are a combination of the three types of anonymization, which anonymize all three kinds of developer-defined names. 
\end{enumerate}

\lstset{style=mycstyle}
\begin{lstlisting}[language=C++,
emph={bubble_sort, begin, end, pred, it_end, finished, next, it},
label={code:goodname},caption={A piece of code with meaningful variable/function names.}, captionpos=b]{}
    void bubble_sort(It begin, It end, Pred pred=Pred()){
        if ( std::distance( begin, end ) <= 1 ){ return; }
        auto it_end    = end;
        bool finished  = false;
        while ( !finished ){ //loop stop when no adjacent elements are not in required order. 
            finished = true;
            std::advance( it_end, -1 );
            for (auto it = begin; it! = it_end; ++ it ){
                auto next = detail::advance( it, 1 );
                if (pred( * next, * it)){
                    std::swap( * it, * next);
                    finished = false;
                }
            }
        }   
    }
\end{lstlisting}
\begin{lstlisting}[language=C++,
emph={var1, var2, var3, var4, var5, var6, fun1, fun2, fun2},
emphstyle=\color{purple}\ttfamily\scriptsize,
label={code:badname},caption={The same code without meaningful variable/function names.}, captionpos=b]{}
    void fun1(It var1, It var2, Pred fun2=Fun2()){
        if ( std::distance( var1, var2 ) <= 1 ){ return; }
        auto var3  = var2;
        bool var4  = false;
        while ( !var4 ){
            var4 = true;
            std::advance( var3, -1 );
            for (auto var5 = var1; var5! = var3; ++ var5 ){
                auto var6 = detail::advance( var5, 1 );
                if (fun2( * var6, * var5)){
                    std::swap( * var5, * var6);
                    var4 = false;
                }
            }
        }   
    }
\end{lstlisting}



Besides, we adopt two strategies to anonymize the names:
The first strategy, called "randomly-generated," randomly generates strings (e.g., ``oe4yqk4cit2maq7t") without any literal meaning.
The second strategy, called "shuffling," shuffles the names in the code. In this way, the shuffled names do not reflect the intention of the variables, functions, or invocations. For example, this strategy could replace ``bubble\_sort" with ``aes\_encryption."

Based on the four types of name sets to replace and the two replacing strategies, we eventually generated 16 variants of the original dataset from \cite{guo2020graphcodebert}. The details of the 16 datasets are shown in Table~\ref{tab:anonymize}.

\subsection{Experiment Design}

\noindent\textbf{Model Training and Fine-Tuning.} 
We directly adopt the pre-trained model from GraphCodeBERT~\cite{guo2020graphcodebert}, which is pre-trained on three tasks. The first task involves masked language modeling to learn representations from the source code. The second task is data flow edge prediction, aimed at learning representations from data flow. This is achieved by initially masking some variables' data flow edges and then having GraphCodeBERT predict those edges. The final task is variable alignment between the source code and data flow, which involves predicting the locations where a variable is identified to align representations between the source code and the data flow. GraphCodeBERT includes 12 layers of Transformer with 768-dimensional hidden states and 12 attention heads.

The pre-trained model is then fine-tuned on the downstream tasks on an NVIDIA DGX-1. Specifically, we train a model for each of the 16 datasets that we have crafted by randomly selecting 80\% of the data samples as the training set and the remaining 20\% as the test set.

\noindent\textbf{Evaluation Matrix.}
We use Mean Reciprocal Rank (MRR) as our evaluation metric for the code search task, which assesses how well the model ranks the correct code snippet among 999 distractor snippets when given the documentation comment as a query. For the code clone detection task, we use the F1 score to measure the model's performance.

\begin{table}[t!]%htpb!
    \setlength{\tabcolsep}{4pt}
  \centering
  \captionsetup{justification=centering}
  \footnotesize
  \caption{{\bf Results on Code Search.} The results are shown in $\frac{MRR}{Dataset}$ format.}
  \label{tab:search}
  \begin{tabular}{lcccccc}
  \toprule
  {\bf Lang-} & \multirow{2}{*}{\bf Orig.}  & \multirow{2}{*}{\bf Anon.} &  {\bf w/o } & {\bf w/o }  & {\bf w/o} & \multirow{2}{*}{\bf All} \\
  {\bf uage}& & & {\bf Var.} & {\bf Def.} & {\bf Inv.} & \\
  \midrule
  \multirow{2}{0.5cm}{Java}  & \multirow{2}{0.7cm}{70.36\%} & Random  & $\frac{67.73\%}{\mathcal{D}1}$   & $\frac{60.89\%}{\mathcal{D}3}$ & $\frac{69.84\%} {\mathcal{D}5}$ & $\frac{17.42}{\mathcal{D}7}$ \\
  & &  Shuffling & $\frac{67.14\%}{\mathcal{D}2}$	& $\frac{58.36\%}{\mathcal{D}4}$ &	$\frac{69.84\%}{\mathcal{D}6}$ &	$\frac{17.03\%}{\mathcal{D}8}$ \\
  \midrule
  \multirow{2}{0.5cm}{Python}  & \multirow{2}{0.7cm}{68.17\%}  & Random  &  $\frac{59.8\%}{\mathcal{D}1}$  & $\frac{55.43\%}{\mathcal{D}3}$ & $\frac{65.61\%}{\mathcal{D}5}$ &$\frac{24.09\%}{\mathcal{D}7}$ \\
  & & Shuffling & $\frac{59.78\%}{\mathcal{D}2}$ &	$\frac{55.65\%}{\mathcal{D}4}$ &	$\frac{65.61\%}{\mathcal{D}6}$ &	$\frac{23.73\%}{\mathcal{D}8}$  \\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}[t!]%htpb!
  \setlength{\tabcolsep}{4pt}
  \centering
  \captionsetup{justification=centering}
  \footnotesize
  \caption{{\bf Results on Clone Detection.} The results are shown in $\frac{F1}{Dataset}$ format.}
  \label{tab:clone}
  \begin{tabular}{ccccccc}
  \toprule
  {\bf Lang-} & \multirow{2}{*}{\bf Orig.}  & \multirow{2}{*}{\bf Anon.} &  {\bf w/o } & {\bf w/o }  & {\bf w/o} & \multirow{2}{*}{\bf All} \\
  {\bf uage}& & & {\bf Var.} & {\bf Def.} & {\bf Inv.} & \\
  \midrule
  \multirow{2}{0.5cm}{Java}  & \multirow{2}{0.7cm}{94.87\%} & Random & $\frac{92.64\%}{\mathcal{D}9}$& $\frac{93.97\%}{\mathcal{D}11}$ & $\frac{94.72\%}{\mathcal{D}13}$ & $\frac{86.77\%}{\mathcal{D}15}$ \\
  & & Shuffling & $\frac{92.52\%}{\mathcal{D}10}$ &	$\frac{94.27\%}{\mathcal{D}12}$ &	$\frac{93.67\%}{\mathcal{D}14}$ &	$\frac{84.76\%}{\mathcal{D}16}$ \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{Experiment Results}
Table~\ref{tab:search} and Table~\ref{tab:clone} show the experiment results (MRR or F1 score) on the downstream tasks of code search and code clone detection, respectively. The second column shows the model performance reported in the original paper~\cite{guo2020graphcodebert}. The third column indicates the anonymization methods adopted.

The fourth, fifth, and sixth columns display the model performance when anonymizing different parts of the code: variable names, method/function definition names, and method/function invocation names, respectively. The last column shows the model performance when anonymizing all three types of names together. To associate the results with the corresponding datasets in Table~\ref{tab:anonymize}, we provide not only the model performance but also the dataset numbers that were trained and tested on.


\subsection{Analysis and Discussion}


The results show that anonymizing variable names, method/function definition names, and method/function invocation names leads to a significant decline in model performance, regardless of whether we replace developer-defined names with ``randomly-generated" strings or ``shuffling" strings. Additionally, on average, datasets with ``shuffling" strings perform worse than those with randomly-generated strings, suggesting that ``shuffling" strings can mislead the models.

In terms of method/function names, anonymizing method/function definition names results in more significant degradation than anonymizing method/function invocation names, especially in code search tasks. This indicates that method/function definition names often strongly correlate with the overall function goal in most scenarios within the training dataset. Trained LLMs tend to infer a correlation between the method/function name and its actual purpose when tasked with code search. This predictability stems from open-source program datasets, which are well-organized and intended to be user-readable, unlike malware or commercial applications that obscure their true purpose from users. As for method/function invocation names, their impact is less pronounced because they operate within the function scope and influence only part of the function logic, similar to variable names. As shown in the results table, the score for changes in method/function invocation names is closer to changes in variable names than changes in method/function definition names.

As for different programming languages, we can discern the differences in degradation from the results of the code search task. Overall, if a single attribute is anonymized but the others remain unchanged, the degradation of LLM is more significant for Python than Java. However, if all three naming attributes in the code are changed, the Java task shows the most severe degradation, leading to an MRR score as low as 17.03%.

This disparity can be attributed to several factors related to language characteristics, naming conventions, and the overall ecosystem of the two languages. Java, as a statically typed language, provides additional context through type declarations, which compensates for the loss of meaningful names. In contrast, Python, a dynamically typed language, relies more heavily on descriptive variable and function names for context and clarity, aligning with its design philosophy of readability and simplicity. Developers expect meaningful names in Python to quickly understand code functionality.

Java code often includes more boilerplate and verbosity, which can provide additional contextual clues even if names are anonymized. Moreover, the Java ecosystem's strong typing and adherence to naming conventions in frameworks and libraries ensure consistency and maintainability in large codebases. These structural and syntactical elements help mitigate the impact of partial anonymization. However, modifying all names for both variables and method/functions in Java code disrupts these patterns, significantly impairing the LLM's ability to understand the code. 

Similarly, in Python, anonymizing all three types of names led to a score as low as 23.73\%, indicating that Python's reliance on descriptive naming for understanding is also critical. The combination of these factors underscores the challenges in adapting LLMs effectively to programming languages, necessitating models that can handle both literal and logical features to enhance performance in code analysis tasks.


Compared to the clone detection task, the performance degradation is more pronounced in the code search task. This difference arises from the inherent goals of the tasks: code search involves mapping a natural language description to a code block, while clone detection focuses solely on comparing code fragments.

In code search tasks, well-defined variable and method/function names are crucial because they help CodeBERT relate the code snippet to its descriptive purpose. Consequently, the model relies more on the natural language elements of the code and less on its underlying logic. Conversely, code cloning typically involves copying code with slight alterations, often in the names of variables or methods/functions, to avoid plagiarism or detection. Thus, detecting code clones requires ignoring these names and focusing on the actual logic flow of the code.

This fundamental difference between the two tasks explains the varying degrees of performance degradation observed in CodeBERT during code analysis. It also indicates that CodeBERT is well-trained for code clone detection tasks, recognizing that naming conventions have minimal impact on the actual logic of the code. Therefore, there is a growing need for pre-trained models specifically tailored to programming languages rather than natural language, to enhance their effectiveness in code analysis tasks.

Overall, our experiments reflect that current source-code level representation learning methods still largely rely on the \textit{literal features} and ignore the \textit{logical features}. However, the \textit{literal features} are not always reliable, as mentioned in \autoref{sec:intro}. The current models still cannot effectively learn the logical features in the source code. Attackers can simply randomize the \textit{literal features} to mislead the CodeBERT and GraphCodeBERT models.

%The \textit{logic feature} hidden in the source code contains all the information of a piece of the program. But current models still cannot effectively learn them.  


%\subsection{Observations}
%\fixme{(remove this section?)}
%Through a set of experiments and empirical analysis, this section tries to explain the learning ability of current BERT-based source code representation learning schemes.
%The results show that CodeBERT and GraphCodeBERT are efficient to learn \textit{literal features} but less efficient to learn \textit{logic features}. 


