% \vspace{-0.5em}
\section{Introduction}
\label{sec:intro}
Deep learning has demonstrated its significant learning ability in natural language processing (NLP). To deploy a natural language task, such as translation and text classification, researchers first pre-train a model to embed words into vectors using ELMo~\cite{sarzynska2021detecting}, GPT~\cite{radford2018improving}, and BERT~\cite{devlin2018bert}. These pre-trained models are initially trained on a large unsupervised text corpus and then fine-tuned on different downstream tasks. Such models are called large language models (LLMs) due to their relatively large number of model parameters. These LLMs have been deployed to the source code to learn program representations. Similar to natural language, the program representation learned from the source code using pre-trained models can be applied to several downstream program analysis tasks.

In 2020, \citeauthor{feng2020codebert} proposed a pre-trained model called CodeBERT~\cite{feng2020codebert}, based on Bidirectional Encoder Representations from Transformers (BERT), that learns general-purpose representations to support downstream NL-PL applications such as natural language code search, code documentation generation, and more. In 2021, \citeauthor{guo2020graphcodebert} proposed a new pre-trained model, GraphCodeBERT~\cite{guo2020graphcodebert}, which improves CodeBERT by enabling the model to capture more program semantic information, such as data flow. Recently, researchers have adopted ChatGPT for various code analysis tasks, such as code refinement~\cite{guo2024exploring}, vulnerability discovery~\cite{chen2023chatgpt}, and more. In industry, GitHub Copilot~\cite{copilot}, powered by GPT, assists developers by suggesting code snippets, completing functions, and providing contextual code recommendations directly within Integrated Development Environments.


The difference between natural language and programming language leads to unintended consequences if these NLP models are directly applied to programming language. In natural language, the meaning of a word is deterministic in a specific context, whereas in programming language, a programmer can assign any string to any variable, method, or function as their names. In such cases, most strings in the code could be replaced by other words and may not carry meaningful information. If a large language model still heavily relies on the literal meaning of a variable, method, or function name, it may encounter problems when the assigned name does not contain useful information or has a controversial meaning.

Furthermore, a limited number of words are used in natural language, while in programming language, the number of words can be unlimited because a programmer can casually create a string to name a variable, regardless of whether the created string is interpretable. Therefore, it is doubtful whether the word embedding adopted in natural language is still efficient for solving program analysis tasks. If a model designer ignores the numerous differences between natural language and programming language and naively adopts methods from NLP, the designed model may suffer from the above limitations.

In this paper, we propose a taxonomy that groups features in programming languages into two categories: literal features and logical features. We aim to investigate how these different types of features affect the performance of current pre-trained models in downstream tasks and to what extent they influence the results. Specifically, to achieve this goal, we create eight types of datasets that mask off different types of features to quantitatively measure their impact. In each dataset, either variable names or method/function names are replaced with nonsense or misleading names. We then use well-trained models (CodeBERT) to perform two code analysis tasks on these created datasets and measure the impact of naming on the model's performance.

Based on the experimental results, we find that naming strategies have a significant impact on the performance of code analysis tasks based on LLMs, indicating that code representation learning heavily relies on well-defined names in code. Our experiments with CodeBERT reveal the extent to which literal and logical features impact performance. While these models exhibit remarkable capabilities, their effectiveness can be influenced by unreliable features in the analyzed code. Instances such as code generated from decompilation~\cite{katz2018using} or non-conventional code naming~\cite{butler2015investigating} might yield reduced accuracy, as LLMs' generalization ability is limited to the patterns and examples present in their training data.

Additionally, since ChatGPT is a conversational large language model that can provide intuitive feedback, we conduct a case study to investigate the pre-trained model's ability to comprehend programming language in specific settings.



















