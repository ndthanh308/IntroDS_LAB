\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen-zhu2019}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97, pp.\
  242--252, 2019.

\bibitem[Amari et~al.(2020)Amari, Ba, Grosse, Li, Nitanda, Suzuki, Wu, and
  Xu]{amari2020does}
Amari, S.-i., Ba, J., Grosse, R., Li, X., Nitanda, A., Suzuki, T., Wu, D., and
  Xu, J.
\newblock When does preconditioning help or hurt generalization?
\newblock \emph{arXiv preprint arXiv:2006.10732}, 2020.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  322--332. PMLR, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{arXiv preprint arXiv:1904.11955}, 2019{\natexlab{b}}.

\bibitem[Bach(2017)]{bach2017breaking}
Bach, F.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 629--681, 2017.

\bibitem[Barzilai et~al.(2022)Barzilai, Geifman, Galun, and
  Basri]{barzilai2022kernel}
Barzilai, D., Geifman, A., Galun, M., and Basri, R.
\newblock A kernel perspective of skip connections in convolutional networks.
\newblock \emph{arXiv preprint arXiv:2211.14810}, 2022.

\bibitem[Basri et~al.(2019)Basri, Jacobs, Kasten, and
  Kritchman]{basri2019convergence}
Basri, R., Jacobs, D., Kasten, Y., and Kritchman, S.
\newblock The convergence rate of neural networks for learned functions of
  different frequencies.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4763--4772, 2019.

\bibitem[Basri et~al.(2020{\natexlab{a}})Basri, Galun, Geifman, Jacobs, Kasten,
  and Kritchman]{basri2020Nonuniform}
Basri, R., Galun, M., Geifman, A., Jacobs, D., Kasten, Y., and Kritchman, S.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{a}}.

\bibitem[Basri et~al.(2020{\natexlab{b}})Basri, Galun, Geifman, Jacobs, Kasten,
  and Kritchman]{basri2020frequency}
Basri, R., Galun, M., Geifman, A., Jacobs, D., Kasten, Y., and Kritchman, S.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  685--694. PMLR, 2020{\natexlab{b}}.

\bibitem[Belfer et~al.(2021)Belfer, Geifman, Galun, and
  Basri]{belfer2021spectral}
Belfer, Y., Geifman, A., Galun, M., and Basri, R.
\newblock Spectral analysis of the neural tangent kernel for deep residual
  networks.
\newblock \emph{arXiv preprint arXiv:2104.03093}, 2021.

\bibitem[Bietti \& Bach(2020)Bietti and Bach]{bietti2020deep}
Bietti, A. and Bach, F.
\newblock Deep equals shallow for relu networks in kernel regimes.
\newblock \emph{arXiv preprint arXiv:2009.14397}, 2020.

\bibitem[Bowman \& Montufar(2022)Bowman and Montufar]{bowman2022spectral}
Bowman, B. and Montufar, G.
\newblock Spectral bias outside the training set for deep networks in the
  kernel regime.
\newblock \emph{arXiv preprint arXiv:2206.02927}, 2022.

\bibitem[Braun(2005)]{braun2005spectral}
Braun, M.~L.
\newblock \emph{Spectral properties of the kernel matrix and their relation to
  kernel methods in machine learning}.
\newblock PhD thesis, Universit{\"a}ts-und Landesbibliothek Bonn, 2005.

\bibitem[Cagnetta et~al.(2022)Cagnetta, Favero, and Wyart]{cagnetta2022wide}
Cagnetta, F., Favero, A., and Wyart, M.
\newblock How wide convolutional neural networks learn hierarchical tasks.
\newblock \emph{arXiv preprint arXiv:2208.01003}, 2022.

\bibitem[Cai et~al.(2019)Cai, Gao, Hou, Chen, Wang, He, Zhang, and
  Wang]{cai2019gram}
Cai, T., Gao, R., Hou, J., Chen, S., Wang, D., He, D., Zhang, Z., and Wang, L.
\newblock Gram-gauss-newton method: Learning overparameterized neural networks
  for regression problems.
\newblock \emph{arXiv preprint arXiv:1905.11675}, 2019.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao2019}
Cao, Y., Fang, Z., Wu, Y., Zhou, D.-X., and Gu, Q.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:2009.01198}, 2019.

\bibitem[Carlson et~al.(2015)Carlson, Collins, Hsieh, Carin, and
  Cevher]{carlson2015preconditioned}
Carlson, D.~E., Collins, E., Hsieh, Y.-P., Carin, L., and Cevher, V.
\newblock Preconditioned spectral descent for deep learning.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Cho \& Saul(2009)Cho and Saul]{cho2009kernel}
Cho, Y. and Saul, L.
\newblock Kernel methods for deep learning.
\newblock In Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C., and
  Culotta, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~22. Curran Associates, Inc., 2009.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Daniely, A., Frostig, R., and Singer, Y.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  2253--2261, 2016.

\bibitem[Fridovich-Keil et~al.(2021)Fridovich-Keil, Gontijo-Lopes, and
  Roelofs]{fridovich2021spectral}
Fridovich-Keil, S., Gontijo-Lopes, R., and Roelofs, R.
\newblock Spectral bias in practice: The role of function frequency in
  generalization.
\newblock \emph{arXiv preprint arXiv:2110.02424}, 2021.

\bibitem[Geifman et~al.(2022)Geifman, Galun, Jacobs, and
  Basri]{geifman2022spectral}
Geifman, A., Galun, M., Jacobs, D., and Basri, R.
\newblock On the spectral bias of convolutional neural tangent and gaussian
  process kernels.
\newblock \emph{arXiv preprint arXiv:2203.09255}, 2022.

\bibitem[Huang et~al.(2020)Huang, Wang, Tao, and Zhao]{huang2020deep}
Huang, K., Wang, Y., Tao, M., and Zhao, T.
\newblock Why do deep residual networks generalize better than deep feedforward
  networks?---a neural tangent kernel perspective.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 2698--2709, 2020.

\bibitem[Ionescu et~al.(2017)Ionescu, Popa, and Sminchisescu]{ionescu2017large}
Ionescu, C., Popa, A., and Sminchisescu, C.
\newblock Large-scale data-dependent kernel approximation.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  19--27. PMLR,
  2017.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Kennedy et~al.(2013)Kennedy, Sadeghi, Khalid, and
  McEwen]{kennedy2013classification}
Kennedy, R.~A., Sadeghi, P., Khalid, Z., and McEwen, J.~D.
\newblock Classification and construction of closed-form kernels for signal
  representation on the 2-sphere.
\newblock In \emph{Wavelets and sparsity XV}, volume 8858, pp.\  169--183.
  SPIE, 2013.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lee et~al.(2020)Lee, Shen, Song, Wang, et~al.]{lee2020generalized}
Lee, J.~D., Shen, R., Song, Z., Wang, M., et~al.
\newblock Generalized leverage score sampling for neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 10775--10787, 2020.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{li2019enhanced}
Li, Z., Wang, R., Yu, D., Du, S.~S., Hu, W., Salakhutdinov, R., and Arora, S.
\newblock Enhanced convolutional neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:1911.00809}, 2019.

\bibitem[Ma \& Belkin(2017)Ma and Belkin]{ma2017diving}
Ma, S. and Belkin, M.
\newblock Diving into the shallows: a computational perspective on large-scale
  shallow learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma2018power}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3325--3334. PMLR, 2018.

\bibitem[Misiakiewicz \& Mei(2021)Misiakiewicz and
  Mei]{misiakiewicz2021learning}
Misiakiewicz, T. and Mei, S.
\newblock Learning with convolution and pooling operations in kernel methods.
\newblock \emph{arXiv preprint arXiv:2111.08308}, 2021.

\bibitem[Murray et~al.(2022)Murray, Jin, Bowman, and
  Montufar]{murray2022characterizing}
Murray, M., Jin, H., Bowman, B., and Montufar, G.
\newblock Characterizing the spectrum of the ntk via a power series expansion.
\newblock \emph{arXiv preprint arXiv:2211.07844}, 2022.

\bibitem[Neal(2012)]{neal2012bayesian}
Neal, R.~M.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Nocedal \& Wright(1999)Nocedal and Wright]{nocedal1999numerical}
Nocedal, J. and Wright, S.~J.
\newblock \emph{Numerical optimization}.
\newblock Springer, 1999.

\bibitem[Rosasco et~al.(2010)Rosasco, Belkin, and De~Vito]{rosasco2010learning}
Rosasco, L., Belkin, M., and De~Vito, E.
\newblock On learning with integral operators.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (2), 2010.

\bibitem[Saitoh \& Sawano(2016)Saitoh and Sawano]{saitoh2016theory}
Saitoh, S. and Sawano, Y.
\newblock \emph{Theory of reproducing kernels and applications}.
\newblock Springer, 2016.

\bibitem[Sch{\"o}lkopf et~al.(2001)Sch{\"o}lkopf, Herbrich, and
  Smola]{scholkopf2001generalized}
Sch{\"o}lkopf, B., Herbrich, R., and Smola, A.~J.
\newblock A generalized representer theorem.
\newblock In \emph{International conference on computational learning theory},
  pp.\  416--426. Springer, 2001.

\bibitem[Simon(2022)]{simon2022kernel}
Simon, J.~B.
\newblock On kernel regression with data-dependent kernels.
\newblock \emph{arXiv preprint arXiv:2209.01691}, 2022.

\bibitem[Sindhwani et~al.(2005)Sindhwani, Niyogi, and
  Belkin]{sindhwani2005beyond}
Sindhwani, V., Niyogi, P., and Belkin, M.
\newblock Beyond the point cloud: from transductive to semi-supervised
  learning.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pp.\  824--831, 2005.

\bibitem[Song \& Yang(2019)Song and Yang]{song2019quadratic}
Song, Z. and Yang, X.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock \emph{arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N.,
  Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7537--7547, 2020.

\bibitem[Tirer et~al.(2022)Tirer, Bruna, and Giryes]{tirer2022kernel}
Tirer, T., Bruna, J., and Giryes, R.
\newblock Kernel-based smoothness analysis of residual networks.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pp.\
  921--954. PMLR, 2022.

\bibitem[Wang et~al.(2022)Wang, Yu, and Perdikaris]{wang2022and}
Wang, S., Yu, X., and Perdikaris, P.
\newblock When and why pinns fail to train: A neural tangent kernel
  perspective.
\newblock \emph{Journal of Computational Physics}, 449:\penalty0 110768, 2022.

\bibitem[Williams(1997)]{williams1997computing}
Williams, C.~K.
\newblock Computing with infinite networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  295--301, 1997.

\bibitem[Xiao(2022)]{xiao2022eigenspace}
Xiao, L.
\newblock Eigenspace restructuring: a principle of space and frequency in
  neural networks.
\newblock In \emph{Conference on Learning Theory}, pp.\  4888--4944. PMLR,
  2022.

\bibitem[Xu et~al.(2019)Xu, Zhang, Luo, Xiao, and Ma]{xu2019frequency}
Xu, Z.-Q.~J., Zhang, Y., Luo, T., Xiao, Y., and Ma, Z.
\newblock Frequency principle: Fourier analysis sheds light on deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1901.06523}, 2019.

\bibitem[Xu et~al.(2022)Xu, Zhang, and Luo]{xu2022overview}
Xu, Z.-Q.~J., Zhang, Y., and Luo, T.
\newblock Overview frequency principle/spectral bias in deep learning.
\newblock \emph{arXiv preprint arXiv:2201.07395}, 2022.

\bibitem[Yang(2019)]{yang2019scaling}
Yang, G.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Yang(2020)]{yang2020tensor}
Yang, G.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock \emph{arXiv preprint arXiv:2006.14548}, 2020.

\bibitem[Yang \& Salman(2019)Yang and Salman]{yang2019fine}
Yang, G. and Salman, H.
\newblock A fine-grained spectral perspective on neural networks.
\newblock \emph{arXiv preprint arXiv:1907.10599}, 2019.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2019)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.-J.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Martens, and Grosse]{zhang2019fast}
Zhang, G., Martens, J., and Grosse, R.~B.
\newblock Fast convergence of natural gradient descent for over-parameterized
  neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\end{thebibliography}
