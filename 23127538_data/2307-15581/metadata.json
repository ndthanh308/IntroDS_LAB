{
  "title": "Learning to Open Doors with an Aerial Manipulator",
  "authors": [
    "Eugenio Cuniato",
    "Ismail Geles",
    "Weixuan Zhang",
    "Olov Andersson",
    "Marco Tognon",
    "Roland Siegwart"
  ],
  "submission_date": "2023-07-28T14:28:32+00:00",
  "revised_dates": [],
  "abstract": "The field of aerial manipulation has seen rapid advances, transitioning from push-and-slide tasks to interaction with articulated objects. So far, when more complex actions are performed, the motion trajectory is usually handcrafted or a result of online optimization methods like Model Predictive Control (MPC) or Model Predictive Path Integral (MPPI) control. However, these methods rely on heuristics or model simplifications to efficiently run on onboard hardware, producing results in acceptable amounts of time. Moreover, they can be sensitive to disturbances and differences between the real environment and its simulated counterpart. In this work, we propose a Reinforcement Learning (RL) approach to learn motion behaviors for a manipulation task while producing policies that are robust to disturbances and modeling errors. Specifically, we train a policy to perform a door-opening task with an Omnidirectional Micro Aerial Vehicle (OMAV). The policy is trained in a physics simulator and experiments are presented both in simulation and running onboard the real platform, investigating the simulation to real world transfer. We compare our method against a state-of-the-art MPPI solution, showing a considerable increase in robustness and speed.",
  "categories": [
    "cs.RO"
  ],
  "primary_category": "cs.RO",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15581",
  "pdf_url": "https://arxiv.org/pdf/2307.15581v1",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 5782127,
  "size_after_bytes": 1341154
}