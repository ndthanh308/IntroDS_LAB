\section{Benchmark}
\label{sec:benchmark}

This section presents the selection and testing of various recent NeRF-based methods~\cite{muller2022instant,chen2022tensorf,sun2022direct} on the presented \textit{\ours} dataset, with a detailed description of the experimental setting for each baseline. Additionally, we assess the quality of the reconstructed vehicles in terms of their appearance and 3D surface reconstruction, utilizing depth maps generated during volume rendering.

\subsection{Compared methods}
To overcome challenges related to illumination and reflective surfaces during the process of reconstructing vehicles, it is crucial to choose an appropriate neural rendering approach.
We tested selected approaches on \textit{\ours} without modifying the implementation details available in the original repositories, whenever possible. However, some parameters had to be adjusted in order to fit our models (which are larger compared to reference dataset meshes) to the scene. All tests were performed on a GeForce GTX 1080 Ti. 
%We have decided that the chosen architectures should strike a balance between training time and the quality of the reconstruction to achieve a real-time system for vehicle inspection. 
After considering various NeRF systems, we have selected the following baselines:
\begin{itemize}
 \item \tit{Instant-NGP~\cite{muller2022instant}} Since the original implementation of Instant-NGP is in CUDA, we decided to use an available PyTorch implementation\footnote{\url{https://github.com/kwea123/ngp\_pl}} of this approach in order to have a fair comparison with the other approaches. In our experiments, a batch size of 8192 was maintained, with a scene scale of 0.5 and a total of 30,000 iteration steps.
 \item \tit{TensoRF~\cite{chen2022tensorf}} In our setting, a batch of 4096 rays was used. Additionally, we increased the overall scale of the scene from 1 to 3.5. These adjustments were made after experimentation and careful consideration of the resulting reconstructions. Training lasts 30,000 iterations.
\item \tit{DVGO~\cite{sun2022direct}} In this work, the training process consists of two phases: a coarse training phase of 5,000 iterations, followed by a fine training phase of 20,000 iterations that aims to improve the model's ability to learn intricate details of the scene. In our experiments, we applied a batch size of 8192 while maintaining the default scene size.
\end{itemize}

\begin{table*}[t]
    \centering
    \caption{Quantitative results on the \textit{\ours} test set for each vehicle model.}
    \begin{tabular}{l c c c c c c c c c c}
        \toprule
        Method & Metric & \textsc{Bmw} & \textsc{Tesla} & \textsc{Smart} & \textsc{Mbz$_1$} & \textsc{Mbz$_2$} & \textsc{Ford} & \textsc{Jeep} & \textsc{Volvo} & \textit{Avg} \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{PSNR$\uparrow$} & $39.48$ & $39.46$ & $39.57$ & $36.87$ & $39.15$ & $33.67$ & $35.00$ & $35.93$ & $37.39$ \\
        DVGO~\cite{sun2022direct} & & $39.91$ & $39.89$ & $40.34$ & $37.45$ & $39.37$ & $33.82$ & $35.32$ & $36.28$ & $37.80$ \\
        TensoRF~\cite{chen2022tensorf} & & $40.68$ & $39.92$ & $40.38$ & $38.07$ & $40.84$ & $34.33$ & $34.87$ & $36.77$ & $\mathbf{38.23}$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{SSIM$\uparrow$} & $0.985$ & $0.987$ & $0.988$ & $0.985$ & $0.987$ & $0.959$ & $0.978$ & $0.979$ & $0.981$ \\
        DVGO~\cite{sun2022direct} & & $0.987$ & $0.988$ & $0.990$ & $0.987$ & $0.988$ & $0.964$ & $0.980$ & $0.981$ & $0.983$ \\
        TensoRF~\cite{chen2022tensorf} & & $0.989$ & $0.987$ & $0.99$ & $0.989$ & $0.991$ & $0.966$ & $0.975$ & $0.982$ & $\mathbf{0.984}$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{LPIPS$\downarrow$} & $0.029$ & $0.029$ & $0.02$ & $0.028$ & $0.024$ & $0.062$ & $0.036$ & $0.032$ & $0.032$ \\
        DVGO~\cite{sun2022direct} & & $0.022$ & $0.022$ & $0.014$ & $0.019$ & $0.020$ & $0.051$ & $0.029$ & $0.022$ & $\mathbf{0.025}$ \\
        TensoRF~\cite{chen2022tensorf} & & $0.023$ & $0.026$ & $0.017$ & $0.02$ & $0.017$ & $0.051$ & $0.039$ & $0.027$ & $0.028$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{D-RMSE$\downarrow$} & $0.640$ & $0.369$ & $0.377$ & $0.496$ & $0.500$ & $0.406$ & $0.558$ & $0.674$ & $0.503$ \\
        DVGO~\cite{sun2022direct} & & $0.561$ & $0.353$ & $0.305$ & $0.437$ & $0.454$ & $0.339$ & $0.469$ & $0.561$ & $\mathbf{0.435}$ \\
        TensoRF~\cite{chen2022tensorf} & & $0.590$ & $0.357$ & $0.335$ & $0.467$ & $0.482$ & $0.375$ & $0.536$ & $0.626$ & $0.471$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{SN-RMSE$\downarrow$} & $4.24$ & $3.38$ & $3.41$ & $4.26$ & $4.13$ & $5.15$ & $4.60$ & $4.67$ & $4.23$ \\
        DVGO~\cite{sun2022direct} & & $4.27$ & $3.48$ & $3.20$ & $4.19$ & $4.24$ & $5.04$ & $4.67$ & $4.71$ & $4.22$ \\
        TensoRF~\cite{chen2022tensorf} & & $3.96$ & $3.24$ & $3.10$ & $4.00$ & $3.91$ & $4.91$ & $4.41$ & $4.48$ & $\mathbf{4.00}$ \\
        \bottomrule
    \end{tabular}
    \label{tab:category-results}
\end{table*}

\iffalse
\begin{itemize}
    \item \tit{Instant-NGP~\cite{muller2022instant}} MÃ¼ller \etal~presented a method that enhances the naive NeRF implementation's training speed significantly. Instant-NGP utilizes a multi-resolution hash encoding that learns both the weights of an MLP and a set of learning encoding parameters, which are organized into L levels. Essentially, Instant-NGP locates the surrounding voxels at various L resolutions based on an input coordinate x and performs a hashing search using the indices of the voxels' corners. This process retrieves the corresponding feature vectors in the hash table, which are then interpolated based on their position with respect to x. The input of the MLP is the concatenation of the features at different resolutions, resulting in the final output in terms of density and color. Since the original implementation of Instant-NGP is in CUDA, we decide to use an available PyTorch implementation of this approach in order to have a fair comparison with the other approaches.
    \item \tit{TensoRF~\cite{chen2022tensorf}} In their work, Chen \etal~introduced a method based on voxels that uses a condensed tensorial representation of radiance fields. Specifically, this technique combines the traditional CP decomposition~\cite{carroll1970analysis} with a new vector-matrix decomposition method, resulting in fewer components and leading to faster training and higher-quality reconstruction. By employing a tensorial factorization technique, TensoRF achieves excellent computational and memory efficiency without compromising the reconstruction quality.
    \item \tit{DVGO~\cite{sun2022direct}} This method presented by Sun \etal~involves encapsulating scene properties in a voxel grid, which allows for real-time rendering with high-quality reconstruction. However, optimizing the voxel grid directly can result in "cloud" noise in empty space. To address this, DVGO uses two solutions. Firstly, the density of the voxel grid is constrained to produce near-zero opacities. Secondly, voxels visible from fewer viewpoints are given a lower learning rate. By doing so, the method avoids converging to suboptimal geometries. 
\end{itemize}
\fi

\iffalse
\subsection{Implementation details}
For each of the studied works, we tested the implementations without modifying the default settings used by the repositories for the Blender dataset. However, some parameters had to be adjusted because our models are larger and therefore the scene is different from the reference dataset. All tests were performed on a GeForce GTX 1080 Ti. For more implementation details, please refer to the original repositories cited in the paper[ , , ].

\begin{itemize}
    \item \textbf{Instant-NGP} 
    For the Instant-NGP implementation, a batch size of 8192 was maintained, with a scale of 0.5 and a total of 30,000 iteration steps. 
    \item \textbf{TensoRF} 
    In our implementation, a batch of 4096 rays was used, and we found it necessary to tune some hyperparameters to obtain an acceptable reconstruction of the scenes. Specifically, we adjusted the values of "near" and "far" from 4-6 to 2 and 10, respectively. Additionally, we increased the overall scale of the scene from 1 to 3.5. These adjustments were made after experimentation and careful consideration of the resulting reconstructions. It's worth noting that hyperparameters may need to be adjusted depending on the specific dataset and model being used.
    \item \textbf{DVGO} 
\end{itemize}
\fi

\begin{table*}[t]
    \centering
    \caption{Quantitative results on the \textit{\ours} test set for each vehicle component averaged over the vehicle models.}
    \begin{tabular}{l c c c c c c }
        \toprule
        Method & Component & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & D-RMSE$\downarrow$ & SN-RMSE$\downarrow$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{\textit{bumper}} & $33.05$ & $0.986$ & $0.019$ & $0.281$ & $0.79$ \\
        DVGO~\cite{sun2022direct} & & $34.41$ & $0.989$ & $0.011$ & $\mathbf{0.236}$ & $0.72$ \\
        TensoRF~\cite{chen2022tensorf} & & $\mathbf{35.49}$ & $\mathbf{0.991}$ & $\mathbf{0.010}$ & $0.311$ & $\mathbf{0.68}$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{\textit{light}} & $28.71$ & $0.993$ & $0.009$ & $0.421$ & $0.48$ \\
        DVGO~\cite{sun2022direct} & & $29.10$ & $0.995$ & $\mathbf{0.006}$ & $\mathbf{0.384}$ & $0.43$ \\
        TensoRF~\cite{chen2022tensorf} & & $\mathbf{29.68}$ & $\mathbf{0.996}$ & $\mathbf{0.006}$ & $0.438$ & $\mathbf{0.38}$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{\textit{mirror}} & $29.60$ & $0.994$ & $0.011$ & $0.427$ & $0.43$ \\
        DVGO~\cite{sun2022direct} & & $31.16$ & $\mathbf{0.996}$ & $\mathbf{0.007}$ & $\mathbf{0.345}$ & $\mathbf{0.38}$ \\
        TensoRF~\cite{chen2022tensorf} & & $\mathbf{31.68}$ & $\mathbf{0.996}$ & $0.008$ & $0.372$ & $0.39$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{\textit{hood/trunk}} & $32.28$ & $0.977$ & $0.052$ & $0.260$ & $1.33$ \\
        DVGO~\cite{sun2022direct} & & $32.68$ & $0.981$ & $\mathbf{0.038}$ & $\mathbf{0.259}$ & $1.35$ \\
        TensoRF~\cite{chen2022tensorf} & & $\mathbf{33.75}$ & $\mathbf{0.983}$ & $0.040$ & $0.302$ & $\mathbf{1.24}$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{\textit{fender}} & $32.44$ & $0.990$ & $0.021$ & $0.253$ & $0.87$ \\
        DVGO~\cite{sun2022direct} & & $33.55$ & $\mathbf{0.993}$ & $\mathbf{0.013}$ & $\mathbf{0.223}$ & $0.85$ \\
        TensoRF~\cite{chen2022tensorf} & & $\mathbf{34.36}$ & $\mathbf{0.993}$ & $0.015$ & $0.267$ & $\mathbf{0.77}$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{\textit{door}} & $34.19$ & $0.969$ & $0.079$ & $0.182$ & $0.67$ \\
        DVGO~\cite{sun2022direct} & & $35.48$ & $0.977$ & $\mathbf{0.042}$ & $\mathbf{0.173}$ & $0.74$ \\
        TensoRF~\cite{chen2022tensorf} & & $\mathbf{36.25}$ & $\mathbf{0.979}$ & $0.051$ & $0.191$ & $\mathbf{0.62}$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{\textit{wheel}} & $33.12$ & $0.995$ & $0.008$ & $0.391$ & $0.87$ \\
        DVGO~\cite{sun2022direct} & & $33.65$ & $0.995$ & $0.006$ & $\mathbf{0.267}$ & $\mathbf{0.79}$ \\
        TensoRF~\cite{chen2022tensorf} & & $\mathbf{34.55}$ & $\mathbf{0.996}$ & $\mathbf{0.005}$ & $0.334$ & $\mathbf{0.79}$ \\
        \midrule
        iNGP~\cite{muller2022instant} & \multirow{3}{*}{\textit{window}} & $26.44$ & $0.897$ & $0.166$ & $0.879$ & $2.52$ \\
        DVGO~\cite{sun2022direct} & & $26.54$ & $\mathbf{0.899}$ & $\mathbf{0.147}$ & $\mathbf{0.779}$ & $2.57$ \\
        TensoRF~\cite{chen2022tensorf} & & $\mathbf{26.74}$ & $0.896$ & $0.160$ & $0.834$ & $\mathbf{2.38}$ \\
        \bottomrule
    \end{tabular}
    \label{tab:panels-results}
\end{table*}

\subsection{Metrics}
\label{sec:metrics}
The effectiveness of the chosen methods has been assessed thanks to the typical perceptual metrics used in NeRF-based reconstruction tasks, namely PSNR, SSIM~\cite{wang2004image}, and LPIPS~\cite{zhang2018unreasonable}. 

However, the appearance-based metrics are strongly related to the emitted radiance besides the learned volume density. We suggest two supplementary depth-based metrics for the sole purpose of assessing the volume density. Since it is not feasible to obtain ground truth 3D models of the vehicles in real-world scenarios, we utilize the depth map as our knowledge of the 3D surface of the objects. Specifically, we define a depth map as a matrix
\begin{equation}
    D = \{d_{ij}\}, d_{ij} \in [0, R]
\end{equation}
in which each value $d_{ij}$ ranges from $0$ to the maximum depth value $R$. Furthermore, we estimate the surface normals from the depth maps~\cite{Pini21Depth}. Initially, we establish the orientation of a surface normal as:
\begin{equation}
    \mathbf{d} = \langle d_x, d_y, d_z \rangle = \left(-\frac{\partial d_{ij}}{\partial i}, -\frac{\partial d_{ij}}{\partial j}, 1 \right) \approx \left( d_{(i+1)j} - d_{ij}, d_{i(j+1)} - d_{ij}, 1 \right)
\end{equation}
where the first two elements represent the depth gradients in the $i$ and $j$ directions, respectively. Afterward, we normalize the normal vector to obtain a unit-length vector $\mathbf{n}(d_{ij}) = \frac{\mathbf{d}}{\left\lVert \mathbf{d} \right\rVert}$.

%\begin{equation}
%    \label{eq:normals}
%    \mathbf{n}(d_{ij}) = \frac{\mathbf{d}}
%{\left\lVert \mathbf{d} \right\rVert}
%\end{equation}
We assess the 3D reconstruction's quality through the following metrics:
\begin{itemize}
    \item \tit{Depth Root Mean Squared Error (D-RMSE)} This metric measures the average difference in meters between the ground truth and predicted depth maps.
    \begin{equation}
        \textnormal{D-RMSE} = \sqrt{\frac{\sum_{i=0}^{M}\sum_{j=0}^{N} (\hat{d}_{ij} - d_{ij})^2}{M \cdot N}}
    \end{equation}
    \item \tit{Surface Normal Root Mean Squared Error (SN-RMSE)} This metric measures the average angular error in degrees between the angle direction of the ground truth and predicted surface normals.
    \begin{equation}
        \textnormal{SN-RMSE} = 
        \sqrt{\frac{\sum_{i=0}^{M}\sum_{j=0}^{N} (\arccos(\mathbf{n}(\hat{d}_{ij})) - \arccos(\mathbf{n}(d_{ij})))^2}{M \cdot N}}
    \end{equation}
\end{itemize}
D-RMSE and SN-RMSE are computed only for those pixels with a positive depth value in both GT and predicted depth maps. This avoids computing depth estimation errors on background pixels (which have a fixed depth value of 0).

% Figure environment removed

% Figure environment removed

\subsection{Results}
The following section presents both quantitative and qualitative results obtained from the selected NeRF baselines. We will discuss their performance on the \textit{\ours} dataset, by analyzing the impact of viewing camera angle and the number of training images.

%\tit{Quantitative results} 
According to Table~\ref{tab:category-results}, all the selected NeRF approaches obtain satisfying results. Although the baselines demonstrate similar performances in terms of appearance scores (PSNR, SSIM, and LPIPS), our evaluation using depth-based metrics (D-RMSE and SN-RMSE) reveals significant differences in the 3D reconstruction of the vehicles. DVGO outperforms its competitors by achieving better depth estimation, resulting in a $+13.5\%$ improvement compared to iNGP and a $+7.6\%$ improvement compared to TensoRF. In contrast, TensoRF predicts a more accurate 3D surface with the lowest angular error on the surface normals.
%, as computed in Equation~\ref{eq:normals}.

Since our use case is related to vehicle inspection, in Table~\ref{tab:panels-results} we report results computed on each car component. For this purpose, we mask both GT and predictions using a specific component mask before computing the metrics. However, this would lead to an unbalanced ratio between background and foreground pixels, due to the limited components' area, and finally to a biased metric value. By computing D-RMSE and SN-RMSE only on foreground pixels (see Sec.~\ref{sec:metrics}), depth-based metrics are not affected by this issue. For PSNR, SSIM, and LPIPS, instead, we compute component-level metrics over the image crop delimited by the bounding boxes around each mask.
As expected, it is worth noting that NeRF struggles to reconstruct transparent objects (\eg~mirrors, lights, and windows) obtaining the highest errors in terms of depth and normal estimation. However, over the single components, TensoRF outperforms the competitors in most of the metrics and in particular on the surface normal estimation. The errors in the reconstruction of specific components' surfaces can also be appreciated in the qualitative results of Fig.~\ref{fig:qualitatives}.

Moreover, we analyze the performances of each method in terms of the number of training images. We trained the baselines on every version of the \textit{\ours} dataset and report the results in Fig.~\ref{fig:metrics_num_imgs}. It is worth noting that reducing the number of training images has a significant impact on all the metrics independently of the method. However, Instant-NGP demonstrates to be more robust to the number of camera viewpoints having a smoother drop, especially in terms of LPIPS, D-RMSE, and SN-RMSE.

Finally, we discuss how the training camera viewpoints' distribution around the vehicle may affect the performance of each method from certain camera angles. In particular, as depicted in Fig.~\ref{fig:polar_plot}, it is evident how between $180^\circ$ and $270^\circ$ and between $0^\circ$ and $45^\circ$ there are considerable variations in the metrics. Indeed, in these areas the datasets contain more sparsity in terms of camera viewpoints and, as expected, all the methods are affected.

% Figure environment removed