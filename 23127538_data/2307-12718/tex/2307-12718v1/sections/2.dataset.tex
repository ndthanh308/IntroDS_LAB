\section{The \textit{\ours} dataset}

% Figure environment removed

\label{sec:dataset}
In this section, we detail the source data and the procedure exploited for generating our \textit{\ours} dataset. In particular, we describe how we gathered 3D models, set up the blender scenes, and designed the image capture process.

\subsection{Synthetic 3D models and scene setup}
All the 3D models included in \textit{\ours} scenes have been downloaded from Sketchfab\footnote{\url{https://sketchfab.com}}, a large collection of free 3D objects for research use.
Table \ref{tab:3d_models} provides a detailed list of all the starting models used. Each of them has been edited in Blender to enhance its realism; specifically, we improved the materials, colors, and lighting in each scene to create a more challenging environment.

The scenes have been set up accordingly to the Google Blender dataset~\cite{mildenhall2021nerf}. The lighting conditions and rendering settings were customized to create a more realistic environment. The vehicle was placed at the center of the scene at position (0,0,0), with nine lights distributed around the car and varying emission strengths to create shadows and enhance reflections on the materials' surfaces. To improve realism, we resized objects to match their real-world size. The camera and lights were placed in order to provide an accurate representation of the environment, making the scenes similar to real-world scenarios.


\begin{table*}[t]
    \centering
    \caption{Summary of the source 3D models from which our dataset has been generated, including their key features.}
    \begin{tabular}{l c c c c c }
        \toprule
        Model name & Acronym & \#Triangles & \#Vertices & \#Textures & \#Materials\\
        \midrule
        Tesla Model & \textsc{Tesla} & 684.3k & 364.4k & 22 & 58 \\
        Smart & \textsc{Smart} & 42.8k & 26.4k & 0 & 31 \\
        Ford Raptor & \textsc{Ford} & 257.1k & 156.5k & 12 & 50 \\
        BMW M3 E46 & \textsc{Bmw} & 846.9k & 442.4k & 7 & 39 \\
        Mercedes GLK & \textsc{Mbz$_1$} & 1.3M &  741.4k & 0 & 15 \\
        Mercedes CLS & \textsc{Mbz$_2$} & 1.0M & 667k & 0 & 18 \\
        Volvo S90 & \textsc{Volvo} & 3.3M & 1.7M & 56 & 44 \\
        Jeep Compass & \textsc{Jeep} & 334.7k & 189.6k & 7 & 39 \\
        \bottomrule
    \end{tabular}
    \label{tab:3d_models}
\end{table*}

\subsection{Dataset building}
The dataset was built using the Python interface provided in Blender, allowing us to control objects in the environment. For each rendered image, we captured not only the RGB color values but also the corresponding depth map, as well as the pixel-wise semantic segmentation masks for eight vehicle components: bumpers, lights, mirrors, hoods/trunks, fenders, doors, wheels, and windows. Examples of these segmentation masks can be seen in Fig.~\ref{fig:first_page}. Please note that all the pixels belonging to a component (\eg~doors) are grouped into the same class, regardless of the specific component location (\eg~front/rear/right/left door). The bpycv\footnote{\url{https://github.com/DIYer22/bpycv}} utility has been used for collecting additional metadata, enabling us to evaluate NeRF models on the RGB reconstruction and depth estimation of the overall vehicle as well as each of its subparts.

For the rendering of training images, the camera randomly moved on the hemisphere centered in (0,0,0) and above the ground. The camera rotation angle was sampled from a uniform distribution before each new capture. For building the test set, the position of the camera was kept at a fixed distance from the ground and rotated around the Z-axis with a fixed angle equal to $\frac{2\pi}{\#test\_views}$ radians before each new capture.

In order to guarantee the fairness of the current and future comparisons, we explicitly provide four different versions of each scene, by varying the number of training images (40, 60, 80, and 100 images, respectively). Different versions of the same scene have no overlap in training camera poses, while the test set is always the same and contains 200 images for each scene.\\ We release the code for dataset creation and metrics evaluation at \url{https://github.com/davidedinuc/carpatch}.
