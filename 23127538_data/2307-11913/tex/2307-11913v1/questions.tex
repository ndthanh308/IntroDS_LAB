\section{Discussion}

In this work, we have given the first efficient offline and online algorithms with non-trivial guarantees for \wtd. Several interesting problems remains open:
\begin{enumerate}
\item For the case of two distinct weight classes, we show in
  \Cref{sec:hardness} that it is
  UG-Hard to obtain an $\Omega(N^c)$-approximation algorithm for some
  constant $c>0$, even with $(2-\varepsilon)$-resource
  augmentation. Can we extend such a hardness result to more weight classes? For
  example, can we show that for three distinct weight classes, it is
  UG-Hard to obtain a $C$-approximation algorithm for any {\em
    constant} $C$, even with $(3-\varepsilon)$-resource augmentation?
  The principal reason why our hardness proof for $\ell=2$ does not
  extend here is because one needs to recursively cycle through all
  subsets (of a certain size) of $V$ to create an integrality gap
  instance for the natural LP relaxation. If the size of these subsets
  is large, then the length of the input becomes very large. If the
  size of these subsets is small, then it is not clear how to extend
  this to a hardness proof.
\item In \Cref{sec:offline}, we give an offline constant approximation algorithm which
  requires slightly more than $2 \ell$-resource augmentation. Can we
  get a constant approximation algorithm (or even an optimal
  algorithm) with exactly $\ell$-resource augmentation? We conjecture
  that the integrality gap of~\ref{LP:tag} is constant (or even $1$)
  if the integral solution is allowed $\ell$-resource augmentation.
\item In the online case, we give a $O(\ell^2 \log \ell)$-competitive
  algorithm with $2\ell$-resource augmentation in \Cref{sec:online}. Can we get a
  constant-competitive algorithm with $O(\ell)$-resource augmentation,
  i.e., a result in the same vein as our offline algorithm?
\end{enumerate}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
