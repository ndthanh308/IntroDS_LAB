\section{Introduction}
\label{sec:intro}

The \kser problem is a foundational problem in online algorithms and
has been extensively studied over the last 30
years~\cite{onlinebook}. In this problem, there are a set of $k$ servers
that must serve requests arriving online at the vertices of an
$n$-point metric space. The goal is to minimize the total movement
cost of the servers. The \kser problem was defined by Manasse et al.~\cite{ManasseMS90}, who also showed a lower bound of $k$ on the competitive ratio of any deterministic algorithm for this problem. Koutsoupias and Papadimitriou~\cite{KoutsoupiasP95} gave a $(2k-1)$-compeititive algorithm for \kser.   There has been much progress in the recent past on obtaining randomized algorithms with polylogarithmic (in $k$ and $n$) competitive ratio~\cite{BansalBMN15, BubeckCLLM18, Lee18, BuchbinderGMN19}. 
The \wtd version of this problem, introduced by
Fiat and Ricklin~\cite{FiatR94}, allows the servers to have non-uniform
positive weights; the cost of moving a server is now scaled by its
weight. In this paper, we consider the \wtd problem on a uniform
metric, namely when all $n$ points of the metric space are at unit
distance from each other, which means that the cost of moving a server
between any two distinct points is simply the weight of the
server. Note that the corresponding unweighted problem for the uniform
metric % , where the servers have equal weight,
is the extensively studied \page problem~\cite{onlinebook}. Indeed, one of
the original motivations for studying the \wtd problem came from a
version of paging with non-uniform replacement costs for different
cache slots~\cite{FiatR94}. In the rest of this paper, we % when we talk of
% the \wtd problem, we 
will implicitly assume that the underlying metric space is a uniform
metric.

The original paper of Fiat and Ricklin~\cite{FiatR94} introducing the
\wtd problem (on uniform metrics) gave a deterministic algorithm with
a competitive ratio of about $2^{2^{2k}}$. They also showed a lower
bound of $(k + 1)!/2$ for deterministic algorithms. Chiplunkar and
Viswanathan~\cite{ChiplunkarV} improved this lower bound to
$(k+1)!-1$, and gave a randomized algorithm that is
$1.6^{2^k}$-competitive against \emph{adaptive} online adversaries;
this also implies a deterministic competitive ratio of $2^{2^{k+1}}$
using the simulation technique of Ben-David et
al.~\cite{Ben-DavidBKTW94}. Bansal, Elias, and
Koumutsos~\cite{BansalEK} showed that this competitive ratio is
essentially tight for deterministic algorithms by showing a lower
bound of $2^{2^{k-4}}$. They also gave a deterministic {\em work
  function algorithm} with a competitive ratio of
$2^{2^{k+O(\log k)}}$. If the number of distinct server weights is
$\ell$ and there are $k_j$ servers of weight $W_j$, then the
competitive ratio of their algorithm is
$\exp(O(\ell k^3 \prod_{j=1}^\ell (k_j + 1)))$, which is an exponential
improvement over the general bound when $\ell$ is a
constant. Unlike
the \kser and \page problems, it is unknown if randomization
qualitatively improves the competitive ratio for \wtd, although the
best known lower bound for randomized algorithms against oblivious
adversaries is only singly exponential in $k$~\cite{AyyadevaraC21} as
against the doubly exponential lower bound for deterministic
algorithms.

The above competitive ratios depend only on $k$, and are
  independent of the size $n$ of metric space. Moreover, the hard
  instances are for metric spaces with the number of points $n$ that
  are exponentially larger than the number of servers $k$. This is not
  a coincidence, since better algorithms exist for smaller values of
  $n$. Indeed, the \wtd problem can be modeled as a metrical task
  system, where each state $\omega$ is a configuration (specifying the
  location of each of the $k$ servers), and the distance between any
  two states $\omega, \omega'$ is the cost to move between the
  configurations. Since there are $N = n^k$ states, one can obtain an
  $n^k$-competitive deterministic algorithm~\cite{BLS}, and an
  $O(\poly(k \log n))$-competitive randomized algorithm against
  \emph{oblivious} adversaries~\cite{BBBT,BBN,BCLL,CoesterL}; all these algorithms
  use $\poly(n^{k})$ time to explicitly maintain and manipulate the
  entire metric space, and hence are not efficient.

  In this paper we ask: \emph{is it possible to get efficient (randomized)
  \emph{online} algorithms that have competitive ratios of the form
  $\poly(k \log n)$, or even better? Is it possible to get such approximation ratios
  even in the \emph{offline} setting?} We show that obtaining improved competitive or approximation ratios in polynomial time is possible, provided we allow for {\em resource augmentation} in the number of servers. 

  Resource augmentation in online algorithms has been widely studied in paging and scheduling settings~(see e.g. \cite{KalyanasundaramP95, SleatorT85}). It is often a much needed assumption that allows for obtaining bounded or improved competitive ratios for such problems. Bansal et al.~\cite{BansalEJK19} studied the \kser problem on trees under resource augmentation. 


\subsection{Our Results}
\label{sec:our-results}

Our first result establishes computational hardness of approximating
the \wtd problem in the offline setting. Unlike \page or \kser, which
are exactly solvable offline in polynomial time, we show that under
the Unique Games conjecture, the \wtd problem cannot be approximated to any subpolynomial factor even when we allow $c$-resource augmentation for any constant $c < 2$.

\begin{restatable}[Hardness]{theorem}{Hardness}
\label{thm:hard2}
For any constant $\eps > 0$, it is UG-hard to obtain an $N^{\nf{1}{2}-\eps}$-approximation algorithm for \wtd  with two weight classes, even when we are allowed $c$-resource augmentation for any constant $c < 2$. Here $N$ represents the size of the input (including the request sequence length). 
\end{restatable}

Next, we show that the natural time indexed LP relaxation for \wtd (see~\ref{LP:tag}) has a
large integrality gap, unless we allow for a resource augmentation of
almost $\ell$, the number of distinct server weights.

\begin{restatable}[Integrality Gap]{theorem}{IntGap}
  \label{thm:int}
  For any constant $\varepsilon >
  0$, the integrality gap of the relaxation~\ref{LP:tag} for \wtd
  is unbounded, even with $(\ell-\varepsilon)$-resource augmentation.
\end{restatable}

 It is worth noting  that an optimal fractional solution to~\ref{LP:tag} can be easily rounded to give an $\ell$-approximation ratio with $\ell$-resource augmentation. Indeed, we know that for each request, there exists a weight class which services this request to an extent of at least $\nf{1}{\ell}$. We can then scale this fractional solution by a factor $\ell$ and reduce this problem to $\ell$ instances of standard \page problem. The integrality gap result shows that any rounding algorithm with bounded competitive ratio must incur almost $\ell$-resource augmentation. 
We complement this integrality gap result with our main technical result, which gives an offline 
$O(1/\eps)$-approximation with $(2+\eps)\ell$-resource augmentation,
for any $\eps\in (0, 1)$.

\begin{restatable}[Offline Algorithm]{theorem}{Offline}
  \label{thm:main}
  Let $\cI$ be an instance of \wtd with $k_j$ servers of weight $W_j$ for all $j \in [\ell]$. 
  For any $\eps \in (0, 1)$, there is a polynomial time algorithm for $\cI$ that uses at most $2(1+\eps)\ell \cdot k_j$ servers of weights $W_j$ for each $j \in [\ell]$ and has server movement cost at most $O(1/\eps)$ times the optimal cost of $\cI$.
\end{restatable}

Finally, we obtain an online algorithm for \wtd with $2\ell$-resource
augmentation. The competitive ratio of the online algorithm is
$O(\ell^2\log \ell)$. (In constrast to the offline setting, it is no
longer clear how to achieve an $\ell$-competitive algorithm even if
we augment the number of servers by a factor of $\ell$.)

\begin{restatable}[Online Algorithm]{theorem}{Online}
  \label{thm:online}
  Let $\cI$ be an instance of \wtd with $k_j$ servers of weight $W_j$ for all $j \in [\ell]$. 
  There is a randomized (polynomial time) online algorithm for $\cI$ that uses at most $2\ell k_j$ servers of weights $W_j$ for each $j \in [\ell]$ and has expected server movement cost at most $O(\ell^2 \log \ell)$ times the optimal cost of $\cI$.
\end{restatable}

Since $\ell \le k$, the competitive ratio of the 
online algorithm is $O(k^2\log k)$. This implies that an $O(\ell^2)$-resource augmentation 
achieves at least an exponential improvement in the 
competitive ratio of the \wtd problem. Moreover, 
by rounding the weights to powers of $2$, we can assume that
$\ell \le O(\log W)$, where $W$ is the aspect ratio of the 
server weights. Hence, the competitive ratio of the 
online algorithm is $O(\log^2 W \log \log W)$.
Finally, note that for $\ell = O(1)$,
the above theorem gives a $O(1)$-competitive online algorithm with 
$O(1)$-resource augmentation. This can be seen as a generalization
of the classic result for the \page problem that achieves
a randomized competitive ratio of $O(\log \frac{k}{k-h+1})$
where the algorithm's cache has $k$ slots while the 
adversary's has only $h < k$ slots~\cite{Young91}.

\subsection{Our Techniques}
\label{sec:techniques}

In this section, we give an overview of the main techniques in the paper. The UG hardness of \wtd is based on a reduction from the \vc problem. Given an instance of the vertex cover problem, the corresponding \wtd consists of one point in the uniform metric space for each vertex of the graph.  The main observation is that if we know the minimum vertex cover size, we can keep one heavy weight server at each point corresponding to this vertex cover, which never change their positions. One can then generate an input sequence where the optimal solution pays a small cost, whereas an algorithm which does not cover an edge using heavy servers pays a much higher cost. The UG-hardness result for \vc translates to a corresponding resource augmentation lower bound for \wtd. Extending this approach to more than two weight classes (with stronger lower bounds on resource augmentation) turns out to be more challenging because the length of the input sequence becomes exponential in $n$. Instead, we show that the natural LP relaxation has a large integrality gap. The large gap instance consists of cycling through a sequence of subsets of the metric spaces with carefully varying frequency. The fractional solution is able to maintain a low cost by uniformly spreading servers over such cycles, but the integral solution is forced to service some of the cycles by small number of servers only. 

Our main technical result shows how to round a solution to the LP relaxation. The relaxation has both covering and packing type constraints, and the rounding carefully addresses one set of constraints without violating the other. We first scale the LP by a factor of about $2 \ell$, thus increasing both the resource augmentation and the cost. As a result, each request $\sigma_t$ is covered to an extent of $2\ell,$  and we can split this coverage across those weight classes which cover $\sigma_t$ to an extent of at least 1. Now for a fixed weight class, we consider the requests which are covered by it to an extent of at least 1. We show how to integrally round this solution so that this coverage property is satisfied and yet, we do not violate any packing constraint. After this, we show that the packing constraints can be ignored. This allows to scale down the LP solution by a factor $\ell$ (which saves the cost by this factor) and uses total unimodularity of the constraint matrix to round it. 

We extend our approximation algorithm to the online setting. The first step is to maintain an online fractional solution to the LP relaxation. Standard (weighted) paging algorithms for this problem rely on the fact that even the optimal offline algorithm needs to place a server at a requested location. But this turns out to be trickier here as we do not know the weight of the server which serves this location in the optimal solution. So we serve a request by ensuring that fractional mass from each weight classes is transferred at the same rate. The overall analysis proceeds by a careful accounting in the potential function. The online fractional solution satisfies the stronger guarantee that each request is served by servers of a particular weight class only. This allows us to reduce the rounding problem to independent instances of the \page problem. 


We now give an overview of the rest of the submission.  In \S\ref{sec:int-gap}, we give details of the integrality gap construction; we defer the UG hardness proof to \S\ref{sec:hardness}. The offline rounding of the LP relaxation is given in \S\ref{sec:offline}, and then we extend this algorithm to the online case in \S\ref{sec:online}. 

\subsection{Preliminaries}
\label{sec:prelims}

In the \wtd problem on the uniform metric, we are given a set of $n$
points $V=\{1, \ldots, n\}$, such that $d(v,v') = 1$ for each
$v \neq v'$. There are $k$ servers, labeled $1, \ldots, k$, with
server $i$ having weight $w_i \geq 0$. The input specifies a request
sequence $(\sigma_1, \ldots, \sigma_T)$ of length $T$, with each
request $\sigma_t$ arriving at {\em time} $t$ being a point in $V$.  A
solution $f: [k] \times \{0,\ldots, T\} \to V$ specifies the position
of each server at each time $t \in [T]$ (where the initial positions
$f(i,0)$ are specified as part of the problem statement) such that for
each time $t$ there exists some server $i_t$ such that
$f(i_t,t) = \sigma_t$. The cost of the solution $f$ is the total
weighted distance travelled by the servers, i.e.,
$$ \nicefrac12 \sum_{i=1}^k w_i \; \sum_{t=1}^T \mathbbm{1}[f(i,t) \neq f(i,t-1)]. $$
The goal is to find a solution with the minimum cost. 
We say that an instance has $\ell$ {\em weight classes} if the set $\{w_1, \ldots, w_k\}$ has cardinality $\ell$. 
For an instance with $\ell$ different weight classes, we denote the
distinct weights by $W_1, \ldots, W_\ell$, and  let $k_j$ denote the
number of servers of weight $W_j$, with $\sum_j k_j = k$.
%Let $V_1, V_2, \ldots, V_\ell$ be the set of servers with weight $W_1,
%W_2, \ldots, W_\ell$ respectively.
For such an instance and a parameter $c \geq 1$, we say that the
algorithm uses \emph{$c$-resource augmentation} if it  uses $\lfloor
ck_j \rfloor$ servers of weight $W_j$ for each $j=1, \ldots, \ell$. 

We now describe the  natural LP relaxation for \wtd. It has a variable $x(v,j,t)$ for each request
time $t$, weight class $j \in [\ell]$ and vertex $v \in V$; it denotes
the fractional mass of servers of weight $W_j$ that are present at
point $v$ at time $t$. Let $\sigma_t$ denote the vertex requested at
time $t$. It is easy to verify that this is a valid relaxation. %Then the LP relaxation is as follows:
\begin{alignat}{2}
    \label{LP:tag}
    \tag{LP}
    \min \nicefrac12 \sum_{j \in [\ell]}  W_j  \sum_{t} \sum_{v \in V} &
    \; |x_{v,j,t}-x_{v,j,t-1}| && \\
    \sum_{v \in V} x_{v,j,t} & \leq k_j &\quad &\forall t, j \in [\ell]  \label{eq:cons1} \\
    \sum_{j \in [\ell]} x_{\sigma_{t},j,t} & \geq 1 & &\forall t \label{eq:cons2} \\
    x_{v,j,t} & \geq 0 && \forall t,v \in V,j \in [\ell] \notag
\end{alignat}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
