\section*{Methods}
%Why AutoPet dataset
\label{S: Methods}
\subsection*{Data Acquisition}
%Why CT

Two of the most common volumetric modalities are CT and MRI. 
While MRI often focuses on soft tissue analysis and brain imaging, CT is a common choice in the clinical routine due to its acquisition time and broad field of use. As we aim to generate models to segment any anatomy utilizing various sources, we start by selecting a dataset that acts as a solid basis for full-body label aggregation.
% Why AuotPET
The recently published AutoPET dataset~\cite{gatidis2022whole} is a PET-CT dataset that perfectly fits our requirements since nuclear medicine often requires full-body CT scans to track therapy. In addition to the full-body CTs, this dataset might enable future multi-modal segmentation tasks~\cite{xue2021multi, marinov2023mirror} due to the separate PET domain and lesion annotations. Future multimodal tasks could make use of the provided anatomical structures, which, however, is not the focus of this work.

%Which volumes have been selected? 
%Why did we choose a subset?
We select a subset of $566$ CTs of the AutoPET dataset. The selection criterion is based on similar slice thickness in the axial dimension leading to a homogeneous dataset. Furthermore, we make sure that the images show important regions of interest. Our region of interest starts from the head and ends slightly below the hip which includes all thoracic and abdominal organs. The chosen subset consists of the CTs which contain between $336$ and $400$ slices in the original dicom files. We exclude CTs with fewer slices as these tend to show an insufficient subpart of the body contradicting the desired full-body dataset. CT images containing more than 400 slices tend to include more irrelevant content. %background and anatomical structures outside of our region of interest such as the legs. 
This leaves us with a homogeneous dataset of size $566$. Our final DAP Atlas dataset does deviate from this selection, as we filter out implausible predictions, in a final post-processing step leaving us with $533$ CT images. The filter criteria will be described in detail later. In the following, we will refer to the DAP Atlas dataset as the dataset containing $533$ images. 

%Description of the subset
Our DAP Atlas is similar to AutoPET regarding the age and gender distributions as well as pathological findings. We show a descriptive analysis of the dataset regarding the aforementioned dimensions in Fig.~\ref{fig:descriptive_statistics}. 

% Figure environment removed

%Maybe we need a table showing average 

\subsection*{Knowledge Acquisition}


 % Figure environment removed
% % Figure environment removed


%Where do the individual labels come from
The dataset aggregates multiple sources of anatomical segmentation knowledge which we differentiate into public knowledge which is present in the form of publicly available datasets and private knowledge which are private datasets available to us. Besides the segmentation knowledge, we leverage rule-based knowledge which is derived from anatomical textbook knowledge and represents what could be described as the common sense of a radiologist. These rules contain for instance, which anatomical structures are possible in which part of the human body. We display the DAP Atlas construction workflow in Fig.~\ref{fig:merging} and discuss the details in the following.

A large amount of labels of the dataset is derived from public segmentation knowledge which is present in fragmented form through publicly available datasets. These contain annotations of organs of interest on CT images showing parts of the human body. We extract this knowledge by training neural networks on these public datasets, which learn to predict the labels of the respective datasets. Typically, training a neural network is a data and task-specific problem and requires finetuning a large set of hyperparameters which is impracticable for our desired applications as we intend to train a vast number of models on various heterogeneous datasets. To overcome this problem, we use the nnU-Net~\cite{isensee2021nnu}, a framework that automatically configures a U-Net and adapts the training procedure to the data at hand. This Auto-ML framework provides segmentation results surpassing several more complex works without any of their additional engineering overhead. We employ standard nnU-Nets and train them on publicly available datasets. The used publicly available datasets are shown along with their obtained label category in Fig.~\ref{Tab:Label_Usage}. After training, these networks are used to carry over the extracted knowledge by predicting the learned labels into our full-body selected DAP Atlas CT images. We describe the used datasets and the merging procedure in the following. %and show more detailed information about the used datasets and the source of each label in the supplementary information. 

%TODO: Include pathological labels?

\input{tabel_labels}

%Brief description of the datasets
\begin{itemize}
    \item \textbf{Pediatric~\cite{jordan2022pediatric}:} This dataset consists of $359$ chest-abdomen-pelvis and abdomen-pelvis CT images of patients between the age of $5$ and $16$ years. It provides $29$ anatomical structures annotated by experts. Patients were selected based on random clinical indications from the university clinic of Children's Wisconsin.
    \item \textbf{Total Segmentator~\cite{wasserthal2022totalsegmentator}:} The TotalSegmentator dataset is large and diverse with $1024$ CT images of different body parts with labels for $104$ anatomical structures. The dataset was collected by randomly sampling from the PACs systems of multiple sites. Its annotation is based on an interactive semi-automatic approach. Here, models are first trained on a few manual annotations. These models infer predictions on unlabeled scans which are lastly refined by an expert. This cycle repeats with an ever-increasing number of training images.
    \item \textbf{SegThor~\cite{lambert2020segthor}:} A dataset consisting of $60$ thoracic CTs collected at the Henri Becquerel Center. The patients were selected based on lung cancer or Hodkin's lymphoma diagnosis. The CTs contain annotations for four organs at risk whose tissues must remain intact during radiation therapy. The annotations of the dataset are provided by an experienced radiotherapist. 
    \item \textbf{CT50Abdomen~\cite{ma2021abdomenct}:} The dataset is part of the CT1k Abdomen datasets extension in which the authors provide 50 abdominal CT images with previously less annotated structures such as the adrenal glands. Annotations are provided by multiple junior annotators and checked by senior radiologists.
    \item \textbf{MAL Cervix~\cite{landman2015miccai}:} This dataset is part of the Beyond the Cranial Vault challenge. It consists of $30$ training and $20$ testing abdominal CT images acquired via a full bladder drinking protocol and annotated by a trained radiation oncologist. It focuses on the digestive and reproductive systems of female cervical cancer patients.
    \item \textbf{Amos~\cite{ji2022amos}:} A diverse dataset with $500$ CT images collected from different scanners and sites covering 15 abdominal organ categories. The selection of patients relates to abdominal tumors or abnormalities examinations. Annotations rely on a combination of junior and senior radiologist labor.
    \item \textbf{RibSeg~\cite{yang2021ribseg}:} The RibSeg dataset consists of $490$ CT Scans taken from publicly available RibFrac~\cite{jin2020deep} dataset. The authors use a semi-automatic morphology-based segmentation approach based on thresholding, point cloud segmentation, and morphological operations. They check the proposed segmentations by hand and refine them if necessary. 
    \item \textbf{Verse ~\cite{sekuboyina2021verse}:} A large dataset for vertebra segmentation. It consists of two subsets and has a total of $374$ CT scans of $355$ patients from multiple detectors and sites with voxel-wise annotations for individual vertebras. Segmentations have been performed semi-automatically with initial proposals being generated by an in-house pipeline. The proposals are refined by a team of trained medical students and experts and finally approved by a radiologist with more than $30$ years of experience. 
    \item \textbf{ATM~\cite{zhang2023multi}:} This dataset establishes a benchmark for Airway Tree Modelling by providing $500$ chest CT scans from different sites and includes scans of healthy patients, patients with pulmonary diseases, and even noisy COVID-19 CTs. Annotations of the pulmonary airways were performed by a team of three experts with each radiologist having more than five years of experience.
    \item \textbf{PARSE~\cite{kuanquan_wang_2022_6361906}:} The PARSE dataset is part of the Pulmonary Artery Segmentation Challenge and contains a total of $203$ CT images from $203$ patients which have been diagnosed with pulmonary nodular diseases. The CTs were generated using devices from two different manufacturers, with data collected from four distinct sites. Each of the images has been annotated by five experts with each expert having at least five years of experience in the field. 
    \item \textbf{Pelvic CT~\cite{liu2021deep}:} A large-scale dataset that focuses on the segmentation of pelvic bone structures such as hip bones or sacrum. It consists of $1184$ CT images collected from different source datasets combining images from multiple sites, scanners, and even metal artifacts. The labeling was conducted by a team of junior and senior radiologists.
    
\end{itemize}

%Include Labels
% \input{table_spacing}


%Third source of knowledge: Own annotations, BCA and Alex's Labels
Besides the previously described dataset, we leverage non-publicly available datasets and models. One of the models is the body composition analysis model~\cite{koitka2021fully} which differentiates between different types of tissues. From this model, we obtain labels such as \textit{fat} or the general class \textit{muscles}. In total, we extract $9$ labels from the body composition model source. 
A second private source dataset consisting of $104$ diverse head and neck contrast CT images from four different source cohorts~\cite{giske2011local, stoiber2017analyzing, bejarano2019longitudinal, bejarano2018head, clark2013cancer}.
%The total of $104$ images have been annotated by medical students with a focus on anatomical structures required for the. 
This dataset focuses on the diagnosis and treatment of oropharyngeal or hypopharyngeal head and neck cancer and has been annotated by medical students. It provides fine-grained classes for head neck vessels and bone structures. We train a standard nnU-Net on $86$ images to extract the dataset knowledge and add $12$ unique, previously unavailable labels from this dataset, mostly vessels in the head-neck region. 

%Second source of truth: Rules and Remapping
After obtaining the labels from the different nnU-Net predictions, we use anatomically derived rules to refine the current predictions and generate $7$ additional labels. An intuitive example for a new label that can be derived from the combination of obtained labels and medical common sense is the skull. It can be derived from a thresholding procedure obtained by the bone window present in CT images. Bones typically lead to CT values between 350 and 3000 Hounsfield Units (HU) which serve as the described thresholds. The obtained set of voxels can be restricted to the area above the C5 vertebra which previously was obtained. Finally, we remove already predicted vertebras from the thresholded voxels which leaves us with an accurate mask for the skull. We furthermore exploit the behavior of the neural network predictions which have only been trained on parts of the anatomy and typically confuse structures that look similar in the CT images. Common systematic errors are to predict gonads as the eyeballs or colon as the nasal cavity. We exploit these systematic mistakes and remap the produced labels according to the location within the human body. By employing these simple rules we add $7$ additional labels. 


% Figure environment removed


\subsection*{Knowledge Aggregation}
In order to aggregate the predictions of the individual models, we define a common labeling scheme %which is derived from established medical nomenclature standards.  
to which we map the obtained masks. Since some of these labels present multiple versions of the same anatomical structure, such as the class \textit{aorta} which is present in Total Segmentator, Amos, and SegThor it is necessary to combine these predictions. 
%Might need to delete this sentence
%We check the label quality of the models for the individual predictions on randomly sampled volumes and exclude predictions of suboptimal quality. 
Unless stated otherwise, we merge the predictions of models trained on the different source datasets into a single mask which is the union of all individual masks. 
This procedure is simple and stable. It also helps in aggregating masks of the same anatomical structures which are only predicted within certain regions of the human body on which the respective models have been trained. An example of this behavior can be found in the mask for the class \textit{aorta} predicted by the SegThor~\cite{lambert2020segthor} model. While the aorta spans outside the thorax, this model only predicts it within the thorax region. Only by merging the mask for \textit{aorta} of this model with additional masks from other models leaves us with a full mask for the aorta spanning over the entire anatomy. Thus merging these labels combines the knowledge present in different parts of the human body into a single, unified anatomy which is the goal of this work. 

When integrating the different anatomical structures into the Atlas labeling scheme, we aggregate them according to their anatomical hierarchical level from course to fine starting from general tissues such as \textit{muscles} or \textit{fat}. On top, we gradually add the different organs and finally fine-grained vessel structures such as \textit{Pulmonary Arteries}. During the aggregation process, we employ basic anatomical knowledge to improve individual predictions on the fly. One of these operations is that we split the predicted voxels into left and right clusters for paired organs such as the hip bones, kidneys or adrenal glands and resolve conflicts. Furthermore, we restrict predictions based on previous labels or eliminate non-largest connected components if it is deemed appropriate. %Furthermore, we employ basic morphological operations such morphological growing exclude predictions of structures which are likely to be confused such are adjacent bone structures. Examples for this are hip bones and the two femurs

%Volle List der Labens angeben

\subsubsection*{From Aggregated Predictions to a Unified Dataset}
\label{SS: Label Aggregation}
After integrating the labels into the common DAP Atlas CT volumes, it is an integrated dataset, but the different masks are still predictions of models which were trained on heterogeneous source datasets and thus generate heterogeneous masks. To unite these different, integrated masks into a single seamless dataset, we perform one iteration of self-training. The benefits of this procedure are four-fold: As previously mentioned, we bring the labels which originate from datasets of different resolutions into the common Atlas resolution leading to a truly seamless integration. A second reason to perform self-training is to eliminate non-systematic random noise. The network receives consistent feedback from consistent predictions, while noisy predictions are non-systematic. This observation is a well-known fact in image classification~\cite{liu2020early} which states that before the memorization of training data, networks tend to ignore noisy predictions and focus on consistent feedback. We make sure to not overfit the network on the dataset by closely monitoring training and validation losses. A third reason, to perform self-training is to distill the fragmented knowledge into a single model capable to predict the entire anatomy, this massively decreases the necessary time to predict the anatomy, since it reduces the inference time from $n$ expert models to a single model. Finally, self-training hampers the exact reconstruction of private data from expert models which were directly trained on private source datasets. 

%Beschreibung von V1
We generate the first version of the DAP Atlas dataset by applying the obtained unified anatomical model on the selected Atlas target volumes. While the overall label quality is good, we notice certain patterns which were repeatedly done wrong and with which the networks seemed to struggle. These systematic exceptions are the confusion of voxels that belong to paired structures such as the left and right kidney or adjacent vertebrae. Further, we observe implausible predictions of structures within body regions that are not possible, e.g. colon being predicted outside the abdomen. Finally, we observe structures belonging to the reproductive system to be predicted for the wrong sex. 
These errors are relatively easy to correct by once again applying anatomical rules. To address these, we propose Algorithm~\ref{Alg: Post-Processing}. 
We furthermore use two sets of rules to filter out implausible prediction: During Algorithm ~\ref{Alg: Rib counting}: We exclude predictions leading to different orderings induced by median points and minimum points of the ribs. Additionally, we examine the normal vector of the hyperplane during Algorithm~\ref{Alg: Left-Right-Splitting} and exclude predictions leading to hyperplanes that deviate too much from the axial directions. This reduces the number of images in the Atlas dataset from $566$ to $533$ CTs.

After applying Algorithm~\ref{Alg: Post-Processing} to the raw labels, we receive the final version of the dataset, which is rated as very impressive by a consulted radiologist. We describe the extensive validation procedure of the dataset in Section \textit{Technical Validation}. 

While the dataset is convincing, we acknowledge that the performance of the developed anatomical model is dependent on Algorithm~\ref{Alg: Post-Processing} which is undesirable as it requires the availability of anchor predictions which may not always be available for arbitrary CTs. As an additional contribution besides the dataset, we develop a more robust, model based on the available Atlas Knowledge which is more suitable for a clinical environment in which the model has to process arbitrary CT Volumes. We will refer to the previous model used to generate the dataset as the Atlas dataset model (V1) and the novel model as the Atlas prediction model (V2).

\subsubsection*{Developing a Prediction Model from the Atlas Dataset}
The goal of the Atlas prediction model is to eliminate the need for post-processing which is impractical within a clinical setting in which the model should be able to deliver convincing results on arbitrary CT volumes. When examining the different steps of Algorithm~\ref{Alg: Post-Processing}, we notice two steps that are easy to address algorithmically: sex-based consistency and non-largest connected component suppression as defined in Algorithms~\ref{Alg: Sex-based consistency} and~\ref{Alg: Non largest CC supression} respectively, as these methods simply suppress predictions and do not rely on other anchor predictions such as Algorithm~\ref{Alg: Left-Right-Splitting}. We thus aim to develop a training procedure that eliminates the need for left-right splitting (Algorithm~\ref{Alg: Left-Right-Splitting}), area-restrictions (Algorithm~\ref{Alg: Area Restriction}), and rib counting (Algorithm~\ref{Alg: Rib counting}).

To tackle these challenges we develop a custom training strategy for the Atlas prediction model. First, we apply Algorithm ~\ref{Alg: Post-Processing} during the aggregation phase of the individual expert models to maximize the agreement with the desired output which has been approved by experts. Next, we observe that due to the large number of classes, the standard nnU-Net~\cite{isensee2021nnu} learning rate schedule is suboptimal as it closely follows a linear learning rate schedule allocating approximately the same number of epochs for small and large learning rates. We find that the proposed task is more difficult than most standard segmentation tasks and thus increase the number of training epochs from $1000$ to $5000$. Finally, we fine-tune the network for another $1000$ epochs with a fixed learning rate of $0.001$ and without the standard mirror augmentation. This allows the network to focus on the improvements on smaller structures and helps to mitigate the right-left and rib confusion. We show a comparison of the raw output of the Atlas dataset model, the post-processed volume, and the raw output of the Atlas prediction model in Fig.~\ref{fig:V1_V2_comparison}. As it can be seen, the output of the robust model has a large agreement with the post-processed predictions of the first model without relying on Algorithm~\ref{Alg: Post-Processing}. We analyze this behavior and find that the vast majority of predicted structures have an agreement of more than $90\%$ IoU between the post-processed V1 Model and the raw V2 predictions.

Besides the DAP Atlas dataset, we also release the robust segmentation model which can be used to perform inference without post-processing. It furthermore tends to perform better for out-of-distribution tasks which are common within a clinical setting. We examine this behavior in Section \textit{Technical Validation}.

% Figure environment removed

%We examine the same 25 volumes and compare these to previously discussed limitations. 

%TODO Do the comparison The discussion and the figure

%Besides the proposed dataset generated by the previously described procedure and approved by experts, we also release a second version of the dataset which has been generated by the robust model, while there is no substantial difference among the two datasets as they have a mean IOU of almost $90\%$ we do not perform the same costly evaluation procedure twice. We generally recommend the usage of the approved standard V1 dataset and the robust models for custom inference without post-processing, as it tends to perform better for out-of-distribution tasks. We examine this behaviour in Section~\ref{S: Technical Validation}.

