%!TEX root = ../main.tex
\newpage
\appendix
\onecolumn

The appendix is organized as follows: in Appendix~\ref{app:deferred_proofs}, we provide proofs of the claims made in the main text. 
Next, in Appendix~\ref{app:time_complexity}, we provide algorithms and time complexity analyses for the algorithms referenced in the main text. 
In Appendix~\ref{app:experiments}, we provide additional details about our experimental setup. 
Then in Appendix~\ref{app:broader_impacts}, we discuss the broader impacts and limitations of $\loki$. 
Finally, in Appendix~\ref{app:vis}, we provide more details and examples of the locus visualizations shown in Figure~\ref{fig:banner}. 

%\nick{Reminder to add result we promised to R3: ``In addition, the updated version of our draft contains a synthetic data experiment that further validates the theoretical results.''}

% \nick{Reminder to release code}

% \nick{Reminder to add broader impacts etc... from author checklist}


\section{Deferred Proofs}
\label{app:deferred_proofs}

\subsection{Proof of Theorem~\ref{theorem:learning} ($\loki$ sample complexity)}
In this section, we provide a formal statement and proof of our sample complexity result for $\loki$, including additional required definitions and assumptions. 
Our proof relies on the standard finite-sample complexity bound for maximum likelihood estimates of multiclass logistic regression parameters---we provide a statement and proof of this result first. 
\begin{lemma}[Multiclass logistic regression finite-sample complexity bound]
\label{lemma:logreg}
    Under the following multiclass logistic regression model
    \[\mathbb{P}(\hat{y} = k | x) = \frac{\exp\{ -x^\top \beta_k \}}{\sum_{c \in [K]} \exp\{-x^\top \beta_c \} } \]
    where we have access to maximum likelihood estimates $\hat{\beta}_k$ obtained over $n$ samples, $(x_i, y_i) \sim \mathcal{D}_{\mathcal{B}, \set{Y}}$ where $\mathcal{B} \subseteq \mathbb{R}^d$ is the unit 2-norm ball and $y_i \in [K]$ for $i \in [n]$, 
    we have the following parameter estimation bound:
    \[ ||\hat{\beta}_k - \beta_k|| \leq O\left(K \sqrt{\frac{d}{n}}\right), \]
    with high probability. 
\end{lemma}
\begin{proof}
    MLE theory tells us that for sufficiently large $n$, we have 
    \[ \frac{\hat{\beta}_{k,j} - \beta_{k,j}}{\text{SE}(\hat{\beta}_{k,j})} \sim \mathcal{N}(0, 1), \]
    where $\hat{\beta}_{k,j}$ is our maximum likelihood estimate over $n$ samples. 
    Then we can rewrite the standard error in terms of the observed Fisher information as 
    \begin{align*}
        & \frac{\hat{\beta}_{k,j} - \beta_{k,j}}{\text{SE}(\hat{\beta}_{k,j})} = 
\sqrt{\mathcal{I}(\hat{\beta}_{k,j})} (\hat{\beta}_{k,j} - \beta_{k,j}) \sim \mathcal{N}(0, 1) \\
        &\Rightarrow \sum_{j \in [d]} \mathcal{I}(\hat{\beta}_{k,j}) (\hat{\beta}_{k,j} - \beta_{k,j})^2 \sim \chi^2_d, 
    \end{align*}
    where $\chi^2_d$ is the chi-squared distribution with $d$ degrees of freedom. 

    Then using Markov's inequality, we obtain that
    \begin{align*}
        \mathbb{P}\left( \sum_{j \in [d]} \mathcal{I}(\hat{\beta}_{k,j}) (\hat{\beta}_{k,j} - \beta_{k,j})^2 > a d \right) &\leq \frac{1}{a} \\
        \mathbb{P}\left( \sum_{j \in [d]} (\hat{\beta}_{k,j} - \beta_{k,j})^2 > a \frac{d}{n} \right) &\leq \frac{1}{a} \\
        \mathbb{P}\left( ||\hat{\beta}_{k} - \beta_{k}||_2 > \sqrt{a \frac{d}{n}} \right) &\leq \frac{1}{a}.
    \end{align*}
    However, we require the bound to hold for all $k \in [K]$, so we apply a union bound
    \begin{align*}
        \mathbb{P}\left( \bigcup_{k \in [K]} ||\hat{\beta}_{k} - \beta_{k}||_2 > \sqrt{a \frac{d}{n}} \right) &\leq \frac{K}{a}
    \end{align*}
    and set $a = \tilde{a} K^2$
    \begin{align*}
        \mathbb{P}\left( \bigcup_{k \in [K]} ||\hat{\beta}_{k} - \beta_{k}||_2 > K\sqrt{\tilde{a} \frac{d}{n}} \right) &\leq \frac{1}{\tilde{a}K} \\
        \Rightarrow \mathbb{P}\left( \bigcap_{k \in [K]} ||\hat{\beta}_{k} - \beta_{k}||_2 \leq K\sqrt{\tilde{a} \frac{d}{n}}, \right) &> 1 - \frac{1}{\tilde{a}K}.
    \end{align*}
    Hence, with high probability, it holds that $||\hat{\beta}_{k} - \beta_{k}|| \leq O\left(K\sqrt{\frac{d}{n}}\right)$. 
\end{proof}

\begin{corollary}[]
Due to Lemma~\ref{lemma:logreg}, we also have the following using Lipschitzness and the fact that $x_i \in \mathcal{B}$ for all $i \in [n]$. 
\[ \left|\widehat{\mathbb{P}}_k - \mathbb{P}_k\right| \leq O\left(K\sqrt{\frac{d}{n}}\right). \]
\end{corollary}


Now that we have obtained a sample complexity bound for logistic regression, we will proceed with the required tools to prove the sample complexity bound for $\loki$. 
For the purposes of this proof, we define the Fréchet variance as the function over which the Fréchet mean returns the minimizer. 
\begin{definition}[Fréchet variance]
    The Fréchet variance is defined as 
    \[ \Psi_\mathbf{w}(V) := \sum_{i \in [K]} \mathbf{w}_i d^2(V, V_i). \]
\end{definition}

Additionally, we will require a technical assumption related to the sensitivity of the Fréchet variance to different node choices.
\begin{assumption}[Fréchet variance is $\frac{1}{\alpha}$-bi-Lipschitz]
\label{assumption:bilipschitz}
For a metric space defined by a graph $G = (\set{V}, \set{E})$, the Fréchet variance is $K$-bi-Lipschitz if there exists a $K \geq 1$ such that
\[ \frac{1}{K} d^2(V, \tilde{V}) \leq |\Psi_\mathbf{w}(V) - \Psi_\mathbf{w}(\tilde{V})| \leq K d^2(V, \tilde{V}) \]
for all $V, \tilde{V} \in \set{V}$, and a fixed $\mathbf{w} \in \Delta^{K-1}$. 
For our purposes, such a $K$ always exists: consider setting $K = \text{diam}(G)^2 \max_{V_1, V_2 \in \set{V}} |\Psi_\mathbf{w}(V_1) - \Psi_\mathbf{w}(V_2)|$. 
However, this is a very conservative bound that holds for all graphs that we consider. 
Instead, we assume access to the largest $\alpha = \frac{1}{K} \leq 1$ such that 
\[\alpha d^2(V, \tilde{V}) \leq |\Psi_\mathbf{w}(V) - \Psi_\mathbf{w}(\tilde{V})|,\] which may be problem dependent. 
\end{assumption}

\begin{theorem}[$\loki$ sample complexity]
\label{thm:learning_formal}
Let $G=(\set{V}, \set{E})$ with $\set{V} = \{V_i\}_{i=1}^N$ be an arbitrary graph. 
Let $\set{Y}=\set{V}$ and let $\Lambda = \{V_j\}_{j \in [K]} \subseteq \set{Y}$ be the set of observed classes. 
Under the following logistic model: 
$\mathbb{P}(\widehat{V} = V_i \ | \ x) =: \mathbb{P}_i = \frac{\exp\{-x^\top\beta_i\}}{\sum_{j\in [K]} \exp\{-x^\top\beta_j\}} \mathbf{1}\{ \widehat{V} \in \Lambda\}$, 
where we have access to estimates $\widehat{\mathbb{P}}_i$ for all $i \in [K]$, 
and assuming realizability for predicting some class $V_*$, 
$V_* \in m_{\Lambda} ([\mathbb{P}_i]_{i \in [K]}) = \argmin_{V \in \set{Y}} \sum_{i \in [K]} \mathbb{P}_i d^2(V, V_i)$, 
then with high probability, the sample complexity of estimating the target $V_*$ with prediction $\widehat{V}$ is 
\[d^2(V_*, \widehat{V}) \leq O( {K^2 \sqrt{d}}\times\text{diam}(G) ^2)/({\alpha \sqrt{n}}) ,\]
where $d$ is the dimensionality of the input, $\text{diam}(G)$ is the graph diameter, $n$ is the number of samples, and $\alpha$ is our parameter under Assumption~\ref{assumption:bilipschitz}. 
\end{theorem}
\begin{proof}
We consider the relationships between four quantities: $\Psi_{\mathbb{P}}(V_*)$, $\Psi_{\mathbb{P}}(\widehat{V})$, $\Psi_{\widehat{\mathbb{P}}}(V_*)$, $\Psi_{\widehat{\mathbb{P}}}(\widehat{V})$, where $\mathbb{P}$ is the true logistic model, $\widehat{\mathbb{P}}$ is the MLE, $V_* \in m_{\Lambda}(\mathbb{P})$ is the target class, and $\widehat{V} \in m_{\Lambda}(\widehat{\mathbb{P}})$ is the predicted class. While it is problem-dependent as to whether $\Psi_{\mathbb{P}}(V_*) \leq \Psi_{\widehat{\mathbb{P}}}(\widehat{V})$ or $\Psi_{\mathbb{P}}(V_*) > \Psi_{\widehat{\mathbb{P}}}(\widehat{V})$, a similar argument holds for both cases. 
So without loss of generality, we assume that $\Psi_{\mathbb{P}}(V_*) \leq \Psi_{\widehat{\mathbb{P}}}(\widehat{V})$. Then by the definition of the Fréchet mean, the following inequalities hold:
\[\Psi_{\mathbb{P}}(V_*) \leq \Psi_{\widehat{\mathbb{P}}}(\widehat{V}) \leq \Psi_{\widehat{\mathbb{P}}}(V_*), \]
and consequently, 
\begin{equation}
    \label{eq:upperlower}
    \Psi_{\widehat{\mathbb{P}}}(\widehat{V}) - \Psi_{\mathbb{P}}(V_*) \leq \Psi_{\widehat{\mathbb{P}}}(V_*) - \Psi_{\mathbb{P}}(V_*).
\end{equation}
We proceed by obtaining upper and lower bounds of the existing bounds in Equation~\ref{eq:upperlower}. 
First, we will obtain an upper bound on $\Psi_{\widehat{\mathbb{P}}}(V_*) - \Psi_{\mathbb{P}}(V_*)$.
\begin{align*}
    |\Psi_{\widehat{\mathbb{P}}}(V_*) - \Psi_{\mathbb{P}}(V_*)| &= \left|\sum_{i \in [K]} (\widehat{\mathbb{P}} - \mathbb{P}) d^2(V_*, V_i) \right| && \text{Fréchet variance definition} \\
    &\leq \text{diam}(G)^2 ||\widehat{\mathbb{P}} - \mathbb{P}||_1. 
\end{align*}

Next, we will obtain a lower bound on $\Psi_{\widehat{\mathbb{P}}}(\widehat{V}) - \Psi_{\mathbb{P}}(V_*)$.
\begin{align*}
    |\Psi_{\widehat{\mathbb{P}}}(\widehat{V}) - \Psi_{\mathbb{P}}(V_*)| &\geq | \Psi_{\mathbb{P}}(V_*) - \Psi_{\mathbb{P}}(\widehat{V}) | - | \Psi_{\hat{\mathbb{P}}}(\widehat{V}) - \Psi_{\mathbb{P}}(\widehat{V}) | && \text{triangle inequality} \\
    &\geq \alpha d^2(V_*, \widehat{V}) - \text{diam}(G)^2 ||\widehat{\mathbb{P}} - \mathbb{P}||_1 && \text{Assumption~\ref{assumption:bilipschitz}}. 
\end{align*}

Combining both of these bounds with Equation~\ref{eq:upperlower}, we obtain
\begin{align*}
    \alpha d^2(V_*, \widehat{V}) - \text{diam}(G)^2 ||\widehat{\mathbb{P}} - \mathbb{P}||_1 \leq \Psi_{\widehat{\mathbb{P}}}(\widehat{V}) - \Psi_{\mathbb{P}}(V_*) &\leq \Psi_{\widehat{\mathbb{P}}}(V_*) - \Psi_{\mathbb{P}}(V_*) \\
    &\leq \text{diam}(G)^2 ||\widehat{\mathbb{P}} - \mathbb{P}||_1
\end{align*}
\begin{align*}
    \Rightarrow \alpha d^2(V_*, \widehat{V}) &\leq 2 \text{diam}(G)^2 ||\widehat{\mathbb{P}} - \mathbb{P}||_1 \\
    &\leq 2 \text{diam}(G)^2 C K^2 \sqrt{\frac{d}{n}} \\
    & \text{by Lemma~\ref{lemma:logreg} and Lipschitzness} \\
    \Rightarrow d^2(V_*, \widehat{V}) \leq O \left( K^2 \sqrt{d} \times \text{diam}(G)^2 \right) / (\alpha \sqrt{n}),
\end{align*}
which completes the proof. 
\end{proof}











\subsection{Proof of Theorem~\ref{thm:min_locus_trees_main} (minimum locus cover for trees)}

\begin{theorem}[Minimum locus cover for trees]
\label{thm:min_locus_trees}
The minimum locus cover for a tree graph $T$ is $\Lambda = \text{Leaves}(T)$. 
\end{theorem}

\begin{proof}
We would like to show to show two things: 
\begin{enumerate}
    \item any node can be resolved by an appropriate construction of $\bm{w}$ using only the leaves and
    \item removing any leaf makes that leaf node unreachable, no matter what the other nodes are in $\bm{y}$--i.e., we must use at least all of the leaf nodes. 
\end{enumerate}
If both of these conditions hold (i.e., that the leaves form a locus cover and that they are the smallest such set), then the the leaves of any tree form a minimum locus cover. 
To set up the problem, we begin with some undirected tree, $T$, whose nodes are our set of labels: $T = (\mathcal{Y}, E)$ where $\Lambda = \text{Leaves}(T) \subseteq \mathcal{Y}$ is the set of leaf nodes. Moreover, let $\bm{\Lambda}$ be a tuple of the leaf nodes. We will start by proving that $\Lambda$ is a locus cover. We will show this by cases on $v \in \mathcal{Y}$, the node that we would like to resolve:
\begin{enumerate}
    \item If $v \in \Lambda$, i.e. $v$ is a leaf node, then setting $\bm{w}_i = \mathbf{1}\{\bm{\Lambda}_i = v\}$ yields 
    \begin{align*}
        m_{\bm{\Lambda}}(\bm{w}) &= \argmin_{y \in \mathcal{Y}} \sum_{i=1}^K \mathbf{1}\{\bm{\Lambda}_i = v\} d^2(y, \bm{\Lambda}_i) \\
        &= \argmin_{y \in \mathcal{Y}} d^2(y, v) = \{v\}.
    \end{align*} 
    \item If $v \not\in \Lambda$, we have a bit more work to do. Since $v$ is an internal node, consider any pair of leaves, $l_1 \neq l_2$ such that $v$ is along the (unique) path between $l_1$ and $l_2$: $v \in \Gamma(l_1, l_2)$. Then set 
    $\bm{w}_i =
        \begin{cases}
        \frac{d(v, l_2)}{d(l_1, l_2)} \text{ if } \bm{\Lambda_i} = l_1, \\
        \frac{d(v, l_1)}{d(l_1, l_2)} \text{ if } \bm{\Lambda_i} = l_2, \\
        0 \text{ otherwise }
        \end{cases} $
    yields
    \begin{align*}
        m_{\bm{\Lambda}}(\bm{w}) &= \argmin_{y \in \mathcal{Y}} \frac{d(v, l_2) d^2(y, l_1) + d(v, l_1) d^2(y, l_2)}{d(l_1, l_2)} \\
        &= \argmin_{y \in \mathcal{Y}} d(v, l_2) d^2(y, l_1) + d(v, l_1) d^2(y, l_2) \\
        &= \{v\}. 
    \end{align*}
\end{enumerate}
Thus $\Lambda$ is a locus cover. 
Next, we will show that it is the smallest such set. 
Let $l \in \Lambda$ be any leaf node, and define $\Lambda' = \Lambda \setminus \{l\}$. 
We must show that $\Lambda'$ is not a locus cover. 
Assume for contradiction that $\Lambda'$ is a locus cover. 
This means that given some tuple $\bm{y}'$ whose entries are the elements of $\Lambda'$, we have $\Pi(\bm{\Lambda}') = \mathcal{Y}$. 
This implies that the path between any two nodes in $\Lambda$ is also contained in $\Pi(\bm{\Lambda}')$. 
Since the leaves form a locus cover and any $y \in \mathcal{Y}$ can be constructed by the appropriate choice of $\bm{w}$ along with a path between two leaf nodes, $l$ must either be one of the entries of $\bm{\Lambda}'$ (one of the endpoints of the path) or it must be an internal node. Both cases are contradictions--$\bm{\Lambda}'$ cannot include $l$ by assumption, and $l$ is assumed to be a leaf node. 
It follows that $\Lambda$ is a minimum locus cover. 
\end{proof}


\subsection{Proof of Lemma~\ref{lemma:tree_pairwise} (tree pairwise decomposability)}

\begin{lemma}[Tree pairwise decomposability]
\label{lemma:tree_pairwise}
Let $T = (\set{Y}, \set{E})$ be a tree and $\Lambda \subseteq \set{Y}$. Then $\Pi(\Lambda)$ is pairwise decomposable. 
\end{lemma}

\begin{proof}
Assume for contradiction that $\exists y^* \in \mathcal{Y}$ where $y^* \notin \Pi(\{\lambda_i, \lambda_j\})$, $\forall \lambda_i, \lambda_j \in \Lambda$, but $y^* \in \Pi(\Lambda)$. Then, $y^* \in m_\Lambda (\mathbf{w})$ for some $\mathbf{w}$. 
Note that $y^* \notin \Pi(\{\lambda_i, \lambda_j\}) $ implies $y^* \notin \Gamma(\lambda_i, \lambda_j)$,  $\forall \lambda_i, \lambda_j \in \Lambda$, because $\Pi(\{\lambda_i, \lambda_j\}) = \Gamma(\lambda_i,\lambda_j)$ due to the uniqueness of paths in trees. 
As such, $\cap_{\lambda \in \Lambda} \Gamma( y^*, \lambda)$ must contain an immediate relative $y'$ of $y^*$ where
% above notation emphasizes that you can't have y* at the root of T' with some lambdas branching off leaves of one child of y* and other lambdas branching off leaves of another child of y*
% note: originally we had, let y' be the parent of y* but that isn't general enough; there's examples where it doesn't work eg by having y* be the root and a single stem comes off then splits
\begin{align*}
\sum^{|\Lambda|}_{i=1} \mathbf{w}_i d^2(y^*, \lambda_i) &= \sum^{|\Lambda|}_{i=1} \mathbf{w}_i (d(y', \lambda_i)+1)^2 \\
&> \sum^{|\Lambda|}_{i=1} \mathbf{w}_i d^2(y', \lambda_i).
\end{align*}
Therefore, $y^* \notin \Pi(\Lambda)$ because $y^*$ is not a minimizer of $\sum^{|\Lambda|}_{i=1} \mathbf{w}_i d^2(y, \lambda_i)$. So, it must be that $y^*\in \Pi(\{\lambda_i, \lambda_j\})$ for some $\lambda_i, \lambda_j \in \Lambda$ when $y \in \Pi(\Lambda)$.
\end{proof}




\subsection{Proof of Theorem~\ref{thm:algphylo} (Algorithm~\ref{alg:phylo} correctness)}

\begin{theorem}[Algorithm~\ref{alg:phylo} correctness]
\label{thm:algphylo}
Algorithm~\ref{alg:phylo} returns a locus cover for phylogenetic trees. 
\end{theorem} 

\begin{proof}
We first prove that Algorithm \ref{alg:phylo} will halt, then according to the stopping criterion, it is guaranteed that Algorithm \ref{alg:phylo} returns a locus cover. 
Suppose the algorithm keeps running until it reaches the case in which all of the leaf nodes are included in $\Lambda$ (i.e. $\Lambda = \set{Y}$), this results in the trivial locus cover of the phylogenetic tree and the algorithm halts.
\end{proof}







\subsection{Proof of Lemma~\ref{lemma:phylotree_pairwise} (phylogenetic tree pairwise decomposability)}

\begin{lemma}[Phylogenetic tree pairwise decomposability]
\label{lemma:phylotree_pairwise}
Let $T = (\set{V}, \set{E})$ be a tree with $\set{Y} = \text{Leaves}(T)$ and $\Lambda \subseteq \set{Y}$. Then $\Pi(\Lambda)$ is pairwise decomposable. 
\end{lemma}

\begin{proof}
Suppose to reach a contradiction that $\exists y^* \in \mathcal{Y}$ where $y^* \in \Pi(\Lambda)$, but $y^* \notin \Pi(\{\lambda_i, \lambda_j\})$ $\forall \lambda_i, \lambda_j \in \Lambda$. 

For arbitrary $\lambda_i, \lambda_j \in \Lambda$, define:
$$v'(\lambda_i, \lambda_j)=\argmin_{v \in \Gamma(\lambda_i, \lambda_j)}d(y^*, v).$$
Also let $\{\lambda_1, \lambda_2\} = \argmin_{\lambda_i, \lambda_j \in \Lambda} d(y^*, v'(\lambda_i, \lambda_j))$ and $v' = v'(\lambda_1, \lambda_2)$, so that $v'$ represents the closest vertex to $y^*$ which lies on the path between any two $\lambda \in \Lambda$. 
There exists some $\mathbf{w}$ for which $v' = \argmin_{v \in \Gamma(\lambda_1, \lambda_2)} (\mathbf{w}_1 d^2(v, \lambda_1) + \mathbf{w}_2 d^2(v, \lambda_2))$. 
For this $\mathbf{w}$, we have that $m_{\{\lambda_1, \lambda_2\}}(\mathbf{w})=\argmin_{y \in \mathcal{Y}} d(y, v') \not \ni y^*$. 
Let $y'$ be a vertex of $m_{\{\lambda_i, \lambda_j\}}(\mathbf{w})$ that is closest to $y^*$. Clearly, $d(y', v') < d(y^*, v')$.

For any $v \in \mathcal{V}$, let $d_C(v) = |\Gamma(v,y^*) \cap \Gamma(v,y')|$, let $d_{y^*}(v)=|\Gamma(v,y^*) \setminus \Gamma(v,y')|$ and $d_{y'}(v)=|\Gamma(v,y') \setminus \Gamma(v,y^*)|$ so that $d_{y^*}(v)\cup d_{y^*}(v) = \Gamma(y^*, y')$. We have $d_{y'}(v')<d_{y^*}(v')$.

Suppose $\exists \lambda_3, \lambda_4 \in \Lambda$ yielding $u' = v'(\lambda_3, \lambda_4) \in \Gamma(y^*, y')$ such that $d(y^*, u') \leq d(y', u')$. Then, $d_{y^*}(u')<d_{y^*}(v')$ implying $d(y^*, u')<d(y^*,v')$, which contradicts the definition of $v'$. % for any path-internal node between two lamdas that lies on the path between y* and y', it must be that y' is the better fit than y*

Suppose now that $\exists \lambda_3, \lambda_4 \in \Lambda$ yielding $u' = v'(\lambda_3, \lambda_4) \notin \Gamma(y^*, y')$ such that $d(y^*, u') \leq d(y', u')$. Then, either $\cap_{\lambda \in \Lambda} d_C(\lambda) \neq \emptyset$ (i.e. all lambdas lie in the same branch off of $\Gamma(y', y^*)$), in which case the path between any two lambdas will have the same closest node $v'$ to $y^*$, or it is possible to choose a $\lambda_5 \in \Lambda$ such that $d_C(\lambda_3)\cap d_C(\lambda_5) = \emptyset$. %lambda5 lies in another branch
In this situation, it must be that $\Gamma(\lambda_3, \lambda_5) \cap \Gamma(y', y^*) \neq \emptyset$, which implies $\exists t \in \Gamma(\lambda_3, \lambda_5)$ such that $d(y^*,t)<d(y^*, u')$ which violates the definition of $u'$. 
%The path between lambda 3 and 5 (or 4 and 5) will have to have internal nodes on the path between y* and y', which puts us back to the prior paragraph's contradiction

Altogether, for all $v \in \Gamma(\lambda_i, \lambda_j)$ with arbitrary $\lambda_i, \lambda_j \in \Lambda$, we have that $d(y',v)<d(y^*,v)$. This implies that for all $\lambda \in \Lambda$, there exists a $y' \in \set{Y}$ such that $d(y',\lambda)<d(y^*,\lambda)$ and therefore that $y^* \notin \Pi(\Lambda)$.
\end{proof}

% \subsection{Proof of Algorithm \ref{alg:phylo} returns minimum locus cover}
% We currently hold a counter-example that Algorithm \ref{alg:phylo} does not return minimum locus cover for all cases. But we present a potential useful justification to a algorithm may return minimum locus cover as an enlightenment to improve the current algorithm as well as justifying future design:

% \begin{theorem}[Algorithm~\ref{alg:phylo} returns a minimum locus cover for phylogenetic trees]
% \end{theorem}

% \begin{proof}
% We first prove that Algorithm \ref{alg:phylo} will halt at a certain time point and will not run forever, then according to the stopping criterion, it is guaranteed that Algorithm \ref{alg:phylo} returns a locus cover. Suppose the algorithm keeps running until it reaches the extreme case when all the leaf nodes are included in $\Lambda$ (i.e. $\Lambda=\set{Y}$), this generates to the naive locus cover of the phylogenetic tree and the algorithm will stop.

% Next, we prove that the set returned by Algorithm \ref{alg:phylo} is a minimum locus cover. 

% We argue that selecting nodes with longest spanning path in the graph is global optimal strategy as otherwise, the left-out node $v$ will never be in any locus cover $\Pi(\Lambda)$ with any $\Lambda$ such that $v \notin \Lambda$ and has to be included into $\Lambda$ eventually. 

% Suppose at round $k$, we have set $\Lambda_k$ and its corresponding locus cover $\Pi(\Lambda_k)$. Assume the longest path with at least one end-point not covered by the locus cover is $\Gamma(y_i, y_j)$ and that $y_i \notin \Pi(\Lambda_k)$ (Otherwise, the algorithm stops? here we are specifically assuming that if endpoints of longest path is covered by $\Pi(\Lambda)$ then so will the endpoints of all short paths. more strictly, nodes of the whole tree). Meanwhile, denote arbitrary shorter path is $\Gamma(z_i, z_j)$. If no such shorter path exists, then following the algorithm, we directly append $\{y_i, y_j\}$ to $\Lambda$ and the proof is done. Otherwise, if we include $\{z_i, z_j\}$ instead of $\{y_i, y_j\}$, then we show that $y_i \notin \Pi(\Lambda_k \cup \{z_i,z_j\})$.

% Since we assumed that $y_i \notin \Pi(\Lambda_k)$, if $y_i \in \Pi(\Lambda_k \cup \{z_i,z_j\})$, then the weight corresponds to $z_i$ can't be 0. And suppose $y_i \in \Pi(\Lambda_k \cup \{z_i,z_j\})$, this means there exists $\bm{w} \in \Delta^{K-1}$ s.t.
% \begin{align*}
%     y_i &= \argmin_{y \in \set{Y}} \sum_{\bm{\lambda}_i\in \Lambda_k \cup \{z_i,z_j\}} \bm{w}_i d^2(y, \bm{\lambda}_i)\\
%     &= \argmin_{y \in \set{Y}} \sum_{\bm{\lambda}_i\in \Lambda_k} \bm{w}_i d^2(y, \bm{\lambda}_i)+\gamma_i d^2(y, z_i).
% \end{align*}

% If we can show for arbitrary $\bm{w}$ we have:
% $$
% \sum_{\bm{\lambda}_i\in \Lambda_k} \bm{w}_i d^2(y_i, \bm{\lambda}_i) \geq \sum_{\bm{\lambda}_i\in \Lambda_k} \bm{w}_i d^2(z_i, \bm{\lambda}_i),
% $$
% then since $\gamma_i$ is strictly greater than 0, and $d^2(z_i,z_i)< d^2(y_i, z_i)$, $y_i$ can't be the minimizer of the objective function.
% \end{proof}



\subsection{Proof of Theorem~\ref{thm:min_loc_grid} (minimum locus cover for grid graphs)}

\begin{theorem}[Minimum locus cover for the grid graphs]
\label{thm:min_loc_grid}
The minimum locus cover for a grid graph is the pair of vertices in the furthest opposite corners.
\end{theorem}

\begin{proof}
We would like to show two things: 
\begin{enumerate}
    \item any vertex can be resolved by construction of $\bm{w}$ using the furthest pair of two vertices (i.e. opposite corners) and
    \item there does not exist a locus cover of size one for grid graphs with more than one vertex. 
\end{enumerate}
Let $G = (\set{V}, \set{E})$ be a grid graph and let $\Lambda = \{\lambda_1, \lambda_2\}$ be a set of two vertices who are on opposite corners of $G$ (i.e., perhipheral vertices achieving the diameter of $G$). 
For notational convenience, set ${\bm \Lambda} = \left[ \lambda_1, \lambda_2 \right]$
We can reach any interior vertex by an appropriate setting the weight vector ${\bm w} = [{\bm w}_1, {\bm w}_2]$. 
Then set 
    $\bm{w} = \left[\frac{d(v, \lambda_2)}{d(\lambda_1, \lambda_2)}, \frac{d(v, \lambda_1)}{d(\lambda_1, \lambda_2)}\right]$.
Finally, the Fréchet mean is given by
\begin{align*}
    m_{\bm{\Lambda}}(\bm{w}) &= \argmin_{y \in \mathcal{Y}} \frac{d(v, \lambda_2) d^2(y, \lambda_1) + d(v, \lambda_1) d^2(y, \lambda_2)}{d(\lambda_1, \lambda_2)} \\
    &= \argmin_{y \in \mathcal{Y}} d(v, \lambda_2) d^2(y, \lambda_1) + d(v, \lambda_1) d^2(y, \lambda_2) \\
    &\ni v. 
\end{align*}
Hence $\Lambda$ is a locus cover. 
We can clearly see that $\Lambda$ is a minimum locus cover---the only way to obtain a smaller set $\Lambda'$ is for it to include only a single vertex. 
However, if $\Lambda' = \{v'\}$ contains only a single vertex, it cannot be a locus cover so long as $G$ contains more than one vertex---$v'$ is always guaranteed to be a unique minimizer of the Fréchet mean under $\Lambda'$ which misses all other vertices in $G$. 
Thus $\Lambda$ is a minimum locus cover. 
\end{proof}



\subsection{Proof of Lemma~\ref{lemma:locus_grid_subspace} (loci of grid subspaces)}
\begin{lemma}[Locus of grid subspaces]
\label{lemma:locus_grid_subspace}
Given any pair of vertices in $\Lambda$, we can find a subset $G'$ of the original grid graph $G = (\set{Y}, \set{E})$ which takes the given pair as two corner. $\Pi(\boldsymbol{\Lambda})$ equals to all the points inside $G'$.
\end{lemma}

\begin{proof}
The result follows from application of Theorem~\ref{thm:min_loc_grid} to the choice of metric subspace. 
%From Theorem~\ref{thm:min_loc_grid}, we have that the minimum locus cover is a set of opposite corners in the grid graph. 
%Extending this result, we know that $\forall \lambda_i, \lambda_j \in G$, we can find a subset grid graph $G'$  with $\lambda_i, \lambda_j$ as two corners. It is trivial that $\lambda_i, \lambda_j$ is the pair of vertices with farthest distance in $G'$. Therefore, $\Pi(\boldsymbol{\Lambda})$ equals to all the points inside $G'$.
\end{proof}



\subsection{Proof of Lemma~\ref{lemma:grid_pairwise} (grid pairwise decomposability)}
\begin{lemma}[Grid pairwise decomposability]
\label{lemma:grid_pairwise}
Let $G = (\set{Y}, \set{E})$ be a grid graph and $\Lambda \subseteq \set{Y}$. Then $\Pi(\Lambda)$ is pairwise decomposable. 
\end{lemma}

\begin{proof}
% We apply the result of Lemma~\ref{lemma:locus_grid_subspace} to all pairs of vertices in $\Lambda$. 
% We have shown that the size of $\Pi(\Lambda)$ depends on the distance between two furthest pairs of vertices. 
% No matter how many points in $\Lambda$, the furthest distance is only calculated by any pair of points in $\Lambda$. 
% Therefore, $\Pi(\Lambda)$ is pairwise decomposable.
Suppose we have a grid graph $G = (\set{Y}, \set{E})$ and a vertex $\hat{y} \in \Pi(\Lambda)$ with $\Lambda \subseteq \set{Y}$. 
Due to the fact that $\hat{y} \in \Pi(\Lambda)$ and the fact that $G$ is a grid graph, we have that $\hat{y} \in \Gamma(\lambda_\alpha, \lambda_\beta)$ for some $\lambda_\alpha, \lambda_\beta \in \Lambda$. 
Then by Lemma~\ref{lemma:grid_pairwise}, 
$$\hat{y} \in \Pi(\{\lambda_\alpha, \lambda_\beta\}) \subseteq \cup_{\lambda_i, \lambda_j \in \Lambda} \Pi(\{\lambda_i, \lambda_j\}).$$
Therefore loci on grid graphs are pairwise decomposable. 
\end{proof}


% \subsection{Identifying locus cover for grid graphs}
% \begin{proof}
% It is clear that $\forall y \in \mathcal{Y}$, we can get the set of distances from $y$ to $\forall \lambda \in \Lambda$. If the set of distances is unique among all $y$, the locus cover is identified because $m_{\boldsymbol{\Lambda}}(\mathbf{w}) = \{ y \}$. If we are given all the corners, the distance set should be unique. In any 2D space, $\lambda_1, \lambda_2, \lambda_3, \lambda_4 \in \Lambda$ are the corners in that space and they form a 2D grid graph $G'$. $y'$ is the projection of $y$. We know that $y' \in G'$ 
% \begin{align*}
% y' &\in m_{\Lambda'} (w') \\
% &= \argmin_{y \in \mathcal{Y}} \sum^{4}_{i=1} w'_i d^2(y, \lambda_i).
% \end{align*}
% It is trivial that $m_{\boldsymbol{\Lambda'}}(\mathbf{w'}) = \{ y' \}$ for any 2D subspace. Therefore, $m_{\boldsymbol{\Lambda}}(\mathbf{w}) = \{ y \}$.
% \end{proof}



\subsection{Proof of Theorem~\ref{thm:trivial_complete} (no nontrivial locus covers for complete graphs)}
\begin{theorem}[Trivial locus cover for the complete graph]
\label{thm:trivial_complete}
There is no non-trivial locus cover for the complete graph. 
\end{theorem}
\begin{proof}
We show that there is no nontrivial locus cover for complete graphs by showing that removing any vertex from the trivial locus cover renders that vertex unreachable. 
We proceed by strong induction on the number of vertices, $n$.
\\
\textbf{Base case} We first set $n = 3$. Let $\bm{K_3} = (\set{V}, \set{V}^2)$ be the complete graph with three vertices: $\set{V} = \{v_1, v_2, v_3\}$, and without loss of generality, let $\Lambda = \{ v_2, v_3 \}$ be our set of observed classes. 
There are two cases on the weight vector $\bm{w} = [w_2, w_3]$:\\
Case 1:  Suppose $\bm{w} \not\in \text{int}\Delta^{2}$. 
This means that either $w_2 = 1$ or $w_3 = 1$---which leads to the Fréchet mean being either $v_2$ or $v_3$, respectively. 
Neither of these instances correspond to $v_1$ being a minimizer. 
\\
Case 2: Suppose $\bm{w} \in \text{int}\Delta^{2}$. 
Then the Fréchet mean is given by 
$$m_{\bm{\lambda}}(\bm{w}) = \argmin_{y \in \mathcal{Y}} w_2 d^2(y, v_2) + w_3 d^2(y, v_3)$$
Assume for contradiction that $v_1 \in \Pi({\bm\Lambda})$:
\begin{align*}
    w_2 d^2(v_1, v_2) + w_3 d^2(v_1, v_3) &= w_2 + w_3 \\
    &> w_3 && \text{because } \bm{w} \in \text{int}\Delta^{2} \\
    &= w_2 d^2(v_2, v_2) + w_3 d^2(v_2, v_3). 
\end{align*}
Therefore, $v_1 \notin \Pi({\bm\Lambda})$.
This is a contradiction.
Thus there is no nontrivial locus cover for $\bm{K_3}$. 

\textbf{Inductive step}: Let $\bm{K_{n-1}} = (\set{V}, \set{V}^2)$ be the complete graph with $n-1$ vertices. Assume that there is no nontrivial locus cover for $\bm{K_{n-1}}$.
We will show that there is no nontrivial locus cover for $\bm{K_{n}}$.
Let the weight vector be $\bm{w} = [w_1, ... w_{n-1}]$ corresponding to vertices $v_1, ..., v_{n-1} \in \set{V}$ with $\Lambda = \{v_1, ..., v_{n-1}\}$. 
We want to show that $v_n \not\in \Pi(\bm{\Lambda})$. 
We proceed by cases on $\bm{w}$. 
\\
Case 1: Suppose $\bm{w} \not\in \text{int}\Delta^{n-2}$ where $m$ entries of $\bm{w}$ are zero, then by strong induction we know that $v_n \not\in \Pi(\bm{\Lambda}) = \{v_i\}_{i=1}^{n-m}$, i.e., there is no nontrivial locus cover for $\bm{K_{n-m}}$.
\\
Case 2: Suppose $\bm{w} \in \text{int}\Delta^{n-2}$. \\
Assume for contradiction that $v_n \in \Pi(\bm{\Lambda})$. 
Using this assumption, we obtain the following 
\begin{align*}
    \sum_{i=1}^{n-1} w_i d^2(v_n, v_i) &=\sum_{i=1}^{n-1} w_i \\
    &>\sum_{i=1}^{n-2} w_i && \text{because } \bm{w} \in \text{int}\Delta^{n-2} \\
    &=\sum_{i=1}^{n-1} w_i d^2(v_{n-1}, v_i). 
 \end{align*}
 Therefore, $v_n$ is not a minimizer, so $v_n \notin \Pi(\Lambda)$. 
 This is a contradiction, hence $\bm{K_n}$ has no nontrivial locus cover. 
\end{proof}



\subsection{Proof of Theorem~\ref{thm:active_largest} (active next-class selection for trees)}
\begin{proof}
We will prove the result by showing that the two optimization problems are equivalent. 
Let $v$ be a solution to the following optimization problem:
\begin{argmaxi*}|s|
{y \in \mathcal{Y} \setminus \Pi(\Lambda)}{d(y, b)}
{}{}
\addConstraint{b \in \partial_{\text{in}} T'}
\addConstraint{\Gamma(b, y) \setminus \{b\} \subseteq \mathcal{Y} \setminus \Pi(\Lambda)}, 
\end{argmaxi*}
where $\partial_{\text{in}} T'$ is the inner boundary of $T'$, the subgraph of $T$ whose vertices are $\Pi(\Lambda)$. 
This optimization problem can be equivalently rewritten as 
\begin{argmaxi*}|s|
{y \in \mathcal{Y} \setminus \Pi(\Lambda)}{|\Gamma(y, b)|}
{}{}
\addConstraint{b \in \partial_{\text{in}} T'}
\addConstraint{\Gamma(b, y) \setminus \{b\} \subseteq \mathcal{Y} \setminus \Pi(\Lambda)}, 
\end{argmaxi*}
and we can furthermore introduce additional terms that do not change the maximizer  
\begin{argmaxi*}|s|
{y \in \mathcal{Y} \setminus \Pi(\Lambda)}{| \cup_{\lambda_i, \lambda_j \in \Lambda} \Gamma(\lambda_i, \lambda_j) \cup \Gamma(b, y)|}
{}{}
\addConstraint{b \in \partial_{\text{in}} T'}
\addConstraint{\Gamma(b, y) \setminus \{b\} \subseteq \mathcal{Y} \setminus \Pi(\Lambda)}. 
\end{argmaxi*}
Equivalently, we can also connect $b$ to one of the elements of $\Lambda$
\begin{argmaxi*}|s|
{y \in \mathcal{Y} \setminus \Pi(\Lambda)}{| \cup_{\lambda_i, \lambda_j \in \Lambda} \Gamma(\lambda_i, \lambda_j) \cup \Gamma(\lambda_i, b) \cup \Gamma(b, y)|}
{}{}
\addConstraint{b \in \partial_{\text{in}} T'}
\addConstraint{\Gamma(b, y) \setminus \{b\} \subseteq \mathcal{Y} \setminus \Pi(\Lambda)}. 
\end{argmaxi*}
Due to the uniqueness of paths in trees, this optimization problem also has the following equivalent form without any dependence on $b$:
\begin{align*}
    v \in \argmax_{y \in \set{Y}\setminus\Lambda} | \cup_{\lambda_i, \lambda_j \in \Lambda} \Gamma(\lambda_i, \lambda_j) \cup \Gamma(\lambda_i, y) | 
    &= \argmax_{y \in \set{Y}\setminus\Lambda} | \cup_{\lambda_i, \lambda_j \in \Lambda} \Pi(\{\lambda_i, \lambda_j\}) \cup \Pi(\{\lambda_i, y\}) | \\
    &= \argmax_{y \in \set{Y}\setminus\Lambda} | \cup_{\lambda_i, \lambda_j \in \Lambda \cup \{y\}} \Pi(\{\lambda_i, \lambda_j\}) | \\
    &= \argmax_{y \in \set{Y}\setminus\Lambda} | \Pi(\Lambda \cup \{y\}) | \\
    &\text{using Lemma~\ref{lemma:tree_pairwise}}. 
\end{align*}
Therefore $v$ is a maximizer of $| \Pi(\Lambda \cup \{v\}) |$, as required. 
\end{proof}



\section{Algorithms and Time Complexity Analyses}
\label{app:time_complexity}
%\renewcommand{\thealgorithm}{}

We provide time complexity analyses for Algorithms~\ref{alg:phylo},~\ref{alg:locus_pairwise}, and~\ref{alg:locus_general}. 

\subsection{Analysis of Algorithm~\ref{alg:phylo} (locus cover for phylogenetic trees)}
We provide Algorithm~\ref{alg:phylo} with comments corresponding to the time complexity of each step.
\begin{algorithm}[]
\caption{Locus cover for phylogenetic trees}
\label{alg:phylo}
\begin{algorithmic}
\Require phylogenetic tree $T=(\set V, \set E)$, $\set{Y}=\text{Leaves}(T)$
\State $N \gets |\set{Y}|$
\State $P \gets \text{sortbylength}( [\Gamma(y_i, y_j)]_{i,j \in [N]} )$ \Comment{$N|\set{E}| + N^2\log N$}
\State $P \gets \text{reverse}(P)$ \Comment{$O(N^2)$}
\State $\Lambda \gets \emptyset$
\For{$\Gamma(y_i, y_j)$ in $P$} \Comment{$O(N^2)$}
%\State $y_i, y_j \gets \text{endpoints}(\Gamma(y_i, y_j))$
\If{$\Pi(\Lambda) = \set{Y}$} \Comment{$O(K^2D\max\{N|\set{E}|, N^2\log N\})$}
    \State \Return $\Lambda$ %\Comment{This is a comment}
\Else
    \State $\Lambda \gets \Lambda \cup \{y_i, y_j\}$
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Combining these, we obtain the following time complexity:
$$ O( N|\set{E}| + N^2\log N + N^2 + N^2 K^2D\max\{N|\set{E}|, N^2\log N \}) = O(N^2 K^2D\max\{N|\set{E}|, N^2\log N\}). $$

\subsection{Analysis of Algorithm~\ref{alg:locus_pairwise} (computing a pairwise decomposable locus)}
We first provide Algorithm~\ref{alg:locus_pairwise} here, with comments corresponding to the time complexity of each step.
\begin{algorithm}[]
\caption{Computing a pairwise decomposable locus}
\label{alg:locus_pairwise}
\begin{algorithmic}
\Require $\Lambda$, $\set{Y}$, $G=(\set{V}, \set{E})$
\State $\Pi \gets \emptyset$
\State $D \gets \text{diam}(G)$ \Comment{$O(N|\set{E}| + N^2\log N)$}
%\For{$y$ in $\set{Y}$}
\For{$\lambda_i, \lambda_j \in \Lambda$} \Comment{$O(K^2)$}
\For{$w_1$ in $\left\{\frac{0}{D}, \frac{1}{D}, ..., \frac{D}{D}\right\}$} \Comment{$O(D)$}
\State $\mathbf{w} \gets [w_1, 1 - w_1]$
\State $\Pi \gets \Pi \cup m_{\{\lambda_i, \lambda_j\}}(w)$ \Comment{$O(N|\set{E}| + N^2\log N)$}
\EndFor
\EndFor
%\EndFor
\State \Return $\Pi$
\end{algorithmic}
\end{algorithm}

We first compute the diameter of the graph $G = (\set{Y}, \set{E})$ with $N=|\set{Y}|$, which is done in $O(N|\set{E}| + N^2\log N)$ time using Dijkstra's algorithm to compute the shortest paths between all pairs of vertices. 
%This computation will be reused for the Fréchet mean as well. 
We then iterate over all pairs of elements in $\Lambda$ with $K=|\Lambda|$, which amounts to $O(K^2)$ iterations.
Within this, we perform $O(D)$ computations of the Fréchet mean, for which each iteration requires $O(N|\set{E}| + N^2\log N)$ arithmetic operations or comparisons. 
Combining these, the final time complexity is $$O(N|\set{E}| + N^2\log N + K^2D(N|\set{E}| + N^2\log N)) = O(K^2D\max\{N|\set{E}|, N^2\log N\}).$$


\subsection{Analysis of Algorithm~\ref{alg:locus_general} (computing a generic locus)}
We provide Algorithm~\ref{alg:locus_general} with comments corresponding to the time complexity of each step.
\begin{algorithm}[]
\caption{Computing a generic locus}
\label{alg:locus_general}
\begin{algorithmic}
\Require $\Lambda$, $\set{Y}$, $G=(\set{V}, \set{E})$
\State $\Pi \gets \emptyset$
\State $D \gets \text{diam}(G)$ \Comment{$O(N|\set{E}| + N^2\log N)$}
%\For{$y$ in $\set{Y}$}
\For{$\mathbf{w}$ in $\left\{\frac{0}{D}, \frac{1}{D}, ..., \frac{D}{D}\right\}^{|\Lambda|}$} \Comment{$O(D^K)$}
\State $\Pi \gets \Pi \cup m_{{\bf\Lambda}}(w)$ \Comment{$O(N|\set{E}| + N^2\log N)$}
\EndFor
%\EndFor
\State \Return $\Pi$
\end{algorithmic}
\end{algorithm}

Following a similar argument from the analysis of Algorithm~\ref{alg:locus_pairwise}, the time complexity is 
$$O(N|\set{E}| + N^2\log N + D^K(N|\set{E}| + N^2\log N)) = O(D^K),$$
where $K$ is the number of observed classes. 

\section{Experimental Details}
\label{app:experiments}
In this section, we provide additional details about our experimental setups.\footnote{Code implementing all of our experiments will be released upon publication.} 

\subsection{CLIP experiments}
All experiments are carried out using CLIP frozen weights. There are no training and hyperparameters involved in this experiment. We use the default CIFAR100 test set. The label prompt we use is "a photo of a [class\_name]." 

\subsection{ImageNet experiments} 
We sample 50 images for each class from ImageNet as a training dataset then use the validation dataset in ImageNet to evaluate $\loki$'s performance. 
We take the embeddings from SimCLR and train a baseline one-vs-rest classifier. 
We sample different numbers of observed classes, $K$, from 1000 classes in two ways, uniformly and Gibbs distribution with concentration parameter 0.5.

\subsection{LSHTC experiments} 
We generate a summary graph of the LSHTC class graph (resulting in supernodes representing many classes) by iteratively:
\begin{enumerate}
    \item randomly selecting a node or supernode
    \item merging its neighbors into the node to create a supernode
\end{enumerate}
until the graph contains at most 10,000 supernodes. 
In the LSHTC dataset, each datapoint is assigned to multiple classes. 
We push each class to its supernode, then apply majority vote to determine the final class. 
We sample different number of observed classes, $K$, from 10,000 classes. 
Then split half of dataset as training dataset and the rest as testing. 
We train a baseline classifier using a 5-NN model and compare its performance with $\loki$.

\section{Broader Impacts and Limitations}
\label{app:broader_impacts}
As a simple adaptor of existing classifiers, it is possible for our method to inherit biases and failure modes that might be present in existing pretrained models. 
On the other hand, if the possibility of harmful mis-classifications are known a priori, information to mitigate these harms can be baked into the metric space used by $\loki$. 
Finally, while we have found that $\loki$ often works well in practice, it is possible for the per-class probabilities that are output by a pretrained model to be sufficiently mis-specified relative to the metric over the label space, or for the metric itself to be mis-specified. 
Nonetheless, we have found that off-the-shelf or self-derived metric spaces to work well in practice. 


% \section{ADDITIONAL EXPERIMENTS}

% If you have additional experimental results, you may include them in the supplementary materials.


\section{Random Locus Visualizations}
\label{app:vis}
In Figures~\ref{fig:banner},~\ref{fig:tree_loci},~\ref{fig:phylotree_loci},~\ref{fig:smallworld_loci},~\ref{fig:erdos_loci}, and~\ref{fig:albert_loci}, we provide visualizations of classification regions of the probability simplex when using $\loki$ with only three observed classes out of 100 total classes, and different types of random metric spaces. 
The first example in Figure~\ref{fig:banner} shows the prediction regions on the probability simplex when using standard $\argmax$ prediction---the three regions correspond to predicting one of the three classes (0, 39, and 99) and no regions corresponding to any of the other classes $\{1, ..., 38, 40, ..., 98\}$. We compute these regions using a version of Algorithm~\ref{alg:locus_general}, and while it does have exponential time complexity, the exponent is only three in this case since we consider only three observed classes. 

On the other hand, the other three examples in Figure~\ref{fig:banner} and Figures~\ref{fig:tree_loci},~\ref{fig:phylotree_loci},~\ref{fig:smallworld_loci},~\ref{fig:erdos_loci}, and~\ref{fig:albert_loci} all show the prediction regions on the probability simplex when using $\loki$. 
Figure~\ref{fig:tree_loci} shows this for random tree graphs.
Here, the prediction regions are striped or contain a single triangle-shaped region in the center---these correspond, respectively, to intermediate classes along branches of the tree leading up from the observed class and the prediction region formed by the common-most parent node. 
Figure~\ref{fig:phylotree_loci} shows similar regions, although these are more complex and are thus more difficult to interpret, as phylogenetic trees are metric subspaces equipped with the induced metric from trees. 
Furthermore, in order to generate phylogenetic trees with 100 leaves, we needed to create much larger trees than the ones used for Figure~\ref{fig:tree_loci}, which led to narrower prediction regions due to the higher graph diameter. 
Finally, Figures~\ref{fig:smallworld_loci},~\ref{fig:erdos_loci}, and~\ref{fig:albert_loci} each show the prediction regions using $\loki$ when the metric space is a random graph produced using Watts-Strogatz, Erd\H{o}s-Rényi, and Barabási–Albert, models respectively. 
These prediction regions are more complex and represent complex relationships between the classes. 


% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

\vfill

