\section{Introduction}
\label{sec:intro}

%We generalize standard multiclass classification under the 0-1 loss to enable the usage of metric information that relates the labels. This applies to few- and zero-shot classification, hierarchical supervised learning, partial label learning, weak supervision, and structured prediction. 

%What is the problem?
The use of pretrained models is perhaps the most significant recent shift in machine learning.
%
Such models can be used out-of-the-box, either to predict classes observed during pretraining (as in ImageNet-trained ResNets \cite{resnet}) or to perform zero-shot classification on any set of classes \cite{clip}.
%
While this is exciting, label spaces are often so huge that models are unlikely to have seen \emph{even a single point} with a particular label. 
%
Without additional modification, pretrained models will simply fail to reliably predict such classes.
%
Instead, users turn to fine-tuning.
%
This requires additional labeled data and training cycles and so sacrifices much of the promise of zero-shot usage. 

%A major challenge in supervised learning is handling extremely high-cardinality label spaces.
%
%This setting is common to many domains and applications, including medical diagnosis (e.g. patients have various complicated conditions), automatic question answering systems (e.g. responses are highly flexible), and more generally, modeling string categorical response (e.g. ImageNet dataset \cite{imagenet} constructed based on words).
%
%If there are thousands---or even millions---of possible classes, the training dataset is unlikely to contain even one point whose label is part of many (or most) classes.
%If there are thousands---or even millions---of possible classes, the training dataset is unlikely to contain points representing every class. 
%
%Without additional modifications, the trained model cannot predict such classes, and so cannot be reliably deployed in real-world settings. 

How can we adapt pretrained models to predict new classes, without fine-tuning or retraining?
%
At first glance, this is challenging: predictive signal comes from labeled training data.
%
However, \emph{relational} information between the classes can be exploited to enable predicting a class even when there are no examples with this label in the training set.
%
Such relational data is commonly available, or can be constructed with the help of knowledge bases \cite{chen2022zeroshot}, ontologies \cite{ontozsl}, or powerful foundation models \cite{stewart2017label}.


% OLD 2
%Why is it interesting and important?
%To handle this scenario, a variety of approaches have been proposed in different areas of machine learning. 
%
%Solutions have been offered in structured prediction, hierarchical classification \cite{barz2019hierarchy}, extreme classification \cite{xc_graph_1}, and in zero-shot learning \cite{rios-kavuluru-2018-shot}.
% 
%The common thread among these techniques is to use available structure in the label space.
%
%Specifically, \emph{relational} information between the classes can be exploited to enable predicting a class even when there are no examples with this label in the training set.
%
%Such relational data is commonly available, or can be constructed with the help of knowledge bases, ontologies, or powerful foundation models \cite{stewart2017label}.

%Why is it hard? (E.g., why do naive approaches fail?)
%Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
The best way to exploit relational structure remains unclear, with a number of key challenges: 
%
We might wish to know what particular subset of classes is rich enough to enable predicting many (or all) remaining labels.
%
This is crucial in determining whether a training set is usable or, even with the aid of structure, insufficient.
%
It is also unclear how approaches that use relational information interact with the statistical properties of learning, such as training sample complexity.
%
Finally, performing adaptation requires an efficient and scalable algorithm. 

% Figure environment removed

%What are the key components of my approach and results? Also include any specific limitations.
This work addresses these challenges.
%
It proposes a simple and practical approach to learning in structured label spaces, with theoretical guarantees.
%
First, we offer a simple way to translate the soft outputs (i.e., probability vectors) produced by \emph{any} supervised learning model into a more general model that can exploit geometric information for label structure.
%
In other words, our approach, called $\loki$,\footnote{Refers to the `locus' (plural: \textit{loci}) of the Fr√©chet mean.} is a simple \emph{adaptor} for pretrained models. 
%
$\loki$ can be applied via a fixed linear transformation of the model outputs.
%
$\loki$'s simplicity makes it applicable to a broad range of settings while enabling very high-cardinality predictions subject to a potentially small model output budget. We provide a visualization of this key idea in Figure~\ref{fig:banner}. 

%\fred{this PP shows up very close to the figure above---any way to create some space?} 
Theoretically, we provide a rich set of results for the metric-based adaptation setting.
%
First, we introduce a learning-theoretic result in terms of the sample complexity of the pretrained model.
% 
It captures a key tradeoff between the number of classes and metric structure, the problem dimensionality, and the number of samples used to train the model prior to adaptation. 
%
Next we exhaustively study the properties of training sets, determining for a wide class of relational structures the minimum number (and structure) of subsets that enable reliable prediction.
%
Finally we show how to exploit this result in an active learning-like approach to selecting points that will improve deficient training datasets.

Experimentally, we show that using structural information improves prediction in high-cardinality settings. 
%
We demonstrate the strength of the active learning-based approach to dataset expansion over random baselines.
%
Finally, and most excitingly, we show that even in zero-shot models like CLIP, where it is possible to produce probability vectors over any possible class, the use of our adaptor leads to a \textbf{19.53\%} relative improvement. 
%improves performance by {\color{red} X\%} points.
