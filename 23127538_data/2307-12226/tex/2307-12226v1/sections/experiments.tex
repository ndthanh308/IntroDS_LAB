%!TEX root = ../main.tex

\section{Experimental Results}
\label{sec:experiments}

\begin{table}[t!]
    \caption{
    \textbf{CIFAR-100.} Improving CLIP predictions using $\loki$. 
    Results are reported as $\E[d^2(y, \hat{y})]$ in the respective metric space. 
    CLIP-like zero-shot models can be improved using $\loki$ even without access to an external metric, and internal class embedding distances are used.  
    When an external metric is available, $\loki$ outperforms CLIP using the default CIFAR-100 hierarchy and WordNet. 
    } \label{table:clip_improver}
  \begin{center}
    \begin{tabular}{rr|ccc}
    \toprule 
    Model & Metric Space& $\argmax$ & $\loki$ & Relative Improvement \\
    \midrule
    \rowcolor[gray]{.9} 
    CLIP-RN50 \cite{clip}& Internal & 0.2922  & {\bf 0.2613}  & {\bf 10.57\%}  \\
    CLIP-ViT-L-14 \cite{clip}& Internal & 0.1588  & {\bf 0.1562}  &  {\bf1.63\%}  \\
    \rowcolor[gray]{.9} 
    ALIGN \cite{align}       & Internal & 0.1475  & {\bf 0.1430}  & {\bf 3.02\%}  \\
    %AltCLIP \cite{altclip}   & Internal & {\bf0.1074}  & 0.1088  & -1.33\%  \\
    \midrule
    CLIP-RN50 \cite{clip}& $K_{100}$           & 0.5941$^*$  & 0.5941$^*$  & 0.0\%$^*$  \\
    \rowcolor[gray]{.9} 
    CLIP-RN50 \cite{clip} & Default tree & 7.3528  & {\bf 7.1888} & {\bf 2.23\%} \\
    CLIP-RN50 \cite{clip} & WordNet tree & 24.3017  & {\bf 19.5549} & {\bf 19.53\%} \\
    %\rowcolor[gray]{.9}
    \bottomrule
    \end{tabular}
    \\
    $^*$ methods equivalent under the complete graph as $\loki$ reduces to $\argmax$ prediction. \\
    
  \end{center}
\end{table}



% here are the claims we're going to validate
% sections for the types of experiments
% each one should follow this structure
% what we're validating/explaining
% explain
% don't call them imagenet / wordnet
% call them large label space smth
% results paragraph 
% repeat 
%
% tables? 

% In this section, we provide experimental results to validate the following claims:
% \begin{compactenum}
%     \item $\loki$ adapts to large label spaces with many unobserved classes, as shown using ImageNet. 
%     \item When using CLIP, a zero-shot foundation model, $\loki$ can improve performance on CIFAR-100 even when all classes are observed. 
%     \item The active learning approach given in Theorem~\ref{thm:active_largest} consistently yields a larger locus than random next class selection. 
% \end{compactenum}

In this section, we provide experimental results to validate the following claims:
\begin{enumerate}
    %\item When using zero-shot foundation models like CLIP, $\loki$ can improve performance even when all classes are observed and no external metric is available. 
    \item $\loki$ improves performance of zero-shot foundation models even with no external metric. 
    \item $\loki$ adapts to label spaces with a large number of unobserved classes. 
    \item The active approach given in Theorem~\ref{thm:active_largest} yields a larger locus than the passive baseline.  
\end{enumerate}


\subsection{$\loki$ Improves Zero-Shot Models}
We evaluate the capability of $\loki$ to improve upon zero-shot models where all classes are observed.

\paragraph{Setup} 
Our experiment compares the zero-shot prediction performance of CLIP \cite{clip} on CIFAR-100 \cite{cifar100} to CLIP logits used with $\loki$. 
First, we consider the setting in which no external metric relating the labels is available, and instead derive internal metric information from Euclidean distances between text embeddings from the models using their respective text encoders. 
Second, we consider three external metric spaces for use with $\loki$: the complete graph $K_{100}$ and two phylogenetic trees: the default CIFAR-100 superclasses \cite{cifar100} and WordNet \cite{barz2019hierarchy}. 
%Performance is measured using the mean squared distance with respect to each graph. 

\paragraph{Results} 
The results of this experiment are given in Table~\ref{table:clip_improver}. 
When no external metric information is available, $\loki$ still outperforms CLIP-like models that use the standard prediction rule---in other words, $\loki$ seems to unconditionally improve CLIP. 
As expected, under the complete graph, our method becomes equivalent to the standard prediction mechanism used by CLIP. 
On the other hand, $\loki$ outperforms CLIP when using the default CIFAR-100 tree hierarchy and even more so when using the WordNet geometry, with a ${\bf 19.53\%}$ relative improvement in mean squared distance over the CLIP baseline.
We postulate that the strong performance using WordNet is due to the richer geometric structure compared to that of the default CIFAR-100 hierarchy. 


\begin{wraptable}{R}{.35\linewidth}

\caption{
\textbf{PubMed.} $\loki$ improves baseline for all settings of $K$. 
The metric space is Euclidean distances applied to SimCSE embeddings. 
} \label{table:pubmed}
\begin{center}
    \begin{tabular}{r|cc}
    \toprule 
    $K$ & 5-NN  & 5-NN + \loki   \\
    \midrule
    \rowcolor[gray]{.9} 
    100 &1.68666   & {\bf 1.42591} \\
    250 &1.52374   & {\bf 1.47801} \\
    \rowcolor[gray]{.9} 
    500 &1.64074  & {\bf 1.45921} \\
    \bottomrule
    \end{tabular}
\end{center}
\caption{
\textbf{LSHTC.} $\loki$ improves baseline for all settings of $K$. 
We summarize the metric space graph by generating 10,000 supernodes. 
} \label{table:lshtc}
\begin{center}
    \begin{tabular}{r|cc}
    \toprule 
    $K$ & 5-NN  & 5-NN + \loki   \\
    \midrule
    \rowcolor[gray]{.9} 
    50  & 1.2519   &{\bf 0.2896} \\
    100 & 1.4476   &{\bf 0.3467} \\
    \rowcolor[gray]{.9} 
    150 & 1.7225  &{\bf 0.3969} \\
    200 & 1.7092  &{\bf 0.3171} \\
    \rowcolor[gray]{.9} 
    250 & 1.6465  &{\bf 0.3995} \\
    300 & 1.6465  &{\bf 0.4004} \\
    \bottomrule
    \end{tabular}
\end{center}

\end{wraptable}


\subsection{$\loki$ on Partially-Observed Label Spaces}

To validate our approach on partially observed label spaces, we evaluate the performance of adapting a logistic classifier trained on SimCLR embeddings of ImageNet \cite{imagenet}, 5-NN models trained on a 9,419 class subset of the PubMed dataset,\footnote{\url{https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification}} and the 325,056-class LSHTC dataset \cite{Partalas2015LSHTCAB}. 

\paragraph{Setup} 
For ImageNet, we use the WordNet phylogenetic tree as the metric space \cite{barz2019hierarchy}. 
In this setting, we sample random subsets of size $K$ of the 1000 ImageNet classes and compare a baseline one-vs-rest classifier to the same classifier but using $\loki$ to adapt predictions to classes beyond the original $K$. 
We conduct two sets of experiments. 
In the first, we sample $K$ classes uniformly, while in the second, we adopt a more realistic sampler---we sample from a Gibbs distribution: $P(\lambda | \theta) = \frac{1}{Z}\exp(-\theta d(\lambda, \lambda^c))$,
where $\lambda^c = m_{\set{Y}}(\boldsymbol{1}_N)$ is the centroid of the metric space, $\theta$ is the concentration parameter around the centroid, and $Z$ is the normalizer. 
While the Gibbs distribution sampler is more realistic, it is also the more challenging setting---classes which have low probability according to this distribution are less likely to appear in the locus. 
For PubMed, we derive our metric from Euclidean distances between SimCSE class embeddings \cite{gao2021simcse}. 
Finally for LSHTC, we summarize the default graph by randomly selecting nodes and merging them with their neighbors until we obtain a graph with 10,000 supernodes representing sets of classes. 

% Figure environment removed


\paragraph{Results} Figure~\ref{fig:imagenet_rel_improve} shows the mean squared distances compared to the baseline one-vs-rest classifiers, across various settings of $K$. 
We find that $\loki$ always significantly outperforms the baseline, even in the more challenging setting of sampling according to a Gibbs distribution. 
Tables~\ref{table:pubmed}~and~\ref{table:lshtc} show our improvements when using $\loki$ over the baseline 5-NN models. 
While $\loki$ consistently yields an improvement on PubMed and LSHTC, the improvement is more dramatic on LSHTC. 
%We additionally include accuracy results for LSHTC in the Appendix---we show an improvement using $\loki$ despite accuracy being misspecified for our problem setting. 

% Furthermore, we show in Figure~\ref{fig:imagenet_rel_improve} that $\loki$ consistently outperforms the baseline one-vs-rest classifier in terms of $\hat{\E}[d^2(y, \hat{y})]$ under both uniform class sampling and sampling from the Gibbs distribution. 
% Notably, the performance of both methods improve more slowly as $K$ increases under sampling from the Gibbs distribution---nonetheless, $\loki$ still outperforms the baseline. 







\subsection{Large Loci via Active Learning}
Finally, we evaluate our active next class selection approach. 
We expect that compared to passive (random) selection, our active approach will lead to larger loci, and in practice, a larger set of possible classes that can be predicted using $\loki$ while observing fewer classes during training. 

\paragraph{Setup} We compare with a baseline that randomly selects the next class. 
We first generate a synthetic random tree with size $N$ and fix an initial $K$. In active selection, we use the approach described in Theorem~\ref{thm:active_largest} to select the next node. 
As a passive baseline, we randomly sample (without replacement) the remaining nodes that are not in $\Lambda$.

\paragraph{Results} The results are shown in Figure~\ref{fig:imagenet_rel_improve}. 
We set $N = 20$ and the initial $K = 3$, then we average the result over 100 independent trials.
The active approach consistently outperforms the random baseline as it attains a larger locus with fewer nodes selected. 


% % Figure environment removed


