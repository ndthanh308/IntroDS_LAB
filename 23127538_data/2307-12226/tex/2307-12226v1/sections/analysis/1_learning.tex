%!TEX root = ../../main.tex

% \subsection{Learning Reduction to One Binary Classifier}
% What is the interplay between predicting unobserved classes based on metric space information and standard learning-theoretic notions like sample complexity? 
% Our first result illustrates this tradeoff for a simple setting.

% Suppose that we wish to perform $m$-class classification where the label space is endowed with a metric induced by the path graph. This problem is effectively ordinal classification---however, in contrast to standard approaches which train $m-1$ binary classifiers, we would like to do so using \textit{only a single binary classifier}. 

% \begin{lemma}[Reduction to one binary classifier]
% \label{lemma:learning}
% Let $G=(\set{V}, \set{E})$ with $\set{V} = \{V_i\}_{i=1}^m$ and $\set{E}=\{(V_i, V_{i+1})\}_{i=1}^{m-1}$ be a path graph. 
% Let $\set{Y}=\set{V}$ and let $\Lambda = \{V_1, V_m\}$ be the set of observed classes. 
% Under the following model: 
% \begin{align*}
%     \mathbb{P}(y=V_1 \ | \ x \text{ and } y \in \Lambda) =: \mathbb{P}_1 &= \frac{1}{1 + \exp\{-x^\top\beta\}}\\ 
%     &= \frac{d(V_\star, V_m)}{d(V_1, V_m)},
% \end{align*}
% \[ \mathbb{P}(y=V_m \ | \ x \text{ and } y \in \Lambda) =: \mathbb{P}_m = 1 - \mathbb{P}_1,\]
% where we have access to $\widehat{\mathbb{P}}_1$ and $\widehat{\mathbb{P}}_m = 1 - \widehat{\mathbb{P}}_1$, the sample complexity of estimating an intermediate unobserved class along the path, using $\loki$, is $O(d m^2)$. 
% \end{lemma}

% This means that the number of samples required to estimate $\mathbb{P}_1$ accurately enough to predict an unobserved class scales with $m^2$---quadratic in the number of intermediate unobserved nodes between $V_1$ and $V_m$. 

\subsection{Sample Complexity}
What is the interplay between predicting unobserved classes based on metric space information and standard learning-theoretic notions like the sample complexity needed to train a model? 
Our first result illustrates this tradeoff and relates it to the squared distance loss. 

%Suppose that we wish to perform $m$-class classification where the label space is endowed with a metric induced by the shortest path distance on a graph. 
Suppose that we only observe $K \leq N$ of the classes at training time, and that we train a $K$-class logistic classifier. 
We use $\loki$ to adapt our pretrained logistic classifier to enable classification of any of the $N$ classes. 
We have that (a formal statement is given in the Appendix), 
%\textit{regardless of whether or not they were observed at training time.}

\begin{theorem}[\textbf{Informal} $\loki$ sample complexity]
\label{theorem:learning}
Let $G=(\set{V}, \set{E})$ with $\set{V} = \{V_i\}_{i=1}^N$ be an arbitrary graph. 
Let $\set{Y}=\set{V}$ and let $\Lambda = \{V_j\}_{j \in [K]} \subseteq \set{Y}$ be the set of observed classes. 
Under the following logistic model: 
%\begin{align*}
    $\mathbb{P}(\widehat{V} = V_i \ | \ x) =: \mathbb{P}_i = \frac{\exp\{-x^\top\beta_i\}}{\sum_{j\in [K]} \exp\{-x^\top\beta_j\}} \mathbf{1}\{ \widehat{V} \in \Lambda\}$, 
%\end{align*}
where we have access to estimates $\widehat{\mathbb{P}}_i$ for all $i \in [K]$, 
and assuming realizability for predicting some class $V_*$, 
%\begin{align*}
$    V_* \in m_{[\mathbb{P}_i]_{i \in [K]}} = \argmin_{V \in \set{Y}} \sum_{i \in [K]} \mathbb{P}_i d^2(V, V_i)$, 
%\end{align*}
then with high probability, the sample complexity of estimating the target $V_*$ with prediction $\widehat{V}$ is 
%\[d^2(V_*, \widehat{V}) \leq O( \frac{K^2 \sqrt{d}}\times\text{diam}(G) ^2)/({\alpha \sqrt{n}}) ,\]
\begin{align*}
    d^2(V_*, \widehat{V}) \leq O\left( \frac{K^2 \sqrt{d}}{\alpha \sqrt{n}} \text{diam}(G) ^2\right),
\end{align*}
where $d$ is the dimensionality of the input, $\text{diam}(G)$ is the graph diameter, $n$ is the number of samples, and $\alpha$ is related to the sensitivity of the Fr√©chet variance to different node choices. 
\end{theorem}

In other words, we can control the prediction error \textit{on the level of individual predictions} as a function of the number of samples used to train the underlying classifier.  
%Additionally, we see how this prediction error scales with the diameter of the metric space associated with the label set. 

