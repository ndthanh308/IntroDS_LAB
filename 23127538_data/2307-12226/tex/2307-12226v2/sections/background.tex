%!TEX root = ../main.tex
\section{Background}
%First, we provide some brief background, notation, and introduce our problem setting. 
First, we introduce the problem setting, notation, and mathematical tools that we will use. 
Afterward, we discuss how $\loki$ relates to prior work. 

%\subsection{Problem Setting}
\paragraph{Problem Setting} As in conventional supervised learning, we have a dataset $(x_1, y_1), \ldots, (x_n, y_n)$ drawn from a distribution on $\mathcal{X} \times \Y$, where $\mathcal{X}$ and $\mathcal{Y}$ are the input and label spaces. In our setting, $N := |\mathcal{Y}|$ is finite but large; often $N \gg n$---so that many labels will simply not be found in the training dataset. We let $\Lambda \subseteq \set{Y}$ with $|\Lambda| = K$ be the set of observed labels.
For convenience of notation, we also define $\boldsymbol{\lambda} := [\lambda_i]_{i=1}^K$, $\boldsymbol{y} := [y_i]_{j=1}^N$ to be the vectors of elements in $\Lambda$ and $\mathcal{Y}$.

In addition to our dataset, we have access to a relational structure on $\Y$. We assume that $\Y$ is a metric space with metric (distance) $d : \Y^2 \rightarrow \mathbb{R}$; $d$ encodes the relational structure of the label space. 
Specifically, we model this metric space using a graph, $G = (\set{Y}, \set{E})$ where $\set{E} \subseteq \set{Y}^2 \times \mathbb{R}_{+}$ is a set of edges relating the labels and the standard shortest-path distance $d: \set{Y}^2 \to \mathbb{R}_{\geq 0}$. 
In addition to its use in prediction, the metric $d$ can be used for evaluating a model by measuring, for example, $\frac{1}{n} \sum_{i=1}^n d^2(f(x_i), y)$---the analogue of the empirical square loss.

\paragraph{Fréchet Mean Estimator} 
Drawing on ideas from structured prediction \cite{Ciliberto2016, Rudi18}, we use a simple predictor that exploits the label metric. It relies on computing the \textit{Fréchet mean} \cite{Frechet1948}, given by
\begin{align} m_{\bm{y}}(\bm{w}) := \argmin_{y \in \set{Y}} \sum_{i=1}^K \bm{w}_i d^2(y, \bm{y}_i),
\label{def:fm}
\end{align}
where $\w \in \mathbb{R}_{\geq 0}^K$ is a set of weights. 
The Fr\'{e}chet mean generalizes the barycenter to metric spaces and is often used in geometric machine learning \cite{Lou20}. 
%As $m_{\bm{y}}(\cdot)$ is $0$-positively homogeneous in $\bm{w}$ for any minimizer, we restrict $\bm{w} \in \Delta^{K-1} \subseteq \mathbb{R}_{\geq 0}^K$ without loss of generality.

\paragraph{Locus of the Fréchet mean.} 
The \textit{locus of the Fréchet mean} is the set of all Fréchet means under different weights \cite{pca_phylogenetic}. 
%It is essentially the image of $m_{\bm{y}}$. 
%As the minimizer need not be unique, we write the locus as
%
We write it as $\Pi(\bm{y}) := \cup_{\bm{w} \in \Delta^{K-1}}  m_{\bm{y}}(\bm{w})$.

$\Pi(\bm{y})$ can be thought of the set of all labels in $\set{Y}$ that are reachable by the Fréchet mean given $\{\bm{y}_i\}_{i=1}^K \subseteq \set{Y}$ and different choices of its parameter $\bm{w}$. 
%
Intuitively, we can think of the locus for a given dataset as describing how usable it is for predicting beyond the observed labels. 
%
Trivially, if $\{\bm{y}_i\}_{i=1}^K = \set{Y}$, then $\Pi(\bm{y}) = \set{Y}$. % (consider the corners of the unit simplex).
%
We are primarily interested in the case in which $\{\bm{y}_i\}_{i=1}^K \subset \set{Y}$ yet we still have $\Pi(\bm{y}) = \set{Y}$, or that $|\Pi(\bm{y})|$ is at least sufficiently large. 

\subsection{Relation to Prior work}
$\loki$ is primarily related to two areas: zero-shot learning and structured prediction. 

\paragraph{Zero-Shot Learning}
Like zero-shot learning (ZSL), $\loki$ is capable of predicting unobserved classes.
%Standard ZSL involves first learning a classifier on one set of classes and exploits some form of side information in order to predict a second set of classes that are disjoint from the first. 
Our framework is most closely related to \textit{generalized} ZSL, which uses side information to predict both observed and unobserved classes. 
%---the union of the two aforementioned sets \cite{generalized_ZSL, GAN_KG_ZSL, zeroshotsurvey}. 
Many types of external knowledge are used in ZSL, including text \cite{text_zsl, text_zsl_2, text_zsl_3}, attributes \cite{attr_zsl_1, attr_zsl_2, attr_zsl_3}, knowledge graphs \cite{KG_zsl_1, KG_zsl_2, KG_zsl_3}, ontologies \cite{ont_zsl_1}, and logical rules \cite{ont_zsl_2}. 
% 
Our work is most closely related to ZSL approaches that rely on knowledge graph information. 
Often, ZSL methods that use knowledge graph information rely on the use of graph neural network architectures \cite{KG_zsl_1, Kampffmeyer2018RethinkingKG, kipf2017semisupervised}.
\textit{However, we note that these architectures can be heavyweight and can be challenging to scale up to extremely large graphs, whereas $\loki$ does not have architectural requirements and scales linearly in the size of the graph when $K$ is small.} 
%\nick{TODO discuss the specific types of settings in ZSL that this relates to, and GNNs, but discuss the limitations of taking a GNN-based approach in our problem setting (for example, it might be hard to scale them to extremely large graphs, whereas for us, the scaling is essentially linear for $|\Lambda| << |\mathcal{Y}|$)}

% \paragraph{Extreme Classification}
% Extreme classification (XC) aims to learn systems of feature extractors and classifiers that can label a new point with a relevant subset of classes from an extremely high cardinality label space with potentially hundreds of thousands to millions of classes. 
% Many techniques in XC employ graph side information  \cite{xc_graph_1, xc_graph_2}, but tend to be highly specialized and involve complex architectures. 
% \textit{In contrast, $\loki$ is merely a linear transformation of standard classifier outputs.} 

% XC problems are evaluated using a variety of non-accuracy performance metrics, as accuracy alone is insufficient for such high cardinality label spaces \cite{Bhatia16}. 
% In Section~\ref{sec:experiments}, we validate $\loki$ using the expected squared distance, which is more appropriate than accuracy for high cardinality label spaces equipped with a distance metric. 

%\nick{TODO relation to extreme classification / discussion of how they often employ a variety of non-accuracy eval metrics and can’t rely solely on acc for huge label spaces}

\paragraph{Structured Prediction}
Structured prediction (SP) operates in label spaces that are endowed with algebraic or geometric structure \cite{bakir_predicting_2007, NIPS2015_52d2752b}. 
This includes problems such as predicting permutations \cite{NEURIPS2018_b3dd760e}, non-Euclidean and manifold regression \cite{Petersen2016FrchetRF, NEURIPS2018_f6185f0e}, and learning on graphs \cite{NEURIPS2019_ea697987}. 
$\loki$ is the most immediately related to the latter, however, any finite metric space can be represented as a graph, which further lends to the flexibility of our approach. 
Even in discrete structured prediction settings, the cardinality of the label space may be combinatorially large. 
\textit{As such, $\loki$ can be viewed as a simple method for adapting classifiers to structured prediction. }
%Or more formally, as a learning reduction from structured prediction to classification.} 

The Fréchet mean has been used %previously in supervised learning. 
in structured prediction---but in approaches requiring training. 
%
%Generically, for $x \in \mathcal{X}$, it suffices to convert $x$ into a set of weights $w$ to make a prediction (in our case, these are classifier outputs). 
%Once $w$ is computed, the optimization problem in \ref{def:fm} is solved and the resulting minimizer is the predicted label.
In \cite{Ciliberto2016}, 
$ \hat{f}(x) =$  $\arg\min_{y \in \Y}$  $\frac{1}{n}\sum_{i=1}^n \alpha_i(x)d^2(y, \bm{y}_i)$, 
where $\alpha(x) = (K + n \nu I)^{-1} K_x$. 
$K$ is the kernel matrix for a kernel 
$k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, so that $K_{i,j} = k(x_i, x_j)$, $(K_x)_i = k(x, x_i)$. 
$\nu$ is a regularization parameter. 
In other words, the weight $w_i$ corresponding to $\bm{y}_i$ is the average produced by solving a kernel regression problem at all points $x_k$ in the dataset where $y_k = \bm{y}_i$. 
%
It has also used in weak supervision (WS) for metric-equipped label spaces
%, and have also used the Fréchet mean for prediction 
\cite{vishwakarma2022lifting, shin2022universalizing}, where the goal is to produce labeled data for training structured prediction models. 
%Without metric information, WS does not trivially scale to high cardinality classification tasks. 
%In light of this, we view $\loki$ as a potential starting point for applying WS to high cardinality classification problems. 


% \nick{TODO discuss Ciliberto thing, combinatorially large but metric label spaces, Weak Supervision for structured prediction.}



% \emph{Example}: In \cite{Ciliberto2016}, 
% $ \hat{f}(x) =$  $\arg\min_{y \in \Y}$  $\frac{1}{n}\sum_{i=1}^n \alpha_i(x)d^2(y, \bm{y}_i)$, 
% where $\alpha(x) = (K + \nu \b{I})^{-1} K_x$. 
% $K$ is the kernel matrix for a kernel 
% $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, so that $K_{i,j} = k(x_i, x_j)$, $(K_x)_i = k(x, x_i)$. 
% $\nu$ is a regularization parameter. 
% In other words, the weight $w_i$ corresponding to $\bm{y}_i$ is the average produced by solving a kernel regression problem at all points $x_k$ in the dataset where $y_k = \bm{y}_i$. 

