%!TEX root = ../../main.tex

% \subsection{Learning Reduction to One Binary Classifier}
% What is the interplay between predicting unobserved classes based on metric space information and standard learning-theoretic notions like sample complexity? 
% Our first result illustrates this tradeoff for a simple setting.

% Suppose that we wish to perform $m$-class classification where the label space is endowed with a metric induced by the path graph. This problem is effectively ordinal classification---however, in contrast to standard approaches which train $m-1$ binary classifiers, we would like to do so using \textit{only a single binary classifier}. 

% \begin{lemma}[Reduction to one binary classifier]
% \label{lemma:learning}
% Let $G=(\set{V}, \set{E})$ with $\set{V} = \{V_i\}_{i=1}^m$ and $\set{E}=\{(V_i, V_{i+1})\}_{i=1}^{m-1}$ be a path graph. 
% Let $\set{Y}=\set{V}$ and let $\Lambda = \{V_1, V_m\}$ be the set of observed classes. 
% Under the following model: 
% \begin{align*}
%     \mathbb{P}(y=V_1 \ | \ x \text{ and } y \in \Lambda) =: \mathbb{P}_1 &= \frac{1}{1 + \exp\{-x^\top\beta\}}\\ 
%     &= \frac{d(V_\star, V_m)}{d(V_1, V_m)},
% \end{align*}
% \[ \mathbb{P}(y=V_m \ | \ x \text{ and } y \in \Lambda) =: \mathbb{P}_m = 1 - \mathbb{P}_1,\]
% where we have access to $\widehat{\mathbb{P}}_1$ and $\widehat{\mathbb{P}}_m = 1 - \widehat{\mathbb{P}}_1$, the sample complexity of estimating an intermediate unobserved class along the path, using $\loki$, is $O(d m^2)$. 
% \end{lemma}

% This means that the number of samples required to estimate $\mathbb{P}_1$ accurately enough to predict an unobserved class scales with $m^2$---quadratic in the number of intermediate unobserved nodes between $V_1$ and $V_m$. 

\subsection{Sample Complexity}
What is the interplay between predicting unobserved classes based on metric space information and standard learning-theoretic notions like the sample complexity needed to train a model? 
Our first result illustrates this tradeoff, and relates it to the squared distance loss. 
Suppose that we only observe $K \leq N$ of the classes at training time, and that we fit a $K$-class Gaussian mixture model. 
We use $\loki$ to adapt our pretrained classifier to enable classification of any of the $N$ classes. We have that (a formal statement and interpretation are in the Appendix),  
%\textit{regardless of whether or not they were observed at training time.}

% \begin{theorem}[\textbf{Informal} $\loki$ sample complexity]
% \label{theorem:learning}
%     Let $G=(\set{V}, \set{E})$ with $\set{V} = \{V_i\}_{i=1}^N$ be an arbitrary graph. 
%     Let $\set{Y}=\set{V}$ and let $\Lambda = \{V_j\}_{j \in [K]} \subseteq \set{Y}$ be the set of observed classes. 
%     Under the following logistic model: 
%     %\begin{align*}
%         $\mathbb{P}(\widehat{V} = V_i \ | \ x) =: \mathbb{P}_i = \frac{\exp\{-x^\top\beta_i\}}{\sum_{j\in [K]} \exp\{-x^\top\beta_j\}} \mathbf{1}\{ \widehat{V} \in \Lambda\}$, 
%     %\end{align*}
%     where we have access to estimates $\widehat{\mathbb{P}}_i$ for all $i \in [K]$, 
%     and assuming realizability for predicting some class $V_*$, 
%     %\begin{align*}
%     $    V_* \in m_{[\mathbb{P}_i]_{i \in [K]}} = \argmin_{V \in \set{Y}} \sum_{i \in [K]} \mathbb{P}_i d^2(V, V_i)$, 
%     %\end{align*}
%     then with high probability, the sample complexity of estimating the target $V_*$ with prediction $\widehat{V}$ is 
%     \[d^2(V_*, \widehat{V}) \leq O( {K^2 \sqrt{d}}\times\text{diam}(G) ^2)/({\alpha \sqrt{n}}) ,\]
%     %\begin{align*}
%     %    $d^2(V_*, \widehat{V}) \leq O\left( \frac{K^2 \sqrt{d}}{\alpha \sqrt{n}} \text{diam}(G) ^2\right),$
%     %\end{align*}
%     where $d$ is the dimensionality of the input, $\text{diam}(G)$ is the graph diameter, $n$ is the number of samples, and $\alpha$ is related to the sensitivity of the Fréchet variance to different node choices. 
% \end{theorem}
% %In other words, we achieve the same rate as standard supervised learning, but have to pay more depending on the interplay between the graph

\begin{theorem}[\textbf{Informal} $\loki$ sample complexity]
\label{theorem:learning}
    %
    Let $\mathcal{Y}$ be a set of classes represented by $d$ dimensional vectors under the Euclidean distance, and let $\Lambda \subseteq \mathcal{Y}$ be the set of $K$ observed classes. 
    %
    Assume that $n$ training examples are generated by an identity covariance Gaussian mixture model over classes $\Lambda$, and that test examples are generated over all classes $\mathcal{Y}$. 
    %
    Assume that we estimate a Gaussian mixture model on the training set and obtain probability estimates $\hat{\mathbb{P}}(y_i | x)$ for $i \in [K]$ for a sample $(x, y_*)$ from the test distribution. 
    %
    Then with high probability, under the following model, 
    \[
        \hat{y}_* \in m_\Lambda([\hat{\mathbb{P}}(y_i | x)]_{i \in [K]}) 
        = \argmin_{y \in \mathcal{Y}} \sum_{i \in [K]} \hat{\mathbb{P}}(y_i | x) d^2(y, y_i)
    \]
    the sample complexity of estimating target $y_*$ from the test distribution $\mathcal{D}_{\text{test}}$ with prediction $\hat{y}_*$ is:
    \[
        \E_{(x, y_*)\sim \mathcal{D}_{\text{test}}} [d^2(y_*, \hat{y}_*)] \leq O\left( \frac{d}{\alpha} \sqrt{\frac{\log K/\delta}{n}} \left( \frac{1}{ \left( R^{1 - \frac{2}{d}} - \frac{\log R}{R}\right) } + \sqrt{d} \right) \right)
    \]
    where $\alpha$ is related to the sensitivity of the Fréchet variance to different node choices. 

\end{theorem}