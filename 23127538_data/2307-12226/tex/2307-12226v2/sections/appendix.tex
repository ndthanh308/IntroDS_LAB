%!TEX root = ../main.tex
\newpage
\appendix
\onecolumn

The appendix is organized as follows: in Appendix~\ref{app:deferred_proofs}, we provide proofs of the claims made in the main text. 
Next, in Appendix~\ref{app:time_complexity}, we provide algorithms and time complexity analyses for the algorithms referenced in the main text. 
Appendix~\ref{app:experiments} contains additional experimental results. 
In Appendix~\ref{app:experimental_details}, we provide additional details about our experimental setup. 
Then in Appendix~\ref{app:broader_impacts}, we discuss the broader impacts and limitations of $\loki$. 
Finally, in Appendix~\ref{app:vis}, we provide more details and examples of the locus visualizations shown in Figure~\ref{fig:banner}. 

%\nick{Reminder to add result we promised to R3: ``In addition, the updated version of our draft contains a synthetic data experiment that further validates the theoretical results.''}

% \nick{Reminder to release code}

% \nick{Reminder to add broader impacts etc... from author checklist}


\section{Deferred Proofs}
\label{app:deferred_proofs}

\subsection{Proof of Theorem~\ref{theorem:learning} ($\loki$ sample complexity)}
In this section, we provide a formal statement and proof of our sample complexity result for $\loki$, including additional required definitions and assumptions. 

First, we define the required tools to prove the sample complexity bound for $\loki$. 
For the purposes of this proof, we define the Fréchet variance as the function over which the Fréchet mean returns the minimizer. 
\begin{definition}[Fréchet variance]
    The Fréchet variance is defined as 
    \[ \Psi_\mathbf{w}(V) := \sum_{i \in [K]} \mathbf{w}_i d^2(V, V_i). \]
\end{definition}

Additionally, we will require a technical assumption related to the sensitivity of the Fréchet variance to different node choices.
\begin{assumption}[Fréchet variance is $\frac{1}{\alpha}$-bi-Lipschitz]
\label{assumption:bilipschitz}
For a metric space defined by a graph $G = (\set{V}, \set{E})$, the Fréchet variance is $K$-bi-Lipschitz if there exists a $K \geq 1$ such that
\[ \frac{1}{K} d^2(V, \tilde{V}) \leq |\Psi_\mathbf{w}(V) - \Psi_\mathbf{w}(\tilde{V})| \leq K d^2(V, \tilde{V}) \]
for all $V, \tilde{V} \in \set{V}$, and a fixed $\mathbf{w} \in \Delta^{K-1}$. 
For our purposes, such a $K$ always exists: consider setting $K = \text{diam}(G)^2 \max_{V_1, V_2 \in \set{V}} |\Psi_\mathbf{w}(V_1) - \Psi_\mathbf{w}(V_2)|$. 
However, this is a very conservative bound that holds for all graphs that we consider. 
Instead, we assume access to the largest $\alpha = \frac{1}{K} \leq 1$ such that 
\[\alpha d^2(V, \tilde{V}) \leq |\Psi_\mathbf{w}(V) - \Psi_\mathbf{w}(\tilde{V})|,\] which may be problem dependent. 
\end{assumption}

% \begin{theorem}[$\loki$ sample complexity]
% \label{thm:learning_formal}
% Let $G=(\set{V}, \set{E})$ with $\set{V} = \{V_i\}_{i=1}^N$ be an arbitrary graph. 
% Let $\set{Y}=\set{V}$ and let $\Lambda = \{V_j\}_{j \in [K]} \subseteq \set{Y}$ be the set of observed classes. 
% Under the following logistic model: 
% $\mathbb{P}(\widehat{V} = V_i \ | \ x) =: \mathbb{P}_i = \frac{\exp\{-x^\top\beta_i\}}{\sum_{j\in [K]} \exp\{-x^\top\beta_j\}} \mathbf{1}\{ \widehat{V} \in \Lambda\}$, 
% where we have access to estimates $\widehat{\mathbb{P}}_i$ for all $i \in [K]$, 
% and assuming realizability for predicting some class $V_*$, 
% $V_* \in m_{\Lambda} ([\mathbb{P}_i]_{i \in [K]}) = \argmin_{V \in \set{Y}} \sum_{i \in [K]} \mathbb{P}_i d^2(V, V_i)$, 
% then with high probability, the sample complexity of estimating the target $V_*$ with prediction $\widehat{V}$ is 
% \[d^2(V_*, \widehat{V}) \leq O( {K^2 \sqrt{d}}\times\text{diam}(G) ^2)/({\alpha \sqrt{n}}) ,\]
% where $d$ is the dimensionality of the input, $\text{diam}(G)$ is the graph diameter, $n$ is the number of samples, and $\alpha$ is our parameter under Assumption~\ref{assumption:bilipschitz}. 
% \end{theorem}

\begin{theorem}[$\loki$ sample complexity]
    \label{thm:learning_formal}
    %
    %Let $\mathcal{Y}$ be a $\frac{2R}{\sqrt{d}{K} - 1}$-net on the $d$ dimensional 2-norm ball of radius $R$ under the Euclidean distance, and let $\Lambda \subseteq \mathcal{Y}$ be the set of $K$ observed classes. 
    Let $\mathcal{Y}$ be a set of points on the $d$ dimensional 2-norm ball of radius $R$, and let $\Lambda \subseteq \mathcal{Y}$ be the set of $K$ observed classes.
    %
    Assume that $\Lambda$ forms a $2R/(\sqrt{d}{K} - 1)$-net under the Euclidean distance. 
    %
    Assume that training examples are generated by drawing $n$ samples from the following process: draw $x \sim \mathcal{N}(y, I)$ where $y \sim \text{Unif}(\Lambda)$, and at test time, draw $x \sim \mathcal{N}(y, I)$ where $y \sim \text{Unif}(\mathcal{Y})$. 
    %
    Assume that we estimate a Gaussian mixture model with $K$ components (each having identity covariance) on the training set and obtain probability estimates $\hat{\mathbb{P}}(y_i | x)$ for $i \in [K]$ for a sample $(x, y_*)$ from the test distribution. 
    %
    Then with high probability, under the following model, 
    \[
        \hat{y}_* \in m_\Lambda([\hat{\mathbb{P}}(y_i | x)]_{i \in [K]}) 
        = \argmin_{y \in \mathcal{Y}} \sum_{i \in [K]} \hat{\mathbb{P}}(y_i | x) d^2(y, y_i)
    \]
    the sample complexity of estimating target $y_*$ from the test distribution $\mathcal{D}_{\text{test}}$ with prediction $\hat{y}_*$ is:
    \[
        \E_{(x, y_*)\sim \mathcal{D}_{\text{test}}} [d^2(y_*, \hat{y}_*)] \leq O\left( \frac{d}{\alpha} \sqrt{\frac{\log K/\delta}{n}} \left( \frac{1}{ \left( R^{1 - \frac{2}{d}} - \frac{\log R}{R}\right) } + \sqrt{d} \right) \right)
    \]
    where $d$ is the dimensionality of the input, $M$ is the number of classes that are near $x$, and $\alpha$ is our parameter under Assumption~\ref{assumption:bilipschitz}. 
    \end{theorem}
    
    \begin{proof}
    We begin by detailing the data-generating process. 
    
    \textbf{At training time,} our underlying data-generating process, $\mathcal{D}_{\text{train}}$, is as follows: 
    \begin{itemize}
        \item We begin with a $\frac{2R}{\sqrt{d}{K} - 1}$-net, $\mathcal{Y}$, on the $d$ dimensional 2-norm ball of radius $R$ under the Euclidean distance, and let $\Lambda \subseteq \mathcal{Y}$
        \item Draw $y \sim \text{Unif}(\mathcal{Y})$.
        \item Discard draws if $y \not\in \Lambda$. 
        \item Draw $x \sim \mathcal{N}(y, I)$. 
    \end{itemize}
    
    \textbf{At test time,} we do not discard draws and allow for classes not in $\Lambda$ and use the following data generating process, $\mathcal{D}_{\text{test}}$:
    \begin{itemize}
        \item We begin with a $\frac{2R}{\sqrt{d}{K} - 1}$-net, $\mathcal{Y}$, on the $d$ dimensional 2-norm ball of radius $R$ under the Euclidean distance, and let $\Lambda \subseteq \mathcal{Y}$
        \item Draw $y \sim \text{Unif}(\mathcal{Y})$.
        %\item Discard draws if $y \not\in \Lambda$. 
        \item Draw $x \sim \mathcal{N}(y, I)$. 
    \end{itemize}
    
    Given a labeled training dataset $D = \{(x_i, y_i)\}_{i=1}^n$ containing $n$ points drawn from $\mathcal{D}_{\text{train}}$ with $|\Lambda| = K$ distinct classes, we would like to fit a $K$-component Gaussian mixture model with identity covariance. 
    We first perform mean estimation of each of the classes separately using the median-of-means estimator \cite{JMLR:v17:14-273, 10.3150/14-BEJ645, lerasle2011robust}. 
    Using thie estimator yields the following parameter estimation bound \cite{10.3150/14-BEJ645, pmlr-v99-cherapanamjeri19b}: 
    \[
    || y_i - \hat{y_i} ||_2 \leq O\left(\sqrt{\frac{d \log K/\delta}{n}}\right)
    \]
    with probability $1-\delta$. 
    
    Next, we consider the relationships between four quantities: $\Psi_{\mathbb{P}}(y_*)$, $\Psi_{\mathbb{P}}(\hat{y}_*)$, $\Psi_{\widehat{\mathbb{P}}}(y_*)$, $\Psi_{\widehat{\mathbb{P}}}(\hat{y}_*)$, where $\mathbb{P}$ is the vector of probabilities from the true Gaussian mixture, $\widehat{\mathbb{P}}$ is the vector of probabilities from the estimated model, $y_* \in m_{\Lambda}(\mathbb{P})$ is the target class, and $\hat{y}_* \in m_{\Lambda}(\widehat{\mathbb{P}})$ is the predicted class. 
    While it is problem-dependent as to whether $\Psi_{\mathbb{P}}(y_*) \leq \Psi_{\widehat{\mathbb{P}}}(\hat{y}_*)$ or $\Psi_{\mathbb{P}}(y_*) > \Psi_{\widehat{\mathbb{P}}}(\hat{y}_*)$, a similar argument holds for both cases. 
    So without loss of generality, we assume that $\Psi_{\mathbb{P}}(y_*) \leq \Psi_{\widehat{\mathbb{P}}}(\hat{y}_*)$. 
    Then by the definition of the Fréchet mean, the following inequalities hold:
    \[\Psi_{\mathbb{P}}(y_*) \leq \Psi_{\widehat{\mathbb{P}}}(\hat{y}_*) \leq \Psi_{\widehat{\mathbb{P}}}(y_*), \]
    and consequently, 
    \begin{equation}
        \label{eq:upperlower}
        \Psi_{\widehat{\mathbb{P}}}(\hat{y}_*) - \Psi_{\mathbb{P}}(y_*) \leq \Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*).
    \end{equation}
    We proceed by obtaining upper and lower bounds of the existing bounds in Equation~\ref{eq:upperlower}. 
    First, we will obtain an upper bound on $\Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*)$.
    \begin{align}
        |\Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*)| 
        &= \left|\sum_{i \in [K]} (\widehat{\mathbb{P}} - \mathbb{P}) d^2(y_*, V_i) \right| \nonumber\\
        &= \left| \sum_{i \in [K]} \left(\widehat{\mathbb{P}}(y_i | x) - \mathbb{P}(y_i | x)\right) || y_* - y_i ||_2^2 \right| \nonumber\\
        &= \left| \sum_{i \in [K]} \left(\frac{\widehat{\mathbb{P}}(x | y_i) \mathbb{P}(y_i)}{\sum_{j \in [K]} \widehat{\mathbb{P}}(x | y_j) \mathbb{P}(y_j) } - \frac{\mathbb{P}(x | y_i) \mathbb{P}(y_i)}{\sum_{j \in [K]} \mathbb{P}(x | y_j) \mathbb{P}(y_j)}\right) || y_* - y_i ||_2^2 \right|  \nonumber\\
        %
    \end{align}
    \begin{align}
        &= \Bigg| \sum_{i \in [K]} \Bigg( \frac{\exp\{-\frac{1}{2}||x - \hat{y}_i||_2^2\} \frac{1}{K}}{\sum_{j \in [K]} \exp\{-\frac{1}{2}||x - \hat{y}_j||_2^2\} \frac{1}{K} } \nonumber\\
        & \quad\quad\quad\quad - \frac{\exp\{-\frac{1}{2}||x - y_i||_2^2\} \frac{1}{K}}{\sum_{j \in [K]} \exp\{-\frac{1}{2}||x - y_j||_2^2\} \frac{1}{K}} \Bigg) || y_* - y_i ||_2^2 \Bigg|.  \nonumber\\
        &= \Bigg|\sum_{i \in [K]} \Bigg( \frac{\exp\{-\frac{1}{2}||x - \hat{y}_i||_2^2\} }{\sum_{j \in [K]} \exp\{-\frac{1}{2}||x - \hat{y}_j||_2^2\}  } \nonumber\\
        & \quad\quad\quad\quad - \frac{\exp\{-\frac{1}{2}||x - y_i||_2^2\} }{\sum_{j \in [K]} \exp\{-\frac{1}{2}||x - y_j||_2^2\} } \Bigg) || y_* - y_i ||_2^2 \Bigg|. \label{bd_1} 
    \end{align}
    For notational convenience, we define the following: \\
    $a_i := \exp\{-\frac{1}{2}||x - y_i||_2^2\}$, \\
    $\hat{a}_i := \exp\{-\frac{1}{2}||x - \hat{y}_i||_2^2\}$, \\
    $b := \sum_{j \in [K]} \exp\{-\frac{1}{2}||x - y_j||_2^2\}$, \\
    $\hat{b} := \sum_{j \in [K]} \exp\{-\frac{1}{2}||x - \hat{y}_j||_2^2\}$, and \\
    $c_i := ||\hat{y}_* - y_i||_2^2$. 
    
    Then (\ref{bd_1}) becomes:
    \begin{align}
        \left|\sum_{i \in [K]} \left( \frac{\hat{a}_i}{\hat{b}} - \frac{a_i}{b} \right) c_i \right| &= \left|\sum_{i \in [K]} \left(  \frac{\hat{a}_i}{\hat{b}} - \frac{\hat{a}_i}{b} + \frac{\hat{a}_i}{b} - \frac{a_i}{b} \right) c_i \right| \nonumber\\
        &= \left|\sum_{i \in [K]} \left( \frac{\hat{a}_i - a_i }{b} + \hat{a}_i \left(\frac{1}{\hat{b}} - \frac{1}{b}\right) \right) c_i \right| \nonumber\\
        &\leq \left|\sum_{i \in [K]} \left( \frac{\hat{a}_i - a_i }{b} \right) c_i \right| + \left|\sum_{i \in [K]} \left( \hat{a}_i \left(\frac{1}{\hat{b}} - \frac{1}{b}\right) \right) c_i \right| \nonumber\\
        &= \left|\sum_{i \in [K]} \frac{a_i}{b} \left( \frac{\hat{a}_i}{a_i} - 1 \right) c_i \right| + \left| \frac{b - \hat{b}}{b} \sum_{i \in [K]} \frac{\hat{a}_i}{\hat{b}} c_i \right| \nonumber\\
        &= \left|\sum_{i \in [K]} \frac{a_i}{b} \left( \frac{\hat{a}_i}{a_i} - 1 \right) c_i \right| + \left| \left( \sum_{i \in [K]} \frac{a_i}{b} \left(\frac{\hat{a}_i}{a_i} - 1\right) \right) \left( \sum_{i \in [K]} \frac{\hat{a}_i}{\hat{b}} c_i \right) \right|. \label{bd_2}
    \end{align}
    
    
    
    
    
    Now we define the following: $L := ||x - y_z ||_2^2$ with $z \in \argmin_{j} ||x - y_j ||_2^2$ is the smallest distance to a class mean, and $L + E_i := ||x - y_i ||_2^2$ with $E_i > 0$. Similarly, define $\hat{L} := ||x - \hat{y}_z ||_2^2$ with $z \in \argmin_{j} ||x - \hat{y}_j ||_2^2$ is the smallest distance to an estimated class mean, and $\hat{L} + \hat{E}_i := ||x - \hat{y}_i ||_2^2$ with $\hat{E}_i > 0$. Finally, we define $T := \sqrt{\hat{L}} + \sqrt{\hat{L} + \hat{E}_i} + \sqrt{L} + \sqrt{L + E_i}$. 
    
    Then we can bound the parts separately:
    % \begin{align}
    %     \E_{D, (x, y_*)} \left( \frac{\exp\{-\frac{1}{2}||x - y_i||_2^2\}}{\sum_{j \in [K]} \exp\{-\frac{1}{2}||x - y_j||_2^2\}} || y_* - y_i ||_2^2 \right)
    %     &\leq \E_{D, (x, y_*)} \left( \frac{\exp\{-\frac{1}{2} (L + E_i) \}}{ \exp\{-\frac{1}{2} L\} + \exp\{-\frac{1}{2} (L + E_i)\} } || y_* - y_i ||_2^2 \right) \nonumber\\
    %     &\leq \E_{D, (x, y_*)} \left(\frac{|| y_* - y_i ||_2^2}{\exp\{ \frac{1}{2} E_i \}} \right)\nonumber\\
    %     &\leq \E_{D, (x, y_*)} \left( \frac{2||x - y_i||_2^2 + 2 ||x - y_*||_2^2}{\exp\{ \frac{1}{2} E_i \}} \right) \nonumber\\
    %     &= \frac{2(L + E_i + d)}{\exp\{ \frac{1}{2} E_i \}} \label{bd_ab} 
    % \end{align}

    For $i\neq z$, we have
    \begin{align}
        \frac{a_i}{b} &= \left( \frac{\exp\{-\frac{1}{2}||x - y_i||_2^2\}}{\sum_{j \in [K]} \exp\{-\frac{1}{2}||x - y_j||_2^2\}} \right) \nonumber\\
        &\leq \left( \frac{\exp\{-\frac{1}{2} (L + E_i) \}}{ \exp\{-\frac{1}{2} L\} + \exp\{-\frac{1}{2} (L + E_i)\} } \right) \nonumber\\
        %&\leq\left(\frac{1}{\exp\{ \frac{1}{2} E_i \}} \right)\nonumber\\
        %&\leq \left( \frac{1}{\exp\{ \frac{1}{2} E_i \}} \right) \nonumber\\
        &= \frac{1}{\exp\{ \frac{1}{2} E_i \}} \label{bd_ab} 
    \end{align}
    and in the case of $i=z$, we bound $\frac{a_z}{b} \leq 1$. 
    So overall, we have $\frac{a_i}{b} \leq \frac{1}{\exp\{ \mathbf{1}_{i\neq z} \frac{1}{2} E_i \}}$ for all $i \in [K]$. 
    
    \begin{align}
        c_i &= || y_* - y_i ||_2^2 \nonumber\\
        &\leq 2||x - y_i||_2^2 + 2 ||x - y_*||_2^2 \nonumber\\
        & = 2(L + E_i + ||x - y_*||_2^2). \label{bd_c}
    \end{align}
    
    \begin{align}
        \frac{\hat{a}_i}{a_i} - 1 &= \exp\left\{ \frac{1}{2} || x - y_i ||_2^2 - \frac{1}{2} || x - \hat{y}_i ||_2^2 \right\} - 1 \nonumber\\
        &\approx 1 + \frac{1}{2} || x - y_i ||_2^2 - \frac{1}{2} || x - \hat{y}_i ||_2^2 -1 \nonumber\\
        &= \frac{1}{2} \langle \hat{y}_i - y_i, 2 x - y_i - \hat{y}_i \rangle \nonumber\\
    \end{align}
    \begin{align}
        &\leq \frac{1}{2} || \hat{y}_i - y_i ||\cdot|| 2 x - y_i - \hat{y}_i || \nonumber\\
        &\leq || \hat{y}_i - y_i || \left(|| x - y_i|| + ||x - \hat{y}_i ||\right) \nonumber\\
        &= || \hat{y}_i - y_i || \left(|| x - y_i|| + ||x - y_i + y_i - \hat{y}_i ||\right) \nonumber\\
        &\leq || \hat{y}_i - y_i || \left(2|| x - y_i|| + ||y_i - \hat{y}_i ||\right) \nonumber\\
        &= || \hat{y}_i - y_i || \left(2\sqrt{L + E_i} + ||y_i - \hat{y}_i ||\right). \label{bd_aa_one}
    \end{align}
    Next, we must control $E_i - \hat{E_i}$ in order to obtain the bound for $\frac{\hat{a}_i}{\hat{b}}$.
    % \begin{align*}
    %     \hat{E_i} - E_i &= (||x - \hat{y}_i||^2 - \hat{L}) - (||x - y_i||^2 - L) \\
    %     &= ||x - \hat{y}_i||^2 - ||x - y_i||^2 + ||x - y_z||^2 - ||x - \hat{y}_z||^2 \\
    %     &= \langle y_i - \hat{y}_i, 2x - \hat{y}_i - y_i \rangle + \langle \hat{y}_z - y_z, 2x - y_z - \hat{y}_z \rangle \\
    %     &\leq ||y_i - \hat{y}_i|| \cdot || 2x - \hat{y}_i - y_i || + ||\hat{y}_z - y_z|| \cdot || 2x - y_z - \hat{y}_z || \\
    %     &\leq ||y_i - \hat{y}_i|| \left( || x - \hat{y}_i|| + || x - y_i || \right) + ||\hat{y}_z - y_z|| \left( || x - y_z|| + || x - \hat{y}_z || \right) \\
    %     &\leq c \sqrt{\frac{d \log K/\delta}{n}} \left( || x - \hat{y}_i|| + || x - y_i || + || x - y_z|| + || x - \hat{y}_z ||\right) \\
    %     &= c \sqrt{\frac{d \log K/\delta}{n}} \left(\sqrt{\hat{L}} + \sqrt{\hat{L} + \hat{E}_i} + \sqrt{L} + \sqrt{L + E_i} \right) \\
    % \end{align*}
    \begin{align*}
         E_i - \hat{E_i} &= (||x - y_i||^2 - L) - (||x - \hat{y}_i||^2 - \hat{L}) \\
        &=  ||x - y_i||^2 - ||x - \hat{y}_i||^2 + ||x - \hat{y}_z||^2 - ||x - y_z||^2 \\
        &= \langle \hat{y}_i - y_i, 2x - y_i - \hat{y}_i \rangle + \langle y_z - \hat{y}_z, 2x - \hat{y}_z - y_z \rangle \\
        &\leq ||\hat{y}_i - y_i|| \cdot || 2x - y_i - \hat{y}_i || + ||y_z - \hat{y}_z|| \cdot || 2x - \hat{y}_z - y_z || \\
        &\leq ||y_i - \hat{y}_i|| \left( || x - \hat{y}_i|| + || x - y_i || \right) + ||\hat{y}_z - y_z|| \left( || x - y_z|| + || x - \hat{y}_z || \right) \\
        &\leq c \sqrt{\frac{d \log K/\delta}{n}} \left( || x - \hat{y}_i|| + || x - y_i || + || x - y_z|| + || x - \hat{y}_z ||\right) \\
        &= c \sqrt{\frac{d \log K/\delta}{n}} \left(\sqrt{\hat{L}} + \sqrt{\hat{L} + \hat{E}_i} + \sqrt{L} + \sqrt{L + E_i} \right) \\
        \hat{E_i} &\geq \max\left\{0, E_i - c T \sqrt{\frac{d \log K/\delta}{n}}\right\}. 
    \end{align*}
    
    
    Using (\ref{bd_ab}), we obtain 
    \begin{align}
        \frac{\hat{a}_i}{\hat{b}} &\leq \frac{1}{\exp\{ \mathbf{1}_{i\neq z}  \frac{1}{2} \hat{E}_i \}} \nonumber\\
        &\leq \frac{1}{ \exp\left\{ \mathbf{1}_{i\neq z}  \max\left\{0, \frac{1}{2} E_i - \frac{c T}{2} \sqrt{\frac{d \log K/\delta}{n}} \right\}\right\}} \label{bd_ab_hat}
    \end{align}
    
    Plugging (\ref{bd_ab}), (\ref{bd_c}), (\ref{bd_aa_one}), and (\ref{bd_ab_hat}) into (\ref{bd_2}), we obtain
    \begin{align*}
        &\left|\sum_{i \in [K]} \frac{a_i}{b} \left( \frac{\hat{a}_i}{a_i} - 1 \right) c_i \right| + \left| \left( \sum_{i \in [K]} \frac{a_i}{b} \left(\frac{\hat{a}_i}{a_i} - 1\right) \right) \left( \sum_{i \in [K]} \frac{\hat{a}_i}{\hat{b}} c_i \right) \right| \\
    \end{align*}
    \begin{align*}
        &\leq \sum_{i \in [K]} \frac{|| \hat{y}_i - y_i || \left(2\sqrt{L + E_i} + ||y_i - \hat{y}_i ||\right) }{\exp\{ \mathbf{1}_{i\neq z} \frac{1}{2} E_i \}} 2(L + E_i + ||x - y_*||_2^2)  \\
        &\quad\quad + \left( \sum_{i \in [K]} \frac{|| \hat{y}_i - y_i || \left(2\sqrt{L + E_i} + ||y_i - \hat{y}_i ||\right) }{\exp\{ \mathbf{1}_{i\neq z} \frac{1}{2} E_i \}} \right) \\
        &\quad\quad \cdot \left( \sum_{i \in [K]} \frac{2(L + E_i + ||x - y_*||_2^2)}{\exp\left\{ \mathbf{1}_{i\neq z}\max\left\{ 0, \frac{1}{2} E_i - \frac{c T}{2} \sqrt{\frac{d \log K/\delta}{n}} \right\} \right\}} \right)  \\
        %
        %&\leq O\left(\sqrt{\frac{d \log K/\delta}{n}} \sum_{i \in [K]} \frac{L + E_i + ||x - y_*||_2^2}{\exp\{ \mathbf{1}_{i\neq z} \frac{1}{2} E_i \}} \left(1 + \sum_{i \in [K]} \frac{L + E_i + ||x - y_*||_2^2}{\exp\left\{ \mathbf{1}_{i\neq z} \max \left\{ 0,  \frac{1}{2} E_i - \frac{c T}{2} \sqrt{\frac{d \log K/\delta}{n}} \right\} \right\}} \right) \right) \\
        %&\leq O\left(\sqrt{\frac{d \log K/\delta}{n}}\sum_{i \in [K]} \frac{L + E_i + ||x - y_*||_2^2}{\exp\left\{ \mathbf{1}_{i\neq z} \max\left\{0, \frac{1}{2} E_i - \frac{c T}{2} \sqrt{\frac{d \log K/\delta}{n}} \right\}\right\}} \right) \\
        %
        &\leq O\left(\sqrt{\frac{d \log K/\delta}{n}}\sum_{i \in [K]} \frac{L + E_i + ||x - y_*||_2^2}{\exp\left\{ \mathbf{1}_{i\neq z} \frac{1}{2} E_i \right\}} \right) \\
    \end{align*}
    
    Next, recalling the fact that our class means form an $\varepsilon$-net on the radius-$R$ ball, we use Lemma 5.2 from \cite{Vershynin2010IntroductionTT} to bound $L$ as 
    %$L \leq \frac{2R}{\sqrt{n}{K} - 1}$. 
    $L \leq \frac{2R}{\sqrt{d}{K} - 1}$. 
    
    \begin{align}
        &\leq O\left(\sqrt{\frac{d \log K/\delta}{n}}\sum_{i \in [K]} \frac{\frac{R}{\sqrt{d}K} + E_i + ||x - y_*||_2^2}{\exp\left\{ \mathbf{1}_{i\neq z} \frac{1}{2} E_i \right\}} \right). \label{interm_psi_bound}
    \end{align}

    Next, we will consider cases on $E_i$. We will consider the cases in which $E_i < \log R^2$ and $E_i \geq \log R^2$. 

    We will begin with the case in which $E_i < \log R^2$, then Equation~\ref{interm_psi_bound} becomes:
    \begin{align*}
        &\leq O\left(\sqrt{\frac{d \log K/\delta}{n}}\left(\sum_{i \in [K]} \frac{R}{\sqrt{d}K} + ||x - y_*||_2^2 \right)\right).
        %&= O\left(\sqrt{\frac{d \log K/\delta}{n}} \left(\frac{R}{\sqrt{d}} + K||x - y_*||_2^2 \right)\right). 
    \end{align*}

    Next, the case in which $E_i \geq \log R^2$, then Equation~\ref{interm_psi_bound} becomes:
    \begin{align*}
        &\leq O\left(\sqrt{\frac{d \log K/\delta}{n}}\sum_{i \in [K]} \frac{\frac{R}{\sqrt{d}K} + 2\log R + ||x - y_*||_2^2}{R} \right) \\
        %
        &\leq O\left(\sqrt{\frac{d \log K/\delta}{n}} \left( \sum_{i \in [K]} \frac{1}{\sqrt{d} K} + ||x - y_*||_2^2 \right) \right). 
        %&\leq O\left(\sqrt{\frac{d \log K/\delta}{n}} \left( \frac{1}{\sqrt{d} R} + K||x - y_*||_2^2 \right) \right) \\
    \end{align*}

    Setting $M \leq K$ to be the number of terms for which $E_i < \log R^2$ holds and by combining the two bounds, we obtain the following:
    \begin{align*}
        &\leq O\left(\sqrt{\frac{d \log K/\delta}{n}} \left(\left( \frac{M R}{\sqrt{d}K} + \frac{K - M}{\sqrt{d} K} \right) + K||x - y_*||_2^2 \right) \right). 
    \end{align*}

    % Solving for $M$ explicitly by again using Lemma 5.2 from \cite{Vershynin2010IntroductionTT}, we obtain 
    % \begin{align*}
    %     \varepsilon &\leq \frac{2R}{\sqrt{d}K - 1} \\
    %     K &\leq \frac{2R}{\sqrt{d} \varepsilon} + 1 \\
    %     \Rightarrow M &\leq \frac{4 \log R}{\sqrt{d} \varepsilon} + 1 \\
    %     &\leq O\left(\frac{\log R}{\sqrt{d} \varepsilon} \right). 
    % \end{align*}

    % Plugging this into our bound, we obtain
    % \begin{align*}
    %     &\leq O\left(\sqrt{\frac{d \log K/\delta}{n}} \left( \frac{R\log R}{\varepsilon d K} + \frac{1}{\sqrt{d} R} - \frac{\log R}{\varepsilon d K R} \right) + K||x - y_*||_2^2 \right)
    % \end{align*}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
    Ultimately, our bound is 
    \begin{align}
        |\Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*)| \leq 
        O\left(\sqrt{\frac{d \log K/\delta}{n}} \left(\left( \frac{M R}{\sqrt{d}K} + \frac{K - M}{\sqrt{d} K} \right) + K||x - y_*||_2^2 \right) \right). \label{psi_bound}
    \end{align}
    
    Next, we will obtain a lower bound on $\Psi_{\widehat{\mathbb{P}}}(\hat{y}_*) - \Psi_{\mathbb{P}}(y_*)$.
    \begin{align*}
        |\Psi_{\widehat{\mathbb{P}}}(\hat{y}_*) - \Psi_{\mathbb{P}}(y_*)| &\geq | \Psi_{\mathbb{P}}(y_*) - \Psi_{\mathbb{P}}(\hat{y}_*) | - | \Psi_{\hat{\mathbb{P}}}(\hat{y}_*) - \Psi_{\mathbb{P}}(\hat{y}_*) | && \text{triangle inequality} \\
        &\geq \alpha d^2(y_*, \hat{y}_*) - |\Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*)| && \text{Assumption~\ref{assumption:bilipschitz}}. 
    \end{align*}
    
    Combining both of these bounds with Equation~\ref{eq:upperlower}, we obtain
    \begin{align*}
        \alpha d^2(y_*, \hat{y}_*) - |\Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*)| \leq \Psi_{\widehat{\mathbb{P}}}(\hat{y}_*) - \Psi_{\mathbb{P}}(y_*) &\leq \Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*) \\
        &\leq |\Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*)|
    \end{align*}
    \begin{align*}
        &\Rightarrow \alpha d^2(y_*, \hat{y}_*) \leq 2 |\Psi_{\widehat{\mathbb{P}}}(y_*) - \Psi_{\mathbb{P}}(y_*)| \\
        % &\leq O\left(\sqrt{\frac{d \log K/\delta}{n}}\sum_{i \in [K]} \frac{\frac{R}{\sqrt{n}{K}} + E_i + ||x - y_*||_2^2}{\exp\left\{ \frac{1}{2} E_i - \frac{c T}{2} \sqrt{\frac{d \log K/\delta}{n}} \right\}} \right) \\
        &\Rightarrow d^2(y_*, \hat{y}_*) \leq O\left( \frac{1}{\alpha} \sqrt{\frac{d \log K/\delta}{n}} \left( \frac{M R}{\sqrt{d}K} + \frac{K - M}{\sqrt{d} K}  + K||x - y_*||_2^2 \right) \right) \\
        %%%
        &\Rightarrow \E_{(x, y_*)\sim \mathcal{D}_{\text{test}}} [d^2(y_*, \hat{y}_*)] \leq O\left( \frac{1}{\alpha} \sqrt{\frac{d \log K/\delta}{n}} \left( \frac{M R}{\sqrt{d}K} + \frac{K - M}{\sqrt{d} K}  + dK \right) \right),
    \end{align*}

    Next, we obtain a bound on $M$. Since $M \leq K$ is the number of terms for which $E_i < \log R^2$, we have that $M = VK$, where $V \in [0, 1]$ is the ratio of the volumes of the ball for which $E_i < \log R^2$, and the ball containing the entire set of classes $\mathcal{Y}$. 
    We have 
    \begin{align*}
        E_i = \|x - y_i\|_2^2 -  \|x - y_z\|_2^2 &< \log R^2 \\
        \|x - y_i\|_2^2 &< \log R^2 + L \\
        &\leq \log R^2 + \frac{2R}{\sqrt{d}K - 1} \leq R^2 \\
        \Rightarrow \sqrt{\log R^2 + \frac{2R}{\sqrt{d}K - 1}} &\leq R,
    \end{align*}
    which is an upper bound of the radius of the ball for which $E_i < \log R^2$ is true. 
    Now we construct $V$:
    \begin{align*}
        V &= \left(\frac{\sqrt{\log R^2 + \frac{2R}{\sqrt{d}K - 1}}}{R}\right)^d. 
    \end{align*}
    Combining this with our error bound, we have 
    \begin{align*}
        &\E_{(x, y_*)\sim \mathcal{D}_{\text{test}}} [d^2(y_*, \hat{y}_*)] \leq \\
        &O\left( \frac{1}{\alpha} \sqrt{\frac{d \log K/\delta}{n}} \left( \frac{\left(\frac{\sqrt{\log R^2 + \frac{2R}{\sqrt{d}K - 1}}}{R}\right)^d R}{\sqrt{d}} + \frac{1 - \left(\frac{\sqrt{\log R^2 + \frac{2R}{\sqrt{d}K - 1}}}{R}\right)^d}{\sqrt{d}}  + dK \right) \right). 
    \end{align*}
    However, we can choose $K$ to weaken our dependence on $R$. 
    %We do so by choosing the $K$ that minimizes $V$, recalling that $V \in [0, 1]$:
    \begin{align*}
        %\left(\frac{\sqrt{\log R^2 + \frac{2R}{\sqrt{d}K - 1}}}{R}\right)^d = 0 \\
        %\Rightarrow K = \frac{2R}{\sqrt{2 d\log R}} + 1. 
        V = \left(\frac{\sqrt{\log R^2 + \frac{2R}{\sqrt{d}K - 1}}}{R}\right)^d \leq \frac{1}{R} \\
        %
        \Rightarrow K \geq \frac{2}{\sqrt{d} \left( R^{1 - \frac{2}{d}} - \frac{\log R}{R}\right) } + 1. 
    \end{align*}
    Finally, we have 
    \begin{align*}
        \E_{(x, y_*)\sim \mathcal{D}_{\text{test}}} [d^2(y_*, \hat{y}_*)] 
        %\leq O\left( \frac{1}{\alpha} \sqrt{\frac{d \log K/\delta}{n}} \left( \frac{1}{\sqrt{d} R}  + \frac{\sqrt{d} R}{\sqrt{\log R}} + d \right) \right). 
        &\leq O\left( \frac{1}{\alpha} \sqrt{\frac{d \log K/\delta}{n}} \left( \frac{1}{\sqrt{d}} + \frac{1 - \frac{1}{R}}{\sqrt{d}} + \frac{\sqrt{d}}{ \left( R^{1 - \frac{2}{d}} - \frac{\log R}{R}\right) } + d \right) \right) \\
        &\leq O\left( \frac{1}{\alpha} \sqrt{\frac{d \log K/\delta}{n}} \left( \frac{1}{\sqrt{d}} + \frac{\sqrt{d}}{ \left( R^{1 - \frac{2}{d}} - \frac{\log R}{R}\right) } + d \right) \right) \\
        &\leq O\left( \frac{d}{\alpha} \sqrt{\frac{\log K/\delta}{n}} \left( \frac{1}{ \left( R^{1 - \frac{2}{d}} - \frac{\log R}{R}\right) } + \sqrt{d} \right) \right) 
    \end{align*}
    
    which completes the proof. 
    \end{proof}

    \paragraph{Discussion of Theorem~\ref{thm:learning_formal}.} 
    The above bound contains several standard quantities, including the dimensionality of the inputs $d$, the radius $R$ of the ball from which labels are drawn, the number of classes and samples used to estimate the model $K$ and $n$ respectively, and $\delta$, which is used to control the probability that the bound holds. 
    Naturally, the bound scales up with higher $d$ and $K$, and improves with an increased number of samples $n$. 
    The dependence on $R$ is also related to its dependence on $d$ --- so long as we have that 
    $K \geq 2 (\sqrt{d} ( R^{1 - \frac{2}{d}} - \log R / R ) )^{-1} + 1$ 
    and for $d \geq 2$, $R$ increases, which improves the bound. 
    The bound includes a metric space dependent quantity $\alpha$, which is related to the amount that the Fréchet variance can change subject to changes in its argument, i.e., which node is considered. 
    
    
    
    
    
    
    % \subsection{Proof of Theorem~\ref{theorem:learning} ($\loki$ sample complexity)}
    % In this section, we provide a formal statement and proof of our sample complexity result for $\loki$, including additional required definitions and assumptions. 
    % Our proof relies on the standard finite-sample complexity bound for maximum likelihood estimates of multiclass logistic regression parameters---we provide a statement and proof of this result first. 
    % \begin{lemma}[Multiclass logistic regression finite-sample complexity bound]
    % \label{lemma:logreg}
    %     Under the following multiclass logistic regression model
    %     \[\mathbb{P}(\hat{y} = k | x) = \frac{\exp\{ -x^\top \beta_k \}}{\sum_{c \in [K]} \exp\{-x^\top \beta_c \} } \]
    %     where we have access to maximum likelihood estimates $\hat{\beta}_k$ obtained over $n$ samples, $(x_i, y_i) \sim \mathcal{D}_{\mathcal{B}, \set{Y}}$ where $\mathcal{B} \subseteq \mathbb{R}^d$ is the unit 2-norm ball and $y_i \in [K]$ for $i \in [n]$, 
    %     we have the following parameter estimation bound:
    %     \[ ||\hat{\beta}_k - \beta_k|| \leq O\left(K \sqrt{\frac{d}{n}}\right), \]
    %     with high probability. 
    % \end{lemma}
    % \begin{proof}
    %     MLE theory tells us that for sufficiently large $n$, we have 
    %     \[ \frac{\hat{\beta}_{k,j} - \beta_{k,j}}{\text{SE}(\hat{\beta}_{k,j})} \sim \mathcal{N}(0, 1), \]
    %     where $\hat{\beta}_{k,j}$ is our maximum likelihood estimate over $n$ samples. 
    %     Then we can rewrite the standard error in terms of the observed Fisher information as 
    %     \begin{align*}
    %         & \frac{\hat{\beta}_{k,j} - \beta_{k,j}}{\text{SE}(\hat{\beta}_{k,j})} = 
    % \sqrt{\mathcal{I}(\hat{\beta}_{k,j})} (\hat{\beta}_{k,j} - \beta_{k,j}) \sim \mathcal{N}(0, 1) \\
    %         &\Rightarrow \sum_{j \in [d]} \mathcal{I}(\hat{\beta}_{k,j}) (\hat{\beta}_{k,j} - \beta_{k,j})^2 \sim \chi^2_d, 
    %     \end{align*}
    %     where $\chi^2_d$ is the chi-squared distribution with $d$ degrees of freedom. 
    
    %     Then using Markov's inequality, we obtain that
    %     \begin{align*}
    %         \mathbb{P}\left( \sum_{j \in [d]} \mathcal{I}(\hat{\beta}_{k,j}) (\hat{\beta}_{k,j} - \beta_{k,j})^2 > a d \right) &\leq \frac{1}{a} \\
    %         \mathbb{P}\left( \sum_{j \in [d]} (\hat{\beta}_{k,j} - \beta_{k,j})^2 > a \frac{d}{n} \right) &\leq \frac{1}{a} \\
    %         \mathbb{P}\left( ||\hat{\beta}_{k} - \beta_{k}||_2 > \sqrt{a \frac{d}{n}} \right) &\leq \frac{1}{a}.
    %     \end{align*}
    %     However, we require the bound to hold for all $k \in [K]$, so we apply a union bound
    %     \begin{align*}
    %         \mathbb{P}\left( \bigcup_{k \in [K]} ||\hat{\beta}_{k} - \beta_{k}||_2 > \sqrt{a \frac{d}{n}} \right) &\leq \frac{K}{a}
    %     \end{align*}
    %     and set $a = \tilde{a} K^2$
    %     \begin{align*}
    %         \mathbb{P}\left( \bigcup_{k \in [K]} ||\hat{\beta}_{k} - \beta_{k}||_2 > K\sqrt{\tilde{a} \frac{d}{n}} \right) &\leq \frac{1}{\tilde{a}K} \\
    %         \Rightarrow \mathbb{P}\left( \bigcap_{k \in [K]} ||\hat{\beta}_{k} - \beta_{k}||_2 \leq K\sqrt{\tilde{a} \frac{d}{n}}, \right) &> 1 - \frac{1}{\tilde{a}K}.
    %     \end{align*}
    %     Hence, with high probability, it holds that $||\hat{\beta}_{k} - \beta_{k}|| \leq O\left(K\sqrt{\frac{d}{n}}\right)$. 
    % \end{proof}
    
    % \begin{corollary}[]
    % Due to Lemma~\ref{lemma:logreg}, we also have the following using Lipschitzness and the fact that $x_i \in \mathcal{B}$ for all $i \in [n]$. 
    % \[ \left|\widehat{\mathbb{P}}_k - \mathbb{P}_k\right| \leq O\left(K\sqrt{\frac{d}{n}}\right). \]
    % \end{corollary}











\subsection{Proof of Theorem~\ref{thm:min_locus_trees} (minimum locus cover for trees)}

\begin{theorem}[Minimum locus cover for trees]
\label{thm:min_locus_trees}
The minimum locus cover for a tree graph $T$ is $\Lambda = \text{Leaves}(T)$. 
\end{theorem}

\begin{proof}
We would like to show to show two things: 
\begin{enumerate}
    \item any node can be resolved by an appropriate construction of $\bm{w}$ using only the leaves and
    \item removing any leaf makes that leaf node unreachable, no matter what the other nodes are in $\bm{y}$--i.e., we must use at least all of the leaf nodes. 
\end{enumerate}
If both of these conditions hold (i.e., that the leaves form a locus cover and that they are the smallest such set), then the the leaves of any tree form a minimum locus cover. 
To set up the problem, we begin with some undirected tree, $T$, whose nodes are our set of labels: $T = (\mathcal{Y}, E)$ where $\Lambda = \text{Leaves}(T) \subseteq \mathcal{Y}$ is the set of leaf nodes. Moreover, let $\bm{\Lambda}$ be a tuple of the leaf nodes. We will start by proving that $\Lambda$ is a locus cover. We will show this by cases on $v \in \mathcal{Y}$, the node that we would like to resolve:
\begin{enumerate}
    \item If $v \in \Lambda$, i.e. $v$ is a leaf node, then setting $\bm{w}_i = \mathbf{1}\{\bm{\Lambda}_i = v\}$ yields 
    \begin{align*}
        m_{\bm{\Lambda}}(\bm{w}) &= \argmin_{y \in \mathcal{Y}} \sum_{i=1}^K \mathbf{1}\{\bm{\Lambda}_i = v\} d^2(y, \bm{\Lambda}_i) \\
        &= \argmin_{y \in \mathcal{Y}} d^2(y, v) = \{v\}.
    \end{align*} 
    \item If $v \not\in \Lambda$, we have a bit more work to do. Since $v$ is an internal node, consider any pair of leaves, $l_1 \neq l_2$ such that $v$ is along the (unique) path between $l_1$ and $l_2$: $v \in \Gamma(l_1, l_2)$. Then set 
    $\bm{w}_i =
        \begin{cases}
        \frac{d(v, l_2)}{d(l_1, l_2)} \text{ if } \bm{\Lambda_i} = l_1, \\
        \frac{d(v, l_1)}{d(l_1, l_2)} \text{ if } \bm{\Lambda_i} = l_2, \\
        0 \text{ otherwise }
        \end{cases} $
    yields
    \begin{align*}
        m_{\bm{\Lambda}}(\bm{w}) &= \argmin_{y \in \mathcal{Y}} \frac{d(v, l_2) d^2(y, l_1) + d(v, l_1) d^2(y, l_2)}{d(l_1, l_2)} \\
        &= \argmin_{y \in \mathcal{Y}} d(v, l_2) d^2(y, l_1) + d(v, l_1) d^2(y, l_2) \\
        &= \{v\}. 
    \end{align*}
\end{enumerate}
Thus $\Lambda$ is a locus cover. 
Next, we will show that it is the smallest such set. 
Let $l \in \Lambda$ be any leaf node, and define $\Lambda' = \Lambda \setminus \{l\}$. 
We must show that $\Lambda'$ is not a locus cover. 
Assume for contradiction that $\Lambda'$ is a locus cover. 
This means that given some tuple $\bm{y}'$ whose entries are the elements of $\Lambda'$, we have $\Pi(\bm{\Lambda}') = \mathcal{Y}$. 
This implies that the path between any two nodes in $\Lambda$ is also contained in $\Pi(\bm{\Lambda}')$. 
Since the leaves form a locus cover and any $y \in \mathcal{Y}$ can be constructed by the appropriate choice of $\bm{w}$ along with a path between two leaf nodes, $l$ must either be one of the entries of $\bm{\Lambda}'$ (one of the endpoints of the path) or it must be an internal node. Both cases are contradictions--$\bm{\Lambda}'$ cannot include $l$ by assumption, and $l$ is assumed to be a leaf node. 
It follows that $\Lambda$ is a minimum locus cover. 
\end{proof}


\subsection{Proof of Lemma~\ref{lemma:tree_pairwise} (tree pairwise decomposability)}

\begin{lemma}[Tree pairwise decomposability]
\label{lemma:tree_pairwise}
Let $T = (\set{Y}, \set{E})$ be a tree and $\Lambda \subseteq \set{Y}$. Then $\Pi(\Lambda)$ is pairwise decomposable. 
\end{lemma}

\begin{proof}
Assume for contradiction that $\exists y^* \in \mathcal{Y}$ where $y^* \notin \Pi(\{\lambda_i, \lambda_j\})$, $\forall \lambda_i, \lambda_j \in \Lambda$, but $y^* \in \Pi(\Lambda)$. Then, $y^* \in m_\Lambda (\mathbf{w})$ for some $\mathbf{w}$. 
Note that $y^* \notin \Pi(\{\lambda_i, \lambda_j\}) $ implies $y^* \notin \Gamma(\lambda_i, \lambda_j)$,  $\forall \lambda_i, \lambda_j \in \Lambda$, because $\Pi(\{\lambda_i, \lambda_j\}) = \Gamma(\lambda_i,\lambda_j)$ due to the uniqueness of paths in trees. 
As such, $\cap_{\lambda \in \Lambda} \Gamma( y^*, \lambda)$ must contain an immediate relative $y'$ of $y^*$ where
% above notation emphasizes that you can't have y* at the root of T' with some lambdas branching off leaves of one child of y* and other lambdas branching off leaves of another child of y*
% note: originally we had, let y' be the parent of y* but that isn't general enough; there's examples where it doesn't work eg by having y* be the root and a single stem comes off then splits
\begin{align*}
\sum^{|\Lambda|}_{i=1} \mathbf{w}_i d^2(y^*, \lambda_i) &= \sum^{|\Lambda|}_{i=1} \mathbf{w}_i (d(y', \lambda_i)+1)^2 \\
&> \sum^{|\Lambda|}_{i=1} \mathbf{w}_i d^2(y', \lambda_i).
\end{align*}
Therefore, $y^* \notin \Pi(\Lambda)$ because $y^*$ is not a minimizer of $\sum^{|\Lambda|}_{i=1} \mathbf{w}_i d^2(y, \lambda_i)$. So, it must be that $y^*\in \Pi(\{\lambda_i, \lambda_j\})$ for some $\lambda_i, \lambda_j \in \Lambda$ when $y \in \Pi(\Lambda)$.
\end{proof}




\subsection{Proof of Theorem~\ref{thm:algphylo} (Algorithm~\ref{alg:phylo} correctness)}

\begin{theorem}[Algorithm~\ref{alg:phylo} correctness]
\label{thm:algphylo}
Algorithm~\ref{alg:phylo} returns a locus cover for phylogenetic trees. 
\end{theorem} 

\begin{proof}
We first prove that Algorithm \ref{alg:phylo} will halt, then according to the stopping criterion, it is guaranteed that Algorithm \ref{alg:phylo} returns a locus cover. 
Suppose the algorithm keeps running until it reaches the case in which all of the leaf nodes are included in $\Lambda$ (i.e. $\Lambda = \set{Y}$), this results in the trivial locus cover of the phylogenetic tree and the algorithm halts.
\end{proof}







\subsection{Proof of Lemma~\ref{lemma:phylotree_pairwise} (phylogenetic tree pairwise decomposability)}

\begin{lemma}[Phylogenetic tree pairwise decomposability]
\label{lemma:phylotree_pairwise}
Let $T = (\set{V}, \set{E})$ be a tree with $\set{Y} = \text{Leaves}(T)$ and $\Lambda \subseteq \set{Y}$. Then $\Pi(\Lambda)$ is pairwise decomposable. 
\end{lemma}

\begin{proof}
Suppose to reach a contradiction that $\exists y^* \in \mathcal{Y}$ where $y^* \in \Pi(\Lambda)$, but $y^* \notin \Pi(\{\lambda_i, \lambda_j\})$ $\forall \lambda_i, \lambda_j \in \Lambda$. In other words, $y^*$ is a Fréchet mean for some weighting on $\Lambda$, but $y^*$ is not a Fréchet mean for any weighting for any two vertices within $\Lambda$.

For arbitrary $\lambda_i, \lambda_j \in \Lambda$, define the closest vertex to $y^*$ on the path between $\lambda_i$ and $\lambda_j$ as
$$v'(\lambda_i, \lambda_j)=\argmin_{v \in \Gamma(\lambda_i, \lambda_j)}d(y^*, v).$$
Also, let $$\{\lambda_1, \lambda_2\} = \argmin_{\lambda_i, \lambda_j \in \Lambda} d(y^*, v'(\lambda_i, \lambda_j)),$$ the pair of vertices in $\Lambda$ whose path $\Gamma(\lambda_1, \lambda_2)$ passes \textit{closest} to $y^*$. To avoid notational clutter, define $v' = v'(\lambda_1, \lambda_2)$. Thus, $v'$ represents the closest vertex to $y^*$ lying on the path between any two $\lambda_i, \lambda_j \in \Lambda$. 

Because $v'$ lies on the path $\Gamma(\lambda_1, \lambda_2)$, there exists some weighting $\mathbf{w}$ for which $$v' = \argmin_{v \in \Gamma(\lambda_1, \lambda_2)} (\mathbf{w}_1 d^2(v, \lambda_1) + \mathbf{w}_2 d^2(v, \lambda_2)).$$
For this $\mathbf{w}$, we have that $m_{\{\lambda_1, \lambda_2\}}(\mathbf{w})=\argmin_{y \in \mathcal{Y}} d(y, v') $, the set of leaf nodes of smallest distance from $v'$. By the initial assumption that $y^*\not \in \Pi(\{\lambda_1, \lambda_2\})$, we have that $y^* \not \in m_{\{\lambda_1, \lambda_2\}}(\mathbf{w})$. Let $y'$ be a vertex of $m_{\{\lambda_i, \lambda_j\}}(\mathbf{w})$ that is closest to $y^*$. Clearly,  $d(y', v')^2 < d(y^*, v')^2$.

For any $v \in \mathcal{V}$, respectively define the sets of vertices (1) shared in common on the two paths $\Gamma(v,y^*)$ and $\Gamma(v,y')$, (2) on the path $\Gamma(v,y^*)$ that are not on $\Gamma(v,y')$, and (3) on the path $\Gamma(v,y')$ that are not on $\Gamma(v,y^*)$ as
\begin{align*}
    d_C(v) &= \Gamma(v,y^*) \cap \Gamma(v,y'),\\
    d_{y^*}(v)&=\Gamma(v,y^*) \setminus \Gamma(v,y'), \text{ and}\\
    d_{y'}(v)&=\Gamma(v,y') \setminus \Gamma(v,y^*).
\end{align*}
We have that $|d_{y^*}(v)| + |d_{y^*}(v)| +1 = |\Gamma(y^*, y')|$ (the difference by $1$ represents the final vertex shared in common by both paths) and also that $|d_{y'}(v')|<|d_{y^*}(v')|$ since $y'$ is closer to $v'$ than $y^*$ is.

Suppose $\exists \lambda_3, \lambda_4 \in \Lambda$ yielding % $u' = v'(\lambda_3, \lambda_4) \in \Gamma(y^*, y')$ such that $d^2(y^*, u') \leq d^2(y', u')$
$$u' = v'(\lambda_3, \lambda_4) \in \Gamma(y^*, y') \text{ such that } d^2(y^*, u') \leq d^2(y', u').$$
This is to say that $u'$ is defined analogously to $v'$, with the additional restrictions that $u'$ lie on the path $\Gamma(y^*, y')$ and $u'$ be at least as close to $y^*$ as to $y'$.
Then, $|d_{y^*}(u')|<|d_{y^*}(v')|$ implying $d^2(y^*, u')<d^2(y^*,v')$, which contradicts the definition of $v'$. % for any path-internal node between two lamdas that lies on the path between y* and y', it must be that y' is the better fit than y*

Suppose now that $\exists \lambda_3, \lambda_4 \in \Lambda$ yielding % $u' = v'(\lambda_3, \lambda_4) \notin \Gamma(y^*, y')$ such that $d^2(y^*, u') \leq d^2(y', u')$
$$u' = v'(\lambda_3, \lambda_4) \notin \Gamma(y^*, y') \text{ such that } d^2(y^*, u') \leq d^2(y', u').$$
This is to say that $u'$ is again defined analogously to $v'$, but $u'$ is not on $\Gamma(y^*,y')$ and is at least as close to $y^*$ as to $y'$.
Then, either $\cap_{\lambda \in \Lambda} d_C(\lambda) \neq \emptyset$ (i.e. all lambdas lie in the same branch off of $\Gamma(y', y^*)$), in which case the path between any two lambdas will have the same closest node $v'$ to $y^*$, or it is possible to choose a $\lambda_5 \in \Lambda$ such that $d_C(\lambda_3)\cap d_C(\lambda_5) = \emptyset$. %lambda5 lies in another branch
In this situation, it must be that $\Gamma(\lambda_3, \lambda_5) \cap \Gamma(y', y^*) \neq \emptyset$, which implies there exists a vertex $t \in \Gamma(\lambda_3, \lambda_5)$ such that $d^2(y^*,t)<d^2(y^*, u')$ which violates the definition of $u'$. 
%The path between lambda 3 and 5 (or 4 and 5) will have to have internal nodes on the path between y* and y', which puts us back to the prior paragraph's contradiction

Altogether, for all $v \in \Gamma(\lambda_i, \lambda_j)$ with arbitrary $\lambda_i, \lambda_j \in \Lambda$, we have that $d^2(y',v)<d^2(y^*,v)$. This implies that for all $\lambda \in \Lambda$, there exists a $y' \in \set{Y}$ such that $d^2(y',\lambda)<d^2(y^*,\lambda)$ and therefore that $y^* \notin \Pi(\Lambda)$ which contradicts that $y^* \in \Pi(\Lambda)$. It must be that  $\forall y^* \in \mathcal{Y}$ where $y^* \in \Pi(\Lambda)$, we have $y^* \in \Pi(\{\lambda_i, \lambda_j\})$ $\forall \lambda_i, \lambda_j \in \Lambda$.
\end{proof}

% \subsection{Proof of Algorithm \ref{alg:phylo} returns minimum locus cover}
% We currently hold a counter-example that Algorithm \ref{alg:phylo} does not return minimum locus cover for all cases. But we present a potential useful justification to a algorithm may return minimum locus cover as an enlightenment to improve the current algorithm as well as justifying future design:

% \begin{theorem}[Algorithm~\ref{alg:phylo} returns a minimum locus cover for phylogenetic trees]
% \end{theorem}

% \begin{proof}
% We first prove that Algorithm \ref{alg:phylo} will halt at a certain time point and will not run forever, then according to the stopping criterion, it is guaranteed that Algorithm \ref{alg:phylo} returns a locus cover. Suppose the algorithm keeps running until it reaches the extreme case when all the leaf nodes are included in $\Lambda$ (i.e. $\Lambda=\set{Y}$), this generates to the naive locus cover of the phylogenetic tree and the algorithm will stop.

% Next, we prove that the set returned by Algorithm \ref{alg:phylo} is a minimum locus cover. 

% We argue that selecting nodes with longest spanning path in the graph is global optimal strategy as otherwise, the left-out node $v$ will never be in any locus cover $\Pi(\Lambda)$ with any $\Lambda$ such that $v \notin \Lambda$ and has to be included into $\Lambda$ eventually. 

% Suppose at round $k$, we have set $\Lambda_k$ and its corresponding locus cover $\Pi(\Lambda_k)$. Assume the longest path with at least one end-point not covered by the locus cover is $\Gamma(y_i, y_j)$ and that $y_i \notin \Pi(\Lambda_k)$ (Otherwise, the algorithm stops? here we are specifically assuming that if endpoints of longest path is covered by $\Pi(\Lambda)$ then so will the endpoints of all short paths. more strictly, nodes of the whole tree). Meanwhile, denote arbitrary shorter path is $\Gamma(z_i, z_j)$. If no such shorter path exists, then following the algorithm, we directly append $\{y_i, y_j\}$ to $\Lambda$ and the proof is done. Otherwise, if we include $\{z_i, z_j\}$ instead of $\{y_i, y_j\}$, then we show that $y_i \notin \Pi(\Lambda_k \cup \{z_i,z_j\})$.

% Since we assumed that $y_i \notin \Pi(\Lambda_k)$, if $y_i \in \Pi(\Lambda_k \cup \{z_i,z_j\})$, then the weight corresponds to $z_i$ can't be 0. And suppose $y_i \in \Pi(\Lambda_k \cup \{z_i,z_j\})$, this means there exists $\bm{w} \in \Delta^{K-1}$ s.t.
% \begin{align*}
%     y_i &= \argmin_{y \in \set{Y}} \sum_{\bm{\lambda}_i\in \Lambda_k \cup \{z_i,z_j\}} \bm{w}_i d^2(y, \bm{\lambda}_i)\\
%     &= \argmin_{y \in \set{Y}} \sum_{\bm{\lambda}_i\in \Lambda_k} \bm{w}_i d^2(y, \bm{\lambda}_i)+\gamma_i d^2(y, z_i).
% \end{align*}

% If we can show for arbitrary $\bm{w}$ we have:
% $$
% \sum_{\bm{\lambda}_i\in \Lambda_k} \bm{w}_i d^2(y_i, \bm{\lambda}_i) \geq \sum_{\bm{\lambda}_i\in \Lambda_k} \bm{w}_i d^2(z_i, \bm{\lambda}_i),
% $$
% then since $\gamma_i$ is strictly greater than 0, and $d^2(z_i,z_i)< d^2(y_i, z_i)$, $y_i$ can't be the minimizer of the objective function.
% \end{proof}



\subsection{Proof of Theorem~\ref{thm:min_loc_grid} (minimum locus cover for grid graphs)}

\begin{theorem}[Minimum locus cover for the grid graphs]
\label{thm:min_loc_grid}
The minimum locus cover for a grid graph is the pair of vertices in the furthest opposite corners.
\end{theorem}

\begin{proof}
We would like to show two things: 
\begin{enumerate}
    \item any vertex can be resolved by construction of $\bm{w}$ using the furthest pair of two vertices (i.e. opposite corners) and
    \item there does not exist a locus cover of size one for grid graphs with more than one vertex. 
\end{enumerate}
Let $G = (\set{V}, \set{E})$ be a grid graph and let $\Lambda = \{\lambda_1, \lambda_2\}$ be a set of two vertices who are on opposite corners of $G$ (i.e., perhipheral vertices achieving the diameter of $G$). 
For notational convenience, set ${\bm \Lambda} = \left( \lambda_1, \lambda_2 \right)$
We can reach any interior vertex by an appropriate setting the weight vector ${\bm w} = [{\bm w}_1, {\bm w}_2]$. 
Then set 
    $\bm{w} = \left(\frac{d(v, \lambda_2)}{d(\lambda_1, \lambda_2)}, \frac{d(v, \lambda_1)}{d(\lambda_1, \lambda_2)}\right)$.
Finally, the Fréchet mean is given by
\begin{align*}
    m_{\bm{\Lambda}}(\bm{w}) &= \argmin_{y \in \mathcal{Y}} \frac{d(v, \lambda_2) d^2(y, \lambda_1) + d(v, \lambda_1) d^2(y, \lambda_2)}{d(\lambda_1, \lambda_2)} \\
    &= \argmin_{y \in \mathcal{Y}} d(v, \lambda_2) d^2(y, \lambda_1) + d(v, \lambda_1) d^2(y, \lambda_2) \\
    &\ni v. 
\end{align*}
Hence $\Lambda$ is a locus cover. 
We can clearly see that $\Lambda$ is a minimum locus cover---the only way to obtain a smaller set $\Lambda'$ is for it to include only a single vertex. 
However, if $\Lambda' = \{v'\}$ contains only a single vertex, it cannot be a locus cover so long as $G$ contains more than one vertex---$v'$ is always guaranteed to be a unique minimizer of the Fréchet mean under $\Lambda'$ which misses all other vertices in $G$. 
Thus $\Lambda$ is a minimum locus cover. 
\end{proof}



\subsection{Proof of Lemma~\ref{lemma:locus_grid_subspace} (loci of grid subspaces)}
\begin{lemma}[Locus of grid subspaces]
\label{lemma:locus_grid_subspace}
Given any pair of vertices in $\Lambda$, we can find a subset $G'$ of the original grid graph $G = (\set{Y}, \set{E})$ which takes the given pair as two corner. $\Pi(\boldsymbol{\Lambda})$ equals to all the points inside $G'$.
\end{lemma}

\begin{proof}
The result follows from application of Theorem~\ref{thm:min_loc_grid} to the choice of metric subspace. 
%From Theorem~\ref{thm:min_loc_grid}, we have that the minimum locus cover is a set of opposite corners in the grid graph. 
%Extending this result, we know that $\forall \lambda_i, \lambda_j \in G$, we can find a subset grid graph $G'$  with $\lambda_i, \lambda_j$ as two corners. It is trivial that $\lambda_i, \lambda_j$ is the pair of vertices with farthest distance in $G'$. Therefore, $\Pi(\boldsymbol{\Lambda})$ equals to all the points inside $G'$.
\end{proof}



\subsection{Proof of Lemma~\ref{lemma:grid_pairwise} (grid pairwise decomposability)}
\begin{lemma}[Grid pairwise decomposability]
\label{lemma:grid_pairwise}
Let $G = (\set{Y}, \set{E})$ be a grid graph and $\Lambda \subseteq \set{Y}$. Then $\Pi(\Lambda)$ is pairwise decomposable. 
\end{lemma}

\begin{proof}
% We apply the result of Lemma~\ref{lemma:locus_grid_subspace} to all pairs of vertices in $\Lambda$. 
% We have shown that the size of $\Pi(\Lambda)$ depends on the distance between two furthest pairs of vertices. 
% No matter how many points in $\Lambda$, the furthest distance is only calculated by any pair of points in $\Lambda$. 
% Therefore, $\Pi(\Lambda)$ is pairwise decomposable.
Suppose we have a grid graph $G = (\set{Y}, \set{E})$ and a vertex $\hat{y} \in \Pi(\Lambda)$ with $\Lambda \subseteq \set{Y}$. 
Due to the fact that $\hat{y} \in \Pi(\Lambda)$ and the fact that $G$ is a grid graph, we have that $\hat{y} \in \Gamma(\lambda_\alpha, \lambda_\beta)$ for some $\lambda_\alpha, \lambda_\beta \in \Lambda$. 
Then by Lemma~\ref{lemma:grid_pairwise}, 
$$\hat{y} \in \Pi(\{\lambda_\alpha, \lambda_\beta\}) \subseteq \cup_{\lambda_i, \lambda_j \in \Lambda} \Pi(\{\lambda_i, \lambda_j\}).$$
Therefore loci on grid graphs are pairwise decomposable. 
\end{proof}


% \subsection{Identifying locus cover for grid graphs}
% \begin{proof}
% It is clear that $\forall y \in \mathcal{Y}$, we can get the set of distances from $y$ to $\forall \lambda \in \Lambda$. If the set of distances is unique among all $y$, the locus cover is identified because $m_{\boldsymbol{\Lambda}}(\mathbf{w}) = \{ y \}$. If we are given all the corners, the distance set should be unique. In any 2D space, $\lambda_1, \lambda_2, \lambda_3, \lambda_4 \in \Lambda$ are the corners in that space and they form a 2D grid graph $G'$. $y'$ is the projection of $y$. We know that $y' \in G'$ 
% \begin{align*}
% y' &\in m_{\Lambda'} (w') \\
% &= \argmin_{y \in \mathcal{Y}} \sum^{4}_{i=1} w'_i d^2(y, \lambda_i).
% \end{align*}
% It is trivial that $m_{\boldsymbol{\Lambda'}}(\mathbf{w'}) = \{ y' \}$ for any 2D subspace. Therefore, $m_{\boldsymbol{\Lambda}}(\mathbf{w}) = \{ y \}$.
% \end{proof}



\subsection{Proof of Theorem~\ref{thm:trivial_complete} (no nontrivial locus covers for complete graphs)}
\begin{theorem}[Trivial locus cover for the complete graph]
\label{thm:trivial_complete}
There is no non-trivial locus cover for the complete graph. 
\end{theorem}
\begin{proof}
We show that there is no nontrivial locus cover for complete graphs by showing that removing any vertex from the trivial locus cover renders that vertex unreachable. 
We proceed by strong induction on the number of vertices, $n$.
\\
\textbf{Base case} We first set $n = 3$. Let $\bm{K_3} = (\set{V}, \set{V}^2)$ be the complete graph with three vertices: $\set{V} = \{v_1, v_2, v_3\}$, and without loss of generality, let $\Lambda = \{ v_2, v_3 \}$ be our set of observed classes. 
There are two cases on the weight vector $\bm{w} = [w_2, w_3]$:\\
Case 1:  Suppose $\bm{w} \not\in \text{int}\Delta^{2}$. 
This means that either $w_2 = 1$ or $w_3 = 1$---which leads to the Fréchet mean being either $v_2$ or $v_3$, respectively. 
Neither of these instances correspond to $v_1$ being a minimizer. 
\\
Case 2: Suppose $\bm{w} \in \text{int}\Delta^{2}$. 
Then the Fréchet mean is given by 
$$m_{\bm{\lambda}}(\bm{w}) = \argmin_{y \in \mathcal{Y}} w_2 d^2(y, v_2) + w_3 d^2(y, v_3)$$
Assume for contradiction that $v_1 \in \Pi({\bm\Lambda})$:
\begin{align*}
    w_2 d^2(v_1, v_2) + w_3 d^2(v_1, v_3) &= w_2 + w_3 \\
    &> w_3 && \text{because } \bm{w} \in \text{int}\Delta^{2} \\
    &= w_2 d^2(v_2, v_2) + w_3 d^2(v_2, v_3). 
\end{align*}
Therefore, $v_1 \notin \Pi({\bm\Lambda})$.
This is a contradiction.
Thus there is no nontrivial locus cover for $\bm{K_3}$. 

\textbf{Inductive step}: Let $\bm{K_{n-1}} = (\set{V}, \set{V}^2)$ be the complete graph with $n-1$ vertices. Assume that there is no nontrivial locus cover for $\bm{K_{n-1}}$.
We will show that there is no nontrivial locus cover for $\bm{K_{n}}$.
Let the weight vector be $\bm{w} = [w_1, ... w_{n-1}]$ corresponding to vertices $v_1, ..., v_{n-1} \in \set{V}$ with $\Lambda = \{v_1, ..., v_{n-1}\}$. 
We want to show that $v_n \not\in \Pi(\bm{\Lambda})$. 
We proceed by cases on $\bm{w}$. 
\\
Case 1: Suppose $\bm{w} \not\in \text{int}\Delta^{n-2}$ where $m$ entries of $\bm{w}$ are zero, then by strong induction we know that $v_n \not\in \Pi(\bm{\Lambda}) = \{v_i\}_{i=1}^{n-m}$, i.e., there is no nontrivial locus cover for $\bm{K_{n-m}}$.
\\
Case 2: Suppose $\bm{w} \in \text{int}\Delta^{n-2}$. \\
Assume for contradiction that $v_n \in \Pi(\bm{\Lambda})$. 
Using this assumption, we obtain the following 
\begin{align*}
    \sum_{i=1}^{n-1} w_i d^2(v_n, v_i) &=\sum_{i=1}^{n-1} w_i \\
    &>\sum_{i=1}^{n-2} w_i && \text{because } \bm{w} \in \text{int}\Delta^{n-2} \\
    &=\sum_{i=1}^{n-1} w_i d^2(v_{n-1}, v_i). 
 \end{align*}
 Therefore, $v_n$ is not a minimizer, so $v_n \notin \Pi(\Lambda)$. 
 This is a contradiction, hence $\bm{K_n}$ has no nontrivial locus cover. 
\end{proof}



\subsection{Proof of Theorem~\ref{thm:active_largest} (active next-class selection for trees)}
\begin{proof}
We will prove the result by showing that the two optimization problems are equivalent. 
Let $v$ be a solution to the following optimization problem:
\begin{argmaxi*}|s|
{y \in \mathcal{Y} \setminus \Pi(\Lambda)}{d(y, b)}
{}{}
\addConstraint{b \in \partial_{\text{in}} T'}
\addConstraint{\Gamma(b, y) \setminus \{b\} \subseteq \mathcal{Y} \setminus \Pi(\Lambda)}, 
\end{argmaxi*}
where $\partial_{\text{in}} T'$ is the inner boundary of $T'$, the subgraph of $T$ whose vertices are $\Pi(\Lambda)$. 
This optimization problem can be equivalently rewritten as 
\begin{argmaxi*}|s|
{y \in \mathcal{Y} \setminus \Pi(\Lambda)}{|\Gamma(y, b)|}
{}{}
\addConstraint{b \in \partial_{\text{in}} T'}
\addConstraint{\Gamma(b, y) \setminus \{b\} \subseteq \mathcal{Y} \setminus \Pi(\Lambda)}, 
\end{argmaxi*}
and we can furthermore introduce additional terms that do not change the maximizer  
\begin{argmaxi*}|s|
{y \in \mathcal{Y} \setminus \Pi(\Lambda)}{| \cup_{\lambda_i, \lambda_j \in \Lambda} \Gamma(\lambda_i, \lambda_j) \cup \Gamma(b, y)|}
{}{}
\addConstraint{b \in \partial_{\text{in}} T'}
\addConstraint{\Gamma(b, y) \setminus \{b\} \subseteq \mathcal{Y} \setminus \Pi(\Lambda)}. 
\end{argmaxi*}
Equivalently, we can also connect $b$ to one of the elements of $\Lambda$
\begin{argmaxi*}|s|
{y \in \mathcal{Y} \setminus \Pi(\Lambda)}{| \cup_{\lambda_i, \lambda_j \in \Lambda} \Gamma(\lambda_i, \lambda_j) \cup \Gamma(\lambda_i, b) \cup \Gamma(b, y)|}
{}{}
\addConstraint{b \in \partial_{\text{in}} T'}
\addConstraint{\Gamma(b, y) \setminus \{b\} \subseteq \mathcal{Y} \setminus \Pi(\Lambda)}. 
\end{argmaxi*}
Due to the uniqueness of paths in trees, this optimization problem also has the following equivalent form without any dependence on $b$:
\begin{align*}
    v \in \argmax_{y \in \set{Y}\setminus\Lambda} | \cup_{\lambda_i, \lambda_j \in \Lambda} \Gamma(\lambda_i, \lambda_j) \cup \Gamma(\lambda_i, y) | 
    &= \argmax_{y \in \set{Y}\setminus\Lambda} | \cup_{\lambda_i, \lambda_j \in \Lambda} \Pi(\{\lambda_i, \lambda_j\}) \cup \Pi(\{\lambda_i, y\}) | \\
    &= \argmax_{y \in \set{Y}\setminus\Lambda} | \cup_{\lambda_i, \lambda_j \in \Lambda \cup \{y\}} \Pi(\{\lambda_i, \lambda_j\}) | \\
    &= \argmax_{y \in \set{Y}\setminus\Lambda} | \Pi(\Lambda \cup \{y\}) | \\
    &\text{using Lemma~\ref{lemma:tree_pairwise}}. 
\end{align*}
Therefore $v$ is a maximizer of $| \Pi(\Lambda \cup \{v\}) |$, as required. 
\end{proof}



\section{Algorithms and Time Complexity Analyses}
\label{app:time_complexity}
%\renewcommand{\thealgorithm}{}

We provide time complexity analyses for Algorithms~\ref{alg:phylo},~\ref{alg:locus_pairwise}, and~\ref{alg:locus_general}. 

\subsection{Analysis of Algorithm~\ref{alg:phylo} (locus cover for phylogenetic trees)}
We provide Algorithm~\ref{alg:phylo} with comments corresponding to the time complexity of each step.
\begin{algorithm}[]\label{alg:phylo}
\caption{Locus cover for phylogenetic trees}
\begin{algorithmic}
\Require phylogenetic tree $T=(\set V, \set E)$, $\set{Y}=\text{Leaves}(T)$
\State $N \gets |\set{Y}|$
\State $P \gets \text{sortbylength}( [\Gamma(y_i, y_j)]_{i,j \in [N]} )$ \Comment{$N|\set{E}| + N^2\log N$}
\State $P \gets \text{reverse}(P)$ \Comment{$O(N^2)$}
\State $\Lambda \gets \emptyset$
\For{$\Gamma(y_i, y_j)$ in $P$} \Comment{$O(N^2)$}
%\State $y_i, y_j \gets \text{endpoints}(\Gamma(y_i, y_j))$
\If{$\Pi(\Lambda) = \set{Y}$} \Comment{$O(K^2D\max\{N|\set{E}|, N^2\log N\})$}
    \State \Return $\Lambda$ %\Comment{This is a comment}
\Else
    \State $\Lambda \gets \Lambda \cup \{y_i, y_j\}$
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Combining these, we obtain the following time complexity:
$$ O( N|\set{E}| + N^2\log N + N^2 + N^2 K^2D\max\{N|\set{E}|, N^2\log N \}) = O(N^2 K^2D\max\{N|\set{E}|, N^2\log N\}). $$

\subsection{Analysis of Algorithm~\ref{alg:locus_pairwise} (computing a pairwise decomposable locus)}
We first provide Algorithm~\ref{alg:locus_pairwise} here, with comments corresponding to the time complexity of each step.
\begin{algorithm}[]\label{alg:locus_pairwise}
\caption{Computing a pairwise decomposable locus}
\begin{algorithmic}
\Require $\Lambda$, $\set{Y}$, $G=(\set{V}, \set{E})$
\State $\Pi \gets \emptyset$
\State $D \gets \text{diam}(G)$ \Comment{$O(N|\set{E}| + N^2\log N)$}
%\For{$y$ in $\set{Y}$}
\For{$\lambda_i, \lambda_j \in \Lambda$} \Comment{$O(K^2)$}
\For{$w_1$ in $\left\{\frac{0}{D}, \frac{1}{D}, ..., \frac{D}{D}\right\}$} \Comment{$O(D)$}
\State $\mathbf{w} \gets [w_1, 1 - w_1]$
\State $\Pi \gets \Pi \cup m_{\{\lambda_i, \lambda_j\}}(w)$ \Comment{$O(N|\set{E}| + N^2\log N)$}
\EndFor
\EndFor
%\EndFor
\State \Return $\Pi$
\end{algorithmic}
\end{algorithm}

We first compute the diameter of the graph $G = (\set{Y}, \set{E})$ with $N=|\set{Y}|$, which is done in $O(N|\set{E}| + N^2\log N)$ time using Dijkstra's algorithm to compute the shortest paths between all pairs of vertices. 
%This computation will be reused for the Fréchet mean as well. 
We then iterate over all pairs of elements in $\Lambda$ with $K=|\Lambda|$, which amounts to $O(K^2)$ iterations.
Within this, we perform $O(D)$ computations of the Fréchet mean, for which each iteration requires $O(N|\set{E}| + N^2\log N)$ arithmetic operations or comparisons. 
Combining these, the final time complexity is $$O(N|\set{E}| + N^2\log N + K^2D(N|\set{E}| + N^2\log N)) = O(K^2D\max\{N|\set{E}|, N^2\log N\}).$$


\subsection{Analysis of Algorithm~\ref{alg:locus_general} (computing a generic locus)}
We provide Algorithm~\ref{alg:locus_general} with comments corresponding to the time complexity of each step.
\begin{algorithm}[]\label{alg:locus_general}
\caption{Computing a generic locus}
\begin{algorithmic}
\Require $\Lambda$, $\set{Y}$, $G=(\set{V}, \set{E})$
\State $\Pi \gets \emptyset$
\State $D \gets \text{diam}(G)$ \Comment{$O(N|\set{E}| + N^2\log N)$}
%\For{$y$ in $\set{Y}$}
\For{$\mathbf{w}$ in $\left\{\frac{0}{D}, \frac{1}{D}, ..., \frac{D}{D}\right\}^{|\Lambda|}$} \Comment{$O(D^K)$}
\State $\Pi \gets \Pi \cup m_{{\bf\Lambda}}(w)$ \Comment{$O(N|\set{E}| + N^2\log N)$}
\EndFor
%\EndFor
\State \Return $\Pi$
\end{algorithmic}
\end{algorithm}

Following a similar argument from the analysis of Algorithm~\ref{alg:locus_pairwise}, the time complexity is 
$$O(N|\set{E}| + N^2\log N + D^K(N|\set{E}| + N^2\log N)) = O(D^K),$$
where $K$ is the number of observed classes. 

\section{Additional Experiments}
\label{app:experiments}

% Figure environment removed

\subsection{Additional calibration experiments}
In the main text, we evaluated the effect of calibrating the Softmax outputs on the performance of $\loki$ using a metric space based on the internal features of the CLIP model.
Here, we perform the same analysis using two external metric spaces. 


\paragraph{Setup}
We perform our calibration analysis using the default and WordNet tree. 

\paragraph{Results}
Our results are shown in Figure~\ref{fig:app_calibration}. 
Like our experiment using an internally-derived metric, we find that the optimal Softmax temperature is close to the CLIP default and the optimal temperature for calibration. 
For the default tree, we again found that temperature scaling can be used to improve accuracy using $\loki$. 

\paragraph{Setup}
We calibrate via Softmax temperature scaling \cite{pmlr-v70-guo17a} using CLIP on CIFAR-100. 
We do not use an external metric space, and instead use Euclidean distance applied to the CLIP text encoder. 

\paragraph{Results}
The reliability diagram in Figure~\ref{fig:calibration} shows that the optimal Softmax temperature for $\loki$ is both close to the default temperature used by CLIP and to the optimally-calibrated temperature. 
In Figure~\ref{fig:calibration} (right), we find that appropriate tuning of the temperature parameter \textit{can lead to improved accuracy with CLIP}, even when no external metric space is available. 


\subsection{Ablation of $\loki$ formulation}
$\loki$ is based on the Fréchet mean, which is defined as $\argmin_{y \in \mathcal{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^{2}(y, \lambda_i)$. 
However, this is not the only approach that can be considered. For example, the Fréchet \textit{median}, often used in robust statistics, is defined as $\argmin_{y \in \mathcal{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d(y, \lambda_i)$, without squaring the distances. 
More generally, we define $\argmin_{y \in \mathcal{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^{\beta}(y, \lambda_i)$ and evaluate different choices of $\beta$. 

\paragraph{Setup} 
We conduct this experiment on ImageNet using SimCLR as our pretrained classifier with $K=$ 250, 500, and 750 randomly selected classes. 

\paragraph{Results} 
From this analysis, we conclude that using the Fréchet mean is the optimal formulation for Loki, as it achieved the lowest mean squared distance for all settings of $K$. 


\begin{table}[h]
\caption{Expected squared distances of SimCLR+Loki alternatives on ImageNet. 
We ablate over the choice of distance exponent in $\loki$ (where $\textcolor{red}{\beta} = 2$, corresponding to the Fréchet mean), including the Fréchet median ($\textcolor{red}{\beta} = 1$). 
That is, we tune
$\textcolor{red}{\beta} : \hat{y} \in \text{arg min}_{y \in \mathcal{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^{\textcolor{red}{\beta}}(y, \lambda_i)$ and find that the optimal setting is $\textcolor{red}{\beta} = 2$, corresponding to $\loki$. 
%Loki uses $\textcolor{red}{\beta} = 2$.
}
\begin{center}
    \begin{tabular}{l|l|l|l}
    \toprule
    $\textcolor{red}{\beta}=$                & $K=250$ & $K=500$   & $K=750$ \\
    \midrule
    \rowcolor[gray]{.9} 
    0.5                &    61.28      &   46.57      &    36.31     \\
    1                  &    52.98      &   41.06      &    33.14     \\
    \rowcolor[gray]{.9} 
    {\bf 2 (Loki)}     &{\bf46.78}     &{\bf37.76}    &{\bf29.88}    \\
    4                  &    47.99      &   39.58      &    34.65     \\
    \rowcolor[gray]{.9} 
    8                  &    65.29      &   58.42      &    54.90     \\
    \bottomrule
    \end{tabular}
\end{center}
\end{table}

\subsection{Comparison across metric spaces}
Expected squared distances cannot be directly compared across metric spaces, as they may be on different scales. Our solution is to use a form of normalization: we divide the expected squared distance by the square of the graph diameter. This brings all of the values to the 0-1 range, and since $\E[d^2(y, \hat{y})] / \text{diam}(G)^2$ indeed also generalizes the 0-1 error, this enables comparison between 0-1 errors and those from different metric spaces. We provide these results in Table 1, again for our CLIP experiments on CIFAR-100. 
This evaluation metric enables us to determine which metric spaces have geometry best `fit' to our pretrained models. 

\paragraph{Setup} 
Using $\E[d^2(y, \hat{y})] / \text{diam}(G)^2$, we re-evaluate our CLIP results on CIFAR-100 shown in Table~\ref{table:clip_improver} for the ResNet-50 architecture. 

\paragraph{Results} 
For CIFAR-100, we observe that the WordNet metric space resulted in the lowest error and therefore has the best geometry. 

\begin{table}[h]
\label{app:cross_metric_table}
\caption{Comparison across metric spaces for CLIP on CIFAR-100 by normalizing by the squared diameter of the metric space: 
$\mathbb{E}[d^2(y, \hat{y})] / \textcolor{red}{\text{diam}(G)^2}$.}
\begin{center}
    \begin{tabular}{l|l|l}
    \toprule
    Metric space $G$ & $\textcolor{red}{\text{diam}(G)^2}$ & $\mathbb{E}[d^2(y, \hat{y})] / \textcolor{red}{\text{diam}(G)^2}$ \\
    \midrule
    \rowcolor[gray]{.9} 
    Complete graph   & 1                  & 0.5941 (0-1 error)                               \\
    Default tree     & 16                 & 0.4493                                           \\
    \rowcolor[gray]{.9} 
    {\bf WordNet tree}     & 169                & {\bf 0.1157}                                           \\
    CLIP features    & 0.9726             & 0.2686 \\
    \bottomrule
    \end{tabular}
\end{center}
\end{table}




\section{Experimental Details}
\label{app:experimental_details}
In this section, we provide additional details about our experimental setup.\footnote{Code implementing all of our experiments is available here: \url{https://github.com/SprocketLab/loki}.} 

\subsection{CLIP experiments}
All experiments are carried out using CLIP frozen weights. There are no training and hyperparameters involved in experiments involving CLIP, except for the Softmax temperature in the calibration analysis. 
We evaluted using the default CIFAR-100 test set. 
The label prompt we use is "a photo of a [class\_name]." 

\subsection{ImageNet experiments} 
To construct our datasets, we randomly sample 50 images for each class from ImageNet as our training dataset then use the validation dataset in ImageNet to evaluate $\loki$'s performance.
%
We extract the image embeddings by using  SimCLRv1\footnote{Embeddings are extracted from the checkpoints stored at: \url{https://github.com/tonylins/simclr-converter}} and train a baseline one-vs-rest classifier.
%
We use WordNet phylogenetic tree as the metric space. 
%
The structure of phylogenetic tree can be found at here\footnote{\url{https://github.com/cvjena/semantic-embeddings/tree/master/ILSVRC}}. 
%
We test different numbers of observed classes, $K$, from 1000 classes.
%
Observed classes are sampled in two ways, uniformly and Gibbs distribution with the concentration parameter 0.5.


\subsection{LSHTC experiments} 
We generate a summary graph of the LSHTC class graph (resulting in supernodes representing many classes) by iteratively:
\begin{enumerate}
    \item randomly selecting a node or supernode
    \item merging its neighbors into the node to create a supernode
\end{enumerate}
until the graph contains at most 10,000 supernodes.
%
In the LSHTC dataset, each datapoint is assigned to multiple classes. 
%
We push each class to its supernode, then apply majority vote to determine the final class. 
%
We test different numbers of observed classes, $K$, from 10,000 classes. 
%
We collect datapoints which are in the observed classes.
%
Then split half of dataset as training dataset and make the rest as testing dataset, including those datapoints which are not in the observed classes. 
%
We train a baseline classifier using a 5-NN model and compare its performance with $\loki$.

\section{Broader Impacts and Limitations}
\label{app:broader_impacts}
As a simple adaptor of existing classifiers, it is possible for our method to inherit biases and failure modes that might be present in existing pretrained models. 
On the other hand, if the possibility of harmful mis-classifications are known a priori, information to mitigate these harms can be baked into the metric space used by $\loki$. 
Finally, while we have found that $\loki$ often works well in practice, it is possible for the per-class probabilities that are output by a pretrained model to be sufficiently mis-specified relative to the metric over the label space, or for the metric itself to be mis-specified. 
Nonetheless, we have found that off-the-shelf or self-derived metric spaces to work well in practice. 


% \section{ADDITIONAL EXPERIMENTS}

% If you have additional experimental results, you may include them in the supplementary materials.


\section{Random Locus Visualizations}
\label{app:vis}
In Figures~\ref{fig:banner},~\ref{fig:tree_loci},~\ref{fig:phylotree_loci},~\ref{fig:smallworld_loci},~\ref{fig:erdos_loci}, and~\ref{fig:albert_loci}, we provide visualizations of classification regions of the probability simplex when using $\loki$ with only three observed classes out of 100 total classes, and different types of random metric spaces. 
The first example in Figure~\ref{fig:banner} shows the prediction regions on the probability simplex when using standard $\argmax$ prediction---the three regions correspond to predicting one of the three classes (0, 39, and 99) and no regions corresponding to any of the other classes $\{1, ..., 38, 40, ..., 98\}$. We compute these regions using a version of Algorithm~\ref{alg:locus_general}, and while it does have exponential time complexity, the exponent is only three in this case since we consider only three observed classes. 

On the other hand, the other three examples in Figure~\ref{fig:banner} and Figures~\ref{fig:tree_loci},~\ref{fig:phylotree_loci},~\ref{fig:smallworld_loci},~\ref{fig:erdos_loci}, and~\ref{fig:albert_loci} all show the prediction regions on the probability simplex when using $\loki$. 
Figure~\ref{fig:tree_loci} shows this for random tree graphs.
Here, the prediction regions are striped or contain a single triangle-shaped region in the center---these correspond, respectively, to intermediate classes along branches of the tree leading up from the observed class and the prediction region formed by the common-most parent node. 
Figure~\ref{fig:phylotree_loci} shows similar regions, although these are more complex and are thus more difficult to interpret, as phylogenetic trees are metric subspaces equipped with the induced metric from trees. 
Furthermore, in order to generate phylogenetic trees with 100 leaves, we needed to create much larger trees than the ones used for Figure~\ref{fig:tree_loci}, which led to narrower prediction regions due to the higher graph diameter. 
Finally, Figures~\ref{fig:smallworld_loci},~\ref{fig:erdos_loci}, and~\ref{fig:albert_loci} each show the prediction regions using $\loki$ when the metric space is a random graph produced using Watts-Strogatz, Erd\H{o}s-Rényi, and Barabási–Albert, models respectively. 
These prediction regions are more complex and represent complex relationships between the classes. 


% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

\vfill

