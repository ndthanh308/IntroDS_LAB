%!TEX root = ../main.tex

\section{Framework}
% introduce algo immediately
% add algo right below it 
We introduce our framework---$\loki$. 
We show how $\loki$ can be used to adapt any supervised classifier over a set of $K$ classes to a much richer set of possible class predictions. 
It does so by weighting the Fréchet mean by the classifier's per-class prediction probabilities or logits, allowing it to predict any class in the locus of the Fréchet mean---potentially far more classes than the initial $K$. 
Next, we show how $\loki$ can be expressed as a \textit{fixed} linear transformation of a model's outputs. 
Finally, we show that $\loki$ relates to standard classification.%> in the absence of relational structure between the classes. 
% Specifically, we will show how the Fréchet mean and the Locus of the Fréchet mean can be used in conjunction with binary one-vs-rest classifiers to predict unobserved classes. 
% %
% We assume access to an unweighted and undirected graph whose nodes represent classes and whose edges represent relationships between the classes. 
% %
% In particular, the shortest-path distance between two nodes representing classes encodes the relatedness of the two classes. 
% %
% If two classes are highly related, this relationship can be encoded in the graph by their nodes being adjacent---conversely, if two classes are maximally unrelated, then this can be encoded as peripheral nodes achieving the diameter of the graph. 

% \paragraph{Obtaining Relational Structure} The structure of the graph itself will strongly depend on the classification problem at hand.
% For example, ImageNet has a natural structure induced by the WordNet hierarchy---this is a tree graph whose leaves correspond to the 1000 classes of ImageNet. 
% Another simple example is rating prediction---suppose we would like to predict natural number movie ratings on a 1 to 10 scale---we can encode natural numbers geometrically using a path graph, as they have a well-ordering. 
% Finally, one might wish to perform classification where the classes have some spatial relationship (e.g., location classification)---in this case, the relationship can be encoded using a 2D grid graph. 
% Of course, not all classification problems have such a nice relational structure between their classes---we model this case using the complete graph, i.e., every class is distance 1 from every other class, and show that our framework conveniently reduces to one-vs-rest classification under the 0-1 loss. 

\subsection{$\loki$: Adapting Pretrained Models}
\label{sec:framework}
%$\loki$ incorporates metric information into classification problems via a learning reduction to standard classification. 
%\nick{TODO rewrite the above}
%We describe our approach to adapting classifier outputs among observed classes $\Lambda$ to predict a larger set of classes in the label space $\mathcal{Y}$ using metric space information of $\mathcal{Y}$. 
We describe our approach to adapting pretrained classifiers--trained on a set of classes $\Lambda$---to the metric geometry of the label space $\mathcal{Y}$, enabling the prediction of unobserved classes. 

We model unobserved classes $\mathcal{Y} \setminus \Lambda$ using the Fréchet mean among observed classes weighted by their \textit{prediction probabilities} $P(y=\lambda_i| x \text{ and } y \in \Lambda)$. 
We denote the vector of model outputs as $\mathbf{P}_{y|x} := [\mathbf{P}_{\lambda_i|x}]_{i=1}^K = [P(y=\lambda_i| x \text{ and } y \in \Lambda)]_{i=1}^K$. 
Then predictions using $\loki$ are given by
$$ \hat{y} \in m_{\boldsymbol{\lambda}}(\mathbf{P}_{y|x}) = \argmin_{y \in \set{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^2(y, \lambda_i). $$


\subsection{$\loki$ as a linear transformation of model outputs}
Most standard classifiers output a vector of prediction probabilities, $\mathbf{P}_{y|x}$, whose entries correspond to the confidence of predicting a specific class. 
Predictions are typically given by $\hat{y} \in \argmax_{i \in [K]} (\mathbf{P}_{y|x})_i$. 
$\loki$ generalizes this prediction rule when viewed as a linear transformation of $\mathbf{P}_{y|x}$. 
Consider the $\loki$ prediction rule
%\begin{align*}
    $\hat{y} \in m_{\boldsymbol{\lambda}}(\mathbf{P}_{y|x}) = \argmin_{y \in \set{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^2(y, \lambda_i) = \argmax_{j \in [N]} (\mathbf{D} \mathbf{P}_{y|x})_j,$
%\end{align*}
where $\mathbf{D}_{j, i} = \left[ -d^2(y_j, \lambda_i) \right]$; $\mathbf{D} \in \R^{N \times K}$ is the matrix of negative squared distances between the observed classes and the rest of the label space. 
Thus $\loki$ can be used within standard classification pipelines when the model output $\mathbf{P}_{y|x}$ is multiplied by the fixed matrix $\mathbf{D}$.%, producing transformed logits. 
%If probability vector outputs are desired, then the softmax can be applied to $\mathbf{D} \mathbf{P}_{y|x}$.

% \nick{TODO rewrite}
% At its core, $\loki$ is an alternative to the standard $\argmax$ over per-class probabilities or logits. 
% Ideally such an alternative should be able to interface with the cross entropy loss and allow for backpropagation. 
% Observe that we can rewrite $\loki$ as an $\argmax$ over logits
% \begin{align*}
%     m_{\boldsymbol{\lambda}}(\mathbf{P}_{y|x}) &= \argmin_{y \in \set{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^2(y, \lambda_i) \\
%     &= \argmax_{j \in [|\set{Y}|]} -\sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^2(y_j, \lambda_i),
% \end{align*}
% where $\mathbf{P}_{y|x}$ itself is the output of a softmax layer. 
% Then in order to use $\loki$ with the cross entropy loss, simply output the transformed vector of logits: $\left[-\sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^2(y_j, \lambda_i)\right]_{j \in [|\set{Y}|]}$. 
% Once this vector is passed to the cross entropy loss, it can be used to backpropagate through the locus of the Fréchet mean. 


\subsection{Generalizing Standard Classification}
% TODO argue that BTW, this reduces to standard classification in the case of an uninformative metric
We provide a simple intuition for our approach.
%
The fact that $\loki$ reduces to standard classification among the observed classes has several implications. 
This includes the idea that under our framework, forms of few-shot, zero-shot, hierarchical, and partial label learning all reduce to standard classification when additional metric information is introduced. 

\paragraph{Generalizing the arg max prediction rule} In the absence of this metric information---a situation that we model using the complete graph and setting $\Lambda = \mathcal{Y}$---our framework also recovers standard classification. 
Indeed, both in terms of error modeling and in terms of inter-class similarity, the intuition of standard classification and of the 0-1 loss are captured well by the unweighted complete graph---simply treat all differences equally. 
This graph is given as $K_{N} := (\set{Y}, \set{Y}^2 \times \{1\})$---i.e., every label is equidistant from every other label. 
Plugging this into $\loki$, we obtain the following:
\begin{align*}
    \hat{y} \in m_{\boldsymbol{\lambda}}(\mathbf{P}_{y|x}) &= \argmin_{y \in \set{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} d^2(y, \lambda_i) 
    = \argmin_{y \in \set{Y}} \sum_{i=1}^K \mathbf{P}_{\lambda_i|x} \boldsymbol{1}\{ y \neq \lambda_i \} 
    = \argmax_{i \in [K]} \mathbf{P}_{\lambda_i|x}, 
\end{align*}
which is exactly the standard classification prediction rule.

\paragraph{Generalizing the 0-1 loss via the expected squared distance} \emph{The expected squared distance $\E[d^2(y, \hat{y})]$ is the standard loss function for many structured prediction problems}. Note that \textbf{accuracy fails in such settings}---since it cannot distinguish between small and large errors. This is most clearly seen in the extreme case of regression, where test accuracy will virtually always be zero no matter how good a trained model is. At the other extreme, the complete graph, this loss function becomes the standard 0-1 loss: $\E[d^2(y, \hat{y})] = \E[\boldsymbol{1}\{y \neq \hat{y}\}]$. 
As an adaptor for structured label spaces, we use the empirical version of this loss to evaluate $\loki$. 
Note that the expected squared distance subsumes other metrics as well. 
For example, when $\set{Y} = \mathbb{R}$, we can derive the standard MSE by setting $d(y, \hat{y}) = |y - \hat{y}|$, which is just the standard L1 distance metric. 
%Recall at top-k can be obtained in a similar fashion at the cost of being a true metric.
Other scores such as recall at top-k can be similarly obtained at the cost of $d(\cdot, \cdot)$ being a true metric. 
In other words, $\E[d^2(y, \hat{y})]$ is an very general metric that supports any metric space, and we use it throughout this work. 
%When meaningful semantic information relating classes are available, it enables taking advantage of this information. 
%When it is not available, it reproduces standard measures. 

