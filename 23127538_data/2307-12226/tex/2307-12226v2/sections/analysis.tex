%!TEX root = ../main.tex

\section{Theoretical Results}
% One of two ways:
% give learning theory result first
% then analysis of minimal set
% then active learning
% OR
% 
% them move all proofs out -- gives us a sense of how long the draft is
% 
% don't so subsubsubsubsubsections 
% 
% We first provide a learning-theoretic result that captures the tradeoff between training set sample complexity, label space richness, and model dimension. %example in which we reduce the problem of predicting unobserved classes to that of binary classification. 
% Next, we characterize when it is possible to predict any class in the metric space using an optimally chosen subset of classes.
% Finally, we demonstrate an active learning strategy for selecting the optimal next class. All proofs are deferred to the Appendix.

\paragraph{Challenges and Opportunities}
The $\argmax$ of per-class model probabilities is a ubiquitous component of classification pipelines in machine learning. 
In order to predict unobserved classes using metric space information, $\loki$ replaces this standard component. 
As a simple but significant change to standard pipelines, $\loki$ opens up a new area for fundamental questions. 
There are three main flavors of theoretical questions that arise in this setting:
\begin{enumerate}
    \item How does the performance of $\loki$ change as a function of the number of samples? 
    \item What minimal sets of observed classes are required to predict any class in the metric space? 
    \item How can we acquire new classes that maximize the total number of classes we can predict? 
\end{enumerate}
Excitingly, we provide a comprehensive answer to these questions for the most common metric spaces used in practice.
First, we provide a general error bound in terms of the number of samples, observed classes, problem dimension, and the diameter of the metric space, that holds for any finite metric space. 
Second, we characterize the sets of observed classes that are required to enable prediction of any class, and show how this set differs for various types of metric spaces of interest. 
Finally, we provide an active learning algorithm for selecting additional classes to observe so as to maximize the number of classes that can be predicted and we characterize the types of metric spaces for which the locus can be computed efficiently. 
These results provide a strong theoretical grounding for $\loki$. %n exhaustive set of answers to the space of theoretical questions about $\loki$. 


%In this section, we address these questions for a variety of different metric spaces, including trees, phylogenetic trees, grid graphs, and the trivial case of the complete graph. 

%... We address all of them and comprehensively solve the entire theory space. 


%We provide a comprehensive theoretical analysis of $\loki$. 




\input{sections/analysis/1_learning}
\input{sections/analysis/2_optimal}
\input{sections/analysis/3_realistic}
