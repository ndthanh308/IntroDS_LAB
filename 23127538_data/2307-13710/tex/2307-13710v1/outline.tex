% The use of this template is recommended for all ACS journals, and most other journals. 
% If the target journal has its own template, you may use that instead. 
% Otherwise, make life easier for everyone and just stick to this one.
% The main exception is when you are writing for an extremely high impact journal, in which case I may give you different instructions.

\documentclass[manuscript=article]{achemso}
\usepackage{xcolor}
\usepackage{float}


% This makes sure that the references have titles. Which is a requirement in many journals and also useful for me to know if you have cited the proper literature.
\setkeys{acs}{usetitle=true}

% The following are the minimum packages you need. You can add others as needed, e.g., longtable.
% The rule is that you keep external packages to a minimum. Do not import esoteric packages, which may cause problems with the journals. 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage[version=3]{mhchem}


% If you have a supplementary information, uncomment the lines below. It will allow you to automatically cross-reference the SI.
%\usepackage{xr}
%\externaldocument{ESI}

\title[Cluster Sampling for Robust Training of Machine Learning Interatomic Potentials]{Cluster Sampling for Robust Training of Machine Learning Interatomic Potentials}
\author{Ji Qi}
\affiliation[UCSD]{Department of NanoEngineering, University of California San Diego, 9500 Gilman Dr, Mail Code 0448, La Jolla, CA 92093-0448, United States}
\author{Author 2}
\affiliation[UCSD]{Department of NanoEngineering, University of California San Diego, 9500 Gilman Dr, Mail Code 0448, La Jolla, CA 92093-0448, United States}
\author{Add authors as needed}
\affiliation[UCSD]{Department of NanoEngineering, University of California San Diego, 9500 Gilman Dr, Mail Code 0448, La Jolla, CA 92093-0448, United States}
\author{Tuan Anh Pham}
\email{pham16@llnl.gov}
\affiliation[LLNL]{Quantum Simulations Group and Laboratory for Energy Applications for the Future (LEAF), Lawrence Livermore National Laboratory, Livermore, CA, 94550, USA}
\author{Shyue Ping Ong}
\email{ongsp@eng.ucsd.edu}
\affiliation[UCSD]{Department of NanoEngineering, University of California San Diego, 9500 Gilman Dr, Mail Code 0448, La Jolla, CA 92093-0448, United States}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Machine learning interatomic potentials (ML-IAPs) enable the accurate simulation of materials at larger size and time scales, and play increasingly important roles in computational understanding and design of materials. However, ML-IAPs are only as accurate and robust as the data they are trained on. In this work, we present a 2-stage cluster sampling (2SCS) approach to select a robust subset of structures from large and complex configuration space with minimal loss of information in feature space. We demonstrate the power of this approach by its application to the Materials Project relaxation trajectories dataset with over one million structures and 89 elements to develop an improved materials 3-body graph network (M3GNet) universal potential that extrapolate more reliably to structures unseen in the training data. Furthermore, we show that universal potentials such as M3GNet can be used in place of \textit{ab initio} methods to rapidly create a large configuration space of training structures for target materials systems. Combined with 2SCS sampling, we develop a highly accurate moment tensor potential for Ti-H system without the need for iterative optimization. This work paves the way towards robust high throughput training of ML-IAPs across any compositional complexity.
\end{abstract}

\section{Introduction}


% % Figure environment removed

\begin{enumerate}
    \item Introduce ML-IAPs
    \item Robustness of ML-IAPs depends on training data. DFT computations is the main cost of generating training data.
    \item Background on approaches for reliable ML-IAP training. Active learning. The Aidan Thompson paper. Any others? Introduce their pros and cons.
    \item The model complexity of most ML-IAPs increase squarely with the number of elements, making it almost impossible to fit for material systems with high compositional complexity. Background on M3GNet universal potential.
    \item In this paper, we demonstrate two main innovations to training data generation for ML-IAPs with a DImensionality REduction Clustering sTratified Sampling (DIRECTS) strategy. The first is to use DIRECTS as a rigorous approach to sampling training data from the space of existing data with minimal loss of information in feature space. We will demonstrate the development of an improved M3GNet universal potential (M3GNet-UP-2023) with better simulation reliability for hypothetical materials and high temperature molecular dynamics. The second is to utilize universal potentials such as M3GNet as a means to rapidly generate training structures coupled with DIRECTS to develop robust, custom ML-IAPs for subsystems without iterations of optimization. We will demonstrate this using a diffusion active model system known to be challenging for reliable MD, the Ti-H system, where we consider multiple crystalline phases as well as grain boundary structures.
    
    % \item In this work, a generalizable workflow combining M3GNet MD and a two-stage cluster sampling (2SCS) strategy is proposed to sample a diverse feature space without iterative collection of new structures. M3GNet MD will be showcased to sample a equally or more broad feature space of Ti-H than that of active learning MTP. 2SCS provides a comprehensive coverage of the feature space. By contrast, manual sampling with fixed intervals always lead to extra holes in the feature space and over sampling of common configurations. The Ti-H MTP trained with 2SCS data set shows overall the best reliability. The 2SCS has also been applied to the MPF.2021.02.08 dataset and has achieved a overall better coverage of the distribution of energies and forces in MP, outperforming the human designed structure selection strategy to sample representative cases. This 2SCS selected data set ameliorates the overfitting issue of the M3GNet 0.0.1 version and provides improved accuracy for extrapolation on hypothetical materials.
    
\end{enumerate}

\section{Methods}
\subsection{Configuration space}
\subsubsection{Materials project relaxation trajectory dataset}
\subsubsection{Ti-H chemical system}

\subsection{Feature space}
\subsubsection{Materials graph network feature}
\subsubsection{Materials graph network feature with three-body interactions}
\subsubsection{Bispectrum coefficient}

\subsection{Training data sampling}
\subsubsection{Manual sampling}
\subsubsection{Cluster analysis sampling}

\subsection{ML-IAP fitting}

\subsection{DFT calculations}


\section{Results}


\subsection{Materials project structure sampling}

% Figure environment removed

\begin{enumerate}
    \item The manual sampling method follows the strategy in Chi's paper to sample 185,975 first, middle and last snapshot structures from the 1,317,296 MP relaxation trajectory structures. Though this strategy attempted to sample diverse configuration space, it still miss a large amount of information of relatively uncommon structures. As in Fig. \ref{subfig:MP_feature_75th_all} and \ref{subfig:MP_feature_75th_MS}, for common structures with the 75$^{th}$ element of M3GNet structure feature less than 1,  the MS set provides a good coverage, but for uncommon structures with feature 75 larger than 1, the MS set has poor coverage. The coverage score of MS set for the feature 75 is 62.5, while the averaged coverage score of MS set for all the 128 feature elements is 74.5. 
    \item The flawed coverage of MS set is also illustrated in the PCA plot. (see Fig. \ref{subfig:MP_PCA_MS}) With the first two PCs cover 47.2\% explained variance of the feature space of the 1.3 million MP structures, the MS set apparently under sampled structures on the boundary regions of the first two PCs.
    \item CS set provides an extraordinary coverage to the feature space of the MP data. As in Fig. \ref{subfig:MP_feature_75th_CS} and \ref{subfig:MP_PCA_CS}, the CS set covers all value ranges of the 75$^{th}$ element of M3GNet structure feature and shows minimal loss of information in PCA feature space, respectively. Such great coverage of both common and uncommon configurations lead to an average coverage score of 99.2. 
    % \item  MS set has totally 186,001 structures, and a random split of 90:5:5 were performed to get train, validation and test sets. The CS set has 167,472 structures, roughly the same size as the MS training set, and the CS set will directly serve as training set for M3GNet CS to avoid loss of information by train test split.
\end{enumerate}

% Figure environment removed

\begin{enumerate}
    \item This figure can be put into SI. The 45th feature elements is taken out for better illustration as in Fig. \ref{fig:MP_PCA_feature_coverage}
\end{enumerate}


% Figure environment removed

\begin{enumerate}
    \item CS structures have a much more comprehensive coverage of energy, force and stress in the MP dataset. The CS set samples structures with relatively uncommon energy, force and stress so well that the purple bars are almost fully covered by yellow bars in those cases, i.e., the CS set collects all the uncommon structures.
    \item The MADs of energies, forces and stresses in CS set are thus much larger than either the MS set or the MP set. 
\end{enumerate}





    

\subsection{A more reliable M3GNet IAP}

% Figure environment removed

\begin{enumerate}
 
    \item Two test sets are established. To serve as test structures, structures already included in the CS training set is removed, resulting in 7,387 structures in the MS test set. Another test set of 8,000 structures is sampled by CS from the remaining ~1,000,000 trajectory structures not in either the MS set or the CS training set. 
    \item M3GNet-CS generally outperforms M3GNet-MS for the CS test set, while the two M3GNets performed comparably well for the MS test set. 
   \item \textcolor{red}{The M3GNet 0.0.1 is no longer provided for comparison, as it is likely trained with non-converged structures with 100 electronic steps.}
   \item \textcolor{red}{Structures with isolated atoms (all neighbors $>$ 5 Å away) will be removed from train and test structures.} 
   \item \textcolor{red}{Current training and evaluations haven't included the structures without threebody interactions in 4 Å because of code failure. They will be included.}
\end{enumerate}


% Figure environment removed

\begin{enumerate}
    \item M3GNet CS provides better accuracy for extrapolation than MS M3GNet. It is thus more reliable to be used for hypothetical materials screening.
    \item \textcolor{red}{More results of O, S, Cl, Na containing compounds are coming}.
    \item \textcolor{red}{We will compare CS, MS and 0.0.1 versions M3GNet in each figure}.
\end{enumerate}


\subsection{Ti-H structure generation: M3GNet MD}

% Figure environment removed


% Figure environment removed

\begin{enumerate}
    \item M3GNet MD/NPT covers the feature space of AIMD and active learning strurctures.
    \item \textcolor{red}{Checking the different feature space of different descriptor.}
    % \item PC1 is highly correlated with composition, while PC2 is related to different degree of vibration at different temperatures.
\end{enumerate}

\subsection{Ti-H structure sampling and MTP fitting}

% Figure environment removed

\begin{enumerate}
    \item As in Fig. \ref{subfig:TiH_sampling_coverage_score_27th}, CS has a comprehensive coverage of the distribution of the 27th element in MEGNet structure features of all 200 k structures from M3GNet MD. MS over samples the most common features while under samples the distinctive features. This trend is generally true as the coverage score averaged over all 96 feature elements is 93 for CS and 71 for MS. Coverage score for each feature element is defined as \textcolor{red}{awaiting input}.
    \item The comprehensive coverage of feature space by CS can be visualized with PCA 2D plot. As in Fig. \ref{subfig:TiH_sampling_MS} and Fig. \ref{subfig:TiH_sampling_CS}, the holes and exposed boundaries in the interval sampling feature space are well covered by CS. 
    \item It has been reported that flaws of feature coverage leads to extrapolation and thus large error for the under sampled configuration space [\textcolor{red}{cite Yunxing's benchmark paper}], therefore, ML-IAPs trained with the CS set can be expected to outperform those trained with MS set.
\end{enumerate}

% Figure environment removed

% Figure environment removed

\begin{enumerate}
    \item AL MTP and MS MTP both have some extremely large errors for certain structures, while CS MTP is having little outliers. 
    \item The MS MTP likely needs further optimization with active learning, while the CS MTP is ready to use.
    \item Energy MAEs are relatively high, partially because of the large range of distribution from -2 to -8 eV/atom.
    \textcolor{red}{The MAEs are still high, which can be improved by using formation energy as training energy, increasing model complexity, screening out some unphysical structures in M3GNet MD trajectory. }
\end{enumerate}

\section{Discussion}
\begin{enumerate}
    \item A generalizable workflow is proposed to fit ML-IAPs with imporved tranferability. It can be used for specific chemical systems to get rid of the iterative improvement of ML-IAPs. Meanwhile, it can also work with large existing data set of over millions of structures and select the most representative structures, which is a highly desirable but previously unachievable functionality.
    \item The sampling method in this work can be used not just for construction of a robust training set, but also for any purpose that requires the selection of a subset of configurations without loss of feature space information, e.g., finding possible equilibrium chemical ordering of high entropy materials from a huge set of MC/MD snapshot structures.
    \item This strategy provides a plausible way forward to augment the MP relaxation trajectory data set with MD trajectory snapshots, whose size is easily over million-scale and definitely out of control by previous methods.
    \item This method can be further improved with better designed structural descriptors.

\end{enumerate}

% This section is non-optional and you are not allowed to combine Results and Discussion. 

% The Discussion section is where you really go into the implications of your findings. What does your results mean for the field? Have you provided some fundamental insight into a scientific process? 

% A good Discussion section is the difference between a mediocre paper and an insightful article.


\section{Methods}
% Methods should be concise but descriptive enough that people know you have done your work right. But don't overburden the reader with too many details. 

% \section{Conclusion}

% Complete your paper by summarizing your findings and the implications, i.e., a summary of your results and discussion.

% Figure environment removed

\section{Appendix}
Manuscripts submitted to npj Computational Materials do not need to adhere to our formatting requirements at the point of initial submission; formatting requirements only apply at the time of acceptance. 

We encourage authors to incorporate the manuscript text and figures into a single PDF or Microsoft Word file. Suitably high resolution figures may be inserted within the text at appropriate positions or grouped at the end. Each figure legend should be presented on the same page as its figure. We can accept LaTeX files at the acceptance stage, but before then please supply compiled PDFs.

\begin{acknowledgement}

% This section needs to be added. Typically, it reads something like ``This work was supported by the U.S. Department of Energy, Office of Science, Basic Energy Sciences under Award DE- SCXXXXXX. A portion of the computations performed in this work also used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1053575.''.

\end{acknowledgement}

\begin{suppinfo}
 
% If you have supplementary information, most journals require you to briefly describe what's in the SI here. 

\end{suppinfo}

% This is where you add your references. Uncomment the line below and point it to your bibtex file.
%\bibliography{refs}

\clearpage

\listoffigures

\end{document}