
\section{Applicability to Optimal Control}

\subsection{Problem formulation and optimality conditions}
The following is a generic optimal control problem formulation written in terms of \emph{generalized inequalities}, denoted by "$\preceq$" as in \cite{Boyd2004}. This choice is motivated by the potential applicability of the new method to convex optimal control problems.
\begin{prob} \label{prob:optimal-control}
Determine $\bm{x}(t)$, $\bm{u}(t)$, $t_0$ and $t_f$ such that
\begin{equation} \label{eq:ocp-cost}
        \Psi_0 \big( t_0, \bm{x}(t_0)\big) + \Psi_f \big( t_f, \bm{x}(t_f)\big)
                          + \int_{t_0}^{t_f} h \bigl(t, \bm{x}(t), \bm{u}(t) \bigr) \dl{t}
\end{equation}
is minimized, subject to
\begin{gather}
    \dot{\bm{x}} (t) = \bm{f} \bigl(t, \bm{x}(t), \bm{u}(t) \bigr) \label{eq:dynamics} \,, \\
    \bm{g} \bigl(t, \bm{x}(t), \bm{u}(t) \bigr)  \preceq_{\mathcal{G}} \bm{0} \,, \label{eq:path-constraints} \\
    \bm{\phi}_0 \bigl(t_0, \bm{x}(t_0) \bigr) \preceq_{\Phi_0} \bm{0} \,, \quad 
    \bm{\phi}_f \bigl(t_f, \bm{x}(t_f) \bigr) \preceq_{\Phi_f} \bm{0} \,, \label{eq:endpoint-constraints} \\
    \forall t \in \closedinterval{t_0}{t_f} \,,
\end{gather}
where $\bm{x}(t) \in \mathbb{R}^{n_x}$ and $\bm{u}(t) \in \mathbb{R}^{n_u}$ denote, respectively, the state and the control variables of the dynamic system in \eqref{eq:dynamics}.
Moreover, the constraint sets $\mathcal{G} \subseteq \mathbb{R}^{n_g}$, $\Phi_0 \subseteq \mathbb{R}^{n_{\phi,0}}$ and $\Phi_f \subseteq \mathbb{R}^{n_{\phi,f}}$ are arbitrary intersections of \emph{proper cones}, see \cite{Boyd2004}, and $\Psi_0(\cdot)$, $\Psi_f(\cdot)$, $h(\cdot)$, $\bm{f}(\cdot)$, $\bm{g}(\cdot)$, $\bm{\phi}_0(\cdot)$, and $\bm{\phi}_f(\cdot)$ are arbitrary twice-differentiable functions characterized by the following mappings:
\begin{alignat}{2}
    \Psi_0 &: \mathbb{R} \times \mathbb{R}^{n_x} &&\mapsto \mathbb{R} \,, \\
        \Psi_f &: \mathbb{R} \times \mathbb{R}^{n_x} &&\mapsto \mathbb{R} \,, \\
        h &: \mathbb{R} \times \mathbb{R}^{n_x} \times \mathbb{R}^{n_u} &&\mapsto \mathbb{R} \,, \\
        \bm{f} &: \mathbb{R} \times \mathbb{R}^{n_x} \times \mathbb{R}^{n_u} &&\mapsto \mathbb{R}^{n_x} \,, \\
        \bm{g} &: \mathbb{R} \times \mathbb{R}^{n_x} \times \mathbb{R}^{n_u} &&\mapsto \mathbb{R}^{n_g} \,, \\
        \bm{\phi}_0 &: \mathbb{R} \times \mathbb{R}^{n_x} &&\mapsto \mathbb{R}^{n_{\phi,0}} \,, \\
        \bm{\phi}_f &: \mathbb{R} \times \mathbb{R}^{n_x} &&\mapsto \mathbb{R}^{n_{\phi,f}} \,,
    \end{alignat}
where the integers $n_x$, $n_u$, $n_g$, $n_{\phi,0}$ and $n_{\phi,f}$ are non-negative and serve as arbitrary dimension specifiers.
\end{prob}

\begin{defn} \label{defn:dual-cone}
    Consider a cone $K \subseteq \mathbb{R}^{n_k}$ for some integer $n_k$. The \emph{dual cone} of $K$ is such that, see \cite{Boyd2004},
    \begin{equation}
        K^{\bm{\star}} = \{ \bm{y} \in \mathbb{R}^{n_k} : \mathbf{v}^\intercal \bm{y} \geq 0, \: \forall \mathbf{v} \in K \} \,.
    \end{equation}
\end{defn}

By introducing $\bm{\lambda}(t) \in \mathbb{R}^{n_x}$, hereinafter referred to as \emph{costate vector},
as well as 
$\bm{\mu}(t) \in \mathbb{R}^{n_g}$, 
$\bm{\nu}_0 \in \mathbb{R}^{n_{\phi,0}}$ and 
$\bm{\nu}_f \in \mathbb{R}^{n_{\phi,f}}$, 
one may define the Hamiltonian as:
\begin{multline} \label{eq:hamiltonian}
    \mathcal{H}( t, \bm{x}, \bm{u}, \bm{\lambda}, \bm{\mu}) = h ( t, \bm{x}, \bm{u} ) + \bm{\lambda}^\intercal \bm{f}( t, \bm{x}, \bm{u} ) \\
    + \bm{\mu}^\intercal \bm{g}( t, \bm{x}, \bm{u} ) \,,
\end{multline}
such that, in addition to \eqref{eq:path-constraints} and \eqref{eq:endpoint-constraints}, the solution to Problem \ref{prob:optimal-control} satisfies the following conditions
\begin{align}
    \nabla_{\bm{\lambda}} \mathcal{H}( t, \bm{x}, \bm{u}, \bm{\lambda}, \bm{\mu} ) - \dot{\bm{x}}^\intercal &= \bm{0}^\intercal \,, \label{eq:condition-lambda} \\
    \nabla_{\bm{x}} \mathcal{H}( t, \bm{x}, \bm{u}, \bm{\lambda}, \bm{\mu} ) + \dot{\bm{\lambda}}^\intercal &= \bm{0}^\intercal \,, \label{eq:condition-state} \\
    \nabla_{\bm{u}} \mathcal{H}( t, \bm{x}, \bm{u}, \bm{\lambda}, \bm{\mu} ) &= \bm{0}^\intercal \label{eq:condition-control} \,, \\
    \bm{\mu}^\intercal \bm{g}( t, \bm{x}, \bm{u} ) &= 0 \,, \label{eq:condition-slack-g} \\ 
    \bm{\nu}_0^\intercal \bm{\phi}_0 \bigl(t_0, \bm{x}(t_0) \bigr) &= 0 \,, \label{eq:condition-slack-nu-0} \\ 
    \bm{\nu}_f^\intercal \bm{\phi}_f \bigl(t_f, \bm{x}(t_f) \bigr) &= 0 \,, \label{eq:condition-slack-nu-f} \\
    \bm{\mu}(t) &\succeq_{\mathcal{G}^{\bm{\star}}} \bm{0} \\
    \bm{\nu}_0 &\succeq_{\Phi_0^{\bm{\star}}} \bm{0} \\
    \bm{\nu}_f &\succeq_{\Phi_f^{\bm{\star}}} \bm{0} \\
    \nabla_{\bm{x}} \Psi_0 \bigl(t_0, \bm{x}(t_0) \bigr) + \bm{\nu}_0^\intercal \nabla_{\bm{x}} \bm{\phi}_0 \bigl(t_0, \bm{x}(t_0) \bigr) &= -\bm{\lambda}^\intercal (t_0) \,, \label{eq:condition-state-0} \\
    \nabla_{\bm{x}} \Psi_f \bigl(t_f, \bm{x}(t_f) \bigr) + \bm{\nu}_f^\intercal \nabla_{\bm{x}} \bm{\phi}_f \bigl(t_f, \bm{x}(t_f) \bigr) &= \bm{\lambda}^\intercal (t_f) \,, \label{eq:condition-state-f} \\ 
    \diffp{\Psi_0}{t_0} \bigl(t_0, \bm{x}(t_0) \bigr) + \bm{\nu}_0^\intercal \diffp{\bm{\phi}_0}{t_0} \bigl(t_0, \bm{x}(t_0) \bigr) &= \mathcal{H}(t_0) \,,  \label{eq:condition-t0}\\
    \diffp{\Psi_f}{t_f} \bigl(t_f, \bm{x}(t_f) \bigr) + \bm{\nu}_f^\intercal \diffp{\bm{\phi}_f}{t_f} \bigl(t_f, \bm{x}(t_f) \bigr) &= - \mathcal{H}(t_f) \,, \label{eq:condition-tf}
\end{align}
where the operator $\nabla_{\mathbf{x}}$ yields the row vector of partial derivatives of the associated function with respect to the components of vector $\mathbf{x}$, and the short-hand notation $\mathcal{H}(t_0)$ stands for $\mathcal{H} \bigl( t_0, \bm{x}(t_0), \bm{u}(t_0), \bm{\lambda}(t_0), \bm{\mu}(t_0) \bigr)$. The analogous is true for $\mathcal{H}(t_f)$.
Furthermore, $\mathcal{G}^{\bm{\star}}$, $\Phi_0^{\bm{\star}}$ and $\Phi_f^{\bm{\star}}$ denote the dual cones of $\mathcal{G}$, $\Phi_0$ and $\Phi_f$, respectively, see Definition \ref{defn:dual-cone}.
In equations \eqref{eq:hamiltonian}, \eqref{eq:condition-lambda}, \eqref{eq:condition-state}, \eqref{eq:condition-control} and \eqref{eq:condition-slack-g} the parametrization of variables $\bm{x}(t)$, $\bm{u}(t)$, $\bm{\lambda}(t)$ and $\bm{\mu}(t)$ with respect to $t$ has been omitted for simplicity of notation.
Equations \eqref{eq:condition-lambda} through \eqref{eq:condition-tf} are called \emph{first order optimality conditions}.
For the formulation of the Hamiltonian and the derivation of optimality conditions see, \emph{e.g.}, \cite{Bryson1975,Kirk2004,Boyd2004}.

\subsection{Algebraic transcription and constraint enforcement}
To employ the new method we discretize Problem \ref{prob:optimal-control} according to the following strategy.
\begin{description}
    \item[State Variables]{Sampled according to $\mathcal{T}_N \cup \{\tau_\xid\}$, with $\tau_\xid$ given by Corollary \ref{cor:final}. Forming a set of $N+1$ points that are indexed with $\setS$.}
    \item[Control Variables]{Sampled according to $\mathcal{T}_N$. Forming a set of $N$ points that are indexed with $\setC$.}
\end{description}
Please recall that $\mathcal{T}_N$ is the set of roots of the Lobatto polynomial defined in \eqref{eq:set-lobatto-roots}, and that the index sets $\setS$ and $\setC$ are defined in \eqref{eq:set-s} and \eqref{eq:set-c}, respectively.

Note that by discretizing the problem we reduce the dimensionality of the search space to a finite set, therefore, there is always an intrinsic error present between discrete and analytic variables.
Regardless, for the sake of simplicity and intuitiveness, we shall use identical variable naming for respective quantities.
% Note that by introducing discretization, the solution obtained will have an intrinsic error. Regardless, for simplicity, we use identical notation for the decision variables \lambda, \mu, x, u.
% In order to construct a finite dimensional problem that can be solved with the new method we discretize Problem \ref{prob:optimal-control} according to the following strategy.


Given the domain of interest $\closedinterval{t_0}{t_f}$ and the normalized domain $\tau \in \closedinterval{-1}{1}$, a \emph{one-to-one} relationship can be established through the linear mapping $t : \closedinterval{-1}{1} \mapsto \closedinterval{t_0}{t_f}$, defined as:
\begin{equation} \label{eq:domain-mapping}
    t(\tau) = \frac{t_f-t_0}{2} \tau + \frac{t_f+t_0}{2} \,.
\end{equation}

% Algebraic path constraints.
In this way, we are able to enforce the path constraints specified in \eqref{eq:path-constraints} as:
\begin{equation} \label{eq:discrete-path-constraints}
    \bm{g}( t_k, \bm{x}_k, \bm{u}_k ) \preceq_{\mathcal{G}} \bm{0} \,, \quad k \in \setC \,,
\end{equation}
where $t_k = t( \tau_k )$ according to \eqref{eq:domain-mapping}, $\bm{x}_k$ and $\bm{u}_k$ denote $\bm{x}(t_k)$ and $\bm{u}(t_k)$, respectively, and $\tau_k \in \mathcal{T}_N$ with $k \in \setC$.

% Definite integral
Moreover, recalling the Gaussian quadrature from \eqref{eq:gauss-quadrature} and the domain mapping in \eqref{eq:domain-mapping}, one may approximate the definite integral of Problem \ref{prob:optimal-control} with the following algebraic form, see, \emph{e.g.}, \cite{Garg2011,Patterson2014,Garrido2021,GarridoMScThesis}:
\begin{equation} \label{eq:algebraic-integral}
    \int_{t_0}^{t_f} h \bigl(t, \bm{x}(t), \bm{u}(t) \bigr) \dl{t} \approx \frac{t_f - t_0}{2} \sum_{k \in \setC} w_k h( t_k, \bm{x}_k, \bm{u}_k ) \,.
\end{equation}

% Equations of motion
Finally, the differential constraints expressed in \eqref{eq:dynamics} are enforced with the following algebraic construction: 
\begin{equation} \label{eq:diff-scheme}
    \bm{f}(t_k, \bm{x}_k, \bm{u}_k) - \frac{2}{t_f-t_0} \sum_{i \in \setS} \bm{D}_{ki} \bm{x}_i = \bm{0} \,, \quad k \in \setC \,,
\end{equation}
where $\bm{D}$ denotes the differentiation matrix of the new method, see Theorem \ref{thm:new-matrix-is-inversion-ready}.


\subsection{Karush–Kuhn–Tucker conditions}
In this section, let $1$ and $N$ be the indices associated with the initial and final domain endpoints, respectively.
In this way, we may define a \emph{KKT Hamiltonian} as:
\begin{multline} \label{eq:kkt-hamiltonian}
    \tilde{H}( t_0, t_f, \bm{x}_k, \bm{u}_k, \bm{\lambda}_k, \bm{\mu}_k, \bm{\nu}_0, \bm{\nu}_f ) = \\
    \tfrac{t_f-t_0}{2} w_k \big(
    h_k
    + \bm{\lambda}_k^\intercal \bm{f}_k 
    + \bm{\mu}_k^\intercal \bm{g}_k 
    \big) \\
    + \big( \Psi_0(t_0, \bm{x}_1) + \bm{\nu}_0^\intercal \bm{\phi}_0(t_0, \bm{x}_1) \big) \delta_{1k} \\
    + \big( \Psi_f(t_f, \bm{x}_N) + \bm{\nu}_f^\intercal \bm{\phi}_f(t_f, \bm{x}_N) \big) \delta_{Nk} \,,
\end{multline}
where
$\bm{\lambda}_k$ and $\bm{\mu}_k$ denote $\bm{\lambda}(t_k)$ and $\bm{\mu}(t_k)$, respectively, with $k \in \setC$.
Moreover, $\delta_{ik}$ denotes the Kronecker delta defined in \eqref{eq:lagrange-kronecker-delta},
and the short-hand $\mathbf{f}_k = \mathbf{f}\bigl( t_k, \bm{x}(t_k), \bm{u}(t_k) \bigr)$ is used for functions $h$, $\bm{f}$ and $\bm{g}$ of Problem \ref{prob:optimal-control}.

To simplify notation, we introduce the following short-hand for the KKT Hamiltonian function:
\begin{equation}
    \tilde{H}_k = \tilde{H}( t_0, t_f, \bm{x}_k, \bm{u}_k, \bm{\lambda}_k, \bm{\mu}_k, \bm{\nu}_0, \bm{\nu}_f ) \,,
\end{equation}
where the initial and final domain endpoints, $t_0$ and $t_f$, as well as the multipliers, $\bm{\nu}_0$ and $\bm{\nu}_f$, do not change for $k \in \setC$.

Using the KKT Hamiltonian, we construct an \emph{augmented cost function} as follows:
\begin{multline} \label{eq:augmented-cost}
    J( t_0, t_f, \bm{x}_j, \bm{u}_m, \bm{\lambda}_m, \bm{\mu}_m, \bm{\nu}_0, \bm{\nu}_f ) = \\
    \sum_{k \in \setC} \Bigl\{ \tilde{H}_k - w_k \bm{\lambda}_k^\intercal \sum_{i \in \setS} \bm{D}_{ki} \bm{x}_i \Bigr\}\,, \\ \quad j \in \setS, \: m \in \setC,
\end{multline}
In this way, the augmented cost in \eqref{eq:augmented-cost} is analogous to the Lagrangian function in the context of constrained parametric optimization with respect to the transcription strategy expressed in \eqref{eq:algebraic-integral}, \eqref{eq:discrete-path-constraints}, and \eqref{eq:diff-scheme}, see, \emph{e.g.}, \cite{Bertsekas2016,Boyd2004,Garg2011}.

As an intermediate step towards the KKT conditions, we highlight the gradient of the augmented cost with respect to the discrete state variables below:
\begin{equation} \label{eq:kkt-x-intermediate}
    \nabla_{\bm{x}_j} J(\cdot) = \sum_{k \in \setC} \bigl\{ \nabla_{\bm{x}_k} \tilde{H}_k \delta_{kj} - w_k \bm{\lambda}_k^\intercal \bm{D}_{kj} \bigr\} \,, \quad j \in \setS \,.
\end{equation}
Equation \eqref{eq:kkt-x-intermediate} exhibits a \emph{left} multiplication between the costate samples, $\bm{\lambda}_k$ with $k \in \setC$, and the differentiation matrix $\bm{D}$.
To convert this into a \emph{right} multiplication and obtain a linear combination in column space, we define an auxiliary matrix $\dualD \in \mathbb{R}^{N \times N}$ as:
\begin{equation} \label{eq:dual-matrix}
    \dualD_{ki} = \frac{\delta_{ki}}{w_k}( \delta_{Nk} - \delta_{1k} ) - \frac{w_i}{w_k} \bm{D}_{ik} \,, \quad k, i \in \setC\,,
\end{equation}
where $w_k$ are the quadrature weights obtained from \eqref{eq:weights}.
In this way, the KKT conditions, which are obtained by setting the gradient of $J(\cdot)$ with respect to each variable to zero, are expressed as follows:
\begin{align}
    \nabla_{\bm{\lambda}_k} \tilde{H}_k - w_k \sum_{i \in \setS} \bm{D}_{ki} \bm{x}_i^\intercal &= \bm{0}^\intercal \,, \label{eq:kkt-l} \\
    \nabla_{\bm{x}_k} \tilde{H}_k + w_k \sum_{i \in \setC} \dualD_{ki} \bm{\lambda}_i^\intercal &= \bm{\lambda}_N^\intercal \delta_{Nk} - \bm{\lambda}_1^\intercal \delta_{1k} \,, \label{eq:kkt-x} \\
    \sum_{i \in \setC} w_i \bm{\lambda}_i \bm{D}_{i\xid} &= \bm{0} \,, \label{eq:kkt-x-sigma} \\
    \nabla_{\bm{u}_k} \tilde{H}_k &= \bm{0}^\intercal \,, \label{eq:kkt-u} \\
    \bm{\mu}_k^\intercal \bm{g}_k &= 0 \,, \\ 
    \bm{\nu}_0^\intercal \bm{\phi}_0 (t_0, \bm{x}_1 ) &= 0 \,, \\ 
    \bm{\nu}_f^\intercal \bm{\phi}_f (t_f, \bm{x}_N ) &= 0 \,, \\
    \sum_{i \in \setC} \diffp{\tilde{H}_i}{t_0} = \sum_{i \in \setC} \diffp{\tilde{H}_i}{t_f} &= 0 \,,
\end{align}
with $k \in \setC$, see, \emph{e.g.}, \cite{Ross2003,Benson2006,Fahroo2008,Garg2011}.
Note that \eqref{eq:kkt-x} is a consequence of \eqref{eq:kkt-x-intermediate} for $j \in \setC$.
Similarly, \eqref{eq:kkt-x-sigma} is a consequence of \eqref{eq:kkt-x-intermediate} for $j = \xid$.

\begin{prop} \label{prop:costate-degree}
    Condition \eqref{eq:kkt-x-sigma} can be satisfied if the discrete samples of the costate variables, $\bm{\lambda}_i$ with $i \in \setC$, are interpolatable with a polynomial of degree no greater than $N-2$.
\end{prop}
\begin{proof}[Proof of Theorem \ref{prop:costate-degree}]
    If the samples of the costate variables, $\bm{\lambda}_i$ with $i \in \setC$, are interpolatable with a polynomial $\bm{\lambda}(\tau)$ of degree no greater than $N-2$, the Gauss-Lobatto quadrature rule imposed in condition \eqref{eq:kkt-x-sigma} is exact. Therefore, the following holds:
    \begin{equation}
        \sum_{i \in \setC} w_i \bm{\lambda}_i \bm{D}_{i\xid} = \int_{-1}^{1} \bm{\lambda}(\tau) \dot{l}_{\xid} (\tau) \dl \tau \,.
    \end{equation}
    Integrating by parts and rewriting the integral term with a Gaussian quadrature rule yields
    \begin{equation}
        \bm{\lambda}(\tau_N) l_{\xid} (\tau_N) - \bm{\lambda}(\tau_1) l_{\xid} (\tau_1) - \sum_{i \in \setC} w_i \dot{\bm{\lambda}}_i l_{\xid}(\tau_i) \,,
    \end{equation}
    which is identically zero due to \eqref{eq:lagrange-kronecker-delta}. Therefore, condition \eqref{eq:kkt-x-sigma} is satisfied.
\end{proof}

\begin{thm} \label{thm:dual-matrix-is-diff-matrix}
    Matrix $\dualD \in \mathbb{R}^{N\times N}$ specified in \eqref{eq:dual-matrix} is a differentiation matrix over the set of abscissas $\mathcal{T}_N$ with order of accuracy $N-2$, according to Definition \ref{defn:differentiation-matrix}.
\end{thm}
\begin{proof}[Proof of Theorem \ref{thm:dual-matrix-is-diff-matrix}]
    Consider a polynomial $p(\tau)$ of degree $N_p$, with $\tau \in \closedinterval{-1}{1}$ sampled at the nodes $\tau_i \in \mathcal{T}_N$, with $i \in \setC$. We apply the matrix $\dualD$ to the samples $p(\tau_i)$, as follows:
    \begin{multline} \label{eq:dual-D-with-poly}
        \sum_{i \in \setC} \dualD_{ki} p(\tau_i) = \frac{\delta_{Nk} - \delta_{1k}}{w_k} \sum_{i \in \setC} \delta_{ki} p(\tau_i) \\ 
        - \frac{1}{w_k} \sum_{i \in \setC} w_i \dot{l}_k (\tau_i) p(\tau_i)
    \end{multline}
    where $l_k(\cdot)$ denotes the Lagrange interpolating polynomial of index $k$ defined in \eqref{eq:poly-interp}. As a result, the polynomial $\dot{l}_k(\cdot)$ has degree $N-1$. In this context, let $N_p$ be such that $(N-1) + N_p \leq 2N - 3$ leading to $N_p \leq N - 2$, then the Gaussian quadrature rule in \eqref{eq:dual-D-with-poly} is exact, allowing us to write the right-hand-side of \eqref{eq:dual-D-with-poly} as:
    \begin{equation}
         \frac{\delta_{Nk} - \delta_{1k}}{w_k} p(\tau_k) - \frac{1}{w_k} \int_{-1}^{1} \dot{l}_k (\tau) p(\tau) \dl \tau \,.
    \end{equation}
    Performing integration by parts yields
    \begin{multline}
        \frac{\delta_{Nk} - \delta_{1k}}{w_k} p(\tau_k) +
        \frac{1}{w_k} \int_{-1}^{1} l_k (\tau) \dot{p}(\tau) \dl \tau \\
         - \frac{1}{w_k}\bigl[ l_k (\tau_N) p(\tau_N) -  l_k (\tau_1) p(\tau_1)\bigr] \,.
    \end{multline}
    From \eqref{eq:lagrange-kronecker-delta}, it is apparent that the endpoint terms vanish if $k \notin \{ 1,\, N \}$, and that these terms cancel-out if $k \in \{ 1, \, N \}$.
    Furthermore, from Lemma \ref{lem:lagrange-inner-prod}, the integral term can be simplified, yielding
    \begin{equation}
            \sum_{i \in \setC} \dualD_{ki} p(\tau_i) = \frac{1}{w_k} w_k \dot{p}(\tau_k) = \dot{p}(\tau_k) \,.
    \end{equation}
\end{proof}

% \begin{rem}
%     Due to Theorem \ref{thm:dual-matrix-is-diff-matrix}, disregarding the points at the boundaries, the condition \eqref{eq:kkt-x} is the direct algebraic transcription of the costate equation expressed in \eqref{eq:condition-state}.
% \end{rem}

\subsection{Covector mapping}
It has been shown that there is a one-to-one correspondence between the discrete dual variables of the optimal control problem (with respect to which the KKT conditions above have been written) and the Lagrange multipliers that are obtained from the solver in use, see, \emph{e.g.}, \cite{Ross2001,Ross2003,Fahroo2008,Garg2011,Benson2006,GarridoMScThesis}. For the augmented cost in \eqref{eq:augmented-cost} and the KKT Hamiltonian definition in \eqref{eq:kkt-hamiltonian}, this mapping is given as:
\begin{align}
    \bm{\lambda}_k &= \frac{2}{w_k(t_f - t_0)} \tilde{\bm{\lambda}}_k \,, \quad k \in \setC \\
    \bm{\mu}_k &= \frac{2}{w_k(t_f - t_0)} \tilde{\bm{\mu}}_k \,, \quad k \in \setC \\
    \bm{\nu}_0 &= \tilde{\bm{\nu}}_0 \\
    \bm{\nu}_f &= \tilde{\bm{\nu}}_f
\end{align}
where $\tilde{\bm{\lambda}}_k$, $\tilde{\bm{\mu}}_k$, $\tilde{\bm{\nu}}_0$, and $\tilde{\bm{\nu}}_f$ denote the Lagrange multipliers which are respectively associated with the differential constraints, path constraints and initial and terminal endpoint constraints.

\section{Numerical results}
In this section we implement the new method for two classical optimal control problems.
These are the low-thrust orbit raising problem \cite{Fahroo2008,Garg2011,Bryson1975} and the scalar nonlinear initial value problem \cite{Garg2011}, the later of which has a known analytic solution.
The complex step method for partial derivates is used with a step size of \num{1e-12}, see, \emph{e.g.}, \cite{Alonso2003}, and the NLP solver IPOPT \cite{Waechter2005} was employed.

Table \ref{tab:ipopt-options} shows the solver settings used for all numerical results that follow.
The tolerance values shown in Table~\ref{tab:ipopt-options} directly enforce the precision with which the KKT conditions are satisfied.
\begin{table}[bt!]
    \centering
    \noindent
    \caption{List of IPOPT options used for the benchmark problems.}
    \label{tab:ipopt-options}
    \vspace{1mm}
    \begin{tabular}{lc}
        \toprule
        Option & Value \\ \midrule
        \texttt{tol} & \num{1e-9} \\
        \texttt{dual\_inf\_tol} & \num{1e-9} \\
        \texttt{acceptable\_tol} & \num{1e-9} \\
        \texttt{max\_iter} & \num{1e3} \\
        \texttt{hessian\_approximation} & \texttt{'limited-memory'} \\
        \texttt{linear\_solver} & \texttt{'mumps'} \\
        \bottomrule
    \end{tabular}
\end{table}

\input{orbit-raising}
\input{convergence}

