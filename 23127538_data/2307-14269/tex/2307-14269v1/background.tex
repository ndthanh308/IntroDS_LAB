
\section{Preliminaries}
To obtain an adequate discretization scheme, this paper hinges on the concept of a \emph{differentiation matrix}. This motivates the following definition.
\begin{defn} \label{defn:differentiation-matrix}
    Let $\mathcal{A} = \{\tau_i \in \mathbb{R}: i=1,2,\dots,M \}$ and $\mathcal{B} = \{\tau_j \in \mathbb{R}: j=1,2,\dots,N \}$ be sets of abscissas such that $\mathcal{B} \subseteq \mathcal{A}$, then a matrix $\bm{D} \in \mathbb{R}^{N \times M}$ is called a \emph{first derivative differentiation matrix} over the set $\mathcal{A}$ if it satisfies
    \begin{equation} \label{eq:diff-defn}
        \bm{D} \bm{V} = \bm{V'}
    \end{equation}
where, for some integer $R$ such that $1 \leq R \leq M-1$, $\bm{V} \in \mathbb{R}^{M \times (R+1)}$ denotes a Vandermonde matrix over the abscissas of set $\mathcal{A}$, see, \emph{e.g.}, \cite{Trefethen1997}, and $\bm{V'} \in \mathbb{R}^{N \times (R+1)}$ denotes the respective matrix of first derivatives over the abscissas of set $\mathcal{B}$. 
    If \eqref{eq:diff-defn} holds for an integer $R$ but it does not hold for $R+1$, then $R$ denotes \emph{the order of accuracy of the method}.
    Matrices $\bm{V}$ and $\bm{V'}$ are defined, respectively, as
    \begin{align}
        \bm{V} &= \begin{bsmallmatrix}
            1 & \tau_1 & \tau_1^2 & \displaystyle \cdots & \tau_1^R \\
            1 & \tau_2 & \tau_2^2 & \displaystyle \cdots & \tau_2^R \\
            \vdots & \vdots & \vdots & \ddots & \vphantom{\int\limits^x}\vdots \\
            1 & \tau_M & \tau_M^2 & \displaystyle \cdots & \tau_M^R
        \end{bsmallmatrix}, \quad \tau_i \in \mathcal{A} \\[2ex]
        \bm{V'} &=  \begin{bsmallmatrix}
            0 & 1 & 2\tau_1  & \displaystyle \cdots & R\tau_1^{R-1} \\
            0 & 1 & 2\tau_2  & \displaystyle \cdots & R\tau_2^{R-1} \\
            \vdots & \vdots & \vdots & \ddots & \vphantom{\int\limits^x}\vdots \\
            0 & 1 & 2\tau_N  & \displaystyle \cdots & R\tau_N^{R-1}
        \end{bsmallmatrix}, \quad \tau_j \in \mathcal{B}
    \end{align}
\end{defn}

\begin{lem} \label{lem:diff-have-null-space}
    Let $\bm{D} \in \mathbb{R}^{N\times M}$ be some differentiation matrix according to Definition \ref{defn:differentiation-matrix}, then the vector $\bm{1} \in \{1\}^{M \times 1}$ is a member of the null-space of $\bm{D}$.
\end{lem}
\begin{proof}[Proof of Lemma \ref{lem:diff-have-null-space}]
    From Definition \ref{defn:differentiation-matrix} we have, necessarily, $\bm{D} \bm{1} = \bm{0}$, with $\bm{1} \in \{1\}^{M\times 1}$ and $\bm{0} \in \{0\}^{N\times 1}$.
    This implies that there exists at least one vector $\bm{z} \in \mathbb{R}^{M\times 1}$ such that $\bm{D} \bm{z} = \bm{0}$ with $\bm{z} \neq \bm{0}$.
\end{proof}

\begin{lem} \label{lem:rank-bounds}
    The rank of a differentiation matrix $\bm{D} \in \mathbb{R}^{N\times M}$ is bounded as follows:
    \begin{equation}
        \min(N, R) \leq \texttt{\upshape Rank}(\bm{D}) \leq \min( N, M-1 )\,,
    \end{equation}
    where $R$ denotes the order of accuracy of the particular method according to Definition \ref{defn:differentiation-matrix}.
\end{lem}
\begin{proof}[Proof of Lemma \ref{lem:rank-bounds}]
    From Definition \ref{defn:differentiation-matrix}, a lower bound is established from the rank of matrix $\bm{V'}$:
    \begin{equation}
        \texttt{\upshape Rank}(\bm{V'}) = \min(N, R) \leq \texttt{\upshape Rank}(\bm{D}) \,,
    \end{equation}
    where $R$ denotes the order of accuracy of the particular method of differentiation.
    An upper bound can be established from the dimensions of matrix $\bm{D}$ itself, as $\texttt{\upshape Rank}(\bm{D}) \leq \min( N, M )$.
    This boundary is narrowed further by subtracting to the number of columns the value $1$, which is the minimum dimension of the null-space of $\bm{D}$, see Lemma \ref{lem:diff-have-null-space} and \cite[Theorem 2.3]{Friedberg2003}.
\end{proof}

The class of matrices specified in Definition \ref{defn:differentiation-matrix} allows the computation of total derivates of some continuous quantity given $M$ observations of the quantity itself. We now consider the inverse problematic: obtain the sequence of values from observations of the derivates.
\begin{prob} \label{prob:discrete-initial-value-problem}
    Let $y: \mathbb{R} \mapsto \mathbb{R}$ be a continuous function, furthermore, let $\bm{y} \in \mathbb{R}^{M\times 1}$ be a discrete sequence of values $y_i$ associated with a sequence of samples of $\tau_i \in \mathbb{R}$ with $i=1,2,\dots,M$, as follows:
    \begin{equation}
    \begin{split}
        \bm{y} &= \begin{bmatrix}
            y_1 & y_2 & \dots & y_M
        \end{bmatrix}^\intercal \\
        &= \begin{bmatrix}
            y(\tau_1) & y(\tau_2) & \dots & y(\tau_M)
        \end{bmatrix}^\intercal \,,
    \end{split}
    \end{equation}
    where $^\intercal$ denotes the \emph{transpose} operator.
    Determine the sequence $\bm{y}$ such that
\begin{equation} \label{eq:system-derivative-observations}
   \begin{bmatrix}
       \bm{D} \\ \bm{c}^\intercal
   \end{bmatrix}
   \bm{y} = \begin{bmatrix}
       \bm{d} \\ b
   \end{bmatrix} \,,
\end{equation}
where $\bm{D} \in \mathbb{R}^{N \times M}$ is a differentiation matrix according to Definition \ref{defn:differentiation-matrix}, $\bm{c} \in \mathbb{R}^{M \times 1}$ is a coefficient vector such that $\bm{c}^\intercal \bm{1} \neq 0$, \emph{i.e.}, $\bm{c}$ spans the null-space of $\bm{D}$, $\bm{d} \in \mathbb{R}^{N\times 1}$ is a sequence of observations of the derivative of $y(\tau)$ with respect to $\tau$ at $N$ locations, and $b \in \mathbb{R}$ is an arbitrary bias specification. 
\end{prob}

\begin{rem} \label{lem:diff-requirement}
    The linear system of Problem \ref{prob:discrete-initial-value-problem} has a unique solution if and only if matrix $\bm{D}$ consists of $N$ rows and $N+1$ columns, and if this matrix is full-rank.
\end{rem}
% \begin{proof}[Proof of Lemma \ref{lem:diff-requirement}]
%     Let $\bm{A} \in \mathbb{R}^{(N+1)\times M}$ be the compound coefficient matrix of the linear system of Problem \ref{prob:discrete-initial-value-problem}, such that $\bm{A}^\intercal = \begin{bmatrix} \bm{D}^\intercal & \bm{c} \end{bmatrix}$.
%     The system has a unique solution if and only if $\bm{A}$ is square and invertible. In this regard, $N+1 = M$ is necessary such that $\bm{A}$ is square, therefore if $\bm{D}$ has $N$ rows it must have $N+1$ columns. Furthermore, $\bm{A}$ is invertible if and only if its rows, therefore the rows of $\bm{D}$, are linearly independent, thus $\texttt{Rank}(\bm{D}) = N$ is necessary.
% \end{proof}

Remark \ref{lem:diff-requirement} motivates the following definition.

\begin{defn} \label{defn:inversion-ready}
    Let $\bm{D} \in \mathbb{R}^{N \times (N+1)}$ be a differentiation matrix according to Definition \ref{defn:differentiation-matrix} over some set of abscissas $\{\tau_i \in \mathbb{R} : i = 1,2,\ldots, N+1\}$ and for some order of accuracy $R$.
    Matrix $\bm{D}$ is an \emph{inversion-ready differentiation matrix} if $\texttt{Rank}(\bm{D}) = N$.
\end{defn}

Definition \ref{defn:inversion-ready} establishes a class of matrices which can be used to solve Problem \ref{prob:discrete-initial-value-problem}.

