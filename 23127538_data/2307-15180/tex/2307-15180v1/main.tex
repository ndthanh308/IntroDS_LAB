% \documentclass{uai2023} % for initial submission
\documentclass[accepted]{uai2023} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2023} % ptmx math instead of Computer
                                         % Modern (has noticable issues)
% \documentclass[mathfont=newtx]{uai2023} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
\bibliographystyle{plainnat}
\renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{xcolor}

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example
\newcommand{\vcuong}[1]{{\textcolor{red}{#1}}}
\newcommand{\minisection}[1]{\vspace{2mm}\noindent{\textbf{#1}}}
% \newcommand{\minisection}[1]{\noindent{\textbf{#1}}}
\DeclareMathOperator*{\argmax}{argmax}

\title{EnSolver: Uncertainty-Aware CAPTCHA Solver Using Deep Ensembles}

% The standard author block has changed for UAI 2023 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author{Duc C.~Hoang}
\author{Cuong V.~Nguyen}
\author{Amin~Kharraz}
% \author[1,2]{Further~Coauthor}
% \author[3]{Further~Coauthor}
% \author[1]{Further~Coauthor}
% \author[3]{Further~Coauthor}
% \author[3,1]{Further~Coauthor}
% Add affiliations after the authors
\affil{%
    Knight Foundation School of Computing and Information Sciences\\
    
    Florida International University\\
    
    Miami, Florida, USA
}
% \affil[2]{%
%     Second Affiliation\\
%     Address\\
%     …
% }
% \affil[3]{%
%     Another Affiliation\\
%     Address\\
%     …
%   }
  
\begin{document}
\maketitle

\begin{abstract}
The popularity of text-based CAPTCHA as a security mechanism to protect websites from automated bots has prompted researches in CAPTCHA solvers, with the aim of understanding its failure cases and subsequently making CAPTCHAs more secure. Recently proposed solvers, built on advances in deep learning, are able to crack even the very challenging CAPTCHAs with high accuracy. However, these solvers often perform poorly on out-of-distribution samples that contain visual features different from those in the training set. Furthermore, they lack the ability to detect and avoid such samples, making them susceptible to being locked out by defense systems after a certain number of failed attempts. In this paper, we propose EnSolver, a novel CAPTCHA solver that utilizes deep ensemble uncertainty estimation to detect and skip out-of-distribution CAPTCHAs, making it harder to be detected. We demonstrate the use of our solver with object detection models and show empirically that it performs well on both in-distribution and out-of-distribution data, achieving up to 98.1\% accuracy when detecting out-of-distribution data and up to 93\% success rate when solving in-distribution CAPTCHAs. Source code for this paper is available at \url{https://github.com/HoangCongDuc/ensolver.git}.
\end{abstract}
\section{Introduction}
Automated web bots are getting increasingly more sophisticated in imitating human behaviors and evading detection. In most important and consequential situations, these adversarial operations can result in credential stuffing, account hijacking and data breaches, vulnerability scanning and exploitations. 
Defending against automated web bots has never been trivial. In addition to behavioral analysis~\citep{cloudflare2, datadome1} and fingerprinting techniques~\citep{laperdrix2020browser, NikiforakisKJKPV13}, 
CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) \citep{von2003captcha} has been a common security mechanism to defend against malicious automatic bot activities. That is, the website generates a CAPTCHA challenge and requests the remote agent to solve the CAPTCHA. The core insight is that solving a CAPTCHA is straightforward for real users but difficult for automated bots. Among different types of CAPTCHAs, text-based CAPTCHAs~\citep{gao2016robustness} present users with a noisy image of a short text string and ask them to enter the correct string. They are among the most popular types of CAPTCHAs due to simple implementation and user-friendliness~\citep{deng3e}.

In this adversarial landscape, as text-based CAPTCHAs became more popular, many automatic solving techniques have also been developed to evade detection. These techniques are interesting from the security viewpoint as they help researchers understand the weaknesses of these CAPTCHAs and subsequently make them more secure~\citep{tang2018research}. With the recent advances in deep learning (DL) and computer vision, state-of-the-art (SotA) text-based CAPTCHA solvers often employ an end-to-end approach, where a sophisticated DL model will predict the output text string directly from the raw pixels of the input image~\citep{deng3e, noury2020deep}. Although these solvers can crack very challenging CAPTCHAs, they are unable to make correct predictions on out-of-distribution samples (i.e., images that are visually different from those in their training set). By exploiting this limitation, defense systems can detect these automatic CAPTCHA solvers and lock their access if a solver fails a certain number of attempts. Thus, from the attacker's perspective, it is desirable to equip the solvers with the ability to recognize and skip these out-of-distribution samples and request a new CAPTCHA to increase their success rate.

In this paper, we propose EnSolver (abbreviation for \underline{En}semble \underline{Solver}), a novel end-to-end text-based CAPTCHA solver that is capable to detect and skip out-of-distribution CAPTCHAs. EnSolver uses the deep ensembles technique~\citep{lakshminarayanan2017simple} to quantify its predictive uncertainty, subsequently deciding whether to answer a given CAPTCHA or to skip and request a new one. It uses object detectors~\citep{ge2021yolox} as base models in the ensemble to make predictions directly from an input image's pixel values without requiring any traditional image processing. Using deep ensembles, our solver computes a distribution over predicted text strings and uses this distribution to predict the final text string along with its uncertainty estimation. The solver then decides whether to skip the input image based on this quantity. To train and evaluate our solver, we also construct a new CAPTCHA dataset that contains bounding box labels, which are required to train object detection models.

We evaluate our solver's out-of-distribution detection accuracy and success rate on our new dataset and eight public CAPTCHA datasets~\citep{deng3e}. The experiment results show that our solver can detect out-of-distribution CAPTCHAs and avoid making wrong predictions with up to 98.1\% accuracy on average. At the same time, it can return correct predictions on in-distribution CAPTCHAs with up to 93\% success rate, which is on par with other SotA text-based CAPTCHA solvers~\citep{deng3e, li2021end, tian2020generic}.

\minisection{Contributions.} This paper makes the following contributions:
\begin{enumerate}
    \item We propose EnSolver, a novel uncertainty-aware CAPTCHA solver based on deep ensembles that can avoid making wrong predictions on out-of-distribution inputs.
    \item We demonstrate the use of this solver with object detectors as base models and build a new dataset to train such models.
    \item We show empirically that our solver achieve high success rates on both in-distribution and out-of-distribution datasets.
\end{enumerate}
% (1) We propose EnSolver, a novel uncertain-aware CAPTCHA solver based on deep ensembles. (2) We demonstrate the use of this solver with object detectors as base models and build a new dataset to train such models. (3) We show empirically that our solver can achieve very high success rates on both in-distribution and out-of-distribution datasets.

\section{Related Work}

\minisection{CAPTCHA Solvers.} Early solvers often use a segmentation-based approach consisting of two main stages: character segmentation and character recognition~\citep{chellapilla2004using, yan2007breaking, yan2008low}. \citet{yan2007breaking} used the color difference to localize characters in simple 2-color CAPTCHAs. Their subsequent work~\citep{yan2008low} used a histogram of foreground pixels to vertically segment the characters into chunks and then find the connected components to get individual characters. Once the characters are segmented, it is easy to recognize them individually using a neural network~\citep{chellapilla2004using, kopp2017breaking}. Since segmentation is a crucial stage in many solvers, a number of anti-segmentation features (e.g., character overlapping, hollow scheme, noise arcs, and complicated background) have been employed to make CAPTCHAs more resistant against such solvers~\citep{tang2018research}. To bypass these anti-segmentation features, various preprocessing techniques were then developed. For example, \citet{gao2016robustness} proposed using image binarization, noise arcs removal, and image rectification before segmentation, while \citet{ye2018yet} used an image-to-image translation model \citep{isola2017image} to turn a noisy input CAPTCHA into an easier image for the downstream solver. \citet{tian2020generic} used three generators networks to decompose an input CAPTCHA into a background and a character layer.

Recently, end-to-end solvers have been proposed that use a single DL model to solve a CAPTCHA directly from the input image without any segmentation. \citet{noury2020deep} proposed a CNN-based solver that has multiple character classification heads, each of which is responsible to predict a character in the CAPTCHA image. However, this model can only predict CAPTCHAs with fixed length. This problem was overcome by~\citet{tian2020generic} using a null character class. \citet{li2021end} used a convolutional recurrent neural network to train a solver on cycle-GAN generated data and then employed active transfer learning to optimize it on real schemes using a small labeled dataset. \citet{deng3e} proposed a semi-supervised solver based on the encoder-decoder architecture and attention mechanism which only requires small portion of the training dataset to be labeled.

\minisection{Uncertainty Estimation and Out-of-distribution Detection.} Our paper is related to uncertainty estimation and out-of-distribution detection~\citep{abdar2021review}. Here we review only the latest related work for DL models. Previous works~\citep{guo2017calibration, pereyra2017regularizing} have shown that modern DL models often do not provide well-calibrated uncertainty, in the sense that they tend to make incorrect predictions on out-of-distribution data with high confidence. Several lines of work have been proposed to improve uncertainty estimation for DL. For example, Bayesian neural networks~\citep{neal1995bayesian} provide a principled framework for studying uncertainty in DL. However, exact Bayesian inference is intractable for modern DL architectures and approximate inference is usually required~\citep{blundell2015weight, chen2014stochastic, gal2016dropout, ritter2018scalable, rudner2021tractable, zhang2020csgmcmc}. Popular approximate inference methods include variational inference~\citep{blundell2015weight, rudner2021tractable}, Markov chain Monte Carlo methods~\citep{chen2014stochastic, zhang2020csgmcmc}, and Laplace approximation~\citep{ritter2018scalable}. Besides Bayesian approaches, Monte Carlo dropout~\citep{gal2016dropout} and deep ensembles~\citep{lakshminarayanan2017simple} are also commonly used for DL uncertainty quantification. Once a good uncertainty estimation is obtained, it can be straightforwardly used for out-of-distribution detection~\citep{hendrycks2017baseline, lakshminarayanan2017simple}.

Among these methods, deep ensembles~\citep{lakshminarayanan2017simple} provide a simple way for uncertainty estimation that can be considered an approximation of Bayesian model averaging~\citep{wilson2020bayesian}. Its main idea is to train multiple neural networks (called base models) and use them for inference. Training a base model is the same as training an ordinary neural network and the model diversity is ensured by random parameter initialization and shuffling of the training dataset for each base model. This phenomenon was explained by~\citet{fort2019deep} using a loss landscape perspective. \citet{d2021repulsive} later proposed an improvement to deep ensembles by introducing a kernelized repulsive term in the update rule to ensure model diversity when the number of parameters is large and the effect of random initialization reduces.


\section{Uncertainty-Aware CAPTCHA Solver Using Deep Ensembles}
\label{sec:EnSolver}

In this section, we describe our proposed uncertainty-aware CAPTCHA solver in detail. This solver, named \textbf{EnSolver}, uses an ensemble of DL-based CAPTCHA solvers to make predictions along with the corresponding uncertainty estimates on input text-based CAPTCHA images. These uncertainty estimates allow EnSolver to detect inputs dissimilar to the training data (i.e., out-of-distribution inputs) and thus can ``skip'' inputs that are hard to crack. 
Detecting out-of-distribution CAPTCHAs is an important feature in the solving process because web applications often define policies on the number of failed attempts before locking an account or injecting a long delay before showing the next CAPTCHA. Consequently, if a trained solver is equipped with a pre-filtering mechanism that can effectively detect and skip CAPTCHAs that are not likely to be solved correctly, it can effectively bypass these account lockout policies and avoid triggering subsequent access failures used to lock out web sessions.

% Figure environment removed


\subsection{Uncertainty-Aware CAPTCHA Solver}

\begin{algorithm}[tb]
\caption{Uncertainty-aware CAPTCHA Solver}\label{alg:overall}
\begin{algorithmic}
    \Require Trained model $m$, input image $x$, threshold $\tau$
    \State $(y, u) \gets \Call{predict\_with\_uncertainty}{m, x}$
    \If{$u < \tau$}
        \State Predict with $y$
    \Else
        \State Skip $x$
    \EndIf
\end{algorithmic}
\end{algorithm}

For a conventional DL-based solver, a DL model $m$ is trained and then used to make a prediction $y = m(x)$ if given an input CAPTCHA image $x$. In this case, $y$ is the string that the model $m$ returns when the input image $x$ is fed into the model. Several types of DL models have been proposed for this problem that include generative adversarial networks~\citep{ye2018yet}, convolutional neural networks~\citep{noury2020deep}, and attention-based
encoder-decoder networks~\citep{deng3e}.

Although experimentally accurate, these deep learning-based solvers falter on images with visual characteristics (e.g., text font, character size) unseen in the training data. When encountering these out-of-distribution samples, it is better to not give an answer than to give wrong answers and get locked out by the defense system. Additionally, most CAPTCHA systems allow users to request a new image, thus skipping an answer enables the solver to exploit this feature to switch to an easier image.

To allow this new capability in a CAPTCHA solver, we propose the uncertainty-aware CAPTCHA solver, whose details are given in Algorithm~\ref{alg:overall}. Our new solver is equipped with a trained uncertainty-aware model $m$ and a real-valued threshold $\tau$. When given an input CAPTCHA image $x$, the model $m$ first predicts the string $y$ together with an uncertainty level $u$ via the function call $\Call{predict\_with\_uncertainty}{m, x}$. The uncertainty level $u$ is a real number indicating the extent to which the model $m$ is \emph{unconfident} that $y$ is the text string shown in the input image $x$. Note that the higher the uncertainty level $u$ is, the less likely that the prediction $y$ is correct. With the uncertainty level $u$, the next step is to compare it with the threshold $\tau$. If $u$ is below this threshold, our solver will return the text string $y$ as the answer for the CAPTCHA. Otherwise, it will skip $x$ and request a new CAPTCHA image if possible.

Note that for our uncertainty-aware solver, the range of $u$ and the choice of $\tau$ depend on the specific uncertainty estimation method. Since our method relies on the uncertainty level $u$ predicted by the uncertainty-aware model $m$, a model with a good uncertainty estimation capability is essential for our method to work well. In the next section, we shall describe EnSolver, an instance of the above solver, which uses the deep ensembles~\citep{d2021repulsive, lakshminarayanan2017simple} approach to obtain the uncertainty level $u$.


\subsection{EnSolver: The Ensemble Solver}


EnSolver is an uncertainty-aware CAPTCHA solver that employs a deep ensemble of CAPTCHA solvers for uncertainty estimation. Deep ensemble \citep{d2021repulsive, lakshminarayanan2017simple} is a popular uncertainty estimation approach that requires significantly less modification to the original model architecture as well as the training and inference pipelines, as compared to other uncertainty estimation approaches such as Bayesian methods \citep{blundell2015weight, chen2014stochastic, gal2016dropout, rudner2021tractable, zhang2020csgmcmc}.

We choose deep ensemble for uncertainty estimation since it allows our approach to have a greater applicability, especially to solvers with a very complex model architecture, while not compromising the quality of the uncertainty estimates. We will demonstrate our approach for one such complex model, which uses an object detector, in the next section. For these complex models, using a Bayesian method for uncertainty estimation is unnecessarily hard and inefficient since it requires a major modification to the model architecture and training procedure \citep{blundell2015weight, gal2016dropout, ritter2018scalable, zhang2020csgmcmc}. We must emphasize that despite the simplicity of our approach, we can achieve 98\% out-of-distribution detection accuracy, as we will show in our experiments in Section~\ref{sec:experiment}. This is consistent with several previous works \citep{ovadia2019can, wilson2020bayesian} that showed the competitiveness of deep ensembles compared to Bayesian methods such as variational inference or Laplace approximation.

For EnSolver, the model $m$ in Algorithm~\ref{alg:overall} is an ensemble of $M$ base models $(m_1, m_2, \ldots, m_M)$, each of which is a conventional CAPTCHA solver. The uncertainty level $u$ for this solver will be estimated from the agreement among the base models on a given input image $x$. We now describe how to train this ensemble of models and how to use it for uncertainty estimation.

\minisection{Training.} Given the number of base models $M$ and a labeled training set $\mathcal{D}$, we follow~\citep{lakshminarayanan2017simple} and build the deep ensemble by training $M$ base models separately with different random initializations. This training process is illustrated in Algorithm~\ref{alg:de_train}. In general, each base model $m_i$ may have its own architecture and can be trained with its own training process. However, in most cases in practice~\citep{blundell2015weight, d2021repulsive}, we only need to use one architecture and one training process (e.g., stochastic gradient descent with cross entropy loss). To speed up the training process, we can also train the base models in parallel using different GPUs~\citep{lakshminarayanan2017simple}. Our final ensemble is the set of $M$ well-trained base models $m = (m_1, m_2, \ldots, m_M)$.

\begin{algorithm}[tb]
\caption{Training a Deep Ensemble}\label{alg:de_train}
\begin{algorithmic}
    \Require Labeled training dataset $\mathcal{D}$ \\ 
             \hspace{1.05cm} Number of base models $M$
    \ForAll{$i \in \{1, 2, \ldots, M\}$}
        \State Initialize base model $m_i$ randomly
        \State Train $m_i$ using $\mathcal{D}$
    \EndFor
    \State $m \gets (m_1, m_2, \ldots, m_M)$
    \State \Return $m$
\end{algorithmic}
\end{algorithm}


\minisection{Uncertainty Estimation.} Given a trained ensemble model $m = (m_1, m_2, \ldots, m_M)$ and any input image $x$, we compute the predicted output string $y$ and the uncertainty estimate $u$ using the $\Call{predict\_with\_uncertainty}{m, x}$ function in Algorithm \ref{alg:de_uncertainty}. More specifically, we first use each base model $m_i$ to predict a string $y_i$ on the input image $x$. Since some models may predict a similar string, especially when the string is the correct prediction, we will obtain a set $(s_1, s_2, \ldots, s_N)$ of $N$ distinct strings among the $M$ predictions. Note that the set of predictions $(y_1, y_2, \ldots, y_M)$ imposes a predictive distribution on $(s_1, s_2, \ldots, s_N)$ where the probability $p_j$ of $s_j$ is:
%
\begin{equation}
p_j = \frac{\# \left\{ i \in \{ 1, 2, \ldots, M \} |y_i = s_j \right\}}{M},
\end{equation}
%
which is the ratio of the number of models that predict $s_j$. In this distribution, $p_{\textrm{max}} = \max_j p_j$ measures the agreement among the predictions of our models. It has a high value when a large proportion of the base models give the same prediction, i.e., when the uncertainty is low. Thus, we can use $u = 1 - p_{\textrm{max}}$ as the quantification of the predictive uncertainty. Note that the value of $u$ ranges from 0 to $1-\frac{1}{M}$. The predicted string $y$ returned from our procedure is the string that has the maximum number of predictions, i.e., the one corresponding with $p_{\textrm{max}}$. The architecture of EnSolver is depicted in figure \ref{fig:ensolver}.

% The EnSolver model described above provides an estimate of the \emph{epistemic} uncertainty~\citep{abdar2021review}, which arises from the inadequacy of the training data where different models can have equal performance on the training set.
% This uncertainty arises when the predictions of different models do not agree, which typically occurs when the input is distant from the training samples.
% As a result, the epistemic uncertainty is suitable for identifying out-of-distribution inputs.
% In addition to the epistemic uncertainty, there is another common source of uncertainty in model predictions called the \emph{aleatoric} uncertainty~\citep{abdar2021review}, which originates from the input of the model.
% For instance, a character in the input image may resemble both the letter 'c' and 'C', so both predictions need to be made with high uncertainty levels to account for the other possibility.
% As the EnSolver model estimates uncertainty based on the outputs from various base models, it provides an estimation of the epistemic uncertainty.
% In our experiments, we also use a baseline model with aleatoric uncertainty estimation.
% The results show that using epistemic uncertainty results in better out-of-distribution detection accuracy.

\begin{algorithm}[tb]
\caption{Uncertainty Estimation with Deep Ensemble}\label{alg:de_uncertainty}
\begin{algorithmic}
    \Require Ensemble model $m = (m_1, m_2, \ldots, m_M)$ \\ 
             \hspace{1.05cm} Input image $x$
    \Function{predict\_with\_uncertainty}{$m, x$}
        \ForAll{$i \in \{1, 2, \ldots, M\}$}
            \State $y_i \gets \text{predict}(m_i, x)$
        \EndFor
        \State $(s_1, s_2, \ldots, s_N) \gets \text{unique}(y_1, y_2, \ldots, y_M)$
        \ForAll{$j \in \{1, 2, \ldots, N\}$}
            \State $\displaystyle p_j \gets \frac{\# \left\{ i \in \{ 1, 2, \ldots, M \} |y_i = s_j \right\}}{M}$
        \EndFor
        \State $p_{\textrm{max}} \gets \max_{j=1, \ldots, N} \{ p_j \}$
        \State $j_{\textrm{max}} \gets \argmax_{j=1, \ldots, N} \{ p_j \}$
        \State $u \gets 1 - p_{\textrm{max}}$
        \State $y = s_{j_{\textrm{max}}}$
        \State \Return $(y, u)$
    \EndFunction
\end{algorithmic}
\end{algorithm}


\section{EnSolver with Object Detection Models}
\label{sec:ensolver_od}

The previous section has described a generic EnSolver framework that can be applied with any base model types. We now demonstrate in this section how to use EnSolver with DL-based object detectors as our base models. To our knowledge, ours is the first work to use DL-based object detection models for solving text-based CAPTCHAs. This is due to the lack of an available dataset with quality bounding box labels to train and test such models. In this section, we will also describe our approach to create a new dataset for this purpose.


\subsection{Object Detectors as Base Models}
\label{sec:base_models}

% Figure environment removed

An object detection model is used to locate and identify objects in an image \citep{ge2021yolox, redmon2016you, ren2015faster, litransformer}. SotA object detection models, such as the YOLO series \citep{bochkovskiy2020yolov4, ge2021yolox, redmon2016you, wang2022yolov7}, use deep learning in their architecture and perform end-to-end predictions, where images are taken as inputs and the bounding boxes as well as labels of the detected objects are returned. They are natural models for text-based CAPTCHA solvers because each character in a CAPTCHA image can be treated as an object to be located and identified. Furthermore, using object detection models as solvers has several advantages compared to previous approaches. For example, as opposed to segmentation-based methods~\citep{gao2016robustness, kopp2017breaking}, we do not need a character segmentation stage, and as opposed to~\citet{noury2020deep}, we do not need to specify the number of characters beforehand. Using object detection also allows us to make use of architectural innovations from a vast amount of previous work and open-source implementations with optimized technicalities.

In this paper, we use YOLOX~\citep{ge2021yolox} as our base models. This is a newly developed objection detection model in the YOLO series that has been shown to provide good performance and efficiency. YOLOX has the anchor-free detection mechanism which helps reduce the number of hyperparameters that need tuning, making it simpler to train and inference. It also uses a decoupled detection head to avoid the conflict between the object classification and bounding box regression tasks in training phase. To use this model as a solver, we treat each individual character in a CAPTCHA image as an object and train the detector to localize and classify those characters for each input image. From the detection result, the locations of the detected bounding boxes are used to sort the characters horizontally whereas their classes are used to identify the corresponding characters. For each of the base models, this procedure would return a text string as its output. Figure~\ref{fig:od} provides an illustration for this solver. After obtaining the predicted text strings from all the base objection detection models, we combine them using the algorithms in Section~\ref{sec:EnSolver} to obtain our final uncertainty-aware solver.


\subsection{New Dataset for Object Detection Solvers}

To use object detection models as CAPTCHA solvers, the training dataset must include the bounding box locations as well as the label for each character in every input image. However, available public CAPTCHA datasets only contain a text string as the final label for each image, which is insufficient for training our models. Thus, we need a completely new dataset with sufficient information to train and test our method. For this purpose, we develop our own CAPTCHA generator that is capable of generating very challenging images with different colors and noise patterns. When creating a CAPTCHA image, we keep track of the exact location of each character when it is drawn on the image that would provide us with the ground truth bounding box. To ensure that the generated CAPTCHAs are not trivial, we use the 3-stage process below to generate each CAPTCHA image.

\minisection{1.~Creating background.} We use a range of different colors for each background image instead of a monochromatic background since the latter can easily be exploited for background segmentation. At the same time, we need to ensure that the colors vary smoothly across the image because otherwise the background and text may become indistinguishable even for a human user. Specifically, to achieve these requirements, for each background, we randomize two different RGB triplets to set the colors at the left-most and right-most columns. The RGB values at internal columns are then obtained by horizontally interpolating the RGB values at the two extreme columns. For all images, we keep the RGB values on each column constant vertically.

\minisection{2.~Placing characters.} After generating a background image, we randomize a ground truth text string containing alphanumeric characters with a random length from 5 to 9. Then for each character in the string, we create an image solely containing the character with random facecolor, font and size. Next, a random perspective transform is applied to introduce character distortion. Finally, the distorted character is pasted into the generated background. Characters are positioned horizontally following the ground truth text string order, with random spacing. Characters are positioned horizontally following the ground truth text string order, with arbitrary spacing. The position and the size of each character determine its ground truth bounding box, which is combined with the ground truth text string and then stored as the ground truth label.

\minisection{3.~Adding foreground noise.} The image obtained from the previous two stages can already be used as a CAPTCHA, but we can still enhance its resistance against CAPTCHA solvers by adding some random noises to the image. Specifically, we draw several random curves and dots with various colors into the image where the curves can strike through the characters. The number of curves and dots as well as their thickness are chosen so that they significantly increase the CAPTCHA complexity while keeping the characters on the image recognizable by humans.

We generate 10,000 CAPTCHA images with the aforementioned process for our training dataset. An additional 1,000 images are generated for model testing. Some samples of our data are shown in Figures~\ref{fig:od} and~\ref{fig:datasets}(a). The code for our data generator will be released publicly upon the publication of this paper.


\section{Experiments}
\label{sec:experiment}

In this section, we conduct experiments to show that our proposed solver can successfully skip out-of-distribution samples while being confident to give predictions for in-distribution samples. First, we describe our experiment settings in Section~\ref{sec:exp-setting}. We then show in Section~\ref{sec:exp-out-dist} the performance of our method when detecting out-of-distribution CAPTCHAs. In Section~\ref{sec:success_rate}, we evaluate the success rate of our solver on datasets that contain both in-distribution and out-of-distribution samples. Finally, we analyze in Section~\ref{sec:exp-hyperparams} the impact of the ensemble size and the uncertainty threshold on the performance of our solver.

\subsection{Experiment Settings}
\label{sec:exp-setting}

\minisection{Data Preparation.} Besides our generated dataset, we also utilize publicly available text-based CAPTCHAs from several popular websites for testing our method. Following \citet{deng3e}, we use CAPTCHAs from 8 other different schemes, each of which has 1,000 labeled samples (see Figure~\ref{fig:datasets} for examples). These public datasets do not have ground truth bounding box information and thus cannot be used to train our objection detection models. In our experiments, they are only used for evaluating the performance of our method for out-of-distribution data detection. From the examples shown in Figure~\ref{fig:datasets}, we can see that those CAPTCHAs have different visual features, background, and noise patterns from those of our generated dataset, thus they are treated as out-of-distribution samples in our experiments. Since the label strings of these public datasets are case-insensitive, in our experiments, we convert all predictions and ground truth label strings to all-lowercase strings before computing the scores, although our models' detections are case-sensitive.

% Figure environment removed

\minisection{Base Models Training.} As mentioned in Section~\ref{sec:base_models}, we use YOLOX~\citep{ge2021yolox} as our base models. We implement these object detection models using the {\tt mmdetection} library~\citep{mmdetection}. The detectors are trained to detect characters from 62 different classes, including 52 English alphabet letters (both lowercase and uppercase) and 10 digits. Each base model is trained for 10 epochs using the Nesterov SGD optimizer \citep{nesterov1983method} with learning rate 0.01 and momentum 0.9. During the training, we save the model checkpoints for every epoch and use the checkpoint with the best mAP on the test set as our base model.

\minisection{Ensemble Model.} We test our method with the ensemble models containing $M = 2$, $6$ and $10$ base learners (named \textbf{EnS2}, \textbf{EnS6}, and \textbf{EnS10} respectively in our Tables and Figures). Note that in order to have nonzero uncertainty, we need at least 2 models in our ensemble. Furthermore, previous work on deep ensembles \citep{lakshminarayanan2017simple} showed that 10 base models are often enough to give a decent uncertainty estimation in most cases while not significantly increasing the time complexity to train the ensemble. In our experiments in Sections~\ref{sec:exp-out-dist} and~\ref{sec:success_rate}, the uncertainty threshold $\tau$ is set at 0.5, which is a natural value for the threshold since it means that at least half of the base models agree with each other. In Section~\ref{sec:exp-hyperparams}, we will vary the ensemble size and the threshold value used in our approach.


\minisection{Baseline Solver.} We compare our method with a baseline solver (named \textbf{Baseline} in our results) that uses only one single base model. Since the base model is an object detection model, it also predicts a confidence score ranging from 0 to 1 for each detected character. The minimum score among the predicted characters is used as the confidence score for the entire input image, denoted by $c$. We use $u=1-c$ as the uncertainty estimate in Algorithm~\ref{alg:overall} for this baseline solver. Note that this baseline can only be used with an object detection model and we would need different uncertainty estimation methods for other types of models. We also emphasize that our method operates with any DL-based models without requiring this confidence score. In addition to this baseline solver, we also compare with the variant where the solver never skips and will give predictions for all input images. We name this variant \textbf{Baseline-w/o-skip} in our results.

\subsection{Out-of-distribution Detection Experiment}
\label{sec:exp-out-dist}

In this experiment, we evaluate the accuracy of our solver when predicting whether an input CAPTCHA is from an out-of-distribution dataset. For this purpose, we construct 8 new test datasets by merging our generated test dataset with 1,000 random CAPTCHAs from each of the 8 public datasets in Figure~\ref{fig:datasets}(b)-(i). Hence, each new test dataset contains 2,000 samples with 1,000 in-distrubition samples (from our generated dataset) and 1,000 out-of-distribution samples (from a public dataset).

We train all solvers, including the baselines, using the training set of our generated data then run the solvers on the new test datasets above. If a solver skips an image, it is counted as predicting an out-of-distribution sample. Otherwise, we regard the solver as predicting an in-distribution sample. Hence, this is a binary classification problem, and we compare the accuracies of the solvers on this problem.

% The result for this experiment is shown in Table~\ref{tab:ood_acc}. From the result, the baseline can achieve 96.2\% average accuracy on this out-of-distribution detection problem. Note that for Baseline-w/o-skip, its accuracy is always 50\%. Regarding our EnSolver method, using two base models in the ensemble (EnS2) results in a slightly lower accuracy (94.9\% on average) because we do not use the detection confidence score, as opposed to the baseline. However, if we increase the number of base models in our ensemble, we can achieve a higher average accuracy (97.5\% and 98.1\% for EnS6 and EnS10 respectively). This trend is observed for all 8 datasets used in this experiment.
Table~\ref{tab:ood_acc} presents the experiment results, where the baseline achieves 96.2\% accuracy on out-of-distribution detection. Note that for Baseline-w/o-skip, its accuracy is always 50\%. The EnSolver method, EnS2, performs slightly worse at 94.9\% due to the lack of detection confidence score, unlike the baseline. However, increasing the number of ensemble base models boosts accuracy to 97.5\% and 98.1\% for EnS6 and EnS10 respectively. This trend holds across all eight datasets in the experiment.

\begin{table}
    \begin{center}
    % \captionsetup{justification=centering}
    {\caption{Out-of-distribution detection accuracy on 8 test datasets. Each dataset is a combination of our in-distribution dataset and a public dataset indicated in the first column. Baseline-w/o-skip gives 50\% accuracy for all datasets. The last row shows the average accuracies over the 8 datasets. Bold numbers indicate the best results in the corresponding row.}
    \label{tab:ood_acc}}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lcccc}
    \toprule
    Dataset (out-of-dist.) &  Baseline &  EnS2 &  EnS6 &  EnS10 \\
    \midrule
    Apple &     0.961 & 0.948 & 0.975 & \textbf{0.981} \\
    Ganji &     0.966 & 0.951 & 0.976 & \textbf{0.981} \\
    Google &    0.963 & 0.951 & 0.976 & \textbf{0.981} \\
    Microsoft & 0.959 & 0.946 & 0.974 & \textbf{0.981} \\
    Sina &      0.960 & 0.951 & 0.976 & \textbf{0.981} \\
    Weibo &     0.965 & 0.951 & 0.976 & \textbf{0.981} \\
    Wikipedia & 0.958 & 0.943 & 0.973 & \textbf{0.978} \\
    Yandex &    0.966 & 0.951 & 0.976 & \textbf{0.981} \\
    \midrule
    Average & 0.962 & 0.949 & 0.975 & \textbf{0.981} \\
    \bottomrule
    \end{tabular}
    }
    \end{center}
\end{table}


\subsection{Solver Success Rate}
\label{sec:success_rate}

We simulate the CAPTCHA cracking process to assess our solver's success rate on images from a mixed in-distribution and out-of-distribution CAPTCHA population. For in-distribution data, we expect the solver to return correct answers without skipping, so a response for an in-distribution input image is considered successful only if the predicted text is correct. For out-of-distribution data, however, our solver is allowed to skip an input image, so a response is considered successful in this case if either it is a skip or the predicted text is correct. In this experiment, we define the success rate of a solver on a test dataset as the fraction of successful responses over the dataset size.

% Figure~\ref{fig:acc_id} shows the success rates on our generated test dataset (i.e., in-distribution dataset). On this dataset, the regular baseline achieves 88.9\% success rate, while Baseline-w/o-skip can achieve 90.4\% success rate. The regular baseline misclassified some samples as out-of-distribution, resulting in a slightly lower success rate. EnS2 has the lowest success rate among all solvers (87\%), which can be explained by its lowest out-of-distribution detection accuracy observed in Section~\ref{sec:exp-out-dist} and Table~\ref{tab:ood_acc}. This result is improved when the ensemble has more base models (in EnS6 and EnS10) that can detect in-distribution samples with higher accuracy. Both EnS6 and EnS10 achieve better success rates (92.2\% and 93\% respectively) than the baselines due to the higher prediction accuracy of the ensemble models. Note that our success rates are competitive with SotA results~\citep{tian2020generic, li2021end, deng3e} where the success rates are respectively 86.5\%, 90.5\%, and 93\%, averaged over their best datasets (those with success rates at least 75\%).
Figure~\ref{fig:acc_id} presents success rates on our test dataset. The regular baseline yields an 88.9\% success rate, while Baseline-w/o-skip improves to 90.4\%. The lower rate for the regular baseline results from some misclassifications as out-of-distribution. EnS2, with the lowest rate (87\%), is attributed to its poor out-of-distribution detection as seen in Section~\ref{sec:exp-out-dist} and Table~\ref{tab:ood_acc}. This improves with more base models in EnS6 and EnS10, achieving 92.\% and 93\% respectively, due to their higher prediction accuracy. Note that these rates are competitive with SotA results~\citep{tian2020generic, li2021end, deng3e}, where they average 86.5\%, 90.5\%, and 93\% on their top datasets.

Table~\ref{tab:acc_skip} compares the success rates on the out-of-distribution datasets. Without the ability to skip, Baseline-w/o-skip almost always gives wrong predictions. With uncertainty estimation, either by object detection confidence score or by deep ensembles, the other solvers can avoid giving wrong predictions and thus achieve very high success rates. Overall, our EnS10 solver achieves the best success rates on 7/8 out-of-distribution datasets, among which it has perfect success rate (100\%) on 6 of them.

\begin{table}
    % \begin{center}
    % \captionsetup{justification=centering, margin=0pt}
    {\caption{Success rates when cracking out-of-distribution CAPTCHAs collected from the datasets in the first column. Bold numbers indicate the best results in the corresponding row.}
    \label{tab:acc_skip}}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lrrrrr}
    \toprule
    Dataset &  Baseline- &  Baseline &  EnS2 &  EnS6 & EnS10 \\
    (out-of-dist.)       &  w/o-skip &       &       &       \\
    \midrule
    Apple &        0.000 &       0.990 &       0.994 &       0.999 & {\bf 1.000} \\
    Ganji &        0.000 & {\bf 1.000} &       0.999 & {\bf 1.000} & {\bf 1.000} \\
   Google &        0.000 &       0.995 & {\bf 1.000} & {\bf 1.000} & {\bf 1.000} \\
Microsoft &        0.000 &       0.987 &       0.989 &       0.997 & {\bf 0.999} \\
     Sina &        0.000 &       0.989 & {\bf 1.000} & {\bf 1.000} & {\bf 1.000} \\
    Weibo &        0.000 &       0.999 & {\bf 1.000} & {\bf 1.000} & {\bf 1.000} \\
Wikipedia &        0.005 &       0.985 &       0.986 & {\bf 0.996} &       0.995 \\
   Yandex &        0.000 & {\bf 1.000} &       0.999 & {\bf 1.000} & {\bf 1.000} \\
    \bottomrule
    \end{tabular}
    }
    % \end{center}
\end{table}

% Figure environment removed


\subsection{Impact of Ensemble Size and Uncertainty Threshold}
\label{sec:exp-hyperparams}

% Figure environment removed

In this section, we investigate the impact of the ensemble size $M$ and the uncertainty threshold $\tau$ on the performance of our proposed solver. Using the same experiment settings as in Sections~\ref{sec:exp-out-dist} and~\ref{sec:success_rate}, we compare the average out-of-distribution detection accuracies and success rates across the eight datasets. Figures~\ref{fig:ood_acc} and~\ref{fig:success_rate} show these results when we vary the values of $M \in \{ 2, 3, \ldots, 10 \}$ and $\tau \in [0, 1]$.

From the out-of-distribution detection result in Figure~\ref{fig:ood_acc}, we can see that the choice of $\tau = 0.5$ in the previous experiments is not optimal. Typically, uncertainty thresholds exceeding 0.5 outperform those below, with a 0.7-0.8 threshold yielding optimal accuracy. On the other hand, using a higher number of base models $M$ makes it easier to find a threshold that gives a good out-of-distribution detection accuracy (99\% for $M \ge 4$).

Figure~\ref{fig:success_rate} shows the impact of $M$ and $\tau$ on the average success rate of EnSolver over the out-of-distribution datasets in Section~\ref{sec:success_rate} and Table~\ref{tab:acc_skip}. Similar to the previous result, the solvers with more base models (i.e., larger $M$) has a larger range of well-performing thresholds. For all values of $M$, we can achieve up to 100\% success rate with any $\tau < 0.5$. We note that for this metric, lower threshold values give better results since they encourage our solvers to skip more samples, which is considered a success when dealing with out-of-distribution data. Therefore, in general, we need to choose a threshold value that provides a good balance between the out-of-distribution detection accuracy in Figure~\ref{fig:ood_acc} and the success rate in Figure~\ref{fig:success_rate}.


\section{Conclusion}

We proposed a novel end-to-end, uncertainty-aware text-based CAPTCHA solver that can detect and skip out-of-distribution CAPTCHAs. Our solver uses a deep ensemble of object detection models to obtain the uncertainty estimate of a prediction. Our experiments showed that the solver can return correct answers when given in-distribution inputs and skip answering out-of-distribution inputs with high success rates. We hope that our work can help security experts better understand the capability of automatic CAPTCHA solvers to improve the defense against these attacks.
For example, CAPTCHA systems should also limit the number of times the remote client can request a new image besides the number of times the client making wrong predictions. However, this should require further research to determine a balance between system security and user experience.

% Although we only develop a solver for text-based CAPTCHA, we believe that our approach is universal and extensible to all other types of CAPTCHAs provided that there have been deep learning models that can solve the task automatically.
% Therefore, systems using these CAPTCHA types should also take our result into consideration and design appropriate defense mechanisms against this potential threat.

% It is important to acknowledge that high computational cost is a limitation of our proposed method.
% Running the training process for multiple base models incurs a significant expense, especially if the base model size or the number of models is large.
% Nevertheless, this challenge can be mitigated by implementing the technique proposed by \citet{chen2017checkpoint}.
% Similarly, in inference, the ensemble model also requires computing power and memory multiple times higher than that of a single model.
% However, it is worth noting that the objective of our work is not to devise a commercially viable system.
% Instead, our main purpose is to demonstrate a potential threat to current security systems that use CATCHA, so that experts can use our findings to develop necessary countermeasures.

\section*{Ethical Statement}
Our work can potentially be exploited by attackers to bypass the CAPTCHA anti-bot protection in websites. However, we believe this risk is outweighed by the awareness among the security experts about this new capability of automatic CAPTCHA solvers.

\bibliography{citations}
\end{document}