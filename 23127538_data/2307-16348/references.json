{
  "2303-16194": {
    "title": "BC-IRL: Learning Generalizable Reward Functions from Demonstrations",
    "authors": [
      "Andrew Szot",
      "Amy Zhang",
      "Dhruv Batra",
      "Z. Kira",
      "Franziska Meier"
    ],
    "submission_date": "2023-03-28",
    "semantic_scholar_id": "d8c0b2fbacdade8bde676feff9da9aaeda244839"
  },
  "2205-12401": {
    "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
    "authors": [
      "Xi-Xi Liang",
      "Katherine Shu",
      "Kimin Lee",
      "P. Abbeel"
    ],
    "submission_date": "2022-05-24",
    "semantic_scholar_id": "cc9f2fd320a279741403c4bfbeb91179803c428c"
  },
  "2111-03026": {
    "title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning",
    "authors": [
      "Kimin Lee",
      "Laura M. Smith",
      "A. Dragan",
      "P. Abbeel"
    ],
    "submission_date": "2021-11-04",
    "semantic_scholar_id": "51965de80f86432d42749427db1e5bb0fa1e204c"
  },
  "2108-07259": {
    "title": "APReL: A Library for Active Preference-based Reward Learning Algorithms",
    "authors": [
      "Erdem Biyik",
      "Aditi Talati",
      "Dorsa Sadigh"
    ],
    "submission_date": "2021-08-16",
    "semantic_scholar_id": "ca9a66f802d83706c353d11ea5d18b276092c9f9"
  },
  "2106-05091": {
    "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
    "authors": [
      "Kimin Lee",
      "Laura M. Smith",
      "P. Abbeel"
    ],
    "submission_date": "2021-06-09",
    "semantic_scholar_id": "45f573f302dc7e77cbc5d1a74ccbac3564bbebc8"
  },
  "2010-07467": {
    "title": "Human-Guided Robot Behavior Learning: A GAN-Assisted Preference-Based Reinforcement Learning Approach",
    "authors": [
      "Huixin Zhan",
      "Feng Tao",
      "Yongcan Cao"
    ],
    "submission_date": "2020-10-15",
    "semantic_scholar_id": "9f17313e0d7b173765800e6f471407544e0f2202"
  },
  "2006-14091": {
    "title": "Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences",
    "authors": [
      "Erdem Biyik",
      "Dylan P. Losey",
      "Malayandi Palan",
      "Nicholas C. Landolfi",
      "Gleb Shevchuk",
      "Dorsa Sadigh"
    ],
    "submission_date": "2020-06-24",
    "semantic_scholar_id": "e5fc53230b7abced54d83058286dfff7f631289d"
  },
  "2006-08910": {
    "title": "Preference-based Reinforcement Learning with Finite-Time Guarantees",
    "authors": [
      "Yichong Xu",
      "Ruosong Wang",
      "Lin F. Yang",
      "Aarti Singh",
      "A. Dubrawski"
    ],
    "submission_date": "2020-06-16",
    "semantic_scholar_id": "c4f946b43c372c674f632076163ece7e5d54481b"
  },
  "2002-09089": {
    "title": "Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences",
    "authors": [
      "Daniel S. Brown",
      "Russell Coleman",
      "R. Srinivasan",
      "S. Niekum"
    ],
    "submission_date": "2020-02-21",
    "semantic_scholar_id": "658018e556484e3d9c6bcc00c726bf5eb503ef86"
  },
  "1910-04365": {
    "title": "Asking Easy Questions: A User-Friendly Approach to Active Reward Learning",
    "authors": [
      "Erdem Biyik",
      "Malayandi Palan",
      "Nicholas C. Landolfi",
      "Dylan P. Losey",
      "Dorsa Sadigh"
    ],
    "submission_date": "2019-10-10",
    "semantic_scholar_id": "8cc77d98ea62a4ad0515e74dcd2635a0d7b338d3"
  },
  "1904-06387": {
    "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
    "authors": [
      "Daniel S. Brown",
      "Wonjoon Goo",
      "P. Nagarajan",
      "S. Niekum"
    ],
    "submission_date": "2019-04-12",
    "semantic_scholar_id": "2fc328f3702d6f8730235b1b3ddf7cc5fc096c0d"
  },
  "1901-10995": {
    "title": "Go-Explore: a New Approach for Hard-Exploration Problems",
    "authors": [
      "Adrien Ecoffet",
      "Joost Huizinga",
      "J. Lehman",
      "Kenneth O. Stanley",
      "J. Clune"
    ],
    "submission_date": "2019-01-30",
    "semantic_scholar_id": "c520bf47db3360ae3a52219771390a354ed8a91f"
  },
  "1811-06521": {
    "title": "Reward learning from human preferences and demonstrations in Atari",
    "authors": [
      "Borja Ibarz",
      "Jan Leike",
      "Tobias Pohlen",
      "G. Irving",
      "S. Legg",
      "Dario Amodei"
    ],
    "submission_date": "2018-11-15",
    "semantic_scholar_id": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a"
  },
  "1801-01290": {
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "authors": [
      "Tuomas Haarnoja",
      "Aurick Zhou",
      "P. Abbeel",
      "S. Levine"
    ],
    "submission_date": "2018-01-04",
    "semantic_scholar_id": "811df72e210e20de99719539505da54762a11c6d"
  },
  "1709-10163": {
    "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces",
    "authors": [
      "Garrett Warnell",
      "Nicholas R. Waytowich",
      "Vernon J. Lawhern",
      "P. Stone"
    ],
    "submission_date": "2017-09-01",
    "semantic_scholar_id": "abcf11a9af3d83f85c5fbfffc5901d416ca7a73f"
  },
  "1707-06347": {
    "title": "Proximal Policy Optimization Algorithms",
    "authors": [
      "John Schulman",
      "Filip Wolski",
      "Prafulla Dhariwal",
      "Alec Radford",
      "Oleg Klimov"
    ],
    "submission_date": "2017-07-20",
    "semantic_scholar_id": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b"
  },
  "1706-03741": {
    "title": "Deep Reinforcement Learning from Human Preferences",
    "authors": [
      "P. Christiano",
      "Jan Leike",
      "Tom B. Brown",
      "Miljan Martic",
      "S. Legg",
      "Dario Amodei"
    ],
    "submission_date": "2017-06-12",
    "semantic_scholar_id": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd"
  },
  "1606-06565": {
    "title": "Concrete Problems in AI Safety",
    "authors": [
      "Dario Amodei",
      "Chris Olah",
      "J. Steinhardt",
      "P. Christiano",
      "John Schulman",
      "Dandelion Man√©"
    ],
    "submission_date": "2016-06-21",
    "semantic_scholar_id": "e86f71ca2948d17b003a5f068db1ecb2b77827f7"
  },
  "1603-00448": {
    "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization",
    "authors": [
      "Chelsea Finn",
      "S. Levine",
      "P. Abbeel"
    ],
    "submission_date": "2016-03-01",
    "semantic_scholar_id": "04162cb8cfaa0f7e37586823ff4ad0bff09ed21d"
  },
  "1509-02971": {
    "title": "Continuous control with deep reinforcement learning",
    "authors": [
      "T. Lillicrap",
      "Jonathan J. Hunt",
      "A. Pritzel",
      "N. Heess",
      "Tom Erez",
      "Yuval Tassa",
      "David Silver",
      "D. Wierstra"
    ],
    "submission_date": "2015-09-09",
    "semantic_scholar_id": "024006d4c2a89f7acacc6e4438d156525b60a98f"
  },
  "1507-04888": {
    "title": "Maximum Entropy Deep Inverse Reinforcement Learning",
    "authors": [
      "Markus Wulfmeier",
      "Peter Ondruska",
      "I. Posner"
    ],
    "submission_date": "2015-07-17",
    "semantic_scholar_id": "9ba266a4a4644e877fc37a64be3beddce8904cf7"
  },
  "2203-10050": {
    "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning",
    "authors": [
      "Jongjin Park",
      "Younggyo Seo",
      "Jinwoo Shin",
      "Honglak Lee",
      "P. Abbeel",
      "Kimin Lee"
    ],
    "submission_date": null,
    "semantic_scholar_id": "f2a2401a35b6b892d43642b31700e83e88b2ebb8"
  }
}