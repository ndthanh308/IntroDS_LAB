%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
%\usepackage{bbm}
\usepackage{subfigure}
\usepackage{pdfpages}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Rating-Based Reinforcement Learning}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Devin White\textsuperscript{\rm 1}, Mingkang Wu\textsuperscript{\rm 1},
    Ellen Novoseller\textsuperscript{\rm 2}, Vernon J. Lawhern\textsuperscript{\rm 2}, Nicholas Waytowich\textsuperscript{\rm 2},
    Yongcan Cao\textsuperscript{\rm 1}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}University of Texas, San Antonio\\
    \textsuperscript{\rm 2}DEVCOM Army Research Laboratory
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2}, 
    % J. Scott Penberthy\textsuperscript{\rm 3}, 
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    %1900 Embarcadero Road, Suite 101\\
    %Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    %proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
This paper develops a novel rating-based reinforcement learning (RbRL) approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. 
The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We finally conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the performance of the new rating-based reinforcement learning approach. 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%subtopic: RL and its issue 
With the development of deep neural network theory and improvements in computing hardware, deep reinforcement learning (RL) has become capable of handling complex tasks with large state and/or action spaces (\textit{e.g.}, Go and Atari games) and yielding human-level or better-than-human-level performance~\cite{silver2016mastering,mnih2015human}. Numerous approaches, such as DQN~\cite{mnih2015human}, DDPG~\cite{lillicrap2015continuous}, PPO~\cite{schulman2017proximal}, and SAC~\cite{haarnoja2018soft} have been developed to address challenges such as stability, exploration, and convergence for various applications~\cite{li2019reinforcement} such as robotic control, autonomous driving, and gaming. Despite the important and fundamental advances behind these algorithms, one key obstacle for the wide application of deep RL is the required knowledge of a reward function, which is often unavailable in practical applications. %Hence, expert knowledge is required to design the reward function~\cite{ng1999policy}.     
%Applications, such as web recommendation, healthcare system, and transportation control

%%subtopic: Existing approaches for MDP\R 
Although human experts could design reward functions in some domains, the cost is high because human experts need to understand the relationship between the mission objective and state-action values and may need to spend extensive time adjusting reward parameters and trade-offs not to encounter adverse behaviors such as reward hacking~\cite{amodei2016concrete}. Another approach is to utilize qualitative human inputs \textit{indirectly} to learn a reward function, such that humans guide reward function design \textit{without} directly handcrafting the reward. 
%Existing work on learning rewards includes three main approaches: inverse reinforcement learning (IRL), preference-based reinforcement learning (PbRL), and ordinal feedback reinforcement learning (OFRL). 
Existing work on reward learning includes inverse reinforcement learning (IRL)~\cite{ziebart2008maximum}, preference-based reinforcement learning (PbRL)~\cite{christiano2017deep}, and the combination of demonstrations and relative preferences, e.g. learning from preferences over demonstrations~\cite{brown2019extrapolating}.


%%subtopic: Existing methods limitations  
Existing human-guided reward learning approaches have demonstrated effective performance in various tasks. However, they suffer from some key limitations. For example, IRL requires expert demonstrations and hence, cannot be directly applied to tasks that are difficult for humans to demonstrate. 
%OFRL is an offline learning strategy due to the need for rankings of a pool of demonstrations. For online learning settings, it is difficult to create rankings when a new sample needs to compare with all existing ones that are ranked. Meanwhile, it may not be feasible for experts to distinguish neighboring rankings. 
PbRL is a practical approach to learning rewards for RL, since it is straightforward for humans to provide accurate relative preference information.
%can be collected over three different types \cite{wirth2017survey}, \textit{i.e.}, action preferences, state preferences, and trajectory preferences. 
Yet, RL from pairwise preferences suffers from some key disadvantages. First, each pairwise preference provides only a single bit of information, which can result in sample inefficiency. %and require significant human time to gather sufficient data for training a well-performing policy. 
In addition, due to their binary nature,
%use of queries can be very ineffective because preference selection of sample pairs is binary information and hence 
standard preference queries do not indicate how much better or worse one sample is than another. Furthermore, because preference queries are relative, they cannot directly provide a global view of each sample's absolute quality (good vs. bad); for instance, if all choices shown to the user are of poor quality, the user cannot say, ``A is better than B, but they're both bad!''. Thus, a PbRL algorithm may be more easily trapped in a local optimum, and cannot know to what extent its performance approaches the user's goal. Finally, PbRL methods often require strict preferences, such that comparisons between similar-quality or incomparable trajectories cannot be used in reward learning. While some works use weak preference queries~\cite{biyik2020asking, biyik2022aprel}, in which the user can state that two choices are equally preferable, there is no way to specify the quality (good vs. poor) of such trajectories; thus, valuable information remains untapped.  
%need to be provided for sample pairs. In other words, PbRL assumes no ambiguity when giving preferences. When the sample pairs are similar, they will not be used in PbRL but still contain important value. 
%Hence, the current PbRL method can hardly be used in complex  %Therefore, PbRL needs to design its query strategy so that a representative set of preferences can be obtained. Besides that, PbRL does not fully leverage the advantages of having human in the loop of reinforcement learning given human can provide more information than a binary preference. 

%\textcolor{red}{Add a figure to illustrate the main idea of RbRL?}

%% Our method
The objective of this paper is to design a new rating-based RL (RbRL) approach that infers reward functions via multi-class human ratings. 
%RbRL differs from IRL, PbRL, and OFRL because it leverages human ratings on individual samples, which is different from IRL that uses demonstrations, as well as PbRL and OFRL that use relative pairwise comparisons.
RbRL differs from IRL and PbRL in that it leverages human ratings on individual samples, whereas IRL uses demonstrations and PbRL uses relative pairwise comparisons.
In each query, RbRL displays one trajectory to a human and requests the human to provide a discrete rating. The number of rating classes can be as low as two, e.g. ``bad'' and ``good'', and can be as high as desired. For example, when the number of rating classes is $5$, the $5$ possible human ratings could correspond to ``very bad'', ``bad'', ``ok'', ``good'', and ``very good''. 
It is worth mentioning that the statement ``samples A and B are both rated as
`good' '' may provide more information than stating that ``A and B are equally preferable'', which can be inferred by the former.
However, ``A and B are equally preferable'' may be important
information for fine-tuning. In addition, a person can also intentionally assign high ratings to samples that contain rare states, which would be beneficial for addressing the exploration issue~\cite{ecoffet2019go} in RL. For both PbRL and RbRL, obtaining good samples requires exploration, and both will suffer
without any well-performing samples.


%%subtopic: contributions
The main contributions of this paper are as follows. First, we propose a novel RbRL framework for reward function and policy learning from qualitative, absolute human evaluations. Second, we design a new multi-class cross-entropy loss function that accepts multi-class human ratings as the input. The new loss function is based on the computation of a relative episodic reward index 
%in the set $[0,1]$ 
and the design of a new multi-class probability distribution function based on this index.
Third, we conduct several experimental studies to quantify the impact of the number of rating classes on the performance of RbRL, and compare RbRL and PbRL under both synthetic and real human feedback. Our studies suggest that (1) too few or too many rating classes can be disadvantageous, (2) RbRL can outperform PbRL under both synthetic and real human feedback, and (3) people find RbRL to be less demanding, discouraging, and frustrating than PbRL.
%Third, we conduct a comprehensive IRB approved human subject study of RbRL and its comparison with PbRL, including human time, agent performance, and human perception and acceptance of ratings versus preferences, with 20 participants who have no prior knowledge of PbRL, RbRL, or which method is the proposed approach or baseline. Analyzing the results, we find that RbRL can yield a 38\% human time savings per query relative to PbRL. An analysis of the resulting agent performance and questionnaire responses suggests that (1) RbRL can outperform PbRL under the same amount of human time and (2) RbRL is less demanding, discouraging, and frustrating than PbRL. As it is nonobvious to generate synthetic ratings, the current tests don't include synthetic experiments. %\textcolor{red}{IRB approved study with participants who have no knowledge of PbRL and RbRL and which one is the baseline to compare against.} % . Fourth, we compare PbRL and RbRL in different environments under the same human labels. Finally, we provide a ablation study of RbRL. 

\section{Related Work}

Inverse Reinforcement Learning (IRL) seeks to infer reward functions from demonstrations such that the learned reward functions generate behaviors that are similar to the demonstrations. Numerous IRL methods~\cite{ng2000algorithms}, such as maximum entropy IRL~\cite{ziebart2008maximum, wulfmeier2015maximum}, nonlinear IRL~\cite{finn2016guided}, Bayesian IRL~\cite{levine2011nonlinear, choi2011map, choi2012nonparametric}, adversarial IRL~\cite{fu2018learning}, and behavioral cloning IRL~\cite{szot2022bc} have been developed to infer reward functions. The need for demonstrations often makes these IRL methods costly, since human experts are needed to provide demonstrations.

Instead of requiring human demonstrations, PbRL~\cite{wirth2017survey,christiano2017deep, ibarz2018reward, liang2021reward,zhan2021human,xu2020preference, lee2021pebble, park2022surf} leverages human pairwise preferences over trajectory pairs to learn reward functions. Querying humans for pairwise preferences rather than demonstrations can dramatically save human time. In addition, by leveraging techniques such as adversarial neural networks~\cite{zhan2021human}, additional human time can be saved by learning a well-performing model to predict human preference. Another benefit of PbRL is that humans can provide preferences with respect to uncertainty to promote exploration~\cite{liang2021reward}. Despite these benefits, PbRL can be ineffective, especially for complex environments, because pairwise preferences only provide relative information rather than directly evaluating sample quality; while in some domains, sampled pairs may be selected carefully to infer global information, in practice, even if one sample is preferred over another, it does not necessarily mean that this sample is good. People can also have difficulty when comparing similar samples, thus taking more time and potentially yielding inaccurate preference labels. Notably, several works have sought to improve sample efficiency of PbRL; for instance, PEBBLE~\cite{lee2021pebble} considers off-policy PbRL, and SURF~\cite{park2022surf} explores data augmentations in PbRL. These contributions are orthogonal to ours, as they could straightforwardly be applied within our proposed RbRL framework.

Other methods for learning reward functions from humans include combining relative rankings and demonstrations, e.g. by inferring rewards via rankings over a pool of demonstrations~\cite{brown2020safe, brown2019extrapolating, brown2020better} to extrapolate better-than-demonstrator performance from the learned rewards, %.  
%Other methods of combining pairwise preferences and demonstrations include
or first learning from demonstrations and then fine-tuning with preferences~\cite{ibarz2018reward, biyik2022learning}. Finally, in the TAMER framework~\cite{knox2009interactively, warnell2018deep, celemin2015coach}, a person gives positive (encouraging) and negative (discouraging) feedback to an agent with respect to specific states and actions, instead of over entire trajectories. These methods generally take actions greedily with respect to the learned reward, which may not yield an optimal policy in continuous control settings. %such as we consider.

\section{Problem Formulation}\label{sec:pre}

%\newsec{Problem Formulation} 
%\ellen{I used revision tracking here because I edited this section significantly. I tried to formulate things as an MDP \textit{without} reward, since RbRL doesn't apply to situations where we know a true environment reward. Please let me know if this seems fine.} \textcolor{red}{YC: I like this idea. What does $n$ mean in the tuple?}
We consider a Markov decision process without reward (MDP\textbackslash R) augmented with ratings, which is a tuple of the form $(\mathcal{S}, \mathcal{A}, T, \rho, \gamma, n)$. Here, $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of possible actions, $T: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1]$ is a state transition probability function specifying the probability $p(s' \mid s, a)$ of reaching state $s' \in \mathcal{S}$ after taking action $a$ in state $s$, $\rho: \mathcal{S} \to [0, 1]$ specifies the initial state distribution, 
%$R$ is the reward function that describes the reward associated with a state or a state-action pair. 
$\gamma$ is a discount factor, and $n$ is the number of rating classes. The learning agent interacts with the environment through rollout trajectories, where a length-$k$ trajectory segment takes the form $(s_1, a_1, s_2, a_2, \ldots, s_k, a_k)$. A \textit{policy} $\pi$ is a function that maps states to actions, such that $\pi(a \mid s)$ is the probability of taking action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$.

In traditional RL, the environment would receive a reward signal $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$, mapping state-action pairs to a numerical reward, such that at time-step $t$, 
%Here, we assume that the environment reward function $r\in R$ is determined by the current state $s\in S$ and action $a \in A$, \textit{i.e.}, $r_{t} = r(s_t, a_t)$, where $t$ is the time step. 
the algorithm receives a reward $r_{t} = r(s_t, a_t)$, where $(s_t, a_t)$ is the state-action pair at time $t$.
Accordingly, the standard RL problem can be formulated as a search for the optimal policy $\pi^*$, where
$\pi^* = \arg\max_{\pi}\sum_{t=0}^{\infty} \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi}} \Big [ \gamma^t r(s_t, a_t)\Big],
$
$ a_t \sim \pi (\cdot \vert  s_t) $, and $ \rho_{\pi} $ is the marginal state-action distribution induced by the policy $\pi$. Note that standard RL assumes the availability of the reward function $r$. When such a reward function is unavailable, standard RL and its variants may not be used to derive control policies. Instead, we assume that the user can assign any given trajectory segment $\tau = (s_1, a_1, \ldots, s_k, a_k)$ a rating in the set $\{0, 1, \ldots, n -1\}$ indicating the quality of that segment, where $0$ is the lowest possible rating, while $n - 1$ is the highest possible rating.

The algorithm presents a series of trajectory segments $\sigma$ to the human and receives corresponding human ratings. Let $X := \{(\sigma_i, c_i)\}_{i=1}^l$ be the dataset of observed human ratings, where $c_i \in \{0, \ldots, n -1\}$ is the rating class assigned to segment $\sigma_i$, and $l$ is the number of rated segments contained in $X$ at the given point during learning.

Note that descriptive labels can also be given to the rating classes. For example, for $n=4$ rating classes, we can call the rating class $0$ ``very bad'', the rating class $1$ ``bad'', the rating class $2$ ``good'', and the rating class $3$ ``very good''. With $n=3$ rating classes, we can call the rating class $0$ ``bad'', the rating class $1$ ``neutral'', and class $2$ ``good''.



\section{Rating-based Reinforcement Learning} \label{sec:RbRL}

Different from the binary-class reward learning in~\citet{christiano2017deep} that utilizes relative human preferences between segment pairs, RbRL utilizes non-binary multi-class ratings for individual segments. We call this a multi-class reinforcement learning approach based on ratings. %Ratings-based feedback is advantageous, since segments are assigned absolute ratings instead of labeled as preferred or non-preferred. 
%Note that in PbRL~\cite{christiano2017deep}, even if one segment is preferred to another, its rating may be low because both segments are of poor quality. Hence, ratings may yield more information than preferences. 
%Another benefit of ratings instead of preferences is that segments can be evaluated in an absolute sense rather than a relative sense. 
%Practically, humans can provide ratings relatively easily with basic knowledge about desired and undesired behaviors. Despite the reasonableness and usefulness of creating a new rating-based reinforcement learning approach, one challenge is the design of a new multi-class (cross-entropy) loss function that operates on multi-class ratings from humans. 

\subsection{Modeling Reward and Return} Our approach learns a reward model $\hat{r}: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ that predicts state-action rewards $\hat{r}(s, a)$. We further define $\hat{R}(\sigma) := \sum_{t = 1}^{k} \gamma^{t - 1} \hat{r}(s_t, a_t)$ as the cumulative discounted reward, or the \textit{return}, of length-$k$ trajectory segment $\sigma$. Larger $\hat{R}(\sigma)$ corresponds to a higher predicted human rating for segment $\sigma$.
%is large among its peers in $X$, and vice versa. 
Next, we define $\tilde{R}(\sigma)$ as a function mapping a trajectory segment $\sigma$ to an estimated total discounted reward, normalized to fall in the interval $[0, 1]$ based on the dataset of rated trajectory segments $X$:
%Before presenting the specific form of $Q_{\sigma}(i)$, we first discuss the principle of assigning the rating class for a trajectory segment $\sigma$ among a given set $X$.
\begin{equation}\label{eq:eta}
    \tilde{R}(\sigma) = \frac{\hat{R}(\sigma) - \min_{\sigma' \in X} \hat{R}(\sigma')} {\max_{\sigma' \in X}\hat{R}(\sigma') - \min_{\sigma' \in X} \hat{R}(\sigma')}.
\end{equation} 
%Note that this definition ensures ${\tilde{R}(\sigma)}\in[0,1]$,
%This normalization operation maps each sample into the set $[0,1]$. 
%and that a lower $\tilde{R}(\sigma)$ indicates that $\sigma$ is estimated to have a lower discounted cumulative reward.
%$\hat{R}(\sigma) := \sum_{t = 1}^{k} \gamma^t \hat{r}(s_t, a_t)$ is large among its peers in $X$, and vice versa. To quantify how high the discounted cumulative reward of a sample is among its peers, we first compute the normalized value for each sample, given by: %Q_{\mu}(\sigma^x)

\subsection{Novel Rating-Based Cross-Entropy Loss Function}
% More concise alternative:
%Recall that we denote the number of rating classes as $n$, where $0$ corresponds to the lowest possible rating and $n-1$ corresponds to the highest possible rating.
%Without loss of generality, we consider $n$ rating classes labeled from $0$ to $n-1$, where $0$ corresponds to the least desired rating (or lowest rating), and $n-1$ corresponds to the most desired rating (or highest rating).
%, and $n$ is the total number of rating classes. 
%Thus, a higher-rated trajectory segment is more desired. 
%When the number of rating classes is relatively small, we can also provide a more descriptive label for each rating. For example, if the total number of rating classes is $4$, we can call the rating class $0$ ``very bad'', the rating class $1$ ``bad'', the rating class $2$ ``good'', and the rating class $3$ ``very good''. With 3 total rating classes, we can call the rating class $0$ `` bad'', the rating class $1$ ``neutral'', and the rating class $2$ ``good''.
%It is worth mentioning that RbRL is different from ranking-based OFRL in~\cite{brown2019extrapolating, brown2020better} because ranking-based OFRL in~\cite{brown2019extrapolating, brown2020better} still used relative comparison, collected a single sample in each ranking, and utilized the loss function in~\eqref{eq:loss} to learn reward function, hence inheriting the limitations mentioned in Section~\ref{sec:pre}. %In addition, each ranking in~\cite{brown2019extrapolating, brown2020better} has only one trajectory to be used to create relative comparison, hence requiring 


To construct a new (cross-entropy) loss function that can take multi-class human ratings as the input, we need to estimate the human's rating class predictions. 
In addition, the range of the estimated rating class should belong to the interval $[0,1]$ for the cross-entropy computation. 
We here propose a new multi-class cross-entropy loss given by:
\begin{equation}\label{eq:crossentropy}
    L(\hat{r}) = -\sum_{\sigma\in X} \left( \sum_{i=0}^{n-1}\mu_{\sigma}(i) \log \big(Q_{\sigma}(i)\big) \right),
\end{equation}
where $X$ is the collected dataset of user-labeled segments, $\mu_{\sigma}(i)$ is an indicator that equals 1 when the user assigns rating $i$ to trajectory segment $\sigma$,
%distribution over $\{0,1,\cdots,n-1\}$ indicating the $i$th rating that the user prefers for the segment, 
and $Q_{\sigma}(i) \in [0, 1]$ is the estimated probability that the human assigns the segment $\sigma$ to the $i$th rating class. Next, we will model the probabilities $Q_{\sigma}(i)$ of the human choosing each rating class. Notably, we do this \textit{without} comparing the segment $\sigma$ to other segments. 

\subsection{Modeling Human Rating Probabilities} \label{sec:Probability}
%\subsection{Normalization of Discounted Cumulative Reward}

We next describe our model for $Q_{\sigma}(i)$ based on the normalized predicted returns $\tilde{R}(\sigma)$. To model the probability that $\sigma$ belongs to a particular class, we will first model separations between the rating classes in reward space.

We define rating class boundaries $\bar{R}_0, \bar{R}_1, \ldots, \bar{R}_n$ in the space of normalized trajectory returns such that $0 := \bar{R}_0 \le \bar{R}_1 \le \ldots \le \bar{R}_n := 1$.
Then, if a segment $\sigma$ has normalized predicted return $\tilde{R}(\sigma)$ such that $\bar{R}_i \le \tilde{R}(\sigma) \le \bar{R}_{i + 1}$, we wish to model that $\sigma$ belongs to rating class $i$ with the highest probability.
%These values will be our learned rating class boundaries in normalized return space.

For example, when the total number of rating classes is $n = 4$, we aim to model the lower and upper return bounds for rating classes $0, 1, 2,$  and $3$, which for instance, could respectively correspond to ``very bad'', ``bad'', ``good'', and ``very good''. In this case, if $0 \le \tilde{R}(\sigma) < \bar{R}_1$, then we would like our model to predict that $\sigma$ most likely belongs to class 0 (``very bad''), while if $\bar{R}_2 \le \tilde{R}(\sigma) < \bar{R}_3$, then our model should predict that $\sigma$ most likely belongs to class 2 (``good'').

%We next present how to model a human's probability of rating a segment $\sigma$ with class $i$: $Q_{\sigma}(i)$. To do so, we first model each trajectory segment's normalized discounted reward, and then learn a set of values in the space of normalized trajectory rewards that delineate the boundaries between the human rating categories.

%\newsec{Modeling Rating Probabilities}
Given the rating category separations $\bar{R}_i$, we model $Q_{\sigma}(i)$ as a function of the normalized predicted returns $\tilde{R}(\sigma)$:
%, defined in the next paragraph 
%to describe the relative high/low of the discounted cumulative rewards,  
%and rating category separations $\bar{R}_i$: 
%Note that the cumulative probability for one sample in one of the $n$ rating classes should be $1$, namely, $\sum_{i=0}^{n-1}Q_{\sigma}(i) = 1$. 
%Another requirement of defining $Q_{\sigma}(i)$ is to ensure that $Q_{\sigma}(i)$ always gives the highest probability for the rating class that its corresponding $\eta$ value falls within. For example, if $\eta(\sigma) \in [\bar{r}_i, \bar{r}_{i + 1}),$ this sample should have a rating of $i$. 
%We here propose to define $Q_{\sigma}(i)$ as:
\begin{equation}\label{eq:Qfun_new}
     Q_{\sigma}(i) = \frac{e^{-k(\tilde{R}(\sigma)-\bar{R}_i)(\tilde{R}(\sigma)-\bar{R}_{i + 1})}}{\sum_{j=0}^{n-1} e^{-k(\tilde{R}(\sigma)-\bar{R}_j)(\tilde{R}(\sigma)-\bar{R}_{j + 1})}},
\end{equation}
where $k$ is a hyperparameter modeling human label noisiness, 
%$\bar{R}_i,~i=0,\cdots,n$, are the quantitative values describing the boundaries of each rating class, 
and the denominator ensures that $\sum_{i=0}^{n-1}Q_{\sigma}(i) = 1$, i.e. that the class probabilities sum to 1.
%and $\underline{\eta}_i$ and $\bar{r}_i$ are the lower and upper bounds for the $i$th rating class. 

To gain intuition for Equation~\eqref{eq:Qfun_new}, note that when $\tilde{R}(\sigma) \in (\bar{R}_i, \bar{R}_{i + 1})$, such that the predicted return falls within rating class $i$'s predicted boundaries, then $-(\tilde{R}(\sigma)-\bar{R}_i)(\tilde{R}(\sigma)-\bar{R}_{i + 1})\ge 0$ while $-(\tilde{R}(\sigma)-\bar{R}_j)(\tilde{R}(\sigma)-\bar{R}_{j + 1})\le 0$ for all $j\neq i$. This means that $Q_{\sigma}(i)\ge Q_{\sigma}(j),~j\neq i$, so that the model assigns category $i$ the highest class probability, as desired. Furthermore, we note that $Q_{\sigma}(i)$ is maximized when $\tilde{R}(\sigma) = \frac{1}{2}(\bar{R}_i + \bar{R}_{i + 1})$, such that the predicted return falls directly in the center of category $i$'s predicted range. As $\tilde{R}(\sigma)$ becomes increasingly further from $\frac{1}{2}(\bar{R}_i + \bar{R}_{i + 1})$, the modeled probability $Q_\sigma(i)$ of class $i$ monotonically decreases. These probability trends are illustrated in Figure~\ref{fig:rating_class_probs} in the Appendix. %~\ref{sec:rateprob}. 
We next show how to compute the %normalized predicted return $\tilde{R}(\sigma)$ and the determination of 
class boundaries $\bar{R}_i,~i=1,\ldots,n - 1$.


\subsection{Modeling Boundaries between Rating Categories}
Next, we discuss how to model the boundaries between rating categories, $0 =: \bar{R}_0 \le \bar{R}_1 \le \ldots \le \bar{R}_n := 1$.
This requires selecting the range, or the upper and lower bounds of $\tilde{R}$, corresponding to each rating class.  
We determine these boundary values based on the distribution of $\tilde{R}(\sigma)$ for the trajectory segments $\sigma \in X$  and the number of observed samples in $X$ from each rating class. We select the $\bar{R}_i$ values such that 
% the lower and upper bounds for each rating class is selected such that
the number of training data samples that the model assigns to each modeled rating class matches the number of samples in $X$ that the human assigned to that rating class.
%\footnote{
Note that this does not require the predicted ratings based on $\tilde{R}(\sigma)$ to match the human ratings for $\sigma$ in the training data $X$, but 
%, namely the estimated reward $\hat{r}$, will match human ratings, but 
ensures that the proportions of segments in the training dataset $X$ assigned to each rating class matches that in $X$. %}
This matching in rating class proportions is desirable for learning an appropriate reward function based on human preference, since different humans could give ratings in significantly different proportions depending on their preferences and latent reward functions, as modeled by $\hat{R}$.
%Such a method to select the range of each rating is critical to learn an appropriate reward function based on human preference, because the distribution of $\eta$ can be drastically different for different samples. Hence, the selection of the range of each rating should be based on specific $\eta$ distribution to reflect individual's preference. 

%Considering the two rules, (1) a lower $\eta$ means a lower rating and (2) the number of samples in the range of $\eta$ for each rating class should match that from human ratings, 
To define each $\bar{R}_i$ so that the number of samples in each modeled rating category reflects the numbers of ratings in the human data,
we first sort the estimated returns $\tilde{R}(\sigma)$ for all $\sigma \in X$ from lowest to highest, and label these sorted estimates as $\tilde{R}_1 \le \tilde{R}_2 \le \cdots\le \tilde{R}_l$, where $l$ is the cardinality of $X$. 
Denoting via $k_j$ the number of segments that the human assigned to rating class $j,~j \in \{0,\cdots,n-1\}$, we can then model each category boundary $\bar{R}_i, i \notin \{0, n\}$ (since $\bar{R}_0 := 0$ and $\bar{R}_n := 1$ by definition), as follows:
\begin{equation}  \bar{R}_i = \frac{\tilde{R}_{k_{i-1}^{\text{cum}}} + \tilde{R}_{1 + k_{i-1}^{\text{cum}}}}{2},\quad i \in \{1, 2, \ldots, n - 1\},
\end{equation}
where $k_i^{\text{cum}} := \sum_{j = 0}^i k_j$ is the total number of segments that the human assigned to any rating category $j \le i$.
%For the rating class $0$, its lower bound can be selected as $0$ and the upper bound can be selected as $\frac{\tilde{r}_{k_0}+\tilde{r}_{k_0+1}}{2}$. 
When the user has not assigned any ratings within a particular category, i.e., $k_i = 0$ for some $i$, then we define the upper bound for category $i$ as $\bar{R}_{k_{i+1}} := \bar{R}_{k_{i}}$. %, such the lower and upper bounds for this rating category are identical.

This definition guarantees that when all normalized return predictions $\tilde{R}(\sigma), \sigma \in X,$ are distinct, then our model places $k_0$ segments within interval $[\bar{R}_0, \bar{R}_1)$, $k_i$ segments within each interval $(\bar{R}_i, \bar{R}_{i + 1})$ for $1 \le i \le n - 2$, and $k_{n-1}$ segments in $(\bar{R}_{n-1}, \bar{R}_n]$, and thus predicts that $k_i$ segments most likely have rating $i$.
%$[0,\frac{\tilde{r}_{k_0}+\tilde{r}_{k_0+1}}{2})$. Similarly, for the rating class $1$, its lower bound can be selected as $\frac{\tilde{r}_{k_0}+\tilde{r}_{k_0+1}}{2}$, which is the same as the upper bound of the rating $0$. Its upper bound can be selected as $\frac{\tilde{r}_{k_0+k_1}+\tilde{r}_{k_0+k_1+1}}{2}$. This will ensure that $k_1$ segments fall within the set $[\frac{\tilde{r}_{k_0}+\tilde{r}_{k_0+1}}{2},\frac{\tilde{r}_{k_0+k_1}+\tilde{r}_{k_0+k_1+1}}{2})$. A similar approach is applied to the assignment of lower bound and upper bound for other rating classes. The upper bound of the last rating class, e.g., $n-1$, is $1$.



%In fact, $Q_{\sigma}(i)$ can be much higher than $Q_{\sigma}(j),~j\neq i$, when $\eta$ is not equal to 



%%%%
% Section 4: Synthetic Experiments
% Section 5: Human Experiments
% Section 6: Discussion and Future Work



\section{Synthetic Experiments}

%We focus on answering a few questions: (1) Can we use human rating as a new mechanism to learn reward function and control policy? (2) Can RbRL perform better than by PbRL given the same number of human time? and (3) Is RbRL more demanding, discouraged, and stressed than PbRL? 

\subsection{Setup} 

We conduct synthetic experiments based on the setup in~\citet{Lee2021BPref} to evaluate RbRL relative to the PbRL baseline~\cite{Lee2021BPref}. The code can be found at \url{https://rb.gy/tdpc4y}. The goal is to learn to perform a task by obtaining feedback from a teacher, in this case a synthetic human. 
For the PbRL baseline, we generate synthetic feedback such that in each queried pair of segments, the segment with the higher ground truth cumulative reward is preferred.
In contrast to the synthetic preferences between sample pairs in~\citet{Lee2021BPref}, RbRL was given synthetic ratings generated for individual samples, where these ratings were given by comparing the sample's ground truth return to the ground truth rating class boundaries. For simplicity, we selected these ground truth rating class boundaries so that rating classes are evenly spaced in reward space. 

For the synthetic PbRL experiments, we selected preference queries using the ensemble disagreement approach in~\citet{Lee2021BPref}. We extend this method to select rating queries for the synthetic RbRL experiments, designing an ensemble-based approach as in~\citet{Lee2021BPref} to select trajectory segments for which to obtain synthetic ratings. 
First, we train a reward predictor ensemble and obtain the predicted reward for every candidate segment and ensemble member.
%which is followed by the calculation of the standard deviation for each segment. 
We then select the segment with the largest standard deviation over the ensemble to receive a rating label. We study the Walker and Quadruped tasks in~\citet{Lee2021BPref}, with 1000 and 2000 synthetic queries, respectively. %\textcolor{black}{1000 and 2000 queries were provided for Walker and Quadruped, respectively. }

For all synthetic experiments, the reward network parameters are optimized to minimize the cross entropy loss~\eqref{eq:crossentropy} based on the respective batch of data via the computation of~\eqref{eq:Qfun_new}. 
We use the same neural network structures for both the reward predictor and control policy and the same hyperparameters as in~\citet{Lee2021BPref}.

% Figure environment removed

\subsection{Results}

Figure \ref{fig:Bpref} shows the performance 
%mean rewards (solid lines) and standard errors (shaded areas) across 10 runs 
of RbRL for different numbers of rating classes (\textit{i.e.} values of $n$) and PbRL for two environments from~\citet{Lee2021BPref}: Walker and Quadruped. We observe that a higher number of rating classes yields better performance for  Walker. In addition, RbRL with $n=5,6$ outperforms PbRL. However, for Quadruped, while RbRL with $n=2,3$ still outperforms PbRL, a higher number of rating classes decreases performance; this decrease may be caused by the selection of rating class boundaries used to generate the synthetic feedback. The results indicate that RbRL is effective and can provide better performance than PbRL even if synthetic ratings feedback is generated using reward thresholds that are evenly distributed, without further optimization of their selection. We expect further optimization of the boundaries used to generate synthetic feedback to yield improved performance. %We believe that both of these results could be improved based on altering the rating class boundary. 
For our experiments, we defined the rating boundaries by finding the maximum possible reward range for a segment and evenly dividing by the number of rating classes.  


%\textcolor{red}{For our experiments, we designed a disagreement procedure which would work with ratings. In order to do this, we find the predicted reward for every segment based on the ensemble member, then calculate the standard deviation of the ensemble member to the other ensemble members for each segment. The segments with the largest standard deviation amongst the ensemble are then chosen and given a label. There are 2000 labels provided for all runs, and trained for 4 million timesteps. Rating labels are given based on the number of ratings and a max reward per segment. For example if there are 4 ratings and the max reward is 40, then 0-10 would be rating 1, 10-20 for ratings 2, 20-30 for ratings 3 and 30-40 for ratings 4. This allows us to hold consistent ratings categories across different batches. The graphs in Figure \ref{fig:CheetahMean}and \ref{fig:Cheetah} are the mean across 10 runs and the standard error for all 10 runs.} 

\section{Human Experiments}

\subsection{Setup} 
We conduct all human experiments by following a similar setup to~\citet{christiano2017deep}. In particular, our tests were approved by the UTSA IRB Office, including proper steps to ensure privacy and informed consent of all participants. In particular, the goal is to learn to perform a given task by obtaining feedback from a teacher, in this case a human. Different from PbRL in~\citet{christiano2017deep}, which asks humans to provide their preferences between sample pairs, typically in the form of short video segments, RbRL asks humans to evaluate individual samples, also in the form of short video segments, to provide their ratings, e.g., ``segment performance is good'' or ``segment performance is bad''. 

For all human experiments, 
we trained a reward predictor by minimizing the cross entropy loss~\eqref{eq:crossentropy} based on the respective batch of data via the computation of~\eqref{eq:Qfun_new}. 
We used the same neural network structures for both the reward predictor and control policy and the same hyperparameters as in~\citet{christiano2017deep}.

\subsection{RbRL with Different Numbers of Rating Classes}

%\textcolor{red}{- Mingkang's results} 
To evaluate the impact of the number of rating classes $n$ on RbRL's performance, we first conduct tests in which a human expert (an author on the study) provides ratings with $n=2,\ldots,8$ in the Cheetah MuJoCo environment. In particular, three experiment runs were conducted for each $n\in\{2,3,\ldots,8\}$. Fig.~\ref{fig:pvn} shows the performance of RbRL for each $n$.
%, where the solid bar represents the mean performance and the vertical line represents the variance
It can be observed that RbRL performs better for $n\in\{3,4,\ldots,7\}$ than for $n \in \{2,8\}$, indicating that allowing more rating classes is typically beneficial. However, an overly large number of rating classes $n$ will lead to difficulties and inaccuracies in the human ratings, and hence $n$ must be set to a reasonable value. Indeed, for smaller $n$, one can more intuitively assign physical meanings to each $n$, whereas for overly large $n$, it becomes difficult to assign such physical meanings, and hence it will be more challenging for users to provide consistent ratings.

% Figure environment removed





\subsection{RbRL Human User Study}

%\textcolor{red}{This subsection is too long. Need to reduce or move some contents to Appendix.}
To evaluate the effectiveness of RbRL for non-expert users, we conducted an IRB-approved human user study.
%One primary metric to compare RbRL and PbRL is to evaluate the agent performance under the same amount of human time. %In other words, we will compare the agent performance when participants are given 30min during the tests. 
We conducted tests on 3 of the OpenAI Gym MuJoCo Environments also used in~\citet{christiano2017deep}: Swimmer, Hopper and Cheetah. A total of 20 participants were recruited (7 for Cheetah, 7 for Swimmer, and 6 for Hopper). %Here is how we conducted the experiments to quantify human time. 
In our experiments, we provided a single 1 to 2 second long video segment to query users for each rating, while we provided pairs of 1 to 2 second videos to obtain human pairwise preferences. 
\textcolor{black}{For Cheetah, the goal is to move the agent to the right as fast as possible; this is the same goal encoded in the default hand-crafted environment reward. Similarly, the goal for Swimmer matches that of the default hand-crafted environment reward. However, for Hopper, we instructed users to teach the agent to perform a backflip, which differs from the goal encoded by the default hand-crafted environment reward. We chose to study the back flip task to see how well RbRL can learn new behaviors for which a reward is unknown.} Thus, the performance of Cheetah and Swimmer can be evaluated via the hand-crafted environment rewards, while the Hopper task cannot be evaluated via its hand-crafted environment reward. For Hopper, the performance of RbRL will be evaluated based on evaluating the agent's behavior when running the learned policies from RbRL. %In our tests, participants are provided 30 minutes to give their queries, \textit{i.e.}, ratings for RbRL and preferences for PbRL.

During the user study, each participant performed two tests---one for RbRL and one for PbRL---in one of the three MuJoCo environments, both for $n=2$ rating classes. To eliminate potential bias, we assigned each participant a randomized order in which to perform the PbRL and RbRL experiment runs. Because the participants had no prior knowledge of the MuJoCo environments tested, we provided sample videos to show desired and undesired behaviors so that the participants could better understand the task. 
Upon request, the participants could also conduct mock tests before we initiated human data collection. 
\textcolor{black}{For each experiment run, the participant was given 30 minutes to give rating/preference labels. Once finished, the participant filled out a questionnaire about the tested algorithm. The participant was then given a 10 minute break before conducting the second test and completing the questionnaire about the other algorithm. Afterwards, the participant completed a questionnaire comparing the two algorithms.} The questionnaires can be found in the Appendix. %~\ref{sec:ques}.
Policy and reward learning occurred during the 30 minutes in which the user answered queries, and then continued after the human stepped away until code execution reached 4 million environment time-steps. %\ellen{fill in} policy learning iterations.

 

\subsubsection{Performance} \label{sec:performance}

Figure~\ref{fig:meanreward} shows the performance of PbRL and RbRL %mean rewards and standard error 
across the seven participants for the Cheetah and Swimmer tasks. We see that RbRL performs similarly to or better than PbRL. In particular, RbRL can learn quickly in both cases, evidenced by the fast reward growth early during learning. Figure~\ref{fig:meanreward} additionally displays results when an expert (an author on the study) provided ratings and preferences for Cheetah and Swimmer.
%under the same setting as mentioned above. 
For consistency, the same expert tested PbRL and RbRL in each environment. We observe that for the expert trials, RbRL performs consistently better than PbRL given the same human time. These results suggest that RbRL can outperform PbRL regardless of the user's environment domain knowledge. It can also be observed that the RbRL and PbRL trials with expert users outperform the trials in which feedback is given by non-experts.


% Figure environment removed

%% Figure environment removed

 
%% Figure environment removed

%% Figure environment removed


Although RbRL performs similarly to PbRL in the Cheetah task, we observed that some participants performed very poorly in this environment, perhaps due to lack of understanding of the task. In the Appendix, the raw data of all participants for Cheetah and Swimmer is provided to show performance under each individual participant's guidance. From the individual results for Cheetah (RbRL), we can see that one of the trials performs very poorly (with the final reward less than $-10$). For all other tests, including both PbRL and RbRL, the final reward is in positive territory, usually more than $20$. 
Hence, it may be more meaningful to evaluate the mean results for individuals who perform reasonably. 
Figure~\ref{fig:top3} shows the mean reward for the top 3 non-expert users at different iterations for Cheetah and Swimmer. It can be observed that RbRL consistently outperforms PbRL and learns the goal faster than PbRL. 

To compare PbRL and RbRL in the Hopper backflip task, we ran the learned policies for the 6 participants to generate videos. Videos for the best learned policies from PbRL and RbRL can be found at \url{rb.gy/nt1qm6}, and indicate that (1) both RbRL and PbRL can learn the backflip, and (2) the backflip learned via RbRL fits better with our understanding of a desired backflip. %than that learned via PbRL.  
% short link: https://bit.ly/3wgF5OH

\subsubsection{User Questionnaire Results}

The previous results in this section focus on evaluating the performance of RbRL and PbRL via the ground-truth environment reward (Cheetah and Swimmer) and the learned behavior (Hopper). 
To understand how the non-expert users view their experience of giving rating and preferences, we conduct a post-experiment user questionnaire,
%. The specific questions in the questionnaire are 
shown in the last section of the Appendix. The questionnaire asked users for feedback about their experience supervising PbRL and RbRL and to compare PbRL and RbRL. %Here is how each questionnaire was processed. 
%For each questionnaire, we first convert each response to a value in the range from $0$ to $20$ and then normalize it between $0$ and $1$, where $0$ refers to the very left bar while $1$ refers to the very right bar. Then we compute the average of all participants' questionnaire and show the statistics in Figure~\ref{fig:survey}.
Figure~\ref{fig:survey} displays the normalized survey results from the 20 user study participants.
In particular, the top subfigure of Figure~\ref{fig:survey} shows the participants' responses with respect to their separate opinions about PbRL and RbRL. These responses suggest that PbRL is more demanding and difficult than RbRL, leading users to feel more insecure and discouraged than when using RbRL. The bottom subfigure of Figure~\ref{fig:survey} shows the survey responses when users were asked to compare PbRL and RbRL; these results confirm the above findings and also show that users perceive themselves as completing the task more successfully when providing ratings (RbRL). One interesting observation is that the participants prefer RbRL and PbRL equally, which differs from the other findings. However, one participant stated that he/she preferred PbRL because PbRL is more challenging, which is counter-intuitive. This suggests that ``liking'' one algorithm more than the other is a very subjective concept, making the responses for this question less informative than those for the other survey questions.

%We also conducted a quantitative analysis of the human time effectiveness when humans were asked to give ratings and preferences. These results are given in Appendix~\ref{sec:HumanTime}.
%when prompted with the question of "Which did you like more?" the results were split. We believe that this could be a poorly phrased question due to the subjective nature of the question. %In Figure 10 you can see the results from the questionnaire for both PbRL and RbRL as well as their Standard Error. Figure 11 shows the average response for the comparison Questionnaire as well as their Standard Error. 

% Figure environment removed


\subsubsection{Human Time}~\label{sec:HumanTime}
%Our last objective is to understand and quantify how effective for non-experts  to give ratings (for RbRL) and give preferences (for PbRL). 
We also conducted a quantitative analysis of human time effectiveness when humans were asked to give ratings and preferences. Figure~\ref{fig:time} shows the average number of human queries provided in 30 minutes for Cheetah, Swimmer, Hopper, and for all three environments combined. %The black vertical bar indicates the standard error. 
It can be observed that the participants can provide more ratings than pairwise preferences in all environments, indicating that it is easier and more efficient to provide ratings than to provide pairwise preferences. On average, participants can provide approximately 14.03 ratings per minute, while they provide only 8.7 preferences per minute, which means that providing a preference requires 62\% more time than providing a rating. For Cheetah, providing a preference requires 100\%+ more time than providing a rating, which is mainly due to the need to compare video pairs that are very similar. For Swimmer and Hopper, the environments and goals are somewhat more complicated. Hence, providing ratings can be slightly more challenging, but is still easier than providing pairwise preferences.

% Figure environment removed


\section{Discussion and Open Challenges}

%The proposed RbRL method utilizes human ratings of individual samples to guide the reward learning and policy learning, hence providing a new mechanism that does not rely on human's relative preferences over sample pairs. We will now provide some discussions about RbRL vs PbRL. 

%To give ratings in RbRL and preferences in PbRL, humans need to know the basics of individual environments. For ratings, humans may be given a few examples that perform differently. Humans may set his/her own standards of different ratings afterwards.

%In the environments that we tested, humans select their ratings based on the \textcolor{black}{agent's performance when navigating throughout a video segment}. For preferences, the human selects his/her preferred sample from a pair of samples. This requires more careful judgement regarding sample differences, especially when the pair is quite similar. Because it is challenging to distinguish between similar samples, the ratings of individual samples in RbRL can be more efficient than preferences over sample pairs in PbRL. This helps to explain why RbRL requires less human time than PbRL for the same number of human samples. 

One key difference between PbRL and RbRL is the value of the acquired human data. Because ratings in RbRL are not relative, they have the potential to provide more global value than preferences, especially when queries are not carefully selected. For environments with large state-action spaces, ratings can provide more value for reward learning. One limitation of ratings feedback is that the number of data samples in different rating classes can be very different, leading to imbalanced datasets. Reward learning in RbRL can be negatively impacted by this data imbalance issue (although our experiments still show the benefits of RbRL over PbRL). Hence, on-policy training with a large number of training steps may not help reward learning in RbRL because the collected human ratings data can become very unbalanced.
We expect that addressing the data imbalance issue would further improve RbRL performance.

One challenge for RbRL is that ratings may not be given consistently during learning, especially considering users' attention span and fatigue level over time. Future work includes developing mechanisms to quantify users' consistency levels, the impact of user inconsistency, or solutions to user inconsistency. %this is part of our future work.
Another potential limitation of RbRL is that it learns a less refined reward function than PbRL because RbRL does not seek to distinguish between samples from the same rating class. Hence, future work could integrate RbRL and PbRL to create a multi-phase learning strategy, where RbRL provides fast initial global learning while PbRL further refines performance via local queries based on sample pairs. 

One open challenge is the lack of effective human interfaces in existing code bases. For example, in~\citet{Lee2021BPref}, only synthetic human feedback is available. Although a human interface is available for the algorithm in~\citet{christiano2017deep}, the use of Google cloud makes it difficult to set up and operate efficiently. One of our future goals is to address this challenge by developing an effective human interface for reinforcement learning from human feedback, including preferences, ratings, and their variants. 

\section*{Acknowledgements}
The authors were supported in part by the Army Research Lab under grant W911NF2120232, Army Research Office under grant W911NF2110103, and Office of Naval Research under grant N000142212474. We thank Feng Tao, Van Ngo, Gabriella Forbis for their helpful feedback, code, and tests.

%Another interesting direction is to optimize the current RbRL approach by tuning the hyperparameters, such as the boundaries of different rating classes.
%Another interesting issue and open challenge is the problem of optimizing multiple objectives when giving ratings and preferences. In our current setting, we only focus on one objective. %For example, in Sea Quest, the agent needs to balance the need for collecting more rewards and moving up to collect oxygen. 
%Hence, it is difficult to compare two or more different behaviors via a single rating system. If multiple rating systems for different objectives are designed properly, the agent is expected to further learn how to balance the two different objectives. 
%We are interested in addressing rating-based multi-objective reinforcement learning as part of our future work.

%\section*{Acknowledgements}
%The authors were supported in part by Army Research Lab under grant W911NF2120232, Army Research Office under grant W911NF2110103, and Office of Naval Research under grant N000142212474. We would like to thank Feng Tao, Van Ngo, Gabriella Forbis who provided helpful feedback, code, and tests.



\bibliography{example_paper}


\newpage
\appendix
\onecolumn





%\section{Hyperparameters}\label{sec:hyper}

%The following two tables list the key hyperparameters used in the user study experiment. The hyperparameters are the same as those used in~\cite{christiano2017deep}.
%\begin{table}
%\begin{center}
%\caption{Hyperparameters of Rating-based Reinforcement Learning}
%\begin{tabular}{ |p{1.65cm}|p{2.05cm}|p{2.15cm}|p{1.65cm}|p{2.8cm}|p{1.65cm}|p{1.65cm}|  }
% \hline
%Noisiness
%in Human Labels ($k$) &Learning Rate & Minibatch Size & Training Steps & Entropy Coefficient & Parallel Workers & Policy Alpha \\
% \hline
% 30&$7*10^{-4}$ & 32  & 50 million & 0.01 & 4 & 0.99 \\
% \hline


%\end{tabular}
%\end{center}

%\begin{center}
%\begin{tabular}{ |p{2.0cm}|p{1.5cm}|p{2.2cm}|p{2.2cm}|p{2.5cm}|p{2.5cm}| }

 %\hline
 %5Policy Epsilon & Number of Agents & Initial$~~~$ Ratings & Max Number of Ratings & A2C log interval & Reward Predictor Learning Rate \\
 %\hline
 %$10^{-5}$ & 4 & 500  & 5500 & 100 & $2e^{-4}$\\
 %\hline
%\end{tabular}
%\end{center}
%\end{table}

\section{RbRL Implementation Details}

All tests and experiments of our RbRL and PbRL algorithms were trained for $4$ million time-steps with a segment length of $50$. For the Walker environment, 1,000 synthetic labels were provided with the reward predictor being updated every 20,000 time-steps. For the Quadruped environment, 2,000 synthetic labels were provided with the reward predictor being updated every 30,000 time-steps. All other hyper-parameters are the same as those in~\citet{Lee2021BPref}.

\section{Raw Data for Agent Performance from Individual Participants in the User Study}\label{sec:Raw}

Figure~\ref{fig:human_study_results_individual} displays the individual reward curves for non-expert users who participated in the human user study, for the Cheetah and Swimmer tasks.

% Figure environment removed

%\section{Expert Results}\label{sec:expert}
 %\begin{tabular}{c}
  %% Figure removed
  %\\
  %Reward at different iterations for Cheetah with rating and preferences given by experts
  %&
  %% Figure removed
    %\\ Reward at different iterations for Swimmer with rating and preferences given by experts
 %\end{tabular}

\newpage

\section{Modeling Rating Class Probabilities} \label{sec:rateprob}

Figure~\ref{fig:rating_class_probs} provides an intuitive illustration of the modeled class probabilities $Q_\sigma(i)$, as calculated via Equation~\eqref{eq:Qfun_new}, with specific relevant parameter choices detailed in the figure caption.

% Figure environment removed



\section{Questionnaire}\label{sec:ques}

The three-part questionnaire given to the user study participants appears next.

\includepdf[pages=-]{USQ4.pdf}






%\section{Physical Meaning for Different $n$} \label{sec:PhyMean}

%Table~\ref{tab:Different rating classes} displays the physical meanings of the rating classes used in our human experiments for all numbers of rating classes (values of $n$) considered.

%\begin{table*}[hhhhh]
%    \centering
%    \begin{tabular}{|c|c|}
%    \hline Number of Rating Classes & Rating Classes  \\
%    \hline $n = 2$ & ``good", ``bad" \\
%    \hline $n = 3$ &``good", "neutral","bad" \\
%    \hline $n = 4$ & ``very good", ``good", ``bad", ``very bad" \\
%    \hline $n = 5$ & ``very good", ``good", ``neutral",``bad", ``very bad" \\
%    \hline $n = 6$ & ``very good", ``good", ``slightly good",  ``slightly bad",``bad", ``very bad" \\
%    \hline $n = 7$ & ``very good", ``good", ``slightly good", ``neutral''  ``slightly bad",``bad", ``very bad"  \\
%    \hline $n = 8$ & ``perfect'', ``very good", ``good", ``slightly good", ``neutral''  ``slightly bad",``bad", ``very bad" \\ 
%    \hline
%    \end{tabular}
%    \caption{Physical meaning of rating classes for $n=2,3,\ldots,8$ used in our human experiments.}
%    \label{tab:Different rating classes}
%\end{table*}

\end{document}
