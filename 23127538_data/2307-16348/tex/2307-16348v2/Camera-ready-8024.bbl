\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman,
  and Man{\'e}}]{amodei2016concrete}
Amodei, D.; Olah, C.; Steinhardt, J.; Christiano, P.; Schulman, J.; and
  Man{\'e}, D. 2016.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}.

\bibitem[{B{\i}y{\i}k et~al.(2022)B{\i}y{\i}k, Losey, Palan, Landolfi,
  Shevchuk, and Sadigh}]{biyik2022learning}
B{\i}y{\i}k, E.; Losey, D.~P.; Palan, M.; Landolfi, N.~C.; Shevchuk, G.; and
  Sadigh, D. 2022.
\newblock Learning reward functions from diverse sources of human feedback:
  Optimally integrating demonstrations and preferences.
\newblock \emph{The International Journal of Robotics Research}, 41(1): 45--67.

\bibitem[{B{\i}y{\i}k et~al.(2020)B{\i}y{\i}k, Palan, Landolfi, Losey, Sadigh
  et~al.}]{biyik2020asking}
B{\i}y{\i}k, E.; Palan, M.; Landolfi, N.~C.; Losey, D.~P.; Sadigh, D.; et~al.
  2020.
\newblock Asking Easy Questions: A User-Friendly Approach to Active Reward
  Learning.
\newblock In \emph{Conference on Robot Learning}, 1177--1190.

\bibitem[{B{\i}y{\i}k, Talati, and Sadigh(2022)}]{biyik2022aprel}
B{\i}y{\i}k, E.; Talati, A.; and Sadigh, D. 2022.
\newblock {APReL}: A library for active preference-based reward learning
  algorithms.
\newblock In \emph{ACM/IEEE International Conference on Human-Robot Interaction
  (HRI)}, 613--617.

\bibitem[{Brown et~al.(2020)Brown, Coleman, Srinivasan, and
  Niekum}]{brown2020safe}
Brown, D.; Coleman, R.; Srinivasan, R.; and Niekum, S. 2020.
\newblock Safe imitation learning via fast {B}ayesian reward inference from
  preferences.
\newblock In \emph{International Conference on Machine Learning}, 1165--1177.

\bibitem[{Brown et~al.(2019)Brown, Goo, Nagarajan, and
  Niekum}]{brown2019extrapolating}
Brown, D.; Goo, W.; Nagarajan, P.; and Niekum, S. 2019.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 783--792.

\bibitem[{Brown, Goo, and Niekum(2020)}]{brown2020better}
Brown, D.~S.; Goo, W.; and Niekum, S. 2020.
\newblock Better-than-demonstrator imitation learning via automatically-ranked
  demonstrations.
\newblock In \emph{Conference on Robot Learning}, 330--359.

\bibitem[{Celemin and Ruiz-del Solar(2015)}]{celemin2015coach}
Celemin, C.; and Ruiz-del Solar, J. 2015.
\newblock {COACH}: Learning continuous actions from corrective advice
  communicated by humans.
\newblock In \emph{2015 International Conference on Advanced Robotics (ICAR)},
  581--586.

\bibitem[{Choi and Kim(2011)}]{choi2011map}
Choi, J.; and Kim, K.-E. 2011.
\newblock {MAP} inference for {B}ayesian inverse reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 24.

\bibitem[{Choi and Kim(2012)}]{choi2012nonparametric}
Choi, J.; and Kim, K.-E. 2012.
\newblock Nonparametric {B}ayesian inverse reinforcement learning for multiple
  reward functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 25.

\bibitem[{Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei}]{christiano2017deep}
Christiano, P.~F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.; and Amodei, D.
  2017.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Ecoffet et~al.(2019)Ecoffet, Huizinga, Lehman, Stanley, and
  Clune}]{ecoffet2019go}
Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K.~O.; and Clune, J. 2019.
\newblock Go-explore: a new approach for hard-exploration problems.
\newblock \emph{arXiv preprint arXiv:1901.10995}.

\bibitem[{Finn, Levine, and Abbeel(2016)}]{finn2016guided}
Finn, C.; Levine, S.; and Abbeel, P. 2016.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 49--58.

\bibitem[{Fu, Luo, and Levine(2018)}]{fu2018learning}
Fu, J.; Luo, K.; and Levine, S. 2018.
\newblock Learning Robust Rewards with Adverserial Inverse Reinforcement
  Learning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine}]{haarnoja2018soft}
Haarnoja, T.; Zhou, A.; Abbeel, P.; and Levine, S. 2018.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 1861--1870.

\bibitem[{Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and
  Amodei}]{ibarz2018reward}
Ibarz, B.; Leike, J.; Pohlen, T.; Irving, G.; Legg, S.; and Amodei, D. 2018.
\newblock Reward learning from human preferences and demonstrations in {A}tari.
\newblock \emph{Advances in Neural Information Processing Systems}, 31.

\bibitem[{Knox and Stone(2009)}]{knox2009interactively}
Knox, W.~B.; and Stone, P. 2009.
\newblock Interactively shaping agents via human reinforcement: The {TAMER}
  framework.
\newblock In \emph{Proceedings of the fifth international conference on
  Knowledge capture}, 9--16.

\bibitem[{Lee et~al.(2021)Lee, Smith, Dragan, and Abbeel}]{Lee2021BPref}
Lee, K.; Smith, L.; Dragan, A.; and Abbeel, P. 2021.
\newblock {B-Pref}: Benchmarking Preference-Based Reinforcement Learning.
\newblock \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Lee, Smith, and Abbeel(2021)}]{lee2021pebble}
Lee, K.; Smith, L.~M.; and Abbeel, P. 2021.
\newblock {PEBBLE}: Feedback-Efficient Interactive Reinforcement Learning via
  Relabeling Experience and Unsupervised Pre-training.
\newblock In \emph{International Conference on Machine Learning}, 6152--6163.

\bibitem[{Levine, Popovic, and Koltun(2011)}]{levine2011nonlinear}
Levine, S.; Popovic, Z.; and Koltun, V. 2011.
\newblock Nonlinear inverse reinforcement learning with {G}aussian processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 24.

\bibitem[{Li(2019)}]{li2019reinforcement}
Li, Y. 2019.
\newblock Reinforcement learning applications.
\newblock \emph{arXiv preprint arXiv:1908.06973}.

\bibitem[{Liang et~al.(2022)Liang, Shu, Lee, and Abbeel}]{liang2021reward}
Liang, X.; Shu, K.; Lee, K.; and Abbeel, P. 2022.
\newblock Reward Uncertainty for Exploration in Preference-based Reinforcement
  Learning.
\newblock In \emph{International Conference on Learning Representation}.

\bibitem[{Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra}]{lillicrap2015continuous}
Lillicrap, T.~P.; Hunt, J.~J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.;
  Silver, D.; and Wierstra, D. 2015.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{International Conference on Learning Representations}.

\bibitem[{Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski et~al.}]{mnih2015human}
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.~A.; Veness, J.; Bellemare,
  M.~G.; Graves, A.; Riedmiller, M.; Fidjeland, A.~K.; Ostrovski, G.; et~al.
  2015.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518(7540): 529--533.

\bibitem[{Ng, Russell et~al.(2000)}]{ng2000algorithms}
Ng, A.~Y.; Russell, S.~J.; et~al. 2000.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2.

\bibitem[{Park et~al.(2022)Park, Seo, Shin, Lee, Abbeel, and
  Lee}]{park2022surf}
Park, J.; Seo, Y.; Shin, J.; Lee, H.; Abbeel, P.; and Lee, K. 2022.
\newblock {SURF}: Semi-supervised Reward Learning with Data Augmentation for
  Feedback-efficient Preference-based Reinforcement Learning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov}]{schulman2017proximal}
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}.

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot
  et~al.}]{silver2016mastering}
Silver, D.; Huang, A.; Maddison, C.~J.; Guez, A.; Sifre, L.; Van Den~Driessche,
  G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.;
  et~al. 2016.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock \emph{Nature}, 529(7587): 484.

\bibitem[{Szot et~al.(2022)Szot, Zhang, Batra, Kira, and Meier}]{szot2022bc}
Szot, A.; Zhang, A.; Batra, D.; Kira, Z.; and Meier, F. 2022.
\newblock {BC-IRL}: Learning Generalizable Reward Functions from
  Demonstrations.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Warnell et~al.(2018)Warnell, Waytowich, Lawhern, and
  Stone}]{warnell2018deep}
Warnell, G.; Waytowich, N.; Lawhern, V.; and Stone, P. 2018.
\newblock Deep {TAMER}: Interactive agent shaping in high-dimensional state
  spaces.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32.

\bibitem[{Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz
  et~al.}]{wirth2017survey}
Wirth, C.; Akrour, R.; Neumann, G.; F{\"u}rnkranz, J.; et~al. 2017.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18(136): 1--46.

\bibitem[{Wulfmeier, Ondruska, and Posner(2015)}]{wulfmeier2015maximum}
Wulfmeier, M.; Ondruska, P.; and Posner, I. 2015.
\newblock Maximum entropy deep inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1507.04888}.

\bibitem[{Xu et~al.(2020)Xu, Wang, Yang, Singh, and
  Dubrawski}]{xu2020preference}
Xu, Y.; Wang, R.; Yang, L.; Singh, A.; and Dubrawski, A. 2020.
\newblock Preference-based reinforcement learning with finite-time guarantees.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:
  18784--18794.

\bibitem[{Zhan, Tao, and Cao(2021)}]{zhan2021human}
Zhan, H.; Tao, F.; and Cao, Y. 2021.
\newblock Human-guided Robot Behavior Learning: A {GAN}-assisted
  Preference-based Reinforcement Learning Approach.
\newblock \emph{IEEE Robotics and Automation Letters}, 6(2): 3545--3552.

\bibitem[{Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey}]{ziebart2008maximum}
Ziebart, B.~D.; Maas, A.; Bagnell, J.~A.; and Dey, A.~K. 2008.
\newblock Maximum entropy inverse reinforcement learning.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}.

\end{thebibliography}
