@article{silver2016mastering,
  title={Mastering the game of {Go} with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016}
}


@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={International Conference on
Learning Representations},
  year={2015}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}


@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={1861--1870},
  year={2018}
}

@article{li2019reinforcement,
  title={Reinforcement learning applications},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1908.06973},
  year={2019}
}


@article{machado2018revisiting,
  title={Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents},
  author={Machado, Marlos C and Bellemare, Marc G and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={523--562},
  year={2018}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning},
  author={Ng, Andrew Y and Russell, Stuart J and others},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={2},
  year={2000}
}

@article{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning},
  author={Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2008}
}

@inproceedings{finn2016guided,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={49--58},
  year={2016}
}


@article{wirth2017survey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes and others},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={136},
  pages={1--46},
  year={2017}
}


@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@inproceedings{weng2011markov,
  title={Markov decision processes with ordinal rewards: Reference point-based preferences},
  author={Weng, Paul},
  booktitle={Twenty-First International Conference on Automated Planning and Scheduling},
  year={2011}
}

@inproceedings{daniel2014active,
  title={Active Reward Learning.},
  author={Daniel, Christian and Viering, Malte and Metz, Jan and Kroemer, Oliver and Peters, Jan},
  booktitle={Robotics: Science and Systems},
  volume={98},
  year={2014}
}

@inproceedings{brown2019extrapolating,
  title={Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations},
  author={Brown, Daniel and Goo, Wonjoon and Nagarajan, Prabhat and Niekum, Scott},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={783--792},
  year={2019}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Proceedings of the International Conference on Machine Learning},
  volume={99},
  pages={278--287},
  year={1999}
}


@article{ecoffet2019go,
  title={Go-explore: a new approach for hard-exploration problems},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1901.10995},
  year={2019}
}
















@article{arora2021survey,
  title={A survey of inverse reinforcement learning: Challenges, methods and progress},
  author={Arora, Saurabh and Doshi, Prashant},
  journal={Artificial Intelligence},
  volume={297},
  pages={103500},
  year={2021}
}

@article{wulfmeier2015maximum,
  title={Maximum entropy deep inverse reinforcement learning},
  author={Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},
  journal={arXiv preprint arXiv:1507.04888},
  year={2015}
}

@article{choi2011map,
  title={Map inference for {B}ayesian inverse reinforcement learning},
  author={Choi, Jaedeug and Kim, Kee-Eung},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}

@article{levine2011nonlinear,
  title={Nonlinear inverse reinforcement learning with {G}aussian processes},
  author={Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}

@article{choi2012nonparametric,
  title={Nonparametric {B}ayesian inverse reinforcement learning for multiple reward functions},
  author={Choi, Jaedeug and Kim, Kee-Eung},
  journal={Advances in Neural Information Processing Systems},
  volume={25},
  year={2012}
}

@article{Lee2021BPref,
  title={{B-Pref}: Benchmarking Preference-Based Reinforcement Learning},
  author={Lee, Kimin and Smith, Laura and Dragan, Anca and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{liang2021reward,
  title={Reward Uncertainty for Exploration in Preference-based Reinforcement Learning},
  author={Liang, Xinran and Shu, Katherine and Lee, Kimin and Abbeel, Pieter},
  booktitle={International Conference on Learning Representation},
  year={2022}
}

@article{zhan2021human,
  title={Human-guided Robot Behavior Learning: A {GAN}-assisted Preference-based Reinforcement Learning Approach},
  author={Zhan, Huixin and Tao, Feng and Cao, Yongcan},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={2},
  pages={3545--3552},
  year={2021}
}

@article{xu2020preference,
  title={Preference-based reinforcement learning with finite-time guarantees},
  author={Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18784--18794},
  year={2020}
}

@inproceedings{brown2020better,
  title={Better-than-demonstrator imitation learning via automatically-ranked demonstrations},
  author={Brown, Daniel S and Goo, Wonjoon and Niekum, Scott},
  booktitle={Conference on Robot Learning},
  pages={330--359},
  year={2020}
}

@article{pomerleau1991efficient,
  title={Efficient training of artificial neural networks for autonomous navigation},
  author={Pomerleau, Dean A},
  journal={Neural Computation},
  volume={3},
  number={1},
  pages={88--97},
  year={1991}
}

@article{ibarz2018reward,
  title={Reward learning from human preferences and demonstrations in {A}tari},
  author={Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{brown2020safe,
  title={Safe imitation learning via fast {B}ayesian reward inference from preferences},
  author={Brown, Daniel and Coleman, Russell and Srinivasan, Ravi and Niekum, Scott},
  booktitle={International Conference on Machine Learning},
  pages={1165--1177},
  year={2020}
}

@article{amodei2016concrete,
  title={Concrete problems in {AI} safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@inproceedings{biyik2022aprel,
  title={{APReL}: A library for active preference-based reward learning algorithms},
  author={B{\i}y{\i}k, Erdem and Talati, Aditi and Sadigh, Dorsa},
  booktitle={ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={613--617},
  year={2022}
}

@inproceedings{biyik2020asking,
  title={Asking Easy Questions: A User-Friendly Approach to Active Reward Learning},
  author={B{\i}y{\i}k, Erdem and Palan, Malayandi and Landolfi, Nicholas C and Losey, Dylan P and Sadigh, Dorsa and others},
  booktitle={Conference on Robot Learning},
  pages={1177--1190},
  year={2020}
}

@article{biyik2022learning,
  title={Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences},
  author={B{\i}y{\i}k, Erdem and Losey, Dylan P and Palan, Malayandi and Landolfi, Nicholas C and Shevchuk, Gleb and Sadigh, Dorsa},
  journal={The International Journal of Robotics Research},
  volume={41},
  number={1},
  pages={45--67},
  year={2022}
}

@inproceedings{lee2021pebble,
  title={{PEBBLE}: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training},
  author={Lee, Kimin and Smith, Laura M and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={6152--6163},
  year={2021}
}

@inproceedings{park2022surf,
  title={{SURF}: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning},
  author={Park, Jongjin and Seo, Younggyo and Shin, Jinwoo and Lee, Honglak and Abbeel, Pieter and Lee, Kimin},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{warnell2018deep,
  title={Deep {TAMER}: Interactive agent shaping in high-dimensional state spaces},
  author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}

@inproceedings{knox2009interactively,
  title={Interactively shaping agents via human reinforcement: The {TAMER} framework},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={Proceedings of the fifth international conference on Knowledge capture},
  pages={9--16},
  year={2009}
}

@inproceedings{celemin2015coach,
  title={{COACH}: Learning continuous actions from corrective advice communicated by humans},
  author={Celemin, Carlos and Ruiz-del-Solar, Javier},
  booktitle={2015 International Conference on Advanced Robotics (ICAR)},
  pages={581--586},
  year={2015}
}

@inproceedings{szot2022bc,
  title={{BC-IRL}: Learning Generalizable Reward Functions from Demonstrations},
  author={Szot, Andrew and Zhang, Amy and Batra, Dhruv and Kira, Zsolt and Meier, Franziska},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{fu2018learning,
  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

