\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and
  Man{\'e}, D.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[B{\i}y{\i}k et~al.(2020)B{\i}y{\i}k, Palan, Landolfi, Losey, Sadigh,
  et~al.]{biyik2020asking}
B{\i}y{\i}k, E., Palan, M., Landolfi, N.~C., Losey, D.~P., Sadigh, D., et~al.
\newblock Asking easy questions: A user-friendly approach to active reward
  learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1177--1190, 2020.

\bibitem[B{\i}y{\i}k et~al.(2022{\natexlab{a}})B{\i}y{\i}k, Losey, Palan,
  Landolfi, Shevchuk, and Sadigh]{biyik2022learning}
B{\i}y{\i}k, E., Losey, D.~P., Palan, M., Landolfi, N.~C., Shevchuk, G., and
  Sadigh, D.
\newblock Learning reward functions from diverse sources of human feedback:
  Optimally integrating demonstrations and preferences.
\newblock \emph{The International Journal of Robotics Research}, 41\penalty0
  (1):\penalty0 45--67, 2022{\natexlab{a}}.

\bibitem[B{\i}y{\i}k et~al.(2022{\natexlab{b}})B{\i}y{\i}k, Talati, and
  Sadigh]{biyik2022aprel}
B{\i}y{\i}k, E., Talati, A., and Sadigh, D.
\newblock {APReL}: A library for active preference-based reward learning
  algorithms.
\newblock In \emph{ACM/IEEE International Conference on Human-Robot Interaction
  (HRI)}, pp.\  613--617, 2022{\natexlab{b}}.

\bibitem[Brown et~al.(2019)Brown, Goo, Nagarajan, and
  Niekum]{brown2019extrapolating}
Brown, D., Goo, W., Nagarajan, P., and Niekum, S.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pp.\  783--792, 2019.

\bibitem[Brown et~al.(2020{\natexlab{a}})Brown, Coleman, Srinivasan, and
  Niekum]{brown2020safe}
Brown, D., Coleman, R., Srinivasan, R., and Niekum, S.
\newblock Safe imitation learning via fast {B}ayesian reward inference from
  preferences.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1165--1177, 2020{\natexlab{a}}.

\bibitem[Brown et~al.(2020{\natexlab{b}})Brown, Goo, and
  Niekum]{brown2020better}
Brown, D.~S., Goo, W., and Niekum, S.
\newblock Better-than-demonstrator imitation learning via automatically-ranked
  demonstrations.
\newblock In \emph{Conference on Robot Learning}, pp.\  330--359,
  2020{\natexlab{b}}.

\bibitem[Celemin \& Ruiz-del Solar(2015)Celemin and Ruiz-del
  Solar]{celemin2015coach}
Celemin, C. and Ruiz-del Solar, J.
\newblock {COACH}: Learning continuous actions from corrective advice
  communicated by humans.
\newblock In \emph{2015 International Conference on Advanced Robotics (ICAR)},
  pp.\  581--586, 2015.

\bibitem[Choi \& Kim(2011)Choi and Kim]{choi2011map}
Choi, J. and Kim, K.-E.
\newblock Map inference for {B}ayesian inverse reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Choi \& Kim(2012)Choi and Kim]{choi2012nonparametric}
Choi, J. and Kim, K.-E.
\newblock Nonparametric {B}ayesian inverse reinforcement learning for multiple
  reward functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pp.\  49--58, 2016.

\bibitem[Fu et~al.(2018)Fu, Luo, and Levine]{fu2018learning}
Fu, J., Luo, K., and Levine, S.
\newblock Learning robust rewards with adverserial inverse reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pp.\  1861--1870, 2018.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and
  Amodei]{ibarz2018reward}
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D.
\newblock Reward learning from human preferences and demonstrations in {A}tari.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Knox \& Stone(2009)Knox and Stone]{knox2009interactively}
Knox, W.~B. and Stone, P.
\newblock Interactively shaping agents via human reinforcement: The {TAMER}
  framework.
\newblock In \emph{Proceedings of the fifth international conference on
  Knowledge capture}, pp.\  9--16, 2009.

\bibitem[Lee et~al.(2021{\natexlab{a}})Lee, Smith, Dragan, and
  Abbeel]{Lee2021BPref}
Lee, K., Smith, L., Dragan, A., and Abbeel, P.
\newblock {B-Pref}: Benchmarking preference-based reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{a}}.

\bibitem[Lee et~al.(2021{\natexlab{b}})Lee, Smith, and Abbeel]{lee2021pebble}
Lee, K., Smith, L.~M., and Abbeel, P.
\newblock {PEBBLE}: Feedback-efficient interactive reinforcement learning via
  relabeling experience and unsupervised pre-training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6152--6163, 2021{\natexlab{b}}.

\bibitem[Levine et~al.(2011)Levine, Popovic, and Koltun]{levine2011nonlinear}
Levine, S., Popovic, Z., and Koltun, V.
\newblock Nonlinear inverse reinforcement learning with {G}aussian processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Li(2019)]{li2019reinforcement}
Li, Y.
\newblock Reinforcement learning applications.
\newblock \emph{arXiv preprint arXiv:1908.06973}, 2019.

\bibitem[Liang et~al.(2022)Liang, Shu, Lee, and Abbeel]{liang2021reward}
Liang, X., Shu, K., Lee, K., and Abbeel, P.
\newblock Reward uncertainty for exploration in preference-based reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representation}, 2022.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Ng, A.~Y., Russell, S.~J., et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pp.\ ~2, 2000.

\bibitem[Park et~al.(2022)Park, Seo, Shin, Lee, Abbeel, and Lee]{park2022surf}
Park, J., Seo, Y., Shin, J., Lee, H., Abbeel, P., and Lee, K.
\newblock {SURF}: Semi-supervised reward learning with data augmentation for
  feedback-efficient preference-based reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Szot et~al.(2022)Szot, Zhang, Batra, Kira, and Meier]{szot2022bc}
Szot, A., Zhang, A., Batra, D., Kira, Z., and Meier, F.
\newblock {BC-IRL}: Learning generalizable reward functions from
  demonstrations.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Warnell et~al.(2018)Warnell, Waytowich, Lawhern, and
  Stone]{warnell2018deep}
Warnell, G., Waytowich, N., Lawhern, V., and Stone, P.
\newblock Deep {TAMER}: Interactive agent shaping in high-dimensional state
  spaces.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz,
  et~al.]{wirth2017survey}
Wirth, C., Akrour, R., Neumann, G., F{\"u}rnkranz, J., et~al.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (136):\penalty0 1--46, 2017.

\bibitem[Wulfmeier et~al.(2015)Wulfmeier, Ondruska, and
  Posner]{wulfmeier2015maximum}
Wulfmeier, M., Ondruska, P., and Posner, I.
\newblock Maximum entropy deep inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1507.04888}, 2015.

\bibitem[Xu et~al.(2020)Xu, Wang, Yang, Singh, and Dubrawski]{xu2020preference}
Xu, Y., Wang, R., Yang, L., Singh, A., and Dubrawski, A.
\newblock Preference-based reinforcement learning with finite-time guarantees.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18784--18794, 2020.

\bibitem[Zhan et~al.(2021)Zhan, Tao, and Cao]{zhan2021human}
Zhan, H., Tao, F., and Cao, Y.
\newblock Human-guided robot behavior learning: A {GAN}-assisted
  preference-based reinforcement learning approach.
\newblock \emph{IEEE Robotics and Automation Letters}, 6\penalty0 (2):\penalty0
  3545--3552, 2021.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  2008.

\end{thebibliography}
