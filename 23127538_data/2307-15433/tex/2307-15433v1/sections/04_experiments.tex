%!TEX root = ../main.tex

\section{Baseline results}
\label{sec:results}

We performed detection and classification experiments to produce the first baseline results on the presented datasets.
Species classification is only done on the EU-Moths dataset since the NID dataset has only bounding box annotations so far.
For evaluating the species classifier, we utilized the ground-truth bounding box annotations and only used the cropped image patches as inputs.

We repeated each experiment ten times and provided in Tables~\ref{tab:detection_results} and~\ref{tab:classification_results} the mean and standard deviation of the evaluation metrics across different runs.
We fine-tuned all models for \num{60} epochs and L2-regularization with a weight decay of~\num{5e-4}.
For both models, we utilized standard image augmentation methods: random cropping, random horizontal and vertical flipping, and color jittering (contrast, brightness, and saturation).

The SSD model was trained with an AdamW~\cite{AdamW} optimizer and a learning rate of \num{1e-3} for all epochs.
We used the VGG16~\cite{simonyan2014very} backbone architecture pre-trained on the ImageNet~\cite{ImageNet} with an input size of $300$px for the EU-Moths dataset and $512$px for the NID dataset.

The classification model was trained with an \mbox{RMSProp}~\cite{tieleman2012lecture} optimizer with an initial learning rate of \num{1e-4}, reduced by \num{0.1} after \num{20} and \num{40} epochs.
Further, we utilized label smoothing~\cite{inception} with a smoothing factor of \num{0.1}.
We used the InceptionV3 CNN architecture~\cite{inception} with the default input size of $299$px.
Additionally, we used two different pre-training methods.
Besides the typical ImageNet~\cite{ImageNet} pre-training, we used a pre-training on the iNaturalist~\cite{iNaturalist} dataset provided by Cui~\etal~\cite{Cui_2018_CVPR_large}.
Finally, we extract additional parts, as described in Sect.~\ref{sec:meth:fgvc}, and combine the predictions on these parts with the predictions on the entire image.

\subsection{Insect detection}
\label{sub:detection_results}

First, we report the detection performance on both datasets in Table~\ref{tab:detection_results} and we use mean average precision (mAP) as the evaluation metric.
The precision is computed based on two different intersection over union (IoU) thresholds of the predicted and the ground-truth bounding boxes.
The IoU-thresholds \num{0.5} and \num{0.75} (corresponding mAP denoted as \emph{mAP@0.50} and \emph{mAP@0.75}) are two typical choices used in the MC-COCO object detection benchmark~\cite{lin2014mscoco}.

\begin{table}[t]
	\begin{center}
	\begin{tabular}{lcc}
		\toprule
		 & \thead{mAP@0.75} & \thead{mAP@0.50} \\
		\midrule
		\thead{EU-Moths}
			& \num{88.88} \scriptsize{$(\pm \num{0.77})$}
			& \num{99.01} \scriptsize{$(\pm \num{0.09})$} \\
		\thead{NID Dataset}
			& \num{26.19} \scriptsize{$(\pm \num{5.64})$}
			& \num{91.21} \scriptsize{$(\pm \num{0.34})$} \\
		\bottomrule
	\end{tabular}
	\caption{
		Detection results on the EU-Moths and NID datasets.
	}
	\label{tab:detection_results}
	\end{center}
\end{table}

Based on the results, the SSD method performs significantly better on the EU-Moths dataset.
The baseline detector for the NID dataset achieves a stable mean-average precision of over \pcent{90} for the \emph{mAP@0.50} metric.
Nevertheless, the detector performs much worse for the more precise metric, the \emph{mAP@0.75}.
A possible explanation for this may be many small insects (as seen in Figure~\ref{fig:example}), where minor discrepancies between the prediction and ground truth degrade the results.
Even though we increased the input size for this dataset, the small sizes of some insects represent a challenge for the applied detection model.

% % Figure environment removed

% Finally, we visualized the predicted bounding boxes for a selection of validation images in Figure~\ref{fig:example_detections}.
% The ground truth boxes are drawn in \emph{black}, the estimated boxes are \emph{blue}, and the resulting IoU between the prediction and ground truth is displayed near the boxes.

\subsection{Species classification}
\label{sub:classification_results}

Table~\ref{tab:classification_results} shows the classification accuracies on the EU-Moth dataset for different setups.
First, we can observe the effect of the pre-training on different datasets.
Data used in the pre-training proposed by Cui~\etal~\cite{Cui_2018_CVPR_large} is more related to the domain of insects, and we can see this benefit in our reported results.

\begin{table}[t]
	\centering
	\begin{tabular}{lcc}
		\toprule
		 %& \multicolumn{2}{c}{\thead{Accuracy (in \%)}} \\
		 & \thead{ImageNet} & \thead{iNaturalist} \\		 
		 & \thead{pre-training} & \thead{pre-training} \\
		\midrule
		\thead{No Parts}

			& \num{89.46} \scriptsize{$(\pm \num{0.88})$}
			& \num{90.54} \scriptsize{$(\pm \num{1.10})$} \\
		\thead{With Parts}
			& \num{91.50} \scriptsize{$(\pm \num{0.61})$}
			& \num{93.13} \scriptsize{$(\pm \num{0.76})$} \\
		\bottomrule
	\end{tabular}
	\caption{
		Baseline classification results (accuracy in \%) on cropped images of the EU-Moths dataset.
		% The results show the effects of the two pre-training datasets, and the usage of additional information in the form of parts.
	}
	\label{tab:classification_results}
\end{table}

Finally, utilizing methods for additional information extraction in the form of parts also improves the classification performance by approximately \pcent{2}, as Table~\ref{tab:classification_results} shows.
We achieved the best results using the part-based approach: \pcent{91.50} and \pcent{93.13} with ImagenNet and iNaturalist pre-training, respectively.


\endinput

Our Blob Detector:
╒══════════╤═════════╤═════════╕
│ Metric   │ Score   │ Score   │
│          │  Tr-set │  Val-set│
╞══════════╪═════════╪═════════╡
│ mAP@25   │ xx.xx%  │ 37.23%  │
├──────────┼─────────┼─────────┤
│ mAP@33   │ 41.28%  │ 31.57%  │
├──────────┼─────────┼─────────┤
│ mAP@50   │ 17.50%  │ 16.40%  │
├──────────┼─────────┼─────────┤
│ mAP@75   │ 0.99%   │ 1.80%   │
╘══════════╧═════════╧═════════╛

SSD Results (evaluation preset):
│ mAP@75   │ 17,85%  │
│ mAP@75   │ 29,93%  │
│ mAP@75   │ 26,57%  │
│ mAP@75   │ 33,00%  │
│ mAP@75   │ 29,34%  │
│ mAP@75   │ 24,13%  │
│ mAP@75   │ 28,53%  │
│ mAP@75   │ 15,37%  │
│ mAP@75   │ 26,60%  │
│ mAP@75   │ 30,54%  │
avg: 26,19% +/- 5.64

│ mAP@50   │ 91,71%  │
│ mAP@50   │ 91,19%  │
│ mAP@50   │ 91,01%  │
│ mAP@50   │ 91,63%  │
│ mAP@50   │ 90,93%  │
│ mAP@50   │ 91,08%  │
│ mAP@50   │ 91,64%  │
│ mAP@50   │ 91,26%  │
│ mAP@50   │ 90,90%  │
│ mAP@50   │ 90,74%  │
avg: 91,21% +/- 0.34

│ mAP@33   │ 96,26%  │
│ mAP@33   │ 96,33%  │
│ mAP@33   │ 96,67%  │
│ mAP@33   │ 96,86%  │
│ mAP@33   │ 96,08%  │
│ mAP@33   │ 96,22%  │
│ mAP@33   │ 96,66%  │
│ mAP@33   │ 96,41%  │
│ mAP@33   │ 94,81%  │
│ mAP@33   │ 96,43%  │
avg: 96,27%



SSD Results (visualization preset):
│ mAP@75   │ 14.50%  │
│ mAP@75   │ 25.27%  │
│ mAP@75   │ 23.56%  │
│ mAP@75   │ 27.83%  │
│ mAP@75   │ 24.82%  │
│ mAP@75   │ 20.65%  │
│ mAP@75   │ 25.30%  │
│ mAP@75   │ 12.97%  │
│ mAP@75   │ 22.78%  │
│ mAP@75   │ 26.58%  │
avg: 22.43%

│ mAP@50   │ 64.92%  │
│ mAP@50   │ 66.71%  │
│ mAP@50   │ 70.58%  │
│ mAP@50   │ 63.57%  │
│ mAP@50   │ 58.80%  │
│ mAP@50   │ 63.59%  │
│ mAP@50   │ 66.10%  │
│ mAP@50   │ 63.20%  │
│ mAP@50   │ 64.65%  │
│ mAP@50   │ 63.07%  │
avg: 64.52%

│ mAP@33   │ 67.09%  │
│ mAP@33   │ 68.69%  │
│ mAP@33   │ 73.20%  │
│ mAP@33   │ 65.91%  │
│ mAP@33   │ 61.30%  │
│ mAP@33   │ 65.64%  │
│ mAP@33   │ 68.57%  │
│ mAP@33   │ 64.82%  │
│ mAP@33   │ 66.30%  │
│ mAP@33   │ 66.23%  │
avg: 66.78%


MCC:
╒══════════╤═════════╕
│ Metric   │ Score   │
╞══════════╪═════════╡
│ mAP@25   │ 12.36%  │
├──────────┼─────────┤
│ mAP@33   │ 8.01%   │
├──────────┼─────────┤
│ mAP@50   │ 1.89%   │
├──────────┼─────────┤
│ mAP@75   │ 0.10%   │
╘══════════╧═════════╛
