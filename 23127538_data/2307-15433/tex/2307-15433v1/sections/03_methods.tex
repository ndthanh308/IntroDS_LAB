%!TEX root = ../main.tex

\section{The datasets}
\label{sec:dataset}

In the following, we present two datasets that we are using within our project for moth detection and species recognition.
They contain images of nocturnal insects, primarily moths, photographed on a white background.
In contrast to other publicly available datasets constructed for species classification such as iNaturalist~\cite{iNaturalist}, standardized camera trap images lead to a more homogeneous setting: the insects are photographed on a uni-color planar surface.

% Figure environment removed

\noindent\textbf{European moths (EU-Moths) dataset\footnote{\scriptsize{\url{https://inf-cv.uni-jena.de/eu_moths_dataset}}}:}\quad
This dataset consists of \num{200} species most common in Central Europe.
Each of the species is represented by approximately \num{11} images.
We consider a random but balanced split in eight training and three test images per species, resulting in roughly \num{1600} training and \num{600} test images in total.
Furthermore, we manually annotated bounding boxes for each insect.
For this dataset, citizen scientists photographed the insects manually and mainly on a relatively homogeneous background.
About \pcent{92} of the images contain only a single individual like it is shown in the first two rows of Figure~\ref{fig:examples:eumoths}.
The last row depicts images with more than one insect of interest.

\noindent\textbf{Nocturnal insects dataset (NID)\footnote{\scriptsize{\url{https://inf-cv.uni-jena.de/nid_dataset}}}:}\quad
Our camera trap setup takes high-resolution images of the insects resting on an illuminated surface.
We use a UV-LED lamp since this is the most attractive radiation for nocturnal insects than white light~\cite{brehm2017new}.
A 20-megapixel camera captured the images at an interval of two minutes (setup shown in Figure~\ref{fig:prototype}).

In five months (June - October \num{2021}), the system captured images during \num{95} nights, and we removed empty images without any insects at the beginning and the end of every night.
In total, we gathered more than \num{27000} images.

We first selected images from ten nights equally distributed over the entire period and manually annotated bounding boxes around insects in \num{818} images to evaluate detection methods.
As a result, we ended up with \num{9095} bounding box annotations.
Figure~\ref{fig:example} shows one of the images with two exemplary bounding boxes and the corresponding image patches.
In our baseline experiments, we use the first five nights for training and parameter tuning and data from the last five nights for the evaluation.

% Figure environment removed

\section{Baseline methods}
\label{sec:methods}
As presented in preliminary work~\cite{Korsch21_DLP}, we deploy a two-stage pipeline for moth species detection and classification: (1) insect localization and (2) fine-grained moth species identification.
This separation is vital since the later prototypes will operate autonomously in the field and transmit the gathered images to central storage.
To reduce the amount of transmitted data, we will perform the detection directly at the moth scanner and transfer only the small image patches to the central storage.

\subsection{Single-shot detector}
\label{sub:meth:single_shot_detector}
We used a CNN-based state-of-the-art object detection model, namely the single-shot MultiBox detector (SSD) proposed by Liu~\etal~\cite{liu2016ssd}.
The authors utilize feature maps from multiple intermediate stages of the backbone CNN to predict location offsets and class confidences for a set of prior locations.
For more details about the loss functions, we refer to the original paper of Liu~\etal~\cite{liu2016ssd}.

\subsection{Fine-grained species classification}
\label{sec:meth:fgvc}

Neural networks, especially CNNs, yield state-of-the-art results in image classification tasks.
As we presented in our previous work~\cite{Korsch19_CSPARTS}, one can utilize a linear classifier with a sparsity-inducing L1-regularization to identify the most informative feature subsets of a high-dimensional (e.g., \num{2048} in case of InceptionV3) feature vector.
In combination with gradient maps~\cite{simonyan2013deep}, we use this subset of features to identify the regions of interest, the so-called saliency map, for an input image.
Afterward, we estimate with $k$-means clustering the spatial extent of coherent regions based on the identified saliency map and place bounding boxes around each region. 
The image patches of these bounding boxes serve as an unsupervised part representation, i.e., each region corresponds to a single part. 
These detected parts are finally used as additional input for the CNN classifier.
We refer to our previous work~\cite{Korsch19_CSPARTS,Korsch21_DLP} for more detail about the method and implementation details.

\endinput
\todo{only copied from the other paper}

Nowadays, neural networks like CNNs yield the best results in classification by extracting high-level features from the input image in the form of a high-dimensional feature vector (e.g., $D=2048$ in case of InceptionV3).
In the context of a fine-grained recognition task, the classifier has to focus on a specific feature dimension to distinguish a class from the others.
Therefore, we first estimate the most informative features for the current classification task.
It is realized by utilizing a linear classifier with a sparsity-inducing L1-regularization.
An optimization with L1-regularization forces the classifier's decisions to perform the classification on a small subset of feature dimensions.
This kind of implicit feature selection is classification-specific.
Furthermore, it allows identifying for each class the feature dimensions that best distinguish this class from all other classes by selecting dimensions with classifier weights above a certain threshold.

\noindent\textbf{Informative Image Regions:}\quad
% In this step, we identify most important regions in the image with the respect to the classification decision.
We utilize gradient maps~\cite{simonyan2013deep} to estimate the most informative pixels in the image, identified by large gradients.
As described previously, we restrict the computation of the gradients only to the feature dimensions used by the L1-regularized classifier.
Thus, we incorporate the initial classification in the estimation of the part regions.
Like Simonyan \etal~\cite{simonyan2013deep} and Simon \etal~\cite{Simon_2015_ICCV}, we use back-propagation through the CNN to identify the regions of interest for each selected feature dimension.
We compute a saliency map $\vec{M}(\vec{I})$ for an image $\vec{I}$ based on the feature dimension subset $\mathfrak{D} \subset \{1, \dots, D\}$ as follows:

\begin{equation}
	\label{eq:saliency_sum}
	M_{x,y}(\vec{I})
	  = \dfrac{1}{|\mathfrak{D}|} \sum_{d \in \mathfrak{D}} \left| \dfrac{\partial}{\partial I_{x,y}} f^{(d)}(\vec{I}) \right|
	  % = \dfrac{1}{|\mathfrak{D}|} \sum_{d \in \mathfrak{D}} \left| \dfrac{\partial}{\partial I_{x,y}} \dfrac{1}{s \cdot u}\sum_{j=1}^s \sum_{j'=1}^u F_{j,j'}^{(d)}(\vec{I}) \right|
	  \quad  .
\end{equation}

\noindent\textbf{Part Estimation:}\quad
Next, we normalize the values of the saliency map to the range $[0 \dots 1]$, and discard regions of low saliency by setting values beneath the mean saliency value to $0$.
We use the resulting sparse saliency map to estimate the spatial extent of coherent regions.
Like Zhang \etal~\cite{zhang2019unsupervised}, we achieve this by \mbox{$k$-means} clustering of pixel coordinates $(x, y)$ and the saliencies $M_{x,y}$ (Eq.~\ref{eq:saliency_sum}).
Additionally, we also consider the RGB values at the corresponding positions in the input image.
The clusters are initialized with $k$ peaks computed by non-maximum suppression, identifying locations with the largest saliencies.
Consequently, the number of peaks determines the number of parts to detect.
Finally, it is straightforward to identify a bounding box around each estimated cluster, and the resulting bounding boxes serve as parts for the following classification.

\noindent\textbf{Extraction and Aggregation of Part Features:}\quad
In the final step, we extract image patches with the help of the estimated bounding boxes and treat them as regular images.
The neural network should extract different features from these image patches than from the original image because the level of detail varies between these types of input.
Therefore, we process the part images by the same CNN architecture as the original image but with a separate set of weights.
Afterward, for every part image, the CNN extracts a feature vector, resulting in a set of part features for every single image.
There are different ways to aggregate these features to a single feature vector and perform the classification.
We have chosen to average over the part features, which results in a single feature vector with the same dimension as for the original image.
This aggregation strategy yielded better results in our experiments than, for example, concatenation of part features.
Finally, classification is performed based on the global feature and the aggregated part feature.
For joint optimization of both CNNs, we average the cross-entropy losses of the global prediction $\vec{p}$ and part prediction $\vec{q}$.
It equals to computing the geometrical mean of normalized class probabilities and enforces both classifiers to be certain about the correct class:

\begin{align}
	L_{final}\left(\left\{\vec{p},\vec{q}\right\}, y\right)
		&= \frac{1}{2}\left(L\left(\vec{p}, y\right) + L\left(\vec{q}, y\right) \right) \\
		&= -\frac{1}{2}\left(
			\sum^{C}_{i=1} y_i \cdot \log (p_{i}) +
			\sum^{C}_{i=1} y_i \cdot \log (q_{i}) \right)\\
		&= -\sum^{C}_{i=1} y_i \cdot \log \left(
			\sqrt{p_i \cdot q_i}
		\right) \quad .
\end{align}


\subsection{Blob Detector and Its Improvements} % (fold)
\label{sub:blob_detector_and_its_improvements}

Even though the application of a CNN-based detector offers an obvious out-of-the box solution, it comes with a big drawbacks.
First, the training of the CNN is known to require a large amount of data.
Next, to ensure that also small insects are detected properly we needed to use the larger input size of the SSD, namely $512 \times 512$px.
This results in much higher GPU memory consumption during the training.
Finally, the amount of computations required to process a single image is very high~\cite{canziani2016analysis} compared to basic computer vision operations.
\todo{e.g. filtering, edge detection, contour detection, and similar algorithms}

Under the assumption that the insects are located on a uni-color planar surface one can simplify the task from a general object detection to a more simpler blob detection task.
The main idea of a blob detector is a generic algorithm of (1) image pre-processing, (2) image binarization, and (3) contour and bounding-box estimation.
All of these steps are based on basic computer vision algorithms.
A prominent example was presented by Bjerge~\etal~\cite{bjerge2021automated}.
The authors perform first a background subtraction followed by a binarization and a contour estimation step.
In their work, the authors mention that the bounding box estimation was unsuccessful when multiple individuals were too close to each other.

In contrast to the detector of Bjerge~\etal, our detection algorithm does not require a background image for the pre-processing.
We assume the background is the part of the image containing only low-frequency information and subtracting it from the original image is equally to a high-pass filter operation.
As the result, we first perform high-pass filtering and binarize the result by applying global mean thresholding.
We experimented with other thresholding methods (e.g., Otsu's method or local Gaussian method), but due to the before-mentioned pre-processing, the global mean thresholding performed most stable.
Afterwards we apply morphological operations to remove the thresholding noise and then we estimate the bounding boxes by estimating contours in the binary image.
For the case when multiple individuals are enclosed in a single bounding box, we perform the blob detection again for every estimated bounding box.
Finally, we remove redundant bounding boxes with the non-maximum suppression.

