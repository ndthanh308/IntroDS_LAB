%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins
\usepackage{graphicx}
\usepackage{algorithm}

\usepackage{xcolor}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\xiaohan}[1]{{\textcolor[rgb]{0.8,0.3,0.2}{[XZ: {\it #1}]}}}
\newcommand{\shiqi}[1]{{\textcolor[rgb]{0.1,0.6,0.1}{[Shiqi: {\it #1}]}}}

\newcommand{\yifeng}[1]{{\textcolor[rgb]{0.8,0.1,0.1}{[Yifeng: {\it #1}]}}}

\newcommand{\yan}[1]{{\textcolor[rgb]{0.6,0.1,0.6}{[Yan: {\it #1}]}}}

\newcommand{\commentp}[1]{{\textcolor[rgb]{0.6,0.1,0.6}{[Peter: {\it #1}]}}}


% Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Symbolic State Space Optimization for Long Horizon \\Mobile Manipulation Planning}

\author{Xiaohan Zhang$^1$, Yifeng Zhu$^2$, Yan Ding$^1$, Yuqian Jiang$^2$, Yuke Zhu$^2$, Peter Stone$^{2,3}$, Shiqi Zhang$^1$% <-this % stops a space
\thanks{$^1$~Department of Computer Science, The State University of New York at Binghamton \texttt{\{xzhan244; yding25; zhangs\}@binghamton.edu}}
\thanks{$^2$~Department of Computer Science, The University of Texas at Austin \texttt{\{yifeng.zhu@; jiangyuqian@; yukez@cs.; pstone@cs.\}utexas.edu}}
\thanks{$^3$~Sony AI}
%}
}

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
In existing task and motion planning (TAMP) research, it is a common assumption that experts manually specify the state space for task-level planning.  
A well-developed state space enables the desirable distribution of limited computational resources between task planning and motion planning.  
However, developing such task-level state spaces can be non-trivial in practice.  
In this paper, we consider a long horizon mobile manipulation domain including repeated navigation and manipulation.
We propose Symbolic State Space Optimization~(S3O) for computing a set of abstracted locations and their 2D geometric groundings for generating task-motion plans in such domains.
Our approach has been extensively evaluated in simulation and demonstrated on a real mobile manipulator working on clearing up dining tables. 
Results show the superiority of the proposed method over TAMP baselines in task completion rate and execution time. 
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
% \xiaohan{\# TODO:
% \begin{itemize}
%     \item consecutive ->> long horizon
%     \item S3O
%     \item cite pose estimation work
%     % \item we propose S3O (full name) for computing task and motion plans for long horizon mobile manipulation. 
%     \item We propose symbolic state space optimization (S3O) for computing a set of symbolic locations and their 2D geometric groundings for task and motion planning (TAMP) in long horizon mobile manipulation domains. 
%     \item Framework split it into a few sections
%     \item real robot demo needs to require more from task planning
% \end{itemize}
% }
% Task and motion planning (TAMP) algorithms and systems have been used for robot long-horizon planning at both discrete and continuous levels~\cite{lagriffoul2018platform,garrett2021integrated}.

At the task level, robots frequently use symbolic planners to sequence high-level actions~\cite{ghallab2016automated}.
At the motion level, each high-level action is grounded to low-level trajectories in continuous spaces using motion planners~\cite{choset2005principles}. 
TAMP algorithms aim to bridge the gap between task planning and motion planning towards enabling robots to fulfill task-level goals and maintain motion-level feasibility at the same time~\cite{lagriffoul2018platform,garrett2021integrated}.
% TAMP algorithms aim to bridge the gap between task planning and motion planning towards enabling robots to fulfill task-level goals and maintain motion-level feasibility at the same time~\cite{toussaint2015logic, zhu2020hierarchical, garrett2018ffrob, jiang2019task, thomas2021mptp, wells2019learning, migimatsu2020object, mcmahon2017robot, zhao2018reactive, ding2022learning, ding2020task}.
A common and widely accepted assumption for most TAMP research is that the task planner is predefined by a domain expert who manually specifies a symbolic state space.
In this paper, we discuss TAMP in a long horizon mobile manipulation domain where the robot is given a task of repeated navigation to perform manipulation behaviors (e.g., pick and place) in different places. 
% TAMP systems would define a symbolic state space where objects (i.e., $o$) are initially located in different abstracted locations (i.e., $l$).
% % such \texttt{at($o_1$, $l_1$)} and \texttt{at($o_2$, $l_2$)}.
% Locations $l$ are further mapped to different sets of obstacle-free 2D poses, where each pose specifies a 2D position and an orientation.
 
% 1. TAMP \& TAMP has predefined symbolic state space
% 2. 
%     b. example --- TAMP for mobile manipulation (our setting)
%     c. justify why we need cross-space search
%     d. we focus on a more challenging setting -- real robots, action uncertainty, also from perception
% 3. 
%     a. we propose a approach to search over the state spaces of task planners 
%     naive approach
%     our heurisitics

%     b. without predefining state space to efficiently optimize task-motion plans, where the plan quality in success rate and task completion rate and efficiency execution time
    
Nevertheless, manually constructing state spaces might not be desirable in some scenarios.
% For instance, one state variable $\mathcal{X}$ for a GTAMP-MM problem is \texttt{at($o_1$)}, indicating the current location of object $o_1$.
% The value domain of $\mathcal{X}$ is a set of symbolic locations in which each symbol maps to a set of obstacle-free 2D poses, where each pose specifies a 2D position and an orientation.
% For example, the robot might be given a task to pick up multiple objects in the environment, as shown in Figure~\ref{}.
Fig.~\ref{fig:domain} shows a situation where in long horizon mobile manipulation domains, if each object is placed at a separate symbolic location as defined in the task-level state space, the robot will always need to navigate before picking up the next object.
This is because the task planner believes only a navigation action can bring the robot to the location required by the next manipulation action. 
% , since from a task planner's perspective, only a navigation action can bring the robot to the next location that is the same as the next object's location. 
In practice, however, the robot often picks up multiple objects from a single position, for example, as restaurant waiters can easily identify a standing location that allows them to pick up multiple dishes at once.  
Especially when objects are located close to each other, it is unnecessary for the robot to navigate before every manipulation. 
This observation motivated the development of this research on optimizing symbolic state spaces for task planners to best facilitate TAMP for long horizon mobile manipulation. 
% \textbf{To this end, TAMP algorithms are preferred to not only search in a single state space, but equip with cross-space search capabilities to approach global optimality.}
% A common assumption for TAMP research is that the planner has access to perfect world models, relying on which the robot can compute a feasible task-motion plan.
% \xiaohan{action uncertainty}

% % Figure environment removed


% Figure environment removed


One of the challenges in optimizing symbolic state spaces for TAMP, which is the focus of this work, comes from the uncertainties in action and perception. 
We consider failures in navigation and manipulation behaviors, e.g., due to the robot being too close to obstacles or too far from the target objects.
% To this end, we propose a novel TAMP approach that optimizes task planners based on probabilistically evaluated motion-level feasibility under uncertainty.
To this end, we propose \textbf{S}ymbolic \textbf{S}tate \textbf{S}pace \textbf{O}ptimization~(S3O) based on probabilistically evaluated action feasibility under uncertainty.
S3O partitions the continuous configuration space into a set of abstracted locations with their 2D geometric groundings to compute efficient and feasible task-motion plans in long horizon mobile manipulation domains.

Fig.~\ref{fig:s3o_overview} shows an overview of S3O which first constructs a candidate set of object-centric symbolic state spaces using Voronoi Partitioning~\cite{okabe1997locational}.
% \commentp{are we really considering different planners?  Or just different state spaces for the planner?}
Then the algorithm ranks each state space by a scoring function developed using feasibility evaluation from robot perception.
% As the number of objects increases, the size of the task planner candidate set will exponentially blow up.
The ranking mechanism effectively reduces the search complexity of state spaces by controlling the size of the candidate set.
% by overcoming the issue of an exponentially blowing up size as the number of objects increases. 
% \yifeng{Does your method blow up the size or not?}
% To improve task-level search efficiency, we rank and re-weigh each task planner in the candidate set based on motion-level feasibility.
% The feasibility evaluator in our system is trained using a visually grounded TAMP method~\cite{zhang2021planning}.
% we use a recently-developed visually grounded method~\cite{zhang2021planning} to probabilistically evaluate action feasibility re-weigh task planners for speeding up the outer loop search.
Top-ranked state spaces are used for constructing the task planner in the TAMP system where we further apply an Evolution Strategy~(ES) algorithm~\cite{hansen2003reducing} for efficient motion-level search.
% where plan feasibility and efficiency are served as its objective.
% One recent research introduced a visually grounded method, where they trained lightweight learning models for probabilistically evaluating action feasibility on an offline dataset~\cite{zhang2022visually}.
% One recent research used fully convolutional networks (FCNs)~\cite{long2015fully} to evaluate action feasibility in consecutive mobile manipulation domains~\cite{zhang2022visually}.
% where they trained lightweight learning models for probabilistically evaluating action feasibility on an offline dataset.
% Their feasibility evaluators show effectiveness in a predefined task planner scenario.
% We apply similar evaluators and propose a novel TAMP approach that optimizes task planners based on probabilistically evaluated motion-level feasibility under uncertainty.
% In this paper, we explore cross-space search strategies in a more challenging TAMP setting in which action and perception uncertainties are considered for mobile manipulation, and we are aiming to improve execution-time robot performance~\cite{zhang2022visually}.
% When deploying a TAMP system in the real world, due to uncertainty in robot perception and actuation, one can hardly say a feasible plan at planning time is absolutely executable at run time.
% Thus, it is important to take into account real-world uncertainty for the development of TAMP algorithms.  
% In this paper, we use Grounded Task and Motion Planning (GTAMP) to refer to a group of TAMP research that mainly focuses on connecting the planning framework to the real world and improving execution-time robot performance. 
% GTAMP can be applied to both manipulation and navigation domains (including actions that take extended periods of time).
% Action feasibility evaluation plays a significant role in the former setting, and plan efficiency is much more important to consider in the latter setting since an efficient plan is more appreciated at robot execution time.
% In this research, we aim to balance feasibility with efficiency and focus on GTAMP as applied to \textbf{mobile manipulation domains} (GTAMP-MM), where it is difficult to deal with feasibility evaluation for mobile manipulation skills, and also challenging to optimize over action feasibility with plan efficiency.
% \textbf{Our first contribution of this paper is to present the formal definition of GTAMP-MM including complete statements of assumptions and objectives.} 
% Recent work on GTAMP-MM introduced a visually grounded method, where they trained a lightweight learning model on an offline dataset consisting of visual observations~\cite{zhang2022visually}.
% Their models show effectiveness to predict the feasibility of a navigation and manipulation action pair for planning-time optimization.
% To this end, \textbf{configuring a symbolic state space that suits the current task is critical for GTAMP-MM problems}.    
% A trivial cross-space search strategy is to enumerate all possible symbolic state spaces for constructing task and motion planners. 
% In the given example, Voronoi Partitioning~\cite{okabe1997locational} can be used to separate the 2D continuous space into different symbolic areas by object positions, and further, one can consider area merging options as different state spaces.
% In doing so, the number of state spaces will be exponentially growing when the number of objects keeps increasing, resulting in a very complicated search problem for TAMP: 1) the outer search loop selects one of the task planners (defined by a state space); 2) the middle loop includes task-level sequencing (e.g, the permutation of objects); 3) the inner loop is for searching at the 2D continuous level.
% \textbf{Thus, we propose a sampling-based TAMP framework to efficiently optimize task-motion plans without predefining the state space, where the plan quality is evaluated by task completion rate and robot execution time.}
The proposed framework has been quantitatively evaluated in simulation and qualitatively demonstrated on real hardware, where the robot works on the task of ``clearing up dining tables.''
% Robots that are used in simulation and real world are shown in Fig.~\ref{fig:robot}.
Compared to existing TAMP baselines, experimental results show that our approach consistently leads to high-quality task-motion plans in terms of task completion rate and plan execution time.



% % Figure environment removed

% Figure environment removed

\section{Related Work}
% \commentp{There should be at least a sentence here.}
In this section, we describe the three most related research areas, including those task and motion planning (TAMP) methods that optimize plan efficiency and feasibility, research that learns symbolic representations for robot planning, and the application domain of long horizon mobile manipulation. 

\subsection{TAMP for Efficient and Feasible Behaviors}
Task and motion planning research can be categorized into two groups: one includes high-level actions that take no more than a few seconds (e.g., picking up, putting down and pushing objects), the other requires robot actions taking relatively long time (e.g., long-term navigation)~\cite{lo2020petlon}.
The former type of TAMP has a long history in the literature and focuses mostly on action feasibility~\cite{toussaint2015logic, zhu2020hierarchical, garrett2018ffrob, wells2019learning, migimatsu2020object, mcmahon2017robot, zhao2018reactive}, while some recent methods have considered behavioral efficiency in the latter type of TAMP, usually in robot navigation, autonomous driving, or mobile manipulation domains~\cite{lo2020petlon,jiang2019task,ding2020task,thomas2021mptp,zhang2022visually, ding2022glad, ding2023task}.
One common assumption for these TAMP methods is the predefined task planner.
Unlike those methods, we probabilistically compute action feasibility via visual perception to optimize the state space of the task planner. 

% we focus on optimizing task planners based on motion-level feasibility that is evaluated via visual perception.
% We observe that an optimized task planner achieves a better trade-off between action feasibility and efficiency.
% \commentp{This still seems awkward to me.  It sounds like we're altering the planning algorithm itself.}

\subsection{Symbol Learning for Robot Planning}
Learning-based methods have shown effectiveness in model acquisition and symbol generation for robot planning.
Researchers have learned action preconditions and effects models for enabling purely symbolic planning~\cite{konidaris2018skills} and integrated task-motion planning~\cite{wang2021learning}.
Some other work focuses on symbol learning and mapping, such as connecting natural language to learned symbolic abstractions~\cite{gopalan2020simultaneously}, learning state abstractions for bootstrapping motion planning~\cite{shah2022using}, and learning to ground the physical meanings of object attribute symbols in the real world~\cite{ding2022learning}.
In our work, we also learn to generate and map each symbol from the continuous space, but going beyond that, we further optimize the efficiency and feasibility of task-motion plans for the robot to execute using the learned symbols.

\subsection{Long Horizon Mobile Manipulation}
There is rich literature on learning and planning coordinated actions for mobile manipulation~\cite{yamamoto1992coordinating, thakar2023survey}. 
Most existing methods focused on positioning the base of a mobile manipulator in such a way that manipulability is maximized~\cite{berenson2008optimization,diankov2008openrave,stulp2012learning,ren2016method}.
A convincing technique is using robot reachability maps~\cite{zacharias2008positioning,vahrenkamp2013robot}. 
Recent research applies reinforcement learning in a hierarchical style to tackle this problem~\cite{jauhri2022robot, gu2022multi,xia2020relmogen,li2020hrl4in, lew2022robotic}.
In this paper, we not only consider coordinated navigation and manipulation, but also optimize a sequence of mobile manipulation actions over a long horizon. Sequential mobile manipulation tasks~\cite{carriker1991path} have been studied, including works that aimed to minimize platform movements to reach a set of poses in the workspace~\cite{xu2020planning} or to minimize the overall cost of completing the task~\cite{reister2022combining}. 
As compared to our approach, we also consider perception and assume execution-time uncertainty from both perception and actuation.

% % Figure environment removed
\section{Problem Statement}
\label{sec:problem}
We present the terminologies, assumptions, and objectives of the TAMP problem we focus on in this research: a long horizon mobile manipulation task where the robot repeatedly navigates and picks up multiple objects in different locations.
% \subsection{Domain Description}
% In this subsection, we first present the formal problem description by introducing symbols, actions, and robot perception components.
% % \commentp{What do you mean by "applied domain"?  Isn't this where we describe the formal problem description?}
\vspace{0.5em}

\noindent\textbf{Symbols and Symbol Mapping: }
% \yan{leave some space for this headline, including other headlines?}
$\mathcal{O} = \{o_1,o_2 ...\}$ is a set of target objects that can be moved by a robot.
$\mathcal{L} = \{l_1,l_2 ... \}$ is a set of symbolic locations.
Let $y \in \mathcal{Y}$ be a set of xy poses in continuous space.
% $\mathcal{L}$ is usually manually specified by a domain expert but is unknown in this research. 
% $\mathcal{S}$ is a set of task-level states describing the entire domain.
% Considering a task of moving object $o_1$ from $l_1$ to $l_2$, the initial and goal states will include \texttt{at($o_1$, $l_1$)} and \texttt{at($o_1$, $l_2$)} respectively.  
$Sym: \mathcal{Y}\rightarrow \mathcal{L}$ is a function that maps any 2D geometric position $y \in \mathcal{Y}$ to a symbolic location $l \in \mathcal{L}$.
The symbolic state space of our problem is defined in the form of $\langle \mathcal{L}, Sym, \mathcal{Y} \rangle$. 
% \commentp{Doesn't $\mathcal{Y}$ also play a role?}

% We assume $Sym$ to be unknown as well.\\

\noindent\textbf{Actions: }
The robot is equipped with skills of performing a set of actions denoted as $\mathcal{A}: \mathcal{A}^n \cup \mathcal{A}^m$, where $\mathcal{A}^n$ and $\mathcal{A}^m$ are \emph{navigation} actions and \emph{manipulation} actions respectively.
A navigation action 
$a^n: \langle l_r, l'_r, y_r, y'_r \rangle \in \mathcal{A}^n$ 
is specified at both low and high levels: 1) the robot's current and next symbolic locations that are denoted as $l_r, l'_r\in \mathcal{L}$; 2) the corresponding 2D coordinates $y_r, y'_r$ mapped by $Sym$.
$r$ is a symbol to denote ``robot'' as being distinguished from symbol $o$ for ``object''.
A manipulation action
$a^m: \langle o, l, y_r, y_o \rangle \in \mathcal{A}^m$
is specified by an object (i.e., $o$) to be manipulated, the object's 2D location, $y_o$, the object's and the robot's symbolic location, $l \in \mathcal{L}$.
%Note that the object and the robot are required to be co-located so that the robot can perform the manipulation action.
The robot and the object to be manipulated should be in the same symbolic location.
In this work, we consider \texttt{pickup} as a manipulation action and \texttt{goto} as a navigation action.
Actions are defined via preconditions and effects. 
For instance, the action \texttt{pickup($o_1$)} has preconditions of \texttt{at(robot, $l_1$)} and \texttt{at($o_1$, $l_1$)}, meaning that to pick up the object $o_1$, the object must be co-located with the robot base in the same symbolic location $l_1$.
The effects of \texttt{pickup($o_1$)} include $o_1$ being moved into the robot’s hand, i.e., \texttt{inhand($o_1$)}.

\vspace{0.5em}
\noindent\textbf{Action Uncertainty: }
% Let $C$ be the motion-level world configuration. 
Let $\mathcal{T}: \mathcal{T}^n \cup \mathcal{T}^m$ be a set of probability distributions for modeling action uncertainties.
% \yan{two ``:'' are pretty close?}
For a navigation action,
$\mathcal{T}^n(\hat{y}'_r|y_r, y'_r)$ represents the probability of a mobile robot aiming to navigate to goal $y'_r$, while landing in $\hat{y}'_r$, given the current robot position $y_r$.
%In this work, we use a bivariate normal distribution for $\mathcal{T}^n$.
For a manipulation action, $\mathcal{T}^m(\hat{y}'_o|y_o,y_r)$
represents the probability of the robot given an end effector goal position $y_o$ of ``reaching'' the object, while ending up at a position $\hat{y}'_o$, given the robot's standing position $y_r$.
In practice, $\mathcal{T}^n$ and $\mathcal{T}^m$ are determined by the robot’s navigation and manipulation systems.
In this paper, both $\mathcal{T}^n$and $\mathcal{T}^m$ are treated as black box.

\vspace{0.5em}
\noindent\textbf{Perception: }
% The robot visually perceives the environment through top-down views\commentp{How does the robot get top-down views?} over the areas where manipulation and navigation actions are performed. 
% The robot can use vision to perceive the areas where manipulation and navigation actions are performed. 
% For instance, top down views or 2D pose estimation. 
The robot visually perceives the environment. 
While we provide the robot with top-down view images in this work, our approach can be combined with perception methods that rely on first-person view for object pose estimation~\cite{wang2019densefusion}. 
%We use $\textit{IM}$ to represent a 2D image that captures the current obstacle configuration, as shown in the ``Image Input'' of Fig.~\ref{fig:overview} (bottom right). 
% To facilitate robot learning, we provide a dataset (as illustrated in the ``Dataset'' box of Fig.~\ref{fig:overview}). 
% Each instance includes a top-down view image, and a target object with a predefined position, while each label is in the form of a heatmap. 
% Each pixel of a heatmap is associated with a 2D position, and has a ``feasibility'' value that represents the success rate of the robot navigating to the 2D position, and manipulating the target object from there. 
A map is generated in a pre-processing step, and provided to the robot as prior information for navigation purposes using rangefinder sensors. 
Please note that dynamic obstacles such as randomly-placed chairs are not in the map. 

\vspace{0.5em}
\noindent\textbf{Problem Formulation: }
% \label{sec:problem}
%\noindent\textbf{Input: }
The input of the problem is a tuple $\langle \mathcal{Y}_o^{init}, y_r^{init}, \mathcal{A} \rangle$.
$\mathcal{Y}_o^{init}$ is a set of objects' initial positions and $y_r^{init}$ is the robot's initial base pose.
% \commentp{It's awkward that a set of positions and a single position use essentially the same notation...}
%\noindent\textbf{Output: }
The problem outputs a task-motion plan $p$ which is in the form of a sequence of navigation actions $a^n \in A^n$ and manipulation actions $a^m \in A^m$.
% The planner generates a set of plans $\mathcal{P}$ that fulfill the goal conditions.
% $Pln^{\mathcal{L}, Sym}$ assumes that an input tuple $\langle s^{0}, s^G, C, \mathcal{A} \rangle$ is given, where $s^{0}, s^{G}$ are the initial and goal states, $C$ is the motion-level world configuration, and $\mathcal{A}$ is a set of actions.
%\noindent\textbf{Objective: }
% $Pln^{\mathcal{L}, Sym}$ is a task-motion planner parameterized by locations $\mathcal{L}$ and the symbol mapping function $Sym$.
% $y^o_{s^{0}}$ and $y^o_{s^{G}}$ are the initial and goal positions of objects, $y^r_{s^0}$ is the initial position of the robot, $C$ is the world configuration, and $\mathcal{A}$ is a set of actions.
The problem finds a task planner $Pln^{\mathcal{L}, Sym,\mathcal{Y}}$ that is parameterized by the symbolic state space $\langle L, Sym, \mathcal{Y}\rangle$, in order to compute a task-motion plan $p$, where the objective is to maximize the plan utility for improving task completion rate and reducing robot execution time. 
% \commentp{That's 2 competing objectives.  How are they balanced?}


% % Figure environment removed









% The objective function of GTAMP-MM can be formally defined as follows:
% \begin{equation*}
%     \argmax_{p \in \mathcal{P}}(\sum_{a \in p}R(s,a,s')) \textnormal{, where }P = Pln^{\mathcal{S}, Sym}(s^{0}, s^G, C, \mathcal{A})
% \end{equation*}
% where $s^0$ and $s^G$ are mapped using $L$, $Sym$, $y^o_{s^{0}}$, and $y^o_{s^{G}}$. 







% \begin{table*}

% \centering
% %\scriptsize
% \caption{CMA-ES + Voronoi / Random + Voronoi performance under a planning timeout (budget). Planning trials that take more than the budget (300s) are listed as \textit{null}. $R$ is the reward.}
% % \scriptsize
% \begin{tabular}[t]{@{}lcccccccc@{}}
% 	\toprule
% 	 \multirow{2}{*}{Domain} & \multirow{2}{*}{State Space} & \multirow{2}{*}{Navigation Cost} & \multirow{2}{*}{Plan Utility} & \multirow{2}{*}{Planning Time (s)} & \multirow{2}{*}{Task Completion} & \multicolumn{3}{c}{Parameters}\\ \cmidrule{7-9}
% 	 & & & & & & $R$ & CPU Num \\ \midrule
% 	 1-object & N/A & / 30.53 &  / 46.80 & / 11.60 & & 100 & 1 \\ \midrule
%     \multirow{2}{*}{3-object} & A &  / 94.18 &  / 96.26 & / 39.96 & & \multirow{2}{*}{300} & \multirow{2}{*}{2}\\ \cmidrule{2-6}
% 	& B &  / 75.64 &  / 128.11 & / 40.76 & \\ \midrule
%     \multirow{2}{*}{5-object} & A &  / 117.00 &  / 152.41 & / 454.73 & & \multirow{2}{*}{500} & \multirow{2}{*}{12}\\ \cmidrule{2-6}
% 	& B &  / 113.66 &  / 164.28 &  / 258.23 & &\\ \midrule
%     \multirow{2}{*}{7-object} & A & null / null & null / null & null & N/A & \multirow{2}{*}{700} & \multirow{2}{*}{12}\\ \cmidrule{2-6}
% 	& B & null / 158.58 & null / 186.48 & null / 939.02 & &\\ \bottomrule
% \end{tabular}
% \label{tab:feature_space_of_contexts}
% \end{table*}

% Figure environment removed
\section{Symbolic State Space Optimization (S3O)}
In this section, we present the paper's main contribution called Symbolic State Space Optimization (S3O) which optimizes the state space for the task planner based on probabilistically evaluated action feasibility.
% \yan{need an ``overview'' of the following three headlines?}
S3O first constructs symbolic state spaces using object-centric Voronoi Partitioning and robot reachability.
And then it ranks a set of candidate state spaces based on evaluated action feasibility under uncertainty. 
%We apply two key techniques in the proposed TAMP algorithm: one for manipulating task planners and the other for optimizing motion-level mobile manipulation. 
% The proposed method introduces contributions at both task and motion levels.
% At the task level, we propose Task Planner Optimization (TAPO), where we use visually grounded feasibility evaluation~\cite{zhang2022visually} to rank all possible task planners, and then we weighted-sample from the set of top-ranked task planners at planning time.
% At the motion level, we propose long horizon Mobile Manipulation Optimization (CMMO), where we use an Evolution Strategy (ES) algorithm~\cite{hansen2003reducing} for motion-level optimization, where plan feasibility and efficiency serve as its objective.


% \subsection{Symbolic State Space Optimization (S3O)}
% \label{sec:task}
% This subsection describes how our system automatically generates state spaces and ranks the state space candidates by the evaluated action feasibility.

\vspace{0.5em}
\noindent\textbf{Constructing Symbolic State Spaces: }
We construct state spaces following two principles: 1) states (i.e., locations) should be determined by which object(s) they are the closest to; 2) the distance from the object to each pose in a state should be within the maximum reachability (1 meter in our case) of the robot.
% We construct state spaces following the principle that states (i.e., locations) should be determined by which object(s) they are the closest to.
% ; 2) each pose in a state should be less than the robot maximum reach distance to the object(s) in that state.
 Thus, in our framework, we consider poses that are around the objects within 1 meter, and generate areas by object positions in the 2D configuration space using the Voronoi Partitioning algorithm.
The distance from each 2D pose in an area to its corresponding object position is less than that from every other object position. 
Each area in the Voronoi diagram is considered as a symbolic location $l$, and the whole Voronoi partition corresponds to a set of locations $\mathcal{L}$ as well as a symbol mapping function $Sym$ to map each 2D pose to a location $l \in \mathcal{L}$.
% Then we simply filter out poses that are too far from the object for the robot to reach.
% We consider the reachability of 1 meter for our mobile manipulator.
% Note that it is not guaranteed that 
% \shiqi{I don't understand this 1-meter setting. Isn't it the scoring function for filtering out those infeasible state spaces? I am not sure you are talking about the same thing, or something different.}
% \xiaohan{The scoring function can definitely filter out those infeasible ones, but my concern is that without our proposed scoring function, a reasonable baseline should also **not** consider those poses that are way too far from the object.}
% \shiqi{That's fine, but I don't think the point is clear here. People might misunderstand it as being for summarizing the scoring function. }
% \commentp{some additional motivation for using Voronoi diagram is needed.  I understand states being determined by which object they are closest to.  But within a single Voronoi region, isn't there a big difference between locations that are within reach of the object, and those that are not?  How is that distinction drawn?}
Further, possible adjacency area merging operations are conducted in the Voronoi diagram.
Each area merging operation that results in a new symbolic state space (i.e., $\langle \mathcal{L}, Sym, \mathcal{Y}\rangle$) is considered as a state space candidate.


% The number of state spaces can be mathematically calculated given the number of objects.
% For example, there are 52, 203, and 877 different state spaces for 5, 6, and 7 objects respectively.
% \commentp{I don't understand enough about the area merging operation to be able to begin to verify those numbers.  How can I arrive at 52 for 5 objects?  Or maybe an example with even fewer objects would help.}

\vspace{0.5em}
\noindent\textbf{Scoring Function for State Space Ranking: }
In order to deal with a large number of objects, we compute scores for each state space candidate, i.e., $\langle \mathcal{L}, Sym, \mathcal{Y}\rangle$.
The score is calculated using the following function that is based on action feasibility:
\begin{align}
    \textit{Score}(\langle \mathcal{L}, Sym, \mathcal{Y} \rangle) = \sum_{o \in \mathcal{O}}{Fea}^t(l, o), \textnormal{if \texttt{at($o$, $l$)}}
    \label{eqn:score}
\end{align}
% \shiqi{I think you should include the angle brackets in the score parentheses, since there's only one paramaeter here, which is the whole state space candidate. }
where ${Fea}^t(l, o)$ is the task-level action feasibility function that computes the probability of the robot navigating to location $l$ and picking up object $o$. 
Intuitively, if the symbolic state space has a high accumulative task-level feasibility value over all the objects, this state space will be evaluated with a high score.
Fig.~\ref{fig:feasibility} shows the evaluated action feasibility (represented as heatmaps) and a top-ranked Voronoi Partition for the state space. 

After ranking the state spaces by the scores computed using Eqn.~\ref{eqn:score}, we select the top $K$ state spaces to construct $K$ task planners at robot planning time.
In each TAMP search iteration, our system normalizes the scores to produce a probability distribution from which one of the task planners is chosen.
The system plans in parallel, each with a sampled task planner, and uses $\argmax$ to find the state space (i.e., $\langle \mathcal{L}, Sym, \mathcal{Y}\rangle$) that generates a plan of the highest utility.

\vspace{0.5em}
\noindent\textbf{Action Feasibility Evaluation: } Robot perception is used to probabilistically evaluate action feasibility, represented as function $Fea: {Fea}^t \cup {Fea}^m$.
The task-level feasibility function ${Fea}^t(l, o)$ takes a symbolic location $l$ and an object $o$ as input, while the motion-level feasibility function ${Fea}^m(y_r, y_o)$ takes a robot 2D pose $y^r$ and an object 2D pose $y^o$ as input. Both task-level and motion-level feasibility functions output feasibility values ranging from 0.0 (infeasible) to 1.0 (feasible).
In this work, ${Fea}^t$ serves as a key component in the proposed scoring function (Eqn.~\ref{eqn:score}).
${Fea}^m$ is used to compute: 1) ${Fea}^t$, which is discussed in the next paragraph, and 2) the plan utility, which is formally defined in the next section.
% We will discuss how to compute ${Fea}^t$ from ${Fea}^m$ in the next paragraph and  be discussed in the next section.

% \shiqi{I'd suggest starting this paragraph with a few lines to clearly state the role of this Fea function in our framework. What's its input and output? Why should we care? Since it's not new, we don't need to talk too much about how it works.} 
Our task-level feasibility function ${Fea}^t(l, o)$ shares the same definition as what was initially introduced in~\cite{zhang2022visually}. 
Briefly summarizing here, ${Fea}^t(l, o)$ relies on ${Fea}^m(y_r, y_o)$ and a sampling function $Smp$.
${Fea}^m(y_r, y_o)$ computes the motion-level feasibility of robot navigating to 2D position $y_r$ and picking up the object that is at position $y_o$. 
$Smp$ samples 2D positions $y_r$ that satisfy $Sym(y_r) = l$, where the positions are weighted by ${Fea}^m(y_r, y_o)$.
%Note that $Smp$ can only sample from those positions $y_r$ that satisfy .
In other words, positions of higher motion-level feasibility are more likely to be sampled. 
Computing ${Fea}^t(l, o)$ is to calculate the average motion-level feasibility over $N$ samples that are drawn using $Smp$.

% In the work of~\cite{zhang2022visually}, ${Fea}^m(y_r, y_o)$ is extracted from a learned Fully Convolutional Network model~\cite{long2015fully}, which is trained using robot data from past experience.
% The robot data is represented as gray-scale heatmap images, where the color of the pixel indicates the motion-level feasibility value.
% However, one limitation of the above-mentioned model is that it can only deal with objects that are of a predefined distance from the table edge due to the limitation of its training dataset.
% In this work, we use the same architecture for motion-level feasibility evaluation, but we trained the model using a newly-collected dataset by not only diversifying the obstacle (i.e., chair) positions but also with randomly-placed objects on the table.
% Our motion-level feasibility function equips the robot with the capability of handling more generalized object pick and place tasks.
We extract ${Fea}^m(y_r, y_o)$ from a learned Fully Convolutional Network model~\cite{long2015fully}, which is trained using robot data from past experience, represented as gray-scale heatmap images.
% , where the color of the pixel indicates the probability of navigating to the corresponding  and successfully performing manipulation action at.
We trained the model by collecting a dataset that diversifies the obstacle (i.e., chair) positions and is with randomly-placed objects on the table.
One recent work uses the same architecture for motion-level feasibility evaluation~\cite{zhang2022visually}, but their model can only deal with objects that are of a predefined distance from the table edge due to the limitation of its training dataset.
In comparison, the motion-level feasibility function extracted from our model equips the robot with the capability of handling more generalized object pick and place tasks.
% The robot data is represented as gray-scale heatmap images, where the color of the pixel indicates the motion-level feasibility value.



% Intuitively, the state space with a higher score is more likely to be sampled at planning time.
% \commentp{Are we planning many times?  If once, why not just go with the top-ranked planner?}
% We use feasibility to rank symbolic state spaces, and by further considering utility functions, we 
% \xiaohan{TBA}

% TAPO enables the TAMP system to construct task planners from a candidate set of symbolic state spaces based on the evaluated motion-level feasibility.
% After selecting a task planner, the robot still needs an efficient motion-level search algorithm for optimizing feasibility and efficiency in mobile manipulation domains.
% In the next subsection, we introduce long horizon Mobile Manipulation Optimization (CMMO), a TAMP algorithm that uses an Evolution Strategy (ES) algorithm~\cite{hansen2003reducing} for efficient motion-level search

% Figure environment removed

\section{Computing Task-motion Plans}
\label{sec:motion}
Long horizon mobile manipulation domains require robots to complete tasks as accurately and quickly as possible.
This subsection details how we use the optimized state spaces from S3O to compute feasible and efficient task-motion plans.
% \paragraph{Visually Grounded Feasibility Evaluation}
% In our mobile manipulation domain, motion-level feasibility $\textit{Fea}(y_r,y_o)$ is a function of 2D positions $y_r$ and $y_o$, and is the probability of a robot successfully navigating to $y_r$ and picking up an object that is in position $y_o$. 
% $\textit{Fea}(y_r, y_o)$ can be extracted from gray-scale heatmap image $h^{y_o}$ that is centered around $y_o$: 
% \begin{align}
%     \textit{Fea}(y^r,y^o) = h^{y^o}[y^r]
% \end{align}
% We use a similar FCN-based feasibility evaluator $\Psi$ introduced in~\cite{zhang2022visually} to generate heatmap $h^{y^o}$, given a top-down view image $\textit{IM}^{y^o}$ captured right above unloading position $y^o$: 
% \begin{align}
%     h^{y^o} = \Psi(\textit{IM}^{y^o})
% \label{eqn:heatmap}
% \end{align}

% \paragraph{Efficient Motion-level Optimization using CMA-ES}
As described in Sec.~\ref{sec:problem}, the objective of the problem is to maximize the overall task completion rate and minimize the robot execution time.
Robot execution time is largely affected by how much time each action takes (especially long-range navigation actions), and the task completion rate depends on manipulation action feasibility. 
To this end, at planning time, we design the cost function for an action $a$ as:
\begin{equation}
    Cst(a)=\begin{cases}
      len(y_r, y'_r) / v + \gamma, \textnormal{ if $a\in \mathcal{A}^n$}\\
      \delta, \textnormal{ if $a\in \mathcal{A}^m$}
    \end{cases}    
\end{equation}
where function $len$ is able to measure the trajectory length of executing a navigation action and $v$ is the robot speed.
$\gamma$ is a constant cost for navigation when the robot starts moving, which motivates the robot to select as few navigation actions as possible.
$\delta$ is a constant cost for manipulation actions which is relatively small as compared to the cost for navigation actions.

We further use the action cost function to design the action reward function. 
Let $\lambda$ be a successful reward bonus of picking up one object.
The action reward function is designed as follows:
\begin{equation}
R(a)=\begin{cases}
      -Cst(a), \textnormal{if $a\in \mathcal{A}^n$}\\
      -Cst(a)+Fea^m(y^r, y^o) \cdot \lambda, \textnormal{if $a\in \mathcal{A}^m$}
    \end{cases}
% \label{eqn:reward}
\end{equation}
% \begin{equation*}
%     R(p) = \lambda \sum\limits_{a^m \in p} \sum\limits_{\hat{y}^r_{s'}, \hat{y}^o_{s'} \in \mathcal{Y}}\mathcal{T}^n(\hat{y}^r_{s'}|y^r_s, y^r_{s'}, C) \cdot \mathcal{T}^m(\hat{y}^o_{s'}|y^o_s,\hat{y}^r_{s'},C)
% \end{equation*}
% where $\hat{y}^r_{s'}$ denotes the position that the robot navigates to and stands at (for picking up the object). 
% The intuition behind this design is that we consider a \textit{compound} probability of the current manipulation action and the previous navigation action.
% where $s'\oslash s^G$ outputs true (or false) when the goal configuration is partially completed (or not).
% In this case, successfully picking up an object results in $s'\oslash s^G = \emph{True}$.
% Evolution Strategy (ES) techniques have shown great advantages for sampling-based optimization problems.

We use the CMA-ES optimization technique~\cite{hansen2003reducing} to serve as the sampling algorithm for motion-level 2D poses that the robot navigates to and performs the manipulation action(s) at.
Fig.~\ref{fig:cma-es} shows an example of the samples drawn from early and late iterations of the CMA-ES sampler. 
Each sample we draw is in the form of $\langle y_r^1[x], y_r^1[y], y^2_r[x], y_r^2[y],... \rangle$, where $y_r^{i}[x]$ ($y_r^{i}[y]$) denotes the $x$ ($y$) coordinate of the robot pose for navigating to and picking up the $i$th object from.
We maintain an independent CMA-ES sampler for each fixed task-level sequence, so we are able to form a complete task-motion plan $p$ by simply chaining the sampled $xy$ positions.
Two consecutive pairs of $xy$ positions (i.e., $\langle y_r^{i}[x],y_r^{i}[y] \rangle$ and $\langle y_r^{i+1}[x],y_r^{i+1}[y] \rangle$) can be used to parameterize a navigation action, and every single pair of $xy$ positions plus an object position (i.e., $y_o$) can be used to parameterize a manipulation action.
This enables us to convert a sample to a sequence of actions and then evaluate the sample by computing $\sum R(a)$.
$\sum R(a)$ is the utility of a task-motion plan and serves as the objective function for the CMA-ES sampler.









% \subsection{Illustrative Planning Example}
% In this section, we showcase a complete task and motion planning procedure using our system.

% As demonstrated in Fig.~\ref{fig:overview}, the environment includes seven objects.
% Each object is paired with a randomly generated chair that is no more than 1.5 meters from the object.

% \section{Illustrative Trial}

% \paragraph{Voronoi Partitioning}
% To solve an ALOMA problem, we first need an algorithm to generate candidates for $\langle \mathcal{L}, Sym \rangle$, which can be considered as a 2D partitioning problem.
% We use the Voronoi partitioning approach to generate regions, each corresponding to one target object position.
% \xiaohan{Figures of the 4 domains}

% \paragraph{Feasibility Evaluation}



% In Figure~\ref{fig:illustrative_trial}, state spaces A and B are two candidates for $\langle \mathcal{L}, Sym \rangle$.
% For state space A, we evaluate the feasibility of picking up objects from every symbolic state.
% Example feasibility heatmaps of region $l_1$, $l_2$,$l_3$, and $l_4$ are shown in Figure~\ref{fig:illustrative_trial}.
% We can observe that there is no single position that is evaluated to be feasible to pick up more than one object.
% In other words, from the planner's perspective, the robot cannot execute two (or more) consecutive manipulation actions for picking up the objects.
% One should always move its base to a feasible position before a manipulation action.
% Please note that this is reasonable because for instance, the task planner will believe that it is impossible to \texttt{pickup($o_1$)} if the robot is at $l_2$ (i.e., \texttt{at(robot, $l_2$)}) but the object is at $l_1$ (i.e., \texttt{at($o_1$, $l_1$)}).
% However, the 2D physical positions of $l_3$ and $l_5$ might be very close to each other, so that the robot does not have to move at all before the next manipulation action. Especially the case if there is relatively large energy consumption when the robot starts.
% This example motivates the necessity of learning and optimizing the most suitable state space given the current task.

% State space B is another candidate for $\langle \mathcal{L}, Sym \rangle$.
% Compared to A, the location $l_1$ is merged with $l_2$, and $l_3$ is merged with $l_4$.
% Intuitively, two objects that are close to each other should be considered in the same symbolic location.
% In doing so, the feasibility heatmaps (as shown in Figure~\ref{fig:illustrative_trial}) are able to reason about positions that can pick up two objects at the same time.
% For instance, the robot may consider picking up $o_1$ and $0_2$ while standing in the same position, and $o_3$ with $o_4$.
% Our developed algorithm is aiming to select such state space that is more appropriate for the robot task, and generates a feasible and efficient task-motion plan. 

% Our method searches state space following the steps below:
% \begin{itemize}
%     \item for each searched task-motion plan, remove a random navigation action (uniformly sample) under some probability $p$
%     \item re-evaluate the new plan (with removed navigation action)
%     \item if the evaluation result is better than any other searched plans, combine the corresponding states
%     \item keep searching using the updated state space and repeat step 1-3

% \end{itemize}

% Figure environment removed

\section{Experiments}
We conducted extensive experiments in simulation, where a mobile manipulator performs navigation and manipulation actions to ``collect dishes'' in a ``restaurant'' scenario. 
We also demonstrated the computed plan using our method on a real robot system. 
Our main hypothesis is that under a planning time budget, the proposed framework outperforms existing TAMP algorithms in task completion rate and robot execution time. 
% % Figure environment removed

% % Figure environment removed




% % Figure environment removed



\subsection{Baselines}
% Our framework is evaluated through comparisons with four baselines. 
Ours and the baseline methods differ from each other in how to construct and optimize task planners (state spaces in particular).
%and which TAMP strategy to use given the selected task planner.
We compare S3O with basic object-centric Voronoi Partitioning (denoted as ``V'').
After selecting a task planner, there are different TAMP strategies we can choose from.
We consider two TAMP algorithms for navigation domains from the literature, which are GROP~\cite{zhang2022visually} and PETLON~\cite{lo2020petlon}.
% The way we compute task-motion plans is similar to GROP, but relies on the proposed CMA-ES sampling algorithm.
Our TAMP component is built on GROP and further incorporates the proposed CMA-ES sampling algorithm for motion-level optimization.
Thus, we denote our TAMP strategy as GROP$^*$.
Combining different methods from task planner construction and TAMP strategy, we consider the following five methods in total: S3O-GROP$^*$, S3O-GROP, V-GROP$^*$, V-GROP, and V-PETLON.
To improve clarity, we briefly summarize the major differences between the five methods:
% Both baselines are TAMP algorithms from the literature.
\begin{itemize}
    \item \textbf{S3O-GROP$^*$} (proposed): It optimizes state spaces using S3O, and samples navigation goals using CMA-ES. The algorithm optimizes both plan efficiency and action feasibility.
    \item \textbf{S3O-GROP}: One ablative version of S3O-GROP$^*$.
    It is the same as S3O-GROP$^*$ except that CMA-ES is not used here.
    \item \textbf{V-GROP$^*$}: One ablative version of S3O-GROP$^*$. 
    It is the same as S3O-GROP$^*$ except that there is no state space optimization.
    \item \textbf{V-GROP}~\cite{zhang2022visually}: It does not optimize the state space, and samples navigation goals only by evaluated feasibility. The algorithm optimizes both plan efficiency and action feasibility.
    \item \textbf{V-PETLON}~\cite{lo2020petlon}: It does not optimize the state space, and selects navigation goals by just randomly sampling an obstacle-free position that is close to the object position. The algorithm optimizes plan efficiency but does not evaluate action feasibility.
\end{itemize}
% \shiqi{Looks like you forgot to update the baseline names.}
In comparison, our method, S3O-GROP$^*$, constructs the task planner using S3O and selects navigation goals by CMA-ES sampling, whose objective includes both motion-level feasibility and long horizon mobile manipulation cost. 
% As compared to GROP, CMMO also optimizes plan efficiency and action feasibility, but in a more sample-efficient way by using CMA-ES.
Note that we did not include ``S3O-PETLON'' as one of the baselines as there is no feasibility evaluation in the original PETLON algorithm, thus S3O is inapplicable.

\subsection{Experimental Setup}
The simulation environment contains seven tables of different sizes: one long table as the ``bar area'', two mid-sized tables, and four small tables that are able to take one person per table.
Objects to be collected are randomly generated on the tables, and an obstacle (i.e., chair) that is not mapped beforehand is placed near each object with a randomly generated position and orientation.
The number of objects is dynamically changed for different environments, ranging from 5 to 7.
An RGB camera is attached to the ceiling to capture overhead images of environments for robot perception. 
We assume the robot can hold multiple objects at the same time. 
Task completion is evaluated based on whether ``dishes'' on the tables are successfully ``collected'' or not. 

% \xiaohan{Same as GROP}
The mobile manipulator in simulation includes a UR5e robot arm, a Robotiq 2F-140 gripper, an RMP 110 mobile base, and a Velodyne VLP-16 lidar sensor on the mobile base. 
We used the Building-Wide Intelligence (BWI) codebase~\cite{khandelwal2017bwibots} to construct our simulation platform, which relies on the Gazebo physics engine~\cite{koenig2004design}.
Rapidly exploring Random Tree (RRT) approach~\cite{lavalle1998rapidly} is used to compute motion-level manipulation plans. 
The navigation stack was built using the \texttt{move\_base} package of Robot Operating System (ROS)~\cite{quigley2009ros}.
The robot's task planner is ASP-based~\cite{gelfond2014knowledge,lifschitz2002answer} and the Clingo solver is applied for computing task plans~\cite{gebser2014clingo}.
% The collected dataset described in Sec.~\ref{sec:task} was fed into an FCN for training the model for feasibility evaluation.
 We adopted the FCN-VGG16 model~\cite{long2015fully} for predicting action feasibility heatmaps.
%  and trained the model with batch size $4$ and learning rate $e^{-3}$.
The model is trained using a machine equipped with an Intel 3.80GHz i7-10700k CPU and a GeForce RTX 3070 GPU on a Ubuntu system.


\subsection{Planning Parameters}
At planning time, we do parallel computing using 12 CPUs on a different machine from training the FCN model.
The machine for planning is equipped with an 11th Gen Intel(R) 2.30GHz Core(TM) i7-11800H CPU.
The planning time budget is set to 300 seconds.
For each task-level sequence, the maximum number of motion-level samples that can be drawn is 200.
The manipulation constant cost $\delta$ is set to 5, and the navigation constant starting cost $\gamma$ is set to 20.
The reward for a successful manipulation action $\lambda$ has a value of 150.
The robot velocity $v$ for computing action cost is set to 0.4m/s.
There are other parameters for the CMA-ES sampler. 
We consider the first 20 generations for CMA-ES sampling.
Since the number of motion-level samples is fixed (i.e., 200), the population size of each generation is set to 10.
After ranking all possible state spaces, we choose the top 5 of them according to the computed scores.



% \begin{itemize}
%     \item cpu\_count = 1, 2, 12 for parallel computing. Each process is for searching a task plan.
%     \item state\_sample\_times = 200. The total number of motion-level points sampled for each symbolic state.
%     \item nav\_variance = 0.03. The variance for bivariate normal distribution that is used for modeling navigation uncertainty in the work. Covariance is set to be 0 for now.
%     \item nav\_sample\_times = 5. How many samples we draw from the normal distribution. This is also the sampling strategy when we collect the dataset.
%     \item collision\_cost = 999. This is the cost when there is no navigation plan found.
%     \item man\_cost = 1 for every manipulation action.
%     \item Reward $R$ = 100. \xiaohan{Production or addition??} 
%     \item robot\_v = 1. Robot velocity.
%     \item base\_bringup\_cost = 20. The constant energy cost when the robot base is start moving.
%     \item timeout = 1000 (s).
%     \item reachability = 1. This is the maximum distance between the robot and the object where the object can be fetched. This parameter is mainly used for direct rejecting some samples.
% \end{itemize}





\subsection{Task Completion Rate and Robot Execution Time}
Fig.~\ref{fig:exp_main} shows the main results of task completion rate and robot execution time.
There were a total of 100 different tasks.
We grouped the tasks based on their difficulties: Easy, Moderate, and Difficult. 
A task's \emph{difficulty} is measured by the total area that a robot can navigate to and pick up an object from. 
For instance, a task with all feasible picking up positions being surrounded by obstacles has a high difficulty. 
After sorting the tasks based on their difficulties, we evenly placed them into the three groups. 

Our system consistently performed the best in task completion rate (left subfigure) in all three settings, while maintaining the lowest robot execution time (right subfigure). 
% We also see that GROP performed particularly well in hard tasks where it produced the highest completion rate and the lowest action costs.  
% While PETLON generated efficient plans (comparable to GROP), it does not reason about feasibility, resulting in low completion rate.
% DVH generates feasible plans (like FCN-Planning), but it does not consider action costs, resulting in long execution time in task completions. 
We also see that methods that use S3O (i.e., S3O-GROP$^*$ and S3O-GROP) have better or at least similar performance compared with the methods that use basic Voronoi Partitioning (i.e., V-GROP$^*$, V-GROP, and V-PETLON).
While only considering methods that use Voronoi Partitioning, the one that uses GROP$^*$ generates plans that are of the least execution time and maintains a similar (higher) success rate as compared to V-GROP (V-PETLON).
Both GROP$^*$ and GROP consider feasibility when sampling navigation points, but the former also takes efficiency into account by using CMA-ES.
That is the reason why V-GROP$^*$ and V-GROP share similar success rates but the former performs better in plan efficiency.
Overall, the results support our hypothesis.


\begin{table}

\centering
%\scriptsize
\caption{Ablation study on the impact of different strategies for constructing the task planner. \textbf{Task completion rate / robot execution time} are reported in the table. S3O is our method that does task planner optimization; S3O-Random is an ablative version that uniformly selects the task planner from the candidate set without score ranking.}
\scriptsize



\begin{tabular}[t]{@{}lcc@{}}
	\toprule
	 Task Difficulty & S3O & S3O-Random\\ \midrule
	 Easy & \textbf{0.52} $\pm$ 0.02 / 107.90 $\pm$ 5.35 & 0.30 $\pm$ 0.06 / \textbf{96.95} $\pm$ 2.40\\ \midrule
      Moderate & \textbf{0.36} $\pm$ 0.03 / 118.68 $\pm$ 2.78 & 0.18 $\pm$ 0.08 / \textbf{97.89} $\pm$ 4.22\\ \midrule
	 Difficult & \textbf{0.29} $\pm$ 0.02 / 131.25 $\pm$ 3.64 & 0.17 $\pm$ 0.08 / \textbf{102.85} $\pm$ 6.30\\ 
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

% Figure environment removed

\subsection{Ablation Study}
We also conducted an ablation study (as shown in TABLE~\ref{tab:ablation}) to learn the impact of different strategies for constructing the task planner, specifically how to select a state space from a set of state space candidates at planning time.
Without a predefined state space, we compare two methods for state space selection: the proposed S3O (with score ranking), and an approach that uniformly samples state spaces from all possible candidates (denoted as ``Random'').
% For motion-level search, we compare the CMA-ES sampler with the sampler that is used in GROP.
% The former is configured with an objective function considering both efficiency and feasibility, but the latter only takes feasibility into account.
We observe that by considering S3O, the robot achieves a higher task completion rate for all tasks.
When uniformly selects a state space to construct the task planner, the system produces more cost-efficient plans, however, suffers from very poor performance in completing the task.
The reason is that the random selection strategy treats every state space candidate equally, even though some state spaces are unreasonable for the current task.
For instance, if two objects are too far from each other for the robot to reach both, it will be reasonable to separate the two objects into different locations instead of merging them into a single one.
However, given the limited planning time, it is almost impossible for S3O-Random to select the most suitable state space especially when there are many objects, thus resulting in much lower task completion rates. 
On the other hand, it is expected to see more Voronoi area merging operations (including feasible and infeasible ones) for S3O-Random than our method which prefers only the feasible ones.
As a result, the S3O-Random agent frequently chooses to navigate only a few times and tries to complete the whole task, which is not ideal.
In comparison, S3O (ours) seeks balance in task completion rate and robot execution time.

% \subsection{Illustrative Execution Example}


\subsection{Real Robot Demonstration}
We demonstrated the generated plan using S3O-GROP$^*$ on a real robot, as shown in Fig.~\ref{fig:real}.
We use the Human Support Robot (HSR) from Toyota~\cite{yamamoto2019development}.
The robot is given a ``tidy home'' task, including collecting three empty cans and moving the apple to the white plate.
Using our planning framework, the robot planned to navigate to the first position to do three manipulation actions: ``collect'' (i.e., pick up the object and put it into a garbage bag mounted on the robot) the green and red cans, and pick up the apple.
While holding the apple in hand, the robot then went to the second position to place the apple on the plate. 
Finally, the robot planned to go to the third position to collect the blue can. 
A demo video has been uploaded as a supplementary file.
% \subsection{Plan}
% \begin{itemize}
%     \item Map a predefined dining env (done)
%     \item cache the navigation cost function for the dining env, Store in file (running)
%     \item split the state space by objects instead of tables
%     \item test object feasibility of random position
%     \item test chair/object sampling
%     \item sample 5 difficulties to group tasks: 5-9 number of objects. Each difficulty contains 20 tasks (in total 100 tasks). Store in file
%     \item sample chair positions (6 chairs) for those 100 tasks. Store in file
%     \item Load each task, pre-calculate top-10 state spaces and their weights. Store in file
%     \item Load each task, and the state spaces and the weights, optimize once, store the sequence and standing positions in file (300 plans)
%     \item Load each plan, execute once
% \end{itemize}



% Metrics:
% \begin{itemize}
%     \item Navigation cost (Planning time)
%     \item Plan utility (Planning time)
%     \item Expected Task completion rate (Planning time)
%     \item Reward (Planning time)
%     \item Execution time (runtime)
%     \item Completion rate (runtime)
% \end{itemize}


% \subsection{Test Domains}


% % % Figure environment removed

% \xiaohan{A figure shows the combined state of those 3 domains. (blended)}

% \subsection{CMA-ES Generations for the 7-Object State}
% \xiaohan{Two sets of figures: split state space and combined state space}

% \subsection{Big Table}
% Parameters:

% Other notes: there are 1, 2, 5, 15, 52, 203, 877 state combinations for state num of 1-7.

% 4, 6, 7, 9, 10, 19


% % Figure environment removed


\section{Conclusion}
This paper introduces Symbolic State Space Optimization~(S3O), which constructs state space candidates from object-centric partitioning of the configuration space and ranks each candidate by probabilistically evaluating action feasibility values.
S3O is applied to a TAMP system for long horizon mobile manipulation tasks where we further improve motion-level search efficiency using the CMA-ES algorithm.
% The proposed approach optimizes task planners based on probabilistically evaluated motion-level feasibility under uncertainty.
The resulting framework is called S3O-GROP$^*$, which was extensively evaluated in simulation and demonstrated it using a real robot.
% We have extensively evaluated our proposed framework 
Results showed that S3O-GROP$^*$ produces task-motion plans that are of higher quality than existing TAMP algorithms in terms of task completion rate and robot execution time. 

% In this paper, we empirically evaluated the performance of GROP, while there is room to improve the evaluation through formal analysis, e.g., about its completeness and optimality.
% The difficulty comes from the different problem representations of task planning and motion planning.
% Also, the FCN-based feasibility evaluator is data-driven, where formal analysis is difficult.
% The current implementation of GROP relies on top-down views. 
% It would be interesting to investigate the feasibility of applying egocentric vision to GROP. 
% Due to the various viewpoints, we expect GROP to require a greater amount training data in this setting. 

\section*{Acknowledgements}
This work has taken place in the Autonomous Intelligent Robotics (AIR) group at SUNY Binghamton and the Learning Agents Research Group (LARG) at UT Austin. 
AIR research is supported in part by grants from NSF (IIS-1925044), Ford Motor Company, OPPO, and SUNY Research Foundation.
LARG research is supported in part by NSF (FAIN-2019844), ONR (N00014-18-2243), ARO (W911NF-19-2-0333), DARPA, Bosch, and UT Austin's Good Systems grand challenge.  
Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work.
The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.

 


\bibliographystyle{IEEEtran} 
\bibliography{ref}




\end{document}
