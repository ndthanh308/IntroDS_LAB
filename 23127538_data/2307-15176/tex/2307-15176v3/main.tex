
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

%%% KATIE'S ADDITIONS %%% 
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{cleveref}
\Crefformat{equation}{Eq.~#2#1#3}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{ifthen}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{colortbl}
% \usepackage[table]{xcolor}         % colors
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{bbm}
\usepackage{sectsty}

\usepackage{enumitem}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{multirow}

%%%======= ALGORITHMIC COMPLAINING
%https://tex.stackexchange.com/questions/542346/latex-error-command-and-already-defined-require-with-ifacconf-document-clas
\usepackage{etoolbox}

% save the meaning of \AND and undefine it to keep algorithmic happy
\let\classAND\AND
\let\AND\relax
% load algorithmic
\usepackage{algorithmic}
% save the new meaning of \AND and restore the one of the class
\let\algoAND\AND
\let\AND\classAND
% but when we start \begin{algorithmic} we want its own \AND
\AtBeginEnvironment{algorithmic}{\let\AND\algoAND}

\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicendif}{\textbf{end}}
%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\kkcomment}[1]{\textcolor{blue}{[#1 -KAK]}}
\newcommand{\sfcomment}[1]{\textcolor{purple}{[#1 -SF]}}
\newcommand{\rohit}{\textcolor{green!60!black}}
\newcommand{\jbcomment}[1]{\textcolor{red}{[#1 -JB]}}

\def\gcheckmark{{\color{green!60!black} \ding{51}}}
\def\rxmark{{\color{red} \ding{55}}}
% \newcommand{\todo}{\textcolor{red}{TODO}}
\newcommand\ci{\perp\!\!\!\perp}
\newcommand{\G}{{\mathcal G}}
\DeclareMathOperator{\doo}{do}
\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\Pa}{Pa}

%%%%% MACRO FOR DE-Anonymized VERSION %%%
\newboolean{isAnonymized} % Declare a new boolean
\setboolean{isAnonymized}{false} % Set to be true or false

%Example usage to use in the body 
% \ifthenelse{\boolean{isAnonymized}}
% {Is Anonymized} % This happens if the boolean is true
% {Is not Anonymized} % This happens if the boolean is false

%%%%%% MACRO FOR CHANGES for review  
\newcommand{\tmlrChanges}[1]{{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{RCT Rejection Sampling for Causal Estimation Evaluation}

\author{}

\author{\name Katherine A.~Keith \email kak5@williams.edu \\
      \addr Williams College
      \AND
      \name Sergey Feldman \email sergey@allenai.org\\
      \addr Allen Institute for Artificial Intelligence 
      \AND
      \name David Jurgens \email jurgens@umich.edu\\
      \addr University of Michigan
      \AND
      \name Jonathan Bragg \email jbragg@allenai.org\\
      \addr Allen Institute for Artificial Intelligence
      \AND 
      \name Rohit Bhattacharya \email rb17@williams.edu\\
      \addr Williams College
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{11}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=F74ZZk5hPa}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates---such as text data, genomics, or the behavioral social sciences---researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation.  However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call \emph{RCT rejection sampling}, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. 
In addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world RCT---which we release publicly---consisting of approximately 70k observations and text data as high-dimensional covariates. 
Together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation.
\end{abstract}

\section{Introduction}\label{sec:intro}

Across the empirical sciences, confounding is a significant obstacle to unbiased estimation of causal effects from observational data. Covariate adjustment on a relevant set of confounders  aka \emph{backdoor adjustment} \citep{pearl2009causality} is a popular technique for mitigating such confounding bias. In settings with only a few covariates, simple estimation strategies---e.g., parametric models or contingency tables---often suffice to compute the adjusted estimates. 
However, modern applications of causal inference have had to contend with thousands of covariates in fields like natural language processing \citep{keith2020text, feder2022causal}, genetics \citep{stekhoven2012causal}, or the behavioral social sciences \citep{li2016matching, eckles2021bias}. In these high-dimensional scenarios, more sophisticated methods are needed and often involve machine learning. Recent approaches include non-parametric and semi-parametric  estimators \citep{hill2011bayesian, chernozhukov2018double, athey2018approximate, farrell2021deep, bhattacharya2022semiparametric}, causally-informed covariate selection \citep{ maathuis2009estimating, belloni2014inference, shortreed2017outcome}, proxy measurement and correction \citep{kuroki2014measurement, wood2018challenges}, and causal representation learning \citep{johansson2016learning, shi2019adapting, veitch2020adapting}. 

 
Despite all this recent work targeted at high-dimensional confounding,
these methods have not been systematically and empirically benchmarked.  
Such evaluations are essential in determining which methods work well in practice and under what conditions.
However, unlike supervised learning problems which have  ground-truth labels available for evaluating predictive performance on a held-out test set, analogous causal estimation problems require ground-truth labels for counterfactual outcomes of an individual under multiple versions of the treatment, data that is generally impossible to measure \citep{holland1986statistics}.

A promising evaluation strategy is to directly subsample data from a randomized controlled trial (RCT) in a way that induces confounding. Causal effect estimates obtained using the confounded observational samples can then be compared against the ground-truth estimates from the RCT to assess the performance of different causal estimators. This idea has appeared in works like \citet{hill2011bayesian} and \citet{zhang2021bounding} and was recently formalized by \citet{gentzel2021and}. %We discuss other evaluation strategies and their limitations in Section~\ref{sec:related-work}.

We contribute to this evaluation strategy --- which we subsequently refer to as \emph{RCT subsampling} --- via theory that clarifies why and how RCT subsampling algorithms should be constrained in order to produce valid downstream empirical comparisons. In particular, we prove previous subsampling algorithms can produce observational samples from which the causal effect is provably not identified, which makes recovery of the RCT ground-truth impossible (even with infinite samples). To address this issue, we present a new RCT subsampling algorithm, which we call \emph{RCT rejection sampling}, that appropriately constrains the subsampling such that the observed data distribution permits identification. 

In addition to improving the theoretical foundations of RCT subsampling, we provide evaluation designers a scaffolding to apply the theory. We implement a proof of concept evaluation pipeline with a novel, real-world RCT dataset---which we release publicly---consisting of approximately 70k observations and text data as high-dimensional covariates. We highlight important finite data considerations: selecting an RCT dataset and examining when empirical evaluation is appropriate; 
empirically verifying a necessary precondition for RCT subsampling; specifying and diagnosing an appropriate confounding function using finite samples; applying baseline estimation models; and briefly speculate on additional challenges that could arise. For each of these considerations, we walk through specific approaches we take in 
 our proof of concept pipeline.

In summary, our contributions are
% \vspace{-0.72cm}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
% \begin{itemize}[leftmargin=*]
% \itemsep0em
    \item We provide a proof using existing results in causal graphical models showing that previous RCT subsampling procedures (e.g.,~\citet{gentzel2021and})
    may draw observational data in a way that prevents non-parametric identification of the causal effect due to selection bias (\S\ref{subsec:gentzel}). 
    \item  We propose a new subsampling algorithm, which we call \emph{RCT rejection sampling}, that is theoretically guaranteed to produce an observational dataset where samples are drawn according to a distribution where the effect is identified via a backdoor functional (\S\ref{subsec:rejection}). Using three settings of synthetic data, we show our algorithm results in low bias, which is not always the case for a previous algorithm (\S\ref{subsec:synthetic}).
    \item  For evaluation designers who plan to use RCT rejection sampling for their own datasets, we highlight several finite data considerations and implement a proof of concept pipeline with a novel, real-world RCT dataset and application of baseline estimation models (\S\ref{sec:finite-data}). 
    \item We release this novel, real-world RCT dataset of approximately 70k observations that has text as covariates (\S\ref{ssec:proof-concept-data}). We also release our code.\footnote{
    \ifthenelse{\boolean{isAnonymized}}
    {Upon publication, we will provide a public repository for the code and data. Currently, this is removed for annonymization. } % This happens if the boolean above is true
    {Code and data at \url{https://github.com/kakeith/rct_rejection_sampling}.} % This happens if the boolean above is false
        
    }  
\end{itemize}
These contributions build towards
a more extensive future research agenda in empirical evaluation for causal estimation (\S\ref{sec:conclusion}). 

\section{Related Work in Empirical Evaluation of Causal Estimators} \label{sec:related-work}

\begin{table*}[t!]
  \centering
 \resizebox{\linewidth}{!}{ %makes it fit within the margin limits
      \begin{tabular}{ll|lll|ll}
      && \multicolumn{3}{c}{\textbf{General}} & \multicolumn{2}{c}{\textbf{Application to Backdoor Adjustment}} \\ 
      \textbf{Dataset} & \textbf{Eval.~strategy} & \textbf{DoF} & \textbf{Data avail.} & \textbf{DGP realism} & \textbf{Covariates (num.)} & \textbf{Data public?} \\
      \toprule
      Simulation, normal assmpts. \citep{d2021deconfounding} &  Synthetic & \rxmark~Many &  \gcheckmark~High & \rxmark~Low & \gcheckmark~High (1000+) & \gcheckmark~Yes \\
      IHDP-ACIC 2016 \citep{dorie2019automated} & Semi-synthetic & \rxmark~Many &  \gcheckmark~High & \rxmark~Medium &  \rxmark~Medium (58) & \gcheckmark~Yes  \\
      PeerRead theorems \citep{veitch2020adapting} & Semi-synthetic & \rxmark~Many &  \gcheckmark~High & \rxmark~Medium & \gcheckmark~High (Text vocab)
      & \gcheckmark~Yes  %had to reproduce this ourself 
      \\
      RCT repositories \citep{gentzel2021and} & RCT subsampling & \gcheckmark~Few &  \gcheckmark~High-RCTs & \gcheckmark~High & \rxmark~Low (1-2) & \gcheckmark~Yes\\
     Job training \citep{lalonde1986evaluating} &  COS & \gcheckmark~Few &  \rxmark~Low & \gcheckmark~High & \rxmark~Low (4) & \gcheckmark~Yes\\
       Facebook peer effects \citep{eckles2021bias} & COS & \gcheckmark~Few &  \rxmark~Low & \gcheckmark~High & \gcheckmark~High (3700) & \rxmark~No \\
      \bottomrule 
      This work & RCT subsampling & \gcheckmark~Few &  \gcheckmark~High-RCTs & \gcheckmark~High & \gcheckmark~High (Text vocab) & \gcheckmark~Yes \\ 
      \bottomrule 
  \end{tabular}
  }
  \caption{
Select related work in empirical evaluation of causal estimators compared on general desiderata of: \gcheckmark~few degrees of freedom (DoF) for the evaluation designer, \gcheckmark~high data availability, and \gcheckmark~realistic data-generating processes (DGP). We also examine the accompanying datasets presented for evaluating backdoor adjustment. Here, we want a \gcheckmark~high number of covariates to make the evaluation non-trivial and \gcheckmark~public availability of the data for reuse and reproducibility.  
\label{t:related-work}}
\end{table*}


As we discussed briefly in Section~\ref{sec:intro}, empirical evaluation of causal estimation methods for observational data is difficult but important.

We argue an evaluation strategy should in general (i) reduce the \emph{evaluation designers' degrees of freedom} i.e.~limit the number of choices researchers have to (inadvertently) pick an evaluation that favors their own method \citep{gentzel2019case};
(ii) has the necessary data (e.g.,~RCTs) available, (iii) ensure the data generating process (DGP) reflects the real world, and (iv) make the data publicly available for reuse and reproducibility. For applications of backdoor adjustment, we argue non-trivial evaluation should additionally (v) include a high number of covariates that could be used in the adjustment set.
Table~\ref{t:related-work} compares select previous work (and our own) according to the above desiderata. We briefly discuss these and other related work, and make a qualitative argument for the RCT subsampling strategy we contribute to. 

\textbf{Synthetic evaluations} are ones in which researchers specify the entire DGP, e.g.,~\citet{d2021deconfounding, schmidt2022searching}. This allows for infinite data availability, but is prone to encoding researcher preferences and can lead to over-simplification (or overly complex DGPs) compared to real-world observational scenarios.

\textbf{Semi-synthetic evaluations} use some real data but specify the rest of the synthetic DGP. This approach has been used in causal inference competitions \citep{dorie2019automated,shimoni2018benchmarking} and settings with text-data as confounding variables \citep{roberts2020adjusting,veitch2020adapting,weld2022adjusting}. 
Other semi-synthetic work fits generative models to real-world data \citep{neal2020realcause,parikh2022validating} or uses pre-trained language models to generate high-dimensional confounders from variables in a synthetic DGP \citep{wood2021generating}. 
Although more realistic than synthetic data, semi-synthetic DGPs can also make unrealistic assumptions;
for example, \citet{reisach2021beware} demonstrate this issue in the context of evaluating causal discovery algorithms.

\textbf{Constructed observational studies} (COSs) start with RCTs and then find non-experimental control samples that come from a similar population \citep{lalonde1986evaluating,hill2004comparison,arceneaux2006comparing,shadish2008can,jaciw2016assessing,gordon2019comparison,eckles2021bias,zeng2022uncovering,gordon2022close}.\footnote{For example, \citet{lalonde1986evaluating} adjusts for four covariates---age, years of schooling, high school drop-out status, and race (Table 4, Footnote C in \citeauthor{lalonde1986evaluating})---in his non-experimental group in a study of the effects of job training on earnings.
}
The advantage of COSs over (semi-)synthetic data is that they have few researcher degrees of freedom; however, non-experimental control groups often do not exist or do not come from similar-enough populations; see \citet{dahabreh2022randomized} for more details on identification from COSs. 

\textbf{Subsampling RCTs} uses an RCT as ground-truth and then subsamples the RCT data to create a confounded observational dataset. 
For example, \citet{zhang2021bounding} subsample from the International Stroke Trial (IST) of roughly 20k patients to estimate the treatment effect of aspirin allocation. This strategy also appears in prior work \citep{hill2011bayesian,kallus2018removing} and was recently formalized by \citet{gentzel2021and}.
While this approach is limited by the availability of RCTs and sampling decreases the number of units available to the estimation methods, it does not require the comparable non-experimental control group required by COSs, resulting in greater data availability. There are also fewer researcher degrees of freedom compared to synthetic or semi-synthetic approaches. Because of these tradeoffs, we believe this is one of the most promising strategies for empirically evaluating causal estimation methods and we build upon this strategy in the remainder of this work.

\section{Subsampling from RCTs}\label{sec:sampling}

We preface this section with a brief description of causal graphs, a prerequisite to understanding subsequent results. Then we provide identification and non-identification proofs for RCT subsampling algorithms and evidence from synthetic data. 


\subsection{Background: Causal graphical models}

A causal model of a directed acyclic graph (causal DAG) $\G(V)$ can be viewed as the set of distributions induced by a system of structural equations: For each variable $V_i \in V$ there exists a structural equation $V_i \gets f_i(\pa_i, \epsilon_i)$ \citep{pearl2009causality}. This function maps the variable's parents' values---$\pa_i$ of $V_i$ in $\G(V)$---and an exogenous noise term\footnote{Typically these noise terms are assumed to be mutually independent, but this assumption is not strictly necessary \citep{richardson2013single}.  Our results are non-parametric in the sense that we do not require any  distributional assumptions on the noise terms or the specific form (e.g.,~linearity) of the structural equations $f_i(\cdot)$.}, $\epsilon_i$, to values of $V_i$. 
The system of equations induces a joint distribution $P(V)$ that is Markov relative to the DAG $\G(V)$, i.e., $P(V=v) = \prod_{V_i \in V}P(V_i=v_i \mid \Pa_i=\pa_i)$. Independences in the distribution can be read off from $\G$ via the well-known d-separation criterion \citep{pearl2009causality}.
Interventions in the model are typically formalized using the do-operator \citep{pearl2009causality}, where $Y|\doo(T=t)$ denotes the value of an outcome $Y$ under an intervention that sets the treatment $T$ to value $t$.

Here, our causal estimand of interest is the average treatment effect (ATE)  defined as, 
\begin{align}
    \text{ATE} \equiv \E[Y | \doo(T=t)] - \E[Y | \doo(T=t')],
\end{align}
where $t$ and $t'$ denote distinct values of $T$. A causal parameter is said to be \emph{identified} if it can be expressed as a function of the observed data $P(V)$. Given a set of variables $Z \subset V$ that satisfy the \emph{backdoor criterion} w.r.t $T$ and $Y$\footnote{The set $Z$ satisfies the backdoor criterion if no variable in $Z$ is a causal descendant of $T$, and $Z$ blocks all backdoor paths between $T$ and $Y$, i.e., all paths of the form $T \leftarrow \cdots \rightarrow Y$.},
the ATE is identified via the well-known \emph{backdoor adjustment functional} \citep{pearl1995causal}.

% Figure environment removed

\subsection{RCT subsampling: Setup and conditions}

We now describe the specific setup and objectives of RCT subsampling.\footnote{Here our target of interest is the ATE, though similar principles apply to other causal parameters like conditional average treatment effects.} 
We start with a dataset $D_{\text{RCT}}$ consisting of $n$ iid draws from an RCT of pre-treatment covariates $C = \{C_1, \dots, C_k\}$, treatment $T$, and outcome $Y$. Since this data is assumed to come from an RCT, the observed data distribution $P(C, T, Y)$ is Markov relative to the causal DAG shown in Fig.~\ref{fig:selection-bias}(a) where $T \ci C$. The goal of RCT subsampling is to construct an observational dataset $D_{\text{OBS}}$ that consists of $m \leq n$ iid draws from $D_{\text{RCT}}$ that satisfies the following conditions which enable appropriate evaluation of causal estimation methods:
\begin{enumerate}
    \item[(I)] \textbf{Dependence induced.} $D_{\text{OBS}}$ consists of samples drawn according to a new distribution $P^*(C, T, Y)$ that satisfies the dependence relation $T \not\ci C$. 
    \item[(II)] \textbf{ATE identified.} There exists a functional $g$ of the RCT distribution and a functional $h$ of the subsampled data distribution such that $\text{ATE} = g(P(C, T, Y)) = h(P^*(C, T, Y))$. 
\end{enumerate}

Here, (II) is an important identification pre-condition that ensures that it is possible, at least in theory, to compute  estimates of the ATE from $D_{\text{OBS}}$ to match the ATE from $D_{\text{RCT}}$, the latter of which is treated as ground-truth in evaluation. From Fig.~\ref{fig:selection-bias}(a) it is clear that two sets of variables satisfy the backdoor criterion: the set $C$ and the empty set. Thus, the ATE is identified from the RCT distribution $P(C, T, Y)$ via the following two backdoor adjustment functionals,

{\begin{small}
\begin{align}
    \text{ATE} &= \sum_c P(c) \times \big( \E[Y\mid t, c] - \E[Y\mid t', c] \big) \label{eq:conditional-id} \\
    &= \E[Y \mid t] - \E[Y\mid t']. \label{eq:marginal-id}
\end{align}
\end{small}}

\noindent Thus, a subsampling algorithm satisfies (II) if there is a  functional $h(P^*(C, T, Y))$ that is equal to \eqref{eq:conditional-id} or \eqref{eq:marginal-id}. 

For our purposes, we add the condition (I) so that estimation in the observational data does not reduce to \eqref{eq:marginal-id}. That is, we aim to produce samples according to a distribution $P^*$ such that some adjustment is in fact necessary to produce unbiased ATE estimates. %, i.e., $\E^*[Y\mid t] - \E^*[Y\mid t'] \not= \E[Y\mid t] - \E[Y\mid t']$, where $\E^*$ denotes the expectation taken with respect to densities in $P^*$.
We note that (I) by itself is not sufficient to guarantee this;  RCT subsampling procedures also require that there exists at least one pre-treatment covariate correlated with the outcome, i.e., $\exists \ C_i \in C$ such that $C_i \not\ci Y$ in $P(C, T, Y)$ \citep{gentzel2021and}. However, this condition is easily testable, and we implement these checks in our synthetic experiments and real-world proof of concept (\S\ref{ss:precondition}). 

We now show a theoretical gap in existing approaches to subsampling RCTs, and propose a new algorithm that is theoretically guaranteed to satisfy conditions (I) and (II).


\subsection{Non-identification in prior work}\label{subsec:gentzel}
We claim that prior work that proposes RCT subsampling can result in observational samples from which the causal effect is \emph{not identified} non-parametrically unless additional constraints are placed on the subsampling process.
%We discuss a sufficient set of constraints in the next section in the context of our rejection sampling approach.
We consider Algorithm 2 in \citet{gentzel2021and} which does not explicitly impose such constraints and can be summarized as follows. Let $S$ be a binary variable indicating selection into the observational data from  $D_{\text{RCT}}$. 
A structural equation $S \leftarrow \mathbbm{1}(T = \text{Bernoulli}(f(C)))$ is used to generate the selection variable,
where $f$ is a function defined by the researcher and $\mathbbm{1}$ corresponds to the indicator function. $D_\text{{OBS}}$ is  created by retaining only samples from $D_{\text{RCT}}$ where $S=1$. This results in $P^*(C, T, Y) = P(C, T, Y \mid S=1)$ which is Markov relative to the causal DAG in  Fig.~\ref{fig:selection-bias}(b). From this DAG, it is easy to check via d-separation that condition (I) is satisfied as $T\not\ci C \mid S=1$. However, the following proposition shows that condition (II) is not satisfied.

\begin{proposition}
Given $n$ iid samples from a distribution $P$ that is Markov relative to Fig.~\ref{fig:selection-bias}(a),  Algorithm 2 in \citet{gentzel2021and} draws samples according to a distribution $P^*$ such that condition (II) is not satisfied. \label{proposition-gentzel}
\end{proposition}

We provide a proof in Appendix~\ref{sec:appendix-proof-gentzel}.

The intuition behind the proof of Proposition~\ref{proposition-gentzel} is as follows. Identification of the ATE relies on two pieces: the conditional mean of the outcome given treatment and covariates and the marginal distribution of covariates. From Fig.~\ref{fig:selection-bias}(b), we have $\E[Y | T, C] = \E[Y | T, C, S=1]$, but $P(C) \not= P(C|S=1)$. Indeed this marginal distribution cannot be identified via any non-parametric functional of the subsampled distribution $P^*(C, T, Y)$ \citep{bareinboim2015recovering}. However, this non-identification result holds assuming that there is no additional knowledge/constraints on how $P^*$ is generated; in the next section we modify the sampling to place constraints on the generated distribution $P^*$ that mitigate this issue.

\subsection{RCT rejection sampling}\label{subsec:rejection}
We propose Algorithm~\ref{alg:R2C}, which uses a rejection sampling procedure to subsample RCTs. 
Rejection sampling is useful when the target distribution is difficult to sample from but there exists a proposal distribution which is easier to sample from and the proposal distribution (times a constant) forms an ``upper envelope'' for the target distribution \cite[Chapter 23.2]{murphy2012machine}.
Similar ideas on resampling data based on ratios of propensity scores appear in \citet{thams2023statistical} and \citet{bhattacharya2022testability} in the context of testing independence constraints in post-intervention distributions. Though the rejection sampler also selects samples based on a function of $T$ and $C$, as in Fig.~\ref{fig:selection-bias}(b), we prove that additional constraints placed by the sampling strategy ensure identification holds in the new observed data distribution. 

The intuition behind our algorithm is as follows. Sufficient constraints for maintaining identifiability of the ATE in $P^*(C, T, Y)$ via the  functional in \eqref{eq:conditional-id}  are to ensure that
$P^*(C)=P(C)$ and $P^*(Y\mid T, C)=P(Y\mid T, C)$.\footnote{One could also consider maintaining equality of just the conditional mean of $Y$ rather than the full conditional density.} When this holds, it follows that \eqref{eq:conditional-id} is equivalent to the adjustment functional $h(P^*(C, T, Y)) = \sum_c P^*(c) \times (\E^*[Y\mid T=t,  c] - \E^*[Y\mid T=t',  c])$, where $\E^*$ denotes the expectation taken w.r.t $P^*(Y\mid T, C)$. To also satisfy (I), we propose resampling with weights that modify $P(T)$ to a new conditional distribution $P^*(T\mid C)$. 

The considerations listed in the prior paragraph inform our choice of an acceptance probability of $\frac{1}{M} \times \frac{P^*(T\mid C)}{P(T)}$ in the rejection sampler, where $M$ is the usual upper bound on the likelihood ratio used in the rejection sampler, which in our case is $\frac{P^*(T|C)}{P(T)}$.\footnote{In practice, we approximate $M$ from $D_{\text{RCT}}$ as $\frac{\max_i \in \{1, \dots, n\} P^*(T=t_i|C_i)}{\min_{i \in \{1, \dots, n\}} \widehat{P}(T=t_i)}$.} Here, $P^*(T\mid C)$ is a function specified by the evaluation designer that satisfies positivity ($\forall c, 0 < P^*(T\mid C=c) < 1$ almost surely), and is a non-trivial  function of $C$ in the sense that $P^*(T\mid C) \not= P^*(T)$ for at least some values of $T$ and $C$. 


\begin{algorithm}[t]
\begin{algorithmic}[1]
   \STATE {\bfseries Inputs:} 
   $D_{\text{RCT}}$ consisting of $n$ i.i.d.~draws from $P(C, T, Y)$; $P^{*}(T|C)$, a function specified by evaluation designers; $M\geq\sup \frac{P^*(T|C)}{P(T)}$, a constant computed empirically
 \STATE {\bfseries Output:} $D_{\text{OBS}}$, a subset of $D_{\text{RCT}}$ constructed according to a distribution $P^*(C, T, Y)$ which satisfies conditions (I) and (II)
 \STATE
   \FOR{each unit $i \in D_{\text{RCT}}$}
    \STATE Sample $U_i$ uniform on $(0, 1)$
   \IF{$U_i > \frac{P^*(T=t_i|C_i)}{\hat{P}(T=t_i) M}$}
   \STATE Discard $i$
   \ENDIF
   \ENDFOR
   \STATE  {\bfseries Return:} $D_{\text{OBS}} \leftarrow D_{\text{RCT}} - \{\text{discarded units} \}$
    % \STATE {\bfseries Return:} $D_{\text{Sample}}$ \leftarrow $D_{\text{RCT}}$ - $\{\text{discarded units}\}$
\end{algorithmic}
\caption{RCT rejection sampling} 
   \label{alg:R2C}
\end{algorithm}

\begin{theorem}
Given $n$ iid samples from a distribution $P$ that is Markov relative to Fig.~\ref{fig:selection-bias}(a), a confounding function $P^*(T|C)$ satisfying positivity, and $M\geq\sup \frac{P^*(T|C)}{P(T)}$, the rejection sampler in Algorithm~\ref{alg:R2C} draws samples from a distribution $P^*$, such that conditions (I) and (II) are satisfied. \label{theorem:rejection}
\end{theorem}

\begin{proof}
Rejection sampling generates samples from a target distribution $P^*(V_1, \dots, V_k)$ by accepting samples from a proposal distribution $P(V_1, \dots, V_K)$ with probability
\begin{align*}
    \frac{1}{M}\times \frac{P^*(V_1, \dots, V_k)}{P(V_1, \dots, V_k)},
\end{align*}
where $M$ is a finite upper bound on the likelihood ratio $P^*/P$ over the support of $V_1, \dots, V_k$.

We start with samples from an RCT, so our proposal distribution factorizes according to the causal DAG in Fig.~\ref{fig:selection-bias}(a): $P(C, T, Y) = P(C)\times P(T)\times P(Y\mid T, C)$.

Our target distribution is one where $T\not\ci C$, and factorizes as $P^*(C, T, Y) = P^*(C)\times P^*(T\mid C) \times P^*(Y\mid T, C)$, with additional constraints that $P^*(C) = P(C)$ and $P^*(Y\mid T, C) = P(Y\mid T, C)$. This establishes the likelihood ratio,
%
\begin{align*}
    \frac{P^*(C, T, Y)}{P(C, T, Y)} &=  \frac{P(C)\times P^*(T\mid C) \times P(Y\mid T, C)}{P(C) \times P(T) \times P(Y\mid T, C)} \\
    &= \frac{P^*(T\mid C)}{P(T)},
\end{align*}
%
and any choice of $M \geq \sup \frac{P^*(T|C)}{P(T)}$ used in the rejection sampler in Algorithm~\ref{alg:R2C} produces samples from the desired distribution $P^*$, where the additional constraints satisfy the identification condition (II) and specification of $P^*(T|C)$ such that it truly depends on $C$ satisfies condition (I).
\end{proof}

Since $P^*$ satisfies $T\not\ci C$ and yields identification via the usual adjustment functional obtained in a conditionally ignorable causal model, Algorithm~\ref{alg:R2C} can be thought of as producing samples exhibiting confounding bias similar  to the causal DAG in Fig.~\ref{fig:selection-bias}(c), despite the selection mechanism. %We use this interpretation moving forward, 
A longer argument for this qualitative claim is
in Appendix~\ref{sec:appendix-markov}.


We conclude this section by noting that similar to prior works on RCT subsampling algorithms, the subsampling strategy in Algorithm~\ref{alg:R2C} only requires researchers to specify a single function, $P^*(T\mid C)$. Hence, our procedure satisfies our original desideratum of limited researcher degrees of freedom, while providing stronger theoretical guarantees for downstream empirical evaluation. However, specification of $P^*(T\mid C)$ may still be challenging when $C$ is high-dimensional. In Section~\ref{ss:confounding-function}, we discuss this finite data consideration and we use a proxy strategy for our proof of concept in which we have a low-dimensional confounding set $C$ along with a set of high-dimensional covariates $X$ that serve as proxies of this confounding set. 


\subsection{Evidence from synthetic data}\label{subsec:synthetic}

% Katie 2022-01-12 from expr-rejection-sampling.ipynb
\begin{table*}[t!]
\centering 
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lllrrr}
  \toprule
 &\textbf{Synthetic DGP Setting} & \textbf{Sampling Algorithm} & \textbf{Abs. Bias (std.)} & \textbf{Rel.~Abs.~Bias (std.)} & \tmlrChanges{\textbf{CI Cov.}} \\
  \toprule 
   %%% Synthetic 1
  \multirow{2}{*}{1} & \multirow{2}{*}{$|C|=1$, $P(T=1)=0.3$} &Algorithm 2 from \citet{gentzel2021and} & 0.222 (0.010) & 0.089 (0.004) & \tmlrChanges{0.00} \\
  && RCT rejection sampling (This work) & \textbf{0.009 (0.007)} & \textbf{0.004 (0.003)} &  \tmlrChanges{0.97}\\
 \midrule 
 %%% Synthetic 2 
 \multirow{2}{*}{2} & \multirow{2}{*}{$|C|=1$, $P(T=1)=0.5$} & Algorithm 2 from \citet{gentzel2021and} & 0.009 (0.006) & 0.003 (0.003) &\tmlrChanges{0.98}\\ 
 && RCT rejection sampling & 0.007 (0.005) & 0.003 (0.002) & \tmlrChanges{0.98}\\ 
 \midrule 
  %%% Synthetic 3
 \multirow{2}{*}{3} & \multirow{2}{*}{$|C| =5$, Nonlinear} & Algorithm 2 from \citet{gentzel2021and} & 0.252 (0.010) & 0.979 (0.037) & \tmlrChanges{0.00}\\ 
 && RCT rejection sampling & \textbf{0.012 (0.009)} &  \textbf{0.046 (0.034)} & \tmlrChanges{0.98}\\
 \bottomrule
\end{tabular}
}
\caption{
Absolute bias (abs.~bias) between ATE from $D_{\text{RCT}}$ and the estimated ATE via backdoor adjustment on $D_{\text{OBS}}$ created by each sampling algorithm. We also report abs.~bias relative to the RCT ATE (rel.~abs.~bias) and the mean and standard deviation (std.) across samples from 1000 random seeds. 
\tmlrChanges{In the final column, we report the confidence interval coverage (CI Cov.)---the proportion of 1000 random seeds for which the 95\% confidence interval contains the true (RCT) ATE.}
The DGPs for Settings 1-3 are given in Appendix~\ref{sec:appendix-dgps}.
\label{table:sampling-results}
}
\end{table*}


Using synthetic DGPs for $D_{\text{RCT}}$, we produce $D_{\text{OBS}}$ using Algorithm 2 from \citet{gentzel2021and} and separately via our RCT rejection sampler. We then compute ATE estimates using \eqref{eq:conditional-id} for $D_{\text{OBS}}$ and compare it to the ground-truth estimates using \eqref{eq:marginal-id} in $D_{\text{RCT}}$. Section~\ref{sec:appendix-dgps} in the appendix gives the full details of the data-generating processes (DGPs) for three settings. Briefly, the DGP in Setting 1 has a single confounding covariate $C$, sets $P(T=1)=0.3$, and has an interaction term $TC$ in the structural equation for $Y$. Setting 2 is the same as Setting 1 except we set $P(T=1)=0.5$.  Setting 3 is a non-linear DGP with five covariates, $C_1, \dots, C_5$.  All methods are provided with the true adjustment set and functional form for the outcome regression, i.e., our experiments here use oracle estimators to validate the identification theory proposed in the previous subsection.

\tmlrChanges{
We construct 95\% confidence intervals via bootstrapping and the percentile method \cite{wasserman2004all} and report confidence interval coverage. After obtaining a sample $D_{\text{OBS}}$ from the RCT via either RCT rejection sampling or Algorithm 2 from \citeauthor{gentzel2021and}, we resample $D_{\text{OBS}}$ with replacement and calculate the ATE for that bootstrap sample. We repeat this for 1000 bootstrap samples and obtain a 95\% confidence interval by taking the 2.5\% and 97.5\% points of the bootstrap distribution as the endpoints of the confidence interval. Across our 1000 random seeds of the synthetic DGPs, we measure the proportion of these confidence intervals that contain the true (RCT) ATE and report this metric as confidence interval coverage. See Appendix Section~\ref{appendix-sec:ci} for additional confidence interval plots.  
}

Table~\ref{table:sampling-results} shows that our proposed RCT rejection sampler results in a reduction of absolute bias compared to Algorithm 2 from ~\citet{gentzel2021and} by a factor of over 24 for Setting 1 (0.22/0.009) and a factor of 21 in Setting 3 (0.252/0.012). For Setting 3, \citeauthor{gentzel2021and}'s procedure results in almost a 100\% increase in bias relative to the gold RCT ATE of $-0.26$. In Setting 2 where $P(T=1)=0.5$, the differences in absolute bias between the two algorithms is less pronounced.\footnote{We note \citet{gentzel2021and} primarily focus on the setting for which $P(T=1)=P(T=0)=0.5$. However, their approach does not seem to generalize well outside of this setting, theoretically and empirically.}  
\tmlrChanges{In Settings 1 and 2, the confidence interval coverage for \citeauthor{gentzel2021and}'s procedure is 0 whereas our RCT rejection sampling algorithm results in coverage of 0.97 and 0.98 for Settings 1 and 2 respectively, both slightly above the nominal 0.95 coverage.}
The results of the simulation are consistent with our theoretical findings that our algorithm permits identifiability under more general settings than prior work.

\section{Finite Data Considerations and Proof of Concept}\label{sec:finite-data}

In the previous section, we provided theoretical guarantees for RCT rejection sampling and confirmed the algorithm results in low bias on synthetic data. In this section, we demonstrate how to put this proposed theory into practice and highlight considerations when working with finite real-world data. 
Our goal is to surface questions that must be asked and answered in creating useful and high-quality causal evaluation.


We also describe our specific approach towards each consideration as we create a proof of concept pipeline for empirical evaluation of high-dimensional backdoor adjustment methods. For this proof of concept, we use a large-scale, real-world RCT dataset with text as covariates.
Although our approaches are specific to our proof of concept dataset, we believe other evaluation designers will benefit from a real-world example of how to put the theory and considerations into practice. 

\subsection{Considerations prior to using a specific RCT dataset}

A necessary component for RCT subsampling is obtaining a real-world RCT dataset. This ensures a more realistic data generating processes compared to synthetic or semi-synthetic approaches (see Table~\ref{t:related-work}).
As \citet{gentzel2021and} note, there are many RCT repositories from a variety of disciplines from which evaluation designers could gather data. However, \citeauthor{gentzel2021and} find many of these existing datasets only have one or two covariates that satisfy $C \not \ci Y$ (see Consideration~\#1 below).

As we briefly mentioned in Section~\ref{sec:intro}, for settings with just a few covariates one can often use simple estimation strategies with theoretical guarantees---e.g., parametric models or contingency tables---and empirical evaluation may not be particularly informative in this setting.
Along these lines, we recommend that evaluation designers first ask themselves, \emph{Is empirical evaluation of causal estimators appropriate and necessary for this setting?} 
Not all settings are in need of empirical evaluation.

A potentially untapped resource for RCT rejection sampling data is A/B tests from large online platforms. Other work, e.g., \citet{eckles2021bias}, have used these types of experiments for constructed observational studies and we use such a dataset in our proof of concept. The large scale of these experiments can be advantageous since RCT subsampling reduces the number of units in the observational dataset by roughly half. Further, they often contain rich metadata and many covariates, which can be used to induce confounding in a way that emulates a high-dimensional setting.

\subsubsection{Proof of concept approach}\label{ssec:proof-concept-data}

For our proof of concept, we choose a setting for which empirical evaluation is appropriate and needed: high-dimensional backdoor adjustment. Our high-dimensional covariates are the thousands of vocabulary words from text data, an application area that has generated a large amount of interest from applied practitioners, see \citet{keith2020text,feder2022causal}. 

We use and publicly release a large, novel, real-world RCT (approximately 70k observations) that was run on an online scholarly search engine.\footnote{
\ifthenelse{\boolean{isAnonymized}}
% Anonymized version below 
{Upon camera ready, we will release the dataset and provide more details about the website but remove them for now because of anonymity reasons.} 
% Non-Anonymized version below 
{The RCT was conducted on the Allen Institute for Artificial Intelligence's Semantic Scholar platform \url{https://www.semanticscholar.org/}.} 
Owners of the website conducted this experiment and gave us permission to use and release this data.
} 
Users arrive on a webpage that hosts metadata about a single academic paper and proceed to interact with this page. The RCT's randomized binary treatment is swapping the ordering of two buttons---a PDF reader and a new ``enhanced reader''. We set $T=1$ as the setting where the ``enhanced reader'' is displayed first. The  outcome of interest is a user clicking ($Y=1$) or not clicking ($Y=0$) on the enhanced reader button. The former action transports the user to a different webpage that provides a more interactive view of the publication. The RCT suggests that the treatment has a positive causal effect with an ATE of 0.113 computed using a simple difference of conditional means in the treated and untreated populations. See Appendix~\ref{sec:appendix-rct} for more details about the RCT.

\subsection{Consideration \#1: Checking necessary precondition $C \not\ci Y $}\label{ss:precondition} 

As we mentioned in Section~\ref{sec:sampling}, a necessary precondition for RCT subsampling in general is the existence of a causal edge between $C$ and $Y$, implying $C \not\ci Y $. 
The relationship between $C$ and $Y$ is naturally occurring (not modified by evaluation designers) and the amount of confounding induced by sampling is, in part, contingent on this relationship \citep{gentzel2021and}. 
One can empirically check $C \not\ci Y $ via independence tests, e.g.,~evaluating the odds ratio when both $C$ and $Y$ are binary variables. 
If there do not exist covariates, $C$ such that $C \not\ci Y $, one cannot move forward in the evaluation pipeline using RCT subsampling.  

\subsubsection{Proof of concept approach} 

For our proof of concept, we use a subpopulation strategy to ensure the precondition $C \not\ci Y$ is satisfied. 
We choose a single interpretable covariate to induce confounding: the field of study of manuscripts. 
Since $C \ci Y$ if and only if the odds ratio between $C$ and $Y$ is 1, we choose subpopulations of the full RCT that have high a odds ratio between a subset of the categorical field of study variable and the outcome $Y$. 
Specifically, we choose $C$ to be a binary covariate representing one of two fields of study; for \emph{Subpopulation A}, the field is either Physics or Medicine. In Appendix~\ref{sec-appendix:sub-b}, we implement the evaluation pipeline for an additional subpopulation with $C$ chosen as the articles with Engineering or Business as the field of study. Substantively, one can interpret this high odds ratio as natural differences in click rates from users viewing articles from different fields of study.  

We combine this subpopulation strategy with a proxy strategy in the next section to ensure that the estimation procedures only have access to high-dimensional covariates instead of our low-dimensional $C$. This has the benefit of simplifying  the process of specifying $P^*(T\mid C)$ while still ensuring that downstream modeling must still contend with high-dimensional adjustment.

% Figure environment removed

\subsection{Consideration \#2: Specification of $P^*(T|C)$}

Evaluation designers using RCT rejection sampling have one degree of freedom: specification of $P^{*}(T|C)$. We describe one specific approach in our proof of concept to choose $P^{*}(T|C)$, but we anticipate evaluation designers using RCT rejection sampling to create large empirical benchmarks may want to include many different parameterizations of $P^{*}(T|C)$ to evaluate empirical performance of methods under numerous settings. Consideration~\#3 describes approaches to diagnosing the choice of $P^{*}(T|C)$ for a specific finite dataset. 

\subsubsection{Proof of concept approach}\label{ss:proxy-tractable}

\textbf{Proxy strategy.} 
In Section~\ref{sec:sampling}, we briefly mentioned that specifying a suitable confounding function $P^*(T \mid C)$ may be difficult when $C$ is high-dimensional. 
A key property of our RCT is that it has high-dimensional text data, $X$, that is a proxy (with almost perfect predictive accuracy) for low-dimensional structured metadata---categories of scientific articles, e.g., Physics or Medicine. We use this structured metadata as the covariates $C$ in our RCT rejection sampler, but provide the causal estimation methods only $X, Y$ and $T$. Note that as evaluation designers, we still have access to $C$ to run diagnostics. This proxy strategy helps simplify the specification of $P^{*}(T \mid C)$ we use in the rejection sampler and avoids direct specification of a function involving high-dimensional covariates. Others have used similar proxy strategies for text in semi-synthetic evaluations, e.g.,~\citet{roberts2020adjusting,veitch2020adapting}. Such a technique may also be applied in other RCTs, e.g., healthcare studies where one or two important biomarkers  serve as low-dimensional confounding variables, and the larger electronic health record data serves as the proxy $X$.

\textbf{Using $X$.} For each document $i$, the high-dimensional covariates $X_i$ is a bag-of-words representation of the document's concatenated title and abstract given a 2000-unigram vocabulary. A new vocabulary is created for each RCT subpopulation; see Appendix~\ref{sec:appendix-vocab} for details. We check that there is high predictive accuracy of $P(C|X)$ to ensure the plausibility of causal estimation models only having access to $X$.\footnote{We leave to future work correcting for measurement error with noisy proxies $X$; see \citet{wood2018challenges}.} To measure this predictive accuracy, we model $P(C|X)$ with a logistic regression\footnote{
%Katie : from ml_utils.py CLASSIFIER 
Using scikit-learn \citet{pedregosa2011scikit} and an elasticnet penalty, L1 ratio 0.1, class-weight balanced, and SAGA solver.
} classifier. Averaged across held-out test folds, the F1 score is 0.98 and the average precision is 0.99 for Subpopulation A (Physics, Medicine).

\textbf{Specifying $P^*(T|C)$.} Since $C$ is binary in our proof of concept pipeline, we choose a simple interpretable piece-wise function,
\begin{align} \label{eqn:our-p-star}
P^{*}(T_i=1|C_i) = \begin{cases}
                \zeta_0 \text{ if } C_i=0 \\
                \zeta_1 \text{ if } C_i=1
                \end{cases}
\end{align}
for each document $i$, where $0 <\zeta_0 <1$ and $0 < \zeta_1 < 1$ are parameters chosen by the evaluation designers. We choose $\zeta_0, \zeta_1$ for the remainder of the proof of concept pipeline via the diagnostics in Consideration \#3.

\subsection{Consideration \#3: Diagnostics for $P^*(T|C)$}\label{ss:confounding-function}

We recommend running diagnostics on the sampling procedure because, although Theorem~\ref{theorem:rejection} proves that our RCT rejection sampling permits identification, this is an asymptotic statement, and the sampler can produce finite samples where confounding bias is induced, but difficult to correct for.

\subsubsection{Proof of concept approach} 

% Katie 2022-01-06 this came from causal-eval/scripts/pipeline_DB-CY.ipynb
% Figure environment removed


For our proof of concept pipeline, we step through the following diagnostics on the sampling procedure. Recall the notation from Section~\ref{sec:sampling}, $D_{\text{RCT}}$ is our RCT dataset (here, from Subpopulation A) and $D_{\text{OBS}}$ is the resulting observational dataset after RCT rejection sampling. 
First, we check empirically that overlap is satisfied for $C$ in $D_{\text{OBS}}$, i.e.,~$0 < \hat{P}(T=1 | C=c) < 1$ for all $c$. 


Second, in Figure \ref{fig:diagnostics-subpopa} we compare the amount of confounding induced to the error in the oracle adjustment for different $\zeta_0, \zeta_1$ in Equation~\ref{eqn:our-p-star} across 100 random seeds (blue dots). On the y-axis, we plot the absolute difference between the ATE for $D_{\text{RCT}}$ (GoldATE) and exact backdoor estimates\footnote{The exact backdoor equation is tractable because $T, C$ and $Y$ are all binary.} obtained from using the oracle adjustment set $C$. On the x-axis, we plot the absolute difference between the ATE on $D_{\text{RCT}}$ (GoldATE) and the unadjusted naive estimate on $D_{\text{OBS}}$. In general, we want more samples to fall below the $y=x$ line (shown in red) since this means that more confounding was induced than there is error in estimation. Samples above the $y=x$ have more error from the sampling process than the amount of confounding induced, and thus are not useful in benchmarking methods that adjust for confounding. 
Of the settings in Figure~\ref{fig:diagnostics-subpopa}, $\zeta_0 = 0.85$ and $\zeta_1 = 0.15$ had the best proportion of sampled datasets lying below the red line, so we choose these parameters for our proof of concept pipeline. We leave to future work providing more guidance on choosing settings of $P^{*}(T|C)$ for a comprehensive benchmark.

\subsection{Consideration \#4: Modeling}
The primary goal of this work is to create clear steps to follow during the evaluation design phase. Although this stage precedes a thorough modeling effort, we recommend that one runs baseline models to check for potential issues.

\subsubsection{Proof of concept approach}
As a proof of concept, we apply baseline causal estimation models to the resulting $D_{\text{OBS}}$ datasets after RCT rejection sampling (with many random seeds); as we mention above. 
%in practice we encourage the modeling group to be different from the evaluation design group. 
We implement\footnote{We attempted to use the \texttt{EconML} package \citet{econml}, but at of the time of our experiments it did not support using sparse matrices, which is required for our high-dimensional datasets.}   commonly-used causal estimation methods via two steps: (1) fitting base learners %(i.e.~predictive models with cross-fitting) 
and (2) using causal estimators that combine the base learners via plug-in principles or second-stage regression. %, i.e.~double machine learning \cite{chernozhukov2018double}.
We took care to ensure we use the \emph{same} pre-trained base learners---same functional form and learned weights---as inputs into any appropriate causal estimator. 

\textbf{Base learners.} We implement base learners for:\footnote{Note for a binary outcome $Y$, we can rewrite the above equations with probabilities, as $\E[Y \mid \cdot]=P(Y=1 \mid \cdot)$.}
{\begin{small}
\begin{align}
    Q_{T_0}(x) &:= \E[Y|T=0, X = x] \label{eqn:base-q0}\\
    Q_{T_1}(x) &:= \E[Y|T=1, X=x] \label{eqn:base-q1}\\
    g(x) &:= P(T=1|X=x) \label{eqn:base-g} \\
    Q_{X}(x) &:= \E[Y|X=x] \label{eqn:base-qx}
\end{align} \end{small}}

\noindent In our application both $T$ and $Y$ are binary variables, so we use an ensemble of gradient boosted decision trees (catboost) \footnote{Using CatBoost \citep{Dorogush2018CatBoostGB} with default parameters and without cross-validation.} and logistic regression\footnote{
Using scikit-learn \citep{pedregosa2011scikit} and an elasticnet penalty, L1 ratio 0.1, balanced class weights, and SAGA solver. We tune the regularization parameter $C$ via cross-validation over the set  $C \in {1e^{-4}, 1e^{-3}, 1e^{-2}, 1e^{-1}, 1e^{0}, 1e^{1}}$.} for our base learners.
We fit our models using cross-fitting \citep{hansen2000sample,newey2018cross} and cross-validation; see Appendix~\ref{sec:appendix-modeling} for more details. 

\textbf{Causal estimators.} 
After training base learners with cross-fitting, we implement the following plug-in causal estimators: backdoor adjustment (outcome regression) (Q), inverse propensity of treatment weighting (IPTW), and augmented inverse propensity of treatment weighting (AIPTW) \citep{Robins1994EstimationOR}.  We also use DoubleML \citep{chernozhukov2018double} which applies an ordinary least squares on residuals from the base learners.
See Appendix~\ref{sec:appendix-modeling} for exact estimation equations.

\begin{table*}[t!]
% FIRST TABLE
% Katie: Results 2023-01-11 from scripts/pipeline_DB-CY.
  \centering
  \resizebox{0.95\linewidth}{!}{ %makes it fit within the margin limits
      \begin{tabular}{lrrrrrrrr}
      \toprule
      & \multicolumn{2}{c}{$\hat{g}(x)$} 
      & \multicolumn{2}{c}{$\hat{Q}_{T_0}(x)$} 
      & \multicolumn{2}{c}{$\hat{Q}_{T_1}(x)$}
       & \multicolumn{2}{c}{$\hat{Q}_X(x)$}
      \\
       \textbf{Prediction Ave. Prec.} ($\uparrow$ better)
 & \multicolumn{1}{c}{train} 
 & \multicolumn{1}{c}{inference} 
 & \multicolumn{1}{c}{train} 
 & \multicolumn{1}{c}{inference} 
 & \multicolumn{1}{c}{train} 
 & \multicolumn{1}{c}{inference} 
 & \multicolumn{1}{c}{train} 
 & \multicolumn{1}{c}{inference} 
\\
      \toprule
     %%%%% COPY AND PASTE FROM .IPYNB BELOW %%%%
      linear& 0.89 (0.07) \cellcolor {yellow!89.0}& 0.59 (0.02) \cellcolor {green!59.0}& 0.63 (0.32) \cellcolor {yellow!63.0}& 0.03 (0.01) \cellcolor {green!3.0}& 0.85 (0.17) \cellcolor {yellow!85.0}& 0.13 (0.03) \cellcolor {green!13.0}& 0.71 (0.2) \cellcolor {yellow!71.0}& 0.06 (0.01) \cellcolor {green!6.0}
      \\
      catboost (nonlinear)& 0.97 (0.0) \cellcolor {yellow!97.0}& 0.60 (0.02) \cellcolor {green!60.0}& 1.0 (0.0) \cellcolor {yellow!100}& 0.03 (0.01) \cellcolor {green!3.0}& 0.99 (0.01) \cellcolor {yellow!99.0}& 0.13 (0.02) \cellcolor {green!13.0}& 0.98 (0.01) \cellcolor {yellow!98.0}& 0.05 (0.01) \cellcolor {green!5.0} \\
      \bottomrule
  \end{tabular}
  }
  % \vspace{0.5cm}
 \resizebox{0.9\linewidth}{!}{ %makes it fit within the margin limits 
  \begin{tabular}{lrrrrrr}
      \toprule
      \textbf{Causal Rel.~Abs.~Error} ($\downarrow$ better)
      & Unadjusted (baseline) & Backdoor $C$ (oracle) & $\hat{\tau}_Q$ & $\hat{\tau}_{\text{IPTW}}$& $\hat{\tau}_{\text{AIPTW}}$& $\hat{\tau}_{\text{DML}}$ \\
      \toprule
      linear& 0.21 (0.08)& 0.12 (0.09)& 1.46 (1.04)\cellcolor {red!100}& 0.47 (0.16)\cellcolor {red!47.0}& 1.6 (0.66)\cellcolor {red!100}& 1.91 (0.9)\cellcolor {red!100}
      \\
      catboost (nonlinear)& 0.21 (0.08)& 0.12 (0.09)& 0.24 (0.1)\cellcolor {red!24.0}& 0.14 (0.11)\cellcolor {red!14.0}& 0.11 (0.1)\cellcolor {red!11.0}& 0.13 (0.1)\cellcolor {red!13.0} 
      \\
      \bottomrule
  \end{tabular}
  }
  % \vspace{0.5cm}
  \caption{Modeling results for subpopulation A. 
\textbf{Top:} Predictive models' average precision (ave.~prec.) for training (yellow) and inference (green) data splits. \textbf{Bottom:} Causal estimation models' relative absolute error (rel.~abs.~error) between the models' estimated ATE and the RCT ATE. Here, darker shades of red indicate worse causal estimates. Baselines, unadjusted conditional mean on the samples (unadjusted) and the backdoor adjustment with the oracle $C$ (backdoor C), are uncolored. 
We use two baselearner settings: linear and catboost (nonlinear). We report both the average and standard deviation (in parentheses) over 100 random seeds during sampling. 
All settings use $P^*(T|C)$ in \eqref{eqn:our-p-star} parameterized by $\zeta_0=0.85, \zeta_1=0.15$.
\label{t:big-results}}
\end{table*}

\textbf{Modeling results.} Table~\ref{t:big-results} shows results for Subpopulation A. Since both $T$ and $Y$ are binary, we report average precision (AP) for the base learners on both the training and inference folds; 
this metric is only calculated for observed (not counterfactual) observations. We also report the relative absolute error (RAE) between estimators on $D_{\text{OBS}}$ and the ATE for $D_{\text{RCT}}$. 
Comparing predictive base learners, the propensity score model, $g(x)$, has much higher AP on inference folds than models that involve the outcome. As we previously mentioned, RCT subsampling allows us to set the relationship between $X$ (via proxy for $C$) and $T$ but not $X$ and $Y$ so the low AP for outcome models could reflect the difficulty in estimating this ``natural'' relationship between $X$ and $Y$. See Section~\ref{sss:consider5-poc} for additional discussion on low average precision for the outcome models ($Q$). 

For causal estimators, we see that the doubly robust estimator AIPTW using catboost has the lowest estimation error---on par with estimates obtained using the oracle backdoor adjustment set $C$. It appears the doubly robust estimators using linear models do not recover from the poor predictive performance of the outcome models and IPTW is better in this setting. 
Though seemingly counterintuitive that the linear and catboost models have similar  predictive performance but large differences in the causal estimation error, this discrepancy between predictive performance and causal estimation error is consistent with theoretical results on fitting nuisance models in causal inference \citep{tsiatis2006semiparametric, shortreed2017outcome}
and empirical results from semi-synthetic evaluation \citep{shi2019adapting,wood2021generating}.
Although we used best practices from machine learning to fit our base learners, these results suggest future work is needed to adapt machine learning practices to the goals of causal estimation.

\subsection{Consideration \#5: Additional finite data challenges}\label{ss:consider6}

The broader purpose of this line of empirical evaluation is to understand the real-world settings for which certain estimation approaches are successful or unsuccessful. We believe bridging the gap between theory that holds asymptotically and finite data is important for drawing valid causal inference, but evaluation designers might encounter unforeseen challenges particular to finite data that need to be examined carefully.  


\subsubsection{Proof of concept approach} \label{sss:consider5-poc}

In our proof of concept pipeline, we hypothesize the outcome models have very low average precision (Table~\ref{t:big-results}) because of finite data issues with class imbalance. 
In particular, for Subpopulation A, 82\% of our data is $C=1$ and $\E[Y]=0.07$ so there are few examples to learn from in the smallest category ($C=0$, $Y=1$): only 34 documents. This shows that even with our RCT that has a relatively large size (roughly 4k units) compared to other real-world RCTs, there are challenges with having sufficient support. 

\section{Discussion and Future Work}\label{sec:conclusion}
Unlike predictive evaluation, empirical evaluation for causal estimation is challenging and still at a nascent stage. In this work, we argue that one of the most promising paths forward is to use a RCT subsampling strategy, and to this line of work, we contribute an RCT rejection sampler with theoretical guarantees. We showed the utility of this algorithm in a proof of concept pipeline with a novel, real-world RCT to empirically evaluate high-dimensional backdoor adjustment methods.

Of course, there are critics of emprical evaluation. \citet{hernan2019comment} pejoratively compared a competition for causal estimation to evaluating ``spherical cows in a vacuum'' and claimed this discounted necessary subject-matter expertise. Even in the machine learning community, researchers warn against ``mindless bake-offs'' of methods \citep{langley2011changing}, and in some cases the creation of benchmarks has led to the community overfitting to benchmarks, e.g.,~\citet{recht2019imagenet}. However, in the absence of theory or when theoretical assumptions do not match reality, we see empirical evaluation as a necessary, but not exclusive, part of the broader field of causal inference.

A fruitful future direction is for evaluation designers to use our RCT rejection sampler to create comprehensive benchmarks for various phenomena of interest: not only high-dimensional confounding but also heterogeneous treatment effects, unmeasured confounding, missing data, etc. This would involve gathering more RCTs and establishing interesting ways to set $P^{*}(T|C)$. Our proof of concept evaluation pipeline demonstrated the utility of RCT subsampling but there were many avenues we chose not to pursue such as: %extensions beyond binary outcome and binary treatment,
%measuring confidence interval coverage, 
measurement error, causal null hypothesis tests, or moving to more sophisticated natural language processing approaches beyond bag-of-words, e.g.,~the CausalBERT model \citep{veitch2020adapting}.

In another direction, applied practitioners need guidance on which causal estimation method to use given their specific observational data. 
% \kkcomment{Rohit's point about in the loop, observational to RCTs... genetics, knock-out} 
Although other work has attempted to link observational data to experimental data (real or synthetic) in which the ground-truth is known \citep{neal2020realcause,kallus2018removing}, we believe RCT subsampling could help with meta analyses of which combination of techniques work best under which settings. Overall, we see this work as contributing to a much larger research agenda on empirical evaluation for causal estimation.

\subsubsection*{Broader Impact Statement}
We conducted this research with ethical due diligence. Our real-world RCT dataset was implemented by owners of the online platform and in full compliance with the platforms user agreement. The platform owners gave us explicit permission to use and access this dataset. Our dataset contains paper titles and abstracts, which are already publicly available from many sources, and we have removed any potentially personally identifiable information from the dataset, e.g., author names, user ids, user IP addresses, or session ids. By releasing this data, we do not anticipate any harm to authors or users. 

Like any technological innovation, our proposed RCT rejection sampling algorithm and evaluation pipeline have the potential for dual useto both benefit or harm society depending on the actions of the humans using the technology. We anticipate there could be substantial societal benefit from more accurate estimation of causal effects of treatments in the medical or public policy spheres. However, other applications of causal inference could potentially harm society by controlling or manipulating individuals. Despite these tradeoffs in downstream applications, we feel strongly this paper's contributions will result in net overall benefit to the research community and society at large. 


\ifthenelse{\boolean{isAnonymized}}
{} % This happens if the boolean is true
{\subsubsection*{Author Contributions}
KK conceived the original idea of the project and managed the project. 
RB contributed the ideas behind Algorithm 1 as well as the proofs in Section 3 and the Appendix. RB and KK implemented the synthetic experiments in Section 3. KK gathered and cleaned the data for the proof of concept pipeline in Section 4. KK and SF implemented the proof of concept empirical pipeline in Section 4. KK and RB wrote the first draft of the manuscript. KK, SF, DJ, JB, and RB guided the research ideas and experiments and edited the manuscript. 
} % This happens if the boolean is false


\ifthenelse{\boolean{isAnonymized}}
{} % This happens if the boolean is true
{\subsubsection*{Acknowledgments}
The authors gratefully thank David Jensen, Amanda Gentzel, Purva Pruthi, Doug Downey, Brandon Stewart, Zach Wood-Doughty and Jacob Eisenstein for comments on earlier drafts of this manuscript. The authors also thank anonymous reviewers from ICML and TMLR for helpful comments. Special thanks to the Semantic Scholar team at the Allen Institute for Artificial Intelligence for help gathering the real-world RCT dataset. 
} % This happens if the boolean is false

\bibliography{bib}
\bibliographystyle{tmlr}

\clearpage

\appendix
\section{Proof of Proposition~\ref{proposition-gentzel}} \label{sec:appendix-proof-gentzel}

\begin{proof} 
Since the selection variable $S$ is created using a structural equation dependent on $T$ and $C$ and samples are retained only when $S=1$, the distribution of the selected samples  $P^*(C, T, Y) = P(C, T, Y \mid S=1)$ and is Markov relative to Fig.~\ref{fig:selection-bias}(b). In the absence of additional constraints on $P^*$ (e.g., linearity assumptions) or the relation between $P$ and $P^*$, it is known that the ATE is non-parametrically identified in the presence of selection bias if and only if no variable that is a causal ancestor of the outcome in a graph where one deletes the treatment variable is also a causal ancestor of the selection variable \citep[Theorem 2]{bareinboim2015recovering}. In this case,  if we delete $T$ from Fig.~\ref{fig:selection-bias}(b), $C$ remains a causal ancestor of $Y$ and $S$. Hence, the effect is not identified via any non-parametric functional of the observed data and condition (II) cannot be satisfied.
\end{proof}

\section{Viewing Algorithm~\ref{alg:R2C} as drawing from a conditionally ignorable causal model}\label{sec:appendix-markov}

A causal model of a DAG $\G(V)$ can be interpreted as (i) a set of statistical distributions $P(V)$ that factorize according to $\G$: $P(V)=\prod_{V_i\in V}P(V_i\mid \Pa_i)$; and (ii) a set of post intervention distributions given by the g-formula aka truncated factorization \citep{robins1986new, spirtes2000causation, pearl2009causality}: for every $A\subset V$, we have $P(V\setminus A \mid \doo(A=a)) = \prod_{V_i \in V\setminus A} P(V_i \mid \Pa_i)\  \vert_{A=a}$.

If it were possible to recollect data through a conditionally randomized experiment where treatment is assigned with probability $P^*(T\mid C)$ instead of $P(T)$, the observed data distribution over $C, T, Y$ would factorize according to the standard conditionally ignorable model shown in Fig.~\ref{fig:selection-bias}(c). That is, the distribution factorizes as $P^*(C, T, Y) = P(C) \times P^*(T\mid C) \times P(Y\mid T, C)$ and implies no independence constraints on the observed data. The distribution of samples $P^*(C, T, Y)$ output by the rejection sampler in Algorithm~\ref{alg:R2C} also implies no independence constraints on $C, T, Y$ and thus has the exact same factorization. This establishes statistical equivalence of the conditionally ignorable model and the one obtained via our rejection sampler.

However, statistical equivalence alone is insufficient, as it is statistically equivalent to any complete DAG on the variables $C, T, Y$ that imply no independence constraints on the observed data. However, it is easy to see that all post-intervention distributions obtained via truncated factorization in the conditionally ignorable model also have the same identifying functionals in the model obtained through the rejection sampler due to the extra constraints imposed in Algorithm~\ref{alg:R2C}. For example, $P^*(T, Y \mid \doo(C=c)) = P^*(T\mid C=c)\times P(Y\mid T, C)$ and $P^*(C, Y \mid \doo(T=t)) = P(C)\times P(Y\mid T=t, C)$ in both the conditionally ignorable model and the one obtained via rejection sampling. Since the distribution obtained from the rejection sampler is indistinguishable from a conditionally ignorable model both in terms of the statistical model and all post-intervention distributions,  applying Algorithm~\ref{alg:R2C} can be viewed as essentially drawing samples from a standard conditionally ignorable model. That is, despite the selection mechanism, the resulting distribution is indistinguishable from Figure~\ref{fig:selection-bias}(c), the regular causal DAG model we use to represent observed confounding.


\section{Synthetic DGPs to evaluate sampling algorithms}\label{sec:appendix-dgps}

%KK: provenance--https://github.com/allenai/PURL/blob/obs-sampl-checklist/scripts/simulations/expr-rejection-sampling.ipynb
We use the following data-generating process (DGP) to create synthetic data with 100k units for which the true ATE is equal to 2.5. We call this \textbf{Setting 1}: 
\begin{align*} 
C &\sim \text{Binomial}(0.5) \\
T &\sim \text{Binomial}(0.3) \\
Y &\sim 0.5 C + 1.5 T + 2 TC + \text{Normal}(0, 1)
\end{align*}
We set the researcher-specified confounding function $P^{*}(T=1|C) = \sigma(-1 + 2.5 C)$, where $\sigma$ is the logistic (expit) function, for RCT Rejection sampling and the same function as $f$ in \citet{gentzel2021and}'s Algorithm 2. 

\textbf{Setting 2} has the same DGP as Setting 1 but we change $T \sim \text{Binomial}(0.5)$. For Settings 1 and 2, we provide the backdoor adjustment estimator with the true adjustment set, such that it adjusts for both $C$ and the interaction term, $TC$.

\textbf{Setting 3} involves more covariates that are combined in non-linear ways:
\begin{align*}
C_1 \sim&\  \text{Binomial}(0.5) \\
C_2 \sim& \ C_1 - \text{Uniform}(-0.5, 1) \\
C_3 \sim& \ \text{Normal}(0, 1) \\
C_4 \sim& \ \text{Normal}(0, 1) \\
C_5 \sim& \ C_3 + C_4 +  \text{Normal}(0, 1) \\
T \sim& \ \text{Binomial}(0.3) \\
Y \sim& \ 0.5C_4 + 2 T C_1 C_2 - 1.5 T + C_2 C_3 + C_5 + \text{Normal}(0, 1) 
\end{align*}
\noindent In this setting, we set $P^{*}(T=1|C) = \sigma(0.5 C_1 + -0.7 C_2 + 1.2 C_3 + 1.5 C_4 + -1.2 C_5 +0.5 C_1 C_2)$. We provide the parametric backdoor estimator with the true adjustment set: $C_4, T C_1 C_2, C_2 C_3, C_5$. 

See Table~\ref{table:sampling-dgps} for the ATEs for each of the three settings. 

\begin{table*}[h!]
\centering 
% \resizebox{0.6\linewidth}{!}{
\begin{tabular}{lr}
  \toprule
 \textbf{DGP Setting} & RCT ATE \\
  \toprule 
  1 - Linear, $P(T=1)=0.3$ & 2.48\\
 2 - Linear, $P(T=1)=0.5$ & 2.49\\ 
3 - Nonlinear, 5 $C$'s & -0.26\\
 \bottomrule
\end{tabular}
% }
\caption{RCT ATEs for the synthetic DGPs. 
\label{table:sampling-dgps}
}
\end{table*}

\section{RCT dataset details}\label{sec:appendix-rct}

We expand on the details of the RCT described in Section~\ref{ssec:proof-concept-data}. The RCT was conducted on the Allen Institute for Artificial Intelligence's Semantic Scholar\footnote{\url{https://www.semanticscholar.org/}} platform.

%Katie 2022-01-09 stats from casual-eval/raw_data_processing/semantic_reader_button/select_c.ipynb
 The experiment ran for 25 days from 2022-06-04 to 2022-06-28 and resulted in 53,281 unique users arriving on 50,833 unique paper pages. This is after filtering to users who are ``active'', meaning prior to the experiment they clicked somewhere on the website at least once.  
 %Katie from Kelsey: https://docs.google.com/document/d/1xWwOgKhf7WRgQ8_S9ZlY_tfRLm9bhCwgBvmvJGknR_I/edit
 Treatment is randomized for the combination of a unique user's browser plus user's device. We then post-process to recognize logged-in users across devices/browsers and remove them from the results if they switched treatments. The outcome of interest is if a user clicks on the ``enhanced reader'' button at least once during a session ($Y=1$).
 
 Intuitively, a positive causal effect is expected since we expect advertising a feature of the website to result in increased click rate. The final ATE was 0.113 as computed by a simple difference in conditional means $\E[Y\mid T=1] - \E[Y \mid T=0]$.

%TODO: camera ready 
% put in screenshots from this doc https://docs.google.com/document/d/1xWwOgKhf7WRgQ8_S9ZlY_tfRLm9bhCwgBvmvJGknR_I/edit

\section{Creating vocabulary for $X$}\label{sec:appendix-vocab}
A new vocabulary is created for each RCT subpopulation.
To create each vocabulary, we use binary indicators, remove stopwords, remove numbers and strip accents. Words must occur in at least 5 documents. We ignore terms that have a document frequency strictly lower than 10\%.

\section{Modeling} \label{sec:appendix-modeling}

\textbf{Base learners.} 
Base learners $Q_{T_0}$ and $Q_{T_1}$ in Equations~\ref{eqn:base-q0} and~\ref{eqn:base-q1} respectively are fit as described by \citet{kunzel2019metalearners} as ``T-learners'' (\emph{not} ``S-learners''), i.e.~we take all samples for which we have observed $T=0$ and then regress $X$ on $Y$ to get a trained model for $Q_{T_0}$ and likewise for units with observed $T=1$ and $Q_{T_1}$.
In preliminary experiments, we used the ``S-learner'' but found high-dimensional $X$ dominated $T$ and there were no differences learned between observed and counterfactual $T$ settings. 


\textbf{Cross-fitting with cross-validation.}
We fit our models using \emph{cross-fitting} \citep{newey2018cross} which is also called sample-splitting \citep{hansen2000sample}. 
Here, we divide the data into $K$ folds. For each inference fold $j$, the other $K-1$ folds (shorthand $-j$) are used as the training set to fit the base learners---e.g.,~$\hat{Q}_{T_0}^{-j}$ or $\hat{g}^{-j}$---where the superscript here indicates the data the model is fit on. The single hyperparameter for logistic regression is selected via cross-validation, where the training set is again split into folds. No cross-validation is performed for CatBoost. Then for each unit $i$ in the inference set, we use the trained models to infer $\hat{Q}_{T_0}(x_i) = \hat{Q}^{-j}_{T_0}(x_i)$. Then  these are inserted into the plug-in estimators to compute the average treatment effect, $\tau$ for each estimator below. 

\textbf{Causal estimators.} 
After training base learners with cross-fitting, we implement the following plug-in causal estimators: backdoor adjustment (outcome regression) (Q), inverse propensity of treatment weighting (IPTW), adjusted inverse propensity of treatment weighting (AIPTW) \citep{Robins1994EstimationOR}. 

\begin{small}
\begin{align} 
\hat{\tau}_Q &:= \frac{1}{n} \sum_i \bigg( \hat{Q}_{T_1}(x_i) -  \hat{Q}_{T_0}(x_i) \bigg) 
\\ %T-learner 
\hat{\tau}_{\text{IPTW}} &:= \frac{1}{n} \sum_i \bigg( \frac{y_i t_i}{\hat{g}(x_i)} - \frac{y_i(1-t_i)}{1-\hat{g}(x_i)} \bigg) \\
\hat{\tau}_{\text{AIPTW}} &:= \frac{1}{n} \sum_i \bigg(
\hat{Q}_{T_1}(x_i) - \hat{Q}_{T_0}(x_i)  + t_i \frac{y_i - \hat{Q}_{T_1}(x_i)}{\hat{g}(x_i)}
 \nonumber \\ 
& \hspace{3cm} - (1- t_i)  \frac{y_i - \hat{Q}_{T_0}(x_i)}{1-\hat{g}(x_i)} \bigg)
\end{align} 
\end{small}

We also use DoubleML \citep{chernozhukov2018double} which applies an ordinary least squares on residuals from the base learners
\begin{align}
\hat{\tau}_{\text{DML}} &:= \hat{E}\big[(y - \hat{Q}_X(x) ) \big| (t - \hat{g}(x))\big]
\end{align}

 \section{Proof of concept pipeline for an additional subpopulation}\label{sec-appendix:sub-b}

 As an additional proof of concept, we follow the same steps for the proof of concept in Section~\ref{sec:finite-data} but we use a different subpopulation---\emph{Subpopulation B} for which the covariates $C$ are Engineering and Business document categories. Table~\ref{t:descript-stats-subpopB} provides the descriptive statistics for this subpopulation. To measure the predictive accuracy for this subpopulation, we again model $P(C|X)$ with a logistic regression classifier. Averaged across held-out test folds, the F1 score is 0.92 and the average precision is 0.97.

 Addressing consideration \#4, we also create diagnostic plots for Subpopulation B in Figure~\ref{figure:diagnostics-subB}. In the subsequent pipeline, we use $P^*(T|C)$ in \eqref{eqn:our-p-star} parameterized by $\zeta_0=0.85, \zeta_1=0.15$. Examing the modeling results in Table~\ref{t:subpopB-modeling-results}, we hypothesize that the slight performance improvement in estimators using catboost over adjustment with the oracle set $C$ is due to inclusion of extra covariates in $X$ granting additional statistical efficiency \citep{rotnitzky2020efficient}.
 Regarding class imbalance, Subpopulation B is slightly more balanced (compared to Subpopulation A) with  55\% business, $\E[Y] = 0.05$ so that smallest category ($C=0$, $Y=1$) has 49 documents. However, this subpopulation also suffers from finite data and class imbalance issues, as evidenced by the low average precision for the inference folds of the outcome models. 

 % Katie 2022-01-09 first row came from casual-eval/raw_data_processing/semantic_reader_button/select_c.ipynb
\begin{table}[h!]
  \centering
  \resizebox{0.7\linewidth}{!}{ %makes it fit within the margin limits
      \begin{tabular}{llrrr}
      \toprule
      % Katie 2022-01-09 first row came from casual-eval/raw_data_processing/semantic_reader_button/select_c.ipynb
      RCT Dataset & $C$ categories & n & RCT ATE & OR($C$, $Y$) \\
      \toprule
      Subpopulation B & Engineering, Business & 2,238 & 0.075 & 1.4 \\
      \bottomrule
  \end{tabular}
  }
  \caption{For \textbf{Subpopulation B}, RCT dataset descriptive statistics including the number of units in the subpopulation ($n$) and the odds ratio, $OR(C, Y)$.  \label{t:descript-stats-subpopB}
  }
\end{table}

% Figure environment removed

\begin{table*}[h!]
\begin{minipage}{\linewidth}
  \centering
  \resizebox{0.95\linewidth}{!}{ %makes it fit within the margin limits
      \begin{tabular}{lrrrrrrrr}
      \toprule
      & \multicolumn{2}{c}{$\hat{g}(x)$} 
      & \multicolumn{2}{c}{$\hat{Q}_{T_0}(x)$} 
      & \multicolumn{2}{c}{$\hat{Q}_{T_1}(x)$}
       & \multicolumn{2}{c}{$\hat{Q}_X(x)$}
      \\
       \textbf{Prediction Ave. Prec.} ($\uparrow$ better)
 & \multicolumn{1}{c}{train} 
 & \multicolumn{1}{c}{inference} 
 & \multicolumn{1}{c}{train} 
 & \multicolumn{1}{c}{inference} 
 & \multicolumn{1}{c}{train} 
 & \multicolumn{1}{c}{inference} 
 & \multicolumn{1}{c}{train} 
 & \multicolumn{1}{c}{inference} 
\\
      \toprule
     %%%%% COPY AND PASTE FROM .IPYNB BELOW %%%%
      linear& 0.98 (0.01) \cellcolor {yellow!98.0}& 0.78 (0.02) \cellcolor {green!78.0}& 0.67 (0.24) \cellcolor {yellow!67.0}& 0.03 (0.02) \cellcolor {green!3.0}& 0.56 (0.22) \cellcolor {yellow!56.0}& 0.17 (0.03) \cellcolor {green!17.0}& 0.59 (0.17) \cellcolor {yellow!59.0}& 0.14 (0.02) \cellcolor {green!14.0}
      \\
      catboost (nonlinear)& 0.99 (0.0) \cellcolor {yellow!99.0}& 0.79 (0.02) \cellcolor {green!79.0}& 0.97 (0.02) \cellcolor {yellow!97.0}& 0.03 (0.01) \cellcolor {green!3.0}& 0.99 (0.01) \cellcolor {yellow!99.0}& 0.21 (0.04) \cellcolor {green!21.0}& 0.99 (0.0) \cellcolor {yellow!99.0}& 0.15 (0.03) \cellcolor {green!15.0} \\
      \bottomrule
  \end{tabular}
  }
  
 \resizebox{0.8\linewidth}{!}{ %makes it fit within the margin limits 
  \begin{tabular}{lrrrrrr}
      \toprule
      \textbf{Causal Rel.~Abs.~Error} ($\downarrow$ better)
      & Unadjusted & Backdoor $C$ & $\hat{\tau}_Q$ & $\hat{\tau}_{\text{IPTW}}$& $\hat{\tau}_{\text{AIPTW}}$& $\hat{\tau}_{\text{DML}}$ \\
      \toprule
      linear& 0.14 (0.06)& 0.14 (0.1)& 2.84 (1.43)\cellcolor {red!100}& 0.43 (0.94)\cellcolor {red!43.0}& 1.51 (2.28)\cellcolor {red!100}& 0.48 (0.2)\cellcolor {red!48.0}
      \\
      catboost (nonlinear)& 0.14 (0.06)& 0.14 (0.1)& 0.11 (0.08)\cellcolor {red!11.0}& 0.11 (0.08)\cellcolor {red!11.0}& 0.12 (0.09)\cellcolor {red!12.0}& 0.14 (0.1)\cellcolor {red!14.0} 
      \\
      \bottomrule
  \end{tabular}
  }
\end{minipage}

\captionof{table}{ Modeling results for Subpopulation B. 
\textbf{Top:} Predictive models' average precision (ave.~prec.) for training (yellow) and inference (green) data splits. \textbf{Bottom:} Causal estimation models' relative absolute error (rel.~abs.~error) between the models' estimated ATE and the RCT ATE. Here, darker shades of red indicate worse causal estimates. Baselines, unadjusted conditional mean on the samples (unadjusted) and the backdoor adjustment with the oracle $C$ (backdoor C), are uncolored. 
We use two baselearner settings: linear and catboost (nonlinear). We report both the average and standard deviation (in parentheses) over 100 random seeds during sampling. 
All settings use $P^*(T|C)$ in \eqref{eqn:our-p-star} parameterized by $\zeta_0=0.85, \zeta_1=0.15$.
\label{t:subpopB-modeling-results}}
\end{table*}


\tmlrChanges{\section{Confidence Interval Plots}\label{appendix-sec:ci}}

% Figure environment removed

\tmlrChanges{In Figure~\ref{fig:cis}, we plot the confidence intervals given by the bootstrap percentile method (see Section~\ref{subsec:synthetic} for more details). First, we construct confidence intervals for the original RCT data with a difference in means estimator. Then we plot the confidence intervals for the two sampling proceduresRCT rejection sampling and Algorithm 2 from Gentzel et al.---applied to that same RCT data. We examine synthetic DGP \#1 (see Section \ref{sec:appendix-dgps} for details on the synthetic DGPs) for a single random seed across two different sizes of synthetic RCT data, 100K samples and 3K samples.}

\tmlrChanges{
In the plots, the dot is the mean of the 1000 bootstrap samples. The horizontal bars indicate the endpoints of the 95\% confidence interval. We note that the confidence intervals for the sampling procedures are wider because sampling reduces the size of the dataset (roughly by half). Also as expected, all the confidence intervals are wider for the smaller (3K) RCT data setting. Note, in both settings, our RCT rejection sampling contains the true (RCT) ACE (red line in the plot) while Gentzel et. als algorithm does not.}

\hfill{3cm}

\tmlrChanges{\section{Varying Confounding Strength}\label{appendix-sec:vary-confound}}

% Figure environment removed


\end{document}
