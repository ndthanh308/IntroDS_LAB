\documentclass{article}

\DeclareUnicodeCharacter{2212}{-}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% % ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}


%our custom packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{nccmath}
\usepackage{bbm}
%\usepackage{bbold}
\usepackage{mathtools}



\usepackage{comment}
\usepackage{caption}
\usepackage{subcaption}

%\usepackage{natbib}

\usepackage[ruled]{algorithm2e}
\usepackage{algorithmic}

\input{defs}



\newcommand{\LK}{\mathcal{L}_K}

\newcommand{\mcD}{\mc{D}}
\newcommand{\mcZ}{\mc{Z}}
\newcommand{\mcX}{\mc{X}}
\newcommand{\mcQ}{\mc{Q}}
\newcommand{\mcP}{\mc{P}}
\newcommand{\mcH}{\mc{H}}
\newcommand{\mcT}{\mc{T}}
\newcommand{\mcN}{\mc{N}}
\newcommand{\mcC}{\mc{C}}

\newcommand{\divides}{\mid}
\newcommand{\notdivides}{\nmid}

\newcommand{\Dtrain}{\mathcal{D}_{\text{train}}}

\newcommand{\onevec}{\mathbbm{1}}
\newcommand{\partd}[2]{\frac{\partial #1}{\partial #2}}


\usepackage{xcolor}
\newcommand{\notemk}[1]{{\color{blue}{ #1 }}}
\newcommand{\notebp}[1]{{\color{red}{ #1 }}}

% \newcommand{\notemk}[1]{{\color{blue}{}}}
% \newcommand{\notebp}[1]{{\color{red}{ }}}

\title{Implicitly Normalized Explicitly Regularized Density Estimation}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Mark Kozdoba  \\
  Technion - Israel Institute of Technology\\
  \And
  Binyamin Perets \\
  Technion - Israel Institute of Technology\\
  \And
  Shie Mannor\\
  Technion - Israel Institute of Technology\\
}


\begin{document}


\maketitle


\begin{abstract}
We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. 
We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 algorithms. 
\end{abstract}
% \iffalse
% In this paper we propose a new approach to density estimation that is based on regularizing a Sobolev norm of the density. In addition to controlling the complexity of the desnity, such normalization implicitly ensures that density has a finite positive integral. However, the explicit value of that normlization constant is not readily available although approximations may be possible.
% The new approach, which is provably different from Kernel Density Estimation makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling.  The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain solutions efficiently.
% The proposed approach is useful for a variety of tasks such as Anomaly Detection, where only relative value is important.
% Even in this case, while it is not possible to use likelihood for hyper parameter selection, it is possible to use the Fisher divergence by appropriately adapting Score Matching algorithms. We show that this approach then yields performance competitive with state-of-the-art anomaly detection methods on the thorough recent ADBench dataset collection.
% \end{abstract}
% \fi


\section{Introduction}
\label{sec:intro}
Density estimation is one of the central problems in statistical learning. In recent years, there has been a tremendous amount of work in the development of parametric neural network based density estimation methods, such as Normalizing Flows \cite{papamakarios2021normalizing}, Neural ODEs \cite{chen2018neural}, and Score Based methods, \cite{song2021scorebased}. However, the situation appears to be different for non parametric density estimation methods, \cite{wasserman2006all}, \cite{hardle2004nonparametric}. 
While there is recent work for low dimensional (one or two dimensional) data,  see for instance \cite{takada2008asymptotic}, \cite{uppal2019nonparametric}, \cite{cui2020nonparametric}, \cite{ferraccioli2021nonparametric}  (see also the survey \cite{kirkby2023spline}), there still are very few non-parametric methods applicable in higher dimensions.   
Compared to parametric models, non parametric methods 
are often conceptually simpler, and the model bias (e.g., prior knowledge, type of smoothness) is explicit. This may allow better interpretability, and better regularization control in smaller data regimes. 


Let $\mc{S} = \Set{x_i}_{i=1}^N \subset \RR^d$ be a set of data points sampled i.i.d from some unknown distribution.  In this paper we introduce and study a density estimator of the following form: 
\begin{equation}
\label{eq:estimate_def1}
f^* :=  \argmin_{f \in \mcH^{\tau}}  -\frac{1}{N}\sum_{i=1}^N \log f^2(x_i) + 
\norm{f}^2_{\mcH^{\tau}}.
\end{equation} 
Here $\mcH^{\tau}$ is a Sobolev type Reproducing Kernel Hilbert Space (RKHS) of functions, having a norm of the form
\begin{equation}
\label{eq:norm_def1}
\norm{f}_{\mcH^{\tau}}^2  = \int_{\RR^d} f^2(x) dx + \tau     
\int_{\RR^d} \Abs{(Df)}^2(x) dx,
\end{equation}
where $D$ represents a combination of derivatives of a certain order.
The density estimate is given by the function $(f^*)^2$. Note that 
$(f^*)^2$ is clearly non-negative, and $\norm{f}_{\mcH^{\tau}} < \infty$ implies $\int_{\RR^d} (f^*)^2(x) dx < \infty$. Thus $(f^*)^2$ is integrable, although not necessarily integrates to 1. 
Note also that \eqref{eq:estimate_def1} is essentially a regularized maximum likelihood estimate, where in addition to bounding  the total mass of $(f^*)^2$, we also bound the norm of the derivatives of $f^*$ of certain order. The fact that $\mcH^{\tau}$ is an RKHS allows us to compute $f^*$ via the standard Representer Theorem. Observe that it would not be possible to control only the norm $L_2$ norm of $f^*$ and maintain computabilty, since $L_2$ is not an RKHS. However, adding the derivatives with any coefficient $\tau>0$ makes the space into an RKHS, allows to control smoothness, and implicitly controls $\norm{f^*}_{L_2}$. Thus we call the objective Implicitly Normalized Explicitly Regularized density estimator (INER). 


Despite it being natural and simple, the objective \eqref{eq:estimate_def1} has not been studied in the literature as a tool for multidimensional data analysis. It has been introduced in \cite{goodd1971nonparametric} and further studied in \cite{klonias1984class}, in the context of spline based methods in one dimension. Our goal in this paper is to develop the necessary ingredients for INER to become useful in high dimensions. Specifically, for $d>1$, the kernel corresponding to $\mcH^{\tau}$, which we call the SDO kernel (Single Derivative Order; see Section \ref{sec:SDO_kernel_approximation}), no longer has an analytical expression. However, we show that nevertheless, it can be approximated by an appropriate sampling procedure. Next, standard gradient descent optimization of \eqref{eq:estimate_def1} produces poor results. We show that this may be improved by an appropriate initialization, and further improved by using a certain \emph{natural gradient} rather than the standard one. Finally, the solutions of \eqref{eq:estimate_def1} are unnormalized (see also the discussion below). 
This introduces a particular nuance in the context of hyperparameter tuning, as it prevents the utilization of the maximum likelihood measure to establish the optimal parameter. To bypass these normalization challenges, we apply a score-based method, which uses log-likelihood \emph{gradients} for divergence measurement, thereby eliminating the need for normalization. More specifically, we incorporate the concept of score-matching \citep{hyvarinen2005estimation,song2020sliced,song2019generative}, a technique that has recently garnered renewed interest. 
With these contributions in place, we show that INER achieves the remarkable performance of scoring \emph{second best} on a recent comprehensive Anomaly Detection benchmark, \cite{han2022adbench}, which includes more than 45 datasets and more than 15 specialized AD methods. 

In addition to the above contributions, we provide a family of examples where one prove that INER and the standard Kernel Density Estimator (with the same kernel) may arbitrarily differ. Thus, INER is a genuinely new estimator, with different properties than Kernel Density Estimation (KDE). We also show that examples as above occur naturally in real datasets.    


While \eqref{eq:estimate_def1} provides us with a function that may be normalized to be a density, computing the normalization constant is not straightforward, and is outside the scope of this paper. Instead, we will focus on the Anomaly Detection (AD) applications, that do not require the normalization constant. Indeed, AD is based on the comparison of the likelihoods of different points, requiring only the ratios of the density values at these points, which are independent of normalization. Note also that standard MCMC sampling algorithms, such as Langevin Dynamics or Hamiltonian Monte Carlo, do not require the knowledge of the normalization. 

%We introduce the hyperparameter optimization for QL, based on the score matching on the test set. See \cite{hyvarinen2005estimation} for score functions. The score computation includes the Hatchinson trace representation recently used in \cite{grathwohl2018scalable}, \cite{song2020sliced}, as well as additional approximations. 

The rest of the paper is organized as follows: In Section \ref{sec:literature} we review the related literature. In Section \ref{sec:INER_density_estimator} we introduce the INER estimator, treat the associated optimization questions, and provide an example where INER differs from KDE. The SDO kernel and the associated sampling approximation are discussed in Section \ref{sec:SDO_kernel_approximation}. Section \ref{sec:Exp} contains the experimental results. 


\section{Literature and Related Work}
\label{sec:literature}
A as discussed in Section \ref{sec:intro}, a scheme that is equivalent to \eqref{eq:estimate_def1} was studied in \cite{goodd1971nonparametric} and \cite{klonias1984class}; 
see also \cite{eggermont2001maximum}.  However, these works concentrated solely on 1d case, and used spline methods to solve \eqref{eq:main_optimization_problem} in the special case that amounts to the use of one particular kernel. Our more general RKHS formulation in Section \eqref{sec:basic_framework} allows the use of a variety of kernels. Most importantly, however, as discussed in Section \eqref{sec:intro}, in this work we have developed and evaluated the high dimensional version of INER. 


The most common non parametric density estimator is the Kernel Density Estimator (KDE), \cite{hardle2004nonparametric,wasserman2006all}. For comparison, we have evaluated KDE, with the two most popular kernels, Gaussian and Laplacian, on the AD benchmark. However, these methods did not perform well (Section \ref{sec:experiments_AD}) on this task. We have also evaluated KDE with the SDO kernel that we introduce in Section \ref{sec:SDO_kernel_approximation}, and which has not been previously considered in the literature for $d>1$. Remarkably, we find that using this kernel significantly improves the AD performance compared to Gaussain and Laplacian kernels. However, the performance is still subpar to the INER estimator. 


Another common group of non parametric density estimators are the   \emph{projection methods}, \cite{wainwright2019high}. These methods have mostly been studied in one dimensional setting, see the survey \cite{kirkby2023spline}. It is worth noting that with the exception of \cite{uppal2019nonparametric}, the estimators produced by these methods are not densities, in the sense that they do not integrate to 1, but more importantly, may take negative values. In the context of minmax bounds, projection methods in high dimensions were recently analyzed in \cite{singh2018nonparametric}, extending a classical work \cite{kerkyacharian1993density}. However, to the best of our knowledge, such methods have never have been practically applied in high dimensions. 



Fisher Divergence is a similarity measure between distributions, which is based on the score function -- the gradient of the log likelihood. In particular, it does not require the normalization of the density. The divergence between data and a model can be approximated via the methods of \cite{hyvarinen2005estimation}, which have been recently computationally improved in \cite{song2020sliced} in the context of score based generative models, \cite{song2019generative}. As discussed in Section \ref{sec:intro}, here we use Fisher Divergence as a quality metric for hyperparameter selection. In particular, we adapt the Hutchinson trace representation based methods used in \cite{song2020sliced} and \cite{grathwohl2018ffjord} to the case of models of the form \eqref{eq:main_optimization_problem}. Full details are given in the Supplementary Material. 


%Possibly comment on \cite{wang2019nonparametric}.


%The Laplace Kernel can be viewed a sort of Sobolev norm. This was shown in \cite{rakhlin2019consistency}. However, these are not the same norms as we use. 


The concept of a gradient that is independent of a parametrisation was proposed in \cite{amari1998natural}, and in \cite{mason1999boosting}. 
In \cite{kakade2001natural} it was introduced into Reinforcement Learning, where it is widely used today.  Here we consider specifically a Hilbert Space version of the notion, which also has a variety of applications, although typically not in RL. See for instance \cite{mason1999boosting}, \cite{yao2007early}, and \cite{shen2020sinkhorn} for a sample of early and more recent applications. Natural Gradient in Hilbert Spaces is also referred to as Functional Gradient in the literature.  While we are not aware of a dedicated treatment of the subject, introductory notes may be found at \cite{bagnell12funcgrad} and in the works cited above. 



\section{The INER Desnity Estimator}
\label{sec:INER_density_estimator}
In this Section we describe the general INER Density Estimation Framework, formulated in an abstract Reproducing Kernel Hilbert Space. We first introduce the general optimization problem and discuss a few of its properties. In Section \ref{sec:gradients} we discuss the gradient descent optimization and introduce the natural gradient.  In Section \ref{sec:two_regions} we provide an analytic example where the INER and KDE desnity estimators  may differ arbitrarily. 

\subsection{The Basic Framework}
\label{sec:basic_framework}
Let $\mcX$ be a set and let $\mcH$ be a Reproducing Kernel Hilbert Space (RKHS) of functions on $\mcX$, with kernel $k: \mcX \times \mcX \rightarrow \RR$. 
In particular, $\mcH$ is equipped with an inner product 
$\inner{\cdot}{\cdot}_{\mcH}$ and for 
every $x\in \mcX$, the function 
$k(x,\cdot) = k_x(\cdot) :\mcX \rightarrow \RR$ is in 
$\mcH$ and satisfies the reproducing 
property, $\inner{k_x}{f}_{\mcH} = f(x)$
for all $f \in \mcH$. The norm on $\mcH$ is denoted by  $\norm{f}_{\mcH}^2 = \inner{f}{f}_{\mcH}$, and the subscript $\mcH$ may be dropped when it is clear from context.  We refer to \cite{scholkopf2002learning} for a general introduction to RKHS theory. 



Given a set of points $S = \Set{x_1, \ldots, x_N} \subset \mcX$, we define the INER estimator as the solution to the following optimization problem:
\begin{equation}
\label{eq:main_optimization_problem}
f^* = \argmin_{f\in \mcH} -\frac{1}{N} \sum_i \log f^2(x_i)  + \norm{f}_{\mcH}^2.    
\end{equation}

As discussed in Section \ref{sec:intro}, for appropriate spaces $\mcH$, the function $(f^*)^2$ corresponds to an unnormalized density (That is, $\int_{\RR^d} (f^*)^2(x) dx < \infty$, but not necessarily $\int_{\RR^d} (f^*)^2(x) dx = 1$). 
We now discuss a few basic properties of the solution to \eqref{eq:main_optimization_problem}. First, by the Representer Theorem for RKHS, the minimizer of \eqref{eq:main_optimization_problem} has the form 
\begin{equation}
%\label{eq:f_star_alphas_repr}
\label{eq:f_kernel_definition}
    f(x) = f_{\alpha}(x) = \sum_{i=1}^N \alpha_i k_{x_i}(x) , \text{ for some } 
    \alpha = (\alpha_1, \ldots, \alpha_N) \in \RR^N. 
\end{equation}
Thus one can solve \eqref{eq:main_optimization_problem} by optimizing over a finite dimensional vector $\alpha$. Next, it is worth noting that standard RKHS problems, such as regression, typically use the term $\lambda \norm{h}^2_{\mcH}$, where $\lambda>0$ controls the  regularization strength. However, due to the special structure of \eqref{eq:main_optimization_problem}, any solution with $\lambda \neq 1$ is a rescaling by a constant of a $\lambda = 1$ solution. Thus considering only $\lambda = 1$ in \eqref{eq:main_optimization_problem} is sufficient. In addition, we note that any solution of \eqref{eq:main_optimization_problem} satisfies $\norm{f}_{\mcH}^2 = 1$. See Lemma \ref{lem:minimization_props} in Supplementary Material for full details on these two points.  

Next, observe that the objective 
\begin{equation}
\label{eq:L_f_def}
L(f) = 
-\frac{1}{N} \sum_i \log f^2(x_i)  + \norm{f}_{\mcH}^2  
= -\frac{1}{N} \sum_i \log \inner{f}{k_{x_i}}_{\mcH}^2  + \norm{f}_{\mcH}^2  
\end{equation} 
is not convex in $f$. This is due to the fact that the scalar function 
$a \mapsto - \log a^2$ from $\RR$ to $\RR$ is not convex and is undefined at $0$.  However, the restriction of $a \mapsto - \log a^2$ 
to $a \in (0, \infty)$ is convex.  Similarly, the restriction of $L$ the positive cone of functions $\mcC = \Set{ f \setsep f(x) \geq 0 \spaceo \forall x\in \mcX}$ is convex.  Empirically, we have found that the lack of convexity results in poor solutions found by gradient descent. Intuitively, this is caused by $f$ changing sign, which implies that $f$ should pass through zero at some points. If these points happen to be near the test set, this results in low likelihoods.  At the same time, there seems to be no computationally affordable way to restrict the optimization to the positive cone $\mcC$. We resolve this issue in two steps: First, we use a non-negative $\alpha$ initialization, $\alpha_i \geq 0$. Note that for $f$ given by \eqref{eq:f_kernel_definition}, if the kernel is non-negative, then $f$ is non-negative. Although some kernels are non-negative, the SDO kernel, and especially its finite sample approximation (Section \ref{sec:sampling_kernel_v1}) may have negative values. At the same time, there are few such values, and empirically such initialization tremendously improves the performance of the gradient descent.  Second, we use the \emph{natural gradient}, as discussed in the next section. One can show that for non-negative kernels, $\mcC$ is in fact invariant under natural gradient steps (supplementary material Section \ref{sec:invariance_of_cone_under_nat}). This does not seem to be true for the regular gradient. Empirically, this results in a more stable algorithm and further performance improvement. A comparison of standard and natural gradients w.r.t negative values is given in Section \ref{sec:positive_cone_nat_vs_alpha_test}.  



\subsection{Gradients and Minimization}
\label{sec:gradients}
We are interested in the minimization of $L(f)$, defined by \eqref{eq:L_f_def}. Using the representation \eqref{eq:f_kernel_definition}
%\begin{equation}
%\label{eq:f_kernel_definition}
%    f(x) = f_{\alpha}(x) =  \sum_{i=1}^N \alpha_i k(x_i,x) 
%\end{equation}
for $x \in \mcX$, we can equivalently consider minimization in $\alpha \in \RR^N$. Let $K = \Set{k(x_i,x_j)}_{i,j\leq N} \in \RR^{N \times N}$ denote the empirical kernel matrix. Then standard computations show that $\norm{f_{\alpha}}^2_{\mcH} = \inner{K\alpha}{\alpha}_{\RR^N}$ and we have $(f_{\alpha}(x_1), \ldots, f_{\alpha}(x_N)) = K\alpha$ (as column vectors). Thus one can consider $L(f_{\alpha}) = L(\alpha)$ 
as a functional $L : \RR^N \rightarrow \RR$ and explicitly compute the gradient w.r.t $\alpha$. This gradient is given in \eqref{eq:grad_alpha}. 

However, it is also useful to consider the Natural Gradient -- the gradient of $L(f)$ as a function of $f$, directly in the space $\mcH$. 
Briefly, a directional Fr\'echet derivative, \cite{munkres2018analysis}, of $L$ at point $f \in \mcH$ in direction $h \in \mcH$ is defined as the limit 
$D_hL(f) = \lim_{\eps \rightarrow 0} \eps^{-1}\cdot \Brack{L(f+\eps h) -L(f)}$. As a function of $h$, $D_hL(f)$ can be shown to be a bounded and linear functional, and thus by the Riesz Representation Theorem, there is a vector, which we denote $\grad_f L$, such that 
    $D_h L(f) = \inner{\grad_f L}{h}$  for all $h \in \mcH$.
We call $\grad_f L$ the Natural Gradient of $L$, since its uses the native space $\mcH$. Intuitively, this definition parallels the regular gradient definition, but uses the $\mcH$ inner product to define the vector $\grad_f L$, instead of the standard, ``parametrization dependent'' inner product in $\RR^N$, that is used to define $\grad_{\alpha} L$.  For the purposes of this paper, it is sufficient to note that similarly to the regular gradient, the natural gradient satisfies the chain rule, and we have $\grad_f \norm{f}^2_{\mcH} = 2f$ and $\grad_f \inner{g}{f}_{\mcH} = g$ for all $g\in \mcH$. The explicit gradient expressions are given below:
\begin{lem}[Gradients]
\label{lem:gradients}
The standard and the natural gradients of $L(f)$ are given by 
\begin{equation}
\label{eq:grad_alpha}
    \grad_{\alpha} L = 2 \SqBrack{ K \alpha - \frac{1}{N} K \Brack{K\alpha}^{-1}} \in \RR^N \text{ and }
    \grad_{f} L = 2 \SqBrack{ 
       f - \frac{1}{N} \sum_{i=1}^N f^{-1}(x_i) k_{x_i}
    } \in \mcH
\end{equation}
where for a vector $v\in \RR^d$, $v^{-1}$ means coordinatewise inversion. 
\iffalse
For the gradient in $\mcH$ we have
\begin{equation}
\label{eq:grad_f}
    \grad_{f} L = 2 \SqBrack{ 
       f - \frac{1}{N} \sum_{i=1}^N f^{-1}(x_i) k_{x_i}
    } \in \mcH
\end{equation}
\fi
\end{lem}
If one chooses the functions $k_{x_i}$ as a basis for the space $H_S = span \Set{k_{x_i}}_{i\leq N} \subset \mcH$, then $\alpha$ in \eqref{eq:f_kernel_definition} may be regarded as coefficients in this basis. For $f = f_{\alpha} \in H_S$ one can then write 
in this basis $\grad_{f} L = 2 \SqBrack{ \alpha - \frac{1}{N} (K\alpha)^{-1}} \in \RR^N$. 
Therefore in the $\alpha$-basis we have the following standard and natural gradient iterations, respectively: 
\begin{equation}
\label{eq:gradient_steps_in_alpha_coords}
\alpha \leftarrow \alpha - 2 \lambda     
\SqBrack{ K \alpha - \frac{1}{N} K \Brack{K\alpha}^{-1}} \text{ and } 
\alpha \leftarrow \alpha - 2 \lambda     
\SqBrack{\alpha - \frac{1}{N} \Brack{K\alpha}^{-1}},
\end{equation}
where $\lambda$ is the learning rate. 


\subsection{Difference between INER and KDE Models}
\label{sec:two_regions}
In this Section we construct an analytic example where the INER estimator may differ arbitrarily from the KDE estimator with the same kernel. Thus, the models are not equivalent, and encode different prior assumptions. Briefly, we consider a block model, with two clusters. We'll show that in this particular setting, 
in KDE the clusters influence each other more strongly, i.e the points in one cluster contribute to the weight of the points in other cluster, yielding more uniform models. In contrast, in INER, rather surprisingly, the density does not depend on the mutual position of the clusters (in a certain sense). Note that this is not a matter of \emph{bandwith} of the KDE, since both models use the same kernel. We believe that this property may explain the better performance of INER in Anomaly Detection tasks, although further investigation would be required to verify this. 



Given a set of datapoints $S = \Set{x_i}$, for the purposes of this section the KDE estimator is the function 
\begin{equation}
\label{eq:kde_iner_comp_kde}
    f_{kde}(x) = f_{kde,S}(x) = \frac{1}{\Abs{S}} \sum_{i} k_{x_i}(x).
\end{equation}
Let $f_{iner}$ be the solution of \eqref{eq:main_optimization_problem}.  
We will compare the ratios $f_{kde}(x_i)/f_{kde}(x_j)$ versus the corresponding quantities for INER, $f^2_{iner}(x_i)/f^2_{iner}(x_j)$
for some pairs $x_i,x_j$. Note that these ratios do not depend on the normalization of $f_{kde}$ and $f^2_{iner}$, and can be computed from the unnormalized versions. In particular, we do not require $k_{x_i}$ to be normalized in \eqref{eq:kde_iner_comp_kde}.


Consider a set $S$ with two components, $S = S_1 \cup S_2$,
with $S_1 = \Set{x_1,\ldots, x_N}$ and $S_2 = \Set{x'_1, \ldots x'_M}$ and with the following kernel values:
\begin{equation}
\label{eq:two_region_kernel} 
K =  \begin{cases}
        k(x_i,x_i) = k(x'_j,x'_j) = 1   & \text{ for all $i\leq N,j\leq M$} \\ 
        k(x_i,x_j) =  \gamma^2 & \text{for $i \neq j$}\\ 
        k(x'_i,x'_j) =  \gamma'^2 & \text{for $i \neq j$}\\ 
        k(x_i,x'_j) =  \alpha \gamma \gamma' &   \text{for all $i,j$ }      
    \end{cases}
\end{equation}
This configuration of points is a block model with two components, or two clusters. The correlations between elements in the first cluster are $\gamma^2$, and are $\gamma'^2$ in the second cluster. Inter-cluster correlations are $\alpha \gamma \gamma'$. We assume that $\gamma,\gamma,\alpha \in [0,1]$ and w.l.o.g take $\gamma > \gamma'$.
While this is an idealized scenario to allow analytic computations, settings closely approximating the configuration \eqref{eq:two_region_kernel} often appear in real data. See Section \ref{sec:kde_iner_comparison_empirical_example} for an illustration on the letter dataset from the ADBench suite. In particular, Figures \ref{fig:KDE_mtx} and \ref{fig:KDE_hist} show a two cluster configuration in that data, and the distribution of $k(x,x')$ values. 


The KDE estimator for $K$ is simply  
\begin{flalign}
    f_{kde}(x_t) = \frac{1}{N+M} \SqBrack{ 1 + (N-1) \gamma^2 + M \alpha \gamma \gamma'} 
    \approx \frac{N}{N+M} \gamma^2 + \frac{M}{N+M} \alpha \gamma \gamma',
\end{flalign}
for $x_t \in S_1$, where the second, approximate equality, holds for large $M,N$. To simplify the presentation, we shall use this approximation. However, all computations and conclusions also hold with the precise equality. For $x'_t \in S_2$ we similarly have 
   $ f_{kde}(x'_t)  \approx \frac{N}{N+M} \alpha \gamma \gamma' + \frac{M}{N+M} \gamma'^2$,
and when $M=N$, the density ratio is 
\begin{equation}
\label{eq:kde_iner_kde_ratio}
    \frac{f_{kde}(x_t)}{f_{kde}(x'_t)} = \frac{\gamma^2 + \alpha \gamma \gamma'}{\gamma'^2 + \alpha \gamma \gamma'}.
\end{equation}

The derivation of the INER estimator is considerably more involved. 
Here we sketch the argument, while full details are given in Supplementary Material Section \ref{sec:kde_vs_iner_comparison_proofs}.
First, recall from the previous section that the natural gradient in the $\alpha$ coordinates is given by $2\Brack{\alpha - N^{-1} (K\alpha)^{-1}}$. Since the optimizer of \eqref{eq:main_optimization_problem} must satisfy $\grad_f L =0$, we are looking for $\alpha \in \RR^{N+M}$ such that $\alpha = (K\alpha)^{-1}$ (the term $N^{-1}$ can be accounted for by renormalization). Due to the symmetry of $K$ and since the minimizer is unique, we may take $\alpha = (a,\ldots,a, b,\ldots,b)$, where $a$ is in first $N$ coordinates and $b$ is in the next $M$. 
Then $\alpha = (K\alpha)^{-1}$ is equivalent to $a,b$ solving the following system:
\begin{flalign}
\label{eq:ql_system_definition}
\begin{cases}
    a &= a^{-1} \SqBrack{1 + (N-1) \gamma^2} + b^{-1} M \alpha \gamma \gamma' \\
    b &= a^{-1} N \alpha \gamma \gamma'   + b^{-1} \SqBrack{1 + (M-1) \gamma'^2}      
\end{cases}
\end{flalign}
This is a non linear system in $a,b$. However, it turns out that it may be explicitly solved, up to a knowledge of a certain sign variable (see Proposition \ref{prop:two_variable_system}). Moreover, for $M=N$, the dependence on that sign variable vanishes, and we obtain
\begin{prop}
\label{cor:kde_iner_diff_cor}
Consider the kernel and point configuration described by \eqref{eq:two_region_kernel}, with $M=N$. Then for every $x_t\in S_1, x'_s \in S_2$, 
\begin{equation}
\label{eq:kde_iner_iner_ratio}
    \frac{f_{iner}(x_t)}{f_{iner}(x'_s)} = \frac{\gamma^2}{\gamma'^2}.
\end{equation}
In particular, the ratio does not depend on $\alpha$.
\end{prop}
It remains to compare the ratio \eqref{eq:kde_iner_iner_ratio} to KDE's ratio \eqref{eq:kde_iner_kde_ratio}. If $\alpha = 0$, when the clusters are maximally separated, the ratios coincide. However, let us consider the case, say, $\alpha = \half$, and assume that $\gamma' \ll \gamma$. Then in the denominator of \eqref{eq:kde_iner_kde_ratio} the larger term is $\alpha \gamma \gamma'$, which comes from the influence of the first cluster on the second. This makes the whole ratio to be of the order of a constant. On the other hand, in INER there is no such influence, and the ratio \eqref{eq:kde_iner_iner_ratio} may be arbitrarily large. We thus expect the gap between the cluster densities to be larger for INER, which is indeed the case empirically. One occurence of this on real data is illustrated in Figure \ref{fig:K_ratios} (see Section \ref{sec:kde_iner_comparison_empirical_example} for details).


\section{Single Derivative Order Kernel Approximation}
\label{sec:SDO_kernel_approximation}
In this Section we introduce the Single Derivative Order kernel, which corresponds to norms of the form \eqref{eq:norm_def1} discussed in Section \ref{sec:intro}. In Section \ref{sec:kernel_integral_form} we introduce the relevant Sobolev functional spaces and derive the Fourier transform of the norm. In Section \ref{sec:sampling_kernel_v1} we describe a sampling procedure that can be used to approximate the SDO.  



\subsection{The Kernel in Integral Form}
\label{sec:kernel_integral_form}
For a function $f:\RR^d \rightarrow \CC$ and a tuple $\alpha \in \Brack{\NN \cup \Set{0}}^d$,
let  
$D^{\alpha} = \frac{\partial f}{\partial x_1^{\alpha_1} \ldots \partial x_d^{\alpha_d}} $
denote the $\alpha$ indexed derivative. By convention, for $\alpha = (0,0,\ldots, 0)$ we set $D^{\alpha} f = f$.
Set also $\alpha! = \prod_{j=1}^d \alpha_j!$ and $\Abs{\alpha}_1 = \sum_{j=1}^d \alpha_j$. Set $\norm{f}_{L_2}^2 = \int \Abs{f(x)}^2 dx $. Then, for $m \in \NN$ and $a>0$ denote 
\begin{equation}
\label{eq:norm_definition_formal}
\norm{f}_a^2 = \norm{f}_{L_2}^2 + a  \sum_{|\alpha|_1 = m} \frac{m!}{\alpha!} \norm{\Brack{D^{\alpha} f}}_{L_2}^2.
\end{equation}
The norm $\norm{f}_a^2$ induces a topology that is equivalent to that of a standard $L_2$ Sobolev space of order $m$. We refer to \cite{adams2003sobolev}, \cite{saitoh2016theory} for background on Sobolev spaces. However, here we are interested in properties of the norm that are  finer than the above equivalence. For instance, note that for all $a\neq 0$ the norms $\norm{f}_a$ are mutually equivalent, but nevertheless, a specific value of $a$ is crucial in applications, for regularization purposes. Clearly, in this context, $a$ represents the previously mentioned regularization coefficient $\tau$.

Let $\mcH^a = \Set{f :\RR^d \rightarrow \CC \setsep \norm{f}_a^2 < \infty}$ be the space of functions with a finite $\norm{f}_a^2$ norm.
Denote by 
\begin{equation}
    \inner{f}{g}_{\mcH^a} = \inner{f}{g}_{L_2} + 
    a  \sum_{|\alpha|_1 = m} \frac{m!}{\alpha!} \inner{\Brack{D^{\alpha} f}}{\Brack{D^{\alpha} g}}_{L_2}^2
\end{equation} 
the inner product that induces the norm $\norm{f}_a^2$.
\begin{thm}
\label{thm:kernel_form}
For $m > d/2$ and any $a>0$, the space $\mcH^a$ admits a reproducing kernel $k^a(x,y)$ satisfying $\inner{k^a_x}{f}_{\mcH^a} = f(x)$ for all $f\in \mcH^a$ and $x\in \RR^d$. 
The kernel is given by 
\begin{equation}
\label{eq:kernel_fourier_inv}    
k^a(x,y) = \int_{\RR^d} \frac{e^{2\pi i \inner{y-x}{z}}}{
     1 +   
    a  \cdot (2\pi)^{2m} \norm{z}^{2m} 
} dz = 
\int_{\RR^d} \frac{1}{
     1 +   
    a  \cdot (2\pi)^{2m} \norm{z}^{2m} 
} \cdot  e^{2\pi i \inner{y}{z}} \cdot \overline{e^{2\pi i \inner{x}{z}}} dz.
\end{equation}
\end{thm}
The proof of Theorem \ref{thm:kernel_form} follows the standard approach of deriving kernels in Sobolev spaces, via computation and inversion of the Fourier transform, see \cite{saitoh2016theory}. However, compact expressions such as  \eqref{eq:kernel_fourier_inv} are only possible for some choices of derivative coefficients. Since the particular form \eqref{eq:norm_definition_formal} was not previously considered in the literature (except for $d=1$, see below), we provide the full proof in the Supplementary Material. 



% Figure environment removed


\subsection{Kernel Evaluation via Sampling}
\label{sec:sampling_kernel_v1}
To solve the optimization problem \eqref{eq:main_optimization_problem} in $\mcH^a$, we need to be able to evaluate the kernel $k^a$ at various points. For $d=1$, closed analytic expressions were obtained in cases $m=1,2,3$ in \cite{thomas1996computing}. In particular, for $m=1$, $k^a$ coincides with the Laplacian kernel $k_h(x,y) = e^{-h \Abs{x-y}}$. 
However, for $d>1$, it seems unlikely that there are closed expressions. See \cite{novak2018reproducing} for a discussion of this issue for a similar family of norms.  


To resolve this, note that the form \eqref{eq:kernel_fourier_inv} may be interpreted as an average of the terms $e^{2\pi i \inner{y}{z}} \cdot \overline{e^{2\pi i \inner{x}{z}}}$, where 
$z$ is sampled from an unnormalized density 
$w^a(z) = (1 +   a  \cdot (2\pi)^{2m} \norm{z}^{2m})^{-1} $ on $\RR^d$.  This immediately suggest that if we can sample from 
$w^a(z)$, then we can approximate $k^a$ by summing over a finite set of samples $z_j$ instead of computing the full integral. 


In fact, a similar scheme was famously previously employed in \cite{rahimi2007random}. There, it was observed that by Bochners's Theorem, \cite{rudin2017fourier}, any stationary kernel can be represented as $k(x,y) = \int \nu(z) e^{2\pi i \inner{y}{z}} \cdot \overline{e^{2\pi i \inner{x}{z}}} dz$ for some non-negative measure $\nu$. Thus, if one can sample $z_1,\ldots,z_T$ from $\nu$, one can construct an approximation
\begin{equation}
\label{eq:kernel_sample_approx}
    \hat{k}^a(x,y) = \frac{1}{T} \sum_{t=1}^T 
    \cos\Brack{\inner{z_t}{x} +b_t} \cdot \cos\Brack{\inner{z_t}{y} +b_t},
\end{equation}
where $b_t$ are additional i.i.d samples, sampled uniformly from 
$[0,2\pi]$.  In \cite{rahimi2007random}, this approximation was used as a dimension reduction for \emph{known} analytic kernels, such as the Gaussian, for which the appropriate $\nu$ are known. Note that the samples $z_t,b_t$ can be drawn once, and subsequently used for all $x,y$ (at least in a bounded region, see the uniform approximation result in \cite{rahimi2007random}).  

For the case of interest in this paper, the SDO kernel, Bochner's representation is given by \eqref{eq:kernel_fourier_inv} in Theorem \ref{thm:kernel_form}. Thus, to implement the sampling scheme \eqref{eq:kernel_sample_approx} it remains to describe how one can sample from the density $w^a(z)$ on $\RR^d$. To this end, note that 
$w^a(z)$ is spherically symmetric, and thus can be decomposed as 
$z = r \theta$, where $\theta$ is sampled uniformly from a unit sphere $S^{d-1}$ and the radius $r$ is sampled from a \emph{one dimensional} density $u^a(r) = \frac{r^{d-1}}{1 + a (2\pi r)^{2m}}$ (see the Supplementary Material for full details on this change of variables).  Next, note that sampling $\theta$ is easy. Indeed, let $g_1,\ldots,g_d$ be i.i.d standard Gaussians. Then $\theta \sim (g_1,\ldots,g_d)/\sqrt{\sum_i g_i^2}$.  Thus the problem is reduced to sampling a one dimensional distribution with a single mode, with known (unnormalized) density. This can be efficiently achieved by methods such as Hamiltonian Monte Carlo (HMC). However, we found that in all cases a sufficiently fine grained discretization of the line was sufficient. 


\section{Experiments}\label{sec:Exp}
In this section we present the evaluation of INER on the ADBench anomaly detection benchmark, empirically test the advantage of natural gradient descent for maintaining a non-negative $f$, and compare the likelihoods of INER and SDO based KDE, illustrating the result of Section \ref{sec:two_regions}.

% Figure environment removed

\subsection{Anomaly Detection Results for ADbench}
\label{sec:experiments_AD}
This section presents an evaluation of our approach on real-world tasks and data, focusing on Anomaly Detection (AD) where normalized density is not a concern. 
AD was chosen for evaluation due to its inherent attributes that align closely with density estimation, including the differentiation of samples from the latent and out-of-distribution. We compare our results to a golden standard AD benchmark, ADbench (\cite{han2022adbench}), that evaluates a wide range of 15 AD algorithms on over 47 labeled datasets. In addition, we evaluate KDE using both Gaussian and Laplace kernels, and as an ablation study, we compare INER to KDE with SDO kernel.

We focus on the unsupervised setup, in which no labeled anomalies are given to the methods in the training phase. 
%We believe this setup is the most suitable one to evaluate  density estimation based approaches. 
For all density-based approaches, we employ the negative of the density as the 'anomaly score'. 
The ADbench paper evaluates success on each dataset using AUC-ROC. In addition to AUC-ROC, we also focus on a ranking system as follows: for each dataset, we convert raw AUC-ROC scores of the methods into rankings from 1 to 18. Here, 18 denotes the best performance on a given dataset, and 1 the worst. This mitigates bias inherent in averaging AUC-ROC scores themselves across datasets, due to generally higher AUC-ROC scores on easier datasets. This is important since no single AD method consistently outperforms others in all situations, as discussed in detail in \cite{han2022adbench}.


For both AUC-ROC and rank evaluations, \textbf{INER emerges as the 2nd best AD method overall}.
%, and the best under robustness settings}. 
Notably, this achievement is with the 'vanilla' version of our method, without any pre or post-processing dedicated to AD. In contrast, many other methods are specifically tailored for AD and include extensive pre and post-processing. In Figure \ref{fig:ADbench}, for each algorithm we present the 
box plot with the average ranking over all datasets (along with quantiles). The algorithms are sorted by the average ranking. A similar plot for raw AUC-ROC values is given in the supplementary material, and it presents a similar picture. 


Hyperparameter tuning for the smoothness parameter ($\tau$) in both INER and KDE with the SDO kernel employs the FD-based approach outlined in Section \ref{sec:intro} and further detailed in the Supplementary Material. Figure \ref{fig:FD} illustrates the variation of FD across the parameter of INER for several datasets, demonstrating that a clear minimum is readily identifiable in practice. In the figure, the Y-axis is scaled as $log\left(FD(\tau) + |min(FD(\tau))| + 1 \right)$ for optimal visibility. As for computational cost, the entire set of 47 datasets was processed in 186 minutes using a single 3090RTX GPU and one CPU, averaging about 4 minutes per dataset.


In addition to performing well on the standard ADBench benchmark, and perhaps even more impressively, INER excels also on the more demanding setup of \emph{duplicate anomalies}, which was also extensively discussed in \citep{han2022adbench}. Here,  \textbf{INER rises to the forefront as the top AD method} (with an average AUC-ROC of 71.6 for X5 duplicates - a lead of 4$\%$ over the closest contender). This scenario, which is analogous to numerous practical situations such as equipment failures, is a focal point for ADbench's assessment of unsupervised anomaly detection methods due to its inherent difficulty, leading to substantial drops in performance for former leaders like Isolation Forest. More detailed explanations are available in the Supplementary Material. Figure \ref{fig:DE_methods} depicts the divergence from the mean AUC-ROC results for each density estimation method across all datasets included in ADbench. The distinct advantage of INER and its minimal correlation with other methods underscore our assertion that INER is a truly distinctive density estimation method.



\subsection{Natural-Gradient vs Standard Gradient Comparison}
\label{sec:positive_cone_nat_vs_alpha_test}
We conduct an experiment to demonstrate that the standard gradient descent may significantly amplify the fraction of negative values in a solution, while the natural gradient keeps it constant. See also the related discussion in Section \ref{sec:basic_framework}. We have randomly chosen 15 datasets from ADBench, and for each dataset we have used 50 non negative $\alpha$ initializations. Then we have run both algorithms for 1000 iteartions. The fraction of negative values 
of $f_{\alpha}$ (on the train set) was measured at initialization, and in the end of each run. In Figure \ref{fig:pos_neg_frac}, for each dataset and for each method, we show an average of the highest 5 fractions among the 50 initializations. Thus, for instance, for the 'shuttle' data, the initial fraction is negligible, and is unchanged by the natural gradient. However, the standard gradient (``alpha GD'' in the Figure, blue) yields about 70\% negative values in the 5 worst cases (i.e. 10\% of initializations). 

\iffalse
In this section, we present empirical evidence suggesting that employing natural gradient based gradient descent is more likely to optimize a convex loss function due to the non-negativity of $f$, enhancing the robustness of the optimization procedure. This validation sheds light on the interplay between the optimization procedure and the properties of the kernel $K$. The evaluation utilizes datasets from ADbench. 

Figure \ref{fig:pos_neg_frac} presents this evaluation, which begins with determining the fraction of negative values in the initial $f_0$ (only positive alphas were used) and then measures the fraction of negative values in the final $f$ post-optimization, for both the natural gradient and alpha gradient methods. This examination involves 15 randomly chosen datasets, each subjected to 50 repetitions, and the mean fraction of negative values is calculated for the top five repetitions for each dataset. As Figure \ref{fig:pos_neg_frac} demonstrates, over half of the datasets tested using alpha gradient descent produced substantial fractions of negative values, with some instances primarily yielding negative $f$. This, along with the absence of such outcomes in natural gradient descent, underscores the significance of employing natural gradient in our methodology.
\fi



% Figure environment removed


\subsection{INER vs KDE Comparison}
\label{sec:kde_iner_comparison_empirical_example}
In this experiment, we empirically compare the behaviour of INER and KDE for the same kernel. See the related discussion in Section \ref{sec:two_regions}.  We have performed spectral clustering of the ``letter'' dataset from ADBech, using the empirical SDO kernel as affinity matrix. We then have chosen two clusters that most resemble the two block model \eqref{eq:two_region_kernel} in Section \ref{sec:two_regions}. The kernel values inside and between the clusters are shown in Figures \ref{fig:KDE_mtx}, \ref{fig:KDE_hist}. 
Next, we train the INER and KDE models for just these two clusters (to be compatible with the setting of Section \ref{sec:two_regions}. The results are similar for densities trained on full data). The log of these INER and KDE densities in shown in Figure \ref{fig:K_ratios} (smoothed by running average). By adding appropriate constant, we have arranged that the mean of both log densities is 0 on the first cluster. Then one can clearly see that the gap between the values on the first and second cluster is larger for the INER model, yielding a less uniform model, as expected from the theory in Section \ref{sec:two_regions}.

\iffalse
In this section, we explore the implications of the theoretical results discussed in Section \ref{sec:two_regions}, focusing on a bi-modal distribution case. We selected a suitable dataset, one that shows two distinct, approximately equal-sized clusters. Figure \ref{fig:KDE_mtx} displays the kernel after spectral clustering, and Figure \ref{fig:KDE_hist} illustrates the disparities in kernel values between the clusters through histograms.

Next, we estimate densities using both the KDE and INER methods. To compare the density estimators, we normalize the densities for the larger cluster, and present the log-likelihoods for points sampled from each cluster (Figure \ref{fig:K_ratios}). For clarity, individual point likelihoods are shown as '+' markers, while a rolling window is used to provide a smoothed approximation of these values (shown as a line).
Our results (Figure \ref{fig:K_ratios}) show a distinct advantage for INER, as it displays a greater difference in log-likelihoods between the clusters. This ability to distinguish between different clusters underscores INER's effectiveness as a density estimator. Remember that these are log ratios, which means even small differences can be meaningful. This increased discriminatory power, recognizing and emphasizing the inherent structure in the data, provides a more accurate representation of the underlying densities.
\fi 
\newpage
\bibliographystyle{apalike}
\bibliography{a_density_estimates.bib}

\newpage
\appendix

\section{Overview Of The Supplementary Material}
This Supplementary Material is organized as follows: 

\begin{itemize}
    \item Some basic properties of the INER objective and of the related minimization problem: Section \ref{sec:basic_properties_of_minimizer}
    \item Derivation of the Gradients of the INER objective: Section \ref{sec:gradients_lemma_proof}
    \item Invariance of the non-negative function cone under natural gradient steps:  Section \ref{sec:invariance_of_cone_under_nat}
    \item Proofs related to the INER vs KDE comparison: Section \ref{sec:kde_vs_iner_comparison_proofs}
    \item Derivation of the integral form of SDO kernel, proof of Theorem 3:  Section \ref{sec:full_kernel_derivation_proof}
    \item Additional details on sampling approximation procedure:  Section \ref{sec:SDO_sampling_approximation_details}
    \item Details on the Fisher Divergence estimation for INER Hyperparameter tuning : \ref{SEC:FD}
    \item Comparison of the raw AUC-ROC metric on ADBench data: Section \ref{sec:supp_aucroc}  
    \item Discussion of an additional test regime, with duplicated anomalies: Section \ref{sec:supp_duplicate_anomalies}
    \item On-the-fly hyperparameter tuning procedure that was used to save time by finding the first \emph{stable} local minimum: Section \ref{sec:supp:HP tuning FD}
\end{itemize}

The code used for all the experiments in the paper will be publicly released with the final version of the paper.

\section{Basic Minimizer Properties}
\label{sec:basic_properties_of_minimizer}
As discussed in Section \ref{sec:basic_framework}, the minimizer of the INER objective \eqref{eq:main_optimization_problem} always has $\mcH$ norm 1. In addition, there is no added value in multiplying the norm by a regularization scalar, since this only rescales the solution. Below we prove these statements. 
\begin{lem}
\label{lem:minimization_props}
Define 
\begin{equation}
\label{eq:minimization_lemma_objective}
    f = \argmin_{h\in \mcH} -\frac{1}{N} \sum_i \log h^2(x_i)  + \norm{h}_{\mcH}^2.
\end{equation}
Then $f$ satisfies $\norm{f}^2 = 1$. 
Moreover, if 
\begin{equation}
    f' = \argmin_{h\in \mcH} -\frac{1}{N} \sum_i \log h^2(x_i)  + \lambda^2 \norm{h}_{\mcH}^2,
\end{equation}
for some $\lambda >0$, 
then $f' = \lambda^{-1} f$. 
\end{lem}

\begin{proof}
For any $h\in \mcH$ and $a>0$, 
\begin{flalign}
\label{eq:a_optimization}
    &\argmin_{a>0} -\frac{1}{N} \sum_i \log (ah)^2(x_i)  + \norm{ah}^2 = \\
    &\argmin_a -\frac{1}{N} \sum_i \log h^2(x_i)  -\log a^2 + a^2 \norm{h}^2.
\end{flalign}
Taking derivative w.r.t $a$ we have 
\begin{equation}
    -\frac{2a}{a^2} + 2a \norm{h}^2 = 0. 
\end{equation}
Thus optimal $a$ for the problem \eqref{eq:main_optimization_problem} must satisfy  
 $\norm{ah}^2 = a^2 \norm{h}^2 = 1$. 
To conclude the proof of the first claim, choose $h$ in \eqref{eq:a_optimization} to be the minimizer in \eqref{eq:minimization_lemma_objective}, $h=f$.
Note that if $\norm{f}_{\mcH} \neq 1$, then we can choose $a = \norm{f}_{\mcH}^{-1} \neq 1$ to further decrease the value of the objective contradicting the fact that $f$ is the minimizer. 


For the second claim, denoting $g = \lambda h$, 
\begin{flalign}
    & \argmin_{h \in \mcH} -\frac{1}{N} \sum_i \log h^2(x_i)  + \lambda^2 \norm{h}^2      \\        
     &= \lambda^{-1} \argmin_{g  \in \lambda \mcH = \mcH} -\frac{1}{N} \sum_i \log g^2(x_i)  + \norm{g}^2     + \frac{1}{N} \sum_i \log \lambda^2  \\ 
     &= \lambda^{-1} \argmin_{g  \in \mcH} -\frac{1}{N} \sum_i \log g^2(x_i)  + \norm{g}^2  \\ 
     &= \lambda^{-1} f.
\end{flalign}


\end{proof}


\section{Derivation of the Gradients, Proof Of Lemma \ref{lem:gradients}}
\label{sec:gradients_lemma_proof}
In this section we derive the expressions for standard and the natural gradients of the objective \eqref{eq:L_f_def}, as given in Lemma \ref{lem:gradients}.
\begin{proof}[Proof Of Lemma \ref{lem:gradients}]
We first derive the expression for $\grad_{\alpha} L$ in \eqref{eq:grad_alpha}. Recall that 
$\norm{f}_{\mcH}^2 = \inner{\alpha}{K \alpha}_{\RR^N}$ for $\alpha \in \RR^N$, where $K_{ij} = k(x_i,x_j)$. This follows directly from the form \eqref{eq:f_kernel_definition}, and the fact that $\inner{k_x}{k_y} = k(x,y)$ for all $x,y\in \mcH$, by the reproducing property. For this term we have  $\grad_{\alpha} \inner{\alpha}{K \alpha} = 2 K \alpha$. 
Next, similarly by using \eqref{eq:f_kernel_definition},
$\grad_{\alpha} f(x) = (k(x_1,x), \ldots, k(x_N,x))$ for every $x \in \RR^d$.
Finally, we have 
\begin{flalign}
    \grad_{\alpha} \frac{1}{N} \sum_{i=1}^N \log f^2(x_i) &= 
    \frac{1}{N} \sum_{i=1}^N f^{-2}(x_i) \cdot 2 f(x_i) \cdot \grad_{\alpha} f(x_i) \\ 
    &= 
    2 \frac{1}{N} \sum_{i=1}^N f^{-1}(x_i) \cdot \grad_{\alpha} f(x_i) \\ 
    &= 2 \frac{1}{N} K (f(x_1), \ldots, f(x_N))^{-1} \\ 
    &= 2 \frac{1}{N} K \Brack{K\alpha}^{-1}.
\end{flalign}
This yields \eqref{eq:grad_alpha}. 

For $\grad_f L$, we similarly have $\grad_f \norm{f}_{\mcH}^2 = 2 f$, as discussed in section \ref{sec:gradients}.  
Moreover, 
\begin{flalign}    
    \grad_f \frac{1}{N} \sum_{i=1}^N \log f^2(x_i) &= 
    \grad_f \frac{1}{N} \sum_{i=1}^N \log \inner{f}{x_i}^2_{\mcH}  \\
    &= 
    \frac{1}{N} \sum_{i=1}^N \inner{f}{x_i}^{-2}_{\mcH}  
    \cdot 2\inner{f}{x_i}_{\mcH} \cdot 
    \grad_f \inner{f}{x_i}_{\mcH} \\ 
    &= 2\frac{1}{N} \sum_{i=1}^N \inner{f}{x_i}^{-1}_{\mcH}   
    {x_i}.
\end{flalign}
This completes the proof.
\end{proof}


\section{SDO Kernel Details}
In Section \ref{sec:full_kernel_derivation_proof} we provide a full proof of Theorem \ref{thm:kernel_form}, while Section \ref{sec:SDO_sampling_approximation_details} contains additional details on the sampling approximation of SDO. 
\subsection{SDO Kernel Derivation}
\label{sec:full_kernel_derivation_proof}

%\ref{sec:SDO_kernel_approximation}, \ref{sec:kernel_integral_form}, \ref{sec:sampling_kernel_v1}
We will prove a claim that is slightly more general than Theorem \ref{thm:kernel_form}. For a tuple $\bar{a} \in \RR_{+}^m$, define the norm 
\begin{equation}
\label{eq:a_bar_norm_def}
    \norm{f}_{\bar{a}}^2 = \sum_{l=0}^m  
    a_l \sum_{|\alpha|_1 = l} \frac{l!}{\alpha!} \norm{\Brack{D^{\alpha} f}}_{L_2}^2,
\end{equation}
where $D^{\alpha}$ are the $\alpha$-indexed derivative, as discussed in Section \ref{sec:kernel_integral_form}. The SDO norm is a special case with $a_0 = 1$, $a_m = a$, and $a_l = 0$ for $0<l<m$.
Let $\mcH^{\bar{a}}$ be the subspace of $L_2$ of functions with finite norm, 
\begin{equation}
    \mcH^{\bar{a}} = \Set{ f \in L_2 \setsep \norm{f}_{\bar{a}} < \infty}
\end{equation}
and let the associated inner product be denoted by 
\begin{equation}
    \inner{f}{g}_{\bar{a}} = \sum_{l=0}^m  
    a_l \sum_{|\alpha|_1 = l} \frac{l!}{\alpha!} \inner{\Brack{D^{\alpha} f}}{\Brack{D^{\alpha} g}}_{L_2}.
\end{equation}
Define the Fourier transform 
\begin{equation}
\label{eq:fourier_def}
    \mc{F}f(z) = \int_{\RR^d} f(u) e^{-2\pi i  \inner{z}{u}} du,
\end{equation}
and recall that we have (see for instance \cite{stein1971introduction}, \cite{grafakos2008classical})
\begin{equation}
\label{eq:fourier_derivative}
    \mc{F}\Brack{D^{\alpha} f}(z) = \Brack{\prod_{j=1}^d \Brack{2\pi i z_j}^{\alpha_j} } \mc{F}f (z) \text{ for all } z \in \RR^d. 
\end{equation}

The following connection between the $L_2$ and the derivative derived  norms is well known for the standard Sobolev spaces (\citep{williams2006gaussian,saitoh2016theory,novak2018reproducing}).
However, since \eqref{eq:a_bar_norm_def} somewhat differs from the standard definitions, we provide the argument for completeness. 
\begin{lem}
\label{lem:norm_in_fourier}
Set for $z \in \RR^d$
\begin{equation}
    \label{eq:va_def}
    v_{\bar{a}}(z) = \Brack{ 1 + \sum_{l=1}^m  
    a_l  \cdot (2\pi)^{2l} \norm{z}^{2l} 
          }^{\half}.
\end{equation}
Then for every $f \in \mcH^{\bar{a}}$ we have 
    \begin{equation}
        \norm{f}_{\bar{a}}^2 = \norm{v_{\bar{a}}(z) \cdot \mc{F}[f]}^2_{L_2}.
    \end{equation}
\end{lem}
\begin{proof}
\begin{flalign}
    \norm{f}_{\bar{a}}^2 &= \sum_{l=0}^m  
    a_l \sum_{|\alpha|_1 = l} \frac{l!}{\alpha!} \norm{D^{\alpha} f}_{L_2}^2 \\ 
    &= \sum_{l=0}^m  
    a_l \sum_{|\alpha|_1 = l} \frac{l!}{\alpha!} \norm{\mc{F} \SqBrack{D^{\alpha} f}}_{L_2}^2 \\
    &= \int dz \SqBrack{ \sum_{l=0}^m  
    a_l \sum_{|\alpha|_1 = l} \frac{l!}{\alpha!} \Abs{\mc{F} \SqBrack{D^{\alpha} f}(z) }^2}  \\
    &= \int dz \SqBrack{ \Abs{\mc{F}\SqBrack{f}(z)}^2 + \sum_{l=1}^m  
    a_l \sum_{|\alpha|_1 = l} \frac{l!}{\alpha!} 
        \Brack{\prod_{j=1}^d \Brack{2\pi  z_j}^{2 \alpha_j} }   
        \Abs{\mc{F}\SqBrack{f}(z)}^2  }   \\ 
    &= \int dz \Abs{\mc{F}\SqBrack{f}(z)}^2 \SqBrack{ 1 + \sum_{l=1}^m  
    a_l \sum_{|\alpha|_1 = l} \frac{l!}{\alpha!} 
        \Brack{\prod_{j=1}^d \Brack{2\pi  z_j}^{2 \alpha_j} }   
          }   \\
    &= \int dz \Abs{\mc{F}\SqBrack{f}(z)}^2 \SqBrack{ 1 + \sum_{l=1}^m  
    a_l   \cdot (2\pi)^{2l} \sum_{|\alpha|_1 = l} \frac{l!}{\alpha!} 
        \prod_{j=1}^d z_j^{2 \alpha_j} 
          }    \\ 
    &= \int dz \Abs{\mc{F}\SqBrack{f}(z)}^2 \SqBrack{ 1 + \sum_{l=1}^m  
    a_l  \cdot (2\pi)^{2l} \norm{z}^{2l} 
          }             
\end{flalign}  

\end{proof}

Using the above Lemma, the derivation of the kerenl is standard. 
Suppose $k^{\bar{a}}$ is the kernel corresponding to $\norm{f}_{\bar{a}}$ on $\mcH^{\bar{a}}$. 
It remains to observe that by the reproducing property and by Lemma \ref{lem:norm_in_fourier}, for all 
$x \in \RR^d$
\begin{flalign}
f(x) &= \inner{f}{k^{\bar{a}}_x}_{\bar{a}}  \\ 
     &= \int_{\RR^d} dz \spaceo  \mc{F}[f](z) \overline{\mc{F}[k^{\bar{a}}_x](z)} v_{\bar{a}}^2(z).
\end{flalign}
On the other hand, by the Fourier inversion formula,  we  have 
\begin{flalign}
f(x) &=  \int dz \spaceo  \mc{F}[f](z) e^{2\pi i \inner{x}{z}}.
\end{flalign}
This implies that 
\begin{equation}
    \int dz \spaceo  \mc{F}[f](z) e^{2\pi i \inner{x}{z}} = 
    \int_{\RR^d} dz \spaceo  \mc{F}[f](z) \overline{\mc{F}[k^{\bar{a}}_x](z)} v_{\bar{a}}^2(z)
\end{equation}
holds for all $f \in \mcH^{\bar{a}}$, which by standard continuity considerations yields 
\begin{equation}
    \mc{F}[k^{\bar{a}}_x](z) = \frac{e^{-2\pi i \inner{x}{z}}}{v_{\bar{a}}^2(z)}.
\end{equation}
Using Fourier inversion again we obtain
\begin{equation}
\label{eq:kernel_fourier_inv_old}    
k^{\bar{a}}(x,y) = \int_{\RR^d} \frac{e^{2\pi i \inner{y-x}{z}}}{v_{\bar{a}}^2(z)} dz = 
\int_{\RR^d} \frac{e^{2\pi i \inner{y-x}{z}}}{
     1 + \sum_{l=1}^m  
    a_l  \cdot (2\pi)^{2l} \norm{z}^{2l} 
} dz.
\end{equation}

\subsection{Sampling Approximation}
\label{sec:SDO_sampling_approximation_details}

As discussed in Section \ref{sec:sampling_kernel_v1}, we are interested in sampling points $z\in \RR^d$ from a finite non negative measure with density given by $w^a(z) = (1 +   a  \cdot (2\pi)^{2m} \norm{z}^{2m})^{-1}$. 
With a slight overload of notation, we will also denote by 
$w_a$ the scalar function $w_a: \RR \rightarrow \RR$, 
\begin{equation}
    w^a(r) = (1 +   a  \cdot (2\pi)^{2m} r^{2m})^{-1}.
\end{equation}

First, note that $w_a(z)$ depends on $z$ only through the norm $\norm{z}$, and thus a spherically symmetric  function. Therefore, with a spherical change of variables,  we can rewrite the integrals w.r.t $w_a^{-2}$ as follows: For any 
$f: \RR^d \rightarrow \CC$,
\begin{flalign}
    \int_{\RR^d} w_a(z) f(z) dz &= \int_{0}^\infty dr \int_{S^{d-1}} d\theta    \spaceo w_a(r) A_{d-1}(r) f(r \theta) \\
    &= 
    A_{d-1}(1) \int_{0}^\infty dr \int_{S^{d-1}} d\theta  \spaceo   \SqBrack{w_a(r) r^{d-1}} \cdot  f(r \theta).    \label{eq:w_a_sampling_full_expression}
\end{flalign}
Here $S^{d-1}$ the unit sphere in $\RR^d$, $\theta$ is sampled from the uniform probability measure on the sphere, $r$ is the radius, and 
\begin{equation}
    A_{d-1}(r) = \frac{2 \pi^{d/2}}{\Gamma(d/2)}r^{d-1} 
\end{equation}
is the $d-1$ dimensional volume of the sphere or radius $r$ in $\RR^d$. The meaning of \eqref{eq:w_a_sampling_full_expression} is that 
to sample from $w_a^{-2}$, we can sample $\theta$ uniformly from the sphere (easy), and $r$ from a density 
\begin{equation}
    \zeta(r) = w_a(r) r^{d-1}  = \frac{r^{d-1}}{1 +   a  \cdot (2\pi)^{2m} r^{2m}}
\end{equation}
on the real line. Note that the condition $m>d/2$ that we impose throughout is necessary. Indeed,  without this condition the decay of $\zeta(r)$ would not be fast enough at infinity, and the density would not have a finite mass. 


As discussed in Section \ref{sec:sampling_kernel_v1}, $\zeta(r)$ is a density on a real line, with a single mode and an analytic expression, which allows easy computation of the derivatives. Such distributions can be efficiently sampled using, for instance,  off-the-shelf Hamiltonian Monte Carlo (HMC) samplers, \cite{betancourt2017conceptual}. In our experiments we have used an even simpler scheme, by discretizing $\RR$ into a grid of 10000 points, with limits wide enough to accommodate a wide range of parameters $a$.

%\section{Temporary Section}
%\section{Temporary Section}
%\section{Temporary Section}


\section{A Few Basic Properties of the Kernel}
\begin{prop} The kernel \eqref{eq:kernel_fourier_inv} is real valued and satisfies 
    \begin{equation}
        K^a(x,y) = \int_{\RR^d} \frac{\cos\Brack{2\pi \inner{y-x}{z}}}{1 +   
    a  \cdot (2\pi)^{2m} \norm{z}^{2m}} dz.
    \end{equation}    
\end{prop}
\begin{proof}
    Write $e^{2\pi i \inner{y-x}{z}} = cos(2\pi \inner{y-x}{z}) + i \sin(2\pi \inner{y-x}{z})$ 
    and observe that  $sin$ is odd in $z$, while $1 +   
    a  \cdot (2\pi)^{2m} \norm{z}^{2m}$  is even.
\end{proof}


\begin{prop} For all $x,y \in \RR^d$,
\begin{equation}
K^b(x,y) = b^{-\frac{d}{2m}} K^1(b^{-\frac{1}{2m}} x, b^{-\frac{1}{2m}} y) 
\end{equation}
\end{prop}
\begin{proof}
    Write $u = b^{\frac{1}{2m}}z$ and note that $du = (b^\frac{1}{2m})^d dz$. 
    We have 
\begin{flalign}
    K^b(x,y) &= \int_{\RR^d} \frac{\cos\Brack{2\pi \inner{y-x}{z}}}{
    1 + b \norm{2 \pi  \cdot z }^{2m} 
    } dz \\ 
    &= \int_{\RR^d} \frac{\cos\Brack{2\pi \inner{b^{-\frac{1}{2m}}(y-x)}{u}}}{
    1 + \norm{2 \pi u }^{2m} 
    } du  \cdot b^{-\frac{d}{2m}} \\ 
    &= 
    b^{-\frac{d}{2m}} K^1(b^{-\frac{1}{2m}} x, b^{-\frac{1}{2m}} y).
\end{flalign}
\end{proof}

\section{KDE vs INER Comparison Proofs}
\label{sec:kde_vs_iner_comparison_proofs}
In this section we develop the ingredients required to prove Proposition \ref{cor:kde_iner_diff_cor}. In section \ref{sec:two_block_solution} we reduce the solution of the INER problem for the two block model to a solution of a non-linear system in two variables, and derive the solution of this system. 
In section \ref{sec:iner_two_block_prop_proof} we use these results to prove Proposition \ref{cor:kde_iner_diff_cor}.


\subsection{Solution Of INER for a 2-Block Model}
\label{sec:two_block_solution}

As discussed in section \ref{sec:two_regions}, any INER solution 
$f$ must be a zero point of the natural gradient, $\grad_f L = 0$. 
Using the expressions given following Lemma \ref{lem:gradients}, this implies $\alpha = \frac{1}{N} (K\alpha)^{-1}$. Since we are only interested in $f$ up to a scalar normalization, we can equivalently assume simply $\alpha = (K\alpha)^{-1}$. 
Further, 
by symmetry consideration we may take $\alpha = (a,\ldots,a, b,\ldots,b)$, where $a$ is in first $N$ coordinates and $b$ is in the next $M$. Then, as mentioned in section \ref{sec:two_regions}, $\alpha = (K\alpha)^{-1}$ is equivalent to $a,b$ solving the following system:
\begin{flalign}
\label{eq:ql_system_definition_1}
\begin{cases}
    a &= a^{-1} \SqBrack{1 + (N-1) \gamma^2} + b^{-1} M \alpha \gamma \gamma' \\
    b &= a^{-1} N \alpha \gamma \gamma'   + b^{-1} \SqBrack{1 + (M-1) \gamma'^2}      
\end{cases}
\end{flalign}

It turns out that it is possible to derive an expression for the ratio of the squares of the solutions to this system in the general case. 

\begin{prop}[Two Variables INER System]
\label{prop:two_variable_system}
Let $a,b$ be solutions of 
\begin{equation}
\label{eq:two_variables_system_general}
    \begin{cases}
        a &= H_{11} a^{-1} + H_{12} b^{-1}  \\
        b &= H_{21} a^{-1} + H_{22} b^{-1} 
    \end{cases}
\end{equation}
Then 
\begin{flalign}
\label{eq:two_variables_prop_result}
    a^2 / b^2 = H_{11}^2
    \Brack{     
    \frac{  -(H_{21} + H_{12})  - \rho
     \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}
    {2 H_{11}H_{22} + H_{12}\SqBrack{ -(H_{21} - H_{12}) + \rho
    \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}  
    }     
 }^2 
\end{flalign}
for a $\rho$ satisfying $\rho \in \Set{+1,-1}$.
\end{prop}

\begin{proof}
Write $u = a^{-1}$, $v=b^{-1}$, and multiply the first and second equations by $u$ and $v$ respectively. Then we have
\begin{equation}
    \begin{cases}
        1 &= H_{11} u^2 + H_{12} uv  \\
        1 &= H_{21} uv + H_{22} v^2. 
    \end{cases}
\end{equation}
We write 
\begin{equation}
\label{eq:v_via_u_expression}
    v = \Brack{1 - H_{11} u^2} / H_{12}u.
\end{equation}
Also, from the first equation, 
\begin{equation}
    H_{12} uv = 1 - H_{11} u^2.  
\end{equation}
Substituting into the second equation, 
\begin{equation}
    1 = \frac{H_{21}}{H_{12}} \Brack{1 - H_{11} u^2} 
    + H_{22} \frac{\Brack{1 - H_{11} u^2}^2}{\Brack{ H_{12}u }^2}.
\end{equation}
Finally setting $s = u^2$ and multiplying by $H_{12}^2 s$,
\begin{equation}
    H_{12}^2 s = H_{21} H_{12} s \Brack{1 - H_{11} s} 
    + H_{22} \Brack{1 - H_{11} s}^2.    
\end{equation}
Collecting terms, we have 
\begin{equation}
    s^2 (H_{11}^2 H_{22} - H_{11}H_{12}H_{21}) + s (H_{12}H_{21} - H_{12}^2 -2 H_{11}H_{22} ) + H_{22} = 0.    
\end{equation}
Solving this, we get 
\begin{equation}
    s = \frac{-(H_{12}H_{21} - H_{12}^2 -2 H_{11}H_{22} ) \pm \sqrt{(H_{12}H_{21} - H_{12}^2 -2 H_{11}H_{22} )^2 - 4(H_{11}^2 H_{22} - H_{11}H_{12}H_{21})H_{22}}}{2(H_{11}^2 H_{22} - H_{11}H_{12}H_{21})}.
\end{equation}
The expression inside the square root satisfies 
\begin{flalign}
    &(H_{12}H_{21} - H_{12}^2 -2 H_{11}H_{22} )^2 - 4(H_{11}^2 H_{22} - H_{11}H_{12}H_{21})H_{22} \\ 
    &= (H_{12}(H_{21} - H_{12}) -2 H_{11}H_{22} )^2 - 4(H_{11}^2 H_{22} - H_{11}H_{12}H_{21})H_{22} \\ 
    &= H_{12}^2(H_{21} - H_{12})^2 -4 H_{11}H_{22}H_{12}(H_{21} - H_{12}) 
    +4H_{11}H_{12}H_{21}H_{22} \\ 
    &= H_{12}^2(H_{21} - H_{12})^2 +4 H_{11}H_{22}H_{12}^2  \\ 
    &= H_{12}^2 \SqBrack{(H_{21} - H_{12})^2 +4H_{11}H_{22} }
\end{flalign}
Thus, simplifying, we have 
\begin{equation}
    u^2 = s =  \frac{-(H_{12}(H_{21} - H_{12}) -2 H_{11}H_{22} ) + \rho
    H_{12} \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}
    {2H_{11}(H_{11} H_{22} - H_{12}H_{21})}, 
\end{equation}
where $\rho \in \Set{+1, -1}$.

Rewriting \eqref{eq:v_via_u_expression} again, we have 
\begin{equation}
\label{eq:vsq_via_usq_expression}
    v^2 = \frac{\Brack{1 - H_{11} u^2}^2}{  H_{12}^2 u^2}.
\end{equation}

Further, 
\begin{flalign}
    a^2 / b^2 &= v^2 / u^2  = \frac{\Brack{1 - H_{11} u^2}^2}{  H_{12}^2 u^4} \\ 
    &= \Brack{ \frac{1 - H_{11} u^2}{  H_{12} u^2} }^2 \\
    &= \Brack{ \frac{1}{  H_{12} u^2} - \frac{H_{11}}{H_{12}} }^2 \\ 
    &= \Brack{\frac{H_{11}}{H_{12}} }^2
    \Brack{     
    \frac{2(H_{11} H_{22} - H_{12}H_{21})}{-(H_{12}(H_{21} - H_{12}) -2 H_{11}H_{22} ) + \rho
    H_{12} \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}     
- 1 }^2   \\ 
    &= \Brack{\frac{H_{11}}{H_{12}} }^2
    \Brack{     
    \frac{2(H_{11} H_{22} - H_{12}H_{21}) +(H_{12}(H_{21} - H_{12}) -2 H_{11}H_{22} ) - \rho
    H_{12} \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}
    {-(H_{12}(H_{21} - H_{12}) -2 H_{11}H_{22} ) + \rho
    H_{12} \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}     
 }^2   \\ 
 &=\Brack{\frac{H_{11}}{H_{12}} }^2
    \Brack{     
    \frac{ - 2H_{12}H_{21} +H_{12}(H_{21} - H_{12})  - \rho
    H_{12} \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}
    {-(H_{12}(H_{21} - H_{12}) -2 H_{11}H_{22} ) + \rho
    H_{12} \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}     
 }^2 \\ 
 &=H_{11}^2
    \Brack{     
    \frac{ - 2H_{21} +H_{21} - H_{12}  - \rho
     \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}
    {-(H_{12}(H_{21} - H_{12}) -2 H_{11}H_{22} ) + \rho
    H_{12} \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}     
 }^2 \\ 
&=H_{11}^2
    \Brack{     
    \frac{  -(H_{21} + H_{12})  - \rho
     \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}
    {-(H_{12}(H_{21} - H_{12}) -2 H_{11}H_{22} ) + \rho
    H_{12} \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}     
 }^2 \\ 
&=H_{11}^2
    \Brack{     
    \frac{  -(H_{21} + H_{12})  - \rho
     \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}
    {2 H_{11}H_{22} + H_{12}\SqBrack{ -(H_{21} - H_{12}) + \rho
    \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}  
    }     
 }^2 
\end{flalign}

\end{proof}


\subsection{Proof Of Proposition \ref{cor:kde_iner_diff_cor}}
\label{sec:iner_two_block_prop_proof}

Similarly to the case with KDE, we will use the following approximation of  the system \eqref{eq:ql_system_definition_1} 
\begin{flalign}
\label{eq:ql_system_approx}
\begin{cases}
    a &= a^{-1} N \gamma^2 + b^{-1} M \alpha \gamma \gamma' \\
    b &= a^{-1} N \alpha \gamma \gamma'   + b^{-1} M \gamma'^2      
\end{cases}
\end{flalign}



\begin{proof}
Let $f$ be the INER solution.
By definition, the ratio $\frac{f(x_t)}{f(x'_s)}$ is given by 
$a^2 / b^2$ where $a,b$ are the solutions to  \eqref{eq:ql_system_approx}. That is, we take 
 $H_{12} = H_{21} =  \alpha \gamma \gamma'$, $H_{11} = \gamma^2$, and $H_{22} = \gamma'^2$ in Proposition \ref{prop:two_variable_system}. Note that we have removed the dependence on $N$, 
 since it does not affect the ratio. 
By Proposition \ref{prop:two_variable_system}, substituting into \eqref{eq:two_variables_prop_result},
\begin{flalign}
    &H_{11}^2
    \Brack{     
    \frac{  -(H_{21} + H_{12})  - \rho
     \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}
    {2 H_{11}H_{22} + H_{12}\SqBrack{ -(H_{21} - H_{12}) + \rho
    \sqrt{(H_{21} - H_{12})^2 +4H_{11}H_{22}}}  
    } 
    }^2 \\  
    &= H_{11}^2
    \Brack{     
    \frac{  -H_{12}  - \rho
     \sqrt{H_{11}H_{22}}}
    { H_{11}H_{22} + H_{12} \rho
    \sqrt{H_{11}H_{22}}
    } 
    }^2 \\  
    &= 
    \gamma^4 
    \Brack{     
    \frac{  -2 \alpha \gamma \gamma'  - 2\rho
     \gamma \gamma'}
    {2 \gamma^2 \gamma'^2 + 2 \alpha \gamma \gamma'  \rho
    \gamma \gamma'  }
    }^2 \\ 
    &= 
    \Brack{\frac{\gamma^2 \gamma \gamma'}{\gamma^2 \gamma'^2}}^2
    \frac{  (\alpha   + \rho)^2  }
    { (1  + \alpha  \rho)^2}
\end{flalign}
It remains to note that 
$\frac{  (\alpha   + \rho)^2  }
    { (1  + \alpha  \rho)^2} = 1$ for any $\alpha$ and $\rho \in \Set{+1,-1}$.
\end{proof}

%\section{Temporary Section}
%\section{Temporary Section}


\section{Invariance of $\mcC$ under Natural Gradient}
\label{sec:invariance_of_cone_under_nat}
Define the non-negative cone of functions $\mcC \subset \mcH$ by
\begin{equation}
    \mcC = \Set{f \in \mcH \setsep f(x) \geq 0 \spaceo \forall x\in \mcX}.
\end{equation}
As discussed in section \ref{sec:basic_framework},  the functional $L(f)$ is convex on $\mcC$. 

We now show that if the kernel $k$ is non-negative, then the cone $\mcC$ is invariant under the natural gradient steps. In particular, this means that if one starts with initialization in $\mcC$ (easy to achieve), then the optimization trajectory stays in $\mcC$, without a need for computationally heavy projection methods. Note that this is unlikely to be true for the standard gradient.  
Recall that the expression \eqref{eq:grad_alpha} for the natural gradient is given in Lemma \ref{lem:gradients}.
\begin{prop}
\label{cor:NgPosCone}
Assume that $k(x,x')\geq 0$ for all $x,x' \in \mcX$ and that  $\lambda < 0.5$. If $f \in \mcC$, then also 
$f' := f - 2 \lambda \SqBrack{ 
       f - \frac{1}{N} \sum_{i=1}^N f^{-1}(x_i) k_{x_i}
    } \in C
$.
\end{prop}
\begin{proof}
Indeed, by opening the brackets, 
\begin{align*}
f' = \left( 1 - 2 \lambda\right)f + 2 \lambda \SqBrack{ \frac{1}{N} \sum_{i=1}^N f^{-1}(x_i) k_{x_i}},   
\end{align*}
which is a non-negative combination of functions in $\mcC$, thus yielding the result.
\end{proof}

\section{Fisher Divergence for Hyper-Parameters Selection}\label{SEC:FD}
The handling of unnormalized models introduces a particular nuance in the context of hyperparameter tuning, as it prevents the use of the maximum likelihood of the data in order to establish the optimal parameter. When confronted with difficulties associated with normalization, it is common to resort to score-based methods. The score function is defined as 
\begin{equation}
    s(x; \tau) = \nabla_{x} \log p_m(x;\tau), 
\end{equation}
where $p_m(x;\tau)$ is a possibly unnormalized probability density on $\RR^d$, evaluated at $x\in \RR^d$, and dependent on the hyperparameter $\tau$. Since the normalization constant is independent of $x$, and $s$ is defined via the gradient in $x$,  $s$ is independent of the normalization.  As a result, distance metrics between distributions that are based on the score function, such as the Fisher Divergence, can be evaluated using non-normalized distributions. 


In this work, we employ this concept, leveraging it to identify the choice of parameters that minimizes divergence between the test set and the density learned on the train set.  Specifically, we apply \emph{score-matching} (\cite{hyvarinen2005estimation}), a particular approach  to measuring the Fisher divergence between a dataset sampled from an unknown distribution and a proposed distribution model. 

Given independent and identically distributed samples ${x_1, \ldots , x_N } \in \RR^D$ from a distribution $p_d(x)$ and an un-normalized density learned, $\tilde{p_m}(x; \tau)$ (where $\tau$ is a parameter). Score matching sets out to reduce the Fisher divergence between $p_d$ and $\tilde{p_m}(\cdot; \tau)$, formally expressed as $$L(\tau) = \frac{1}{2} \cdot E_{p_d}[\lVert s_m(x; \tau)  s_d(x) \rVert ^2]$$
As detailed in \cite{hyvarinen2005estimation}, the technique of integration by parts can derive an expression that does not depend on the unknown latent score function $s_d$: 
 $$L(\tau; {x_1,\ldots,x_n}) = \frac{1}{N} \sum_{i=1}^{N} \left[ tr( \nabla_x s_m(x_i; \tau )) + \frac{1}{2} \cdot \lVert s_m(x_i; \tau )\rVert ^2 \right]+ C $$
In this context, C is a constant independent of $\tau, tr(\cdot) $ denotes the trace of a matrix, and $\nabla_x s_m(x_i; \tau ) = \nabla_{x}^2 log(\tilde{p_m}(x_i; \tau))$ is the Hessian of the learned log-density function evaluated at $x_i$.

Although this method holds promise, it's worth noting the computational burden tied to the calculation of the Hessian trace. To mitigate this, we rely on two techniques. First, we utilize Hutchinsons trace estimator \citep{doi:10.1080/03610918908812806}, a robust estimator that facilitates the estimation of any matrix's trace through a double product with a random vector $\epsilon$:
$$Tr(H) = E_{\epsilon}\left[  \epsilon^T H \epsilon \right].$$
Here $\epsilon$ is any random vector on $\RR^d$ with mean zero and covariance $I$. This expression allows to reduce amount of computation of $Tr(H)$, by computing the products $H\epsilon$ directly, for a few samples of $\epsilon$, without the need to compute the full $H$ itself. A similar strategy has been recently employed in  \cite{grathwohl2018ffjord} in a different context, for a trace computation of a Jacobian of a density transformation, instead of the score itself.   

In more detail, score computations can be performed efficiently and in a 'lazy' manner using automatic differentiation, offered in frameworks such as PyTorch. This allows us to compute a vector-Hessian product $H\epsilon$ per sample without having to calculate the entire Hessian for all samples, a tensor of dimensions $N\times (d\times d)$, in advance. More specifically, we utilize PyTorch's automatic differentiation for computing the score function, which is a matrix of $N\times d$. Subsequently, this is multiplied by $\epsilon$. We then proceed with a straightforward differentiation $\nabla_x s(x_i) = \frac{1}{h} \cdot \left( s \left(x_i + h\cdot \epsilon \right) - s \left(x_i \right) \right)$ for small step $h$, followed by a summation which is lazily calculated through PyTorch (see Algorithm \ref{algo:fastHes}). 

\begin{algorithm}
\caption{Calculating Hutchinson's Trace Estimator}
\label{algo:fastHes}
\begin{algorithmic}[1]
    \REQUIRE Score function $s$, small constant $h$, sample $x$, \# of random vectors $n$
    \STATE Initialize $traceEstimator$ to 0
    \FOR{$i=1$ to $n$}
        \STATE Sample random vector $\epsilon$ from normal distribution
        \STATE Calculate $s(\tau; x + h*\epsilon)$
        \STATE Calculate $(s(\tau; x + h*\epsilon) - s(\tau; x))$
        \STATE Compute $(1/h) \cdot (s(\tau; x + h*\epsilon) - s(\tau; x)) \cdot \epsilon$
        \STATE Add result to $traceEstimator$
    \ENDFOR
    \STATE Return $\frac{traceEstimator}{n}$
\end{algorithmic}
\end{algorithm}


Finally, due to the computational challenges tied to the calculation of the Hessian for diminutive $\tau$ values, we pragmatically targets a robust local minimum (seeking relative minima of order 3) with the maximum $\tau$ value. 
The details of this procedure are elaborately discussed in the Supplementary Material, Section \ref{sec:supp:HP tuning FD}.



\section{Experiments}

\subsection{AUC-ROC Performance Analysis}
\label{sec:supp_aucroc}

In Section \ref{sec:experiments_AD} we presented a comparative study of various methods according to their ranking across different datasets. This section provides an analysis of the raw AUC-ROC values themselves.

We present two figures to elucidate our findings. Figure \ref{fig:supp:boxplot} is similar to the box plot from shown in Figure \ref{fig:ADbench} in the main text but includes raw AUCROC values instead of rankings. As the figure shows, INER also secures the second-highest position in terms of average raw AUC-ROC (second only to IForest), as represented by the order of methods on the X-axis. Furthermore, the median line within the boxes indicates that INER is the method with the highest median score.
Figure \ref{fig:supp:heatmap} shows a heatmap representation of the AUC-ROC values. In this visualization, the size of the circle symbolizes the corresponding AUC value, while the color gradient signifies the deviation in AUC value from INER. The purpose of this heatmap is to offer a graphical interpretation of the AUC-ROC performance levels, demonstrating how they diverge from the performance of INER.

% Figure environment removed


\subsection{Duplicate Anomalies.}
\label{sec:supp_duplicate_anomalies}

Duplicate anomalies are often encountered in in various applications due to factors such as recording errors \cite{8416441}, a circumstance termed as "anomaly masking" \citep{10.1007/s10618-015-0444-8,10.5555/3045390.3045676}, posing significant hurdles for diverse AD algorithms. The significance of this factor is underscored in ADbench \citep{han2022adbench}, where duplicate anomalies are regarded as the most difficult setup for anomaly detection, thereby attracting considerable attention. To replicate this scenario, ADbench duplicates anomalies up to six times within training and test sets, subsequently examining the ensuing shifts in AD algorithms' performance across the 47 datasets discussed in our work. 


As shown in ADBench, unsupervised methods are considerably susceptible to repetitive anomalies. In particular, as shown in Fig. 7a in the main text there, performance degradation is directly proportional to the increase in anomaly duplication. With anomalies duplicated six times, unsupervised methods record a median AUC-ROC reduction of -16.43$\%$, where in INER the drop is less then 2$\%$. This universal performance deterioration can be attributed to two factors: 1) The inherent assumption made in these methods that anomalies are a minority in the dataset, a presumption crucial for detection. The violation of this belief due to the escalation in duplicated anomalies triggers the noticed performance downturn. 2) Methods based on nearest neighbours assume that anomalies significantly deviate from the norm (known as "Point anomalies"). However, anomaly duplication mitigates this deviation, rendering the anomaly less distinguishable.
Notice that while the first factor is less of a problem for a density based AD algorithm ( any time the anomalies are still not the major part of the data), the second factor could harmful to DE based AD algorithms as well.
The evidence of INER possessing the highest median and its robustness to duplicate anomalies, along with a probable resistance to a high number of anomalies, not only emphasizes its superiority as a DE method but also underscores its potential to serve as a state-of-the-art AD method.


\subsection{Hyperparameter Tuning with On-the-fly Fisher-Divergence Minimization}\label{sec:supp:HP tuning FD}
A primary task at hand involves hyperparameter tuning to select the optimal $\tau$ according to the Fisher-Divergence (FD) procedure, as detailed in section \ref{SEC:FD}. 
As discussed in Section \ref{sec:experiments_AD}, we perform hyper parameter tuning by minimizing the fisher divergence to the data.
This procedure requires the identification of a global minimum in the FD between the test set  and the model trained on the training set. 
However, deriving the Hessian for small $\tau$ values proves to be challenging.
Note that small $\tau$ values signify overfitting to the training data, consequently, this leads to a density that is mainly close to zero between samples, thereby making the process highly susceptible to significant errors in numerically calculating the derivatives. This situation results in a Hessian that is fraught with noise. Hence, our strategy focuses on locating a stable local minimum with the highest possible $\tau$. In this context, we define a stable local minimum as a point preceded and succeeded by three points, each greater than the focal point. This definition effectively sets an order of 3 for our search.
For efficient computation, we employ an on-the-fly calculation approach. The process initiates with a $\tau$ value that corresponds to the 'order'-th largest tested point. Subsequently, we explore one larger and one smaller $\tau$ value. If both these points exceed the currently tested point, we continue sampling. Otherwise, we shift the tested point to a smaller value. To avoid redundant calculations, the results are continually stored, ensuring each result is computed only once. This methodology provides a balance between computational efficiency and thorough exploration of the hyperparameter space.


\end{document}


