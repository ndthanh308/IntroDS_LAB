@article{su2022,
    title={{A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language}},
    url = {https://arxiv.org/pdf/2209.05481.pdf},
    author="Su, Bing and Du, Dazhao and Yang, Zhao and Zhou, Youjie and Li, Jiangmeng and Rao, Anyi and Sun, Hao and Lu, Zhiwu and Wen Ji-Rong",
    publisher = {arXiv},
    year={2022},
}

@inproceedings{conneau2020unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
}
@article{weininger1988,
    title={SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
    author={Weininger, David},
    journal={Journal of Chemical Information and Modeling},
    volume={28},
    number={1},
    pages={31--36},
    year={1988},
    publisher={ACS Publications}
}

@article{heller2015,
    title={InChI, the IUPAC International Chemical Identifier},
    author={Heller, Stephen R and McNaught, Alan and Stein, Stephen and Tchekhovskoi, Dmitrii and Pletnev, Igor},
    journal={Journal of Cheminformatics},
    volume={7},
    number={1},
    pages={1--34},
    year={2015},
    publisher={Springer}
}

@article{kearnes2016,
    title={Molecular graph convolutions: moving beyond fingerprints},
    author={Kearnes, Steven and Goldman, Brian and Pande, Vijay},
    journal={Journal of Computer-Aided Molecular Design},
    volume={30},
    number={8},
    pages={595--608},
    year={2016},
    publisher={Springer}
}

@article{duvenaud2015,
    title={Convolutional networks on graphs for learning molecular fingerprints},
    author={Duvenaud, David K and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gómez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alán and Adams, Ryan P},
    journal={Advances in Neural Information Processing Systems},
    volume={28},
    pages={2224--2232},
    year={2015}
}

@article{kipf2016,
    title={Semi-Supervised Classification with Graph Convolutional Networks},
    author={Kipf, Thomas N and Welling, Max},
    journal={arXiv preprint arXiv:1609.02907},
    year={2016}
}

@article{velickovic2017,
    title={Graph Attention Networks},
    author={Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
    journal={arXiv preprint arXiv:1710.10903},
    year={2017}
}

@inproceedings{gilmer2017,
    title={Neural message passing for quantum chemistry},
    author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
    booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
    pages={1263--1272},
    year={2017},
    organization={JMLR. org}
}

@article{wu2018,
    title={MoleculeNet: a benchmark for molecular machine learning},
    author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
    journal={Chemical Science},
    volume={9},
    number={2},
    pages={513--530},
    year={2018},
    publisher={Royal Society of Chemistry}
}

@article{radford2018,
    title={Improving Language Understanding by Generative Pre-Training},
    author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
    journal={arXiv preprint arXiv:1801.06146},
    year={2018}
}

@article{devlin2018,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

@article{raffel2019,
    title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
    journal={arXiv preprint arXiv:1910.10683},
    year={2019}
}

@article{korolev2020,
    title={ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction},
    author={Korolev, Sergey and Tavakoli, Mohammadamin and Lo, Ryan},
    journal={arXiv preprint arXiv:2010.09885},
    year={2020}
}

@article{napolitano2021,
    title={MolBERT: Molecular Representation Learning with BERT},
    author={Napolitano, Flavio and Candelieri, Antonio and Grandi, Mattia},
    journal={arXiv preprint arXiv:2102.01327},
    year={2021}
}

@article{gomez2016,
    title={Automatic chemical design using a data-driven continuous representation of molecules},
    author={Gómez-Bombarelli, Rafael and Duvenaud, David and Hernández-Lobato, José Miguel and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Alán},
    journal={ACS Central Science},
    volume={4},
    number={2},
    pages={268--276},
    year={2016},
    publisher={ACS Publications}
}

@inproceedings{kusner2017,
    title={Grammar Variational Autoencoder},
    author={Kusner, Matt J and Paige, Brooks and Hernández-Lobato, José Miguel},
    booktitle={Proceedings of the 34th International Conference on Machine Learning},
    volume={70},
    pages={1945--1954},
    year={2017},
    organization={PMLR}
}

@article{radford2021,
    title={Learning Transferable Visual Models From Natural Language Supervision},
    author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack},
    journal={arXiv preprint arXiv:2103.00020},
    year={2021}
}

@inproceedings{jia2021,
    title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
    author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V and Hays, James and Perona, Pietro and Shlens, Jonathon and others},
    booktitle={Proceedings of the 2021 Conference on Neural Information Processing Systems},
    year={2021}
}

@misc{OGB,
  doi = {10.48550/ARXIV.2005.00687},
  url = {https://arxiv.org/abs/2005.00687},
  author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  title = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
  year = {2020},
}


@inproceedings{rajpurkar2018know,
    title="Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author="Rajpurkar, Pranav and Jia, Robin and Liang, Percy",
    booktitle="Association for Computational Linguistics (ACL)",
    year="2018",
}


@article{MoleculeSTM,
  doi = {10.48550/ARXIV.2212.10789},
  url = {https://arxiv.org/abs/2212.10789},
  author = {Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Anima},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Quantitative Methods (q-bio.QM), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  title = {{Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing}},
  journal = {NeurIPS 2022 Workshop on AI for Science},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{dmolpub,
  title={Deep Learning for Molecules and Materials},
  journal={Living Journal of Computational Molecular Science},
  author={White, Andrew D},
  url={https://dmol.pub},
  year={2021},
  volume={3},
  number={1},
  pages={1499},
  doi={10.33011/livecoms.3.1.1499}
}

@article{PubChem,
    author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and Zaslavsky, Leonid and Zhang, Jian and Bolton, Evan E},
    title = "{PubChem 2023 update}",
    journal = {Nucleic Acids Research},
    volume = {51},
    number = {D1},
    pages = {D1373-D1380},
    year = {2022},
    month = {10},
    abstract = "{PubChem (https://pubchem.ncbi.nlm.nih.gov) is a popular chemical information resource that serves a wide range of use cases. In the past two years, a number of changes were made to PubChem. Data from more than 120 data sources was added to PubChem. Some major highlights include: the integration of Google Patents data into PubChem, which greatly expanded the coverage of the PubChem Patent data collection; the creation of the Cell Line and Taxonomy data collections, which provide quick and easy access to chemical information for a given cell line and taxon, respectively; and the update of the bioassay data model. In addition, new functionalities were added to the PubChem programmatic access protocols, PUG-REST and PUG-View, including support for target-centric data download for a given protein, gene, pathway, cell line, and taxon and the addition of the ‘standardize’ option to PUG-REST, which returns the standardized form of an input chemical structure. A significant update was also made to PubChemRDF. The present paper provides an overview of these changes.}",
    issn = {0305-1048},
    doi = {10.1093/nar/gkac956},
    url = {https://doi.org/10.1093/nar/gkac956},
    eprint = {https://academic.oup.com/nar/article-pdf/51/D1/D1373/48441598/gkac956.pdf},
}

@misc{WEF,
    author = {{World Economic Forum}},
    title = {Global Risks Report},
    year = {2022},
    url = {https://www.weforum.org/reports/global-risks-report-2022/},
}

@article{ genmods,
url={https://doi.org/10.1002/wcms.1608},
author = {Bilodeau, Camille and Jin, Wengong and Jaakkola, Tommi and Barzilay, Regina and Jensen, Klavs F.},
title = {Generative models for molecular discovery: Recent advances and challenges},
journal = {WIREs Computational Molecular Science},
volume = {12},
number = {5},
pages = {e1608},
keywords = {generative adversarial networks, generative models, molecular representation, normalizing flow models, variational autoencoders},
doi = {https://doi.org/10.1002/wcms.1608},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcms.1608},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wcms.1608},
abstract = {Abstract Development of new products often relies on the discovery of novel molecules. While conventional molecular design involves using human expertise to propose, synthesize, and test new molecules, this process can be cost and time intensive, limiting the number of molecules that can be reasonably tested. Generative modeling provides an alternative approach to molecular discovery by reformulating molecular design as an inverse design problem. Here, we review the recent advances in the state-of-the-art of generative molecular design and discusses the considerations for integrating these models into real molecular discovery campaigns. We first review the model design choices required to develop and train a generative model including common 1D, 2D, and 3D representations of molecules and typical generative modeling neural network architectures. We then describe different problem statements for molecular discovery applications and explore the benchmarks used to evaluate models based on those problem statements. Finally, we discuss the important factors that play a role in integrating generative models into experimental workflows. Our aim is that this review will equip the reader with the information and context necessary to utilize generative modeling within their domain. This article is categorized under: Data Science > Artificial Intelligence/Machine Learning},
year = {2022}
}



@online{infoext,
    author = "Diego Romero",
    title = "{Chat-GPT info extraction from URL - HuggingFace}",
    url  = "https://huggingface.co/spaces/dromerosm/chatgpt-info-extraction",
    keywords = "latex,knuth"
}

@inproceedings{domains,
 author = {Suchin Gururangan and Ana Marasović and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
 title = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
 year = {2020},
 booktitle = {Proceedings of ACL},
}

@misc{chemberta2,
  doi = {10.48550/ARXIV.2209.01712},
  
  url = {https://arxiv.org/abs/2209.01712},
  
  author = {Ahmad, Walid and Simon, Elana and Chithrananda, Seyone and Grand, Gabriel and Ramsundar, Bharath},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Biomolecules (q-bio.BM), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences, I.2.7; I.2.1; J.2; J.3},
  
  title = {ChemBERTa-2: Towards Chemical Foundation Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{lo-etal-2020-s2orc,
    title = "{S}2{ORC}: The Semantic Scholar Open Research Corpus",
    author = "Lo, Kyle  and
      Wang, Lucy Lu  and
      Neumann, Mark  and
      Kinney, Rodney  and
      Weld, Daniel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.447",
    doi = "10.18653/v1/2020.acl-main.447",
    pages = "4969--4983",
    abstract = "We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.",
}



@article{guo2021gmd,
  title={GMD: A graph-based molecular dataset for machine learning},
  author={Guo, Qiang and Huang, Lijie and Zhang, Jie and Xu, Zhiwei and Wang, Lu},
  journal={Scientific Data},
  volume={8},
  number={1},
  pages={1--10},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{wu2018moleculenet,
  title={MoleculeNet: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Kunal and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}
@article{ghorbani2019moflow,
  title={MoFlow: Deep Learning for Gaussian Process Flows},
  author={Ghorbani, Amir and Kheradpisheh, Saeid Reza and Ganjtabesh, Mohammad and Rodrigues, Marcos R and Sengupta, Soham and Cheung, Ngai-Man and Bagheri, Mohammad and Wong, Alexander},
  journal={arXiv preprint arXiv:1910.01702},
  year={2019}
}


@inproceedings{devlin2018bert,
  title={{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{lin2021kvp,
  title={{KVP-LM: Knowledge-Guided Pretraining for Language Modeling}},
  author={Lin, Bill Yuchen and Lin, Hongyu and Chen, Yubo and Liu, Zhiyuan and Sun, Maosong and Han, Xu},
  journal={arXiv preprint arXiv:2105.09054},
  year={2021}
}

@article{SMILES,
  added-at = {2011-12-21T01:05:11.000+0100},
  author = {Weininger, David},
  biburl = {https://www.bibsonomy.org/bibtex/228aa27d2f95c7abc5f2ee8108bce0cee/fairybasslet},
  interhash = {9355c8b62f703f5a56486af179ea6dfc},
  intrahash = {28aa27d2f95c7abc5f2ee8108bce0cee},
  journal = {J. Chem. Inf. Comput. Sci.},
  keywords = {SMILES},
  number = 1,
  pages = {31-36},
  publisher = {American Chemical Society},
  timestamp = {2019-03-11T21:06:37.000+0100},
  title = {{SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules}},
  volume = 28,
  year = 1988
}

@misc{cramming,
  doi = {10.48550/ARXIV.2212.14034},
  url = {https://arxiv.org/abs/2212.14034},
  author = {Geiping, Jonas and Goldstein, Tom},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {{Cramming: Training a Language Model on a Single GPU in One Day}},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Papineni2002BLEU,
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  title     = {{BLEU}: a method for automatic evaluation of machine translation},
  booktitle = {Proceedings of the 40th annual meeting of the Association for Computational Linguistics (ACL)},
  year      = {2002},
  pages     = {311--318}
}
@inproceedings{lavie2007meteor,
  title={{The METEOR metric for automatic evaluation of machine translation}},
  author={Lavie, Alon and Agarwal, Abhaya},
  booktitle={Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2007}
}

@inproceedings{lim2018cross,
  title={Cross-modal molecule retrieval with natural language queries},
  author={Lim, Koen and Kim, Sungsoo Ray and Kim, Gunhee},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8010--8020},
  year={2018}
}


@misc{rna,
    author = "Raphael Townshend",
    title = "{Large Language Models Unlock the RNA Universe}",
    howpublished = "Class lecture for CS342: Foundation Models",
    month = February,
    year = 2023,
    note = "Stanford University"
}
% SMILES tutorial 
% https://archive.epa.gov/med/med_archive_03/web/html/smiles.html 

@article{AlphaFold,
  title={{Highly accurate protein structure prediction with AlphaFold}},
  author={John M. Jumper and Richard Evans and Alexander Pritzel and Tim Green and Michael Figurnov and Olaf Ronneberger and Kathryn Tunyasuvunakool and Russ Bates and Augustin Z{\'i}dek and Anna Potapenko and Alex Bridgland and Clemens Meyer and Simon A A Kohl and Andy Ballard and Andrew Cowie and Bernardino Romera-Paredes and Stanislav Nikolov and Rishub Jain and Jonas Adler and Trevor Back and Stig Petersen and David A. Reiman and Ellen Clancy and Michal Zielinski and Martin Steinegger and Michalina Pacholska and Tamas Berghammer and Sebastian Bodenstein and David Silver and Oriol Vinyals and Andrew W. Senior and Koray Kavukcuoglu and Pushmeet Kohli and Demis Hassabis},
  journal={Nature},
  year={2021},
  volume={596},
  pages={583 - 589}
}

@article{nguyen2023climax,
  title={{ClimaX: A foundation model for weather and climate}},
  author={Nguyen, Tung and Brandstetter, Johannes and Kapoor, Ashish and Gupta, Jayesh K and Grover, Aditya},
  journal={arXiv preprint arXiv:2301.10343},
  year={2023}
}

@misc{InfoNCE,
  doi = {10.48550/ARXIV.1807.03748},  
  url = {https://arxiv.org/abs/1807.03748},
  author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  title = {Representation Learning with Contrastive Predictive Coding},
  publisher = {arXiv},
  year = {2018},
}


@inproceedings{s2orc,
    title = "{S}2{ORC}: The Semantic Scholar Open Research Corpus",
    author = "Lo, Kyle  and
      Wang, Lucy Lu  and
      Neumann, Mark  and
      Kinney, Rodney  and
      Weld, Daniel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.447",
    doi = "10.18653/v1/2020.acl-main.447",
    pages = "4969--4983",
    abstract = "We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.",
}

@misc{neuralcoref,
    author = {Thomas Wolf},
    url = {https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30},
    year = {2017},
    title = {State-of-the-art neural coreference resolution for chatbots},
}

@misc{word2vec,
  doi = {10.48550/ARXIV.1301.3781},
  url = {https://arxiv.org/abs/1301.3781},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title = {Efficient Estimation of Word Representations in Vector Space},
  publisher = {arXiv},
  year = {2013},
}

@article{SciBERT,
  doi = {10.48550/ARXIV.1903.10676}, 
  url = {https://arxiv.org/abs/1903.10676},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  title = {{SciBERT: A Pretrained Language Model for Scientific Text}},
  publisher = {arXiv},  
  year = {2019},
}

@misc{TinyBioBERT,
  doi = {10.48550/ARXIV.2209.03182},
  url = {https://arxiv.org/abs/2209.03182},
  author = {Rohanian, Omid and Nouriborji, Mohammadmahdi and Kouchaki, Samaneh and Clifton, David A.},
  title = {On the Effectiveness of Compact Biomedical Transformers},
  publisher = {arXiv},  
  year = {2022},
}

@article{BioBERT,
	doi = {10.1093/bioinformatics/btz682},
	url = {https://doi.org/10.1093%2Fbioinformatics%2Fbtz682},
	year = 2019,
	month = {sep},
	publisher = {Oxford University Press ({OUP})},
	volume = {36},
	number = {4},
	pages = {1234--1240},
	author = {Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang}, 
	editor = {Jonathan Wren}, 
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},  
	journal = {Bioinformatics}
}


@misc{MoleculeSTM,
  doi = {10.48550/ARXIV.2212.10789},
  url = {https://arxiv.org/abs/2212.10789},
  author = {Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Anima},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Quantitative Methods (q-bio.QM), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  title = {Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{dmolpub,
  title={Deep Learning for Molecules and Materials},
  journal={Living Journal of Computational Molecular Science},
  author={White, Andrew D},
  url={https://dmol.pub},
  year={2021},
  volume={3},
  number={1},
  pages={1499},
  doi={10.33011/livecoms.3.1.1499}
}


@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}


@article{su2022molecular,
  title={A molecular multimodal foundation model associating molecule graphs with natural language},
  author={Su, Bing and Du, Dazhao and Yang, Zhao and Zhou, Yujie and Li, Jiangmeng and Rao, Anyi and Sun, Hao and Lu, Zhiwu and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2209.05481},
  year={2022}
}

@article{liu2022multi,
  title={Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing},
  author={Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2212.10789},
  year={2022}
}

@inproceedings{zang2020moflow,
  title={MoFlow: an invertible flow model for generating molecular graphs},
  author={Zang, Chengxi and Wang, Fei},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={617--626},
  year={2020}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{dinh2014nice,
  title={Nice: Non-linear independent components estimation},
  author={Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1410.8516},
  year={2014}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10684--10695},
  year={2022}
}

@article{you2020graph,
  title={Graph contrastive learning with augmentations},
  author={You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={5812--5823},
  year={2020}
}

@article{wu2018moleculenet,
  title={MoleculeNet: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@article{hu2019strategies,
  title={Strategies for pre-training graph neural networks},
  author={Hu, Weihua and Liu, Bowen and Gomes, Joseph and Zitnik, Marinka and Liang, Percy and Pande, Vijay and Leskovec, Jure},
  journal={arXiv preprint arXiv:1905.12265},
  year={2019}
}

@article{xu2018powerful,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1810.00826},
  year={2018}
}

@article{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}

@article{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{lo2019s2orc,
  title={S2ORC: The semantic scholar open research corpus},
  author={Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Dan S},
  journal={arXiv preprint arXiv:1911.02782},
  year={2019}
}

@misc{liang2021foundation,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}
}

@article{beltagy2019scibert,
title={SciBERT: A Pretrained Language Model for Scientific Text},
author={Iz Beltagy and Kyle Lo and Arman Cohan},
journal={arXiv preprint arXiv:1903.10676},
year={2019}
}


@Article{moleculenet,
author ="Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay",
title  ="MoleculeNet: a benchmark for molecular machine learning",
journal  ="Chem. Sci.",
year  ="2018",
volume  ="9",
issue  ="2",
pages  ="513-530",
publisher  ="The Royal Society of Chemistry",
doi  ="10.1039/C7SC02664A",
url  ="http://dx.doi.org/10.1039/C7SC02664A"}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{MoFlow,
	doi = {10.1145/3394486.3403104},
	year = 2020,
	month = {aug},
	publisher = {{ACM}},
	author = {Chengxi Zang and Fei Wang},
	title = {{MoFlow}: An Invertible Flow Model for Generating Molecular Graphs},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} International Conference on Knowledge Discovery {\&} Data Mining}
}

@misc{yousefzadegan2022evaluation,
  title={Evaluation of generative machine learning models: Judging the quality of generated data with the use of neural networks},
  author={Yousefzadegan Hedin, Sam},
  year={2022}
}

@article{kappa,
author = {Jacob Cohen},
title ={A Coefficient of Agreement for Nominal Scales},
journal = {Educational and Psychological Measurement},
volume = {20},
number = {1},
pages = {37-46},
year = {1960},
doi = {10.1177/001316446002000104},

URL = { 
        https://doi.org/10.1177/001316446002000104
    
},
eprint = { 
        https://doi.org/10.1177/001316446002000104
    
}

}

@misc{hewitt2022truncation,
      title={Truncation Sampling as Language Model Desmoothing}, 
      author={John Hewitt and Christopher D. Manning and Percy Liang},
      year={2022},
      eprint={2210.15191},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{White2023,
   author = {White, Andrew D.},
   title = {The future of chemistry is language},
   journal = {Nature Reviews Chemistry},
   year = {2023},
   note = {ISSN: 2397-3358},
   abstract = {Large language models such as GPT-4 have been approaching human-level ability across many expert domains. GPT-4 can accomplish complex tasks in chemistry purely from English instructions, which may transform the future of chemistry.},
   doi = {10.1038/s41570-023-00502-0},
   url = {https://doi.org/10.1038/s41570-023-00502-0}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@article{wang2022molecular,
  title={Molecular contrastive learning of representations via graph neural networks},
  author={Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Barati Farimani, Amir},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={279--287},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

%moleculenet
@article{bace,
  title={Computational modeling of $\beta$-secretase 1 (BACE-1) inhibitors using ligand based approaches},
  author={Subramanian, Govindan and Ramsundar, Bharath and Pande, Vijay and Denny, Rajiah Aldrin},
  journal={Journal of chemical information and modeling},
  volume={56},
  number={10},
  pages={1936--1949},
  year={2016},
  publisher={ACS Publications}
}
@article{bbbp,
  title={A Bayesian approach to in silico blood-brain barrier penetration modeling},
  author={Martins, Ines Filipa and Teixeira, Ana L and Pinheiro, Luis and Falcao, Andre O},
  journal={Journal of chemical information and modeling},
  volume={52},
  number={6},
  pages={1686--1697},
  year={2012},
  publisher={ACS Publications}
}
@misc{tox21,
  title = {Tox21 Challenge},
  howpublished = {\url{https://tripod.nih.gov/tox21/challenge/}},
  note = {Accessed: 2023-05-19}
}
@article{toxcast,
  title={ToxCast chemical landscape: paving the road to 21st century toxicology},
  author={Richard, Ann M and Judson, Richard S and Houck, Keith A and Grulke, Christopher M and Volarath, Patra and Thillainadarajah, Inthirany and Yang, Chihae and Rathman, James and Martin, Matthew T and Wambaugh, John F and others},
  journal={Chemical research in toxicology},
  volume={29},
  number={8},
  pages={1225--1251},
  year={2016},
  publisher={ACS Publications}
}
@article{sider,
  title={The SIDER database of drugs and side effects},
  author={Kuhn, Michael and Letunic, Ivica and Jensen, Lars Juhl and Bork, Peer},
  journal={Nucleic acids research},
  volume={44},
  number={D1},
  pages={D1075--D1079},
  year={2016},
  publisher={Oxford University Press}
}
@article{clintox,
  title={A data-driven approach to predicting successes and failures of clinical trials},
  author={Gayvert, Kaitlyn M and Madhukar, Neel S and Elemento, Olivier},
  journal={Cell chemical biology},
  volume={23},
  number={10},
  pages={1294--1301},
  year={2016},
  publisher={Elsevier}
}
@article{muv,
  title={Maximum unbiased validation (MUV) data sets for virtual screening based on PubChem bioactivity data},
  author={Rohrer, Sebastian G and Baumann, Knut},
  journal={Journal of chemical information and modeling},
  volume={49},
  number={2},
  pages={169--184},
  year={2009},
  publisher={ACS Publications}
}
