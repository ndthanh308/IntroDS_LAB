{
  "title": "Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training",
  "authors": [
    "Mohammad Majd Saad Al Deen",
    "Maren Pielka",
    "JÃ¶rn Hees",
    "Bouthaina Soulef Abdou",
    "Rafet Sifa"
  ],
  "submission_date": "2023-07-27T07:40:11+00:00",
  "revised_dates": [],
  "abstract": "This paper addresses the classification of Arabic text data in the field of Natural Language Processing (NLP), with a particular focus on Natural Language Inference (NLI) and Contradiction Detection (CD). Arabic is considered a resource-poor language, meaning that there are few data sets available, which leads to limited availability of NLP methods. To overcome this limitation, we create a dedicated data set from publicly available resources. Subsequently, transformer-based machine learning models are being trained and evaluated. We find that a language-specific model (AraBERT) performs competitively with state-of-the-art multilingual approaches, when we apply linguistically informed pre-training methods such as Named Entity Recognition (NER). To our knowledge, this is the first large-scale evaluation for this task in Arabic, as well as the first application of multi-task pre-training in this context.",
  "categories": [
    "cs.CL"
  ],
  "primary_category": "cs.CL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14666",
  "pdf_url": "https://arxiv.org/pdf/2307.14666v1",
  "comment": "submitted to IEEE SSCI 2023",
  "num_versions": null,
  "size_before_bytes": 1116805,
  "size_after_bytes": 315072
}