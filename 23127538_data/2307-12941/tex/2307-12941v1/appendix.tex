\section{Related Work}
% universality + interpretability + dissimilarity, LMC, 
{\bf Convergent learning.} Also referred to as \textit{universality}, convergent learning is the conjecture that different deep learning models learn very similar representations when trained on similar data \cite{DBLP:conf/nips/LiYCLH15, olah2020an}. 
% The conjecture has important consequences for low-level approaches to deep learning interpretability. 
Much of the work in mechanistic interpretability \cite{olah2020an, elhage2021mathematical, olsson2022context, nanda2023progress} has leveraged the universality conjecture to motivate research for toy models, with the hope that the methods and interpretations developed for these more tractable models will scale to larger and more capable models. 
Attempts to test for convergent learning include \textit{representation dissimilarity} comparisons, notably neuron alignment \cite{DBLP:conf/nips/LiYCLH15, li2020representation, godfrey2022on}.
% correlation analysis / centered kernel alignment \cite{morcos2018importance, kornblith2019similarity}.
% However, methods often disagree on layer similarity, and common methods fail basic statistical sanity checks \cite{ding2021grounding}. 
Recently, \cite{chughtai2023toy} examine universality by reverse-engineering a toy transformer model for a group composition task.

{\bf Linear Mode Connectivity:} It has been conjectured in \cite{EntezariSSN22, ainsworth2023git, jordan2023repair} that, once the permutation symmetries of neural networks are taken into consideration, linear interpolations between different parameters $\theta$, of the form $\theta_\alpha=(1-\alpha) \theta_1+\alpha \theta_2$ for $0<\alpha<1$, have constant loss. 

{\bf Privileged Basis:} It is often taken for granted in the interpretability literature that the activation basis is \textit{privileged}, at least in layers with elementwise operations (namely nonlinearities) \cite{erhan2009visualizing, zeiler2014visualizing, zhou2014object,
yosinski2015understanding}. 
% While random projections of layer activations also appear meaningful \cite{szegedy2013intriguing} in such layers, there is evidence that directions closer to the coordinate basis are more semantically aligned with human concepts \cite{Bau2017NetworkDQ, olah2020an}.
On the other hand, while the residual stream of transformer models has no obvious elementwise operation and is thus not an obviously priveleged basis, \cite{dettmers2022llm} provides evidence for outlier basis-aligned features.
To study this phenomenon, \cite{elhage2023basis} gave evidence that transformers can learn rotationally invariant representations in their residual stream using a similar procedure to what we describe in Section \ref{sec:rand-rot}.
We ask the complementary question of whether layers with nonlinearities can learn in an arbitrary basis. 

% \section{Additional Experiments for Section~\ref{sec:rand-rot}}\label{app:rand-rot}


% \begin{itemize}
%     \item More widths.
%     \item NTK/frozen gating experiments.
% \end{itemize}


%\section{Additional Experiments for Section~\ref{sec:sgd-basis}}\label{app:sgd-basis}

\section{Additional Experiments for Section~\ref{sec:towards-unique}}\label{app:towards-unique}

% Figure environment removed

Here, we examine the phenonemon of convergent bases across 1) different model widths and 2) different residual stream structures. One plausible candidate for the converging basis phenomenon displayed in Figure \ref{fig:res-stream-convergence} is that structure of the residual stream. A ResNet-20 is modified so that there is no longer a non-linearity after skip-connections. For the modified ResNet-20, the residual stream is now completely `linear', where all layers exclusively perform linear operations on the residual stream. Figure \ref{fig:res-stream-modified} compares the perm-corr and identity stitching between a normal and modified ResNet-20. The results are largely comparable, suggesting that the convergent basis phenomonenon is not caused by the present of a linear residual stream.

Next, we examine the effect of width. Figure \ref{fig:res-stream-convergence-perm-width} measures the perm-corr for a 4x-width and 8x-width ResNet-20, and Figure \ref{fig:res-stream-convergence-stitching} measures the identity stitching penalty for a 4x-width and 8x-width ResNet-20. We find that width can explain some of the identity stitching success, however there is little difference between the respective networks for perm-corr in Figure \ref{fig:res-stream-convergence-perm-width}.


% Figure environment removed


% Figure environment removed

% Figure environment removed

