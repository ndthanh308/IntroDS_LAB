\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2023)Ainsworth, Hayase, and
  Srinivasa]{ainsworth2023git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=CQsmMYmlP5T}.

\bibitem[Bansal et~al.(2021)Bansal, Nakkiran, and
  Barak]{model_stitching_bansal}
Yamini Bansal, Preetum Nakkiran, and Boaz Barak.
\newblock Revisiting model stitching to compare neural representations.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 225--236. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/01ded4259d101feb739b06c399e9cd9c-Paper.pdf}.

\bibitem[Bau et~al.(2017)Bau, Zhou, Khosla, Oliva, and
  Torralba]{Bau2017NetworkDQ}
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Network dissection: Quantifying interpretability of deep visual
  representations.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 3319--3327, 2017.

\bibitem[Bills et~al.(2023)Bills, Cammarata, Mossing, Tillman, Gao, Goh,
  Sutskever, Leike, Wu, and Saunders]{bills2023language}
Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh,
  Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders.
\newblock Language models can explain neurons in language models.
\newblock
  \url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html},
  2023.

\bibitem[Cammarata et~al.(2020)Cammarata, Goh, Carter, Schubert, Petrov, and
  Olah]{cammarata2020curve}
Nick Cammarata, Gabriel Goh, Shan Carter, Ludwig Schubert, Michael Petrov, and
  Chris Olah.
\newblock Curve detectors.
\newblock \emph{Distill}, 2020.
\newblock \doi{10.23915/distill.00024.003}.
\newblock https://distill.pub/2020/circuits/curve-detectors.

\bibitem[Chughtai et~al.(2023)Chughtai, Chan, and Nanda]{chughtai2023toy}
B.~Chughtai, Lawrence Chan, and Neel Nanda.
\newblock A toy model of universality: Reverse engineering how networks learn
  group operations.
\newblock \emph{ARXIV.ORG}, 2023.
\newblock \doi{10.48550/arXiv.2302.03025}.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llm}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
  Thomas Unterthiner, M.~Dehghani, Matthias Minderer, G.~Heigold, S.~Gelly,
  Jakob Uszkoreit, and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{International Conference On Learning Representations}, 2020.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
N~Elhage, N~Nanda, C~Olsson, T~Henighan, N~Joseph, B~Mann, A~Askell, Y~Bai,
  A~Chen, T~Conerly, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schiefer, Henighan, Kravec,
  Hatfield-Dodds, Lasenby, Drain, Chen, Grosse, McCandlish, Kaplan, Amodei,
  Wattenberg, and Olah]{elhage2022superposition}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan,
  Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen,
  Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg,
  and Christopher Olah.
\newblock Toy models of superposition.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock URL \url{https://transformer-circuits.pub/2022/toy_model/index.html}.

\bibitem[Elhage et~al.(2023)Elhage, Lasenby, and Olah]{elhage2023basis}
Nelson Elhage, Robert Lasenby, and Christopher Olah.
\newblock Privileged bases in the transformer residual stream.
\newblock \emph{Transformer Circuits Thread}, 2023.
\newblock URL
  \url{https://transformer-circuits.pub/2023/privileged-basis/index.html}.

\bibitem[Entezari et~al.(2022)Entezari, Sedghi, Saukh, and
  Neyshabur]{EntezariSSN22}
Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur.
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=dNigytemkL}.

\bibitem[Erhan et~al.(2009)Erhan, Bengio, Courville, and
  Vincent]{erhan2009visualizing}
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Visualizing higher-layer features of a deep network.
\newblock \emph{University of Montreal}, 1341\penalty0 (3):\penalty0 1, 2009.

\bibitem[Godfrey et~al.(2022)Godfrey, Brown, Emerson, and
  Kvinge]{godfrey2022on}
Charles Godfrey, Davis Brown, Tegan Emerson, and Henry Kvinge.
\newblock On the symmetries of deep learning models and their internal
  representations.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=8qugS9JqAxD}.

\bibitem[Jordan et~al.(2023)Jordan, Sedghi, Saukh, Entezari, and
  Neyshabur]{jordan2023repair}
Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur.
\newblock {REPAIR}: {RE}normalizing permuted activations for interpolation
  repair.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=gU5sJ6ZggcX}.

\bibitem[Konečný et~al.(2016)Konečný, McMahan, Yu, Richtárik, Suresh, and
  Bacon]{konečný2016federated}
Jakub Konečný, H.~Brendan McMahan, Felix~X. Yu, Peter Richtárik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv: 1610.05492}, 2016.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and
  Hinton]{kornblith2019similarity}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{International Conference on Machine Learning}, pages
  3519--3529. PMLR, 2019.

\bibitem[Lenc and Vedaldi(2014)]{lenc2014understanding}
Karel Lenc and A.~Vedaldi.
\newblock Understanding image representations by measuring their equivariance
  and equivalence.
\newblock \emph{Computer Vision And Pattern Recognition}, 2014.
\newblock \doi{10.1007/s11263-018-1098-y}.

\bibitem[Li et~al.(2020)Li, Grandvalet, Flamary, Courty, and
  Dou]{li2020representation}
Xuhong Li, Yves Grandvalet, R{\'e}mi Flamary, Nicolas Courty, and Dejing Dou.
\newblock Representation transfer by optimal transport.
\newblock \emph{arXiv preprint arXiv:2007.06737}, 2020.

\bibitem[Li et~al.(2015)Li, Yosinski, Clune, Lipson, and
  Hopcroft]{DBLP:conf/nips/LiYCLH15}
Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John~E. Hopcroft.
\newblock Convergent learning: Do different neural networks learn the same
  representations?
\newblock In \emph{Proceedings of the 1st Workshop on Feature Extraction:
  Modern Questions and Challenges, {FE} 2015, co-located with the 29th Annual
  Conference on Neural Information Processing Systems {(NIPS} 2015), Montreal,
  Canada, December 11-12, 2015}, volume~44 of \emph{{JMLR} Workshop and
  Conference Proceedings}, pages 196--212. JMLR.org, 2015.
\newblock URL \url{http://proceedings.mlr.press/v44/li15convergent.html}.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
  Xie]{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
  and Saining Xie.
\newblock A convnet for the 2020s.
\newblock \emph{CVPR}, 2022.

\bibitem[Morcos et~al.(2018)Morcos, Barrett, Rabinowitz, and
  Botvinick]{morcos2018importance}
Ari~S. Morcos, David G.~T. Barrett, Neil~C. Rabinowitz, and Matthew Botvinick.
\newblock On the importance of single directions for generalization, 2018.
\newblock URL \url{https://arxiv.org/abs/1803.06959}.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and
  Steinhardt]{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=9XFSbDPmdW}.

\bibitem[Olah et~al.(2020)Olah, Cammarata, Schubert, Goh, Petrov, and
  Carter]{olah2020an}
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and
  Shan Carter.
\newblock An overview of early vision in inceptionv1.
\newblock \emph{Distill}, 2020.
\newblock \doi{10.23915/distill.00024.002}.
\newblock https://distill.pub/2020/circuits/early-vision.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Raffel(2023)]{raffelOS}
Colin Raffel.
\newblock Building machine learning models like open source software.
\newblock \emph{Commun. ACM}, 66\penalty0 (2):\penalty0 38–40, jan 2023.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/3545111}.
\newblock URL \url{https://doi.org/10.1145/3545111}.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs,
  {Gontijo-Lopes}, Morcos, Namkoong, Farhadi, Carmon, Kornblith, and
  Schmidt]{wortsmanModelSoupsAveraging2022}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs,
  Raphael {Gontijo-Lopes}, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair
  Carmon, Simon Kornblith, and Ludwig Schmidt.
\newblock Model soups: Averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time, July 2022.

\bibitem[Yosinski et~al.(2015)Yosinski, Clune, Nguyen, Fuchs, and
  Lipson]{yosinski2015understanding}
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson.
\newblock Understanding neural networks through deep visualization.
\newblock In \emph{Deep Learning Workshop, International Conference on Machine
  Learning (ICML)}, 2015.

\bibitem[Zeiler and Fergus(2014)]{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{European conference on computer vision}, pages 818--833.
  Springer, 2014.

\bibitem[Zhou et~al.(2015)Zhou, Khosla, Àgata Lapedriza, Oliva, and
  Torralba]{zhou2014object}
Bolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude Oliva, and Antonio Torralba.
\newblock Object detectors emerge in deep scene cnns.
\newblock In \emph{ICLR}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6856}.

\end{thebibliography}
