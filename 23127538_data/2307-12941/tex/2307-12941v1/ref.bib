@inproceedings{model_stitching_bansal,
 author = {Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {225--236},
 publisher = {Curran Associates, Inc.},
 title = {Revisiting Model Stitching to Compare Neural Representations},
 url = {https://proceedings.neurips.cc/paper/2021/file/01ded4259d101feb739b06c399e9cd9c-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{ainsworth2023git,
  title     = {Git Re-Basin: Merging Models modulo Permutation Symmetries},
  author    = {Samuel Ainsworth and Jonathan Hayase and Siddhartha Srinivasa},
  booktitle = {International Conference on Learning Representations},
  year      = {2023},
  url       = {https://openreview.net/forum?id=CQsmMYmlP5T}
}


@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2022/toy_model/index.html}}

@article{olah2020an,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {An Overview of Early Vision in InceptionV1},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/early-vision},
  doi = {10.23915/distill.00024.002}
}

@article{cammarata2020curve,
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
  title = {Curve Detectors},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/curve-detectors},
  doi = {10.23915/distill.00024.003}
}


 @misc{bills2023language,
         title={Language models can explain neurons in language models},
         author={
            Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William
         },
         year={2023},
         howpublished = {\url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}}
      }


@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{chughtai2023toy,
  title     = {A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations},
  author    = {B. Chughtai and Lawrence Chan and Neel Nanda},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2302.03025},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/f7b87b580bb945e9818ab751fb193441de6e4e47}
}

@inproceedings{DBLP:conf/nips/LiYCLH15,
  author    = {Yixuan Li and Jason Yosinski and Jeff Clune and Hod Lipson and John E. Hopcroft},
  title     = {Convergent Learning: Do different neural networks learn the same representations?},
  booktitle = {Proceedings of the 1st Workshop on Feature Extraction: Modern Questions and Challenges, {FE} 2015, co-located with the 29th Annual Conference on Neural Information Processing Systems {(NIPS} 2015), Montreal, Canada, December 11-12, 2015},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {44},
  pages     = {196-212},
  publisher = {JMLR.org},
  year      = {2015},
  url       = {http://proceedings.mlr.press/v44/li15convergent.html},
  timestamp = {Wed, 29 May 2019 08:41:44 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/LiYCLH15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, N and Nanda, N and Olsson, C and Henighan, T and Joseph, N and Mann, B and Askell, A and Bai, Y and Chen, A and Conerly, T and others},
  journal={Transformer Circuits Thread},
  year={2021}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@inproceedings{
nanda2023progress,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=9XFSbDPmdW}
}

@inproceedings{
wang2023interpretability,
title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NpsVSN6o4ul}
}

@article{li2020representation,
  title={Representation transfer by optimal transport},
  author={Li, Xuhong and Grandvalet, Yves and Flamary, R{\'e}mi and Courty, Nicolas and Dou, Dejing},
  journal={arXiv preprint arXiv:2007.06737},
  year={2020}
}

@inproceedings{
godfrey2022on,
title={On the Symmetries of Deep Learning Models and their Internal Representations},
author={Charles Godfrey and Davis Brown and Tegan Emerson and Henry Kvinge},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=8qugS9JqAxD}
}

@misc{morcos2018importance,
  doi = {10.48550/ARXIV.1803.06959},
  url = {https://arxiv.org/abs/1803.06959},
  author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {On the importance of single directions for generalization},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

@article{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12116--12128},
  year={2021}
}

@article{nguyen2020wide,
  title={Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth},
  author={Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
  journal={arXiv preprint arXiv:2010.15327},
  year={2020}
}

@article{tang2020similarity,
  title={Similarity of neural networks with gradients},
  author={Tang, Shuai and Maddox, Wesley J and Dickens, Charlie and Diethe, Tom and Damianou, Andreas},
  journal={arXiv preprint arXiv:2003.11498},
  year={2020}
}

@article{ding2021grounding,
  title={Grounding representation similarity with statistical testing},
  author={Ding, Frances and Denain, Jean-Stanislas and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2108.01661},
  year={2021}
}

@inproceedings{EntezariSSN22,
  author    = {Rahim Entezari and Hanie Sedghi and Olga Saukh and Behnam Neyshabur},
  title     = {The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=dNigytemkL},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/EntezariSSN22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
jordan2023repair,
title={{REPAIR}: {RE}normalizing Permuted Activations for Interpolation Repair},
author={Keller Jordan and Hanie Sedghi and Olga Saukh and Rahim Entezari and Behnam Neyshabur},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=gU5sJ6ZggcX}
}


@article{Bau2017NetworkDQ,
  title={Network Dissection: Quantifying Interpretability of Deep Visual Representations},
  author={David Bau and Bolei Zhou and Aditya Khosla and Aude Oliva and Antonio Torralba},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={3319-3327}
}


@inproceedings{yosinski2015understanding,
Author = {Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod Lipson},
Booktitle = {Deep Learning Workshop, International Conference on Machine Learning (ICML)},
Title = {Understanding Neural Networks Through Deep Visualization},
Year = {2015}}

@inproceedings{zhou2014object,
  author={Bolei Zhou and Aditya Khosla and Àgata Lapedriza and Aude Oliva and Antonio Torralba},
  title={Object Detectors Emerge in Deep Scene CNNs.},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6856},
  booktitle={ICLR},
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{erhan2009visualizing,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={University of Montreal},
  volume={1341},
  number={3},
  pages={1},
  year={2009}
}

@article{szegedy2013intriguing,
  title     = {Intriguing properties of neural networks},
  author    = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and D. Erhan and Ian J. Goodfellow and R. Fergus},
  journal   = {International Conference On Learning Representations},
  year      = {2013},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad}
}

@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{elhage2023basis,
   title={Privileged Bases in the Transformer Residual Stream},
   author={Elhage, Nelson and Lasenby, Robert and Olah, Christopher},
   year={2023},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2023/privileged-basis/index.html}}

@article{liu2022convnet,
  title   = {A ConvNet for the 2020s},
  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},
  year    = {2022},
  journal = {CVPR}
}

@article{dosovitskiy2020image,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {A. Dosovitskiy and L. Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and M. Dehghani and Matthias Minderer and G. Heigold and S. Gelly and Jakob Uszkoreit and N. Houlsby},
  journal   = {International Conference On Learning Representations},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a}
}

@article{konečný2016federated,
  title   = {Federated Learning: Strategies for Improving Communication Efficiency},
  author  = {Jakub Konečný and H. Brendan McMahan and Felix X. Yu and Peter Richtárik and Ananda Theertha Suresh and Dave Bacon},
  year    = {2016},
  journal = {arXiv preprint arXiv: 1610.05492}
}

@article{raffelOS,
author = {Raffel, Colin},
title = {Building Machine Learning Models Like Open Source Software},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3545111},
doi = {10.1145/3545111},
abstract = {Proposing a community-based system for model development.},
journal = {Commun. ACM},
month = {jan},
pages = {38–40},
numpages = {3}
}

@article{lenc2014understanding,
  title     = {Understanding image representations by measuring their equivariance and equivalence},
  author    = {Karel Lenc and A. Vedaldi},
  journal   = {Computer Vision And Pattern Recognition},
  year      = {2014},
  doi       = {10.1007/s11263-018-1098-y},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/371400e61632592146f40b621fb3dbb6971721be}
}

@misc{wortsmanModelSoupsAveraging2022,
  title = {Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy without Increasing Inference Time},
  shorttitle = {Model Soups},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and {Gontijo-Lopes}, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  year = {2022},
  month = jul,
  number = {arXiv:2203.05482},
  eprint = {2203.05482},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.05482},
  urldate = {2023-06-23},
  abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results "model soups." When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}
