\documentclass[]{hld2023}
\usepackage{multicol}
\usepackage{hyperref}



\newcommand{\Cov}{\mathrm{Cov}}

\hldauthor{%
\Name{Davis Brown\nametag{\thanks{Equal contribution.}
  \addtocounter{footnote}{-1}\addtocounter{Hfootnote}{-1}}} \Email{davis.brown@pnnl.gov}\\
\addr {Pacific Northwest National Laboratory} 
\AND
\Name{Nikhil Vyas\nametag{\footnotemark}} \Email{nikhil@g.harvard.edu}\\
\addr {Harvard University} 
\AND
\Name{Yamini Bansal} \Email{ybansal@google.com}\\
\addr {Google DeepMind} 
}

\title{On Privileged and Convergent Bases in Neural Network Representations}


\begin{document}

\maketitle


\begin{abstract}


In this study, we investigate whether the representations learned by neural networks possess a privileged and convergent basis. Specifically, we examine the significance of feature directions represented by individual neurons. First, we establish that arbitrary rotations of neural representations cannot be inverted (unlike linear networks), indicating that they do not exhibit complete rotational invariance. Subsequently, we explore the possibility of multiple bases achieving identical performance. To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. Our study reveals two findings: (1) Even in wide networks such as WideResNets, neural networks do not converge to a unique basis; (2) Basis correlation increases significantly when a few early layers of the network are frozen identically.

Furthermore, we analyze Linear Mode Connectivity, which has been studied as a measure of basis correlation. Our findings give evidence that while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation.


%In this study, we investigate whether the representations learned by neural networks possess a privileged and/or a unique basis. Specifically, we examine the significance of feature directions represented by individual neurons. First, we explore we explore if neural networks trained with different initializations converge to the same basis and find that they do not. We also explore Linear Mode Connectivity (after aligning the neurons), which has been studied as a measure of basis correlation. Our findings give evidence that while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation. Second, we explore whether neuronal basis is privileged or if the basis can be arbitrarily rotated without affecting performance. We establish that arbitrary rotations of neural representations cannot be inverted (unlike linear networks), indicating that they do not exhibit complete rotational invariance. Subsequently, we explore the possibility of multiple bases achieving identical performance. To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. Our study reveals two findings: (1) Even in wide networks such as WideResNets, neural networks do not converge to a unique basis; (2) Basis correlation increases significantly when a few early layers of the network are frozen identically.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

While neural networks are black-box function approximators that are trained end-to-end to optimize a loss objective, their emergent internal layer-wise representations are important objects for both \emph{understanding deep learning} and \emph{direct downstream use}. Internal representations of neural networks can be useful tools for interpretability \cite{olah2020an, cammarata2020curve, bills2023language}, teaching us how neural networks perform the computations they do, as well as for understanding the implicit biases of gradient-based neural network training \cite{ainsworth2023git, model_stitching_bansal}. Moreover, representations are often directly used for downstream tasks that the network was not originally trained for, like in transfer learning or representation learning. Thus, we would like to develop a better understanding of the mathematical properties of neural network representations.

One such property is whether neural networks representations have a \emph{privileged basis} \cite{elhage2022superposition}. That is, are the features represented by each individual neuron significant, or is information stored only at a population level in neurons? This question is important, for instance, in interpretability, where attempts have been made to interpret features represented by individual neurons (such as edge or curve detectors in convolutional networks \cite{cammarata2020curve}). This question is also closely related to that of \emph{invariances} exhibited by neural representations --- what are the set of transformations that can be applied to representations that keep the final network accuracy unchanged? In particular, if the representations are rotation invariant, then an individual neuron does not carry significant information.


% An important set of properties of representations that we would like to understand are the invariances exhibited by them. That is, what are the set of transformations of the internal representations that keep the final function computed by the network unchanged? This question is important, for instance, in interpretability --- if representations exhibit rotation invariance, then a particular neuron may not be interpretable by itself. Additionally, we would like to understand if the representations of networks learnt by two different runs of SGD similar to each other

To understand this further, consider the simple case of a two-layer neural network without any non-linear activation functions. That is, the function output of the network is $f(x) = W_2W_1x$ with weights $W_2, W_1$ and inputs $x$. Here, the first layer representations are $W_1x$. This representation exhibits rotation invariance - we can rotate the first layer representations by an arbitrary orthonormal matrix $O$ (giving us rotated first layer representations $OW_1x$) but the subsequent layer can invert this rotation and recover the original function $f(x) = W_2O^{-1}OW_1x$. Thus, an individual neuron could represent any arbitrary feature for the same functional outputs in the network.

%in non-linear networks, work in interpretability has shown that single neurons with semantic meaning seem to emerge reliably. But here we want to verify this more rigorously. Also we dont know if it happens by chance, or is it actually important for the neural network. 

% Neural networks are black box function approximators, but nevertheless they seem to have internal structure. 
% Neural networks are composed of linear functions followed by non-linearities - so we might think that they have linear invariance. That is, the relevant information is stored in the directions or the distances between the embeddings, but the \emph{basis} in which this information is represented is not necessarily important. More importantly, that this basis need not have any correspondence with the neurons of the network. There have been a variety of works that argue this \cite{}(disentanglement ICML paper), (Sanjeev paper). Indeed, consider a linear network where you rotate each representation with a matrix A and apply the inverse in the next layer - such a network would preserve the output function, but also rotate the basis of the intermediate representation. However, neural networks in practice contain non-linearities - so it is not apriori obvious if such a transformation keeps the function output unchanged. Work from interpretability also seems to suggest that the neurons correspond to semantic or meaningful directions. 

% In contrast, recent work \cite{ainsworth2023git, jordan2023repair} suggests that neural networks trained with SGD with different randomness, converge to the same internal representations up to permutations at large widths. Similarly, literature in interpretability of non-linear neural networks suggests that individual neurons repeatedly converge to significant (and semantically meaningful) features (such as curve detectors in \cite{olah2020an}). 


{\bf Our Contributions:} In our work, we start by showing that the perceived permutation invariance of representations at high width is actually a result of a of kind of noise-averaging --- the correlation between the activities of neurons after accounting for permutations remains \textit{nearly} constant, as we scale the width (Section \ref{sec:sgd-basis}). This shows that while metrics like linear mode connectivity may suggest permutation invariance, the effect disappears when examined at a neuron level. Since this casts some doubt on the presence of a privileged basis, in Section \ref{sec:rand-rot} we ask if \emph{any} basis of neural representations is likely to work equally well. To do so, we consider a random rotation of a layer, and ask if it can be inverted by the later layers with training. We find that this is not the case. Thus, combined these results suggest that while the basis of neural representations matter, there is no one unique basis that is required to achieve the same functional accuracy. Finally, in Section \ref{sec:towards-unique}, we ask what kinds of constraints can be imposed on the network to obtain a unique neural basis consistent across different training runs. 

%However, it is unknown if having such a privileged basis is unique or if it is mathematically necessary. In this work, we study this question. \emph{Note that we do not make any claims about whether the the privileged basis vectors are semantically meaningful, but only in the allowed mathematical structures of these representations.} %We are interested in understanding the mathematical structure of the representations.



% \subsection{Our Contributions}

% \emph{Does SGD learn networks with related basis?} First, we highlight the benefits of using permutation-stitching \cite{} and permutation-CKA tool to probe this question. Using this, we show the following:
% \begin{itemize}
%     \item We verify that as the network width gets larger, different networks converge to the same basis. This has been suggested by prior work \cite{}
%     \item Smaller width network neurons behave like a sample from a larger width network
%     \item For certain architectures with a residual stream, freezing the first few layers allows the rest of the network to have the \emph{same} basis. 
% \end{itemize}


% \emph{Can any basis work?} We start by studying the rotation invariance of representations, and show that arbitrary rotations of the representations cannot be inverted when non-linearities are present. (Section \ref{})  

% This shows that the directions represented by individual neurons are indeed significant. However, this does not tell us if there is only one such possible basis, or if there can be multiple possible ones. To study this, we turn our attention to the representations learnt by practical networks and ask how their basis are related to each other.

% % Basis is important, but is there only basis there is useful or are there multiple basis that are good. But that is hard, so we restrict the scope to networks trained in the same way.

% Prior work has considered this question in toy settings \cite{} (superposition, NLP word embeddings), or by using linear mode connectivity as a tool to investigate the invariances. In this work, we use permutation-stitching as our main tool of choice (See Section \ref{} for differences with LMC)


\section{Related Work}
% universality + interpretability + dissimilarity, LMC, 
{\bf Convergent learning.} Also referred to as \textit{universality}, convergent learning is the conjecture that different deep learning models learn very similar representations when trained on similar data \cite{DBLP:conf/nips/LiYCLH15, olah2020an}. 
% The conjecture has important consequences for low-level approaches to deep learning interpretability. 
Much of the work in mechanistic interpretability \cite{olah2020an, elhage2021mathematical, olsson2022context, nanda2023progress} has leveraged the universality conjecture to motivate research for toy models, with the hope that the methods and interpretations developed for these more tractable models will scale to larger and more capable models. 
Recently, \cite{chughtai2023toy} examined universality by reverse-engineering a toy transformer model for a group composition task.
Attempts to test for convergent learning include \textit{representation dissimilarity} comparisons, notably neuron alignment \cite{DBLP:conf/nips/LiYCLH15, li2020representation, godfrey2022on} and correlation analysis / centered kernel alignment \cite{morcos2018importance, kornblith2019similarity}.

Model stitching \cite{lenc2014understanding, model_stitching_bansal} extracts features from the early layers of model $f$ and inserts them into the later layers of model $g$ (usually via a learned, low-capacity connecting layer \( \varphi \)). If the representations between these models can be combined such that the resulting `stitched' model, \(g_{>l}\circ \varphi \circ f_{\leq l} \), achieves a low loss on a downstream task, the models are called `stitching connected' for the layer $l$ for that task.

{\bf Linear Mode Connectivity:} It has been conjectured in \cite{EntezariSSN22, ainsworth2023git, jordan2023repair} that, for different models learned by SGD with equal loss, once the permutation symmetries of neural networks are taken into consideration, linear interpolations between them of the form $\theta_\alpha=(1-\alpha) \theta_1+\alpha \theta_2$ for $0<\alpha<1$ have at least constant loss. 

{\bf Privileged Basis:} It is often taken for granted in the interpretability literature that the activation basis is \textit{privileged}, at least in layers with elementwise operations (namely nonlinearities) \cite{erhan2009visualizing, zeiler2014visualizing, zhou2014object,
yosinski2015understanding, Bau2017NetworkDQ}. 
% While random projections of layer activations also appear meaningful \cite{szegedy2013intriguing} in such layers, there is evidence that directions closer to the coordinate basis are more semantically aligned with human concepts \cite{Bau2017NetworkDQ, olah2020an}.
On the other hand, while the residual stream of transformer models has no obvious elementwise operation and is thus not an obviously priveleged basis, \cite{dettmers2022llm} provides evidence for outlier basis-aligned features.
To study this phenomenon, \cite{elhage2023basis} demonstrate that transformers can learn rotationally invariant representations in their residual stream using a similar procedure to what we describe in Section \ref{sec:rand-rot}.
We ask the complementary question of whether layers with nonlinearities can learn in an arbitrary basis. 




% % Figure environment removed

% % Figure environment removed


% % Figure environment removed


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}