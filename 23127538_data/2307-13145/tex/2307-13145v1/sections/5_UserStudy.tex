\section{User Study}
\label{sec:userstudy}

\noindent
\textbf{Participants and Ethical Considerations.} We deployed HarrySpotter in a 2-week study with 29 users (13 female), aged between 18-49 years (median = 34). To be eligible for the study, participants were required to own an Android or iOS smartphone and be located in London, UK. In compliance with GDPR and the Data Protection Act, all individual user data were anonymized to ensure the privacy and confidentiality of the participants. The study was approved by the Ethics Committee of Goldsmiths, University of London.

\noindent \textbf{Procedure.} All participants underwent a pre-screening process where we collected demographic information and obtained the unique identifier of their device for generating the app download link. After installing the app, participants were prompted to grant access to the camera and location. Basic instructions were provided on how to use the app, such as annotating objects, with no specific guidelines on what to annotate or how frequently. To maintain study integrity, no information regarding the relationship between personality and engagement techniques was revealed to the participants.

\noindent \textbf{Materials and Apparatus.} At the end of the study, we administered a 6-item questionnaire (Table~\ref{tab:questions}) and the 10-item TIPI personality questionnaire~\cite{gosling2003very}. The 6-item questionnaire included statements derived from~\cite{lindqvist2011m} and had previously been validated in the context of the Foursquare app to assess users' motivations for engagement. Participants rated both questionnaires using a 7-point Likert scale (1: Strongly Disagree; 7: Strongly Agree).

\noindent
\textbf{Self-reports and Big-Five personality traits.} We coded the Likert-scale answers to the 6-item questionnaire and the TIPI~\cite{gosling2003very}. On average, our participants scored as follows on a 1-7 scale: average in Openness ($\mu\textrm{=5.14}$, $\sigma\textrm{=0.8}$), high in Conscientiousness ($\mu\textrm{=5.12}$, $\sigma\textrm{=0.85}$), average in Neuroticism ($\mu\textrm{=4.59}$, $\sigma\textrm{=0.85}$), average in Agreeableness ($\mu\textrm{=4.5}$, $\sigma\textrm{=0.71}$), and low in Extraversion ($\mu\textrm{=4.43}$, $\sigma\textrm{=0.94}$). These trait distributions aligned with the normative personality values derived from a large sample of the U.S. population~\cite{soto2011age}.

\noindent\textbf{Annotations.} Each annotation in our study involved storing the raw image and its corresponding label in a database. To ensure data quality, we implemented checks for image duplication and semantic correctness. To prevent duplication, we utilized FAISS~\cite{johnson2019billion}, a framework for indexing images based on visual similarity. This allowed us to retrieve the most visually similar images for comparison. We penalized scores for each annotation based on visual similarity to the user's previously uploaded images. For instance, if an image closely resembled a previously captured one, the user would not receive a reward in the form of spell energy. To assess semantic correctness, we first subjected the uploaded image to an off-the-shelf object detector~\cite{cheng2019panoptic}. We then calculated the WordNet semantic distance~\cite{miller1998wordnet} between the detected label and the user-generated label. The awarded spell energy was proportional to the semantic similarity, discouraging grossly inaccurate or garbled labels. Regarding annotation quantity, we recorded the total number of annotations $n_k$ uploaded by each user $k$ along with their respective images. For annotation quality, three independent annotators rated each annotation on a 1-5 Likert scale, with 5 indicating a perfect match between the image and the user-generated label. For example, if an image depicted a ``computer mouse'' and the user's label was ``mouse,'' the annotator would assign a score of 5. To ensure reliable results, we calculated a Fleiss kappa score of 0.57, indicating moderate to good agreement among the three annotators. We compiled a set of $n$ images $I$ annotated by each user $k$ as ${I_1, I_2, ..., I_n}$. The quality score for user $k$ was determined by the median of the quality scores assigned to their $n$ annotated images.

Before using the six self-reports and the quantity and quality of annotation metrics into our regression models, we conducted a Shapiro-Wilk test for normality. As the eight variables exhibited skewed distributions, we applied a log transformation to them. Among the five personality traits, only Extraversion showed a slight skewness, so we also applied a log transformation to it.


