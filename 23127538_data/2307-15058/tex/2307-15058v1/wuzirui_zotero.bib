
@misc{kerr_lerf_2023,
	title = {{LERF}: {Language} {Embedded} {Radiance} {Fields}},
	shorttitle = {{LERF}},
	url = {http://arxiv.org/abs/2303.09553},
	doi = {10.48550/arXiv.2303.09553},
	abstract = {Humans describe the physical world using natural language to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract associations, or actionable affordances. In this work we propose Language Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf models like CLIP into NeRF, which enable these types of open-ended language queries in 3D. LERF learns a dense, multi-scale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embeddings across training views to provide multi-view consistency and smooth the underlying language field. After optimization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding vision-language models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hierarchically across the volume. The project website can be found at https://lerf.io .},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09553 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No INSPIRE recid found},
}

@inproceedings{wang_f2-nerf_2023,
	title = {F\${\textasciicircum}\{2\}\$-{NeRF}: {Fast} {Neural} {Radiance} {Field} {Training} with {Free} {Camera} {Trajectories}},
	shorttitle = {F\${\textasciicircum}\{2\}\$-{NeRF}},
	url = {https://arxiv.org/abs/2303.15951v1},
	abstract = {This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF) for novel view synthesis, which enables arbitrary input camera trajectories and only costs a few minutes for training. Existing fast grid-based NeRF training frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed for bounded scenes and rely on space warping to handle unbounded scenes. Existing two widely-used space-warping methods are only designed for the forward-facing trajectory or the 360-degree object-centric trajectory but cannot process arbitrary trajectories. In this paper, we delve deep into the mechanism of space warping to handle unbounded scenes. Based on our analysis, we further propose a novel space-warping method called perspective warping, which allows us to handle arbitrary trajectories in the grid-based NeRF framework. Extensive experiments demonstrate that F2-NeRF is able to use the same perspective warping to render high-quality images on two standard datasets and a new free trajectory dataset collected by us. Project page: https://totoro97.github.io/projects/f2-nerf.},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Peng and Liu, Yuan and Chen, Zhaoxi and Liu, Lingjie and Liu, Ziwei and Komura, Taku and Theobalt, Christian and Wang, Wenping},
	month = mar,
	year = {2023},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@article{tian_vibus_2022,
	title = {{VIBUS}: {Data}-efficient {3D} {Scene} {Parsing} with {VIewpoint} {Bottleneck} and {Uncertainty}-{Spectrum} {Modeling}},
	shorttitle = {{VIBUS}},
	url = {http://arxiv.org/abs/2210.11472},
	doi = {10.48550/arXiv.2210.11472},
	abstract = {Recently, 3D scenes parsing with deep learning approaches has been a heating topic. However, current methods with fully-supervised models require manually annotated point-wise supervision which is extremely user-unfriendly and time-consuming to obtain. As such, training 3D scene parsing models with sparse supervision is an intriguing alternative. We term this task as data-efficient 3D scene parsing and propose an effective two-stage framework named VIBUS to resolve it by exploiting the enormous unlabeled points. In the first stage, we perform self-supervised representation learning on unlabeled points with the proposed Viewpoint Bottleneck loss function. The loss function is derived from an information bottleneck objective imposed on scenes under different viewpoints, making the process of representation learning free of degradation and sampling. In the second stage, pseudo labels are harvested from the sparse labels based on uncertainty-spectrum modeling. By combining data-driven uncertainty measures and 3D mesh spectrum measures (derived from normal directions and geodesic distances), a robust local affinity metric is obtained. Finite gamma/beta mixture models are used to decompose category-wise distributions of these measures, leading to automatic selection of thresholds. We evaluate VIBUS on the public benchmark ScanNet and achieve state-of-the-art results on both validation set and online test server. Ablation studies show that both Viewpoint Bottleneck and uncertainty-spectrum modeling bring significant improvements. Codes and models are publicly available at https://github.com/AIR-DISCOVER/VIBUS.},
	urldate = {2023-07-17},
	journal = {Journal of Photogrammetry and Remote Sensing},
	author = {Tian, Beiwen and Luo, Liyi and Zhao, Hao and Zhou, Guyue},
	month = oct,
	year = {2022},
	note = {arXiv:2210.11472 [cs]
abstractTranslation:
abstractTranslation:},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_pq-transformer_2022,
	title = {{PQ}-{Transformer}: {Jointly} {Parsing} {3D} {Objects} and {Layouts} {From} {Point} {Clouds}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {{PQ}-{Transformer}},
	doi = {10.1109/LRA.2022.3143224},
	abstract = {3D scene understanding from point clouds plays a vital role for various robotic applications. Unfortunately, current state-of-the-art methods use separate neural networks for different tasks like object detection or room layout estimation. Such a scheme has two limitations: 1) Storing and running several networks for different tasks are expensive for typical robotic platforms. 2) The intrinsic structure of separate outputs are ignored and potentially violated. To this end, we propose the first transformer architecture that predicts 3D objects and layouts simultaneously, using point cloud inputs. Unlike existing methods that either estimate layout keypoints or edges, we directly parameterize room layout as a set of quads. As such, the proposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the novel quad representation, we propose a tailored physical constraint loss function that discourages object-layout interference. The quantitative and qualitative evaluations on the public benchmark ScanNet show that the proposed PQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a quasi-real-time (8.91 FPS) rate without efficiency-oriented optimization. Moreover, the new physical constraint loss can improve strong baselines, and the F1-score of the room layout is significantly promoted from 37.9\% to 57.9\%.11Code and models can be accessed at https://github.com/OPEN-AIR-SUN/PQ-Transformer.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chen, Xiaoxue and Zhao, Hao and Zhou, Guyue and Zhang, Ya-Qin},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Layout, Object detection, Point cloud compression, Proposals, Semantics, Task analysis, Three-dimensional displays, Transformers, layout, point cloud},
	pages = {2519--2526},
}

@inproceedings{jin_adapt_2023,
	title = {{ADAPT}: {Action}-aware {Driving} {Caption} {Transformer}},
	shorttitle = {{ADAPT}},
	doi = {10.1109/ICRA48891.2023.10160326},
	abstract = {End-to-end autonomous driving has great potential in the transportation industry. However, the lack of transparency and interpretability of the automatic decision-making process hinders its industrial adoption in practice. There have been some early attempts to use attention maps or cost volume for better model explainability which is difficult for ordinary passengers to understand. To bridge the gap, we propose an end-to-end transformer-based architecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides user-friendly natural language narrations and reasoning for each decision making step of autonomous vehicular control and action. ADAPT jointly trains both the driving caption task and the vehicular control prediction task, through a shared video representation. Experiments on BDD-X (Berkeley DeepDrive eXplanation) dataset demonstrate state-of-the-art performance of the ADAPT framework on both automatic metrics and human evaluation. To illustrate the feasibility of the proposed framework in real-world applications, we build a novel deployable system that takes raw car videos as input and outputs the action narrations and reasoning in real time. The code, models and data are available at https://github.com/jxbbb/ADAPT.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Jin, Bu and Liu, Xinyu and Zheng, Yupeng and Li, Pengfei and Zhao, Hao and Zhang, Tong and Zheng, Yuhang and Zhou, Guyue and Liu, Jingjing},
	month = may,
	year = {2023},
	keywords = {Adaptation models, Decision making, Measurement, Streaming media, Training, Transformers, Transportation industry},
	pages = {7554--7561},
}

@inproceedings{li_lode_2023,
	title = {{LODE}: {Locally} {Conditioned} {Eikonal} {Implicit} {Scene} {Completion} from {Sparse} {LiDAR}},
	shorttitle = {{LODE}},
	url = {http://arxiv.org/abs/2302.14052},
	doi = {10.48550/arXiv.2302.14052},
	abstract = {Scene completion refers to obtaining dense scene representation from an incomplete perception of complex 3D scenes. This helps robots detect multi-scale obstacles and analyse object occlusions in scenarios such as autonomous driving. Recent advances show that implicit representation learning can be leveraged for continuous scene completion and achieved through physical constraints like Eikonal equations. However, former Eikonal completion methods only demonstrate results on watertight meshes at a scale of tens of meshes. None of them are successfully done for non-watertight LiDAR point clouds of open large scenes at a scale of thousands of scenes. In this paper, we propose a novel Eikonal formulation that conditions the implicit representation on localized shape priors which function as dense boundary value constraints, and demonstrate it works on SemanticKITTI and SemanticPOSS. It can also be extended to semantic Eikonal scene completion with only small modifications to the network architecture. With extensive quantitative and qualitative results, we demonstrate the benefits and drawbacks of existing Eikonal methods, which naturally leads to the new locally conditioned formulation. Notably, we improve IoU from 31.7\% to 51.2\% on SemanticKITTI and from 40.5\% to 48.7\% on SemanticPOSS. We extensively ablate our methods and demonstrate that the proposed formulation is robust to a wide spectrum of implementation hyper-parameters. Codes and models are publicly available at https://github.com/AIR-DISCOVER/LODE.},
	urldate = {2023-07-17},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {arXiv},
	author = {Li, Pengfei and Zhao, Ruowen and Shi, Yongliang and Zhao, Hao and Yuan, Jirui and Zhou, Guyue and Zhang, Ya-Qin},
	month = feb,
	year = {2023},
	note = {arXiv:2302.14052 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{10160470,
	title = {Unsupervised road anomaly detection with language anchors},
	doi = {10.1109/ICRA48891.2023.10160470},
	booktitle = {2023 {IEEE} international conference on robotics and automation ({ICRA})},
	author = {Tian, Beiwen and Liu, Mingdao and Gao, Huan-ang and Li, Pengfei and Zhao, Hao and Zhou, Guyue},
	year = {2023},
	pages = {7778--7785},
}

@inproceedings{yang_unisim_2023,
	title = {{UniSim}: {A} {Neural} {Closed}-{Loop} {Sensor} {Simulator}},
	shorttitle = {{UniSim}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Yang, Ze and Chen, Yun and Wang, Jingkang and Manivasagam, Sivabalan and Ma, Wei-Chiu and Yang, Anqi Joyce and Urtasun, Raquel},
	year = {2023},
	pages = {1389--1399},
}

@misc{xiangli_assetfield_2023,
	title = {{AssetField}: {Assets} {Mining} and {Reconfiguration} in {Ground} {Feature} {Plane} {Representation}},
	shorttitle = {{AssetField}},
	url = {http://arxiv.org/abs/2303.13953},
	doi = {10.48550/arXiv.2303.13953},
	abstract = {Both indoor and outdoor environments are inherently structured and repetitive. Traditional modeling pipelines keep an asset library storing unique object templates, which is both versatile and memory efficient in practice. Inspired by this observation, we propose AssetField, a novel neural scene representation that learns a set of object-aware ground feature planes to represent the scene, where an asset library storing template feature patches can be constructed in an unsupervised manner. Unlike existing methods which require object masks to query spatial points for object editing, our ground feature plane representation offers a natural visualization of the scene in the bird-eye view, allowing a variety of operations (e.g. translation, duplication, deformation) on objects to configure a new scene. With the template feature patches, group editing is enabled for scenes with many recurring items to avoid repetitive work on object individuals. We show that AssetField not only achieves competitive performance for novel-view synthesis but also generates realistic renderings for new scene configurations.},
	urldate = {2023-07-09},
	publisher = {arXiv},
	author = {Xiangli, Yuanbo and Xu, Linning and Pan, Xingang and Zhao, Nanxuan and Dai, Bo and Lin, Dahua},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13953 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@book{gao__nodate,
	title = {视觉 {SLAM} 十四讲：从理论到实践},
	author = {Gao, Xiang},
}

@inproceedings{xie_s-nerf_2023,
	title = {S-{NeRF}: {Neural} {Radiance} {Fields} for {Street} {Views}},
	shorttitle = {S-{NeRF}},
	url = {https://openreview.net/forum?id=gx2yJS-ENqI},
	abstract = {Neural Radiance Fields (NeRFs) aim to synthesize novel views of objects and scenes, given the object-centric camera views with large overlaps. However, we conjugate that this paradigm does not fit the nature of the street views that are collected by many self-driving cars from the large-scale unbounded scenes. Also, the onboard cameras perceive scenes without much overlapping. Thus, existing NeRFs often produce blurs, "floaters" and other artifacts on street-view synthesis. In this paper, we propose a new street-view NeRF (S-NeRF) that considers novel view synthesis of both the large-scale background scenes and the foreground moving vehicles jointly. Specifically, we improve the scene parameterization function and the camera poses for learning better neural representations from street views. We also use the the noisy and sparse LiDAR points to boost the training and learn a robust geometry and reprojection based confidence to address the depth outliers. Moreover, we extend our S-NeRF for reconstructing moving vehicles that is impracticable for conventional NeRFs. Thorough experiments on the large-scale driving datasets (e.g., nuScenes and Waymo) demonstrate that our method beats the state-of-the-art rivals by reducing 7～40\% of the mean-squared error in the street-view synthesis and a 45\% PSNR gain for the moving vehicles rendering.},
	language = {en},
	urldate = {2023-05-15},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Xie, Ziyang and Zhang, Junge and Li, Wenye and Zhang, Feihu and Zhang, Li},
	month = feb,
	year = {2023},
	keywords = {/unread},
}

@inproceedings{zeng2023nrhints,
	title = {Relighting neural radiance fields with shadow and highlight hints},
	booktitle = {{ACM} {SIGGRAPH} 2023 conference proceedings},
	author = {Zeng, Chong and Chen, Guojun and Dong, Yue and Peers, Pieter and Wu, Hongzhi and Tong, Xin},
	year = {2023},
	keywords = {⛔ No DOI found},
}

@misc{zhang_arf_2022,
	title = {{ARF}: {Artistic} {Radiance} {Fields}},
	shorttitle = {{ARF}},
	url = {http://arxiv.org/abs/2206.06360},
	doi = {10.48550/arXiv.2206.06360},
	abstract = {We present a method for transferring the artistic features of an arbitrary style image to a 3D scene. Previous methods that perform 3D stylization on point clouds or meshes are sensitive to geometric reconstruction errors for complex real-world scenes. Instead, we propose to stylize the more robust radiance field representation. We find that the commonly used Gram matrix-based loss tends to produce blurry results without faithful brushstrokes, and introduce a nearest neighbor-based loss that is highly effective at capturing style details while maintaining multi-view consistency. We also propose a novel deferred back-propagation method to enable optimization of memory-intensive radiance fields using style losses defined on full-resolution rendered images. Our extensive evaluation demonstrates that our method outperforms baselines by generating artistic appearance that more closely resembles the style image. Please check our project page for video results and open-source implementations: https://www.cs.cornell.edu/projects/arf/ .},
	urldate = {2023-07-15},
	publisher = {arXiv},
	author = {Zhang, Kai and Kolkin, Nick and Bi, Sai and Luan, Fujun and Xu, Zexiang and Shechtman, Eli and Snavely, Noah},
	month = jun,
	year = {2022},
	note = {arXiv:2206.06360 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zheng_editablenerf_2023,
	title = {{EditableNeRF}: {Editing} {Topologically} {Varying} {Neural} {Radiance} {Fields} by {Key} {Points}},
	shorttitle = {{EditableNeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_EditableNeRF_Editing_Topologically_Varying_Neural_Radiance_Fields_by_Key_Points_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-15},
	author = {Zheng, Chengwei and Lin, Wenbin and Xu, Feng},
	year = {2023},
	pages = {8317--8327},
}

@misc{liu_editing_2021,
	title = {Editing {Conditional} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2105.06466},
	doi = {10.48550/arXiv.2105.06466},
	abstract = {A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.},
	urldate = {2023-07-15},
	publisher = {arXiv},
	author = {Liu, Steven and Zhang, Xiuming and Zhang, Zhoutong and Zhang, Richard and Zhu, Jun-Yan and Russell, Bryan},
	month = jun,
	year = {2021},
	note = {arXiv:2105.06466 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@article{zeng_relighting_2023,
	title = {Relighting {Neural} {Radiance} {Fields} with {Shadow} and {Highlight} {Hints}},
	language = {en},
	journal = {Conference Proceedings},
	author = {Zeng, Chong and Chen, Guojun and Dong, Yue},
	year = {2023},
	keywords = {⛔ No DOI found},
}

@misc{hu_nerf-rpn_2023,
	title = {{NeRF}-{RPN}: {A} general framework for object detection in {NeRFs}},
	shorttitle = {{NeRF}-{RPN}},
	url = {http://arxiv.org/abs/2211.11646},
	doi = {10.48550/arXiv.2211.11646},
	abstract = {This paper presents the first significant object detection framework, NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model, NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting a novel voxel representation that incorporates multi-scale 3D neural volumetric features, we demonstrate it is possible to regress the 3D bounding boxes of objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN is a general framework and can be applied to detect objects without class labels. We experimented NeRF-RPN with various backbone architectures, RPN head designs and loss functions. All of them can be trained in an end-to-end manner to estimate high quality 3D bounding boxes. To facilitate future research in object detection for NeRF, we built a new benchmark dataset which consists of both synthetic and real-world data with careful labeling and clean up. Code and dataset are available at https://github.com/lyclyc52/NeRF\_RPN.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Hu, Benran and Huang, Junkai and Liu, Yichen and Tai, Yu-Wing and Tang, Chi-Keung},
	month = mar,
	year = {2023},
	note = {arXiv:2211.11646 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{huang_sc-neus_2023,
	title = {{SC}-{NeuS}: {Consistent} {Neural} {Surface} {Reconstruction} from {Sparse} and {Noisy} {Views}},
	shorttitle = {{SC}-{NeuS}},
	url = {http://arxiv.org/abs/2307.05892},
	abstract = {The recent neural surface reconstruction by volume rendering approaches have made much progress by achieving impressive surface reconstruction quality, but are still limited to dense and highly accurate posed views. To overcome such drawbacks, this paper pays special attention on the consistent surface reconstruction from sparse views with noisy camera poses. Unlike previous approaches, the key difference of this paper is to exploit the multi-view constraints directly from the explicit geometry of the neural surface, which can be used as effective regularization to jointly learn the neural surface and refine the camera poses. To build effective multi-view constraints, we introduce a fast differentiable on-surface intersection to generate on-surface points, and propose view-consistent losses based on such differentiable points to regularize the neural surface learning. Based on this point, we propose a jointly learning strategy for neural surface and camera poses, named SC-NeuS, to perform geometry-consistent surface reconstruction in an end-to-end manner. With extensive evaluation on public datasets, our SC-NeuS can achieve consistently better surface reconstruction results with fine-grained details than previous state-of-the-art neural surface reconstruction approaches, especially from sparse and noisy camera views.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Huang, Shi-Sheng and Zou, Zi-Xin and Zhang, Yi-Chi and Huang, Hua},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05892 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{raghavan_neural_2023,
	title = {Neural {Free}-{Viewpoint} {Relighting} for {Glossy} {Indirect} {Illumination}},
	url = {http://arxiv.org/abs/2307.06335},
	abstract = {Precomputed Radiance Transfer (PRT) remains an attractive solution for real-time rendering of complex light transport effects such as glossy global illumination. After precomputation, we can relight the scene with new environment maps while changing viewpoint in real-time. However, practical PRT methods are usually limited to low-frequency spherical harmonic lighting. All-frequency techniques using wavelets are promising but have so far had little practical impact. The curse of dimensionality and much higher data requirements have typically limited them to relighting with fixed view or only direct lighting with triple product integrals. In this paper, we demonstrate a hybrid neural-wavelet PRT solution to high-frequency indirect illumination, including glossy reflection, for relighting with changing view. Specifically, we seek to represent the light transport function in the Haar wavelet basis. For global illumination, we learn the wavelet transport using a small multi-layer perceptron (MLP) applied to a feature field as a function of spatial location and wavelet index, with reflected direction and material parameters being other MLP inputs. We optimize/learn the feature field (compactly represented by a tensor decomposition) and MLP parameters from multiple images of the scene under different lighting and viewing conditions. We demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputed rendering of challenging scenes involving view-dependent reflections and even caustics.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Raghavan, Nithin and Xiao, Yan and Lin, Kai-En and Sun, Tiancheng and Bi, Sai and Xu, Zexiang and Li, Tzu-Mao and Ramamoorthi, Ravi},
	month = jul,
	year = {2023},
	note = {arXiv:2307.06335 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{wang_prolificdreamer_2023,
	title = {{ProlificDreamer}: {High}-{Fidelity} and {Diverse} {Text}-to-{3D} {Generation} with {Variational} {Score} {Distillation}},
	shorttitle = {{ProlificDreamer}},
	url = {http://arxiv.org/abs/2305.16213},
	doi = {10.48550/arXiv.2305.16213},
	abstract = {Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., \$7.5\$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., \$512{\textbackslash}times512\$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page: https://ml.cs.tsinghua.edu.cn/prolificdreamer/},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Wang, Zhengyi and Lu, Cheng and Wang, Yikai and Bao, Fan and Li, Chongxuan and Su, Hang and Zhu, Jun},
	month = may,
	year = {2023},
	note = {arXiv:2305.16213 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{li_pac-nerf_2022,
	title = {{PAC}-{NeRF}: {Physics} {Augmented} {Continuum} {Neural} {Radiance} {Fields} for {Geometry}-{Agnostic} {System} {Identification}},
	shorttitle = {{PAC}-{NeRF}},
	url = {https://openreview.net/forum?id=tVkrbkz42vc},
	abstract = {Existing approaches to system identification (estimating the physical parameters of an object) from videos assume known object geometries. This precludes their applicability in a vast majority of scenes where object geometries are complex or unknown. In this work, we aim to identify parameters characterizing a physical system from a set of multi-view videos without any assumption on object geometry or topology. To this end, we propose "Physics Augmented Continuum Neural Radiance Fields" (PAC-NeRF), to estimate both the unknown geometry and physical parameters of highly dynamic objects from multi-view videos. We design PAC-NeRF to only ever produce physically plausible states by enforcing the neural radiance field to follow the conservation laws of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian representation of the neural radiance field, i.e., we use the Eulerian grid representation for NeRF density and color fields, while advecting the neural radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian representation seamlessly blends efficient neural rendering with the material point method (MPM) for robust differentiable physics simulation. We validate the effectiveness of our proposed framework on geometry and physical parameter estimation over a vast range of materials, including elastic bodies, plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate significant performance gain on most tasks.},
	language = {en},
	urldate = {2023-07-12},
	author = {Li, Xuan and Qiao, Yi-Ling and Chen, Peter Yichen and Jatavallabhula, Krishna Murthy and Lin, Ming and Jiang, Chenfanfu and Gan, Chuang},
	month = sep,
	year = {2022},
}

@misc{monnier_differentiable_2023,
	title = {Differentiable {Blocks} {World}: {Qualitative} {3D} {Decomposition} by {Rendering} {Primitives}},
	shorttitle = {Differentiable {Blocks} {World}},
	url = {http://arxiv.org/abs/2307.05473},
	doi = {10.48550/arXiv.2307.05473},
	abstract = {Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives. While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D representations made of a small set of textured primitives. Such representations are interpretable, easy to manipulate and suited for physics-based simulations. Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering. Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss. We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives. We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions. We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio. We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations. Code and video results are available at https://www.tmonnier.com/DBW .},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Monnier, Tom and Austin, Jake and Kanazawa, Angjoo and Efros, Alexei A. and Aubry, Mathieu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05473 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_neural_2023,
	title = {Neural {Point}-based {Volumetric} {Avatar}: {Surface}-guided {Neural} {Points} for {Efficient} and {Photorealistic} {Volumetric} {Head} {Avatar}},
	shorttitle = {Neural {Point}-based {Volumetric} {Avatar}},
	url = {http://arxiv.org/abs/2307.05000},
	doi = {10.48550/arXiv.2307.05000},
	abstract = {Rendering photorealistic and dynamically moving human heads is crucial for ensuring a pleasant and immersive experience in AR/VR and video conferencing applications. However, existing methods often struggle to model challenging facial regions (e.g., mouth interior, eyes, hair/beard), resulting in unrealistic and blurry results. In this paper, we propose \{{\textbackslash}fullname\} (\{{\textbackslash}name\}), a method that adopts the neural point representation as well as the neural volume rendering process and discards the predefined connectivity and hard correspondence imposed by mesh-based approaches. Specifically, the neural points are strategically constrained around the surface of the target expression via a high-resolution UV displacement map, achieving increased modeling capacity and more accurate control. We introduce three technical innovations to improve the rendering and training efficiency: a patch-wise depth-guided (shading point) sampling strategy, a lightweight radiance decoding process, and a Grid-Error-Patch (GEP) ray sampling strategy during training. By design, our \{{\textbackslash}name\} is better equipped to handle topologically changing regions and thin structures while also ensuring accurate expression control when animating avatars. Experiments conducted on three subjects from the Multiface dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods, especially in handling challenging facial regions.},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Wang, Cong and Kang, Di and Cao, Yanpei and Bao, Linchao and Shan, Ying and Zhang, Song-Hai},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05000 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{lindenberger_pixel-perfect_2021,
	title = {Pixel-{Perfect} {Structure}-{From}-{Motion} {With} {Featuremetric} {Refinement}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lindenberger_Pixel-Perfect_Structure-From-Motion_With_Featuremetric_Refinement_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-07-12},
	author = {Lindenberger, Philipp and Sarlin, Paul-Edouard and Larsson, Viktor and Pollefeys, Marc},
	year = {2021},
	pages = {5987--5997},
}

@misc{liu_zero-1--3_2023,
	title = {Zero-1-to-3: {Zero}-shot {One} {Image} to {3D} {Object}},
	shorttitle = {Zero-1-to-3},
	url = {http://arxiv.org/abs/2303.11328},
	doi = {10.48550/arXiv.2303.11328},
	abstract = {We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11328 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{warburg_nerfbusters_2023,
	title = {Nerfbusters: {Removing} {Ghostly} {Artifacts} from {Casually} {Captured} {NeRFs}},
	shorttitle = {Nerfbusters},
	url = {https://arxiv.org/abs/2304.10532v1},
	abstract = {Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such as floaters or flawed geometry when rendered outside the camera trajectory. Existing evaluation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To push forward progress in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for training, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regularizers do not remove floaters nor improve scene geometry. Thus, we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures.},
	language = {en},
	urldate = {2023-04-22},
	author = {Warburg, Frederik and Weber, Ethan and Tancik, Matthew and Holynski, Aleksander and Kanazawa, Angjoo},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{farokhmanesh_neural_2023,
	title = {Neural {Fields} for {Interactive} {Visualization} of {Statistical} {Dependencies} in {3D} {Simulation} {Ensembles}},
	url = {http://arxiv.org/abs/2307.02203},
	doi = {10.48550/arXiv.2307.02203},
	abstract = {We present the first neural network that has learned to compactly represent and can efficiently reconstruct the statistical dependencies between the values of physical variables at different spatial locations in large 3D simulation ensembles. Going beyond linear dependencies, we consider mutual information as a measure of non-linear dependence. We demonstrate learning and reconstruction with a large weather forecast ensemble comprising 1000 members, each storing multiple physical variables at a 250 x 352 x 20 simulation grid. By circumventing compute-intensive statistical estimators at runtime, we demonstrate significantly reduced memory and computation requirements for reconstructing the major dependence structures. This enables embedding the estimator into a GPU-accelerated direct volume renderer and interactively visualizing all mutual dependencies for a selected domain point.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Farokhmanesh, Fatemeh and Höhlein, Kevin and Neuhauser, Christoph and Westermann, Rüdiger},
	month = jul,
	year = {2023},
	note = {arXiv:2307.02203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{vavilala_blocks2world_2023,
	title = {{Blocks2World}: {Controlling} {Realistic} {Scenes} with {Editable} {Primitives}},
	shorttitle = {{Blocks2World}},
	url = {http://arxiv.org/abs/2307.03847},
	doi = {10.48550/arXiv.2307.03847},
	abstract = {We present Blocks2World, a novel method for 3D scene rendering and editing that leverages a two-step process: convex decomposition of images and conditioned synthesis. Our technique begins by extracting 3D parallelepipeds from various objects in a given scene using convex decomposition, thus obtaining a primitive representation of the scene. These primitives are then utilized to generate paired data through simple ray-traced depth maps. The next stage involves training a conditioned model that learns to generate images from the 2D-rendered convex primitives. This step establishes a direct mapping between the 3D model and its 2D representation, effectively learning the transition from a 3D model to an image. Once the model is fully trained, it offers remarkable control over the synthesis of novel and edited scenes. This is achieved by manipulating the primitives at test time, including translating or adding them, thereby enabling a highly customizable scene rendering process. Our method provides a fresh perspective on 3D scene rendering and editing, offering control and flexibility. It opens up new avenues for research and applications in the field, including authoring and data augmentation.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Vavilala, Vaibhav and Jain, Seemandhar and Vasanth, Rahul and Bhattad, Anand and Forsyth, David},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03847 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_nerflix_2023,
	title = {From {NeRFLiX} to {NeRFLiX}++: {A} {General} {NeRF}-{Agnostic} {Restorer} {Paradigm}},
	shorttitle = {From {NeRFLiX} to {NeRFLiX}++},
	url = {http://arxiv.org/abs/2306.06388},
	doi = {10.48550/arXiv.2306.06388},
	abstract = {Neural radiance fields (NeRF) have shown great success in novel view synthesis. However, recovering high-quality details from real-world scenes is still challenging for the existing NeRF-based approaches, due to the potential imperfect calibration information and scene representation inaccuracy. Even with high-quality training frames, the synthetic novel views produced by NeRF models still suffer from notable rendering artifacts, such as noise and blur. To address this, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm that learns a degradation-driven inter-viewpoint mixer. Specially, we design a NeRF-style degradation modeling approach and construct large-scale training data, enabling the possibility of effectively removing NeRF-native rendering artifacts for deep neural networks. Moreover, beyond the degradation removal, we propose an inter-viewpoint aggregation framework that fuses highly related high-quality training images, pushing the performance of cutting-edge NeRF models to entirely new levels and producing highly photo-realistic synthetic views. Based on this paradigm, we further present NeRFLiX++ with a stronger two-stage NeRF degradation simulator and a faster inter-viewpoint mixer, achieving superior performance with significantly improved computational efficiency. Notably, NeRFLiX++ is capable of restoring photo-realistic ultra-high-resolution outputs from noisy low-resolution NeRF-rendered views. Extensive experiments demonstrate the excellent restoration ability of NeRFLiX++ on various novel view synthesis benchmarks.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Zhou, Kun and Li, Wenbo and Jiang, Nianjuan and Han, Xiaoguang and Lu, Jiangbo},
	month = jul,
	year = {2023},
	note = {arXiv:2306.06388 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chen_flow_2022,
	title = {Flow {Supervised} {Neural} {Radiance} {Fields} for {Static}-{Dynamic} {Decomposition}},
	doi = {10.1109/ICRA46639.2022.9811680},
	abstract = {We present an approach to synthesize novel views from dynamics scenes captured by multi-view videos of cameras mounted on a driving vehicle. We unify existing methods and propose a new training loss to explicitly disentangle the static background from the dynamic foreground objects using scene flow's magnitude, learnt only from proxy 2D optical flow supervision. We obtain high quality static and dynamic contents separately, which allow us to combine them freely for novel view and time syntheses. We establish a dataset consisting of 5 dynamic scenes with varying difficulties on which we conduct experiments, and show that our method is able to handle challenging scenarios in real-world traffics and create high quality novel view and time syntheses.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Chen, Quei-An and Tsukada, Akihiro},
	month = may,
	year = {2022},
	keywords = {Cameras, Computer vision, Dynamics, Image motion analysis, Optical losses, Robot vision systems, Training},
	pages = {10641--10647},
}

@inproceedings{wang_neural_2023-1,
	title = {Neural {Fields} {Meet} {Explicit} {Geometric} {Representations} for {Inverse} {Rendering} of {Urban} {Scenes}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Neural_Fields_Meet_Explicit_Geometric_Representations_for_Inverse_Rendering_of_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-11},
	author = {Wang, Zian and Shen, Tianchang and Gao, Jun and Huang, Shengyu and Munkberg, Jacob and Hasselgren, Jon and Gojcic, Zan and Chen, Wenzheng and Fidler, Sanja},
	year = {2023},
	pages = {8370--8380},
}

@misc{guo_streetsurf_2023,
	title = {{StreetSurf}: {Extending} {Multi}-view {Implicit} {Surface} {Reconstruction} to {Street} {Views}},
	shorttitle = {{StreetSurf}},
	url = {http://arxiv.org/abs/2306.04988},
	doi = {10.48550/arXiv.2306.04988},
	abstract = {We present a novel multi-view implicit surface reconstruction technique, termed StreetSurf, that is readily applicable to street view images in widely-used autonomous driving datasets, such as Waymo-perception sequences, without necessarily requiring LiDAR data. As neural rendering research expands rapidly, its integration into street views has started to draw interests. Existing approaches on street views either mainly focus on novel view synthesis with little exploration of the scene geometry, or rely heavily on dense LiDAR data when investigating reconstruction. Neither of them investigates multi-view implicit surface reconstruction, especially under settings without LiDAR data. Our method extends prior object-centric neural surface reconstruction techniques to address the unique challenges posed by the unbounded street views that are captured with non-object-centric, long and narrow camera trajectories. We delimit the unbounded space into three parts, close-range, distant-view and sky, with aligned cuboid boundaries, and adapt cuboid/hyper-cuboid hash-grids along with road-surface initialization scheme for finer and disentangled representation. To further address the geometric errors arising from textureless regions and insufficient viewing angles, we adopt geometric priors that are estimated using general purpose monocular models. Coupled with our implementation of efficient and fine-grained multi-stage ray marching strategy, we achieve state of the art reconstruction quality in both geometry and appearance within only one to two hours of training time with a single RTX3090 GPU for each street view sequence. Furthermore, we demonstrate that the reconstructed implicit surfaces have rich potential for various downstream tasks, including ray tracing and LiDAR simulation.},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Guo, Jianfei and Deng, Nianchen and Li, Xinyang and Bai, Yeqi and Shi, Botian and Wang, Chiyu and Ding, Chenjing and Wang, Dongliang and Li, Yikang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.04988 [cs]
titleTranslation:},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{xiangli_bungeenerf_2022,
	title = {{BungeeNeRF}: {Progressive} {Neural} {Radiance} {Field} for {Extreme} {Multi}-scale {Scene} {Rendering}},
	shorttitle = {{BungeeNeRF}},
	url = {http://arxiv.org/abs/2112.05504},
	doi = {10.48550/arXiv.2112.05504},
	abstract = {Neural radiance fields (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce BungeeNeRF, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shallow base block, as training progresses, new blocks are appended to accommodate the emerging details in the increasingly closer views. The strategy progressively activates high-frequency channels in NeRF's positional encoding inputs and successively unfolds more complex details as the training proceeds. We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale scenes with drastically varying views on multiple data sources (city models, synthetic, and drone captured data) and its support for high-quality rendering in different levels of detail.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Xiangli, Yuanbo and Xu, Linning and Pan, Xingang and Zhao, Nanxuan and Rao, Anyi and Theobalt, Christian and Dai, Bo and Lin, Dahua},
	month = jul,
	year = {2022},
	note = {arXiv:2112.05504 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Done Reading, ⛔ No INSPIRE recid found},
}

@inproceedings{runz_maskfusion_2018,
	address = {Munich, Germany},
	title = {{MaskFusion}: {Real}-{Time} {Recognition}, {Tracking} and {Reconstruction} of {Multiple} {Moving} {Objects}},
	isbn = {978-1-5386-7459-8},
	shorttitle = {{MaskFusion}},
	url = {https://ieeexplore.ieee.org/document/8613746/},
	doi = {10.1109/ISMAR.2018.00024},
	urldate = {2023-07-11},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} ({ISMAR})},
	publisher = {IEEE},
	author = {Runz, Martin and Buffier, Maud and Agapito, Lourdes},
	month = oct,
	year = {2018},
	pages = {10--20},
}

@inproceedings{wu_referring_2023,
	title = {Referring {Multi}-{Object} {Tracking}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Referring_Multi-Object_Tracking_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-07},
	author = {Wu, Dongming and Han, Wencheng and Wang, Tiancai and Dong, Xingping and Zhang, Xiangyu and Shen, Jianbing},
	year = {2023},
	pages = {14633--14642},
}

@misc{shi_garfgeometry-aware_2022,
	title = {{GARF}:{Geometry}-{Aware} {Generalized} {Neural} {Radiance} {Field}},
	shorttitle = {{GARF}},
	url = {http://arxiv.org/abs/2212.02280},
	abstract = {Neural Radiance Field (NeRF) has revolutionized free viewpoint rendering tasks and achieved impressive results. However, the efficiency and accuracy problems hinder its wide applications. To address these issues, we propose Geometry-Aware Generalized Neural Radiance Field (GARF) with a geometry-aware dynamic sampling (GADS) strategy to perform real-time novel view rendering and unsupervised depth estimation on unseen scenes without per-scene optimization. Distinct from most existing generalized NeRFs, our framework infers the unseen scenes on both pixel-scale and geometry-scale with only a few input images. More specifically, our method learns common attributes of novel-view synthesis by an encoder-decoder structure and a point-level learnable multi-view feature fusion module which helps avoid occlusion. To preserve scene characteristics in the generalized model, we introduce an unsupervised depth estimation module to derive the coarse geometry, narrow down the ray sampling interval to proximity space of the estimated surface and sample in expectation maximum position, constituting Geometry-Aware Dynamic Sampling strategy (GADS). Moreover, we introduce a Multi-level Semantic Consistency loss (MSC) to assist more informative representation learning. Extensive experiments on indoor and outdoor datasets show that comparing with state-of-the-art generalized NeRF methods, GARF reduces samples by more than 25{\textbackslash}\%, while improving rendering quality and 3D geometry estimation.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Shi, Yue and Rong, Dingyi and Ni, Bingbing and Chen, Chang and Zhang, Wenjun},
	month = dec,
	year = {2022},
	note = {arXiv:2212.02280 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{niemeyer_regnerf_2022,
	title = {{RegNeRF}: {Regularizing} {Neural} {Radiance} {Fields} for {View} {Synthesis} {From} {Sparse} {Inputs}},
	shorttitle = {{RegNeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Niemeyer_RegNeRF_Regularizing_Neural_Radiance_Fields_for_View_Synthesis_From_Sparse_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-11},
	author = {Niemeyer, Michael and Barron, Jonathan T. and Mildenhall, Ben and Sajjadi, Mehdi S. M. and Geiger, Andreas and Radwan, Noha},
	year = {2022},
	pages = {5480--5490},
}

@article{yang_cubeslam_2019,
	title = {{CubeSLAM}: {Monocular} {3D} {Object} {SLAM}},
	volume = {35},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{CubeSLAM}},
	url = {http://arxiv.org/abs/1806.00557},
	doi = {10.1109/TRO.2019.2909168},
	abstract = {We present a method for single image 3D cuboid object detection and multi-view object SLAM in both static and dynamic environments, and demonstrate that the two parts can improve each other. Firstly for single image object detection, we generate high-quality cuboid proposals from 2D bounding boxes and vanishing points sampling. The proposals are further scored and selected based on the alignment with image edges. Secondly, multi-view bundle adjustment with new object measurements is proposed to jointly optimize poses of cameras, objects and points. Objects can provide long-range geometric and scale constraints to improve camera pose estimation and reduce monocular drift. Instead of treating dynamic regions as outliers, we utilize object representation and motion model constraints to improve the camera pose estimation. The 3D detection experiments on SUN RGBD and KITTI show better accuracy and robustness over existing approaches. On the public TUM, KITTI odometry and our own collected datasets, our SLAM method achieves the state-of-the-art monocular camera pose estimation and at the same time, improves the 3D object detection accuracy.},
	number = {4},
	urldate = {2023-07-11},
	journal = {IEEE Transactions on Robotics},
	author = {Yang, Shichao and Scherer, Sebastian},
	month = aug,
	year = {2019},
	note = {arXiv:1806.00557 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	pages = {925--938},
}

@misc{wang_dsp-slam_2021,
	title = {{DSP}-{SLAM}: {Object} {Oriented} {SLAM} with {Deep} {Shape} {Priors}},
	shorttitle = {{DSP}-{SLAM}},
	url = {http://arxiv.org/abs/2108.09481},
	doi = {10.48550/arXiv.2108.09481},
	abstract = {We propose DSP-SLAM, an object-oriented SLAM system that builds a rich and accurate joint map of dense 3D models for foreground objects, and sparse landmark points to represent the background. DSP-SLAM takes as input the 3D point cloud reconstructed by a feature-based SLAM system and equips it with the ability to enhance its sparse map with dense reconstructions of detected objects. Objects are detected via semantic instance segmentation, and their shape and pose is estimated using category-specific deep shape embeddings as priors, via a novel second order optimization. Our object-aware bundle adjustment builds a pose-graph to jointly optimize camera poses, object locations and feature points. DSP-SLAM can operate at 10 frames per second on 3 different input modalities: monocular, stereo, or stereo+LiDAR. We demonstrate DSP-SLAM operating at almost frame rate on monocular-RGB sequences from the Friburg and Redwood-OS datasets, and on stereo+LiDAR sequences on the KITTI odometry dataset showing that it achieves high-quality full object reconstructions, even from partial observations, while maintaining a consistent global map. Our evaluation shows improvements in object pose and shape reconstruction with respect to recent deep prior-based reconstruction methods and reductions in camera tracking drift on the KITTI dataset.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Wang, Jingwen and Rünz, Martin and Agapito, Lourdes},
	month = oct,
	year = {2021},
	note = {arXiv:2108.09481 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{wang_neus_2021,
	title = {{NeuS}: {Learning} {Neural} {Implicit} {Surfaces} by {Volume} {Rendering} for {Multi}-view {Reconstruction}},
	shorttitle = {{NeuS}},
	url = {http://arxiv.org/abs/2106.10689},
	doi = {10/gr8wgx},
	abstract = {We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.},
	urldate = {2022-10-09},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {NeurIPS 2021},
	author = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
	month = dec,
	year = {2021},
	note = {arXiv:2106.10689 [cs]
titleTranslation:
titleTranslation:
titleTranslation:},
	keywords = {2021, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading, IDR, NeuS, NeurIPS, SDF, ⛔ No INSPIRE recid found},
}

@inproceedings{li_neuralangelo_2023,
	title = {Neuralangelo: {High}-{Fidelity} {Neural} {Surface} {Reconstruction}},
	shorttitle = {Neuralangelo},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_Neuralangelo_High-Fidelity_Neural_Surface_Reconstruction_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Zhaoshuo and Müller, Thomas and Evans, Alex and Taylor, Russell H. and Unberath, Mathias and Liu, Ming-Yu and Lin, Chen-Hsuan},
	year = {2023},
	pages = {8456--8465},
}

@misc{wang_tracking_2023,
	title = {Tracking {Everything} {Everywhere} {All} at {Once}},
	url = {http://arxiv.org/abs/2306.05422},
	doi = {10.48550/arXiv.2306.05422},
	abstract = {We present a new test-time optimization method for estimating dense and long-range motion from a video sequence. Prior optical flow or particle video tracking algorithms typically operate within limited temporal windows, struggling to track through occlusions and maintain global consistency of estimated motion trajectories. We propose a complete and globally consistent motion representation, dubbed OmniMotion, that allows for accurate, full-length motion estimation of every pixel in a video. OmniMotion represents a video using a quasi-3D canonical volume and performs pixel-wise tracking via bijections between local and canonical space. This representation allows us to ensure global consistency, track through occlusions, and model any combination of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and real-world footage show that our approach outperforms prior state-of-the-art methods by a large margin both quantitatively and qualitatively. See our project page for more results: http://omnimotion.github.io/},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Wang, Qianqian and Chang, Yen-Yu and Cai, Ruojin and Li, Zhengqi and Hariharan, Bharath and Holynski, Aleksander and Snavely, Noah},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05422 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wang_co-slam_2023,
	title = {Co-{SLAM}: {Joint} {Coordinate} and {Sparse} {Parametric} {Encodings} for {Neural} {Real}-{Time} {SLAM}},
	shorttitle = {Co-{SLAM}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-10},
	author = {Wang, Hengyi and Wang, Jingwen and Agapito, Lourdes},
	year = {2023},
	pages = {13293--13302},
}

@phdthesis{ranjan_towards_2019,
	title = {Towards {Geometric} {Understanding} of {Motion}},
	url = {https://ps.is.mpg.de/publications/ranjan-thesis},
	school = {University of Tübingen},
	author = {Ranjan, Anurag},
	month = dec,
	year = {2019},
}

@inproceedings{zhu_nice-slam_2022,
	title = {{NICE}-{SLAM}: {Neural} {Implicit} {Scalable} {Encoding} for {SLAM}},
	shorttitle = {{NICE}-{SLAM}},
	doi = {10/gq2mfm},
	abstract = {Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over- smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R. and Pollefeys, Marc},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Cameras, Computer vision, Geometry, Network architecture, RGBD sensors and analytics, Scalability, Simultaneous localization and mapping, Visualization, ⛔ No INSPIRE recid found},
	pages = {12776--12786},
}

@misc{teigen_rgb-d_2023,
	title = {{RGB}-{D} {Mapping} and {Tracking} in a {Plenoxel} {Radiance} {Field}},
	url = {http://arxiv.org/abs/2307.03404},
	doi = {10.48550/arXiv.2307.03404},
	abstract = {Building on the success of Neural Radiance Fields (NeRFs), recent years have seen significant advances in the domain of novel view synthesis. These models capture the scene's volumetric radiance field, creating highly convincing dense photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this technical report, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both the mapping and tracking tasks while also being faster than competing neural network-based approaches.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Teigen, Andreas L. and Park, Yeonsoo and Stahl, Annette and Mester, Rudolf},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03404 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{dinh_density_2016,
	title = {Density estimation using {Real} {NVP}},
	url = {https://openreview.net/forum?id=HkpbnH9lx},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	language = {en},
	urldate = {2023-07-09},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	month = nov,
	year = {2016},
}

@inproceedings{fathony_multiplicative_2020,
	title = {Multiplicative {Filter} {Networks}},
	url = {https://openreview.net/forum?id=OmtmcPkkhT},
	abstract = {Although deep networks are typically used to approximate functions over high dimensional inputs, recent work has increased interest in neural networks as function approximators for low-dimensional-but-complex functions, such as representing images as a function of pixel coordinates, solving differential equations, or representing signed distance fields or neural radiance fields. Key to these recent successes has been the use of new elements such as sinusoidal nonlinearities, or Fourier features in positional encodings, which vastly outperform simple ReLU networks. In this paper, we propose and empirically demonstrate that an arguably simpler class of function approximators can work just as well for such problems: multiplicative filter networks. In these networks, we avoid traditional compositional depth altogether, and simply multiply together (linear functions of) sinusoidal or Gabor wavelet functions applied to the input. This representation has the notable advantage that the entire function can simply be viewed as a linear function approximator over an exponential number of Fourier or Gabor basis functions, respectively. Despite this simplicity, when compared to recent approaches that use Fourier features with ReLU networks or sinusoidal activation networks, we show that these multiplicative filter networks largely outperform or match the performance of these recent approaches on the domains highlighted in these past works.},
	language = {en},
	urldate = {2023-07-09},
	author = {Fathony, Rizal and Sahu, Anit Kumar and Willmott, Devin and Kolter, J. Zico},
	month = oct,
	year = {2020},
}

@inproceedings{pumarola_d-nerf_2021,
	title = {D-{NeRF}: {Neural} {Radiance} {Fields} for {Dynamic} {Scenes}},
	shorttitle = {D-{NeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Pumarola_D-NeRF_Neural_Radiance_Fields_for_Dynamic_Scenes_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and Moreno-Noguer, Francesc},
	year = {2021},
	keywords = {/unread},
	pages = {10318--10327},
}

@inproceedings{turki_suds_2023,
	title = {{SUDS}: {Scalable} {Urban} {Dynamic} {Scenes}},
	shorttitle = {{SUDS}},
	url = {http://arxiv.org/abs/2303.14536},
	doi = {10.48550/arXiv.2303.14536},
	abstract = {We extend neural radiance fields (NeRFs) to dynamic large-scale urban scenes. Prior work tends to reconstruct single video clips of short durations (up to 10 seconds). Two reasons are that such methods (a) tend to scale linearly with the number of moving objects and input videos because a separate model is built for each and (b) tend to require supervision via 3D bounding boxes and panoptic labels, obtained manually or via category-specific models. As a step towards truly open-world reconstructions of dynamic cities, we introduce two key innovations: (a) we factorize the scene into three separate hash table data structures to efficiently encode static, dynamic, and far-field radiance fields, and (b) we make use of unlabeled target signals consisting of RGB images, sparse LiDAR, off-the-shelf self-supervised 2D descriptors, and most importantly, 2D optical flow. Operationalizing such inputs via photometric, geometric, and feature-metric reconstruction losses enables SUDS to decompose dynamic scenes into the static background, individual objects, and their motions. When combined with our multi-branch table representation, such reconstructions can be scaled to tens of thousands of objects across 1.2 million frames from 1700 videos spanning geospatial footprints of hundreds of kilometers, (to our knowledge) the largest dynamic NeRF built to date. We present qualitative initial results on a variety of tasks enabled by our representations, including novel-view synthesis of dynamic urban scenes, unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. To compare to prior work, we also evaluate on KITTI and Virtual KITTI 2, surpassing state-of-the-art methods that rely on ground truth 3D bounding box annotations while being 10x quicker to train.},
	urldate = {2023-03-28},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Turki, Haithem and Zhang, Jason Y. and Ferroni, Francesco and Ramanan, Deva},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14536 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, ⛔ No INSPIRE recid found},
}

@inproceedings{xu_grid-guided_2023,
	title = {Grid-{Guided} {Neural} {Radiance} {Fields} for {Large} {Urban} {Scenes}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Grid-Guided_Neural_Radiance_Fields_for_Large_Urban_Scenes_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Xu, Linning and Xiangli, Yuanbo and Peng, Sida and Pan, Xingang and Zhao, Nanxuan and Theobalt, Christian and Dai, Bo and Lin, Dahua},
	year = {2023},
	pages = {8296--8306},
}

@misc{kong_vmap_2023,
	title = {{vMAP}: {Vectorised} {Object} {Mapping} for {Neural} {Field} {SLAM}},
	shorttitle = {{vMAP}},
	url = {http://arxiv.org/abs/2302.01838},
	doi = {10.48550/arXiv.2302.01838},
	abstract = {We present vMAP, an object-level dense SLAM system using neural field representations. Each object is represented by a small MLP, enabling efficient, watertight object modelling without the need for 3D priors. As an RGB-D camera browses a scene with no prior information, vMAP detects object instances on-the-fly, and dynamically adds them to its map. Specifically, thanks to the power of vectorised training, vMAP can optimise as many as 50 individual objects in a single scene, with an extremely efficient training speed of 5Hz map update. We experimentally demonstrate significantly improved scene-level and object-level reconstruction quality compared to prior neural field SLAM systems.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Kong, Xin and Liu, Shikun and Taher, Marwan and Davison, Andrew J.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01838 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{paschalidou_neural_2021,
	title = {Neural {Parts}: {Learning} {Expressive} {3D} {Shape} {Abstractions} {With} {Invertible} {Neural} {Networks}},
	shorttitle = {Neural {Parts}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Paschalidou_Neural_Parts_Learning_Expressive_3D_Shape_Abstractions_With_Invertible_Neural_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-07-08},
	author = {Paschalidou, Despoina and Katharopoulos, Angelos and Geiger, Andreas and Fidler, Sanja},
	year = {2021},
	pages = {3204--3215},
}

@misc{song_nerfplayer_2022,
	title = {{NeRFPlayer}: {A} {Streamable} {Dynamic} {Scene} {Representation} with {Decomposed} {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRFPlayer}},
	url = {http://arxiv.org/abs/2210.15947},
	doi = {10.48550/arXiv.2210.15947},
	abstract = {Visually exploring in a real-world 4D spatiotemporal space freely in VR has been a long-term quest. The task is especially appealing when only a few or even single RGB cameras are used for capturing the dynamic scene. To this end, we present an efficient framework capable of fast reconstruction, compact modeling, and streamable rendering. First, we propose to decompose the 4D spatiotemporal space according to temporal characteristics. Points in the 4D space are associated with probabilities of belonging to three categories: static, deforming, and new areas. Each area is represented and regularized by a separate neural field. Second, we propose a hybrid representations based feature streaming scheme for efficiently modeling the neural fields. Our approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single hand-held cameras and multi-camera arrays, achieving comparable or superior rendering performance in terms of quality and speed comparable to recent state-of-the-art methods, achieving reconstruction in 10 seconds per frame and real-time rendering.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Song, Liangchen and Chen, Anpei and Li, Zhong and Chen, Zhang and Chen, Lele and Yuan, Junsong and Xu, Yi and Geiger, Andreas},
	month = oct,
	year = {2022},
	note = {arXiv:2210.15947 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Multimedia, Dynamic Scene, ETH, InnoPeak, NeRF, New York State University, University at Buffalo, University of Tubingen, ⛔ No INSPIRE recid found},
}

@inproceedings{guo_vid2avatar_2023,
	title = {{Vid2Avatar}: {3D} {Avatar} {Reconstruction} from {Videos} in the {Wild} via {Self}-supervised {Scene} {Decomposition}},
	shorttitle = {{Vid2Avatar}},
	url = {http://arxiv.org/abs/2302.11566},
	abstract = {We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires reconstructing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameterized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the background model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sampling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D human geometry reconstructions. We evaluate our methods on publicly available datasets and show improvements over prior art.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Guo, Chen and Jiang, Tianjian and Chen, Xu and Song, Jie and Hilliges, Otmar},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11566 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{alekseev_gabornet_2019,
	title = {{GaborNet}: {Gabor} filters with learnable parameters in deep convolutional neural networks},
	shorttitle = {{GaborNet}},
	url = {http://arxiv.org/abs/1904.13204},
	doi = {10.48550/arXiv.1904.13204},
	abstract = {The article describes a system for image recognition using deep convolutional neural networks. Modified network architecture is proposed that focuses on improving convergence and reducing training complexity. The filters in the first layer of the network are constrained to fit the Gabor function. The parameters of Gabor functions are learnable and are updated by standard backpropagation techniques. The system was implemented on Python, tested on several datasets and outperformed the common convolutional networks.},
	urldate = {2023-07-09},
	publisher = {arXiv},
	author = {Alekseev, Andrey and Bobe, Anatoly},
	month = apr,
	year = {2019},
	note = {arXiv:1904.13204 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{sandstrom_point-slam_2023,
	title = {Point-{SLAM}: {Dense} {Neural} {Point} {Cloud}-based {SLAM}},
	shorttitle = {Point-{SLAM}},
	url = {http://arxiv.org/abs/2304.04278},
	doi = {10.48550/arXiv.2304.04278},
	abstract = {We propose a dense neural simultaneous localization and mapping (SLAM) approach for monocular RGBD input which anchors the features of a neural scene representation in a point cloud that is iteratively generated in an input-dependent data-driven manner. We demonstrate that both tracking and mapping can be performed with the same point-based neural scene representation by minimizing an RGBD-based re-rendering loss. In contrast to recent dense neural SLAM methods which anchor the scene features in a sparse grid, our point-based approach allows dynamically adapting the anchor point density to the information density of the input. This strategy reduces runtime and memory usage in regions with fewer details and dedicates higher point density to resolve fine details. Our approach performs either better or competitive to existing dense neural RGBD SLAM methods in tracking, mapping and rendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source code is available at https://github.com/tfy14esa/Point-SLAM.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Sandström, Erik and Li, Yue and Van Gool, Luc and Oswald, Martin R.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04278 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{uzolas_template-free_2023,
	title = {Template-free {Articulated} {Neural} {Point} {Clouds} for {Reposable} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2305.19065},
	abstract = {Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when synthesizing novel views of time-evolving 3D scenes. However, the common reliance on backward deformation fields makes reanimation of the captured object poses challenging. Moreover, the state of the art dynamic models are often limited by low visual fidelity, long reconstruction time or specificity to narrow application domains. In this paper, we present a novel method utilizing a point-based representation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an associated skeletal model from even sparse multi-view video. Our forward-warping approach achieves state-of-the-art visual fidelity when synthesizing novel views and poses while significantly reducing the necessary learning time when compared to existing work. We demonstrate the versatility of our representation on a variety of articulated objects from common datasets and obtain reposable 3D reconstructions without the need of object-specific skeletal templates. Code will be made available at https://github.com/lukasuz/Articulated-Point-NeRF.},
	urldate = {2023-07-08},
	publisher = {arXiv},
	author = {Uzolas, Lukas and Eisemann, Elmar and Kellnhofer, Petr},
	month = may,
	year = {2023},
	note = {arXiv:2305.19065 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@article{geiger_vision_2013,
	title = {Vision meets robotics: {The} {KITTI} dataset},
	volume = {32},
	issn = {0278-3649},
	shorttitle = {Vision meets robotics},
	url = {https://doi.org/10.1177/0278364913491297},
	doi = {10/f5bnnx},
	abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
	number = {11},
	urldate = {2023-02-17},
	journal = {International Journal of Robotics Research},
	author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
	year = {2013},
	keywords = {/unread, Dataset, GPS, KITTI, SLAM, autonomous driving, benchmarks, cameras, computer vision, field robotics, laser, mobile robotics, object detection, optical flow, stereo, tracking},
	pages = {1231--1237},
}

@misc{su_-nerf_2021,
	title = {A-{NeRF}: {Articulated} {Neural} {Radiance} {Fields} for {Learning} {Human} {Shape}, {Appearance}, and {Pose}},
	shorttitle = {A-{NeRF}},
	url = {http://arxiv.org/abs/2102.06199},
	doi = {10.48550/arXiv.2102.06199},
	abstract = {While deep learning reshaped the classical motion capture pipeline with feed-forward networks, generative models are required to recover fine alignment via iterative refinement. Unfortunately, the existing models are usually hand-crafted or learned in controlled conditions, only applicable to limited domains. We propose a method to learn a generative neural body model from unlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We equip them with a skeleton to apply to time-varying and articulated motion. A key insight is that implicit models require the inverse of the forward kinematics used in explicit surface models. Our reparameterization defines spatial latent variables relative to the pose of body parts and thereby overcomes ill-posed inverse operations with an overparameterization. This enables learning volumetric body shape and appearance from scratch while jointly refining the articulated pose; all without ground truth labels for appearance, pose, or 3D shape on the input videos. When used for novel-view-synthesis and motion capture, our neural model improves accuracy on diverse datasets. Project website: https://lemonatsu.github.io/anerf/ .},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Su, Shih-Yang and Yu, Frank and Zollhoefer, Michael and Rhodin, Helge},
	month = oct,
	year = {2021},
	note = {arXiv:2102.06199 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{xu_motion_2018,
	title = {Motion {Segmentation} by {Exploiting} {Complementary} {Geometric} {Models}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Motion_Segmentation_by_CVPR_2018_paper.html},
	urldate = {2023-07-04},
	author = {Xu, Xun and Cheong, Loong Fah and Li, Zhuwen},
	year = {2018},
	pages = {2859--2867},
}

@inproceedings{xu_point-nerf_2022,
	title = {Point-{NeRF}: {Point}-based {Neural} {Radiance} {Fields}},
	shorttitle = {Point-{NeRF}},
	doi = {10/gq9msm},
	abstract = {Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30× faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Computational photography, Done Reading, Image and video synthesis and generation, Pipelines, Point cloud compression, Rendering (computer graphics), Solid modeling, Surface reconstruction, Three-dimensional displays, Training, ⛔ No INSPIRE recid found},
	pages = {5428--5438},
}

@inproceedings{dosovitskiy_carla_2017,
	title = {{CARLA}: {An} {Open} {Urban} {Driving} {Simulator}},
	shorttitle = {{CARLA}},
	url = {https://proceedings.mlr.press/v78/dosovitskiy17a.html},
	abstract = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform’s utility for autonomous driving research.},
	language = {en},
	urldate = {2023-06-27},
	booktitle = {Proceedings of the 1st {Annual} {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
	month = oct,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1--16},
}

@misc{chen_geosim_2021,
	title = {{GeoSim}: {Realistic} {Video} {Simulation} via {Geometry}-{Aware} {Composition} for {Self}-{Driving}},
	shorttitle = {{GeoSim}},
	url = {http://arxiv.org/abs/2101.06543},
	doi = {10.48550/arXiv.2101.06543},
	abstract = {Scalable sensor simulation is an important yet challenging open problem for safety-critical domains such as self-driving. Current works in image simulation either fail to be photorealistic or do not model the 3D environment and the dynamic objects within, losing high-level control and physical realism. In this paper, we present GeoSim, a geometry-aware image composition process which synthesizes novel urban driving scenarios by augmenting existing images with dynamic objects extracted from other scenes and rendered at novel poses. Towards this goal, we first build a diverse bank of 3D objects with both realistic geometry and appearance from sensor data. During simulation, we perform a novel geometry-aware simulation-by-composition procedure which 1) proposes plausible and realistic object placements into a given scene, 2) render novel views of dynamic objects from the asset bank, and 3) composes and blends the rendered image segments. The resulting synthetic images are realistic, traffic-aware, and geometrically consistent, allowing our approach to scale to complex use cases. We demonstrate two such important applications: long-range realistic video simulation across multiple camera sensors, and synthetic data generation for data augmentation on downstream segmentation tasks. Please check https://tmux.top/publication/geosim/ for high-resolution video results.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Chen, Yun and Rong, Frieda and Duggal, Shivam and Wang, Shenlong and Yan, Xinchen and Manivasagam, Sivabalan and Xue, Shangjie and Yumer, Ersin and Urtasun, Raquel},
	month = may,
	year = {2021},
	note = {arXiv:2101.06543 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@article{li_aads_2019,
	title = {{AADS}: {Augmented} autonomous driving simulation using data-driven algorithms},
	volume = {4},
	issn = {2470-9476},
	shorttitle = {{AADS}},
	url = {https://www.science.org/doi/10.1126/scirobotics.aaw0863},
	doi = {10/gjnj25},
	abstract = {A scalable simulation system combines augmented images with synthesized traffic to enable training and testing of autonomous driving.
          , 
            Simulation systems have become essential to the development and validation of autonomous driving (AD) technologies. The prevailing state-of-the-art approach for simulation uses game engines or high-fidelity computer graphics (CG) models to create driving scenarios. However, creating CG models and vehicle movements (the assets for simulation) remain manual tasks that can be costly and time consuming. In addition, CG images still lack the richness and authenticity of real-world images, and using CG images for training leads to degraded performance. Here, we present our augmented autonomous driving simulation (AADS). Our formulation augmented real-world pictures with a simulated traffic flow to create photorealistic simulation images and renderings. More specifically, we used LiDAR and cameras to scan street scenes. From the acquired trajectory data, we generated plausible traffic flows for cars and pedestrians and composed them into the background. The composite images could be resynthesized with different viewpoints and sensor models (camera or LiDAR). The resulting images are photorealistic, fully annotated, and ready for training and testing of AD systems from perception to planning. We explain our system design and validate our algorithms with a number of AD tasks from detection to segmentation and predictions. Compared with traditional approaches, our method offers scalability and realism. Scalability is particularly important for AD simulations, and we believe that real-world complexity and diversity cannot be realistically captured in a virtual environment. Our augmented approach combines the flexibility of a virtual environment (e.g., vehicle movements) with the richness of the real world to allow effective simulation.},
	number = {28},
	urldate = {2023-05-12},
	journal = {Science Robotics},
	author = {Li, W. and Pan, C. W. and Zhang, R. and Ren, J. P. and Ma, Y. X. and Fang, J. and Yan, F. L. and Geng, Q. C. and Huang, X. Y. and Gong, H. J. and Xu, W. W. and Wang, G. P. and Manocha, D. and Yang, R. G.},
	month = mar,
	year = {2019},
	keywords = {/unread},
	pages = {eaaw0863},
}

@misc{bergman_generative_2023,
	title = {Generative {Neural} {Articulated} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2206.14314},
	abstract = {Unsupervised learning of 3D-aware generative adversarial networks (GANs) using only collections of single-view 2D photographs has very recently made much progress. These 3D GANs, however, have not been demonstrated for human bodies and the generated radiance ﬁelds of existing frameworks are not directly editable, limiting their applicability in downstream tasks. We propose a solution to these challenges by developing a 3D GAN framework that learns to generate radiance ﬁelds of human bodies or faces in a canonical pose and warp them using an explicit deformation ﬁeld into a desired body pose or facial expression. Using our framework, we demonstrate the ﬁrst high-quality radiance ﬁeld generation results for human bodies. Moreover, we show that our deformation-aware training procedure signiﬁcantly improves the quality of generated bodies or faces when editing their poses or facial expressions compared to a 3D GAN that is not trained with explicit deformations.},
	language = {en},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Bergman, Alexander W. and Kellnhofer, Petr and Yifan, Wang and Chan, Eric R. and Lindell, David B. and Wetzstein, Gordon},
	month = jan,
	year = {2023},
	note = {arXiv:2206.14314 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{oquab_dinov2_2023,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{tang_compressible-composable_2022,
	title = {Compressible-composable {NeRF} via {Rank}-residual {Decomposition}},
	url = {https://openreview.net/forum?id=aPXMGv7aeOn},
	abstract = {Neural Radiance Field (NeRF) has emerged as a compelling method to represent 3D objects and scenes for photo-realistic rendering. However, its implicit representation causes difficulty in manipulating the models like the explicit mesh representation. Several recent advances in NeRF manipulation are usually restricted by a shared renderer network, or suffer from large model size. To circumvent the hurdle, in this paper, we present a neural field representation that enables efficient and convenient manipulation of models. To achieve this goal, we learn a hybrid tensor rank decomposition of the scene without neural networks. Motivated by the low-rank approximation property of the SVD algorithm, we propose a rank-residual learning strategy to encourage the preservation of primary information in lower ranks. The model size can then be dynamically adjusted by rank truncation to control the levels of detail, achieving near-optimal compression without extra optimization. Furthermore, different models can be arbitrarily transformed and composed into one scene by concatenating along the rank dimension. The growth of storage cost can also be mitigated by compressing the unimportant objects in the composed scene. We demonstrate that our method is able to achieve comparable rendering quality to state-of-the-art methods, while enabling extra capability of compression and composition. Code is available at https://github.com/ashawkey/CCNeRF.},
	language = {en},
	urldate = {2023-07-06},
	author = {Tang, Jiaxiang and Chen, Xiaokang and Wang, Jingbo and Zeng, Gang},
	month = may,
	year = {2022},
}

@misc{mei_rome_2023,
	title = {{RoMe}: {Towards} {Large} {Scale} {Road} {Surface} {Reconstruction} via {Mesh} {Representation}},
	shorttitle = {{RoMe}},
	url = {http://arxiv.org/abs/2306.11368},
	doi = {10.48550/arXiv.2306.11368},
	abstract = {Large-scale road surface reconstruction is becoming important to autonomous driving systems, as it provides valuable training and testing data effectively. In this paper, we introduce a simple yet efficient method, RoMe, for large-scale Road surface reconstruction via Mesh representations. To simplify the problem, RoMe decomposes a 3D road surface into a triangle-mesh and a multilayer perception network to model the road elevation implicitly. To retain fine surface details, each mesh vertex has two extra attributes, namely color and semantics. To improve the efficiency of RoMe in large-scale environments, a novel waypoint sampling method is introduced. As such, RoMe can properly preserve road surface details, with only linear computational complexity to road areas. In addition, to improve the accuracy of RoMe, extrinsics optimization is proposed to mitigate inaccurate extrinsic calibrations. Experimental results on popular public datasets also demonstrate the high efficiency and accuracy of RoMe.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Mei, Ruohong and Sui, Wei and Zhang, Jiaxin and Zhang, Qian and Peng, Tao and Yang, Cong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11368 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhu_i2-sdf_2023,
	title = {I2-{SDF}: {Intrinsic} {Indoor} {Scene} {Reconstruction} and {Editing} via {Raytracing} in {Neural} {SDFs}},
	shorttitle = {I2-{SDF}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_I2-SDF_Intrinsic_Indoor_Scene_Reconstruction_and_Editing_via_Raytracing_in_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-06},
	author = {Zhu, Jingsen and Huo, Yuchi and Ye, Qi and Luan, Fujun and Li, Jifan and Xi, Dianbing and Wang, Lisha and Tang, Rui and Hua, Wei and Bao, Hujun and Wang, Rui},
	year = {2023},
	pages = {12489--12498},
}

@inproceedings{wang_cadsim_2022,
	title = {{CADSim}: {Robust} and {Scalable} in-the-wild {3D} {Reconstruction} for {Controllable} {Sensor} {Simulation}},
	shorttitle = {{CADSim}},
	url = {https://openreview.net/forum?id=Mp3Y5jd7rnW},
	abstract = {Realistic simulation is key to enabling safe and scalable development of self-driving vehicles. A core component is simulating the sensors so that the entire autonomy system can be tested in simulation. Sensor simulation involves modeling traffic participants, such as vehicles, with high-quality appearance and articulated geometry, and rendering them in real-time. The self-driving industry has employed artists to build these assets. However, this is expensive, slow, and may not reflect reality. Instead, reconstructing assets automatically from sensor data collected in the wild would provide a better path to generating a diverse and large set that provides good real-world coverage. However, current reconstruction approaches struggle on in-the-wild sensor data, due to its sparsity and noise. To tackle these issues, we present CADSim which combines part-aware object-class priors via a small set of CAD models with differentiable rendering to automatically reconstruct vehicle geometry, including articulated wheels, with high-quality appearance. Our experiments show our approach recovers more accurate shape from sparse data compared to existing approaches. Importantly, it also trains and renders efficiently. We demonstrate our reconstructed vehicles in a wide range of applications, including accurate testing of autonomy perception systems.},
	language = {en},
	urldate = {2023-07-06},
	booktitle = {6th {Annual} {Conference} on {Robot} {Learning}},
	author = {Wang, Jingkang and Manivasagam, Sivabalan and Chen, Yun and Yang, Ze and Bârsan, Ioan Andrei and Yang, Anqi Joyce and Ma, Wei-Chiu and Urtasun, Raquel},
	month = aug,
	year = {2022},
}

@inproceedings{gao_dynamic_2021,
	title = {Dynamic {View} {Synthesis} {From} {Dynamic} {Monocular} {Video}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Gao_Dynamic_View_Synthesis_From_Dynamic_Monocular_Video_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-06-24},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Gao, Chen and Saraf, Ayush and Kopf, Johannes and Huang, Jia-Bin},
	year = {2021},
	pages = {5712--5721},
}

@article{park_hypernerf_2021,
	title = {{HyperNeRF}: a higher-dimensional representation for topologically varying neural radiance fields},
	volume = {40},
	issn = {0730-0301},
	shorttitle = {{HyperNeRF}},
	url = {https://dl.acm.org/doi/10.1145/3478513.3480487},
	doi = {10.1145/3478513.3480487},
	abstract = {Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this "hyper-space". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between "moments", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1\% for interpolation and 8.6\% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at hypernerf.github.io.},
	number = {6},
	urldate = {2023-06-24},
	journal = {ACM Transactions on Graphics},
	author = {Park, Keunhong and Sinha, Utkarsh and Hedman, Peter and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Martin-Brualla, Ricardo and Seitz, Steven M.},
	year = {2021},
	keywords = {3D synthesis, dynamic scenes, neural radiance fields, neural rendering, novel view synthesis},
	pages = {238:1--238:12},
}

@inproceedings{li_dynibar_2023,
	title = {{DynIBaR}: {Neural} {Dynamic} {Image}-{Based} {Rendering}},
	shorttitle = {{DynIBaR}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-05},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Zhengqi and Wang, Qianqian and Cole, Forrester and Tucker, Richard and Snavely, Noah},
	year = {2023},
	pages = {4273--4284},
}

@misc{jiang_instantavatar_2022,
	title = {{InstantAvatar}: {Learning} {Avatars} from {Monocular} {Video} in 60 {Seconds}},
	shorttitle = {{InstantAvatar}},
	url = {http://arxiv.org/abs/2212.10550},
	doi = {10.48550/arXiv.2212.10550},
	abstract = {In this paper, we take a significant step towards real-world applicability of monocular neural avatar reconstruction by contributing InstantAvatar, a system that can reconstruct human avatars from a monocular video within seconds, and these avatars can be animated and rendered at an interactive rate. To achieve this efficiency we propose a carefully designed and engineered system, that leverages emerging acceleration structures for neural fields, in combination with an efficient empty space-skipping strategy for dynamic scenes. We also contribute an efficient implementation that we will make available for research purposes. Compared to existing methods, InstantAvatar converges 130x faster and can be trained in minutes instead of hours. It achieves comparable or even better reconstruction quality and novel pose synthesis results. When given the same time budget, our method significantly outperforms SoTA methods. InstantAvatar can yield acceptable visual quality in as little as 10 seconds training time.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Jiang, Tianjian and Chen, Xu and Song, Jie and Hilliges, Otmar},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10550 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{wen_bundlesdf_2023,
	title = {{BundleSDF}: {Neural} 6-{DoF} {Tracking} and {3D} {Reconstruction} of {Unknown} {Objects}},
	shorttitle = {{BundleSDF}},
	url = {http://arxiv.org/abs/2303.14158},
	doi = {10.48550/arXiv.2303.14158},
	abstract = {We present a near real-time method for 6-DoF tracking of an unknown object from a monocular RGBD video sequence, while simultaneously performing neural 3D reconstruction of the object. Our method works for arbitrary rigid objects, even when visual texture is largely absent. The object is assumed to be segmented in the first frame only. No additional information is required, and no assumption is made about the interaction agent. Key to our method is a Neural Object Field that is learned concurrently with a pose graph optimization process in order to robustly accumulate information into a consistent 3D representation capturing both geometry and appearance. A dynamic pool of posed memory frames is automatically maintained to facilitate communication between these threads. Our approach handles challenging sequences with large pose changes, partial and full occlusion, untextured surfaces, and specular highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets, demonstrating that our method significantly outperforms existing approaches. Project page: https://bundlesdf.github.io},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Wen, Bowen and Tremblay, Jonathan and Blukis, Valts and Tyree, Stephen and Muller, Thomas and Evans, Alex and Fox, Dieter and Kautz, Jan and Birchfield, Stan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14158 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{wang_normalized_2019,
	title = {Normalized {Object} {Coordinate} {Space} for {Category}-{Level} {6D} {Object} {Pose} and {Size} {Estimation}},
	url = {http://arxiv.org/abs/1901.02970},
	doi = {10.48550/arXiv.1901.02970},
	abstract = {The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to "instance-level" 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce a Normalized Object Coordinate Space (NOCS)---a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J.},
	month = jun,
	year = {2019},
	note = {arXiv:1901.02970 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhu_latitude_2022,
	title = {{LATITUDE}: {Robotic} {Global} {Localization} with {Truncated} {Dynamic} {Low}-pass {Filter} in {City}-scale {NeRF}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	shorttitle = {{LATITUDE}},
	url = {http://arxiv.org/abs/2209.08498},
	abstract = {Neural Radiance Fields (NeRFs) have made great success in representing complex 3D scenes with high-resolution details and efficient memory. Nevertheless, current NeRF-based pose estimators have no initial pose prediction and are prone to local optima during optimization. In this paper, we present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter, which introduces a two-stage localization mechanism in city-scale NeRF. In place recognition stage, we train a regressor through images generated from trained NeRFs, which provides an initial value for global localization. In pose optimization stage, we minimize the residual between the observed image and rendered image by directly optimizing the pose on tangent plane. To avoid convergence to local optimum, we introduce a Truncated Dynamic Low-pass Filter (TDLF) for coarse-to-fine pose registration. We evaluate our method on both synthetic and real-world data and show its potential applications for high-precision navigation in large-scale city scenes. Codes and data will be publicly available at https://github.com/jike5/LATITUDE.},
	urldate = {2022-10-05},
	booktitle = {2023 {IEEE} {Conference} on {Robotics} and {Automation} ({ICRA} 2023)},
	publisher = {arXiv},
	author = {Zhu, Zhenxin and Chen, Yuantao and Wu, Zirui and Hou, Chao and Shi, Yongliang and Li, Chuxuan and Li, Pengfei and Zhao, Hao and Zhou, Guyue},
	month = sep,
	year = {2022},
	note = {arXiv:2209.08498 [cs]},
	keywords = {2022, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Done Reading, ICRA, My, NeRF, Simultaneous localization and mapping, ⛔ No INSPIRE recid found},
}

@inproceedings{sawdayee_orex_2023,
	title = {{OReX}: {Object} {Reconstruction} {From} {Planar} {Cross}-{Sections} {Using} {Neural} {Fields}},
	shorttitle = {{OReX}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Sawdayee_OReX_Object_Reconstruction_From_Planar_Cross-Sections_Using_Neural_Fields_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-05},
	author = {Sawdayee, Haim and Vaxman, Amir and Bermano, Amit H.},
	year = {2023},
	pages = {20854--20862},
}

@misc{masuda_event-based_2023,
	title = {Event-based {Camera} {Tracker} by \${\textbackslash}nabla\$t {NeRF}},
	url = {http://arxiv.org/abs/2304.04559},
	abstract = {When a camera travels across a 3D world, only a fraction of pixel value changes; an event-based camera observes the change as sparse events. How can we utilize sparse events for efﬁcient recovery of the camera pose? We show that we can recover the camera pose by minimizing the error between sparse events and the temporal gradient of the scene represented as a neural radiance ﬁeld (NeRF). To enable the computation of the temporal gradient of the scene, we augment NeRF’s camera pose as a time function. When the input pose to the NeRF coincides with the actual pose, the output of the temporal gradient of NeRF equals the observed intensity changes on the event’s points. Using this principle, we propose an event-based camera pose tracking framework called TeGRA which realizes the pose update by using the sparse event’s observation. To the best of our knowledge, this is the ﬁrst camera pose estimation algorithm using the scene’s implicit representation and the sparse intensity change from events.},
	language = {en},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Masuda, Mana and Sekikawa, Yusuke and Saito, Hideo},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04559 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{pavllo_shape_2023,
	title = {Shape, {Pose}, and {Appearance} {From} a {Single} {Image} via {Bootstrapped} {Radiance} {Field} {Inversion}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Pavllo_Shape_Pose_and_Appearance_From_a_Single_Image_via_Bootstrapped_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-05},
	author = {Pavllo, Dario and Tan, David Joseph and Rakotosaona, Marie-Julie and Tombari, Federico},
	year = {2023},
	pages = {4391--4401},
}

@inproceedings{yuan_star_2021,
	title = {{STaR}: {Self}-{Supervised} {Tracking} and {Reconstruction} of {Rigid} {Objects} in {Motion} {With} {Neural} {Rendering}},
	shorttitle = {{STaR}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yuan_STaR_Self-Supervised_Tracking_and_Reconstruction_of_Rigid_Objects_in_Motion_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-07-05},
	author = {Yuan, Wentao and Lv, Zhaoyang and Schmidt, Tanner and Lovegrove, Steven},
	year = {2021},
	pages = {13144--13152},
}

@misc{guo_neural_2023,
	title = {Neural {3D} {Scene} {Reconstruction} from {Multi}-view {Images} without {3D} {Supervision}},
	url = {http://arxiv.org/abs/2306.17643},
	doi = {10.48550/arXiv.2306.17643},
	abstract = {Neural scene reconstruction methods have achieved impressive performance in reconstructing complex geometry and low-textured regions in large scenes. However, these methods heavily rely on 3D supervised information which is costly and time-consuming to obtain in the real world. In this paper, we propose a novel neural reconstruction method that reconstructs scenes without 3D supervision. We perform differentiable volume rendering for scene reconstruction by using accessible 2D images as supervision. We impose geometry to improve the reconstruction quality of complex geometry regions in the scenes, and impose plane constraints to improve the reconstruction quality of low-textured regions in the scenes. Specifically, we introduce a signed distance function (SDF) field, a color field, and a probability field to represent the scene, and optimize the fields under the differentiable ray marching to reconstruct the scene. Besides, we impose geometric constraints that project 3D points on the surface to similar-looking regions with similar features in different views. We also impose plane constraints to make large planes keep parallel or vertical to the wall or floor. These two constraints help to reconstruct accurate and smooth geometry structures of the scene. Without 3D supervision information, our method achieves competitive reconstruction compared with some existing methods that use 3D information as supervision on the ScanNet dataset.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Guo, Yi and Sun, Che and Jia, Yunde and Wu, Yuwei},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17643 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{siddiqui_panoptic_2023,
	title = {Panoptic {Lifting} for {3D} {Scene} {Understanding} {With} {Neural} {Fields}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Siddiqui, Yawar and Porzi, Lorenzo and Bulò, Samuel Rota and Müller, Norman and Nießner, Matthias and Dai, Angela and Kontschieder, Peter},
	year = {2023},
	pages = {9043--9052},
}

@inproceedings{deng_depth-supervised_2022,
	title = {Depth-supervised {NeRF}: {Fewer} {Views} and {Faster} {Training} for {Free}},
	shorttitle = {Depth-supervised {NeRF}},
	url = {https://ieeexplore.ieee.org/document/9880067/},
	doi = {10/gq54wm},
	abstract = {A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as “free” depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Deng, Kangle and Liu, Andrew and Zhu, Jun-Yan and Ramanan, Deva},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Geometry, Pipelines, RGBD sensors and analytics, Rendering (computer graphics), Surface fitting, Three-dimensional displays, Training, Uncertainty, Vision + graphics, ⛔ No INSPIRE recid found},
	pages = {12872--12881},
}

@inproceedings{yu_monosdf_2022,
	title = {{MonoSDF}: {Exploring} {Monocular} {Geometric} {Cues} for {Neural} {Implicit} {Surface} {Reconstruction}},
	shorttitle = {{MonoSDF}},
	url = {https://openreview.net/forum?id=dMK7EwoTYp},
	abstract = {In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yu, Zehao and Peng, Songyou and Niemeyer, Michael and Sattler, Torsten and Geiger, Andreas},
	month = oct,
	year = {2022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Czech Technical University, Done Reading, ETH, Max Planck, Monocular depth estimate, NIPS 2022, Shared, University of Tubingen, normal prior, ⛔ No INSPIRE recid found},
}

@inproceedings{hu_planning-oriented_2023,
	title = {Planning-{Oriented} {Autonomous} {Driving}},
	shorttitle = {{UniAD}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-26},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Hu, Yihan and Yang, Jiazhi and Chen, Li and Li, Keyu and Sima, Chonghao and Zhu, Xizhou and Chai, Siqi and Du, Senyao and Lin, Tianwei and Wang, Wenhai and Lu, Lewei and Jia, Xiaosong and Liu, Qiang and Dai, Jifeng and Qiao, Yu and Li, Hongyang},
	year = {2023},
	pages = {17853--17862},
}

@inproceedings{li_multi-view_2023,
	title = {Multi-{View} {Inverse} {Rendering} for {Large}-{Scale} {Real}-{World} {Indoor} {Scenes}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_Multi-View_Inverse_Rendering_for_Large-Scale_Real-World_Indoor_Scenes_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-04},
	author = {Li, Zhen and Wang, Lingli and Cheng, Mofang and Pan, Cihui and Yang, Jiaqi},
	year = {2023},
	pages = {12499--12509},
}

@misc{wu_asyncnerf_2022,
	title = {{AsyncNeRF}: {Learning} {Large}-scale {Radiance} {Fields} from {Asynchronous} {RGB}-{D} {Sequences} with {Time}-{Pose} {Function}},
	copyright = {All rights reserved},
	shorttitle = {{AsyncNeRF}},
	url = {http://arxiv.org/abs/2211.07459},
	doi = {10.48550/arXiv.2211.07459},
	abstract = {Large-scale radiance fields are promising mapping tools for smart transportation applications like autonomous driving or drone delivery. But for large-scale scenes, compact synchronized RGB-D cameras are not applicable due to limited sensing range, and using separate RGB and depth sensors inevitably leads to unsynchronized sequences. Inspired by the recent success of self-calibrating radiance field training methods that do not require known intrinsic or extrinsic parameters, we propose the first solution that self-calibrates the mismatch between RGB and depth frames. We leverage the important domain-specific fact that RGB and depth frames are actually sampled from the same trajectory and develop a novel implicit network called the time-pose function. Combining it with a large-scale radiance field leads to an architecture that cascades two implicit representation networks. To validate its effectiveness, we construct a diverse and photorealistic dataset that covers various RGB-D mismatch scenarios. Through a comprehensive benchmarking on this dataset, we demonstrate the flexibility of our method in different scenarios and superior performance over applicable prior counterparts. Codes, data, and models will be made publicly available.},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Wu, Zirui and Chen, Yuantao and Yang, Runyi and Zhu, Zhenxin and Hou, Chao and Shi, Yongliang and Zhao, Hao and Zhou, Guyue},
	month = nov,
	year = {2022},
	note = {arXiv:2211.07459 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Done Reading, ⛔ No INSPIRE recid found},
}

@inproceedings{xiu_econ_2023,
	title = {{ECON}: {Explicit} {Clothed} {Humans} {Optimized} via {Normal} {Integration}},
	shorttitle = {{ECON}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Xiu_ECON_Explicit_Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-03},
	author = {Xiu, Yuliang and Yang, Jinlong and Cao, Xu and Tzionas, Dimitrios and Black, Michael J.},
	year = {2023},
	pages = {512--523},
}

@inproceedings{wei_nerfingmvs_2021,
	title = {{NerfingMVS}: {Guided} {Optimization} of {Neural} {Radiance} {Fields} for {Indoor} {Multi}-{View} {Stereo}},
	shorttitle = {{NerfingMVS}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Wei_NerfingMVS_Guided_Optimization_of_Neural_Radiance_Fields_for_Indoor_Multi-View_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-07-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Wei, Yi and Liu, Shaohui and Rao, Yongming and Zhao, Wang and Lu, Jiwen and Zhou, Jie},
	year = {2021},
	pages = {5610--5619},
}

@article{colombari_segmentation_2007,
	title = {Segmentation and tracking of multiple video objects},
	volume = {40},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320306003402},
	doi = {10.1016/j.patcog.2006.07.008},
	abstract = {This paper describes a technique that produces a content-based representation of a video shot composed by a background (still) mosaic and one or more foreground moving objects. Segmentation of moving objects is based on ego-motion compensation and on background modelling using tools from robust statistics. Region matching is carried out by an algorithm that operates on the Mahalanobis distance between region descriptors in two subsequent frames and uses singular value decomposition to compute a set of correspondences satisfying both the principle of proximity and the principle of exclusion. The sequence is represented as a layered graph, and specific techniques are introduced to cope with crossing and occlusion. Examples of MPEG-4 (main profile) encoding are reported.},
	language = {en},
	number = {4},
	urldate = {2023-07-04},
	journal = {Pattern Recognition},
	author = {Colombari, A. and Fusiello, A. and Murino, V.},
	month = apr,
	year = {2007},
	keywords = {Content-based representation, MPEG, Mosaicing, Motion segmentation, Video coding, Video sequence analysis},
	pages = {1307--1317},
}

@misc{li_toist_2022,
	title = {{TOIST}: {Task} {Oriented} {Instance} {Segmentation} {Transformer} with {Noun}-{Pronoun} {Distillation}},
	shorttitle = {{TOIST}},
	url = {http://arxiv.org/abs/2210.10775},
	doi = {10.48550/arXiv.2210.10775},
	abstract = {Current referring expression comprehension algorithms can effectively detect or segment objects indicated by nouns, but how to understand verb reference is still under-explored. As such, we study the challenging problem of task oriented detection, which aims to find objects that best afford an action indicated by verbs like sit comfortably on. Towards a finer localization that better serves downstream applications like robot interaction, we extend the problem into task oriented instance segmentation. A unique requirement of this task is to select preferred candidates among possible alternatives. Thus we resort to the transformer architecture which naturally models pair-wise query relationships with attention, leading to the TOIST method. In order to leverage pre-trained noun referring expression comprehension models and the fact that we can access privileged noun ground truth during training, a novel noun-pronoun distillation framework is proposed. Noun prototypes are generated in an unsupervised manner and contextual pronoun features are trained to select prototypes. As such, the network remains noun-agnostic during inference. We evaluate TOIST on the large-scale task oriented dataset COCO-Tasks and achieve +10.9\% higher \${\textbackslash}rm\{mAP{\textasciicircum}\{box\}\}\$ than the best-reported results. The proposed noun-pronoun distillation can boost \${\textbackslash}rm\{mAP{\textasciicircum}\{box\}\}\$ and \${\textbackslash}rm\{mAP{\textasciicircum}\{mask\}\}\$ by +2.8\% and +3.8\%. Codes and models are publicly available at https://github.com/AIR-DISCOVER/TOIST.},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Li, Pengfei and Tian, Beiwen and Shi, Yongliang and Chen, Xiaoxue and Zhao, Hao and Zhou, Guyue and Zhang, Ya-Qin},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10775 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xiu_econ_2023-1,
	title = {{ECON}: {Explicit} {Clothed} humans {Optimized} via {Normal} integration},
	shorttitle = {{ECON}},
	url = {http://arxiv.org/abs/2212.07422},
	doi = {10.48550/arXiv.2212.07422},
	abstract = {The combination of deep learning, artist-curated scans, and Implicit Functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry, but produce disembodied limbs or degenerate shapes for novel poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit representation and explicit body regularization. To this end, we make two key observations: (1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a "canvas" for stitching together detailed surface patches. Based on these, our method, ECON, has three main steps: (1) It infers detailed 2D normal maps for the front and back side of a clothed person. (2) From these, it recovers 2.5D front and back surfaces, called d-BiNI, that are equally detailed, yet incomplete, and registers these w.r.t. each other with the help of a SMPL-X body mesh recovered from the image. (3) It "inpaints" the missing geometry between d-BiNI surfaces. If the face and hands are noisy, they can optionally be replaced with the ones of SMPL-X. As a result, ECON infers high-fidelity 3D humans even in loose clothes and challenging poses. This goes beyond previous methods, according to the quantitative evaluation on the CAPE and Renderpeople datasets. Perceptual studies also show that ECON's perceived realism is better by a large margin. Code and models are available for research purposes at econ.is.tue.mpg.de},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Xiu, Yuliang and Yang, Jinlong and Cao, Xu and Tzionas, Dimitrios and Black, Michael J.},
	month = mar,
	year = {2023},
	note = {arXiv:2212.07422 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{qian_magic123_2023,
	title = {Magic123: {One} {Image} to {High}-{Quality} {3D} {Object} {Generation} {Using} {Both} {2D} and {3D} {Diffusion} {Priors}},
	shorttitle = {Magic123},
	url = {http://arxiv.org/abs/2306.17843},
	doi = {10.48550/arXiv.2306.17843},
	abstract = {We present Magic123, a two-stage coarse-to-fine approach for high-quality, textured 3D meshes generation from a single unposed image in the wild using both2D and 3D priors. In the first stage, we optimize a neural radiance field to produce a coarse geometry. In the second stage, we adopt a memory-efficient differentiable mesh representation to yield a high-resolution mesh with a visually appealing texture. In both stages, the 3D content is learned through reference view supervision and novel views guided by a combination of 2D and 3D diffusion priors. We introduce a single trade-off parameter between the 2D and 3D priors to control exploration (more imaginative) and exploitation (more precise) of the generated geometry. Additionally, we employ textual inversion and monocular depth regularization to encourage consistent appearances across views and to prevent degenerate solutions, respectively. Magic123 demonstrates a significant improvement over previous image-to-3D techniques, as validated through extensive experiments on synthetic benchmarks and diverse real-world images. Our code, models, and generated 3D assets are available at https://github.com/guochengqian/Magic123.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Qian, Guocheng and Mai, Jinjie and Hamdi, Abdullah and Ren, Jian and Siarohin, Aliaksandr and Li, Bing and Lee, Hsin-Ying and Skorokhodov, Ivan and Wonka, Peter and Tulyakov, Sergey and Ghanem, Bernard},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17843 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cho_implicit_2023,
	title = {Implicit {3D} {Human} {Mesh} {Recovery} using {Consistency} with {Pose} and {Shape} from {Unseen}-view},
	url = {http://arxiv.org/abs/2306.17651},
	doi = {10.48550/arXiv.2306.17651},
	abstract = {From an image of a person, we can easily infer the natural 3D pose and shape of the person even if ambiguity exists. This is because we have a mental model that allows us to imagine a person's appearance at different viewing directions from a given image and utilize the consistency between them for inference. However, existing human mesh recovery methods only consider the direction in which the image was taken due to their structural limitations. Hence, we propose "Implicit 3D Human Mesh Recovery (ImpHMR)" that can implicitly imagine a person in 3D space at the feature-level via Neural Feature Fields. In ImpHMR, feature fields are generated by CNN-based image encoder for a given image. Then, the 2D feature map is volume-rendered from the feature field for a given viewing direction, and the pose and shape parameters are regressed from the feature. To utilize consistency with pose and shape from unseen-view, if there are 3D labels, the model predicts results including the silhouette from an arbitrary direction and makes it equal to the rotated ground-truth. In the case of only 2D labels, we perform self-supervised learning through the constraint that the pose and shape parameters inferred from different directions should be the same. Extensive evaluations show the efficacy of the proposed method.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Cho, Hanbyel and Cho, Yooshin and Ahn, Jaesung and Kim, Junmo},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17651 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{seo_flipnerf_2023,
	title = {{FlipNeRF}: {Flipped} {Reflection} {Rays} for {Few}-shot {Novel} {View} {Synthesis}},
	shorttitle = {{FlipNeRF}},
	url = {http://arxiv.org/abs/2306.17723},
	doi = {10.48550/arXiv.2306.17723},
	abstract = {Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis with its remarkable quality of rendered images and simple architecture. Although NeRF has been developed in various directions improving continuously its performance, the necessity of a dense set of multi-view images still exists as a stumbling block to progress for practical application. In this work, we propose FlipNeRF, a novel regularization method for few-shot novel view synthesis by utilizing our proposed flipped reflection rays. The flipped reflection rays are explicitly derived from the input ray directions and estimated normal vectors, and play a role of effective additional training rays while enabling to estimate more accurate surface normals and learn the 3D geometry effectively. Since the surface normal and the scene depth are both derived from the estimated densities along a ray, the accurate surface normal leads to more exact depth estimation, which is a key factor for few-shot novel view synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss and Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more reliable outputs with reducing floating artifacts effectively across the different scene structures, and enhance the feature-level consistency between the pair of the rays cast toward the photo-consistent pixels without any additional feature extractor, respectively. Our FlipNeRF achieves the SOTA performance on the multiple benchmarks across all the scenarios.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Seo, Seunghyeon and Chang, Yeonjin and Kwak, Nojun},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17723 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wang_dm-nerf_2023,
	title = {{DM}-{NeRF}: {3D} {Scene} {Geometry} {Decomposition} and {Manipulation} from {2D} {Images}},
	shorttitle = {{DM}-{NeRF}},
	url = {https://github.com/vLAR-group/DM-NeRF},
	abstract = {In this paper, we study the problem of 3D scene geometry decomposition and manipulation from 2D views. By leveraging the recent implicit neural representation techniques, particularly the appealing neural radiance fields, we introduce an object field component to learn unique codes for all individual objects in 3D space only from 2D supervision. The key to this component is a series of carefully designed loss functions to enable every 3D point, especially in non-occupied space, to be effectively optimized even without 3D labels. In addition, we introduce an inverse query algorithm to freely manipulate any specified 3D object shape in the learned scene representation. Notably, our manipulation algorithm can explicitly tackle key issues such as object collisions and visual occlusions. Our method, called DM-NeRF, is among the first to simultaneously reconstruct, decompose, manipulate and render complex 3D scenes in a single pipeline. Extensive experiments on three datasets clearly show that our method can accurately decompose all 3D objects from 2D views, allowing any interested object to be freely manipulated in 3D space such as translation, rotation, size adjustment, and deformation.},
	urldate = {2023-07-02},
	booktitle = {International {Conference} {On} {Learning} {Representations}},
	author = {Wang, Bing and Chen, Lu and Yang, Bo},
	month = mar,
	year = {2023},
	note = {arXiv:2208.07227 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{kirillov_panoptic_2019,
	title = {Panoptic {Segmentation}},
	url = {http://arxiv.org/abs/1801.00868},
	abstract = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
	month = apr,
	year = {2019},
	note = {arXiv:1801.00868 [cs]
titleTranslation:},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yu_dair-v2x_2022,
	title = {{DAIR}-{V2X}: {A} {Large}-{Scale} {Dataset} for {Vehicle}-{Infrastructure} {Cooperative} {3D} {Object} {Detection}},
	shorttitle = {{DAIR}-{V2X}},
	url = {http://arxiv.org/abs/2204.05575},
	abstract = {Autonomous driving faces great safety challenges for a lack of global perspective and the limitation of long-range perception capabilities. It has been widely agreed that vehicle-infrastructure cooperation is required to achieve Level 5 autonomy. However, there is still NO dataset from real scenarios available for computer vision researchers to work on vehicle-infrastructure cooperation-related problems. To accelerate computer vision research and innovation for Vehicle-Infrastructure Cooperative Autonomous Driving (VICAD), we release DAIR-V2X Dataset, which is the first large-scale, multi-modality, multi-view dataset from real scenarios for VICAD. DAIR-V2X comprises 71254 LiDAR frames and 71254 Camera frames, and all frames are captured from real scenes with 3D annotations. The Vehicle-Infrastructure Cooperative 3D Object Detection problem (VIC3D) is introduced, formulating the problem of collaboratively locating and identifying 3D objects using sensory inputs from both vehicle and infrastructure. In addition to solving traditional 3D object detection problems, the solution of VIC3D needs to consider the temporal asynchrony problem between vehicle and infrastructure sensors and the data transmission cost between them. Furthermore, we propose Time Compensation Late Fusion (TCLF), a late fusion framework for the VIC3D task as a benchmark based on DAIR-V2X. Find data, code, and more up-to-date information at https://thudair.baai.ac.cn/index and https://github.com/AIR-THU/DAIR-V2X.},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Yu, Haibao and Luo, Yizhen and Shu, Mao and Huo, Yiyi and Yang, Zebang and Shi, Yifeng and Guo, Zhenglong and Li, Hanyu and Hu, Xing and Yuan, Jirui and Nie, Zaiqing},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05575 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{kuang_ir-mcl_2023,
	title = {{IR}-{MCL}: {Implicit} {Representation}-{Based} {Online} {Global} {Localization}},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{IR}-{MCL}},
	url = {http://arxiv.org/abs/2210.03113},
	doi = {10/gr8wgm},
	abstract = {Determining the state of a mobile robot is an essential building block of robot navigation systems. In this paper, we address the problem of estimating the robots pose in an indoor environment using 2D LiDAR data and investigate how modern environment models can improve gold standard Monte-Carlo localization (MCL) systems. We propose a neural occupancy field to implicitly represent the scene using a neural network. With the pretrained network, we can synthesize 2D LiDAR scans for an arbitrary robot pose through volume rendering. Based on the implicit representation, we can obtain the similarity between a synthesized and actual scan as an observation model and integrate it into an MCL system to perform accurate localization. We evaluate our approach on self-recorded datasets and three publicly available ones. We show that we can accurately and efficiently localize a robot using our approach surpassing the localization performance of state-of-the-art methods. The experiments suggest that the presented implicit representation is able to predict more accurate 2D LiDAR scans leading to an improved observation model for our particle filter-based localization. The code of our approach will be available at: https://github.com/PRBonn/ir-mcl.},
	urldate = {2023-02-07},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kuang, Haofei and Chen, Xieyuanli and Guadagnino, Tiziano and Zimmerman, Nicky and Behley, Jens and Stachniss, Cyrill},
	year = {2023},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	pages = {1--8},
}

@inproceedings{li_simultaneous_2015,
	title = {Simultaneous video defogging and stereo reconstruction},
	doi = {10/gr8wg8},
	abstract = {We present a method to jointly estimate scene depth and recover the clear latent image from a foggy video sequence. In our formulation, the depth cues from stereo matching and fog information reinforce each other, and produce superior results than conventional stereo or defogging algorithms. We first improve the photo-consistency term to explicitly model the appearance change due to the scattering effects. The prior matting Laplacian constraint on fog transmission imposes a detail-preserving smoothness constraint on the scene depth. We further enforce the ordering consistency between scene depth and fog transmission at neighboring points. These novel constraints are formulated together in an MRF framework, which is optimized iteratively by introducing auxiliary variables. The experiment results on real videos demonstrate the strength of our method.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Li, Zhuwen and Tan, Ping and Tan, Robby T. and Zou, Danping and Zhou, Steven Zhiying and Cheong, Loong-Fah},
	month = jun,
	year = {2015},
	keywords = {/unread, Cameras, Coherence, Image color analysis, Laplace equations, Mathematical model, Media},
	pages = {4988--4997},
}

@article{stotko_slamcast_2019,
	title = {{SLAMCast}: {Large}-{Scale}, {Real}-{Time} {3D} {Reconstruction} and {Streaming} for {Immersive} {Multi}-{Client} {Live} {Telepresence}},
	volume = {25},
	issn = {1941-0506},
	shorttitle = {{SLAMCast}},
	doi = {10/gr8wg9},
	abstract = {Real-time 3D scene reconstruction from RGB-D sensor data, as well as the exploration of such data in VR/AR settings, has seen tremendous progress in recent years. The combination of both these components into telepresence systems, however, comes with significant technical challenges. All approaches proposed so far are extremely demanding on input and output devices, compute resources and transmission bandwidth, and they do not reach the level of immediacy required for applications such as remote collaboration. Here, we introduce what we believe is the first practical client-server system for real-time capture and many-user exploration of static 3D scenes. Our system is based on the observation that interactive frame rates are sufficient for capturing and reconstruction, and real-time performance is only required on the client site to achieve lag-free view updates when rendering the 3D model. Starting from this insight, we extend previous voxel block hashing frameworks by introducing a novel thread-safe GPU hash map data structure that is robust under massively concurrent retrieval, insertion and removal of entries on a thread level. We further propose a novel transmission scheme for volume data that is specifically targeted to Marching Cubes geometry reconstruction and enables a 90\% reduction in bandwidth between server and exploration clients. The resulting system poses very moderate requirements on network bandwidth, latency and client-side computation, which enables it to rely entirely on consumer-grade hardware, including mobile devices. We demonstrate that our technique achieves state-of-the-art representation accuracy while providing, for any number of clients, an immersive and fluid lag-free viewing experience even during network outages.},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Stotko, Patrick and Krumpen, Stefan and Hullin, Matthias B. and Weinmann, Michael and Klein, Reinhard},
	month = may,
	year = {2019},
	keywords = {/unread, Bandwidth, Collaboration, Hardware, RGB-D, Real-time systems, Remote collaboration, Servers, Telepresence, Three-dimensional displays, live telepresence, real-time reconstruction, real-time streaming, voxel hashing},
	pages = {2102--2112},
}

@article{ren_deep_2019,
	title = {Deep {Video} {Dehazing} {With} {Semantic} {Segmentation}},
	volume = {28},
	issn = {1057-7149, 1941-0042},
	url = {https://ieeexplore.ieee.org/document/8492451/},
	doi = {10/gfxj6z},
	abstract = {Recent research have shown the potential of using convolutional neural networks (CNNs) to accomplish single image dehazing. In this paper, we take one step further to explore the possibility of exploiting a network to perform haze removal for videos. Unlike single image dehazing, video-based approaches can take advantage of the abundant information that exists across neighboring frames. In this paper, assuming that a scene point yields highly correlated transmission values between adjacent video frames, we develop a deep learning solution for video dehazing, where a CNN is trained end-to-end to learn how to accumulate information across frames for transmission estimation. The estimated transmission map is subsequently used to recover a haze-free frame via atmospheric scattering model. In addition, as the semantic information of a scene provides a strong prior for image restoration, we propose to incorporate global semantic priors as input to regularize the transmission maps so that the estimated maps can be smooth in the regions of the same object and only discontinuous across the boundaries of different objects. To train this network, we generate a dataset consisted of synthetic hazy and haze-free videos for supervision based on the NYU depth dataset. We show that the features learned from this dataset are capable of removing haze that arises in outdoor scenes in a wide range of videos. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world videos.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Ren, Wenqi and Zhang, Jingang and Xu, Xiangyu and Ma, Lin and Cao, Xiaochun and Meng, Gaofeng and Liu, Wei},
	month = apr,
	year = {2019},
	keywords = {/unread, Atmospheric modeling, Coherence, Convolutional neural networks, Image color analysis, Image segmentation, Semantics, Task analysis, Video dehazing, convolutional neural network, defogging, transmission map},
	pages = {1895--1908},
}

@inproceedings{hertz_sape_2021,
	title = {{SAPE}: {Spatially}-{Adaptive} {Progressive} {Encoding} for {Neural} {Optimization}},
	shorttitle = {{SAPE}},
	url = {https://openreview.net/forum?id=0wqGMmfqBw},
	abstract = {Multilayer-perceptrons (MLP) are known to struggle learning functions of high-frequencies, and in particular, instances of wide frequency bands. We present a progressive mapping scheme for input signals of MLP networks, enabling them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. We introduce Spatially Adaptive Progressive Encoding (SAPE) layers, which gradually unmask signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of our method on variety of domains and applications: regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.},
	language = {en},
	urldate = {2023-05-01},
	author = {Hertz, Amir and Perel, Or and Giryes, Raja and Sorkine-hornung, Olga and Cohen-or, Daniel},
	month = oct,
	year = {2021},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@article{yuan_neural_2022,
	title = {Neural {Radiance} {Fields} from {Sparse} {RGB}-{D} {Images} for {High}-{Quality} {View} {Synthesis}},
	issn = {1939-3539},
	doi = {10/gr8wgw},
	abstract = {The recently proposed neural radiance fields (NeRF) use a continuous function formulated as a multi-layer perceptron (MLP) to model the appearance and geometry of a 3D scene. This enables realistic synthesis of novel views, even for scenes with view dependent appearance. Many follow-up works have since extended NeRFs in different ways. However, a fundamental restriction of the method remains that it requires a large number of images captured from densely placed viewpoints for high-quality synthesis and the quality of the results quickly degrades when the number of captured views is insufficient. To address this problem, we propose a novel NeRF-based framework capable of high-quality view synthesis using only a sparse set of RGB-D images, which can be easily captured using cameras and LiDAR sensors on current consumer devices. First, a geometric proxy of the scene is reconstructed from the captured RGB-D images. Renderings of the reconstructed scene along with precise camera parameters can then be used to pre-train a network. Finally, the network is fine-tuned with a small number of real captured images. We further introduce a patch discriminator to supervise the network under novel views during fine-tuning, as well as a 3D color prior to improve synthesis quality. We demonstrate that our method can generate arbitrary novel views of a 3D scene from as few as 6 RGB-D images. Extensive experiments show the improvements of our method compared with the existing NeRF-based methods, including approaches that also aim to reduce the number of input images.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Yuan, Yu-Jie and Lai, Yu-Kun and Huang, Yi-Hua and Kobbelt, Leif and Gao, Lin},
	year = {2022},
	keywords = {/unread, Cameras, Geometry, Image reconstruction, Neural Radiance Fields, Neural Rendering, Novel View Synthesis, Rendering (computer graphics), Task analysis, Three-dimensional displays, Training, ⛔ No INSPIRE recid found},
	pages = {1--16},
}

@article{bernhard_3d_2023,
	title = {{3D} {Gaussian} {Splatting} for {Real}-{Time} {Radiance} {Field} {Rendering}},
	journal = {ACM Transactions on Graphics},
	author = {Bernhard, Kerbl and Georgios, Kopanas and Thomas, Leimkühler and George, Drettakis},
	month = may,
	year = {2023},
	keywords = {/unread, ⛔ No DOI found},
}

@article{xie_neural_2022,
	title = {Neural {Fields} in {Visual} {Computing} and {Beyond}},
	volume = {41},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14505},
	doi = {10/gr8wgv},
	language = {en},
	number = {2},
	urldate = {2023-05-02},
	journal = {Computer Graphics Forum},
	author = {Xie, Yiheng and Takikawa, Towaki and Saito, Shunsuke and Litany, Or and Yan, Shiqin and Khan, Numair and Tombari, Federico and Tompkin, James and Sitzmann, Vincent and Sridhar, Srinath},
	month = may,
	year = {2022},
	keywords = {/unread},
	pages = {641--676},
}

@article{tavanaei_deep_2019,
	title = {Deep learning in spiking neural networks},
	volume = {111},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608018303332},
	doi = {10/gfv25r},
	abstract = {In recent years, deep learning has been a revolution in the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained in a supervised manner using backpropagation. Huge amounts of labeled examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and arguably the only viable option if one wants to understand how the brain computes. SNNs are also more hardware friendly and energy-efficient than ANNs, and are thus appealing for technology, especially for portable devices. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy, but also computational cost and hardware friendliness. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while the SNNs typically require much fewer operations.},
	urldate = {2023-05-02},
	journal = {Neural Networks},
	author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timothée and Maida, Anthony},
	month = mar,
	year = {2019},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	pages = {47--63},
}

@article{gortler_visual_2019,
	title = {A {Visual} {Exploration} of {Gaussian} {Processes}},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/visual-exploration-gaussian-processes},
	doi = {10/gf8nrv},
	abstract = {How to turn a collection of small building blocks into a versatile tool for solving regression problems.},
	language = {en},
	number = {4},
	urldate = {2023-04-20},
	journal = {Distill},
	author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
	month = apr,
	year = {2019},
	keywords = {/unread},
	pages = {10.23915/distill.00017},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10/c4wch5},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	language = {en},
	number = {5},
	urldate = {2023-04-07},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {/unread, Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
}

@article{song_simultaneous_2020,
	title = {Simultaneous {Deep} {Stereo} {Matching} and {Dehazing} with {Feature} {Attention}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-020-01294-2},
	doi = {10/gjgv62},
	abstract = {Unveiling the dense correspondence under the haze layer remains a challenging task, since the scattering effects result in less distinctive image features. Contrarily, dehazing is often confused by the airlight-albedo ambiguity which cannot be resolved independently at each pixel. In this paper, we introduce a deep convolutional neural network that simultaneously estimates a disparity and clear image from a hazy stereo image pair. Both tasks are synergistically formulated by fusing depth information from the matching cost and haze transmission. To learn the optimal fusion of depth-related features, we present a novel encoder-decoder architecture that extends the core idea of attention mechanism to the simultaneous stereo matching and dehazing. As a result, our method estimates high-quality disparity for the stereo images in scattering media, and produces appearance images with enhanced visibility. Finally, we further propose an effective strategy for adaptation to camera-captured images by distilling the cross-domain knowledge. Experiments on both synthetic and real-world scenarios including comparisons with state-of-the-art methods demonstrate the effectiveness and flexibility of our approach.},
	language = {en},
	number = {4},
	urldate = {2023-03-07},
	journal = {International Journal of Computer Vision},
	author = {Song, Taeyong and Kim, Youngjung and Oh, Changjae and Jang, Hyunsung and Ha, Namkoo and Sohn, Kwanghoon},
	month = apr,
	year = {2020},
	keywords = {/unread},
	pages = {799--817},
}

@inproceedings{satat_towards_2018,
	title = {Towards photography through realistic fog},
	doi = {10/gm5sxw},
	abstract = {Imaging through fog has important applications in industries such as self-driving cars, augmented driving, airplanes, helicopters, drones and trains. Here we show that time profiles of light reflected from fog have a distribution (Gamma) that is different from light reflected from objects occluded by fog (Gaussian). This helps to distinguish between background photons reflected from the fog and signal photons reflected from the occluded object. Based on this observation, we recover reflectance and depth of a scene obstructed by dense, dynamic, and heterogeneous fog. For practical use cases, the imaging system is designed in optical reflection mode with minimal footprint and is based on LIDAR hardware. Specifically, we use a single photon avalanche diode (SPAD) camera that time-tags individual detected photons. A probabilistic computational framework is developed to estimate the fog properties from the measurement itself, without prior knowledge. Other solutions are based on radar that suffers from poor resolution (due to the long wavelength), or on time gating that suffers from low signal-to-noise ratio. The suggested technique is experimentally evaluated in a wide range of fog densities created in a fog chamber It demonstrates recovering objects 57cm away from the camera when the visibility is 37cm. In that case it recovers depth with a resolution of 5cm and scene reflectance with an improvement of 4dB in PSNR and 3.4× reconstruction quality in SSIM over time gating techniques.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Computational} {Photography} ({ICCP})},
	author = {Satat, Guy and Tancik, Matthew and Raskar, Ramesh},
	month = may,
	year = {2018},
	keywords = {/unread, Cameras, Estimation, Media, Photonics, Scattering, Signal to noise ratio},
	pages = {1--10},
}

@article{heide_imaging_2014,
	title = {Imaging in scattering media using correlation image sensors and sparse convolutional coding},
	volume = {22},
	copyright = {© 2014 Optical Society of America},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-22-21-26338},
	doi = {10/gfz5mc},
	abstract = {Correlation image sensors have recently become popular low-cost devices for time-of-flight, or range cameras. They usually operate under the assumption of a single light path contributing to each pixel. We show that a more thorough analysis of the sensor data from correlation sensors can be used can be used to analyze the light transport in much more complex environments, including applications for imaging through scattering and turbid media. The key of our method is a new convolutional sparse coding approach for recovering transient (light-in-flight) images from correlation image sensors. This approach is enabled by an analysis of sparsity in complex transient images, and the derivation of a new physically-motivated model for transient images with drastically improved sparsity.},
	language = {EN},
	number = {21},
	urldate = {2023-03-07},
	journal = {Optics Express},
	author = {Heide, Felix and Xiao, Lei and Kolb, Andreas and Hullin, Matthias B. and Heidrich, Wolfgang},
	month = oct,
	year = {2014},
	keywords = {/unread, Femtosecond pulses, Gated imaging, Image sensors, Scattering media, Streak cameras, Underwater imaging},
	pages = {26338--26350},
}

@article{murez_photometric_2017,
	title = {Photometric {Stereo} in a {Scattering} {Medium}},
	volume = {39},
	issn = {1939-3539},
	doi = {10/gbrss5},
	abstract = {Photometric stereo is widely used for 3D reconstruction. However, its use in scattering media such as water, biological tissue and fog has been limited until now, because of forward scattered light from both the source and object, as well as light scattered back from the medium (backscatter). Here we make three contributions to address the key modes of light propagation, under the common single scattering assumption for dilute media. First, we show through extensive simulations that single-scattered light from a source can be approximated by a point light source with a single direction. This alleviates the need to handle light source blur explicitly. Next, we model the blur due to scattering of light from the object. We measure the object point-spread function and introduce a simple deconvolution method. Finally, we show how imaging fluorescence emission where available, eliminates the backscatter component and increases the signal-to-noise ratio. Experimental results in a water tank, with different concentrations of scattering media added, show that deconvolution produces higher-quality 3D reconstructions than previous techniques, and that when combined with fluorescence, can produce results similar to that in clear water even for highly turbid media.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Murez, Zak and Treibitz, Tali and Ramamoorthi, Ravi and Kriegman, David J.},
	month = sep,
	year = {2017},
	keywords = {/unread, Backscatter, Cameras, Light sources, Media, Photometric stereo, Scattering, Surface reconstruction, Three-dimensional displays, fluorescence, scattering medium},
	pages = {1880--1891},
}

@article{wang_eaa-net_2021,
	title = {{EAA}-{Net}: {A} novel edge assisted attention network for single image dehazing},
	volume = {228},
	issn = {09507051},
	shorttitle = {{EAA}-{Net}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121005414},
	doi = {10/gr8wf7},
	abstract = {Traditional dehazing convolutional neural networks (CNNs) learn the feature maps only from hazy images to the corresponding hazy-free ones, which usually lead to some important feature loss like texture information. The paper proposes an effective edge assisted attention network (EAA-Net) for single image haze removal, aiming to preserve the texture information and improve the overall quality of the dehazed outcomes. The proposed EAA-Net mainly consists of three parts, i.e., a dehaze branch (DB), an edge branch (EB), and a feature fusion residual block (FFRB). In order to solve the loss of the texture information during the dehazing process, the forward information of the EB is concatenated with the DB’s information, and then the fusion result is passed to the FFRB. In addition, the multilevel information boost module is employed into the dehaze branch as a dehaze unit, and it can enhance the multilevel feature information for a better dehazing effect. Extensive experiments conducted on benchmark datasets demonstrate that the proposed EAA-Net performs favorably against some recent state-of-the-art approaches, which includes a better capacity of detail and color maintaining, and competitive consequences both quantitatively and qualitatively.},
	number = {C},
	urldate = {2023-03-07},
	journal = {Knowledge-Based Systems},
	author = {Wang, Chao and Shen, Hao-Zhen and Fan, Fan and Shao, Ming-Wen and Yang, Chuan-Sheng and Luo, Jian-Cheng and Deng, Liang-Jian},
	month = sep,
	year = {2021},
	keywords = {/unread, Attention network, CNN, Edge assisted, Single image dehazing},
	pages = {107279},
}

@inproceedings{chen_gated_2019,
	title = {Gated {Context} {Aggregation} {Network} for {Image} {Dehazing} and {Deraining}},
	url = {https://ieeexplore.ieee.org/document/8658661/},
	doi = {10/ghdq3h},
	abstract = {Image dehazing aims to recover the uncorrupted content from a hazy image. Instead of leveraging traditional low-level or handcrafted image priors as the restoration constraints, e.g., dark channels and increased contrast, we propose an end-to-end gated context aggregation network to directly restore the final haze-free image. In this network, we adopt the latest smoothed dilation technique to help remove the gridding artifacts caused by the widely-used dilated convolution with negligible extra parameters, and leverage a gated sub-network to fuse the features from different levels. Extensive experiments demonstrate that our method can surpass previous state-of-the-art methods by a large margin both quantitatively and qualitatively. In addition, to demonstrate the generality of the proposed method, we further apply it to the image deraining task, which also achieves the state-of-the-art performance.},
	booktitle = {2019 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Chen, Dongdong and He, Mingming and Fan, Qingnan and Liao, Jing and Zhang, Liheng and Hou, Dongdong and Yuan, Lu and Hua, Gang},
	month = jan,
	year = {2019},
	keywords = {/unread, Atmospheric modeling, Convolution, Fuses, Image restoration, Logic gates, Mathematical model, Task analysis},
	pages = {1375--1383},
}

@article{song_vision_2022,
	title = {Vision {Transformers} for {Single} {Image} {Dehazing}},
	abstract = {Image dehazing is a representative low-level vision task that estimates latent haze-free images from hazy images. In recent years, convolutional neural network-based methods have dominated image dehazing. However, vision Transformers, which has recently made a breakthrough in high-level vision tasks, has not brought new dimensions to image dehazing. We start with the popular Swin Transformer and find that several of its key designs are unsuitable for image dehazing. To this end, we propose DehazeFormer, which consists of various improvements, such as the modified normalization layer, activation function, and spatial information aggregation scheme. We train multiple variants of DehazeFormer on various datasets to demonstrate its effectiveness. Specifically, on the most frequently used SOTS indoor set, our small model outperforms FFA-Net with only 25\% \#Param and 5\% computational cost. To the best of our knowledge, our large model is the first method with the PSNR over 40 dB on the SOTS indoor set, dramatically outperforming the previous state-of-the-art methods. We also collect a large-scale realistic remote sensing dehazing dataset for evaluating the method's capability to remove highly non-homogeneous haze.},
	language = {en},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Image Processing},
	author = {Song, Yuda and He, Zhuqing and Qian, Hui and Du, Xin},
	month = apr,
	year = {2022},
	keywords = {/unread, ⛔ No DOI found},
}

@article{zhao_loss_2017,
	title = {Loss {Functions} for {Image} {Restoration} {With} {Neural} {Networks}},
	volume = {3},
	issn = {2333-9403},
	doi = {10/gfsvgd},
	abstract = {Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is ℓ2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.},
	number = {1},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
	month = mar,
	year = {2017},
	keywords = {/unread, Image processing, Image quality, Image restoration, Measurement, Neural networks, image restoration, loss functions, neural networks},
	pages = {47--57},
}

@article{ren_single_2020,
	title = {Single {Image} {Dehazing} via {Multi}-scale {Convolutional} {Neural} {Networks} with {Holistic} {Edges}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-019-01235-8},
	doi = {10/gjgv5g},
	abstract = {Single image dehazing has been a challenging problem which aims to recover clear images from hazy ones. The performance of existing image dehazing methods is limited by hand-designed features and priors. In this paper, we propose a multi-scale deep neural network for single image dehazing by learning the mapping between hazy images and their transmission maps. The proposed algorithm consists of a coarse-scale net which predicts a holistic transmission map based on the entire image, and a fine-scale net which refines dehazed results locally. To train the multi-scale deep network, we synthesize a dataset comprised of hazy images and corresponding transmission maps based on the NYU Depth dataset. In addition, we propose a holistic edge guided network to refine edges of the estimated transmission map. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world images in terms of quality and speed.},
	language = {en},
	number = {1},
	urldate = {2023-03-06},
	journal = {International Journal of Computer Vision},
	author = {Ren, Wenqi and Pan, Jinshan and Zhang, Hua and Cao, Xiaochun and Yang, Ming-Hsuan},
	month = jan,
	year = {2020},
	keywords = {/unread},
	pages = {240--259},
}

@article{cai_dehazenet_2016,
	title = {{DehazeNet}: {An} {End}-to-{End} {System} for {Single} {Image} {Haze} {Removal}},
	volume = {25},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{DehazeNet}},
	url = {http://ieeexplore.ieee.org/document/7539399/},
	doi = {10/f88xc2},
	abstract = {Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts convolutional neural network-based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, the layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called bilateral rectified linear unit, which is able to improve the quality of recovered haze-free image. We establish connections between the components of the proposed DehazeNet and those used in existing methods. Experiments on benchmark images show that DehazeNet achieves superior performance over existing methods, yet keeps efficient and easy to use.},
	number = {11},
	journal = {IEEE Transactions on Image Processing},
	author = {Cai, Bolun and Xu, Xiangmin and Jia, Kui and Qing, Chunmei and Tao, Dacheng},
	month = nov,
	year = {2016},
	keywords = {/unread, Atmospheric modeling, Attenuation, BReLU, Computational modeling, Dehaze, Estimation, Image color analysis, Image restoration, Scattering, deep CNN, image restoration},
	pages = {5187--5198},
}

@article{cantor_optics_1978,
	title = {Optics of the atmosphere–{Scattering} by molecules and particles},
	volume = {14},
	issn = {1558-1713},
	doi = {10/brqksg},
	number = {9},
	journal = {IEEE Journal of Quantum Electronics},
	author = {Cantor, A.},
	month = sep,
	year = {1978},
	keywords = {/unread, Books, Equations, Meteorology, Mie scattering, Optical refraction, Optical scattering, Optical variables control, Particle scattering, Rayleigh scattering, Stimulated emission},
	pages = {698--699},
}

@inproceedings{berman_non-local_2016,
	title = {Non-local {Image} {Dehazing}},
	doi = {10/gfxhxw},
	abstract = {Haze limits visibility and reduces image contrast in outdoor images. The degradation is different for every pixel and depends on the distance of the scene point from the camera. This dependency is expressed in the transmission coefficients, that control the scene attenuation and amount of haze in every pixel. Previous methods solve the single image dehazing problem using various patch-based priors. We, on the other hand, propose an algorithm based on a new, non-local prior. The algorithm relies on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors, that form tight clusters in RGB space. Our key observation is that pixels in a given cluster are often non-local, i.e., they are spread over the entire image plane and are located at different distances from the camera. In the presence of haze these varying distances translate to different transmission coefficients. Therefore, each color cluster in the clear image becomes a line in RGB space, that we term a haze-line. Using these haze-lines, our algorithm recovers both the distance map and the haze-free image. The algorithm is linear in the size of the image, deterministic and requires no training. It performs well on a wide variety of images and is competitive with other stateof-the-art methods.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Berman, Dana and Treibitz, Tali and Avidan, Shai},
	month = jun,
	year = {2016},
	keywords = {/unread, Atmospheric modeling, Cameras, Clustering algorithms, Histograms, Image color analysis, Image segmentation, Quantization (signal)},
	pages = {1674--1682},
}

@article{fattal_dehazing_2014,
	title = {Dehazing {Using} {Color}-{Lines}},
	volume = {34},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2651362},
	doi = {10/f6tzrv},
	abstract = {Photographs of hazy scenes typically suffer having low contrast and offer a limited visibility of the scene. This article describes a new method for single-image dehazing that relies on a generic regularity in natural images where pixels of small image patches typically exhibit a 1D distribution in RGB color space, known as color-lines. We derive a local formation model that explains the color-lines in the context of hazy scenes and use it for recovering the scene transmission based on the lines' offset from the origin. The lack of a dominant color-line inside a patch or its lack of consistency with the formation model allows us to identify and avoid false predictions. Thus, unlike existing approaches that follow their assumptions across the entire image, our algorithm validates its hypotheses and obtains more reliable estimates where possible.
            In addition, we describe a Markov random field model dedicated to producing complete and regularized transmission maps given noisy and scattered estimates. Unlike traditional field models that consist of local coupling, the new model is augmented with long-range connections between pixels of similar attributes. These connections allow our algorithm to properly resolve the transmission in isolated regions where nearby pixels do not offer relevant information.
            An extensive evaluation of our method over different types of images and its comparison to state-of-the-art methods over established benchmark images show a consistent improvement in the accuracy of the estimated scene transmission and recovered haze-free radiances.},
	number = {1},
	urldate = {2023-03-06},
	journal = {ACM Transactions on Graphics},
	author = {Fattal, Raanan},
	month = dec,
	year = {2014},
	keywords = {/unread, Image dehazing, contrast enhancement, transmission estimation},
	pages = {1--14},
}

@article{nishino_bayesian_2012,
	title = {Bayesian {Defogging}},
	volume = {98},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-011-0508-1},
	doi = {10/c3fw5z},
	abstract = {Atmospheric conditions induced by suspended particles, such as fog and haze, severely alter the scene appearance. Restoring the true scene appearance from a single observation made in such bad weather conditions remains a challenging task due to the inherent ambiguity that arises in the image formation process. In this paper, we introduce a novel Bayesian probabilistic method that jointly estimates the scene albedo and depth from a single foggy image by fully leveraging their latent statistical structures. Our key idea is to model the image with a factorial Markov random field in which the scene albedo and depth are two statistically independent latent layers and to jointly estimate them. We show that we may exploit natural image and depth statistics as priors on these hidden layers and estimate the scene albedo and depth with a canonical expectation maximization algorithm with alternating minimization. We experimentally evaluate the effectiveness of our method on a number of synthetic and real foggy images. The results demonstrate that the method achieves accurate factorization even on challenging scenes for past methods that only constrain and estimate one of the latent variables.},
	language = {en},
	number = {3},
	urldate = {2023-03-06},
	journal = {International Journal of Computer Vision},
	author = {Nishino, Ko and Kratz, Louis and Lombardi, Stephen},
	month = jul,
	year = {2012},
	keywords = {/unread},
	pages = {263--278},
}

@article{fujimura_dehazing_2021,
	title = {Dehazing cost volume for deep multi-view stereo in scattering media with airlight and scattering coefficient estimation},
	volume = {211},
	issn = {10773142},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314221000977},
	doi = {10/gr8wfz},
	abstract = {We propose a learning-based multi-view stereo (MVS) method in scattering media, such as fog or smoke, with a novel cost volume, called the dehazing cost volume. Images captured in scattering media are degraded due to light scattering and attenuation caused by suspended particles. This degradation depends on scene depth; thus, it is difficult for traditional MVS methods to evaluate photometric consistency because the depth is unknown before three-dimensional (3D) reconstruction. The dehazing cost volume can solve this chicken-and-egg problem of depth estimation and image restoration by computing the scattering effect using swept planes in the cost volume. We also propose a method of estimating scattering parameters, such as airlight, and a scattering coefficient, which are required for our dehazing cost volume. The output depth of a network with our dehazing cost volume can be regarded as a function of these parameters; thus, they are geometrically optimized with a sparse 3D point cloud obtained at a structure-from-motion step. Experimental results on synthesized hazy images indicate the effectiveness of our dehazing cost volume against the ordinary cost volume regarding scattering media. We also demonstrated the applicability of our dehazing cost volume to real foggy scenes.},
	language = {en},
	urldate = {2023-03-04},
	journal = {Computer Vision and Image Understanding},
	author = {Fujimura, Yuki and Sonogashira, Motoharu and Iiyama, Masaaki},
	month = oct,
	year = {2021},
	keywords = {/unread, Airlight, Depth estimation, Multi-view stereo, Scattering coefficient, Scattering media},
	pages = {103253},
}

@article{li_distance-aware_2022,
	title = {Distance-{Aware} {Occlusion} {Detection} {With} {Focused} {Attention}},
	volume = {31},
	issn = {1057-7149, 1941-0042},
	url = {https://ieeexplore.ieee.org/document/9864137/},
	doi = {10/gr8wf6},
	abstract = {For humans, understanding the relationships between objects using visual signals is intuitive. For artificial intelligence, however, this task remains challenging. Researchers have made significant progress studying semantic relationship detection, such as human-object interaction detection and visual relationship detection. We take the study of visual relationships a step further from semantic to geometric. In specific, we predict relative occlusion and relative distance relationships. However, detecting these relationships from a single image is challenging. Enforcing focused attention to task-specific regions plays a critical role in successfully detecting these relationships. In this work, (1) we propose a novel three-decoder architecture as the infrastructure for focused attention; 2) we use the generalized intersection box prediction task to effectively guide our model to focus on occlusion-specific regions; 3) our model achieves a new state-of-the-art performance on distance-aware relationship detection. Specifically, our model increases the distance F1-score from 33.8\% to 38.6\% and boosts the occlusion F1-score from 34.4\% to 41.2\%. Our code is publicly available.},
	journal = {IEEE Transactions on Image Processing},
	author = {Li, Yang and Tu, Yucheng and Chen, Xiaoxue and Zhao, Hao and Zhou, Guyue},
	year = {2022},
	keywords = {/unread, Decoding, Feature extraction, Focused attention, Legged locomotion, Semantics, Task analysis, Transformers, Visualization, object pair detection, relative distance detection, relative occlusion detection, transformer model, visualizations of attention weights},
	pages = {5661--5676},
}

@article{lowe_distinctive_2004,
	title = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
	volume = {60},
	issn = {0920-5691},
	url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
	doi = {10/bqrmsp},
	language = {en},
	number = {2},
	urldate = {2023-02-07},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	month = nov,
	year = {2004},
	keywords = {/unread},
	pages = {91--110},
}

@inproceedings{schonberger_structure--motion_2016,
	title = {Structure-from-{Motion} {Revisited}},
	doi = {10/ggfhmv},
	abstract = {Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schönberger, Johannes L. and Frahm, Jan-Michael},
	month = jun,
	year = {2016},
	keywords = {/unread, Cameras, Image reconstruction, Image registration, Internet, Pipelines, Robustness, Transmission line matrix methods},
	pages = {4104--4113},
}

@article{max_optical_1995,
	title = {Optical models for direct volume rendering},
	volume = {1},
	issn = {1941-0506},
	doi = {10/fm2mw5},
	abstract = {This tutorial survey paper reviews several different models for light interaction with volume densities of absorbing, glowing, reflecting, and/or scattering material. They are, in order of increasing realism, absorption only, emission only, emission and absorption combined, single scattering of external illumination without shadows, single scattering with shadows, and multiple scattering. For each model the paper provides the physical assumptions, describes the applications for which it is appropriate, derives the differential or integral equations for light transport, presents calculation methods for solving them, and shows output images for a data set representing a cloud. Special attention is given to calculation methods for the multiple scattering model.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Max, N.},
	month = jun,
	year = {1995},
	keywords = {/unread, Absorption, Computational modeling, Grid computing, Interpolation, Light scattering, Optical computing, Optical scattering, Rendering (computer graphics), Stimulated emission, X-ray scattering},
	pages = {99--108},
}

@inproceedings{ansari_wireless_2019,
	title = {Wireless {Software} {Synchronization} of {Multiple} {Distributed} {Cameras}},
	doi = {10/ghk64q},
	abstract = {We present a method for precisely time-synchronizing the capture of image sequences from a collection of smartphone cameras connected over WiFi. Our method is entirely software-based, has only modest hardware requirements, and achieves an accuracy of less than 250 µs on unmodified commodity hardware. It does not use image content and synchronizes cameras prior to capture. The algorithm operates in two stages. In the first stage, we designate one device as the leader and synchronize each client device’s clock to it by estimating network delay. Once clocks are synchronized, the second stage initiates continuous image streaming, estimates the relative phase of image timestamps between each client and the leader, and shifts the streams into alignment. We quantitatively validate our results on a multi-camera rig imaging a high-precision LED array and qualitatively demonstrate significant improvements to multi-view stereo depth estimation and stitching of dynamic scenes. We release as open source libsoftwaresync, an Android implementation of our system, to inspire new types of collective capture applications.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Computational} {Photography} ({ICCP})},
	author = {Ansari, Sameer and Wadhwa, Neal and Garg, Rahul and Chen, Jiawen},
	month = may,
	year = {2019},
	keywords = {/unread, Cameras, Clocks, Hardware, Software, Streaming media, Synchronization, Wireless fidelity, camera synchronization, computational photography, multi-view stereo, rolling shutter, wireless},
	pages = {1--9},
}

@article{meli_logic_2023,
	title = {Logic programming for deliberative robotic task planning},
	issn = {0269-2821, 1573-7462},
	url = {http://arxiv.org/abs/2301.07550},
	doi = {10/gr8wgq},
	abstract = {Over the last decade, the use of robots in production and daily life has increased. With increasingly complex tasks and interaction in different environments including humans, robots are required a higher level of autonomy for efficient deliberation. Task planning is a key element of deliberation. It combines elementary operations into a structured plan to satisfy a prescribed goal, given specifications on the robot and the environment. In this manuscript, we present a survey on recent advances in the application of logic programming to the problem of task planning. Logic programming offers several advantages compared to other approaches, including greater expressivity and interpretability which may aid in the development of safe and reliable robots. We analyze different planners and their suitability for specific robotic applications, based on expressivity in domain representation, computational efficiency and software implementation. In this way, we support the robotic designer in choosing the best tool for his application.},
	urldate = {2023-01-19},
	journal = {Artificial Intelligence Review},
	author = {Meli, Daniele and Nakawala, Hirenkumar and Fiorini, Paolo},
	month = jan,
	year = {2023},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Robotics},
}

@misc{mildenhall_nerf_2021,
	title = {{NeRF} in the {Dark}: {High} {Dynamic} {Range} {View} {Synthesis} from {Noisy} {Raw} {Images}},
	url = {http://arxiv.org/abs/2111.13679},
	doi = {10.48550/arXiv.2111.13679},
	abstract = {Neural Radiance Fields (NeRF) is a technique for high quality novel view synthesis from a collection of posed input images. Like most view synthesis methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images have been processed by a lossy camera pipeline that smooths detail, clips highlights, and distorts the simple noise distribution of raw sensor data. We modify NeRF to instead train directly on linear raw images, preserving the scene's full dynamic range. By rendering raw output images from the resulting NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In addition to changing the camera viewpoint, we can manipulate focus, exposure, and tonemapping after the fact. Although a single raw image appears significantly more noisy than a postprocessed one, we show that NeRF is highly robust to the zero-mean distribution of raw noise. When optimized over many noisy raw inputs (25-200), NeRF produces a scene representation so accurate that its rendered novel views outperform dedicated single and multi-image deep raw denoisers run on the same wide baseline input images. As a result, our method, which we call RawNeRF, can reconstruct scenes from extremely noisy images captured in near-darkness.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Mildenhall, Ben and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul and Barron, Jonathan T.},
	month = nov,
	year = {2021},
	note = {arXiv:2111.13679 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Electrical Engineering and Systems Science - Image and Video Processing, ⛔ No INSPIRE recid found},
}

@misc{yifan_iso-points_2021,
	title = {Iso-{Points}: {Optimizing} {Neural} {Implicit} {Surfaces} with {Hybrid} {Representations}},
	url = {http://arxiv.org/abs/2012.06434},
	doi = {10.48550/arXiv.2012.06434},
	abstract = {Neural implicit functions have emerged as a powerful representation for surfaces in 3D. Such a function can encode a high quality surface with intricate details into the parameters of a deep neural network. However, optimizing for the parameters for accurate and robust reconstructions remains a challenge, especially when the input data is noisy or incomplete. In this work, we develop a hybrid neural surface representation that allows us to impose geometry-aware sampling and regularization, which significantly improves the fidelity of reconstructions. We propose to use {\textbackslash}emph\{iso-points\} as an explicit representation for a neural implicit function. These points are computed and updated on-the-fly during training to capture important geometric features and impose geometric constraints on the optimization. We demonstrate that our method can be adopted to improve state-of-the-art techniques for reconstructing neural implicit surfaces from multi-view images or point clouds. Quantitative and qualitative evaluations show that, compared with existing sampling and optimization methods, our approach allows faster convergence, better generalization, and accurate recovery of details and topology.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Yifan, Wang and Wu, Shihao and Oztireli, Cengiz and Sorkine-Hornung, Olga},
	month = apr,
	year = {2021},
	note = {arXiv:2012.06434 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading, I.3.7, I.4.5, ⛔ No INSPIRE recid found},
}

@article{zhao_human_2022,
	title = {Human {Performance} {Modeling} and {Rendering} via {Neural} {Animated} {Mesh}},
	volume = {41},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3550454.3555451},
	doi = {10/gr8wgf},
	abstract = {We have recently seen tremendous progress in the neural advances for photo-real human modeling and rendering. However, it's still challenging to integrate them into an existing mesh-based pipeline for downstream applications. In this paper, we present a comprehensive neural approach for high-quality reconstruction, compression, and rendering of human performances from dense multi-view videos. Our core intuition is to bridge the traditional animated mesh workflow with a new class of highly efficient neural techniques. We first introduce a neural surface reconstructor for high-quality surface generation in minutes. It marries the implicit volumetric rendering of the truncated signed distance field (TSDF) with multi-resolution hash encoding. We further propose a hybrid neural tracker to generate animated meshes, which combines explicit non-rigid tracking with implicit dynamic deformation in a self-supervised framework. The former provides the coarse warping back into the canonical space, while the latter implicit one further predicts the displacements using the 4D hash encoding as in our reconstructor. Then, we discuss the rendering schemes using the obtained animated meshes, ranging from dynamic texturing to lumigraph rendering under various bandwidth settings. To strike an intricate balance between quality and bandwidth, we propose a hierarchical solution by first rendering 6 virtual views covering the performer and then conducting occlusion-aware neural texture blending. We demonstrate the efficacy of our approach in a variety of mesh-based applications and photo-realistic free-view experiences on various platforms, i.e., inserting virtual human performances into real environments through mobile AR or immersively watching talent shows with VR headsets.},
	number = {6},
	urldate = {2023-01-06},
	journal = {ACM Transactions on Graphics},
	author = {Zhao, Fuqiang and Jiang, Yuheng and Yao, Kaixin and Zhang, Jiakai and Wang, Liao and Dai, Haizhao and Zhong, Yuhui and Zhang, Yingliang and Wu, Minye and Xu, Lan and Yu, Jingyi},
	year = {2022},
	keywords = {/unread, human modeling, human performance capture, neural rendering, virtual human, ⛔ No INSPIRE recid found},
	pages = {235:1--235:17},
}

@inproceedings{ham_robust_2015,
	title = {Robust image filtering using joint static and dynamic guidance},
	doi = {10/gr8wg5},
	abstract = {Regularizing images under a guidance signal has been used in various tasks in computer vision and computational photography, particularly for noise reduction and joint upsampling. The aim is to transfer fine structures of guidance signals to input images, restoring noisy or altered structures. One of main drawbacks in such a data-dependent framework is that it does not handle differences in structure between guidance and input images. We address this problem by jointly leveraging structural information of guidance and input images. Image filtering is formulated as a nonconvex optimization problem, which is solved by the majorization-minimization algorithm. The proposed algorithm converges quickly while guaranteeing a local minimum. It effectively controls image structures at different scales and can handle a variety of types of data from different sensors. We demonstrate the flexibility and effectiveness of our model in several applications including depth super-resolution, scale-space filtering, texture removal, flash/non-flash denoising, and RGB/NIR denoising.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ham, Bumsub and Cho, Minsu and Ponce, Jean},
	month = jun,
	year = {2015},
	keywords = {/unread, Color, Image edge detection, Image resolution, Joints, Kernel, Linear programming, Noise reduction},
	pages = {4823--4831},
}

@article{zhang_nerfactor_2021,
	title = {{NeRFactor}: neural factorization of shape and reflectance under an unknown illumination},
	volume = {40},
	issn = {0730-0301},
	shorttitle = {{NeRFactor}},
	url = {https://doi.org/10.1145/3478513.3480496},
	doi = {10/gqkpvd},
	abstract = {We address the problem of recovering the shape and spatially-varying reflectance of an object from multi-view images (and their camera poses) of an object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our videos, code, and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.},
	number = {6},
	urldate = {2022-12-27},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, Xiuming and Srinivasan, Pratul P. and Deng, Boyang and Debevec, Paul and Freeman, William T. and Barron, Jonathan T.},
	year = {2021},
	keywords = {/unread, appearance factorization, inverse rendering, lighting estimation, material editing, reflectance estimation, relighting, shape estimation, view synthesis, ⛔ No INSPIRE recid found},
	pages = {237:1--237:18},
}

@inproceedings{charles_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	doi = {10/ggdspw},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
	month = jul,
	year = {2017},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer architecture, Feature extraction, Machine learning, Semantics, Shape, Three-dimensional displays, ⛔ No INSPIRE recid found},
	pages = {77--85},
}

@article{ruckert_adop_2022,
	title = {{ADOP}: approximate differentiable one-pixel point rendering},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{ADOP}},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530122},
	doi = {10/gr8wfr},
	abstract = {In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points.
            https://github.com/darglein/ADOP},
	number = {4},
	urldate = {2022-12-11},
	journal = {ACM Transactions on Graphics},
	author = {Rückert, Darius and Franke, Linus and Stamminger, Marc},
	month = jul,
	year = {2022},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, image-based rendering, inverse rendering, machine learning, neural rendering, novel view synthesis},
	pages = {1--14},
}

@article{bongiorno_vector-based_2021,
	title = {Vector-based pedestrian navigation in cities},
	volume = {1},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-021-00130-y},
	doi = {10/gm8kh8},
	abstract = {How do pedestrians choose their paths within city street networks? Researchers have tried to shed light on this matter through strictly controlled experiments, but an ultimate answer based on real-world mobility data is still lacking. Here, we analyze salient features of human path planning through a statistical analysis of a massive dataset of GPS traces, which reveals that (1) people increasingly deviate from the shortest path when the distance between origin and destination increases and (2) chosen paths are statistically different when origin and destination are swapped. We posit that direction to goal is a main driver of path planning and develop a vector-based navigation model; the resulting trajectories, which we have termed pointiest paths, are a statistically better predictor of human paths than a model based on minimizing distance with stochastic effects. Our findings generalize across two major US cities with different street networks, hinting to the fact that vector-based navigation might be a universal property of human path planning.},
	language = {en},
	number = {10},
	urldate = {2022-12-07},
	journal = {Nature Computational Science},
	author = {Bongiorno, Christian and Zhou, Yulun and Kryven, Marta and Theurel, David and Rizzo, Alessandro and Santi, Paolo and Tenenbaum, Joshua and Ratti, Carlo},
	month = oct,
	year = {2021},
	keywords = {/unread, Computational science, Geography, Society},
	pages = {678--685},
}

@inproceedings{kaiming_he_single_2009,
	address = {Miami, FL},
	title = {Single image haze removal using dark channel prior},
	isbn = {978-1-4244-3992-8},
	url = {https://ieeexplore.ieee.org/document/5206515/},
	doi = {10/chjgvb},
	urldate = {2022-12-03},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {{Kaiming He} and {Jian Sun} and {Xiaoou Tang}},
	month = jun,
	year = {2009},
	keywords = {/unread},
	pages = {1956--1963},
}

@article{komorowski_artificial_2018,
	title = {The {Artificial} {Intelligence} {Clinician} learns optimal treatment strategies for sepsis in intensive care},
	volume = {24},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-018-0213-5/},
	doi = {10/gfgvnf},
	abstract = {Sepsis is the third leading cause of death worldwide and the main cause of mortality in hospitals1–3, but the best treatment strategy remains uncertain. In particular, evidence suggests that current practices in the administration of intravenous fluids and vasopressors are suboptimal and likely induce harm in a proportion of patients1,4–6. To tackle this sequential decision-making problem, we developed a reinforcement learning agent, the Artificial Intelligence (AI) Clinician, which extracted implicit knowledge from an amount of patient data that exceeds by many-fold the life-time experience of human clinicians and learned optimal treatment by analyzing a myriad of (mostly suboptimal) treatment decisions. We demonstrate that the value of the AI Clinician’s selected treatment is on average reliably higher than human clinicians. In a large validation cohort independent of the training data, mortality was lowest in patients for whom clinicians’ actual doses matched the AI decisions. Our model provides individualized and clinically interpretable treatment decisions for sepsis that could improve patient outcomes.},
	language = {en},
	number = {11},
	urldate = {2022-12-02},
	journal = {Nature Medicine},
	author = {Komorowski, Matthieu and Celi, Leo A. and Badawi, Omar and Gordon, Anthony C. and Faisal, A. Aldo},
	month = nov,
	year = {2018},
	keywords = {/unread, Biomedical engineering, Computational models, Machine learning, Outcomes research},
	pages = {1716--1720},
}

@article{kolda_tensor_2009,
	title = {Tensor {Decompositions} and {Applications}},
	volume = {51},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/abs/10.1137/07070111X},
	doi = {10/dzcrv6},
	abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N≥3) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
	number = {3},
	urldate = {2022-12-01},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	month = aug,
	year = {2009},
	keywords = {/unread, 15A69, 65F99, canonical decomposition (CANDECOMP), higher-order principal components analysis (Tucker), higher-order singular value decomposition (HOSVD), multilinear algebra, multiway arrays, parallel factors (PARAFAC), tensor decompositions},
	pages = {455--500},
}

@article{bonatto_real-time_2021,
	title = {Real-{Time} {Depth} {Video}-{Based} {Rendering} for 6-{DoF} {HMD} {Navigation} and {Light} {Field} {Displays}},
	volume = {9},
	issn = {2169-3536},
	doi = {10/gnj5bx},
	abstract = {This paper presents a novel approach to provide immersive free navigation with 6 Degrees of Freedom in real-time for natural and virtual scenery, for both static and dynamic content. Stemming from the state-of-the-art in Depth Image-Based Rendering and the OpenGL pipeline, this new View Synthesis method achieves free navigation at up to 90 FPS and can take any number of input views with their corresponding depth maps as priors. Video content can be played thanks to GPU decompression, supporting free navigation with full parallax in real-time. To render a novel viewpoint, each selected input view is warped using the camera pose and associated depth map, using an implicit 3D representation. The warped views are then blended all together to generate the chosen virtual view. Various view blending approaches specifically designed to avoid visual artifacts are compared. Using as few as four input views appears to be an optimal trade-off between computation time and quality, allowing to synthesize high-quality stereoscopic views in real-time, offering a genuine immersive virtual reality experience. Additionally, the proposed approach provides high-quality rendering of a 3D scenery on holographic light field displays. Our results are comparable - objectively and subjectively - to the state of the art view synthesis tools NeRF and LLFF, while maintaining an overall lower complexity and real-time rendering.},
	journal = {IEEE Access},
	author = {Bonatto, Daniele and Fachada, Sarah and Rogge, Ségolène and Munteanu, Adrian and Lafruit, Gauthier},
	year = {2021},
	keywords = {/unread, Light fields, Navigation, Real-time systems, Rendering (computer graphics), Resists, Streaming media, Three-dimensional displays, Virtual reality, free viewpoint navigation, real-time view synthesis, reference view synthesizer, stereo image processing, stereo vision},
	pages = {146868--146887},
}

@article{deng_fov-nerf_2022,
	title = {{FoV}-{NeRF}: {Foveated} {Neural} {Radiance} {Fields} for {Virtual} {Reality}},
	volume = {28},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{FoV}-{NeRF}},
	url = {https://ieeexplore.ieee.org/document/9872532/},
	doi = {10/gr8wgc},
	abstract = {Virtual Reality (VR) is becoming ubiquitous with the rise of consumer displays and commercial VR platforms. Such displays require low latency and high quality rendering of synthetic imagery with reduced compute overheads. Recent advances in neural rendering showed promise of unlocking new possibilities in 3D computer graphics via image-based representations of virtual or physical environments. Specifically, the neural radiance fields (NeRF) demonstrated that photo-realistic quality and continuous view changes of 3D scenes can be achieved without loss of view-dependent effects. While NeRF can significantly benefit rendering for VR applications, it faces unique challenges posed by high field-of-view, high resolution, and stereoscopic/egocentric viewing, typically causing low quality and high latency of the rendered images. In VR, this not only harms the interaction experience but may also cause sickness. To tackle these problems toward six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. We then jointly optimize the latency/performance and visual quality while mutually bridging human perception and neural scene synthesis to achieve perceptually high-quality immersive interaction. We conducted both objective analysis and subjective studies to evaluate the effectiveness of our approach. We find that our method significantly reduces latency (up to 99\% time reduction compared with NeRF) without loss of high-fidelity rendering (perceptually identical to full-resolution ground truth). The presented approach may serve as the first step toward future VR/AR systems that capture, teleport, and visualize remote environments in real-time.},
	number = {11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Deng, Nianchen and He, Zhenyi and Ye, Jiannan and Duinkharjav, Budmonde and Chakravarthula, Praneeth and Yang, Xubo and Sun, Qi},
	month = nov,
	year = {2022},
	keywords = {/unread, Cameras, Electronic mail, Foveated Rendering, Gaze-Contingent Graphics, Neural Representation, Rendering (computer graphics), Sensitivity, Stereo image processing, Three-dimensional displays, Virtual Reality, Visualization},
	pages = {3854--3864},
}

@article{fan_image_2018,
	title = {Image {Visual} {Realism}: {From} {Human} {Perception} to {Machine} {Computation}},
	volume = {40},
	issn = {1939-3539},
	shorttitle = {Image {Visual} {Realism}},
	doi = {10/gr8wgg},
	abstract = {Visual realism is defined as the extent to which an image appears to people as a photo rather than computer generated. Assessing visual realism is important in applications like computer graphics rendering and photo retouching. However, current realism evaluation approaches use either labor-intensive human judgments or automated algorithms largely dependent on comparing renderings to reference images. We develop a reference-free computational framework for visual realism prediction to overcome these constraints. First, we construct a benchmark dataset of 2,520 images with comprehensive human annotated attributes. From statistical modeling on this data, we identify image attributes most relevant for visual realism. We propose both empirically-based (guided by our statistical modeling of human data) and deep convolutional neural network models to predict visual realism of images. Our framework has the following advantages: (1) it creates an interpretable and concise empirical model that characterizes human perception of visual realism; (2) it links computational features to latent factors of human image perception.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Fan, Shaojing and Ng, Tian-Tsong and Koenig, Bryan Lee and Herberg, Jonathan Samuel and Jiang, Ming and Shen, Zhiqi and Zhao, Qi},
	month = sep,
	year = {2018},
	keywords = {/unread, Benchmark testing, Computational modeling, Face, Rendering (computer graphics), Solid modeling, Visual realism, Visualization, convolutional neural network, human psychophysics, statistical modeling},
	pages = {2180--2193},
}

@article{quadri_survey_2022,
	title = {A {Survey} of {Perception}-{Based} {Visualization} {Studies} by {Task}},
	volume = {28},
	issn = {1077-2626, 1941-0506, 2160-9306},
	url = {https://ieeexplore.ieee.org/document/9492011/},
	doi = {10/gr6323},
	abstract = {Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Quadri, Ghulam Jilani and Rosen, Paul},
	month = dec,
	year = {2022},
	keywords = {/unread, Crowdsourcing, Data visualization, Encoding, Task analysis, Taxonomy, Visual analytics, Visualization, evaluation, graphical perception, perception, survey, visual analytics tasks},
	pages = {5026--5048},
}

@article{zeng_octree-based_2013,
	series = {Computational {Visual} {Media} {Conference} 2012},
	title = {Octree-based fusion for realtime {3D} reconstruction},
	volume = {75},
	issn = {1524-0703},
	url = {https://www.sciencedirect.com/science/article/pii/S1524070312000768},
	doi = {10/f5hkg2},
	abstract = {This paper proposes an octree-based surface representation for KinectFusion, a realtime reconstruction technique of in-door scenes using a low-cost moving depth camera and a commodity graphics hardware. In KinectFusion, the scene is represented as a signed distance function (SDF) and stored as an uniform grid of voxels. Though the grid-based SDF is suitable for parallel computation in graphics hardware, most of the storage are wasted, because the geometry is very sparse in the scene volume. In order to reduce the memory cost and save the computation time, we represent the SDF in an octree, and developed several octree-based algorithms for reconstruction update and surface prediction that are suitable for parallel computation in graphics hardware. In the reconstruction update step, the octree nodes are adaptively split in breath-first order. To handle scenes with moving objects, the corresponding nodes are automatically detected and removed to avoid storage overflow. In the surface prediction step, an octree-based ray tracing method is adopted and parallelized for graphic hardware. To further reduce the computation time, the octree is organized into four layers, called top layer, branch layer, middle layer and data layer. The experiments showed that, the proposed method consumes only less than 10\% memory of original KinectFusion method, and achieves faster performance. Consequently, it can reconstruct scenes with more than 10 times larger size than the original KinectFusion on the same hardware setup.},
	language = {en},
	number = {3},
	urldate = {2022-11-22},
	journal = {Graphical Models},
	author = {Zeng, Ming and Zhao, Fukai and Zheng, Jiaxiang and Liu, Xinguo},
	month = may,
	year = {2013},
	keywords = {/unread, 3D reconstruction, Graphics hardware, KinectFusion, Octree, Ray casting, Signed distance function},
	pages = {126--136},
}

@inproceedings{shen_sgam_2022,
	title = {{SGAM}: {Building} a {Virtual} {3D} {World} through {Simultaneous} {Generation} and {Mapping}},
	shorttitle = {{SGAM}},
	url = {https://openreview.net/forum?id=17KCLTbRymw},
	abstract = {We present simultaneous generation and mapping (SGAM), a novel 3D scene generation algorithm. Our goal is to produce a realistic, globally consistent 3D world on a large scale. Achieving this goal is challenging and goes beyond the capacities of existing 3D generation or video generation approaches, which fail to scale up to create large, globally consistent 3D scene structures. Towards tackling the challenges, we take a hybrid approach that integrates generative sensor model- ing with 3D reconstruction. Our proposed approach is an autoregressive generative framework that simultaneously generates sensor data at novel viewpoints and builds a 3D map at each timestamp. Given an arbitrary camera trajectory, our method repeatedly applies this generation-and-mapping process for thousands of steps, allowing us to create a gigantic virtual world. Our model can be trained from RGB-D sequences without having access to the complete 3D scene structure. The generated scenes are readily compatible with various interactive environments and rendering engines. Experiments on CLEVER and GoogleEarth datasets demon- strates ours can generate consistent, realistic, and geometrically-plausible scenes that compare favorably to existing view synthesis methods. Our project page is available at https://yshen47.github.io/sgam.},
	language = {en},
	urldate = {2022-11-05},
	author = {Shen, Yuan and Ma, Wei-Chiu and Wang, Shenlong},
	month = oct,
	year = {2022},
	keywords = {MIT, UIUC},
}

@article{adamkiewicz_vision-only_2022,
	title = {Vision-{Only} {Robot} {Navigation} in a {Neural} {Radiance} {World}},
	volume = {7},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/9712211/},
	doi = {10/gqfh86},
	abstract = {Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. Neural Radiance Fields (NeRFs) represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an onboard RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot’s objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot’s full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through a narrow gap.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Adamkiewicz, Michal and Chen, Timothy and Caccavale, Adam and Gardner, Rachel and Culbertson, Preston and Bohg, Jeannette and Schwager, Mac},
	month = apr,
	year = {2022},
	keywords = {/unread, Cameras, Collision avoidance, Navigation, Pipelines, Planning, Robot vision systems, Robots, Stanford, localization, motion and path planning, neural radiance fields, vision-based navigation},
	pages = {4606--4613},
}

@article{niesner_real-time_2013,
	title = {Real-time {3D} reconstruction at scale using voxel hashing},
	volume = {32},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2508363.2508374},
	doi = {10/gbd5tz},
	abstract = {Online 3D reconstruction is gaining newfound interest due to the availability of real-time consumer depth cameras. The basic problem takes live overlapping depth maps as input and incrementally fuses these into a single 3D model. This is challenging particularly when real-time performance is desired without trading quality or scale. We contribute an online system for large and fine scale volumetric reconstruction based on a memory and speed efficient data structure. Our system uses a simple spatial hashing scheme that compresses space, and allows for real-time access and updates of implicit surface data, without the need for a regular or hierarchical grid data structure. Surface data is only stored densely where measurements are observed. Additionally, data can be streamed efficiently in or out of the hash table, allowing for further scalability during sensor motion. We show interactive reconstructions of a variety of scenes, reconstructing both fine-grained details and large scale environments. We illustrate how all parts of our pipeline from depth map pre-processing, camera pose estimation, depth map fusion, and surface rendering are performed at real-time rates on commodity graphics hardware. We conclude with a comparison to current state-of-the-art online systems, illustrating improved performance and reconstruction quality.},
	number = {6},
	urldate = {2022-10-27},
	journal = {ACM Transactions on Graphics},
	author = {Nießner, Matthias and Zollhöfer, Michael and Izadi, Shahram and Stamminger, Marc},
	year = {2013},
	keywords = {/unread, GPU, data structure, real-time reconstruction, scalable},
	pages = {169:1--169:11},
}

@article{engel_direct_2018,
	title = {Direct {Sparse} {Odometry}},
	volume = {40},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7898369/},
	doi = {10/gdk24j},
	abstract = {Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
	month = mar,
	year = {2018},
	keywords = {/unread, Cameras, Computational modeling, Geometry, Optimization, Robustness, Three-dimensional displays, Visual odometry, SLAM, 3D reconstruction, structure from motion, Visualization},
	pages = {611--625},
}

@article{campos_orb-slam3_2021,
	title = {{ORB}-{SLAM3}: {An} {Accurate} {Open}-{Source} {Library} for {Visual}, {Visual}–{Inertial}, and {Multimap} {SLAM}},
	volume = {37},
	issn = {1941-0468},
	shorttitle = {{ORB}-{SLAM3}},
	doi = {10/gkzqvv},
	abstract = {This article presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multimap SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a tightly integrated visual-inertial SLAM system that fully relies on maximum a posteriori (MAP) estimation, even during IMU initialization, resulting in real-time robust operation in small and large, indoor and outdoor environments, being two to ten times more accurate than previous approaches. The second main novelty is a multiple map system relying on a new place recognition method with improved recall that lets ORB-SLAM3 survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting them. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information from high parallax co-visible keyframes, even if they are widely separated in time or come from previous mapping sessions, boosting accuracy. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.5 cm in the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, representative of AR/VR scenarios. For the benefit of the community we make public the source code.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and M. Montiel, José M. and D. Tardós, Juan},
	month = dec,
	year = {2021},
	keywords = {/unread, Computer vision, Feature extraction, Inertial navigation, Optimization, Robustness, Simultaneous localization and mapping, inertial navigation, simult- aneous localization and mapping},
	pages = {1874--1890},
}

@article{mur-artal_orb-slam_2015,
	title = {{ORB}-{SLAM}: {A} {Versatile} and {Accurate} {Monocular} {SLAM} {System}},
	volume = {31},
	issn = {1941-0468},
	shorttitle = {{ORB}-{SLAM}},
	doi = {10/f7xpqn},
	abstract = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Mur-Artal, Raúl and Montiel, J. M. M. and Tardós, Juan D.},
	month = oct,
	year = {2015},
	keywords = {/unread, Cameras, Computational modeling, Feature extraction, Lifelong mapping, Optimization, Real-time systems, Simultaneous localization and mapping, Visualization, localization, monocular vision, recognition, simultaneous localization and mapping (SLAM)},
	pages = {1147--1163},
}

@article{zhang_microsoft_2012,
	title = {Microsoft {Kinect} {Sensor} and {Its} {Effect}},
	volume = {19},
	issn = {1070-986X},
	url = {http://ieeexplore.ieee.org/document/6190806/},
	doi = {10/ggsgw2},
	number = {2},
	urldate = {2022-10-26},
	journal = {IEEE Multimedia},
	author = {Zhang, Zhengyou},
	month = feb,
	year = {2012},
	keywords = {/unread},
	pages = {4--10},
}

@article{zabatani_intel_2020,
	title = {Intel® {RealSense}™ {SR300} {Coded} {Light} {Depth} {Camera}},
	volume = {42},
	issn = {1939-3539},
	doi = {10/gr8wgk},
	abstract = {Intel® RealSense™ SR300 is a depth camera capable of providing a VGA-size depth map at 60 fps and 0.125mm depth resolution. In addition, it outputs an infrared VGA-resolution image and a 1080p color texture image at 30 fps. SR300 form-factor enables it to be integrated into small consumer products and as a front facing camera in laptops and Ultrabooks™. The SR300 depth camera is based on a coded-light technology where triangulation between projected patterns and images captured by a dedicated sensor is used to produce the depth map. Each projected line is coded by a special temporal optical code, that enables a dense depth map reconstruction from its reflection. The solid mechanical assembly of the camera allows it to stay calibrated throughout temperature and pressure changes, drops, and hits. In addition, active dynamic control maintains a calibrated depth output. An extended API LibRS released with the camera allows developers to integrate the camera in various applications. Algorithms for 3D scanning, facial analysis, hand gesture recognition, and tracking are within reach for applications using the SR300. In this paper, we describe the underlying technology, hardware, and algorithms of the SR300, as well as its calibration procedure, and outline some use cases. We believe that this paper will provide a full case study of a mass-produced depth sensing product and technology.},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zabatani, Aviad and Surazhsky, Vitaly and Sperling, Erez and Moshe, Sagi Ben and Menashe, Ohad and Silver, David H. and Karni, Zachi and Bronstein, Alexander M. and Bronstein, Michael M. and Kimmel, Ron},
	month = oct,
	year = {2020},
	keywords = {/unread, 3D camera, Cameras, Image reconstruction, Intel, Optical imaging, Optical sensors, Pipelines, RealSense, Reflective binary codes, SR300, Three-dimensional displays, coded light, depth reconstruction},
	pages = {2333--2345},
}

@article{zhang_microsoft_2012-1,
	title = {Microsoft {Kinect} {Sensor} and {Its} {Effect}},
	volume = {19},
	issn = {1941-0166},
	doi = {10/ggsgw2},
	abstract = {Recent advances in 3D depth cameras such as Microsoft Kinect sensors (www.xbox.com/en-US/kinect) have created many opportunities for multimedia computing. The Kinect sensor lets the computer directly sense the third dimension (depth) of the players and the environment. It also understands when users talk, knows who they are when they walk up to it, and can interpret their movements and translate them into a format that developers can use to build new experiences. While the Kinect sensor incorporates several advanced sensing hardware, this article focuses on the vision aspect of the Kinect sensor and its impact beyond the gaming industry.},
	number = {2},
	journal = {IEEE MultiMedia},
	author = {Zhang, Zhengyou},
	month = feb,
	year = {2012},
	keywords = {/unread, Cameras, Games, Microsoft Kinect, Sensors, Three dimensional displays, Video recording, computer vision, human-computer interaction, motion capture, multimedia},
	pages = {4--10},
}

@article{zhang_editable_2021,
	title = {Editable free-viewpoint video using a layered neural representation},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3450626.3459756},
	doi = {10/gmpcws},
	abstract = {Generating free-viewpoint videos is critical for immersive VR/AR experience, but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper, we propose the first approach for editable free-viewpoint video generation for large-scale view-dependent dynamic scenes using only 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity, including the environment itself, is formulated into a spatio-temporal coherent neural layered radiance representation called ST-NeRF. Such a layered representation supports manipulations of the dynamic scene while still supporting a wide free viewing experience. In our ST-NeRF, we represent the dynamic entity/layer as a continuous function, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.},
	number = {4},
	urldate = {2022-10-25},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, Jiakai and Liu, Xinhang and Ye, Xinyi and Zhao, Fuqiang and Zhang, Yanshun and Wu, Minye and Zhang, Yingliang and Xu, Lan and Yu, Jingyi},
	month = aug,
	year = {2021},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No INSPIRE recid found},
	pages = {1--18},
}

@inproceedings{yen-chen_inerf_2021,
	title = {{iNeRF}: {Inverting} {Neural} {Radiance} {Fields} for {Pose} {Estimation}},
	shorttitle = {{iNeRF}},
	doi = {10/gnxwtm},
	abstract = {We present iNeRF, a framework that performs mesh-free pose estimation by "inverting" a Neural Radiance Field (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis — synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation – given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset [21], iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform categorylevel object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Yen-Chen, Lin and Florence, Pete and Barron, Jonathan T. and Rodriguez, Alberto and Isola, Phillip and Lin, Tsung-Yi},
	month = sep,
	year = {2021},
	keywords = {/unread, Cameras, Emphasis, Optimization, Pose estimation, Task analysis, Three-dimensional displays, Training, Training data, ⛔ No INSPIRE recid found},
	pages = {1323--1330},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {Dropout},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	keywords = {deep learning, model combination, neural networks, regularization, ⛔ No DOI found},
	pages = {1929--1958},
}

@inproceedings{kendall_geometric_2017,
	title = {Geometric {Loss} {Functions} for {Camera} {Pose} {Regression} with {Deep} {Learning}},
	url = {http://ieeexplore.ieee.org/document/8100177/},
	doi = {10/gg5v2h},
	abstract = {Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet [22] is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNets performance across datasets ranging from indoor rooms to a small city.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Kendall, Alex and Cipolla, Roberto},
	month = jul,
	year = {2017},
	keywords = {2017, CVPR, Cameras, Geometry, Machine learning, Measurement, Neural networks, Quaternions, Reading, Relocalization, Robustness},
	pages = {6555--6564},
}

@article{fawzi_discovering_2022,
	title = {Discovering faster matrix multiplication algorithms with reinforcement learning},
	volume = {610},
	copyright = {2022 The Author(s)},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05172-4},
	doi = {10/gqxw7g},
	abstract = {Abstract
            
              Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero
              1
              for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago
              2
              . We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
	language = {en},
	number = {7930},
	urldate = {2022-10-06},
	journal = {Nature},
	author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
	month = oct,
	year = {2022},
	keywords = {Applied mathematics, Computer science, Read Later},
	pages = {47--53},
}

@article{muller_instant_2022,
	title = {Instant neural graphics primitives with a multiresolution hash encoding},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{InstantNGP}},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
	doi = {10/gqkpt7},
	abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
	language = {en},
	number = {4},
	urldate = {2022-10-05},
	journal = {ACM Transactions on Graphics},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	month = jul,
	year = {2022},
	keywords = {2022, Emphasis, Fast Rendering, Fast Training, Hash Encoding, NeRF, Reading, SIGGRAPH, ⛔ No INSPIRE recid found},
	pages = {1--15},
}

@misc{mao_robust_2023,
	title = {Robust {Perception} through {Equivariance}},
	url = {http://arxiv.org/abs/2212.06079},
	doi = {10.48550/arXiv.2212.06079},
	abstract = {Deep networks for computer vision are not reliable when they encounter adversarial examples. In this paper, we introduce a framework that uses the dense intrinsic constraints in natural images to robustify inference. By introducing constraints at inference time, we can shift the burden of robustness from training to the inference algorithm, thereby allowing the model to adjust dynamically to each individual image's unique and potentially novel characteristics at inference time. Among different constraints, we find that equivariance-based constraints are most effective, because they allow dense constraints in the feature space without overly constraining the representation at a fine-grained level. Our theoretical results validate the importance of having such dense constraints at inference time. Our empirical experiments show that restoring feature equivariance at inference time defends against worst-case adversarial perturbations. The method obtains improved adversarial robustness on four datasets (ImageNet, Cityscapes, PASCAL VOC, and MS-COCO) on image recognition, semantic segmentation, and instance segmentation tasks. Project page is available at equi4robust.cs.columbia.edu.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Mao, Chengzhi and Zhang, Lingyu and Joshi, Abhishek and Yang, Junfeng and Wang, Hao and Vondrick, Carl},
	month = jun,
	year = {2023},
	note = {arXiv:2212.06079 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{liu_what_2023,
	title = {What {You} {Can} {Reconstruct} {From} a {Shadow}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Liu_What_You_Can_Reconstruct_From_a_Shadow_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-30},
	author = {Liu, Ruoshi and Menon, Sachit and Mao, Chengzhi and Park, Dennis and Stent, Simon and Vondrick, Carl},
	year = {2023},
	pages = {17059--17068},
}

@article{liu_shadows_2022,
	title = {Shadows {Shed} {Light} on {3D} {Objects}},
	journal = {arXiv preprint arXiv:2206.08990},
	author = {Liu, Ruoshi and Menon, Sachit and Mao, Chengzhi and Park, Dennis and Stent, Simon and Vondrick, Carl},
	year = {2022},
	keywords = {⛔ No DOI found},
}

@misc{van_hoorick_revealing_2022,
	title = {Revealing {Occlusions} with {4D} {Neural} {Fields}},
	url = {http://arxiv.org/abs/2204.10916},
	abstract = {For computer vision systems to operate in dynamic situations, they need to be able to represent and reason about object permanence. We introduce a framework for learning to estimate 4D visual representations from monocular RGB-D, which is able to persist objects, even once they become obstructed by occlusions. Unlike traditional video representations, we encode point clouds into a continuous representation, which permits the model to attend across the spatiotemporal context to resolve occlusions. On two large video datasets that we release along with this paper, our experiments show that the representation is able to successfully reveal occlusions for several tasks, without any architectural changes. Visualizations show that the attention mechanism automatically learns to follow occluded objects. Since our approach can be trained end-to-end and is easily adaptable, we believe it will be useful for handling occlusions in many video understanding tasks. Data, code, and models are available at https://occlusions.cs.columbia.edu/.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Van Hoorick, Basile and Tendulkar, Purva and Suris, Didac and Park, Dennis and Stent, Simon and Vondrick, Carl},
	month = apr,
	year = {2022},
	note = {arXiv:2204.10916 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{liu_one-2-3-45_2023,
	title = {One-2-3-45: {Any} {Single} {Image} to {3D} {Mesh} in 45 {Seconds} without {Per}-{Shape} {Optimization}},
	shorttitle = {One-2-3-45},
	url = {http://arxiv.org/abs/2306.16928},
	abstract = {Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Liu, Minghua and Xu, Chao and Jin, Haian and Chen, Linghao and T, Mukund Varma and Xu, Zexiang and Su, Hao},
	month = jun,
	year = {2023},
	note = {arXiv:2306.16928 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{cheng_id-pose_2023,
	title = {{ID}-{Pose}: {Sparse}-view {Camera} {Pose} {Estimation} by {Inverting} {Diffusion} {Models}},
	shorttitle = {{ID}-{Pose}},
	url = {http://arxiv.org/abs/2306.17140},
	abstract = {Given sparse views of an object, estimating their camera poses is a long-standing and intractable problem. We harness the pre-trained diffusion model of novel views conditioned on viewpoints (Zero-1-to-3). We present ID-Pose which inverses the denoising diffusion process to estimate the relative pose given two input images. ID-Pose adds a noise on one image, and predicts the noise conditioned on the other image and a decision variable for the pose. The prediction error is used as the objective to find the optimal pose with the gradient descent method. ID-Pose can handle more than two images and estimate each of the poses with multiple image pairs from triangular relationships. ID-Pose requires no training and generalizes to real-world images. We conduct experiments using high-quality real-scanned 3D objects, where ID-Pose significantly outperforms state-of-the-art methods.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Cheng, Weihao and Cao, Yan-Pei and Shan, Ying},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17140 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao_michelangelo_2023,
	title = {Michelangelo: {Conditional} {3D} {Shape} {Generation} based on {Shape}-{Image}-{Text} {Aligned} {Latent} {Representation}},
	shorttitle = {Michelangelo},
	url = {http://arxiv.org/abs/2306.17115},
	doi = {10.48550/arXiv.2306.17115},
	abstract = {We present a novel alignment-before-generation approach to tackle the challenging task of generating general 3D shapes based on 2D images or texts. Directly learning a conditional generative model from images or texts to 3D shapes is prone to producing inconsistent results with the conditions because 3D shapes have an additional dimension whose distribution significantly differs from that of 2D images and texts. To bridge the domain gap among the three modalities and facilitate multi-modal-conditioned 3D shape generation, we explore representing 3D shapes in a shape-image-text-aligned space. Our framework comprises two models: a Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE) and a conditional Aligned Shape Latent Diffusion Model (ASLDM). The former model encodes the 3D shapes into the shape latent space aligned to the image and text and reconstructs the fine-grained 3D neural fields corresponding to given shape embeddings via the transformer-based decoder. The latter model learns a probabilistic mapping function from the image or text space to the latent shape space. Our extensive experiments demonstrate that our proposed approach can generate higher-quality and more diverse 3D shapes that better semantically conform to the visual or textural conditional inputs, validating the effectiveness of the shape-image-text-aligned space for cross-modality 3D shape generation.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Zhao, Zibo and Liu, Wen and Chen, Xin and Zeng, Xianfang and Wang, Rui and Cheng, Pei and Fu, Bin and Chen, Tao and Yu, Gang and Gao, Shenghua},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17115 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_end--end_2023,
	title = {End-to-end {Autonomous} {Driving}: {Challenges} and {Frontiers}},
	shorttitle = {End-to-end {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2306.16927},
	doi = {10.48550/arXiv.2306.16927},
	abstract = {The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. To facilitate future research, we maintain an active repository that contains up-to-date links to relevant literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Chen, Li and Wu, Penghao and Chitta, Kashyap and Jaeger, Bernhard and Geiger, Andreas and Li, Hongyang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.16927 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{zhi_-place_2021,
	title = {In-{Place} {Scene} {Labelling} and {Understanding} with {Implicit} {Scene} {Representation}},
	shorttitle = {{SemanticNeRF}},
	url = {http://arxiv.org/abs/2103.15875},
	abstract = {Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.},
	urldate = {2023-04-07},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Zhi, Shuaifeng and Laidlow, Tristan and Leutenegger, Stefan and Davison, Andrew J.},
	month = aug,
	year = {2021},
	note = {arXiv:2103.15875 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@inproceedings{qiao_vip-deeplab_2021,
	title = {{ViP}-{DeepLab}: {Learning} {Visual} {Perception} with {Depth}-aware {Video} {Panoptic} {Segmentation}},
	shorttitle = {{ViP}-{DeepLab}},
	doi = {10.1109/CVPR46437.2021.00399},
	abstract = {In this paper, we present ViP-DeepLab, a unified model attempting to tackle the long-standing and challenging inverse projection problem in vision, which we model as restoring the point clouds from perspective image sequences while providing each point with instance-level semantic interpretations. Solving this problem requires the vision models to predict the spatial location, semantic class, and temporally consistent instance label for each 3D point. ViP-DeepLab approaches it by jointly performing monocular depth estimation and video panoptic segmentation. We name this joint task as Depth-aware Video Panoptic Segmentation, and propose a new evaluation metric along with two derived datasets for it, which will be made available to the public. On the individual sub-tasks, ViP-DeepLab also achieves state-of-the-art results, outperforming previous methods by 5.1\% VPQ on Cityscapes-VPS, ranking 1st on the KITTI monocular depth estimation benchmark, and 1st on KITTI MOTS pedestrian. The datasets and the evaluation codes are made publicly available1.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Qiao, Siyuan and Zhu, Yukun and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {Estimation, Measurement, Pattern recognition, Predictive models, Semantics, Solid modeling, Three-dimensional displays},
	pages = {3996--4007},
}

@inproceedings{ost_neural_2021,
	title = {Neural {Scene} {Graphs} for {Dynamic} {Scenes}},
	url = {http://arxiv.org/abs/2011.10379},
	doi = {10.48550/arXiv.2011.10379},
	abstract = {Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.},
	urldate = {2023-02-03},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Ost, Julian and Mannan, Fahim and Thuerey, Nils and Knodt, Julian and Heide, Felix},
	month = mar,
	year = {2021},
	note = {arXiv:2011.10379 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No INSPIRE recid found},
}

@misc{wang_posediffusion_2023,
	title = {{PoseDiffusion}: {Solving} {Pose} {Estimation} via {Diffusion}-aided {Bundle} {Adjustment}},
	shorttitle = {{PoseDiffusion}},
	url = {http://arxiv.org/abs/2306.15667},
	doi = {10.48550/arXiv.2306.15667},
	abstract = {Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Wang, Jianyuan and Rupprecht, Christian and Novotny, David},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15667 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{niemeyer_giraffe_2021,
	title = {{GIRAFFE}: {Representing} {Scenes} {As} {Compositional} {Generative} {Neural} {Feature} {Fields}},
	shorttitle = {{GIRAFFE}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Niemeyer_GIRAFFE_Representing_Scenes_As_Compositional_Generative_Neural_Feature_Fields_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-06-27},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Niemeyer, Michael and Geiger, Andreas},
	year = {2021},
	pages = {11453--11464},
}

@inproceedings{kundu_panoptic_2022,
	title = {Panoptic {Neural} {Fields}: {A} {Semantic} {Object}-{Aware} {Neural} {Scene} {Representation}},
	shorttitle = {Panoptic {Neural} {Fields}},
	language = {en},
	urldate = {2023-03-12},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kundu, Abhijit and Genova, Kyle and Yin, Xiaoqi and Fathi, Alireza and Pantofaru, Caroline and Guibas, Leonidas J. and Tagliasacchi, Andrea and Dellaert, Frank and Funkhouser, Thomas},
	year = {2022},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {12871--12881},
}

@inproceedings{barron_mip-nerf_2022,
	title = {Mip-{NeRF} 360: {Unbounded} {Anti}-{Aliased} {Neural} {Radiance} {Fields}},
	shorttitle = {Mip-{NeRF} 360},
	doi = {10.1109/CVPR52688.2022.00539},
	abstract = {Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on "unbounded" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub "mip-NeRF 360" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57\% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.},
	urldate = {2022-12-15},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
	month = mar,
	year = {2022},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No INSPIRE recid found},
}

@misc{sabour_robustnerf_2023,
	title = {{RobustNeRF}: {Ignoring} {Distractors} with {Robust} {Losses}},
	shorttitle = {{RobustNeRF}},
	url = {http://arxiv.org/abs/2302.00833},
	doi = {10.48550/arXiv.2302.00833},
	abstract = {Neural radiance fields (NeRF) excel at synthesizing new views given multi-view, calibrated images of a static scene. When scenes include distractors, which are not persistent during image capture (moving objects, lighting variations, shadows), artifacts appear as view-dependent effects or 'floaters'. To cope with distractors, we advocate a form of robust estimation for NeRF training, modeling distractors in training data as outliers of an optimization problem. Our method successfully removes outliers from a scene and improves upon our baselines, on synthetic and real-world scenes. Our technique is simple to incorporate in modern NeRF frameworks, with few hyper-parameters. It does not assume a priori knowledge of the types of distractors, and is instead focused on the optimization problem rather than pre-processing or modeling transient objects. More results on our page https://robustnerf.github.io/public.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Sabour, Sara and Vora, Suhani and Duckworth, Daniel and Krasin, Ivan and Fleet, David J. and Tagliasacchi, Andrea},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00833 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Done Reading},
}

@inproceedings{rematas_urban_2022,
	title = {Urban {Radiance} {Fields}},
	doi = {10/gq6psr},
	abstract = {The goal of this work is to perform 3D reconstruction and novel view synthesis from data captured by scanning platforms commonly deployed for world mapping in urban outdoor environments (e.g., Street View). Given a sequence of posed RGB images and lidar sweeps acquired by cameras and scanners moving through an outdoor scene, we produce a model from which 3D surfaces can be extracted and novel RGB images can be synthesized. Our approach extends Neural Radiance Fields, which has been demonstrated to synthesize realistic novel images for small scenes in controlled settings, with new methods for leveraging asynchronously captured lidar data, for addressing exposure variation between captured images, and for leveraging predicted image segmentations to supervise densities on rays pointing at the sky. Each of these three extensions provides significant performance improvements in experiments on Street View data. Our system produces state-of-the-art 3D surface reconstructions and synthesizes higher quality novel views in comparison to both traditional methods (e.g. COLMAP) and recent neural representations (e.g. Mip-NeRF).},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Rematas, Konstantinos and Liu, Andrew and Srinivasan, Pratul and Barron, Jonathan and Tagliasacchi, Andrea and Funkhouser, Thomas and Ferrari, Vittorio},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Image and video synthesis and generation, Image segmentation, Laser radar, NeRF, Neural networks, Read Later, Solid modeling, Surface reconstruction, Three-dimensional displays, Urban areas, Vision + graphics, ⛔ No INSPIRE recid found},
	pages = {12922--12932},
}

@inproceedings{zheng_steps_2023,
	title = {{STEPS}: {Joint} {Self}-supervised {Nighttime} {Image} {Enhancement} and {Depth} {Estimation}},
	shorttitle = {{STEPS}},
	url = {http://arxiv.org/abs/2302.01334},
	abstract = {Self-supervised depth estimation draws a lot of attention recently as it can promote the 3D sensing capabilities of self-driving vehicles. However, it intrinsically relies upon the photometric consistency assumption, which hardly holds during nighttime. Although various supervised nighttime image enhancement methods have been proposed, their generalization performance in challenging driving scenarios is not satisfactory. To this end, we propose the first method that jointly learns a nighttime image enhancer and a depth estimator, without using ground truth for either task. Our method tightly entangles two self-supervised tasks using a newly proposed uncertain pixel masking strategy. This strategy originates from the observation that nighttime images not only suffer from underexposed regions but also from overexposed regions. By fitting a bridge-shaped curve to the illumination map distribution, both regions are suppressed and two tasks are bridged naturally. We benchmark the method on two established datasets: nuScenes and RobotCar and demonstrate state-of-the-art performance on both of them. Detailed ablations also reveal the mechanism of our proposal. Last but not least, to mitigate the problem of sparse ground truth of existing datasets, we provide a new photo-realistically enhanced nighttime dataset based upon CARLA. It brings meaningful new challenges to the community. Codes, data, and models are available at https://github.com/ucaszyp/STEPS.},
	urldate = {2023-06-26},
	booktitle = {2023 {IEEE} {Conference} on {Robotics} and {Automation} ({ICRA} 2023)},
	author = {Zheng, Yupeng and Zhong, Chengliang and Li, Pengfei and Gao, Huan-ang and Zheng, Yuhang and Jin, Bu and Wang, Ling and Zhao, Hao and Zhou, Guyue and Zhang, Qichao and Zhao, Dongbin},
	month = feb,
	year = {2023},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_contranerf_2023,
	title = {{ContraNeRF}: {Generalizable} {Neural} {Radiance} {Fields} for {Synthetic}-to-real {Novel} {View} {Synthesis} via {Contrastive} {Learning}},
	shorttitle = {{ContraNeRF}},
	url = {http://arxiv.org/abs/2303.11052},
	abstract = {Although many recent works have investigated generalizable NeRF-based novel view synthesis for unseen scenes, they seldom consider the synthetic-to-real generalization, which is desired in many practical applications. In this work, we first investigate the effects of synthetic data in synthetic-to-real novel view synthesis and surprisingly observe that models trained with synthetic data tend to produce sharper but less accurate volume densities. For pixels where the volume densities are correct, fine-grained details will be obtained. Otherwise, severe artifacts will be produced. To maintain the advantages of using synthetic data while avoiding its negative effects, we propose to introduce geometry-aware contrastive learning to learn multi-view consistent features with geometric constraints. Meanwhile, we adopt cross-view attention to further enhance the geometry perception of features by querying features across input views. Experiments demonstrate that under the synthetic-to-real setting, our method can render images with higher quality and better fine-grained details, outperforming existing generalizable novel view synthesis methods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our method also achieves state-of-the-art results.},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Yang, Hao and Hong, Lanqing and Li, Aoxue and Hu, Tianyang and Li, Zhenguo and Lee, Gim Hee and Wang, Liwei},
	month = jun,
	year = {2023},
	note = {arXiv:2303.11052 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_overview_2023,
	title = {An {Overview} about {Emerging} {Technologies} of {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2306.13302},
	doi = {10.48550/arXiv.2306.13302},
	abstract = {Since DARPA started Grand Challenges in 2004 and Urban Challenges in 2007, autonomous driving has been the most active field of AI applications. This paper gives an overview about technical aspects of autonomous driving technologies and open problems. We investigate the major fields of self-driving systems, such as perception, mapping and localization, prediction, planning and control, simulation, V2X and safety etc. Especially we elaborate on all these issues in a framework of data closed loop, a popular platform to solve the long tailed autonomous driving problems.},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Huang, Yu and Chen, Yue and Yang, Zijiang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.13302 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cabon_virtual_2020,
	title = {Virtual {KITTI} 2},
	url = {http://arxiv.org/abs/2001.10773},
	abstract = {This paper introduces an updated version of the well-known Virtual KITTI dataset which consists of 5 sequence clones from the KITTI tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15 degrees). For each sequence, we provide multiple sets of images containing RGB, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset's capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Cabon, Yohann and Murray, Naila and Humenberger, Martin},
	month = jan,
	year = {2020},
	note = {arXiv:2001.10773 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{tancik_nerfstudio_2023,
	title = {Nerfstudio: {A} {Modular} {Framework} for {Neural} {Radiance} {Field} {Development}},
	volume = {1},
	shorttitle = {Nerfstudio},
	url = {http://arxiv.org/abs/2302.04264},
	doi = {10.1145/3588432.3591516},
	abstract = {Neural Radiance Fields (NeRF) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of NeRF research, we propose a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play components for implementing NeRF-based methods, which make it easy for researchers and practitioners to incorporate NeRF into their projects. Additionally, the modular design enables support for extensive real-time visualization tools, streamlined pipelines for importing captured in-the-wild data, and tools for exporting to video, point cloud and mesh representations. The modularity of Nerfstudio enables the development of Nerfacto, our method that combines components from recent papers to achieve a balance between speed and quality, while also remaining flexible to future modifications. To promote community-driven development, all associated code and data are made publicly available with open-source licensing at https://nerf.studio.},
	number = {1},
	urldate = {2023-06-24},
	journal = {ACM Transactions on Graphics},
	author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and McAllister, David and Kanazawa, Angjoo},
	month = may,
	year = {2023},
	note = {arXiv:2302.04264 [cs]
titleTranslation:},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{liu_robust_2023,
	title = {Robust {Dynamic} {Radiance} {Fields}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Robust_Dynamic_Radiance_Fields_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-24},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, Yu-Lun and Gao, Chen and Meuleman, Andréas and Tseng, Hung-Yu and Saraf, Ayush and Kim, Changil and Chuang, Yung-Yu and Kopf, Johannes and Huang, Jia-Bin},
	year = {2023},
	pages = {13--23},
}

@misc{wang_end--end_2023,
	title = {End-to-end {2D}-{3D} {Registration} between {Image} and {LiDAR} {Point} {Cloud} for {Vehicle} {Localization}},
	url = {http://arxiv.org/abs/2306.11346},
	abstract = {Robot localization using a previously built map is essential for a variety of tasks including highly accurate navigation and mobile manipulation. A popular approach to robot localization is based on image-to-point cloud registration, which combines illumination-invariant LiDAR-based mapping with economical image-based localization. However, the recent works for image-to-point cloud registration either divide the registration into separate modules or project the point cloud to the depth image to register the RGB and depth images. In this paper, we present I2PNet, a novel end-to-end 2D-3D registration network. I2PNet directly registers the raw 3D point cloud with the 2D RGB image using differential modules with a unique target. The 2D-3D cost volume module for differential 2D-3D association is proposed to bridge feature extraction and pose regression. 2D-3D cost volume module implicitly constructs the soft point-to-pixel correspondence on the intrinsic-independent normalized plane of the pinhole camera model. Moreover, we introduce an outlier mask prediction module to filter the outliers in the 2D-3D association before pose regression. Furthermore, we propose the coarse-to-fine 2D-3D registration architecture to increase localization accuracy. We conduct extensive localization experiments on the KITTI Odometry and nuScenes datasets. The results demonstrate that I2PNet outperforms the state-of-the-art by a large margin. In addition, I2PNet has a higher efficiency than the previous works and can perform the localization in real-time. Moreover, we extend the application of I2PNet to the camera-LiDAR online calibration and demonstrate that I2PNet outperforms recent approaches on the online calibration task.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Wang, Guangming and Zheng, Yu and Guo, Yanfeng and Liu, Zhe and Zhu, Yixiang and Burgard, Wolfram and Wang, Hesheng},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11346 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{mei_rome_2023,
	title = {{RoMe}: {Towards} {Large} {Scale} {Road} {Surface} {Reconstruction} via {Mesh} {Representation}},
	shorttitle = {{RoMe}},
	url = {http://arxiv.org/abs/2306.11368},
	abstract = {Large-scale road surface reconstruction is becoming important to autonomous driving systems, as it provides valuable training and testing data effectively. In this paper, we introduce a simple yet efficient method, RoMe, for large-scale Road surface reconstruction via Mesh representations. To simplify the problem, RoMe decomposes a 3D road surface into a triangle-mesh and a multilayer perception network to model the road elevation implicitly. To retain fine surface details, each mesh vertex has two extra attributes, namely color and semantics. To improve the efficiency of RoMe in large-scale environments, a novel waypoint sampling method is introduced. As such, RoMe can properly preserve road surface details, with only linear computational complexity to road areas. In addition, to improve the accuracy of RoMe, extrinsics optimization is proposed to mitigate inaccurate extrinsic calibrations. Experimental results on popular public datasets also demonstrate the high efficiency and accuracy of RoMe.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Mei, Ruohong and Sui, Wei and Zhang, Jiaxin and Zhang, Qian and Peng, Tao and Yang, Cong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11368 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liao_multi-view_2023,
	title = {Multi-view {3D} {Object} {Reconstruction} and {Uncertainty} {Modelling} with {Neural} {Shape} {Prior}},
	url = {http://arxiv.org/abs/2306.11739},
	abstract = {3D object reconstruction is important for semantic scene understanding. It is challenging to reconstruct detailed 3D shapes from monocular images directly due to a lack of depth information, occlusion and noise. Most current methods generate deterministic object models without any awareness of the uncertainty of the reconstruction. We tackle this problem by leveraging a neural object representation which learns an object shape distribution from large dataset of 3d object models and maps it into a latent space. We propose a method to model uncertainty as part of the representation and define an uncertainty-aware encoder which generates latent codes with uncertainty directly from individual input images. Further, we propose a method to propagate the uncertainty in the latent code to SDF values and generate a 3d object mesh with local uncertainty for each mesh component. Finally, we propose an incremental fusion method under a Bayesian framework to fuse the latent codes from multi-view observations. We evaluate the system in both synthetic and real datasets to demonstrate the effectiveness of uncertainty-based fusion to improve 3D object reconstruction accuracy.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Liao, Ziwei and Waslander, Steven L.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11739 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{liu_towards_2023,
	title = {Towards {Real}-time {Scalable} {Dense} {Mapping} using {Robot}-centric {Implicit} {Representation}},
	url = {http://arxiv.org/abs/2306.10472},
	abstract = {Real-time and high-quailty dense mapping is essential for robots to perform fine tasks. However, most existing methods can not achieve both speed and quality. Recent works have shown that implicit neural representations of 3D scenes can produce remarkable results, but they are limited to small scenes and lack real-time performance. To address these limitations, we propose a real-time scalable mapping method using robot-centric implicit representation. We train implicit features with a multi-resolution local map and decode them as signed distance values through a shallow neural network. We maintain the learned features in a scalable manner using a global map that consists of a hash table and a submap set. We exploit the characteristics of the local map to achieve highly efficient training and mitigate the catastrophic forgetting problem in incremental implicit mapping. Extensive experiments validate that our method outperforms existing methods in reconstruction quality, real-time performance, and applicability. The code of our system will be available at {\textbackslash}url\{https://github.com/HITSZ-NRSL/RIM.git\}.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Liu, Jianheng and Chen, Haoyao},
	month = jun,
	year = {2023},
	note = {arXiv:2306.10472 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{yu_sdfstudio_2022,
	title = {{SDFStudio}: {A} {Unified} {Framework} for {Surface} {Reconstruction}},
	url = {https://github.com/autonomousvision/sdfstudio},
	author = {Yu, Zehao and Chen, Anpei and Antic, Bozidar and Peng, Songyou and Bhattacharyya, Apratim and Niemeyer, Michael and Tang, Siyu and Sattler, Torsten and Geiger, Andreas},
	month = dec,
	year = {2022},
	keywords = {/unread},
}

@inproceedings{jang_codenerf_2021,
	title = {{CodeNeRF}: {Disentangled} {Neural} {Radiance} {Fields} for {Object} {Categories}},
	shorttitle = {{CodeNeRF}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Jang_CodeNeRF_Disentangled_Neural_Radiance_Fields_for_Object_Categories_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-06-24},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Jang, Wonbong and Agapito, Lourdes},
	year = {2021},
	pages = {12949--12958},
}

@inproceedings{zhu_occlusion-free_2023,
	title = {Occlusion-{Free} {Scene} {Recovery} via {Neural} {Radiance} {Fields}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Occlusion-Free_Scene_Recovery_via_Neural_Radiance_Fields_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhu, Chengxuan and Wan, Renjie and Tang, Yunkai and Shi, Boxin},
	year = {2023},
	pages = {20722--20731},
}

@misc{tagliasacchi_volume_2022,
	title = {Volume {Rendering} {Digest} (for {NeRF})},
	url = {http://arxiv.org/abs/2209.02417},
	doi = {10.48550/arXiv.2209.02417},
	abstract = {Neural Radiance Fields employ simple volume rendering as a way to overcome the challenges of differentiating through ray-triangle intersections by leveraging a probabilistic notion of visibility. This is achieved by assuming the scene is composed by a cloud of light-emitting particles whose density changes in space. This technical report summarizes the derivations for differentiable volume rendering. It is a condensed version of previous reports, but rewritten in the context of NeRF, and adopting its commonly used notation.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Tagliasacchi, Andrea and Mildenhall, Ben},
	month = aug,
	year = {2022},
	note = {arXiv:2209.02417 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading, Emphasis, I.3, I.4, ⛔ No INSPIRE recid found},
}

@inproceedings{yan_nerf-ds_2023,
	title = {{NeRF}-{DS}: {Neural} {Radiance} {Fields} for {Dynamic} {Specular} {Objects}},
	shorttitle = {{NeRF}-{DS}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Yan_NeRF-DS_Neural_Radiance_Fields_for_Dynamic_Specular_Objects_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-22},
	author = {Yan, Zhiwen and Li, Chen and Lee, Gim Hee},
	year = {2023},
	pages = {8285--8295},
}

@inproceedings{chen_tensorf_2022,
	address = {Berlin, Heidelberg},
	title = {{TensoRF}: {Tensorial} {Radiance} {Fields}},
	isbn = {978-3-031-19823-6},
	shorttitle = {{TensoRF}},
	url = {https://doi.org/10.1007/978-3-031-19824-3_20},
	doi = {10/gr8whc},
	abstract = {We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CANDECOMP/PARAFAC (CP) decomposition – that factorizes tensors into rank-one components with compact vectors – in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction ({\textless}30 min) with better rendering quality and even a smaller model size ({\textless}4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time ({\textless}10 min) and retaining a compact model size ({\textless}75 MB).},
	urldate = {2022-11-28},
	booktitle = {Computer {Vision} – {ECCV} 2022: 17th {European} {Conference}, {Tel} {Aviv}, {Israel}, {October} 23–27, 2022, {Proceedings}, {Part} {XXXII}},
	publisher = {Springer-Verlag},
	author = {Chen, Anpei and Xu, Zexiang and Geiger, Andreas and Yu, Jingyi and Su, Hao},
	year = {2022},
	keywords = {/unread, Done Reading, ⛔ No INSPIRE recid found},
	pages = {333--350},
}

@misc{meuleman_progressively_2023,
	title = {Progressively {Optimized} {Local} {Radiance} {Fields} for {Robust} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2303.13791},
	doi = {10.48550/arXiv.2303.13791},
	abstract = {We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive optimization significantly improves the robustness of the reconstruction. For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the Tanks and Temples dataset and our collected outdoor dataset, Static Hikes, show that our approach compares favorably with the state-of-the-art.},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Meuleman, Andreas and Liu, Yu-Lun and Gao, Chen and Huang, Jia-Bin and Kim, Changil and Kim, Min H. and Kopf, Johannes},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13791 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{shen_gina-3d_2023,
	title = {{GINA}-{3D}: {Learning} {To} {Generate} {Implicit} {Neural} {Assets} in the {Wild}},
	shorttitle = {{GINA}-{3D}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Shen_GINA-3D_Learning_To_Generate_Implicit_Neural_Assets_in_the_Wild_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-20},
	author = {Shen, Bokui and Yan, Xinchen and Qi, Charles R. and Najibi, Mahyar and Deng, Boyang and Guibas, Leonidas and Zhou, Yin and Anguelov, Dragomir},
	year = {2023},
	pages = {4913--4926},
}

@article{jin_multi-grid_2023,
	title = {Multi-grid representation with field regularization for self-supervised surface reconstruction from point clouds},
	issn = {0097-8493},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849323001097},
	doi = {10.1016/j.cag.2023.06.016},
	abstract = {Surface reconstruction from point clouds plays a crucial role in computer vision. The current state-of-the-art methods solve this problem by learning signed distance functions (SDFs) with ground truth distance supervisions, which are difficult to obtain. Moreover, most recent works represent each shape with a single or several latent codes, which fail to provide detailed guidance to reconstruct the local geometry. To address these issues, we propose MGSDF, a novel method for high-fidelity and fast surface reconstruction from raw point clouds. Specifically, we design a scalable representation with learnable hierarchical feature grids to capture multi-level geometric details. We introduce a self-supervised learning scheme that optimizes the SDF directly from the raw point cloud by pulling the space onto the surface. In addition, we propose a field regularization constraint on the predicted distance values and gradients on the zero-level set of SDFs for robust optimization. Our experimental results demonstrate significant improvements over the state-of-the-art in surface reconstruction from clean, noisy and varying density point clouds under widely used benchmarks.},
	language = {en},
	urldate = {2023-06-20},
	journal = {Computers \& Graphics},
	author = {Jin, Chuan and Wu, Tieru and Zhou, Junsheng},
	month = jun,
	year = {2023},
	keywords = {3D surface reconstruction, Field regularization, Multi-grid representation, Self-supervised schema},
}

@misc{kulhanek_tetra-nerf_2023,
	title = {Tetra-{NeRF}: {Representing} {Neural} {Radiance} {Fields} {Using} {Tetrahedra}},
	shorttitle = {Tetra-{NeRF}},
	url = {https://arxiv.org/abs/2304.09987v1},
	abstract = {Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.},
	language = {en},
	urldate = {2023-04-22},
	author = {Kulhanek, Jonas and Sattler, Torsten},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@inproceedings{gkioxari_learning_2022,
	title = {Learning {3D} {Object} {Shape} and {Layout} without {3D} {Supervision}},
	doi = {10.1109/CVPR52688.2022.00174},
	abstract = {A 3D scene consists of a set of objects, each with a shape and a layout giving their position in space. Understanding 3D scenes from 2D images is an important goal, with ap-plications in robotics and graphics. While there have been recent advances in predicting 3D shape and layout from a single image, most approaches rely on 3D ground truth for training which is expensive to collect at scale. We overcome these limitations and propose a method that learns to predict 3D shape and layout for objects without any ground truth shape or layout information: instead we rely on multi-view images with 2D supervision which can more easily be col-lected at scale. Through extensive experiments on ShapeNet, Hypersim, and ScanNet we demonstrate that our approach scales to large datasets of realistic images, and compares favorably to methods relying on 3D ground truth. On Hy-persim and ScanNet where reliable 3D ground truth is not available, our approach outperforms supervised approaches trained on smaller and less diverse datasets.11Project page https://gkioxari.github.io/usl/},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Gkioxari, Georgia and Ravi, Nikhila and Johnson, Justin},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {3D from single images, Computer vision, Image analysis, Image recognition, Image segmentation, Recognition: detection, Scene analysis and understanding, Segmentation, Shape, Three-dimensional displays, Training, categorization, grouping and shape analysis, retrieval},
	pages = {1685--1694},
}

@misc{perez_enhancing_2023,
	title = {Enhancing {Neural} {Rendering} {Methods} with {Image} {Augmentations}},
	url = {http://arxiv.org/abs/2306.08904},
	abstract = {Faithfully reconstructing 3D geometry and generating novel views of scenes are critical tasks in 3D computer vision. Despite the widespread use of image augmentations across computer vision applications, their potential remains underexplored when learning neural rendering methods (NRMs) for 3D scenes. This paper presents a comprehensive analysis of the use of image augmentations in NRMs, where we explore different augmentation strategies. We found that introducing image augmentations during training presents challenges such as geometric and photometric inconsistencies for learning NRMs from images. Specifically, geometric inconsistencies arise from alterations in shapes, positions, and orientations from the augmentations, disrupting spatial cues necessary for accurate 3D reconstruction. On the other hand, photometric inconsistencies arise from changes in pixel intensities introduced by the augmentations, affecting the ability to capture the underlying 3D structures of the scene. We alleviate these issues by focusing on color manipulations and introducing learnable appearance embeddings that allow NRMs to explain away photometric variations. Our experiments demonstrate the benefits of incorporating augmentations when learning NRMs, including improved photometric quality and surface reconstruction, as well as enhanced robustness against data quality issues, such as reduced training data and image degradations.},
	language = {en},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Pérez, Juan C. and Rojas, Sara and Zarzar, Jesus and Ghanem, Bernard},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08904 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zimmer_a9_2023,
	title = {A9 {Intersection} {Dataset}: {All} {You} {Need} for {Urban} {3D} {Camera}-{LiDAR} {Roadside} {Perception}},
	shorttitle = {A9 {Intersection} {Dataset}},
	url = {http://arxiv.org/abs/2306.09266},
	abstract = {Intelligent Transportation Systems (ITS) allow a drastic expansion of the visibility range and decrease occlusions for autonomous driving. To obtain accurate detections, detailed labeled sensor data for training is required. Unfortunately, high-quality 3D labels of LiDAR point clouds from the infrastructure perspective of an intersection are still rare. Therefore, we provide the A9 Intersection Dataset, which consists of labeled LiDAR point clouds and synchronized camera images. Here, we recorded the sensor output from two roadside cameras and LiDARs mounted on intersection gantry bridges. The point clouds were labeled in 3D by experienced annotators. Furthermore, we provide calibration data between all sensors, which allow the projection of the 3D labels into the camera images and an accurate data fusion. Our dataset consists of 4.8k images and point clouds with more than 57.4k manually labeled 3D boxes. With ten object classes, it has a high diversity of road users in complex driving maneuvers, such as left and right turns, overtaking, and U-turns. In experiments, we provided multiple baselines for the perception tasks. Overall, our dataset is a valuable contribution to the scientific community to perform complex 3D camera-LiDAR roadside perception tasks. Find data, code, and more information at https://a9-dataset.com.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Zimmer, Walter and Creß, Christian and Nguyen, Huu Tung and Knoll, Alois C.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09266 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{fridovich-keil_k-planes_2023,
	title = {K-{Planes}: {Explicit} {Radiance} {Fields} in {Space}, {Time}, and {Appearance}},
	shorttitle = {K-{Planes}},
	url = {https://arxiv.org/abs/2301.10241},
	doi = {10.48550/arXiv.2301.10241},
	abstract = {We introduce k-planes, a white-box model for radiance fields in arbitrary dimensions. Our model uses d choose 2 planes to represent a d-dimensional scene, providing a seamless way to go from static (d=3) to dynamic (d=4) scenes. This planar factorization makes adding dimension-specific priors easy, e.g. temporal smoothness and multi-resolution spatial structure, and induces a natural decomposition of static and dynamic components of a scene. We use a linear feature decoder with a learned color basis that yields similar performance as a nonlinear black-box MLP decoder. Across a range of synthetic and real, static and dynamic, fixed and varying appearance scenes, k-planes yields competitive and often state-of-the-art reconstruction fidelity with low memory usage, achieving 1000x compression over a full 4D grid, and fast optimization with a pure PyTorch implementation. For video results and code, please see https://sarafridov.github.io/K-Planes.},
	language = {en},
	urldate = {2023-01-28},
	booktitle = {Computer {Vision} {And} {Pattern} {Recognition}, 2023},
	author = {Fridovich-Keil, Sara and Meanti, Giacomo and Warburg, Frederik and Recht, Benjamin and Kanazawa, Angjoo},
	year = {2023},
	keywords = {/unread, Done Reading, ⚠️ Invalid DOI, ⛔ No INSPIRE recid found},
}

@misc{pang_global_2023,
	title = {Global {Planning} for {Contact}-{Rich} {Manipulation} via {Local} {Smoothing} of {Quasi}-dynamic {Contact} {Models}},
	url = {http://arxiv.org/abs/2206.10787},
	doi = {10.48550/arXiv.2206.10787},
	abstract = {The empirical success of Reinforcement Learning (RL) in the setting of contact-rich manipulation leaves much to be understood from a model-based perspective, where the key difficulties are often attributed to (i) the explosion of contact modes, (ii) stiff, non-smooth contact dynamics and the resulting exploding / discontinuous gradients, and (iii) the non-convexity of the planning problem. The stochastic nature of RL addresses (i) and (ii) by effectively sampling and averaging the contact modes. On the other hand, model-based methods have tackled the same challenges by smoothing contact dynamics analytically. Our first contribution is to establish the theoretical equivalence of the two methods for simple systems, and provide qualitative and empirical equivalence on a number of complex examples. In order to further alleviate (ii), our second contribution is a convex, differentiable and quasi-dynamic formulation of contact dynamics, which is amenable to both smoothing schemes, and has proven through experiments to be highly effective for contact-rich planning. Our final contribution resolves (iii), where we show that classical sampling-based motion planning algorithms can be effective in global planning when contact modes are abstracted via smoothing. Applying our method on a collection of challenging contact-rich manipulation tasks, we demonstrate that efficient model-based motion planning can achieve results comparable to RL with dramatically less computation. Video: https://youtu.be/12Ew4xC-VwA},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Pang, Tao and Suh, H. J. Terry and Yang, Lujie and Tedrake, Russ},
	month = feb,
	year = {2023},
	note = {arXiv:2206.10787 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{huang_octscenes_2023,
	title = {{OCTScenes}: {A} {Versatile} {Real}-{World} {Dataset} of {Tabletop} {Scenes} for {Object}-{Centric} {Learning}},
	shorttitle = {{OCTScenes}},
	url = {http://arxiv.org/abs/2306.09682},
	doi = {10.48550/arXiv.2306.09682},
	abstract = {Humans possess the cognitive ability to comprehend scenes in a compositional manner. To empower AI systems with similar abilities, object-centric representation learning aims to acquire representations of individual objects from visual scenes without any supervision. Although recent advancements in object-centric representation learning have achieved remarkable progress on complex synthesis datasets, there is a huge challenge for application in complex real-world scenes. One of the essential reasons is the scarcity of real-world datasets specifically tailored to object-centric representation learning methods. To solve this problem, we propose a versatile real-world dataset of tabletop scenes for object-centric learning called OCTScenes, which is meticulously designed to serve as a benchmark for comparing, evaluating and analyzing object-centric representation learning methods. OCTScenes contains 5000 tabletop scenes with a total of 15 everyday objects. Each scene is captured in 60 frames covering a 360-degree perspective. Consequently, OCTScenes is a versatile benchmark dataset that can simultaneously satisfy the evaluation of object-centric representation learning methods across static scenes, dynamic scenes, and multi-view scenes tasks. Extensive experiments of object-centric representation learning methods for static, dynamic and multi-view scenes are conducted on OCTScenes. The results demonstrate the shortcomings of state-of-the-art methods for learning meaningful representations from real-world data, despite their impressive performance on complex synthesis datasets. Furthermore, OCTScenes can serves as a catalyst for advancing existing state-of-the-art methods, inspiring them to adapt to real-world scenes. Dataset and code are available at https://huggingface.co/datasets/Yinxuan/OCTScenes.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Huang, Yinxuan and Chen, Tonglin and Shen, Zhimeng and Huang, Jinghao and Li, Bin and Xue, Xiangyang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09682 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yu_pixelnerf_2021,
	title = {{pixelNeRF}: {Neural} {Radiance} {Fields} from {One} or {Few} {Images}},
	shorttitle = {{pixelNeRF}},
	url = {http://arxiv.org/abs/2012.02190},
	doi = {10.48550/arXiv.2012.02190},
	abstract = {We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf},
	urldate = {2023-05-25},
	booktitle = {Computer {Vision} {And} {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
	month = may,
	year = {2021},
	note = {titleTranslation:},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@article{ranftl_towards_2022,
	title = {Towards {Robust} {Monocular} {Depth} {Estimation}: {Mixing} {Datasets} for {Zero}-{Shot} {Cross}-{Dataset} {Transfer}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Towards {Robust} {Monocular} {Depth} {Estimation}},
	url = {https://ieeexplore.ieee.org/document/9178977/},
	doi = {10.1109/TPAMI.2020.3019967},
	number = {3},
	urldate = {2023-06-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ranftl, Rene and Lasinger, Katrin and Hafner, David and Schindler, Konrad and Koltun, Vladlen},
	month = mar,
	year = {2022},
	pages = {1623--1637},
}

@misc{wang_panoocc_2023,
	title = {{PanoOcc}: {Unified} {Occupancy} {Representation} for {Camera}-based {3D} {Panoptic} {Segmentation}},
	shorttitle = {{PanoOcc}},
	url = {http://arxiv.org/abs/2306.10013},
	doi = {10.48550/arXiv.2306.10013},
	abstract = {Comprehensive modeling of the surrounding 3D world is key to the success of autonomous driving. However, existing perception tasks like object detection, road structure segmentation, depth \& elevation estimation, and open-set object localization each only focus on a small facet of the holistic 3D scene understanding task. This divide-and-conquer strategy simplifies the algorithm development procedure at the cost of losing an end-to-end unified solution to the problem. In this work, we address this limitation by studying camera-based 3D panoptic segmentation, aiming to achieve a unified occupancy representation for camera-only 3D scene understanding. To achieve this, we introduce a novel method called PanoOcc, which utilizes voxel queries to aggregate spatiotemporal information from multi-frame and multi-view images in a coarse-to-fine scheme, integrating feature learning and scene representation into a unified occupancy representation. We have conducted extensive ablation studies to verify the effectiveness and efficiency of the proposed method. Our approach achieves new state-of-the-art results for camera-based semantic segmentation and panoptic segmentation on the nuScenes dataset. Furthermore, our method can be easily extended to dense occupancy prediction and has shown promising performance on the Occ3D benchmark. The code will be released at https://github.com/Robertwyq/PanoOcc.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Wang, Yuqi and Chen, Yuntao and Liao, Xingyu and Fan, Lue and Zhang, Zhaoxiang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.10013 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{xu_c2f2neus_2023,
	title = {{C2F2NeUS}: {Cascade} {Cost} {Frustum} {Fusion} for {High} {Fidelity} and {Generalizable} {Neural} {Surface} {Reconstruction}},
	shorttitle = {{C2F2NeUS}},
	url = {http://arxiv.org/abs/2306.10003},
	doi = {10.48550/arXiv.2306.10003},
	abstract = {There is an emerging effort to combine the two popular technical paths, i.e., the multi-view stereo (MVS) and neural implicit surface (NIS), in scene reconstruction from sparse views. In this paper, we introduce a novel integration scheme that combines the multi-view stereo with neural signed distance function representations, which potentially overcomes the limitations of both methods. MVS uses per-view depth estimation and cross-view fusion to generate accurate surface, while NIS relies on a common coordinate volume. Based on this, we propose to construct per-view cost frustum for finer geometry estimation, and then fuse cross-view frustums and estimate the implicit signed distance functions to tackle noise and hole issues. We further apply a cascade frustum fusion strategy to effectively captures global-local information and structural consistency. Finally, we apply cascade sampling and a pseudo-geometric loss to foster stronger integration between the two architectures. Extensive experiments demonstrate that our method reconstructs robust surfaces and outperforms existing state-of-the-art methods.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Xu, Luoyuan and Guan, Tao and Wang, Yuesong and Liu, Wenkai and Zeng, Zhaojie and Wang, Junle and Yang, Wei},
	month = jun,
	year = {2023},
	note = {arXiv:2306.10003 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{lee_single_2023,
	title = {Single {View} {Scene} {Scale} {Estimation} {Using} {Scale} {Field}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-17},
	author = {Lee, Byeong-Uk and Zhang, Jianming and Hold-Geoffroy, Yannick and Kweon, In So},
	year = {2023},
	pages = {21435--21444},
}

@inproceedings{dong_fast_2023,
	title = {Fast {Monocular} {Scene} {Reconstruction} {With} {Global}-{Sparse} {Local}-{Dense} {Grids}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Fast_Monocular_Scene_Reconstruction_With_Global-Sparse_Local-Dense_Grids_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-17},
	author = {Dong, Wei and Choy, Christopher and Loop, Charles and Litany, Or and Zhu, Yuke and Anandkumar, Anima},
	year = {2023},
	pages = {4263--4272},
}

@misc{shin_binary_2023,
	title = {Binary {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2306.07581},
	abstract = {In this paper, we propose binary radiance fields (BiRF), a storage-efficient radiance field representation employing binary feature encoding that encodes local features using binary encoding parameters in a format of either \$+1\$ or \$-1\$. This binarization strategy lets us represent the feature grid with highly compact feature encoding and a dramatic reduction in storage size. Furthermore, our 2D-3D hybrid feature grid design enhances the compactness of feature encoding as the 3D grid includes main components while 2D grids capture details. In our experiments, binary radiance field representation successfully outperforms the reconstruction performance of state-of-the-art (SOTA) efficient radiance field models with lower storage allocation. In particular, our model achieves impressive results in static scene reconstruction, with a PSNR of 31.53 dB for Synthetic-NeRF scenes, 34.26 dB for Synthetic-NSVF scenes, 28.02 dB for Tanks and Temples scenes while only utilizing 0.7 MB, 0.8 MB, and 0.8 MB of storage space, respectively. We hope the proposed binary radiance field representation will make radiance fields more accessible without a storage bottleneck.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Shin, Seungjoo and Park, Jaesik},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07581 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_nemo_2022,
	title = {{NeMo}: {3D} {Neural} {Motion} {Fields} from {Multiple} {Video} {Instances} of the {Same} {Action}},
	shorttitle = {{NeMo}},
	url = {http://arxiv.org/abs/2212.13660},
	doi = {10.48550/arXiv.2212.13660},
	abstract = {The task of reconstructing 3D human motion has wideranging applications. The gold standard Motion capture (MoCap) systems are accurate but inaccessible to the general public due to their cost, hardware and space constraints. In contrast, monocular human mesh recovery (HMR) methods are much more accessible than MoCap as they take single-view videos as inputs. Replacing the multi-view Mo- Cap systems with a monocular HMR method would break the current barriers to collecting accurate 3D motion thus making exciting applications like motion analysis and motiondriven animation accessible to the general public. However, performance of existing HMR methods degrade when the video contains challenging and dynamic motion that is not in existing MoCap datasets used for training. This reduces its appeal as dynamic motion is frequently the target in 3D motion recovery in the aforementioned applications. Our study aims to bridge the gap between monocular HMR and multi-view MoCap systems by leveraging information shared across multiple video instances of the same action. We introduce the Neural Motion (NeMo) field. It is optimized to represent the underlying 3D motions across a set of videos of the same action. Empirically, we show that NeMo can recover 3D motion in sports using videos from the Penn Action dataset, where NeMo outperforms existing HMR methods in terms of 2D keypoint detection. To further validate NeMo using 3D metrics, we collected a small MoCap dataset mimicking actions in Penn Action,and show that NeMo achieves better 3D reconstruction compared to various baselines.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Wang, Kuan-Chieh and Weng, Zhenzhen and Xenochristou, Maria and Araujo, Joao Pedro and Gu, Jeffrey and Liu, C. Karen and Yeung, Serena},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13660 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_interactive_2023,
	title = {Interactive {Segment} {Anything} {NeRF} with {Feature} {Imitation}},
	url = {http://arxiv.org/abs/2305.16233},
	doi = {10.48550/arXiv.2305.16233},
	abstract = {This paper investigates the potential of enhancing Neural Radiance Fields (NeRF) with semantics to expand their applications. Although NeRF has been proven useful in real-world applications like VR and digital creation, the lack of semantics hinders interaction with objects in complex scenes. We propose to imitate the backbone feature of off-the-shelf perception models to achieve zero-shot semantic segmentation with NeRF. Our framework reformulates the segmentation process by directly rendering semantic features and only applying the decoder from perception models. This eliminates the need for expensive backbones and benefits 3D consistency. Furthermore, we can project the learned semantics onto extracted mesh surfaces for real-time interaction. With the state-of-the-art Segment Anything Model (SAM), our framework accelerates segmentation by 16 times with comparable mask quality. The experimental results demonstrate the efficacy and computational advantages of our approach. Project page: {\textbackslash}url\{https://me.kiui.moe/san/\}.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Chen, Xiaokang and Tang, Jiaxiang and Wan, Diwen and Wang, Jingbo and Zeng, Gang},
	month = may,
	year = {2023},
	note = {arXiv:2305.16233 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{pan_learning_2023,
	title = {Learning to {Estimate} {6DoF} {Pose} from {Limited} {Data}: {A} {Few}-{Shot}, {Generalizable} {Approach} using {RGB} {Images}},
	shorttitle = {Learning to {Estimate} {6DoF} {Pose} from {Limited} {Data}},
	url = {http://arxiv.org/abs/2306.07598},
	doi = {10.48550/arXiv.2306.07598},
	abstract = {The accurate estimation of six degrees-of-freedom (6DoF) object poses is essential for many applications in robotics and augmented reality. However, existing methods for 6DoF pose estimation often depend on CAD templates or dense support views, restricting their usefulness in realworld situations. In this study, we present a new cascade framework named Cas6D for few-shot 6DoF pose estimation that is generalizable and uses only RGB images. To address the false positives of target object detection in the extreme few-shot setting, our framework utilizes a selfsupervised pre-trained ViT to learn robust feature representations. Then, we initialize the nearest top-K pose candidates based on similarity score and refine the initial poses using feature pyramids to formulate and update the cascade warped feature volume, which encodes context at increasingly finer scales. By discretizing the pose search range using multiple pose bins and progressively narrowing the pose search range in each stage using predictions from the previous stage, Cas6D can overcome the large gap between pose candidates and ground truth poses, which is a common failure mode in sparse-view scenarios. Experimental results on the LINEMOD and GenMOP datasets demonstrate that Cas6D outperforms state-of-the-art methods by 9.2\% and 3.8\% accuracy (Proj-5) under the 32-shot setting compared to OnePose++ and Gen6D.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Pan, Panwang and Fan, Zhiwen and Feng, Brandon Y. and Wang, Peihao and Li, Chenxin and Wang, Zhangyang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07598 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{roessle_ganerf_2023,
	title = {{GANeRF}: {Leveraging} {Discriminators} to {Optimize} {Neural} {Radiance} {Fields}},
	shorttitle = {{GANeRF}},
	url = {http://arxiv.org/abs/2306.06044},
	doi = {10.48550/arXiv.2306.06044},
	abstract = {Neural Radiance Fields (NeRF) have shown impressive novel view synthesis results; nonetheless, even thorough recordings yield imperfections in reconstructions, for instance due to poorly observed areas or minor lighting changes. Our goal is to mitigate these imperfections from various sources with a joint solution: we take advantage of the ability of generative adversarial networks (GANs) to produce realistic images and use them to enhance realism in 3D scene reconstruction with NeRFs. To this end, we learn the patch distribution of a scene using an adversarial discriminator, which provides feedback to the radiance field reconstruction, thus improving realism in a 3D-consistent fashion. Thereby, rendering artifacts are repaired directly in the underlying 3D representation by imposing multi-view path rendering constraints. In addition, we condition a generator with multi-resolution NeRF renderings which is adversarially trained to further improve rendering quality. We demonstrate that our approach significantly improves rendering quality, e.g., nearly halving LPIPS scores compared to Nerfacto while at the same time improving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Roessle, Barbara and Müller, Norman and Porzi, Lorenzo and Bulò, Samuel Rota and Kontschieder, Peter and Nießner, Matthias},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06044 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{saratchandran_curvature-aware_2023,
	title = {Curvature-{Aware} {Training} for {Coordinate} {Networks}},
	url = {http://arxiv.org/abs/2305.08552},
	doi = {10.48550/arXiv.2305.08552},
	abstract = {Coordinate networks are widely used in computer vision due to their ability to represent signals as compressed, continuous entities. However, training these networks with first-order optimizers can be slow, hindering their use in real-time applications. Recent works have opted for shallow voxel-based representations to achieve faster training, but this sacrifices memory efficiency. This work proposes a solution that leverages second-order optimization methods to significantly reduce training times for coordinate networks while maintaining their compressibility. Experiments demonstrate the effectiveness of this approach on various signal modalities, such as audio, images, videos, shape reconstruction, and neural radiance fields.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Saratchandran, Hemanth and Chng, Shin-Fang and Ramasinghe, Sameera and MacDonald, Lachlan and Lucey, Simon},
	month = may,
	year = {2023},
	note = {arXiv:2305.08552 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_nerf_2020,
	title = {{NeRF}++: {Analyzing} and {Improving} {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}++},
	url = {http://arxiv.org/abs/2010.07492},
	doi = {10.48550/arXiv.2010.07492},
	abstract = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
	urldate = {2022-11-22},
	publisher = {arXiv},
	author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
	month = oct,
	year = {2020},
	note = {arXiv:2010.07492 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Done Reading, ⛔ No INSPIRE recid found},
}

@inproceedings{zhou_continuity_2019,
	title = {On the {Continuity} of {Rotation} {Representations} in {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_On_the_Continuity_of_Rotation_Representations_in_Neural_Networks_CVPR_2019_paper.html},
	urldate = {2023-06-16},
	author = {Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
	year = {2019},
	pages = {5745--5753},
}

@article{masuda_event-based_2023,
	title = {Event-based {Camera} {Tracker} by ∇{tNeRF}},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3283919},
	abstract = {When a camera travels across a 3D world, only a fraction of pixel value changes; an event-based camera observes the change as sparse events. How can we utilize sparse events for efficient recovery of the camera pose? We show that we can recover the camera pose by minimizing the error between sparse events and the temporal gradient of the scene represented as a neural radiance field (NeRF). To enable the computation of the temporal gradient of the scene, we augment NeRF’s camera pose as a time function. When the input pose to the NeRF coincides with the actual pose, the output of the temporal gradient of NeRF equals the observed intensity changes on the event’s points. Using this principle, we propose an event-based camera pose tracking framework called TeGRA which realizes the pose update by using the sparse event’s observation. To the best of our knowledge, this is the first camera pose estimation algorithm using the scene’s implicit representation and the sparse intensity change from events.},
	journal = {IEEE Access},
	author = {Masuda, Mana and Sekikawa, Yusuke and Saito, Hideo},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Cameras, Event-based Camera, Motion Tracking, NeRF, Optimized production technology, Pose estimation, Robot vision systems, Solid modeling, Three-dimensional displays, Tracking},
	pages = {1--1},
}

@inproceedings{dou_multiplicative_2023,
	title = {Multiplicative {Fourier} {Level} of {Detail}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Dou_Multiplicative_Fourier_Level_of_Detail_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-14},
	author = {Dou, Yishun and Zheng, Zhong and Jin, Qiaoqiao and Ni, Bingbing},
	year = {2023},
	pages = {1808--1817},
}

@misc{yu_nerfbridge_2023,
	title = {{NerfBridge}: {Bringing} {Real}-time, {Online} {Neural} {Radiance} {Field} {Training} to {Robotics}},
	shorttitle = {{NerfBridge}},
	url = {http://arxiv.org/abs/2305.09761},
	abstract = {This work was presented at the IEEE International Conference on Robotics and Automation 2023 Workshop on Unconventional Spatial Representations. Neural radiance fields (NeRFs) are a class of implicit scene representations that model 3D environments from color images. NeRFs are expressive, and can model the complex and multi-scale geometry of real world environments, which potentially makes them a powerful tool for robotics applications. Modern NeRF training libraries can generate a photo-realistic NeRF from a static data set in just a few seconds, but are designed for offline use and require a slow pose optimization pre-computation step. In this work we propose NerfBridge, an open-source bridge between the Robot Operating System (ROS) and the popular Nerfstudio library for real-time, online training of NeRFs from a stream of images. NerfBridge enables rapid development of research on applications of NeRFs in robotics by providing an extensible interface to the efficient training pipelines and model libraries provided by Nerfstudio. As an example use case we outline a hardware setup that can be used NerfBridge to train a NeRF from images captured by a camera mounted to a quadrotor in both indoor and outdoor environments. For accompanying video https://youtu.be/EH0SLn-RcDg and code https://github.com/javieryu/nerf\_bridge.},
	urldate = {2023-06-14},
	publisher = {arXiv},
	author = {Yu, Javier and Low, Jun En and Nagami, Keiko and Schwager, Mac},
	month = may,
	year = {2023},
	note = {arXiv:2305.09761 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{wang_autorecon_2023,
	title = {{AutoRecon}: {Automated} {3D} {Object} {Discovery} and {Reconstruction}},
	shorttitle = {{AutoRecon}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_AutoRecon_Automated_3D_Object_Discovery_and_Reconstruction_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-14},
	author = {Wang, Yuang and He, Xingyi and Peng, Sida and Lin, Haotong and Bao, Hujun and Zhou, Xiaowei},
	year = {2023},
	pages = {21382--21391},
}

@inproceedings{zhang_ref-npr_2023,
	title = {Ref-{NPR}: {Reference}-{Based} {Non}-{Photorealistic} {Radiance} {Fields} for {Controllable} {Scene} {Stylization}},
	shorttitle = {Ref-{NPR}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Ref-NPR_Reference-Based_Non-Photorealistic_Radiance_Fields_for_Controllable_Scene_Stylization_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-14},
	author = {Zhang, Yuechen and He, Zexin and Xing, Jinbo and Yao, Xufeng and Jia, Jiaya},
	year = {2023},
	pages = {4242--4251},
}

@misc{cheng_lu-nerf_2023,
	title = {{LU}-{NeRF}: {Scene} and {Pose} {Estimation} by {Synchronizing} {Local} {Unposed} {NeRFs}},
	shorttitle = {{LU}-{NeRF}},
	url = {http://arxiv.org/abs/2306.05410},
	abstract = {A critical obstacle preventing NeRF models from being deployed broadly in the wild is their reliance on accurate camera poses. Consequently, there is growing interest in extending NeRF models to jointly optimize camera poses and scene representation, which offers an alternative to off-the-shelf SfM pipelines which have well-understood failure modes. Existing approaches for unposed NeRF operate under limited assumptions, such as a prior pose distribution or coarse pose initialization, making them less effective in a general setting. In this work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses and neural radiance fields with relaxed assumptions on pose configuration. Our approach operates in a local-to-global manner, where we first optimize over local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and geometry for this challenging few-shot task. The mini-scene poses are brought into a global reference frame through a robust pose synchronization step, where a final global optimization of pose and scene can be performed. We show our LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making restrictive assumptions on the pose prior. This allows us to operate in the general SE(3) pose setting, unlike the baselines. Our results also indicate our model can be complementary to feature-based SfM pipelines as it compares favorably to COLMAP on low-texture and low-resolution images.},
	urldate = {2023-06-14},
	publisher = {arXiv},
	author = {Cheng, Zezhou and Esteves, Carlos and Jampani, Varun and Kar, Abhishek and Maji, Subhransu and Makadia, Ameesh},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05410 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{stratton_volume-droid_2023,
	title = {Volume-{DROID}: {A} {Real}-{Time} {Implementation} of {Volumetric} {Mapping} with {DROID}-{SLAM}},
	shorttitle = {Volume-{DROID}},
	url = {http://arxiv.org/abs/2306.06850},
	doi = {10.48550/arXiv.2306.06850},
	abstract = {This paper presents Volume-DROID, a novel approach for Simultaneous Localization and Mapping (SLAM) that integrates Volumetric Mapping and Differentiable Recurrent Optimization-Inspired Design (DROID). Volume-DROID takes camera images (monocular or stereo) or frames from a video as input and combines DROID-SLAM, point cloud registration, an off-the-shelf semantic segmentation network, and Convolutional Bayesian Kernel Inference (ConvBKI) to generate a 3D semantic map of the environment and provide accurate localization for the robot. The key innovation of our method is the real-time fusion of DROID-SLAM and Convolutional Bayesian Kernel Inference (ConvBKI), achieved through the introduction of point cloud generation from RGB-Depth frames and optimized camera poses. This integration, engineered to enable efficient and timely processing, minimizes lag and ensures effective performance of the system. Our approach facilitates functional real-time online semantic mapping with just camera images or stereo video input. Our paper offers an open-source Python implementation of the algorithm, available at https://github.com/peterstratton/Volume-DROID.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Stratton, Peter and Garimella, Sandilya Sai and Saxena, Ashwin and Amutha, Nibarkavi and Gerami, Emaad},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{fujieda_neural_2023,
	title = {Neural {Intersection} {Function}},
	url = {http://arxiv.org/abs/2306.07191},
	doi = {10.48550/arXiv.2306.07191},
	abstract = {The ray casting operation in the Monte Carlo ray tracing algorithm usually adopts a bounding volume hierarchy (BVH) to accelerate the process of finding intersections to evaluate visibility. However, its characteristics are irregular, with divergence in memory access and branch execution, so it cannot achieve maximum efficiency on GPUs. This paper proposes a novel Neural Intersection Function based on a multilayer perceptron whose core operation contains only dense matrix multiplication with predictable memory access. Our method is the first solution integrating the neural network-based approach and BVH-based ray tracing pipeline into one unified rendering framework. We can evaluate the visibility and occlusion of secondary rays without traversing the most irregular and time-consuming part of the BVH and thus accelerate ray casting. The experiments show the proposed method can reduce the secondary ray casting time for direct illumination by up to 35\% compared to a BVH-based implementation and still preserve the image quality.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Fujieda, Shin and Kao, Chih-Chen and Harada, Takahiro},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07191 [cs]},
	keywords = {Computer Science - Graphics},
}

@inproceedings{peng_cagenerf_2022,
	title = {{CageNeRF}: {Cage}-based {Neural} {Radiance} {Field} for {Generalized} {3D} {Deformation} and {Animation}},
	shorttitle = {{CageNeRF}},
	url = {https://openreview.net/forum?id=kUnHCGiILeU},
	abstract = {While implicit representations have achieved high-fidelity results in 3D rendering, it remains challenging to deforming and animating the implicit field. Existing works typically leverage data-dependent models as deformation priors, such as SMPL for human body animation. However, this dependency on category-specific priors limits them to generalize to other objects. To solve this problem, we propose a novel framework for deforming and animating the neural radiance field learned on {\textbackslash}textit\{arbitrary\} objects. The key insight is that we introduce a cage-based representation as deformation prior, which is category-agnostic. Specifically, the deformation is performed based on an enclosing polygon mesh with sparsely defined vertices called {\textbackslash}textit\{cage\} inside the rendering space, where each point is projected into a novel position based on the barycentric interpolation of the deformed cage vertices. In this way, we transform the cage into a generalized constraint, which is able to deform and animate arbitrary target objects while preserving geometry details. Based on extensive experiments, we demonstrate the effectiveness of our framework in the task of geometry editing, object animation and deformation transfer.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Peng, Yicong and Yan, Yichao and Liu, Shengqi and Cheng, Yuhao and Guan, Shanyan and Pan, Bowen and Zhai, Guangtao and Yang, Xiaokang},
	month = oct,
	year = {2022},
}

@inproceedings{zhan_general_2023,
	title = {General {Neural} {Gauge} {Fields}},
	url = {https://openreview.net/forum?id=XWkWK2UagFR},
	abstract = {The recent advance of neural fields, such as neural radiance fields, has significantly pushed the boundary of scene representation learning. Aiming to boost the computation efﬁciency and rendering quality of 3D scenes, a popular line of research maps the 3D coordinate system to another measuring system, e.g., 2D manifolds and hash tables, for modeling neural fields. The conversion of coordinate systems can be typically dubbed as {\textbackslash}emph\{gauge transformation\}, which is usually a pre-defined mapping function, e.g., orthogonal projection or spatial hash function. This begs a question: can we directly learn a desired gauge transformation along with the neural field in an end-to-end manner? In this work, we extend this problem to a general paradigm with a taxonomy of discrete and continuous cases, and develop an end-to-end learning framework to jointly optimize the gauge transformation and neural fields. To counter the problem that the learning of gauge transformations can collapse easily, we derive a general regularization mechanism from the principle of information conservation during the gauge transformation. To circumvent the high computation cost in gauge learning with regularization, we directly derive an information-invariant gauge transformation which allows to preserve scene information inherently and yield superior performance.},
	language = {en},
	urldate = {2023-05-02},
	author = {Zhan, Fangneng and Liu, Lingjie and Kortylewski, Adam and Theobalt, Christian},
	month = feb,
	year = {2023},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@misc{zhang_bundlerecon_2023,
	title = {{BundleRecon}: {Ray} {Bundle}-{Based} {3D} {Neural} {Reconstruction}},
	shorttitle = {{BundleRecon}},
	url = {http://arxiv.org/abs/2305.07342},
	doi = {10.48550/arXiv.2305.07342},
	abstract = {With the growing popularity of neural rendering, there has been an increasing number of neural implicit multi-view reconstruction methods. While many models have been enhanced in terms of positional encoding, sampling, rendering, and other aspects to improve the reconstruction quality, current methods do not fully leverage the information among neighboring pixels during the reconstruction process. To address this issue, we propose an enhanced model called BundleRecon. In the existing approaches, sampling is performed by a single ray that corresponds to a single pixel. In contrast, our model samples a patch of pixels using a bundle of rays, which incorporates information from neighboring pixels. Furthermore, we design bundle-based constraints to further improve the reconstruction quality. Experimental results demonstrate that BundleRecon is compatible with the existing neural implicit multi-view reconstruction methods and can improve their reconstruction quality.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Zhang, Weikun and Zhu, Jianke},
	month = may,
	year = {2023},
	note = {arXiv:2305.07342 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_inpaintnerf360_2023,
	title = {{InpaintNeRF360}: {Text}-{Guided} {3D} {Inpainting} on {Unbounded} {Neural} {Radiance} {Fields}},
	shorttitle = {{InpaintNeRF360}},
	url = {http://arxiv.org/abs/2305.15094},
	doi = {10.48550/arXiv.2305.15094},
	abstract = {Neural Radiance Fields (NeRF) can generate highly realistic novel views. However, editing 3D scenes represented by NeRF across 360-degree views, particularly removing objects while preserving geometric and photometric consistency, remains a challenging problem due to NeRF's implicit scene representation. In this paper, we propose InpaintNeRF360, a unified framework that utilizes natural language instructions as guidance for inpainting NeRF-based 3D scenes.Our approach employs a promptable segmentation model by generating multi-modal prompts from the encoded text for multiview segmentation. We apply depth-space warping to enforce viewing consistency in the segmentations, and further refine the inpainted NeRF model using perceptual priors to ensure visual plausibility. InpaintNeRF360 is capable of simultaneously removing multiple objects or modifying object appearance based on text instructions while synthesizing 3D viewing-consistent and photo-realistic inpainting. Through extensive experiments on both unbounded and frontal-facing scenes trained through NeRF, we demonstrate the effectiveness of our approach and showcase its potential to enhance the editability of implicit radiance fields.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Wang, Dongqing and Zhang, Tong and Abboud, Alaa and Süsstrunk, Sabine},
	month = may,
	year = {2023},
	note = {arXiv:2305.15094 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{schob_neural_2023,
	title = {Neural {Radiance} {Fields} for {Landscape} {Architecture}},
	doi = {10.14627/537740046},
	abstract = {In this paper, we examine potential applications of Neural Radiance Fields (NeRF) in the field of landscape architecture. NeRF is a state-of-the-art method for novel view synthesis and volumetric scene reconstruction based on real-world training data. Our paper addresses NeRF and its derived models with a focus on the use and application of Instant-NGP, a method developed by researchers from the technology company NVIDIA. We discuss experimental applications of NeRF based on the case study of the post-disaster landscape of Ahr Valley, Germany, affected by a 100-year flood in 2021. In particular, we are interested in the benefits of NeRF in comparison to other landscape modeling methods, such as Structure-from-Motion (SfM) or Multi-View-Stereo (MVS), which use similar data as input. This study shows that the application of NeRF technology can be a promising alternative for capturing and visualizing landscape scenes. The study focuses especially on tasks and situations where the larger spatial context-the landscape-is of interest and importance. The technological aspects of how NeRF models work are relevant, but our main focus is on their potential implications for the field of landscape architecture. Technical development and research in the scientific field of computer vision are accelerating rapidly. As users, rather than developers, of digital tools, we believe that NeRF technology requires professional validation through real-world landscape projects.},
	author = {Schob, Maximilian and Rekittke, Jörg},
	month = may,
	year = {2023},
	pages = {428--442},
}

@inproceedings{vo_dna_2023,
	title = {{DNA}: {Deformable} {Neural} {Articulations} {Network} for {Template}-{Free} {Dynamic} {3D} {Human} {Reconstruction} {From} {Monocular} {RGB}-{D} {Video}},
	shorttitle = {{DNA}},
	url = {https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Vo_DNA_Deformable_Neural_Articulations_Network_for_Template-Free_Dynamic_3D_Human_CVPRW_2023_paper.html},
	language = {en},
	urldate = {2023-06-12},
	author = {Vo, Khoa and Pham, Trong-Thang and Yamazaki, Kashu and Tran, Minh and Le, Ngan},
	year = {2023},
	pages = {3675--3684},
}

@misc{xie_chore_2022,
	title = {{CHORE}: {Contact}, {Human} and {Object} {REconstruction} from a single {RGB} image},
	shorttitle = {{CHORE}},
	url = {http://arxiv.org/abs/2204.02445},
	abstract = {Most prior works in perceiving 3D humans from images reason human in isolation without their surroundings. However, humans are constantly interacting with the surrounding objects, thus calling for models that can reason about not only the human but also the object and their interaction. The problem is extremely challenging due to heavy occlusions between humans and objects, diverse interaction types and depth ambiguity. In this paper, we introduce CHORE, a novel method that learns to jointly reconstruct the human and the object from a single RGB image. CHORE takes inspiration from recent advances in implicit surface learning and classical model-based fitting. We compute a neural reconstruction of human and object represented implicitly with two unsigned distance fields, a correspondence field to a parametric body and an object pose field. This allows us to robustly fit a parametric body model and a 3D object template, while reasoning about interactions. Furthermore, prior pixel-aligned implicit learning methods use synthetic data and make assumptions that are not met in the real data. We propose a elegant depth-aware scaling that allows more efficient shape learning on real data. Experiments show that our joint reconstruction learned with the proposed strategy significantly outperforms the SOTA. Our code and models are available at https://virtualhumans.mpi-inf.mpg.de/chore},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Xie, Xianghui and Bhatnagar, Bharat Lal and Pons-Moll, Gerard},
	month = jul,
	year = {2022},
	note = {arXiv:2204.02445 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{weng_holistic_2021,
	title = {Holistic {3D} {Human} and {Scene} {Mesh} {Estimation} {From} {Single} {View} {Images}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Weng_Holistic_3D_Human_and_Scene_Mesh_Estimation_From_Single_View_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-06-12},
	author = {Weng, Zhenzhen and Yeung, Serena},
	year = {2021},
	pages = {334--343},
}

@misc{radl_analyzing_2023,
	title = {Analyzing the {Internals} of {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2306.00696},
	doi = {10.48550/arXiv.2306.00696},
	abstract = {Modern Neural Radiance Fields (NeRFs) learn a mapping from position to volumetric density via proposal network samplers. In contrast to the coarse-to-fine sampling approach with two NeRFs, this offers significant potential for speedups using lower network capacity as the task of mapping spatial coordinates to volumetric density involves no view-dependent effects and is thus much easier to learn. Given that most of the network capacity is utilized to estimate radiance, NeRFs could store valuable density information in their parameters or their deep features. To this end, we take one step back and analyze large, trained ReLU-MLPs used in coarse-to-fine sampling. We find that trained NeRFs, Mip-NeRFs and proposal network samplers map samples with high density to local minima along a ray in activation feature space. We show how these large MLPs can be accelerated by transforming the intermediate activations to a weight estimate, without any modifications to the parameters post-optimization. With our approach, we can reduce the computational requirements of trained NeRFs by up to 50\% with only a slight hit in rendering quality and no changes to the training protocol or architecture. We evaluate our approach on a variety of architectures and datasets, showing that our proposition holds in various settings.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Radl, Lukas and Kurz, Andreas and Steinberger, Markus},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00696 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{chen_panogrf_2023,
	title = {{PanoGRF}: {Generalizable} {Spherical} {Radiance} {Fields} for {Wide}-baseline {Panoramas}},
	shorttitle = {{PanoGRF}},
	url = {http://arxiv.org/abs/2306.01531},
	doi = {10.48550/arXiv.2306.01531},
	abstract = {Achieving an immersive experience enabling users to explore virtual environments with six degrees of freedom (6DoF) is essential for various applications such as virtual reality (VR). Wide-baseline panoramas are commonly used in these applications to reduce network bandwidth and storage requirements. However, synthesizing novel views from these panoramas remains a key challenge. Although existing neural radiance field methods can produce photorealistic views under narrow-baseline and dense image captures, they tend to overfit the training views when dealing with {\textbackslash}emph\{wide-baseline\} panoramas due to the difficulty in learning accurate geometry from sparse \$360{\textasciicircum}\{{\textbackslash}circ\}\$ views. To address this problem, we propose PanoGRF, Generalizable Spherical Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance fields incorporating \$360{\textasciicircum}\{{\textbackslash}circ\}\$ scene priors. Unlike generalizable radiance fields trained on perspective images, PanoGRF avoids the information loss from panorama-to-perspective conversion and directly aggregates geometry and appearance features of 3D sample points from each panoramic view based on spherical projection. Moreover, as some regions of the panorama are only visible from one view while invisible from others under wide baseline settings, PanoGRF incorporates \$360{\textasciicircum}\{{\textbackslash}circ\}\$ monocular depth priors into spherical depth estimation to improve the geometry features. Experimental results on multiple panoramic datasets demonstrate that PanoGRF significantly outperforms state-of-the-art generalizable view synthesis methods for wide-baseline panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay).},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Chen, Zheng and Cao, Yan-Pei and Guo, Yuan-Chen and Wang, Chen and Shan, Ying and Zhang, Song-Hai},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01531 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{li_compact_2023,
	title = {Compact {Real}-time {Radiance} {Fields} with {Neural} {Codebook}},
	url = {http://arxiv.org/abs/2305.18163},
	doi = {10.48550/arXiv.2305.18163},
	abstract = {Reconstructing neural radiance fields with explicit volumetric representations, demonstrated by Plenoxels, has shown remarkable advantages on training and rendering efficiency, while grid-based representations typically induce considerable overhead for storage and transmission. In this work, we present a simple and effective framework for pursuing compact radiance fields from the perspective of compression methodology. By exploiting intrinsic properties exhibiting in grid models, a non-uniform compression stem is developed to significantly reduce model complexity and a novel parameterized module, named Neural Codebook, is introduced for better encoding high-frequency details specific to per-scene models via a fast optimization. Our approach can achieve over 40 \${\textbackslash}times\$ reduction on grid model storage with competitive rendering quality. In addition, the method can achieve real-time rendering speed with 180 fps, realizing significant advantage on storage cost compared to real-time rendering methods.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Li, Lingzhi and Wang, Zhongshu and Shen, Zhen and Shen, Li and Tan, Ping},
	month = may,
	year = {2023},
	note = {arXiv:2305.18163 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yan_learning_2023,
	title = {Learning from {Multi}-{View} {Representation} for {Point}-{Cloud} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2306.02558},
	doi = {10.48550/arXiv.2306.02558},
	abstract = {A critical problem in the pre-training of 3D point clouds is leveraging massive 2D data. A fundamental challenge is to address the 2D-3D domain gap. This paper proposes a novel approach to point-cloud pre-training that enables learning 3D representations by leveraging pre-trained 2D-based networks. In particular, it avoids overfitting to 2D representations and potentially discarding critical 3D features for 3D recognition tasks. The key to our approach is a novel multi-view representation, which learns a shared 3D feature volume consistent with deep features extracted from multiple 2D camera views. The 2D deep features are regularized using pre-trained 2D networks through the 2D knowledge transfer loss. To prevent the resulting 3D feature representations from discarding 3D signals, we introduce the multi-view consistency loss that forces the projected 2D feature representations to capture pixel-wise correspondences across different views. Such correspondences induce 3D geometry and effectively retain 3D features in the projected 2D features. Experimental results demonstrate that our pre-trained model can be successfully transferred to various downstream tasks, including 3D detection and semantic segmentation, and achieve state-of-the-art performance.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Yan, Siming and Song, Chen and Kong, Youkang and Huang, Qixing},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02558 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tong_scene_2023,
	title = {Scene as {Occupancy}},
	url = {http://arxiv.org/abs/2306.02851},
	doi = {10.48550/arXiv.2306.02851},
	abstract = {Human driver can easily describe the complex traffic scene by visual system. Such an ability of precise perception is essential for driver's planning. To achieve this, a geometry-aware representation that quantizes the physical 3D scene into structured grid map with semantic labels per cell, termed as 3D Occupancy, would be desirable. Compared to the form of bounding box, a key insight behind occupancy is that it could capture the fine-grained details of critical obstacles in the scene, and thereby facilitate subsequent tasks. Prior or concurrent literature mainly concentrate on a single scene completion task, where we might argue that the potential of this occupancy representation might obsess broader impact. In this paper, we propose OccNet, a multi-view vision-centric pipeline with a cascade and temporal voxel decoder to reconstruct 3D occupancy. At the core of OccNet is a general occupancy embedding to represent 3D physical world. Such a descriptor could be applied towards a wide span of driving tasks, including detection, segmentation and planning. To validate the effectiveness of this new representation and our proposed algorithm, we propose OpenOcc, the first dense high-quality 3D occupancy benchmark built on top of nuScenes. Empirical experiments show that there are evident performance gain across multiple tasks, e.g., motion planning could witness a collision rate reduction by 15\%-58\%, demonstrating the superiority of our method.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Tong, Wenwen and Sima, Chonghao and Wang, Tai and Wu, Silei and Deng, Hanming and Chen, Li and Gu, Yi and Lu, Lewei and Luo, Ping and Lin, Dahua and Li, Hongyang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02851 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{jiang_h2-mapping_2023,
	title = {H2-{Mapping}: {Real}-time {Dense} {Mapping} {Using} {Hierarchical} {Hybrid} {Representation}},
	shorttitle = {H2-{Mapping}},
	url = {http://arxiv.org/abs/2306.03207},
	doi = {10.48550/arXiv.2306.03207},
	abstract = {Constructing a high-quality dense map in real-time is essential for robotics, AR/VR, and digital twins applications. As Neural Radiance Field (NeRF) greatly improves the mapping performance, in this paper, we propose a NeRF-based mapping method that enables higher-quality reconstruction and real-time capability even on edge computers. Specifically, we propose a novel hierarchical hybrid representation that leverages implicit multiresolution hash encoding aided by explicit octree SDF priors, describing the scene at different levels of detail. This representation allows for fast scene geometry initialization and makes scene geometry easier to learn. Besides, we present a coverage-maximizing keyframe selection strategy to address the forgetting issue and enhance mapping quality, particularly in marginal areas. To the best of our knowledge, our method is the first to achieve high-quality NeRF-based mapping on edge computers of handheld devices and quadrotors in real-time. Experiments demonstrate that our method outperforms existing NeRF-based mapping methods in geometry accuracy, texture realism, and time consumption. The code will be released at: https://github.com/SYSU-STAR/H2-Mapping},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Jiang, Chenxing and Zhang, Hanwen and Liu, Peize and Yu, Zehuan and Cheng, Hui and Zhou, Boyu and Shen, Shaojie},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03207 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{li_rgb-d_2017,
	title = {{RGB}-{D} {SLAM} in {Dynamic} {Environments} {Using} {Static} {Point} {Weighting}},
	volume = {2},
	issn = {2377-3766},
	doi = {10.1109/LRA.2017.2724759},
	abstract = {We propose a real-time depth edge based RGB-D SLAM system for dynamic environment. Our visual odometry method is based on frame-to-keyframe registration, where only depth edge points are used. To reduce the influence of dynamic objects, we propose a static weighting method for edge points in the keyframe. Static weight indicates the likelihood of one point being part of the static environment. This static weight is added into the intensity assisted iterative closest point (IAICP) method to perform the registration task. Furthermore, our method is integrated into a SLAM (Simultaneous Localization and Mapping) system, where an efficient loop closure detection strategy is used. Both our visual odometry method and SLAM system are evaluated with challenging dynamic sequences from the TUM RGB-D dataset. Compared to state-of-the-art methods for dynamic environment, our method reduces the tracking error significantly.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Li, Shile and Lee, Dongheui},
	month = oct,
	year = {2017},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Computer vision for other robotic applications, Dynamics, Estimation, Image edge detection, SLAM (Simultaneous Localization and Mapping), Simultaneous localization and mapping, Three-dimensional displays, Visualization, visual tracking},
	pages = {2263--2270},
}

@article{xie_moving_2021,
	title = {Moving {Object} {Segmentation} and {Detection} for {Robust} {RGBD}-{SLAM} in {Dynamic} {Environments}},
	volume = {70},
	issn = {1557-9662},
	doi = {10.1109/TIM.2020.3026803},
	abstract = {Localization accuracy is a fundamental requirement for Simultaneous Localization and Mapping (SLAM) systems. Traditional visual SLAM (vSLAM) schemes are usually based upon the assumption of static environments, so they do not perform well in dynamic environments. While a number of vSLAM frameworks have been reported for dynamic environments, the localization accuracy is usually unsatisfactory. In this article, we present a novel motion detection and segmentation method using Red Green Blue-Depth (RGB-D) data to improve the localization accuracy of feature-based RGB-D SLAM in dynamic environments. To overcome the problem due to undersegmentation generated by the semantic segmentation network, a mask inpainting method is developed to ensure the completeness of object segmentation. In the meantime, an optical flow-based motion detection method is proposed to detect dynamic objects from moving cameras, allowing robust detection by removing irrelevant information. Experiments performed on the public Technical University of Munich (TUM) RGB-D data set show that the presented scheme outperforms the state-of-art RGB-D SLAM systems in terms of trajectory accuracy, improving the localization accuracy of RGB-D SLAM in dynamic environments.},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Xie, Wanfang and Liu, Peter Xiaoping and Zheng, Minhua},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Instrumentation and Measurement},
	keywords = {Cameras, Dynamic environments, Dynamics, Motion segmentation, Object segmentation, Optical imaging, Red Green Blue-Depth (RGB-D)-simultaneous localization and mapping (SLAM), Semantics, Simultaneous localization and mapping, localization accuracy, motion detection, object segmentation},
	pages = {1--8},
}

@article{chen_g2ifu_2023,
	title = {{G2IFu}: {Graph}-based implicit function for single-view {3D} reconstruction},
	volume = {124},
	issn = {0952-1976},
	shorttitle = {{G2IFu}},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197623006772},
	doi = {10.1016/j.engappai.2023.106493},
	abstract = {As the demand for 3D models increases, there is growing interest in reconstructing 3D objects from images using AI. In this paper, we propose G2IFu, a graph-based implicit function that successfully reconstructs a highly detailed 3D object mesh from a single image. Unlike previous methods that learn implicit functions with points, G2IFu aims to map graphs to implicit values. We make the following contributions: (1) Compared to independent 3D points, graphs have a larger perception space and contain specific spatial structure information. Therefore, we extend a 3D point p to a graph Gp by generating hypothesis points and establishing edges between them. We then predict the corresponding implicit value using a graph convolution network. Our experiments show that this method can effectively improve the prediction accuracy of implicit functions. (2) We introduce a prior boundary loss based on Gp to make the network pay more attention to the “key” points near the shape surface. To the best of our knowledge, G2IFu is the first model that introduces a graph into neural implicit representation. (3) Inspired by previous methods, we utilize the image’s global and local features to initialize Gp. We also introduce a self-attention module into G2IFu for better performance. We conduct experiments on the ShapeNet dataset and demonstrate that G2IFu can generate higher-quality 3D object shapes than previous single-view reconstruction methods. Additionally, we extend G2IFu to multi-view 3D reconstruction and achieve good performance.},
	language = {en},
	urldate = {2023-06-12},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Chen, Rongshan and Yang, Yuancheng and Tong, Chao},
	month = sep,
	year = {2023},
	keywords = {3D reconstruction, Graph, Implicit representation, Single image},
	pages = {106493},
}

@inproceedings{hedman_baking_2021,
	title = {Baking {Neural} {Radiance} {Fields} for {Real}-{Time} {View} {Synthesis}},
	url = {https://ieeexplore.ieee.org/document/9710808/},
	doi = {10/gptvs4},
	abstract = {Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF’s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. "bake") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF’s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF’s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Hedman, Peter and Srinivasan, Pratul P. and Mildenhall, Ben and Barron, Jonathan T. and Debevec, Paul},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Graphics, Graphics processing units, Image and video synthesis, Multilayer perceptrons, Octrees, Portable computers, Stereo, Streaming media, Three-dimensional displays, ⛔ No INSPIRE recid found},
	pages = {5855--5864},
}

@inproceedings{weder_removing_2023,
	title = {Removing {Objects} {From} {Neural} {Radiance} {Fields}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Weder_Removing_Objects_From_Neural_Radiance_Fields_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Weder, Silvan and Garcia-Hernando, Guillermo and Monszpart, Áron and Pollefeys, Marc and Brostow, Gabriel J. and Firman, Michael and Vicente, Sara},
	year = {2023},
	pages = {16528--16538},
}

@misc{jang_d-tensorf_2022,
	title = {D-{TensoRF}: {Tensorial} {Radiance} {Fields} for {Dynamic} {Scenes}},
	shorttitle = {D-{TensoRF}},
	url = {http://arxiv.org/abs/2212.02375},
	abstract = {Neural radiance field (NeRF) attracts attention as a promising approach to reconstructing the 3D scene. As NeRF emerges, subsequent studies have been conducted to model dynamic scenes, which include motions or topological changes. However, most of them use an additional deformation network, slowing down the training and rendering speed. Tensorial radiance field (TensoRF) recently shows its potential for fast, high-quality reconstruction of static scenes with compact model size. In this paper, we present D-TensoRF, a tensorial radiance field for dynamic scenes, enabling novel view synthesis at a specific time. We consider the radiance field of a dynamic scene as a 5D tensor. The 5D tensor represents a 4D grid in which each axis corresponds to X, Y, Z, and time and has 1D multi-channel features per element. Similar to TensoRF, we decompose the grid either into rank-one vector components (CP decomposition) or low-rank matrix components (newly proposed MM decomposition). We also use smoothing regularization to reflect the relationship between features at different times (temporal dependency). We conduct extensive evaluations to analyze our models. We show that D-TensoRF with CP decomposition and MM decomposition both have short training times and significantly low memory footprints with quantitatively and qualitatively competitive rendering results in comparison to the state-of-the-art methods in 3D dynamic scene modeling.},
	urldate = {2023-06-10},
	publisher = {arXiv},
	author = {Jang, Hankyu and Kim, Daeyoung},
	month = dec,
	year = {2022},
	note = {arXiv:2212.02375 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{mi_switch-nerf_2023,
	title = {Switch-{NeRF}: {Learning} {Scene} {Decomposition} with {Mixture} of {Experts} for {Large}-scale {Neural} {Radiance} {Fields}},
	shorttitle = {Switch-{NeRF}},
	url = {https://openreview.net/forum?id=PQ2zoIZqvm},
	abstract = {The Neural Radiance Fields (NeRF) have been recently applied to reconstruct building-scale and even city-scale scenes. To model a large-scale scene efficiently, a dominant strategy is to employ a divide-and-conquer paradigm via performing scene decomposition, which decomposes a complex scene into parts that are further processed by different sub-networks. Existing large-scale NeRFs mainly use heuristic hand-crafted scene decomposition, with regular 3D-distance-based or physical-street-block-based schemes. Although achieving promising results, the hand-crafted schemes limit the capabilities of NeRF in large-scale scene modeling in several aspects. Manually designing a universal scene decomposition rule for different complex scenes is challenging, leading to adaptation issues for different scenarios. The decomposition procedure is not learnable, hindering the network from jointly optimizing the scene decomposition and the radiance fields in an end-to-end manner. The different sub-networks are typically optimized independently, and thus hand-crafted rules are required to composite them to achieve a better consistency. To tackle these issues, we propose Switch-NeRF, a novel end-to-end large-scale NeRF with learning-based scene decomposition. We design a gating network to dispatch 3D points to different NeRF sub-networks. The gating network can be optimized together with the NeRF sub-networks for different scene partitions, by a design with the Sparsely Gated Mixture of Experts (MoE). The outputs from different sub-networks can also be fused in a learnable way in the unified framework to effectively guarantee the consistency of the whole scene. Furthermore, the proposed MoE-based Switch-NeRF model is carefully implemented and optimized to achieve both high-fidelity scene reconstruction and efficient computation. Our method establishes clear state-of-the-art performances on several large-scale datasets. To the best of our knowledge, we are the first to propose an applicable end-to-end sparse NeRF network with learning-based decomposition for large-scale scenes. Codes are released at https://github.com/MiZhenxing/Switch-NeRF.},
	language = {en},
	urldate = {2023-03-13},
	author = {Mi, Zhenxing and Xu, Dan},
	month = feb,
	year = {2023},
	keywords = {/unread},
}

@inproceedings{bian_nope-nerf_2023,
	title = {{NoPe}-{NeRF}: {Optimising} {Neural} {Radiance} {Field} {With} {No} {Pose} {Prior}},
	shorttitle = {{NoPe}-{NeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-11},
	author = {Bian, Wenjing and Wang, Zirui and Li, Kejie and Bian, Jia-Wang and Prisacariu, Victor Adrian},
	year = {2023},
	pages = {4160--4169},
}

@inproceedings{perazzo_directvoxgo_2022,
	title = {{DirectVoxGO}++: {Fast} {Neural} {Radiance} {Fields} for {Object} {Reconstruction}},
	volume = {1},
	shorttitle = {{DirectVoxGO}++},
	doi = {10.1109/SIBGRAPI55357.2022.9991779},
	abstract = {In recent years, new reconstruction techniques based on Neural Radiance Fields (NeRFs) have created new forms to model objects instead of the traditional mesh and point cloud-based representations, allowing for more photorealistic rendering. However, these techniques were too slow to be used in practical settings, taking in the range of hours in high-end GPUs. Due to these limitations, new techniques have been created for fast reconstruction of scenes, such as DirectVoxGO. Alongside this limitation, one issue with NeRFs is that they were initially unable to separate the foreground from the background and had problems with 360 scenes until the emergence of new techniques such as NeRF++. Our method extends DirectVoxGO to allow the handling of unbounded scenes inspired by some ideas from NeRF++, adapting it to incorporate elements from a neural hashing approach employed by other works. Our technique improved photorealism compared with DirectVoxGO and Plenoxels on a subset of the LF dataset on average in at least 2\%, 8\%, and 8\% for PSNR, SSIM, and LPIPS metrics, respectively, while also being an order of magnitude faster than NeRF++. Code will be available in https://github.com/danperazzo/dvgoplusplus.},
	booktitle = {2022 35th {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} ({SIBGRAPI})},
	author = {Perazzo, Daniel and Lima, João Paulo and Velho, Luiz and Teichrieb, Veronica},
	month = oct,
	year = {2022},
	note = {ISSN: 2377-5416},
	keywords = {Graphics processing units, Kernel, Measurement, Photorealism, Pipelines, Rendering (computer graphics), Training},
	pages = {156--161},
}

@inproceedings{guo_neural_2022,
	title = {Neural {3D} {Scene} {Reconstruction} with the {Manhattan}-world {Assumption}},
	doi = {10.1109/CVPR52688.2022.00543},
	abstract = {This paper addresses the challenge of reconstructing 3D indoor scenes from multi-view images. Many previous works have shown impressive reconstruction results on textured objects, but they still have difficulty in handling low-textured planar regions, which are common in indoor scenes. An approach to solving this issue is to incorporate planer constraints into the depth map estimation in multiview stereo-based methods, but the per-view plane estimation and depth optimization lack both efficiency and multiview consistency. In this work, we show that the planar constraints can be conveniently integrated into the recent implicit neural representation-based reconstruction methods. Specifically, we use an MLP network to represent the signed distance function as the scene geometry. Based on the Manhattan-world assumption, planar constraints are employed to regularize the geometry in floor and wall regions predicted by a 2D semantic segmentation network. To resolve the inaccurate segmentation, we encode the semantics of 3D points with another MLP and design a novel loss that jointly optimizes the scene geometry and semantics in 3D space. Experiments on ScanNet and 7-Scenes datasets show that the proposed method outperforms previous methods by a large margin on 3D reconstruction quality. The code and supplementary materials are available at https://zju3dv.github.io/manhattan\_sdf.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Guo, Haoyu and Peng, Sida and Lin, Haotong and Wang, Qianqian and Zhang, Guofeng and Bao, Hujun and Zhou, Xiaowei},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {3D from multi-view and sensors, Estimation, Geometry, Pattern recognition, Reconstruction algorithms, Robustness, Semantics, Three-dimensional displays},
	pages = {5501--5510},
}

@inproceedings{fu_panoptic_2022,
	title = {Panoptic {NeRF}: {3D}-to-{2D} {Label} {Transfer} for {Panoptic} {Urban} {Scene} {Segmentation}},
	shorttitle = {Panoptic {NeRF}},
	doi = {10.1109/3DV57658.2022.00042},
	abstract = {Large-scale training data with high-quality annotations is critical for training semantic and instance segmentation models. Unfortunately, pixel-wise annotation is labor-intensive and costly, raising the demand for more efficient labeling strategies. In this work, we present a novel 3D-to-2D label transfer method, Panoptic NeRF1, which aims for obtaining per-pixel 2D semantic and instance labels from easy-to-obtain coarse 3D bounding primitives. Our method utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D semantic cues transferred from existing datasets. We demonstrate that this combination allows for improved geometry guided by semantic information, enabling rendering of accurate semantic maps across multiple views. Furthermore, this fusion process resolves label ambiguity of the coarse 3D annotations and filters noise in the 2D predictions. By inferring in 3D space and rendering to 2D labels, our 2D semantic and instance labels are multiview consistent by design. Experimental results show that Panoptic NeRF outperforms existing label transfer methods in terms of accuracy and multi-view consistency on challenging urban scenes of the KITTI-360 dataset.},
	booktitle = {2022 {International} {Conference} on {3D} {Vision} ({3DV})},
	author = {Fu, Xiao and Zhang, Shangzhan and Chen, Tianrun and Lu, Yichong and Zhu, Lanyun and Zhou, Xiaowei and Geiger, Andreas and Liao, Yiyi},
	month = sep,
	year = {2022},
	note = {ISSN: 2475-7888},
	keywords = {Annotations, Geometry, Rendering (computer graphics), Semantics, Three-dimensional displays, Training, Training data},
	pages = {1--11},
}

@inproceedings{tosi_nerf-supervised_2023,
	title = {{NeRF}-{Supervised} {Deep} {Stereo}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Tosi_NeRF-Supervised_Deep_Stereo_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-10},
	author = {Tosi, Fabio and Tonioni, Alessio and De Gregorio, Daniele and Poggi, Matteo},
	year = {2023},
	pages = {855--866},
}

@inproceedings{he_towards_2023,
	title = {Towards {Scalable} {Neural} {Representation} for {Diverse} {Videos}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-10},
	author = {He, Bo and Yang, Xitong and Wang, Hanyu and Wu, Zuxuan and Chen, Hao and Huang, Shuaiyi and Ren, Yixuan and Lim, Ser-Nam and Shrivastava, Abhinav},
	year = {2023},
	pages = {6132--6142},
}

@inproceedings{weber_power_2023,
	title = {Power {Bundle} {Adjustment} for {Large}-{Scale} {3D} {Reconstruction}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Weber_Power_Bundle_Adjustment_for_Large-Scale_3D_Reconstruction_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-10},
	author = {Weber, Simon and Demmel, Nikolaus and Chan, Tin Chon and Cremers, Daniel},
	year = {2023},
	pages = {281--289},
}

@inproceedings{ci_gfpose_2023,
	title = {{GFPose}: {Learning} {3D} {Human} {Pose} {Prior} {With} {Gradient} {Fields}},
	shorttitle = {{GFPose}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-10},
	author = {Ci, Hai and Wu, Mingdong and Zhu, Wentao and Ma, Xiaoxuan and Dong, Hao and Zhong, Fangwei and Wang, Yizhou},
	year = {2023},
	pages = {4800--4810},
}

@inproceedings{shao_tensor4d_2023,
	title = {{Tensor4D}: {Efficient} {Neural} {4D} {Decomposition} for {High}-{Fidelity} {Dynamic} {Reconstruction} and {Rendering}},
	shorttitle = {{Tensor4D}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Shao_Tensor4D_Efficient_Neural_4D_Decomposition_for_High-Fidelity_Dynamic_Reconstruction_and_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Shao, Ruizhi and Zheng, Zerong and Tu, Hanzhang and Liu, Boning and Zhang, Hongwen and Liu, Yebin},
	year = {2023},
	pages = {16632--16642},
}

@inproceedings{williams_neural_2022,
	title = {Neural {Fields} {As} {Learnable} {Kernels} for {3D} {Reconstruction}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Williams_Neural_Fields_As_Learnable_Kernels_for_3D_Reconstruction_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-10},
	author = {Williams, Francis and Gojcic, Zan and Khamis, Sameh and Zorin, Denis and Bruna, Joan and Fidler, Sanja and Litany, Or},
	year = {2022},
	pages = {18500--18510},
}

@misc{tan_enhance-nerf_2023,
	title = {Enhance-{NeRF}: {Multiple} {Performance} {Evaluation} for {Neural} {Radiance} {Fields}},
	shorttitle = {Enhance-{NeRF}},
	url = {http://arxiv.org/abs/2306.05303},
	doi = {10.48550/arXiv.2306.05303},
	abstract = {The quality of three-dimensional reconstruction is a key factor affecting the effectiveness of its application in areas such as virtual reality (VR) and augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate realistic images from any viewpoint. It simultaneously reconstructs the shape, lighting, and materials of objects, and without surface defects, which breaks down the barrier between virtuality and reality. The potential spatial correspondences displayed by NeRF between reconstructed scenes and real-world scenes offer a wide range of practical applications possibilities. Despite significant progress in 3D reconstruction since NeRF were introduced, there remains considerable room for exploration and experimentation. NeRF-based models are susceptible to interference issues caused by colored "fog" noise. Additionally, they frequently encounter instabilities and failures while attempting to reconstruct unbounded scenes. Moreover, the model takes a significant amount of time to converge, making it even more challenging to use in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color to balance low and high reflectivity objects display, utilizes a decoding architecture with prior knowledge to improve recognition, and employs multi-layer performance evaluation mechanisms to enhance learning capacity. It achieves reconstruction of outdoor scenes within one hour under single-card condition. Based on experimental results, Enhance-NeRF partially enhances fitness capability and provides some support to outdoor scene reconstruction. The Enhance-NeRF method can be used as a plug-and-play component, making it easy to integrate with other NeRF-based models. The code is available at: https://github.com/TANQIanQ/Enhance-NeRF},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Tan, Qianqiu and Liu, Tao and Xie, Yinling and Yu, Shuwan and Zhang, Baohua},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05303 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{liu_marching-primitives_2023,
	title = {Marching-{Primitives}: {Shape} {Abstraction} {From} {Signed} {Distance} {Function}},
	shorttitle = {Marching-{Primitives}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Marching-Primitives_Shape_Abstraction_From_Signed_Distance_Function_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Liu, Weixiao and Wu, Yuwei and Ruan, Sipu and Chirikjian, Gregory S.},
	year = {2023},
	pages = {8771--8780},
}

@inproceedings{wang_flow_2023,
	title = {Flow {Supervision} for {Deformable} {NeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Flow_Supervision_for_Deformable_NeRF_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Wang, Chaoyang and MacDonald, Lachlan Ewen and Jeni, László A. and Lucey, Simon},
	year = {2023},
	pages = {21128--21137},
}

@inproceedings{attal_hyperreel_2023,
	title = {{HyperReel}: {High}-{Fidelity} 6-{DoF} {Video} {With} {Ray}-{Conditioned} {Sampling}},
	shorttitle = {{HyperReel}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Attal_HyperReel_High-Fidelity_6-DoF_Video_With_Ray-Conditioned_Sampling_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Attal, Benjamin and Huang, Jia-Bin and Richardt, Christian and Zollhöfer, Michael and Kopf, Johannes and O’Toole, Matthew and Kim, Changil},
	year = {2023},
	pages = {16610--16620},
}

@inproceedings{kim_generalizable_2023,
	title = {Generalizable {Implicit} {Neural} {Representations} via {Instance} {Pattern} {Composers}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Generalizable_Implicit_Neural_Representations_via_Instance_Pattern_Composers_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Kim, Chiheon and Lee, Doyup and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin},
	year = {2023},
	pages = {11808--11817},
}

@inproceedings{li_regularize_2023,
	title = {Regularize {Implicit} {Neural} {Representation} by {Itself}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_Regularize_Implicit_Neural_Representation_by_Itself_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Li, Zhemin and Wang, Hongxia and Meng, Deyu},
	year = {2023},
	pages = {10280--10288},
}

@inproceedings{bokhovkin_neural_2023,
	title = {Neural {Part} {Priors}: {Learning} {To} {Optimize} {Part}-{Based} {Object} {Completion} in {RGB}-{D} {Scans}},
	shorttitle = {Neural {Part} {Priors}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Bokhovkin_Neural_Part_Priors_Learning_To_Optimize_Part-Based_Object_Completion_in_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Bokhovkin, Aleksei and Dai, Angela},
	year = {2023},
	pages = {9032--9042},
}

@inproceedings{dogaru_sphere-guided_2023,
	title = {Sphere-{Guided} {Training} of {Neural} {Implicit} {Surfaces}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Dogaru_Sphere-Guided_Training_of_Neural_Implicit_Surfaces_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Dogaru, Andreea and Ardelean, Andrei-Timotei and Ignatyev, Savva and Zakharov, Egor and Burnaev, Evgeny},
	year = {2023},
	pages = {20844--20853},
}

@inproceedings{messikommer_data-driven_2023,
	title = {Data-{Driven} {Feature} {Tracking} for {Event} {Cameras}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Messikommer_Data-Driven_Feature_Tracking_for_Event_Cameras_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Messikommer, Nico and Fang, Carter and Gehrig, Mathias and Scaramuzza, Davide},
	year = {2023},
	pages = {5642--5651},
}

@inproceedings{ye_nef_2023,
	title = {{NEF}: {Neural} {Edge} {Fields} for {3D} {Parametric} {Curve} {Reconstruction} {From} {Multi}-{View} {Images}},
	shorttitle = {{NEF}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Ye_NEF_Neural_Edge_Fields_for_3D_Parametric_Curve_Reconstruction_From_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Ye, Yunfan and Yi, Renjiao and Gao, Zhirui and Zhu, Chenyang and Cai, Zhiping and Xu, Kai},
	year = {2023},
	pages = {8486--8495},
}

@inproceedings{gao_surfelnerf_2023,
	title = {{SurfelNeRF}: {Neural} {Surfel} {Radiance} {Fields} for {Online} {Photorealistic} {Reconstruction} of {Indoor} {Scenes}},
	shorttitle = {{SurfelNeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Gao_SurfelNeRF_Neural_Surfel_Radiance_Fields_for_Online_Photorealistic_Reconstruction_of_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Gao, Yiming and Cao, Yan-Pei and Shan, Ying},
	year = {2023},
	pages = {108--118},
}

@inproceedings{agaram_canonical_2023,
	title = {Canonical {Fields}: {Self}-{Supervised} {Learning} of {Pose}-{Canonicalized} {Neural} {Fields}},
	shorttitle = {Canonical {Fields}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Agaram_Canonical_Fields_Self-Supervised_Learning_of_Pose-Canonicalized_Neural_Fields_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Agaram, Rohith and Dewan, Shaurya and Sajnani, Rahul and Poulenard, Adrien and Krishna, Madhava and Sridhar, Srinath},
	year = {2023},
	pages = {4500--4510},
}

@inproceedings{johari_eslam_2023,
	title = {{ESLAM}: {Efficient} {Dense} {SLAM} {System} {Based} on {Hybrid} {Representation} of {Signed} {Distance} {Fields}},
	shorttitle = {{ESLAM}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Johari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Johari, Mohammad Mahdi and Carta, Camilla and Fleuret, François},
	year = {2023},
	pages = {17408--17419},
}

@inproceedings{jin_perspective_2023,
	title = {Perspective {Fields} for {Single} {Image} {Camera} {Calibration}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Perspective_Fields_for_Single_Image_Camera_Calibration_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Jin, Linyi and Zhang, Jianming and Hold-Geoffroy, Yannick and Wang, Oliver and Blackburn-Matzen, Kevin and Sticha, Matthew and Fouhey, David F.},
	year = {2023},
	pages = {17307--17316},
}

@inproceedings{xu_discoscene_2023,
	title = {{DisCoScene}: {Spatially} {Disentangled} {Generative} {Radiance} {Fields} for {Controllable} {3D}-{Aware} {Scene} {Synthesis}},
	shorttitle = {{DisCoScene}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Xu, Yinghao and Chai, Menglei and Shi, Zifan and Peng, Sida and Skorokhodov, Ivan and Siarohin, Aliaksandr and Yang, Ceyuan and Shen, Yujun and Lee, Hsin-Ying and Zhou, Bolei and Tulyakov, Sergey},
	year = {2023},
	pages = {4402--4412},
}

@inproceedings{truong_sparf_2023,
	title = {{SPARF}: {Neural} {Radiance} {Fields} {From} {Sparse} and {Noisy} {Poses}},
	shorttitle = {{SPARF}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Truong_SPARF_Neural_Radiance_Fields_From_Sparse_and_Noisy_Poses_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Truong, Prune and Rakotosaona, Marie-Julie and Manhardt, Fabian and Tombari, Federico},
	year = {2023},
	pages = {4190--4200},
}

@inproceedings{humayun_splinecam_2023,
	title = {{SplineCam}: {Exact} {Visualization} and {Characterization} of {Deep} {Network} {Geometry} and {Decision} {Boundaries}},
	shorttitle = {{SplineCam}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Humayun_SplineCam_Exact_Visualization_and_Characterization_of_Deep_Network_Geometry_and_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Humayun, Ahmed Imtiaz and Balestriero, Randall and Balakrishnan, Guha and Baraniuk, Richard G.},
	year = {2023},
	pages = {3789--3798},
}

@inproceedings{wang_pet-neus_2023,
	title = {{PET}-{NeuS}: {Positional} {Encoding} {Tri}-{Planes} for {Neural} {Surfaces}},
	shorttitle = {{PET}-{NeuS}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_PET-NeuS_Positional_Encoding_Tri-Planes_for_Neural_Surfaces_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Wang, Yiqun and Skorokhodov, Ivan and Wonka, Peter},
	year = {2023},
	pages = {12598--12607},
}

@inproceedings{singh_polynomial_2023,
	title = {Polynomial {Implicit} {Neural} {Representations} for {Large} {Diverse} {Datasets}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Singh_Polynomial_Implicit_Neural_Representations_for_Large_Diverse_Datasets_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Singh, Rajhans and Shukla, Ankita and Turaga, Pavan},
	year = {2023},
	pages = {2041--2051},
}

@inproceedings{huang_neural_2023,
	title = {Neural {Kernel} {Surface} {Reconstruction}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-01},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Huang, Jiahui and Gojcic, Zan and Atzmon, Matan and Litany, Or and Fidler, Sanja and Williams, Francis},
	year = {2023},
	pages = {4369--4379},
}

@inproceedings{chen_mobilenerf_2023,
	title = {{MobileNeRF}: {Exploiting} the {Polygon} {Rasterization} {Pipeline} for {Efficient} {Neural} {Field} {Rendering} on {Mobile} {Architectures}},
	shorttitle = {{MobileNeRF}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Chen_MobileNeRF_Exploiting_the_Polygon_Rasterization_Pipeline_for_Efficient_Neural_Field_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-05},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chen, Zhiqin and Funkhouser, Thomas and Hedman, Peter and Tagliasacchi, Andrea},
	year = {2023},
	pages = {16569--16578},
}

@inproceedings{park_temporal_2023,
	title = {Temporal {Interpolation} {Is} {All} {You} {Need} for {Dynamic} {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2302.09311},
	doi = {10.48550/arXiv.2302.09311},
	abstract = {Temporal interpolation often plays a crucial role to learn meaningful representations in dynamic scenes. In this paper, we propose a novel method to train spatiotemporal neural radiance fields of dynamic scenes based on temporal interpolation of feature vectors. Two feature interpolation methods are suggested depending on underlying representations, neural networks or grids. In the neural representation, we extract features from space-time inputs via multiple neural network modules and interpolate them based on time frames. The proposed multi-level feature interpolation network effectively captures features of both short-term and long-term time ranges. In the grid representation, space-time features are learned via four-dimensional hash grids, which remarkably reduces training time. The grid representation shows more than 100 times faster training speed than the previous neural-net-based methods while maintaining the rendering quality. Concatenating static and dynamic features and adding a simple smoothness term further improve the performance of our proposed models. Despite the simplicity of the model architectures, our method achieved state-of-the-art performance both in rendering quality for the neural representation and in training speed for the grid representation.},
	urldate = {2023-06-07},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Park, Sungheon and Son, Minjung and Jang, Seokhwan and Ahn, Young Chun and Kim, Ji-Yeon and Kang, Nahyup},
	month = mar,
	year = {2023},
	note = {arXiv:2302.09311 [cs]
titleTranslation:},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_efficient_2023,
	title = {Efficient {Large}-scale {Scene} {Representation} with a {Hybrid} of {High}-resolution {Grid} and {Plane} {Features}},
	url = {http://arxiv.org/abs/2303.03003},
	doi = {10.48550/arXiv.2303.03003},
	abstract = {Existing neural radiance fields (NeRF) methods for large-scale scene modeling require days of training using multiple GPUs, hindering their applications in scenarios with limited computing resources. Despite fast optimization NeRF variants have been proposed based on the explicit dense or hash grid features, their effectivenesses are mainly demonstrated in object-scale scene representation. In this paper, we point out that the low feature resolution in explicit representation is the bottleneck for large-scale unbounded scene representation. To address this problem, we introduce a new and efficient hybrid feature representation for NeRF that fuses the 3D hash-grids and high-resolution 2D dense plane features. Compared with the dense-grid representation, the resolution of a dense 2D plane can be scaled up more efficiently. Based on this hybrid representation, we propose a fast optimization NeRF variant, called GP-NeRF, that achieves better rendering results while maintaining a compact model size. Extensive experiments on multiple large-scale unbounded scene datasets show that our model can converge in 1.5 hours using a single GPU while achieving results comparable to or even better than the existing method that requires about one day's training with 8 GPUs.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Zhang, Yuqi and Chen, Guanying and Cui, Shuguang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03003 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{yang_implicit_2022,
	title = {Implicit neural representation for physics-driven actuated soft bodies},
	volume = {41},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3528223.3530156},
	doi = {10.1145/3528223.3530156},
	abstract = {Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.},
	number = {4},
	urldate = {2023-06-07},
	journal = {ACM Transactions on Graphics},
	author = {Yang, Lingchen and Kim, Byungsoo and Zoss, Gaspard and Gözcü, Baran and Gross, Markus and Solenthaler, Barbara},
	year = {2022},
	keywords = {deep learning, differentiable physics, digital human},
	pages = {122:1--122:10},
}

@misc{sun_human_2023,
	title = {Human {3D} {Avatar} {Modeling} with {Implicit} {Neural} {Representation}: {A} {Brief} {Survey}},
	shorttitle = {Human {3D} {Avatar} {Modeling} with {Implicit} {Neural} {Representation}},
	url = {http://arxiv.org/abs/2306.03576},
	doi = {10.48550/arXiv.2306.03576},
	abstract = {A human 3D avatar is one of the important elements in the metaverse, and the modeling effect directly affects people's visual experience. However, the human body has a complex topology and diverse details, so it is often expensive, time-consuming, and laborious to build a satisfactory model. Recent studies have proposed a novel method, implicit neural representation, which is a continuous representation method and can describe objects with arbitrary topology at arbitrary resolution. Researchers have applied implicit neural representation to human 3D avatar modeling and obtained more excellent results than traditional methods. This paper comprehensively reviews the application of implicit neural representation in human body modeling. First, we introduce three implicit representations of occupancy field, SDF, and NeRF, and make a classification of the literature investigated in this paper. Then the application of implicit modeling methods in the body, hand, and head are compared and analyzed respectively. Finally, we point out the shortcomings of current work and provide available suggestions for researchers.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Sun, Mingyang and Yang, Dingkang and Kou, Dongliang and Jiang, Yang and Shan, Weihua and Yan, Zhe and Zhang, Lihua},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03576 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{gao__nodate,
	title = {自动驾驶与机器人中的 {SLAM} 技术},
	author = {Gao, Xiang},
}

@misc{ma_learning_2023,
	title = {Learning {Signed} {Distance} {Functions} from {Noisy} {3D} {Point} {Clouds} via {Noise} to {Noise} {Mapping}},
	url = {http://arxiv.org/abs/2306.01405},
	doi = {10.48550/arXiv.2306.01405},
	abstract = {Learning signed distance functions (SDFs) from 3D point clouds is an important task in 3D computer vision. However, without ground truth signed distances, point normals or clean point clouds, current methods still struggle from learning SDFs from noisy point clouds. To overcome this challenge, we propose to learn SDFs via a noise to noise mapping, which does not require any clean point cloud or ground truth supervision for training. Our novelty lies in the noise to noise mapping which can infer a highly accurate SDF of a single object or scene from its multiple or even single noisy point cloud observations. Our novel learning manner is supported by modern Lidar systems which capture multiple noisy observations per second. We achieve this by a novel loss which enables statistical reasoning on point clouds and maintains geometric consistency although point clouds are irregular, unordered and have no point correspondence among noisy observations. Our evaluation under the widely used benchmarks demonstrates our superiority over the state-of-the-art methods in surface reconstruction, point cloud denoising and upsampling. Our code, data, and pre-trained models are available at https://github.com/mabaorui/Noise2NoiseMapping/},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Ma, Baorui and Liu, Yu-Shen and Han, Zhizhong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01405 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{martin_nerf-qa_2023,
	title = {{NeRF}-{QA}: {Neural} {Radiance} {Fields} {Quality} {Assessment} {Database}},
	shorttitle = {{NeRF}-{QA}},
	url = {http://arxiv.org/abs/2305.03176},
	doi = {10.48550/arXiv.2305.03176},
	abstract = {This short paper proposes a new database - NeRF-QA - containing 48 videos synthesized with seven NeRF based methods, along with their perceived quality scores, resulting from subjective assessment tests; for the videos selection, both real and synthetic, 360 degrees scenes were considered. This database will allow to evaluate the suitability, to NeRF based synthesized views, of existing objective quality metrics and also the development of new quality metrics, specific for this case.},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Martin, Pedro and Rodrigues, António and Ascenso, João and Queluz, Maria Paula},
	month = may,
	year = {2023},
	note = {arXiv:2305.03176 [cs]},
	keywords = {Computer Science - Multimedia},
}

@misc{li_compact_2023-1,
	title = {Compact {Real}-time {Radiance} {Fields} with {Neural} {Codebook}},
	url = {http://arxiv.org/abs/2305.18163},
	doi = {10.48550/arXiv.2305.18163},
	abstract = {Reconstructing neural radiance fields with explicit volumetric representations, demonstrated by Plenoxels, has shown remarkable advantages on training and rendering efficiency, while grid-based representations typically induce considerable overhead for storage and transmission. In this work, we present a simple and effective framework for pursuing compact radiance fields from the perspective of compression methodology. By exploiting intrinsic properties exhibiting in grid models, a non-uniform compression stem is developed to significantly reduce model complexity and a novel parameterized module, named Neural Codebook, is introduced for better encoding high-frequency details specific to per-scene models via a fast optimization. Our approach can achieve over 40 \${\textbackslash}times\$ reduction on grid model storage with competitive rendering quality. In addition, the method can achieve real-time rendering speed with 180 fps, realizing significant advantage on storage cost compared to real-time rendering methods.},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Li, Lingzhi and Wang, Zhongshu and Shen, Zhen and Shen, Li and Tan, Ping},
	month = may,
	year = {2023},
	note = {arXiv:2305.18163 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{azzarelli_towards_2023,
	title = {Towards a {Robust} {Framework} for {NeRF} {Evaluation}},
	url = {http://arxiv.org/abs/2305.18079},
	doi = {10.48550/arXiv.2305.18079},
	abstract = {Neural Radiance Field (NeRF) research has attracted significant attention recently, with 3D modelling, virtual/augmented reality, and visual effects driving its application. While current NeRF implementations can produce high quality visual results, there is a conspicuous lack of reliable methods for evaluating them. Conventional image quality assessment methods and analytical metrics (e.g. PSNR, SSIM, LPIPS etc.) only provide approximate indicators of performance since they generalise the ability of the entire NeRF pipeline. Hence, in this paper, we propose a new test framework which isolates the neural rendering network from the NeRF pipeline and then performs a parametric evaluation by training and evaluating the NeRF on an explicit radiance field representation. We also introduce a configurable approach for generating representations specifically for evaluation purposes. This employs ray-casting to transform mesh models into explicit NeRF samples, as well as to "shade" these representations. Combining these two approaches, we demonstrate how different "tasks" (scenes with different visual effects or learning strategies) and types of networks (NeRFs and depth-wise implicit neural representations (INRs)) can be evaluated within this framework. Additionally, we propose a novel metric to measure task complexity of the framework which accounts for the visual parameters and the distribution of the spatial data. Our approach offers the potential to create a comparative objective evaluation framework for NeRF methods.},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Azzarelli, Adrian and Anantrasirichai, Nantheera and Bull, David R.},
	month = may,
	year = {2023},
	note = {arXiv:2305.18079 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{song_darf_2023,
	title = {D{\textbackslash}"{aRF}: {Boosting} {Radiance} {Fields} from {Sparse} {Inputs} with {Monocular} {Depth} {Adaptation}},
	shorttitle = {D{\textbackslash}"{aRF}},
	url = {http://arxiv.org/abs/2305.19201},
	doi = {10.48550/arXiv.2305.19201},
	abstract = {Neural radiance fields (NeRF) shows powerful performance in novel view synthesis and 3D geometry reconstruction, but it suffers from critical performance degradation when the number of known viewpoints is drastically reduced. Existing works attempt to overcome this problem by employing external priors, but their success is limited to certain types of scenes or datasets. Employing monocular depth estimation (MDE) networks, pretrained on large-scale RGB-D datasets, with powerful generalization capability would be a key to solving this problem: however, using MDE in conjunction with NeRF comes with a new set of challenges due to various ambiguity problems exhibited by monocular depths. In this light, we propose a novel framework, dubbed D{\textbackslash}"aRF, that achieves robust NeRF reconstruction with a handful of real-world images by combining the strengths of NeRF and monocular depth estimation through online complementary training. Our framework imposes the MDE network's powerful geometry prior to NeRF representation at both seen and unseen viewpoints to enhance its robustness and coherence. In addition, we overcome the ambiguity problems of monocular depths through patch-wise scale-shift fitting and geometry distillation, which adapts the MDE network to produce depths aligned accurately with NeRF geometry. Experiments show our framework achieves state-of-the-art results both quantitatively and qualitatively, demonstrating consistent and reliable performance in both indoor and outdoor real-world datasets. Project page is available at https://ku-cvlab.github.io/DaRF/.},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Song, Jiuhn and Park, Seonghoon and An, Honggyu and Cho, Seokju and Kwak, Min-Seop and Cho, Sungjin and Kim, Seungryong},
	month = may,
	year = {2023},
	note = {arXiv:2305.19201 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{han_volume_2023,
	title = {Volume {Feature} {Rendering} for {Fast} {Neural} {Radiance} {Field} {Reconstruction}},
	url = {http://arxiv.org/abs/2305.17916},
	doi = {10.48550/arXiv.2305.17916},
	abstract = {Neural radiance fields (NeRFs) are able to synthesize realistic novel views from multi-view images captured from distinct positions and perspectives. In NeRF's rendering pipeline, neural networks are used to represent a scene independently or transform queried learnable feature vector of a point to the expected color or density. With the aid of geometry guides either in occupancy grids or proposal networks, the number of neural network evaluations can be reduced from hundreds to dozens in the standard volume rendering framework. Instead of rendering yielded color after neural network evaluation, we propose to render the queried feature vectors of a ray first and then transform the rendered feature vector to the final pixel color by a neural network. This fundamental change to the standard volume rendering framework requires only one single neural network evaluation to render a pixel, which substantially lowers the high computational complexity of the rendering framework attributed to a large number of neural network evaluations. Consequently, we can use a comparably larger neural network to achieve a better rendering quality while maintaining the same training and rendering time costs. Our model achieves the state-of-the-art rendering quality on both synthetic and real-world datasets while requiring a training time of several minutes.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Han, Kang and Xiang, Wei and Yu, Lu},
	month = may,
	year = {2023},
	note = {arXiv:2305.17916 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{huang_local_2023,
	title = {Local {Implicit} {Ray} {Function} for {Generalizable} {Radiance} {Field} {Representation}},
	url = {http://arxiv.org/abs/2304.12746},
	doi = {10.48550/arXiv.2304.12746},
	abstract = {We propose LIRF (Local Implicit Ray Function), a generalizable neural rendering approach for novel view rendering. Current generalizable neural radiance fields (NeRF) methods sample a scene with a single ray per pixel and may therefore render blurred or aliased views when the input views and rendered views capture scene content with different resolutions. To solve this problem, we propose LIRF to aggregate the information from conical frustums to construct a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates and the features of conical frustums as inputs and predicts a local volumetric radiance field. Since the coordinates are continuous, LIRF renders high-quality novel views at a continuously-valued scale via volume rendering. Besides, we predict the visible weights for each input view via transformer-based feature matching to improve the performance in occluded areas. Experimental results on real-world scenes validate that our method outperforms state-of-the-art methods on novel view rendering of unseen scenes at arbitrary scales.},
	urldate = {2023-05-30},
	booktitle = {Computer {Vision} {And} {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Huang, Xin and Zhang, Qi and Feng, Ying and Li, Xiaoyu and Wang, Xuan and Wang, Qing},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12746 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{liao_deep_2018,
	title = {Deep {Marching} {Cubes}: {Learning} {Explicit} {Surface} {Representations}},
	shorttitle = {Deep {Marching} {Cubes}},
	doi = {10.1109/CVPR.2018.00308},
	abstract = {Existing learning based solutions to 3D surface prediction cannot be trained end-to-end as they operate on intermediate representations (e.g., TSDF) from which 3D surface meshes must be extracted in a post-processing step (e.g., via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface prediction. We first demonstrate that the marching cubes algorithm is not differentiable and propose an alternative differentiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demonstrate that the model allows for predicting sub-voxel accurate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object's inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liao, Yiyi and Donné, Simon and Geiger, Andreas},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Face, Shape, Solid modeling, Surface reconstruction, Surface treatment, Three-dimensional displays, Topology},
	pages = {2916--2925},
}

@misc{zhang_fastmesh_2023,
	title = {{FastMESH}: {Fast} {Surface} {Reconstruction} by {Hexagonal} {Mesh}-based {Neural} {Rendering}},
	shorttitle = {{FastMESH}},
	url = {http://arxiv.org/abs/2305.17858},
	abstract = {Despite the promising results of multi-view reconstruction, the recent neural rendering-based methods, such as implicit surface rendering (IDR) and volume rendering (NeuS), not only incur a heavy computational burden on training but also have the difficulties in disentangling the geometric and appearance. Although having achieved faster training speed than implicit representation and hash coding, the explicit voxel-based method obtains the inferior results on recovering surface. To address these challenges, we propose an effective mesh-based neural rendering approach, named FastMESH, which only samples at the intersection of ray and mesh. A coarse-to-fine scheme is introduced to efficiently extract the initial mesh by space carving. More importantly, we suggest a hexagonal mesh model to preserve surface regularity by constraining the secondorder derivatives of vertices, where only low level of positional encoding is engaged for neural rendering. The experiments demonstrate that our approach achieves the state-ofthe-art results on both reconstruction and novel view synthesis. Besides, we obtain 10-fold acceleration on training comparing to the implicit representation-based methods.},
	language = {en},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Zhang, Yisu and Zhu, Jianke and Lin, Lixiang},
	month = may,
	year = {2023},
	note = {arXiv:2305.17858 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_urbanbis_2023,
	title = {{UrbanBIS}: a {Large}-scale {Benchmark} for {Fine}-grained {Urban} {Building} {Instance} {Segmentation}},
	shorttitle = {{UrbanBIS}},
	url = {http://arxiv.org/abs/2305.02627},
	doi = {10.1145/3588432.3591508},
	abstract = {We present the UrbanBIS benchmark for large-scale 3D urban understanding, supporting practical urban-level semantic and building-level instance segmentation. UrbanBIS comprises six real urban scenes, with 2.5 billion points, covering a vast area of 10.78 square kilometers and 3,370 buildings, captured by 113,346 views of aerial photogrammetry. Particularly, UrbanBIS provides not only semantic-level annotations on a rich set of urban objects, including buildings, vehicles, vegetation, roads, and bridges, but also instance-level annotations on the buildings. Further, UrbanBIS is the first 3D dataset that introduces fine-grained building sub-categories, considering a wide variety of shapes for different building types. Besides, we propose B-Seg, a building instance segmentation method to establish UrbanBIS. B-Seg adopts an end-to-end framework with a simple yet effective strategy for handling large-scale point clouds. Compared with mainstream methods, B-Seg achieves better accuracy with faster inference speed on UrbanBIS. In addition to the carefully-annotated point clouds, UrbanBIS provides high-resolution aerial-acquisition photos and high-quality large-scale 3D reconstruction models, which shall facilitate a wide range of studies such as multi-view stereo, urban LOD generation, aerial path planning, autonomous navigation, road network extraction, and so on, thus serving as an important platform for many intelligent city applications.},
	urldate = {2023-05-30},
	author = {Yang, Guoqing and Xue, Fuyou and Zhang, Qi and Xie, Ke and Fu, Chi-Wing and Huang, Hui},
	month = may,
	year = {2023},
	note = {arXiv:2305.02627 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{wei_neumanifold_2023,
	title = {{NeuManifold}: {Neural} {Watertight} {Manifold} {Reconstruction} with {Efficient} and {High}-{Quality} {Rendering} {Support}},
	shorttitle = {{NeuManifold}},
	url = {http://arxiv.org/abs/2305.17134},
	doi = {10.48550/arXiv.2305.17134},
	abstract = {We present a method for generating high-quality watertight manifold meshes from multi-view input images. Existing volumetric rendering methods are robust in optimization but tend to generate noisy meshes with poor topology. Differentiable rasterization-based methods can generate high-quality meshes but are sensitive to initialization. Our method combines the benefits of both worlds; we take the geometry initialization obtained from neural volumetric fields, and further optimize the geometry as well as a compact neural texture representation with differentiable rasterizers. Through extensive experiments, we demonstrate that our method can generate accurate mesh reconstructions with faithful appearance that are comparable to previous volume rendering methods while being an order of magnitude faster in rendering. We also show that our generated mesh and neural texture reconstruction is compatible with existing graphics pipelines and enables downstream 3D applications such as simulation. Project page: https://sarahweiii.github.io/neumanifold/},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Wei, Xinyue and Xiang, Fanbo and Bi, Sai and Chen, Anpei and Sunkavalli, Kalyan and Xu, Zexiang and Su, Hao},
	month = may,
	year = {2023},
	note = {arXiv:2305.17134 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_planerf_2023,
	title = {{PlaNeRF}: {SVD} {Unsupervised} {3D} {Plane} {Regularization} for {NeRF} {Large}-{Scale} {Scene} {Reconstruction}},
	shorttitle = {{PlaNeRF}},
	url = {http://arxiv.org/abs/2305.16914},
	abstract = {Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in our loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstruction for large-scale outdoor scenes and achieves SoTA rendering quality on the KITTI-360 NVS benchmark.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Wang, Fusang and Louys, Arnaud and Piasco, Nathan and Bennehar, Moussab and Roldão, Luis and Tsishkou, Dzmitry},
	month = may,
	year = {2023},
	note = {arXiv:2305.16914 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.4.5, I.2.10},
}

@misc{kim_ncho_2023,
	title = {{NCHO}: {Unsupervised} {Learning} for {Neural} {3D} {Composition} of {Humans} and {Objects}},
	shorttitle = {{NCHO}},
	url = {http://arxiv.org/abs/2305.14345},
	abstract = {Deep generative models have been recently extended to synthesizing 3D digital humans. However, previous approaches treat clothed humans as a single chunk of geometry without considering the compositionality of clothing and accessories. As a result, individual items cannot be naturally composed into novel identities, leading to limited expressiveness and controllability of generative 3D avatars. While several methods attempt to address this by leveraging synthetic data, the interaction between humans and objects is not authentic due to the domain gap, and manual asset creation is difficult to scale for a wide variety of objects. In this work, we present a novel framework for learning a compositional generative model of humans and objects (backpacks, coats, scarves, and more) from real-world 3D scans. Our compositional model is interaction-aware, meaning the spatial relationship between humans and objects, and the mutual shape change by physical contact is fully incorporated. The key challenge is that, since humans and objects are in contact, their 3D scans are merged into a single piece. To decompose them without manual annotations, we propose to leverage two sets of 3D scans of a single person with and without objects. Our approach learns to decompose objects and naturally compose them back into a generative human model in an unsupervised manner. Despite our simple setup requiring only the capture of a single subject with objects, our experiments demonstrate the strong generalization of our model by enabling the natural composition of objects to diverse identities in various poses and the composition of multiple objects, which is unseen in training data.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Kim, Taeksoo and Saito, Shunsuke and Joo, Hanbyul},
	month = may,
	year = {2023},
	note = {arXiv:2305.14345 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@phdthesis{wu_multi-sensororial_2023,
	title = {Multi-sensororial {Scene} {Representation} {Learning} with {Hybrid} {Neural} {Implicit} {Fields}},
	copyright = {Creative Commons Attribution-NonCommercial 4.0 International License (CC-BY-NC)},
	url = {https://github.com/wuzirui/undergrad_thesis},
	language = {Chinese},
	school = {Beijing Institute of Technology},
	author = {Wu, Zirui},
	month = jun,
	year = {2023},
	note = {titleTranslation: 基于混合神经隐式场的多元传感融合场景地图表征},
}

@misc{peng_learning_2023,
	title = {Learning {Occupancy} for {Monocular} {3D} {Object} {Detection}},
	url = {http://arxiv.org/abs/2305.15694},
	doi = {10.48550/arXiv.2305.15694},
	abstract = {Monocular 3D detection is a challenging task due to the lack of accurate 3D information. Existing approaches typically rely on geometry constraints and dense depth estimates to facilitate the learning, but often fail to fully exploit the benefits of three-dimensional feature extraction in frustum and 3D space. In this paper, we propose {\textbackslash}textbf\{OccupancyM3D\}, a method of learning occupancy for monocular 3D detection. It directly learns occupancy in frustum and 3D space, leading to more discriminative and informative 3D features and representations. Specifically, by using synchronized raw sparse LiDAR point clouds, we define the space status and generate voxel-based occupancy labels. We formulate occupancy prediction as a simple classification problem and design associated occupancy losses. Resulting occupancy estimates are employed to enhance original frustum/3D features. As a result, experiments on KITTI and Waymo open datasets demonstrate that the proposed method achieves a new state of the art and surpasses other methods by a significant margin. Codes and pre-trained models will be available at {\textbackslash}url\{https://github.com/SPengLiang/OccupancyM3D\}.},
	urldate = {2023-05-28},
	publisher = {arXiv},
	author = {Peng, Liang and Xu, Junkai and Cheng, Haoran and Yang, Zheng and Wu, Xiaopei and Qian, Wei and Wang, Wenxiao and Wu, Boxi and Cai, Deng},
	month = may,
	year = {2023},
	note = {arXiv:2305.15694 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zeng_self-supervised_2023,
	title = {Self-{Supervised} {Learning} for {Point} {Clouds} {Data}: {A} {Survey}},
	shorttitle = {Self-{Supervised} {Learning} for {Point} {Clouds} {Data}},
	url = {http://arxiv.org/abs/2305.11881},
	doi = {10.48550/arXiv.2305.11881},
	abstract = {3D point clouds are a crucial type of data collected by LiDAR sensors and widely used in transportation applications due to its concise descriptions and accurate localization. Deep neural networks (DNNs) have achieved remarkable success in processing large amount of disordered and sparse 3D point clouds, especially in various computer vision tasks, such as pedestrian detection and vehicle recognition. Among all the learning paradigms, Self-Supervised Learning (SSL), an unsupervised training paradigm that mines effective information from the data itself, is considered as an essential solution to solve the time-consuming and labor-intensive data labelling problems via smart pre-training task design. This paper provides a comprehensive survey of recent advances on SSL for point clouds. We first present an innovative taxonomy, categorizing the existing SSL methods into four broad categories based on the pretexts' characteristics. Under each category, we then further categorize the methods into more fine-grained groups and summarize the strength and limitations of the representative methods. We also compare the performance of the notable SSL methods in literature on multiple downstream tasks on benchmark datasets both quantitatively and qualitatively. Finally, we propose a number of future research directions based on the identified limitations of existing SSL research on point clouds.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Zeng, Changyu and Wang, Wei and Nguyen, Anh and Yue, Yutao},
	month = may,
	year = {2023},
	note = {arXiv:2305.11881 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_deceptive-nerf_2023,
	title = {Deceptive-{NeRF}: {Enhancing} {NeRF} {Reconstruction} using {Pseudo}-{Observations} from {Diffusion} {Models}},
	shorttitle = {Deceptive-{NeRF}},
	url = {http://arxiv.org/abs/2305.15171},
	abstract = {This paper introduces Deceptive-NeRF, a new method for enhancing the quality of reconstructed NeRF models using synthetically generated pseudo-observations, capable of handling sparse input and removing floater artifacts. Our proposed method involves three key steps: 1) reconstruct a coarse NeRF model from sparse inputs; 2) generate pseudo-observations based on the coarse model; 3) refine the NeRF model using pseudo-observations to produce a high-quality reconstruction. To generate photo-realistic pseudo-observations that faithfully preserve the identity of the reconstructed scene while remaining consistent with the sparse inputs, we develop a rectification latent diffusion model that generates images conditional on a coarse RGB image and depth map, which are derived from the coarse NeRF and latent text embedding from input images. Extensive experiments show that our method is effective and can generate perceptually high-quality NeRF even with very sparse inputs.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Liu, Xinhang and Kao, Shiu-hong and Chen, Jiaben and Tai, Yu-Wing and Tang, Chi-Keung},
	month = may,
	year = {2023},
	note = {arXiv:2305.15171 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yan_od-nerf_2023,
	title = {{OD}-{NeRF}: {Efficient} {Training} of {On}-the-{Fly} {Dynamic} {Neural} {Radiance} {Fields}},
	shorttitle = {{OD}-{NeRF}},
	url = {http://arxiv.org/abs/2305.14831},
	doi = {10.48550/arXiv.2305.14831},
	abstract = {Dynamic neural radiance fields (dynamic NeRFs) have demonstrated impressive results in novel view synthesis on 3D dynamic scenes. However, they often require complete video sequences for training followed by novel view synthesis, which is similar to playing back the recording of a dynamic 3D scene. In contrast, we propose OD-NeRF to efficiently train and render dynamic NeRFs on-the-fly which instead is capable of streaming the dynamic scene. When training on-the-fly, the training frames become available sequentially and the model is trained and rendered frame-by-frame. The key challenge of efficient on-the-fly training is how to utilize the radiance field estimated from the previous frames effectively. To tackle this challenge, we propose: 1) a NeRF model conditioned on the multi-view projected colors to implicitly track correspondence between the current and previous frames, and 2) a transition and update algorithm that leverages the occupancy grid from the last frame to sample efficiently at the current frame. Our algorithm can achieve an interactive speed of 6FPS training and rendering on synthetic dynamic scenes on-the-fly, and a significant speed-up compared to the state-of-the-art on real-world dynamic scenes.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Yan, Zhiwen and Li, Chen and Lee, Gim Hee},
	month = may,
	year = {2023},
	note = {arXiv:2305.14831 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_nerflix_2023,
	title = {{NeRFLiX}: {High}-{Quality} {Neural} {View} {Synthesis} by {Learning} a {Degradation}-{Driven} {Inter}-viewpoint {MiXer}},
	shorttitle = {{NeRFLiX}},
	url = {http://arxiv.org/abs/2303.06919},
	doi = {10.48550/arXiv.2303.06919},
	abstract = {Neural radiance fields (NeRF) show great success in novel view synthesis. However, in real-world scenes, recovering high-quality details from the source images is still challenging for the existing NeRF-based approaches, due to the potential imperfect calibration information and scene representation inaccuracy. Even with high-quality training frames, the synthetic novel views produced by NeRF models still suffer from notable rendering artifacts, such as noise, blur, etc. Towards to improve the synthesis quality of NeRF-based approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by learning a degradation-driven inter-viewpoint mixer. Specially, we design a NeRF-style degradation modeling approach and construct large-scale training data, enabling the possibility of effectively removing NeRF-native rendering artifacts for existing deep neural networks. Moreover, beyond the degradation removal, we propose an inter-viewpoint aggregation framework that is able to fuse highly related high-quality training images, pushing the performance of cutting-edge NeRF models to entirely new levels and producing highly photo-realistic synthetic views.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Zhou, Kun and Li, Wenbo and Wang, Yi and Hu, Tao and Jiang, Nianjuan and Han, Xiaoguang and Lu, Jiangbo},
	month = mar,
	year = {2023},
	note = {arXiv:2303.06919 [cs]
titleTranslation:},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@book{de_berg_computational_2000,
	address = {Berlin, Heidelberg},
	title = {Computational {Geometry}: {Algorithms} and {Applications}},
	isbn = {978-3-662-04247-2 978-3-662-04245-8},
	shorttitle = {Computational {Geometry}},
	url = {http://link.springer.com/10.1007/978-3-662-04245-8},
	language = {en},
	urldate = {2023-05-23},
	publisher = {Springer Berlin Heidelberg},
	author = {De Berg, Mark and Van Kreveld, Marc and Overmars, Mark and Schwarzkopf, Otfried Cheong},
	year = {2000},
	doi = {10.1007/978-3-662-04245-8},
}

@misc{jiang_registering_2023,
	title = {Registering {Neural} {Radiance} {Fields} as {3D} {Density} {Images}},
	url = {http://arxiv.org/abs/2305.12843},
	doi = {10.48550/arXiv.2305.12843},
	abstract = {No significant work has been done to directly merge two partially overlapping scenes using NeRF representations. Given pre-trained NeRF models of a 3D scene with partial overlapping, this paper aligns them with a rigid transform, by generalizing the traditional registration pipeline, that is, key point detection and point set registration, to operate on 3D density fields. To describe corner points as key points in 3D, we propose to use universal pre-trained descriptor-generating neural networks that can be trained and tested on different scenes. We perform experiments to demonstrate that the descriptor networks can be conveniently trained using a contrastive learning strategy. We demonstrate that our method, as a global approach, can effectively register NeRF models, thus making possible future large-scale NeRF construction by registering its smaller and overlapping NeRFs captured individually.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Jiang, Han and Li, Ruoxuan and Sun, Haosen and Tai, Yu-Wing and Tang, Chi-Keung},
	month = may,
	year = {2023},
	note = {arXiv:2305.12843 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{barron_zip-nerf_2023,
	title = {Zip-{NeRF}: {Anti}-{Aliased} {Grid}-{Based} {Neural} {Radiance} {Fields}},
	shorttitle = {Zip-{NeRF}},
	url = {http://arxiv.org/abs/2304.06706},
	doi = {10.48550/arXiv.2304.06706},
	abstract = {Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8\% - 77\% lower than either prior technique, and that trains 24x faster than mip-NeRF 360.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2304.06706 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{fang_nerfuser_2023,
	title = {{NeRFuser}: {Large}-{Scale} {Scene} {Representation} by {NeRF} {Fusion}},
	shorttitle = {{NeRFuser}},
	url = {http://arxiv.org/abs/2305.13307},
	doi = {10.48550/arXiv.2305.13307},
	abstract = {A practical benefit of implicit visual representations like Neural Radiance Fields (NeRFs) is their memory efficiency: large scenes can be efficiently stored and shared as small neural nets instead of collections of images. However, operating on these implicit visual data structures requires extending classical image-based vision techniques (e.g., registration, blending) from image sets to neural fields. Towards this goal, we propose NeRFuser, a novel architecture for NeRF registration and blending that assumes only access to pre-generated NeRFs, and not the potentially large sets of images used to generate them. We propose registration from re-rendering, a technique to infer the transformation between NeRFs based on images synthesized from individual NeRFs. For blending, we propose sample-based inverse distance weighting to blend visual information at the ray-sample level. We evaluate NeRFuser on public benchmarks and a self-collected object-centric indoor dataset, showing the robustness of our method, including to views that are challenging to render from the individual source NeRFs.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Fang, Jiading and Lin, Shengjie and Vasiljevic, Igor and Guizilini, Vitor and Ambrus, Rares and Gaidon, Adrien and Shakhnarovich, Gregory and Walter, Matthew R.},
	month = may,
	year = {2023},
	note = {arXiv:2305.13307 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zuo_view_2023,
	title = {View {Synthesis} with {Sculpted} {Neural} {Points}},
	url = {https://openreview.net/forum?id=0ypGZvm0er0},
	abstract = {We address the task of view synthesis, generating novel views of a scene given a set of images as input. In many recent works such as NeRF (Mildenhall et al., 2020), the scene geometry is parameterized using neural implicit representations (i.e., MLPs). Implicit neural representations have achieved impressive visual quality but have drawbacks in computational efficiency. In this work, we propose a new approach that performs view synthesis using point clouds. It is the first point-based method that achieves better visual quality than NeRF while being 100× faster in rendering speed. Our approach builds on existing works on differentiable point-based rendering but introduces a novel technique we call “Sculpted Neural Points (SNP)”, which significantly improves the robustness to errors and holes in the reconstructed point cloud. We further propose to use view-dependent point features based on spherical harmonics to capture non-Lambertian surfaces, and new designs in the point-based rendering pipeline that further boost the performance. Finally, we show that our system supports fine-grained scene editing. Code is available at https://github.com/princeton-vl/SNP.},
	language = {en},
	urldate = {2023-05-23},
	author = {Zuo, Yiming and Deng, Jia},
	month = feb,
	year = {2023},
}

@inproceedings{ashkenazi_nern_2023,
	title = {{NeRN}: {Learning} {Neural} {Representations} for {Neural} {Networks}},
	shorttitle = {{NeRN}},
	url = {https://openreview.net/forum?id=9gfir3fSy3J},
	abstract = {Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabilize the learning process. We demonstrate the effectiveness of NeRN in reconstructing widely used architectures on CIFAR-10, CIFAR-100, and ImageNet. Finally, we present two applications using NeRN, demonstrating the capabilities of the learned representations.},
	language = {en},
	urldate = {2023-05-23},
	author = {Ashkenazi, Maor and Rimon, Zohar and Vainshtein, Ron and Levi, Shir and Richardson, Elad and Mintz, Pinchas and Treister, Eran},
	month = feb,
	year = {2023},
}

@inproceedings{belbute-peres_simple_2023,
	title = {Simple initialization and parametrization of sinusoidal networks via their kernel bandwidth},
	url = {https://openreview.net/forum?id=yVqC6gCNf4d},
	abstract = {Neural networks with sinusoidal activations have been proposed as an alternative to networks with traditional activation functions. Despite their promise, particularly for learning implicit models, their training behavior is not yet fully understood, leading to a number of empirical design choices that are not well justified. In this work, we first propose a simplified version of such sinusoidal neural networks, which allows both for easier practical implementation and simpler theoretical analysis. We then analyze the behavior of these networks from the neural tangent kernel perspective and demonstrate that their kernel approximates a low-pass filter with an adjustable bandwidth. Finally, we utilize these insights to inform the sinusoidal network initialization, optimizing their performance for each of a series of tasks, including learning implicit models and solving differential equations.},
	language = {en},
	urldate = {2023-05-23},
	author = {Belbute-Peres, Filipe de Avila and Kolter, J. Zico},
	month = feb,
	year = {2023},
}

@inproceedings{wu_voxurf_2023,
	title = {Voxurf: {Voxel}-based {Efficient} and {Accurate} {Neural} {Surface} {Reconstruction}},
	shorttitle = {Voxurf},
	url = {https://openreview.net/forum?id=DSy8tP4WctmZ},
	abstract = {Neural surface reconstruction aims to reconstruct accurate 3D surfaces based on multi-view images. Previous methods based on neural volume rendering mostly train a fully implicit model with MLPs, which typically require hours of training for a single scene. Recent efforts explore the explicit volumetric representation to accelerate the optimization via memorizing significant information with learnable voxel grids. However, existing voxel-based methods often struggle in reconstructing fine-grained geometry, even when combined with an SDF-based volume rendering scheme. We reveal that this is because 1) the voxel grids tend to break the color-geometry dependency that facilitates fine-geometry learning, and 2) the under-constrained voxel grids lack spatial coherence and are vulnerable to local minima. In this work, we present Voxurf, a voxel-based surface reconstruction approach that is both efficient and accurate. Voxurf addresses the aforementioned issues via several key designs, including 1) a two-stage training procedure that attains a coherent coarse shape and recovers fine details successively, 2) a dual color network that maintains color-geometry dependency, and 3) a hierarchical geometry feature to encourage information propagation across voxels. Extensive experiments show that Voxurf achieves high efficiency and high quality at the same time. On the DTU benchmark, Voxurf achieves higher reconstruction quality with a 20x training speedup compared to previous fully implicit methods. Our code is publicly available at https://github.com/wutong16/Voxurf/.},
	language = {en},
	urldate = {2023-05-23},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Wu, Tong and Wang, Jiaqi and Pan, Xingang and Xu, Xudong and Theobalt, Christian and Liu, Ziwei and Lin, Dahua},
	month = feb,
	year = {2023},
}

@inproceedings{berzins_neural_2023,
	title = {Neural {Implicit} {Shape} {Editing} using {Boundary} {Sensitivity}},
	url = {https://openreview.net/forum?id=CMPIBjmhpo},
	abstract = {Neural fields are receiving increased attention as a geometric representation due to their ability to compactly store detailed and smooth shapes and easily undergo topological changes. Compared to classic geometry representations, however, neural representations do not allow the user to exert intuitive control over the shape. Motivated by this, we leverage boundary sensitivity to express how perturbations in parameters move the shape boundary. This allows us to interpret the effect of each learnable parameter and study achievable deformations. With this, we perform geometric editing: finding a parameter update that best approximates a globally prescribed deformation. Prescribing the deformation only locally allows the rest of the shape to change according to some prior, such as semantics or deformation rigidity. Our method is agnostic to the model and its training and updates the NN in-place. Furthermore, we show how boundary sensitivity helps to optimize and constrain objectives (such as surface area and volume), which are difficult to compute without first converting to another representation, such as a mesh.},
	language = {en},
	urldate = {2023-05-23},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Berzins, Arturs and Ibing, Moritz and Kobbelt, Leif},
	month = feb,
	year = {2023},
}

@inproceedings{sun_direct_2022,
	title = {Direct {Voxel} {Grid} {Optimization}: {Super}-fast {Convergence} for {Radiance} {Fields} {Reconstruction}},
	shorttitle = {{DVGO}},
	url = {https://ieeexplore.ieee.org/document/9879963/},
	doi = {10/grg664},
	abstract = {We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and fiexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Sun, Cheng and Sun, Min and Chen, Hwann-Tzong},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Done Reading, Geometry, Interpolation, Optimization, Pattern recognition, Solid modeling, Task analysis, Training, ⛔ No INSPIRE recid found},
	pages = {5449--5459},
}

@inproceedings{teed_droid-slam_2021,
	title = {{DROID}-{SLAM}: {Deep} {Visual} {SLAM} for {Monocular}, {Stereo}, and {RGB}-{D} {Cameras}},
	volume = {34},
	shorttitle = {{DROID}-{SLAM}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html},
	abstract = {We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.},
	urldate = {2023-03-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Teed, Zachary and Deng, Jia},
	year = {2021},
	keywords = {/unread, ⛔ No DOI found},
	pages = {16558--16569},
}

@inproceedings{wang_regularizing_2021,
	title = {Regularizing {Nighttime} {Weirdness}: {Efficient} {Self}-{Supervised} {Monocular} {Depth} {Estimation} in the {Dark}},
	shorttitle = {depth evaluation},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Regularizing_Nighttime_Weirdness_Efficient_Self-Supervised_Monocular_Depth_Estimation_in_the_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-05-20},
	author = {Wang, Kun and Zhang, Zhenyu and Yan, Zhiqiang and Li, Xiang and Xu, Baobei and Li, Jun and Yang, Jian},
	year = {2021},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {16055--16064},
}

@inproceedings{zhang_unreasonable_2018,
	title = {The {Unreasonable} {Effectiveness} of {Deep} {Features} as a {Perceptual} {Metric}},
	shorttitle = {lpips},
	doi = {10/gfz33w},
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Computer architecture, Distortion, Measurement, Network architecture, Task analysis, Training, Visualization},
	pages = {586--595},
}

@inproceedings{hore_image_2010,
	title = {Image {Quality} {Metrics}: {PSNR} vs. {SSIM}},
	shorttitle = {Image {Quality} {Metrics}},
	doi = {10.1109/ICPR.2010.579},
	abstract = {In this paper, we analyse two well-known objective image quality metrics, the peak-signal-to-noise ratio (PSNR) as well as the structural similarity index measure (SSIM), and we derive a simple mathematical relationship between them which works for various kinds of image degradations such as Gaussian blur, additive Gaussian white noise, jpeg and jpeg2000 compression. A series of tests realized on images extracted from the Kodak database gives a better understanding of the similarity and difference between the SSIM and the PSNR.},
	booktitle = {2010 20th {International} {Conference} on {Pattern} {Recognition}},
	author = {Horé, Alain and Ziou, Djemel},
	month = aug,
	year = {2010},
	note = {2138 citations (Semantic Scholar/DOI) [2023-05-20]
ISSN: 1051-4651},
	keywords = {/unread, Additives, Degradation, Image coding, Image quality, PSNR, SSIM, Sensitivity, Transform coding, image quality metrics, ⛔ No INSPIRE recid found},
	pages = {2366--2369},
}

@misc{hu_consistentnerf_2023,
	title = {{ConsistentNeRF}: {Enhancing} {Neural} {Radiance} {Fields} with {3D} {Consistency} for {Sparse} {View} {Synthesis}},
	shorttitle = {{ConsistentNeRF}},
	url = {http://arxiv.org/abs/2305.11031},
	abstract = {Neural Radiance Fields (NeRF) has demonstrated remarkable 3D reconstruction capabilities with dense view images. However, its performance significantly deteriorates under sparse view settings. We observe that learning the 3D consistency of pixels among different views is crucial for improving reconstruction quality in such cases. In this paper, we propose ConsistentNeRF, a method that leverages depth information to regularize both multi-view and single-view 3D consistency among pixels. Specifically, ConsistentNeRF employs depth-derived geometry information and a depth-invariant loss to concentrate on pixels that exhibit 3D correspondence and maintain consistent depth relationships. Extensive experiments on recent representative works reveal that our approach can considerably enhance model performance in sparse view conditions, achieving improvements of up to 94\% in PSNR, 76\% in SSIM, and 31\% in LPIPS compared to the vanilla baselines across various benchmarks, including DTU, NeRF Synthetic, and LLFF.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Hu, Shoukang and Zhou, Kaichen and Li, Kaiyu and Yu, Longhui and Hong, Lanqing and Hu, Tianyang and Li, Zhenguo and Lee, Gim Hee and Liu, Ziwei},
	month = may,
	year = {2023},
	note = {arXiv:2305.11031 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{reiser_merf_2023,
	title = {{MERF}: {Memory}-{Efficient} {Radiance} {Fields} for {Real}-time {View} {Synthesis} in {Unbounded} {Scenes}},
	shorttitle = {{MERF}},
	url = {http://arxiv.org/abs/2302.12249},
	doi = {10.48550/arXiv.2302.12249},
	abstract = {Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Reiser, Christian and Szeliski, Richard and Verbin, Dor and Srinivasan, Pratul P. and Mildenhall, Ben and Geiger, Andreas and Barron, Jonathan T. and Hedman, Peter},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12249 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No INSPIRE recid found},
}

@inproceedings{cao_hexplane_2023,
	title = {{HexPlane}: {A} {Fast} {Representation} for {Dynamic} {Scenes}},
	shorttitle = {{HexPlane}},
	url = {http://arxiv.org/abs/2301.09632},
	doi = {10.48550/arXiv.2301.09632},
	abstract = {Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D vision. Prior approaches build on NeRF and rely on implicit representations. This is slow since it requires many MLP evaluations, constraining real-world applications. We show that dynamic 3D scenes can be explicitly represented by six planes of learned features, leading to an elegant solution we call HexPlane. A HexPlane computes features for points in spacetime by fusing vectors extracted from each plane, which is highly efficient. Pairing a HexPlane with a tiny MLP to regress output colors and training via volume rendering gives impressive results for novel view synthesis on dynamic scenes, matching the image quality of prior work but reducing training time by more than \$100{\textbackslash}times\$. Extensive ablations confirm our HexPlane design and show that it is robust to different feature fusion mechanisms, coordinate systems, and decoding mechanisms. HexPlanes are a simple and effective solution for representing 4D volumes, and we hope they can broadly contribute to modeling spacetime for dynamic 3D scenes.},
	urldate = {2023-02-03},
	booktitle = {Computer {Vision} {And} {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Cao, Ang and Johnson, Justin},
	month = jan,
	year = {2023},
	note = {arXiv:2301.09632 [cs]
titleTranslation:},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@inproceedings{long_sparseneus_2022,
	title = {{SparseNeuS}: {Fast} {Generalizable} {Neural} {Surface} {Reconstruction} from {Sparse} {Views}},
	shorttitle = {{SparseNeuS}},
	url = {http://arxiv.org/abs/2206.05737},
	doi = {10.48550/arXiv.2206.05737},
	abstract = {We introduce SparseNeuS, a novel neural rendering based method for the task of surface reconstruction from multi-view images. This task becomes more difficult when only sparse images are provided as input, a scenario where existing neural reconstruction approaches usually produce incomplete or distorted results. Moreover, their inability of generalizing to unseen new scenes impedes their application in practice. Contrarily, SparseNeuS can generalize to new scenes and work well with sparse images (as few as 2 or 3). SparseNeuS adopts signed distance function (SDF) as the surface representation, and learns generalizable priors from image features by introducing geometry encoding volumes for generic surface prediction. Moreover, several strategies are introduced to effectively leverage sparse views for high-quality reconstruction, including 1) a multi-level geometry reasoning framework to recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color blending scheme for more reliable color prediction; 3) a consistency-aware fine-tuning scheme to control the inconsistent regions caused by occlusion and noise. Extensive experiments demonstrate that our approach not only outperforms the state-of-the-art methods, but also exhibits good efficiency, generalizability, and flexibility.},
	urldate = {2022-11-28},
	booktitle = {European {Conference} {On} {Computer} {Vision}},
	publisher = {Springer},
	author = {Long, Xiaoxiao and Lin, Cheng and Wang, Peng and Komura, Taku and Wang, Wenping},
	month = aug,
	year = {2022},
	note = {arXiv:2206.05737 [cs]
version: 2
titleTranslation:},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@inproceedings{ueda_neural_2022,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Density}-{Distance} {Fields}},
	volume = {13692},
	shorttitle = {{NeDDF}},
	url = {http://arxiv.org/abs/2207.14455},
	doi = {https://doi.org/10.1007/978-3-031-19824-3_4},
	abstract = {The success of neural fields for 3D vision tasks is now indisputable. Following this trend, several methods aiming for visual localization (e.g., SLAM) have been proposed to estimate distance or density fields using neural fields. However, it is difficult to achieve high localization performance by only density fields-based methods such as Neural Radiance Field (NeRF) since they do not provide density gradient in most empty regions. On the other hand, distance field-based methods such as Neural Implicit Surface (NeuS) have limitations in objects’ surface shapes. This paper proposes Neural Density-Distance Field (NeDDF), a novel 3D representation that reciprocally constrains the distance and density fields. We extend distance field formulation to shapes with no explicit boundary surface, such as fur or smoke, which enable explicit conversion from distance field to density field. Consistent distance and density fields realized by explicit conversion enable both robustness to initial values and high-quality registration. Furthermore, the consistency between fields allows fast convergence from sparse point clouds. Experiments show that NeDDF can achieve high localization performance while providing comparable results to NeRF on novel view synthesis. The code is available at https://github.com/ueda0319/neddf.},
	language = {en},
	urldate = {2022-10-09},
	booktitle = {Computer {Vision} - \{{ECCV}\} 2022 - 17th {European} {Conference}, {Tel} {Aviv}, {Israel}, {October} 23-27, 2022, {Proceedings}, {Part} \{{XXXII}\}},
	publisher = {Springer},
	author = {Ueda, Itsuki and Fukuhara, Yoshihiro and Kataoka, Hirokatsu and Aizawa, Hiroaki and Shishido, Hidehiko and Kitahara, Itaru},
	month = jul,
	year = {2022},
	note = {arXiv:2207.14455 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Done Reading, ⛔ No INSPIRE recid found},
	pages = {53--68},
}

@inproceedings{darmon_improving_2022,
	title = {Improving neural implicit surfaces geometry with patch warping},
	url = {http://arxiv.org/abs/2112.09648},
	doi = {10.48550/arXiv.2112.09648},
	abstract = {Neural implicit surfaces have become an important technique for multi-view 3D reconstruction but their accuracy remains limited. In this paper, we argue that this comes from the difficulty to learn and render high frequency textures with neural networks. We thus propose to add to the standard neural rendering optimization a direct photo-consistency term across the different views. Intuitively, we optimize the implicit geometry so that it warps views on each other in a consistent way. We demonstrate that two elements are key to the success of such an approach: (i) warping entire patches, using the predicted occupancy and normals of the 3D points along each ray, and measuring their similarity with a robust structural similarity (SSIM); (ii) handling visibility and occlusion in such a way that incorrect warps are not given too much importance while encouraging a reconstruction as complete as possible. We evaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL benchmarks and show it outperforms state of the art unsupervised implicit surfaces reconstructions by over 20\% on both datasets.},
	urldate = {2023-03-13},
	booktitle = {Computer {Vision} {And} {Pattern} {Recognition}},
	publisher = {arXiv},
	author = {Darmon, François and Bascle, Bénédicte and Devaux, Jean-Clément and Monasse, Pascal and Aubry, Mathieu},
	month = may,
	year = {2022},
	note = {arXiv:2112.09648 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_pet-neus_2023,
	title = {{PET}-{NeuS}: {Positional} {Encoding} {Triplanes} for {Neural} {Surfaces}},
	shorttitle = {{PET}-{NeuS}},
	url = {https://openreview.net/forum?id=MHXO5xRCSXh},
	abstract = {The signed distance function (SDF) represented by an MLP network is commonly used for multi-view neural surface reconstruction. We build on the successful recent method NeuS to extend it by three new components. The first component is to borrow the Tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only. Discretizing the scene space with Tri-planes leads to a more expressive data structure but involving tri-planes will introduce noise due to discrete discontinuities. The second component is to use a new type of positional encoding with learnable weights to combat noise in the reconstruction process. We divide the features in the tri-plane into multiple frequency bands and modulate them with sin and cos functions of different frequency. The third component is to use learnable convolution operations on the tri-plane features using self-attention convolution to produce features with different frequency. The experiments show that PET-NeuS achieves high-fidelity surface reconstruction on standard datasets. Following previous work and using the Chamfer metric as the most important way to measure surface reconstruction quality, we are able to improve upon the NeuS baseline by 25{\textbackslash}\% on Nerf-synthetic (0.84 compared to 1.12) and by 14{\textbackslash}\% on DTU (0.75 compared to 0.87). The qualitative evaluation reveals how our method can better control the interference of high-frequency noise.},
	language = {en},
	urldate = {2023-03-13},
	author = {Wang, Yiqun and Skorokhodov, Ivan and Wonka, Peter},
	month = feb,
	year = {2023},
	keywords = {/unread, ⛔ No DOI found},
}

@inproceedings{yariv_volume_2021,
	title = {Volume {Rendering} of {Neural} {Implicit} {Surfaces}},
	url = {http://arxiv.org/abs/2106.12052},
	doi = {10.48550/arXiv.2106.12052},
	abstract = {Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.},
	urldate = {2023-02-17},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Yariv, Lior and Gu, Jiatao and Kasten, Yoni and Lipman, Yaron},
	month = dec,
	year = {2021},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yariv_multiview_2020,
	title = {Multiview {Neural} {Surface} {Reconstruction} by {Disentangling} {Geometry} and {Appearance}},
	url = {http://arxiv.org/abs/2003.09852},
	doi = {10.48550/arXiv.2003.09852},
	abstract = {In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.},
	urldate = {2023-03-27},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {arXiv},
	author = {Yariv, Lior and Kasten, Yoni and Moran, Dror and Galun, Meirav and Atzmon, Matan and Basri, Ronen and Lipman, Yaron},
	month = oct,
	year = {2020},
	note = {arXiv:2003.09852 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@inproceedings{shah_airsim_2017,
	series = {Springer {Proceedings} in {Advanced} {Robotics}},
	title = {{AirSim}: {High}-{Fidelity} {Visual} and {Physical} {Simulation} for {Autonomous} {Vehicles}},
	volume = {5},
	shorttitle = {{AirSim}},
	url = {https://arxiv.org/abs/1705.05065},
	doi = {10.1007/978-3-319-67361-5_40},
	abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Field and {Service} {Robotics}, {Results} of the 11th {International} {Conference}, \{{FSR}\} 2017, {Zurich}, {Switzerland}, 12-15 {September} 2017},
	publisher = {Springer},
	author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
	year = {2017},
	note = {titleTranslation:},
	keywords = {/unread},
	pages = {621--635},
}

@inproceedings{song_deep_2019,
	address = {United Kingdom},
	title = {Deep network for simultaneous stereo matching and dehazing},
	shorttitle = {Deep network for simultaneous stereo matching and dehazing},
	url = {http://www.scopus.com/inward/record.url?scp=85072312755&partnerID=8YFLogxK},
	abstract = {Unveiling the image structure and dense correspondence under the haze layer remains a challenging task, since the scattering effects cause image features to be less distinctive. In this paper, we introduce a deep network that simultaneously estimates a clear latent image and disparity from a hazy stereo image pair. To this end, inspired by a physical model of hazy image acquisition, we propose a dehazing loss function which serves as an additional cue for establishing dense correspondence. We show that stereo matching and dehazing can be synergistically formulated by incorporating depth information from haze transmission into the stereo matching process, and vice versa. As a result, our method estimates high-quality disparity for scenes in scattering media, and produces appearance images with enhanced visibility. We quantitatively evaluate the proposed method on synthetic datasets and provide an extensive ablation study. Experimental results demonstrate that our approach outperforms the recent state-of-the-art methods on both dehazing and stereo matching tasks.},
	urldate = {2023-03-07},
	booktitle = {29th {British} {Machine} {Vision} {Conference}, {BMVC} 2018},
	author = {Song, Taeyong and Kim, Youngjung and Oh, Changjae and Sohn, Kwanghoon},
	month = jan,
	year = {2019},
	note = {titleTranslation:},
	keywords = {/unread},
}

@inproceedings{barron_mip-nerf_2021,
	title = {Mip-{NeRF}: {A} {Multiscale} {Representation} for {Anti}-{Aliasing} {Neural} {Radiance} {Fields}},
	shorttitle = {Mip-{NeRF}},
	doi = {10/gq3nkn},
	abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (à la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Computational modeling, Computer vision, Error analysis, Image resolution, Low-level and physics-based vision, Multilayer perceptrons, Neural networks, Stereo, Training, ⛔ No INSPIRE recid found},
	pages = {5835--5844},
}

@misc{noauthor_being_nodate,
	title = {Being {Friends} with {Floaters}: {Learning} {Radiance} {Fields} from {Hazy} {Images}},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@misc{zhu_nicer-slam_2023,
	title = {{NICER}-{SLAM}: {Neural} {Implicit} {Scene} {Encoding} for {RGB} {SLAM}},
	shorttitle = {{NICER}-{SLAM}},
	url = {http://arxiv.org/abs/2302.03594},
	doi = {10.48550/arXiv.2302.03594},
	abstract = {Neural implicit representations have recently become popular in simultaneous localization and mapping (SLAM), especially in dense visual SLAM. However, previous works in this direction either rely on RGB-D sensors, or require a separate monocular SLAM approach for camera tracking and do not produce high-fidelity dense 3D scene reconstruction. In this paper, we present NICER-SLAM, a dense RGB SLAM system that simultaneously optimizes for camera poses and a hierarchical neural implicit map representation, which also allows for high-quality novel view synthesis. To facilitate the optimization process for mapping, we integrate additional supervision signals including easy-to-obtain monocular geometric cues and optical flow, and also introduce a simple warping loss to further enforce geometry consistency. Moreover, to further boost performance in complicated indoor scenes, we also propose a local adaptive transformation from signed distance functions (SDFs) to density in the volume rendering equation. On both synthetic and real-world datasets we demonstrate strong performance in dense mapping, tracking, and novel view synthesis, even competitive with recent RGB-D SLAM systems.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Cui, Zhaopeng and Oswald, Martin R. and Geiger, Andreas and Pollefeys, Marc},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03594 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{zhong_shine-mapping_2022,
	title = {{SHINE}-{Mapping}: {Large}-{Scale} {3D} {Mapping} {Using} {Sparse} {Hierarchical} {Implicit} {Neural} {Representations}},
	shorttitle = {{SHINE}-{Mapping}},
	url = {http://arxiv.org/abs/2210.02299},
	doi = {10.48550/arXiv.2210.02299},
	abstract = {Accurate mapping of large-scale environments is an essential building block of most outdoor autonomous systems. Challenges of traditional mapping methods include the balance between memory consumption and mapping accuracy. This paper addresses the problems of achieving large-scale 3D reconstructions with implicit representations using 3D LiDAR measurements. We learn and store implicit features through an octree-based hierarchical structure, which is sparse and extensible. The features can be turned into signed distance values through a shallow neural network. We leverage binary cross entropy loss to optimize the local features with the 3D measurements as supervision. Based on our implicit representation, we design an incremental mapping system with regularization to tackle the issue of catastrophic forgetting in continual learning. Our experiments show that our 3D reconstructions are more accurate, complete, and memory-efficient than current state-of-the-art 3D mapping methods.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Zhong, Xingguang and Pan, Yue and Behley, Jens and Stachniss, Cyrill},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02299 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Done Reading, ⛔ No INSPIRE recid found},
}

@misc{zhong_snake_2022,
	title = {{SNAKE}: {Shape}-aware {Neural} {3D} {Keypoint} {Field}},
	shorttitle = {{SNAKE}},
	url = {http://arxiv.org/abs/2206.01724},
	doi = {10.48550/arXiv.2206.01724},
	abstract = {Detecting 3D keypoints from point clouds is important for shape reconstruction, while this work investigates the dual question: can shape reconstruction benefit 3D keypoint detection? Existing methods either seek salient features according to statistics of different orders or learn to predict keypoints that are invariant to transformation. Nevertheless, the idea of incorporating shape reconstruction into 3D keypoint detection is under-explored. We argue that this is restricted by former problem formulations. To this end, a novel unsupervised paradigm named SNAKE is proposed, which is short for shape-aware neural 3D keypoint field. Similar to recent coordinate-based radiance or distance field, our network takes 3D coordinates as inputs and predicts implicit shape indicators and keypoint saliency simultaneously, thus naturally entangling 3D keypoint detection and shape reconstruction. We achieve superior performance on various public benchmarks, including standalone object datasets ModelNet40, KeypointNet, SMPL meshes and scene-level datasets 3DMatch and Redwood. Intrinsic shape awareness brings several advantages as follows. (1) SNAKE generates 3D keypoints consistent with human semantic annotation, even without such supervision. (2) SNAKE outperforms counterparts in terms of repeatability, especially when the input point clouds are down-sampled. (3) the generated keypoints allow accurate geometric registration, notably in a zero-shot setting. Codes are available at https://github.com/zhongcl-thu/SNAKE},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Zhong, Chengliang and You, Peixing and Chen, Xiaoxue and Zhao, Hao and Sun, Fuchun and Zhou, Guyue and Mu, Xiaodong and Gan, Chuang and Huang, Wenbing},
	month = oct,
	year = {2022},
	note = {arXiv:2206.01724 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@article{zhang_efficient_2023,
	title = {Efficient {Large}-scale {Scene} {Representation} with a {Hybrid} of {High}-resolution {Grid} and {Plane} {Features}},
	url = {https://arxiv.org/abs/2303.03003},
	doi = {10/gr8wf8},
	abstract = {Existing neural radiance fields (NeRF) methods for large-scale scene modeling require days of training using multiple GPUs, hindering their applications in scenarios with limited computing resources. Despite fast optimization NeRF variants have been proposed based on the explicit dense or hash grid features, their effectivenesses are mainly demonstrated in object-scale scene representation. In this paper, we point out that the low feature resolution in explicit representation is the bottleneck for large-scale unbounded scene representation. To address this problem, we introduce a new and efficient hybrid feature representation for NeRF that fuses the 3D hash-grids and high-resolution 2D dense plane features. Compared with the dense-grid representation, the resolution of a dense 2D plane can be scaled up more efficiently. Based on this hybrid representation, we propose a fast optimization NeRF variant, called GP-NeRF, that achieves better rendering results while maintaining a compact model size. Extensive experiments on multiple large-scale unbounded scene datasets show that our model can converge in 1.5 hours using a single GPU while achieving results comparable to or even better than the existing method that requires about one day's training with 8 GPUs.},
	language = {en},
	urldate = {2023-03-12},
	author = {Zhang, Yuqi and Chen, Guanying and Cui, Shuguang},
	year = {2023},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@misc{zhang_nerflets_2023,
	title = {Nerflets: {Local} {Radiance} {Fields} for {Efficient} {Structure}-{Aware} {3D} {Scene} {Representation} from {2D} {Supervision}},
	shorttitle = {Nerflets},
	url = {http://arxiv.org/abs/2303.03361},
	doi = {10.48550/arXiv.2303.03361},
	abstract = {We address efficient and structure-aware 3D scene representation from images. Nerflets are our key contribution -- a set of local neural radiance fields that together represent a scene. Each nerflet maintains its own spatial position, orientation, and extent, within which it contributes to panoptic, density, and radiance reconstructions. By leveraging only photometric and inferred panoptic image supervision, we can directly and jointly optimize the parameters of a set of nerflets so as to form a decomposed representation of the scene, where each object instance is represented by a group of nerflets. During experiments with indoor and outdoor environments, we find that nerflets: (1) fit and approximate the scene more efficiently than traditional global NeRFs, (2) allow the extraction of panoptic and photometric renderings from arbitrary views, and (3) enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive editing.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Zhang, Xiaoshuai and Kundu, Abhijit and Funkhouser, Thomas and Guibas, Leonidas and Su, Hao and Genova, Kyle},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03361 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No INSPIRE recid found},
}

@misc{zhang_nerfusion_2022,
	title = {{NeRFusion}: {Fusing} {Radiance} {Fields} for {Large}-{Scale} {Scene} {Reconstruction}},
	shorttitle = {{NeRFusion}},
	url = {http://arxiv.org/abs/2203.11283},
	doi = {10.48550/arXiv.2203.11283},
	abstract = {While NeRF has shown great success for neural reconstruction and rendering, its limited MLP capacity and long per-scene optimization times make it challenging to model large-scale indoor scenes. In contrast, classical 3D reconstruction methods can handle large-scale scenes but do not produce realistic renderings. We propose NeRFusion, a method that combines the advantages of NeRF and TSDF-based fusion techniques to achieve efficient large-scale reconstruction and photo-realistic rendering. We process the input image sequence to predict per-frame local radiance fields via direct network inference. These are then fused using a novel recurrent neural network that incrementally reconstructs a global, sparse scene representation in real-time at 22 fps. This global volume can be further fine-tuned to boost rendering quality. We demonstrate that NeRFusion achieves state-of-the-art quality on both large-scale indoor and small-scale object scenes, with substantially faster reconstruction than NeRF and other recent methods.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Zhang, Xiaoshuai and Bi, Sai and Sunkavalli, Kalyan and Su, Hao and Xu, Zexiang},
	month = mar,
	year = {2022},
	note = {arXiv:2203.11283 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading, ⛔ No INSPIRE recid found},
}

@inproceedings{avidan_digging_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Digging into {Radiance} {Grid} for {Real}-{Time} {View} {Synthesis} with {Detail} {Preservation}},
	volume = {13675},
	isbn = {978-3-031-19784-0},
	url = {https://link.springer.com/10.1007/978-3-031-19784-0_42},
	doi = {10/gr8wf4},
	abstract = {Neural Radiance Fields (NeRF) [31] series are impressive in representing scenes and synthesizing high-quality novel views. However, most previous works fail to preserve texture details and suffer from slow training speed. A recent method SNeRG [11] demonstrates that baking a trained NeRF as a Sparse Neural Radiance Grid enables real-time view synthesis with slight scarification of rendering quality. In this paper, we dig into the Radiance Grid representation and present a set of improvements, which together result in boosted performance in terms of both speed and quality. First, we propose an HieRarchical Sparse Radiance Grid (HrSRG) representation that has higher voxel resolution for informative spaces and fewer voxels for other spaces. HrSRG leverages a hierarchical voxel grid building process inspired by [30, 55], and can describe a scene at high resolution without excessive memory footprint. Furthermore, we show that directly optimizing the voxel grid leads to surprisingly good texture details in rendered images. This direct optimization is memory-friendly and requires multiple orders of magnitude less time than conventional NeRFs as it only involves a tiny MLP. Finally, we find that a critical factor that prevents fine details restoration is the misaligned 2D pixels among images caused by camera pose errors. We propose to use the perceptual loss to add tolerance to misalignments, leading to the improved visual quality of rendered images.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Zhang, Jian and Huang, Jinchi and Cai, Bowen and Fu, Huan and Gong, Mingming and Wang, Chaohui and Wang, Jiaming and Luo, Hongchen and Jia, Rongfei and Zhao, Binqiang and Tang, Xing},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {/unread, 3D representation, Real-time rendering, View synthesis, ⛔ No INSPIRE recid found},
	pages = {724--740},
}

@misc{zhang_3dshape2vecset_2023,
	title = {{3DShape2VecSet}: {A} {3D} {Shape} {Representation} for {Neural} {Fields} and {Generative} {Diffusion} {Models}},
	shorttitle = {{3DShape2VecSet}},
	url = {http://arxiv.org/abs/2301.11445},
	doi = {10.48550/arXiv.2301.11445},
	abstract = {We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Zhang, Biao and Tang, Jiapeng and Niessner, Matthias and Wonka, Peter},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11445 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No INSPIRE recid found},
}

@inproceedings{yuan_nerf-editing_2022,
	title = {{NeRF}-{Editing}: {Geometry} {Editing} of {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{Editing}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-10-25},
	author = {Yuan, Yu-Jie and Sun, Yang-Tian and Lai, Yu-Kun and Ma, Yuewen and Jia, Rongfei and Gao, Lin},
	year = {2022},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {18353--18364},
}

@misc{yang_learning_2023,
	title = {Learning a {Diffusion} {Prior} for {NeRFs}},
	url = {http://arxiv.org/abs/2304.14473},
	doi = {10.48550/arXiv.2304.14473},
	abstract = {Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D representation for objects and scenes derived from 2D data. Generating NeRFs, however, remains difficult in many scenarios. For instance, training a NeRF with only a small number of views as supervision remains challenging since it is an under-constrained problem. In such settings, it calls for some inductive prior to filter out bad local minima. One way to introduce such inductive priors is to learn a generative model for NeRFs modeling a certain class of scenes. In this paper, we propose to use a diffusion model to generate NeRFs encoded on a regularized grid. We show that our model can sample realistic NeRFs, while at the same time allowing conditional generations, given a certain observation as guidance.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Yang, Guandao and Kundu, Abhijit and Guibas, Leonidas J. and Barron, Jonathan T. and Poole, Ben},
	month = apr,
	year = {2023},
	note = {arXiv:2304.14473 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No INSPIRE recid found},
}

@inproceedings{watson_heightfields_2023,
	title = {Heightfields for {Efficient} {Scene} {Reconstruction} for {AR}},
	url = {https://openaccess.thecvf.com/content/WACV2023/html/Watson_Heightfields_for_Efficient_Scene_Reconstruction_for_AR_WACV_2023_paper.html},
	language = {en},
	urldate = {2023-01-04},
	author = {Watson, Jamie and Vicente, Sara and Mac Aodha, Oisin and Godard, Clément and Brostow, Gabriel and Firman, Michael},
	year = {2023},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {5850--5860},
}

@misc{wang_benchmarking_2023,
	title = {Benchmarking {Robustness} in {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2301.04075},
	doi = {10.48550/arXiv.2301.04075},
	abstract = {Neural Radiance Field (NeRF) has demonstrated excellent quality in novel view synthesis, thanks to its ability to model 3D object geometries in a concise formulation. However, current approaches to NeRF-based models rely on clean images with accurate camera calibration, which can be difficult to obtain in the real world, where data is often subject to corruption and distortion. In this work, we provide the first comprehensive analysis of the robustness of NeRF-based novel view synthesis algorithms in the presence of different types of corruptions. We find that NeRF-based models are significantly degraded in the presence of corruption, and are more sensitive to a different set of corruptions than image recognition models. Furthermore, we analyze the robustness of the feature encoder in generalizable methods, which synthesize images using neural features extracted via convolutional neural networks or transformers, and find that it only contributes marginally to robustness. Finally, we reveal that standard data augmentation techniques, which can significantly improve the robustness of recognition models, do not help the robustness of NeRF-based models. We hope that our findings will attract more researchers to study the robustness of NeRF-based approaches and help to improve their performance in the real world.},
	urldate = {2023-01-17},
	publisher = {arXiv},
	author = {Wang, Chen and Wang, Angtian and Li, Junbo and Yuille, Alan and Xie, Cihang},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04075 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{wang_neuris_2022,
	title = {{NeuRIS}: {Neural} {Reconstruction} of {Indoor} {Scenes} {Using} {Normal} {Priors}},
	shorttitle = {{NeuRIS}},
	url = {http://arxiv.org/abs/2206.13597},
	doi = {10.48550/arXiv.2206.13597},
	abstract = {Reconstructing 3D indoor scenes from 2D images is an important task in many computer vision and graphics applications. A main challenge in this task is that large texture-less areas in typical indoor scenes make existing methods struggle to produce satisfactory reconstruction results. We propose a new method, named NeuRIS, for high quality reconstruction of indoor scenes. The key idea of NeuRIS is to integrate estimated normal of indoor scenes as a prior in a neural rendering framework for reconstructing large texture-less shapes and, importantly, to do this in an adaptive manner to also enable the reconstruction of irregular shapes with fine details. Specifically, we evaluate the faithfulness of the normal priors on-the-fly by checking the multi-view consistency of reconstruction during the optimization process. Only the normal priors accepted as faithful will be utilized for 3D reconstruction, which typically happens in the regions of smooth shapes possibly with weak texture. However, for those regions with small objects or thin structures, for which the normal priors are usually unreliable, we will only rely on visual features of the input images, since such regions typically contain relatively rich visual features (e.g., shade changes and boundary contours). Extensive experiments show that NeuRIS significantly outperforms the state-of-the-art methods in terms of reconstruction quality.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Wang, Jiepeng and Wang, Peng and Long, Xiaoxiao and Theobalt, Christian and Komura, Taku and Liu, Lingjie and Wang, Wenping},
	month = oct,
	year = {2022},
	note = {arXiv:2206.13597 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Done Reading, NeuS, ⛔ No INSPIRE recid found},
}

@misc{wallingford_neural_2023,
	title = {Neural {Radiance} {Field} {Codebooks}},
	url = {http://arxiv.org/abs/2301.04101},
	doi = {10.48550/arXiv.2301.04101},
	abstract = {Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1\% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29\% relative improvement). Finally, we show that NRC improves on the task of depth ordering by 5.5\% accuracy in THOR.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Wallingford, Matthew and Kusupati, Aditya and Fang, Alex and Ramanujan, Vivek and Kembhavi, Aniruddha and Mottaghi, Roozbeh and Farhadi, Ali},
	month = apr,
	year = {2023},
	note = {arXiv:2301.04101 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No INSPIRE recid found},
}

@inproceedings{turki_mega-nerf_2022,
	title = {Mega-{NeRF}: {Scalable} {Construction} of {Large}-{Scale} {NeRFs} for {Virtual} {Fly}- {Throughs}},
	shorttitle = {Mega-{NeRF}},
	doi = {10/gr8wgr},
	abstract = {We use neural radiance fields (NeRFs) to build interac-tive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected pri-marily from drones. In contrast to single object scenes (on which NeRFs are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thou-sands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) pro-hibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs. To address these challenges, we begin by analyzing visi-bility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to dif-ferent regions of the scene. We introduce a simple geomet-ric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different NeRF sub-modules that can be trained in parallel. We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12\%. We also evaluate re-cent NeRF fast renderers on top of Mega-NeRF and intro-duce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Turki, Haithem and Ramanan, Deva and Satyanarayanan, Mahadev},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {3D from multi-view and sensors, Buildings, Clustering algorithms, Datasets and evaluation, Deep learning architectures and techniques, Done Reading, Efficient learning and inferences, Image and video synthesis and generation, Rendering (computer graphics), Three-dimensional displays, Training, Urban areas, Vision applications and systems, Visualization, ⛔ No INSPIRE recid found},
	pages = {12912--12921},
}

@misc{tian_mononerf_2022,
	title = {{MonoNeRF}: {Learning} a {Generalizable} {Dynamic} {Radiance} {Field} from {Monocular} {Videos}},
	shorttitle = {{MonoNeRF}},
	url = {http://arxiv.org/abs/2212.13056},
	doi = {10.48550/arXiv.2212.13056},
	abstract = {In this paper, we target at the problem of learning a generalizable dynamic radiance field from monocular videos. Different from most existing NeRF methods that are based on multiple views, monocular videos only contain one view at each timestamp, thereby suffering from ambiguity along the view direction in estimating point features and scene flows. Previous studies such as DynNeRF disambiguate point features by positional encoding, which is not transferable and severely limits the generalization ability. As a result, these methods have to train one independent model for each scene and suffer from heavy computational costs when applying to increasing monocular videos in real-world applications. To address this, We propose MonoNeRF to simultaneously learn point features and scene flows with point trajectory and feature correspondence constraints across frames. More specifically, we learn an implicit velocity field to estimate point trajectory from temporal features with Neural ODE, which is followed by a flow-based feature aggregation module to obtain spatial features along the point trajectory. We jointly optimize temporal and spatial features by training the network in an end-to-end manner. Experiments show that our MonoNeRF is able to learn from multiple scenes and support new applications such as scene editing, unseen frame synthesis, and fast novel scene adaptation.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Tian, Fengrui and Du, Shaoyi and Duan, Yueqi},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13056 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{tiwari_pose-ndf_2022,
	title = {Pose-{NDF}: {Modeling} {Human} {Pose} {Manifolds} with {Neural} {Distance} {Fields}},
	shorttitle = {Pose-{NDF}},
	url = {http://arxiv.org/abs/2207.13807},
	doi = {10.48550/arXiv.2207.13807},
	abstract = {We present Pose-NDF, a continuous model for plausible human poses based on neural distance fields (NDFs). Pose or motion priors are important for generating realistic new poses and for reconstructing accurate poses from noisy or partial observations. Pose-NDF learns a manifold of plausible poses as the zero level set of a neural implicit function, extending the idea of modeling implicit surfaces in 3D to the high-dimensional domain SO(3){\textasciicircum}K, where a human pose is defined by a single data point, represented by K quaternions. The resulting high-dimensional implicit function can be differentiated with respect to the input poses and thus can be used to project arbitrary poses onto the manifold by using gradient descent on the set of 3-dimensional hyperspheres. In contrast to previous VAE-based human pose priors, which transform the pose space into a Gaussian distribution, we model the actual pose manifold, preserving the distances between poses. We demonstrate that PoseNDF outperforms existing state-of-the-art methods as a prior in various downstream tasks, ranging from denoising real-world human mocap data, pose recovery from occluded data to 3D pose reconstruction from images. Furthermore, we show that it can be used to generate more diverse poses by random sampling and projection than VAE-based methods.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Tiwari, Garvita and Antic, Dimitrije and Lenssen, Jan Eric and Sarafianos, Nikolaos and Tung, Tony and Pons-Moll, Gerard},
	month = jul,
	year = {2022},
	note = {arXiv:2207.13807 [cs]},
	keywords = {2022, Best Paper, Computer Science - Computer Vision and Pattern Recognition, ECCV, Human Pose Manifold, Manifolds, Max Planck, Meta, Pose-NDF, University of Tubingen, ⛔ No INSPIRE recid found},
}

@misc{tancik_fourier_2020,
	title = {Fourier {Features} {Let} {Networks} {Learn} {High} {Frequency} {Functions} in {Low} {Dimensional} {Domains}},
	url = {http://arxiv.org/abs/2006.10739},
	doi = {10.48550/arXiv.2006.10739},
	abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10739 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ⛔ No INSPIRE recid found},
}

@inproceedings{tancik_block-nerf_2022,
	title = {Block-{NeRF}: {Scalable} {Large} {Scene} {Neural} {View} {Synthesis}},
	shorttitle = {Block-{NeRF}},
	url = {https://ieeexplore.ieee.org/document/9879943/},
	doi = {10/gq57nd},
	abstract = {We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to de-compose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Tancik, Matthew and Casser, Vincent and Yan, Xinchen and Pradhan, Sabeek and Mildenhall, Ben P. and Srinivasan, Pratul and Barron, Jonathan T. and Kretzschmar, Henrik},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Buildings, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer architecture, Computer vision, Image reconstruction, Pattern recognition, Photogrammetry and remote sensing, Read Later, Rendering (computer graphics), Transient analysis, Vision applications and systems, ⛔ No INSPIRE recid found},
	pages = {8238--8248},
}

@inproceedings{sun_neuralrecon_2021,
	title = {{NeuralRecon}: {Real}-{Time} {Coherent} {3D} {Reconstruction} {From} {Monocular} {Video}},
	shorttitle = {{NeuralRecon}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-01-04},
	author = {Sun, Jiaming and Xie, Yiming and Chen, Linghao and Zhou, Xiaowei and Bao, Hujun},
	year = {2021},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {15598--15607},
}

@inproceedings{sucar_imap_2021,
	title = {{iMAP}: {Implicit} {Mapping} and {Positioning} in {Real}-{Time}},
	shorttitle = {{iMAP}},
	doi = {10/gqrgzs},
	abstract = {We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking.Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Efficient training and inference methods, Heuristic algorithms, Neural generative models, Representation learning, Simultaneous localization and mapping, Solid modeling, Stereo, Streaming media, Technological innovation, Three-dimensional displays, Training, ⛔ No INSPIRE recid found},
	pages = {6209--6218},
}

@misc{son_singraf_2022,
	title = {{SinGRAF}: {Learning} a {3D} {Generative} {Radiance} {Field} for a {Single} {Scene}},
	shorttitle = {{SinGRAF}},
	url = {https://arxiv.org/abs/2211.17260v2},
	abstract = {Generative models have shown great promise in synthesizing photorealistic 3D objects, but they require large amounts of training data. We introduce SinGRAF, a 3D-aware generative model that is trained with a few input images of a single scene. Once trained, SinGRAF generates different realizations of this 3D scene that preserve the appearance of the input while varying scene layout. For this purpose, we build on recent progress in 3D GAN architectures and introduce a novel progressive-scale patch discrimination approach during training. With several experiments, we demonstrate that the results produced by SinGRAF outperform the closest related works in both quality and diversity by a large margin.},
	language = {en},
	urldate = {2023-04-21},
	author = {Son, Minjung and Park, Jeong Joon and Guibas, Leonidas and Wetzstein, Gordon},
	month = nov,
	year = {2022},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@inproceedings{avidan_conditional-flow_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Conditional-{Flow} {NeRF}: {Accurate} {3D} {Modelling} with {Reliable} {Uncertainty} {Quantification}},
	volume = {13663},
	isbn = {978-3-031-20062-5},
	shorttitle = {Conditional-{Flow} {NeRF}},
	url = {https://link.springer.com/10.1007/978-3-031-20062-5_31},
	doi = {10/gr8wfx},
	abstract = {A critical limitation of current methods based on Neural Radiance Fields (NeRF) is that they are unable to quantify the uncertainty associated with the learned appearance and geometry of the scene. This information is paramount in real applications such as medical diagnosis or autonomous driving where, to reduce potentially catastrophic failures, the confidence on the model outputs must be included into the decision-making process. In this context, we introduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to incorporate uncertainty quantification into NeRF-based approaches. For this purpose, our method learns a distribution over all possible radiance fields modelling the scene which is used to quantify the uncertainty associated with the modelled scene. In contrast to previous approaches enforcing strong constraints over the radiance field distribution, CF-NeRF learns it in a flexible and fully data-driven manner by coupling Latent Variable Modelling and Conditional Normalizing Flows. This strategy allows to obtain reliable uncertainty estimation while preserving model expressivity. Compared to previous state-of-the-art methods proposed for uncertainty quantification in NeRF, our experiments show that the proposed method achieves significantly lower prediction errors and more reliable uncertainty values for synthetic novel view and depth-map estimation.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Shen, Jianxiong and Agudo, Antonio and Moreno-Noguer, Francesc and Ruiz, Adria},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {/unread, 3D, Scene representation, Uncertainty quantification, ⛔ No INSPIRE recid found},
	pages = {540--557},
}

@misc{shi_city-scale_2022,
	title = {City-scale {Incremental} {Neural} {Mapping} with {Three}-layer {Sampling} and {Panoptic} {Representation}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	url = {http://arxiv.org/abs/2209.14072},
	abstract = {Neural implicit representations are drawing a lot of attention from the robotics community recently, as they are expressive, continuous and compact. However, city-scale incremental implicit dense mapping based on sparse LiDAR input is still an under-explored challenge. To this end,we successfully build the first city-scale incremental neural mapping system with a panoptic representation that consists of both environment-level and instance-level modelling. Given a stream of sparse LiDAR point cloud, it maintains a dynamic generative model that maps 3D coordinates to signed distance field (SDF) values. To address the difficulty of representing geometric information at different levels in city-scale space, we propose a tailored three-layer sampling strategy to dynamically sample the global, local and near-surface domains. Meanwhile, to realize high fidelity mapping, category-specific prior is introduced to better model the geometric details, leading to a panoptic representation. We evaluate on the public SemanticKITTI dataset and demonstrate the significance of the newly proposed three-layer sampling strategy and panoptic representation, using both quantitative and qualitative results. Codes and data will be publicly available.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Shi, Yongliang and Yang, Runyi and Li, Pengfei and Wu, Zirui and Zhao, Hao and Zhou, Guyue},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14072 [cs]},
	keywords = {2022, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Done Reading, Implicit Representation, RA-L, SDF, ⛔ No INSPIRE recid found},
}

@misc{sethuraman_waternerf_2022,
	title = {{WaterNeRF}: {Neural} {Radiance} {Fields} for {Underwater} {Scenes}},
	shorttitle = {{WaterNeRF}},
	url = {http://arxiv.org/abs/2209.13091},
	doi = {10.48550/arXiv.2209.13091},
	abstract = {Underwater imaging is a critical task performed by marine robots for a wide range of applications including aquaculture, marine infrastructure inspection, and environmental monitoring. However, water column effects, such as attenuation and backscattering, drastically change the color and quality of imagery captured underwater. Due to varying water conditions and range-dependency of these effects, restoring underwater imagery is a challenging problem. This impacts downstream perception tasks including depth estimation and 3D reconstruction. In this paper, we advance state-of-the-art in neural radiance fields (NeRFs) to enable physics-informed dense depth estimation and color correction. Our proposed method, WaterNeRF, estimates parameters of a physics-based model for underwater image formation, leading to a hybrid data-driven and model-based solution. After determining the scene structure and radiance field, we can produce novel views of degraded as well as corrected underwater images, along with dense depth of the scene. We evaluate the proposed method qualitatively and quantitatively on a real underwater dataset.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Sethuraman, Advaith Venkatramanan and Ramanagopal, Manikandasriram Srinivasan and Skinner, Katherine A.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.13091 [cs, eess]
version: 1},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing, NeRF, Read Later, ⛔ No INSPIRE recid found},
}

@inproceedings{shen_omninerf_2022,
	title = {{OmniNeRF}: {Hybriding} {Omnidirectional} {Distance} and {Radiance} fields for {Neural} {Surface} {Reconstruction}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	shorttitle = {{OmniNeRF}},
	url = {http://arxiv.org/abs/2209.13433},
	abstract = {3D reconstruction from images has wide applications in Virtual Reality and Automatic Driving, where the precision requirement is very high. Ground-breaking research in the neural radiance field (NeRF) by utilizing Multi-Layer Perceptions has dramatically improved the representation quality of 3D objects. Some later studies improved NeRF by building truncated signed distance fields (TSDFs) but still suffer from the problem of blurred surfaces in 3D reconstruction. In this work, this surface ambiguity is addressed by proposing a novel way of 3D shape representation, OmniNeRF. It is based on training a hybrid implicit field of Omni-directional Distance Field (ODF) and neural radiance field, replacing the apparent density in NeRF with omnidirectional information. Moreover, we introduce additional supervision on the depth map to further improve reconstruction quality. The proposed method has been proven to effectively deal with NeRF defects at the edges of the surface reconstruction, providing higher quality 3D scene reconstruction results.},
	urldate = {2022-10-05},
	author = {Shen, Jiaming and {Bolin Song} and {Zirui Wu} and {Yi Xu}},
	month = sep,
	year = {2022},
	note = {arXiv:2209.13433 [cs]},
	keywords = {2022, Computer Science - Computer Vision and Pattern Recognition, Done Reading, My, NeRF, ⛔ No INSPIRE recid found},
}

@misc{seo_mixnerf_2023,
	title = {{MixNeRF}: {Modeling} a {Ray} with {Mixture} {Density} for {Novel} {View} {Synthesis} from {Sparse} {Inputs}},
	shorttitle = {{MixNeRF}},
	url = {http://arxiv.org/abs/2302.08788},
	doi = {10.48550/arXiv.2302.08788},
	abstract = {Neural Radiance Field (NeRF) has broken new ground in the novel view synthesis due to its simple concept and state-of-the-art quality. However, it suffers from severe performance degradation unless trained with a dense set of images with different camera poses, which hinders its practical applications. Although previous methods addressing this problem achieved promising results, they relied heavily on the additional training resources, which goes against the philosophy of sparse-input novel-view synthesis pursuing the training efficiency. In this work, we propose MixNeRF, an effective training strategy for novel view synthesis from sparse inputs by modeling a ray with a mixture density model. Our MixNeRF estimates the joint distribution of RGB colors along the ray samples by modeling it with mixture of distributions. We also propose a new task of ray depth estimation as a useful training objective, which is highly correlated with 3D scene geometry. Moreover, we remodel the colors with regenerated blending weights based on the estimated ray depth and further improves the robustness for colors and viewpoints. Our MixNeRF outperforms other state-of-the-art methods in various standard benchmarks with superior efficiency of training and inference.},
	urldate = {2023-02-20},
	publisher = {arXiv},
	author = {Seo, Seunghyeon and Han, Donghoon and Chang, Yeonjin and Kwak, Nojun},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08788 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{saragadam_wire_2023,
	title = {{WIRE}: {Wavelet} {Implicit} {Neural} {Representations}},
	shorttitle = {{WIRE}},
	url = {http://arxiv.org/abs/2301.05187},
	doi = {10.48550/arXiv.2301.05187},
	abstract = {Implicit neural representations (INRs) have recently advanced numerous vision-related areas. INR performance depends strongly on the choice of the nonlinear activation function employed in its multilayer perceptron (MLP) network. A wide range of nonlinearities have been explored, but, unfortunately, current INRs designed to have high accuracy also suffer from poor robustness (to signal noise, parameter variation, etc.). Inspired by harmonic analysis, we develop a new, highly accurate and robust INR that does not exhibit this tradeoff. Wavelet Implicit neural REpresentation (WIRE) uses a continuous complex Gabor wavelet activation function that is well-known to be optimally concentrated in space-frequency and to have excellent biases for representing images. A wide range of experiments (image denoising, image inpainting, super-resolution, computed tomography reconstruction, image overfitting, and novel view synthesis with neural radiance fields) demonstrate that WIRE defines the new state of the art in INR accuracy, training time, and robustness.},
	urldate = {2023-01-17},
	publisher = {arXiv},
	author = {Saragadam, Vishwanath and LeJeune, Daniel and Tan, Jasper and Balakrishnan, Guha and Veeraraghavan, Ashok and Baraniuk, Richard G.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.05187 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Electrical Engineering and Systems Science - Image and Video Processing, ⛔ No INSPIRE recid found},
}

@inproceedings{rudnev_nerf_2022,
	title = {{NeRF} for {Outdoor} {Scene} {Relighting}},
	shorttitle = {{NeRF}-{OSR}},
	url = {http://arxiv.org/abs/2112.05140},
	doi = {10/gr8wgt},
	abstract = {Photorealistic editing of outdoor scenes from photographs requires a profound understanding of the image formation process and an accurate estimation of the scene geometry, reflectance and illumination. A delicate manipulation of the lighting can then be performed while keeping the scene albedo and geometry unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene relighting based on neural radiance fields. In contrast to the prior art, our technique allows simultaneous editing of both scene illumination and camera viewpoint using only a collection of outdoor photos shot in uncontrolled settings. Moreover, it enables direct control over the scene illumination, as defined through a spherical harmonics model. For evaluation, we collect a new benchmark dataset of several outdoor sites photographed from multiple viewpoints and at different times. For each time, a 360 degree environment map is captured together with a colour-calibration chequerboard to allow accurate numerical evaluations on real data against ground truth. Comparisons against SoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at higher quality and with realistic self-shadowing reproduction. Our method and the dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Rudnev, Viktor and Elgharib, Mohamed and Smith, William and Liu, Lingjie and Golyanik, Vladislav and Theobalt, Christian},
	month = jul,
	year = {2022},
	note = {arXiv:2112.05140 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Done Reading, ⛔ No INSPIRE recid found},
}

@inproceedings{roessle_dense_2022,
	title = {Dense {Depth} {Priors} for {Neural} {Radiance} {Fields} from {Sparse} {Input} {Views}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878792/},
	doi = {10/grrsw7},
	abstract = {Neural radiance fields (NeRF) encode a scene into a neural representation that enables photo-realistic rendering of novel views. However, a successful reconstruction from RGB images requires a large number of input views taken under static conditions \&\#x2014; typically up to a few hundred images for room-size scenes. Our method aims to synthesize novel views of whole rooms from an order of magnitude fewer images. To this end, we leverage dense depth priors in order to constrain the NeRF optimization. First, we take advantage of the sparse depth data that is freely available from the structure from motion (SfM) preprocessing step used to estimate camera poses. Second, we use depth completion to convert these sparse points into dense depth maps and uncertainty estimates, which are used to guide NeRF optimization. Our method enables data-efficient novel view synthesis on challenging indoor scenes, using as few as 18 images for an entire scene.},
	language = {English},
	urldate = {2022-10-25},
	publisher = {IEEE Computer Society},
	author = {Roessle, Barbara and Barron, Jonathan T. and Mildenhall, Ben and Srinivasan, Pratul P. and Niebner, Matthias},
	month = jun,
	year = {2022},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
	pages = {12882--12891},
}

@misc{rosinol_nerf-slam_2022,
	title = {{NeRF}-{SLAM}: {Real}-{Time} {Dense} {Monocular} {SLAM} with {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{SLAM}},
	url = {http://arxiv.org/abs/2210.13641},
	doi = {10.48550/arXiv.2210.13641},
	abstract = {We propose a novel geometric and photometric 3D mapping pipeline for accurate and real-time scene reconstruction from monocular images. To achieve this, we leverage recent advances in dense monocular SLAM and real-time hierarchical volumetric neural radiance fields. Our insight is that dense monocular SLAM provides the right information to fit a neural radiance field of the scene in real-time, by providing accurate pose estimates and depth-maps with associated uncertainty. With our proposed uncertainty-based depth loss, we achieve not only good photometric accuracy, but also great geometric accuracy. In fact, our proposed pipeline achieves better geometric and photometric accuracy than competing approaches (up to 179\% better PSNR and 86\% better L1 depth), while working in real-time and using only monocular images.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Rosinol, Antoni and Leonard, John J. and Carlone, Luca},
	month = oct,
	year = {2022},
	note = {arXiv:2210.13641 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@inproceedings{pearl_nan_2022,
	title = {{NAN}: {Noise}-{Aware} {NeRFs} for {Burst}-{Denoising}},
	shorttitle = {{NAN}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Pearl_NAN_Noise-Aware_NeRFs_for_Burst-Denoising_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-06},
	author = {Pearl, Naama and Treibitz, Tali and Korman, Simon},
	year = {2022},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {12672--12681},
}

@misc{rana_inter-view_2023,
	title = {Inter-{View} {Depth} {Consistency} {Testing} in {Depth} {Difference} {Subspace}},
	url = {http://arxiv.org/abs/2301.11752},
	doi = {10.48550/arXiv.2301.11752},
	abstract = {Multiview depth imagery will play a critical role in free-viewpoint television. This technology requires high quality virtual view synthesis to enable viewers to move freely in a dynamic real world scene. Depth imagery at different viewpoints is used to synthesize an arbitrary number of novel views. Usually, depth images at multiple viewpoints are estimated individually by stereo-matching algorithms, and hence, show lack of interview consistency. This inconsistency affects the quality of view synthesis negatively. This paper proposes a method for depth consistency testing in depth difference subspace to enhance the depth representation of a scene across multiple viewpoints. Furthermore, we propose a view synthesis algorithm that uses the obtained consistency information to improve the visual quality of virtual views at arbitrary viewpoints. Our method helps us to find a linear subspace for our depth difference measurements in which we can test the inter-view consistency efficiently. With this, our approach is able to enhance the depth information for real world scenes. In combination with our consistency-adaptive view synthesis, we improve the visual experience of the free-viewpoint user. The experiments show that our approach enhances the objective quality of virtual views by up to 1.4 dB. The advantage for the subjective quality is also demonstrated.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Rana, Pravin Kumar and Flierl, Markus},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11752 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, Electrical Engineering and Systems Science - Image and Video Processing, ⛔ No INSPIRE recid found},
}

@inproceedings{park_nerfies_2021,
	title = {Nerfies: {Deformable} {Neural} {Radiance} {Fields}},
	shorttitle = {Nerfies},
	doi = {10/gq3nmm},
	abstract = {We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub "nerfies." We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Adaptation models, Computer vision, Deformable models, Geometry, Mobile handsets, Optimization methods, Rendering (computer graphics), Stereo, ⛔ No INSPIRE recid found},
	pages = {5845--5854},
}

@inproceedings{park_deepsdf_2019,
	title = {{DeepSDF}: {Learning} {Continuous} {Signed} {Distance} {Functions} for {Shape} {Representation}},
	shorttitle = {{DeepSDF}},
	url = {https://ieeexplore.ieee.org/document/8954065/},
	doi = {10/gg8ftv},
	abstract = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from Multiview and Sensors, Computational modeling, Computer vision, Deep Learning, Geometry, Interpolation, Representation Learning, Shape, Solid modeling, Three-dimensional displays, Vision + Graphics, ⛔ No INSPIRE recid found},
	pages = {165--174},
}

@inproceedings{avidan_activenerf_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ActiveNeRF}: {Learning} {Where} to {See} with {Uncertainty} {Estimation}},
	volume = {13693},
	isbn = {978-3-031-19827-4},
	shorttitle = {{ActiveNeRF}},
	url = {https://link.springer.com/10.1007/978-3-031-19827-4_14},
	doi = {10/gr8wfs},
	abstract = {Recently, Neural Radiance Fields (NeRF) has shown promising performances on reconstructing 3D scenes and synthesizing novel views from a sparse set of 2D images. Albeit effective, the performance of NeRF is highly influenced by the quality of training samples. With limited posed images from the scene, NeRF fails to generalize well to novel views and may collapse to trivial solutions in unobserved regions. This makes NeRF impractical under resource-constrained scenarios. In this paper, we present a novel learning framework, ActiveNeRF, aiming to model a 3D scene with a constrained input budget. Specifically, we first incorporate uncertainty estimation into a NeRF model, which ensures robustness under few observations and provides an interpretation of how NeRF understands the scene. On this basis, we propose to supplement the existing training set with newly captured samples based on an active learning scheme. By evaluating the reduction of uncertainty given new inputs, we select the samples that bring the most information gain. In this way, the quality of novel view synthesis can be improved with minimal additional resources. Extensive experiments validate the performance of our model on both realistic and synthetic scenes, especially with scarcer training data.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Pan, Xuran and Lai, Zihang and Song, Shiji and Huang, Gao},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {/unread, Active learning, Neural radiance fields, Uncertainty estimation, ⛔ No INSPIRE recid found},
	pages = {230--246},
}

@inproceedings{oechsle_unisurf_2021,
	title = {{UNISURF}: {Unifying} {Neural} {Implicit} {Surfaces} and {Radiance} {Fields} for {Multi}-{View} {Reconstruction}},
	shorttitle = {{UNISURF}},
	doi = {10/gp46pc},
	abstract = {Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF’s estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Oechsle, Michael and Peng, Songyou and Geiger, Andreas},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from a single image and shape-from-x, 3D from multiview and other sensors, Computer vision, Done Reading, Image reconstruction, Rendering (computer graphics), Solid modeling, Stereo, Surface reconstruction, Three-dimensional displays, ⛔ No INSPIRE recid found},
	pages = {5569--5579},
}

@inproceedings{mildenhall_nerf_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	isbn = {978-3-030-58452-8},
	shorttitle = {{NeRF}},
	url = {https://arxiv.org/abs/2003.08934},
	doi = {10/gj826m},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction \$\$({\textbackslash}theta ,{\textbackslash}phi )\$\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for  convincing comparisons.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {/unread, 3D deep learning, Done Reading, Emphasis, Image-based rendering, Scene representation, View synthesis, Volume rendering, ⛔ No INSPIRE recid found},
	pages = {405--421},
}

@inproceedings{moreau_imposing_2023,
	title = {{ImPosing}: {Implicit} {Pose} {Encoding} for {Efficient} {Visual} {Localization}},
	shorttitle = {{ImPosing}},
	url = {https://openaccess.thecvf.com/content/WACV2023/html/Moreau_ImPosing_Implicit_Pose_Encoding_for_Efficient_Visual_Localization_WACV_2023_paper.html},
	language = {en},
	urldate = {2023-01-19},
	author = {Moreau, Arthur and Gilles, Thomas and Piasco, Nathan and Tsishkou, Dzmitry and Stanciulescu, Bogdan and de La Fortelle, Arnaud},
	year = {2023},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {2892--2902},
}

@misc{mescheder_occupancy_2019,
	title = {Occupancy {Networks}: {Learning} {3D} {Reconstruction} in {Function} {Space}},
	shorttitle = {Occupancy {Networks}},
	url = {http://arxiv.org/abs/1812.03828},
	doi = {10.48550/arXiv.1812.03828},
	abstract = {With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
	month = apr,
	year = {2019},
	note = {arXiv:1812.03828 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{mehta_level_2022,
	title = {A {Level} {Set} {Theory} for {Neural} {Implicit} {Evolution} under {Explicit} {Flows}},
	url = {http://arxiv.org/abs/2204.07159},
	doi = {10.48550/arXiv.2204.07159},
	abstract = {Coordinate-based neural networks parameterizing implicit surfaces have emerged as efficient representations of geometry. They effectively act as parametric level sets with the zero-level set defining the surface of interest. We present a framework that allows applying deformation operations defined for triangle meshes onto such implicit surfaces. Several of these operations can be viewed as energy-minimization problems that induce an instantaneous flow field on the explicit surface. Our method uses the flow field to deform parametric implicit surfaces by extending the classical theory of level sets. We also derive a consolidated view for existing methods on differentiable surface extraction and rendering, by formalizing connections to the level-set theory. We show that these methods drift from the theory and that our approach exhibits improvements for applications like surface smoothing, mean-curvature flow, inverse rendering and user-defined editing on implicit geometry.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Mehta, Ishit and Chandraker, Manmohan and Ramamoorthi, Ravi},
	month = jul,
	year = {2022},
	note = {arXiv:2204.07159 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, ⛔ No INSPIRE recid found},
}

@misc{liu_neural_2021,
	title = {Neural {Sparse} {Voxel} {Fields}},
	url = {http://arxiv.org/abs/2007.11571},
	doi = {10.48550/arXiv.2007.11571},
	abstract = {Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Liu, Lingjie and Gu, Jiatao and Lin, Kyaw Zaw and Chua, Tat-Seng and Theobalt, Christian},
	month = jan,
	year = {2021},
	note = {arXiv:2007.11571 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, ⛔ No INSPIRE recid found},
}

@inproceedings{lindell_bacon_2022,
	title = {{BACON}: {Band}-{Limited} {Coordinate} {Networks} for {Multiscale} {Scene} {Representation}},
	shorttitle = {{BACON}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Lindell_BACON_Band-Limited_Coordinate_Networks_for_Multiscale_Scene_Representation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-12},
	author = {Lindell, David B. and Van Veen, Dave and Park, Jeong Joon and Wetzstein, Gordon},
	year = {2022},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {16252--16262},
}

@misc{lin_parallel_2022,
	title = {Parallel {Inversion} of {Neural} {Radiance} {Fields} for {Robust} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2210.10108},
	doi = {10.48550/arXiv.2210.10108},
	abstract = {We present a parallelized optimization method based on fast Neural Radiance Fields (NeRF) for estimating 6-DoF target poses. Given a single observed RGB image of the target, we can predict the translation and rotation of the camera by minimizing the residual between pixels rendered from a fast NeRF model and pixels in the observed image. We integrate a momentum-based camera extrinsic optimization procedure into Instant Neural Graphics Primitives, a recent exceptionally fast NeRF implementation. By introducing parallel Monte Carlo sampling into the pose estimation task, our method overcomes local minima and improves efficiency in a more extensive search space. We also show the importance of adopting a more robust pixel-based loss function to reduce error. Experiments demonstrate that our method can achieve improved generalization and robustness on both synthetic and real-world benchmarks.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Lin, Yunzhi and Müller, Thomas and Tremblay, Jonathan and Wen, Bowen and Tyree, Stephen and Evans, Alex and Vela, Patricio A. and Birchfield, Stan},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10108 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, ⛔ No INSPIRE recid found},
}

@misc{li_dehazing-nerf_2023,
	title = {Dehazing-{NeRF}: {Neural} {Radiance} {Fields} from {Hazy} {Images}},
	shorttitle = {Dehazing-{NeRF}},
	url = {http://arxiv.org/abs/2304.11448},
	doi = {10.48550/arXiv.2304.11448},
	abstract = {Neural Radiance Field (NeRF) has received much attention in recent years due to the impressively high quality in 3D scene reconstruction and novel view synthesis. However, image degradation caused by the scattering of atmospheric light and object light by particles in the atmosphere can significantly decrease the reconstruction quality when shooting scenes in hazy conditions. To address this issue, we propose Dehazing-NeRF, a method that can recover clear NeRF from hazy image inputs. Our method simulates the physical imaging process of hazy images using an atmospheric scattering model, and jointly learns the atmospheric scattering model and a clean NeRF model for both image dehazing and novel view synthesis. Different from previous approaches, Dehazing-NeRF is an unsupervised method with only hazy images as the input, and also does not rely on hand-designed dehazing priors. By jointly combining the depth estimated from the NeRF 3D scene with the atmospheric scattering model, our proposed model breaks through the ill-posed problem of single-image dehazing while maintaining geometric consistency. Besides, to alleviate the degradation of image quality caused by information loss, soft margin consistency regularization, as well as atmospheric consistency and contrast discriminative loss, are addressed during the model training process. Extensive experiments demonstrate that our method outperforms the simple combination of single-image dehazing and NeRF on both image dehazing and novel view image synthesis.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Li, Tian and Li, L. U. and Wang, Wei and Feng, Zhangchi},
	month = apr,
	year = {2023},
	note = {arXiv:2304.11448 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{li_nerfacc_2022,
	title = {{NerfAcc}: {A} {General} {NeRF} {Acceleration} {Toolbox}},
	shorttitle = {{NerfAcc}},
	url = {http://arxiv.org/abs/2210.04847},
	doi = {10.48550/arXiv.2210.04847},
	abstract = {We propose NerfAcc, a toolbox for efficient volumetric rendering of radiance fields. We build on the techniques proposed in Instant-NGP, and extend these techniques to not only support bounded static scenes, but also for dynamic scenes and unbounded scenes. NerfAcc comes with a user-friendly Python API, and is ready for plug-and-play acceleration of most NeRFs. Various examples are provided to show how to use this toolbox. Code can be found here: https://github.com/KAIR-BAIR/nerfacc.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Li, Ruilong and Tancik, Matthew and Kanazawa, Angjoo},
	month = oct,
	year = {2022},
	note = {arXiv:2210.04847 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, ⛔ No INSPIRE recid found},
}

@inproceedings{lin_barf_2021,
	title = {{BARF}: {Bundle}-{Adjusting} {Neural} {Radiance} {Fields}},
	shorttitle = {{BARF}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lin_BARF_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-10-13},
	author = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, Antonio and Lucey, Simon},
	year = {2021},
	keywords = {Done Reading, Emphasis, ⛔ No INSPIRE recid found},
	pages = {5741--5751},
}

@misc{li_nerfacc_2023,
	title = {{NerfAcc}: {Efficient} {Sampling} {Accelerates} {NeRFs}},
	shorttitle = {{NerfAcc}},
	url = {http://arxiv.org/abs/2305.04966},
	doi = {10.48550/arXiv.2305.04966},
	abstract = {Optimizing and rendering Neural Radiance Fields is computationally expensive due to the vast number of samples required by volume rendering. Recent works have included alternative sampling approaches to help accelerate their methods, however, they are often not the focus of the work. In this paper, we investigate and compare multiple sampling approaches and demonstrate that improved sampling is generally applicable across NeRF variants under an unified concept of transmittance estimator. To facilitate future experiments, we develop NerfAcc, a Python toolbox that provides flexible APIs for incorporating advanced sampling methods into NeRF related methods. We demonstrate its flexibility by showing that it can reduce the training time of several recent NeRF methods by 1.5x to 20x with minimal modifications to the existing codebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be implemented in native PyTorch using NerfAcc.},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Li, Ruilong and Gao, Hang and Tancik, Matthew and Kanazawa, Angjoo},
	month = may,
	year = {2023},
	note = {arXiv:2305.04966 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{lazova_control-nerf_2022,
	title = {Control-{NeRF}: {Editable} {Feature} {Volumes} for {Scene} {Rendering} and {Manipulation}},
	shorttitle = {Control-{NeRF}},
	url = {http://arxiv.org/abs/2204.10850},
	doi = {10.48550/arXiv.2204.10850},
	abstract = {We present a novel method for performing flexible, 3D-aware image content manipulation while enabling high-quality novel view synthesis. While NeRF-based approaches are effective for novel view synthesis, such models memorize the radiance for every point in a scene within a neural network. Since these models are scene-specific and lack a 3D scene representation, classical editing such as shape manipulation, or combining scenes is not possible. Hence, editing and combining NeRF-based scenes has not been demonstrated. With the aim of obtaining interpretable and controllable scene representations, our model couples learnt scene-specific feature volumes with a scene agnostic neural rendering network. With this hybrid representation, we decouple neural rendering from scene-specific geometry and appearance. We can generalize to novel scenes by optimizing only the scene-specific 3D feature representation, while keeping the parameters of the rendering network fixed. The rendering function learnt during the initial training stage can thus be easily applied to new scenes, making our approach more flexible. More importantly, since the feature volumes are independent of the rendering model, we can manipulate and combine scenes by editing their corresponding feature volumes. The edited volume can then be plugged into the rendering model to synthesize high-quality novel views. We demonstrate various scene manipulations, including mixing scenes, deforming objects and inserting objects into scenes, while still producing photo-realistic results.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Lazova, Verica and Guzov, Vladimir and Olszewski, Kyle and Tulyakov, Sergey and Pons-Moll, Gerard},
	month = apr,
	year = {2022},
	note = {arXiv:2204.10850 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@inproceedings{jiang_sdfdiff_2020,
	title = {{SDFDiff}: {Differentiable} {Rendering} of {Signed} {Distance} {Fields} for {3D} {Shape} {Optimization}},
	shorttitle = {{SDFDiff}},
	url = {https://yuejiang-nj.github.io/papers/CVPR2020_SDFDiff/project_page.html},
	doi = {10/ghbb9k},
	abstract = {We propose SDFDiff, a novel approach for image-based shape optimization using differentiable rendering of 3D shapes represented by signed distance functions (SDFs). Compared to other representations, SDFs have the advantage that they can represent shapes with arbitrary topology, and that they guarantee watertight surfaces. We apply our approach to the problem of multi-view 3D reconstruction, where we achieve high reconstruction quality and can capture complex topology of 3D objects. In addition, we employ a multi-resolution strategy to obtain a robust optimization algorithm. We further demonstrate that our SDF-based differentiable renderer can be integrated with deep learning models, which opens up options for learning approaches on 3D objects without 3D supervision. In particular, we apply our method to single-view 3D reconstruction and achieve state-of-the-art results.},
	urldate = {2022-12-09},
	author = {Jiang, Yue and Ji, Dantong and Han, Zhizhong and Zwicker, Matthias},
	year = {2020},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@misc{jiang_neuman_2022,
	title = {{NeuMan}: {Neural} {Human} {Radiance} {Field} from a {Single} {Video}},
	shorttitle = {{NeuMan}},
	url = {http://arxiv.org/abs/2203.12575},
	doi = {10.48550/arXiv.2203.12575},
	abstract = {Photorealistic rendering and reposing of humans is important for enabling augmented reality experiences. We propose a novel framework to reconstruct the human and the scene that can be rendered with novel human poses and views from just a single in-the-wild video. Given a video captured by a moving camera, we train two NeRF models: a human NeRF model and a scene NeRF model. To train these models, we rely on existing methods to estimate the rough geometry of the human and the scene. Those rough geometry estimates allow us to create a warping field from the observation space to the canonical pose-independent space, where we train the human model in. Our method is able to learn subject specific details, including cloth wrinkles and accessories, from just a 10 seconds video clip, and to provide high quality renderings of the human under novel poses, from novel views, together with the background.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag},
	month = sep,
	year = {2022},
	note = {arXiv:2203.12575 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@inproceedings{jeong_perfception_2022,
	title = {{PeRFception}: {Perception} using {Radiance} {Fields}},
	shorttitle = {{PeRFception}},
	url = {https://openreview.net/forum?id=MzaPEKHv-0J},
	abstract = {The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of NeRF called Plenoxels, we create the first large-scale radiance fields datasets for perception tasks, called the PeRFception, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4{\textbackslash}\%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the classification and segmentation models that directly take this radiance fields format as input and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data are publicly available in "https://postech-cvlab.github.io/PeRFception/".},
	language = {en},
	urldate = {2023-03-13},
	author = {Jeong, Yoonwoo and Shin, Seungjoo and Lee, Junha and Choy, Chris and Anandkumar, Anima and Cho, Minsu and Park, Jaesik},
	month = oct,
	year = {2022},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@misc{huang_ponder_2022,
	title = {Ponder: {Point} {Cloud} {Pre}-training via {Neural} {Rendering}},
	shorttitle = {Ponder},
	url = {http://arxiv.org/abs/2301.00157},
	doi = {10.48550/arXiv.2301.00157},
	abstract = {We propose a novel approach to self-supervised learning of point cloud representations by differentiable neural rendering. Motivated by the fact that informative point cloud features should be able to encode rich geometry and appearance cues and render realistic images, we train a point-cloud encoder within a devised point-based neural renderer by comparing the rendered images with real images on massive RGB-D data. The learned point-cloud encoder can be easily integrated into various downstream tasks, including not only high-level tasks like 3D detection and segmentation, but low-level tasks like 3D reconstruction and image synthesis. Extensive experiments on various tasks demonstrate the superiority of our approach compared to existing pre-training methods.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Huang, Di and Peng, Sida and He, Tong and Zhou, Xiaowei and Ouyang, Wanli},
	month = dec,
	year = {2022},
	note = {arXiv:2301.00157 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{houchens_neuralodf_2022,
	title = {{NeuralODF}: {Learning} {Omnidirectional} {Distance} {Fields} for {3D} {Shape} {Representation}},
	shorttitle = {{NeuralODF}},
	url = {http://arxiv.org/abs/2206.05837},
	doi = {10.48550/arXiv.2206.05837},
	abstract = {In visual computing, 3D geometry is represented in many different forms including meshes, point clouds, voxel grids, level sets, and depth images. Each representation is suited for different tasks thus making the transformation of one representation into another (forward map) an important and common problem. We propose Omnidirectional Distance Fields (ODFs), a new 3D shape representation that encodes geometry by storing the depth to the object's surface from any 3D position in any viewing direction. Since rays are the fundamental unit of an ODF, it can be used to easily transform to and from common 3D representations like meshes or point clouds. Different from level set methods that are limited to representing closed surfaces, ODFs are unsigned and can thus model open surfaces (e.g., garments). We demonstrate that ODFs can be effectively learned with a neural network (NeuralODF) despite the inherent discontinuities at occlusion boundaries. We also introduce efficient forward mapping algorithms for transforming ODFs to and from common 3D representations. Specifically, we introduce an efficient Jumping Cubes algorithm for generating meshes from ODFs. Experiments demonstrate that NeuralODF can learn to capture high-quality shape by overfitting to a single object, and also learn to generalize on common shape categories.},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Houchens, Trevor and Lu, Cheng-You and Duggal, Shivam and Fu, Rao and Sridhar, Srinath},
	month = aug,
	year = {2022},
	note = {arXiv:2206.05837 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Distance Fields, Read Later, ⛔ No INSPIRE recid found},
}

@misc{garbin_fastnerf_2021,
	title = {{FastNeRF}: {High}-{Fidelity} {Neural} {Rendering} at {200FPS}},
	shorttitle = {{FastNeRF}},
	url = {http://arxiv.org/abs/2103.10380},
	doi = {10.48550/arXiv.2103.10380},
	abstract = {Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Garbin, Stephan J. and Kowalski, Marek and Johnson, Matthew and Shotton, Jamie and Valentin, Julien},
	month = apr,
	year = {2021},
	note = {arXiv:2103.10380 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@inproceedings{gropp_implicit_2020,
	title = {Implicit {Geometric} {Regularization} for {Learning} {Shapes}},
	shorttitle = {{IGR}},
	url = {http://arxiv.org/abs/2002.10099},
	doi = {10.5555/3524938.3525293},
	abstract = {Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.},
	urldate = {2022-12-04},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Gropp, Amos and Yariv, Lior and Haim, Niv and Atzmon, Matan and Lipman, Yaron},
	month = jul,
	year = {2020},
	note = {titleTranslation:},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Statistics - Machine Learning, ⚠️ Invalid DOI, ⛔ No INSPIRE recid found},
}

@inproceedings{fridovich-keil_plenoxels_2022,
	title = {Plenoxels: {Radiance} {Fields} without {Neural} {Networks}},
	shorttitle = {Plenoxels},
	doi = {10/gq34wg},
	abstract = {We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Fridovich-Keil, Sara and Yu, Alex and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, Benchmark testing, Codes, Computer vision, Done Reading, Efficient learning and inferences, Gradient methods, Neural networks, Optimization methods, Three-dimensional displays, Vision + graphics, Visualization, ⛔ No INSPIRE recid found},
	pages = {5491--5500},
}

@misc{fan_nerf-sos_2022,
	title = {{NeRF}-{SOS}: {Any}-{View} {Self}-supervised {Object} {Segmentation} on {Complex} {Scenes}},
	shorttitle = {{NeRF}-{SOS}},
	url = {http://arxiv.org/abs/2209.08776},
	doi = {10.48550/arXiv.2209.08776},
	abstract = {Neural volumetric representations have shown the potential that Multi-layer Perceptrons (MLPs) can be optimized with multi-view calibrated images to represent scene geometry and appearance, without explicit 3D supervision. Object segmentation can enrich many downstream applications based on the learned radiance field. However, introducing hand-crafted segmentation to define regions of interest in a complex real-world scene is non-trivial and expensive as it acquires per view annotation. This paper carries out the exploration of self-supervised learning for object segmentation using NeRF for complex real-world scenes. Our framework, called NeRF with Self-supervised Object Segmentation NeRF-SOS, couples object segmentation and neural radiance field to segment objects in any view within a scene. By proposing a novel collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS encourages NeRF models to distill compact geometry-aware segmentation clusters from their density fields and the self-supervised pre-trained 2D visual features. The self-supervised object segmentation framework can be applied to various NeRF models that both lead to photo-realistic rendering results and convincing segmentation maps for both indoor and outdoor scenarios. Extensive results on the LLFF, Tank \& Temple, and BlendedMVS datasets validate the effectiveness of NeRF-SOS. It consistently surpasses other 2D-based self-supervised baselines and predicts finer semantics masks than existing supervised counterparts. Please refer to the video on our project page for more details:https://zhiwenfan.github.io/NeRF-SOS.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Fan, Zhiwen and Wang, Peihao and Jiang, Yifan and Gong, Xinyu and Xu, Dejia and Wang, Zhangyang},
	month = sep,
	year = {2022},
	note = {arXiv:2209.08776 [cs]
version: 1},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{deng_nerf-loam_2023,
	title = {{NeRF}-{LOAM}: {Neural} {Implicit} {Representation} for {Large}-{Scale} {Incremental} {LiDAR} {Odometry} and {Mapping}},
	shorttitle = {{NeRF}-{LOAM}},
	url = {http://arxiv.org/abs/2303.10709},
	doi = {10.48550/arXiv.2303.10709},
	abstract = {Simultaneously odometry and mapping using LiDAR data is an important task for mobile systems to achieve full autonomy in large-scale environments. However, most existing LiDAR-based methods prioritize tracking quality over reconstruction quality. Although the recently developed neural radiance fields (NeRF) have shown promising advances in implicit reconstruction for indoor environments, the problem of simultaneous odometry and mapping for large-scale scenarios using incremental LiDAR data remains unexplored. To bridge this gap, in this paper, we propose a novel NeRF-based LiDAR odometry and mapping approach, NeRF-LOAM, consisting of three modules neural odometry, neural mapping, and mesh reconstruction. All these modules utilize our proposed neural signed distance function, which separates LiDAR points into ground and non-ground points to reduce Z-axis drift, optimizes odometry and voxel embeddings concurrently, and in the end generates dense smooth mesh maps of the environment. Moreover, this joint optimization allows our NeRF-LOAM to be pre-trained free and exhibit strong generalization abilities when applied to different environments. Extensive evaluations on three publicly available datasets demonstrate that our approach achieves state-of-the-art odometry and mapping performance, as well as a strong generalization in large-scale environments utilizing LiDAR data. Furthermore, we perform multiple ablation studies to validate the effectiveness of our network design. The implementation of our approach will be made available at https://github.com/JunyuanDeng/NeRF-LOAM.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Deng, Junyuan and Chen, Xieyuanli and Xia, Songpengcheng and Sun, Zhen and Liu, Guoqing and Yu, Wenxian and Pei, Ling},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10709 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{de_luigi_deep_2023,
	title = {Deep {Learning} on {Implicit} {Neural} {Representations} of {Shapes}},
	url = {http://arxiv.org/abs/2302.05438},
	doi = {10.48550/arXiv.2302.05438},
	abstract = {Implicit Neural Representations (INRs) have emerged in the last few years as a powerful tool to encode continuously a variety of different signals like images, videos, audio and 3D shapes. When applied to 3D shapes, INRs allow to overcome the fragmentation and shortcomings of the popular discrete representations used so far. Yet, considering that INRs consist in neural networks, it is not clear whether and how it may be possible to feed them into deep learning pipelines aimed at solving a downstream task. In this paper, we put forward this research problem and propose inr2vec, a framework that can compute a compact latent representation for an input INR in a single inference pass. We verify that inr2vec can embed effectively the 3D shapes represented by the input INRs and show how the produced embeddings can be fed into deep learning pipelines to solve several tasks by processing exclusively INRs.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {De Luigi, Luca and Cardace, Adriano and Spezialetti, Riccardo and Ramirez, Pierluigi Zama and Salti, Samuele and Di Stefano, Luigi},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05438 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
}

@misc{chen_factor_2023,
	title = {Factor {Fields}: {A} {Unified} {Framework} for {Neural} {Fields} and {Beyond}},
	shorttitle = {Factor {Fields}},
	url = {http://arxiv.org/abs/2302.01226},
	doi = {10.48550/arXiv.2302.01226},
	abstract = {We present Factor Fields, a novel framework for modeling and representing signals. Factor Fields decomposes a signal into a product of factors, each of which is represented by a neural or regular field representation operating on a coordinate transformed input signal. We show that this decomposition yields a unified framework that generalizes several recent signal representations including NeRF, PlenOxels, EG3D, Instant-NGP, and TensoRF. Moreover, the framework allows for the creation of powerful new signal representations, such as the Coefficient-Basis Factorization (CoBaFa) which we propose in this paper. As evidenced by our experiments, CoBaFa leads to improvements over previous fast reconstruction methods in terms of the three critical goals in neural signal representation: approximation quality, compactness and efficiency. Experimentally, we demonstrate that our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when reconstructing 3D signed distance fields and higher compactness for radiance field reconstruction tasks compared to previous fast reconstruction methods. Besides, our CoBaFa representation enables generalization by sharing the basis across signals during training, enabling generalization tasks such as image regression with sparse observations and few-shot radiance field reconstruction.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Chen, Anpei and Xu, Zexiang and Wei, Xinyue and Tang, Siyu and Su, Hao and Geiger, Andreas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01226 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, ⛔ No INSPIRE recid found},
}

@article{dadon_ddnerf_2022,
	title = {{DDNeRF}: {Depth} {Distribution} {Neural} {Radiance} {Fields}},
	shorttitle = {{DDNeRF}},
	url = {https://arxiv.org/abs/2203.16626},
	doi = {10/gr8wf2},
	abstract = {In recent years, the field of implicit neural representation has progressed significantly. Models such as neural radiance fields (NeRF), which uses relatively small neural networks, can represent high-quality scenes and achieve state-of-the-art results for novel view synthesis. Training these types of networks, however, is still computationally very expensive. We present depth distribution neural radiance field (DDNeRF), a new method that significantly increases sampling efficiency along rays during training while achieving superior results for a given sampling budget. DDNeRF achieves this by learning a more accurate representation of the density distribution along rays. More specifically, we train a coarse model to predict the internal distribution of the transparency of an input volume in addition to the volume's total density. This finer distribution then guides the sampling procedure of the fine model. This method allows us to use fewer samples during training while reducing computational resources.},
	language = {en},
	urldate = {2023-01-19},
	author = {Dadon, David and Fried, Ohad and Hel-Or, Yacov},
	year = {2022},
	keywords = {/unread, ⛔ No INSPIRE recid found},
}

@inproceedings{chan_efficient_2022,
	title = {Efficient {Geometry}-{Aware} {3D} {Generative} {Adversarial} {Networks}},
	shorttitle = {{EG3D}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-13},
	author = {Chan, Eric R. and Lin, Connor Z. and Chan, Matthew A. and Nagano, Koki and Pan, Boxiao and De Mello, Shalini and Gallo, Orazio and Guibas, Leonidas J. and Tremblay, Jonathan and Khamis, Sameh and Karras, Tero and Wetzstein, Gordon},
	year = {2022},
	keywords = {/unread, ⛔ No INSPIRE recid found},
	pages = {16123--16133},
}

@misc{carlson_cloner_2022,
	title = {{CLONeR}: {Camera}-{Lidar} {Fusion} for {Occupancy} {Grid}-aided {Neural} {Representations}},
	shorttitle = {{CLONeR}},
	url = {http://arxiv.org/abs/2209.01194},
	doi = {10.48550/arXiv.2209.01194},
	abstract = {Recent advances in neural radiance fields (NeRFs) achieve state-of-the-art novel view synthesis and facilitate dense estimation of scene properties. However, NeRFs often fail for large, unbounded scenes that are captured under very sparse views with the scene content concentrated far away from the camera, as is typical for field robotics applications. In particular, NeRF-style algorithms perform poorly: (1) when there are insufficient views with little pose diversity, (2) when scenes contain saturation and shadows, and (3) when finely sampling large unbounded scenes with fine structures becomes computationally intensive. This paper proposes CLONeR, which significantly improves upon NeRF by allowing it to model large outdoor driving scenes that are observed from sparse input sensor views. This is achieved by decoupling occupancy and color learning within the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trained using LiDAR and camera data, respectively. In addition, this paper proposes a novel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside the NeRF model, and leverage this occupancy grid for improved sampling of points along a ray for volumetric rendering in metric space. Through extensive quantitative and qualitative experiments on scenes from the KITTI dataset, this paper demonstrates that the proposed method outperforms state-of-the-art NeRF models on both novel view synthesis and dense depth prediction tasks when trained on sparse input data.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Carlson, Alexandra and Ramanagopal, Manikandasriram Srinivasan and Tseng, Nathan and Johnson-Roberson, Matthew and Vasudevan, Ram and Skinner, Katherine A.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01194 [cs]
version: 1},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Done Reading, ⛔ No INSPIRE recid found},
}

@inproceedings{azinovic_neural_2022,
	title = {Neural {RGB}-{D} {Surface} {Reconstruction}},
	doi = {10/gq6vvz},
	abstract = {Obtaining high-quality 3D reconstructions of room-scale scenes is of paramount importance for upcoming applications in AR or VR. These range from mixed reality applications for teleconferencing, virtual measuring, virtual room planing, to robotic applications. While current volume-based view synthesis methods that use neural radiance fields (NeRFs) show promising results in reproducing the appearance of an object or scene, they do not reconstruct an actual surface. The volumetric representation of the surface based on densities leads to artifacts when a surface is extracted using Marching Cubes, since during optimization, densities are accumulated along the ray and are not used at a single sample point in isolation. Instead of this volumetric representation of the surface, we propose to represent the surface using an implicit function (truncated signed distance function). We show how to incorporate this representation in the NeRF framework, and extend it to use depth measurements from a commodity RGB-D sensor, such as a Kinect. In addition, we propose a pose and camera re-finement technique which improves the overall reconstruction quality. In contrast to concurrent work on integrating depth priors in NeRF which concentrates on novel view synthesis, our approach is able to reconstruct high-quality, metrical 3D reconstructions.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Azinović, Dejan and Martin-Brualla, Ricardo and Goldman, Dan B and Nießner, Matthias and Thies, Justus},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, 3D from multi-view and sensors, 3D from single images, Computer vision, Current measurement, Done Reading, Mixed reality, Planing, RGBD sensors and analytics, Surface reconstruction, Teleconferencing, Three-dimensional displays, Vision + graphics, ⛔ No INSPIRE recid found},
	pages = {6280--6291},
}

@inproceedings{vedaldi_hardgan_2020,
	title = {{HardGAN}: {A} {Haze}-{Aware} {Representation} {Distillation} {GAN} for {Single} {Image} {Dehazing}},
	volume = {12351},
	shorttitle = {{HardGAN}},
	url = {https://link.springer.com/10.1007/978-3-030-58539-6_43},
	doi = {10/ght882},
	abstract = {In this paper, we present a Haze-Aware Representation Distillation Generative Adversarial Network (HardGAN) for single-image dehazing. Unlike previous studies that intend to model the transmission map and global atmospheric light jointly to restore a clear...},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer, Cham},
	author = {Deng, Qili and Huang, Ziling and Tsai, Chung-Chi and Lin, Chia-Wen},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {CCF: B},
	keywords = {/unread},
	pages = {722--738},
}

@inproceedings{liu_griddehazenet_2019,
	title = {{GridDehazeNet}: {Attention}-{Based} {Multi}-{Scale} {Network} for {Image} {Dehazing}},
	shorttitle = {{GridDehazeNet}},
	url = {https://ieeexplore.ieee.org/document/9010659/},
	doi = {10/ghfhqq},
	abstract = {We propose an end-to-end trainable Convolutional Neural Network (CNN), named GridDehazeNet, for single image dehazing. The GridDehazeNet consists of three modules: pre-processing, backbone, and post-processing. The trainable pre-processing module can generate learned inputs with better diversity and more pertinent features as compared to those derived inputs produced by hand-selected pre-processing methods. The backbone module implements a novel attention-based multi-scale estimation on a grid network, which can effectively alleviate the bottleneck issue often encountered in the conventional multi-scale approach. The post-processing module helps to reduce the artifacts in the final output. Experimental results indicate that the GridDehazeNet outperforms the state-of-the-arts on both synthetic and real-world images. The proposed hazing method does not rely on the atmosphere scattering model, and we provide an explanation as to why it is not necessarily beneficial to take advantage of the dimension reduction offered by the atmosphere scattering model for image dehazing, even if only the dehazing results on synthetic images are concerned.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Xiaohong and Ma, Yongrui and Shi, Zhihao and Chen, Jun},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {/unread, Atmospheric modeling, Data models, Estimation, Image color analysis, Image restoration, Scattering},
	pages = {7313--7322},
}

@inproceedings{ren_gated_2018,
	title = {Gated {Fusion} {Network} for {Single} {Image} {Dehazing}},
	url = {https://ieeexplore.ieee.org/document/8578441/},
	doi = {10/gfxhxs},
	abstract = {In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale approach such that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ren, Wenqi and Ma, Lin and Zhang, Jiawei and Pan, Jinshan and Cao, Xiaochun and Liu, Wei and Yang, Ming-Hsuan},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Atmospheric modeling, Attenuation, Decoding, Estimation, Image color analysis, Image restoration, Logic gates},
	pages = {3253--3261},
}

@article{qin_ffa-net_2020,
	title = {{FFA}-{Net}: {Feature} {Fusion} {Attention} {Network} for {Single} {Image} {Dehazing}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{FFA}-{Net}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6865},
	doi = {10/ghczxr},
	abstract = {In this paper, we propose an end-to-end feature fusion at-tention network (FFA-Net) to directly restore the haze-free image. The FFA-Net architecture consists of three key components:1) A novel Feature Attention (FA) module combines Channel Attention with Pixel Attention mechanism, considering that different channel-wise features contain totally different weighted information and haze distribution is uneven on the different image pixels. FA treats different features and pixels unequally, which provides additional flexibility in dealing with different types of information, expanding the representational ability of CNNs. 2) A basic block structure consists of Local Residual Learning and Feature Attention, Local Residual Learning allowing the less important information such as thin haze region or low-frequency to be bypassed through multiple local residual connections, let main network architecture focus on more effective information. 3) An Attention-based different levels Feature Fusion (FFA) structure, the feature weights are adaptively learned from the Feature Attention (FA) module, giving more weight to important features. This structure can also retain the information of shallow layers and pass it into deep layers.The experimental results demonstrate that our proposed FFA-Net surpasses previous state-of-the-art single image dehazing methods by a very large margin both quantitatively and qualitatively, boosting the best published PSNR metric from 30.23 dB to 36.39 dB on the SOTS indoor test dataset. Code has been made available at GitHub.},
	language = {en},
	number = {07},
	urldate = {2023-03-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Qin, Xu and Wang, Zhilin and Bai, Yuanchao and Xie, Xiaodong and Jia, Huizhu},
	month = apr,
	year = {2020},
	note = {Number: 07},
	keywords = {/unread},
	pages = {11908--11915},
}

@inproceedings{qu_enhanced_2019,
	title = {Enhanced {Pix2pix} {Dehazing} {Network}},
	url = {https://ieeexplore.ieee.org/document/8953692/},
	doi = {10/gr8wgb},
	abstract = {In this paper, we reduce the image dehazing problem to an image-to-image translation problem, and propose Enhanced Pix2pix Dehazing Network (EPDN), which generates a haze-free image without relying on the physical scattering model. EPDN is embedded by a generative adversarial network, which is followed by a well-designed enhancer. Inspired by visual perception global-first theory, the discriminator guides the generator to create a pseudo realistic image on a coarse scale, while the enhancer following the generator is required to produce a realistic dehazing image on the fine scale. The enhancer contains two enhancing blocks based on the receptive field model, which reinforces the dehazing effect in both color and details. The embedded GAN is jointly trained with the enhancer. Extensive experiment results on synthetic datasets and real-world datasets show that the proposed EPDN is superior to the state-of-the-art methods in terms of PSNR, SSIM, PI, and subjective visual effect.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Qu, Yanyun and Chen, Yizi and Huang, Jingying and Xie, Yuan},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Low-level Vision},
	pages = {8152--8160},
}

@article{li_end--end_2018,
	title = {End-to-{End} {United} {Video} {Dehazing} and {Detection}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12287},
	doi = {10/gr8wf9},
	abstract = {The recent development of CNN-based image dehazing has revealed the effectiveness of end-to-end modeling. However, extending the idea to end-to-end video dehazing has not been explored yet. In this paper, we propose an End-to-End Video Dehazing Network (EVD-Net), to exploit the temporal consistency between consecutive video frames. A thorough study has been conducted over a number of structure options, to identify the best temporal fusion strategy. Furthermore, we build an End-to-End United Video Dehazing and Detection Network (EVDD-Net), which concatenates and jointly trains EVD-Net with a video object detection model. The resulting augmented end-to-end pipeline has demonstrated much more stable and accurate detection results in hazy video.},
	language = {en},
	number = {1},
	urldate = {2023-02-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Boyi and Peng, Xiulian and Wang, Zhangyang and Xu, Jizheng and Feng, Dan},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {/unread, Machine Learning Applications},
}

@inproceedings{avidan_directed_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Directed {Ray} {Distance} {Functions} for {3D} {Scene} {Reconstruction}},
	volume = {13662},
	isbn = {978-3-031-20086-1},
	url = {https://link.springer.com/10.1007/978-3-031-20086-1_12},
	doi = {10/gr8wf5},
	abstract = {We present an approach for full 3D scene reconstruction from a single unseen image. We trained on dataset of realistic non-watertight scans of scenes. Our approach uses a predicted distance function, since these have shown promise in handling complex topologies and large spaces. We identify and analyze two key challenges for predicting such image conditioned distance functions that have prevented their success on real 3D scene data. First, we show that predicting a conventional scene distance from an image requires reasoning over a large receptive field. Second, we analytically show that the optimal output of the network trained to predict these distance functions does not obey all the distance function properties. We propose an alternate distance function, the Directed Ray Distance Function (DRDF), that tackles both challenges. We show that a deep network trained to predict DRDFs outperforms all other methods quantitatively and qualitatively on 3D reconstruction from single image on Matterport3D, 3DFront, and ScanNet. (Project Page: https://nileshkulkarni.github.io/scene\_drdf)},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Kulkarni, Nilesh and Johnson, Justin and Fouhey, David F.},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {/unread, Distance functions, Single image 3D},
	pages = {201--219},
}

@inproceedings{zhang_densely_2018,
	title = {Densely {Connected} {Pyramid} {Dehazing} {Network}},
	url = {https://ieeexplore.ieee.org/document/8578435/},
	doi = {10/gftv9j},
	abstract = {We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the corresponding dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods. Code and dataset is made available at: https://github.com/hezhangsprinter/DCPDN.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, He and Patel, Vishal M.},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Atmospheric modeling, Degradation, Estimation, Gallium nitride, Generative adversarial networks, Image edge detection, Optimization},
	pages = {3194--3203},
}

@article{ye_decoupling_2023,
	title = {Decoupling {Human} and {Camera} {Motion} from {Videos} in the {Wild}},
	url = {https://arxiv.org/abs/2302.12827},
	doi = {10/gr8wf3},
	abstract = {We propose a method to reconstruct global human trajectories from videos in the wild. Our optimization method decouples the camera and human motion, which allows us to place people in the same world coordinate frame. Most existing methods do not model the camera motion; methods that rely on the background pixels to infer 3D human motion usually require a full scene reconstruction, which is often not possible for in-the-wild videos. However, even when existing SLAM systems cannot recover accurate scene reconstructions, the background pixel motion still provides enough signal to constrain the camera motion. We show that relative camera estimates along with data-driven human motion priors can resolve the scene scale ambiguity and recover global human trajectories. Our method robustly recovers the global 3D trajectories of people in challenging in-the-wild videos, such as PoseTrack. We quantify our improvement over existing methods on 3D human dataset Egobody. We further demonstrate that our recovered camera scale allows us to reason about motion of multiple people in a shared coordinate frame, which improves performance of downstream tracking in PoseTrack. Code and video results can be found at https://vye16.github.io/slahmr.},
	language = {en},
	urldate = {2023-02-27},
	author = {Ye, Vickie and Pavlakos, Georgios and Malik, Jitendra and Kanazawa, Angjoo},
	year = {2023},
	keywords = {/unread},
}

@inproceedings{dellenbach_ct-icp_2022,
	title = {{CT}-{ICP}: {Real}-time {Elastic} {LiDAR} {Odometry} with {Loop} {Closure}},
	shorttitle = {{CT}-{ICP}},
	url = {https://ieeexplore.ieee.org/document/9811849/},
	doi = {10/gr8wfw},
	abstract = {Multi-beam LiDAR sensors are increasingly used in robotics, particularly with autonomous cars for localization and perception tasks, both relying on the ability to build a precise map of the environment. For this, we propose a new real-time LiDAR-only odometry method called CT-ICP (for Continuous-Time ICP), completed into a full SLAM with a novel loop detection procedure. The core of this method, is the introduction of the combined continuity in the scan matching, and discontinuity between scans. It allows both the elastic distortion of the scan during the registration for increased precision, and the increased robustness to high frequency motions from the discontinuity. We build a complete SLAM on top of this odometry, using a fast pure LiDAR loop detection based on elevation image 2D matching, providing a pose graph with loop constraints. To show the robustness of the method, we tested it on seven datasets: KITTI, KITTI-raw, KITTI-360, KITTICARLA, ParisLuco, Newer College, and NCLT in driving and high-frequency motion scenarios. Both the CT-ICP odometry and the loop detection are made available online. CT-ICP is currently first, among those giving access to a public code, on the KITTI odometry leaderboard, with an average Relative Translation Error (RTE) of 0.59\% and an average time per scan of 60ms on a CPU with a single thread.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Dellenbach, Pierre and Deschaud, Jean-Emmanuel and Jacquet, Bastien and Goulette, Francois},
	month = may,
	year = {2022},
	keywords = {/unread, Codes, Laser radar, Location awareness, Real-time systems, Robustness, Sensors, Simultaneous localization and mapping},
	pages = {5580--5586},
}

@inproceedings{vedaldi_convolutional_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Convolutional {Occupancy} {Networks}},
	volume = {12348},
	isbn = {978-3-030-58580-8},
	url = {https://link.springer.com/10.1007/978-3-030-58580-8_31},
	doi = {10/gm75nj},
	abstract = {Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Peng, Songyou and Niemeyer, Michael and Mescheder, Lars and Pollefeys, Marc and Geiger, Andreas},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {/unread},
	pages = {523--540},
}

@inproceedings{wu_contrastive_2021,
	title = {Contrastive {Learning} for {Compact} {Single} {Image} {Dehazing}},
	url = {https://ieeexplore.ieee.org/document/9578448/},
	doi = {10/grkh22},
	abstract = {Single image dehazing is a challenging ill-posed problem due to the severe information degeneration. However, existing deep learning based dehazing methods only adopt clear images as positive samples to guide the training of dehazing network while negative information is unexploited. Moreover, most of them focus on strengthening the dehazing network with an increase of depth and width, leading to a significant requirement of computation and memory. In this paper, we propose a novel contrastive regularization (CR) built upon contrastive learning to exploit both the information of hazy images and clear images as negative and positive samples, respectively. CR ensures that the restored image is pulled to closer to the clear image and pushed to far away from the hazy image in the representation space.Furthermore, considering trade-off between performance and memory storage, we develop a compact dehazing network based on autoencoder-like (AE) framework. It involves an adaptive mixup operation and a dynamic feature enhancement module, which can benefit from preserving information flow adaptively and expanding the receptive field to improve the network’s transformation capability, respectively. We term our dehazing network with autoencoder and contrastive regularization as AECR-Net. The extensive experiments on synthetic and real-world datasets demonstrate that our AECR-Net surpass the state-of-the-art approaches. The code is released in https://github.com/GlassyWu/AECR-Net.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wu, Haiyan and Qu, Yanyun and Lin, Shaohui and Zhou, Jian and Qiao, Ruizhi and Zhang, Zhizhong and Xie, Yuan and Ma, Lizhuang},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Adaptive systems, Codes, Computer vision, Deep learning, Memory management, Performance gain, Training},
	pages = {10546--10555},
}

@inproceedings{narasimhan_structured_2005,
	title = {Structured light in scattering media},
	volume = {1},
	doi = {10/cnwv2k},
	abstract = {Virtually all structured light methods assume that the scene and the sources are immersed in pure air and that light is neither scattered nor absorbed. Recently, however, structured lighting has found growing application in underwater and aerial imaging, where scattering effects cannot be ignored. In this paper, we present a comprehensive analysis of two representative methods - light stripe range scanning and photometric stereo - in the presence of scattering. For both methods, we derive physical models for the appearances of a surface immersed in a scattering medium. Based on these models, we present results on (a) the condition for object detectability in light striping and (b) the number of sources required for photometric stereo. In both cases, we demonstrate that while traditional methods fail when scattering is significant, our methods accurately recover the scene (depths, normals, albedos) as well as the properties of the medium. These results are in turn used to restore the appearances of scenes as if they were captured in clear air. Although we have focused on light striping and photometric stereo, our approach can also be extended to other methods such as grid coding, gated and active polarization imaging.},
	booktitle = {Tenth {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV}'05) {Volume} 1},
	author = {Narasimhan, S.G. and Nayar, S.K. and Sun, Bo and Koppal, S.J.},
	month = oct,
	year = {2005},
	note = {ISSN: 2380-7504},
	keywords = {/unread, Computer science, Layout, Light scattering, Optical attenuators, Optical imaging, Optical scattering, Photometry, Robots, Sun, Surface reconstruction},
	pages = {420--427 Vol. 1},
}

@inproceedings{caraffa_stereo_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Stereo {Reconstruction} and {Contrast} {Restoration} in {Daytime} {Fog}},
	isbn = {978-3-642-37447-0},
	doi = {10/gr8whb},
	abstract = {Stereo reconstruction serves many outdoor applications, and thus sometimes faces foggy weather. The quality of the reconstruction by state of the art algorithms is then degraded as contrast is reduced with the distance because of scattering. However, as shown by defogging algorithms from a single image, fog provides an extra depth cue in the gray level of far away objects. Our idea is thus to take advantage of both stereo and atmospheric veil depth cues to achieve better stereo reconstructions in foggy weather. To our knowledge, this subject has never been investigated earlier by the computer vision community. We thus propose a Markov Random Field model of the stereo reconstruction and defogging problem which can be optimized iteratively using the α-expansion algorithm. Outputs are a dense disparity map and an image where contrast is restored. The proposed model is evaluated on synthetic images. This evaluation shows that the proposed method achieves very good results on both stereo reconstruction and defogging compared to standard stereo reconstruction and single image defogging.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2012},
	publisher = {Springer},
	author = {Caraffa, Laurent and Tarel, Jean-Philippe},
	editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
	year = {2013},
	keywords = {/unread, Data Stereo, Markov Random Field, Markov Random Field Model, Remote Object, Stereo Pair},
	pages = {13--25},
}

@inproceedings{elhayek_spatio-temporal_2012,
	title = {Spatio-temporal motion tracking with unsynchronized cameras},
	doi = {10/gnrfmq},
	abstract = {We present a new spatio-temporal method for markerless motion capture. We reconstruct the pose and motion of a character from a multi-view video sequence without requiring the cameras to be synchronized and without aligning captured frames in time. By formulating the model-to-image similarity measure as a temporally continuous functional, we are also able to reconstruct motion in much higher temporal detail than was possible with previous synchronized approaches. By purposefully running cameras unsynchronized we can capture even very fast motion at speeds that off-the-shelf but high quality cameras provide.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Elhayek, A. and Stoll, C. and Hasler, N. and Kim, K. I. and Seidel, H.-P. and Theobalt, C.},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919
CCF: A},
	keywords = {/unread, Cameras, Image color analysis, Joints, Motion segmentation, Streaming media, Synchronization, Tracking},
	pages = {1870--1877},
}

@article{barron_shape_2020,
	title = {Shape, {Illumination}, and {Reflectance} from {Shading}},
	url = {http://arxiv.org/abs/2010.03592},
	doi = {10/gr8wg7},
	abstract = {A fundamental problem in computer vision is that of inferring the intrinsic, 3D structure of the world from flat, 2D images of that world. Traditional methods for recovering scene properties such as shape, reflectance, or illumination rely on multiple observations of the same scene to overconstrain the problem. Recovering these same properties from a single image seems almost impossible in comparison -- there are an infinite number of shapes, paint, and lights that exactly reproduce a single image. However, certain explanations are more likely than others: surfaces tend to be smooth, paint tends to be uniform, and illumination tends to be natural. We therefore pose this problem as one of statistical inference, and define an optimization problem that searches for the *most likely* explanation of a single image. Our technique can be viewed as a superset of several classic computer vision problems (shape-from-shading, intrinsic images, color constancy, illumination estimation, etc) and outperforms all previous solutions to those constituent problems.},
	urldate = {2023-02-02},
	journal = {TPAMI 2015},
	author = {Barron, Jonathan T. and Malik, Jitendra},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03592 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{jeong_self-calibrating_2021,
	title = {Self-{Calibrating} {Neural} {Radiance} {Fields}},
	doi = {10/gr8wg6},
	abstract = {In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires learning the geometry of the scene, and we use Neural Radiance Fields (NeRF). We also propose a new geometric loss function, viz., projected ray distance loss, to incorporate geometric consistency for complex non-linear camera models. We validate our approach on standard real image datasets and demonstrate that our model can learn the camera intrinsics and extrinsics (pose) from scratch without COLMAP initialization. Also, we show that learning accurate camera models in a differentiable manner allows us to improve PSNR over baselines. Our module is an easy-to-use plugin that can be applied to NeRF variants to improve performance. The code and data are currently available at https://github.com/POSTECH-CVLab/SCNeRF},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Jeong, Yoonwoo and Ahn, Seokjun and Choy, Christopher and Anandkumar, Animashree and Cho, Minsu and Park, Jaesik},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread, 3D from multiview and other sensors, Calibration, Cameras, Codes, Computational photography, Computer vision, Geometry, Nonlinear distortion, Standards, Stereo, Vision applications and systems},
	pages = {5826--5834},
}

@inproceedings{williams_pyramidal_1983,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '83},
	title = {Pyramidal parametrics},
	isbn = {978-0-89791-109-2},
	url = {https://dl.acm.org/doi/10.1145/800059.801126},
	doi = {10/c3w23d},
	abstract = {The mapping of images onto surfaces may substantially increase the realism and information content of computer-generated imagery. The projection of a flat source image onto a curved surface may involve sampling difficulties, however, which are compounded as the view of the surface changes. As the projected scale of the surface increases, interpolation between the original samples of the source image is necessary; as the scale is reduced, approximation of multiple samples in the source is required. Thus a constantly changing sampling window of view-dependent shape must traverse the source image. To reduce the computation implied by these requirements, a set of prefiltered source images may be created. This approach can be applied to particular advantage in animation, where a large number of frames using the same source image must be generated. This paper advances a “pyramidal parametric” prefiltering and sampling geometry which minimizes aliasing effects and assures continuity within and between target images. Although the mapping of texture onto surfaces is an excellent example of the process and provided the original motivation for its development, pyramidal parametric data structures admit of wider application. The aliasing of not only surface texture, but also highlights and even the surface representations themselves, may be minimized by pyramidal parametric means.},
	urldate = {2023-05-03},
	booktitle = {Proceedings of the 10th annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Williams, Lance},
	year = {1983},
	keywords = {/unread, Antialiasing, Illumination models, Modeling, Pyramidal data structures, Reflectance mapping, Texture mapping, Visible surface algorithms},
	pages = {1--11},
}

@inproceedings{chen_projective_2022,
	title = {Projective {Manifold} {Gradient} {Layer} for {Deep} {Rotation} {Regression}},
	doi = {10/gr8wg4},
	abstract = {Regressing rotations on SO(3) manifold using deep neural networks is an important yet unsolved problem. The gap between the Euclidean network output space and the non-Euclidean SO(3) manifold imposes a severe challenge for neural network learning in both forward and backward passes. While several works have proposed different regression-friendly rotation representations, very few works have been devoted to improving the gradient back-propagating in the backward pass. In this paper, we propose a manifold-aware gradient that directly backpropagates into deep network weights. Leveraging Riemannian optimization to construct a novel projective gradient, our proposed regularized projective manifold gradient (RPMG) method helps networks achieve new state-of-the-art performance in a variety of rotation estimation tasks. Our proposed gradient layer can also be applied to other smooth manifolds such as the unit sphere. Our project page is at https://jychen18.github.io/RPMG.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Chen, Jiayi and Yin, Yingda and Birdal, Tolga and Chen, Baoquan and Guibas, Leonidas J. and Wang, He},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer vision, Deep learning, Manifolds, Neural networks, Pattern recognition, Pose estimation, Pose estimation and tracking, Task analysis},
	pages = {6636--6645},
}

@inproceedings{wang_programmable_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Programmable {Triangulation} {Light} {Curtains}},
	isbn = {978-3-030-01219-9},
	doi = {10/gr8wg3},
	abstract = {A vehicle on a road or a robot in the field does not need a full-featured 3D depth sensor to detect potential collisions or monitor its blind spot. Instead, it needs to only monitor if any object comes within its near proximity which is an easier task than full depth scanning. We introduce a novel device that monitors the presence of objects on a virtual shell near the device, which we refer to as a light curtain. Light curtains offer a light-weight, resource-efficient and programmable approach to proximity awareness for obstacle avoidance and navigation. They also have additional benefits in terms of improving visibility in fog as well as flexibility in handling light fall-off. Our prototype for generating light curtains works by rapidly rotating a line sensor and a line laser, in synchrony. The device is capable of generating light curtains of various shapes with a range of 20–30 m in sunlight (40 m under cloudy skies and 50 m indoors) and adapts dynamically to the demands of the task. We analyze properties of light curtains and various approaches to optimize their thickness as well as power requirements. We showcase the potential of light curtains using a range of real-world scenarios.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Wang, Jian and Bartels, Joseph and Whittaker, William and Sankaranarayanan, Aswin C. and Narasimhan, Srinivasa G.},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {/unread, Computational imaging, Proximity sensors},
	pages = {20--35},
}

@inproceedings{kendall_posenet_2015,
	title = {{PoseNet}: {A} {Convolutional} {Network} for {Real}-{Time} 6-{DOF} {Camera} {Relocalization}},
	shorttitle = {{PoseNet}},
	doi = {10/gc4n9z},
	abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degrees accuracy for large scale outdoor scenes and 0.5m and 5 degrees accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
	month = dec,
	year = {2015},
	note = {ISSN: 2380-7504},
	keywords = {2015, Cameras, ICCV, Neural networks, PoseNet, Quaternions, Read Later, Real-time systems, Relocalization, Robot vision systems, Simultaneous localization and mapping, Training},
	pages = {2938--2946},
}

@inproceedings{dong_physics-based_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Physics-{Based} {Feature} {Dehazing} {Networks}},
	isbn = {978-3-030-58577-8},
	doi = {10/gr8wgz},
	abstract = {We propose a physics-based feature dehazing network for image dehazing. In contrast to most existing end-to-end trainable network-based dehazing methods, we explicitly consider the physics model of the haze process in the network design and remove haze in a deep feature space. We propose an effective feature dehazing unit (FDU), which is applied to the deep feature space to explore useful features for image dehazing based on the physics model. The FDU is embedded into an encoder and decoder architecture with residual learning, so that the proposed network can be trained in an end-to-end fashion and effectively help haze removal. The encoder and decoder modules are adopted for feature extraction and clear image reconstruction, respectively. The residual learning is applied to increase the accuracy and ease the training of deep neural networks. We analyze the effectiveness of the proposed network and demonstrate that it can effectively dehaze images with favorable performance against state-of-the-art methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Dong, Jiangxin and Pan, Jinshan},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {/unread, Deep convolutional neural networks, Feature dehazing unit, Image dehazing, Physics model},
	pages = {188--204},
}

@inproceedings{fujimura_photometric_2018,
	title = {Photometric {Stereo} in {Participating} {Media} {Considering} {Shape}-{Dependent} {Forward} {Scatter}},
	doi = {10/gr8wg2},
	abstract = {Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Fujimura, Yuki and Iiyama, Masaaki and Hashimoto, Atsushi and Minoh, Michihiko},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Backscatter, Cameras, Image reconstruction, Media, Scattering, Shape, Three-dimensional displays},
	pages = {7445--7453},
}

@inproceedings{tsiotsios_backscatter_2014,
	title = {Backscatter {Compensated} {Photometric} {Stereo} with 3 {Sources}},
	url = {https://ieeexplore.ieee.org/document/6909686},
	doi = {10/gr8wfv},
	abstract = {Photometric stereo offers the possibility of object shape reconstruction via reasoning about the amount of light reflected from oriented surfaces. However, in murky media such as sea water, the illuminating light interacts with the medium and some of it is backscattered towards the camera. Due to this additive light component, the standard Photometric Stereo equations lead to poor quality shape estimation. Previous authors have attempted to reformulate the approach but have either neglected backscatter entirely or disregarded its non-uniformity on the sensor when camera and lights are close to each other. We show that by compensating effectively for the backscatter component, a linear formulation of Photometric Stereo is allowed which recovers an accurate normal map using only 3 lights. Our backscatter compensation method for point-sources can be used for estimating the uneven backscatter directly from single images without any prior knowledge about the characteristics of the medium or the scene. We compare our method with previous approaches through extensive experimental results, where a variety of objects are imaged in a big water tank whose turbidity is systematically increased, and show reconstruction quality which degrades little relative to clean water results even with a very significant scattering level.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Tsiotsios, Chourmouzios and Angelopoulou, Maria E. and Kim, Tae-Kyun and Davison, Andrew J.},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919
CCF: A},
	keywords = {/unread, Backscatter, Brightness, Cameras, Lighting, Mathematical model, Scattering, Shape, backscatter, photometric stereo, restoration, scattering, underwater vision},
	pages = {2259--2266},
}

@inproceedings{klein_parallel_2007,
	title = {Parallel {Tracking} and {Mapping} for {Small} {AR} {Workspaces}},
	doi = {10/cd2xnv},
	abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
	booktitle = {2007 6th {IEEE} and {ACM} {International} {Symposium} on {Mixed} and {Augmented} {Reality}},
	author = {Klein, Georg and Murray, David},
	month = nov,
	year = {2007},
	keywords = {/unread, Algorithm design and analysis, Cameras, Concurrent computing, Handheld computers, Layout, Robot vision systems, Robustness, Simultaneous localization and mapping, Tracking, Yarn},
	pages = {225--234},
}

@inproceedings{yang_asynchronous_2021,
	title = {Asynchronous {Multi}-{View} {SLAM}},
	url = {https://ieeexplore.ieee.org/document/9561481/},
	doi = {10/gqrgmj},
	abstract = {Existing multi-camera SLAM systems assume synchronized shutters for all cameras, which is often not the case in practice. In this work, we propose a generalized multi-camera SLAM formulation which accounts for asynchronous sensor observations. Our framework integrates a continuous-time motion model to relate information across asynchronous multi-frames during tracking, local mapping, and loop closing. For evaluation, we collected AMV-Bench, a challenging new SLAM dataset covering 482 km of driving recorded using our asynchronous multi-camera robotic platform. AMV-Bench is over an order of magnitude larger than previous multi-view HD outdoor SLAM datasets, and covers diverse and challenging motions and environments. Our experiments emphasize the necessity of asynchronous sensor modeling, and show that the use of multiple cameras is critical towards robust and accurate SLAM in challenging outdoor scenes. The supplementary material is located at: https://www.cs.toronto.edu/ ajyang/amv-slam},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Yang, Anqi Joyce and Cui, Can and Barsan, Ioan Andrei and Urtasun, Raquel and Wang, Shenlong},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {/unread, Automation, Cameras, Conferences, Robot vision systems, Simultaneous localization and mapping, Tracking, Tracking loops},
	pages = {5669--5676},
}

@inproceedings{geng_training_2021,
	title = {On {Training} {Implicit} {Models}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html},
	urldate = {2022-11-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Geng, Zhengyang and Zhang, Xin-Yu and Bai, Shaojie and Wang, Yisen and Lin, Zhouchen},
	year = {2021},
	keywords = {/unread, ⛔ No DOI found},
	pages = {24247--24260},
}

@inproceedings{long_neuraludf_2022,
	title = {{NeuralUDF}: {Learning} {Unsigned} {Distance} {Fields} for {Multi}-view {Reconstruction} of {Surfaces} with {Arbitrary} {Topologies}},
	shorttitle = {{NeuralUDF}},
	url = {https://www.xxlong.site/NeuralUDF/},
	abstract = {We present a novel method, called NeuralUDF, for reconstructing surfaces with arbitrary topologies from 2D images via volume rendering. Recent advances in neural rendering based reconstruction have achieved compelling results. However, these methods are limited to objects with closed surfaces since they adopt Signed Distance Function (SDF) as surface representation which requires the target shape to be divided into inside and outside. In this paper, we propose to represent surfaces as the Unsigned Distance Function (UDF) and develop a new volume rendering scheme to learn the neural UDF representation. Specifically, a new density function that correlates the property of UDF with the volume rendering scheme is introduced for robust optimization of the UDF fields. Experiments on the DTU and DeepFashion3D datasets show that our method not only enables high-quality reconstruction of non-closed shapes with complex typologies, but also achieves comparable performance to the SDF based methods on the reconstruction of closed surfaces.},
	urldate = {2023-03-30},
	author = {Long, Xiaoxiao and Lin, Cheng and Liu, Lingjie and Liu, Yuan and Wang, Peng and Theobalt, Christian and Komura, Taku and Wang, Wenping},
	year = {2022},
	keywords = {/unread, ⛔ No DOI found},
}

@inproceedings{geiger_are_2012,
	title = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
	shorttitle = {{KITTI}},
	url = {http://ieeexplore.ieee.org/document/6248074/},
	doi = {10/gf7nxj},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Geiger, A. and Lenz, P. and Urtasun, R.},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919
CCF: A},
	keywords = {/unread, Benchmark testing, Cameras, Measurement, Optical imaging, Optical sensors, Visualization},
	pages = {3354--3361},
}

@inproceedings{takikawa_neural_2021,
	title = {Neural {Geometric} {Level} of {Detail}: {Real}-time {Rendering} with {Implicit} {3D} {Shapes}},
	shorttitle = {Neural {Geometric} {Level} of {Detail}},
	doi = {10/gnsbtd},
	abstract = {Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2–3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Takikawa, Towaki and Litalien, Joey and Yin, Kangxue and Kreis, Karsten and Loop, Charles and Nowrouzezahrai, Derek and Jacobson, Alec and McGuire, Morgan and Fidler, Sanja},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Measurement, Neural networks, Octrees, Rendering (computer graphics), Shape, Surface reconstruction, Three-dimensional displays},
	pages = {11353--11362},
}

@inproceedings{li_aod-net_2017,
	title = {{AOD}-{Net}: {All}-in-{One} {Dehazing} {Network}},
	shorttitle = {{AOD}-{Net}},
	url = {http://ieeexplore.ieee.org/document/8237773/},
	doi = {10/gfxhxv},
	abstract = {This paper proposes an image dehazing model built with a convolutional neural network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed based on a re-formulated atmospheric scattering model. Instead of estimating the transmission matrix and the atmospheric light separately as most previous models did, AOD-Net directly generates the clean image through a light-weight CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other deep models, e.g., Faster R-CNN, for improving high-level tasks on hazy images. Experimental results on both synthesized and natural hazy image datasets demonstrate our superior performance than the state-of-the-art in terms of PSNR, SSIM and the subjective visual quality. Furthermore, when concatenating AOD-Net with Faster R-CNN, we witness a large improvement of the object detection performance on hazy images.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Li, Boyi and Peng, Xiulian and Wang, Zhangyang and Xu, Jizheng and Feng, Dan},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {/unread, Atmospheric modeling, Computational modeling, Estimation, Image restoration, Scattering, Visualization},
	pages = {4780--4788},
}

@inproceedings{bai_neural_2022,
	title = {Neural {Deep} {Equilibrium} {Solvers}},
	url = {https://openreview.net/forum?id=B0oHOwT5ENL},
	abstract = {A deep equilibrium (DEQ) model abandons traditional depth by solving for the fixed point of a single nonlinear layer \$f\_{\textbackslash}theta\$. This structure enables decoupling the internal structure of the layer (which controls representational capacity) from how the fixed point is actually computed (which impacts inference-time efficiency), which is usually via classic techniques such as Broyden's method or Anderson acceleration. In this paper, we show that one can exploit such decoupling and substantially enhance this fixed point computation using a custom neural solver. Specifically, our solver uses a parameterized network to both guess an initial value of the optimization and perform iterative updates, in a method that generalizes a learnable form of Anderson acceleration and can be trained end-to-end in an unsupervised manner. Such a solution is particularly well suited to the implicit model setting, because inference in these models requires repeatedly solving for a fixed point of the same nonlinear layer for different inputs, a task at which our network excels. Our experiments show that these neural equilibrium solvers are fast to train (only taking an extra 0.9-1.1\% over the original DEQ's training time), require few additional parameters (1-3\% of the original model size), yet lead to a \$2{\textbackslash}times\$ speedup in DEQ network inference without any degradation in accuracy across numerous domains and tasks.},
	language = {en},
	urldate = {2022-11-14},
	author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
	month = mar,
	year = {2022},
	keywords = {/unread},
}

@inproceedings{sun_neural_2022,
	title = {Neural {3D} {Reconstruction} in the {Wild}},
	url = {http://arxiv.org/abs/2205.12955},
	doi = {10/gqkshk},
	abstract = {We are witnessing an explosion of neural implicit representations in computer vision and graphics. Their applicability has recently expanded beyond tasks such as shape generation and image-based rendering to the fundamental problem of image-based 3D reconstruction. However, existing methods typically assume constrained 3D environments with constant illumination captured by a small set of roughly uniformly distributed cameras. We introduce a new method that enables efficient and accurate surface reconstruction from Internet photo collections in the presence of varying illumination. To achieve this, we propose a hybrid voxel- and surface-guided sampling technique that allows for more efficient ray sampling around surfaces and leads to significant improvements in reconstruction quality. Further, we present a new benchmark and protocol for evaluating reconstruction performance on such in-the-wild scenes. We perform extensive experiments, demonstrating that our approach surpasses both classical and neural reconstruction methods on a wide variety of metrics.},
	urldate = {2023-05-01},
	booktitle = {Special {Interest} {Group} on {Computer} {Graphics} and {Interactive} {Techniques} {Conference} {Proceedings}},
	author = {Sun, Jiaming and Chen, Xi and Wang, Qianqian and Li, Zhengqi and Averbuch-Elor, Hadar and Zhou, Xiaowei and Snavely, Noah},
	month = aug,
	year = {2022},
	note = {arXiv:2205.12955 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	pages = {1--9},
}

@inproceedings{martin-brualla_nerf_2021,
	title = {{NeRF} in the {Wild}: {Neural} {Radiance} {Fields} for {Unconstrained} {Photo} {Collections}},
	shorttitle = {{NeRF} in the {Wild}},
	doi = {10/gnkn98},
	abstract = {We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multi-layer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Image color analysis, Learning systems, Lighting, Photorealism, Rendering (computer graphics), Solid modeling, Three-dimensional displays},
	pages = {7206--7215},
}

@inproceedings{zhou_nerd_2021,
	title = {{NeRD}: {Neural} {3D} {Reflection} {Symmetry} {Detector}},
	shorttitle = {{NeRD}},
	doi = {10/grkjh9},
	abstract = {Recent advances have shown that symmetry, a structural prior that most objects exhibit, can support a variety of single-view 3D understanding tasks. However, detecting 3D symmetry from an image remains a challenging task. Previous works either assume the symmetry is given or detect the symmetry with a heuristic-based method. In this paper, we present NeRD, a Neural 3D Reflection Symmetry Detector, which combines the strength of learning-based recognition and geometry-based reconstruction to accurately recover the normal direction of objects’ mirror planes. Specifically, we enumerate the symmetry planes with a coarse-to-fine strategy and find the best ones by building 3D cost volumes to examine the intra-image pixel correspondence from the symmetry. Our experiments show that the symmetry planes detected with our method are significantly more accurate than the planes from direct CNN regression on both synthetic and real datasets. More importantly, we also demonstrate that the detected symmetry can be used to improve the performance of downstream tasks such as pose estimation and depth map regression by a wide margin over existing methods. The code of this paper has been made public at https://github.com/zhou13/nerd.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhou, Yichao and Liu, Shichen and Ma, Yi},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Computer vision, Costs, Detectors, Pattern recognition, Pose estimation, Reflection, Three-dimensional displays},
	pages = {15935--15944},
}

@inproceedings{bai_multiscale_2020,
	title = {Multiscale {Deep} {Equilibrium} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/3812f9a59b634c2a9c574610eaba5bed-Abstract.html},
	abstract = {We propose a new class of implicit networks, the multiscale deep equilibrium model (MDEQ), suited to large-scale and highly hierarchical pattern recognition domains. An MDEQ directly solves for and backpropagates through the equilibrium points of multiple feature resolutions simultaneously, using implicit differentiation to avoid storing intermediate states (and thus requiring only O(1) memory consumption). These simultaneously-learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. We illustrate the effectiveness of this approach on two large-scale vision tasks: ImageNet classification and semantic segmentation on high-resolution images from the Cityscapes dataset. In both settings, MDEQs are able to match or exceed the performance of recent competitive computer vision models: the first time such performance and scale have been achieved by an implicit deep learning approach. The code and pre-trained models are at https://github.com/locuslab/mdeq.},
	urldate = {2022-11-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
	year = {2020},
	keywords = {/unread, ⛔ No DOI found},
	pages = {5238--5250},
}

@inproceedings{xu_multi-scale_2018,
	title = {Multi-scale {Voxel} {Hashing} and {Efficient} {3D} {Representation} for {Mobile} {Augmented} {Reality}},
	doi = {10/gr8wgs},
	abstract = {In recent years, Visual-Inertial Odometry (VIO) technologies have been making great strides in both research community and industry. With the development of ARKit and ARCore, mobile Augmented Reality (AR) applications have become popular. However, collision detection and avoidance is largely un-addressed with these applications. In this paper, we present an efficient multi-scale voxel hashing algorithm for representing a 3D environment using a set of multi-scale voxels. The input to our algorithm is the 3D point cloud generated by a VIO system (e.g., ARKit). We show that our method can process the 3D points and convert them into multi-scale 3D representation in real time, while maintaining a small memory footprint. The 3D representation can be used to efficiently detect collision between digital objects and real objects in an environment in AR applications.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Xu, Yi and Wu, Yuzhang and Zhou, Hui},
	month = jun,
	year = {2018},
	note = {ISSN: 2160-7516},
	keywords = {/unread, Augmented reality, Cameras, Collision avoidance, Real-time systems, Simultaneous localization and mapping, Three-dimensional displays, Visualization},
	pages = {1586--15867},
}

@inproceedings{dong_multi-scale_2020,
	title = {Multi-{Scale} {Boosted} {Dehazing} {Network} {With} {Dense} {Feature} {Fusion}},
	doi = {10/ghbcmb},
	abstract = {In this paper, we propose a Multi-Scale Boosted Dehazing Network with Dense Feature Fusion based on the U-Net architecture. The proposed method is designed based on two principles, boosting and error feedback, and we show that they are suitable for the dehazing problem. By incorporating the Strengthen-Operate-Subtract boosting strategy in the decoder of the proposed model, we develop a simple yet effective boosted decoder to progressively restore the haze-free image. To address the issue of preserving spatial information in the U-Net architecture, we design a dense feature fusion module using the back-projection feedback scheme. We show that the dense feature fusion module can simultaneously remedy the missing spatial information from high-resolution features and exploit the non-adjacent features. Extensive evaluations demonstrate that the proposed model performs favorably against the state-of-the-art approaches on the benchmark datasets as well as real-world hazy images.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Dong, Hang and Pan, Jinshan and Xiang, Lei and Hu, Zhe and Zhang, Xinyi and Wang, Fei and Yang, Ming-Hsuan},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Atmospheric modeling, Boosting, Decoding, Feature extraction, Fuses, Image restoration, Task analysis},
	pages = {2154--2164},
}

@inproceedings{xu_mr-gnn_2019,
	title = {{MR}-{GNN}: {Multi}-{Resolution} and {Dual} {Graph} {Neural} {Network} for {Predicting} {Structured} {Entity} {Interactions}},
	shorttitle = {{MR}-{GNN}},
	url = {http://arxiv.org/abs/1905.09558},
	doi = {10/gg8nm5},
	abstract = {Predicting interactions between structured entities lies at the core of numerous tasks such as drug regimen and new material design. In recent years, graph neural networks have become attractive. They represent structured entities as graphs and then extract features from each individual graph using graph convolution operations. However, these methods have some limitations: i) their networks only extract features from a fix-sized subgraph structure (i.e., a fix-sized receptive field) of each node, and ignore features in substructures of different sizes, and ii) features are extracted by considering each entity independently, which may not effectively reflect the interaction between two entities. To resolve these problems, we present MR-GNN, an end-to-end graph neural network with the following features: i) it uses a multi-resolution based architecture to extract node features from different neighborhoods of each node, and, ii) it uses dual graph-state long short-term memory networks (L-STMs) to summarize local features of each graph and extracts the interaction features between pairwise graphs. Experiments conducted on real-world datasets show that MR-GNN improves the prediction of state-of-the-art methods.},
	urldate = {2023-02-06},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Xu, Nuo and Wang, Pinghui and Chen, Long and Tao, Jing and Zhao, Junzhou},
	month = aug,
	year = {2019},
	note = {arXiv:1905.09558 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3968--3974},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	shorttitle = {{MoCo}},
	doi = {10/ghbcfj},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {/unread, Buildings, Dictionaries, Loss measurement, Task analysis, Training, Unsupervised learning, Visualization},
	pages = {9726--9735},
}

@inproceedings{kendall_modelling_2016,
	title = {Modelling uncertainty in deep learning for camera relocalization},
	doi = {10/gc4n9p},
	abstract = {We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6° accuracy for very large scale outdoor scenes and 0.5m and 10° accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the model's relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the model's uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Kendall, Alex and Cipolla, Roberto},
	month = may,
	year = {2016},
	keywords = {Bayes methods, Cameras, Computational modeling, Measurement uncertainty, Neural networks, Simultaneous localization and mapping, Uncertainty},
	pages = {4762--4769},
}

@inproceedings{kasten_learning_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning to {Estimate} {Multi}-view {Pose} from {Object} {Silhouettes}},
	isbn = {978-3-031-25085-9},
	doi = {10/gr8wgp},
	abstract = {While Structure-from-Motion pipelines certainly have their success cases in the task of 3D object reconstruction from multiple images, they still fail on many common objects that lack distinctive texture or have complex appearance qualities. The central problem lies in 6DOF camera pose estimation for the source images: without the ability to obtain a good estimate of the epipolar geometries, all state-of-the-art methods will fail. Although alternative solutions exist for specific objects, general solutions have proved elusive. In this work, we revisit the notion that silhouette cues can provide reasonable constraints on multi-view pose configurations when texture and priors are unavailable. Specifically, we train a neural network to holistically predict camera poses and pose confidences for a given set of input silhouette images, with the hypothesis that the network will be able to learn cues for multi-view relationships in a data-driven way. We show that our network generalizes to unseen synthetic and real object instances under reasonable assumptions about the input pose distribution of the images, and that the estimates are suitable to initialize state-of-the-art 3D reconstruction methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022 {Workshops}},
	publisher = {Springer Nature Switzerland},
	author = {Kasten, Yoni and Price, True and Geraghty, David and Frahm, Jan-Michael},
	editor = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
	year = {2023},
	keywords = {/unread},
	pages = {129--147},
}

@inproceedings{ma_intrinsic_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Intrinsic {Image} {Decomposition}: {A} {Comprehensive} {Review}},
	isbn = {978-3-319-71607-7},
	shorttitle = {Intrinsic {Image} {Decomposition}},
	doi = {10/ggnr9d},
	abstract = {Image understanding and analysis is one of the important tasks in the image processing. Multiple factors influence the appearance of an object in an image. However, extracting the intrinsic images from the observer image can eliminate the environmental impact effectively and make the image understanding more accurately. The intrinsic images represent the inherent shape, color and texture information of the object. Intrinsic image decomposition is recovering shading image and reflectance image from a single input image and remains a challenging problem because of its severely ill-posed problem. In order to deal with these problems, researches have proposed various algorithms for decomposing the intrinsic image. In this paper we survey the recent advances in intrinsic image decomposition. First, we introduce the existing datasets for intrinsic image decomposition. Second, we introduce and analyze the existing intrinsic image decomposition algorithms. Finally, we use the existing algorithms to experiment on the intrinsic image datasets, and analyze and summarize the experimental results.},
	language = {en},
	booktitle = {Image and {Graphics}},
	publisher = {Springer International Publishing},
	author = {Ma, Yupeng and Feng, Xiaoyi and Jiang, Xiaoyue and Xia, Zhaoqiang and Peng, Jinye},
	editor = {Zhao, Yao and Kong, Xiangwei and Taubman, David},
	year = {2017},
	keywords = {/unread, Computer vision, Intrinsic image dataset, Intrinsic image decomposition, Retinex theory},
	pages = {626--638},
}

@inproceedings{solodskikh_integral_2023,
	title = {Integral {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Solodskikh_Integral_Neural_Networks_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-05-18},
	author = {Solodskikh, Kirill and Kurbanov, Azim and Aydarkhanov, Ruslan and Zhelavskaya, Irina and Parfenov, Yury and Song, Dehua and Lefkimmiatis, Stamatios},
	year = {2023},
	keywords = {/unread},
	pages = {16113--16122},
}

@inproceedings{kondo_immersive_2022,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '22},
	title = {Immersive {Real} {World} through {Deep} {Billboards}},
	isbn = {978-1-4503-9369-0},
	url = {https://doi.org/10.1145/3532834.3536210},
	doi = {10/gr8wgj},
	abstract = {An aspirational goal for virtual reality (VR) is to bring in a rich diversity of real world objects losslessly. Existing VR applications often convert objects into explicit 3D models with meshes or point clouds, which allow fast interactive rendering but also severely limit its quality and the types of supported objects, fundamentally upper-bounding the “realism” of VR. Inspired by the classic “billboards” technique in gaming, we develop Deep Billboards that model 3D objects implicitly using neural networks, where only 2D image is rendered at a time based on the user’s viewing direction. Our system, connecting a commercial VR headset with a server running neural rendering, allows real-time high-resolution simulation of detailed rigid objects, hairy objects, actuated dynamic objects and more in an interactive VR world, drastically narrowing the existing real-to-simulation (real2sim) gap. Additionally, we augment Deep Billboards with physical interaction capability, adapting classic billboards from screen-based games to immersive VR. At our pavilion, the visitors can use our off-the-shelf setup for quickly capturing their favorite objects, and within minutes, experience them in an immersive and interactive VR world – with minimal loss of reality. Our project page: https://sites.google.com/view/deepbillboards/},
	urldate = {2022-11-27},
	booktitle = {{ACM} {SIGGRAPH} 2022 {Immersive} {Pavilion}},
	publisher = {Association for Computing Machinery},
	author = {Kondo, Naruya and Kuroki, So and Hyakuta, Ryosuke and Matsuo, Yutaka and Gu, Shixiang Shane and Ochiai, Yoichi},
	year = {2022},
	keywords = {/unread, billboard, image based rendering, neural networks},
	pages = {1--2},
}

@inproceedings{thomas_high-confidence_2015,
	title = {High-{Confidence} {Off}-{Policy} {Evaluation}},
	volume = {29},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9541},
	doi = {10/gr8wgd},
	abstract = {Many reinforcement learning algorithms use trajectories collected from the execution of one or more policies to propose a new policy. Because execution of a bad policy can be costly or dangerous, techniques for evaluating the performance of the new policy without requiring its execution have been of recent interest in industry. Such off-policy evaluation methods, which estimate the performance of a policy using trajectories collected from the execution of other policies, heretofore have not provided confidences regarding the accuracy of their estimates. In this paper we propose an off-policy method for computing a lower confidence bound on the expected return of a policy.},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
	month = feb,
	year = {2015},
	note = {ISSN: 2374-3468, 2159-5399
Issue: 1
Journal Abbreviation: AAAI},
	keywords = {/unread},
}

@inproceedings{huang_di-fusion_2021,
	title = {{DI}-{Fusion}: {Online} {Implicit} {3D} {Reconstruction} {With} {Deep} {Priors}},
	shorttitle = {{DI}-{Fusion}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Huang_DI-Fusion_Online_Implicit_3D_Reconstruction_With_Deep_Priors_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-04-09},
	author = {Huang, Jiahui and Huang, Shi-Sheng and Song, Haoxuan and Hu, Shi-Min},
	year = {2021},
	keywords = {/unread},
	pages = {8932--8941},
}

@inproceedings{bai_deep_2022,
	title = {Deep {Equilibrium} {Optical} {Flow} {Estimation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Bai_Deep_Equilibrium_Optical_Flow_Estimation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-14},
	author = {Bai, Shaojie and Geng, Zhengyang and Savani, Yash and Kolter, J. Zico},
	year = {2022},
	keywords = {/unread},
	pages = {620--630},
}

@inproceedings{bai_deep_2019,
	title = {Deep {Equilibrium} {Models}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html},
	abstract = {We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective “depth” of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88\% memory reduction in our experiments. The code is available at https://github.com/locuslab/deq.},
	urldate = {2022-11-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	year = {2019},
	keywords = {/unread, ⛔ No DOI found},
}

@inproceedings{hongwen_closet_nodate,
	title = {{CloSET}: {Modeling} {Clothed} {Humans} on {Continuous} {Surface} with {Explicit} {Template} {Decomposition}},
	abstract = {Creating animatable avatars from static scans requires the modeling of clothing deformations in different poses. Existing learning-based methods typically add pose-dependent deformations upon a minimally-clothed mesh template or a learned implicit template, which have limitations in capturing details or hinder end-to-end learning. In this paper, we revisit point-based solutions and propose to decompose explicit garment-related templates and then add pose-dependent wrinkles to them. In this way, the clothing deformations are disentangled such that the pose-dependent wrinkles can be better learned and applied to unseen poses. Additionally, to tackle the seam artifact issues in recent state-of-the-art point-based methods, we propose to learn point features on a body surface, which establishes a continuous and compact feature space to capture the fine-grained and pose-dependent clothing geometry. To facilitate the research in this field, we also introduce a high-quality scan dataset of humans in real-world clothing. Our approach is validated on two existing datasets and our newly introduced dataset, showing better clothing deformation results in unseen poses.},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Hongwen, Zhang and Siyou, Lin and Ruizhi, Shao and Yuxiang, Zhang and Zerong, Zheng and Han, Huang and Yandong, Guo and Yebin, Liu},
	note = {titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:
titleTranslation:},
	keywords = {/unread, ⛔ No DOI found},
}

@inproceedings{sturm_benchmark_2012,
	title = {A benchmark for the evaluation of {RGB}-{D} {SLAM} systems},
	url = {http://ieeexplore.ieee.org/document/6385773/},
	doi = {10/ggqcf9},
	abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
	booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Sturm, Jrgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
	month = oct,
	year = {2012},
	note = {ISSN: 2153-0866
CCF: C},
	keywords = {Calibration, Cameras, Simultaneous localization and mapping, Trajectory, Visualization},
	pages = {573--580},
}

@article{yang_vox-fusion_2022,
	title = {Vox-{Fusion}: {Dense} {Tracking} and {Mapping} with {Voxel}-based {Neural} {Implicit} {Representation}},
	shorttitle = {Vox-{Fusion}},
	url = {https://arxiv.org/abs/2210.15858v1},
	doi = {10/gr8whd},
	abstract = {In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods. Our approach is inspired by the recently developed implicit mapping and positioning system and further extends the idea so that it can be freely applied to practical scenarios. Specifically, we leverage a voxel-based neural implicit surface representation to encode and optimize the scene inside each voxel. Furthermore, we adopt an octree-based structure to divide the scene and support dynamic expansion, enabling our system to track and map arbitrary scenes without knowing the environment like in previous works. Moreover, we proposed a high-performance multi-process framework to speed up the method, thus supporting some applications that require real-time performance. The evaluation results show that our methods can achieve better accuracy and completeness than previous methods. We also show that our Vox-Fusion can be used in augmented reality and virtual reality applications. Our source code is publicly available at https://github.com/zju3dv/Vox-Fusion.},
	language = {en},
	urldate = {2022-11-16},
	author = {Yang, Xingrui and Li, Hai and Zhai, Hongjia and Ming, Yuhang and Liu, Yuqian and Zhang, Guofeng},
	month = oct,
	year = {2022},
	keywords = {/unread},
}

@misc{zhu_x-nerf_2022,
	title = {X-{NeRF}: {Explicit} {Neural} {Radiance} {Field} for {Multi}-{Scene} 360\${\textasciicircum}\{{\textbackslash}circ\} \$ {Insufficient} {RGB}-{D} {Views}},
	shorttitle = {X-{NeRF}},
	url = {http://arxiv.org/abs/2210.05135},
	doi = {10.48550/arXiv.2210.05135},
	abstract = {Neural Radiance Fields (NeRFs), despite their outstanding performance on novel view synthesis, often need dense input views. Many papers train one model for each scene respectively and few of them explore incorporating multi-modal data into this problem. In this paper, we focus on a rarely discussed but important setting: can we train one model that can represent multiple scenes, with 360\${\textasciicircum}{\textbackslash}circ \$ insufficient views and RGB-D images? We refer insufficient views to few extremely sparse and almost non-overlapping views. To deal with it, X-NeRF, a fully explicit approach which learns a general scene completion process instead of a coordinate-based mapping, is proposed. Given a few insufficient RGB-D input views, X-NeRF first transforms them to a sparse point cloud tensor and then applies a 3D sparse generative Convolutional Neural Network (CNN) to complete it to an explicit radiance field whose volumetric rendering can be conducted fast without running networks during inference. To avoid overfitting, besides common rendering loss, we apply perceptual loss as well as view augmentation through random rotation on point clouds. The proposed methodology significantly out-performs previous implicit methods in our setting, indicating the great potential of proposed problem and approach. Codes and data are available at https://github.com/HaoyiZhu/XNeRF.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Zhu, Haoyi and Fang, Hao-Shu and Lu, Cewu},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05135 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bhat_zoedepth_2023,
	title = {{ZoeDepth}: {Zero}-shot {Transfer} by {Combining} {Relative} and {Metric} {Depth}},
	shorttitle = {{ZoeDepth}},
	url = {http://arxiv.org/abs/2302.12288},
	doi = {10.48550/arXiv.2302.12288},
	abstract = {This paper tackles the problem of depth estimation from a single image. Existing work either focuses on generalization performance disregarding metric scale, i.e. relative depth estimation, or state-of-the-art results on specific datasets, i.e. metric depth estimation. We propose the first approach that combines both worlds, leading to a model with excellent generalization performance while maintaining metric scale. Our flagship model, ZoeD-M12-NK, is pre-trained on 12 datasets using relative depth and fine-tuned on two datasets using metric depth. We use a lightweight head with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier. Our framework admits multiple configurations depending on the datasets used for relative depth pre-training and metric fine-tuning. Without pre-training, we can already significantly improve the state of the art (SOTA) on the NYU Depth v2 indoor dataset. Pre-training on twelve datasets and fine-tuning on the NYU Depth v2 indoor dataset, we can further improve SOTA for a total of 21\% in terms of relative absolute error (REL). Finally, ZoeD-M12-NK is the first model that can jointly train on multiple datasets (NYU Depth v2 and KITTI) without a significant drop in performance and achieve unprecedented zero-shot generalization performance to eight unseen datasets from both indoor and outdoor domains. The code and pre-trained models are publicly available at https://github.com/isl-org/ZoeDepth .},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Bhat, Shariq Farooq and Birkl, Reiner and Wofk, Diana and Wonka, Peter and Müller, Matthias},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12288 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wu_visual_2023,
	title = {Visual {ChatGPT}: {Talking}, {Drawing} and {Editing} with {Visual} {Foundation} {Models}},
	shorttitle = {Visual {ChatGPT}},
	url = {http://arxiv.org/abs/2303.04671},
	doi = {10.48550/arXiv.2303.04671},
	abstract = {ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called {\textbackslash}textbf\{Visual ChatGPT\}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at {\textbackslash}url\{https://github.com/microsoft/visual-chatgpt\}.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04671 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@phdthesis{davis_visual_2016,
	type = {Thesis},
	title = {Visual vibration analysis},
	copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
	url = {https://dspace.mit.edu/handle/1721.1/107330},
	abstract = {This dissertation shows how regular cameras can be used to record and analyze the vibrations of visible objects. Through careful temporal analysis, we relate subtle changes in video to the vibrations of recorded surfaces, and use that information to reason about the physical properties of objects and the forces that drive their motion. We explore several applications of our approach to extracting vibrations from video - using it to recover sound from distant surfaces, estimate the physical properties of visible objects, and even predict how objects will respond to new, previously unseen forces. Our work impacts a variety of fields, ranging from computer vision, to long-distance structural health monitoring and nondestructive testing, surveillance, and even visual effects for film. By imaging the vibrations of objects, we offer cameras as low-cost vibration sensors with dramatically higher spatial resolution than the devices traditionally used in engineering. In doing so, we turn every camera into a powerful tool for vibration analysis, and provide an exciting new way to image the world.},
	language = {eng},
	urldate = {2022-11-14},
	school = {Massachusetts Institute of Technology},
	author = {Davis, Myers Abraham},
	year = {2016},
	note = {Accepted: 2017-03-10T15:05:39Z},
	keywords = {/unread},
}

@misc{zhu_vdn-nerf_2023,
	title = {{VDN}-{NeRF}: {Resolving} {Shape}-{Radiance} {Ambiguity} via {View}-{Dependence} {Normalization}},
	shorttitle = {{VDN}-{NeRF}},
	url = {http://arxiv.org/abs/2303.17968},
	abstract = {We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for better geometry under non-Lambertian surface and dynamic lighting conditions that cause significant variation in the radiance of a point when viewed from different angles. Instead of explicitly modeling the underlying factors that result in the view-dependent phenomenon, which could be complex yet not inclusive, we develop a simple and effective technique that normalizes the view-dependence by distilling invariant information already encoded in the learned NeRFs. We then jointly train NeRFs for view synthesis with view-dependence normalization to attain quality geometry. Our experiments show that even though shape-radiance ambiguity is inevitable, the proposed normalization can minimize its effect on geometry, which essentially aligns the optimal capacity needed for explaining view-dependent variations. Our method applies to various baselines and significantly improves geometry without changing the volume rendering pipeline, even if the data is captured under a moving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.},
	urldate = {2023-04-22},
	publisher = {arXiv},
	author = {Zhu, Bingfan and Yang, Yanchao and Wang, Xulong and Zheng, Youyi and Guibas, Leonidas},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17968 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fan_uva_2023,
	title = {{UVA}: {Towards} {Unified} {Volumetric} {Avatar} for {View} {Synthesis}, {Pose} rendering, {Geometry} and {Texture} {Editing}},
	shorttitle = {{UVA}},
	url = {http://arxiv.org/abs/2304.06969},
	abstract = {Neural radiance field (NeRF) has become a popular 3D representation method for human avatar reconstruction due to its high-quality rendering capabilities, e.g., regarding novel views and poses. However, previous methods for editing the geometry and appearance of the avatar only allow for global editing through body shape parameters and 2D texture maps. In this paper, we propose a new approach named {\textbackslash}textbf\{U\}nified {\textbackslash}textbf\{V\}olumetric {\textbackslash}textbf\{A\}vatar ({\textbackslash}textbf\{UVA\}) that enables local and independent editing of both geometry and texture, while retaining the ability to render novel views and poses. UVA transforms each observation point to a canonical space using a skinning motion field and represents geometry and texture in separate neural fields. Each field is composed of a set of structured latent codes that are attached to anchor nodes on a deformable mesh in canonical space and diffused into the entire space via interpolation, allowing for local editing. To address spatial ambiguity in code interpolation, we use a local signed height indicator. We also replace the view-dependent radiance color with a pose-dependent shading factor to better represent surface illumination in different poses. Experiments on multiple human avatars demonstrate that our UVA achieves competitive results in novel view synthesis and novel pose rendering while enabling local and independent editing of geometry and appearance. The source code will be released.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Fan, Jinlong and Zhang, Jing and Tao, Dacheng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06969 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kondo_vaxnerf_2021,
	title = {{VaxNeRF}: {Revisiting} the {Classic} for {Voxel}-{Accelerated} {Neural} {Radiance} {Field}},
	shorttitle = {{VaxNeRF}},
	url = {http://arxiv.org/abs/2111.13112},
	doi = {10.48550/arXiv.2111.13112},
	abstract = {Neural Radiance Field (NeRF) is a popular method in data-driven 3D reconstruction. Given its simplicity and high quality rendering, many NeRF applications are being developed. However, NeRF's big limitation is its slow speed. Many attempts are made to speeding up NeRF training and inference, including intricate code-level optimization and caching, use of sophisticated data structures, and amortization through multi-task and meta learning. In this work, we revisit the basic building blocks of NeRF through the lens of classic techniques before NeRF. We propose Voxel-Accelearated NeRF (VaxNeRF), integrating NeRF with visual hull, a classic 3D reconstruction technique only requiring binary foreground-background pixel labels per image. Visual hull, which can be optimized in about 10 seconds, can provide coarse in-out field separation to omit substantial amounts of network evaluations in NeRF. We provide a clean fully-pythonic, JAX-based implementation on the popular JaxNeRF codebase, consisting of only about 30 lines of code changes and a modular visual hull subroutine, and achieve about 2-8x faster learning on top of the highly-performative JaxNeRF baseline with zero degradation in rendering quality. With sufficient compute, this effectively brings down full NeRF training from hours to 30 minutes. We hope VaxNeRF -- a careful combination of a classic technique with a deep method (that arguably replaced it) -- can empower and accelerate new NeRF extensions and applications, with its simplicity, portability, and reliable performance gains. Codes are available at https://github.com/naruya/VaxNeRF .},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Kondo, Naruya and Ikeda, Yuya and Tagliasacchi, Andrea and Matsuo, Yutaka and Ochiai, Yoichi and Gu, Shixiang Shane},
	month = nov,
	year = {2021},
	note = {arXiv:2111.13112 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lisus_towards_2023,
	title = {Towards {Open} {World} {NeRF}-{Based} {SLAM}},
	url = {http://arxiv.org/abs/2301.03102},
	abstract = {Neural Radiance Fields (NeRFs) offer versatility and robustness in map representations for Simultaneous Localization and Mapping (SLAM) tasks. This paper extends NICE-SLAM, a recent state-of-the-art NeRF-based SLAM algorithm capable of producing high quality NeRF maps. However, depending on the hardware used, the required number of iterations to produce these maps often makes NICE-SLAM run at less than real time. Additionally, the estimated trajectories fail to be competitive with classical SLAM approaches. Finally, NICE-SLAM requires a grid covering the considered environment to be defined prior to runtime, making it difficult to extend into previously unseen scenes. This paper seeks to make NICE-SLAM more open-world-capable by improving the robustness and tracking accuracy, and generalizing the map representation to handle unconstrained environments. This is done by improving measurement uncertainty handling, incorporating motion information, and modelling the map as having an explicit foreground and background. It is shown that these changes are able to improve tracking accuracy by 85\% to 97\% depending on the available resources, while also improving mapping in environments with visual information extending outside of the predefined grid.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Lisus, Daniil and Holmes, Connor and Waslander, Steven},
	month = may,
	year = {2023},
	note = {arXiv:2301.03102 [cs]},
	keywords = {/unread, Computer Science - Robotics},
}

@misc{cheng_tuvf_2023,
	title = {{TUVF}: {Learning} {Generalizable} {Texture} {UV} {Radiance} {Fields}},
	shorttitle = {{TUVF}},
	url = {http://arxiv.org/abs/2305.03040},
	abstract = {Textures are a vital aspect of creating visually appealing and realistic 3D models. In this paper, we study the problem of generating high-fidelity texture given shapes of 3D assets, which has been relatively less explored compared with generic 3D shape modeling. Our goal is to facilitate a controllable texture generation process, such that one texture code can correspond to a particular appearance style independent of any input shapes from a category. We introduce Texture UV Radiance Fields (TUVF) that generate textures in a learnable UV sphere space rather than directly on the 3D shape. This allows the texture to be disentangled from the underlying shape and transferable to other shapes that share the same UV space, i.e., from the same category. We integrate the UV sphere space with the radiance field, which provides a more efficient and accurate representation of textures than traditional texture maps. We perform our experiments on real-world object datasets where we achieve not only realistic synthesis but also substantial improvements over state-of-the-arts on texture controlling and editing. Project Page: https://www.anjiecheng.me/TUVF},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Cheng, An-Chieh and Li, Xueting and Liu, Sifei and Wang, Xiaolong},
	month = may,
	year = {2023},
	note = {arXiv:2305.03040 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hu_trivol_2023,
	title = {{TriVol}: {Point} {Cloud} {Rendering} via {Triple} {Volumes}},
	shorttitle = {{TriVol}},
	url = {http://arxiv.org/abs/2303.16485},
	doi = {10.48550/arXiv.2303.16485},
	abstract = {Existing learning-based methods for point cloud rendering adopt various 3D representations and feature querying mechanisms to alleviate the sparsity problem of point clouds. However, artifacts still appear in rendered images, due to the challenges in extracting continuous and discriminative 3D features from point clouds. In this paper, we present a dense while lightweight 3D representation, named TriVol, that can be combined with NeRF to render photo-realistic images from point clouds. Our TriVol consists of triple slim volumes, each of which is encoded from the point cloud. TriVol has two advantages. First, it fuses respective fields at different scales and thus extracts local and non-local features for discriminative representation. Second, since the volume size is greatly reduced, our 3D decoder can be efficiently inferred, allowing us to increase the resolution of the 3D space to render more point details. Extensive experiments on different benchmarks with varying kinds of scenes/objects demonstrate our framework's effectiveness compared with current approaches. Moreover, our framework has excellent generalization ability to render a category of scenes/objects without fine-tuning.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Hu, Tao and Xu, Xiaogang and Chu, Ruihang and Jia, Jiaya},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16485 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_urbangiraffe_2023,
	title = {{UrbanGIRAFFE}: {Representing} {Urban} {Scenes} as {Compositional} {Generative} {Neural} {Feature} {Fields}},
	shorttitle = {{UrbanGIRAFFE}},
	url = {http://arxiv.org/abs/2303.14167},
	doi = {10.48550/arXiv.2303.14167},
	abstract = {Generating photorealistic images with controllable camera pose and scene contents is essential for many applications including AR/VR and simulation. Despite the fact that rapid progress has been made in 3D-aware generative models, most existing methods focus on object-centric images and are not applicable to generating urban scenes for free camera viewpoint control and scene editing. To address this challenging task, we propose UrbanGIRAFFE, which uses a coarse 3D panoptic prior, including the layout distribution of uncountable stuff and countable objects, to guide a 3D-aware generative model. Our model is compositional and controllable as it breaks down the scene into stuff, objects, and sky. Using stuff prior in the form of semantic voxel grids, we build a conditioned stuff generator that effectively incorporates the coarse semantic and geometry information. The object layout prior further allows us to learn an object generator from cluttered scenes. With proper loss functions, our approach facilitates photorealistic 3D-aware image synthesis with diverse controllability, including large camera movement, stuff editing, and object manipulation. We validate the effectiveness of our model on both synthetic and real-world datasets, including the challenging KITTI-360 dataset.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Yang, Yuanbo and Yang, Yifei and Guo, Hanlei and Xiong, Rong and Wang, Yue and Liao, Yiyi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14167 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_uncertainty-driven_2023,
	title = {Uncertainty-{Driven} {Dense} {Two}-{View} {Structure} from {Motion}},
	url = {http://arxiv.org/abs/2302.00523},
	doi = {10.48550/arXiv.2302.00523},
	abstract = {This work introduces an effective and practical solution to the dense two-view structure from motion (SfM) problem. One vital question addressed is how to mindfully use per-pixel optical flow correspondence between two frames for accurate pose estimation -- as perfect per-pixel correspondence between two images is difficult, if not impossible, to establish. With the carefully estimated camera pose and predicted per-pixel optical flow correspondences, a dense depth of the scene is computed. Later, an iterative refinement procedure is introduced to further improve optical flow matching confidence, camera pose, and depth, exploiting their inherent dependency in rigid SfM. The fundamental idea presented is to benefit from per-pixel uncertainty in the optical flow estimation and provide robustness to the dense SfM system via an online refinement. Concretely, we introduce a pipeline consisting of (i) an uncertainty-aware dense optical flow estimation approach that provides per-pixel correspondence with their confidence score of matching; (ii) a weighted dense bundle adjustment formulation that depends on optical flow uncertainty and bidirectional optical flow consistency to refine both pose and depth; (iii) a depth estimation network that considers its consistency with the estimated poses and optical flow respecting epipolar constraint. Extensive experiments show that the proposed approach achieves remarkable depth accuracy and state-of-the-art camera pose results superseding SuperPoint and SuperGlue accuracy when tested on benchmark datasets such as DeMoN, YFCC100M, and ScanNet.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Chen, Weirong and Kumar, Suryansh and Yu, Fisher},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00523 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{parr_matrix_2018,
	title = {The {Matrix} {Calculus} {You} {Need} {For} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1802.01528},
	doi = {10.48550/arXiv.1802.01528},
	abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Parr, Terence and Howard, Jeremy},
	month = jul,
	year = {2018},
	note = {arXiv:1802.01528 [cs, stat]
version: 2},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{basri_convergence_2019,
	title = {The {Convergence} {Rate} of {Neural} {Networks} for {Learned} {Functions} of {Different} {Frequencies}},
	url = {http://arxiv.org/abs/1906.00425},
	doi = {10.48550/arXiv.1906.00425},
	abstract = {We study the relationship between the frequency of a function and the speed at which a neural network learns it. We build on recent results that show that the dynamics of overparameterized neural networks trained with gradient descent can be well approximated by a linear system. When normalized training data is uniformly distributed on a hypersphere, the eigenfunctions of this linear system are spherical harmonic functions. We derive the corresponding eigenvalues for each frequency after introducing a bias term in the model. This bias term had been omitted from the linear network model without significantly affecting previous theoretical results. However, we show theoretically and experimentally that a shallow neural network without bias cannot represent or learn simple, low frequency functions with odd frequencies. Our results lead to specific predictions of the time it will take a network to learn functions of varying frequency. These predictions match the empirical behavior of both shallow and deep networks.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Basri, Ronen and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
	month = dec,
	year = {2019},
	note = {arXiv:1906.00425 [cs, eess, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@misc{yang_swin3d_2023,
	title = {{Swin3D}: {A} {Pretrained} {Transformer} {Backbone} for {3D} {Indoor} {Scene} {Understanding}},
	shorttitle = {{Swin3D}},
	url = {http://arxiv.org/abs/2304.06906},
	abstract = {Pretrained backbones with fine-tuning have been widely adopted in 2D vision and natural language processing tasks and demonstrated significant advantages to task-specific networks. In this paper, we present a pretrained 3D backbone, named \{{\textbackslash}SST\}, which first outperforms all state-of-the-art methods in downstream 3D indoor scene understanding tasks. Our backbone network is based on a 3D Swin transformer and carefully designed to efficiently conduct self-attention on sparse voxels with linear memory complexity and capture the irregularity of point signals via generalized contextual relative positional embedding. Based on this backbone design, we pretrained a large \{{\textbackslash}SST\} model on a synthetic Structed3D dataset that is 10 times larger than the ScanNet dataset and fine-tuned the pretrained model in various downstream real-world indoor scene understanding tasks. The results demonstrate that our model pretrained on the synthetic dataset not only exhibits good generality in both downstream segmentation and detection on real 3D point datasets, but also surpasses the state-of-the-art methods on downstream tasks after fine-tuning with +2.3 mIoU and +2.2 mIoU on S3DIS Area5 and 6-fold semantic segmentation, +2.1 mIoU on ScanNet segmentation (val), +1.9 mAP@0.5 on ScanNet detection, +8.1 mAP@0.5 on S3DIS detection. Our method demonstrates the great potential of pretrained 3D backbones with fine-tuning for 3D understanding tasks. The code and models are available at https://github.com/microsoft/Swin3D .},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Yang, Yu-Qi and Guo, Yu-Xiao and Xiong, Jian-Yu and Liu, Yang and Pan, Hao and Wang, Peng-Shuai and Tong, Xin and Guo, Baining},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06906 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{nolte_stroke-based_2022,
	title = {Stroke-based {Rendering}: {From} {Heuristics} to {Deep} {Learning}},
	shorttitle = {Stroke-based {Rendering}},
	url = {http://arxiv.org/abs/2302.00595},
	doi = {10.48550/arXiv.2302.00595},
	abstract = {In the last few years, artistic image-making with deep learning models has gained a considerable amount of traction. A large number of these models operate directly in the pixel space and generate raster images. This is however not how most humans would produce artworks, for example, by planning a sequence of shapes and strokes to draw. Recent developments in deep learning methods help to bridge the gap between stroke-based paintings and pixel photo generation. With this survey, we aim to provide a structured introduction and understanding of common challenges and approaches in stroke-based rendering algorithms. These algorithms range from simple rule-based heuristics to stroke optimization and deep reinforcement agents, trained to paint images with differentiable vector graphics and neural rendering.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Nolte, Florian and Melnik, Andrew and Ritter, Helge},
	month = dec,
	year = {2022},
	note = {arXiv:2302.00595 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{shim_snerl_2023,
	title = {{SNeRL}: {Semantic}-aware {Neural} {Radiance} {Fields} for {Reinforcement} {Learning}},
	shorttitle = {{SNeRL}},
	url = {http://arxiv.org/abs/2301.11520},
	doi = {10.48550/arXiv.2301.11520},
	abstract = {As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Shim, Dongseok and Lee, Seungjae and Kim, H. Jin},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11520 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chen_single-stage_2023,
	title = {Single-{Stage} {Diffusion} {NeRF}: {A} {Unified} {Approach} to {3D} {Generation} and {Reconstruction}},
	shorttitle = {Single-{Stage} {Diffusion} {NeRF}},
	url = {http://arxiv.org/abs/2304.06714},
	doi = {10.48550/arXiv.2304.06714},
	abstract = {3D-aware image synthesis encompasses a variety of tasks, such as scene generation and novel view synthesis from images. Despite numerous task-specific methods, developing a comprehensive model remains challenging. In this paper, we present SSDNeRF, a unified approach that employs an expressive diffusion model to learn a generalizable prior of neural radiance fields (NeRF) from multi-view images of diverse objects. Previous studies have used two-stage approaches that rely on pretrained NeRFs as real data to train diffusion models. In contrast, we propose a new single-stage training paradigm with an end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent diffusion model, enabling simultaneous 3D reconstruction and prior learning, even from sparsely available views. At test time, we can directly sample the diffusion prior for unconditional generation, or combine it with arbitrary observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates robust results comparable to or better than leading task-specific methods in unconditional generation and single/sparse-view 3D reconstruction.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Chen, Hansheng and Gu, Jiatao and Chen, Anpei and Tian, Wei and Tu, Zhuowen and Liu, Lingjie and Su, Hao},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06714 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{guo_shadowformer_2023,
	title = {{ShadowFormer}: {Global} {Context} {Helps} {Image} {Shadow} {Removal}},
	shorttitle = {{ShadowFormer}},
	url = {http://arxiv.org/abs/2302.01650},
	doi = {10.48550/arXiv.2302.01650},
	abstract = {Recent deep learning methods have achieved promising results in image shadow removal. However, most of the existing approaches focus on working locally within shadow and non-shadow regions, resulting in severe artifacts around the shadow boundaries as well as inconsistent illumination between shadow and non-shadow regions. It is still challenging for the deep shadow removal model to exploit the global contextual correlation between shadow and non-shadow regions. In this work, we first propose a Retinex-based shadow model, from which we derive a novel transformer-based network, dubbed ShandowFormer, to exploit non-shadow regions to help shadow region restoration. A multi-scale channel attention framework is employed to hierarchically capture the global information. Based on that, we propose a Shadow-Interaction Module (SIM) with Shadow-Interaction Attention (SIA) in the bottleneck stage to effectively model the context correlation between shadow and non-shadow regions. We conduct extensive experiments on three popular public datasets, including ISTD, ISTD+, and SRD, to evaluate the proposed method. Our method achieves state-of-the-art performance by using up to 150X fewer model parameters.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Guo, Lanqing and Huang, Siyu and Liu, Ding and Cheng, Hao and Wen, Bihan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01650 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ren_scaling_2023,
	title = {Scaling {Forward} {Gradient} {With} {Local} {Losses}},
	url = {http://arxiv.org/abs/2210.03310},
	doi = {10.48550/arXiv.2210.03310},
	abstract = {Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.},
	urldate = {2023-04-30},
	publisher = {arXiv},
	author = {Ren, Mengye and Kornblith, Simon and Liao, Renjie and Hinton, Geoffrey},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03310 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{atzmon_sal_2020,
	title = {{SAL}: {Sign} {Agnostic} {Learning} of {Shapes} from {Raw} {Data}},
	shorttitle = {{SAL}},
	url = {http://arxiv.org/abs/1911.10414},
	doi = {10.48550/arXiv.1911.10414},
	abstract = {Recently, neural networks have been used as implicit representations for surface reconstruction, modelling, learning, and generation. So far, training neural networks to be implicit representations of surfaces required training data sampled from a ground-truth signed implicit functions such as signed distance or occupancy functions, which are notoriously hard to compute. In this paper we introduce Sign Agnostic Learning (SAL), a deep learning approach for learning implicit shape representations directly from raw, unsigned geometric data, such as point clouds and triangle soups. We have tested SAL on the challenging problem of surface reconstruction from an un-oriented point cloud, as well as end-to-end human shape space learning directly from raw scans dataset, and achieved state of the art reconstructions compared to current approaches. We believe SAL opens the door to many geometric deep learning applications with real-world data, alleviating the usual painstaking, often manual pre-process.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Atzmon, Matan and Lipman, Yaron},
	month = mar,
	year = {2020},
	note = {arXiv:1911.10414 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{huang_s4r_2023,
	title = {{S4R}: {Self}-{Supervised} {Semantic} {Scene} {Reconstruction} from {RGB}-{D} {Scans}},
	shorttitle = {{S4R}},
	url = {http://arxiv.org/abs/2302.03640},
	doi = {10.48550/arXiv.2302.03640},
	abstract = {Most deep learning approaches to comprehensive semantic modeling of 3D indoor spaces require costly dense annotations in the 3D domain. In this work, we explore a central 3D scene modeling task, namely, semantic scene reconstruction, using a fully self-supervised approach. To this end, we design a trainable model that employs both incomplete 3D reconstructions and their corresponding source RGB-D images, fusing cross-domain features into volumetric embeddings to predict complete 3D geometry, color, and semantics. Our key technical innovation is to leverage differentiable rendering of color and semantics, using the observed RGB images and a generic semantic segmentation model as color and semantics supervision, respectively. We additionally develop a method to synthesize an augmented set of virtual training views complementing the original real captures, enabling more efficient self-supervision for semantics. In this work we propose an end-to-end trainable solution jointly addressing geometry completion, colorization, and semantic mapping from a few RGB-D images, without 3D or 2D ground-truth. Our method is the first, to our knowledge, fully self-supervised method addressing completion and semantic segmentation of real-world 3D scans. It performs comparably well with the 3D supervised baselines, surpasses baselines with 2D supervision on real datasets, and generalizes well to unseen scenes.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Huang, Junwen and Artemorv, Alexey and Chen, Yujin and Zhi, Shuaifeng and Xu, Kai and Niessner, Matthias},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03640 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@phdthesis{schonberger_robust_2018,
	type = {Doctoral {Thesis}},
	title = {Robust {Methods} for {Accurate} and {Efficient} {3D} {Modeling} from {Unstructured} {Imagery}},
	copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
	url = {https://www.research-collection.ethz.ch/handle/20.500.11850/295763},
	language = {en},
	urldate = {2023-02-07},
	school = {ETH Zurich},
	author = {Schönberger, Johannes L.},
	year = {2018},
	doi = {10.3929/ethz-b-000295763},
	note = {Accepted: 2018-10-15T06:22:19Z},
	keywords = {/unread},
}

@misc{jin_reliable_2023,
	title = {Reliable {Image} {Dehazing} by {NeRF}},
	url = {http://arxiv.org/abs/2303.09153},
	doi = {10.48550/arXiv.2303.09153},
	abstract = {We present an image dehazing algorithm with high quality, wide application, and no data training or prior needed. We analyze the defects of the original dehazing model, and propose a new and reliable dehazing reconstruction and dehazing model based on the combination of optical scattering model and computer graphics lighting rendering model. Based on the new haze model and the images obtained by the cameras, we can reconstruct the three-dimensional space, accurately calculate the objects and haze in the space, and use the transparency relationship of haze to perform accurate haze removal. To obtain a 3D simulation dataset we used the Unreal 5 computer graphics rendering engine. In order to obtain real shot data in different scenes, we used fog generators, array cameras, mobile phones, underwater cameras and drones to obtain haze data. We use formula derivation, simulation data set and real shot data set result experimental results to prove the feasibility of the new method. Compared with various other methods, we are far ahead in terms of calculation indicators (4 dB higher quality average scene), color remains more natural, and the algorithm is more robust in different scenarios and best in the subjective perception.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Jin, Zheyan and Chen, Shiqi and Feng, Huajun and Xu, Zhihai and Li, Qi and Chen, Yueting},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09153 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jin_reliable_2023-1,
	title = {Reliable {Image} {Dehazing} by {NeRF}},
	url = {http://arxiv.org/abs/2303.09153},
	doi = {10.48550/arXiv.2303.09153},
	abstract = {We present an image dehazing algorithm with high quality, wide application, and no data training or prior needed. We analyze the defects of the original dehazing model, and propose a new and reliable dehazing reconstruction and dehazing model based on the combination of optical scattering model and computer graphics lighting rendering model. Based on the new haze model and the images obtained by the cameras, we can reconstruct the three-dimensional space, accurately calculate the objects and haze in the space, and use the transparency relationship of haze to perform accurate haze removal. To obtain a 3D simulation dataset we used the Unreal 5 computer graphics rendering engine. In order to obtain real shot data in different scenes, we used fog generators, array cameras, mobile phones, underwater cameras and drones to obtain haze data. We use formula derivation, simulation data set and real shot data set result experimental results to prove the feasibility of the new method. Compared with various other methods, we are far ahead in terms of calculation indicators (4 dB higher quality average scene), color remains more natural, and the algorithm is more robust in different scenarios and best in the subjective perception.},
	urldate = {2023-03-17},
	publisher = {arXiv},
	author = {Jin, Zheyan and Chen, Shiqi and Feng, Huajun and Xu, Zhihai and Li, Qi and Chen, Yueting},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09153 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{song_rethinking_2022,
	title = {Rethinking {Performance} {Gains} in {Image} {Dehazing} {Networks}},
	url = {http://arxiv.org/abs/2209.11448},
	abstract = {Image dehazing is an active topic in low-level vision, and many image dehazing networks have been proposed with the rapid development of deep learning. Although these networks' pipelines work fine, the key mechanism to improving image dehazing performance remains unclear. For this reason, we do not target to propose a dehazing network with fancy modules; rather, we make minimal modifications to popular U-Net to obtain a compact dehazing network. Specifically, we swap out the convolutional blocks in U-Net for residual blocks with the gating mechanism, fuse the feature maps of main paths and skip connections using the selective kernel, and call the resulting U-Net variant gUNet. As a result, with a significantly reduced overhead, gUNet is superior to state-of-the-art methods on multiple image dehazing datasets. Finally, we verify these key designs to the performance gain of image dehazing networks through extensive ablation studies.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Song, Yuda and Zhou, Yang and Qian, Hui and Du, Xin},
	month = sep,
	year = {2022},
	note = {arXiv:2209.11448 [cs]
version: 1},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zheng_revisiting_2023,
	title = {Revisiting {Discriminative} vs. {Generative} {Classifiers}: {Theory} and {Implications}},
	shorttitle = {Revisiting {Discriminative} vs. {Generative} {Classifiers}},
	url = {http://arxiv.org/abs/2302.02334},
	doi = {10.48550/arXiv.2302.02334},
	abstract = {A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires \$O({\textbackslash}log n)\$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires \$O(n)\$ samples, where \$n\$ is the feature dimension. To establish it, we present a multiclass \${\textbackslash}mathcal\{H\}\$-consistency bound framework and an explicit bound for logistic loss, which are of independent interests. Simulation results on a mixture of Gaussian validate our theoretical findings. Experiments on various pre-trained deep vision models show that naive Bayes consistently converges faster as the number of data increases. Besides, naive Bayes shows promise in few-shot cases and we observe the ``two regimes'' phenomenon in pre-trained supervised models. Our code is available at https://github.com/ML-GSAI/Revisiting-Dis-vs-Gen-Classifiers.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Zheng, Chenyu and Wu, Guoqiang and Bao, Fan and Cao, Yue and Li, Chongxuan and Zhu, Jun},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02334 [cs, stat]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{heo_robust_2023,
	title = {Robust {Camera} {Pose} {Refinement} for {Multi}-{Resolution} {Hash} {Encoding}},
	url = {http://arxiv.org/abs/2302.01571},
	doi = {10.48550/arXiv.2302.01571},
	abstract = {Multi-resolution hash encoding has recently been proposed to reduce the computational cost of neural renderings, such as NeRF. This method requires accurate camera poses for the neural renderings of given scenes. However, contrary to previous methods jointly optimizing camera poses and 3D scenes, the naive gradient-based camera pose refinement method using multi-resolution hash encoding severely deteriorates performance. We propose a joint optimization algorithm to calibrate the camera pose and learn a geometric representation using efficient multi-resolution hash encoding. Showing that the oscillating gradient flows of hash encoding interfere with the registration of camera poses, our method addresses the issue by utilizing smooth interpolation weighting to stabilize the gradient oscillation for the ray samplings across hash grids. Moreover, the curriculum training procedure helps to learn the level-wise hash encoding, further increasing the pose refinement. Experiments on the novel-view synthesis datasets validate that our learning frameworks achieve state-of-the-art performance and rapid convergence of neural rendering, even when initial camera poses are unknown.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Heo, Hwan and Kim, Taekyung and Lee, Jiyoung and Lee, Jaewon and Kim, Soohyun and Kim, Hyunwoo J. and Kim, Jin-Hwa},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01571 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{verbin_ref-nerf_2021,
	title = {Ref-{NeRF}: {Structured} {View}-{Dependent} {Appearance} for {Neural} {Radiance} {Fields}},
	shorttitle = {Ref-{NeRF}},
	url = {http://arxiv.org/abs/2112.03907},
	doi = {10.48550/arXiv.2112.03907},
	abstract = {Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Verbin, Dor and Hedman, Peter and Mildenhall, Ben and Zickler, Todd and Barron, Jonathan T. and Srinivasan, Pratul P.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.03907 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{mirzaei_reference-guided_2023,
	title = {Reference-guided {Controllable} {Inpainting} of {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2304.09677},
	doi = {10.48550/arXiv.2304.09677},
	abstract = {The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led to a desire for NeRF editing tools. Here, we focus on inpainting regions in a view-consistent and controllable manner. In addition to the typical NeRF inputs and masks delineating the unwanted region in each view, we require only a single inpainted view of the scene, i.e., a reference view. We use monocular depth estimators to back-project the inpainted view to the correct 3D positions. Then, via a novel rendering technique, a bilateral solver can construct view-dependent effects in non-reference views, making the inpainted region appear consistent from any view. For non-reference disoccluded regions, which cannot be supervised by the single reference view, we devise a method based on image inpainters to guide both the geometry and appearance. Our approach shows superior performance to NeRF inpainting baselines, with the additional advantage that a user can control the generated scene via a single inpainted image. Project page: https://ashmrz.github.io/reference-guided-3d},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Mirzaei, Ashkan and Aumentado-Armstrong, Tristan and Brubaker, Marcus A. and Kelly, Jonathan and Levinshtein, Alex and Derpanis, Konstantinos G. and Gilitschenski, Igor},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09677 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tinchev_real-time_2023,
	title = {Real-time {LIDAR} localization in natural and urban environments},
	url = {http://arxiv.org/abs/2301.13583},
	doi = {10.48550/arXiv.2301.13583},
	abstract = {Localization is a key challenge in many robotics applications. In this work we explore LIDAR-based global localization in both urban and natural environments and develop a method suitable for online application. Our approach leverages efficient deep learning architecture capable of learning compact point cloud descriptors directly from 3D data. The method uses an efficient feature space representation of a set of segmented point clouds to match between the current scene and the prior map. We show that down-sampling in the inner layers of the network can significantly reduce computation time without sacrificing performance. We present substantial evaluation of LIDAR-based global localization methods on nine scenarios from six datasets varying between urban, park, forest, and industrial environments. Part of which includes post-processed data from 30 sequences of the Oxford RobotCar dataset, which we make publicly available. Our experiments demonstrate a factor of three reduction of computation, 70\% lower memory consumption with marginal loss in localization frequency. The proposed method allows the full pipeline to run on robots with limited computation payload such as drones, quadrupeds, and UGVs as it does not require a GPU at run time.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Tinchev, Georgi and Penate-Sanchez, Adrian and Fallon, Maurice},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13583 [cs]},
	keywords = {/unread, Computer Science - Robotics},
}

@misc{li_read_2022,
	title = {{READ}: {Large}-{Scale} {Neural} {Scene} {Rendering} for {Autonomous} {Driving}},
	shorttitle = {{READ}},
	url = {http://arxiv.org/abs/2205.05509},
	doi = {10.48550/arXiv.2205.05509},
	abstract = {Synthesizing free-view photo-realistic images is an important task in multimedia. With the development of advanced driver assistance systems{\textasciitilde}(ADAS) and their applications in autonomous vehicles, experimenting with different scenarios becomes a challenge. Although the photo-realistic street scenes can be synthesized by image-to-image translation methods, which cannot produce coherent scenes due to the lack of 3D information. In this paper, a large-scale neural rendering method is proposed to synthesize the autonomous driving scene{\textasciitilde}(READ), which makes it possible to synthesize large-scale driving scenarios on a PC through a variety of sampling schemes. In order to represent driving scenarios, we propose an \{{\textbackslash}omega\} rendering network to learn neural descriptors from sparse point clouds. Our model can not only synthesize realistic driving scenes but also stitch and edit driving scenes. Experiments show that our model performs well in large-scale driving scenarios.},
	urldate = {2022-12-11},
	publisher = {arXiv},
	author = {Li, Zhuopeng and Li, Lu and Ma, Zeyu and Zhang, Ping and Chen, Junbo and Zhu, Jianke},
	month = may,
	year = {2022},
	note = {arXiv:2205.05509 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zheng_pointavatar_2023,
	title = {{PointAvatar}: {Deformable} {Point}-based {Head} {Avatars} from {Videos}},
	shorttitle = {{PointAvatar}},
	url = {http://arxiv.org/abs/2212.08377},
	doi = {10.48550/arXiv.2212.08377},
	abstract = {The ability to create realistic, animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting in the color estimation, thus they are limited in re-rendering the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Zheng, Yufeng and Yifan, Wang and Wetzstein, Gordon and Black, Michael J. and Hilliges, Otmar},
	month = feb,
	year = {2023},
	note = {arXiv:2212.08377 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{chang_pointersect_2023,
	title = {Pointersect: {Neural} {Rendering} with {Cloud}-{Ray} {Intersection}},
	shorttitle = {Pointersect},
	url = {http://arxiv.org/abs/2304.12390},
	doi = {10.48550/arXiv.2304.12390},
	abstract = {We propose a novel method that renders point clouds as if they are surfaces. The proposed method is differentiable and requires no scene-specific optimization. This unique capability enables, out-of-the-box, surface normal estimation, rendering room-scale point clouds, inverse rendering, and ray tracing with global illumination. Unlike existing work that focuses on converting point clouds to other representations--e.g., surfaces or implicit functions--our key idea is to directly infer the intersection of a light ray with the underlying surface represented by the given point cloud. Specifically, we train a set transformer that, given a small number of local neighbor points along a light ray, provides the intersection point, the surface normal, and the material blending weights, which are used to render the outcome of this light ray. Localizing the problem into small neighborhoods enables us to train a model with only 48 meshes and apply it to unseen point clouds. Our model achieves higher estimation accuracy than state-of-the-art surface reconstruction and point-cloud rendering methods on three test sets. When applied to room-scale point clouds, without any scene-specific optimization, the model achieves competitive quality with the state-of-the-art novel-view rendering methods. Moreover, we demonstrate ability to render and manipulate Lidar-scanned point clouds such as lighting control and object insertion.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Chang, Jen-Hao Rick and Chen, Wei-Yu and Ranjan, Anurag and Yi, Kwang Moo and Tuzel, Oncel},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12390 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{hu_point2pix_2023,
	title = {{Point2Pix}: {Photo}-{Realistic} {Point} {Cloud} {Rendering} via {Neural} {Radiance} {Fields}},
	shorttitle = {{Point2Pix}},
	url = {http://arxiv.org/abs/2303.16482},
	doi = {10.48550/arXiv.2303.16482},
	abstract = {Synthesizing photo-realistic images from a point cloud is challenging because of the sparsity of point cloud representation. Recent Neural Radiance Fields and extensions are proposed to synthesize realistic images from 2D input. In this paper, we present Point2Pix as a novel point renderer to link the 3D sparse point clouds with 2D dense image pixels. Taking advantage of the point cloud 3D prior and NeRF rendering pipeline, our method can synthesize high-quality images from colored point clouds, generally for novel indoor scenes. To improve the efficiency of ray sampling, we propose point-guided sampling, which focuses on valid samples. Also, we present Point Encoding to build Multi-scale Radiance Fields that provide discriminative 3D point features. Finally, we propose Fusion Encoding to efficiently synthesize high-quality images. Extensive experiments on the ScanNet and ArkitScenes datasets demonstrate the effectiveness and generalization.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Hu, Tao and Xu, Xiaogang and Liu, Shu and Jia, Jiaya},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16482 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_polynomial_2023,
	title = {Polynomial {Neural} {Fields} for {Subband} {Decomposition} and {Manipulation}},
	url = {http://arxiv.org/abs/2302.04862},
	doi = {10.48550/arXiv.2302.04862},
	abstract = {Neural fields have emerged as a new paradigm for representing signals, thanks to their ability to do it compactly while being easy to optimize. In most applications, however, neural fields are treated like black boxes, which precludes many signal manipulation tasks. In this paper, we propose a new class of neural fields called polynomial neural fields (PNFs). The key advantage of a PNF is that it can represent a signal as a composition of a number of manipulable and interpretable components without losing the merits of neural fields representation. We develop a general theoretical framework to analyze and design PNFs. We use this framework to design Fourier PNFs, which match state-of-the-art performance in signal representation tasks that use neural fields. In addition, we empirically demonstrate that Fourier PNFs enable signal manipulation applications such as texture transfer and scale-space interpolation. Code is available at https://github.com/stevenygd/PNF.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Yang, Guandao and Benaim, Sagie and Jampani, Varun and Genova, Kyle and Barron, Jonathan T. and Funkhouser, Thomas and Hariharan, Bharath and Belongie, Serge},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04862 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{anonymous_pmb_2022,
	title = {{PMB}: {Compositional} {Attribute}-object {Understanding} with {Pronouns}},
	copyright = {All rights reserved},
	shorttitle = {{PMB}},
	url = {https://openreview.net/forum?id=9RlvVqBDBy&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dthecvf.com%2FCVPR%2F2023%2FConference%2FAuthors%23your-submissions)},
	abstract = {Deep neural networks, as highly non-linear end-to-end models, still struggle to recognize compositional attribute-object pairs in a zero-shot manner. State-of-the-art methods leverage pre-trained language models to generate regression targets, so that the embeddings are better anchored in the feature space. However, we note that current text encoder outputs are not regularized thus may lose the rich structure. To this end, we introduce pronouns so that regression targets are augmented from adjectives (e.g., running) to adjective-pronoun pairs (e.g., running something). Meanwhile, we design a first-in-first-out memory bank for every and each attribute/object, which intrinsically regularizes the regression target. We evaluate our framework on three large-scale datasets MIT-States, UT-Zappos and VAW-CZSL and demonstrate clear improvements. Codes, data and models will be made publicly available.},
	language = {en},
	urldate = {2023-01-11},
	author = {Anonymous},
	month = nov,
	year = {2022},
	keywords = {/unread, ⛔ No DOI found},
}

@misc{weng_personnerf_2023,
	title = {{PersonNeRF}: {Personalized} {Reconstruction} from {Photo} {Collections}},
	shorttitle = {{PersonNeRF}},
	url = {http://arxiv.org/abs/2302.08504},
	doi = {10.48550/arXiv.2302.08504},
	abstract = {We present PersonNeRF, a method that takes a collection of photos of a subject (e.g. Roger Federer) captured across multiple years with arbitrary body poses and appearances, and enables rendering the subject with arbitrary novel combinations of viewpoint, body pose, and appearance. PersonNeRF builds a customized neural volumetric 3D model of the subject that is able to render an entire space spanned by camera viewpoint, body pose, and appearance. A central challenge in this task is dealing with sparse observations; a given body pose is likely only observed by a single viewpoint with a single appearance, and a given appearance is only observed under a handful of different body poses. We address this issue by recovering a canonical T-pose neural volumetric representation of the subject that allows for changing appearance across different observations, but uses a shared pose-dependent motion field across all observations. We demonstrate that this approach, along with regularization of the recovered volumetric geometry to encourage smoothness, is able to recover a model that renders compelling images from novel combinations of viewpoint, pose, and appearance from these challenging unstructured photo collections, outperforming prior work for free-viewpoint human rendering.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Weng, Chung-Yi and Srinivasan, Pratul P. and Curless, Brian and Kemelmacher-Shlizerman, Ira},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08504 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{gupta_patch_2023,
	title = {Patch {Gradient} {Descent}: {Training} {Neural} {Networks} on {Very} {Large} {Images}},
	shorttitle = {Patch {Gradient} {Descent}},
	url = {http://arxiv.org/abs/2301.13817},
	doi = {10.48550/arXiv.2301.13817},
	abstract = {Traditional CNN models are trained and tested on relatively low resolution images ({\textless}300 px), and cannot be directly operated on large-scale images due to compute and memory constraints. We propose Patch Gradient Descent (PatchGD), an effective learning strategy that allows to train the existing CNN architectures on large-scale images in an end-to-end manner. PatchGD is based on the hypothesis that instead of performing gradient-based updates on an entire image at once, it should be possible to achieve a good solution by performing model updates on only small parts of the image at a time, ensuring that the majority of it is covered over the course of iterations. PatchGD thus extensively enjoys better memory and compute efficiency when training models on large scale images. PatchGD is thoroughly evaluated on two datasets - PANDA and UltraMNIST with ResNet50 and MobileNetV2 models under different memory constraints. Our evaluation clearly shows that PatchGD is much more stable and efficient than the standard gradient-descent method in handling large images, and especially when the compute memory is limited.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Gupta, Deepak K. and Mago, Gowreesh and Chavan, Arnav and Prasad, Dilip K.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13817 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yin_or-nerf_2023,
	title = {{OR}-{NeRF}: {Object} {Removing} from {3D} {Scenes} {Guided} by {Multiview} {Segmentation} with {Neural} {Radiance} {Fields}},
	shorttitle = {{OR}-{NeRF}},
	url = {http://arxiv.org/abs/2305.10503},
	abstract = {The emergence of Neural Radiance Fields (NeRF) for novel view synthesis has led to increased interest in 3D scene editing. One important task in editing is removing objects from a scene while ensuring visual reasonability and multiview consistency. However, current methods face challenges such as time-consuming object labelling, limited capability to remove specific targets, and compromised rendering quality after removal. This paper proposes a novel object-removing pipeline, named OR-NeRF, that can remove objects from 3D scenes with either point or text prompts on a single view, achieving better performance in less time than previous works. Our method uses a points projection strategy to rapidly spread user annotations to all views, significantly reducing the processing burden. This algorithm allows us to leverage the recent 2D segmentation model Segment-Anything (SAM) to predict masks with improved precision and efficiency. Additionally, we obtain colour and depth priors through 2D inpainting methods. Finally, our algorithm employs depth supervision and perceptual loss for scene reconstruction to maintain consistency in geometry and appearance after object removal. Experimental results demonstrate that our method achieves better editing quality with less time than previous works, considering both quality and quantity.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Yin, Youtan and Fu, Zhoujie and Yang, Fan and Lin, Guosheng},
	month = may,
	year = {2023},
	note = {arXiv:2305.10503 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{rahaman_spectral_2019,
	title = {On the {Spectral} {Bias} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1806.08734},
	doi = {10.48550/arXiv.1806.08734},
	abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with \$100{\textbackslash}\%\$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets {\textbackslash}emph\{easier\} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A. and Bengio, Yoshua and Courville, Aaron},
	month = may,
	year = {2019},
	note = {arXiv:1806.08734 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{arora_exact_2019,
	title = {On {Exact} {Computation} with an {Infinitely} {Wide} {Neural} {Net}},
	url = {http://arxiv.org/abs/1904.11955},
	doi = {10.48550/arXiv.1904.11955},
	abstract = {How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its width --- namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers --- is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of infinite width. The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for the performance of a pure kernel-based method on CIFAR-10, being \$10{\textbackslash}\%\$ higher than the methods reported in [Novak et al., 2019], and only \$6{\textbackslash}\%\$ lower than the performance of the corresponding finite deep net architecture (once batch normalization, etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
	month = nov,
	year = {2019},
	note = {arXiv:1904.11955 [cs, stat]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{raymond_online_2023,
	title = {Online {Loss} {Function} {Learning}},
	url = {http://arxiv.org/abs/2301.13247},
	doi = {10.48550/arXiv.2301.13247},
	abstract = {Loss function learning is a new meta-learning paradigm that aims to automate the essential task of designing a loss function for a machine learning model. Existing techniques for loss function learning have shown promising results, often improving a model's training dynamics and final inference performance. However, a significant limitation of these techniques is that the loss functions are meta-learned in an offline fashion, where the meta-objective only considers the very first few steps of training, which is a significantly shorter time horizon than the one typically used for training deep neural networks. This causes significant bias towards loss functions that perform well at the very start of training but perform poorly at the end of training. To address this issue we propose a new loss function learning technique for adaptively updating the loss function online after each update to the base model parameters. The experimental results show that our proposed method consistently outperforms the cross-entropy loss and offline loss function learning techniques on a diverse range of neural network architectures and datasets.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Raymond, Christian and Chen, Qi and Xue, Bing and Zhang, Mengjie},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13247 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{gao_object-centric_2023,
	title = {Object-{Centric} {Voxelization} of {Dynamic} {Scenes} via {Inverse} {Neural} {Rendering}},
	url = {http://arxiv.org/abs/2305.00393},
	doi = {10.48550/arXiv.2305.00393},
	abstract = {Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its effectiveness on various benchmarks with multiple objects, diverse dynamics, and real-world shapes and textures. We present visualization at https://sites.google.com/view/dynavol-visual.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Gao, Siyu and Zhao, Yanpeng and Wang, Yunbo and Yang, Xiaokang},
	month = apr,
	year = {2023},
	note = {arXiv:2305.00393 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{xu_novel_2023,
	title = {Novel {Intensity} {Mapping} {Functions}: {Weighted} {Histogram} {Averaging}},
	shorttitle = {Novel {Intensity} {Mapping} {Functions}},
	url = {http://arxiv.org/abs/2111.07283},
	doi = {10.48550/arXiv.2111.07283},
	abstract = {It is challenging to align the brightness distribution of the images with different exposures due to possible color distortion and loss of details in the brightest and darkest regions of input images. In this paper, a novel intensity mapping algorithm is first proposed by introducing a new concept of weighted histogram averaging (WHA). The proposed WHA algorithm leverages the correspondence between the histogram bins of two images which are built up by using the non-decreasing property of the intensity mapping functions (IMFs). Extensive experiments indicate that the proposed WHA algorithm significantly surpasses the related state-of-the-art intensity mapping methods.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Xu, Yilun and Li, Zhengguo and Chen, Weihai and Wen, Changyun},
	month = jan,
	year = {2023},
	note = {arXiv:2111.07283 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jacot_neural_2020,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}},
	shorttitle = {Neural {Tangent} {Kernel}},
	url = {http://arxiv.org/abs/1806.07572},
	doi = {10.48550/arXiv.1806.07572},
	abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
	month = feb,
	year = {2020},
	note = {arXiv:1806.07572 [cs, math, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{hu_neural_2023,
	title = {Neural {Wavelet}-domain {Diffusion} for {3D} {Shape} {Generation}, {Inversion}, and {Manipulation}},
	url = {http://arxiv.org/abs/2302.00190},
	doi = {10.48550/arXiv.2302.00190},
	abstract = {This paper presents a new approach for 3D shape generation, inversion, and manipulation, through a direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets. Then, we design a pair of neural networks: a diffusion-based generator to produce diverse shapes in the form of the coarse coefficient volumes and a detail predictor to produce compatible detail coefficient volumes for introducing fine structures and details. Further, we may jointly train an encoder network to learn a latent space for inverting shapes, allowing us to enable a rich variety of whole-shape and region-aware shape manipulations. Both quantitative and qualitative experimental results manifest the compelling shape generation, inversion, and manipulation capabilities of our approach over the state-of-the-art methods.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Hu, Jingyu and Hui, Ka-Hei and Liu, Zhengzhe and Li, Ruihui and Fu, Chi-Wing},
	month = jan,
	year = {2023},
	note = {arXiv:2302.00190 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{debbagh_neural_2023,
	title = {Neural {Radiance} {Fields} ({NeRFs}): {A} {Review} and {Some} {Recent} {Developments}},
	shorttitle = {Neural {Radiance} {Fields} ({NeRFs})},
	url = {http://arxiv.org/abs/2305.00375},
	abstract = {Neural Radiance Field (NeRF) is a framework that represents a 3D scene in the weights of a fully connected neural network, known as the Multi-Layer Perception(MLP). The method was introduced for the task of novel view synthesis and is able to achieve state-of-the-art photorealistic image renderings from a given continuous viewpoint. NeRFs have become a popular field of research as recent developments have been made that expand the performance and capabilities of the base framework. Recent developments include methods that require less images to train the model for view synthesis as well as methods that are able to generate views from unconstrained and dynamic scene representations.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Debbagh, Mohamed},
	month = apr,
	year = {2023},
	note = {arXiv:2305.00375 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{ma_neural_2022,
	title = {Neural {Point}-based {Shape} {Modeling} of {Humans} in {Challenging} {Clothing}},
	url = {http://arxiv.org/abs/2209.06814},
	doi = {10.48550/arXiv.2209.06814},
	abstract = {Parametric 3D body models like SMPL only represent minimally-clothed people and are hard to extend to clothing because they have a fixed mesh topology and resolution. To address these limitations, recent work uses implicit surfaces or point clouds to model clothed bodies. While not limited by topology, such methods still struggle to model clothing that deviates significantly from the body, such as skirts and dresses. This is because they rely on the body to canonicalize the clothed surface by reposing it to a reference shape. Unfortunately, this process is poorly defined when clothing is far from the body. Additionally, they use linear blend skinning to pose the body and the skinning weights are tied to the underlying body parts. In contrast, we model the clothing deformation in a local coordinate space without canonicalization. We also relax the skinning weights to let multiple body parts influence the surface. Specifically, we extend point-based methods with a coarse stage, that replaces canonicalization with a learned pose-independent "coarse shape" that can capture the rough surface geometry of clothing like skirts. We then refine this using a network that infers the linear blend skinning weights and pose dependent displacements from the coarse representation. The approach works well for garments that both conform to, and deviate from, the body. We demonstrate the usefulness of our approach by learning person-specific avatars from examples and then show how they can be animated in new poses and motions. We also show that the method can learn directly from raw scans with missing data, greatly simplifying the process of creating realistic avatars. Code is available for research purposes at \{{\textbackslash}small{\textbackslash}url\{https://qianlim.github.io/SkiRT\}\}.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Ma, Qianli and Yang, Jinlong and Black, Michael J. and Tang, Siyu},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06814 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{gao_nerf_2023,
	title = {{NeRF}: {Neural} {Radiance} {Field} in {3D} {Vision}, {A} {Comprehensive} {Review}},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2210.00379},
	abstract = {Neural Radiance Field (NeRF), a new novel view synthesis with implicit scene representation has taken the field of Computer Vision by storm. As a novel view synthesis and 3D reconstruction method, NeRF models find applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. Since the original paper by Mildenhall et al., more than 250 preprints were published, with more than 100 eventually being accepted in tier one Computer Vision Conferences. Given NeRF popularity and the current interest in this research area, we believe it necessary to compile a comprehensive survey of NeRF papers from the past two years, which we organized into both architecture, and application based taxonomies. We also provide an introduction to the theory of NeRF based novel view synthesis, and a benchmark comparison of the performance and speed of key NeRF models. By creating this survey, we hope to introduce new researchers to NeRF, provide a helpful reference for influential works in this field, as well as motivate future research directions with our discussion section.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Gao, Kyle and Gao, Yina and He, Hongjie and Lu, Dening and Xu, Linlin and Li, Jonathan},
	month = may,
	year = {2023},
	note = {arXiv:2210.00379 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, I.4},
}

@misc{wang_nerf--_2022,
	title = {{NeRF}--: {Neural} {Radiance} {Fields} {Without} {Known} {Camera} {Parameters}},
	shorttitle = {{NeRF}--},
	url = {http://arxiv.org/abs/2102.07064},
	doi = {10.48550/arXiv.2102.07064},
	abstract = {Considering the problem of novel view synthesis (NVS) from only a set of 2D images, we simplify the training process of Neural Radiance Field (NeRF) on forward-facing scenes by removing the requirement of known or pre-computed camera parameters, including both intrinsics and 6DoF poses. To this end, we propose NeRF\$--\$, with three contributions: First, we show that the camera parameters can be jointly optimised as learnable parameters with NeRF training, through a photometric reconstruction; Second, to benchmark the camera parameter estimation and the quality of novel view renderings, we introduce a new dataset of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset (BLEFF); Third, we conduct extensive analyses to understand the training behaviours under various camera motions, and show that in most scenarios, the joint optimisation pipeline can recover accurate camera parameters and achieve comparable novel view synthesis quality as those trained with COLMAP pre-computed camera parameters. Our code and data are available at https://nerfmm.active.vision.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Wang, Zirui and Wu, Shangzhe and Xie, Weidi and Chen, Min and Prisacariu, Victor Adrian},
	month = apr,
	year = {2022},
	note = {arXiv:2102.07064 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_nerf-loc_2023,
	title = {{NeRF}-{Loc}: {Visual} {Localization} with {Conditional} {Neural} {Radiance} {Field}},
	shorttitle = {{NeRF}-{Loc}},
	url = {http://arxiv.org/abs/2304.07979},
	doi = {10.48550/arXiv.2304.07979},
	abstract = {We propose a novel visual re-localization method based on direct matching between the implicit 3D descriptors and the 2D image with transformer. A conditional neural radiance field(NeRF) is chosen as the 3D scene representation in our pipeline, which supports continuous 3D descriptors generation and neural rendering. By unifying the feature matching and the scene coordinate regression to the same framework, our model learns both generalizable knowledge and scene prior respectively during two training stages. Furthermore, to improve the localization robustness when domain gap exists between training and testing phases, we propose an appearance adaptation layer to explicitly align styles between the 3D model and the query image. Experiments show that our method achieves higher localization accuracy than other learning-based approaches on multiple benchmarks. Code is available at {\textbackslash}url\{https://github.com/JenningsL/nerf-loc\}.},
	urldate = {2023-04-19},
	publisher = {arXiv},
	author = {Liu, Jianlin and Nie, Qiang and Liu, Yong and Wang, Chengjie},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07979 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sun_nerf-loc_2022,
	title = {{NeRF}-{Loc}: {Transformer}-{Based} {Object} {Localization} {Within} {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{Loc}},
	url = {http://arxiv.org/abs/2209.12068},
	doi = {10.48550/arXiv.2209.12068},
	abstract = {Neural Radiance Fields (NeRFs) have been successfully used for scene representation. Recent works have also developed robotic navigation and manipulation systems using NeRF-based environment representations. As object localization is the foundation for many robotic applications, to further unleash the potential of NeRFs in robotic systems, we study object localization within a NeRF scene. We propose a transformer-based framework NeRF-Loc to extract 3D bounding boxes of objects in NeRF scenes. NeRF-Loc takes a pre-trained NeRF model and camera view as input, and produces labeled 3D bounding boxes of objects as output. Concretely, we design a pair of paralleled transformer encoder branches, namely the coarse stream and the fine stream, to encode both the context and details of target objects. The encoded features are then fused together with attention layers to alleviate ambiguities for accurate object localization. We have compared our method with the conventional transformer-based method and our method achieves better performance. In addition, we also present the first NeRF samples-based object localization benchmark NeRFLocBench.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Sun, Jiankai and Xu, Yan and Ding, Mingyu and Yi, Hongwei and Wang, Jingdong and Zhang, Liangjun and Schwager, Mac},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12068 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{deng_nasa_2022,
	title = {{NASA}: {Neural} {Articulated} {Shape} {Approximation}},
	shorttitle = {{NASA}},
	url = {http://arxiv.org/abs/1912.03207},
	doi = {10.48550/arXiv.1912.03207},
	abstract = {Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Deng, Boyang and Lewis, J. P. and Jeruzalski, Timothy and Pons-Moll, Gerard and Hinton, Geoffrey and Norouzi, Mohammad and Tagliasacchi, Andrea},
	month = jul,
	year = {2022},
	note = {arXiv:1912.03207 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{yoshiyama_ndjir_2023,
	title = {{NDJIR}: {Neural} {Direct} and {Joint} {Inverse} {Rendering} for {Geometry}, {Lights}, and {Materials} of {Real} {Object}},
	shorttitle = {{NDJIR}},
	url = {http://arxiv.org/abs/2302.00675},
	doi = {10.48550/arXiv.2302.00675},
	abstract = {The goal of inverse rendering is to decompose geometry, lights, and materials given pose multi-view images. To achieve this goal, we propose neural direct and joint inverse rendering, NDJIR. Different from prior works which relies on some approximations of the rendering equation, NDJIR directly addresses the integrals in the rendering equation and jointly decomposes geometry: signed distance function, lights: environment and implicit lights, materials: base color, roughness, specular reflectance using the powerful and flexible volume rendering framework, voxel grid feature, and Bayesian prior. Our method directly uses the physically-based rendering, so we can seamlessly export an extracted mesh with materials to DCC tools and show material conversion examples. We perform intensive experiments to show that our proposed method can decompose semantically well for real object in photogrammetric setting and what factors contribute towards accurate inverse rendering.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Yoshiyama, Kazuki and Narihira, Takuya},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00675 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{zimny_multiplanenerf_2023,
	title = {{MultiPlaneNeRF}: {Neural} {Radiance} {Field} with {Non}-{Trainable} {Representation}},
	shorttitle = {{MultiPlaneNeRF}},
	url = {http://arxiv.org/abs/2305.10579},
	abstract = {NeRF is a popular model that efficiently represents 3D objects from 2D images. However, vanilla NeRF has a few important limitations. NeRF must be trained on each object separately. The training time is long since we encode the object's shape and color in neural network weights. Moreover, NeRF does not generalize well to unseen data. In this paper, we present MultiPlaneNeRF -- a first model that simultaneously solves all the above problems. Our model works directly on 2D images. We project 3D points on 2D images to produce non-trainable representations. The projection step is not parametrized, and a very shallow decoder can efficiently process the representation. Using existing images as part of NeRF can significantly reduce the number of parameters since we train only a small implicit decoder. Furthermore, we can train MultiPlaneNeRF on a large data set and force our implicit decoder to generalize across many objects. Consequently, we can only replace the 2D images (without additional training) to produce a NeRF representation of the new object. In the experimental section, we demonstrate that MultiPlaneNeRF achieves comparable results to state-of-the-art models for synthesizing new views and has generalization properties.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Zimny, Dominik and Tabor, Jacek and Zięba, Maciej and Spurek, Przemysław},
	month = may,
	year = {2023},
	note = {arXiv:2305.10579 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chang_mono-star_2023,
	title = {Mono-{STAR}: {Mono}-camera {Scene}-level {Tracking} and {Reconstruction}},
	shorttitle = {Mono-{STAR}},
	url = {http://arxiv.org/abs/2301.13244},
	doi = {10.48550/arXiv.2301.13244},
	abstract = {We present Mono-STAR, the first real-time 3D reconstruction system that simultaneously supports semantic fusion, fast motion tracking, non-rigid object deformation, and topological change under a unified framework. The proposed system solves a new optimization problem incorporating optical-flow-based 2D constraints to deal with fast motion and a novel semantic-aware deformation graph (SAD-graph) for handling topology change. We test the proposed system under various challenging scenes and demonstrate that it significantly outperforms existing state-of-the-art methods.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Chang, Haonan and Ramesh, Dhruv Metha and Geng, Shijie and Gan, Yuqiu and Boularias, Abdeslam},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13244 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{yuan_monocular_2023,
	title = {Monocular {Scene} {Reconstruction} with {3D} {SDF} {Transformers}},
	url = {http://arxiv.org/abs/2301.13510},
	doi = {10.48550/arXiv.2301.13510},
	abstract = {Monocular scene reconstruction from posed images is challenging due to the complexity of a large environment. Recent volumetric methods learn to directly predict the TSDF volume and have demonstrated promising results in this task. However, most methods focus on how to extract and fuse the 2D features to a 3D feature volume, but none of them improve the way how the 3D volume is aggregated. In this work, we propose an SDF transformer network, which replaces the role of 3D CNN for better 3D feature aggregation. To reduce the explosive computation complexity of the 3D multi-head attention, we propose a sparse window attention module, where the attention is only calculated between the non-empty voxels within a local window. Then a top-down-bottom-up 3D attention network is built for 3D feature aggregation, where a dilate-attention structure is proposed to prevent geometry degeneration, and two global modules are employed to equip with global receptive fields. The experiments on multiple datasets show that this 3D transformer network generates a more accurate and complete reconstruction, which outperforms previous methods by a large margin. Remarkably, the mesh accuracy is improved by 41.8\%, and the mesh completeness is improved by 25.3\% on the ScanNet dataset. Project page: https://weihaosky.github.io/sdfformer.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Yuan, Weihao and Gu, Xiaodong and Li, Heng and Dong, Zilong and Zhu, Siyu},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13510 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{song_moda_2023,
	title = {{MoDA}: {Modeling} {Deformable} {3D} {Objects} from {Casual} {Videos}},
	shorttitle = {{MoDA}},
	url = {http://arxiv.org/abs/2304.08279},
	doi = {10.48550/arXiv.2304.08279},
	abstract = {In this paper, we focus on the challenges of modeling deformable 3D objects from casual videos. With the popularity of neural radiance fields (NeRF), many works extend it to dynamic scenes with a canonical NeRF and a deformation model that achieves 3D point transformation between the observation space and the canonical space. Recent works rely on linear blend skinning (LBS) to achieve the canonical-observation transformation. However, the linearly weighted combination of rigid transformation matrices is not guaranteed to be rigid. As a matter of fact, unexpected scale and shear factors often appear. In practice, using LBS as the deformation model can always lead to skin-collapsing artifacts for bending or twisting motions. To solve this problem, we propose neural dual quaternion blend skinning (NeuDBS) to achieve 3D point deformation, which can perform rigid transformation without skin-collapsing artifacts. Besides, we introduce a texture filtering approach for texture rendering that effectively minimizes the impact of noisy colors outside target deformable objects. Extensive experiments on real and synthetic datasets show that our approach can reconstruct 3D models for humans and animals with better qualitative and quantitative performance than state-of-the-art methods.},
	urldate = {2023-04-19},
	publisher = {arXiv},
	author = {Song, Chaoyue and Chen, Tianyi and Chen, Yiwen and Wei, Jiacheng and Foo, Chuan Sheng and Liu, Fayao and Lin, Guosheng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08279 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{schirmer_modeling_2022,
	title = {Modeling {Irregular} {Time} {Series} with {Continuous} {Recurrent} {Units}},
	url = {http://arxiv.org/abs/2111.11344},
	doi = {10.48550/arXiv.2111.11344},
	abstract = {Recurrent neural networks (RNNs) are a popular choice for modeling sequential data. Modern RNN architectures assume constant time-intervals between observations. However, in many datasets (e.g. medical records) observation times are irregular and can carry important information. To address this challenge, we propose continuous recurrent units (CRUs) -- a neural architecture that can naturally handle irregular intervals between observations. The CRU assumes a hidden state, which evolves according to a linear stochastic differential equation and is integrated into an encoder-decoder framework. The recursive computations of the CRU can be derived using the continuous-discrete Kalman filter and are in closed form. The resulting recurrent architecture has temporal continuity between hidden states and a gating mechanism that can optimally integrate noisy observations. We derive an efficient parameterization scheme for the CRU that leads to a fast implementation f-CRU. We empirically study the CRU on a number of challenging datasets and find that it can interpolate irregular time series better than methods based on neural ordinary differential equations.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Schirmer, Mona and Eltayeb, Mazin and Lessmann, Stefan and Rudolph, Maja},
	month = jul,
	year = {2022},
	note = {arXiv:2111.11344 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Read Later, Statistics - Machine Learning, Time Series Modeling},
}

@misc{maggio_loc-nerf_2022,
	title = {Loc-{NeRF}: {Monte} {Carlo} {Localization} using {Neural} {Radiance} {Fields}},
	shorttitle = {Loc-{NeRF}},
	url = {http://arxiv.org/abs/2209.09050},
	doi = {10.48550/arXiv.2209.09050},
	abstract = {We present Loc-NeRF, a real-time vision-based robot localization approach that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our system uses a pre-trained NeRF model as the map of an environment and can localize itself in real-time using an RGB camera as the only exteroceptive sensor onboard the robot. While neural radiance fields have seen significant applications for visual rendering in computer vision and graphics, they have found limited use in robotics. Existing approaches for NeRF-based localization require both a good initial pose guess and significant computation, making them impractical for real-time robotics applications. By using Monte Carlo localization as a workhorse to estimate poses using a NeRF map model, Loc-NeRF is able to perform localization faster than the state of the art and without relying on an initial pose estimate. In addition to testing on synthetic data, we also run our system using real data collected by a Clearpath Jackal UGV and demonstrate for the first time the ability to perform real-time global localization with neural radiance fields. We make our code publicly available at https://github.com/MIT-SPARK/Loc-NeRF.},
	urldate = {2023-03-26},
	publisher = {arXiv},
	author = {Maggio, Dominic and Abate, Marcus and Shi, Jingnan and Mario, Courtney and Carlone, Luca},
	month = sep,
	year = {2022},
	note = {arXiv:2209.09050 [cs]},
	keywords = {/unread, Computer Science - Robotics},
}

@misc{chen_local--global_2023,
	title = {Local-to-{Global} {Registration} for {Bundle}-{Adjusting} {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2211.11505},
	doi = {10.48550/arXiv.2211.11505},
	abstract = {Neural Radiance Fields (NeRF) have achieved photorealistic novel views synthesis; however, the requirement of accurate camera poses limits its application. Despite analysis-by-synthesis extensions for jointly learning neural 3D representations and registering camera frames exist, they are susceptible to suboptimal solutions if poorly initialized. We propose L2G-NeRF, a Local-to-Global registration method for bundle-adjusting Neural Radiance Fields: first, a pixel-wise flexible alignment, followed by a frame-wise constrained parametric alignment. Pixel-wise local alignment is learned in an unsupervised way via a deep network which optimizes photometric reconstruction errors. Frame-wise global alignment is performed using differentiable parameter estimation solvers on the pixel-wise correspondences to find a global transformation. Experiments on synthetic and real-world data show that our method outperforms the current state-of-the-art in terms of high-fidelity reconstruction and resolving large camera pose misalignment. Our module is an easy-to-use plugin that can be applied to NeRF variants and other neural field applications. The Code and supplementary materials are available at https://rover-xingyu.github.io/L2G-NeRF/.},
	urldate = {2023-03-10},
	publisher = {arXiv},
	author = {Chen, Yue and Chen, Xingyu and Wang, Xuan and Zhang, Qi and Guo, Yu and Shan, Ying and Wang, Fei},
	month = mar,
	year = {2023},
	note = {arXiv:2211.11505 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chun_local_2023,
	title = {Local {Neural} {Descriptor} {Fields}: {Locally} {Conditioned} {Object} {Representations} for {Manipulation}},
	shorttitle = {Local {Neural} {Descriptor} {Fields}},
	url = {http://arxiv.org/abs/2302.03573},
	doi = {10.48550/arXiv.2302.03573},
	abstract = {A robot operating in a household environment will see a wide range of unique and unfamiliar objects. While a system could train on many of these, it is infeasible to predict all the objects a robot will see. In this paper, we present a method to generalize object manipulation skills acquired from a limited number of demonstrations, to novel objects from unseen shape categories. Our approach, Local Neural Descriptor Fields (L-NDF), utilizes neural descriptors defined on the local geometry of the object to effectively transfer manipulation demonstrations to novel objects at test time. In doing so, we leverage the local geometry shared between objects to produce a more general manipulation framework. We illustrate the efficacy of our approach in manipulating novel objects in novel poses -- both in simulation and in the real world.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Chun, Ethan and Du, Yilun and Simeonov, Anthony and Lozano-Perez, Tomas and Kaelbling, Leslie},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03573 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yang_learning_2021,
	title = {Learning {Object}-{Compositional} {Neural} {Radiance} {Field} for {Editable} {Scene} {Rendering}},
	url = {http://arxiv.org/abs/2109.01847},
	doi = {10.48550/arXiv.2109.01847},
	abstract = {Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Yang, Bangbang and Zhang, Yinda and Xu, Yinghao and Li, Yijin and Zhou, Han and Bao, Hujun and Zhang, Guofeng and Cui, Zhaopeng},
	month = sep,
	year = {2021},
	note = {arXiv:2109.01847 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{driess_learning_2022,
	title = {Learning {Multi}-{Object} {Dynamics} with {Compositional} {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2202.11855},
	doi = {10.48550/arXiv.2202.11855},
	abstract = {We present a method to learn compositional multi-object dynamics models from image observations based on implicit object encoders, Neural Radiance Fields (NeRFs), and graph neural networks. NeRFs have become a popular choice for representing scenes due to their strong 3D prior. However, most NeRF approaches are trained on a single scene, representing the whole scene with a global model, making generalization to novel scenes, containing different numbers of objects, challenging. Instead, we present a compositional, object-centric auto-encoder framework that maps multiple views of the scene to a set of latent vectors representing each object separately. The latent vectors parameterize individual NeRFs from which the scene can be reconstructed. Based on those latent vectors, we train a graph neural network dynamics model in the latent space to achieve compositionality for dynamics prediction. A key feature of our approach is that the latent vectors are forced to encode 3D information through the NeRF decoder, which enables us to incorporate structural priors in learning the dynamics models, making long-term predictions more stable compared to several baselines. Simulated and real world experiments show that our method can model and learn the dynamics of compositional scenes including rigid and deformable objects. Video: https://dannydriess.github.io/compnerfdyn/},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Driess, Danny and Huang, Zhiao and Li, Yunzhu and Tedrake, Russ and Toussaint, Marc},
	month = jul,
	year = {2022},
	note = {arXiv:2202.11855 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{shi_learning_2023,
	title = {Learning {3D}-aware {Image} {Synthesis} with {Unknown} {Pose} {Distribution}},
	url = {http://arxiv.org/abs/2301.07702},
	doi = {10.48550/arXiv.2301.07702},
	abstract = {Existing methods for 3D-aware image synthesis largely depend on the 3D pose distribution pre-estimated on the training set. An inaccurate estimation may mislead the model into learning faulty geometry. This work proposes PoF3D that frees generative radiance fields from the requirements of 3D pose priors. We first equip the generator with an efficient pose learner, which is able to infer a pose from a latent code, to approximate the underlying true pose distribution automatically. We then assign the discriminator a task to learn pose distribution under the supervision of the generator and to differentiate real and synthesized images with the predicted pose as the condition. The pose-free generator and the pose-aware discriminator are jointly trained in an adversarial manner. Extensive results on a couple of datasets confirm that the performance of our approach, regarding both image quality and geometry quality, is on par with state of the art. To our best knowledge, PoF3D demonstrates the feasibility of learning high-quality 3D-aware image synthesis without using 3D pose priors for the first time.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Shi, Zifan and Shen, Yujun and Xu, Yinghao and Peng, Sida and Liao, Yiyi and Guo, Sheng and Chen, Qifeng and Yeung, Dit-Yan},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07702 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{moreno_laser_2023,
	title = {Laser: {Latent} {Set} {Representations} for {3D} {Generative} {Modeling}},
	shorttitle = {Laser},
	url = {http://arxiv.org/abs/2301.05747},
	doi = {10.48550/arXiv.2301.05747},
	abstract = {NeRF provides unparalleled fidelity of novel view synthesis: rendering a 3D scene from an arbitrary viewpoint. NeRF requires training on a large number of views that fully cover a scene, which limits its applicability. While these issues can be addressed by learning a prior over scenes in various forms, previous approaches have been either applied to overly simple scenes or struggling to render unobserved parts. We introduce Laser-NV: a generative model which achieves high modelling capacity, and which is based on a set-valued latent representation modelled by normalizing flows. Similarly to previous amortized approaches, Laser-NV learns structure from multiple scenes and is capable of fast, feed-forward inference from few views. To encourage higher rendering fidelity and consistency with observed views, Laser-NV further incorporates a geometry-informed attention mechanism over the observed views. Laser-NV further produces diverse and plausible completions of occluded parts of a scene while remaining consistent with observations. Laser-NV shows state-of-the-art novel-view synthesis quality when evaluated on ShapeNet and on a novel simulated City dataset, which features high uncertainty in the unobserved regions of the scene.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Moreno, Pol and Kosiorek, Adam R. and Strathmann, Heiko and Zoran, Daniel and Schneider, Rosalia G. and Winckler, Björn and Markeeva, Larisa and Weber, Théophane and Rezende, Danilo J.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.05747 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liao_kitti-360_2022,
	title = {{KITTI}-360: {A} {Novel} {Dataset} and {Benchmarks} for {Urban} {Scene} {Understanding} in {2D} and {3D}},
	shorttitle = {{KITTI}-360},
	url = {http://arxiv.org/abs/2109.13410},
	doi = {10.48550/arXiv.2109.13410},
	abstract = {For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Liao, Yiyi and Xie, Jun and Geiger, Andreas},
	month = jun,
	year = {2022},
	note = {arXiv:2109.13410 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_inv_2023,
	title = {{INV}: {Towards} {Streaming} {Incremental} {Neural} {Videos}},
	shorttitle = {{INV}},
	url = {http://arxiv.org/abs/2302.01532},
	doi = {10.48550/arXiv.2302.01532},
	abstract = {Recent works in spatiotemporal radiance fields can produce photorealistic free-viewpoint videos. However, they are inherently unsuitable for interactive streaming scenarios (e.g. video conferencing, telepresence) because have an inevitable lag even if the training is instantaneous. This is because these approaches consume videos and thus have to buffer chunks of frames (often seconds) before processing. In this work, we take a step towards interactive streaming via a frame-by-frame approach naturally free of lag. Conventional wisdom believes that per-frame NeRFs are impractical due to prohibitive training costs and storage. We break this belief by introducing Incremental Neural Videos (INV), a per-frame NeRF that is efficiently trained and streamable. We designed INV based on two insights: (1) Our main finding is that MLPs naturally partition themselves into Structure and Color Layers, which store structural and color/texture information respectively. (2) We leverage this property to retain and improve upon knowledge from previous frames, thus amortizing training across frames and reducing redundant learning. As a result, with negligible changes to NeRF, INV can achieve good qualities ({\textgreater}28.6db) in 8min/frame. It can also outperform prior SOTA in 19\% less training time. Additionally, our Temporal Weight Compression reduces the per-frame size to 0.3MB/frame (6.6\% of NeRF). More importantly, INV is free from buffer lag and is naturally fit for streaming. While this work does not achieve real-time training, it shows that incremental approaches like INV present new possibilities in interactive 3D streaming. Moreover, our discovery of natural information partition leads to a better understanding and manipulation of MLPs. Code and dataset will be released soon.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Wang, Shengze and Supikov, Alexey and Ratcliff, Joshua and Fuchs, Henry and Azuma, Ronald},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01532 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{yu_inpaint_2023,
	title = {Inpaint {Anything}: {Segment} {Anything} {Meets} {Image} {Inpainting}},
	shorttitle = {Inpaint {Anything}},
	url = {http://arxiv.org/abs/2304.06790},
	doi = {10.48550/arXiv.2304.06790},
	abstract = {Modern image inpainting systems, despite the significant progress, often struggle with mask selection and holes filling. Based on Segment-Anything Model (SAM), we make the first attempt to the mask-free image inpainting and propose a new paradigm of ``clicking and filling'', which is named as Inpaint Anything (IA). The core idea behind IA is to combine the strengths of different models in order to build a very powerful and user-friendly pipeline for solving inpainting-related problems. IA supports three main features: (i) Remove Anything: users could click on an object and IA will remove it and smooth the ``hole'' with the context; (ii) Fill Anything: after certain objects removal, users could provide text-based prompts to IA, and then it will fill the hole with the corresponding generative content via driving AIGC models like Stable Diffusion; (iii) Replace Anything: with IA, users have another option to retain the click-selected object and replace the remaining background with the newly generated scenes. We are also very willing to help everyone share and promote new projects based on our Inpaint Anything (IA). Our codes are available at https://github.com/geekyutao/Inpaint-Anything.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Yu, Tao and Feng, Runseng and Feng, Ruoyu and Liu, Jinming and Jin, Xin and Zeng, Wenjun and Chen, Zhibo},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06790 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{guo_incremental_2022,
	title = {Incremental {Learning} for {Neural} {Radiance} {Field} with {Uncertainty}-{Filtered} {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2212.10950},
	doi = {10.48550/arXiv.2212.10950},
	abstract = {Recent neural radiance field (NeRF) representation has achieved great success in the tasks of novel view synthesis and 3D reconstruction. However, they suffer from the catastrophic forgetting problem when continuously learning from streaming data without revisiting the previous training data. This limitation prohibits the application of existing NeRF models to scenarios where images come in sequentially. In view of this, we explore the task of incremental learning for neural radiance field representation in this work. We first propose a student-teacher pipeline to mitigate the catastrophic forgetting problem. Specifically, we iterate the process of using the student as the teacher at the end of each incremental step and let the teacher guide the training of the student in the next step. In this way, the student network is able to learn new information from the streaming data and retain old knowledge from the teacher network simultaneously. Given that not all information from the teacher network is helpful since it is only trained with the old data, we further introduce a random inquirer and an uncertainty-based filter to filter useful information. We conduct experiments on the NeRF-synthetic360 and NeRF-real360 datasets, where our approach significantly outperforms the baselines by 7.3\% and 25.2\% in terms of PSNR. Furthermore, we also show that our approach can be applied to the large-scale camera facing-outwards dataset ScanNet, where we surpass the baseline by 60.0\% in PSNR.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Guo, Mengqi and Li, Chen and Lee, Gim Hee},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10950 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{girdhar_imagebind_2023,
	title = {{ImageBind}: {One} {Embedding} {Space} {To} {Bind} {Them} {All}},
	shorttitle = {{ImageBind}},
	url = {http://arxiv.org/abs/2305.05665},
	doi = {10.48550/arXiv.2305.05665},
	abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	month = may,
	year = {2023},
	note = {arXiv:2305.05665 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@misc{isik_humanrf_2023,
	title = {{HumanRF}: {High}-{Fidelity} {Neural} {Radiance} {Fields} for {Humans} in {Motion}},
	shorttitle = {{HumanRF}},
	url = {http://arxiv.org/abs/2305.06356},
	doi = {10.1145/3592415},
	abstract = {Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis.},
	urldate = {2023-05-15},
	author = {Işık, Mustafa and Rünz, Martin and Georgopoulos, Markos and Khakhulin, Taras and Starck, Jonathan and Agapito, Lourdes and Nießner, Matthias},
	month = may,
	year = {2023},
	note = {arXiv:2305.06356 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{zhi_ilabel_2021,
	title = {{ILabel}: {Interactive} {Neural} {Scene} {Labelling}},
	shorttitle = {{ILabel}},
	url = {http://arxiv.org/abs/2111.14637},
	doi = {10.48550/arXiv.2111.14637},
	abstract = {Joint representation of geometry, colour and semantics using a 3D neural field enables accurate dense labelling from ultra-sparse interactions as a user reconstructs a scene in real-time using a handheld RGB-D sensor. Our iLabel system requires no training data, yet can densely label scenes more accurately than standard methods trained on large, expensively labelled image datasets. Furthermore, it works in an 'open set' manner, with semantic classes defined on the fly by the user. ILabel's underlying model is a multilayer perceptron (MLP) trained from scratch in real-time to learn a joint neural scene representation. The scene model is updated and visualised in real-time, allowing the user to focus interactions to achieve efficient labelling. A room or similar scene can be accurately labelled into 10+ semantic categories with only a few tens of clicks. Quantitative labelling accuracy scales powerfully with the number of clicks, and rapidly surpasses standard pre-trained semantic segmentation methods. We also demonstrate a hierarchical labelling variant.},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Zhi, Shuaifeng and Sucar, Edgar and Mouton, Andre and Haughton, Iain and Laidlow, Tristan and Davison, Andrew J.},
	month = dec,
	year = {2021},
	note = {arXiv:2111.14637 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{dai_hybrid_2023,
	title = {Hybrid {Neural} {Rendering} for {Large}-{Scale} {Scenes} with {Motion} {Blur}},
	url = {http://arxiv.org/abs/2304.12652},
	doi = {10.48550/arXiv.2304.12652},
	abstract = {Rendering novel view images is highly desirable for many applications. Despite recent progress, it remains challenging to render high-fidelity and view-consistent novel views of large-scale scenes from in-the-wild images with inevitable artifacts (e.g., motion blur). To this end, we develop a hybrid neural rendering model that makes image-based representation and neural 3D representation join forces to render high-quality, view-consistent images. Besides, images captured in the wild inevitably contain artifacts, such as motion blur, which deteriorates the quality of rendered images. Accordingly, we propose strategies to simulate blur effects on the rendered images to mitigate the negative influence of blurriness images and reduce their importance during training based on precomputed quality-aware weights. Extensive experiments on real and synthetic data demonstrate our model surpasses state-of-the-art point-based methods for novel view synthesis. The code is available at https://daipengwa.github.io/Hybrid-Rendering-ProjectPage.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Dai, Peng and Zhang, Yinda and Yu, Xin and Lyu, Xiaoyang and Qi, Xiaojuan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12652 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao_humannerf_2022,
	title = {{HumanNeRF}: {Efficiently} {Generated} {Human} {Radiance} {Field} from {Sparse} {Inputs}},
	shorttitle = {{HumanNeRF}},
	url = {http://arxiv.org/abs/2112.02789},
	doi = {10.48550/arXiv.2112.02789},
	abstract = {Recent neural human representations can produce high-quality multi-view rendering but require using dense multi-view inputs and costly training. They are hence largely limited to static models as training each frame is infeasible. We present HumanNeRF - a generalizable neural representation - for high-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet assists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated pixel-alignment feature across multi-view inputs along with a pose embedded non-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can already produce reasonable rendering on sparse video inputs of unseen subjects and camera settings. To further improve the rendering quality, we augment our solution with an appearance blending module for combining the benefits of both neural volumetric rendering and neural texture blending. Extensive experiments on various multi-view dynamic human datasets demonstrate the generalizability and effectiveness of our approach in synthesizing photo-realistic free-view humans under challenging motions and with very sparse camera view inputs.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Zhao, Fuqiang and Yang, Wei and Zhang, Jiakai and Lin, Pei and Zhang, Yingliang and Yu, Jingyi and Xu, Lan},
	month = mar,
	year = {2022},
	note = {arXiv:2112.02789 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{weng_humannerf_2022,
	title = {{HumanNeRF}: {Free}-viewpoint {Rendering} of {Moving} {People} from {Monocular} {Video}},
	shorttitle = {{HumanNeRF}},
	url = {http://arxiv.org/abs/2201.04127},
	doi = {10.48550/arXiv.2201.04127},
	abstract = {We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on a given monocular video of a human performing complex body motions, e.g. a video from YouTube. Our method enables pausing the video at any frame and rendering the subject from arbitrary new camera viewpoints or even a full 360-degree camera path for that particular frame and body pose. This task is particularly challenging, as it requires synthesizing photorealistic details of the body, as seen from various camera angles that may not exist in the input video, as well as synthesizing fine details such as cloth folds and facial appearance. Our method optimizes for a volumetric representation of the person in a canonical T-pose, in concert with a motion field that maps the estimated canonical representation to every frame of the video via backward warps. The motion field is decomposed into skeletal rigid and non-rigid motions, produced by deep networks. We show significant performance improvements over prior work, and compelling examples of free-viewpoint renderings from monocular video of moving humans in challenging uncontrolled capture scenarios.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Weng, Chung-Yi and Curless, Brian and Srinivasan, Pratul P. and Barron, Jonathan T. and Kemelmacher-Shlizerman, Ira},
	month = jun,
	year = {2022},
	note = {arXiv:2201.04127 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{liang_hr-neus_2023,
	title = {{HR}-{NeuS}: {Recovering} {High}-{Frequency} {Surface} {Geometry} via {Neural} {Implicit} {Surfaces}},
	shorttitle = {{HR}-{NeuS}},
	url = {http://arxiv.org/abs/2302.06793},
	doi = {10.48550/arXiv.2302.06793},
	abstract = {Recent advances in neural implicit surfaces for multi-view 3D reconstruction primarily focus on improving large-scale surface reconstruction accuracy, but often produce over-smoothed geometries that lack fine surface details. To address this, we present High-Resolution NeuS (HR-NeuS), a novel neural implicit surface reconstruction method that recovers high-frequency surface geometry while maintaining large-scale reconstruction accuracy. We achieve this by utilizing (i) multi-resolution hash grid encoding rather than positional encoding at high frequencies, which boosts our model's expressiveness of local geometry details; (ii) a coarse-to-fine algorithmic framework that selectively applies surface regularization to coarse geometry without smoothing away fine details; (iii) a coarse-to-fine grid annealing strategy to train the network. We demonstrate through experiments on DTU and BlendedMVS datasets that our approach produces 3D geometries that are qualitatively more detailed and quantitatively of similar accuracy compared to previous approaches.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Liang, Erich and Deng, Kenan and Zhang, Xi and Wang, Chun-Kai},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06793 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_hf-neus_2022,
	title = {{HF}-{NeuS}: {Improved} {Surface} {Reconstruction} {Using} {High}-{Frequency} {Details}},
	shorttitle = {{HF}-{NeuS}},
	url = {http://arxiv.org/abs/2206.07850},
	doi = {10.48550/arXiv.2206.07850},
	abstract = {Neural rendering can be used to reconstruct implicit representations of shapes without 3D supervision. However, current neural surface reconstruction methods have difficulty learning high-frequency geometry details, so the reconstructed shapes are often over-smoothed. We develop HF-NeuS, a novel method to improve the quality of surface reconstruction in neural rendering. We follow recent work to model surfaces as signed distance functions (SDFs). First, we offer a derivation to analyze the relationship between the SDF, the volume density, the transparency function, and the weighting function used in the volume rendering equation and propose to model transparency as transformed SDF. Second, we observe that attempting to jointly encode high-frequency and low-frequency components in a single SDF leads to unstable optimization. We propose to decompose the SDF into a base function and a displacement function with a coarse-to-fine strategy to gradually increase the high-frequency details. Finally, we design an adaptive optimization strategy that makes the training process focus on improving those regions near the surface where the SDFs have artifacts. Our qualitative and quantitative results show that our method can reconstruct fine-grained surface details and obtain better surface reconstruction quality than the current state of the art. Code available at https://github.com/yiqun-wang/HFS.},
	urldate = {2023-05-14},
	publisher = {arXiv},
	author = {Wang, Yiqun and Skorokhodov, Ivan and Wonka, Peter},
	month = sep,
	year = {2022},
	note = {arXiv:2206.07850 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{wang_hf-neus_2022-1,
	title = {{HF}-{NeuS}: {Improved} {Surface} {Reconstruction} {Using} {High}-{Frequency} {Details}},
	shorttitle = {{HF}-{NeuS}},
	url = {https://arxiv.org/abs/2206.07850v2},
	abstract = {Neural rendering can be used to reconstruct implicit representations of shapes without 3D supervision. However, current neural surface reconstruction methods have difficulty learning high-frequency geometry details, so the reconstructed shapes are often over-smoothed. We develop HF-NeuS, a novel method to improve the quality of surface reconstruction in neural rendering. We follow recent work to model surfaces as signed distance functions (SDFs). First, we offer a derivation to analyze the relationship between the SDF, the volume density, the transparency function, and the weighting function used in the volume rendering equation and propose to model transparency as transformed SDF. Second, we observe that attempting to jointly encode high-frequency and low-frequency components in a single SDF leads to unstable optimization. We propose to decompose the SDF into a base function and a displacement function with a coarse-to-fine strategy to gradually increase the high-frequency details. Finally, we design an adaptive optimization strategy that makes the training process focus on improving those regions near the surface where the SDFs have artifacts. Our qualitative and quantitative results show that our method can reconstruct fine-grained surface details and obtain better surface reconstruction quality than the current state of the art. Code available at https://github.com/yiqun-wang/HFS.},
	language = {en},
	urldate = {2023-03-13},
	journal = {arXiv.org},
	author = {Wang, Yiqun and Skorokhodov, Ivan and Wonka, Peter},
	month = jun,
	year = {2022},
	doi = {10.48550/arXiv.2206.07850},
	keywords = {/unread},
}

@misc{du_gradient_2019,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1811.03804},
	doi = {10.48550/arXiv.1811.03804},
	abstract = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	month = may,
	year = {2019},
	note = {arXiv:1811.03804 [cs, math, stat]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{mingyang_graphreg_2023,
	title = {{GraphReg}: {Dynamical} {Point} {Cloud} {Registration} with {Geometry}-aware {Graph} {Signal} {Processing}},
	shorttitle = {{GraphReg}},
	url = {http://arxiv.org/abs/2302.01109},
	doi = {10.48550/arXiv.2302.01109},
	abstract = {This study presents a high-accuracy, efficient, and physically induced method for 3D point cloud registration, which is the core of many important 3D vision problems. In contrast to existing physics-based methods that merely consider spatial point information and ignore surface geometry, we explore geometry aware rigid-body dynamics to regulate the particle (point) motion, which results in more precise and robust registration. Our proposed method consists of four major modules. First, we leverage the graph signal processing (GSP) framework to define a new signature, (i.e., point response intensity for each point), by which we succeed in describing the local surface variation, resampling keypoints, and distinguishing different particles. Then, to address the shortcomings of current physics-based approaches that are sensitive to outliers, we accommodate the defined point response intensity to median absolute deviation (MAD) in robust statistics and adopt the X84 principle for adaptive outlier depression, ensuring a robust and stable registration. Subsequently, we propose a novel geometric invariant under rigid transformations to incorporate higher-order features of point clouds, which is further embedded for force modeling to guide the correspondence between pairwise scans credibly. Finally, we introduce an adaptive simulated annealing (ASA) method to search for the global optimum and substantially accelerate the registration process. We perform comprehensive experiments to evaluate the proposed method on various datasets captured from range scanners to LiDAR. Results demonstrate that our proposed method outperforms representative state-of-the-art approaches in terms of accuracy and is more suitable for registering large-scale point clouds. Furthermore, it is considerably faster and more robust than most competitors.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Mingyang, Zhao and Lei, Ma and Xiaohong, Jia and Dong-Ming, Yan and Tiejun, Huang},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01109 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ahmadi_hdpv-slam_2023,
	title = {{HDPV}-{SLAM}: {Hybrid} {Depth}-augmented {Panoramic} {Visual} {SLAM} for {Mobile} {Mapping} {System} with {Tilted} {LiDAR} and {Panoramic} {Visual} {Camera}},
	shorttitle = {{HDPV}-{SLAM}},
	url = {http://arxiv.org/abs/2301.11823},
	doi = {10.48550/arXiv.2301.11823},
	abstract = {This paper proposes a novel visual simultaneous localization and mapping (SLAM), called Hybrid Depth-augmented Panoramic Visual SLAM (HDPV-SLAM), generating accurate and metrically scaled vehicle trajectories using a panoramic camera and a titled multi-beam LiDAR scanner. RGB-D SLAM served as the design foundation for HDPV-SLAM, adding depth information to visual features. It seeks to overcome the two problems that limit the performance of RGB-D SLAM systems. The first barrier is the sparseness of LiDAR depth, which makes it challenging to connect it with visual features extracted from the RGB image. We address this issue by proposing a depth estimation module for iteratively densifying sparse LiDAR depth based on deep learning (DL). The second issue relates to the challenges in the depth association caused by a significant deficiency of horizontal overlapping coverage between the panoramic camera and the tilted LiDAR sensor. To overcome this difficulty, we present a hybrid depth association module that optimally combines depth information estimated by two independent procedures, feature triangulation and depth estimation. This hybrid depth association module intends to maximize the use of more accurate depth information between the triangulated depth with visual features tracked and the DL-based corrected depth during a phase of feature tracking. We assessed HDPV-SLAM's performance using the 18.95 km-long York University and Teledyne Optech (YUTO) MMS dataset. Experimental results demonstrate that the proposed two modules significantly contribute to HDPV-SLAM's performance, which outperforms the state-of-the-art (SOTA) SLAM systems.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Ahmadi, Mostafa and Naeini, Amin Alizadeh and Arjmandi, Zahra and Zhang, Yujia and Sheikholeslami, Mohammad Moein and Sohn, Gunho},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11823 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{venkat_geometry-biased_2023,
	title = {Geometry-biased {Transformers} for {Novel} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2301.04650},
	doi = {10.48550/arXiv.2301.04650},
	abstract = {We tackle the task of synthesizing novel views of an object given a few input images and associated camera viewpoints. Our work is inspired by recent 'geometry-free' approaches where multi-view images are encoded as a (global) set-latent representation, which is then used to predict the color for arbitrary query rays. While this representation yields (coarsely) accurate images corresponding to novel viewpoints, the lack of geometric reasoning limits the quality of these outputs. To overcome this limitation, we propose 'Geometry-biased Transformers' (GBTs) that incorporate geometric inductive biases in the set-latent representation-based inference to encourage multi-view geometric consistency. We induce the geometric bias by augmenting the dot-product attention mechanism to also incorporate 3D distances between rays associated with tokens as a learnable bias. We find that this, along with camera-aware embeddings as input, allows our models to generate significantly more accurate outputs. We validate our approach on the real-world CO3D dataset, where we train our system over 10 categories and evaluate its view-synthesis ability for novel objects as well as unseen categories. We empirically validate the benefits of the proposed geometric biases and show that our approach significantly improves over prior works.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Venkat, Naveen and Agarwal, Mayank and Singh, Maneesh and Tulsiani, Shubham},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04650 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{venkat_geometry-biased_2023-1,
	title = {Geometry-biased {Transformers} for {Novel} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2301.04650},
	doi = {10.48550/arXiv.2301.04650},
	abstract = {We tackle the task of synthesizing novel views of an object given a few input images and associated camera viewpoints. Our work is inspired by recent 'geometry-free' approaches where multi-view images are encoded as a (global) set-latent representation, which is then used to predict the color for arbitrary query rays. While this representation yields (coarsely) accurate images corresponding to novel viewpoints, the lack of geometric reasoning limits the quality of these outputs. To overcome this limitation, we propose 'Geometry-biased Transformers' (GBTs) that incorporate geometric inductive biases in the set-latent representation-based inference to encourage multi-view geometric consistency. We induce the geometric bias by augmenting the dot-product attention mechanism to also incorporate 3D distances between rays associated with tokens as a learnable bias. We find that this, along with camera-aware embeddings as input, allows our models to generate significantly more accurate outputs. We validate our approach on the real-world CO3D dataset, where we train our system over 10 categories and evaluate its view-synthesis ability for novel objects as well as unseen categories. We empirically validate the benefits of the proposed geometric biases and show that our approach significantly improves over prior works.},
	urldate = {2023-01-17},
	publisher = {arXiv},
	author = {Venkat, Naveen and Agarwal, Mayank and Singh, Maneesh and Tulsiani, Shubham},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04650 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_freenerf_2023,
	title = {{FreeNeRF}: {Improving} {Few}-shot {Neural} {Rendering} with {Free} {Frequency} {Regularization}},
	shorttitle = {{FreeNeRF}},
	url = {http://arxiv.org/abs/2303.07418},
	doi = {10.48550/arXiv.2303.07418},
	abstract = {Novel view synthesis with sparse inputs is a challenging problem for neural radiance fields (NeRF). Recent efforts alleviate this challenge by introducing external supervision, such as pre-trained models and extra depth signals, and by non-trivial patch-based rendering. In this paper, we present Frequency regularized NeRF (FreeNeRF), a surprisingly simple baseline that outperforms previous methods with minimal modifications to the plain NeRF. We analyze the key challenges in few-shot neural rendering and find that frequency plays an important role in NeRF's training. Based on the analysis, we propose two regularization terms. One is to regularize the frequency range of NeRF's inputs, while the other is to penalize the near-camera density fields. Both techniques are ``free lunches'' at no additional computational cost. We demonstrate that even with one line of code change, the original NeRF can achieve similar performance as other complicated methods in the few-shot setting. FreeNeRF achieves state-of-the-art performance across diverse datasets, including Blender, DTU, and LLFF. We hope this simple baseline will motivate a rethinking of the fundamental role of frequency in NeRF's training under the low-data regime and beyond.},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Yang, Jiawei and Pavone, Marco and Wang, Yue},
	month = mar,
	year = {2023},
	note = {arXiv:2303.07418 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{geng_gapartnet_2023,
	title = {{GAPartNet}: {Cross}-{Category} {Domain}-{Generalizable} {Object} {Perception} and {Manipulation} via {Generalizable} and {Actionable} {Parts}},
	shorttitle = {{GAPartNet}},
	url = {http://arxiv.org/abs/2211.05272},
	doi = {10.48550/arXiv.2211.05272},
	abstract = {For years, researchers have been devoted to generalizable object perception and manipulation, where cross-category generalizability is highly desired yet underexplored. In this work, we propose to learn such cross-category skills via Generalizable and Actionable Parts (GAParts). By identifying and defining 9 GAPart classes (lids, handles, etc.) in 27 object categories, we construct a large-scale part-centric interactive dataset, GAPartNet, where we provide rich, part-level annotations (semantics, poses) for 8,489 part instances on 1,166 objects. Based on GAPartNet, we investigate three cross-category tasks: part segmentation, part pose estimation, and part-based object manipulation. Given the significant domain gaps between seen and unseen object categories, we propose a robust 3D segmentation method from the perspective of domain generalization by integrating adversarial learning techniques. Our method outperforms all existing methods by a large margin, no matter on seen or unseen categories. Furthermore, with part segmentation and pose estimation results, we leverage the GAPart pose definition to design part-based manipulation heuristics that can generalize well to unseen object categories in both the simulator and the real world. Our dataset, code, and demos are available on our project page.},
	urldate = {2023-04-22},
	publisher = {arXiv},
	author = {Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
	month = mar,
	year = {2023},
	note = {arXiv:2211.05272 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@book{shirley_fundamentals_nodate,
	title = {Fundamentals of {Computer} {Graphics}},
	language = {en},
	author = {Shirley, Peter},
	keywords = {/unread, ❓ Multiple DOI},
}

@misc{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	doi = {10.48550/arXiv.1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv:1506.01497 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{arora_fine-grained_2019,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1901.08584},
	doi = {10.48550/arXiv.1901.08584},
	abstract = {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	month = may,
	year = {2019},
	note = {arXiv:1901.08584 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{shan_fine_2023,
	title = {Fine {Robotic} {Manipulation} without {Force}/{Torque} {Sensor}},
	url = {http://arxiv.org/abs/2301.13413},
	doi = {10.48550/arXiv.2301.13413},
	abstract = {Force Sensing and Force Control are essential to many industrial applications. Typically, a 6-axis Force/Torque (F/T) sensor is mounted between the robot's wrist and the end-effector in order to measure the forces and torques exerted by the environment onto the robot (the external wrench). Although a typical 6-axis F/T sensor can provide highly accurate measurements, it is expensive and vulnerable to drift and external impacts. Existing methods aiming at estimating the external wrench using only the robot's internal signals are limited in scope: for example, wrench estimation accuracy was mostly validated in free-space motions and simple contacts as opposed to tasks like assembly that require high-precision force control. Here we present a Neural Network based method and argue that by devoting particular attention to the training data structure, it is possible to accurately estimate the external wrench in a wide range of scenarios based solely on internal signals. As an illustration, we demonstrate a pin insertion experiment with 100-micron clearance and a hand-guiding experiment, both performed without external F/T sensors or joint torque sensors. Our result opens the possibility of equipping the existing 2.7 million industrial robots with Force Sensing and Force Control capabilities without any additional hardware.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Shan, Shilin and Pham, Quang-Cuong},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13413 [cs]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jain_enhanced_2023,
	title = {Enhanced {Stable} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2303.17094},
	abstract = {We introduce an approach to enhance the novel view synthesis from images taken from a freely moving camera. The introduced approach focuses on outdoor scenes where recovering accurate geometric scaffold and camera pose is challenging, leading to inferior results using the state-of-the-art stable view synthesis (SVS) method. SVS and related methods fail for outdoor scenes primarily due to (i) over-relying on the multiview stereo (MVS) for geometric scaffold recovery and (ii) assuming COLMAP computed camera poses as the best possible estimates, despite it being well-studied that MVS 3D reconstruction accuracy is limited to scene disparity and camera-pose accuracy is sensitive to key-point correspondence selection. This work proposes a principled way to enhance novel view synthesis solutions drawing inspiration from the basics of multiple view geometry. By leveraging the complementary behavior of MVS and monocular depth, we arrive at a better scene depth per view for nearby and far points, respectively. Moreover, our approach jointly refines camera poses with image-based rendering via multiple rotation averaging graph optimization. The recovered scene depth and the camera-pose help better view-dependent on-surface feature aggregation of the entire scene. Extensive evaluation of our approach on the popular benchmark dataset, such as Tanks and Temples, shows substantial improvement in view synthesis results compared to the prior art. For instance, our method shows 1.5 dB of PSNR improvement on the Tank and Temples. Similar statistics are observed when tested on other benchmark datasets such as FVS, Mip-NeRF 360, and DTU.},
	urldate = {2023-04-22},
	publisher = {arXiv},
	author = {Jain, Nishant and Kumar, Suryansh and Van Gool, Luc},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17094 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{medina_evolving_2023,
	title = {Evolving {Flying} {Machines} in {Minecraft} {Using} {Quality} {Diversity}},
	url = {http://arxiv.org/abs/2302.00782},
	doi = {10.48550/arXiv.2302.00782},
	abstract = {Minecraft is a great testbed for human creativity that has inspired the design of various structures and even functioning machines, including flying machines. EvoCraft is an API for programmatically generating structures in Minecraft, but the initial work in this domain was not capable of evolving flying machines. This paper applies fitness-based evolution and quality diversity search in order to evolve flying machines. Although fitness alone can occasionally produce flying machines, thanks in part to a more sophisticated fitness function than was used previously, the quality diversity algorithm MAP-Elites is capable of discovering flying machines much more reliably, at least when an appropriate behavior characterization is used to guide the search for diverse solutions.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Medina, Alejandro and Richey, Melanie and Mueller, Mark and Schrum, Jacob},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00782 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@misc{prokudin_dynamic_2023,
	title = {Dynamic {Point} {Fields}},
	url = {http://arxiv.org/abs/2304.02626},
	abstract = {Recent years have witnessed significant progress in the field of neural surface reconstruction. While the extensive focus was put on volumetric and implicit approaches, a number of works have shown that explicit graphics primitives such as point clouds can significantly reduce computational complexity, without sacrificing the reconstructed surface quality. However, less emphasis has been put on modeling dynamic surfaces with point primitives. In this work, we present a dynamic point field model that combines the representational benefits of explicit point-based graphics with implicit deformation networks to allow efficient modeling of non-rigid 3D surfaces. Using explicit surface primitives also allows us to easily incorporate well-established constraints such as-isometric-as-possible regularisation. While learning this deformation model is prone to local optima when trained in a fully unsupervised manner, we propose to additionally leverage semantic information such as keypoint dynamics to guide the deformation learning. We demonstrate our model with an example application of creating an expressive animatable human avatar from a collection of 3D scans. Here, previous methods mostly rely on variants of the linear blend skinning paradigm, which fundamentally limits the expressivity of such models when dealing with complex cloth appearances such as long skirts. We show the advantages of our dynamic point field framework in terms of its representational power, learning efficiency, and robustness to out-of-distribution novel poses.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Prokudin, Sergey and Ma, Qianli and Raafat, Maxime and Valentin, Julien and Tang, Siyu},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02626 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_dropout_2023,
	title = {Dropout {Reduces} {Underfitting}},
	url = {http://arxiv.org/abs/2303.01500},
	doi = {10.48550/arXiv.2303.01500},
	abstract = {Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations and is only activated later in training. Experiments on ImageNet and various vision tasks demonstrate that our methods consistently improve generalization accuracy. Our results encourage more research on understanding regularization in deep learning and our methods can be useful tools for future neural network training, especially in the era of large data. Code is available at https://github.com/facebookresearch/dropout .},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Liu, Zhuang and Xu, Zhiqiu and Jin, Joseph and Shen, Zhiqiang and Darrell, Trevor},
	month = mar,
	year = {2023},
	note = {arXiv:2303.01500 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_dynamic_2023,
	title = {Dynamic {Local} {Feature} {Aggregation} for {Learning} on {Point} {Clouds}},
	url = {http://arxiv.org/abs/2301.02836},
	doi = {10.48550/arXiv.2301.02836},
	abstract = {Existing point cloud learning methods aggregate features from neighbouring points relying on constructing graph in the spatial domain, which results in feature update for each point based on spatially-fixed neighbours throughout layers. In this paper, we propose a dynamic feature aggregation (DFA) method that can transfer information by constructing local graphs in the feature domain without spatial constraints. By finding k-nearest neighbors in the feature domain, we perform relative position encoding and semantic feature encoding to explore latent position and feature similarity information, respectively, so that rich local features can be learned. At the same time, we also learn low-dimensional global features from the original point cloud for enhancing feature representation. Between DFA layers, we dynamically update the constructed local graph structure, so that we can learn richer information, which greatly improves adaptability and efficiency. We demonstrate the superiority of our method by conducting extensive experiments on point cloud classification and segmentation tasks. Implementation code is available: https://github.com/jiamang/DFA.},
	urldate = {2023-01-17},
	publisher = {arXiv},
	author = {Li, Zihao and Gao, Pan and Yuan, Hui and Wei, Ran},
	month = jan,
	year = {2023},
	note = {arXiv:2301.02836 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shao_doublefield_2022,
	title = {{DoubleField}: {Bridging} the {Neural} {Surface} and {Radiance} {Fields} for {High}-fidelity {Human} {Reconstruction} and {Rendering}},
	shorttitle = {{DoubleField}},
	url = {http://arxiv.org/abs/2106.03798},
	doi = {10.48550/arXiv.2106.03798},
	abstract = {We introduce DoubleField, a novel framework combining the merits of both surface field and radiance field for high-fidelity human reconstruction and rendering. Within DoubleField, the surface field and radiance field are associated together by a shared feature embedding and a surface-guided sampling strategy. Moreover, a view-to-view transformer is introduced to fuse multi-view features and learn view-dependent features directly from high-resolution inputs. With the modeling power of DoubleField and the view-to-view transformer, our method significantly improves the reconstruction quality of both geometry and appearance, while supporting direct inference, scene-specific high-resolution finetuning, and fast rendering. The efficacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior capability for high-quality human model reconstruction and photo-realistic free-viewpoint human rendering. Data and source code will be made public for the research purpose. Please refer to our project page: http://www.liuyebin.com/dbfield/dbfield.html.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Shao, Ruizhi and Zhang, Hongwen and Zhang, He and Chen, Mingjia and Cao, Yanpei and Yu, Tao and Liu, Yebin},
	month = mar,
	year = {2022},
	note = {arXiv:2106.03798 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{poole_dreamfusion_2022,
	title = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
	shorttitle = {{DreamFusion}},
	url = {http://arxiv.org/abs/2209.14988},
	doi = {10.48550/arXiv.2209.14988},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14988 [cs, stat]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{mozer_discrete_2017,
	title = {Discrete {Event}, {Continuous} {Time} {RNNs}},
	url = {http://arxiv.org/abs/1710.04110},
	doi = {10.48550/arXiv.1710.04110},
	abstract = {We investigate recurrent neural network architectures for event-sequence processing. Event sequences, characterized by discrete observations stamped with continuous-valued times of occurrence, are challenging due to the potentially wide dynamic range of relevant time scales as well as interactions between time scales. We describe four forms of inductive bias that should benefit architectures for event sequences: temporal locality, position and scale homogeneity, and scale interdependence. We extend the popular gated recurrent unit (GRU) architecture to incorporate these biases via intrinsic temporal dynamics, obtaining a continuous-time GRU. The CT-GRU arises by interpreting the gates of a GRU as selecting a time scale of memory, and the CT-GRU generalizes the GRU by incorporating multiple time scales of memory and performing context-dependent selection of time scales for information storage and retrieval. Event time-stamps drive decay dynamics of the CT-GRU, whereas they serve as generic additional inputs to the GRU. Despite the very different manner in which the two models consider time, their performance on eleven data sets we examined is essentially identical. Our surprising results point both to the robustness of GRU and LSTM architectures for handling continuous time, and to the potency of incorporating continuous dynamics into neural architectures.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Mozer, Michael C. and Kazakov, Denis and Lindsey, Robert V.},
	month = oct,
	year = {2017},
	note = {arXiv:1710.04110 [cs]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6},
}

@misc{oquab_dinov2_2023,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07193 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{niemeyer_differentiable_2020,
	title = {Differentiable {Volumetric} {Rendering}: {Learning} {Implicit} {3D} {Representations} without {3D} {Supervision}},
	shorttitle = {Differentiable {Volumetric} {Rendering}},
	url = {http://arxiv.org/abs/1912.07372},
	doi = {10.48550/arXiv.1912.07372},
	abstract = {Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
	month = mar,
	year = {2020},
	note = {arXiv:1912.07372 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv:1502.01852 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_dehazenerf_2023,
	title = {{DehazeNeRF}: {Multiple} {Image} {Haze} {Removal} and {3D} {Shape} {Reconstruction} using {Neural} {Radiance} {Fields}},
	shorttitle = {{DehazeNeRF}},
	url = {http://arxiv.org/abs/2303.11364},
	doi = {10.48550/arXiv.2303.11364},
	abstract = {Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance for 3D computer vision tasks, including novel view synthesis and 3D shape reconstruction. However, these methods fail in adverse weather conditions. To address this challenge, we introduce DehazeNeRF as a framework that robustly operates in hazy conditions. DehazeNeRF extends the volume rendering equation by adding physically realistic terms that model atmospheric scattering. By parameterizing these terms using suitable networks that match the physical properties, we introduce effective inductive biases, which, together with the proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view haze removal, novel view synthesis, and 3D shape reconstruction where existing approaches fail.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Chen, Wei-Ting and Yifan, Wang and Kuo, Sy-Yen and Wetzstein, Gordon},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11364 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shen_deep_2021,
	title = {Deep {Marching} {Tetrahedra}: a {Hybrid} {Representation} for {High}-{Resolution} {3D} {Shape} {Synthesis}},
	shorttitle = {Deep {Marching} {Tetrahedra}},
	url = {http://arxiv.org/abs/2111.04276},
	doi = {10.48550/arXiv.2111.04276},
	abstract = {We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Shen, Tianchang and Gao, Jun and Yin, Kangxue and Liu, Ming-Yu and Fidler, Sanja},
	month = nov,
	year = {2021},
	note = {arXiv:2111.04276 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{kobayashi_decomposing_2022,
	title = {Decomposing {NeRF} for {Editing} via {Feature} {Field} {Distillation}},
	url = {http://arxiv.org/abs/2205.15585},
	doi = {10.48550/arXiv.2205.15585},
	abstract = {Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations. However, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional. In particular, it has been difficult to selectively edit specific regions or objects. In this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes. We propose to distill the knowledge of off-the-shelf, self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field. Given a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training and enable us to semantically select and edit regions in the radiance field. Our experiments validate that the distilled feature fields (DFFs) can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations.},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Kobayashi, Sosuke and Matsumoto, Eiichi and Sitzmann, Vincent},
	month = oct,
	year = {2022},
	note = {arXiv:2205.15585 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{iftekhar_dds_2023,
	title = {{DDS}: {Decoupled} {Dynamic} {Scene}-{Graph} {Generation} {Network}},
	shorttitle = {{DDS}},
	url = {http://arxiv.org/abs/2301.07666},
	doi = {10.48550/arXiv.2301.07666},
	abstract = {Scene-graph generation involves creating a structural representation of the relationships between objects in a scene by predicting subject-object-relation triplets from input data. However, existing methods show poor performance in detecting triplets outside of a predefined set, primarily due to their reliance on dependent feature learning. To address this issue we propose DDS -- a decoupled dynamic scene-graph generation network -- that consists of two independent branches that can disentangle extracted features. The key innovation of the current paper is the decoupling of the features representing the relationships from those of the objects, which enables the detection of novel object-relationship combinations. The DDS model is evaluated on three datasets and outperforms previous methods by a significant margin, especially in detecting previously unseen triplets.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Iftekhar, A. S. M. and Ruschel, Raphael and Kumar, Satish and You, Suya and Manjunath, B. S.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07666 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kocaballi_conversational_2023,
	title = {Conversational {AI}-{Powered} {Design}: {ChatGPT} as {Designer}, {User}, and {Product}},
	shorttitle = {Conversational {AI}-{Powered} {Design}},
	url = {http://arxiv.org/abs/2302.07406},
	doi = {10.48550/arXiv.2302.07406},
	abstract = {The recent advancements in Large Language Models (LLMs), particularly conversational LLMs like ChatGPT, have prompted changes in a range of fields, including design. This study aims to examine the capabilities of ChatGPT in a human-centered design process. To this end, a hypothetical design project was conducted, where ChatGPT was utilized to generate personas, simulate interviews with fictional users, create new design ideas, simulate usage scenarios and conversations between an imaginary prototype and fictional users, and lastly evaluate user experience. The results show that ChatGPT effectively performed the tasks assigned to it as a designer, user, or product, providing mostly appropriate responses. The study does, however, highlight some drawbacks such as forgotten information, partial responses, and a lack of output diversity. The paper explains the potential benefits and limitations of using conversational LLMs in design, discusses its implications, and suggests directions for future research in this rapidly evolving area.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Kocaballi, A. Baki},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07406 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@misc{cheng_continuous_2023,
	title = {Continuous {U}-{Net}: {Faster}, {Greater} and {Noiseless}},
	shorttitle = {Continuous {U}-{Net}},
	url = {http://arxiv.org/abs/2302.00626},
	doi = {10.48550/arXiv.2302.00626},
	abstract = {Image segmentation is a fundamental task in image analysis and clinical practice. The current state-of-the-art techniques are based on U-shape type encoder-decoder networks with skip connections, called U-Net. Despite the powerful performance reported by existing U-Net type networks, they suffer from several major limitations. Issues include the hard coding of the receptive field size, compromising the performance and computational cost, as well as the fact that they do not account for inherent noise in the data. They have problems associated with discrete layers, and do not offer any theoretical underpinning. In this work we introduce continuous U-Net, a novel family of networks for image segmentation. Firstly, continuous U-Net is a continuous deep neural network that introduces new dynamic blocks modelled by second order ordinary differential equations. Secondly, we provide theoretical guarantees for our network demonstrating faster convergence, higher robustness and less sensitivity to noise. Thirdly, we derive qualitative measures to tailor-made segmentation tasks. We demonstrate, through extensive numerical and visual results, that our model outperforms existing U-Net blocks for several medical image segmentation benchmarking datasets.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Cheng, Chun-Wun and Runkel, Christina and Liu, Lihao and Chan, Raymond H. and Schönlieb, Carola-Bibiane and Aviles-Rivero, Angelica I.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00626 [cs, eess]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{fonseca_continuous_2023,
	title = {Continuous {Spatiotemporal} {Transformers}},
	url = {http://arxiv.org/abs/2301.13338},
	doi = {10.48550/arXiv.2301.13338},
	abstract = {Modeling spatiotemporal dynamical systems is a fundamental challenge in machine learning. Transformer models have been very successful in NLP and computer vision where they provide interpretable representations of data. However, a limitation of transformers in modeling continuous dynamical systems is that they are fundamentally discrete time and space models and thus have no guarantees regarding continuous sampling. To address this challenge, we present the Continuous Spatiotemporal Transformer (CST), a new transformer architecture that is designed for the modeling of continuous systems. This new framework guarantees a continuous and smooth output via optimization in Sobolev space. We benchmark CST against traditional transformers as well as other spatiotemporal dynamics modeling methods and achieve superior performance in a number of tasks on synthetic and real systems, including learning brain dynamics from calcium imaging data.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Fonseca, Antonio H. de O. and Zappala, Emanuele and Caro, Josue Ortega and van Dijk, David},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13338 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{jiang_continuous_2023,
	title = {Continuous {Trajectory} {Generation} {Based} on {Two}-{Stage} {GAN}},
	url = {http://arxiv.org/abs/2301.07103},
	doi = {10.48550/arXiv.2301.07103},
	abstract = {Simulating the human mobility and generating large-scale trajectories are of great use in many real-world applications, such as urban planning, epidemic spreading analysis, and geographic privacy protect. Although many previous works have studied the problem of trajectory generation, the continuity of the generated trajectories has been neglected, which makes these methods useless for practical urban simulation scenarios. To solve this problem, we propose a novel two-stage generative adversarial framework to generate the continuous trajectory on the road network, namely TS-TrajGen, which efficiently integrates prior domain knowledge of human mobility with model-free learning paradigm. Specifically, we build the generator under the human mobility hypothesis of the A* algorithm to learn the human mobility behavior. For the discriminator, we combine the sequential reward with the mobility yaw reward to enhance the effectiveness of the generator. Finally, we propose a novel two-stage generation process to overcome the weak point of the existing stochastic generation process. Extensive experiments on two real-world datasets and two case studies demonstrate that our framework yields significant improvements over the state-of-the-art methods.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Jiang, Wenjun and Zhao, Wayne Xin and Wang, Jingyuan and Jiang, Jiawei},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07103 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lin_componerf_2023,
	title = {{CompoNeRF}: {Text}-guided {Multi}-object {Compositional} {NeRF} with {Editable} {3D} {Scene} {Layout}},
	shorttitle = {{CompoNeRF}},
	url = {http://arxiv.org/abs/2303.13843},
	doi = {10.48550/arXiv.2303.13843},
	abstract = {Recent research endeavors have shown that combining neural radiance fields (NeRFs) with pre-trained diffusion models holds great potential for text-to-3D generation.However, a hurdle is that they often encounter guidance collapse when rendering complex scenes from multi-object texts. Because the text-to-image diffusion models are inherently unconstrained, making them less competent to accurately associate object semantics with specific 3D structures. To address this issue, we propose a novel framework, dubbed CompoNeRF, that explicitly incorporates an editable 3D scene layout to provide effective guidance at the single object (i.e., local) and whole scene (i.e., global) levels. Firstly, we interpret the multi-object text as an editable 3D scene layout containing multiple local NeRFs associated with the object-specific 3D box coordinates and text prompt, which can be easily collected from users. Then, we introduce a global MLP to calibrate the compositional latent features from local NeRFs, which surprisingly improves the view consistency across different local NeRFs. Lastly, we apply the text guidance on global and local levels through their corresponding views to avoid guidance ambiguity. This way, our CompoNeRF allows for flexible scene editing and re-composition of trained local NeRFs into a new scene by manipulating the 3D layout or text prompt. Leveraging the open-source Stable Diffusion model, our CompoNeRF can generate faithful and editable text-to-3D results while opening a potential direction for text-guided multi-object composition via the editable 3D scene layout.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Lin, Yiqi and Bai, Haotian and Li, Sijia and Lu, Haonan and Lin, Xiaodong and Xiong, Hui and Wang, Lin},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13843 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhu_cat-nerf_2023,
	title = {{CAT}-{NeRF}: {Constancy}-{Aware} {Tx}\${\textasciicircum}2\${Former} for {Dynamic} {Body} {Modeling}},
	shorttitle = {{CAT}-{NeRF}},
	url = {http://arxiv.org/abs/2304.07915},
	doi = {10.48550/arXiv.2304.07915},
	abstract = {This paper addresses the problem of human rendering in the video with temporal appearance constancy. Reconstructing dynamic body shapes with volumetric neural rendering methods, such as NeRF, requires finding the correspondence of the points in the canonical and observation space, which demands understanding human body shape and motion. Some methods use rigid transformation, such as SE(3), which cannot precisely model each frame's unique motion and muscle movements. Others generate the transformation for each frame with a trainable network, such as neural blend weight field or translation vector field, which does not consider the appearance constancy of general body shape. In this paper, we propose CAT-NeRF for self-awareness of appearance constancy with Tx\${\textasciicircum}2\$Former, a novel way to combine two Transformer layers, to separate appearance constancy and uniqueness. Appearance constancy models the general shape across the video, and uniqueness models the unique patterns for each frame. We further introduce a novel Covariance Loss to limit the correlation between each pair of appearance uniquenesses to ensure the frame-unique pattern is maximally captured in appearance uniqueness. We assess our method on H36M and ZJU-MoCap and show state-of-the-art performance.},
	urldate = {2023-04-19},
	publisher = {arXiv},
	author = {Zhu, Haidong and Zheng, Zhaoheng and Zheng, Wanrong and Nevatia, Ram},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07915 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_cerberus_2022,
	title = {Cerberus {Transformer}: {Joint} {Semantic}, {Affordance} and {Attribute} {Parsing}},
	shorttitle = {Cerberus {Transformer}},
	url = {http://arxiv.org/abs/2111.12608},
	doi = {10.48550/arXiv.2111.12608},
	abstract = {Multi-task indoor scene understanding is widely considered as an intriguing formulation, as the affinity of different tasks may lead to improved performance. In this paper, we tackle the new problem of joint semantic, affordance and attribute parsing. However, successfully resolving it requires a model to capture long-range dependency, learn from weakly aligned data and properly balance sub-tasks during training. To this end, we propose an attention-based architecture named Cerberus and a tailored training framework. Our method effectively addresses the aforementioned challenges and achieves state-of-the-art performance on all three tasks. Moreover, an in-depth analysis shows concept affinity consistent with human cognition, which inspires us to explore the possibility of weakly supervised learning. Surprisingly, Cerberus achieves strong results using only 0.1\%-1\% annotation. Visualizations further confirm that this success is credited to common attention maps across tasks. Code and models can be accessed at https://github.com/OPEN-AIR-SUN/Cerberus.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Chen, Xiaoxue and Liu, Tianyu and Zhao, Hao and Zhou, Guyue and Zhang, Ya-Qin},
	month = mar,
	year = {2022},
	note = {arXiv:2111.12608 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{caddeo_collision-aware_2023,
	title = {Collision-aware {In}-hand {6D} {Object} {Pose} {Estimation} using {Multiple} {Vision}-based {Tactile} {Sensors}},
	url = {http://arxiv.org/abs/2301.13667},
	doi = {10.48550/arXiv.2301.13667},
	abstract = {In this paper, we address the problem of estimating the in-hand 6D pose of an object in contact with multiple vision-based tactile sensors. We reason on the possible spatial configurations of the sensors along the object surface. Specifically, we filter contact hypotheses using geometric reasoning and a Convolutional Neural Network (CNN), trained on simulated object-agnostic images, to promote those that better comply with the actual tactile images from the sensors. We use the selected sensors configurations to optimize over the space of 6D poses using a Gradient Descent-based approach. We finally rank the obtained poses by penalizing those that are in collision with the sensors. We carry out experiments in simulation using the DIGIT vision-based sensor with several objects, from the standard YCB model set. The results demonstrate that our approach estimates object poses that are compatible with actual object-sensor contacts in \$87.5{\textbackslash}\%\$ of cases while reaching an average positional error in the order of \$2\$ centimeters. Our analysis also includes qualitative results of experiments with a real DIGIT sensor.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Caddeo, Gabriele M. and Piga, Nicola A. and Bottarel, Fabrizio and Natale, Lorenzo},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13667 [cs]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{michaux_cant_2023,
	title = {Can't {Touch} {This}: {Real}-{Time}, {Safe} {Motion} {Planning} and {Control} for {Manipulators} {Under} {Uncertainty}},
	shorttitle = {Can't {Touch} {This}},
	url = {http://arxiv.org/abs/2301.13308},
	doi = {10.48550/arXiv.2301.13308},
	abstract = {A key challenge to the widespread deployment of robotic manipulators is the need to ensure safety in arbitrary environments while generating new motion plans in real-time. In particular, one must ensure that a manipulator does not collide with obstacles, collide with itself, or exceed its joint torque limits. This challenge is compounded by the need to account for uncertainty in the mass and inertia of manipulated objects, and potentially the robot itself. The present work addresses this challenge by proposing Autonomous Robust Manipulation via Optimization with Uncertainty-aware Reachability (ARMOUR), a provably-safe, receding-horizon trajectory planner and tracking controller framework for serial link manipulators. ARMOUR works by first constructing a robust, passivity-based controller that is proven to enable a manipulator to track desired trajectories with bounded error despite uncertain dynamics. Next, ARMOUR uses a novel variation on the Recursive Newton-Euler Algorithm (RNEA) to compute the set of all possible inputs required to track any trajectory within a continuum of desired trajectories. Finally, the method computes an over-approximation to the swept volume of the manipulator; this enables one to formulate an optimization problem, which can be solved in real-time, to synthesize provably-safe motion. The proposed method is compared to state of the art methods and demonstrated on a variety of challenging manipulation examples in simulation and on real hardware, such as maneuvering a dumbbell with uncertain mass around obstacles.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Michaux, Jonathan and Holmes, Patrick and Zhang, Bohao and Chen, Che and Wang, Baiyue and Sahgal, Shrey and Zhang, Tiancheng and Dey, Sidhartha and Kousik, Shreyas and Vasudevan, Ram},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13308 [cs, eess, math]},
	keywords = {/unread, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
}

@misc{zhang_cardiac_2022,
	title = {Cardiac {Evidence} {Mining} for {Eating} {Monitoring} using {Collocative} {Electrocardiogram} {Imagining}},
	url = {https://www.techrxiv.org/articles/preprint/Cardiac_Evidence_Mining_for_Eating_Monitoring_using_Collocative_Electrocardiogram_Imagining/18093275/2},
	doi = {10.36227/techrxiv.18093275.v2},
	abstract = {Eating monitoring has remained an open challenge in medical research for years due to the lack of non-invasive sensors for continuous monitoring and the reliable methods for automatic behavior detection. In this paper, we present a pilot study using the wearable 24-hour ECG for sensing and tailoring the sophisticated deep learning for ad-hoc and interpretable detection. This is accomplished using a collocative learning framework in which 1) we construct collocative tensors as pseudo-images from 1D ECG signals to improve the feasibility of 2D image-based deep models; 2) we formulate the cardiac logic of analyzing the ECG data in a comparative way as periodic attention regulators so as to guide the deep inference to collect evidence in a human comprehensible manner; and 3) we improve the interpretability of the framework by enabling the backtracking of evidence with a set of methods designed for Class Activation Mapping (CAM) decoding and decision tree/forest generation. The effectiveness of the proposed framework has been validated on the largest ECG dataset of eating behavior with superior performance over conventional models, and its capacity of cardiac evidence mining has also been verified through the consistency of the evidence it backtracked and that of the previous medical studies.},
	language = {en},
	urldate = {2022-11-26},
	publisher = {TechRxiv},
	author = {Zhang, Xulu and Yang, Zhenqun and Jiang, Dongmei and Liao, Ga and Li, Qing and Jain, Ramesh and Wei, Xiaoyong},
	month = jan,
	year = {2022},
	keywords = {/unread},
}

@misc{lin_capturing_2022,
	title = {Capturing, {Reconstructing}, and {Simulating}: the {UrbanScene3D} {Dataset}},
	shorttitle = {Capturing, {Reconstructing}, and {Simulating}},
	url = {http://arxiv.org/abs/2107.04286},
	doi = {10.48550/arXiv.2107.04286},
	abstract = {We present UrbanScene3D, a large-scale data platform for research of urban scene perception and reconstruction. UrbanScene3D contains over 128k high-resolution images covering 16 scenes including large-scale real urban regions and synthetic cities with 136 km{\textasciicircum}2 area in total. The dataset also contains high-precision LiDAR scans and hundreds of image sets with different observation patterns, which provide a comprehensive benchmark to design and evaluate aerial path planning and 3D reconstruction algorithms. In addition, the dataset, which is built on Unreal Engine and Airsim simulator together with the manually annotated unique instance label for each building in the dataset, enables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D bounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with physical engine and lighting system not only produce variety of data but also enable users to simulate cars or drones in the proposed urban environment for future research.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Lin, Liqiang and Liu, Yilin and Hu, Yue and Yan, Xingguang and Xie, Ke and Huang, Hui},
	month = jul,
	year = {2022},
	note = {arXiv:2107.04286 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{zhang_beyond_2023,
	title = {Beyond {NeRF} {Underwater}: {Learning} {Neural} {Reflectance} {Fields} for {True} {Color} {Correction} of {Marine} {Imagery}},
	shorttitle = {Beyond {NeRF} {Underwater}},
	url = {http://arxiv.org/abs/2304.03384},
	doi = {10.48550/arXiv.2304.03384},
	abstract = {Underwater imagery often exhibits distorted coloration as a result of light-water interactions, which complicates the study of benthic environments in marine biology and geography. In this research, we propose an algorithm to restore the true color (albedo) in underwater imagery by jointly learning the effects of the medium and neural scene representations. Our approach models water effects as a combination of light attenuation with distance and backscattered light. The proposed neural scene representation is based on a neural reflectance field model, which learns albedos, normals, and volume densities of the underwater environment. We introduce a logistic regression model to separate water from the scene and apply distinct light physics during training. Our method avoids the need to estimate complex backscatter effects in water by employing several approximations, enhancing sampling efficiency and numerical stability during training. The proposed technique integrates underwater light effects into a volume rendering framework with end-to-end differentiability. Experimental results on both synthetic and real-world data demonstrate that our method effectively restores true color from underwater imagery, outperforming existing approaches in terms of color consistency.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Johnson-Roberson, Matthew},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03384 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{brissman_camera_2023,
	title = {Camera {Calibration} without {Camera} {Access} -- {A} {Robust} {Validation} {Technique} for {Extended} {PnP} {Methods}},
	url = {http://arxiv.org/abs/2302.06949},
	doi = {10.48550/arXiv.2302.06949},
	abstract = {A challenge in image based metrology and forensics is intrinsic camera calibration when the used camera is unavailable. The unavailability raises two questions. The first question is how to find the projection model that describes the camera, and the second is to detect incorrect models. In this work, we use off-the-shelf extended PnP-methods to find the model from 2D-3D correspondences, and propose a method for model validation. The most common strategy for evaluating a projection model is comparing different models' residual variances - however, this naive strategy cannot distinguish whether the projection model is potentially underfitted or overfitted. To this end, we model the residual errors for each correspondence, individually scale all residuals using a predicted variance and test if the new residuals are drawn from a standard normal distribution. We demonstrate the effectiveness of our proposed validation in experiments on synthetic data, simulating 2D detection and Lidar measurements. Additionally, we provide experiments using data from an actual scene and compare non-camera access and camera access calibrations. Last, we use our method to validate annotations in MegaDepth.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Brissman, Emil and Forssén, Per-Erik and Edstedt, Johan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06949 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{gal_bayesian_2016,
	title = {Bayesian {Convolutional} {Neural} {Networks} with {Bernoulli} {Approximate} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1506.02158},
	doi = {10.48550/arXiv.1506.02158},
	abstract = {Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data -- as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jan,
	year = {2016},
	note = {arXiv:1506.02158 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{tewari_advances_2022,
	title = {Advances in {Neural} {Rendering}},
	url = {http://arxiv.org/abs/2111.05849},
	doi = {10.48550/arXiv.2111.05849},
	abstract = {Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Tewari, Ayush and Thies, Justus and Mildenhall, Ben and Srinivasan, Pratul and Tretschk, Edgar and Wang, Yifan and Lassner, Christoph and Sitzmann, Vincent and Martin-Brualla, Ricardo and Lombardi, Stephen and Simon, Tomas and Theobalt, Christian and Niessner, Matthias and Barron, Jonathan T. and Wetzstein, Gordon and Zollhoefer, Michael and Golyanik, Vladislav},
	month = mar,
	year = {2022},
	note = {arXiv:2111.05849 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{ding_efficient_2023,
	title = {An {Efficient} {Convex} {Hull}-{Based} {Vehicle} {Pose} {Estimation} {Method} for {3D} {LiDAR}},
	url = {http://arxiv.org/abs/2302.01034},
	doi = {10.48550/arXiv.2302.01034},
	abstract = {Vehicle pose estimation is essential in the perception technology of autonomous driving. However, due to the different density distributions of the LiDAR point cloud, it is challenging to achieve accurate direction extraction based on 3D LiDAR by using the existing pose estimation methods. In this paper, we proposed a novel convex hull-based vehicle pose estimation method. The extracted 3D cluster is reduced to the convex hull, reducing the computation burden. Then a novel criterion based on the minimum occlusion area is developed for the search-based algorithm, which can achieve accurate pose estimation. The proposed algorithm is validated on the KITTI dataset and a manually labeled dataset acquired at an industrial park. The results show that our proposed method can achieve better accuracy than the three mainstream algorithms while maintaining real-time speed.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Ding, Ningning},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01034 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gauthier_alien_2023,
	title = {Alien {Coding}},
	url = {http://arxiv.org/abs/2301.11479},
	doi = {10.48550/arXiv.2301.11479},
	abstract = {We introduce a self-learning algorithm for synthesizing programs for OEIS sequences. The algorithm starts from scratch initially generating programs at random. Then it runs many iterations of a self-learning loop that interleaves (i) training neural machine translation to learn the correspondence between sequences and the programs discovered so far, and (ii) proposing many new programs for each OEIS sequence by the trained neural machine translator. The algorithm discovers on its own programs for more than 78000 OEIS sequences, sometimes developing unusual programming methods. We analyze its behavior and the invented programs in several experiments.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Gauthier, Thibault and Olšák, Miroslav and Urban, Josef},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11479 [cs, math]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Number Theory},
}

@misc{pascual_systematic_2023,
	title = {A {Systematic} {Review} on {Human} {Modeling}: {Digging} into {Human} {Digital} {Twin} {Implementations}},
	shorttitle = {A {Systematic} {Review} on {Human} {Modeling}},
	url = {http://arxiv.org/abs/2302.03593},
	doi = {10.48550/arXiv.2302.03593},
	abstract = {Human Digital Twins (HDTs) are digital replicas of humans that either mirror a complete human body, some parts of it as can be organs, flows, cells, or even human behaviors. An HDT is a human specific replica application inferred from the digital twin (DT) manufacturing concept, defined as a technique that creates digital replicas of physical systems or processes aimed at optimizing their performance and supporting more accurate decision-making processes. The main goal of this paper is to provide readers with a comprehensive overview of current efforts in the HDT field, by browsing its basic concepts, differences with DTs, existing developments, and the distinct areas of application. The review methodology includes an exhaustive review of scientific literature, patents, and industrial initiatives, as well as a discussion about ongoing and foreseen HDT research activity, emphasizing its potential benefits and limitations.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Pascual, Heribert and Bruin, Xavi Masip and Alonso, Albert and Cerdà, Judit},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03593 [cs]},
	keywords = {/unread, Computer Science - Other Computer Science},
}

@misc{sulzer_survey_2023,
	title = {A {Survey} and {Benchmark} of {Automatic} {Surface} {Reconstruction} from {Point} {Clouds}},
	url = {http://arxiv.org/abs/2301.13656},
	doi = {10.48550/arXiv.2301.13656},
	abstract = {We survey and benchmark traditional and novel learning-based algorithms that address the problem of surface reconstruction from point clouds. Surface reconstruction from point clouds is particularly challenging when applied to real-world acquisitions, due to noise, outliers, non-uniform sampling and missing data. Traditionally, different handcrafted priors of the input points or the output surface have been proposed to make the problem more tractable. However, hyperparameter tuning for adjusting priors to different acquisition defects can be a tedious task. To this end, the deep learning community has recently addressed the surface reconstruction problem. In contrast to traditional approaches, deep surface reconstruction methods can learn priors directly from a training set of point clouds and corresponding true surfaces. In our survey, we detail how different handcrafted and learned priors affect the robustness of methods to defect-laden input and their capability to generate geometric and topologically accurate reconstructions. In our benchmark, we evaluate the reconstructions of several traditional and learning-based methods on the same grounds. We show that learning-based methods can generalize to unseen shape categories, but their training and test sets must share the same point cloud characteristics. We also provide the code and data to compete in our benchmark and to further stimulate the development of learning-based surface reconstruction https://github.com/raphaelsulzer/dsr-benchmark.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Sulzer, Raphael and Landrieu, Loic and Marlet, Renaud and Vallet, Bruno},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13656 [cs]},
	keywords = {/unread, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sola_micro_2021,
	title = {A micro {Lie} theory for state estimation in robotics},
	url = {http://arxiv.org/abs/1812.01537},
	doi = {10.48550/arXiv.1812.01537},
	abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Solà, Joan and Deray, Jeremie and Atchuthan, Dinesh},
	month = dec,
	year = {2021},
	note = {arXiv:1812.01537 [cs]},
	keywords = {/unread, Computer Science - Robotics},
}

@misc{tzathas_3d_2023,
	title = {{3D} {Neural} {Sculpting} ({3DNS}): {Editing} {Neural} {Signed} {Distance} {Functions}},
	shorttitle = {{3D} {Neural} {Sculpting} ({3DNS})},
	url = {http://arxiv.org/abs/2209.13971},
	doi = {10.48550/arXiv.2209.13971},
	abstract = {In recent years, implicit surface representations through neural networks that encode the signed distance have gained popularity and have achieved state-of-the-art results in various tasks (e.g. shape representation, shape reconstruction, and learning shape priors). However, in contrast to conventional shape representations such as polygon meshes, the implicit representations cannot be easily edited and existing works that attempt to address this problem are extremely limited. In this work, we propose the first method for efficient interactive editing of signed distance functions expressed through neural networks, allowing free-form editing. Inspired by 3D sculpting software for meshes, we use a brush-based framework that is intuitive and can in the future be used by sculptors and digital artists. In order to localize the desired surface deformations, we regulate the network by using a copy of it to sample the previously expressed surface. We introduce a novel framework for simulating sculpting-style surface edits, in conjunction with interactive surface sampling and efficient adaptation of network weights. We qualitatively and quantitatively evaluate our method in various different 3D objects and under many different edits. The reported results clearly show that our method yields high accuracy, in terms of achieving the desired edits, while at the same time preserving the geometry outside the interaction areas.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Tzathas, Petros and Maragos, Petros and Roussos, Anastasios},
	month = jan,
	year = {2023},
	note = {arXiv:2209.13971 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{lu_large-scale_2023,
	title = {A {Large}-{Scale} {Outdoor} {Multi}-modal {Dataset} and {Benchmark} for {Novel} {View} {Synthesis} and {Implicit} {Scene} {Reconstruction}},
	url = {http://arxiv.org/abs/2301.06782},
	doi = {10.48550/arXiv.2301.06782},
	abstract = {Neural Radiance Fields (NeRF) has achieved impressive results in single object scene reconstruction and novel view synthesis, which have been demonstrated on many single modality and single object focused indoor scene datasets like DTU, BMVS, and NeRF Synthetic.However, the study of NeRF on large-scale outdoor scene reconstruction is still limited, as there is no unified outdoor scene dataset for large-scale NeRF evaluation due to expensive data acquisition and calibration costs. In this paper, we propose a large-scale outdoor multi-modal dataset, OMMO dataset, containing complex land objects and scenes with calibrated images, point clouds and prompt annotations. Meanwhile, a new benchmark for several outdoor NeRF-based tasks is established, such as novel view synthesis, surface reconstruction, and multi-modal NeRF. To create the dataset, we capture and collect a large number of real fly-view videos and select high-quality and high-resolution clips from them. Then we design a quality review module to refine images, remove low-quality frames and fail-to-calibrate scenes through a learning-based automatic evaluation plus manual review. Finally, a number of volunteers are employed to add the text descriptions for each scene and key-frame to meet the potential multi-modal requirements in the future. Compared with existing NeRF datasets, our dataset contains abundant real-world urban and natural scenes with various scales, camera trajectories, and lighting conditions. Experiments show that our dataset can benchmark most state-of-the-art NeRF methods on different tasks. We will release the dataset and model weights very soon.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Lu, Chongshan and Yin, Fukun and Chen, Xin and Chen, Tao and YU, Gang and Fan, Jiayuan},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06782 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lee_qff_2022,
	title = {{QFF}: {Quantized} {Fourier} {Features} for {Neural} {Field} {Representations}},
	shorttitle = {{QFF}},
	url = {https://arxiv.org/abs/2212.00914v1},
	abstract = {Multilayer perceptrons (MLPs) learn high frequencies slowly. Recent approaches encode features in spatial bins to improve speed of learning details, but at the cost of larger model size and loss of continuity. Instead, we propose to encode features in bins of Fourier features that are commonly used for positional encoding. We call these Quantized Fourier Features (QFF). As a naturally multiresolution and periodic representation, our experiments show that using QFF can result in smaller model size, faster training, and better quality outputs for several applications, including Neural Image Representations (NIR), Neural Radiance Field (NeRF) and Signed Distance Function (SDF) modeling. QFF are easy to code, fast to compute, and serve as a simple drop-in addition to many neural field representations.},
	language = {en},
	urldate = {2023-04-22},
	author = {Lee, Jae Yong and Wu, Yuqun and Zou, Chuhang and Wang, Shenlong and Hoiem, Derek},
	month = dec,
	year = {2022},
	keywords = {/unread},
}

@misc{hu_multiscale_2023,
	title = {Multiscale {Representation} for {Real}-{Time} {Anti}-{Aliasing} {Neural} {Rendering}},
	url = {https://arxiv.org/abs/2304.10075v1},
	abstract = {The rendering scheme in neural radiance field (NeRF) is effective in rendering a pixel by casting a ray into the scene. However, NeRF yields blurred rendering results when the training images are captured at non-uniform scales, and produces aliasing artifacts if the test images are taken in distant views. To address this issue, Mip-NeRF proposes a multiscale representation as a conical frustum to encode scale information. Nevertheless, this approach is only suitable for offline rendering since it relies on integrated positional encoding (IPE) to query a multilayer perceptron (MLP). To overcome this limitation, we propose mip voxel grids (Mip-VoG), an explicit multiscale representation with a deferred architecture for real-time anti-aliasing rendering. Our approach includes a density Mip-VoG for scene geometry and a feature Mip-VoG with a small MLP for view-dependent color. Mip-VoG encodes scene scale using the level of detail (LOD) derived from ray differentials and uses quadrilinear interpolation to map a queried 3D location to its features and density from two neighboring downsampled voxel grids. To our knowledge, our approach is the first to offer multiscale training and real-time anti-aliasing rendering simultaneously. We conducted experiments on multiscale datasets, and the results show that our approach outperforms state-of-the-art real-time rendering baselines.},
	language = {en},
	urldate = {2023-04-22},
	author = {Hu, Dongting and Zhang, Zhenkai and Hou, Tingbo and Liu, Tongliang and Fu, Huan and Gong, Mingming},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{mittal_neural_2023,
	title = {Neural {Radiance} {Fields}: {Past}, {Present}, and {Future}},
	shorttitle = {Neural {Radiance} {Fields}},
	url = {https://arxiv.org/abs/2304.10050v1},
	abstract = {The various aspects like modeling and interpreting 3D environments and surroundings have enticed humans to progress their research in 3D Computer Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in Computer Graphics, Robotics, Computer Vision, and the possible scope of High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D models have gained traction from res with more than 500 preprints related to NeRFs published. This paper serves as a bridge for people starting to study these fields by building on the basics of Mathematics, Geometry, Computer Vision, and Computer Graphics to the difficulties encountered in Implicit Representations at the intersection of all these disciplines. This survey provides the history of rendering, Implicit Learning, and NeRFs, the progression of research on NeRFs, and the potential applications and implications of NeRFs in today's world. In doing so, this survey categorizes all the NeRF-related research in terms of the datasets used, objective functions, applications solved, and evaluation criteria for these applications.},
	language = {en},
	urldate = {2023-04-22},
	author = {Mittal, Ansh},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{tao_lidar-nerf_2023,
	title = {{LiDAR}-{NeRF}: {Novel} {LiDAR} {View} {Synthesis} via {Neural} {Radiance} {Fields}},
	shorttitle = {{LiDAR}-{NeRF}},
	url = {https://arxiv.org/abs/2304.10406v1},
	abstract = {We introduce a new task, novel view synthesis for LiDAR sensors. While traditional model-based LiDAR simulators with style-transfer neural networks can be applied to render novel views, they fall short in producing accurate and realistic LiDAR patterns, because the renderers they rely on exploit game engines, which are not differentiable. We address this by formulating, to the best of our knowledge, the first differentiable LiDAR renderer, and propose an end-to-end framework, LiDAR-NeRF, leveraging a neural radiance field (NeRF) to enable jointly learning the geometry and the attributes of 3D points. To evaluate the effectiveness of our approach, we establish an object-centric multi-view LiDAR dataset, dubbed NeRF-MVL. It contains observations of objects from 9 categories seen from 360-degree viewpoints captured with multiple LiDAR sensors. Our extensive experiments on the scene-level KITTI-360 dataset, and on our object-level NeRF-MVL show that our LiDAR- NeRF surpasses the model-based algorithms significantly.},
	language = {en},
	urldate = {2023-04-22},
	author = {Tao, Tang and Gao, Longfei and Wang, Guangrun and Chen, Peng and Hao, Dayang and Liang, Xiaodan and Salzmann, Mathieu and Yu, Kaicheng},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{toschi_relight_2023,
	title = {{ReLight} {My} {NeRF}: {A} {Dataset} for {Novel} {View} {Synthesis} and {Relighting} of {Real} {World} {Objects}},
	shorttitle = {{ReLight} {My} {NeRF}},
	url = {https://arxiv.org/abs/2304.10448v1},
	abstract = {In this paper, we focus on the problem of rendering novel views from a Neural Radiance Field (NeRF) under unobserved light conditions. To this end, we introduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world objects under one-light-at-time (OLAT) conditions, annotated with accurate ground-truth camera and light poses. Our acquisition pipeline leverages two robotic arms holding, respectively, a camera and an omni-directional point-wise light source. We release a total of 20 scenes depicting a variety of objects with complex geometry and challenging materials. Each scene includes 2000 images, acquired from 50 different points of views under 40 different OLAT conditions. By leveraging the dataset, we perform an ablation study on the relighting capability of variants of the vanilla NeRF architecture and identify a lightweight architecture that can render novel views of an object under novel light conditions, which we use to establish a non-trivial baseline for the dataset. Dataset and benchmark are available at https://eyecan-ai.github.io/rene.},
	language = {en},
	urldate = {2023-04-22},
	author = {Toschi, Marco and De Matteo, Riccardo and Spezialetti, Riccardo and De Gregorio, Daniele and Di Stefano, Luigi and Salti, Samuele},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{xu_revisiting_2023,
	title = {Revisiting {Implicit} {Neural} {Representations} in {Low}-{Level} {Vision}},
	url = {https://arxiv.org/abs/2304.10250v1},
	abstract = {Implicit Neural Representation (INR) has been emerging in computer vision in recent years. It has been shown to be effective in parameterising continuous signals such as dense 3D models from discrete image data, e.g. the neural radius field (NeRF). However, INR is under-explored in 2D image processing tasks. Considering the basic definition and the structure of INR, we are interested in its effectiveness in low-level vision problems such as image restoration. In this work, we revisit INR and investigate its application in low-level image restoration tasks including image denoising, super-resolution, inpainting, and deblurring. Extensive experimental evaluations suggest the superior performance of INR in several low-level vision tasks with limited resources, outperforming its counterparts by over 2dB. Code and models are available at https://github.com/WenTXuL/LINR},
	language = {en},
	urldate = {2023-04-22},
	author = {Xu, Wentian and Jiao, Jianbo},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{lin_3d_2022,
	title = {{3D} {GAN} {Inversion} for {Controllable} {Portrait} {Image} {Animation}},
	url = {https://arxiv.org/abs/2203.13441v1},
	abstract = {Millions of images of human faces are captured every single day; but these photographs portray the likeness of an individual with a fixed pose, expression, and appearance. Portrait image animation enables the post-capture adjustment of these attributes from a single image while maintaining a photorealistic reconstruction of the subject's likeness or identity. Still, current methods for portrait image animation are typically based on 2D warping operations or manipulations of a 2D generative adversarial network (GAN) and lack explicit mechanisms to enforce multi-view consistency. Thus these methods may significantly alter the identity of the subject, especially when the viewpoint relative to the camera is changed. In this work, we leverage newly developed 3D GANs, which allow explicit control over the pose of the image subject with multi-view consistency. We propose a supervision strategy to flexibly manipulate expressions with 3D morphable models, and we show that the proposed method also supports editing appearance attributes, such as age or hairstyle, by interpolating within the latent space of the GAN. The proposed technique for portrait image animation outperforms previous methods in terms of image quality, identity preservation, and pose transfer while also supporting attribute editing.},
	language = {en},
	urldate = {2023-04-21},
	author = {Lin, Connor Z. and Lindell, David B. and Chan, Eric R. and Wetzstein, Gordon},
	month = mar,
	year = {2022},
	keywords = {/unread},
}

@misc{chan_generative_2023,
	title = {Generative {Novel} {View} {Synthesis} with {3D}-{Aware} {Diffusion} {Models}},
	url = {https://arxiv.org/abs/2304.02602v1},
	abstract = {We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method's ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.},
	language = {en},
	urldate = {2023-04-21},
	author = {Chan, Eric R. and Nagano, Koki and Chan, Matthew A. and Bergman, Alexander W. and Park, Jeong Joon and Levy, Axel and Aittala, Miika and De Mello, Shalini and Karras, Tero and Wetzstein, Gordon},
	month = apr,
	year = {2023},
	keywords = {/unread},
}

@misc{shue_3d_2022,
	title = {{3D} {Neural} {Field} {Generation} using {Triplane} {Diffusion}},
	url = {https://arxiv.org/abs/2211.16677v1},
	abstract = {Diffusion models have emerged as the state-of-the-art for image generation, among other tasks. Here, we present an efficient diffusion-based model for 3D-aware generation of neural fields. Our approach pre-processes training data, such as ShapeNet meshes, by converting them to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Thus, our 3D training scenes are all represented by 2D feature planes, and we can directly train existing 2D diffusion models on these representations to generate 3D neural fields with high quality and diversity, outperforming alternative approaches to 3D-aware generation. Our approach requires essential modifications to existing triplane factorization pipelines to make the resulting features easy to learn for the diffusion model. We demonstrate state-of-the-art results on 3D generation on several object classes from ShapeNet.},
	language = {en},
	urldate = {2023-04-21},
	author = {Shue, J. Ryan and Chan, Eric Ryan and Po, Ryan and Ankner, Zachary and Wu, Jiajun and Wetzstein, Gordon},
	month = nov,
	year = {2022},
	keywords = {/unread},
}

@phdthesis{wojciech_efficient_nodate,
	title = {Efficient {Monte} {Carlo} {Methods} for {Light} {Transport} in {Scattering} {Media}},
	abstract = {In this dissertation we focus on developing accurate and efficient Monte Carlo methods for synthesizing images containing general participating media. Participating media such as clouds, smoke, and fog are ubiquitous in the world and are responsible for many important visual phenomena which are of interest to computer graphics as well as related fields. In these situations, the medium participatesin lighting interactions by scattering or absorbing photons as they travel through the scene. Though these effects add atmosphere and considerable depth to rendered images they are computationally very expensive to simulate. Most practical solutions make significant simplifying assumptions about the medium in order to maintain efficiency. Unfortunately, simulating light transport in general scattering media efficiently and with high accuracy is a challenging undertaking. In this dissertation, we address this problem by presenting two complementary techniques for efficiently computing lighting within arbitrary participating media.

We first turn to the irradiance caching method for computing indirect illumination on surfaces. Irradiance caching gains efficiency by computing an accurate representation of lighting only at a sparse set of locations and reusing these values through interpolation whenever possible. We derive the mathematical concepts that form the foundation of this approach and analyze its strengths and weaknesses. Drawing inspiration from this algorithm, we then introduce a novel volumetric radiance caching method for efficiently simulating global illumination within participating media. In developing the technique we also introduce efficient methods for evaluating the gradient of the lighting within participating media. Our gradient analysis has immediate applicability for improved interpolation quality in both surface and media-based caching methods.

We also develop a novel photon mapping technique for participating media. We present a theoretical reformulation of volumetric photon mapping which provides significant new insights. This reformulation makes it easier to qualify the error introduced by the radiance estimate, but, more importantly, also allows us to develop more efficient rendering techniques. Conventional photon mapping methods compute lighting by first simulating the propagation of photons from light sources and then using this collection of photons to estimate lighting at any pointin the scene. Our reformulation allows us to compute the accumulated lighting along the length of entire rays as they penetrate the medium. This algorithmic improvement provides for significantly reduced render times and even the potential for real-time visualization of light transport in participating media.},
	school = {UCSD},
	author = {Wojciech, Jarosz},
	note = {titleTranslation:
titleTranslation:},
	keywords = {/unread},
}

@book{__nodate,
	title = {矩阵力量},
	author = {生姜},
	keywords = {/unread},
}

@book{__nodate-1,
	title = {数学要素},
	url = {https://github.com/Visualize-ML/Book3_Elements-of-Mathematics},
	author = {生姜, DrGinger},
	keywords = {/unread},
}

@phdthesis{noauthor_uncertainty_nodate,
	title = {Uncertainty in {Deep} {Learning} ({PhD} {Thesis}) {\textbar} {Yarin} {Gal} - {Blog} {\textbar} {Oxford} {Machine} {Learning}},
	url = {https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html},
	abstract = {So I finally submitted my PhD thesis, collecting already published results on how to obtain uncertainty in deep learning, and lots of bits and pieces of new research I had lying around...},
	urldate = {2022-10-14},
}
