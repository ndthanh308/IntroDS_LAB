\section{Related Work}

% \subsection{Driving View Simulation}
% % We validate that Neural Point
% % Light Fields make it possible to predict videos along unseen
% % trajectories previously only feasible to generate by explicitly modeling the scene.
% The current mainstream simulation softwares (Udacity, Carla, AriSim, Apollo \etc) are developed based on the game engine (Unity, Unreal4 \etc).
% As a data-driven simulation engine, VISTA could transform real-world datasets into virtual environments, and avoide some traditional issues of simulators, such as their lack of photorealism and ability to accurately model reality.
    
% With the rise of deep learning now, simulation has further developed in the field of autonomous driving. The automatic driving platform collects data through simulation, which can greatly increase the training time, far exceeding the time of road test, and speed up the model iteration speed.

% \chenming{Learning Robust Control Policies For End-To-End Autonomous Driving From Data-Driven Simulation}
\subsection{Driving View Simulation}
The use of AD simulation has become increasingly popular in recent years, as it provides a valuable tool for verifying planning and control systems, synthesizing data for training and testing, and significantly reducing the time required for these tasks. There are currently two major types of simulators in use: model-based and data-driven. Model-based simulators, such as PyBullet~\cite{coumans2016pybullet}, MuJoCo~\cite{todorov2012mujoco}, and CARLA~\cite{dosovitskiy2017carla}, use computer graphics techniques to simulate vehicles and environments. However, creating these models and vehicle movements is often an expensive and time-consuming manual task, and the resulting images may not always be realistic enough, which can lead to degraded performance in deploying perception systems. Data-driven simulators, like AADS~\cite{li2019aads} and VISTA~\cite{amini2022vista, amini2020learning}, overcome these issues by using real-world datasets to create photorealistic simulations that are fully annotated and ready for training and testing of autonomous driving systems. Their driving view synthesis algorithms are largely based on conventional projection-based methods. A concurrent work UniSim~\cite{yang2023unisim} proposes an end-to-end simulation system using NeRF. Our method also builds upon the NeRF technique, which is capable of synthesizing photorealistic images, outperforming conventional view synthesis algorithms.


% AD simulation is used to verify planning and control systems, synthesize data for training and testing, significantly reducing the time, far exceeding that of road tests, and speeding up model iteration speed. The usage of simulator in AD and robotics grows rapidly in recent years. Existing simulators can be categories into model-based~\cite{coumans2016pybullet,todorov2012mujoco, dosovitskiy2017carla} and data-driven~\cite{amini2020learning,wang2022learning,manivasagam2020lidarsim,wang2020v2vnet,wangcadsim}. For example, CARLA~\cite{dosovitskiy2017carla} uses game engine and computer graphics models to simulate vehicles and environments. Creating CG models and vehicle movements remains an expensive and time-consuming manual task. Furthermore, these images are not always realistic enough, which can lead to degraded performance.
% AADS~\cite{li2019aads} enhances real-world pictures with simulated traffic flow to create photorealistic simulation images and renderings. The resulting images are photorealistic, fully annotated, and ready for training and testing of autonomous driving systems from perception to planning.
% Similarly, VISTA~\cite{amini2022vista} is a data-driven simulation engine used for autonomous driving perception and control. VISTA can transform real-world datasets into virtual environments, avoiding traditional problems of simulators, such as their lack of photorealism and inability to accurately model reality. It uses depth estimation and bilinearly interpolate projected 3D points to sythesize novel views, while our method is built upon NeRF technique, which has the capability of synthesizing more photorealistic images compared to conventional view synthesis algorithms.


% Figure environment removed


\subsection{Neural View Synthesis (NVS)}
NVS is a long-standing task in the vision community, with significant potential for use in robotics. Recent surveys of methods can be found in~\cite{tewari2020state,tewari2021advances}.
Traditional novel view synthesis methods can be classified into three categories: image-based~\cite{gortler1996lumigraph,levoy1996light}, learning-based~\cite{niklaus20193d,rockwell2021pixelsynth}, and geometry-based~\cite{riegler2020free,riegler2021stable} approaches. 
% Image-based methods warp and blend relevant patches in the inputs to generate new views based on image quality measurements~\cite{gortler1996lumigraph,levoy1996light}. Learning-based methods predict blending weights and view-dependent effects via neural networks or other hand-crafted heuristics~\cite{hedman2018deep,riegler2020free,thies2020image}. 
% With the advance of deep learning, researchers also studied how to synthesize novel views even with a single image, but it often requires large amounts of training data~\cite{niklaus20193d,rockwell2021pixelsynth,tucker2020single,wiles2020synsin}. 
% Different from the above methods, geometry-based methods~\cite{riegler2020free,riegler2021stable} need to reconstruct an explicit 3D model first and render images at target viewpoints. 
% For example, Aliev~\etal~\cite{aliev2020neural} assigned multi-resolution features to point clouds to perform neural rendering, and Thies~\etal~\cite{thies2019deferred} stored neural textures on 3D meshes and then rendered the novel view with a traditional graphics pipeline. 
% Other geometry representations include multi-planes images~\cite{flynn2019deepview,li2021mine,mildenhall2019local}, voxel grids~\cite{henzler2020learning,penner2017soft}, depth~\cite{riegler2020free,riegler2021stable,tulsiani2018layer}.
% While discrete representations are often used for producing high-quality results, they can be data- and memory-intensive. Additionally, the accuracy of reconstructed geometry can limit the rendering resolution. 
Recently, neural implicit representations have shown promise for novel view synthesis.
NeRF~\cite{mildenhall2020nerf} is a seminal work that learns a continuous function to map spatial coordinates to density and color using MLPs to model a 5D radiance field. Subsequent works have extended NeRF to various scenarios, such as non-rigid and dynamic scenes~\cite{park:nerfies,park:hypernerf,Guo_2022_NDVG_ACCV}, larger unbounded scenes~\cite{zhang2020nerf++,tancik2022block,xiangli2022bungeenerf,rematas2022urban,martin2021nerf}, relighting~\cite{boss2021nerd,srinivasan2021nerv,zhang2021nerfactor}, and generalization ability~\cite{chen2021mvsnerf,trevithick2021grf,yu2021_pixelnerf_cvpr21,wang2021_ibrnet_cvpr21}. Additionally, some methods have been developed to optimize neural rendering more efficiently, such as~\cite{rebain2021_derf_cvpr21,Reiser2021_kiloNeRF_iccv21}, which subdivides the scene into multiple cells for efficient processing, and~\cite{yu2021plenoxels,sun2021direct,muller2022instant}, which exploit voxel-grid representations to speed up the optimization of radiance fields.
%\cite{neff2021_donerf_egsr21,lindell2021_autoint_cvpr21, piala2021terminerf, liu2020_nsvf_nips20,yu2021_plenoctrees_iccv21,lombardi2021mixture} focus more on efficient sampling along each ray for color accumulation, and

For novel view synthesis of street view, Block-NeRF~\cite{tancik2022block} decomposes a scene into blocks and trains NeRF individually along with appearance embeddings, learned to pose refinement, and controllable exposure. Urban-NeRF~\cite{rematas2022urban} and S-NeRF~\cite{anonymous2023snerf} incorporate LiDAR observations to supervise NeRF's depth to deal with unconstrained geometry in street views. 
In READ~\cite{Li23READ}, a point cloud representation is used for large-scale driving scenarios, which require pre-processing and may have view inconsistency problems. Kundu et al.~\cite{kundu2022panoptic} further extend NeRF to learn 3D semantic information in outdoor scenes.
In contrast to previous work, our paper is the first to use HD Map to assist in training NVS models.



\subsection{Limited Input View Synthesis with Geometric Constraint}
To improve NeRF's performance with fewer training views, several methods have been proposed. 
DS-NeRF \cite{deng2022depth} utilizes depth supervision to optimize a scene with a limited number of images. SynSin~\cite{wiles2020synsin} proposes a technique for synthesizing single-image views by warping the depth map and image features to generate new views of a scene from a single input image. 
GeoAug~\cite{chen2022geoaug} proposes a data augmentation method for NeRF that is based on geometry constraints with implicit depth supervision. Since ground truth images are not available for novel views, the rendered images of a novel pose are warped to the nearby training view based on the predicted depth maps and relative pose to match the RGB image supervision. GeCoNeRF\cite{kwak2023geconerf} warps the image of the seen viewpoint to the adjacent unseen viewpoint through depth and supervises the consistency at the image feature level. FWD~\cite{cao2022fwd} utilizes explicit depths and point cloud renders for fast rendering and fuses the warped features from multiple images to synthesize new images.
Unlike existing methods that use unsupervised or augmentation approaches, our technique leverages map priors to guide the training of the neural radiance field.