
\section{Experiments}
We assessed the efficacy of our proposed approach in comparison to state-of-the-art NeRF methods. Our focus was on its capability to extrapolate beyond the original trajectory and interpolate novel view trajectories.
In addition to the rendered images presented in this section, we have compiled the synthesized images into video clips for qualitative evaluation in the supplemental video.
% We verify the effectiveness of our proposed method against state-of-the-art NeRF methods from the perspectives of novel view trajectory interpolation and out-of-trajectory extrapolation. 
% Note that, besides the rendered images shown in this section, we also compile the synthesized images into video clips for qualitative evaluation in the supplemental video.

% % Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{Baselines}
\input{tables/interpolation}
\input{tables/extrapolation}
% The state-of-the-art NeRF method for unbounded scene is Mip-NeRF 360~\cite{barron2022mip}. 
Mip-NeRF 360~\cite{barron2022mip} achieves impressive performance on unbounded real-world scenes.
However, its training and testing speeds are relatively slower when compared to neural graphic primitive (NGP)-based methods~\cite{muller2022instant}. In our paper, we choose a recently proposed method -- Nerfacto~\cite{tancik2023nerfstudio} as our baseline, which combines Mip-NeRF 360's strengths and the advantages of Instant-NGP. For the fairness, we opted to disable camera pose optimization and appearance embedding when presenting the evaluation metrics, as these could potentially impact computational results (optimized poses have no groundtruth images to evaluate). For simplicity, we refer to our utilized baseline method as ``NeRF-base". 
We implement our method by PyTorch, and run all experiments on a single NVIDIA Tesla V100 GPU. 
The initial learning rate is $2e^{-3}$ and decays to $1\times 10^{-4}$ for $30,000$ iterations in all experiments.

\subsection{Dataset and Metrics}

To evaluate the proposed method, we train MAP-NeRF on Argoverse2~\cite{wilson2023argoverse} , which has been specifically designed for autonomous driving. For our experiments, we utilized only the dataset's images, poses, and maps. To simulate forward driving scenarios, we only employed front-facing cameras (\ie left-rear, center, and right-rear). Our method and the baseline method were trained on six selected sequences from the Argoverse2 dataset, from which four of them are evaluated and visualized in the paper, and the others are shown in the supplemental video. Each sequence has $319$ frames, and image size is $1550\times 2048$.
% the input images are all with $width=2048$ and $height=1550$.

We evaluate every $9$ frames in the sequences, and the others will be used for the scene training.
To determine the pose of the images, we rely on the odometry poses provided by the dataset. To assess the quality of the synthesized images, we use a range of metrics that are commonly employed, including Peak Signal-to-Noise Ratio (PSNR), i.e., the $-log_{10}(\mathrm{MSE})$, Structural Similarity (SSIM)~\cite{wang2004image}, and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable}. These metrics allow for a comprehensive evaluation of the image quality.

\subsection{Evaluation Results and Comparisons} 
\subsubsection{Novel View Trajectory Interpolation}
We compare our Map-NeRF with NeRF-base in terms of evaluation metrics on novel view trajectory interpolation.
% Our method exhibits similar performance compared to the one of NeRF-base, evaluated values can be found in \tabref{tab:eval_nvs_metrics_interpolation}. 
As shown in \tabref{tab:eval_nvs_metrics_interpolation}, our method exhibits marginally better performance compared to the one of NeRF-base.
For a qualitative comparison, please refer to the supplemental video.
%\jiadai{The metrics here are not prominent and there are no interesting visualizations. It is relatively thin. You can consider putting it later or enriching the description.}

\subsubsection{Out-of-Trajectory View Extrapolation}

% Figure environment removed
% % Figure environment removed
This section presents both qualitative and quantitative results regarding deviated trajectory views. The qualitative results are shown in \figref{figExtrapolation}. We train our method against the baseline and render images on deviated views for qualitative evaluation.
Quantitative evaluation is challenging due to the lack of ground truth data. To address this challenge, we use a trajectory containing lane changes from the Argoverse2 dataset, as displayed in \figref{figAblation}. We hold out successive $50$ frames from the trajectory of lane changes for training and use them as ground truth images for quantitative evaluation. Experimental results refer to \tabref{tab:eval_nvs_metrics_extrapolation}. While this approach is not identical to view extrapolation, it approximates it and presents a challenging scenario for all NeRF methods. This can demonstrate the ability of our method for view extrapolation.


\subsection{Ablation Study}
We conduct several ablation experiments to further validate the effectiveness of the proposed three components, \ie, $\mathcal{L}_{gd}$, $\mathcal{L}_{v}$ and uncertainty tempering (UT in short). 
The ablation study is conducted on the same dataset we use for out-of-trajectory view extrapolation, but with different combinations of the three components. 
As shown in \tabref{tab:loss_ablation}, our final setup (g) using all losses gains an improvement of $2.06$ PSNR compared to (a), which only use $\mathcal{L}_{rgd}$ to supervise. As we add ground density supervision, multi-view consistency supervision and uncertainty tempering, we see consistent improvements in our test scene. This indicates that our proposed methods are suitable way to improve the deviated view synthesis.

\input{tables/ablation_study}

\subsection{Simulation Showcase}
This section provides an example of using our proposed method to simulate realistic visual sensors by rendering deviated views. We extract a few images from these views and apply a pre-trained lane segmentation algorithm, CondLaneNet trained on CULane dataset~\cite{liu2021condlanenet}, to obtain perception results. As shown in \figref{figLaneDet}, our proposed method produces more plausible and consistent results compared to the baselines. This demonstrates the potential of our method for use in building a data-driven AD simulator, and its ability to produce semantically meaningful results.