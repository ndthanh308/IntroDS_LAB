% \usepackage{listings}
% \usepackage{xcolor}

% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{codered}{rgb}{0.79,0.15,0.15}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},   
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{codepurple},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codered},
%     basicstyle=\ttfamily\footnotesize,
%     breakatwhitespace=false,         
%     breaklines=true,                 
%     captionpos=b,                    
%     keepspaces=true,                 
%     numbers=left,                    
%     numbersep=5pt,                  
%     showspaces=false,                
%     showstringspaces=false,
%     showtabs=false,                  
%     tabsize=2
% }

\newcommand{\co}{{\color{blue}\{crystal\}}}
\newcommand{\pc}{{\color{blue}\{peter\}}}
% \newcommand{\egu}{{\color{blue}\{edward\}}} % already defined in macros.tex
\section{Design-To-Performance}
\label{sec:design_to_perf}

To assess the suitability of a particular design, it is common to evaluate \textit{performance metrics} based on features of the design, such as geometry and materials used. Common metrics include mechanical performance, dynamic functionality, or adherence to geometric restrictions. It is common to compute performance with respect to an individual criterion or multiple criteria. The purpose of this evaluation can be to form a set of quantitative metrics to describe the design further, as a foundation for numerical optimization or to verify whether a design meets given specifications. This performance assessment can result in a single quantitative result or an array of results. A more complex design evaluation can further classify or compare between designs in order to enable further optimization or to select a final part for production. 

Within the range of performance evaluation, there are objective, semi-subjective, and subjective criteria that all contribute to the final design performance. Objective criteria include quantitative features that are calculable or measurable, including features such as object weight, size, load capacity, impact resistance, speed, battery life, vibration resistance, and price. Semi-subjective criteria include features that are generally agreed upon but require some insight or estimations to evaluate. Such criteria may be evaluated by proxy measurements, and may vary based on the evaluator, the culture, or the use case; examples include ergonomics, product lifespan, sustainability, portability, safety, and accessibility. Subjective criteria include features that may differ markedly based on the evaluator, such as comfort, aesthetics, customer satisfaction, novelty, and value. With this in mind, we aim to answer the following pair of questions: 

\begin{itemize}
    \item \textbf{Q1} Can \gpt evaluate the performance of an input design that is consistent with classical, objective metrics?
    \item \textbf{Q2} Can \gpt support performance evaluation in ways not possible with classical approaches, such as using semi\nobreakdash-subjective and subjective metrics?
\end{itemize}

This section describes the current abilities of \gpt and identifies best practices, limitations, and full failures in its capabilities to address each of these questions through the use of several examples per question.

Evaluations were tested using different input styles (\eg, method of design description) and requested output forms (\eg, direct classification or function creation). Demonstrative examples are shown in Figure~\ref{fig:performance_prompt_design}. We did not test all combinations of design style and output requests but focused on key comparisons and types. In particular, to address Q1 we focused on comparing text-based designs (DS1) and generic designs (DS2), comparing output requests for direct evaluation in a text response (RF1) and evaluation by the creation of a function (RF2), and comparing code-based designs described with salient semantics (DS3) and no semantics (DS4). To address Q2 with more subjective features we also tested output requests for categorization (RF3) along with ranking and pairwise comparisons between designs, and separately used scoring (RF4) with varying levels of complexity. 

% Figure environment removed 
% \wojciech{This is not clear. Expand what each of these mean.}


\subsection{Objective Evaluation (Q1)} 
\label{designToPerformance_Objective}

Once a design or design space has been created, a typical design process proceeds by evaluating basic geometric features such as size, weight, and strength of the object. In effect, this answers the question: does the item do what it was created to do? Most typically, certain features need to satisfy functional requirements in order to be suitable designs. 

\subsubsection{Mechanical properties}

Here, we focus on analyzing the mechanical integrity of (1) a chair and (2) a cabinet. 
We began with a simple input design in text form (DS1) and a request for direct evaluation in calculated form (RF1) with an additional binary output asking whether a chair of a given design could support a given load. The specific prompt is included in Figure~\ref{fig:chair_abstraction}. \gpt immediately demonstrated the capacity to handle ambiguity well, assuming a type of wood (oak) and producing numerical material properties for that material when both were unspecified. It made and stated further assumptions about load and failure types. 
It evaluated the failure point by comparing the yield stress to compressive stress, computed as one quarter of the applied load over the cross-section of a chair leg. This is included in the chat snippet shown in Figure~\ref{fig:chair_abstraction}. However, in text form it outputted 94,692.2 Pa, while direct evaluation of the equation it listed in the output gives 94,937.7 Pa; thus, \gpt occasionally failed to perform basic correct in-line arithmetic or algebra. Although the number is only off by a small amount in this case, it can sporadically differ by much greater magnitudes. Along with the evaluation, it included discussion of other, more sophisticated considerations for failure, such as the type of connection between the legs and the seat. Also, upon repeating the same prompt, \gpt would vary whether it included self-weight in the load analysis and whether it evaluated uniform weight or only one leg, leading to small variations in results. 

When asking for a function to evaluate chair failure (RF2), \gpt successfully generated Python code to evaluate whether a chair will break due to excessive compressive stress on the legs, using the same formula as described in the text exchange (RF1). \gpt was able to readily add multiple types of failure without error, also incorporating bending failure of the seat, and excessive stress on the back using simple beam bending and structural mechanics equations. This multi-part failure assessment is included in Figure~\ref{fig:chair_abstraction}. It further automatically generated a function that could intake a parametric chair design with sensible feature parameters like \texttt{leg\_cross\_sectional\_area}, \texttt{seat\_thickness} and \texttt{seat\_material\_bending\_strength}, allowing versatile use of this evaluation. 

When generating the function, it continued to handle ambiguity by make assumptions including that the load would be distributed across all four legs, centered and uniform on the seat, and that the load on the back of the chair would be one third of the total weight. In the case of writing the function (RF2) as compared to text evaluation (RF1), it did not explicitly list all of the assumptions; rather, they had to be interpreted based on the equations used. \gpt also incorporated several small errors and oversights in both cases. For instance, when generating a function to evaluate seat bending failure, it treated the seat as a simply supported cantilever beam, and assumed that the chair would break along the width (separating front from back) rather than along the length or at an angle to the base. It also assumed that the bending stress on the back was evaluated as the load over the total area of the back rather than at the connection surface of the back to the seat of the chair. However, as these functions were identified, they could be further refined by iterated discourse with \gpt to produce a more correct function. 

% Figure environment removed

In a comparison of these two output form requests, RF1 and RF2, functional evaluation was easier to read, more accurate, and able to be implemented for a variety of input designs, but directly incorporated more assumptions into equations. During both types of evaluation, \gpt actively reported on potential causes of error in the evaluation, such as how the chair legs were attached to the seat. It consistently overlooked potential causes of failure such as buckling of the legs unless specifically prompted. We found \gpt to adequately assess most basic mechanical properties of interest. 

Some properties relying on an understanding of the spatial arrangement of chair components were not able to be adequately assessed. \gpt had significant trouble generating a suitable evaluation of stability, and failed entirely to calculate a reasonable center of gravity for an input design despite many attempts. The closest attempt using the simple assumption that the center of gravity would be in the center of the chair seat. 

However, other complex  physical properties were readily assessed. \gpt generated first-order code to assess the failure of a chair upon impact with a spherical projectile, with no difference in quality of the computation compared to static mechanical properties.

To evaluate \gpt's performance on code-based input (DS3 and DS4), we provided \gpt with an \jscad chair specification. When the parameters and parts of the chair were clearly-named salient features (DS3) like \texttt{backThickness}, \texttt{leg1}, \texttt{chairSeat}, and \texttt{chairBack}, \gpt was readily able to recognize the item as a chair and analyze desired properties, such as the breaking load of the seat. However, when we used identically-structured code with variable and object names that had been obscured (DS4), it could not recognize parts of the item to assess properties, for example to locate the seat or synonyms of the seat.
This was true whether the names had been slightly obscured (\eg, as \texttt{XZ\_height}, \texttt{stick1}, \texttt{platform}, and \texttt{barrier}, respectively) or entirely obscured (\eg, as \texttt{Q}, \texttt{A1}, \texttt{B1} and so on). 
When asked about the design in the two obscured forms, \gpt guessed that the final item was a table with a narrow bookshelf and exhibited poor interpretation of the design and parts. Even when \gpt was challenged, it claimed that it could not be a chair because the back was not connected appropriately to the chair seat; this was an incorrect interpretation of the code, again indicating poor spatial reasoning. In a second case, when an input design for a cabinet (DS3) had one variable named \texttt{shelfAllowance} (used to slightly reduce the shelf width for easy assembly), \gpt erroneously assumed that this indicated number of shelves. These results reinforce the idea that \llms perform based on semantics, and that a design without clear descriptive words becomes much less manageable, causing DS4 to generally fail. 

The evaluation process was repeated with DS3 and RF2 for the \jscad design of a cabinet as a box with shelves, a door, and a handle. From the inputted design, \gpt was prompted to create functions to evaluate a set of criteria: storage capacity, load capacity, material cost, and, for a more ambiguous feature, accessibility for a person in a wheelchair. Storage capacity was computed as total volume enclosed by the cabinet, excluding shelves, as expected. In assessing load capacity, \gpt used the ``sagulator" formula, a standard estimation found online for carpentry. However, \gpt's implementation gives strange results and \gpt was unable to provide a more correct form of the equation. For price, \gpt computed the volume of the cabinet walls and a cost per volume. Finally, to address accessibility, it estimated height and depth ranges that would be beneficial, assigning a higher accessibility score to shorter and deeper cabinets. However, it did not provide a source for the height and depth ranges that it scored more highly. 

This points to a potential limitation in the use of \gpt and \llms for this kind of analysis: the source material for equations and standards of analysis may be unknown or even intentionally anonymized. Even when the equations are the standard first-order textbook equations per topic, they are almost always unreferenced. When different standards exist, across different countries or for different use cases, much more refinement would be needed to use \gpt to assess the mechanical integrity of a design. In addition, these equations often work well for objects of a typical design, but for edge cases or unusual designs they would miss key failure modes, such as the buckling of a table with very slender legs or the excessive bending of a chair made from rubber. In a particularly apparent example of this type of failure (\ie, creating functions based on pattern-matching rather than judicious observation of likely failures), \gpt was asked over a series of iterations to help write code to render a spoon with sizes within a set of ranges in \jscad, then to assess ergonomics, which it evaluated based on dimensions. Finally, we requested \gpt to create a function to compute the spoon's breaking strength. Since it had been inadvertently primed by the long preceding discussion of spoon geometry, it proposed a strength evaluation using the basic heuristic of whether the spoon is within a standard size range (Figure~\ref{fig:spoon_assessment}). \gpt had to be prompted specifically for a yield analysis before offering a mechanics-based equation. At that point, it continued to handle ambiguity well and chose a most likely breaking point (the point between the handle and spoon scoop). But for a novice design engineer who might have assumed \gpt's initial output was sound, this bold proposition of an unreasonable strength analysis on first pass without further explanation causes some alarm. This serves as a reminder to not rely on \gpt alone without external validation of every step. 

% Figure environment removed

When assessing designs in text form (DS1, RF1) at an abstract level, \gpt was found to readily identify problems and present a sophisticated discussion of problem areas and considerations for the particular design in question and the metrics being considered. As such, we propose the workflow for rigorous performance evaluation using \gpt to begin with a text-based discussion of the design (DS1 or DS2 with RF1) to understand the relevant features, with no other preceding text in that chat, followed by the development of equations with enough sophistication for the use case, presented in the form of functions for rapid assessment of an input design (RF2). This workflow is depicted in Figure~\ref{fig:performance_workflow}, along with additional steps to ideally validate the final result. 

If an input design of a specific type was used, whether \jscad or another DSL, the form of the input was also provided using well-named variables with each iteration of the chat requesting new code to ensure the variable names did not mutate over time as would otherwise happen. 

There was a failure of \gpt to suggest refinement to the performance codes without specific prompting. For example, there are simple differences in von Mises, Tresca, and Mohr-Coulomb yield criteria for evaluating material failure under applied stress; however, \gpt would simply default to the most common, von Mises, without comment. It would regularly object that the analysis function was an oversimplification; additionally, it would assert that for proper evaluation, more features should be evaluated, more sophisticated tools such as FEA should be used, and structural analysis should be validated by a licensed professional engineer, especially for designs in which factor of safety is a concern. These are all valid points: despite \gpt's very large internal knowledge, it pattern-matches and does not reason at a level to generate the most correct or sophisticated analysis, and will tend to generate more simple rather than more complex equation-based analysis unless specifically walked through refining the code. However, it is capable of more sophisticated text-based discussion, which is why we have found that beginning with text and proceeding to functions provides a more effective workflow, as in Figure~\ref{fig:performance_workflow}.

% Figure environment removed

\subsubsection{Quadcopter}
\label{sec:perf_quadcopter}
We next explored the assessment of dynamic electronic device, a quadcopter, as an example of using the workflow of Figure~\ref{fig:performance_workflow}. \gpt was provided with specifications for the quadcopter that included battery voltage, battery capacity, total weight, and the dimensions of the copter (DS1). We prompted it to generate functions that evaluated the maximum amount of time the copter could hover in the air, the maximum distance it could travel, and the maximum vertical or horizontal velocity and acceleration with which it could travel (RF2). From the provided physical parameters, \gpt was able to generate equations to calculate the copter's inertial tensor, voltage-torque relation, and other kinematics and dynamics. We also independently asked \gpt to generate the physical parameters that would be needed to calculate such metrics, and it came up with the following: maximum thrust, total copter weight, battery capacity, aerodynamic characteristics (\eg{} drag coefficient, rotor size, blade design), responsiveness and efficiency of the control system of the copter, additional payload, environmental conditions, and operational constraints. Although these parameters are all highly relevant, \gpt's output lacked many crucial considerations without explicit prompting in text form. 

In this evaluation, \gpt did not initially include the constraint that the voltage of the controller needed to stay constant, even though this would be obvious to someone familiar with the domain of knowledge. This means that seemingly ``obvious'' considerations need to be explicitly included in the prompt in order for a feasible output to be generated. When asked to include this constraint, \gpt was able to understand the underlying reasons for the constraint, stating that a constant voltage is mandatory for the stability and accuracy of the flight controller. Through this exploration, we also determined that \gpt is able to successfully suggest a product and evaluate the copter based on specific batteries from a particular seller, such as HobbyKing LiPo batteries (\eg{} 3S 2200mAh 11.1V). 

\gpt seems to lack basic spatial intuition of what a copter should look like if the prompt only included the dimensions of the entire copter rather than the dimensions of individual parts. It would hence incorrectly assume that the shape of the copter was a uniform convex solid such as a cylinder or rectangular prism, simplifying and limiting the possible analysis significantly. Thus, we would need to incorporate \gpt's geometric design of the copter's frame, where the dimensions of all components are known, to properly assess aerodynamic performance. And, as with our prior trials assessing chair and cabinet designs, \gpt repeatedly failed to calculate center of gravity or stability metrics, even when given sufficient detail about the design and much iterated discussion. 

% Iteration 1: % without dimensions of copter
% \begin{lstlisting}[language=Python]
%     battery\_life = (battery\_capacity * battery\_voltage) / power\_consumption
%     battery\_life\_minutes = battery\_life * 60
%     longest\_flight\_time = round(battery\_life\_minutes, 2)
%     maximum\_speed = (power\_consumption * weight) / (battery\_voltage)
%     longest\_distance = speed * (longest\_flight\_time * 60)
% \end{lstlisting}

% Iteration 2: {\color{blue} TODO fill in when finishing copter demo}

% \begin{lstlisting}[language=Python]
% \end{lstlisting}

For the most part, \gpt was able to perform the correct arithmetic operations using its own performance functions. But because the generated functions lack complete real-world considerations,it is best to compare \gpt's calculated performance results with what is observed in simulation. We find that these performance functions are a reasonable approximator of copter performance in simulation. The \llm recognizes that the reliability of these results are directly dependent on the accuracy of the inputs, and additional inputs or conditions such as motor efficiency and aerodynamics need to be included in the prompt to match the real copter. 

\subsubsection{Finite element analysis}

To investigate the computational performance analysis capabilities of \gpt, and to build on the first-order mechanical calculations already done, we challenged it to develop a comprehensive framework for advanced performance analysis and structural evaluation using the finite element method (FEM). The primary focus was determining the likelihood of a chair breaking when subjected to external forces. Figure \ref{fig:chair_stress} lists the response and final code generated by \gpt. With the application of FEM through the external library FEniCS, \gpt evaluates the von Mises stress, a crucial parameter in material failure prediction. By comparing this stress with the yield strength of the material, one could assess if the chair would fail under the applied load. For the development of the code, substantial back-and-forth iteration was required to create successful code due to its overall complexity. One helpful point for gradually increasing complexity was to create code for a 2D example before asking \gpt to create a 3D version. In spite of these challenges, \gpt was highly efficient and successful in formulating a precise solution using the FEniCS library, an advanced tool for numerical solutions of PDEs. Not only did \gpt integrate the library into the Python code correctly, but it also applied a wide variety of FEniCS features, including defining material properties and boundary conditions and solving the problem using FEM. Caution must be taken, as \gpt occasionally suggests libraries and functions that do not exist. However, with correction it quickly recovers and suggests valid options. 

The stress distribution visualization in Figure \ref{fig:chair_stress} is performed on the chair previously designed by \gpt in Figure~\ref{fig:cad_chair_design} and is the output of \gpt 's code rendered in Paraview (which \gpt also gives assistance to use), as well as on a chair mesh found from other sources. The result reveals a susceptibility to high stress at the back attachment section of the chair design proposed by \gpt, as seen in Figure \ref{fig:cad_chair_design}. This observation underscores the potential for future enhancements in this object's design.

Beyond code generation, \gpt also lends support in the local installation of these external libraries, such as FEniCS, so users can run the generated code. This assistance proves invaluable for practitioners who may have limited familiarity with these libraries, which are initially suggested by \gpt itself. Notably, studies have delved into the potential of \gpt to generate code integrating other external libraries, like OpenFOAM, for the purpose of performing computational performance analysis \cite{kashefi2023chatgpt}. 

It's worth noting that \gpt's capabilities in utilizing these libraries have certain limitations. It can only harness some of the basic features of FEniCS and struggles with more specific, custom usages of the library, such as applying complex loading conditions. Furthermore, \gpt assumes homogeneous material properties for the chair, an oversimplification that doesn't align with the more diverse characteristics found in real-world materials. Moreover, the training date cutoff for \gpt means that sometimes only older functions or libraries may be used, without current updates. 


% Figure environment removed


\subsection{Subjective Evaluation (Q2)} 
\label{designToPerformance_Subjective}

Subjective properties have a higher dependence on lexical input, making their evaluation using \llms an intriguing proposition. We began with an assessment to compare the use of semantics for assessing subjective properties via 3 output forms: categorization or labeling (RF3), pairwise comparison, and overall ranking. We generated a simple parametric 4-legged chair with a back, then input eight versions with different leg lengths, seat widths, and back heights into \gpt (DS1). \gpt was then asked three similar queries: (1) assign to each chair a label of "large," "medium," or "small" (RF3); (2) rank all chairs from largest to smallest; and (3) in a pairwise comparison, indicate if a given chair was larger or smaller than another. Each of these inputs were given independently, to not influence the different tests based on prior answers in the chat dialogue. In each case, \gpt assigned the same overall ranking. Figure~\ref{fig:chair_size} shows the chairs rendered in ranked order including the labels for categorization, using a combined implicit consideration of seat area, back height, and leg height. In a similar query, spoons of different handle length and thickness, and scoop length, width, and curvature were compared, finding similar results. In that case, \gpt elected to compare spoons by the length of the scoop alone, handling the ambiguity of the question by making a decision about what single quantity mattered most. When handling higher levels of ambiguity, \eg assigning comfort levels to shoes described in text input, \gpt sometimes refused to answer. To bypass this, we determined that it was essential to ask \gpt directly to give an output of a certain kind, such as classification into set categories. For instance, the question "Is this shoe comfortable?" would raise objections, a non-answer, and a discussion of what contributes to general shoe comfort. We could circumvent this by asking "Please state whether this shoe is likely very comfortable, comfortable, uncomfortable, or very uncomfortable, and provide one sentence of justification." Despite it's continued objections, \gpt's responses were usually reasonably justified, noting aspects like use of breathable material, adjustability of laces, shock absorption, and traction of the sole. These results indicate that the semantics of the type of assessment (ranking, categorization, or scoring) do not have a large influence on the final result of subjective analysis, as long as some type is chosen. However, certain prompt structures may be required to avoid refusals to answer, and the simplest prompt structure to ensure this was asking for any certain kind of output response. 

% % Figure environment removed


% Figure environment removed


\subsubsection{Semi-subjective evaluation of sustainability}
To challenge \gpt to evaluate subjective criteria dependent on more abstract input parameters, we asked it to create a list of key criteria that go into evaluating sustainability, and to evaluate chair designs based on these criteria, scoring each category from one to ten (RF4). Given \gpt's limited understanding of numerically-specified meshes or spatial arrangements, we used text-based information (DS1) for commercial chairs from Ikea and Home Depot. \gpt was unable to access this information on its own when prompted with product names, so for this test case, the text from product pages was pasted into the \gpt chat dialogue. This information included each chair's name, a text description of its design, material, and appearance, and some numerical information such as dimensions, weight, and cost (Figure~\ref{fig:chair_style}). Upon requesting the evaluated score for sustainability metrics, it outputted seemingly reasonable numbers with justification based on the text description. 


% Figure environment removed

The justification for each property score were generally reasonable but rarely entirely correct. For example, the remark in Figure~\ref{fig:chair_style} for \texttt{modular\_design} about swapping seat shells was a misinterpretation of the product description: chairs with different seat shell colors were available for purchase, but a single chair could not swap shells. In addition, for this example and most other tests, \gpt refrained from assigning high scores (9-10/10) or low scores (1-3/10) within each category, which likely contributed to errors. 
A further function generated by \gpt readily combined the individual property scores into an overall sustainability score for a given input design. 

\subsubsection{Fully subjective aesthetic evaluation}
% \paragraph{cabinet} \co{}

% \paragraph{chair} \co{}
To evaluate the aesthetic design of an item, the physical appearance must be known, so again the listings from product pages were used as the input data. When prompted to create a function to evaluate aesthetics in general, \gpt refused, noting that it is "highly subjective and can vary greatly depending on individual tastes and preferences" and wrote a function in python with a rather simple subfunction for aesthetics: \lstinline{# Here we'll use a provided aesthetic score}. 

% \begin{lstlisting}[language=Python, style=mystyle, caption=Aesthetic evaluation...?]
%     # Here we'll use a provided aesthetic score
% \end{lstlisting}

In a more carefully curated prompting setup, a range of historical periods were identified that influence chair design, including Egyptian, Greek and Roman, Renaissance, Bauhaus (a semi-minimalist German-inspired design including rounded features), and Minimalist. \gpt identified criteria to differentiate between these historical styles based on seven properties: material choice, decorative complexity, evidence of handcrafting, extent of ornamentation, deviation from standard proportions, upholstery use and quality, and material innovation. Based on these categories, \gpt evaluated each historical period and chair, and created a function to use the scores to categorize the style of each chair. A selection of text from one input/output is included in Figure~\ref{fig:chair_style}. In every output \gpt would also give a reminder that scores were approximate or arbitrary and should be adjusted. And as before, scoring on a 1-10 scale was generally limited to intermediate values in the range, for instance for \texttt{Degree of Decorative Complexity}, a score of 3/10 is given even though the justification lists that no decorative elements were indicated. Even so, the results of the categorization (Figure~\ref{fig:chair_style}) seem generally reasonable with most chairs placed into categories that appear subjectively appropriate; a plain metal stool was classified as minimalist, a soft lounge chair with a floral pattern was classified as Renaissance, and a double end chaise lounge was classified as Greek and Roman. A couple of types of mistakes occurred in the classification. First, most chairs were sorted into the Minimalist category, including the faux leather swivel lounge chair and two soft-sided recliners (not shown). Second, several other design styles that may have been a better fit were included in the scoring but were not found to be best fits in the evaluation, indicating that this set of \gpt's scoring for the historical periods was not appropriately distributed to capture the right features for all chairs. Third, upon re-evaluating scores over a few iterations, we found that different categories could be established and chairs could switch categories at times due to subjective scoring. Nevertheless, these general issues persisted, such as occasional mistaken categorizations and having one "catch-all" category that was used more than others. 
 

% % Figure environment removed

In a similar testing setup, \gpt was used to identify criteria to help a user decide the most appropriate room in a house in which to place a chair of a given design. In this second case, it created categories for criteria used to select the room of a house for a chair including size, comfort,  weight, pet-friendliness, and weather resistance. It further created a list of weightings for the importance of each of these criteria based on the room in question, and ideal ranges for the quantitative features size and weight. It was finally used to create a function to distribute a set of chairs to the set of most appropriate rooms in a house. However, upon evaluation, the results were mediocre: for instance, a lounge chair was sent to the kitchen. It otherwise sorted a soft chair to the living room, a weather-resistant chair to the porch, and a sturdy chair with a soft lining to the study room. More careful selection of evaluation criteria could certainly improve on these results, as well as the inclusion of more details about the chairs and their desired purposes in the rooms in question. 

\subsection{Discussion}
In the evaluation of performance, \gpt was generally successful, though it exhibited an array of intriguing behavior patterns. In this section, we elaborate on \gpt's key capabilities (C), limitations (L), dualisms (D), and opportunities in the context of design to performance, as illustrated by our example cases in the present section. 

% \noindent \textbf{Capabilities:} \linebreak
\noindent \textbf{C.1 Extensive Knowledge Base in Performance:}
Through discussing in text form, \gpt could suggest design considerations and metrics at a fairly sophisticated level. Even when asked to evaluate ambiguous requests, when details are left out, or when the performance metric is complex, \gpt is still able to output reasonable first-order approximation functions. The generated output evaluation functions usually worked, having no coding errors in python; errors in javascript or \jscad were more frequent, but they were usually directly resolvable. \gpt was also able to sort items into categories, and to generate rankings among a set of designs without giving explicit intermediate evaluations. 

\noindent \textbf{C.2 Iteration Support:} 
\gpt was able to eventually assess any property we tested, although the quality of assessment varied. When mistakes were made, further questioning could support the refinement of code to a point where it improved. Particularly for the complex example of the FEA, this took many steps to refine but \gpt responded well enough to stay on track, respond to troubleshooting feedback as well as conceptual feedback, and finally create usable code. 

\noindent \textbf{C.3 Modularity Support:} 
Functions could be effectively built up point by point, with modifications made according to changing needs. \gpt could adjust part of a scoring system, such as switching one item for another, or to create the same type of scoring system for another use case using the framework of the first system to create the second one. 

% \noindent \textbf{Limitations:} \linebreak
\noindent \textbf{L.1 Reasoning Challenges:}
\gpt relied on semantic clues, such as variable names, to understand and assess designs. It overall failed to appropriately evaluate performance that required spatial reasoning, like center of gravity or stability, for items having multiple components. In addition, earlier parts of conversation could cause issues for \gpt to poorly choose evaluation metrics, such as a discussion of spoon dimensions leading it to evaluate whether a spoon is ``strong" based on whether its size is within a normal range. When considering subjective metrics that are not typically quantified, \gpt would object. Upon requesting more sophisticated or more abstract evaluation, it would refuse to answer on the first attempt. 

\noindent \textit{Potential Solutions:}
To understand designs, they must be described with enough text-based semantic clues for \gpt to handle. Spatial reasoning issues could be resolved using external methods, such as external FEA analysis or other existing APIs to perform these evaluations. To choose the quality of evaluation equations, more discussion with \gpt could reveal the use-case for the chosen equations and alternatives, allowing a user to decide if another option may be more suitable. To assess subjective metrics, it worked best to develop scoring systems by breaking down a subjective feature into smaller, more quantifiable parts that \gpt could approach. And to bypass refusals to give a concrete answer, prompt engineering on its own could solve this, by requesting a specific enough type of output. 

\noindent \textbf{L.2 Correctness and Verification:}
The source material for equations used by \gpt in evaluation was usually undefined, which can contribute to error, and often embeds assumptions. When calling external libraries, \gpt occasionally invented fake libraries that could not function. Or, when working with \jscad designs it occasionally created designs using nonfunctional methods or nonworking code and simply complained that the language had been updated past its training cutoff. 

\noindent \textit{Potential Solutions:}
An external checker would be needed to verify the source of equations against an objective standard to ensure reliability, and when challenged, \gpt can uncover assumptions in choices of evaluation equations. External options for checking could include using metrics and equations established by published standards for engineering codes and proposed for items such as sustainability, safety, and ergonomics as appropriate to the use case. 
To solve the use of fake libraries or using fake methods, once \gpt was challenged enough times it would eventually offer an existing option. A more efficient solution when it cycled through fake options for \jscad programming was to input a working example of any kind into \gpt along with the request for a working code, using its capacity for modularity to help it structure a working response. 

\noindent \textbf{L.3 Scalability:} 
Other challenges provided obstacles to evaluation. For objective criteria, first order analysis is readily available on all metrics tested, but the scalability in complexity is limited. It was possible but more difficult to get more advanced characterization, for example generating code for FEA for mechanics. As another challenge, the quality of evaluation was found to be best when 1-2 performance metrics were analyzed at once. When too much was requested at once the output quality decreased. 

\noindent \textit{Potential Solutions:}
To handle the limitation of scalability of the complexity of analysis in a given domain, use of existing domain-specific APIs would be suggested. To handle the limitation in amount of metrics to be assessed, the analysis for metrics should be developed one by one into subfunctions that are then stitched together. However, making a longer chat in this format then runs into memory issues of \gpt, for which we found it to forget sets of function inputs and other details within two exchanges. This, in turn, requires giving reminders of the important parts of previous answers (such as the overall function input) when generating each subfunction. When generating the FEA code, a suitable solution was to have \gpt keep repeating the same entire code, and occasionally switch between asking for 2D and 3D versions to create something simple enough before increasing the challenge level, and iterating back again when next parts of the code were found to break, until the entire function worked. 

% \noindent \textbf{Opportunities:} 
\paragraph{Opportunities}
We recommended that a good workflow for analyzing performance would utilize a buildup of complexity, beginning with discussing the design in text form and then generating a function to evaluate a design input in a parametric form. Many issues arising from performance evaluation could be attenuated by relying more on existing methods, libraries, and APIs that have already been created for the use-case in question. 














% % good at 
% \begin{enumerate}
% \item \gpt complains when asked to evaluate when requests are ambiguous, details are left out, or when the performance metric is complex, but is still able to output reasonable first-order approximation functions
% \item It could also generate rankings between a set of designs without giving explicit intermediate scorings. 
% \item To assess subjective metrics, it worked best to develop scoring systems by breaking down a subjective feature into smaller, more quantifiable parts. The use of categories also worked. 
% \item A good workflow for analyzing performance is discussing the design in text form and then generating a function inputting a design in parametric form. It was able to eventually assess any property we tested. Output evaluation functions it generated usually always worked, the code had no errors. 
% % like w/design section, the buildup helped
% \item Through discussing in text form, \gpt could suggest design considerations and metrics at a fairly sophisticated level

% finds challenging & solutions 
% \item For objective criteria, first order analysis is easy, it's possible but harder to get more advanced characterization, for example generating code for FEA for mechanics
% % limited complexitly, scalability in complexity 
% \item The source material for equations it uses in evaluation can be a mystery, can contribute to error, sometimes embed assumptions, but \gpt can uncover those assumptions if asked 
% % reliability / verification. It can try to justify but even be wrong. May need an external checker. 
% \item The quality/accuracy of evaluation is best when 1-2 performance metric are analyzed at once. When too much is requested at once the output quality decreases. 
% % how to solve? do one thing at a time. write subfunctions. 
% % forgets --- keep reminding it. every 2 messages. Go down trees. Give it reminders of the answer it gave you back. 

% % unsolved failures 
% % how to solve???? using an existing API 
% \item Failed to appropriately evaluate performance that required spatial reasoning, like center of gravity or stability, for items having multiple components 


% \end{enumerate} 

% types of failure: things it's bad at reasoning at, verification, scalability
% highlight similarities and differences between sections -- after


% \paragraph{subjective criteria: categorization, ranking, and scoring}

% In order to more carefully address \gpt's evaluation process, three methods of output were considered: 
% \begin{enumerate}
%     \item Categorization
%     \item Scoring
%     \item Ranking
% \end{enumerate}



% What we have tried:
% - try to have it generate reproduce functions from high-level specifications which a user assumes "exists" 
% - 
% examples: given a circuit test for correctness -> does it do what it's supposed to do

% - Making suggestions of discrete changes


% given a design - can it describe what it does, can it undertand a design, evlaaute how it can be used?

% Copter example -


% \begin{itemize}

% \item \textbf{Q} Can GPT assess the performance for a given text input design of an item? 
% \item \textbf{Q} Can GPT generate generalized evaluation functions from high-level specifications for a parametrized design which a user assumes "exists"? (ex. "a chair of a typical design") 
% \item \textbf{Q} What kinds of metrics can it assess? What kinds of objects can it assess? 
% \item \textbf{Q} To what accuracy can it assess objective performance metrics like weight and strength? Can it generate functions with higher level complexity, like FEA for mechanics?

% %------------------

% \item \textbf{Q} Can it assess more subjective metrics? 
% \item \textbf{Q} Can GPT assess performance when objectives not easily quantifiable? 
% \item \textbf{Q} Can GPT assess performance when not everything about the design is known?

% \item \textbf{Q} How to use GPT for multi-objective metrics? Can it help us with weighting? Can it \textit{interpolate} metrics, so manipulating the \textit{metric space}?
% \item \textbf{Q} If we cannot come up with an analytic metric, can it still help us to evaluate the performance by comparing two or even more designs?
% \end{itemize}

% \paragraph{Overview}
% Give a simple example.
% Given this chair made of wood:
% Binary: Can it support 100kg?
% Scalar: what is the maximum weight it can support?
% Vector: What is the maximum weight it can support and how tall is it?

% \paragraph{Input}
% The input is a design specified using a prompt. It can be converted to code description, the input is also the performance metric. 
% How to specify the performance metric? 
% Give examples: weight, cost, amount of material, size, aesthetics, mechanical (or other physical) properties, 

% \paragraph{Output}
% The output is the behavior. Evaluation of the behaviour.
% This can be a binary value (yes/no).
% This can be a continuous score. Or multiple values.


% \paragraph{Quantitative Metrics}
% - show that is struggles with spacial reasoning and more quantitative approaches 
% - use APIs to run metrics/simulation etc.  




% \paragraph{Scoring, Categorizing, and Ranking}
% - how tall is this?
% - 2 version and ask which one is taller?
% - say something can be tall, medium, short and see how good it is





% \paragraph{Performance evaluation examples}
% Static objects
% Dynamic objects
% Stationary objects with on-board (embedded) function (electronics or chemicals, computation (programs))?
% Subjective objects or those that require human feedback (drugs, website design, aesthetics, human interaction)


% Start with boxes, tables, chairs, 
% Cars - 
% Quadcopter - how far does it fly on a battery?  How fast can it go?
% Pharmaceuticals - yes/no is drug effective inhibitor (does it pass a randomized trial)?


