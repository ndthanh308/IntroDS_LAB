\section{Text-To-Design} 
\label{sec:text_to_design}

% A pivotal step in getting a large language model (LLM) to generate a design involves crafting a prompt that clearly delineates the design language to be employed. This encompasses the specification of both the language's syntax and the semantics associated with each operator, presented in a manner that the LLM can comprehend. 

% We usually accomplish this via a three-step process. We first develop a succinct language specification that defines the operators and outlines their respective inputs and outputs.
% Following this, we demonstrate an example employing this language to devise a simple model.
% Finally, we incorporate additional semantic information about the model or design guidelines that can assist the LLM.

% Next, we will delve into how to establish design prompts across four domains: 2D vector drawings, 3D parametric geometry, actuated systems, and electronics. Each of these domains adheres to similar specifications. In each of these domains, we discuss both prompt generation and algorithms for converting outputs from the LLM to standard formats 


% \wojciech{Too much focus on spatial}
For our first line of inquiry, we explore the extent to which \gpt is able to generate designs across a variety of domains. 
Even within the specific context of manufacturable design, the concept of a ``design'' is quite broad, and exists at many scales.
For example, we may want to specify a single self-contained part, or a sizable hierarchical assembly containing several levels of sub-assemblies and/or other individual component modules. 
Such assemblies may be completely customized/self-contained, with all parts designed simultaneously, or they may be hybrid designs that integrate existing, pre-manufactured elements such as brackets or motors.
In many cases, our target design tasks also include dynamic considerations such as assembly mating or articulated joints.

Although these complex tasks may initially seem out-of-scope for lexical models such as {\llm}s, there are many modeling and design paradigms that can be expressed in terms of potentially-\llm-compatible language.
To guide our exploration of \gpt's ability to interface with each of these models, we pose the following questions:

\begin{itemize}
\item \textbf{Q1} Can \gpt generate a meaningful design when provided with a high-level description of the goal and a given modeling language?
\item \textbf{Q2} To what extent is the user able to control the designs created by \gpt? Is \gpt able to interpret and respect user-defined constraints, such as spatial relationships between objects or integration of standard pre-fabricated parts?
\item \textbf{Q3} Is \gpt able to incorporate high-level abstractions used by human designers, such as modular (de)composition?
\end{itemize}

% \wojciech{For the copter example, you first need get the appropriate components (e.g., What are the main components of the copter?). There should be a question on this.}\liane{part sourcing is addressed in the manufacturing section. All examples in this section explicitly assume that we already know the parts we want (we also make an explicit reference to the fact that we'll explore gpt sourcing in section 6.}


\subsection{Simple, self-contained designs from high-level input (Q1)}

% Can \gpt successfully instantiate and coordinate (a set of)  primitives from a given modeling language in order to build/approximate a meaningful design? Does \gpt seem particularly well- (or poorly-) suited to any particular modeling paradigm(s)?
%Can \gpt generate a meaningful design when provided with a high-level description of the goal and a set of primitives from a given modeling language?


To explore \gpt's capacity for design, we first test its ability to do one- (or few-) shot generation of an object from a minimal high-level text description as input. 
Ideally, we would like to understand \gpt's ability to complete design tasks independent of any particular modeling paradigm.
However, it is not immediately clear how much dependence there may be on the specific representation that is chosen, because the variation in possible language-based modeling paradigms is significant. 
Some languages are very general and versatile, with a wide variety of features and capabilites, while others may be highly-specialized for a specific set of tasks or outcomes. 
Similarly, some languages are well-established with plentiful online documentation or examples, while others may be custom-defined, poorly documented, or otherwise underrepresented in \gpt's training repository. 
Finally, some languages are fairly streamlined, while others may be syntactically complex and/or require the use/coordination of many modules.
Each possibility offers unique capabilities and challenges. 
Thus, we set out to test a wide variety of them, in an effort to determine 
\llms' ability to use each representation;
whether there are any conclusions that seem to span across different representations;
and
whether any particular representations seem uniquely well- or poorly-suited for \llm integration.


% \liane{for an initial draft of each section, it'd be great to list out e.g. what kind of representation you're using (if \gpt needs to use an API/function calls, generate a mesh directly, come up with a mix of discrete components/continuous values, etc.); how much you had to tell/teach \gpt vs. whether it could just use the API directly; the examples/experiments you tried; what \gpt was good at/struggled with; and include [not necessarily polished] figures of your results. Things might shuffle around, but this will give us a good starting point}


\subsubsection{Vector Graphics with SVG/DXF} 
\input{sections/GraphArrows/TextToDesignExperiments/VectorGraphics_basic}


\subsubsection{CSG with self-defined primitives}
\label{sec:textToDesign_CSG_boxes}
\input{sections/GraphArrows/TextToDesignExperiments/CSG_customPrimivites_basicDesigns}

\subsubsection{CSG with PyVista}
\input{sections/GraphArrows/TextToDesignExperiments/CSG_PyVista_basicDesigns}

\subsubsection{CSG with \jscad}
\label{sec:textToDesign_JSCAD_basic}
\input{sections/GraphArrows/TextToDesignExperiments/CSG_JSCAD_basicDesigns}

\subsubsection{Sketch-based CAD with OnShape}
\label{sec:textToDesign_OnShape_basic}
\input{sections/GraphArrows/TextToDesignExperiments/CAD_OnShape_basicDesigns}


\subsubsection{URDF}
\label{sec:textToDesign_urdf}
\input{sections/GraphArrows/TextToDesignExperiments/Robots_URDF_basicDesigns}



\subsubsection{Graph-based DSL}
\input{sections/GraphArrows/TextToDesignExperiments/Robots_graph_basicDesigns}

% \subsubsection{Gerber file?}
% \input{sections/GraphArrows/TextToDesignExperiments/Electronics_Gerber_basicDesigns}



\subsubsection{Summary Discussion}
In light of these experiments, we conclude that \gpt is capable of generating designs based on high-level text input, even across a wide variety of representations and problem domains. 
We note that several of \gpt's capabilites and limitations remain consistent independent of the representation.
For example, in all cases, \gpt is able to generate sensible, well-structured code with semantically meaningful variables and comments.
Moreover, independent of the representation or the problem domain, \gpt consistently shows superior performance with respect to the high-level, \textit{discrete} elements of a problem (\eg, identifying the correct type and quantity of each primitive/operation) as opposed to the lower-level continuous parameter assignments (\eg, correctly positioning the primitives relative to one another). 
A more detailed discussion of capabilities, limitations and opportunities will follow in \sref{sec:textToDesign_discussion}.
For now, we rely on the similarities between various representations to justify a reduced scope for our future experiments.
In particular, moving forward, we study each question with respect to only a subset of the design representations and domains introduced above. 









\subsection{Interpreting and Respecting User Control (Q2)}
% Can \gpt generate a design matching some \textit{specific} user intent, when provided with a more detailed description?
% Generating Specific Designs from Lower-Level Guidance/Feedback
% \item \textbf{Q3} To what extent is the user able to control the designs created by \gpt? Is \gpt able to interpret and respect user-defined constraints, such as spatial relationships between objects, mating constraints for multi-part assemblies, or integration of standard pre-fabricated parts?

The above examples demonstrate \gpt's ability to generate a design based on very high-level semantic input.
However, we also wanted to test its ability to generate designs that adhere to a specific user-given intent.
This section also tests whether \gpt is able to overcome its own potential biases induced by the training data, in order to generate something that truly adheres to a user's specified constraints -- whether or not those constraints match the ``common'' form of a given design target. 
In particular, we choose to study whether \gpt is able to 
(1) understand and respect semantically meaningful spatial constraints, and
(2) incorporate specific pre-fabricated elements into a design.



\subsubsection{Spatial Constraints}
\label{sec:textTODesign_spatial_constraints}
% For example, can we specify the constraints such as "non-overlapping", "symmetric", "above/below", "at a specific relative/global position"? What types of constraints? Which ones does it understand natively, and if there are multiple ways to phrase a constraint, do some approaches seem to work better than others?
Through the general experiments above, \gpt has already shown some capacity to respect high-level spatial constraints, such as a design element's absolute size or its position relative to another element of the design. 
\gpt's compliance with such requests was frequently flawed at the outset, but the results were generally workable after some amount of interactive feedback. 
This section aims to explore the types of constraints \gpt is able to natively understand, and how we might best interact with \gpt in order to improve the chance of successful compliance with such constraints.

%% non-overlapping, position, relative location of 
\input{sections/GraphArrows/TextToDesignExperiments/CSG_JSCAD_constraints}


%% symmetry 





%\subsubsection{Generating Mated, Multi-Part Assemblies}
%% Can \gpt generate multi-part assemblies with valid mating characteristics? 
%
%\input{sections/GraphArrows/TextToDesignExperiments/CAD_OnShape_multiPartAssemblies}



\subsubsection{Incorporating pre-fabricated elements}
\label{sec:prefabbed-ele}

It's also common to design an object around specific pre-manufactured elements, such as hinges, brackets, or motors. 
We explore the possibility of using \gpt to source the parts in \sref{sec:part_sourcing} -- at that time, we explore whether \gpt can identify the required part categories, provide options, and/or select a set of options that are compatible with one another and the intended overall design. 

For now, we assume that the user has a specific (set of) part(s) in mind that they would like to incorporate into their design.
Then we investigate whether, given these components, \gpt is able to (1) build a reasonable proxy of this design, then (2) effectively use it as a module within a larger assembly.

\paragraph{Cabinet with Standard Hardware} 
\input{sections/GraphArrows/TextToDesignExperiments/CSG_JSCAD_integratingPrefabbedParts}

\paragraph{Quadcopter}
\input{sections/GraphArrows/TextToDesignExperiments/Robots_customPrimitives_integratingPrefabbedParts}






\subsection{Incorporating Abstractions such as Modular/Hierarchical Designs (Q3)}
\label{sec:textTODesign_abstractions}
\input{sections/GraphArrows/TextToDesignExperiments/CSG_JSCAD_modules}









\subsection{Discussion}
\label{sec:textToDesign_discussion}
% \team{Liane}

In this section, we elaborate on the key capabilities (C), limitations (L), and dualisms (D) previously outlined, particularly as they relate to the domain of text-to-design.

\noindent \textbf{C.1 Extensive Knowledge Base in Design and Manufacturing:} Within the text-to-design space, \gpt exhibited proficiency in supporting high-level structure and discrete composition. For instance, \gpt consistently generated the correct primitives (type and quantity) for a given task, regardless of the specific design language it was using. 
\gpt also demonstrated a capacity for interpreting and auto-completing under-specified prompts, as in the case of the CSG table example, where \gpt inferred and provided reasonable values for a set of missing parameters (see \sref{sec:textToDesign_CSG_boxes}).
Finally, \gpt generated readable, explainable, and maintainable code that contained descriptive variable names and comments, along with appropriate modularity and other high-level structural elements.  


\noindent \textbf{C.2 Iteration Support:} Even when \gpt did not immediately arrive at a suitable design solution, it often succeeded in rectifying errors after a reasonably small number of user interactions. For example, it was able to successfully adjust the placement of the cabinet handle after a handful of additional prompts. The ability to engage in iterative design is also very helpful when building up complex structures such as the wheeled robot from \sref{sec:textToDesign_urdf} or the L-bracket proxy discussed in \sref{sec:prefabbed-ele}, because users can start with a simple prompt, then iteratively increase the complexity to arrive at a suitable result.

\noindent \textbf{C.3 Modularity Support:} \gpt effectively incorporates modules and hierarchical structures, using natural language as a powerful tool for conceptualization and orientation.

\noindent \textbf{L.1 Reasoning Challenges:} Spatial reasoning posed a significant challenge for \gpt. Well-crafted domain-specific languages (DSLs) may be able to mitigate this issue. We noted specific difficulties with constructive solid geometry (CSG) due to the computational requirements for object placement. Sketch and extrude languages that utilize reference points can minimize this challenge to an extent, as they offload the computation to reference resolution. This approach is effective for simpler designs but falters when managing complex sequences of transformations. As discussed in the sketch-based car example from \sref{sec:textToDesign_OnShape_basic}, we found that DSLs that balance the benefits of reference-based language with global positioning information may be more effective. 

\gpt's lack of spatial awareness also created difficulties with constraint handling, such as when \gpt was asked to ensure that elements were non-overlapping. We found that iterative refinements and careful prompting often provided a workaround for these issues. For example, \gpt typically failed to respect ``non-overlapping'' constraints, but it generally responded well to the instruction that some element should be ``in contact with (but not protruding into)'' another element.

\noindent \textbf{L.2 Correctness and Verification:} \gpt is not able to reliably verify its own output, and it frequently makes contradictory claims. For example, when asked to place a handle on the right side of the cabinet structure, \gpt frequently placed the handle on the left-hand side of the cabinet, then immediately declared its design a success, because the handle was on the right, as requested. This seems to suggest that external verification tools may be helpful, particularly in cases where the contradictions are less obvious.

\noindent \textbf{L.3 Scalability:} \gpt's success seems to decline as the number of simultaneous requests increases. For example, it is best to issue 1-2 constraints or correct 1-2 issues at a time, rather than trying to issue several constraints or correct several issues at once. 
Similarly, \gpt encountered challenges when interpreting high-level information to build proxies for more complex designs all at once; instead, the models must be built iteratively, with gradually increasing complexity.
This iterative modeling was most effective when the user provides explicit instructions about both the aspects that should change, as well as the aspects that should remain unaltered (either because they are already correct, or because they will be addressed later). 
Despite \gpt's initial difficulty creating complex models, \gpt is able to effectively use and combine existing modules to create more intricate models. 

\noindent \textbf{L.4 Iterative Editing:}
As discussed in \sref{sec:prefabbed-ele}, \gpt seems to exhibit limited memory and attention span. In particular, it often ``forgets'' things from previous messages. We address this by occasionally reminding \gpt of its previous input/output, either by asking it to summarize a previous interaction/finding, or by explicitly including a prior result as a starting point in our prompt.


% \noindent \textbf{D.1 Context Information:} \liane{todo}
\noindent \textbf{D.2 Unprompted Responses:} \gpt is frequently able to recognize and address under-specified problem statements.
For example, in the CSG table specification (\sref{sec:textToDesign_CSG_boxes}), \gpt correctly inferred the need to assign a tabletop thickness value.
Similarly, when augmenting the cabinet with a door and a handle in \sref{sec:textTODesign_spatial_constraints}, \gpt responded with several distinct approaches for handle design.
This can be powerful, as it may alert the user to parameters or variations which may otherwise have gone overlooked; then, users have an explicit opportunity to consider and refine the specification accordingly.
Moreover, it allows users to undertake a design process and begin receiving feedback without first needing to craft a perfect specification or prompt.
However, if \gpt confidently hallucinates a particular solution to an under-specified aspect of a design problem -- rather than explicitly prompting the user to consider a range of options -- it may limit and/or bias their exploration in unexpected ways.





% finds the discrete set of primitives, can't align them reliably (e.g. Felix and PyVista ) this is true accross different DSLs
% With CSG everythin live in global space and things are either pretruding or separated
% sketch and extrude is better but still struggles
% The DSL can help but it's not enough to solve the problem 
% What are the characteristics of the DSL changes that have helped?
% It has fewer things to do so that be helpful but it will still get things wrong if it's still hve to do multi-step reasoning about the different frames of reference is still hard (order specific) you start with a good reference point from which you can do the rest of the design but if it needs  a lot of reasoning it has a hard time. (.e.g composing transformations or chaining or logic is hard). 
% Expose high level constructs that are explicit of the thigns that it can do (e..g rotate by 90 degrees). Felix may have done somethign similar (added a global positioning -
% ***********This is the conclusion*********:  GPT is bad a two things: computation and staking references  so it  struggles with CSG and it struggles with many references. Solution references with some global information. 

% Good at re-using. It is easier for it to build things iterative/ with modulatiry. Do a lot of iterations to make you thing work and can replicate things well

% Hard to handle constraints or controlled specificatons of where you want things to be (e.g. the handle). This is of course even worse if you're trying to get mulitple thigns at once. 


% Good at:
% 1) high-level structure (discrete part of problems)
% 2) good commenting the code and naming variables in ways that are semantically meaningful 
% 3) good at re-using things once it has a design (maybe partially because the stuff it creates has good semantic information - this will be discussed more later)  

% Bad at
% 1) Continous positioning of elements in a global space and reasoning about local coordinate systems and compositions of transformations. 

% Solution: DSLs that combine referencing and global information. 

% 2) Respecting Constraints 

% Solution: feedback loop and alternative phrasing. 

% 3) Taking a lot of information at once. 

% Solution: leverage modularity 




\ignore{

- Text-To-Design Takeaways
	- good at high level structure, bad at placement
	- can use a wide range of DSLs, but best if they're intrinsically consistent, seems to have trouble reasoning about local coordinate systems (needs to be reminded about e.g. centered positioning, sketch based CAD hard,)
	- better to build it up gradually, has trouble with large requests all at once
	- constraints are difficult -- has trouble reasoning about them, but can usually be accomplished after some back and forth / playing with the word choice or prompt style 

	- has difficulty directly interpreting/building proxy from high level information, but can use it once it exists to create more complex designs.
	-
As part of this exploration, we also examine \gpt's ability to provide semantically-meaningful design specifications -- \eg, scripts including variables rather than hard-coded ``magic'' numbers, and reusable functions or modules where appropriate.

\begin{itemize}
    \item \textbf{Q} Is there a universally effective style of prompt that seems to generate the desired design quickly, with few errors (wrt design or boilerplate code)?
    \item \textbf{Q} Is there a limit to the number of primitives, constraints, modules, levels of hierarchy that \gpt seems to encounter? 
    \item \textbf{Q} (How) can we iteratively evaluate validity and the correctness of the design, and use this evaluation either to improve the model through feedback or to automatically post-process the design so that it is valid and correct.?
\end{itemize}


When specifying geometric constraints, effectiveness of that constraint being reflected in the final design varies based on the adjectives used to describe the constraint. For example, asking \gpt to make two primitives "non-overlapping" may still produce a result where the two bodies intersect, while asking \gpt to make it so that a body "does not protrude" into the other is effective in preventing intersections. So far, a pattern as to which adjectives are interpreted correctly by \gpt has not been observed. 

Specifying constraints via numeric means --- i.e. offset the second cube so that it's top face is aligned with z = 0 --- is almost always effective, whereas asking for geometric relations --- i.e. offset the second cube so its top face is aligned with the bottom face of the first cube --- does not guarantee consistent interpretation.

Introducing multiple constraints in a single prompt often results in some of the constraints being ignored, whereas introducing the same constraints in sequential prompts results in the desired behaviour. 

}













% ============================
% old formulation
% ============================
\ignore{
\paragraph{Problem setup}

There are three aspects to a prompt for specifying a design task: The objective, the primitives, and the additional constraints.  We describe in each turn, and then provide an example of a complete prompt and its output.



\includepdf[pages=1]{figures/BohanExample.pdf}

Function box(x1,x2,x3, dx1, dx2, dx3) creates a box centered at point (x1, x2, x3) with dimensions (dx1, dx2, dx3). Create a capital letter T from non-overlapping boxes.

This prompt has three components. Specifying a primitive, specifying the objective/goal, and constraints.


\paragraph{Design Representation}
Design can be expressed code, i.e. formally (e.g., CAD, OpenSCAD, SVG, STL, etc.). Sequential List of commands. 
Design can be represented using graph. Nodes (components). Edges connections (e.g., connectivity) between components. 


\paragraph{Design specifications}
Describe a text example of a goal.

\paragraph{Design primitive}
How do specify design primitives.
Can you specify different primitives? How many? Can you mix different primitives. 

\paragraph{Constraints}
Describe different types of constraints.
Describe which constraints work and which do not work.
Symmetries. Spatial constraints (on the top, on the side, colinear, at the distance from X, parallel, perpendicular, at an an angle, reflective, minimum, maximum size, setting something to a value)

\paragraph{Hierarchy}
Describe how to introduce hierarchy.
E.g,, Specify in detail a leg for a table and a tabletop and then how to make the whole table. A leg for a table is made of a few components. a tabletop is made from a few components.
Check if you can make multiple levels of hierarchy.
Check if constraints still hold across multiple levels of hierarchy.

}
