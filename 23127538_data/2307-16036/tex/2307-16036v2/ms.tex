%\documentclass[12pt,preprint]{aastex}
%\documentclass[numberedappendix,appendixfloats]{emulateapj}
%\documentclass[aps,10pt,twocolumn,showpacs,preprintnumbers,amsmath,amssymb,floatfix]{revtex4}
%\documentclass[aps,10pt,twocolumn,showpacs,preprintnumbers,amsmath,amssymb,floatfix,nofootinbib]{revtex4}
%\documentclass[aps,prd,10pt,twocolumn,showpacs,preprintnumbers,amsmath,amssymb,floatfix,nofootinbib]{revtex4-1}
%\documentclass[aps,prd,10pt,twocolumn,showpacs,preprintnumbers,amsmath,amssymb,floatfix,nofootinbib]{revtex4} %hedp submission
\documentclass[aip, amsmath,amssymb, reprint]{revtex4-1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% JOURNAL SHORTCUTS

\def\mnras{{Mon.~ Not.~ R.~ Astron.~ Soc.~}}
\def\pr{{Phys.~ Rep.~ }}
\def\prd{{Phys.~ Rev.~ D.~}}
\def\prl{{Phys.~ Rev.~ Lett.~}}
\def\apj{{Astrophys.~ J.~}}
\def\apjs{{Astrophys.~ J.~ Suppl.~}}
\def\apjl{{Astrophys.~ J.~ Lett.~}}
\def\aa{{Astron.~ Astrophys.~}}
\def\nat{{Nature (London)~}}
\def\astropart{{Astro-particle Phys.~}}
\def\rvmp{{Rev.~ Mod.~ Phys.~}}

\def\mnras{{MNRAS}}
\def\pr{{PR}}
\def\prd{{PRD}}
\def\prl{{PRL}}
\def\apj{{ApJ}}
\def\apjs{{ApJS}}
\def\apjl{{ApJL}}
\def\aap{{A\&A}}
\def\nat{{Nature}}
\def\astropart{{Astro-particle Phys.~}}
\def\rvmp{{Rev.~ Mod.~ Phys.~}}
\def\physrep{{Phys.~ Rep.~}}
\def\jcap{Journal of Cosmology and Astro-Particle Physics}




%\documentclass{emulateapj}
\usepackage{natbib} %Default!

%\usepackage{graphicx, epsfig, bm, amsmath} 
\usepackage{color}
%\usepackage{hyperref} %Default

%\usepackage{ifthen} %Testing
%\usepackage{xstring} %Testing

\usepackage{listings}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage{array}
\usepackage[hyperfootnotes=true]{hyperref} %Testing
\usepackage{afterpage}
\usepackage{color}
\definecolor{orange}{cmyk}{0,0.5,1,0}
%\usepackage{ulem}
%\numberwithin{figure}{section}
%\usepackage{fullpage}
%\linespread{1.8}   % Uncomment so David can edit!
%\addtolength{\topmargin}{0.45in}

%\renewcommand{\textfraction}{0.05}
%\renewcommand{\floatpagefraction}{0.95}

%\newcommand{\blue}{\blue}
\newcommand{\blue}{}


\newcommand{\Msun}{\mathrm{M}_{\odot}}

% The following will make LaTeX stop floating figures to the last page
%\renewcommand\floatpagefraction{.9}
%\renewcommand\topfraction{.9}
%\renewcommand\bottomfraction{.9}
%\renewcommand\textfraction{.05}   

\begin{document}

\title{{\blue Applying Machine Learning Methods to Laser
Acceleration of Protons: Lessons Learned
from Synthetic Data}}


\author{Ronak Desai}
\email{desai.458@buckeyemail.osu.edu}
\affiliation{Department of Physics, The Ohio State University, Columbus, OH, 43210, USA}
\author{Thomas Zhang}
\affiliation{Department of Physics, The Ohio State University, Columbus, OH, 43210, USA}
\author{Ricky Oropeza}
\affiliation{Department of Physics, The Ohio State University, Columbus, OH, 43210, USA}
\author{John J. Felice}
\affiliation{Department of Physics, The Ohio State University, Columbus, OH, 43210, USA}
\author{Joseph R. Smith}
\affiliation{Department of Physics, Marietta College, Marietta, OH, 45750, USA}
\author{Alona Kryshchenko}
\affiliation{Department of Mathematics, California State University Channel Islands, Camarillo, CA 93012, USA}
\author{Chris Orban}
\affiliation{Department of Physics, The Ohio State University, Columbus, OH, 43210, USA}
\author{Michael L. Dexter}
\affiliation{Air Force Institute of Technology, WPAFB, OH 45433}
\author{Anil K. Patnaik}
\affiliation{Air Force Institute of Technology, WPAFB, OH 45433}

\date{\today}

\begin{abstract}
Researchers in the field of ultra-intense laser science are beginning to embrace machine learning methods. In this study we consider three different machine learning methods -- a two-hidden layer neural network, Support Vector Regression and Gaussian Process Regression -- and compare how well they can learn from a synthetic data set for proton acceleration in the Target Normal Sheath Acceleration regime. The synthetic data set was generated from a previously published theoretical model by Fuchs et al. 2005 that we modified. Once trained, these machine learning methods can assist with efforts to maximize the peak proton energy, or with the more general problem of configuring the laser system to produce a proton energy spectrum with desired characteristics. In our study we focus on both the accuracy of the machine learning methods and the performance on one GPU including the memory consumption.  Although it is arguably the least sophisticated machine learning model we considered, Support Vector Regression performed very well in our tests. 
\end{abstract}

\maketitle

\section{Introduction}
  \label{sec:intro}

The field of ultra-intense laser science is increasingly beginning to embrace machine learning methods \cite{Anirudh_etal2022,dopp_etal2023}. This is especially true as the repetition rates of ultra-intense laser systems increase and data acquisition systems improve (e.g. \cite{Heuer_etal2022}). However, to date, there has been limited use of machine learning (ML) to enhance and control proton acceleration from ultra-intense laser systems.  Recently, \citet{Loughran_etal2023} provide results from training a ML model on proton acceleration data using a laser system that operates at 1~Hz repetition rate using gaussian process regression and Bayesian optimization. \citet{Ma_etal2021} also describe at a high level ongoing efforts towards similar goals on laser systems operating at repetition rate of a few Hz using a neural network approach.

There are ultra-intense laser systems that operate at higher than 1~Hz repetition rates, and these systems are already being used in efforts to accelerate protons. \citet{Morrison_etal2018}, for example, accelerated protons to $\sim$2~MeV energies using few mJ ultra-intense laser pulses at a repetition rate of 1~kHz. This experiment could potentially be replicated on the many mJ class, $\sim$kHz repetition rate laser systems that exist today (e.g. \cite{Cao2023}). It is also true that future industrial or defense applications of ultra-intense laser systems will likely operate closer to this repetition rate regime \cite{Palmer2018}. Ideally, we would like to train ML models on these systems in quasi-real time. A natural question is therefore, of the ML models that are being used today, can they be quickly trained on tens of thousands of shots or more? Can this be achieved with modest computational resources such as a single GPU, or would it require a supercomputer or GPU cluster? 
The ability to train a ML model in quasi-real time could greatly assist efforts to optimize and/or control the properties of ion beams resulting from the laser interaction. Specifically, the ML model could help discover ways to increase the max proton energy, or it could help with the inverse problem of wanting a particular ion energy distribution and needing to know the laser parameters to produce that distribution.

In our study, which is essentially a numerical experiment, we will train three different ML models on synthetic data with added noise, described in Sec.~\ref{sec:synthetic}. For brevity, we do not actually use these models to perform hypothetical optimization or control tasks. Instead, we are only concerned with the the accuracy, performance and GPU memory consumption of the ML models. 

In Sec.~\ref{sec:synthetic}, we discuss our modified \citet{Fuchs2005} model and the noise added to it when generating the synthetic data set. In Sec.~\ref{sec:ml}, we describe the three different machine learning models used in this study. In Sec.\ref{sec:results}, we show our results. In Sec.\ref{sec:discuss} and Sec.\ref{sec:concl}, we summarize and conclude.


\section{Synthetic Data}
\label{sec:synthetic}

\subsection{Modified Fuchs et al. Model}

We generate synthetic data based on a physical model introduced by Fuchs et al. (2005) \cite{Fuchs2005}, which introduces a cutoff time to the  plasma expansion model by \citet{Mora_2003}. This model has five input parameters: laser intensity, wavelength, pulse duration, target thickness, and spot size. It also includes empirical models to estimate quantities such as laser absorption and hot electron temperature.
We extend the model (in a trivial way) by adding focal distance as an input and using the ideal Gaussian beam formula to determine the effective spot size and intensity. As will be discussed later, in our study we kept the pulse duration and wavelength fixed. The spot size was also ``fixed" in the sense that the spot size at peak focus was always the same, but for non-zero focal distance the effective spot size on target depends on the distance from the target to peak focus. This approach of keeping the pulse duration and laser wavelength fixed while moving the target through the focus of the laser is similar to experimental studies like \citet{Morrison_etal2018} and \citet{Loughran_etal2023}, which both found interesting features in the proton acceleration results even when the target was not at the peak focus.

The \citet{Fuchs2005} model provides an estimate of both the maximum proton energy as well as the total proton energy and average proton energy. In this way, the Fuchs model provides three outputs. To be clear, these are the protons accelerated by the laser interaction, not necessarily the protons in the bulk target that do not gain significant energy.

There are many semi-analytic models that exist that could have been used for this purpose (e.g. \cite{Schreiber_etal2006,Passoni_Lontano2008,Passoni_etal2010,Zimmer_etal2021}) and that are potentially more accurate than \citet{Fuchs2005}, especially at high intensity. We used \citet{Fuchs2005} because it is well known in the field, it is relatively simple, and because we are restricting our domain of interest to protons accelerated by the Target Normal Sheath Acceleration (TNSA) mechanism (e.g. \cite{Clark_etal2000,Hatchett_etal2000,Snavely_etal2000,Passoni_etal2010}).

We did make one substantial modification of the Fuchs model to improve agreement with experiments. In order to improve the predicted maximum proton energy in the intensity regime between $10^{18}$~W~cm$^{-2}$ and $10^{19}$~W~cm$^{-2}$, we changed the relationship between the acceleration timescale of the protons ($\tau_{\rm acc}$) and the laser pulse duration ($\tau_{\rm laser}$).
\begin{equation}
    \tau_{\rm acc} = 4.0 \cdot \tau_{\rm laser}
\end{equation}
This modification is inspired by \cite{DjordjeviÄ‡_etal2021}, who in their Sec.~5.3 treated the multiplier between the laser pulse duration and the proton acceleration timescale as a free parameter to be constrained by 1D Particle-in-Cell simulation results. Without this modification (i.e. the unmodified Fuchs et al. model), we found that the max proton energies in this intensity regime were too low compared to experiments in this intensity regime (e.g. \cite{Morrison_etal2018}).

Note that in computing the total proton energy and average proton energy from the \citet{Fuchs2005} model, we assumed the minimum kinetic energy to be zero for simplicity rather than using a non-zero cutoff.

\subsection{Range of Synthetic Data Generated}

We generate a synthetic data based on a laser system that can generate 40 femtosecond pulses, a spot size of 1.8 microns Full-Width Half-Max, and target thicknesses between 0.5 microns and 10 microns.  These parameters are similar to that of Morrison et al. 2018\cite{Morrison_etal2018}. We generated synthetic data with total pulse energies between 2.27~mJ and 22.72~mJ. At peak focus, these pulses correspond to intensities of $10^{18}$~W~cm$^{-2}$ and $10^{19}$~W~cm$^{-2}$ respectively. The focal distance was varied between -10 microns and +10 microns. Consequently the intensity on target for the generated points ranges from $\sim10^{17}$~W~cm$^{-2}$ to $10^{19}$~W~cm$^{-2}$. 

We generated 25,000 data points by randomly sampling this parameter space.
The highest proton energy in the data set was near 2.3~MeV while the lowest proton energy was 1.6~keV. The random sampling of the parameter space is an important detail because a real, kHz repetition rate laser experiment would sample the parameter space in a very specific way, for example, by scanning through the focal distance or translating the target to monotonically investigate different thicknesses.  We intentionally sampled the parameter space in a randomized way to avoid ``gaps" in the synthetic data set that might bias the results. The random sampling of the intensity was exponentiated uniform, so as not to focus too much on high intensity or low intensity, while the random sampling of other parameters was uniform across the range. In future work, we will investigate more realistic synthetic data sets that better mimic how data from real kHz laser experiments are gathered. 

We use up to 20,000 of the generated data points for training the ML models and reserve the remaining 5,000 points for testing. Our decision to use 20,000 data points as the maximum number of points in the training set, rather than set a higher or lower number, was informed by a few different priorities. On a 1~Hz repetition rate laser system like the one described in Loughran et al.\cite{Loughran_etal2023}, collecting 20,000 data points would take 5.5 hours, assuming continuous operation. Currently, there are a number of high-power laser systems operating near 1~Hz repetition rate, so 20,000 data points represent roughly one full day of experimental time on these systems. 

As mentioned earlier, we are also interested in kHz repetition rate systems (e.g. \cite{Morrison_etal2018}). These systems produce 20,000 shots in 20 seconds and 3.6 Million shots in an hour, but a number of practical limitations ranging from data acquisition systems to diagnostics to changing the laser parameters on a millisecond timescale or the limited speed that the target can be moved can mean that many of the shots on a kHz system probe essentially the same conditions. These and other practical limitations can easily cause an hour long experiment on a kHz laser system to only produce tens of thousands of shots with distinctly different conditions.

Focusing on up to 20,000 points is therefore relevant to both repetition rate regimes and it is a stepping stone towards the  million-plus shot data sets that kHz laser systems with upgraded data acquisition and diagnostics will eventually produce.

\subsection{Noise model}
\label{sec:noise}

To better represent experimental data, we added noise to all three output quantities of the model -- max proton energy, total proton energy, and average proton energy. 
Our noise model involved making Fuchs model predictions for our parameter space and sampling from a normal distribution using \emph{each} prediction as the mean. We generated data sets with different noise levels by assuming that the standard deviation is between 5\% and 30\% of the mean value. When, in later sections, we refer to a 10\% Gaussian noise level, for example, this means that we generated data by sampling a normal distribution assuming that the standard deviation was 10\% of the mean. The ``Gaussian" in this context refers to the Gaussian function that underlies the normal distribution.  Because the standard deviation is assumed to be some fraction of the mean, as the Fuchs model predictions become larger, the noise level also becomes proportionally larger.


The higher noise levels can potentially produce negative values for the proton energy (which would be unphysical), so we resample those points if that occurs.
We include with this publication the the Python code that was used to generate the synthetic data and the code that was used to train the models \cite{Ronak_link,Orban_link, ronak_desai_2023_data}.

\section{Machine Learning Methods}
\label{sec:ml}

%ML library versions (Placeholder comment until I figure out where to put them in the paper):
% Rapids: 22.08.00 (Ronak's version is 22.10.01)
% PyTorch: 1.12.1

We use the synthetic data to train three different machine learning models. These are Neural Network (NN), Support Vector Regression (SVR), and Gaussian Process Regression (GPR). In contrast to a neural network, SVR and GPR apply a ``kernel" function to map data to a higher dimensional space.

There are a number of caveats and important details in comparing these models, and we do not attempt to make a definitive comparison. What we do aim to do is to make reasonable efforts to compare the methods and their performance on a single GPU. Specifically, we use a NVIDIA Volta V100 GPU on the Pitzer cluster at the Ohio Supercomputer Center with 32~GB of GPU memory. Unless otherwise noted, we present results from training the ML models in single precision. 

As mentioned earlier, there are three independent variables in the data set -- target thickness, intensity and focal distance. First, we applied a logarithm to both the intensity and the three outputs of the model: maximum proton energy, total proton energy, average proton energy. Then, we applied standard z-score normalization\cite{han2011data} to all three inputs of the training set. We also apply z-score scaling to the outputs using the standard deviation and the mean of the training set. 

\subsection{Support Vector Regression}

SVR is based on Support Vector Machines which were developed for classification problems. In SVR, one specifies a tolerance value that is used to determine which points follow the relation given by a particular curve and which points are considered outliers. This makes the task of finding the best fit curve like a classification problem. For more information about SVR, see Smola 2004\cite{Smola2004ATO}. 

To accelerate the calculations with GPU, we used the cuML library,\cite{cuml} which is part of the RAPIDS package. The version for our cuML library was 22.10.01. Our investigation used an epsilon of 0.001 and we used the so-called radial basis function (RBF) kernel. The $\gamma$ value is set by cuML's ``scale" option, which automatically sets the $\gamma$ value equal to 1 / (number of features * input variance).  For more details of our specific implementation please see the attached Python file \cite{Ronak_link,Orban_link, ronak_desai_2023_training}.

\subsection{Neural Network}
\label{sec:nn}

We used PyTorch version 1.12.1 to implement a neural network (NN) model using a two hidden layer architecture described in Fig.~\ref{fig:nn_arch}. By using a smaller number of nodes in the second layer, this approach is intended to model the main trends in the data. With 64 nodes in the first hidden layer and 16 nodes in the second hidden layer, there are 1347 weights (including bias weights) in the network.  We used a ``leaky'' ReLU activation function between fully connected nodes.

% Figure environment removed

We train the neural network using the ``Adam" scheme for backpropagation as described in \citet{kingma2017adam}. We use an initial learning rate of 0.001, which the Adam scheme can increase or decrease as the model improves. For comparing with the other two ML models, we show NN model predictions after 35 epochs of training. 

\subsection{Gaussian Process Regression}
\label{sec:gp}

Gaussian Process Regression (GPR) is a Bayesian method which differs substantially from SVR and NN. The approach works by selecting a set of analytic functions that go through a series of data points. The GPR model can make a data-informed predictions by taking the weighted average of these functions, with the weights being the likelihood that any one of these functions is the ``true" function that correctly captures the model. A potential advantage over SVR and NN is that the GPR model uncertainty can be simply determined by the variance of these functions without any additional work. \citet{Schulz2017ATO} provides an excellent introduction to GPR.

To accelerate the computations with GPU, we used the GPyTorch library \cite{gardner2018gpytorch} version 1.9.1. We found this to be faster than the GPFlow library for our problem case.  As discussed in \cite{gardner2018gpytorch}, numerical instabilities can arise in cases with low or zero noise which can affect the accuracy of the GPR model. To avoid this we configured GPyTorch to use a maximum of 50 iterations.  In our tests (not shown for brevity) this improved the accuracy of GPR for Gaussian noise levels of $\leq$5\%. We used the RBF kernel for our GPR model.

\section{Results}
\label{sec:results}

% Figure environment removed

\subsection{Accuracy of Trained Models}

Fig.~\ref{fig:accuracy} highlights results for the Mean Absolute Percentage Error (MAPE) for the different ML models trained on data with 10\% Gaussian noise and different numbers of training points. To determine the percentage error, the ML models were compared to 5,000 testing points (regardless of the number of training points). Naturally, when a ML model prediction is compared to the 5,000 testing points, each of these points will have  a percentage error associated with it. In Fig.~\ref{fig:accuracy} we present the mean of that percentage error after an absolute value is taken (i.e.~the ``absolute" in MAPE is referring to the absolute value). In understanding Fig.~\ref{fig:accuracy} it is important to note that if noiseless data were compared, for all of the same input conditions, to noise-added data assuming a 10\% Gaussian noise level (Sec.\ref{sec:noise}), one would find that the noiseless data typically differs from the noise-added data by 8 percent.  
Therefore, we do not expect the ML models to be able to predict better than about 8\% in this comparison because the 5,000 testing points do include 10\% Gaussian noise. We show this expected lower limit on the MAPE in Fig.~\ref{fig:accuracy} as dashed black horizontal lines. 

Fig.~\ref{fig:rms_accuracy} shows the accuracy in terms of root mean square error for predicting the max proton energy using the three different ML models with different levels of Gaussian noise. 
Similar to Fig.~\ref{fig:accuracy}, this plot was made by comparing trained ML models to 5,000 testing points. Dot-dashed black horizontal lines indicate how accurate the ML model would be if the only source of error was the noise.
In Fig.~\ref{fig:accuracy}, at 10\% Gaussian noise, there are differences in the ML models, but overall, the model predictions are comparable to the expected lower limit on the MAPE, even with as few as $\sim$2,000 points. When the Gaussian noise level increases to 20\% or 30\%, the ML models require more training points to approach the expected lower limit on the MAPE. 

We are interested in determining not only whether the ML models are reasonably accurate compared to the known level of noise in our synthetic data set, but also how well the ML models (which are trained on noisy data) compare to noiseless data. Fig.~\ref{fig:err} compares the accuracy of the ML models trained on 20,000 data points with between 0\% to 30\% Gaussian noise compared to 5,000 test data points that both do and do not include noise. The figure shows that when the ML models are compared to the noisy data, except for the low noise NN results, the RMS error tends to be dominated by the noise as expected. Compared to noiseless data, the ML models (which were trained on noisy data) tend to have an RMS error that is smaller than the data sets they were trained on. Again, the only exception is the NN results for $\leq$5\% Gaussian noise.


% Figure environment removed

% Figure environment removed



\subsection{Performance and Memory Consumption}

Fig.~\ref{fig:performance} shows our primary results for the execution time of the different ML models using between 1,000 and 20,000 synthetic data points for training with 10\% Gaussian noise. As mentioned earlier, our primary results are from using one Nvidia V-100 GPU with 32~GB of memory with single precision. An important note is that the ``training time" on the y-axis in Fig.~\ref{fig:performance} includes the time to train on all three of the outputs of interest -- the max proton energy, total proton energy and average proton energy. For GPR and SVR, we treated these three outputs separately which means that the training time for these models included three separate instances of training on the synthetic data. The training time for GPR and SVR are therefore the sum of these three training events. For the measured training time for NN, we used the same neural network to predict the three output quantities (Fig.~\ref{fig:nn_arch}), rather than create and train three separate neural networks. Tests with running the NN model separately for all three outputs (not shown) produced similar performance results.

% Figure environment removed

Of the three models, the clear winner with respect to execution time is SVR, followed by NN and, last, GPR. Although we expected the execution time for GPR to approach $O(N^3)$ \cite{wang2022intuitive}, the increase in the execution time with increasing numbers of training data points was not as steep as this, at least for the range of $N$ that we probed and the data set we used. Tests with double precision (not shown) did not fundamentally change the ordering of the results -- SVR remained the fastest method while GPR remained the slowest.

Table~\ref{tab:memory} summarizes the average GPU memory consumption of the three different ML models on 20,000 synthetic data points. The NN had the lowest memory consumption while GPR consumed the most memory. All results are for single precision. In tests with double precision (not shown), GPR consumed about twice as much GPU memory while SVR and NN consumed about the same amount of GPU memory as single precision.


\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & SVR & NN & GPR    \\
         \hline
       Ave GPU Memory Utilization (GiB)  & 1.9 & 1.1  & 10.7    \\
       \hline
    \end{tabular}
    \caption{Memory Consumption Results (20,000 synthetic data points). [Associated dataset available at https://doi.org/10.5281/zenodo.8221343]\cite{ronak_desai_2023_metrics}
    }
    \label{tab:memory}
\end{table}

\section{Discussion}
\label{sec:discuss}

The three ML models overall did an adequate job of training on the synthetic data set and predicting both noisy and noiseless data when compared to test data that the models had not seen before. The only exception to this was the low noise results for the neural network model, where the inaccuracy of the neural network model was the dominant source of error (Fig.~\ref{fig:err}). Overall the neural network was not as accurate as SVR or GPR, which was true across the range of noise levels we examined and for both noisy and noiseless data (Fig.~\ref{fig:err}). As a comment on this result, it is important to note that the approach of the neural network model is very different from either SVR and GPR in that it is essentially trying to fit 1347 free parameters using 20,000 data points or less. A reasonable question is whether our choice of using only two hidden layers (Fig.~\ref{fig:nn_arch}) instead some larger number of hidden layers could have affected this result. In Appendix~\ref{ap:hidden} we consider this question and conclude that between one and two layers seem to give the best results if the total number of parameters are approximately fixed. 

An interesting result that we did not necessarily expect was that SVR and GPR often achieved high accuracy even with as few as $\sim$2000 training points (Fig.~\ref{fig:accuracy} \& Fig.~\ref{fig:rms_accuracy}). The NN results were not as impressive but in some cases reasonably high accuracy was achieved for $<$5,000 training points. 
Achieving high accuracy without extreme numbers of training points certainly is a reflection of the quality of the ML models, but it is also true that the Fuchs et al.\cite{Fuchs2005} based model that we used to generate the synthetic data set is a relatively well behaved function without any sharp features. The relative simplicity of the Fuchs et al.\cite{Fuchs2005} generated data set (compared, for example, to an MNIST data set with hand written numbers between 0 and 9\cite{mnist}) certainly helped the ML models to accurately represent our synthetic data sets even from a relatively small number of points. In future work, it will be interesting to see if more sophisticated synthetic data sets with less ideal noise characteristics or experimental data will require more points to reach a similar level of agreement.

Another expectation we had was that SVR would be less accurate than GPR or NN because it is arguably the least sophisticated ML model that we used. But we found, at least for our data sets, that the SVR was similarly as accurate as the other models.

We examined the performance of the ML models running on one GPU. We found that the SVR training time was much less than GPR and NN, which is perhaps not surprising since SVR has a reputation for being a fast method (e.g. \cite{towardsdatascience,Xu_etal2011}).
We found that the GPR method consumed the most GPU RAM of the three models. Ultra-intense laser experiments that produce significantly more then 20,000 data points on a short timescale may need to worry about GPU RAM constraints.

Overall, the performance of the ML models in terms of execution time were such that training on quasi-real time data from a kHz ultra-intense laser system using only one GPU should be feasible, especially for SVR. Of the three ML models, the longest execution time was the GPR which took about 30 seconds to train on 20,000 data points (Fig.~\ref{fig:performance}). It is difficult to use our results to extrapolate to an order of magnitude more data than 20,000 points, but overall we find that even the most time intensive ML model is still in the range where one could envision quasi-real time operation with a kHz repetition rate laser system with single-shot diagnostics. 

We stress that these results depend on the particular parameter scan that we chose and the details of the Fuchs et al. \cite{Fuchs2005} model. More complicated data sets with more features will likely require more compute time to adequately train the ML models. 

\section{Conclusions}
\label{sec:concl}

We tested three different machine learning models on a synthetic data set for laser-accelerated protons by training these models on up to 20,000 synthetic data points. The data set was generated using a modified Fuchs et al. \cite{Fuchs2005} model that included Gaussian noise to simulate the kind of noise that could be present in a real experiment. The machine learning models were Gaussian Process Regression (GPR), Support Vector Regression (SVR) and a two-hidden-layer neural network (NN). 

Overall, we find that the three ML models adequately modeled the synthetic data sets. When ML model predictions were compared to data points that were not included in the training set, the dominant source of error was typically the noise that was artificially added to the synthetic data sets. When compared to noiseless data, the models, which were trained on noisy data, typically deviated from the noiseless data at a level that was well below the noise that was added to the data set (i.e. the models successfully averaged out the noise). GPR and SVR were the most accurate models. 

In terms of performance on one GPU, SVR was by far the fastest model for execution time. GPR was the slowest, taking 30 seconds to train on 20,000 data points. The neural network used the least amount of GPU memory while GPR consumed 5-10 times more GPU memory than the other models. Generally the performance results indicate that quasi-real time training of these models on kHz repetition rate laser systems should be feasible, especially with SVR.

We stress that these results should not be regarded as the final word on the usefulness of these ML models in the context of laser acceleration of protons. More complex data sets may yield different results. Moreover, there are certainly ML models that deserve attention that we did not study here. 

We provide Jupyter notebooks with our Python code. We also provide the 25,000 point synthetic data set that we generated using this code \cite{Ronak_link,Orban_link}. By providing these files we hope to encourage others to compare other ML models against our results as a benchmark.

\appendix
\onecolumngrid

\section{On the Number of Hidden Layers}
\label{ap:hidden}

The number of hidden layers is an important concern when designing a neural network. Work done by Hornik et al 1989\cite{HORNIK1989359} and Leshno et al 1993\cite{LESHNO1993861} showed that neural networks with at least one hidden layer and using a non-polynomial activation function can serve as an universal approximator for any function given a sufficient number of nodes. Adding more hidden layers can, in some cases, reduce the training time and allow for faster convergence for a neural network. However, this depends on the data set with more complex data sets benefiting from additional hidden layers.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         Hidden Layers & Nodes per Hidden Layer & Total Parameters    \\
         \hline
         1 & 192 & 1347\\
         \hline
         2 & 33 & 1356\\
         \hline
         3 & 24 & 1371\\
         \hline
         4 & 20 & 1403\\
         \hline
         5 & 17 & 1346\\
         \hline
         6 & 15 & 1308\\
         \hline
    \end{tabular}
    \caption{Details of six different neural networks}
    \label{tab:Parameters}
\end{table}

% Figure environment removed

As discussed in Sec.~\ref{sec:nn}, our fiducial neural network model assumes two hidden layers (Fig.~\ref{fig:nn_arch}). In this Appendix, we provide results from tests we performed with larger numbers of hidden layers. To determine if additional hidden layers would be advantageous for our data set, we performed a numerical experiment where we trained neural networks with different numbers of hidden layers and compared their accuracy on noiseless data that was not in the training set. We note that, unlike the two-hidden layer neural network depicted in Fig. \ref{fig:nn_arch}, which has more nodes in the first layer than the second, the hidden layers for the neural networks in these tests used the same number of nodes for simplicity. In order to ensure that each neural network has roughly the same number of parameters, we set the number of nodes in each hidden layer to be fewer for neural networks with more hidden layers. Table \ref{tab:Parameters} shows exactly how many free parameters were in each neural network. This was an important step because keeping the number of nodes per layer the same would naturally advantage models with more hidden layers, as those models would have many times more free parameters than the models with fewer hidden layers.

We found that neural networks with more hidden layers did not necessarily produce more accurate results for our specific problem case. Fig. \ref{fig:multi_layer} shows the mean average percent error for the max proton energy from the six different neural network architectures versus the number of training points. The figure shows that neural networks with more hidden layers did not yield more accurate results than neural networks with fewer hidden layers.

%\twocolumngrid
\section*{Acknowledgements}
%\acknowledgements
 
Supercomputer allocations for this project included time from the Ohio Supercomputer Center. We also thank Pedro Gaxiola for help with early work with SVR. We acknowledge support provided by the National Science Foundation (NSF) under Grant No.  2109222. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.  CO and RO were supported in summer 2022 by the Air Force Office of Science Research summer faculty program.  This work was supported by Air Force Office of Scientific Research (AFOSR) Award (PM: Dr. Andrew B. Stickrath). This work was also supported by Department of Energy (PM: Dr. Kramer Akli).


\section*{References}


\bibliography{ms}
\bibliographystyle{apsrev}



\end{document}
