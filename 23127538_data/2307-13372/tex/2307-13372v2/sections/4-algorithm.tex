\looseness -1 We now propose a practical algorithm for \subrl that can efficiently handle submodular rewards. The core of our approach follows a greedy gradient update on the policy $\pi$. As common in the modern \RL literature, we make use of approximation techniques for the policies to derive a method applicable to large state-action spaces. This means the policy $\pi_{\theta}(a|s)$\footnote{autoregressive policies (RNNs or transformers) can be used to capture history-dependence in the same algorithm} 
is parameterized by $\theta \in \Theta$ where $\Theta \subset \R^l$ is compact. In the case of tabular $\pi$, $\theta$ specifies an independent distribution over actions for each state.

\mypar{Approach} 
\looseness -1 The objective from \cref{eq: obj} can be equivalently formulated as $\theta^{\star} \in \argmax_{\theta \in \Theta} J(\pi_\theta)$ as $\theta$ indexes our policy class. Due to the nonlinearity of the parameterization, it is often not feasible to find a global optimum for the above problem. In practice, with appropriate initialization and hyperparameters, variants of gradient descent are known to perform well empirically for MDPs. Precisely, \vspace{-0.4em}
\begin{align}
    \theta \leftarrow \theta + \argmax_{\delta \theta: \delta \theta + \theta \in \Theta} \delta \theta^\top \nabla_{\theta} J(\pi_\theta) - \frac{1}{2 \alpha} \|\delta\theta\|^2. \label{eqn: gd} 
\end{align}
Various \PG methods arise with different methods for gradient estimation and applying regularization \citep{kakade2001natural,schulman2015trust,schulman2017proximal}. The key challenge to all of them is computation of the gradient $\nabla J(\pi_\theta)$. Below, we devise an unbiased gradient estimator for general non-additive functions.

\mypar{Gradient Estimator} As common in the policy gradient (\PG) literature, we can use the score function $g(\traj, \pi_\theta) \coloneqq \nabla_{\theta} (\log \prod\nolimits_{i=0}^{\horizon-1}\pi_{\theta}(a_i|s_i))$ to calculate the gradient $\nabla_{\theta} J$. Namely, Given an \mdp and the policy parameters $\theta$, \vspace{-0.4em} 
\begin{equation}  \label{prop: score}\nabla_{\theta} J(\pi_{\theta}) = \sum_{\traj} f(\traj; \pi_\theta) g(\traj, \pi_\theta) F(\traj).
\end{equation}
\looseness -1 As \cref{prop: score} shows, we do not require knowledge of the environment if sampled trajectories are available. It also does not require full observability of the states nor any structural assumption on the \mdp. On the other hand, the score gradients suffer from high variance due to sparsity induced by trajectory rewards \citep{FU_score,prajapat2021competitive,sutton2018reinforcement}. Hence, we take the \smdp structure into account to develop efficient algorithms. \vspace{-0.1em}

\emph{Marginal gain:} We define the marginal gain for a state $s$ in the trajectory $\traj_{0:j}$ up to horizon $j$ as \vspace{-0.25em}\[F(s|\traj_{0:j}) = F(\traj_{0:j}\cup \{s\}) - F(\traj_{0:j}).\vspace{-0.25em}\] Our approach aims to maximize the marginal gain associated with each action instead of maximizing state rewards. 
This approach shares similarities with the greedy algorithm commonly used in submodular maximization, which maximizes marginal gains and is known for its effectiveness. Moreover, decomposing the trajectory return into marginal gains and incorporating it in the policy gradient with suitable baselines \cite{greensmith2004variance} removes sparsity and thus helps to reduce variance. Inspired by the policy gradient method for additive rewards \citep{sutton1999policy,baxter2001infinite}, we propose the following for \smdp: %For $\traj_{l:l'}$, the events from time step $l$ to $l'$, we have:
\vspace{-0.1em}
\begin{restatable*}{theorm}{restatePG} \label{thm: PG} Given an \smdp and the policy parameters $\theta$, with any set function $\Obj$,\vspace{-0.5em}
    \begin{align}
        \nabla_{\theta} J(\pi_{\theta}) = \E_{\traj \sim f(\traj ; \pi_\theta)} \left[ \sum_{i=0}^{H-1} \nabla_{\theta} \log \pi_{\theta}(a_i|s_i) \left(\sum_{j=i}^{H-1}F(s_{j+1}|\traj_{0:j}) - b(\traj_{0:i})\right) \right] \label{eqn: grad_esti}
    \end{align}\vspace{-1.1em}
\end{restatable*} \vspace{-0.6em}
We use an importance sampling estimator (log trick) to obtain \cref{prop: score}. To reduce variance, we subtract a baseline $b(\traj_{0:i})$ from the score gradient, which can be a function of the past trajectory $\traj_{0:j}$. This incorporates the causality property in the estimator, ensuring that the action at timestep $j$ cannot affect previously observed states. After simplifying and considering marginals, we obtain \cref{thm: PG} (proof is in \cref{apx: PG}). This estimator assigns a higher weight to policies with high marginal gains and a lower weight to policies with low marginal gains. Empirically this performs very well (\cref{sec: experiments}). 

We can optimize this approach by using an appropriate baseline as a function of the history $\traj_{0:j}$, which leads to an actor-critic type method. The versatility of the approach is demonstrated by the fact that \cref{thm: PG} holds for any choice of baseline critic. We explain later in the experiments how to choose a baseline. One can perform a Monte Carlo estimate ~\citep{baird1993} or generalized advantage function (\GAE)~\citep{schulman2015highdimensional} to estimate returns based on the marginal gain.
% However, what the ideal critic function is for non-additive functions needs to be clarified, and we leave it as future work. 
To encourage exploration, similar to standard \PG, we can employ a soft policy update based on entropy penalization, resulting in diverse trajectory samples. Entropy penalization in \subrl can be thought of as the sum of modular and submodular rewards, which is a submodular function.

\mypar{Algorithm} The outline of the steps is given in \cref{alg:subrl}. We represent the agent by a stochastic policy parameterized by a neural network. 
% At each state, the agent uses a categorical distribution over the set of actions. We apply a softmax operation to the distribution to enforce the simplex constraint. 
The algorithm operates in epochs and assumes a way to generate samples from the environment, e.g., via a simulator. In each epoch, the agent recursively samples actions from its stochastic policy and applies them in the environment leading to a \emph{roll out} of the trajectory where it collects samples (\cref{alg: sample_collect}). We execute multiple ($B$) batches in each epoch for accurate gradient estimation.
To update the policy, we compute the estimator of the policy gradient as per \cref{thm: PG}, where we utilize marginal gains of the trajectory instead of immediate rewards as in  standard RL (\cref{alg: estimator}). Finally, we use stochastic gradient ascent to update the policy parameters.

