% 1. submodularity: set function, DR property, monotonicity
\mypar{Submodularity} Let $\V$ be a ground set. A set function $\Obj: 2^{\V} \to \R$ is {\em submodular} if $\forall A \subseteq B \subseteq \V$, $v\in \V \backslash B$, we have, $\Obj(A \cup \{v\}) - \Obj(A) \geq \Obj(B \cup \{v\}) - \Obj(B)$. The property captures a notion of diminishing returns, i.e., adding an element $v$ to $A$ will help at least as much as adding it to the superset $B$. We denote the marginal gain of element $v$ as $\Delta(v|A)\coloneqq \Obj(A \cup \{v\}) - \Obj(A)$. Functions for which $\Delta(v|A)$ is {\em independent} of $A$ are called {\em modular}.
$\Obj$ is said to be {\em monotone} if $\forall A \subseteq B \subseteq \V$, we have, $\Obj(A) \leq F(B)$ (or, equiv., $\Delta(v\mid A)\geq 0$ for all $v$, $A$).

\looseness=-1
\mypar{Controlled Markov Process (\cmp)} A \cmp is a tuple formed by $\langle \V, \A, \mP, \rho, H \rangle$, where $\V$ is the ground set, $v \in \V$ is a state, $\A$ is the action space with $a\in\A$. $\rho$ denotes the initial state distribution and $\mP = \{P_h\}_{h=0}^{H-1}$, where $P_{\h}(v'|v,a)$ is the distribution of successive state $v'$ after taking action $a$ at state $v$ at horizon $\h$. We consider an episodic setting with finite horizon $H$.

\looseness= -1
\mypar{Submodular \mdp} We define a {\em submodular \mdp}, or \smdp, as a \cmp with a monotone submodular reward function, i.e., a tuple formed by $\langle \S, \A, \mP, \rho,  H, \Obj\rangle$. Hereby, using $\S = H \times \V$ to designate time-augmented states (i.e., each $s=(\h,v) \in \S$ is a state augmented with the current horizon step), we assume $\Obj:2^{\S} \to \R$\footnote{Without loss of generality, this can be extended to state-action based rewards} is a monotone submodular reward function. The non-stationary transition distribution $P_{\h}(v'|v, a)$ of the \cmp can be equivalently converted to $P((\h+1,v')|(\h, v), a) = P(s'|s, a)$ since $s$ accounts for time. An episode starts at $s_0 \sim \rho$, and at each time step $\h\geq 0$ at state $s_{\h}$, the agent draws its action $a_{\h}$ according to a policy $\pi$ (see below). The environment evolves to a new state $s_{\h+1}$ following the \cmp. A realization of this stochastic process is a trajectory $\tau = \big( (s_{\h},a_{\h})_{\h=0}^{H-1}, s_{H}\big)$, an ordered sequence with a fixed horizon $H$. $\traj_{l:l'}=\big( (s_{\h},a_{\h})_{\h=l}^{l'-1}, s_{l'}\big)$ denotes the part from time step $l$ to $l'$. Note that $\tau=\tau_{0:H}$. For each (partial) trajectory $\tau_{l:l'}$, we use the notation $F(\tau_{l:l'})$ to refer to the objective $F$ evaluated on the set of (state,time)-pairs visited by $\tau_{l:l'}$.
%interpreted as a subset of $\S$, and we 

% \vspace{-2em}
%%%%%%%%%%%%%Give intution that the optimal policy is non markovian
\looseness=-1
\mypar{Policies} The agent acts in the \smdp according to a {\em policy}, which in general maps histories $\tau_{0:h}$ to (distributions over) actions $a_h$.
A policy $\pi(a_{\h}|\tau_{0:h})$ is called {\em Markovian} if its actions only depend on the current state $s$, i.e., $\pi(a_{\h}|\tau_{0:h})=\pi(a_{\h}|s_h)$. The set of all Markovian policies is denoted by $\Pi_{\mv}$. Similarly, we can consider Non-Markovian policies $\pi(a_{\h}|\traj_{\h-k:\h})$ that only depend on the previous past $k$ steps $\traj_{\h-k:\h}$. The set of all Non-Markovian policies conditioned on history up to past $k$ steps is denoted by $\Pi^k_{\nm}$, and we use $\Pi_{\nm}:=\Pi^H_{\nm}$ to allow arbitrary history-dependence.


\looseness=-1
\mypar{Problem Statement} For a given submodular MDP, we want to find a policy to maximize its expected reward. For a given policy $\pi$, let $f(\tau;\pi)$ denote the probability distribution over the random trajectory $\tau$ following agent's policy $\pi$, \vspace{-0.5 em}
\begin{align} \label{eq: trajectory_distribution}
\vspace{-3.5 em}
f(\tau; \pi) = \rho(s_0) \prod_{\h=0}^{\horizon-1} \pi(a_{\h}|\tau_{0:\h}) P(s_{\h+1}|s_{\h},a_{\h}). \numberthis
\vspace{-2.5 em}
\end{align}
The performance measure $J(\pi)$ is defined as the expectation of the submodular set function over the trajectory distribution induced by the policy $\pi$ and the goal is to find a policy that maximizes the performance measure within a given family of policies $\Pi$ (e.g., Markovian ones). Precisely,
\begin{align}
\vspace{-1.5 em}
    \pi^{\star} = \argmax_{\pi \in \Pi} J(\pi),~\text{where}~ J(\pi)=\! \sum_{\tau} \!f(\tau;\pi) \Obj(\tau). \label{eq: obj}
\vspace{-1.5 em}
\end{align}
Given the non-additive reward function $F$, the optimal policy on Markovian and non-Markovian policy classes can differ (since the reward depends on the history, the policy may need to take the history into account for taking optimal actions). 
In general, even representing arbitrary non-Markovian policies is intractable, as its description size would grow exponentially with the horizon.
%would be of size$Omega((|\S||\A|)^{H}$ it requires. 
%Perhaps suprisingly, as we will show below, in some cases 
It is easy to see that for {\em deterministic} MDPs, the optimal policy is indeed attained by a deterministic Markovian policy:
% \begin{proposition}
\begin{restatable*}{prop}{restatedetmarkov} \label{prop:deterministicmarkov}For any deterministic \mdp with a fixed initial state, the optimal Markovian policy achieves the same value as the optimal non-Markovian policy.
\end{restatable*}  \vspace{-0.5em}
% While this is not true in general, in this paper, we focus on optimizing over the family of Markovian policies.  
%Therefore, \cref{eq: obj} defines the submodular reinforcement learning (\subrl) problem as optimizing in the class for Markovian policies $(\Pi_{\mv})$.
%We aim to find an optimal deterministic policy within the Markovian policy class $\Pi_M$. 
The following proposition guarantees that, even for stochastic transitions, there always exists an optimal deterministic policy among the family of Markovian policies $\Pi_M$. Thus, we do not incur a loss compared to stochastic policies.
%the existence of such a policy for objectives expressed as \cref{eq: obj}. 
% \begin{proposition}
\begin{restatable*}{prop}{restatedetMDP} \label{prop:restatedetMDP}
% For any set function $F$, there exists an optimal Markovian policy that is deterministic.
For any set function $F$, among the Markovian policies $\Pi_\mv$, there exists an optimal policy that is deterministic.
\end{restatable*}\vspace{-0.5em}
The proof is in \cref{apx: deterministic_policy}. 
The result extends to any non-Markovian policy class $\Pi^{k}_{\nm}$ for as well, since one can group together the past $k$ states and treat it as high-dimensional Markovian state space.

\mypar{Examples of submodular rewards} 
We first observe that classical MDPs are a strict special case of submodular MDPs.  Indeed, for some classical reward function $r:\V\to\R_{+}$, by setting $F((h_1,v_1),\dots,(h_k,v_k)):=\sum_{l=1}^k \gamma^{h_l} r(v_l)$, $F(\tau)$ simply recovers the (discounted) sum of rewards of the states visited by trajectory $\tau$ (hereby, $\gamma\in[0,1]$, i.e., use $\gamma=1$ for the undiscounted setting).

A generic way to construct submodular rewards is to take a submodular set function $F':2^\V\to\R$ defined on the ground set $\V$, and define $F(\traj) \coloneqq F'(\mathbf{T} \traj)$, using an operator $\mathbf{T}: 2^{\horizon \times \V} \to 2^{\V}$ that drops the time indices. Thus, $F(\tau)$ measures the value of the set of states $\mathbf{T} \traj\subseteq \V$ visited by $\tau$. Note that each state is counted only {\em once}, i.e., even if $F'$ is a modular function, $F$ exhibits diminishing returns. There are many practical examples of $F'$, such as coverage functions, experimental design criteria such as mutual information and others that have found applications in machine learning tasks \citep[cf.][]{krause2014submodular,bilmes2022submodularity}. Moreover, many operations preserve submodularity, which can be exploited to build complex submodular objectives from simpler ones \citep[section 1.2]{krause2014submodular} and can potentially be used in reward shaping. While submodularity is often considered for discrete $\V$, the concept naturally generalizes to continuous domains.






































