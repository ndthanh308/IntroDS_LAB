In reinforcement learning (\RL), the agent aims to learn a policy by interacting with its environment 
%, receives rewards and learns a policy to maximize the cumulative sum of the 
in order to maximize rewards obtained. Typically, in \RL, the environments are modelled as a (controllable) Markov chain, and the rewards are considered {\em additive} and {\em independent of the trajectory}. In this well-understood setting, referred to as Markov Decision Processes (\mdp), the Bellman optimality principle allows to find an optimal policy 
%gives the optimal algorithm that runs 
in polynomial time for finite Markov chains \citep{puterman_MDPbook, sutton2018reinforcement}. However, many interesting problems cannot straight-forwardly be modelled via additive rewards.
%fall outside the additive (modular) rewards realm. 
In this paper, we consider a rich and fundamental class of non-additive rewards, in particular {\em submodular reward functions}. Applications for planning under submodular rewards abound, from coverage control \citep{near_optimal_safe_cov}, entropy maximization, experiment design \citep{sensor-placement-andreas}, informative path planning,  orienteering problem \cite{GUNAWAN2016315}, resource allocation to Mapping~\citep{Swarm-SLAM}.

Submodular functions capture an intuitive diminishing returns property that naturally arises in these applications: {\em the reward obtained by visiting a state decreases in light of similar states visited previously}. 
E.g., in a biodiversity monitoring application (see \cref{fig: DR}), if the agent has covered a particular region, the information gathered from neighbouring regions becomes redundant and tends to diminish.
To tackle such history-dependent, non-Markovian rewards, one could naively augment the state to include all the past states visited so far. This approach, however, exponentially increases the state-space size, leading to intractability. 
In this paper, we make the following contributions:

\textbf{First,} we introduce \subrl, a paradigm for reinforcement learning with submodular reward functions. While this is the first work to consider submodular objectives in \RL, we connect it to related areas such as submodular optimization, convex \RL in \cref{sec: related_works}. To establish limits of the \subrl framework, we derive a lower bound that establishes hardness of approximation up to log factors (i.e., ruling out any constant factor approximation) in general (\cref{sec: NP_hard}). \textbf{Second,} despite the hardness, we show that, in many important cases,  \subrl instances can often be effectively solved in practice. In particular, we propose an algorithm, \subPO, motivated by the greedy algorithm in classic submodular optimization. It is a simple policy-gradient-based algorithm for \subrl that handles non-additive rewards by greedily maximizing marginal gains (\cref{sec: algo}).
\textbf{Third,} we show that in some restricted settings, \subPO performs {\em provably} well.  
In particular, under specific assumptions on the underlying \mdp, \subrl reduces to an instance of constrained continuous DR-submodular optimization over the policy space. Even though the reduced problem is still NP-hard, we guarantee convergence to the information-theoretically optimal constant factor approximation of $1-1/e$, generalizing previous results on submodular bandits. \rev{Moreover, for general \MDPs, if the submodular function has bounded curvature, we show \subPO achieves a constant factor approximation.}
(\cref{sec: theory}). \textbf{Lastly,} we demonstrate the practical utility of \subPO in simulation as well as real-world applications. Namely, we showcase its use in biodiversity monitoring, Bayesian experiment design, informative path planning, building exploration, car racing and Mujoco robotics tasks. Our algorithm is sample efficient, discovers effective strategies, and scales well to high dimensional spaces (\cref{sec: experiments}).

% Figure environment removed































