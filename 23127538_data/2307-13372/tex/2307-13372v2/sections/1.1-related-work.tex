\mypar{Beyond Markovian \RL}
%We extend the modelling ability of classical reinforcement learning with Markov chain and Markovian rewards and consider a class of non-Markovian reward functions. 
Several prior works in \RL identify the deficiency in the modelling ability of classical Markovian rewards. This manifests itself especially when exploration is desired, e.g., when the transition dynamics are not completely known \citep{tarbouriech2020active, Hazan2019} or when the reward is not completely known \cite{lindner2021information, Belogolovsky2021}. \citet{chatterji2021theory} considers binary feedback drawn from the logistic model at the end episode and explores state representation, such as additivity, to propose an algorithm capable of learning non-Markovian policies.
While all these address in some aspect the shortcomings of Markovian rewards, they tend to focus on a specific aspect instead of postulating a new class of reward functions as we do in this work. %Regarding the expressivity \citep{abel2021expressivity} of the non-additive rewards, if policy search is over the Markovian class, the optimal policy is deterministic (\cref{prop:restatedetMDP}) and thus there exists a Markovian reward that leads to the same optimal policy. However, since we do not restrict to the Markovian policies, given a policy class with compact history representation, \subPO can learn behaviours beyond the expressivity of the Markovian rewards.

%Regarding the expressivity of the non-additive rewards \citep{abel2021expressivity}, if policy search is over the Markovian class, the optimal policy is deterministic (\cref{prop:restatedetMDP}), thus there exists a Markovian reward that leads to the same optimal policy. However, since we do not restrict to the Markovian policies, SubPO can learn behaviours beyond the expressivity of the Markovian rewards.}
%Under submodular rewards, if policy search is over the Markovian class, the optimal policy is deterministic (\cref{prop:restatedetMDP}), thus there exists a Markovian reward that would lead to the same optimal policy. However, finding such Markovian rewards is NP-hard to approximate (\cref{thm: inappx}), hence does not help to solve the problem. }
%In contrast to finding such surrogate Markovian rewards, submodularity provides a natural way to capture the task. Moreover, since we do not restrict to the Markovian policies, SubPO can learn behaviours beyond the expressivity of the Markovian rewards \citep{abel2021expressivity}.}
% In contrast to finding such surrogate Markovian rewards, submodularity provides a natural way to capture the task, and since we do not restrict to the Markovian policies, given a policy class with compact history representation, SubPO can learn behaviours beyond the expressivity of the Markovian rewards \citep{abel2021expressivity}.}
% \add{The optimal policies for submodular rewards cannot be captured by the Markovian rewards in general since the optimal policies are non-Markovian. However, when the policy search is restricted to the Markovian class, it is correct that the optimal policy is deterministic, and hence there exists a Markovian reward that would lead to the same optimal policy. .
% Interestingly, in order to optimize non-additive rewards, if policy search is over the Markovian class, the optimal policy is deterministic (\cref{prop:restatedetMDP}), thus there exists a Markovian reward that would lead to the same optimal policy. However, finding such Markovian rewards is NP-hard to approximate (\cref{thm: inappx}), hence does not help to solve the problem. In contrast to finding such surrogate Markovian rewards, submodularity provides a natural way to capture the task. Moreover, since we do not restrict to the Markovian policies, given a policy class with compact history representation, SubPO can learn behaviours beyond the expressivity of the Markovian rewards \citep{abel2021expressivity}.
% However, finding such Markovian rewards has to be NP-hard to approximate. Hence it does not help to solve the problem.

% The submodular rewards provide a natural way to capture the task, and our empirical observations suggest they can be optimised as easily as additive rewards leaving us to conclude that the search for a “surrogate” Markovian reward is not required. Moreover, since we do not restrict to the Markovian policies, given a policy class with compact history representation, SubPO can learn behaviours beyond the expressivity of the Markovian rewards.} \add{Expressivity/chatterjee}

\mypar{Convex \RL} Convex \RL also seeks to optimize a family of non-additive rewards.  The goal is to find a policy that optimizes a convex function over the state visitation distribution (which averages over the randomness in the \mdp and the policies actions).  This framework has applications, e.g., in exploration and experimental design \citep{Hazan2019, Zahavy2021,Duff2002, tarbouriech2020active,Mutny2023}. %and  \citep{}
While sharing some motivating applications, convex and submodular \RL are rather different in nature.  Beyond the high-level distinction that convex and submodular function classes are complimentary, our non-additive (submodular) rewards are defined over the {\em actual sequence} of states visited by the policy, not its {\em average behaviour}. \citet{mutti2022challenging} points out that this results in substantial differences, noting the deficiency of convex \RL in modelling expected utilities of the form as in~\cref{eq: obj}, which we address in our work.

\mypar{Submodular Maximization}
Submodular functions are widely studied in combinatorial optimization and operations research and have found many applications in machine learning \citep{krause2014submodular,bilmes2022submodularity,tohidi20}.
%concepts popularized with 
The seminal work of  \citet{nemhauser1978analysis} shows that greedy algorithms enjoy a constant factor $1-1/e$ approximation for maximizing monotone submodular functions under cardinality constraints, which is information- and complexity- theoretically optimal \citep{Feige-no-other-effi-algo}. Beyond simpler cardinality (and matroid) constraints, more complex constraints have been considered: most relevant is the s-t-submodular orienteering problem, which aims to find an s-t-path in a graph of bounded length maximizing a submodular function of the visited nodes \citep{chekuri_rg}, and can be viewed as a special case of \subrl on deterministic \smdp's with deterministic starting state and hard constraint on the goal state.  It has been used as an abstraction for informative path planning \citep{Singh2009}. We generalize the setup and connect it with modern policy gradient techniques. \citet{wang2020planning} considers planning under the surrogate multi-linear extension of submodular objectives. Certain problems considered in our work satisfy a notion called \emph{adaptive submodularity}, which generalizes the greedy approximation guarantee over a set of policies \citep{golovin2011adaptive}. While adaptive submodularity allows capturing history-dependence, it fails to address complex constraints (such as those imposed by \cmp's). 

While submodularity is typically considered for discrete domains (i.e., for functions defined on $\{0,1\}^{|\V|}$, the concept can be generalized to continuous domains, e.g., $[0,1]^{|\V|}$ using notions such as DR-submodularity \citep{greedy-supermodular}. This notion forms a class of non-convex problems admitting provable approximation guarantees in polynomial time, which we exploit in Section~\ref{sec: theory}. The problem of learning submodular functions has also been considered \citep{balcan2011learning}. \citet{dolhansky2016deep} introduce the class of deep submodular functions, neural network models guaranteed to yield functions that are submodular in their input. These may be relevant when learning unknown rewards using function approximation, which is an interesting direction for future work. 

Since submodularity is a natural characterization of diminishing returns, numerous tasks involving exploration or discouraging repeated actions \citep{basu2019blocking} can be captured via submodular functions. In addition to our experiments discussing experiment design, item collection and coverage objectives, \cref{tab:subrlapplications} provides a summary of problems that can be addressed with \subrl.

%The bandit problem has been considered in the submodular setting as well. 
The submodular bandit problem is at the interface of learning and optimizing submodular functions \citep{streeter2008online, Chen2017InteractiveSB, YisongLSB}. Algorithms with no-regret (relative to the 1-1/e approximation) exist, whose performance can be improved by exploiting linearity \citep{YisongLSB} or smoothness \citep{Chen2017InteractiveSB} in the objective.  Our results in Section~\ref{sec: theory} can be viewed as addressing (a generalization of) the submodular stochastic bandit problem.  Exploiting further linearity or smoothness to improve sample complexity is interesting direction for future work. 
% \rev{Given the submodular functions capturing tasks that need explorations, other frameworks such as blocking bandits \citep{basu2019blocking} can also be arguably modelled with submodular functions}. 