\section{Proof for Propositions}
\label{apx: deterministic_policy}
% For completion, we briefly prove the following propositions about the \mdp with any set function $F$.
\restatedetmarkov \vspace{-0.5em}
\begin{proof} The proof consists of two parts. First, we establish the existence of an optimal deterministic non-Markovian policy for a deterministic MDP $\mathcal{M}$ with non-Markovian rewards $F$. Second, we demonstrate that the trajectory generated by the optimal deterministic \nm policy $(\pi_\nm)$ in $\mathcal{M}$ can also be generated by a deterministic Markovian policy $(\pi_\mv)$. Since both policies yield the same trajectory, resulting in: $J(\pi_{\nm}) = J(\pi_{\mv})$. (Notably, it is sufficient to look at a value of the single induced trajectory since we consider a deterministic policy in a deterministic \mdp with a fixed initial state.)

\emph{i)} We can construct an extended MDP, denoted as $\mathcal{M}_e$, where the state space consists of trajectories $\tau_{0:h}$ for all time steps $h \in [H]$. In $\mathcal{M}_e$, the trajectory rewards $F$ are Markovian. Due to Markovian rewards in $\mathcal{M}_e$, there exists an optimal deterministic Markovian policy for $\mathcal{M}_e$ \citep{puterman_MDPbook} that, when projected back to $\mathcal{M}$, corresponds to a non-Markovian policy $(\pi_\nm)$.

\emph{ii)} Without loss of generality, let $\tau = ((s_0, a_0), (s_1, a_1), \dots, s_H)$ be a trajectory induced by $\pi_{\nm} \in \Pi_{\nm}$. If the states are augmented with time, the same trajectory $\tau$ is induced by a deterministic Markovian policy defined as $\pi_{\mv}(a_i|s_i) = 1$ and $\pi_{\mv}(a_j|s_i) = 0$ for $i \neq j$, $\forall i,j$. Due to the identical induced trajectory, we have $J(\pi_{\mv}) = J(\pi_{\nm})$.
\end{proof}

\restatedetMDP \vspace{-0.5em}
\begin{proof} 
Given any stochastic policy, it is possible to select a state and modify the action distribution at that state to a deterministic action such that the value of the new policy will be at least equal to the value of the original policy. Below we show a construction to ensure the monotonic improvement of the policy,

W.l.o.g., suppose there are two actions, denoted as $a_k$ and $a'_k$, available at a state $s_k$ (the argument remains applicable for any finite number of actions). Consider the objective $J(\pi)$ as shown below (for simplicity, we wrote only the trajectories affected with action at horizon $k$ at state $s_k$),
\begin{align*}
     J(\pi) &= \! \sum_{\traj \in \Gamma}  \mu(s_0) \!\! \left[ \prod_{i=0}^{H-1}  p^i(s_{i+1}|s_i,a_{i})  \pi^i(a_{i}|s_i) \right] \!\! F(\{(s_0,a_0), \hdots (s_k,a_{k}\!) \hdots s_H\}\!) \\
     \!\!\!\!\!\!&= \!\!\!\!\!\!\!\!\!\!\!\! \sum_{\traj \in \Gamma:(s_k,a_{k})\in\traj} \!\!\!\!\!\!\!\!\!\!\!\! \mu(s_0) \!\! \left[ \prod_{i=0, i \neq k}^{H-1}  p^i(s_{i+1}|s_i,a_{i})  \pi^i(a_{i}|s_i) \right] p^k(s_{k+1}|s_k,a_{k}) \underbrace{\pi^k(a_{k}|s_k)}_{x}\! F(\traj)\\
     \!\!\!\!\!\!&+ \!\!\!\!\!\!\!\!\!\!\!\! \sum_{\traj \in \Gamma:(s_k,a'_{k})\in\traj} \!\!\!\!\!\!\!\!\!\!\!\! \mu(s_0) \!\! \left[ \prod_{i=0, i \neq k}^{H-1}  p^i(s_{i+1}|s_i,a_{i})  \pi^i(a_{i}|s_i) \right] p^k(s_{k+1}|s_k,a'_{k}) \underbrace{\pi^k(a'_{k}|s_k)}_{y}\! F(\traj)\\
     &+ constant \tag{remaining terms, do not vary with $x$ and $y$}
\end{align*}
The objective is to maximize $J(\pi)$ subject to simplex constraints. The policy remains fixed for all states except $s_k$. It is important to note that $J$ is a linear function of variables $x$ and $y$. Given that $J$ is linear within the simplex constraints, the optimal solution lies on a vertex of the simplex $(\pi^k(a_{k}|s_k) + \pi^k(a'_{k}|s_k) = 1)$. This vertex represents a deterministic decision for state $s_k$.

We can define a new policy $\pi'$ based on $\pi$ by adjusting the action distribution at $s_k$ to the optimal deterministic action (either $a'_k$ or $a_k$). It is evident that $J(\pi') \geq J(\pi)$. This process can be repeated for all states. By starting with any optimal stochastic policy, we can obtain a deterministic policy that is at least as good as the original stochastic policy. This concludes the proof.
\end{proof}
