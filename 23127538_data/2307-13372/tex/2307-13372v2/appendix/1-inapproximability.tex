\section{Inapproximability Proof} \vspace{-0.5em}
\label{apx: inapprox}
First, we introduce a set of known hard problems that we will use to establish the hardness of \subrl.

\mypar{Group Steiner tree (\gst)} The input to the group Steiner problem is an edge-weighted graph $G=(V, E, l)$ and $k$ subsets of nodes $g_1, g_2, \hdots, g_k$ called \emph{groups}. Starting from a root node $r$, the goal in \gst is to find a minimum weight tree $T^{\star} = (V(T^{\star}), E(T^{\star}))$ in $G$ such that each group is visited at least once, i.e., $V(T^{\star}) \cap g_i \neq \emptyset, \forall i \in [k]$.

\mypar{Covering Steiner problem (\scp)} The input to \scp is an edge-weighted graph $G=(V, E, l)$ and $k$ subsets of groups $g_1, g_2, \hdots, g_k$, each group has a positive integer $d_i$ representing a minimum visiting requirement. Starting from a root node $r$, the goal in \scp is to find a minimum weight tree $T^{\star} = (V(T^{\star}), E(T^{\star}))$ in $G$ such that the tree covers at least $d_i$ nodes in group $g_i$, i.e., $|V(T^{\star}) \cap g_i| \geq d_i, \forall i \in [k]$. The \scp generalizes \gst problem to an arbitrary constraint $d_i$.

\mypar{Submodular Orienteering Problem (\sop)} In rooted \sop, we are given a root node $r\in V(T)$, and the goal is to find a \emph{walk} $\traj$ of length at most $B$ that maximizes some submodular function $F$ defined on the nodes of the underlying graph. 

\emph{Approximation ratio:} Let $x$ be an input instance of a maximization problem. The approximation ratio $\beta$ is defined as $\beta \geq \frac{\opt(x)}{\algo(x)}$, $\beta \geq 1$, where $\opt$ is the global optimal and $\algo$ is the value attained by the algorithm. The hardness lower bound and approximation upper bounds refer to the lower and upper bound on the approximation ratio $\beta$. 

% Figure environment removed

\restateinapprox \vspace{-1 em}
\begin{proof} 
We reduce \subrl to rooted \sop, demonstrating the inapproximability of \subrl. \cref{lem: rooted_sop} establishes the hardness of rooted \sop.

Given an instance of \sop with a graph $G=(V, E)$ and a root node $r \in V$, the goal is to find a walk $\traj$ that maximizes a submodular function $F(\traj)$, subject to a budget constraint $|\traj|\leq B$. This can be converted to a \smdp (input to \subrl), $\langle \S, \A, P, \rho, \Obj, H \rangle$ tuple in polynomial time as follows:

% To construct an \smdp  from the given \sop instance, we follow these steps in polynomial time:
\textit{i)} Set $\S \leftarrow H \times V$, $\A \leftarrow E$, $H \xleftarrow{} B$, $\rho(0,r) = 1$ and the submodular function $F$ remains unchanged.
\textit{ii)} Iterate over the edges $e \in E$. Let $e=(v',v)$, set $P((h+1, v')|(h,v),a) = 1$, for all $h \in [H-1]$ where action $a = e$. 

The solution to the \subrl problem is a policy $\pi$ that can be rolled out to obtain a solution for the \sop, which is also a polynomial time operation. By assuming the existence of a polynomial time algorithm for \subrl with an approximation ratio $\beta = o(\log^{1-\gamma} \opt)$, we can approximate \sop with $F(\traj) > \frac{F(\traj^{\star})}{\beta}$ (\cref{fig:SOPtoSubRL}). However, this contradicts the fact that rooted \sop cannot be approximated better than $\Omega(\log^{1-\gamma} \opt)$ (\cref{lem: rooted_sop}). Proved by contradiction. \end{proof} \vspace{-0.5em}


% , and the solution of \subrl, a policy $\pi$ can be rolled out to get a walk $\traj$. Assuming a polynomial time algorithm for \subrl with $\beta = o(\log^{1-\gamma} \opt)$, we can approximate \sop with $F(\traj) \geq \frac{\opt}{\beta}$. However, this contradicts the known lower bound of $\Omega(\log^{1-\gamma} \opt)$ for \sop \citep{chekuri_rg}.

% \begin{proof} 
% Following the reduction, as shown in \cref{fig:SOPtoSubRL}, we can use the \subrl algorithm to solve the \sop problem. Given an instance of the \sop, we can construct an \mdp for \subrl from a graph as follows. $(G=(V,E))$ by setting $\S \leftarrow V$, $(\A, \mathcal{P}) \leftarrow E$ and $H \xleftarrow{} B$ and the submodular function $F(.)$ stays the same (\cref{fig:SOPtoSubRL}, step a). The solution of the \subrl is a policy $(\pi)$ that can be rolled out to get a solution for the \sop(\cref{fig:SOPtoSubRL}, step c). By contradiction let's assume we have a polynomial time algorithm for \subrl with approximation ratio $\beta = o(\log^{1-\gamma} \opt)$(\cref{fig:SOPtoSubRL}, step b). Using this, \sop can be approximated with $f(\traj) \geq \frac{f(\traj^{\star})}{\beta}$.  This contradicts the fact that \sop can't be approximated better than $\Omega(\log^{1-\gamma} \opt)$(\cref{fig:SOPtoSubRL}, step d). Hence proved.
% \manish{Discussion on ZTIME quasi-polynomial   }


% \end{proof}

% In this section, we prove that our problem is not solvable beyond the log factors using the NP-Hardness of the Submodular orienteering problem. In SOP, given a graph $G=(V, E)$ and a budget constraint $B$, the agent needs to traverse a path $\traj$, that maximizes a coverage function within a budget constraint of $|\traj| \leq B$.

The results shows that there is no algorithm that can guarantee $J(\pi) \geq \frac{\opt}{\log^{1-\gamma}\opt}$ for all the input instances. As \opt increases with the input size of the problem, the ratio $\frac{1}{\log^{1-\gamma} \opt}$ degrades and hence no algorithm can approximate the problem up to any constant factor $c>0$. For the sake of completeness, we include Theorem 4.1 from \cite{chekuri_rg} and modify it for the rooted \sop. The reduction scheme is the same as \cite{chekuri_rg}. 
\begin{lemma}[Theorem 4.1 from \cite{chekuri_rg}] \label{lem: rooted_sop}
    The rooted submodular orienteering problem (SOP) in undirected graphs is hard to approximate within a factor of $\Omega(\log^{1-\gamma} \opt)$ unless $\NP \subseteq \ZTIME(n^{polylog(n)})$.
\end{lemma} \vspace{-0.5em}
\begin{proof} The group Steiner problem (\gst) is hard to approximate to within a factor of $\Omega(\log^{2-\gamma} \opt)$ unless \NP has quasi-polynomial time Las Vegas algorithms \citep{Halperin2003inapprox}. 
We reduce the problem of rooted \sop to \gst, proving the inapproximability of rooted \sop. This represents that if we have an efficient algorithm for \sop, then we can recover a solution for \gst by using the same \sop algorithm.

\mypar{Submodular function $F$} Given an \scp instance, define a submodular function $F(S) = \sum_{i=1}^{k} \min(d_i, |S\cap g_i|)$. $F$ is a monotone submodular set function. 

Consider an optimal solution of \scp as $T^{\star}$ of cost \opt.
% \manish{This cost can be integral weight or fraction.}.  
We can take an Euler tour of the tree $T^{\star}$ and obtain a tour from $r$ of length at most $2 \opt$ that covers all groups. %\manish{TODO: Draw a figure to explain this}

% Either explain how does an algorithm for \sop will solve \scp or how can a \scp problem will be casted to \sop problem.
\mypar{Reduction} We will reduce rooted \sop problem to the \scp (\scp generalises \gst with any $d_i$>0). Let's say we have an algorithm $\A$ for \sop with $\Omega(\log \sum_i d_i)$. ($\sum_i d_i$ is optimal value for \sop). 
In a single iteration, $\A$ will generate a walk that covers $f(V(T^{\star})/\log f(V(T^{\star}))$ of length $B$, which can be converted to a tour P of length at most $2B$.
We can remove the nodes in $P$ and reduce the coverage requirement of the groups that are partially covered and repeat the above procedure. Using \cref{lem: stand_set_cover_logk}, all groups will be covered up to the requisite amount in $\BigO(\log^2 \sum_i d_i)$ iterations. Combining all the tours yields a tree of length $\BigO(\log^2 \sum_i d_i ) B$ that is a "feasible solution" of the \scp.
% \manish{what does tree length mean? $\BigO(\log^2 \sum_i d_i ) 2B$?} 
$B$ can be evaluated using binary search and is within a constant factor of \opt. %(Think: If B = 1, then the method would not find a solution, if B = \opt then it should be enough? since optimal solution can't be of length more than \opt and \sop shall also return a solution within B) \manish{Ask Mojmir about binary search. B=K\opt, then in approximability ratio \opt will cancel out. This results in a constant factor in the approximability ratio.} 
When specialized to the \gst, i.e., $d_i=1$, the approximability ratio becomes $\BigO(\log^2 k)$. 

\mypar{Contradiction} Following the reduction above, assuming an algorithm $\A$ for \sop with an approximation ratio of $\log k$ results in $\BigO(\log^2 k)$ approximation ratio for \gst. Hence, an $\alpha=o(\log k)$ approximation algorithm for \sop will give an approximation of $\BigO(\alpha \log k)$ for \gst. But \gst is hard to approximate to within a factor of $\Omega(\log^{2-\gamma} \opt)$. Hence \sop is hard to approximate within a factor of $\Omega(\log^{1-\gamma} \opt)$.
% \manish{Give emphasis on $\alpha$-approximation for \sop in undirected graphs implies an $\BigO(\alpha \log k)$ approximation for the group Steiner problem in undirected }
% \begin{itemize}
%     \item overall explain how GST is solved using SOP
% \end{itemize}   
\end{proof}

\begin{lemma} \label{lem: stand_set_cover_logk}
    A algorithm $\A$ for \sop with approximability ratio $\Omega(\log k)$ can cover $k$ nodes after $\BigO(\log^2 k)$ iterations.
\end{lemma}\vspace{-0.5em}
\begin{proof} Let $L_{n}$ be the nodes available after the $n^{th}$ iteration with an algorithm having $\beta \geq \Omega(\log k)$.
    \begin{align*}
        L_0 &\xleftarrow[]{} k\\
        L_1 &\xleftarrow[]{} L_0 - \frac{L_0}{\log L_0} \\
        &\vdots\\
        L_{n+1} &\xleftarrow[]{} \underbrace{L_n - \frac{L_n}{\log L_n}}_{x - \frac{x}{\log x}} \\
    \end{align*} \vspace{-0.5em}
In the first iteration with $x=k$
\begin{align*}
    L_1 = x - \frac{x}{\log x} = x(1-\frac{1}{\log x}) \leq x e^{\frac{-1}{\log x}} = k e^{\frac{-1}{\log k}}.
\end{align*}
By definition, $L_1 \geq L_2 \geq \hdots L_n$, 
hence $L_i \leq k e^{-1/\log k} ~\forall i \in [1,n]$. \\
Nodes available after $n^{th}$ iteration : $L_0 (1-\frac{1}{\log L_0}) (1-\frac{1}{\log L_1}) \hdots (1-\frac{1}{\log L_n})$.

\begin{align*}
    L_0 (1-\frac{1}{\log L_0}) (1-\frac{1}{\log L_1}) \hdots (1-\frac{1}{\log L_n}) &\leq k \underbrace{e^{-1/\log k}\times e^{-1/\log k} \hdots e^{-1/\log k}}_{n~~times}\\
    &= k e^{-n/ \log k} 
\end{align*}

for $n > \log^2 k$, nodes available: $k e^{-n/\log k} < k e^{-\log^2 k/\log k} = k e^{- \log k} = 1$. Hence Proved.
% Hence with algorithm $\A$ having approximability ratio $\Omega(\log k)$, after $\BigO(\log^2 k)$ iterations no nodes are available.
\end{proof}

%The assumption "unless $\NP \subseteq \ZTIME(n^{polylog(n)})$" is a slightly stronger assumption than the common usual $\small{\textsc{P}} \neq \NP$, implying there is no deterministic algorithm in time $n^{polylog(n)}$ that can solve all the problems in $\NP$. However, this is also a widely 

% How critical is it that $F$ is a submodular function for \subPO ?

\section{Discussion}

\mypar{Submodularity} Since the algorithm and the theoretical hardness result readily extend to general set functions beyond submodular rewards, a natural question that arises is how critical is that $F$ is a submodular function and what can we say beyond submodular rewards? In this work, submodularity emerges in the lower bound (inapproximability hardness), implying that the problem is not just intractable but intractable even to approximate up to any constant factor. Additionally, it emerges in the upper bound of $1-c$ under curvature assumption and in the upper bound of $(1-1/e)$ under the simplified \smdp setting. There cannot exist an algorithm for bandit \smdp with better guarantees \citep{Feige-no-other-effi-algo}, and \subPO is able to achieve the optimal ratio $(1-1/e)$, thus utilising submodularity to provide intuition on why \subPO (a REINFORCE type strategy) is a right strategy.

Overall, submodularity lets us characterise the spectrum of the computational complexity of the SubRL framework, while some results, e.g., our algorithm \subPO, inapproximability hardness, naturally carry over to the general non-Markovain rewards beyond submodular $F$ (general non-additive reward function).

\mypar{Policy class} The restriction to Markovian policies in the theoretical limits section is mainly for emphasizing the “hardness result” even for the simple policy class, implying the source of hardness is not the representation of non-Markovian policy (which is an exponential object itself). The overall goal is to learn a policy that achieves a higher objective value; hence, we do not, in general, restrict it to the Markovian policy class. We treat the problem of learning state representation separately, which can be done, e.g. with RNN, and is an add-on to the \subPO, e.g. \subPOnm optimises in a non-Markovian policy class.

\mypar{Expressivity of rewards} The optimal policies for submodular rewards cannot be captured by the Markovian rewards in general since the optimal policies are non-Markovian. 
However, when the policy search is restricted to the Markovian class, the optimal policy is deterministic (\cref{prop:restatedetMDP}), and hence there exists a Markovian reward that would lead to the same optimal policy. But this does not help to solve the problem since finding such Markovian rewards has to be NP-hard to approximate \cref{thm: inappx}.

In contrast to finding such surrogate Markovian rewards, submodularity provides a natural way to capture the task. Moreover, since we do not restrict to the Markovian policies, given a policy class with compact history representation, \subPO can learn behaviours beyond the expressivity of the Markovian rewards \citep{abel2021expressivity}.

\mypar{Applications} Since submodularity is a natural characterization of diminishing returns, numerous tasks involving exploration or discouraging repeated actions can be captured via submodular functions. In addition to our experiments discussing experiment design, item collection and coverage objectives, the following \cref{tab:subrlapplications} provides a summary of problems that can be addressed with \subrl.

% \subsection{Application of \subrl}
\begin{table}[!h]
\hspace{-5em}
\setlength{\arrayrulewidth}{0.5mm}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{1.6}
% \begin{tabular}{ | c{5em} | c{1em} | c{1em} | } 
\begin{tabular}{  c  c  c  } 
  \hline
 Tasks & Relevant works & Submodular reward function $F(\tau)$  \\ 
  \hline 
State entropy exploration  & \citep{Hazan2019} & $
        F(\tau) = \frac{-1}{|\tau|} \sum_{v \in \V} \mathbb{I}_{(v,\cdot)\in\tau} \log \frac{|\{t: (v,t)\in \tau \} |}{|\tau|} 
    $  \\[0.1em] 
D-Optimal Experimental Design &\citep{Mutny2023} &$F(\tau) = I(y_\tau;f)$, $I(y;f) = H(y_\tau) - H(y_\tau | f)$ \\
Steiner covering & \citep{chekuri_rg} & $F(\traj) = \sum\nolimits_{i \in \mathcal{G}} \min(|\traj \cap g_i|, d_i)$, pick $d_i$ items of group $g_i$  \\
% Goal reachability & &$|\tau \cap S_g|$ \\
State coverage functions & \citep{near_optimal_safe_cov}& $F(\tau) = \sum_{v \in \V} | \{ t \in [H]:(v,t)\in \tau \}|$, $F(\tau) =|\bigcup_{v\in \tau} D^v|$  \\
Weighted coverage function & \citep{karimi2017stochastic} & $F(\tau) = g(\bigcup_{s \in \tau} D^s)$, $g(V) = \sum_{v \in V} \rho(v)$ \\
Discourage repeated action/ & \citep{basu2019blocking} & \!\!\!\!\!\!$F(\tau) =|\bigcup_{s\in \tau} D^s|$, e.g., $s\!=\!(v,t)$ and \\[-0.4em]
(including coverage on Time) & &$\!D^s\!\!\coloneqq\!\{(v,t),\!(v,t+1),\!(v,t+2)\}$\\
Log determinant objectives & \citep{wang2020planning} & $F(\traj) = \log \det \left(\sum_{s\in \tau} F(\{s\}) + \lambda I \right)$\\
Facility location & \cite{krause2014submodular} & $F(\tau) = \sum_{i=1} \max_{j \in \tau} M_{i,j}$, $M_{ij} \geq 0$\\
% Exploration under time-varying process & $F(\tau) = \sum_{v \in V} \max \{ \alpha, \min\{ |S \cap S_v|, 1 \} \}, 0 \leq \alpha \leq 1$.\\
\hline
\end{tabular} 
\caption{ A few examples that can be tackled with submodular reinforcement learning framework}
\label{tab:subrlapplications}
\end{table}