\mypar{Beyond Markovian \RL}
%We extend the modelling ability of classical reinforcement learning with Markov chain and Markovian rewards and consider a class of non-Markovian reward functions. 
Several prior works in \RL identify the deficiency in the modelling ability of classical Markovian rewards. This manifests itself especially when exploration is desired, e.g., when the transition dynamics are not completely known \cite{Tarbouriech2019, Hazan2019} or when the reward is not completely known \cite{lindner2021information, Belogolovsky2021}. While all these addresses in some aspect the shortcomings of Markovian rewards, they tend to focus on a specific aspect instead of postulating a new class of reward functions as we do in this work.

\mypar{Convex \RL} Convex \RL also seeks to optimize a family of non-additive rewards.  The goal is to find a policy that optimizes a convex function over the state visitation distribution (which averages over the randomness in the \mdp and the policies actions).  This framework has applications, e.g., in exploration and experimental design \citep{Hazan2019, Zahavy2021,Duff2002, Tarbouriech2019, tarbouriech2020active,Mutny2023}. %and  \citep{}
While sharing some motivating applications, convex and submodular \RL are rather different in nature.  Beyond the high-level distinction that convex and submodular function classes are complimentary, our non-additive (submodular) rewards are defined over the {\em actual sequence} of states visited by the policy, not its {\em average behaviour}. \citet{mutti2022challenging} points out that this results in substantial differences, noting the deficiency of convex \RL in modelling expected utilities of the form as in~\cref{eq: obj}, which we address in our work.

\mypar{Submodular Maximization}
Submodular functions are widely studied in combinatorial optimization and operations research and have found many applications in machine learning \citep{krause2014submodular,bilmes2022submodularity,tohidi20}.
%concepts popularized with 
The seminal work of  \citet{nemhauser1978analysis} shows that greedy algorithms enjoy a constant factor $1-1/e$ approximation for maximizing monotone submodular functions under cardinality constraints, which is information- and complexity- theoretically optimal \citep{Feige-no-other-effi-algo}. Beyond simpler cardinality (and matroid) constraints, more complex constraints have been considered: most relevant is the s-t-submodular orienteering problem \citep{chekuri_rg}, which aims to find an s-t-path in a graph of bounded length maximizing a submodular function of the visited nodes, and can be viewed as a special case of \subrl on deterministic SMDPs with deterministic starting state and hard constraint on the goal state.  It has been used as an abstraction for informative path planning \citep{Singh2009}. We generalize the setup and connect it with modern policy gradient techniques. 
Certain problems of the form considered in this work can satisfy a notion called \emph{adaptive submodularity}, which generalizes the greedy approximation guarantee over a set of policies \citep{golovin2011adaptive}. While adaptive submodularity allows capturing history-dependence, it fails to address complex constraints (such as those imposed by CMPs).

While submodularity is typically considered for discrete domains (i.e., for functions defined on $\{0,1\}^{|\V|}$, the concept can be generalized to continuous domains, e.g., $[0,1]^{|\V|}$ using notions such as DR-submodularity \citep{greedy-supermodular}. This notion forms a class of non-convex problems admitting provable approximation guarantees in polynomial time, which we exploit in Section~\ref{sec: theory}.

The problem of learning submodular functions has also been considered \citep{balcan2011learning}. \citet{dolhansky2016deep} introduce the class of deep submodular functions, neural network models guaranteed to yield functions that are submodular in their input.  These may be relevant for our setting when learning unknown rewards using function approximation, which is an interesting direction for future work.

%The bandit problem has been considered in the submodular setting as well. 
The submodular bandit problem is at the interface of learning and optimizing submodular functions \citep{streeter2008online, Chen2017InteractiveSB, YisongLSB}. Algorithms with no-regret (relative to the 1-1/e approximation) exist, whose performance can be improved by exploiting linearity \citep{YisongLSB} or smoothness \citep{Chen2017InteractiveSB} in the objective.  Our results in Section~\ref{sec: theory} can be viewed as addressing (a generalization of) the submodular stochastic bandit problem.  Exploiting further linearity or smoothness  to improve sample complexity is an interesting direction for future work.
