% Figure environment removed
\looseness -1 We empirically study the performance of \subrl on multiple environments. They are i) Informative path planning, ii) Item collection, iii) Bayesian D-experimental design, iv) Building exploration, v) Car racing and vi) Mujoco-Ant. The environments involve discrete (i-iv) and continuous (v-vi) state-action spaces and capture a range of submodular rewards, illustrating the versatility of the framework.

The problem is challenging in two aspects: firstly, how to maximize submodular rewards, and secondly, how to maintain an effective state representation to enable history-dependent policies. Our experiments mainly focus on the first aspect and demonstrate that even with a simple Markovian state representation, by \emph{greedily maximizing marginal gains}, one can achieve good performance similar to the ideal case of non-Markovian representation in many environments. However, we do not claim that a Markovian state representation is sufficient in general. For instance, in the building exploration and item collection environments, Markovian policies are insufficient, and a history-dependent approach is necessary for further optimization. Natural avenues are to augment the state representation to incorporate additional information, e.g., based on domain knowledge, or to use a non-Markovian parametric policy class such as RNNs.
Exploring such representations is application specific, and beyond the scope of our work.

We consider two variants of \subPO: \subPOm and \subPOnm, corresponding to Markovian and non-Markovian policies, respectively. \subPOnm uses a stochastic policy that conditions an action on the history. We model the policy using a neural network that maps the history to a distribution over actions, whereas \subPOm maps the state to a distribution over actions. Disregarding sample complexity, we expect \subPOnm %to represents 
perform the best, % one can achieve 
since it can track the complete history. In our experiments, we always compare with  \emph{modular \RL} (\mrl), a baseline that represents standard RL, where the additive reward for any state $s$ is $F(\{s\})$. 
In \mrl, we maximize the undiscounted sum of additive rewards, whereas, in contrast, \subPO maximizes marginal gains. The rest of the process remains the same, i.e., we use the same policy gradient method.
We implemented all algorithms in Pytorch and will make the code and the videos public. We deploy Pytorch's automatic differentiation package to compute an unbiased gradient estimator. Experiment details and extended empirical analysis are in \cref{apx: experiments}. 
% For the first four environments, we consider a finite state action space where the agent takes a high-level action moving up, down, left, right or stay. 
Below we explain our observations for each environment: 

% \subsection{Finite state action domains} 
\mypar{Informative path planning} We simulate a bio-diversity monitoring task, where we aim to cover areas with a high density of gorilla nests with a quadrotor in the Kagwene Gorilla Sanctuary (\cref{fig: gorilla-env}). The quadrotor at location $s$ covers a limited sensing region around it, $\Discat[s]$. 
The quadrotor starts in a random initial state and follows deterministic dynamics. It is equipped with five discrete actions representing directions.
Let $\density:\Domain \to \R$ be the nest density obtained by fitting a smooth rate function \citep{mojmir-cox} over Gorilla nest counts \citep{gorilla-kagwene}. The objective function is given by $\Obj(\traj) = g(\sum_{s \in \traj} \Discat[s])$, where $g(V) = \sum_{v \in V} \density(v)$. As shown in \cref{fig: constant_H40}, we observe that \mrl repeatedly maximizes its modular reward and gets stuck at a high-density region, whereas \subPO achieves performance as good as \subPOnm while being more sample efficient.
To generalize the analysis, we replace the nest density with randomly generated synthetic multimodal functions and observe a similar trend (\cref{apx: experiments}).

\mypar{Item collection} \cref{fig: steiner_covering}, the environment consists of a grid with a group of items $\mathcal{G} = \{\textit{banana, apple, strawberries, watermelon}\}$ located at $g_i\subseteq \V$, $i \in \mathcal{G}$. We consider stochastic dynamics such that with probability 0.9, the action we take is executed, and with probability 0.1, a random action is executed \textit{(up, down, left, right, stay)}. 
The agent has to find a policy that generates trajectories $\traj$, which picks $d_i$ items from group $g_i$, for each $i$. Formally, the submodular reward function can be defined as $F(\traj) = \sum\nolimits_{i \in \mathcal{G}} \min(|\traj \cap g_i|, d_i)$. 
We performed the experiment with 10 different randomly generated environments and 20 runs in each. 
In this environment, the agent must keep track of items collected so far to optimize for future items. Hence as shown in \cref{fig: steiner_plot}, \subPOnm based on non-Markovian policy achieves good performance, and \subPOm achieves a slightly lower but yet comparable performance just by maximizing marginal gains.
% Figure environment removed

\mypar{Bayesian D-experimental design} In this experiment, we seek to estimate an a-priori unknown function $f$. The function $f$ is assumed to be regular enough to be modelled using Gaussian Processes. 
Where should we sample $f$ to estimate it as well as possible? Formally, our goal is to optimize over trajectories $\traj$ that provide maximum mutual information between $f$ and the observations $y_{\traj} = f_{\traj} + \epsilon_{\traj}$ at the points $\traj$. The mutual information is given by $I(y_{\traj};f) = H(y_{\traj}) - H(y_{\traj}|f)$, representing the reduction in uncertainty of $f$ after knowing $y_{\traj}$, where $H(y_{\traj})$ is entropy. We define the monotonic submodular function $F(\traj) = I(y_{\traj};f)$.  
The gorilla nest density $f$ is an a-priori unknown function. We generate 10 different environments by assuming random initialization and perform 20 runs on each to compute statistical confidence.  
In \cref{fig: entropy_H40}, we observe a similar trend that \mrl gets stuck at a high uncertainty region and cannot effectively optimize the information gained by the entire trajectory, whereas \subPOm achieves performance as good as \subPOnm while being very sample efficient due to the smaller search space of the Markovian policy class.


% Figure environment removed

\mypar{Building exploration} The environment consists of two rooms connected by a corridor. The agent at $s$ covers a nearby region $\Discat[s]$ around itself, marked as a green patch in \cref{fig: two_room}. The task is to find a trajectory $\traj$ that maximizes the submodular function $F(\traj) = |\cup_{s\in \traj}\Discat[s]|$. The agent starts in the corridor's middle and has deterministic dynamics. The horizon $H$ is just enough to cover both rooms.
Based on the time-augmented state space, there exists a deterministic Markovian policy that can solve the task. 
However, it is a challenging environment for exploration with Monte Carlo samples using Markovian policies. As shown in \cref{fig: plot_two_room}, \subPO achieves a sub-optimal solution of exploring primarily one side, whereas \subPOnm tracks the history and learns to explore the other room.

\textbf{Car Racing} is an interesting high-dimensional environment, with continuous state-action space, where a race car tries to finish the racing lap as fast as possible \citep{prajapat2021competitive}. The environment is accompanied by an important challenge of learning a policy to maneuver the car at the limit of handling. The track is challenging, consisting of 13 turns with different curvature (\cref{fig: car_track}). The car has a six-dimensional state space representing position and velocities. The control commands are two-dimensional, representing throttle and steering. Detailed information is in the \cref{apx: experiments}. The car is equipped with a camera and observes a patch around its state $s$ as $\Discat[s]$. The objective function is $\Obj(\traj) = |\cup_{s\in \traj}\Discat[s]|$. We set a finite horizon of 700. The \subPOnm will have a large state space of $700 \times 6$, which makes it difficult to train. 
For variance reduction, we use a baseline $b(s)$ in \cref{eqn: grad_esti} that estimates the cumulative sum of marginal gains. 
As shown in \cref{fig: plot_car_racing}, under the coverage-based reward formulation, the agent trained with \mrl tries to explore a little bit but gets stuck with a stationary action at the beginning of the episode to get a maximum modular reward. However, the \subPO agent tries to maximise the marginal again at each timestep and hence learns to drive on the race track (\href{https://youtu.be/jXp0QxIQ--E}{https://youtu.be/jXp0QxIQ--E}). 
Although it is possible to use alternative reward functions to train the car using standard \RL, the main objective of this study is to demonstrate \subPO on the continuous domains and how submodular functions in \RL can provide versatility to achieve surrogate goals.

\mypar{MuJoCo} \looseness -1 The MuJoCo ant task is a high-dimensional locomotion task, as depicted in \cref{fig: mujoco_ant}. The state space dimension is 30, containing information about the robot's global pose and the internal actuator's orientation. The control input dimension is 8, consisting of torque commands for each actuator. The Ant at any location $s$ covers locations in 2D space, $\Discat[s]$ and receives a reward based on it. The goal is to maximize $F(\traj) = |\cup_{s\in\traj} \Discat[s]|$. The results depicted in \cref{fig: plt_mujoco} demonstrate that \subPO maximizes marginal gain and learns to explore the environment, while \mrl learns to stay stationary, maximizing modular rewards. The environment carries the core challenge of continuous control and high-dimensional observation spaces. This experiment shows that \subPO can effectively scale to high-dimensional domains.
