
\looseness=-1
In general, \cref{sec: NP_hard} shows \subrl is NP-hard to approximate. A natural question is: Is it possible to do better under additional structural assumptions? 
In this section, we present one interesting case under which \subPO can be approximated to a constant factor via DR-submodular optimization.
\begin{definition}[DR submodularity and DR-property, \cite{bian2017continuous}] For $\X \subseteq \R^d$, a function $f: \X \to \R$ is DR-submodular (has the DR property) if $\forall \mathbf{a} \leq \mathbf{b} \in \X$, $\forall i \in [d]$, $\forall k \in \R_{+}$ s.t. $(ke_i + \mathbf{a})$ and $(k e_i + \mathbf{b})$ are still in $\X$, it holds, $f(k \mathbf{e}_i + \mathbf{a}) - f(\mathbf{a}) \geq f(k \mathbf{e}_i + \mathbf{b}) - f(\mathbf{b}). ~($Notation: $\mathbf{e}_i$ denotes $i^{th}$ basis vector and for any two vectors $\mathbf{a}, \mathbf{b}, \mathbf{a} \leq \mathbf{b}$ means $a_i \leq b_i \forall i\in [d])$
\end{definition}
Monotone DR-submodular functions form a family of generally non-convex functions, which can be approximately optimized.  As discussed below, gradient-based algorithms find constant factor approximations over general convex, downward-closed polytopes.

Under the following condition on the Markov chain, we can show that as long as the policy is parametrized in a particular way, the objective is indeed monotone DR-submodular. 
\begin{definition}[$\epsilon$-Bandit \smdp] \label{asm: bandit} An \smdp s.t. for any $v_j, v_k \in \V$, $j \neq k$, $\forall h \in [H]$ and $\forall v' \in \V$, $P_h(v_j|v',a_{j})= 1-\epsilon_h$, and $P_h(v_k|v',a_{j}) = \frac{\epsilon_h}{|\V|-1}$ for $\epsilon_h \in \left[0, \frac{|\V|}{|\V|+1}\right]$ is an $\epsilon$-Bandit \smdp.
\end{definition}
% Figure environment removed
% We call \mdp system that satisfies the above $\epsilon$-Bandit \smdp. 
This represents a "nearly deterministic" \mdp where there is a unique action for each state in the \mdp, which takes us to it with $1-\epsilon$ probability and with the rest, we end up in any other state \cref{fig:bandit_mdp}. 
% On top of this, we are allowed to shrink or increase the state space over the horizon indicated by changing state space $\S^h$. 
While limiting, it generalizes the bandit scenario, which would occur when $\epsilon = 0$. In the following, we consider a class of state-independent policies that can change in each horizon, denoting the horizon dependence with $\pi^h(a)$.
% In the following, we consider a class of state-independent policy that is allowed to change within each round, so we denote it with $\pi^h(a)$, where $\Delta^{|\A|-1}$ is a simplex constraint. 
We now formally establish the connection between \subrl and DR-submodularity,
\looseness=-1
% 
\begin{restatable*}{theorm}{restateDRsub} 
For horizon dependent policy $\pi$ parameterized as $\pi^h(a) \forall h \in [H]$ in an $\epsilon$-Bandit \smdp, and $F(\traj)$ is a monotone submodular function, then $J(\pi)$ is monotone DR-submodular.
\label{thm: dr_sub}
\end{restatable*}
The proof is in \cref{apx: dr_sub}. It builds on two steps; firstly, we use a  reparameterization trick to handle policy simplex constraints. We relax the equality constraints on $\pi$ to lie on a convex polytope $\mathcal{P} = \{ \pi^h(a) ~|~ 0 \leq \pi^h(a) \leq 1, 0 \leq \sum_{j, j \neq k} \pi^h(a_j) \leq 1, \forall k\in[|\A|], \forall h\in [H] \}$ and enforce the equality constraints directly in the objective \cref{eq: obj}. Secondly, under the assumptions of \cref{thm: dr_sub}, we show that the Hessian of $J(\pi)$ only has non-positive entries, which is an equivalent characterization of twice differentiable DR-submodular functions. Furthermore, the result can be generalized to accommodate state and action spaces that vary with horizons, although, for simplicity, we assumed fixed spaces. 

The convex polytope $\mathcal{P}$ belongs to a class of down-closed convex constraints.
\citet{bian2017guaranteed} proposes a modified \frank algorithm for DR-submodular maximization with down-closed constraints. This variant can achieve an $(1-1/e)$ approximation guarantee and has a sub-linear convergence rate. 
The algorithm proceeds as follows: the gradient oracle is the same as \cref{thm: PG}, while employing a tabular policy parameterization.
The polytopic constraints $\mathcal{P}$ are ensured through a \frank step, which involves solving a linear program over the policy domain. Finally, the policy is updated with a specific step size defined in \citep{bian2017guaranteed}.
Furthermore, \citet{hassani2017gradient} shows that any stationary point in the optimization landscape of DR-submodular maximization under general convex constraints is guaranteed to be $1/2$ optimal. Therefore, any gradient-based optimizer can be used for the $\epsilon-$ Bandit \smdp class, and will result in an $1/2$-optimal policy.
In \cref{sec: related_works}, we elaborate on how this setting generalizes previous works on submodular bandits.

\mypar{General \smdp} Although we cannot obtain a provable result for the general \smdp setting (\cref{thm: inappx}), we can show that the \subPO algorithm, under the assumption of modular rewards, reduces to standard \PG, and hence recovers the guarantees and benefits of the modular \PG. 
In particular, with tabular policy parameterization, under mild regularity assumptions, any stationary point of the modular \PG cost function is a global optimum \cite{bhandari2019global}. 
Moreover, interestingly, we can quantify the deviation of the submodular function from a modular function using the notion of curvature \citep{iyer2013fast}.

\emph{Curvature:} The notion of curvature reflects how much the marginal values $\Delta(v|A)$ can decrease as the function of A. The total curvature of $F$ is defined as, $c = 1 - \min_{A, j \notin A} \Delta(j|A)/F(j).$
Note that $c\in[0,1]$, and if $c=0$ then the marginal gain is independent of $A$ (i.e., $F$ is modular). With assumptions of bounded curvature, $c\in(0,1)$, we can guarantee the constant factor optimality for \subPO (proof in \cref{apx: proof_curvature}),
\begin{restatable*}{prop}{restateboundedC} \label{prop: bounded_C}
    Let \subPO under tabular policy parameterization converges to a policy $\pi$. For any monotone submodular function $\Obj$ with a bounded curvature $c\in(0,1)$, \subPO guarentees $J(\pi) \geq (1-c) J(\pi^{\star})$, where $\pi^{\star}$ is an optimal non-Markovian policy.
\end{restatable*}

\vspace{-1em}