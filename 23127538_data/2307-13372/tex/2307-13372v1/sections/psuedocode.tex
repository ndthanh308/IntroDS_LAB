\savebox{\algleft}{
\begin{minipage}[t]{.60\textwidth}
\vspace{-10.5em}
\begin{algorithm}[H]
\caption{Submodular Policy Optimization (\subPO)}
\begin{algorithmic}[1]
% \SetAlgoLined
\State \textbf{Input:} $\smdp \M, \pi, N, B$
\For{epoch $k = 1:N$} 
% \textit{\textbf{(data collection)}}
\!\!\!\!\For{batch $b = 1:B$} 
\!\!\!\!\For{$h=0:H-1$} 
\State \!\!\!\!Sample $a_h \sim \pi(a_h|s_h)$, execute $a_h$
\State \!\!\!\!$D \!\leftarrow \!\{s_h,a_h, F(\traj_{0:h+1})\}$ \label{alg: sample_collect} 
\EndFor
\EndFor 
% \textit{\textbf{(Gradient estimator)}}
\State Estimate $\nabla_{\theta} J(\pi_{\theta})$ as per \cref{thm: PG} \label{alg: estimator} %, \textcolor{red}{Entropy Expl. $J(\pi) 
 % \leftarrow J(\pi) + \alpha H(\pi)$ }
% \State \textcolor{red}{Momentum $\nabla_{\pi} J(\pi_{k}) \! \leftarrow \! \beta \nabla_{\pi} J(\pi_{k-1})\! + (1-\beta)\nabla_{\pi} J(\pi)$, (try Adams?), Value net. critic $v_{\theta}(\traj_{0:i})$} \\
% \textit{\textbf{(Policy update)}}
% \State $\pi_{k+1} = \argmax_{\pi \in \Pi} \pi \cdot \nabla_{\pi} J(\pi_k)$
% \State $\pi_{k+1} = (1-\alpha_k)\pi_{k} + \alpha_k \pi_{k+1}$, $\alpha_k = \frac{2}{2+k}$
% \State \textcolor{red}{Projected gradient descent on simplex or GD + softmax for simplex constraint}
\State Update policy parameters $(\theta)$  using \cref{eqn: gd}
% \State $\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\pi)$, using \cref{eqn: gd}
\EndFor
\end{algorithmic}
\label{alg:subrl}
\end{algorithm}
\end{minipage}}

\savebox{\figright}{
\begin{minipage}[t]{.35\textwidth}
\centering
    \scalebox{0.35}{\input{images/subrl-eps-bandit}} 
\end{minipage}     }

% % Figure environment removed