\label{apx: experiments}


In this work, we examined state-action spaces that are both discrete (finite) and continuous. Here, we provide an overview of the hyperparameters and experiment specifics for each type. 
\subsection{Finite state action spaces}
We consider a grid world environment with domain size $30\times30$, having a total of 900 states. Horizon $H=40$, i.e., the agent has to find a path of length 40 that jointly maximize the objective.
The action set comprised of $\{right, up, left, down, stay\}$. The agent's policy was parameterized by a two-layer multi-layer perceptron, consisting of 64 neurons in each layer. The non-linearity in the network was induced by employing the Rectified Linear Unit (ReLU) activation function.
By employing a stochastic policy, the agent generated a categorical distribution over the action set for each state. Subsequently, this distribution was passed through a softmax probability function. We employed a batch size of $B=500$ and a low entropy coefficient of $\alpha=0$ or $0.005$, depending on the specific characteristics of the environment.


We randomly generated ten different environments and conducted 20 runs for each environment, resulting in a total of 200 experiments. These experiments were executed concurrently on a server with 400 cores, allocating two cores for each job. It takes less than 20 minutes to complete 150 epochs and obtain a saturated policy, indicating no further improvement. All our plot shows the training curve (objective vs epochs).

For instances where we utilized randomly sampled environments, such as coverage with GP samples, gorilla nest density, or item collection environment, we have included the corresponding environment files in the attached code for easy reference.


To conduct additional analysis using the \emph{Informative Path Planning Environment}, we made modifications to the underlying function previously based on gorilla nest density. Specifically, we introduced three variations: i) a constant function, ii) a bimodal distribution, and iii) a multi-modal distribution randomly sampled from a Gaussian Process (GP). As depicted in Figure \ref{fig: extra_empirical}, we noticed a consistent trend across all three variations. The \mrl algorithm repeatedly maximized its modular reward but became trapped in high-density regions. In contrast, the \subPOm algorithm demonstrated performance comparable to \subPOnm while exhibiting greater sample efficiency.
 


% Figure environment removed

\subsection{continuous state-action spaces}
% For continuous domain, constraints are enforced using the Tanh operation. 

\mypar{Car Racing}
\label{appx: ORCADescription}
In the car racing environment, our objective is to achieve the fastest completion of a one-lap race. To accomplish this, we aim to learn a policy that effectively controls a car operating at its handling limits. Our simulation study closely emulates the experimental platform employed at ETH Zurich, which utilizes miniature autonomous race cars. Building upon \citet{Liniger2017OptimizationBasedAR}, we model the dynamics of each car using a dynamic bicycle model augmented with Pacejka tire models \citep{mf}. However, we deviate from the approach presented in  \citep{Liniger2017OptimizationBasedAR} by formulating the dynamics in curvilinear coordinates, where the car's position and orientation are represented relative to a reference path. This coordinate transformation significantly simplifies the reward definition and facilitates policy learning. The state representation of an individual car is denoted as $z = [\rho, d, \mu, V_x, V_y, \psi]^T$. Here, $\rho$ measures the progress along the reference path, $d$ quantifies the deviation from the reference path, $\mu$ characterizes the local heading relative to the reference path, $V_x$ and $V_y$ represent the longitudinal and lateral velocities in the car's frame, respectively, and $\psi$ represents the car's yaw rate. The car's inputs are represented as $[D, \delta]^T$, where $D \in [-1, 1]$ represents the duty cycle input to the electric motor, ranging from full braking at $-1$ to full acceleration at $1$, and $\delta \in [-1, 1]$ corresponds to the steering angle.

The car is equipped with a camera and observes a patch around its state $s$ as $\Discat[s]$. The objective function is $\Obj(\traj) = |\cup_{s\in \traj}\Discat[s]|$. For simplicity, we define the observation patch of some 5 m and spanning the entire width of the race track. We add additive reward penalization to avoid hitting the walls. If a narrow-width patch is used, then one can eliminate the use of reward penalization (used to avoid hitting the boundaries) as coverage near the boundaries is low and the agent learns to drive in the middle or go to the extreme if they gain due to going fast. The test track, depicted in Figure \ref{fig: car_track}, consists of 13 turns with varying curvatures. We utilize an optimized X-Y path as the reference path obtained through a lap time optimization tool. It is worth noting that using a pre-optimized reference path is not obligatory, but we observed improved algorithm convergence when employing this approach. To convert the continuous-time dynamics into a discrete-time Markov Decision Process (MDP), we discretize the dynamics using an RK4 integrator with a sampling time of 0.03 seconds.

For the training, we started the players on the start line ($\rho = 0$) and randomly  assigned $d \in \{0.1, -0.1\}$. We limit one episode to 700 horizon. For each policy gradient step, we generated a batch of 8 game trajectories and ran the training for roughly 6000 epochs until the player consistently finished the lap. This takes roughly 1 hour of training for a single-core CPU. We use Adam optimizer with a slow learning rate of $10^{-3}$. For our experiments, we ran 20 different random runs and reported the mean of all the seeds. As a policy, we use a multi-layer perceptron with two hidden layers, each with 128 neurons and used ReLU activation functions and a Tanh output layer to enforce the input constraints. For variance reduction, we use a baseline $b(s)$ in \cref{eqn: grad_esti} that estimates the cumulative sum of marginal gains. One can think of this as a heuristic to estimate marginal gains.
The same setting was also used for the competing algorithm \mrl. 


\mypar{MuJoCo} It is a physics engine for simulating high-dimensional continuous control tasks in robotics. In this experiment, we consider the Ant environment, which is depictive of the core challenges that arise in the Mujoco environment. For detailed information about the Mujoco Ant environment, we refer the reader to \href{https://gymnasium.farama.org/environments/mujoco/ant/}{https://gymnasium.farama.org/environments/mujoco/ant/}.



For the training, we use the default random initialization of Mujoco-Ant. We consider a bounded domain of $[-20,20]^2$. The agent covers a discrete grid of 5x5 around its location in the 2D space (only for efficient reward computation, we discretize the domain into a $400 \times 400$ grid, dynamics is continuous) and receives a reward based on the coverage. To train the agent faster, one can also couple MuJoCo's inbuilt additive rewards (tuned for faster walking) with the submodular coverage rewards, which results in submodular rewards. 

We limit one episode to a horizon of 400. We use a vectorized environment that samples a batch of 15 trajectories at once. We trained for roughly 20,000 epochs until the agent consistently walks and explores the domain. This takes roughly 6 hours of training for a single-core CPU. We use Adam optimizer with a learning rate of $10^{-2}$. For our experiments, we ran 20 different random runs and reported the mean of all the seeds. As a policy, we use a multi-layer perceptron with two hidden layers, each with 128 neurons and used ReLU activation functions and a Tanh output layer to enforce the input constraints. Similar to the car racing environment, we use a baseline $b(s)$ in \cref{eqn: grad_esti} that estimates the cumulative sum of marginal gains.
The same setting was also used for the competing algorithm \mrl. 

