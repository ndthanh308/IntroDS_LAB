\label{apx: dr_sub}
\subsection{DR-Submodularity Proof}
% Figure environment removed
\restateDRsub
\begin{proof}
$\epsilon$-Bandit \mdp considers \emph{state independent transitions} (only horizon dependent), i.e., $v_j, v_k \in \V$, $j \neq k$, $\forall h \in [H]$ and $\forall v' \in \V$, $P_h(v_j|v',a_{j})= 1-\epsilon_h$, and $P_h(v_k|v',a_{j}) = \frac{\epsilon_h}{|\V|-1}$ for $\epsilon_h \in \left[0, \frac{|\V|}{|\V|+1}\right]$. For simplicity, we consider a fixed size $\V$, but it can also vary with the horizon. To denote explicit dependent on the horizon, we use $P_h$ and $v$ instead of directly $s$.

Similarly, policy parameterization in \cref{thm: dr_sub} considers \emph{state independent policy} (only horizon dependent) $\pi^h(a|v') = \pi^h(a|v'') \forall h\in [H], \forall v',v'' \in \V$, in short notation we denote them as $\pi^h(a)$. 

Note that $\pi^{h}(a_{i}|v_i)$ corresponds to the self-loop actions ("stay"). In the proof, we reparameterize the probability of self-loop actions with that of other actions (i.e., $\pi^{h}(a_{i}|v_i) = 1 - \sum_{a \neq a_i}\pi^h(a|v_i)$), resulting in relaxation of the simplex constraint $\left(\sum_{a} \pi^h(a|v)=1, \forall h , v \to \sum_{a \neq a_i} \pi^h(a|v_i) \leq 1 \right)$. %of technique: we will substitute  and differentiate \cref{eq: obj} wrt $\pi^{h}(a_{i+1}|s_i)$. 

In the following, for ease of notation, we denote $v_i\coloneqq (i,v)$, in particular, $F((v'_i,a_i)_{i=0}^{H-1}, v_H) \coloneqq F((i,v',a_i)_{i=0}^{H-1}, (H,v))$. Consider the objective $J(\pi)$,
\begin{align*}
    J(\pi) &= \sum_{\traj \in \Gamma} \mu(v_0) \prod_{h=0}^{H-1} p_h(v_{h+1}|v_h,a_{h}) \pi^h(a_{h}|v_h) F((v_i,a_i)_{i=0}^{H-1}, v_H) \\ %\!\! F(\{(s_0,a_0), \hdots \underbrace{(s_i,a_{i}\!),(s_{i+1},a_{i+1}\!),(s_{i+2},a_{i+2}\!)}_{A} \hdots s_H\}\!) 
    &= \sum_{\traj \in \Gamma} \mu(v_0) \prod_{h=0}^{H-1} p_h(v_{h+1}|a_{h}) \pi^h(a_{h}) F((v_i,a_i)_{i=0}^{H-1},v_H) \tag{state independent assumptions}
\end{align*}
% Note in the equation above notation is overloaded, $v_h$ is a state at horizon $h$ and also a state where a unique action $a_h$ is a self-loop one. %$v_{\traj_h}?$

We show DR-submodularity by showing $\forall \pi \in \mP, \frac{\partial^2 J(\pi)}{\partial \pi' \partial \pi''} \leq 0, \forall \pi', \pi''\in \mP$. 
% For this, consider the gradient of the objective, (
We first reparameterize the self-loop actions (which bring the agent back to the same state) in $J(\pi)$ by substituting $\pi^{h}(a^l_{h}|v^l_h) = 1 - \sum_{a \neq a^l_h}\pi^h(a|v^l_h)$ in $J(\pi)$. Here $a^l$ is a looping action for state $v^l$ at horizon $h$.
% Consider $J(\pi)$ with all the self-loop actions (which bring back to the same state) being reparameterized as per $\pi^{h}(a^l_{h}|v^l_h) = 1 - \sum_{a \neq a^l_h}\pi^h(a|v^l_h)$. 

First, we prove monotonicity of $J(\pi)$ by showing $\frac{\partial J(\pi)}{\partial \pi^{h}(a'_{h})} \geq 0$. %We consider the terms in $\frac{\partial J(\pi)}{\partial \pi^{h}(a'_{h})}$ corresponding to the state $v^l$ and show that they are $\geq 0$. This can be sum over all the states to get $\frac{\partial J(\pi)}{\partial \pi^{h}(a'_{h})} \geq 0$.
\begin{align*}
    \frac{\partial J(\pi)}{\partial \pi^{h}(a'_{h})} = \sum_{v^l\in \V}\frac{\partial J_{v^l}(\pi)}{\partial \pi^{h}(a'_{h})}, \mathrm{where},
\end{align*}
\begin{align*}
% J(\pi) &= \sum_{\traj \in \Gamma} \mu(s_0) \left[\prod_{i=0}^{H-1} p^i(s_{i+1}|s_i,a_{i+1}) \pi^{i}(a_{i+1}|s_i) \right] F(\{(s_0,a_1), (s_1,a_2), \hdots s_H\})\\
& \frac{\partial J_{v^l}(\pi)}{\partial \pi^{h}(a'_{h})} = \\%\!\!\!\!\!\!\!\sum_{\substack{\traj \in \Gamma:(v_h,a'_{h})\in\traj,\\ \forall v \in |\V|, v \neq v^l}} \!\!\!\!\!\!\!\!\!\!\!\! \mu(v_0) \!\! \prod_{i=0}^{H-1} \!\! p_i(v_{i+1}|a_{i}) \!\! \left[\!\prod_{h\neq i}^{H-1}  \!\!\! \pi^h(a_{h}) \right] \!\! F(\traj) - \!\!\!\!\!\!\!\!\!\!\!\!\sum_{\substack{\traj \in \Gamma:(v_h,a^l_{h})\in\traj,\\ \forall v \in |\V|, v \neq v^l}} \!\!\!\!\!\!\!\!\!\!\!\! \mu(v_0) \!\! \prod_{i=0}^{H-1} \!\! p_i(v_{i+1}|a_{i}) \!\! \left[\!\prod_{h\neq i}^{H-1}  \!\!\! \pi^h(a_{h}) \right] \!\! F(\traj) \\ 
\!\!\!\!\!\!\!& \!\!\!\!\!\! \sum_{\traj \in \Gamma:(v^l_h,a'_{h})\in\traj} \!\!\!\!\!\!\!\!\!\!\!\! \mu(v_0) \!\! \prod_{i=0}^{H-1} \!\! p_i(v_{i+1}|a_{i}) \!\! \left[\!\prod_{h\neq i}^{H-1}  \!\!\! \pi^h(a_{h}) \right] \!\! F((v_0,a_0), \hdots \underbrace{(v^l_h,a'_{h}\!),(v'_{h+1},a_{h+1}\!),(v_{h+2},a_{h+2}\!)}_{A} \hdots v_H\!)\\
\!\!\!\!\!\!\!&- \!\!\!\!\!\!\!\!\!\!\!\! \sum_{\traj \in \Gamma:(v^l_h,a^l_{h})\in\traj} \!\!\!\!\!\!\!\!\!\!\!\! \mu(v_0) \!\! \prod_{i=0}^{H-1} p_i(v_{i+1}|a_{i}) \!\!\left[\!\prod_{h\neq i}^{H-1} \!\!\! \pi^h(a_{h}) \right] \!\! F((v_0,a_0), \hdots \underbrace{(v^l_h,a^l_{h}),(v^l_{h+1},a_{h+1}),(v_{h+2},a_{h+2})}_{B} \hdots v_H\!) 
\intertext{
We prove $\frac{\partial J_{v^l}(\pi)}{\partial \pi^{h}(a'_{h})} \geq 0$ for any $v^l$, which implies $\frac{\partial J(\pi)}{\partial \pi^{h}(a'_{h})}\geq0$. Note: For every trajectory in $E \coloneqq \{\traj \in \Gamma:(v^l_h, a'_{h})\in\traj\}$ ($1^{st}$ summation), we have a trajectory in $L \coloneqq \{\traj \in \Gamma:(v^l_h, a_{h})\in\traj\}$ ($2^{nd}$ summation) that differ only in $v'_{h+1}$ and $v^l_{h+1}$ and all other states are exactly same. For every trajectory in L, there is a trajectory in E with a higher value.


We define a short notation $f^{-}_{h} \coloneqq \mu(v_0) \prod\limits_{i \neq h}^{H-1} p_i(v_{i+1}|a_{i}) \pi^i(a_{i}) $, denotes trajectory distribution ignoring transition at $v_h$. }
&= \!\!\!\!\! \sum_{\traj \in \Gamma:(v^l_h,a'_{h})\in\traj} \!\!\!\!\!\!\!\!\! {f'}^{-}_{h} p_h(v'_{h+1}|a'_h) F((v_0,a_0), \hdots (v^l_h,a'_{h}),(v'_{h+1},a_{h+1}),(v_{h+2},a_{h+2}) \hdots v_H)\\
&- \!\!\!\!\! \sum_{\traj \in \Gamma:(v^l_h,a^l_{h})\in\traj} \!\!\!\!\!\!\!\!\! {f^l}^{-}_{h} p_h(v^l_{h+1}|a^l_{h})  F((v_0,a_0), \hdots (v^l_h,a^l_{h}),(v^l_{h+1},a_{h+1}),(v_{h+2},a_{h+2}) \hdots v_H) \numberthis \label{eqn: proof-cancel-out}

\intertext{
Note ${f'}^{-}_{h}$ and ${f^l}^{-}_{h}$ are equal due to state independent transition and policy.

Drop the actions (the function $F$ depends on the states) and let $R \coloneqq \traj \backslash v'$,}
 % only, we can drop actions and denote by $F'(.)$.}
&= \sum_{\traj \in \Gamma:(v^l_h,a'_{h})\in\traj} {f'}^{-}_{h} ( 1 - \epsilon_h - \frac{\epsilon_h}{|\V|-1}) \left( F( R \cup \{v'_{h+1}\}) - F( R)\right) \geq 0   \tag{since, $\epsilon_h \leq \frac{|\V|-1}{|\V|}$}
\end{align*}
The reason for $( 1 - \epsilon_h - \frac{\epsilon_h}{|\V|-1}):$ $(1-\epsilon_h) = p_h(v'_{h+1}|a'_{h})=p_h(v^l_{h+1}|a^l_{h})$ and $\frac{\epsilon_h}{|\V|-1} = p_h(v'_{h+1}|a^l_{h})= p_{h}(v^l_{h+1}|a'_{h})$.
In \cref{eqn: proof-cancel-out}, the two terms (corresponding to set L and E) are subtracted, with probability $(1-\epsilon_h)$ the first term will be larger since $F( R \cup \{v'_{h+1}\}) - F( R) \geq 0$ and with probability $\frac{\epsilon_h}{|\V|-1}$ the second term will be larger since the stochastic transition (e.g., looping action $a^l_h$ can jump to next state $v'_{h+1}$ and $a'_{h}$ stays to the same state $v^l_{h+1}$. This can happen with probability $\frac{\epsilon_h}{|\V|-1}$). In other stochastic transitions, in expectation, the two terms will sum to zero. 

In the above, we have proved the monotonicity of $J(\pi)$ given $F(\cdot)$ is a monotone function. We have,
\begin{align*}
       \frac{\partial J(\pi)}{\partial \pi^{h}(a'_{h})} &= \sum_{\traj} \mu(v_0) ( 1 - \frac{|\V|\epsilon_h}{|\V|-1}) \prod_{h=0}^{H-1} p_h(v_{h+1}|a_{h}) \left[\prod_{h\neq i}^{H-1} \pi^h(a_{h}) \right] \big( F( R \cup \{v'_{h+1}\}) - F( R)\big) \geq 0 
\end{align*}

To obtain the hessian terms, we can follow the same process as above at some horizon $g$ and state $a''$. 
Let $R \coloneqq \traj \backslash (v',v'')$, ($v''$ is the state corresponding to action $a''$),
\begin{align*}
    \frac{\partial^2 J(\pi)}{\partial \pi^g(a''_{g}) \partial \pi^{h}(a'_{h})} &= \sum_{\traj \in \Gamma} f'^{-}_{g,h} ( 1 - \frac{|\V|\epsilon_g}{|\V|-1}) ( 1 - \frac{|\V|\epsilon_h}{|\V|-1}) \\ 
    &\quad\quad\Big( F( R \cup \{v''_{g+1}, v'_{h+1}\}) - F( R \cup \{v'_{h+1}\}) - ( F( R \cup \{v''_{g+1}\})) - F(R)) \Big)\\
    &= \sum_{\traj \in \Gamma} f'^{-}_{g,h} ( 1 - \frac{|\V|\epsilon_g}{|\V|-1}) ( 1 - \frac{|\V|\epsilon_h}{|\V|-1}) \\
    &\quad \quad\bigg( F( \underbrace{R \cup \{v''_{g+1}\}}_{A} \cup \{v'_{h+1}\}) - F( \underbrace{R \cup \{v''_{g+1}\}}_{A}) - ( F( R \cup \{v'_{h+1}\}) - F(R)) \bigg) \\
    &\leq 0 \tag{ By submodularity of $F$, $R \subseteq A$, $\Delta_F(v'|A) \leq \Delta_F(v'|R)$ }
\end{align*}
Hence $J(\pi)$ is monotone DR-submodular.
\end{proof}


\subsection{Bounded Curvature}
\label{apx: proof_curvature}
\restateboundedC
\begin{proof} 
Consider the objectives $J(\pi)$ and $H(\pi)$ defined with submodular reward $F(\traj)$ and its corresponding modular rewards $F_m(\traj) = \sum_{s\in \traj} F(s)$ respectively,
\begin{align*}
    J(\pi) = \sum_{\traj} f(\traj; \pi) F(\traj),~~\mathrm{and}~~H(\pi) = \sum_{\traj} f(\traj; \pi) F_m(\traj).
\end{align*}
For any policy $\pi$, $J(\pi) \geq (1-c) H(\pi)$ using curvature definition. Consider $\nabla_{\pi} J(\pi)$,
\begin{align*}
    \nabla_{\pi} J(\pi) & = \sum_{\traj} \nabla_{\pi} f(\traj; \pi) F(\traj) \\
   & =  \sum_{\traj} \nabla_{\pi} f(\traj; \pi) \left( \sum_{i=0}^{|\traj|-1} F(s_{i+1}|\traj_{0:i}) + F(\{s_0\})\right) \\
   % &\geq \sum_{\traj} \nabla_{\pi} f(\traj; \pi) \left( \sum_{i=0}^{|\traj|-1} (1-c) F(\{s_{i+1}\}) + F(\{s_0\})\right)\\
   &\geq  \sum_{\traj} \nabla_{\pi} f(\traj; \pi) \left( \sum_{i=0}^{|\traj|} (1-c) F(\{s_{i}\})\right) \tag{using curvature definition}\\
   &=  (1-c) \sum_{\traj} \nabla_{\pi} f(\traj; \pi) F_m(\traj) = (1-c) \nabla_{\pi} H(\pi). 
\end{align*}
Similarly, since $F_m(\traj)\geq F(\traj)\geq (1-c)F_m(\traj)$, the following holds component-wise, 
\begin{align}
      \nabla_{\pi} H(\pi)|_{\pi = \pi'} \geq \nabla_{\pi} J(\pi)|_{\pi = \pi'} \geq  (1-c) \nabla_{\pi} H(\pi)|_{\pi = \pi'}  ~~\forall \pi'. \label{eg: gradient_sandwich}
\end{align}
At the convergence of \subPO, the stationary point $\pi$ satisfies, 
\begin{align*}
    &\max_{\pi' \in \Pi}\langle \nabla_{\pi} J(\pi), \pi'-\pi\rangle \leq 0 \\
    \implies &\max_{\pi' \in \Pi}\langle \nabla_{\pi} H(\pi), \pi'-\pi\rangle \leq 0. \tag{using \cref{eg: gradient_sandwich}, $c\neq1$}%\implies \max_{\pi' \in \kappa}\langle Q_\pi, \pi'-\pi\rangle_{\eta_\pi} \leq 0 
\end{align*}
Hence $\pi$ is also a stationary point for modular objective $H(\pi)$. 
Under mild regularity assumptions, any stationary point of the policy gradient cost function with modular rewards is a global optimum \citep{bhandari2019global}. Let $\pi$ be the policy where \subPO converges, then,
    \begin{align}
    J(\pi) = \sum_{\traj} f(\traj; \pi) F(\traj) &\geq (1-c) \sum_{\traj} f(\traj; \pi) F_m(\traj) \label{eq:curv_bound}\\
    &\geq (1-c) \sum_{\traj} f(\traj; \pi^{\star}) F_m(\traj) \label{eq:stationary_point}\\
    &\geq (1-c) \sum_{\traj} f(\traj; \pi^{\star}) F(\traj) = (1-c)J(\pi^{\star}) \label{eq:Fm_geq_F}
        % J(\pi) &\geq (1-c)J(\pi^\star)
    \end{align}
\cref{eq:curv_bound} follows using curvature definition for any policy $\pi$. \cref{eq:stationary_point} follows since $\pi$ is optimal of $H(\pi)$ where as $\pi^{\star}\in \Pi_{\nm}$ is optimal for $J(\pi)$. Finally, \cref{eq:Fm_geq_F} follows since $F_m(\traj) \geq F(\traj)$.
\end{proof}    
