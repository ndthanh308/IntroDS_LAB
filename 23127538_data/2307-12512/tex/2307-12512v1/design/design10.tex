\section{Circumventing low-spatial diversity}
\label{sec:design}

In the following sections, we tackle the fundamental challenge in single-vantage point localization. First, we will explore improving our phase measurements to improve location accuracy by increasing the antenna separation (Sec.~\ref{sec:des-res}). However, this comes with the unintended side-effect of introducing ambiguities to our location prediction. So, we explore the use of time difference of arrival (TDoA) measurements to combat these ambiguities (Sec.~\ref{sec:des-amb}). Finally, we explore fusing these measurements in an accurate and low-latency fashion by leveraging a particle filter (Sec.~\ref{sec:des-opt}). By exploring the key ideas here, \name will fulfill \textbf{R2} and furnish few-cm level localization.


\subsection{Improving localization resolution}\label{sec:des-res}

The prior learning from Sec.~\ref{sec:back} is that we reduce our resiliency to noise when we try to localize tags from a single vantage point.
Lacking spatial diversity adds vulnerability to the optimization creating large outlier measurements and preventing few-cm scale localization. However, when we have two closely (less than half-wavelength) separated antennas, we can find the phase difference ($\Delta \phi$) between this pair as 
\begin{align*}
\Delta \phi = \frac{2\pi d}{\lambda} \sin(\theta)
\end{align*}
where $\theta$ is the incoming angle of arrival w.r.t. to the normal of this pair of antennas, $d = \frac{\lambda}{2}$ is the distance between them, and $\lambda$ is the wavelength at $3.5$ GHz UWB center frequency.\footnote{we develop this intuition assuming far field, but later in Sec.~\ref{sec:des-opt} we consider the exact phase difference measurement} However, the typical UWB phase has a resolution of around $8$ bits \footnote{For example, although having a 16-bit real part and a 16-bit imaginary part in each CIR sample, DW1000 has a 7-bit phase resolution because the phase must be corrected by RCPHASE register, which is a 7-bits and for the adjustment of receiver carrier phase.}, which provides a phase resolution of $1.4^\circ$, and consequently a localization resolution of $~2.1$ cm at a distance of $3$ m from the localization module. 
However, increasing the inter-antenna separation, $d$, linearly increases the measured phase difference. We can leverage this to improve our localization resolution to the $\sim 1$ mm limit when the antenna separation is $1$ m.

Prior works~\cite{wang2014rf, cao2021itracku} have leveraged this fact to increase accuracy for handwriting tracking purposes. But, widening this separation comes at the cost of introducing more phase ambiguities. This is apparent when we return to the AoA equation and observe that our phase-difference measurements, $\Delta \phi$, wrap over $2 \pi$ for a larger separation than half-wavelength separation for angles between $-90^\circ < \theta < 90^\circ$. This is not an issue for tracking purposes, where the changes in location of these ambiguities mirror the true changes in the location and continue to provide a similar trajectory estimate. However, for \name, we find predicting and tracking these incorrect locations can degrade the localization accuracy by an order of magnitude to several tens of centimeters. 
 

\subsection{Ruling out ambiguities} \label{sec:des-amb}

To overcome ambiguities, a simple solution is adding more antennas between the two we have placed so far. These additional antennas will help eliminate phase ambiguities by reducing the consecutive antenna distance while employing a 1-m antenna array aperture. Fig.~\ref{fig:pdoa-tdoa-change-N} (a, b) depicts these ambiguities that exist in such a system by showing the likely positions of the tag. Considering the simulation environment from Sec.~\ref{sec:back}, we deploy two arrays with spacing $33.3$ cm and $25$ cm for the same antenna aperture of $1$ m. Next, we deploy a tag at the center of the space and predict its potential locations (pixel color intensities) in both scenarios. We observe that keeping the same aperture of 1 m, we have similar measurement errors (peak widths) in both cases, consistent with our previous findings, but reducing separation creates fewer ambiguities. Deploying $23$ antennas within this $1$ m, each spaced half-wavelength apart, will remove all our ambiguities at the cost of increased hardware complexity.  

Alternatively, we observe TDoA measurements are free from ambiguities and can potentially be leveraged to disambiguate the predictions from PDoA. Similarly to the previous PDoA images, in Fig.~\ref{fig:pdoa-tdoa-change-N} (c, d), we only show the tag's location likelihoods when relying on TDoA measurements. The TDoA peak, although very erroneous (larger peak widths), is unambiguous. Additionally, increasing the number of antennas reduces this error/peak width. To recap, by reducing the antenna separation (or increasing the number of antennas), we increase the separations between the ambiguities coming from PDoA measurements and tighten our peak widths coming from TDoA. Consequently, at the correct antenna spacing, our ambiguous peaks will be wide enough to be rejected by our TDoA measurements. We find this sweet spot when we use $6$ antennas, $4\times$ fewer antennas than would have otherwise been required. 

\subsection{Jointly optimizing for TDoA and PDoA measurements}\label{sec:des-opt}

We can now extend the key intuitions to leverage TDoA and PDoA to develop a localization algorithm to meet our few-cm accuracy requirement. As further explained in Sec.~\ref{sec:implementation}, via careful engineering and hardware design choices, we measure PDoA with a standard deviation $\sigma_\theta = 5^\circ$ and TDoA with a standard deviation of $\sigma_t = 150 ps$. This \textit{measurements} can be modeled as a zero-mean Gaussian:  
\begin{align*}
    \mathrm{TDoA\ between\ Rx\ i\ and\ j: }\ &t_{i,j} \sim \mathcal{N}(0, \sigma_t) \\
    \mathrm{PDoA\ between\ Rx\ i\ and\ j: }\ &\theta_{i,j} \sim \mathcal{N}(0, \sigma_\theta).
\end{align*}
Additionally, given a candidate tag location, $\vec{p}$, and receiver locations $\vec{x_i}, \forall i \in [1, 2,  \dots, N]$ we can also compute the \textit{expected} PDoA and TDoA as 
\begin{align}
    \mathrm{TDoA:}\ &\hat{t}_{ij} = \frac{|\vec{p} - \vec{x_i|}}{c} - \frac{|\vec{p} - \vec{x_j}|}{c} \nonumber \\ 
    \mathrm{PDoA:}\ &\hat{\theta}_{ij} = \mod \left(2\pi \left(\frac{|\vec{p} - \vec{x_i}|}{\lambda} - \frac{|\vec{p} - \vec{x_j}|}{\lambda}\right), 2\pi\right) \label{eq:pdoa}
\end{align}
where $\vec{p}$ is the location of the tag and $\vec{x}_i / \vec{x}_j$ are the locations of the 6 UWB antennas placed within a linear $1$ m array. $c$ and $\lambda$ are the speed of light and UWB wavelength, respectively. Note here we forgo the far-field assumption made in Sec.~\ref{sec:des-res}. 

% Figure environment removed

The location ($\vec{p}$) which gives the closest expected measurements to the actual measurements is the likely tag location, 
\begin{align}\label{eq:opt}
    \min_{\vec{p}} \begin{bmatrix} \vec{e_t}^T & \vec{e_\theta}^T \end{bmatrix}^T \Sigma^{-1}\begin{bmatrix} \vec{e_t}^T & \vec{e_\theta}^T \end{bmatrix} 
\end{align}
where $\vec{e}_t$ and $\vec{e}_\theta$ measure the error between our predictions and the actual measurements, and 
\begin{align*}    
\Sigma = \mathrm{diag}(\sigma_t^2, \cdots, \sigma_t^2, \sigma_\theta^2, \cdots \sigma_\theta^2)
\end{align*}
is a diagonal covariance matrix containing the TDoA and PDoA measurements standard deviations. Note here that since each receiver on \name's localization module is independently measuring the TDoA and PDoA, we have a diagonal covariance matrix. 

The simplest way to find this best tag location is to perform a grid search over our space to find the minimum point for Eq.~\ref{eq:opt}. Aiming for cm-level localization, we choose a grid size of $1 \times 1$ mm. But this exhaustive search can be time-consuming (around $61.2$ s / location on a 12-core CPU), precluding real-time localization in dynamic situations. Alternatively, we can leverage gradient descent-based optimization techniques~\cite{gil2007numerical} to arrive at the most likely tag position. However, these techniques fail when we do not have a good initial estimate of the location, which is the case when looking to localize a tag in a large environment~\cite{glorot2010understanding}.  

To surmount this challenge, we provide the final insight ---selectively searching over the large space instead can reduce the computation complexity for localization. The brute force approach unnecessarily searches over each grid point for every packet. We can instead sample our environment more sparsely and slowly converge to our ideal location over a few packets. This is, in fact, the key idea behind particle filters~\cite{aernouts2020combining}, which are commonly used in state estimation scenarios with highly non-convex error functions and poor initialization. 

Armed with this insight, for the first packet we receive, we uniformly distribute a set of particles (500 particles/$\mathrm{m}^2$) in our environment and compute the likelihood of these positions. When we receive consecutive packets, we can re-sample the set of particles with the highest probability and continue converging to our true locations. However, despite the fewer likelihood computations required, particle filters commonly furnish non-real-time estimates (with a latency of $~7.2$ ms on a 12-core CPU). To combat this problem, \name adaptively re-samples and reduces the number of particles based on the current confidence of the estimate. As we do not know the tag's location, many particles are initially required to sample the search space uniformly. However, our particles converge close to the true location over time, improving our confidence in the location estimate. We can reduce the number of particles needed as we no longer need to explore the space uniformly. Empirically, this adaptive particle filter implementation converges within five measurements and provides a location estimate with a $1.2$ ms latency on a 12-core CPU. 

