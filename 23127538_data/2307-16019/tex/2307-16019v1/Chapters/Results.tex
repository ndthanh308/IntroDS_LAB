

\begin{table*}[!tb]
\centering
\resizebox{12cm}{!}{%
\begin{tabular}{c|cccc|cccc|cccc}
\hline
&\multicolumn{4}{c}{AWA2}&\multicolumn{4}{c}{CUB} &\multicolumn{4}{c}{SUN} \\
\hline
Model&T1&U&S&H&T1&U&S&H&T1&U&S&H  \\
\hline
PROTO-LTN~\cite{Martone2022PROTOtypicalLT}&67.6   &32.0 &83.7 &46.2&48.8  &20.8 &54.3 &30.0 &60.4  &20.4  &\textbf{36.8}  &26.2 \\
DEM~\cite{dem}&67.1&30.5&\textbf{86.4}&45.1&51.7&19.6&57.9&29.2 &61.9&20.5&34.3&25.6 \\
VSE~\cite{vse}&84.4&45.6&88.7&60.2    &71.9&39.5&68.9&50.2&-&-&-&-\\
%DAZLE~\cite{DAZLE}&67.9&75.7&60.3 &67.1&65.9&59.6&56.7&58.1&59.3&24.3&52.3&33.2\\
%SP-AEN\cite{SP-AEN}&-&-&-&-&55.4&34.7&70.6&46.6  &59.2&24.9&38.6&30.3\\

TCN~\cite{TCN}&71.2&61.2&65.8&63.4&59.5 &52.6&52.0&52.3&61.5&31.2&37.3&34.0\\

CSNL~\cite{Sikka2020ZeroShotLW} $\dagger$&61.0&-&-&0.0&32.5&-&-&0.7&-&-&-&-\\
\hline
%QSFL\cite{QSFL}&63.5 &52.1&72.8&60.7&58.8 &33.3&48.1&39.4&56.2 &30.9&18.4&23.1 \\

AREN~\cite{AREN}&67.9& 54.7&79.1&64.7& 71.8&63.2&69.0&66.0&60.6&40.3&32.3&35.9 \\
APN~\cite{APN}&68.4 &56.5&78.0&65.5&72.0 &65.3&69.3&67.2&61.6 &41.9&34.0&37.6 \\
AMGML~\cite{Li2021AttributeModulatedGM}&71.7 &56.0&74.6&64.0&70.0 &58.2&55.7&56.9&59.7 &42.0&35.1&38.3 \\
%VSABN~\cite{Gao2022VisualSemanticAB}&67.6 & 56.1&71.8&63.0&-&-&-&-&57.0 &40.1&33.4&36.4 \\
%VGSE~\cite{Xu2022VGSEVS}&64.0&51.2&81.8 &63.0&28.9& 21.9&45.5 &29.5&38.1 &24.1&31.8&27.4 \\
CC-ZSL\cite{CC-ZSL}&68.8 &62.2&\underline{83.1}&71.1&74.3&\underline{66.1}&\underline{73.2}&\underline{69.5}&62.4& 44.4&36.9&40.3 \\

\hline
Cycle-CLSWGAN~\cite{cycle-CLSWGAN}&-&-&-&-&58.4&45.7&61.0&52.3&60.0&\textbf{49.4}&33.6&40.0  \\
LisGan~\cite{LisGAN}&-&-&-&-&58.8&46.5&57.9&51.6&60.0&42.9&37.8&40.2 \\
E-PGN~\cite{E-PGN}&73.4&52.6&83.5&64.6&72.4&52.0&61.1&56.2&-&-&-&- \\
TGMZ~\cite{TGMZ}& \textbf{78.4} &64.1& 77.3&70.1&66.1 &60.3&56.8&58.5&-&-&-&-\\
CEGZSL~\cite{CEGZSL}&70.4&63.1&78.6&70.0&\underline{77.5}&63.9&66.8&65.3&63.3&48.8&\underline{38.6}&\underline{43.1} \\
DFCA-GZSL~\cite{Su2023DualalignedFC}&\underline{74.7}&\underline{66.5}&81.5&\underline{73.3}&\textbf{80.0}&\textbf{70.9}&63.1&66.8&62.6&48.9&\textbf{38.8}&\textbf{43.3}\\
%TransZero~\cite{Chen2021TransZeroAT}&70.1& 61.3&82.3&70.2&76.8&69.3&68.3&68.8& 65.6&52.6&33.4&40.8\\

%ACMR~\cite{Fang2021LearningAC}&-&60.0&80.2&68.7&-& 53.1&57.7&55.3&-&49.1&39.5&43.8\\

%Co-GZSL~\cite{Li2023CoGZSLFC}&-&65.3&77.1&70.7&-&70.1&61.6& 65.6&-&-&-&-\\


\hline
FLVN $\dagger$&69.8&65.8&82.3&73.1& 71.2 & 62.6 &  83.1 &  71.5 &61.7&48.4&32.7&39.0\\
&\textpm 0.8 &\textpm 0.9&\textpm 0.3&\textpm 0.6& \textpm 0.2 & \textpm  0.5 & \textpm 0.3 & \textpm 0.2 &\textpm0.18&\textpm0.57&\textpm0.2&\textpm 0.1\\
&(71.0)&\textbf({67.1})&(82.8)&(\textbf{74.1})&(71.4)& (63.2)&\textbf({83.4}) & (\textbf{71.7})&(61.9)&\underline({48.9})&(32.9)& (39.1) \\
\hline
\end{tabular}}
\vspace*{5mm}
    \caption{Performance on AWA2, CUB and SUN test set.  For FLVN, we show mean Â± standard deviation and maximum (in parentheses) values for TOP1zsl (T1), TOP1gzsl unseen (U), TOP1gzsl seen (S), and Hgzsl (H) over three runs.  Metrics are described in~\cite{awa2}. The first section of the table includes embedding-based models, the second section attention-based models, and the third section generative models. The best performance values are shown in bold. $\dagger$  highlights method which incorporate external knowledge.}
\label{tab:table_test}
\end{table*}
Experimental results presented in Table~\ref{tab:table_test} show how the proposed FLVN architecture reaches competitive performances with respect to other embedding-based methods, particularly CC-ZSL~\cite{CC-ZSL} and APN~\cite{APN}. In general, FVLN performs better in terms of harmonic mean (H) and shows a greater ability to recognize both seen and unseen classes, achieving state-of-the-art performance on two out of three benchmark datasets. In particular, FLVN improves the accuracy of unseen classes and the harmonic mean by 0. 89\% and 1.3\%, respectively, in AWA2, and the accuracy of seen classes and harmonic mean by 12\% and 3\%, respectively, on CUB. These results suggest that the proposed architecture is capable of recognizing seen and unseen samples more evenly, showing less confusion in classifying examples. On SUN, FLVN performs comparably to the state of the art; it is important to note that, in this case, axioms on macro classes are not introduced, therefore FLVN cannot leverage external semantic knowledge. 

Compared to the closest methods in terms of performance, the proposed method requires a relatively simple architecture. Unlike APN, FVLN does not require additional weights for each attribute or supplemental regularization terms to construct the loss function. FVLN only requires a single backbone replica instead of two as in CC-ZSL, which is based on the teacher-student framework~\cite{CC-ZSL}. Compared to generative methods, FLVN does not require to generate additional training samples or make assumptions about unseen classes at train time. 

Although complete ablation studies are left to future work, qualitative observations from our experiments suggest that axioms included in the knowledge base may have quite different impact on the overall performance. With respect to our previous architecture \cite{Martone2022PROTOtypicalLT}, it was crucial to change the  $\texttt{isOfClass}$ grounding for end-to-end training to succeed. The introduction of the $\texttt{isOfMacroclass}$ predicate, on the other hand, introduces smaller improvements. In fact, during training related classes appear to naturally cluster in feature space, as evident from the distribution of the feature space already reported in previous work \cite{Martone2022PROTOtypicalLT}. On the other hand, the $\texttt{isOfClass_{masked}}$ offered important benefits, allowing to account for mistakes in the semantic annotation of classes or traits that exist at the class level but are not visible or easily inferred from a single image (e.g., ``All zebras are agile''). Finally, the \texttt{hasSameAttribute} predicate improved performance particularly on the CUB (fine-grained bird recognition) and SUN (scene recognition) datasets, in which extracting fine-grained image-level attributes is more difficult.






 