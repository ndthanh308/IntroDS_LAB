There is an increasing interest in the computer vision community towards the integration of learning and reasoning, with specific emphasis towards the fusion of deep neural networks and symbolic artificial intelligence \cite{Yu2021RecentAI,LTN,van2021modular}. By integrating perception, reasoning and learning, Neuro-symbolic (NeSy) architectures can improve explainability, generalization and robustness of deep learning systems. In addition, NeSy techniques can incorporate prior symbolic knowledge in the training objective of deep neural networks, compensating for the lack of supervision from labelled examples \cite{manigrasso2021faster,donadello2019compensating}.  The latter property is of particular interest in the context of Zero-Shot Learning (ZSL), which aims at recognizing objects from unseen classes by associating both seen and unseen classes to auxiliary information, usually in the form of class attributes \cite{awa2}.

In this paper, we introduce the Fuzzy Logic Visual Network (FLVN), an architecture designed to learn a joint embedding space for visual features and class attributes to tackle ZSL scenarios. To constraint learning based on prior knowledge available from resources such as WordNet, it leverages the Logic Tensor Network (LTN)\cite{LTN} paradigm, a NeSy framework that formulates learning as maximizing the satisfiability of a knowledge base $\mathcal{K}$ based on a first order logic (FOL). Thus, FLVN is composed by a convolutional neural network (CNN), that computes embedding, followed by a LTN.

The proposed FLVN improves and extends previous NeSy architectures for ZSL, and in particular our previous architecture ProtoLTN \cite{Martone2022PROTOtypicalLT}, in several ways. First, there are substantial differences in the LTN formulation, and especially the way the isOfClass predicate is formulated. FLVN, learns to project visual features into the semantic attribute space, facilitating the integration of prior knowledge into the learning process by the LTN. FLVN is trained end-to-end, whereas ProtoLTN used features extracted from a pre-trained frozen network, and only the class-level prototypes were trained. This implementation outperforms our previous model \cite{Martone2022PROTOtypicalLT}, offering an alternative approach for grounding the predicates in the FOL language by changing the how the distance measure between extracted features and semantic vectors is calculated. 

Compared to most existing approaches to ZSL, FLVN incorporates into the training process several logical axioms that combine both class-specific prior knowledge (e.g., ``a zebra is an ungulate'') with high-level inductive biases (e.g., ``two images of the same class should have similar embeddings''), that were not yet included in ProtoLTN \cite{Martone2022PROTOtypicalLT}. Finally, we introduce axioms to represent exceptions within the dataset to make the architecture more robust in the context of ZSL,  where class attributes are specified at the class level rather than the image level. To the best of our knowledge, this aspect is not taken into account in current approaches to ZSL, including neuro-symbolic ones \cite{Martone2022PROTOtypicalLT}. Through extensive experiments on multiple benchmarks, we show that FLVN outperforms existing ZSL architectures.  The implementation, developed in PyTorch using the LTNtorch package \cite{LTNtorch}, is available at
\url{https://gitlab.com/grains2/flvn}.

The remainder of the paper is structured as follows. Section \ref{sec:related} introduces background on the LTN framework and related work. Section \ref{sec:flvn} describes the proposed architecture. In Sections \ref{sec:Experiments} and \ref{sec:Results}, we analyze the model behavior in a ZSL and a generalized ZSL (GZSL) setting on common benchmarks. Finally, in Section \ref{sec:Conclusion}, we discuss the results and future work.