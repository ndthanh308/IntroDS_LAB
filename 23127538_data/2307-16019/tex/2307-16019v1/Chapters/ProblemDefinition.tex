We introduce a comprehensive and trainable framework for the task of ZSL, depicted in Figure~\ref{Figure:architecture}. 
It comprises two main modules: a feature extractor and a LTN that formulates the training objective.
\subsubsection{Feature extractor.} 
 The feature (embedding) extractor is a CNN that maps the input $x$ to a feature space $f_{\theta}(x) \in \mathbb{R} ^{H \times W \times B}$, where $H$, $W$ and $B$ represent the height, width, and number of channels of the features, respectively. Through mean pooling over $H$ and $W$, we obtain global discriminative characteristics $g_{\theta}(x) \in \mathbb{R}^{B \times 1}$, and utilize a linear projection to transform them into a semantic space represented by $V \in \mathbb{R}^{B \times M}$, where $B$ is the dimension of the vector space of features, and $M$ denotes the length of the attribute vector \cite{APN}. 



% Figure environment removed


\subsubsection{Logic Tensor Network.}
The LTN  formulates the learning objective as the maximum satisfiability of a $\KK$ .  For each training batch, the $\KK$ is updated introducing axioms that represent labelled examples ($\phi_1$), as well as prior knowledge ($\phi_2, \phi_3, \phi_4, \phi_5,\phi_6$). The maximum satisfiability loss is then defined based on the aggregation of all axioms as follows:
\begin{equation}
        \mathcal{L}^{\text{ep}} = 1 -\left(\bigwedge_{\phi \in \mathcal{K}} \phi \right)  = 1 -\GG(\phi) %, \GG(\phi_{\text{neg}})
        \label{eq:loss}
\end{equation}

This section first define the variables, predicates and domain that form the FOL language, followed by the definition of the knowledge base $\KK$. 

\textbf{Groundings}. Variables and their domains are grounded as follows:

\begin{equation}
\GG(l)= \mathbb{N}^{C}, 
\GG(q)= \mathbb{N}^{Q}
\end{equation}
\begin{equation}
\GG(a)=  \GG(a^{\text{mask}})= \mathbb{R}^{M \times C} 
\end{equation}
\begin{equation}
\GG(a^{\text{macro}})= \mathbb{R}^{M \times Q} 
\end{equation}
\begin{equation}
\GG(x)=  g_\theta(f_\theta(\GG(\texttt{images}))) = \mathbb{R}^{M}  \\
\end{equation}
where the variable $l$ represents the class labels belonging to set of classes $C$, $q$ represent the macroclass label belonging to set of macroclasses $Q$, and each class/macroclass is described by a set of non-binary semantic attributes denoted by $a$ and $a^{\text{macro}}$, respectively. Functions $f_\theta$ and $g_\theta$ are employed to embed images into the attribute space, resulting in the final representation $\GG(x)$. The FOL language contains four main predicates: $\texttt{isOfClass}(x,l)$ and $\texttt{isOfClass}_{\text{masked}}(x,l)$ denote the fact that an image $x$ belongs to class $l$,
$\texttt{isOfMacro}(x,q)$ that an image $x$ belongs to the macroclass $q$, and $\texttt{hasSameAttribute}(x_1, x_2)$ that two images have the same attributes. 

The $\GG(\texttt{isOfClass})$ predicate is grounded by the similarity  between the input image and the corresponding class attribute vectors. First, we compute the similarity between the image $x$ and a class $l_c$ by calculating the scaled product of the global features mapped in the attribute space with the semantic vectors:
\begin{equation}
p\left(x,l\right)=\frac{\exp \left(x^T V a_{l} \right)}{\sum_{s=1}^S \exp \left(x^T V a_s \right)}
\label{eq:prob}
\end{equation}
where $a_{y}$ represents the semantic attribute vector associated with class $l$. To obtain a prediction score for an example $x$, we calculate the dot product between the output of $p$ (Eq. \ref{eq:prob}) and the one-hot encoding $l_{c}^T$ for class $c \in C$, as follows:

\begin{equation}
\GG(\texttt{isOfClass}):x,l_c \rightarrow {l_c}^T p(\GG(x),l_c)
\end{equation}


 Similarly, we define $\GG(\texttt{isOfMacro})$ and $\GG(\texttt{isOfClass}_{\text{masked}})$. Since attributes for macro-classes are in principle unknown, we define a trainable attribute vector $a_m^{\text{macro}}$ for macro-class $q_m$ to compute $\GG(\texttt{isOfMacro})$. On the other hand, $\GG(\texttt{isOfClass}_{\text{masked}})$ uses the masked attribute vector $a_c^{\text{masked}}$ for class $l_c$, in which missing attributes $k$ are set to 0, while preserving the rest of the attributes in $a_c$. Finally, the grounding for $\texttt{hasSameAttribute}$  is defined as:
\begin{equation}
\GG(\texttt{hasSameAttribute}):x_1,x_2 \rightarrow   \texttt{sigmoid}(\alpha  \texttt{d}(\GG(x_1),\GG(x_2))) 
\label{eq:gt_sameattributes}
\end{equation}
where $\texttt{d}$ is the cosine similarity, $\alpha$ a scale factor and $\GG(x_1)$, $\GG(x_2)$ correspond to the embeddings of the two images.


\textbf{Learning from labeled examples}. 
We incorporate labelled examples by introducing an axiom $\phi_1$ stating that all facts about labeled example should be true, that is, all labeled samples should be classified correctly:
\begin{equation}
\phi_{\text{1}} = \forall \text{Diag}(x,l_c) (\texttt{isOfClass}(x,l_c))   \label{eq:isOfClass} \footnote{Diagonal Quantification quantifies over pairs of instances, e.g., images and their labels. A more formal definition can be found in \cite{LTN}.}
\end{equation}

To account for the class hierarchy, we also introduce an axiomatic statement $\phi_2$ to indicate that ``if an image contains a zebra'', then ``the image belongs to the family of ungulates'':

\begin{equation}
\phi_{\text{2}} = \forall \text{Diag}(x,l_c,q_m) (\texttt{isOfClass}(x,l_c) \implies \texttt{isOfMacro}(x,q_m)) 
\label{eq:isOfMacro}
\end{equation}

\textbf{Learning better feature representations}. The following axiom encodes the assumption that features extracted from two images of the same class should possess the same attributes:

\begin{equation}
\phi_{\text{3}} = \forall \text{Diag}(x_1,l_{c_1}) \bigg( \forall\text{Diag}(x_2, l_{c_2} ):{c_1}={c_2} \texttt{ }
 \texttt{hasSameAttribute}(x_1,x_2) \bigg)
 \label{eq:sameattribute}
\end{equation}

Likewise, images from different classes should possess different attributes:
\begin{equation}
\phi_{\text{4}} = \forall \text{Diag}(x_1,l_{c_1}) \bigg(\forall\text{Diag}(x_2, l_{c_2} ):{c_1}!={c_2} \texttt{ }
 \neg\texttt{hasSameAttribute}(x_1,x_2)\bigg)
 \label{eq:notsameattribute}
\end{equation}

To further emphasize the similarity between visual attributes and semantic vectors, the following axiom enforces the similarity between image embeddings and attribute vectors of the same class:
\begin{equation}
\phi_{\text{5}} = \forall \text{Diag}(x,l_{c}) \bigg( \forall\text{Diag}(a, l_{a} ): c=a
 \texttt{ hasSameAttribute}(x,a) \bigg)
 \label{eq:sameattribute_class}
\end{equation} 

\textbf{Learning with refutation}. In classical ZSL benchmarks such as AWA2, attributes are associated with class labels using a crisp or fuzzy matrix. However, this association does not entail that all examples of a class will exhibit exactly the same attributes: attributes may be expressed by a subset of training samples, or may be occluded. The existential statement $\phi_6$ represents the fact that some class attributes may not be present for all samples (e.g, ``there exists a zebra that is not agile''). Given that image-level attributes are not available, we simply remove randomly selected attributes by defining the $\texttt{isOfClass}_{\text{masked}}$ predicate:
%\begin{equation}
%\phi_{\text{6}} = \forall \text{Diag} (a_{masked}^{seen}, l^{seen})
%( \exists x , \texttt{isOfClass}_{masked}(x,a_{masked}^{seen})  )
%\label{eq:isOfClassmasked}
%\end{equation}
\begin{equation}
\phi_{\text{6}} = \forall  l^{\text{seen}}
( \exists x , \texttt{isOfClass}_{\text{masked}}(x,l^{\text{seen}})  )
\label{eq:isOfClassmasked}
\end{equation}
where $l^{\text{seen}}$ denotes the list of seen classes.

\textbf{Grounding logical connectives and aggregators}.
The knowledge base $\KK$ is an aggregation of formulas  updated at each training step. To solve the maximum satisfiability problem using gradient descent, logical connectives and aggregators must be grounded into Real Logic. Given  two truth values $a$ and $b$ in $[0,1]$, we adopted the symmetric configuration from \cite{LTN}, using the standard negation $\neg: N_S(a) =1-a $ and the Reichenbach implication $\rightarrow: I_R(a, b)  =1-a+a b$. The existential quantifier $\exists$ was approximated by the generalized mean $A_{pM}$, and the universal quantifier $\forall$ by the generalized mean w.r.t. the error $A_{pME}$, respectively \cite{LTN,van2008visualizing}.  Given $n$ truth values $a_1, \ldots, a_n$ all in $[0,1]$:
\begin{equation}
 \exists: A_{p M}\left(a_1, \ldots, a_n\right)=\left(\frac{1}{n} \sum_{i=1}^n a_i^{p_{\exists}}\right)^{\frac{1}{p_{\exists}}} \quad p_{\exists} \geqslant 1 \\
 \label{eq:aggregmean}
\end{equation}
\begin{equation}
\forall: A_{p M E}\left(a_1, \ldots, a_n\right)=1-\left(\frac{1}{n} \sum_{i=1}^n\left(1-a_i\right)^{p_{\forall}}\right)^{\frac{1}{p_{\forall}}} \quad p_{\forall} \geqslant 1\\
\label{eq:aggregmeanerror}
\end{equation}

$A_{p M E}$ is a measure of how much, on average, truth values $a_i$ deviate from the true value of 1. The $A_{p M E}$ was also used to approximate $\bigwedge$ in Eq.[\ref{eq:loss}]. Further details on the role of $p_{\exists}$ and $p_{\forall}$ can be found in previous works \cite{LTN}.

%Introducing general logical prepositions allows the architecture to recognize the characteristics of the classes being examined; however, demonstrating the presence of a false or invalid logical preposition (refutation) is even more important in order to manage exceptions to these rules and strengthen the validity of the developed theory. For this reason, we introduce a series of axioms useful for identifying the classes represented in the dataset with a hierarchical relationship.





%- w_{\text{n}} where the weight $w_{\text{n}}$ reflects the expectation that negations play a less discriminative role than affirmation in classification, in our case $w_{\text{n}}=0$ since we do not introduce negative axioms, 

\textbf{Querying the knowledge base}. 
At inference time, the class with the highest score is selected as the predicted class:

\begin{equation}
\hat{y}=\underset{\tilde{y} \in \mathcal{Y}^U}{argmax} (g(x)^{\mathrm{T}} V a_{\tilde{y}})
\end{equation}

FVLN was evaluated in both ZSL and GZSL settings.  In the ZSL setting, only unseen images are assumed to be present at test time, whereas in the GZSL setting, the model is tested on both seen and unseen classes. This setup induces a bias towards seen classes. To mitigate it, we employed the Calibrated Stacking method, as proposed in \cite{Chen2022MSDNMS,Wang2023GeneralizedZA}, to diminish the classification score of seen classes. The class score is thus calculated as $\hat{y}$:
\begin{equation}
\hat{y}=\underset{\tilde{y} \in \mathcal{Y}^U \cup \mathcal{Y}^S}{argmax }( g(x)^{\mathrm{T}} V a_{\tilde{y}}-\gamma \mathbb{I}\left[\tilde{y} \in \mathcal{Y}^S\right])
\end{equation}
where $\mathbb{I}=1$ if $\tilde{y}$ is from a seen class and zero otherwise, $\gamma$ is a calibration coefficient tuned on a validation set and $\mathcal{Y}^S$ are the labels of seen classes.

\textbf{Construction of the training batch}. Following the approach in \cite{CC-ZSL}, for a positive input image $x_{i}$, we select a set of positive examples $x^{+}$ and $K$ negative examples ${ x_{1}^{-},...,x_{K}^{-} }$. Positive examples are selected from the same category as $x_{i}$, while negative examples are randomly selected from the remaining classes.

