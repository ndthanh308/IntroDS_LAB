In this section we examine the datasets used in our experiments, the knowledge base adopted for each dataset and the list of hyperparameters chosen.

\textbf{Dataset}. The experiments were conducted on the AWA2~\cite{awa2}, CUB~\cite{CUB}, and SUN~\cite{SUN} benchmarks. The evaluation metrics for GZSL were based on the standards defined in previous work~\cite{awa2}. 
Following a similar approach outlined in~\cite{Sikka2020ZeroShotLW}, we construct a semantic hierarchy by grouping the classes from Awa2 and CUB datasets into a total of 9 and 49 macroclasses, respectively.


%\LM{Sarebbe utile indicare quante classi e macro-classi ci sono nei vari dataset, non diciamo nulla sulla gerarchia utilizzata}

\textbf{Knowledge base}. The knowledge base $\KK$ composition was detailed in Section \ref{sec:flvn}. To build class hierarchy, we map classes from the AWA2 and CUB datasets to their corresponding synsets in WordNet as done by Sikka et al. \cite{Sikka2020ZeroShotLW}. Macroclasses for each dataset were defined by selecting the synset root whose subtree contained classes from the selected dataset, and then defining its immediate children (using WordNet) as classes. Since classes in the SUN dataset~\cite{SUN} lack a semantic hierarchy, axioms related to macroclasses were not included in the knowledge base. For all experiments, in the predicate $\texttt{isofClass}_{masked}$, $k=15$ attributes were randomly dropped. The $\alpha$ parameter in Eq. \ref{eq:gt_sameattributes} was set to 0.01 for AWA2 dataset and 1 on CUB e SUN. To account for the presence of outliers in the knowledge base, we initially set the aggregation function parameters (defined in Eqs. \ref{eq:aggregmean} and \ref{eq:aggregmeanerror})  as $p_{\exists}=2$ and $p_\forall=2$. Both parameters were incremented by 2 every 4 epochs for the AWA2 and CUB datasets; for SUN, the aggregation values were increased at specific epochs (2, 4, 24, and 32) until reaching $p_\exists=6$ and $p_\forall=6$, following the schedule suggested in~\cite{LTN}.

\textbf{Hyperparameter selection}. The embedding function $f_{\theta}$ is based on an ImageNet pre-trained ResNet101 model  that converts the $224 \times 224$ image  into a vector $\mathbf{x} \in \reals^{H \times W \times B}$, where $B=2048$, and $H$ and $W$ represent the height and width of the extracted features; the function $g_\theta$ then converts these features into the attribute space dimensionally consistent with the dataset.

To mitigate overfitting, we initially trained the head with a frozen backbone and subsequently fine-tuned the entire network. We used the Adam optimizer with learning rate of $1e-4$ for the pre-training phase for AWA2 and CUB and 5e-4 for SUN. The learning rate was then reduced to $1e-6\alpha$ for CUB and SUN and $1e-7\alpha$ for AWA2 in the fine-tuning phase, where $\alpha=0.8^{epoch//10}$ and \textit{epoch} is the current epoch out of a total of 300 epochs. Additionally, we incorporated a smoothing factor by multiplying the L2 norm with a scaling factor of 5e-4 for AWA2, 5e-6 for CUB, and 1e-3 for SUN. 

Training batches included positive and negative examples with a ratio of 12 to 12 for AWA2, 12 to 8 for CUB, and 12 to 4 for SUN datasets. At inference time, we set the scaling factor $\gamma$ to adjust the scores obtained for the seen classes to 0.7 for AWA2 and CUB and 0.4 for SUN. In all experiments, we applied a random crop and random flip with 0.5 probability for data augmentation. We implemented the architecture in PyTorch, using the LTNtorch library~\cite{LTNtorch}, and trained it on a single GPU nVidia 2080 Ti. Each experiment was repeated three times to calculate the mean and standard deviation.  








