\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{footnote}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}
\newcommand{\eg}{e.\,g., }
\newcommand{\ie}{i.\,e., }


  
%% Title
\title{TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting }
%%%% Cite as
%%%% Update your official citation here when published 
% \thanks{\textit{\underline{Citation}}: 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
% }

\author{
  Nancy Xu \\
  KTH Royal Institute of Technology \\
  Stockholm, Sweden\\
  \texttt{nancyx@kth.se} \\
  %% examples of more authors
   \And
  Chrysoula Kosma \\
  \'Ecole Polytechnique IPP \\
  Palaiseau, France \\
  \texttt{kosma@lix.polytechnique.fr} \\
  \And
  Michalis Vazirgiannis \\
  \'Ecole Polytechnique IPP \\
  Palaiseau, France \\
  KTH Royal Institute of Technology \\
  Stockholm, Sweden\\
  \texttt{mvazirg@lix.polytechnique.fr} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

% \author{Nancy Xu\inst{1} \and
% Chrysoula Kosma\inst{2} \and
% Michalis Vazirgiannis\inst{1,2}}
% %
% \authorrunning{N. Xu et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{KTH Royal Institute of Technology, Stockholm, Sweden \\
% \email{nancyx@kth.se}\and
% \'Ecole Polytechnique IPP, Palaiseau, France \\
% \email{\{kosma,mvazirg\}@lix.polytechnique.fr}}
% %


\begin{document}
\maketitle


\begin{abstract}
%The abstract should briefly summarize the contents of the paper in 15--250 words.
% Time series forecasting lies at the core of important real-world applications in nearly all fields of science and engineering.
% The abundance of large time series datasets that consist of several co-occuring patterns and noisy components has led to the development of various neural network architectures for forecasting.
% The need for modeling the interactions of multiple time series as long as their temporal evolution has drawn attention to algorithms that extract representations of time observations in the spectral domain.
% Recent works on multivariate time series, have focused on jointly learning a graph structure while performing a supervised task, such as classification or regression, by parameterizing their correlation graph in a differentiable way.
% In this paper, we extend correlation graphs to dynamic temporal representations that can capture the evolution of inter-series patterns.
% Our experimental evaluation indicates that the proposed graph structure learning and forecasting architecture achieves comparable performance to other state-of-the-art methods while offering significantly faster training times.

%Time series forecasting lies at the core of important real-world applications in many fields of science and engineering. 
%Graph neural network approaches which learn a graph structure based on the variables in a time series have recently seen great success in forecasting tasks, however, such solutions are costly to train and difficult to scale. 
%In this paper, we propose TimeGNN, a method that learns dynamic temporal graph representations that can capture the evolution of inter-series patterns. 
%TimeGNN achieves inference times \textcolor{red}{4 to 80 times} faster than other state-of-the-art \textcolor{red}{graph-based} methods while achieving comparable forecasting performance. 
Time series forecasting lies at the core of important real-world applications in many fields of science and engineering. 
The abundance of large time series datasets that consist of complex patterns and long-term dependencies has led to the development of various neural network architectures.
Graph neural network approaches, which jointly learn a graph structure based on the correlation of raw values of multivariate time series while forecasting, have recently seen great success.
However, such solutions are often costly to train and difficult to scale. 
In this paper, we propose TimeGNN, a method that learns dynamic temporal graph representations that can capture the evolution of inter-series patterns along with the correlations of multiple series. 
TimeGNN achieves inference times 4 to 80 times faster than other state-of-the-art graph-based methods while achieving comparable forecasting performance.
\end{abstract}


% keywords can be removed
\keywords{Time Series Forecasting \and Graph Structure Learning \and GNNs}


\section{Introduction}
From financial investment and market analysis \cite{ding2015deep} to traffic \cite{li2017diffusion}, electricity management, healthcare \cite{chauhan2015anomaly}, and climate science, accurately predicting the future real values of series based on available historical records forms a coveted task over time in various scientific and industrial fields. 
There are a wide variety of methods employed for time series forecasting, ranging from statistical \cite{box2015time} to recent deep learning approaches \cite{lim2021time}. However, there are several major challenges present.
Real-world time series data are often subject to noisy and irregular observations, missing values, repeated patterns of variable periodicities and very long-term dependencies.

While the time series are supposed to represent continuous phenomena, the data is usually collected using sensors. Thus, observations are determined by a sampling rate with potential information loss.
On the other hand, standard sequential neural networks, such as recurrent (RNNs) \cite{rumelhart1986learning} and convolutional networks (CNNs) \cite{lecun1998gradient}, are discrete and assume regular spacing between observations. 
Several continuous analogues of such architectures that implicitly handle the time information have been proposed to address irregularly sampled missing data \cite{rubanova2019latent,romero2021ckconv}.

The variable periodicities and long-term dependencies present in the data make models prone to shape and temporal distortions, overfitting and poor local minima while training with standard loss functions (\eg MSE).
Variants of DTW and MSE have been proposed to mitigate these phenomena and can increase the forecasting quality of deep neural networks \cite{le2022deep,kosma2022time}.

A novel perspective for boosting the robustness of neural networks for complex time series is to extract representative embeddings for patterns after transforming them to another representation domain, such as the spectral one.
Spectral approaches have seen much use in the text domain. 
Graph-based text mining (\ie Graph-of-Words) \cite{rousseau2013graph} can be used for capturing the relationships between the terms and building document-level representations.

It is natural, then, that such approaches might be suitable for more general sequence modeling. 
Capitalizing on the recent success of graph neural networks (GNNs) on graph structured data, a new family of algorithms jointly learns a correlation graph between interrelated time series while simultaneously performing forecasting \cite{wu2020connecting,cao2020spectral,shang2021discrete}.
The nodes in the learnable graph structure represent each individual time series and the links between them express their temporal similarities.
However, since such methods rely on series-to-series correlations, they do not explicitly represent the inter-series temporal dynamics evolution. 

Some preliminary studies have proposed simple computational methods for mapping time series to temporal graphs where each node corresponds to a time step, such as the visibility graph \cite{lacasa2008time} and the recurrence network \cite{donner2010recurrence}.
In this paper, we propose a novel neural network, \textit{TimeGNN}, that extends these previous approaches by jointly learning dynamic temporal graphs for time series forecasting on raw data. 
TimeGNN (i) extracts temporal embeddings from sliding windows of the input series using dilated convolutions of different receptive sizes, (ii) constructs a learnable graph structure, which is forward and directed, based on the similarity of the embedding vectors in each window in a differentiable way, (iii) applies standard GNN architectures to learn embeddings for each node and produces forecasts based on the representation vector of the last time step.

We evaluate the proposed architecture on various real-world datasets and compare it against several deep learning benchmarks, including graph-based approaches. 
Our results indicate that TimeGNN is significantly less costly in both inference and training while achieving comparable forecasting performance.


\section{Related Work}
\noindent\textbf{Time series forecasting models.}
Time series forecasting has been a long-studied challenge in several application domains.
In terms of statistical methods, linear models including the autoregressive integrated moving average (ARIMA) \cite{box2015time} and its multivariate extension, the vector autoregressive model (VAR) \cite{hamilton2020time} constitute the most dominant approaches.
The need for capturing non-linear patterns and overcoming the strong assumptions for statistical methods, \eg the stationarity assumption, has led to the application of deep neural networks, initially introduced in sequential modeling, to the time series forecasting setting. 
Those models include recurrent neural networks (RNNs) \cite{rumelhart1986learning} and their improved variants for alleviating the vanishing gradient problem, namely the LSTM \cite{hochreiter1997long} and the GRU \cite{cho2014learning}. 
An alternative method for extracting long-term dependencies via large receptive fields can be achieved by leveraging stacked dilated convolutions, as proposed along with the Temporal Convolution Network (TCN) \cite{bai2018empirical}.
Bridging CNNs and LSTMs to capture both short-term local dependency patterns among variables and long-term patterns, the Long- and Short-term Time-series network (LSTNet) \cite{lai2018modeling} has been proposed.
For univariate point forecasting, the recently proposed N-BEATS model \cite{oreshkin2019n} introduces a deep neural architecture based on a deep stack of fully-connected layers with basis expansion.
Attention-based approaches have also been employed for time-series forecasting, including
Transformer \cite{vaswani2017attention} and Informer \cite{zhou2021informer}.
Finally, for efficient long-term modeling, the most recent Autoformer architecture \cite{wu2021autoformer} introduces an auto-correlation mechanism in place of self-attention, which extracts and aggregates similar sub-series based on the series periodicity.

\noindent\textbf{Graph neural networks.}
Over the past few years, graph neural networks (GNNs) have been applied with great success to machine learning problems on graphs in various fields, including chemistry for drug screening \cite{kearnes2016molecular} and biology for predicting the functions of proteins modeled as graphs \cite{gligorijevic2021structure}. 
The field of GNNs has been largely dominated by the so-called message passing neural networks (MPNNs) \cite{gilmer2017neural}, where each node updates its feature vector by aggregating the feature vectors of its neighbors. 
In the case of time series data on arbitrary known graphs, \eg in traffic forecasting, several architectures that combine sequential models with GNNs have been proposed \cite{li2017diffusion,yu2017spatio,seo2018structured,zhao2019t}.

\noindent\textbf{Joint graph structure learning and forecasting.}
However, since spatial-temporal forecasting requires an apriori topology which does not apply in the case of most real-world time series datasets, graph structure learning has arisen as a viable solution.
Recent models perform joint graph learning and forecasting for multivariate time series data using GNNs, intending to capture temporal patterns and exploit the interdependency among time series while predicting the series' future values.
The most dominant algorithms include NRI \cite{kipf2018neural}, MTGNN \cite{wu2020connecting} and GTS \cite{shang2021discrete}, in which the graph nodes represent the individual time series and their edges represent their temporal evolution. %StemGNN \cite{cao2020spectral} 
MTGNN obtains the graph adjacency from the as a degree-$k$ structure from the pairwise scores of embeddings of each series in the multivariate collection, which might pose challenges to end-to-end learning.
On the other hand, NRI and GTS employ the Gumbel softmax trick \cite{jang2016categorical} to sample from the edge probabilities a discrete adjacency matrix in a differentiable way.
Both models compute fixed-size representations of each node based on the time series, with the former dynamically producing the representations per individual window and the latter extracting global representations from the whole training series.
MTGNN combines temporal convolution with graph convolution layers, and GTS uses a Diffusion Convolutional Recurrent Neural Network (DCRNN) \cite{li2017diffusion}, where the hidden representations of nodes are diffused using graph convolutions at each step.

\section{Method}
%Variable key
% K = window size 
%   = number of GNN iterations 
% E = adjacency matrix/edges? 
Let $\{X_{it}\} \in \mathbb{R}^{m \times L}$ be a multivariate time series.
Each time series consists of $m$ channels and has a length equal to $L$ which corresponds to the observation times $\{t_1, t_2, \ldots, t_L\}$.
Then, $\mathbf{X}_t \in \mathbb{R}^{m}$ represents the observed values at time step $t$.
Let also $\mathcal{G}$ denote the set of temporal dynamic graph structures that we want to infer.

Given the observed values of $T$ previous time steps of the time series, \ie $\mathbf{X}_{t-T}, \ldots, \mathbf{X}_{t-1}$, the goal is to forecast the next $\tau$ time steps ($\tau=1$ for 1-step forecasting), \ie $\hat{\mathbf{X}}_{t}, \hat{\mathbf{X}}_{t+1}, \ldots, \hat{\mathbf{X}}_{t+\tau-1}$. 
These values can be obtained by the forecasting model $\mathcal{F}$ with parameters $\Phi$ and the graphs $\mathcal{G}$ as follows:

\begin{equation}
\hat{\mathbf{X}}_{t}, \hat{\mathbf{X}}_{t+1}, \ldots, \hat{\mathbf{X}}_{t+\tau-1} = \mathcal{F}(\mathbf{X}_{t-T}, \ldots, \mathbf{X}_{t-1} ; \mathcal{G} ; \Phi)
\label{eq:1}
\end{equation}
\subsection{Time Series Feature Extraction}
%The feature extraction module uses an inception style convolutional network to extract temporal features from the time series. Unlike previous methods which extract one feature vector per variable, our method extracts one feature vector per timestep in the window.

%The feature extraction module contains three branches.  Each branch has a 1D convolutional layer with a kernel size of 1. Two of these branches have a second 1D convolutional layer with a kernel size of 3 and 5 and dilation of 3 and 5 respectively. The results of each branch are then concatenated and filtered with a fully connected layer to obtain the temporal features.  
%Draw figure?

% Figure environment removed

%\textcolor{red}{Alternative Description with maths}

Unlike previous methods which extract one feature vector per variable in the multivariate input, our method extracts one feature vector per time step in each window $k$ of length $T$.
More specifically, temporal sub-patterns are learned using stacked dilated convolutions, similar to the main blocks of the inception architecture \cite{lin2013network}.

Given the sliding windows $\mathbf{S} = \{\mathbf{X}_{t-T+k-K}, \ldots, \mathbf{X}_{t+k-K-1}\}_{k=1}^K$, we perform the following convolutional operations to extract three feature maps $\mathbf{f}_0^k$, $\mathbf{f}_1^k$, $\mathbf{f}_2^k$, per window $\mathbf{S}^k$. 
Let $\mathbf{f}_i^k \in \mathbb{R}^{T \times d}$ for hidden dimension $d$ of the convolutional kernels, such that:
\begin{equation}
    \begin{gathered}
    \mathbf{f}_0^k = \mathbf{S}^k \ast \mathbf{C}_0^{1,1} +\mathbf{b}_{01} \\
    \mathbf{f}_1^k = (\mathbf{S}^k \ast \mathbf{C}_1^{1,1} +\mathbf{b}_{11}) \ast \mathbf{C}_2^{3,3} + \mathbf{b}_{23} \\
    \mathbf{f}_2^k = (\mathbf{S}^k \ast \mathbf{C}_2^{1,1} + \mathbf{b}_{21}) \ast \mathbf{C}_2^{5,5} + \mathbf{b}_{25}
    \end{gathered}
\end{equation}
where $\ast$ the convolutional operator, $\mathbf{C}_0^{1,1}$, $\mathbf{C}_1^{1,1}$, $\mathbf{C}_2^{1,1}$ convolutional kernels of size 1 and dilation rate 1, $\mathbf{C}_2^{3,3}$ a convolutional kernel of size 3 and dilation rate 3, $\mathbf{C}_2^{5,5}$ a convolutional kernel of size 5 and dilation rate 5, and $\mathbf{b}_{01}, \mathbf{b}_{11}, \mathbf{b}_{21}, \mathbf{b}_{23}, \mathbf{b}_{25}$ the corresponding bias terms.

The final representations per window $k$ are obtained using a fully connected layer on the concatenated features $\mathbf{f}_0^k, \mathbf{f}_1^k, \mathbf{f}_2^k$, \ie $\mathbf{z}^k = \text{FC}(\mathbf{f}_0^k\|\mathbf{f}_1^k\|\mathbf{f}_2^k)$, such that $\mathbf{z}^k \in \mathbb{R}^{T \times d}$.
In the next sections, we refer to each time step of the hidden representation of the feature extraction module in each window $k$ as $\mathbf{z}_i^k, \forall i \in \{1, \ldots T\}$.

\subsection{Graph Structure Learning}
%The temporal features extracted in the previous section are used as a part of link prediction. As in \cite{shang2021discrete}, the link predictor module takes pairs of feature vectors and outputs a 2 dimensional link probability for each pair. This module consists of two fully connected layers and uses the Gumbel reparameterization trick to convert the link probabilities into a binary 2 dimensional vector using annealing.

% gumbel reparameterization equation? 

%The first column of this binary vector corresponds to a kxk adjacency matrix E. To ensure that the graph is directed and acyclical, a mask is applied to the lower triangle of E. 

The set $\mathcal{G} = \{\mathcal{G}^k\}, k \in \mathbb{N}^*$ describes the collection of graph structures that are parameterized for all individual sliding window of length $T$ of the series, where $K$ defines the total number of windows.
The goal of the graph learning module is to learn each adjacency matrix $\mathbf{A}^k \in \{0,1\}^{T \times T}$ for a temporal window of observations $\mathbf{S}^k$.
Following the works of \cite{kipf2018neural,shang2021discrete}, we use the Gumbel softmax trick to sample a discrete adjacency matrix as described below.

Let $\mathbf{A}^k$ be a random variable of the matrix Bernoulli distribution parameterized by $\boldsymbol{\theta}^k \in [0,1]^{T \times T}$, so that $A_{ij}^k \sim Ber(\theta_{ij}^k)$ is independent for pairs $(i,j)$.
By applying the Gumbel reparameterization trick \cite{jang2016categorical} for enabling differentiability in sampling, we can obtain the following:
\begin{equation}
    \begin{gathered}
    A_{ij}^k = \sigma((\log(\theta_{ij}^k/(1-\theta_{ij}^k)) + (\mathbf{g}_{i,j}^1 - \mathbf{g}_{i,j}^2))/s),\\
    \mathbf{g}_{i,j}^1,\mathbf{g}_{i,j}^2 \sim Gumbel(0,1) \forall i,j
    \end{gathered}
\end{equation}
where $\mathbf{g}_{i,j}^1,\mathbf{g}_{i,j}^2$ vectors of i.i.d samples drawn from Gumbel distribution, $\sigma$ the sigmoid activation and $s$ a parameter that controls the smoothness of samples, so that the distribution converges to categorical values when $s \xrightarrow{}0$.
The link predictor is applied on each pair of extracted features $(\mathbf{z}_i^k,\mathbf{z}_j^k)$ of window $k$ and maps their similarity to a $\theta_{ij}^k \in [0,1]$ by applying fully connected layers and a sigmoid activation:
\begin{equation}
\theta_{ij}^k = \sigma\Big( \text{FC}\big(\text{FC}(\mathbf{z}_i^k\|\mathbf{z}_j^k)\big)\Big)
\end{equation}
In order to obtain directed and forward (\ie no look-back in previous time steps in the history) graph structures $\mathcal{G}$ we only learn the upper triangular part of the adjacency matrices.

\subsection{Graph Neural Network for Forecasting}
%A set of node embeddings V and edges E representing the graph G = (V,E) have now been generated for each input window. A graph neural network is used to perform message passing on these graphs, and an output module uses the resulting node embeddings to perform forecasting. 
%The graph neural network is made up a series of SageConv operators which are applied to each graph for $n$ iterations. 

%SageConv formula
%\begin{equation}
%    v’_{i} = W_1v_i + W_2 \cdot mean_{j \in N(i)} v_j 
%\end{equation}

%Each iteration produces an updated vector $V’_{1…n}$, which are combined using trainable weights into the final vector V’. From V’, we select $v’_{t}$ as input into the output module. The output module consists of two fully connected layers which reduce the vector into the final output dimension. 
%To train the model, we use the mean squared error loss along with the Adam optimizer. 

Once the collection $\mathcal{G}$ of learnable graph structures per sliding window $k$ are sampled, standard GNN architectures can be applied for capturing the node-to-node relations, \ie the temporal graph dynamics.
GraphSAGE \cite{hamilton2017inductive} was chosen as the basic building GNN block of the node embedding learning architecture. GraphSAGE can effectively generalize across different graphs with the same attributes, which is fitting for this task. 
GraphSAGE is an inductive framework that exploits node feature information and generates node embeddings (\ie $\mathbf{h}_u$ for node $u$) via a learnable function, by sampling and aggregating features from a node's local neighborhood (\ie $\mathcal{N}(u)$). 

Let $(\mathcal{V}^k, \mathcal{E}^k)$ correspond to the set of nodes and edges of the learnable graph structure for each $\mathcal{G}^k$.
The node embedding update process for each $p \in \{1, \ldots, P\}$ aggregation steps, employs the mean-based aggregator, namely convolutional, by calculating the element-wise mean of the vectors in $\{\mathbf{h}_u^{p-1}, \forall u \in \mathcal{N}(u)\}$, such that:
\begin{equation}
\mathbf{h}_u^{p} \xleftarrow{} \sigma(\mathbf{W} \cdot \text{MEAN}(\{\mathbf{h}_u^{p-1}\} \cup \{\mathbf{h}_u^{p-1} \forall u \in \mathcal{N}(u)\}))
\end{equation}
where $\mathbf{W}$ trainable weights. 
The final normalized (\ie $\mathbf{\tilde{h}}_u^{p}$) representation of the last node (\ie time step) in each forward and directed graph denoted as $\mathbf{z}_{u_T} = \mathbf{\tilde{h}}_{u_T}^p$ is passed to the output module.
The output module consists of two fully connected layers which reduce the vector into the final output dimension, so as to correspond to the forecasts $\hat{\mathbf{X}}_{t}, \hat{\mathbf{X}}_{t+1}, \ldots, \hat{\mathbf{X}}_{t+\tau-1}$.
Figure \ref{fig:model} demonstrates the several components of the proposed TimeGNN architecture, including feature extraction, graph learning, GNN and output modules for forecasting.

\subsection{Training \& Inference}
To train the parameters of Equation \ref{eq:1} for the time series point forecasting task, we use the mean absolute loss.
Let $\mathbf{\hat{X}}^i, i \in \{1,...,K\}$ denote the predicted vector values for $K$ samples then the MAE loss is defined as:
\[\text{MAE} = \frac{1}{K}\sum_{i=1}^{K}\|\mathbf{\hat{X}}^i-\mathbf{X}^i\|\]

%\textcolor{blue}{for the test set how are weights for the graphs sampled (do we take the final train? -> this is in 4.1)}

The optimized weights for the feature extraction, graph structure learning and GNN and output modules are selected based on the minimum loss during training, which is evaluated as described in the experimental setup (section \ref{sec:4.3})

\section{Experimental Evaluation}
We next describe the experimental setup, including the datasets and baselines we use for comparisons.
We also demonstrate and analyze the results obtained by employing the proposed TimeGNN architecture and the baseline models.
\subsection{Datasets}
This work was evaluated on the following multivariate time series datasets: 
%using all variables in the multivariate case and the last variable in the univariate setting: 

\noindent\textbf{Exchange-Rate} which consists of the daily exchange rates of 8 countries from 1990 to 2016, following the preprocessing of \cite{lai2018modeling}.

\noindent\textbf{Weather} that contains hourly observations of 12 climatological features over a period of four years \footnote{https://www.ncei.noaa.gov/data/local-climatological-data/}, preprocessed as in \cite{zhou2021informer}.

\noindent\textbf{Electricity-Load} which is based on the UCI Electricity Consuming Load dataset \footnote{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014} that records the electricity consumption of 370 Portuguese clients from 2011 to 2014. As in \cite{zhou2021informer}, the recordings are binned into hourly intervals over the period of 2012 to 2014 and clients reduced to 321 due to missing information. %\cite{Dua2019Dataset}

% Figure environment removed
% % Figure environment removed

\subsection{Baselines}
We consider five baseline models for comparisons for time series forecasting. 
We chose two graph methods, MTGNN\cite{wu2020connecting} and GTS\cite{shang2021discrete}, and three non-graph methods, LSTNet\cite{lai2018modeling}, LSTM\cite{hochreiter1997long}, and TCN\cite{bai2018empirical}. 
For LSTM and TCN, the size of the hidden dimension and number of layers follow TimeGNN. In this paper they are fixed to three layers with hidden dimensions of 32, 64, and 128 for the Exchange-Rate, Weather, and Electricity datasets respectively. 
In the case of MTGNN, GTS, and LSTNet, parameters were kept as close as possible to the ones mentioned in their experimental setups including training loss and optimizers. 

\subsection{Experimental Setup} \label{sec:4.3}
Each model is trained for two runs for 50 epochs and the average mean squared error (MSE), mean absolute error (MAE), and $R^2$ score on the test set are recorded. The model chosen for evaluation is either the model that performs the best on the validation set during training or the final model after 50 epochs, whichever scores better. 

The same dataloader is used for all models where the train, validation, and test splits are 0.7, 0.1, and 0.2 respectively. The data is split first and each split is scaled using the standard scalar. The dataloader uses windows of length 96 and batch size 16. The forecasting horizons tested are 1, 3, 6, and 9 time steps into the future, where the exact value of the time step is dependent on the dataset (eg, 3 time steps would correspond to 3 hours into the future for the weather dataset and 3 days into the future for the exchange dataset). In this paper, we use single-step forecasting for ease of comparison with other baseline methods. 
For training, we use the Adam optimizer with a learning rate of 0.001.

Models are trained on the Alvis cluster at National Academic Infrastructure for Supercomputing in Sweden (NAISS). 
Experiments for the Weather and Exchange datasets were conducted on an NVIDIA T4 and Electricity on an NVIDIA A40.

% Figure environment removed

\begin{table}[t]
    \caption{Exchange-Rate Multivariate.}
    \label{tab:exchange-multi}
    \centering
    \tiny
    \def\arraystretch{1.3}
    \begin{tabular}{ c|c|c|c|c|c|c|c| }
       & \textbf{Metric} & \textbf{LSTM} & \textbf{TCN} & \textbf{LSTN} & \textbf{GTS} & \textbf{MTGNN} & \textbf{TimeGNN} \\
      \hline
      \multirow{3}{*}{\rotatebox[origin=c]{90}{h=1}} 
        & mse & 0.328 ± 0.007 & 0.094 ± 0.118 & \textbf{0.004 ± 0.000} & 0.005 ± 0.001 & 0.006 ± 0.002 & 0.129 ± 0.012 \\ 
        & mae & 0.475 ± 0.033 & 0.191 ± 0.163 & \textbf{0.033 ± 0.000} & 0.041 ± 0.004 & 0.048 ± 0.011 & 0.294 ± 0.029 \\ 
        & $R^2$ & 0.424 ± 0.013 & 0.834 ± 0.208 & \textbf{0.993 ± 0.000} & 0.992 ± 0.001 & 0.990 ± 0.003 & 0.773 ± 0.022 \\ 
      \hline
      \multirow{3}{*}{\rotatebox[origin=c]{90}{h=3}} 
        & mse & 0.611 ± 0.001 & 0.063 ± 0.035 & 0.013 ± 0.003 & \textbf{0.009 ± 0.000} & 0.012 ± 0.000 & 0.368 ± 0.059 \\ 
        & mae & 0.631 ± 0.031 & 0.190 ± 0.041 & 0.078 ± 0.012 & \textbf{0.063 ± 0.000} & 0.078 ± 0.000 & 0.501 ± 0.045 \\ 
        & $R^2$ & -0.078 ± 0.002 & 0.890 ± 0.061 & 0.978 ± 0.006 & \textbf{0.984 ± 0.000} & 0.979 ± 0.000 & 0.350 ± 0.104 \\ 
      \hline
      \multirow{3}{*}{\rotatebox[origin=c]{90}{h=6}} 
        & mse & 0.877 ± 0.105 & 0.189 ± 0.221 & 0.033 ± 0.005 & \textbf{0.014 ± 0.001} & 0.024 ± 0.001 & 0.354 ± 0.031 \\ 
        & mae & 0.775 ± 0.032 & 0.290 ± 0.214 & 0.139 ± 0.008 & \textbf{0.081 ± 0.005} & 0.111 ± 0.000 & 0.453 ± 0.052 \\ 
        & $R^2$ & -0.555 ± 0.185 & 0.665 ± 0.393 & 0.942 ± 0.008 & \textbf{0.975 ± 0.002} & 0.958 ± 0.002 & 0.371 ± 0.056 \\ 
      \hline 
      \multirow{3}{*}{\rotatebox[origin=c]{90}{h=9}} 
        & mse & 0.823 ± 0.118 & 0.123 ± 0.030 & 0.030 ± 0.006 & \textbf{0.020 ± 0.001} & 0.035 ± 0.003 & 0.453 ± 0.149 \\ 
        & mae & 0.743 ± 0.080 & 0.277 ± 0.037 & 0.124 ± 0.011 & \textbf{0.096 ± 0.001} & 0.140 ± 0.008 & 0.543 ± 0.084 \\ 
        & $R^2$ & -0.465 ± 0.211 & 0.782 ± 0.053 & 0.946 ± 0.010 & \textbf{0.965 ± 0.001} & 0.938 ± 0.005 & 0.193 ± 0.265 \\   
      \hline
    \end{tabular}
\end{table}

\begin{table}[t]
     \caption{Weather Multivariate.}
        \label{tab:weather-multi}
    \centering
    \tiny
    \def\arraystretch{1.3}
    \begin{tabular}{ c|c|c|c|c|c|c|c| }
       & \textbf{Metric} & \textbf{LSTM} & \textbf{TCN} & \textbf{LSTN} & \textbf{GTS} & \textbf{MTGNN} & \textbf{TimeGNN} \\
      \hline
      \multirow{3}{*}{\rotatebox[origin=c]{90}{h=1}} 
        & mse & \textbf{0.162 ± 0.001} & 0.176 ± 0.006 & 0.193 ± 0.001 & 0.209 ± 0.003 & 0.232 ± 0.008 & 0.178 ± 0.001 \\ 
        & mae & 0.202 ± 0.003 & 0.220 ± 0.011 & 0.236 ± 0.002 & 0.213 ± 0.004 & 0.230 ± 0.002 & \textbf{0.185 ± 0.000} \\ 
        & $R^2$ & \textbf{0.824 ± 0.001} & 0.809 ± 0.006 & 0.790 ± 0.001 & 0.772 ± 0.003 & 0.748 ± 0.008 & 0.806 ± 0.001 \\ 
       \hline
       \multirow{3}{*}{\rotatebox[origin=c]{90}{h=3}} 
        & mse & \textbf{0.221 ± 0.000} & 0.232 ± 0.003 & 0.233 ± 0.001 & 0.320 ± 0.005 & 0.263 ± 0.003 & 0.234 ± 0.001 \\ 
        & mae & 0.265 ± 0.000 & 0.275 ± 0.000 & 0.285 ± 0.000 & 0.320 ± 0.001 & 0.273 ± 0.000 & \textbf{0.249 ± 0.001} \\ 
        & $R^2$ & \textbf{0.759 ± 0.000} & 0.747 ± 0.004 & 0.746 ± 0.001 & 0.651 ± 0.005 & 0.713 ± 0.003 & 0.745 ± 0.001 \\ 
       \hline
       \multirow{3}{*}{\rotatebox[origin=c]{90}{h=6}} 
        & mse & 0.268 ± 0.004 & 0.274 ± 0.002 & \textbf{0.266 ± 0.001} & 0.374 ± 0.003 & 0.301 ± 0.003 & 0.287 ± 0.002 \\ 
        & mae & 0.320 ± 0.004 & 0.323 ± 0.001 & 0.321 ± 0.000 & 0.388 ± 0.002 & 0.311 ± 0.002 & \textbf{0.297 ± 0.001} \\ 
        & $R^2$ & \textbf{0.708 ± 0.005} & 0.702 ± 0.003 & 0.711 ± 0.001 & 0.592 ± 0.003 & 0.672 ± 0.003 & 0.688 ± 0.002 \\ 
       \hline 
       \multirow{3}{*}{\rotatebox[origin=c]{90}{h=9}} 
        & mse & 0.292 ± 0.007 & 0.307 ± 0.009 & \textbf{0.288 ± 0.000} & 0.399 ± 0.002 & 0.329 ± 0.001 & 0.316 ± 0.001 \\ 
        & mae & 0.342 ± 0.003 & 0.350 ± 0.005 & 0.345 ± 0.003 & 0.420 ± 0.004 & 0.339 ± 0.004 & \textbf{0.331 ± 0.001} \\ 
        & $R^2$ & 0.682 ± 0.007 & 0.666 ± 0.010 & \textbf{0.686 ± 0.000} & 0.565 ± 0.003 & 0.642 ± 0.001 & 0.656 ± 0.001 \\  
      \hline
    \end{tabular}
\end{table}

\begin{table}[t]
     \caption{Electricity-Load Multivariate.}
        \label{tab:electricity-multi}
    \centering
    \tiny
    \def\arraystretch{1.3}
    \begin{tabular}{ c|c|c|c|c|c|c|c| }
       & \textbf{Metric} & \textbf{LSTM} & \textbf{TCN} & \textbf{LSTN} & \textbf{GTS} & \textbf{MTGNN} & \textbf{TimeGNN} \\
      \hline
      \multirow{3}{*}{\rotatebox[origin=c]{90}{h=1}}
        & mse & 0.226 ± 0.002 & 0.267 ± 0.001 & 0.064 ± 0.001 & 0.135 ± 0.002 & \textbf{0.046 ± 0.000} & 0.211 ± 0.003 \\ 
        & mae & 0.323 ± 0.000 & 0.375 ± 0.002 & 0.167 ± 0.001 & 0.246 ± 0.001 & \textbf{0.131 ± 0.000} & 0.309 ± 0.001 \\ 
        & $R^2$ & 0.754 ± 0.002 & 0.709 ± 0.001 & 0.931 ± 0.001 & 0.853 ± 0.002 & \textbf{0.950 ± 0.000} & 0.770 ± 0.003 \\ 

       \hline
       \multirow{3}{*}{\rotatebox[origin=c]{90}{h=3}} 
        & mse & 0.255 ± 0.001 & 0.329 ± 0.015 &\textbf{ 0.065 ± 0.001} & 0.303 ± 0.019 & 0.079 ± 0.001 & 0.179 ± 0.003 \\ 
        & mae & 0.339 ± 0.000 & 0.406 ± 0.013 & \textbf{0.163 ± 0.002} & 0.388 ± 0.019 & 0.171 ± 0.000 & 0.320 ± 0.002 \\ 
        & $R^2$ & 0.723 ± 0.001 & 0.642 ± 0.016 & \textbf{0.929 ± 0.001} & 0.670 ± 0.020 & 0.914 ± 0.001 & 0.829 ± 0.003 \\ 

       \hline
       \multirow{3}{*}{\rotatebox[origin=c]{90}{h=6}} 
        & mse & 0.253 ± 0.005 & 0.331 ± 0.010 & 0.125 ± 0.006 & 0.334 ± 0.000 & \textbf{0.097 ± 0.000} & 0.246 ± 0.004 \\ 
        & mae & 0.340 ± 0.006 & 0.408 ± 0.009 & 0.238 ± 0.005 & 0.413 ± 0.000 & \textbf{0.189 ± 0.001} & 0.332 ± 0.004 \\ 
        & $R^2$ & 0.726 ± 0.005 & 0.640 ± 0.011 & 0.864 ± 0.006 & 0.637 ± 0.000 & \textbf{0.895 ± 0.001} & 0.732 ± 0.004 \\ 
       \hline 
       \multirow{3}{*}{\rotatebox[origin=c]{90}{h=9}}
        & mse & 0.271 ± 0.009 & 0.349 ± 0.022 & 0.144 ± 0.013 & 0.289 ± 0.021 & \textbf{0.108 ± 0.002} & 0.258 ± 0.010 \\ 
        & mae & 0.351 ± 0.003 & 0.410 ± 0.019 & 0.251 ± 0.013 & 0.368 ± 0.020 & \textbf{0.198 ± 0.002} & 0.344 ± 0.007 \\ 
        & $R^2$ & 0.706 ± 0.010 & 0.621 ± 0.024 & 0.844 ± 0.014 & 0.686 ± 0.023 & \textbf{0.883 ± 0.002} & 0.719 ± 0.011 \\ 
      \hline
    \end{tabular}
\end{table}

\subsection{Results}

\noindent \textbf{Scalability:} We compare the inference and training times of TimeGNN with MTGNN and GTS, which also contain graph structure learning modules in Figure \ref{fig:lambdas}. 
GTS is the most computationally costly in both inference and training time due to the use of the entire training dataset for graph construction. 
In contrast, MTGNN learns static node features and is subsequently more efficient. 

In inference time, there is relatively little difference between 8 and 12 variables for all models. However, when the number of variables increases to 321, there is a noticeable jump in inference time for MTGNN and GTS as their graph sizes increase. 
TimeGNN's graph does not increase in size with the number of variables and consequently, the inference time remains constant across datasets. 
The training epoch times follow the observations in inference time. 
GTS remains the most costly followed by MTGNN and finally TimeGNN. 

%might normalize dataset size for camera ready

Since TimeGNN's graph structure relies on the window size rather than the number of variables, the cost of increasing the window size on the Electricity-Load dataset is shown in Figure \ref{fig:windows}. 
As the window size increases, so does the cost of inference and training for all models. GTS rapidly becomes more costly as window size increases. 
As the graph learning module does not interact with the window size, the increase in cost can primarily be attributed to GTS's encoder and decoder module for forecasting. Similarly, MTGNN's graph learning module does not rely on the window size. 
MTGNN's inference times do not increase as dramatically as GTS's, showing the robustness of its forecasting modules. 
TimeGNN does not show significant growth in inference and training cost as the window size increases and remains the fastest of the GNN methods examined. 
This demonstrates that the graph learning module does not become overly cumbersome as window sizes vary. 

\noindent \textbf{Forecasting Quality:}  Table \ref{tab:exchange-multi} summarizes the forecasting performance on the eight-variable Exchange-rate dataset for different horizons $h \in \{1,3,6,9\}$. 
In general, GTS has the best forecasting performance on this dataset. 
Since the Exchange-rate dataset is quite small, the use of the training data during graph construction may give GTS an advantage over the other methods. 
TimeGNN however shows signs of overfitting during training and is unable to match the other two GNNs. 

Table \ref{tab:weather-multi} contains the forecasting metrics for the twelve-variable Weather dataset. 
Here the purely recurrent methods perform the best in MSE and $R^2$ scores across all horizons. 
However, TimeGNN surpasses the recurrent methods on MAE scores, suggesting that TimeGNN is producing more significant outlier predictions than the recurrent methods. 
At the same time, TimeGNN remains competitive with the recurrent methods on the other metrics and is the best performing among the GNN methods. 

Table \ref{tab:electricity-multi} contains the forecasting metrics for the 321-variable Electricity-load dataset. 
MTGNN has in general the best forecasting performance, followed closely by LSTNet. TimeGNN falls behind the other GNN methods and underfits during training. %\textcolor{red}{This is possibly due to the simplistic architecture not being expressive enough to learn the more complicated dataset.}
This can be attributed to the large number of correlated variables in the dataset, which possibly makes the series correlations harder to be captured with time-domain graphs rather than correlation ones.
However, it is important to highlight that TimeGNN still performs better than the LSTM and TCN benchmarks on this dataset.

%implies that the representation still has merit? 

\section{Conclusion}
We have presented a novel method of representing and dynamically generating graphs from raw time series. 
While conventional methods construct graphs based on the variables, we instead construct graphs such that each time step is a node. 
We use this method in TimeGNN, a model consisting of a graph construction module and a simple GNN-based forecasting module, and examine its performance against state-of-the-art neural networks, including some that perform jointly graph learning and forecasting. 
While TimeGNN's relative performance differs between datasets, this representation is clearly able to capture and learn the underlying properties of time series. 
Additionally, it is far faster and more scalable than existing graph methods as both the number of variables and the window size increase. 
However, there are several avenues for further improvement. 
The forecasting module, purposefully kept simple in order to examine the effects of the learnable temporal representations, could be extended to a more complex architecture.
The graph learning module could also be modified to include edge weights learned on separately extracted features.

\section*{Acknowledgments}
The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) at ALVIS partially funded by the Swedish Research Council through grant agreement no. 2022-06725.

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
