@Inbook{Ricci2022,
author="Ricci, Francesco
and Rokach, Lior
and Shapira, Bracha",
editor="Ricci, Francesco
and Rokach, Lior
and Shapira, Bracha",
title="Recommender Systems: Techniques, Applications, and Challenges",
bookTitle="Recommender Systems Handbook",
year="2022",
publisher="Springer US",
address="New York, NY",
pages="1--35",
abstract="Recommender systems (RSs) are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user. In this introductory chapter, we briefly discuss basic RS ideas and concepts. Our main goal is to delineate, in a coherent and structured way, the chapters in this handbook. Additionally, we aim to help the reader navigate the rich and detailed content that this handbook offers.",
isbn="978-1-0716-2197-4",
doi="10.1007/978-1-0716-2197-4_1",
url="https://doi.org/10.1007/978-1-0716-2197-4_1"
}

@inproceedings{csr:2021,
author = {Zhang, Yongfeng and Chen, Xu and Zhang, Yi and Chen, Xianjie},
title = {CSR 2021: The 1st International Workshop on Causality in Search and Recommendation},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462817},
doi = {10.1145/3404835.3462817},
abstract = {Most of the current machine learning approaches to IR---including search and recommendation tasks---are mostly designed based on the basic idea of matching, which work from the perceptual and similarity learning perspective. This include both the learning of features from data such as representation learning, and the learning of similarity matching functions from data such as neural function learning. Though many models have been widely used in practical ranking systems such as search and recommendation, their design philosophy limits the models to the correlative signals in data. However, advancing from correlative learning to causal learning in search and recommendation is an important problem, because causal modeling can help us to think outside of the observational data for representation learning and ranking. More specially, causal learning can bring benefits to the IR community on various dimensions, including but not limited to Explainable IR models, Unbiased IR models, Fairness-aware IR models, Robust IR models and Cognitive Reasoning IR models. This workshop focuses on the research and application of causal modeling in search, recommendation and a broader scope of IR tasks. The workshop will gather both researchers and practitioners in the field for discussions, idea communications, and research promotions. It will also generate insightful debates about the recent regulations on AI Ethics, to a broader community including but not limited to IR, machine learning, AI, Data Science, and beyond. Workshop homepage is available online at https://csr21.github.io/.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2677–2680},
numpages = {4},
keywords = {causal learning, search, recommendation, counterfactual learning, causality, information retrieval},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}


@inproceedings{consequences:2022,
author = {Jeunen, Olivier and Joachims, Thorsten and Oosterhuis, Harrie and Saito, Yuta and Vasile, Flavian},
title = {CONSEQUENCES — Causality, Counterfactuals and Sequential Decision-Making for Recommender Systems},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3547409},
doi = {10.1145/3523227.3547409},
abstract = {Recommender systems are more and more often modelled as repeated decision making processes – deciding which (ranking of) items to recommend to a given user. Each decision to recommend or rank an item has a significant impact on immediate and future user responses, long-term satisfaction or engagement with the system, and possibly valuable exposure for the item provider. This interactive and interventionist view of the recommender uncovers a plethora of unanswered research questions, as it complicates the typically adopted offline evaluation or learning procedures in the field. We need an understanding of causal inference to reason about (possibly unintended) consequences of the recommender, and a notion of counterfactuals to answer common “what if”-type questions in learning and evaluation. Advances at the intersection of these fields can foster progress in effective, efficient and fair learning and evaluation from logged data. These topics have been emerging in the Recommender Systems community for a while, but we firmly believe in the value of a dedicated forum and place to learn and exchange ideas. We welcome contributions from both academia and industry and bring together a growing community of researchers and practitioners interested in sequential decision making, offline evaluation, batch policy learning, fairness in online platforms, as well as other related tasks, such as A/B testing.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {654–657},
numpages = {4},
keywords = {recommender systems, off-policy evaluation and learning, counterfactuals, fairness in rankings},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@inproceedings{li2010contextual,
  title={A contextual-bandit approach to personalized news article recommendation},
  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={661--670},
  year={2010}
}

@inproceedings{li2016collaborative,
  title={Collaborative filtering bandits},
  author={Li, Shuai and Karatzoglou, Alexandros and Gentile, Claudio},
  booktitle={Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval},
  pages={539--548},
  year={2016}
}

@Article{Smith2017,
 author = {Brent Smith and Greg Linden},
 title = {Two decades of recommender systems at Amazon.com},
 year = {2017},
 url = {https://www.amazon.science/publications/two-decades-of-recommender-systems-at-amazon-com},
 journal = {IEEE Internet Computing},
}

@article{amatriain2015recommender,
  title={Recommender systems in industry: A netflix case study},
  author={Amatriain, Xavier and Basilico, Justin},
  journal={Recommender systems handbook},
  pages={385--419},
  year={2015},
  publisher={Springer}
}

@inproceedings{10.1145/3460231.3474618,
author = {Gr\"{u}n, Andreas and Neufeld, Xenija},
title = {Challenges Experienced in Public Service Media Recommendation Systems},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474618},
doi = {10.1145/3460231.3474618},
abstract = {After multiple years of successfully applying recommendation algorithms at ZDF, a German Public Service Media provider, we have faced certain challenges in regards to the optimization of our systems and the resulting recommendations. The design and the optimization of our systems are guided by various, partially competing objectives and are, therefore, influenced by various factors. Similarly to commercial video on demand services, ZDF is interested in binding its audience by providing personalized recommendations in its streaming media service. However, more importantly, as a Public Service Media provider, we are committed to offer diverse, universal, unbiased, and transparent recommendations while following established editorial guidelines and strict privacy regulations. Additionally, we are committed to provide environmentally-friendly or green recommendations optimizing our systems for run time and power consumption. With the intent to start a public discussion, we describe the challenges that arise when optimizing Public Service Media recommendation systems towards machine learning metrics, business Key Performance Indicators, Public Service Media values, and run-time simultaneously, while aiming to keep the results transparent.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {541–544},
numpages = {4},
keywords = {green recommendations, recommender systems, Public Service Media, personalization, Key Performance Indicators},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{jeunen:2021,
author = {Jeunen, Olivier and Goethals, Bart},
title = {Pessimistic Reward Models for Off-Policy Learning in Recommendation},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474247},
doi = {10.1145/3460231.3474247},
abstract = {Methods for bandit learning from user interactions often require a model of the reward a certain context-action pair will yield – for example, the probability of a click on a recommendation. This common machine learning task is highly non-trivial, as the data-generating process for contexts and actions is often skewed by the recommender system itself. Indeed, when the deployed recommendation policy at data collection time does not pick its actions uniformly-at-random, this leads to a selection bias that can impede effective reward modelling. This in turn makes off-policy learning – the typical setup in industry – particularly challenging. In this work, we propose and validate a general pessimistic reward modelling approach for off-policy learning in recommendation. Bayesian uncertainty estimates allow us to express scepticism about our own reward model, which can in turn be used to generate a conservative decision rule. We show how it alleviates a well-known decision making phenomenon known as the Optimiser’s Curse, and draw parallels with existing work on pessimistic policy learning. Leveraging the available closed-form expressions for both the posterior mean and variance when a ridge regressor models the reward, we show how to apply pessimism effectively and efficiently to an off-policy recommendation use-case. Empirical observations in a wide range of environments show that being conservative in decision-making leads to a significant and robust increase in recommendation performance. The merits of our approach are most outspoken in realistic settings with limited logging randomisation, limited training samples, and larger action spaces.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {63–74},
numpages = {12},
keywords = {Contextual Bandits, Probabilistic Models, Offline Reinforcement Learning},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@article{jeunen:2022,
author = {Jeunen, Olivier and Goethals, Bart},
title = {Pessimistic Decision-Making for Recommender Systems},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568029},
doi = {10.1145/3568029},
abstract = {Modern recommender systems are often modelled under the sequential decision-making paradigm, where the system decides which recommendations to show in order to maximise some notion of either imminent or long-term reward. Such methods often require an explicit model of the reward a certain context-action pair will yield – for example, the probability of a click on a recommendation. This common machine learning task is highly non-trivial, as the data-generating process for contexts and actions can be skewed by the recommender system itself. Indeed, when the deployed recommendation policy at data collection time does not pick its actions uniformly-at-random, this leads to a selection bias that can impede effective reward modelling. This in turn makes off-policy learning – the typical setup in industry – particularly challenging. Existing approaches for value-based learning break down in such environments. In this work, we propose and validate a general pessimistic reward modelling approach for off-policy learning in recommendation. Bayesian uncertainty estimates allow us to express scepticism about our own reward model, which can in turn be used to generate a conservative decision rule. We show how it alleviates a well-known decision making phenomenon known as the Optimiser’s Curse, and draw parallels with existing work on pessimistic policy learning. Leveraging the available closed-form expressions for both the posterior mean and variance when a ridge regressor models the reward, we show how to apply pessimism effectively and efficiently to an off-policy recommendation use-case. Empirical observations in a wide range of simulated environments show that pessimistic decision-making leads to a significant and robust increase in recommendation performance. The merits of our approach are most outspoken in realistic settings with limited logging randomisation, limited training samples, and larger action spaces. We discuss the impact of our contributions in the context of related applications like computational advertising, and present a scope for future research based on hybrid off-/on-policy bandit learning methods for recommendation.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = {oct},
keywords = {Contextual Bandits; Offline Reinforcement Learning; Probabilistic Models}
}


@inproceedings{rendle2008online,
  title={Online-updating regularized kernel matrix factorization models for large-scale recommender systems},
  author={Rendle, Steffen and Schmidt-Thieme, Lars},
  booktitle={Proceedings of the 2008 ACM conference on Recommender systems},
  pages={251--258},
  year={2008}
}