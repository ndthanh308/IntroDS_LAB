

\section{Introduction}  \label{sec:1}
\vspace{-1.5mm}

Segmentation is a critical task in the field of medical image analysis.
Contemporary approaches mainly utilize \textit{discrete} data representations (e.g., rasterized label masks) with convolutional neural networks (CNNs) \cite{isensee2021nnu,fan2020pranet,ronneberger2015unet} or Transformers \cite{hatamizadeh2022unetr} to classify image entities in a bottom-up manner.
While undeniably effective, this paradigm is hindered by \textit{unfavorable scaling} and \textit{representation} capabilities.
Scaling to higher resolution predictions often involves increasing input size which incurs quadratic or cubic memory and computation increases.
This also introduces the risk of discretization artifacts or quality loss when transitioning between different resolutions. 
Furthermore, bottom-up prediction with popular pixel-wise losses like Cross Entropy overemphasizes per-pixel discrimination over regional or object-level understanding~\cite{zhao2019regionMIseg}.
This often leads to predictions with unrealistic object shapes and locations, especially in settings with limited annotations and out-of-distribution data.
To address these limitations, we seek to develop a novel approach that directly models object shapes instead of pixels, and employs continuous representations instead of discrete ones. 
This alternative approach is further inspired by findings from neuroscience, which emphasize the importance of contour discrimination and scale-invariant representations for recognition tasks in primates~\cite{pasupathy2015neuralprimatebasis}.


% Thus, we aim to explore an alternative paradigm that directly models object shapes instead of pixels and adopts continuous representations rather than discrete ones. 
% % The importance of boundary awareness and scale-invariant representations are further supported by recent studies in neuroscience~\cite{pasupathy2015neuralprimatebasis} 
% This approach is supported by neuroscience~\cite{pasupathy2015neuralprimatebasis} which highlights the importance of contour discrimination and scale-invariant representations within the visual cortex of primates.
% However, its spatial in- variance nature hinders the ability of modeling useful con- text among pixels (within images). Thus a main stream of subsequent effort delves into network designs for effective context aggregation, e.g., dilated convolution[80, 8, 9], spa- tial pyramid pooling[84], multi-layer feature fusion[58, 47] and neural attention [35, 24]. In addition, as the widely adopted pixel-wise cross entropy loss fundamentally lacks the spatial discrimination power, some alternative optimiza- tion criteria are proposed to explicitly
% \textit{discrete data representations} which \textit{limits scaling and representation quality}. 
% To obtain higher resolution predictions, one must accept quadratic or cubic scaling of memory and computation with an increased risk of discretization artifacts.
% % since prediction and input sizes are rigidly related. 
% % Additionally, transitions between sizes or data structures often introduce discretization artifacts which hinder performance.
% Further, bottom-up prediction often results in unconstrained and unrealistic predictions of shape and structure locations with limited boundary understanding, especially in settings with limited annotations or out-of-distribution data. 



% An approach with superior would be valuable 
% An improved approach with superior efficiency in data \& compute and robustness 
Recent advances in implicit neural representations (INRs) introduce a promising alternative to the conventional discrete paradigm.
INRs have been widely adopted in the computer vision community for shape reconstruction~\cite{Chibane2020IFNet,Park2019DeepSDF,Mescheder2018OccNet}, texture synthesis~\cite{oechsle2019texture}, and novel view synthesis~\cite{Mildenhall2020NeRFRS}.
For shape reconstruction, a function, parameterized by a neural network, $f_\theta: (\textbf{p}, \textbf{z}) \rightarrow [0, 1]$ learns to map a continuous spacial coordinate $\textbf{p}$ and a shape embedding $\textbf{z}$ to an occupancy score.
This approach enables direct modeling of object shape as the network decision boundary while providing smooth predictions at arbitrary resolutions that are invariant to input size and lack discretization artifacts. 
It also increases memory efficiency \cite{dupont2021coincompressionINR}, making it an attractive option for medical imaging analysis.
This has prompted the adoption of INRs in medical image works to learn organ templates \cite{yang2022implicitatlas}, synthesize cell shapes \cite{wiesner2022inrCellShape}, and reconstruct radiology images \cite{shen2022nerp}.
% As such, other works have utilized this for organ shape representation and cell tracking.
% $f_\theta$ is commonly parameterized by a light neural network that implicitly models the shape boundary via an iso-surface.
% This formulation enables direct object shape modeling, smooth predictions at arbitrary resolutions (invariant to input size) that lack discretization artifacts, and increased memory efficiency \cite{dupont2021coincompressionINR}.


Despite the promising results in other fields, the use of INRs in medical image segmentation has been limited. 
Most relevant studies utilizing INRs mainly differ in the type of information encoded into the conditional embedding $\textbf{z}$.
OSSNet \cite{Reich2021OSSNetME} encodes a global embedding from an entire volume to guide voxel-wise occupancy prediction. 
While using a single embedding facilitates shape regularization, it often leads to the loss of fine-grained boundary details.
DISSM~\cite{raju2022dissm} also adopts global shape embeddings but instead of directly prediction prediction occupancy, it generates a coarse template that is aligned to an input image.
However, this approach is not end-to-end and contains components such as non-rigid alignment and boundary refinement that operate expensively in discrete space.
To better maintain local acuity, IOSNet~\cite{Khan2022IOSNet} \& NUDF~\cite{Srensen2022NUDF} extract a separate embedding for each input coordinate by concatenating point-wise features from multi-scale CNN feature maps.
Although locally expressive, point-wise features can undermine global contextual understanding and suffer from the same unconstrained point-wise prediction problem as discrete segmentation methods.
More importantly these methods leave performance on the table by directly applying methods designed for reconstruction given the different requirements of segmentation tasks.
Also, by directly adopting pipelines designed for 3D reconstruction, these approaches do not adequantely address the needs for semantic segmentation.
For example, segmentation involves understanding of local textures as well as global context while reconstruction can be sufficiently accurate by looking at points.


To overcome these limitations, we propose SwIPE (\underline{S}egmentation \underline{w}ith \underline{I}mplicit \underline{P}atch \underline{E}mbeddings) which operates at the patch abstraction level to enable both local boundary acuity and global shape understanding.
We formulate this process into a mapping stage and a shape decoding stage while recognizing the needs of segmentation along the way. 
To ensure necessary contextual aggregation for expressive shape embeddings, we introduce a novel scale-based attention mechanism that dynamically weights features, driven by the insight that different parts of a shape necessitate different degrees of attention on global/abstract and local/fine-grained details.
We also incorporate global coordinates for added contextual understanding, which fits seamlessly into the implicit framework.
For robust patch prediction with adequate global shape regularization, we propose three mechansisms to help achieve this.
We introduce a stochastic patch overreach scheme to alleviate boundary discontinuity between patches and faciliate contextual understanding beyond a local scope. 
We also condition local patches on global embeddings. 



First, context aggregation, we need to better aggregate context.
1) We propose a new scale-based attention mechanism to dynamically weigh features driven by the intuition that different parts of a shape require varying focus on global/abstract and local/fine-grained details.
2) We inject the true global coordinates for better spacial awareness. 

Second, anatomical structures are commonly similar across images, but using a patch-based system naively does not guarantee global alignment.
We need global structure preservation.
We introduce three mechanisms to regularize global shape. 
1) stochastic training mechanism: patch overreach to improve boundary discontinuity and improve global understanding.
2) 

SwIPE is evaluated on various criteria including accuracy, data efficiency, model efficiency, and robustness to out-of-distribution data. 
In terms of accuracy, we demonstrate that SwIPE outperforms state-of-the-art segmentation methods on 2D sessile polyp segmentation and 3D abdominal organ segmentation tasks while using only 10\% of the parameters. 
Furthermore, SwIPE is reasonably robust to non-standard shapes such as pathologies, and still performs better than a competitive baseline for polyp segmentation. 
Finally, we show that SwIPE exhibits superior robustness to out-of-distribution samples across different datasets and modalities. 
These results demonstrate the effectiveness of SwIPE for medical image segmentation and its potential for practical applications.


We evaluate SwIPE through the lens of accuracy (task performance), data efficiency (accuracy when annotations are limited), model efficiency (complexity \& compute time), and robustness to out-of-distribution data (datasets \& modalities)

First, our model is more performant and efficient than SOTA segmentation methods for each task 2D sessile polyp segmentation and 3D abdominal organ segmentation. 
We do this while being model-efficient with 10x less parameters. 
Method is reasonably robust to non-standard shapes (e.g., pathologies) and still beats a very competetive baseline for polyp segmentation. 

Finally, we show that our model has superior \textbf{robustness} of our method to out-of-distribution samples across different datasets and modalities. 










implicit shapes, which enable high-resolution predictions without the need for rasterization or explicit shape representations. Implicit methods are more data-efficient and have the ability to regularize shape, which makes them a promising direction for medical image segmentation.


% These limitations are exacerbated by limited annotations and distribution shifts commonly observed in medical data.
% With the added challenge of limited annotations and common distribution shifts from data distributions
% particularly in limited-annotations or out-of-distribution data. 
% which may lead to egregious prediction errors which limits robustness to out-of-distribution data. 



For example, if one desires high-resolution predictions, which results in exponential scaling of memory and computation. 


Input image sizes are rigidly associated with 
Given the rigid relationship between input image size and 


the discrete nature paradigm presents two main limitations}. 
\textbf{(1)} Rasterized resolutions scale exponentially with respect to memory and potentially introduce discretization artifacts. 
% \textbf{(2)} mask predictions are unconstrained to plausible shapes which can limit generalizability and robustness of the model, particularly when presented with out-of-distribution data or in limited annotation settings.
\textbf{(2)} Model predictions are not constrained to plausible shapes which can lead to egregious prediction errors (see Figure \ref{}), particularly when presented with out-of-distribution data or in limited annotation settings.
% \textbf{(3)} .

To address the challenges in \textbf{(1)}, recent medical image segmentation works \cite{Khan2022IOSNet,Srensen2022NUDF} have replaced voxel-based convolutional decoders with multilayer perceptions (MLPs) that model anatomical shapes implicitly. 
These approaches are inspired by new deep implicit shape models (DISMs) \cite{Park2019DeepSDF,Mescheder2018OccNet,Chibane2020IFNet} from the computer vision community, where an MLP network is trained to map image positions (e.g., x, y, z coordinates) to a score indicating the likelihood that a position lies inside the target structure. 
These models can overcome memory constraints by using implicit representations since decoder memory is constant and independent of spatial resolution. 
They also eliminate discretization artifacts by learning a continuous mapping function (parameterized by MLPs) that predicts smoothly-varying scores from continuous image coordinates.
However, these approaches do not constrain predictions to anatomically-plausible shapes (challenge \textbf{(2)}) or leverage image context to accurately predict challenging regions, such as around ambiguous structure boundaries.

% MOVE to last section for related works!! Don't bog down on details.
To constrain shape predictions to anatomically plausible shapes, previous work has employed two approaches. The first approach involves applying an explicit constraint by spatially transforming a shape atlas to align both rigid and non-rigid poses in an input image. This has traditionally been done using statistical shape models \cite{Cremers2007SSMsurvey} in combination with atlas-based segmentation \cite{iglesias2015AtlasSurvey}, but recent methods have incorporated deep learning modules \cite{sinclair2022AtlasISTN} into this framework. The second approach involves using implicit priors through auto-encoders \cite{Oktay2017ACNN} or adversarial learning \cite{yang2017shapeadversarial}. While the first approach can be computationally expensive and non-differentiable, the second approach can be difficult to train and sensitive to optimization hyperparameters. In this paper, we propose a new method that leverages deep implicit shape models to constrain anatomically plausible shapes for medical image segmentation while also effectively leveraging image context.

% For challenge , existing methods often fall under three categories.
% First, \textbf{Statistical Shape Modeling (SSM)} aim to .
% Second, 
% Third, \textbf{Implicit shape constraints}


To our best knowledge, no work has effectively leveraged both DISMs and shape constraints for segmentation without the expensive use of explicit pose alignment in image space. 





The advantages of this framework 
However, these approaches fail to constrain shapes 
More importantly, they fail to constrain predicted masks to anatomically-plausible shapes or \textbf{(2)}. 

In this paper, we propose a new approach for constraining medical image segmentation shapes using implicit shape representations. Our method leverages the benefits of using implicit shape models, such as efficient memory usage and continuous mapping functions, while also incorporating prior shape knowledge to ensure that predictions are anatomically plausible. Our approach is designed to address the limitations of previous medical image segmentation methods, including the lack of shape constraints and the inability to generalize to out-of-distribution data or limited annotation settings. We demonstrate the effectiveness of our approach through extensive experiments on multiple medical image datasets. Our results show that our method outperforms state-of-the-art techniques in terms of both accuracy and generalizability.



Our \textbf{contributions} can be summarized below.
\vspace*{-2mm}
\begin{enumerate}
    \item We propose
    \item We evaluate
\end{enumerate}