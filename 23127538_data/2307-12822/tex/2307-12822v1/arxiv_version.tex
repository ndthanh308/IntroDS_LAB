\documentclass[11pt]{article}

\usepackage{appendix}
\usepackage{amssymb,amsmath,amsfonts,dsfont}
\usepackage{graphicx}
%\usepackage{subfig}
\usepackage[figuresright]{rotating}
\usepackage[applemac]{inputenc}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{amscd,amsthm}
\usepackage{ifthen}
\usepackage{booktabs}
\usepackage{bm}

\usepackage[skip=0pt]{caption}
% float placement
\renewcommand{\topfraction      }{0.85}
\renewcommand{\bottomfraction   }{0.4}
\renewcommand{\textfraction     }{0.1}
\renewcommand{\floatpagefraction}{0.7}

\usepackage{hyperref}
\hypersetup{
	bookmarks=true,         % show bookmarks bar?
	unicode=false,          % non-Latin characters in Acrobat‚Äö√Ñ√∂‚àö√ë‚àö¬•s bookmarks
	pdftoolbar=true,        % show Acrobat‚Äö√Ñ√∂‚àö√ë‚àö¬•s toolbar?
	pdfmenubar=true,        % show Acrobat‚Äö√Ñ√∂‚àö√ë‚àö¬•s menu?
	pdffitwindow=false,     % window fit to page when opened
	pdfstartview={FitH},    % fits the width of the page to the window
	pdfauthor={Anselm Krainovic},     % author
	pdfsubject={Subject},   % subject of the document
	pdfcreator={Anselm Krainovic},   % creator of the document
	pdfproducer={Producer}, %!TEX encoding = UTF-8 Unicode: producer of the document
	pdfnewwindow=true,      % links in new window
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=red,          % color of internal links
	citecolor=green,        % color of links to bibliography
	filecolor=magenta,      % color of file links
	urlcolor=cyan           % color of external links
}
\usepackage{breakurl}

\usepackage[
backend=bibtex,
url=false,isbn=false,doi=false,eprint=false,
date=year,
hyperref=auto,
style=alphabetic,
natbib=true,
sorting=nty,
firstinits=true,
maxnames=2, maxbibnames=10,
]{biblatex}

\bibliography{bibliography.bib} %do not use a soce in the name of the bib file

\AtEveryBibitem{%
  \clearname{translator}%
  \clearlist{publisher}%
  \clearfield{pagetotal}%
  \clearname{editor}
  \clearfield{pages}
  \clearfield{series}
  \clearlist{address}
  \clearfield{address}
  \clearname{address}
} 

\usepackage{tikz}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots} 
\usetikzlibrary{pgfplots.groupplots}
\usetikzlibrary{matrix,arrows,decorations.pathmorphing}
\usepackage{pgfplots,pgfplotstable}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{shadows,arrows}
\usetikzlibrary{positioning}

\usepackage{changepage} %\adjustwidth environment

\usetikzlibrary {arrows.meta} %to customize arrow tips
\usetikzlibrary{fit} %to fit rectangles around nodes

\usepgfplotslibrary{colorbrewer}
\pgfplotsset{compat=newest}
%\usetikzlibrary{external}

\usetikzlibrary{calc}
\usetikzlibrary{positioning}
%\tikzexternalize

\usepackage{graphics}
\usepackage{comment}
\usepackage{readarray}
%\usepackage[dvipsnames]{xcolor}         % colors
\usepackage{caption} % to use \captionof
\usepackage{changepage}

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{wrapfig}



\newdimen\nodeDist
\usepackage{url}
\usepackage{breakurl}
\usepackage{outlines} % to make lists with items and subitems
\usepackage{readarray}
\usepackage{changepage}

\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{fact}{Fact}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{remark}{Remark}
\usepackage[vlined,ruled,linesnumbered]{algorithm2e}

%%%%%%%%%%%
\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}
\newtheorem{assumption}{Assumption}


\newcommand\bn{\mathrm{bn}}
\newcommand{\vxt}[1]{\bm{#1}}

\usepackage{comment}
\long\def\comment#1{}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
%\newcommand{\rh}[1]{{\bf{{\blue{{RH --- #1}}}}}}
%\newcommand{\js}[1]{{\bf{{\red{{JS --- #1}}}}}}
%\newcommand{\orange}[1]{\textcolor{orange}{#1}}
%\newcommand{\ak}[1]{{\bf{{\orange{{AK --- #1}}}}}}

\usepackage{tikz}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots} 
\usetikzlibrary{pgfplots.groupplots}
\usetikzlibrary{matrix,arrows,decorations.pathmorphing}
\usepackage{pgfplots,pgfplotstable}
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.10}

\usetikzlibrary{matrix,arrows,decorations.pathmorphing}

\usepackage{amsmath,amssymb,ifthen,color,framed,bm}

\usepackage{colortbl}
\definecolor{tblblue}{RGB}{101,124,191}
%\definecolor{tblblue}{rgb}{0.93,0.93,1.0}
\definecolor{tblred}{rgb}{1,0.93,0.93}
\definecolor{DarkBlue}{rgb}{0,0,0.7} 
\definecolor{BrickRed}{RGB}{203,65,84}

\usepackage{caption}

\newcommand\loss{\ell}
\newcommand\stepsize{\eta}
\newcommand\iter{j}
\newcommand\noise{z}
\newcommand\m{m}

\newcommand\risk{R}
\newcommand\emprisk{\hat R}

\newcommand\vtheta{{\bm \theta}}
\newcommand\valpha{\bm \alpha}
\newcommand\vvarepsilon{{\bm \xi}}


\newcommand\mSigma{\bm \Sigma}
\newcommand\mGamma{\bm \Gamma}
\newcommand\mLambda{\bm \Lambda}


\newcommand\setD{\mathcal D}
\newcommand\setY{\mathcal Y}

\newcommand\relu{\mathrm{relu}}

\newcommand\mystackrel[2]{\stackrel{\text{#1}}{#2}}
\newcommand\sign{\mathrm{sign}}

\newcommand\N{N}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\tr}{tr}


\newcommand\Jacobian[1]{\mathbf J_{#1}}
%%%% Paper specific macros

\newcommand\randnoise{z}
\newcommand\vrandnoise{\vz}

\newcommand\advnoise{\ve}
\newcommand\jitter{\vw}
\newcommand\vsignal{\vx}
\newcommand\vlatent{\vc}

%\newcommand\stdrandnoise{\xi}
\newcommand\stdrandnoise{\sigma_{z}}
\newcommand\stdjitter{\sigma_{w}}
\newcommand\advnoiselevel{\epsilon}
\newcommand\signalenergy{ \EX{\norm[2]{\vx}^2} }
\newcommand\randnoiseenergy{\EX{\norm[2]{\vx}^2} }
\newcommand\subspacerandnoiseenergy{\mathbb{E}[\| \mU^T \vz \|^2]}

\newcommand\stdlatent{\sigma_{c}}
\newcommand\varlatent{\sigma_{c}^2}

\newcommand\advNSR{\frac{\advnoiselevel^2}{\mathbb{E}[\| \vx \|^2]}}

%\usepackage[figuresright]{rotating}
%\usepackage{amsmath,amssymb,amscd,amsthm}
%\usepackage{graphicx}% Include figure files
%\usepackage{dcolumn}% Align table columns on decimal point
%\usepackage{bm}% bold math
%\usepackage{ifthen}
\usepackage{rh_defs_21}
%\usepackage{enumitem,amssymb}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{subfigure}
\usepackage{bbm}

\begin{document}


\begin{center}
	
	{\bf{\LARGE{
				Learning Provably Robust Estimators for Inverse Problems via Jittering 
	}}}
	
	\vspace*{.2in}
	
	{\large{
			\begin{tabular}{cccc}
				Anselm~Krainovic$^*$, Mahdi~Soltanolkotabi$^\dagger$, and Reinhard~Heckel$^*$
			\end{tabular}
	}}
	
	\vspace*{.05in}
	
	\begin{tabular}{c}
	$^*$Department of Computer Engineering, Technical University of Munich \\ 
    $^\dagger$Ming Hsieh Department of Electrical Engineering, University of Southern California
	\end{tabular}
	
	
	\vspace*{.1in}
	
	
	\today
	
	\vspace*{.1in}
	
\end{center}


\begin{abstract}
Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. 
% version 0: 
Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. 
Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing. 
\end{abstract}



\section{Introduction}
Deep neural networks achieve state-of-the-art performance for image reconstruction tasks including compressive sensing, super-resolution, and denoising. Due to their excellent performance, deep networks are now used in a variety of imaging technologies, for example in MRI and CT. 
However, concerns have been voiced that neural networks can be sensitive to worst-case or adversarial perturbations. Those concerns are fuelled by neural networks being sensitive to small, adversarially selected perturbations for prediction tasks such as image classification~\citep{szegedy_IntriguingPropertiesNeural_2014b}. 

Recent empirical work for image reconstruction tasks~
\citep{huang_InvestigationsRobustnessDeep_2018,antun_InstabilitiesDeepLearning_2020,genzel_SolvingInverseProblems_2022,darestani_MeasuringRobustnessDeep_2021a} found that worst-case perturbations can have a significantly larger effect on the image quality than random perturbations. 
This sensitivity to worst-case perturbations is not unique to neural networks, classical imaging methods are similarly sensitive~\citep{darestani_MeasuringRobustnessDeep_2021a}. 

This raises the question of whether networks can be designed or trained to be worst-case robust. A successful method proposed in the context of classification is adversarial training, which optimizes a robust or adversarial loss during training~\citep{madry_DeepLearningModels_2018}. However, the robust loss requires finding worst-case perturbations during training which is difficult and computationally expensive. 

In this work, we study jittering, a simple regularization technique that adds noise during training as a robustness-enhancing technique for inverse problems. It is long known that adding noise during training has regularizing effect and can be beneficial for generalization
~\citep{bishop_TrainingNoiseEquivalent_1995a,holmstrom_UsingAdditiveNoise_1992,reed_NeuralSmithingSupervised_1999}.
Prior work also studied adding noise for enhancing adversarial robustness for classification~\citep{zantedeschi_EfficientDefensesAdversarial_2017, kannan_AdversarialLogitPairing_2018, gilmerAdversarialExamplesAre2019}.
However, jittering has not been systematically studied as a robustness enhancing technique for training robust networks for inverse problems.

We consider the following signal reconstruction problem. 
Let $f \colon \reals^m \to \reals^n$ be an estimator (a neural network in practice) for a signal (often an image) $\vx \in \reals^n$ based on the measurement $\vy = \mA \vx + \vrandnoise \in \reals^m$, where $\mA$ is a measurement matrix and $\vrandnoise$ is random noise. We want to learn an estimator that has small robust risk defined as
\begin{align}
\label{eq:robustrisk}
R_{\advnoiselevel}(f) 
= 
\EX[(\vx,\vy)]{ \max_{\norm[2]{\advnoise} \leq \advnoiselevel } \norm[2]{ f(\vy + \advnoise) - \vx }^2 }. 
\end{align}
%In words, we want to learn an estimator that is worst-case robust as measured by the 
The robust risk is the expected worst-case error 
with respect to a $\ell_2$-perturbation of norm at most $\advnoiselevel$ of $f$ measured with the mean-squared error.   

\paragraph{Theoretical results.}
We start with Gaussian denoising of a signal lying in a subspace, and first 
characterize the optimal linear robust denoiser, i.e., the estimator that minimizes the robust risk $R_{\advnoiselevel}(f)$. 
While the resulting estimator is quite intuitive, proving optimality is fairly involved and relies on interesting applications of Jensen's inequality. 

Second, we show that the optimal linear robust estimator minimizes the Jittering-risk 
\begin{align}
\label{eq:jitteringrisk}
J_{\stdjitter}(f)
= 
\EX[(\vx,\vy),\jitter]{\norm[2]{ f(\vy+\jitter) - \vx }^2},
\end{align}
where $\jitter \sim \mc N(0,\stdjitter^2\mI)$ is Gaussian jittering noise with noise variance $\stdjitter^2$ that depends on the desired robustness level $\advnoiselevel$.
% Figure environment removed
%\begin{minipage}[t]{0.3\linewidth}
%\centering
%\label{table:student}
%\end{minipage}\hfill
%\begin{minipage}[t]{0.6\linewidth}
%\centering        
%\input{figures_onecolumn_mainbody/jitter_rob_comparison_adv_updated.tikz}
%\captionof{figure}{2-D scatterplot of the Student Database}
%\label{fig:image}
%\end{minipage}
This finding implies that instead of performing robust training via minimizing an empirical version of robust risk, we can train a denoiser via jittering, i.e., injecting Gaussian noise during training, at least for denoising a signal lying in a subspace. 
Figure~\ref{fig:one}, left panel, demonstrates the equivalence of training via minimizing a jittering risk and robust training numerically for the subspace model. It is evident that both methods of training yield an equally robust estimator. 

Moreover, we discuss extensions of our theory for linear inverse problems $\vy = \mA \vx + \vz$ and find that jittering can result in slightly suboptimal worst-case estimators for some classes of forward operators.

%%%
\paragraph{Empirical results for real-world denoising, deconvolution and compressive sensing.} 
Jittering is also effective for learning robust neural network estimators for solving inverse problems in practice. 
Figure~\ref{fig:one}, second from left to right, depicts the worst-case risk achieved by training a U-net model for denoising, compressive sensing, and deconvolution, with standard training (blue),  with jittering (purple), and with adversarial training (orange).  
For denoising, we see that jittering is as effective for obtaining a worst-case robust estimator as adversarial training, as suggested by theory. 
For compressive sensing and deconvolution, we find that jittering can be suboptimal beyond denoising, but is still effective for enhancing robustness.

Those findings make jittering a potentially attractive method for learning robust estimators in the context of inverse problems, since jittering can also be implemented easily and needs far less computational resources than adversarial training. 
Moreover, those findings imply that training on real data which often contains slight noise is somewhat robustness enhancing. 

%%%
\section{Related work}
\label{sec:related_work}

%\paragraph{Empirically characterizing worst-case robustness for imaging tasks.}
\paragraph{Empirical investigation of worst-case robustness for imaging.} 
Several works investigated the sensitivity of neural networks for image reconstruction tasks to adversarial perturbations, for limited angle tomography \citep{huang_InvestigationsRobustnessDeep_2018}, MRI and CT
\citep{antun_InstabilitiesDeepLearning_2020, genzel_SolvingInverseProblems_2022, darestani_AcceleratedMRIUnTrained_2021}, and image-to-image tasks (colorization, deblurring, denoising, and super-resolution)~\citep{choiDeepImageDestruction2022, yan_AdversariallyRobustDeep_2022b, choiEvaluatingRobustnessDeep2019}. 
Collectively, those works show that neural networks for imaging problems are significantly more sensitive to adversarial perturbations than to random perturbations, as expected. The effect of adversarial $\ell_2$-perturbations measured in mean-squared-error is roughly proportional to the energy of the perturbations in most of those problems, demonstrating that up to a constant (that might be large) neural networks can be relatively stable for imaging tasks. 
Classical reconstruction methods, in particular $\ell_1$-regularized least-squares, are similarly sensitive to adversarial perturbations~\citep{darestani_AcceleratedMRIUnTrained_2021}.
%\paragraph{Learning robust methods with robust optimization.}

\paragraph{Learning robust methods with robust optimization.}
To learn robust classifiers, \citet{madry_DeepLearningModels_2018} proposed to minimize a robust loss %, in our context an empirical version of the robust risk $R_\epsilon$, 
and to find worst-case perturbation during training with projected gradient descent.
Adversarial training can be effective for learning robust methods, but is computationally  expensive due to the cost of finding adversarial perturbations. 
A variety of heuristics exist to lower the computational cost of robust training for neural networks. % for image reconstruction problems.
For example, \citet{raj_ImprovingRobustnessDeepLearningBased_2020b} consider a compressive sensing reconstruction problem and propose to generate adversarial perturbations for training with an auxiliary network instead of solving a maximization problem.
As another example, \citet{Wong2020Fast} considers adversarial training of classifiers and propose to calculate adversarial perturbations during training by first randomly perturbing the initial point and then applying a single step of projected gradient descent.

\paragraph{Jittering for enhancing robustness in inverse problems.} The literature is somewhat split on whether  jittering is effective for enhancing worst-case robustness for imaging. 
% jittering in image reconstruction context
\citet{genzel_SolvingInverseProblems_2022} suggested that jittering can enhance worst-case robustness. 
Contrary, \citet{gandikota.advRobDeblurring.22} consider the robustness to $\ell_{\infty}$-perturbations for neural-network based deblurring and observed that the DeepWiener architecture, trained with Jittering at constant noise levels, is sensitive to adversarial perturbations.

%~\citep{dong.deepWiener.22, gandikota.advRobDeblurring.22}. blind and non-blind
%It is  questionable, however, if the amount of noise is suitable for the desired robustness level. \\
\paragraph{Robustness for inverse problems versus robustness for classification problems.}
%The following paragraphs discusses results from the robust classification literature.
Robustness in general and adding noise during training in particular, has been intensively studied in the classification setting. However, inverse problems and classification/prediction problems are very different. 
Adversarial robustness for classifiers is defined as the (average) minimal distance to the decision boundary, and random noise robustness as the minimal noise strength (for example the radius of Gaussian noise sphere) 
%such that one has sufficient probability of crossing
such that one likely crosses the decision boundary~\citep{fawziRobustnessClassifiersAdversarial2016}.
For inverse problems, there is no notion of a decision boundary. 
Therefore, results and intuitions from classification, which are often based on geometric insights on distances to surfaces (see for example \citet{fawziRobustnessClassifiersAdversarial2016} and \citet{shafahiAreAdversarialExamples2019}) do not apply to inverse problems. 


\paragraph{Jittering for enhancing robustness in classification.} 
Prior work in classification considered Gaussian data augmentation or adding Gaussian noise during training (which is conceptually very similar to jittering) as an robustness-enhancing technique and found that adding noise enhances adversarial robustness, but reported mixed results on its effectiveness. 
\citet{fawziRobustnessClassifiersAdversarial2016} proved for linear classifiers that adding Gaussian noise during training increases adversarial robustness, and   \citet{gilmerAdversarialExamplesAre2019} demonstrated that empirically adding Gaussian noise during training also increases adversarial robustness for neural networks in the context of classification. 
 \citet{rusakSimpleWayMake2020} also found Gaussian noise addition beneficial for corruption robustness (including noise, compression and weather artifacts). Furthermore,
\citet{kannan_AdversarialLogitPairing_2018} and \citet{zantedeschi_EfficientDefensesAdversarial_2017} considered adding Gaussian noise during training together with other regularization methods and report that adding noise at a fixed noise level alone yields a noticeable increase in robustness. 
Contrary, \citet{carliniMagNetEfficientDefenses2017} reevaluated the methods proposed by \citet{zantedeschi_EfficientDefensesAdversarial_2017} and reported that the robustness gains are small compared to adversarial training.

\paragraph{Randomized smoothing.}
Randomized smoothing is a very successful technique for obtaining robust classifiers~\citep{cohen_CertifiedAdversarialRobustness_2019, carliniCertifiedAdversarialRobustness2022a}, and is based on constructing a smoothened classifier from a base classifier by averaging the base classifier's outputs under Gaussian noise perturbation. The smoothed classifier is provably robust within a specified radii, without making any restrictions on the base classifier. 
Despite similarities at first sight, randomized smoothing considers surrogate smoothed models, whereas jittering is a training technique (see the appendix on a detailed discussion).

\section{Theory for robust reconstruction of a signal lying in a subspace}
\label{sec:robust_denoising_subspace}
In this section, we characterize the optimal robust estimator for denoising a signal in a subspace. While the resulting estimator is quite intuitive, proving optimality is fairly involved and relies on interesting applications of Jensen's inequality. We then show that the optimal robust estimator is also the unique minimizer of the jittering loss. 
Finally, we conjecture a precise characterization of optimal estimators for linear inverse problems beyond denoising, and show that jittering can result in suboptimal estimators for linear inverse problems beyond denoising.


\subsection{Problem setup}

We consider a signal reconstruction problem, where the goal is to estimate a signal $\vx$ based on a noisy measurement $\vy = \mA \vx + \vrandnoise$, where $\vrandnoise \sim \mc N(0, \stdrandnoise^2 1/m \mI)$ is Gaussian noise and $\mA \in \reals^{m \times n}$ a measurement or forward operator. The random noise is scaled so that the expected noise energy is $\EX{\norm[2]{\vrandnoise}^2} = \stdrandnoise^2$.
The random noise is denoted by $\vrandnoise$, to distinguish it from the adversarial noise or worst-case error, denoted by $\advnoise$. 
We assume that the signal is (approximately) chosen uniformly from the intersection of a sphere and a subspace. Specifically, the signal is generated as $\vx = \mU \vc$, where $\vc \sim \mc N(0, \stdlatent^2 1/d \mI)$ is Gaussian and $\mU \in \reals^{n\times d}$ is an orthonormal basis for a $d$-dimensional subspace of $\reals^n$.  The expected signal energy is $\EX{\norm[2]{\vsignal}^2} = \stdlatent^2$.

%We study the estimation of the signal from a noisy observation $\vy = \vx + \ve$, where $\ve$ is additive noise. 
We consider a linear estimator of the form $f(\vy) = \mH \vy$ for estimating the signal from the measurement. 
For the standard reconstruction problem of estimating the signal $\vx$ from the measurement $\vy$, performance is often measured in terms of the expected least-squared error. 
We are interested in robust reconstruction and consider the expected worst-case reconstruction error with respect to an $\ell_2$-perturbation, defined in equation~\eqref{eq:robustrisk}, and given by
\begin{align*}
R_{\advnoiselevel}(f) 
&= \EX[(\vx,\vy)]{ 
%E_\epsilon(f(\vy),\vx) 
\max_{\norm[2]{\advnoise} \leq \advnoiselevel} \norm[2]{\mH(\vy + \advnoise) - \vx}^2
}.
\end{align*}
For $\advnoiselevel=0$, the robust risk reduces to the standard expected mean-squared error.


%%%
\subsection{Denoising}

We start with denoising where the forward map is the identity, i.e., $\mA = \mI$. 
The following theorem characterizes the optimal worst-case robust denoiser.
\begin{theorem}
    %The conjecture ~\ref{conj:concentration_robust} holds true for linear denoising ($\mA = \mI)$. 
    For $d\to \infty$, the optimal worst-case estimator, i.e., the estimator minimizing the worst-case risk $R_{\advnoiselevel }(f)$ amongst all estimators of the form $f(\vy) = \mH \vy$ with $\mH$ symmetric is $\mH = \alpha \mU \transp{\mU}$, where
    \begin{align*}
        \alpha 
        = 
        \begin{dcases} 
        \frac{\varlatent - \frac{\advnoiselevel \stdlatent \stdrandnoise \sqrt{\frac{d}{n}}}{\sqrt{\varlatent + \stdrandnoise^2 \frac{d}{n} - \advnoiselevel^2 }}}{\varlatent + \stdrandnoise^2 \frac{d}{n}}
        & \text{if } \stdlatent^2 > \advnoiselevel^2 \\
        0 & \text{ else. } 
        \end{dcases}
    \end{align*}
   % minimizes the robust risk $R_{\advnoiselevel }(f)$ amongst all linear estimators of the form $f(\vy) = \mH \vy$, with $\mH$ symmetric.
    \label{thm:main}
\end{theorem}



The worst-case optimal estimator projects onto the signal subspace, and then shrinks towards zero, by a factor determined by the noise variance $\stdrandnoise^2$ and the worst-case noise energy $\advnoiselevel^2$. 
We consider the asymptotic setup where $d\to \infty$ only for expositional convenience; our proof shows that the estimator in the theorem is also near optimal for finite $d$.

To understand the implications of the theorem, let us first consider the case where the worst-case perturbation is zero. Then, the optimal estimator simply projects on the signal-subspace and shrinks towards zero, by a factor of $\alpha = \frac{\varlatent}{\varlatent + \stdrandnoise^2 \frac{d}{n}}$. The larger the noise, the more shrinkage. 

% Figure environment removed

Next, consider the most interesting regime, where non-zero adversarial noise is present. 
If the adversarial noise energy is larger than the signal energy, the estimator projects onto zero. However, this is an extreme regime since the adversarial noise can cancel the signal, and no good estimate of the signal can be achieved.

For the more practical regime where the adversarial noise energy is smaller than the signal energy, the theorem states that the optimal estimator projects onto the signal-subspace and shrinks towards zero---just like the optimal estimator for the noise-free case---but this time by a factor $\alpha$, that decreases in the adversarial noise energy $\advnoiselevel^2$.

%This has the following important implication: There is a duality between adversarial training and random noise. To see this, note that increasing 
The proof of Theoreom~\ref{thm:main} is in the appendix. Note that for estimator $\mH = \alpha \mU \transp{\mU}$, a worst-case perturbation can be computed in closed form for a fixed $\vy$ and $\vx$: a worst-case perturbation is the vector that points into the direction of the signal plus noise lying in the signal subspace, i.e., $\ve = \mU 
\epsilon
\frac{
(1-\alpha)\vc + \alpha\transp{\mU} \vz
}{\norm[2]{ (1-\alpha)\vc + \alpha\transp{\mU} \vz  }}$. However, for a general estimator $\mH$, the perturbation can not be written in closed form, which makes proving optimality quite challenging. Our proof relies on a characterization of the inner maximization problem as the solution to an optimization problem in one variable, and several unusual applications of Jensen's inequality. 


%%%
% Implications for Obtaining
\subsubsection{Robust denoisers via jittering}% and relations to Jacobian and $\ell_2$-regularization}
\label{sec:jitteringlinearmodel}

An important consequence of the characterization of the worst-case optimal estimator in Theorem~\ref{thm:main} is that, at least for the linear setup considered in this section, a worst-case optimal estimator can be obtained by regularization with jittering. 
%A worst-case optimal estimator can also be obtained with a first-order approximation of jittering, Jacobian regularization, and in our linear setup Jacobian regularization actually reduces to simple $\ell_2$-regularization.
%$\ell_2$-regularization, and Jacobian regularization, as we show next. 
%While those three regularization methods yield equal estimators in the linear denoising setup considered here, for non-linear denoisers jittering performs best, as shown in the experimental results section.


%\paragraph{Robust estimators via jittering.}
Recall from the introduction that regularization via jittering simply adds Gaussian noise $\jitter \sim \mc N(0,\stdjitter^2\mI)$ to the measurement during training. The jittering risk~\eqref{eq:jitteringrisk} of the estimator $f(\vy) = \mH \vy$ for denoising is
$
J_{\stdjitter}(f)
= 
\EX[(\vx,\vy),\jitter]{\norm[2]{ f(\vy+\jitter) - \vx }^2}
$. 
Choosing the variance of the jittering noise level accordingly as a function of the desired robustness level $\epsilon$ yields an optimal worst-case robust estimator by minimizing the jittering loss, as formalized by the following corollary of Theorem~\ref{thm:main}.
\begin{corollary}
\label{cor:jittering_nl_by_perturbation_level}
For $\advnoiselevel^2 < \stdlatent^2$, the symmetric linear estimator $f(\vy) = \mH \vy$ that minimizes the jittering risk $J_{\stdjitter}$ with noise level chosen as a function of the desired noise level $\epsilon$ as  
$%\begin{align}
%    \label{eq:jittering_nl_by_perturbation_level}
    \stdjitter(\advnoiselevel) = 
    \sqrt{
        \frac{
         \advnoiselevel^2 \stdrandnoise^2 \frac{d}{n} + \stdrandnoise \sqrt{\frac{d}{n}}\stdlatent \advnoiselevel \sqrt{\stdlatent^2 - \advnoiselevel^2 + \stdrandnoise^2\frac{d}{n}}
        }{
            d(\stdlatent^2 - \advnoiselevel^2)
        }
    }
$ %\end{align}
also minimizes the worst-case risk $R_{\advnoiselevel}$.
\end{corollary}
Hence, if we aim for a robustness level $\advnoiselevel < \stdlatent$,  we can simply apply training via Jittering instead of adversarial training by choosing the Jittering noise level using the explicit formula for the jittering noise level $\stdjitter(\advnoiselevel)$ in corollary~\ref{cor:jittering_nl_by_perturbation_level}.

Figure~\ref{fig:one}, left panel, shows the results of numerical simulation for adversarial training and jittering.
In the implementation we treat the linear reconstructions as neural networks with a single layer without bias and perform adversarial training and jittering for each perturbation level.
The simulations show that the robust risk performance of the models are identical, as predicted by the theory.
Details on how adversarial training is performed are in Section~\ref{sec:experiments}.

\subsubsection{Robustness accuracy trade-off}
% Alternatives: "Modelling Capacity of the subspace model" limitations of the subspace model or: scope of the subspace Model
%Before concluding the theoretical part, we discuss two consequences of the established theory.
%\paragraph{Robustness accuracy trade-off}
Another consequence of Theorem~\ref{thm:main} is an explicit robustness-accuracy trade-off: increased worst-case robustness comes at a loss of accuracy. 
In the practically relevant regime of $0 \leq \advnoiselevel^2 < \stdlatent^2$ the standard risk of the optimal worst-case estimator $f_{\alpha(\advnoiselevel)}$ is  $R_0(f_{\alpha(\advnoiselevel)}) =
    \stdlatent^2 \cdot
        %\frac{ d \frac{\stdrandnoise^2 }{ n }} { 1 + %\frac{\stdrandnoise^2}{n} - \frac{\epsilon^2}{\EX[]{\| \vx %\|^2}}}
        %& \EX[]{\| \vx \|^2} \geq \epsilon^2 \\
        \frac{\stdrandnoise^2 \frac{d}{n}}{\stdlatent^2 + \stdrandnoise^2 \frac{d}{n} - \advnoiselevel^2}$.
%\begin{align}
%    R_0(f_{\alpha(\advnoiselevel)}) =
%    \stdlatent^2 \cdot
%        %\frac{ d \frac{\stdrandnoise^2 }{ n }} { 1 + %\frac{\stdrandnoise^2}{n} - \frac{\epsilon^2}{\EX[]{\| \vx %%\|^2}}}
%        %& \EX[]{\| \vx \|^2} \geq \epsilon^2 \\
%        \frac{\stdrandnoise^2 \frac{d}{n}}{\stdlatent^2 + \stdrandnoise^2 \frac{d}{n} - \advnoiselevel^2}
%    \label{eq:standard_risk_optimal_robust_estimator}
%\end{align}
This expression yields the optimal standard error for $\advnoiselevel = 0$ and is strictly monotonically increasing in $\advnoiselevel$, hence showing the loss of accuracy when increasing robustness. 
%This trade-off is closely related to the equivalence of Cor.~\ref{cor:jittering_nl_by_perturbation_level}  and 
Robustness-accuracy tradeoffs can also be observed in other machine learning settings,
for classification and regression settings, see for example ~\citep{tsiprasRobustnessMayBe2019}. 
For linear inverse problems with applications in control, robustness accuracy-tradeoffs were recently characterized by~\citet{lee_AdversarialTradeoffsLinear_2021} and \citet{javanmard_PreciseTradeoffsAdversarial_2020b}.

%%%

\subsection{General linear inverse problems}
\label{subsection:general_linear_inverse_problems}
In the previous section, we characterized the optimal worst-case robust estimator and found that jittering yields optimal robust denoisers. 
In this section, we derive a conjecture for the worst-case optimal robust estimator for more general linear inverse problems of reconstructing a signal $\vx$ from a measurement $\vy = \mA \vx + \vz$, with a forward operator $\mA$ (with $\mA \neq \mI$ in general), and show that this estimator is in general not equal to the estimator obtained with jittering, thus jittering is in general sub-optimal. %For stating the optimal estimators, we consider the singular value decomposition $\mA \mU = \mW^T \mLambda \mV$ with singular values $\lambda_i$.

\paragraph{Optimal robust estimator.} Let $\mA \mU = \mW^T \mLambda \mV$ be the singular value decomposition of the matrix $\mA \mU$ with singular values $\lambda_i$. As formalized by Lemma~\ref{lem:minlambda} of the appendix, the robust-risk~\eqref{eq:robustrisk} of the estimator $f$ can be written as an expectation involving a minimization problem over a single variable (instead of a maximization over an $n$-dimensional variable, as in the original definition): 
\begin{align}
\label{align:robust_risk_min_lambdamainbody}
R_{\advnoiselevel}(\mH) 
&= \EX[\vv] {
    \min_{\lambda \geq \sigma_i^2} \lambda \epsilon^2 + \vv^T (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \vv
}, \quad \vv = (\mH \mA - \mI) \vx + \mH \vz.
\end{align}
Here, $\sigma_i$ are the singular values of the matrix $\mH$. 
In order to find the optimal robust estimator we wish to solve the optimization problem $\arg \min_{\mH} R_{\advnoiselevel}(\mH)$. The difficulty in solving this optimization problem is that we can't solve the minimization problem within the expectation~\eqref{align:robust_risk_min_lambdamainbody} in closed form. 
In order to prove Theorem~\ref{thm:main} for denoising (i.e., for $\mA = \mI$) we derived an upper and a matching lower bound of the risks using several unusual applications of Jensen's inequality. The proof does not generalize in a straightforward manner to the more general case where $\mA \neq \mI$. 
However, for large $d$, the random variable $\vv^T (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \vv$ concentrates around it's expectation, and thus we conjecture that for large $d$, we can exchange expectation and minimization, which yields:
    \begin{align}
    \label{eq:riskconjecture}
        R_{\advnoiselevel}(\mH) = 
         \min_{\lambda \geq \sigma_i^2} \lambda \epsilon^2 + \EX[\vv]{\vv^T (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \vv}.
    \end{align}
The expectation in the risk expression~\eqref{eq:riskconjecture} can be explicitly computed, which yields the following conjecture for the worst-case optimal estimator:
\begin{conjecture}
\label{co:worst-case}
    For $d \to \infty$ the optimal worst-case estimator, i.e.,  the estimator minimizing the worst-case risk $R_{\epsilon}(f)$ amongst all estimators of the form $f(\vy) = \mH \vy$ is $\mH = \mU \mV \diag(\sigma_i) \mW^T$ with
    \begin{equation*}
        \sigma_i = \frac{1+\lambda \lambda_i^2}{2 \lambda_i} + \frac{d}{m} \frac{\lambda}{2 \lambda_i} \frac{\sigma_z^2}{\sigma_c^2} - \sqrt{\left( \frac{1 + \lambda \lambda_i^2}{2 \lambda_i} + \frac{d}{m} \frac{\lambda}{2 \lambda_i} \frac{\sigma_z^2}{\sigma_c^2}\right)^2 - \lambda},
    \end{equation*}
    if $\lambda_i \neq 0$ and $\sigma_i = 0$ otherwise. Here, the parameter $\lambda$ is a solution of:
    \begin{align*}
        \argmin_{\lambda \geq 0} \lambda \epsilon^2 + \sum_{i=1}^d \frac{1-\lambda \lambda_i^2}{2} \frac{\sigma_c^2}{d} - \frac{\lambda}{2} \frac{\stdrandnoise^2}{m}  + \sqrt{\left( \frac{1 + \lambda \lambda_i^2}{2} \frac{\sigma_c^2}{d} + \frac{\lambda}{2} \frac{\sigma_z^2}{m} \right)^2 - \lambda \lambda_i^2 \frac{\sigma_c^4}{d^2}}.
    \end{align*}
\end{conjecture}
The optimization problem involved is convex and box-constrained and can thus be solved numerically. 
%\rh{can we write something as below?}
Besides the argument above, we confirmed our conjecture with numerical simulations. 

\paragraph{Optimal jittering estimator.} 
Unlike for denoising, for general inverse problems, the jittering-risk minimizing estimator is in general not equal to the optimal worst-case estimator, but the two estimators are often close. 
The optimal estimator minimizing the jittering risk is given as (see Appendix~\ref{sec:app_theory_general_inverse_problems}):
\begin{align}
\label{eq:jitteringest}
\mH_J(\stdjitter) = \mU \mV \diag\left( \frac{\sigma_c^2 \lambda_i}{\sigma_c^2 \lambda_i^2 + \sigma_z^2 \frac{d}{m} + \sigma_w^2 d} \right) \transp{\mW},
\end{align}    
where as before $\mA \mU = \mW^T \mLambda \mV$ is the singular value decomposition with singular values $\lambda_i$. 
While the estimator~\eqref{eq:jitteringest} has the same form as the worst-case optimal estimator in Conjecture~\ref{co:worst-case}, the diagonal matrix in the two estimators is in general slightly different. 

\paragraph{Numerical Simulation.} 
The worst-case sub-optimality of the jittering-risk optimal estimator~\eqref{eq:jitteringest} depends on the singular values of the matrix $\mA \mU$; if they are equal the jittering-risk estimator is optimal, and if they are not equal there is typically a small gap.
To illustrate the gap, we consider a forward operator $\mA$ with linearly decaying singular values $ \frac{i}{n}$, for $1 \leq i \leq n$ with signal energy $\sigma_c^2 = 1$ and noise level $\sigma_z = 0.2$.
We compare the (conjectured) optimal robust estimator specified by Conjecture~\ref{co:worst-case}  with the optimal Jittering estimator~\eqref{eq:jitteringest} at noise level $\stdjitter$, where $\stdjitter$ is optimized such that one obtains minimal robust risk $R_{\epsilon}$ at a given perturbation level $\epsilon$.
The results in Figure~\ref{fig:theory_scaling_factor_and_gap_beyond_denoising}, right panel, show a small gap in robust risk, which implies that Jittering is suboptimal for this case. 
However, simulations with varying forward operators and noise levels indicate that the gap is small relative to the robust risk of the standard estimator. Experiments on image deconvolution using U-Net presented in Section~\ref{sec:experiments} show similar results. %For practical applications, we suggest performing simulations using the expressions for linear reconstructions.


\section{Experiments}
\label{sec:experiments} 

In this section, we train standard convolutional neural-networks
with standard training, adversarial training, and jittering for three inverse problems: denoising images, image deconvolution, and compressive sensing, and study their robustness. 
We find that Jittering yields well-performing robust denoisers at a computational cost similar to standard training, which is significantly cheaper than adversarial training. 
We also find that jittering yields robust estimators for deconvolution and compressive sensing. 
This indicates that training on real data which often contains slight measurement noise is robustness enhancing. 

\subsection{Problem setup}
We start by describing the datasets, networks, and methodology.

\paragraph{Natural images.}
We consider denoising and deconvolution of natural images,  where our goal is to reconstruct an image $\vx \in \mathbb{R}^n$ from a noisy measurement $\vy = \mA \vx + \vrandnoise$, where $\vrandnoise \sim \mc N(0, \stdrandnoise^2 1/n  \mI)$ is Gaussian noise and $\mA$ a measurement matrix, which is equal to identity for denoising, and implements a convolution for deconvolution. For deconvolution we use a $8 \times 8$-sized discretization of the $2$-dimensional Gaussian normal distribution with standard deviation $2$. The kernel is visualized in Figure~\ref{fig:unet_results_all_Methods} in the appendix.
We obtain train and validation datasets $\{ (\vx_1, \vy_1),\ldots,(\vx_N,\vy_N) \}$ of sizes $34$k and $4$k, respectively, from colorized images of size $n = 128 \cdot 128 \cdot 3$ generated by randomly cropping and  flipping ImageNet images. The methods are tested on $2$k original-sized images.

\paragraph{Medical data.}
We also perform experiments on accelerated singlecoil magnetic resonance imaging (MRI) data, where the goal is to reconstruct an image $\vx \in \reals^n$ from a noisy and subsampled measurement in the frequency domain $\vy = \mM \mF \vx + \vz \in \reals^{2 m}$.
We use the fastMRI singlecoil knee dataset~\citep{zbontar_FastMRIOpenDataset_2018}, which contains the images $\vx$ and fully sampled measurements ($\mM = \mI$). We process it by random subsampling at acceleration factor $4$ and obtain train, validation and test datasets with approximately $31$k, $3.5$k and $7$k slices, respectively.
While perturbations are sought in frequency domain, the inverse Fourier transform is applied to the measurements before feeding the $320 \times 320$ cropped and normalized images into the network.


% denoising method
\paragraph{Network architecture.}
We use the U-net architecture~\citep{ronneberger_UNetConvolutionalNetworks_2015} since it gives excellent performance for denoising~\citep{brooks_UnprocessingImagesLearned_2019} and medical image reconstruction tasks, such as computed tomography~\citep{jin_DeepConvolutionalNeural_2017} and is used as a building block for state-of-the-art methods for magnetic resonance imaging~\citep{zbontar_FastMRIOpenDataset_2018, sriram_EndtoEndVariationalNetworks_2020}. For natural images, we use a U-net with $3 \times 3$ padded convolutions with ReLU activation functions, $2 \times 2$ max-pooling layers for downscaling and transposed convolutions for upscaling. The network has $120$k parameters. 
%The feature channel sizes are $(8, 16, 32, 64)$ for the encoder and reversely for the decoder ($120$k learnable parameters in total).
For MRI reconstruction we use a U-Net architecture similar to ~\citet{zbontar_FastMRIOpenDataset_2018} with $3 \times 3$ padded convolutions, leaky ReLU activation function, $2 \times 2$ average pooling and transposed convolutions ($480$k learnable parameters).
We denote the U-Net by the parameterized mapping $f_\vtheta \colon \mathbb{R}^m \to \mathbb{R}^n$ in the following.

\paragraph{Evaluation.}
We evaluate networks by measuring its robustness via the empirical robust risk defined as
%\begin{equation}
$    \label{eq:emp_rob_risk}
    \hat R_\epsilon(\vtheta)
    =
    \sum_{i=1}^N \max_{\norm[2]{\advnoise_i} \leq \advnoiselevel } \norm[2]{ f_{\vtheta}(\vy_i + \advnoise_i) - \vx_i}^2$,
%\end{equation}
For evaluation, the robust empirical risk is computed over the test set. 
We assess the accuracy by computing the standard empirical risk $\hat{R}_0(\vtheta)$.\\
%, which is 
%\begin{equation}
%    \label{eq:emp_std_ridk}
%    \hat R_0(\vtheta)
%    =
%    \sum_{i=1}^N  \norm[2]{ f_{\theta}(\vy_i) - \vx_i}^2.
%\end{equation}
Computing the robust empirical risk is non-trivial since it requires finding adversarial perturbations for solving the inner maximization problem. This is explained next.
We also study the computational cost of the different methods, which we measure in terms of GPU cost and time.
%, as well as by comparing convergence rates of the different training methods.

\paragraph{Finding adversarial perturbations.}
To evaluate the empirical risk and for robust training, we need to compute adversarial perturbations $\ve = \arg \max_{\norm[2]{\advnoise} \leq \advnoiselevel } \norm[2]{ f_{\theta}(\vy + \advnoise) - \vx}^2$. We find the perturbations by running $N_a$ projected gradient ascent steps, starting with initial perturbation $\ve^0 = 0$ and iterate
\begin{align*}
    \ve^{j+1} 
    =
    \mathcal{P}_{B(0, \advnoiselevel)}\left(\ve^{j} + 2.5 \frac{\advnoiselevel}{N_a} \frac{ \Delta \ve^j}{\| \Delta \ve^{j} \|_2} \right),
    \quad \text{where} \quad 
    \Delta \ve^j
    = 
    \nabla_{\ve^j} \norm[2]{f_{\theta}(\ve^j + \vy) - \vx}^2.
\end{align*}
Here, $\mathcal{P}_{B(0, \advnoiselevel)}$ is the projection into the $\ell_2$-ball $B(0; \advnoiselevel)$ of radius $\advnoiselevel$ around the origin.
%% version end
The gradient is normalized to facilitate step size optimization with multiplier $2.5$ such that the iteration can reach and move along the boundary, as suggested by~\citet{madry_DeepLearningModels_2018}.

\paragraph{Training methods.} 
\textbf{Standard training} minimizes the standard empirical risk $\hat{R}_0$. 
\textbf{Adversarial training} minimizes the empirical robust risk $\hat{R}_{\epsilon}$. To minimize the empirical robust risk, we approximate the inner maximization, $\max_{\norm[2]{\advnoise} \leq \advnoiselevel } \norm[2]{ f_{\theta}(\vy_i + \advnoise) - \vx_i}^2$, with $\norm[2]{ f_{\theta}(\tilde \vy_i) - \vx_i}^2$, where $\tilde \vy_i$ is the adversarially perturbed measurement computed as described above.
\text{Training via jittering} minimizes
\begin{align*}
    \hat J_{\stdjitter}(\vtheta) 
    =
    \sum_{i=1}^{N} \EX[\vw \sim \mathcal{N}(0, \stdjitter^2 \mI)]{\norm[2]{f_\theta ( \vy_i + \vw ) - \vx_i }^2},
\end{align*}
where the jittering level $\stdjitter$ is chosen depending on the desired robustness level.
To approximate the expectation we draw independent jittering noise samples $\vw$ in each iteration of SGD. We treat the jittering noise level as a hyperparameter optimized using the validation dataset (shown in the appendix).

%Moreover, we consider $\ell_2$ and Jacobian regularization with the results shown in the appendix.\\
Throughout, we use PyTorch's Adam optimizer with learning rate $10^{-3}$ and batch size $50$ for natural images, and $10^{-2}$ and $1$ for MRI data. %The regularization of Jittering, $\ell_2$ and Jacobian regularization are treated as hyperparameters, with the choices shown in the appendix.
As perturbation levels, we consider values within the practically interesting regime of $\advnoiselevel^2 / \EX{\norm[2]{\mA \vx}^2} < 0.3$ for natural images and $0.03$ for MRI data. Note that for $\advnoiselevel^2 > \signalenergy$, Theorem~\ref{thm:main} predicts for denoising ($\mA = \mI$) that the optimal robust estimator is zero everywhere. Figure~\ref{fig:robust_risk_large_perturbations} in the appendix shows that for large perturbations $\epsilon$ the trained U-net denoiser also maps to zero. 
%for a larger range of parameters, and shows that for large values of the perturbation parameter $\epsilon$, the correspondingly trained U-net also maps to zero.


% Figure environment removed


\subsection{Results}
We now discuss the results of the denoising, deconvolution, and compressive sensing experiments. 

\paragraph{Robust and standard performance.}
Figure~\ref{fig:one}, 
%show that networks trained with Jittering have comparable robust risks compared to the adversarially trained ones.
shows that the standard estimator is relatively robust for Gaussian denoising and increasingly sensitive for more ill-posed problems (deconvolution and compressive sensing). The experiments further show that jittering is effective for enhancing robustness, in particular relative to the sensitivity of the standard estimator. Nevertheless, as suggested by theory, we see a gap between the robust risk of adversarial training and jittering for image deconvolution and compressive sensing. For Gaussian denoising, however, Jittering is particularly effective and yields increasingly better performing networks in terms of standard risks for larger perturbations.

\paragraph{Choice of the jittering level.}
The results are based on choosing the jittering noise levels via hyperparameter search for each task. Figure~\ref{fig:unet_results_all_Methods} shows the results for Gaussian denoising: It can be seen that the choice of noise level is impotant for minimizing the robust risk. The estimated noise levels also aligns well the theoretical prediction. Details on this and the parameter choices for deconvolution and the compressive sensing experiments are in the appendix.

\paragraph{Computational complexity.} We measured the GPU time until convergence and memory utilization of the methods and present the results in the Table~\ref{table:comp_efficiency_medium} of the appendix. Performing adversarial training is by a factor of the projected gradient ascent steps more expensive than standard training. 
Moreover, training via jittering has similar computational cost as standard training, since it solely consists of drawing and adding Gaussian noise on the training data. 

\paragraph{Visual reconstructions.}
For the linear subspace setting adversarial training and jittering are equivalent. For Gaussian denoing with a neural network, however, they perform differently. For larger perturbations jittering tends to yield smoother images than networks trained adversarially, as can be seen in the example reconstructions shown in Figure~\ref{fig:experiments_visual_examples}. This effect is particularly noticeable for the Gaussian deconvolution task. In the appendix, we show examples using smaller perturbation levels. Moreover, we discuss an approximation to jittering, Jacobian regularization, which similarly enhances robustness. It is computationally more expensive, but yields less smooth reconstructions.

% Figure environment removed

\section{Conclusion}

%We characterized the optimal $\ell_2$ robust estimators for 
In this paper, we characterized the optimal worst-case robust estimator for Gaussian subspace denoising and found that the optimal estimator can be provably learned with jittering.
%We further conjectured a characterization of the optimal robust estimator for general linear inverse problems beyond denoising and demonstrated that Jittering can yield suboptimal results.
%in a subspace model analytically and derive that jittering risk minimizers are also robust risk minimizers, when choosing the jittering noise level accordingly for the desired perturbation level.
Our results for training neural networks for Gaussian denoising of images show that jittering enables the training of neural networks that are as robust as neural networks trained adversarially, but at a fraction of the computational cost, and without the hassle of having to find adversarial perturbations.
While we demonstrated that jittering can yield suboptimal robust estimators in general, in practice, jittering is effective at improving the robustness for compressive sensing and image deconvolution. 
Moreover, our results imply that training on real data that contains slight measurements noise is robustness enhancing. 

\paragraph{Reproducability} The repository at \url{https://github.com/MLI-lab/robust_reconstructors_via_jittering} contains the code to reproduce all results in the main body of this paper.

\paragraph{Acknowledgments} A.K. and R.H. are supported by the Institute of Advanced Studies at the Technical University of
Munich, the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 456465471,
464123524, the DAAD, and the German Federal Ministry of Education and Research, and the Bavarian
State Ministry for Science and the Arts. M.S. is supported by the Packard Fellowship in Science and Engineering, a Sloan Research Fellowship in Mathematics, an NSF-CAREER under award \#1846369, DARPA FastNICS programs, and NSF-CIF awards \#1813877 and \#2008443.

% 
%\newpage
\printbibliography{}

\newpage
\appendix

\section{Proof of Theorem~\ref{thm:main}}

In the main body we stated an analytical characterization of the optimal worst-case robust denoiser. We present the proof in the following and show that the risk
\begin{align}
\label{eq:robust_training_with_noise}
\min_\mH R_\epsilon(\mH)
&=
\min_\mH \EX[\vx, \vrandnoise]{
\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{ \mH (\vx + \ve + \vrandnoise) - \vx}^2
}
\end{align}
is minimized by $\mH = \sigma \mU \transp{\mU}$ with

\begin{equation*}
\sigma 
= 
\begin{dcases} 
\frac{\varlatent - \frac{\advnoiselevel \stdlatent \stdrandnoise \sqrt{\frac{d}{n}}}{\sqrt{\varlatent + \stdrandnoise^2 \frac{d}{n} - \advnoiselevel^2 }}}{\varlatent + \stdrandnoise^2 \frac{d}{n}}
& \text{if } \stdlatent^2 > \advnoiselevel^2 \\
0 & \text{ else } 
\end{dcases}
\end{equation*}
The scaling factor $\sigma$ of optimal worst-case estimator is visualized in Figure~\ref{fig:theory_scaling_factor_and_gap_beyond_denoising}.

%% Figure environment removed

We start by proving a lower bound of the risk, which relies on a characterization of the maximization and many unexpected applications of Jensens inequality. 

We then compute the risk for $\mH = \sigma \mU \transp{\mU}$ and show that it is equivalent to the lower bound on the risk.

\subsection{Lower bounding the risk}

Towards lower bounding the risk we define, for notational convenience
\begin{align*}
\begin{bmatrix}
\mH_\parallel & \mH_0 \\
\transp{\mH}_0 & \mH_\perp \\
\end{bmatrix}
=
\begin{bmatrix}
\transp{\mU} \mH \mU & \transp{\mU} \mH \mU_\perp \\
\transp{\mU}_\perp \mH \mU & \transp{\mU}_\perp \mH \mU_\perp 
\end{bmatrix}.
\end{align*}
With 
\begin{align*}
\mH 
&= 
\begin{bmatrix}
\mU & \mU_\perp
\end{bmatrix}
\begin{bmatrix}
\mH_\parallel & \mH_0 \\
\transp{\mH}_0 & \mH_\perp \\
\end{bmatrix}
\begin{bmatrix}
\transp{\mU} \\ \transp{\mU}_\perp
\end{bmatrix} 
\end{align*}
we get
\begin{align*}
\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{ \mH (\vx + \ve + \vrandnoise) - \vx}^2
%
&=
\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{ \begin{bmatrix}
\mU & \mU_\perp
\end{bmatrix}
\begin{bmatrix}
\mH_\parallel & \mH_0 \\
\transp{\mH}_0 & \mH_\perp \\
\end{bmatrix}
\begin{bmatrix}
\transp{\mU} \\ \transp{\mU}_\perp
\end{bmatrix}  (\mU \vc + \ve + \vrandnoise) - \mU \vc}^2 \\
%
&=
\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{ 
\begin{bmatrix}
\mH_\parallel & \mH_0 \\
\transp{\mH}_0 & \mH_\perp \\
\end{bmatrix}
\begin{bmatrix}
\vc + \transp{\mU} \vz + \transp{\mU} \ve \\
\transp{\mU}_\perp \vz + \transp{\mU}_\perp \ve
\end{bmatrix}  
- 
\begin{bmatrix}
\vc \\
0
\end{bmatrix}
}^2
\\
&\geq
\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{ 
\begin{bmatrix}
\mH_\parallel & \mH_0 
\end{bmatrix}
\begin{bmatrix}
\vc + \transp{\mU} \vz + \transp{\mU} \ve \\
\transp{\mU}_\perp \vz + \transp{\mU}_\perp \ve
\end{bmatrix}  
- 
\vc
}^2
\\
&\geq
\max_{\norm[2]{\ve_\parallel} \leq \epsilon} \norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_{\parallel}) - \vc
+ 
\mH_0 \vz_{\perp}
}^2.
\end{align*}
Here, we defined $\ve_\parallel = \transp{\mU} \ve$ and $\vz_\parallel = \transp{\mU} \vz$ for notational convenience, and the last inequality follows by adding $\transp{\mU}_\perp \ve = 0$ as a constraint to the maximization, which gives a lower bound. 

Next, note that since $\vz=\begin{bmatrix} \vz_\parallel \\ \vz_\perp \end{bmatrix} \sim \mc N(0, \sigma_z^2/n \mI)$ is Gaussian, the vector $\begin{bmatrix} \vz_\parallel \\ -\vz_\perp \end{bmatrix}$ is equally Gaussian distributed $\mc N(0, \sigma_z^2/n \mI)$. This implies that
\begin{align*}
\EX{
\max_{\norm[2]{\ve_\parallel} \leq \epsilon} \norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_{\parallel}) - \vc
+ 
\mH_0 \vz_{\perp}
}^2
}=\EX{
\max_{\norm[2]{\ve_\parallel} \leq \epsilon} \norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_{\parallel}) - \vc
- 
\mH_0 \vz_{\perp}
}^2
}.
\end{align*}
Then
\begin{align*}
&\EX{
\max_{\norm[2]{\ve_\parallel} \leq \epsilon} \norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_{\parallel}) - \vc
+ 
\mH_0 \vz_{\perp}
}^2
} \\
%
&=
\frac{1}{2}
\EX{
\max_{\norm[2]{\ve_1} \leq \epsilon} \norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_1) - \vc
+ 
\mH_0 \vz_{\perp}
}^2
+
\max_{\norm[2]{\ve_2} \leq \epsilon} \norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_2) - \vc
-
\mH_0 \vz_{\perp}
}^2
} \\
%
&\geq
\EX{
\max_{\norm[2]{\ve_1} \leq \epsilon} 
\frac{1}{2}
\norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_1) - \vc
+ 
\mH_0 \vz_{\perp}
}^2
+
\frac{1}{2}
\norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_1) - \vc
-
\mH_0 \vz_{\perp}
}^2
} \\
%
&\geq
\EX{
\max_{\norm[2]{\ve_1} \leq \epsilon} 
\norm[2]{ 
\mH_\parallel (\vc + \vz_{\parallel} + \ve_1) - \vc
}^2
},
\end{align*}
where the last step follows from Jensens inequality. 
Thus, we have shown that
\begin{align}
\label{eq:lobonewopt}
\min_{\mH}
\EX{
\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{ \mH (\vx + \ve + \vrandnoise) - \vx}^2
}
\geq
\min_{\mH_\parallel}
\EX{
\max_{\norm[2]{\ve} \leq \epsilon} 
\norm[2]{ 
\mH_\parallel (\vc + \vz + \ve) - \vc
}^2
}.
\end{align}
For simplicity of exposition, we drop the $\parallel$-notation. Thus, with a slight abuse of notation, the vectors on the left and right hand side have different dimensions. On the left hand side, $\vz, \ve \in \reals^n$, while on the right hand side $\vz, \ve \in \reals^d$ and $\vz$ throughout has iid $\mc N(0,\sigma_z^2/n)$ entries. 

%Next, we'll apply the lemma below, proven in Section~\ref{sec:proofoflem}, for characterizing the maximization inside of the expectation:

Next, we'll apply the lemma below for characterizing the maximization inside of the expectation. 
The proof is in Section~\ref{sec:proofoflem}. Similar computations as used to prove the lemma are on page 19-20 in the paper~\citet{lee_AdversarialTradeoffsLinear_2021} for deriving robustness-accuracy trade-off bounds. 

%While the problem itself differs a variation of Lemma~\ref{lem:minlambda} within the proof to derive the trade-off bounds. \rh{this is a bit unclear; either be more specific, or remove this sentence, and add it in the appendix when we state the lemma, and explain then what the different of the lemma and the statement in the paper is.}


 \begin{lemma}
 \label{lem:minlambda}
 For any $\vz$, 
 \begin{align}
 \max_{\norm[2]{\ve} \leq \epsilon}
 \norm[2]{ \vz - \mH \ve }^2 
 = 
 \min_{\lambda \colon \lambda \geq \lambda_{\max}(\transp{\mH}\mH) }
 \lambda \epsilon^2 + \transp{\vz} \inv{ \left( \mI - \frac{1}{\lambda} \mH \transp{\mH} \right) } \vz.
 \end{align}
 \end{lemma}

Using Lemma~\ref{lem:minlambda} the term within the expectation is 
\begin{align*}
&\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{(\mH-\mI) \vc + \mH \vz + \mH \ve}^2 
\\
&= \min_{\lambda \colon \lambda \geq \lambda_{\max}(\transp{\mH}\mH) }
\lambda \epsilon^2 
+ \transp{\left( (\transp{\mH} - \mI)  \vc + \mH \vrandnoise \right)}
\inv{ \left( \mI - \frac{1}{\lambda} \mH \transp{\mH} \right) }
\left( (\transp{\mH} - \mI)  \vc + \mH \vrandnoise \right)\\
%
&=
\min_{\lambda \colon \lambda \geq \lambda_{\max}(\transp{\mH}\mH) }
\lambda \epsilon^2 
+ \transp{\vc}(\mH - \mI)
\inv{ \left( \mI - \frac{1}{\lambda} \mH \transp{\mH} \right) }
(\mH - \mI) \vc  \\
&\hspace{3.5cm}+
2 \transp{\vrandnoise}\transp{\mH}
\inv{ \left( \mI - \frac{1}{\lambda} \mH \transp{\mH} \right) }
(\mH - \mI)  \vc \\
&\hspace{3.5cm}+\transp{\vrandnoise} \transp{\mH}
\inv{ \left( \mI - \frac{1}{\lambda} \mH \transp{\mH} \right) }
\mH \vrandnoise.
\end{align*}
Now let's rewrite using the eigenvalue decomposition of the symmetric matrix $\mH_\parallel=\mV\mSigma\mV^T$. With this notation, the optimization problem on the right hand side of inequality~\eqref{eq:lobonewopt} is
\begin{align*}
\min_{\mH_\parallel}
&\EX{
\max_{\norm[2]{\ve} \leq \epsilon} 
\norm[2]{ 
\mH_\parallel (\vc + \vz + \ve) - \vc
}^2
} \\
%
&=\min_{\mV,\sigma_i}
\EX[\vc, \vrandnoise]{
\min_{\lambda \colon \lambda \geq \sigma_i^2}
\lambda \epsilon^2 
+ 
\sum_{i=1}^d \frac{(\transp{\vv}_i \vc)^2(\sigma_i-1)^2 + (\transp{\vv}_i \vrandnoise)^2 \sigma_i^2 + 2 \sigma_i (\sigma_i - 1) (\transp{\vv}_i \vrandnoise) (\transp{\vv}_i \vc)}{1-\frac{\sigma_i^2}{\lambda}}} \\
%
&=
\min_{\mV,\sigma_i}
\EX[\vc, \vrandnoise]{\min_{\lambda \colon \lambda \geq \sigma_i^2 }
\lambda \epsilon^2 + \sum_{i=1}^n \frac{\left( (\transp{\vv}_i\vc)(\sigma_i-1) + \transp{\vv}_i \vrandnoise \sigma_i \right)^2}{1-\frac{\sigma_i^2}{\lambda}}}\\
%
&\mystackrel{i}{=}
\min_{\sigma_i}
\EX[\vc, \vrandnoise]{\min_{\lambda \colon \lambda \geq \sigma_i^2 }
\lambda \epsilon^2 + \sum_{i=1}^d \frac{\left( c_i (\sigma_i-1) + z_i \sigma_i \right)^2}{1-\frac{\sigma_i^2}{\lambda}}} \\
%
&\mystackrel{ii}{=}
\min_{\sigma_i}
\EX[\vg]{\min_{\lambda \colon \lambda \geq \sigma_i^2 }
\lambda \epsilon^2 
+ 
\sum_{i=1}^d \frac{
g_i^2
\left( \frac{\sigma_c^2}{d} (\sigma_i-1)^2 + \frac{\sigma_z^2}{n} \sigma_i^2 \right)}{1-\frac{\sigma_i^2}{\lambda}}},
\end{align*}
where minimization above is over an orthonormal $\mV \in \reals^{n\times n}$ and over the singular values $\sigma_i$.
Inequality (i) follows from $\vc$ and $\vz$ having iid Gaussian entries and inequality (ii) holds since the random variables $c_i (\sigma_i-1) + z_i \sigma_i$ are iid zero-mean Gaussian with variance $\frac{\sigma_c^2}{d} (\sigma_i-1)^2 + \frac{\sigma_z^2}{n} \sigma_i^2$, and $g_i \sim \mc N(0,1)$.



First note that the function $\frac{x^2}{y}$ is convex in $(x,y)$ when $y>0$. Also the extended value function is increasing in the first input and decreasing in the second input. Furthermore the mappings $(x,z)\mapsto x-1$ and $(x,z)\mapsto 1-\frac{x^2}{z}$ are convex and concave. Thus by the composition rule of convex functions we conclude that the functions
\begin{align*}
\frac{x^2}{1-\frac{(x-1)^2}{z}}
\quad \text{and}
\frac{(x-1)^2}{1-\frac{(x-1)^2}{z}}
\end{align*}
are jointly convex in $(x,z)$. 
%Thus the function
%\begin{align*}
%z\epsilon^2+\frac{(x-1)^2}{1-\frac{x^2}{z}}
%\end{align*}
%is also jointly convex in $(x,z)$. 

Jensen's inequality states that for a convex function $\psi$ we have $
\frac{\sum_i z_i^2 \psi(x_i)}{\sum_i z_i^2} 
\geq
\psi \left(\frac{\sum_i z_i^2 \vx_i}{\sum_{i} z_i^2} \right)
$. 
%Thus by Jensen's inequality the sum in the expectation in the left-hand-side of equation~\eqref{eq:lowerbnew} can be lower-bounded as
Thus by Jensen's inequality the sum in the expectation in the right-hand-side of the equation above can be lower-bounded as
\begin{align*}
\sum_{i=1}^d \frac{
g_i^2 \frac{\sigma_c^2}{d} (\sigma_i-1)^2 
+ 
g_i^2 \frac{\sigma_z^2}{n} \sigma_i^2 }{1-\frac{\sigma_i^2}{\lambda}} 
%
&=
\norm[2]{\vg}^2 \sum_{i=1}^d \frac{
\frac{g_i^2}{\norm[2]{\vg}^2} \frac{\sigma_c^2}{d} (\sigma_i-1)^2 
+ 
\frac{g_i^2}{\norm[2]{\vg}^2} \frac{\sigma_z^2}{n} \sigma_i^2 }{1-\frac{\sigma_i^2}{\lambda}} \\
%
&\geq
\norm[2]{\vg}^2 \frac{ \frac{\sigma_c^2}{d} \left( \sum_{i=1}^d \frac{g_i^2}{\norm[2]{\vg}^2} \sigma_i-1\right)^2 
+ 
\frac{\sigma_z^2}{n} \left( \sum_{i=1}^d \frac{g_i^2}{\norm[2]{\vg}^2} \sigma_i \right)^2 }{1-\frac{ \left( \sum_{i=1}^d \frac{g_i^2}{\norm[2]{\vg}^2} \sigma_i \right)^2}{\lambda}} \\
%
&=
%
\norm[2]{\vg}^2 \frac{ \frac{\sigma_c^2}{d} \left( \bar \sigma(\vg) -1\right)^2 
+ 
\frac{\sigma_z^2}{n} \left( \bar \sigma(\vg) \right)^2 }{1-\frac{ \left( \bar \sigma(\vg) \right)^2}{\lambda}}
%
\end{align*}
where $\bar \sigma(\vg) = \sum_{i=1}^d \frac{g_i^2}{\norm[2]{\vg}^2} \sigma_i$. 


%Solving the inner maximization we get
%\begin{align}
%\min_{\sigma_i} 
% \EX[\vg]{
%\min_{\lambda \colon \lambda \geq \sigma_i^2 } \lambda\epsilon^2
%+\norm[2]{\vg}^2 \frac{1}{d} \frac{ \sigma_c^2 \left( \bar \sigma(\vg) -1\right)^2 
%+ 
%\sigma_z^2\frac{d}{n} \left( \bar \sigma(\vg) \right)^2 }{1-\frac{ \left( \bar \sigma(\vg) \right)^2}{\lambda}}
%} 
%=
%\EX{
%\left( \epsilon \bar \sigma(\vg) + \norm[2]{\vg} \frac{1}{\sqrt{d}}  \sqrt{(\bar \sigma(\vg) -1)^2 \sigma_c^2 + \bar \sigma^2(\vg) \frac{d}{n} \sigma_z^2 }  \right)^2 } \\
%\end{align}
%\paragraph{Conditioning approach:}

%%%%%%
Now consider the event $\mathcal{E} = \{ \vg \colon \norm[2]{\vg}^2 \ge (1-\delta)d\}$ which holds with probability at least $1-e^{-\frac{\delta^2}{2}d}$. On this event, we have
\begin{align*}
\lambda \epsilon^2 
+
\sum_{i=1}^d \frac{
g_i^2 \frac{\sigma_c^2}{d} (\sigma_i-1)^2 
+ 
g_i^2 \frac{\sigma_z^2}{n} \sigma_i^2 }{1-\frac{\sigma_i^2}{\lambda}} 
%
&\geq
\lambda \epsilon^2
+
(1-\delta) \frac{ \sigma_c^2 \left( \bar \sigma(\vg) -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left( \bar \sigma(\vg) \right)^2 }{1-\frac{ \left( \bar \sigma(\vg) \right)^2}{\lambda}}.
\end{align*}
Using the same argument as before the right hand side of the above inequality is jointly convex in $(\lambda,\bar{\sigma}(\vc))$. Since partial minimization of a jointly convex function preserves convexity we conclude that the function
\begin{align*}
\psi(\bar \sigma)
=
\min_{\lambda \colon \lambda \geq \sigma_i^2 }
\mathbbm{1}_{\setE} \lambda\epsilon^2
+ \mathbbm{1}_{\setE}
(1-\delta) \frac{ \sigma_c^2 \left( \bar \sigma(\vg) -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left( \bar \sigma(\vg) \right)^2 }{1-\frac{ \left( \bar \sigma(\vg) \right)^2}{\lambda}}
\end{align*}
is convex in $\bar{\sigma}$. Thus
by using convexity in terms of $\bar{\sigma}$, applying Jensen's inequality (i.e., $\EX{\psi(\bar \sigma)} \geq \psi(\EX{\bar \sigma})$) we have
\begin{align*}
&\min_{\sigma_i} 
 \EX[\vg]{
\min_{\lambda \colon \lambda \geq \sigma_i^2 }  \lambda\epsilon^2
+ \norm[2]{\vg}^2 \frac{1}{d} \frac{ \sigma_c^2 \left( \bar \sigma(\vg) -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left( \bar \sigma(\vg) \right)^2 }{1-\frac{ \left( \bar \sigma(\vg) \right)^2}{\lambda}}
} \\
%
&\geq \min_{\sigma_i} 
 \EX[\vg]{
\min_{\lambda \colon \lambda \geq \sigma_i^2 } \mathbbm{1}_{\setE} \lambda\epsilon^2
+ \mathbbm{1}_{\setE}
(1-\delta) \frac{ \sigma_c^2 \left( \bar \sigma(\vg) -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left( \bar \sigma(\vg) \right)^2 }{1-\frac{ \left( \bar \sigma(\vg) \right)^2}{\lambda}}
} \\
%
&\geq \min_{\sigma_i} 
 \EX[\vg|\setE]{
\min_{\lambda \colon \lambda \geq \sigma_i^2 }  \lambda\epsilon^2
+ 
(1-\delta) \frac{ \sigma_c^2 \left( \bar \sigma(\vg) -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left( \bar \sigma(\vg) \right)^2 }{1-\frac{ \left( \bar \sigma(\vg) \right)^2}{\lambda}}
} \\
%
&\geq 
 \min_{\sigma_i} 
 \min_{\lambda \colon \lambda \geq \sigma_i^2 } \lambda\epsilon^2
+ 
(1-\delta) \frac{ \sigma_c^2 \left( \EX{\bar \sigma(\vg)} -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left( \EX{\bar \sigma(\vg)} \right)^2 }{1-\frac{ \left( \EX{\bar \sigma(\vg)} \right)^2}{\lambda}} \\
%
&\geq 
 \min_{\sigma_i} 
 \min_{\lambda \colon \lambda \geq \sigma_i^2 } \lambda\epsilon^2
+ 
(1-\delta) \frac{ \sigma_c^2 \left( \bar \sigma -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left( \bar \sigma \right)^2 }{1-\frac{ \left( \bar \sigma \right)^2}{\lambda}}
\end{align*}
where we defined
$\bar{\sigma}=\sum_{i=1}^d \EX[\vg|\setE]{
\frac{g_i^2}{\norm[2]{\vg}^2}}
\sigma_i$. 
Putting things together, we have shown that
\begin{align*}
\min_\mH R_\epsilon(\mH) 
&\geq
\min_{\mH_\parallel}
\EX{
\max_{\norm[2]{\ve} \leq \epsilon} 
\norm[2]{ 
\mH_\parallel (\vc + \vz + \ve) - \vc
}^2
} \\
%
&=
\min_{\sigma_i}
\EX[\vg]{
\min_{\lambda \colon \lambda \geq \sigma_i^2 }
\lambda \epsilon^2 
+
\sum_{i=1}^d \frac{
g_i^2 \frac{\sigma_c^2}{d} (\sigma_i-1)^2 
+ 
g_i^2 \frac{\sigma_z^2}{n} \sigma_i^2 }{1-\frac{\sigma_i^2}{\lambda}}
} \\
%
&\geq 
 \min_{\bar \sigma} 
 \min_{\lambda \colon \lambda \geq \sigma_i^2 } \lambda\epsilon^2
+ 
(1-\delta) \frac{ \sigma_c^2 \left( \bar \sigma -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left(\bar \sigma \right)^2 }{1-\frac{ \left( \bar \sigma \right)^2}{\lambda}} \\
%
&\geq
 \min_{\bar \sigma} 
 \min_{\lambda \colon \lambda \geq \bar \sigma^2 } \lambda\epsilon^2
+ 
(1-\delta) \frac{ \sigma_c^2 \left( \bar \sigma -1\right)^2 
+ 
\sigma_z^2\frac{d}{n} \left(\bar \sigma \right)^2 }{1-\frac{ \left( \bar \sigma \right)^2}{\lambda}},
\end{align*}
where the last inequality follows from $\sigma_{\max} \geq \bar \sigma$. For $d\to \infty$, we can choose $\delta$ arbitrarily small, which yields
\begin{align}
\label{eq:optlambdasigma}
\min_\mH R_\epsilon(\mH) 
\geq
\min_{\sigma} 
 \min_{\lambda \colon \lambda \geq \sigma^2 } \lambda\epsilon^2
+ 
 \frac{ \sigma_c^2 \left( \sigma -1\right)^2 
+ 
\sigma_z^2\frac{d}{n}  \sigma^2 }{1-\frac{ \sigma^2}{\lambda}}.
\end{align}


%%%
\paragraph{Solving the optimization problem~\eqref{eq:optlambdasigma}:}

Consider the inner minimization problem in equation~\eqref{eq:optlambdasigma}, i.e., 
\begin{align*}
\min_{\lambda \colon \lambda \geq \sigma^2 }
f(\lambda) \qquad \text{ with } f(\lambda) = \lambda \epsilon^2 + \frac{c(\sigma)}{1-\frac{\sigma^2}{\lambda}},
\end{align*}
where $c(\sigma) = \sigma_c^2 \left( \sigma -1\right)^2 
+ 
\sigma_z^2\frac{d}{n}  \sigma^2$ for notational convenience. 
Since $f$ is differentiable on $(\sigma^2, \infty)$ we can calculate its critical point $\lambda^\ast$ by setting the derivative with respect to $\lambda$ to zero, which yields
\begin{align*}
\lambda^\ast
= \frac{\sqrt{c(\sigma)}}{\epsilon} \sigma + \sigma^2.
\end{align*}
From this expression, we see that the constraint $\lambda^\ast > \sigma^2$ is satisfied and by convexity of $f$ we know that $\lambda^\ast$ is the unique minimizer. Hence, we have:
\begin{align}
\label{eq:optlambdarisk}
\min_{\lambda \colon \lambda \geq \sigma^2 } f(\lambda) 
= 
f(\lambda^\ast) 
=\left( \epsilon \sigma + \sqrt{c(\sigma)} \right)^2.
\end{align}
It follows that 
\begin{align}
\label{eq:lowerboundriskminsig}
\min_\mH R_\epsilon(\mH)
\geq
\min_\sigma g(\sigma),
%
\quad
g(\sigma)
=
\left( \epsilon \sigma + \sqrt{ 
\sigma_c^2 (\sigma-1)^2 + \sigma_z^2 \frac{d}{n} \sigma^2
} \right)^2.
\end{align}
Calculating the derivative of $g(\sigma)$ and setting it to zero, we get:
\begin{align*}
2 \left( \epsilon \sigma + \sqrt{ \sigma_c^2 (\sigma-1)^2 + \sigma_z^2 \frac{d}{n} \sigma^2 } \right) 
\left( \epsilon + \frac{ 
2 \sigma_c^2 (\sigma-1) + \sigma_z^2 \frac{d}{n} 2 \sigma
}{
2\sqrt{ \sigma_c^2 (\sigma-1)^2 + \sigma_z^2 \frac{d}{n} \sigma ^2 }
} \right) = 0
\end{align*}



The left factor is non-negative, and the right factor is zero if $\sigma_c^2 > \epsilon^2$ and if
%The left factor is non-negative, and the right factor admits a root if and only if $\sigma_c^2 < \epsilon^2$ in the form of
\begin{align}
\label{eq:optsigma}
\sigma^\ast = 
%\begin{dcases} 
        \frac{\varlatent - \frac{\advnoiselevel \stdlatent \stdrandnoise \sqrt{\frac{d}{n}}}{\sqrt{\varlatent + \stdrandnoise^2 \frac{d}{n} - \advnoiselevel^2 }}}{\varlatent + \stdrandnoise^2 \frac{d}{n}}.
        %& \text{if } \stdlatent^2 > \advnoiselevel^2 \\
        %0 & \text{ else } 
        %\end{dcases}
\end{align}
If $\sigma_c^2 \leq \epsilon^2$, then the function $g(\sigma)$ is monotonically increasing on $[0, \infty)$ and hence $\sigma_* = 0$ is the minimizer.
%In the case of $\sigma_c^2 \geq \epsilon^2$ the right factor  monotonically increasing on $[0, \infty)$ and hence $\sigma_* = 0$ is the minimizer by the constraint.



%%%
\subsection{Upper bound for the risk of the estimator $\mH = \sigma \mU \transp{\mU}$}

We upper bound the risk of the estimator $\mH = \sigma \mU \transp{\mU}$. For $\mH = \sigma \mU \transp{\mU}$ the risk~\eqref{eq:robust_training_with_noise} becomes 
\begin{align*}
R_\epsilon(\sigma \mU \transp{\mU})
%
&=
 \EX[\vx, \vrandnoise]{
\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{ \mH (\vx + \ve + \vrandnoise) - \vx}^2
} \\
%
&=
 \EX[\vx, \vrandnoise]{
\max_{\norm[2]{\ve} \leq \epsilon} \norm[2]{ \sigma \mU \transp{\mU} (\mU\vc + \ve + \vrandnoise) - \mU \vc}^2
} \\
%
&=
 \EX[\vc, \vz]{
\max_{\norm[2]{\ve_\parallel} \leq \epsilon} \norm[2]{ (\sigma - 1) \vc + \sigma \ve_{\parallel} + \sigma \vz_{\parallel} }^2
}.
\end{align*}
With Lemma~\ref{lem:minlambda}, 
\begin{align*}
R_\epsilon(\sigma \mU \transp{\mU})
%
&=
\min_\mH \EX[\vc, \vz_\parallel]{
\min_{\lambda \colon \lambda \geq \sigma^2 }
\lambda \epsilon^2 
+ 
\transp{\left( (\sigma - 1) \vc + \sigma \vz_\parallel  \right)}
\inv{ \left( \mI - \frac{1}{\lambda} \sigma^2 \mI \right) }
\left( (\sigma - 1) \vc + \sigma \vz_\parallel  \right)
} \\
%
&=
\min_\mH \EX[\vc, \vz]{
\min_{\lambda \colon \lambda \geq \sigma^2 }
\lambda \epsilon^2 
+
\sum_{i=1}^d
\frac{
\left( (\sigma-1) c_i + \sigma z_i \right)^2
}{1 - \frac{\sigma^2}{\lambda}}
} \\
%
%
&=
\min_\mH \EX[\vg]{
\min_{\lambda \colon \lambda \geq \sigma^2 }
\lambda \epsilon^2 
+
\sum_{i=1}^d g_i^2
\frac{ (\sigma-1)^2 \frac{\sigma_c^2}{d} +  \sigma  \frac{\sigma_z^2}{n}
}{1 - \frac{\sigma^2}{\lambda}}
}.
\end{align*}
Using the optimal $\lambda^\ast$, by equation~\ref{eq:optlambdarisk}, we get
\begin{align*}
R_\epsilon(\sigma \mU \transp{\mU})
%
&=
\EX{
\left( \epsilon \sigma + \norm[2]{\vg} \frac{1}{\sqrt{d}}  \sqrt{(\sigma-1)^2 \sigma_c^2 + \sigma \frac{d}{n} \sigma_z^2 }  \right)^2 } \\
%
&=
\EX{
\left( \epsilon \sigma \right)^2
+2  \epsilon \sigma \norm[2]{\vg} \frac{1}{\sqrt{d}}  \sqrt{(\sigma-1)^2 \sigma_c^2 + \sigma \frac{d}{n} \sigma_z^2 }
+
 \norm[2]{\vg}^2 \frac{1}{d}
\left( (\sigma-1)^2 \sigma_c^2 + \sigma \frac{d}{n} \sigma_z^2 \right)
} \\
%
&\mystackrel{i}{\leq}
\left( \epsilon \sigma \right)^2
+2  \epsilon \sigma   \sqrt{(\sigma-1)^2 \sigma_c^2 + \sigma \frac{d}{n} \sigma_z^2 }
+
\left( (\sigma-1)^2 \sigma_c^2 + \sigma \frac{d}{n} \sigma_z^2 \right)
\\ 
% 
&\mystackrel{ii}{=} \left( \epsilon \sigma +  \sqrt{(\sigma-1)^2 \sigma_c^2 + \sigma \frac{d}{n} \sigma_z^2 } \right)^2,
\end{align*}
where equation (i) follows by using Jensen's inequality once again (specifically, using $\left(\EX{\norm[2]{\vg}}\right)^2 \leq \EX{\norm[2]{\vg}^2} = d$). 
Noting that~(ii) is equal to the lower bound of the risk for any symmetric $\mH$ in  equation~\eqref{eq:lowerboundriskminsig} shows that $\mH = \sigma \mU \transp{\mU}$ with the optimal parameter $\sigma^\ast$ derived above is optimal.

%%%%%

\subsection{Proof of Lemma~\ref{lem:minlambda}}
 \label{sec:proofoflem}
% \begin{lemma}
% \label{lem:minlambda}
% For any $\vz$, 
% \begin{align}
% \max_{\norm[2]{\ve} \leq \epsilon}
% \norm[2]{ \vz - \mH \ve }^2 
% = 
% \min_{\lambda \colon \lambda \geq \lambda_{\max}(\transp{\mH}\mH) }
% \lambda \epsilon^2 + \transp{\vz} \inv{ \left( \mI - \frac{1}{\lambda} \mH \transp{\mH} \right) } \vz.
% \end{align}
% \end{lemma}
 
The optimization problem 
\begin{align}
\label{eq:optproblmax}
\max_{\norm[2]{\ve} \leq \epsilon}
 \norm[2]{ \vz - \mH \ve }^2
\end{align}
can be written as  
\begin{align*}
\min_\ve - \norm[2]{ \vz - \mH \ve }^2   \text{ subject to } \epsilon^2 - \ve^T\ve \geq 0.
\end{align*}
The corresponding Lagrangian is, for $\lambda \geq 0$
\begin{align*}
L(\ve, \lambda) &= - (\vz - \mH \ve)^T(\vz - \mH \ve) - \lambda (\epsilon^2 - \ve^T\ve)\\
&= - \vz^T\vz + 2\vz^T\mH \ve - \ve^T \mH^T\mH \ve - \lambda \epsilon^2 + \lambda \ve^T\ve\\
&= \ve^T (\lambda \mI - \mH^T\mH)\ve + 2\vz^T\mH \ve - \lambda \epsilon^2 - \vz^T\vz
\end{align*}

The Lagrange Dual Function is
\begin{align*}
q(\lambda) 
&= \inf_\ve  L(\ve, \lambda).
\end{align*}
Using that $\min_\ve 2\transp{\vc}\ve + \transp{\ve}\mB \ve = - \transp{\vc}\inv{\mB} \vc$, we get, provided that $\lambda \geq \lambda_{\max}(\transp{\mH} \mH)$, 
\begin{align*}
q(\lambda) 
&= 
- \transp{\vz}\transp{\mH} \inv{ (\lambda \mI - \mH^T\mH)}\transp{\mH}\vz - \lambda \epsilon^2 - \transp{\vz}\vz \\
&=- \transp{\vz}\left(\mI + \mH(\lambda \mI - \transp{\mH}\mH)^{-1}\mH\right)\vz  - \lambda \epsilon^2 \\
%
&= - \transp{\vz} \inv{\left(\mI-\frac{1}{\lambda}\transp{\mH}\mH \right)}\vz 
- \lambda \epsilon^2
\end{align*}
where the last equality follows from the Woodburry Identity. 
% $(A + UCV)^{-1} = A^{-1} - A^{-1} U(C^{-1} + VA^{-1} U)^{-1}VA^{-1}$. 
Thus, the dual problem is
\begin{align*}
\max_{\lambda \geq 0} q(\lambda) 
&= \min_{\lambda \geq 0} -q(\lambda) \\
%
&= \min_{ \lambda \geq \lambda_{\max} (\transp{\mH}\mH)} \lambda \epsilon^2 + \vz^T\left(\mI-\frac{1}{\lambda}\mH^T\mH\right)^{-1}\vz.
\end{align*}
Even though the primal problem~\eqref{eq:optproblmax} is non-convex, strong duality holds, since the primal program is a quadratic program that is strictly feasible, see~\citet[Appendix~B.1]{boyd_ConvexOptimization_2004}. The primal problem is strictly feasible if there exists a vector $\ve$ such that $\epsilon^2 - \transp{\ve} \ve > 0$. This is trivially satisfied as long as $\epsilon > 0$.



   
\section{Additional proofs for denoising}
    We state two more proofs on optimal worst-case denoisers and jittering.

    %\subsection{Jittering yields optimal robust linear estimators}
    \subsection{Proof of Corollary~\ref{cor:jittering_nl_by_perturbation_level}}
        %This is the proof of Corollary~\ref{cor:jittering_nl_by_perturbation_level}; thus the title of the subsection should be 'Proof of .. ', and then the proof should be briefly given. 
        
        In the main text it was stated that symmetric linear estimators $f(\vy) = \mH \vy$ that minimize the jittering risk $J_{\stdjitter}$ with noise level chosen as a function of the desired noise level $\epsilon$ as  
        \begin{align*}
            \stdjitter(\advnoiselevel) = 
            \sqrt{
                \frac{
                 \advnoiselevel^2 \stdrandnoise^2 \frac{d}{n} + \stdrandnoise \sqrt{\frac{d}{n}}\stdlatent \advnoiselevel \sqrt{\stdlatent^2 - \advnoiselevel^2 + \stdrandnoise^2\frac{d}{n}}
                }{
                    d(\stdlatent^2 - \advnoiselevel^2)
                }
            }
        \end{align*}
        also minimizes the worst-case risk $R_{\advnoiselevel}$.
        
        The result follows from Theorem~\ref{thm:main}. For that let $f_{r}$ and $f_{j}$ be linear estimators minimizing the robust risk $R_{\advnoiselevel}$ and the jittering risk $J_{\stdjitter}$, respectively. By Theorem~\ref{thm:main}, the two estimators are scaled projections onto the subspace, i.e., $f_{r}(\vy) = \alpha_r \mU \transp{\mU}$ and $f_j(\vy) = \alpha_j \mU \transp{\mU}$ with
        \begin{equation*}
            \alpha_r = \frac{\varlatent - \frac{\advnoiselevel \stdlatent \stdrandnoise \sqrt{\frac{d}{n}}}{\sqrt{\varlatent - \advnoiselevel^2 + \stdrandnoise^2 \frac{d}{n}}}}{\varlatent + \stdrandnoise^2 \frac{d}{n}}
            \quad \text{ and } \quad
            \alpha_j = \frac{\varlatent}{\varlatent + \stdrandnoise^2 \frac{d}{n} + \stdjitter^2 d}. 
        \end{equation*}
        Setting $\alpha_r = \alpha_j$ and solving for the standard deviation $\stdjitter$ yields the result.

    
    %\subsection{Jacobian regularization and jittering}

\subsection{Form of perturbations for estimators $\mH = \alpha \mU \transp{\mU}$}
We noted in the main body that for estimators $\mH = \alpha \mU \transp{\mU}$ worst-case perturbations can be computed in closed form for fixed $\vy$ and $\vx$. We calculate:
%How do the perturbations look like? Assume that our estimator has the form $\mH = \alpha \mU \transp{\mU}$. Thus
\begin{align*}
\hat \ve 
%
&= \arg \max_{\norm[2]{\ve} \leq \epsilon}
\norm[2]{\mH (\vx + \vz + \ve) - \vx}^2 \\
%
&= \arg \max_{\norm[2]{\ve} \leq \epsilon}
\norm[2]{ (\mH-\mI)\mU\vc + \mU \vz + \mH \ve }^2 \\
%
&= \arg \max_{\norm[2]{\ve} \leq \epsilon}
\norm[2]{ (\alpha \mU \transp{\mU} -\mI)\mU\vc + \alpha \mU \transp{\mU} \vz +  \alpha \mU \transp{\mU} \ve }^2 \\
%
&= \arg \max_{\norm[2]{\ve'} \leq \epsilon}
\norm[2]{(\alpha - 1) \mU \vc + \alpha \mU\transp{\mU} \vz + \alpha \mU\transp{\mU} \ve') }^2 \\
%
&= \mU \arg \max_{\norm[2]{\ve'} \leq \epsilon}
\norm[2]{ (\alpha - 1) \vc + \transp{\mU}\vz + \sigma \ve' }^2 \\
%
&= 
\mU 
\epsilon
\frac{
(1-\alpha)\vc + \alpha\transp{\mU} \vz
}{\norm[2]{ (1-\alpha)\vc + \alpha\transp{\mU} \vz  }} .
\end{align*}
%Thus the perturbations are aligned with the signal. 
Thus the perturbation points into the direction of the signal plus noise lying in the signal subspace.

\section{Theory for general linear inverse problems}
\label{sec:app_theory_general_inverse_problems}

In the following, we consider linear inverse problems $\vy = \mA \vx + \vz$, with a linear forward operator $\mA \in \mathbb{R}^{m \times n}$ and noise $\vz \sim \mathcal{N}(0, \sigma_z^2/m \mI)$. For denoising $(\mA = \mI)$ we stated an explicit analytical characterization of the optimal worst-case estimator and presented a proof in the appendix. The proof, however, does not generalize in a straightforward manner to more general inverse problems. We conjecture the worst-case optimal linear estimator for large dimensions $d$ and present numerical simulation results. Moreover, we state the proof for the optimal jittering estimator, and demonstrate that it can yield sub-optimal worst-case estimators in general.

    \subsection{Optimal worst-case robust estimator}
        %Let $\mA \mU = \mW^T \mLambda \mV$ be the singular value decomposition of the matrix $\mA \mU$ with singular values $\lambda_i$.
        As formalized by Lemma~\ref{lem:minlambda}, the robust-risk~\eqref{eq:robustrisk} of the estimator $f$ can be written as an expectation involving a minimization problem over a single variable (instead of a maximization over an $n$-dimensional variable, as in the original definition): 
\begin{align}
\label{align:robust_risk_min_lambda}
R_{\advnoiselevel}(\mH) 
&= \EX[\vv] {
    \min_{\lambda \geq \sigma_i^2} \lambda \epsilon^2 + \vv^T (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \vv
}, \quad \vv = (\mH \mA - \mI) \vx + \mH \vz.
\end{align}
Here, $\sigma_i$ are the singular values of the matrix $\mH$. 
In order to find the optimal robust estimator we wish to solve the optimization problem $\arg \min_{\mH} R_{\advnoiselevel}(\mH)$. The difficulty in solving this optimization problem is that we can't solve the minimization problem within the expectation~\eqref{align:robust_risk_min_lambda} in closed form. 
In order to prove Theorem~\ref{thm:main} for denoising (i.e., for $\mA = \mI$) we derived an upper and a matching lower bound of the risks using several unusual applications of Jensen's inequality. The proof does not generalize in a straightforward manner to the more general case where $\mA \neq \mI$. 
However, for large $d$, the random variable $\vv^T (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \vv$ concentrates around it's expectation, and thus we conjecture that for large $d$, we can exchange expectation and minimization, which yields:
\begin{align}
    \label{eq:appendix_riskconjecture}
        R_{\advnoiselevel}(\mH) = 
        \min_{\lambda \geq \sigma_i^2} \lambda \epsilon^2 + \EX[\vv]{\vv^T (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \vv}.
\end{align}
Based on equation~\eqref{eq:appendix_riskconjecture} we derive a characterization of the optimal worst-case estimator. We proceed similar to the denoising case and start with rearranging the terms in expectation:
\begin{align*}
    \vv^T (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \vv 
    &= \transp{((\mH \mA - \mI) \vx + \mH \vz)} (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} ((\mH \mA - \mI) \vx + \mH \vz) \\
    &= \transp{\vc} \transp{((\mH \mA - \mI) \mU)} (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} (\mH \mA - \mI) \mU) \vc \\
    &+ 2 \transp{\vz} \transp{\mH} (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} (\mH \mA - \mI) \mU \vc
    + \transp{\vz} \transp{\mH} (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \mH \vz.
\end{align*}
Now, let $\mA \mU = \transp{\mW} \mLambda \mV$ be the singular value decomposition of the matrix $\mA \mU$ and  $\mH = \mU \transp{\mV} \mSigma \mW$ the singular value decomposition of $\mH$. We then have:
\begin{align*}
   \mH \transp{\mH} &= \mU \transp{\mV} \mSigma \mW \transp{(\mU \transp{\mV} \mSigma \mW)} 
   = \mU \transp{\mV} \mSigma \mW \transp{\mW} \mSigma \mV \transp{\mU}
   = \mU \transp{\mV} \mSigma^2 \mV \transp{\mU} \\
   \mH \mA \mU &= \mU \transp{V} \mSigma \mW \transp{\mW} \mLambda \mV = \mU \transp{\mV} \mSigma \mLambda \mV.
\end{align*}
For the individual parts in the summation it follows:
\begin{align*}
 \transp{\vc} &\transp{(\mH \mA \mU - \mU)} (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} (\mH \mA \mU - \mU) \vc \\
 &= \transp{\vc} \transp{(\mU \transp{\mV} \mSigma \mLambda \mV - \mU)} (\mI - \frac{1}{\lambda} \mU \transp{\mV} \mSigma^2 \mV \transp{\mU})^{-1} (\mU \transp{\mV} \mSigma \mLambda \mV - \mU) \vc \\
 &= \transp{\vc} \transp{(\mU \transp{\mV} \mSigma \mLambda \mV - \mU)} \mU \transp{\mV}  (\mI - \frac{1}{\lambda}  \mSigma^2)^{-1}  \mV \transp{\mU} (\mU \transp{\mV} \mSigma \mLambda \mV- \mU) \vc \\
 &= \transp{(\mV \vc)} (\mSigma \mLambda - \mI) (\mI - \frac{1}{\lambda} \mSigma^2)^{-1}  (\mSigma \mLambda -  \mI) {\mV \vc} = \sum_{i=1}^d \frac{(\sigma_i \lambda_i - 1)^2 c_i}{1 - \frac{\sigma_i^2}{\lambda}}
\end{align*}
where we define $c_i := \transp{\vv_i} \vc$ using the $i$-th row vector $\vv_i$ of the matrix $\mV$. Similarly, we set $z_i := \transp{\vw_i} \vz$, with $\vw_i$ the $i$-th row vector of $\mW$, and get for the other parts:
\begin{align*}
 &2 \transp{\vz} \transp{\mH} (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} (\mH \mA \mU - \mU) \vc 
 %&= \transp{\vz} \transp{(\mU \transp{V} \mSigma \mW)} (\mI - \frac{1}{\lambda} \mU \transp{V} \mSigma^2 \mV \transp{\mU})^{-1} (\mU \transp{V} \mSigma \mLambda \mV - \mU) \vc \\
 %&= \transp{\mW \vz} \mSigma \mV \transp{\mU} \mU \transp{V}  (\mI - \frac{1}{\lambda}  \mSigma^2)^{-1}  \mV \transp{\mU} (\mU \transp{V} \mSigma \mLambda \mV- \mU) \vc \\
 %&= \transp{\mW \vz} \mSigma (\mI - \frac{1}{\lambda} \mSigma^2)^{-1}  (\mSigma \mLambda \mI -  \mI) {\mV \vc} \\
 = \sum_{i=1}^d 2 \frac{z_i \sigma_i (\sigma_i \lambda_i - 1) c_i}{1 - \frac{\sigma_i^2}{\lambda}} \\
 &\transp{\vz} \transp{\mH} (\mI - \frac{1}{\lambda} \mH \mH^T)^{-1} \mH \vz
 %&= \transp{\vz} \transp{(\mU \transp{V} \mSigma \mW)} (\mI - \frac{1}{\lambda} \mU \transp{V} \mSigma^2 \mV \transp{\mU})^{-1} (\mU \transp{V} \mSigma \mW) \vc \\
 %&= \transp{\mW \vz} \mSigma \mV \transp{\mU} \mU \transp{V}  (\mI - \frac{1}{\lambda}  \mSigma^2)^{-1}  \mV \transp{\mU} \mU \transp{V} \mSigma \mW  \vz \\
 %&= \transp{\mW \vz} \mSigma (\mI - \frac{1}{\lambda} \mSigma^2)^{-1}  (\mSigma \mLambda \mI -  \mI) {\mW \vc} \\
 = \sum_{i=1}^d \frac{z_i^2 \sigma_i^2}{1 - \frac{\sigma_i^2}{\lambda}}.
\end{align*}
Hence, for the term in expectation in Eq.~\eqref{eq:appendix_riskconjecture} we get:
\begin{align*}
    \EX[\vv]{\transp{\vv}(\mI - \frac{1}{\lambda} \mH \transp{\mH})^{-1} \vv}
    &= \EX[\vc, \vz]{\sum_{i=1}^d \frac{\left( c_i (\sigma_i \lambda_i - 1) + z_i \sigma_i \right)^2}{1 - \frac{\sigma_i^2}{\lambda}}} \\
    &= \EX[\vg]{\sum_{i=1}^d \frac{g_i^2 \left( \frac{\sigma_i^2}{d} (\sigma_i \lambda_i - 1)^2 + \frac{\sigma_z^2}{m} \sigma_i^2\right)}{1 - \frac{\sigma_i^2}{\lambda}}},
\end{align*}
where we note that $c_i (\sigma_i \lambda_i-1) + z_i \sigma_i$ are iid zero-mean gaussian with variance $\frac{\sigma_c^2}{d} (\lambda_i \sigma_i-1)^2 + \frac{\sigma_z^2}{m} \sigma_i^2$ and $g_i \sim \mathcal{N}(0,1)$.
From this it follows, assuming the robust risk conjecture~\eqref{eq:appendix_riskconjecture}, that the optimal robust estimator minimizes:
\begin{align}
    \label{align:optimal_robust_estimator}
    \min_{\mH} R_{\advnoiselevel}(\mH) &= 
        \min_{\sigma_i, \lambda \geq \sigma_i^2} f(\lambda, \sigma_1, \dots, \sigma_n), \\
    \label{align:optimal_robust_estimator_f}
        f(\lambda, \sigma_1, \dots, \sigma_n) &= \lambda \epsilon^2 + \sum_{i=1}^d \frac{\frac{\stdrandnoise^2}{m} \sigma_i^2 + \frac{\stdlatent^2}{d}(\sigma_i \lambda_i - 1)^2}{1 - \frac{\sigma_i^2}{\lambda}}.
\end{align}
We first calculate the unconstrained minimizer of the function $f$ and get:
\begin{align}
    \frac{\partial f}{\partial \sigma_i} &= \lambda \frac{\lambda \lambda_i^2 \sigma_i \frac{\sigma_c^2}{d} - \lambda_i (\lambda + \sigma_i^2) \frac{\sigma_c^2}{d} + \sigma_i ( \frac{\sigma_c^2}{d} + \lambda \frac{\sigma_z^2}{m}) }{\lambda - \sigma_i^2}
    %\frac{\partial f}{\partial \sigma_i} \mid_{\sigma_i=\sigma_{i,\pm}^*} &= \lambda \frac{\lambda \lambda_i^2 \sigma_{i,\pm}^* \frac{\sigma_c^2}{d} - \lambda_i (\lambda + \sigma_{i,\pm}^*^2) \frac{\sigma_c^2}{d} + \sigma_{i,\pm}^* ( \frac{\sigma_c^2}{d} + \lambda \frac{\sigma_z^2}{n}) }{\lambda - \sigma_{i,\pm}^2} = 0  \\
\end{align}
For $\lambda_i = 0$ we obtain $\frac{\partial f}{\partial \sigma_i} = 0 \Rightarrow \sigma_i = 0$ and for $\lambda_i \neq 0$:
\begin{align*}
    \frac{\partial f}{\partial \sigma_i} \mid_{\sigma_i=\sigma_{i,\pm}^*} &= 0 \Rightarrow \sigma_{i,\pm}^* = \frac{1+\lambda_i^2 \lambda}{2 \lambda_i} + \frac{d}{m} \frac{\lambda}{2 \lambda_i} \frac{\sigma_z^2}{\sigma_c^2} \pm \sqrt{\left( \frac{1 + \lambda_i^2 \lambda}{2 \lambda_i} + \frac{d}{m} \frac{\lambda}{2 \lambda_i} \frac{\sigma_z^2}{\sigma_c^2}\right)^2 - \lambda}.
    %\label{eq:appendix_optimal_singular_values}
\end{align*}
Note, for suitable $a(\lambda)$ the solutions $\sigma_{i,\pm}^*$ are of the form
\begin{align*}
    \sigma_{i,\pm} &= a(\lambda) \pm \sqrt{ a(\lambda)^2 - \lambda}.
\end{align*}
On further examining the constraint $\sigma_i^2 \leq \lambda$ in the problem~\eqref{align:optimal_robust_estimator} note that for real numbers $a > b$ it holds $\sqrt{a^2-b^2} \geq a - b$. Hence, it follows:
\begin{align*}
    \sigma_{i, \pm} - \sqrt{\lambda} = a(\lambda) - \sqrt{\lambda} \pm \sqrt{a(\lambda)^2 - \lambda} \gtrless 0.
\end{align*}
Hence, we can rule out $\sigma_{i,+}$ and set $\sigma_{i}^* := \sigma_{i,-}^*$. Finally, inserting $\sigma_i^*$ for $1 \leq i \leq d$ into the robust risk \eqref{align:optimal_robust_estimator_f} yields:
\begin{align}
        \min_{\mH} R_{\epsilon} (\mH) = 
        \min_{\lambda \geq 0} \lambda \epsilon^2 +  \sum_{i=1}^d \frac{1-\lambda \lambda_i^2}{2} \frac{\sigma_c^2}{d} - \frac{\lambda}{2} \frac{\stdrandnoise^2}{m}  + \sqrt{\left( \frac{1 + \lambda \lambda_i^2}{2} \frac{\sigma_c^2}{d} + \frac{\lambda}{2} \frac{\sigma_z^2}{m} \right)^2 - \lambda \lambda_i^2 \frac{\sigma_c^4}{d^2}}.
        \label{eq:appendix_robust_risk_minimization_lambda}
\end{align}
The optimization problem involved is convex and box-constrained and can thus be solved numerically.
%Also note, that a valid solution of $\lambda$, which yields a non-negative discriminant in the optimization problem also yields a non-negative discriminate for the singular values in Eq.~\eqref{eq:appendix_optimal_singular_values}.
%\rh{can we write something as below?}
Besides the argument above, we confirmed our conjecture with numerical simulations.

    \subsection{Optimal jittering estimator}
    
    We now derive the optimal jittering estimator, i.e. the estimator $f(\vy) = \mH \vy$ that minimizes the jittering risk
    \begin{align*}
        J_{\stdjitter}(f) &= \EX[(\vx, \vrandnoise, \jitter)]{\norm[2]{\mH (\mA \vx + \jitter + \randnoise) - \vsignal}^2},
    \end{align*}
    where as before the signal is assumed to lie within a subspace $\vx = \mU \vc$.
    We first calculate the expectation by using that $\vx = \mU \vc$ with $\vc \sim \mathcal{N}(0, \sigma_c^2/d \mI)$, $\vz \sim \mathcal{N}(0, \sigma_z^2/m \mI)$ and $\vw \sim \mathcal{N}(0, \sigma_w^2 \mI)$ are Gaussian distributed.
    \begin{align*}
        J_{\stdjitter}(f) &= \EX[(\vsignal, \vrandnoise, \jitter)]{\norm[2]{\mH (\mA \vsignal + \vrandnoise + \jitter) - \vsignal}^2} \\
        %&= \EX[\vsignal, \vrandnoise, \jitter]{\norm[2]{\mH (\mA - \mI) \vsignal + \mH (\vrandnoise + \jitter)}^2} \\
        &= \EX[\vsignal]{\norm[2]{\mH (\mA - \mI) \mU \vc}^2}  + \EX[\vrandnoise]{\norm[2]{\mH \vrandnoise}^2} + \EX[\jitter]{\norm[2]{\mH \jitter}^2} \\
        &= \tr ( (\mH \mA - \mI) \mU \transp{\mU} \transp{(\mH \mA - \mI)}) \frac{\stdlatent^2}{d} + \tr ( \mH \transp{\mH} ) \frac{\stdrandnoise^2}{m} + \tr (\mH \transp{\mH}) \stdjitter^2,
        %&= \EX[\vc, \vrandnoise, \jitter]{\norm[2]{\mH (\mA - \mI) \mU \vc}^2} 
        %\EX[\vc, \vrandnoise, \jitter]{\norm[2]{ \mH (\vsignal + \jitter)}^2}
        %&= \tr
    \end{align*}
    Hence, the optimal jittering estimator minimizes
    \begin{align*}
        J_{\stdjitter}(f) = \tr( \mH \mX \transp{\mH} ) - 2 \tr (\mH \mY),
    \end{align*}
    where
     $\mX := \frac{\stdlatent^2}{d} \mA \mU \transp{(\mA \mU)} + \left(\stdjitter^2 + \stdrandnoise^2/m \right) \mI$ and $\mY := \frac{\stdlatent^2}{d} \mA \mU \transp{\mU}$.
    %\begin{align*}
    % \mX := \frac{\stdlatent^2}{d} \mA \mU \transp{(\mA \mU)} + \left(\stdjitter^2 + \stdrandnoise^2/m \right) \mI \quad \text{ and } \quad \mY := \frac{\stdlatent^2}{d} \mA \mU \transp{\mU}.
    %\end{align*} 
    Using matrix calculus we calculate the optimal estimator as
    \begin{align*}
        \nabla_{\mH} J_{\stdjitter}(f) \mid_{\mH = \mH^*} &= 2 \mH^* \mX - 2 \transp{\mY} = 0 \\
        \Rightarrow \mH^* &= \transp{\mY} \mX^{-1}.
    \end{align*} 
    Now, let $\mA \mU = \transp{\mW} \mLambda \mV$ be the singular value decomposition of $\mA \mU$. Then:
    \begin{align*}
     \mX &:=
     %&= \frac{\stdlatent^2}{d} \transp{\mW} \mLambda \mV \transp{\transp{\mW} \mLambda \mV} + \left(\stdjitter^2 + \stdrandnoise^2/m \right) \mI \\
    \frac{\stdlatent^2}{d} \transp{\mW} \mLambda^2 \mW + \left(\stdjitter^2 + \stdrandnoise^2/m \right) \mI \\
     \mY &:=
    \frac{\stdlatent^2}{d} \transp{\mW} \mLambda \mV \transp{\mU}.
    \end{align*}
    We get the optimal jittering estimator as:
    \begin{align}
        \argmin_{\mH} J_{\stdjitter}(f) &= \transp{\mY} \mX^{-1} = 
        \frac{\stdlatent^2}{d} \mU \transp{\mV} \mLambda \mW
        \left(\frac{\stdlatent^2}{d} \transp{\mW} \mLambda^2 \mW + \left(\stdjitter^2 + \stdrandnoise^2/m \right) \mI \right)^{-1} \\
        &= 
        \frac{\stdlatent^2}{d} \mU \transp{\mV} \mLambda
        \left(\frac{\stdlatent^2}{d} \mLambda^2 + \left(\stdjitter^2 + \stdrandnoise^2/m \right) \mI \right)^{-1} \mW \\
        &= \mU \transp{\mV} \diag \left( \frac{\stdlatent^2 \lambda_i}{ \stdlatent^2 \lambda_i^2 + d \stdjitter^2 + \stdrandnoise^2 \frac{d}{m}} \right) \mW.
        \label{eq:appendix_optimal_jittering_estimator}
    \end{align} 
    %In the main body of the paper we stated that the optimal jittering estimator for the problem above is given as:
    %\begin{align*}
    %\mH_J = \mU \mV \diag\left( \frac{\sigma_c^2 \lambda_i}{\sigma_c^2 \lambda_i^2 + \sigma_z^2 \frac{d}{n} + \sigma_w^2 d} \right) \transp{\mW}.
    %\end{align*}

    \subsection{Numerical simulations supporting Conjecture~\ref{co:worst-case}}
        In the following we present numerical simulations for general linear inverse problems in the subspace model to support the Conjecture~\ref{co:worst-case} further. The results show that the (empirical) optimal robust risk, obtained via adversarial training, is the same as the robust risk of the conjectured optimal estimator. Moreover, the results show that jittering can yield suboptimal robust estimators in some cases.

        We consider linear inverse problems $\vy = \mA \vx + \vz$ with the signal $\vx$ lying in a subspace $\vx = \mU \vc$ with $\vc \sim \mathcal{N}(0, \stdlatent^2/d \mI)$ and Gaussian noise $\vz \sim \mathcal{N}(0, \stdrandnoise^2/m \mI)$. 
        For the simulations, we choose $\stdlatent / \sqrt{d} = 1$ and $\stdrandnoise / \sqrt{n} = 0.1$ with dimensions $d = 50$ and $n = 100$. Moreover, we consider two diagonal forward operators $\mA = \diag(\lambda_i)$: an operator with linear decaying singular values ($\lambda_i = \frac{i}{n}$) and one with geometrically decaying singular values $\lambda_i = 0.7^i$.

        \paragraph{Optimal worst-case estimator.}
        We first estimate the optimal robust risks by performing adversarial training and compare it with the robust risk of the conjectured optimal estimator. 
        Adversarial training is performed as described in Section~\ref{sec:experiments}, where we generate data $\{ (\vy_i, \vx_i) \}$ using the respective forward model in the subspace. The estimator $f(\vy) = \mH \vy$ can be viewed as a neural network with one layer and without bias.
        Figure~\ref{fig:subspace_model_beyond_denoising_singular_values_worst_case_robust} shows that the robust risk of the conjectured estimator is essentially the same as the (empirical) optimal robust risk.
        % Figure environment removed

        \paragraph{Suboptimality of jittering.}
        We further investigate whether optimal robust estimators can be obtained via jittering. To that end, we calculate the minimal robust risk attainable via jittering
        %For assessing the ability of jittering to yield optimal robust estimators, we calculate the robust risk of jittering estimators, with the jittering noise level $\stdjitter$ to selected to obtain the minimal robust risk, i.e.:
        \begin{equation*}
            \min_{\sigma_w} R_{\epsilon} (\argmin_{f} J_{\stdjitter}(f)),
        \end{equation*}
        where $f(\vy) = \mH \vy$ is a linear reconstruction operator as before. We make use of the analytic expression of the jittering estimator Eq.~\eqref{eq:appendix_optimal_jittering_estimator} and calculate the attainable robust risk by minimizing the robust risk of jittering with respect to the jittering noise level.
        The results are compared to the (conjectured) optimal robust risks via Eq.~\eqref{eq:appendix_robust_risk_minimization_lambda} and the robust risk of the standard estimator.
        Figure~\ref{fig:subspace_model_beyond_denoising} shows the results of the calculations for the forward operator with linear and geometrically decaying singular values described above. It can be seen that the standard estimator is noticeable less robust compared to denoising setups. Moreover, a gap can be observed between the robust risk obtained via jittering and the optimal robust risks.
        
        % Figure environment removed
        

%%%
\section{Details on the experimental results and further experimental results}

    In this section, we present details on the experimental results in Section~\ref{sec:experiments}, and present further experimental results on U-nets trained with robustness-enhancing methods for image denoising, deconvolution, and compressive sensing. The setup and methods considered are as described in the main body.

    \subsection{Optimal robust denoiser for large perturbation levels}

    In the main body, we presented empirical results for perturbation levels in the range $\epsilon^2 / \signalenergy \in [0,0.3]$, since this is the practically most relevant regime. 
Figure~\ref{fig:robust_risk_large_perturbations} shows the risk of linear estimators and U-nets trained adversarially for perturbation levels in the range $\epsilon^2 / \signalenergy \in [0,1.5]$. 
From those plots, we see that the transition at $\epsilon^2 / \signalenergy = 1$ predicted by Theorem~\ref{thm:main}  for the estimator to map to zero occurs for the subspace model (as predicted by the theory) as well as for the U-net for image denoising.  
    % Figure environment removed

    \subsection{Convolution kernel for deconvolution experiments}
    
    In addition to experiments on denoising image data, we consider a deconvolution setup $\vy = \vk \star \vx + \vz$ in this work. The kernel $\vk$ is Gaussian, applied channel-wise and visualized in Figure~\ref{fig:deconvolution_Gaussian_kernel}.
    % Figure environment removed


    \subsection{Hyperparameter selection}
    In the experiments we treat the Jittering noise level $\stdjitter$ as a hyperparameter, which we optimize over a validation dataset to obtain robust estimators at the desired perturbation levels.
    The hyperparameter search is performed by choosing a grid of jittering noise levels for each task. For each noise level neural networks (U-Nets) are trained via Jittering, and subsequently evaluated on the considered perturbation levels.
    %Note, that the  networks are not fully trained and the data is smoothed using a uniform filter. Hence, the robust risks shown are not strictly optimal.
    Figures~\ref{fig:empirical_jittering_choice_rule_denoising}, \ref{fig:empirical_jittering_choice_rule_mri} and \ref{fig:empirical_jittering_choice_rule_deconv} show the robust risks of jittering for the considered tasks, as well as the derived jittering choice rule. The smooth curves on the left panels represent the robust risk at a particular robustness level and are obtained by applying uniform filters on the evaluation results.
    %This might yield increased minima in the robust risk, but does not influence the location of the minimizer. 
    It can be seen that for image denoising the empirical jittering choice is close to the prediction from theory.% Interestingly the prediction also matches the optimal jittering noise levels for deconvolution, when assuming a small subspace dimension.
    
    % Figure environment removed
    % Figure environment removed
    % Figure environment removed

    \newpage

    \subsection{Computational complexity}

    We measured the GPU time until convergence and memory utilization of the robustness-enhancing schemes on the task of Gaussian denoising of colorized images. Figure~\ref{fig:unet_robust_risk_convergence} shows the training error of adversarial training, training via jittering and standard training as a function of the number of epochs. It shows networks trained at two perturbation levels for adversarial training and jittering (parameter choice taken from Figure~\ref{fig:empirical_jittering_choice_rule_denoising}). We find that all methods require a similar number of epochs for convergence (roughly $600$ epochs). Table~\ref{table:comp_efficiency_medium} presents the measured GPU time until convergence and average memory consumption. It can be seen that adversarial training is by a factor of the projected gradient ascent steps ($3$ in this plot) more expensive than jittering. Moreover, training via jittering has similar computational cost as standard training in terms of GPU time. All three methods require a similar amount of GPU memory to train.

    
    % Figure environment removed

    \begin{table}
        \caption{GPU time until convergence and memory utilization of adversarial training, jittering and standard training for the task of denoising colorized images. The required number of epochs is estimated from the data visualized in Figure~\ref{fig:unet_robust_risk_convergence}. GPU time and memory consumption are measured on a Nvidia RTX A6000 GPU.}
        \vskip 0.05in
        \label{table:comp_efficiency_medium}
        \centering
        \begin{tabular}{lcc}
        \toprule
        Method & Total GPU hours &  Memory \\
        \midrule
        Adversarial training & $9.7$ h & $9971$ MiB      \\
        Training via jittering & $3.0$ h & $9523$ MiB             \\
        Standard training & $3.0$ h & $9523$ MiB                        \\
        \bottomrule
        \end{tabular}
    \end{table}

\section{Discussion of the related work on randomized smoothing}
\label{sec:appendix_related_work}

%\subsection{Randomized smoothing}
Randomized smoothing is a very successful technique for obtaining robust classifiers~\citep{cohen_CertifiedAdversarialRobustness_2019, carliniCertifiedAdversarialRobustness2022a}. 
Randomized smoothing constructs a smoothed classifier based on a base classifier by averaging the base classifier's outputs under Gaussian noise perturbation. The smoothed classifier allows for certified radii in which it is provably robust, without making any restrictions on the base classifier. However, \citet{salmanDenoisedSmoothingProvable2020} demonstrated that it can give loose bounds, since the base classifier is not trained to be robust to Gaussian noise. For that reason \citet{salmanDenoisedSmoothingProvable2020} propose denoised smoothing, which considers a composition of the base classifier with a denoising method.
At first sight, randomized smoothing might sound similar to the Jittering approach investigated here. However, as we argue below, randomized smoothing it is conceptually very different from Jittering. 

Given a classifier $f \colon \mathbb{R}^d \to \{1,\ldots,K\}$ randomized smoothing constructs a  smoothed classifier $g$ from the classifier $f$ as:
\begin{align*}
g(\vx) = \arg \min_{k \in \{1,\ldots,K\}} \EX[\ve \sim \mc N(0, \sigma^2 \mI)]{ \ind{f(\vx + \ve) \neq k}},
\end{align*}
where the parameter $\sigma^2$ controls the robustness-accuracy tradeoff. 

For an inverse problem, where we aim to reconstruct a signal $\vx \in \mathbb{R}^n$ from a measurement $\vy \in \mathbb{R}^m$ using a given reconstruction method $f$, replacing the $0/1$ by the $\ell_2$ loss yields:
\begin{align*}
g(\vy) &= \arg \min_{\vx} \EX[\ve \sim \mc N(0, \sigma^2 \mI)]{ \norm[2]{f(\vy + \ve) - \vx }^2} \\
 &= \EX[\ve \sim \mc N(0, \sigma^2 \mI)]{ f(\vy + \ve) }.
\end{align*}
For a linear estimator $f(\vy) = \mH \vy$ we see that $g(\vy) = f(\vy)$, so for the linear setting considered in the theory part of this paper randomized smoothing would not change the original estimator. If one considers $f \colon \mathbb{R}^n \to [0,1]^n$ the smoothed estimator $g(\vy)$ differs from $f(\vy)$ and robustness gains can be expected, which follows from \citet{salman_ProvablyRobustDeep_2019}, Lemma~1. In summary, randomized smoothing is very  different to jittering in that it constructs a surrogate smoothed model $g(\vy)$ based on a given fixed estimator $f(\vy)$, whereas jittering is a training technique.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Regularizing beyond jittering for enhancing robustness}

    Training neural networks via jittering, with noise levels chosen for larger perturbations, yields smoother reconstructions compared to adversarial training (see results in Section~\ref{sec:experiments}). In this section, we investigate two related regularization methods, $\ell_2$-regularization and Jacobian regularization, discuss the connection to regularization with jittering, and present experimental results for denoising grayscale images.

    \subsection{$\ell_2$- and Jacobian regularization in the subspace model}
    In the subspace model we established that the optimal jittering estimator is also worst-case optimal, when using a suitable choice of noise level $\stdjitter(\epsilon)$. It turns out, that jittering can further be approximated with an explicit regularizer, Jacobian regularization. For the linear setup considered here, this approximation becomes exact and therefore Jacobian regularization also enables training a worst-case robust estimator. 
    %We next show that in the linear case, an explicit regularizer, Jacobian regularization, also enables training a worst-case robust estimator. Jacobian regularization can be understood as a linear approximation to jittering, that becomes exact for a linear estimator. 
    Specifically, using the linear approximation of the function $f$ around the point $\vy$, we get
    \begin{align}
        \EX[\vw]{ \norm[2]{f(\vy + \vw) - \vx }^2 }
        \approx
        \EX[\vw]{ \norm[2]{f(\vy) + \Jacobian{\vy}\vw  - \vx }^2 }
        =
        \norm[2]{f(\vy) - \vx }^2
        + 
        %\EX[\vw]{ \norm[2]{\Jacobian{\vy}\vw}^2 }
        \sigma_w^2  \norm[F]{\Jacobian{\vy}}^2.
        \label{eq:jacapprox}
    \end{align}

Here, $\Jacobian{\vy}$ is the Jacobian of the function $f$ at $\vy$. The approximation is good for small values of the noise variance $\sigma_w^2$, and is exact for the linear estimator $f(\vy) = \mH \vy$ we consider in this section. 
The approximate relation~\eqref{eq:jacapprox} motivates the Jacobian regularized risk, defined as
\begin{align}
\Jac_{\lambda}(f)
= 
\EX[(\vx,\vy)]{ 
\norm[2]{f(\vy) - \vx }^2
+ \lambda  \norm[F]{\Jacobian{\vy}}^2
}
\label{align:jac_reg}
\end{align}

The connection between jittering and Jacobian regularization is well known in the literature and discussed by~\citet{reedSimilaritiesErrorRegularization1995}. Recall that for the linear estimator considered in this section the approximation in equation~\eqref{eq:jacapprox} is exact, and therefore Jacobian regularization is equivalent to jittering. Thus, Jacobian regularization yields a provably robust estimator, if the regularization parameter is chosen as $\lambda = \sigma_w^2(\epsilon)$ according to corollary~\ref{cor:jittering_nl_by_perturbation_level}. 

For the linear case, Jacobian regularization is even equivalent to $\ell_2$-regularizataion, since the Jacobian of the function $f(\vy) = \mH \vy$ is $\Jacobian{\vy} = \mH$, and thus even $\ell_2$-regularization yields a robust estimator. 

    \subsection{Experimental results on grayscale image denoising}

    In the following we present results on Gaussian denoising of grayscale images. While $\ell_2$ regularization is equivalent to jittering in the subspace model, we find that the parameter choice $\lambda = \sigma_w^2(\epsilon)$ does not yield robust neural networks using $\ell_2$ regularization. In contrast, Jacobian regularization turns out to be quite effective for learning neural network denoisers, but is computationally demanding compared to jittering.

    \subsubsection{Problem setup}

    We consider once again the dataset of natural image and convert the images to grayscale. We perform Gaussian denoising, , i.e. the problem is to reconstruct the image $\vx$ from a measurement $\vy = \vx + \vz$, with $\vz \sim \mathcal{N}(0, \sigma_z^2/n \mI)$ and $\sigma_z / \sqrt{n} = 0.2$.

    The estimators are chosen as neural networks (U-nets) with the same architecture as for colorized images. We consider adversarial training, jittering and standard training as baseline and compare against $\ell_2$ and Jacobian regularization:

    \paragraph {$\ell_2$ regularization.} Implemented as weight-decay in PyTorch's SGD optimizer to minimize
    \begin{align*}
    %\label{align:ell2_regularization
        \hat W_\lambda(\vtheta) = \sum_{i=1}^N \norm[2]{f_\vtheta(\vy_i) - \vx_i}^2 + \lambda\norm[2]{\vtheta}^2.
    \end{align*}
    
    \paragraph{Jacobian regularization.}
    We train networks with the Jacobian regularized empirical risk
    \begin{align*}
    \widehat{\Jac}_{\lambda}(\vtheta)
    = 
     \sum_{i=1}^N \norm[2]{ f_\vtheta(\vy_i) - \vx_i }^2 + \lambda \| \Jacobian{\vy_i}  \|^2_F,
    \end{align*}
    where $\Jacobian{\vy_i}$ is the Jacobian of the network $f_\theta$ with respect to it's input (not it's parameters) at $\vy_i$. This regularization can be viewed as an approximation of the jittering risk, as described in Section~\ref{sec:jitteringlinearmodel}.
    Calculating the full Jacobian $\Jacobian{\vy_i}$ with PyTorch requires $n$-many calls of the backward function, which is very expensive, since $n$ is large. 
    To mitigate this cost, we approximate the norm of the Jacobian, $\norm[F]{\Jacobian{\vy_i}}^2$ with $\norm[2]{\transp{\Jacobian{\vy_i}} \vw}^2$, where $\vw \sim \mathcal{N}(0, \mI)$. This approximation of the norm concentrates around the actual squared norm of the Jacobian, and only costs one call of the PyTorch-backward function.

    For the experiments we use stochastic gradient descent (SGD) with learning rate $10^{-2}$, momentum $0.9$ and batch size $100$. We evaluate using the empirical pixel-wise robust risk $\hat{R}_{\epsilon} / n$.

    \subsubsection{Results}

        The experimental results, plotted in Figure~\ref{fig:appendix_grayscale_results}, show that the networks trained with jittering and Jacobian regularization have similar robust risks compared to the adversarial trained one. Weight-decay or $\ell_2$ regularization yields worse performing estimators than jittering and Jacobian regularization.
        While for the linear subspace setting, adversarial training, Jacobian and $\ell_2$ regularization are equivalent, for Gaussian denoising they perform differently. Figure~\ref{fig:experiments_apple_denoising} shows that Jacobian regularization, unlike Jittering, does not yield smoothed images for larger perturbations. However, Jacobian regularization requires approximately $1.4$ as much GPU memory and $4$ times more time per epoch.
        
        % Figure environment removed

     
    % Figure environment removed

 
\end{document}