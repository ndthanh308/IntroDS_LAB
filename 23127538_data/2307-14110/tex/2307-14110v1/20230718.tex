\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper
\IEEEoverridecommandlockouts                              % This command is only needed if 
% you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.
\let\proof\degree
\let\endproof\relax

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
% \usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm,multirow,xcolor}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{cases}
\usepackage{array}
\usepackage{color}
% \usepackage{xcolor}
\usepackage{float}
\usepackage{cite}
% \usepackage{amssymb}
\usepackage[skip=6pt]{caption}

% \usepackage{appendix}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsthm, amssymb}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\usepackage{soul}
\usepackage{epstopdf}
\setlength{\textfloatsep}{5pt}
%\linespread{0.996}



%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
% \newcommand{\yujie}[1]{{\color{blue}  \textbf{[yujie: #1]}}}
% \newcommand{\wei}[1]{{\color{cyan}  \textbf{[wei: #1]}}}
\newcommand{\qrz}[1]{{\emph{[QRZhang: #1]}}}

\makeatother
%%%%%%%%%%%%%%%%%%%%%%

\title{\LARGE \bf
	%Reinforcement Learning Based Tuning-free Filter For Attitude Estimation
	%Extended Kalman Filter for Sensor Fusion with Reinforcement Learning Compensation
	Reinforced Potential Field for Multi-Robot Motion Planning in \\ Cluttered Environments
}


\author{Dengyu Zhang, Xinyu Zhang,   Zheng Zhang, 
 Bo Zhu, and Qingrui Zhang % <-this % stops a space
% \thanks{$^{1}$Y.~Tang and W.~Pan are with  the Department of Cognitive Robotics, Delft University of Technology, Netherlands.}
% \thanks{$^{2}$L.~Hu is with the School of Computer Science and Electronic Engineering, University of Essex, UK.}%
% \thanks{$^{3}$Q.~Zhang is with the School of Aeronautics and Astronautics, Sun Yat-Sen University, China. }
% \thanks{This work is supported in part by the National Nature Science Foundation of
% China under Grant 62103451 and Shenzhen Science and Technology Program JCYJ20220530145209021}
% \thanks{All authors are with the School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen 518107, P.R. China (\{zhubo5, zhangqr9\}@mail.sysu.edu.cn; Corresponding author: Qingrui Zhang) }
% }
\thanks{All authors are with the School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen 518107, P.R. China (\{ zhangdy56, zhangxy385, zhangzh363\}@mail2.sysu.edu.cn, \{zhubo5, zhangqr9\}@mail.sysu.edu.cn; Corresponding author: Qingrui Zhang)}
}


\begin{document}
	
	\maketitle
	\thispagestyle{empty}
	\pagestyle{empty}


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{abstract}
 Motion planning is challenging for multiple robots in cluttered environments without communication, especially in view of real-time efficiency, motion safety, distributed computation, and trajectory optimality, \emph{etc}. In this paper, a reinforced potential field method is developed for distributed multi-robot motion planning, which is a synthesized design of reinforcement learning and artificial potential fields. An observation embedding with a self-attention mechanism is presented to model the robot-robot and robot-environment interactions. A soft wall-following rule is developed to improve the trajectory smoothness. Our method belongs to reactive planning, but environment properties are implicitly encoded. The total amount of robots in our method can be scaled up to any number.  The performance improvement over a vanilla APF and RL method has been demonstrated via numerical simulations. Experiments are also performed using quadrotors to further illustrate the competence of our method.
	% Multi-robot motion planning is a fundamental problem that aims at exploring the optimal way from starting point to a destination that satisfies the constraints including maneuverability and environmental constraints. Multi-robot motion planning runs into many challenges in cluttered environments, such as real-time efficiency, safety, and performance. To solve problems of multi-robot motion planning in cluttered environments, the reinforced potential field (RPF) algorithm, a motion planning method combining the artificial potential field (APF) method and deep reinforcement learning,  is proposed in this paper. The reinforced potential field method introduces the Attention mechanism to embed observation information, trains the neural network via the proximal policy optimization (PPO) algorithm, then applies the soft wall-following rule to smooth the trajectory computed by the neural network instructed APF. The novelty of our approach is that we combine state-of-the-art reinforcement learning method with traditional potential field method to gain both intelligence and safety, and introduce the attention mechanism to improve robots' collaboration.
	\end{abstract}

% Motion planning is challenging for multiple robots in cluttered environments without communication, especially in view of real-time efficiency, motion safety, distributed computation, and trajectory optimality, \emph{etc}. In this paper, a reinforced potential field method is developed for distributed multi-robot motion planning, which is a synthesized design of reinforcement learning and artificial potential fields. An observation embedding with a self-attention mechanism is presented to model the robot-robot and robot-environment interactions. A soft wall-following rule is developed to improve the trajectory smoothness. Our method belongs to reactive planning, but environment properties are implicitly encoded. The total amount of robots in our method can be scaled up to any number. Advantages of our It can be applied to a scenario with any number of robots. \qrz{The performance improvement over vanilla APF and RL has been demonstrated via numerical simulations}. Experiments are also performed using quadrotors to further illustrate the competence of our method.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{INTRODUCTION}

% \qrz{Multi-robot systems are a more efficient and flexible  than single robots, have attracted extensive research attention in many applications, e.g.,  package delivery, planetary exploration, supply chain automation，and search-and-rescue etc. } \cite{Chen2021JPROC}
With recent advances in automatic technologies and artificial intelligence, robots are envisioned to run efficiently, reliably, and safely with little human intervention in a myriad of applications, \emph{e.g.,} package delivery, planetary exploration, supply chain automation, and search-and-rescue operation \emph{etc.} \cite{wurm2008IROS, Mathew2015TASE,bai2018RAS}. The burgeoning applications in diverse tasks put great demands on robot autonomy to support safe and intelligent operations in complex environments. Motion planning is one of the fundamental modules of robot autonomy, which computes dynamically-feasible trajectories for robots to reach their respective goal regions \cite{mohanan2018RAS}. Yet, motion planning is a challenging task, especially for multiple robots with no communication in cluttered environments \cite{snape2010IIRS, Zhao2018ACC}. Finding a safe and smooth motion towards a target is demanding in a congested environment with numerous obstacles.  A robot also needs to comply with motions of other robots in the shared space, while communications among robots are scarcely available \cite{chen2017IROS,kretzschmar2016IJRR}.
The decision difficulty, planning cost, and collision risk will all grow exponentially with the amount of robots and obstacles in cluttered environments. 
% \cite{riviere2020IRAL,Tingxiang2020IJRR}.

Most methods of motion planning in multi-robot cluttered environments fall into two categories, namely proactive and reactive algorithms. In proactive planning, robots make active decisions based on current and forthcoming states of the environment \cite{Sandeep2019IJRR,Alonso2018TRO,Truong2017TASE}. Hence, sophisticated models are prerequisite for the prediction of environment states in the near future, interactive information among neighbouring robots, or intentions of other intelligent agents \cite{Luber2012IROS}. Most proactive planning algorithms are based on online optimization, \emph{e.g.}, dynamic window approach (DWA), model predictive control (MPC) \cite{zhu2019IRAL,soria2021Nature}, \emph{etc}. The computation of those optimization-based algorithms is cumbersome for lightweight platforms in cluttered environments, \emph{e.g.}, UAVs and UGVs, etc. Convergence is not ensured in short-time windows. Communication is required among robots to share prediction results. 

 % Figure environment removed


In reactive planning, robots determine their motion directly based on the instantaneous measurements from their proprioceptive and  exteroceptive sensors \cite{arslan2019IJRR,cole2018ITR}. Robot motion is generated in real-time as a function of a certain vector field or virtual forces, \emph{e.g.}, guiding vector field \cite{Yao2021TRO}, Dipole-like field \cite{panagou2014ICRA}, and artificial potential field (APF) \cite{ZhangZheng2022}, \emph{etc.} Although solutions for reactive planning are not optimal in general, they are mostly computationally faster than the optimization-based proactive methods.  Communication among robots is not necessary for reactive planning, which is an advantage for distributed scenarios. Hence, reactive planning is suitable for lightweight platforms with limited resources. However, non-smooth and oscillatory trajectories are often generated by reactive planning. Conventional reactive planning methods would experience the deadlock issue in multi-robot cluttered environments. 

% , \emph{e.g., GVF and APF}\cite{soria2021Nature}% 
%by a vector field arising from some closed-loop feedback policy issuing online force or velocity commands in real-time as a function of the instantaneous robot state. By
% Reactive controllers are a type of local planner that generate the trajectory directly as the environment is sensed. These approaches utilize distance sensors to determine course changes.While these solutions are generally not optimal, they are typically computationally faster than the optimized solutions and do not require convergence of an optimization algorithm to generate a viable solution. Their drawback however is that they do not address the smoothness of the trajectory. This can be problematic if the desired navigation requires more thrust than that the vehicle can produce, and/or if the higher derivatives of the trajectory are not bounded, which may violate vehicle controller requirements.
%  states and surrounding environment observations. The exteroceptive sensors measurements of
%he exteroceptive sensors measurements of
% It is, therefore, expected to have efficient motion planning algorithm that reacts fast to Robots are, therefore,   
% such as mission planning, trajectory tracking
% 10 [4, 5, 6], collision avoidance [7, 8, 9], etc.

In this paper, a reinforced potential field (RPF) method is proposed for distributed multi-robot motion planning in cluttered environments. The proposed method belongs to reactive planning {methods}. In the proposed design, reinforcement learning (RL) is introduced to implicitly encode the diverse impact of surrounding environments and capture motions of other robots in the shared space. The dynamic changes in surroundings will be feedback into an APF module, so robots can adjust their motion in time. An observation embedding with self-attention mechanism is employed to characterize both the robot-environment interactions and the interactive information among robots, thereby inferring the relative importance of neighboring agents or obstacles. A soft wall-following rule is designed to smooth the APF outputs. The efficiency of the overall algorithm is evaluated extensively using numerical simulations, and also validated through experiments with quadrotors. In summary, the main contributions of this paper are three fold:
 \begin{enumerate}
     \item A distributed reactive planning framework is developed for multiple robots in cluttered environments. The proposed design integrates RL into conventional reactive planning methods. With the inference capability of RL, the proposed design is able to make active responses to dynamic change of surrounding environments. 
     \item An observation embedding with self-attention mechanism is {introduced to our algorithm}, which is able to extract the hidden features of the  environments.
     \item A soft wall-following rule is presented, which could improve the smoothness of the planned trajectory.
 \end{enumerate}


% arbitrary number of agents, providing a good understanding of the crowd behavior for planning.
% and  implicitly encoded by using Reinforcement learning, whose impact on the  introduced to implicitly encode surrounding 

% extracts features for pairwise interactions between the robot and each human and captures the interactions among humans via local maps Home structure,

% Compared with model-based approaches, learning-based methods have been shown to produce paths that more closely resemble human behaviors, but often at a much higher computational cost. This is because computing/ matching trajectory features often requires anticipating the joint paths of all nearby pedestrians [2], and might depend


% Introducing the soft wall-following rule makes the field smoother and introducing the deep reinforcement learning method dissolves the local minima to some extent. Deep reinforcement learning collects a quantity of experiment or simulation data and learns from them, resulting in powerful intelligent artificial neural networks. Compared with traditional artificial potential field method, the reinforced potential field can make decisions depending on information in robot's neighborhood, guiding robots to move in a better way. Compared with methods of reinforcement learning only, by our approach, robot's safety is guaranteed without trial and error. To improve the performance of robots' collaboration, we introduce the attention mechanism to embed robots' observations before inputting them into the neural network.

    % Multi-robot motion planning (MRMP) is a fundamental problem in robotics and artificial intelligence. The goal of MRMP is to compute feasible trajectories for multiple robots to reach their destinations, avoiding collision with obstacles or each other. The difficulty and computation of MRMP grow exponentially in cluttered environments where there are a lot of robots and obstacles, while robots in a cluttered environment may crash into obstacles because of any delay. Real-time algorithms have little delay but lost consideration of some information, which cannot deal with complex cluttered environments. Besides the limitation of computation and performance, another limitation appears that robots can only observe a nearby region. MRMP needs to find a balance between computation efficiency, collision-free performance, and local observation. 

    % Motion planning methods can be concluded into global motion planning methods and local motion planning methods. Global motion planning methods, including A*, Dijkstra, RRT, can find collision-free paths in any complex environment, but require complete global information, which are not suitable for real robots with limited observations in cluttered environments. Local motion planning methods contain reactive methods and optimize-based methods. Optimize-based methods such as MPC take model dynamics into consideration, making robots perform well but cost a lot. Reactive methods are more computationally efficient. 
 
    % Artificial potential field is a classical real-time algorithm for collision avoidance that ensures robots' safety without complete global information, which has been enhanced by researchers and evolved into several different reactive methods\cite{Khatib1986APF, Koditschek1987LM, Connolly1992, Koditschek1990, Filippidis2011, Paternain2020}. The idea of artificial potential field is to decompose high-level planning problems via virtual potential fields, in which obstacles represent positive potential and goal points represent negative potential. Naturally, objects move towards the place with the lowest potential energy and avoid places with higher potential energy. Under the concept of artificial potential field, the motion planning problem can be formulated as an accumulation of potential fields. Artificial potential field method is efficient, safe, and suitable for real sensors, but the traditional artificial potential field method still has problems, including local minima and non-smoothness. The local minima can easily appear in cluttered environments, blocking robots from their goals, and the non-smoothness can be critical for precise robots like unmanned aerial vehicles. 



    % In this work, we propose the reinforced potential field method. Introducing the soft wall-following rule makes the field smoother and introducing the deep reinforcement learning method dissolves the local minima to some extent. Deep reinforcement learning collects a quantity of experiment or simulation data and learns from them, resulting in powerful intelligent artificial neural networks. Compared with traditional artificial potential field method, the reinforced potential field can make decisions depending on information in robot's neighborhood, guiding robots to move in a better way. Compared with methods of reinforcement learning only, by our approach, robot's safety is guaranteed without trial and error. To improve the performance of robots' collaboration, we introduce the attention mechanism to embed robots' observations before inputting them into the neural network.

    The rest of this paper is organized as follows. In Section \ref{sec:Related}, the related works are summarized. Preliminaries are provided in Section \ref{sec:preli}. Section \ref{sec:alg} presents the implementation details of the proposed algorithm. Numerical simulations and experimental results are given in Section \ref{sec:exp}. Conclusion remarks are summarized in Section \ref{sec:Conclusion}.
	
	\section{Related Works}
 \label{sec:Related}
    %To achieve the goal of motion planning, proactive planning methods need to predict the future states of other robots. One approach is to introduce a communication network for robots to share their intentions. Hence, robots can then update their own plan to be collision free with other robots’ plans, as in these MPC works. 
    In model-based proactive planning, forthcoming states of the environment are used in the current decision-making. Several methods have been proposed, such as  velocity obstacles (VOs) \cite{tan2020IRS}, decentralized MPC \cite{Tallamraju2018SSRR} and sequential MPC \cite{Aoki2022ITSC}. VO-based methods make predictions in light of the current velocity, where uncertainties are handled by extending bounding volumes \cite{tan2020IRS}. The VO method was modified in an optimal reciprocal collision avoidance (ORCA) approach to improve the trajectory  smoothness \cite{snape2010IIRS}. Based on VO or ORCA, both decentralized and sequential MPC algorithms are studied for multi-robot motion planning \cite{Alonso2018TRO}. 
    % where a $\varepsilon$CCA method is proposed introduced fro both homogeneous and heterogeneous groups of intelligent robots.
%  in \cite{kamel2017IROS}, taking motion uncertainties into account. However, bounding volumes may lead to infeasible results in cluttered environments. 
% In \cite{zhu2019IRAL}, a chance-constrained NMPC method is developed, which aims to minimize the collision probability of robots in cluttered environments. 
Proactive methods can achieve safe collision avoidance with good performance. However, communication is necessary for intelligent robots in MPC to share their planned decisions in the forthcoming horizon. The smooth and safe planning of MPC is obtained at the cost of high computation.  When the amount of robots and obstacles grows, these methods may be neither available nor reliable for small-scale robots with limited resources.
	% Motion planning methods can be concluded into two types: global motion planning methods and local motion planning methods. The global motion planning method searches based on the information of the whole image, and the local motion planning only plans based on the local information near the agent. Global motion planning methods mainly include A* algorithm, Dijkstra algorithm, and rapid random search tree (RRT) method, which can solve complex motion planning problems including mazes. Focusing on cluttered environments, robots can only observe their neighborhood, which are not able to provide enough information for global planning methods. 
    
    % Local motion planning can be mainly concluded into two categories: reactive methods and proactive methods. The reactive method generates a specific response when robots get close to obstacles, and does not take far-away obstacles into consideration. Proactive methods lead robots to predict their motion in near future and find the optimal planning result. Proactive methods for local motion planning mainly include Dynamic Window Approach (DWA), trajectory optimization, and Model Predicted Control (MPC). Fox et al. \cite{Fox1997} proposed a dynamic window method to establish optimization objectives and constraints, solve the velocity-angular velocity binary $(v, \omega)$, and choose the best in the arc trajectory. The trajectory optimization method also establishes optimization objectives and constraints, and selects the best among polynomial trajectories. Model Predictive Control is a proactive feedback control method \cite{Grune2016}, which is also a model-based approach. Compared with other proactive methods, model predictive control combines the model information of the controlled object itself, and has better control stability. In general, proactive methods require more computation time than reactive methods. Most proactive methods are difficult to meet real-time requirements.


Reactive planning methods are designed mainly based on Artificial Potential Fields (APFs). In APFs, two types of virtual forces are defined to regulate robots' motion, namely attractive force for target reaching and  repulsive force for collision avoidance \cite{Khatib1986APF}.  APF-based methods are capable of offering safe robot navigation in sparse stationary environments \cite{ge2002AR,mohanan2018RAS}. However, APF-based methods suffer from the local minima issue, so no assurance is guaranteed for global convergence to a target location \cite{mohanan2018RAS}. In \cite{arslan2019IJRR}, the local minima issue was addressed for convex cluttered environments with sufficiently separated obstacles. The issue is still open for general cluttered environments.  An alternative reactive planning is based on vector fields (VFs) that could avoid the local minima issue \cite{Yao2021TRO}.
% In \cite{Yao2021TRO}, a vector-field guided path-following algorithm is proposed for robots in open environments, where globalconvergence to desired path is ensured. 
In \cite{panagou2014ICRA}, a Dipole-like vector field is designed for multi-robot motion planning. Global convergence is ensured for environments with sufficiently separated obstacles \cite{panagou2014ICRA}. However, VF-based methods assume the  environment is fully known a priori. 

    In learning-based methods, collision-free planning policies are learned mainly by reinforcement learning  (RL) through the maximization of certain return functions using data samples\cite{kretzschmar2016IJRR,riviere2020IRAL}.  In \cite{chen2017IROS}, a deep value learning method is used to generate human-like navigation for robots in crowded dynamic environments. In \cite{chen2019ICRA}, the deep value learning approach is modified by adding a socially attentive network that models crowd-robot interactions. In \cite{Tingxiang2020IJRR}, a sensor-level decentralized collision-avoidance policy is learned using proximal policy optimization (PPO) method, which has an end-to-end feature. Vanilla learning-based methods always suffer from low data efficiency and poor generalization. Hence, a synthesized design of learning-based and force-based methods is proposed for distributed motion planning in dynamic dense environments \cite{Semnani2020IRAL}. In our previous works \cite{ZhangZheng2022}, artificial potential field is also integrated into the Dueling Double Deep Q Network (D3QN) to boost the policy learning for the multi-robot cooperative hunting problem.  In this paper, we will develop a new synthesized design that explicitly models robot-environment interactions.  Future environment states are used to further improve the algorithm's performance. 

    % Reactive motion is generated by a vector field arising from some closed-loop feedback policy issuing online force or velocity commands in real time as a function of the instantaneous robot state.
    % The simple, computationally efficient artificial potential field approach to real-time obstacle avoidance \cite{Khatib1986APF} may fall into local minima and cannot escape by itself.\cite{Koditschek1987LM} This local obstacle avoidance method yield safe robot navigation algorithms but offer no assurance of convergence to a designated goal location. The method to design artificial potential fields via harmonic functions \cite{Connolly1992} can eliminate local minima, but it is difficult to construct harmonic functions in real scenes. Navigation functions \cite{Koditschek1990} solves the local minima problem in more general situations, requiring complete global information. The adaptive navigation function method \cite{Filippidis2011} continuously computes and adjusts the navigation function in the neighborhood according to the currently encountered obstacles. This improvement can reduce the dependence on global information needed by navigation functions. \cite{Paternain2020} Adding stochastic extension with provable convergence properties, reactive methods that may permit its implementation in settings where only local, noisy sensor information is available. Reactive planning methods are computationally efficient and sensor-friendly, while non-smooth, oscillatory trajectories are often generated by them. Our work inherits the advantages of reactive methods and aims at solving their defects.
 
	% A variety of deep neural networks architectures have been proposed in recent years to improve multi-robot motion planning. Contrary to supervised learning, where the learner structure gets examples of good and bad behavior, the reinforcement learning agents discover by trial and error how to behave to get the most reward.\cite{sutton2018reinforcement} The original concept using a value-based method is the Deep-Q Learning Network (DQN) introduced in \cite{MnihDQN}. The DQN method trains the artificial neural network to select the action with the highest value in discrete action space. Proximal Policy Optimization (PPO) \cite{schulman2017proximal} is the state-of-the-art reinforcement learning algorithm, which outperforms other online policy gradient methods. The PPO algorithm trains and optimizes neural network in continuous action space. Compared with existing proactive and reactive motion planning methods, reinforcement learning generates human-like control laws based on experience without too much model information. Hence, Deep RL can be enhanced to guide multi-robots move socially by extending neural networks. \cite{chen2019ICRA, chen2017IROS, kretzschmar2016IJRR} Experience-based training gathers a quantity of information to handle different environments and situations, but trials can give no guarantee to a successful plan, which can be solved in our proposed design combining deep RL with others. Zhang et al. \cite{ZhangZheng2022} combined D3QN and artificial potential field to implement a multi-robot pursuit method. The combination of reinforcement learning and artificial potential field method can form an intelligent algorithm that is both safe and adaptable in different environments.
	
	\section{Preliminaries}
 \label{sec:preli}
 % No communication is assumed among robots, so
	\subsection{Problem Formulation}\label{sec:problem_formulation}
In this paper, the motion planning problem is considered for $N$ robots navigating to their preassigned goals in the same environment with obstacles ($N\geq1$). The total set of robots is defined to be $\mathcal{V}=\left\{1,\;2\,\ldots,N\right\}$. All robots share the same space with different goal positions, so there might be conflicts for robots to directly move towards their respective goals.  The environment is partially observable for each robot. All robots move at a desired speed $v$. They can only change their heading angles for both reaching goals and avoiding collision \footnote{Note that the proposed framework can be easily extended to the case with a changing desired speed.}. Such a task is challenging, as robot only has one input to change its motion for two tasks in cluttered multi-robot environments.  This paper is the upper-level motion planning, so a well-designed inner-loop controller is assumed to be implemented on the robot.  Hence, a first-order point-mass robot is considered for a robot $\forall$ $i\in\mathcal{V}$ as below. 
    \begin{equation}
        \dot{\boldsymbol{p}}_i = \boldsymbol{v}_i 
        \label{eq:SysDyn}
    \end{equation}
where $\boldsymbol{p}_i\in\mathbb{R}^m$ is the robot position vector with $m\geq2$, and $\boldsymbol{v}_i\in\mathbb{R}^m$ is the velocity vector.


    % Figure environment removed
 % The problem background (relative distance $d_{j,i}$, the bearing $\phi_{j,i}$, and the heading $\Phi_{j,i}$ of robot $j$ are accessible to robot $i$. But the information of robot $k$ can not be observed by robot $i$ because robot $k$ is not within the perception range $d_r$ of robot $i$. Each robot can also observe the nearest obstacle.

As shown in Fig. \ref{fig:problem_formulation}, all robots of interest in the environment are homogeneous, which has a safe radius of $r$.  Let $d_{o, i}$  be the distance from the robot $i$ to the surface of an obstacle, and $d_{j, i}= \|\boldsymbol{p}_j-\boldsymbol{p}_i\|$ be the distance between the robots $j$ and $i$ as in Fig. \ref{fig:problem_formulation}. Hence, collisions will occur if $d_{o, i}<r$ or $d_{j,i}<2r$. We use $\phi_{o,i}$ to denote the azimuth angle of an obstacle in the local frame of the robot $i$ as shown in Fig. \ref{fig:problem_formulation}. Similarly, $\phi_{j,i}$ is the azimuth angle of a robot $j$ in the local frame of the robot $i$. The relative heading angle of a robot $j$ to robot $i$ is specified by $\psi_{j,i}$.  Assume that exteroceptive sensors are available for each robot $i$ to observe the environment, \emph{e.g.}, LiDAR or visual sensors.  The detection or sensing range is represented by $d_r$ for each robot as illustrated in Fig. \ref{fig:problem_formulation}. Hence, based on the exteroceptive sensors, an robot $i$ is able to obtain all necessary environment states in its detection range for planning, including obstacle information \{$d_{o, i}$, $\phi_{o,i}$\} and neighouring robot information \{$d_{j,i}$, $\phi_{j,i}$, $\psi_{j,i}$\}. The goal position for an robot $i$ is represented by $\boldsymbol{p}_g$, so $d_{g,i}=\|\boldsymbol{p}_{g,i}-\boldsymbol{p}_i\|$. In summary, the objective of this paper is  formulated as 
\begin{equation} \label{eq:Obj}
\left\{\begin{array}{l}
     \lim_{t\to\infty} d_{g,i}(t)<r \text{ with }  d_{g,i}(0)>0   \\
     d_{j,i}(t)>2r \quad \forall\; j\in\mathcal{V}, \text{ } t>0, \text{ \& } j\neq i \\
     \text{No collision with any obstacle}
\end{array}
\right.
    \forall\; i\in \mathcal{V}
\end{equation}

% the observations of each robot $i$ are the relative distance $d_{j,i}$, the bearing angle $\phi_{j,i}$, and the heading angles $\Phi_{j,i}$ of other robots $j$ within the perception range $d_p$. The relative distance $d_{o,i}$ and the corresponding bearing $\phi_{o,i}$ of the nearest obstacle are also observable. The following assumptions are further made. 
% 	\begin{enumerate}
% 		\item The robots have the radius of $r$. It implies collisions occur if the distance between the robot and obstacles is less than $r$ or the distance between two robots is less than $2r$.
% 		\item The robots move at a desired speed $v$. The decisions to be made only include the steering command. 
%             \item The robots are aware of their own goals throughout the task.
% 	\end{enumerate}
 % {zz: Check the symbol system to avoid symbol abuse. Revise Fig.2 by adding all the above symbols correctly. From now on, $i$ denotes the discussed robot, $j$ and $k$ denote its neighbors. Revise Fig.2 accordingly.}
 
% {The dynamics of robot is simplified as a central mass point model.}
%     \begin{equation}
%         \dot{\boldsymbol{p}} = \boldsymbol{v}
%     \end{equation}
% where $\boldsymbol{p}$ denotes the position of robots, and $\boldsymbol{v}$ denotes their velocity.

	
 \subsection{Reinforcement Learning}\label{sec:RL}
 % The motion planning problem can be formulated as a Markov Decision Process (MDP) described by a tuple 
 In RL, state transitions of environments are formulated as a Markov Decision Process (MDP) that is denoted by a tuple
$(\mathcal{S},\mathcal{A},\mathcal{P},{R},\gamma)$, where
	\begin{itemize}
		\item $\mathcal S$ is the state space with $s_t \in \mathcal S$ denoting the environment state at the timestep $t$.
		\item $\mathcal A$ is the action space with $a_t \in \mathcal A$ being the action executed at the timestep  $t$.
		\item $\mathcal{P}$ is the state-transition probability function.
		\item ${R}$ is the reward function; 
		\item $\gamma \in \left(0,1\right)$ is a discount factor balancing the instant reward with future reward.
	\end{itemize}
 % $r_t \sim \mathcal R (s_t)$denotes the reward obtained by an agent when it's in the state $s_t$.
 % $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{P}$ is the state-transition model, $R$ is the reward function, $\gamma$ is the discount factor. 
Let $\pi(a|s)$ be the stochastic policy in RL with $\pi(a|s) : \mathcal{S}\times \mathcal{A} \to \left[0\; 1\right]$. The expectation of the accumulated reward $R_t$ is 
 \begin{equation}
     \mathbb{E} [\mathcal{R}_t]= \mathbb{E}_{a\sim\pi, s\sim\mathcal{P}}[\sum_{t=k}^T\gamma^{t-k} R_t] \label{eq:ReturnRt}
 \end{equation}
  where $a_t\sim\pi(a_t|s_t)$, $s_{t+1}\sim \mathcal{P}(s_{t+1} | s_t,a_t)$, and $T$ is the task horizon.
 The objective of RL is to learn an optimal policy $\pi^*(a|s)$ that maximizes the expectation of accumulated rewards $\mathbb{E}[\mathcal{R}_t]$ in \eqref{eq:ReturnRt}. The maximization problem can be resolved using diverse methods. In this paper, we adopt the well-known proximal policy optimization (PPO) algorithm \cite{schulman2017proximal}. Assume that the policy $\pi(a|s)$ is approximated using a deep neural network with the parameter set $\theta$, so the parameterized policy is denoted by $\pi_\theta(a|s)$. In PPO, the policy parameter is optimized by maximizing a surrogate objective as below. to limit the update of parameters into the trust region as follows \cite{schulman2017proximal}.
 \begin{equation}  \label{eq:SurrObj}
 \begin{array}{ll}
      L^{CLIP} =& \mathbb{E}\left[\min\left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t, \right.\right.\\
      & \left.\left.{\rm clip}(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon)A_t\right)\right]
 \end{array}
 \end{equation}
 where $A_t$ is the advantage function and $\epsilon$ is the clip parameter. More details can be found in \cite{schulman2017proximal}.
%  $q_t$ is defined as 
%  \begin{equation} 
%      q_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
%  \end{equation}
%  % Let $\pi_\theta(a|s)$ be a stochastic policy where $\theta$ is the parameters. The objective of RL is to find the optimal policy that maximizes the expectation of discounted accumulated reward.
%  % \begin{equation}
%  %     \max_\theta \mathbb{E} [\mathcal{R}_t]=\max_\theta \mathbb{E}_{a\sim\pi, s\sim\mathcal{P}}[\sum_{t=k}^T\gamma^{t-k} R_t] \label{eq:ReturnRt}
%  % \end{equation}
%  % where $T$ is the task horizon. 
%  Towards this objective, policy gradient methods perform stochastic gradient ascent on the accumulated reward with respect to $\theta$ \cite{2016Asynchronous}. The gradient can be written as 
%   \begin{equation} \label{eq:policy_gradient}
%      \nabla_\theta \mathbb{E} [R_t]=\mathbb{E} [\sum_{s_t,a_t} \nabla_\theta \log \pi_\theta (a_t|s_t)Q_{\pi_\theta}(s_t,a_t)]
%  \end{equation}
%  where $Q_{\pi_\theta}(s_t,a_t)$ is the action value of the state-action pair $(s_t,a_t)$. However, directly optimizing the accumulated reward with (\ref{eq:policy_gradient}) usually leads to a large update on the policy $\pi_\theta$. Since the gradient is estimated locally, the large update may result in policy collapse. Therefore, PPO maximizes a surrogate objective to limit the update of parameters into the trust region as follows \cite{schulman2017proximal}.
%  \begin{equation} 
%      L^{CLIP} = \mathbb{E}[\min(q_tA_t, {\rm clip}(q_t, 1-\epsilon, 1+\epsilon)A_t)]
%  \end{equation}
%  where $A_t$ is the advantage function and $\epsilon$ is the clip parameter. $q_t$ is defined as 
%  \begin{equation} 
%      q_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
%  \end{equation}
%  To calculate the advantage function for state-action pairs, the state value function $V_{\theta}(s)$ is fit via regression to target values.
%  \begin{equation} 
%      L_t^{VF} = (V_{\theta}(s_t) - V_t^{targ})^2
%  \end{equation}
%  where $V_t^{targ}=r_t+\gamma V_{\theta_v}(s_{t+1})$. In addition,  PPO introduced an entropy bonus $S[\pi_\theta](s_t)$ to prevent a premature policy. Overall, the integrated objective used can be represented as 
%  \begin{equation} 
%      L = \mathbb{E}[L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2S[\pi_\theta](s_t)]
%  \end{equation}
% where $c_1$ and $c_2$ are hyperparameters to adjust the importance of each term.
 
 % \begin{equation*}
 %     V^{\pi}(s_t)=\sum_{a_t}\pi(a_t|s_t)\sum_{s_{t+1}}\mathcal{T}(s_{t+1}|s_t,a_t)\big(r+\gamma V^{\pi}(s_{t+1}) \big) 
 % \end{equation*}$
	% Reinforcement learning has two fundamental concepts: an agent and an environment. The agent gives action instructions according to the state of the current environment, and the environment generates a certain reaction after receiving the action instructions, thereby generating a new state at the next moment.The agent generates actions through a certain strategy $\pi(a|s)$, and the actions in the environment will affect the state $s_{t+1}$ of the next moment according to the state transition function $f(s_t,a_t)$
	
	% A reinforcement learning problem can be written as a standard Markov decision process described by the six-tuple $\langle \mathcal S, \mathcal A, \mathcal T, \mathcal R, \mathcal \gamma \rangle$, where:
	% \begin{itemize}
	% 	\item $\mathcal S$ denotes state space, a set of all environment states. $s_t \in \mathcal S$denotes the state of an agent at time $t$.
	% 	\item $\mathcal A$ denotes action space, a set of all actions of an agent.$a_t \in \mathcal A$denotes the action taken by an agent at time $t$.
	% 	\item $\mathcal T$ denotes transfer function, $s_{t+1}\sim f(s_t,a_t)$denotes the probability that the agent performs an action $a_t$ in the state $s_t$ and transfers to the next state $s_{t+1}$
	% 	\item $\mathcal R$ denotes reward function, $r_t \sim \mathcal R (s_t)$denotes the reward obtained by an agent when it's in the state $s_t$.
	% 	\item $\gamma$ is discount factor.
	% \end{itemize}
	% The goal of reinforcement learning is to learn an optimal strategy $\pi^*(a|s)$, so that the cumulative benefit $R_t = \sum_{t=k}^{T} \gamma^{j-t}r_t$ is expected to be the largest, where $s$ is the state of the agent (local observations), $a$ is the action of the agent, and $r_t$ is the reward value of the action frame at time $t$.
	
	% In order to find the optimal policy, an action-value function is introduced to measure how good an action is. The action-value function $Q^\pi (s,a)$ is the reward after executing the action $a$ in the current state $s$. The larger the action-value function, the better the action; the smaller the action-value function, the worse the corresponding action. In any state, as long as the action value function of each action is calculated, the optimal action can be found.
 %    \begin{equation}
	% Q^\pi (s,a) = \mathrm{E}[R_t|s_t=s,a_t=a,\pi]
 %    \end{equation}
	
	% The expected return $Q^*(s,a)$ of the optimal strategy $\pi^*$ is the highest among all strategies.
 %    \begin{equation}
	% Q^*(s,a) = \max{\mathrm{E}[R_t|s_t=s,a_t=a,\pi]}
 %    \end{equation}
	
	% The above formula is called the optimal action-value function. The reward is the accumulation of rewards, and the rewards at two adjacent moments differ only by the reward value of one moment, so the action value function of two adjacent moments is also the reward value that differs only by one moment. Write down the state and action at a moment as $s'$ and $a'$, and the action-value function can be written in a recursive form, called the Bellman equation
 %    \begin{equation}
 %        Q_{i+1}(s,a) = \mathrm{E}[r+\gamma\max_{a'} Q_i(s',a')|s,a]
 %    \end{equation}
	
	% When the exact action-value function has not been found, the currently obtained action-value function can still be used instead of the exact action-value function for iteration, which is the idea of the temporal difference. After approximate substitution, enough iterations can make $Q(s,a)$ converge to $Q^*(s,a)$. The iteration method is as follow
 %    \begin{equation}
 %    Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [r_{t+1} + \gamma \max_a Q(s_{t+1},a) - Q(s_t,a_t)]
 %    \end{equation}
	
	% Use the deep neural network to fit the state action value function $Q(s,a)$ in reinforcement learning to select the optimal strategy, that is, get the deep Q network (DQN). The loss function for updating the parameters of a deep neural network is a squared error function
	% $$L(\theta_i) = \mathrm{E}[(Y_i-Q(s,a|\theta_i))^2]$$
	
	
	
\subsection{Artificial Potential Field}\label{sec:APF}
Artificial potential field methods guide robots to their predefined goals by the combination of multiple forces. The attractive force exerted on robot $i$ is defined as follows.
 % The total field is composed of an attractive field, a repulsive field, and an individual field.
	% \begin{equation}
	% \boldsymbol{F}_i = \boldsymbol{F}_{a,i} + \boldsymbol{F}_{r,i} + \boldsymbol{F}_{in,i}
	% \end{equation}
	\begin{equation}
	\boldsymbol{F}_{a,i} = \frac{\boldsymbol{p}_{g,i} - \boldsymbol{p}_i}{d_{g,i}} \label{eq:AttrForce}
	\end{equation}
% where $\boldsymbol{p}_i$ is the position of the robot $i$ and $\boldsymbol{p}_{g,i}$ is the position of the goal of robot $i$.
Accordingly, the repulsive force from an obstacle is
		\begin{equation}
            \label{eq:repulsive_force}
		\boldsymbol{F}_{r,i} = \left\{
		\begin{aligned}
		&\eta\left( \frac{1}{d_{o,i}} - \frac{1}{\rho} \right) \frac{\boldsymbol{p}_i - \boldsymbol{p}_{o,i}}{d_{o,i}^3}&\text{if } d_{o,i} \leq \rho\\
		&0 &\text{if } d_{o,i} > \rho
		\end{aligned}
		\right.
		\end{equation} \label{eq:RepForce}
	% \begin{footnotesize}
	% 	\begin{equation}
 %            \label{eq:repulsive_force}
	% 	\boldsymbol{F}_{r,i} = \left\{
	% 	\begin{aligned}
	% 	&\eta\left( \frac{1}{||\boldsymbol{p}_{o,i}-\boldsymbol{p}_i||} - \frac{1}{d_{o,i}} \right) \frac{\boldsymbol{p}_i - \boldsymbol{p}_{o,i}}{{||\boldsymbol{p}_i - \boldsymbol{p}_{o,i}||}^3}&\text{if} ||\boldsymbol{p}_i - \boldsymbol{p}_{o,i}|| \leq \rho\\
	% 	&0 &\text{if} ||\boldsymbol{p}_i - \boldsymbol{p}_{o,i}|| > \rho
	% 	\end{aligned}
	% 	\right.
	% 	\end{equation} \label{eq:RepForce}
	% \end{footnotesize}
 where $\eta$ is the scale factor. $\boldsymbol{p}_{o,i}$ is the position of the nearest obstacle to robot $i$. $\rho$ is the influence range of obstacles.
 
The inter-robot force that keeps robots away from each other is  
 \begin{equation} \label{eq:individual_force}
	\boldsymbol{F}_{in,i} = \sum_{j\neq i, j \in \mathcal{N}_i} \left( 0.5 - \frac{\lambda}{d_{j,i}} \right) \frac{\boldsymbol{p}_j-\boldsymbol{p}_i}{d_{j,i}}
	\end{equation}
where $\mathcal{N}_i$ is the set of robots that can be detected by robot $i$, namely $\mathcal{N}_i=\left\{j\vert\; \forall\;j\in\mathcal{V}\text{, }j\neq i\text{, \& }d_{j,i}<d_r\right\}$, $\lambda$ is a parameter adjusting the compactness of the multi-robot system. The smaller $\lambda$ is, the more 
compact the system is \cite{ZhangZheng2022}. 
The resultant force, therefore, is 
\begin{equation}
	\boldsymbol{F}_i = \boldsymbol{F}_{a,i} + \boldsymbol{F}_{r,i} + \boldsymbol{F}_{in,i}
\end{equation}
The direction of $\boldsymbol{F}_i$ is used as the commanded heading angle for robot $i$.
\section{Algorithm}\label{sec:alg}
    % Figure environment removed
    % The resultant forces calculated by APF with the aforementioned parameter pair guide robots to their respective goals. The learned policies can be deployed to real quadrotors with low-level PID controller.

The reinforced potential field (RPF) algorithm proposed in this paper integrates attention-based embedding, PPO, and APF. Thereinto, the attention-based embedding transforms the observations into fixed-length vector according to the importance of neighboring robots and obstacles to the current decision-making process. Outputs of the attention-based embedding are fed into Multiple-Layer Perceptron (MLP) network to generate reasonable choices for a modified APF with a soft wall following mechanism. Therefore, we will describe the proposed algorithm from the aforementioned three aspects in this section. 
    % The reinforced potential field method combines PPO, APF and attention mechanism. PPO need embedded information of environments before computing an output. Our approach introduces attention mechanism to embed information into array with fixed length. PPO requires the embedded information and produces actions via policy network. The actions are passed to APF module to adjust parameters $\lambda$ and $\eta$. APF module generates trajectories, which are followed by quadrotors with low level PID controller. When flying to their goal, quadrotors observe the environment and obtain the relative position of obstacles and other quadrotors. The observed information is then feed to attention embedding module for the decision of next time, completing a closed loop. The functions of mentioned module is listed as follow:
    % \begin{enumerate}
    %     \item \textbf{Attention Embedding}: Embed information into array and feed the array to PPO as states.
    %     \item \textbf{PPO}: Reinforcment learning algorithm. The trained policy produce actions based on current states. The optimized actions are passed to APF module as potential field parameters.
    %     \item \textbf{APF}: With the extension of soft wall following, APF generates a smooth trajectoty.
    % \end{enumerate}
    
\subsection{Attention-based Embedding}
In fully-connected neural networks, the input size is required to be fixed. However, in a multi-robot scenario, the number of neighbours of a robot $i$ would be dynamically changing, so it would be inefficient to use a fully-connected neural network to encode surrounding environments. To tackle this problem, the mean embedding methods are employed by averaging all the information of observed neighbors \cite{huttenrauch2019deep}. However, it is obvious that the information from different neighbors contributes differently to the current decision-making process in a sophisticated multi-robot systems, \emph{e.g.}, gregarious animals. However, the pure average in the  mean embedding methods may underrate the information of a significant neighbor. Therefore, we introduce the self-attention mechanism into the observation embedding process in this paper to explicitly characterize the relative importance of surrounding observations, namely robot-robot interactions and robot-environment interactions.

The observations of each robot is firstly split to two parts, namely $\boldsymbol{o}_{i}=\left\{\boldsymbol{o}_{loc,i},\boldsymbol{o}_{nei,i}\right\}$. The first part $\boldsymbol{o}_{loc,i}$ includes the relative distance and azimuth angles of the nearest obstacle and the goal information, so $\boldsymbol{o}_{loc,i}=\left[d_{o,i}, \phi_{o,i},d_{g,i},\phi_{g,i}\right]^T$. The second part $\boldsymbol{o}_{nei,i}$ consists of the information of observed neighbors $\boldsymbol{o}_{nei,i}=\left\{\boldsymbol{w}_j|\forall\; j \in \mathcal{N}_i\right\}$, where $\boldsymbol{w}_j$ is
\begin{equation}\label{eq:NeighInf}
    \boldsymbol{w}_j = \left[d_{j,i},\phi_{j,i}, \psi_{j, i}\right]^T
\end{equation}
% The agents can observe target information and obstacle information $\boldsymbol{o}_{loc,M}$, including the distance and direction of the target point $d_{o,M},\phi_{o,M}$, The distance and direction $d_{e,M},\phi_{e,M}$ of the obstacle.
	
% Each agent can obtain the relative distance and angle $\boldsymbol{w}_M$ of other agents. Also, agents can get other agents' relative orientation $\varPsi$ from each other. The distance, bearing angle and relative orientation of agent $i$ observed by the M agent is recorded as $d_{i,M}, \phi_{i,M}, \varPsi_{i,M}$.

 
The attention-based embedding then employs two one-layer fully-connected networks to encode $\boldsymbol{w}_j$ into two fixed-length high-dimensional vectors $\boldsymbol{e}_j$ and $\boldsymbol{h}_j$, respectively. Both networks use ReLU activation functions.
    \begin{equation}
	\begin{aligned}
	\boldsymbol{e}_j &= \phi_e(\boldsymbol{o}_{loc,i}, \boldsymbol{w}_j) \\
	\boldsymbol{h}_j &= \Psi_h(e_j)
	\end{aligned}
    \end{equation}
The attention score is calculated by inputting $\boldsymbol{e}_j$ and the mean embedding of all observed neighbors $\boldsymbol{e}_m$ into another one-layer fully-connected network. This network also uses ReLU activation functions.
    \begin{equation}
	\begin{aligned}
	\boldsymbol{b}_j &= \Psi_b (\boldsymbol{e}_j, \boldsymbol{e}_m)
	\end{aligned}
    \end{equation}
where $\boldsymbol{e}_m = \frac{1}{\left| \mathcal{N}_i\right|}\sum_{j \in \mathcal{N}_i}\boldsymbol{e_j}$, where $\left| \mathcal{N}_i\right|$ is the number of observed neighbors by a robot $i$. 
The information of each observed neighbor is weighted according to their corresponding attention scores.
    \begin{equation}
	\boldsymbol{c}_i = \sum_{j \in \mathcal{N}_i}{{\rm softmax}(\boldsymbol{b}_j)\boldsymbol{h}_j}
    \end{equation}
Finally,  $\boldsymbol{o}_{loc,i}$ is concatenated with $\boldsymbol{c}_i$ to represent the observations of robot $i$ with a fixed-length vector $\hat{\boldsymbol{o}}_i$ while highlighting the information from significant neighbors. 

In the conventional artificial potential field methods, the scale factor $\eta$ in (\ref{eq:repulsive_force}) determines the strength of obstacles' influence, while $\lambda$ in (\ref{eq:individual_force}) adjusts the magnitude of individual force exerted from neighbors. Once these parameters are determined, they are invariant throughout the whole task. However, it is more promising to dynamically adjust these parameters online because their optimal values depend on the environment states in one episode. To encode the dynamic environment information into $\eta$ and $\lambda$, the outputs of the attention-based embedding will be fed into an MLP network. The whole framework will be optimized using PPO.

\subsection{Optimization by PPO}

% Therefore, the classical RL algorithm, PPO, is introduced to learn the optimal parameter pair $(\eta,\lambda)$ in different situations.
% The smaller $\lambda$ is, the more compact the agent group is. Take the $(\eta,\lambda)$ parameter pair as the action space, and the corresponding strategy is recorded as $\pi_\theta(\eta,\lambda | s)$
	
% As shown in the Fig. \ref{Fig:Framework}, the policy network takes $\hat{\boldsymbol{o}}_i$, which is the output of attention-based embedding, as input.
The action space has two dimensions corresponding to $\eta$ and $\lambda$, respectively. The range of candidate action pairs is $[0,0.1]\times[0,5]$, which is empirically selected according to the specific scenarios used in Section \ref{sec:exp}. All robots share the same policy network, so the same reward function is used for all robots.  
The total reward function for each robot is defined as 
{
\begin{equation}
	R = R_{m} + R_{s} + R_{o} + R_{p}
	\end{equation}
 }
The reward $R_m$ encourages robots to reach their goals via the shortest trajectory. For a robot $i$, one has
    \begin{equation}
    R_m = \left\{
    \begin{aligned}
        &300 - 100\frac{d_{a}}{d_{s}}  &\text{if }d_{g,i}<r \\
        &0 &\text{otherwise}
    \end{aligned}
    \right.
    \end{equation}
where $d_{s}$ is the Euclidean distance between the start point and the goal position, and $d_{a}$ denotes the trajectory length achieved by the robot $i$. 

The reward function $R_{s}$ encourages robots to move smoothly by giving a punishment of $-5$ when the difference of headings at two adjacent timesteps exceeds $45^\circ$. 

% The reward function $R_{o}$ gives a punishment of $-100$ when a robot collide with neighbors. 

The reward function { $R_o$ } is used to prevent robots from collisions with obstacles. For a robot $i$, one has 
	\begin{equation}
	R_o = \left\{
	\begin{aligned}
	-100,\quad &{\rm if }\  d_{o,i} < r\\
	-20,\quad &{\rm if }\  r \leq d_{o,i} < 2r\\
	0,\quad &{\rm otherwise}
	\end{aligned}
	\right.
	\end{equation}
% where $d_o$ is the relative distance of the nearest obstacle, and $d_p$ is the radius of robots. 
The reward $R_p$ provides a dense reward to accelerate the learning process. For a robot $i$, one has 
    \begin{equation}
        R_p = \left\{
	\begin{aligned}
	1 - \frac{d_{g,i}}{d_m},\quad &{\rm if}\ d_{g,i} < d_m \\
	0,\quad &{\rm otherwise}
	\end{aligned}
	\right.
    \end{equation}
where $d_{g,i}$ is the relative distance of the goal, and $d_m>0$ is a hyperparameter. 

To extend the classical PPO algorithm to multi-robot settings, the parameter sharing technique is employed. That is, all robots share the same policy network. The shared policy network updates parameters using transitions collected by all robots. The overall procedure of the proposed algorithm is shown in Algorithm \ref{Alg:1}.  
% When training the neural network, agents are set to random start points in the training scene shown in figure \ref{fig:train_scene} every episode. Destinations of all agents are generated in a suitably random way, which keeps a distance between destinations and start points. 
   
 
	\begin{algorithm}[htbp]
		\caption{Multi-robot Reinforced Potential Field}
		\label {Alg:1}
		\begin{algorithmic}[1]
         \State {Algorithm initialization}
			\For {episode = $1,2,\cdots,$}
			\State {Reset the environment}
			\For {$t=1,2,\cdots,$}
			\For{$i = 1,2,\cdots, N$ }
			\If {$d_{g,i}>r$ }
			\State {Obtain $(\eta,\lambda)$ from $\pi_\theta(a|s)$}
   % Calculate the parameter pair $(\eta,\lambda)$ according to the policy network	
			\State {Calculate $\boldsymbol{F}_{a,i},\boldsymbol{F}_{r,i},\boldsymbol{F}_{in,i}$ by APF}
            \If {$\boldsymbol{F}_{ar,i}^T\boldsymbol{F}_{a,i} < 0$}
            \State {Choose $n_{1,2}$ as $\boldsymbol{F}_i$}
            \Else \If {$\boldsymbol{F}_{r,i}^T \boldsymbol{F}_{a,i} < 0$}
            \State {Calculate $\boldsymbol{F}_{soft,i}$ as $\boldsymbol{F}_i$}
            \EndIf
            \EndIf
		\State {Move along the direction of $\boldsymbol{F}_i$}
		\State {Get the reward $R$ and observation $\boldsymbol{o}_i$}
            % \State{Get the observation $\boldsymbol{o}_i'$}
		\EndIf
		\EndFor
            % \If {$t \equiv 0 \ (\mathrm{mod} \ Z)$}
            
             \If {$t \ {\mathrm{mod} \ Z} = 0$}
		\State {Update the PPO agent for $K$ epochs}
		\EndIf
            \EndFor
		\EndFor
		\end{algorithmic}
	\end{algorithm}

 %    PPO uses a deep neural network to fit a state-value function mentioned in Sec \ref{sec:RL}. The strategy ratio $q_t(\theta)$ is introduced to represent the change of the strategy during reinforcement learning iterations.
 %    \begin{equation}
	% q_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
 %    \end{equation}
    
 %    The loss function used by the PPO method when training the deep neural network is the sum of the following loss functions
 %    \begin{equation}
 %    L_t^{CLIP+VF+S} = \hat{\mathbb{E}}[L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\theta) + c_2S[\pi_\theta](s_t)]
 %    \end{equation}
	% Where $S$ is the entropy value during iteration. $L_t^{VF}$ is the square error, which makes the deep neural network continuously approach the state value function $V_t^{targ}$
 %    \begin{equation}
 %    L_t^{VF} = (V_\theta(s_t) - V_t^{targ})^2
 %    \end{equation}
	% $L_t^{CLIP}$ is the clipping error. The advantage estimation functions $A_t$ are introduced here to estimate whether the strategy becomes better. If the strategy becomes better, $A_t>0$, the cutting error will be limited within a certain range, thus slowing down the speed of strategy optimization; if the strategy becomes worse, $A_t<0$, the cutting error is a negative value of the limit makes the strategy leave the poor interval quickly.
 %    \begin{equation}
 %    L^{CLIP}(\theta) = \hat{\mathbb{E}}_t[\min(q_t(\theta)\hat{A_t}, {\rm clip}(q_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A_t})]
 %    \end{equation}
	
	% The advantage estimation function used in the training of the PPO method is as follows:
 %    \begin{equation}
 %    \hat{A_t} = -V(s_t) + r_t + \gamma r_{t+1} + \cdots + \gamma^{T-t+1}r_{T-1} + \gamma^{T-t}V(s_T)
 %    \end{equation}
	% After the total loss function $L_t^{CLIP+VF+S}$ is calculated by the above method, the Adam optimizer is used to optimize and iterate the deep neural network, and the deep reinforcement learning model can be trained.
	
	

    \subsection{APF with soft wall following}
    Conventional artificial potential field method mentioned in Sec \ref{sec:APF} {can} be stuck in a location, especially when $\boldsymbol{F}_{r,i} = \boldsymbol{F}_{a,i}$ and $\boldsymbol{F}_{in,i}=0$. In order to tackle this problem, our previous work introduced a wall following rule \cite{ZhangZheng2022}. The wall-following rule means that when the agent is close to the obstacle, it should move along the edge of obstacles. As shown in Fig \ref{fig:wall-following}, $\boldsymbol{n}_1$ or $\boldsymbol{n}_2$ is the better direction for robots rather than $\boldsymbol{F}_{ar, i} =\boldsymbol{F}_{a,i}+\boldsymbol{F}_{r,i}$.
    The wall following rule is activated when the angle between $F_{ar,i}$ and  $F_{a,i}$ exceeds $90^{\circ}$. 
    
    The selection of $\boldsymbol{n}_1$ and $\boldsymbol{n}_2$ depends on the current heading direction and inter-robot force $\boldsymbol{F}_{in,i}$.
    If $\boldsymbol{F}_{in,i}$ exceeds the specified a threshold $\bar{F}_{in,i}$, we select from $\boldsymbol{n}_1$ and $\boldsymbol{n}_2$ the one that has a smaller angle with $\boldsymbol{F}_{in,i}$ as shown in Fig. \ref{fig:wall-following} (a). Otherwise, choose the one with a smaller angle with the current motion direction as the robot $k$ in Fig. \ref{fig:wall-following}.
    % $F_{ar,i}$ represents the resultant force of attraction and repulsion. 
    
	
    % Figure environment removed
	
When the angle between $\boldsymbol{F}_{ar,i}$ and $\boldsymbol{n}_{1}$ (or $\boldsymbol{n}_{2}$) is very large, there might be a sharp turn as the blue trajectory shown in Fig. \ref{fig:wall-following}(b). To solve the problem, we introduce a soft rule to the wall following method. The nearby external area of an obstacle is divided into two sub-areas as shown in Fig. \ref{fig:wall-following}(b). In sub-area A, a robot $i$ chooses $\boldsymbol{n}_1$ or $\boldsymbol{n}_2$ according to the aforementioned wall following rule.  In the sub-area B, the robot chooses a direction $\boldsymbol{F}_{soft}$ between $\boldsymbol{n}_{1\text{ or }2}$ and $\boldsymbol{F}_{ar,i}$. Hence, $\boldsymbol{F}_{soft,i}$ is defined as
    \begin{equation}
	\boldsymbol{F}_{soft,i} = \frac{\boldsymbol{F}_{ar,i} + 2\|\boldsymbol{F}_{r,i}\|  \boldsymbol{n}_{1\text{ or }2}}{\|\boldsymbol{F}_{ar,i} + 2\|\boldsymbol{F}_{r,i}\|  \boldsymbol{n}_{1\text{ or }2}\|}
    \end{equation}
    
    At the exact moment that a robot $i$ enters the sub-area B, $\boldsymbol{F}_{soft,i}$ is the same as $\boldsymbol{F}_{ar,i}$. When agents get closer to obstacle, $\boldsymbol{F}_{soft,i}$ becomes closer to $\boldsymbol{n}_{1\text{ or }2}$.

    The sub-area A and B around an obstacle in Fig \ref{fig:wall-following} are defined as follows: If $\boldsymbol{F}_{ar,i} ^T \boldsymbol{F}_{a,i} < 0$, it belongs to sub-area A; if $\boldsymbol{F}_{r,i}^T \boldsymbol{F}_{a,i} < 0$, it is in sub-area B.
    \begin{equation}
	\boldsymbol{F} = \left\{
	\begin{aligned}
	&\boldsymbol{n}_{1\text{ or }2}, &\boldsymbol{F}_{ar,i} \cdot \boldsymbol{F}_{a,i} < 0 \\
	&\boldsymbol{F}_{soft}, &\boldsymbol{F}_{r,i}^T \boldsymbol{F}_{a,i} < 0\  and \boldsymbol{F}_{ar,i} \cdot \mathbf{F}_{a,i} \geq 0
	\end{aligned}
	\right.
    \end{equation}
	% In places where the agents are close to each other, the artificial potential field also encounters the problem of local minima. For example, after the agent $j$ shown in the figure reaches the goal point, it may block other agents from reaching the goal. Therefore, for one agent, other agents are also regarded as obstacles.

 % Figure environment removed
 
	

 \begin{table}[htbp]
     \centering
     \caption{Traning setup}
     \label{tab:train_params}
     \begin{tabular}{c|c}
          \bottomrule
          Parameters & Values \\
          \midrule
          Initial learning rate $\alpha_0$ & 0.0003 \\
          $\beta$ & 0.999 \\
          Actor neural network &  MLP with 2 hidden layers \\
          &(256 neurons per hidden layer) \\
          Critic neural network &  MLP with 2 hidden layers \\
          &(256 neurons per hidden layer) \\
          Sample batch size $Z$ & 100 \\
          PPO epochs $K$ & 1 \\
          Training episodes & 1000 \\
          Maximum steps per episode & 1000 \\
          $\tau$ & 0.9 \\
          $\gamma$ & 0.999 \\
          Clip parameter $\epsilon$ & 0.2 \\
          $c_1$ & 0.5 \\
          $c_2$ & 0.001 \\
          \bottomrule
     \end{tabular}
 \end{table}
	
	\section{Simulation and experiments}
 \label{sec:exp}
 In this section, numerical simulation and experimental results are presented. The proposed algorithm is trained in two different arenas with six robots as shown in Fig. \ref{fig:train_scene}. The trained results are thereafter evaluated in both numerical simulations with different setups and real-world experiments. 
 
 In the first arena, the environment is occupied with many obstacles as shown in Fig. \ref{fig:train_scene} (a). The obstacles have the same radius of $0.5$ $m$. Both the initial and goal positions of each robot are randomly given. All robots need to reach their goal positions, while avoiding collisions with each other and obstacles. The second training arena is shown in Figure \ref{fig:train_scene} (b). It is an open environment with no obstacles. All robots need to avoid collisions with each other. Initially, all robots are distributed evenly on a circle with a radius of $2$ $m$. The initial positions of all robots are randomly generated. Robots are required to swap their locations.  

 The same setup is chosen for the two aforementioned training arenas. Some training details are summarized in Table \ref{tab:train_params}. In the training, the desired speed of agents is set to $0.5$ $m/s$. The safety radius $r$ of robots is $0.1$ $m$. The perception range $d_p$ of a robot is chosen to be $6$ $m$. The time step at training is set to be $0.1$ $s$. The hyperparameter $d_m$ is chosen to be $10$ $m$. The influence range $\rho$ of an obstacle is chosen $10$ $m$. The wall following threshold $\bar{F}_{in,i}$ is picked to be $1$, $\forall$ $i\in\mathcal{V}$. The learning rate decays as follows. 
\begin{equation}
        \alpha = \alpha_0 \times \beta ^ {j}
\end{equation}
where $\alpha_0$ is the initial learning rate,  $\beta$ is a hyperparameter, and $j$ is the number of the current episode. The values of $\alpha_0$, $\beta$ are given in Table \ref{tab:train_params}. 

The proposed RPF is compared with three different methods, namely RPF without attention, vanilla PPO, and vanilla APF. In the comparison, the proposed RPF in this paper is termed as ``RPF 1''. When the attention module is replaced by pure linear embedding as in our previous work \cite{ZhangZheng2022}, we use the term ``RPF 2'' to represent it. The ``RPF 2'' algorithm is trained based on exactly the same reward functions with ``RPF 1'' using the same training setup. 

For vanilla PPO, we found that it's very difficult to train a policy in the cluttered environment shown in Fig \ref{fig:train_scene} (a). For comparison, we train the vanilla PPO policy in an environment with smaller obstacles. The training environment would be easier than the one used in ``RPF 1'' and ``RPF 2''. Hence,  the radii of all obstacles are set to be $0.1$ $m$. The locations of the obstacles are the same with those in Fig \ref{fig:train_scene} (a). Other learning configurations are the same  with those as ``RPF 1''  as given in table \ref{tab:train_params}. The decision of PPO robot is generated as follows.
\begin{equation}
    \boldsymbol{F}_i = \frac{\boldsymbol{v}_i + a_t \boldsymbol{v}_{i\perp}}{\|\boldsymbol{v}_i + a_t \boldsymbol{v}_{i\perp}\|}
\end{equation}
where $\boldsymbol{v}_{i\perp}$ is a vector perpendicular to $\boldsymbol{v}_i$, and $a_t$ is the output by the PPO policy with $a_t\in[-2.5, 2.5]$.

We choose the parameters of a vanilla APF to be $\eta=0.05$ and $\lambda=2$.  For a fair comparison, we choose two metrics to evaluate the performance of all methods in the simulation. 

The \textbf{traveling distance} metric $l_i$ evaluates the average total traveling length by a robot, which is defined as below.
 \begin{equation}
   l_i = \sum_{i \in \mathcal{V}}\sum_{t}^T ||\boldsymbol{v}_i(t)||\Delta t
   \label{eq:distance}
 \end{equation}
  where $T$ represents the total timesteps for robots to reach their goal. The large the traveling distance metric is, the poor the algorithm's performance is. 

The \textbf{motion smoothness} metric $\xi_i$ evaluates how oscillatory the motion of a robot, which is defined as 
    \begin{equation}
        \xi = \frac{\sum_{i\in\mathcal{V}} \sum_t^T ||\Delta \boldsymbol{v_i}|| / ||\boldsymbol{v_i}||}{T}
        \label{eq:change_angle}
    \end{equation}
Similarly, the large the  motion smoothness metric is, the poor the performance of an algorithm is. 
  % Critical vibration appears when agents turn too fast, which is incompatible with real control system. The vibration can be reflected by total change of steering angle. 

 % In the 
 % Both the first and second training arena are typical scenarios used to test and ,  with  compared trained in simulation 
 \subsection{Simulation Evaluation} \label{subsec:sim}
 In the simulation evaluation, the trained proposed algorithm is compared with the other three algorithms in three different scenarios to show its efficiency and generalization.  In the first two scenarios, open environments with no obstacles are considered. In the second scenario, we built up a new evaluation arena with different obstacle distributions. All simulations and comparisons are given in Fig. \ref{fig:compare_circle} - \ref{fig:compare_lab}. Both the traveling distance and motion smoothness metrics in Figs. \ref{fig:compare_circle} and \ref{fig:compare_lab} are averaged among all robots to evaluate the whole performance of the algorithms in a multi-robot setting.

% Figure environment removed
 In the first evaluation, we consider $8$ robots distributed on a circle with a radius of $3$ $m$ and swap locations with robots on the other side. This scenario is very similar to our second training arena but with a different number of robots and a different size of circle. The robot trajectories are shown in Fig. \ref{fig:compare_circle}. In this scenario, PPO has the best performance in by both the traveling distance metric and the motion smoothness metric as shown in Fig. \ref{fig:compare_circle}. The APF method has the worst performance by both metrics. The $RPF$ 1 (with the self-attention module) performs better than $RPF$ 1 (with no self-attention). It is that it is necessary to rank the importance of surrounding information. 

 % Figure environment removed
 Although the vanilla PPO has the best performance in the first scenario, it cannot generalize well to a scenario with more difference as shown in Figs. \ref{fig:ppo_fail} and \ref{fig:compare_lab}. In the second evaluation in Figs. \ref{fig:ppo_fail}, we further compare our method $RPF$ 1 with the vanilla PPO in a different scenaro. In this case, $8$ robots are initially distributed on a circle with a radius of $8$ $m$ that is large than the detection range ($6$ $m$) of a robot. The same position swap task is performed. In such a scenario, the vanilla PPO failed to complete the task safely.
 % are considered
 %    We built two simulation scenes different from the training scenes to evaluate the performance of RPF, both two scenes have different obstacle allocations and different numbers of robots. The first scene is a typical circle exchange scene, where 8 robots are distributed on a circle and move to the opposite side. In the circle exchange scene, there are conflicts at the circle's center for all robots. 
 %    The second scene contains dense obstacles, designed for evaluating collision avoidance ability. It also corresponds to the environment of our laboratory, so the simulation results have reference value for real-world experiments. 
 %    The simulation results are shown in Fig \ref{fig:compare_lab} and Fig \ref{fig:compare_circle}.

 

 In the third scenario, we further evaluate the performance of all four algorithms in a different environment. In this scenario, the second robot collides with an obstacle, thus leading to a mission failure. Due to the mission failure of the second robot, the PPO algorithm has small values by both the distance metric and the motion smooth metric. In comparison with PPO, both $RPF$ 1 and $RPF$ 2 show better generalization, but  $RPF$ 1 has much improvement by both the traveling distance and motion smoothness metrics. It further demonstrates the efficiency of the introduced attention-based observation embeddings. 
 
 % has better performance in both
 % In the first scene, the motion smoothness of RPF is significantly better than APF. As for traveling distance, the four methods don't vary too much. The attention mechanism can process the observations more precisely than mean embedding, so ``RPF 1" outperforms ``RPF 2" in both two scenes. PPO performs perfectly in the circle exchange scene, but it's just a local optimum and may fails its mission in scenes with big difference from training scenes, as shown in \ref{fig:compare_circle}. It's common for deep learning to fall into local optimum when the state space isn't large enough. Another local optimum also appeared when training: robots stay at where they are to avoid collision, refusing to reach their goal. Taking the universality into consideration, RPF is better than PPO in cluttered environments. 


 
  % Figure environment removed

 % In the second scene, the traveling distance and the motion smoothness of ``RPF 1" and ``RPF 2" are significantly better than APF, which shows the role of RL. Also, ``RPF 1" outperforms ``RPF 2" in terms of distance and smoothness, which reflects the role of the attention mechanism. In the simulation of PPO, robot 3 runs into an obstacle and failed to reach its goal, so the traveling distance of robot 3 can not be calculated correctly. The failure of PPO is not because the learning hasn't be convergence. The PPO converges to a local optimum, because the dimension of state space is not large enough. PPO learn policy only by trial and experience, which doesn't provide a guarantee for obstacle avoidance. Moreover, the universality of PPO is not as good as RPF, and there is huge difficulty in training a PPO policy.


 \subsection{Experiment Evaluation}
 % Figure environment removed
 Experiment evaluation is performed to demonstrate the competence of the proposed algorithm in a real-life platform. A similar environment to the third simulation in Section \ref{subsec:sim} as shown in Fig \ref{fig:compare_lab}. In the experiment, we apply the RPF 1 algorithm to Crazyflie quadrotors without any modification. The OptiTrack optical motion capture system is used to measure the states of quadrotors. The Crazyflie drones can track position commands from the high-level planner, using the implemented inner-loop controller. 

 Three different experiments are performed with a different number of robots as shown in Fig \ref{fig:real_exp}. A robot would change its trajectory if more robots are considered in the environment. The experiment also illustrates the generalization of the proposed algorithm to real-life systems. 
 
 % The experiment result is shown in Fig \ref{fig:real_exp}. When there is no conflict between drones, they can find the optimal path to their goal. When there are two drones in conflict, they adjust their decision to avoid collision. When the environment is filled with three drones, all of them are able to go smoothly to their goal. 

\section{Conclusion} \label{sec:Conclusion}
    Reinforced potential field, a novel motion planning algorithm integrating deep RL with APF, was presented in this paper. The proposed design was able to make active responses to dynamic changes in surrounding environments. An observation embedding with self-attention mechanism was developed to implicitly environment dynamic changes. A soft wall-following rule was presented to further improve the motion smoothness performance.
    The performance improvement of our algorithm over existing benchmarks, including PPO and APF, was demonstrated via numerical simulations. Real experiments were also performed to show the competence of the proposed method in real systems. In future works, we aim to integrate real sensor observations into our method for implementation in more diverse scenarios.
	
	\bibliography{references.bib}
	\bibliographystyle{IEEEtran}
	
\end{document}



