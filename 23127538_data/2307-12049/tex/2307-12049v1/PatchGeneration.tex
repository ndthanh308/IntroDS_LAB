% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{mathrsfs}
\usepackage{diagbox}
\usepackage{array}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\usepackage{cuted}


\newcommand{\tabincell}[2]{
\begin{tabular}{@{}#1@{}}#2\end{tabular}
}

\definecolor{stage_one}{RGB}{254,209,160}
\definecolor{stage_two}{RGB}{206,238,220}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach}

\author{Cheng Wen\textsuperscript{1}  \quad\quad  Baosheng Yu\textsuperscript{1}  \quad\quad  Rao Fu\textsuperscript{2} \quad\quad  Dacheng Tao\textsuperscript{1}\\
\textsuperscript{1}The University of Sydney, Australia \\
\textsuperscript{2}Inria, France
% School of Computer Science, The University of Sydney, NSW 2008, Australia\\
% {\tt\small cwen6671@uni.sydney.edu.au; baosheng.yu@sydney.edu.au; dacheng.tao@gmail.com} 
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
A generative model for high-fidelity point clouds is of great importance in synthesizing 3d environments for applications such as autonomous driving and robotics. Despite the recent success of deep generative models for 2d images, it is non-trivial to generate 3d point clouds without a comprehensive understanding of both local and global geometric structures. In this paper, we devise a new 3d point cloud generation framework using a divide-and-conquer approach, where the whole generation process can be divided into a set of patch-wise generation tasks. Specifically, all patch generators are based on learnable priors, which aim to capture the information of geometry primitives. We introduce point- and patch-wise transformers to enable the interactions between points and patches. Therefore, the proposed divide-and-conquer approach contributes to a new understanding of point cloud generation from the geometry constitution of 3d shapes. Experimental results on a variety of object categories from the most popular point cloud dataset, ShapeNet, show the effectiveness of the proposed patch-wise point cloud generation, where it clearly outperforms recent state-of-the-art methods for high-fidelity point cloud generation. The code is available at \url{https://github.com/wenc13/PatchGeneration}
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
With the rapid development of depth sensing and laser scanning technologies~\cite{zhang2012microsoft,keselman2017intel}, 3d data has become more and more popular for modeling scenes and objects in real-world environments, especially in the applications such as autonomous driving~\cite{nagy2018real} and virtual reality~\cite{wirth2019pointatme}. As a standard 3d acquisition format, the point cloud is a compact geometric representation, which is simple and useful for understanding geometric shape structures of complex objects and large-scale scenes in real-world applications. However, it is non-trivial to collect and annotate large-scale point clouds in many real-world applications. Therefore, with the recent development of deep generative models~\cite{goodfellow2014generative}, point cloud generation has received increasing attention from the community, especially in the related point cloud tasks such as shape completion~\cite{huang2020pf,cai2022learning} and synthesis~\cite{achlioptas2018learning,tang2022warpinggan,hoogeboom2022equivariant}.

Recently, deep generative models have shown great success in generating realistic 2d images from complex distributions by using generative adversarial networks (GANs)~\cite{goodfellow2014generative}, variational autoencoders (VAEs)~\cite{kingma2013auto} and diffusion models~\cite{ho2020denoising}. Though remarkable progress has been made in generating 2d images, point cloud generation still remains very challenging due to the irregular sampling patterns in 3d space, especially for complex object shapes/structures~\cite{achlioptas2018learning,valsesia2018learning,shu20193d,yang2019pointflow,hui2020progressive,cai2020learning,kim2020softflow,klokov2020discrete,wen2021learning,li2021sp,luo2021diffusion,zhang2021learning,tang2022warpinggan}. Motivated by that complex shapes can also be composed from a set of geometry primitives (or meta shapes), we thus propose to explore the great potential of patch-wise point cloud generation using a divide-and-conquer approach. We show an intuitive example of patch-wise point cloud generation in Fig.~\ref{fig.teaser}. Specifically, the overall point cloud generation process is first divided into a set of patch generation tasks, where the learnable 2d patch priors are used to generate point cloud patches. After that, those generated patches are combined into a single point cloud as the final point cloud. The intuition behind such a patch-wise generation is that we consider the 3d shape to be constructed by a set of geometry primitives learned by networks.

% Figure environment removed

To jointly learn patch priors and generate point clouds, the overall patch-wise point cloud generation framework follows the widely used VAE-GAN~\cite{larsen2016autoencoding}. To learn patch priors, each patch prior is a set of 2d points that are randomly initialized and then learned by deep neural networks during training. With a random latent code, the patch generator transforms each patch prior into a set of 3d points (or a point cloud patch), and the union of these 3d point sets will form the target point cloud. Specifically, during training, the input of each patch generator is a concatenation of the latent representation of training sample and the 2d patch prior. During testing, we use the concatenation of a randomly sampled latent code and the learned 2d patch prior as the input of each patch generator. Furthermore, to develop effective patch generators by enabling the information flow in different patches, we introduce transformers to patch-wise point cloud generation to explore local and global relationships, i.e., different points in each patch and different patches. Specifically, we explore three different types of patch generators: 1) the mlp generator (or MLP-G) as the baseline; 2) the transformer generator (or PointTrans-G), which uses transformers to explore the point-wise relationship in each patch; and 3) the dual-transformer generator (or DualTrans-G), where transformers are used to explore both point-wise and patch-wise relationships.

Previous works~\cite{groueix2018papier,deprelle2019learning,williams2019deep} also adopt the multi-patch strategy, but all of them focus on shape analysis tasks (i.e., reconstruction and matching) rather than generation. For example,  AtlasNet~\cite{groueix2018papier} is always compared with other works~\cite{luo2021diffusion} to evaluate the reconstruction quality of auto-encoders. By contrast, our patch-wise method is actually based on point clouds, and we use a VAE-GAN in this paper as our baseline. In this paper, our main contributions can be summarized as follows:
\begin{enumerate}
\item We introduce a novel divide-and-conquer approach for point cloud generation, where the point cloud is constructed from a set of patches learned in a patch-wise manner.
\item We devise three different types of generator modules using transformers to explore both point- and patch-wise relationships for point cloud generation.
\item We conduct extensive qualitative and quantitative experiments on the most popular point cloud generation dataset, ShapeNet~\cite{shapenet2015}, to show the effectiveness of the proposed method as well as the usefulness of each key component.
\end{enumerate}


\section{Related Work}

% In this section, we first introduce important deep learning methods for 3d point cloud analysis, including recent popular transformer architectures. We then review recent point cloud generation methods.

\paragraph{\textbf{Deep Learning for Point Cloud.}}
% Learning-based methods have been successfully applied to point clouds analysis, including classification/segmentation~\citep{qi2017pointnet,li2018so}, 3d object detection/tracking~\citep{chen2017multi,zhou2018voxelnet}, 3d shape completion~\citep{huang2020pf,yuan2018pcn}, registration~\citep{aoki2019pointnetlk,wang2019deep}, and upsampling~\citep{li2019pu,yu2018pu}. However, it is still very challenging to apply deep neural networks for point cloud analysis, 
% Applying deep neural networks for point cloud analysis is non-trivial, since the irregular and unordered nature of point cloud makes it difficult to directly generalize existing CNN paradigms for structured data analysis. 
Learning a robust deep representation from point cloud is of great importance for point cloud understanding~\cite{qi2017pointnet,ren2022benchmarking,chu2022tpc}.
Among recent methods, PointNet is a pioneering pointwise CNN~\cite{qi2017pointnet}, while several following hierarchical architectures have been further developed to better capture local structures, such as PointNet++~\cite{qi2017pointnet++}, PointCNN~\cite{li2018pointcnn},  PointConv~\cite{wu2019pointconv}, and KPConv~\cite{thomas2019kpconv}. Specifically,  PointCNN~\cite{li2018pointcnn} also introduces a new $\chi$-conv transformation which transforms points into a latent canonical order and then applies a convolutional operator. Given the irregular and unordered nature of point clouds, the recent success of transformer architectures provides a promising mechanism to encode rich relationships between points~\cite{vaswani2017attention}. Inspired by this, various transformer architectures have been recently proposed for point cloud analysis~\cite{zhao2021point,guo2021pct,mazur2021cloud,pan20213d,yang2022unified}. Specifically, Zhao et al.~\cite{zhao2021point} propose to apply self-attention in the local neighborhood of each point, where the proposed transformer layer is invariant to the permutation of the point set, making it suitable for point set processing tasks. Guo et al.~\cite{guo2021pct} propose a novel point cloud transformer framework or PCT to replace the original self-attention module with a more suitable offset-attention module, which includes an implicit Laplace operator and a normalization refinement. Mazur and Lempitsky~\cite{mazur2021cloud} introduce a new building block, which combines the ideas of spatial transformers and multi-view convolutional networks with the efficiency of standard convolutional layers in two dense grids. The new block operates via multiple parallel heads, whereas each head rasterizes feature representations of individual points into a low-dimensional space in a differentiable way and then uses dense convolution to propagate information across points.

\paragraph{\textbf{Point Cloud Generation.}} 
In the past few years, deep generative models have been explored for point cloud analysis~\cite{achlioptas2018learning,shu20193d,hui2020pdgn,wen2021learning,yang2019pointflow,kim2021setvae,luo2021diffusion,klokov2020discrete,zhang2021learning,tang2022warpinggan,zeng2022lion,wu2022fast,tyszkiewicz2023gecco}. Specifically, Achlioptas et al.~\cite{achlioptas2018learning} first explore different GAN architectures in both raw data space and latent space with a pretrained autoencoder, which has become a popular baseline for point cloud generation. Then, Shu et al.~\cite{shu20193d} adopt GAN with a tree structure, namely TreeGAN, to preserve ancestor information instead of neighbor information to generate new points. To enhance the generation quality, Hui et al.~\cite{hui2020pdgn} and Wen et al.~\cite{wen2021learning} further divide the difficult task of generating high-fidelity samples into multiple steps, which significantly improves point cloud generation performance. In addition, VAE also attracts increasing attention in point cloud generation due to its elegant formulation and promising performance. For example, Yang et al.~\cite{yang2019pointflow} propose a probabilistic VAE to generate point clouds by modeling it as a two-level hierarchical distribution, but it converges slowly and usually fails in cases with thin structures. Kim et al.~\cite{kim2021setvae} adopt attention-based set transformers~\cite{lee2019set} into the VAE framework and extend it to a hierarchy of latent variables to account for flexible subset structures. The probabilistic model is also a popular method for point cloud generation. Specifically, Luo and Hu~\cite{luo2021diffusion} learn the reverse diffusion process that transforms the noise distribution to the desired shape distribution. Klokov et al.~\cite{klokov2020discrete} introduce a latent variable model using normalizing flows with affine coupling layers to generate 3d point clouds of an arbitrary size given a latent shape representation. Zhang et al.~\cite{zhang2021learning} propose a Markov chain based 3D generative model that iteratively mends the shape to a learned distribution. Recently, Tang et al.~\cite{tang2022warpinggan} proposes the WarpingGAN to formulate the generation process as the learning of a function that warps multiple 3D priors into different local regions.

% Figure environment removed


\section{Preliminaries}

This section provides background knowledge for deep generative models and transformer architectures.

\paragraph{\textbf{VAE-GAN.}}
Given a training set $X = \{x_i\}^n_{i=1}$, we assume $x_i$ is sampled from a generative process $p(x|z)$, where $z$ refers to the latent variable. The objective of VAE~\cite{kingma2013auto} is to simultaneously train an inference network (or encoder) $q_{\phi}(z|x)$ and a generator network (or decoder) $p_{\psi}(x|z)$.
% The inference network, also called the encoder, outputs parameters for Gaussian distribution $q_{\phi}(z|x) = \mathcal{N}(z|\mu_{\phi}(x), \sigma_{\phi}(x))$, while the generator network, also called the decoder, is a deterministic neural network $f_{\theta}(z)$ parameterized by $\theta$. 
% % Generative density $p_{\theta}(x|z)$ is assumed to be subject to a Gaussian distribution: $p_{\theta}(x|z) = \mathcal{N}(f_{\theta}(z), \sigma^2I)$. 
% The inference network, also called the encoder, and the generator network, also called the decoder, 
Therefore, the VAE model can be trained by jointly minimizing the negative of evidence lower bound (ELBO),
\begin{small}
\begin{equation}
\label{L_ELBO}
\begin{aligned}
\mathcal{L}_{ELBO}(\phi, \psi; x) = &-\mathbb{E}_{z\sim q_{\phi}(z|x)}[log(p_{\psi}(x|z))] \\
&+ KL[q_{\phi}(z|x)||p(z)],
\end{aligned}
\end{equation}
\end{small}

\noindent
where $p(z)$ is the prior distribution, i.e., $\mathcal{N}(0, I)$ and $KL$ indicates the Kullback–Leibler divergence. The first term $-\mathbb{E}_{z\sim q_{\phi}(z|x)}[log(p_{\psi}(x|z))]$ can be reduced to a standard point-wise reconstruction loss and the second term is the regularization term to prevent the conditional $q_{\phi}(z|x)$ from deviating from the Gaussian prior $\mathcal{N}(0, I)$. Lastly, the whole network is jointly optimized by
\begin{small}
\begin{equation}
\label{min_ELBO}
\mathop{min}\limits_{\phi,\ \psi} \; \mathbb{E}_{x\sim p_{data}(x)}\mathcal{L}_{ELBO}(\phi, \psi; x),
\end{equation}
\end{small}

\noindent
where $p_{data}$ is the distribution induced by the training set $X$. The objective of VAE-GAN~\cite{larsen2016autoencoding} augments Eq.~(\ref{min_ELBO}) with the GAN objective. Specifically, the modified ELBO computes the reconstruction loss in the feature space of the discriminator:
\begin{small}
\begin{equation}
\label{L_modified_ELBO}
\begin{aligned}
&\mathcal{L}_{ELBO}(\phi, \psi, D; x) = \\
&\ \mathbb{E}_{z\sim q_{\phi}(z|x)}[||F_{D}(x) - F_{D}(G(z))||_2^2] \\ 
&+ KL[q_{\phi}(z|x)||p(z)],
% {\color{red}{\mathcal{L}_{ELBO}(\phi, \psi, D; x) = \mathbb{E}_{z\sim q_{\phi}(z|x)}[||F_{D}(x) - F_{D}(G(z))||_2^2] + KL[q_{\phi}(z|x)||p(z)],}}
\end{aligned}
\end{equation}
\end{small}

\noindent
where $D$ is the discriminator, $G$ is the generator and $F_{D}(\cdot)$ denotes the feature embedding from the discriminator. 
The modified GAN objective considers both reconstructed point clouds (latent code from $q_{\phi}(z|x)$) and sampled point clouds (latent code from the prior $p(z)$) as its fake samples:
\begin{small}
\begin{equation}
\label{L_GAN}
\begin{aligned}
\mathcal{L}_{GAN}(\phi, \psi, D; x) = 
&\ \mathbb{E}_{z\sim q_{\phi}(z|x)} log(1 - D(G(z))) \\
&+ \mathbb{E}_{z\sim p(z)} log(1 - D(G(z))) \\
&+ log D(x).
% {\color{red}{\mathcal{L}_{GAN}(\phi, \psi, D; x) = 
% \mathbb{E}_{z\sim q_{\phi}(z|x)} log(1 - D(G(z)))
% + \mathbb{E}_{z\sim p(z)} log(1 - D(G(z)))
% + log D(x).}}
\end{aligned}
\end{equation}
\end{small}

\noindent
Overall, the final objective becomes:
\begin{small}
\begin{equation}
\label{min_max_ELBO}
\mathop{min}\limits_{\phi,\ \psi}\mathop{max}\limits_{D} \ \mathcal{L}_{ELBO}(\phi, \psi, D; x) + \mathcal{L}_{GAN}.
% {\color{red}{\mathop{min}\limits_{\phi,\ \psi}\mathop{max}\limits_{D} \ \mathcal{L}_{ELBO}(\phi, \psi, D; x) + \mathcal{L}_{GAN}.}}
\end{equation}
\end{small}

\paragraph{\textbf{Transformer.}}
A vanilla transformer~\cite{vaswani2017attention,dosovitskiy2020image} consists of an encoder module and a decoder module. Each encoder/decoder layer is composed of a multi-head self-attention layer and a position-wise feed-forward network. Specifically, each self-attention layer in the transformer adopts a ``query-key-value" mechanism as follows. The input feature sequence is first transformed into three different vectors, queries $Q \in R^{N \times d_q}$, keys $K \in R^{N \times d_k}$ and values $V \in R^{N \times d_v}$, where $N$ is the length of feature sequence and $d_q, d_k, d_v$ denote the dimensions of keys, queries, and values, respectively. Then, the scaled dot-product attention used in the transformer can be formulated as follows:
\begin{small}
\begin{equation}
Attention(Q, K, V) = softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V.
\end{equation}
\end{small}

% where $N$ denotes the lengths of queries, keys, values); $d_q, d_k, d_v$ denote the dimensions of keys, queries and values, respectively. 

% transformer was first used in the field of neural language processing (NLP), and to apply transformer to vision tasks, the input, such as image or point cloud, is always split into patches. These patches are treated the same way as tokens (words) in an NLP application. transformers have achieved great success in many fields, such as image recognition and audio processing. Therefore, it is natural to apply it for point cloud applications. 

% and provide the sequence of linear embeddings of these patches as an input to a transformer
% Before transformer can be used to translate sentences, each word in the sentence needs to be embedded into a vector with dmodel dimensions. We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence. The illustration of the transformer encoder was inspired by


\section{Method}
\label{section_method}

In this section, we introduce the proposed patch-wise point cloud generation. Specifically, we first present an overview of our network and then describe each component in the proposed method in detail.

\subsection{Overview}

As mentioned in the previous section, we adopt the VAE-GAN~\cite{larsen2016autoencoding} framework, and our model consists of three components, one encoder, one decoder, and one discriminator. The main VAE-GAN framework for patch-wise point cloud generation is shown in Fig.~\ref{fig.main}. It also can be considered as a VAE network equipped with a discriminative network, in which the VAE component trains the decoder to reconstruct real samples with plausible variation, while the discriminative module enforces the decoder to produce realistic point clouds from the prior distribution. Different from typical VAE-GAN models, we have a set of different patch generators, which are based on the learnable patch priors. Finally, the output of all patch generators is combined to generate the final point cloud. We introduce the patch-wise generation in a divide-and-conquer manner in the next section.

% We build our framework starting from variational autoencoders which have been used for point cloud generation (Bowman et al., 2015), where point cloud x is generated conditioned on latent code z.

\subsection{Patch-wise Generation}
\label{patch_wise_generation}

% The vanilla VAE employs an unstructured vector z in which the dimensions are entangled. To model and control the attributes of interest in an interpretable way, we augment the unstructured variables z with a set of structured variables c each of which targets a salient and independent semantic feature of sentences. We want our sentence generator to condition on the combined vector (z, c), and generate samples that fulfill the attributes as specified in the structured code c. 

\paragraph{\textbf{Divide.}}
As mentioned in the previous section, we adopt the VAE-GAN~\cite{larsen2016autoencoding} framework, and our model consists of three components, one encoder, one decoder, and one discriminator. The main VAE-GAN framework for patch-wise point cloud generation is shown in Fig.~\ref{fig.main}. It also can be considered as a VAE network equipped with a discriminative network, in which the VAE component trains the decoder to reconstruct real samples with a plausible variation. In contrast, the discriminative module enforces the decoder to produce realistic point clouds from the prior distribution. Different from typical VAE-GAN models, we have a set of different patch generators, which are based on the learnable patch priors. Finally, the output of all patch generators is combined to generate the final point cloud. We introduce the patch-wise generation in a divide-and-conquer manner in the next section.

% Figure environment removed

\paragraph{\textbf{Combine.}}
The output point cloud $O$ can thus be formed as the union of the generated patches of point clouds, i.e.,
\begin{small}
\begin{equation}
O = \cup_{i=1}^k \mathcal{G}_i(z, \mathcal{P}_i).
\end{equation}
\end{small}

\noindent
However, simply assembling the generated patches may not always lead to satisfying point clouds. Therefore, the point-wise reconstruction term in Eq.~(\ref{L_ELBO}) imposes a constraint on the final generation and contributes to producing desirable results. Specifically, we train the network by minimizing the Chamfer distance~\cite{fan2017point} between the $O$ and training data $x_i \in X$.
The patch-wise generators are shown in Fig.~\ref{fig.multi_patch_generation}. We aim to automatically learn the patch priors $\{\mathcal{P}_i, 1 \leq i \leq k\}$ over a training collection. The intuition behind the approach is that if the patch priors $\{\mathcal{P}_i, 1 \leq i \leq k\}$ have useful geometry information to reconstruct the target, the patch generators $\{\mathcal{G}_i, 1 \leq i \leq k\}$ should be easier to learn and more interpretable. The patch priors $\{\mathcal{P}_i, 1 \leq i \leq k\}$ are learned over the entire training set and do not depend on the input during testing. That is, at the testing time, we take the patch priors $\{\mathcal{P}_i, 1 \leq i \leq k\}$ and a randomly sampled latent vector as the input of the patch generators $\{\mathcal{G}_i, 1 \leq i \leq k\}$ to generate the output 3d point cloud. 

\subsection{Network Structures}
\label{architecture}
In this subsection, we introduce the detailed structures of the encoder, decoder, and discriminator.

\paragraph{\textbf{Encoder.}}
We use a simplified version of the PointNet~\cite{qi2017pointnet} as the encoder. Specifically, we utilize multi-layer perceptron (MLP) layers to learn both low- and high-level representations in a pointwise manner. Given the training data with $N$ points, the input and output of each MLP layer are point features with size $N \times C_{in}$ and $N \times C_{out}$, where $C_{in}$ and $C_{out}$ denote the numbers of channels in the input and output feature attributes, respectively. In our setting, we represent each 3d point of the training data as a 1024 dimensional vector using three hidden MLP layers of 128, 256, and 512 neurons with the ReLU activation function. We then apply a max-pooling layer over all point features followed by a fully connected layer, producing a global shape feature used as input to the patch generators.

\paragraph{\textbf{Decoder.}}
Patch generators $\{\mathcal{G}_i, 1 \leq i \leq k\}$ aim to synthesize point clouds by using patch priors $\mathcal{P}_i \ (1 \leq i \leq k)$. The deformation from patch priors to point cloud is not difficult, and we can expect the quality of generation to increase using more complex generators. As mentioned before, each patch is a set of 2d points with size $N_k$, where $\sum N_k = N$. The input to each generator module $\mathcal{G}_i \ (1 \leq i \leq k)$ is a feature vector with size $N_k \times (d_z + d_{patch})$, where $d_z$ and $d_{patch}$ are the dimensions of latent code and the patch priors, respectively. We always use two-dimensional patch priors, i.e., $d_{patch} = 2$. The output of the patch generator is with the size $N_k \times 3$. The final output point cloud is the union of the $k$ outputs of the generator modules. In this paper, we explore three types of generator modules: MLP generator (MLP-G), point transformer generator (PointTrans-G), and point-patch transformer generator (DualTrans-G), as shown in Fig.~\ref{fig.generator_modules}. The last two generator modules incorporate the popular transformer structure. Transformer in PointTrans-G is called the pointwise transformer, and in DualTrans-G called the patchwise transformer. The two types of transformers have the same architecture but are used to extract point-point and patch-patch relationships, respectively. We introduce the details of different generators as follows:
\begin{enumerate}
\item MLP-G: this generator is built on MLP layers, which takes as input the concatenation of the latent code and the coordinates of 2d points from the associated patch. The output is a collection of learned 3d points which will be used to construct the 3d shape. The MLP layer is the same architecture as the one used in the encoder but with different dimensions.

% \begin{wrapfigure}{r}{0.37\columnwidthth}
%   \begin{center}
%     % Figure removed
%   \end{center}
%   \caption{Local point features extraction. Features to be convoluted can be retrieved easily through the anchor point (the orange point) and its neighborhoods (gray points in the circle).}
%   \label{fig.MLP-G}
% \end{wrapfigure}
% \vspace{1mm}
\item PointTrans-G: in the MLP-G, each point in the patch is deformed in a pointwise manner. There is no information exchange between different points. Inspired by the recent transformer models on vision tasks, here we insert pointwise transformer layers between MLP layers to extract local features by aggregating the information of neighborhood points. Different from the vanilla transformer~\cite{vaswani2017attention}, the transformer used in our work only contains the encoder module.

% % Figure environment removed
% Specifically, each point convolution layer consists of a sampling operation and a grouping operation, as shown in Fig. 3. The input size of this layer is NC0 in and the output size is NsC0 out, where Ns indicates the number of sampled points.
% \vspace{1mm}
\item DualTrans-G: based on the PointTrans-G, we extend the usage of the transformer between different patches. Specifically, patches are first fed into the pointwise transformer layer and then the patchwise transformer layer. Thus there is information flow between different patches, as well as different points on the local patch. We call this structure DualTrans because it allows to learn both local and global information.
% % Figure environment removed
% After each MLP block, we also attach a point convolution layer or the set abstraction (SA) layer [30] to extract local features by aggregating the information of neighbor points. Specifically, each point convolution layer consists of a sampling operation and a grouping operation, as shown in Fig. 3. The input size of this layer is NC0 in and the output size is NsC0 out, where Ns indicates the number of sampled points.
\end{enumerate}
% These three types of generator modules are shown in Fig.~\ref{fig.generator_modules}. In the next section, we will give both the qualitative and quantitative comparisons between them.
Note that, in MLP-G and PointTrans-G, each patch is associated with a generator module, and patches are transformed one by one; In DualTrans-G, all patches are fed into the generator module simultaneously. An illustration of different generator modules is shown in Fig.~\ref{fig.generator_modules}.

% Figure environment removed

\paragraph{\textbf{Discriminator.}} 
In adversarial learning, a discriminator distinguishes whether the input is produced by the generator or sampled from the ground truth distribution, 
while the generator aims to fool the discriminator by generating realistic samples. 
% {\color{red}{while the generator aims to fool the discriminator by generating realistic samples.}} 
In this paper, we use both discriminative and contrastive features to boost the generator for high-fidelity point cloud generation. Specifically, we first employ a feature extraction backbone with the MLP layers. We then apply max-pooling over point features followed by two different branches, a discriminative one and a contrastive one, aiming to generate discriminative and contrastive shape features.

\subsection{Optimization}

Though the objective in Eq.~(\ref{min_max_ELBO}) improves the overall quality, it can not accurately explore instance-level fidelity. Inspired by~\cite{oord2018representation}, we adopt a similar contrastive learning loss to further enhance the patch-wise generation process as follows:
% As mentioned before, in the discriminator, we introduce the contrastive loss to boost the quality of the generation.
% Before stating our loss. In the VAE-GAN framework, there two set of samples, the reconstructed one $Dec(z_{Enc})$ and the prior one $Dec(z_{Prior})$. Thus, the contrastive loss have two part with the same formulation.
% Specifically, the contrastive loss can be defined as,
\begin{small}
\begin{equation}
\label{L_contrastive}
\begin{aligned}
&\mathcal{L}_{con}(\phi, \psi, D; \hat{X}) = \\
% &= -\mathbb{E}_{\hat{X}}[log\frac{e^{h(F_{D}(\hat{x_i}), F_{D}(aug(\hat{x_i})))}}{e^{h(F_{D}(\hat{x_i}), F_{D}(aug(\hat{x_i})))} + \sum \limits_{1 \leq i, j \leq n} e^{h(F_{D}(\hat{x_i}), F_{D}(\hat{x_j}))}}].
&-\mathbb{E}_{\hat{X}}[log\frac{f(\hat{x_i}, aug(\hat{x_i}))}{f(\hat{x_i}, aug(\hat{x_i})) + \sum \limits_{1 \leq i, j \leq n, i \neq j} f(\hat{x_i}, \hat{x_j})}],
% L_{rec} = \sum_{p \in P}\mathop{min}\limits_{q \in Q}\|p - q\|^2_2 + \sum_{q \in Q}\mathop{min}\limits_{p \in P}\|p - q\|^2_2,
\end{aligned}
\end{equation}
\end{small}

\noindent
where $\hat{X}$ is the output of the decoder. The latent code fed into the decoder can be either $z_{e}$ computed by the encoder or the $z_{p}$ sampled from the prior distribution, and $\hat{x_i}, \hat{x_j} \in \hat{X}$. 
% is an index for a training sample (instance), ${x_j}^n_{j=1}$ is the union of positive samples and negative samples, 
In Eq.~(\ref{L_contrastive}), $f(\cdot, \cdot) = e^{h(F_{D}(\cdot), F_{D}(\cdot))}$, where $h(\cdot , \cdot)$ is the cosine similarity function that measures compatibility between feature embeddings, and $F_D(\cdot)$ denotes the feature embeddings from the discriminator $D$. 
% Following the popular choice from [23], $d(x, y)$ is the cosine similarity between the embeddings of $x$ and $y$. Note that unlike in contrastive self-supervised learning methods [63, 23, 6] where two views (independent augmentations) of an instance constitutes a positive pair, an input instance $x_i$ and its reconstruction $f_{\psi}(z)$ comprises a positive pair in our work. Likewise, the reconstruction $f_{\psi}(z)$ and any instance that is not $x_i$ can be a negative pair. 
The positive pair comprises instance $\hat{x_i}$, and its augmentation $aug(\hat{x_i})$, such as rotating and jittering, and the negative pair consists of different instance tuples $(\hat{x_i}, \hat{x_j})_{i \neq j} $ in the same batch. 
% The contrastive loss encourages the matching between the reconstruction and the input image instance, while pushing the reconstruction away from the rest of the images in the entire training set. Thus, the instance-level and set-level contradistinctive terms collaborate with each to encourage the high fidelity of the reconstruction and synthesis.
Therefore, the overall learning objective is,
\begin{small}
\begin{equation}
\label{min_max_loss}
\begin{aligned}
\mathop{min}\limits_{\phi,\ \psi}\mathop{max}\limits_{D} \ \mathcal{L}_{ELBO}(\psi, \phi, D; x) + \mathcal{L}_{GAN} + \mathcal{L}_{con}.
\end{aligned}
\end{equation}
\end{small}

To better understand the role of contrastive loss, we first explain the discriminative loss, which encourages generated samples to be close to real samples and far away from fake samples. From this aspect, it can be regarded as the ``class-level contrastive loss" (real samples for positive pairs and fake samples for negative pairs). However, it does not measure the instance-level fidelity among results in the same batch. Therefore, the motivation behind the contrastive loss in our paper is to promote each generated sample to be as close as possible to its augmented one while being different from all other samples in the current batch. In this way, the contrastive loss brings a significant improvement, which is one of the major differences compared with Atlasnet~\cite{groueix2018papier,deprelle2019learning}. The contrastive loss encourages each instance to be different from the other samples in the sample batch, thus leading to high-fidelity generation.


\section{Experiments}

In this section, we first introduce datasets and implementation details. We then compare our method with recent state-of-the-art point cloud generation methods and perform ablation studies on its important components. 

\subsection{Implementation Details}

We sample points uniformly on mesh objects from the ShapeNet~\cite{shapenet2015} dataset. In our experiments, we choose three widely-used object categories, chair, car, and airplane, and each category with 2048 points. We use eight patches for the point cloud generation, and the dimension of latent code $z_{e}$ and $z_{p}$ is 128. We implement the method using PyTorch. All our models are trained on NVIDIA GeForce RTX 2080Ti GPU. 
We use the Adam optimizer with $\beta=0.5$, and the initial learning rate is $0.0001$. 

As we introduce three different types of generator modules in this paper, we conduct each experiment using all three generator modules. 
The sizes of MLP layers used in MLP-G and PointTrans-G are listed in Fig.~\ref{fig.generator_modules}.
Here, we give the parameters of the transformer layers in PointTrans-G and DualTrans-G. As mentioned before, we use two types of transformer layers, pointwise and patchwise transformer layers. Though they are used to extract different features, they share the same structure, just with different parameters. We use a 4-head self-attention pointwise transformer and an 8-head self-attention patchwise transformer. The output channels of the pointwise transformer layer in PointTrans-G keep the same as the MLP layer before it. In DualTrans-G, the output channels of pointwise and patchwise transformer layers are 512 and 1024, respectively. We stack four interleaving structures of pointwise transformer and patchwise transformer as shown in Fig.~\ref{fig.generator_modules}. More experimental results will be presented in our appendix.
The code is also available in the supplementary materials and will be made publicly available.

% The MLP-G consists of four MLP layers with output channels 128, 64, 32 and 3. MLPT-G consists of three MLP layers with output channesl 128, 64 and 3, and we attach a point transformer layer after each MLP layer. Each point transformer layer consists of two encoder module with four heads of self-attention. 
% The structures of the three generator modules are shown in Table~\ref{tbl.structure}.

% Following the evaluate metrics used in previous works \citep{achlioptas2018learning,yang2019pointflow}, we use 2,048 points for each category in both training and testing stages.

% The discriminator takes a point cloud containing 2,048 points as the input, which is then embedded into high dimension pointwisely by four MLP and SA layers. We demonstrate the configurations of each layer in Fig.~\ref{fig.main}. Specifically, the max pooling is used in the network to reduce the impact of different point permutations. Note that, we use WGAN~\citep{arjovsky2017wasserstein} as our basic framework, and thus there is no batch normalization layer in the discriminator. The first generator transforms the latent vector to a fixed-size feature using five upsampling layers whose upsampling rates are (1, 2, 8, 16, 8) and following the oversampling layer embed these features to size $rN \times 3$. If not mentioned, the oversampling layer in our experiment is another upsampling layer and we set $r=k=4$ in our experiments. The second generator mainly refine the result from the first step. For the backbone network in the second generator, we adopt four MLP blocks as in the discriminator, shown in Fig.~\ref{fig.main}.

% \begin{table}[h]
%   \centering
%   \resizebox{\columnwidth}{!}{
%   \begin{tabular}{lcccc}
%     \toprule
%     \multirow{2}{*}[-0.5ex]{Generator} &\multicolumn{2}{c}{Pointwise transformer} &\multicolumn{2}{c}{Patchwise transformer} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5}
%     &\multirow{1}{*}{heads}  &\multirow{1}{*}{out\_dim}  &\multirow{1}{*}{head}  &\multirow{1}{*}{out\_dim} \\
%     \midrule
%     \multirow{1}*{PointTrans-G}
%     &4 &*    &- &-  \\
%     \multirow{1}*{DualTrans-G}
%     &8 &512  &8 &1024  \\
%     \bottomrule
%   \end{tabular}}
%   \caption{The detail configuration of two types of transformer layers. The output channel of pointwise transformer layer in PointTrans-G keeps the same as the MLP layer before it. In addition, the last patchwise transformer layer in DualTrans-G will output point clouds with channel 3.}
%   \vspace{-2mm}
%   \label{tbl.structure}
% \end{table}


% Figure environment removed


\subsection{Comparison with Recent State-of-the-Arts}

In Table~\ref{tbl.results}, we compare the proposed method with recent state-of-the-art methods, including raw-GAN~\cite{achlioptas2018learning}, latent-GAN~\cite{achlioptas2018learning}, PC-GAN~\cite{li2018point}, WarpingGAN~\cite{tang2022warpinggan}, PointFlow~\cite{yang2019pointflow}, PVD~\cite{zhou20213d}, DPC~\cite{luo2021diffusion} and SetVAE~\cite{kim2021setvae}. 
% We demonstrate the point cloud generation performance using different object categories and provide quantitative comparisons with the following six methods, raw-GAN \citep{achlioptas2018learning}, latent-GAN \citep{achlioptas2018learning}, GraphCNN-GAN \citep{valsesia2018learning}, PointFlow \citep{yang2019pointflow}, tree-GAN \citep{shu20193d}, and PDGN \citep{hui2020pdgn}, using their official implementations with default training parameters. 
We adopt the popular evaluation metrics for the point cloud generation, i.e., JSD, MMD-CD, MMD-EMD, COV-CD, COV-EMD~\cite{achlioptas2018learning} and 1-NNA~\cite{yang2019pointflow}. 
JSD scores and MMD-EMD scores are multiplied by $10^2$. MMD-CD scores are multiplied by $10^3$. For JSD, MMD, and 1-NNA, lower scores denote better performance. 
% {\color{red}{JSD scores and MMD-EMD scores are multiplied by $10^2$. MMD-CD scores are multiplied by $10^3$. For JSD, MMD, and 1-NNA, lower scores denote better performance. }}
As shown in Table~\ref{tbl.results}, our model outperforms raw-GAN across all three categories with a large margin and obtains either comparable or the best score under all evaluation metrics. Taken the chair category for example, our work achieve the best performance with the MMD-CM, MMD-EMD, COV-CD and 1-NNA-CD.

\begin{table}[!ht]
    \centering
    \renewcommand\arraystretch{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llccccccc}
        \toprule
        \multirow{2}{*}[-0.5ex]{Class} &\multirow{2}{*}[-0.5ex]{Model} &\multirow{2}{*}[-0.5ex]{JSD($\downarrow$)} &\multicolumn{2}{c}{MMD($\downarrow$)} &\multicolumn{2}{c}{COV($\uparrow$, \%)} &\multicolumn{2}{c}{1-NNA($\downarrow$, \%)} \\
        %\cline{4-5} \cline{6-7}
        \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
        &  &  &CD  &EMD  &CD  &EMD  &CD  &EMD \\
        \midrule
        \multirow{11}*{Chair}    
        &raw-GAN       &11.5          &2.57          &12.8          &33.99          &9.97           &71.75          &99.47 \\
        &latent-GAN    &4.59          &2.46          &8.91          &41.39          &25.68          &64.43          &85.27 \\
        &PC-GAN        &3.90          &2.75          &8.20          &36.50          &38.98          &76.03          &78.37 \\
        &WarpingGAN    &-             &-             &8.7           &-              &53.75          &-              &- \\
        &PointFlow     &\textbf{1.74} &2.42          &7.87          &46.83          &46.98          &60.88          &59.89 \\
        &PVD           &1.78          &2.45          &7.73          &47.19          &47.13          &58.44          &\textbf{55.76} \\
        &DPC           &1.80          2.58           &7.78          &48.94          &47.52          &60.11          &69.06 \\
        &SetVAE        &-             &2.55          &7.82          &46.98          &45.01          &58.76          &61.48 \\
        &Ours (MLP-G)  &2.14          &2.45          &7.83          &47.21          &46.52          &59.90          &60.31 \\
        &Ours (PointTrans-G) &1.92    &2.47          &7.75          &47.17          &46.37          &58.55          &60.74 \\
        &Ours (DualTrans-G) &1.87     &\textbf{2.37} &\textbf{7.69} &\textbf{47.30} &46.63          &\textbf{57.82} &60.06 \\
        \midrule
        \multirow{11}*{Car}      
        &raw-GAN       &12.8          &1.27          &8.74          &15.06          &9.38           &97.87          &99.86 \\
        &latent-GAN    &4.43          &1.55          &6.25          &38.64          &18.47          &63.07          &88.07 \\
        &PC-GAN        &5.85          &1.12          &5.83          &23.56          &30.29          &92.19          &90.87 \\
        &WarpingGAN    &-             &-             &-             &-              &-              &-              &- \\
        &PointFlow     &0.87          &0.91          &5.22          &44.03          &46.59          &60.65          &62.36 \\
        &PVD           &0.84          &0.87          &5.10          &47.94          &46.70          &\textbf{57.15} &\textbf{56.44} \\
        &DPC           &-             &-             &-             &-              &-              &-              &- \\
        &SetVAE        &-             &0.88          &\textbf{5.05} &48.58          &44.60          &59.66          &63.35 \\
        &Ours (MLP-G)  &0.83          &0.87          &5.17          &48.75          &\textbf{46.81} &60.34          &61.95 \\
        &Ours (PointTrans-G) &0.80    &0.87          &5.24          &48.30          &46.22          &59.81          &62.26 \\
        &Ours (DualTrans-G) &\textbf{0.78} &\textbf{0.85} &5.14     &\textbf{49.12} &46.54          &59.60          &61.81 \\
        \midrule
        \multirow{11}*{Airplane} 
        &raw-GAN       &7.44          &0.261          &5.47         &42.72          &18.02          &93.58          &99.51 \\
        &latent-GAN    &4.62          &0.239          &4.27         &43.21          &21.23          &86.30          &97.28 \\
        &PC-GAN        &4.63          &0.287          &3.57         &36.46          &40.94          &94.35          &92.32 \\
        &WarpingGAN    &-             &-              &3.3          &-              &48.75          &-              &- \\
        &PointFlow     &4.92          &0.217          &3.24         &\textbf{46.91} &48.40          &75.68          &75.06 \\
        &PVD           &4.77          &0.225          &3.18         &46.77          &48.42          &75.32          &\textbf{70.57} \\
        &DPC           &4.67          &0.218          &3.06         &48.71          &45.47          &64.83          &75.12 \\
        &SetVAE        &-             &0.199          &3.07         &43.45          &44.93          &75.31          &77.65 \\
        &Ours (MLP-G)  &4.58          &0.236          &3.04         &44.30          &49.10          &76.90          &76.11 \\
        &Ours (PointTrans-G)  &\textbf{4.57} &0.186   &3.12         &45.87          &48.58          &75.21          &75.40 \\
        &Ours (DualTrans-G) &4.61     &\textbf{0.180} &\textbf{3.04}&45.55          &\textbf{49.22} &\textbf{74.97} &75.31 \\
        \bottomrule
    \end{tabular}}
    \caption{Quantitative comparisons with recent state-of-the-art methods. The best scores are highlighted in bold. JSD scores and MMD-EMD scores are multiplied by $10^2$. MMD-CD scores are multiplied by $10^3$.}
    \label{tbl.results}
\end{table}

% Figure environment removed



\subsection{Visualization}

In this subsection, we mainly unveil how these 2d patches are used to construct the final 3d shape. To better illustrate the process of patch-wise generation, we plot the patches with different colors in Fig.~\ref{fig.evolution_patches}, where three patch generators perform quite differently. Specifically, MLP-G generates each patch independently from the patch prior, and we find that there is little overlap between different generated patches. We have similar observations for PointTrans-G. Different from the above-mentioned two patch generators, DualTrans-G enhances the information flow between both points and patches, where each patch spreads over the surface of the 3d shape and produces uniformly distributed points. Additionally, we show some generated point clouds by our model for different object categories, chair, car and airplane, in Fig.~\ref{fig.results}.



\begin{table}[!ht]
    \centering
    \renewcommand\arraystretch{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering}}
    \toprule
    \multirow{2}{*}[-0.5ex]{Generator} &\multicolumn{2}{c}{MLP-G}  &\multicolumn{2}{c}{PointTrans-G} &\multicolumn{2}{c}{DualTrans-G} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    &Params &FLOPs &Params &FLOPs &Params &FLOPs \\
    \midrule
    \multirow{1}*{\, 2 patches}
    &60.52K    &58.7M  &1.44M   &2.68G   &93.55M  &0.60G  \\
    \multirow{1}*{\, 4 patches}
    &116.94K   &58.7M  &2.48M   &1.87G   &85.12M  &1.04G   \\
    \multirow{1}*{\, 8 patches}
    &229.78K   &58.7M  &4.57M   &1.47G   &80.91M  &1.92G  \\
    \multirow{1}*{16 patches}
    &455.46K   &58.7M  &8.73M   &1.27G   &78.81M  &3.67G  \\
    \multirow{1}*{32 patches}
    &906.82K   &58.7M  &17.07M  &1.17G   &77.77M  &7.19G  \\
    \bottomrule
    \end{tabular}}
    \caption{Model complexity of different patches.}
    \label{tbl.patches}
\end{table}



% Figure environment removed

\begin{table*}[!ht]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lcccccccc}
        \toprule
        \multirow{2}{*}[-0.5ex]{Class} &\multirow{2}{*}{Model} &\multirow{2}{*}[-0.5ex]{JSD($\downarrow$)} &\multicolumn{2}{c}{MMD($\downarrow$)} &\multicolumn{2}{c}{COV($\uparrow$, \%)} &\multicolumn{2}{c}{1-NNA($\downarrow$, \%)} \\
        %\cline{4-5} \cline{6-7}
        \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
        &  &  &CD  &EMD  &CD  &EMD  &CD  &EMD \\
        \midrule
        \multirow{3}*{Chair}    
        &MLP-G         &2.14 $|$ 2.20   &2.45 $|$ 2.49   &7.83 $|$ 7.80   &47.21 $|$ 47.09   &46.52 $|$ 46.46   &59.90 $|$ 59.94   &60.31 $|$ 60.41  \\
        &PointTrans-G  &1.92 $|$ 2.13   &2.47 $|$ 2.55   &7.75 $|$ 7.86   &47.17 $|$ 47.15   &46.37 $|$ 46.35   &58.55 $|$ 58.61   &60.74 $|$ 60.79  \\
        &DualTrans-G   &1.87 $|$ 1.99   &2.37 $|$ 2.38   &7.69 $|$ 7.73   &47.30 $|$ 47.26   &46.63 $|$ 46.60   &57.82 $|$ 57.88   &60.06 $|$ 60.15  \\
        \midrule
        \multirow{3}*{Car}      
        &MLP-G         &0.83 $|$ 0.89   &0.87 $|$ 0.87   &5.17 $|$ 5.23   &48.75 $|$ 48.70   &46.81 $|$ 46.73   &60.34 $|$ 60.47   &61.95 $|$ 62.06  \\
        &PointTrans-G  &0.80 $|$ 0.87   &0.87 $|$ 0.88   &5.24 $|$ 5.36   &48.30 $|$ 48.28   &46.22 $|$ 46.14   &59.81 $|$ 59.89   &62.26 $|$ 62.33  \\
        &DualTrans-G   &0.78 $|$ 0.85   &0.85 $|$ 0.90   &5.14 $|$ 5.24   &49.12 $|$ 49.10   &46.54 $|$ 46.51   &59.60 $|$ 59.63   &61.81 $|$ 61.86  \\
        \midrule
        \multirow{3}*{Airplane} 
        &MLP-G         &4.58 $|$ 4.63   &0.236 $|$ 0.241  &3.04 $|$ 3.07   &44.30 $|$ 44.25   &49.10 $|$ 49.13   &76.90 $|$ 76.86   &76.11 $|$ 76.34  \\
        &PointTrans-G  &4.57 $|$ 4.60   &0.186 $|$ 0.188  &3.12 $|$ 3.17   &45.87 $|$ 45.82   &48.58 $|$ 48.52   &75.21 $|$ 75.25   &75.40 $|$ 75.48  \\
        &DualTrans-G   &4.61 $|$ 4.64   &0.180 $|$ 0.179  &3.04 $|$ 3.09   &45.55 $|$ 45.49   &49.22 $|$ 49.02   &74.97 $|$ 75.06   &75.31 $|$ 75.40  \\
        \bottomrule
    \end{tabular}}
    \caption{Quantitative metrics with (left) and without (without) contrastive loss.}
    \label{tbl.contrastive}
\end{table*}



\subsection{Ablation Studies}

\paragraph{\textbf{Number of Patches}}
We compare the generated results of three generators when using different numbers of patches and investigate how the number of patches influences the generation process. Taking the chair class as an example, we show the MLP-G generation process of 2-patches and 4-patches in Fig.~\ref{fig.patches}, where 2-patches and 4-patches generation does not fully construct the 3d shape, mainly because fewer patches cannot fit the surface of the 3d shape. However, with more patches, the complexity of the model will increase, and we also show the model complexity in Table~\ref{tbl.patches}. 
In MLP-G, when the number of patches increase, the number of MLP generator modules also increase and thus lead to a larger amount of parameters. No matter how many patches are divided, the total number of points keeps the same. Thus the FLOPs keep the same. The parameters in PointTrans-G are the same as the MLP-G, but the FLOPs decrease. With fewer points, the self-attention matrix in the pointwise transformer layer decrease, thus, fewer FLOPs. The DualTrans-G is different from them because all patches are fed in simultaneously. 
In practice, we find that 8-patches is a good tradeoff in most cases.



% Figure environment removed

% % Figure environment removed


% \paragraph{Attention Head}
% The choice of depth and head of transformer in our model is also an important hyper-parameters in MLPT-G and DualT-G. In Table~\ref{tbl.transfomer}, we give the details of model complexity. In our experiments, these parameters does not have a huge influence on the final results, but on the model parameters and training time.

% \begin{table}[!ht]
%   \centering
%   \resizebox{0.4\columnwidth}{!}{
%   \begin{tabular}{lcccc}
%     \toprule
%     \multirow{2}{*}[-0.5ex]{Generator} &\multirow{2}{*}[-0.5ex]{Depth} &\multirow{2}{*}[-0.5ex]{Head}  &\multicolumn{2}{c}{MLPT-G} \\
%     % &\multirow{2}{*}[-0.5ex]{Params(M)}  &\multirow{2}{*}[-0.5ex]{GFLOPs} \\
%     \cmidrule(lr){4-5}
%     & & &Params &FLOPs \\
%     \midrule
%     \multirow{4}*{Point transformer}
%     &2   &6  &4.53M   &1.45G  \\
%     &2   &8  &4.57M   &1.47G  \\
%     &4   &4  &8.53M   &2.89G  \\
%     &6   &4  &12.49M  &4.30G  \\
%     \bottomrule
%   \end{tabular}}
%   \vspace{2mm}
%   \caption{Model complexity of different transformer settings.}
%   \label{tbl.transfomer}
% \end{table}

% \begin{table}[!ht]
%   \centering
%   \resizebox{0.4\columnwidth}{!}{
%   \begin{tabular}{lcccc}
%     \toprule
%     \multirow{2}{*}[-0.5ex]{Generator} &\multirow{2}{*}[-0.5ex]{Depth} &\multirow{2}{*}[-0.5ex]{Head} &\multicolumn{2}{c}{DualT-G} \\
%     % &\multirow{2}{*}[-0.5ex]{Params(M)}  &\multirow{2}{*}[-0.5ex]{GFLOPs} \\
%     \cmidrule(lr){4-5}
%     & & &Params &FLOPs\\
%     \midrule
%     \multirow{4}*{Point transformer}
%     &2   &6   &M &G   \\
%     &2   &8   &M &G   \\
%     &4   &4   &M &G   \\
%     &6   &4   &M &G   \\
%     \midrule
%     \multirow{4}*{Patch transformer}
%     &4   &6   &  &  \\
%     &4   &10  &  &  \\
%     &6   &8   &  &  \\
%     &8   &8   &  &  \\
%     \bottomrule
%   \end{tabular}}
%   \vspace{2mm}
%   \caption{Model complexity of different transformer settings.}
%   \label{tbl.transfomer}
% \end{table}

% \begin{table*}[ht]
%   \centering
%   \begin{tabular}{lcccccccc}
%     \toprule
%     \multirow{2}{*}[-0.5ex]{Class} &\multirow{2}{*}{Model} &\multirow{2}{*}[-0.5ex]{JSD($\downarrow$)} &\multicolumn{2}{c}{MMD($\downarrow$)} &\multicolumn{2}{c}{COV($\uparrow$, \%)} &\multicolumn{2}{c}{1-NNA($\downarrow$, \%)} \\
%     %\cline{4-5} \cline{6-7}
%     \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
%     &  &  &CD  &EMD  &CD  &EMD  &CD  &EMD \\
%     \midrule
%     \multirow{3}*{Chair}    
%     &MLP-G    &2.20   &2.49   &7.80   &47.09   &46.46   &59.94   &60.41  \\
%     &PointTrans-G   &2.13   &2.55   &7.86   &47.15   &46.35   &58.61   &60.79  \\
%     &DualTrans-G  &1.99   &2.38   &7.73   &47.26   &46.60   &57.88   &60.15  \\
%     \midrule
%     \multirow{3}*{Car}      
%     &MLP-G    &0.89   &0.87   &5.23   &48.70   &46.73   &60.47   &62.06  \\
%     &PointTrans-G   &0.87   &0.88   &5.36   &48.28   &46.14   &59.89   &62.33  \\
%     &DualTrans-G  &0.85   &0.90   &5.24   &49.10   &46.51   &59.63   &61.86  \\
%     \midrule
%     \multirow{3}*{Airplane} 
%     &MLP-G    &4.63   &0.241  &3.07   &44.25   &49.13   &76.86   &76.34  \\
%     &PointTrans-G   &4.60   &0.188  &3.17   &45.82   &48.52   &75.25   &75.48  \\
%     &DualTrans-G  &4.64   &0.179  &3.09   &45.49   &49.02   &75.06   &75.40  \\
%     \bottomrule
%   \end{tabular}
%   \caption{Quantitative metrics without contrastive loss. The evaluation metrics are the same as that in Table~\ref{tbl.results}.}
%   \label{tbl.contrastive}
% \end{table*}


\paragraph{\textbf{Contrastive Loss}}
This work introduces the contrastive loss to boost the generation process further. Here, we give the qualitative and quantitative comparisons with results generated without the supervision of contrastive loss. Moreover, taking the class chair as an example, we present the quantitative results in Table~\ref{tbl.contrastive} using the same evaluation metrics. 
% Comparing with the results in Table~\ref{tbl.results}, we see that contrastive loss benefits the generation quality a lot. 
In addition, we show some examples of generated point clouds in Fig.~\ref{fig.contrastive}.


% Figure environment removed



\subsection{Generation Process Probing}

The key difference between our work and other methods is that we use multiple patches to generate point clouds. Thus, it is beneficial to explore the generation process of how these patches gradually construct the 3D shape. Taking the chair class as an example, we demonstrate the results of three generator modules at different epochs in Fig.~\ref{fig.evolution}. 
% and plot the result patches at the last epoch with different colors in Fig.~\ref{fig.evolution_patches} for better visualization. 
As expected, the three generator modules perform differently. MLP-G takes as input patches and deforms them independently. 
% Thus, we can see from Fig.~\ref{fig.evolution_patches} that the final output consists of the deformed patches but with no intersection between each other. 
The same situation also occurs in the generation of PointTrans-G.
% however the difference is that in the MLP-G, each point in local patch is transformed independently, as show in th fig that, the point seem be affine transformation, however the points in the MLPT-G is more. The DualT-G, which is more.
Different from that, DualTrans-G enhances the information flow between points and patches. 
% Each patch spreads over the surface of the 3D shape, producing uniformly distributed points.

% The key difference between our work and other methods is that we use multiple patches to generate point clouds. Therefore, it is beneficial to unveil how these patches construct the 3D shape. Taken as the chair class for example, we plot the result patches with different colors in Fig.~\ref{fig.evolution_patches} for better visualization. As expected, the three generator modules perform differently. The MLP-G takes as input patches and deform them independently. Thus, we can see from Fig.~\ref{fig.evolution_patches} that the final output consists of the deformed patches but with no intersection between each other. The same situation also occurs in the generation of MLPT-G.
% however the difference is that in the MLP-G, each point in local patch is transformed independently, as show in th fig that, the point seem be affine transformation, however the points in the MLPT-G is more. The DualT-G, which is more.
% Different from that, the DualT-G enhances the information flow between points and patches. Each patch spreads over the surface of the 3D shape, producing uniformly distributed points.


% Figure environment removed

\subsection{Divide-and-Conquer Rethinking}
% In this paper, we propose the divide-and-conquer method to generate high-fidelity point clouds. We divide the generation process into a set of patch generation tasks, and then combine these transformed patches together to construct the final 3D shape. In subsection 4.2 of the main paper, we have pointed out that

As mentioned in Section~\ref{patch_wise_generation}, the pointwise reconstruction loss guarantees the final assembled point cloud to be satisfying. Therefore, it is helpful to unveil what the results of patch-wise generation would be like if there is no pointwise reconstruction loss. In our work, we adopt the VAE-GAN~\cite{larsen2016autoencoding} framework because it naturally incorporates the pointwise reconstruction loss. Here, we try to reproduce the generation using the GAN~\cite{goodfellow2014generative} architecture, and the loss is
\begin{small}
\begin{equation}
\begin{aligned}
\mathcal{L}_{GAN} = 
&\ \mathbb{E}_{x\sim p_{data}}[log(D(x))] \\
&+ \mathbb{E}_{z\sim p(z)}[log(1 - D(G(z)))],
\end{aligned}
\end{equation}
\end{small}

\noindent
where $p_{data}$ is the distribution of the training set and $p(z)$ is the prior distribution. $D$ is the discriminator, and $G$ is the generator (or the decoder). For a fair comparison, we reuse the decoder and the discriminator of VAE-GAN, as shown in Fig.~\ref{fig.GAN}. We keep all training hyper-parameters the same as VAE-GAN. Taking MLP-G generator as an example, we show the generation result on the chair category in Fig.~\ref{fig.GAN_generation}, where we find that, without the supervision of pointwise reconstruction loss, the results would be worse. 

% Figure environment removed


% For better visualization, we also demonstrate the learned patches with different colors in Fig.~\ref{fig.GAN_evolution_patches}.

% \begin{table}[h]
%   \centering
%   \resizebox{0.4\columnwidth}{!}{
%   \begin{tabular}{lcc}
%     \toprule
%     \multirow{1}{*}{Model} &\multirow{1}{*}{MN10(\%)} &\multirow{1}{*}{MN40(\%)} \\
%     %&\multicolumn{2}{c}{COV($\uparrow$, \%)} &\multicolumn{2}{c}{1-NNA($\downarrow$, \%)} \\
%     %\cline{4-5} \cline{6-7}
%     %\cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
%     % &  &  & \\
%     \midrule
%     % \multirow{1}*{SPH \citep{kazhdan2003rotation}}
%     % &79.8          &68.2          \\
%     % \multirow{1}*{LFD \citep{chen2003visual}}
%     % &79.9          &75.5          \\
%     % \multirow{1}*{T-L Network \citep{girdhar2016learning}}
%     % &-             &74.4          \\
%     % \multirow{1}*{VConv-DAE \citep{sharma2016vconv}}
%     % &80.5          &75.5          \\
%     \multirow{1}*{3D-GAN \citep{wu2016learning}}
%     &91.0          &83.3          \\
%     % \multirow{1}*{MRTNet \citep{gadelha2018multiresolution}}
%     % &91.7          &86.4          \\
%     \multirow{1}*{PointFlow \citep{yang2019pointflow}}
%     &93.7          &86.8          \\
%     \multirow{1}*{PDGN \citep{hui2020pdgn}}
%     &\textbf{94.2} &87.3          \\
%     \multirow{1}*{Ours}
%     &94.0          &\textbf{87.4} \\
%     \bottomrule
%   \end{tabular}}
%   \vspace{2mm}
%   \caption{Classification results on ModelNet10 (MN10) and ModelNet40 (MN40).}
%   \label{tbl.classification}
%   \vspace{-2mm}
% \end{table}

% \begin{table}[!ht]
%     \centering
%     \caption{Classification results on ModelNet10 (MN10) and ModelNet40 (MN40).}
%     \label{tbl.classification}
%     % \resizebox{0.5\columnwidth}{!}{
%     \begin{tabular}{lcc}
%     \toprule
%     \multirow{1}{*}{Model} &\multirow{1}{*}{MN10(\%)} &\multirow{1}{*}{MN40(\%)} \\
%     %&\multicolumn{2}{c}{COV($\uparrow$, \%)} &\multicolumn{2}{c}{1-NNA($\downarrow$, \%)} \\
%     %\cline{4-5} \cline{6-7}
%     %\cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
%     % &  &  & \\
%     \midrule
%     % \multirow{1}*{SPH \citep{kazhdan2003rotation}}
%     % &79.8          &68.2          \\
%     % \multirow{1}*{LFD \citep{chen2003visual}}
%     % &79.9          &75.5          \\
%     % \multirow{1}*{T-L Network \citep{girdhar2016learning}}
%     % &-             &74.4          \\
%     % \multirow{1}*{VConv-DAE \citep{sharma2016vconv}}
%     % &80.5          &75.5          \\
%     \multirow{1}*{3D-GAN \cite{wu2016learning}}
%     &91.0          &83.3          \\
%     % \multirow{1}*{MRTNet \citep{gadelha2018multiresolution}}
%     % &91.7          &86.4          \\
%     \multirow{1}*{PointFlow \cite{yang2019pointflow}}
%     &93.7          &86.8          \\
%     \multirow{1}*{PDGN \cite{hui2020pdgn}}
%     &\textbf{94.2} &87.3          \\
%     \multirow{1}*{Ours}
%     &94.0          &\textbf{87.4} \\
%     \bottomrule
%     \end{tabular}
% \end{table}

\begin{table}[!ht]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \multirow{1}{*}{Model} &\multirow{1}{*}{3D-GAN~\cite{wu2016learning}} &\multirow{1}{*}{PointFlow~\cite{yang2019pointflow}} &\multirow{1}{*}{PDGN~\cite{hui2020pdgn}} &\multirow{1}{*}{Ours} \\
    \midrule
    \multirow{1}*{MN10(\%)}
    &91.0   &93.7   &\textbf{94.2}   &94.0  \\
    \multirow{1}*{MN40(\%)}
    &83.3   &86.8   &87.3   &\textbf{87.4}  \\
    \bottomrule
    \end{tabular}}
    \caption{Classification results on ModelNet10 (MN10) and ModelNet40 (MN40).}
    \label{tbl.classification}
\end{table}


\subsection{Unsupervised Representation Learning}

To further evaluate the proposed method for unsupervised representation learning, we conduct 3d object classification experiments as follows. Following previous works~\cite{wu2016learning,yang2019pointflow}, we train our network on ShapeNet \cite{shapenet2015} and test it on two popular datasets, ModelNet10~\cite{wu20153d} and ModelNet40~\cite{wu20153d}.
% Similar to~\citep{wu2016learning,yang2019pointflow}, we evaluate the unsupervised representation learning ability of our proposed method. A general way to evaluate the learned representations is to conduct the 3D object classification experiments. 
% Following the common practice, we train our network on ShapeNet \citep{shapenet2015} and test it on two popular subsets of ModelNet \citep{wu20153d}, ModelNet10 and ModelNet40. 
Specifically, we feed our network with the full ShapeNet dataset and then extract the embedded features of the trained discriminator to learn a linear SVM for classification.
% we evaluate our model on two popular subsets of ModelNet \citep{wu20153d}, ModelNet10 and ModelNet40, for 3D object classification. 
% Note that the training and test categories are not identical, which also shows the out-of-category generalization power of our network. we simply train a linear SVM to classify the generated point clouds. The settings of ModelNet10 are consistent with ModelNet40. 
We compare our network with recent state-of-the-art point cloud generation methods in terms of classification accuracy in Table~\ref{tbl.classification}. The results show that our method can extract discriminative features and thus generate high-quality 3d point clouds.
% Note that for a fair comparison we only compare the point cloud generation methods in the classification experiment. 
% It can be seen that our method achieves close performance with PDGN \citep{hui2020pdgn} and outperforms most other generation methods on both ModelNet10 and ModelNet40 datasets. The results demonstrate that our discriminator architecture can extract discriminative features and thus supervise the generators to produce high-quality 3D point clouds.

% \section{Experiments}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

\section{Conclusion}

In this paper, we devise a novel patch-wise point cloud generation framework with three different generators: MLP-G, PointTrans-G, and DualTrans-G. To generate realistic point clouds, we feed the generator with the concatenation of both latent representations and 2d patches. Extensive experiments show that the proposed method is able to produce high-fidelity point clouds and outperforms most recent point cloud generation methods under a variety of different evaluation metrics.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{PatchGeneration}
}

\clearpage

\begin{strip}
\begin{center}
    \Large
   \textbf{Supplementary Material - Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach}
\end{center}
\end{strip}

\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}

\section{Different Patches}
% Figure environment removed

Taken the chair class for example, in this section we present the gallery of point clouds generated with different patches and quantitative results in Table~\ref{tbl.patches_2_to_32}. As shown in Fig~\ref{fig.patches_2_to_32}, for better view we plot the patches with different colors in the right column of each sub-figure. As mentioned in our main paper, we find that 8-patches is a proper tradeoff in most cases.

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccccc}
        \toprule
        \multirow{2}{*}[-0.5ex]{Model} &\multirow{2}{*}[-0.5ex]{Patches} &\multirow{2}{*}[-0.5ex]{JSD($\downarrow$)} &\multicolumn{2}{c}{MMD($\downarrow$)} &\multicolumn{2}{c}{COV($\uparrow$, \%)} &\multicolumn{2}{c}{1-NNA($\downarrow$, \%)} \\
        \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
        &  &  &CD  &EMD  &CD  &EMD  &CD  &EMD \\
        \midrule
        \multirow{5}*{MLP-G}    
        &2    &2.87     &2.77      &8.60   &43.30   &40.76      &65.14      &69.47 \\
        &4    &2.59     &2.62      &8.14   &45.55   &44.35      &62.13      &67.59 \\
        &8    &2.14     &2.45      &7.83   &47.21   &46.52      &59.90      &60.31 \\
        &16   &2.05     &2.45      &7.92   &47.53   &46.31      &59.84      &61.17 \\
        &32   &2.17     &2.51      &7.90   &47.16   &46.18      &60.28      &62.48 \\
        \midrule
        \multirow{5}*{PointTrans-G}    
        &2    &2.71     &2.61      &8.38   &42.89   &43.02      &64.17      &68.67 \\
        &4    &2.23     &2.59      &8.02   &44.56   &45.44      &62.64      &65.54 \\
        &8    &1.92     &2.47      &7.75   &47.17   &46.37      &58.55      &60.74 \\
        &16   &1.84     &2.44      &7.70   &47.42   &46.61      &58.97      &61.45 \\
        &32   &1.89     &2.45      &7.73   &47.05   &46.24      &59.96      &62.50 \\
        \midrule
        \multirow{5}*{DualTrans-G}    
        &2    &3.15     &2.65      &8.45   &42.08   &42.97      &62.20      &66.87 \\
        &4    &2.02     &2.51      &8.34   &45.84   &45.76      &60.43      &64.33 \\
        &8    &1.87     &2.37      &7.69   &47.30   &46.63      &57.82      &60.06 \\
        &16   &1.89     &2.33      &7.72   &47.21   &46.68      &58.73      &61.13 \\
        &32   &1.97     &2.40      &7.73   &47.19   &46.13      &58.60      &61.66 \\
        \bottomrule
    \end{tabular}}
    \caption{Quantitative comparisons with different patches. The evaluation metrics are the same as our main paper.}
    \label{tbl.patches_2_to_32}
\end{table}


\section{Generation without Patches}
All three variations of our model are based on patch-based generators and thus it is beneficial to show a direct comparison against a non-patch-based variant. Here, we present the qualitative and quantitative generation without patches in Fig~\ref{fig.no_patches_generation} and Table~\ref{tbl.no_patches_generation}. As shown in Fig~\ref{fig.no_patches_generation}, the main drawback of this method is that the points are not uniformly distributed.

% Figure environment removed

\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccc}
        \toprule
        \multirow{2}{*}[-0.5ex]{Model} &\multirow{2}{*}[-0.5ex]{JSD($\downarrow$)} &\multicolumn{2}{c}{MMD($\downarrow$)} &\multicolumn{2}{c}{COV($\uparrow$, \%)} &\multicolumn{2}{c}{1-NNA($\downarrow$, \%)} \\
        \cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8}
        &  &CD  &EMD  &CD  &EMD  &CD  &EMD \\
        \midrule
        VAE           &7.45    &2.62    &10.2    &40.45    &22.86    &69.43    &89.75 \\
        MLP-G         &2.14    &2.45    &7.83    &47.21    &46.52    &59.90    &60.31 \\
        PointTrans-G  &1.92    &2.47    &7.75    &47.17    &46.37    &58.55    &60.74 \\
        DualTrans-G   &1.87    &2.37    &7.69    &47.30    &46.63    &57.82    &60.06 \\
        \bottomrule
    \end{tabular}}
    \caption{Quantitative comparisons with none-patch generation. The evaluation metrics are the same as our main paper.}
    \label{tbl.no_patches_generation}
\end{table}

\section{Details in Our Network}
In the main paper, we have shown the overall framework of our point cloud generative network. Here, we provide more details of the three generator modules. Note that, each type of transformer layer only contains one encoder module, thus we do not list the depth parameter in the following tables (Table~\ref{tbl.MLP-G},~\ref{tbl.PointTrans-G} and~\ref{tbl.DualTrans-G}).

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{1.8cm}<{\centering} p{1.8cm}<{\centering} p{1.8cm}<{\centering} p{1.8cm}<{\centering}}
        \toprule
        \multirow{1}{*}{MLP-G} &\multirow{1}{*}{Layer Type} &\multirow{1}{*}{In\_dim}  &\multirow{1}{*}{Out\_dim} \\
        \midrule
        \multirow{1}*{Layer 1}
        &MLP   &130    &128  \\
        \multirow{1}*{Layer 2}
        &MLP   &128    &64  \\
        \multirow{1}*{Layer 3}
        &MLP   &64     &32  \\
        \multirow{1}*{Layer 4}
        &MLP   &32     &3  \\
        \bottomrule
    \end{tabular}}
    \caption{Details of MLP-G.}
    \label{tbl.MLP-G}
\end{table}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{1.8cm}<{\centering} p{1.8cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering}}
        \toprule
        \multirow{1}{*}{PointTrans-G} &\multirow{1}{*}{Layer Type} &\multirow{1}{*}{In\_dim}  &\multirow{1}{*}{Out\_dim}  &\multirow{1}{*}{Heads} \\
        \midrule
        \multirow{1}*{Layer 1}
        &MLP         &130    &128  &-  \\
        \multirow{1}*{Layer 2}
        &pointwise   &128    &128  &4  \\
        \multirow{1}*{Layer 3}
        &pointwise   &128    &128  &4  \\
        \multirow{1}*{Layer 4}
        &MLP         &128    &64   &-  \\
        \multirow{1}*{Layer 5}
        &pointwise   &64     &64   &4  \\
        \multirow{1}*{Layer 6}
        &pointwise   &64     &64   &4  \\
        \multirow{1}*{Layer 7}
        &MLP         &64     &3    &-  \\
        \bottomrule
    \end{tabular}}
    \caption{Details of PointTrans-G.}
    \label{tbl.PointTrans-G}
\end{table}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{1.8cm}<{\centering} p{1.8cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering}}
        \toprule
        \multirow{1}{*}{DualTrans-G} &\multirow{1}{*}{Layer Type} &\multirow{1}{*}{In\_dim}  &\multirow{1}{*}{Out\_dim}  &\multirow{1}{*}{Heads} \\
        \midrule
        \multirow{1}*{Layer 1}
        &pointwise    &130  &1024   &8  \\
        \multirow{1}*{Layer 2}
        &patchwise   &1024  &1024   &8  \\
        \multirow{1}*{Layer 3}
        &pointwise   &1024  &1024   &8  \\
        \multirow{1}*{Layer 4}
        &patchwise   &1024  &1024   &8  \\
        \multirow{1}*{Layer 5}
        &pointwise   &1024  &1024   &8  \\
        \multirow{1}*{Layer 6}
        &patchwise   &1024  &1024   &8  \\
        \multirow{1}*{Layer 7}
        &pointwise   &1024  &1024   &8  \\
        \multirow{1}*{Layer 8}
        &patchwise   &1024  &3      &8  \\
        \bottomrule
    \end{tabular}}
    \caption{Details of DualTrans-G.}
    \label{tbl.DualTrans-G}
\end{table}


\section{Model Complexity Analysis}

In Subsection 5.4 of the main paper, we have given the model complexity with different input patches. Here, we discuss the parameters and FLOPs in details. For convenience, we paste the model complexity table of our main paper here as Table~\ref{tbl.patches-appendix}. Before we analyse the model complexity, we first illustrate two core components of the transformer layer, i.e., the self-attention module and the position-wise feed-forward network (FFN) in Table~\ref{tbl.transformer}~\cite{lin2021survey}.

% We assume that the hidden dimension f the model is  and that the input
% sequence length is The intermediate dimension of FFN is set to 4 and the dimension of keys
% and values are set to  in Vaswani et al. [137].

% \begin{enumerate}[1)]
% \item For MLP layer, given the input channel $in\_dim$ and output channel $out\_dim$, we can calculate the trainable parameters $Para$. If the number of MLP layer doubles, the trainable parameters will be $2 \times Para$. Given $N$ input points, the FLOPs is $N\_Flop$. When the input points double, the FLOPs will be $2 \times N\_Flop$.

% \item For the self-attention layer, given the input channel $in\_dim$ and output channel $out\_dim$, we can calculate the trainable parameters $Para$. If the number of self-attention layer doubles, the trainable parameters will be $2 \times Para$. If the input is $N$ points, the FLOPs is $N\_Flop$. When the input points double, the FLOPs will be $4 \times N\_Flop$ because of the matrix multiply.

% \begin{small}
% \begin{equation}
% Attention(Q, K, V) = softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V.
% \end{equation}
% \end{small}
% \end{enumerate}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{1.6cm} p{1.4cm}<{\centering} p{1.4cm}<{\centering} p{1.4cm}<{\centering} p{1.4cm}<{\centering} p{1.4cm}<{\centering} p{1.4cm}<{\centering}}
        \toprule
        \multirow{2}{*}[-0.5ex]{Generator} &\multicolumn{2}{c}{MLP-G}  &\multicolumn{2}{c}{PointTrans-G} &\multicolumn{2}{c}{DualTrans-G} \\
        % &\multirow{2}{*}[-0.5ex]{Params(M)}  &\multirow{2}{*}[-0.5ex]{GFLOPs} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        &Params &FLOPs &Params &FLOPs &Params &FLOPs \\
        \midrule
        \multirow{1}*{\, 2 patches}
        &60.52K    &58.7M  &1.44M   &2.68G   &93.55M  &0.60G  \\
        \multirow{1}*{\, 4 patches}
        &116.94K   &58.7M  &2.48M   &1.87G   &85.12M  &1.04G  \\
        \multirow{1}*{\, 8 patches}
        &229.78K   &58.7M  &4.57M   &1.47G   &80.91M  &1.92G  \\
        \multirow{1}*{16 patches}
        &455.46K   &58.7M  &8.73M   &1.27G   &78.81M  &3.67G  \\
        \multirow{1}*{32 patches}
        &906.82K   &58.7M  &17.07M  &1.17G   &77.77M  &7.19G  \\
        \bottomrule
    \end{tabular}}
    \caption{Model complexity of different patches.}
    \label{tbl.patches-appendix}
\end{table}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{p{2.6cm} p{1.8cm}<{\centering} p{1.8cm}<{\centering}}
        \toprule
        \multirow{1}{*}{Module} &\multirow{1}{*}{Parameters} &\multirow{1}{*}{Complexity} \\
        \midrule
        \multirow{1}*{self-attention}
        &$4D^2$   &$O(T^2 \cdot D)$  \\
        \multirow{1}*{position-wise FFN}
        &$8D^2$   &$O(T \cdot D)$    \\
        \bottomrule
    \end{tabular}}
    \caption{Parameters and complexity of self-attention and position-wise FFN. $D$ is the hidden dimension and $T$ is the input sequence length. The intermediate dimension of FFN is set to $4D$.}
    \label{tbl.transformer}
\end{table}

\paragraph{\textbf{MLP-G}}
In this generator module, when the number of patches increases, the Params will increase because each patch associates to a MLP generator module. Although the FLOPs of each MLP generator module decrease, the total number of points (2048 in our experiments) keep fixed, and thus the total FLOPs keep the same.

\paragraph{\textbf{PointTrans-G}}
The parameters in PointTrans-G will increase with more input patches. Although the FLOPs of the MLP layers keep the same, the FLOPs of pointwise transformer layer decrease because of the $O(T^2 \cdot D)$ complexity in Table~\ref{tbl.transformer}.

\paragraph{\textbf{DualTrans-G}}
The DualTrans-G is different from the previous two generator modules because all patches are fed in simultaneously. 
% There are two different types of transformers, pointwise transformer and patchwise transformer. 
The output of the last patchwise transformer layer is point clouds with size $N_k \times 3$, where $N_k$ is the number of points on current patch. When the number of patches increases, $N_k$ decreases and then leads to small Params. In our settings, the input and output channels of transformer layers are fixed. Therefore, when the number of patches doubles, the overall FLOPs will increase.
% The number of patches is the same as the pointwise transformer. With more patches, the 

\paragraph{\textbf{Self-Attention Heads}}
The number of transformer layers and the self-attention heads in each type of transformer are also important hyper-parameters of Point-Trans-G and DualTrans-G. Here, we give the model complexity when using different parameters, shown in Table~\ref{tbl.PointTrans-G_transfomer} and Table~\ref{tbl.DualTrans-G_transfomer}. In our experiments, these parameters does not have a huge influence on the final generated results, but on the model parameters and training time.

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        \multirow{1}{*}{PointTrans-G} &\multirow{1}{*}{Num} &\multirow{1}{*}{Heads} &\multirow{1}{*}{Params} &\multirow{1}{*}{FLOPs} \\
        \midrule
        \multirow{8}*{pointwise transformer}
        &\multirow{2}*{2}   &6  &4.53M    &1.45G  \\
        &~                  &8  &4.57M    &1.47G  \\
        \cmidrule(lr){2-5}
        &\multirow{3}*{4}   &4  &8.53M    &2.89G  \\
        &~                  &6  &8.46M    &2.85G  \\
        &~                  &8  &8.53M    &2.89G  \\
        \cmidrule(lr){2-5}
        &\multirow{3}*{6}   &4  &12.49M   &4.30G  \\
        &~                  &6  &12.39M   &4.24G  \\
        &~                  &8  &12.49M   &4.30G  \\
        \bottomrule
    \end{tabular}}
    \caption{PointTrans-G complexity of different transformer settings. Num is the number of pointwise transformer layers between MLP layers and Heads is the number of self-attention heads in the transformer.}
    \label{tbl.PointTrans-G_transfomer}
\end{table}

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        \multirow{1}{*}{DualTrans-G} &\multirow{1}{*}{Num} &\multirow{1}{*}{Heads} &\multirow{1}{*}{Params} &\multirow{1}{*}{FLOPs} \\
        \midrule
        \multirow{8}*{\tabincell{c}{pointwise transformer \\ or \\ patchwise transformer}}
        &\multirow{2}*{4}   &6   &80.84M   &1.92G  \\
        &~                  &10  &80.84M   &1.92G  \\
        \cmidrule(lr){2-5}
        &\multirow{3}*{6}   &6   &0.13G    &3.06G  \\
        &~                  &8   &0.13G    &3.07G  \\
        &~                  &10  &0.13G    &3.06G  \\
        \cmidrule(lr){2-5}
        &\multirow{3}*{8}   &6   &0.18G   &4.21G  \\
        &~                  &8   &0.18G   &4.21G  \\
        &~                  &10  &0.18G   &4.21G  \\
        \bottomrule
    \end{tabular}}
    \caption{DualTrans-G complexity of different transformer settings. Num is the number of transformer layers and Heads is the number of self-attention heads in each transformer. Num and Heads apply to both transformers. For example, Num=4 and Heads=6 mean that there are 4 pointwise transformer layers and 4 patchwise transformer layers in DualTrans-G, and each transformer have 6 self-attention heads.}
    \label{tbl.DualTrans-G_transfomer}
\end{table}

%-------------------------------------------------------------------------
\section{More Results}

In this section, we present additional generated results of our method in Figure~\ref{fig.results_supp}. Furthermore, we provide a qualitative comparison by showcasing examples from three representative generative works, namely raw-GAN~\cite{achlioptas2018learning}, PointFlow~\cite{yang2019pointflow}, and SetVAE~\cite{kim2021setvae} in Figure~\ref{fig.others}.

The raw-GAN~\cite{achlioptas2018learning} was the first work to propose a deep generative model that utilizes sampled Gaussian noise vectors to generate point cloud shapes. Yang et al.\cite{yang2019pointflow} introduced the PointFlow, a point cloud generation method that uses continuously normalizing flows. Kim et al.\cite{kim2021setvae} presented the SetVAE, which applies set transforms to the classic VAE model.

% Figure environment removed


% Figure environment removed

% \cleardoublepage

\end{document}
