%State-of-the-art approaches to ontology learning (OL) are generally focusing on extracting knowledge from unstructured text by relying on traditional NLP methods to enrich or construct ontologies.
One of the earliest approaches~\cite{hearst1998automated} used lexicosyntactic patterns to extract new lexicosemantic concepts and relations from large collections of unstructured text, enhancing WordNet~\cite{miller1995wordnet}. 
WordNet is a lexical database comprising a lexical ontology of concepts (nouns, verbs, etc.) and lexico-semantic relations (synonymy, hyponymy, etc.). Hwang~\cite{hwang1999incompletely} proposed an alternative approach for constructing a dynamic ontology specific to an application domain. The method involved iteratively discovering types and taxonomy from unstructured text using a seed set of terms representing high-level domain types. In each iteration, newly discovered specialized types were incorporated, and the algorithm detected relations between linguistic features. The approach utilized a simple ontology algebra based on inheritance hierarchy and set operations.
%WordNet is a lexical database defined by an ontology of lexical concepts, e.g., nouns, verbs, etc., and lexico-semantic relations, e.g., synonymy, hyponymy, etc. Another approach by Hwang~\cite{hwang1999incompletely} proposed a method to dynamically construct an ontology for a given application domain including type and taxonomy discovery from unstructured text. It was an iterative algorithm based on a seed set of terms representing high-level types of a domain which in every iteration of the algorithm was bootstrapped with newly discovered specialized types. The algorithm detected relations between linguistic features and implemented simple ontology algebra based on inheritance hierarchy and set operations. 
Agirre et al.\cite{agirre2000enriching} enhanced WordNet by extracting topically related words from web documents. This unique approach added topical signatures to enrich WordNet. Kietz et al.\cite{kietz2000method} introduced the On-To-Knowledge system, which utilized a generic core ontology like GermaNet~\cite{hamp1997germanet} or WordNet as the foundational structure. It aimed to discover a domain-specific ontology from corporate intranet text resources. For concept extraction and pruning, it employed statistical term frequency count heuristics, while association rules were applied for relation identification in corporate texts.
%Agirre et al.~\cite{agirre2000enriching} enriched WordNet by extracting topically related words through queries made to the documents on the World Wide Web. Enriching WordNet with topical signatures of terms was a unique perspective to OL. Kietz et al.~\cite{kietz2000method} presented the On-To-Knowledge system, which leveraged a generic core ontology such as GermaNet~\cite{hamp1997germanet} or WordNet, as a top-level structure, to discover a corporate domain-specific goal ontology from corporate intranet text resources. For domain-specific concept and relation acquisition, it applied statistical term frequency count heuristics for concept extraction and pruning as well as association rules for relations on the corporate texts.
Roux et al.\cite{roux2000ontology} proposed a method to expand a genetics ontology by reusing existing domain ontologies and enhancing concepts through verb patterns extracted from unstructured text. Their system utilized linguistic tools like part-of-speech taggers and syntactic parsers. Wagner \cite{wagner2000enriching} employed statistical analysis of corpora to enrich WordNet in non-English languages by discovering relations, adding new terms to concepts, and acquiring concepts through the automatic acquisition of verb preferences.
%Roux et al.~\cite{roux2000ontology} proposed a method to build a larger ontology for genetics by reusing existing genetics' domain ontologies and enriching its concepts by applying a system of verb patterns that specify relations between concepts extracted from unstructured text. Their system operated based on outputs from traditional linguistic tools such as part-of-speech taggers and syntactic parsers. Wagner~\cite{wagner2000enriching} used statistical analysis of corpora to enrich lexical WordNet in other languages than English by discovering new relations, enriching concepts with new terms, and acquiring new concepts via the automatic acquisition of selectional preferences of verbs relating terms. 
Moldovan and Girju~\cite{moldovan2001interactive} introduced the Knowledge Acquisition from Text (KAT) system to enrich WordNet's finance domain coverage. Their method involved four stages: (1) discovering new concepts from a seed set of terms, expanding the concept list using dictionaries; (2) identifying lexical patterns from new concepts; (3) discovering relations from lexical patterns; and (4) integrating extracted information into WordNet using a knowledge classification algorithm.
%Moldovan and Girju~\cite{moldovan2001interactive} introduced the Knowledge Acquisition from Text (KAT) system to discover finance domain-specific concepts and relations based on a seed set of terms to lexically enrich WordNet's coverage of this domain. Their method operated in four stages: (1) discovery of new concepts based on a seed set of terms from POS-tagged and parsed sentences, expansion of the concept list based on dictionaries; (2) discovery of lexical patterns from new concepts; (3) discovering relations from lexical patterns; and (4) a knowledge classification algorithm to integrate the extracted information in WordNet. 
In \cite{alfonseca2002unsupervised}, an unsupervised method is presented to enhance ontologies with domain-specific information using NLP techniques such as NER and WSD. The method utilizes a general NER system to uncover a taxonomic hierarchy and employs WSD to enrich existing synsets by querying the internet for new terms and disambiguating them through cooccurrence frequency.
%In \cite{alfonseca2002unsupervised}, a fully unsupervised method is proposed to enrich ontologies with domain-dependent information using NLP methods including NER and WSD. The approach uses a general NER system to discover a taxonomic hierarchy and employs WSD to enrich existing synsets by querying the internet for new terms and disambiguating them based on cooccurrence frequency. 
Khan and Luo~\cite{khan2002ontology} employed clustering techniques to find new terms, utilizing WordNet for typing. They used the self-organizing tree algorithm~\cite{dopazo1997phylogenetic}, inspired by molecular evolution, to establish an ontology hierarchy. Additionally, Xu et al.~\cite{xu2002domain} focused on automatically acquiring domain-specific terms and relations through a TFIDF-based single-word term classifier, a lexico-syntactic pattern finder based on known relations and collocations, and a relation extractor utilizing discovered lexico-syntactic patterns.
%Khan and Luo~\cite{khan2002ontology} used clustering techniques to discover new terms that were typed using WordNet. And inspired from the field of molecular evolution, they used the self-organizing tree algorithm~\cite{dopazo1997phylogenetic} to discover a hierarchy for an ontology. Finally, Xu et al.~\cite{xu2002domain} focused on the automatic acquisition of domain-relevant terms and their relations using: TFIDF-based single-word term classifier, lexico-syntactic pattern finder based on known relations from lexicosemantic databases or term collocations, and relation extractor by applying the discovered lexico-syntactic patterns. 
% \footnote{Access the related work as a structured comparison on the Open Research Knowledge Graph (ORKG) at \url{https://orkg.org/comparison/R586328/}.}

%The seminal work by Maedche and Staab~\cite{maedche2001ontology} offered a framework for ontology learning for the semantic web which was formulated as a methodology involving four main tasks, i.e. 1) import and reuse, 2) extraction, 3) pruning, and 4) refinement.
Predominantly, the approaches for OL~\cite{wkatrobski2020ontology} that stand out so far are based on lexico-syntactic patterns for term and relation extraction as well as clustering for type discovery. Otherwise, they build on seed-term-based bootstrapping methods. The reader is referred to further detailed reviews~\cite{r3-soa1,r3-soa2} on this theme for a comprehensive overall methodological picture for OL. Traditional NLP was defined by modular pipelines by which machines were equipped step-wise with annotations at the linguistic, syntactic, and semantic levels to process text. LLMs have ushered in a new era of possibilities for AI systems that obviate the need for modular NLP systems to understand natural language which we tap into for the first time for the OL task in this work.