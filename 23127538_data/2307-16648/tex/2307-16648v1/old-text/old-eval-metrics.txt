\subsection{Evaluation Metrics} \label{metrics}

\subsubsection{MAP@K}: Since in task A, we try to query LLMs for entity types identifications capabilities so, Mean Average Precision at K (MAP@K), which is a commonly used evaluation metric in information retrieval is the metric for this task evaluation. In essence, MAP@K measures the average precision of the top K-ranked items that are returned by a system for a given query or prompt (we used entities in templates as a prompt). 

$$MAP@K = \frac{1}{N} \sum_{j=1}^{N} \frac{1}{r} \sum_{k=1}^{K} P@k \cdot rel(k)$$

$$
  rel(k)=\left\{
  \begin{array}{@{}lll@{}}
    0, && \text{if}\;item\;at\;k_{th}\;is\;relevent \\
    1, && \text{otherwise}
  \end{array}\right.
$$
Where $N$ is the total number of samples in the test set, $r$ is the number of relevant items, $K$ is the number of items to consider in the evaluation, and $P@k$ is the precision at cutoff $k$. $rel_k$ is a binary indicator variable for relatedness.
\vspace{-5mm}
\subsubsection{F1-Score}: For tasks B and C we use the F1-Score evaluation metric, which is calculated as the harmonic mean of precision and recall. The objective is to minimize the number of false positives and false negatives.