
@inproceedings{hambardzumyan-etal-2021-warp,
    title = "{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming",
    author = "Hambardzumyan, Karen  and
      Khachatrian, Hrant  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.381",
    doi = "10.18653/v1/2021.acl-long.381",
    pages = "4921--4933"
}

@InProceedings{setexpan,
author="Shen, Jiaming
and Wu, Zeqiu
and Lei, Dongming
and Shang, Jingbo
and Ren, Xiang
and Han, Jiawei",
editor="Ceci, Michelangelo
and Hollm{\'e}n, Jaakko
and Todorovski, Ljup{\v{c}}o
and Vens, Celine
and D{\v{z}}eroski, Sa{\v{s}}o",
title="SetExpan: Corpus-Based Set Expansion via Context Feature Selection and Rank Ensemble",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="288--304",
abstract="Corpus-based set expansion (i.e., finding the ``complete'' set of entities belonging to the same semantic class, based on a given corpus and a tiny set of seeds) is a critical task in knowledge discovery. It may facilitate numerous downstream applications, such as information extraction, taxonomy induction, question answering, and web search.",
isbn="978-3-319-71249-9"
}

@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{kojima2023large,
      title={Large Language Models are Zero-Shot Reasoners}, 
      author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
      year={2023},
      eprint={2205.11916},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{min2022rethinking,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{
    wei2022finetuned,
    title={Finetuned Language Models are Zero-Shot Learners},
    author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@inproceedings{NEURIPS20229d560961,
     author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
     pages = {24824--24837},
     publisher = {Curran Associates, Inc.},
     title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
     url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
     volume = {35},
     year = {2022}
}

@misc{yao2023tree,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{khot2023decomposed,
      title={Decomposed Prompting: A Modular Approach for Solving Complex Tasks}, 
      author={Tushar Khot and Harsh Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
      year={2023},
      eprint={2210.02406},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{r3-soa2,
  title={A survey on methods of ontology learning from text},
  author={Lourdusamy, Ravi and Abraham, Stanislaus},
  booktitle={Intelligent Computing Paradigm and Cutting-edge Technologies: Proceedings of the First International Conference on Innovative Computing and Cutting-edge Technologies (ICICCT 2019), Istanbul, Turkey, October 30-31, 2019 1},
  pages={113--123},
  year={2020},
  organization={Springer}
}

@article{r3-soa1,
  title={A survey of ontology learning techniques and applications},
  author={Asim, Muhammad Nabeel and Wasim, Muhammad and Khan, Muhammad Usman Ghani and Mahmood, Waqar and Abbasi, Hafiza Mahnoor},
  journal={Database},
  volume={2018},
  pages={bay101},
  year={2018},
  publisher={Oxford University Press}
}

@article{pubmedbert,
  title={Domain-specific language model pretraining for biomedical natural language processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}

@article{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@misc{SNOMEDCT-US,
  title = {{US Edition of SNOMED CT}},
  author = {{SNOMED International}},
  address = {{London, United Kingdom}},
  publisher = {{SNOMED International}},
  month = {March},
  year = {2023},
  url={https://www.nlm.nih.gov/healthit/snomedct/us_edition.html}
}

@misc{NCI,
  title = {{NCI Thesaurus}},
  author = {{National Cancer Institute, National Institutes of Health}},
  address = {{Bethesda, MD}},
  publisher = {{National Cancer Institute}},
  edition = {22.09d},
  month = {September},
  year = {2022},
  url={http://ncit.nci.nih.gov}
}

@misc{MEDCIN,
  title = {{MEDCIN}},
  author = {{Medicomp Systems}},
  address = {{Chantilly, VA}},
  publisher = {{Medicomp Systems}},
  month = {January},
  year = {2023},
  url = {https://medicomp.com}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{cui2021template,
  title={Template-based named entity recognition using BART},
  author={Cui, Leyang and Wu, Yu and Liu, Jian and Yang, Sen and Zhang, Yue},
  journal={arXiv preprint arXiv:2106.01760},
  year={2021}
}

@misc{noy2001ontology,
  title={Ontology development 101: A guide to creating your first ontology},
  author={Noy, Natalya F and McGuinness, Deborah L and others},
  year={2001},
  publisher={Stanford knowledge systems laboratory technical report KSL-01-05 and~…}
}

@article{amatriain2023transformer,
  title={Transformer models: an introduction and catalog},
  author={Amatriain, Xavier},
  journal={arXiv preprint arXiv:2302.07730},
  year={2023}
}

@article{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1909.01066},
  year={2019}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{dale2021gpt,
  title={GPT-3: What’s it good for?},
  author={Dale, Robert},
  journal={Natural Language Engineering},
  volume={27},
  number={1},
  pages={113--118},
  year={2021},
  publisher={Cambridge University Press}
}

@article{thiergart2021understanding,
  title={Understanding emails and drafting responses--An approach using GPT-3},
  author={Thiergart, Jonas and Huber, Stefan and {\"U}bellacker, Thomas},
  journal={arXiv preprint arXiv:2102.03062},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{dodge2019show,
  title={Show your work: Improved reporting of experimental results},
  author={Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A},
  journal={arXiv preprint arXiv:1909.03004},
  year={2019}
}

@article{guha2016schema,
  title={Schema. org: evolution of structured data on the web},
  author={Guha, Ramanathan V and Brickley, Dan and Macbeth, Steve},
  journal={Communications of the ACM},
  volume={59},
  number={2},
  pages={44--51},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{weibel2000dublin,
  title={The Dublin core metadata initiative},
  author={Weibel, Stuart L and Koch, Traugott},
  journal={D-lib magazine},
  volume={6},
  number={12},
  pages={1082--9873},
  year={2000}
}

@inproceedings{rebele2016yago,
  title={YAGO: A multilingual knowledge base from wikipedia, wordnet, and geonames},
  author={Rebele, Thomas and Suchanek, Fabian and Hoffart, Johannes and Biega, Joanna and Kuzey, Erdal and Weikum, Gerhard},
  booktitle={The Semantic Web--ISWC 2016: 15th International Semantic Web Conference, Kobe, Japan, October 17--21, 2016, Proceedings, Part II 15},
  pages={177--185},
  year={2016},
  organization={Springer}
}

@article{bodenreider2004unified,
  title={The unified medical language system (UMLS): integrating biomedical terminology},
  author={Bodenreider, Olivier},
  journal={Nucleic acids research},
  volume={32},
  number={suppl\_1},
  pages={D267--D270},
  year={2004},
  publisher={Oxford University Press}
}

@article{wkatrobski2020ontology,
  title={Ontology learning methods from text-an extensive knowledge-based approach},
  author={W{\c{a}}tr{\'o}bski, Jaros{\l}aw},
  journal={Procedia Computer Science},
  volume={176},
  pages={3356--3368},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{xu2002domain,
  title={A Domain Adaptive Approach to Automatic Acquisition of Domain Relevant Terms and their Relations with Bootstrapping.},
  author={Xu, Feiyu and Kurz, Daniela and Piskorski, Jakub and Schmeier, Sven},
  booktitle={LREC},
  year={2002}
}

@inproceedings{missikoff2002usable,
  title={The usable ontology: An environment for building and assessing a domain ontology},
  author={Missikoff, Michele and Navigli, Roberto and Velardi, Paola},
  booktitle={The Semantic Web—ISWC 2002: First International Semantic Web Conference Sardinia, Italy, June 9--12, 2002 Proceedings},
  pages={39--53},
  year={2002},
  organization={Springer}
}

@inproceedings{lonsdale2002peppering,
  title={Peppering knowledge sources with SALT: Boosting conceptual content for ontology generation},
  author={Lonsdale, Deryle and Ding, Yihong and Embley, David W and Melby, Alan},
  booktitle={Proceedings of the AAAI Workshop on Semantic Web Meets Language Resources, Edmonton, Alberta, Canada},
  year={2002}
}

@article{dopazo1997phylogenetic,
  title={Phylogenetic reconstruction using an unsupervised growing neural network that adopts the topology of a phylogenetic tree},
  author={Dopazo, Joaquin and Carazo, Jos{\'e} Mar{\'\i}a},
  journal={Journal of molecular evolution},
  volume={44},
  number={2},
  pages={226--233},
  year={1997},
  publisher={Springer}
}

@inproceedings{khan2002ontology,
  title={Ontology construction for information selection},
  author={Khan, Latifur and Luo, Feng},
  booktitle={14th IEEE International Conference on Tools with Artificial Intelligence, 2002.(ICTAI 2002). Proceedings.},
  pages={122--127},
  year={2002},
  organization={IEEE}
}

@inproceedings{alfonseca2002unsupervised,
  title={An unsupervised method for general named entity recognition and automated concept discovery},
  author={Alfonseca, Enrique and Manandhar, Suresh},
  booktitle={Proceedings of the 1st international conference on general WordNet, Mysore, India},
  pages={34--43},
  year={2002}
}

@article{moldovan2001interactive,
  title={An interactive tool for the rapid development of knowledge bases},
  author={Moldovan, Dan I and GiRJU, ROXANA C},
  journal={International Journal on Artificial Intelligence Tools},
  volume={10},
  number={01n02},
  pages={65--86},
  year={2001},
  publisher={World Scientific}
}

@inproceedings{labeldecomposition,
	doi = {10.1145/3485447.3511998},
	url = {https://doi.org/10.1145\%2F3485447.3511998},
	year = 2022,
	month = {apr},
	publisher = {{ACM}},
	author = {Xiang Chen and Ningyu Zhang and Xin Xie and Shumin Deng and Yunzhi Yao and Chuanqi Tan and Fei Huang and Luo Si and Huajun Chen},
	title = {{KnowPrompt}: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction},
	booktitle = {Proceedings of the {ACM} Web Conference 2022}
}

@inproceedings{constrainedanswerdesign,
    title = "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
    author = "Yin, Wenpeng  and
      Hay, Jamaal  and
      Roth, Dan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1404",
    doi = "10.18653/v1/D19-1404",
    pages = "3914--3923",
    abstract = "Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects: the {``}topic{''} aspect includes {``}sports{''} and {``}politics{''} as labels; the {``}emotion{''} aspect includes {``}joy{''} and {``}anger{''}; the {``}situation{''} aspect includes {``}medical assistance{''} and {``}water shortage{''}. ii) We extend the existing evaluation setup (label-partially-unseen) {--} given a dataset, train on some labels, test on all labels {--} to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way.",
}

@misc{chatgpt,
  title={ChatGPT},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/chat-gpt/}},
  note={Accessed May 5, 2023}
}

@misc{labelparaphrasing,
      title={How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering}, 
      author={Zhengbao Jiang and Jun Araki and Haibo Ding and Graham Neubig},
      year={2021},
      eprint={2012.00955},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{hahn2001joint,
  title={Joint knowledge capture for grammars and ontologies},
  author={Hahn, Udo and Mark{\'o}, Korn{\'e} G},
  booktitle={Proceedings of the 1st international conference on Knowledge capture},
  pages={68--75},
  year={2001}
}

@inproceedings{wagner2000enriching,
  title={Enriching a lexical semantic net with selectional preferences by means of statistical corpus analysis.},
  author={Wagner, Andreas},
  booktitle={ECAI Workshop on Ontology Learning},
  volume={61},
  year={2000},
  organization={Citeseer}
}

@inproceedings{roux2000ontology,
  title={An Ontology Enrichment Method for a Pragmatic Information Extraction System gathering Data on Genetic Interactions.},
  author={Roux, Claude and Proux, Denys and Rechenmann, Francois and Julliard, Laurent},
  booktitle={ECAI Workshop on Ontology Learning},
  year={2000}
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@inproceedings{hamp1997germanet,
  title={Germanet-a lexical-semantic net for german},
  author={Hamp, Birgit and Feldweg, Helmut},
  booktitle={Automatic information extraction and building of lexical semantic resources for NLP applications},
  year={1997}
}

@inproceedings{kietz2000method,
  title={A method for semi-automatic ontology acquisition from a corporate intranet},
  author={Kietz, Joerg-Uwe and Maedche, Alexander and Volz, Raphael},
  booktitle={EKAW-2000 Workshop “Ontologies and Text”, Juan-Les-Pins, France, October 2000},
  year={2000}
}

@inproceedings{agirre2000enriching,
  title={Enriching very large ontologies using the WWW},
  author={Agirre, Eneko and Ansa, Olatz and Hovy, Eduard and Mart{\'\i}nez, David},
  booktitle={Proceedings of the First International Conference on Ontology Learning-Volume 31},
  pages={25--30},
  year={2000}
}

@inproceedings{hwang1999incompletely,
  title={Incompletely and imprecisely speaking: using dynamic ontologies for representing and retrieving information.},
  author={Hwang, Chung Hee},
  booktitle={KRDB},
  volume={21},
  pages={14--20},
  year={1999},
  organization={Citeseer}
}

@article{hearst1998automated,
  title={Automated discovery of WordNet relations},
  author={Hearst, Marti A},
  journal={WordNet: an electronic lexical database},
  volume={2},
  year={1998},
  publisher={MIT Press Cambridge}
}

@article{maedche2001ontology,
  title={Ontology learning for the semantic web},
  author={Maedche, Alexander and Staab, Steffen},
  journal={IEEE Intelligent systems},
  volume={16},
  number={2},
  pages={72--79},
  year={2001},
  publisher={IEEE}
}

@article{konys2019knowledge,
  title={Knowledge repository of ontology learning tools from text},
  author={Konys, Agnieszka},
  journal={Procedia Computer Science},
  volume={159},
  pages={1614--1628},
  year={2019},
  publisher={Elsevier}
}

@article{prompting,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3560815},
doi = {10.1145/3560815},
abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website including constantly updated survey and paperlist.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {195},
numpages = {35},
keywords = {Pre-trained language models, prompting}
}

@misc{geonames,
  title = {GeoNames geographical database},
  year = {2023},
  url = {http://www.geonames.org/}
}

@InProceedings{wn18rr,
	Author = {Dettmers, Tim and Pasquale, Minervini and Pontus, Stenetorp and Riedel, Sebastian},
	Booktitle = {Proceedings of the 32th AAAI Conference on Artificial Intelligence},
	Title = {Convolutional 2D Knowledge Graph Embeddings},
	Url = {https://arxiv.org/abs/1707.01476},
	Year = {2018},
        pages  = {1811--1818},
  	Month = {February}
}

@article{umls,
    author = {Bodenreider, Olivier},
    title = "{The Unified Medical Language System (UMLS): integrating biomedical terminology}",
    journal = {Nucleic Acids Research},
    volume = {32},
    number = {suppl\_1},
    pages = {D267-D270},
    year = {2004},
    month = {01},
    abstract = "{ The Unified Medical Language System ( http://umlsks.nlm.nih.gov ) is a repository of biomedical vocabularies developed by the US National Library of Medicine. The UMLS integrates over 2 million names for some 900 000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. Vocabularies integrated in the UMLS Metathesaurus include the NCBI taxonomy, Gene Ontology, the Medical Subject Headings (MeSH), OMIM and the Digital Anatomist Symbolic Knowledge Base. UMLS concepts are not only inter‐related, but may also be linked to external resources such as GenBank. In addition to data, the UMLS includes tools for customizing the Metathesaurus (MetamorphoSys), for generating lexical variants of concept names (lvg) and for extracting UMLS concepts from text (MetaMap). The UMLS knowledge sources are updated quarterly. All vocabularies are available at no fee for research purposes within an institution, but UMLS users are required to sign a license agreement. The UMLS knowledge sources are distributed on CD‐ROM and by FTP. }",
    issn = {0305-1048},
    doi = {10.1093/nar/gkh061},
    url = {https://doi.org/10.1093/nar/gkh061},
    eprint = {https://academic.oup.com/nar/article-pdf/32/suppl\_1/D267/7621558/gkh061.pdf},
}




@InProceedings{schemaorg,
author="Patel-Schneider, Peter F.",
editor="Mika, Peter
and Tudorache, Tania
and Bernstein, Abraham
and Welty, Chris
and Knoblock, Craig
and Vrande{\v{c}}i{\'{c}}, Denny
and Groth, Paul
and Noy, Natasha
and Janowicz, Krzysztof
and Goble, Carole",
title="Analyzing Schema.org",
booktitle="The Semantic Web -- ISWC 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="261--276",
abstract="Schema.org is a way to add machine-understandable information to web pages that is processed by the major search engines to improve search performance. The definition of schema.org is provided as a set of web pages plus a partial mapping into RDF triples with unusual properties, and is incomplete in a number of places. This analysis of and formal semantics for schema.org provides a complete basis for a plausible version of what schema.org should be.",
isbn="978-3-319-11964-9"
}


@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@misc{palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{t5,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2020},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{flant5,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{torontobookcorpus,
      title={Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books}, 
      author={Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
      year={2015},
      eprint={1506.06724},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ontologylearning,
  title={Ontology learning for the semantic web},
  author={Maedche, Alexander and Staab, Steffen},
  journal={IEEE Intelligent systems},
  volume={16},
  number={2},
  pages={72--79},
  year={2001},
  publisher={IEEE},
  doi={10.1109/5254.920602}
}

@article{gruber1995toward,
  title={Toward principles for the design of ontologies used for knowledge sharing?},
  author={Gruber, Thomas R},
  journal={International journal of human-computer studies},
  volume={43},
  number={5-6},
  pages={907--928},
  year={1995},
  publisher={Elsevier}
}

@inproceedings{lms-as-kb,
  title={Language Models as Knowledge Bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  year={2019},
  organization={Association for Computational Linguistics}
}


@inproceedings{
petroni2020how,
title={How Context Affects Language Models' Factual Predictions},
author={Fabio Petroni and Patrick Lewis and Aleksandra Piktus and Tim Rockt{\"a}schel and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
booktitle={Automated Knowledge Base Construction},
year={2020},
url={https://openreview.net/forum?id=025X0zPfn}
}



@article{jiang-etal-2020-know,
    title = "How Can We Know What Language Models Know?",
    author = "Jiang, Zhengbao  and
      Xu, Frank F.  and
      Araki, Jun  and
      Neubig, Graham",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.28",
    doi = "10.1162/tacl\_a\_00324",
    pages = "423--438",
}


@inproceedings{akkalyoncu-yilmaz-etal-2019-applying,
    title = "Applying {BERT} to Document Retrieval with Birch",
    author = "Akkalyoncu Yilmaz, Zeynep  and
      Wang, Shengjin  and
      Yang, Wei  and
      Zhang, Haotian  and
      Lin, Jimmy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-3004",
    doi = "10.18653/v1/D19-3004",
    pages = "19--24",
    abstract = "We present Birch, a system that applies BERT to document retrieval via integration with the open-source Anserini information retrieval toolkit to demonstrate end-to-end search over large document collections. Birch implements simple ranking models that achieve state-of-the-art effectiveness on standard TREC newswire and social media test collections. This demonstration focuses on technical challenges in the integration of NLP and IR capabilities, along with the design rationale behind our approach to tightly-coupled integration between Python (to support neural networks) and the Java Virtual Machine (to support document retrieval using the open-source Lucene search library). We demonstrate integration of Birch with an existing search interface as well as interactive notebooks that highlight its capabilities in an easy-to-understand manner.",
}

@inproceedings{levy-etal-2017-zero,
    title = "Zero-Shot Relation Extraction via Reading Comprehension",
    author = "Levy, Omer  and
      Seo, Minjoon  and
      Choi, Eunsol  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1034",
    doi = "10.18653/v1/K17-1034",
    pages = "333--342",
    abstract = "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.",
}

@misc{yang2019simple,
      title={Simple Applications of BERT for Ad Hoc Document Retrieval}, 
      author={Wei Yang and Haotian Zhang and Jimmy Lin},
      year={2019},
      eprint={1903.10972},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{dai2019transformerxl,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{
dalvi2022discovering,
title={Discovering Latent Concepts Learned in {BERT}},
author={Fahim Dalvi and Abdul Rafae Khan and Firoj Alam and Nadir Durrani and Jia Xu and Hassan Sajjad},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=POTMtpYI1xH}
}

@misc{sajjad2022analyzing,
      title={Analyzing Encoded Concepts in Transformer Language Models}, 
      author={Hassan Sajjad and Nadir Durrani and Fahim Dalvi and Firoj Alam and Abdul Rafae Khan and Jia Xu},
      year={2022},
      eprint={2206.13289},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}