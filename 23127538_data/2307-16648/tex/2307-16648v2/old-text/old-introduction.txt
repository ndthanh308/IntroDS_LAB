%LLMs with their volumes of underlying text are shown to be knowledge bases, but can we take this one step further 

%the advent of LLMs have opened up new avenues of research...

%Large language models (LLMs) like GPT-3 \cite{gpt3} or BERT \cite{bert} have made significant progress in natural language processing and have shown remarkable performance on various tasks such as language translation, question answering, and text generation. A specific characteristic of LLMs is, that they are good at crystallizing textual knowledge, which can be found in various existing documents. They are not suited to generate completely new insights. A task, which can potentially benefit a lot from this characteristic of LLMs is Ontology Learning since here it's about extracting a common understanding of concepts and relationships from a variety of sources; i.e. extract a shared conceptualization~\cite{gruber1995toward}.

%discuss the phenomenon of emergence in the context of LLMs
LLMs have demonstrated emergent abilities that have opened up new possibilities in NLP. One of the most impressive emergent abilities of LLMs is their ability to generate human-like text, which has important implications for tasks such as content creation and dialogue systems~\cite{dodge2019show,radford2019language,thiergart2021understanding,dale2021gpt}. LLMs can also perform tasks such as writing coherent stories and articles, and can even summarize lengthy texts with a high degree of accuracy. Another emergent ability of LLMs is their capability for multilingual communication, as they can now understand and generate text in multiple languages. Additionally, LLMs can be fine-tuned for specific domains, enabling them to perform specialized tasks such as medical diagnosis or legal analysis. These emergent abilities of LLMs are driving breakthroughs in a wide range of fields, and as researchers continue to push the boundaries of what LLMs can do, we can expect to see even more impressive capabilities emerge in the future.

%\textit{Are LLMs indeed capable of OL?} is the central question examined via the LLMs4OL paradigm proposed in this work. Recent investigations of LLMs are abuzz with discoveries of hundreds of ``emergent'' abilities of LLMs~\cite{srivastava2022beyond,wei2022emergent}. They range from multiplication to generating executable computer code to, apparently, decoding movies based on emojis. 
%LLMs can also perform tasks such as writing coherent stories and articles, and can even summarize lengthy texts with a high degree of accuracy.
%``An ability is emergent if it is not present in smaller models but is present in larger models.''~\cite{wei2022emergent}. By the sheer scale of text, in the range of billions to trillions of tokens, that power LLMs, they are showing signs of Artificial General Intelligence (AGI) displaying an ability to capture complex linguistic relationships to generate human-like responses~\cite{dodge2019show,radford2019language,thiergart2021understanding,dale2021gpt} on new tasks that were not known to have been explicitly encoded during the models training. Of course scientists question whether LLMs truly show AGI or are they getting really good at statistics, i.e. learning heuristics that are out of reach for those with fewer parameters or lower-quality data~\cite{srivastava2022beyond}. Regardless, investigations are rife to empirically quantify the performance of LLMs on known and new tasks toward establishing a shared scientific understanding of the operations in LLMs. By the LLMs4OL task proposed and investigated in this work, we take this one step further. 