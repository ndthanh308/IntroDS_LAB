\section{Experiments}

To validate the effectiveness of our proposed method, we conduct a series of experiments to answer the following research questions:
\begin{itemize}[leftmargin=*]
\item[-] \textbf{RQ1:} How does ULC perform on the CVR prediction task compared to the state-of-the-art methods?
\item[-] \textbf{RQ2:} How do the label correctness and data freshness of counterfactual labeling affect the performance of ULC?
\item[-] \textbf{RQ3:} In addition to embedding-based alternative training, how do other common schemes such as joint learning perform?
\item[-] \textbf{RQ4:} How does the ULC model perform on samples with different delay time?
\end{itemize}

\subsection{Dataset and Settings}
\subsubsection{Datasets} 
To our knowledge, there exists only one public dataset \cite{201401} widely used in the research of the delayed feedback problem in the offline setting. Other public CVR datasets do not have noticeably delayed feedback or lack enough temporal information. Following the common settings in previous work \cite{201401,202006} of using one public and one private dataset, we also introduce a collected private production dataset. 

\textbf{Criteo dataset.} This public dataset contains clicks and the corresponding conversions from Criteo live traffic data. Each sample corresponds to a single click and is described by several categorical features and continuous features, with the corresponding conversion information, if any. It also includes the timestamps of the click and the possible conversion behavior. We use this dataset's last 23 days of data to conduct our experiments. Following previous work \cite{201401, 202006}, three consecutive weeks of data are leveraged as training data, data of the 22nd day is used for validation and the last day is for the testing.
Note that this dataset tracks conversion behavior for each click sample, so the ground truth $c_i$ is available for testing. For validation, we assume that $c_i$ is unknown and use the label-corrected loss for parameter selection. The processed dataset includes 6,363,085 click samples with a conversion rate of 0.2294.

\textbf{Production dataset.} This dataset is collected from a real production platform with game advertising.
In-game payments are treated as conversions. Specifically, we collected and sampled one-month consecutive user feedback logs. The data format is similar to Criteo, and the last two days are used for validation and testing, respectively. The dataset includes over 2,400,000 click samples with a conversion rate of about 0.005.
Statistics are shown in Table \ref{datasets}.

\begin{table}[h]\small
\centering
\caption{Statistics of the datasets.}
\label{datasets}
\begin{tabular}
{p{0.6cm}p{0.4cm}p{0.9cm}p{0.6cm}p{1.3cm}p{0.4cm}p{0.9cm}p{0.6cm}} 
%\hline
\toprule
\textbf{Name} & \textbf{Days} & \textbf{\#clicks} & 
\textbf{CVR} & | \textbf{Name} & \textbf{Days} & \textbf{\#clicks} & \textbf{CVR}\\ 
%\hline
\midrule
\textbf{Criteo} & \;\;23& 6,363,085 & 0.2294& |\,\textbf{Production} &\;\;30 & 2,400,000 & 0.0050\\ 
\bottomrule
%\hline
\end{tabular}%}
\end{table}

\subsubsection{Evaluation Metrics}
We adopt three metrics that are widely used in CVR prediction tasks \cite{202106, 202103}. The first metric is area under ROC curve (\textbf{AUC}) that measures the pairwise ranking performance of the CVR prediction model. The second is area under the precision-recall curve (\textbf{PRAUC})\cite{202103}, which also measures the pairwise ranking performance.
The third one is the log loss (\textbf{LL}), which measures the accuracy of the absolute value of the CVR prediction. 

To further analyze the benefits gained by solving the delayed feedback problem, we calculate the relative improvements (\textbf{RI}) to the maximum gain (i.e., the improvement of the oracle model over the vanilla model) on the above three metrics. For method $f$, the relative improvements on metric $M(\cdot)$ is defined as $\frac{M(f) - M(Vanilla)}{M(Oralce) - M(Vanilla)}$. Then we can obtain \textbf{RI-AUC}, \textbf{RI-PRAUC} and \textbf{RI-LL}.

\begin{table*}[t]
\centering
\caption{Performance comparisons of the proposed model with baseline models on the public Criteo dataset. The best results are in boldface, and the best baselines are underlined. The superscripts ** indicate $p \le 0.01$ for the t-test of ULC vs. the best baseline. $\uparrow$ means the higher the better, and  $\downarrow$ is for the lower the better.}
\label{main}
\resizebox{0.72\textwidth}{!}{%
\begin{tabular}{l|l|cccccc}
\toprule
\textbf{Backbone} & \textbf{Method}     & \textbf{AUC $\uparrow$}                  & \textbf{PRAUC $\uparrow$}                & \textbf{LL $\downarrow$}                    & \textbf{RI-AUC $\uparrow$}               & \textbf{RI-PRAUC $\uparrow$}             & \textbf{RI-LL $\uparrow$}  \\ \midrule
\multirow{6}{*}{\textbf{MLP}} & \textbf{Vanilla}   & 0.8208                         & 0.6356                         & 0.4505                         & 0.000                          & 0.000                          & 0.000          \\ 
& \textbf{Oracle}   & 0.8441                         & 0.6595                         & 0.4025                         & 1.000                 & 1.000                          & 1.000          \\ \cline{2-8}
& \textbf{DFM}       & 0.8264                         & 0.6398                        & 0.4378                        & 0.2403                         & 0.1757                         & 0.2645         \\
& \textbf{FSIW}      & \underline{0.8335}                   & \underline{0.6465}                   & \underline{0.4178}                   & \underline{0.5450}                   & \underline{0.4560}                   & \underline{0.6812}   \\
& \textbf{nnDF} & 0.6859 & 0.4169 & 0.5969 & -5.789 & -9.150 & -3.05         \\  \cline{2-8}
& \textbf{ULC(ours)}     & \textbf{0.8403**}              & \textbf{0.6543**}              & \textbf{0.4104**}              & \textbf{0.8369**}                       & \textbf{0.7824**}              & \textbf{0.8354**}       \\ \hline
\multirow{6}{*}{\textbf{DeepFM}} & \textbf{Vanilla}   & 0.822                        & 0.6379                         & 0.4451                        & 0.000                          & 0.000                          & 0.000          \\ 
& \textbf{Oracle}   & 0.8442                         & 0.66                         & 0.4023                         & 1.000                 & 1.000                          & 1.000          \\ \cline{2-8}
& \textbf{DFM}       & 0.8266                         & 0.6416                         & 0.4319                        & 0.2072                         & 0.1674                         & 0.3084         \\
& \textbf{FSIW}      & \underline{0.8326}                   & \underline{0.6451}                   & \underline{0.4195}                   & \underline{0.4774}                   & \underline{0.3257}                   & \underline{0.5981}   \\
& \textbf{nnDF} & 0.6994 & 0.4332 & 0.596 & -5.522 & -9.262 & -3.525         \\ \cline{2-8}
& \textbf{ULC(ours)}     & \textbf{0.8393**}              & \textbf{0.6525**}              & \textbf{0.4104**}              & \textbf{0.7792**}                       & \textbf{0.6606**}              & \textbf{0.8107**}       \\ \hline
\multirow{6}{*}{\textbf{AutoInt}} & \textbf{Vanilla}   & 0.8232                         & 0.6383                         & 0.4468                         & 0.000                          & 0.000                          & 0.000          \\ 
& \textbf{Oracle}   & 0.8442                        & 0.6601                         & 0.4025                        & 1.000                 & 1.000                          & 1.000          \\ \cline{2-8}
& \textbf{DFM}       & 0.8276                         & 0.6412                         & 0.4306                         & 0.2095                         & 0.1330                         & 0.3656         \\
& \textbf{FSIW}      & \underline{0.8329}                   & \underline{0.646}                   & \underline{0.4192}                   & \underline{0.4619}                   & \underline{0.3532}                   & \underline{0.6230}   \\
& \textbf{nnDF} & 0.6903 & 0.4214 & 0.6123 & -6.328 & -9.949 & -3.735         \\ \cline{2-8}
& \textbf{ULC(ours)}     & \textbf{0.8388**}              & \textbf{0.6517**}              & \textbf{0.4114**}              & \textbf{0.7428**}                       & \textbf{0.6146**}              & \textbf{0.7990**}       \\ \hline
\multirow{6}{*}{\textbf{DCNV2}} & \textbf{Vanilla}   & 0.8229                         & 0.6386                         & 0.4454                         & 0.000                          & 0.000                          & 0.000          \\ 
& \textbf{Oracle}   & 0.8447                         & 0.6606                         & 0.4018                         & 1.000                 & 1.000                          & 1.000          \\ \cline{2-8}
& \textbf{DFM}       & 0.8272                         & 0.6417                        & 0.4325                         & 0.1972                         & 0.1409                         & 0.2958         \\
& \textbf{FSIW}      & \underline{0.8328}                   & \underline{0.6461}                   & \underline{0.4174}                   & \underline{0.4541}                   & \underline{0.3409}                   & \underline{0.6422}   \\
& \textbf{nnDF} & 0.6867 & 0.423 & 0.6101 & -6.247 & -9.8 & -3.777         \\ \cline{2-8}
& \textbf{ULC(ours)}     & \textbf{0.8391**}              & \textbf{0.6519**}              & \textbf{0.4105**}              & \textbf{0.7431**}                       & \textbf{0.6045**}              & \textbf{0.8004**}       \\
\bottomrule
\end{tabular}%
}
\end{table*}

\subsubsection{Compared Methods}
The following state-of-the-art methods are our baselines for solving delayed feedback in CVR prediction:

\begin{itemize}[leftmargin=*]
\item \textbf{Vanilla}: a CVR model trained with the observed conversion label. This is the lower bound of possible improvements.
\item \textbf{Oracle}: a CVR model trained with the ground truth label instead of observed labels. This is the upper bound of possible improvements.
\item \textbf{DFM}\cite{201401}: a CVR model trained using delayed feedback loss.
\item \textbf{FSIW}\cite{202006}: a CVR model trained using FSIW loss and pre-trained auxiliary models.
\item \textbf{nnDF}\cite{202204}: a CVR model trained using the nnDF loss.
\item \textbf{ULC(ours)}: a CVR model alternately trained with the LC model and the LC loss.
\end{itemize}


The above methods can be applied to different CVR models. Due to the column anonymity of the Criteo dataset, we cannot use the models that rely on user modeling. We consider the following classical models that focus on feature interactions as backbones:

\begin{itemize}[leftmargin=*]
\item \textbf{MLP}: the classical fully connected neural networks.
\item \textbf{DeepFM} \cite{deepfm}: a model combining the factorization machines and deep neural networks.
\item \textbf{AutoInt} \cite{autoint}: a model using multi-head self-attention to learn the high-order feature interactions automatically.
\item \textbf{DCNV2} \cite{dcnv2}: a model using deep and cross networks to learn effective explicit and implicit feature crosses.
\end{itemize}

\subsubsection{Implementation Details}
The embedding size is 64 for all the methods. The MLP model in all the backbones is a simple three-layer model with hidden units [256,256,128] and Leaky ReLU activation. For AutoInt, the layer number is 3, the number of heads is 2, and the attention size is 64. For DCNV2, we use the stacked structure and one cross-layer. Adam \cite{adam} is used as the optimizer, and the learning rate is tuned in the range of [1e-3, 5e-4, 1e-4] with L2 regularization tuned in [0, 1e-7, 1e-6, 1e-5, 1e-4]. The batch size is set to 1024 for all the methods except nnDF. Given that the nnDF approach cannot apply to batch-wise training, we set the batch size to the size of the whole training set. For a fair comparison, we consistently use the MLP as the auxiliary model for all methods that rely on auxiliary models. The additional hyperparameters for the baselines are fine-tuned. Early stopping is applied to obtain the best parameters. We repeat each experiment 5 times with different random seeds and report the average results and make the statistical tests. \footnote{The codes can be found at https://github.com/yfwang2021/ULC. A mindspore version: https://gitee.com/mindspore/models/tree/master/research/recommend/ULC.}

\subsection{Overall Performance: RQ1}
From Table \ref{main}, we can observe that our proposed method ULC outperforms all the baselines and achieves state-of-the-art performance on all the backbones. There are some further observations. First, the oracle model works significantly better than the vanilla model, which validates that the delayed feedback problem indeed hurts the performance of CVR model. Second, FSIW performs significantly better than the DFM and Vanilla models, which is consistent with previous studies \cite{202006}. However, nnDF is significantly weaker than Vanilla method. It is because nnDF loss requires global dependence computation and can only be optimized using full training data when updating, which leads to a weaker performance than batch-wise optimization methods. Third, compared to the best baseline, our method shows a significant improvement of 0.76\% in the AUC metric, 1.02\% in the PRAUC metric, and 1.85\% in the LL metric on average across the four backbones, which demonstrates the effectiveness of our proposed method.

We further analyze the benefits gained by solving the delayed feedback problem. 
As shown in Table \ref{main}, our method narrows the gap between Vanilla and Oracle by 77.55\% in the AUC metric, 66.55\% in the PRAUC metric, and 83.13\% in the LL metric on average across the four backbones. Compared to the best baseline, our method shows a significant improvement of 60.3\% in the RI-AUC metric, 81.43\% in the RI-PRAUC metric, and 27.76\% in the RI-LL metric on average across the four backbones. This shows that our method can effectively alleviate the delayed feedback problem.

Fig.\ref{private_performance} shows the offline performance on the production dataset. For limited space, we only present the results with MLP as the backbone. It is clearly observed that our proposed method alleviates the delayed feedback problem and outperforms the two best baselines.

To guarantee the reproducibility of our work, and also due to the page limitation, we make the following further detailed analyses on the public-available dataset.

% Figure environment removed

\subsection{Analysis on Counterfactual Labeling: RQ2}
% Figure environment removed

In counterfactual labeling, only the samples converted between CD and AD are treated as positive samples (i.e., $w = 1$), which leads to some samples converted after AD being mislabeled as negative samples. A long time interval between CD and AD can improve label correctness of counterfactual labeling but reduce data freshness as only clicked data before CD are utilized. We further analyze the effect of different time intervals.

Experimental results using different time intervals are shown in Fig.\ref{time_interval}. First, increasing the time interval can effectively increase the recall of counterfactual labeling on positive samples. Second, the best $\tau$ on the Criteo dataset is around a week. Besides, smaller or larger $\tau$ will reduce the performance of CVR model. Smaller $\tau$ leads to more mislabeled samples in the training data of LC model, which in turn leads to lower performance of CVR model. Larger $\tau$ values, while reducing the mislabeled samples, will make the training data of the LC model older, which leads to its inability to correct well for false negative samples in the CVR training data, since these false negative samples are relatively fresh.

\subsection{Effectiveness of Alternative Training: RQ3}
In addition to embedding-based alternative training, there are also some other design schemes that enable the LC model to exploit the knowledge of CVR prediction model. We conduct experiments on these schemes.

\subsubsection{Joint Learning Strategy}
An obvious solution is to jointly train the LC model and the CVR model, which also enables the LC model to utilize the information learned by the CVR model. We use a simple shared-bottom structure \cite{mmoe} to validate the effectiveness of this scheme. The joint loss is a linear weighting of the LC model loss and the CVR model loss.

\subsubsection{Prediction-based Alternative Training Strategy}
In alternative training, in addition to using the learned representation of the CVR model, another easily thought of option is to leverage its prediction. Note that in Section 4.3, we mention that some potential positive samples that have a long delay and convert after AD may be mislabeled as negative samples during counterfactual labeling. To alleviate this problem, we consider using the prediction of the CVR model to mine these potentially positive samples. Intuitively, samples with high predicted CVR are more likely to be potentially positive samples. Thus, we design three simple strategies to process the training data of the LC model: (i) hard strategy, i.e., negative samples ($w = 0$) with predicted CVR above a predefined threshold are treated as positive samples ($w = 1$); (ii) soft strategy, i.e., using predicted CVR as the label for each negative sample; (iii) drop strategy, i.e., dropping negative samples with predicted CVR above a predefined threshold from the training data of the LC model, as the labels of these samples are not reliable.

\subsubsection{Comparisons on Different Strategies}
The results of the above strategies on Criteo with MLP as backbone are shown in Fig.\ref{alternatives}. Results on AUC are similar to PRAUC and hence omitted. We have the following observations:
(i) using a simple joint learning scheme cannot improve performance. Instead, there is a large loss of performance. The reason is that the inaccurate LC model at the early training stage will mislead the CVR model, which in turn affects the subsequent training. (ii) the three prediction-based strategies cannot improve the performance of the CVR model and even cause a slight degradation. The potential positive samples after AD have a higher delay than $\tau$, and the number of these samples is very small compared to the number of true negative samples (about 1:50 ratio). Using only predicted CVR cannot effectively discover these samples; instead, it introduces noise.

% Figure environment removed

% Figure environment removed

\subsubsection{Sensitivity of Alternative Training Rounds}
We further analyze the effect of alternative training rounds on the performance of ULC. $n$ controls the rounds of alternative training, and $n=0$ means no alternative training. We conduct experiments using different values of $n$ on the Criteo dataset with MLP as the backbone. As shown in Fig.\ref{alt_number}, alternative training once can significantly improve the performance of the CVR prediction model, which validates the effectiveness of alternative training. Besides, one round is enough, and more rounds have little impact, which is reasonable since the first round that changes the initialization of the LC model from random to the embeddings of CVR model brings more significant changes than the subsequent rounds. Note that even without alternative learning, the performance of ULC is still significantly better than the best baseline, which reflects the effectiveness of using the LC model for label correction. Results on AUC are similar to PRAUC and hence omitted.

\subsection{Analysis on Different Delay Time: RQ4}
The delay time is an important property of delayed feedback. We further analyze the performance of CVR model and LC model on samples with different delay time.

\subsubsection{CVR performance on different delay time}
For a fair comparison, we divide the positive samples in the test set into five groups in ascending order based on their delay time. Each group has the same number of positive samples. Then, each group is combined with all the negative samples in the test set to form test sets with different delay times. In this way, the number of positive and negative samples is the same for different test sets. Further, since the log loss is sensitive to the conversion rate, to ensure that the conversion rate in the test set is consistent with the original test set, we duplicate five copies of each positive sample.

Experiment results on the Criteo dataset with MLP as backbone are shown in Fig.\ref{delay_interval}. We have the following observations: 
(i) for the Oracle model without the delayed feedback problem, its performance decreases somewhat as the sample delay time increases, which indicates that samples with a long delay time are more likely to be hard samples. (ii) as the sample delay time increases, the Oracle model performs increasingly better than Vanilla, which is because positive samples are more likely to be false negative samples as the delay time increases. (iii) our method significantly outperforms the Vanilla model and the best baseline on samples with high delays (e.g., G3, G4, and G5), and our boost increases as the delay time increases, which reflects the effectiveness of our method.

% Figure environment removed

An interesting phenomenon is that the Vanilla model performs better than the Oracle model on samples with short delays (G1). It may be because samples with short delays have a higher percentage of observed positive samples than actual positive samples. Further analysis can be found in Appendix.

\subsubsection{LC performance on different delay time}
We further analyze the performance of the LC model on samples with different delay time. Similarly, we divide the false negative samples in the training data into five groups in ascending order based on their delay time. Each group has the same number of false positive samples. Then, as the goal of the LC model is to distinguish between false negative samples and true negative samples, each group is combined with all the true negative samples in the training data to form evaluation data with different delay time.

\begin{table}[h]
\centering
\caption{Label correction performance of ULC w.r.t. samples with different delay time. G5 is the group with the longest delay, and G1 has the shortest delay. $\uparrow$ means the higher the better, and  $\downarrow$ is for the lower the better.}
\label{lc_performance}
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{l|ccccc}
\toprule
Metrics & G1    & G2    & G3     & G4    & G5     \\ \midrule
AUC $\uparrow$    & 0.8698 & 0.8350 & 0.8117 & 0.7896 & 0.7811 \\
PRAUC $\uparrow$  & 0.1397 & 0.0757 & 0.0545 & 0.0434 & 0.0398 \\
LL  $\downarrow$    & 0.0549 & 0.0584 & 0.0604 & 0.0621 & 0.0628 \\ \bottomrule
\end{tabular}%
}
\end{table}

Experiment results on the Criteo dataset are shown in Table \ref{lc_performance}. We have the following observations: (i) AUC ranges from 0.7811 to 0.8698 at different delay time, which reflects that the LC model can effectively recognize false negative samples from all negative samples. (ii) the performance of the LC model decreases as the delay time of the false negative samples increases. There are two possible reasons for this. First, samples with longer delays are more likely to be hard samples. Second, in counterfactual labeling, false negative samples with longer delays are more likely to convert after AD and be recognized as true negative examples, which damages the performance of LC model.