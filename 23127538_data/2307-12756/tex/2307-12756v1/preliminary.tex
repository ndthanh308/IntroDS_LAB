\section{Preliminaries}
\subsection{Notations}
In online advertising platforms, the user behaviors for the display ads are logged to train the CVR prediction model. Suppose we collect training data $\mathcal{D}$ at timestamp $T$, i.e., we can obtain all the user behaviors and corresponding features before $T$. Let $\mathcal{D} = \{\left(x_i, v_i, e_i, cts_i, cvt_i \right), i=1, 2, ...\}$. The notation $i$ denotes the $i$-th sample. Each sample represents a click record of users. For the $i$-th sample $\left(x_i, v_i, e_i, cts_i, cvt_i\right)$, $x_i$ denotes the feature information of this sample. $cts_i$ denotes the click timestamp. $v_i$ is a binary value that denotes whether the clicked ad has a further conversion before the observed timestamp $T$. If $v_i = 1$, $cvt_i$ will record the corresponding conversion timestamp. Otherwise, $cvt_i$ is empty. $e_i$ denotes the time elapsed from $cts_i$ to $T$, i.e., $T - cts_i$. 

Let $c_i$ denote whether the $i$-th sample will finally lead to a conversion. Note that we cannot wait forever for the possible conversion to happen. In practice, a long time window $w_a$ is applied depending on the specific scenario, e.g., one month for Criteo \cite{201401}. Only conversions within the time window after clicks are considered valid. In other words, if $e_i \ge w_a$, then $v_i = c_i$. If $e_i < w_a$, $c_i$ is unknown. Thus, $c_i$ is not included in the training data $\mathcal{D}$. For test data, we can wait enough time to obtain $c_i$ for evaluation.

For easy reading, the notations are summarized in Table \ref{notations}.



\begin{table}[h]
\caption{Notations and Explanations of Variables}
\label{notations}
\begin{tabular}{l|l}
\hline
\textbf{Notation} & \textbf{Explanation}         \\
\hline
$T$     & the training data collection timestamp          \\
$x_i$     & the feature vector of $i$-th sample                        \\
$v_i$      & the observed conversion label of $i$-th sample                  \\
$e_i$     & the elapsed time of $i$-th sample          \\
$c_i$     & the true conversion label of $i$-th sample          \\
$cts_i$     & the click timestamp of $i$-th sample          \\
$cvt_i$     & the conversion timestamp of $i$-th sample          \\
\hline
\end{tabular}
\end{table}

\subsection{Task Formulation}
The conversion rate is defined as the probability of the final conversion for a clicked ad, i.e., $p_{CVR} = p(c_i=1|x_i)$. The CVR prediction task under delayed feedback is aimed to use the training data $\mathcal{D}$ collected at $T$ to predict $p_{CVR}$ for the clicked ads after $T$.

Note that training samples clicked before $T - w_a$ (i.e., $e_i \ge w_a$) can be fed directly into the model without any processing as their labels are correct. Since the core issue for delayed feedback is how to handle the fresh data with unknown labels, we omit these data in the rest of this paper for simplicity, which does not influence the correctness of our proof and method.

\subsection{Vanilla and Oracle Loss}
Next, we introduce the two basic loss functions in the delayed feedback problem. Note that CVR prediction is essentially a binary classification problem. Generally, the cross-entropy loss is adopted for training the CVR model. Let $f(\cdot;\theta)$ denote the CVR model with trainable parameters $\theta$. Suppose we can now foresee the future and obtain an ideal dataset $\mathcal{D}^*$, which contains $c_i$ for each sample. Then the cross-entropy loss can be written as:
\begin{equation}
    \mathcal{L}_{oracle}(\mathcal{D}^*) = \frac{1}{|\mathcal{D}^*|} \sum_{i = 1}^{|\mathcal{D}^*|} \left(c_i\log f(x_i;\theta) + (1 - c_i) \log (1 - f(x_i;\theta))\right) ,
\end{equation}

Equation (1) is called the oracle loss $\mathcal{L}_{oracle}$ as we suppose the final conversion label $c_i$ for each click record is available. 

However, in practice, we cannot obtain the oracle label $c_i$ for each sample at the data collection timestamp $T$. If we ignore the delayed feedback and replace the oracle label $c_i$ with the observed label $v_i$, we can get the vanilla loss $\mathcal{L}_{vanilla}$ for CVR model training:
\begin{equation}
    \mathcal{L}_{vanilla}(\mathcal{D}) = \frac{1}{|\mathcal{D}|} \sum_{i = 1}^{|\mathcal{D}|} \left(v_i\log f(x_i;\theta) + (1 - v_i) \log (1 - f(x_i;\theta))\right) .
\end{equation}

Note that some samples may convert after the data collection timestamp $T$. Obviously, the vanilla loss will incorrectly treat some positive samples as negative samples, which will damage the performance of CVR prediction model.
