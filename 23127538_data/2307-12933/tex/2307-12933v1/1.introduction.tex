\section{Introduction}
Model-based Reinforcement Learning (RL) has achieved great success on continuous control tasks \cite{gps,svg,steve,me-trpo,gobigger}. Model-based RL algorithms learn the true dynamics by fitting a model (usually a neural network) to the repeated interactions with the environment and use the model to generate imaginary data or perform online planning, which provides better sample efficiency than model-free RL \cite{dqn,ppo,td3,gem}. %For example, the recent works MBPO \cite{mbpo} and SLBO \cite{slbo} achieve comparable asymptotic performance to state-of-the-art model-free algorithms with fewer interactions.

A typical kind of model-based RL algorithm performs online planning to optimize the future action sequence over a long time horizon, i.e., model-based planning \cite{gps,pets,poplin,latco}. However, model-based planning has two weaknesses. First, it can hardly be applied in real-time, because it needs to solve an optimization problem on each time step and cannot remember the solution for reuse in the future similar states \cite{poplin}. Second, it only optimizes the maximum of the reward sum over the future states, rather than the trade-off between exploration and exploitation, which limits the ability to discover diverse states and better policies \cite{gps}. To reduce the time consumption during the application and incorporate the foresight of planning and the exploration ability of RL, some recent works distill the result of model-based planning into an RL policy \cite{gps,poplin}. Specifically, POPLIN uses the cross entropy method (CEM) \cite{cem} to optimize the action planning and uses behavior cloning to distill the planning result into the policy network. However, some essential theoretical properties of such kind of distillation are not well-understood, i.e., \textbf{(1)} whether the distilled policy achieves a higher value than the old policy; \textbf{(2)} whether the distilled policy has a guarantee of convergence to the optimal policy; \textbf{(3)} whether the distilled policy incorporates the foresight of planning and achieves a higher value than the original model-free policy update.

In this paper, we theoretically analyze the problems mentioned above. We choose Soft Actor-Critic (SAC) \cite{sac} as the RL component of our analysis due to its state-of-the-art performance in both model-free and model-based paradigms. Originally, the policy improvement of SAC is a one-step optimization. %This optimizes the reward plus entropy only on the current state $\boldsymbol{s}_t$, with the future states following the old policy.
We first define a planning problem by extending the one-step optimization of SAC under the model-based paradigm to a multi-step optimization problem of action planning. For each state $\boldsymbol{s}_t$, the optimal planning solution returns a policy $\pi_{\boldsymbol{s}_t}^H$ defined on a horizon of states $\boldsymbol{s}_{t:t+H-1}$ starting from $\boldsymbol{s}_t$.
Then, we propose a simple approach to distill the solution of the above multi-step optimization to the policy, which is an extended form of the policy improvement of SAC. This approach reserves the returned policy $\pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_t)$ for the first state $\boldsymbol{s}_t$ and discards the returned policy $\pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_{t+1:H-1})$ for the future states.

\begin{table*}[ht]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Algorithms  &Ensemble Dynamics  &Multiple Horizon  &Regularization  &Planning Theorem\\ % Multiple Policy
    \midrule
    SAC\cite{sac}    &\XSolidBrush    &\XSolidBrush    &\XSolidBrush   &\XSolidBrush\\
    MBPO\cite{mbpo}    &\CheckmarkBold    &\XSolidBrush    &\XSolidBrush   &\XSolidBrush\\
    POPLIN\cite{poplin}&\CheckmarkBold    &\CheckmarkBold  &\XSolidBrush   &\XSolidBrush\\
    M2AC\cite{m2ac}    &\CheckmarkBold    &\CheckmarkBold  &\XSolidBrush   &\XSolidBrush\\
    MPDP(our work)     &\CheckmarkBold    &\CheckmarkBold  &\CheckmarkBold     &\CheckmarkBold\\
    \bottomrule
  \end{tabular}
  \caption{Key features of different model-free and model-based algorithms.}
  \label{tab:component}
\end{table*}



Afterwards, we derive the theoretical result that the extended policy improvement is promising to achieve a higher return and lead the policy to converge to the optimal policy.
Thus the extension incorporates the farsight planning and has the potential to improve remarkably upon original one-step policy improvement.
Furthermore, to develop a practical algorithm, we discuss the solver of the defined multi-step optimization and design regularization to reduce the model error. Based on the above theory and discussion, we propose a new model-based RL algorithm, \textit{\textbf{M}odel-based \textbf{P}lanning \textbf{D}istilled to \textbf{P}olicy (MPDP)}. Compared to POPLIN, which uses behavior cloning for distillation and realizes the stochastic exploration via the CEM sampling, MPDP utilizes a distillation approach with theoretically guaranteed improvement and inherits the stochastic exploration of SAC, thus has a naturally strong ability to explore better policies.
For illustrating the effectiveness of MPDP, a thorough component comparison of relevant algorithms is given in \cref{tab:component}.

\textbf{Summary of Contributions:}
\textbf{(1)} We propose a model-based extended policy improvement method, which utilizes model-based planning to distill RL policy and model regularization to reduce the impact of model errors.
\textbf{(2)} We demonstrate that our method has a theoretical guarantee of monotonic improvement and convergence. And we theoretically analyze how the planning horizon affects policy improvement.
\textbf{(3)} Experimental results empirically show that MPDP achieves better sample efficiency and asymptotic performance than state-of-the-art model-free and model-based planning algorithms on the MuJoCo \cite{mujoco}. % as well as validate the design choices of MPDP.