\newpage
\appendix
\onecolumn
\centerline{\textbf{\LARGE{Appendix: Theoretically Guaranteed Policy Improvement}}}
\vspace{10pt}
\centerline{\textbf{\LARGE{Distilled from Model-Based Planning}}}
~\\ % blank row



\subsection*{A. Proof of Lemma and Theorem}

In this section, we provide proofs for bounds presented in the main paper.

\paragraph{\cref{lem:improve} (Policy Improvement).}
\label{apx:improve}
\textit{
Let $\pi_{\boldsymbol{s}_t}^H$ be the optimizer of the optimization objective of \cref{eq:objective}. 
When the new policy $\pi_{new}(\cdot|\boldsymbol{s}_t) = \pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_t)$,
$V^{\pi_{new}}(\boldsymbol{s}_t) \geq V^{\pi_{old}}(\boldsymbol{s}_t)$ for all $\boldsymbol{s}_t \in S $.
}

\begin{proof} 
Before the proof, we need to show that
\begin{align}
\label{eq:v_old_new}
V^{\pi_{old}}(\boldsymbol{s}_t) \leq J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H),
\end{align}
because $\pi_{\boldsymbol{s}_t}^H$ is the optimal solution and $V^{\pi_{old}}(\boldsymbol{s}_t)=J_{\boldsymbol{s}_t}^H(\pi_{old})\leq J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$.

Next, we will prove that
\begin{align}
\label{eq:v_new_new}
J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)\leq \mathbb{E}_{\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{s}_t}^H} \left[ r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \gamma \cdot J_{\boldsymbol{s}_{t+1}}^H(\pi_{\boldsymbol{s}_{t+1}}^H) \right],
\end{align}
which follows 

\begin{equation*}
\begin{aligned}
J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)
&=
\mathbb{E}_{\boldsymbol{a}_{t:t+H-1} \sim \pi_{\boldsymbol{s}_t}^H} \left[
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \cdots
    + 
    \gamma^{H-1}
    \cdot
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1})
    + 
    \gamma^H
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H}) 
% \phantom{=\;\;}
\right]\\
&=
\mathbb{E}_{\boldsymbol{a}_{t:t+H-1} \sim \pi_{\boldsymbol{s}_t}^H,
\boldsymbol{a}_{t+H} \sim \pi_{old}} \Big[  
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \cdots
    + 
    \gamma^{H-1}
    \cdot
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1}) \\
    &\phantom{=\;\;}+
    \gamma^{H}
    \cdot
    r^{\pi_{old}}(\boldsymbol{s}_{t+H},\boldsymbol{a}_{t+H}) 
    + 
    \gamma^{H+1}
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H+1}) 
\Big]\\
&=
\mathbb{E}_{
\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{s}_t}^H,
\boldsymbol{a}_{t+1:t+H-1} \sim \pi_{\boldsymbol{s}_t}^H,
\boldsymbol{a}_{t+H} \sim \pi_{old}
} \Big[
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + 
    \gamma
    \cdot
    \left[ \right.
    r^{\pi_{\boldsymbol{s}_{t}}^H}(\boldsymbol{s}_{t+1},\boldsymbol{a}_{t+1}) + \cdots
    + 
    \gamma^{H-2}
    \cdot
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1}) \\
    &\phantom{=\;\;}+
    \gamma^{H-1}
    \cdot
    r^{\pi_{old}}(\boldsymbol{s}_{t+H},\boldsymbol{a}_{t+H}) 
    + 
    \gamma^{H}
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H+1}) 
    % \phantom{=\;\;}
    \left.\right]
% \phantom{=\;\;}
\Big]\\
&\leq
\mathbb{E}_{
\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{s}_t}^H,
\boldsymbol{a}_{t+1:t+H} \sim \pi_{\boldsymbol{s}_{t+1}}^H
} \Big[
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + 
    \gamma
    \cdot
    \left[ \right.
    r^{\pi_{\boldsymbol{s}_{t+1}}^H}(\boldsymbol{s}_{t+1},\boldsymbol{a}_{t+1}) + \cdots 
    + 
    \gamma^{H-1}
    \cdot
    r^{\pi_{\boldsymbol{s}_{t+1}}^H}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1}) \\
    &\phantom{=\;\;}+ 
    \gamma^{H}
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H+1}) 
    % \phantom{=\;\;}
    \left.\right]
% \phantom{=\;\;}
\Big]\\
&=\mathbb{E}_{\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{s}_t}^H} \left[ r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \gamma \cdot J_{\boldsymbol{s}_{t+1}}^H(\pi_{\boldsymbol{s}_{t+1}}^H) \right].
\end{aligned}
\end{equation*}


We finish the proof by applying \cref{eq:v_old_new} and iteratively applying \cref{eq:v_new_new}:

\begin{align*}
 V^{\pi_{old}}(\boldsymbol{s}_t) &\leq J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)\\
 &\leq 
 \mathbb{E}_{\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{s}_t}^H} \left[ r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) 
 + \gamma \cdot J_{\boldsymbol{s}_{t+1}}^H(\pi_{\boldsymbol{s}_{t+1}}^H) \right]
 \\
 &\leq 
 \mathbb{E}_{\boldsymbol{a}_{t} \sim \pi_{\boldsymbol{s}_t}^H,
 \boldsymbol{a}_{t+1} \sim \pi_{\boldsymbol{s}_{t+1}}^H} \left[ r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) 
 + \gamma \cdot r^{\pi_{\boldsymbol{s}_{t+1}}^H}(\boldsymbol{s}_{t+1},\boldsymbol{a}_{t+1})
%  +\gamma^2 \cdot J_{\boldsymbol{s}_{t+2}}^H(\pi_{\boldsymbol{s}_{t+2}}^H) \phantom{=\;\;}
 \right]\\
 &\vdots\\
 &\leq 
 \mathbb{E}_{\boldsymbol{a}_{t} \sim \pi_{new}} \Big[ r^{\pi_{new}}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) +...
 \Big]\\
 &= V^{\pi_{new}}(\boldsymbol{s}_t).
\end{align*}
\end{proof}



\paragraph{\cref{thm:policy_conv} (Policy Convergence).}
\label{apx:policy_conv}
\textit{
Let $\pi_0$ be any initial policy. Assuming $|A|<\infty$, if the policy evaluation in \cref{eq:sac_evaluation} and the policy improvement with the objective in \cref{eq:objective} are alternatively carried out, $\pi_0$ converges to a policy $\pi_*$, with $V^{\pi_{*}}(\boldsymbol{s}_t) \geq V^{\pi}(\boldsymbol{s}_t)$ for any $\boldsymbol{s}_t \in S $.
}
\begin{proof}
First, let $\pi_{i}$ be the policy at the $i$-th iteration. Because $V^{\pi_{i}}(\boldsymbol{s}_t)$ monotonically increases with $i$ and is bounded, the sequence $\pi_{i}$ converges to some $\pi_{*}$.

We will next prove that, when the old policy $\pi_{old}=\pi_{*}$, $V^{\pi_{*}}(\boldsymbol{s}_t) = J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$. First, because $\pi_{\boldsymbol{s}_t}^H$ is the optimal solution of $J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$, as shown in the proof of \cref{lem:improve}, $V^{\pi_{*}}(\boldsymbol{s}_t)=V^{\pi_{old}}(\boldsymbol{s}_t) \leq J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$. Secondly, because $\pi_{*}$ is the fixed point, $\pi_{new}=\pi_{old}=\pi_{*}$ and $V^{\pi_{*}}(\boldsymbol{s}_t)=V^{\pi_{new}}(\boldsymbol{s}_t) \geq J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$, which completes the proof.

Finally, let $\pi$ be any other policy with $\pi \neq \pi_{*}$. We have $V^{\pi_{*}}(\boldsymbol{s}_t)=J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H) \geq J_{\boldsymbol{s}_t}^H(\pi)$ and expand the inequality as:

\begin{align*}
V^{\pi_{*}}(\boldsymbol{s}_t)
&\geq 
J_{\boldsymbol{s}_t}^H(\pi) 
=
\mathbb{E}_{
\boldsymbol{a}_{t:t+H-1} \sim \pi
}
 \Big[ 
    r^{\pi}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + 
    \cdots
    + 
    \gamma^{H-1}
    \cdot
    r^{\pi}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1}) 
    + 
    \gamma^{H}
    \cdot
    V^{\pi_{*}}(\boldsymbol{s}_{t+H}) 
\Big]\\
&\geq
\mathbb{E}_{
\boldsymbol{a}_{t:t+2H-1} \sim \pi
}
 \Big[ 
    r^{\pi}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + 
    \cdots
    + 
    \gamma^{2H-1}
    \cdot
    r^{\pi}(\boldsymbol{s}_{t+2H-1},\boldsymbol{a}_{t+2H-1}) 
    + 
    \gamma^{2H}
    \cdot
    V^{\pi_{*}}(\boldsymbol{s}_{t+2H}) 
\Big]\\
&\vdots\\
&\geq
\mathbb{E}_{
\boldsymbol{a}_{t:\infty} \sim \pi
}
 \Big[ 
    r^{\pi}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + 
    \cdots 
\Big]\\
&=V^{\pi}(\boldsymbol{s}_t).
\end{align*}

\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{\cref{lem:monotone} (Policy Monotone with Horizon).}
\label{apx:h_monotone}
\textit{
Let $\pi_{\boldsymbol{s}_t}^H$ and $\pi_{\boldsymbol{s}_t}^{H+1}$ be the optimizer of $J_{\boldsymbol{s}_t}^H(\pi)$ and $J_{\boldsymbol{s}_t}^{H+1}(\pi)$. Then $J_{\boldsymbol{s}_t}^{H+1}(\pi_{\boldsymbol{s}_t}^{H+1}) \geq J_{\boldsymbol{s}_t}^{H}(\pi_{\boldsymbol{s}_t}^{H})$ for all $H\geq 1$ and $\boldsymbol{s}_t \in S$.
}
\begin{proof}
\begin{align*}
J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)
&=
\mathbb{E}_{\boldsymbol{a}_{t:t+H-1} \sim \pi_{\boldsymbol{s}_t}^H} \left[
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \cdots
    + 
    \gamma^{H-1}
    \cdot
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1}) 
    + 
    \gamma^H
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H}) 
\right]\\
&=
\mathbb{E}_{\boldsymbol{a}_{t:t+H-1} \sim \pi_{\boldsymbol{s}_t}^H,
\boldsymbol{a}_{t+H} \sim \pi_{old}} \Big[
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \cdots
    + 
    \gamma^{H-1}
    \cdot
    r^{\pi_{\boldsymbol{s}_t}^H}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1}) \\
    &\phantom{=\;\;}+ 
    \gamma^{H}
    \cdot
    r^{\pi_{old}}(\boldsymbol{s}_{t+H},\boldsymbol{a}_{t+H}) 
    + 
    \gamma^{H+1}
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H+1}) 
\Big]\\
&\leq
\mathbb{E}_{\boldsymbol{a}_{t:t+H} \sim \pi_{\boldsymbol{s}_t}^{H+1}} \left[
    r^{\pi_{\boldsymbol{s}_t}^{H+1}}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \cdots
    + 
    \gamma^{H}
    \cdot
    r^{\pi_{\boldsymbol{s}_t}^{H+1}}(\boldsymbol{s}_{t+H},\boldsymbol{a}_{t+H}) 
    + 
    \gamma^{H+1}
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H+1}) 
\right]\\
&=
J_{\boldsymbol{s}_t}^{H+1}(\pi_{\boldsymbol{s}_t}^{H+1}).
\end{align*}
\end{proof}




\paragraph{\cref{thm:optim} (Policy Convergence with Horizon).}
\label{apx:optim}
\textit{
Let $\pi_{\boldsymbol{s}_t}^H$ be the optimal solution of $J_{\boldsymbol{s}_t}^H(\pi)$, and $\pi_{new}(\cdot|\boldsymbol{s}_t)=\pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_t)$. $\pi_{*}$ denotes the optimal policy. As $H$ increases, $V^{\pi_{new}}$ and $J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$ converge to $V^{\pi_{*}}$ for all $\boldsymbol{s}_t \in S$. Specifically, $J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H) \geq V^{\pi_{*}}(\boldsymbol{s}_{t}) 
-    
\frac{\gamma^H
\cdot r^{max}}{1-\gamma}$ with $r^{max}$ the maximum of $r^{\pi}(\boldsymbol{s},\boldsymbol{a})$ over all $\pi$ and $(\boldsymbol{s},\boldsymbol{a})\in |\mathcal{S}|\times|\mathcal{A}|$.
}
\begin{proof}
We have show that $V^{\pi_{new}}\geq J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$ in the proof of \cref{lem:improve}, hence we only need to prove that $J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$ converges to $V^{\pi_{*}}$. We start the proof with the fact that $J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H) \geq J_{\boldsymbol{s}_t}^H(\pi_{*})$ and expand this inequality as:
\begin{align*}
    J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H) &\geq
\mathbb{E}_{\boldsymbol{a}_{t:t+H-1} \sim \pi_{*}} \Big[
    r^{\pi_{*}}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \cdots 
    + 
    \gamma^{H-1}
    \cdot
    r^{\pi_{*}}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1}) 
    + 
    \gamma^H
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H}) 
\Big]\\
&=
\mathbb{E}_{\boldsymbol{a}_{t:t+H-1} \sim \pi_{*}} \Big[
    r^{\pi_{*}}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \cdots 
    + 
    \gamma^{H-1}
    \cdot
    r^{\pi_{*}}(\boldsymbol{s}_{t+H-1},\boldsymbol{a}_{t+H-1}) \\
    &\phantom{=\;\;}+ 
    \gamma^H
    \cdot
    V^{\pi_{*}}(\boldsymbol{s}_{t+H})
    +
    \gamma^H
    \cdot
    V^{\pi_{old}}(\boldsymbol{s}_{t+H})
    -
    \gamma^H
    \cdot
    V^{\pi_{*}}(\boldsymbol{s}_{t+H}) 
\Big]\\
&=
V^{\pi_{*}}(\boldsymbol{s}_{t}) 
-    
\gamma^H
\cdot 
\mathbb{E}_{\boldsymbol{a}_{t:t+H-1} \sim \pi_{*}} \Big[ 
    V^{\pi_{*}}(\boldsymbol{s}_{t+H}) -
    V^{\pi_{old}}(\boldsymbol{s}_{t+H})
\Big]\\
&\geq
V^{\pi_{*}}(\boldsymbol{s}_{t}) 
-    
\gamma^H
\cdot 
\Vert 
V^{\pi_{*}} -
    V^{\pi_{old}}
\Vert_{\infty},\\
&\geq
V^{\pi_{*}}(\boldsymbol{s}_{t}) 
-    
\frac{\gamma^H
\cdot r^{max}}{1-\gamma}.
\end{align*}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{B. Implementation}
\subsection*{B.1 Experiment Setup}

We implement MPDP based on the open-source platform DI-engine \footnote{https://github.com/opendilab/DI-engine}.
And \cref{tab:hyparam} provides the key hyperparameters in MPDP.
We follow the original implementations for all baseline algorithms with regard to the reward sum over 1000 steps.
We evaluate MPDP along with the baseline algorithms on six continuous control tasks provided in MuJoCo-v2 \cite{mujoco}. 


\begin{table*}[ht]
  \centering
  \begin{tabular}{cc}
    \toprule
    Hyperparameter  & Value \\ 
    \midrule
    Ensemble size    & 7 \\
    Replay buffer size  & $10^6$  \\
    Batch size    & 256 \\
    Learning rate & $3 \cdot 10^{-4}$ \\
    Threshold $u_T$    & -5 \\
    Entropy coefficient $\alpha$    & 0.2 \\
    Regularization coefficient $\beta$    & 0.5 \\
    Maximum horizon $H_{max}$  & 25 \\
    Policy updates per environment step  & 20  \\
    Environment steps per model training   & 250 \\
    \bottomrule
  \end{tabular}
  \caption{Hyperparameter setup for MPDP.}
  \label{tab:hyparam}
\end{table*}



\subsection*{B.2 Experiment Environments}

We visualize the six continuous control tasks in MuJoCo-v2 including  InvertedPendulum, Hopper, HalfCheetah, Ant, Walker2d, and Humanoid, as shown in \cref{supp:envs}. 
The first task InvertedPendulum is designed to control the pole to keep balance, and the other five tasks aim to keep the agent moving forward without falling.


% Figure environment removed
