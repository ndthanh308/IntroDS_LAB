\section{Preliminaries}
% In this section, we will first define the essential notations, then summarize the framework of Soft Actor-Critic and at last introduce a common model-based paradigm.

\subsection{Notation}
We consider continuous control tasks which can be formulated as infinite-horizon Markov Decision Processes (MDP) $(\mathcal{S},\mathcal{A},p,r,\gamma)$, where the state space $\mathcal{S}$ and the action space $\mathcal{A}$ are both continuous. State transition $p:\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}^+$ and $r:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ are the dynamics of the environment and the reward function, respectively. $\gamma$ is the discount factor. Additionally, we define $\pi(\boldsymbol{a}|\boldsymbol{s}):\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^+$ as the RL policy on the state $\boldsymbol{s}$, with $Q(\boldsymbol{s},\boldsymbol{a})$ and $V(\boldsymbol{s})$ as the corresponding value functions.

\subsection{Soft Actor-Critic}
Soft Actor-Critic(SAC) \cite{sac} develops a maximum entropy objective to incentivize the policy to explore more widely, which is the discounted sum of both the reward and the entropy, formalized as:
\begin{gather}
\label{eq:sac_obj}
    J_{\boldsymbol{s_t}}(\pi) = 
    \mathbb{E}_{\boldsymbol{a_t} \sim \pi}
    \left[
        \sum_{t=0}^{\infty}
            \gamma^t 
            \cdot     [r(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) 
    - \alpha \cdot log\pi(\boldsymbol{a}_{t}|\boldsymbol{s}_{t})]
    \right].
\end{gather}

The coefficient $\alpha$ balances the importance of
the reward and entropy, and hence controls the exploration of the policy. we omit $\alpha$ in the rest of this paper for simplicity.
The policy evaluation of SAC is based on the maximum entropy objective, i.e., the value function $Q$ and $V$ also contain the discounted sum of the entropy over the subsequent states. The Bellman backup operator $\mathcal{T}^{\pi}$ of SAC is given by:
\begin{gather}
\label{eq:sac_evaluation}
\mathcal{T}^{\pi}Q(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) 
= r(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) 
+ \gamma \cdot V(\boldsymbol{s}_{t+1}), \\
V(\boldsymbol{s}_{t}) = 
\mathbb{E}_{\boldsymbol{a}_{t} \sim \pi}
    \left[
    Q(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) -
    log\pi(\boldsymbol{a}_{t}|\boldsymbol{s}_{t})
    \right].
\end{gather}


In the policy improvement step of SAC, the new policy optimizes the $V(\boldsymbol{s}_{t})$ on each state $\boldsymbol{s}_{t}$:
\begin{gather}
\pi_{new}(\cdot|\boldsymbol{s}_{t}) = \arg \max_{\pi} \mathbb{E}_{\boldsymbol{a}_{t} \sim \pi}
    \left[
    Q^{\pi_{old}}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) -
    log\pi(\boldsymbol{a}_{t}|\boldsymbol{s}_{t})
    \right].
\end{gather}
We reformulate the objective as:
\begin{gather}
\begin{aligned}
\pi_{new}(\cdot|\boldsymbol{s}_{t}) = \arg \max_{\pi} \mathbb{E}_{\boldsymbol{a}_{t} \sim \pi}
    &\left[   \right.
    r(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) 
 -log\pi(\boldsymbol{a}_{t}|\boldsymbol{s}_{t})
  \\
 &+ \gamma \cdot
V^{\pi_{old}}(\boldsymbol{s}_{t+1})
   \left. \right].
\label{eq:sac_improvement}
\end{aligned}
\end{gather}

This objective leads the new policy to optimize the modified reward $r(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) -log\pi(\boldsymbol{a}_{t}|\boldsymbol{s}_{t})$ only on the current state $\boldsymbol{s}_{t}$ w.r.t. $\boldsymbol{a}_{t}$, with the subsequent states following the old policy $\pi_{old}$, which is myopic under the model-based paradigm, because the dynamics of the environment can be approximated by the environment model, which enables the joint optimization of actions over multiple future time-steps.

\subsection{Environment Model}
A common setting used in model-based RL is model ensemble \cite{pets,me-trpo,mbpo,slbo,m2ac}, where an ensemble of models learn the distribution of the transitions from historical interactions. Typically, the models are parametric function approximators $p_{1:K}(\cdot|\boldsymbol{s},\boldsymbol{a})$ and are trained via maximum likelihood: $\sum_{i=1}^{K}\mathbb{E}\left[log( p_i(\boldsymbol{s}_{t+1}|\boldsymbol{s}_t,\boldsymbol{a}_t)) \right]$.