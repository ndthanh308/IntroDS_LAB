
\section{Experiment}
Our experiment goal is to investigate the following questions: \textbf{(1)} How the sample efficiency and the asymptotic performance of MPDP compared to state-of-the-art(SOTA) model-based planning algorithms? \textbf{(2)} How the proposed extended policy improvement and the design choices affect the performance?

% % Figure environment removed

\subsection{Comparison}

\paragraph{Baseline.} 
In this section, we focus on understanding how well MPDP performs compared to SOTA model-based planning algorithms. We choose PETS \cite{pets}, which uses CEM to perform model-based action planning; and POPLIN \cite{poplin}, which extends CEM from action space to the domain of policy network parameters and distills the planning results into the policy with behavior cloning. 
Additionally, we compare our proposed approach to the SOTA model-free methods and model-based methods without planning. For model-free algorithms, we compare to SAC \cite{sac} and DDPG \cite{ddpg}, which are the two competitive policy learning algorithms. %In the case of SAC, the effect of our farsighted policy improvement is also measured.
For model-based RL, we choose MBPO \cite{mbpo} and M2AC \cite{m2ac}, which are the previous SOTA model-based baselines. 
% For model-based RL, we choose MBPO \cite{mbpo}, a model-based algorithm which justifies the model usage by truncated model rollouts; and M2AC \cite{m2ac}, which uses the inconsistency among an ensemble of models to estimate the model error and discards the imaginary transitions with high estimated errors. 
MPDP, PETS, POPLIN, MBPO and M2AC share the same model architecture. The implementation details of our method are in Appendix B.

\paragraph{Results.}  
The performance curves on all six environments of MuJoCo are shown in \cref{fig:result}. It demonstrates that MPDP significantly outperforms the SOTA model-based planning algorithms (PETS and POPLIN) on both sample efficiency and asymptotic performance. For example, on the highly dimensional Ant task, MPDPâ€™s performance at 140k steps is equivalent to that of POPLIN at 200k steps. 



Further, the results in \cref{fig:result} reveal that MPDP achieves much higher convergence speed than the SOTA of model-free algorithms (SAC and DDPG) on the all tasks and obtains comparable asymptotic performance, which also validates that incorporating our extended policy improvement benefits a lot. We also observe that MPDP achieves better performance than the SOTA model-based algorithms, MBPO and M2AC on some complex tasks like Humanoid, and is comparable to them on the rest of tasks.

\subsection{Ablation Study}
In this section, we conduct a series of ablation studies on MPDP to investigate the effect of the designed adaptive horizon and regularization on the model error. We choose the Hopper task in the MuJoCo for the experiments.

\paragraph{Horizon.}
To verify that our method can really adapt the horizon to the model error, i.e. the adaptive horizon does not fall into a very small range and increases as the model generalizes better, we profile the average horizon of MPDP during the training on Hopper with different error threshold $u_T$ in \cref{fig:horizon_length}. As shown in the curves, the horizon grows from 2 to 12 as the training proceeds, where the model becomes more accurate in \cref{fig:beta_error}. It also proves that MPDP does not degenerate to SAC.


% Figure environment removed

% Figure environment removed



\paragraph{Model Error.}
We validate that the regularization based on OvR does push the policy to explore areas with low dynamic model error. We vary $\beta$ at \cref{eq:beta_obj}  with \{0.2, 0.5, 0.7\} and evaluate the model error as shown in \cref{fig:beta_error}. 
The result demonstrates that the model error decreases with $\beta$, which verifies the effectiveness of the designed regularization.
We also plot the final performance of corresponding $\beta$ in \cref{fig:beta_perform}. 
However, we find that a too large regularization harms the asymptotic performance due to the excessive restriction on the exploration area of the policy. \cref{fig:beta_perform} also implies that a larger regularization brings more stable results.

% Figure environment removed


