\section{Distillation from Planning into Policy}
In this section, we propose an approach to distilling the solution of model-based planning into the policy, which is a multi-step extension of the original policy improvement of SAC. We will first derive this extension. Then, we will verify its theoretical properties and advantages. Finally, based on our theory, we will develop a practical reinforcement learning algorithm by discussing the essential design choices in the next section.

\subsection{Multi-step Optimization}
\label{multi_optim}
The policy improvement of SAC optimizes the trade-off between the expected cumulative reward and entropy only with regard to the action distribution on the current time-step $\boldsymbol{s}_t$, with the future states $\boldsymbol{s}_{t+1:\infty}$ following the old policy $\pi_{old}$, formalized in \cref{eq:sac_improvement}. Under the model-based paradigm, we assume that the true dynamics of the environment is accessible. Because we can always obtain a dynamic model with a lower generalization error \cite{me-trpo,mbpo}, as the training proceeds. This assumption enables us to quantify the expected future state and the according reward and entropy with regard to the future action sequence over a given horizon $H$, and derive a more foresighted optimization form than the original SAC. Specifically, we extend the one-step optimization in \cref{eq:sac_improvement} to a multi-step optimization problem of the action planning over $H$ steps based on the environment model, with the objective $J_{\boldsymbol{s}_t}^H(\pi)$ on the state $\boldsymbol{s}_t$ defined as: 

\begin{gather}
    J_{\boldsymbol{s}_t}^H(\pi) 
    =
    \mathbb{E}_{\boldsymbol{a_t} \sim \pi}
    \left[
        \sum_{i=0}^{H-1}
            \gamma^i 
            \cdot r^{\pi}(\boldsymbol{s}_{t+i},\boldsymbol{a}_{t+i})
            + V^{\pi_{old}}(\boldsymbol{s}_{t+H})
    \right], \label{eq:objective}\\
    r^{\pi}(\boldsymbol{s}_{t+i},\boldsymbol{a}_{t+i}) 
    = 
    r(\boldsymbol{s}_{t+i},\boldsymbol{a}_{t+i}) 
    - log\pi(\boldsymbol{a}_{t+i}|\boldsymbol{s}_{t+i}).
\end{gather}
Here $H$ is the planning horizon, $\pi$ is the policy only defined on $\boldsymbol{s}_t$ and its subsequent $H-1$ steps. $r^{\pi}(\boldsymbol{s},\boldsymbol{a})$ is the sum of the reward and the logarithmic likelihood, which inherits the maximum entropy objective of SAC. Specifically, when $H=1$, this objective degenerates to that of SAC. 

\subsection{Extended Policy Improvement}
The improvement property of distillation from planning into an RL policy has not been well discussed. Another work\cite{maac} proves that the solution of action planning achieves a higher value, but it does not develop a  distillation approach to obtain a policy $\pi_{new}$ with provably higher value $V^{\pi_{new}}(\boldsymbol{s}_{t})$, i.e., a policy with higher cumulative rewards.
In this section, we propose a distillation approach, also an extended form of the original policy improvement step in SAC, based on the multi-step optimization.
% in \cref{{multi_optim}}. 
We will show that the proposed extended policy improvement provably achieves a new policy with a higher value than the old policy with respect to the maximum entropy target \cref{eq:sac_obj} defined in SAC.

\paragraph{Distillation.} We use $\pi_{\boldsymbol{s}_t}^H$ to denote the optimal solution of $J_{\boldsymbol{s}_t}^H(\pi)$. After the policy improvement, we define the new policy $\pi_{new}(\cdot|\boldsymbol{s}_t)$ as $\pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_t)$, i.e., although $\pi_{\boldsymbol{s}_t}^H$ is define on $H$ steps of states $\boldsymbol{s}_{t:t+H-1}$, we only adopt the policy $\pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_t)$ of the current state $\boldsymbol{s}_t$ and discard the policy $\pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_{t+1:t+H-1})$ over the following states. 

\paragraph{Improvement.} We present the improvement property of this distillation in \cref{lem:improve}. Please note that \cref{lem:improve} is a more general multi-step extension of  the Lemma 2\footnote{https://arxiv.org/pdf/1801.01290.pdf}  in SAC \cite{sac}. Our result reveals that, if we optimize the policy jointly over a horizon starting from each state $\boldsymbol{s}_t$ and only adopt the optimal policy on the first state $\boldsymbol{s}_t$, the resulting new policy has a monotonic improvement. Specifically, when $H=1$, \cref{lem:improve} degenerates to the Lemma 2 in SAC (see Appendix A. for more details).
\begin{lemma} 
\label{lem:improve}
Let $\pi_{\boldsymbol{s}_t}^H$ be the optimizer of the optimization objective of \cref{eq:objective}. 
When the new policy $\pi_{new}(\cdot|\boldsymbol{s}_t) = \pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_t)$,
$V^{\pi_{new}}(\boldsymbol{s}_t) \geq V^{\pi_{old}}(\boldsymbol{s}_t)$ for all $\boldsymbol{s}_t \in S $.
\end{lemma}
\iffalse
\begin{proof}
See the \cref{apx:improve}
\end{proof}
\fi

\subsection{Policy Convergence}
The monotonic increasing property of our extended form is crucial, because it facilitates the derivation of the proposition that this form will provably converge to the optimal maximum entropy policy defined in SAC. We present the result in \cref{thm:policy_conv}.
\begin{theorem}
\label{thm:policy_conv}
Let $\pi_0$ be any initial policy. Assuming $|\mathcal{A}|<\infty$, if the policy evaluation in \cref{eq:sac_evaluation} and the policy improvement with the objective in \cref{eq:objective} are alternatively carried out, $\pi_0$ converges to a policy $\pi_*$, with $V^{\pi_{*}}(\boldsymbol{s}_t) \geq V^{\pi}(\boldsymbol{s}_t)$ for any $\boldsymbol{s}_t \in S $.
\end{theorem}
\iffalse
\begin{proof}
See the \cref{apx:policy_conv}.
\end{proof}
\fi

\subsection{The Effect of Planning Horizon}
We have shown that the proposed extension of policy improvement, based on optimization of the action planning over multiple time steps, can always lead to a higher value via the developed distillation, which is guaranteed to converge to the optimal policy. In this section, we will discuss another problem: does the extended form of policy improvement incorporate the farsight of planning and benefit SAC? Or more generally, does a larger planning horizon $H$ always result in a better value? 

Unfortunately, there exist some special cases where a larger $H$ leads to a smaller value due to a bad initial policy $\pi_{old}$. 
Although a larger $H$ is not equivalent to a higher value, we can still show the potential advantage of increasing $H$ in two aspects. 

\textbf{(1)} A larger horizon results in a higher optimization objective defined in \cref{eq:objective}, as formalized in \cref{lem:monotone}.

\begin{lemma}
\label{lem:monotone}
Let $\pi_{\boldsymbol{s}_t}^H$ and $\pi_{\boldsymbol{s}_t}^{H+1}$ be the optimal solution of $J_{\boldsymbol{s}_t}^H(\pi)$ and $J_{\boldsymbol{s}_t}^{H+1}(\pi)$. Then $J_{\boldsymbol{s}_t}^{H+1}(\pi_{\boldsymbol{s}_t}^{H+1}) \geq J_{\boldsymbol{s}_t}^{H}(\pi_{\boldsymbol{s}_t}^{H})$ for all $H\geq 1$ and $\boldsymbol{s}_t \in S$.
\end{lemma}
\iffalse
\begin{proof}
See the \cref{apx:h_monotone}.
\end{proof}
\fi

% \cref{lem:monotone} proves that a larger $H$ can always achieve a higher objective value when the subsequent states after the planning horizon follow $\pi_{old}$.

\textbf{(2)} Although the resulting policy does not have a value monotonically increasing with $H$, we can prove that $\pi_{new}$ converges to the optimal policy as $H$ increases, which is formalized in \cref{thm:optim}.

\begin{theorem}
\label{thm:optim}
Let $\pi_{\boldsymbol{s}_t}^H$ be the optimal solution of $J_{\boldsymbol{s}_t}^H(\pi)$, and $\pi_{new}(\cdot|\boldsymbol{s}_t)=\pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_t)$. $\pi_{*}$ denotes the optimal policy. As $H$ increases, $V^{\pi_{new}}$ and $J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H)$ converge to $V^{\pi_{*}}$ for all $\boldsymbol{s}_t \in S$. Specifically, $V^{\pi_{new}} \geq J_{\boldsymbol{s}_t}^H(\pi_{\boldsymbol{s}_t}^H) \geq V^{\pi_{*}}(\boldsymbol{s}_{t}) 
-    
\frac{\gamma^H
\cdot r^{max}}{1-\gamma}$ with $r^{max}$ the maximum of $r^{\pi}(\boldsymbol{s},\boldsymbol{a})$ over all $\pi$ and $(\boldsymbol{s},\boldsymbol{a})\in |\mathcal{S}|\times|\mathcal{A}|$.
\end{theorem}
\iffalse
\begin{proof}
See the \cref{apx:optim}.
\end{proof}
\fi

Starting from \cref{thm:optim}, it can be naturally derived that, we can always find a larger $\hat{H}$ than $H$, which results in a policy with a larger value. We formalize this conclusion as \cref{thm:exist}.

\begin{theorem}
\label{thm:exist}
Let $\pi_{\boldsymbol{s}_t}^H$ be the optimal solution of $J_{\boldsymbol{s}_t}^H(\pi)$, and $\pi_{new}^H(\cdot|\boldsymbol{s}_t)=\pi_{\boldsymbol{s}_t}^H(\cdot|\boldsymbol{s}_t)$. There exists another $\hat{H} > H$, with $V^{\pi_{new}^{\hat{H}}}\geq V^{\pi_{new}^{H}}$ for all $\boldsymbol{s}_t \in S$, assuming $|\mathcal{S}|<\infty$.
\end{theorem}

\begin{proof}
According to \cref{thm:optim}, we can always find a $\hat{H}$ with $V^{\pi_{*}}-V^{\pi_{new}^{\hat{H}}} \leq V^{\pi_{*}}-V^{\pi_{new}^{H}}$ on all states, which means $V^{\pi_{new}^{\hat{H}}}\geq V^{\pi_{new}^{H}}$.
\end{proof}