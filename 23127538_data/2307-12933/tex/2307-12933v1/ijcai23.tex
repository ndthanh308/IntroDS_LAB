%%%% ijcai23.tex

\typeout{IJCAI--23 Instructions for Authors}

% These are the instructions for authors for IJCAI-23.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai23.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai23}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode,algorithmicx}  % for our paper algo format
\usepackage[switch]{lineno}


% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}  % for lemma

% table
\usepackage{bbding}

% figure
\usepackage{subfigure}

% cleveref
\usepackage[capitalize,noabbrev]{cleveref}

% equation
\usepackage{amssymb}
\usepackage{mathtools}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.
% Please **do not** include Title and Author information
\pdfinfo{
/TemplateVersion (IJCAI.2023.0)
}

\title{Theoretically Guaranteed Policy Improvement\\ Distilled from Model-Based Planning}


% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\author{
Chuming Li$^{1,2}$\footnote{Equal contribution.} \and
Ruonan Jia$^{1,3*}$\and
Jie Liu$^{1}$\and
Yinmin Zhang$^{1,2}$\and \\
Yazhe Niu$^1$\and
Yaodong Yang$^4$ \and 
Yu Liu$^1$ \and
Wanli Ouyang$^1$
\affiliations
$^1$Shanghai Artificial Intelligence Laboratory \quad 
$^2$University of Sydney\\
$^3$Tsinghua University \quad
$^4$Peking University
\emails
\{lichuming.lcm, jiaruonan97, liuyuisanai\}@gmail.com, \{liujie, niuyazhe\}@pjlab.org.cn,\\
\{yinmin.zhang, wanli.ouyang\}@sydney.edu.au, yaodong.yang@pku.edu.cn
}



\begin{document}

\maketitle

\begin{abstract}
Model-based reinforcement learning (RL) has demonstrated remarkable successes on a range of continuous control tasks due to its high sample efficiency. 
To save the computation cost of conducting planning online, recent practices tend to distill optimized action sequences into an RL policy during the training phase.
Although the distillation can incorporate both the foresight of planning and the exploration ability of RL policies, the theoretical understanding of these methods is yet unclear.
In this paper, we extend the policy improvement step of Soft Actor-Critic (SAC) by developing an approach to distill from model-based planning to the policy. We then demonstrate that such an approach of policy improvement has a theoretical guarantee of monotonic improvement and convergence to the maximum value defined in SAC. 
We discuss effective design choices and implement our theory as a practical algorithm---\textit{\textbf{M}odel-based \textbf{P}lanning \textbf{D}istilled to \textbf{P}olicy (MPDP)}---that updates the policy jointly over multiple future time steps.
Extensive experiments show that MPDP achieves better sample efficiency and asymptotic performance than both model-free and model-based planning algorithms on six continuous control benchmark tasks in MuJoCo.
\end{abstract}

\input{1.introduction}
\input{2.related_work}
\input{3.preliminaries}
\input{4.theory}
\input{5.method}
\input{6.experiment}

\section{Conclusion}
In this paper, we investigate the theoretical guarantee of distillation from model-based planning into an RL policy. We first extend the one-step optimization of SAC to a multi-step optimization formulation. Then, we develop a distillation approach based on the solution of the proposed multi-step optimization. It provably has the guarantee of monotonic improvement and convergence to the optimal policy. We further theoretically verify its potential to incorporate the foresight planning. Based on the theory, we discuss several design choices to instantiate a practical algorithm MPDP. 
Experimental results confirm that MPDP outperforms the state-of-the-art model-based planning algorithms in both sample efficiency and asymptotic performance on a range of continuous control tasks in MuJoCo.

One limitation is that the generalization ability of the horizon-adapted policy may not be strong enough because we fit the horizon to the model error for fast convergence speed. Thus, our method is efficient for task-specific but not exploration-oriented problems. We leave this to future work.

\newpage
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai23}

\input{supp}

\end{document}
