\section{Related Work}

\paragraph{Model-based Reinforcement Learning.}
Model-based reinforcement learning methods show a promising prospect for real-world decision-making problems due to their data efficiency. However, learning an accurate model is challenging, especially in complex environments. Many papers \cite{pets,me-trpo,mbpo,pdml} commonly use ensemble probabilistic networks to construct uncertainty-aware environment models.


The previously proposed model-based methods \cite{mve,steve,emc-ac,vagram} allow the model rollout to a fixed depth, and value estimations are split into a model-based reward and a model-free value. To guarantee the monotonic improvement, the recent work %stochastic lower bounds optimization
\cite{slbo} builds a lower bound of the expected reward and then maximizes the lower bound jointly over the policy and the model. Furthermore, model-based policy optimization \cite{mbpo} utilizes short model-generated rollouts to do policy improvement and evaluation, and also provides a guarantee of monotonic improvement.


Current model-based RL mainly focuses on better model usage. For example, M2AC \cite{m2ac} implements a masking mechanism based on the modelâ€™s uncertainty to decide whether its prediction should be used or not. Another line of works \cite{gps,svg} aims to exploit the differentiability of the learned model in model-based RL. Model-augmented actor-critic \cite{maac} uses the path-wise derivative of the learned model and policy across future time steps.
Our work estimates value function by utilizing the model error as regularization.


\paragraph{Model-based Planning.}
Many recent papers on deep model-based RL \cite{pets,VisualForesight,mpc} optimize the future action trajectories over a given horizon starting from the current state, which is usually referred as model-based planning.
Model predictive control \cite{mpc} is a common control approach for model-based planning. It frequently solves the action planning over a limited horizon and conducts the first action on the environment.
Random Shooting optimizes the action sequence among the randomly generated candidates to maximize the expected reward under the learned dynamic model, and PETS \cite{pets} uses the cross entropy method \cite{cem} to improve the efficiency of the random search. However, shooting methods usually rely on the local search in the action space and are not effective on high-dimension environments. 
To solve this problem,  the latest work \cite{latco} utilizes the collocation-based planning in a learned latent space.
% MSAC \cite{msac} uses long-term planning with a simple overshooting. 
In contrast, we extend the policy improvement step of SAC to distill from model-based planning to the policy, which reduces the cost in the deployment phase.
% A further step is to build powerful planning methods, such as Monte Carlo Tree Search  \cite{mcts1,mcts2}, which is successfully implemented in AlphaZero \cite{alphazero} and Muzero \cite{muzero}.

In addition, some recent works distill the result from model-based policy planning into an RL policy. POPLIN \cite{poplin} formulates action planning at each time step as an optimization problem w.r.t. the parameters of the policy network, and uses behavior cloning to distill the resulted action into the policy network. GPS \cite{gps2,gps} uses KL divergence to minimize the distance between the policy and the planning result. However, the essential theoretical properties of such distillation are not well-understood.
Instead, we propose an algorithm to improve the policy with the solution of model-based planning over multiple time steps, and give the theoretical guarantee of its improvement and convergence.

\paragraph{Actor-Critic Methods.}
Actor-critic algorithms are typically derived from policy iteration, which alternates between policy evaluation and policy improvement.
Deep deterministic policy gradient \cite{ddpg} is a common model-free actor-critic method, however, the critic is usually overestimated to predict Q value, which leads to the worse policy.
Moreover, twin delayed deep deterministic policy \cite{td3} mainly utilizes the clipped double Q learning to alleviate the above overestimation.
SAC \cite{sac,taec} is the SOTA algorithm of policy learning under the model-based paradigm. In the framework of SAC, the actor aims to maximize expected reward with entropy and the critic evaluates the expected cumulative reward with entropy. Due to the splendid performance of SAC, we choose it as the RL instance to prove the theoretical properties, by distilling the planning into an RL policy.