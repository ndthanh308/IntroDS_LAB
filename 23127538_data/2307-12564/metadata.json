{
  "title": "Towards Generalising Neural Topical Representations",
  "authors": [
    "Xiaohao Yang",
    "He Zhao",
    "Dinh Phung",
    "Lan Du"
  ],
  "submission_date": "2023-07-24T07:17:33+00:00",
  "revised_dates": [
    "2024-04-09T01:13:31+00:00",
    "2024-05-22T00:09:33+00:00",
    "2024-06-14T00:15:46+00:00"
  ],
  "abstract": "Topic models have evolved from conventional Bayesian probabilistic models to recent Neural Topic Models (NTMs). Although NTMs have shown promising performance when trained and tested on a specific corpus, their generalisation ability across corpora has yet to be studied. In practice, we often expect that an NTM trained on a source corpus can still produce quality topical representation (i.e., latent distribution over topics) for the document from different target corpora to a certain degree. In this work, we aim to improve NTMs further so that their representation power for documents generalises reliably across corpora and tasks. To do so, we propose to enhance NTMs by narrowing the semantic distance between similar documents, with the underlying assumption that documents from different corpora may share similar semantics. Specifically, we obtain a similar document for each training document by text data augmentation. Then, we optimise NTMs further by minimising the semantic distance between each pair, measured by the Topical Optimal Transport (TopicalOT) distance, which computes the optimal transport distance between their topical representations. Our framework can be readily applied to most NTMs as a plug-and-play module. Extensive experiments show that our framework significantly improves the generalisation ability regarding neural topical representation across corpora. Our code and datasets are available at: https://github.com/Xiaohao-Yang/Topic_Model_Generalisation.",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "primary_category": "cs.CL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12564",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 32509899,
  "size_after_bytes": 896329
}