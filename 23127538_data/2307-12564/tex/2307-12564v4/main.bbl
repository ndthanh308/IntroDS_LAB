\begin{thebibliography}{10}

\bibitem{blei2003modeling}
D.~M. Blei and M.~I. Jordan, ``Modeling annotated data,'' in {\em Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval}, pp.~127--134, 2003.

\bibitem{reisenbichler2019topic}
M.~Reisenbichler and T.~Reutterer, ``Topic modeling in marketing: recent advances and research opportunities,'' {\em Journal of Business Economics}, vol.~89, no.~3, pp.~327--356, 2019.

\bibitem{laureate2023systematic}
C.~D.~P. Laureate, W.~Buntine, and H.~Linger, ``A systematic review of the use of topic models for short text social media analysis,'' {\em Artificial Intelligence Review}, pp.~1--33, 2023.

\bibitem{liu2016overview}
L.~Liu, L.~Tang, W.~Dong, S.~Yao, and W.~Zhou, ``An overview of topic modeling and its current applications in bioinformatics,'' {\em SpringerPlus}, vol.~5, no.~1, pp.~1--22, 2016.

\bibitem{blei2003latent}
D.~M. Blei, A.~Y. Ng, and M.~I. Jordan, ``Latent dirichlet allocation,'' {\em Journal of machine Learning research}, vol.~3, no.~Jan, pp.~993--1022, 2003.

\bibitem{zhao2020neural}
H.~Zhao, D.~Phung, V.~Huynh, T.~Le, and W.~Buntine, ``Neural topic model via optimal transport,'' {\em arXiv preprint arXiv:2008.13537}, 2020.

\bibitem{wang2022generalizing}
J.~Wang, C.~Lan, C.~Liu, Y.~Ouyang, T.~Qin, W.~Lu, Y.~Chen, W.~Zeng, and P.~Yu, ``Generalizing to unseen domains: A survey on domain generalization,'' {\em IEEE Transactions on Knowledge and Data Engineering}, 2022.

\bibitem{zhou2022domain}
K.~Zhou, Z.~Liu, Y.~Qiao, T.~Xiang, and C.~C. Loy, ``Domain generalization: A survey,'' {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol.~45, no.~4, pp.~4396--4415, 2022.

\bibitem{wei2019eda}
J.~Wei and K.~Zou, ``Eda: Easy data augmentation techniques for boosting performance on text classification tasks,'' {\em arXiv preprint arXiv:1901.11196}, 2019.

\bibitem{shorten2021text}
C.~Shorten, T.~M. Khoshgoftaar, and B.~Furht, ``Text data augmentation for deep learning,'' {\em Journal of big Data}, vol.~8, pp.~1--34, 2021.

\bibitem{feng2021survey}
S.~Y. Feng, V.~Gangal, J.~Wei, S.~Chandar, S.~Vosoughi, T.~Mitamura, and E.~Hovy, ``A survey of data augmentation approaches for nlp,'' {\em arXiv preprint arXiv:2105.03075}, 2021.

\bibitem{bayer2022survey}
M.~Bayer, M.-A. Kaufhold, and C.~Reuter, ``A survey on data augmentation for text classification,'' {\em ACM Computing Surveys}, vol.~55, no.~7, pp.~1--39, 2022.

\bibitem{zhao2021topic}
H.~Zhao, D.~Phung, V.~Huynh, Y.~Jin, L.~Du, and W.~Buntine, ``Topic modelling meets deep neural networks: A survey,'' {\em arXiv preprint arXiv:2103.00498}, 2021.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{rezende2014stochastic}
D.~J. Rezende, S.~Mohamed, and D.~Wierstra, ``Stochastic backpropagation and approximate inference in deep generative models,'' in {\em International conference on machine learning}, pp.~1278--1286, PMLR, 2014.

\bibitem{peyre2019computational}
G.~Peyr{\'e}, M.~Cuturi, {\em et~al.}, ``Computational optimal transport: With applications to data science,'' {\em Foundations and Trends{\textregistered} in Machine Learning}, vol.~11, no.~5-6, pp.~355--607, 2019.

\bibitem{flamary2021pot}
R.~Flamary, N.~Courty, A.~Gramfort, M.~Z. Alaya, A.~Boisbunon, S.~Chambon, L.~Chapel, A.~Corenflos, K.~Fatras, N.~Fournier, {\em et~al.}, ``Pot: Python optimal transport,'' {\em Journal of Machine Learning Research}, vol.~22, no.~78, pp.~1--8, 2021.

\bibitem{cuturi2013sinkhorn}
M.~Cuturi, ``Sinkhorn distances: Lightspeed computation of optimal transport,'' {\em Advances in neural information processing systems}, vol.~26, 2013.

\bibitem{yurochkin2019hierarchical}
M.~Yurochkin, S.~Claici, E.~Chien, F.~Mirzazadeh, and J.~M. Solomon, ``Hierarchical optimal transport for document representation,'' {\em Advances in Neural Information Processing Systems}, vol.~32, 2019.

\bibitem{mikolov2013efficient}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word representations in vector space,'' {\em arXiv preprint arXiv:1301.3781}, 2013.

\bibitem{pennington2014glove}
J.~Pennington, R.~Socher, and C.~D. Manning, ``Glove: Global vectors for word representation,'' in {\em Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)}, pp.~1532--1543, 2014.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep bidirectional transformers for language understanding,'' {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{kusner2015word}
M.~Kusner, Y.~Sun, N.~Kolkin, and K.~Weinberger, ``From word embeddings to document distances,'' in {\em International conference on machine learning}, pp.~957--966, PMLR, 2015.

\bibitem{bonneel2011displacement}
N.~Bonneel, M.~Van De~Panne, S.~Paris, and W.~Heidrich, ``Displacement interpolation using lagrangian mass transport,'' in {\em Proceedings of the 2011 SIGGRAPH Asia conference}, pp.~1--12, 2011.

\bibitem{newman2010automatic}
D.~Newman, J.~H. Lau, K.~Grieser, and T.~Baldwin, ``Automatic evaluation of topic coherence,'' in {\em Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics}, pp.~100--108, 2010.

\bibitem{lau2014machine}
J.~H. Lau, D.~Newman, and T.~Baldwin, ``Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality,'' in {\em Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics}, pp.~530--539, 2014.

\bibitem{ma2019nlpaug}
E.~Ma, ``Nlp augmentation.'' https://github.com/makcedward/nlpaug, 2019.

\bibitem{patrini2020sinkhorn}
G.~Patrini, R.~Van~den Berg, P.~Forre, M.~Carioni, S.~Bhargav, M.~Welling, T.~Genewein, and F.~Nielsen, ``Sinkhorn autoencoders,'' in {\em Uncertainty in Artificial Intelligence}, pp.~733--743, PMLR, 2020.

\bibitem{miao2017discovering}
Y.~Miao, E.~Grefenstette, and P.~Blunsom, ``Discovering discrete latent topics with neural variational inference,'' in {\em International Conference on Machine Learning}, pp.~2410--2419, PMLR, 2017.

\bibitem{srivastava2017autoencoding}
A.~Srivastava and C.~Sutton, ``Autoencoding variational inference for topic models,'' {\em arXiv preprint arXiv:1703.01488}, 2017.

\bibitem{zhang2018whai}
H.~Zhang, B.~Chen, D.~Guo, and M.~Zhou, ``Whai: Weibull hybrid autoencoding inference for deep topic modeling,'' {\em arXiv preprint arXiv:1803.01328}, 2018.

\bibitem{burkhardt2019decoupling}
S.~Burkhardt and S.~Kramer, ``Decoupling sparsity and smoothness in the dirichlet variational autoencoder topic model.,'' {\em J. Mach. Learn. Res.}, vol.~20, no.~131, pp.~1--27, 2019.

\bibitem{tian2020learning}
R.~Tian, Y.~Mao, and R.~Zhang, ``Learning vae-lda models with rounded reparameterization trick,'' in {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.~1315--1325, 2020.

\bibitem{card2017neural}
D.~Card, C.~Tan, and N.~A. Smith, ``Neural models for documents with metadata,'' {\em arXiv preprint arXiv:1705.09296}, 2017.

\bibitem{dieng2020topic}
A.~B. Dieng, F.~J. Ruiz, and D.~M. Blei, ``Topic modeling in embedding spaces,'' {\em Transactions of the Association for Computational Linguistics}, vol.~8, pp.~439--453, 2020.

\bibitem{bianchi2020pre}
F.~Bianchi, S.~Terragni, and D.~Hovy, ``Pre-training is a hot topic: Contextualized document embeddings improve topic coherence,'' {\em arXiv preprint arXiv:2004.03974}, 2020.

\bibitem{bianchi2020cross}
F.~Bianchi, S.~Terragni, D.~Hovy, D.~Nozza, and E.~Fersini, ``Cross-lingual contextualized topic models with zero-shot learning,'' {\em arXiv preprint arXiv:2004.07737}, 2020.

\bibitem{xu2022hyperminer}
Y.~Xu, D.~Wang, B.~Chen, R.~Lu, Z.~Duan, M.~Zhou, {\em et~al.}, ``Hyperminer: Topic taxonomy mining with hyperbolic embedding,'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~31557--31570, 2022.

\bibitem{huynh2020otlda}
V.~Huynh, H.~Zhao, and D.~Phung, ``Otlda: A geometry-aware optimal transport approach for topic modeling,'' {\em Advances in Neural Information Processing Systems}, vol.~33, pp.~18573--18582, 2020.

\bibitem{nan2019topic}
F.~Nan, R.~Ding, R.~Nallapati, and B.~Xiang, ``Topic modeling with wasserstein autoencoders,'' {\em arXiv preprint arXiv:1907.12374}, 2019.

\bibitem{zhang2023topic}
D.~C. Zhang and H.~W. Lauw, ``Topic modeling on document networks with dirichlet optimal transport barycenter,'' {\em IEEE Transactions on Knowledge and Data Engineering}, 2023.

\bibitem{chen2014topic}
Z.~Chen and B.~Liu, ``Topic modeling using topics from many domains, lifelong learning and big data,'' in {\em International conference on machine learning}, pp.~703--711, PMLR, 2014.

\bibitem{chen2015lifelong}
Z.~Chen, ``Lifelong machine learning for topic modeling and beyond,'' in {\em Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop}, pp.~133--139, 2015.

\bibitem{blum2016generalized}
A.~Blum and N.~Haghtalab, ``Generalized topic modeling,'' {\em arXiv preprint arXiv:1611.01259}, 2016.

\bibitem{chen2019affinity}
Y.~Chen, J.~Wu, J.~Lin, R.~Liu, H.~Zhang, and Z.~Ye, ``Affinity regularized non-negative matrix factorization for lifelong topic modeling,'' {\em IEEE Transactions on Knowledge and Data Engineering}, vol.~32, no.~7, pp.~1249--1262, 2019.

\bibitem{gupta2020neural}
P.~Gupta, Y.~Chaudhary, T.~Runkler, and H.~Schuetze, ``Neural topic modeling with continual lifelong learning,'' in {\em International Conference on Machine Learning}, pp.~3907--3917, PMLR, 2020.

\bibitem{qin2021lifelong}
X.~Qin, Y.~Lu, Y.~Chen, and Y.~Rao, ``Lifelong learning of topics and domain-specific word embeddings,'' in {\em Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pp.~2294--2309, 2021.

\bibitem{zhang2022lifelong}
X.~Zhang, Y.~Rao, and Q.~Li, ``Lifelong topic modeling with knowledge-enhanced adversarial network,'' {\em World Wide Web}, vol.~25, no.~1, pp.~219--238, 2022.

\bibitem{lei2023nmtf}
Z.~Lei, H.~Liu, J.~Yan, Y.~Rao, and Q.~Li, ``Nmtf-ltm: Towards an alignment of semantics for lifelong topic modeling,'' {\em IEEE Transactions on Knowledge and Data Engineering}, 2023.

\bibitem{iwata2021few}
T.~Iwata, ``Few-shot learning for topic modeling,'' {\em arXiv preprint arXiv:2104.09011}, 2021.

\bibitem{duan2022bayesian}
Z.~Duan, Y.~Xu, J.~Sun, B.~Chen, W.~Chen, C.~Wang, and M.~Zhou, ``Bayesian deep embedding topic meta-learner,'' in {\em International Conference on Machine Learning}, pp.~5659--5670, PMLR, 2022.

\bibitem{xu2024context}
Y.~Xu, J.~Sun, Y.~Su, X.~Liu, Z.~Duan, B.~Chen, and M.~Zhou, ``Context-guided embedding adaptation for effective topic modeling in low-resource regimes,'' {\em Advances in Neural Information Processing Systems}, vol.~36, 2024.

\bibitem{chang2021word}
C.-H. Chang and S.-Y. Hwang, ``A word embedding-based approach to cross-lingual topic modeling,'' {\em Knowledge and Information Systems}, vol.~63, no.~6, pp.~1529--1555, 2021.

\bibitem{grootendorst2022bertopic}
M.~Grootendorst, ``Bertopic: Neural topic modeling with a class-based tf-idf procedure,'' {\em arXiv preprint arXiv:2203.05794}, 2022.

\bibitem{wang2023prompting}
H.~Wang, N.~Prakash, N.~K. Hoang, M.~S. Hee, U.~Naseem, and R.~K.-W. Lee, ``Prompting large language models for topic modeling,'' in {\em 2023 IEEE International Conference on Big Data (BigData)}, pp.~1236--1241, IEEE, 2023.

\bibitem{pham2023topicgpt}
C.~M. Pham, A.~Hoyle, S.~Sun, and M.~Iyyer, ``Topicgpt: A prompt-based topic modeling framework,'' {\em arXiv preprint arXiv:2311.01449}, 2023.

\bibitem{chang2024enhanced}
S.~Chang, R.~Wang, P.~Ren, and H.~Huang, ``Enhanced short text modeling: Leveraging large language models for topic refinement,'' {\em arXiv preprint arXiv:2403.17706}, 2024.

\bibitem{yue2019domain}
X.~Yue, Y.~Zhang, S.~Zhao, A.~Sangiovanni-Vincentelli, K.~Keutzer, and B.~Gong, ``Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data,'' in {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.~2100--2110, 2019.

\bibitem{liu2021learning}
C.~Liu, X.~Sun, J.~Wang, H.~Tang, T.~Li, T.~Qin, W.~Chen, and T.-Y. Liu, ``Learning causal semantic representation for out-of-distribution prediction,'' {\em Advances in Neural Information Processing Systems}, vol.~34, pp.~6155--6170, 2021.

\bibitem{gong2019dlow}
R.~Gong, W.~Li, Y.~Chen, and L.~V. Gool, ``Dlow: Domain flow for adaptation and generalization,'' in {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.~2477--2486, 2019.

\bibitem{li2021semantic}
D.~Li, J.~Yang, K.~Kreis, A.~Torralba, and S.~Fidler, ``Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization,'' in {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.~8300--8311, 2021.

\bibitem{li2017domain}
W.~Li, Z.~Xu, D.~Xu, D.~Dai, and L.~Van~Gool, ``Domain generalization and adaptation using low rank exemplar svms,'' {\em IEEE transactions on pattern analysis and machine intelligence}, vol.~40, no.~5, pp.~1114--1127, 2017.

\bibitem{li2019episodic}
D.~Li, J.~Zhang, Y.~Yang, C.~Liu, Y.-Z. Song, and T.~M. Hospedales, ``Episodic training for domain generalization,'' in {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.~1446--1455, 2019.

\bibitem{balaji2018metareg}
Y.~Balaji, S.~Sankaranarayanan, and R.~Chellappa, ``Metareg: Towards domain generalization using meta-regularization,'' {\em Advances in neural information processing systems}, vol.~31, 2018.

\bibitem{wang2020unseen}
Z.~Wang, Q.~Wang, C.~Lv, X.~Cao, and G.~Fu, ``Unseen target stance detection with adversarial domain generalization,'' in {\em 2020 International Joint Conference on Neural Networks (IJCNN)}, pp.~1--8, IEEE, 2020.

\bibitem{wang2020meta}
B.~Wang, M.~Lapata, and I.~Titov, ``Meta-learning for domain generalization in semantic parsing,'' {\em arXiv preprint arXiv:2010.11988}, 2020.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for contrastive learning of visual representations,'' in {\em International conference on machine learning}, pp.~1597--1607, PMLR, 2020.

\bibitem{nguyen2021contrastive}
T.~Nguyen and A.~T. Luu, ``Contrastive learning for neural topic model,'' {\em Advances in Neural Information Processing Systems}, vol.~34, pp.~11974--11986, 2021.

\bibitem{chen2020big}
T.~Chen, S.~Kornblith, K.~Swersky, M.~Norouzi, and G.~E. Hinton, ``Big self-supervised models are strong semi-supervised learners,'' {\em Advances in neural information processing systems}, vol.~33, pp.~22243--22255, 2020.

\bibitem{lang1995newsweeder}
K.~Lang, ``Newsweeder: Learning to filter netnews,'' in {\em Machine Learning Proceedings 1995}, pp.~331--339, Elsevier, 1995.

\bibitem{phan2008learning}
X.-H. Phan, L.-M. Nguyen, and S.~Horiguchi, ``Learning to classify short and sparse text \& web with hidden topics from large-scale data collections,'' in {\em Proceedings of the 17th international conference on World Wide Web}, pp.~91--100, 2008.

\bibitem{vitale2012classification}
D.~Vitale, P.~Ferragina, and U.~Scaiella, ``Classification of short texts by deploying topical annotations,'' in {\em European Conference on Information Retrieval}, pp.~376--387, Springer, 2012.

\bibitem{zhang2015character}
X.~Zhang, J.~Zhao, and Y.~LeCun, ``Character-level convolutional networks for text classification,'' {\em Advances in neural information processing systems}, vol.~28, 2015.

\bibitem{nguyen2015improving}
D.~Q. Nguyen, R.~Billingsley, L.~Du, and M.~Johnson, ``Improving topic models with latent feature word representations,'' {\em Transactions of the Association for Computational Linguistics}, vol.~3, pp.~299--313, 2015.

\bibitem{schutze2008introduction}
H.~Schutze, C.~D. Manning, and P.~Raghavan, {\em Introduction to information retrieval}.
\newblock Cambridge University Press, 2008.

\bibitem{aletras2013evaluating}
N.~Aletras and M.~Stevenson, ``Evaluating topic coherence using distributional semantics,'' in {\em Proceedings of the 10th international conference on computational semantics (IWCS 2013)--Long Papers}, pp.~13--22, 2013.

\bibitem{yang2015efficient}
Y.~Yang, D.~Downey, and J.~Boyd-Graber, ``Efficient methods for incorporating knowledge into topic models,'' in {\em Proceedings of the 2015 conference on empirical methods in natural language processing}, pp.~308--317, 2015.

\bibitem{zhao2018dirichlet}
H.~Zhao, L.~Du, W.~Buntine, and M.~Zhou, ``Dirichlet belief networks for topic structure learning,'' {\em Advances in neural information processing systems}, vol.~31, 2018.

\bibitem{roder2015exploring}
M.~R{\"o}der, A.~Both, and A.~Hinneburg, ``Exploring the space of topic coherence measures,'' in {\em Proceedings of the eighth ACM international conference on Web search and data mining}, pp.~399--408, 2015.

\bibitem{xuan2016bayesian}
J.~Xuan, J.~Lu, G.~Zhang, R.~Y. Da~Xu, and X.~Luo, ``Bayesian nonparametric relational topic model through dependent gamma processes,'' {\em IEEE Transactions on Knowledge and Data Engineering}, vol.~29, no.~7, pp.~1357--1369, 2016.

\bibitem{qiang2020short}
J.~Qiang, Z.~Qian, Y.~Li, Y.~Yuan, and X.~Wu, ``Short text topic modeling techniques, applications, and performance: a survey,'' {\em IEEE Transactions on Knowledge and Data Engineering}, vol.~34, no.~3, pp.~1427--1445, 2020.

\end{thebibliography}
