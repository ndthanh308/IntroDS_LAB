\section{Related Work}\label{sec:related}


%\subsection{Adversarial Example Generation}
%\noindent\textbf{Digital Adversarial Examples.} 
Since the concept of AEs was initially proposed~\cite{szegedy2013intriguing}, many techniques have been put forward to generate AEs against state-of-the-art machine learning classifiers in the digital space~\cite{wang2019security}, where attackers have precise control over adversarial modifications. In this paper, we focus on AEs in the physical domain.

%~\cite{goodfellow2014explaining, madry2017towards}. % FGSM, BIM, ILLCM, PGD, C&W
% Move next line to intro or bg
%Designed for attacking in the digital space, the resulting adversarial examples are fed directly to the target classifier and can be precisely controlled by the attacker, even down to the modification of a single pixel~\cite{su2019one}.

%\begin{itemize}
%    \item linear properties of classifier manifolds
%\end{itemize}


\noindent\textbf{Physical Adversarial Examples.}
% In this section, we survey existing efforts of transferring adversarial examples to the physical world. 
Kurakin et al.~\cite{kurakin2018adversarial} study how digital-domain attacks perform in the physical world using photographs of printed-out AEs, finding that they can survive certain transformations like sensor noise, rescaling, and brightness changes.
However, Lu et al.~\cite{lu2017no} showed that the robustness of adversarial examples generated using this approach is decreased over multiple viewings from different angles and distances. In order to generate more robust AEs in the real world, Athalye et al.~\cite{athalye2018synthesizing} propose Expectation over Transformation, optimizing AEs over a set of transformations to account for varying physical environmental conditions by projecting 2-dimensional perturbations onto 3D-printed objects.~\cite{jia2022fooling} also, consider a set of environmental transformations when fabricating malicious traffic signs. In all cases, these approaches focused on static, object-modifying attacks.

Many works create physical AEs by applying adversarial patterns directly to objects themselves. For example, Sharif et al.~\cite{sharif2016accessorize} print adversarial patterns on eyeglass frames to fool facial recognition, and Eykholt et al.~\cite{eykholt2018robust} place stickers on street signs mimicking graffiti to cause misclassification. %Chen et al.~\cite{chen2018shapeshifter}
Another set of approaches~\cite{thys2019fooling, brown2017adversarial,zhao2019seeing} generate adversarial patches. Li et al.~\cite{li2019adversarial} demonstrate an adversarial sticker not limited to a single object by applying a translucent sticker in front of the camera itself. Similarly, a key constraint of all these attacks is that they are object-modifying, static attacks, and cannot adapt or be modified once applied.
%Athalye et al~\cite{athalye2018synthesizing} 3D-print objects with adversarial patterns on the surface.

Other works use light-based perturbations projected onto surfaces in facial recognition  attacks~\cite{nguyen2020adversarial,zhou2018invisible} 
and object detection and classification~\cite{lovisotto2020slap, nichols2018projecting}. Projecting AEs onto objects has the benefits of being more dynamic and not leaving behind physical artifacts. However, they suffer from increased sensitivity to environmental lighting conditions and are still difficult to scale to multiple objects. The idea of using light projection to attack has also been applied to the camera sensors. Exploiting ghost effect and auto-exposure control in optical imaging systems, Man et al.~\cite{man2020ghostimage} use a projector to inject arbitrary patterns (e.g. a stop sign) into the victim camera's field of view. This attack suffers from increased sensitivity to environmental lighting conditions as well. Also, it is not easy to apply the attack in a real-world setting (e.g. a moving camera on an autonomous vehicle).  Wang et al.~\cite{wang2021can} utilized infrared (IR) lights to attack autonomous vehicles. However, IR lights can be strongly interfered by solar radiation. This significantly limits the capability of the attack. Along the direction of attacking the vulnerabilities in sensors, Kohler et al.~\cite{kohler2021they, sayles2021invisible} exploited the rolling shutter in CMOS image sensors using a bright, modulated light source to cause image disruptions. Ji et al.~\cite{ji2021poltergeist} exploited the inertial sensors  meant for image stabilization. In their attack, an adversary controls the output of an inertial sensor by emitting deliberately designed acoustic signals. This produces blurred images that will be misclassified in the decision-making pipeline.

%\noindent\textbf{Perception attacks on Safety-Critical Systems.}

\begin{comment}
\begin{itemize}
    \item Adversarial examples in the physical world~\cite{kurakin2018adversarial}
    \item Robust physical world attacks on deep learning visual classification~\cite{eykholt2018robust}
    \item Adversarial Camera stickers~\cite{li2019adversarial}
    \item projecting trouble~\cite{nichols2018projecting}
    \item Dynamic adversarial patch~\cite{hoory2020dynamic}
    \item Invisible Mask~\cite{zhou2018invisible}
    \item Adversarial light projection attacks~\cite{nguyen2020adversarial}
    \item Seeing isn't believing~\cite{zhao2019seeing}
    \item SLAP~\cite{lovisotto2020slap}
    \item ShapeShifter~\cite{chen2018shapeshifter}
    \item Phamton of the ADAS~\cite{nassi2020phantom}
    \item No Need to Worry~\cite{lu2017no}
    \item Synthesizing robust adversarial examples~\cite{athalye2018synthesizing}
    \item Making an Invisibility Cloak~\cite{wu2020making}
    \item Fooling automated surveillance cameras~\cite{thys2019fooling}
\end{itemize}


\subsection{Dynamic Adversarial Examples?}
In this section, we
\begin{itemize}
    \item real time attack on a speech recognition system
    \item ...
\end{itemize}

\begin{itemize}
    \item Zhou et al.~\cite{zhou2018invisible} attack face recognition system by illuminating the subject using infrared light according to adversarial examples calculated by attack algorithms. However, their approaches are subjected to two major issues: firstly, the method requires laborious manual calibration to match the physical adversarial samples to those calculated adversarial samples. By using their calibration tools, time limit of tuning is less than ten minutes. Secondly, because of the simplified light spot model, finding the actual local minimum can be continued with attackers' hands instead of an optimizer. 
    \item Seeing isn't believing~\url{https://dl.acm.org/doi/10.1145/3319535.3354259}: we are attacking the sensor side, but they enumerate challenges of physical; limitations is because physical space is usually unpredictable; main approach is to apply random transformation 
    \item Thys et al.~\cite{thys2019fooling} develop an approach for generating physical (printed-out) adversarial patches that, when held in front of a person, prevents their detection by a YOLOv2 person detector. The physical patch, while effective, relies heavily on proper positioning and orientation relative to the person.
    \item Eykholt et al.~\cite{eykholt2018robust} focus on generating stealthy ``grafitti-like'' perturbation stickers to be placed on street signs to misclassify them.  
\end{itemize}
\end{comment}