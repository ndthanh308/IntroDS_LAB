\begin{abstract}
Camera-based autonomous systems that emulate human perception are increasingly being integrated into safety-critical platforms.
Consequently, an established body of literature has emerged that explores adversarial attacks targeting the underlying machine learning models. 
% These attacks produce high-confidence misclassifications by digitally manipulating pixels in an image, taking advantage of the complex and non-convex functions learned by these classifiers.
%These attacks take advantage of incongruities in the learned manifolds of classifiers by carefully manipulating image pixels to produce misclassifications with high confidence, and are usually generated and applied digitally. %\eric{``incongruities in the learned manifolds of classifiers'' is accurate but reads slightly weird, like we're trying to sound fancy} 
Adapting adversarial attacks to the physical world is desirable for the attacker, as this removes the need to compromise digital systems. However, the real world poses challenges related to the ``survivability" of adversarial manipulations given environmental noise in perception pipelines and the dynamicity of autonomous systems.
%Modeling these attacks in the real, physical world is desired because they eliminate compromised camera requirements, but they are much more challenging given the environmental noise in perception pipelines and the dynamicity of autonomous systems. 
%\eric{we don't explicitly mention the wide-range of existing security practices to protect against digital-domain attacks, minimizing the attack surface}\luis{It may be too much for the abstract. It's already long as is. We also shouldn't have a paragraph break.}
%However, these digital domain attacks require access to electronic systems that may be rigorously secured. 
%While effective, one practical drawback of these attacks is the system access required to digitally manipulate classifier inputs.
%Adversaries have recently taken advantage of static, physical-domain attacks focusing on camera-observed objects. However, these static attacks on physical objects have limited practicality and may be susceptible to human-in-the-loop detection.
%To circumvent this, recent work focuses on attacks in the physical domain involving the malicious modification of camera-observed objects. These physical adversarial attacks don't require direct access to classifier inputs, but are usually static with limited portability, and may be susceptible to human-in-the-loop detection.
%\eric{reason for a paragraph break here?}
In this paper, we take a sensor-first approach.
%We demonstrate that a small device placed on a camera sensor can be used to perform dynamic adversarial attacks and evade human-in-the-loop detection.
%We demonstrate how dynamic attacks can exploit the optics of cameras to produce targeted misclassification for several objects under a variety of illumination conditions.
We present \namenospace, a man-in-the-middle perception attack that leverages transparent displays to generate dynamic physical adversarial examples. \name exploits the camera's optics to induce misclassifications under a variety of illumination conditions.
%Not only can we generate dynamic payloads, but we also formalize the transformation of a digital attack projected onto the physical domain by modeling the optics of the camera-scene transformation function.
To generate dynamic perturbations, we formalize the projection of a digital attack into the physical domain by modeling the transformation function of the captured image through the optical pipeline. %\eric{are ``dynamic payloads'' and ``camera scene'' the best way to describe this?}
%%We focus on an autonomous driving use case, where camera sensors detect and classify nearby objects to inform vehicle decisions. 
%Our extensive experiments show that \name generates robust adversarial perturbations that can mislead both classifiers and object detectors with up to $70\%$ attack success rate (ASR).  
% \eric{$99\%$ success is a very bold claim. how true is this figure?}
Our extensive experiments show that \namenospace's generated adversarial perturbations are much more robust across varying environmental light conditions relative to existing physical perturbation frameworks, achieving a high attack success rate (ASR) while bypassing state-of-the-art physical adversarial detection frameworks. %\eric{is ``light conditions'' clear in this context?}\luis{Reworded.} %This makes the attack a bigger threat in a real world scenario. 
We demonstrate that the dynamic nature of \name enables attackers to adapt adversarial examples across a variety of objects with a significantly higher ASR compared to state-of-the-art physical world attack frameworks. 
% that the dynamic nature of \name enables attackers to adapt adversarial examples across a va
%$30\%$ higher ASR compared to state-of-the-art, static approaches. 
%\eric{couple of things: 1. replace ``all kinds of traffic signs'' with something more precise, like ``a set of 10 roadways signs as defined by the  Manual on Uniform Traffic Control Devices prepared by the US Department of Transportation Federal Highway Administration Office of Transportation Operations'' (or cite this manual). 2. this is the first time that we mention the self-driving car application, so instantly jumping into the fact that we can attack more traffic signs than some uncited state-of-the-art approach is confusing without first laying out the proper context. 3. ``enables it to attack'' sounds weird. does \name perform the attack, or does an attacker utilize \name? we should make a decision and be consistent throughout the paper.}\luis{Attempted a fix} 
%\name is able to bypass the state-of-the-art physical adversarial detection framework. %\eric{no citation in the abstract?}\luis{The abstract should be self-contained without citations. I removed the Sentinet name.} 
Finally, we discuss mitigation strategies against the \name attack. % how the \name framework can be leveraged at training time to make existing defenses more robust. 
%\eric{this is confusion for 2 reasons. 1, it's a new item that follows a sentence already qualified with ``finally.'' 2. if this is a defense, then we should call it a defense rather than dancing around it.}\luis{It should read better now. I don't think we should explicitly label our approach as a defense since a large majority of our evaluation focuses on attacks.}

%We also find our generated attacks can evade detection by SentiNet, a framework for identifying physical adversarial examples.

% Our code and data have been made publicly available.
\end{abstract}

% From SLAP:
% We study the feasibility of SLAP in the self-driving scenario, targeting both object detector and traffic sign recognition tasks, focusing on the detection of stop signs. We conduct experiments in a variety of ambient light conditions, including outdoors, showing how in non-bright settings the proposed method generates AE that are extremely robust, causing misclassifications on state-of-the-art neural networks with up to 99% success rate. Our experiments show that SLAP-generated AE do not present detectable behaviours seen in adversarial patches and therefore bypass SentiNet, a physical AE detection method. We evaluate other defences including an adaptive defender using adversarial learning which is able to thwart the attack effectiveness up to 80% even in favourable attacker conditions.
