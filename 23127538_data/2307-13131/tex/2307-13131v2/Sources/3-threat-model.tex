\section{Threat Model}\label{sec:overview}
This section describes the system and attacker models we consider for the \name framework, including a description of the use-cases we focus on and overall attack goals. %We then provide an overview of the design challenges and goals.

\subsection{System Model}
%This work focuses on safety-critical autonomous systems that rely on camera-based perception. We generally consider perception-to-actuation scenarios composed of several steps. Initially, the scene from the optical sensor is converted into a digital image. Next, a deep learning-based perception model is used to perform both object detection and classification on the image. Specifically, perceived objects (e.g., a pedestrian or stop sign) are first detected and isolated within the image, then fed into a trained classifier (e.g., a traffic sign classifier). Finally, the classifier output informs the autonomous system's actions. Example use cases can be crowd surveillance where object recognition is used on camera feeds to detect human subjects of interest, or autonomous navigation, where a vehicle uses cameras to navigate through its environment.%, and security camera scenarios, where facial recognition is used for authentication. 
%\eric{this is repeated from Section 2.a}
This work focuses on safety-critical autonomous systems that rely on camera-based perception. Examples include crowd surveillance where object recognition is used on camera feeds to detect human subjects of interest, or autonomous navigation, where a vehicle uses cameras to navigate through its environment. In general, these perception-to-acutation pipelines are composed of several steps. Initially, the scene from the optical sensor is converted into a digital image. Next, a deep learning-based perception model is used to perform both object detection and classification on the image. 
% We test several well-known model architectures, like the Resnet-50 classifier and the Faster-RCNN and YOLO-V3 object detectors, in the common context of traffic sign recognition. The resulting model outputs inform the autonomous system's actions.

% Moved from attacker goals
We assume several protection mechanisms on the autonomous system to preclude basic attacks. Autonomous vehicles are often equipped with obstruction detection mechanisms and will disable capabilities such as autonomous steering when an obstruction is detected~\cite{souman2021human}. Additionally, some applications have human-in-the-loop control~\cite{sreenu2019intelligent} or may review footage in post-incident analyses, detecting attacks trying to directly fool the classifier (e.g., with overlaid images).

%In this work we focus on autonomous driving scenarios, in which a vehicle uses cameras to navigate through its environment. This process consists of several steps. Initially, the scene from the optical sensor is converted into a digital image. Next the twin steps of object detection and classification occur. Objects (e.g. a pedestrian or stop sign) are first detected and isolated within the image, then fed into a trained classifier (e.g. a traffic sign classifier). The classifier output informs the vehicle's actions.
%\eric{we use the terms ``optical sensor'' and ``camera'' interchangeably, and I'm concerned that it may lead to confusion for a reader. show we just pick one term? }
%\luis{We also need to generalize beyond autonomous driving.}
% \noindent\textbf{Motivating Example.}
% As is shown in \autoref{fig:setup-diagram}, an autonomous driving vehicle is equipped with a camera. The camera continuously captures the scene in front the vehicle. The video frames are sent to a traffic sign recognition module. The outputs of the recognition module controls what actions the vehicle needs to take (e.g., stop in front of a stop sign or slow down when a 55 speed limit sign is observed).

% An adversary intends to attack this vehicle using a gadget consisting of a transparent display (\autoref{fig:trans-display}) installed in front of the camera's optical sensor. The adversary can remotely control the gadget to display adversarial perturbations. In this way, the normal operation of the vehicle is disturbed, potentially leading to catastrophic consequences such as a crash.

% Based on different level of knowledge the adversary has of the camera under attack, he needs to take different actions and the effectiveness of the attack also varies. Under a White box scenario, where the adversary has full knowledge about the recognition module, i.e., he has access to the architecture and weights of the model as well as the gradient information during the inference of a frame, he needs to prepare various pictures of the traffic sign to be attacked, and generate a universal adversarial perturbation (UAP) based these images. Under a black box setting, the adversary needs to utilize the transferability of adversarial examples, i.e., he needs to craft the UAP based on a different recognition model, and apply the UAP to the camera under attack. He can use a publically available traffic sign dataset to train a widely used image recognition model. He then crafts UAPs based on this model.

% The attacker can dynamically change the displayed perturbation. This gives him the advantage of ...

\begin{comment}
\subsection{System Model}
\begin{itemize}
    \item The system has an optical sensor monitoring a particular space
    \item The system makes decisions based on inferences from the optical sensor
    \item The environment can change dynamically--either the sensor is moving or the objects coming into the space are moving.
    \item We assume the owner of the sensor will either tune in to the live feed of the sensor at some point or will want to retroactively view incidents that may have led to a particular decision
\end{itemize}
\end{comment}
%\luis{ TODO (MAYBE): Add assumption about size of image; cite "AML is not a problem in physical world."}
\subsection{Attacker Model}
\begin{comment}
    \item An attacker prepare universal adversarial perturbations (UAP) in advance.
    \item Alternatively, an attacker places a discreet secondary sensor observing the same space as the target sensor
    \item The attacker places a transparent displaying gadget in front of the target sensor, The device can display an arbitrary pattern controlled by the attack.
    %\autoref{fig:setup-diagram} shows the pico projector setup.
    \item Although the attacker does not have access to any software component of the target system, we assume the attacker has knowledge of the model. This model is consistent with prior works, and transferrability/generalizability of attacks is outside of the scope of the paper.
    \item Basically we assume a white box attack scenario, i.e., the attacker has access to the internal states (e.g. weights, gradients w.r.t. an input image, etc) of the model. 
    \item 
\end{comment}

% As depicted in Figure~\ref{fig:attack-overview}, 
We assume an adversary attaches a portable and low-profile gadget, consisting of an adversary-controlled transparent display and auxiliary sensor, in front of the victim's optical sensor or camera. We assume that an auxiliary sensor can sense when a target object enters the victim camera's field of view. The auxiliary sensor can take on many forms depending on the target application's real-time requirements and the attacker's resource constraints. For instance, the auxiliary sensor can be another camera attached alongside the victim camera with on-device object detection, or the sensor could be a GPS tracking device coupled with a map of traffic signs throughout the city. Alternatively, the ``sensor" could be the attacker remotely observing the victim camera's movement while controlling the transparent gadget. The auxiliary sensor informs the transparent display gadget which adversarial perturbation to display. We assume that the adversary can prepare adversarial perturbations in advance for each target class or that the real-time requirements for the target application allow the controller to generate perturbations at runtime\footnote{We note that current commercial off-the-shelf embedded technology may not be able to run real-time perturbation generation, so we assume that the adversary can prepare perturbations in advance as a practical workaround.}. %\matt{This assumption seems shaky when taken as a whole; I think that it might be better to word this something like: we can concede that current COTS embedded tech may not be strong enough to run real-time perturbation generation, so (as a practical workaround) we assume that an adversary can prepare perturbations in advance.}
%Finally, we assume that the transparent display does not degrade the performance of the target application when no light is projected onto the display, i.e., when there are no perturbations. \matt{Do we need to assume this? We have ablation studies showing no loss in accuracy when nothing is projected}  
%Unlike prior dynamic physical attack frameworks, which make changes to specific target objects, we do not assume to know the predetermined path of the target application so that the attacker can situationally adapt perturbations given the auxiliary sensor data. 
Previous physical attack frameworks require prior knowledge of a victim's intended route in order to physically modify objects (e.g. with stickers) before they are in view, limiting dynamicity. Using information from an auxiliary sensor (e.g. a GPS),  we can situationally adapt perturbations to attack the current object in view.

\noindent\textbf{Attacker goals.} The attacker's primary objective is to fool the victim application's recognition module, leading to erroneous object predictions which compromise safety, all while evading detection. This means that 1) employing denial-of-service attacks (e.g. applying opaque obstructions) or directly presenting adversarial images (e.g. an incorrect traffic sign) will be ineffective; 2) perturbing system optics during other states of normal operation is also undesirable, as it increases risk of detection; and 3) the attack should be able function under varied environmental conditions, which especially important for light-based attacks. \name is designed to achieve this goal while navigating the aforementioned constraints, generating dynamic perturbations optimized to look like chromatic aberrations or distortions of the camera sensor.  
%For \namenospace, the attacker's primary objective is to fool the victim application's recognition module, leading to erroneous object predictions which compromise safety. 
% Specifically, the attacker aims to opportunistically maximize the damage to an autonomous cyber-physical system by forcing an error in a safety-critical state -- e.g., forcing the misclassification of a criminal's face to someone else's face in the scene of a surveillance camera. 
%Conversely, the attacker does not want to perturb the optics for other states of normal operation, as that increases the risk of detection. Similarly, the attacker does \textit{not} want to employ simple denial-of-service attacks, e.g., by placing an opaque material over a camera lens or damaging or disabling the sensor.
% Moved to system model
%Autonomous vehicles are often equipped with obstruction detection mechanisms and will disable capabilities such as autonomous steering when an obstruction is detected~\cite{souman2021human} -- which \name is designed to circumvent. Finally, directly presenting target images (e.g. intended traffic sign) on the transparent display won't work as 1) some applications such as crowd surveillance have human-in-the-loop control~\cite{sreenu2019intelligent}; 2) even though some applications such as autonomous driving do not require human-in-the-loop control, leaving human perceptible evidence could potentially put the adversary in trouble in post-incident analyses. This is why \name needs to optimize for adversarial patterns that look like dazzle or color distortion of the camera sensor.


%the attacker wants to avoid. 

%\yi{now we have the models, we discuss next what we want to do with it.  }

%\eric{is this list intended to be in the final draft of the paper?}
%\yi{No. Need to integrate the list with the paragraph below.}
%An adversary attaches a transparent display gadget in front of the victim optical sensor or camera. He is able to remotely control the gadget to display an arbitrary image. The adversary prepare adversarial perturbations in advance. During the normal operation of the victim vehicle, the adversary displays these perturbations to attack the vehicle.
%\noindent\textbf{Attacker goals.} The goal of the attacker is to fool the recognition module of the victim vehicle to output erroneous predictions on perceived traffic signs. This would ultimately result in fatalities such as car crash.
%\eric{Do we specify targeted or untargeted attacks as the attacker goals? Currently reads that the goal is an untargeted attack.}
% \begin{itemize}
%     \item Perform targeted adversarial example attack on the target sensor based on a dynamically changing scene
%     \item The perturbation should be minimized to avoid detection when a user observes the attack
% \end{itemize}
