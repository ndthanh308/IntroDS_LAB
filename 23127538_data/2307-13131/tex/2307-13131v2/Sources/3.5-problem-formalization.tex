% Figure environment removed
\section{Methodology}\label{sec:design} 
In this section, we first formalize the research challenges for dynamically generating adversarial perturbations at the sensor-level. We then briefly provide an overview of how our design aims to address each research challenge. Finally, we describe the methodology of each design component for our proposed attack framework. 

% Figure environment removed

\subsection{Problem Formalization}
%\matt{todo: check for double parenthesized abbreviations and abbreviations that are only used once (e.g. CDTF defined twice with two different references)}
\noindent\textbf{Research Challenge \#1: Designing perturbations robust to optical transformations.} Directly applying an adversarial perturbation computed in the digital domain fails because it does not account for the mutations introduced by the camera-display transfer function (CDTF)~\cite{wengrowski2016optimal}, i.e., the transformation of a pixel projected in the physical world to the pixel captured by the camera. In our case, an adversarial perturbation is displayed and perceived by the camera, going through a set of transformations as depicted in Figure~\ref{fig:perturbation_pair}. The color spectrum emitted by an electronic display may not match the color spectral sensitivity function of the camera~\cite{wengrowski2016optimal}, leading to less color-robust perturbations. 
% Prior work~\cite{lovisotto2020slap, wengrowski2019light} addressed this challenge by using a neural network to learn the color transformation. 
Moreover, because the transparent display is in close proximity to the victim camera, the displayed perturbation is blurred in the perceived image due to the Bokeh effect~\cite{ignatov2020rendering}. The image formation pipeline processes within the camera converts the light received by the image sensor to a final digital image that is then affected by exposure, the sensor's sensitivity (ISO), and contrast transforms. %It would be too complicated to model each of these transformations one by one, so we employ an end-to-end approach in our attack~\cite{wengrowski2019light}.

\noindent\textbf{Research Challenge \#2: Designing perturbations robust to environmental conditions.} Multiple factors must be considered to make an adversarial perturbation robust to dynamically changing conditions in the physical environment. For instance, the camera's perspective, distance, and rotation relative to a target object will all impact the efficacy of a perturbation. The scene's environmental background and ambient illuminance also play a vital role in object recognition. %We propose an automated data collection framework that incorporates these factors during the perturbation training and generation process. This approach is analogous to data augmentation techniques employed by prior dynamic approaches~\cite{lovisotto2020slap}. However, in our case, we explicitly account for the impact of environmental factors to enhance situational awareness in downstream perturbation tasks. For instance, the training process may find that certain classes of adversarial examples only work for specific illuminance ranges. 

\noindent\textbf{Research Challenge \#3: Efficiently generating perturbations for real-time applications.} In addition to optical and physical mutations, the transient nature of real-time applications introduces a significant challenge to AE generation.
Environmental conditions can change significantly within a short period of time.
%The variance in an environment may be significant across multiple frames. 
Because generating a perturbation incurs latency in the attack pipeline, attack perturbations should be generated to perform well across this environmental variance, rather than attempt to optimize for a single frame. %Thus, Cataract adopts universal adversarial perturbation (UAP) models~\cite{moosavi2017universal} to generate perturbations that will be robust against variance across multiple time frames for a given scene.

\noindent\textbf{Design Overview.} Figure~\ref{fig:training} depicts an overview of the \name framework. To address the digital-to-physical mapping of \textbf{Research Challenge \#1} and \textbf{Research Challenge \#2}, we employ an end-to-end deep learning approach for modeling the optical transformations (we provide details in Section~\ref{sec:modeling}). We incorporate environmental background noise and illuminance variations into an automated data collection framework for perturbation training and generation (details in Section~\ref{sec:crafting-adversarial}). Finally, to address the real-time dynamics discussed in \textbf{Research Challenge \#3}, we adopt a universal adversarial perturbation (UAP) model~\cite{moosavi2017universal} that is robust against variance across multiple time frames for a given scene. Moreover, we leverage semantic information to efficiently generate this UAP (detailed in Section~\ref{sec:crafting-adversarial}).



%%% Begin comment 4/14
%%%\subsection{Research Challenges}
% \yi{Bring the challenges of Tnet upfront and in a more formal way with definitions and equations.}
%%%We now highlight the key research challenges for our novel attack given the provided system and attacker model.

%%%\begin{itemize}
   %%% \item Pixel level perturbation might not survive display-camera optical path, therefore, we propose to use dot-like patterns to construct adversarial perturbations. We proposed a perturbation generation function based on the $\alpha$-blending model.
%%%    \item To make sure evasion when transferring the perturbation from the digital space to the physical space, we propose to use a neural network model to model the transformation between a digital perturbation and a physical perturbation. This transformation model calibrates the digital perturbation with what is actually captured by the camera.
   %%% \item ...
%%%\end{itemize}

%%%\noindent\textbf{Research Challenge \#1: Perturbations robust to optical transformations.} Directly applying an adversarial perturbation computed in the digital domain fails because it does not account for the mutations introduced by the camera-display transfer function (CDTF)~\cite{wengrowski2016optimal}, i.e., the transformation of a pixel projected in the physical world to the pixel captured by the camera. In our case, an adversarial perturbation is displayed and perceived by the camera goes through a set of transformations. The color spectrum emitted by an electronic display may not match the color spectral sensitivity function of the camera~\cite{wengrowski2016optimal}. Prior work~\cite{lovisotto2020slap, wengrowski2019light} addressed this challenge by using a neural network to learn the color transformation. Moreover, because the transparent display is in close proximity to the victim camera, the displayed perturbation is blurred in the perceived image due to Bokeh effect~\cite{ignatov2020rendering}. The image formation pipeline processes within the camera convert the light received on the image sensor to a final digital image that may be affected by exposure, the sensor's sensitivity (ISO), and contrast transforms. It would be too complicated to model each of these transformations one by one, so we employ an end-to-end approach in our attack~\cite{wengrowski2019light}.



%%%Designing physical adversarial perturbations usually requires knowing the mapping between the digital and physical versions of the perturbation. Ignoring this mapping when designing an adversarial perturbation might result in a failed attack. In the case of projection-based physical adversarial attacks, the mapping is defined between a perturbation image generated on the computer and the corresponding projected image captured by the victim camera. No matter what object a digital perturbation is projected to, it will go through a set of transformations across the displaying and sensing pipelines~\cite{lovisotto2020slap, li2019adversarial}. Specifically, in our case, these transformations include: 1) characteristics of the transparent display device; 2) ambient light of the environment; 3) characteristics of the victim camera such as exposure, white balance etc. Existing projection based physical adversarial attacks deal with this problem in various ways. Li et al. \cite{li2019adversarial} propose a dot-based perturbation generation function and decompose the aforementioned mapping into individual dots. However, instead of derive the mapping directly, they only create a finite set of dots (finite number of colors and locations) and obtain the physical version of them. Then when generating a perturbation, they pick dots from this set in a heuristic way. The disadvantage of their approach is that


%%%\begin{itemize}
   %%% \item There is a high chance that the perturbation found by this approach is sub-optimal.
    %%%\item This approach does not scale to more difficult attack scenarios which require a lot more dots. 
%%%\end{itemize}
%%%Lovisotto et al. \cite{lovisotto2020slap} decompose the mapping between perturbation images into individual pixels, i.e., they try to find the mapping between a digital color and the color perceived by the victim camera. They use a neural network to model the mapping. The disadvantage of this approach is 
%%%\begin{itemize}
   %%% \item A different mapping needs to be constructed when the object changed.
    %%%\item The HUE (i.e. color spectrum) of the perturbation can be greatly compressed when shined on an object. This limits the power of the attack. 
    %%%\item The intensity of the perturbation can be greatly reduced when performing the attack in strong ambient light. In their paper, Their attack only success at a very low light condition environment. 
%%%\end{itemize}

% Begin comment 3/30
%%\noindent\textbf{Research Challenge \#1: Perturbations robust to optical transformations.} Directly applying an adversarial perturbation computed in the digital domain fails because it does not account for the mutations introduced by the camera-display transfer function (CDTF)~\cite{wengrowski2016optimal}, i.e., the transformation of a pixel projected in the physical world to the pixel captured by the camera. In our case, an adversarial perturbation is displayed and perceived by the camera goes through a set of transformations. The color spectrum emitted by an electronic display may not match the color spectral sensitivity function of the camera~\cite{wengrowski2016optimal}. Prior work~\cite{lovisotto2020slap, wengrowski2019light} addressed this challenge by using a neural network to learn the color transformation. Moreover, because the transparent display is in close proximity to the victim camera, the displayed perturbation is blurred in the perceived image due to Bokeh effect~\cite{ignatov2020rendering}. The image formation pipeline processes within the camera convert the light received on the image sensor to a final digital image that may be affected by exposure, the sensor's sensitivity (ISO), and contrast transforms. It would be too complicated to model each of these transformations one by one, so we employ an end-to-end approach in our attack~\cite{wengrowski2019light}.

%%The mutations introduced by the CDTF induce pixel-level transformations and, thus, small modulations to pixel values do not survive when displayed and perceived on an object. Prior dynamic attack frameworks~\cite{lovisotto2020slap}, which focus on object-level attacks, restrict the pixel-level granularity to a fixed grid for perturbations. In our case, such an approach would result in a very complex perturbation pattern since we would need to model the CDTF mutations in the optical channel and, thus, require significantly more data. Instead, we reduce the complexity of the problem by using highly structured patterns in the perturbation. In our attack, the adversarial perturbation only consists of a certain number of dots with various colors, significantly reducing the search space for optimization-based techniques.
%end comment 3/30

%Adversarial perturbations in the digital domain that alter the original image at pixel-level granularity. Small modulations to pixel values wouldn't survive when displayed and perceived on an object in the real world due to mutations introduced by the CDTF . If we follow the approach in prior work \cite{lovisotto2020slap}, i.e., restricting the granularity in a fixed grid, the perturbation pattern can be very complicated. This means it would require significantly more data to model the optical channel (a.k.a CDTF). 
%Instead, we take the approach of reducing the complexity of the problem by using highly structured patterns in the perturbation. In our attack, the adversarial perturbation only consists of certain number of dots with various colors.
%\eric{We need information about why dots simplify the problem and are expected to work better. How about ``reduce the search space for optimization-based techniques''?}

%\noindent\textbf{Optical transformations.} Directly applying an adversarial perturbation computed in the digital domain fail because it fails to account for the mutations introduced by the CDTF. When an adversarial perturbation is displayed and perceived by the camera, it goes through a set of transformations. \eric{we said this in the previous paragraph. can we combine the previous paragraph with this one, or reorder?} The color spectrum emitted by an electronic display may not match the color spectral sensitivity function of the camera~\cite{wengrowski2016optimal}. Prior work \cite{lovisotto2020slap, wengrowski2019light} addressed this by using a neural network to learn the color transformation. Because the transparent display is in close proximity to the victim camera, the displayed perturbation will be blurred in the perceived image due to Bokeh effect \cite{}. The image formation pipeline processes within the camera converts the light received on the APS-C sensor to a final digital image that may be effected by exposure, ISO, and contrast transforms. It would be too complicated to model each of these transformation one by one, so in our attack we employ an end-to-end approach~\cite{wengrowski2019light}. We randomly generate large number of our dot-like perturbations and collect the perceived perturbation for each of the displayed perturbation. We use these displayed and perceived perturbation pairs to train a neural network to learn all the transformations in a data-driven fashion.

%%%\noindent\textbf{Research Challenge \#2: Perturbations robust to environmental conditions.} Multiple factors must be considered to make an adversarial perturbation robust to the dynamic physical environmental conditions. For instance, the camera's perspective, distance, and rotation relative to a target object will impact the efficacy of a perturbation. The scene's environmental background and illuminance also play a vital role in object recognition. We propose an automated data collection framework that incorporates these factors during the perturbation training and generation process. This approach is analogous to data augmentation techniques employed by prior dynamic approaches~\cite{lovisotto2020slap}. However, in our case, we explicitly account for the impact of environmental factors to enhance situational awareness in downstream perturbation tasks. For instance, the training process may find that certain classes of adversarial examples only work for specific illuminance ranges. 

%\eric{do we have a figure or citation to clarify this paragraph?}

%%%\noindent\textbf{Research Challenge \#3: Efficient perturbations for real-time applications.} In addition to optical and physical mutations, Cataract must also take in the transient nature of real-time applications. The variance in an environment may be significant across multiple frames. Because generating a perturbation incurs latency in the attack pipeline, Cataract should employ attack perturbations that perform well across the environmental variance rather than optimize an attack for a single frame. Thus, Cataract adopts universal adversarial perturbation (UAP) models~\cite{moosavi2017universal} to generate perturbations that will be robust against variance across multiple time frames for a given scene. 

 

%%%\subsection{Proposed Approach for \namenospace}

%%%In this paper, we propose a more generalized projection based physical attack approach. We solve the aforementioned limitations of prior works in different components in our attack. 
%%%\begin{itemize}
    %%%\item \name addresses \textbf{Research Challenge \#1} by...
    %%%\item the adversarial perturbation is applied in close proximity to the camera sensor. In this way, the projected perturbation is much less affected by the object/scene. Therefore, there is no need to constructed a mapping for each different objects. This is discussed in detail in \autoref{sec:design}-B.
    %%%\item Also because our attack is applied in close proximity ot the camera sensor, it is less affected by the ambient light. In \autoref{sec:evaluation} we show that our attack can survive stronger light conditions.
    %%%\item To find a more optimal attack, we make our attack pipeline fully differentiable. In this way, we are able to use powerful gradient-based adversarial perturbation generation algorithms. We utilize a iterative gradient descent optimization to find adversarial perturbations. This is described in \autoref{sec:design}-C.
    %%%\item To make our attack pipeline fully differentiable, we directly construct a mapping between a digital perturbation image and a physical perturbation image. We approximate the mapping using a convolutional neural network. This is described in \autoref{sec:design}-B.  
    %%%\item Our attack can be easily scaled as each component can be automated, this includes collecting data to train the digital-to-physical mapping, optimizing for an adversarial perturbation.
%%%\end{itemize}
%%%\end{comment}%end comment 4/14/22
