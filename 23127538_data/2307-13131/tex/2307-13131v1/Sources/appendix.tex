\appendix
\label{sec:appendix}
\section{Ablation Study on \name}
We perform an ablation study to investigate the contributions of \namenospace's components toward the final attack performance. We first consider the effect of removing TNet in \namenospace/d, i.e., when optimizing for an adversarial perturbation, we use $I_d$ directly as $I_p$ without passing it through TNet. We also consider the effectiveness of randomly generated perturbations in \namenospace/r. For \namenospace/r, we randomly generate 20 perturbations to attack the signs and select the perturbation with the highest ASR. Finally, we study the contribution from considering the variation of environmental factors. For this study, we fix all the factors, e.g., background, perspective, rotation, distance, and the light conditions, when preparing data to optimize a perturbation model using \namenospace/s. We report the results in Figure~\ref{fig:ablation}. For all three models, the comprehensive \name achieves significantly better results. Random perturbations perform the worst among all the variants except for ResNet-50. This can again be explained by the fact that classifiers are in general easier to attack when compared to object detectors~\cite{chen2018shapeshifter}. Some random guesses might fall inside vulnerable regions of the model's loss surface. However, the attack performance is still far from the performance of our approach. For \namenospace/s, not considering the environmental factors when crafting the perturbation results in highly sensitive perturbations with unstable attack performance. This is more obvious when the ambient light is stronger -- above $1500$ lux the perturbations have no effect.

% Figure environment removed


\section{Evaluation Against Defense: Feature Squeezing}
In this section, we study the effectiveness of a defense solution feature squeezing against our attack. We evaluate the effectiveness of bit-depth reduction on our adversarial perturbations by comparing its impact on classifier accuracy for benign and adversarial samples. The results are shown in Figure~\ref{fig:feature-sq}.

As the input image bit depth decreases, both benign and adversarial classifier accuracy remains relatively stable until the bit depth drops below 2 bits/channel. At this point, we observe a general increase in adversarial accuracy.
We hypothesize that this is due to the dot perturbation's color robustness -- each dot maintains a constant color gradually fading outwards. As a result, decreasing the bit-depth only slightly attenuates the perturbation until the final step, in which each channel only has 2 possible values, more significantly affecting the perturbation.

% Figure environment removed

\section{Measured Illuminance}
Measured illuminance of various ambient environments can be found in Tabel~\ref{tab:light}. \name can support much stronger ambient light compared to prior works.

\begin{table}[h]
\centering
\caption{The measured illuminance of various ambient environments~\cite{eun2016bright} as well as how \name and prior work were performed under each ambient light level. \Tdot~ indicates high ASR, \LEFTcircle~ indicates low to medium ASR, and \Twdot~ indicates not working at all. \name supports much stronger ambient light.}
\begin{tabular}{r|ccc}
\toprule
% \tabularnewline
Ambient Environment              & \multicolumn{1}{l}{Illuminance} & \name & SLAP \\ \hline
Indoor w/ Ceiling Lights       & 50 lux                          &    \Tdot      &   \Tdot   \\
Before Sunset & 120 lux                         &   \Tdot       &     \Tdot \\
Overcast Afternoon                      & 400-600 lux                                &    \Tdot      &   \LEFTcircle   \\
Midday, Overcast          & 1k-2k lux                   &    \Tdot      &   \Twdot   \\
Midday, Clear              &           40k-100k lux                      &    \LEFTcircle      &  \Twdot    \\ 
\bottomrule
\end{tabular}
\label{tab:light}
\end{table}