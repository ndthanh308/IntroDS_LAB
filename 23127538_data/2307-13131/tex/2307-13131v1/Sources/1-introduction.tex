\section{Introduction}\label{sec:intro}
% Prevalence of Deep learning and subsequent attacks

%First paragraph:Intro to DL
Safety-critical autonomous \yi{would the word autonomous gives the reviewers the impression that this is about autonomous vehicles?} systems commonly rely on optical sensors coupled with artificial intelligence to mimic human perception across a variety of domains, including autonomous navigation~\cite{pendleton2017perception}, crowd surveillance and analysis~\cite{sreenu2019intelligent}, and facial recognition for authentication~\cite{mehdipour2016comprehensive}. %Humans can interpret images with ease; however replicating this capability on autonomous systems using artificial intelligence algorithms is a rigorous challenge with a vast body of existing research~\cite{}. Recently, these algorithms have made their way into many industrial and consumer-facing applications. A prime example of this is the automotive industry, which has been increasingly adopting a camera-first paradigm \cite{} to replace more complex or expensive LIDAR and ultrasonic sensor approaches~\cite{takacs2018highly}. Deep learning especially has been established as a popular class of algorithms for computer vision tasks~\cite{}. The ability to simultaneously learn latent features and classifiers through optimization in the training process has made feed-forward deep learning an attractive option when sufficient training data exists.
%But there are risks associated with relying on artificial intelligence for safety-critical systems.
Although the associated deep learning algorithms enable promising performance across domains~\cite{goodfellow2016deep}, they introduce vulnerabilities that are non-obvious and can be targeted by adversaries. %\eric{1. why are the associated ML algorithms powerful? can we use a more precise term than ``powerful''? 2. citations needed for ``powerful algorithms'', ``difficult to understand vulnerabilities'', and ``can be targeted by adversaries''.}
There have already been real-world instances where perceptual models have failed, resulting in fatalities~\cite{banks2018driver}.
Researchers have shown various autonomous sensors to be exploitable~\cite{yan2016can}, even production vehicles such as Tesla cars can be compromised~\cite{deng2020analysis}. %\eric{1. ``a Tesla automobile'' vs. ``Tesla automobiles''. 2. anything can be targeted. should read ``successfully attacked'' or something similar}


%%%%%%%%%% this is the original figure one%%%%%%%%%%
% % Figure environment removed

\begin{comment}
    Safety-critical autonomous systems are becoming reliant on optical sensors to mimic human perception tasks~\cite{}. Although humans interpret imagery with ease, this is challenging for autonomous systems~\cite{}, and powerful new classes of artificial intelligence algorithms have been developed and employed to address this problem. Deep learning has been established as a popular class of algorithms for computer vision tasks~\cite{}. The ability to simultaneously learn latent features and classifiers though optimization in the training process has made feed-forward deep learning an attractive option when sufficient training data exists~\cite{}. Recently, these algorithms have made their way into many industrial and consumer-facing applications~\cite{}. The automotive industry is increasingly adopting camera-first approaches to replace the expensive LIDAR and ultrasonic approaches~\cite{takacs2018highly}. But there are risks associated with relying on artificial intelligence for safety-critical systems~\cite{}. There have already been instances where these perceptual models have failed and resulted in fatalities~\cite{banks2018driver}. These vulnerabilities have been targeted by attackers-- a Tesla car was targeted, and various autonomous sensors were shown to be exploitable~\cite{yan2016can}.
\end{comment}


\begin{comment}
\begin{table}[b]
\begin{center}
\begin{tabular}{| l|c|c| } 
\cline{1-3}
            & \textit{object-level attacks}  & \textit{sensor pipeline attacks} \\ 
 \hline            
 \textit{static}     & %Robust Physical Attacks~
 \cite{eykholt2018robust,chen2018shapeshifter,athalye2018synthesizing,thys2019fooling,zhao2019seeing,wu2020making} & %Adversarial stickers~
 \cite{li2019adversarial} \\ 
 \hline
 \textit{dynamic}    & %Dynamic Adversarial Patch ~
 \cite{hoory2020dynamic,lovisotto2020slap,nichols2018projecting,nassi2020phantom}    & \textbf{\name} \\ 
 \hline
\end{tabular}
\end{center}
\caption{This table summarizes the different classes of physical-domain Adversarial Machine Learning (AML) attacks along with recent examples of each attack. In this paper, we propose to formalize the emerging domain of physical-domain AML attacks. We also propose a new method, combining dynamic and sensor pipeline-based attacks.%\eric{is this table still accurate?}\luis{I updated it to include most related work citations}
}
\label{tab:attack-cats}
\end{table}
\end{comment}


%

%Second paragraph: how realistic are these attacks?
%With such interconnected cyber-physical systems, there are a wide variety of threats and threat models to consider even within autonomous vision systems.
Much of the initial research on adversarial machine learning~\cite{huang2011adversarial} has focused on digital-domain software attacks, a holdover from more thoroughly-explored computer vision research.
More recent work has explored the feasibility of adversarial machine learning attacks on sensors in the physical domain, where dynamic environmental conditions make precision attacks more difficult to execute~\cite{eykholt2018robust,chen2018shapeshifter,athalye2018synthesizing,thys2019fooling,zhao2019seeing,wu2020making}.
%\matt{todo: rewrite following paragraph to fit new table 1. Additionally, table 1 is described later on as well in S2B.}
%Existing physical-domain approaches can be broadly classified along two axes, as exemplified in Table~\ref{tab:relatedWorks}.
%On one axis, attacks can range in their proximity to the digital domain: either augmenting perceived objects themselves to attack autonomous vision systems, or augmenting the sensor pipeline just before an image is fed to the software. On the other axis, attacks can be static or dynamic in nature, being either fixed and context independent, or adaptive using contextual and environmental information.
%As shown in Table~\ref{tab:relatedWorks}, we categorize physical domain attacks based on the location of the perturbation, i.e., object-level modification (OM) versus sensor-level modification (SM), as well as the dynamicity of the attack, i.e., static attacks (SA) calculated ahead of time versus dynamic attacks (DA) that can change at runtime. Additionally, we distinguish online attacks (OA) that generate dynamic perturbations based on the current context of the attack. 

The development of robust, physical-domain attacks has trended from static, object-level attacks~\cite{eykholt2018robust,chen2018shapeshifter,athalye2018synthesizing,thys2019fooling,zhao2019seeing,wu2020making, jia2022fooling, man2020ghostimage, wang2021daedalus, wang2021can} to dynamic, object-level attacks~\cite{hoory2020dynamic,lovisotto2020slap,nichols2018projecting,nassi2020phantom} for misclassifying specific objects for non-static domains, e.g., autonomous vehicles detecting objects in various environments. However, object-level approaches hinge on augmented objects entering the target perception pipeline, e.g., an autonomous vehicle passing by a maliciously modified stop sign. Moreover, to attack multiple objects, each object needs to be modified individually. To overcome the aforementioned problems, recent attacks~\cite{ji2021poltergeist, kohler2021they, man2020ghostimage, wang2021can, li2019adversarial, zolfi2021translucent} have started exploring sensor-modifying attacks. These attacks typically exploit vulnerable sensors (e.g. cameras, CMOS sensors or inertial sensors) in the perception pipeline of the victim system. Most relevant to this paper, in~\cite{li2019adversarial, zolfi2021translucent} showed that stickers with static, adversarial perturbations could be placed on the lens of a target camera. However, such an approach cannot adapt to various scenes and environmental conditions due to the static nature of the stickers. %\luis{I modified the previous paragraph significantly. I tried to sumarize the limitations for each cell in Table~\ref{tab:attack-cats}}%Although existing attacks show how vulnerable autonomous vision systems can be with impressive results, these attacks have limitations. \eric{citation?} Attacks that augment specific target objects cannot easily scale to multiple target objects. \eric{will it be clear to the reader why?} Also, it is more difficult for adversarial perturbations applied on the objects to survive strong light conditions. \eric{this sentence comes way out of left field, and no citation! Context is needed for a bold claim like this!} Static attacks have the disadvantage that once a perturbation is generated and applied, it cannot be changed. \eric{citation??} The aforementioned limitations significantly reduce the threats of existing attack approaches in real-world scenarios.



\begin{comment}
    What is the risk associated with these attacks? The latter example resides in the domain of adversarial machine learning (AML) attacks. Initial research focused on the efficacy of AML attacks only in the digital domain~\cite{}, which may be secured through traditional means. Recent work has explored the feasibility of AML attacks in the physical domain, where dynamic and complex circumstances make precise attacks more difficult~\cite{}. The physical-domain approaches can be broadly categorized as \textit{static} or \textit{dynamic} attacks, and orthogonally attacks that target at an \textit{object-level} or attacks that target the \textit{autonomous sensor pipeline}. Table~\ref{tab:attack-cats} summarizes the different classes and corresponding examples of physical AML attacks. While \textit{dynamic} attacks may change based on the underlying system state, \textit{static} attacks do not. \textit{Object-level} attacks are defined as modifications to one or more specific physical objects in an environment. Attacks on \textit{autonomous sensor pipelines} target the systems of sensors and autonomous algorithms designed to perform and react to state estimations of their environment. 
\end{comment}
%\eric{These definitions are a bit wordy. Any suggestions on how to simplify?}




% In this paper, we aim to generate more intelligent model to attack the sensor pipeline
In this paper, we fill the gap by proposing an intelligent model, \namenospace, to dynamically attack the sensor pipeline. \name utilizes a portable transparent display in front of an optical sensor to display a carefully crafted adversarial perturbation to alter the perception of the sensor. 
% -- depicted in Figure~\ref{fig:attack-overview}
An auxiliary sensor is utilized to detect when a target object, e.g., a ``Stop" sign, enters the victim camera's field of view, which dynamically informs the generation of an adversarial perturbation. To craft adversarial perturbations, \name first models the optical sensor pipeline, i.e., the optical path of a digitally generated adversarial perturbation from the transparent display to the optical sensor, using a feed-forward neural network. 
%\eric{this is an important step worth emphasizing. why can't I just generate a digital perturbation without modeling the optical sensor pipeline? because our attack has to account for the spectral sensitivity of the camera sensor, the spectral emmitance of the transparent display, and the radiometric properties of light in free space~\cite{wengrowski2016optimal,wengrowski2017reading}. Rather than modeling each of these components individually, a camera-display transfer function (CDTF) jointly models these component systems. Following Wengrowski et al., a trained neural network models the CDTF~\cite{wengrowski2019light}.} 
The sensor pipeline model enables \name to simulate physical adversarial attacks by implicitly accounting for the spectral sensitivity of the camera sensor, the spectral emittance of the transparent display, and the radiometric properties of light in free space~\cite{wengrowski2016optimal,wengrowski2017reading}. \name then utilizes a gradient-based iterative approach to generate adversarial perturbations. To make sure adversarial perturbations can survive dynamic conditions in a physical environment, \name simulates various environmental factors (e.g., perspective, distance, and illuminance) when searching for an adversarial perturbation. 

\name has the following advantages over prior attack frameworks: 1) our attack does not require knowledge of when and where a target object will enter the periphery of a target sensor since perturbation decisions can be made at runtime, whereas prior works require a target sensor to follow a particular path; 
2) our attack is more flexible than prior approaches because the adversarial perturbations can be adjusted dynamically (e.g., targeting different traffic signs for misclassification). According to our extensive empirical evaluations, this results in better attack performance; and 3) our attack is robust across various environmental lighting conditions.
%\matt{double-check the following sentence}
%This is an important factor for any light-based attack, as additional ambient light negatively affects the relative attack strength.
%Due to our perturbation's close physical proximity to the optical sensor, it is less affected by environmental light. %\eric{need to first explain why environmental light conditions matters. could briefly describe the threat model from a sensor perspective, or cite state-of-the-art work where environmental lighting has a significant effect} \eric{is \name a model or an attack? currently using both in this paragraph.} 
%\matt{todo: no more face, right? (possible discussion point?)}
We empirically evaluate \name in the traffic sign recognition domain. %traffic sign classification and face verification. 
We will make our code open source for reproduction and future research.

% We aim to decompose the various components of the adversarial sample generation pipeline to model the physical sensor pipeline separately from the training data distribution.

% We further aim to show this in real-time
%Fourth Paragraph: real-time attacks
% Because the physical world is chaotic, we similarly show how to make this system robust to changes in the physical domain.
% \begin{itemize}
% \item{Here we need to distinguish ourselves from the real-time adversarial attack paper} 
% \item{We first start with static objects (targets) and move on to dynamic objects}
% \end{itemize}

\noindent\textbf{Contributions:} The contributions set forth in the paper are summarized as follows:
\begin{itemize}
  \item We introduce a new class of dynamic, sensor-first  machine learning attacks in the physical domain. %\eric{will readers know what ``AML'' means?}
  \item We formalize the contextualization of adversarial examples in the optical sensor pipeline. %\luis{The SLAP paper does this; need to position contrib. better. Something like, "Contextualization of adversarial examples in the optical sensor pipeline."}
  \item We develop and validate a novel, compact device designed to attack the optics of a camera system.
  \item We extensively evaluate the efficacy of our sensor-first attack on a traffic sign recognition task across various levels of illumination, significantly outperforming existing approaches.%for safety-critical autonomous driving, as well as facial recognition for authentication. % use-cases.%\luis{SLAP paper also does this. Something along of the lines of autonomous driving car driving through a city on a non-deterministic path with diverse objects/targets. More generally, evaluate on applications where adversary may have changing target objects.}
%   \item Introduction of the new \textbf{NAMED} dataset of $N$ images used to train our proposed attacks.
%   \item Public release of all code used to run the experiments presented in this paper. \luis{Latter two contributions are not contributions, but we can add them as sentences after.}
\end{itemize}
%\luis{If there's space, we may want to add a line saying that we'll open source everything.}

% Comparison to SLAP:
% \begin{itemize}
%     \item Their attack design, especially their projection model (color mapping) is constructed specifically for stop signs. It remains unclear weather this can be applied to other traffic signs, not to mention other smart camera applications such as face recognition.
%     \item SLAP only tested illumination conditions up to 600 lux, this is far from the average lux of a sunday day (120000 lux). Our display gadget can survive much brighter light condition than theirs.
%     \item ...
% \end{itemize}


% Physically more efficient attack (route a car to a different location requires multiple misclassification, but our sensor based attack do not have to put stickers on multiple stop signs).


%https://github.com/lionelmessi6410/awesome-real-world-adversarial-examples; "how many systems are using optical sensors for autonomous decision making"; stop sign problem--> because there is a human in the loop, we have to make it inconspicuous;  we can minimize the attack; if we want to misclassify a dog, we project a dog on the glass; how are we are using these perceptual metric--> the perceptual metric is based on pretrained nueral network for object classification; it's trying to do feature mathing; it should be similar; as we take an adversarial approach, we're using this human detection network; is this going to be fighting against; we should take perceptual loss coefficient and put down to zero and then increase it; "If you wanted to attack a car that is driving through a neighborhood, you can do it by defacing every stop sign; with every stop sign, you're increasing the probability of detection; if you attack the sensor directly, then you limit--> this is a very physically efficient attack; "We're the first to do real-time attacks; this is the first iteration proof-of-concept; but we should know what is the tradeoff; what is the threshold of success? You may need a very accurate adversarial algorithm"--> we can make an argument for speed;; a car is going to be traveling at 70 mph; it will be traveling by a stationary object; if it's sampling at 2 samples/second, it will get 12 samples of this stationary object; how many should be misclassification; There is a threshold we need to 
%Define what are teh metrics of success--> come up with math problem that shows how multiple frames would be processed; "let's have a graph; one axis is the advesrary algorithm and the other axis is the speed; we should show a window or danger zone; maybe phiar


% Notes 10/16
% - Discuss some notion of human in the loop
% - Our method is only designed to be semi-visible to a person
% - It leaves us exposed if we're not doing experiments on Tesla
% - Camera sensors are embedded vision sensors (Intel)
% - We've created our own analog to the vision sensors that exist
% - Human in the loop isn't seeing the same feed
% - humans want to look back and see what happened
% - ring doorbell example; camera that is using some AI decisions; there is a human in the loop
% - advantage: we can handle more classes and more poses
