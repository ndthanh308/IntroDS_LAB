\section{Limitations and Future Work}\label{sec:discussion}
In this section, we discuss the limitations of \name and enumerate future research directions toward bridging the gap between digital domain- and physical domain-based adversarial machine learning.

\noindent\textbf{Practicality of transparent display implementation.} The prototype implementation of \namenospace's transparent provided a low-cost and portable implementation to enable proof-of-concept research for sensor-level, dynamic adversarial attacks. Emerging technologies such as the Microsoft HoloLens~\cite{noor2016hololens} or the Google Glass~\cite{muensterer2014google} have shown the industry trend to develop compact and computationally efficient implementations of transparent displays. Future work could also incorporate advancements in attenuating the impact of ambient light on perception, especially in the context of outdoor displays~\cite{lanca2019effects,eun2016bright}. Thus, the emergence of such technologies would enable the adversaries to implement \name in real-world settings at scale. Also, Itoh et. al \cite{itoh2021towards} provide a comprehensive overview of optical see-through head-mounted displays, comparing designs that tradeoff between various factors such as form factor, size, and light efficiency. Future work can explore a more optimal design across these tradeoffs. 

% work can explore how state-of-the-art AR technology can be used to overcome the challenges enumerated in this work. 
%\luis{TODO: Add discussion on light attenuation filter (e.g., ``sunglasses" effect.), if we do not do any experiments. I am not adding it for now in case. But we can cite this paper (from Nils) as necessary~\cite{lanca2019effects}}
%\luis{Update high illuminance value in paragraph below.}
%%\noindent\textbf{Low-light limitations.} Our results showed that \name can only maintain high ASR for illuminance values up to about 1500 lux. \namenospace's limitation for high illuminance is mainly due to a limitation in the form factor of the transparent display. First, a low-cost form factor was used to realize \namenospace. Future work can explore engineering more robust solutions using state-of-the-art AR technology to employ transparent displays that better account for outdoor illuminance\footnote{The problem of the brightness of displays in outdoor environments is not unique to AR as outdoor commercial display technology also needs to account for high illuminance factors~\cite{eun2016bright}.} Despite the engineering limitations, our results out-performed the state-of-the-art while increasing the dynamicity of attacks.


%\luis{Discuss how real-time attacks such as ~\cite{gong2019real} are not necessary, but discuss how future work can explore more dynamic attacks can be crafted for systems that reason across samples/frames, etc.}

%\luis{In our experiment with maps, we should be sure to say that we are maximizing the amount of opportunities in which the vehicle would crash. The misclassifications wouldn't cause a re-route of the path, and may not necessarily cause an accident for each individual misclassification.}

%\luis{Discuss elastic pathing for predicting trajectories of people walking into a frame~\cite{gao2014elastic}}
%%\noindent\textbf{Feasibility of real-time attacks.} As we discussed, UAP models provide robustness to the environmental variance across multiple frames, they are not as accurate as adversarial examples crafted for a single frame. Generating custom perturbations for every frame prior to the victim sensor perceiving the target object is essentially physically impossible. However, recent works have shown that real-time adversarial examples can be generated if the input signal can be estimated accurately~\cite{gong2019real}. More generally, future work can explore the feasibility of domain-specific, real-time man-in-the-middle perception attacks with adversarial optics. For instance, in the context of security cameras, elastic pathing~\cite{gao2014elastic} can be used to predict trajectories of humans walking in a scene and, thus, to generate adversarial examples for future video frames. Similarly, we have discussed how auxiliary information such as open street maps~\cite{vargas2020openstreetmap} can be used to predict trajectories of objects relative to a victim sensor. %Moreover,  
%\luis{Compare latency of attacks using object detection versus non-}

\noindent\textbf{Attack imperceptibility for autonomous systems.} A common notion in adversarial examples research is to ensure perturbations are \textit{imperceptible} to humans in the perception loop. For instance, adversarial examples for spam detectors aimed to bypass machine learning classifiers while not looking suspicious to the target human~\cite{tygar2011adversarial}. For autonomous systems, humans are not expected to be in the loop, i.e., they are not expected to monitor the video camera feed that is being fed into the perception pipeline. Even if a human was monitoring the video feed, the perturbations and objects would most likely be fleeting. Thus, imperceptibility in the context of cyber-physical autonomous systems may target humans who are performing post-incident analyses to investigate likely causes for a malfunction. However, certain autonomous systems domains and applications still require imperceptibility of perturbations, e.g., automatic speech recognition attacks on smart home assistants should be imperceptible to humans who are in the same room~\cite{qin2019imperceptible}. Future work can investigate the need for perceptibility across autonomous systems to understand to \textit{whom} and \textit{when} perturbations need to be imperceptible.   

\noindent\textbf{Real-world considerations.} Several challenges arise when applying \name to the real-world. First, the impact of the additional display on lighting, real-world impact, such as additional reflections need to be considered. \name employs the digital-to-physical mapping to deal with these factors. Moreover, the data used for generating the perturbations and the actual video feed frames at runtime can be different, known as the distributional drift problem. One solution to this can be obtaining more comprehensive training dataset. Also, in some scenarios accessing the victim camera can be challenging for \name. But it remains a big threat to those systems where camera sensors are accessible (e.g. surveillance, authentication, etc.). Also setting changes on the victim system, such as hardware revisions, and software updates can degrade the accuracy of \name. In this case, iterations of the attack will be needed. Currently, \name generates perturbations that can only work on a specific camera. it can be beneficial to make the attack transferable across different cameras in future work. Finally, more complex perception systems employ more complicated sensor arrays (e.g., multi-camera, multi-modal) as the perceptual module. Future work can explore camera pipeline attacks on such systems. 
% \noindent\textbf{Additional Mitigations against \namenospace.} In addition to our evaluation of existing mitigations in Section~\ref{sec:evaluation}, we propose additional mitigation techniques. Because the defenses are trained with digital perturbation, future approaches should emphasize the digital-to-physical mapping for defenses. Beyond training robustness, case-based solutions could provide challenge-response attestation~\cite{shoukry2015pycra} to attest the classification pipeline, e.g., attesting the results for a known scene. However, future research would need to handle the cases where attackers are aware of such defense mechanisms.


%\noindent\textbf{Generalizability across domains and modalities.} %We demonstrated that our framework works robustly across two very different domains: autonomous vehicle scenarios and facial recognition for authentication. 
%Future work can explore attacking perception pipelines across different camera models and different sensing modalities for safety-critical autonomous systems, e.g., attacking microphone perception pipelines that are used to mitigate urban noise pollution~\cite{bello2019sonyc} or facial recognition for authentication. The \name framework is a practical solution for real-world perception attacks in dynamic environments.
%\noindent\textbf{Blurring the line between physical and digital adversarial examples.} % possibly backpropagate this point to the intro
%luis{TODO: remove if we don't have facial recognition results}
%\luis{For obstacle detection, Baidu fuses Lidar and camera information. Thus, reducing the objectness may not be as feasible in real-world systems. However, Tesla is camera-only.}