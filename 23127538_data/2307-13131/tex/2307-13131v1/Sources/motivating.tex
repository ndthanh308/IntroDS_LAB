\section{Motivating Example}
\label{sec:motivating}


As is shown in \autoref{fig:setup-diagram}, an autonomous driving vehicle is equipped with a camera sensor. The camera sensor continuously precepts the scene in front the vehicle. The video frames percepted by the camera sensor are sent to a traffic sign recognition module. The outputs of the recognition module controls what actions the vehicle needs to take (e.g., stop in front of a stop sign or slow down when a 55 speed limit sign is observed).

An adversary intended to attack this vehicle prepares a transparent displaying gadget (\autoref{fig:trans-display}) and install the gadget in front of the camera sensor. The adversary can remotely control the gadget to display adversarial perturbations. In this way, the normal operation of the vehicle is disturbed and possibly leads to catastrophic consequences such as crash.

Based on different level of knowledge the adversary has of the camera under attack, he needs to take different actions and the effectiveness of the attack also varies. Under a White box scenario, where the adversary has full knowledge about the recognition module, i.e., he has access to the architecture and weights of the model as well as the gradient information during the inference of a frame, he needs to prepare various pictures of the traffic sign to be attacked, and generate a universal adversarial perturbation (UAP) based these images. Under a black box setting, the adversary needs to utilize the transferability of adversarial examples, i.e., he needs to craft the UAP based on a different recognition model, and apply the UAP to the camera under attack. He can use a publically available traffic sign dataset to train a widely used image recognition model. He then crafts UAPs based on this model.

The attacker can dynamically change the displayed perturbation. This gives him the advantage of ...