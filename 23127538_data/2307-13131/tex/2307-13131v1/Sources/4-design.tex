

%\section{Dynamic Man-in-the-middle Attacks on Camera-based Autonomy}\label{sec:design}


% Figure environment removed

%\luis{We should reframe this as the section on how to incorporate the optical channel into the perturbation calculations}
%\luis{Before the implementation section, we can have a second design section talking about how to incorporate this into a eistic setting. Maybe that IS implementation. We'll see how the paper evolves.}
%\luis{Maybe have two use cases for tradeoffs for a real system in the second system}
%\luis{One thing to formalize: getting the information to know what perturbation to generate; elastic pathing from Janne; How to queue up an attack\url{https://dl.acm.org/doi/abs/10.1145/2632048.2632077}}
% We need to get some kind of confidence to predict what will be in the scene; our system is depending on the predicitons of this model, our systme has to be robust enough that it will still work on predictions from this model within a certain confidence interval 


% Components of the attack model:
% \begin{itemize}
%     \item Target Image: $I$
%     \item Digitally generated perturbation pattern: $p$
%     \item Projection transformation due to projecting angle: $T(\cdot)$
%     \item Transformation of the pattern on the glass to the pattern being captured by the camera (in AdvStickers being the alpha blending model): $\alpha(\cdot)$
% \end{itemize}
% Final digital image sent to the classifier: 
% \begin{equation}
%     \alpha(I+T(p))
% \end{equation}

% Assume we have a image $x$. A classification $f$ takes the image as input, and output which class $y$ the image belongs to. The true label of $x$ is $y*$ while the target label is $y_{targ}$. We aim to create a perturbation $\delta$ such that $f(x+\delta) = y_{targ}$. The misprediction objective is usually defined as 
% \begin{equation}
%     \max_{\delta}\;l(x+\delta, y*)-l(x+\delta, y_{targ})
% \end{equation}
% where $l(\cdot)$ is the loss function of the classification model.

%(assuming we already introduced on a high level how an camera sensor on an autonomous vehicle works, we only give mathematical definition of some components if necessary)

%In this section, we describe the design of our proposed attack framework. Figure~\ref{fig:training} depicts an overview of the \name framework. We first describe how physical perturbations are modeled from digital perturbations with a digital-to-physical mapping. Then we describe how we optimize for adversarial perturbations using this mapping.


\subsection{Modeling Digital-to-Physical Perturbations}\label{sec:modeling}

% The normal operation of a recognition model can be described as 
% \begin{equation}
%     y = F(x)
% \end{equation}
% where $x$ is a frame of the video stream perceived by the victim camera. $y$ is the true label of the perceived object. $F(\cdot)$ is the recognition model. 
Under the additive adversarial perturbation model, the effect of a physical adversarial perturbation can be abstracted as 
\begin{equation}
    y' = F(x+I_p),\quad y\neq y'
\end{equation}
where $x$ is a frame of the video stream perceived by the victim camera. $I_p$ is a physical adversarial perturbation. $F(\cdot)$ is the recognition model. $y$ is the true label of the perceived object. $y'$ is an intentionally misclassified label. $I_p$ can be produced in different ways such as printed on a sticker and attached to the object \cite{eykholt2018robust}, printed on a transparent paper and attached to the camera sensor \cite{li2019adversarial} or projected onto the object using a projector \cite{lovisotto2020slap}. In our attack, $I_p$ is presented on a transparent display and attached to the camera sensor. The camera sensor can see through the display.

Applying adversarial perturbation computed in the digital domain fails due to the mutations/transformations introduced by the CDTF~\cite{wengrowski2016optimal}. %camera-display transfer function (CDTF) \cite{wengrowski2017reading}. 
Such mutations are caused by the characteristics of the displaying and camera sensing pipeline. On the transparent display side, the optics of the holographic combiner and projection source such as color spectrum compression and shifting, distortion will already produce an image different from the source image. On the sensor side, the optics of the camera, such as focal length, lens distortion, exposure, and white balance also affect the final perceived image. Therefore, in order to produce effective adversarial perturbations in the physical domain, it is necessary to understand and consider these mutations in the design loop. There is prior effort attempting to model these mutations. Lovisotto et al.~\cite{lovisotto2020slap} used a multi-layer perceptron (MLP) to model the color spectrum mapping between intended and perceived colors given the color of the projection surface. while this mapping model works fine for their attack purpose, it requires discretizing the color space to reduce complexity, which could lead to imprecision. Besides, to achieve a more precise digital-to-physical mapping and thus a more powerful attack, other mutating factors mentioned above also need to be considered. In this paper, we propose establishing a digital-to-physical mapping that takes all these mutations into consideration. 

% To model these mutations, we propose establishing a mapping between an adversarial perturbation computed in the digital domain and its corresponding physical perturbation.

\noindent\textbf{Digital-to-physical mapping.} Given a pair of digital and physical perturbation image $I_d$ and $I_p$, we wish to find a mapping $T(\cdot)$ such that 
\begin{equation}
    I_p = T_{\Theta, D, C, E}(I_d)
\end{equation}
where $\Theta$ represents the domain-constraint parameters of the perturbation generation (e.g., the size, shape, and color constraints of a perturbation), $D$ represents the transparent display, $C$ denotes the camera, and $E$ denotes  environmental parameters (e.g., ambient light). It would be too complicated to model each component, i.e., aforementioned mutations, individually. Therefore, we take a data-driven approach. We employ a convolutional neural network to approximate $T(\cdot)$.


% take care of the display $D$ and the camera $C$ (denoted as $T_{D, C}(\cdot)$) and we take care of the environment $E$ on top of it (denoted as $T_E(\cdot)$)
% \begin{equation}
%     I_p = T_E(T_{D, C}(I_d)).
%     \label{eq:mapping}
% \end{equation}
% % Since $T(\cdot)$'s job is to approximates the mutations/transformations introduced by the CDTF, we call it TNet. 
% 


\noindent\textbf{Fitting the model.} $T(\cdot)$ is fitted with pairs of digital and physical perturbations. We generate a dataset of $I_d$ and $I_p$ pairs as the training set. For each pair, we first randomly generate and display $I_d$ on the transparent display. We then take two images, one with $I_d$ turned on (mutated image) and one with $I_d$ turned off (the background). We derive $I_p$ by subtracting the background from the mutated image. When we collect data for fitting $T(\cdot)$, no object is present in the scene. 

As for generating $I_d$, we use highly structured patterns consisting of dots with different colors. Extending the idea of using small dots to create adversarial perturbations in \cite{li2019adversarial}, we define a dot-based adversarial perturbation generation function $I_{d, \theta}$. The parameters contained in $\theta$ describe the dots, and are defined as 1) $c(i^{(k)}, j^{(k)})$ - center coordinates of the $k$th dot; 2) $r_k$ - radius of the $k$th dot; 3) $\gamma_k$ - RGB color of the $k$th dot; 4) $\alpha_{\textnormal{max}}$ - maximum alpha blending value; 5) $\beta$ - exponential dropoff of alpha value; 6) $n$ - maximum number of dots.
% \begin{itemize}
%     \item $c(i^{(k)}, j^{(k)})$ - center coordinates of the $k$th dot
%     \item $r_k$ - radius of the $k$th dot
%     \item $\gamma_k$ - RGB color of the $k$th dot
%     \item $\alpha_{\textnormal{max}}$ - maximum alpha blending value
%     \item $\beta$ - exponential dropoff of alpha value
%     \item $n$ - maximum number of dots.
% \end{itemize}
$I_d$ can be computed as
\begin{equation}
    I_d(i,j) = \sum_{k=1}^{n}\alpha_{\textnormal{max}}e^{-\beta \cdot d_k}\cdot\gamma_k
\end{equation}
where $d_k(i,j)$ is the distance of the pixel $(i,j)$ with respect to the center of the $k$th dot
\begin{equation}
    d_k(i,j) = \frac{(i-i^{(k)})^2+(j-j^{(k)})^2}{r_k^2}. \\
\end{equation}
$I_d$ superimposes $n$ dots together to create a pattern with various colors and shapes. \autoref{fig:p_digital} shows the example of a digital perturbation. The number of dots in an adversarial perturbation affects its capacity; an adversarial perturbation with fewer dots is less powerful than one with more dots. A perturbation with more dots can create more complex patterns and, thus, can alter the frames and the prediction of the target recognition model more easily. The advantages of using such highly structured patterns are 1) small modulations to pixel values, as in pixel level adversarial perturbations, do not survive when displayed and perceived on an object; 2) It would require significantly more data in the training set in order to ensure generalizability, i.e., to enable $T(\cdot)$ to output physical perturbations based on unseen digital perturbations.



To fit $T(\cdot)$, we want the output of $T(\cdot)$ to be as similar as possible to the ground truth physical perturbation $I_p$. To achieve this goal, we combine both mean square error (MSE) and Learned Perceptual Image Patch Similarity (LPIPS)--a metric optimized for perceptual loss \cite{zhang2018perceptual}, to form the loss function $l$
\begin{equation}
    l = (1 - a) * \textnormal{MSE}(T(I_d), I_p) + a * \textnormal{LPIPS}(T(I_d), I_p)
    \label{eq:loss}
\end{equation}
where $a$ controls the ratio of the two terms. The MSE term encourages the colors to be as close as possible, the LPIPS loss term emphasizes maintaining the structural patterns. 

% \noindent\textbf{Ambient Light.} During our investigation we observed that the visual strength of a physical perturbation is affected by the ambient light. In a stronger ambient light, the physical perturbation will look dimmer and vice versa. Collecting training pairs for various light conditions to training the digital-to-physical mapping would require significantly more data. Therefore, we break the mapping into two steps (\autoref{eq:mapping}). We first exclude ambient light $E$ from the problem and build $T_{D, C}(\cdot)$. Then we adjust $T_{D,C}(I_d)$ to match the desired ambient light when generating the perturbations. Specifically, we adjust the brightness of $T_{D,C}(I_d)$ to fit the desired ambient light before applying it to the frames. A brightness adjustment operation is defined as 
% \begin{equation}
%     im_2  = ratio * im_1,~ratio > 0
% \end{equation}
% where $im_1$ is the image of which brightness need to be adjusted, and $im_2$ is the result image. $ratio$ is the the brightness factor. $ratio=0$ represents a black image while $ratio=1$ represents the original image. We use the following strategy to find the right $ratio$. We first make a guess as the initial value. next we take an image under the desired ambient light. Then we gradually change $ratio$ until the final output $T_E(T_{D,C}(I_d))$ is close enough to the taken image.



% \yi{need a figure showing optical path?}




% \subsection{Attack Overview}
% Let the target recognition model be $F(\cdot)$ and let $x$ be individual video frames the victim camera perceives. Let $y$ be the true label of a perceived object. The normal operation of the autonomous system can be described as: 
% % \eric{what does ``module behind the victim camera'' mean?}
% \begin{equation}
%     y = F(x)
% \end{equation}

% The goal of the adversary is to craft and display an adversarial perturbation on the transparent gadget to alter a perceived frame such that the altered frame will cause a mis-prediction\footnote{We do not specify whether the attack is targeted, i.e., an attacker wants the mis-prediction to be a specific $y'$, or un-targeted, i.e., where an attacker just wants to cause a mis-prediction. The choice of targeted versus un-targeted attacks is domain-specific and can be engineered accordingly.}, i.e., 
% %\eric{again, do we specify targeted or untargeted?}
% %\yi{targeted or untargeted is application specific, so we don't specify it here}
% \begin{equation}
%     y' = F(x'),\quad y\neq y'
% \end{equation}
% where $y'$ is the predicted label of the altered frame $x'$.

% Following the definition of additive perturbation in adversarial examples, an altered frame can be viewed as the addition of the original frame and the perturbation
% \begin{equation}
%     x' = x + p
%     \label{eq:additive}
% \end{equation}
% where $p$ is the perturbation perceived by the victim camera.


% When an adversarial perturbation is displayed on the transparent gadget and then perceived by the victim camera, it goes through a set of transformations.




%\eric{we repeat this about 60 times. maybe just reference \autoref{fig:diagram}instead}
%\yi{attempted a fix}
% As discussed in Section~\ref{sec:overview}, the perceived perturbation will be different from its digital version due to optical characteristics of the displaying and sensing pipeline. For the rest of the paper, we consider an adversarial perturbation computed digitally to be a digital perturbation and the corresponding perceived perturbation to be a physical perturbation. Based on the additive perturbation model \autoref{eq:additive}, a physical perturbation is computed by subtracting a frame without the perturbation on it (background) from the frame with the perturbation on it. %As is mentioned in \autoref{sec:overview}, from a digital perturbation to a physical perturbation, there exist a set of transformations due to the optical characteristics of the displaying and sensing pipeline. 
% % \eric{confusing} 
% % Previous approaches deal with this by either highly restricting the search space of allowed perturbations \cite{li2019adversarial}, manually constructing a color space mapping for a very specific case \cite{lovisotto2020slap}, or using a data-driven approach for modeling a complex transfer function \cite{wengrowski2019light}. These approaches cannot be generalized easily.\eric{not sure what this means} 

% In order to model the displaying and sensing pipeline optics, we take an end-to-end approach using a neural network which we call Tnet. Tnet takes a digital perturbation as input and outputs the corresponding physical perturbation. Tnet is trained on a large number of digital and physical perturbation pairs. Tnet has the advantage that it is fully differentiable, which enables us to adopt optimization techniques from digital adversarial example generation algorithms to design our adversarial perturbations. 

% Physical adversarial examples usually suffer from environmental noise such as viewing angles and distance and time-varying light conditions. Therefore, we propose to use the expectation of transformation, a widely used technique in physical adversarial example generation to augment our optimization process. We will describe each of the aforementioned steps in detail.

% As presented in Figure \ref{fig:diagram}, the optical channel is assembled as follows: A transparent display is placed in front of the camera under attack. An adversary displays an arbitrary image on the transparent display. This image will be captured by the camera. The goal of the adversary is to display an adversarial perturbation that can fool the DNN system behind the camera. 

% The adversary takes a gradient-based attack approach to generate such an adversarial perturbation, i.e., for an image (or a set of images) the adversary wants to attack (e.g mislead the classification of a stop sign image to something else), he computes the gradients on the input image with respect to a predefined loss, then alter the input image in a gradient descent fashion. 

% There are two challenges in the aforementioned process which we tackled in this paper. First of all, what is displayed on the transparent display (digital perturbation) and what is captured by the camera (physical perturbation) can be quite different due to the optical characteristics of the setup, we propose to use a transformation model Tnet $T()$ to model the mapping between a digital perturbation and a physical one. Secondly, pixel level perturbations might not survive the digital to physical mapping. To make the attack more robust, we propose to use a pattern generating function to generate adversarial perturbations. We introduce each of the components in detail below.





% The camera at the same time will capture the object behind the glass (denote as $I$). So the final captured image is 
% \begin{equation}
% I+T(p)
% \end{equation}
% As the perturbation is additive. Note that although the object $I$ will also go through the camera optics to generate the captured image, we assume a image classification system is already working for images captured in this setup (without the projection), so we only consider the effect of $T(\cdot)$ on the perturbation $p$.



% % Figure environment removed

% Since $T(\cdot)$ has too many components, it is very difficult to model them one by one. Therefore, we uses a single neural network model to model all of them end-to-end.

% We consider an $\alpha$-blending operation of a single dot to construct our perturbation generating function:
% \begin{equation}
%     \begin{aligned}
%         p&=\alpha(i,j)\cdot\gamma \\
%         \alpha(i,j)&=\alpha_{max}\cdot\exp(-d(i,j)^{\beta}) \\
%         d(i,j)&=\frac{(i-i^{(c)})^2+(j-j^{(c)})^2}{r^2}
%     \end{aligned}
% \end{equation}
% where
% \begin{itemize}
%     \item $\gamma$ - RGB color
%     \item $(i^{(c)}, j^{(c)})$ - center location of a dot
%     \item $\alpha_{max}$ - maximum alpha blending value
%     \item $\beta$ - exponential dropoff of alpha value
% \end{itemize}

% The final perturbation with multiple dots in it will be the composition of all individual dots
% \begin{equation}
%     p_{final} = \sum_i p_i
% \end{equation}

% Follow the standard expression in adversarial machine learning, we seek an perturbation $p$ such that 

% % Figure environment removed

% each step of the full attack pipeline are already introduced on a high level above. We then describe each step in detail below

% \subsection{Modeling the Displaying and Sensing Pipeline}

% When a digital perturbation is displayed on our transparent display and then perceived by the victim camera as the physical perturbation, it goes through a set of transformations. These transformations are caused by the characteristics of the displaying and camera sensing pipeline. Such characteristics include optics of the holographic combiner and projection source, camera optics such as white balance, distortion and lens distortion. 
% % Moreover, environmental light condition also affects what the perceived perturbation will look like. 
% We propose to use a neural network we call it Tnet, $T(\cdot)$, to model the displaying and camera sensing pipeline. Tnet takes a digital perturbation $d$ as input and outputs the corresponding physical perturbation $p$, i.e.,
% \begin{equation}
%     p = T(d)
% \end{equation}

% % Previous work approach this problem in different ways. \eric{which problem?} 
% There have been attempts trying to model at least some parts of the sensing and displaying pipeline when designing a physical adversarial attack \cite{li2019adversarial,lovisotto2020slap}.
% In \cite{li2019adversarial}, the authors only allow very limited numbers of possible digital perturbations and manually collect their corresponding physical perturbations. Therefore, the effectiveness of this approach is highly limited. In \cite{lovisotto2020slap}, the authors construct a mapping between digital RGB values and their corresponding physical RGB values. Unlike our attack approach--which attacks the camera sensing pipeline--they attack the object. Therefore, a different mapping function needs to be constructed for each object. In our approach, we model the CDTF directly, as inspired by~\cite{wengrowski2019light}. Our attack is automatic and can search a much larger perturbation space much more efficiently.  Also, once Tnet is constructed, it can be applied to different objects (e.g., different traffic signs) and even other security camera applications. %It also needs to be note that our dynamic sensor attack is by definition distinct from the aforementioned previous works and faces a different set of challenges.

% \noindent\textbf{Tnet Architecture.} We propose to adapt a UNet architecture~\cite{ronneberger2015u} to construct Tnet. UNet maps an input image to an output image, which suits perfectly for the functionality of Tnet. Tnet takes a digital perturbation as input, passes it through a set of convolutional layers and deconvolutional layers to output a physical perturbation. %\eric{a figure or citation could be helpful here}

% \noindent\textbf{Training Tnet.} To train Tnet, we want the output of Tnet to be as similar as possible to the ground truth physical perturbation $p'$. To achieve this goal, we need to carefully design the loss function and dataset for training. For loss function, we combine both mean square error (MSE) and LPIPS perceptual loss \cite{zhang2018perceptual} to form the loss function
% \begin{equation}
%     loss = (1 - a) * MSE(p, p') + a * Perceptual(p, p')
%     \label{eq:loss}
% \end{equation}
% where $a$ controls the ratio of the two terms. The MSE term encourages the colors to be as close as possible while the perceptual loss term emphasize on the structure of the patterns. 
% % We will show below that our highly structured adversarial pattern benefits a lot from the perceptual loss term.
% %\eric{missing important details like optimizer, learning rate, batch size, number of epochs, number of trainable parameters, elapsed training time}
% %\yi{We left these implementation details to section V.}

% In order to train Tnet, we need to generate a dataset consisting of the digital perturbation and physical perturbation pairs (e.g. \autoref{fig:perturbation_pair}). We first generate a large number of digital perturbations by randomly varying some of the parameters of a perturbation generating function (Section~\ref{sec:crafting-adversarial}). We then display the digital perturbations on our transparent gadget. We place the gadget in front of a camera identical to the camera under attack and collect a physical perturbation for each digital perturbation.
% % \eric{reference the figure illustrating our dataset}




\subsection{Crafting Adversarial Perturbations}\label{sec:crafting-adversarial}

In this section, we describe how we design a digital and physical co-optimization framework %, which refer to as \textit{TNet},\yi{do we want to bring up the term TNet at this point?} 
to find the adversarial perturbation that can evade the target recognition model.

% \noindent\textbf{Generating Digital Perturbation.} Extending the idea of using small dots to create adversarial perturbations in \cite{li2019adversarial}, we adopt the $\alpha$-blending model to generate our adversarial perturbations. We define a dot-based adversarial perturbation generation function $d(\theta)$. $\theta$ contains parameters defining the dots as
% \begin{itemize}
%     \item $c(i^{(k)}, j^{(k)})$ - coordinates of the center of the $k$th dot
%     \item $r_k$ - radius of the $k$th dot
%     \item $\gamma_k$ - RGB color of the $k$th dot
%     \item $\alpha_{max}$ - maximum alpha blending value
%     \item $\beta$ - exponential dropoff of alpha value
%     \item $n$ - maximum number of dots.
% \end{itemize}
% $d$ can be computed as
% \begin{equation}
%     d(i,j) = \sum_{k=1}^{n}\alpha_{max}e^{-\beta \cdot dist_k}\cdot\gamma_k
% \end{equation}
% where $dist_k(i,j)$ is the distance of the pixel $(i,j)$ with respect to the center of the $k$th dot
% \begin{equation}
%     dist_k(i,j) = \frac{(i-i^{(k)})^2+(j-j^{(k)})^2}{r_k^2}. \\
% \end{equation}
% $d$ superposes $n$ dots together to create a pattern with various colors and shapes. \autoref{fig:p_digital} shows the example of a digital perturbation. The number of dots in an adversarial perturbation affects the capacity of it. An adversarial perturbation with fewer dots is less powerful than one with more dots. A perturbation with more dots can create more complex patterns and, thus, can alter the frames and the prediction of the target recognition model more easily.
% % \eric{them?}



% A brightness adjustment transformation can be implemented by converting the input image to the HSV/HSL space \cite{wiki:HSL_and_HSV}, adjusting the brightness and then converting it back. To estimate the desired brightness level, we display a random perturbation and capture two images, one with the perturbation on and one with it off. We compute the subtraction between the two images and measure the brightness of it in the HSV/HSL space.

\noindent\textbf{Attack Objective.} 
% In adversarial example (AE) generation research, a unique AE can be computed for each individual image under attack, or one AE can be computed to work for a set of images. This is called a universal adversarial perturbation (UAP). 
As is mentioned in research challenge \#3, UAP fits better in our attack scenario: an adversary prepares adversarial perturbations for the classes of images they intend to attack beforehand.
% \eric{why does UAP fits better in our attack scenario?} 
%For instance, recognize stop signs as some other traffic signs or recognize the face of a burglar as the house owner.
Each one of these UAPs can cause an incorrect prediction for all the instances of the object in the corresponding class. The adversary then attaches the attack gadget to the victim camera, and remotely controls which UAP to display on the gadget. Thus, we define our attack objective as
\begin{equation}
    \arg\min_{\theta}\quad \mathbf{E}_{x\sim D(x|y)}l[(F(x+I_p))]
\label{eq:objective}
\end{equation}
where $\theta$ represents the free parameters of the perturbation generation function (e.g., $c$ and $\gamma$).  $l$ is the loss function of the attack. A sample $x$ is drawn from a distribution $D(x|y)$ (e.g., the class of all stop signs). Depending on the type of attacks the adversary wants to perform and the target recognition model, $l$ can take various forms. For instance, for an untargeted attack on a traffic sign classifier (i.e., to classify a frame into any class other than the true class), $l$ is the reciprocal of the cross entropy loss with respect to the true class. For an untargeted attack on a traffic sign detector, $l$ is the average class score of all the predicted bounding boxes with respect to the class under attack (see Section~\ref{sec:prototype} for details). 
%\luis{TODO: Yi, can you address Saman's comments and clarify the above parameters?}

\noindent\textbf{Optimizing a Successful Attack.} As each module in our attack pipeline is differentiable, it is theoretically possible to approach a solution of \autoref{eq:objective} using gradient descent. However in practice, we found the effectiveness of an optimized perturbation to be highly sensitive to the initialization of free parameters, namely, $c$ and $\gamma$. Also, as shown in~\cite{li2019adversarial}, the gradients with respect to the free parameters present a highly non-convex loss surface. Therefore, we first find a good initialization using a coarse grained greedy block coordinate descent search. We then apply fine-grained gradient descent using this initialization. Specifically, we split a perturbation into blocks of the same size. The center of these blocks are the candidate locations of the dots $c$. We also discretize the RGB color space to obtain a fixed set of candidate colors. We then optimize for one dot at a time. For each dot, we try all the candidate locations and colors and pick the one that gives the maximum loss (to have a higher chance of misleading the recognition model). We repeat this process until convergence. Next, using the dots computed above as initialization, we iteratively compute the gradients of the loss with respect to the free parameters and extract the sign of the gradient. We then add a small step in the direction of the sign. Detailed steps of our optimization are shown in Algorithm~\ref{algo:attack}. Note during the optimization of the attack the weights of $T(\cdot)$ remain fixed.
% \eric{good}

\noindent\textbf{Serving the Attack.} Attack optimization is performed in advance for the objects under attack. The generated perturbations are stored in a database. At runtime, the attacker first uses the auxiliary sensor to determine the context of the runtime environment (e.g., a target object entering the scene). \name then displays a pre-generated perturbation fetched from the database to create an unsafe classification.

\begin{algorithm}%[H]
\DontPrintSemicolon
  \KwInput{Images under attack $X=\{x1,x2,...x_n\}$, attack objective $l$, maximum number of iterations \textit{maxiter}, perturbation generation function $\theta$}
  \KwOutput{Digital perturbation $I_d$}

%   \KwData{Testing set $x$}
%   $\sum_{i=1}^{\infty} := 0$ \tcp*{this is a comment}
%   \tcc{Now this is an if...else conditional loop}
%   \If{Condition 1}
%     {
%         Do something    \tcp*{this is another comment}
%         \If{sub-Condition}
%         {Do a lot}
%     }
%     \ElseIf{Condition 2}
%     {
%     	Do Otherwise \;
%         \tcc{Now this is a for loop}
%         \For{sequence}    
%         { 
%         	loop instructions
%         }
%     }
%     \Else
%     {
%     	Do the rest
%     }
    % \tcc{Now this is a While loop}


    Initialize $\theta$ with coarse grained greedy coordinate descent search \\
    \While{\textsc{\textbf{not}} converge \& iter $\leq$ maxiter}
   {
        1. Compute digital perturbation $I_d$ \\
        2. Compute physical perturbation $I_p=T(I_d)$ \\
        % 3. Estimate and adjust the brightness of $T_{D,C}(I_d)$ based on the ambient light to get $I_p$ \\
        3. Apply $I_p$ to X  \\
        4. Query the victim model and compute the attack objective $l$ \\
        5. Perform a step of gradient descent to update $\theta$
   }
\caption{Physical AE Generation}
\label{algo:attack}

\end{algorithm}


\noindent\textbf{Robust Physical Adversarial Examples} 
To make an adversarial perturbation robust against dynamic physical environmental conditions, we consider the following factors when preparing images for the adversarial perturbation generation process:
\begin{itemize}
\item \textbf{Background.} The context of the background plays a role in recognizing an object. We use images with various backgrounds within the same context.

\item \textbf{Perspective and rotation.} The object might come in a different perspective and rotated in front of the camera. We prepare images with various perspectives and rotations. This is application-specific, e.g., when attacking a traffic sign recognition model, we consider perspectives between $-30^{\circ}$ and $30^{\circ}$, with rotations between $-5^{\circ}$ and $5^{\circ}$.

\item \textbf{Distance.} The object might also be located at various distances from the camera. To account for this, we consider various sizes of the object.
% \item \textbf{Illuminance.} To consider different environmental light conditions, we apply brightness transformation to the simulated attacked images.

\item \textbf{Illuminance.} To account for different light conditions, we vary the ambient light of the environment when collecting images. Specifically, we consider an illuminance ranging from 30 lux to 3000 lux.
\end{itemize}


% To make an adversarial perturbation work under various environmental noise, e.g., viewing angles, distance, and light conditions, etc., we propose to use Expectation over Transformation (EOT), a common method in physical AE generation. EOT works by synthesizing training images using a set of transformations that represents various environmental noise. These images augment the training dataset used for computing an adversarial perturbation. In this way, the generated adversarial perturbations can be more robust. 

% We mainly consider the following transformations
% \begin{itemize}
%     \item To deal with this, we consider the following brightness transformation
%     \begin{equation}
%         g(i, j) = a f(i, j) + b
%     \end{equation}
%     where $g(i, j)$ and $f(i, j)$ are source and target image pixels respectively. We 
%     \item The object under attack might be presented to the victim camera in various angles. To account for this, we first crop the object from the image. We then apply perspective transformation to the object. Finally, we attach the object back to a background image. 
%     \item The object under attack might also be presented to the victim camera in different distances. Following similar approach as angles, we change the size of the object.
% \end{itemize}

\noindent\textbf{Dynamic Adversarial Attack.} One major advantage of our proposed attack compared to prior static attacks is that in our attack approach, the adversary can change the perturbation based upon different scenarios. For example, in a traffic sign recognition use case, if a car is following a particular route, we can assume there are several different safety-critical traffic signs along the route. 
% \eric{a bunch? how about ``several''?} 
To maximize the damage of the attack, The adversary wants to successfully attack as many critical traffic signs as possible. In the static attack setting, the perturbation cannot be changed, and the adversary can only compute a single UAP to accommodate all different kinds of traffic signs. In our dynamic attack setting, we can change the perturbation based on external information such as GPS of the car and a traffic sign map~\cite{ertler2020mapillary, vargas2020openstreetmap} to maximize the attack success rate. Moreover, an attacker can enable perturbations only when desired to minimize alerting the vehicle to a camera obstruction or unwanted behavior during normal operation. 
% \eric{``however'' is used too commonly throughout this paper. not every sentence needs a qualifier.}

\yi{should describe how we store computed perturbations in a database and fetch based on auxiliary information}
% \yi{can define multiple timestamps, prepare one perturbation for each timestamp.}

% so to find the extreme point we can apply say gradient descent.  But here we only need a $(n,r,c)$ that can increase $loss$ and possibly alter the prediction, so following the approach of FSGM, we only update $(n,r,c)$ once
% \begin{equation}
%     (r,c,\gamma)_{adv}=(r,c,\gamma)+\epsilon\cdot sign(\nabla_{(r,c,\gamma)}F(x+T(P(r,c,\gamma)),label)).
% \end{equation}

% Initial value of $(r,c,\gamma)$ are set to be $(0,0,(255,255,255))$ to represent black (so $T(P(r,c,\gamma))$ will be all $0$ and this does not affect the prediction of the model for the original $x$).  

% Things to discuss:
% \begin{enumerate}
%     \item Model architecture
%     \item how to create a training dataset to fully model all the components described above
% \end{enumerate}

%\luis{Add section here about calculating the trajectory. }
%\luis{This section will discuss how we can leverage semantic information. For instance, we will evaluate two use cases. The targeted, dynamic attacks will be domain-specific, and we will show the tradeoff between accuracy and robustness and time-to-generate pertubation. In both cases, we will be provided semantic information through some auxiliary sensor(s). In the case of the security camera, a second sensor could determine that a person will be entering the scene; however, the path is less deterministic; thus, we'd opt for a more robust AML attack (e.g., UAP). In the case of the self-driving car, the semantic information could be provided by knowledge of the map, the car's trajectory through some auxiliary sensors. Thus, if we know there will be some safety signs at a particular location, and we have access to GPS, we could deterministically generate a more accurate perturbation for safety signs}
%\luis{We should use this citation as a means for a bounding box; we should be careful because tehre are a lot of related works on segmentation, etc. We should be clear why we use the bounding box first and then go from there.}