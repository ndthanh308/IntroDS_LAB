\section{Methodology}
\label{sec:methodology}

We used a mixed-methods approach that combines an exploratory study through mining code review data from the OpenStack and Qt communities and an industrial survey with 63 practitioners. In this section, we first present our seven Research Questions (RQs). Then we describe the research process (see Fig.~\ref{fig: Overview of the mixed-methods research process}) and detail the methods used to collect, label, extract, and analyze the data in this study. 
% Figure environment removed

% We detail the methodology followed in this study in this section. We explain the methods used to collect, analyse and report our results for the research questions answered in this study. 
% The goal of this study described through the Goal Question Metric approach \citep{caldiera1994goal} is to investigate code snippets in code reviews \textbf{for the purpose of} exploration \textbf{with respect to} the distribution, purposes, and acceptance of code snippets


\subsection{Research Questions}
\label{subsec:research_questions}
% The main goal of this study is to investigate how code smells are addressed during the course of a modern code review process. 
% Specifically, we analyse code reviews for the purpose of understanding the nature of code smells in code reviews from a code reviewer and developer point of view. This goal is decomposed into the following five research questions (RQs): \\
% Then main goal of this study is to mine the knowledge and information about code snippets in the context of code reviews, and to construct a practical guideline of using code snippets in code reviews. Specifically, this paper analyzes code review data collected from OpenStack and Qt communities to explore the extent of using code snippets, the reviewers' intentions of providing code snippets, the developers' acceptance of code snippet comment, and the reasons developers not accept code snippet suggestions in code reviews from the perspective of developers and reviewers. Besides, we also conducted a survey study to understand the knowledge and experience of code snippets in code review practice from industrial practitioners' perspective for the sake of finding the best practice of using code snippets in code reviews. The goal of our study is decomposed into the following seven research questions (RQs): \\ 
The goal of this study is to understand the practices of using code snippets in the context of code review, and to develop a practical guideline of using code snippets in code reviews. This goal can be further decomposed into seven RQs as listed below. Specifically, the first four RQs (i.e., RQ1, RQ2, RQ3, and RQ4) are answered through an exploratory study by analyzing the code review data collected from the OpenStack and Qt communities. The results of these four RQs can help understand the extent of using code snippets in code reviews, the reviewers' purposes of providing code snippets in code reviews, the developers' acceptance of code snippets in code reviews, and the reasons developers do not accept code snippet suggestions in code reviews. 
% the first four RQs are answered through an exploratory study by analyzing code review data collected from the OpenStack and Qt communities, in order to understand the extent of using code snippets in code reviews, the reviewers' purposes of providing code snippets in code reviews, the developers' acceptance of code snippets in code reviews, and the reasons developers do not accept code snippet suggestions in code reviews. 
The last three RQs (RQ5, RQ6, and RQ7) are answered through an industrial survey of 63 practitioners, in order to understand the scenarios in which reviewers provide code snippets, developers' attitudes towards code snippets in code reviews, and the characteristics that developers want code snippets to have in code reviews.
% in order to understand the knowledge and experience of code snippets in code review practice and find the best practice of using code snippets in code reviews.


\noindent\textbf{RQ1: To what extent are code snippets used in code reviews?}

\noindent\textbf{Motivation:} Currently, we do not know how frequently code snippets are used in code reviews in the modern code review process. As an exploratory study on code snippets in code reviews, this RQ aims to explore the distribution and proportion of review comments that contain code snippets. The answer of this RQ can help to get a basic overview of code snippets in code reviews. 

\noindent\textbf{RQ2: What are the purposes of code snippets provided by reviewers in code reviews?}

\noindent\textbf{Motivation:} Previous work has studied the purposes of shared links in code reviews to investigate what kinds of information could be provided through link sharing \citep{wang2021understanding}. Code snippets can also provide important information in code reviews similar to shared links, however, we have no idea about the purposes of providing code snippets in code reviews. According to the results of RQ1, we found that most code snippets in review comments are provided by reviewers. This RQ aims to understand the purposes of code snippets provided by reviewers in code reviews. The answer of this RQ can help to get a better understanding of the roles code snippets play in code review.

\noindent\textbf{RQ3: How do developers treat code snippet suggestions in code reviews?}

\noindent\textbf{Motivation:} The reaction of developers to the review comments that contain code snippets is meaningful to explore the impact of code snippets in code reviews. According to the results of RQ2, we found that the most common purpose for reviewers to provide code snippets in code reviews is \textit{Suggestion} for developers. Developers may take different actions towards reviewers' code snippet suggestions. They could decide whether or not to accept reviewers' code snippet suggestions or just ignore them. This RQ aims to explore how developers treat code snippet suggestions in code reviews by investigating how many code snippet suggestions are accepted, ignored, and not accepted, respectively. Such information can help to understand the usefulness of the code snippets in code reviews.

\noindent\textbf{RQ4: What are the reasons that developers do not accept code snippet suggestions in code reviews?}

\noindent\textbf{Motivation:} During the code review process, not all code snippet suggestions would be accepted by developers. Sometimes, developers may not follow reviewers' code review suggestions even though developers agree with the reviewers, for other reasons. This RQ aims to explore the reasons why developers do not accept reviewers' code snippet suggestions. The answer of this RQ can help reviewers be aware of the reasons behind developers' nonacceptance of the code snippet suggestions and thus help reviewers improve their code reviews and avoid the situations that result in code snippet suggestions not being accepted.

\noindent\textbf{RQ5: In which scenarios do reviewers often provide code snippets in code reviews?}

\noindent\textbf{Motivation:} During the code review process, in some scenarios reviewers tend to provide code snippets in review comments, while in some other scenarios, they tend to provide plain text or shared links instead. This RQ aims to investigate the situations when reviewers provide code snippets in code reviews. The answer of this RQ can help acquire the knowledge and practices of using code snippets in code reviews. Such information can also guide reviewers when to provide code snippets in code reviews.

\noindent\textbf{RQ6: What are the attitudes of developers towards code snippets provided in code reviews?}

\noindent\textbf{Motivation:} Intuitively, developers may have a positive attitude towards code snippets provided in code reviews as code is more convincing and direct than review comments in plain text. This RQ aims to understand the actual attitudes of developers towards code snippets in code reviews. Answering this RQ can help to guide whether reviewers are encouraged to provide code snippet in order to make the discussions during the code review process more smoothly.

\noindent\textbf{RQ7: What are the characteristics of code snippets that developers expect reviewers to provide in code reviews?}

\noindent\textbf{Motivation:} Code snippets with specific characteristics (e.g., code quality and readability) in code reviews may decide whether developers will accept reviewers' suggestions or not. This RQ aims to summarize the characteristics that developers hope code snippets in code reviews to have. The results of this RQ can help reviewers be aware of the key points of providing code snippets in code reviews and thus make their code snippet suggestions more acceptable.
% Such information can help reviewers to provide more acceptable code snippet suggestions and make reviewers take note of the key points of providing code snippets in code reviews.


\subsection{Exploratory Study Design}
\subsubsection{Data Collection}
\label{datacollection}
For the exploratory study, we used four active projects from two large and popular open-source communities: OpenStack and Qt. OpenStack is a set of software components that provide common services for cloud infrastructure, and is contributed by many well-known software companies, e.g., IBM, VMware, and NEC \citep{thongtanunam2018review}. Qt is a cross-platform tool and UI framework for creating graphical user interfaces and developing multi-platform applications. In addition to Qt company, many organizations and individuals who use Qt as a development platform also contribute in the open development of Qt through the Qt community. Meanwhile, the OpenStack and Qt communities have made a serious investment in code review for many years \citep{hirao2022code}, and the code review data from these two communities have been widely used in many studies related to code review (e.g., \cite{wang2019why, thongtanunam2016revisiting, ruangwan2019impact, hamasaki2013who, ueda2018how, wang2021understanding}).

Based on the reasons above, we argue that it is appropriate and representative to conduct our code review research based on these two communities. The OpenStack and Qt communities are both composed of a set of projects, and we selected the two most active projects in each community as our subject projects (based on the number of closed code changes): Nova (a cloud computing fabric controller) and Neutron (a networking service) in the OpenStack community and Qt Base (a module which offers classes for embedded Linux devices) and Qt Creator (a cross-platform Integrated Development Environment (IDE)) in the Qt community.

Both OpenStack and Qt communities use Gerrit\footnote{\url{https://www.gerritcodereview.com/}} to support their code review process. Gerrit is a Web-based code review tool built on top of Git, a well-known distributed control system for code. Gerrit provides various REST APIs to acquire code review data. By using the RESTful API provided by Gerrit, we were able to collect all the closed code changes of the OpenStack projects (Nova and Neutron) and the Qt projects (Qt Base and Qt Creator) from 2020 to 2021. Then, we extracted all available review comments for these code changes and stored the review data in local files for further analysis. Table \ref{Overview of the collected code review data for each project} shows the details of the code review data collected from each project. In total, we collected 34,370 code changes and 127,182 review comments from the OpenStack and Qt communities, which were used as the basic code review data for the exploratory study.

\begin{table}[htbp]
\centering
\renewcommand\arraystretch{1.5}
\caption{Overview of the collected code review data from each project}
\label{Overview of the collected code review data for each project}
\begin{tabular}{p{3cm}m{3cm}<{\centering}m{5cm}<{\centering}}
\hline
\textbf{Project} & \textbf{\#Code Changes} & \textbf{\#Review Comments (RCs)} \\ \hline
Nova             & 2,960                           & 20,748                             \\ 
Neutron          & 3,475                           & 13,541                             \\ 
Qt Base          & 17,181                          & 66,878                             \\ 
Qt Creator       & 10,754                          & 26,015                             \\ \hline       
\textbf{Total}   & 34,370                          & 12,7182                            \\ \hline
\end{tabular}
\end{table}


\subsubsection{Data Labelling}
\label{datalabelling}
After collecting the code review data from the OpenStack and Qt communities, we got a total of 127,182 review comments to label if they contain code snippets. It is time-consuming to label such a large number of code review comments manually, and we decided to eliminate some irrelevant code review comments by using certain measures. The whole process of data labelling is divided into the following five steps:

In \textbf{step one}, we removed the review comments that were generated by bots (i.e., Zuul in OpenStack and Qt Sanity Bot in Qt) since we aimed at exploring code snippets in code reviews from the perspective of developers and reviewers.

In \textbf{step two}, considering that Nova and Neutron projects are mainly written in Python (more than 97\%), and Qt Base and Qt Creator projects are mainly written in C++ (more than 85\%), we decided to focus on the review comments that contain Python code snippets in Nova and Neutron and the review comments that contain C++ code snippets in Qt Base and Qt Creator. In addition, we only retained the review comments from the source files written in the main programming language of each project, as these source files are directly related to the code snippets written in corresponding programming languages. In particular, for Nova and Neutron projects, we only kept review comments from the source files with \textsc{.py} file extension, and for Qt Base and Qt Creator projects, we only kept review comments in the source files with \textsc{.cpp} or \textsc{.h} file extension. After the two steps, a part of the irrelevant review comments were filtered out. The counts of remaining review comments for each project are presented in Table \ref{Count of review comments for the projects in OpenStack and Qt communities}.

In \textbf{step three}, the first and second authors manually labelled the remaining review comments after a pilot data labelling. Specifically, the pilot data labelling process is composed of the following substeps: (1) With a 95\% confidence level and a 3\% margin of error \citep{israel1992determining}, the first author randomly selected 993 review comments from the code review data of the OpenStack projects in 2020 and 1,021 review comments from the code review data of the Qt projects in 2021. We randomly selected review comment data from different projects in different years in order to increase the diversity of the pilot labelling data. (2) The first and second authors labelled independently whether the review comments should be included or not. (3) Review comments labelled by the two authors were compared to measure the inter-rater reliability and Cohen's Kappa coefficient \citep{Jacob1960Coefficient} was calculated as a way to verify the consistency on the labelled review comments between the two authors. The Cohen's Kappa coefficient for the OpenStack community is 0.86, while it is 0.92 for the Qt community, both higher than 0.8, thus indicating a high degree of consistency between the two authors.

In the data labelling process, the first two authors followed a set of inclusion and exclusion criteria: (I1) If a review comment contains code snippets (source code or pseudocode) with at least one valid statement, we include it. (E1) If a review comment only contains the name of a certain variable or function, we exclude it. (E2) If the code snippets contained in a review comment come from log files (e.g., error stack trace), we exclude it. To better illustrate the inclusion and exclusion criteria, consider the following two exemplary review comments. The first review comment is included as it contains a valid code statement. The second review comment is excluded. Though it contains a valid code statement, the code snippet is from the system error output log. 
% \textcolor{blue}{During the labelling process, if the first two authors were unsure whether or not to include a review comment, the third author was invited to discuss until an agreement was reached. \textcolor{blue}{After manually labelling all the candidates, we finally got a total of 3,244 review comments that contain code snippets from the four projects of the OpenStack and Qt communities.}


% In \textbf{step three}, the first and the second authors conducted a pilot manual labelling by following a set of inclusion and exclusion criteria: (I1) If a review comment contains code snippets (source code or pseudocode) with at least one valid statement, we include it. (E1) If a review comment only contains the name of a certain variable or function, we exclude it. (E2) If the code snippets contained in a review comment come from log files (e.g., error stack trace), we exclude it. To better illustrate the inclusion and exclusion criteria, consider the following two exemplary review comments. The first review comment is included as it contains a valid code statement.

\begin{qoutebox}{white}{}
    \textbf{Link:} \url{http://alturl.com/9gnck}\\
    \textbf{Reviewer:} ``shorter suggestion: \\
\texttt{return parent.isValid() ? treeItemAtIndex(parent)->childCount()
                        : m\_rootItem->childCount();}''
\end{qoutebox}

% The second review comment is excluded, though it contains a valid code statement, the code snippet is from the system error output log.

\begin{qoutebox}{white}{}
    \textbf{Link:} \url{http://alturl.com/e2ib3}\\
    \textbf{Reviewer:} ``src/plugins/qmldesigner/components/itemlibrary/itemlibraryassetimporter.cpp:504: error: `m\_importFiles' was not declared in this scope \\
 \texttt{ 504 |  if (model \&\& !m\_importFiles.isEmpty()) \{ } \\
An ifdef is missing somewhere. Please fix.''
\end{qoutebox}


It is easy to tell whether a review comment contains code snippets or not based on the inclusion and exclusion criteria as most of the review comments we collected are trivial cases. During the data labelling process, if the first two authors were unsure whether or not to include a review comment, which is very rare (only 14), the third author was invited to discuss until an agreement was reached. After manually labelling all the candidates, we finally got a total of 3,213 review comments that contain code snippets from the four projects of the OpenStack and Qt communities. Table \ref{Count of review comments for the projects in OpenStack and Qt communities} shows the count of review comments after each step of data labelling. Note that we used the data after \textbf{step two} to answer RQ1, and the data after \textbf{step three} to answer RQ2.

\begin{table}[htbp]
\renewcommand\arraystretch{1.5}
\centering
\caption{Counts of review comments of the four projects in the OpenStack and Qt communities}
\label{Count of review comments for the projects in OpenStack and Qt communities}
\begin{tabular}{p{1.9cm}m{1.9cm}<{\centering}m{3.4cm}<{\centering}m{3.5cm}<{\centering}}
\hline
\textbf{Project} & \textbf{\#RCs}         & \textbf{\#RCs after Step Two}         & \textbf{\#RCs after Step Three}  \\ \hline
Nova             & 20,748                 & 14,680                                & 625                              \\ 
Neutron          & 13,541                 & 7,951                                 & 342                              \\ 
Qt Base          & 66,878                 & 35,021                                & 1,672                            \\ 
Qt Creator       & 26,015                 & 11,952                                & 574                              \\ \hline       
\textbf{Total}   & 127,182                & 69,604                                & 3,213                            \\ \hline
\end{tabular}
\end{table}



% Specifically, the pilot data labelling process is composed of the following substeps: (1) With a 95\% confidence level and a 3\% margin of error \citep{israel1992determining}, the first author randomly selected 993 review comments from the code review data of the OpenStack projects in 2020 and 1,021 review comments from the code review data of the Qt projects in 2021. (2) The first and second authors labelled independently whether the review comments should be included or not. (3) Review comments labelled by the two authors were compared, and the level of agreement for labelling review comments from OpenStack and Qt between the two authors were calculated using the Cohenâ€™s Kappa coefficient \citep{Jacob1960Coefficient}. The Cohen's Kappa coefficient for the OpenStack community is 0.86, while it is 0.92 for the Qt community, both higher than 0.8, thus indicating a high degree of consistency between the two authors.

%In \textbf{step four}, we used a programming language detection tool, Guesslang\footnote{\url{http://guesslang.readthedocs.io/}} to assist us with the labelling process. Guesslang is an open-source machine learning application, and it could automatically detect the programming languages of a given source code. Considering the popularity of Guesslang (used by the well-known IDE, Visual Studio Code) and the fact that it supports the detection both Python and C++ languages, we decided to use this tool to assist us in labelling whether review comments contain code snippets. For a review comment, Guesslang predicts the probability of each line of the comment belonging to a given programming language, and we took the maximum probability of all the lines in the comment as the final probability that the review comment contains a code snippet. For the review comments labelled as containing code snippets in the pilot data labelling in step three, we used Guesslang to detect the probability that they contain Python code (for review comments from the OpenStack projects) or C++ code (for review comments from the Qt projects), and picked the minimum of the probabilities as the threshold values of a review comment containing Python code or C++ code. Subsequently, we used Guesslang to automatically label the 22,631 remaining review comments from OpenStack and 46,973 remaining comments from Qt obtained in step two, i.e., when the probability of a review comment containing Python code or C++ code is less than the corresponding threshold value, it is considered as not containing code snippets and is filtered out. After this step (step four), the counts of remaining review comments for each project are shown in Table \ref{Count of review comments for the projects in OpenStack and Qt communities}.

% In \textbf{step five}, the first two authors labelled the 55,673 remaining review comments respectively. In the labelling process, if they were unsure whether or not to include a review comment, the first three authors would discuss until they reached an agreement. After manually labelling all the candidates, we finally got a total of 3,197 review comments that contain code snippets from the four projects of the OpenStack and Qt communities.



% \begin{table}[htbp]
% \renewcommand\arraystretch{1.5}
% \centering
% \caption{Counts of review comments (RCs) of the four projects in the OpenStack and Qt communities}
% \label{Count of review comments for the projects in OpenStack and Qt communities}
% \begin{tabular}{lp{1.5cm}p{2.4cm}p{2.4cm}p{2.4cm}}
% \hline
% \textbf{Project} & \textbf{Count of RCs} & \textbf{Count of RCs after Step two} & \textbf{Count of RCs after Step Four} & \textbf{Count of RCs after Step Five} \\ \hline
% Nova             & 20748                 & 14680                                             & 7153                                  & 614                                   \\ 
% Neutron          & 13541                 & 7951                                              & 3930                                  & 339                                   \\ 
% Qt Base          & 66878                 & 35021                                             & 33209                                 & 1670                                  \\ 
% Qt Creator       & 26015                 & 11952                                             & 11381                                 & 574                                   \\ \hline       
% \textbf{Total}   & 127182                & 69604                                             & 55673                                 & 3197                                  \\ \hline
% \end{tabular}
% \end{table}


\subsubsection{Data Extraction and Analysis}
\textbf{1) Data Extraction}

Before data analysis, the first and second authors conducted a pilot data extraction by randomly selecting 10 code snippet comments from each of the four project in each year (80 review comments in total). The two authors extracted the data items listed in Table \ref{Data items extracted and their corresponding RQs} independently. If any disagreements arouse, the third author was involved to discuss with the two authors and came to an agreement. After the pilot data extraction, the first two authors extracted the data items from all the code snippet comments identified. During this process, any uncertain part was discussed between the first three authors until they reached a consensus to increase the correctness of the extracted data. Finally, the first author rechecked and compared all the extracted data to further enhance the accuracy of the extracted data.
% Before data analysis, the first and second authors extracted the data items listed in Table \ref{Data items extracted and their corresponding RQs} independently. Table \ref{Data items extracted and their corresponding RQs} also presents the corresponding RQ of each data item. During data extraction, any uncertain part was discussed between the first three authors to come to an agreement. 

\begin{table}[htbp]
\renewcommand\arraystretch{1.5}
\caption{Data items extracted and their corresponding RQs}
\label{Data items extracted and their corresponding RQs}
\begin{tabular}{m{0.5cm}<{\centering}m{2.4cm}m{7.3cm}m{0.5cm}<{\centering}}
\hline
\textbf{\#} & \textbf{Data Item}  & \textbf{Description}                                                                                                             & \textbf{RQ}   \\ \hline
D1          & Selected            & \textit{Whether or not a review comment contains code snippets}                                                                  & RQ1   \\ \hline
D2          & Identity            & \textit{The identity of the people who makes the review comments (i.e., reviewer or developer)}                                  & RQ1   \\ \hline
D3          & Purpose             & \textit{The intention of providing code snippets in a review comment}                                                            & RQ2   \\ \hline
D4          & Detailed purpose    & \textit{The elaborated intention of providing code snippets in a review comment}                                                 & RQ2   \\ \hline
D5          & Developer's action  & \textit{The action taken by the developer (i.e., accept, ignore, or not accept) towards the reviewer's code snippet suggestion}  & RQ3   \\ \hline
D6          & Evidence            & \textit{The proof that the developer accepted the code snippet suggestion}                                                       & RQ3   \\ \hline
D7          & Not accept reason   & \textit{The reason why the developer did not accept the reviewer's code snippet suggestion}                                      & RQ4   \\ \hline
\end{tabular}
\end{table}

Note that different data were used to answer the RQs. To answer RQ1, we used all the review comments after \textbf{step two} of data labelling (see Section \ref{datalabelling}) as the raw data, and we counted how many review comments in the raw data contain code snippets to investigate the extent of using code snippets in code reviews. To answer RQ2, we analyzed all the review comments that contain code snippets gained from RQ1 (i.e., the review comments after \textbf{step three}) to get the purposes of reviewers when using code snippets in code reviews. To answer RQ3, we collected the code snippet suggestions identified in RQ2 to explore the reaction of developers towards these suggestions, and we then used the unaccepted suggestions as the data to answer RQ4.

\textbf{2) Data Analysis}

% After the data extraction, we used open coding and open coding and Constant Comparison methods from Grounded Theory (GT) \citep{stol2016grounded} to answer our RQs. In the following we explain the analysis of RQ1, RQ2, RQ3, and RQ4 for the exploratory study.\\
After the data extraction, the first and second authors analyzed the extracted data to answer the RQs of the exploratory study.
% , and any conflicts were discussed and addressed by the first three authors using a negotiated agreement approach \citep{campbell2013coding} to come to an agreement, which improved the reliability of the data analysis results. In the following we explain the data analysis of RQ1, RQ2, RQ3, and RQ4 for the exploratory study. All the labelling and analysis results have been provided online for replication purpose \citep{replpack}.

\noindent\textbf{RQ1: To what extent are code snippets used in code reviews?}

To answer RQ1, we analyzed (1) the percentage of review comments that contain code snippets, (2) the percentages of code snippets provided in review comments by reviewers and developers, and (3) the percentages of developers and reviewers who had provided code snippets in review comments, to investigate the usage of code snippets in code reviews. By analyzing these three aspects, we could know the basic overview of how often reviewers and developers provide code snippets in code reviews, as well as the main identity of people who provide code snippets in code reviews.

\noindent\textbf{RQ2: What are the purposes of code snippets provided by reviewers in code reviews?}

To answer RQ2, we used Open Coding and Constant Comparison methods from Grounded Theory (GT) \citep{stol2016grounded} to find the purposes of reviewers providing code snippets in code reviews. As the purposes of reviewers providing code snippets in code reviews might be expected to be closely related to the purposes of shared links and comments in general, we considered both taxonomies proposed by \cite{wang2021understanding} and \cite{li2017automatic} as the base to build the taxonomy in this study. For every review comment containing code snippets provided by reviewers, we read through the textual content of the comment, its corresponding source code, and its contextual information to understand the reviewer's purpose of providing code snippets. The detailed steps of data analysis for answering RQ2 are the following:
\begin{enumerate}[itemindent=1.5em]
    \item The first two authors coded reviewers' purposes of providing code snippets in code review comments by highlighting the text sections related.
    \item The first author rechecked all the coding results to make sure that the extracted data of RQ2 were correctly coded.
    \item The first two authors grouped similar codes into categories. The process is iterative, in which the two coders went back and forth between the codes and categories to refine the taxonomy.
    % \item \textcolor{blue}{The first two authors combined all the codes into higher-level concepts and transformed them into categories and subcategories.}
    % \item \textcolor{blue}{The first two authors reviewed the generated categories, constantly compared them with the exploratory dataset, and made suitable adjustments.}
    % \item \textcolor{blue}{The first two authors named the generated categories and subcategories.}
    \item The third author then examined the analysis results and disagreements were eliminated through discussions with the first two authors.
\end{enumerate}

% During the data analysis process, the third author was involved in cases of disagreement between the two coders.
A review comment may contain multiple code snippets, in which case we fully considered the contextual information and selected the most significant purpose as the final purpose of providing code snippets in this review comment. The reason why we did not identify multiple purposes in review comments with multiple code snippets is that a piece of code review comment is an integral part of delivering code review information, which could not be analyzed in separate sections. For example, a reviewer provided two pieces of code snippets in the following review comments. The first code snippet serves as a specific code use case to assist reviewers in elaborating on the aforementioned example, while the second one was made by the reviewer to suggest the developer to weaken the return type of the function. Based on the comprehensive analysis and feedback from the developer, we concluded that the first code snippet which elaborates the example at length is used to help the developer better understand the flaws in the current code and thus better understand the suggestions in the second code snippet. Therefore, we selected the purpose of providing the second code snippet by the reviewer as the main purpose of the code snippets provided in this review comment. Note that only 10 (0.31\%, 10/3213) review comments contain multiple code snippets, which is very rare.

\begin{qoutebox}{white}{}
    \textbf{Link:} \url{http://alturl.com/n9zbr}\\
    \textbf{Reviewer:} ``In case if value is an lvalue, T will be deduced as lvalue ref, for example: [code snippet]... which is not what we want. You should decay the return type, i.e.: [code snippet]...''\\
    \textbf{Developer:} ``Done.''
\end{qoutebox}

\vspace{12pt}

\noindent\textbf{RQ3: How do developers treat code snippet suggestions in code reviews?}

To answer RQ3, we manually checked and analyzed the contextual information of code snippet comments, including the whole code review discussions and associated source code. 
% First, we looked through developers' responses to reviewers' code snippet suggestions to preliminarily assess the acceptance of developers. Then, we compared the code snippets suggested by reviewers with the changes made by developers to the source code in subsequent patches of the same code change or patches of other code changes (if the developer explicitly mentioned that the change is made in another code change). 
First, we looked through developers' responses to reviewers' code snippet suggestions. Second, we compared the suggested code snippets with the changes made by developers to the reviewed source code. We combined both textual and code information to conclude the acceptance of code snippet suggestions. According to our analysis results, the actions developers took towards reviewers' code snippet suggestions are in three categories:
\begin{itemize}
    \item \textbf{Accept:} (1) developers change the relevant code based on the code snippets provided by the reviewers or (2) developers clearly show a positive attitude toward the code snippet suggestions provided by the reviewers. \\
    For situation (1), Fig. \ref{fig: Example of a developer accepting a reviewer' code snippet suggestion} presents an example of a developer accepting a reviewer's code snippet suggestion. In this review comment, the reviewer provided a code snippet suggestion to make the code more readable (i.e., ``\textit{just nit: IMHO easier to read would be something like: [code snippet]}''), and the developer replied ``\textit{Done}'' and made corresponding code change to the relevant source code, which means that the developer accepted the code snippet suggestion.\\
    For situation (2), the reviewer provided a suggestion for optimizing the code and attached a relevant code snippet in the review comment below. The developer first thanked the reviewer and made it clear that he would try it out, which also indicates that the developer accepted the reviewer's code snippet suggestion.
    \begin{qoutebox}{white}{}
    \textbf{Link:} \url{http://alturl.com/hroeq}\\
    \textbf{Reviewer:} ``If we wanted to optimize as much as possible, we could limit to only the neutron\_pg\_drop row with something like: [code snippet]''\\
    \textbf{Developer:} ``Thanks for the tip, I'll definitely try that out...''
    \end{qoutebox}
    \item \textbf{Ignore:} developers neither respond to reviewers nor change the relevant code in the subsequent patchsets.
    \item \textbf{Not Accept:} (1) developers articulate totally different opinions towards reviewers' code snippet suggestions or (2) developers respond to reviewers but the developers do not change the relevant code or they do not change the relevant code according to the reviewers' suggestions.\\
    In the following review comment, the developer said that he would not change the current code to make the code more complex, which clearly shows that he would not accept the reviewer's code snippet suggestion.
    \begin{qoutebox}{white}{}
    \textbf{Link:} \url{http://alturl.com/vpw3e}\\
    \textbf{Reviewer:} ``How about adding: [code snippet] ...''\\
    \textbf{Developer:} ``... libvirt will do something similar but until we actually need that for some reason i would prefer not to add the extra complexity.''
    \end{qoutebox}
\end{itemize}

% Figure environment removed

This process was performed by the same two coders in the data analysis of RQ2. Most of the identified code snippet suggestions in RQ2 are trivial cases which are straightforward and simple to tell whether they were accepted, ignored, or not accepted. Of all the 2,322 code snippet suggestions identified in OpenStack and Qt communities, the two coders could not decide on 28 of the suggestions. For the 28 code snippet suggestions, the third author was involved to discuss with the two coders and reach an agreement.


% % Figure environment removed


% \begin{qoutebox}{white}{}
%     \textbf{Link:} \url{http://alturl.com/hroeq}\\
%     \textbf{Reviewer:} ``If we wanted to optimize as much as possible, we could limit to only the neutron\_pg\_drop row with something like: <code snippet>''\\
%     \textbf{Developer:} ``Thanks for the tip, I'll definitely try that out...''
% \end{qoutebox}


\noindent\textbf{RQ4: What are the reasons that developers do not accept code snippet suggestions in code reviews?}

To answer RQ4, we extracted the relevant data item listed in Table \ref{Data items extracted and their corresponding RQs} and analyzed all the code snippet suggestions which were not accepted by developers based on the results of RQ3. Through manually checking on the whole corresponding discussions among developers and reviewers, we used Open Coding and Constant Comparison methods to identify the reasons that developers do not accept code snippet suggestions in code reviews, and this process was similar to the data analysis for answering RQ2.



\subsection{Survey Study Design}
A survey is used to gather information from or about people to describe, compare, or explain their knowledge, attitudes, and behavior \citep{fink2003survey}. To validate the results of RQ1 and RQ2 and to obtain practitioners' insights on code snippets in code reviews (e.g., usage scenarios), we decided to conduct an industrial survey as a complementary data collection tool to understand practitioners' perceptions and current practices regarding code snippets in code reviews. A survey study could be conducted by self-administered questionnaires, telephone surveys, and one-to-one interviews to collect data \citep{kitchenham2008personal}. We finally chose to employ an online self-administered questionnaire to collect the information because using the online questionnaire could help us get evidence from potential participants which may come from different countries and facilitate the collection of responses from a large number of participants \citep{campbell2013coding}.

\subsubsection{Creating the Questionnaire and Recruitment of Participants}
We formulated the survey questionnaire covering five RQs (RQ1, RQ2, RQ5, RQ6, and RQ7). The questionnaire was prepared in both English and Chinese in order to get more responses. The questionnaire in English could help us invite potential participants from the World, and the questionnaire in Chinese could increase the probability of practitioners in China filling them out. To ensure that the meaning of the questionnaire is the same in both languages, the first two authors who are native Chinese speakers translated the questionnaire in English into Chinese, and the third author checked and refined the translation. The questionnaire is composed of seven parts: the Welcome page shows the questionnaire requirements, introduction, and an example of code snippet used in review comments, one question (SQ1) about participants' background information, two questions (SQ2 and SQ3) about validating RQ1 and RQ2, and three questions (SQ4, SQ5, and SQ6) about answering RQ5, RQ6, and RQ7 (see Table \ref{Survey questions on code snippets in code reviews} and Table \ref{Survey questions and their analysis methods for answering the RQs}). After preparing the survey questionnaire, we needed to select the survey participants. Our target participants are software developers with code review experience, and we used the following contact channels to invite the potential respondents with code review experience:
\begin{itemize}
    \item \textbf{Developers from the OpenStack and Qt communities:} To engage developers from the OpenStack and Qt communities in our survey study, we employed two ways: (1) send survey invitation emails to the 640 developers who had provided code snippets in code reviews collected in the dataset of our exploratory study and (2) send a survey invitation email to the developer mailing lists of OpenStack and Qt. For (1), when sending the survey invitation emails to the 640 developers, we first apologized for bothering them. Besides, we only emailed these developers once. For (2), we also sent the survey invitation email only one time to the developer mailing lists of OpenStack and Qt in order to contact a wide range developers of OpenStack and Qt. With these measures, developers from OpenStack and Qt could voluntarily choose whether to fill out the survey questionnaire with minimal interruption.
    % We collected 640 developers who had provided code snippets in code reviews from our dataset and sent survey invitation emails to them. We also sent the survey invitation to the developer mailing lists of OpenStack and Qt, which have potential participants for our survey study.
    \item \textbf{Developers from well-known software companies:} We contacted developers from well-known software companies in China, i.e., ByteDance, Alibaba, and Tencent, to participate in our survey study. These companies are the leading and most prestigious IT companies in China. ByteDance and Alibaba have been credited as the top 50 most innovative companies in 2023\footnote{\url{http://alturl.com/wef4g}}, while Tencent is the world's tenth most valuable company as of Feburary 2022\footnote{\url{http://alturl.com/n9rgs}}. To collect as many responses as possible, we also employed snowball sampling \citep{shull2007guide} by requesting theses developers to share the survey invitation with individuals or groups deemed as potential participants.
    % We contacted developers from well-known software companies in China and invited them as our potential participants. We also requested these developers to share the survey invitation with individuals or groups deemed as relevant participants.
    \item \textbf{Developers from professional software development groups:} We posted the survey invitation on software development groups listed in Table \ref{Posted Linkedin groups and the corresponding post links} on Linkedin, where software developers from around the world share their issues, experiences, and knowledge.
    \begin{table}[htbp]
    \renewcommand\arraystretch{1.5}
    \caption{Software development groups in Linkedin used to post our survey}
    \label{Posted Linkedin groups and the corresponding post links}
    \begin{tabular}{m{1cm}<{\centering}m{6.5cm}m{3.5cm}}  \hline
    \textbf{\#}     & \textbf{Linkedin Group}                                                                           & \textbf{URL}                 \\ \hline
    \textbf{LP1}    & Software Developer                                                                                & \url{http://alturl.com/aybty} \\ \hline
    \textbf{LG2}    & Agile and Lean Software Development                                                               & \url{http://alturl.com/fgvm3} \\ \hline
    \textbf{LP3}    & Software Engineer - Full Stack Developer                                                          & \url{http://alturl.com/cxkxo} \\ \hline
    \textbf{LP4}    & Java / J2EE / Core Java / Corejava / Java Developer / Software Engineer - (JAVA)                  & \url{http://alturl.com/bzd3y} \\ \hline
    \textbf{LP5}    & Python Developer / Full Stack Developer / Web Developer / Software Developer / Data Analyst       & \url{http://alturl.com/uoqz5} \\ \hline
    \textbf{LP6}    & Software Developer, Programmer and Architect ( Java | Python | PHP | C\# | C++ | GO | Swift)      & \url{http://alturl.com/v7y5p} \\ \hline
    \textbf{LP7}    & Software Engineer / Developer / Programmer / Data Analyst / Data Scientist / Data Engineer / RPA  & \url{http://alturl.com/5h74j} \\ \hline
    \end{tabular}
    \end{table}
\end{itemize}

Note that throughout the entire survey study, we upheld the privacy of OpenStack and Qt community members while gathering and utilizing information regarding code reviews. We tried our best to make sure that our research was conducted ethically, ensuring the confidentiality of the data collected, and all the data acquired was solely utilized for research purposes.



\subsubsection{Evaluating and Validating the Questionnaire}
Before disseminating the survey invitations, we conducted a pilot survey with developers from the OpenStack and Qt communities to evaluate and validate the questionnaire. For the 640 developers who had provided code snippets in code reviews that we collected in the dataset of our exploratory study, we ranked them based on the number of review comments they made as an indicator of activeness, and sent the survey invitations to the top 100 most active code reviewers.
% We sent our survey invitations to a set of 100 participants randomly selected from the 640 developers who had provided code snippets in code reviews from our dataset for a pilot survey. 
Out of the 100 participants contacted for the pilot survey, 10 replied. By looking through the pilot results, we checked the understandability of the survey questions and the effectiveness of each question. We found that the length of the survey is appropriate, the questions are clear and easy to understand, and the answers to the questions are meaningful, and consequently we did not refine the questionnaire. Finally, our survey questionnaire\footnote{\url{https://forms.gle/7eKfjhzHtnBXhEMJ7}} is composed of 4 closed-ended questions and 2 open-ended questions. Table \ref{The welcome page of the survey} presents the Welcome page of the survey questionnaire, and Table \ref{Survey questions on code snippets in code reviews} presents the survey questions of the questionnaire.
% and Appendix.~\ref{appendix} shows the whole questionnaire.

\begin{table}[htbp]
\renewcommand\arraystretch{1.5}
\caption{The Welcome page of the survey questionnaire}
\label{The welcome page of the survey}
\centering
\begin{tabular}{|p{11.7cm}|}  
\hline  
\vspace{4pt}
\textbf{The goal} of this survey is trying to understand the practice and purposes of providing code snippets in code reviews. This questionnaire is designed to capture the knowledge and experience of industrial practitioners in the use of code snippets in code reviews. Our questionnaire contains 4 closed-ended questions and 2 open-ended questions, which may take about \textbf{3-5 minutes}. \\ 
\vspace{3pt}
Note that no personal information will be involved in this questionnaire and nor will your response be disclosed to the third parties. Please feel free to contact us if you have any questions or concerns.\\
\vspace{3pt}
Thank you very much for your participation! \\ 
\vspace{4pt}
\\ \hline
\vspace{4pt}
As shown in the example below, a reviewer made a suggestion towards code style and provided a \textbf{code snippet} to the developer in his/her code review comment, which intends to help the developer write more readable code. \\ 
\vspace{4pt}
% Figure removed
\vspace{4pt} 
\\ \hline
\end{tabular}
\end{table}


\begin{table}[htbp]
\renewcommand\arraystretch{1.5}
\caption{Survey questions on code snippets in code reviews}
\label{Survey questions on code snippets in code reviews}
\centering
\begin{tabular}{|p{0.5cm}<{\centering}|p{2cm}|p{4cm}|p{4cm}|}  
\hline  
\textbf{ID}     &\textbf{Type of Questions}                   & \textbf{Questions}                        & \textbf{Type of Answers}  \\ \hline
SQ1 &Background information about participants    & Q1. How many years have you been involved in software development?                              
& \textless 1 year / 1 $\sim$ 3 years / 3 $\sim$ 5 years / \textgreater 5 years \\ \hline
SQ2 &Question for validating RQ1                  & Q2. How often do you provide code snippets in review comments when conducting code reviews?     
& Never / Very few / Sometimes / Often \\ \hline
SQ3 &Question for validating RQ2                  & Q3. (Multiple Choice) As a reviewer in code reviews, for what purposes do you provide code snippets in review comments?                           
& Improving Code Implementation - Point out alternative solutions or advice to improve the current code in the patchsets (e.g., design or detailed implementation). / Following Code Style - Make the style of the current code consistent with the best code conventions. / Correcting Code - Show what kind of mistakes developers have made in current code and to correct the error code. / Complementing Code Implementation - Remind developers that the current code implementations are incomplete and they should complement the code with the provided code snippets. / Elaborating - Help reviewers supplement their explanations or illustrations. / Providing Context - Provide additional information related to what reviewers had said in the review comments. / Other \\ \hline
SQ4 &Question for answering RQ5                   & Q4. (Optional) As a reviewer in code reviews, in which scenarios you tend to provide code snippets in code review comments?     
& Free text \\ \hline
SQ5 &Question for answering RQ6                   & Q5. As a developer in code reviews, what is your attitude towards the provided code snippets in code reviews?                  
& Positive. I think it is a good thing to see review comments that contain code snippets. / Neutral. I don't care if review comments contain code snippets or not. / Negative. I think it is troublesome to see review comments that contain code snippets. / Other  \\ \hline
SQ6 &Question for answering RQ7                   & Q6. (Optional) As a developer in code reviews, what characteristics of code snippets would you like reviewers to provide in code review comments?   & Free text \\ \hline
\end{tabular}
\end{table}



\subsubsection{Conducting the Survey and Analyzing Survey Data}
After finalizing the questionnaire, we sent out the survey invitations to the participants we recruited. The invitations were sent on March 26, 2023, and as of May 31, 2023, a total of 63 responses were collected. For the 63 participants who filled out the survey questionnaire, 43 are developers from OpenStack and Qt communities or LinkedIn groups, while 17 are developers from well-known software companies in China. However, we were unable to tell the response rate for both open-source and closed-source cases. This is because we lacked the information regarding the number of developers who received our survey invitation emails from the developer mailing lists of OpenStack and Qt. Additionally, the anonymity of the survey responses made it challenging to identify the sources of the respondents.

We used descriptive statistics \citep{kaur2018descriptive}, Open Coding, and Constant Comparison techniques \citep{glaser2017discovery} to analyze the quantitative (i.e., closed-ended questions) and qualitative (i.e., open-ended questions) responses to the survey questions. Table \ref{Survey questions and their analysis methods for answering the RQs} presents the survey questions and their analysis methods for answering the RQs. 

\begin{table}[htbp]
\renewcommand\arraystretch{1.5}
\centering
\caption{Survey questions and their analysis methods for answering the RQs}
\label{Survey questions and their analysis methods for answering the RQs}
\begin{tabular}{p{3cm}<{\centering}lp{2cm}<{\centering}}
\hline
\textbf{Survey Question} & \textbf{Data Analysis Method}           & \textbf{RQ}           \\ \hline
SQ1                       & Descriptive Statistics                 & Demographic           \\ \hline
SQ2                       & Descriptive Statistics                 & RQ1                   \\ \hline
SQ3                       & Descriptive Statistics                 & RQ2                   \\ \hline
SQ4                       & Open Coding and Constant Comparison    & RQ5                   \\ \hline
SQ5                       & Descriptive Statistics                 & RQ6                   \\ \hline
SQ6                       & Open Coding and Constant Comparison    & RQ7                   \\ \hline
\end{tabular}
\end{table}

According to the survey results, the distribution of software development experience of the survey participants is shown in Fig.~\ref{fig: Distribution of software development work experience of the participants}. We can find that 79.4\% (11.1\% + 68.3\% = 79.4\%) of the participants have more than 3 years of software development experience, and nearly 70\% of the participants have more than 5 years of software development experience, which somewhat indicates that the survey results are representative.
% Figure environment removed

Note that two examples are provided for the two open-ended survey questions respectively to help participants better understand the terms used in the questions. The examples were formed based on the results of the exploratory study, which may somewhat restrict practitioners from answering the two open-ended survey questions. In order to get valid survey results, we excluded the responses that are inconsistent with the question, randomly filled in, or meaningless when we analyzed the data. During the process of analyzing the data, the first three authors discussed inconsistent opinions till they reached an agreement. The survey results have been provided in our dataset \citep{replpack}.


