\section{Details on parameter optimization}
\label{sec:details-param-optim}

\subsection{Penalized log-likelihood criterion}
\label{sec:penalized-loglik}

We select the parameters~$\alpha$ and~$\sigma^2$ of the the Szeg√∂
kernel~$\sigma^2 k_\alpha$ (cf.~Section~3 in the article), together
with the dominant poles $\mathbf p = \left( p_1, \ldots, p_K \right)$
in the case of the hybrid method, by maximizing a penalized
log-likelihood criterion:
\begin{equation}\label{equ:penalized-loglik}
  J\left( \alpha, \sigma^2, \mathbf p \right)
  \;=\;  \max_{\mathbf r \in \Cset^K}
  \ln\left( p\left(\mathbf y|\alpha, \sigma^2, \mathbf p, \mathbf r\right) \right)
    + \ln(\rho(\alpha)),
\end{equation}
%
where the first term is the log-likelihood of the model, maximized
(profiled) analytically with respect to the residues~$\mathbf r$ of
the rational mean function model~$m$, and the second term is a penalty
term, designed to pull~$\alpha$ away from~$0$.

More precisely, we take for~$\rho(\alpha)$ the probability density
function (pdf) of a log-normal random variable with
parameters~$\mu_{\alpha}$ (to be specified) and~$\sigma_\alpha = 3$; %
in other words, we use a ``vague'' prior distribution on~$\alpha$,
such that~$\log(\alpha)$ is Gaussian with mean~$\mu_\alpha$ and
variance~$\sigma_\alpha^2$. %
The parameter~$\mu_\alpha$ is chosen in such a way that the log-normal
density for~$\alpha$ has its mode
at~$\left| \Omega \right| = \omega_{\mathrm{max}}
-\omega_{\mathrm{min}}$, or equivalently that the prior density
for~$\alpha / \left| \Omega \right|$ has its mode at~$1$. %
Using that the mode of the lognormal density is
at~$e^{\mu_\alpha - \sigma_\alpha^2}$, we deduce that
$\mu_\alpha = \sigma_\alpha^2 + \ln \left| \Omega \right|$. %
The resulting pdf
%
\begin{equation}
  \rho(\alpha) \;=\; \frac 1{\alpha\sigma_\alpha\sqrt{2\pi}}
  \exp\bigl(\frac {-(\ln(\alpha) -\mu_\alpha)^2}{2\sigma_\alpha^2}\bigr)
\end{equation}
%
is shown in Figure~\ref{fig:Prior} for~$\left| \Omega \right| = 1$. %
It can be seen that the chosen parameters allow for the choice
of~$\alpha$ within a range of several orders of magnitude.

% Figure environment removed

The penalized log-likelihood criterion~\eqref{equ:penalized-loglik} is
maximized numerically using bound-constrained gradient-based
optimization---more precisely, interior point algorithm available from
Matlab's \texttt{fmincon} function---with a multistart procedure. %
Details about the bounds for the search domain and the initial points
for the local search are provided in
Sections~\ref{sec:bounds-search}--\ref{sec:starting-points}.

\begin{remark}
  This parameter selection procedure can be considered as
  \emph{maximum a posteriori} estimate in the Bayesian sense. %
  Indeed, the penalized log-likelihood
  criterion~\eqref{equ:penalized-loglik} can be seen as the
  log-posterior density, up to a constant, assuming a lognormal prior
  for~$\alpha$ and an improper uniform prior for all the other
  parameters.
\end{remark}

\begin{remark}%
  Even when the complex kernel~$k$ is strictly positive definite, the
  distribution of data under the GP model does not always admit a
  probability density function with respect to Lebesgue's measure
  on~$\Rset^{2n}$ (cf.~related discussion regarding the strict
  positive definiteness of~$\tilde k$ in Section~2.2 of the
  article). %
  When this happens, a suitable reference measure has to be used in
  order to define the likelihood function. %
  For instance, when the pseudo-kernel $c(s, s_0) = k(s, s_0^*)$ is
  used to enforce the symmetry condition, the value at~$\omega = 0$
  must be real, which yields a degenerate distribution if the response
  is evaluated at~$\omega = 0$: %
  the solution is simply to remove the imaginary part of the
  response at this point from the vector of observed variables. %
  See Section~2 of~\cite{lataire2016transfer} for related
  considerations.
\end{remark}


\subsection{Bounds for the search domain}
\label{sec:bounds-search}

We optimize with respect to the transformed kernel parameters
\begin{align*}
  \theta_1 & \;=\; \ln\left( \frac{\sigma^2}{2\pi} \right),\\
  \theta_2 & \;=\; \alpha,
\end{align*}
within the optimization bounds
\begin{align*}
  -15 & \;\le\; \theta_1 \;\le\; 15,\\
  10^{-6}|\Omega| & \;\le\; \theta_2 \;\le\; |\Omega|,
\end{align*}
where
$\left| \Omega \right| = \omega_{\max} - \omega_{\min} = \max \left\{
  \omega_i,\, 1 \le i \le n \right) - \min \left\{ \omega_i,\, 1 \le i
  \le n \right)$.

For the hybrid model, the poles are optimized simultaneously with the
kernel hyper-parameters, within the bounds
\begin{align*}
  -|\Omega| & \;\le\; \Re\left( p_i \right) \;\le\; -10^{-6}|\Omega|, \\
  \max\left\{10^{-6}|\Omega|,~\omega_{\min}-\frac{|\Omega|}3\right\}
            & \;\le\; \Im\left(p_i\right) \;\le\; \omega_{\max}+\frac{|\Omega|} 3.
\end{align*}
The bounds are enlarged, if needed, in such a way that all the poles
in the starting point of the optimization are contained within them.


\subsection{Starting point(s)}
\label{sec:starting-points}

We use a multistart procedure to optimize the kernel
parameters~$\alpha$ and~$\sigma^2$. %
More precisely, we start $N_\mathrm{ms} = 20$ local optimizations,
with the initial value of~$\alpha$ uniformly distributed
between~$10^{-6}|\Omega|$ and~$|\Omega|$. %
For a given value of~$\alpha$, and a given choice of poles in the case
of the hybrid algorithm, the GLS (generalized least squares) estimate
is used as a starting point for~$\sigma^2$.

For the poles in the hybrid algorithm, we start from equidistant
poles~$p_1, \ldots, p_K$ close to the frequency axis:
\begin{equation*}
  p_k \;=\; - \delta_{\Re}\, \left| \Omega \right|
  \,+\, \i \left( \omega_{\min} + \left( k -\frac{1}{2} \right)\, \delta_\Im \right),
  \qquad 1 \le k \le K,
\end{equation*}
where $\delta_{\Re} = 10^{-3}$ (weak attenuation) and
$\delta_\Im = \left| \Omega \right| / K_{\max}$. %
The kernel parameters are initialized a\textcolor{blue}{s} described previously (with the
GLS estimate for~$\sigma^2$ and a multi-start procedure
for~$\alpha$).
