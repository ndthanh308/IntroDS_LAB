\section{Introduction}
This supplementary material is structured in the following way. %
We first present a non-intrusive implementation of the new method presented in
the paper in Section~\ref{sec:non-intrusive}. 
Section~\ref{sec:additional_results} reports additional numerical results and details on the examples used in the main text. In particular, some investigations related to the estimation of the kernel parameter $\alpha$ are given in Section~\ref{sec:alpha}, a partial fraction representation for the circuit model is derived in Section~\ref{sec:circuit}, Section~\ref{sec:kmax_study} discusses the choice and influence of the maximum number of poles in the mean model $K_{\mathrm{max}}$ and results for a spiral antenna, which are comparable to the PAC-MAN model shown in the main paper, are contained in Section~\ref{sec:add-num-results}. %
Finally, some theoretical results regarding circular complex/real RKHS
are collected in Section~\ref{sec:circular}.


\section{Non-intrusive implementation}
\label{sec:non-intrusive}

\subsection{Zero-mean case}

The main idea is to construct an isomorphic real-valued GP
$\tilde g(\tilde{\mathbf x})\sim \operatorname{GP}(0,\tilde k)$ on an
\textit{augmented input space}
$\tilde{\mathbf x}\in \bigl(\mathbb R^n\times \{0,1\}\bigr)$, s.t.,
\begin{equation}
  \begin{aligned}
    \tilde g(\begin{bmatrix}\mathbf x&0\end{bmatrix}) &= \operatorname{Re}[g(\mathbf x)],\\
    \tilde g(\begin{bmatrix}\mathbf x&1\end{bmatrix}) &= \operatorname{Im}[g(\mathbf x)].
  \end{aligned}\label{eq:idea}
\end{equation}
The augmented training data
$\bigl(\tilde {\mathbf x},\tilde y\bigr) \in \bigl(\mathbb R^n \times
\{0,1\}\bigr) \times \mathbb R$ is for each observation
$\bigl(\mathbf x^{(i)}, y^{(i)}\bigr)\in \mathbb R^n\times\mathbb C$
obtained as:
\begin{align*}
  \tilde{\mathbf x}^{(i,1)} &= \begin{bmatrix}
    \mathbf x^{(i)} & 0
  \end{bmatrix},~~\tilde {y}^{(i,1)} = 
                      \operatorname{Re}[y^{(i)}],\\
  \tilde{\mathbf x}^{(i,2)} &= \begin{bmatrix}
    \mathbf x^{(i)} & 1
  \end{bmatrix},~~\tilde {y}^{(i,2)} = 
                      \operatorname{Im}[y^{(i)}].
\end{align*}
%
The \textit{new} covariance function $\tilde k$ can be derived by
enforcing~\eqref{eq:idea},
\begin{align*}
  \tilde k(\tilde {\mathbf x}, \tilde {\mathbf x}')&=\tilde k([\mathbf x~j],[\mathbf x'~j'])
  =\begin{cases}
    \frac 12\operatorname{Re}[k(\mathbf x,\mathbf x')+c(\mathbf x,\mathbf x')] &j=j'=0\\
    \frac 12\operatorname{Re}[k(\mathbf x,\mathbf x')-c(\mathbf x,\mathbf x')] &j=j'=1\\
    \frac 12\operatorname{Im}[-k(\mathbf x,\mathbf x')+c(\mathbf x,\mathbf x')] &j=0,j'=1\\
    \frac 12\operatorname{Im}[k(\mathbf x,\mathbf x')+c(\mathbf x,\mathbf x')] &j=1,j'=0
  \end{cases},
\end{align*}
%
Note that this approach requires to define the modified covariance
function $\tilde k$, however, no (other) internal functions of
existing GP implementations need to be able to cope with complex
numbers, which is why we refer to the implementation as
\emph{non-intrusive}.


\subsection{Linear model in the mean function}

Consider now the superposition
%
\begin{equation*}
  \underline g(\mathbf x) = g(\mathbf x) + \mathbf h(\mathbf x)^\mathrm{T} \mathbf b
\end{equation*}
%
of a mean-free (real/)complex Gaussian Process
$g(\mathbf x)\sim \operatorname{CGP}(0, k, c)$ and a complex linear
model, where $h(\mathbf x):\mathbb R^n\rightarrow \mathbb C^m$ denote
explicit basis functions and $\mathbf b\in\mathbb C^m$ the
corresponding coefficients.
%
Define the augmented process %
\begin{equation*}
  \tilde{\underline g}(\tilde{\mathbf x}) =
  \tilde g(\tilde{\mathbf x})+\tilde{\mathbf h}(\tilde{\mathbf x})^\mathrm{T}\tilde{\mathbf b},  
\end{equation*}
%
where $\tilde g(\tilde{\mathbf x})~\sim \mathrm{GP}(0, \tilde k),$
$\tilde{\mathbf b}\in \mathbb R^{2m}$,
$\tilde{\mathbf h}(\tilde{\mathbf x}):\mathbb
R^n\times\{0,1\}\rightarrow \mathbb R^{2m}$ and we require, similarly
as in the last subsection, that
\begin{align*}
  \tilde{\underline g}([\mathbf x~0])&=\operatorname{Re}[\underline g(\mathbf x)],\\
  \tilde{\underline g}([\mathbf x~1])&=\operatorname{Im}[\underline g(\mathbf x)].
\end{align*}
%
Incorporating~\eqref{eq:idea}, we can conclude that
\begin{align*}
  \tilde{\mathbf h}([\mathbf x~0])^\mathrm{T}\tilde{\mathbf b}&=\operatorname{Re}[\mathbf h(\mathbf x)^\mathrm{T} \mathbf b]=\operatorname{Re}[\mathbf h(\mathbf x)]^\mathrm{T}\operatorname{Re}[\mathbf b]-\operatorname{Im}[\mathbf h(\mathbf x)]^\mathrm{T}\operatorname{Im}[\mathbf b]\\
  \tilde{\mathbf h}([\mathbf x~1])^\mathrm{T}\tilde{\mathbf b}&=\operatorname{Im}[\mathbf h(\mathbf x)^\mathrm{T} \mathbf b]=\operatorname{Im}[\mathbf h(\mathbf x)]^\mathrm{T}\operatorname{Re}[\mathbf b]+\operatorname{Re}[\mathbf h(\mathbf x)]^\mathrm{T}\operatorname{Im}[\mathbf b].
\end{align*}
needs to be fulfilled.
%
This can be achieved by setting 
\begin{equation*}
  \tilde{\mathbf h}(\tilde{\mathbf x})=\tilde{\mathbf h}([\mathbf x~j])=\begin{bmatrix}
    \begin{cases}
      +\operatorname{Re}[\mathbf h(\mathbf x)] & j=0\\
      +\operatorname{Im}[\mathbf h(\mathbf x)] & j=1
    \end{cases}&\hspace{2em}
    \begin{cases}
      -\operatorname{Im}[\mathbf h(\mathbf x)] & j=0\\
      +\operatorname{Re}[\mathbf h(\mathbf x)] & j=1
    \end{cases}
  \end{bmatrix}^{\mathrm{T}},
\end{equation*}
%
which leads to the coefficients vector
\begin{equation*}
  \tilde{\mathbf b} = \begin{bmatrix}
    \operatorname{Re}[\mathbf b]\\
    \operatorname{Im}[\mathbf b]
  \end{bmatrix}.
\end{equation*}

\section{Complements to the numerical results}
\label{sec:additional_results}

\subsection{Estimation of alpha for low order rational functions}
\label{sec:alpha}
We conduct an additional study to investigate whether the selected values of $\alpha$ provide insights into the properties of the approximated function. We employ the Szegö kernel-based approximation using log-likelihood maximization as described in the main part of the paper. We consider low order rational functions, as introduced in Section 2 of the paper, but vary the real part of the poles. The associated functions 
\begin{equation}\label{eq:Frat_alpha}
	F_{\mathrm{rat}, \beta}(\i\omega; \beta)
	= \frac 1 {\i\omega-(-\beta)}
	+ \frac {0.5}{\i\omega-(-\beta-0.5\i)}
	+ \frac {0.5}{\i\omega-(-\beta+0.5\i)},\;
	\omega\in[0,1],
\end{equation}
where $F_{\mathrm{rat}, \beta}(\i\omega; \beta) \in H^2_\mathrm{sym}(\Gamma_{\beta+\epsilon})$,
are plotted in Fig.~\ref{fig:rational_functions}, along with the respective selected values of $\alpha$, where we have always employed $n=20$ training points. It can be seen that the selected values of $\alpha$ are quite close to the real part of the poles of the function for the case with and without the Hermitian symmetry condition. Unfortunately, this relation is not easy to reveal or even investigate for more complicated examples.

% Figure environment removed


\subsection{Partial fraction representation of RLC circuit}
\label{sec:circuit}
The residues $c_i, c_i^*$ and poles $a_i, a_i^*$ of the partial fraction representation of the electric circuit admittance  
\begin{equation}
  Y(s) = \sum_{i=1}^N \frac {c_i}{s-a_i}+\frac
  {c_i^*}{s -a_i^*},
\end{equation}
are given as
\begin{align}
a_i &= \frac {-R_i}{2L_i}+ \i\sqrt{\frac 1 {L_iC_i}-\Bigl(\frac {R_i}{2L_i}\Bigr)^2},\\
c_i &= \frac {a_i} {L(a_i-a_i^*)} = \frac {\sqrt{\frac 1 {L_iC_i}-\Bigl(\frac {R_i}{2L_i}\Bigr)^2}+\frac {R_i}{2L_i}\i}{2L_i\sqrt{\frac 1 {L_iC_i}-\Bigl(\frac {R_i}{2L_i}\Bigr)^2}},
\end{align}
where we assumed an underdamped system, i.e. \begin{equation}\frac {R_i}2\sqrt{\frac{C_i}{L_i}}<1,\label{[eq:cond_underdamped}\end{equation} which implies that the argument of the square roots is positive.


\subsection{The choice of $K_\mathrm{max}$}
\label{sec:kmax_study}

For the experiments contained in the paper we limit the maximum number of poles pairs to $K_{\max} = \min\{5,\lfloor n/4 \rfloor\}$, where the limit of $5$ is considered as an arbitrary \textit{small} number, based on the assumption that the approximated functions have a small number of dominant poles. Note that increasing $K_\mathrm{max}$ would increase the computational cost of the model selection. In Fig.~\ref{fig:study_wrt_kmax}, we demonstrate that the particular choice of this value is indeed not very important for the particular random circuit realization with admittance $Y_2$ and $n=50$ training points, by computing repeated approximations based on different values of $K_\mathrm{max}$. It can be observed that for $K_\mathrm{max} > 2$ the different selected approximations have a similar error magnitude. The black line additionally indicates the selected number of pole pairs. Note that, even when the same number of pole pairs is chosen for different $K_\mathrm{max}$, the respective approximations are not necessarily exactly the same, as different initial values are employed for the gradient-based optimization as explained in Section 3.2 of the paper. 


% Figure environment removed


\subsection{Additional numerical example}
\label{sec:add-num-results}

% Figure environment removed
The model is a spiral antenna, depicted in Figure~\ref{fig:spiral},  where we consider the reflection coefficient $S_{11}$ on a frequency range of [\SI{4}{GHz},\,\SI{6}{GHz}] as quantity of interest. The data sets are obtained using the boundary element method in CST Microwave Studio \cite{CST_2019aa}. The results are qualitatively the same as for the PAC-MAN model, see  Fig.~\ref{fig:spiral} (bottom). 


\section{Complements for the theoretical section}

\subsection{Circular complex/real RKHSs}
\label{sec:circular}    

\begin{definition}
  We say that a complex/real RKHS is \emph{circular} if it has a
  vanishing pseudo-kernel.
\end{definition}

The terms ``proper'' or ``strictly complex'' are also sometimes used
instead of ``circular'', in the statistics and signal processing
literature, for the case where the
pseudo-covariance of a complex Gaussian random vector or function
vanishes (see, e.g., \cite{picinbono:1996, boloix2018complex}).

\begin{theorem} \label{thm:circular}
  Let $H_\Cset$ denote a complex RKHS with kernel~$k_0$, and let $H$
  denote the complex/real RKHS obtained by considering~$H_\Cset$ as a
  real vector space, endowed with the inner product:
  $\left<f, g\right> \mapsto \Re\left( \left<f, g\right>_\Cset \right)$. %
  Then $H$ is the circular complex/real RKHS with complex kernel
  $k = 2 k_0$.  
\end{theorem}

Since a complex/real RKHS is uniquely characterized by its $(k, c)$
pair, the converse holds as well: %
given a circular complex/real RKHS~$H$ with complex kernel~$k$, there
is a unique complex RKHS~$H_\Cset$ (namely, the complex RKHS with
kernel $k_0 = \frac{1}{2} k$) such that $H$ is obtained from~$H_\Cset$
as in Theorem~\ref{thm:circular}.

\begin{proof}
  The main idea is already included in the proof of Theorem~2.14 of
  the article, but we give here a slightly more detailed version. %
  Let $\varphi_\re$ and~$\varphi_\im$ denote the real and imaginary
  evaluation kernels of~$H$. %
  Then, for all~$f \in H$ and~$s \in \Sset$,
  \begin{equation*}
    \left< f,\, k_0(\cdot,s) \right>
    \;=\; \Re\left( \left< f,\, k_0(\cdot,s) \right> \right)
    \;=\; \Re\left( f(s) \right)
  \end{equation*}
  and
  \begin{align*}
    \left< f,\, \i k_0(\cdot,s) \right>
    & \;=\; \Re\left( \left< f,\, \i k_0(\cdot,s) \right> \right)
      \;=\; \Re\left( -\i\, \left< f,\, k_0(\cdot,s) \right> \right)\\
    & \;=\; \Re\left( -i\ f(s) \right)
      \;=\; \Im\left( f(s) \right),
  \end{align*}
  which proves that~$\varphi_\re = k_0$ and~$\varphi_\im = \i k_0$. %
  The complex kernel and pseudo-kernel of~$H$ are thus given by
  \begin{align*}
    k & \;=\; \varphi_\re - \i \varphi_\im \;=\; 2 k_0,\\
    c & \;=\; \varphi_\re + \i \varphi_\im \;=\; 0.
  \end{align*}
\end{proof}


\subsection{A relation between the Szegö and rational quadratic kernels}
\label{sec:relation-szego-rq}

Consider the Szegö kernel on $H^2(\Gamma_\alpha)$:
\begin{align*}
  k_\alpha\left( s, s_0 \right)
  & \;=\; \frac 1 {2\pi\left( 2\alpha + s + s_0^* \right)}\\
  & \;=\; \frac{1}{2\pi}\, \frac{%
    \left( 2\alpha + x + x_0 \right) - \i \left( y - y_0 \right)}{%
    \left( 2\alpha + x + x_0 \right)^2 + \left( y - y_0 \right)^2},
\end{align*}
where $s=x+\i y$, $s_0 = x_0 + \i y_0 \in \Gamma_\alpha$.
%
In the circular case, the corresponding kernels for the real and
imaginary parts are given by:
%
\begin{equation*}
  k_\re (s, s_0) \;=\; k_\im (s, s_0)
  \;=\; \frac{1}{4\pi}\, \frac{2\alpha + x + x_0}{%
    \left( 2\alpha + x + x_0 \right)^2 + \left( y - y_0 \right)^2},
\end{equation*}
%
For a fixed value of~$x = x_0 > -\alpha$, this is of the form
%
\begin{equation*}
  (y, y_0) \;\mapsto\; \frac{1}{4\pi}\, \frac{A}{%
    A^2 + \left( y - y_0 \right)^2},
  \qquad \text{with } A = 2\alpha + x + x_0 > 0,
\end{equation*}
which is a special case of the so-called \emph{rational quadratic}
kernel (see, e.g.,~\citeSM{rasmussen:2006:gpml, sollich:2004:using}),
also called \emph{generalized inverse multiquadric} kernel (see,
e.g.,~\citeSM{hu:1998:collocation, bozzini:2013:generalized}).

\phantomsection%trick
\bibliographystyleSM{siamplain}
\bibliographySM{references}
