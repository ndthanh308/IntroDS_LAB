\label{sec:hybrid}
We focus from now, unless otherwise specified, on functions satisfying
the property $f^*(s)=f(s^*)$, and we employ the Szegö
kernel~\eqref{equ:szego}, together with the
pseudo-kernel~\eqref{equ:pseudo-szego}, for complex/real interpolation. %
In practice, the convergence of complex/real RKHS interpolation can be
significantly slower than that of rational approximations techniques
(such as AAA or VF) when the function has a few dominant poles~$p_i$,
i.e., poles with small attenuation~$\Re[p_i] \approx 0$. %
In this section, we discuss how complex/real RKHS interpolation with
the Szegö kernel and associated pseudo-kernel can be combined with a
small number of rational basis functions for the approximation of such
frequency response functions.


\subsection{Gaussian process model}
\label{sec:conjPoles}
We propose to use a complex GP model with rational mean
function~$m = \sum_{\ell = 1}^L \beta_\ell h_\ell$
(cf.~Remark~\ref{rem:semi-RKHS}), covariance
function~$\sigma^2 k_\alpha$ and pseudo-covariance
function~$\sigma^2 c_\alpha$, %
where $k_\alpha$ denotes the Szegö kernel~\eqref{equ:szego},
$c_\alpha$ the associated pseudo-kernel~\eqref{equ:pseudo-szego}, %
and $\sigma^2$, $\alpha$, $\beta_1$, \ldots, $\beta_L$ are
real parameters with $\sigma^2 > 0$ and~$\alpha > 0$. %
For the mean function~$m$ we assume a rational function satisfying the
property $m^*(s) = m(s^*)$, of the form
\begin{equation}\label{eq:basis}
  m(s) = \sum_{i=1}^K \left\{ \frac{1}{s - p_i}\, r_i \,+\, \frac{1}{s - p_i^*}\, r_i^* \right\},
\end{equation}
with residues $r_1, \ldots, r_K \in \Cset$ and (stable) complex
conjugate poles $p_1, p_1^*, \ldots, p_K, p_K^* \in \Cset$ such that
$\Re(p_i) < 0$ and~$\Im(p_i) > 0$ for all~$i$. %
This representation is similar to the one used in VF
\cite{gustavsen2006improving, gustavsen1999rational}. %
Equation~\eqref{eq:basis} can be rewritten
as~$m = \sum_{\ell = 1}^L \beta_\ell h_\ell$ with $L = 2K$,
\begin{equation*}
  \beta_\ell = \begin{cases}
    \Re(r_i) & \text{if } \ell = 2i-1,\\
    \Im(r_i) & \text{if } \ell = 2i,
  \end{cases}
  \quad \text{and} \quad
  h_\ell(s) = \begin{cases}
    \frac{1}{s - p_i} + \frac{1}{s - p_i^*} & \text{if } \ell = 2i-1,\\
    \frac{\i}{s - p_i} - \frac{\i}{s - p_i^*} & \text{if } \ell = 2i.
  \end{cases}
\end{equation*}
Note that $m$ is an element of $H^2_\mathrm{sym}(\Gamma_{\alpha'})$
with $\alpha' = \min_{1\le i \le K} \left| \Re(p_i) \right| + \epsilon, \epsilon > 0$. %
For simplicity we only consider complex conjugate poles in~\eqref{eq:basis}, but
real poles could be included as well, as in VF. %
In the context of the present work, we typically consider a small
number~$K$ of pole pairs ($K \le K_{\mathrm{max}} = \min \left( 5, \lfloor n/4 \rfloor \right)$ in the examples).

For a given number~$K$ of pole pairs, we select the hyper-parameters
$\sigma^2$, $\alpha$, $\mathbf p = \left( p_1, \ldots, p_K \right)$
and $\mathbf r = \left( r_1, \ldots, r_K \right)$ by maximization of a
penalized log-likelihood function, where the penalty stems from a vague
log-normal prior on~$\alpha$; %
see Supplementary Material for details. %
An original procedure for the selection of an appropriate number~$K$
of pole pairs will be presented in the next section.

\begin{remark}
  Note that we do not include a constant basis function, as is
  usually done in Gaussian process modeling, to ensure that the
  interpolant satisfies the desired property (namely, goes to zero)
  when $\omega \to \pm\infty$.
\end{remark}

\begin{remark}
  Alternatively, the residues~$r_1, \ldots, r_K$ could be integrated
  out analytically using a Gaussian prior, resulting in additional
  terms in the covariance and pseudo-covariance functions of the GP;
  see, e.g., \cite{hallemans2022frf}. %
  This would allow the uncertainty on the residues, for a given set of
  poles, to be reflected in the uncertainty quantification (posterior
  variances) produced by the GP model. %
  We do not pursue this idea further in this article, since our focus is
  on interpolation rather than uncertainty quantification.
\end{remark}


\subsection{Adaptive pole selection}
\label{sec:adap_proc}

Selecting a suitable number~$K$ of pole pairs to be included in the
mean function~\eqref{eq:basis} is a crucial step to ensure good
accuracy of the proposed hybrid method. %
In this section we propose a model selection procedure to select this
number automatically, in a data-driven manner. %
While this procedure relies on the well-established idea of
(leave-one-out) cross-validation, it contains an original ingredient
in the form an ``instability penality'', which will be described
below.

First we build $K_{\max} + 1$ interpolants~$f^{(K)}_n$, where the
superscript $K$ indicates the number of pole pairs, ranging from~$0$
(zero-mean Gaussian process model) to~$K_{\max}$. %
Following standard VF practice \cite{gustavsen1999rational}, we begin
with the maximum number of poles, $K = K_{\max}$, using an equidistant
distribution of poles close to the frequency axis as a starting point
for optimization. %
The other interpolants are then constructed iteratively, going
backwards: at each step optimization is initialized using~$K$ of
the~${K + 1}$ poles selected at the previous step, %
by removing the least relevant pole according to the (penalized)
log-likelihood function. %

Model selection is then based on leave-one-out (LOO) cross-validation,
i.e., on the error indicators
\begin{equation}
  \epsilon^K_\LOO
  = \frac 1 n \sum_{i=1}^n \left| f(\i \omega_i)
    - \hat f^{(K)}_{n-1,i} (\i\omega_i) \right|^2,~~K=0,1,\ldots,K_{\max},
  \label{eq:LOO}
\end{equation} 
where $\hat f^{(K)}_{n-1,i}$ denotes a model constructed without the
$i$-th data point. %
Keeping the poles and kernel hyper-parameters fixed, when removing points, makes it possible to reduce the computational effort, but
was found to introduce an undesired preference for models with a
larger number of poles. %
Hence, we employ the LOO criterion with re-tuning, using the poles and
hyper-parameters of ~$f_n^{(K)}$ as an initial guess when
constructing~$\hat f^{(K)}_{n-1, i}$, $1 \le i \le n$.

% Figure environment removed

Furthermore, we introduce an additional penalty term, which also takes
\textit{global} model variations into account. %
This approach can be motivated by the example illustrated in
Figure~\ref{fig:loo_selection_issue} (top). %
The corresponding vibro-acoustic benchmark model will be described in
Section~\ref{sec:numerics}, however, here we simply consider the
approximation of the dashed function, based on interpolation of the
training points (black dots), as a general example. %
At the top, it can be observed that the LOO criterion \eqref{eq:LOO}
leads to the selection of a model (solid lines)
$\hat f_n^{(5)}$ which
wrongly identifies a pole at $\approx \SI{4520}{\per \second}$. %
However, this effect is rather local, it mainly takes place between
two training points (illustrated by black dots). %
At the bottom, we show the models
$\hat f^{(K)}_{n-1,i}(\omega),~~i = 1, \ldots, n$, which show strong
variations close to $\approx \SI{4510}{\per \second}$ but rather small
errors at the training points $\omega_i$. %
To take this into account, we introduce an instability penalty term,
which leads to the criterion
\begin{equation}
  \epsilon^K_{\LOO, \mathrm{stab}} = \epsilon^K_\LOO
  + \lambda\, \frac 1 n \frac 1M \sum_{i=1}^n \sum_{j=1}^M \left| f^{(K)}_n (\i\hat \omega_j)
    - \hat f^{(K)}_{n-1,i} (\i\hat \omega_j) \right|^2,
  \label{eq:LOOstab}
\end{equation}
where $\{\hat \omega_j\}_{j=1}^M$ denotes a fine grid on $\Omega$
(more precisely, an equidistant grid with $M = 10n + 1$ points). %
The weighting factor $\lambda$ is chosen as
\begin{equation}
  \lambda = 0.2\, \frac{\epsilon^0_\LOO}{\frac 1 n \frac 1M \sum_{i=1}^n \sum_{j=1}^M \left| f^{(0)}_n (\i\hat \omega_j)
      - \hat f^{(0)}_{n-1,i} (\i\hat \omega_j) \right|^2},
\end{equation}
i.e., $0.2$ after normalizing both terms w.r.t.\ the respective values
of the purely kernel-based interpolation model. %
To our knowledge, this approach for model selection has not been
considered before, although it is related to the continuously-defined
LOO error \cite{jin2002sequential,kim2009construction,
  fuhg2020state}. %
The continuously-defined LOO error was employed for sequential
sampling, while we propose to use it to construct an
instability penalty for model selection. %
Stability selection \cite{liu2020surrogate, meinshausen2010stability}
is another related approach, which is also based on resampling of the
data, but usually employed for variable selection.

Employing the stabilized criterion \eqref{eq:LOOstab} for model
selection gives satisfactory results for the benchmark examples
considered in this work. %
For illustration, we consider the convergence studies for two models,
which will be described in Section~\ref{sec:numerics}. %
Figure~\ref{fig:conv_error_indicators} shows the root-mean-square-errors
(RMSEs) of the available models with gray dots and the accuracy of the
selected models by the different criterions. %
It can be observed that the stabilized criterion
$\epsilon^K_{\LOO, \mathrm{stab}}$ gives the best results, while LOO
residuals with retuning is superior to the approach without
retuning.

\begin{remark}\label{rmk:hallemans}%
  The combination of kernel methods with a small number of rational
  basis functions has also been considered in \cite{hallemans2022frf}
  for data-driven modeling of frequency response functions. %
  Therein, the authors employ first order stable spline kernels, which
  encode stability, causality and smoothness and add a rational basis
  for capturing the resonsant poles of the transfer function. %
  A prior is formed over the impulse responses linked to the resonant
  poles, which allows to derive additional kernels (one for each
  resonant pole) via the Fourier transform. %

  Our approach proceeds in a similar way, as our VF-inspired rational
  basis could also be transformed into additional kernels through a
  prior over $\beta$. %
  Differences can be found in the model selection strategies, which
  are based on the local rational method in \cite{hallemans2022frf},
  whereas our approach is based on statistical model selection. %
  Additionaly, our focus here is on providing a complete background on
  the RKHS concepts of complex/real interpolation, whereas
  \cite{hallemans2022frf} is additionally targeting uncertainty
  quantification for the data-driven modeling procedure.
\end{remark}

% Figure environment removed
