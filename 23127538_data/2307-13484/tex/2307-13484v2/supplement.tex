\section{Introduction}

This supplementary material is structured in the following way. %
We first present additional material on the new method presented in
the paper: %
Section~\ref{sec:non-intrusive} provides details on a non-intrusive
implementation, %
and Section~\ref{sec:details-param-optim} contains
specifics on parameter optimization. %
We then collect material relevant to the examples. %
In particular, a partial fraction representation of the circuit model
in Section~\ref{sec:circuit} and an additional numerical example in
Section~\ref{sec:add-num-results}. %
The algorithm applied to this example, a spiral antenna, yields
results which are comparable to the pacman model shown in the main
paper. %
Finally, some theoretical results regarding circular complex/real RKHS
are collected in Section~\ref{sec:circular}.


\section{Implementation details}

\subsection{Non-intrusive implementation}
\label{sec:non-intrusive}

\subsubsection{Zero-mean case}

The main idea is to construct an isomorphic real-valued GP
$\tilde g(\tilde{\mathbf x})\sim \operatorname{GP}(0,\tilde k)$ on an
\textit{augmented input space}
$\tilde{\mathbf x}\in \bigl(\mathbb R^n\times \{0,1\}\bigr)$, s.t.,
\begin{equation}
  \begin{aligned}
    \tilde g(\begin{bmatrix}\mathbf x&0\end{bmatrix}) &= \operatorname{Re}[g(\mathbf x)],\\
    \tilde g(\begin{bmatrix}\mathbf x&1\end{bmatrix}) &= \operatorname{Im}[g(\mathbf x)].
  \end{aligned}\label{eq:idea}
\end{equation}
The augmented training data
$\bigl(\tilde {\mathbf x},\tilde y\bigr) \in \bigl(\mathbb R^n \times
\{0,1\}\bigr) \times \mathbb R$ is for each observation
$\bigl(\mathbf x^{(i)}, y^{(i)}\bigr)\in \mathbb R^n\times\mathbb C$
obtained as:
\begin{align*}
  \tilde{\mathbf x}^{(i,1)} &= \begin{bmatrix}
    \mathbf x^{(i)} & 0
  \end{bmatrix},~~\tilde {y}^{(i,1)} = 
                      \operatorname{Re}[y^{(i)}],\\
  \tilde{\mathbf x}^{(i,2)} &= \begin{bmatrix}
    \mathbf x^{(i)} & 1
  \end{bmatrix},~~\tilde {y}^{(i,2)} = 
                      \operatorname{Im}[y^{(i)}].
\end{align*}
The \textit{new} covariance function $\tilde k$ can be derived by
enforcing~\eqref{eq:idea},
\begin{align*}
  \tilde k(\tilde {\mathbf x}, \tilde {\mathbf x}')&=\tilde k([\mathbf x~j],[\mathbf x'~j'])
  =\begin{cases}
    \frac 12\operatorname{Re}[k(\mathbf x,\mathbf x')+c(\mathbf x,\mathbf x')] &j=j'=0\\
    \frac 12\operatorname{Re}[k(\mathbf x,\mathbf x')-c(\mathbf x,\mathbf x')] &j=j'=1\\
    \frac 12\operatorname{Im}[-k(\mathbf x,\mathbf x')+c(\mathbf x,\mathbf x')] &j=0,j'=1\\
    \frac 12\operatorname{Im}[k(\mathbf x,\mathbf x')+c(\mathbf x,\mathbf x')] &j=1,j'=0
  \end{cases},
\end{align*}
Note that this approach requires to define the modified covariance
function $\tilde k$, however, no (other) internal functions of
existing GP implementations need to be able to cope with complex
numbers, which is why we refer to the implementation as
\emph{non-intrusive}.


\subsubsection{Linear model in the mean function}

Consider now the superposition
\begin{equation*}
  \underline g(\mathbf x) = g(\mathbf x) + \mathbf h(\mathbf x)^\mathrm{T} \mathbf b
\end{equation*}
of a mean-free (real/)complex Gaussian Process
$g(\mathbf x)\sim \operatorname{CGP}(0, k, c)$ and a complex linear
model, where $h(\mathbf x):\mathbb R^n\rightarrow \mathbb C^m$ denote
explicit basis functions and $\mathbf b\in\mathbb C^m$ the
corresponding coefficients. %
Define the augmented process
\begin{equation*}
  \tilde{\underline g}(\tilde{\mathbf x}) =
  \tilde g(\tilde{\mathbf x})+\tilde{\mathbf h}(\tilde{\mathbf x})^\mathrm{T}\tilde{\mathbf b},  
\end{equation*}
where $\tilde g(\tilde{\mathbf x})~\sim \mathrm{GP}(0, \tilde k),$
$\tilde{\mathbf b}\in \mathbb R^{2m}$,
$\tilde{\mathbf h}(\tilde{\mathbf x}):\mathbb
R^n\times\{0,1\}\rightarrow \mathbb R^{2m}$ and we require, similarly
as in the last subsection, that
\begin{align*}
  \tilde{\underline g}([\mathbf x~0])&=\operatorname{Re}[\underline g(\mathbf x)],\\
  \tilde{\underline g}([\mathbf x~1])&=\operatorname{Im}[\underline g(\mathbf x)].
\end{align*}
Incorporating~\eqref{eq:idea}, we can conclude that
\begin{align*}
  \tilde{\mathbf h}([\mathbf x~0])^\mathrm{T}\tilde{\mathbf b}&=\operatorname{Re}[\mathbf h(\mathbf x)^\mathrm{T} \mathbf b]=\operatorname{Re}[\mathbf h(\mathbf x)]^\mathrm{T}\operatorname{Re}[\mathbf b]-\operatorname{Im}[\mathbf h(\mathbf x)]^\mathrm{T}\operatorname{Im}[\mathbf b]\\
  \tilde{\mathbf h}([\mathbf x~1])^\mathrm{T}\tilde{\mathbf b}&=\operatorname{Im}[\mathbf h(\mathbf x)^\mathrm{T} \mathbf b]=\operatorname{Im}[\mathbf h(\mathbf x)]^\mathrm{T}\operatorname{Re}[\mathbf b]+\operatorname{Re}[\mathbf h(\mathbf x)]^\mathrm{T}\operatorname{Im}[\mathbf b].
\end{align*}
needs to be fulfilled. %
This can be achieved by setting 
\begin{equation*}
  \tilde{\mathbf h}(\tilde{\mathbf x})=\tilde{\mathbf h}([\mathbf x~j])=\begin{bmatrix}
    \begin{cases}
      +\operatorname{Re}[\mathbf h(\mathbf x)] & j=0\\
      +\operatorname{Im}[\mathbf h(\mathbf x)] & j=1
    \end{cases}&\hspace{2em}
    \begin{cases}
      -\operatorname{Im}[\mathbf h(\mathbf x)] & j=0\\
      +\operatorname{Re}[\mathbf h(\mathbf x)] & j=1
    \end{cases}
  \end{bmatrix}^{\mathrm{T}},
\end{equation*}
which leads to the coefficients vector
\begin{equation*}
  \tilde{\mathbf b} = \begin{bmatrix}
    \operatorname{Re}[\mathbf b]\\
    \operatorname{Im}[\mathbf b]
  \end{bmatrix}.
\end{equation*}


\subsection{Details on parameter optimization}
\label{sec:details-param-optim}

\subsubsection{Penalized log-likelihood criterion}
\label{sec:penalized-loglik}

We select the parameters~$\alpha$ and~$\sigma^2$ of the the Szegö
kernel~$\sigma^2 k_\alpha$ (cf.~Section~3 in the article), together
with the dominant poles $\mathbf p = \left( p_1, \ldots, p_K \right)$
in the case of the hybrid method, by maximizing a penalized
log-likelihood criterion:
\begin{equation}\label{equ:penalized-loglik}
  J\left( \alpha, \sigma^2, \mathbf p \right)
  \;=\;  \max_{\mathbf r \in \Cset^K}
  \ln\left( p\left(\mathbf y|\alpha, \sigma^2, \mathbf p, \mathbf r\right) \right)
    + \ln(\rho(\alpha)),
\end{equation}
where the first term is the log-likelihood of the model, maximized
(profiled) analytically with respect to the residues~$\mathbf r$ of
the rational mean function model~$m$, and the second term is a penalty
term, designed to pull~$\alpha$ away from~$0$.

More precisely, we take for~$\rho(\alpha)$ the probability density
function (pdf) of a log-normal random variable with
parameters~$\mu_{\alpha}$ (to be specified) and~$\sigma_\alpha = 3$; %
in other words, we use a ``vague'' prior distribution on~$\alpha$,
such that~$\log(\alpha)$ is Gaussian with mean~$\mu_\alpha$ and
variance~$\sigma_\alpha^2$. %
The parameter~$\mu_\alpha$ is chosen in such a way that the log-normal
density for~$\alpha$ has its mode
at~$\left| \Omega \right| = \omega_{\mathrm{max}}
-\omega_{\mathrm{min}}$, or equivalently that the prior density
for~$\alpha / \left| \Omega \right|$ has its mode at~$1$. %
Using that the mode of the lognormal density is
at~$e^{\mu_\alpha - \sigma_\alpha^2}$, we deduce that
$\mu_\alpha = \sigma_\alpha^2 + \ln \left| \Omega \right|$. %
The resulting pdf
\begin{equation}
  \rho(\alpha) \;=\; \frac 1{\alpha\sigma_\alpha\sqrt{2\pi}}
  \exp\bigl(\frac {-(\ln(\alpha) -\mu_\alpha)^2}{2\sigma_\alpha^2}\bigr)
\end{equation}
is shown in Figure~\ref{fig:Prior} for~$\left| \Omega \right| = 1$. %
It can be seen that the chosen parameters allow for the choice
of~$\alpha$ within a range of several orders of magnitude.

% Figure environment removed

The penalized log-likelihood criterion~\eqref{equ:penalized-loglik} is
maximized numerically using bound-constrained gradient-based
optimization---more precisely, interior point algorithm available from
Matlab's \texttt{fmincon} function---with a multistart procedure. %
Details about the bounds for the search domain and the initial points
for the local search are provided in
Sections~\ref{sec:bounds-search}--\ref{sec:starting-points}.

\begin{remark}
  This parameter selection procedure can be considered as
  \emph{maximum a posteriori} estimate in the Bayesian sense. %
  Indeed, the penalized log-likelihood
  criterion~\eqref{equ:penalized-loglik} can be seen as the
  log-posterior density, up to a constant, assuming a lognormal prior
  for~$\alpha$ and an improper uniform prior for all the other
  parameters.
\end{remark}

\begin{remark}%
  Even when the complex kernel~$k$ is strictly positive definite, the
  distribution of data under the GP model does not always admit a
  probability density function with respect to Lebesgue's measure
  on~$\Rset^{2n}$ (cf.~related discussion regarding the strict
  positive definiteness of~$\tilde k$ in Section~2.2 of the
  article). %
  When this happens, a suitable reference measure has to be used in
  order to define the likelihood function. %
  For instance, when the pseudo-kernel $c(s, s_0) = k(s, s_0^*)$ is
  used to enforce the symmetry condition, the value at~$\omega = 0$
  must be real, which yields a degenerate distribution if the response
  is evaluated at~$\omega = 0$: %
  the solution is simply to remove the imaginary part of the
  response at this point from the vector of observed variables. %
  See Section~2 of~\cite{lataire2016transfer} for related
  considerations.
\end{remark}


\subsubsection{Bounds for the search domain}
\label{sec:bounds-search}

We optimize with respect to the transformed kernel parameters
\begin{align*}
  \theta_1 & \;=\; \ln\left( \frac{\sigma^2}{2\pi} \right),\\
  \theta_2 & \;=\; \alpha,
\end{align*}
within the optimization bounds
\begin{align*}
  -15 & \;\le\; \theta_1 \;\le\; 15,\\
  10^{-6}|\Omega| & \;\le\; \theta_2 \;\le\; |\Omega|,
\end{align*}
where
$\left| \Omega \right| = \omega_{\max} - \omega_{\min} = \max \left\{
  \omega_i,\, 1 \le i \le n \right) - \min \left\{ \omega_i,\, 1 \le i
  \le n \right)$.

For the hybrid model, the poles are optimized simultaneously with the
kernel hyper-parameters, within the bounds
\begin{align*}
  -|\Omega| & \;\le\; \Re\left( p_i \right) \;\le\; -10^{-6}|\Omega|, \\
  \max\left\{10^{-6}|\Omega|,~\omega_{\min}-\frac{|\Omega|}3\right\}
            & \;\le\; \Im\left(p_i\right) \;\le\; \omega_{\max}+\frac{|\Omega|} 3,
\end{align*}
where we set the maximum number of poles pairs to
$K_{\max} = \min\{5,\lfloor n/4 \rfloor\}.$ %
The bounds are enlarged, if needed, in such a way that all the poles
in the starting point of the optimization are contained within them.


\subsubsection{Starting point(s)}
\label{sec:starting-points}

We use a multistart procedure to optimize the kernel
parameters~$\alpha$ and~$\sigma^2$. %
More precisely, we start $N_\mathrm{ms} = 20$ local optimizations,
with the initial value of~$\alpha$ uniformly distributed
between~$10^{-6}|\Omega|$ and~$|\Omega|$. %
For a given value of~$\alpha$, and a given choice of poles in the case
of the hybrid algorithm, the GLS (generalized least squares) estimate
is used as a starting point for~$\sigma^2$.

For the poles in the hybrid algorithm, we start from equidistant
poles~$p_1, \ldots, p_K$ close to the frequency axis:
\begin{equation*}
  p_k \;=\; - \delta_{\Re}\, \left| \Omega \right|
  \,+\, \i \left( \omega_{\min} + \left( k -\frac{1}{2} \right)\, \delta_\Im \right),
  \qquad 1 \le k \le K,
\end{equation*}
where $\delta_{\Re} = 10^{-3}$ (weak attenuation) and
$\delta_\Im = \left| \Omega \right| / K_{\max}$. %
The kernel parameters are initialized a described previously (with the
GLS estimate for~$\sigma^2$ and a multi-start procedure
for~$\alpha$).


\section{Complements for the results section}

\subsection{Partial fraction representation of RLC circuit}
\label{sec:circuit}
The residues $c_i, c_i^*$ and poles $a_i, a_i^*$ of the partial fraction representation of the electric circuit admittance  
\begin{equation}
  Y(s) = \sum_{i=1}^N \frac {c_i}{s-a_i}+\frac
  {c_i^*}{s -a_i^*},
\end{equation}
are given as
\begin{align}
a_i &= \frac {-R_i}{2L_i}+ \i\sqrt{\frac 1 {L_iC_i}-\Bigl(\frac {R_i}{2L_i}\Bigr)^2},\\
c_i &= \frac {a_i} {L(a_i-a_i^*)} = \frac {\sqrt{\frac 1 {L_iC_i}-\Bigl(\frac {R_i}{2L_i}\Bigr)^2}+\frac {R_i}{2L_i}\i}{2L_i\sqrt{\frac 1 {L_iC_i}-\Bigl(\frac {R_i}{2L_i}\Bigr)^2}},
\end{align}
where we assumed an underdamped system, i.e. \begin{equation}\frac {R_i}2\sqrt{\frac{C_i}{L_i}}<1,\label{[eq:cond_underdamped}\end{equation} which implies that the argument of the square roots is positive.


\subsection{Additional numerical example}
\label{sec:add-num-results}

% Figure environment removed
The model is a spiral antenna, depicted in Figure~\ref{fig:spiral},  where we consider the reflection coefficient $S_{11}$ on a frequency range of [\SI{4}{GHz},\,\SI{6}{GHz}] as quantity of interest. The data sets are obtained using the boundary element method in CST Microwave Studio \cite{CST_2019aa}. The results are qualitatively the same as for the PAC-MAN model, see  Fig.~\ref{fig:spiral} (bottom). 


\section{Complements for the theoretical section}

\subsection{Circular complex/real RKHSs}
\label{sec:circular}    

\begin{definition}
  We say that a complex/real RKHS is \emph{circular} if it has a
  vanishing pseudo-kernel.
\end{definition}

The terms ``proper'' or ``strictly complex'' are also sometimes used
instead of ``circular'', in the statistics and signal processing
literature, for the case where the
pseudo-covariance of a complex Gaussian random vector or function
vanishes (see, e.g., \cite{picinbono:1996, boloix2018complex}).

\begin{theorem} \label{thm:circular}
  Let $H_\Cset$ denote a complex RKHS with kernel~$k_0$, and let $H$
  denote the complex/real RKHS obtained by considering~$H_\Cset$ as a
  real vector space, endowed with the inner product:
  $\left<f, g\right> \mapsto \Re\left( \left<f, g\right>_\Cset \right)$. %
  Then $H$ is the circular complex/real RKHS with complex kernel
  $k = 2 k_0$.  
\end{theorem}

Since a complex/real RKHS is uniquely characterized by its $(k, c)$
pair, the converse holds as well: %
given a circular complex/real RKHS~$H$ with complex kernel~$k$, there
is a unique complex RKHS~$H_\Cset$ (namely, the complex RKHS with
kernel $k_0 = \frac{1}{2} k$) such that $H$ is obtained from~$H_\Cset$
as in Theorem~\ref{thm:circular}.

\begin{proof}
  The main idea is already included in the proof of Theorem~2.14 of
  the article, but we give here a slightly more detailed version. %
  Let $\varphi_\re$ and~$\varphi_\im$ denote the real and imaginary
  evaluation kernels of~$H$. %
  Then, for all~$f \in H$ and~$s \in \Sset$,
  \begin{equation*}
    \left< f,\, k_0(\cdot,s) \right>
    \;=\; \Re\left( \left< f,\, k_0(\cdot,s) \right> \right)
    \;=\; \Re\left( f(s) \right)
  \end{equation*}
  and
  \begin{align*}
    \left< f,\, \i k_0(\cdot,s) \right>
    & \;=\; \Re\left( \left< f,\, \i k_0(\cdot,s) \right> \right)
      \;=\; \Re\left( -\i\, \left< f,\, k_0(\cdot,s) \right> \right)\\
    & \;=\; \Re\left( -i\ f(s) \right)
      \;=\; \Im\left( f(s) \right),
  \end{align*}
  which proves that~$\varphi_\re = k_0$ and~$\varphi_\im = \i k_0$. %
  The complex kernel and pseudo-kernel of~$H$ are thus given by
  \begin{align*}
    k & \;=\; \varphi_\re - \i \varphi_\im \;=\; 2 k_0,\\
    c & \;=\; \varphi_\re + \i \varphi_\im \;=\; 0.
  \end{align*}
\end{proof}


\subsection{A relation between the Szegö and rational quadratic kernels}
\label{sec:relation-szego-rq}

Consider the Szegö kernel on $H^2(\Gamma_\alpha)$:
\begin{align*}
  k_\alpha\left( s, s_0 \right)
  & \;=\; \frac 1 {2\pi\left( 2\alpha + s + s_0^* \right)}\\
  & \;=\; \frac{1}{2\pi}\, \frac{%
    \left( 2\alpha + x + x_0 \right) - \i \left( y - y_0 \right)}{%
    \left( 2\alpha + x + x_0 \right)^2 + \left( y - y_0 \right)^2},
\end{align*}
where $s=x+\i y$, $s_0 = x_0 + \i y_0 \in \Gamma_\alpha$. %
In the circular case, the corresponding kernels for the real and
imaginary parts are given by:
\begin{equation*}
  k_\re (s, s_0) \;=\; k_\im (s, s_0)
  \;=\; \frac{1}{4\pi}\, \frac{2\alpha + x + x_0}{%
    \left( 2\alpha + x + x_0 \right)^2 + \left( y - y_0 \right)^2},
\end{equation*}
For a fixed value of~$x = x_0 > -\alpha$, this is of the form
\begin{equation*}
  (y, y_0) \;\mapsto\; \frac{1}{4\pi}\, \frac{A}{%
    A^2 + \left( y - y_0 \right)^2},
  \qquad \text{with } A = 2\alpha + x + x_0 > 0,
\end{equation*}
which is a special case of the so-called \emph{rational quadratic}
kernel (see, e.g.,~\citeSM{rasmussen:2006:gpml, sollich:2004:using}),
also called \emph{generalized inverse multiquadric} kernel (see,
e.g.,~\citeSM{hu:1998:collocation, bozzini:2013:generalized}).

\phantomsection%trick
\bibliographystyleSM{siamplain}
\bibliographySM{references}
