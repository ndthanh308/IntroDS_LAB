% SIAM Article Template
\documentclass[onefignum,onetabnum]{siamart190516}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

% SIAM Shared Information Template
% This is information that is shared between the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.


% Packages and macros go here
\usepackage{lipsum}
\usepackage{amsfonts,bm}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{algorithmic}
\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi

% Include section numbers in PDF table of content
\hypersetup{bookmarksnumbered=true}

% Add a serial/Oxford comma by default.
\newcommand{\creflastconjunction}{, and~}

% Used for creating new theorem and remark environments
\newsiamremark{remark}{Remark}
\newsiamremark{hypothesis}{Hypothesis}
\crefname{hypothesis}{Hypothesis}{Hypotheses}
\newsiamthm{claim}{Claim}

% Sets running headers as well as PDF title and authors
\headers{Kernel-based interpolation for complex-valued functions}{J. Bect, N. Georg, U. Römer and S. Schöps}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{Rational kernel-based interpolation for complex-valued frequency response functions\thanks{Submitted to the editors DATE.
\funding{The work of N. Georg and U. Römer was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -- RO4937/1-1. The work of N. Georg is also supported by the Graduate School CE within the Centre for Computational Engineering at Technische Universität Darmstadt.}}}

% Authors: full names plus addresses.
%% Remark: Due to the need for multiple affiliations for the same author, I changed the template here, as outlined in Section 2 of  https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.2064&rep=rep1&type=pdf
\author{
Julien Bect\footnotemark[3]
\and Niklas Georg\footnotemark[2]\ \footnotemark[4]
\and Ulrich Römer\footnotemark[2]
\and Sebastian Schöps\footnotemark[4]}

\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\vect}{span}

\newcommand \Cset {\mathbb C}
\newcommand \Dset {\mathbb D}  % unit disk
\newcommand \Kset {\mathbb K}  % \Rset or \Cset
\newcommand \Nset {\mathbb N}  % non-negative integers
\newcommand \Rset {\mathbb R}
\newcommand \Xset {\mathbb X}
\newcommand \Sset {\mathbb S}

\newcommand \Esp  {\mathsf E}

\newcommand   \Acal {\mathcal{A}}
\renewcommand \i    {\mathrm{i}}    % Complex imaginary unit
\newcommand   \Hol  {\mathrm{Hol}}  % Holomorphic

% To avoid ambiguity, we should avoid the use of \subset to mean "subset of equal".
\renewcommand \subset {\subseteq}

% Macros for real and imaginary part
\newcommand{\re}{\mathrm{R}}
\newcommand{\im}{\mathrm{I}}

\newcommand{\tra}{{\mskip -1mu\scriptstyle\mathrm{T}}}
\newcommand{\her}{{\mskip -1mu\scriptstyle\mathrm{H}}}

\newcommand{\dt}{\mathrm{d}t}

%%% Local Variables: 
%%% mode:latex
%%% TeX-master: "ex_article"
%%% End: 

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

%\externaldocument{supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

%% Custom packages:
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tudacolors}
\usepackage{circuitikz}
\usepackage{siunitx}
\usepackage[algo2e]{algorithm2e} 
\usetikzlibrary{patterns}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes}
\usepgfplotslibrary{fillbetween}
\usepackage{tikz-3dplot}
\usepackage{pdfcomment}
\newcommand{\jul}[1]{\pdfcomment[author=Julien]{#1}}
\newcommand{\nik}[1]{\pdfcomment[author=Niklas,color=red]{#1}}
\newcommand{\seb}[1]{\pdfcomment[author=Sebastian,color=orange]{#1}}
\newcommand{\uli}[1]{\pdfcomment[author=Uli,color=green]{#1}}


%\usetikzlibrary{external}
%\tikzexternalize[prefix=tikzextern/]
\usetikzlibrary{plotmarks, positioning, backgrounds}

\newtheorem{assumption}{Assumption}



\begin{document}

\maketitle

%% Remark: Due to the need for multiple affiliations for the same author, I changed the template here, as outlined in Section 2 of  https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.2064&rep=rep1&type=pdf
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Institut für Dynamik und Schwingungen, 
	Technische Universität Braunschweig, Braunschweig, Germany
	(\email{n.georg@tu-braunschweig.de}, \email{u.roemer@tu-braunschweig.de}).}
\footnotetext[3]{Laboratoire des signaux et systèmes, CentraleSupélec, Univ. Paris-Sud, CNRS, Université Paris-Saclay, Paris, France (\email{julien.bect@centralesupelec.fr}).}
\footnotetext[4]{\thanks{Computational Electromagnetics Group,  Technische Universität Darmstadt,  Darmstadt, Germany (\email{sebastian.schoeps@tu-darmstadt.de}).}}
\renewcommand{\thefootnote}{\arabic{footnote}}


% REQUIRED
This work is concerned with the kernel-based approximation of a
complex-valued function from data, where the frequency response
function of a partial differential equation in the frequency domain is
of particular interest. %
In this setting, kernel methods are employed more and more frequently,
however, standard kernels do not perform well. %
Moreover, the role and mathematical implications of the underlying
pair of kernels, which arises naturally in the complex-valued case, remain
to be addressed. %
We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the
problem of complex-valued interpolation with a kernel pair as minimum
norm interpolation in these spaces. %
Moreover, we combine the interpolant with a low-order rational
function, where the order is adaptively selected based on a new model
selection criterion. %
Numerical results on examples from different fields, including electromagnetics
and acoustic examples, illustrate the performance of the method, also
in comparison to available rational approximation methods.

% REQUIRED
\begin{keywords}
  complex-valued Kernel methods, dynamical systems, frequency response function, model selection, rational approximation, 
\end{keywords}

% REQUIRED
\begin{AMS}
  65N99, 60G15, 46E22
 \end{AMS}

\section{Introduction}
We consider dynamical systems of the form
\begin{equation}
  \label{eq:generalDiscretePDEDynamic}
  \mathbf{M} \ddot{\mathbf{u}}(t) + \mathbf{D} \dot{\mathbf{u}}(t)
  + \mathbf{K}  \mathbf{u}(t) = \mathbf g(t),
\end{equation}
to be endowed with initial conditions and
$\mathbf K, \mathbf D, \mathbf M \in \mathbb R^{n_h\times n_h}$,
$\mathbf{u}(t),\mathbf{g}(t) \in \mathbb R^{n_h}$. %
We are in particular interested in approximating scalar time-dependent
quantities derived from the solution, of the form
\begin{equation}
  \label{eq:generalScalarQuantity}
  f(t) =  \mathbf{j}^\tra \mathbf{u}(t),
  \qquad \mathbf{j} \in \mathbb{R}^{n_h},
\end{equation}
which are commonly used to assess engineering designs. %
System \eqref{eq:generalDiscretePDEDynamic} may stem from a partial
differential equation after spatial discretization with $n_h$ degrees
of freedom. %
In a mechanics context, $\mathbf{K},\mathbf{D},\mathbf{M}$ are
referred to as stiffness, damping and mass matrix, but problems
arising in many areas of science and engineering can be brought into
this form. Our numerical results will cover electromagnetic and
acoustic field problems in particular.  In view of the linearity of
the equation, a frequency domain analysis is often adopted. %
Assuming for simplicity that~$\mathbf{u}$ and~$\dot{\mathbf{u}}$
vanish at~$t = 0$, the (one-sided) Laplace transform
of~\eqref{eq:generalDiscretePDEDynamic}--\eqref{eq:generalScalarQuantity}
with respect to the time variable~$t$ is
\begin{equation}
  \label{eq:generalDiscretePDE}
  \begin{split}
    \left( s^2 \mathbf{M}  + s \mathbf{D} + \mathbf{K} \right)\, \hat{\mathbf{u}}(s)
    &= \hat{\mathbf{g}}(s), \\
    \hat{f}(s) &= \mathbf{j}^\tra \mathbf{\hat{u}}(s),
  \end{split}
\end{equation}
where $s$ denotes the complex frequency variable, also known as the
Laplace variable. %
Assuming a suitably normalized excitation $\hat{\mathbf{g}}(s)$, the
frequency response function is defined as the value
$\omega \mapsto \hat f (\i \omega)$ of~$\hat f$ on the imaginary axis,
where $\omega$ is called the angular frequency, and we are typically
interested more specifically in its value on a certain interval
$\Omega = \left[ \omega_{\min},\omega_{\max} \right] \subseteq
[0,+\infty)$. %
In the following, we omit explicitly indicating frequency domain
variables to simplify the notation.

The location of the poles of $\hat f$ strongly depends on the
properties of $\mathbf{K},\mathbf{D},\mathbf{M}$, see
\cite{tisseur2001quadratic}. %
We assume, in particular, that no pole is placed on the frequency axis
$\i \mathbb{R}$ and that the frequency response function is
holomorphic on the shifted right
half-plane~$\Gamma_\alpha = \{s \in\mathbb C \,\mid
\,\Re[s]>-\alpha\},\,\alpha>0$. The real parts of all poles are strictly negative
for instance if $\mathbf{K},\mathbf{D},\mathbf{M}$ are symmetric
positive definite, see Section 3 of \cite{tisseur2001quadratic}. %
The same holds true if the homogeneous version of
\eqref{eq:generalDiscretePDEDynamic} is stable, in the sense that all
solutions decay exponentially to zero as $t \rightarrow \infty$. %
The holomorphy of response functions has recently been studied also in
the context of partial differential equations, see \cite[Proposition
5.3]{Bonizzoni_2020aa} for instance. %
There, the frequency response map for an acoustic scattering problem
was studied and appropriate damping terms ensured a locally
holomorphic response function, with a negative real part for all
poles\footnote{Because of a different convention
  \cite{Bonizzoni_2020aa} establishes a negative \emph{imaginary} part
  of the eigenvalues}.%

Adopting a data-driven approach, ~\eqref{eq:generalDiscretePDE} must
be solved repeatedly on a set of interpolation/training points
$\omega_i \in \Omega$, with $s_i = \i \omega_i$. %
Numerical efficiency demands a small training set
\begin{equation}
  \{\omega_i, f(\i\omega_i)\}_{i=1}^n,
  \quad
  \mathrm{where}~\omega_i\in\Omega, f(\i\omega_i)\in\mathbb C,
  i=1, \ldots, n,\label{eq:training_set}
\end{equation} 
hence, there is a need for accurate interpolation in the frequency
domain.

The data-driven approximation of frequency response functions has
attracted considerable interest in the literature, see for instance
\cite{gustavsen1999rational,lataire2016transfer, nakatsukasa2018aaa}
and the references therein. %
Among the numerous available approaches we mention vector fitting
\cite{gustavsen1999rational} and the adaptive Antoulas-Anderson method
\cite{nakatsukasa2018aaa} in particular, which are widely used,
state-of-the-art approximation methods.

Vector Fitting (VF) is a rational approximation technique,
specifically tailored to functions in the frequency domain. %
It is based on a representation in terms of partial fractions as
\begin{equation}
  f(\i\omega) \approx \sum_{m=1}^{M} \frac{r_m} {\i\omega-p_m}
  + d +\i\omega h,\label{eq:part_frac}
\end{equation}
where the $M$ poles $p_m$ are relocated in each iteration by solving a
linear least-square problem, see
\cite{gustavsen2006improving,gustavsen1999rational} for details. %
The implementation guarantees that all poles are stable,
i.e. $\mathcal R[p_m]<0$, and are either real or come in
complex-conjugate pairs. %

The adaptive Antoulas-Anderson (AAA) method \cite{nakatsukasa2018aaa}
employs the barycentric interpolation
\begin{equation}
  \label{eq:AAA}
  f(\i\omega) \approx  r(\omega)
  = \frac{n(\omega)}{d(\omega)}
  = \frac{\sum_{j\in J}{\frac {w_jf(\i \omega_j)}{\omega-\omega_j}}}{\sum_{j\in J} \frac{w_j}{\omega-\omega_j}},
\end{equation} 
where $J \subseteq \{1, \ldots, n\}$ has cardinality~$m$. %
The rational function in \eqref{eq:AAA} is of type $(m-1,m-1)$, which
can be seen by multiplying both numerator and denominator by
$\prod_{j\in J} (\omega-\omega_j)$. %
Moreover, $r(\omega_j) = f(\i \omega_j)$ for all $j\in J$. %
The weights~$w_j$ and nodes~$\omega_j$, $j\in J$, are determined
adaptively in a two-step procedure, based on linear least squares
problems and a greedy strategy \cite{nakatsukasa2018aaa}.

Other data-driven approaches, related to rational interpolation and
model order reduction are the Loewner framework
\cite{antoulas2017tutorial} and the recent contribution
\cite{nobile2020non}, which employs the Heaviside representation. 
A Bayesian rational Polynomial Chaos-type model has been put forth in \cite{schneider2023sparse} to capture the effect of uncertain parameters, e.g., on frequency response functions. 
A complex-valued version of support vector machine regression has been
presented in \cite{treviso2021multiple}, which is restricted to the
so-called circular case with a single kernel only. %
Complex interpolation with a pair of kernels has been addressed in
\cite{boloix2017widely,picinbono1995widely} and also from a Gaussian
process regression perspective in \cite{boloix2018complex,
  hallemans2022frf}. 

Despite recent progress with complex kernel methods, a general
framework with a complete mathematical background on the underlying
reproducing kernel Hilbert spaces is missing. %
In this paper, we introduce a new kernel-based interpolation method
which is well adapted to frequency responses. %
We will put special emphasis on the complex-valued setting and show
that the data are used more efficiently if a dedicated kernel method
is constructed and interpolation of the real- and imaginary part
individually is avoided. %
To address problems with a few dominant poles we include a low-order
rational basis into the kernel method and present a new model
selection scheme. %
We compare our rational kernel-based interpolation method against both
AAA and vector fitting and observe an improved or at least comparable
performance for a variety of test cases. %
Finally, the paper develops the required notions of reproducing spaces
and minimum norm interpolation for complex-valued kernel methods in
general.

The material is structured in the following way. %
In \Cref{sec:theory} we introduce the concept of a complex/real
kernel Hilbert space and consider the special case of frequency
response functions as well as the connections to complex-valued
Gaussian process regression. %
\Cref{sec:alg} introduces our new method, which employs a
kernel, a pseudo-kernel and an additional rational basis for capturing
dominant poles. %
Finally, \Cref{sec:numerics} reports several examples from
PDE-based applications, comparing our method to AAA and vector fitting
before conclusions are drawn.

\medbreak\itshape

Nota bene: %
A method sharing some similarities with the one proposed in
Section~\ref{sec:alg} has been published recently in the automatic
control literature \cite{hallemans2022frf}. %
We became aware of it at very late stage in the writing of the present
article. %
After introducing our new method in Section~\ref{sec:alg}, we discuss
similarities and differences in Remark~\ref{rmk:hallemans}.

\medbreak\normalshape


\section{Complex/Real RKHS interpolation}
\label{sec:theory}
In order to address kernel-based interpolation of the frequency
response function, we start by recalling basic facts on reproducing
kernel Hilbert spaces (RKHSs); %
see, e.g., \cite{Paulsen_2016aa} for a comprehensive introduction to
this topic.

\begin{definition}[Complex RKHS] \label{def:complex-rkhs}
  A complex RKHS~$H$ over a non-empty set~$\Sset$ is a complex Hilbert
  space of functions~$\Sset\rightarrow\mathbb C$ such that, for all
  $s \in \Sset$, the evaluation functional
  $\delta_s: H \rightarrow \Cset$, $f \mapsto f(s)$, is continuous.
\end{definition}

The Riesz representation theorem implies that there exists a unique
function $k:\Sset\times \Sset \rightarrow \Cset$, called the
reproducing kernel of~$H$, such that $k(\cdot, s) \in H$ and
\begin{equation} \label{equ:reproduction-property}
  f(s) = \delta_s(f) = \left< f, k(\cdot,s) \right>_H
\end{equation}
for all~$s\in \Sset$ and~$f \in H$, %
where $\left< \cdot,\cdot \right>_H$ denotes the Hermitian inner
product of~$H$. %
Equation~\eqref{equ:reproduction-property} is called the reproduction
property, and it is easily seen that the kernel~$k$ is
\emph{Hermitian} %
(i.e., $k(s, s_0) = k(s_0, s)^*$ for all~$s, s_0 \in \Sset$) and
\emph{positive definite}: for all $n \in \Nset^*$ and all~$(s_1, \alpha_1)$,
\ldots, $(s_n, \alpha_n) \in \Sset \times \Cset$,
\begin{equation}
	\label{eq:kernel_positiveDefinite}
  \sum_{1 \le i, j\le n} \alpha_i^* \alpha_j k(s_i, s_j) \;\ge\; 0.
\end{equation}

\begin{theorem}[Moore-Aronszajn]
	\label{thm:Moore-Aronszajn}
  For any positive definite Hermitian
  kernel~$k: \Sset\times \Sset \rightarrow \Cset$, there exists a
  unique complex Hilbert space~$H$ of functions on~$\Sset$ such that
  the reproduction property holds with reproducing kernel~$k$.
\end{theorem}

Real RKHSs are defined similarly, replacing~$\Cset$ by~$\Rset$ in
Definition~\ref{def:complex-rkhs}: %
in this case $H$~is a real Hilbert space, the reproducing kernel is
symmetric positive definite, and a suitably modified statement of the
Moore-Aronszajn theorem holds as well.

\begin{theorem}[Interpolation] %
  \label{thm:complexRKHSinterpolation} %
  Let $H$ be a real or complex RKHS over~$\Sset$ %
  with kernel $k: \Sset \times \Sset \to \Kset$, %
  where $\Kset = \Rset$ or~$\Cset$ depending on the type of RKHS. %
  Let $n \in \Nset^*$, $s_1, \ldots, s_n \in \Sset$ %
  and $y_1, \ldots, y_n \in \Kset$. %
  Then there exists a function $g \in H$ such that $g(s_i)=y_i$ for
  all $i \in \{ 1, \ldots, n \}$ if, and only if, the system
  \begin{equation}\label{equ:interp-system}
    \begin{bmatrix}
      k(s_1,s_1) &\dots & k(s_1,s_n)\\
      \vdots & \ddots & \vdots\\
      k(s_n,s_1) &\dots & k(s_n,s_n)
    \end{bmatrix}
    \begin{bmatrix}
      \gamma_1\\
      \vdots\\
      \gamma_{n}
    \end{bmatrix}
    = \begin{bmatrix}
      y_1\\\vdots\\y_n
    \end{bmatrix}
  \end{equation}
  admits a solution. %
  Furthermore, for any solution of~\eqref{equ:interp-system},
  $g = \sum_{i=1}^n \gamma_i\, k(\cdot,s_i)$ is the unique interpolant
  of the data $\left( s_1, y_1 \right)$, \ldots,
  $\left( s_n, y_n \right)$ with minimal norm in~$H$.
\end{theorem}
A positive definite kernel is called \emph{strictly} positive definite
if the kernel matrix $K_n=(k(s_i,s_j))_{1\leq i,j\leq n}$ is
invertible (equivalently, if \eqref{eq:kernel_positiveDefinite}~is
strict for all~$\left( \alpha_1, \ldots, \alpha_n \right) \neq 0$)
whenever $s_1,\ldots,s_n$ are distinct points. %
This ensures that \eqref{equ:interp-system} has a unique solution.

We will proceed by introducing several complex RKHS and their
kernels. For $s \in \mathbb C$, let $\Re[s]$ and $\Im[s]$ denote the
real and imaginary part, respectively. %
An important example is the Hardy space $H^2(D)$ on the unit disc, where $D=\{s \in \Cset: |s| <1 \}$. 
This space plays a role in the analysis of the stability of discrete
dynamical systems, see \cite{baratchart1991identification}, for
instance.  
Here, in the context of continuous-time dynamical systems, we are more
interested in the corresponding Hardy space 
\begin{equation}
  H^2(\Gamma_\alpha) \!=\!
  \left\{
    f\in \Hol(\Gamma_\alpha)\!:\!\|f\|_{H^2(\Gamma_\alpha)}
    = \sup_{x>-\alpha}\left(\int_{-\infty}^{\infty} \left| f(x + \i y)^2 \right|\,\mathrm d y \right)^{\frac 1 2} <\infty
  \right\},
\end{equation}
where $\Hol(\Gamma_\alpha)$ denotes the space of holomorphic functions
on~$\Gamma_\alpha$. %
Note, that there is a Banach space isometry between the $H^2$ spaces on disc and half-plane, see \cite[Chapter
8]{hoffman:1962} for details. %
\begin{theorem}\label{thm:gammaAlpha}
  The space $H^2(\Gamma_\alpha)$ is a complex RKHS, with strictly
  positive definite reproducing kernel~$k$ given by
  \begin{equation}\label{equ:szego} 
    k_\alpha\left( s, s_0 \right)
    = \frac{1}{2\pi\, \left( 2\alpha + s + s_0^*\right)},
    \quad s, s_0\in\Gamma_{\alpha}.
  \end{equation}
\end{theorem}

A proof is given in Appendix~\ref{proof:thmGammaAlpha}. %
Following standard terminology in complex analysis (see, e.g., \cite{krantz:2001:function}), we will refer
to~$k_{\alpha}$ as the \emph{Szegö kernel} for the
domain~$\Gamma_\alpha$. %
Evaluating \eqref{equ:szego} only on the imaginary axis $s=\i \omega$,
the expression simplifies to
\begin{equation}
  k_\alpha\left( \i\omega, \i\omega_0 \right) =
  \frac 1 {2\pi\left( 2\alpha+\i(\omega - \omega_0) \right)},
  \quad \omega, \omega_0 \in \Omega.\label{eq:SzegoFrequency}
\end{equation}

We consider the stable spline kernel
\cite{pillonetto2010new,lataire2016transfer} as another example. This
kernel has been proposed in the time domain to model functions with a
certain smoothness, which additionally incorporate impulse response
stability \cite{pillonetto2010new}. The corresponding kernel for the
frequency domain transfer function has been obtained in
\cite{lataire2016transfer} and reads
\begin{multline}
  \label{eq:stablespline}
  k_\alpha\left( \i\omega, \i\omega_0 \right) =
  \frac{1}{2} \frac{1}{3 \alpha + \i (\omega - \omega_0)} \times \\
  \left( %
    \frac{1}{2 \alpha + \i \omega} + \frac{1}{2 \alpha - \i \omega_0} %
    - \frac{1}{3(3 \alpha + \i \omega)} - \frac{1}{3(3 \alpha - \i \omega_0)} %
  \right).
\end{multline}
Other related kernels can be found in the control literature, see \cite{lataire2016transfer,hallemans2022frf}.


\subsection{Complex/real RKHS interpolation}
\label{sec:complex-real-RKHS}

The frequency response function fulfills the symmetry property
$f^*(s) = f(s^*)$ for all~$s \in \Gamma_\alpha$, since it is the
Laplace transform of a real-valued function. %
We are thus naturally led to cast our interpolation problem not
in~$H^2(\Gamma_{\alpha})$ but in the subset
\begin{equation} \label{equ:H2sym}
  H^2_\mathrm{sym}(\Gamma_{\alpha}) \;=\; \bigl\{
  f\in H^2(\Gamma_{\alpha}):\; \forall s\in\Gamma_\alpha,\; f^*(s)=f(s^*)
  \bigr\}.
\end{equation}
This set of complex-valued functions, however, cannot by endowed with
the structure of a complex RKHS. %
In fact, it is not even a vector space over~$\Cset$: %
indeed, for any $f\in H^2_\mathrm{sym}(\Gamma_\alpha)$ and
$s\in \Gamma_{\alpha}$, we would have
$(\i f)^*(s) = -\i f^*(s) = -\i f(s^*)$ and
$(\i f)^*(s) = (\i f)(s^*) = \i f(s^*)$, %
which is a contradiction if $f(s^*) \neq 0$.

Observing that the subset of~$H^2(\Gamma_{\alpha})$ defined
by~\eqref{equ:H2sym} is a real vector space of complex-valued functions,
we define in the following a new type of function space, which we call
a complex/real RKHS.
%
\begin{definition}[Complex/real RKHS] \label{def:CRrkhs}%
  Let $\Sset$ denote a non-empty set and let $H$ denote a real Hilbert
  space of complex-valued functions on~$\Sset$. %
  We say that $H$ is a complex/real RKHS if the evaluation functionals
  are continuous (i.e., for all $s \in \Sset$, the function
  $\delta_s: H \to \Cset$, $f \mapsto f(s)$, is continuous).
\end{definition}

In the remaining part of this section we will establish general
results related to these
spaces. Section~\ref{sec:complex-real-RKHS-sym} will then present
consequences for the RKHS with the symmetry property
$f^*(s) = f(s^*)$.

\begin{remark} \label{rem:complex-RKHS-subspaces}
  Any complex RKHS~$H$ (such as $H^2(\Gamma_{\alpha})$) can be seen as a
  complex/real RKHS by forgetting the complex structure, i.e., by
  considering $H$ as a real vector space, endowed with the real inner
  product $\left<f, g\right> \mapsto \Re \left( \left< f, g \right>_H \right)$. %
  More generally, any real subspace of~$H$ (such
  as~$H^2_\mathrm{sym}(\Gamma_{\alpha})$), endowed with this inner
  product, is clearly a complex/real RKHS. %
  The converse statement is false, however.
\end{remark}

\begin{proposition} \label{prop:counterexample-dim2}
  There exists a complex/real RKHS of dimension two over the reals
  that is not a real subspace of a complex RKHS.
\end{proposition}

The elements of a complex/real RKHS are complex-valued functions
over~$\Sset$, but can be conveniently represented as real-valued
functions over $\tilde\Sset = \Sset \times \{\re,\im\}$ through the
mapping $\Acal: \Cset^\Sset \rightarrow \Rset^{\tilde\Sset}$ defined
by
\begin{equation} \label{eq:mappingNonIntrusive}
  (\Acal f)(s,a) = G_a(f(s)), 
\end{equation}
where $G_\re(s) = \Re(s)$ and $G_\im(s) = \Im(s)$. %
This mapping defines an isometric isomorphism of real Hilbert spaces
between $H$ and the real vector space
$\tilde H = \Acal H \subset \Rset^{\tilde\Sset}$, endowed with the
image inner product. %
The image space $\tilde H$ is easily seen to be a real RKHS if and
only if~$H$ is a complex/real RKHS: %
this observation will be useful both from a theoretical point of view,
to establish properties of complex/real RKHSs, and from a practical
point of view (see Section~\ref{sec:numerics}).

\begin{remark}
  Complex/real RKHSs can also been seen a special case of
  vector-valued RKHSs~\cite{burbea:1984:banach,
    micchelli:2005:vector}, through the usual identification
  of~$\Cset$ with~$\Rset^2$.
\end{remark}

The term ``functional'' is used in a loose sense in
Definition~\ref{def:CRrkhs}, since $H$ is a real vector space
while~$\delta_s$ is a complex-valued function. %
Therefore, in contrast with the usual case of complex RKHSs, the
continuous functionals~$\delta_s$, $s \in \Sset$, do not belong to the
topological dual of~$H$. %
The real and imaginary evaluation functions however---namely,
$\Re \circ \delta_s$ and~$\Im \circ \delta_s$---do belong to the
topological dual, and can thus be expressed through inner products.

\begin{proposition} \label{prop:repr-eval-func} %
  Let $H$ be a complex/real RKHS on a set~$\Sset$, and set %
  \begin{equation}
    k_{a\mskip 1mu a_0}(s, s_0) \;=\; \tilde k\left( (s, a),\, (s_0, a_0) \right),
    \qquad s, s_0 \in \Sset,
    \quad a, a_0 \in \{ \re, \im \},
  \end{equation}
  where $\tilde k$ denotes the reproducing kernel of~$\tilde H = \Acal H$. %
  Then, for all $s \in \Sset$, we have
  \begin{equation}
    \delta_s \;=\;
    \underbrace{\left<\, \bm{\cdot}\,,\, \varphi_\re(\cdot,s) \right>_H}_{\Re \circ\, \delta_s}
    \;+\; \i \underbrace{\left<\, \bm{\cdot}\,,\, \varphi_\im(\cdot,s) \right>_H}_{\Im \circ\, \delta_s},
  \end{equation}
  where %
  $\varphi_\re = k_{\re\re} \,+\, \i\, k_{\im\re}$ and %
  $\varphi_\im = k_{\re\im} \,+\, \i\, k_{\im\im}$.
\end{proposition}

This result associates to each complex/real RKHS a
pair~$\left( \varphi_\re, \varphi_\im \right)$ of kernels
$\varphi_{a}:\Sset \times \Sset \to \Cset$, $a \in \{ \re, \im\}$. %
Characterizing admissible choices for this pair of kernels, in the
spirit of Theorem~\ref{thm:Moore-Aronszajn} for complex RKHSs, %
is possible but not convenient. %
Instead, motivated by the connection between complex/real RKHSs and
complex Gaussian processes (to be discussed in
Section~\ref{sec:relation-GPs}), and in particular the work of
Picinbono \cite{picinbono:1996}, we introduce another pair of kernels
as follows.

\begin{definition} \label{def:complex-kernels} %
  Let $H$ denote a complex/real RKHS and let $k_{\re\re}$, $k_{\im\im}$,
  $k_{\re\im}$, $k_{\im\re}$, $\varphi_\re$ and~$\varphi_\im$ be defined as in
  Proposition~\ref{prop:repr-eval-func}. %
  Then we define the \emph{complex kernel}~$k$ of the complex/real
  RKHS as
  \begin{equation}
    \label{eq:complex-kernel}
    k %
    \;=\;  \left( k_{\re\re} + k_{\im\im} \right) %
    \,+\, \i\, \left( k_{\im\re} - k_{\re\im} \right)
    \;=\; \varphi_\re - \i \varphi_\im,
  \end{equation}
  and its \emph{pseudo-kernel} $c$ as:
  \begin{equation}
    \label{eq:pseudo-kernel}
    c %
    \;=\; \left( k_{\re\re} - k_{\im\im} \right) %
    \,+\, \i\, \left( k_{\im\re} + k_{\re\im} \right)
    \;=\; \varphi_\re + \i \varphi_\im.
  \end{equation}
\end{definition}

\begin{proposition}\label{prop:dense-subspace}
  The functions of the form
  $\gamma\, k(\cdot,s_0) + \gamma^*\, c(\cdot, s_0)$, with
  $\gamma \in \Cset$ and $s_0 \in \Sset$, span a dense subset of~$H$.
\end{proposition}

\begin{remark}
  Proposition~\ref{prop:dense-subspace} suggests that the concept of a
  complex/real RKHS, introduced in this article, provides a rigorous
  formalization of the idea of a ``wide-linear complex-valued RKHS''
  (WL-RKHS) proposed in~\cite{boloix2017widely} (see Definition~3.1).
\end{remark}

It can be shown that the complex/real RKHS obtained by forgetting the
complex structure of a complex RKHS with reproducing kernel~$k_0$, as
described in Remark~\ref{rem:complex-RKHS-subspaces}, is the
complex/real RKHS with complex kernel $k = 2 k_0$ and vanishing
pseudo-kernel---which, borrowing terminology from the signal
processing literature \cite{picinbono:1996}, can be called
\emph{circular}. %
The factor~$2$ in the relation between~$k$ and~$k_0$ is the price to
pay for the consistency of Definition~\ref{def:complex-kernels} with
the concepts of covariance and pseudo-covariance functions for complex
Gaussian processes (see Section~\ref{sec:relation-GPs}). %
More generally, we have the following characterization of the set of
admissible $(k, c)$ pairs.

\begin{theorem} \label{thm:CR-RKHS-characterization}\setlength{\parskip}{2pt}%
  For a given complex/real RKHS~$H$, the kernels~$k$ and~$c$
  introduced in Definition~\ref{def:complex-kernels} satisfy the
  following:
  \begin{enumerate}[i) ]
  \item $k$ is complex-valued, Hermitian and positive definite.
  \item $c$ is complex-valued and symmetric.
  \end{enumerate}
  Moreover, for all~$n \ge 1$ and all~$s_1, \ldots s_n \in \Sset$:
  \begin{enumerate}[i) ] \setcounter{enumi}{2}
  \item $\ker K_n \subset \ker C_n^*$ and,
  \item if $K_n$ is positive definite,
    $K_n^* - C_n^* K_n^{-1} C_n$ is positive semi-definite,
  \end{enumerate}
  where $K_n = \left( k(s_i,s_j) \right)_{1 \le i,j \le n}$ and
  $C_n = \left( c(s_i,s_j) \right)_{1 \le i,j \le n}$.

  Conversely, for any pair of functions
  $k, c:\Sset \times \Sset \to \Cset$ that satisfies these four
  properties, there exists a unique complex/real RKHS on~$\Sset$ with
  complex kernel~$k$ and pseudo-kernel~$c$.
\end{theorem}


\begin{theorem}[Interpolation in a complex/real RKHS] %
  \label{thm:interp:cr} %
  Let $H$ denote a complex/real RKHS over~$\Sset$ %
  with complex kernel~$k$ and pseudo-kernel~$c$. %
  Let $n \in \Nset^*$, $s_1, \ldots, s_n \in \Sset$ %
  and $y_1, \ldots, y_n \in \Cset$. %
  Then there exists a function $g \in H$ such that $g(s_i)=y_i$ for
  all $i \in \{ 1, \ldots, n \}$ if, and only if, the system
  \begin{equation}
    \label{equ:interp-system:cr}
    K_n \gamma + C_n \gamma^* = y
  \end{equation}
  admits a solution $\gamma \in \Cset^n$, where
  $K_n = \left( k(s_i,s_j) \right)_{1 \le i,j \le n}$,
  $C_n = \left( c(s_i,s_j) \right)_{1 \le i,j \le n}$,
  and $y = \left( y_1, \ldots, y_n \right)^\tra$. %
  Furthermore, for any solution of~\eqref{equ:interp-system:cr},
  \begin{equation}
    \label{equ:interpolant:cr}
    g = \sum_{i=1}^n \gamma_i\, k(\cdot,s_i)
    + \sum_{i=1}^n \gamma_i^*\, c(\cdot,s_i)
  \end{equation}
  is the unique interpolant of the data $\left( s_1, y_1 \right)$,
  \ldots, $\left( s_n, y_n \right)$ with minimal norm in~$H$.
\end{theorem}

For the usual setting of real or complex RKHSs, strictly positive
definite kernels guarantee that the interpolation
system~\eqref{equ:interp-system} has a solution for any
data~$y_1, \ldots, y_n$. %
This remains true for the system~\eqref{equ:interp-system:cr} in the
case of a complex/real RKHS if the associated real kernel~$\tilde k$
is strictly positive definite on
$\tilde\Sset = \Sset \times \{\re,\im\}$.

\subsection{Complex/real RKHS with symmetry condition} \label{sec:complex-real-RKHS-sym}

We now characterize, in full generality, the complex/real
RKHSs where a symmetry condition of the form $f^*(s) = f(s^*)$ holds
for all~$f \in H$ and~$s \in \Sset$. %
The following theorem provides a necessary and sufficient condition on~$k$
for such a space to exist and gives the expression of the
corresponding pseudo-kernel. %
The expression appeared previously in \cite[Equations~(48)--(49)]{lataire2016transfer}
for a special type of kernel.

\begin{theorem}\label{thm:hermitian}
  Let $\Sset$ denote a non-empty set, equipped with an involution
  $s \mapsto s^*$ and $k:\Sset \times \Sset \to \Cset$ denote a
  Hermitian positive definite kernel on~$\Sset$. %
  Then the following assertions are equivalent:
  \begin{enumerate}[i) ]
    % 
  \item There exists a complex/real RKHS $H$ on~$\Sset$, with complex
    kernel~$k$, such that
    \begin{equation}
      \label{equ:symmetry}
      \forall f \in H,\; \forall s \in \Sset,\quad f^*(s) = f(s^*).
    \end{equation}
    %
  \item There exists a complex/real RKHS $H$ on~$\Sset$, with complex
    kernel~$k$ and pseudo-kernel $c$ defined by
    \begin{equation}
      \label{equ:pseudo-kern-symm}
      \forall s,s_0 \in \Sset,\quad %
      c(s,s_0) = k(s, s_0^*).
    \end{equation}
    %
  \item $\forall s,s_0 \in \Sset$, $k(s, s_0^*) = k(s_0, s^*)$.
    %
  \end{enumerate}

  \smallbreak
  
  If any (and consequently all) of these assertions holds, then the
  complex/real RKHS~$H$ with complex covariance~$k$ and pseudo
  kernel~\eqref{equ:pseudo-kern-symm} is the unique RKHS on~$\Sset$
  with complex covariance~$k$ such that~\eqref{equ:symmetry} holds. %
  Moreover, denoting by $H_\Cset$ the complex RKHS with kernel~$k$, we
  have $H_\Cset = H \oplus \i H$,
  $H = \left\{ f \in H_\Cset \mid \text{\eqref{equ:symmetry} holds}
  \right\}$ and $\left< f, g \right> = \Re \left< f, g \right>_{H_\Cset}$
  for all $f, g \in H$.
\end{theorem}

It follows from this theorem that $H^2_\mathrm{sym}(\Gamma_{\alpha})$
can be characterized as the complex/real RKHS over~$\Gamma_\alpha$
with complex kernel~\eqref{equ:szego} and pseudo-kernel:
\begin{equation}\label{equ:pseudo-szego} 
  c_\alpha\left( s, s_0 \right) = \frac 1 {2 \pi (2\alpha + s + s_0)},\quad s,\, s_0\in\Gamma_{\alpha}.
\end{equation}

More generally, Theorem~\ref{thm:hermitian} shows that the problem of
minimum-norm interpolation in a complex RKHS, with a symmetry
constraint of the form~\eqref{equ:symmetry}, can be solved by
considering the equivalent problem of minimal-norm interpolation in
the complex/real RKHS with the same complex kernel and the
pseudo-kernel given by~\eqref{equ:pseudo-kern-symm}.
%
In presence of the symmetry condition, even if the complex kernel $k$
is strictly positive definite, $\tilde{k}$ is not and an additional
condition on the data is required to ensure that~\eqref{equ:interp-system}
has a solution.

\begin{theorem}\label{thm:existence-uniqueness-hermit}%
  In the setting of Theorem~\ref{thm:hermitian}, assume that $k$ is
  strictly positive definite, $c$~is given
  by~\eqref{equ:pseudo-kern-symm}, and $s_1, \ldots, s_n \in \Sset$
  are distinct. %
  Then \eqref{equ:interp-system:cr}~has a solution if, and only if,
  $y_j = y_i^*$ for all $i, j$ such that $s_j = s_i^*$. %
  When this holds, there is a unique solution such that
  $\gamma_i = \gamma_j^*$ for all $i, j$ such that~$s_j = s_i^*$.
\end{theorem}


For illustration, we consider the third order rational function
\begin{equation}\label{eq:Frat}
  F_{\mathrm{rat}}(\i\omega)
  = \frac 1 {\i\omega-(-0.1)}
  + \frac {0.5}{\i\omega-(-0.1-0.5\i)}
  + \frac {0.5}{\i\omega-(-0.1+0.5\i)},\;
  \omega\in[0,1],
\end{equation}
which is the Laplace transform of the real-valued
function $t \mapsto e^{-0.1t}\bigl(1+\cos(0.5t)\bigr)$
and thus belongs to
$H^2_\mathrm{sym}(\Gamma_{0.1+\epsilon}) \subset H^2(\Gamma_{0.1+\epsilon})$ for all~$\epsilon > 0$. %
To illustrate the importance of the choice of pseudo-kernel,
we conduct a convergence study in
terms of the root-mean-square error (RMSE) of the approximations,
using equidistant training points
(details on the implementation and selection of
hyper-parameters will be given in the following sections). %
In Figure~\ref{fig:PCov_demo} we demonstrate that choosing a suitable
pseudo-kernel might have a significant impact on the convergence
properties of the (complex/real) RKHS interpolation. %
For the test function~\eqref{eq:Frat}, the pseudo-kernel~\eqref{equ:pseudo-kern-symm}
improves the convergence significantly. %
Note that the test function is a low order rational function which is
here only used to illustrate the impact of the pseudo-kernel. %
Accordingly, rational interpolation techniques as AAA or VF reach
machine accuracy already with $\approx8$ training points and are hence
excluded in the convergence plot for clarity. %
However, it can already be observed that complex/real RKHS
interpolation with the Szegö kernel outperforms the alternative
approach of separate kernel approximations for real and imaginary
part with a Gaussian kernel, as well as polynomial interpolation on
Chebyshev nodes.

% Figure environment removed


\subsection{Relation to Gaussian process interpolation}
\label{sec:relation-GPs}
This section draws connections between minimum norm interpolation in a
RKHS and the posterior mean prediction of a Gaussian process (GP), for
both the complex and complex/real case. GPs are widely used, but to
the authors knowledge this is the first time that the RKHS associated
to any complex GP prediction is characterized. %
Another intention of this section is to make results from the GP
literature available for interpolation with a complex/real RKHS. %
In particular, we are interested in employing statistical methods for
model selection (see, e.g., \cite{petit:2022:parameter} and references
therein)---this will be further developed in
Section~\ref{sec:adap_proc}. %
We consider zero-mean processes in this section, %
for simplicity 
see Remark~\ref{rem:semi-RKHS} below.

\newcommand \kGP {\mathfrak{k}}
\newcommand \cGP {\mathfrak{c}}

Complex GPs are covered for instance in \cite{miller1969complex}. %
A complex GP is a complex process, where the real and imaginary part
considered jointly are a real GP. We consider a zero-mean complex-valued random process $\xi$ on
$\Sset$, with covariance function~$\kGP$ and pseudo-covariance
function~$\cGP$:
\begin{align}
  \Esp\left( \xi(s) \xi(s_0)^* \right) & = \kGP(s,s_0), \\
  \Esp \left( \xi(s) \xi(s_0) \right) & = \cGP(s,s_0).
\end{align}

Relying on the mapping~$\Acal$, we can work in a real-valued setting,
i.e., with a real-valued GP $\tilde{\xi}$ indexed
on~$\tilde{\Sset}$. %
In the real-valued case, it is well-known that the conditional mean of
a GP is identical to the minimum-norm interpolant in the RKHS
associated to its covariance function. %
Hence, using~$\Acal$, the conditional mean of a complex GP~$\xi$ is
also identical to a minimum-norm interpolant, but this time in a
complex/real RKHS, the complex kernel~$k$ and pseudo-kernel~$c$ of which are
equal to~$\kGP$ and~$\cGP$ respectively (this follows from
Equations~\eqref{eq:complex-kernel}--\eqref{eq:pseudo-kernel}). %
It is given by Equation~\eqref{equ:interpolant:cr} in general, which
simplifies to
\begin{equation}
  \Esp \left( \xi(s) | y  \right) =  \sum_{i=1}^n \gamma_i k(s,s_i),
  \quad \text{with} \   K_n \gamma  = y,
\end{equation}
if the pseudo-covariance is zero (i.e., in the circular case).

\begin{remark} \label{rem:real-imaginary-independent-interpolation}
  A common approach to deal with complex data is to
  use GP interpolation for the real and imaginary part separately
  (see, e.g., \cite{Fuhrlander_2020ab}). %
  This corresponds, using notations from
  Proposition~\ref{prop:repr-eval-func}, to $k_{RI} = k_{IR} = 0$, and
  therefore to a complex GP with covariance $k = k_{RR} + k_{II}$ and
  pseudo-covariance $c = k_{RR} - k_{II}$.
\end{remark}

\begin{remark} \label{rem:GP-regression_circular}
  GP regression with both covariance and pseudo-covariance function
  has also been considered under the name widely linear posterior
  mean. In \cite{picinbono:1996} it is first shown that the posterior
  mean is widely linear \cite{picinbono1995widely}, which leads to
  \begin{equation}
    \Esp \left( \xi(s) | y  \right) = (k_{s,n} - c_{s,n} K_n^{-*}C_n^H)P_n^{-*}y + (c_{s,n} - k_{s,n} K_n^{-1}C_n)P_n^{-1}y^*,
  \end{equation}
  where $P_n = K_n^* - C_n^H K_n^{-1}C_n$ and $P_n^{-*}$ denotes the
  complex conjugate of the inverse of $P_n$. The formulas for the
  circular and non-circular case can also be found in
  \cite{boloix2018complex}.
\end{remark}

\begin{remark} \label{rem:semi-RKHS} %
  In practice, GP models often include a non-zero mean function~$m$,
  usually written as a linear combination
  $m(x) = \sum_{\ell=1}^L \beta_\ell h_\ell(x)$ of known basis functions~$h_\ell$,
  with unknown coefficients~$\beta_\ell$. %
  If the coefficients are estimated by maximum likelihood (as in
  Section~\ref{sec:alg}), the posterior mean of the GP is then equal
  to the interpolant with minimal \emph{semi-norm} in~$G = V + H$,
  where $V = \vect \{ h_1, \ldots h_L \}$ and the semi-norm is
  defined by
  $\left| g \right|_G = \inf_{v \in V} \lVert g - v \rVert_H$.
\end{remark}
     
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

\section{Hybrid algorithm}
\label{sec:alg}
\label{sec:hybrid}
We focus from now, unless otherwise specified, on functions satisfying
the property $f^*(s)=f(s^*)$, and we employ the Szegö
kernel~\eqref{equ:szego}, together with the
pseudo-kernel~\eqref{equ:pseudo-szego}, for complex/real interpolation. %
In practice, the convergence of complex/real RKHS interpolation can be
significantly slower than that of rational approximations techniques
(such as AAA or VF) when the function has a few dominant poles~$p_i$,
i.e., poles with small attenuation~$\Re[p_i] \approx 0$. %
In this section, we discuss how complex/real RKHS interpolation with
the Szegö kernel and associated pseudo-kernel can be combined with a
small number of rational basis functions for the approximation of such
frequency response functions.

\subsection{Gaussian process model}
\label{sec:conjPoles}
%
We propose to use a complex GP model with rational mean
function~$m = \sum_{\ell = 1}^L \beta_\ell h_\ell$
(cf.~Remark~\ref{rem:semi-RKHS}), covariance
function~$\sigma^2 k_\alpha$ and pseudo-covariance
function~$\sigma^2 c_\alpha$, %
where $k_\alpha$ denotes the Szegö kernel~\eqref{equ:szego},
$c_\alpha$ the associated pseudo-kernel~\eqref{equ:pseudo-szego}, %
and $\sigma^2$, $\alpha$, $\beta_1$, \ldots, $\beta_L$ are
real parameters with $\sigma^2 > 0$ and~$\alpha > 0$. %
For the mean function~$m$ we assume a rational function satisfying the
property $m^*(s) = m(s^*)$, of the form
\begin{equation}\label{eq:basis}
  m(s) = \sum_{i=1}^K \left\{ \frac{1}{s - p_i}\, r_i \,+\, \frac{1}{s - p_i^*}\, r_i^* \right\},
\end{equation}
%
with residues $r_1, \ldots, r_K \in \Cset$ and (stable) complex
conjugate poles $p_1, p_1^*, \ldots, p_K, p_K^* \in \Cset$ such that
$\Re(p_i) < 0$ and~$\Im(p_i) > 0$ for all~$i$. %
This representation is similar to the one used in VF
\cite{gustavsen2006improving, gustavsen1999rational}. %
Equation~\eqref{eq:basis} can be rewritten
as~$m = \sum_{\ell = 1}^L \beta_\ell h_\ell$ with $L = 2K$,
\begin{equation*}
  \beta_\ell = \begin{cases}
    \Re(r_i) & \text{if } \ell = 2i-1,\\
    \Im(r_i) & \text{if } \ell = 2i,
  \end{cases}
  \quad \text{and} \quad
  h_\ell(s) = \begin{cases}
    \frac{1}{s - p_i} + \frac{1}{s - p_i^*} & \text{if } \ell = 2i-1,\\
    \frac{\i}{s - p_i} - \frac{\i}{s - p_i^*} & \text{if } \ell = 2i.
  \end{cases}
\end{equation*}
%
Note that $m$ is an element of $H^2_\mathrm{sym}(\Gamma_{\alpha'})$
with $\alpha' = \min_{1\le i \le K} \left| \Re(p_i) \right| + \epsilon, \epsilon > 0$. %
For simplicity we only consider complex conjugate poles in~\eqref{eq:basis}, but
real poles could be included as well, as in VF. %
%
In the context of the present work, we typically consider a small
number~$K$ of pole pairs ($K \le K_{\mathrm{max}} = \min \left( 5, \lfloor n/4 \rfloor \right)$ in the examples).

For a given number~$K$ of pole pairs, we select the hyper-parameters
$\sigma^2$, $\alpha$, $\mathbf p = \left( p_1, \ldots, p_K \right)$
and $\mathbf r = \left( r_1, \ldots, r_K \right)$ by maximization of a
penalized log-likelihood function, where the penalty stems from a vague
log-normal prior on~$\alpha$; %
see Supplementary Material for details. %
An original procedure for the selection of an appropriate number~$K$
of pole pairs will be presented in the next section.

\begin{remark}
  Note that we do not include a constant basis function, as is
  usually done in Gaussian process modeling, to ensure that the
  interpolant satisfies the desired property (namely, goes to zero)
  when $\omega \to \pm\infty$.
\end{remark}

\begin{remark}
  Alternatively, the residues~$r_1, \ldots, r_K$ could be integrated
  out analytically using a Gaussian prior, resulting in additional
  terms in the covariance and pseudo-covariance functions of the GP;
  see, e.g., \cite{hallemans2022frf}. %
  This would allow the uncertainty on the residues, for a given set of
  poles, to be reflected in the uncertainty quantification (posterior
  variances) produced by the GP model. %
  We do not pursue this idea further in this article, since our focus is
  on interpolation rather than uncertainty quantification.
\end{remark}


\subsection{Adaptive pole selection}
\label{sec:adap_proc}

Selecting a suitable number~$K$ of pole pairs to be included in the
mean function~\eqref{eq:basis} is a crucial step to ensure good
accuracy of the proposed hybrid method. %
In this section we propose a model selection procedure to select this
number automatically, in a data-driven manner. %
While this procedure relies on the well-established idea of
(leave-one-out) cross-validation, it contains an original ingredient
in the form an ``instability penality'', which will be described
below.

First we build $K_{\max} + 1$ interpolants~$f^{(K)}_n$, where the
superscript $K$ indicates the number of pole pairs, ranging from~$0$
(zero-mean Gaussian process model) to~$K_{\max}$. %
Following standard VF practice \cite{gustavsen1999rational}, we begin
with the maximum number of poles, $K = K_{\max}$, using an equidistant
distribution of poles close to the frequency axis as a starting point
for optimization. %
The other interpolants are then constructed iteratively, going
backwards: at each step optimization is initialized using~$K$ of
the~${K + 1}$ poles selected at the previous step, %
by removing the least relevant pole according to the (penalized)
log-likelihood function. %

Model selection is then based on leave-one-out (LOO) cross-validation,
i.e., on the error indicators
\begin{equation}
  \epsilon^K_\mathrm{loo}
  = \frac 1 n \sum_{i=1}^n \left| f(\i \omega_i)
    - \hat f^{(K)}_{n-1,i} (\i\omega_i) \right|^2,~~K=0,1,\ldots,K_{\max},
  \label{eq:loo}
\end{equation} 
where $\hat f^{(K)}_{n-1,i}$ denotes a model constructed without the
$i$-th data point. %
Keeping the poles and kernel hyper-parameters fixed, when removing points, makes it possible to reduce the computational effort, but
was found to introduce an undesired preference for models with a
larger number of poles. %
Hence, we employ the LOO criterion with re-tuning, using the poles and
hyper-parameters of ~$f_n^{(K)}$ as an initial guess when
constructing~$\hat f^{(K)}_{n-1, i}$, $1 \le i \le n$.

% Figure environment removed

Furthermore, we introduce an additional penalty term, which also takes
\textit{global} model variations into account. %
This approach can be motivated by the example illustrated in
Figure~\ref{fig:loo_selection_issue} (top). %
The corresponding vibro-acoustic benchmark model will be described in
Section~\ref{sec:numerics}, however, here we simply consider the
approximation of the dashed function, based on interpolation of the
training points (black dots), as a general example. %
At the top, it can be observed that the LOO-criterion \eqref{eq:loo}
leads to the selection of a model (solid lines)
$\hat f_n^{(5)}$ which
wrongly identifies a pole at $\approx \SI{4520}{\per \second}$. %
However, this effect is rather local, it mainly takes place between
two training points (illustrated by black dots). %
At the bottom, we show the models
$\hat f^{(K)}_{n-1,i}(\omega),~~i = 1, \ldots, n$, which show strong
variations close to $\approx \SI{4510}{\per \second}$ but rather small
errors at the training points $\omega_i$. %
To take this into account, we introduce an instability penalty term,
which leads to the criterion
\begin{equation}
  \epsilon^K_\mathrm{loo,stab} = \epsilon^K_\mathrm{loo}
  + \lambda\, \frac 1 n \frac 1M \sum_{i=1}^n \sum_{j=1}^M \left| f^{(K)}_n (\i\hat \omega_j)
    - \hat f^{(K)}_{n-1,i} (\i\hat \omega_j) \right|^2,
  \label{eq:loostab}
\end{equation}
where $\{\hat \omega_j\}_{j=1}^M$ denotes a fine grid on $\Omega$
(more precisely, an equidistant grid with $M = 10n + 1$ points). %
The weighting factor $\lambda$ is chosen as
\begin{equation}
  \lambda = 0.2\, \frac{\epsilon^0_\mathrm{loo}}{\frac 1 n \frac 1M \sum_{i=1}^n \sum_{j=1}^M \left| f^{(0)}_n (\i\hat \omega_j)
      - \hat f^{(0)}_{n-1,i} (\i\hat \omega_j) \right|^2},
\end{equation}
i.e., $0.2$ after normalizing both terms w.r.t.\ the respective values
of the purely kernel-based interpolation model. %
To our knowledge, this approach for model selection has not been
considered before, although it is related to the continuously-defined
LOO error \cite{jin2002sequential,kim2009construction,
  fuhg2020state}. %
The continuously-defined LOO error was employed for sequential
sampling, while we propose to use it to construct an
instability penalty for model selection. %
Stability selection \cite{liu2020surrogate, meinshausen2010stability}
is another related approach, which is also based on resampling of the
data, but usually employed for variable selection.

Employing the stabilized criterion \eqref{eq:loostab} for model
selection gives satisfactory results for the benchmark examples
considered in this work. %
For illustration, we consider the convergence studies for two models,
which will be described in Section~\ref{sec:numerics}. %
Figure~\ref{fig:conv_error_indicators} shows the root-mean-square-errors
(RMSEs) of the available models with gray dots and the accuracy of the
selected models by the different criterions. %
It can be observed that the stabilized criterion
$\epsilon^K_\mathrm{loo,stab}$ gives the best results, while LOO
residuals with retuning is superior to the approach without
retuning.

\begin{remark}\label{rmk:hallemans}%
  The combination of kernel methods with a small number of rational
  basis functions has also been considered in \cite{hallemans2022frf}
  for data-driven modeling of frequency response functions. %
  Therein, the authors employ first order stable spline kernels, which
  encode stability, causality and smoothness and add a rational basis
  for capturing the resonsant poles of the transfer function. %
  A prior is formed over the impulse responses linked to the resonant
  poles, which allows to derive additional kernels (one for each
  resonant pole) via the Fourier transform. %

  Our approach proceeds in a similar way, as our VF-inspired rational
  basis could also be transformed into additional kernels through a
  prior over $\beta$. %
  Differences can be found in the model selection strategies, which
  are based on the local rational method in \cite{hallemans2022frf},
  whereas our approach is based on statistical model selection. %
  Additionaly, our focus here is on providing a complete background on
  the RKHS concepts of complex/real interpolation, whereas
  \cite{hallemans2022frf} is additionally targeting uncertainty
  quantification for the data-driven modeling procedure.
\end{remark}

% Figure environment removed


\section{Numerical results}
\label{sec:numerics}
We apply the presented approximation techniques to a number of
benchmark functions from different fields. %
We always employ $n$ training points $(\omega_i,f(\i\omega_i)),$ where
the $\omega_i$ are equidistant frequency points in
$[\omega_{\min},\omega_{\max}]$, for simplicity. %
The accuracy of different approximations is then quantified in terms
of the root-mean-square error (RMSE), which is evaluated on a refined
equidistant grid with $201$ points for all numerical examples.

In the following we give a few details on the implementation. %
For AAA \cite{nakatsukasa2018aaa}, we rely on the implementation of
the \texttt{chebfun} toolbox \cite{driscoll2014chebfun}. %
For VF, we employ the \texttt{VectFit3} toolbox
\cite{gustavsen1999rational,gustavsen2006improving,deschrijver2008macromodeling},
where we use complex equidistant starting poles distributed according
to the general recommendation, and always run 30 iterations. %
We apply the ``relaxed non-triviality constraint''
\cite{gustavsen2006improving}, include the constant but not the linear
term, and enforce stable poles. %
The number of complex starting pole pairs is set to the maximum number
of $2\lfloor \frac {n-1}2\rfloor$, which leads to the best results for
the smooth test functions considered. %
For kernel interpolation we consider a separate interpolation of the
real and imaginary part with the squared exponential kernel (SE) and
complex/real interpolation with the Szegö kernel. %
The latter is also considered in combination with an adaptive rational
basis (Sz.-Rat.)\ as described in Section~\ref{sec:hybrid}. %
The implementation is done in \texttt{Matlab} as well, based on the
\texttt{STK} toolbox \cite{bect:2023:stk}. %
To this end, we employ the mapping $\mathcal A$ defined in
\eqref{eq:mappingNonIntrusive} for the complex/real RKHS
interpolation, which allows to realize the implementation based on
real RKHS interpolation on an augmented input space
$\Omega \times\{0,1\}$. %
Note that this approach could be employed with any toolbox for real
RKHS interpolation that provides the option to specify custom kernel
functions. %
The tuning of the hyper-parameters and poles based on the likelihood
function (see Section~\ref{sec:alg}) is carried out using
\texttt{fmincon} in Matlab, i.e., gradient-based optimization (more
precisely an interior point algorithm), which we combine with a
multistart procedure; see Supplementary Material for more details.

\begin{remark}
  By investigating the shape of the likelihood function for a number
  of benchmark problems, we have found that the logarithmic
  reparameterization, discussed in \cite{basak2021numerical} for
  instance, is not beneficial for the parameter $\alpha$. %
  Hence, it is only applied to the scaling parameter~$\sigma$.
\end{remark}


\subsection{Electric circuit (high order rational function)}

We consider in the following a parallel connection of $N$ underdamped
series RLC circuits, as illustrated on the left side in
Figure~\ref{fig:Electric_circuit}. %
The admittance is given as
\begin{equation}
  Y(s) = \sum_{i=1}^N \frac
  {s}{s^2 L_i +s R_i + C_i^{-1}}=\sum_{i=1}^N \frac {c_i}{s-a_i}+\frac
  {c_i^*}{s -a_i^*},\label{eq:admittance}
\end{equation}
where $\Re[a_i] = -\frac{R_i}{2L_i}$ (an explicit representation of
the poles $a_i$ and residues $c_i$ is given in the Supplementary
Material) and we consider the frequency range
$[\SI{10}{kHz}, \SI{25}{kHz}]$. %
First, we assume $N_1=1000$ random series RLC elements, where
$C_i\sim \mathcal U(1,20)\,\SI{}{\micro\farad}$ and
$L_i\sim\mathcal U(0.1,2)\,\SI{}{\milli\henry}$, %
and we assume the resistance $R_i$ to be roughly proportional to the
inductance, with random variations of $\pm 20\%$:
$R_i=L_i(1+\Delta)\, \SI{}{\ohm}(\SI{}{\milli\henry})^{-1}$, where
$\Delta \sim \mathcal U(-0.2, 0.2)$.

Note that for any combination of those parameters, the corresponding
series RLC circuits are underdamped. %
For one particular realization, the distribution of the $2N = 2000$
poles is illustrated in Figure~\ref{fig:Electric_circuit}. %
The corresponding admittance $Y_1(\i\omega)$ is shown in
Figure~\ref{fig:fun_circuit} with dashed black lines. %
We then conduct a convergence study for the particular realization of
the electric circuit, which is shown in
Figure~\ref{fig:ConvStudyCircuit} (top, left). %
We repeat the convergence study for 100 random realizations and depict
the median RMSE at each point in Figure~\ref{fig:ConvStudyCircuit} (top,
right). %
It can be observed that for the considered range of the number of
training points (where $n \le 60 \ll N$) the complex/real Szegö kernel-based interpolation outperforms
AAA and~VF. %
Employing the hybrid algorithm (Sz.-Rat.)\ does not yield an
improvement, but leads to similarly good results.

% Figure environment removed

% Figure environment removed

% Figure environment removed

In our second experiment, we introduce two additional circuit elements
with a very small damping, i.e. we now consider $N_2 = 1002$ and
\begin{align*}
  C_{1001} = \SI{5}{pF,} && L_{1001} =\SI{1}{mH}, && R_{1001}=\SI{0.1}{\ohm},\\
  C_{1002} = \SI{2}{pF}, && L_{1002} =\SI{1}{mH}, && R_{1002}=\SI{0.1}{\ohm}.
\end{align*}
This leads to two additional poles which are closer to the input
domain, as illustrated by the red crosses in
Figure~\ref{fig:Electric_circuit}. %
The corresponding admittance $Y_2(\i \omega)$ differs very little from
$Y_1(\i\omega)$, except for two sharp peaks, as can be seen in
Figure~\ref{fig:fun_circuit}. %
However, the accuracy of the respective RKHS interpolation is significantly
affected. %
In particular, at the bottom of Figure~\ref{fig:ConvStudyCircuit}, it
can be observed that the convergence order of Szegö kernel
interpolation is significantly reduced. %
By adding the rational basis we are able to mitigate the impact of the two
dominant poles: it exhibits fast convergence and an improvement
w.r.t.\ AAA and~VF can again be observed.


\subsection{PDE-based examples}

% Figure environment removed

In the following, we investigate a number of PDE-based examples. %
We start with the acoustic Helmholtz equation, in particular, the
PAC-MAN benchmark example, introduced in \cite{ziegelwanger2017pac}
which is also included in the platform for benchmark cases in
computational acoustics from the European Acoustics Association
\cite{hornikx2015platform}. %
The model, shown in Figure~\ref{fig:Pacman}, has the PAC-MAN shape with
an opening angle of $30^\circ$ and radius of $\SI{1}{m}$. %
As in \cite[Section 6.1]{ziegelwanger2017pac}, we consider as
excitation a vibration of the surface of the PAC-MAN with cylindrical
modes and observe the radiated field $p_i$ at a point in \SI{2}{m}
distance at an angle of $10^\circ$. %
As in \cite{hornikx2015platform}, the computation was done based on
the implementation of the analytical solution provided in
\cite{ziegelwanger2017pac} by replacing the python module
\texttt{scipy} by \texttt{mpmath} for the computation of higher order
Bessel functions. %
In particular, we set the truncation order to~300. %
The complex acoustic pressure field phasor $p_i$ of the total
sound-field versus the frequency
$f\in[\SI{2000}{\hertz},\SI{4000}{\hertz}]$ is shown in
Figure~\ref{fig:Pacman} (top, right). %
We then conduct a convergence study w.r.t.\ the number of training
points, which is depicted in Figure~\ref{fig:Pacman} (bottom, left). %
It can be observed that the complex/real Szegö kernel-based interpolation
outperforms the alternative approaches in the range up to about 40
training points. %
Adding the rational mean function does not further improve the
accuracy, but does not harm the accuracy either.

% Figure environment removed

Next, we consider an electromagnetic model problem, which is a
demonstration example of CST Microwave Studio \cite{CST_2019aa},
solving the full set of Maxwell equations in the frequency domain. %
The model consists of a waveguide junction with 4 ports, which
contains a small metallic disk and is connected to an external cavity
resonator (see Figure~\ref{fig:WGjunction}). %
The structure is excited at the first port and simulated using the
finite element method in the frequency domain. %
In particular, we set the solver accuracy of the 3rd order solver to
$10^{-6}$ and use a curved mesh with standard settings. %
We employ an initial adaptive mesh refinement at \SI{9}{GHz}, where we
set the scattering parameter 
criterion threshold with 2 subsequent checks to
$10^{-4}$. %
As quantity of interest we consider the scattering parameters on a
frequency range of [\SI{7}{GHz},\,\SI{9}{GHz}] using equidistant
sample points, where we restrict ourself to $S_{21}$ and $S_{41}$ for
brevity, however, the results are qualitatively similar for all four
scattering parameters. %
It can be seen that, the QoIs have a dominant pole at around
\SI{8}{GHz}. %
This causes the purely kernel-based interpolations to be inferior
compared to the rational approximations. %
However, the proposed combination of kernel-based interpolation and
rational approximations leads to satisfactory results, with an
accuracy comparable to that of~AAA and~VF.%

% Figure environment removed

The final test case is a vibroacoustic finite element model, taken
from \cite{Roemer_2021aa} and depicted in
Figure~\ref{fig:Vibroacoustic}. %
A 2D Mindlin plate (vibrating structure $D_s$) is excited by a point
force and strongly coupled to a 3D acoustic domain (air
cavity~$D_f$). %
Then, the response at a point in the fluid is evaluated. %
See \cite{Roemer_2021aa} for more details on the model. %
We consider the frequency response on a frequency interval
$\omega \in [\SI{4500}{\per\second},\SI{5000}{\per\second}]$, shown in
Figure~\ref{fig:Vibroacoustic} (top, right). %
The convergence study, given in Figure~\ref{fig:Vibroacoustic} (bottom),
indicates that the proposed approach usually achieves an accuracy at
least comparable to that of~AAA and~VF, with at certain points an
improvement by about an order of magnitude can be observed. %
It can also be seen that the rational mean function improves the
accuracy at the majority of points compared to the pure Szegö
kernel-based interpolation.%


\section{Conclusion}
\label{sec:conclusions}
We have presented a comprehensive framework for kernel-based
interpolation of complex-valued functions and frequency response
functions. %
In the complex-valued case, the pseudo-kernel is an additional
ingredient, which can be used to improve the interpolation
accuracy. %
We have introduced the concept of complex/real reproducing kernel
Hibert spaces to reveal the role of the pseudo-kernel and to establish
results on minimum norm interpolation. %
Furthermore, we have proposed a hybrid method, which complements the kernel-interpolant with a
low-order rational function and a new model selection criterion: this extension
is crucial to account for dominant poles in applications.

The capabilities of the rational-kernel method have been illustrated
with several examples, from circuits to frequency response functions
originating from PDE problems. %
In all examples the performance was at least comparable, in some cases
improved, compared to AAA and vector fitting on the same set of
training data.

The kernel method was further linked to complex-valued Gaussian
process regression, which can be used in future work to include noise
and adaptive sampling. %
A generalization to the multivariate case, where, e.g., uncertain parameters are considered as well, 
and comparisons against multivariate AAA \cite{rodriguez2020p} or rational Polynomial Chaos \cite{schneider2023sparse}, would also be of interest. %

\appendix
\section{Proofs}

\subsection{Proof of Theorem~\ref{thm:gammaAlpha}}
\label{proof:thmGammaAlpha}

We assume without loss of generality that~$\alpha = 0$ in this
proof---i.e., we consider the case of the Hardy space~$H^2(\Gamma_0)$
on the right
half-plane~$\Gamma_0 = \left\{ s \in \mathbb{C} \,\mid\, \Re[s] >0
\right\}$. %
The general case follows by translation.

The fact that~$H^2(\Gamma_0)$ is an RKHS is well known. %
Indeed, recall the one-sided Paley-Wiener theorem
(see, e.g., Chapter~8 of~\cite{hoffman:1962}): %
for all~$f \in H^2(\Gamma_0)$, there exists a
unique~$\widehat f \in L^2(\Rset_+)$ such that
\begin{equation}\label{equ:PW-repr}
  f(s) \;=\; \frac{1}{\sqrt{2\pi}} \int_0^{+\infty} \widehat f(t)\, e^{-st}\, \dt,
  \qquad \forall s \in \Gamma_0,
\end{equation}
and the mapping $f \mapsto \widehat f$ is a surjective isometry:
$\lVert f \rVert_{H^2(\Gamma_0)} = \lVert \widehat f
\rVert_{L^2(\Rset_+)}$. %
This proves that $H^2(\Gamma_0)$~is a Hilbert space, and a simple
application of the Cauchy-Schwartz inequality for
$s = x+\i y \in \Gamma_0$ yields:
\begin{equation*}
  \left| f(s) \right| \;\le\;
  \frac{1}{2\sqrt{\pi x}} \cdot \lVert \widehat f \rVert_{L^2(\Rset_+)},
\end{equation*}
which proves that the evaluation functionals are continuous
on~$H^2(\Gamma_0)$.

Let us now determine the kernel~$k$ of this RKHS. %
Let $s_0 \in \Gamma_0$ and set $h = k(\cdot, s_0)$. %
Then, for any~$f \in H^2(\Gamma_0)$, the reproduction property
combined with~\eqref{equ:PW-repr} yields:
\begin{equation*}
  \left< f,\, h \right>_{H^2(\Gamma_0)}
  \;=\; f(s_0)
  \;=\; \frac{1}{\sqrt{2\pi}} \int_0^{+\infty} \widehat f(t)\, e^{-s_0t}\, \dt
  \;=\; \left< \widehat f,\, \frac{1}{\sqrt{2\pi}}\, e^{-s_0^* (\cdot)} \right>_{L^2(\Rset_+)},
\end{equation*}
which implies that
$\widehat h = \frac{1}{\sqrt{2\pi}}\, e^{-s_0^* (\cdot)}$ since
$f \mapsto \widehat f$ is an isometric isomorphism. %
The expression of the kernel follows:
\begin{equation}\label{equ:kernel:Gamma0}
  k(s, s_0) \;=\; h(s)
  \;=\; \frac{1}{\sqrt{2\pi}} \int_0^{+\infty} \widehat{h}(t)\, e^{-st}\, \dt
  \;=\; \frac{1}{2\pi \left( s + s_0^* \right)}.
\end{equation}

It remains to show that $k$~is strictly positive definite. %
For any $m \ge 1$ and $s_1, \ldots, s_m \in \Gamma_0$, the kernel
matrix $K_m = \left( k(s_i, s_j) \right)_{1 \le i, j \le m}$ can be
seen as the conjugate Gram matrix of~$h_1, \ldots, h_m$
in~$L^2(\Rset_+)$, where
$h_j(t) = \frac{1}{\sqrt{2\pi}}\, e^{-s_j^* t}$, $t \ge 0$. %
Assume that $s_1$, \ldots, $s_m$ are distinct. %
Then it is well known that the complex
exponentials~$e^{-s_1^* (\cdot)}$, \ldots, $e^{-s_m^* (\cdot)}$ are
linearly independent entire functions on~$\Cset$. %
It follows, using the identity theorem, that $h_1$, \ldots, $h_m$ are
linearly independent as well. %
The kernel matrix $K_m$~is thus invertible and, consequently, positive
definite. %
Therefore $k$ is strictly positive definite.

\begin{remark} 
  The expression of the reproducing kernel is also derived in
  \cite[Theorem 2.12]{Bonyo_2020aa} (for the upper half-plane instead
  of~$\Gamma_0$) using a different approach involving the kernel of
  the Hardy space of the unit disk. %
  Note, however, that the factor~$2\pi$ in the denominator
  of~\eqref{equ:kernel:Gamma0} is missing
  in~\cite[Equation~(2.9)]{Bonyo_2020aa}; %
  the discrepancy comes from a missing factor~$\frac{1}{2\pi}$ in the
  definition of the norm on~$H^p(\Dset)$ on page~14.
\end{remark}


\subsection{Proof of Proposition~\ref{prop:counterexample-dim2}}

Take $H = \left\{ \alpha f_0,\, \alpha \in \Cset \right\}$, where
$f_0:\Xset \to \Cset$ is some fixed function, and define a real inner
product over~$H$ by
$\left< \alpha f_0,\, \beta f_0 \right> := \Re\alpha \cdot \Re\beta +
4\, \Im\alpha \cdot \Im \beta$.  %
Assuming that $f_0 \not\equiv 0$, the resulting space is complex/real
RKHS of dimension two, spanned by~$\left\{ f_0,\, \i f_0 \right\}$. %
($H$ is also a complex vector space of dimension~$1$.)

It not possible to embed~$H$ as a subspace of a complex Hilbert
space~$H_\Cset$ with inner product~$\left< \cdot, \cdot \right>_\Cset$
such that $\left<f, g \right> = \Re \left<f, g \right>_\Cset$ for all
$f, g \in H$. %
To see it, note for instance that $\lVert f_0 \rVert = 1$ while
$\lVert \i f_0 \rVert = 2$.


\subsection{Proof of Proposition~\ref{prop:repr-eval-func}}
\label{proof:propReprEvalFunc}

Let $f \in H$, $s_0 \in \Sset$ and~$a_0 \in \{ \re, \im \}$. Then
\begin{align}
  G_{a_0} \left( f(s_0) \right)
  &\;=\; \left( \Acal f \right)(s_0, a_0)
  \;=\; \left< \Acal f,\, \tilde k\left( \cdot,\, (s_0, a_0)\right) \right>_{\tilde H}\\
  &\;=\; \left< f,\, \Acal^{-1} \left( \tilde k\left( \cdot,\, (s_0, a_0)\right) \right) \right>_H.
    \label{equ:proof:repr-eval-func:2}
\end{align}
Taking $a_0 = \re$, we have thus proved that
$\Re \circ \delta_{s_0} = \left<\, \bm{\cdot}\,,\, \varphi_\re \left(
    \cdot, s_0 \right) \,\right>_H$, where
\begin{equation}
  \varphi_\re \left( \cdot, s_0 \right) = \Acal^{-1} \left( %
    \tilde k\left( \cdot,\, (s_0, \re)\right) %
  \right) \in H
\end{equation}
can be computed as follows:
\begin{align}
  \Re \left[ \varphi_\re \left( s, s_0 \right) \right]
  & \;=\; \left( \Acal\left[ \varphi_\re \left( \cdot, s_0 \right) \right] \right)(s, \re)
    \;=\; \tilde k\left( (s, \re),\, (s_0, \re)\right)
    \;=\; k_{\re\re}(s, s_0),\\
  \Im \left[ \varphi_\re \left( s, s_0 \right) \right]
  & \;=\; \left( \Acal\left[ \varphi_\re \left( \cdot, s_0 \right) \right] \right)(s, \im)
    \;=\; \tilde k\left( (s, \im),\, (s_0, \re)\right)
    \;=\; k_{\im\re}(s, s_0).
\end{align}
The expression of~$\varphi_\im \left( \cdot, s_0 \right)$ is derived similarly
by taking $a_0 = \im$ in~\eqref{equ:proof:repr-eval-func:2}.


\subsection{Proof of Proposition~\ref{prop:dense-subspace}}

In a real or complex RKHS, it is well known that the partial kernel
functions~$k(\cdot,\, s_0)$, $s \in \Sset$, span a dense subset of the
Hilbert space. %
Moreover, recall that the bijection~$\Acal$ defined in
Section~\ref{sec:complex-real-RKHS} is an isometric isomorphism
between~$H$ and a real RKHS $\tilde H$
on~$\tilde\Sset = \Sset \times \{\re,\im\}$, whose kernel~$\tilde k$
can be recovered from~$k$ and~$c$ by inverting
\eqref{prop:repr-eval-func}--\eqref{def:complex-kernels}. %
The claim then follows from the observation that any function
on~$\tilde\Sset$ of the form
\begin{equation*}
  \tilde g \;=\;
  \sum_{i=1}^n \alpha_i\, \tilde k\left( \cdot, (s_i, \re) \right)
  + \sum_{i=1}^n \beta_i\, \tilde k\left( \cdot, (s_i, \im) \right),
\end{equation*}
where $\alpha_1, \beta_1, \ldots, \alpha_n, \beta_n \in \Rset$,
corresponds to the image by~$\Acal$ of
\begin{align*}
  g & \;=\;   \sum_{i=1}^n \alpha_i\, \Acal^{-1}\left( \tilde k\left( \cdot, (s_i, \re) \right) \right)
      + \sum_{i=1}^n \beta_i\, \Acal^{-1} \left( \tilde k\left( \cdot, (s_i, \im) \right) \right)\\
    & \;=\;   \sum_{i=1}^n \alpha_i\, \varphi_\re\left( \cdot, s_i \right)
      + \sum_{i=1}^n \beta_i\, \varphi_\im\left( \cdot, s_i \right)\\
    & \;=\;   \sum_{i=1}^n \gamma_i\, k\left( \cdot, s_i \right)
      + \sum_{i=1}^n \gamma_i^*\, c\left( \cdot, s_i \right),
      \qquad \text{with } \gamma_i = \frac{1}{2} \left( \alpha_i + \i \beta_i \right).
\end{align*}


\subsection{Proof of Theorem~\ref{thm:CR-RKHS-characterization}}

Assume first that~$k$ and~$c$ are the complex kernel and pseudo-kernel
associated to a given complex/real RKHS~$H$. %
Let $\tilde\xi$ denote a zero-mean (e.g., Gaussian) real-valued random
process indexed by~$\Sset$ with covariance function equal to the
kernel~$\tilde k$ of the real RKHS~$\tilde H = \Acal H$, and set
$\xi = \tilde\xi(\cdot,\re) + \i\, \tilde\xi(\cdot,\im)$. %
Then $\xi$ is a complex-valued random process on~$\Sset$, with
covariance function~$k$ and pseudo-covariance function~$c$; %
indeed, for all $s, s_0 \in \Sset$,
\begin{align*}
  \Esp\left( \xi(s)\, \xi(s_0)^* \right)
  & \;=\; \left(
    \tilde k\left( (s,\re),\, (s_0, \re) \right)
    + \tilde k\left( (s,\im),\, (s_0, \im) \right)
    \right)\\
  & \quad + \i\, \left(
    \tilde k\left( (s,\im),\, (s_0, \re) \right)
    - \tilde k\left( (s,\re),\, (s_0, \im) \right)
    \right) \;=\; k(s, s_0),
\end{align*}
and similarly~$\Esp\left( \xi(s)\, \xi(s_0) \right) = c(s, s_0)$. %
It follows readily that $k$~is Hermitian and positive definite,
and that $c$ is symmetric, which proves i) and~ii).

Pick~$s_1, \ldots s_n \in \Sset$, and set
$K_n = \left( k(s_i,s_j) \right)_{1 \le i,j \le n}$ and
$C_n = \left( c(s_i,s_j) \right)_{1 \le i,j \le n}$. %
Then~$K_n$ and~$C_n$ are respectively the covariance and
pseudo-covariance matrix of the random
vector~$Z = \left( \xi(s_1),\, \ldots,\, \xi(s_n) \right)^\tra$, and
thus iv) is precisely the ``only if'' part the following result, due
to~\cite{picinbono:1996}.

\begin{proposition} \label{prop:Picinbono}
  Let $n \in \Nset^*$.  Let $K$ be a complex, Hermitian, positive
  definite matrix of order~$n$, and let $C$ be a complex, symmetric
  matrix of the same size.  Then there exists a complex random
  vector~$Z$ with covariance matrix~$K$ and pseudo-covariance
  matrix~$C$ if, and only if, $K^* - C^\her K^{-1} C$ is positive
  semi-definite.
\end{proposition}

It remains to prove~iii): let $u \in \ker K_n$. %
Then $u^\her K_n u = \Esp\left( \left| u^\her Z \right|^2 \right) = 0$,
therefore $u^\her Z = 0$ almost surely, and as a consequence:
\begin{equation*}
  C_n^* u
  \;=\; \Esp\left( Z Z^\tra \right)^*\, u
  \;=\; \Esp\left( Z^* Z^\her u \right)
  \;=\; \Esp\left( Z^* (u^\her Z)^\her \right)
  \;=\; 0.
\end{equation*}
This completes the proof of~i)--iv).

Conversely, assume now that~$k$ and~$c$ are two functions
from~$\Sset \times \Sset$ to~$\Cset$, such that i)--iv) hold. %
Then it is easy to see that there is a unique
function~$\tilde k: \Sset \times \{ \re, \im \} \to \Rset$ such that
\eqref{eq:complex-kernel}--\eqref{eq:pseudo-kernel} hold, given by
\begin{align*}
  k_{\re\re}(s, s_0) & \;=\; \frac{1}{2}\,  \Re \left( k(s,s_0) + c(s,s_0) \right)\\
  k_{\im\im}(s, s_0) & \;=\; \frac{1}{2}\,  \Re \left( k(s,s_0) - c(s,s_0) \right)\\
  k_{\im\re}(s, s_0) & \;=\; \frac{1}{2}\,  \Im \left( k(s,s_0) + c(s,s_0) \right)
                      \;=\; k_{\re\im}(s_0, s).
\end{align*}
It remains to prove that~$\tilde k$ is positive definite. %
It is easy to see that this is true if, and only if, the
matrices~$K_n$ and~$C_n$ defined above are the covariance and
pseudo-covariance matrices of a complex random vector~$Z$, for any
choice of the points~$s_1, \ldots, s_n \in \Sset$. %
Pick such a set of points, and let $r$ denote the rank of~$K_n$. %
Assume without loss of generality that
\begin{equation}
  \label{eq:Kn-block}
  K_n \;=\;
  \begin{pmatrix}
    K_{11} & K_{12}\\
    K_{12}^\her & K_{22}
  \end{pmatrix},
\end{equation}
with $K_{11}$ a positive definite $r \times r$ matrix. %
Then $K_{22} = K_{12}^\her K_{11}^{-1} K_{12}$ and
\begin{equation}
  \label{eq:Kn-block-bis}
  K_n \;=\; M\,
  \begin{pmatrix}
    K_{11} & 0\\
    0 & 0
  \end{pmatrix}\,
  M^\her,
  \qquad \text{where }
  M \;=\;
  \begin{pmatrix}
    \mathrm{I}_r & 0\\
    K_{12}^\her K_{11}^{-1} & \mathrm{I}_{n-r}
  \end{pmatrix}
\end{equation}
Denote by~$C_{11}$ the upper-left $r \times r$ block in~$C_n$. %
Then it follows from iv) that
$K_{11}^* - C_{11}^\her K_{11}^{-1} C_{11}$ is positive semi-definite,
and thus by Proposition~\ref{prop:Picinbono} there exists a complex
random vector~$Z_1$ of size~$r$ with covariance matrix~$K_{11}$ and
pseudo-covariance matrix~$C_{11}$. %
It is then clear from~\eqref{eq:Kn-block-bis} that $K_n$ is the
covariance matrix of
\begin{equation*}
  Z = M\,
  \begin{pmatrix}
    Z_1\\ 0
  \end{pmatrix}.
\end{equation*}
To complete the proof, it remains to observe that~$C_n$ is the
pseudo-covariance matrix of~$Z$:
\begin{equation}
  \label{eq:Cn-block}
  C_n \;=\; M\,
  \begin{pmatrix}
    C_{11} & 0\\
    0 & 0
  \end{pmatrix}\, M^\tra \;=\; \Esp\left( Z Z^\tra \right),
\end{equation}
which follows from the facts that~$C_n$ is symmetric and that
$\ker K_n \subset \ker C_n^*$, respectively by~ii) and~iii).

\subsection{Proof of Theorem~\ref{thm:interp:cr}} %
Using the bijection~$\Acal$ defined in
Section~\ref{sec:complex-real-RKHS}, the interpolation problem
on~$\Sset$ with complex-valued data~$\left( s_1, y_1 \right)$, \ldots,
$\left( s_n, y_n \right)$ can be reformulated as an interpolation
problem on~$\tilde\Sset = \Sset \times \{\re,\im\}$ with real-valued
data~$\left( (s_1, \re), \Re(y_1) \right)$,
$\left( (s_1, \im), \Im(y_1) \right)$, \ldots,
$\left( (s_n, \re), \Re(y_n) \right)$,
$\left( (s_n, \im), \Im(y_n) \right)$. %
The claim then follows from Theorem~\ref{thm:complexRKHSinterpolation}
using, as in the proof of Proposition~\ref{prop:dense-subspace}, the
fact that $\Acal$ is an isometric isomorphism between~$H$ and the real
RKHS $\tilde H = \Acal(H)$.


\subsection{Proof of Theorem~\ref{thm:hermitian}} %
$i) \Rightarrow ii)$. %
Let $H$ denote a complex/real RKHS on~$\Sset$ with complex kernel~$k$,
such that \eqref{equ:symmetry} holds. %
Let $c$ denote the pseudo-covariance of~$H$. %
Let $s_0 \in \Sset$. %
It follows from Proposition~\ref{prop:dense-subspace} that
\begin{equation*}
  f_\gamma \;=\; \gamma\, k(\cdot, s_0) \,+\, \gamma^*\, c(\cdot, s_0)
\end{equation*}
is in~$H$ for all~$\gamma \in \Cset$. %
Using~\eqref{equ:symmetry}, we see then that
\begin{align*}
  f_\gamma(s^*) & \;=\; \gamma\, k(s^*, s_0) \,+\, \gamma^*\, c(s^*, s_0)\\
         & \;=\; \gamma\, c(s, s_0)^* \,+\, \gamma^*\, k(s, s_0)^* \;=\; f_\gamma(s)^*
\end{align*}
holds for all~$\gamma \in \Cset$. %
This yields in particular that
$c(s,s_0) = k(s^*, s_0)^* = k(s_0, s^*)$, and the claim follows from
the symmetry of~$c$:
\begin{equation*}
  c(s, s_0) = c(s_0, s) = k(s, s_0^*).
\end{equation*}
Note that we have actually proved a little more than~$ii)$: if $i)$
holds, then $ii)$ holds for the \emph{same} complex/real RKHS~$H$. %
Since we will now prove that $ii) \Rightarrow iii) \Rightarrow i)$, it
follows that the complex/real RKHS with complex kernel~$k$ and
pseudo-kernel~$c$ defined by~\eqref{equ:pseudo-kern-symm}, if it
exists, is the only complex/real RKHS with complex kernel~$k$ such
that~\eqref{equ:symmetry} holds.

$ii) \Rightarrow iii)$. %
Let $H$ denote a complex/real RKHS on~$\Sset$ with complex kernel~$k$.
Assume that the pseudo-kernel $c$
satisfies~\eqref{equ:pseudo-kern-symm}. %
Then, for all $s, s_0 \in \Sset$,
\begin{equation*}
  k(s, s_0^*) \;=\; c(s, s_0) \;=\; c(s_0, s) \;=\; k(s_0, s^*).
\end{equation*}

\newcommand \ipC  {\left< \cdot,\, \cdot \right>_{\Cset}}
\newcommand \ipR  {\left< \cdot,\, \cdot \right>_{\Rset}}
\newcommand \truc {\diamond}

$iii) \Rightarrow i)$. %
Let $k$ denote a Hermitian positive definite kernel on~$\Sset$ such
that
\begin{equation}\label{equ:assumpt-iii}
  \forall s,s_0 \in \Sset, \quad k(s, s_0^*) = k(s_0, s^*).
\end{equation}
Let $(H_\Cset, \ipC)$ denote the complex RKHS with kernel~$k$ and let
$\ipR = \Re \ipC$. %
Then, as observed in Remark~\ref{rem:complex-RKHS-subspaces},
$(H_\Cset, \ipR)$ is a complex/real RKHS. %
The associated real and imaginary evaluation kernels, which we
denote by~$\varphi_\re^\truc$ and~$\varphi_\im^\truc$ respectively,
are easily seen to be given by~$\varphi_\re^\truc = k$
and~$\varphi_\im^\truc = \i\, k$, %
and the complex kernel and pseudo-kernel follow:
\begin{equation*}
  k^\truc = \varphi_\re^\truc - \i \varphi_\im^\truc = 2k
  \quad \text{and} \quad
  c^\truc = \varphi_\re^\truc + \i \varphi_\im^\truc = 0.
\end{equation*}

\bigbreak Now let~$H$ denote the subset of all the
functions~$f \in H_\Cset$ that satisfy~\eqref{equ:symmetry}: $H$ is
clearly a real subspace of~$H_\Cset$, and thus $\left( H, \ipR \right)$
is a complex/real RKHS as well. %
Moreover, for any $f \in H$,
\begin{align*}
  \Re\, f(s)
  & \;=\; \Re\, \left\{
    \frac{1}{2}\, \left( f(s) + f(s^*)^* \right) \right\}\\
  & \;=\; \frac{1}{2}\, \Bigl\{
    \left< f,\, \varphi_\re^\truc(\cdot,s) \right>_\Rset
    + \left< f,\, \varphi_\re^\truc(\cdot,s^*) \right>_\Rset
    \Bigr\}\\
  & \;=\; \left< f,\,
    \frac{1}{2}\, \left(
    \varphi_\re^\truc(\cdot,s) + \varphi_\re^\truc(\cdot,s^*) \right)
    \right>_\Rset.
\end{align*}
As a consequence of~\eqref{equ:assumpt-iii}, the function
$s \mapsto \frac{1}{2}\, \left( \varphi_\re^\truc(\cdot,s) +
  \varphi_\re^\truc(\cdot,s^*) \right)$ in this inner product satisfies 
	\begin{align*}
		&\frac{1}{2}\, \left( \varphi_\re^\truc(s_0,s)^* +
  \varphi_\re^\truc(s_0,s^*)^* \right) \\
  	&= \frac{1}{2}\, \left( \varphi_\re^\truc(s,s_0) +
  \varphi_\re^\truc(s^*,s_0) \right)
	\end{align*}  
  is an element of~$H$, which proves that the real evaluation
functional~$\varphi_\re$ of~$\left( H, \ipR \right)$ is given by
\begin{equation*}
  \varphi_\re(s, s_0)
  \;=\; \frac{1}{2}\, \left(
    \varphi_\re^\truc(s,s_0) + \varphi_\re^\truc(s,s_0^*) \right)
  \;=\; \frac{1}{2}\, \left(  k(s,s_0) + k(s,s_0^*) \right).
\end{equation*}
Similarly for the imaginary evaluation functional~$\varphi_\im$:
\begin{equation*}
  \varphi_\im(s, s_0)
  \;=\; \frac{1}{2}\, \left(
    \varphi_\im^\truc(s,s_0) - \varphi_\im^\truc(s,s_0^*) \right)
  \;=\; \frac{\i}{2}\, \left(  k(s,s_0) - k(s,s_0^*) \right).
\end{equation*}
Therefore $\varphi_\re - \i \varphi_\im = k$ is the complex kernel
of~$\left( H, \ipR \right)$, which proves~$i)$.

To prove the remaining assertions, assume that~$i$--$iii)$ hold. %
Let $G$ denote the closed linear span
of~$\left\{ k(\cdot,s_0);\; s_0 \in \Sset \right\}$ over~$\Rset$. %
Then we have $G + \i G = H_\Cset$, and it follows
from~\eqref{equ:assumpt-iii} that~$G \subset H$. %
Observing that
\begin{equation*}
  \i H = \left\{
    f \in H_\Cset \mid \forall s \in \Sset,\, f(s^*) = - f(s)^*
  \right\},
\end{equation*}
we conclude that $H \cap \i H = \{ 0 \}$, therefore $G = H$ and
$H \oplus \i H = H_\Cset$. %


\subsection{Proof of Theorem~\ref{thm:existence-uniqueness-hermit}}
Observe first that, without loss of generality, we can add $m$~extra
data points~$(s_i, y_i)$, for some $m \le n$, in such way that 1) the
points~$s_i \in \Sset$ ($1 \le i \le n+m$) are still distinct, and 2)
for each~$i$ we have $s_j = s_i^*$ and~$y_j = y_i^*$ for some~$j$.

\paragraph{Existence} %
Since $k$ is strictly positive definite, we can find~$\alpha_1$,
\ldots, $\alpha_{n+m} \in \Cset$ such that
$h = \sum_{i=1}^{n+m} \alpha_i k(\cdot, s_i)$ interpolates the
extended data~$(s_1, y_1)$, \ldots, $(s_{n+m}, y_{n+m})$. %
This function $h$ belongs to~$H_\Cset$ but not in general to~$H$. %
Set $g(s) = \frac{1}{2} \left( h(s) + h(s^*)^* \right)$. %
Then $g$ clearly satisfies the symmetry condition ($g(s^*) = g(s)^*$
for all~$s \in \Sset$) and still interpolates the extended
data~$(s_1, y_1)$, \ldots, $(s_{n+m}, y_{n+m})$. %
Moreover, using iii) from Theorem~\ref{thm:hermitian}, we obtain that
\begin{equation*}
  g(s) = \frac{1}{2} \sum_{i=1}^{n+m}
  \left( \alpha_i k(s, s_i) + \alpha_i^* k(s, s_i^*) \right),
\end{equation*}
which shows that~$g \in H_\Cset$, and thus $g \in H$. %
Besides, we easily see using~\eqref{equ:pseudo-kern-symm} that: if
$s_i = s_i^*$ then
\begin{equation}
  \frac{1}{2} \Bigl\{
    \alpha_i k(s, s_i) + \alpha_i^* k(s, s_i^*)
  \Bigr\}
  =
  \gamma_i k(s, s_i) + \gamma_i^* c(s, s_i)
\end{equation}
with $\gamma_i = \frac{1}{2} \alpha_i$, and if $s_j = s_i^*$ with
$i \neq j$ then
\begin{equation} \label{equ:trotro}
  \begin{gathered}
    \frac{1}{2} \Bigl\{
    \bigl( \alpha_i k(s, s_i) + \alpha_i^* k(s, s_i^*) \bigr)
    +
    \bigl( \alpha_j k(s, s_j) + \alpha_j^* k(s, s_j^*) \bigr)
    \Bigr\}\\
    =
    \bigl(
      \gamma_i k(s, s_i) + \gamma_i^* c(s, s_i)
    \bigr) + \bigl(
      \gamma_j k(s, s_j) + \gamma_j^* c(s, s_j)
    \bigr)
  \end{gathered}  
\end{equation}
with $\gamma_i = \frac{1}{2} \left( \alpha_i + \alpha_j^* \right)$
and~$\gamma_j = 0$. %
It follows that $g$ can be rewritten under the
form~\eqref{equ:interpolant:cr}, using the fact that~$\gamma_j = 0$
in~\eqref{equ:trotro} to get rid of the $m$ extra terms. %
Thus $\gamma = \left( \gamma_1, \ldots, \gamma_n \right)^\tra$
solves~\eqref{equ:interp-system:cr}, %
which proves the ``existence'' part of the theorem.

\paragraph{Uniqueness} %
Let $g \in H$ denote a function of the
form~\eqref{equ:interpolant:cr}, where the coefficients~$\gamma_i$ are
such that \eqref{equ:interp-system:cr}~holds. %
Using the property that $c(s, s_i) = k(s, s_i^*)$, any such function
can be rewritten as $g = \sum_{i=1}^{n+m} \alpha_i k(\cdot, s_i)$. %
Moreover, since the $s_i$'s are $n + m$ distinct points in~$\Sset$ and
$k$~is strictly positive definite, the
coefficients~$\alpha_i \in \Cset$ are uniquely determined by the
interpolation conditions: $g(s_i) = y_i$, $1 \le i \le n+m$. %
The first $n$ conditions come directly
from~\eqref{equ:interp-system:cr}, and the $m$ additional conditions
must hold as well by symmetry, since $g \in H$. %

For each~$i$ such that $s_i = s_i^*$, it is easily seen that
$\alpha_i = \gamma_i + \gamma_i^*$ is real, and thus the value
of~$\gamma_i$ is uniquely determined by~$\alpha_i$ and the additional
condition that~$\gamma_i = \gamma_i^*$. %
Similarly, if $s_i = s_j^*$ for some $i, j \le n$, $i \neq j$, then
$\alpha_i = \gamma_i + \gamma_j^*$,
$\alpha_j = \gamma_i^* + \gamma_j$, and therefore $\gamma_i$,
$\gamma_j$ are uniquely determined by $\alpha_i$, $\alpha_j$ and the
condition~$\gamma_i = \gamma_j^*$. %
Finally, if $s_i = s_j^*$ for some $i \le n$ and $j > n$, then
$\alpha_i = \gamma_i$. %
We have thus proved that there is a unique
$\gamma = \left( \gamma_1, \ldots, \gamma_n \right)^\tra$, with the
property that $\gamma_i = \gamma_j^*$ when $s_i = s_j^*$, such that
\eqref{equ:interp-system:cr}~holds.

\bigskip

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:


\section*{Acknowledgments}
The authors would like to thank Sabine Langer and Christopher Blech for suggesting acoustic benchmarks and for providing the plate-cavity model.

\bibliographystyle{siamplain}
\bibliography{ms}
\end{document}
