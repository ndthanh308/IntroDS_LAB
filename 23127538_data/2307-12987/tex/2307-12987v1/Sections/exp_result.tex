\subsection{Illustrative Results}\label{sec: result}

\subsubsection{System setup: the learning process of meta-market}
% proxy trading system can be learned
We first show the learning process of the surrogate trading system in Figure~\ref{fig: eval_proxy_model}. 
The figure shows that the loss of the training curve decreases with training epochs.
The loss on the validation set descends at the epoch of ten and flats off after that.
The plot demonstrates a common pattern of learning curves about training deep learning models.
This shows that reparameterizing the multi-agent market simulation using feature extraction is feasible, 
which validates our design of the surrogate trading system. 
In the following, we select the model at the epoch of ten as the surrogate trading system.

% There exist parameters that satisfy reconstruction and consistency 
We demonstrate the training process of the meta-market employing reconstruction (by the surrogate trading system) and temporal consistency loss in Figure~\ref{fig: eval_reconstruction_with_temp}.
It shows that both reconstruction error and behavior variation are reduced in the training process,
where both metrics decrease drastically in the first fifteen epochs and the reduction slows down after that. 
This verifies the possibility of optimizing both objectives concurrently --
there exist several sets of behavior parameters that contribute to similar order streams,
and the proposed two loss functions can optimize the two aspects 
by selecting parameter sets with temporal consistency.

\subsubsection{Market replay: evaluation on order stream reproduction and temporal consistency of the behavior parameters.}

% Overall result on consistency in the training set
We compare our approach with existing arts, as introduced in Section~\ref{sec: exp_set}, and 
show their distribution of behavior variations over the first nine months (training set) in Figure~\ref{fig: pdf_behavior_variation}.
% 
From the histogram, most behavior variations (between trading days) of the comparison schemes reside in the range from two to four. 
Such large variations do not reflect the market regularity that the overall participant behaviors vary continuously. 
Notably, the behavior variation of RandSearch is slightly larger than that of BayesianOPT;
this is because RandSearch introduces more randomness during its parameter searching.
In comparison, \sysname{} consider the temporal consistency of the behaviors and employ temporal consistency loss to train the meta-market.
Thus, most behavior variations of \sysname{}, from the figure, fall into the range from zero to two,
which observes the market regularity in calibration.

% Generality in the test set
To verify the generality of \sysname{}, 
we further compare behavior variations over the last three months~(the test dataset)
and  show the results in Figure~\ref{fig: pdf_test_variation}.
The figure shows that the behavior variation distribution of both comparison schemes 
do not change against the distribution in the training set. 
This is because they conduct searching for each trading day independently.
Undergoing an insignificant behavior variation increase, though,
\sysname{} still performs much better than random search and Bayesian optimization.
This verifies that \sysname{} has learned to conduct consistent calibration by considering the temporal consistency loss.

% Reconstruction error
Figure~\ref{fig: comp_train} compares the reconstruction error by cumulative distribution function~(CDF) 
over calibration on trading days in the training dataset.
From the figure, \sysname{} performs similarly to Bayesian optimization to achieve low reconstruction error,
while random search shows unsatisfactory results. 
This is because both \sysname{} and BayesianOPT involve objectives to reduce the error, 
while the parameter searching of RandSearch is pretty random.
Even though, 
both the comparison schemes require a search process with ten iterations, 
while \sysname{} searches parameters in one shot.
This verifies the efficiency of \sysname{}.

We further verify the reconstruction error of the comparison schemes on the test set in Figure~\ref{fig: comp_test}.
It shows that the performance of RandSearch and BayesianOPT does not change much since they search parameters independent of trading days.
In the figure,
even \sysname{} performs similarly to BayesianOPT in terms of reconstruction error, though,
\sysname{}  
is free from the tedious process of parameter searching 
and can find parameters that observe consistency rules~(as demonstrated in Figure~\ref{fig: pdf_test_variation}).
This shows the superiority of \sysname{} over its comparison schemes,
where it successfully accumulates calibration knowledge from historical data.

% Ablation study in market state loss
We conduct an ablation study on the market state consistency loss function
and compare \sysname{} with or without the loss function
in terms of reconstruction error, as shown in Figure~\ref{fig: abla_study}.
The figure shows that considering the market state facilitates calibration to reduce reconstruction error
in both training and test sets.
Notably, both schemes experience slight error increases from training to test data, though,
the error increase of \sysname{} with the loss function is smaller and more negligible, 
compared to its counterpart. 
This verifies that the market state is informative of calibration;
and, \sysname{} can successfully explore such information and leverage it to enhance its generalization ability.
% shows that considering the market state is beneficial for the generalization ability of the calibration.

\subsubsection{Case study on the market state: \sysname{} can capture real market patterns}

% Correlation in market state
To show that \sysname{} is informative of market state by employing the market state consistency loss,
we calculate the correlation coefficients between the market state indicators and the output behavior parameters
and  average the absolute value of the correlation coefficients  in Table~\ref{tab: correlation}.
From the table, 
the produced behavior parameters of \sysname{} are much more correlated to the market state indicators than 
the comparison schemes. 
% This is because \sysname{} considers the market state and employs the market state consistency loss function.
This shows the ability of \sysname{} to explore the market state information, 
as well as the effectiveness of the market state consistency loss function.
% This shows that \sysname{} successfully calibrates by observing the rule of market state consistency.

We conduct a case study on market trend
and show its correlation coefficients with the calibrated parameters in Figure~\ref{fig: market_trend_corr}:
when the market trend becomes significant, 
\sysname{} calibrates by increasing
the ratio of chartists and institutional investors 
while reducing the number of noise traders and the investment horizon of the whole market.
This reflects the market patterns -- the number of chartists increases will increase since
their trading strategies  fit markets with significant trends; 
naturally, the number of noise traders decreases with clear signals from the trends.
Meanwhile, institutions tend to enter stock markets when the market shows significant trends, or, sometimes, the influences are mutual.
Also, under such scenarios, participants who make profits are more likely to take profit, as well as the liquidation, 
leading to a low investment horizon over the whole market.






