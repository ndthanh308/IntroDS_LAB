%%
%% This is file `sample-acmsmall-conf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall-conf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-conf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf, anonymous, review]{acmart}


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Multi-Granularity Denoising Diffusion Model for Time Series Prediction}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \streetaddress{Rono-Hills}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \streetaddress{30 Shuangqing Rd}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \streetaddress{8600 Datapoint Drive}
  \city{San Antonio}
  \state{Texas}
  \country{USA}
  \postcode{78229}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 
  ...
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{time series prediction, }
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   % Figure removed
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Time series prediction is the task of predicting the future value over a
period of time, which is a critical demand for real applications such as stock trend prediction and energy consumption planning.  
Diffusion models have been utilized for time series prediction by approximating the data distribution to predict future value by sampling from the distribution of interest. 

However, diffusion models for time series prediction suffer two challenges in practice:

\begin{itemize}

    \item One major issue is that diffusion models are often difficult to train, and have limited generalizability, due to the stochastic process of diffusion~\cite{maze2022topodiff} and the absence of explicit guidance methods~\cite{ho2022classifier}.

    \item Second issue is the intrinsic low efficiency of the inference process. Diffusion models such as DDPMs often need hundreds or even thousands of denoising steps to obtain a high-quality sample from the Gaussian noise, contributing to low inference efficiency~\cite{lyu2022accelerating}.
    
\end{itemize}

  % Usually, coarse-grained data reflects overall trends, while fine-grained data represents detailed changes in small time windows, both are crucial to prediction quality in a specific task.  

% In this work, we propose to fuse multi-granularity features using a conditional diffusion model and sample from multi-granularity distributions.    

% Diffusion models have been utilized for time series prediction by approximating the posterior distribution by conditioning on the past values.  While these approximations may work well, they do not correspond to certain conditional distributions such as multi-granularity features of time series.

% 多粒度方法 独立做多粒度 或相关性。如何利用diffusion的优点，diffusion的结构优势

% In the task of time series prediction, coarse-grained data reflects overall trends, while fine-grained data represents detailed changes in small time windows, both are crucial to prediction quality in a specific task.  
In this work, we aims to learn a multivariate probabilistic time series prediction model using autoregressive conditional diffusion models.  In order to decrease the diversity of the samples and increase the quality of each sample, we propose a conditional diffusion model with classifier-free guidance and guide the diffusion process using multi-granularity information.  Specifically, we apply a denoising diffusion operation for each granularity data with different steps (more steps for the fine-grained data and fewer steps for the coarse-grained data).  Then, for each granularity, the information from other granularity data is added as condition on diffusion process.  In this way, multi-granularity information can be applied as a guidance in the diffusion processes for each granularity. 
% As for the sampling stage, a more efficient sampling method starting from coarse-grained distribution can be applied.

% 多粒度信息的利用：challenge 需要大量数据，可以利用多粒度数据作为constraint

% diffusion 1. 数据量  2. sample的效率

\section{Background}

\subsection{Denoising diffusion probabilistic models}

Denoising diffusion probabilistic models aim to learn a model distribution $p_\theta (x_0)$ that approximates a data distribution $q(x_0)$.
Let $x_n$ for $n = 1; \cdots ; T$ be a sequence of latent variables in the same sample space as $x_0$. Diffusion probabilistic models are latent variable models that are composed of two processes: the forward process and the reverse process. The forward process is defined by the following Markov chain:

\begin{equation}
q (x_{1:T} {|} x_0) = \prod^T_{t=1} q (x_{t} {|} x_{t-1}), 
\end{equation}

where 

\begin{equation}
q (x_{t} {|} x_{t-1}) := \mathcal{N} (\sqrt{1-\beta_t} x_{t-1}, \beta_t I)
\label{eq:obj-dis}
\end{equation}

The forward process uses an increasing variance schedule
 $\beta_1, \cdots, \beta_N$ with $\beta_n \in (0, 1)$.  $n$ represents a noise level.  The sampling of $x_t$ can be written as:
 
\begin{equation}
q(x_{t} {|} x_{0}) = \mathcal{N}(x_t; \sqrt{\Bar{\alpha_t}}x_0, (1- \Bar{\alpha}_t)I), 
\end{equation}

where $\alpha_n := 1 - \beta_n$ and $\Bar{\alpha}_n = \prod^n_{i=1} \alpha_i$.  Then the reverse
process denoises $x_t$ to recover $x_0$, and is defined by the following Markov chain:

\begin{equation}
p_\theta(x_{0:N}) := p(x_{N}) \prod^1_{n=N} p_\theta (x_{n-1} {|} x_{n}), 
\end{equation}
\begin{equation}
p_\theta (x_{n-1} {|} x_{n}) := \mathcal{N}(x_{t-1}; \mu_\theta (x_{t}; t), \sigma_\theta(x_t, t)I).
\label{eq:reverse}
\end{equation}

According to denoising diffusion probabilistic models (DDPM)~\cite{ho2020denoising}, the parameterization of $p_\theta (x_{t-1} {|} x_t)$ is as follows:

\begin{equation}
\mu_\theta(x_t, t) = \frac{1}{\alpha_t} ( x_t - \frac{\beta_t}{\sqrt{1-\alpha_t}} \epsilon_\theta(x_t, t)), 
\end{equation}

\begin{equation}
\sigma_\theta(x_t, t) = \frac{1-\alpha_{t-1}}{1-\alpha}{\beta_t} 
\end{equation}

where $\epsilon_\theta$ is a network which predicts     $\mathcal{N}(0, I)$ from $x_n$.  Under this parameterization, Ho et al.~\cite{ho2020denoising} have shown that the reverse process can be trained by solving the following optimization problem:

\begin{equation}
\mathcal{L}( \theta) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I), t, n} {||}(  \epsilon - \epsilon_\theta (x_{t}^{n}, n)){||}^2_2 ,
\end{equation}

where 

\begin{equation}
x_t^{n} = \sqrt{\alpha_{n}} x_t^0 + (1 -\alpha_{n})\epsilon.
\end{equation}

The denoising function $\epsilon_\theta$ estimates the noise vector $\epsilon$ that was added to its noisy input $x_n$.

\subsection{Time series prediction with diffusion models}

Here, we focus on time series prediction task.  Given time series data $ X_t, t \in [1, T]$, we are tasked with modeling the conditional distribution of the future of fine-grained time series given the past multi-granularity time-series data. We split this contiguous sequence into a context interval $[ 1, t_0 ]$ and a prediction interval$[ t_0, T ]$. 

The aim of time series prediction is to learn a model of the conditional distribution of the future time steps given its past:

\begin{equation}
q (x_{t_0:T} {|} x_{1:t_0-1}) = \prod^T_{t=t_0} q (x_{t} {|} x_{1:t-1})
\label{eq:obj-dis}
\end{equation}

To model the temporal dynamics we employ the autoregressive recurrent neural network (RNN) architecture to encode the time series sequence up to time point $t$ of multi-granularity data.  For each granularity, we utilize an individual RNN to update hidden state $h_t^m$:

\begin{equation}
h_t = RNN ( x_t, h_{t-1}).
\end{equation}

where $h_t^m$ is the hidden state of granularity $m$ at time $t$ implemented by a multi-layer recurrent neural network with LSTM cells. 

\section{Method}

% Figure environment removed{}

\subsection{multi-granularity time series prediction}



In the multi-granularity time series prediction task, the multi-granularity data $\{X^1, \cdots ,X^M\}$ represents coarse-grained to fine-grained time-series data. 
% The multivariate vector at time $t$ is given by $x_t^1$ and $x_t^M$.  
For each granularity $m$, the data $X_m = [x^m_1
, \cdots , x^m_T]$.  Given time series data $t \in [1, T]$, we are tasked with modeling the conditional distribution of the future of fine-grained time series given the past multi-granularity time-series data. We split this contiguous sequence into a context interval $[ 1, t_0 ]$ and a prediction interval$[ t_0, T ]$.  

We aim to learn a model of the conditional distribution of the future time steps of a multivariate time series given its past of each granularity $m$:

\begin{equation}
q (x^m_{t_0:T} {|} x^m_{1:t_0-1}) = \prod^T_{t=t_0} q (x^m_{t} {|} x_{1:t-1})
\label{eq:obj-dis}
\end{equation}

To model the temporal dynamics we employ the autoregressive recurrent neural network (RNN) architecture to encode the time series sequence up to time point $t$ of multi-granularity data.  For each granularity, we utilize an individual RNN to update hidden state $h_t^m$:

\begin{equation}
h_t^m = RNN^m ( x_t^m, h^m_{t-1}).
\end{equation}

where $h_t^m$ is the hidden state of granularity $m$ at time $t$ implemented by a multi-layer recurrent neural network with LSTM cells. 

\subsection{Multi-Granularity Diffusion Model}

In this work, we propose to use multi-granularity data 
as conditional guidance of diffusion process, which allows us to exploit useful information in multi-granularity data for accurate prediction. Thus, the forward process is defined by the following Markov chain:

\begin{equation}
q (x^m_{1:T} {|} x_0) = \prod^T_{t=1} q (x^m_{t} {|} x^m_{t-1}), 
\end{equation}

where 

\begin{equation}
q (x^m_{t} {|} x^m_{t-1}) := \mathcal{N} (\sqrt{1-\beta_t} x_{t-1}, \beta_t I)
\label{eq:obj-dis}
\end{equation}

The forward process uses an increasing variance schedule
 $\beta_1, \cdots, \beta_N$ with $\beta_n \in (0, 1)$.  $n$ represents a noise level.  The sampling of $x_t$ can be written as:
 
\begin{equation}
q(x_{t} {|} x_{0}) = \mathcal{N}(x_t; \sqrt{\Bar{\alpha_t}}x_0, (1- \Bar{\alpha}_t)I), 
\end{equation}

where $\alpha_n := 1 - \beta_n$ and $\Bar{\alpha}_n = \prod^n_{i=1} \alpha_i$.  Then the reverse
process denoises $x_t$ to recover $x_0$, and is defined by the following Markov chain:

\begin{equation}
p_\theta(x_{0:N}) := p(x_{N}) \prod^1_{n=N} p_\theta (x_{n-1} {|} x_{n}), 
\end{equation}
\begin{equation}
p_\theta (x_{n-1} {|} x_{n}) := \mathcal{N}(x_{t-1}; \mu_\theta (x_{t}; t), \sigma_\theta(x_t, t)I).
\label{eq:reverse}
\end{equation}
We focus on the conditional diffusion model with the denoising process and aim to model the conditional distribution $p_\theta(x_{0:N^m}^{m,t}{|}H_{t-1} )$ conditioned on multi-granularity time series. Specifically, we extend DDPM in Eq.~\ref{eq:reverse} to the conditional case to define a conditional denoising function of multi-granularity time series as:

\begin{equation}
p_\theta(x_{0:N^m}^{m,t} {|}  H_{t-1} ) := p(x^{m,t}_{N^m}) \prod^1_{n=N^m} p_\theta (x_{t,n^m-1}^m {|} x_{t,n^m}^m, H_{t-1}),
\end{equation}

% 删掉M

where  

% \begin{equation}
% p_\theta (x_{t,n-1}^M {|} x_{t^M,n}^M, H_{t^M-1}) := \mathcal{N} (x_{t,n-1}^M; \mu_\theta(x_{t,n}^M, n^M{|} H_{t^M-1}), \sigma_\theta(x_{t,n}^M, n^M{|} H_{t^M-1})I),
% \end{equation}

\begin{equation}
H_{t-1} = \sum_{m=1}^M h_{t-1}^m.
\end{equation}

Then the conditional distribution of Eq.~\ref{eq:obj-dis} can be approximated by the model: 

\begin{equation}
q (x^m_{t_0:T} {|} x^m_{1:t_0-1}) =  \prod^T_{t=t_0} p_\theta (x_{t,m}^0 {|} H_{t-1}),
\end{equation}

where $\theta$  comprises the weights of RNN for each granularity as well as the denoising diffusion model.  

Training is performed by randomly sampling context and adjoining prediction-sized windows from the training time series data and optimizing the parameters that minimize the negative log-likelihood for each granularity. 

\begin{equation}
\sum_{t=t_0}^{T^M} -log p_\theta(x^m_t{|}H_{t-1})
\end{equation}

According to the DDPM process, the objective for time step $t$ and noise index $n$ can be formalized as:

% \begin{equation}
% \min_\theta \mathcal{L}( \theta) := \min_\theta 
% \sum_{m=1}^M \mathcal{L}^M( \theta),
% \end{equation} 

% where

\begin{equation}
\mathcal{L}^m( \theta) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I), t, n^m} {||}(  \epsilon - \epsilon_\theta (x_{t}^{m,n^m}, H_{t-1}, n^m)){||}^2_2 ,
\end{equation}

where 

\begin{equation}
x_t^{n^m} = \sqrt{\Bar{\alpha}_{n^m}} x_t^0 + (1 -\Bar{\alpha}_{n^m})\epsilon .
\end{equation}

% \begin{equation}
% \mathcal{L}^M( \theta) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} {||}(  \epsilon - \epsilon_\theta (x_{t,M}^0 {|} H_{t-1} )){||}^2_2
% \end{equation}

where $\alpha_n := 1 - \beta_n$ and $\Bar{\alpha}_n = \prod^n_{i=1} \alpha_i$. $\beta_n$ is the variance.  We choose the variance to be $\Sigma_\theta = \beta_n $. $\omega$ is the guidance strength.  Then we can pass autoregressively to the RNN to obtain the next hidden state $h^m_{T+1}$ and repeat until the desired forecast horizon has been reached.

Here we consider two granularities and introduce how the coarse-grained diffusion process is applied as a constraint for fine-grained diffusion.  
The diffusion step of coarse-grained data is $N^C$, while the step of fine-grained data is $N^F$.  $N^C = N^F / 2$.  The variance schedule of coarse-grained data is $\{\beta_1, \cdots , beta_{N^C} \}$.  As for fine-grained diffusion, we share the coarse-grained variance as the variance for first $N^F/2$ steps.  In this way, the variance schedule of fine-grained data can be represented as: $\{\beta_1, \cdots, \beta_{N^C}, \beta_{N^{C+1}}, \cdots, \beta_{N^F} \}$.
% The reverse process for the two granularities are:
% \begin{equation}
% p_\theta(x_{0:N^F}^{F,t} {|}  H_{t-1} ) := p(x^{F,t}_{N^F}) \prod^1_{n=N^F} p_\theta (x_{t,n^F-1}^F {|} x_{t,n^F}^F, H_{t-1}),
% \end{equation}
% \begin{equation}
% p_\theta(x_{0:N^C}^{C,t} {|}  H_{t-1} ) := p(x^{C,t}_{N^C}) \prod^1_{n=N^C} p_\theta (x_{t,n^C-1}^C {|} x_{t,n^C}^C, H_{t-1}),
% \end{equation}
Thus the objective functions for the two granularities are:

\begin{equation}
\Fathcal{L}^F( \theta) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I), t, n^F} {||}(  \epsilon - \epsilon_\theta (\sqrt{\Bar{\alpha}_{n^F}} x_t^0 + (1 -\Bar{\alpha}_{n^F})\epsilon, H_{t-1}, n^F)){||}^2_2 ,
\end{equation}

\begin{equation}
\Cathcal{L}^C( \theta) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I), t, n^C} {||}(  \epsilon - \epsilon_\theta (\sqrt{\Bar{\alpha}_{n^C}} x_t^0 + (1 -\Bar{\alpha}_{n^C})\epsilon, H_{t-1}, n^C)){||}^2_2 ,
\end{equation}

where 

\begin{equation}
\Bar{\alpha}_{n^C} = \prod^{n^C}_{i=1} (1 - \beta_i),
\end{equation}

% \prod^{n^F}_{i=1} (1 - \beta_i) =

\begin{equation}
\Bar{\alpha}_{n^F} =  \left\{  
    \begin{array}
        \prod^{N^C}_{i=1} (1 - \beta_i), & n^F < N^C   \\  
        \prod^{N^C}_{i=1} (1 - \beta_i) \prod^{n^F - N^C}_{i=N^{C+1}} (1 - \beta_i), & N^C <= n^F < N^F
     \end{array}  
\right.  
\end{equation}



After training, we apply conditional sampling with classifier-free guidance~\cite{ho2022classifier} to predict future steps of time series in our data set and compare with the corresponding test set time series. As in training, we run the RNN over the last window of the training set to obtain the hidden state for each granularity.  As for sampling, we obtain a sample $x^{0,m}_{T+1}$ of the next time step according to:

\begin{equation}
x^{n^m-1,m}_t = \frac{1}{\sqrt{\alpha_{n^m}}} (x_t^{n^m,m} - \frac{\beta_{n^m}}{\sqrt{1 - \Bar{\alpha_{n^m}}}} \epsilon_\theta(x_t^{n^m,m}, H_{t-1}, n^m)) + \sqrt{\Sigma_\theta}z,
\end{equation}

% where

% \begin{equation}
% \tilde{\epsilon}_\theta(x_t^{n^m,m}, H_{t-1}, n^m) = (1+\omega) \epsilon_\theta(x_t^{n^m,m}, H_{t-1}, n^m) - \omega \epsilon_\theta(x_t^{n^m,m}, n^m)
% \end{equation}

where $\beta_n$ is the variance, $\alpha_n := 1 -  \beta_n$ and $\Bar{\alpha}_n = \prod^n_{i=1} \alpha_i$.  We choose the variance to be $\Sigma_\theta = \beta_n $. $\omega$ is the guidance strength.  Then we can pass autoregressively to the RNN to obtain the next hidden state $h^m_{T+1}$ and repeat until the desired forecast horizon has been reached.

\section{Experiments}

\subsection{Experiment settings}

\subsubsection{Datasets}
We plan to use three real-world datasets that are commonly used for evaluating probabilistic time series forecasting.

\begin{itemize}
    \item Stock Data: The datasets range from Feb. 16, 2007 to Jan. 1, 2020 with 908,606 records of 749 stocks. We split the sequences by time, the train/val/test is 94/12/12 months. 6 commonly used statistics are extracted as features, including the highest price, the opening price, the lowest price, the closing price, volume-weighted average price, and trading volume.  The granularity of input features is 1 day, 1 hour, 15 mins, 5 mins, and 1 min.

    \item Electricity: hourly electricity consumption of 370 customers.  We aim to predict the daily consumption of each client. The granularity of input features is 1 day, 12 hours, 4 hours, 1 hour, and 15 minutes.

    \item Traffic: hourly occupancy rate of 963 San Fancisco freeway car lanes.  The measurements cover the period from Jan. 1st 2008 to Mar. 30th 2009 and are sampled every 10 minutes. 
    
\end{itemize}

\subsubsection{Comparison Methods.}

The competitive baselines we compared can be categorized into four groups.

\begin{itemize}
    \item General time series forecasting models: GRU, LSTM, Transformer, DeepAR.

    \item Models using different granularities of data such as Multi-grained RNN.

    \item Deep generative models: Transformer-MAF, TimeGrad, CSDI.
    
\end{itemize}

\subsubsection{Evaluation Metrics}

we adopt three commonly used evaluation metrics, including Pearson correlation coefficients (CORR), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE).


% 不同M的T大小关系，
% 公式5左边用公式1
% 公式9
\bibliographystyle{acm}
\bibliography{acmart}

\end{document}
\endinput
%%
%% End of file `sample-acmsmall-conf.tex'.
