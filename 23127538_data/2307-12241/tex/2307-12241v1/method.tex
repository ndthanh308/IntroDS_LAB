%
In this section, we outline the methodology for discovering kinemes from short overlapping video segments. Initially, we discovered kinemes utilising data segments from both control and patient classes (two-class kineme discovery or 2CKD approach; see Section~\ref{Sec:EKF}). Subsequently, we learned kinemes solely from the healthy control cohort and utilised them to represent the head pose data of depressed patients (denoted as healthy control kineme discovery or HCKD) approach. 

Given a time series $\boldsymbol{\theta}$, we divide it into short overlapping segments of uniform length, with a segment duration of $5s$ for the BlackDog and $2s$ for the AVEC dataset. These segment lengths were empirically chosen and provided the best results from among segment lengths spanning $2s$ to $7s$ for both datasets. For both approaches, a total of $K = 16$ kinemes are learned from the two datasets as per the procedure outlined in Section~\ref{Sec:KF}.

%
%----------------------------------------
%
\subsection{Kineme Discovery from Two-class Data} \label{Sec:approach1}
To examine whether the kinemes discovered from head pose angles of both classes are effective cues for depression detection, we learn kinemes from segments corresponding to both patient and control videos. 
Upon discovering the kineme values, the \emph{relative frequency} $\eta_{K_i}$ of each kineme $K_i$ is computed over the two classes as:
\vspace{-1mm}
%
\begin{equation}
    \mathbf{\eta}_{K_i} = \frac{\emph{f } ( K_i) } {\sum_{i=1}^{16} \emph{f } ( K_i)}
\end{equation}
%
where $\emph{f} (K_i)$ represents the frequency of occurrence of the kineme $K_i$ for a particular class. We then compute the relative frequency difference for each kineme between the two classes to identify the ten most differentiating kinemes (four kinemes per class are depicted in Figs.~\ref{fig:kinemes_bdi}, \ref{fig:kinemes_avec}). Next, we generate a feature set by extracting the frequencies of the selected kinemes over the thin-slice chunks considered for analysis. Thus, we obtain a 10-dimensional feature vector representing kineme frequencies for each chunk. 


%
%----------------------------------------
%
\subsection{Kineme Discovery from Control Data}
\label{Sec:approach2}
Here, we learn kinemes representing head motion solely from the control cohort. Subsequently, head pose segments from both the patient and control classes are represented via the discovered kinemes, and reconstruction errors computed.  Let the raw head pose vector $\mathbf{h}^{(i)}$ for the $i^{th}$ segment in the original subspace be denoted as:
%
\begin{equation}
\mathbf{h}^{(i)} = [\theta_p^{i:i+\ell}\, \theta_y^{i:i+\ell}\, \theta_r^{i:i+\ell}]
\end{equation}
%
Let the kineme value associated with this segment be $K^{(i)}$. Based on the kinemes discovered from the control cohort alone, we calculate the reconstructed kineme for the $i^{th}$ segment as $\tilde{\mathbf{h}}^{(i)}$. The reconstructed vector for each kineme is determined by converting the GMM cluster centre for each kineme from the learned space to the original \emph{pitch}-\emph{yaw}-\emph{roll} space. The reconstructed head pose vector for the segment is:
%
\begin{equation}
    \tilde{\mathbf{h}}^{(i)} = [\tilde{\theta}_p^{i:i+\ell}\, \tilde{\theta}_y^{i:i+\ell}\, \tilde{\theta}_r^{i:i+\ell}]
\end{equation}
%
To compute the reconstruction error for both depressed patients and healthy controls, we compute the signed difference between the two vectors for each segment to account for the difference between raw head pose vector and the GMM cluster centres.
We calculate the difference vector $\mathbf{d}^{(i)}$ for each $i^{th}$ segment as: 
\vspace{-1mm}
%
\begin{equation}
    \mathbf{d}^{(i)} = \mathbf{h}^{(i)} - \tilde{\mathbf{h}}^{(i)} = [d_p^{i:i+\ell}\, d_y^{i:i+\ell}\, d_r^{i:i+\ell}]
\end{equation}
%
These signed differences values are added over each angular dimension of pitch (\emph{p}) , yaw (\emph{y}), and roll (\emph{r}) for the segment.
%
\begin{equation}
        {s_e}^{(i)} = \sum_{n=1}^{\ell} d_e^{i:i+n} 
\end{equation}
%
where each ${s_e}^{(i)}$ is calculated for each angular dimension $e \in \{p, y, r\}$ over all segments of both classes. 
Depending on the thin-slice chunk duration considered for classification, we compute different descriptive statistics to generate the feature set. Considering number of elementary kineme chunks in the considered time-window to be $n_c$, we obtain the following feature vector for each angle  $e \in \{p, y, r\}$:
\vspace{-1mm}
%
\begin{equation}
        \mathbf{as_e} = [\lvert{s_e}^{(1)} \rvert, \lvert{s_e}^{(2)}\rvert, \cdots, \lvert{s_e}^{(n_c)}\rvert ] 
\end{equation}
%
where $\lvert \cdot \rvert$ represents the absolute value. We then calculate eight statistical features from the above vectors, namely, \emph{minimum}, \emph{maximum}, \emph{range}, \emph{mean}, \emph{median}, \emph{standard deviation}, \emph{skewness}, and \emph{kurtosis} (total of $8 \times 3$ features over the yaw, pitch, roll dimensions).















