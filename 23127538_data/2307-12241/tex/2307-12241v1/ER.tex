%
%----------------------------------------
%
\begin{sloppypar}
We perform binary classification on the BlackDog and AVEC2013 datasets, plus 4-class classification on AVEC2013. This section details our datasets, experimental settings and learning algorithms.
\end{sloppypar}

%
%----------------------------------------
%
\subsection{Datasets}
\label{sec:datasets}
We examine two datasets in this study: clinically validated data collected at the Black Dog Institute -- a clinical research facility focusing on the diagnosis and treatment of mood disorders such as anxiety and depression (referred to as \emph{BlackDog} dataset) -- and the \emph{Audio/Visual Emotion Challenge} (AVEC2013) depression dataset. 

\textbf{BlackDog Dataset~\cite{alghowinem2016multimodal}:} This dataset comprises responses from healthy controls and depression patients selected as per the criteria outlined in the Diagnostic and Statistic Manual of Mental Disorders (DSM-IV). Healthy controls with no history of mental illness and patients diagnosed with severe depression were carefully selected~\cite{alghowinem2016multimodal}. For our analysis, we focus on the structured interview responses in~\cite{alghowinem2016multimodal}, where participants answered open-ended questions about life events, designed to elicit spontaneous self-directed responses, asked by a clinician. 
%These videos capture subject responses to questions about life events, designed to elicit spontaneous self-directed responses. 
In this study, we analyse video data from 60 subjects (30 depressed patients and 30 healthy controls), with interview durations ranging from $183-1200s$. 

\textbf{AVEC2013 Dataset~\cite{valstar2013avec}:} Introduced for a challenge in 2013, this dataset is a subset of the audio-visual depressive language corpus (AViD-corpus) comprising 340 video recordings of participants performing different PowerPoint guided tasks detailed in~\cite{valstar2013avec}. The videos are divided into three nearly equal partitions (training, development, and test) with videos ranging from $20-50 min$. Each video frame depicts only one subject, although some participants feature in multiple video clips. The participants completed a multiple-choice inventory based on the Beck Depression Index (BDI)~\cite{beck1996comparison} with scores ranging from 0 to 63 denoting the severity of depression. For binary classification, we dichotomise the recordings into the non-depressed and depressed cohorts as per the BDI scores. Subjects with a BDI score $\leq 13$ are categorised as \emph{non-depressed}, while the others are considered as \emph{depressed}. 

\textbf{AVEC2013 Multi-Class Classification:} For fine-grained depression detection over the AVEC2013 dataset, we categorise the dataset based on the BDI score into four classes as detailed below: 
\begin{itemize}
    \item Nil or minimal depression: BDI score 0 - 13
    \item Mild depression: BDI score 14 - 19
    \item Moderate depression: BDI score 20 - 28
    \item Severe depression: BDI score 29 - 63
\end{itemize}


%
%----------------------------------------
%
\subsection{Experimental Settings}
\textbf{Implementation Details:} For binary classification, we evaluate performance for the smaller \emph{BlackDog} dataset via 5-repetitions of 10-fold cross-validation (10FCV). For the AVEC2013, the pre-partitioned train, validation and test sets are employed. We utilise the validation sets for fine-tuning classifier hyperparameters.

\textbf{Chunk vs Video-level Classification:} The videos from both datasets are segmented into smaller chunks of $15s - 135$s length, to examine the influence of \emph{thin-slice} chunk duration on the classifier performance. We repeated the video label for all chunks and metrics are computed over all chunks for chunk-level analysis. Additionally, video-level classification results are obtained by computing the majority label over all video chunks in the test set. 

\textbf{Performance Measures:} For the BlackDog dataset, results are shown as $\mu \pm \sigma$ values over 50 runs ($5\times$ 10FCV repetitions). For AVEC2013, performance on the test set is reported. For both, we evaluate performance via the accuracy (Acc), weighted F1 (F1), precision (Pr), and recall (Re) metrics. The weighted F1-score denotes the mean F1-score over the two classes, weighted by class size.

%
% We discover kinemes as mentioned in Sec.~\ref{Sec:approach2} using the minimally depressed participants of the AVEC2013 dataset and use these learned kinemes to represent the head pose values from all classes. After computing the reconstruction error between the raw and learned vectors, we employ the 8 statistical features over the 3 angular dimensions ($8 \times 3$) for classification using the pre-partitioned sets of train, test and development data. The results for AVEC2013 multi-class classification are presented in Sec.~\ref{sec: 4class_res}.


%
%----------------------------------------
%
\subsection{Classification Methods}
Given that our proposed features do not model spatial or temporal correlations, we employ different machine learning models for detecting depression as described below:
%
\begin{itemize}
    \item \textbf{Logistic Regression (LR)}, a probabilistic classifier that employs a sigmoid function to map input observations to binary labels. We utilise extensive grid-search to fine-tune parameters such as penalty $\in \{ l1, l2, None \}$ and regulariser $\lambda \in \{ 1e^{-6}, \cdots, 1e^{3}\}$.
    \item\textbf{Random Forest (RF)}, where multiple decision trees are generated from training data whose predictions are aggregated for labelling. Fine-tuned parameters include the number of estimators $N \in [2, \cdots,  8]$, maximum depth $\in [3, \cdots,  7]$, and maximum features in split $\in [3, \cdots,  7]$.
    \item \textbf{Support Vector Classifier (SVC)}, a discriminative classifier that works by transforming training data to a high-dimensional space where the two classes can be linearly separated via a hyperplane. For SVC, we examine different kernels $\in \{ rbf, poly, sigmoid\}$ and fine-tune regularisation parameter $C \in \{ 0.1, 1, 10, 100 \}$ and kernel coefficient $\gamma \in \{ 0.0001, \cdots, 1, scale, auto \}$. 
    \item \textbf{Extreme Gradient Boosting (XGB)}, a model built upon a gradient boosting framework, and focused on improving a series of weak learners by employing the gradient descent algorithm in a sequential manner. The fine-tuned hyperparameters include the number of estimators $\{ 50, 100, 150 \}$, maximum depth $\in [3, \cdots,  7]$ of the tree and learning rate $\in [0.0005, \cdots,  0.1]$.
    \item \textbf{Multi Layer Perceptron (MLP)}, where we employed a feed-forward neural network with two hidden dense layers comprising 12 and 6 neurons, resp., with a rectified linear unit (ReLU) activation. For training, we employ categorical cross-entropy as the loss function and fine-tune the following hyperparameters: learning rate $\in \{ 1e^{-4}, 1e^{-3}, 1e^{-2}\}$, and batch size $\in \{ 16, 24, 32, 64 \}$. We utilise the Adam optimiser for updating the network weights during training.
\end{itemize}


%
%--------------------------------------------------
%
\section{Results and Discussion}\label{sec:ResultsDiscussion}
% \vspace{-8mm}

% \vspace*{1mm}
\begin{table*}[ht]
 \caption{Chunk and Video-level classification results on the BlackDog dataset with the 2CKD and HCKD approaches. Accuracy (Acc), F1, Precision (Pr) and Recall (Re) are tabulated as ($\mu \pm \sigma$) values. } \vspace{-2mm}
% \begin{tabular}{|l l||cc||cc|} 
% \hline
%     \bf Classifier & \bf Features & \multicolumn{2}{c||}{\textbf{Chunk-level}} & \multicolumn{2}{c|}{\textbf{Video-level}} \\ 
%     & & \textbf{Acc} & \textbf{F1} & \textbf{Acc} & \textbf{F1} \\ 
%     \hline \hline

%     \textbf{Logistic Regression} & 2CKD & 0.60 ± 0.15 & 0.61 ± 0.14 & 0.60 ± 0.20 & 0.59 ± 0.21 \\ 
%     \textbf{Random Forest}& 2CKD & 0.58 ± 0.13 & 0.60 ± 0.12 &0.61 ± 0.19 & 0.62 ± 0.19\\ 
%     \textbf{SV Classifier}& 2CKD & 0.60 ± 0.15 & 0.62 ± 0.15 & 0.62 ± 0.19 & 0.63 ± 0.19 \\
%     \textbf{Logistic Regression}& HCKD  & 0.77 ± 0.13 & 0.78 ± 0.12 & 0.79 ± 0.16 & 0.78 ± 0.17 \\ 
%     %\hline
%     \textbf{Random Forest}& HCKD &0.71 ± 0.13 & 0.73 ± 0.12 &0.76 ± 0.15 & 0.76 ± 0.16\\ 
%     %\hline
%     \textbf{SV Classifier}& HCKD & \textbf{0.78 ± 0.14} & \textbf{0.79 ± 0.13} & \textbf{0.80 ± 0.18} & \textbf{0.80 ± 0.19} \\ 
%     %\hline
%     \textbf{XG Boost}& HCKD & 0.72 ± 0.13 & 0.72 ± 0.12 & 0.78 ± 0.17 & 0.78 ± 0.17 \\ 
%     %\hline
%     \textbf{MLP Classifier}& HCKD & 0.75 ± 0.13 & 0.76 ± 0.12 & 0.76 ± 0.16 & 0.76 ± 0.16 \\
%     \hline
% \end{tabular} 
%\vspace{-2mm}

%reformatted table
\begin{tabular}{|l l||cc cc||cc cc|} 
\hline
    \bf Condition & \bf Classifier    & \multicolumn{4}{c||}{\textbf{Chunk-level}} & \multicolumn{4}{c|}{\textbf{Video-level}}    \\ 
     & & \textbf{Acc} & \textbf{F1} & \textbf{Pr} & \textbf{Re} & \textbf{Acc} & \textbf{F1} & \textbf{Pr} & \textbf{Re}\\ 
    \hline\hline
    \multirow{5}{*}{\bf 2CKD}& \textbf{LR} &  0.60±0.15 & 0.61±0.14 &  0.67±0.22 & 0.65±0.22 & 0.60±0.20 & 0.59±0.21 &  0.55±0.30 & 0.65±0.33  \\ 
    & \textbf{RF} & 0.58±0.13 & 0.60±0.12 &  0.67±0.21 & 0.61±0.21  &0.61±0.19 & 0.62±0.19 &  0.59±0.32 & 0.59±0.32 \\ 
    & \textbf{SVC} & 0.60±0.15 & 0.62±0.15&  0.68±0.25 & 0.62±0.25  & 0.62±0.19 & 0.63±0.19 &  0.61±0.32 & 0.59±0.33 \\
    %\hline
    & \textbf{XGB}& 0.55±0.17 & 0.54±0.16 &  0.63±0.21 & 0.71±0.22  & 0.53±0.17 & 0.50±0.20 &  0.54±0.23 & 0.79±0.21 \\ 
    %\hline
    & \textbf{MLP} & 0.53±0.15 & 0.52±0.17 &  0.60±0.22 & 0.71±0.21 & 0.51±0.20 & 0.47±0.21 &  0.53±0.27 & 0.74±0.32 \\ \hline
    \multirow{5}{*}{\bf HCKD} & \textbf{LR}  & 0.77±0.13 & 0.78±0.12 &  0.85±0.19 & 0.74±0.21 & 0.79±0.16 & 0.78±0.17 &  0.81±0.30 & 0.66±0.31 \\ 
    %\hline
    & \textbf{RF} &0.71±0.13 & 0.73±0.12&  0.75±0.25 & 0.71±0.20  &0.76±0.15 & 0.76±0.16 &  0.75±0.26 & 0.82±0.25 \\ 
    %\hline
    & \textbf{SVC} & {0.78±0.14} & \textbf{0.79±0.13}&  0.87±0.18 & 0.74±0.20  & {0.80±0.18} & \textbf{0.80±0.19} &  0.83±0.30 & 0.70±0.31 \\ 
    %\hline
    & \textbf{XGB}& 0.72±0.13 & 0.72±0.12&  0.75±0.18 & 0.81±0.15  & 0.78±0.17 & 0.78±0.17 &  0.74±0.27 & 0.82±0.27  \\ 
    %\hline
    & \textbf{MLP} & 0.75±0.13 & 0.76±0.12 &  0.78±0.21 & 0.81±0.21 & 0.76±0.16 & 0.76±0.16 &  0.74±0.29 & 0.77±0.28 \\
    \hline
\end{tabular} 
\vspace{-2mm}
\label{tab:BDI_res1}
\end{table*}
%
% \vspace*{5mm}
%
\begin{table*}[ht]
\caption{\label{tab:AVEC_res1} Chunk and Video-level classification results on the AVEC2013 dataset with the 2CKD and HCKD approaches. Accuracy (Acc), F1, Precision (Pr) and Recall (Re) are tabulated as ($\mu \pm \sigma$) values.}
\begin{tabular}{|l l||cc cc||cc cc|} 
    \hline
    \bf Condition & \bf Classifier    & \multicolumn{4}{c||}{\textbf{Chunk-level}} & \multicolumn{4}{c|}{\textbf{Video-level}}    \\ 
     & & \textbf{Acc} & \textbf{F1} & \textbf{Pr} & \textbf{Re} & \textbf{Acc} & \textbf{F1} & \textbf{Pr} & \textbf{Re}\\ 
    \hline\hline

   \multirow{5}{*}{\bf 2CKD}&  \textbf{LR} & 0.58 & 0.58  & 0.54 & 0.65 & 0.61 & 0.61 & 0.57 & 0.71   \\ 
    %\hline
    & \textbf{RF} & 0.61 & 0.61 & 0.57 & 0.59 & 0.72 & 0.72 & 0.67 & 0.82 \\ 
    %\hline
    & \textbf{SVC} & 0.61 & 0.61 & 0.57 & 0.63  & 0.64 & 0.64 & 0.61 & 0.65 \\ 
    & \textbf{XGB} & 0.59 & 0.58 & 0.57 & 0.44 & 0.67 & 0.67 & 0.65 & 0.65 \\ 
    %\hline
    & \textbf{MLP}    & 0.56 & 0.56 & 0.52 & 0.60 & 0.58 & 0.58 & 0.65 & 0.65 \\ \hline
    \multirow{5}{*}{\bf HCKD} & \textbf{LR} & 0.80 & 0.80 & 0.77 & 0.81  & 0.94  & 0.94 & 0.94  & 0.94\\ 
    %\hline
    & \textbf{RF} & 0.78 & 0.78 & 0.78 & 0.75   & {1.00} & \textbf{1.00} & 1.00 & 1.00 \\ 
    %\hline
    & \textbf{SVC} & {0.82} & \textbf{0.82} & 0.83 & 0.77  &  {1.00} & \textbf{1.00} & 1.00 & 1.00 \\ 
    %\hline
    & \textbf{XGB} & 0.80 & 0.80  & 0.79 & 0.77 & {1.00} & \textbf{1.00} & 1.00 & 1.00\\ 
    %\hline
    & \textbf{MLP}    & 0.81 & 0.80 & 0.80 & 0.77 & {1.00} & \textbf{1.00} & 1.00 & 1.00\\
    \hline
\end{tabular}
% \vspace{-2mm}
\end{table*}
%
% \vspace*{5mm}
\begin{table*}[ht]
 \caption{Comparison with prior works for the two datasets.} \vspace{-2mm}
\begin{tabular}{|l l l||cc cc|} 
\hline
    \bf Dataset & \bf Methods  & \bf Features    & \multicolumn{4}{c|}{\textbf{Evaluation metrics}}     \\ 
     & & &\textbf{Acc} & \textbf{F1} & \textbf{Pr} & \textbf{Re}\\ 
    \hline\hline
    \multirow{4}{*} {\textbf{BlackDog}} & Alghowinem \etal \ \cite{alghowinem2013head} & Head movement &  - & - &  - & 0.71  \\ 
     & Joshi \etal \ \cite{joshi2013can} & Head movement &  0.72 & - &  - & -    \\ 
     & Ours (Chunk-level) & Kinemes &  \textbf{0.75} & {0.76} & { 0.78 }& \textbf{0.81}    \\
    & Ours (Video-level) & Kinemes &  \textbf{0.80} & {0.80} & { 0.83} &{0.70}  \\  \hline
    \multirow{4}{*}{\textbf{AVEC2013}}  & Senoussaoui \etal \ (AVEC2014)~\cite{senoussaoui2014model} & Video features &  0.82 & - &  - & - \\ 
    & Al-gawwam~\etal \ (AVEC2014 - Northwind) \cite{al2018depression} & Eye Blink &  0.85 & - &  - & -    \\ 
    & Al-gawwam~\etal \ (AVEC2014 - Freeform) \cite{al2018depression} & Eye Blink &  0.92 & - &  - & -    \\ 
     & Ours (AVEC2013 at chunk-level) & Kinemes &  0.82 & 0.82 &  0.83 & 0.87    \\
    & Ours (AVEC2013 at Video-level) & Kinemes &  \textbf{1.00} & {1.00} & {1.00} & {1.00}\\  \hline
\end{tabular} 
\vspace{-2mm}
\label{tab:Comp}
\end{table*}

\begin{table*}[ht]
% \hspace*{5mm}
\caption{\label{tab:AVEC_res_4class} Video-level 4-class categorization results on the AVEC dataset obtained with the HCKD approach. Accuracy (Acc), F1, Precision (Pre) and Recall (Re) are tabulated.} 
\vspace{-2mm}
\begin{tabular}{|l l||cc cc|} 
    \hline
    \bf Condition & \bf Classifier     & \multicolumn{4}{c|}{\textbf{Video-level}} \\ 
               & & \textbf{Acc} & \textbf{F1} & \textbf{Pr} & \textbf{Re} \\ 
    \hline\hline

    \multirow{5}{*}{\bf HCKD} & \textbf{LR}  & 0.71 & \textbf{0.72} & 0.73 & 0.71 \\ 
    %\hline
    & \textbf{RF}  & {0.74} & \textbf{0.72}  & 0.80 & 0.74 \\ 
    %\hline
    &  \textbf{SVC} & {0.74} & \textbf{0.72} & 0.75 & 0.74 \\ 
    %\hline
    &  \textbf{XGB}  & 0.71 & 0.69  & 0.68 & 0.71 \\ 
    %\hline
    &  \textbf{MLP}  & 0.69 & 0.66  & 0.64 & 0.69 \\
    \hline
\end{tabular}\vspace{-2mm}
\end{table*}

% \subsection{Results and Discussion}

Table ~\ref{tab:BDI_res1} shows the classification results obtained for the \emph{BlackDog} dataset with the 2CKD and HCKD approaches (Section~\ref{Sec:Meth}). Table ~\ref{tab:AVEC_res1} presents the corresponding results for the \emph{AVEC2013} dataset. These tables present classification measures obtained at the \emph{chunk-level} (best results achieved over $15 - 135s$-long chunks for the two datasets are presented), and the \emph{video-level} (label derived upon computing the mode over the chunk-level labels). Based on these results, we make the following observations:
%
\begin{itemize}
    \item It can be noted from Tables~\ref{tab:BDI_res1} and~\ref{tab:AVEC_res1} that relatively lower accuracies and F1 scores are achieved for both datasets using the 2CKD approach, implying that while class-characteristic kinemes are explanative as seen from Figs.~\ref{fig:kinemes_bdi} and~\ref{fig:kinemes_avec}, they are nevertheless not discriminative enough to effectively distinguish between the two classes. 
    \item In comparison, we note far superior performance with the HCKD method over all classifiers. As a case in point, we obtaine peak chunk-level F1-scores of 0.79 and 0.62, resp., for HCKD and 2CKD on BlackDog, while the corresponding F1-scores are 0.82 and 0.61, resp., on AVEC. This observation reveals considerable and distinguishable differences in the reconstruction errors for the patient and control classes, and convey that patient data are characterised as \emph{anomalies} when kinemes are only learned from the control cohort.
    \item Examining the HCKD precision and recall measures for both datasets, we note higher precision than recall at the chunk-level for the BlackDog dataset. Nevertheless, higher recall is achieved at the video-level with multiple classifiers. Likewise, higher chunk-level precision is noted for AVEC, even if ceiling video-level precision and recall are achieved.
    \item Comparing HCKD chunk and video-level F1-scores for both datasets, similar or higher video-level F1 values can be seen in Table~\ref{tab:BDI_res1}. F1-score differences are starker in Table \ref{tab:AVEC_res1}, where video-level scores are considerably higher than chunk-level scores. These results suggest that aggregating observations over multiple thin-slice chunks is beneficial and enables more precise predictions as shown in~\cite{madan_gahalawat_guha_subramanian_ICMI2021_Kinemes}.
    % This highlights that these selected feature sets are able to detect the two class differences over longer time-duration for the AVEC dataset.
    \item Examining measures achieved with the different classifiers, the support vector classifier achieves the best chunk-level F1-score on both datasets, with the LR classifier performing very comparably. All classifiers achieve very similar performance when video-level labels are compared.
     % \item Comparing against the depression detection model proposed in \cite{joshi2013can}, where head movement is analysed by estimating the displacement of rigid facial points over the BlackDog dataset with an accuracy of 71.7\%, we note that kineme discovery using healthy controls data approach achieves a better performance (80\%) highlighting the efficacy of considering depressed head pose values as anomalous data for the representation discovered solely using healthy subject data.
\end{itemize}
%


%
%----------------------------------------
%
\subsection{Comparison with the state-of-the-art} Our best results are compared against prior classification-based depression detection studies in Table~\ref{tab:Comp}. For the BlackDog dataset, Alghowinem \emph{et al.}~\cite{alghowinem2013head} analysed statistical functional features extracted from a 2D Active Appearance Model, whereas Joshi \emph{et al.}~\cite{joshi2013can} computed a histogram of head movements by estimating the displacement of fiducial facial points. Compared to \textit{N}-average recall of 0.71 reported in~\cite{alghowinem2013head}, and an accuracy of 0.72 noted in~\cite{joshi2013can}, our kineme-based approach achieves better chunk and video-level accuracies (0.75 and 0.80, resp.), and  superior chunk-level recall (0.81). As most previous studies on the AVEC2013 dataset focus on continuous prediction, we compare our model's performance with the AVEC2014~\cite{valstar2014avec} results examining visual features. 

AVEC2014 used the same subjects as AVEC2013, but with additional, specific task data (\emph{Northwind}, \emph{Freeform}) extracted from the AViD videos. For video analysis, Senoussaoui \etal~\cite{senoussaoui2014model} extracted LGBP-TOP features from frame blocks to obtain an accuracy of 0.82 using an SVM classifier. On the other hand, Al-gawwam \etal~\cite{al2018depression} extracted eye-blink features from video data using a facial landmark tracker to achieve an accuracy of 0.92 for the \emph{Northwind} task and 0.88 for the \emph{Freeform} task. Comparatively, our work achieves an accuracy of 0.82 at the chunk-level and 1.00 at the video-level. The next section will detail the performance of a more fine-grained 4-class categorisation on the AVEC2013 dataset.

%
%----------------------------------------
%
\subsection{\textbf{AVEC2013 Multi-class Classification}} 
\label{sec: avec_4class_res}
Table~\ref{tab:AVEC_res_4class} depicts video-level 4-class classification results achieved on the AVEC2013 dataset via the HCKD approach. The 4-class categorisation was performed to further validate the correctness of the HCKD approach, which produces ceiling video-level F1, Precision and Recall measures on AVEC2013 in binary classification. Results are reported on the test set, upon fine-tuning the classifier models on the development set. Reasonably good F1-scores are achieved even with 4-class classification, with a peak F1 of 0.72 obtained with the LR, RF and support vector classifiers. Cumulatively, our empirical results confirm that kinemes encoding atomic head movements are able to effectively differentiate between (a) the patient and control classes, and (b) different depression severity bands. 

% Figure environment removed

%
%----------------------------------------
%

% Figure environment removed

%
%----------------------------------------
%
\subsection{\textbf{Ablative Analysis over Thin Slices}}
\label{sec:Ablative_ts}
Tables~\ref{tab:BDI_res1} and~\ref{tab:AVEC_res1} evaluate detection performance over (\emph{thin-slice}) chunks or short behavioural episodes, and over the entire video, on the BlackDog and AVEC2013 datasets. We further compared labelling performance at the chunk and video-levels using chunks spanning $15-135s$. The corresponding results are presented in Figure~\ref{fig:Chunk_vs_Vid_kineme}. For both plots presented in the figure, the dotted curves denote video-level F1-scores, while solid curves denote chunk-level scores obtained for different classifiers.

For the BlackDog dataset (Fig.~\ref{fig:Chunk_vs_Vid_kineme} (left)), longer time-slices (of length $75-105s$) achieve better performance than shorter ($15-60s$ long) ones at both the chunk and video-levels across all classifiers; these findings are consistent with the finding that more reliable predictions can be achieved with longer observations in general~\cite{madan_gahalawat_guha_subramanian_ICMI2021_Kinemes}. However, a performance drop is noted for very long chunk-lengths of $120-135s$ duration. Decoding results on the AVEC2013 dataset, consistent with Table~\ref{fig:kinemes_avec} results, a clear gap is noted between the chunk and video-level results, with the latter demonstrating superior performance. Very similar F1-scores are observed across classifiers for various chunk lengths. No clear trends are discernible from video-level F1-scores obtained with different chunk-lengths, except that the performance in general decreases for all classifiers with very long chunks.

% analysing the chunk level performance for the AVEC2013 dataset, it can be seen that F1-scores are comparable over different chunk duration, with slight variation between different chunk-sizes.  Comparatively, better performance is achieved for video-level results of the dataset over shorter time slices, which declines with an increase in time length.
% In addition, comparing the chunk and video level performance over varying time lengths, it can be observed that video level classification achieves better F1-scores over both datasets demonstrating that combining the outcomes of smaller episodes can result in a better performance, despite potential inconsistencies in shorter time slices.  


%
%----------------------------------------
%
\subsection{\textbf{Ablative Analysis over Angular Dimensions}}\label{sec:Ablative_ad}
To investigate the impact of the head pose angular dimensions on chunk-level binary depression detection performance, we perform detection utilising ($8 \times 1$) statistical features over each of the (pitch, yaw, and roll) angular dimensions, and concatenate features ($8 \times 2$) for the dimensional pairs to evaluate which angular dimension(s) are more informative.

Figure~\ref{fig:descriptive_comp} presents F1-scores obtained with the different classifiers for uni-dimensional and pairwise-dimensional features. On the BlackDog dataset, a combination of the pitch and yaw-based descriptors produce the best performance across all models, while roll-specific descriptors perform worst. For the AVEC2013 dataset, pitch-based descriptors achieve excellent performance across models. The F1-scores achieved with these features are very comparable to the pitch + yaw and pitch + roll combinations. Here again, roll-specific features achieve the worst performance. Cumulatively, these results convey that pitch is the most informative head pose dimension, with roll being the least informative. With respect to combinations, the pitch + yaw combination in general produces the best results. These results again confirm that responsiveness in social interactions, as captured by pitch (capturing actions such as head nodding) and yaw (capturing head shaking), provides a critical cue for detecting depression, consistent with prior studies ~\cite{hale1997non, alghowinem2013head}.




