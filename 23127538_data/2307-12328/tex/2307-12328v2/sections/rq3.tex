\section{RQ3: How robust are on-device models on iOS against adversarial attacks?}

\subsection{Motivation}
RQ1 demonstrates that on-device models are widely used and serve fundamental roles in iOS apps.
We discover in RQ2 that while most apps will not utilise the exact same on-device model for iOS and Android, the model that serves as the app's basic functionality will be shared between the two platforms. 
Most of the shared models are exposed directly in the source files, and only a very few developers choose to provide additional protection to the models.
Shared models include the white-box model and the gray-box model.
% Considering the widespread use of the on-device model on the iOS platform, it is necessary to evaluate the security of the on-device model on the iOS platform.

White-box on-device models on Android are proved to be vulnerable to adversarial attacks~\cite{huang2021robustness, huang2022smart, karim2020adversarial}.
Due to the closed-source ecosystem and the inability of current methods to attack the gray-box model~\cite{vivek2018gray, ebrahimi2017hotflip}, attacking iOS apps is considered more challenging than attacking Android apps~\cite{garg2021comparative, dehling2015exploring, fredrikson2015model}, resulting in the lack of methods that focus on exploiting iOS-specific on-device models.
However, some studies have demonstrated that there are concurrent cross-platform issues in iOS and Android apps~\cite{ahmad2013comparison, gronli2014mobile, aljedaani2019comparison, garg2021comparative}.
Regardless of the fact that investigating iOS security issues is regarded as being more challenging, given the widespread use of on-device models on iOS, we study how robust these models are against adversarial attacks.

In this RQ, we first present our methodology of attacking on-device models in Section~\ref{sec:attackPipe}, including white-box and iOS-specific gray-box models.
Then, we carry out experiments to evaluate and compare the effectiveness of our approach in Section~\ref{sec:atkEva}.
% We also perform experiments to evaluate the performance of our proposed method in attacking iOS-specific on-device models.
Finally, we successfully attacked the relevant functionality in real-world iOS apps by taking advantage of the flaws of the on-device models in Section~\ref{sec:realworld}.

\subsection{Methodology of Model Attacking}
\label{sec:attackPipe}

% Figure environment removed

According to the empirical study in Section~\ref{sec:frameCompare}, there are two categories of on-device models inside current iOS apps:
(1) White-box Model: both the trained network weights and model structure can be extracted, for example, TF Lite and TensorFlow models; (2) Gray-box Model: only the model structure can be obtained (Core ML models).
Figure~\ref{fig:rq3} shows the methodology of our model attacking.
Regarding white-box models, we propose a more general attack approach for models of TF Lite and TensorFlow in this study.
Current approaches perform poorly when attacking gray-box on-device models of Core ML~\cite{huang2021robustness, huang2022smart, karim2020adversarial, rauber2020fast}.
Regarding gray-box models, we use the Core ML models' white-box counterpart as a bridge to propose an more efficient pipeline for attacking Core ML models.
Our approach is also applicable to white-box and gray-box models that do not fit into these three frameworks.


\begin{comment}
Different from current attack approaches~\cite{huang2022smart, karim2020adversarial, huang2021robustness, kurakin2018adversarial} for white-box on-device models of TF Lite and TensorFlow, we propose a new attack approach that can attack any models of these two frameworks, not just fine-tuning models.
% Models of TF Lite and TensorFlow have been shown to be vulnerable to adversarial attacks.
Unlike models of TF Lite and TensorFlow frameworks, the models of the iOS platform-specific Core ML framework are gray-box models, and third parties can only extract the structure of the model without trained weights.
% Based on our discovery in Section~\ref{sec:frameCompare} that apps reuse some on-device models in both Android and iOS, we propose our attack pipeline for Core ML models. 
First, we locate the white-box counterpart of the Core ML model in the corresponding App.
Second, we produce adversarial examples by attacking the white-box counterpart, and then use these adversarial examples to directly attack the iOS app's gray-box model.
\end{comment}

% Existing on-device models inside iOS can be divided into two categories:
% (1) White Box Model: both the trained network weights and model structure can be extracted (TF Lite and TensorFlow models); (2) Grey Box Model: only the model structure can be obtained (Core ML models).


% \chen{I do not see the relation between white box and grey box, and why do you mention that. Please give an overview of the whole attack pipeline in the first paragraph.}

\subsubsection{How to Attack White-box Models?}
\label{sec:attackWhite}
The attacking approaches proposed by Huang et al.~\cite{huang2021robustness, huang2022smart} require identifying the on-device model's pre-trained model for producing adversarial examples, and then using these adversarial examples to attack the on-device model.
Such approaches can only be used for fine-tuning models and requires the identification of the corresponding pre-trained model.
The fundamental reason why TF-based models cannot be directly attacked is that it is hard to continue training the models in these formats and unable to update the parameters in the model by backpropagation.
Therefore, Huang et al.~\cite{huang2021robustness, huang2022smart} choose to first identify the pre-trained model from TensorFlowHub~\cite{tfhub} which could perform backpropagation to generate adversarial examples.

Our approach does not rely on pre-trained models to generate adversarial examples and also supports direct attacks on models without fine-tuning.
Step 2 (\emph{Convert TF-based models to trainable model}) in Figure~\ref{fig:rq3} shows the pipeline of this approach.
In our approach, given an on-device model, we first retrieve its parameters and structure information and then use them to reproduce the model in a trainable format, such as the \emph{.pt} from PyTorch.
Expressly, we feed the extracted parameters to our reconstructed model structure layer by layer.
After converting to a trainable model, as shown in step 3 (\emph{Attack trainable model}) in Figure~\ref{fig:rq3}, we employ adversarial attacks on the reproduced trainable model to generate adversarial examples, which will be used to attack the original on-device model in the corresponding app.
% We also employ this attack approach against white-box on-device models in Android apps \yujin{This sentence is not clear to me}.
Figure~\ref{fig:conversionExp} shows a concrete example of converting an on-device model into a trainable model format, namely\emph{pt} of PyTorch.
We first reproduce all the neural network structures of this model. Then we load the trained weight dictionary (\emph{parameter\_dict}) of the original on-device model, and feed the weights of the original model for our reproduced network structure according to the shape and name of each network layer. Finally, we save the replicated model in trainable format (\emph{Model.pt}).
We generate adversarial examples based on the training of our replicated trainable models (\emph{Model.pt}). 
we then use these adversarial examples to attack the original on-device model (\emph{Model.tflite}).

% Figure environment removed


\subsubsection{How to Attack Gray-box Models?}
On-device models of the Core ML framework do not share trained parameters with third parties and may compress or encrypt model parameters~\cite{modelEncry}, resulting in the inability to retrieve the parameters of the trained model.
This type of gray-box model is tough to be successfully attacked by current approaches without knowing trained parameters.
% Core ML framework encourages developers to convert other frameworks' trained on-device models to Core ML framework format for use in iOS apps~\cite{CoreMLConvert}.
Steps 1, 2, 3 and 4 in Figure~\ref{fig:rq3} demonstrate our attack pipeline for gray-box models.
In Section~\ref{sec:modelChange}, we find that some iOS and Android apps share on-device models and that some on-device models of Core ML framework are converted from models in Android apps~\cite{CoreMLConvert}.
Therefore, following steps in Section~\ref{sec:ps2}, we first locate the matching on-device model before conversion in the corresponding Android app (Step 1 in Figure~\ref{fig:rq3}).
We retrieve the model's parameters and reconstruct it in a trainable format (Step 2 in Figure~\ref{fig:rq3}). 
Then, we attack the reconstructed model of its counterpart to generate adversarial examples for targeting the original on-device model of Core ML (Step 3 and 4 in Figure~\ref{fig:rq3}).


\subsection{Evaluation of Attacking}
\label{sec:atkEva}
The effectiveness of our proposed approach to attack the white-box model is evaluated first, followed by the effectiveness of our proposed method to attack the gray-box model.



\subsubsection{Dataset}
\input{tables/tab_dataset}




% \input{tables/tab_attackResults}
We evaluate the performance of our proposed attack pipeline in real-world industrial iOS apps from the Apple App Store.
We select 10 representative on-device models of the Core ML framework for testing our approach.
These 10 models all have clear usage scenarios in iOS apps and serve core functions.
Table~\ref{tab:dataset} shows the detail of selected 10 on-device models.
The column \emph{App Name} represents the apps to which the on-device models belong. 
The column \emph{Model in iOS} represents the gray-box model in iOS apps.
The column \emph{Model in Android} represents gray-box models' located white-box counterparts in the Android app.
The column \emph{Model Function} represents the identified model function in apps.
In our present analysis of commonly utilized apps, we have only identified the reuse of image-based on-device models between Android and iOS platforms. 
Therefore, the selected models are all image-based.
However, the advent of large language models (LLMs) suggests the potential for expansion of this phenomenon to other domain models such as natural language processing (NLP), speech recognition, and more. 
As of now, these models are not yet prevalent in cross-platform reuse. 
Nevertheless, our methodology exhibits a high degree of flexibility and adaptability, making it well-suited to accommodate such future developments. 
We will direct our research efforts toward exploring the security implications of these emergent domains and their corresponding large models in the future.

\subsubsection{Baseline}
To demonstrate the effectiveness of our newly proposed method to attack the white-box model, we use the ModelAttacker proposed by Huang et al.~\cite{huang2021robustness, huang2022smart} as the baseline to carry out a control experiment.
ModelAttacker hacks DL models using adversarial attacks by first identifying highly similar pre-trained models from TensorFlow Hub~\cite{tensorflowHub}.
Then, it attacks the identified pre-train model to produce adversarial examples for the original model.


\subsubsection{Experimental Setup}
% Following the pipeline in Section~\ref{sec:ps2} to match pair on-device models of the Core ML framework, 
We first identify Android white-box counterparts of Core ML models (as shown in column \emph{Model in Android}).
Second, following steps in Section~\ref{sec:attackWhite}, we convert these on-device models into trainable models (\emph{.pt}).
We find 10 random images for each model in Table~\ref{tab:dataset} by referring to their identified functionalities as the original inputs~\cite{huang2021robustness, huang2022smart}.
To ensure the validity of the input images selected for our experiments, we employ a two-step collection process. 
In the first step, we identify target images matching the function of the model and the app's specific usage scenario. 
For instance, for model 5 (\emph{NABirdsImageClassifier.mlmodelc}) in Table~\ref{tab:dataset}, which is used for bird identification, we selected various bird images as input for the model. 
In order to ensure the diversity of the input test set, we will deliberately select images that encompass distinct object categories, backgrounds, and lighting conditions. This approach will ensure that our test set is a true reflection of the variability encountered in real-world images, and will help to establish the efficacy of our method in practical settings.
In the second step, we manually input the selected experimental images directly into the corresponding application scenes of the model in the app to verify the model's correct recognition of these images. Following this process, we obtained a set of valid input images. 

In this study, we follow the approach of Huang et al.~\cite{huang2022smart, huang2021robustness} and randomly select 20 images for each on-device model. To ensure the diversity of the selected images, we partition the 20 images into two subsets such that each subset consists of dissimilar images.
To quantitatively evaluate the diversity of the new dataset, we use Maximum Mean Discrepancy (MMD)~\cite{borgwardt2006integrating} as a metric. MMD is a measure of the discrepancy between two probability distributions, frequently used in machine learning to compare the similarity between datasets. MMD compares the means of the two datasets in a reproducing kernel Hilbert space (RKHS) and quantifies their differences. Higher MMD values indicate greater dissimilarity between two datasets or samples.
Based on prior research~\cite{dziugaite2015training, yan2017mind}, we require that the MMD values of the two datasets exceed 0.6 to ensure their diversity. If the MMD values fall below this threshold, we re-collect images until the requirements are satisfied. 


Finally, we use selected images as input to attack the converted trainable models to generate adversarial examples and use generated adversarial examples to evaluate if they will be misclassified by original on-device models on Android and iOS.

In the control experiment, we first find the most similar pre-train models from TensorFlowHub for each model in the column \emph{Model in Android} by following the experiment setup of the baseline.
Then, we use selected 10 images as input to attack the pre-train models to generate adversarial examples.
Finally, we use the generated adversarial examples to attack the original iOS on-device models.
The baseline requires locating pre-trained models with greater than 80\% structural similarity~\cite{huang2021robustness}.
However, we cannot find such models on TensorFlowHub for model 9 (\emph{gender\_nn.tflite}) and model 10 (\emph{optimized\_nudity\_graph.tflite}).
So when comparing the attack success rate of the baseline, we only consider the success rate of  models 1 to 8.

\subsubsection{Evaluation Metrics}
Successful adversarial examples must meet two requirements:  (1) make the model misclassify (2) changes made in the original image of the input cannot be noticed by humans.
% After successfully causing models to misclassify adversarial examples and
Following the same evaluation steps with related works~\cite{huang2021robustness, huang2022smart, karim2020adversarial}, we invite three PhD students with experience in adversarial attacks to manually evaluate whether the modifications made to adversarial example images are too subtle to be noticed.
The attack is deemed successful if the three volunteers think the modifications to the image cannot be easily recognised.
Otherwise, the attack is considered failed.
we count the number of examples that successfully misclassify the model among these 10 input adversarial examples and count the success rate of different types of adversarial attacks.

\subsubsection{Adversarial Attack Algorithms}

The whole process of adversarial attack can be summarized as
\begin{equation}
    x^{'}_{adv} = x + \epsilon * attack_i(\nabla J (\theta,\ x,\ y_{true}))
\end{equation}
where $y_{true}$ represents the original label or class of input image $x$, $\epsilon$ represents a multiplier to ensure the perturbations between input images and adversarial images are small and its value is empirically determined in the experiments to produce unnoticeable image perturbations, $attack_i$ represents the $i_{th}$ type of adversarial attack, $\theta$ represents parameters, and $J$ is the loss.

We select 10 representative attacks: Pointwise (PW) Attack~\cite{yang2019adversarial}, Boundary (BD) Attack~\cite{croce2020minimally}, DDN Attack~\cite{rony2019decoupling}, DeepFool (DF) Attack~\cite{moosavi2016deepfool}, FGSM Attack~\cite{goodfellow2014explaining}, BIM~\cite{kurakin2018adversarial}, PGD~\cite{he2019towards}, C\&W Attack~\cite{carlini2017towards}, Newton Fool (NF) Attack~\cite{jang2017objective} and Clipping-Aware Noise (CAN) Attack~\cite{rauber2020fast} as all these attacks have been proved effective in many tasks and widely-used to evaluate the robustness of on-device models~\cite{huang2021robustness, huang2022smart, karim2020adversarial}.


\subsubsection{Results}
\input{tables/tab_attackResults3}

Table~\ref{tab:results3} shows the adversarial attack results in the dataset.
The column \emph{ATK} shows the type of adversarial attacks.
The column \emph{Ep} denotes the empirical value of the $\epsilon$ that provides the best performance of the current attack in the experiment.
The columns from \emph{M1} to \emph{M10} represent models with ids 1 to 10 in Table~\ref{tab:dataset}.
The subcolumns \emph{M} represent the attack results of the baseline ModelAttacker.
The subcolumns \emph{A} and \emph{I} represent the attack results of our approach on Android and iOS models, respectively.

As shown in the column \emph{Ave (ATKs)}, the average success rate of ModelAttacker's attack is 0.34, which is much lower than that of our approach 0.72 on white-box models and 0.70 on gray-box Core ML models.
The baseline for producing adversarial examples by attacking the corresponding pre-training model has limits and is only applicable for fine-tuning on-device models.
Our proposed attack method is demonstrably more effective and is applicable to non-pre-trained models like \emph{M9} and \emph{M10}.

As shown in the row \emph{Ave}, all of these 10 Core ML models are effectively attacked by our approach, with a success rate of at least 64\%.
The average success rate of attacks against Android equivalents is 0.72 (shown in column \emph{Ave(ATKs)}, row \emph{Ave}), which is slightly higher than the success rate of attacks against the original Core ML models, which is 0.70.
The extremely close success rate implies that gray-box Core ML models are vulnerable to adversarial examples generated by targeting their white-box counterparts, demonstrating the effectiveness of our cross-platform attack approach.

We investigate why there is a slight distinction in the success rates of attacking Android and iOS models.
There are two reasons: (1) Different frameworks may optimise the model differently. Consequently, the models may be updated slightly during the framework conversion process, resulting in subtle variations in model effects.
(2) Android app and iOS app version updates are not synchronised, so the model versions inside the app are not consistent, such as models \emph{geo\_v18.mlmodelc} and \emph{geo\_v17.tflite} in app \emph{merlin bird id by cornell lab}.
Different versions of the model may have a slightly different effect.

The model \emph{M9} (\emph{gender\_nn.mlmodelc}) is the most vulnerable with the highest average success rate of is 81\%,  since \emph{gender\_nn.mlmodelc} has the simplest structure, consisting of several residual blocks.
The Clipping-Aware Noise Attack has the most significant attack effect (85\% average success rate), whereas the success rates of the other 9 attacks are much lower (68\% - 72\%).
We find that the Clipping-Aware Noise Attack generates adversarial images with a narrower range of perturbations that are more likely to survive manual recognition and so have a higher success rate.
% The attack results demonstrate that although the current Core ML grey-box models are tough to hack, its white box counterparts can be leveraged as a bridge to attack it.

\subsubsection{Results Discussion}

Our experiments conclusively demonstrate that the black-box models present in iOS apps, which can match white-box counterparts, are inherently insecure. 
As outlined in Section~\ref{sec:ratio}, we discover that 332 (17.63\%) out of the total 1883 models are shared across both iOS and Android platforms and that these models are frequently reused in critical functionalities of the app. 
This finding highlights a significant ratio that cannot be ignored. 
Given the insecurities present in these models, both academia and industry must pay close attention to the potential unforeseen consequences and take necessary steps to prevent them.

In recent studies, researchers have analyzed the robustness of on-device models in Android apps against adversarial attacks. Our findings indicate that our proposed approach of converting the on-device model into a trainable format and generating adversarial examples through its own training is more effective than common adversarial attacks or attacking the corresponding pre-trained model before attacking the on-device model. Our approach can increase the success rate of attacks from about 45\% to 76\% when attacking white-box models.
In the work of Deng et al.~\cite{deng2022understanding} and Huang et al.~\cite{huang2021robustness}, C\&W and inversion attacks are found to be the most effective countermeasures against white-box models. However, we find that the Clipping-Aware Noise (CAN) attack, which has been recently proposed, has the highest success rate of the attack so far. Compared to C\&W and inversion attacks, CAN attack is relatively new and thus should receive more attention in terms of prevention.

In our black-box model attack experiments, we observed that compared to the NES algorithm~\cite{ilyas2018black} and ModelAttacker, our approach demonstrates a higher success rate in attacking partial black-box models with potential security threats.
Despite the comparatively closed and secure nature of the iOS system, we have shown that on-device models in Android apps are vulnerable to similar threats as those present in iOS apps. To mitigate this issue, app developers should apply the same model protection measures used in the Android platform, such as model obfuscation and encryption, to models in the iOS platform.
Furthermore, DL framework providers could enhance the security of models and attract more users by providing a cross-platform security mechanism for models that use their framework.

In above related studies and our work, it is worth noting that the on-device models examined in our experiments primarily focus on image-related domains and hardly cover other domains, such as natural language processing (NLP) and speech recognition. This is due to the fact that on-device models related to computer vision are currently the most prevalent type of DL models utilized by real-world applications, as discussed in Section~\ref{sec:modelFunction}. 
The models identified in our experiments that are reused in both platforms are primarily image-related. 
From the perspective of attack methods, investigating a broader range of model types would provide a more comprehensive understanding of how different types of models perform in the face of adversarial attacks. Thus, in future work, we plan to investigate the security vulnerabilities of all types of models against adversarial attacks.


% \textcolor{red}{xxx} show the physical attack results of physical attacking the app \emph{seek by inaturalist}.
% The input modified image can also cause the app to misclassify the species of the bird. 

\subsubsection{Examples of Attacking Real-world iOS Apps}
\label{sec:realworld}
After locating the usage scenarios within iOS apps, we input the generated adversarial examples to the iOS apps to attack real-world iOS apps, as illustrated in the step 5 of Figure~\ref{fig:rq3}.
Figure~\ref{fig:phAt1} shows the attack results of the app \emph{gradient: celebrity look alike}.
The adversarial example image directly cause the app to misclassify the gender of the image from male to female.
Figure~\ref{fig:phAt2} shows the attack results of the app \emph{smart bird id}.
The adversarial example image cause the app to misclassify the bird type of the image from \emph{Australia Magpie} to \emph{Satin Bowerbird}.

\input{tables/fig_realworldAttack}

% % Figure environment removed

% % Figure environment removed



\begin{summary}
% Most on-device models, including models of Core ML, TF Lite, and TensorFlow in current iOS apps, are vulnerable to adversarial attacks.
Due to the reuse of models between platforms and apps, gray-box models of iOS-specific Core ML are also vulnerable to adversarial attacks via our approach.
The discovered vulnerabilities of on-device models inside iOS apps could disable some functionalities of real-world iOS apps.
Our findings reveal that the supposedly more secure iOS platform also has potential model security concerns, driving developers and DL frameworks to develop cross-platform model security defences against the possibility of cross-platform attacks.
\end{summary}


