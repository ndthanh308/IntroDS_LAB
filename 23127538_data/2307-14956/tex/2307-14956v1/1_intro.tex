\section{Introduction} \label{sec:intro}
Reproducibility is the cornerstone of science as the scientific process relies heavily on the repeatability of experiments. Computer science is no exception to this rule. Experimentation in computer science is usually relatively cheap and quick that enables experiments to be executed relatively quickly and cheaply. As a negative side effect, quick and cheap experimentation can provide fertile ground for haphazard experiment design and careless experimentation. When a research field -- like machine learning -- is evolving rapidly and new algorithms are proposed every day, reproducibility tends to receive lower priority. This is then combatted by standardization efforts, e.g.~benchmark datasets, standardized evaluation frameworks, etc. Quickly evolving fields -- such as computer vision or natural language processing -- can still have their own benchmarks~\cite{deng2009imagenet,lin2014microsoft,kuznetsova2020open, rajpurkar2016squad, wang2019superglue, weston2015towards}. Compared to other areas of machine learning, reproducibility in recommender systems research is severely underdeveloped. This is probably connected to evaluation being one of the biggest challenges of the field. 

Recommendation quality can be assessed either online or offline. Online A/B tests provide better approximation on how well a recommender achieves its goals, but they are inherently not reproducible. The proprietary nature and commercial value of live recommender systems makes them accessible to only a very limited set of researchers, preventing the independent repetition of experiments. Due to the interactive nature of recommender systems, offline experimentation is just an imperfect proxy of the online setup. While reproducible in theory, multiple factors influence how repeatable an offline experiment is. In recent years, the research community has started discussing reproducibility and paying more attention to evaluation, including, but not limited to: 
\begin{itemize}
    \item \textbf{Public datasets.} Researchers are expected to showcase the performance of their algorithms on task appropriate public datasets. This is made possible by the increasing number of publicly released datasets, e.g.~\cite{requena2020shopper,ni2019justifying,kang2018self}
    \item \textbf{Open sourcing code.} More and more researchers release code along with their papers, which makes repeating the experiments significantly easier.~\cite{tang2018personalized,kang2018self,sun2019bert4rec}
    \item \textbf{Hyperparameter tuning.} Hyperparameters have significant effect on recommendation quality. Providing optimal hyperparameters for datasets and/or giving a general guide on how to get good parameterization is important for having strong baselines.~\cite{rendle2022revisiting,ferrari2021troubling,ferrari2020methodological}
    \item \textbf{Evaluation setups.} Data preprocessing, offline metrics, and executing appropriate experiments can make or break the evaluation part of a research project.~\cite{ferrari2021troubling,ferrari2020methodological}
    \item \textbf{Standardization, benchmarking.} Standardized benchmarks and frameworks supporting consistent evaluation over multiple algorithms allow for an informative comparison of models.~\cite{10.1145/3523227.3551472,argyriou2020microsoft,zhao2021recbole,said2014rival,sun2020we,zhu2022bars}
\end{itemize}

\subsection{Reimplementing recommender algorithms}
In this paper we discuss the reimplementation of recommender algorithms with respect to reproducibility. As far as we know, this is the first paper focusing on this topic. Reimplementing algorithms has been commonplace for recommender systems for decades. Before open sourcing code became widespread, researchers had to resort to reimplementing their baselines, based on the description alone. Even if an algorithm's code is publicly available, reimplementation might be reasonable due to \emph{convenience} and/or \emph{standardization}. Reimplementing an algorithm from scratch is also a good way of deeply understanding its inner workings.

\textbf{Convenience:} The official implementation might have an API unsuitable for the planned experiments. It might be written in a programming language the researcher is not familiar with. The researcher might need all baselines to be in a single (proprietary) framework for consistent evaluation. The official version might be implemented inefficiently or contain bugs. Etc.

\textbf{Standardization} in this case means reimplementing algorithms in benchmarking frameworks with the goal of providing fair, unified, and consistent evaluation over a large set of algorithms; instead of researchers using slightly different evaluation codes under the same name.

While unofficial reimplementations have existed for decades, open sourcing them is fairly new. When done correctly, this can be beneficial to the research community. Having multiple options can significantly increase accessibility. E.g.~researchers with less experience in programming might find it hard to utilize a C library in their experiments, but they might be able to use a Keras module. Releasing an unofficial implementation as part of benchmarking frameworks can contribute towards the standardization of evaluation. The authors of the original algorithm can also benefit from the wider visibility of their work and the contributors of the reimplementation can gain the goodwill of the community.

Unfortunately, there are also risks associated with unofficial releases. Reimplementing an algorithm from scratch can be a non-trivial task, especially if the algorithm is complex or has a lackluster description that lacks important nuances. The task is somewhat easier if a public official implementation exists since it can serve as a reference. Incorrect reimplementations -- that are not in line with the original -- can be harmful to research projects as they make baseline results invalid. Thus flawed, publicly available unofficial implementations can affect several research projects negatively.

Therefore, the quality and correctness of reimplementations is imperative, especially if they are publicly available. In this paper we investigate this topic. Since a thorough study of every unofficial implementation of every algorithm would be infeasible, we selected a single algorithm for this purpose. We chose GRU4Rec~\cite{hidasi2015session,hidasi2018recurrent}, a seminal work in session-based~/~sequential recommendations. GRU4Rec is an ideal candidate for our study for the reasons below:
\begin{enumerate}
    \item It is an older algorithm (released 7+ years ago), but has frequently been used as a baseline for session-based and sequential recommendations, due to it being a seminal work in this area.
    \item An official implementation\footnote{\url{https://github.com/hidasib/GRU4Rec}} was published along the papers, and it has been supported since then.
    \item The official version is implemented in Theano~\cite{al2016theano}, a now discontinued deep learning framework. While Theano still works fine, there is a clear incentive for reimplementation in more modern frameworks, e.g.~PyTorch.
    \item GRU4Rec is not a complex algorithm, but still has distinctive features that are crucial for its good performance.
    \item Papers report inconsistent results for GRU4Rec that might be a result of using incorrect implementations.~\cite{kang2018self,sun2019bert4rec,wu2019session,wang2020global,xu2019graph,luo2020collaborative}
\end{enumerate}

We aim to answer the following research questions:
\begin{itemize}
    \item (RQ1) Do third-party reimplementations implement the same algorithm as the official version?
    \item (RQ2) Are the third-party reimplementations feature complete?
    \item (RQ3) Are the third-party reimplementations correct? Are there any notable bugs or omissions?
    \item (RQ4) How do the differences between the reimplementations and the original affect offline measurements?
    \item (RQ5) How do the differences between the reimplementations and the original affect training times and scalability?
\end{itemize}

The rest of the paper is structured as follows. After a brief overview of GRU4Rec and its main features, Section~\ref{sec:compare} presents an in-depth qualitative comparison between six third-party reimplementations and the original (RQ1, RQ2, RQ3). Section~\ref{sec:quant} presents the results of our offline experiments (RQ4, RQ5). Our findings and their implications are summarized in Section~\ref{sec:discuss}. Section~\ref{sec:related} showcases related work, and Section~\ref{sec:conclusion} concludes this paper.