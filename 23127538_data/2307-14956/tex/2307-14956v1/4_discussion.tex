\section{Discussion}\label{sec:discuss}
The comparison of the six third-party reimplementations with the original resulted in this final tally:
\begin{itemize}
    \item Microsoft Recommender's version is GRU4Rec in name only. It implements a different and flawed algorithm.
    \item The rest of the reimplementations lack at least one GRU4Rec feature that has significant impact on accuracy.
    \item Except for Torch-GRU4Rec, these unofficial implementations suffer from bugs, hard-coded parameters, and divergence from the original; which further deteriorates recommendation accuracy.
    \item KerasGRU4Rec and Recpack scale poorly due to the lack of negative item sampling during training, but all five suffer from some kind of scalability problem.
\end{itemize}

Even though we expected some of the third-party implementations being incorrect or lacking features, this outcome still comes as an unpleasant surprise. Only Torch-GRU4Rec can be deemed as a competent attempt: while it still lacks an important feature, it is mostly a correct and faithful implementation. GRU4Rec is not a complex algorithm, and its main features are highlighted in its papers. Its publicly available official code can be used as both a crutch during reimplementation and as a standard to validate against. Therefore, it is puzzling why the quality of these reimplementations is this low. Considering all of the above, we assume that the problem is not isolated or specific to GRU4Rec. Many algorithms might be affected by incorrect unofficial implementations. To somewhat alleviate the harm, we release feature complete reimplementations of GRU4Rec for both Tensorflow\footnote{\url{https://github.com/hidasib/GRU4Rec_Tensorflow_Official}} and PyTorch\footnote{\url{https://github.com/hidasib/GRU4Rec_PyTorch_Official}} that are validated against the official version.

As discussed in Section~\ref{sec:intro}, incorrect unofficial public implementations might have a devastating effect on multiple research projects, depending on how widespread they are. Unfortunately, most papers don't mention which implementation of the baselines were used during experimentation. Our findings suggest that this is important information, on par with optimal hyperparameters or the description of data preprocessing.

Our study raises some interesting questions for the research community to answer. If a research project showcases suboptimal baseline performance due to using an incorrect third-party implementation, who is responsible? Researchers are responsible for what they publish, but to what extent do they need to double-check whether the tools they use are correct? Does the implementation of every single baseline need to be validated rigorously against its original paper? Are the contributors of reimplementations responsible for the misleading results, even if code is provided as-is and without any guarantees? What should happen if a reimplementation is found incorrect? Is it feasible to correct experiments in a large number, sometimes several years old papers? Should the incorrect third-party implementations get fixed or withdrawn? What if these have been long abandoned by their original contributors?

Our advice to researchers is to either use the official implementation or validate the reimplementation they use against the official version, and clearly state in their paper which implementation they used. Official open source releases can also help reimplementation efforts, which is another reason for publishing code. Most importantly, contributors of third-party implementations should validate their work before releasing it to the public.