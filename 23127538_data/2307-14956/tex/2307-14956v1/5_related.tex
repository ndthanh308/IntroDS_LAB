\section{Related work}\label{sec:related}
We were unable to find any studies closely relating to our work. Looking at the wider area~\cite{ferrari2019we,ferrari2020methodological} examined multiple top-n recommender algorithms presented at prestigious conferences, and found that only 7 out of 18 works could be reproduced with reasonable effort. Furthermore, 6 of those 7 could be outperformed by simple baselines. The major problems they highlight relate to using weak baselines, arbitrary evaluation setups and suboptimal hyperparameters. The latter was also examined in detail in~\cite{rendle2022revisiting} through the example of the now 15 years old iALS algorithm. iALS can be a much stronger baseline -- capable of beating more modern algorithms -- with the appropriate parameterization.

\cite{ferrari2021troubling} reassures the findings of~\cite{ferrari2019we,ferrari2020methodological} for a wider variety of algorithms and baselines. They argue that while the majority of recent papers utilize evaluation setups, datasets, metrics, etc.~of previous work, these are reused without questioning their validity, thus popular but flawed evaluation setups can spread in the community.

A recent work~\cite{petrov2022systematic} reinforces the notion that claimed state-of-the-art performance is often skewed or invalid through the example of BERT4Rec, as the results of the original paper can not be reproduced.

Standardization of evaluation through frameworks like \cite{10.1145/3523227.3551472,argyriou2020microsoft} can help reproducibility, if those frameworks are mostly free of the issues discussed above. However, defining the proper evaluation setups with datasets, and correctly implementing a wide variety of algorithms is not an easy task. Our findings show that algorithms reimplemented in benchmarking frameworks can suffer from serious flaws.