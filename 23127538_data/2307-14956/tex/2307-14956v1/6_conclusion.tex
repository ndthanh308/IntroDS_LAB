\section{Conclusion}\label{sec:conclusion}
When done correctly, reimplementing a popular algorithm can be beneficial for the research community due to increasing accessibility or contributing towards the standardization of evaluation. However, it may also pose immense risks to reproducibility if the unofficial implementation is incorrect or buggy. In this paper we described the results of a thorough comparison between six third-party and the official implementation of GRU4Rec, a seminal algorithm for session-based~/~sequential recommendations. One of the six unofficial reimplementations is GRU4Rec in its name only as it implements a different architecture, which is also conceptually flawed. The remaining five lack important features of the original that contribute to the good performance of the official version. Four of them also suffer from errors~/~bugs of varying severity that are detrimental to their performance. In the most extreme case, out-of-the-box performance is $99.63\%$ lower compared to the official version. The reimplementations are also significantly slower to train. Two of them scale very poorly, thus using them on larger datasets is highly impractical. We think that these findings are alarming. Potentially a significant number of research project outcomes have been falsified by these errors. But more importantly, we don't think that this problem is isolated and specific to GRU4Rec, which is not a too complicated algorithm with a publicly available official implementation. We urge the research community and especially those who reimplement algorithms -- either for their own use or as a public resource -- to validate reimplementations against the original work and the official implementation.