\section{Appendix}

\subsection{Website Implementation}\label{app:site_implementation}
Given the selected websites described in \S\ref{sec:website_selection}, we make the best attempt to reproduce the functionality of commonly used sites in a reproducible way. 
To achieve this, we utilized open-source frameworks for the development of the websites across various categories and imported data from their real-world counterparts.
For the E-commerce category, we constructed a shopping website with approximately 90$k$ products, including the prices, options, detailed product descriptions, images, and reviews, spanning over 300 product categories. This website is developed using Adobe Magento, an open-source e-commerce platform\footnote{\url{https://github.com/magento/magento2}}. Data resources were obtained from data from actual online sites, such as that included in the Webshop data dump\cite{yao2022webshop}.
As for the social forum platform, we deployed an open-source software Postmill\footnote{\url{https://postmill.xyz/}}, the open-sourced counterpart of Reddit\footnote{\url{https://www.reddit.com/}}.
We sampled from the top 50 subreddits\footnote{\url{https://redditlist.com/sfw.html}}. 
We then manually selected many subreddit for northeast US cities as well as subreddit for machine learning and deep learning-related topics. This manual selection encourages cross-website tasks such as seeking information related to the northeast US on both Reddit and the map. 
In total, we have 95 subreddits, 127390 posts, and 661781 users. 
For the collaborative software development platform, we choose GitLab\footnote{\url{https://gitlab.com/gitlab-org/gitlab}}.
We heuristically simulate the code repository characteristics by sampling at least ten repositories for every programming language: $80\%$ of them are sampled from the set of top $90$ percentile wrt stars repos using a discrete probability distribution weighted proportional to their number of stars; the remaining are sampled from the bottom ten percentile set using similar weighted distribution. This is done to ensure fair representation of repos of all kinds, from popular projects with many issues and pull requests to small personal projects. In total, we have 300 repositories and more than 1000 accounts with at least one commit to a repository.
For the content management system, we adapted Adobe Magento's admin portal, deploying the sample data provided in the official guide.
We employ OpenStreetMap\footnote{\url{https://www.openstreetmap.org/}} for map service implementation, confining our focus to the northeast US region due to data storage constraints.
We implement a calculator and a scratchpad ourselves.

Lastly, we configure the knowledge resources as individual websites, complemented with search functionality for efficient information retrieval.
Specifically, we utilize Kiwix\footnote{\url{https://www.kiwix.org/en/}} to host an offline version of English Wikipedia with a knowledge cutoff of May 2023. 
The user manuals for GitLab and Adobe Commerce Merchant documentation are scraped from the official websites.%

\subsection{Environment Delivery and Reset}\label{app:env_reset}
One goal for our evaluation environment is ease of use and reproducibility.
As a result, we deploy our websites in separate Docker images~\footnote{\url{https://www.docker.com/}}, one per website.
The Docker images are fully self-contained with all the code of the website, database, as well as any other software dependencies.
They also do not rely on external volume mounts to function, as the data of the websites are also part of the docker image.
This way, the image is easy to distribution containing all the pre-populated websites for reproducible evaluation.
End users can download our packaged Docker images and run them on their systems and re-deploy the exact websites together with the data used in our benchmarks for their local benchmarking.

Since some evaluation cases may require the agent to modify the data contained in the website, \eg creating a new user, deleting a post, etc., it is crucial to be able to easily reset the website environment to its initial state.
With Docker images, the users could stop and delete the currently running containers for that website and start the container from our original image again to fully reset the environment to the initial state.
Depending on the website, this process may take from a few seconds to one minute.
However, not all evaluation cases would require an environment reset, as many of the intents are information gathering and are read-only for the website data.
Also, combined with the inference time cost for the agent LLMs, we argue that this environment reset method, through restarting Docker containers from the original images, will have a non-negligible but small impact on evaluation time.


\subsection{Prompt for \texttt{fuzzy\_match}}\label{app:prompt_fuzzy_match}
Given the statement ``\{\{\texttt{prediction}\}\}'', would it be correct to infer ``\{\{\texttt{reference}\}\}''? Yes or No.
\subsection{The Prompts of the Baseline Web Agents}\label{app:agent_prompts}
The system message of the reasoning agent for both GPT-3.5 and GPT-4 is in \autoref{fig:reasoning_prompt}, and two examples are in \autoref{fig:reasoning_examples}. The system message of the direct agent for GPT-3.5 is in \autoref{fig:direct_prompt} and the two examples are in \autoref{fig:direct_example}.

% Figure environment removed

% Figure environment removed



% Figure environment removed

% Figure environment removed

\subsection{A Successful Trajectory}\label{app:succ_example}
A successful trajectory is shown in \autoref{fig:success_example}.
% Figure environment removed

% Figure environment removed