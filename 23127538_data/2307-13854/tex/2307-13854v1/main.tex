\documentclass{article}
\usepackage[square,numbers]{natbib}
\usepackage[preprint]{neurips_2022}
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage[dvipsnames]{xcolor}         %
\usepackage{xspace}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{pifont}
\newcommand{\cmark}{\textcolor{green}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}%
\input{commands}

\definecolor{grey}{gray}{0.8}


\newcommand{\draftcomment}[1]{#1}

\title{\ours: A Realistic Web Environment for Building Autonomous Agents}




\author{%
  Shuyan Zhou$^\spadesuit$\thanks{Lead contributors.} \quad
  Frank F. Xu$^\spadesuit$\footnotemark[1] \\
  \textbf{Hao Zhu}$^\spadesuit$\thanks{Equal contribution.} \quad 
  \textbf{Xuhui Zhou}$^\spadesuit$\footnotemark[2] \quad 
  \textbf{Robert Lo}$^\spadesuit$\footnotemark[2] \quad 
  \textbf{Abishek Sridhar}$^\spadesuit$\footnotemark[2] \quad 
  \\
  \textbf{Xianyi Cheng}$^\spadesuit$ \quad 
  \textbf{Yonatan Bisk}$^\spadesuit$ \quad 
  \textbf{Daniel Fried}$^\spadesuit$ \quad 
  \textbf{Uri Alon}$^\spadesuit$ \quad 
  \textbf{Graham Neubig}$^\spadesuit$$^\clubsuit$ \\ 
  $^\spadesuit${Carnegie Mellon University} \quad \quad $^\clubsuit${Inspired Cognition} \\
  \texttt{\{shuyanzh, fangzhex, gneubig\}@cs.cmu.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, current agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation.
In this paper, we build an environment for agent command and control that is \emph{highly realistic} and \emph{reproducible}.
Specifically, we focus on agents that perform tasks on the web, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.
Our environment is enriched with tools~(\eg a map) and external knowledge bases~(\eg user manuals) to encourage human-like task-solving.
Building upon our environment, we release a set of benchmark tasks focusing on evaluating the \emph{functional correctness} of task completions.
The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet.
We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting. 
The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59\%.
These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that \ours can be used to measure such progress.

Our code, data, environment reproduction resources, and video demonstrations are publicly available at \url{https://webarena.dev/}.
\end{abstract}


\section{Introduction}\label{sec:intro}
% Figure environment removed

Autonomous agents that could perform everyday tasks via human natural language commands could significantly augment human capabilities, given their potential to enhance efficiency and promote broader accessibility.
Nonetheless, to fully leverage the power of these autonomous agents, it is crucial to understand their behavior within an environment that is both \emph{authentic} and \emph{reproducible}.
This will measure the ability of agents on real tasks that human users care about and allow them to be evaluated in a fair and consistent manner.

Current environments tend to \emph{over-simplify} real-world situations. %
As a result, the functionality of many environments is a limited version of their real-world counterparts, leading to a lack of task diversity within the environment~\citep{shi2017world,anderson_vision-and-language_2018,gordon2018iqa,misra2016tell,shridhar_alfred:_2019,shridhar_alfworld_2020,yao2022webshop}.
In addition, these simplifications often lower the complexity of tasks as compared to their execution in the real world~\cite{puig_virtualhome:_2018,shridhar_alfred:_2019,yao2022webshop}. 
Finally, some environments are presented as a static resource~\citep{shi2017world, deng2023mind2web} where agents are confined to accessing only those states that were previously cached during data collection, thus limiting the breadth and diversity of exploration.
On the evaluation aspect, many environments focus on comparing the \emph{surface form} of the predicted action sequences with reference action sequences, disregarding the functional correctness of the executions and possible alternative solutions~\citep{puig_virtualhome:_2018,jernite_craftassist_2019, xu2021grounding,li2020mapping, deng2023mind2web}.
These limitations often result in a discrepancy between simulated environments and the real world. 
Such constraints can potentially impact the generalizability of AI agents to successfully understand, adapt, and operate within complex real-world situations.


In this work, we introduce \ours{}, a \emph{realistic} and \emph{reproducible} web environment designed to facilitate the development of autonomous agents capable of executing tasks~(\S\ref{sec:webarena}). An overview of \ours is in \autoref{fig:overview}.
Our environment comprises four fully operational, self-hosted web applications, each representing a distinctive domain prevalent on the internet: online shopping, discussion forums, collaborative development, and business content management. %
Furthermore, \ours{} incorporates several utility tools, such as map, calculator, and scratchpad, to best support possible human-like task executions.
Lastly, \ours{} is complemented by an extensive collection of documentation and knowledge bases that vary from general resources like English Wikipedia to more domain-specific references, such as manuals for using the integrated development tool~\cite{fan2022minedojo}. 
The content populating these websites is extracted from their real-world counterparts, preserving the authenticity of the content served on each platform.
We deliver the hosting services using Docker containers with \texttt{gym}-APIs~\cite{1606.01540}, ensuring both the usability and the reproducibility of \ours.

% Figure environment removed

Along with \ours, we release a ready-to-use benchmark with 812 long-horizon web-based tasks~(\S\ref{sec:benchmark}).
Each task is described as a high-level natural language intent, emulating the abstract language usage patterns typically employed by humans~\cite{bisk-etal-2019-benchmarking}. Two example intents are shown in the upper left of \autoref{fig:overview}.
We focus on evaluating the \emph{functional correctness} of these tasks, \ie, does the result of the execution actually achieve the desired goal~(\S\ref{sec:eval_annotation}).
For instance, to evaluate the example in \autoref{fig:main_example}, our evaluation method verifies the concrete contents in the designated repository.
This evaluation is not only more reliable~\cite{zhong1709seq2sql,chen2021evaluating,wang2022execution} than comparing the vanilla action sequences~\cite{puig_virtualhome:_2018,deng2023mind2web} but also accommodates a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks.

We use this benchmark to evaluate several agents that could follow NL command and perform web-based tasks~(\S\ref{sec:baseline_agents}). These agents are implemented with a variety of approaches, 
from those predicting subsequent action directly based on current observations and history to more sophisticated agents using step-by-step reasoning~\cite{wei2022chain,yao2022react}.%
These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-3.5 and GPT-4. 
Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of 10.59\%. 
We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex task~(\S\ref{sec:error_analysis}).
These outcomes underscore the necessity for further development towards robust and effective agents~\cite{lecun2022path} in \ours.


\section{\ours: Web Sites as an Environment for Autonomous Agents}\label{sec:webarena}
Our goal is to create a \emph{realistic} and \emph{reproducible} web environment. We achieve reproducibility by having the environment be standalone, not relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts.

\subsection{Website Selection}\label{sec:website_selection}
In order to decide which categories of web site to use, we first conducted an analysis of approximately 200 examples from the authors' actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories.
We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities~(\eg Amazon, eBay), (2) social forum platforms for opinion exchanges~ (\eg Reddit, StackExchange), (3) collaborative development platforms for software development~(\eg GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (\eg online store management).

In addition to these platforms, we selected three utility-style tools that are frequently used in web-based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes.

Recognizing the critical role of information-seeking and knowledge acquisition in web-based tasks, we also incorporated various knowledge resources into our environment. These resources range from general information repositories, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 

\paragraph{Implementation} 
We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website~(OneStopShop), GitLab, Reddit, an online store content management system (CMS), a map, and an English Wikipedia. 
Then we imported data from their real-world counterparts via a sampling method. 
As an example, our version of GitLab was developed based on the actual GitLab project.\footnote{\url{https://gitlab.com/gitlab-org/gitlab}} We carefully emulated the features of a typical code repository by including both popular projects with many issues and pull requests and smaller, personal projects.
Details of all websites in \ours can be found in Appendix \ref{app:site_implementation}.
We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state. These details are in Appendix \ref{app:env_reset}.

\subsection{Observation Space}\label{sec:observation}
% Figure environment removed
We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs%
and the web page content of the focused tab. 
\ours is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. 
The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab.
We provide flexible configuration to render the page content in many modes: (see \autoref{fig:observation} for an example):
(1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work \cite{shi2017world,deng2023mind2web,li2020mapping}; (2) the screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.\footnote{\url{https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree}} The accessibility tree is a subset of the DOM tree with elements that are \emph{relevant} and \emph{useful} for displaying the contents of a web page.
Every element is represented as its role~(\eg a link), its text content, and its properties~(\eg whether it is focusable). 
Accessibility trees largely retain the \emph{structured} information of a web page while being more compact than the DOM representation.%

We provide an option to limit the content to the contents within a viewport for all modes. %
This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements.

\subsection{Action Space}\label{sec:actions}
Following previous work on navigation and operation in web and embodied environments \cite{liu2018reinforcement, fan2022minedojo}, we design a compound action space which emulates the keyboard and mouse operations available on web pages.

\autoref{tab:actions} lists all the available actions categorized into three distinct groups. The first category includes elemental operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab management actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forwards and backward in the browsing history.

\ours allows agents to refer to elements on web pages with different approaches.
Elements can be selected either by their on-screen coordinates, represented as $(x, y)$, or by a unique element ID. This ID is an artifact generated when traversing the Document Object Model (DOM) tree or the accessibility tree. 
With element IDs, the element selection is transformed into an $n$-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. 
An accessibility tree with the assigned element IDs is presented in \autoref{fig:observation}.
For example, when the agent issues an action of texttt{click [1582]}, where \texttt{[1582]} is the unique ID for the element ``Add to Cart''. The underlying implementation of \ours will perform the clicking action, and the web page will update accordingly.
This flexibility in element selection enables \ours to support agents designed in various ways (\eg accepting input from different modalities) without compromising fair comparison metrics such as the number of steps taken.

\subsection{User Roles Simulation}\label{sec:user_role}
Users of the same website often have disparate experiences due to their distinct \emph{roles}, \emph{permissions}, and \emph{interaction histories}. 
For instance, within an E-commerce CMS, a shop owner might possess full read and write permissions across all content, whereas an employee might only be granted write permissions for products but not for customer data. We aim to emulate this scenario by generating unique user profiles on each platform.

On the shopping site, we created a customer profile that has over 35 orders within a span of two years. 
On GitLab, we selected a user who maintains several popular open-source projects with numerous merge requests and issues. This user also manages a handful of personal projects privately. 
On Reddit, our chosen profile was a user who actively participates in discussions, with many posts and comments. 
Lastly, on our E-commerce CMS, we set up a user profile for a shop owner who has full read-and-write access to all system contents.%

All users are automatically logged into their accounts using a pre-cached cookie. To our best knowledge, this is the first publicly available agent evaluation environment to implement such a characteristic. Existing literature typically operates under the assumption of universally identical user roles~\cite{shi2017world,liu2018reinforcement,deng2023mind2web}. %

\begin{table}[!t]
    \centering
    \small
    \begin{tabular}{l|l}
    \toprule
    \textbf{Action Type} & \textbf{Description} \\ \midrule
    \texttt{noop} & Do nothing  \\
    \texttt{click(element)} & Click at an element  \\
    \texttt{hover(element)} & Hover on the element \\
    \texttt{type(element, text)} & Input the text to the element \\
    \texttt{key press(key comb)} & Press the key combination~(\eg ctrl+v)  \\
    \midrule
    \texttt{tab focus(page number)} & Bring the open tab with page number to the front\\
    \texttt{new tab} & Open a new tab  \\
    \texttt{tab close} & Close the current page and focus the last open page \\ \midrule
    \texttt{go back} & Visit the last URL visited  \\
    \texttt{go forward} & Undo go back operation \\
    \texttt{goto(URL)} & Go to a URL in the current page \\
    \bottomrule
    \end{tabular}
    \caption{Action Space of \ours{}}
    \label{tab:actions}
\end{table}

\section{Benchmark Suite of Web-based Tasks}\label{sec:benchmark}
We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in \ours.
Each example comes with a metric to evaluate the functional correctness of the task execution. 
In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark.

\subsection{Controlling Agents through High-level Natural Language}
The \ours environment is denoted as $\mathcal{E}$ with state space $\mathcal{S}$, action space $\mathcal{A}$~(\S\ref{sec:actions}) and observation space $\mathcal{O}$~(\S\ref{sec:observation}).
The transition function $\mathcal{T}: \mathcal{S} \times \mathcal{A}$ is deterministic, and it is defined by the underlying implementation of each website in the environment.
Performing a task described by a natural language intent $\mathbf{i}$ can be formulated as a partially observable Markov decision process (POMDP): at each time step $t$, an agent issues an action $a_t$ given the partial observation $o_t$.
Consequently, the action results in a new state $s_{t+1}$ and its corresponding observation $o_{t+1}$. 
We propose a reward function $r(\mathbf{a}, \mathbf{s})$ to measure the success of a task execution, where $\mathbf{a}$ represents the sequence of actions, and $\mathbf{s}$ denotes all intermediate states. 
This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent's actions, such as checking the correctness of the predicted answer.
\subsection{Intent Collection}\label{sec:intent_collection}
We focus on curating \emph{realistic} intents to carry out \emph{complex} and \emph{creative} tasks within \ours.
To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites' content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites.

Next, we instructed the annotators to formulate intents based on the following criteria:
\begin{enumerate}[label=(\arabic*)]
    \item The intent should be \emph{abstract} and \emph{high-level}, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \sent{click the \texttt{science} subreddit}, we encouraged annotators to come up with something more complex like \sent{post a greeting message on \texttt{science} subreddit}, which involves performing multiple actions.
    \item The intent should be \emph{creative}. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (\eg \sent{create a Reddit account \textbf{identical to my GitLab one}}) to make the intents more unique.
    \item The intent should be formulated as a \emph{template} by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \sent{create a Reddit account identical to my GitLab one} can be converted into \sent{create a \{\{site1\}\} account identical to my \{\{site2\}\} one}, with an instantiation like \sent{\{site1: Reddit, site2: GitLab\}} and another like \sent{\{site1: GitLab, site2: OneStopShopping\}}. 
    Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. %
\end{enumerate}

We additionally provided a prompt for the annotators to use to prompt ChatGPT\footnote{\url{https://chat.openai.com/}} for inspiration.
This prompt contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites.
Furthermore, we offered a curated list of examples for annotators to reference. 

% Figure environment removed

\paragraph{Intent Analysis} In total, we curated 241 templates and 812 instantiated intents.
On average, each template is instantiated to 3.3 examples. 
The intent distribution is shown in \autoref{fig:intent_distribution}. 

Furthermore, we classify the intents into three primary categories with examples shown in \autoref{tab:intent_examples}:
\begin{enumerate}[label=(\arabic*)]
    \item Information-seeking tasks: These are tasks where a textual response is expected. Importantly, the information-seeking tasks in \ours often require navigation across multiple pages or focus on \emph{user-centric} content. This makes them distinct from open-domain question-answering tasks~\cite{yang2018hotpotqa,kwiatkowski2019natural}, which focus on querying general knowledge with a simple retrieval step. For instance, to answer \sent{When was the last time I bought the shampoo}, an agent must traverse to the user's purchase history, checking individual order details to identify the most recent shampoo purchase.
    \item Site navigation tasks: This category is composed of tasks that require navigating through web pages using a variety of interactive elements such as search functions and links. The objective is often to locate specific information or navigate to a particular section of a site.
    \item Content and configuration operation tasks: This category encapsulates tasks that require operating in the web environment to create, revise, or configure content or settings. This includes adjusting settings, managing accounts, performing online transactions, generating new web content, and modifying existing content. Examples range from updating a social media status or README file to conducting online purchases and configuring privacy settings.
\end{enumerate}

\subsection{Evaluation Annotation}\label{sec:eval_annotation}
\paragraph{Evalauting Information Seeking Tasks} To measure the correctness of information-seeking tasks where a textual answer is expected, we provide the annotated answer $a^*$ for each intent. 
The $a^*$ is further compared with the predicted answer $\hat{a}$ with one of the following scoring functions $r_{\textrm{info}}(\hat{a}, a^*)$.

First, we define \texttt{exact\_match} where only $\hat{a}$ that is identical with $a^*$ will receive a score of one. This function is primarily applicable to those intent types whose responses follow a more standardized format, similar to the evaluation on question answering literature~\cite{rajpurkar2016squad,yang2018hotpotqa}. 

Second, we create \texttt{must\_include} where any $\hat{a}$ containing $a^*$ receives a score of one. This function is primarily used in scenarios where an unordered list of text is expected or where the emphasis of evaluation is on certain key concepts. In the second example in \autoref{tab:evaluation_examples}, we expect both the correct name and the email address to be presented, irrespective of the precise wording used to convey the answer.

Finally, we introduce \texttt{fuzzy\_match} where we utilize a language model to assess whether $\hat{a}$ accurately captures the key concepts presented in $a^*$. 
Specifically, in this work, we use \texttt{gpt-3.5-turbo-0613} to assess whether $\hat{a}$ could entail each of the key concepts in $a^*$~\cite{demszky2018transforming}. 
The corresponding prompt details are provided in Appendix \ref{app:prompt_fuzzy_match}. 
The \texttt{fuzzy\_match} function is applicable to situations where the key concepts are interrelated or where the format of the answer is diverse. For instance, in responding to \sent{Compare the time for walking and driving route from AMC Waterfront to Randyland}, it is essential to ensure that driving time and walking time are accurately linked with the correct terms. 
The \texttt{fuzzy\_match} function could also flexibly match the time ``2h58min'' with different forms such as ``2 hour 58 minutes'', ``2:58'' and others. 

Our evaluation methods emphasize the \textit{recall} of key concepts within the reference answer rather than precision or overall answer similarity, as measured by metrics such as ROUGE or METEOR.  
This is mainly due to the fact that the questions in our dataset often require the inclusion of specific content, while the precise choice of terminology and phrasing is less critical in our context. By emphasizing recall, we attempt to ensure the helpfulness of the predicted answer, irrespective of its syntactic conformity to the reference.
Existing works that measure answer recall in long-form QA share a similar spirit \cite{jiang2023flare}.


\paragraph{Evaluating Site Navigation and Content \& Config Tasks} The tasks in these categories require accessing web pages that meet certain conditions or performing operations that modify the underlying data storage of the respective websites. 
To assess these, we establish reward functions $r_{\textrm{prog}}(\mathbf{s})$ that programmatically examine the intermediate states $\mathbf{s}$ within an execution trajectory to ascertain whether the outcome aligns with the intended result.
These intermediate states are often the underlying databases of the websites, the status, and the content of a web page at each step of the execution. 

Evaluation of each instance involves two primary components. First, we provide a \texttt{locator}, tasked with retrieving the critical content pertinent to each intent. 
The implementation of this locator may vary from a database query, a website-supported API call, to a JavaScript element selection on the relevant web page, depending on implementation feasibility.
For example, in evaluating the intent of the fifth example in \autoref{tab:evaluation_examples}, the evaluation process first obtains the URL of the latest post by examining the last state in the state sequence $\mathbf{s}$. Then it navigates to the corresponding post page and runs Javascript code ``\texttt{document.querySelector(`.submission\_\_inner').outerText}'' to obtain the content of the post.

Subsequently, we annotate \texttt{keywords} that need to exist within the located content. For example, the evaluation verifies if the post is correctly posted in the ``nyc'' subreddit by examining the URL of the post and if the post contains the requested content by examining the post content.
We reuse the \texttt{exact\_match} and \texttt{must\_include} functions from information-seeking tasks for this purpose.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{cccl}
    \toprule
    \textbf{Function} & \textbf{ID} & \textbf{Intent} & \multicolumn{1}{c}{\textbf{Eval Implementation}} \\
    \midrule
     \multirow{6}{*}{\shortstack{$r_{\textrm{info}}(a^*, \hat{a})$}} & \multirow{2}{*}{\shortstack{1}} & \multirow{2}{*}{\shortstack{Tell me the name of the customer who \\ has the most cancellations in the history}} & \multirow{2}{*}{\shortstack{\texttt{\textcolor{blue}{exact\_match}}($\hat{a}$, \textcolor{red}{``Samantha Jones''})}} \\
      & & \\
      \cmidrule{2-4}
      & \multirow{2}{*}{\shortstack{2}}  &  \multirow{2}{*}{\shortstack{Find the customer name and \\ email with phone number 8015551212}} & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{must\_include}}($\hat{a}$, \textcolor{red}{``Sean Miller''})}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{must\_include}}($\hat{a}$, \textcolor{red}{``sean@gmail.com''})}} \\
      \cmidrule{2-4}
      & \multirow{2}{*}{\shortstack{3}} &  \multirow{2}{*}{\shortstack{Compare walking and driving time \\ from AMC Waterfront to Randyland}} & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{fuzzy\_match}}($\hat{a}$, \textcolor{red}{``Walking: 2h58min''})}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{fuzzy\_match}}($\hat{a}$, \textcolor{red}{\textcolor{red}{``Driving: 21min''}})}} \\
      \midrule 
      \multirow{9}{*}{\shortstack{$r_{\textrm{prog}}(\mathbf{s})$}} & \multirow{4}{*}{\shortstack{4}} & \multirow{4}{*}{\shortstack{Checkout merge requests \\ assigned to me}} & \multirow{1}{*}{\shortstack{\texttt{url = locate\_last\_url($\mathbf{s}$)}}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{exact\_match}(URL, \textcolor{red}{``gitlab.com/merge\_}}}} \\
      & & & \multirow{1}{*}{\shortstack{\hspace{1em}\texttt{\textcolor{red}{requests?assignee\_username''}}}} \\
      & & & \multirow{1}{*}{\shortstack{\hspace{1em}\texttt{\textcolor{red}{=byteblaze''}}}} \\
      \cmidrule{2-4}
      & \multirow{5}{*}{\shortstack{5}} & \multirow{5}{*}{\shortstack{Post to ask ``whether I \\ need a car in NYC''}} & \multirow{1}{*}{\shortstack{\texttt{url =  locate\_latest\_post\_url($\mathbf{s})$}}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{body =  locate\_latest\_post\_body($\mathbf{s})$}}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{must\_include}(URL, \textcolor{red}{``/f/nyc''})}}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{must\_include}(body,}}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\hspace{1em}\textcolor{red}{``whether I need a car in NYC''}}}} \\
      \bottomrule
      \end{tabular}%
    }
    \caption{We introduce two evaluation approaches. $r_{\textrm{info}}$ (top) measures the correctness of performing information-seeking tasks. It compares the predicted answer $\hat{a}$ with the annotated reference $a^*$ with three implementations.
    $r_{\textrm{prog}}$ (bottom) programmatically checks whether the intermediate states during the executions possess the anticipated properties specified by the intent. %
    }
    \label{tab:evaluation_examples}
\end{table}

\paragraph{Unachievable Tasks} Due to constraints such as inadequate evidence, user permissions (\S\ref{sec:user_role}), or the absence of necessary functional support in the website, some tasks ($\sim$4\%) in our benchmark are not possible to complete.
For instance, fulfilling an intent like \sent{Tell me the contact number of OneStopShop} is impracticable in \ours, given that the website does not provide such contact information. 
We label such instances as "N/A" and expect that an agent should produce an equivalent response. 
These examples allow us to assess an agent's ability to avoid making unfounded claims and its adherence to factual accuracy.

\paragraph{Annotation Process} The intents were contributed by the authors following the annotation guideline in \S\ref{sec:intent_collection}. Every author has extensive experience with web-based tasks. 
The reference answers of the information-seeking tasks were curated by the authors and an external annotator. To ensure consistency and accuracy, each question was annotated twice. If the two annotators disagreed, a third annotator finalized the annotation.
The programs to evaluate the remaining examples were contributed by three of the authors who are proficient in JavaScript programming. 
Difficult tasks were often discussed collectively to ensure the correctness of the annotation.
The annotation required the annotator to undertake the full execution and scrutinize the intermediate states.


\section{Baseline Web Agents}\label{sec:baseline_agents}
\paragraph{Prompts} We design two LLM-prompt-based web agents as our baselines where the input observations and the predicted actions are all in text:
\begin{enumerate*}[label=(\arabic*)]
    \item \textit{Direct Agent}: the agent takes as input the observation and directly predict the next action. %
    \item \textit{Reasoning Agent}: the agent first performs a sequence of reasoning steps in text, then issues the next action~\cite{wei2022chain, yao2022react}. The reasoning often involves interpreting the intent, understanding the current state, and estimating the task progress, which could aid task completion. 
\end{enumerate*}
In both prompts, we first elaborate the definition of web-based tasks, then provide a full list of available actions and their explanations. We also include in the prompt a few notes on avoiding common failures, which we observed during our initial experiments on simple examples. For instance, we found that the agent tends to generate a sequence of actions instead of one action at a time, and therefore provide a hint of \sent{You should only issue one action at a time.} Finally, we provide two demonstrations of observation $\rightarrow$ action pairs in the prompt. 
The full prompts can be found in Appendix \ref{app:agent_prompts}.
\vspace{-2mm}
\paragraph{Observations \& Actions} Primarily, we use an accessibility tree with element IDs as the observation space, as shown in the rightmost example of \autoref{fig:observation}. %
Although we provide careful instructions on avoiding common failures, the actions are not flawless. For example, the agent frequently issues the actions in the wrong format, which results in parsing failures. 
To address this, we incorporate these error messages in the observation, thereby enabling the agent to make dynamic adjustments and learn from the failures. 
The agent can identify which element to interact with by the ID of the element. For instance, the agent can issue \texttt{click [1582]} to click the ``Add to Cart'' button.
As part of our future research, we intend to explore the possibility of multi-modal agents that accept image-based observations and perform pixel-based actions.



\paragraph{Configurations} We experiment with \texttt{gpt-3.5-turbo-0613} and \texttt{gpt-4-0613} with a temperature of $1.0$\footnote{We use a high temperature to encourage exploration as we observed frequent early termination during agent development. More details are in \S\ref{sec:error_analysis}.} and a top-$p$ parameter of $0.9$. The maximum number of state transitions is set to 30.
We halt execution if the same action is repeated more than three times on the same observation or if the agent generates three consecutive invalid actions. These situations typically indicate a high likelihood of execution failure and hence warrant early termination. 


\section{Results}

\subsection{Main Results}\label{sec:results}
\begin{table}[t]
    \centering
    \begin{tabular}{ccc}
    \toprule
    \textbf{Agent} & \textbf{Model} & \textbf{Success Rate} \\
    \midrule
    Direct & \texttt{gpt-3.5} &  7.14 \\
    Reasoning & \texttt{gpt-3.5}  & 7.38 \\
    Reasoning & \texttt{gpt-4} & 10.59 \\
    \bottomrule
    \end{tabular}
    \caption{The end-to-end task success rate (\%) on \ours. The direct agent directly predicts the next action given the observation, and the reasoning agent performs the reasoning on the observation, progress, and others \emph{before} the action prediction.}
    \label{tab:main_results}
\end{table}

The main results are shown in \autoref{tab:main_results}. The reasoning agent powered by GPT-4 achieves a relatively modest end-to-end task success rate of 10.63\%. 
When the same reasoning agent is powered by GPT-3.5, the success rate drops to 7.38\%. 
Interestingly, the reasoning agent that explicitly engages in a reasoning process prior to providing the response does not exhibit a marked performance improvement when compared to its simpler counterpart that directly predicts the subsequent action without the intermediate reasoning step. The GPT-3.5 reasoning agent outperforms the direct agent only by successfully completing a marginal number of examples.
These results underline the inherent challenges and complexities of executing tasks that span long horizons, particularly in realistic environments as \ours.





% Figure environment removed

\paragraph{Consistency of Performing Tasks From the Same Template} Tasks derived from the same template typically share similar reasoning and planning procedures, albeit manifesting distinct executions and observations. We visualize the task success rate for templates with at least one successful completion in \autoref{fig:success_template}. 
Of the 41 such templates, the GPT-4 agent manages to achieve a 100\% task success rate on only one template, while GPT-3.5 agents do not achieve full task completion for any of the templates. In several cases, the agents are only able to complete one task variation among others.  
The discrepancy in performance arises from two reasons. First, the complexity of tasks can vary, even within the same template. For instance, the agent is able to correctly perform \sent{Fork metaseq} while it fails to \sent{Fork all repos from Facebook}. The latter task necessitates more repetitive operations, therefore increasing its complexity.
Second, the prediction may have larger variances due to the high temperature we use in the experiment. 
Nevertheless, these results suggest the current instability of agents in handling complex tasks. 
More refined techniques, such as procedural skill libraries \cite{zhou2021hierarchical, wang2023voyager}, may enhance the performance as they could enable the \emph{reuse} of successful past experiences and therefore lead to more consistent outcomes.


\subsection{Error Analysis}\label{sec:error_analysis}
We perform an analysis by looking into the recorded execution trajectories from the GPT-4 reasoning agent and identify the following common failure modes.

\paragraph{Incorrect Infeasibility Determination} A prevalent issue with GPT-4 agents is the early stop of tasks due to a conclusion of non-achievability. This issue often manifests as the agent outputting a \texttt{stop [N/A]} action. This behavior is observed in instances where the GPT-4 agent erroneously identifies approximately 54.9\% of feasible tasks (428 tasks) as impossible.  
For instance, as on the left of \autoref{fig:common_errors}, to achieve the goal of \sent{Fork all repos from Facebook}, the agent first correctly performs the search of Facebook. However, the default search lists the possible projects named Facebook, while the agent needs to further click the user column to find the Facebook account. The agent decides to stop because it thinks that ``It appears that there are no available projects matching Facebook. Hence, it is not possible to fork any Facebook repositories from GitLab.''. 

\paragraph{Observation Bias} Realistic websites frequently present information on similar topics across various sections to ensure optimal user accessibility. However, a GPT-4 agent often demonstrates a tendency to latch onto the first related piece of information it encounters without sufficiently verifying its relevance or accuracy. 
For instance, the homepage of the E-Commerce CMS displays the best-selling items based on \emph{recent purchases}, while historical best-seller data is typically accessed via a separate report. Presented with the task of \sent{What is the top-1 best-selling product in 2022}, the GPT-4 agent defaults to leveraging the readily available information on the homepage, bypassing the necessary step of generating the report to obtain the accurate data.

\paragraph{Failures in Observation Interpretation} Interestingly, while a GPT-4 agent is capable of summarizing the observations, it occasionally overlooks more granular information, such as the previously entered input. As in the right-hand example of \autoref{fig:common_errors}, \texttt{[5172] StaticText} indicates that the search term ``DMV area'' has already been entered. However, the agent disregards this detail and continuously issues the command \texttt{type [2430] [DMV area]} until it reaches the maximum step limit. Furthermore, the agent often neglects the previous action information that is provided alongside the observation.

We hypothesize that these observed failures are related to the current pretraining and supervised fine-tuning on dialogues employed in GPT models~\cite{ouyang2022training}. These models are primarily trained to execute instructions given \emph{immediate} observations (\ie, the dialogue history); thereby, they may exhibit a lack of explorations. 
Furthermore, in dialogue scenarios, subtle differences in NL expressions often have less impact on the overall conversation. As a result, models may tend to overlook minor variations in their observations.



\section{Related Work}
\begin{table}[]
    \centering
    \begin{tabular}{ccccc}
    \toprule
    \multirow{2}{*}{\shortstack{Benchmark}} & \multirow{2}{*}{\shortstack{Dynamic \\ Interaction?}} & \multirow{2}{*}{\shortstack{Realistic \\ Environment?}} & \multirow{2}{*}{\shortstack{Diverse \\ Human Tasks?}} & \multirow{2}{*}{\shortstack{Functional \\ Correctness?}}  \\
    & \\
    \midrule
    Mind2Web~\cite{deng2023mind2web} & \xmark & \cmark & \cmark & \xmark \\
    Form/QAWoB~\cite{shi2017world} & \xmark & \cmark & \cmark & \xmark  \\
    MiniWoB++~\cite{liu2018reinforcement} & \cmark & \xmark & \xmark & \cmark \\
    Webshop~\cite{yao2022webshop} & \cmark & \xmark & \xmark & \cmark \\
    ALFRED~\cite{shridhar_alfred:_2019} & \cmark & \xmark & \xmark & \cmark \\
    VirtualHome~\cite{puig_virtualhome:_2018} & \xmark & \xmark & \cmark & \xmark \\
    AndroidEnv~\cite{toyama2021androidenv} & \cmark & \cmark & \xmark & \xmark \\
    \midrule
    \ours & \cmark & \cmark & \cmark & \cmark \\
    \bottomrule    
    \end{tabular}
    \caption{The comparison between our benchmark and existing benchmarks on grounding natural language instructions to concrete executions. Our benchmark is implemented in our fully interactable highly-realistic \ environment. It features diverse tasks humans may encounter in their daily routines. We design evaluation metrics to assess the functional correctness of task executions.}
    \label{tab:comparision}
\end{table}
\paragraph{Benchmarks for Controlling Agents through Natural Language} Using computer and smartphone platforms as an environment to study controlling agents through natural language has been studied in the literature~\cite{branavan2009reinforcement,shi2017world,liu2018reinforcement, toyama2021androidenv,deng2023mind2web,li2020mapping,xu2021grounding}. However, achieving a balance between \emph{functionality}, \emph{authenticity}, and \emph{support for environmental dynamics} is challenging. Existing works often necessitate a compromise between these aspects, as shown in \autoref{tab:comparision}.
Some works~\cite{shi2017world,deng2023mind2web} depend on a static cache of states during data collection, which constrains the agents' explorations on unfamiliar states and assess functional correctness. Other works~\cite{yao2022webshop,liu2018reinforcement} adopt an overly simplistic representation of real-world environmental complexities, which narrows the breadth and depth of feasible tasks, restricting the potential for more complex or varied task execution.
While AndroidEnv~\cite{toyama2021androidenv} aim to emulate an independent Android setup, they fall short in providing human high-level intents and task evaluations.
Such simplifications are observed in synthetic environments such as AI2THOR~\cite{ai2thor, shridhar_alfred:_2019} and VirtualHome~\cite{puig_virtualhome:_2018}. Finally, gaming environments~\cite{fan2022minedojo, kuttler_nethack_2020} are constrained by their inherent design principles which diverge from human objectives~(\eg agent survives for $n$ days).
\ours addresses these limitations by introducing a self-hostable web environment conducive to performing realistic day-to-day tasks. The feature of self-hosting permits comprehensive end-to-end task execution evaluations, providing a more holistic and robust benchmark suit for future agent developments.


\paragraph{Natural Language Command and Control} Executing complex tasks described in a high-level natural language requires several capabilities, including hierarchical planning, state tracking, error recovery, and others.
Large Language Models (LLMs) have been observed to break down high-level assignments into more manageable sub-tasks~\cite{huang2022language}.
To link these broken-down plans to tangible executions and encourage the reuse of skills, several studies advocate for the representation of task executions as programs~\cite{zhou2021hierarchical,liang2023code,wang2023voyager,gao2023pal}. 
The introduction of structured planning and backtracking methodologies, such as the tree-of-thoughts~\cite{yao2023tree,long2023large}, further allow for a more structured task planning, as well as the capacity to reconsider previous decisions.
Moreover, it is crucial for LLMs to include built-in failure recovery and self-correction mechanisms to increase their robustness~\cite{shinn2023reflexion,kim2023language}. 
However, while these approaches have shown promise, their applications have largely been tested in relatively simple settings. For instance, in environments where an erroneous action can be reversed by a single \texttt{undo} command. However, actions in complex environments such as \ours may not be easily reversible. For example, canceling an incorrectly placed order requires navigating through a series of complex cancellation steps rather than just executing a simple \texttt{undo} command.

\section{Conclusion}
We present \ours, a highly-realistic, standalone, and reproducible web environment designed for the development and testing of autonomous agents. \ours includes fully functional web applications and genuine data from four major categories, providing a realistic platform for agent interaction. It further supports a wide range of tools and external knowledge bases, fostering a focus on human-like problem-solving.
Additionally, we curate a comprehensive benchmark consisting of 812 examples that focus on translating high-level natural language intents into specific web interactions. We also offer metrics to programmatically ascertain whether tasks have been completed according to the desired objectives.

When tested with the state-of-the-art GPT-4 language model, the best-performing agent achieved a modest end-to-end task success rate of 10.59%
Our error analysis suggests that while the agent could steer the interactions toward the task completion, there are several areas for improvement.
Common errors include an overestimation of task infeasibility and an over-reliance on readily available context at the expense of necessary exploration. Additionally, the agent often fails to distinguish nuances in observations, leading to repeated actions that ultimately result in task termination.

These findings underscore the need for future research to focus on enhancing the robustness and efficacy of autonomous agents within the \ours environment. 


\bibliography{custom}
\bibliographystyle{plain}




\clearpage

\appendix
\input{appendix}

\end{document}
