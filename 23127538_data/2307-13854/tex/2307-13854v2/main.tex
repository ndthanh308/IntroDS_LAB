
\documentclass{article}
\usepackage{iclr2024_conference,times}
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xspace}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{pifont}%
\newcommand{\cmark}{\textcolor{green}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}%
\input{commands}

\definecolor{grey}{gray}{0.8}


\newcommand{\draftcomment}[1]{#1}

\title{\ours: A Realistic Web Environment for Building Autonomous Agents}




\author{%
  Shuyan Zhou\thanks{Lead contributors.} \quad
  Frank F. Xu\footnotemark[1] \quad
  \textbf{Hao Zhu}\thanks{Equal contribution.} \quad 
  \textbf{Xuhui Zhou}\footnotemark[2] \quad \\
  \textbf{Robert Lo}\footnotemark[2] \quad 
  \textbf{Abishek Sridhar}\footnotemark[2] \quad 
  \textbf{Xianyi Cheng} \quad 
  \textbf{Tianyue Ou} \quad \\
  \textbf{Yonatan Bisk} \quad 
  \textbf{Daniel Fried} \quad 
  \textbf{Uri Alon} \quad 
  \textbf{Graham Neubig} \\ \\
  {Carnegie Mellon University} \\
  \texttt{\{shuyanzh, fangzhex, gneubig\}@cs.cmu.edu} \\
}

\iclrfinalcopy 
\begin{document}


\maketitle


\begin{abstract}
With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios.
In this paper, we build an environment for language-guided agents that is \emph{highly realistic} and \emph{reproducible}.
Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.
Our environment is enriched with tools~(\eg a map) and external knowledge bases~(\eg user manuals) to encourage human-like task-solving.
Building upon our environment, we release a set of benchmark tasks focusing on evaluating the \emph{functional correctness} of task completions.
The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet.
We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. 
The results demonstrate that solving complex tasks is challenging: our best \textsc{GPT-4}-based agent only achieves an end-to-end task success rate of 14.41\%, significantly lower than the human performance of 78.24\%.
These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \ours can be used to measure such progress.%

Our code, data, environment reproduction resources, and video demonstrations are
publicly available at \url{https://webarena.dev/}.
\end{abstract}


\section{Introduction}\label{sec:intro}
Autonomous agents that  perform everyday tasks via human natural language commands could significantly augment human capabilities, improve efficiency, and increase accessibility.
Nonetheless, to fully leverage the power of autonomous agents, it is crucial to understand their behavior within an environment that is both \emph{authentic} and \emph{reproducible}.
This will allow measurement of the ability of agents on tasks that human users care about in a fair and consistent manner.

Current environments for evaluate agents tend to \emph{over-simplify} real-world situations. %
As a result, the functionality of many environments is a limited version of their real-world counterparts, leading to a lack of task diversity~\citep{shi2017world,anderson_vision-and-language_2018,gordon2018iqa,misra2016tell,shridhar_alfred:_2019,shridhar_alfworld_2020,yao2022webshop}.
In addition, these simplifications often lower the complexity of tasks as compared to their execution in the real world~\citep{puig_virtualhome:_2018,shridhar_alfred:_2019,yao2022webshop}. 
Finally, some environments are presented as a static resource~\citep{shi2017world, deng2023mind2web} where agents are confined to accessing only those states that were previously cached during data collection, thus limiting the breadth and diversity of exploration.
Dor evaluation, many environments focus on comparing the textual \emph{surface form} of the predicted action sequences with reference action sequences, disregarding the \emph{functional correctness} of the executions and possible alternative solutions~\citep{puig_virtualhome:_2018,jernite_craftassist_2019, xu2021grounding,li2020mapping, deng2023mind2web}.
These limitations often result in a discrepancy between simulated environments and the real world, 
and can potentially impact the generalizability of AI agents to successfully understand, adapt, and operate within complex real-world situations.

% Figure environment removed

We introduce \ours{}, a \emph{realistic} and \emph{reproducible} web environment designed to facilitate the development of autonomous agents capable of executing tasks~(\S\ref{sec:webarena}). An overview of \ours is in \autoref{fig:overview}.
Our environment comprises four fully operational, self-hosted web applications, each representing a distinct domain prevalent on the internet: online shopping, discussion forums, collaborative development, and business content management. %
Furthermore, \ours{} incorporates several utility tools, such as map, calculator, and scratchpad, to best support possible human-like task executions.
Lastly, \ours{} is complemented by an extensive collection of documentation and knowledge bases that vary from general resources like English Wikipedia to more domain-specific references, such as manuals for using the integrated development tool~\citep{fan2022minedojo}. 
The content populating these websites is extracted from their real-world counterparts, preserving the authenticity of the content served on each platform.
We deliver the hosting services using Docker containers with \texttt{gym}-APIs~\citep{1606.01540}, ensuring both the usability and the reproducibility of \ours.

Along with \ours, we release a ready-to-use benchmark with 812 long-horizon web-based tasks~(\S\ref{sec:benchmark}).
Each task is described as a high-level natural language intent, emulating the abstract language usage patterns typically employed by humans~\citep{bisk-etal-2019-benchmarking}. Two example intents are shown in the upper left of \autoref{fig:overview}.
We focus on evaluating the \emph{functional correctness} of these tasks, \ie does the result of the execution actually achieve the desired goal~(\S\ref{sec:eval_annotation}).
For instance, to evaluate the example in \autoref{fig:main_example}, our evaluation method verifies the concrete contents in the designated repository.
This evaluation is not only more reliable~\citep{zhong1709seq2sql,chen2021evaluating,wang2022execution} than comparing the textual surface-form action sequences~\citep{puig_virtualhome:_2018,deng2023mind2web} but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks.

We use this benchmark to evaluate several agents that can follow NL command and perform web-based tasks~(\S\ref{sec:baseline_agents}). 
These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as \textsc{GPT-4} and \textsc{PALM-2}. 
Experiment results show that the best \textsc{GPT-4} agent performance is somewhat limited, with an end-to-end task success rate of only 14.41\%, while the human performance is 78.24\%.
We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks ~(\S\ref{sec:analysis}).
These outcomes underscore the necessity for further development towards robust and effective agents~\citep{lecun2022path} in \ours.


\section{\ours: Websites as an Environment for Autonomous Agents}\label{sec:webarena}
Our goal is to create a \emph{realistic} and \emph{reproducible} web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts.

\subsection{Controlling Agents through High-level Natural Language}
The \ours environment is denoted as $\mathcal{E}$\textcolor{black}{$=\langle \mathcal{S},\mathcal{A},\mathcal{O},\mathcal{T} \rangle$} with state space $\mathcal{S}$, action space $\mathcal{A}$~(\S\ref{sec:actions}) and observation space $\mathcal{O}$~(\S\ref{sec:observation}).
The transition function $\mathcal{T}: \mathcal{S} \times \mathcal{A}$\textcolor{black}{$\longrightarrow \mathcal{S}$} is deterministic, and it is defined by the underlying implementation of each website in the environment.
Performing a task described by a natural language intent $\mathbf{i}$ can be formulated as a partially observable Markov decision process (POMDP): at each time step $t$, an agent issues an action $a_t$\textcolor{black}{$\in \mathcal{A}$} given the partial observation $o_t$\textcolor{black}{$\in \mathcal{O}$}.
Consequently, the action results in a new state $s_{t+1}$\textcolor{black}{$\in \mathcal{S}$} and its corresponding observation $o_{t+1}$\textcolor{black}{$\in \mathcal{O}$}. 
We propose a reward function $r(\mathbf{a}, \mathbf{s})$ to measure the success of a task execution, where $\mathbf{a}$ represents the sequence of actions, and $\mathbf{s}$ denotes all intermediate states. 
This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent's actions, such as checking the correctness of the predicted answer.


% Figure environment removed

\subsection{Website Selection}\label{sec:website_selection}
To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors' actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories.
We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities~(\eg Amazon, eBay), (2) social forum platforms for opinion exchanges~ (\eg Reddit, StackExchange), (3) collaborative development platforms for software development~(\eg GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (\eg online store management).

In addition to these platforms, we selected three utility-style tools that are frequently used in web-based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes.
As information-seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into \ours. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. 

\paragraph{Implementation} 
We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website~(OneStopShop), GitLab, Reddit, an online store content management system (CMS), a map, and an English Wikipedia. 
Then we imported sampled data from their real-world counterparts. %
As an example, our version of GitLab was developed based on the actual GitLab project.\footnote{\url{https://gitlab.com/gitlab-org/gitlab}} We carefully emulated the features of a typical code repository by including both popular projects with many issues and pull requests and smaller, personal projects.
Details of all websites in \ours can be found in Appendix \ref{app:site_implementation}.
We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix \ref{app:env_reset}).

\subsection{Observation Space}\label{sec:observation}
% Figure environment removed
We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs
, and the web page content of the focused tab. 
\ours is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. 
The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab.
We provide flexible configuration to render the page content in many modes: (see \autoref{fig:observation} for an example):
(1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work \citep{shi2017world,deng2023mind2web,li2020mapping}; (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.\footnote{\url{https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree}} 
The accessibility tree is a subset of the DOM tree with elements that are \emph{relevant} and \emph{useful} for displaying the contents of a web page.
Every element is represented as its role~(\eg a link), its text content, and its properties~(\eg whether it is focusable). 
Accessibility trees largely retain the \emph{structured} information of a web page while being more compact than the DOM representation.%

We provide an option to limit the content to the contents within a viewport for all modes. %
This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements.

\subsection{Action Space}\label{sec:actions}
Following previous work on navigation and operation in web and embodied environments \citep{shi2017world,liu2018reinforcement}, we design a compound action space that emulates the keyboard and mouse operations available on web pages.
\autoref{tab:actions} lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history.

Building on these actions, \ours provides agents with the flexibility to refer to elements for operation in different ways.
An element can be selected by its on-screen coordinates, %
$(x, y)$, or by a unique element ID that is prepended to each element. 
This ID is %
generated when traversing the Document Object Model (DOM) or accessibility tree. 
With element IDs, the element selection is transformed into an $n$-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. 
For example, issuing the action \texttt{click\!\! [1582]} clicks the button given the observation of \texttt{[1582] Add to Cart}.
This flexible element selection allows \ours to support agents designed in various ways (\eg accepting input from different modalities) without compromising fair comparison metrics such as step count.

\paragraph{User Role Simulation}
Users of the same website often have disparate experiences due to their distinct \emph{roles}, \emph{permissions}, and \emph{interaction histories}. 
We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix \ref{sec:user_role}.



\section{Benchmark Suite of Web-based Tasks}\label{sec:benchmark}
We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in \ours.
Each example has a metric to evaluate the functional correctness of the task execution. 
In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark.

\subsection{Intent Collection}\label{sec:intent_collection}
We focus on curating \emph{realistic} intents to carry out \emph{complex} and \emph{creative} tasks within \ours.
To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites' content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites.

Next, we instructed the annotators to formulate intents based on the following criteria:
\begin{enumerate}[label=(\arabic*),leftmargin=20pt,noitemsep]
    \item The intent should be \emph{abstract} and \emph{high-level}, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \sent{click the \texttt{science} subreddit}, we encouraged annotators to come up with something more complex like \sent{post a greeting message on \texttt{science} subreddit}, which involves performing multiple actions.
    \item The intent should be \emph{creative}. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (\eg \sent{create a Reddit account \textbf{identical to my GitLab one}}) to make the intents more unique.
    \item The intent should be formulated as a \emph{template} by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \sent{create a Reddit account identical to my GitLab one} can be converted into \sent{create a \{\{site1\}\} account identical to my \{\{site2\}\} one}, with an instantiation like \sent{\{site1: Reddit, site2: GitLab\}} and another like \sent{\{site1: GitLab, site2: OneStopShopping\}}. 
    Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. %
\end{enumerate}

We also provided a prompt for the annotators to use with ChatGPT\footnote{\url{https://chat.openai.com/}} for inspiration,
that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites.
Furthermore, we offered a curated list of examples for annotators to reference. 

% Figure environment removed

\paragraph{Intent Analysis} In total, we curated 241 templates and 812 instantiated intents.
On average, each template is instantiated to 3.3 examples. 
The intent distribution is shown in \autoref{fig:intent_distribution}. 

Furthermore, we classify the intents into three primary categories with examples shown in \autoref{tab:intent_examples}:
\begin{enumerate}[label=(\arabic*),leftmargin=20pt,noitemsep]
    \item \textbf{Information-seeking} tasks expect a textual response. Importantly, these tasks in \ours often require navigation across multiple pages or focus on \emph{user-centric} content. This makes them distinct from open-domain question-answering ~\citep{yang2018hotpotqa,kwiatkowski2019natural}, which focuses on querying general knowledge with a simple retrieval step. For instance, to answer \sent{When was the last time I bought the shampoo}, an agent traverses the user's purchase history, checking order details to identify the most recent shampoo purchase.
    \item \textbf{Site navigation}: This category is composed of tasks that require navigating through web pages using a variety of interactive elements such as search functions and links. The objective is often to locate specific information or navigate to a particular section of a site.
    \item \textbf{Content and configuration operation}: This category encapsulates tasks that require operating in the web environment to create, revise, or configure content or settings. This includes adjusting settings, managing accounts, performing online transactions, generating new web content, and modifying existing content. Examples range from updating a social media status or README file to conducting online purchases and configuring privacy settings.
\end{enumerate}

\subsection{Evaluation Annotation}\label{sec:eval_annotation}
\paragraph{Evaluating Information Seeking Tasks} To measure the correctness of information-seeking tasks where a textual answer is expected, we provide the annotated answer $a^*$ for each intent. 
The $a^*$ is further compared with the predicted answer $\hat{a}$ with one of the following scoring functions $r_{\textrm{info}}(\hat{a}, a^*)$.

First, we define \texttt{exact\_match} where only $\hat{a}$ that is identical with $a^*$ receives a score of one. This function is primarily applicable to intent types whose responses follow a more standardized format, similar to the evaluation on question answering literature~\citep{rajpurkar2016squad,yang2018hotpotqa}. 

Second, we create \texttt{must\_include} where any $\hat{a}$ containing $a^*$ receives a score of one. This function is primarily used in %
when %
an unordered list of text is expected or where the emphasis of evaluation is on certain key concepts. In the second example in \autoref{tab:evaluation_examples}, we expect both the correct name and the email address to be presented, irrespective of the precise wording used to convey the answer.

Finally, we introduce \texttt{fuzzy\_match} where we utilize a language model to assess whether $\hat{a}$ is semantically equivalent to $a^*$. 
Specifically, in this work, we use \texttt{gpt-4-0613} to perform this evaluation.
The corresponding prompt details are provided in Appendix \ref{app:prompt_fuzzy_match}. 
The \texttt{fuzzy\_match} function applies to situations where the format of the answer is diverse. For instance, in responding to \sent{Compare the time for walking and driving route from AMC Waterfront to Randyland}, it is essential to ensure that driving time and walking time are accurately linked with the correct terms. 
The \texttt{fuzzy\_match} function could also flexibly match the time ``2h58min'' with different forms such as ``2 hour 58 minutes'', ``2:58'' and others. 



\paragraph{Evaluating Site Navigation and Content \& Config Tasks} The tasks in these categories require accessing web pages that meet certain conditions or performing operations that modify the underlying data storage of the respective websites. 
To assess these, we establish reward functions $r_{\textrm{prog}}(\mathbf{s})$ that programmatically examine the intermediate states $\mathbf{s}$ within an execution trajectory to ascertain whether the outcome aligns with the intended result.
These intermediate states are often the underlying databases of the websites, the status, and the content of a web page at each step of the execution. 

Evaluating each instance involves two components. First, we provide a \texttt{locator}, tasked with retrieving the critical content pertinent to each intent. 
The implementation of this locator varies from a database query, a website-supported API call, to a JavaScript element selection on the relevant web page, depending on implementation feasibility.
For example, the evaluation process for the intent of the fifth example in \autoref{tab:evaluation_examples}, first obtains the URL of the latest post by examining the last state in the state sequence $\mathbf{s}$. Then it navigates to the corresponding post page and obtains the post's content by running the Javascript {\small ``\texttt{document.querySelector(`.submission\_\_inner').outerText}''}.

Subsequently, we annotate \texttt{keywords} that need to exist within the located content. For example, the evaluation verifies if the post is correctly posted in the ``nyc'' subreddit by examining the URL of the post and if the post contains the requested content by examining the post content.
We reuse the \texttt{exact\_match} and \texttt{must\_include} functions from information-seeking tasks for this purpose.

\begin{table}[t]
    \vspace{-10mm}
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{@{}cccl@{}}
    \toprule
    \textbf{Function} & \textbf{ID} & \textbf{Intent} & \multicolumn{1}{c}{\textbf{Eval Implementation}} \\
    \midrule
     \multirow{6}{*}{\shortstack{$r_{\textrm{info}}(a^*, \hat{a})$}} & \multirow{2}{*}{\shortstack{1}} & \multirow{2}{*}{\shortstack{Tell me the name of the customer who \\ has the most cancellations in the history}} & \multirow{2}{*}{\shortstack{\texttt{\textcolor{blue}{exact\_match}}($\hat{a}$, \textcolor{red}{``Samantha Jones''})}} \\
      & & \\
      \cmidrule{2-4}
      & \multirow{2}{*}{\shortstack{2}}  &  \multirow{2}{*}{\shortstack{Find the customer name and \\ email with phone number 8015551212}} & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{must\_include}}($\hat{a}$, \textcolor{red}{``Sean Miller''})}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{must\_include}}($\hat{a}$, \textcolor{red}{``sean@gmail.com''})}} \\
      \cmidrule{2-4}
      & \multirow{2}{*}{\shortstack{3}} &  \multirow{2}{*}{\shortstack{Compare walking and driving time \\ from AMC Waterfront to Randyland}} & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{fuzzy\_match}}($\hat{a}$, \textcolor{red}{``walking: 2h58min''})}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{fuzzy\_match}}($\hat{a}$, \textcolor{red}{\textcolor{red}{``driving: 21min''}})}} \\
      \midrule 
      \multirow{9}{*}{\shortstack{$r_{\textrm{prog}}(\mathbf{s})$}} & \multirow{3}{*}{\shortstack{4}} & \multirow{3}{*}{\shortstack{Checkout merge requests \\ assigned to me}} & \multirow{1}{*}{\shortstack{\texttt{url=locate\_current\_url($\mathbf{s}$)}}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{exact\_match}(URL}, \textcolor{red}{``gitlab.com/merge\_}}} \\
      & & & \multirow{1}{*}{\shortstack{\hspace{1em}\textcolor{red}{requests?assignee\_username=byteblaze''})}} \\
      \cmidrule{2-4}
      & \multirow{4}{*}{\shortstack{5}} & \multirow{4}{*}{\shortstack{Post to ask ``whether I \\ need a car in NYC''}} & \multirow{1}{*}{\shortstack{\texttt{url=locate\_latest\_post\_url($\mathbf{s})$}}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{body=locate\_latest\_post\_body($\mathbf{s})$}}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{must\_include}(URL}, \textcolor{red}{``/f/nyc''})}} \\
      & & & \multirow{1}{*}{\shortstack{\texttt{\textcolor{blue}{must\_include}(body,}\textcolor{red}{``a car in NYC''}})} \\
      \bottomrule
      \end{tabular}%
    }
    \caption{We introduce two evaluation approaches. $r_{\textrm{info}}$ (top) measures the correctness of performing information-seeking tasks. It compares the predicted answer $\hat{a}$ with the annotated reference $a^*$ with three implementations.
    $r_{\textrm{prog}}$ (bottom) programmatically checks whether the intermediate states during the executions possess the anticipated properties specified by the intent. %
    }
    \label{tab:evaluation_examples}
\end{table}

\paragraph{Unachievable Tasks} Due to constraints such as inadequate evidence, user permissions (\S\ref{sec:user_role}), or the absence of necessary functional support on the website, humans may ask for tasks that are not possible to complete.
Inspired by previous work on evaluating question-answering models on unanswerable questions~\citep{rajpurkar2018know}, we design unachievable tasks in \ours. 
For instance, fulfilling an intent like \sent{Tell me the contact number of OneStopShop} is impracticable in \ours, given that the website does not provide such contact information. 
We label such instances as "N/A" and expect an agent to produce an equivalent response. 
These examples allow us to assess an agent's ability to avoid making unfounded claims and its adherence to factual accuracy.

\paragraph{Annotation Process} The intents were contributed by the authors following the annotation guideline in \S\ref{sec:intent_collection}. Every author has extensive experience with web-based tasks. 
The reference answers to the information-seeking tasks were curated by the authors and an external annotator. To ensure consistency and accuracy, each question was annotated twice. If the two annotators disagreed, a third annotator finalized the annotation.
The programs to evaluate the remaining examples were contributed by three of the authors who are proficient in JavaScript programming. 
Difficult tasks were often discussed collectively to ensure the correctness of the annotation.
The annotation required the annotator to undertake the full execution and scrutinize the intermediate states.

\begin{wraptable}[5]{r}{0.25\linewidth}
    \vspace{-10pt}
    \footnotesize
    \begin{tabular}{@{}l@{\hspace{5pt}}c@{}}
        \toprule
        Avg. Time  & 110s\\
        Success Rate$_\textrm{info}$ & 74.68\%\\
        Success Rate$_\textrm{others}$ & 81.32\% \\
         Success Rate$_\textrm{all}$ & 78.24\% \\
        \bottomrule
    \end{tabular}
    \label{tab:human_peformance}
\end{wraptable}

\paragraph{Human Performance} We sample one task from each of the 170 templates and ask five computer science graduate students to perform these tasks. The human performance is on the right. 
Overall, the human annotators complete 78.24\% of the tasks, with lower performance on information-seeking tasks. 
Through examining the recorded trajectories, we found that 50\% of the failures are due to misinterpreting the intent (\eg providing travel distance when asked for travel time), incomplete answers (\eg providing only name when asked for name and email), and incomplete executions (\eg partially filling the product information), while the remaining instances have more severe failures, where the executions are off-target.


\section{Baseline Web Agents}\label{sec:baseline_agents}
We experiment with three LLMs using two prompting strategies, both with two examples in the context. In the first setting, we ask the LLM to directly predict the next action given the current observation, the intent and the previously performed action. 
In the second setting, with the same information, the model first performs chain-of-thought reasoning steps in the text before the action prediction~(CoT,~\citet{wei2022chain, yao2022react}). 
Before the examples, we provide a detailed overview of the browser environment, the allowed actions, and many rules.
To make the model aware of the unachievable tasks, the instruction explicitly asks the agent to stop if it believes the task is impossible to perform. We refer to this directive as Unachievable hint, or \textbf{UA hint}.
This introduction is largely identical to the guidelines we presented to human annotators to ensure a fair comparison.
We use an accessibility tree with element IDs as the observation space.
The agent can identify which element to interact with by the ID of the element. For instance, the agent can issue \texttt{click [1582]} to click the ``Add to Cart'' button with the ID of 1582.
The full prompts can be found in Appendix \ref{app:agent_prompts}.
The detailed configurations of each model can be found in Appendix \ref{sec:exp_configs}.


\section{Results}\label{sec:results}
\subsection{Main Results}\label{sec:main_results}
\begin{wraptable}{r}{0.53\linewidth}
    \vspace{-13pt}
    \footnotesize
    \begin{tabular}{@{}c@{\hspace{3pt}}c@{}l@{\hspace{5pt}}ccc@{}}
        \toprule
        \textbf{CoT} & \textbf{UA Hint} & \multicolumn{1}{c}{\textbf{Model}} & \textbf{SR} & \textbf{SR$_\textrm{AC}$} & \textbf{SR$_\textrm{UA}$} \\
        \midrule
        \cmark & \cmark & \scriptsize \textsc{text-bison-001} &  5.05 & 4.00 & 27.78 \\
        \xmark & \cmark & \scriptsize \textsc{GPT-3.5} & 6.41 & 4.90 & 38.89\\
        \cmark & \cmark & \scriptsize \textsc{GPT-3.5} & 8.75 & 6.44 & 58.33 \\
        \cmark & \cmark & \scriptsize \textsc{GPT-4} & 11.70 & 8.63 & \textbf{77.78} \\ 
        \midrule
        \xmark & \xmark & \scriptsize\textsc{GPT-3.5} &   5.10 & 4.90 & 8.33 \\
        \cmark & \xmark & \scriptsize\textsc{GPT-3.5} &  6.16 & 6.06 & 8.33\\
        \cmark & \xmark & \scriptsize\textsc{GPT-4} & \textbf{14.41} & \textbf{13.02} & 44.44 \\ 
        \midrule
        
        - & \cmark & Human & 78.24  & 77.30 & 100.00 \\
        \bottomrule
    \end{tabular}
    \caption{The end-to-end task success rate (SR \%) on \ours with different prompting strategies. \textbf{CoT}: the model performs step-by-step reasoning before issuing the action. \textbf{UA hint}: ask the model to stop when encountering unachievable questions.}
    \label{tab:main_results}
\end{wraptable}

The main results are shown on the top of \autoref{tab:main_results}. 
\textsc{GPT-4}~\citep{openai2023gpt} with CoT prompting achieves a modest end-to-end task success rate of 11.70\%, which is significantly lower than the human performance of 78.24\%. 
\textsc{GPT-3.5}~\citep{chatgpt} with CoT prompting is only able to successfully perform 8.75\% of the tasks. 
The explicit reasoning procedure is somewhat helpful, it brings 2.34\% improvement over the version without it. 
Further, \textsc{text-bison-001}~\citep{anil2023palm} 
 underperforms \textsc{GPT-3.5}, with a success rate of 5.05\%. 
These results underline the inherent challenges and complexities of executing tasks that span long horizons, particularly in realistic environments such as \ours. %



\subsection{Analysis}\label{sec:analysis}
\paragraph{Do models know when to stop?}  
In our error analysis of the execution trajectories, we observe a prevalent error pattern of early stopping due to the model's conclusion of unachievability. 
For instance, \textsc{GPT-4} erroneously identifies 54.9\% of feasible tasks as impossible. 
This issue primarily stems from the UA hint in the instruction, while this hint allows models to identify unachievable tasks, it also hinders performance on achievable tasks.
To address this, we conduct an ablation study where we remove this hint. We then break down the success rate for both achievable and unachievable tasks.
As shown in \autoref{tab:main_results}, eliminating this instruction led to a performance boost in achievable tasks, enhancing the overall task success rate of \textsc{GPT-4} to 14.41\%.
Despite an overall decline in identifying unachievable tasks, \textsc{GPT-4} retains the capacity to recognize 44.44\% of such tasks.
It does so by generating \emph{reasons of non-achievability}, even without explicit instructions.
On the other hand, \textsc{GPT-3.5} rarely exhibits this level of reasoning. 
Instead, it tends to follow problematic patterns such as hallucinating incorrect answers, repeating invalid actions, or exceeding the step limits.
This result suggests that even subtle differences in instruction design can significantly influence the behavior of a model in performing interactive tasks in complex environments.


\begin{wraptable}[10]{r}{0.29\linewidth}
    \vspace{-10pt}
    % Figure removed
    \vspace{-20pt}
    \caption{Distribution of success rates on templates with $ \geq 1$ successful executions on \textsc{gpt} models (no UA hint).}\label{fig:success_template}
\end{wraptable}

\paragraph{Can a model maintain consistent performance across similar tasks?} Tasks that originate from the same template usually follow similar reasoning and planning processes, even though their observations and executions will differ.
We plot a histogram of per-template success rates for our models in \autoref{fig:success_template}. 
Of the 61 templates, \textsc{GPT-4} manages to achieve a 100\% task success rate on only four templates, while \textsc{GPT-3.5} fails to achieve full task completion for any of the templates. In many cases, the models are only able to complete one task variation with a template.  
These observations indicate that even when tasks are derived from the same template, they can present distinct challenges. 
For instance, while \sent{Fork metaseq} can be a straightforward task, \sent{Fork all repos from Facebook} derived from the same template requires more repetitive operations, hence increasing its complexity.
Therefore, \ours provide a testbed to evaluate more sophisticated methods. In particular, those that incorporate memory components, enabling the \emph{reuse} of successful strategies from past experiments \cite{zhou2021hierarchical, wang2023voyager}.
More error analysis with examples can be found in Appendix \ref{sec:additional_error_analysis}.



\section{Related Work}
\begin{table}[t!]
   \centering
    \small
    \begin{tabular}{l@{\hspace{3pt}}lcccc}
    \toprule
    \multicolumn{2}{c}{\multirow{2}{*}{\shortstack{Benchmark}}} & \multirow{2}{*}{\shortstack{Dynamic \\ Interaction?}} & \multirow{2}{*}{\shortstack{Realistic \\ Environment?}} & \multirow{2}{*}{\shortstack{Diverse \\ Human Tasks?}} & \multirow{2}{*}{\shortstack{Functional \\ Correctness?}}  \\
    & \\
    \midrule
    Mind2Web&\citep{deng2023mind2web} & \xmark & \cmark & \cmark & \xmark \\
    Form/QAWoB&\citep{shi2017world} & \xmark & \cmark & \cmark & \xmark  \\
    MiniWoB++&\citep{liu2018reinforcement} & \cmark & \xmark & \xmark & \cmark \\
    Webshop&\citep{yao2022webshop} & \cmark & \xmark & \xmark & \cmark \\
    ALFRED&\citep{shridhar_alfred:_2019} & \cmark & \xmark & \xmark & \cmark \\
    VirtualHome&\citep{puig_virtualhome:_2018} & \xmark & \xmark & \cmark & \xmark \\
    AndroidEnv&\citep{toyama2021androidenv} & \cmark & \cmark & \xmark & \xmark \\
    \midrule
    \multicolumn{2}{c}{\ours} & \cmark & \cmark & \cmark & \cmark \\
    \bottomrule    
    \end{tabular}
    \caption{The comparison between our benchmark and existing benchmarks on grounding natural language instructions to concrete executions. Our benchmark is implemented in our fully interactable highly-realistic \ environment. It features diverse tasks humans may encounter in their daily routines. We design evaluation metrics to assess the functional correctness of task executions.}
    \label{tab:comparision}
\end{table}
\paragraph{Benchmarks for Controlling Agents through Natural Language} 
Controlling agents via natural language in the digital world have been studied in the literature~\citep{branavan2009reinforcement,shi2017world,liu2018reinforcement, toyama2021androidenv,deng2023mind2web,li2020mapping,xu2021grounding}. 
However, the balance between \emph{functionality}, \emph{authenticity}, and \emph{support for environmental dynamics} remains a challenge. 
Existing benchmarks often compromise these aspects, as shown in \autoref{tab:comparision}. 
Some works rely on static states, limiting agents' explorations and functional correctness evaluation~\citep{shi2017world,deng2023mind2web}, while others simplify real-world complexities, restricting task variety~\citep{yao2022webshop,liu2018reinforcement}.
While AndroidEnv~\citep{toyama2021androidenv} replicates an Android setup, it doesn't evaluate complex and realistic tasks. This gap is also seen in synthetic environments~\citep{ai2thor, shridhar_alfred:_2019, puig_virtualhome:_2018} and extends to gaming environments~\citep{fan2022minedojo, kuttler_nethack_2020}, where the environment mechanisms often diverge from human objectives.


\paragraph{Interactive Decision-Making Agents} 
\cite{nakano2021webgpt} introduce WebGPT which searches the web and reads the search results to answer questions. \cite{gur2023real} propose a web agent that decomposes tasks into more manageable sub-tasks and synthesizes Javascript code for the task executions. Adding a multi-modal dimension, \cite{lee2023pix2struct} and \cite{shaw2023pixels} develop agents that predict actions based on screenshots of web pages rather than relying on the text-based DOM trees.
Performing tasks in interactive environments requires the agents to exhibit several capabilities including hierarchical planning, state tracking, and error recovery.
Existing works~\citep{huang2022language,madaan2022language,li2023take} observe LLMs could break a task into more manageable sub-tasks~\citep{zhou2022show}.
This process can be further refined by representing task executions as programs, a technique that aids sub-task management and skill reuse~\citep{zhou2021hierarchical,liang2023code,wang2023voyager,gao2023pal}. Meanwhile, search and backtracking methods introduce a more structured approach to planning while also allowing for decision reconsideration~\citep{yao2023tree,long2023large}. 
Existing works also incorporate failure recovery, self-correction~\citep{shinn2023reflexion,kim2023language}, observation summarization~\citep{sridhar2023hierarchical} to improve execution robustness.
This complexity of \ours presents a unique challenge and opportunity for further testing and improvement of these methods.

\section{Conclusion}
We present \ours, a highly-realistic, standalone, and reproducible web environment designed for the development and testing of autonomous agents. \ours includes fully functional web applications and genuine data from four major categories, providing a realistic platform for agent interaction. It further supports a wide range of tools and external knowledge bases, fostering a focus on human-like problem-solving.
Additionally, we curate a comprehensive benchmark consisting of 812 examples that focus on translating high-level natural language intents into specific web interactions. We also offer metrics to programmatically ascertain whether tasks have been completed according to the desired objectives.
Our experiments show that even \textsc{GPT-4} only achieves a limited end-to-end task success rate of 14.41\%, significantly lagging behind the human performance of 78.24\%. These findings underscore the need for future research to focus on enhancing the robustness and efficacy of autonomous agents within \ours environment. 

\section*{Acknowledgement}
We would like to thank Emmy Liu, Zhiruo Wang, Zhitong Guo for examining our annotations, Shunyu Yao for providing the raw Amazon product data in Webshop, Pengfei Liu, Zaid Sheikh and Aman Madaan for the helpful discussions.
This material is partly based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government. This project was also partially supported by a gift from AWS AI.
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}




\clearpage

\appendix
\input{appendix}

\end{document}
