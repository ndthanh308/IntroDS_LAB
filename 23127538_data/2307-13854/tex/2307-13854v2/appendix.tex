\section{Appendix}

\subsection{Website Implementation}\label{app:site_implementation}
Given the selected websites described in \S\ref{sec:website_selection}, we make the best attempt to reproduce the functionality of commonly used sites in a reproducible way. 
To achieve this, we utilized open-source frameworks for the development of the websites across various categories and imported data from their real-world counterparts.
For the E-commerce category, we constructed a shopping website with approximately 90$k$ products, including the prices, options, detailed product descriptions, images, and reviews, spanning over 300 product categories. This website is developed using Adobe Magento, an open-source e-commerce platform\footnote{\url{https://github.com/magento/magento2}}. Data resources were obtained from data from actual online sites, such as that included in the Webshop data dump\cite{yao2022webshop}.
As for the social forum platform, we deployed an open-source software Postmill\footnote{\url{https://postmill.xyz/}}, the open-sourced counterpart of Reddit\footnote{\url{https://www.reddit.com/}}.
We sampled from the top 50 subreddits\footnote{\url{https://redditlist.com/sfw.html}}. 
We then manually selected many subreddit for northeast US cities as well as subreddit for machine learning and deep learning-related topics. This manual selection encourages cross-website tasks such as seeking information related to the northeast US on both Reddit and the map. 
In total, we have 95 subreddits, 127390 posts, and 661781 users. 
For the collaborative software development platform, we choose GitLab\footnote{\url{https://gitlab.com/gitlab-org/gitlab}}.
We heuristically simulate the code repository characteristics by sampling at least ten repositories for every programming language: $80\%$ of them are sampled from the set of top $90$ percentile wrt stars repos using a discrete probability distribution weighted proportional to their number of stars; the remaining are sampled from the bottom ten percentile set using similar weighted distribution. This is done to ensure fair representation of repos of all kinds, from popular projects with many issues and pull requests to small personal projects. In total, we have 300 repositories and more than 1000 accounts with at least one commit to a repository.
For the content management system, we adapted Adobe Magento's admin portal, deploying the sample data provided in the official guide.
We employ OpenStreetMap\footnote{\url{https://www.openstreetmap.org/}} for map service implementation, confining our focus to the northeast US region due to data storage constraints.
We implement a calculator and a scratchpad ourselves.

Lastly, we configure the knowledge resources as individual websites, complemented with search functionality for efficient information retrieval.
Specifically, we utilize Kiwix\footnote{\url{https://www.kiwix.org/en/}} to host an offline version of English Wikipedia with a knowledge cutoff of May 2023. 
The user manuals for GitLab and Adobe Commerce Merchant documentation are scraped from the official websites.%

\subsection{Environment Delivery and Reset}\label{app:env_reset}
One goal for our evaluation environment is ease of use and reproducibility.
As a result, we deploy our websites in separate Docker images~\footnote{\url{https://www.docker.com/}}, one per website.
The Docker images are fully self-contained with all the code of the website, database, as well as any other software dependencies.
They also do not rely on external volume mounts to function, as the data of the websites are also part of the docker image.
This way, the image is easy to distribution containing all the pre-populated websites for reproducible evaluation.
End users can download our packaged Docker images and run them on their systems and re-deploy the exact websites together with the data used in our benchmarks for their local benchmarking.

Since some evaluation cases may require the agent to modify the data contained in the website, \eg creating a new user, deleting a post, etc., it is crucial to be able to easily reset the website environment to its initial state.
With Docker images, the users could stop and delete the currently running containers for that website and start the container from our original image again to fully reset the environment to the initial state.
Depending on the website, this process may take from a few seconds to one minute.
However, not all evaluation cases would require an environment reset, as many of the intents are information gathering and are read-only for the website data.
Also, combined with the inference time cost for the agent LLMs, we argue that this environment reset method, through restarting Docker containers from the original images, will have a non-negligible but small impact on evaluation time.

\subsection{User Roles Simulation}\label{sec:user_role}
Users of the same website often have disparate experiences due to their distinct \emph{roles}, \emph{permissions}, and \emph{interaction histories}. 
For instance, within an E-commerce CMS, a shop owner might possess full read and write permissions across all content, whereas an employee might only be granted write permissions for products but not for customer data. We aim to emulate this scenario by generating unique user profiles on each platform.

On the shopping site, we created a customer profile that has over 35 orders within a span of two years. 
On GitLab, we selected a user who maintains several popular open-source projects with numerous merge requests and issues. This user also manages a handful of personal projects privately. 
On Reddit, our chosen profile was a user who actively participates in discussions, with many posts and comments. 
Lastly, on our E-commerce CMS, we set up a user profile for a shop owner who has full read-and-write access to all system contents.%

All users are automatically logged into their accounts using a pre-cached cookie. To our best knowledge, this is the first publicly available agent evaluation environment to implement such a characteristic. Existing literature typically operates under the assumption of universally identical user roles~\cite{shi2017world,liu2018reinforcement,deng2023mind2web}. %
\subsection{Intent Distribution}
The distribution of intents across the websites are shown in \autoref{fig:intent_distribution}.
% Figure environment removed

\subsection{Experiment Configurations}\label{sec:exp_configs}
We experiment with \textsc{gpt-3.5-turbo-16k-0613}, \textsc{gpt-4-0613}, and \textsc{text-bison-001} with a temperature of $1.0$ and a top-$p$ parameter of $0.9$. The maximum number of state transitions is set to 30.
We halt execution if the same action is repeated more than three times on the same observation or if the agent generates three consecutive invalid actions. These situations typically indicate a high likelihood of execution failure and hence warrant early termination. For \textsc{text-bison-001}, we additionally allow ten retries until it generates a valid action.

Primarily, we use a high temperature of 1.0 to encourage the \emph{exploration}. To aid replicating the results, we provide the results of \textsc{gpt-3.5-turbo-16k-0613} with temperature 0.0 in \autoref{tab:temp_zero_results} and the execution trajectories in our code repository.

\begin{table}
    \centering
    \begin{tabular}{@{}c@{\hspace{3pt}}c@{}l@{\hspace{5pt}}c@{}}
        \toprule
        \textbf{CoT} & \textbf{UA Hint} & \multicolumn{1}{c}{\textbf{Model}} & \textbf{SR}  \\
        \midrule
        \cmark & \xmark & \scriptsize\textsc{GPT-3.5} &  6.28 \\
        \bottomrule
    \end{tabular}
    \caption{The task success rate (SR \%) of \textsc{gpt-3.5-turbo-16k-0613} with temperature 0.0.}
    \label{tab:temp_zero_results}
\end{table}



\subsection{Prompt for \texttt{fuzzy\_match}}\label{app:prompt_fuzzy_match}
\fbox{
\parbox{\textwidth}{
Help a teacher to grade the answer of a student given a question. Keep in mind that the student may use different phrasing or wording to answer the question. The goal is to evaluate whether the answer is semantically equivalent to the reference answer.

question: \{\{intent\}\}

reference answer: \{\{reference answer\}\}

all the string 'N/A' that you see is a special sequence that means 'not achievable'

student answer: \{\{prediction\}\}

Conclude the judgement by correct/incorrect/partially correct.
}}

Predictions that are judged as ``correct'' will receive a score of one, while all other predictions will receive a score of zero.


\subsection{The Prompts of the Baseline Web Agents}\label{app:agent_prompts}
The system message of the reasoning agent for both GPT-3.5 and \textsc{gpt-4} is in \autoref{fig:reasoning_prompt}, and two examples are in \autoref{fig:reasoning_examples}. The system message of the direct agent for GPT-3.5 is in \autoref{fig:direct_prompt} and the two examples are in \autoref{fig:direct_example}.
\textbf{UA hint} refers to the instruction of `` If you believe the task is impossible to complete, provide the answer as "N/A" in the bracket.''. We remove this sentence in our ablation studies.

% Figure environment removed

% Figure environment removed



% Figure environment removed

% Figure environment removed

\subsection{Additional Error Analysis}\label{sec:additional_error_analysis}
% Figure environment removed

\paragraph{Observation Bias} Realistic websites frequently present information on similar topics across various sections to ensure optimal user accessibility. However, a \textsc{gpt-4} agent often demonstrates a tendency to latch onto the first related piece of information it encounters without sufficiently verifying its relevance or accuracy. 
For instance, the homepage of the E-Commerce CMS displays the best-selling items based on \emph{recent purchases}, while historical best-seller data is typically accessed via a separate report. Presented with the task of \sent{What is the top-1 best-selling product in 2022}, the \textsc{gpt-4} agent defaults to leveraging the readily available information on the homepage, bypassing the necessary step of generating the report to obtain the accurate data.

\paragraph{Failures in Observation Interpretation} Interestingly, while \textsc{gpt-4} is capable of summarizing the observations, it occasionally overlooks more granular information, such as the previously entered input. As in the right-hand example of \autoref{fig:additional_error}, \texttt{[5172] StaticText} indicates that the search term ``DMV area'' has already been entered. However, the agent disregards this detail and continuously issues the command \texttt{type [2430] [DMV area]} until it reaches the maximum step limit. Furthermore, the agent often neglects the previous action information that is provided alongside the observation.

We hypothesize that these observed failures are related to the current pretraining and supervised fine-tuning on dialogues employed in GPT models~\cite{ouyang2022training}. These models are primarily trained to execute instructions given \emph{immediate} observations (\ie, the dialogue history); thereby, they may exhibit a lack of explorations. 
Furthermore, in dialogue scenarios, subtle differences in NL expressions often have less impact on the overall conversation. As a result, models may tend to overlook minor variations in their observations.


