\subsection{Proof of Theorem~\ref{thm:robinson}}\robin* 
\label{Alternative_proof}
Here,
\begin{equation*}
\hat\bbeta_{\gls} = \argmin_{\bbeta} (Y-X\bbeta)\mathcal{V}^{-1}(Y -X\bbeta)=(X^{\tran}\mathcal{V}^{-1}X)^{-1}X^{\tran}\mathcal{V}^{-1}Y \tag{\ref{eq:beta_gls}}
\end{equation*}
\begin{proof}
We rewrite the model as
\[Y=X \bbeta + Z U + \varepsilon \, \, \text{ where, } U \sim \mathcal{N}(0, \Sigma_u), \text{ and } \epsilon \sim \mathcal{N}(0, \sigma^2_e I)\]
Then,
\[P_r(Y = y, U=u) = P_r(Y = y) P_r(U=u| Y =y) = P_r(Y =y|U = u)P_r(U = u) \]
$ P_r(Y = y)$ corresponds to the likelihood in $(\ref{eq:likelihood})$ and $\hat\bbeta_{\gls}$ in $(\ref{eq:beta_gls})$, and $ P_r(Y =y|U = u) P_r(U = u)$ corresponds to the exponent of $(\ref{eq:loss}$). Now,
\begin{align*} P_r & (U=u|Y = y) \\
& = \frac{1}{\sqrt{|Var(U|y)|}} \exp \left(-\frac{(u-\Ex[U|y])^{\tran} \left[Var(U|y)\right]^{-1} (u-\Ex[U|y])}{2} \right) \end{align*}
which is maximized for $u=\Ex[U|Y = y]$, and
$$\max_{u} P_r(U=u|Y = y) = \frac{1}{(2\pi)^{(p+1)/2}\sqrt{|Var(U|y)|}} = \mbox{a constant }$$
\[\max_{u} P_r(y, U=u) = \max_{u} P_r(Y = y) P_r(U=u|Y = y) = \mbox{ constant } \times p(y)\]
Therefore,
\[\argmax_{\bbeta} \max_{u} P_r(Y = y, U=u) = \argmax_{\bbeta} P_r(Y = y) \frac{1}{\sqrt{|Var(U|y)|}}\]
It is understood that $Var(U|y)$ is constant in terms of $\bbeta$, which implies,
\[\argmax_{\bbeta} \max_{u} P_r(Y = y, U=u) = \argmax_{\bbeta} P_r(Y = y) \]
which is equivalent to saying that
$$\hat\bbeta = \hat\bbeta_{\gls}$$
\end{proof}
\subsection{Consistency of \texorpdfstring{$\hat{\bbeta}_{\ols}$}{beta ols}}
\betaols*
\begin{proof}
To begin with, we assume the observations are arranged by the level of the first factor.
$Y = X\bbeta + \varepsilon$ where $\varepsilon \sim N(0,\mathcal{V})$ for
\begin{align*}
\mathcal{V}& = \underbrace{diag(X_{1}^{(a)}\Sigma_{a}X_{1}^{(a) \tran},\cdots,X_{R}^{(a)}\Sigma_{a}X_{R}^{(a) \tran})}_{B_{R}} \\
& \hspace{2cm}+ P_{C} \underbrace{diag(X_{1}^{(b)}\Sigma_{b}X_{1}^{(b) \tran},\cdots,X_{C}^{(b)}\Sigma_{b}X_{C}^{(b)\tran})}_{B_{C}} P_{C}^{\tran} + \sse\bf{I}_{N}
\end{align*}

with $P_{C}$ being an appropriate permutation matrix. Note that $\hat{\bbeta}_{\ols} = \bbeta + (X^{\tran}X)^{-1}X^{\tran}\varepsilon.$ $\Ex((X^{\tran}X)^{-1}X^{\tran}\varepsilon) = \bf{0}.$
Let $w$ be any $p \times 1$ vector with finite norm.
\begin{align*}
\mathrm{Var}(w^{\tran}(X^{\tran}X)^{-1}X^{\tran}\varepsilon) &= w^{\tran}(X^{\tran}X)^{-1}X^{\tran}\mathcal{V}X(X^{\tran}X)^{-1}w\\
&\leq \lambda_{\max}(B_{R}) w^{\tran}(X^{\tran}X)^{-1}w \\
& \hspace{0.2cm}+ \lambda_{\max}(P_CB_{C}P_C^{\tran}) w^{\tran}(X^{\tran}X)^{-1}w + \sse w^{\tran}(X^{\tran}X)^{-1}w \\
&\leq \lambda_{\max}(B_{R}) \frac{\Vert w \Vert^{2}}{cN} + \lambda_{\max}(P_CB_{C}P_C^{\tran}) \frac{\Vert w \Vert^{2}}{cN} + \sse \frac{\Vert w \Vert^{2}}{cN}\\
&\leq \lambda_{\max}(B_{R}) \frac{\Vert w \Vert^{2}}{cN} + \lambda_{\max}(B_{C}) \frac{\Vert w \Vert^{2}}{cN} + \sse \frac{\Vert w \Vert^{2}}{cN} \to 0
\end{align*}
So $\hat{\bbeta}_{\ols}$ is consistent estimator for $\bbeta$.
\end{proof}
\subsection{Method of Moments}\label{appendix.mom}
\mmconsis*
\begin{proof}
Each of the equations (\ref{eq:mm1}), (\ref{eq:mm2}), and (\ref{eq:mm3}) can be represented as
\begin{equation}\Ex \bigg[ r^{\tran} (I - Q) r\bigg] = \hat r^{\tran} (I - Q) \hat r \text{ for some } Q \in \mathbb{R}^{N \times N}.
\label{eq:matrix_form}
\end{equation}
For example, $Q$ for $(\ref{eq:mm1})$ is given by
\begin{equation}
\label{eq:Q}
Q = \begin{bmatrix}
\mathlarger{\frac{{x^{(s)}_{1.}}x^{(s)^{\tran}}_{1.}}{\|{x^{(s)}_{1.}}\|^2}} & 0 & \cdots & 0 \\
0 & \mathlarger{\frac{{x^{(s)}_{2.}}x^{(s)^{\tran}}_{2.}}{\|{x^{(s)}_{2.}}\|^2}} & \cdots & 0 \\
0 & 0 & \ddots & 0\\
0 & 0 & \cdots & \mathlarger{\frac{{x^{(s)}_{R.}}x^{(s)^{\tran}}_{R.}}{\|{x^{(s)}_{R.}}\|^2}}\\
\end{bmatrix}
\end{equation}
It is sufficient to show that $(\Ex[r^{\tran}(I-Q)r|X]- \hat{r}^{\tran}(I-Q) \hat{r})/N$ converges to zero $ \forall Q \in \{P_1, \cdots, P_p\}$, $\{P'_1, \cdots, P'_p\}, J_n $ and  $\forall \, \, \Sigma_a, \Sigma_b, \sigma^2_e $ where $J_n = 1_n 1_n^{\tran} / n$.
\begin{align*}
\frac{\hat{r}^{\tran}(I-Q) \hat{r}}{N} & =\frac{ (\hat{r}-r+r)^{\tran}(I-Q) (\hat{r} - r + r)}{N}\\
& = \frac{r^{\tran}(I-Q)r + (\hat{r}-r)^{\tran}(I-Q) (\hat{r} - r) + 2 r^{\tran} (I-Q) (\hat{r} - r)}{N} \\
& = \frac{r^{\tran}(I-Q)r}{N} + \frac{(\hat{\bbeta}_{\ols}-\bbeta)^{\tran} X^{\tran}(I-Q) X (\hat{\bbeta}_{\ols} - \bbeta)}{N}\\
& \hspace{5cm} - \frac{2 r^{\tran} (I-Q) X (\hat{\bbeta}_{\ols} - \bbeta)}{N}
\end{align*}
As $\mathbf{x}_{ij} $ have finite fourth moment $\forall i \in \{1, \cdots, R\} \, j \in \{1, \cdots, C\} $, $X^{\tran} X /N = O_p(1) $, and as $\hat{\bbeta}_{\ols}$ is consistent, second and third terms converge to zero in probability.
Now,
\begin{align*}
\frac{(\hat{r}^{\tran}(I-Q) \hat{r}-\Ex[r^{\tran}(I-Q)r|X])}{N} & = \frac{r^{\tran}(I-Q)r}{N} - \frac{\Ex[r^{\tran}(I-Q)r|X])}{N} + o_p(1)
\end{align*}
Now, \begin{equation} \frac{r^{\tran}(I-Q)r - \Ex[r^{\tran}(I-Q)r]}{N} \to 0 \end{equation}
and
\begin{equation} \frac{\Ex[r^{\tran}(I-Q)r|X] - \Ex[r^{\tran}(I-Q)r]}{N} \to 0 \end{equation}
by the strong law of large numbers assuming finite fourth moment of $\mathbf{x}_{ij} $ $\forall i \in \{1, \cdots, R\} $ and $ j \in \{1, \cdots, C\} $.
Let, $\btheta = (diag(\Sigma_a),diag(\Sigma_b),\sigma^2_e)$, then method of moment estimate, $\btheta_n $ is solution of equations in the form $A_n \btheta = B_n$, i.e. , $\btheta_n = A_n^{-1} B_n$. The above proof guarantees that $A_n \to A$ and $B_n \to A \btheta, \, \, \forall \btheta$. Therefore, $\btheta_n \to \btheta$.
\end{proof}
\subsection{Proof of~(\ref{eq:simplify})}
We know from (\ref{eq:random_slopes}) that
\[r_{ij} = \mathbf{x}_{ij} ^{\tran} (\bsa_i + \bsb_j) + \varepsilon_{ij}\]
It could also be represented as
\[
r = \sum_{s'=0}^{p} X_A^{(s')} A_{s'} + \sum_{s'=0}^{p} X_B^{(s')} B_{s'} + \varepsilon.
\]
Here, $A_{s'} \in \mathbb{R}^R$ and $B_{s'} \in \mathbb{R}^C$ represent the stacked random coefficients of the $s'$-th coordinate for clients and items, respectively, and $X_A^{(s')}$ and $X_B^{(s')}$ denote the corresponding design matrices. Then, we have
\begin{align*}
&\Ex\bigg[\sum_{i=1}^{R} \sum_{j=1}^{C} z_{ij} r^2_{ij}-\sum_{i=1}^{R}\frac{\left(\sum_{j=1}^{C} z_{ij} r_{ij}x^{(s)}_{ij}\right)^2}{\sum_{j=1}^{C} z_{ij}x^2_{ij(s)}} \bigg{|} X\bigg] = \Ex[r^{\tran}(I-Q)r]\\
& = \Ex\left(\sum_{s'=0}^{p}X_A^{(s')}A_{s'}+\sum_{s'=0}^{p}X_B^{(s')}B_{s'} + \varepsilon\right)^{\tran}\left(I -Q\right)\\
& \hspace{3cm}\cdot\left(\sum_{s'=0}^{p}X_A^{(s')}A_{s'}+\sum_{s'=0}^{p}X_B^{(s')}B_{s'} + \varepsilon\right)\\
& = \sum_{s'=0}^{p} E\bigg[(X_A^{(s')}A_{s'})^{\tran}(I-Q)(X_A^{(s')}A_{s'})\bigg]\\
& \hspace{1cm}+\sum_{s'=0}^{p} E\bigg[(X_B^{(s')}B_s')^{\tran}(I-Q)(X_B^{(s')}B_{s'})\bigg] + \sigma^2_e (n-R)\\
& = \sum_{s'=0}^{p}\tr\bigg[X_A^{\left(s'\right)^{\tran}}\left(I-Q\right)X_A^{\left(s'\right)}\bigg]\Sigma_{a,s's'} +\sum_{s'=0}^{p} \tr\bigg[X_B^{\left(s'\right)^{\tran}}\left(I-Q\right)X_B^{\left(s'\right)}\bigg]\Sigma_{b,s's'}\\
& \hspace{9.5cm} + \sigma^2_e (n-R)\\
&=\sum_{s'=0}^{p} \sum_{i=1}^{R}\left[ \sum_{j=1}^{C} z_{ij} x^{(s')^2}_{ij}-\frac{\left(\sum_{j=1}^{C} z_{ij} x^{(s)}_{ij} x^{(s')}_{ij} \right)^2}{\sum_{j=1}^{C} z_{ij} x^{(s)^2}_{ij}}\right]\Sigma_{a,s's'} \\
& \hspace{1cm}+\sum_{s'=0}^{p}\sum_{i=1}^R\left[ \sum_{j=1}^{C}z_{ij} x^{(s')^2}_{ij} -\frac{\sum_{j=1}^{C}z_{ij} x^{(s)^2}_{ij} x^{(s')^2}_{ij}}{\sum_{j=1}^{C} z_{ij} x^{(s)^2}_{ij}}\right]\Sigma_{b,s's'} + \sigma^2_e (n-R)\\
\end{align*}
The first part of the last equality follows directly from the definition of $Q$, the last part could be shown using the following argument:
\\
Let, \[X_B^{(s')} = \begin{bmatrix}
W_1\\
W_2\\
\vdots\\
W_R
\end{bmatrix}\]
\begin{align*}
\tr\bigg[\left(X_B^{(s')}\right)^{\tran}(I-Q)\left(X_B^{(s')}\right)\bigg] & = \sum_{i=1}^R \tr\bigg[W^{\tran}_i \left(I - \frac{x^{(s)}_{i.}{x^{(s)}_{i.}}^{\tran}}{\|x^{(s)}_{i.}\|^2}\right)W_i\bigg]\\
& = \sum_{i=1}^R \tr(W^{\tran}_i W_i) - \frac{\tr\left(x^{(s)^{\tran}}_{i.} W_i W_i ^{\tran} x^{(s)}_{i.}\right)}{\|x^{(s)}_{i.}\|^2}
\end{align*}
Now,
\[W_i W^{\tran}_i = \begin{bmatrix}
{x^{(s')}_{i1}}^2 & 0 & \cdots & 0\\
0 & {x^{(s')}_{i2}}^2 & \cdots & 0\\
0 & 0 & \ddots & 0\\
0 & 0 & 0 & {x^{(s')}_{in_i}}^2\\
\end{bmatrix}\]
Thus,
\[\tr\bigg[\left(X_B^{(s')}\right)^{\tran}(I-Q)\left(X_B^{(s')}\right)\bigg] = \sum_{i=1}^R\left[ \sum_{j=1}^{C}z_{ij} x^{(s')^2}_{ij} -\frac{\sum_{j=1}^{C}z_{ij} x^{(s)^2}_{ij} x^{(s')^2}_{ij}}{\sum_{j=1}^{C} z_{ij} x^{(s)^2}_{ij}}\right]\]
The additional equation we added was given by
\[\hat{r}^{\tran}\bigg(I - \frac{1}{n}11^{\tran}\bigg) \hat{r} = E\bigg[r^{\tran}\bigg(I - \frac{1}{n}11^{\tran}\bigg)r\bigg]\]
L.H.S. of the above equation will be given by
\[\sum_{i=1}^R \sum_{j=1}^{C} z_{ij}\hat{r}^2_{ij}- \frac{1}{n}\left(\sum_{i=1}^R\sum_{j=1}^{C}z_{ij}\hat{r}_{ij}\right)^2\]
For R.H.S., similar to above, we will need the following expressions:
\begin{align*}\tr\bigg[\left(X_A^{(s)}\right)^{\tran}\bigg(I-\frac{1}{n} & 11^{\tran}\bigg) \left(X_A^{(s')}\right)\bigg] \\
& = \sum_{i=1}^{R}\bigg( \sum_{j=1}^{C}z_{ij} x^{(s)}_{ij}x^{(s')}_{ij}-\frac{\sum_{j=1}^{C} z_{ij}x^{(s)}_{ij}\sum_{j=1}^{C}z_{ij} x^{(s')}_{ij}}{n}\bigg)\end{align*}
and
\begin{align*}\tr\bigg[\left(X_B^{(s)}\right)^{\tran}\bigg(I-  \frac{1} {n}& 11^{\tran}\bigg) \left(X_B^{(s')}\right)\bigg] \\
& = \sum_{j=1}^{C}\bigg( \sum_{i=1}^{R}z_{ij} x^{(s)}_{ij}x^{(s')}_{ij}-\frac{\sum_{i=1}^{R} z_{ij}x^{(s)}_{ij}\sum_{i=1}^{R}z_{ij} x^{(s')}_{ij}}{n}\bigg)\end{align*}

\subsection{Simplication of covariance of \texorpdfstring{$\hat{\bbeta}$}{beta} }
\label{sec.covariance_of_beta_hat}
\begin{align*}
 & \cov{\hat{\bbeta}} \\
& =(X^{\tran} \tilde{X})^{-1} \tilde{X}^{\tran} \left( X \hat\Sigma_a X^\tran \bullet Z_a Z_a^\tran + X \hat\Sigma_b X^\tran \bullet Z_b Z_b^\tran  + \hat \sigma^2_e I_N \right)  \tilde{X}  (\tilde X^{\tran} X)^{-1} \\
& = 
(X^{\tran} \tilde{X})^{-1}  \left[ \tilde{X}^{\tran} \left( X \hat\Sigma_a X^\tran \bullet Z_a Z_a^\tran \right) \tilde X +  \tilde{X}^{\tran} \left( X \hat\Sigma_b X^\tran \bullet Z_b Z_b^\tran \right) \tilde{X} \right] (X^{\tran} X)^{-1} \\
&  \hspace{7cm} + \hat \sigma^2_e     (\tilde X^{\tran} X)^{-1} \\
\end{align*}

Let $P_a \in \mathbb{R}^{N \times N}$ denotes the permutation matrix that rearranges the data points so that they are grouped by clients and $P_b \in \mathbb{R}^{N \times N}$ denotes the permutation matrix that rearranges the data points so that they are grouped by items. 


\begin{align}
\label{eq.sd_simp}
\tilde{X}^{\tran} \left( X \hat\Sigma_a X^\tran \bullet Z_a Z_a^\tran \right) \tilde X & = \tilde{X}^{\tran} P_a^\tran P_a \left( X \hat\Sigma_a X^\tran \bullet Z_a Z_a^\tran \right)  P_a^\tran P_a\tilde X \\
& = \tilde{X}^{\tran} P_a ^\tran \left(P_a X \hat\Sigma_a X^\tran P_a ^\tran  \bullet  P_a Z_a Z_a^\tran P_a^\tran \right) P_a \tilde X
\end{align}

In order to keep the notation simplified, we shall move ahead assuming that data is grouped by clients, i.e., we shall be denoting $P_a X$ by $X$, $P_a \tilde X$ by $\tilde X$ and $P_a Z_a$ by $Z_a$. The assumption simplifies the structure of $Z_a Z_a ^\tran$ to the block diagonal matrix given by

\[Z_a Z_a ^\tran = \begin{bmatrix}
1_{n_1} 1^\tran_{n_1} & 0 & \cdots & 0\\
0 & 1_{n_2} 1^\tran_{n_2} & \cdots & 0\\
0 & 0 & \ddots & 0\\
0 & 0 & 0 & 1_{n_R} 1^\tran_{n_R}\\
\end{bmatrix}\]

Now, let's define $U = X \hat{\Sigma_a}^{1/2}$ and represent column $i$ of $\tilde{X}$ by $s$ and column $j$ of $\tilde{X}$ by $t$. Therefore, $X \hat\Sigma_a X^\tran = \sum_{q = 0}^{p} U^{(q)} {U^{(q)}} ^\tran $ where $\{U^{(q)}\}$ represents columns of $U$.

 Therefore, $(i,j)$ entry in equation $\ref{eq.sd_simp}$ is given by 

\begin{align*} \sum_{q = 0} ^{p} (s^{\tran}_{1.},  \cdots, s^{\tran}_{R.}) & \begin{bmatrix}
{U^{(q)}}_{1.} {U^{(q)}}_{1.}^\tran & \cdots & 0\\
0  & \ddots & 0\\
0 & \cdots & {U^{(q)}}_{R.} {U^{(q)}}_{R.}^\tran\\
\end{bmatrix} \begin{pmatrix}
t_1\\
 \vdots\\
t_R\\
\end{pmatrix} \\
& = \sum_{q = 0} ^ p \sum_{i = 1}^{R} s_{i.} ^\tran {U^{(q)}}_{i.}  {U^{(q)}}_{i.}^\tran t_{i.}\\
& = \sum_{q = 0} ^ p \sum_{i = 1}^{R} s_{i.} ^\tran {U^{(q)}}_{i.}  {U^{(q)}}_{i.}^\tran t_{i.}\\
& = \sum_{q = 0} ^ p \sum_{i = 1}^{R} \sum_{j = 1}^C z_{ij} s_{ij} ^\tran {U^{(q)}}_{ij}  {U^{(q)}}_{ij}^\tran t_{ij}
 \end{align*}

Therefore, the quantity $\left[ \tilde{X}^{\tran} \left( X \hat\Sigma_a X^\tran \bullet Z_a Z_a^\tran \right) \tilde X +  \tilde{X}^{\tran} \left( X \hat\Sigma_b X^\tran \bullet Z_b Z_b^\tran \right) \tilde{X} \right] \in \mathbb{R}^{(p+1) \times (p+1)}$ can be computed in $\mathcal{O}(N p^3)$ time which makes the computation of $\cov(\hat{\bbeta})$ possible in time linear in $N$. 
\subsection{Proof of Lemma~\ref{lemma:var_E}}\label{sec:proof-lemma-refl}
\begin{align*}
& q_a^k  (a_1, \cdots, a_R) \propto \Prob (y, \{\bsa_i\}_{i=1}^R | \bsb_j = E_{Q^{(k-1)}} [\bsb_j])\\
& \propto \exp \left \{ -\frac{1}{2} \sum_{i=1}^R \sum_{j=1}^C \frac{z_{ij} (y_{ij} -\mathbf{x}_{ij} ^\tran (\bsa_i + \mu_{b,j}^{(k-1)}+ \bbeta))^2}{\left(\sigma^{(k-1)}_e\right)^2} -\frac{1}{2} \sum_{i=1}^R \bsa_i ^\tran (\Sigma_a^{(k-1)})^{-1} \bsa_i \right \}\\
& \propto \exp \Bigg \{ -\frac{1}{2} \sum_{i=1}^R \bsa_i^\tran \Bigg( (\Sigma^{(k-1)}_a)^{-1} +\frac{\sum_{j=1}^{C}z_{ij}\mathbf{x}_{ij}  \mathbf{x}_{ij} ^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2} \Bigg) \bsa_i\\
& \hspace{2cm} + \bsa_i ^\tran \frac{\sum_{j=1}^{C}z_{ij}\big(y_{ij} - \mathbf{x}_{ij} ^{\tran}\big(\bbeta^{(k-1)} +\mu^{(k-1)}_{b,j}\big)\big)\mathbf{x}_{ij} }{\left(\sigma^{(k-1)}_e\right)^{2}} \Bigg\} \\
\end{align*}
If $q_a^k (a_1, \cdots, a_R)$ represents the distribution $\mathcal{N}(\mu^{(k)}_{a,i}, \Sigma^{(k)}_{a,i})$, then
\begin{equation}
q_a^k (a_1, \cdots, a_R) \propto \exp \left \{ -\frac{1}{2} \bsa_i ^\tran {\left(\Sigma^{(k)}_{a,i}\right)}^{-1} \bsa_i + \bsa_i ^\tran {\left(\Sigma^{(k)}_{a,i}\right)}^{-1} \mu^{(k)}_{a,i} \right \}
\end{equation}
Thus,
\begin{equation*}
\mu^{(k)}_{a,i} = \Sigma^{(k)}_{a,i} \left(\frac{\sum_{j=1}^{C}z_{ij}\big(y_{ij} - \mathbf{x}_{ij} ^{\tran}\big(\bbeta^{(k-1)} +\mu^{(k-1)}_{b,j}\big)\big) \mathbf{x}_{ij}}{\left(\sigma^{(k-1)}_e\right)^{2}}\right)
\end{equation*}
and
\begin{equation*}
\Sigma^{(k)}_{a,i} = \left((\Sigma^{(k-1)}_a)^{-1} +\frac{\sum_{j=1}^{C}z_{ij}\mathbf{x}_{ij} \mathbf{x}_{ij}^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2}\right)^{-1}.
\end{equation*}
Similarly,
\begin{align*}
q_b^k & (b_1, \cdots, b_C) \propto \Prob (y, \{\bsb_j\}_{j=1}^C | \bsa_i = E_{Q^{(k)}} [\bsa_i])\\
& \propto \exp \left \{ -\frac{1}{2} \sum_{j=1}^C \sum_{i=1}^R \frac{z_{ij} (y_{ij} - \mathbf{x}_{ij}^\tran (\bsb_j + \mu_{a,i}^{(k)}+ \bbeta))^2}{\left(\sigma^{(k-1)}_e\right)^2} -\frac{1}{2} \sum_{j=1}^C \bsb_j ^\tran (\Sigma_b^{(k-1)})^{-1} \bsb_j \right \}\\
& \propto \exp \Bigg \{ -\frac{1}{2} \sum_{j=1}^C \bsb_j^\tran \Bigg( (\Sigma^{(k-1)}_b)^{-1} +\frac{\sum_{i=1}^{R}z_{ij}\mathbf{x}_{ij} \mathbf{x}_{ij}^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2} \Bigg) \bsb_j\\
& \hspace{2cm} + \bsb_j ^\tran \frac{\sum_{j=1}^{C}z_{ij}\big(y_{ij} - \mathbf{x}_{ij}^{\tran}\big(\bbeta^{(k-1)} +\mu^{(k)}_{a,i}\big)\big)\textbf{x}_{ij}}{\left(\sigma^{(k-1)}_e\right)^{2}} \Bigg\} \\
\end{align*}
For the same reasoning,
\begin{equation*}
\mu^{(k)}_{b,j} = \Sigma^{(k)}_{b,j} \left(\frac{\sum_{i=1}^{R}z_{ij}\big(y_{ij} - \textbf{x}_{ij}^{\tran}\big(\bbeta^{(k-1)} +\mu^{(k-1)}_{a,i}\big)\big)\textbf{x}_{ij}}{\left(\sigma^{(k-1)}_e\right)^{2}}\right)
\end{equation*}
and
\begin{equation*}
\Sigma^{(k)}_{b,j} = \left((\Sigma^{(k-1)}_b)^{-1} +\frac{\sum_{i=1}^{R}z_{ij}\textbf{x}_{ij} \textbf{x}_{ij}^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2}\right)^{-1}
\end{equation*}