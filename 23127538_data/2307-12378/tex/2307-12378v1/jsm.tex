\documentclass{article}
\input{packages}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage{microtype}
% \newcommand{\tran}{\mathsf{T}}
% \newcommand{\swcom}[1]{ {\color{magenta} {#1}} }
\title{Scalable solution to crossed random effects model with random slopes}
\author{Disha Ghandwani\\Stanford University
\and Swarnadip Ghosh\\Radix Trading
\and Trevor Hastie\\Stanford University
\and Art B. Owen \\Stanford University}
\date{\today}
\begin{document}
\maketitle
%The crossed random-effects model is popular in applied statistics with applications in longitudinal studies, e-commerce, and recommender systems, to name a few. However, these models suffer from scalability issues with computational time growing like $N^{3/2}$ (or worse) with $N$ data points. We draw our inspiration from the recommender system used by an online retail clothing company. We have a dataset with 5,000,000 measurements on over 700,000 clients and 5,000 items. Fitting crossed random effects using the maximum likelihood approach could be highly computationally inefficient
% and it could take up to several days to obtain the fit (this is a bit vague, n, p dependent statement)
%making the scope limited in such large-scale settings. There has been previous work on the scalability issues by~\cite{ghos:hast:owen:2021} and~\cite{ghos:hast:owen:logistic2022}. They studied linear and logistic regression models using client and item variables as fixed-effect features, with random intercept terms for clients and items. Here we present a generalization of this problem where we allow effect sizes/slopes to be random too. It helps us in capturing the variability of effect size among clients as well as items. We have developed a scalable solution to the above problem and obtained empirical consistency of our estimates, i.e. as the number of data points increases, our estimates converge to the true parameters. {\color{red} We implement the discussed algorithm on Stitch Fix data.}
The crossed random-effects model is widely used in applied statistics, finding applications in various fields such as longitudinal studies, e-commerce, and recommender systems, among others. However, these models encounter scalability challenges, as the computational time grows disproportionately with the number of data points, typically following a cubic root relationship ($N^{3/2}$ or worse) with $N$. Our inspiration for addressing this issue comes from observing the recommender system employed by an online clothing retailer. Our dataset comprises over 700,000 clients, 5,000 items, and 5,000,000 measurements. When applying the maximum likelihood approach to fit crossed random effects, computational inefficiency becomes a significant concern, limiting the applicability of this approach in large-scale settings. To tackle the scalability issues, previous research by Ghosh et al. (2022a) and Ghosh et al. (2022b) has explored linear and logistic regression models utilizing fixed-effect features based on client and item variables, while incorporating random intercept terms for clients and items. In this study, we present a more generalized version of the problem, allowing random effect sizes/slopes. This extension enables us to capture the variability in effect size among both clients and items. Importantly, we have developed a scalable solution to address the aforementioned problem and have empirically demonstrated the consistency of our estimates. Specifically, as the number of data points increases, our estimates converge towards the true parameters. To validate our approach, we implement the proposed algorithm using Stitch Fix data.







\section{Introduction}\label{sec.introduction}
In the era of Over The Top (OTT) platforms like Netflix, where customer acquisition is booming and a vast array of titles are hosted, the need for computationally efficient recommender systems has become crucial. In such scenarios, it is common for different clients to rate varying numbers of items, which gives a haphazard missingness pattern. We deal with one such problem where $R$ clients rate $C$ items, and it is usually the case that only $N \ll R \times C$ of the ratings are observed. Moreover, we consider a situation where we have access to various characteristics of each (item, client) pair, in addition to the client's rating of the item. For instance, information such as the client's age, gender, and ethnicity, as well as the genre, language, country of origin, and creators of the item, could be utilized in the model fitting process.

To represent the rating provided by client $j$ for item $i$, we use the notation $Y_{ij}$, while the covariate information for the specific client-item pair $(i,j)$ is denoted as $x_{ij}$.  In such settings, ordinary least square is not applicable as the ratings are not independent. A client who gives a high rating to one item may exhibit a tendency to provide high ratings overall, introducing a covariance structure in the ratings from the same client. Similarly, if an item receives a low rating from one client, it is possible that other clients will also rate it poorly due to the item's inherent low quality, resulting in a covariance structure among the ratings for that specific item. To account for these covariance structures, crossed random effects models are employed. These models are effective in capturing the dependencies present in the ratings provided by the same client or for the same item. However, one notable drawback of these models is their limited scalability. As the dataset grows in size, the computational burden associated with fitting crossed random effects models becomes challenging and inefficient.
%{\color{blue}Do you think we may write this after the line ``To account for these covariance structures.....'' : OLS may produce overly conservative estimate of variance which may affect inference of the parameters. Bootstrapping is also hard for crossed random effects. ~\cite{mccu:2000} proved the non-existence of any bootstrap method that can even correctly reproduce the variance of a sample mean in the balanced crossed random effects setting. However, a strategy of independently bootstrapping the rows and columns of the data matrix yield a bootstrap variance which is mildly conservative (See~\cite{pbs}). May we remove ``In such settings, ordinary least square is not applicable as the ratings are not independent ..... ''}\\

~\cite{ghos:hast:owen:2021} considered the crossed random-effects model with random intercept terms for clients and items, given by
\begin{equation}
\label{eq:intercept_model}
Y_{ij} = \beta_0+ a_i +b_j+x_{ij}^{\tran}\beta + e_{ij}, \hspace{0.5cm} \forall i \in \{1,\cdots, R\}, \hspace{0.3cm} j\in \{1,\cdots, C\}.
\end{equation}

Here, the random effects are $a_i \in \mathbb{R}$, $b_j \in \mathbb{R}$ and an error $ e_{ij} \in \mathbb{R}$. The fixed effects are the regression parameters $\beta \in \mathbb{R}^{p}$ and the intercept $\beta_0 \in \mathbb{R}$. They assume that $a_i \sim N(0, \sigma^2_a)$ , $b_j \sim N(0, \sigma^2_b), $
and $e_{ij} \distas{i.i.d} \mathbin{N}(0,\sigma^2_e)$ are all independent. Under the above model,
$$Cov(Y_{ij}, Y_{ij'}) = \sigma^2_a \mbox{ and } Cov(Y_{ij}, Y_{i'j}) = \sigma^2_b \mbox{ for } i \neq i' ,  j \neq j'.$$

%The crossed random effects setting is substantially more challenging than the hierarchical setup, which is better suited to Bayesian computing (See \cite{gelm:hill:2006}).
The standard methods implement crossed random effects model with a computational cost proportional to $(R+C)^3$
and then because $RC\ge N$ we have $(R+C)^3 = \Omega(N^{3/2})$ (See \cite{crelin} for details). ~\cite{crelin} proposed a moment-based method for estimation of variance parameters in the intercept model ($\ref{eq:intercept_model}$) with at most linear cost. Recently \cite{ghos:hast:owen:2021}
develop a backfitting algorithm
based on the work of~\cite{buja:hast:tibs:1989} that has a linear cost in $N$.
A plain Gibbs sampler is also not scalable for the computational cost is $\Omega(N^{3/2})$ (See \cite{crevarcomp}).
~\cite{papa:robe:zane:2020} and~\cite{ghos:zhon:2021:tr}, have established scalability for intercept-only crossed random effect models from a Bayesian point of view.
%~\cite{var_inf_bayesian} derived streamlined mean field variational Bayes algorithms for fitting linear mixed models with crossed random effects.
\\

While fitting the model, \cite{ghos:hast:owen:2021} also provide estimates for $\{a_i\}_{i=1}^R$ and $\{b_j\}_{j=1}^C$ which could be further utilized for providing prediction of ratings on test data. Thus, crossed random effects model can be used as a scalable recommender system. We measured the prediction accuracy of crossed random effects with a random intercept on stitch-fix data with a random train-test split. Where ordinary least square could provide $R^2$ of 4.61 $\%$, the crossed random effect model (\ref{eq:intercept_model}) could provide $R^2$ of 18.42 $\%$. Such a boost in R-square suggests a need for scalable recommender systems based on crossed random effects model. 
Therefore, we attempt to implement a scalable solution to crossed random effects including random slopes.
%We extend this by considering the crossed random effects with random intercepts as well as random slopes:
\begin{equation}
\label{eq:random_slopes_uncompact}
Y_{ij} = \beta_0+ a_i +b_j + x_{ij}^{\tran}(\beta+\tilde{a}_i+\tilde{b}_j) + e_{ij}, \hspace{0.3cm} \forall i \in \{1,\cdots, R\}, \hspace{0.3cm} j\in \{1,\cdots,C\}, \end{equation}
where random effects $\tilde{a}_i \in \mathbb{R}^{p}$ and $\tilde{b}_j \in \mathbb{R}^{p}$ and an error $ e_{ij} \in \mathbb{R}$. We assume that $\tilde{a}_i \sim N_p(0, \tilde{\Sigma}_a)$ and $\tilde{b}_j \sim N_p(0, \tilde{\Sigma}_b)$, with other terms having the same meaning. The crossed random effects model with random slopes helps us in capturing the variability of the effect size of covariates among different clients and items. Figure~\ref{fig:random_slope_intuition} provides a visual representation for model with random intercept and random slopes. The first plot represents a random effects model considering random intercept just for clients, second plot represents random effects model with random intercept as well as random slope for a continuous variable, ``match" and the third plot represents a random effects model with random intercept and random slope for an indicator random variable ``client edgy".
% Figure environment removed
The model~($\ref{eq:random_slopes_uncompact}$) could be rewritten in a compact form in the following way by applying the change of variables:-
\begin{equation}
\label{eq:random_slopes}
Y_{ij} = x_{ij}^{\tran}(\beta + a_i +b_j) + e_{ij}, \hspace{0.5cm} \forall i \in \{1,\cdots, R\}, \hspace{0.3cm} j\in \{1,\cdots,C\}.
\end{equation}
Here, the first coordinate of the covariate vector $x_{ij} \in \mathbb{R}^{p+1}$ is one, representing intercepts term as (\ref{eq:intercept_model}). We assume that $a_i \sim N_{p+1}(0, \Sigma_a)$ , $b_j \sim N_{p+1}(0, \Sigma_b), $
and $e_{ij} \distas{i.i.d} \mathbin{N}(0,\sigma^2_e)$ are all independent. It is worth noting that it is possible to have random effects for only a subset of the covariates, rather than for all of them. In our presentation, we have included random slopes for all the covariates for the sake of simplicity in notation. However, it is important to recognize that the previous generalization can be easily derived from the latter approach.\\

The aim is to estimate the fixed effect $\beta$ and covariance parameters $\Sigma_a$, $\Sigma_b$, and $\sigma^2_e$. When $a_i, b_j$
and $e_{ij}$ are gaussian, the GLS estimate is also the maximum likelihood estimate (MLE). Thus, if covariance matrix of the $N$-vector $Y$ is given by $\mathcal{V} \in \mathbb{R}^{N \times N}$ under the above model, the density of $Y$, hence the likelihood for ($\beta, \Sigma_a, \Sigma_b, \sigma^2_e$) will be given by
\begin{equation}
\label{eq:likelihood}
\frac{1}{\sqrt{(2\pi)^N|\mathcal{V}|}} \exp\left(-\frac{1}{2}(Y - X \beta )^{\tran} \mathcal{V}^{-1}(Y-X \beta)\right)
\end{equation}

Here, the vector $Y$ denotes all of the $Y_{ij}$ placed into a vector in $\mathbb{R}^N$ and $X$ denotes $x_{ij}$ stacked compatibly into a matrix in $ \mathbb{R}^{N \times (p+1)}$. 
% Previous work, in particular~\cite{papa:robe:zane:2020} and~\cite{ghos:zhon:2021:tr}, has established scalability for intercept-only crossed random effect models from a Bayesian point of view.~\cite{var_inf_bayesian} derived streamlined mean field variational Bayes algorithms for fitting linear mixed models with crossed random effects.~\cite{crelin} proposed a moment-based method for estimation of variance parameters in the intercept model ($\ref{eq:intercept_model}$) with at most linear cost.
Under the above model, $(\ref{eq:random_slopes}$) and ($\ref{eq:likelihood}$), the covariance matrix $\mathcal{V}$ of $Y$ is given by
\begin{equation}
\label{eq:V}
\mathcal{V} = \left( X \Sigma_a X^{\tran} \right) \bullet \left(Z_a Z_a^{\tran}\right) + \left(X \Sigma_b X^{\tran}\right) \bullet \left(Z_b Z_b^{\tran}\right) + \sigma^2_e I_N \end{equation}
where $Z_a \in \lbrace 0,1 \rbrace^{N \times R}$ denotes the design matrix for clients, $Z_b \in \lbrace 0,1 \rbrace^{N \times C}$ denotes the design matrix for items and $A \bullet B$ denotes elementwise product of matrix $A$ and $B$. Let data point $t_1$ refer to the pair $(i_1,j_1)$ and data point $t_2$ refer to the pair $(i_2,j_2)$, then
%\[
%\mathcal{V}_{t_1,t_2} =
%\begin{cases}
% 0, & \text{if data points correspond to different items and different clients,}\\
% x_{ij_1}^{\tran} \Sigma_a x_{ij_2}, & \text{if data points correspond to the same client but different items,}\\
% x_{i_1 j}^{\tran} \Sigma_b x_{i_2j}, & \text{if data points correspond to the same client but different items,}\\
% x_{ij}^{\tran} \Sigma_a x_{ij}^{\tran} + x_{ij}^{\tran} \Sigma_b x_{ij} + \sigma^2_e , & \text{if } t_1=t_2 \text{ (variance of the observation } y_{t_1,t_1}).\\
%\end{cases}\]
\begin{equation}
\mathcal{V}_{t_1,t_2} =
\begin{cases}
0, & \text{if } i_1 \neq i_2 \text{ and } j_1 \neq j_2, \\
x_{i_1 j}^{\tran} \Sigma_b x_{i_2j} , & \text{if } i_1 \neq i_2 \text{ and } j_1 = j_2 = j,\\
x_{ij_1}^{\tran} \Sigma_a x_{ij_2} , & \text{if } i_1 = i_2 = i \text{ and } j_1 \neq j_2,\\
x_{ij}^{\tran} \Sigma_a x_{ij}^{\tran} + x_{ij}^{\tran} \Sigma_b x_{ij} + \sigma^2_e , & \text{if } (i_1, j_1) = (i_2, j_2) = (i,j).\\
\end{cases}\end{equation}
\textbf{Note:} The term with $i_1=i_2 = i$ and $j_1=j_2 = j$, represent diagonal terms of the matrix, $\mathcal{V}$, i.e., variance of $y_{ij}$.\\
The covariance matrix is dense and not usefully structured, therefore obtaining $\hat\beta_{\gls}$ using generalized least squares would be computationally expensive. We explore two potential methods to achieve the task.
\begin{itemize}
\item
%In our first approach, we provide consistent estimates of the covariance parameters, and then proceed with estimating the fixed parameter given the covariance parameters. So, the first challenge lies in providing consistent estimates of covariance parameters in $\mathcal{O}(N)$ time, we accomplish that by using the method of moment approach illustrated in Section \ref{sec.covariance}. This approach is most appropriate if the covariances matrices are diagonal. The second challenge lies in estimating the fixed effect parameter given the covariance parameters in $\mathcal{O}(N)$ time; we use a backfitting approach for the purpose, which is explained in Section \ref{sec.backfitting}. Each step of the backfitting algorithm takes computational time of $\mathcal{O}(N)$, and we speed up the convergence rate using a technique known as ``clubbing". The drawback of this approach is that when the value of $p$ is large and the covariance matrices are non-diagonal, the method of moment approach may not produce accurate estimates for the covariance parameters.
In our first approach, we provide consistent estimates of the covariance parameters using the method of moment approach illustrated in Section \ref{sec.covariance}. We then proceed with estimating the fixed parameter given the covariance parameters using a backfitting approach, which is explained in Section \ref{sec.backfitting}. Both of the tasks can be accomplished in $\mathcal{O}(N)$ time.
\end{itemize} The drawback of the first approach is that when the value of $p$ is large and the covariance matrices are non-diagonal, the method of the moments approach may not produce accurate estimates for the covariance parameters. To deal with such a hurdle, we suggest an application of the variational EM algorithm which is closely related to the backfitting algorithm. Variational EM has been previously explored for fast and accurate estimation of non-nested binomial hierarchical models~\cite{max:2021}.
\begin{itemize}
\item
%In our second approach, we maximize an approximation to the likelihood which is useful for estimating covariance parameters with greater precision than the method of moments. The advantage of this approach is its computational efficiency, with each step at most $\mathcal{O}(N)$. The speed of the variational approach could be enhanced using clubbing as well.
In our second approach, we maximize an approximation to the likelihood using ``Variational EM " approach to estimate fixed and covariance parameters simultaneously. It helps us in estimating covariance matrices precisely even when they are unstructured or $p$ is sufficiently large. Another advantage of this approach is its computational efficiency, with each step at most $\mathcal{O}(N)$.
\end{itemize}
We speed up the convergence rate for both of the approaches, backfitting and variational EM using a technique referred to as ``clubbing" discussed in \ref{sec.backfitting}.
\subsection{Paper Outline}
\begin{itemize}
\item In Section \ref{sec.backfitting}, we discuss the backfitting approach with and without clubbing.
\item In Section \ref{sec.covariance}, we discuss the estimation of covariance parameters using the method of moments in two scenarios: when the covariance matrices, $\Sigma_a$ and $\Sigma_b$ are diagonal, and when they are non-diagonal.
\item In Section \ref{sec.variational}, we discuss the application of variational EM in estimating fixed as well as covariance parameters using an approximate maximum likelihood approach.
\end{itemize}
% It must be a lemma
\section{Estimation of the fixed parameter when covariance parameters are known}\label{sec.backfitting}
If the covariance parameters are known, the aim is to obtain $ \hat\beta_{\gls}$ in $\mathcal{O}(N)$ time. We know that the GLS estimate of $\beta$ is
\begin{equation}
\label{eq:beta}
\hat\beta_{\gls} = \argmin_\beta (Y-X\beta)^\tran\mathcal{V}^{-1}(Y -X\beta)=(X^{\tran}\mathcal{V}^{-1}X)^{-1}X^{\tran}\mathcal{V}^{-1}Y
\end{equation}
But, we couldn't use the above formula directly to obtain $ \hat\beta_{\gls}$ as computing $\mathcal{V}^{-1} $ would take at least $\mathcal{O}(N^{3/2})$ time in the above setting. Therefore, it is important to suggest alternative approaches to obtain estimates for $\beta$. The theorem below helps us in setting up the motivation for the backfitting algorithm.
\begin{restatable}{theorem}{robin}
\label{thm:robinson}
Consider the solutions to the following penalized least-squares problem
\begin{equation}
\label{eq:loss}
\min_{\beta,a_i,b_j} \sum_{i=1}^R \sum_{j=1}^{C}\frac{z_{ij}(y_{ij} - x_{ij}^{\tran}(\beta + a_i +b_j))^2}{\sigma^2_e} + \sum_{i=1}^R a_i^{\tran} \Sigma^{-1}_a a_i + \sum_{j=1}^C b_j^{\tran} \Sigma_b^{-1} b_j
\end{equation}
Then $\hat\beta = \hat\beta_{\gls}$ and the $\{\hat{a}_i\}_{i=1}^{R}$ and $\{\hat{b}_j\}_{j=1}^C$ are the best linear unbiased prediction (BLUP) estimates of the random effects.
\end{restatable}
 \textbf{Note: } We define $z$ such that $z_{ij} = 1$ if the rating $Y_{ij}$ for $(i,j)$ client-item pair is observed and $z_{ij} = 0$ otherwise.\\

The theorem was originally given by~\cite{robinson91:_that_blup}, we provide an alternative proof in the appendix. If the covariance parameters are known or somehow estimated, the fixed-effect parameter can be estimated by backfitting algorithms which minimize ($\ref{eq:loss}$). We estimated $\beta$ using backfitting with and without clubbing similar to~\cite{ghos:hast:owen:2021} and~\cite{ghos:hast:owen:logistic2022}.
\subsection{Vanilla Backfitting:} The objective function $(\ref{eq:loss})$ treats $\{a_i\}_{i=1}^R$, $\{b_j\}_{j=1}^C$, and $\beta$ as parameters where $\{a_i\}_{i=1}^R$ and $\{b_j\}_{j=1}^C$ are estimated using generalized ridge regression. The parameters in $(\ref{eq:loss})$ can be split into three groups, $\{a_i\}_{i=1}^R$, $\{b_j\}_{j=1}^C$, and $\beta$. The idea of backfitting is to cycle through the estimation of parameters in each group, keeping the parameters fixed in the remaining groups fixed, i.e., a batch coordinate descent. Minimising the loss given by (\ref{eq:loss}) is equivalent to minimising
\begin{equation}\sum_{i=1}^R \sum_{j=1}^{C}z_{ij}(y_{ij} - x_{ij}^{\tran}(\beta + a_i +b_j))^2 + \sum_{i=1}^R a_i^{\tran} \Lambda a_i + \sum_{j=1}^C b_j^{\tran} \Gamma b_j \end{equation}
where $\Lambda = \sigma^2_e \Sigma^{-1}_a$ and $\Gamma = \sigma^2_e \Sigma^{-1}_b$ are the precision matrices corresponding to the two random effects.
Thus, the fit for $a_i$ keeping $\{b_j\}_{j=1}^{C}$ and $\beta$ fixed is given by
\begin{equation}
\label{eq:(a_i)}
\hat a_i = \bigg(\sum_{j=1}^{C} z_{ij}x_{ij}x^{\tran}_{ij}+ \Lambda\bigg)^{-1}\bigg(\sum_{j=1}^{C}z_{ij}x_{ij} \Big(y_{ij} - x_{ij}^{\tran}(\beta +b_j)\Big)\bigg)
\end{equation}
Similarly, fit for $b_j$ keeping $\{a_i\}_{i=1}^{R}$ and $\beta$ fixed is given by
\begin{equation}
\label{eq:(b_j)}
\hat b_j = \bigg(\sum_{i=1}^{R} z_{ij} x_{ij}x^{\tran}_{ij}+ \Gamma\bigg)^{-1}\bigg(\sum_{i=1}^{R} z_{ij}x_{ij}\Big(y_{ij} - x_{ij}^{\tran}(\beta +a_i)\Big)\bigg)
\end{equation}
The fit for $\beta$ given $a_i$ and $b_j$ is given by:-
\begin{equation}
\label{eq:beta}
\hat\beta = \bigg(\sum_{j=1}^C \sum_{i=1}^{R}z_{ij} x_{ij}x^{\tran}_{ij}\bigg)^{-1}\bigg(\sum_{j=1}^C \sum_{i=1}^{R} \Big(y_{ij} - x_{ij}^{\tran}(a_i+b_j)\Big)z_{ij}x_{ij}\bigg)
\end{equation}
We cycle through these equations until the fits converge.\\
\\
\textbf{Remarks:}
\begin{itemize}
\item For each group of parameters, we form the partial residuals holding the others fixed. Then, we optimize with respect to that group. For the random effect parameters, the problem decouples, as each step in $(\ref{eq:(a_i)})$ and $(\ref{eq:(b_j)})$ indicate.
\item Convergence is measured in terms of the fits of the terms, $x_{ij}^{\tran} \beta, \{x_{ij}^{\tran} a_i \}$ and $ \{x_{ij}^{\tran} b_j\}$.
\item Each step jumps the fits closer to the convergence.
\item The cost of obtaining the above fits in $(\ref{eq:(a_i)})$, $(\ref{eq:(b_j)})$, and $(\ref{eq:beta})$ is $\mathcal{O}(N)$.
\end{itemize}
\subsection{Backfitting with clubbing}
One of the shortcomings of the vanilla backfitting algorithm is that it takes a long time to converge. One of the reasons is that the solution has built-in constraints that are not visible to the individual process. The theorem below provides two such constraints.
\begin{theorem}
The solutions $\{a_i\}_{i=1}^R$ and $\{b_j\}_{j=1}^C$ of ($\ref{eq:loss}$) satisfy $\sum_{i=1}^R \hat{a}_i = 0 $ and $ \, \sum_{j=1}^C \hat{b}_j = 0 $.
\end{theorem}
\begin{proof} It is enough to prove $\sum_{i=1}^R \hat{a}_i = 0 $, the proof for $ \, \sum_{j=1}^C \hat{b}_j = 0 $ is similar. Suppose $\sum_{i=1}^R \hat{a}_i \ne 0 $, define $ \bar{a} = \sum_{i=1}^R \hat{a}_i/ R$ and $\tilde{a}_i = \hat{a}_i - \bar{a} \, \, \forall i$. Then it is easy to show that $$\sum_{i=1}^R \tilde{a}_i^{\tran} \Sigma^{-1}_a \tilde{a}_i = \sum_{i=1}^R (\hat{a}_i - \bar{a})^{\tran} \Sigma^{-1}_a (\hat{a}_i - \bar{a}) < \sum_{i=1}^R \hat{a}_i^{\tran} \Sigma^{-1}_a \hat{a}_i,$$ unless $\bar{a} = 0$.
If we choose, $\tilde \beta = \hat\beta + \bar{a}$, i.e., $\tilde \beta + \tilde{a}_i = \hat\beta + \hat{a}_i$, then \begin{equation} \sum_{i=1}^R \sum_{j=1}^{C}\frac{z_{ij}(y_{ij} - x_{ij}^{\tran}(\tilde \beta + \tilde{a}_i +b_j))^2}{\sigma^2_e} = \sum_{i=1}^R \sum_{j=1}^{C}\frac{z_{ij}(y_{ij} - x_{ij}^{\tran}(\hat\beta + \hat{a}_i +b_j))^2}{\sigma^2_e}. \end{equation}
Thus,
\begin{align*}
\min_{\beta,b_j} \sum_{i=1}^R \sum_{j=1}^{C} & \frac{z_{ij}(y_{ij} - x_{ij}^{\tran}(\beta + \tilde{a}_i +b_j))^2}{\sigma^2_e} + \sum_{i=1}^R \tilde{a}_i^{\tran} \Sigma^{-1}_a \tilde{a}_i \\
& < \min_{\beta,b_j} \sum_{i=1}^R \sum_{j=1}^{C}\frac{z_{ij}(y_{ij} - x_{ij}^{\tran}(\beta + \hat{a}_i +b_j))^2}{\sigma^2_e} + \sum_{i=1}^R \hat{a}_i^{\tran} \Sigma^{-1}_a \hat{a}_i,
\end{align*}
unless $\bar a = 0$ which leads to contradiction.
\end{proof}
The theorem above shows that the constraint $\sum_{i=1}^R \hat{a}_i = 0 $ is automatically imposed if we minimize (\ref{eq:loss}) simultaneously in terms of $\{a_i\}_{i=1}^{R}$ and $\beta$. Similarly, the constraint $\sum_{j=1}^C \hat{b}_j = 0 $ is automatically imposed if we minimize (\ref{eq:loss}) simultaneously in terms of $\{b_j\}_{j=1}^C$ and $\beta$. We refer this technique as ``clubbing". It helps in reaching convergence faster as it automatically imposes the constraint that
\begin{equation}\sum_{i=1}^R a_{i} = 0 \mbox{ and } \sum_{j=1}^C b_{j} = 0 \end{equation}
\subsection{An efficient way to implement clubbing}
\textbf{Simultaneously estimating $\beta$ and $a_i$ fixing $\{b_j\}_{j=1}^C$}
Estimating $\beta$ and $\{a_i\}$ simultaneously would be equivalent to minimizing the loss given by (\ref{eq:loss}) keeping $\{b_j\}$ fixed.
Thus, if $r_{ij} = y_{ij} - x_{ij}b_j $, then we are minimizing
\begin{equation}
\sum _{i=1}^R a_i^{\tran}\Lambda a_i + \sum_{i=1}^R\sum_{j=1}^{C}z_{ij}\big(r_{ij} - x_{ij}^{\tran}(\beta + a_i)\big)^2,
\end{equation}
which would be equivalent to solving the following equations:-
\begin{equation}\label{eq:a_i_clubbed}
\hat a_i = \bigg(\sum_{j=1}^{C} z_{ij}x_{ij}x^{\tran}_{ij}+ \Lambda\bigg)^{-1}\bigg(\sum_{j=1}^{C}z_{ij}x_{ij} \Big(r_{ij} - x_{ij}^{\tran}\beta\Big)\bigg),
\end{equation}
\begin{equation}
\label{eq:beta_clubbed}
\hat\beta = \bigg(\sum_{j=1}^C \sum_{i=1}^{R}z_{ij} x_{ij}x^{\tran}_{ij}\bigg)^{-1}\bigg( \sum_{i=1}^{R} \sum_{j=1}^C \Big(r_{ij} - x_{ij}^{\tran}a_i\Big)z_{ij}x_{ij}\bigg),
\end{equation}
where $\Lambda = \sigma^2_e \Sigma_a^{-1}$.\\
Let $f_a$ denote the fit corresponding to the terms $\{x_{ij}^{\tran} \hat a_i\}$ and $X \hat\beta$ be the vector of fits for the fixed effects. Then from $(\ref{eq:a_i_clubbed})$, we can write
\begin{equation}
\label{eq:sim_ai}
f_a = S_a (r - X \hat \beta),
\end{equation}
and from $(\ref{eq:beta_clubbed})$, we have \begin{equation}
\label{eq:sim_beta}
X^{\tran}X \hat \beta = X^{\tran} (r - f_a).\end{equation}
Here, $S_a$ is the $N \times N$ linear operator that computes these fits, completely by solving $(\ref{eq:a_i_clubbed})$ for each $i$ and fills in the fits. Plugging $(\ref{eq:sim_ai})$ into $(\ref{eq:sim_beta})$, we have
\begin{equation}
X^{\tran}X \hat \beta = X^{\tran} (r - S_a (r - X \hat \beta)).
\end{equation}
Collecting terms, we have
\begin{equation}
X^{\tran} \left(I - S_a\right) X \hat \beta = X^{\tran} \left(I - S_a\right) r,
\end{equation}
and hence
\begin{equation}
\hat \beta = \left[ X^{\tran} \left(I - S_a\right) X\right] ^{-1} X^{\tran} \left(I - S_a\right) r.
\end{equation}
This means that we need to apply $S_a$ to each of the columns of X, compute the fits, and take residuals, to produce $\tilde{X} = (I - S_a) X$, then
\begin{equation}
\hat \beta = \left( \tilde{X}^{\tran} X \right)^{-1} \tilde{X}^{\tran} r.
\end{equation}
Thus, we get new estimates of $\beta$, which could be inserted in equation $(\ref{eq:a_i_clubbed})$ to obtain new estimates of $a_i$'s and repeat the above process taking $r_{ij} = y_{ij} - x_{ij}a_i $ and simultaneously estimate $\beta$ and $b_j$ keeping $\{a_i\}$ fixed by the similar procedure
We continue the above process till fits of the terms, $x_{ij}^{\tran} \beta, \{x_{ij}^{\tran} a_i \}$ and $ \{x_{ij}^{\tran} b_j\}$ converge.\\
\textbf{Time Complexity} The time complexity for each step of backfitting with or without clubbing is $\mathcal{O}(Np^2)$.
\section{Estimation of Covariance Parameters using \\
Method of Moments}\label{sec.covariance}
In section \ref{sec.backfitting}, we discussed the estimation of the fixed parameter $\beta$ when the covariance parameters are known or estimated. In this section, we discuss a scalable approach to obtain consistent estimates for covariance parameters. We discuss the estimation of covariance parameters under two scenarios: when $\Sigma_a$ and $\Sigma_b$ are assumed to be diagonal, and if nothing is assumed.
\subsection{$\Sigma_a$ and $\Sigma_b$ are diagonal matrices.} If
$\Sigma_a$ and $\Sigma_b$ are diagonal matrices, we can obtain consistent estimates of $\Sigma_a, \Sigma_b$, and $\sigma^2_e$ using the method of moments. Our process involves solving equations of the form
\begin{equation}
\label{eq:mm1}
\begin{split}
\Ex\bigg[\sum_{i=1}^{R} \sum_{j=1}^{C} z_{ij} r^2_{ij}-\sum_{i=1}^{R}\frac{\left(\sum_{j=1}^{C} z_{ij} r_{ij}x^{(s)}_{ij}\right)^2}{\sum_{j=1}^{C} z_{ij}x^2_{ij(s)}}\bigg{|}X\bigg] & = \sum_{i=1}^{R} \sum_{j=1}^{C} z_{ij}\hat r^2_{ij}\\
& \hspace{0.5cm} -\sum_{i=1}^{R}\frac{\left(\sum_{j=1}^{C} z_{ij}\hat r_{ij}x^{(s)}_{ij}\right)^2}{\sum_{j=1}^{C} z_{ij}x^2_{ij(s)}}
\end{split}
\end{equation}
\begin{equation}
\label{eq:mm2}
\begin{split}
\Ex\bigg[\sum_{j=1}^{C} \sum_{i=1}^{R} z_{ij} r^2_{ij}-\sum_{j=1}^{C}\frac{\left(\sum_{i=1}^{R} z_{ij} r_{ij}x^{(s)}_{ij}\right)^2}{\sum_{i=1}^{R} z_{ij}x^2_{ij(s)}}\bigg{|}X\bigg] & = \sum_{j=1}^{C} \sum_{i=1}^{R} z_{ij}\hat r^2_{ij}\\
& \hspace{0.5cm} -\sum_{j=1}^{C}\frac{\left(\sum_{i=1}^{R} z_{ij}\hat r_{ij}x^{(s)}_{ij}\right)^2}{\sum_{i=1}^{R} z_{ij}x^2_{ij(s)}}
\end{split}
\end{equation}
for all s $\in \{0, \cdots, p\}$, and
\begin{equation}
\label{eq:mm3}
\Ex\bigg[\sum_{i=1}^{R} \sum_{j=1}^C z_{ij} \left( r_{ij} - \bar{r}_{..} \right)^2 \bigg{|}X\bigg]=\sum_{i=1}^{R} \sum_{j=1}^C z_{ij} \left( \hat r_{ij} - \bar{\hat r}_{..} \right)^2
\end{equation}
\textbf{Note:} Here, $x^{(s)}_{ij}$ represents s-th covariate of the $(i,j)$ client-item pair, $r = Y - X \beta$, and $\hat r = Y - X \beta_{\ols}$ .\\

These equations can be seen as generalization of the moment equations considered by~\cite{ghos:hast:owen:2021} and \ref{appendix.mom} provided more insights into the above equations. 
 
Using the matrix formulation in ($\ref{eq:matrix_form}$), we can show that
\begin{equation}
\label{eq:simplify}
\begin{split}
\Ex\bigg[&\sum_{i=1}^{R} \sum_{j=1}^{C} z_{ij} r^2_{ij}-\sum_{i=1}^{R}\frac{\left(\sum_{j=1}^{C} z_{ij} r_{ij}x^{(s)}_{ij}\right)^2}{\sum_{j=1}^{C} z_{ij}x^2_{ij(s)}} \bigg{|} X\bigg] \\
&=\sum_{s'=0}^{p} \sum_{i=1}^{R}\left[ \sum_{j=1}^{C} z_{ij} x^{(s')^2}_{ij}-\frac{\left(\sum_{j=1}^{C} z_{ij} x^{(s)}_{ij} x^{(s')}_{ij} \right)^2}{\sum_{j=1}^{C} z_{ij} x^{(s)^2}_{ij}}\right]\Sigma_{a,s's'} \\
& \hspace{0.8cm}+\sum_{s'=0}^{p}\sum_{i=1}^R\left[ \sum_{j=1}^{C}z_{ij} x^{(s')^2}_{ij} -\frac{\sum_{j=1}^{C}z_{ij} x^{(s)^2}_{ij} x^{(s')^2}_{ij}}{\sum_{j=1}^{C} z_{ij} x^{(s)^2}_{ij}}\right]\Sigma_{b,s's'} + \sigma^2_e (n-R)\\
\end{split}
\end{equation}
\textbf{Note: } The proof is provided in the appendix. The complexity to compute all the quantities above is $\mathcal{O}(Np^2)$. Once these quantities are computed, we solve equations $(\ref{eq:mm1}), (\ref{eq:mm2})$ and $(\ref{eq:mm3})$ to get covariance parameters. Thus we obtain $2p+3$ equations involving $2p+3$ unknown variance parameters, which we can solve to obtain method of moment estimates for variance parameters.
\begin{restatable}{theorem}{betaols}
\label{betaolsconsistent}
Let $X_{r}^{(a)} (X_{c}^{(b)})$ be the observations arranged in row (column) blocks such that each block has a fixed level for the first (second) factor. Also assume that $\max_{1\leq r \leq R}\frac{\lambda_{\max}(X_{r}^{(a)}\Sigma_{a}X_{r}^{(a) \tran})}{N} \to 0$, $\max_{1\leq c \leq C}\frac{\lambda_{\max}(X_{c}^{(b)}\Sigma_{b}X_{c}^{(b) \tran})}{N} \to 0$ and $\lambda_{min}(X^{\tran}X)/N \geq c$ for some $c > 0$, as $N \to \infty.$ Then $\hat{\beta}_{\ols}$ is consistent.
\end{restatable}
\begin{restatable}{theorem}{mmconsis}
The method of moment estimates obtained using the above method is consistent if the fourth moments for $x_{ij}, i \in \{1, \cdots, R\}, \, j \in \{1, \cdots, C\} $ are uniformly bounded and $\hat{\beta}_{\ols}$ is consistent.
\end{restatable}
% To get rid of fixed terms, y was replaced by $y-X\hat\beta_{ols}$ in the above equations similar to \cite{2}.\\
\subsection{ $\Sigma_a$ and $\Sigma_b$ are unstructured}


If there are no constraints on the structure of $\Sigma_a$ and $\Sigma_b$, the problem of estimating covariance parameters becomes more complicated. We need $\Sigma_a$ and $\Sigma_b$ to be positive definite, and solving linear equations of $\Sigma_a$ and $\Sigma_b$, may not guarantee it. Thus, we need constrained optimization.
We could solve additional equations of the form
\begin{equation} \hat r^{\tran}(I-Q_r)(I-Q_a)\hat r = \Ex[r^{\tran}(I-P_i)(I-P_j)r]\end{equation}
for each r, s $\in \{0, \cdots, p\}$ to guarantee unique solution. The caveat with the approach is that we need an extremely large sample size to guarantee a solution.\\

We have implemented the method of moment approach for the case when covariances matrices are diagonal in $\ref{subsec.diagonal}$. The approach is found to provide empirically consistent estimates for the covariance parameters which are then utilized to estimate the fixed parameter using backfitting.
\section{Likelihood appoaches for estimating all the parameters}\label{sec.variational}
\subsection{Maximum likelihood estimation}
%An alternative method for determining the fixed-parameter $\beta$ and covariance parameters $\Sigma_a$, $\Sigma_b$, and $\sigma_e^2$ could involve using the maximum likelihood-based approach. We defined the
Here, we attempt to maximize the likelihood function for $(\beta, \Sigma_a, \Sigma_b, \sigma^2_e)$ in~(\ref{eq:likelihood}), and therefore the maximum likelihood estimate for ($\beta, \Sigma_a, \Sigma_b, \sigma^2_e$) is given by
\begin{equation}
\label{eq:mle}
\argmax_{\beta, \Sigma_a, \Sigma_b, \sigma^2_e} \frac{1}{(\sqrt{2\pi)^N|\mathcal{V}|}} \exp\left(-\frac{1}{2}(Y - X \beta )^{\tran} \mathcal{V}^{-1}(Y-X \beta)\right)
\end{equation}
with $\mathcal{V}$ defined as in (\ref{eq:V}). We showed in section \ref{sec.backfitting} that if covariance parameters are known, estimating $\hat\beta_{mle}$ using the definition is computationally expensive. We used backfitting to obtain $\hat\beta_{mle}$ in an efficient way. The problem becomes much more complicated if covariance parameters are unknown as the parameters, $\Sigma_a$, $\Sigma_b$, $\sigma_e^2$ are buried into $\mathcal{V}$ in a complex manner. \cite{lme4} developed a package, ``lme4" to implement maximum likelihood estimation for random effects model, though it is computationally expensive. We shall be comparing the efficiency of our proposed algorithm to ``lme4" ahead.
\subsection{Expectation Maximization Algorithm}
One potential solution to simplify the maximum likelihood problem analytically is using the EM algorithm. In our setup, EM would treat the $\{a_i\}_{i=1}^{R}$ and $\{b_j\}_{j=1}^{C}$ as unobserved/missing quantities. In this case, the complete log-likelihood is given by
\begin{equation}
\label{eq: complete_likelihood}
\begin{split}
l(Y;&\{a_i\}_{i=1}^{R} ;\{b_j\}_{j=1}^{C}) = const - \frac{1}{2} \left(N \log(\sigma^2_e) + R \log (|\Sigma_a|)+ C \log (|\Sigma_b|)\right)\\
&- \frac{1}{2} \left(\sum_{i=1}^R \sum_{j=1}^{C}\frac{z_{ij}(y_{ij} - x_{ij}^{\tran}(\beta + a_i +b_j))^2}{\sigma^2_e} + \sum_{i=1}^R a_i^{\tran} \Sigma^{-1}_a a_i + \sum_{j=1}^C b_j^{\tran} \Sigma_b^{-1} b_j\right).
\end{split}
\end{equation}
A part of this expression has occurred before when we discussed the efficient approach, backfitting to obtain $\hat\beta_{\gls}$ in section \ref{thm:robinson}. \\
\\
EM is an iterative algorithm that involves two steps:
\begin{itemize}
\item
\textbf{E-step}, which entails computing the expected value of the complete log-likelihood conditional on $Y$ and the current parameters, say $l(\theta)$.
\item \textbf{M-step}, which involves maximizing this expected value, $l(\theta)$ to generate new parameter estimates, $\hat{\theta}^{(k)}$.
\end{itemize}
In this setup, E-step would require computing $\Ex[ a_i a^{\tran}_{i}| Y]$, $\Ex[ b_j b^{\tran}_{j}| Y]$, and
\begin{equation}
\label{eq:em_residual}
\Ex\left[\sum_{i=1}^R \sum_{j=1}^C z_{ij}\left(y_{ij}- x^{\tran}_{ij}(\beta^{(k)} + a_i + b_j)\right)^2|Y\right].
\end{equation}

Computing (\ref{eq:em_residual}) is required for updating the estimates of $\sigma^2_e$ in the M-step which further involves computing $\Ex[a_i | Y],$ $\Ex[b_j | Y]$, and $\Ex[a^{\tran}_i b_j | Y]$ for $i \in \{0, \cdots, p\}$ and $ j \in \{0, \cdots, p\}$. Everything else is manageable except the terms $\Ex[a^{\tran}_i b_j | Y]$. Here, $(a_1,\cdots, a_R, b_1,\cdots b_C)$ follows a multivariate normal distribution conditional on $Y$ and the covariance matrix for the conditional distribution would only be obtained by inverting a matrix of order $(R+C)p \times (R+C)p$. %The problem arises because the covariance matrix of the conditional distribution is not sparse and the non-diagonal terms of the covariance matrix, i.e., $\Ex[a_i ^{\tran} b_{j} | Y]$ are required in the computation of $\sigma^2_e$ as the iterate for $\sigma^2_e$ in the M-step is given by
%\begin{equation}
%\left(\sigma^{(k)}_e\right)^2 =\Ex\left[ \frac{1}{N}\sum_{i=1}^R \sum_{j=1}^C z_{ij}\left(y_{ij}- x^{\tran}_{ij}(\beta^{(k)} + a_i + b_j)\right)^2|Y\right].
%\end{equation}
Thus, computation of $\sigma^2_e$ presents us with time complexity at least $\{(R+C)p\}^3$ which is greater than or equal to $N^{3/2} p^3$. Variational EM helps us out here as instead of considering the conditional distribution above, we consider such distributions for $(a_1,\cdots, a_R, b_1,\cdots b_C)$ under which $\{a_i\}_{i=1}^R$ are independent of $\{b_j\}_{j=1}^C$.
\subsection{Variational EM Algorithm}
Variational EM was introduced by~\cite{beal:zoubin:2003} and it is applicable when missing data has a graphical model structure as in our case, i.e., $a_i$ and $b_j$ are correlated conditional on $Y$. Since we couldn't directly estimate covariance estimates efficiently and consistently, variational EM helps us in improving the estimates iteratively. Though variational EM does not come with the guarantee of maximization of the likelihood, the estimates are found to be empirically consistent in our simulations. Variational EM also has a close connection with backfitting which we discuss further.
\subsubsection{Review of variational EM}
Let $\alpha$ denote the vector of unobserved quantities, $\theta$ denote the set of parameters, and $l(\theta)$ represent the likelihood function of $\theta$. In the E-step, we calculate the expectation of complete likelihood given $Y$, i.e., sufficient statistics of $\alpha$ get replaced by its expectation under the distribution $\Prob(\alpha|Y)$. Under factored variational EM, instead of finding the expectation of complete likelihood given $Y$, we substitute the conditional distribution of $\alpha$ given $Y$ with $q(\alpha)$ belonging in a restricted set $Q$ of probability distributions.\\
We define
\begin{equation} F(q,\theta) : = \int q(\alpha)\log \frac{\Prob(Y,\alpha|\theta)}{q(\alpha)} d \alpha \end{equation}
which we view as an appproximation to $l(\theta)$.
Here is the relation between $F(q,\theta)$ and $l(\theta)$,
\begin{align*}
F(q,\theta) & = \int q(\alpha)\log \frac{\Prob(Y,\alpha|\theta)}{q(\alpha)} d \alpha\\
& = \int q(\alpha)\log \frac{ \Prob(Y|\theta)\Prob(\alpha|Y,\theta)}{q(\alpha)} d \alpha\\
& = \int q(\alpha) \left\{\log \Prob(Y|\theta) + \log \left( \frac{\Prob(\alpha|Y,\theta)}{q(\alpha)}\right) \right\} d \alpha\\
& = l(\theta) - \mathrm{KL} \left(q|| \Prob(\alpha|Y,\theta)\right).
\end{align*}
As, $\mathrm{KL} \left(q|| \Prob(\alpha|Y,\theta)\right) \geq 0$, $F(q,\theta) \leq l(\theta)$ with the equality for the case $q = \Prob(\alpha|Y,\theta)$. At each variational E-step, we maximize $F(q,\theta) $ with respect to $q$ within the set $Q$. This implies that variational EM maximizes a lower bound to likelihood.
If $Q = \{q: q(\alpha) = \prod q_u(\alpha_u)\}$, it follows from~\cite{beal:zoubin:2003} that for the exponential family, optimal update for distribution of each component of $\alpha$ is given by
\begin{equation}
q_u^{(k)}(\alpha_u) \propto \Prob \left(Y, \alpha_u| \alpha_{-u} = \Ex_{Q^{(k-1)}} (\alpha_{-u})\right)
\end{equation}
Variational EM possesses the property that
\begin{equation}F(q^{(k-1)},\theta^{(k-1)}) \leq F(q^{(k)},\theta^{(k-1)}) \leq F(q^{(k)},\theta^{(k)}) \end{equation}
\subsubsection{Variational 	EM for the crossed random effects model with random slopes}
For the crossed random effects model (\ref{eq:random_slopes}), $\alpha$ would have two components, $\{a_i\}_{i=1}^R$ and $\{b_j\}_{j=1}^C$. Here, $\theta$ represents the set of parameters, ($\beta, \Sigma_a, \Sigma_b, \sigma^2_e$). We choose
\begin{equation}Q = \left\{ q: q(a_1,\cdots, a_R, b_1,\cdots b_C) = q_a(a_1, \cdots, a_R) q_b(b_1, \cdots, b_C)\right\}\end{equation}
\begin{restatable}{lemma}{varE}
\label{thm:var_E}
The optimal distributions for $\{a_i\}_{i=1}^R$ and $\{b_j\}_{i=1}^R$ in the variational E-step are given by \[ q^{(k)}_a(a_1, \cdots, a_R) = \prod_{i=1}^R q^{(k)}(a_i) \quad \mbox{ and } \quad q^{(k)}_b(b_1, \cdots, b_C) = \prod_{j=1}^C q^{(k)}(b_j) \]
with
\begin{equation}q^{(k)}(a_i) = N(\mu^{(k)}_{a,i}, \Sigma^{(k)}_{a,i}) \quad \mbox{and} \quad q^{(k)}(b_j) = N(\mu^{(k)}_{b,j},\Sigma^{(k)}_{b,j}) \end{equation}
where
\begin{equation}
\label{eq:var_mu_a}
\mu^{(k)}_{a,i} = \Sigma^{(k)}_{a,i} \left(\frac{\sum_{j=1}^{C}z_{ij}\big(y_{ij} - x_{ij}^{\tran}\big(\beta^{(k-1)} +\mu^{(k-1)}_{b,j}\big)\big)x_{ij}}{\left(\sigma^{(k-1)}_e\right)^{2}}\right)
\end{equation}
\begin{equation}
\label{eq:var_sigma_a}
\Sigma^{(k)}_{a,i} = \left((\Sigma^{(k-1)}_a)^{-1} +\frac{\sum_{j=1}^{C}z_{ij}x_{ij} x_{ij}^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2}\right)^{-1}
\end{equation}
with $\mu^{(k)}_{b,j}$ and $\Sigma^{(k)}_{b,j}$ having similar definitions.
\end{restatable}
\begin{proof}
The proof can be found in the appendix.
\end{proof}
It is important to note that while updating the distribution of $\{a_i\}_{i=1}^R$, we use parameters of the distribution of $\{b_j\}_{j=1}^C$ updated in the previous step and while updating the distribution of $\{b_j\}_{j=1}^C$, we use parameters of the distribution of $\{a_i\}_{i=1}^R$ recently updated. Thus, the mechanism of the update is very similar to the backfitting approach where we iterate through updates in $\{a_i\}_{i=1}^R$ and $\{b_j\}_{j=1}^C$.
Each of the quantities in (\ref{eq:var_mu_a}) and (\ref{eq:var_sigma_a}) can be computed in $\mathcal{O}(N)$ as computing $\mu^{(k)}_{a,i}$ and $\mu^{(k)}_{b,j}$ is equivalent to obtaining the estimates of $\{a_i\}_{i=1}^{R}$ and $\{b_j\}_{j=1}^{C}$ in each step of backfitting. \\

\textbf{Variational E step}: The expectation of the complete likelihood (\ref{eq: complete_likelihood}) can be computed with respect to the probability distribution $Q^{(k)}$, though we can skip to the M-step and fill in the details there.\\\

\textbf{M step}: The estimate
$\Sigma_a^{(k)}$ is given by
\begin{equation}
\label{eq:Sigma_a_k}
\begin{split}
\Sigma_a^{(k)} : & = \frac{1}{R}\Ex_{Q^{(k)}}\left[ \sum_{i=1}^R a_i a_i^{\tran}\right] \\
& = \frac{1}{R}\sum_{i=1}^R \left(\mu^{(k)}_{a,i} {\mu^{(k)}_{a,i}}^{\tran} + \Sigma^{(k)}_{a,i}\right)
\end{split}
\end{equation}
Similarly,
\begin{equation}
\label{eq:Sigma_b_k}
\Sigma_b^{(k)} = \frac{1 }{C}\sum_{j=1}^C\left(\mu^{(k)}_{b,j} {\mu^{(k)}_{b,j}}^{\tran} + \Sigma^{(k)}_{b,j}\right) \end{equation}
Also,
\begin{equation}
\label{eq:beta_k}
\beta^{(k)} = \bigg(\sum_{i=1}^R\sum_{j=1}^{C}z_{ij}x_{ij} x_{ij}^{\tran}\bigg)^{-1}\sum_{i=1}^{R}\sum_{j=1}^{C}z_{ij}x_{ij}\Ex\left(y_{ij} - x_{ij}^{\tran}\left(\mu^{(k)}_{a,i} + \mu^{(k)}_{b,j}\right)\right)\end{equation}
and finally
\begin{align*}
\left(\sigma^{(k)}_e\right)^2 & = \frac{1}{N}\sum_{i=1}^R \sum_{j=1}^C z_{ij} \Ex_{Q^{(k)}}\left[ \left(y_{ij}- x^{\tran}_{ij}(\beta^{(k)} + a_i + b_j)\right)^2\right]\\
& = \frac{1}{N}\sum_{i=1}^R \sum_{j=1}^C z_{ij}\left(\Ex_{Q^{(k)}}\left[\left(y_{ij}- x^{\tran}_{ij}(\beta^{(k)} + a_i + b_j)\right)\right]\right)^2\\
& \hspace{1.5cm}+ \frac{1}{N}\sum_{i=1}^R \sum_{j=1}^C z_{ij} \textit{Var}_{Q^{(k)}}\left[\left(y_{ij}- x^{\tran}_{ij}(\beta^{(k)} + a_i + b_j)\right)\right]\\
& = \frac{1}{N}\sum_{i=1}^R \sum_{j=1}^C z_{ij}\left(\Ex_{Q^{(k)}}\left[\left(y_{ij}- x^{\tran}_{ij}(\beta^{(k)} + a_i + b_j)\right)\right]\right)^2\\
& \hspace{1.5cm}+ \frac{1}{N}\sum_{i=1}^R \sum_{j=1}^C z_{ij} \textit{Var}_{Q^{(k)}}\left[ x^{\tran}_{ij}( a_i + b_j)\right]\\
& = \frac{1}{N}\sum_{i=1}^R \sum_{j=1}^C z_{ij}\left(y_{ij}- x^{\tran}_{ij}(\beta^{(k)} + \mu^{(k)}_{a,i} + \mu^{(k)}_{b,j})\right)^2\\
&\hspace{4cm} + \frac{1}{N}\sum_{i=1}^R \sum_{j=1}^C z_{ij} x^{\tran}_{ij} \left(\Sigma^{(k)}_{a,i} + \Sigma^{(k)}_{b,j}\right)x_{ij}
\end{align*}
The computational cost of the Variational E-step and M-step is $\mathcal{O}(Np^2)$.
\subsection{Connection of variational EM with backfitting}
If the covariance parameters were known or otherwise previously estimated, the goal of variational EM would be just to estimate $\beta$. In that case, optimal distribution for $\{a_i\}_{i=1}^R$ and $\{b_j\}_{i=1}^R$ in the variational E-step are given by \begin{equation}q^{(k)}(a_i) = N(\mu^{(k)}_{a,i}, \Sigma_{a,i}) \quad \mbox{and} \quad q^{(k)}(b_j) = N(\mu^{(k)}_{b,j},\Sigma_{b,j}) \end{equation}
where
\begin{equation}
\label{eq:var_mu_a_fixed}
\mu^{(k)}_{a,i} = \Sigma_{a,i} \left(\frac{\sum_{j=1}^{C}z_{ij}\big(y_{ij} - x_{ij}^{\tran}\big(\beta +\mu^{(k-1)}_{b,j}\big)\big)x_{ij}}{\sigma_e^{2}}\right)
\end{equation}
\begin{equation}
\label{eq:var_sigma_a_fixed}
\Sigma_{a,i} = \left(\Sigma_a^{-1} +\frac{\sum_{j=1}^{C}z_{ij}x_{ij} x_{ij}^{\tran}}{\sigma_e^2}\right)^{-1}
\end{equation}
with $\mu^{(k)}_{b,j}$ and $\Sigma_{b,j}$ having similar definitions. It is important to note that the variance of $\{a_i\}$ and $\{b_j\}$ are now fixed over the iterations, because the covariance parameters, $\Sigma_a$, $\Sigma_b$, and $\sigma^2_e$ are assumed to be known and fixed quantities. Equations (\ref{eq:var_mu_a_fixed}) and (\ref{eq:var_sigma_a_fixed}) imply that
\begin{equation}
\label{eq:a_i_em}
\mu^{(k)}_{a,i} = \left(\sigma^2_e\Sigma_a^{-1} +\sum_{j=1}^{C}z_{ij}x_{ij} x_{ij}^{\tran}\right)^{-1} \left(\sum_{j=1}^{C}z_{ij}\big(y_{ij} - x_{ij}^{\tran}\big(\beta +\mu^{(k-1)}_{b,j}\big)\big)x_{ij}\right).
\end{equation}
and similarly,
\begin{equation}
\label{eq:b_j_em}
\mu^{(k)}_{b,j} = \left(\sigma^2_e\Sigma_b^{-1} +\sum_{i=1}^{R}z_{ij}x_{ij} x_{ij}^{\tran}\right)^{-1} \left(\sum_{i=1}^{R}z_{ij}\big(y_{ij} - x_{ij}^{\tran}\big(\beta +\mu^{(k)}_{a,i}\big)\big)x_{ij}\right).
\end{equation}
The estimate of $\beta$ in E-step remains unchanged and is given by,
\begin{equation}
\label{eq:beta_em}
\beta^{(k)} = \bigg(\sum_{i=1}^R\sum_{j=1}^{C}z_{ij}x_{ij} x_{ij}^{\tran}\bigg)^{-1}\sum_{i=1}^{R}\sum_{j=1}^{C}z_{ij}x_{ij}\Ex\left(y_{ij} - x_{ij}^{\tran}\left(\mu^{(k)}_{a,i} + \mu^{(k)}_{b,j}\right)\right)\end{equation}
It is easy to see that update of $\mu^{(k)}_{a,i}$, $\mu^{(k)}_{b,j}$, and $\hat \beta$ in each step of variational EM (\ref{eq:a_i_em}) , (\ref{eq:b_j_em}) with known covariance parameters is same as update of $\hat a_{i}$, $\hat b_j$ and $\beta$ in each step of backfitting . This draws a close connection between variational EM and backfitting. We know that $\hat a_i \to \Ex[a_i|Y]$ and $\hat b_j \to \Ex[b_j|Y]$ over the iterations in backfitting which provides a plausible justification for variational EM.
\section{Error in estimating covariance matrices}\label{error_sigma_a_b}
To articulate the errors in estimating $\Sigma_a$ and $\Sigma_b$, we are using two metrics.
\begin{itemize}
\item \textbf{KL divergence}
We define the distance between $\Sigma_a$ and $\hat \Sigma_a$ in the following way. Let $\Prob_{1}$ denote the normal distribution with mean zero and covariance $\Sigma_a$ and $\Prob_{2}$ denote the normal distribution with mean zero and covariance $\wh\Sigma_a$, then we define the distance by
\begin{equation}\Ex_P\bigg[\log \bigg(\frac{d\Prob_{1}}{d\Prob_{2}}\bigg) \bigg] = \frac{1}{2}\Bigg[ \tr\bigg(\wh\Sigma_a^{-1}\Sigma_a\bigg) -\log(|\wh\Sigma_a^{-1}\Sigma_a|) -d\Bigg]\end{equation}
\item \textbf{Frobenium norm of difference between $\Sigma_a$ and $\wh{\Sigma}_a$}
\[\|\Sigma_a-\wh{\Sigma}_a\|_F\]
\end{itemize}
\section{Estimation of Covariance of $\hat{\beta}$}
Let $S_G$ denote the fit due to random components A and B, i.e., $S_G(Y) = X_A \hat{A} + X_B \hat{B}$, then
\begin{equation}\hat{\beta} = (X^{\tran} (I- S_G) X)^{-1} (X^{\tran} (I-S_G) Y)\end{equation}
We could apply the backfitting procedure to obtain an estimate of $S_G(X_j)$ given by $\tilde{S}_G(X_j)$for each covariate $X_j$.
Then, the estimate for $\beta$ will be given by:-
\begin{equation}\hat{\beta} = (X^{\tran} (I- \tilde{S}_G) X)^{-1} (X^{\tran} (I-\tilde{S}_G) Y)\end{equation}
Let $\tilde{X} = (I - \tilde{S}_G) X$, then estimate for covariance matrix of $\hat{\beta}$ will be given by:-
\begin{equation}(X^{\tran} \tilde{X})^{-1} \tilde{X}^{\tran} \widehat{\mathcal{V}} \tilde{X} (X^{\tran} \tilde{X})^{-1}\end{equation}
where $\widehat{\mathcal{V}}$ is an estimate for covariance matrix of $Y$.
\section{Results}
We would compare the time required for maximum likelihood estimation by ``lmer" (function to fit crossed random effects model in the package ``lme4") \cite{lme4} with our methods. We also illustrate the empirical consistency of our methods using simulations. Finally, we implement our method on Stitch Fix data and compare the results with the OLS fit and crossed random effects model with random intercepts implemented by ~\cite{ghos:hast:owen:2021}.
\subsection{Simulations}
\subsubsection{Diagonal case}\label{subsec.diagonal}
For our simulations, we choose $p = 3, \beta_0 = 0.1$, $\beta = (0.2, 0.3, 0.4)$, and $\sigma^2_e = 1$. We considered \[\Sigma_a = \begin{bmatrix}
0.3 & 0 & 0 & 0\\
0 & 0.3 & 0 & 0\\
0 & 0 & 0.3 & 0\\
0 & 0 & 0 & 0.3\\
\end{bmatrix}
\mbox{ }\Sigma_b = \begin{bmatrix}
0.1 & 0 & 0 & 0\\
0 & 0.1 & 0 & 0\\
0 & 0 & 0.1 & 0\\
0 & 0 & 0 & 0.1\\
\end{bmatrix}\]
We choose $R = C = N^{0.6}$ for our simulations which satisfies the sparsity assumption as seen in practics/e-commerce data.
Figure~\ref{fig:diag_sigma_ab} depicts empirical consistency of $\wh{\Sigma}_a$ and $\wh{\Sigma}_b$ estimated through method of moments as errors explained in $\ref{error_sigma_a_b}$ go to zero as $n \rightarrow \infty$. Figure~$\ref{fig:diag_beta_and_sigma2e}$ shows that the accuracy of different estimates of $\beta$, i.e., $\hat{\beta}_{\mathrm{GLS}}$ obtained though different approaches and $\hat{\beta}_{\mathrm{MLE}}$ coincide and thus $\hat{\beta}_{\mathrm{GLS}}$ is consistent. Figure~$\ref{fig:diag_beta_and_sigma2e}$ also depicts the empirical consistency of the estimate of $\sigma^2_e$ obtained through the method of moments approach. Although, the accuracy of maximum likelihood estimates is more than that of the method of moment estimates, figure~$\ref{fig:diag_estimation_time}$ shows that estimation time for the method of moments and backfitting algorithm is significantly lower compared to the maximum likelihood approach which makes it more applicable in settings with large sample size.
% Figure environment removed
% Figure environment removed
% Figure environment removed
\subsubsection{Non diagonal case}
For the non-diagonal case, we also choose $p = 3, \beta = (0.2, 0.3, 0.4), \beta_0 = 0.1, \sigma^2_e = 1$. We considered \[\Sigma_a = \Sigma_b = \begin{bmatrix}
1 & 0.2 & 0.2 & 0.2\\
0.2 & 1 & 0.2 & 0.2\\
0.2 & 0.2 & 1 & 0.2\\
0.2 & 0.2 & 0.2 & 1\\
\end{bmatrix}\]
We choose $R = C = N^{0.6}$ for our simulations similar to the diagonal case. We have proposed the following algorithms
\begin{itemize}
\item \textbf{Backfitting:} We estimate covariance parameters using method of moments and then perform backfitting to estimate the fixed parameter.
\item \textbf{Clubbed Backfitting:} We estimate covariance parameters using method of Moments and then perform backfitting with clubbing to estimate the fixed parameter.
\item \textbf{Variational EM:} We use the method of Moments to get an initial estimate of covariance parameters and then we improve the estimates in each iteration using the variational M-step along with estimating fixed parameters.
\item \textbf{Clubbed Variational EM:} Similar to Variational EM, we start with the method of Moment estimates for covariance parameters and improvise them using variational EM. We apply ``clubbing" trick while estimating $\beta$ to fasten the convergence.
\end{itemize}
% Figure environment removed
Figure~\ref{fig:sigma_ab} depicts empirical consistency of $\wh{\Sigma}_a$ and $\wh{\Sigma}_b$ as errors explained in $\ref{error_sigma_a_b}$ go to zero as $n \rightarrow \infty$. Figure~$\ref{fig:beta_and_sigma2e}$ shows that the accuracy of different estimates of $\beta$, i.e., $\hat{\beta}_{\mathrm{GLS}}$ obtained though different approaches and $\hat{\beta}_{\mathrm{MLE}}$ coincide and thus $\hat{\beta}_{\mathrm{GLS}}$ is consistent. The estimates of $\sigma^2_e$ obtained through the method of moments approach and variational EM depict empirical consistency in the figure~$\ref{fig:beta_and_sigma2e}$ although the estimate obtained through variational EM has a faster rate of convergence. The figure~$\ref{fig:estimation_time_niter}$ shows that our variational EM and backfitting approach take a great lead compared to the ``lmer" approach. The average time complexity of ``lmer" approach is at least $\mathcal{O}(N^{1.5})$ in our setup, while all the approach discussed by us have time complexity $\mathcal{O}(N)$. The number of iterations required to converge grows with $N$ for ``lmer" and the approaches without clubbing, while the number of iterations decreases with N for clubbing-based approaches. Here is the summary of results:
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
& Without Clubbing & With Clubbing\\
\midrule
Backftitting & \tabitem Slow convergence & \tabitem Fast Convegence\\
with MoM & \tabitem Poor estimates for $\Sigma_a, \Sigma_b$ & \tabitem Poor estimates for $\Sigma_a, \Sigma_b$\\
\hline
Variational EM & \tabitem Slow convergence & \tabitem Fast Convegence\\
& \tabitem Good estimates for $\Sigma_a, \Sigma_b$ & \tabitem Good estimates for $\Sigma_a, \Sigma_b$\\
\hline
\end{tabular}
\end{table}
% Figure environment removed
% Figure environment removed


Thus, Clubbed Variational EM provides precise estimates of all the covariance parameters with a small estimation time.
\subsection{Real data}
~\cite{ghos:hast:owen:2021} applied the backfitting algorithm to obtain $\hat{\beta}_{\gls}$ for the Stitch Fix data for the random intercepts model $(\ref{eq:intercept_model})$. Here, we demonstrate how our algorithm can be utilized to apply two crossed-random effects models to the stitch-fix data. The first model we implement considers a random intercept and a random slope for a variable named ``match", which is a continuous variable taking values in the range 0 to 1 representing the prediction of the probability that the client will keep the item purchased. The second model we implement considers random intercept and random slopes for the variable ``match" and multiple indicator variables named ``client edgy", ``client boho", ``item edgy", ``item boho", etc. In both of the models, we assume multivariate normal distributions for the random effects for clients as well as items. It should be noted that the purpose of this section is not to choose the most appropriate random effects model, but we wish to illustrate the application of our algorithm in a random slope setup, thus the choice of the set of variables with random slope is arbitrary. Along with fitting the models, we also measured the prediction accuracy of these models and compared them with ordinary least squares and the random intercept model.
\subsubsection{Random slope for the variable ``match"}
Here, we implement our devised algorithm to fit the random effects model with a random slope considered for the ``match" variable. Thus, the model considered here is,
\[y_{ij} =\tilde{x}^{\tran}_{ij}(a_i +b_j) + x_{ij}^{\tran}\beta + e_{ij} \hspace{0.5cm} \forall i \in \{1,\cdots, R\} \hspace{0.3cm} j\in \{1,,\cdots,C\}, \, \]
where, $\beta$ denotes the fixed effect for the entire covariate vector, $x_{ij}$ listed in table~$\ref{Tab:stitch_fix}$. We assume that ${a}_i \sim N_2(0, {\Sigma}_a)$ and ${b}_j \sim N_2(0, {\Sigma}_b)$ represent random effects for $\tilde{x}_{ij} = (1, \mathrm{match}_{ij})$. The estimate for fixed effect is tabulated in table~$\ref{Tab:stitch_fix}$ as $\hat{\beta}_{\gls,1}$. The estimate obtained for $\sigma^2_e$ is 4.682 and that for $\Sigma_a$ and $\Sigma_b$ is
$$\wh{\Sigma_a} = \begin{pmatrix} 1.817 & -1.130\\
-1.130 & 1.447\\
\end{pmatrix}
\hbox{ and } \wh{\Sigma_b} = \begin{pmatrix}
0.810 & -1.574\\
-1.574 & 4.163
\end{pmatrix}$$


~\cite{ghos:hast:owen:2021} defined naivete and inefficiency of $\ols$ with respect to the random intercept model to establish the utility of fitting a random effects model. Here we carry the work forward by displaying the naivety and inefficiency of $\ols$ with respect to the random effects model discussed above. The values of naivete range from 2.63 to 912.36. These values are larger than naivety values reported in ~\cite{ghos:hast:owen:2021} as the largest value reported by them was 345.28. The largest and second-largest ratios are for material indicators corresponding to Modal and Tencel using the random effects model too. We also identified the linear combination of $\hat{\beta}_{\ols}$ for which OLS is most naive. We maximize the ratio $ x^{\tran} \wh\cov_{\gls,1}(\hat\beta_{\ols})x/x^{\tran} \wh\cov_{\ols}(\hat{\beta}_{\ols})x$ over $x \ne 0$. The resulting maximal ratio is the largest eigenvalue of $\wh\cov_{\ols}(\hat\beta_{\ols})^{-1}\wh\cov_{\gls,1}(\hat\beta_{\ols})$ and it is about 984 while the ratio provided for random intercept model in ~\cite{ghos:hast:owen:2021} was 361. We also quantified inefficiency similar to~\cite{ghos:hast:owen:2021}. The figure~$\ref{fig:naivety_inefficiency}$ plots the naivety and inefficiency values. They range from just over 1.06 to 32.03 and can be interpreted as factors by which using OLS reduces the effective sample size. the ``match" variable is found to be an outlier in terms of efficiency in our setup too. An important thing to note here is that though OLS is much more naive with respect to the random slopes model compared to the random intercept one, the difference in efficiency is not that huge.
\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
& $\hat\beta_{\ols}$ & $\hat\beta_{\gls.0}$ &  $\hat\beta_{\gls.1}$ & $\hat\beta_{\gls.2}$ \\ \hline
Intercept & 4.635* & 5.103* & 5.148* & 5.134* \\ \hline
Match & 5.048* & 3.442* & 3.306* &  3.336* \\ \hline
$\mathbb{I}\{\text { client edgy }\}$ & 0.00102 & 0.00304  & -0.00038 & 0.00038* \\ \hline
$\mathbb{I}\{\text { item edgy }\}$& -0.3358* & -0.3515* & -0.3660* & -0.3916* \\ \hline
$\mathbb{I}\{\text { client edgy }\}$ &  & & & \\
$* \mathbb{I}\{\text { item edgy }\}$ &$0.3925^{*}$&$0.3793^{*}$& $0.3775^{*}$&0.4132*\\\hline
	% I(client edgy) & \multirow{2}{*}{0.3925*} & \multirow{2}{*}{0.3793*} & \multirow{2}{*}{0.005916} & \multirow{2}{*}{0.3775*} & \multirow{2}{*}{0.00607} \\
% I(item edgy) & & & & & \\ \hline
$\mathbb{I}\{\text { client boho }\}$ & 0.1386* & 0.1296*  & 0.1313* & 0.1399* \\ \hline
$\mathbb{I}\{\text { item boho }\}$ & -0.5499* & -0.6266* & -0.6269* &  -0.6670*\\ \hline
$\mathbb{I}\{\text { client boho }\}$ & &  & & \\
$* \mathbb{I}\{\text { item boho }\}$ &$0.3822^{*}$ &$0.3763^{*}$ & $0.3734^{*}$& 0.4137*\\ \hline
% I(client boho) & \multirow{2}{*}{ 0.3822*} & \multirow{2}{*}{0.3763*} & \multirow{2}{*}{0.007123} & \multirow{2}{*}{0.3734*} & \multirow{2}{*}{0.00730} \\
% I( item boho ) & & & & & \\ \hline
Acrylic & -0.06482* & -0.00536 & 0.0110 & 0.03440* \\ \hline
Angora & -0.0126 & 0.07486 & 0.09617 & 0.10050* \\ \hline
Bamboo & -0.04593 & 0.03251 & 0.07282 & 0.07346* \\ \hline
Cashmere & -0.1955* & 0.00893 & 0.02474 & 0.05574* \\ \hline
Cotton & 0.1752* & 0.1033* & 0.1191* & 0.1261* \\ \hline
Cupro & 0.5979 & 0.2089 & 0.0530 & 0.0749 \\ \hline
Faux Fur & 0.2759* & 0.2749 &  0.2832* & 0.2914* \\ \hline
Fur & -0.2021* & -0.07924 & -0.07745 & -0.14690* \\ \hline
Leather & 0.2677* & 0.1674* &  0.1569 & 0.1536* \\ \hline
Linen & -0.3844* & -0.08658 & -0.0844 & 0.1160* \\ \hline
Modal & 0.0026 & 0.1388* & 0.1316 & 0.1436* \\ \hline
Nylon & 0.0335* & 0.08174 & 0.0899 & 0.0826* \\ \hline
Patent Leather & -0.2359 & -0.3764 & -0.5184 & 0.3763 \\ \hline
Pleather & 0.4163* & 0.3292* & 0.3504* & 0.3516* \\ \hline
PU & 0.416* & 0.4579* & 0.4517* & 04618* \\ \hline
PVC & 0.6574* & 0.9688* & 0.8535 & 0.7729* \\ \hline
Rayon & -0.01109* & 0.05155* & 0.0637* & 0.05638* \\ \hline
Silk & -0.1422* & -0.1828* & -0.2160* & -0.2022* \\ \hline
Spandex & -0.3916* & 0.414* & 0.4074* & 0.4099* \\ \hline
Tencel & 0.4966* & 0.1234* & 0.1176 & 0.1422* \\ \hline
Viscose & 0.04066* & -0.02259 & -0.03238 & -0.03795* \\ \hline
Wool & -0.0602* & -0.05883 & -0.06684 & -0.04681* \\ \hline
\end{tabular}
\caption{Stitch fix results}
\label{Tab:stitch_fix}
\end{table}
% Make csv in question
% Figure environment removed
%\subsubsection{Random slope for the variable ``client edgy"}
%The estimate obtained for $\sigma^2_e$ is 4.96 and that for $\Sigma_a$ and $\Sigma_b$ is
%$$\wh{\Sigma_a} =
%\begin{pmatrix}
%1.063 & -0.855\\
%-0.855 & 1.768
%\end{pmatrix}
%\hbox{ and } \wh{\Sigma_b} =\begin{pmatrix}
%0.199 & -0.013 \\
%-0.013 & 0.022
%\end{pmatrix}$$
\subsubsection{Random slope for variables ``match", ``I(client edgy)", ``I(item edgy)", ``I(client edgy)I(item edgy)", ``I(client boho)", ``I(item boho)", ``I(client boho)I(item boho)"}
Thus, the model considered here is
\begin{equation}
\label{eq:Model_2}
y_{ij} = \beta_0+\tilde{x}^{\tran}_{ij}(a_i +b_j) + x_{ij}^{\tran}\beta + e_{ij} \hspace{0.5cm} \forall i \in \{1,\cdots, R\} \hspace{0.3cm} j\in \{1,,\cdots,C\}. \,
\end{equation}
where, $\beta$ denotes the fixed effect for the entire covariate vector, $x_{ij}$ similar as above. We assume that ${a}_i \sim N_8(0, {\Sigma}_a)$ and ${b}_j \sim N_8(0, {\Sigma}_b)$ represent random effects for
\begin{align*}
\tilde{x}_{ij} & = (1, \mathrm{match}_{ij},\text{I(client edgy})_i,\text{I(item edgy})_j, \text{I(client edgy})_i \text{I(item edgy})_j,\\
& \hspace{2cm} \text{I(client boho)}_i,\text{I(item boho)}_j, \text{I(client boho)}_i \text{I(item boho)}_j)
\end{align*}
The estimate obtained for $\sigma^2_e$ is 5.017 and that for $\Sigma_a$ and $\Sigma_b$ is
$$
\wh{\Sigma_a} =
\begin{pmatrix}
1.044 &-0.223 &-0.296 &0.015 &-0.053 &-0.324 &0.036 &-0.023\\
-0.223 &0.912 &-0.091 &0.011 &0 &-0.145 &0.012 &-0.001\\
-0.296 &-0.091 &0.747 &-0.029 &-0.022 &0.003 &-0.009 &0.006\\
0.015 &0.011 &-0.029 &0.426 &-0.232 &-0.01 &-0.003 &-0.001\\
-0.053 &0 &-0.022 &-0.232 &0.59 &0.005 &-0.008 &0.001\\
-0.324 &-0.145 &0.003 &-0.01 &0.005 &0.753 &-0.02 &0.005\\
0.036 &0.012 &-0.009 &-0.003 &-0.008 &-0.02 &0.563 &-0.278\\
-0.023 &-0.001 &0.006 &-0.001 &0.001 &0.005 &-0.278 &0.647
\end{pmatrix}
$$
$$
\wh{\Sigma_b} = \begin{pmatrix}
0.838 & -1.555 & 0.004 & -0.273 & 0.010 & -0.016 & -0.216 & -0.014\\
-1.555 & 3.872 & -0.030 & 0.330 & 0.000 & -0.014 & 0.188 & 0.057\\
0.004 & -0.030 & 0.017 & 0.004 & -0.016 & 0.000 & -0.002 & -0.002\\
-0.273 & 0.330 & 0.004 & 0.345 & -0.032 & 0.011 & 0.064 & 0.007\\
0.010 & 0.000 & -0.016 & -0.032 & 0.055 & -0.003 & -0.002 & 0.003\\
-0.016 & -0.014 & 0.000 & 0.011 & -0.003 & 0.017 & 0.014 & -0.015\\
-0.216 & 0.188 & -0.002 & 0.064 & -0.002 & 0.014 & 0.333 & -0.055\\
-0.014 & 0.057 & -0.002 & 0.007 & 0.003 & -0.015 & -0.055 & 0.059
\end{pmatrix} $$
Similar to the first model, we also display the naivete and inefficiency of $\ols$ with respect to the random effects model \ref{eq:Model_2} discussed above. The values of naivete range from 3.80 to 740.93. The largest and second-largest ratios are for material indicators corresponding to Modal and Tencel using this random effects model too. We also identified the linear combination of $\hat{\beta}_{\ols}$ for which OLS is most naive. We maximize the ratio $ x^{\tran} \wh\cov_{\gls,1}(\hat\beta_{\ols})x/x^{\tran} \wh\cov_{\ols}(\hat{\beta}_{\ols})x$ over $x \ne 0$ similar as above. The resulting maximal ratio is the largest eigenvalue of $\wh\cov_{\ols}(\hat\beta_{\ols})^{-1}\wh\cov_{\gls,1}(\hat\beta_{\ols})$ and it is about 805.59. The figure~$\ref{fig:naivety_inefficiency}$ plots the naivety and inefficiency values. They range from just over 1.02 to 27.34 and can be interpreted as factors by which using OLS reduces the effective sample size. the ``match" variable is found to be an outlier in terms of efficiency using this model too. Comparing the naivety and inefficiency values with the previous model, the model with just a random slope for ``match" appears to be more effective as it providuces much larger naivety values.
% Figure environment removed
\subsection{Prediction error}
We considered a random train-test split of stitch-fix data. We divide the test data into three categories:
\begin{itemize}
\item \textbf{Repeat client and repeat item:} The category includes test points with clients whose information was seen in the training data and items whose information was also available in the training data. The prediction of the rating was given by
\[\hat{Y}_{ij} = x_{ij}^\tran (\hat \beta + \hat a_i + \hat b_j)\]
where $\hat \beta, \hat a_i,$ and $\hat b_j$ are the estimates of $\beta$ and BLUP estimates of $a_i$ and $b_j$ repectively.
\item \textbf{New client and repeat item:} The category includes test points with clients whose information was not seen in the training data and items whose information was available in the training data. The prediction of the rating was given by
\[\hat{Y}_{ij} = x_{ij}^\tran (\hat \beta + \hat b_j),\]
as the prior expectation for $a_i$ is zero.
\item \textbf{Repeat client and new item:} The category includes test points with clients whose information was seen in the training data and items whose information was not available in the training data. The prediction of the rating was given by
\[\hat{Y}_{ij} = x_{ij}^\tran (\hat \beta + \hat a_i),\]
as the prior expectation for $b_j$ is zero.
\end{itemize}
We observed only two test data points in the third category, so we are only reporting prediction errors in the first two categories. In the table $\ref{table:prediction_errors}$, $\gls_0$ represent the crossed random effects model with random intercepts, $\gls_1$ represent the crossed random effects model with random intercepts and random slope for the variable ``match", and $\gls_2$ represent the crossed random effects model with random intercept and random slope for the variable ``match" and random slope for multiple indicator variables discussed above.
\begin{table}[h]
\centering
\begin{tabular}
{|c c|c|c|c|}
\hline
& $\ols$ & $\gls_0$ & $\gls_1$ & $\gls_2$ \\
Repeat client and repeat item & 5.710125 & 4.876266 & 4.867539 & 5.525144 \\
New client and repeat item & 5.682390 & 5.604378 & 5.543491 & 5.749653\\
\hline
\end{tabular}
\caption{Prediction error for different models among different categories}
\label{table:prediction_errors}
\end{table}
For the first category, we observe 99,115 data points and for the second category, we observe 883 data points. $\gls_0$ provides a significant improvement over $\ols$ in the first category but the improvement is not much significant for the second category as the information for the clients is not available in the training data. $\gls_1$ provides a small improvement over $\gls_0$ in both of the categories. However, $\gls_2$ provides worse performance compared to $\gls_0$ which reflects a possible overfitting. Thus, if we were to compare models $\gls_0$, $\gls_1$, and $\gls_2$, $\gls_1$ comes out to be the best one. The results are consistent with the naivety and inefficiency values we discussed before.
\section{Conclusion}
In the paper, we discussed estimation under crossed random effects model with random slopes. We suggested two different algorithms, based on the method of moments combined with backfitting and variational EM. The former is most appropriate when covariance matrices for random effects are diagonal and the latter is useful when the covariance matrices are unstructured. The algorithm based on the method of moments was shown to have theoretical as well as empirical consistency to estimate fixed effects and covariance parameters when covariance matrices for random effects were diagonal. The algorithm based on variational EM was able to accomplish a more difficult task which is to estimate the parameters when the covariance matrices are unstructured and showed great empirical results. Both of the algorithms could complete the task to estimate fixed and covariance parameters in $\mathcal{O}(N)$ and are superior to ``lmer" which takes at least $\mathcal{O}(N^{3/2})$. The algorithms find huge applications in settings with large sample sizes and where the time taken to obtain the fit is crucial. We depicted the performance of our algorithms using simulations as well as real data.
\bibliographystyle{apalike}
\bibliography{bigdata}
\appendix
\section{Some proofs}
\subsection{Proof of Theorem~\ref{thm:robinson}}\robin*
where
\begin{equation*}
\hat\beta_{\gls} = \argmin_\beta (Y-X\beta)\mathcal{V}^{-1}(Y -X\beta)=(X^{\tran}\mathcal{V}^{-1}X)^{-1}X^{\tran}\mathcal{V}^{-1}Y \tag{\ref{eq:beta}}
\end{equation*}
\begin{proof}
We rewrite the model as
\[Y=X \beta + Z U + \epsilon \, \, \text{ where, } U \sim N(0, \Sigma_u), \text{ and } \epsilon \sim N(0, \sigma^2_e I)\]
Then,
\[P_r(Y = y, U=u) = P_r(Y = y) P_r(U=u| Y =y) = P_r(Y =y|U = u)P_r(U = u) \]
$ P_r(Y = y)$ corresponds to the likelihood in $(\ref{eq:likelihood})$ and $\hat\beta_{\gls}$ in $(\ref{eq:beta})$, and $ P_r(Y =y|U = u) P_r(U = u)$ corresponds to the exponent of $(\ref{eq:loss}$). Now,
\[P_r(U=u|Y = y) = \frac{1}{\sqrt{|Var(U|y)|}} \exp \left(-\frac{(u-\Ex[U|y])^{\tran} \left[Var(U|y)\right]^{-1} (u-\Ex[U|y])}{2} \right) \]
which is maximised for $u=\Ex[U|Y = y]$, and
$$\max_{u} P_r(U=u|Y = y) = \frac{1}{(2\pi)^{(p+1)/2}\sqrt{|Var(U|y)|}} = \mbox{a constant }$$
\[\max_{u} P_r(y, U=u) = \max_{u} P_r(Y = y) P_r(U=u|Y = y) = \mbox{ constant } p(y)\]
Therefore,
\[\argmax_{\beta} \max_{u} P_r(Y = y, U=u) = \argmax_{\beta} P_r(Y = y) \frac{1}{\sqrt{|Var(U|y)|}}\]
It is understood that $Var(U|y)$ is constant in terms of $\beta$, which implies,
\[\argmax_{\beta} \max_{u} P_r(Y = y, U=u) = \argmax_{\beta} P_r(Y = y) \]
which is equivalent to saying
$$\hat\beta = \hat\beta_{\gls}$$
\end{proof}
\subsection{Consistency of $\hat{\beta}_{\ols}$}
\betaols*
\begin{proof}
To begin with, we assume the observations are arranged by the level of the first factor.
$Y = X\beta + \epsilon$ where $\epsilon \sim N(0,\mathcal{V})$ for
\begin{align*}
\mathcal{V}& = \underbrace{diag(X_{1}^{(a)}\Sigma_{a}X_{1}^{(a) \tran},\cdots,X_{R}^{(a)}\Sigma_{a}X_{R}^{(a) \tran})}_{B_{R}} \\
& \hspace{2cm}+ P_{C} \underbrace{diag(X_{1}^{(b)}\Sigma_{b}X_{1}^{(b) \tran},\cdots,X_{C}^{(b)}\Sigma_{b}X_{C}^{(b)\tran})}_{B_{C}} P_{C}^{\tran} + \sse\bf{I}_{N}
\end{align*}
with $P_{C}$ being an appropriate permutation matrix. Note that $\hat{\beta}_{\ols} = \beta + (X^{\tran}X)^{-1}X^{\tran}\epsilon.$ $\Ex((X^{\tran}X)^{-1}X^{\tran}\epsilon) = \bf{0}.$
Let $w$ be any $p \times 1$ vector with finite norm.
\begin{align*}
\mathrm{Var}(w^{\tran}(X^{\tran}X)^{-1}X^{\tran}\epsilon) &= w^{\tran}(X^{\tran}X)^{-1}X^{\tran}\mathcal{V}X(X^{\tran}X)^{-1}w\\
&\leq \lambda_{\max}(B_{R}) w^{\tran}(X^{\tran}X)^{-1}w \\
& \hspace{0.2cm}+ \lambda_{\max}(P_CB_{C}P_C^{\tran}) w^{\tran}(X^{\tran}X)^{-1}w + \sse w^{\tran}(X^{\tran}X)^{-1}w \\
&\leq \lambda_{\max}(B_{R}) \frac{\Vert w \Vert^{2}}{cN} + \lambda_{\max}(P_CB_{C}P_C^{\tran}) \frac{\Vert w \Vert^{2}}{cN} + \sse \frac{\Vert w \Vert^{2}}{cN}\\
&\leq \lambda_{\max}(B_{R}) \frac{\Vert w \Vert^{2}}{cN} + \lambda_{\max}(B_{C}) \frac{\Vert w \Vert^{2}}{cN} + \sse \frac{\Vert w \Vert^{2}}{cN} \to 0
\end{align*}
So $\hat{\beta}_{\ols}$ is consistent estimator for $\beta$.
\end{proof}
\subsection{Method of Moments}{\label{appendix.mom}}
\mmconsis*
\begin{proof}
Each of the equations (\ref{eq:mm1}), (\ref{eq:mm2}), and (\ref{eq:mm3}) can be represented as
\begin{equation}\Ex \bigg[ r^{\tran} (I - Q) r\bigg] = \hat r^{\tran} (I - Q) \hat r \text{ for some } Q \in \mathbb{R}^{N \times N}.
\label{eq:matrix_form}
\end{equation}
For example, $Q$ for $(\ref{eq:mm1})$ is given by
\begin{equation}
\label{eq:Q}
Q = \begin{bmatrix}
\mathlarger{\frac{{x^{(s)}_{1.}}x^{(s)^{\tran}}_{1.}}{\|{x^{(s)}_{1.}}\|^2}} & 0 & \cdots & 0 \\
0 & \mathlarger{\frac{{x^{(s)}_{2.}}x^{(s)^{\tran}}_{2.}}{\|{x^{(s)}_{2.}}\|^2}} & \cdots & 0 \\
0 & 0 & \ddots & 0\\
0 & 0 & \cdots & \mathlarger{\frac{{x^{(s)}_{R.}}x^{(s)^{\tran}}_{R.}}{\|{x^{(s)}_{R.}}\|^2}}\\
\end{bmatrix}
\end{equation}
It is sufficient to show that $(\Ex[r^{\tran}(I-Q)r|X]- \hat{r}^{\tran}(I-Q) \hat{r})/N$ converges to zero $ \forall Q \in \{P_1, P_2, \cdots, P_p, P'_1, \cdots, P'_p, J_n \} \text{ and } \forall \, \, \Sigma_a, \Sigma_b, \sigma^2_e $ where $J_n = 1_n 1_n^{\tran} / n$.
\begin{align*}
\frac{\hat{r}^{\tran}(I-Q) \hat{r}}{N} & =\frac{ (\hat{r}-r+r)^{\tran}(I-Q) (\hat{r} - r + r)}{N}\\
& = \frac{r^{\tran}(I-Q)r + (\hat{r}-r)^{\tran}(I-Q) (\hat{r} - r) + 2 r^{\tran} (I-Q) (\hat{r} - r)}{N} \\
& = \frac{r^{\tran}(I-Q)r}{N} + \frac{(\hat{\beta}_{\ols}-\beta)^{\tran} X^{\tran}(I-Q) X (\hat{\beta}_{\ols} - \beta)}{N}\\
& \hspace{5cm} - \frac{2 r^{\tran} (I-Q) X (\hat{\beta}_{\ols} - \beta)}{N}
\end{align*}
As $x_{ij}$ have finite fourth moment $\forall i \in \{1, \cdots, R\} \, j \in \{1, \cdots, C\} $, $X^{\tran} X /N = O_p(1) $, and as $\hat{\beta}_{\ols}$ is consistent, second and third terms converge to zero in probability.
Now,
\begin{align*}
\frac{(\hat{r}^{\tran}(I-Q) \hat{r}-\Ex[r^{\tran}(I-Q)r|X])}{N} & = \frac{r^{\tran}(I-Q)r}{N} - \frac{\Ex[r^{\tran}(I-Q)r|X])}{N} + o_p(1)
\end{align*}
Now, \begin{equation} \frac{r^{\tran}(I-Q)r - \Ex[r^{\tran}(I-Q)r]}{N} \to 0 \end{equation}
and
\begin{equation} \frac{\Ex[r^{\tran}(I-Q)r|X] - \Ex[r^{\tran}(I-Q)r]}{N} \to 0 \end{equation}
by the strong law of large numbers assuming finite fourth moment of $x_{ij}$ $\forall i \in \{1, \cdots, R\} $ and $ j \in \{1, \cdots, C\} $.
Let, $\theta = (diag(\Sigma_a),diag(\Sigma_b),\sigma^2_e)$, then method of moment estimate, $\theta_n $ is solution of equations in the form $A_n \theta = B_n$, i.e. , $\theta_n = A_n^{-1} B_n$. The above proof guarantees that $A_n \to A$ and $B_n \to A \theta, \, \, \forall \theta$. Therefore, $\theta_n \to \theta$.\\
\end{proof}

\subsection{Proof of~(\ref{eq:simplify})}
We know from (\ref{eq:random_slopes}) that
\[r_{ij} = x_{ij}^{\tran} (a_i + b_j) + \epsilon_{ij}\]
It could also be represented as
\[r = \sum_{s'=0}^{p}X_A^{(s')}A_{s'}+\sum_{s'=0}^{p}X_B^{(s')}B_{s'} + \epsilon\]
Here, $A_{s'} \in \mathbb{R}^R$ and $B_{s'} \in \mathbb{R}^C$ represent stacked random coefficient of s'-th coordinate for clients and items respectively, and $X_A^{(s')}$ and $X_B^{(s')}$ represent respective design matrices. Then, we have\\
\begin{align*}
&\Ex\bigg[\sum_{i=1}^{R} \sum_{j=1}^{C} z_{ij} r^2_{ij}-\sum_{i=1}^{R}\frac{\left(\sum_{j=1}^{C} z_{ij} r_{ij}x^{(s)}_{ij}\right)^2}{\sum_{j=1}^{C} z_{ij}x^2_{ij(s)}} \bigg{|} X\bigg] = \Ex[r^{\tran}(I-Q)r]\\
& = \Ex\left(\sum_{s'=0}^{p}X_A^{(s')}A_{s'}+\sum_{s'=0}^{p}X_B^{(s')}B_{s'} + \epsilon\right)^{\tran}\left(I -Q\right)\left(\sum_{s'=0}^{p}X_A^{(s')}A_{s'}+\sum_{s'=0}^{p}X_B^{(s')}B_{s'} + \epsilon\right)\\
& = \sum_{s'=0}^{p} E\bigg[(X_A^{(s')}A_{s'})^{\tran}(I-Q)(X_A^{(s')}A_{s'})\bigg]+\sum_{s'=0}^{p} E\bigg[(X_B^{(s')}B_s')^{\tran}(I-Q)(X_B^{(s')}B_{s'})\bigg] \\
& \hspace{11cm} + \sigma^2_e (n-R)\\
& = \sum_{s'=0}^{p}\tr\bigg[X_A^{\left(s'\right)^{\tran}}\left(I-Q\right)X_A^{\left(s'\right)}\bigg]\Sigma_{a,s's'} +\sum_{s'=0}^{p} \tr\bigg[X_B^{\left(s'\right)^{\tran}}\left(I-Q\right)X_B^{\left(s'\right)}\bigg]\Sigma_{b,s's'}\\
& \hspace{11cm} + \sigma^2_e (n-R)\\
&=\sum_{s'=0}^{p} \sum_{i=1}^{R}\left[ \sum_{j=1}^{C} z_{ij} x^{(s')^2}_{ij}-\frac{\left(\sum_{j=1}^{C} z_{ij} x^{(s)}_{ij} x^{(s')}_{ij} \right)^2}{\sum_{j=1}^{C} z_{ij} x^{(s)^2}_{ij}}\right]\Sigma_{a,s's'} \\
& \hspace{3cm}+\sum_{s'=0}^{p}\sum_{i=1}^R\left[ \sum_{j=1}^{C}z_{ij} x^{(s')^2}_{ij} -\frac{\sum_{j=1}^{C}z_{ij} x^{(s)^2}_{ij} x^{(s')^2}_{ij}}{\sum_{j=1}^{C} z_{ij} x^{(s)^2}_{ij}}\right]\Sigma_{b,s's'} + \sigma^2_e (n-R)\\
\end{align*}
The first part of the last equality follows directly from the definition of $Q$, the last part could be shown using the following argument:
\\
Let, \[X_B^{(s')} = \begin{bmatrix}
W_1\\
W_2\\
\vdots\\
W_R
\end{bmatrix}\]
\begin{align*}
\tr\bigg[\left(X_B^{(s')}\right)^{\tran}(I-Q)\left(X_B^{(s')}\right)\bigg] & = \sum_{i=1}^R \tr\bigg[W^{\tran}_i \left(I - \frac{x^{(s)}_{i.}{x^{(s)}_{i.}}^{\tran}}{\|x^{(s)}_{i.}\|^2}\right)W_i\bigg]\\
& = \sum_{i=1}^R \tr(W^{\tran}_i W_i) - \frac{\tr\left(x^{(s)^{\tran}}_{i.} W_i W_i ^{\tran} x^{(s)}_{i.}\right)}{\|x^{(s)}_{i.}\|^2}
\end{align*}
Now,
\[W_i W^{\tran}_i = \begin{bmatrix}
{x^{(s')}_{i1}}^2 & 0 & \cdots & 0\\
0 & {x^{(s')}_{i2}}^2 & \cdots & 0\\
0 & 0 & \ddots & 0\\
0 & 0 & 0 & {x^{(s')}_{in_i}}^2\\
\end{bmatrix}\]
Thus,
\[\tr\bigg[\left(X_B^{(s')}\right)^{\tran}(I-Q)\left(X_B^{(s')}\right)\bigg] = \sum_{s'=0}^{p}\sum_{i=1}^R\left[ \sum_{j=1}^{C}z_{ij} x^{(s')^2}_{ij} -\frac{\sum_{j=1}^{C}z_{ij} x^{(s)^2}_{ij} x^{(s')^2}_{ij}}{\sum_{j=1}^{C} z_{ij} x^{(s)^2}_{ij}}\right]\]
The additional equation we added was given by
\[\hat{r}^{\tran}\bigg(I - \frac{1}{n}11^{\tran}\bigg) \hat{r} = E\bigg[r^{\tran}\bigg(I - \frac{1}{n}11^{\tran}\bigg)r\bigg]\]
L.H.S. of the above equation will be given by
\[\sum_{i=1}^R \sum_{j=1}^{C} z_{ij}\hat{r}^2_{ij}- \frac{1}{n}\left(\sum_{i=1}^R\sum_{j=1}^{C}z_{ij}\hat{r}_{ij}\right)^2\]
For R.H.S., similar to above, we will be needing the following expressions:
\[\tr\bigg[(X_A^{(s)})^{\tran}\bigg(I-\frac{1}{n}11^{\tran}\bigg)(X_A^{(s')})\bigg] = \sum_{i=1}^{R}\bigg( \sum_{j=1}^{C}z_{ij} x^{(s)}_{ij}x^{(s')}_{ij}-\frac{\sum_{j=1}^{C} z_{ij}x^{(s)}_{ij}\sum_{j=1}^{C}z_{ij} x^{(s')}_{ij}}{n}\bigg)\]
and
\[\tr\bigg[(X_B^{(s)})^{\tran}\bigg(I-\frac{1}{n}11^{\tran}\bigg)(X_B^{(s')})\bigg] = \sum_{j=1}^{C}\bigg( \sum_{i=1}^{R}z_{ij} x^{(s)}_{ij}x^{(s')}_{ij}-\frac{\sum_{i=1}^{R} z_{ij}x^{(s)}_{ij}\sum_{i=1}^{R}z_{ij} x^{(s')}_{ij}}{n}\bigg)\]

\subsection{Proof of Lemma~\ref{thm:var_E}}
\begin{align*}
q_a^k & (a_1, \cdots, a_R) \propto \Prob (y, \{a_i\}_{i=1}^R | b_j = E_{Q^{(k-1)}} [b_j])\\
& \propto \exp \left \{ -\frac{1}{2} \sum_{i=1}^R \sum_{j=1}^C \frac{z_{ij} (y_{ij} -x_{ij}^\tran (a_i + \mu_{b,j}^{(k-1)}+ \beta))^2}{\left(\sigma^{(k-1)}_e\right)^2} -\frac{1}{2} \sum_{i=1}^R a_i ^\tran (\Sigma_a^{(k-1)})^{-1} a_i \right \}\\
& \propto \exp \Bigg \{ -\frac{1}{2} \sum_{i=1}^R a_i^\tran \Bigg( (\Sigma^{(k-1)}_a)^{-1} +\frac{\sum_{j=1}^{C}z_{ij}x_{ij} x_{ij}^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2} \Bigg) a_i\\
& \hspace{2cm} + a_i ^\tran \frac{\sum_{j=1}^{C}z_{ij}\big(y_{ij} - x_{ij}^{\tran}\big(\beta^{(k-1)} +\mu^{(k-1)}_{b,j}\big)\big)x_{ij}}{\left(\sigma^{(k-1)}_e\right)^{2}} \Bigg\} \\
\end{align*}
Thus,
\begin{equation*}
\mu^{(k)}_{a,i} = \Sigma^{(k)}_{a,i} \left(\frac{\sum_{j=1}^{C}z_{ij}\big(y_{ij} - x_{ij}^{\tran}\big(\beta^{(k-1)} +\mu^{(k-1)}_{b,j}\big)\big)x_{ij}}{\left(\sigma^{(k-1)}_e\right)^{2}}\right)
\end{equation*}
and
\begin{equation*}
\Sigma^{(k)}_{a,i} = \left((\Sigma^{(k-1)}_a)^{-1} +\frac{\sum_{j=1}^{C}z_{ij}x_{ij} x_{ij}^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2}\right)^{-1}
\end{equation*}
\begin{align*}
q_b^k & (b_1, \cdots, b_C) \propto \Prob (y, \{b_j\}_{j=1}^C | a_i = E_{Q^{(k)}} [a_i])\\
& \propto \exp \left \{ -\frac{1}{2} \sum_{j=1}^C \sum_{i=1}^R \frac{z_{ij} (y_{ij} -x_{ij}^\tran (b_j + \mu_{a,i}^{(k)}+ \beta))^2}{\left(\sigma^{(k-1)}_e\right)^2} -\frac{1}{2} \sum_{j=1}^C b_j ^\tran (\Sigma_b^{(k-1)})^{-1} b_j \right \}\\
& \propto \exp \Bigg \{ -\frac{1}{2} \sum_{j=1}^C b_j^\tran \Bigg( (\Sigma^{(k-1)}_b)^{-1} +\frac{\sum_{i=1}^{R}z_{ij}x_{ij} x_{ij}^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2} \Bigg) b_j\\
& \hspace{2cm} + b)j ^\tran \frac{\sum_{j=1}^{C}z_{ij}\big(y_{ij} - x_{ij}^{\tran}\big(\beta^{(k-1)} +\mu^{(k)}_{a,i}\big)\big)x_{ij}}{\left(\sigma^{(k-1)}_e\right)^{2}} \Bigg\} \\
\end{align*}
Thus,
\begin{equation*}
\mu^{(k)}_{b,j} = \Sigma^{(k)}_{b,j} \left(\frac{\sum_{i=1}^{R}z_{ij}\big(y_{ij} - x_{ij}^{\tran}\big(\beta^{(k-1)} +\mu^{(k-1)}_{a,i}\big)\big)x_{ij}}{\left(\sigma^{(k-1)}_e\right)^{2}}\right)
\end{equation*}
and
\begin{equation*}
\Sigma^{(k)}_{b,j} = \left((\Sigma^{(k-1)}_b)^{-1} +\frac{\sum_{i=1}^{R}z_{ij}x_{ij} x_{ij}^{\tran}}{\left(\sigma^{(k-1)}_e\right)^2}\right)^{-1}
\end{equation*}
\end{document}

