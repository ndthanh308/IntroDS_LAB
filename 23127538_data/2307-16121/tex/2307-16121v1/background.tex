{\bf Object Detection based on Camera-LiDAR Fusion: }
According to the combination stage of LiDAR and camera data representations, current methods are categorized into data-, feature-, and proposal-level fusion. 
This paper focuses on proposal-level fusion for the following reasons. First, it is difficult to quantify uncertainty in data- and feature-level fusion because existing studies estimate uncertainty based on prediction proposals.
Second, from the literature \cite{pang2020clocs,huang2022multi,pang2022fast}, proposal-level camera-LiDAR fusion achieves competitive and even superior performance compared with data- and feature-level fusion methods. 
Lastly, proposal-level fusion can easily incorporate alternative neural networks into the fusion pipeline and thus allowing easy adaptation to new object detector designs. 


{\bf Predictive Uncertainty Estimation:} Given plausible inputs, predictive uncertainty measures the variability of model predictions \cite{kendall2017uncertainties}. Predictive uncertainty includes \textit{data uncertainty} due to observation noises caused by sensor measurements or environment and \textit{model uncertainty} accounting for the uncertainty of the model that can be reduced by observing enough data.
Traditionally, data and model uncertainties are modeled under the Bayesian deep learning framework. Specifically, given a data sample $\vec{x}$, the predictive uncertainty is $p(\hat{y}|\vec{x},\mathcal{D})=\int p(\hat{y}|\vec{x},\vec{w})p(\vec{w}|\mathcal{D})d\vec{w}$, where $\mathcal{D}$ denotes the training dataset, $\vec{w}$ represents the model weights, $p(\vec{w}|\mathcal{D})$ and $p(\hat{y}|\vec{x},\vec{w})$ characterize the model and data uncertainties.


Data uncertainty is often modeled by Direct Modeling \cite{gal2016uncertainty}, which assumes that the model prediction follows a probability distribution and directly predicts the parameters of such distribution using the network output layers. Model uncertainty is usually approximated using techniques such as Monte Carlo (MC) Dropout \cite{gal2016dropout} and deep ensembles \cite{lakshminarayanan2017simple}, because it is intractable to calculate the weight posterior distribution over the dataset due to vast dimensionality. 
MC Dropout interprets dropout as a Bayesian approximation of deep Gaussian process. The model uncertainty is given by performing $N$ forward passes on the same input with dropout enabled:
$p(\hat{y}|\vec{x},\mathcal{D}) \approx \frac{1}{N}\sum^N_{n=1}p(\hat{y}|\vec{x},\vec{w})$.
Deep ensemble estimates the predictive probability using an ensemble of models which have the same architecture and are trained with random initializations and data shuffled. Since deep ensemble incurs excessive memory footprint, in this paper, we adopt MC Dropout to estimate model uncertainty.

In AD, the object detector usually produces a bounding box for each detected object to describe the object location and the semantic category (e.g., car, pedestrian) with a probability score. While, the predicted bounding box regression variables are deterministic, and the probability score may not effectively characterize the classification uncertainties. Probabilistic object detectors aim to detect objects accurately and apply reliable uncertainty estimation in both classification and bounding box regression tasks, which additionally generate the {\em classification uncertainty} which  is quantified by the probability that the object belongs to the target class and the {\em regression uncertainty} which is evaluated by the variance of the probability distribution over the predicted bounding box. The latter indicates the amount of uncertainty in the position of the box corners.
Each of the classification and regression uncertainties includes data and model uncertainties. 


{\bf Multi-modal Fusion based on Uncertainty:}  
Multi-modal fusion considering the inherent uncertainty of individual modalities has been explored in previous literature.
\cite{subedar2019uncertainty} estimates predictive uncertainty via variational inference across audio and visual modalities for the activity recognition task. Their approach seeks an optimal uncertainty threshold and it fuses predictive distributions that fall below this threshold using average pooling. However, it restricts its consideration to information from non-degraded modalities with low uncertainty.
Similarly, \cite{tian2020uno} merges multiple uncertainty metrics by applying a Min operation on their deviation ratios with respect to the training set. These ratios are subsequently utilized as the "temperature" for calibrating the prediction logit, and the logits are fused via the noisy-or operation. Nonetheless, this approach is primarily designed for the classification task, despite the fact that both classification and regression results serve as crucial indicators in a 3D object detection task.
\cite{chen2022multimodal} fuses detection scores from multiple modalities employing a probabilistic approach based on Bayes' rule, coupled with the weighted average of boxes based on their data uncertainties. However, the method by \cite{chen2022multimodal} necessitates conditional independence across modalities and struggles to adapt to the 3D object detection task given that the output representations from two modalities differ. 
In contrast, our method adaptively fuse LiDAR and camera detections, harnessing both classification and regression uncertainties across all levels. It effectively addresses the representation disparity between LiDAR and camera detectors and possesses the flexibility to accommodate detectors with varying cognitive abilities.
