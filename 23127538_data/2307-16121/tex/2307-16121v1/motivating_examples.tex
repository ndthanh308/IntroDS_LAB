% Motivation Example: Stat Figures
% Figure environment removed


For object detection in real driving scenarios, the classification and regression uncertainties are affected by variations in the environment or sensor data and the detector's cognitive ability determined by $\vec{w}$. 
For example, adverse weather and adversarial attacks can degrade the detection performance and increase the uncertainties. This section provides motivating examples to understand how the predictive uncertainties of LiDAR and camera vary in normal and adverse scenarios. We also preview the advantages of our proposed uncertainty-encoded LiDAR-camera fusion.

Fig.~\ref{fig:qualitative_anal} shows the detection results of the input frames under four representative driving contexts: (a) the clear weather scene from the KITTI dataset \cite{geiger2012we}, (b) the scene under the adversarial perturbation attack \cite{madry2018towards} against camera, (c) clear scenes, and (d) dense fog weather scenes from the STF dataset \cite{bijelic2020seeing}. We compare the detection performance of the camera-based detector RetinaNet \cite{lin2017focal}, LiDAR-based detector SECOND \cite{yan2018second}, uncertainty-regardless LiDAR-camera fusion detector CLOCs \cite{pang2020clocs}, and our uncertainty-encoded LiDAR-camera fusion detector. The detection results in the last three columns are shown in bird's-eye view.

In this section, we use Eqs.~(\ref{eq:shannon_entropy}) and (\ref{eq:total_variance}) presented later to estimate the scalar classification and regression uncertainty scores.
In a more extensive set of experiments, we compute the average classification and regression uncertainty scores of true positive and false positive detections for LiDAR and camera over datasets of the aforementioned driving scenes. False positive detections can lead to incorrect evasive actions that may cause accidents. The results are presented in Fig.~\ref{fig:stat_anal}.

Fig.~\ref{fig:qualitative_anal} and Fig.~\ref{fig:stat_anal} give four observations. First, classification and regression uncertainty scores, especially for false positives, generally increase when sensors are affected by adverse scenarios. 
Second, in the same driving scenes, false positives generate much higher classification and regression uncertainty scores than true positives. However, LiDAR tends to produce higher classification uncertainty for true positives in adverse weather condition of Fig.~\ref{fig:stat_anal_LiDAR_cls}, as a result of the lower quality and challenging nature of the dataset used. This highlights the need for utilizing both classification and regression uncertainty to address deficiencies in the dataset.
Third, camera and LiDAR have different sensitivities and cognitive abilities to the environment changes. Camera's uncertainty scores show greater volatility than LiDAR's. Lastly, camera's and LiDAR's regression uncertainty values are in different orders.

From above, predictive uncertainty is indicative of sensing performance, while camera's and LiDAR's uncertainties exhibit distinct sensitivities under different adverse scenarios. This motivates us to design a new fusion method with encoded uncertainties as part of the input. As previewed by the last column of Fig.~\ref{fig:qualitative_anal}, our method effectively exploits uncertainties for robust object detection under adverse conditions.
