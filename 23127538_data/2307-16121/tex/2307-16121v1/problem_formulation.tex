% UMoE Architecture Figure
% Figure environment removed


This paper considers fusion-based object detection using camera and LiDAR. The 3D point cloud data from LiDAR, denoted by $\vec{x}^L\in \mathbb{R}^3$, and the 2D RGB image data from camera, denoted by $\vec{x}^I \in \mathbb{R}^2$, are processed by the LiDAR detection branch $H_L$ and camera detection branch $H_I$, respectively. 
For each frame, $H_L$ produces detection $\vec{p}^L = \{\vec{p}^L_1, \ldots, \vec{p}^L_{M_L}\}$, where each $\vec{p}^L_{m_L}$ represents a proposal consisting of the 3D bounding box coordinates and the confidence score.
Similarly, $H_I$ generates detection $\vec{p}^I = \{\vec{p}^I_1, \ldots, \vec{p}^I_{M_I}\}$. 
The fusion combining $H_L$ and $H_I$ produces detection $\vec{p}=\{\vec{p}_1,\ldots,\vec{p}_K\}$, where $K={M_L} \times {M_I}$ and each element $\vec{p}_k = (\vec{p}^L_k,\vec{p}^I_k)$ is a proposal pair consisting of a LiDAR proposal and a camera proposal. 
We aim to explicitly use LiDAR's and camera's detection uncertainties in the above fusion process to derive the final detection that is robust under various adverse conditions covered by training data.

To this end, we first employ uncertainty estimation for LiDAR and camera proposals to derive LiDAR detection uncertainty $\vec{u}^{L}=\{\vec{u}_1^{L},\ldots,\vec{u}_{M_L}^{L}\}$ and camera detection uncertainty $\vec{u}^{I}=\{\vec{u}_1^{I},\ldots,\vec{u}_{M_I}^{I}\}$. Then, we aim to derive the weights that determine the importance of the two sensing modalities for each proposal pair $(\vec{p}^L_k,\vec{p}^I_k) \in \vec{p}$ based on uncertainties $\vec{u}^{L}$ and $\vec{u}^{I}$. To preserve the consistency of the input for subsequent fusion models, we implement the weights by refining the confidence score of detection result. Specifically, we aim to find the function represented by $f_u$ that maps LiDAR and camera proposals and their uncertainty to uncertainty-encoded confidence score $\vec{s'}$, i.e., $\vec{s'}= f_u(\vec{p}^{L},\vec{u}^{L},\vec{p}^{I},\vec{u}^{I})$, where $\vec{s'}=\{\vec{{s'}^L},\vec{{s'}^I}\}$. Subsequently, proposals that have their confidence scores replaced by $\vec{s'}$ can be fused to generate the final detection: 
$\vec{p}^*=f_s(\vec{p}^L,\vec{p}^I)$, where $f_s(\cdot)$ is the fusion operation or fusion network.

There are three main challenges in designing $f_u$. First, $f_u$ requires informative and practical uncertainty representations $\vec{u}^{L}$ and $\vec{u}^{I}$ as input. The representation should be distinguishable, ensuring that it assigns different values to true positives and false positives across all scenarios. Besides, in the context of AD, the uncertainty representation should be calculated without any groundtruth labels and computationally efficient.
Second, the two modalities' uncertainties respond to adverse conditions differently. For instance, in the fog scenarios of Figs.~\ref{fig:stat_anal_camera_reg} and~\ref{fig:stat_anal_LiDAR_reg}, the average regression uncertainty score for false positives from camera increases significantly, while the value for LiDAR decreases slightly.
Third, regression uncertainties computed based on 3D and 2D bounding boxes from LiDAR and camera lie in different ranges. The common 2D bounding box representation encodes top-left and bottom-right corners in camera coordinate, while 3D bounding box includes position, dimension, and rotation angle in LiDAR coordinate. The range difference of these encoded elements results in higher regression uncertainty for a 2D bounding box than that of a 3D bounding box. Our results in Fig.~\ref{fig:stat_anal_camera_reg} and~\ref{fig:stat_anal_LiDAR_reg} exhibit this issue.
The above discrepancies render straightforward ways of incorporating uncertainties, e.g., admitting raw uncertainties as fusion input, futile.
