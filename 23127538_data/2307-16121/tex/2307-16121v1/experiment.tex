This section evaluates UMoE in comparison with uncertainty-regardless LiDAR-camera fusion methods on four datasets that cover the scenarios of clear/adverse weather conditions, adversarial attack, and camera blinding attack. 


\subsection{Datasets}
% \vspace{-0.5cm}
    \textbf{Clear Scenario - KITTI}:
    For sunny daytime driving scenarios, we use KITTI 3D object detection dataset \cite{geiger2012we} and follow the standard data split \cite{chen2017multi} with a 3,712 frames \emph{train} set. We randomly divide the standard val split into a \emph{val} set with 1,884 frames and a \emph{test} set with 1,885 frames. This allows us to calculate deviation ratio and enables the evaluation of subsequent attack datasets created based on \emph{test} set.
    
    \textbf{Adversarial Attack Scenario - KITTIAdv}: 
    We synthesize the adversarial attack scenario by perturbing camera images using PGD \cite{madry2018towards} method on the divided KITTI \emph{test} set 
    (See details in Appendix~\ref{sec:appendix_kittiadv}).
    % (See details in Appendix A.1\footnote{The appendix can be found online at}).
    The attack strength of PGD attack is $4/255$.
    
    \textbf{Camera Blind Attack Scenario - KITTIBlind}: We follow methods in \cite{zhang2020detecting} to generate facula with a radius of 112 pixel and overlay it on the divided KITTI \textit{test} set to form our self-synthetic KITTIBlind dataset that mimics strong light exposure affecting the camera modality (details are described in Appendix~\ref{sec:appendix_kittiblind}). 
    
    \textbf{Adverse Weather Scenario - STF}: 
    To simulate adverse weather, we adopt the STF \cite{Bijelic_2020_CVPR} dataset including clear, dense fog and snow scenarios. The STF clear weather training set, validation set and test set has 3686, 921 and 1536 scenes. Its dense fog test set has 88 scenes; snow test set has 1161 scenes. For models trained on clear weather training set, we select the snapshot with the best performance on clear validation set and evaluate it on aforementioned test splits.


\begin{table}
\centering
% \vspace{-0.5cm}
\caption{Comparison $AP_{3D}$ results of the proposed method and the state-of-the-art fusion baselines in KITTI and KITTIBlind (attack) datasets. We highlight the best performance in bold and the second best in underline.}
% \vspace{-0.2cm}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{$AP_{3D}$} \\ \cmidrule(l){3-5} 
 &  & easy & mod. & hard \\ \midrule
\multicolumn{1}{c|}{\multirow{4}{*}{KITTI}} & \multicolumn{1}{c|}{PointPainting} & 89.23 & 79.31 & 76.86 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{EPNet} & \textbf{92.16} & \textbf{82.69} & \textbf{80.10} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{CLOCs} & {\ul 91.61} & 81.86 & 77.68 \\ \cmidrule(l){2-5} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{CLOCs + UMoE} & 90.25 & {\ul 81.87} & {\ul 79.02} \\ \midrule
\multicolumn{1}{c|}{\multirow{4}{*}{KITTIBlind}} & \multicolumn{1}{c|}{PointPainting} & 87.26 & 77.20 & 74.44 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{EPNet} & 87.03 & 75.38 & 73.78 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{CLOCs} & {\ul 87.89} & {\ul 78.28} & {\ul 76.24} \\ \cmidrule(l){2-5} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{CLOCs + UMoE} & \textbf{90.43} & \textbf{81.80} & \textbf{79.08} \\ \bottomrule
\end{tabular}
}
% \vspace{-0.5em}
\label{tab:baseline_result}
% \vspace{-1cm}
\end{table}


\begin{table*}[h]
\centering
\caption{Ablation study about the effects of classification deviation ratio (DR.), regression uncertainty score (Reg.) and the MoE architecture. The best results are in bold and the second best are underlined.}
% \vspace{-0.1cm}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{@{}ccccccccccccccc@{}}
\toprule
\multirow{2}{*}{DR.} & \multirow{2}{*}{Reg.} & \multirow{2}{*}{MoE} & \multicolumn{3}{c}{KITTIAdv} & \multicolumn{3}{c}{KITTIBlind} & \multicolumn{3}{c}{STF Snow} & \multicolumn{3}{c}{STF Dense Fog} \\ \cmidrule(l){4-15} 
 &  &  & easy & mod. & hard & easy & mod. & hard & easy & mod. & hard & easy & mod. & hard \\ \midrule
 & \checkmark & \multicolumn{1}{c|}{\checkmark} & 88.91 & 77.21 & \multicolumn{1}{c|}{74.87} & {\ul 90.07} & {\ul 81.59} & \multicolumn{1}{c|}{{\ul 79.02}} & {\ul 45.91} & {\ul 42.90} & \multicolumn{1}{c|}{{\ul 38.99}} & {\ul 36.58} & 35.00 & {\ul 32.12} \\
\checkmark &  & \multicolumn{1}{c|}{\checkmark} & {\ul 89.13} & {\ul 77.76} & \multicolumn{1}{c|}{{\ul 75.31}} & 89.96 & 81.56 & \multicolumn{1}{c|}{78.91} & 45.14 & 42.51 & \multicolumn{1}{c|}{38.56} & 34.50 & 34.65 & 30.77 \\
\checkmark & \checkmark & \multicolumn{1}{c|}{} & 85.81 & 75.57 & \multicolumn{1}{c|}{73.82} & 87.53 & 80.27 & \multicolumn{1}{c|}{78.18} & 45.59 & 42.76 & \multicolumn{1}{c|}{38.87} & 35.55 & {\ul 35.37} & 31.18 \\
\checkmark & \checkmark & \multicolumn{1}{c|}{\checkmark} & \textbf{89.36} & \textbf{77.82} & \multicolumn{1}{c|}{\textbf{75.32}} & \textbf{90.43} & \textbf{81.80} & \multicolumn{1}{c|}{\textbf{79.08}} & \textbf{47.27} & \textbf{44.25} & \multicolumn{1}{c|}{\textbf{39.75}} & \textbf{37.05} & \textbf{35.48} & \textbf{32.32} \\ \bottomrule
\end{tabular}
}
\label{tab:ablation_study}
\end{table*}

% \vspace{-0.1cm}
\subsection{Implementation}
\label{sec:implementation}
% \vspace{-0.1cm}
This section describes the implementation of our approach and the employed baselines.

\textbf{Uncertainty-regardless fusion baseline}: 
We select the CLOCs \cite{pang2020clocs}, PointPainting \cite{vora2020pointpainting} and EPNet \cite{huang2020epnet} as representative proposal-level, data-level and feature-level fusion baselines. For the CLOCs, we combine SECOND \cite{yan2018second} and RetinaNet \cite{lin2017focal}, named CLOCs\_SecRetina, and adopt PointPillar \cite{lang2019pointpillars} and RetinaNet, named CLOCs\_PointRetina as the 3D and 2D detectors. We use OpenPCDet \cite{openpcdet2020} and Detectron2 \cite{wu2019detectron2} as our 3D and 2D codebases and apply the default settings. For the CLOCs fusion network, we follow \cite{pang2022fast} that use Residual blocks instead of standard $1\times 1$ convolution layers and optimized with Adam optimizer for $20$ epochs. We employed the OneCycleLR learning rate scheduler with an initial learning rate of $6\times10^{-5}$, a maximum learning rate of $6\times10^{-4}$, and a weight decay of $0.01$. A specific description of the CLOCs fusion model structure is detailed further in Appendix~\ref{sec:appendix_imple}. For PointPainting and EPNet, we fork the original implementations without any modification.

\textbf{Uncertainty-encoded fusion}: 
We integrate the UMoE module into CLOCs\_SecRetina and CLOCs\_PointRetina as our uncertainty-encoded fusion models. To enable uncertainty estimation on sensor-specific detectors, we follow two steps to retrain 3D and 2D detectors. First, we add dropout after each DeConv2D layer for the 3D detectors and after each Conv2D layer for the detection head of the RetinaNet. The dropout rate is set at $0.1$ for all the detectors. Next, the additional loss item previously mentioned is added during training to estimate data uncertainty (refer to Appendix~\ref{sec:appendix_imple} for explicit sensor-specific detectors' training settings). During the inference of sensor-specific detectors, we perform $10$ stochastic samplings with dropout enabled, as suggested in \cite{kendall2017uncertainties}. With the retrained sensor-specific detectors fixed, we apply the same settings with the CLOCs fusion network and train our UMoE module with the fusion network in an end-to-end manner.

\vspace{-0.2cm}
\subsection{Overall Performance}
\vspace{-0.1cm}
We report our evaluation results on the most dominant class, cars, in four datasets. The Average Precision of 40 recall position with an IoU threshold of 0.7 in 3D space ($AP_{3D}$) is used as the evaluation metrics. Due to the space limitation, we present visualization figures in Appendix~\ref{sec:appendix_vis_res}.

\textbf{KITTI}: Based on evaluation results on KITTI \textit{test} set, our UMoE module maintains a satisfactory overall performance under clear scenarios. The UMoE-integrated fusion model performs comparably to uncertainty-regardless fusion in Table~\ref{tab:kitti_result}. Similar observations can be found in Table~\ref{tab:baseline_result} when comparing with state-of-the-art fusion baselines.

\textbf{KITTIAdv}: 
With numerous false positives generated from the camera-based detector due to the adversarial attack, $AP_{3D}$ of uncertainty-regardless fusion baseline drops rapidly. However, the fusion models integrated with UMoE outperform the baseline, i.e., improve $AP_{3D}$ from $87.77\%$ to $89.36\%$ on easy objects. This result may be attributed to the fact that UMoE down-weights false positive proposals with large uncertainty.

\textbf{KITTIBlind}: 
As seen in the KITTIBlind dataset results presented in Table~\ref{tab:kitti_result}, our UMoE module outperforms the uncertainty-regardless fusion baseline significantly and maintains a similar level of performance as in clear scenarios. Additionally, the $AP_{3D}$ of all state-of-the-art fusion baselines decrease with affected camera under strong light exposure in Table~\ref{tab:baseline_result}, while our module remains robust.

\textbf{STF}: We show the evaluation results for the STF dataset in Table~\ref{tab:stf_result}. LiDAR and camera degrade simultaneously in extreme weather conditions. The $AP_{3D}$ for both fusion models decreases in these scenes due to noisy LiDAR point clouds and camera images. Our proposed UMoE module significantly improves the $AP_{3D}$ under adverse weather scenarios, with a maximum increase of $10.67\%$ under dense fog weather and $3.75\%$ in snow scenes. Additionally, the UMoE-integrated fusion models achieve comparable or even better results in clear scenarios. These results demonstrate that UMoE can effectively improve robustness in extreme weather conditions.

\noindent\textbf{Statistical significant test}: 
We calculate p-values on moderate $AP_{3D}$ from 10 runs of CLOCs\_SecRetina and its baseline, which are $0.01$, \num{2.9e-14}, \num{1.4e-7}, \num{1.6e-8} in KITTIAdv, KITTIBlind, snow and dense fog scenarios. Each p-value is less than $0.05$ threshold, confirming the significance of our module's improvement.


\vspace{-0.2cm}
\subsection{Ablation Study}
To analyze the effects of uncertainty scoring and the MoE architecture, we conduct ablation studies by removing each component on the CLOCs\_SecRetina model. We report results in Table~\ref{tab:ablation_study}, including $AP_{3D}$ on KITTIAdv dataset, KITTIBlind dataset, and the snow and dense fog test sets from the STF dataset.

{\bf Encoded uncertainty:} 
To study the effectiveness of the encoded uncertainties, we remove classification deviation ratio or regression uncertainty scores from the input tensor $\vec{T}^{I}_\textrm{UMoE}$ and $\vec{T}^{L}_\textrm{UMoE}$. As shown in Table \ref{tab:ablation_study}, utilizing only the classification deviation ratio (row 2) provides relatively limited benefits, though it is particularly advantageous in adversarial attack scenes. Conversely, relying solely on the regression uncertainty score (row 1) generally results in more considerable benefits, especially in dense fog scenarios, likely attributable to its effectiveness in identifying false positives. Moreover, incorporating all components (row 4) culminates in the highest performance across all scenarios, suggesting that both uncertainties serve as crucial cues in the 3D object detection task.

{\bf MoE:}
To investigate the effectiveness of MoE, we remove this architecture and feed uncertainty scores directly to the fusion layer. The results in row 3 demonstrate that even with complete uncertainties, the model without MoE performs poorly in some adversarial attack and snow scenarios. This confirms the necessity of using MoE in handling the uncertainty differences across modalities and ranges.

\vspace{-0.2cm}
\subsection{MC-dropout runtime analysis}
This section briefly analyzes the runtime of the MC-dropout technique applied in our method. As described in Section~\ref{sec:implementation}, we perform 10 MC-dropout runs only on the detection head of sensor-specific detectors during the inference. Under these settings, the running speed is around 6 fps and 40 fps for uncertainty-encoded and uncertianty-regardless RetinaNet, respectively. The speeds are 11 fps and 24 fps for SECOND detector and 15 fps and 40 fps for PointPillar. It is worth noting that our extensive experiments show that the detection performance growth stabilizes after 5 runs. With the development of edge devices, computational redundancy can be exploited when using MC-dropout. Therefore, it will not drastically increase the cost.

\vspace{-0.2cm}
\subsection{Limitations}
Our approach encounters two primary limitations. Firstly, sensor-specific detectors may suffer slight performance degradation due to uncertainty estimation techniques such as MC-Dropout, particularly in clear scenarios, which can impact fusion performance. However, our approach can adapt advanced uncertainty estimation method to minimize such reductions. Secondly, the UMoE module can only mitigate, not eliminate, the effect of adverse scenarios. In situations of all sensors fail, our methodâ€™s enhancement
remains limited.

