% \begin{table*}[tb!]
% \centering
% \caption{$AP_{3D}$ of LiDAR-only detectors on the STF dataset, showcasing the improvement in uncertainty-regardless (blue bars) and uncertainty-encoded (orange bars) fusion results compared to LiDAR-only outcomes. We emphasize larger fusion enhancements in bold.}
% \resizebox{0.8\textwidth}{!}{
% \begin{tabular}{@{}ccccccccccc@{}}
% \toprule
%  &  & \multicolumn{3}{c}{Clear $AP_{3D}$} & \multicolumn{3}{c}{Dense Fog $AP_{3D}$} & \multicolumn{3}{c}{Snow $AP_{3D}$} \\ \cmidrule(l){3-11} 
% \multirow{-2}{*}{Method} & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}MC-Dropout\&\\ Direct Modeling\end{tabular}} & easy & mod. & hard & easy & mod. & hard & easy & mod. & hard \\ \midrule
% \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & 47.06 & 45.01 & \multicolumn{1}{c|}{41.14} & 30.32 & 30.39 & \multicolumn{1}{c|}{26.81} & 40.39 & 38.04 & 34.13 \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Improvement\\ (Fusion)\end{tabular}} & \cellcolor[HTML]{96FFFB}+2.83 & \cellcolor[HTML]{96FFFB}+2.96 & \multicolumn{1}{c|}{\cellcolor[HTML]{96FFFB}+2.74} & \cellcolor[HTML]{96FFFB}+1.23 & \cellcolor[HTML]{96FFFB}+1.53 & \multicolumn{1}{c|}{\cellcolor[HTML]{96FFFB}+1.29} & \cellcolor[HTML]{96FFFB}+3.13 & \cellcolor[HTML]{96FFFB}+3.1 & \cellcolor[HTML]{96FFFB}+2.3 \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\checkmark} & 46.44 & 45.23 & \multicolumn{1}{c|}{40.41} & 32.81 & 33.26 & \multicolumn{1}{c|}{29.60} & 43.18 & 40.37 & 36.17 \\
% \multicolumn{1}{c|}{\multirow{-4}{*}{SECOND}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Improvement\\ (Fusion+UMoE)\end{tabular}} & \cellcolor[HTML]{FFCE93}\textbf{+3.4} & \cellcolor[HTML]{FFCE93}\textbf{+3.18} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCE93}\textbf{+2.99}} & \cellcolor[HTML]{FFCE93}\textbf{+4.24} & \cellcolor[HTML]{FFCE93}\textbf{+2.22} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCE93}\textbf{+2.72}} & \cellcolor[HTML]{FFCE93}\textbf{+4.09} & \cellcolor[HTML]{FFCE93}\textbf{+3.88} & \cellcolor[HTML]{FFCE93}\textbf{+3.58} \\ \midrule
% \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & 41.57 & 39.30 & \multicolumn{1}{c|}{36.30} & 22.81 & 21.30 & \multicolumn{1}{c|}{22.66} & 38.73 & 36.58 & 32.96 \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Improvement\\ (Fusion)\end{tabular}} & \cellcolor[HTML]{96FFFB}+4.96 & \cellcolor[HTML]{96FFFB}+4.43 & \multicolumn{1}{c|}{\cellcolor[HTML]{96FFFB}+3.43} & \cellcolor[HTML]{96FFFB}+5.86 & \cellcolor[HTML]{96FFFB}+6.5 & \multicolumn{1}{c|}{\cellcolor[HTML]{96FFFB}+4.68} & \cellcolor[HTML]{96FFFB}+4.76 & \cellcolor[HTML]{96FFFB}+3.58 & \cellcolor[HTML]{96FFFB}+3.52 \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\checkmark} & 41.31 & 40.12 & \multicolumn{1}{c|}{36.69} & 27.43 & 27.12 & \multicolumn{1}{c|}{25.24} & 35.38 & 33.73 & 30.15 \\
% \multicolumn{1}{c|}{\multirow{-4}{*}{PointPillar}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Improvement\\ (Fusion+UMoE)\end{tabular}} & \cellcolor[HTML]{FFCE93}\textbf{+5.06} & \cellcolor[HTML]{FFCE93}\textbf{+5.34} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCE93}\textbf{+4.94}} & \cellcolor[HTML]{FFCE93}\textbf{+11.91} & \cellcolor[HTML]{FFCE93}\textbf{+10.06} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCE93}\textbf{+7.65}} & \cellcolor[HTML]{FFCE93}\textbf{+7.66} & \cellcolor[HTML]{FFCE93}\textbf{+6.55} & \cellcolor[HTML]{FFCE93}\textbf{+6.05} \\ \bottomrule
% \end{tabular}
% }
% \label{tab:lidar_only_result_stf}
% \vspace{1em}
% \end{table*}


% \begin{table*}[tb!]
% \centering
% \caption{$AP_{3D}$ of LiDAR-only detectors on the KITTI, KITTIAdv and KITTIBlind datasets. Given that only camera images experience degradation in these scenes, a splash symbol is employed to represent identical performance with the KITTI dataset.}
% \vspace{-1em}
% \resizebox{0.8\textwidth}{!}{
% \begin{tabular}{ccccccccccc}
% \hline
%                                                    &                                                                                          & \multicolumn{3}{c}{KITTI $AP_{3D}$}                                                                                                                     & \multicolumn{3}{c}{KITTIAdv $AP_{3D}$}                                                                                                                  & \multicolumn{3}{c}{KITTIBlind $AP_{3D}$}                                                                                          \\ \cline{3-11} 
% \multirow{-2}{*}{Method}                           & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}MC-Dropout\&\\ Direct Modeling\end{tabular}} & easy                                   & mod.                                   & hard                                                        & easy                                   & mod.                                   & hard                                                        & easy                                   & mod.                                  & hard                                   \\ \hline
% \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                                                                    & 90.97                                  & 79.95                                  & \multicolumn{1}{c|}{77.10}                                  & -                                      & -                                      & \multicolumn{1}{c|}{-}                                      & -                                      & -                                     & -                                      \\
% \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Improvement\\ (Fusion)\end{tabular}}      & \cellcolor[HTML]{96FFFB}+0.64          & \cellcolor[HTML]{96FFFB}+1.91          & \multicolumn{1}{c|}{\cellcolor[HTML]{96FFFB}+0.58}          & \cellcolor[HTML]{96FFFB}-3.2           & \cellcolor[HTML]{96FFFB}-2.22          & \multicolumn{1}{c|}{\cellcolor[HTML]{96FFFB}-3.23}          & \cellcolor[HTML]{96FFFB}-3.08          & \cellcolor[HTML]{96FFFB}-1.67         & \cellcolor[HTML]{96FFFB}-0.86          \\
% \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\checkmark}                                                          & 88.23                                  & 78.40                                  & \multicolumn{1}{c|}{77.54}                                  & -                                      & -                                      & \multicolumn{1}{c|}{-}                                      & -                                      & -                                     & -                                      \\
% \multicolumn{1}{c|}{\multirow{-4}{*}{SECOND}}      & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Improvement\\ (Fusion+UMoE)\end{tabular}} & \cellcolor[HTML]{FFCE93}\textbf{+2.02} & \cellcolor[HTML]{FFCE93}\textbf{+3.47} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCE93}\textbf{+1.48}} & \cellcolor[HTML]{FFCE93}\textbf{+1.13} & \cellcolor[HTML]{FFCE93}\textbf{-0.58} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCE93}\textbf{-2.22}} & \cellcolor[HTML]{FFCE93}\textbf{+2.2}  & \cellcolor[HTML]{FFCE93}\textbf{+3.4} & \cellcolor[HTML]{FFCE93}\textbf{+1.54} \\ \hline
% \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                                                                    & 87.90                                  & 78.87                                  & \multicolumn{1}{c|}{76.16}                                  & -                                      & -                                      & \multicolumn{1}{c|}{-}                                      & -                                      & -                                     & -                                      \\
% \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Improvement\\ (Fusion)\end{tabular}}      & \cellcolor[HTML]{96FFFB}\textbf{+2.52} & \cellcolor[HTML]{96FFFB}\textbf{+2.93} & \multicolumn{1}{c|}{\cellcolor[HTML]{96FFFB}\textbf{+2.46}} & \cellcolor[HTML]{96FFFB}-2.42          & \cellcolor[HTML]{96FFFB}-3.46          & \multicolumn{1}{c|}{\cellcolor[HTML]{96FFFB}-2.89}          & \cellcolor[HTML]{96FFFB}-3.31          & \cellcolor[HTML]{96FFFB}-0.59         & \cellcolor[HTML]{96FFFB}-0.46          \\
% \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\checkmark}                                                          & 90.40                                  & 79.31                                  & \multicolumn{1}{c|}{76.53}                                  & -                                      & -                                      & \multicolumn{1}{c|}{-}                                      & -                                      & -                                     & -                                      \\
% \multicolumn{1}{c|}{\multirow{-4}{*}{PointPillar}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Improvement\\ (Fusion+UMoE)\end{tabular}} & \cellcolor[HTML]{FFCE93}-0.52          & \cellcolor[HTML]{FFCE93}+1.16          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCE93}+1.24}          & \cellcolor[HTML]{FFCE93}\textbf{-1.75} & \cellcolor[HTML]{FFCE93}\textbf{-1.49} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCE93}\textbf{-0.98}} & \cellcolor[HTML]{FFCE93}\textbf{-0.41} & \cellcolor[HTML]{FFCE93}\textbf{+1}   & \cellcolor[HTML]{FFCE93}\textbf{+1.28} \\ \hline
% \end{tabular}
% }
% \label{tab:lidar_only_result_kitti}
% \vspace{1em}
% \end{table*}


\subsection{Datasets}
In our experiment, we evaluate on datasets with attacks and adverse weather scenarios. The adverse weather dataset STF is publicly avilable, while KITTIAdv and KITTIBlind are synthetically created by us to simulate attacks. In this section, we provide detailed descriptions of how these two datasets were constructed.

\begin{itemize}
\setlength{\itemsep}{0pt}

\item \textbf{KITTIAdv dataset}\label{sec:appendix_kittiadv} is introduced to emulate scenarios where the camera is dysfunctional by the adversarial perturbation attack. The attack is conducted by adding minute, imperceptible changes to each pixel in the image. We employ the Project Gradient Descent (PGD) method to accomplish our attack goal. Let us denote the initial perturbed image to as $I^{per}_{0}$. The attack is then implemented by updating the perturbation $\delta^{per}_{n}$ via the projected loss gradient of the camera-based detector, across multiple iteration as follows: 
\begin{equation}
    \begin{aligned}
    &\delta^{per}_n=Clip_{\epsilon}\{\alpha \times sign(\nabla_{I}) L(O_{\theta}(I^{per}_n), b^{true})\}, \\
    &I^{per}_{n+1} = I^{per}_n + \delta^{per}_n
    \end{aligned}
\end{equation}
where $\text{Clip}_{\epsilon}\{\cdot\}$ guarantees that the value falls within the $[-\epsilon, \epsilon]$, $\alpha$ is the parameter that controls the attack intensity, $\text{sign}(\cdot)$ denotes the sign function, $O_{\theta}(\cdot, \cdot)$ denotes the camera-based detector parameterized by $\theta$, $L(\cdot)$ represents the loss function of $O_{\theta}(\cdot)$, $b^{\text{true}}$ is the ground truth label, and $0 \leqslant n \leqslant N-1$. In our implementation, we have set the value range $\epsilon$ at $76$, attack intensity $\alpha$ at $1$, and the iteration number $N$ at $4$. 

\item \textbf{KITTIBlind dataset}\label{sec:appendix_kittiblind} contains fabricated scenarios in which the camera is blinded by intense light beams. To create the affected camera data, we superimpose a Gaussian facula onto the image. The radius of this facula is set to 112 pixels on a KITTI image of dimensions $1242\times 374$. For each image sample in the KITTI dataset, we randomly select a location to serve as the center point for the facula placement, within a predefined region in front of the ego vehicle. This region is specified as $[621, 745]$ for the column and $[75, 299]$ for the row. 
\end{itemize}


\subsection{Implementation}\label{sec:appendix_imple}
The implementation details of sensor-specific detectors and the proposal-level fusion network are described as follows:

For the LiDAR-based detectors, we adopt to the default settings provided in the open-source codebase, OpenPCDet, to train the SECOND and PointPillar models.
For the camera-based RetinaNet detector, we leverage another open-source codebase, Detectron2, for model training. We set an initial learning rate of $0.0025$ with a drop factor of $10$ and train the model in with a batch size of $4$ for $90,000$ iterations. During the inference phase, we set the non-maximum suppression (NMS) threshold to $0.5$ for uncertainty-regardless fusion and $0.7$ for uncertainty-encoded fusion in order to meet the requisite specifications for the proposal-level fusion network input.

For the proposal-level fusion method, we utilize the state-of-the-art model, {\em CLOCs}, to demonstrate the effectiveness of our approach. For a detailed pipeline of CLOCs: in each scene, the 3D LiDAR point clouds and 2D RGB image are first processed by their respective sensor-specific models to generate LiDAR and camera proposals. Before the NMS operation, for each pair of LiDAR and camera proposals, an input tensor $T_{CLOCs} = \{IoU, s_I , s_L, d\}$ is constructed. Here, $IoU$ denotes the Intersection over Union for 2D and 3D proposals in the image plane, $s_I$ and $s_L$ represent confidence scores, and $d$ signifies the distance of the LiDAR proposal to the ego vehicle. Subsequently, $T_{CLOCs}$ is processed by its fusion layer, which consists of four $1\times 1$ 2D convolution layers to predict refined confidence scores for more accurate 3D predictions. In \cite{pang2022fast}, authors have replaced the $1\times 1$ 2D convolution layers with residual blocks to achieve superior performance. Although they only provide a description without making the code publicly available, we have reproduced the updated CLOCs as our proposal-level fusion network in the experiment.


\subsection{Statistical Analysis}
Fig.~\ref{fig:stat_anal_appendix} provides statistical analysis of classification deviation ratio and regression uncertainty score for the KITTIBlind dataset and snow scenes from STF dataset. Similar to the observations in Figures \ref{fig:stat_anal} and \ref{fig:dev_ratio}, these scores serve as discriminative indicators between true positives and false positives, and exhibit varying sensitivities to environment changes.

% Stat
% Figure environment removed


% \subsection{Additional Results}\label{sec:appendix_add_res}
% Table~\ref{tab:lidar_only_result_stf} expands upon the fusion results presented in Table~\ref{tab:stf_result}, providing a comprehensive comparison among the uncertainty-regardless fusion baseline, the uncertainty-encoded fusion results, and the LiDAR-only baselines within the context of the STF dataset. It is evident that both fusion techniques contribute significantly to performance enhancement, underscoring the importance of camera-LiDAR fusion in adverse weather conditions. Furthermore, our uncertainty-encoded fusion method (depicted by orange bars) consistently exhibits greater improvement, irrespective of the performance levels of the corresponding LiDAR-only results. For instance, in the case of PointPillar detectors operating within dense fog scenarios, uncertainty estimation contributes to a more robust performance for the LiDAR-only detector, and our method capitalizes on this advantage to yield a substantially greater performance enhancement. Simultaneously, for PointPillar detectors whose performance is compromised due to uncertainty estimation, our method continues to offer a larger improvement and generates results comparable to those of the uncertainty-regardless approach.

% Similarly, LiDAR-only results compared to Table~\ref{tab:kitti_result} are demonstrated in Table~\ref{tab:lidar_only_result_kitti}. As only camera performance deteriorates in the KITTIAdv and KITTIBlind datasets, the uncertainty-regardless fusion yields suboptimal outcomes relative to the LiDAR-only results. Nevertheless, our uncertainty-encoded method typically exhibits either a reduced performance degradation or, in some cases, superior performance in comparison to the LiDAR-only results.


\subsection{Visualization}\label{sec:appendix_vis_res}

% Figure environment removed

% Figure environment removed

Fig.~\ref{fig:appendix_qual_res_stf} and~\ref{fig:appendix_qual_res_kitti} shows some qualitative results of uncertainty-regardless fusion baselines, uncertainty-encoded fusion method, and the corresponding 2D images across all aforementioned scenarios. Detections are represented by red bounding boxes, while ground truth detections are denoted by green bounding boxes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
