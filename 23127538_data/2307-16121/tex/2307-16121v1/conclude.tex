Autonomous driving is moving rapidly toward a higher level of automation in more complex environments, demanding the ability of coping with all kinds of uncertainties. This paper systematically studied how to incorporate the predictive uncertainties of individual sensors in multi-modal fusion, a fundamental task of autonomous driving perception. We score uncertainties and propose a fusion module that exploits the Mixture-of-Expert architecture to encode multi-modal uncertainties in any proposal-level fusion pipelines. Experimental results show that our module significantly improves the fusion performance in adverse scenarios. In addition to LiDAR-camera fusion, the scope of our methods can be broadened to encompass various scenarios, like incorporating additional sensors such as radar, or enhancing the LiDAR-only single-modality detection which is common in industrial-level autonomous driving systems. 
% However, the UMoE module can only mitigate, not eliminate, the effect of adverse scenarios. How to guarantee the performance enhancement when all sensors fail still remains to be explored. 
Far beyond the object detection metrics, evaluating the robustness of multi-modal fusion in various downstream tasks is interesting for future work.