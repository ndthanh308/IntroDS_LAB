To address the above challenges, we propose a fusion module called Uncertainty-encoded Mixture-of-Experts (UMoE) that bridges the sensor-specific detectors and the proposal-level fusion network. Fig.~\ref{fig:UMoE_Archt} illustrates the proposal-level LiDAR-camera fusion framework with our UMoE module integrated. First, LiDAR- and camera-based detectors take sensor data to generate detection proposals. With uncertainty scoring, we extend each proposal with uncertainty scores to build the UMoE input. Then, the expert network for each sensing modality extracts sensor-specific features from the UMoE input. The gating network takes the combined features generated by the preceding expert networks and generates the uncertainty-encoded confidence scores. Finally, detection proposals with updated confidence scores can be applied in the proposal-level fusion networks.

\subsection{Uncertainty Scoring}
\label{sec: uncertainty scoring}
Now we describe our uncertainty scoring approach. Denote object proposals produced by sensor-specific detectors as $\vec{p}=\{s, \vec{b}\}$, where $s$ and $\vec{b}$ denote confidence score and bounding box. By using the MC Dropout uncertainty estimation approach, each proposal $\vec{p}_k$ is assigned with uncertainty $\vec{u}_k = \{\vec{u}_{k,cls}, \vec{u}_{k,reg}\}$ consisting of the classification uncertainty $\vec{u}_{k, cls}\in \mathbb{R}^{C}$ and regression uncertainty $\vec{u}_{k,reg}\in \mathbb{R}^{B}$, where $C$ and $B$ are the numbers of classes and  elements in bounding box representation. To encode $\vec{u}_k$ into UMoE input tensor, we transform the vectors $\vec{u}_{k,cls}$ and $\vec{u}_{k,reg}$ into scalar uncertainty scores $u_{k,cls}\in\mathbb{R}$ and $u_{k,reg}\in\mathbb{R}$ as follows.

First, we use entropy to score classification uncertainty:
\begin{equation}\label{eq:shannon_entropy}
\textstyle
    u_{k,cls} = -\sum_{c=1}^C s_c \log s_c,
\end{equation}
where $s_c=\frac{1}{N} \sum_{n=1}^N p(\hat{y}=c|\vec{x}_k,\vec{w}_n)$ represents the average predicted classification probability of class $c$ over the $N$ forward passes of MC Dropout.
Eq.~\ref{eq:shannon_entropy} yields high classification uncertainty scores $u_{k,cls}$ for proposals with intermediate average predicted classification probability $s_c$, while demonstrating reduced values for proposals with $s_c$ at either extreme. However, the task complexity is modality-dependent, with LiDAR-based 3D detectors generally exhibit lower confidence levels compared to their camera-based 2D counterparts. Consequently, as illustrated in Fig.~\ref{fig:stat_anal_LiDAR_cls} for LiDAR classification uncertainty scores, false positives with low confidence scores have smaller average scores than true positives. 
To ensure the informativeness of the classification uncertainty score, we enhance it with a classification deviation ratio, a quantitative metric designed to evaluate the extent to which a proposal's confidence score and classification uncertainty score deviate from the true positives' distribution. Specifically:
\begin{equation}
\begin{aligned}
\delta_{k, cls} = &\frac{\mu_{u}}{\mu_{u}+\max(0, (u_{k, cls}-\mu_{u}-\sigma_{u}))} \cdot \\
&\frac{\mu_{s}}{\mu_{s}+ \max(0, -(s_{c}-\mu_{s}-\sigma_{s}))}
\end{aligned}
\end{equation}
where $\mu_{u}, \sigma_{u}, \mu_{s}, \sigma_{s}$ are the mean and standard deviation of classification uncertainty scores and the average predicted classification probability for true positives in the validation set. The computed ratio, $\delta_{k, cls}$, serves to assign larger value to proposals whose classification uncertainty score $u_{k, cls}$ and average predicted classification probability $s_{k}$ fall within the distribution of true positives, while assigning smaller values to those proposals that are out-of-distribution. As illustrated in Fig.~\ref{fig:dev_ratio}, the deviation ratio is more informative than the classification uncertainty score, as it effectively captures the differences between false positives and true positives, as well as accounting for adverse conditions.


% Figure environment removed

After that, we use the total variance of $\vec{u}_{k,reg}$ to score regression uncertainty:
\begin{equation}\label{eq:total_variance}
\textstyle
    % u_{reg}^{i} = trace(\frac{1}{T}\sum_{t=1}^T \vec{b}_{\vec{w}_t^{i}}(\vec{b}_{\vec{w}_t^{i}})^{\intercal}-\vec{\mu}^{i}(\vec{\mu}^{i})^{\intercal})
    u_{k,reg} = \mathrm{tr} \left( \frac{1}{N}\sum_{n=1}^N \vec{b}_{k,n}\vec{b}_{k,n}^{\intercal}-\vec{\bar{b}}_k\vec{\bar{b}}_k^{\intercal} \right),
\end{equation}
where $\vec{\bar{b}}_k = \frac{1}{N}\sum_{n=1}^N\vec{b}_{k,n}$ is the mean bounding box coordinates over the $N$ forward passes of MC Dropout. On top of this, we adopt the Direct Modeling method for data uncertainty of bounding boxes. We assume that each regression output follows an independent Gaussian distribution and estimate the variance of these outputs. We then generate Monte Carlo samples from this distribution and add the total variance of these samples to $u_{k,reg}$. The total variance ranges in $[0,\infty]$, where a larger value indicates higher regression uncertainty. 
However, the sizes of the 2D bounding boxes in the image plane have significant influence on the values of the total variance. For instance, a closer object with a larger bounding box leads to a large $u_{k,reg}$. To achieve fair comparisons among objects at different distances, we divide $u_{k,reg}$ by the diagonal length of the averaged bounding box $\vec{\bar{b}}_k$.
We also apply standardization to $u_{k,reg}$ with mean and standard deviation of regression uncertainty calculated from the clear validation set.

With the uncertainty scores, we extend each original proposal $\vec{p}_k =\{s_k,\vec{b}_k\}$ for both LiDAR and camera to $\vec{p}_k = \{s_k,\vec{b}_k,\delta_{k, cls},u_{k,reg}\}$. For a given scene, all of the $K$ LiDAR-camera proposal pairs are used to build the tensors
$\vec{T}^{I}_\mathrm{UMoE}=\{\vec{s}^{I},\vec{\delta}_{cls}^{I},\vec{u}_{reg}^{I}\} \in \mathbb{R}^{1\times K\times 3}$, $\vec{T}^{L}_\mathrm{UMoE}=\{\vec{s}^{L},\vec{\delta}_{cls}^{L},\vec{u}_{reg}^{L}\} \in \mathbb{R}^{1\times K\times 3}$, which will be used as input to the UMoE module.


\subsection{Multi-Modal Uncertainty Fusion via UMoE}
Our UMoE is based on the Mixture of Experts (MoE) architecture \cite{jacobs1991adaptive},
which is designed to handle multiple different tasks in complex scenarios. Existing works \cite{kim2018robust,mees2016choosing} demonstrate MoE's effectiveness in multi-modal perception including LiDAR-camera fusion. The key advantage of MoE is that it contains distinct expert networks to extract features from each of the sensing modalities and then uses a gating network to combine and learn from the different extracted features to give final output. 
To find the mapping $f_{u}$, our UMoE module substitutes the traditional inputs of MoE with $\vec{T}^{I}_\mathrm{UMoE}$ and  $\vec{T}^{L}_\mathrm{UMoE}$
%and $\vec{T}^{IoU}_\mathrm{UMoE}$
to produce uncertainty-encoded confidence scores $\vec{s'}$\iffalse fusion weights\fi.
The detailed designs of our UMoE components are as follows.

% \vspace{-0.5cm}
\subsubsection{Expert networks}
As shown in the motivating example section, different sensing modalities have distinct sensitivities and value ranges for uncertainty scoring. Thus, we exploit different expert network for each sensing modality that maps input tensors to sensor-specific features for further fusion. 
Specifically, the expert network for LiDAR branch $E_{L}$ consists of a set of Residual blocks \cite{he2016deep}. The Residual Block operation is denoted by $\mathrm{ResBlock}(c_{in},c_{out},k)$, where $c_{in}, c_{out}$ are the input and output channel size and $k$ is the kernel size of 2D convolution layers inside. In $E_{L}$, we employ three Residual blocks $\mathrm{ResBlock}(3,9,(1,1))$, $\mathrm{ResBlock}(9,18,(1,1))$, and $\mathrm{ResBlock}(18,18,(1,1))$ sequentially. Similarly, the expert network for camera branch $E_{I}$ follows the same structure. 
The process can be described as:
$\vec{F}^{L} = E_{L}(\vec{T}^{L}_\mathrm{UMoE})$, $\vec{F}^{I} = E_{I}(\vec{T}^{I}_\mathrm{UMoE})$,
where $\vec{F}^{L}, \vec{F}^{I} \in R^{1\times K\times 18}$ are the feature vectors that encode confidence score and uncertainties for each proposal of the corresponding sensing modality. 

% \vspace{-0.5cm}
\subsubsection{Gating network}
The gating network concatenates features $\vec{F}^{L}$, $\vec{F}^{I}$ across all sensor modalities into a joint feature $\vec{F}_{J}$, which yields a tensor with size $1\times K\times 36$. Next, two output branches $G^{L}(\cdot)$ and $G^{I}(\cdot)$ take the same joint feature $F_{J}$ as input and predict uncertainty-encoded confidence scores $\vec{s'}^{L}$ and $\vec{s'}^{I}$, respectively. Each output branch consists of the a single Residual block $\mathrm{ResBlock}(36,1,(1,1))$. The pipeline of the gating network is defined as follows: 
$\vec{s'}^{L} = G^{L}(\vec{F}_J)$, $\vec{s'}^{I} = G^{I}(\vec{F}_J)$, where $\vec{F}_J = \vec{F}^{L} \oplus \vec{F}^{I}$.
The outputs $\vec{s'}^{L}$ and $\vec{s'}^{I}$ are used to substitute the original confidence scores $\vec{s}^L$ and $ \vec{s}^I$ of corresponding proposals based on their uncertainty scores. These refined proposals can then be input into various proposal-level fusion networks for further processing.


\begin{table*}[h]
\centering
\caption{$AP_{3D}$ on KITTI (clear), KITTIAdv (attack) and KITTIBlind (attack) datasets w/o and w/ the UMoE module.}
% \vspace{-1.5em}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\multirow{2}{*}{Method}                                  & \multirow{2}{*}{UMoE}           & \multicolumn{3}{c}{KITTI $AP_{3D}$}                                  & \multicolumn{3}{c}{KITTIAdv $AP_{3D}$}                               & \multicolumn{3}{c}{KITTIBlind $AP_{3D}$}        \\ \cmidrule(l){3-11} 
                                                         &                                 & easy           & mod.           & hard                                & easy           & mod.           & hard                                & easy           & mod.           & hard           \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{CLOCs\_SecRetina}}   & \multicolumn{1}{c|}{}           & \textbf{91.61} & 81.86          & \multicolumn{1}{c|}{77.68}          & 87.77          & 77.73          & \multicolumn{1}{c|}{73.87}          & 87.89          & 78.28          & 76.24          \\
\multicolumn{1}{c|}{}                                    & \multicolumn{1}{c|}{\checkmark} & 90.25          & \textbf{81.87} & \multicolumn{1}{c|}{\textbf{79.02}} & \textbf{89.36} & \textbf{77.82} & \multicolumn{1}{c|}{\textbf{75.32}} & \textbf{90.43} & \textbf{81.80} & \textbf{79.08} \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{CLOCs\_PointRetina}} & \multicolumn{1}{c|}{}           & \textbf{90.42} & \textbf{81.80} & \multicolumn{1}{c|}{\textbf{78.62}} & 85.48          & 75.41          & \multicolumn{1}{c|}{73.27}          & 84.59          & 78.28          & 75.70          \\
\multicolumn{1}{c|}{}                                    & \multicolumn{1}{c|}{\checkmark} & 89.88          & 80.47          & \multicolumn{1}{c|}{77.77}          & \textbf{88.65} & \textbf{77.82} & \multicolumn{1}{c|}{\textbf{75.55}} & \textbf{89.99} & \textbf{80.31} & \textbf{77.81} \\ \bottomrule
\end{tabular}
}
\label{tab:kitti_result}
% \vspace{0.5em}
\end{table*}


\begin{table*}[]
% \vspace{-0.1cm}
\centering
\caption{$AP_{3D}$ performance on STF clear, dense fog and snow test splits w/o and w/ the UMoE module.}
% \vspace{-0.3cm}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{UMoE} & \multicolumn{3}{c}{Clear $AP_{3D}$} & \multicolumn{3}{c}{Dense Fog $AP_{3D}$} & \multicolumn{3}{c}{Snow $AP_{3D}$} \\ \cmidrule(l){3-11} 
 &  & easy & mod. & hard & easy & mod. & hard & easy & mod. & hard \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{CLOCs\_SecRetina}} & \multicolumn{1}{c|}{} & \textbf{49.89} & 47.97 & \multicolumn{1}{c|}{\textbf{43.88}} & 31.55 & 31.92 & \multicolumn{1}{c|}{28.10} & 43.52 & 41.14 & 36.43 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\checkmark} & 49.84 & \textbf{48.41} & \multicolumn{1}{c|}{43.40} & \textbf{37.05} & \textbf{35.48} & \multicolumn{1}{c|}{\textbf{32.32}} & \textbf{47.27} & \textbf{44.25} & \textbf{39.75} \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{CLOCs\_PointRetina}} & \multicolumn{1}{c|}{} & \textbf{46.53} & 43.73 & \multicolumn{1}{c|}{39.73} & 28.67 & 27.80 & \multicolumn{1}{c|}{27.34} & \textbf{43.49} & 40.16 & \textbf{36.48} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\checkmark} & 46.37 & \textbf{45.46} & \multicolumn{1}{c|}{\textbf{41.63}} & \textbf{39.34} & \textbf{37.18} & \multicolumn{1}{c|}{\textbf{32.89}} & 43.04 & \textbf{40.28} & 36.20 \\ \bottomrule
\end{tabular}
}
% \vspace{0.3em}
\label{tab:stf_result}
% \vspace{0.5em}
\end{table*}

% \vspace{-0.5cm}
\subsection{Training}\label{sec:training}
We now present the training of our UMoE module. We first train the sensor-specific detectors and then fix them to train the UMoE module. For sensor-specific detectors, we add dropout layers to enable model uncertainty estimation using the MC-Dropout approach. Moreover, we follow the Direct Modeling approach to add the following loss function to the training of the sensor-specific detectors:
$\mathcal{L}_\mathrm{add}=\frac{1}{2}\exp(-\log(\vec{\sigma}^2))||\vec{b_{gt}}-\vec{b}||+\frac{1}{2}\log(\vec{\sigma}^2)$,
where $\vec{\sigma}$ is the estimated data uncertainty, $\vec{b}$ is the predicted bounding box, and $\vec{b_{gt}}$ is the corresponding ground truth. With modified sensor-specific detectors, we can train the UMoE module solely or with a proposal-level fusion network in an end-to-end manner. In this way, the UMoE module learns to produce uncertainty-encoded confidence score via supervision from final detection $\vec{p}^*$, i.e., the 3D predictions generated by the fusion network.
