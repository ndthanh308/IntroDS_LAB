Perception is a core subsystem of autonomous driving (AD) where onboard sensors such as LiDAR, camera, and radar are used to sense the surrounding environment. Object detection is one of the most critical perception tasks which localizes and identifies the objects of interest as important prerequisites to autonomous navigation. Recently, multi-modal fusion-based AD object detection has received enormous attention from both academia \cite{feng2020deep} and industry \cite{waymo,pony}. In particular, as LiDAR and camera provide fundamentally different and complementary information about the objects (i.e., depth and visual features), the fusion based on these two modalities has shown initial promising results for object detection \cite{huang2022multi}.

Recently, {\em predictive uncertainty} is proposed to measure the variability of model predictions under plausible inputs \cite{kendall2017uncertainties}. It has been used to measure the quality of single-modal object detection results \cite{feng2018towards,miller2019evaluating}. In the context of multi-modal fusion, we conjecture that the uncertainty regarding the sensing result in each modality is valuable to the fusion. For instance, when a sensor experiences transient interference, the resulting high uncertainty value is an important indicator for tuning down the weight of the corresponding sensing result in the fusion. However, incorporating uncertainty into fusion-based AD object detection has not received systematic study. Most existing LiDAR-camera fusion approaches \cite{sindagi2019mvx,yoo20203d,pang2020clocs} do not consider uncertainties. They may yield performance degradation when a sensor experiences sensing quality drop in certain adverse settings.
The study in \cite{feng2020leveraging} is the only work considering uncertainty in fusion-based object detection. 
However, it only considers the uncertainty of LiDAR's sensing result and does not incorporate multi-modal uncertainty into the fusion-based object detection algorithm.
Thus, it falls short of addressing the scenarios adverse on camera.

This paper aims at advancing the state of the art by designing LiDAR-camera fusion for AD object detection with each modality's predictive uncertainty incorporated. However, this turns out to be challenging, because i) there lacks informative and practical uncertainty representations, ii) the LiDAR's and camera's uncertainties, as dimensionless quantities, are not directly comparable, and iii) their sensitivities to various adverse conditions are greatly different. These properties render straightforward ways of incorporating uncertainties,  e.g., admitting raw uncertainties as fusion inputs, futile.

To address the challenges, we propose a new multi-modal fusion module called {\em Uncertainty-Encoded Mixture-of-Experts} (UMoE) for robust AD object detection.
First, UMoE applies the Monte Carlo Dropout \cite{gal2016dropout} and Direct Modeling \cite{kendall2017uncertainties} approaches to estimate each sensor's uncertainty.
Then, individual expert network is used to process each sensor's detection result with uncertainty encoded. 
Lastly, the output features of each expert network are analyzed by a gating network to determine the weights for fusion.
With the uncertainty encoding for both modalities, UMoE can retain the object detection performance or allow more graceful performance degradation when a single sensor or both sensors suffer sensing quality drops in adverse scenarios.


% Motivation Example Figure: Qualitative analysis
% Figure environment removed

This paper's contributions are summarized as follows:
\begin{itemize}
\setlength{\itemsep}{0pt}
\item We identify the challenges caused by the cross-modality properties of uncertainty in the multi-modal fusion design, i.e., distinct uncertainty value ranges and varied sensitivities under different adverse conditions. Based on the understanding, we encode the LiDAR and camera uncertainties into comparable scores that can be leveraged to refine detections across modalities.

\item We propose UMoE that applies encoded uncertainties to weigh and fuse the two sensing modalities for robust AD object detection. As a desirable feature, the UMoE module can be incorporated into any proposal-level fusion methods. \iffalse in a plug-and-play style \fi  To the best of our knowledge, UMoE is the first modularized mechanism incorporating multi-modal uncertainties into AD object detection.
    
\item Experiments show that UMoE outperforms advanced and state-of-the-art LiDAR-camera fusion models on real-world and synthetic datasets, including clear, snowy, foggy, adversarial, and blinding attack scenarios.
\end{itemize}

