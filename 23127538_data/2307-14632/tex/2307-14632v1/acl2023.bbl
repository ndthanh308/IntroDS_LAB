\begin{thebibliography}{29}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Alva-Manchego et~al.(2020)Alva-Manchego, Martin, Bordes, Scarton,
  Sagot, and Specia}]{alva-manchego-etal-2020-asset}
Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton,
  Beno{\^\i}t Sagot, and Lucia Specia. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.424} {{ASSET}: {A}
  dataset for tuning and evaluation of sentence simplification models with
  multiple rewriting transformations}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 4668--4679, Online. Association for
  Computational Linguistics.

\bibitem[{Alva-Manchego et~al.(2019)Alva-Manchego, Martin, Scarton, and
  Specia}]{alva-manchego-etal-2019-easse}
Fernando Alva-Manchego, Louis Martin, Carolina Scarton, and Lucia Specia. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-3009} {{EASSE}: Easier
  automatic sentence simplification evaluation}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP): System Demonstrations}, pages
  49--54, Hong Kong, China. Association for Computational Linguistics.

\bibitem[{Alva-Manchego et~al.(2021)Alva-Manchego, Scarton, and
  Specia}]{alva-manchego-etal-2021-un}
Fernando Alva-Manchego, Carolina Scarton, and Lucia Specia. 2021.
\newblock \href {https://doi.org/10.1162/coli_a_00418} {The (un)suitability of
  automatic evaluation metrics for text simplification}.
\newblock \emph{Computational Linguistics}, 47(4):861--889.

\bibitem[{Barbu et~al.(2015)Barbu, Martín-Valdivia, Martínez-Cámara, and
  López}]{autistic}
Eduard Barbu, Maria Martín-Valdivia, Eugenio Martínez-Cámara, and L.~López.
  2015.
\newblock \href {https://doi.org/10.1016/j.eswa.2015.02.044} {Language
  technologies applied to document simplification for helping autistic people}.
\newblock \emph{Expert Systems with Applications}, 42:5076–5086.

\bibitem[{Brown et~al.(2020{\natexlab{a}})Brown, Mann, Ryder, Subbiah, Kaplan,
  Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020{\natexlab{a}}.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:1877--1901.

\bibitem[{Brown et~al.(2020{\natexlab{b}})Brown, Mann, Ryder, Subbiah, Kaplan,
  Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei. 2020{\natexlab{b}}.
\newblock \href {https://doi.org/10.48550/ARXIV.2005.14165} {Language models
  are few-shot learners}.

\bibitem[{Chamovitz and Abend(2022)}]{chamovitz2022cognitive}
Eytan Chamovitz and Omri Abend. 2022.
\newblock Cognitive simplification operations improve text simplification.
\newblock In \emph{Proceedings of the 26th Conference on Computational Natural
  Language Learning (CoNLL)}, pages 241--265.

\bibitem[{Dong et~al.(2023)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, Li, and
  Sui}]{surveyICL}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun,
  Jingjing Xu, Lei Li, and Zhifang Sui. 2023.
\newblock \href {https://doi.org/10.48550/ARXIV.2301.00234} {A survey for
  in-context learning}.

\bibitem[{Gonen et~al.(2022)Gonen, Iyer, Blevins, Smith, and
  Zettlemoyer}]{Gonen22}
Hila Gonen, Srini Iyer, Terra Blevins, Noah~A. Smith, and Luke Zettlemoyer.
  2022.
\newblock \href {https://doi.org/10.48550/arXiv.2212.04037} {Demystifying
  prompts in language models via perplexity estimation}.
\newblock \emph{CoRR}, abs/2212.04037.

\bibitem[{Kincaid et~al.(1975)Kincaid, Fishburne~Jr, Rogers, and
  Chissom}]{kincaid1975derivation}
J~Peter Kincaid, Robert~P Fishburne~Jr, Richard~L Rogers, and Brad~S Chissom.
  1975.
\newblock Derivation of new readability formulas (automated readability index,
  fog count and flesch reading ease formula) for navy enlisted personnel.
\newblock Technical report, Naval Technical Training Command Millington TN
  Research Branch.

\bibitem[{Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.703} {{BART:}
  denoising sequence-to-sequence pre-training for natural language generation,
  translation, and comprehension}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pages
  7871--7880. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2022)Liu, Shen, Zhang, Dolan, Carin, and Chen}]{kategpt}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu
  Chen. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.deelio-1.10} {What makes good
  in-context examples for gpt-3?}
\newblock In \emph{Proceedings of Deep Learning Inside Out: The 3rd Workshop on
  Knowledge Extraction and Integration for Deep Learning Architectures,
  DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022}, pages 100--114.
  Association for Computational Linguistics.

\bibitem[{Lu et~al.(2022)Lu, Bartolo, Moore, Riedel, and
  Stenetorp}]{lu-etal-2022-fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
  2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.556} {Fantastically
  ordered prompts and where to find them: Overcoming few-shot prompt order
  sensitivity}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 8086--8098,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Martin et~al.(2022)Martin, Fan, de~la Clergerie, Bordes, and
  Sagot}]{martin-etal-2022-muss}
Louis Martin, Angela Fan, {\'E}ric de~la Clergerie, Antoine Bordes, and
  Beno{\^\i}t Sagot. 2022.
\newblock \href {https://aclanthology.org/2022.lrec-1.176} {{MUSS}:
  Multilingual unsupervised sentence simplification by mining paraphrases}.
\newblock In \emph{Proceedings of the Thirteenth Language Resources and
  Evaluation Conference}, pages 1651--1664, Marseille, France. European
  Language Resources Association.

\bibitem[{Nassar et~al.(2019)Nassar, Ananda{-}Rajah, and Haffari}]{NassarAH19}
Islam Nassar, Michelle Ananda{-}Rajah, and Gholamreza Haffari. 2019.
\newblock \href {https://aclanthology.org/U19-1023/} {Neural versus non-neural
  text simplification: {A} case study}.
\newblock In \emph{Proceedings of the The 17th Annual Workshop of the
  Australasian Language Technology Association, {ALTA} 2019, Sydney, Australia,
  December 4-6, 2019}, pages 172--177. Australasian Language Technology
  Association.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu}]{papineni-etal-2002-bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
\newblock \href {https://doi.org/10.3115/1073083.1073135} {{B}leu: a method for
  automatic evaluation of machine translation}.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association
  for Computational Linguistics}, pages 311--318, Philadelphia, Pennsylvania,
  USA. Association for Computational Linguistics.

\bibitem[{Qiang et~al.(2020)Qiang, Li, Yi, Yuan, and Wu}]{qiang2020BERTLS}
Jipeng Qiang, Yun Li, Zhu Yi, Yunhao Yuan, and Xindong Wu. 2020.
\newblock Lexical simplification with pretrained encoders.
\newblock \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence}, page
  8649–8656.

\bibitem[{Rello et~al.(2013)Rello, Baeza-Yates, Bott, and
  Saggion}]{10.1145/2461121.2461126}
Luz Rello, Ricardo Baeza-Yates, Stefan Bott, and Horacio Saggion. 2013.
\newblock \href {https://doi.org/10.1145/2461121.2461126} {Simplify or help?
  text simplification strategies for people with dyslexia}.
\newblock In \emph{Proceedings of the 10th International Cross-Disciplinary
  Conference on Web Accessibility}, W4A '13, New York, NY, USA. Association for
  Computing Machinery.

\bibitem[{Rubin et~al.(2022)Rubin, Herzig, and
  Berant}]{rubin-etal-2022-learning}
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.naacl-main.191} {Learning to
  retrieve prompts for in-context learning}.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2655--2671, Seattle, United States. Association for
  Computational Linguistics.

\bibitem[{Sheang and Saggion(2021)}]{sheang-saggion-2021-controllable}
Kim~Cheng Sheang and Horacio Saggion. 2021.
\newblock \href {https://aclanthology.org/2021.inlg-1.38} {Controllable
  sentence simplification with a unified text-to-text transfer transformer}.
\newblock In \emph{Proceedings of the 14th International Conference on Natural
  Language Generation}, pages 341--352, Aberdeen, Scotland, UK. Association for
  Computational Linguistics.

\bibitem[{Sorensen et~al.(2022)Sorensen, Robinson, Rytting, Shaw, Rogers,
  Delorey, Khalil, Fulda, and Wingate}]{SorensenRRSRDKF22}
Taylor Sorensen, Joshua Robinson, Christopher~Michael Rytting, Alexander~Glenn
  Shaw, Kyle~Jeffrey Rogers, Alexia~Pauline Delorey, Mahmoud Khalil, Nancy
  Fulda, and David Wingate. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.60} {An
  information-theoretic approach to prompt engineering without ground truth
  labels}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, pages 819--862. Association for Computational
  Linguistics.

\bibitem[{Stajner(2021)}]{stajner-2021-automatic}
Sanja Stajner. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.findings-acl.233} {Automatic
  text simplification for social good: Progress and challenges}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pages 2637--2652, Online. Association for Computational
  Linguistics.

\bibitem[{Sulem et~al.(2018)Sulem, Abend, and Rappoport}]{sulem-etal-2018-bleu}
Elior Sulem, Omri Abend, and Ari Rappoport. 2018.
\newblock \href {https://doi.org/10.18653/v1/D18-1081} {{BLEU} is not suitable
  for the evaluation of text simplification}.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 738--744, Brussels, Belgium. Association
  for Computational Linguistics.

\bibitem[{Tanprasert and Kauchak(2021)}]{tanprasert-kauchak-2021-flesch}
Teerapaun Tanprasert and David Kauchak. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.gem-1.1} {Flesch-kincaid is
  not a text simplification evaluation metric}.
\newblock In \emph{Proceedings of the 1st Workshop on Natural Language
  Generation, Evaluation, and Metrics (GEM 2021)}, pages 1--14, Online.
  Association for Computational Linguistics.

\bibitem[{Williams et~al.(2003)Williams, Reiter, and
  Osman}]{williams-etal-2003-experiments}
Sandra Williams, Ehud Reiter, and Liesl Osman. 2003.
\newblock \href {https://aclanthology.org/W03-2317} {Experiments with
  discourse-level choices and readability}.
\newblock In \emph{Proceedings of the 9th {E}uropean Workshop on Natural
  Language Generation ({ENLG}-2003) at {EACL} 2003}, Budapest, Hungary.
  Association for Computational Linguistics.

\bibitem[{Xu et~al.(2016)Xu, Napoles, Pavlick, Chen, and
  Callison-Burch}]{xu-etal-2016-optimizing}
Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch.
  2016.
\newblock \href {https://doi.org/10.1162/tacl_a_00107} {Optimizing statistical
  machine translation for text simplification}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  4:401--415.

\bibitem[{Zhang et~al.(2020)Zhang, Kishore, Wu, Weinberger, and
  Artzi}]{bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q. Weinberger, and Yoav Artzi.
  2020.
\newblock \href {https://openreview.net/forum?id=SkeHuCVFDr} {Bertscore:
  Evaluating text generation with {BERT}}.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net.

\bibitem[{Zhang et~al.(2022)Zhang, Feng, and Tan}]{ZhangFT22}
Yiming Zhang, Shi Feng, and Chenhao Tan. 2022.
\newblock \href {https://aclanthology.org/2022.emnlp-main.622} {Active example
  selection for in-context learning}.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
  December 7-11, 2022}, pages 9134--9148. Association for Computational
  Linguistics.

\bibitem[{Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh}]{zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021.
\newblock \href {http://proceedings.mlr.press/v139/zhao21c.html} {Calibrate
  before use: Improving few-shot performance of language models}.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 12697--12706. {PMLR}.

\end{thebibliography}
