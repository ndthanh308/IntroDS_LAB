\definecolor{Mycolor1}{HTML}{004488}
\definecolor{Mycolor2}{HTML}{DDAA33}
\definecolor{Mycolor3}{HTML}{BB5566}

\begin{table*}[htbp]
         \centering
            \small
             \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{p{0.2\linewidth} p{0.75\linewidth}} \toprule
        Metric   & Top 2 Examples \\   \midrule                                                                                                     
        Compression Ratio   & \textbf{Complex Sentence} They \textcolor{Mycolor1}{manifest} with either \textcolor{Mycolor2}{neurological complications} or with skin problems (or \textcolor{Mycolor3}{occasionally} both).  \newline                                   \textbf{Simple Sentence}  They \textcolor{Mycolor1}{show} either \textcolor{Mycolor2}{brain} or skin \textcolor{Mycolor2}{problems} (or both).                                                                            \\ \hline
                            & \textbf{Complex Sentence} The \textcolor{Mycolor1}{psychological state of sympathy} is closely linked with \textcolor{Mycolor3}{that of} compassion, empathy and \textcolor{Mycolor3}{empathic concern}. \newline \textbf{Simple Sentence}                             \textcolor{Mycolor1}{Sympathy} is closely linked with compassion and empathy.                                                                       \\
        \midrule
        BertScore Precision & \textbf{Complex Sentence} Sthenurine forelimbs were long with two extra-long fingers and claws compared with the \textcolor{Mycolor3}{relatively} small, stiff arms of modern macropods.  \newline \textbf{Simple Sentence}  Sthenurine forelimbs were long with two extra-long fingers and claws compared with the small, stiff arms of modern macropods. \\ \hline
                            & \textbf{Complex Sentence} In 1828, Coenraad Johannes van Houten \textcolor{Mycolor1}{developed} the first cocoa powder producing machine in the Netherlands.    \newline \textbf{Simple Sentence}                            In 1828, Coenraad Johannes van Houten \textcolor{Mycolor1}{created} the first cocoa powder producing machine in the Netherlands.                    \\
        \midrule
        SARI                & \textbf{Complex Sentence} The organic matter in soil \textcolor{Mycolor1}{derives} from plants and animals.  \newline \textbf{Simple Sentence}                                                                               The organic matter in soil \textcolor{Mycolor1}{comes} from plants and animals.                                                                     \\ \hline
                            & \textbf{Complex Sentence} Dennis Lee Hopper (born May 17, 1936) is an American actor, filmmaker and artist.                                           \newline       \textbf{Simple Sentence}        Dennis Lee Hopper was born on May 17, 1936. \underline{He is an American actor, filmmaker and artist.}     \\
        \bottomrule
        \end{tabular}
        \caption{Top 2 examples from each applicable selection metric (random and KATE-GPT selection were not applicable). All samples taken from the ASSET Validation dataset. We color rephrases first in \textcolor{Mycolor1}{blue} and then in \textcolor{Mycolor2}{yellow}, mark significant deletions in \textcolor{Mycolor3}{red}, and \underline{underline} sentence splits.} 
     \label{tab:top2examples}
    \end{table*}

% \todo[inline, color=red!40]{Please move this table (The one with kategpt bertprec sari things) to appendix and fix formatting, i think k's can be rotated and put vertically or something} 

In this section, we perform a qualitative analysis of different model generated simplifications and metric-based prompting examples in order to better understand how different settings affect model outputs. 

\subsection{Explaining Performance as $k$ Increases}
We aim to understand why certain metrics (BERTPrec and KATE-GPT) tend to perform worse as $k$ increases, while other metrics (SARI) tend to perform better as $k$ increases when evaluated on SARI scores (as seen in Fig.~\ref{fig:dataset_all_models}). \ggrev{old text}{In fact, this result is commonly seen in other papers~\citep{zhao2021calibrate,ZhangFT22}, where they describe that adding more training examples can sometimes hurt accuracy.} By analyzing output of metric-based selection on a fixed dataset and model (ASSET, GPT-$175$B) seen in Appendix~\ref{ssec:dv3asset}, we aim to understand the performance of different metrics as the number of examples, or $k$, increases. Our analysis focuses on three different metrics (KATE-GPT, BERTPrec, and SARI) and a particularly difficult example due to its unconventional subject nature, multiple abbreviations, and unknown words, and objectively confusing sentence structure. In general, we see from earlier trends that KATE-GPT and BERTPrec-selected examples tend to get worse (w.r.t SARI) as $k$ increases (see Figure~\ref{fig:davinci_asset_all}-top). We also observe this qualitatively, as $k$ increases, KATE-GPT and BERTPrec examples become closer to the original sentence, with BERTPrec generations even matching the original sentence at $k=15$. %However, SARI-selected examples tend to increase in quality as $k$ increases, with  we see that examples selected using the SARI score metric tend to i) split sentences more frequently and ii) decode potentially confusing abbreviations, like "OEL". 
However, as the value of k increases, SARI-selected examples show an improvement in quality. We observe that examples selected using the SARI score metric tend to: i) split sentences more frequently, and ii) decode potentially confusing abbreviations, such as ``OEL''.

\textbf{Sentence Splitting: } SARI-selected examples are more prone to splitting sentences (see $k$=2,15), which may be in-part due to the style of the top SARI examples, which include sentence splitting; while this is not present in any of the other metrics. See Appendix \ref{ssec:dv3asset} for examples. Sentence splitting is correlated with increased human comprehension of TS outputs~\cite{williams-etal-2003-experiments}. This is particularly interesting because it leads us to infer that models can potentially learn the ``style'' of the reference sentences. 

\textbf{Abbreviations:} In all three cases, ($k=2,8,15$) examples selected by SARI score remove the potentially confusing abbreviation ``OEL'' and instead replace it with either ``original English-language'' or ``English-language'', while KATE-GPT and BERTPrec selected examples only exhibit this behavior for $k=2$ (see Appendix~\ref{ssec:dv3asset}).

\subsection{Analyzing Model Size}
Model size plays a significant role in output sentences, with smaller models (especially GPT-$6.7$B) tending to change very little structurally from the original sentence, regardless of the metric used to select examples. See Appendix \ref{sec:modeloutputs} for a complete list of model outputs on all metrics for the original sentence ``OEL manga series Graystripe's Trilogy There is a three volume original English-language manga series following Graystripe, between the time that he was taken by Twolegs in Dawn until he returned to ThunderClan in The Sight''. From these results, we conclude that GPT-$6.7$B tends to hardly change sentences at all, with both Random and BERTPrec-selected examples having no change from the original sentence. SARI-selected examples adds a comma, but CR-selection prompts the model to rephrase key parts of the sentence. GPT-$13$B performs considerably better when looking at a qualitative analysis, as all examples have removed ``OEL manga series Graystripe's Trilogy'' and restructured the sentence to be more concise, and SARI-selected going as far to remove an ambiguous abbreviation ``OEL''. These qualitative observations are consistent with our results from Figure~\ref{fig:davinci_asset_all}.

\subsection{Analyzing Top Metric-Selected Examples}
In this section, we analyze the top metric-selected examples for compression ratio, BERTPrec, and SARI. In Table~\ref{tab:top2examples} we include the top $2$ examples for each metric from the ASSET validation dataset, and in Appendix~\ref{sec:topasset} we include the remaining top $8$ examples for SARI and BERTPrec selection. 

Looking at the style of both BERTPrec and SARI score, both metrics' top examples barely change from the original sentences, often only changing one or two words (i.e., movie $\rightarrow$ film) but leaving the rest unchanged, primarily using deletion or rewriting operations. However, in CR top examples, we see extreme deletions from the original sentences and several rewriting operations done (which is consistent with our understanding of the compression ratio). Notably, we also see that top SARI examples are the only metric that use sentence splitting (see the $2$nd example under SARI from Table~\ref{tab:top2examples}). 
