% What is TS and why is it important
Text simplification~(TS) is a crucial task in natural language processing, with the goal of converting complex text into simpler, easier-to-understand one. This is particularly important for individuals who struggle with comprehending complex languages, such as second language learners or individuals with cognitive impairments~\citep{stajner-2021-automatic} and disabilities like dyslexia~\citep{10.1145/2461121.2461126} and autism~\citep{autistic}. For the aforementioned reasons, NLP community has shown great interest in the topic, introducing plenty of datasets (e.g., ASSET \citep{alva-manchego-etal-2020-asset}), models, and evaluation metrics (e.g., SARI \citep{xu-etal-2016-optimizing}). 
%Recently, work on a sub-task of simplification, cognitive simplification (CS), aims to improve simplification specifically for those with cognitive disabilities. To this effect, the authors of \citep{cognitive} have also released a new dataset, FestAbility, which is an English parallel corpus aimed at CS.

% large language models for TS
There have been numerous approaches to TS proposed in the literature, including non-neural or rule-based methods~\citep{NassarAH19}, machine translation approaches~\citep{xu-etal-2016-optimizing}, and finetuning of large language models~\citep{sheang-saggion-2021-controllable} on downstream task data. Recently, it has been shown that large language models such as GPT-3, are capable of in-context learning~(ICL)~\cite{brown2020language}---an emerging ability to learn from in-context samples without modifying model parameters.~\footnote{We refer the readers to \url{http://ai.stanford.edu/blog/understanding-incontext/} for a summary of in-context learning inner mechanics.} Despite its strong ability, ICL still mostly falls behind the performance of finetuning techniques~\citep{surveyICL}. 

% Even though ICL has been investigated for several tasks such as NLI, and sentiment analysis;   %ICL has been investigated for several NLP tasks such as NLI, se
%has significantly improved over transfer-learning based approaches by its ability to generalize on downstream tasks without the need for modifying model parameters, known as in-context learning. 

Recent studies have shown that in-context learning is highly variable to a range of factors, such as the number of examples, quality of examples, and even the order of examples~\citep{lu-etal-2022-fantastically,kategpt,surveyICL}. To address these concerns, recent literature has proposed several techniques for selecting the most relevant examples for ICL~\citep{kategpt, SorensenRRSRDKF22,Gonen22,rubin-etal-2022-learning}. The majority of them aim to \textit{retrieve} a set of samples from the validation set that resembles the test set most by either training a separate retrieval model or utilizing an existing encoder to calculate similarities between pairs of sentences. However, adopting these techniques for text-generation tasks with multiple references is nontrivial, and the need to access to the full test set to pick examples from is not desirable, and may not always be possible in real-life scenarios.   

%To the best of our knowledge, very few studies have analyzed the effect and robustness of in-context learning in the TS domain.
%evaluating their performance based primarily on the SARI metric for text simplification~\citep{xu-etal-2016-optimizing}. 

In order to address this problem, we propose a simple yet intuitive metric-based selection technique, which we refer to as \textbf{M}etric-\textbf{B}ased in-context \textbf{L}earning~(MBL), to perform efficient and robust in-context learning with large language models for text generation tasks. Unlike previous ICL techniques, MBL only requires access to the development set and uses more informed measures rather than requiring generating sentence embeddings or training separate specified retrieval models. Furthermore, we perform an extensive set of experiments with GPT-3 models of various sizes ($175$B, $13$B, and $6.7$B)~\footnote{Throughout the paper, GPT-$175$B, GPT-$13$B, and GPT-$6.7$B will refer to the GPT-$3$ model with $175$B, $13$B, and $6.7$B parameters respectively.}, specifically focusing on their performance for TS. We investigate utilizing commonly used TS metrics (e.g., SARI, compression ratio) for example selection and answer several research questions regarding their strengths and weaknesses on a variety of datasets and models. %We propose that utilizing commonly used TS metrics (SARI, compression ratio) 
Through our experiments, we show that metric-based selection can significantly improve the performance of large language models on TS. We also demonstrate that these results are generally robust to various orderings and perform well in out-of-domain settings. This paper provides the following contributions:

\begin{itemize}
  \item We provide a naive yet effective and robust approach to selecting examples for in-context learning, a.k.a., \textit{metric-based learning}~(MBL)~\footnote{We use metric-based selection and learning interchangeably.}, and show that it achieves state-of-the-art results on two well-known benchmark datasets (TurkCorpus and ASSET when the optimal metric is used (see \S\ref{ssec:rq1}))~\footnote{We find SARI score to be optimal for large models, while, Compression Ratio (CR) achieves the best scores for the smaller models.}.

   \item We demonstrate the robustness of MBL to example ordering (see \S\ref{ssec:rq2}) and to out-of-domain test sets with some exceptions (see \S\ref{ssec:rq3}), suggesting that the order of examples and the origin of the development data are not the most important factors for MBL. 
    
    \item We show that MBL improves upon important baselines (e.g., zero-shot, random selection), state-of-the art ICL selection (e.g., KATE-GPT~\cite{kategpt}) and text simplification methods~\citep{sheang-saggion-2021-controllable} (see \S\ref{ssec:rq4}).
  
    \item \textbf{Our results suggest that GPT-175B can be \textit{implicitly controlled}} via optimal metric-based learning, i.e., BERTScore Precision-based learning optimizes BLEU, while SARI-based selection optimizes SARI scores.
    
\end{itemize}
We release all generation outputs, baseline models and evaluation scripts publicly with \url{https://github.com/NLP-KU/metric-based-in-context-learning/}.
%Our approach is especially generalizable as i) it can be applied to any text-generation task and ii) it is not based on test-validation dataset similarities.