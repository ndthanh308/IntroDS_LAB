

\paragraph{Text Simplification (TS) Methods} 
%The prevalence of large language models (LLMs) has vastly increased as they have achieved impressive performance on a wide range of few-shot and zero-shot text rewriting and generation benchmarks. 
Recently, LLMs have been applied to text simplification through transfer learning approaches. For instance, \citet{qiang2020BERTLS} fine-tuned a BERT model on a text simplification dataset, achieving strong results on multiple benchmarks. Similarly, \citet{sheang-saggion-2021-controllable} introduced a transfer learning approach for text simplification using the T5 model and achieved current state-of-the-art results on standard TS benchmarks. Recent work in the TS domain has a particular focus on controllable text simplification, in which different ``control tokens'' are embedded in seq2seq models to control model outputs. This is seen in both \citet{sheang-saggion-2021-controllable} and \citet{chamovitz2022cognitive}, where a large language model (BART, T5, etc.) is modified with several control tokens, like the number of words, Levenshtein similarity, and various text rewriting operations. A vast amount of earlier systems (e.g., \citep{xu-etal-2016-optimizing}) have formulated text simplification as a machine translation task and employed neural machine translation architectures. 

\paragraph{TS Evaluation} Work on the suitability of various metrics for TS has also been an active area of discussion. While the most commonly used metric in TS is currently SARI~\citep{xu-etal-2016-optimizing}, there is a concern over the metric that best correlates with human judgment. \citet{alva-manchego-etal-2021-un} conduct a detailed analysis of several commonly used metrics in the TS field, and suggest BERTScore\_Precision as a primary metric of reference-based evaluation. Following these results, we also use BERTScore\_Precision as a metric to select examples. Recent studies~\citep{sulem-etal-2018-bleu, tanprasert-kauchak-2021-flesch} analyzing the suitability of the other two common metrics, namely BLEU \citep{papineni-etal-2002-bleu} and FKGL \citep{kincaid1975derivation}, strongly advise against these metrics for TS. For these reasons, we do not select examples based on either metric.

\paragraph{Example Selection and Ordering in ICL} While large language models like GPT-$3$ perform exceptionally well on a variety of downstream tasks, selecting examples for in-context learning is nontrivial. Research on example selection is still in early stages, and a unified approach to selecting examples for downstream tasks has not yet been proposed~\citep{surveyICL}. \citet{kategpt} propose selecting the $k$ most similar examples to the test set from the training/development set via measuring cosine distance in an embedding space (e.g., encodings from RoBERTa), and achieve strong results on various tasks like table-to-text generation. On a similar line, \citet{rubin-etal-2022-learning} introduce a more sophisticated method, where the authors train a two-step retrieval model to select ICL examples. Another set of work focus on optimizing prompts via mutual information~\citep{SorensenRRSRDKF22} or perplexity~\citep{Gonen22}, that don't require labeled sets. We consider Kate-GPT~\citep{kategpt} as the closest work to ours, since both the intuition (i.e., choosing from a labeled validation set) and approach (i.e., learning-free) are similar. 

