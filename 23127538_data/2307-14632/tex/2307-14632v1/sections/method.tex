Following the line of work for retrieving the best samples from development set~\cite{kategpt,SorensenRRSRDKF22}, we introduce a simple and intuitive technique based on employing standard evaluation metrics for selecting the examples. 

\paragraph{Task Setup} Given the list of sentences $l = [c, r_1, r_2,..., r_n]$, where $c$ is the complex and $r_i$ is the simple reference sentence; our goal is to find the best $k$ pairs, $[c, r_i]$, such that the final text simplification performance on the test set is maximized. To do so, we go through each $l$ in the development set and measure the distance between each $c$ and $r_i$ according to a metric, $m$. Finally, we pick the top $k$ pairs and fill the prompt template with the samples: ``Complex sentence: \{$c$\}, Simple sentence:\{$r_i$\}''.    

We initially considered a long list of task-specific as well as general generation metrics that contain the standard evaluation metrics for TS, namely as SARI, BLEU, FKGL; as well as a simple analysis metric: Compression Ratio, and a more recent textual similarity metric BERTScore as suggested by \citet{alva-manchego-etal-2021-un}. Following the criticisms by \citet{sulem-etal-2018-bleu} and \citet{tanprasert-kauchak-2021-flesch}, we removed BLEU and FKGL from the list of candidate metrics.

\paragraph{Compression Ratio (CR)} It is simply calculated by dividing the number of characters in $c$ by the number of characters $r_i$. We consider the pairs with higher compression ratios as more preferable candidates for TS.

\paragraph{BERTScore Precision (BP)~\citep{bertscore}} BERTScore computes the cosine similarity between each token in the candidate, $y$, and reference, $x$, sentences. Precision is calculated as:
\begin{equation}
 \textnormal{Prec} = \frac{1}{|y|} \sum_{y_j \in y} \max_{x_i \in x}  x_{i}^\top y_{j}  
\end{equation}
%Research \citep{alva-manchego-etal-2021-un} shows that the precision subscore correlates well with human judgement in text simplification. 
We discard pairs with a score of 1 since they would simply be duplicates.

\paragraph{SARI~\citep{xu-etal-2016-optimizing}} It is the defacto standard evaluation metric for TS. In general terms, it compares prediction against both the input and the reference sentences. It calculates a weighted average of F1 scores for three operations: addition, deletion, and keeping. Precision and recalls for each operation are calculated based on n-gram overlaps between the prediction, input and reference sentences. To calculate the SARI score for each $c$-$r_i$ pair, we denote $r_i$ as the prediction, $c$ as the input, and $[r_1,...,r_{i-1}, r_{i+1},..., r_n]$ as the reference sentences. Hence, this measure can only be applied when there are multiple references. 





