% Figure environment removed
We conduct a comprehensive set of experiments with the setup explained in \S \ref{sec:setup}. Following the work by \citet{lu-etal-2022-fantastically}, we experiment with the $k$ values as $1,2,4,6,8,10,15,20$ examples. We repeat the random baseline experiments three times for each $k$. We aim to answer the following research questions (RQ):

\paragraph{RQ1:} How do different metric-based selection techniques compare?~(\S\ref{ssec:rq1})

\paragraph{RQ2:} Is metric-based sample selection robust to the order of the prompts?~(\S\ref{ssec:rq2}) 

\paragraph{RQ3:} How does metric-based ICL compare to state-of-the-art text simplification methods?~(\S\ref{ssec:rq3})

\paragraph{RQ4:} Does metric-based selection performance on one dataset transfer to other out-of-domain datasets?~(\S\ref{ssec:rq4})

\subsection{RQ1: Effect of Metrics} 
\label{ssec:rq1}
Our main results with our default settings (GPT-$175$B on ASSET) is shown in Fig.~\ref{fig:davinci_asset_all}. First of all, we observe that the random baseline is quite strong on average, however, with a \textbf{large variation} for most $k$ values; while zero-shot results are quite weak for all datasets. Interestingly, SARI-based selection consistently leads to the highest SARI scores for $k>1$, while BERTPrec-based selection gives the highest BLEU and lowest SARI scores consistently for each $k$. Kate-GPT follows BERTPrec-based selection for the BLEU score, while providing results on par or lower than the random baseline for the SARI score. 

% Figure environment removed

Next, we check whether our findings hold for smaller models. In Fig.~\ref{fig:babbage_asset_all}, we show the results of our smallest model, GPT-6.7B on the ASSET dataset. Since the zero-shot results were significantly lower than $k=1$, we show them in Table~\ref{tab:zeroShot}, rather than plotting. Not surprisingly the highest SARI scores are achieved via the largest model; however, the opposite is not true for BLEU. The smallest model achieves the highest BLEU scores that raises another warning flag for using BLEU for TS evaluation. 

Similar to the larger model, BERTPrec-based selection achieves the highest BLEU, and the lowest SARI scores. SARI-based selection provides considerably high SARI scores only for larger $k$s, suggesting the implicit controlling mechanism does not exist, or is only triggered with more samples. We also observe that CR performs relatively better on GPT-6.7B which suggests compression provides a stronger signal (e.g., deletion, shorter tokens) that can be utilized better by smaller models for simplification. 

% Figure environment removed

Finally, we investigate how the quality of the dataset affects the metric-based selection techniques, i.e., whether they are robust to noise. Fig~\ref{fig:dataset_all_models} shows an overview of the SARI scores from all models on the noisy (i.e., TurkCorpus) and the cleaner (i.e., ASSET) dataset. Even though the general patterns are visible, the results on TurkCorpus are moderately less conclusive. 

\begin{table}[!htp]
\begin{center}
\scalebox{0.62}{
\begin{tabular}{lccrr} 
\toprule
\textbf{Dataset}& \textbf{Model} & \textbf{SARI} & \textbf{BLEU} \\
\midrule
\multirow{3}{*}{TurkCorpus} 
& GPT-$175$B & \textbf{32.17} & 42.34 \\
& GPT-$17$B & 27.19 & 38.35 \\
& GPT-$6.7$B & 24.13 & \textbf{57.14} \\
\midrule
\multirow{3}{*}{ASSET} 
& GPT-$175$B & \textbf{38.49} & 60.48 \\
& GPT-$17$B & 30.45 & 40.13 \\
& GPT-$6.7$B & 26.28 & \textbf{69.83} \\
\bottomrule
\end{tabular}
\caption{Zero-shot results} 
\label{tab:zeroShot}
}
\end{center}
\end{table}

\paragraph{Selection Metric versus Evaluation Metric} Even though this was not one of our main research questions, we observe a strong relation between the metric used for MBL and the metric used for evaluation. For all the model and dataset size settings, we observe that BLEU scores are consistently higher when the examples are selected via BERTScore\_Precision. When we evaluate with the SARI score, SARI-metric behaves similarly for the GPT-175B model, however CR-metric performs better for the smaller models. More evidence for the relation between BLEU and BERTScore\_Precision can be found in Appendix~\ref{sec:bleuresults}. This suggests that the behaviour of large-enough GPT models can be \textit{implicitly controlled} via MBL, which paves the way to a new research direction and needs further investigation. 

\subsection{RQ2: Effect of Order}
\label{ssec:rq2}
%In continuation of prior research on ordering of examples for few-shot learning~\citep{lu-etal-2022-fantastically} we perform an experiment to determine if the best metrics are robust across sample ordering. 

Previous research~\citep{lu-etal-2022-fantastically} has shown that the order of the examples may have a significant impact on ICL performance. \ggrev{N/A}{Commonly used orderings~\citep{lu-etal-2022-fantastically} include sorting from highest to lowest quality example, vice versa, and random selection.} Inspired by these findings, we investigate the robustness of our selection metrics across sample orders. To do so, for each metric we perform three different order arrangements, namely as highest $\rightarrow$ lowest, lowest $\rightarrow$ highest, and random ordering for each metric. To have enough variation, we only experiment with $k=6,8,10,15$. As the baseline, we randomly pick samples and arrange them in 3 different randomized orders. % SARI %and BERT\_Prec experiments are run on GPT-175B, while CR experiments are performed on the smaller models. We experiment with both the TurkCorpus and ASSET dataset.
% Figure environment removed

In Fig~\ref{fig:boxplot_davinci_asset}, we show how the performance of GPT-$175$B varies on ASSET when the samples that are i) picked randomly, ii) by SARI-based selection and iii) by BERTPrec-based selection are reordered following the above setup. As can be seen, the best-performing metrics, are also the most robust compared to others. To elaborate, SARI-based selection that provided the highest SARI scores has the least variation, i.e., most robust to order; while BERTPrec-based selection provides the most stable BLEU scores along with the highest.  

% Put the box plots here. They all can be in the same plot. I think we can again have two plots here


\subsection{RQ3: Comparison to State-of-the-art}
\label{ssec:rq3}
%Following previous research on optimal example selection for text generation tasks as a whole, we implement a kNN-based approach to select examples from both the TurkCorpus and ASSET datasets using $6,8,10,15$ examples. We choose to only repeat these experiments on a higher number of examples based on results from \citep{kategpt}. All experiments in this section were evaluated on both the TurkCorpus and ASSET  and only on GPT-175B.
Finally, we compare our best and average model settings to state-of-the-art fine-tuned models~\footnote{The models which do not provide SOTA (e.g., KATE) are not included in the Table. The statistical significance cannot be provided since there is only one setting for the few-shot setting.}. The results are given in Table~\ref{tab:sota}. Here, Random and SARI averages are calculated from \S\ref{ssec:rq1} results, averaged over all $k$, with random selection being additionally averaged over all three random selections. These averages are reported for GPT-$175$B results, because it is generally the best model when considering averages across both datasets. As can be seen, the GPT-$175$B model with SARI-based selection outperforms existing results on all datasets, followed by random best and SARI-averaged. The exact settings (number of examples, model, and ordering) for SARI and Random Best can be found Appendix~\ref{sec:optimalsettings}.


\begin{table}[]
\begin{center}
\scalebox{0.6}{
\begin{tabular}{p{0.7\linewidth}ccc} 
\toprule
\textbf{Model}  & \textbf{ASSET} & \textbf{TurkCorpus} & \textbf{FestAbility} \\
\midrule
Finetuned T5                    & 45.04          & 43.31               & N/A         \\ 

MUSS (BART + ACCESS) & 43.63          & 42.62               & N/A         \\

BART-Large+Classifier           & 38.76          & N/A                 & 27.13       \\

\midrule
(Ours) Random-Best %(A, T k=$8$ rand) 
& 46.93          & 43.14               & N/A         \\

(Ours) Random-Average                  & 45.33          & 40.32               & N/A         \\
\midrule

(Ours) MBL-Best  
& \textbf{47.94} & \textbf{43.46}      & \textbf{44.86} \\

(Ours) MBL-Average                    & 46.63          & 41.78               & 43.55      \\
\bottomrule
\end{tabular}
\caption{Comparison to TS state-of-the-art models. Random- and MBL-best examples are selected from the top examples in all experiments run. %``A'', ``F'' and ``T'' in Random and SARI best refer to the best method of selecting examples for ASSET, FestAbility, and TurkCorpus respectively. 
The best results are shown in bold. For more information on the exact settings for MBL and Random Best, see Appendix~\ref{sec:optimalsettings}.} 
\label{tab:sota}
}
\end{center}
\end{table}



\subsection{RQ4: Task Transfer}
\label{ssec:rq4}

In order to evaluate the suitability of our approach for unseen tasks and datasets, we experiment with choosing samples from a tune set and testing the performance on an unseen set. Here, we use ASSET and TurkCorpus as the tune set and evaluate on all three datasets: ASSET, TurkCorpus, and FestAbility. To investigate different metrics and language models, we perform experiments with GPT-$175$B with SARI-based selection and GPT-$13$B with CR-based selection as both metric selection techniques are generally best on those respective models. We use the best experimental settings from Table \ref{tab:sota} in out-of-domain settings, comparing them with their in-domain counterparts. For example, the best setting for TurkCorpus is k=$6$ with high to low ordering (see Appendix \ref{sec:optimalsettings} for more details on optimal experiment settings), so we compare the results of the model when given this setting on both the TurkCorpus and ASSET datasets. State-of-the-art results in this table refer to the best setting for in-domain experiments (i.e. ASSET evaluated on ASSET or TurkCorpus evaluated on TurkCorpus).  

Our results are given in Table~\ref{tab:ICL}. For easy comparison, the Table also includes in-domain selection results as well as the current state-of-the-art scores taken from Table~\ref{tab:sota}. In the first row, we observe that samples selected from TurkCorpus and tested on ASSET achieve significantly lower SARI scores than their in-domain variant for the GPT-$175$B setting, whereas the gap is lower for the GPT-$13$B. On the other hand, for the TurkCorpus test setting (second row), we see that GPT-$175$B model prompted with the best ASSET examples achieves even better results than the in-domain setting, suggesting a highly successful transfer. This ability cannot be observed for the GPT-$13$B model with CR-based selection. The final row shows the transfer results to another related but different task. It is apparent that both models prompted with ASSET examples achieve marginally higher scores than the TurkCorpus ones.

Taking a look at the BLEU scores, we see that out-of-domain configurations on the TurkCorpus and ASSET datasets generally tend to match or even exceed their in-domain counterparts, suggesting a successful transfer. However, on the FestAbility dataset, we observe notably low BLEU scores, which are in-part due to the nature of FestAbility, in which sentences are often simplified in unconventional ways. Additionally, FestAbility sentences are extremely short, with only 1452 unique tokens in the original sentences and 996 unique tokens in the simplified sentences \citep{chamovitz2022cognitive}, leading to unconventional results.



\begin{table}[!htp]
\begin{center}
\scalebox{0.62}{
\begin{tabular}{lccrr} 
\toprule
\textbf{Test Set}& \textbf{Model Setting} & \textbf{Tune Set} & \textbf{SARI} & \textbf{BLEU} \\
\midrule
\multirow{4}{*}{\rot{90}{ASSET}} & GPT-$175$B, SARI, high to low, k=6 & TurkCorpus & 43.46 & 79.83 \\
& GPT-$175$B, SARI, high to low, k=6 & ASSET & 46.93 & 75.67 \\
& GPT-$13$B, CR, high to low, k=15 & TurkCorpus & 41.73 & 74.57 \\
& GPT-$13$B, CR, high to low, k=15 & ASSET & 41.9 & 76.49 \\
& \textit{State-of-the-art (MBL-Best)} &  & \textit{47.94} & \textit{73.92} \\
& \textit{Zero-shot (GPT-$175$B)} &  & \textit{38.49} & \textit{60.48} \\
\midrule
\multirow{4}{*}{\rot{90}{TurkCorpus}} & GPT-$175$B, SARI, random, k=15 & ASSET & 42.37 & 64.52 \\
& GPT-$175$B, SARI, random, k=15 & TurkCorpus & 41.48 & 85.89 \\
& GPT-$13$B, CR, high to low, k=15 & ASSET & 39.44 & 71.15 \\
& GPT-$13$B, CR, high to low, k=15 & TurkCorpus & 40.37 & 73.83 \\
& \textit{State-of-the-art (MBL-Best)} &  & \textit{43.46} & \textit{79.83} \\
& \textit{Zero-Shot (GPT-$175$B)} &  & \textit{32.17} & \textit{42.34} \\
\midrule
\multirow{5}{*}{\rot{90}{FestAbility}} & GPT-$175$B, SARI, random, k=6 & TurkCorpus & 42.24 & 20.76 \\
& GPT-$175$B, SARI, random, k=15 & ASSET & 44.86 & 17.08 \\
& GPT-$13$B, CR, high to low, k=15 & TurkCorpus & 25.46 & 23.37 \\
& GPT-$13$B, CR, high to low, k=15 & ASSET & 36.63 & 12.01 \\
& \textit{State-of-the-art (MBL-Best)} &  & \textit{44.86} & N/A \\
& \textit{Zero-shot (GPT-$175$B)} &  & \textit{40.77} & \textit{6.9} \\
\bottomrule
\end{tabular}
\caption{ICL out-of-domain results for GPT-$175$B, SARI-based selection and GPT-$13$B, CR-based selection. Examples are picked from the \textit{Tune Set} and tested on the \textit{Test Set}. Zero-shot results are from GPT-$175B$ and given in the final rows for each dataset.} 
\label{tab:ICL}
}
\end{center}
\end{table}
