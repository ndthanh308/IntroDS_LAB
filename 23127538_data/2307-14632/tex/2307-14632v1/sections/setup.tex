To investigate the effects of metric-based selection techniques on TS, we perform a comprehensive set of experiments using various LLMs, sample sizes, and datasets; and compare against strong baseline and state-of-the-art models. Following the criticism~\cite{sulem-etal-2018-bleu} on using BLEU~\citep{papineni-etal-2002-bleu}, we use SARI~\citep{xu-etal-2016-optimizing} as our main evaluation metric. However, we also report BLEU for two reasons: i) to be consistent with previous works~(see~\S \ref{ssec:sota}) and ii) to gain more insights on how the chosen metric for MBL effects the results measured with different metrics. 

\subsection{Models}
Due to its recent success in text generation and in-context learning for various downstream tasks, we experiment with the GPT-3~\citep{gpt3} model. We use three different version with the following parameter sizes: 175B, a.k.a., \texttt{da-vinci-003}, 13B, a.k.a., \texttt{curie}, and 6.7B, a.k.a., \texttt{babbage}. %GPT-3 is trained on the Common Crawl web scrape with additional filtering. 
We used OpenAI API\footnote{https://openai.com/} to generate responses using temperature=$0.7$, max\_tokens=$256$ top\_p=$1$, frequency penalty=$0$ and presence penalty = $0$.

\subsection{Datasets}
    We perform our main experiments on the ASSET~\cite{alva-manchego-etal-2020-asset} and TurkCorpus~\cite{xu-etal-2016-optimizing} datasets. To investigate the transferability of our models, we conduct additional experiments on an out-of-domain cognitive simplification dataset, FestAbility~\cite{chamovitz2022cognitive}.
    
    \paragraph{TurkCorpus} is a widely-used dataset with 2000 validation and 359 test sentences. It has 8 reference sentences for each original sentence in both the validation and test set. 
     
    \paragraph{ASSET} is another widely used TS dataset with the intention of improving upon TurkCorpus. It has the same 2000 validation and 359 original test sentences but introduces 10 new reference sentences for each original sentence. ASSET is deemed to be simpler by human evaluation in both fluency and simplicity ~\citep{alva-manchego-etal-2020-asset}. ASSET improves upon TurkCorpus as it allowed human reviewers to focus on a wider variety of TS operations, which are: lexical paraphrasing, compression, and sentence splitting. Because of this, we emphasize the experiments done on ASSET rather than TurkCorpus while interpreting the results and answering the research questions in \S \ref{sec:setup}. 
    
    \paragraph{FestAbility} is a cognitive simplification dataset with 321 pairs of complex and simple sentences---i.e., only one reference sentence. Each of these is additionally annotated with rewriting operations such as \texttt{<ADDITION>} and \texttt{<DELETION>}. These sentences are generated from the transcript of the virtual accessibility conference, and simplifications are generated from the Yalon Method~\citep{chamovitz2022cognitive}, a specialized method for simplifying text for individuals with cognitive impairments.

\subsection{Baselines}

    For comparison, we implement three baselines: i) random selection ii) KATE-GPT~\citep{kategpt} and iii) zero-shot. In the random setting, we randomly select $c$ and $r_i$ pairs from the validation sets. For KATE-GPT~\citep{kategpt}, we use the default setting that employs RoBERTa-base for contextualized embeddings and cosine similarity for the distance metric. Given that KATE-GPT calculates sentence pair similarities between the development and test set, unlike just the development set (like ours), we choose complex sentences as the representative. Zero-shot setting is simply conducted with the same instruction prompt without providing any examples.

    
\subsection{Text Simplification State-of-the-art}
\label{ssec:sota}
    We compare our results across multiple state-of-the-art systems.
    
    \paragraph{MUSS (BART+ACCESS Supervised)} \citet{martin-etal-2022-muss} fine-tune BART~\citep{bart} and add information from the four simplification tokens trained in ACCESS. 
    
    \paragraph{Finetuned-T5} \citet{sheang-saggion-2021-controllable} finetune T5 by adding multiple control tokens (e.g., compression ratio, Levenshtein similarity ratio, word rank, and number of words) similar to ACCESS, which control the model's outputs. To the best of our knowledge, they achieve the current state-of-the-art on both the TurkCorpus and ASSET datasets, with SARI scores of 43.31 and 45.04 respectively.%We used these results as our strongest baseline. 

\subsection{Evaluation}
Even though SARI is considered the standard evaluation metric in our experiments below, we evaluate the results both with SARI and BLEU to emphasize the behavior differences in metric-based selection. It should be noted that, SARI compares prediction against input and reference(s), while BLEU compares only the prediction against reference(s). We use the package EASSE~\citep{alva-manchego-etal-2019-easse} with the default settings~\footnote{We used the BLEU with $n=5$ against all references provided. The implementation is available at \url{https://github.com/feralvam/easse}.} to generate reports for all of our experiments.