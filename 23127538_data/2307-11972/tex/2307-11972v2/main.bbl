\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahuja et~al.(2020)Ahuja, Shanmugam, Varshney, and Dhurandhar]{ahuja2020}
Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar.
\newblock Invariant {{Risk Minimization Games}}.
\newblock In \emph{Proceedings of the 37th {{International Conference}} on {{Machine Learning}}}, pp.\  145--155, 2020.

\bibitem[Ahuja et~al.(2021{\natexlab{a}})Ahuja, Caballero, Zhang, {Gagnon-Audet}, Bengio, Mitliagkas, and Rish]{ahuja2021}
Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe {Gagnon-Audet}, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish.
\newblock Invariance {{Principle Meets Information Bottleneck}} for {{Out-of-Distribution Generalization}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, 2021{\natexlab{a}}.

\bibitem[Ahuja et~al.(2021{\natexlab{b}})Ahuja, Shanmugam, and Dhurandhar]{ahuja2021a}
Kartik Ahuja, Karthikeyan Shanmugam, and Amit Dhurandhar.
\newblock Linear {{Regression Games}}: {{Convergence Guarantees}} to {{Approximate Out-of-Distribution Solutions}}.
\newblock In \emph{Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, pp.\  1270--1278, 2021{\natexlab{b}}.

\bibitem[AlBadawy et~al.(2018)AlBadawy, Saha, and Mazurowski]{albadawy2018}
Ehab~A. AlBadawy, Ashirbani Saha, and Maciej~A. Mazurowski.
\newblock Deep learning for segmentation of brain tumors: {{Impact}} of cross-institutional training and testing.
\newblock \emph{Medical Physics}, 45\penalty0 (3):\penalty0 1150--1158, 2018.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and {Lopez-Paz}]{arjovsky2020}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David {Lopez-Paz}.
\newblock Invariant {{Risk Minimization}}.
\newblock \emph{arXiv:1907.02893}, 2019.

\bibitem[Barron(1993)]{barron1993}
Andrew~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal function.
\newblock \emph{IEEE Transactions on Information Theory}, 39\penalty0 (3):\penalty0 930--945, 1993.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in {{Terra Incognita}}.
\newblock In \emph{Computer {{Vision}} \textendash{} {{ECCV}} 2018}, volume 11220, pp.\  472--489. 2018.

\bibitem[Chang et~al.(2020)Chang, Zhang, Yu, and Jaakkola]{chang2020}
Shiyu Chang, Yang Zhang, Mo~Yu, and Tommi Jaakkola.
\newblock Invariant {{Rationalization}}.
\newblock In \emph{Proceedings of the 37th {{International Conference}} on {{Machine Learning}}}, pp.\  1448--1458, 2020.

\bibitem[Chen et~al.(2023)Chen, Zhou, Bian, Xie, Wu, Zhang, Kaili, Yang, Zhao, Han, and Cheng]{chen2023}
Yongqiang Chen, Kaiwen Zhou, Yatao Bian, Binghui Xie, Bingzhe Wu, Yonggang Zhang, Ma~Kaili, Han Yang, Peilin Zhao, Bo~Han, and James Cheng.
\newblock Pareto {{Invariant Risk Minimization}}: {{Towards Mitigating}} the {{Optimization Dilemma}} in {{Out-of-Distribution Generalization}}.
\newblock In \emph{The {{Eleventh International Conference}} on {{Learning Representations}}}, 2023.

\bibitem[Christmann \& Steinwart(2008)Christmann and Steinwart]{andreas2008}
Andreas Christmann and Ingo Steinwart.
\newblock \emph{Support {{Vector Machines}}}.
\newblock Information {{Science}} and {{Statistics}}. Springer, 2008.

\bibitem[Cybenko(1989)]{cybenko1989}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals and Systems}, 2\penalty0 (4):\penalty0 303--314, 1989.

\bibitem[Heaven(2020)]{zotero-107}
Will~Douglas Heaven.
\newblock Google's medical {{AI}} was super accurate in a lab. {{Real}} life was a different story.
\newblock https://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-clinic-covid-diabetes-retina-disease/, 2020.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{hornik1989}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Huh \& Baidya(2022)Huh and Baidya]{huh2022}
Dongsung Huh and Avinash Baidya.
\newblock The {{Missing Invariance Principle}} found -- the {{Reciprocal Twin}} of {{Invariant Risk Minimization}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, 2022.

\bibitem[Kamath et~al.(2021)Kamath, Tangella, Sutherland, and Srebro]{kamath2021}
Pritish Kamath, Akilesh Tangella, Danica Sutherland, and Nathan Srebro.
\newblock Does {{Invariant Risk Minimization Capture Invariance}}?
\newblock In \emph{Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, pp.\  4069--4077, 2021.

\bibitem[Koyama \& Yamaguchi(2021)Koyama and Yamaguchi]{koyama2021}
Masanori Koyama and Shoichiro Yamaguchi.
\newblock When is invariance useful in an {{Out-of-Distribution Generalization}} problem ?
\newblock \emph{arXiv:2008.01883}, 2021.

\bibitem[Krueger et~al.(2021)Krueger, Caballero, Jacobsen, Zhang, Binas, Zhang, Priol, and Courville]{krueger2021}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi~Le Priol, and Aaron Courville.
\newblock Out-of-{{Distribution Generalization}} via {{Risk Extrapolation}} ({{REx}}).
\newblock In \emph{Proceedings of the 38th {{International Conference}} on {{Machine Learning}}}, pp.\  5815--5826, 2021.

\bibitem[Lin et~al.(2022{\natexlab{a}})Lin, Dong, Wang, and Zhang]{lin2022}
Yong Lin, Hanze Dong, Hao Wang, and Tong Zhang.
\newblock Bayesian {{Invariant Risk Minimization}}.
\newblock In \emph{{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}, pp.\  16000--16009, 2022{\natexlab{a}}.

\bibitem[Lin et~al.(2022{\natexlab{b}})Lin, Zhu, Tan, and Cui]{lin2022b}
Yong Lin, Shengyu Zhu, Lu~Tan, and Peng Cui.
\newblock Zin: When and how to learn invariance without environment partition?
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Hu, Cui, Li, and Shen]{liu2021}
Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo~Li, and Zheyan Shen.
\newblock Heterogeneous {{Risk Minimization}}.
\newblock In \emph{Proceedings of the 38th {{International Conference}} on {{Machine Learning}}}, pp.\  6804--6814, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Hu, Cui, Li, and Shen]{liu2021a}
Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo~Li, and Zheyan Shen.
\newblock Kernelized {{Heterogeneous Risk Minimization}}, 2021{\natexlab{b}}.

\bibitem[Lu et~al.(2022)Lu, Wu, {Hern{\'a}ndez-Lobato}, and Sch{\"o}lkopf]{lu2022}
Chaochao Lu, Yuhuai Wu, Jos{\'e}~Miguel {Hern{\'a}ndez-Lobato}, and Bernhard Sch{\"o}lkopf.
\newblock Invariant {{Causal Representation Learning}} for {{Out-of-Distribution Generalization}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, 2022.

\bibitem[Mhaskar(1996)]{mhaskar1996}
Hrushikesh Mhaskar.
\newblock Neural {{Networks}} for {{Optimal Approximation}} of {{Smooth}} and {{Analytic Functions}}.
\newblock \emph{Neural Computation}, 8\penalty0 (1):\penalty0 164--177, 1996.

\bibitem[Parascandolo et~al.(2022)Parascandolo, Neitz, Orvieto, Gresele, and Sch{\"o}lkopf]{parascandolo2022}
Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard Sch{\"o}lkopf.
\newblock Learning explanations that are hard to vary.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, 2022.

\bibitem[Perone et~al.(2019)Perone, Ballester, Barros, and {Cohen-Adad}]{perone2019}
Christian~S. Perone, Pedro Ballester, Rodrigo~C. Barros, and Julien {Cohen-Adad}.
\newblock Unsupervised domain adaptation for medical imaging segmentation with self-ensembling.
\newblock \emph{NeuroImage}, 194:\penalty0 1--11, 2019.

\bibitem[Peters et~al.(2016)Peters, B{\"u}hlmann, and Meinshausen]{peters2016}
Jonas Peters, Peter B{\"u}hlmann, and Nicolai Meinshausen.
\newblock Causal inference by using invariant prediction: Identification and confidence intervals.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}, 78\penalty0 (5):\penalty0 947--1012, 2016.

\bibitem[Pogodin et~al.(2023)Pogodin, Deka, Li, Sutherland, Veitch, and Gretton]{pogodin2023}
Roman Pogodin, Namrata Deka, Yazhe Li, Danica~J. Sutherland, Victor Veitch, and Arthur Gretton.
\newblock Efficient {{Conditionally Invariant Representation Learning}}.
\newblock In \emph{The {{Eleventh International Conference}} on {{Learning Representations}}}, 2023.

\bibitem[Rame et~al.(2022)Rame, Dancette, and Cord]{rame2022}
Alexandre Rame, Corentin Dancette, and Matthieu Cord.
\newblock Fishr: {{Invariant Gradient Variances}} for {{Out-of-Distribution Generalization}}.
\newblock In \emph{Proceedings of the 39th {{International Conference}} on {{Machine Learning}}}, pp.\  18347--18377, 2022.

\bibitem[{Rojas-Carulla} et~al.(2018){Rojas-Carulla}, Sch{\"o}lkopf, Turner, and Peters]{rojas-carulla2018a}
Mateo {Rojas-Carulla}, Bernhard Sch{\"o}lkopf, Richard Turner, and Jonas Peters.
\newblock Invariant {{Models}} for {{Causal Transfer Learning}}.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0 (36):\penalty0 1--34, 2018.

\bibitem[Rosenfeld et~al.(2021)Rosenfeld, Ravikumar, and Risteski]{rosenfeld2021}
Elan Rosenfeld, Pradeep~Kumar Ravikumar, and Andrej Risteski.
\newblock The {{Risks}} of {{Invariant Risk Minimization}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, 2021.

\bibitem[Shane(2018)]{zotero-64}
Janelle Shane.
\newblock Do neural nets dream of electric sheep? - {{AI WeirdnessCommentShareCommentShare}}.
\newblock https://www.aiweirdness.com/do-neural-nets-dream-of-electric-18-03-02/, 2018.

\bibitem[Sonoda \& Murata(2017)Sonoda and Murata]{sonoda2017}
Sho Sonoda and Noboru Murata.
\newblock Neural network with unbounded activation functions is universal approximator.
\newblock \emph{Applied and Computational Harmonic Analysis}, 43\penalty0 (2):\penalty0 233--268, 2017.

\bibitem[Tan et~al.(2023)Tan, Yong, Zhu, Qu, Qiu, Yinghui, Cui, and Qi]{tam2023}
Xiaoyu Tan, LIN Yong, Shengyu Zhu, Chao Qu, Xihe Qiu, Xu~Yinghui, Peng Cui, and Yuan Qi.
\newblock Provably invariant learning without domain information.
\newblock In \emph{Proceedings of the 40th {{International Conference}} on {{Machine Learning}}}, 2023.

\bibitem[Toyota \& Fukumizu(2022)Toyota and Fukumizu]{toyota2022}
Shoji Toyota and Kenji Fukumizu.
\newblock Invariance {{Learning}} based on {{Label Hierarchy}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, 2022.

\bibitem[Zhou et~al.(2022)Zhou, Lin, Zhang, and Zhang]{zhou2022}
Xiao Zhou, Yong Lin, Weizhong Zhang, and Tong Zhang.
\newblock Sparse {{Invariant Risk Minimization}}.
\newblock In \emph{Proceedings of the 39th {{International Conference}} on {{Machine Learning}}}, pp.\  27222--27244, 2022.

\end{thebibliography}
