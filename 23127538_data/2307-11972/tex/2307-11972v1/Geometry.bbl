\begin{thebibliography}{10}

\bibitem{arjovsky2020}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David {Lopez-Paz}.
\newblock Invariant {{Risk Minimization}}.
\newblock {\em arXiv:1907.02893}, 2019.

\bibitem{beery2018}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in {{Terra Incognita}}.
\newblock In {\em Computer {{Vision}} \textendash{} {{ECCV}} 2018}, Vol. 11220,
  pp. 472--489. 2018.

\bibitem{zotero-64}
Janelle Shane.
\newblock Do neural nets dream of electric sheep? - {{AI
  WeirdnessCommentShareCommentShare}}.
\newblock
  https://www.aiweirdness.com/do-neural-nets-dream-of-electric-18-03-02/, 2018.

\bibitem{albadawy2018}
Ehab~A. AlBadawy, Ashirbani Saha, and Maciej~A. Mazurowski.
\newblock Deep learning for segmentation of brain tumors: {{Impact}} of
  cross-institutional training and testing.
\newblock {\em Medical Physics}, Vol.~45, No.~3, pp. 1150--1158, 2018.

\bibitem{perone2019}
Christian~S. Perone, Pedro Ballester, Rodrigo~C. Barros, and Julien
  {Cohen-Adad}.
\newblock Unsupervised domain adaptation for medical imaging segmentation with
  self-ensembling.
\newblock {\em NeuroImage}, Vol. 194, pp. 1--11, 2019.

\bibitem{zotero-107}
Will~Douglas Heaven.
\newblock Google's medical {{AI}} was super accurate in a lab. {{Real}} life
  was a different story.
\newblock
  https://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-clinic-covid-diabetes-retina-disease/,
  2020.

\bibitem{peters2016}
Jonas Peters, Peter B{\"u}hlmann, and Nicolai Meinshausen.
\newblock Causal inference by using invariant prediction: Identification and
  confidence intervals.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, Vol.~78, No.~5, pp. 947--1012, 2016.

\bibitem{koyama2021}
Masanori Koyama and Shoichiro Yamaguchi.
\newblock When is invariance useful in an {{Out-of-Distribution
  Generalization}} problem ?
\newblock {\em arXiv:2008.01883}, 2021.

\bibitem{rojas-carulla2018a}
Mateo {Rojas-Carulla}, Bernhard Sch{\"o}lkopf, Richard Turner, and Jonas
  Peters.
\newblock Invariant {{Models}} for {{Causal Transfer Learning}}.
\newblock {\em Journal of Machine Learning Research}, Vol.~19, No.~36, pp.
  1--34, 2018.

\bibitem{ahuja2020}
Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar.
\newblock Invariant {{Risk Minimization Games}}.
\newblock In {\em Proceedings of the 37th {{International Conference}} on
  {{Machine Learning}}}, pp. 145--155, 2020.

\bibitem{chang2020}
Shiyu Chang, Yang Zhang, Mo~Yu, and Tommi Jaakkola.
\newblock Invariant {{Rationalization}}.
\newblock In {\em Proceedings of the 37th {{International Conference}} on
  {{Machine Learning}}}, pp. 1448--1458, 2020.

\bibitem{ahuja2021}
Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe {Gagnon-Audet},
  Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish.
\newblock Invariance {{Principle Meets Information Bottleneck}} for
  {{Out-of-Distribution Generalization}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}, 2021.

\bibitem{ahuja2021a}
Kartik Ahuja, Karthikeyan Shanmugam, and Amit Dhurandhar.
\newblock Linear {{Regression Games}}: {{Convergence Guarantees}} to
  {{Approximate Out-of-Distribution Solutions}}.
\newblock In {\em Proceedings of {{The}} 24th {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, pp. 1270--1278, 2021.

\bibitem{lin2022}
Yong Lin, Hanze Dong, Hao Wang, and Tong Zhang.
\newblock Bayesian {{Invariant Risk Minimization}}.
\newblock In {\em {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and
  {{Pattern Recognition}}}, pp. 16000--16009, 2022.

\bibitem{zhou2022}
Xiao Zhou, Yong Lin, Weizhong Zhang, and Tong Zhang.
\newblock Sparse {{Invariant Risk Minimization}}.
\newblock In {\em Proceedings of the 39th {{International Conference}} on
  {{Machine Learning}}}, pp. 27222--27244, 2022.

\bibitem{liu2021}
Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo~Li, and Zheyan Shen.
\newblock Heterogeneous {{Risk Minimization}}.
\newblock In {\em Proceedings of the 38th {{International Conference}} on
  {{Machine Learning}}}, pp. 6804--6814, 2021.

\bibitem{liu2021a}
Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo~Li, and Zheyan Shen.
\newblock Kernelized {{Heterogeneous Risk Minimization}}, 2021.

\bibitem{lu2022}
Chaochao Lu, Yuhuai Wu, Jos{\'e}~Miguel {Hern{\'a}ndez-Lobato}, and Bernhard
  Sch{\"o}lkopf.
\newblock Invariant {{Causal Representation Learning}} for
  {{Out-of-Distribution Generalization}}.
\newblock In {\em International {{Conference}} on {{Learning
  Representations}}}, 2022.

\bibitem{parascandolo2022}
Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and
  Bernhard Sch{\"o}lkopf.
\newblock Learning explanations that are hard to vary.
\newblock In {\em International {{Conference}} on {{Learning
  Representations}}}, 2022.

\bibitem{krueger2021}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
  Binas, Dinghuai Zhang, Remi~Le Priol, and Aaron Courville.
\newblock Out-of-{{Distribution Generalization}} via {{Risk Extrapolation}}
  ({{REx}}).
\newblock In {\em Proceedings of the 38th {{International Conference}} on
  {{Machine Learning}}}, pp. 5815--5826, 2021.

\bibitem{toyota2022}
Shoji Toyota and Kenji Fukumizu.
\newblock Invariance {{Learning}} based on {{Label Hierarchy}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}, 2022.

\bibitem{lin2022b}
Yong Lin, Shengyu Zhu, Lu~Tan, and Peng Cui.
\newblock Zin: When and how to learn invariance without environment partition?
\newblock In {\em Advances in {{Neural Information Processing Systems}}}, 2022.

\bibitem{huh2022}
Dongsung Huh and Avinash Baidya.
\newblock The {{Missing Invariance Principle}} found -- the {{Reciprocal Twin}}
  of {{Invariant Risk Minimization}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}, 2022.

\bibitem{rame2022}
Alexandre Rame, Corentin Dancette, and Matthieu Cord.
\newblock Fishr: {{Invariant Gradient Variances}} for {{Out-of-Distribution
  Generalization}}.
\newblock In {\em Proceedings of the 39th {{International Conference}} on
  {{Machine Learning}}}, pp. 18347--18377, 2022.

\bibitem{pogodin2023}
Roman Pogodin, Namrata Deka, Yazhe Li, Danica~J. Sutherland, Victor Veitch, and
  Arthur Gretton.
\newblock Efficient {{Conditionally Invariant Representation Learning}}.
\newblock In {\em The {{Eleventh International Conference}} on {{Learning
  Representations}}}, 2023.

\bibitem{chen2023}
Yongqiang Chen, Kaiwen Zhou, Yatao Bian, Binghui Xie, Bingzhe Wu, Yonggang
  Zhang, Ma~Kaili, Han Yang, Peilin Zhao, Bo~Han, and James Cheng.
\newblock Pareto {{Invariant Risk Minimization}}: {{Towards Mitigating}} the
  {{Optimization Dilemma}} in {{Out-of-Distribution Generalization}}.
\newblock In {\em The {{Eleventh International Conference}} on {{Learning
  Representations}}}, 2023.

\bibitem{tam2023}
Xiaoyu Tan, LIN Yong, Shengyu Zhu, Chao Qu, Xihe Qiu, Xu~Yinghui, Peng Cui, and
  Yuan Qi.
\newblock Provably invariant learning without domain information.
\newblock In {\em Proceedings of the 40th {{International Conference}} on
  {{Machine Learning}}}, 2023.

\bibitem{rosenfeld2021}
Elan Rosenfeld, Pradeep~Kumar Ravikumar, and Andrej Risteski.
\newblock The {{Risks}} of {{Invariant Risk Minimization}}.
\newblock In {\em International {{Conference}} on {{Learning
  Representations}}}, 2021.

\bibitem{kamath2021}
Pritish Kamath, Akilesh Tangella, Danica Sutherland, and Nathan Srebro.
\newblock Does {{Invariant Risk Minimization Capture Invariance}}?
\newblock In {\em Proceedings of {{The}} 24th {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, pp. 4069--4077, 2021.

\bibitem{cybenko1989}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of Control, Signals and Systems}, Vol.~2, No.~4, pp.
  303--314, 1989.

\bibitem{hornik1989}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock {\em Neural Networks}, Vol.~2, No.~5, pp. 359--366, 1989.

\bibitem{barron1993}
Andrew~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information Theory}, Vol.~39, No.~3, pp.
  930--945, 1993.

\bibitem{mhaskar1996}
Hrushikesh Mhaskar.
\newblock Neural {{Networks}} for {{Optimal Approximation}} of {{Smooth}} and
  {{Analytic Functions}}.
\newblock {\em Neural Computation}, Vol.~8, No.~1, pp. 164--177, 1996.

\bibitem{sonoda2017}
Sho Sonoda and Noboru Murata.
\newblock Neural network with unbounded activation functions is universal
  approximator.
\newblock {\em Applied and Computational Harmonic Analysis}, Vol.~43, No.~2,
  pp. 233--268, 2017.

\bibitem{andreas2008}
Andreas Christmann and Ingo Steinwart.
\newblock {\em Support {{Vector Machines}}}.
\newblock Information {{Science}} and {{Statistics}}. Springer, 2008.

\end{thebibliography}
