% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[
reprint,
superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
amsmath,amssymb,
aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
floatfix,
]{revtex4-1}

%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{ifthen}
\newboolean{twocolumn}
\setboolean{twocolumn}{true}  
%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}% Include figure files
%\usepackage{dcolumn}% Align table columns on decimal point
%\usepackage{bm}% bold math
%\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{braket}
\usepackage{amsmath}
%\usepackage[normalem]{ulem}
%\usepackage{caption}
%\usepackage{subcaption}
%\usepackage{algorithmicx}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
%\usepackage{subfigure}
\usepackage{xfrac}
\usepackage{bm}

%\usepackage{subcaption}

\usepackage{algpseudocode}
\usepackage{algorithm}

\bibliographystyle{apsrev4-1} %?

\ifthenelse{\boolean{twocolumn}}
{\newcommand\assumptionmargin{0.5cm}
 \newcommand\assumptionlabel{A}}
{\newcommand\assumptionmargin{2.5cm}
 \newcommand\assumptionlabel{Assumption}
}

\newtheorem{conjecture}{Conjecture}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]

%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\begin{document}

%\preprint{APS/123-QED (Modify later)}

\title{Sampling Error Analysis in Quantum Krylov Subspace Diagonalization} % Force line breaks with \\
%\thanks{A footnote to the article title} % Do we need it?

\author{Gwonhak Lee}
 %\altaffiliation[Also at ]{Physics Department, XYZ University.}%Lines break automatically or can be forced with \\
\affiliation{SKKU Advanced Institute of Nanotechnology, Sungkyunkwan University, Suwon 16419, Republic of Korea}

\author{Dongkeun Lee}
\affiliation{Department of Chemistry, Sungkyunkwan University, Suwon 16419, Republic of Korea}

\author{Joonsuk Huh}
\email{joonsukhuh@gmail.com}
\affiliation{SKKU Advanced Institute of Nanotechnology, Sungkyunkwan University, Suwon 16419, Republic of Korea}
\affiliation{Department of Chemistry, Sungkyunkwan University, Suwon 16419, Republic of Korea}
\affiliation{Institute of Quantum Biophysics, Sungkyunkwan University, Suwon 16419, Republic of Korea}%


\date{July 29, 2023}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
    Quantum Krylov subspace diagonalization (QKSD) is an emerging method used in place of quantum phase estimation in the early fault-tolerant era, where limited quantum circuit depth is available. 
    In contrast to the classical Krylov subspace diagonalization (KSD) or the Lanczos method, QKSD exploits the quantum computer to efficiently estimate the eigenvalues of large-size Hamiltonians through a faster Krylov projection.
    However, unlike classical KSD, which is solely concerned with machine precision, QKSD is inherently accompanied by errors originating from a finite number of samples.
    Moreover, due to difficulty establishing an artificial orthogonal basis, ill-conditioning problems are often encountered, rendering the solution vulnerable to noise.
    In this work, we present a nonasymptotic theoretical framework to assess the relationship between sampling noise and its effects on eigenvalues.
    We also propose an optimal solution to cope with large condition numbers by eliminating the ill-conditioned bases.
    Numerical simulations of the one-dimensional Hubbard model demonstrate that the error bound of finite samplings accurately predicts the experimental errors in well-conditioned regions.
    %\begin{description}
    %\item[Usage]
    %Secondary publications and information retrieval purposes.
    %\item[Structure]
    %You may use the \texttt{description} environment to structure your abstract;
    %use the optional argument of the \verb+\item+ command to give the category of each item. 
    %\end{description}
\end{abstract}

%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle

%\tableofcontents

\section{Introduction}\label{sec:Introduction}

Quantum simulations of many-body systems have been a longstanding challenge in the fields of physics and chemistry.
Quantum computing is expected to address those problems intractable to classical approaches \cite{Feynman.1982, Lloyd.1996}.
Although various quantum algorithms, such as quantum phase estimation, are promising for estimating eigenvalues \cite{Abrams.1997, Abrams.1999}, they are not feasible because of the limitations of contemporary quantum computers in the noisy intermediate-scale quantum (NISQ) era \cite{nisq}.

Research has been conducted on the development of quantum-classical hybrid algorithms for NISQ devices \cite{Bharti.2022}.
One of the most widely used algorithms is the variational quantum algorithm (VQA), comprising the variational quantum eigensolver (VQE) and the quantum approximation optimization algorithm \cite{vqa, Peruzzo.2014, Farhi.2014}.
VQAs employ a parameterized quantum circuit and cost function optimizations on classical devices.
However, adopting a classical optimization strategy, such as gradient descent, has been shown to have drawbacks, such as barren plateaus \cite{McClean.2018,Cerezo.2021}, nonconvexity, and NP-hardness \cite{Bittel.2021}.

Quantum Krylov subspace diagonalization (QKSD) has recently received considerable attention for circumventing the drawbacks of VQAs.
It is another type of quantum-classical hybrid algorithm, positioned between the NISQ device and the fault-tolerant quantum computer, that leverages the quantum circuit to project a large-scale Hamiltonian onto a lower-dimensional space, known as the Krylov subspace. Due to its reduced dimension, a classical computer then solves the generalized eigenvalue problem (GEVP) within this subspace.
%The Krylov subspace, which is spanned by a nonorthogonal basis generated by a reference state and a basis operator, is a well-known approach for representing the Hamiltonian in subspaces. 
%Especially, the quantum circuits are used when quantum states and operators in quantum circuits are used to generate the Krylov basis, a method that diagonalizes the Hamiltonian in the Krylov subspace is called the quantum Krylov subspace diagonalization (QKSD).
Existing studies on the QKSD method has utilized various types of the Krylov basis that relies on the base operator and the reference state. 
%generated by the base operator repeatedly applied to the reference state with the help of quantum circuits. 
These include the quantum filter method (QFM) employing real-time evolution as base operator, \cite{quantum_filter_method}, the quantum power method (QPM), whose basis is generated by using the Hamiltonian operator \cite{quantum_power_method}, and the QLanczos method utilizing imaginary-time evolution \cite{quantum_imaginary_time_evolution}.
Several other approaches, including the use of multiple reference states \cite{doi:10.1021/acs.jctc.9b01125} and multifidelity estimation protocol \cite{PhysRevA.105.022417} have been proposed to improve the QKSD method \cite{PRXQuantum.3.020323, Cohn.2021,Cortes.2022b,Shen.2022,Stair.2022}.
%setting the optimal time evolution grid conditions for variational quantum phase estimations \cite{PRXQuantum.3.020323},

All these methodologies necessarily involve extracting matrix elements of the Krylov subspace for the Hamiltonian through measurements. 
Due to the probabilistic nature of quantum regimes, repeating quantum measurements on multiple copies of the quantum circuit is inevitable for attaining accurate outcomes and probabilities. 
The ever-present error arising from the finite number of measurements cannot be overlooked in the QKSD method, as the GEVP is vulnerable to errors \cite{Theory_QSDK,Mathias2004TheDG,10.2307/2156670}.
%, unlike the classical KSD where matrices are deterministically constructed with machine precision.
%{\color{red}One sentence SVD Threshold and condition number.}
%overlap matrix regularization 해주는것이 중요하며, 보통 empirically do regularization \refs{}.
%Seriously, no discussion about measurement
The vulnerability of the GEVP has been highlighted in a previous work that conducts an error analysis of quantum subspace diagonalization \cite{Theory_QSDK}.
In this study, we theoretically analyze the finite sampling error and the error of the QKSD method. 
In the QKSD method, we use the Hadamard test and real-time evolution to measure the entries of the projected Hamiltonian and the overlap matrix.
The finite sampling error from this Hadamard test is modeled as Gaussian noise, which causes perturbations in the GEVP.
Based on this error analysis, we employ a threshold, also called a "truncation point,"
to bypass the ill-conditioning of the overlap matrix in the QKSD method.
Finally, we implement the QKSD algorithm to solve the one-dimensional Fermi-Hubbard model by using a numerical approach under the assumption that the quantum circuit used for simulation does not includes any other errors.
We then estimate its eigenenergy perturbation with respect to the number of shots to verify the analysis.

This paper is organized as follows. First, a preliminary version of the QKSD for a Hamiltonian is explained in Section \ref{sec:QKSD} and then a brief overview of QKSD-based error analyses is provided in Section \ref{sec:GEVP}.
The main results are presented in Section \ref{sec:main_result}, which includes the elaborate model of sampling errors from the Hadamard test, the effect of errors on the final results of the GEVP, and optimal thresholding criteria for alleviating the perturbation effect.
Finally, our result is numerically verified with the 1D Fermi-Hubbard model in Section \ref{sec:result}.

%The crucial things that most works have been overlooked? are the finite number of shots and measurement errors.



\section{QKSD}\label{sec:QKSD}
%\ifthenelse{\boolean{twocolumn}}{\onecolumngrid}{}
 % Figure environment removed
 %\ifthenelse{\boolean{twocolumn}}{\twocolumngrid}{}
 The Krylov subspace method is an algorithm for evaluating the approximated solutions to high-dimensional matrix problems by projecting large original matrices onto a small space.
 In particular, Krylov subspace diagonalization (KSD) solves the eigenvalue problem of the projected matrix to which the original matrix is converted.
 If one adopts KSD to solve a quantum many-body problem, the problem matrix corresponds to the many-body Hamiltonian, $\hat{H}$. Furthermore, the order-$n$ Krylov subspace, $\mathcal{K}_n$, is expressed as follows:
 \begin{equation}
 \label{eq:krylov_subspace}
     \mathcal{K}_n\left(\hat{A}, \ket{\phi_0}\right) := \textrm{span}\left(\{\ket{\phi_0}, \ket{\phi_1} \cdots \ket{\phi_{n-1}}\}\right),
 \end{equation}
 where the nonorthogonal Krylov basis, $\ket{\phi_k} := \hat{A}^{k}\ket{\phi_0}$ is generated by repeatedly applying the base operator, $\hat{A}$, up to $n-1$ times to the reference state vector, $\ket{\phi_0}$.
 Generally, the base operator $\hat{A}$ is a function of $\hat{H}$ and is often chosen to be the original Hamiltonian, such as $\hat{A}=\hat{H}$, in classical KSD.
 Furthermore, the order $n$ represents a relatively small number compared to the size of a problem ($n\ll\dim \hat{H}$), allowing the classical GEVP solver to address matrices of size $n\times n$.
 %The base operator, $\hat{A}=\hat{A}(\hat{H})$, is, in general, a function of $\hat{H}$ usually chosen as the original Hamiltonian, $\hat{A}=\hat{H}$, in classical KSD, and order $n$ represents a relatively small number compared with the size of a problem ($n\ll\dim \hat{H}$) that enables the classical eigenvalue solver to address matrices of size $n\times n$.
 %Even more, the set of powered operators $\{\hat{A}^k=e^{-i\Delta_tk\hat{H}}\}$ can be further generalized to the set with different pattern such as $\{\hat{A}_k=e^{-it_k\hat{H}}\}$. However, we don't include such cases remaining as a future study. Also, we don't include the block diagonalization method which uses multiple reference states. 
 %In Krylov subspaces, an $j^{\text{th}}$ eigenstate of $\hat{H}$, $\ket{\psi_j}$, is approximately expressed as a linear combination of Krylov basis states:
 In Krylov subspace, an ansatz state approximating an eigenstate of $\hat{H}$, $\ket{\psi(\bm{c})}$, is expressed as a linear combination of Krylov basis states:
 \begin{equation}
 \label{eq:original_state}  
     \ket{\psi(\bm{c})} \approx \frac{1}{\mathcal{N}_{\bm{c}}}\sum_{k=0}^{n-1} {c}_{k} \ket{\phi_k}, % = p_{\bm{c_j}}(\hat{A})\ket{\phi_0},
 \end{equation}
 where $\mathcal{N}_{\bm{c}}=(\sum_{kl}{c}_{k}^*{c}_{l}\langle\phi_k|\phi_l\rangle)^{1/2}$ is a normalization factor, and $\bm{c}=(c_1, \cdots, c_{n-1})$ represents the unnormalized expansion coefficients.
 %$c_k$ is the $k^{\text{th}}$ unnormalized expansion coefficient. 
 When Eq. \eqref{eq:original_state} is considered as a variational state parameterized by a vector $\bm{c}_j$, from the variational principle, one can subsequently derive a GEVP along with a matrix pair $(\bm{H}, \bm{S})$:
 \begin{equation}
 \label{eq:gen_eigeq}
     \bm{H}\bm{c}_j = \bm{S}\bm{c}_j E^{(n)}_j,
 \end{equation}
 where the projected Hamiltonian matrix, $\bm{H}$, and the overlap matrix, $\bm{S}$, are respectively given as
 \begin{align}
     \bm{H}_{kl} &:= \bra{\phi_k}\hat{H}\ket{\phi_l}, \label{eq:prj_H} \\ 
     \bm{S}_{kl} &:= \braket{\phi_k|\phi_l} \label{eq:overlap}.
 \end{align}
 Here, $E^{(n)}_j$ is the $j^{\text{th}}$ approximated eigenvalue of Krylov subspace order $n$ ($E^{(n)}_0 \le E^{(n)}_1 \le \cdots \le E^{(n)}_{n-1}$), and $\bm{c}_j$ becomes the corresponding $n$-dimensional eigenvector. Subscript $j$ is dropped for the lowest energy and the corresponding eigenvector for convenience.

 Compared with classical KSD, QKSD is leveraged by quantum simulation algorithms to build a Krylov basis in quantum computers with a reduced level of the computational effort.
 %QKSD approaches encompass various KSD versions applied to a general Hamiltonian and leveraged by quantum simulation algorithms.
 Specifically, the quantum algorithms are expected to efficiently yield Eqs. \eqref{eq:prj_H} and \eqref{eq:overlap} within the small error bound, despite the exponentially increasing dimension of the Hilbert space.
 Various QKSD algorithms have been proposed for each quantum simulation algorithm including QFM \cite{quantum_filter_method}, QPM \cite{quantum_power_method}, and quantum imaginary-time evolution (QITE) \cite{quantum_imaginary_time_evolution}.
 They are differentiated by the choice of base operator $\hat{A}$, which determines the Krylov subspace.
 In the QPM \cite{quantum_power_method}, base operator $\hat{A}$ is chosen as $\hat{H}$, and its basis, $\hat{H}^k\ket{\phi_0}$, is approximately calculated using the finite difference method (FDM).
 However, it exponentially amplifies the sampling errors with the inverse of the FDM time step and requires an exponential number of quantum computer calls.
 In QITE \cite{quantum_imaginary_time_evolution}, the Krylov subspace is constructed by using an imaginary time evolution basis ($e^{-k\Delta_t}\ket{\phi_0}$) based on the sequential unitary approximations to a normalized imaginary time evolution operator.
 However, such approximations involve iterative local tomography, which requires exponential computational effort to expand the range of tomography and compensate for approximation errors.
 Therefore, in this work, we focused on the most widely discussed method \cite{quantum_filter_method, PRXQuantum.3.020323, Cohn.2021}, real-time quantum Krylov diagonalization (QFM), because it is a compact method with a base operator served by a unitary operator that can be implemented in a substantially simper manner in a quantum circuit than other algorithms invoking nonunitary processes.
 
 We considered the order-$n$ quantum Krylov method that uses a real-time evolution basis with time step $\Delta_t$, $\{\ket{\phi_k}:=e^{-i k \Delta_t \hat{H}}\ket{\phi_0}\}_{k=0}^{n-1}$, whose procedure is described in Fig. \ref{fig:method_flowchart}.
 As mentioned elsewhere \cite{quantum_filter_method}, it only requires $n$ elements rather than $n^2$ to define $(\bm{H}, \bm{S})$ if one can perform exact Hamiltonian simulations.
 Using the commutation relation, one can rewrite $\bm{H}_{kl}$ and $\bm{S}_{kl}$ in terms of the index difference, $k-l$, expressed as:
\ifthenelse{\boolean{twocolumn}}{
\begin{equation}
\begin{split}
     \bm{H}_{kl} :=& \bra{\phi_0}e^{ik\hat{H}\Delta_t} \hat{H} e^{-il\hat{H}\Delta_t}\ket{\phi_0}
     \\=& \bra{\phi_0}\hat{H} e^{-i(l-k)\hat{H}\Delta_t}\ket{\phi_0}, \label{eq:prj_H_QKD}     
\end{split}    
\end{equation}
\begin{equation}
\begin{split}
     \bm{S}_{kl} :=& \bra{\phi_0}e^{ik\hat{H}\Delta_t}e^{-il\hat{H}\Delta_t}\ket{\phi_0}
 \\=& \bra{\phi_0}e^{-i(l-k)\hat{H}\Delta_t}\ket{\phi_0}. \label{eq:overlap_QKD}    
\end{split}    
\end{equation}
}{
\begin{align}
 \bm{H}_{kl} :=& \bra{\phi_0}e^{ik\hat{H}\Delta_t} \hat{H} e^{-il\hat{H}\Delta_t}\ket{\phi_0} = \bra{\phi_0}\hat{H} e^{-i(l-k)\hat{H}\Delta_t}\ket{\phi_0} \label{eq:prj_H_QKD} \\
 \bm{S}_{kl} :=& \bra{\phi_0}e^{ik\hat{H}\Delta_t}e^{-il\hat{H}\Delta_t}\ket{\phi_0} = \bra{\phi_0}e^{-i(l-k)\hat{H}\Delta_t}\ket{\phi_0} \label{eq:overlap_QKD}
\end{align}
}
Subsequently, constructing the Toeplitz matrices, $\bm{H}$, becomes simpler; the elements $\bm{H}_{kl} = h_{l-k}$ are obtained from sequence $\{h_k\}_{k=-(n-1)}^{n-1}$ defined as 
\begin{equation}
  h_k := \bra{\phi_0}\hat{H} e^{-ik\hat{H}\Delta_t}\ket{\phi_0}  = \sum_{j=1}^{N_\beta}{\beta_j \bra{\phi_0}\hat{U}_{j} e^{-ik\hat{H}\Delta_t}\ket{\phi_0}}, \label{eq:QKD_H_seq}
\end{equation}
 where each term can be obtained from the Hadamard test.
 The summation in Eq. \eqref{eq:QKD_H_seq} indicates the separated measurements of $\hat{H}$ by applying unitary partitioning \cite{unitary_partitioning} with a set of unitaries, $\{\hat{U}_j\}$:
 \begin{equation}
    \label{eq:pauli_ham_grp}
    \hat{H} = \sum_{j=1}^{N_\beta} \beta_j \hat{U}_j,
 \end{equation}
 where $\beta_j$ are coefficients determined by the unitary partitioning.
 Further details are provided in Appendix \ref{sec:appendix_hamiltonian_partitioning}. Unitary partitioning offers an advantage over the individual measurements of Pauli operators in terms of sampling noise.
 As an alternative to Eq. \eqref{eq:pauli_ham_grp}, one can adopt fragments of the Hamiltonian, $\hat{H}=\sum_j \hat{H}_j$, where each $\hat{H}_j$ can be diagonalized by an efficiently constructed unitary.
 This approach involves using the extended swap test \cite{quantum_filter_method}.
 %Alternative to the Eq. \eqref{eq:pauli_ham_grp}, one can adopt fragments of Hamiltonian, $\hat{H}=\sum_j \hat{H}_j$, where $\hat{H}_j$ can be diagonalized by a unitary that is efficiently constructed, using extended swap test \cite{quantum_filter_method}.
 Similarly, matrix $\bm{S}$ is generated by using sequence $\bm{S}_{kl}=s_{l-k}$, where
 \begin{equation}
     s_k := \bra{\phi_0} e^{-ik\hat{H}\Delta_t}\ket{\phi_0} \label{eq:QKD_S_seq}.
 \end{equation} 
 Accordingly, we must obtain sequences $\{h_k\}_{k=0}^{n-1}$ and $\{s_k\}_{k=0}^{n-1}$ to fill the matrices using the help of time reversal properties, $h_{-k} = h_{k}^{*}$ and $s_{-k} = s_{k}^{*}$. 

 However, the Toeplitz construction is impractical when an approximated simulation, such as Suzuki-Trotter decomposition, possesses a large amount of error, which is inevitable when dealing with Hamiltonians containing noncommuting terms with a shallow quantum circuit.
 Here, let us call $\hat{U}_{\mathrm{ST}}(t)=e^{-it\hat{H}}+\hat{\mathcal{E}}_{\mathrm{ST}}(t)$ a decomposition of $e^{-it\hat{H}}$, where $\hat{\mathcal{E}}_{\mathrm{ST}}(t)$ denotes the simulation error.
 For example, adapting the popular fractal Suzuki-Trotter decomposition \cite{fractal_decomposition, Suzuki_book}, in the order of $r$, the following error becomes $\hat{\mathcal{E}}_{\mathrm{ST}}(k\Delta_t)=O(\Delta_t^{2r})$ at a circuit depth of $O(k 2^r N_\Gamma)$, where $N_\Gamma$ is the number of commuting terms in the Hamiltonian, and $k$ is the number of repeated time steps of $\Delta_t$. % if we set the time step in the trotterization same as that of unit evolution operator for the Krylov ansatz.
 When such deep quantum circuits are not accessible, $\hat{\mathcal{E}}_{\mathrm{ST}}$ becomes nonnegligible; thus, an alternative basis, $\{\ket{\phi_k'}:=\hat{U}_{\mathrm{ST}}(\Delta_t)^k \ket{\phi_0}\}$, should be chosen instead of the exact time evolution basis, $\{\ket{\phi_k}:=e^{-ik\Delta_t \hat{H}}\ket{\phi_0}\}$, for the analysis.
 In such settings, the Toeplitz nature of $\bm{H}$ (see Eq. \eqref{eq:prj_H_QKD}) vanishes because $[\hat{H},\hat{U}_{\mathrm{ST}}(t)]\neq0$, requiring that $n^2$ elements be obtained instead of $n$.
 On the other side, the overlap matrix $\bm{S}$ maintains its Toeplitz property because $\hat{U}_{\mathrm{ST}}(\Delta_t)$ is still unitary, requiring only $n$ elements.  % We remark that the non-Toeplitz basis are free from the Suzuki-Trotterization error by absorbing the error to the basis. So, depending to the availability to the long circuit, one can choose Toeplitz or non-Toeplitz protocol.

\section{General Error Analysis}\label{sec:GEVP}

 The total error of the QKSD algorithm in practice can be attributed to two factors: one is the perturbation of the pair $(\bm{H}, \bm{S})$ owing to the noise from the practical implementation.
 As a representative example of a noise source, sampling noise becomes dominant when the elements are obtained using a limited number of samplings; therefore, the finite sampling issue was mainly addressed in this study.
 Conversely, the other factor corresponds to the difference between the approximation solution obtained from a noiseless KSD and the exact solution.
 Because the Krylov projection approximates smaller spaces, a projection error can be induced, particularly when the Krylov order $n$ is small.
 In addition, the projection error can increase when the overlap between the reference vector $\ket{\phi_0}$ and the true eigenvector is small.
 To estimate the lowest eigenvalue of the Hamiltonian $\hat{H}$, the practical execution of QKSD can produce a noisy outcome $\tilde{E}^{(n)}$, causing the total error to be described as a triangular inequality:
 \begin{equation}
 \label{eq:krylov_total_error}
   \left| \tilde{E}^{(n)} - E \right| \le \left| \tilde{E}^{(n)} - E^{(n)} \right| + \left| E^{(n)} - E \right|. %=: \Delta E_{Pert} + \Delta E_{Krylov}.
 \end{equation}
 Here, $E$ is the lowest exact eigenvalue of the Hamiltonian (i.e., the lowest solution of Schr\"{o}dinger's equation), $E^{(n)}$ is the lowest noiseless Krylov approximated eigenvalue of order $n$, and the first and second terms on the right-hand side of Eq. \eqref{eq:krylov_total_error} are two error factors. In the following two subsections, we review recent works on these two error factors.
 %Subsequently, considering the general error analysis of the QKSD algorithm, a bound is established in Section \ref{sec:main_result} for the perturbation error caused by finite sampling.

\subsection{Generalized Eigenvalue Perturbation}
 Compared with the eigenvalue problem, the generalized eigenvalue of a matrix pair $(\bm{H}, \bm{S})$ tends to be more sensitive to noise.
 In QKSD, this noise can generally involves finite sampling, Suzuki-Trotterization, and other algorithm-dependent errors. 
 For each element of the matrices in Eqs. \eqref{eq:prj_H} and \eqref{eq:overlap}, the errors result in matrix perturbations $\bm{\Delta_H}$ and $\bm{\Delta_S}$ as small deviations.
 As demonstrated previously \cite{STEWART197969, SUN1982331, 10.1007/BFb0062105}, these perturbations can affect the solution of GEVP, expressed as:
 \begin{equation}
 \label{eq:perterb_gen_eigeq}
     \tilde{\bm{H}} \tilde{\bm{c}}_j = \tilde{\bm{S}} \tilde{\bm{c}}_j \tilde{E}^{(n)}_j, 
 \end{equation}
 where $\tilde{E}^{(n)}_j = E^{(n)}_j + \Delta E^{(n)}_j$ is the $j^{\text{th}}$ perturbed eigenvalue, and $\tilde{\bm{c}}_j$ is corresponding perturbed eigenvector of the perturbed matrix pair $(\tilde{\bm{H}}:=\bm{H}+\bm{\Delta_H}, \tilde{\bm{S}}:=\bm{S}+\bm{\Delta_S})$.

%Certainly improve the perturbation theory 
 A previous study by Mathias and Li \cite{Mathias2004TheDG} reported an improved perturbation theory from a geometrical perspective on the complex plane describing the quadratic form of the problem, $\bm{x}^{\dagger}(\bm{H}+i\bm{S})\bm{x}$, for any normalized vector $\bm{x}$.
 Subsequently, Epperly et al. \cite{Theory_QSDK} utilized perturbation theory to describe QKSD perturbation with a real-time evolution ansatz.
 They also proposed the thresholding that cuts off the least-significant singular values of matrix $\bm{S}$ to address the large condition number.
 Because solving the GEVP involves the calculation of $\bm{S}^{-1/2}$, small singular values of $\bm{S}$ may amplify the noise in the matrix pair significantly.
 Such cases in which $\bm{S}$ possesses small singular values are called \textit{ill-conditioned problems}.

% Figure environment removed
 
 Algorithm \ref{alg:thresholding} describes the detailed thresholding technique conducted by removing the singular basis of $\bm{S}$ with smaller singular values than a certain noise level, $\epsilon$.
 Such process produces matrices $(\tilde{\bm{A}}, \tilde{\bm{B}})$ with the size of ${n_\epsilon} \times {n_\epsilon}$ so that the overlap matrix $\tilde{\bm{B}}$ has singular values larger than $\epsilon$.
 Therefore, its inverse matrix becomes numerically stable, although the dimensions of the problem are reduced (${n_\epsilon \le n}$).
 The generalized eigenvalue perturbation theory \cite[Corollary~3.6]{Mathias2004TheDG} was applied to a matrix pair and described in the following.
 Here, the spectral norm is denoted with the symbol $\|\cdot\|$.
 \begin{theorem}[Generalized Eigenvalue Perturbation Theory, Corollary 3.6 of a previous report \cite{Mathias2004TheDG}.]
 \label{theorem:generalized_eigenvalue_perturbation}
  Assume that the $n_\epsilon\times n_\epsilon$ matrix pair $(\tilde{\bm{A}}, \tilde{\bm{B}}):=(\bm{A}+\bm{\Delta_A}, \bm{B}+\bm{\Delta_B})$ is perturbed from $(\bm{A}, \bm{B})$, whose eigenvalues are $E_0^{(n_\epsilon)}\le E_1^{(n_\epsilon)} \cdots \le E_{n_\epsilon-1}^{(n_\epsilon)}$, and $\chi$ is defined as
  \begin{equation}\label{eq:def_chi}
      \chi := \sqrt{\|\bm{\Delta_A}\|^2+\|\bm{\Delta_B}\|^2}.
  \end{equation}
  Assume that error bound $\chi$ is sufficiently small,
  \begin{equation}\label{eq:assume_err_bound}
      \sqrt{2}{n_\epsilon}\chi \le \lambda_{n_\epsilon},
  \end{equation}
  and the following gap condition holds:
  \begin{equation}\label{eq:assume_gap_condition}
      |\tan^{-1}E_1^{({n_\epsilon})} - \tan^{-1}E_0^{({n_\epsilon})}| \ge \sin^{-1}\frac{{n_\epsilon}\chi}{\lambda_{n_\epsilon}},
  \end{equation}
  where $\lambda_{n_\epsilon}$ is the smallest singular value of $\bm{B}$.
  Then, the lowest perturbed eigenangle of $(\tilde{\bm{A}}, \tilde{\bm{B}})$, $\tan^{-1}\tilde{E}_0^{({n_\epsilon})}$ satisfies the following bound:
  \begin{equation}
  \label{eq:gep_theorem_result}
       \left| \tan^{-1}E^{({n_\epsilon})}_0 - \tan^{-1}\tilde{E}^{({n_\epsilon})}_0 \right| \le \sin^{-1}\frac{\sqrt{2}{n_\epsilon}\chi}{d_0}.
  \end{equation}
  Here, $d_0^{-1}$ is the condition number of the eigenangle $\tan^{-1}E_0^{({n_\epsilon})}$, defined as
   \begin{equation}
     d_0^{-1} := |\bm{x}_0^{\dagger}(\bm{A}+i\bm{B})\bm{x}_0|^{-1},
   \end{equation}
   where $\bm{x}_0$ is the unit-norm eigenvector of $(\bm{A}, \bm{B})$ with the lowest eigenvalue.
 \end{theorem}

 Although Theorem \ref{theorem:generalized_eigenvalue_perturbation} may explain the perturbation of $(\tilde{\bm{A}}, \tilde{\bm{B}})$ from the ideal $(\bm{A}, \bm{B})$ obtained by truncating the basis of $(\bm{H}, \bm{S})$ using Algorithm \ref{alg:thresholding}, the error magnitude $\chi$ could not still be explicitly obtained in terms of the error matrices $(\bm{\Delta_H}, \bm{\Delta_S})$.
 Hence, another error magnitude is defined to perform error analysis using explicit matrices:
 \begin{equation}
 \label{eq:def_eta}
 \eta := \sqrt{\|\bm{\Delta_H}\|^2+\|\bm{\Delta_S}\|^2}.
 \end{equation}
 Subsequently, the relationship between the explicit and implicit error magnitudes, $\eta$ and $\chi$, is obtained by the following \cite[Theorem~2.7]{Theory_QSDK}, which is
 \begin{equation}
 \label{eq:chi_bound}
 \chi \le O\left(\eta^{\frac{1}{1+\alpha}}/n\right),
 \end{equation}
 subject to the condition
 \begin{equation}
 \label{eq:asymptotic_epsilon}
 \epsilon = \Theta \left( \eta^{\frac{1}{1+\alpha}} \right),
 \end{equation}
 where $\alpha$ is a constant ranging from 0 to $1/2$.
 Finally, the corresponding asymptotic perturbation bound is given by
 \begin{equation}
 \left| \tan^{-1}E^{(n\rightarrow {n_\epsilon})}_0 - \tan^{-1}\tilde{E}^{(n\rightarrow {n_\epsilon})}_0 \right| \le O\left(\eta^{\frac{1}{1+\alpha}}d_0^{-1} \right).
 \label{eq:informal_eigenangle_error}
 \end{equation}
 Here, the superscript $(n\rightarrow {n_\epsilon})$ denotes the calculated eigenvalue obtained from the ${n_\epsilon}$-dimensional subspace produced by a thresholding from the $n$-dimensional space.

 In summary, the perturbation bound in Eq. \eqref{eq:informal_eigenangle_error} indicates that the perturbation error is sublinear to the error matrix norms and condition number after the truncation of the basis.
 Thus, with additional information about the error matrix norms $\|\bm{\Delta_H}\|$ and $\|\bm{\Delta_S}\|$, one can establish a sampling error analysis for the QKSD algorithm.
 Appendix \ref{sec:appendix_generalized_eignevalue_perturbation} presents a detailed version \cite[Theorem~2.7]{Theory_QSDK}.

\subsection{Krylov Approximations}
 Concerning the effect of projection to the small Krylov subspace, Krylov convergence, represented as the second factor on the right-hand side of Eq. \eqref{eq:krylov_total_error}, is reviewed here.
 For the classical version, this effect was quantified \cite{10.2307/2156670} by plugging an ansatz based on Chebyshev polynomials.
 In contrast, Stair et al. \cite{doi:10.1021/acs.jctc.9b01125} argued that the classical ($\hat{A}=\hat{H}$) and quantum ($\hat{A}=e^{-i\hat{H}\Delta_t}$) Krylov space could be close, up to $O(\Delta_t^2)$.
 Their work allows the utilization of the classical result \cite{10.2307/2156670} for the analysis of QKSD when a small $\Delta_t$ is employed.
 However, a small $\Delta_t$ to make the QKSD similar to the classical KSD causes ill-conditioning and degrades the perturbation error.
 A remarkable QKSD error analysis was proposed by Epperly et al. \cite{Theory_QSDK}, which is advantageous compared with previous studies in two ways: the thresholding effect is considered, and it is directly analogous to classical KSD analysis \cite{10.2307/2156670} by replacing Chebyshev polynomials with trigonometric polynomials.
 Their analysis is reviewed in the following.
 
 \begin{theorem}[Quantum Krylov Error, Theorem 1.2 of  \cite{Theory_QSDK}.]\label{theorem:qksd_err}
  Let $\ket{\psi_0}\cdots \ket{\psi_{N-1}}$ and $E_0\le\cdots\le E_{N-1}$ be the eigenvectors and corresponding eigenvalues, respectively, for a Hamiltonian $\hat{H}$ in the Hilbert space (dimension: $N$). Then, the initial state is expanded on an eigenbasis,
  \begin{equation}
      \ket{\phi_0} = \sum_{j=0}^{N-1}\gamma_i\ket{\psi_i}.
  \end{equation}
  Suppose that the Krylov subspace is generated within time sequence $\{t_j:=\pi j/\Delta E_{N-1}\}_{j=-k}^k$ for a certain integer, $k$; then, the Krylov order can be given as $n=2k+1$, where $\Delta E_j := E_j -E_0$ is also defined.
  Overall, if the GEVP returned by Algorithm \ref{alg:thresholding} with the parameter $\epsilon$ in Eq. \eqref{eq:asymptotic_epsilon} is solved, the following upper bound for the error between the lowest eigenangle and the full Hilbert space solutions holds:
  \ifthenelse{\boolean{twocolumn}}
  {
  \begin{multline}  \label{eq:krylov_convergence}
   \left|\tan^{-1}E_0^{(n)} - \tan^{-1} E_0\right|\\ \le O\left( \frac{1-|\gamma_0|^2}{|\gamma_0|^2} e^{-n O\left( \frac{\Delta E_1}{\Delta E_{N-1}} \right)} + \frac{\Delta E_{N-1}}{|\gamma_0|^2}\eta^{\frac{1}{1+\alpha}} \right).      
  \end{multline}
  }
  {
  \begin{equation}
  \label{eq:krylov_convergence}
   \left|\tan^{-1}E_0^{(n)} - \tan^{-1} E_0\right| \le O\left( \frac{1-|\gamma_0|^2}{|\gamma_0|^2} e^{-n O\left( \frac{\Delta E_1}{\Delta E_{N-1}} \right)} + \frac{\Delta E_{N-1}}{|\gamma_0|^2}\eta^{\frac{1}{1+\alpha}} \right).
  \end{equation}  
  }
 \end{theorem}
 
 In the right-hand side of Eq. \eqref{eq:krylov_convergence}, the first term corresponds to the approximation error between the full Krylov subspace and the Hilbert space, whereas the second term corresponds to the subspace after basis truncation.
 Furthermore, if the effect of thresholding is small, it implies that the Krylov error bound decays exponentially to the Krylov order, with the prefactor being inversely proportional to the overlap, $|\gamma_0|^2$. 
 
\section{Main Results}\label{sec:main_result}

\ifthenelse{\boolean{twocolumn}}{%\onecolumngrid
}{}
 % Figure environment removed
\ifthenelse{\boolean{twocolumn}}{%\twocolumngrid
}{}

 When evaluating the elements of matrices $\bm{H}$ in Eq.\eqref{eq:QKD_H_seq} and $\bm{S}$ in Eq.\eqref{eq:QKD_S_seq}, the Hadamard test subroutine intrinsically suffers from sampling errors, even if the  other noises such as hardware noise and Trotterization error are reduced to a negligible level.
 Although Eq.\eqref{eq:krylov_convergence} shows that the QKSD algorithm converges quickly toward the full Hilbert space solution, it is necessary to assess the tolerance to perturbation errors resulting from finite sampling.
 Thus, in this section, Krylov perturbations are mainly investigated with respect to a finite number of shots.
 Fig.~\ref{fig:error_analysis_flowchart} illustrates an overview of the structure of this section.
 First, the sampling error model using a Gaussian random matrix is described, i.e., the element in the Gaussian random matrix is regarded as the sampling error.
 The variance of the Gaussian element is set from that of the binomial distribution whose random variable is the averaged outcome obtained by implementing a finite number of Hadamard tests. 
 Then, the statistical behavior of the error matrix norms, $\bm{\Delta_H}$ and $\bm{\Delta_S}$, is formulated.
 Last, the norm statistics and Theorem \ref{theorem:generalized_eigenvalue_perturbation} are subsequently used to suggest a nearly optimal threshold to mitigate the effect of ill-conditioning bases and finally derive the sampling perturbation bound of the QKSD.
 
\subsection{Error Model}
 First, the random matrix theory is introduced to address the behavior of a random matrix whose elements contain sampling errors.
 %For the development of the theory, we show how the sampling errors to the matrix elements are represented. 
 Because the single-shot outcome of Hadamard test is binary, the sampling averages follow a binomial distribution.
 For example, if the real part of $\bm{H}_{kl}$ is evaluated using $m$ samplings, its noisy element follows a binomial distribution $\textrm{Re}[\tilde{\bm{H}}_{kl}]\sim 2\textrm{Bin}(m, p)/m-1$, where $p$ denotes the true probability that exactly estimates the matrix element ($2p-1=\textrm{Re}[\bm{H}_{kl}]$).
 Subsequently, if $m$ is sufficiently large, it is approximated by a Gaussian distribution, and the residual matrix for each $\bm{Z}\in\{\bm{H}, \bm{S}\}$ is expressed as
 \begin{equation}\label{eq:delta_z}
     [\bm{\Delta_Z}]_{kl} \sim \mathcal{N}(0, \sigma_{\bm{Z},kl}^{(\mathrm{r})2}) + i\mathcal{N}(0, \sigma_{\bm{Z},kl}^{(\mathrm{i})2}).
 \end{equation}
 Generally, $\bm{\Delta_Z}$ is Hermitian and possibly Toeplitz, and its off-diagnonal elements $[\bm{\Delta_Z}]_{kl}$ are complex random variables for which the real and imaginary parts are independent whereas diagonal elements are real values.
 %Although we could add any other error component such as the Suzuki error to the variance or the mean, we do not shift the mean for the model to satisfy the zero-mean assumption in random matrix theories.
 
 Based on the averaged Bernoulli distribution, the variance of $[\bm{\Delta_Z}]_{kl}$ in Eq. \eqref{eq:delta_z} is inversely proportional to the number of shots $m_{\bm{Z},kl}$.
 Specifically, the sampling variances are given by
 \begin{align}
     \sigma_{\bm{H},kl}^2 =& \begin{cases}
        \frac{4\|\hat{H}\|_\beta^2}{m_{\bm{H},kl}} & \text{for } k\neq l\\
        \frac{2\|\hat{H}\|_\beta^2}{m_{\bm{H},kl}} & \text{for } k=l\\
     \end{cases},\label{eq:var_mat_elem_H}\\
     \sigma_{\bm{S},kl}^2 =& \begin{cases}
         \frac{4}{m_{\bm{S},kl}} & \text{for } k\neq l\\
         \frac{2}{m_{\bm{S},kl}} & \text{for } k=l\\
     \end{cases},\label{eq:var_mat_elem_S}
 \end{align}
 where the same number of shots are allocated for the real and imaginary part in off-diagonals($k\neq l$) and $\|\hat{H}\|_\beta:=\sum_j \beta_j$ is sum of coefficients in the unitary partitioning (Eq. \eqref{eq:pauli_ham_grp}).
 According to the unitary partitioning in Eq. \eqref{eq:pauli_ham_grp}, an element of $\bm{H}$ (see Eq. \eqref{eq:QKD_H_seq}) requires different Hadamard test configurations and the corresponding distribution of $m_{\bm{H},kl}$ shots for each configuration.
 We can determine the optimal distribution of shots that minimizes the variance from the coefficients of unitary partitioning, which can be analytically estimated.
 Subsequently, Eq. \eqref{eq:var_mat_elem_H} shows the variance minimized by the optimal distribution.
 Furthermore, in the middle of the derivations of Eqs.~\eqref{eq:var_mat_elem_H} and \eqref{eq:var_mat_elem_S}, we average the sampling variances over the Haar measure \cite[Ch. 7.2]{watrous_2018} to remove the dependencies to the states and operators and thus make the estimated variances independent of their prior unknown actual expectation values.
 Appendix \ref{sec:appendix_hadamard_sampling_variance} shows the detailed derivations of Eqs. \eqref{eq:var_mat_elem_H} and \eqref{eq:var_mat_elem_S}.
 

\subsection{Norm Behavior of Error Matrices}
 In this section, by employing the aforementioned approach of modeling random matrices, the norm statistical properties of such matrices are derived, as matrix norm is used in the perturbation theorem in Theorem \ref{theorem:generalized_eigenvalue_perturbation} and Eq. \eqref{eq:def_eta}.
 As an instance of random matrix theory, it is well known that the $n\times n$ Gaussian matrix with independent and zero-mean elements has a spectral norm asymptotically growing as fast as $O(\sigma_{max}\sqrt{n})$ with high probability \cite[Theorem~4.4.5]{vershynin_2018}, where $\sigma_{max}$ is the maximum value of the standard deviations of the matrix elements.
 However, in order to obtain a nonasymptotic bound and impose the Hermitian and Toeplitz conditions, we then use a slightly different approach \cite[Theorem~4.1.1]{MAL-048}, where the approach to the matrix concentration is based on the matrix Laplace transform.

 As reviewed in Section \ref{sec:QKSD}, matrices $\bm{H}$ and $\bm{S}$ are constructed using a series of Hadamard tests.
 Specifically, while $\bm{S}$ is always constructed as a Toeplitz matrix, the matrix $\bm{H}$ can be chosen as either Toeplitz or non-Toeplitz depending on whether a precise simulation of $e^{-i\hat{H}n\Delta_t}$ is available.
 First, one considers an $n\times n$ Hermitian Toeplitz matrix constructed from a sequence of $n$ complex values, or, equivalently, $2n-1$ real values.
 By treating these values as random variables in Eq. \eqref{eq:delta_z}, one can then find the statistical behavior of $\|\bm{\Delta_Z}\|$.
 In addition, the bound on its expectation value, shown in \cite[Theorem~4.1.1]{MAL-048}, is minimized with a proper distribution of the total samplings $M_{\bm{Z}}$ to the Hadamard test of each matrix element.
 Appendices \ref{sec:Toeplitz_mat_theory_proof} and \ref{sec:Non_Toeplitz_mat_theory_proof} describe the results and provide detailed proofs of the following theorems.

 \begin{theorem}[Toeplitz Error Matrix Obtained by Sampling]
 \label{theorem:toeplitz_error_matrix}
 Supposing that the Toeplitz Hermitian matrix $\bm{Z}\in\{\bm{H}, \bm{S}\}$ is generated by a sequence of Hadamard tests with a given total number of samplings $M_{\bm{Z}}$, the bound for the expected norm of noise matrix $\|\bm{\Delta_Z}^{(\mathrm{t})}\|$, suggested previously \cite[Theorem~4.1.1]{MAL-048}, can be minimized by adequately distributing $M_{\bm{Z}}$ to each Hadamard test setting:
 \begin{align}
     m_0 = m_0^{(\mathrm{r})} &= \frac{M_{\bm{Z}}}{\sqrt{2}(n-1)+1}, \\
     m_{k>0}^{(\mathrm{r})}= m_{k>0}^{(\mathrm{i})} &= \frac{M_{\bm{Z}}}{2(n-1)+\sqrt{2}} = \frac{m_0}{\sqrt{2}}.
 \end{align}
 Here, $m_k$ denotes the number of samplings for the Hadamard test to obtain the $k^{\text{th}}$ superdiagonal and subdiagonal elements, and the superscripts $(\mathrm{r})$ and $(\mathrm{i})$ correspond to the real and imaginary Hadamard test configurations.
 Consequently, the optimal expectation bound becomes
 \ifthenelse{\boolean{twocolumn}}
 {
 \begin{multline}\label{eq:toeplitz_error_matrix_norm_bound}
     \mathbb{E}[\|\bm{\Delta_Z}^{(\mathrm{t})}\|] \le \frac{2V_{\bm{Z}}(\sqrt{2}n+1-\sqrt{2})\sqrt{\log(2n)}}{\sqrt{M_{\bm{Z}}}} \\ \lesssim \frac{2nV_{\bm{Z}}\sqrt{2\log(2n)}}{\sqrt{M_{\bm{Z}}}} =: \frac{e_{\bm{Z}}^{(\mathrm{t})}(n)}{\sqrt{M_{\bm{Z}}}}
 \end{multline}
 }
 {
 \begin{equation}\label{eq:toeplitz_error_matrix_norm_bound}
     \mathbb{E}[\|\bm{\Delta_Z}^{(\mathrm{t})}\|] \le \frac{2V_{\bm{Z}}(\sqrt{2}n+1-\sqrt{2})\sqrt{\log(2n)}}{\sqrt{M_{\bm{Z}}}} \lesssim \frac{2nV_{\bm{Z}}\sqrt{2\log(2n)}}{\sqrt{M_{\bm{Z}}}} =: \frac{e_{\bm{Z}}^{(\mathrm{t})}(n)}{\sqrt{M_{\bm{Z}}}}
 \end{equation}
 }
 where $V_{\bm{Z}}$ is the variance prefactor depending on matrix $\bm{Z}$, specifically, $V_{\bm{H}}=\|\hat{H}\|_\beta$ and $V_{\bm{S}}=1$, and $e_{\bm{Z}}^{(\mathrm{t})}(n):=2nV_{\bm{Z}}\sqrt{2\log(2n)}$.
 \end{theorem}

 Similar to Theorem \ref{theorem:toeplitz_error_matrix}, the non-Toeplitz version of the error matrix statistics is also presented below. However, independent of the construction method, only the matrix $\bm{H}$ is considered here because $\bm{S}$ is always Toeplitz.

 \begin{theorem}[Non-Toeplitz Error Matrix Obtained by Sampling]
 \label{theorem:non_toeplitz_error_matrix}
 Assuming that a sequence of Hadamard tests generates the non-Toeplitz Hermitian $\bm{H}$ with a total of $M_{\bm{H}}$ samplings, the bound for the expected norm of the noise matrix $\|\bm{\Delta_H}^{(\mathrm{nt})}\|$, suggested previously \cite[Theorem~4.1.1]{MAL-048}, is minimized by the optimal distribution
 \begin{align}
     m_{kk}^{(\mathrm{r})}=m_{k<l}^{(\mathrm{r})}=m_{k<l}^{(\mathrm{i})}=\frac{M_{\bm{H}}}{n^2},
 \end{align}
 where $m_{kl}^{\mathrm{(r,i)}}$ denotes the number of samplings to evaluate the real or imaginary part of the $(k,l)$ and $(l,k)$ sites of $\bm{H}$. The corresponding optimal bound for the expected norm is
 \begin{equation}\label{eq:non_toeplitz_error_matrix_norm_bound}
     \mathbb{E}[\|\bm{\Delta_H}^{(\mathrm{nt})}\|] \le \frac{2n\|\hat{H}\|_\beta\sqrt{n\log(2n)}}{\sqrt{M_{\bm{H}}}} =: \frac{e_{\bm{H}}^{(\mathrm{nt})}(n)}{\sqrt{M_{\bm{H}}}},
 \end{equation}
 where $e_{\bm{H}}^{(\mathrm{nt})}(n):=2n\|\hat{H}\|_\beta\sqrt{n\log(2n)}$.
 \end{theorem}
 
 Under the Toeplitz (and non-Toeplitz) construction, although the number of independent matrix elements is taken as $O(n)$ (and $O(n^2)$), the number of samplings $M_{\bm{Z}}$ should increase as fast as $O(n^2\log{n})$ (and $O(n^3\log{n})$) to maintain the expected error norms below a constant level.
 % Subsequently, the number of overall required samplings has an additive factor of $n\log n$ to the number of elements, reducing the error of each matrix element as the matrix size grows.
 Appendix \ref{sec:appendix_matrix_variance_statistics} reviews the statistical distribution of $\|\bm{\Delta_Z}\|$, known as sub-gaussian from concentration inequality \cite[Theorem~5.6.]{10.1093/acprof:oso/9780199535255.002.0004}.
 Defining a bound factor $\kappa>0$, such that $e_{\bm{Z}}(n)/\sqrt{M_{\bm{Z}}}=(1+\kappa)\mathbb{E}[\|\bm{\Delta_Z}\|]$, and applying the concentration inequality then gives
 \begin{equation}\label{eq:mat_norm_concentration}
     \mathbb{P}\left\{ \|\bm{\Delta_Z}\| \ge \frac{e_{\bm{Z}}(n)}{\sqrt{M_{\bm{Z}}}} \right\} \le \left(\frac{1}{2n}\right)^{\left( 1+\sfrac{1}{\kappa} \right)^{-2}}.
 \end{equation}
 Eq. \eqref{eq:mat_norm_concentration} describes the probability of finding a norm larger than that of the upper bound for the expected norm described in Eqs. \eqref{eq:toeplitz_error_matrix_norm_bound} and \eqref{eq:non_toeplitz_error_matrix_norm_bound}.
 The numerical results show that the value of $\kappa$ is located within the interval $[0.5, 1.0]$; however, the actual statistical distribution of $\|\bm{\Delta_Z}\|$ is much tighter than in Eq. \eqref{eq:mat_norm_concentration}, causing all $\|\bm{\Delta_Z}\|$ values with the 10,000 random initializations to be less than $e_{\bm{Z}}/\sqrt{M_{\bm{Z}}}$.
 Based on such observation, we assume that the following bound condition holds:
 \begin{equation}\label{eq:mat_norm_bound}
     \|\bm{\Delta_Z}\| < \frac{e_{\bm{Z}}(n)}{\sqrt{M_{\bm{Z}}}}.
 \end{equation}

\subsection{QKSD Perturbation Caused by Finite Sampling}
 Finally, by applying Eq. \eqref{eq:mat_norm_bound} to Theorem \ref{theorem:generalized_eigenvalue_perturbation}, we connect the random error matrix statistics to the perturbation theory to estimate the impact of finite sampling.
 Before doing so, however, the unstable basis vectors are resolved by examining the singular values of the matrix $\tilde{\bm{S}}$, thereby restoring the generalized eigenvalue problem from ill-conditioning.
 Specifically, the singular vectors of $\tilde{\bm{S}}$ are removed if corresponding singular values are below parameter $\epsilon$, as specified in Algorithm \ref{alg:thresholding}.
 In practice, $\epsilon$ is set to the noise level, such as machine precision in classical KSD.
 Similarly, as shown in Eq. \eqref{eq:mat_norm_bound}, we set the parameter $\epsilon$ for QKSD as
 \begin{equation}\label{eq:opt_epsilon}
  \epsilon = \frac{e_{\bm{S}}^{(\mathrm{t})}(n)}{\sqrt{M_{\bm{S}}}}.
 \end{equation}
 Although fewer basis vectors are considered after thresholding, the perturbation error of the final solution is significantly reduced in general.
 The numerical experiments in the next section will show that such $\epsilon$ settings yield lower errors in the final solution than any other assignment to $\epsilon$.
 In addition to Eq. \eqref{eq:chi_bound}, which shows the bound of $\chi$ with $\eta$, we assume a simpler bound that removes the dependency on $n$ ($\chi\le K\eta$), where $K>0$ is a factor that is close to unity in the numerical simulation.
 Combining these assumptions with Theorem \ref{theorem:generalized_eigenvalue_perturbation}, we can formulate a nonasymptotic bound for the QKSD solution error affected by finite sampling.


 \begin{theorem}[Nonasymptotic Sampling Perturbation after Thresholding]\label{theorem:main_result}
  Suppose that QKSD algorithm with the order of $n$, whose the matrix construction method is described in Theorem \ref{theorem:toeplitz_error_matrix} or Theorem \ref{theorem:non_toeplitz_error_matrix}, leads to a generalized eigenvalue problem of $(\tilde{\bm{H}}, \tilde{\bm{S}})$.
  Afterward, thresholding algorithm (Algorithm \ref{alg:thresholding}) is applied to $(\tilde{\bm{H}}, \tilde{\bm{S}})$ with the parameter in Eq.\eqref{eq:opt_epsilon} and produces another matrix pair of size ${n_\epsilon}\times {n_\epsilon}$, denoted as $(\tilde{\bm{A}}, \tilde{\bm{B}})$, with the lowest eigenvalue being denoted as $\tilde{E}_0^{(n\rightarrow {n_\epsilon})}$.
  In addition to small gap, small perturbation, and random matrix norm bound assumptions, as in Eqs. \eqref{eq:assume_err_bound}, \eqref{eq:assume_gap_condition}, and \eqref{eq:mat_norm_bound}, respectively, it is assumed that the matrix perturbations before ($\eta$) and after ($\chi$) in the thresholding satisfy
  \begin{equation}
  \label{eq:thm_chi_assumption}
      \chi \le \eta.
  \end{equation}
  Accordingly, the eigenvalue perturbation ($|\tilde{E}^{(n\rightarrow {n_\epsilon})}_0 - E^{(n\rightarrow {n_\epsilon})}_0|$) can be bounded as below, if the relative perturbation is sufficiently small ($|\tilde{E}^{(n\rightarrow {n_\epsilon})}_0 - E^{(n\rightarrow {n_\epsilon})}_0|/E \ll 1$)
  \ifthenelse{\boolean{twocolumn}}
  {
  \begin{multline}
  \label{eq:gep_theorem_result_sampling_noise}
      |\tilde{E}^{(n\rightarrow {n_\epsilon})}_{0} - E^{(n\rightarrow {n_\epsilon})}_{0}| \le (1+E^{(n\rightarrow {n_\epsilon})2}_{0})\\ \times \sin^{-1}\left( \frac{\sqrt{2}{n_\epsilon}(e_{\bm{H}}+e_{\bm{S}})}{d_{0}\sqrt{M}} \right),
  \end{multline}  
  }
  {
  \begin{equation}
  \label{eq:gep_theorem_result_sampling_noise}
     |\tilde{E}^{(n\rightarrow {n_\epsilon})}_{0} - E^{(n\rightarrow {n_\epsilon})}_{0}| \le (1+E^{(n\rightarrow {n_\epsilon})2}_{0}) \sin^{-1}\left( \frac{\sqrt{2}{n_\epsilon}(e_{\bm{H}}+e_{\bm{S}})}{d_{0}\sqrt{M}} \right),
  \end{equation}
  }
  where $d_0^{-1}$ is a condition number for the lowest solution and
  \begin{equation*}
  \begin{split}
    e_{\bm{H}}:=&
    \begin{cases}
        2\sqrt{2}\|\hat{H}\|_\beta n\sqrt{\log(2n)} \quad &\text{Toeplitz case}\\
        2\|\hat{H}\|_\beta n^{3/2}\sqrt{\log(2n)} \quad &\text{Non-Toeplitz case}
    \end{cases},\\
    e_{\bm{S}}:=& 2\sqrt{2}n\sqrt{\log(2n)}
  \end{split}
  \end{equation*}
  and $M=M_{\bm{H}}+M_{\bm{S}}$ is the total number of samplings optimally distributed, as shown below:
  \begin{align}
      M_{\bm{H}} = \frac{e_{\bm{H}}}{e_{\bm{H}}+e_{\bm{S}}}M,\label{eq:opt_MH}\\
      M_{\bm{S}} = \frac{e_{\bm{S}}}{e_{\bm{H}}+e_{\bm{S}}}M\label{eq:opt_MS}.
  \end{align}
 \end{theorem}

 \begin{proof}
  The optimization of $\mathcal{E}_2:=\sqrt{e_{\bm{H}}^2/M_{\bm{H}}+e_{\bm{S}}^2/M_{\bm{S}}}$, which is a bound for $\chi$, based on the assumption of Eq. \eqref{eq:mat_norm_bound} is obtained by setting Eqs. \eqref{eq:opt_MH} and \eqref{eq:opt_MS} under the constraint of $M=M_{\bm{H}}+M_{\bm{S}}$.
  The corresponding optimal value is
  \begin{equation}
  \mathcal{E}_2^{\star}=\frac{e_{\bm{H}}+e_{\bm{S}}}{\sqrt{M}} \ge \eta \ge \chi.
  \end{equation}  
  By replacing $\chi$ with $\mathcal{E}_2^{\star}$, we can convert the total perturbation bound in Eq.\eqref{eq:gep_theorem_result} to Eq.\eqref{eq:gep_theorem_result_sampling_noise} through linear approximations:
  \ifthenelse{\boolean{twocolumn}}{
   \begin{multline}
   \label{eq:linear_apprx_pert}
     \left|\tilde{E} - E\right| = (1+E^2)\left|\tan^{-1}\tilde{E} - \tan^{-1}E\right|\\ + O\left(\left|\tilde{E} - E\right|^2/E\right).
   \end{multline}
  }{
   \begin{equation}
   \label{eq:linear_apprx_pert}
     \left|\tilde{E} - E\right| = (1+E^2)\left|\tan^{-1}\tilde{E} - \tan^{-1}E\right| + O\left(\left|\tilde{E} - E\right|^2/E\right).
   \end{equation}
  }
 \end{proof}

 Overall, because the number of remaining bases is less than $n$, Theorem \ref{theorem:main_result} leads to an asymptotic bound for finite sampling perturbation,
 \begin{equation}
     |\tilde{E}_{0}^{(n\rightarrow {n_\epsilon})} - {E}_{0}^{(n\rightarrow {n_\epsilon})}| = O\left(\frac{\|\hat{H}\|^2 n^z \sqrt{\log n}}{d_0 \sqrt{M}}\right),
 \end{equation}
 where $z=2$ for Toeplitz construction, and $z=5/2$ for non-Toeplitz construction. However, in the numerical simulation, the condition number $d_0^{-1}$ remained large and amplified the sampling error, even though the thresholding alleviated the ill-conditioning.
 Although little is known about $d_0^{-1}$ for the subspace generated by time evolution, it appears to exhibit an exponentially increasing behavior with $n$, in many cases in the numerical results (Sec.~\ref{sec:final_pert_bd}).
 Thus, a significantly large number of Hadamard test samples are required.
 Therefore, to overcome the lack of knowledge about $d_0^{-1}$, one can find another measure for the numerical stability, the condition number of $\bm{S}$ with an operator norm, which is defined as
 \begin{equation}
     \mathrm{cond}(\bm{S}) = \frac{\lambda_{\mathrm{max}}}{\lambda_{\mathrm{min}}},
 \end{equation}
 where $\lambda_{\mathrm{min}}$ and $\lambda_{\mathrm{max}}$ correspond to the minimum and maximum singular values of $\bm{S}$.
 The condition number of $\bm{S}$ becomes large because of this linear dependency on the basis \cite[Appendix A]{quantum_power_method}.
 The linear dependency increases especially when the time step is small ($\Delta_t \ll O(\|\hat{H}\|^{-1})$), which makes the base operator $e^{-i\hat{H}\Delta_t}=\hat{I}-i\hat{H}\Delta_t+O(\|\hat{H}\|^2\Delta_t^2)$ close to identity.

 When constructing the matrix $\bm{H}$, either Toeplitz or non-Toeplitz construction should be employed.
 As mentioned in Section \ref{sec:QKSD}, with a sufficiently small Trotterization error, the Toeplitz construction is superior to the non-Toeplitz one because measurements are allocated to obtain fewer elements.
 Using Theorems \ref{theorem:toeplitz_error_matrix} and \ref{theorem:non_toeplitz_error_matrix}, we derive the asymptotic bound for the circuit depth, which can suppress the simulation error to make the Toeplitz construction more advantageous than non-Toeplitz one.
 The requirement for the circuit depth is 
 \begin{equation}\label{eq:min_ckt_depth}
      D = \Omega\left( N_{\Gamma}\Delta_t \left(\frac{\|\hat{H}\|}{\|\hat{H}\|_\beta}\right)^{1/2} \left(\frac{n^3M_{\bm{H}}}{\log n}\right)^{1/4} \right),
 \end{equation}
 whose details are provided in Appendix \ref{sec:appendix_consideration_of_simulation_error}.
 Such deep quantum circuits can reduce the effect of the Trotterization error in the Toeplitz construction lower than sampling error in the non-Toeplitz one which does not suffer from Trotterization error.


\section{Numerical Simulations}\label{sec:result}
 A numerical simulation of many-body systems is performed using the perturbed QKSD method, which considers the error caused by finite sampling of the Hadamard test.
 By comparing this simulation with the theoretical results in Section \ref{sec:main_result}, we then analyze the statistical behavior of the error matrices, the validation of the optimal thresholding, and the perturbed ground energy in terms of the finite sampling error.

 In this work, we focus on the 1D spinful Fermi-Hubbard model of length $L$, which is given as
 \ifthenelse{\boolean{twocolumn}}
 {
 \begin{align}
  \nonumber \hat{H} = -t\sum_{i=1\cdots L-1}\sum_{\sigma \in \{\uparrow, \downarrow\}}(&\hat{a}^\dagger_{i\sigma}\hat{a}_{{i+1}\sigma} + \hat{a}_{i\sigma}\hat{a}^\dagger_{{i+1}\sigma})\\
  &+ u\sum_{i=1\cdots L}\hat{a}^\dagger_{i\uparrow}\hat{a}_{{i}\uparrow}\hat{a}^\dagger_{i\downarrow}\hat{a}_{{i}\downarrow},\label{eq:Hamiltonian}
 \end{align}
 }
 {
 \begin{equation*}
  \hat{H} = -t\sum_{i=1\cdots L-1}\sum_{\sigma \in \{\uparrow, \downarrow\}}(\hat{a}^\dagger_{i\sigma}\hat{a}_{{i+1}\sigma} + \hat{a}_{i\sigma}\hat{a}^\dagger_{{i+1}\sigma}) + u\sum_{i=1\cdots L}\hat{a}^\dagger_{i\uparrow}\hat{a}_{{i}\uparrow}\hat{a}^\dagger_{i\downarrow}\hat{a}_{{i}\downarrow},
 \end{equation*}
 }
 where $\hat{a}^{(\dagger)}_{i\sigma}$ is a fermionic annihilation (creation) operator at the $i$\textsuperscript{th} site with spin $\sigma\in\{\uparrow,\downarrow\}$.
 First, three parameter cases are considered: $(t, u)=(0.1, 0.2)$ and $(0.2, 0.1)$, indicating far $(t/u<1)$ and near $(t/u>1)$ interactions, respectively, and the $(0.1, 0.8)$ case, which is known to be difficult to solve.
 Although the parameter ratio of $t$ to $u$ matters, the absolute values are set making  $\|\hat{H}\|_\beta$ close to unity.
 %it was also important to set the absolute values making $\|\hat{H}\|_\beta$ close to unity and almost evenly distributing the samplings in Eqs.\eqref{eq:opt_MH} and \eqref{eq:opt_MS}.
 We particularly select the system size $L=8$ in this simulation.
 The Hamiltonian for each setting is mapped in terms of Pauli operators using a qubit mapping scheme that embraces the number, spin, and spatial symmetries, which results in reducing three qubits.
 Thus, the numbers of remaining qubits is $N_q=13$. %and have their spectral norm $\|\hat{H}\|=1.7867$.
 
 Unitary decomposition of the Hamiltonian in Eq. \eqref{eq:pauli_ham_grp} is performed using the sorted insertion grouping algorithm \cite{efficient_quantum_measurement}, which results in $N_\beta = 34$.
 It is assumed that we perfectly simulate the time evolution $e^{-i\hat{H}t}$, thereby neglecting the effect of Hamiltonian simulation errors, such as Suzuki-Trotterization errors, and focusing on the sampling error.
 Furthermore, the Krylov basis of order $n$ is set to $\{\exp{(-ik\Delta_t\hat{H})}\ket{\phi_0}\}_{k=-\lfloor n/2 \rfloor}^{\lfloor n/2\rfloor}$, where $\ket{\phi_0}$ is the Hartree Fock ground state.
 Here, we choose time steps as the time sequence from Theorem \ref{theorem:qksd_err}.
 Since it is difficult to determine $\Delta E_{N-1}$ in advance, we replace it with $\|\hat{H}\|_\beta$.
 Furthermore, only odd number of bases are considered to include the reference state ($k=0$).
 %Consequently, since the calculations were performed based on the Hadamard test simulated by classical sampling from the binomial distribution, the maximum number of samplings was limited ($M\le10^{18}$) due to the default data type(int64).

 %The contents in this section include the numerical results for the behavior of error matrices, optimal basis truncation validations, and estimations of how the sampling error affects calculated energies.

\subsection{Sampling Error Matrices}
  % Figure environment removed
 
 Statistical results for the error matrix norms ($\|\bm{\Delta_H}^{(\mathrm{t})}\|$, $\|\bm{\Delta_H}^{(\mathrm{nt})}\|$, and $\|\bm{\Delta_S}\|$) generated by the 10,000 different random seeds are shown in Fig.~\ref{fig:error_norm}, together with their probabilistic bound, Eqs.~\eqref{eq:toeplitz_error_matrix_norm_bound} and \eqref{eq:non_toeplitz_error_matrix_norm_bound}. 
 We can clearly see that the error norm is inversely proportional to the square root of the sampling number. 
 Although Fig.~\ref{fig:error_norm} displays only the case of $L=8$ and $u/t=8$, the tendency is consistent for the various parameters in the Hamiltonian.
 In Fig.~\ref{fig:error_norm}, with Krylov order $n$, the error norm is bounded by $O(n^{3/2}\sqrt{\log n})$ for the non-Toeplitz construction and $O(n \sqrt{\log n})$ for the Toeplitz construction.
 As Krylov order $n$ and number of samplings $M$ increase, the logarithmic difference between the mean and the bound remains mostly constant.
 In other words, the factor $\kappa$ in Eq.~\eqref{eq:mat_norm_concentration} is invariant under the fixed target Hamiltonian and matrix construction method.
 However, although the tail bound probability can be estimated using Eq.~\eqref{eq:mat_norm_concentration}, all experiments in this study were proved to be within the estimated bound, validating our assumption in Eq.~\eqref{eq:mat_norm_bound}.

\iffalse
 It is noted that an element of the matrices close to zero is equivalent to the binomial distribution with parameter $p\approx 1/2$ and is at the maximum uncertainty. 
 Therefore, it is expected that the factor $\kappa$ would increase as the sparsity of the problem matrices (the number of zero elements) increased.
 Although the simulation showed that the sampling error for the sparse matrix is larger than that for the dense matrix, the effect was turned out to be relatively small in practice.
 For the two matrices with $L=4$ and $L=8$, where the proportions of elements larger than $0.1$ are $0.90$ and $0.66$ respectively, the difference in $\kappa$ is small ($0.608\pm 0.287$ and $0.598\pm 0.221$ respectively).
 Hence, the dependency of the error matrix norm on the Hamiltonian and reference state which defines sparsity is small.
\fi

\iffalse
 % Figure environment removed
\fi



\subsection{Truncation of Unstable Basis}\label{sec:numerical_truncation}

% Figure environment removed
 We analyze the effect of thresholding on reducing errors in the perturbed solution and assess the optimal threshold, $\epsilon$, based on the knowledge of the error matrix norm discussed in the previous section.
 Fig.~\ref{fig:S_eigenvalue_perturbation} depicts the singular-value spectrum of $\tilde{\bm{S}}$ to obtain an insight into selecting the optimal value of $\epsilon$.
 This result confirms that, up to a certain level $e_{\bm{S}}^{(\mathrm{t})}/\sqrt{M_{\bm{S}}}$, the singular values of $\tilde{\bm{S}}$, referred to as $\tilde{\lambda}_i$ for $i\in\{1\cdots n\}$, coincided with the ideal one, $\lambda_i$ (the dashed line in Fig.~\ref{fig:S_eigenvalue_perturbation}).
 Below this level, however, a discrepancy between $\tilde{\lambda}_i$ and $\lambda_i$ occurs on a logarithmic scale, which is attributed to Weyl's inequality stating that an error matrix norm can limit the singular value perturbation.
 This finding agrees with the assumption in Eq. \eqref{eq:mat_norm_bound}, which indicates that it is possible to deduce the threshold $\epsilon$ regardless of prior experiments, 
 \begin{align}\label{ineq:trunc_pt}
     |\tilde{\lambda}_i - \lambda_i| \le \|\bm{\Delta_S}\| < \frac{e^{(\mathrm{t})}_{\bm{S}}(n)}{\sqrt{M_{\bm{S}}}}=\epsilon.
 \end{align}

 \iffalse
 We demonstrate that argument of threshold ($\epsilon=e_{\bm{S}}/\sqrt{M_{\bm{S}}}$) is valid, by showing the sampling perturbation to the $\bm{S}$ matrix which is shown in the figure \ref{fig:S_eigenvalue_perturbation}. The perturbations result in the additive deviation of $\lambda$ by $\|\bm{\Delta_S}\|<\mathcal{E}_{\bm{S}}$, and thus, disagreements between $\tilde{\lambda}$ and $\lambda$ in the logarithmic scale start from the level of $\lambda_m\le e_{\bm{S}}/\sqrt{M_{\bm{S}}}$. This indicates that such prior estimation of $epsilon$ points 
 that we can set the threshold as $\epsilon=\mathcal{E}_{\bm{S}}$ because $\lambda$'s that correspond to $\tilde{\lambda} $ less than $ \mathcal{E}_{\bm{S}}$ are suspected to be very small.
 %Assuming that $\lambda$ decays exponentially, which tends to hold in the most cases of problem with initial overlap is larger than zero, the number of available basis after the truncation increases as a logarithmic function of sampling number. 
 % (Note that plot has log-scaled y axis, and the order of fluctuations of a certain sampling number remain at the constant level.)
 \fi

% Next, because the empirical evidence for the deviation from $\tilde{\lambda}_i$ in Fig.~\ref{fig:S_eigenvalue_perturbation} supports the validity of the inequality in Eq. \eqref{ineq:trunc_pt} implying that Eq. \eqref{eq:opt_epsilon} can be set as the optimal threshold to address ill-conditioning, the threshold value for basis truncation is verified directly analyzing the errors of the evaluated energies.
% Based on the fact that $|\tilde{\lambda}_i - \lambda_i|$ discrepancy becomes large from the level of $\epsilon$ in Eq. \eqref{eq:opt_epsilon}, the basis with singular values less than $\epsilon$ can be thresholded.
 %Moreover, relative difference 에 대해서도 조사를 했다. optimal한 truncation point가 어디인지 알기 위해
 In order to verify the optimality of such thresholding, we investigate the eigenenergy error for different thresholding points.
 Fig.~\ref{fig:sol_pert} illustrates the relative difference between the perturbed Krylov and exact ground energies as a function of $n^{(\mathrm{t})}$, the dimensions of the remaining basis after thresholding. %, and the effects of different threshold choices on the solution. 
 As shown in Fig.~\ref{fig:sol_pert}, the error of the perturbed Krylov solution $|\tilde{E}^{(n\rightarrow n^{(\mathrm{t})})}_0 - E_0|$ starts to diverge at a certain point because of ill-conditioning, whereas the ideal Krylov solution error, $|E^{(n\rightarrow n^{(\mathrm{t})})}_0 - E_0|$ (the black dashed line), consistently decreases as the number of effective bases $n^{(\mathrm{t})}$ increases.
 Furthermore, we validate that if one uses more samplings $M$, the divergence of the solution error appears at a larger $n^{(\mathrm{t})}$ number.
 This divergent point can be estimated closely using the threshold in Eq. \eqref{eq:opt_epsilon}, as represented by the larger marker in Fig.~\ref{fig:sol_pert}.
 Note that while such points served as the optimal thresholds for most Toeplitz construction cases, the non-Toeplitz construction has tolerable corresponding energy differences, even though some threshold points deviate by one or two from the optimal values in Fig.~\ref{fig:sol_pert}.
 \iffalse
 {\color{red} Unfortunately, even with the optimal truncation, it is shown that the problem is prone to noise and a significant amount of samplings (order of $10^{16} \sim 10^{18}$) are required for the perturbation to coincide to the ideal one by more than half number of original basis.}
 \fi

% Figure environment removed

 \subsection{Final Perturbations and Their Bounds}\label{sec:final_pert_bd}
 
 \ifthenelse{\boolean{twocolumn}}{%\onecolumngrid
 }{}
 % Figure environment removed
 \ifthenelse{\boolean{twocolumn}}{%\twocolumngrid
 }{}
 Based on the optimal threshold described in Section \ref{sec:numerical_truncation}, we depicts the error results and the Krylov order $n$ in Fig.~\ref{fig:sol_pert_opt_trunc}.
 The eigenenergy errors tend to decrease as the Krylov order increases, particularly with a larger sampling number. This is because for large $n$, an effective basis of higher dimension can be utilized, and small sampling errors give rise to fewer basis vectors being truncated out.
 %However, some of the plots in Fig.~\ref{fig:sol_pert_opt_trunc} show local peaks because of the sampling perturbations caused by a sudden large condition number $d_{0}^{-1}$ described in the Fig.~\ref{fig:sampling_perturbation}.
 In addition, the non-Toeplitz construction results show slightly more errors than the Toeplitz construction results.
 This disparity obviously occurs because the non-Toeplitz construction suffers more sampling errors as the number of samplings becomes more distributed.
 The error difference between two constructions is based on the fact that the non-Toeplitz construction requires to obtain $O(n^2)$ elements instead of $O(n)$ in the Toeplitz case.
 Although Fig.~\ref{fig:sol_pert_opt_trunc} indicates that the Toeplitz construction with large $n$ is advantageous, note that this result does not included the Trotterization error which degrades the performance of Toeplitz constructions and increases as simulation time $n\Delta_t$ becomes larger.
 In practice, to mitigate the Trotterization error sufficiently small and take advantage of Toeplitz construction, the circuit depth represented in Eq. \eqref{eq:min_ckt_depth} is required.
 % Although the selection of a large $n$ and the Toeplitz construction is advantageous, the Hamiltonian simulation should be more accurate than the non-Toeplitz construction and should be executed for a longer time as $n$ increases.
 
 % Figure environment removed

 Fig.~\ref{fig:sampling_perturbation} shows the ensemble results, indicating the effects of sampling and the bounds described in Eq. \eqref{eq:gep_theorem_result_sampling_noise}.
 The perturbations follow the tendency of the condition number $d_0^{-1}$, plotted together with the shifted log scale.
 After the condition number initially increases exponentially with the Krylov basis, it decreases or becomes saturated at a certain level.
 Although the condition number largely affects the perturbations, obtaining analytical and prior $d_0^{-1}$ information remains challenging.
 In addition, the speculation about sampling perturbations (Theorem \ref{theorem:main_result}) precisely estimates the upper bound for the effect of finite sampling unless the condition number is so large that the $\sin^{-1}$ in Eq. \eqref{eq:gep_theorem_result_sampling_noise} cannot be evaluated.
 % Thus, since our speculations of the sampling of the sampling perturbation fitted within one or two orders unless the condition number was so large that the $\sin^{-1}$ in Eq.\eqref{eq:gep_theorem_result_sampling_noise} could not be evaluated, Theorem \ref{theorem:main_result} precisely estimated the upper bound for the effect of finite sampling on the calculated solution.

 

\section{Conclusion}\label{sec:conclusion}
 Based on random matrix theory, we have developed a theoretical framework to analyze the validity of QKSD regarding sampling noise using a nonasymptotic bound.
 The analysis involves an accurate modeling of sampling errors and an optimal shot distribution for each configuration, minimizing the overall sampling variance.
 We also derived a theorem that precisely estimates finite sampling errors when the condition numbers are small, which was verified by numerical simulations of the 1D Fermi-Hubbard model.
 Finally, finite sampling generates eigenenergy noises by $O(n^z \sqrt{\log n} /(d_0 \sqrt{M}))$, where $z=2$ for the Toeplitz construction, and $z=5/2$ for the non-Toeplitz construction.
 This is because shots are distributed to obtain more independent elements ($O(n^2)$) in non-Toeplitz than the Toeplitz case ($O(n)$).
 The choice of construction method depends on the availability of an accurate simulation of $e^{-i\hat{H}t}$ with an error smaller than the sampling error.
 If such a simulation is unavailable, one can use a non-Toeplitz basis instead, which exacerbates the sampling complexities, to avoid simulation errors.
 
 Furthermore, we presented a near-optimal criteria for basis thresholding to manage large condition numbers by quantifying the sampling errors in the overlap matrix.
 This criteria is applied to determine the stable basis of the overlap matrix when solving the GEVP in the QKSD method.
 The numerical experiments have showed that the error is mostly minimized with the threshold that we presented. 
 Unfortunately, the condition number tended to increase exponentially with the Krylov order, which leads to amplify the noise significantly, even when basis thresholding was applied. 
 Because some noises, such as finite sampling and the Trotterization error, are often related to time complexity, the condition number determines the slope of the trade-off between the runtime and the accuracy of the algorithm.
 Consequently, although the condition number of QKSD was not described analytically in this study, it could potentially be a critical factor that limits performance and should be investigated in the future studies.

 Although several researchers have utilized other strategies to mitigate this numerical instability, fully overcoming this problem remains a challenge.
 The first strategy is that, if one uses orthogonalization, such as the Gram-Schmidt process, the condition number of $\bm{S}$ ($\mathrm{cond}(\bm{S}) =\|\bm{S}\|\|\bm{S}^{-1}\|$) becomes unity, making the problem restore the numerical stability.
 However, stating a new orthonormal basis as a linear combination of the Krylov basis involves the calculation of $\tilde{\bm{\Lambda}}^{-1/2}$, where $\tilde{\bm{\Lambda}}$ is a diagonal matrix whose elements are singular values of $\tilde{\bm{S}}$.
 This situation is the same as generalized eigenvalue perturbation; thus, orthogonalization suffers from the problem of the condition number as well.
 Another approach, the Lanczos method \cite[Appendix F]{quantum_power_method}, utilizes an orthonormal basis to make $\bm{H}$ tridiagonal.
 Although the eigenvalue perturbation is small, this approach depends on the Hamiltonian moments, $\braket{\hat{H}^k}$, whose efficient calculation method with a tolerable additive error is still unknown.
 In addition to approaches based on orthogonal bases, different forms of the Krylov basis can be used to construct a numerically stable overlap matrix $\bm{S}$ whose column vectors are linearly independent.
 %Such subspace has an overlap matrix $\bm{S}$ with highly independent column vectors.
 As an example of a different Krylov basis, the QPM highlights that the linear dependency among the bases generated by the Hamiltonian power tends to be lower than that of the time evolution basis \cite[Appendix~A]{quantum_power_method}.
 However, this basis is susceptible to noise amplification when it is realized in the quantum circuit, which can pose practical challenges when the norm of the Hamiltonian is large. 
\iffalse
 Although QKSD with unitary time propagation suffers from a large condition number, the theoretical development in this study can still be generalized to other frameworks.
 For example, QPM is written as a linear combination of time evolution unitaries, which independent Hadamard tests can evaluate.
 Furthermore, because the adopted random matrix theory \cite[Theorem~5.6]{MAL-048} can treat such linear random variable summations, one should be able to analyze the sampling errors of the QKSD algorithm within such subspaces.
 Overall, the findings of this study are useful for present and future applications.
\fi

Many recent quantum-classical hybrid algorithms incorporate the estimation of the overlap to construct overlap matrices and utilize the classical method to solve optimization problems such as the GEVP and differential equations.
%\cite{PhysRevResearch.3.043212, PhysRevA.104.042418}
If the computation of inverse matrices is involved in the classical method, it is highly probable that no one can avoid the finite sampling error in coping with the large condition number.  
%In this type of problem, a meticulous analysis of errors stemming from finite sampling, as we have conducted in this study, is undoubtedly necessary. 
We thus expect our analysis to provide great impetus to other similar works that require a meticulous analysis of errors stemming from finite sampling.


\begin{acknowledgments}
This work was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF), funded by the Ministry of Education, Science and Technology (NRF-2021M3E4A1038308, NRF-2021M3H3A1038085, NRF-2022M3H3A106307411, NRF-2023M3K5A1094805, NRF-2023M3K5A1094813).
\end{acknowledgments}

\appendix
\section{Anticommutation (Unitary) Grouping}
\label{sec:appendix_hamiltonian_partitioning}
 The qubit Hamiltonian $\hat{H}$ is written as
 \begin{equation}
 \label{eq:pauli_ham}
     \hat{H} = \sum_{k=1}^{N_{P}} \alpha_k \hat{P}_k,
 \end{equation}
 where $\hat{P}_k\in\{\hat{I}, \hat{\sigma_x}, \hat{\sigma_y}, \hat{\sigma_z}\}^{\otimes N_{q}}$ is a Pauli string acting on $N_q$ qubits, and $\alpha_k\in\mathbb{R}$ is the corresponding coefficient.
 Because the number of Pauli strings, $N_P$, considered in the Hamiltonian is often considerably less than its maximum possibilities, $4^{N_q}$, the time propagation unitary can be efficiently realized.
 
 Izmaylov et al. \cite{unitary_partitioning} proposed a method to construct an anticommutation grouping that represents the Hamiltonian as a sum of positively weighted unitaries:
 \begin{equation}
     \hat{H} = \sum_{j=1}^{N_\beta} \beta_j \hat{U}_j.
 \end{equation}
 
 In such partitioning, each of the $N_\beta$ partitions, $\hat{U}_j$, that contain Pauli strings of a set $\mathcal{S}(j)$,
 \begin{equation}
     \hat{U}_j = \sum_{k\in\mathcal{S}(j)} \alpha^\prime_k \hat{P}_k,
 \end{equation}
 becomes unitary if the following conditions hold:
 \begin{alignat*}{3}
  \mathrm{Im} (\alpha^{\prime*}_k \alpha^\prime_l)&=0  \qquad &\forall k, l &\in \mathcal{S}(j),\\
  \{\hat{P}_k, \hat{P}_l\}&=2\delta_{k,l} \hat{I}^{\otimes N_q} \qquad &\forall k, l &\in \mathcal{S}(j),\\
  \sum_{k\in\mathcal{S}(j)}\left|\alpha^{\prime}_k\right|^2 &= 1.
 \end{alignat*}

 Here, the positive coefficients, $\beta_j=(\sum_{k\in\mathcal{S}(j)}\alpha_k^2)^{1/2}$, recover the normalization of $\alpha^{\prime}_k$ to $\alpha_k$.
 The partitioning strategy to minimize the number of partitions, $N_\beta$, is equivalent to the minimum clique cover problem ($\texttt{MCC}$) of a graph whose nodes of Pauli words $\hat{P}_k$ are connected to each other if two Pauli words anticommute.
 Not only is the $\texttt{MCC}$ an NP-hard problem, but it is also not equivalent to minimizing the sampling error, as described in Appendix \ref{sec:hamiltonian_overlap}.
 Instead, it can be heuristically minimized with the $\texttt{SORTED INSERTION}$ algorithm \cite{efficient_quantum_measurement}, which is employed as the default technique throughout this study to determine the partitioning.
 
 Applying unitary partitioning with the Hadamard test, for example, $\bra{\phi_1}\hat{H}\ket{\phi_2} = \sum_j\beta_j\bra{\phi_1}\hat{U}_j\ket{\phi_2}$ for some states $\ket{\phi_1}$ and $\ket{\phi_2}$, we then estimate the overlaps for Krylov matrix elements.
 Any unitary grouping other than individual Pauli grouping reduces the sampling variances from $O(\|\hat{H}\|_\alpha^2/M)$ to $O(\|\hat{H}\|_\beta^2/M)$, with $\|\hat{H}\|_\alpha:=\sum_k{|\alpha_k|} \ge \|\hat{H}\|_\beta:=\sum_j{\beta_j}$ and the sampling number $M$.

 Although one can use other algorithms for overlap estimation, such as the extended swap test \cite{quantum_filter_method}, the process and results of the analysis in this study can be applied in a similar way. However the anticommutation grouping is replaced by commutation grouping, which corresponds to solving the clique covering problem of the complement graph of the anticommutation graph.

%\appendix
 \section{Generalized Eigenvalue Perturbations With Thresholding}
 \label{sec:appendix_generalized_eignevalue_perturbation}
 
 Here, we review another version of Theorem \ref{theorem:generalized_eigenvalue_perturbation} using previous results \cite[Theorem~2.7]{Theory_QSDK} considering thresholding.
 
 \begin{theorem}[Generalized Eigenvalue Perturbations with thresholding, {\cite[Theorem~2.7]{Theory_QSDK}}]
  For perturbed pair $(\tilde{\bm{H}}, \tilde{\bm{S}})=(\bm{H}+\bm{\Delta_H}, \bm{S}+\bm{\Delta_S})$ with size $n\times n$, let $\tilde{\bm{S}}=\tilde{\bm{V}}^{\dagger}\bm{\tilde{\Lambda}_S}\tilde{\bm{V}}$ be diagonalized by $\tilde{\bm{V}}$, where $\bm{\tilde{\Lambda}_S}=\mathrm{diag}\{\tilde{\lambda}_1, \cdots \tilde{\lambda}_{n}\}$ is ordered by nonincreasing $\tilde{\lambda}$'s. Based on threshold parameter $\epsilon>0$, the column vectors in $\tilde{\bm{V}}$ are cut off if the corresponding singular values of $\tilde{\lambda}$ are less than $\epsilon$, resulting in a truncated $n\times {n_\epsilon}$ matrix $\bm{\tilde{V}}_{>\epsilon}$, where ${n_\epsilon}$ is the number of remaining column vectors, i.e., $\tilde{\lambda}_{{n_\epsilon}}\ge\epsilon\ge\tilde{\lambda}_{{n_\epsilon}+1}$. Similarly, let $\bm{\Lambda_S}=\mathrm{diag}\{\lambda_1, \cdots \lambda_n\}$ and $\bm{V}_{>\epsilon}$ be the singular values and the truncated basis matrices obtained from the unperturbed pair $(\bm{H}, \bm{S})$.
  Consider another pair $(\bm{A}, \bm{B})=(\bm{V}_{>\epsilon}^{\dagger}\bm{H}\bm{V}_{>\epsilon},\bm{V}_{>\epsilon}^{\dagger}\bm{S}\bm{V}_{>\epsilon})$ as the thresholded pair and its perturbed pair $(\tilde{\bm{A}}, \tilde{\bm{B}})$ obtained from $(\tilde{\bm{H}}, \tilde{\bm{S}})$ with conjugation by $\tilde{\bm{V}}_{>\epsilon}$.
  Determine if the following three assumptions hold:
  \begin{enumerate}[leftmargin=\assumptionmargin,label=\assumptionlabel \arabic* ]
      \item Pair $(\bm{H}, \bm{S})$ satisfies the following geometric mean bound for some parameters $\mu>0$ and $0\le\alpha\le1/2$,
      \ifthenelse{\boolean{twocolumn}}
      {
      \begin{equation}
          \left| \bm{v}_i^{\dagger} \bm{H} \bm{v}_j \right| \le \mu \min(\lambda_i, \lambda_j)^{1-\alpha}\max(\lambda_i, \lambda_j)^{\alpha} 
      \end{equation}
      for all $1\le i,j\le n$,
      }{
      \begin{equation}
          \left| \bm{v}_i^{\dagger} \bm{H} \bm{v}_j \right| \le \mu \min(\lambda_i, \lambda_j)^{1-\alpha}\max(\lambda_i, \lambda_j)^{\alpha} \qquad\text{for all }\quad 1\le i,j\le n,
      \end{equation}
      }
      where $\bm{v}_i$ is the $i^{\mathrm{th}}$ eigenvector of $\bm{S}$.\label{a:1_Theory_QSDK}
      \item The largest singular value of matrix $\bm{S}$ that is smaller than the threshold, denoted as $\lambda_{n_\epsilon}$, and the next singular value, denoted as $\lambda_{{n_\epsilon}+1}$, are sufficiently separated such that the following inequality holds:
      \begin{equation}
          \lambda_{{n_\epsilon}+1}+\|\bm{\Delta_S}\|\le \epsilon <(1+\rho)\epsilon \le \lambda_{{n_\epsilon}}
      \end{equation}
      for some $\rho > 0$.\label{a:2_Theory_QSDK}
      \item Noise $\|\bm{\Delta_S}\|$ is sufficiently small such that $(1+\rho^{-1})\|\bm{\Delta_S}\|/\epsilon\le 1$.\label{a:3_Theory_QSDK}
  \end{enumerate}

  If these hold, then the perturbation on $(\bm{A}, \bm{B})$ is bounded as 
  \ifthenelse{\boolean{twocolumn}}{
  \begin{multline}
  \label{eq:precise_def_chi}
      \chi := \sqrt{\|\bm{W}^\dagger\tilde{\bm{A}}\bm{W} - \bm{A}\|^2 + \|\bm{W}^\dagger\tilde{\bm{B}}\bm{W} - \bm{B}\|^2} \\ \le 3(2+\mu)n^3(1+\rho^{-1})\left(\frac{\|\bm{S}\|}{\epsilon}\right)^\alpha \|\bm{\Delta_S}\| + \|\bm{\Delta_H}\|
  \end{multline}
  }{
  \begin{equation}
  \label{eq:precise_def_chi}
      \chi := \sqrt{\|\bm{W}^\dagger\tilde{\bm{A}}\bm{W} - \bm{A}\|^2 + \|\bm{W}^\dagger\tilde{\bm{B}}\bm{W} - \bm{B}\|^2} \le 3(2+\mu)n^3(1+\rho^{-1})\left(\frac{\|\bm{S}\|}{\epsilon}\right)^\alpha \|\bm{\Delta_S}\| + \|\bm{\Delta_H}\|
  \end{equation}
  }
  for conjugation, $\bm{W}:=\tilde{\bm{V}}_{>\epsilon}^\dagger\bm{V}_{>\epsilon}$.
  
  Furthermore, let $E^{(n)}_0$ and $E^{(n)}_1$ be the ground and first excited eigenvalues of the pair $(\bm{A}, \bm{B})$. Suppose the following assumptions:
  \begin{enumerate}[leftmargin=\assumptionmargin,label=\assumptionlabel \arabic* ]\addtocounter{enumi}{3}
      \item Error bound $\chi$ is sufficiently small: $\sqrt{2}{n_\epsilon}\chi \le \lambda_{n_\epsilon}.$\label{a:4_Theory_QSDK}
      \item Gap condition $|\tan^{-1}E^{(n)}_1 - \tan^{-1}E^{(n)}_0|\ge \sin^{-1}\frac{{n_\epsilon}\chi}{\lambda_m}$ holds\label{a:5_Theory_QSDK}.
  \end{enumerate}
  Then, with the condition number that corresponds to the eigenangle $\tan^{-1}E^{(n)}_0$, $d_0^{-1}$, the eigenangle recovered by the thresholding applied to $(\tilde{\bm{H}}, \tilde{\bm{S}})$ satisfies
  \begin{equation}
      \left| \tan^{-1}E^{(n)}_0 - \tan^{-1}\tilde{E}^{(n)}_0 \right| \le \sin^{-1}\frac{\sqrt{2}{n_\epsilon}\chi}{d_0}.
  \end{equation}
 
 \end{theorem}

 Based on this theorem, it was previously mentioned \cite{Theory_QSDK} that \ref{a:1_Theory_QSDK} necessarily holds if two parameters are chosen $\mu=\max|\Lambda(\bm{H}, \bm{S})| \lessapprox \|\hat{H}\|$ and $\alpha=1/2$.
 Although Eq. \eqref{eq:precise_def_chi} differs from the definition in Eq. \eqref{eq:def_chi}, Eq. \eqref{eq:precise_def_chi} can also explain the perturbation for $(\tilde{\bm{A}}, \tilde{\bm{B}})$ because the perturbed pair $(\tilde{\bm{A}}, \tilde{\bm{B}})$ and its conjugated pair, $(\bm{W}^{\dagger}\tilde{\bm{A}}\bm{W}, \bm{W}^{\dagger}\tilde{\bm{B}}\bm{W})$ are identical problems.
 Furthermore, \ref{a:4_Theory_QSDK} in \cite{Theory_QSDK} is analogous to
 \begin{equation*}
  r(\bm{\Delta_A}, \bm{\Delta_B})\|\bm{X}\|^2 < c(\bm{A},\bm{B}),
 \end{equation*}
 in another study \cite[Corollary~3.6]{Mathias2004TheDG}, where $\bm{X}$ is a matrix that diagonalizes $\bm{A}+i\bm{B}$.
 Hence, we replaced the dimensional factor, $q$ \cite[Corollary~2.2]{Theory_QSDK}, with precise definitions and bounds from the numerical radius and Crawford number $c(\bm{A}, \bm{B})$, in the original version \cite[Corollary~3.6]{Mathias2004TheDG}:
 \ifthenelse{\boolean{twocolumn}}{
 \begin{align*}
     r(\bm{\Delta_A}, \bm{\Delta_B}) :=& \max_{\substack{\bm{x}\in\mathbb{C}^n\\
     \bm{x}^\dagger\bm{x}=1}}{\left\{ \sqrt{|\bm{x}^{\dagger}\bm{\Delta_A}\bm{x}|^2+ |\bm{x}^{\dagger}\bm{\Delta_B}\bm{x}|^2 } \right\}} \\ \le& \sqrt{2}\chi
  \end{align*}
  \begin{align*}
     c(\bm{A}, \bm{B}) :=& \min_{\substack{\bm{x}\in\mathbb{C}^n\\
     \bm{x}^\dagger\bm{x}=1}}{\left\{ \sqrt{|\bm{x}^{\dagger}\bm{A}\bm{x}|^2+ |\bm{x}^{\dagger}\bm{B}\bm{x}|^2 } \right\}} 
     \\ \ge& \left\|\bm{B}^{-1}\right\|^{-1}=\lambda_{n_\epsilon}.
 \end{align*}}
 { \begin{equation*}
 \begin{split}
     r(\bm{\Delta_A}, \bm{\Delta_B}) :=& \max_{\bm{x}\in\mathbb{C}^n:\bm{x}^\dagger\bm{x}=1}{\left\{ \sqrt{|\bm{x}^{\dagger}\bm{\Delta_A}\bm{x}|^2+ |\bm{x}^{\dagger}\bm{\Delta_B}\bm{x}|^2 } \right\}} \le \sqrt{2}\chi \\
     c(\bm{A}, \bm{B}) :=& \min_{\bm{x}\in\mathbb{C}^n:\bm{x}^\dagger\bm{x}=1}{\left\{ \sqrt{|\bm{x}^{\dagger}\bm{A}\bm{x}|^2+ |\bm{x}^{\dagger}\bm{B}\bm{x}|^2 } \right\}} \ge \left\|\bm{B}^{-1}\right\|^{-1}=\lambda_{n_\epsilon}.
 \end{split}
 \end{equation*}}

%\appendix
\section{Hadamard Test Variance}
 \label{sec:appendix_hadamard_sampling_variance}
 Consider the Hadamard tests performed independently for the real and imaginary parts with $m^{(\mathrm{r})}$ and $m^{(\mathrm{i})}$ samplings, respectively.
 Moreover, under such sampling setting, let $Z=R+iI$ be an estimator for $\braket{\psi|\hat{U}|\psi}$, where $R$ represents the real part of $Z$, and $I$ represents the imaginary part.
 The expectation value of $Z$ is given by the following.
 \begin{equation*}
     \mathbb{E}[Z] = \mathbb{E}[R] + i\mathbb{E}[I] := \mathrm{Re}[\bra{\psi}\hat{U}\ket{\psi}] + i \mathrm{Im}[\bra{\psi}\hat{U}\ket{\psi}] 
 \end{equation*}
 
 If one labels the probability to find $\ket{0}$ at the measurement of ancillary qubit in the single real and imaginary settings as $p$ and $q$, respectively, the expectation values are represented as
 \begin{equation}
 \begin{split}
     \mathbb{E}[R] =& 2p - 1, \\
     \mathbb{E}[I] =& 2q - 1.
 \end{split}
 \end{equation}
 Furthermore, the random variables $R$ and $I$ are averaged Bernoulli distributions, 
 \begin{equation}
 \begin{split}
     R \sim \frac{2}{m^{(\mathrm{r})}}\mathrm{Bin}(m^{(\mathrm{r})}, p) - 1,\\
     I \sim \frac{2}{m^{(\mathrm{i})}}\mathrm{Bin}(m^{(\mathrm{i})}, q) - 1,
 \end{split}
 \end{equation}
 where $\mathrm{Bin}(m, p)$ denotes a binomial distribution with $m$ samplings and a success probability of $p$.
 Thus, assuming identical shot settings for the imaginary and real parts, $m^{(\mathrm{r})}=m^{(\mathrm{i})}=m/2$, the sampling variances become
 \begin{equation}
 \begin{split}
     \mathrm{Var}[R] &= \frac{4p(1-p)}{m/2} = \frac{2(1-\mathbb{E}[R]^2)}{m},\\
     \mathrm{Var}[I] &= \frac{4q(1-q)}{m/2} = \frac{2(1-\mathbb{E}[I]^2)}{m}.
 \end{split}
 \end{equation}
 
 Adding two then gives the variance of $Z$, owing to the independency, resulting in
 \begin{equation}
 \begin{split}
     \mathrm{Var}[Z] &= \mathrm{Var}[R] + \mathrm{Var}[I] \\ 
                   &= \frac{2}{m}\left( 2 - \mathrm{Re}[\bra{\psi}\hat{U}\ket{\psi}]^2 -\mathrm{Im}[\bra{\psi}\hat{U}\ket{\psi}]^2 \right) \\
                   &= \frac{2}{m}\left( 2 - \left|\bra{\psi}\hat{U}\ket{\psi}\right|^2 \right).
 \end{split}
 \end{equation}
 To remove the dependency of the state and operator, Haar measure averaging is applied both on $\hat{U}$ and another unitary, $\hat{V}$ such that $\ket{\psi} = \hat{V}\ket{0}$ (i.e. Fubini-Study measure averaging on $\ket{\psi}$) as below:
 \begin{equation}
     \mathbb{E}_{\hat{U},\hat{V}\sim \mathcal{H}(\mathcal{U}_N)}\left[\left|\bra{0}\hat{V}^{\dagger}\hat{U}\hat{V}\ket{0}\right|^2\right] =
     \frac{1}{N}. \label{eq:haar_average_2}
 \end{equation}
 Here, Eq. \eqref{eq:haar_average_2} holds because of the left- and right- unitary invariant property and a previous result \cite[Corollary 1.2]{https://doi.org/10.48550/arxiv.math/0608108}.
 Therefore, the average variance becomes
 \begin{equation}
     \mathbb{E}_{\hat{U},\ket{\psi}}\left[\mathrm{Var}[Z]\right] =\frac{2}{m}\left(2 - \frac{1}{N}\right).
 \end{equation}
 
 Consequently, in the large Hilbert space ($N\to\infty$) and after averaging out the state/operator dependencies, the variance becomes as follow:
 \begin{equation}
     \mathrm{Var}[Z] \approx \frac{4}{m}, \label{eq:single_h_overlap_var}
 \end{equation}
 which results in Eq. \eqref{eq:var_mat_elem_S}.

 \subsection{Hamiltonian Overlap}
 \label{sec:hamiltonian_overlap}
 Although obtaining a matrix element of $\bm{S}$ by the Hadamard test is straightforward, as written in Eq. \eqref{eq:single_h_overlap_var}, decomposition of the Hamiltonian by a unitary sum in Eq. \eqref{eq:pauli_ham_grp} should be used to evaluate the Hamiltonian overlap $\mathbb{E}[\bm{H}_{kl}] = \bra{\phi}\hat{V}_k^\dagger \hat{H} \hat{V}_l \ket{\phi}$, using a sequence of $N_\beta$ Hadamard tests, where $\hat{V}_k\ket{\phi} = \ket{\phi_k}$ and $\hat{V}_l\ket{\phi} = \ket{\phi_l}$, as below:
 \begin{equation*}
 \begin{split}
          \bra{\phi}\hat{V}_k^\dagger \hat{H} \hat{V}_l \ket{\phi} =& \sum_{j=1}^{N_\beta} \beta_j \bra{\phi}\hat{V}_k^\dagger\hat{U}_j\hat{V}_l\ket{\phi} \\=:& \sum_{j=1}^{N_\beta} \beta_j \bra{\phi}\hat{U}'_j\ket{\phi}.
 \end{split}
 \end{equation*}
 
 By using Eq. \eqref{eq:single_h_overlap_var} again, but with a distribution of given total shots $m=\sum_j^{N_\beta}m_j$, the merged sampling variance of the Hadamard tests is given as
 \begin{equation}
     \mathrm{Var}[\bm{H}_{kl}] \approx \sum_{j=1}^{N_\beta} \frac{4\beta_j^2}{m_j},
 \end{equation}
 which can be optimized with the following Lagrange multiplier approach:
 \begin{equation}
     \mathcal{L}(\{m_j\}_{j=1}^{N_\beta}; \lambda) = \sum_{j=1}^{N_\beta} \frac{4\beta_j^2}{m_j} + \lambda\left( \sum_{j=1}^{N_\beta}m_j - m \right),
 \end{equation}
 where $\lambda$ is a Lagrangian multiplier.
 The resulting distribution of shots and optimized variance then become
 \begin{align}
     m_j &= \frac{\beta_j}{\sum_{j'} \beta_{j'}}m, \\
     \mathrm{Var}[\bm{H}_{kl}]^\star &= \frac{4(\sum_j \beta_j)^2}{m} =: \frac{4\|\hat{H}\|^2_\beta}{m}, \label{eq:ham_overlap_var}
 \end{align}
 which result in Eq. \eqref{eq:var_mat_elem_H}.

 From Eq. \eqref{eq:ham_overlap_var}, it is desirable to find the anticommutation grouping that minimizes $\|\hat{H}\|_\beta$.
 As shown elsewhere \cite{efficient_quantum_measurement}, this formalism is similar to the measurement optimization in the VQE; however, finding such a grouping is NP-hard. Therefore, a heuristic algorithm was suggested instead. Furthermore, one can easily show that any grouping strategy is better than the individual Pauli grouping ($\hat{U}_j = \hat{P}_j$) because $\sum_j \beta_j=\sum_j (\sum_{k\in\mathcal{S}(j)}\alpha_k^2)^{1/2}\le\sum_k|\alpha_k|$.

%\appendix
\section{Behavior of the Random Matrices}
\label{sec:appendix_matrix_variance_statistics}
 In this section, Theorems \ref{theorem:toeplitz_error_matrix} and \ref{theorem:non_toeplitz_error_matrix} are proved.
 However, before that, the random matrix theory is introduced based on the matrix Laplace transform with slight modifications.

 \begin{theorem}[Matrix Gaussian Series, {\cite[Theorem~4.1.1]{MAL-048}}]
 \label{theorem:matrix_gaussian_series}
  Let $\{\bm{A}_k\}$ be a fixed finite sequence of Hermitian matrices with dimensions $n$, and $\{{X}_k\}$ be an independent finite sequence of normal random variables with zero means and the variances of $\{\sigma^2_k\}$. Then, one can introduce a Gaussian matrix series,
  \begin{equation}
  \label{eq:sum_of_random_matrices}
      \bm{\Delta_Z}:=\sum_k {X}_k \bm{A}_k.
  \end{equation}
%  with the expected matrix of
%  \begin{equation}
%      \bm{Z}_\mu := \mathbb{E}[\bm{Z}].
%  \end{equation}
  Let $v(\bm{\Delta_Z})$ be the matrix variance statistic of the sum:
  \begin{equation}
  \label{eq:matrix_variance}
      v(\bm{\Delta_Z}) := \|\mathbb{E}[\bm{\Delta}_{\bm{Z}}^2]\| = \left\|\sum_k \sigma^2_k \bm{A}^2_k\right\|.
%    v(\bm{Z}) := \|\mathbb{E}[(\bm{Z}-\bm{Z}_\mu)^2]\| = \left\|\sum_k \sigma^2_k \bm{A}^2_k\right\|.
  \end{equation}
  Then, the expected norm of $\bm{\Delta_Z}$ is bounded as
  \begin{equation}
  \label{eq:expected_value_sum_of_random_matrices}
      \mathbb{E}[\|\bm{\Delta_Z}\|] \le \sqrt{2v(\bm{\Delta_Z})\log(2n)}.
%      \mathbb{E}[\|\bm{Z} - \bm{Z}_\mu\|] \le \sqrt{2v(\bm{Z})\log(2n)}.
  \end{equation}
  Furthermore, for all $t\ge 0$, the following inequality holds:
  \begin{equation}
  \label{eq:prob_sum_of_random_matrices}
      \mathbb{P}\{\|\bm{\Delta_Z}\|\ge t \} \le 2n \exp{\left(\frac{-t^2}{2v(\bm{\Delta_Z})}\right)}.
%      \mathbb{P}\{\|\bm{Z} - \bm{Z}_\mu\|\ge t \} \le 2n \exp{\left(\frac{-t^2}{2v(\bm{Z})}\right)}.
  \end{equation}
 \end{theorem}
 
 Theorem \ref{theorem:matrix_gaussian_series} is a modified version of its original version that adds the Hermitian condition and removes its unit variance condition.
 This modification is made because, if a random variable $X$ follows the standard-normal distribution $\mathcal{N}(0, 1)$, its scaled random variable $\sigma X$ would have a distribution of $\mathcal{N}(0, \sigma^2)$ and vice versa.
 Eq. \eqref{eq:expected_value_sum_of_random_matrices} provides a tight bound for the expected norm, whereas Eq. \eqref{eq:prob_sum_of_random_matrices} does not offer sufficient insight into the random matrix norm because its right-hand side is larger than 1 unless the deviation is considerably large $(t > \sqrt{2v(\bm{\Delta_Z})\log{2n}})$.
 Hence, the use of the concentration inequality has been suggested \cite[Eq. 4.1.8]{MAL-048}, treating $|\bm{\Delta_Z}|$ as a $v_{\star}$-Lipschitz function of the Gaussian random variables \cite[Theorem 5.6.]{10.1093/acprof:oso/9780199535255.002.0004}. 

 \begin{theorem}[Matrix Gaussian Series Concentration, {\cite[Theorem~5.6.]{10.1093/acprof:oso/9780199535255.002.0004}}]
 \label{theorem:matrix_gaussian_series_concentration}
    The notation from Theorem \ref{theorem:matrix_gaussian_series} is adopted, and a weak variance of $\bm{\Delta_Z}$, denoted as $v_{\star}(\bm{\Delta_Z})$, is introduced, such that
    \begin{equation}
     \begin{split}
     v_{\star}(\bm{\Delta_Z})&:=\sup_{\|\bm{u}\|=\|\bm{v}\|=1}\mathbb{E}[|\bm{u}^{\dagger}\bm{\Delta_Z}\bm{v}|^2] \\
                          &=\sup_{\|\bm{u}\|=\|\bm{v}\|=1}\sum_{k}\sigma_k^2|\bm{u}^{\dagger}\bm{A}_k\bm{v}|^2.
     \end{split}
    \end{equation}
    Then, the following concentration inequality holds because the weak variance is bounded as $v(\bm{\Delta_Z})/n\le v_{\star}(\bm{\Delta_Z}) \le v(\bm{\Delta_Z})$.
    \ifthenelse{\boolean{twocolumn}}
    {
     \begin{multline}
     \label{eq:mat_norm_tail_bound}
         \mathbb{P}\left\{\|\bm{\Delta_Z}\| \ge \mathbb{E}[\|\bm{\Delta_Z}\|] + t\right\} \\ \le 
         \exp{\left(\frac{-t^2}{2v_{\star}(\bm{\Delta_Z})}\right)}\le
         \exp{\left(\frac{-t^2}{2v(\bm{\Delta_Z})}\right)}.
     \end{multline}
    }
    {
     \begin{equation}
     \label{eq:mat_norm_tail_bound}
         \mathbb{P}\left\{\|\bm{\Delta_Z}\| \ge \mathbb{E}[\|\bm{\Delta_Z}\|] + t\right\} \le 
         \exp{\left(\frac{-t^2}{2v_{\star}(\bm{\Delta_Z})}\right)}\le
         \exp{\left(\frac{-t^2}{2v(\bm{\Delta_Z})}\right)}.
     \end{equation}}
 \end{theorem}

 Although this result alone fails to help elucidate $\mathbb{E}[\|\bm{\Delta_Z}\|]$, the tail bound centered by $\mathbb{E}[\|\bm{\Delta_Z}\|]$ is tighter than in Eq. \eqref{eq:prob_sum_of_random_matrices}.
 Therefore, Theorem \ref{theorem:matrix_gaussian_series_concentration} is used instead of Eq. \eqref{eq:prob_sum_of_random_matrices} to represent the concentration of the random matrix.
 Theorem \ref{theorem:matrix_gaussian_series_concentration} is also applied to derive the concentration inequality for $\|\bm{\Delta_Z}\|$ in Eq. \eqref{eq:mat_norm_concentration}.

 In the following subsections, Theorems \ref{theorem:toeplitz_error_matrix} and \ref{theorem:non_toeplitz_error_matrix} are proved.
 Although the efforts here are similar to previous work \cite[Chapter 4.2]{MAL-048}, two additional factors are introduced to meet the needs of the present application: the application to the complex random matrices induced by a sampling error and the optimal distribution of samplings that minimize the bound for matrix variances.
 Specifically, individual complex error matrix elements describe the sampling errors of the Hadamard test with a finite number of shots and are mean zero and variance inversely proportional to the sampling number, as shown in Appendix \ref{sec:appendix_hadamard_sampling_variance}.
 Furthermore, the total number of samplings is adequately distributed to minimize the bound for the expected norm.(see Eq. \eqref{eq:expected_value_sum_of_random_matrices}).

 \subsection{Toeplitz matrix}\label{sec:Toeplitz_mat_theory_proof}
 Here, Theorem \ref{theorem:toeplitz_error_matrix} is proved.
 \begin{proof}
 Consider a Hermitian, Toeplitz and Gaussian random matrix $\bm{\Delta_Z}$ generated by a random sequence with $2n-1$ Gaussian random variables $\{{X}^{(\mathrm{r})}_{k}\}_{k=0}^{n-1}$ and $\{{X}^{(\mathrm{i})}_{k}\}_{k=1}^{n-1}$, where the sequence with a superscript $(\mathrm{r})$ denotes the real part of the matrix elements, and that with a superscript $(\mathrm{i})$ denotes the imaginary part.
 The imaginary diagonal elements are omitted (${X}^{(\mathrm{i})}_0=0$) to satisfy the Hermitian condition.
 Thus, $\bm{\Delta_Z}$ can be written as a Gaussian matrix series that is consistent with Eq. \eqref{eq:sum_of_random_matrices},
 \ifthenelse{\boolean{twocolumn}}
 {
 \begingroup
 \allowdisplaybreaks
 \begin{align}
 \label{eq:sum_of_gaussian_matrices_toeplitz}
     \bm{\Delta_Z} = {X}_{0}^{(\mathrm{r})}\bm{I}_n + \sum_{k=1}^{n-1}
     \begin{split}
     &({X}_{k}^{(\mathrm{r})}+i{X}_{k}^{(\mathrm{i})})\bm{C}^k_1 \\
     +&({X}_{k}^{(\mathrm{r})}-i{X}_{k}^{(\mathrm{i})})\bm{C}^k_{-1},
     \end{split}
 \end{align}
 \endgroup
 }{
 \begin{equation}
 \label{eq:sum_of_gaussian_matrices_toeplitz}
     \bm{\Delta_Z} = {X}_{0}^{(\mathrm{r})}\bm{I}_n + \sum_{k=1}^{n-1}\left[({X}_{k}^{(\mathrm{r})}+i{X}_{k}^{(\mathrm{i})})\bm{C}^k_1 + ({X}_{k}^{(\mathrm{r})}-i{X}_{k}^{(\mathrm{i})})\bm{C}^k_{-1}\right],
 \end{equation}
 }
 where $\bm{I}_n$ is the $n\times n$ identity matrix, and $\bm{C}_1$ and $\bm{C}_{-1}$ are the superdiagonal and subdiagonal matrices shifted by $\pm 1$:
 \begin{equation}
     \bm{C}_1 := 
     \begin{bmatrix}
      0 & 1 & 0 & 0 & \cdots \\
      0 & 0 & 1 & 0 & \cdots \\
      0 & 0 & 0 & 1 & \cdots \\
      0 & 0 & 0 & 0 & \cdots \\
      \vdots & \vdots & \vdots & \vdots & \ddots
     \end{bmatrix}, \quad
      \bm{C}_{-1} := 
     \begin{bmatrix}
      0 & 0 & 0 & 0 & \cdots \\
      1 & 0 & 0 & 0 & \cdots \\
      0 & 1 & 0 & 0 & \cdots \\
      0 & 0 & 1 & 0 & \cdots \\
      \vdots & \vdots & \vdots & \vdots & \ddots
     \end{bmatrix}.
 \end{equation}
 In addition, for a positive integer $k$, $\bm{C}^k_1$ and $\bm{C}^k_{-1}$ are offset by $\pm k$ instead of $\pm1$. For example, their squares are
 \begin{equation}
     \bm{C}^2_1 = 
     \begin{bmatrix}
      0 & 0 & 1 & 0 & \cdots \\
      0 & 0 & 0 & 1 & \cdots \\
      0 & 0 & 0 & 0 & \cdots \\
      0 & 0 & 0 & 0 & \cdots \\
      \vdots & \vdots & \vdots & \vdots & \ddots
     \end{bmatrix}, \quad
      \bm{C}^2_{-1} =
     \begin{bmatrix}
      0 & 0 & 0 & 0 & \cdots \\
      0 & 0 & 0 & 0 & \cdots \\
      1 & 0 & 0 & 0 & \cdots \\
      0 & 1 & 0 & 0 & \cdots \\
      \vdots & \vdots & \vdots & \vdots & \ddots
     \end{bmatrix}.
 \end{equation}
 Therefore, Eq. \eqref{eq:sum_of_gaussian_matrices_toeplitz} describes a Hermitian Toeplitz matrix.
 
 Furthermore, if the variances corresponding to each random Gaussian sequence are given as $\{\sigma_{k}^{(\mathrm{r})2}\}_{k=0}^{n-1}$ and $\{\sigma_{k}^{(\mathrm{i})2}\}_{k=1}^{n-1}$, respectively, the matrix variance $v(\bm{\Delta_Z})$ is given by
 \ifthenelse{\boolean{twocolumn}}{
 \begin{widetext}
 \begin{equation}
 \label{eq:var_z_toeplitz_1}
     \begin{split}
         v(\bm{\Delta_Z}) =& \left\| \sigma_{0}^{(\mathrm{r})2}\bm{I}_n + \sum_{k=1}^{n-1}\sigma_{k}^{(\mathrm{r})2}
         (\bm{C}_1^k+\bm{C}_{-1}^k)^2 - \sigma_{k}^{(\mathrm{i})2}(\bm{C}_1^k-\bm{C}_{-1}^k)^2 \right\|\\
         =& \left\| \sigma_{0}^{(\mathrm{r})2}\bm{I}_n + \sum_{k=1}^{n-1}\sigma_{k}^{(\mathrm{r})2}(\bm{C}_1^{2k}+\bm{C}_{-1}^{2k}+\bm{D}_{k}+\bm{D}_{-k})- \sigma_{k}^{(\mathrm{i})2}(\bm{C}_1^{2k}+\bm{C}_{-1}^{2k}-\bm{D}_k-\bm{D}_{-k})\right\|,
     \end{split}
 \end{equation} 
 \end{widetext}
 }{
 \begin{equation}
 \label{eq:var_z_toeplitz_1}
     \begin{split}
         v(\bm{\Delta_Z}) =& \left\| \sigma_{0}^{(\mathrm{r})2}\bm{I}_n + \sum_{k=1}^{n-1}\sigma_{k}^{(\mathrm{r})2}(\bm{C}_1^k+\bm{C}_{-1}^k)^2 - \sigma_{k}^{(\mathrm{i})2}(\bm{C}_1^k-\bm{C}_{-1}^k)^2 \right\|\\
         =& \left\| \sigma_{0}^{(\mathrm{r})2}\bm{I}_n + \sum_{k=1}^{n-1}\sigma_{k}^{(\mathrm{r})2}(\bm{C}_1^{2k}+\bm{C}_{-1}^{2k}+\bm{D}_{k}+\bm{D}_{-k})- \sigma_{k}^{(\mathrm{i})2}(\bm{C}_1^{2k}+\bm{C}_{-1}^{2k}-\bm{D}_k-\bm{D}_{-k})\right\|,
     \end{split}
 \end{equation}
 }
 where $\bm{D}_k$ and $\bm{D}_{-k}$ are diagonal matrices with elements of $1$ for the first and last $n-k$ diagonals, respectively, and created by the product of $\bm{C}_{1}^{k}$ and $\bm{C}_{-1}^{k}$.
 For simplicity, the variances between the real and imaginary parts are then made the same, $\sigma_{k}^{(\mathrm{r})2} = \sigma_{k}^{(\mathrm{i})2} =: \sigma_k^2/2$, canceling the $\bm{C}_{\pm 1}$ matrices.
 Consequently, Eq. \eqref{eq:var_z_toeplitz_1} is rewritten as
 \ifthenelse{\boolean{twocolumn}}
 {\begin{equation}\label{eq:toeplitz_diag}
 \begin{split}
    v(\bm{\Delta_Z}) =& \left\| \sigma_0^2 \bm{I}_n + \sum_{k=1}^{n-1} \sigma_k^2(\bm{D}_k + \bm{D}_{-k})\right \| \\
    =& \left\| \mathrm{diag}(\{v_l\}_{l=1\cdots n}) \right\|,     
 \end{split}
 \end{equation}}
 {\begin{equation}\label{eq:toeplitz_diag}
    v(\bm{\Delta_Z}) = \left\| \sigma_0^2 \bm{I}_n + \sum_{k=1}^{n-1} \sigma_k^2(\bm{D}_k + \bm{D}_{-k})\right \| = \left\| \mathrm{diag}(\{v_l\}_{l=1\cdots n}) \right\|,
 \end{equation}}
 where
 \begin{equation}\label{eq:toeplitz_minimax}
     v_l := \sigma_0^2 + \sum_{k=1}^{l-1}2\sigma_k^2 + \sum_{k=l}^{n-l}\sigma_k^2.
 \end{equation}
 Applying the Hadamard test error model from Appendix \ref{sec:appendix_hadamard_sampling_variance}, $\sigma_k^2$'s are expressed in terms of the number of samplings $m_k$ demonstrated as Eq. \eqref{eq:single_h_overlap_var} or Eq. \eqref{eq:ham_overlap_var}, depending on $\bm{Z}$.
 Subsequently, the variances in  Eq. \eqref{eq:toeplitz_minimax} are replaced by the numbers of shots approximated as positive real numbers, $\bm{m}=(m_0, m_1, \cdots m_{n-1})\in\mathbb{R}^{n}_{+}$, resulting in
 \begin{equation}\label{eq:toeplitz_minimax_shot}
     v_l(\bm{m}) = 2V_{\bm{Z}}^2\left(\frac{1}{m_0} + \sum_{k=1}^{l-1}\frac{4}{m_k} + \sum_{k=l}^{n-l}\frac{2}{m_k}\right).
 \end{equation}
 Here, the prefactor $V_{\bm{Z}}$ is determined as $V_{\bm{H}}=\|H\|_\beta$ and $V_{\bm{S}}=1$, which is analogous to Eqs.\eqref{eq:single_h_overlap_var} and \eqref{eq:ham_overlap_var}.
 In addition, because the diagonal elements are real, the halved expression $\sigma_0^2=2V_{\bm{Z}}^2/m_0$ is used.
 
 Moreover, each $v_l(\bm{m})$ is a convex function and has a minimum at $\bm{m}^{(l)}$ with the constraint of the total number of shots($\sum_k m_k = M_{\bm{Z}}$) as shown below:
 \begin{equation}\label{eq:minimax_opt_val}
     v_l(\bm{m}^{(l)}) = \frac{2V_{\bm{Z}}^2}{M_{\bm{Z}}}\left(\sqrt{2}(n+1)-1-2(\sqrt{2}-1)l\right)^2,
 \end{equation}
 \ifthenelse{\boolean{twocolumn}}{
 \begin{multline}\label{eq:minimax_opt_shot}
     \bm{m}^{(l)} = (m_0^{(l)}, \underbrace{2m_0^{(l)}, \cdots, 2m_0^{(l)}}_{l-1}, \\ \underbrace{\sqrt{2}m_0^{(l)}, \cdots, \sqrt{2}m_0^{(l)}}_{n-2l+1}, \underbrace{0, \cdots 0}_{l-1}),
 \end{multline}}{
 \begin{equation}\label{eq:minimax_opt_shot}
     \bm{m}^{(l)} = (m_0^{(l)}, \underbrace{2m_0^{(l)}, \cdots, 2m_0^{(l)}}_{l-1}, \underbrace{\sqrt{2}m_0^{(l)}, \cdots, \sqrt{2}m_0^{(l)}}_{n-2l+1}, \underbrace{0, \cdots 0}_{l-1}),
 \end{equation}}
 where $m_0^{(l)}=M_{\bm{Z}}(\sqrt{2}(n+1)-1-2(\sqrt{2}-1)l)^{-1}$ satisfies the constraint.
 Here, $v_{n-l}(\bm{m})=v_l(\bm{m})$.
 Furthermore, for all $l\in \{2\cdots \lceil {n/2}\rceil\}$, $v_l(\bm{m})$ intersects with $v_1(\bm{m})$ at the point of $\bm{m}^{(1)}$,
 \begin{equation}\label{eq:minimax_help}
     v_l(\bm{m}^{(1)}) = v_1(\bm{m}^{(1)}),
 \end{equation}
 which can be checked by inserting Eq. \eqref{eq:minimax_opt_shot} with $l=1$ to Eq. \eqref{eq:toeplitz_minimax_shot}.
 
 Because the spectral norm of the diagonal matrix in Eq. \eqref{eq:toeplitz_diag} is the maximum absolute value of the elements, the following expression holds:
 \begin{equation}
     v(\bm{\Delta_Z}(\bm{m})) = \max_{l\in\{1\cdots \lceil {n/2} \rceil\}}{v_l(\bm{m})}.
 \end{equation}
 The residual matrix, $\bm{\Delta_Z}$, is constructed with the distribution of shots of $\bm{m}$. 
 Moreover, the minimization of $v(\bm{\Delta_Z})$ with respect to $\bm{m}$ leads to the minimum bound for the expected error matrix norm (see Eq. \eqref{eq:expected_value_sum_of_random_matrices}) and a minimax problem over the distribution of shots and the set of functions $v_l$,
 \begin{equation}
     v^{\mathrm{(opt)}}(\bm{\Delta_Z})=\min_{\bm{m}\in \mathcal{M}}\max_{l\in\mathcal{L}} v_l(\bm{m}),
 \end{equation}
 where $\mathcal{M}=\{\bm{m}\in\mathbb{R}^{+n}:\sum_k m_k = M_{\bm{Z}}\}$ and $\mathcal{L}=\{1,\cdots,\lceil n/2 \rceil\}$.
 To solve the minimax problem, we define $w(\bm{m}):=\max_{l\in\mathcal{L}}v_l(\bm{m})$ whose lower bound is
 \begin{equation}\label{eq:minimax_pf_0}
     w(\bm{m}) \ge v_1(\bm{m}) \ge v_1(\bm{m}^{(1)}).
 \end{equation}
 We can also verify that 
 \begin{align}
     w(\bm{m}^{(1)})&=\max \{v_l(\bm{m}^{(1)}):l\in\mathcal{L}\} \label{eq:minimax_pf_1}\\
                        %&=\max \{v_1(\bm{m}^{(1)})\} \label{eq:minimax_pf_2}\\
                        &=v_1(\bm{m}^{(1)})\label{eq:minimax_pf_2},
 \end{align}
 where Eq. \eqref{eq:minimax_pf_2} holds because Eq. \eqref{eq:minimax_help} is satisfied.
 Finally, Eqs. \eqref{eq:minimax_pf_0} and \eqref{eq:minimax_pf_2} imply that
 \begin{equation}
    v^{\mathrm{(opt)}}(\bm{\Delta_Z})=v_1(\bm{m}^{(1)}).
 \end{equation}
 The corresponding optimal point is derived by substituting $l=1$ into Eqs. \eqref{eq:minimax_opt_val} and \eqref{eq:minimax_opt_shot}, resulting in
 \begin{align}
     m_{0} &= m_{0}^{(\mathrm{r})} = \frac{M_{\bm{Z}}}{\sqrt{2}(n-1)+1},\\
     m_{k>0} &= m_{k>0}^{(\mathrm{r})} + m_{k>0}^{(\mathrm{i})} = \frac{\sqrt{2}M_{\bm{Z}}}{\sqrt{2}(n-1)+1}=\sqrt{2}m_0,
 \end{align}
 \begin{align}
     v^{\mathrm{(opt)}}(\bm{\Delta_Z}) &= \frac{2V_{\bm{Z}}^2}{M_{\bm{Z}}}(1+(n-1)\sqrt{2})^2\approx\frac{4V_{\bm{Z}}^2n^2}{M_{\bm{Z}}},
 \end{align}
 which is applied to Eq. \eqref{eq:expected_value_sum_of_random_matrices} to produce Eq. \eqref{eq:toeplitz_error_matrix_norm_bound}. 
 \end{proof}

 The results show that the off-diagonal terms are sampled $\sqrt{2}$ times more than the diagonal ones.
 The observation includes an additional factor $n$ for the matrix variance statistic found in a previous report \cite[Chapter 4.4]{MAL-048}, which accounts for the increased sampling errors as $n$ increases, if the total number of samplings is limited.

 \subsection{Non-Toeplitz matrix}\label{sec:Non_Toeplitz_mat_theory_proof}
 In a manner similar to that in Appendix \ref{sec:Toeplitz_mat_theory_proof}, Theorem \ref{theorem:non_toeplitz_error_matrix} is proved.

 \begin{proof}
 For the non-Toeplitz Gaussian random matrix $\bm{\Delta_H}$ with a size of $n\times n$, imaginary parts are added to Theorem \ref{theorem:matrix_gaussian_series} by specifying the random Hermitian matrices as
 \ifthenelse{\boolean{twocolumn}}
 {\begin{equation}
 \begin{split}
  \bm{\Delta_H} =& \sum_{1\le k<l\le n} \left\{{X}_{k, l}^{(\mathrm{r})} (\bm{E}_{kl}+\bm{E}_{lk}) + i{X}_{k, l}^{(\mathrm{i})} (\bm{E}_{kl}-\bm{E}_{lk})\right\}\\ +& \sum_{1\le k \le n}{{X}_{k,k} \bm{E}_{kk}},
  \end{split}
 \end{equation}}
 {\begin{equation}
  \bm{\Delta_H} = \sum_{1\le k<l\le n} \left\{{X}_{k, l}^{(\mathrm{r})} (\bm{E}_{kl}+\bm{E}_{lk}) + i{X}_{k, l}^{(\mathrm{i})} (\bm{E}_{kl}-\bm{E}_{lk})\right\} + \sum_{1\le k \le n}{{X}_{k,k} \bm{E}_{kk}},
 \end{equation}}
 where $\bm{E}_{kl}$ is a matrix with an element of $1$ at the $(k,l)$ index and $0$ elsewhere, and ${X}_{k,l}^{(r, i)}$ is the Gaussian random variable for the real and imaginary off-diagonal element parts and the real($k=l$) diagonal elements.
 The total $n^2$ independent ${X}$'s are adopted construct the matrix. For simplicity, the variances are set to be equal to their complex counterparts, giving $\sigma^{(\mathrm{r})2}_{k,l}=\sigma^{(\mathrm{i})2}_{k,l}=\sigma^2_{k,l}/2$. Then, $v(\bm{\Delta_Z})$ is given as
 \ifthenelse{\boolean{twocolumn}}{\begin{widetext}
  \begin{align}
        v(\bm{\Delta_H}) &= \left\|\sum_{1\le k<l\le n} \{\sigma^{(\mathrm{r})2}_{k,l}(\bm{E}_{kl}+\bm{E}_{lk})^2-\sigma^{(\mathrm{i})2}_{k,l}(\bm{E}_{kl}-\bm{E}_{lk})^2\} + \sum_{1\le k \le n} \sigma^2_{k,k}\bm{E}^2_{kk}\right\|\\
        &=\left\| \sum_{1\le k < l \le n}\sigma^2_{k,l}(\bm{E}_{kk}+\bm{E}_{ll}) + \sum_{1\le k \le n}\sigma^2_{k,k}\bm{E}^2_{kk} \right\|\\
        &=\left\|\mathrm{diag}\left\{\sigma_{k,k}^2+\sum_{1\le l < k}\sigma_{(l, k)}^2+\sum_{k < l \le n}\sigma_{(k, l)}^2\right\}_{1\le k \le n}\right\|\\
        &=\max_{1\le k\le n}\left\{\sigma_{k,k}^2+\sum_{l\in [n]\setminus{\{k\}}}\sigma_{\mathrm{ord}(k,l)}^2\right\}  \label{eq:mat_var_non_toeplitz}
 \end{align}
 \end{widetext}}{
 \begin{align}
        v(\bm{\Delta_H}) &= \left\|\sum_{1\le k<l\le n} \{\sigma^{(\mathrm{r})2}_{k,l}(\bm{E}_{kl}+\bm{E}_{lk})^2-\sigma^{(\mathrm{i})2}_{k,l}(\bm{E}_{kl}-\bm{E}_{lk})^2\} + \sum_{1\le k \le n} \sigma^2_{k,k}\bm{E}^2_{kk}\right\|\\
        &=\left\| \sum_{1\le k < l \le n}\sigma^2_{k,l}(\bm{E}_{kk}+\bm{E}_{ll}) + \sum_{1\le k \le n}\sigma^2_{k,k}\bm{E}^2_{kk} \right\|\\
        &=\left\|\mathrm{diag}\left\{\sigma_{k,k}^2+\sum_{1\le l < k}\sigma_{(l, k)}^2+\sum_{k < l \le n}\sigma_{(k, l)}^2\right\}_{1\le k \le n}\right\|\\
        &=\max_{1\le k\le n}\left\{\sigma_{k,k}^2+\sum_{l\in [n]\setminus{\{k\}}}\sigma_{\mathrm{ord}(k,l)}^2\right\}  \label{eq:mat_var_non_toeplitz}
 \end{align}}
 where $\mathrm{ord}(\cdot,\cdot)$ performs index ordering to keep the summation condition ($k<l$).
 Subsequently, as we did in Appendix \ref{sec:hamiltonian_overlap}, $\sigma_{k,l}^2$ is replaced by an expression of samplings, $m_{k,l}$ (see Eq.\eqref{eq:single_h_overlap_var}):
 \ifthenelse{\boolean{twocolumn}}
  {\begin{align}
  \begin{split}
    v(\bm{\Delta_H}(\bm{m})) &= \max_{k\in [n]} 2\|\hat{H}\|_\beta^2\left( \frac{1}{m_{(k,k)}} + \sum_{l\in [n]\setminus\{k\}} \frac{2}{m_{\mathrm{ord}(k,l)}} \right) \\
    &=: \max_{k\in [n]} v_k(\bm{m}),
  \end{split}
 \end{align}}
 {\begin{equation}
     v(\bm{\Delta_H}(\bm{m})) = \max_{k\in [n]} 2\|\hat{H}\|_\beta^2\left( \frac{1}{m_{(k,k)}} + \sum_{l\in [n]\setminus\{k\}} \frac{2}{m_{\mathrm{ord}(k,l)}} \right) =: \max_{k\in [n]} v_k(\bm{m}),
 \end{equation}}
 where vector 
 \iffalse
 \begin{equation*}
 \bm{m}=(m_{(1,1)}, m_{(1,2)}, \cdots m_{(1,n)}, m_{(2,2)}, \cdots m_{(2,n)} \cdots m_{(n,n)}) \in \mathbb{R}_{+}^{n(n+1)/2}     
 \end{equation*}
 \fi
 \begin{equation*}
 \bm{m}=(m_{(1,1)}, m_{(1,2)}, \cdots m_{(n,n)}) \in \mathbb{R}_{+}^{n(n+1)/2}     
 \end{equation*}
 represents the distribution of shots to obtain the matrix element ${X}_{k,l}$.
 A different notation is used for the function $v_k(\bm{m})$ and vector $\bm{m}$ from that in Appendix \ref{sec:Toeplitz_mat_theory_proof}.
 
 The minimization of $v(\bm{\Delta_H})$ under $M_{\bm{H}}$ shots, leads to the following minimax problem:
 \begin{equation}
     v^{\mathrm{(opt)}}(\bm{\Delta_H}) = \min_{\bm{m}\in \mathcal{M}} \max_{k\in [n]} v_k(\bm{m}),
 \end{equation}
 where $\mathcal{M}=\{\bm{m}\in \mathbb{R}_+^{n(n+1)/2} : \sum_k m_k = M_{\bm{H}}\}$.
 The minimax problem is solved by introducing two functions:
 \begin{align}
     w_M(\bm{m}) :=& \max_{k\in [n]} v_k(\bm{m}),\\
     w_A(\bm{m}) :=& \frac{1}{n}\sum_{k\in[n]} v_k(\bm{m}).
 \end{align}
 Here, $w_A(\bm{m})$ is a convex function with respect to $\bm{m}$ and has a minimum point with the restriction of $\mathcal{M}$ at $\bm{m}^{(A)}$, whose elements are
 \begin{equation}
 m^{(A)}_{(k,k)}=\frac{M_{\bm{H}}}{n^2},
 \end{equation}
 and
 \begin{equation}
 m^{(A)}_{(k,l)}=\frac{2M_{\bm{H}}}{n^2},
 \end{equation}
 for all $k<l$. In addition, its minimum value is
 \begin{equation}
     w_A(\bm{m}^{(A)}) = \frac{2\|\hat{H}\|_\beta^2n^3}{M_{\bm{H}}}.
 \end{equation}
 Therefore, the following can be stated for all $\bm{m}\in \mathcal{M}$:
 \begin{equation}
     w_M(\bm{m}) \ge w_A(\bm{m}) \ge w_A(\bm{m}^{(A)}).
 \end{equation}
 Moreover, inserting $\bm{m}^{(A)}$ into $w_M(\bm{m})$ leads to
 \begin{equation}
     w_M(\bm{m}^{(A)}) = \frac{2\|\hat{H}\|_\beta^2n^3}{M_{\bm{H}}} = w_A(\bm{m}^{(A)}),
 \end{equation}
 because $v_k(\bm{m}^{(A)})$ has the same value for every $k\in [n]$.
 Therefore, the minimax problem is solved.
 Thus, the resulting optimized number of shots and $v(\bm{\Delta_H})$ are
 \begin{align}
     m_{(k,l)} &= \begin{cases}
        \frac{2M_{\bm{H}}}{n^2} & k \neq l\\
        \frac{M_{\bm{H}}}{n^2} & k = l
     \end{cases},\\
     v^{\mathrm{(opt)}}(\bm{\Delta_H}) &= \frac{2\|\hat{H}\|_\beta^2n^3}{M_{\bm{H}}}.
 \end{align}
 \end{proof}
 Here, prefactor $\|\hat{H}\|_\beta$ is determined from Appendix \ref{sec:appendix_hadamard_sampling_variance}.
 Furthermore, unlike in the Toeplitz case, the result shows that the off-diagonal terms should be sampled two times more than the diagonal terms.
 Although, the variance statistics of a Hermitian standard-normal matrix has been argued to grow with order $n$ \cite[Chapter 4.2.1]{MAL-048}, we discovered that its scale increases to $n^3$ if the sampling complexity is considered (i.e., if the total number of samplings is limited). 

%\appendix
\section{Consideration of Simulation Error}
\label{sec:appendix_consideration_of_simulation_error}
 In this section, we discuss how the simulation error that results from the approximated simulation of $e^{-i\hat{H}t}$ affects the Toeplitz construction of the matrix $\bm{H}$.
 Moreover, we propose an asymptotic criterion for circuit depth to determine a matrix construction method for $\bm{H}$ that incurs fewer total errors.
 As reviewed in the Section \ref{sec:QKSD}, the Toeplitz construction suffers from simulation errors and sampling errors, whereas the non-Toeplitz construction only suffers from sampling errors, that are larger than those of the Toeplitz construction.
 Therefore, sufficient suppression of the simulation error in the Toeplitz construction leads to a smaller overall error than in the non-Toeplitz construction.
 However, because it requires a longer circuit depth, the aim here is to determine the minimum circuit depth that makes the Toeplitz construction more advantageous.
 
 We consider a Trotterization unitary, $\hat{U}_{\mathrm{ST}}(t)$, with a time step of $\Delta_{t}^{(\mathrm{ST})}$ that simulates the time propagation of the Hamiltonian, $e^{-i\hat{H}t}$.
 The simulation error is defined as $\hat{\mathcal{E}}_{\mathrm{ST}}(t) = e^{-i\hat{H}t} - \hat{U}_{\mathrm{ST}}(t)$, whose magnitude is given as $\|\hat{\mathcal{E}}_{\mathrm{ST}}(t)\|=O(\Delta_t^{(\mathrm{ST})2})$.
 In addition, the circuit realization of $\hat{U}_{\mathrm{ST}}(t)$ has a depth of $D=O(N_{\Gamma} t / \Delta_t^{(\mathrm{ST})})$, where $N_\Gamma$ is the number of Hamiltonian fragments which can be easily diagonalized.
 Hence, the magnitude of the simulation error can be rewritten in terms of the circuit depth as
 \begin{equation}
     \|\hat{\mathcal{E}}_{\mathrm{ST}}(t)\|=O\left(\frac{N_\Gamma ^2 t^2}{D^2}\right).   
 \end{equation}

 Here, the approximated basis $\{\hat{U}_{\mathrm{ST}}(k\Delta_t)\ket{\phi_0}\}_{k=0}^{n-1}$ is used as a Krylov basis; thus, the ideal element of $\bm{H}$ becomes
 \begin{equation}
     [\bm{H}]_{kl} = \bra{\phi_0} \hat{U}_{\mathrm{ST}}^\dagger(k\Delta_t)\hat{H}\hat{U}_{\mathrm{ST}}(l\Delta_t) \ket{\phi_0}.
 \end{equation}
 The Toeplitz construction in Eq. \eqref{eq:prj_H_QKD} is possible with the help of the commutation relation, $[\hat{H}, e^{-i\hat{H}}]=0$.
 However, because $\hat{U}_{\mathrm{ST}}$ does not commute with $\hat{H}$, the Toeplitz treatment may cause an error when obtaining the matrix element of $\bm{H}$.
 The Toeplitz treatment of the element with the sampling noise leads to
 \ifthenelse{\boolean{twocolumn}}
 {\begin{widetext}
    \begin{align}
     [\tilde{\bm{H}}^{(\mathrm{t})}]_{kl}&=\bra{\phi_0} \hat{H}\hat{U}_{\mathrm{ST}}((l-k)\Delta_t) \ket{\phi_0} + [\bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}}]_{kl}\\
                                             &=[\bm{H}]_{kl}
                                             + \bra{\phi_0} [\hat{H},\hat{U}_{\mathrm{ST}}^\dagger(k\Delta_t)]\hat{U}_{\mathrm{ST}}(l\Delta_t) \ket{\phi_0} + [\bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}}]_{kl}\\
                                             &=[\bm{H}]_{kl}
                                             + \bra{\phi_0} [\hat{H},\hat{\mathcal{E}}_{\mathrm{ST}}^\dagger(k\Delta_t)]\hat{U}_{\mathrm{ST}}(l\Delta_t) \ket{\phi_0} + [\bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}}]_{kl}\\
                                             &=[\bm{H}]_{kl} + [\bm{\Delta}_{\bm{H},\mathrm{C}}^{(\mathrm{t})}]_{kl} + [\bm{\Delta}_{\bm{H},\mathrm{S}}^{(\mathrm{t})}]_{kl},
 \end{align} 
 \end{widetext}}
 {\begin{align}
     [\tilde{\bm{H}}^{(\mathrm{t})}]_{kl}&=\bra{\phi_0} \hat{H}\hat{U}_{\mathrm{ST}}((l-k)\Delta_t) \ket{\phi_0} + [\bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}}]_{kl}\\
                                             &=[\bm{H}]_{kl}
                                             + \bra{\phi_0} [\hat{H},\hat{U}_{\mathrm{ST}}^\dagger(k\Delta_t)]\hat{U}_{\mathrm{ST}}(l\Delta_t) \ket{\phi_0} + [\bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}}]_{kl}\\
                                             &=[\bm{H}]_{kl}
                                             + \bra{\phi_0} [\hat{H},\hat{\mathcal{E}}_{\mathrm{ST}}^\dagger(k\Delta_t)]\hat{U}_{\mathrm{ST}}(l\Delta_t) \ket{\phi_0} + [\bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}}]_{kl}\\
                                             &=[\bm{H}]_{kl} + [\bm{\Delta}_{\bm{H},\mathrm{C}}^{(\mathrm{t})}]_{kl} + [\bm{\Delta}_{\bm{H},\mathrm{S}}^{(\mathrm{t})}]_{kl},
 \end{align}}
 where $[\bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}}]_{kl}=O(\|\hat{H}\|_\beta \sqrt{n/M_{\bm{H}}})$ is sampling error in the Toeplitz sequence (see Eq. \eqref{eq:QKD_H_seq}), whose magnitude is derived in Appendix \ref{sec:hamiltonian_overlap}.
 In addition to the sampling error, a commutation error appears in the Toeplitz construction of $\bm{H}$, which is
 \begin{align}
     [\bm{\Delta}_{\bm{H},\mathrm{C}}^{(\mathrm{t})}]_{kl}:=&\bra{\phi_0} [\hat{H},\hat{\mathcal{E}}_{\mathrm{ST}}^\dagger(k\Delta_t)]\hat{U}_{\mathrm{ST}}(l\Delta_t) \ket{\phi_0}\\
                          =&O\left(\|\hat{\mathcal{E}}_{ST}(k\Delta_t)\| \|\hat{H}\| \right)\\
                          =&O\left( \frac{N_\Gamma^2 \|\hat{H}\| \Delta_t^2}{D^2}k^2 \right).
 \end{align}
 A bound for spectral norm of $\bm{\Delta}_{\bm{H},\mathrm{C}}^{(\mathrm{t})}$ can be derived from the Frobenius norm ($\|\bm{\Delta}_{\bm{H},\mathrm{C}}^{(\mathrm{t})}\|\le\|\bm{\Delta}_{\bm{H},\mathrm{C}}^{(\mathrm{t})}\|_\mathrm{F}$):
 \begin{equation}
      \|\bm{\Delta}_{\bm{H},\mathrm{C}}^{(\mathrm{t})}\|_\mathrm{F} = \left(\sum_{kl}[\bm{\Delta}_{\bm{H},\mathrm{C}}^{(\mathrm{t})}]_{kl}^2\right)^{1/2}=O\left( \frac{N_\Gamma^2 \|\hat{H}\| \Delta_t^2 n^3}{D^2} \right).
 \end{equation}
 
 Finally, triangular inequality is used to analyze the total error matrix norm,
 \ifthenelse{\boolean{twocolumn}}
 {\begin{align}\label{eq:toeplitz_tot_err_norm_comm}
 \begin{split}
  \|\bm{\Delta}^{(\mathrm{t})}_{\bm{H}}\|\le& \| \bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{C}} \| + \| \bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}} \| \\
  =& O\left(\frac{N_\Gamma^2 \|\hat{H}\| \Delta_t^2 n^3}{D^2} + \frac{\|\hat{H}\|_\beta n \sqrt{\log n}}{\sqrt{M_{\bm{H}}}}\right),     
 \end{split}
 \end{align}}
 {\begin{equation}\label{eq:toeplitz_tot_err_norm_comm}
  \|\bm{\Delta}^{(\mathrm{t})}_{\bm{H}}\|\le \| \bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{C}} \| + \| \bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}} \| = O\left(\frac{N_\Gamma^2 \|\hat{H}\| \Delta_t^2 n^3}{D^2} + \frac{\|\hat{H}\|_\beta n \sqrt{\log n}}{\sqrt{M_{\bm{H}}}}\right),
 \end{equation}}
 where the spectral norm of $\bm{\Delta}^{(\mathrm{t})}_{\bm{H},\mathrm{S}}$ is obtained from Theorem \ref{theorem:toeplitz_error_matrix}.
 
 Meanwhile, for the non-Toeplitz case, only the sampling error whose upper bound is revealed in Theorem \ref{theorem:non_toeplitz_error_matrix} is considered.
 Although only the upper bound is shown, we assume that the result in Theorem \ref{theorem:non_toeplitz_error_matrix} also explains the asymptotic lower bound based on the observation shown in the numerical analysis (Fig.\ref{fig:error_norm}). In other words,
 \begin{equation}\label{eq:nontoeplitz_tot_err_norm_comm}
     \|\bm{\Delta}_{\bm{H}}^{(\mathrm{nt})}\| = \Theta\left( \frac{\|\hat{H}\|_\beta n\sqrt{n\log n}}{\sqrt{M_{\bm{H}}}} \right)
 \end{equation}

 To determine which construction is advantageous, one can compare Eqs. \eqref{eq:toeplitz_tot_err_norm_comm} and \eqref{eq:nontoeplitz_tot_err_norm_comm}.
 Correspondingly, one can show an asymptotic condition for the quantum circuit depth that makes the Toeplitz construction more advantageous than the non-Toeplitz construction ($\|\bm{\Delta}_{\bm{H}}^{(\mathrm{t})}\| \le \|\bm{\Delta}_{\bm{H}}^{(\mathrm{nt})}\|$):
 \begin{equation}\label{eq:circuit_depth_threshold}
     D = \Omega\left( N_{\Gamma}\Delta_t \left(\frac{\|\hat{H}\|}{\|\hat{H}\|_\beta}\right)^{1/2} \left(\frac{n^3M_{\bm{H}}}{\log n}\right)^{1/4} \right).
 \end{equation}
 In other words, if a quantum computer offers circuits longer than Eq. \eqref{eq:circuit_depth_threshold}, the Toeplitz construction suffers less error than the non-Toeplitz construction.
 The dependency of $M_{\bm{H}}$ explains that the simulation error should be suppressed as sampling error is reduced in order to make the Toeplitz construction advantageous.
 Moreover, because the time step for the Krylov basis, $\Delta_t$, determines the maximum simulation time $(t=n\Delta_t)$, the circuit depth becomes proportional to $\Delta_t$.

% The \nocite command causes all entries in a bibliography to be printed out
% whether or not they are actually referenced in the text. This is appropriate
% for the sample file to show the different styles of references, but authors
% most likely will not want to use it.
\nocite{*}

\bibliography{apssamp}% Produces the bibliography via BibTeX.

\end{document}
%
% ****** End of file apssamp.tex ******
