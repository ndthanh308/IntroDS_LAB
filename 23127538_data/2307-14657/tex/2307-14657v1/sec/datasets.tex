\section{Dataset Collection}
\label{sec:datasets}

To conduct our experiments we collected 118,111 Windows PE32
executables, divided in four datasets, 
as summarized in Table~\ref{tbl:datasets}.
% malicious samples collected from VirusTotal~\cite{vt} and
% VirusShare~\cite{virusshare}
% benign samples collected from Chocolatey~\cite{chocolatey}.
This section describes the process for building those datasets.

% \juan{We are ignoring VirusShare since we only collected 586 samples
% from there, but we can change that}

\input{tbl/tbl_datasets}

\subsection{Malware Samples}
\label{sec:malcollect}

We collect PE malware executables from the VirusTotal (VT) feed~\cite{vtfeedapi}.
The VT feed is a real-time stream of JSON-encoded reports. 
Each report contains the analysis results of a sample submitted to VirusTotal --
including file hashes, filetype, size, and 
the detection labels assigned by a large number of antivirus (AV) engines.
%
These reports are generated both by new samples submitted by VT users, as well as by
user-requested re-analysis of files already known to VT.  Samples in the feed
can be of various file types (e.g., PE, APK, PDF),
but our collection focuses on Windows PE executables. 
%
Samples that appear in the feed can be downloaded
% through a separate request,
within 7 days from the moment they appear in the feed.

We want our dataset to be as diverse as possible in terms of the number of families,
but also to be balanced,
so that no malware family is over-represented or under-represented.
%
Our initial target was to collect 1,000 malware families
with a hundred samples each.
The threshold of 100 samples per family was chosen to have enough samples per family to performing
multi-class classification experiments,
taking into account that samples are split into
60\% training, 20\% validation, and 20\% testing.
%
However, due to the collection, filtering, and reclassification process
described below, we ended up with
% \nummalware samples belonging
\numfam families satisfying that threshold,
as shown in Table~\ref{tbl:datasets}.

To the best of our knowledge, 
this is the most diverse labeled malware dataset in terms of families
% analyzed state-of-the-art 
to date.
% used for ML family classification.
The most recent dataset was Motif~\cite{motif} with 454 families. 
While the number of families in Motif is also large, 
it is 21 times smaller than our balanced dataset with 
3,095 samples, and is unbalanced with a median of three samples per family. 
% and a standard deviation of 10.4.
Only one family in Motif reaches 100 samples and 
29\% of the families have only one sample. 
% 154 singleton families
Such a small number of samples for most families does not allow 
building an accurate multi-class classifier, 
as we will show in our evaluation.

\paragraph{Initial collection from VT feed.}
We collected reports and samples from the VT feed
for 82 non-consecutive days between August 2021 and March 2022.
% The collection takes as input the stream of reports.
We only retained reports of samples 
detected by at least one AV engine, 
and with a \trid~\cite{trid}
filetype identification field (available in the report) equal to 
`32-bit non-installer PE executable'.
%
We excluded 64-bit PE executables,
dynamic-link libraries (DLLs), and
executables generated by popular installer software (e.g., NSIS, InnoSetup).
%
These restrictions are placed by our dynamic analysis sandbox,
described in Section~\ref{sec:sandbox},
which currently does not support running
64-bit PE executables or DLLs, and does not interact with GUIs in order to
complete the installation of other programs.
%
% These limitations may introduce a small bias towards 32-bit PE malware.
However, an analysis of the whole VT feed during the 82 collection days shows that
from all malicious PE samples in the feed,
87.6\% are 32-bit executables,
8.2\% are DLLs (32-bit or 64-bit),
3.9\% are 64-bit executables,
and the remaining 0.3\% are other PE types (e.g., OCX, CPL, SCR).
% The removal of installers may introduce a small bias towards malware samples
% in comparison to Potentially Unwanted Programs (PUP) that are commonly
% distributed as installers~\cite{ppipup}.
% Despite the removal of installers our final dataset contains a significant number
% of Potentially Unwanted Programs (PUP) that are commonly
% distributed as installers~\cite{ppipup}.
% \davide{Weird sentence.. is it really important?}
%
% only
% 6.0\% of the malicious Windows PE samples are 64-bit and
% another 5.8\% are DLLs, compared to 65\% that are 32-bit.
% \platon{Quantifying installers' percentage in VT feed is a bit more tricky since
% we also rely on Yara signatures. For now I am just addressing a potential bias
% towards malware vs PUP}

The retained reports are fed to the \avclass malware labeling
tool~\cite{avclass2}, which outputs the most likely family name 
for the sample as well as a confidence factor that captures the number of 
AV engines assign that family to the sample 
(after removing duplicates due to AV engines that copy each other).
% using a plurality vote on the tokenized and cleaned labels.
% and also classifies the sample as PUA or malware based on the presence of
% PUA-related keywords (e.g., grayware, adware, pua, pup).
For each family reported by \avclass, our system downloaded
100 distinct samples.
% The collection tracks the number of samples downloaded for each family output by
% \avclass.  If the sample belongs to a family with less than 100 collected
% samples, then the sample's binary is downloaded from VT.
% Once 100 samples have been collected for a family,
% the collection for that family stops.
Each downloaded sample was then checked again to exclude any remaining non-32-bit PE
executables and installers that were missed by \trid.  In
particular, samples are removed if their PE header does not indicate 
they are 32-bit executables, or if they are detected as installers
using public Yara rules by Avast~\cite{retdecYara}.
As stated, our initial target was to collect 1,000 malware families
with 100 samples each.
However, when this target was reached, many other families had been
collected with less than 100 samples,
resulting in an initial dataset of 239,417 PE32 malware samples from 23,555 families.

\paragraph{Reclassification and family filtering.}
The AV labels of a sample may change over time
as AV vendors refine their detection rules.
These label changes may in turn change the family that \avclass
outputs for a sample.
% Since the same hash may appear multiple times in the feed (i.e., from multiple
% submissions) it may receive a different famliy label from \avclass over time.
% As such, our pipeline is designed to keep the latest family label of a sample
% and update the samples per family collected.  Once our collection finishes, we
% rerun \avclass one last time in order to get the most up-to-date family
% labels.
%Since \avclass performs a majority vote on the (tokenized and cleaned) AV
%labels, a sample could change family.
To account for such changes, we re-collect the updated VT report for our
samples 54 days after the end of our collection process,
and feed the new reports to \avclass to obtain the (possibly) updated family.
From the 239,417 samples, 9.7\% (23,171) were at this point re-classified as a
different family.
% These samples are most often labeled with two distinct family labels but a small
% percentage of those (7.2\%) are assigned up to six different family labels.
\avclass uses a taxonomy to identify a wide range of non-family tokens that may appear in the AV
labels. These include 
file properties (e.g., \vtag{FILE:packed:asprotect}, \vtag{FILE:exploit: gingerbreak}), 
malware classes (e.g., \vtag{CLASS:virus}, \vtag{CLASS:worm}),
behaviors (e.g., \vtag{BEH:ddos}, \vtag{BEH:filedelete}), 
and generic tokens (e.g., \vtag{GEN:malicious}, \vtag{GEN:behaveslike}).  However, the
\avclass taxonomy is assumed to be incomplete by design~\cite{avclass2}.  Thus,
it may output a label for a sample that does not correspond to a real family,
but rather to a previously unknown instance of the above categories.
%
To address this issue, we manually inspected the collected family labels and
conservatively filtered out any labels that may not correspond to real family names.
This step identified 86 likely non-family tokens
not in the \avclass taxonomy, such as
\vtag{gametool}, \vtag{testsample}, \vtag{nsismod}, \vtag{dllinject}, and
\vtag{processhijack}.  We also removed random-looking labels (e.g.,
\vtag{005376ae}) that \avclass failed to filter.
% We observe that those are typically due to recently added AV engines.
As a byproduct of our effort, we will contribute our extended \avclass taxonomy 
to the open-source \avclass project.
% so that they
% can be incorporated into the default taxonomy and
% benefit future users of the tool.

After reclassification and family filtering, the dataset contained
227,296 samples from 13,894 families, out of which 
780 families had at least 100 samples.
% Vt reports: 44,766,719
Thus, despite examining more than 44M VT reports over a period of 
nearly 3 months, we were unable to reach our goal of 
1,000 families with 100 samples.
% In fact, out of over 14K families that appeared in the VT feed, 
% very few included a significant number of unique samples, 
This illustrates the difficulty of building a diverse malware dataset.

% Note that due to the re-classification,
% it is possible for some families to have more than 100 samples.

%Samples per family
% Figure~\ref{fig:samples_per_fam_cumulative} shows the
% the number of \avclass families per number of samples in each family.
% From the 13,894 families,
% 780 families have at least 100 samples,
% 1,192 families have at least 50,
% and 2,943 have at least 10.


%% Figure environment removed

\paragraph{Feature filtering.}
We performed static and dynamic feature extraction (as detailed in
Section~\ref{sec:methodology}) for all samples of the 780 families with at least 100 samples.
This required executing each sample in a sandbox to obtain a behavioral report.
%
\revision{We discarded 122 samples for which the
static feature extraction pipeline failed. The failure reasons 
were corrupted headers (26 binaries), 
empty output from the disassembler 
probably due to obfuscation techniques (95 samples), and the absence of the
entry point in one binary. 
}
We also discarded samples that did not exhibit any runtime behavior,
and sub-sampled families to keep only
100 samples each. The result is a balanced dataset (hereinafter $M_B$)
that contains \nummalware samples from \numfam families.
% and is used in the  remaining of this study.
% 282 grayware
% 120 downloader
%  87 worm
%  78 backdoor
%  41 virus
%  24 ransomware
%  17 spyware
%   9 miner
%   5 tool
%   5 clicker
%   2 dialer
% We verify the family diversity of our malware dataset using 
% the malware class labels from \avclass ouptut.
According to \avclass,
those families belong to 13 malware classes:
36\% (282) of the families are classified as grayware
(including its adware subclass),
15\% (120) as downloaders,
11\% (87) as worms,
10\% (78) as backdoors,
5\% (41) as viruses,
and the remaining 23\% (62) includes ransomware, rogueware,
spyware, miners, hacking tools, clickers, and dialers.

% % Figure environment removed

% % Figure environment removed

% % Figure environment removed


% \input{tbl/tbl_avc2_fams} \input{tbl/tbl_avc2_beh} \input{tbl/tbl_avc2_class}

\paragraph{Dataset statistics.}
Over 93\% of the samples in the $M_B$ dataset 
are detected by at least 20 AV engines, 
while only 0.3\% 
% Figure~\ref{fig:vt_scores} shows the distribution of VT scores collected from
% the last report of each sample.
have a VT score less or equal to 3. 
It is worth noting that the minimum number of detections for samples in
the dataset is two since \avclass requires at least two AV engines to
assign a label to a sample.

% Freshness samples
Samples on the VT feed can be new (i.e., collected and scanned for the very
first time by VT) or resubmitted (i.e., first submitted in the past but
re-scanned on the day they were collected).
% Figure~\ref{fig:freshness_samples} shows
We compute the freshness of samples in the $M_B$ dataset
as the number of days between a sample's collection date and
its VT first seen date.
We observe that 
53.4\% of the samples were collected within a day of being first observed by VT,
7.6\% within a year,
and 37.8\% are old samples first seen over one year before our study.

%AV freshness Figure~\ref{fig:fseen_distr} shows the distibution of VT first
%seen dates of the dataset.  Insteristingly, only 32\% of the samples (62,670)
%have a first seen date inside the collection period of the dataset.
%
% \avclass groups them in 23,575 families, from which 1,009 families include at
% least 100 samples each (107,287 samples).
%
% We use \avclass tags for idenitfying the families of samples.  For each sample
% we select the most popular (by number of AV engines) tag from the family
% category.  In cases where no family tags are present we select as family the
% most popular tag from the unknown category since this most often contains a
% family tag that is unknown to the default \avclass taxonomy.  \avclass groups
% samples in 13,785 families, from which 371 families include at least 100
% samples each (72,576 samples).  Figure~\ref{fig:samples_per_fam_distr} shows
% the distribution of samples per \avclass family.  Table~\ref{tbl:avc2_fams}
% shows the top 20 families by number of samples.  Table~\ref{tbl:avc2_class}
% shows all malware classes detected by \avclass.  Table~\ref{tbl:avc2_beh}
% shows all malware behaviours detected by \avclass.  \platon{We still need to
% do manual checks on the AVC2 labels before using them as groundtruth. We
% should check for aliases and random noise that is considered family}

%% Figure environment removed

% % Figure environment removed

% \paragraph{Final dataset.} \platon{Once we freeze the dataset and close merge
% with the above since it's about the same dataset} Our final malware dataset
% comprises 102,895 distinct samples with 300,062 VT reports.


%Freshness of samples Samples on the VT feed can be new (i.e., collected and
%scanned for the very first time by VT) or resubmitted (i.e., VT has first seen
%them in the past but rescanned on the day we collected it).
%Figure~\ref{fig:freshness_samples} shows how fresh are the samples in our
%dataset by calculating the delta (in days) between the day a sample was scanned
%by VT and first seen. We observe that 36.9\% of the samples was first seen on
%the same day as we collect it from VT feed, 38.8\% within the last day, 42.6\%
%within the last week, 48.1\% within the last month, and 49.4\% within the last
%year.  \platon{It's surprising that only 37\% is really fresh (fseen on day we
%collected them).  Is this generally true for all files on the feed or we
%introduce a bias due to our filtering.  Calculate the same figure for all
%samples in the VT feed from AVCLASS-as-a-service project dataset.}

% Reports per sample Due to the nature of VT feed, a sample may be submitted
% (and scanned) multiple times within our collection period which results in
% different VT reports.  We observe that 60.7\% of the samples (i.e., SHA2) are
% submitted only once, 21.2\% twice, and the remaining 18,1\% of the samples are
% submitted from 3 to 885 times.

% Packing consists of compressing an executable in a smaller executable and later de-compressing it at runtime.
% While packers were originally introduced to save disk space, they quickly became prevalent among malware authors for their ability to generate a totally new executable for evading signature-based detectors.
% On the other hand, software protectors are more general tools that often combine several techniques, including packing, encryption, and code virtualization.
% Industry adopts software protectors for anti-piracy and intellectual property protection purposes.
% Their characteristics, however, made them viable tools for malware authors too, since they are explicitly designed to thwart reverse engineering.
% 
% It is crucial to consider the presence of these technologies because they completely change both the structure and the code of the executables, and in some cases, the features are the same for all executables that have been packaged/protected with the same tool.
% For example, the UPX packer generates a PE file with a constant number of sections (usually with the same names), hides the vast majority of imported functions and all the strings, and besides, its static code includes only the de-compressing routine.
% This fact can negatively influence machine learning algorithms that may produce an overfitted model with respect to the packer/protector's technology and therefore requires further study, which we will discuss in this section.

\paragraph{Packer and protector detection.}
To hamper analysis, malware authors may use packers that compress 
a sample and de-compress it at runtime, as well as more sophisticated 
protectors that may combine different obfuscations such as 
packing, encryption, and code virtualization.
%
To evaluate the impact of packers and other protectors on 
malware classification, 
we determine whether a sample uses an off-the-shelf 
packer or protector by using 
the signature-based Detect It Easy (DIE)~\cite{detectiteasy} tool,
as well as the well-maintained Yara rules of Avast RetDec~\cite{retdecYara}.
Overall, 22\% of the samples in $M_B$ use a packer or protector. 
The most popular packer is  
\texttt{upx} detected on 14.0\% the samples, followed by
\texttt{aspack} (3.2\%) and
\texttt{pecompact} (1.0\%).
The most popular protectors are
\texttt{vmprotect} (1.9\%) and 
\texttt{asprotect} (0.4\%).

\begin{comment} 6-10 commented, I would not mention them
  \texttt{petite} 0.4\%
  \texttt{dxpack} 0.3\%
  \texttt{themida} 0.2\%
  \texttt{armadillo} 0.2\%
  \texttt{telock} 0.2\%
\end{comment}

\subsection{Testing Datasets}
\label{sec:maltest}

We create two other disjoint malware datasets,
which we use in Section~\ref{sec:experimental} to 
% conduct specific experiments and 
test the ability of ML classifiers to generalize beyond the $M_B$ dataset 
they were built upon. 
The first dataset, referred as Malware Unbalanced (or $M_U$) in Table~\ref{tbl:datasets}, contains 18K samples from 1.5K families.
These samples were part of the initial VT feed collection, 
passed the filtering and re-classification steps, 
but their families never reached the threshold of 100 samples and 
thus were excluded from $M_B$.
All samples are detected by at least 20 AV engines and none of the samples 
nor their families are part of $M_B$.

The second dataset, Malware Generic ($M_G$), contains 16.5K samples 
for which AVClass2 was unable to output a family,
due to AV engines using only generic labels. These
samples were separately collected from the VT feed 
between June 23rd and July 6th 2022 and underwent the filtering steps 
to keep only 32-bit non-installer PE executables.
All samples are detected by at least 20 AV engines and 
none of the samples are part of $M_B$.

\subsection{Benign Samples}
\label{sec:benigncollect}

Building a benign dataset by just relying on the number of AV detections 
in the VT report is prone to errors
due to the presence of malicious files that are still unknown to AV engines.
Therefore, we took a more conservative strategy and decided to build a benign dataset
by using a fresh installation of all the community-maintained packages
of Chocolatey~\cite{chocolatey} 
(which undergo a rigorous moderation review process to avoid pollution)
in a clean machine running Windows 10.
After each package was installed, we extracted all the executable files
present on the hard disk, which may correspond to Windows system files or
third-party publishers.

We exclude files that are not 32-bit PE executables and those
with more than three detections on VT.
%\juan{Shouldn't this threshold by zero if we want to be stricter than AVs?}
This allowed us to discard borderline cases,
i.e., benign files with characteristics very similar to malware, like
hacking and scanning tools.
Using this procedure we collected a dataset $B$ of \numbenign benign samples.
The code signatures of those samples indicate a large diversity of publishers with
over 1.4K different signers -- including both small companies and 
large software publishers such as Microsoft, Oracle, and Google.

%those have: a corresponding pickle of the dynamic execution, the report shows <= 3 detections and the file is a PE32 (not 64bit, no installers and so on)

% \paragraph{Malware samples.} Statistics here are computed on the final dataset
% (after removing aliases, and families with a few number of samples)
%
% The total number of families with at least 60 samples is \textbf{1065}.
% Figure~\ref{fig:samples_per_fam_distr2} shows the cumulative distribution of
% families according to their number of available samples.
%
% \begin{itemize} \item
% 			Families with at least \textbf{100} samples: \textbf{328} \item
% 			Families with at least \textbf{95} samples: \textbf{568} \item
% 			Families with at least \textbf{90} samples: \textbf{682} \item
% 			Families with at least \textbf{80} samples: \textbf{839}
% \end{itemize}
%
% % Figure environment removed
%
% The current experiments were considering families with at least \textbf{100}
% samples (328 families) for a total of 36064 (this is not 328*100 because some
% families have more than 100 samples).
%
% The first step is to extract static features. During this process some of the
% samples may be lost because of errors. The only error is related to samples that
% are not 32-bit ('File header machine != 332'). We lose 2627 samples.  The number
% of families with at least 100 samples drops dramatically to \textbf{137}. This
% because some families have exactly 100 samples so if an error occurs there the
% family is lost. However here we introduce a threshold of +-20\%. So we consider
% in the count families with at least \textbf{80} samples.  Their number is
% \textbf{301}. This is possible because in the classification phase we shuffle
% samples between training and test and the final test set is balanced across
% families. We also use 10-fold cross validation to reduce the effect of such
% difference. Figure~\ref{fig:samples_per_fam_distr3} is a new plot that takes
% into account errors due to feature extraction.
%
% % Figure environment removed
%
% \paragraph{Packed malware samples.}
% When removing packed samples the cumulative distribution is the one showed in
% figure~\ref{fig:samples_per_fam_distr_nopacked}. When considering families with
% at least 80 samples (as before) the number drops to 204 families. That's why we
% reach 200 families.
%
% \begin{itemize}
% 	\item Families with at least \textbf{100} samples: \textbf{69}
% 	\item Families with at least \textbf{95} samples: \textbf{154}
% 	\item Families with at least \textbf{90} samples: \textbf{176}
% 	\item Families with at least \textbf{80} samples: \textbf{204}
% 	\item Families with at least \textbf{70} samples: \textbf{225}
% 	\item Families with at least \textbf{60} samples: \textbf{239}
% \end{itemize}
%
% % Figure environment removed
%
% \paragraph{Dynamic analysis}
% \textbf{Important: Originally -
% before filtering out aliases and families with very few samples -
% the whole dataset was composed of \textbf{2913} families and a total of
% \textbf{183,285} samples.} Dynamic analysis of behaviors is available for
% \textbf{82,627} samples. For those the distribution of samples per families is
% the one reported in Figure~~\ref{fig:samples_per_fam_distr_dynamicAll}.
% As for the other cases, some statistics:
%
% \begin{itemize}
% 	\item Families with at least \textbf{100} samples: \textbf{34}
% 	\item Families with at least \textbf{95} samples: \textbf{165}
% 	\item Families with at least \textbf{90} samples: \textbf{327}
% 	\item Families with at least \textbf{80} samples: \textbf{574}
% 	\item Families with at least \textbf{70} samples: \textbf{713}
% \end{itemize}
%
% % Figure environment removed
%
% If we merge the set of families with the one that have been considered for the
% static case (328 families) we have a total number of samples of \textbf{26,773} and
% the following distribution in
% Figure~\ref{fig:samples_per_fam_distr_dynamic_joined}.
% Some statistics in this case:
% \begin{itemize}
% 	\item Families with at least \textbf{100} samples: \textbf{20}
% 	\item Families with at least \textbf{95} samples: \textbf{84}
% 	\item Families with at least \textbf{90} samples: \textbf{147}
% 	\item Families with at least \textbf{80} samples: \textbf{252}
% 	\item Families with at least \textbf{70} samples: \textbf{279}
% \end{itemize}
%
% % Figure environment removed
%
% If we further consider the 204 families for which we have a reasonable number of
% samples after removing packed samples we are left with \textbf{25736} samples
% dynamically analyzed with the distribution in
% Figure~\ref{fig:samples_per_fam_distr_dynamic_joined_nopacked}.
% Statistics in this case:
% \begin{itemize}
% 	\item Families with at least \textbf{100} samples: \textbf{17}
% 	\item Families with at least \textbf{95} samples: \textbf{78}
% 	\item Families with at least \textbf{90} samples: \textbf{139}
% 	\item Families with at least \textbf{80} samples: \textbf{241}
% 	\item Families with at least \textbf{70} samples: \textbf{268}
% \end{itemize}
%
% % Figure environment removed
% \clearpage


