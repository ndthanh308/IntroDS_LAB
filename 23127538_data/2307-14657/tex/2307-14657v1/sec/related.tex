\section{Related Work}
\label{sec:related}

\input{tbl/tbl_related}

%\paragraph{ML-driven malware classification.} 
Table~\ref{tbl:related} presents a categorization of previous
works on Windows malware classification, according to their
goal (binary detection or family classification), 
features (static or dynamic), and 
dataset size (both in terms of malware executables and malware families). 
%
Among the approaches in Table~\ref{tbl:related}, 
% recent decades have witnessed mushroom growth of ML-driven 
% malware classification techniques. 
the choice of the models varies widely including classical models like 
Support Vector Machine, GradientBoost, and Random Forest,
as well as neural networks. 
%
Most approaches perform feature extraction, 
% over the raw static / dynamic analysis reports, 
e.g. extract n-grams of bytes, opcodes, or system calls, 
% and encoding them into computable features, 
but a couple of work directly operate on raw bytes and 
API sequences~\cite{malconv,neurlux}.
 
% There is lack of works that comprehensively study the applicability of 
% ML models for both binary and family classification use and 
% different feature classes. 
MalInsight~\cite{malinsight} is the only study so far to provide a comprehensive 
coverage over the choice of features and classification tasks. 
However, their dataset includes only 5 families. 
At the other end of the spectrum,
Nataraj et al.~\cite{nataraj2011comparative} studied only family classification
on an unbalanced dataset with over 500 classes. 
However, the authors consider each full AV label a different class, 
so that number does not correspond to real malware families.
In contrast, our study investigates the factors impacting the performance of 
ML classifiers using a large-scale balanced dataset with 670 families. 
% We aim to characterize the behaviours of ML-driven malware classifiers in 
% practices over different choices of malware features and malware 
% classification tasks. 

\paragraph{ML challenges and pitfalls.}
% By nature, ML delivers a data-driven predicative analysis. The goal of training a ML model aims to capture the underlying statistical correlation between input feature descriptors and predicted class labels. Therefore, statistical profiles of training / testing data plays an important role in deciding the quality of the ML-driven prediction. 
In cyber security research, two major challenges are raised in the practices of ML-based analysis. First of all, the issue of missing observations affects the prediction accuracy, e.g., in network intrusion detection~\cite{Pawlicki2021IIDS, TavabiWWW20}. Secondly, most ML models follow a core assumption: the training and test data of a ML model should be drawn identically and independently from the same underlying distribution, i.e. the I.I.D. assumption. However, the I.I.D assumption does not hold in practice. Highly diversified and quickly evolving malware technologies make the implementations and behaviours of malware vary significantly and frequently. New variants of malware arise to exploit novel vulnerabilities and evade the detection of anti-virus services. Once a machine-learning-driven malware classifier is deployed in practical security applications, the fast-changing profiles of malware samples break the I.I.D, assumption and cause the deterioration of the classification accuracy  \cite{transcending2022SP}. 
The design of a robust classifier for frequently drifting malware profiles is still an open problem. 

Arp et al.~\cite{Arp2022Usenix} review the use ML-based classification in
cyber security published over the past 10 years. The study summarizes the
existing issues at the different stages of the ML-based pipelines for cyber
security data analysis. For example, the authors demonstrate that the statistical
bias introduced by training sample sampling and inaccurate class label
tagging may introduce spurious correlations into the ML classifiers.
In addition, employing inappropriate performance metrics ignoring the class
imbalance in the testing phase may lead to incorrect interpretation to the
quality of ML-based predictive analysis. In general, according 
to~\cite{Arp2022Usenix}, the performance metrics of a ML-based analysis
pipeline in cyber security practices should be defined by considering the
characteristics of the security data collected and the requirements raised
in the concerned applications. Otherwise, the pipelines may produce
unrealistic performance and interpretations of security incidents.
In our work, we focus instead on the bottlenecks of ML-based malware classification encountered in practices, which may obstruct the accurate classification of malware. 
For instance, we focus on the
impact of the coverage of malware families for training and we dive into the
potential reasons causing failure of ML-based models over certain malware
samples. We also explore how the classifier behaves over out-of-distribution
malware samples, which is an interesting problem in the practical
deployment of ML-driven pipelines. 


%Bridging the gap between ML and malware analysis~\cite{smith2020mind}

\paragraph{Dataset construction.}
% Previous efforts ~\cite{smith2020mind,tesseract,lee2021android} introduce malware datasets used in ML-driven malware classification use. 
In 2015, the Microsoft Malware Classification Challenge
\cite{smith2020mind} was developed as a Kaggle competition to conduct
malware family classification. The corresponding dataset is composed of disassembly and
bytes of 20K Windows malware samples from 9 families. It was released in
the Kaggle competition and has since been used in several studies.
\cite{tesseract,lee2021android} built larger-scale Android malware datasets
for evaluating the performances of ML-driven classification models. More
specifically, \cite{tesseract} evaluates the spatial and temporal bias of
binary classification accuracy over 129,728 Android apps.
\cite{lee2021android} explores the variance-bias trade-off of malware
clustering on 134,698 Android apps. By comparison, our work focuses on the
measurement study of large-scale Windows malware collections. Our goal is
to characterize the applicability and limits of ML-driven malware
classifiers for practical use.  In \cite{motif}, Joyce et al. built a
multi-family dataset containing 3,095 malware samples collected from
454 families. This work offers the most diversified coverage over different
malware families in public malware datasets with manually verified labels.
Interestingly, this dataset has a highly skewed distribution over the
number of malware samples per family. Over half of the families contain
less than 5 samples per family, which poses a few-shot learning challenge
to ML-driven malware classification. 
Our study tried to mimic this distribution to assess the impact of the skewed distribution of malware samples over the accuracy of the trained ML-based classifiers.
We also compare the impact of the skewed distribution and
that of varying malware coverage regarding classification accuracy. 
The empirical study helps identify the limits of ML-based classification methods in practical malware analysis. 

%The 2015 Microsoft Malware Classification Challenge included a
%dataset, consisting of disassembly and byte-code of around
%20K malicious samples from 9 families.
%\textcolor{red}{To add: Class diversity in the classification problem and Out-of-Domain generalization of ML models}

% Joyce et al.~\cite{motif} has created the Malware 
% Open-source Threat Intelligence Family (MOTIF) dataset with 3,095 PE malware
% samples from 454 by analyzing open-source threat reports published by major
% cybersecurity organizations.

