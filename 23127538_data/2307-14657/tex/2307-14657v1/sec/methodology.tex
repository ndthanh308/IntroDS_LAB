%% Figure environment removed

\section{Methodology} \label{sec:methodology}

Our work aims to answer the 8 research questions raised in 
the introduction. 
% These questions are all closely associated with
% the applicability of ML-based malware detection (i.e., binary classification)
% and family classification (i.e., multi-class classification) in practice.
Notably, we aim to explore the performances of ML-driven malware classifiers
that use features extracted statically, dynamically, or a combination of both
with varied coverage of malware families and changed volumes of training
samples.  Developing novel ML-based malware
classification models is beyond the scope of our study. Instead, we focus on
discussing and evaluating the analysed issues using state-of-the-art ML models
for malware classification. 
\revision{As explained next, we use features presented in previous
works~\cite{aghakhani2020malware,mtnet,malinsight,AonzoUsenix2022}. 
This imposes a limitation as other features could provide better results.
}
%
%Figure~\ref{fig:pipeline} summarizes our pipeline.  such as the impact of the
%training set on the classification and the impact of the number of families for
%family classification

\input{tbl/tbl_features}

\subsection{Static Features} \label{sec:features-static}

Hojjat et al.~\cite{aghakhani2020malware} performed a literature review to
identify the static features that carry the most useful information for binary
classification. We implement their feature extraction methodology to extract the
same classes of static features.
%
\revision{Similar to Hojjat et al.~\cite{aghakhani2020malware}, 
we do not attempt to unpack the executables and
% which may require to run the executables 
perform the same feature extraction regardless of whether the files are packed or not.}

The upper half of Table~\ref{tbl:features} summarizes the static feature 
classes (prefixed by \texttt{s-}).
The \emph{s-headers} class captures 29 integer features 
(Section \ref{sec:peHeaders-peSections} in the Appendix)
from the \textit{Optional} and \textit{COFF} headers of the
executable~\cite{microsoftPE}.
%
The \emph{s-sections} class captures 590 Boolean features 
from each section in the executable (Section \ref{sec:peHeaders-peSections} in the Appendix).
%
\revision{
The \emph{s-file} features capture the file size in bytes and the whole
file Shannon entropy~\cite{lyda2007using}.}

\revision{
For the remaining 5 feature classes
% (s-dll, s-imports, s-bytegrams, s-opcodegrams, s-strings),
the exact number of features may differ from 
those reported by~\cite{aghakhani2020malware} because they undergo 
a dataset-dependent feature selection step that retains only the features that
show variability or that provide higher information gain
(IG)~\cite{quinlan1986induction}.
For instance, in \emph{s-bytegrams} and \emph{s-opcodegrams},
the selection process enumerates all values observed in the 
validation set (20\% of samples in $M_B$), 
excludes rare values appearing in less than 1\% of the samples, 
computes IG, 
% and its distribution for the remaining ones, 
uses the elbow method to identify a threshold value for IG, and
only retains features with at least that threshold IG.
%
As in~\cite{aghakhani2020malware}, for \emph{s-dll}, \emph{s-imports}, and \emph{s-strings}, 
the selection process only excludes rare values, 
but does not select an IG threshold.
% Figures~\ref{fig:IG_dlls}--\ref{fig:IG_opcodes} 
% shows the IG distribution for the other four feature classes.
} 

\revision{
The \emph{s-dll} and \emph{s-imports} class contain  
Boolean features extracted from the import table (imported libraries
in case of \emph{s-dll} and imported functions for \emph{s-imports}).
We extracted 637 unique libraries and 28,667 
functions and retained only those that appear in at least 1\% of the files 
in the validation set, reducing the number to 131 DLLs and 3,732 library functions.
%
Similarly, for the \emph{s-strings} class, we extracted 106,352,885
strings of at least 4 characters, filter those that appear in over 1\% of the
files, and kept 10,402 Boolean features capturing whether the string appears or
not in the binary.
%
The \emph{s-bytegrams} class captures the presence of selected 
4-grams, 5-grams, and 6-grams. 
As proposed in~\cite{aghakhani2020malware}, to keep memory usage manageable, 
the selection process for this feature class is performed on 1,000 randomly 
chosen files from $M_B$, instead of the full validation dataset.
From the 1,363,150,788 \emph{s-bytegrams} extracted, 
the selection retained the 13,000 features with the highest IG
(Section~\ref{sec:IG} in the Appendix).}
%
\revision{The \emph{s-opcodegrams} class captures 1-gram, 2-grams and 3-grams
from the sequence of opcodes disassembled using Capstone~\cite{capstone}.
Given an initial set of 255,812 opcode n-grams, we computed the TF-IDF
and used the elbow method on the IG distribution 
to retain the top 2,500 float features (Section~\ref{sec:IG} in the Appendix).}

% \juan{I do not think we need this paragraph} 
% \revision{We verified that the extraction procedure was successful for all 
% the 118,111k samples in our dataset by monitoring the error stream of the 
% tools used to process the binaries, namely \emph{Pe-file}~\cite{pefile} for
% \emph{s-headers, s-sections, s-dll, s-imports}, \emph{Capstone} for
% \emph{s-opcodegrams}, and \emph{GNU-Strings} for \emph{s-strings}. 
% Features in \emph{s-file} and \emph{s-bytegrams} classes were obtained 
% by reading the sample as a bytestream. 
% Furthermore, as in the work by Hojjat et % al~\cite{aghakhani2020malware}, 
% we apply the very same extraction procedure for
% static features of packed samples, as retrieving the original code 
% would require the sample to be executed, 
% invalidating the concept of \emph{static} extraction.}

% We refer the interested reader to~\cite{aghakhani2020malware} for a more
% detailed description of the features and the works that originally proposed
% them.

%However, we slightly modified the feature selection phase. In each experiment,
%we split our dataset in train, validation, and test set by respectively using
%60\%, 20\%, and 20\% of the samples.  We used the validation set to perform
%feature selection for the \emph{DLL imports}, \emph{API imports},
%\emph{N-gram-based}, and \emph{Opcodes n-gram-based} classes: while Hojjat et
%al. tested their Information Gain on the whole dataset and selected the most
%powerful features using the elbow method, we used a validation set to contain
%overfitting.  When considering all the 750 families and their 100 samples, we
%extracted 30,386 features for each sample in our dataset. For the sake of
%completeness, we report the breakdown of each class in the following paragraphs
%by detailing the methodology characterizing each of them.  We refer the
%interested reader to the manuscript of Hojjat et al. to identify previous work
%that investigated the power of each class.
%
%\noindent \textbf{Global (2)} - We computed the size in bytes of the binary and
%the entropy of the whole file according to the Shannon's
%criterion~\cite{lyda2007using}. 
%
%\noindent \textbf{PE headers (29)} - We extracted 13 integer values from the
%\textit{Optional} and \textit{COFF} headers~\cite{microsoftPE} whose names and
%meanings are described in Table~\ref{tbl:staticFeatures_peHeaders} of the
%Appendix.  From the 16 attributes of the characteristic field in the
%\textit{COFF} header, we additionally considered 16 binary features indicating
%whether the corresponding attribute is set or not for the sample. 
%
%\noindent \textbf{PE file sections (590)} - A PE executable file is composed of
%several sections. We initially computed 8 features
%(Table~\ref{tbl:staticFeatures_peSections}) for each of them. Then, we used the
%4-byte-long characteristic field in the header of each section~\cite{PEheader}
%to extract 32 binary features for each bit in the field (i.e., True or False
%whether the bit is set to 1 or 0). The maximum number of file sections we
%encountered in our dataset is 19, which initially resulted in 760 \emph{PE file
%section} features for each binary. Nevertheless, we removed those features for
%which we did not observe any variability across our samples and retained a
%total of 590 features.
%
%\noindent \textbf{DLL imports (131)} -  We extracted dynamically-linked
%libraries (DLLs) declared in the executables, and computed the Information Gain
%(IG) for each of them~\cite{quinlan1986induction} on the validation set. We
%selected the top-131 DLLs by employing the elbow method on the IG distribution
%(Figure~\ref{fig:IG_dlls}), and created the same number of binary features,
%each of them indicating whether the executable imports the library or not. 
%
%\noindent \textbf{API imports (3,732)} -  Following the same methodology as for
%\emph{DLL imports}, we parsed the Import Directory Table (IDT) that declares
%the imported APIs by the PE. By computing the IGs and looking at their
%distribution (Figure~\ref{fig:IG_apis}), we created a vector of 3,732 binary
%features, each of them set to true if the executable imports the corresponding
%API.
%
%\noindent \textbf{N-gram-based features (13,000)} - We extracted 4, 5, and
%6-grams from an executable by considering it as a sequence of bytes. Due to the
%unfeasibility of storing those n-grams for all the samples, we randomly
%selected 1000 files from our dataset and extracted the corresponding byte
%n-grams by retrieving 1,363,150,788 unique values. We filtered out those that
%appear in less than 1\% of the samples and tested the IG of the remaining ones
%(Figure~\ref{fig:IG_ngrams}). We selected the top 13,000 n-grams according to
%the elbow method on the IG distribution, and created the same amount of binary
%features indicating whether the corresponding n-gram is present or not in the
%file.
%
%\noindent \textbf{Opcode n-gram-based features (2,500)}. We disassembled each
%executable into opcode tokens by running Capstone~\cite{capstone} and we built
%opcode n-grams up to a length of 4. Given an initial set of 255,812 unique
%opcode n-grams, we computed the TF-IDF for each of them and built the IG
%distribution (Figure~\ref{fig:IG_opcodes}). As in the previous cases, we cut
%the distribution using the elbow method to retain 2,500 float features. 
%
%\noindent \textbf{Strings (10,402)}. We extracted 104,021,839 printable strings
%by using the \textit{GNU strings} program, filtered out those shorter than 4
%characters, and removed rare strings appearing in less than 1\% of the files.
%We saved 10,402 binary features for each executable, each of them capturing the
%existence or absence of the respective string in the binary.

\subsection{Sandbox} \label{sec:sandbox}

We have built a sandbox for executing malware using the best practices proposed
by previous
works~\cite{rossow2012prudent,miramirkhani2017spotless,maffia2021longitudinal,yong2021inside}.
%
We configured a Windows 10 Pro 32-bit virtual machine (VM) with 2 CPUs (Intel
Xeon Platinum 8160 @ 2.10GHz) and 2 GiB of RAM.  We installed popular apps and
populated the file system with common file types to resemble a legitimate
desktop workstation as suggested by Miramirkhani et
al.~\cite{miramirkhani2017spotless}.  Malware runs on clones of this VM
orchestrated using Proxmox VE~\cite{proxmoxve}.  To improve performance, we
stored all virtual disk images and VM snapshots in a RAM disk.
%
As recommended by Rossow et al.~\cite{rossow2012prudent}, each machine runs on
its isolated local network with full Internet access through an ADSL line of
our institution dedicated to this purpose.
%
Recent works have measured that 40\%--80\% of modern malware use at least one
evasive technique~\cite{maffia2021longitudinal,galloro2022systematical}.  To
limit the impact of such evasions, we base our analysis on the Intel PIN-based
\emph{JuanLesPIN} tool~\cite{JLP,maffia2021longitudinal}, which handles common
evasive techniques, thereby increasing the likelihood that malware detonates.
Unfortunately, it does not support 64-bit Windows executables, so we focus on
32-bit malware. We modified \emph{JuanLesPIN} to monitor Windows APIs
responsible for network, processes, services, registry, mutexes, file system,
and DLL loading. Finally, %We provide the complete list of the APIs in Appendix~\ref{sec:monitoredAPIs}.
we tested our analysis environment with the Al-Khaser~\cite{alkhaser} tool to
confirm that our sandbox could not be identified.
%
To measure the overhead introduced by our analysis system we executed 1,000 malware samples
randomly chosen among those that: (i) terminate the execution, (ii) use at least
one evasive technique, and (iii) \textbf{detonates} according to the threshold
proposed in~\cite{kuechler2021}, 
i.e., the sample calls at least 50 Windows APIs.
% we stick to this definition for the rest of the paper.
We measured their execution time with and without instrumentation by observing a
percentage increase of $\mu=125$, $\sigma=31$, $min=26$, $med=106$, $max=206$.
This overhead is in line with that in~\cite{maffia2021longitudinal}.
%
Kuechler et al.~\cite{kuechler2021} recently showed that the amount of code
executed by malware samples plateaus after two minutes, and little additional
information can be obtained thereafter. 
Thus considering the overhead mentioned above, we took a conservative approach and ran
each sample for up to five minutes.


\subsection{Dynamic Features} \label{sec:features-dynamic}

We extract 7 classes of dynamic features from the API calls (including their
arguments) invoked by the malware during execution in the sandbox.  The features
were chosen to cover those used in previous works that built classifiers from
malware executions (e.g.,~\cite{mtnet,malinsight,AonzoUsenix2022}).
% and to include parameters as those have been shown to increase the
% accuracy~\cite{something}.

The lower half of Table~\ref{tbl:features} summarizes the 7 dynamic feature
classes (prefixed by \texttt{d-}).
% extracted from each malware execution.
%
Categorical features such as filenames and domains are one-hot encoded to
Boolean features.  To encode each feature, we count all its possible values and
exclude those appearing less than five times in the training set. 
% These rare categories are not considered in the one-hot encoding process. 
%
The \emph{d-network} class (438 features) captures the HTTP, TCP, and UDP
traffic.  Of those, 430 features capture unique domains contacted by the malware
and HTTP User-Agent strings used; three count the number of HTTP requests, TCP
connections, and UDP pseudo-sessions; and 5 randomness-related 
% (Gibberish Score~\cite{AonzoUsenix2022}) 
features capture the mean/median/min/max/std likelihood of domain names and URLs
contacted according to a recently proposed Markov Chain
model~\cite{AonzoUsenix2022}.
%~\cite{Hartman1996VirtualME}.
%
The \emph{d-file} class features (60,555) capture the name and extension of
60,547 files created or accessed by the malware, the number of files read,
written, and deleted; and 5 capture the randomness of the filenames. 
% similar to the network feature class. 
%
The \emph{d-mutex} class features (7) capture the number of mutex objects
created and the randomness of the mutex names.
%
The \emph{d-registry} class features (60) capture 55 unique registry keys
written, and the count of registry keys created, opened, read, written, and
deleted.
%
The \emph{d-service} class features (736) capture the count, randomness, and
names of services and service managers created, started, and halted. 
%
The \emph{d-process} class features (28,198) capture the count of processes
created, processes terminated, and shell commands invoked, as well as 28,195
unique process names.
%
The \emph{d-thread} class features (7) capture the number of the threads opened,
created, resumed, terminated, and suspended, as well as the number of the
interactions with the context of a given thread and the number of asynchronous
procedure calls (APC) queued to a thread.  The last two features help capture
suspicious behaviors.

\paragraph{Missing features.}
When a dynamic feature cannot be computed (e.g.,due to lack of activity), 
we assign them default place-holder values that do
not belong to the domain of the features.
% in the case of a probability $[0, 1]$, we assign $-1$.
We refer to such features as \emph{missing} features.  For example, if a sample
has no file system activity, we cannot compute the \emph{d-file} filename
randomness features. As a result, the 5 statistical features related to the
randomness of the file names are thus not available.
\revision{We perform dynamic feature extraction only over detonated 
malware samples 
(i.e., those that called at least 50 APIs as defined in \ref{sec:sandbox}), 
but even for detonated samples, there are still missing observations of feature values.}
% To give a clarifying example, we found an \texttt{emotet} specimen that
% creates the file \texttt{kbuhkupik.exe} in the Desktop folder of the current
% user and an \texttt{necurs} specimen that exhibits no file system activity.
% The former has the vector of statistics on the randomness of filenames
% containing floating point values, while the latter, not having written any
% files, it has Null features. 
%According to our dynamic analysis, malware samples in $76\%$ of the $M_B$
%families have at least four dynamic feature classes where over half of the
%feature values are missing.  $97\%$ of the malware families studied in our work
%have at least 3 dynamic feature classes in which half of the feature values are
%missing. 
To facilitate the analysis of the impact of the missing features, we
define the \emph{feature missing rate} (FMR) of a malware family as the fraction
of family samples that have missing values in the file, registry, service, and
process features
% In the dataset of the study, these 4 feature classes are those the most
% frequently containing missing feature values.  
(which, among the seven dynamic features classes we consider, are the most
relevant for classification according to
Table~\ref{tbl:binary_and_multiclass_FeatImportance}).  Missing values over
\text{all} these four feature classes considerably degrades both the amount and
quality of useful information available to the classifier.  According to our
analysis, over 54\% of the malware families studied in our work contain on
average 77\% of the malware samples per family with missing feature values in
these four dynamic feature classes. Missing observations can negatively impact
ML classifiers by overfitting the data and reducing the model's accuracy.
Recently, Aonzo et al.~\cite{AonzoUsenix2022} showed that classifier models tend
to focus on static features, rather than dynamic ones, precisely because static
features are rarely missing.
%
%\simo{since Usenix's work came out with the dataset and code as well, let's try
%to see if we get the same number? That is, to confirm that our sandbox is not
%broken!} \platon{Does this mean that at least one sample in the family have
%missing features?  Maybe it would be more straightforward to report the
%percentage of samples instead?}
In Section~\ref{sec:best_and_worst} we analyze the impact of missing features in
the classification results.

\begin{comment} 
	Figure.\ref{fig:feature_missing_vs_families} shows the median of
	the numbers of the feature classes per family, where half of the feature
	values are missed from observation.  As we can find, most of the families
	contain missing feature values in the dynamic feature classes. 

	% Figure environment removed 

\end{comment}

	%In the following empirical study, we demonstrate the impact of the missing
	%observations in the feature vectors. First, we randomly sample $K$ of the
	%670 families and $M$ out of the 100 malware samples per family to build the
	%malware training / testing data in both family and binary classification
	%experiments.  In a further step, we perform a preprocessing step to measure
	%the average level
	%of missing feature observations per family. We then rank the malware
	%families with respect to the average level of missing feature observations
	%in an ascending order. After that, we choose the top $K$ families in the
	%ranking list, i.e. choosing the malware families of the most completed
	%feature observations. The resultant classification performances
	%(\textit{F1} and
	%\textit{Accuracy} score) are compared to those derived by randomly
	%selecting the families. As shown below, the classification performances
	%derived using the families with the most complete feature observations are
	%significantly better.  \textbf{Network activity} summarises the network
	%requests in UDP, TCP and HTTP protocols. We extract strings of domain names
	%and user agent strings of the
	%network requests as categorical features. In the whole dataset, there are
	%430 unique domain names / user agent strings. In parallel, the number of
	%UDP, TCP and HTTP requests are used as 3 numerical network activity
	%features. Besides, we evaluate the randomness of the URL strings and domain
	%name strings using the markov chain model of natural language proposed in
	%\cite{Hartman1996VirtualME}.
	%We compute the likelihood of a string of URL/domain name as a natural and
	%regular text. The lower the likelihood value is, the more random the string
	%pattern becomes. Given a Windows executable, we compute the summary
	%statistics of the likelihood scores of the domain name and user agent
	%strings, i.e. the minimum, maximum, mean, median and standard deviation of
	%the scores. These 5
	%randomness-oriented features are included in the network features of the
	%executable. 

	%\textbf{File operations} include the names and extension types of the file
	%read, created and queried while running an executable. They are considered
	%as string-based file features. Over the whole training dataset, we have
	%60,547 unique strings as the categorical features. In addition, we count
	%the number of the files read, created and queried separately as 3 numerical
	%features.
	%Similarly as in the network features, we also include the 5 summary
	%statistics of the likelihood scores of the strings in the file feature set. 

	%\textbf{Mutex} are composed by the number of the mutex objects created
	%during the dynamic analysis and the 5 summary statistics of the string
	%randomness scores computed from the names of the mutex objects. 

	%\textbf{Registry operations} record the Windows registry keys opened,
	%queried, created, deleted and set. In total, there are 55 unique registry
	%keys stored as the categorical features. Besides, the number of the
	%registry keys opened, queried, created, deleted and set are used separately
	%as 5 numerical features. 

	%\textbf{Service} include the names of the services started, halted or
	%created as the string-based categorical feature. In the dataset, there are
	%729 unique name strings. Besides, we compute the 5 summary statistics of
	%the likelihood scores of the service name strings. The number of the
	%service name strings and service managers are also included as 2 numerical
	%features. 

	%\textbf{Process} uses the name string of the created, terminated or invoked
	%shell commands as the categorical feature. There are 28,195 unique name
	%strings of services in the dataset. The number of the created, terminated
	%and invoked shell commands are used as 3 numerical features. 

	%\textbf{Thread} saves the number of the threads opened, created, resumed,
	%terminated, suspended by executing the target sample.  In addition to the 5
	%numerical features, we also count the number of the interactions with the
	%context of a given thread and the amount of APC (asynchronous procedure
	%call) queued to a thread. We include both features to help identify
	%suspicious
	%behaviours. 

	%We follow two perspectives of organising low-level features of executables
	%for malware detection and family classification. The two paradigms of
	%feature representation have been used popularly in malware detection and
	%classification \cite{mtnet,maldy,malinsight}. First, we tackle the task of
	%low-level feature extraction following the techniques of document
	%classification and used by
	%\cite{mtnet,maldy,malinsight}. As part of this process, we consider each
	%windows API and its arguments recorded in the execution traces of dynamic
	%analysis as one single word. The dynamic analysis result of a Windows
	%executable consisting of ensembles of the APIs can thus be treated as a
	%text.  We extract the unigrams and tri-grams of the distinct combinations
	%of the API
	%calls and the arguments. For each executable, the term frequency and
	%inverse document frequency (TF-IDF) vectors of both uni- and tri-gram
	%patterns are computed and concatenated to be the low-level feature
	%representation profiling the behaviours of the executable
	%\cite{mtnet,maldy}. In our study, we remove the rarely occurred uni- and
	%tri-gram patterns, i.e. those appearing less than
	%twice in the whole dataset, to avoid over-cluttered features. In total,
	%there are 604,204 uni-gram patterns and 1,089,263 tri-gram patterns left to
	%define the feature space. To yield input features of reasonably large
	%dimensions, we further adopt mutual information-based feature selection
	%over the TF-IDF feature vectors to choose 1,185,426 uni- and tri-gram
	%sparse features best
	%differentiating malware families. Dimension reduction methods are then
	%applied over the TF-IDF feature vectors of the selected features to
	%generate input vectors of 4,000 dimensions to the downstream classifiers.
	%In this work, we employ two dimension reduction methods \textit{Sparse
	%Random Projection} (SRP) \cite{mtnet} and \textit{Singular Vector
	%Decomposition} (SVD) used for malware
	%and text classification. Second, we consider the dynamic analysis traces of
	%each malware/goodware sample as a temporal sequence of Windows API
	%execution records (the name of the API and its arguments). The
	%time-series-based representation of dynamic analysis results is also widely
	%adopted in the works of Machine-Learning-driven malware detection and
	%classification \cite{maldy}.
	%\paragraph{Feature completeness of dynamic features} The quantified feature
	%vectors of dynamic analysis-based features contain missing observations.
	%\textcolor{red}{It is due to what cause?}. For training / testing machine
	%learning models, the presence of missing observations can bring negative
	%impact, such as making the classifier prone to the overfitting issue and
	%inaccurate over testing samples. In our collected data, over the whole 670
	%families, $80\%$ of the families have on average more than $66.7\%$ of the
	%malware samples containing Null/None feature values. $50\%$ of the families
	%have on average more than $75\%$ of the malware samples containing
	%Null/None discrete feature values. The existence of the
	%missing-observations in the
	%training / testing data can cause deterioration of the classification
	%performances. In the following empirical study, we demonstrate the impact
	%of the missing observations in the feature vectors. First, we randomly
	%sample $K$ of the 670 families and $M$ out of the 100 malware samples per
	%family to build the malware training / testing data in both family and
	%binary classification
	%experiments. In a further step, we perform a preprocessing step to measure
	%the average level of missing feature observations per family. We then rank
	%the malware families with respect to the average level of missing feature
	%observations in an ascending order. After that, we choose the top $K$
	%families in the ranking list, i.e. choosing the malware families of the
	%most completed
	%feature observations. The resultant classification performances
	%(\textit{F1} and \textit{Accuracy} score) are compared to those derived by
	%randomly selecting the families. As shown below, the classification
	%performances derived using the families with the most complete feature
	%observations are significantly better. 

	%\subsection{Additional Feature Extraction methods on Dynamic Analysis
	%Results} Beside using the TF-IDF feature vectors of the uni/3-gram patterns
	%of APIs, we also consider to use the sequences of the APIs as time-series
	%for both detection and classification. We try different sequential formats.
	%First, we select the 3-grams of APIs with the largest TF-IDF values. These
	%3-gram
	%patterns are the most informative to differentiate the behaviours of
	%different malware samples. We arrange the 3-gram patterns in a sequential
	%structure, according to the time step information of each 3-gram element.
	%We then train a word2vec embedding vector for each of the 3-gram patterns.
	%The derived embedding vectors are used as input to a LSTM model to capture
	%the temporal
	%context of the API call sequence for detection and classification. Second,
	%we only train word2vec embedding vectors for each of the API calls. The
	%embedding vectors are then fed into a LSTM to model the sequential
	%structure of the API call sequence. The output of the LSTM model can be
	%used for detection and classification. \textcolor{red}{Beyond that, we make
	%use of the dynamic
	%analysis reports generated by VirusTotal and vectorise the dynamic
	%features, such as information about process, service, network activity,
	%runtime DLL import, file access, registry table edits and mutexes observed
	%while executing a Windows malware. These attributes are encoded with the
	%one-hot encoding scheme and fed into a random forest classifier for
	%detection and family
	%classification use. Currently, this feature extraction method achieves an
	%averaged detection accuracy score of 0.9413 and averaged AUC score of
	%0.9337 after conducting 10-fold cross validation test. }

\subsection{Models} \label{sec:models}

We train multiple models to capture different axis: 
classification task (i.e., binary or family classification), 
features (i.e., static, dynamic, combined), 
\revision{classifiers (i.e., Random Forest, XGBoost),} 
dataset construction (i.e., distribution of families in training
dataset), and a different number of families and samples.

\paragraph{Classification task.} We build models for binary and family
classification tasks. The binary classification models detect whether a
given sample is malicious (positive class) or benign (negative class). The
family classification models identify the family of a given malicious
sample, that is, there is one class per malware family and no goodware
class.  We prefix the name of a model with \emph{binary-} or \emph{family-}
to indicate the classification task.

\paragraph{Features.} We build models that use all static features, all
dynamic features, and all combined features (i.e., all static and all
dynamic).  The name of a model includes \emph{-static-}, \emph{-dynamic-},
or \emph{-combined-} to indicate the features used. 
% Dynamic and combined models only include detonated samples that called 
% at least 50 APIs during their
% execution, a threshold previously proposed to determine whether a sample has
% detonated~\cite{kuechler2021}.

\paragraph{Classifiers.}
\revision{
Given a large number of ML classifiers, it is not possible for us to 
systematically evaluate all of them. 
%
In our experiments we selected \textit{Random Forest} and \textit{XGBoost} because
% they are efficient to train
they are consistently among the best-performing classifiers
evaluated in previous works
(summarized in Table~\ref{tbl:related} and Section~\ref{sec:related}).
Moreover, being tree-based, they are easier to interpret, they allow 
direct analysis of feature importance, and they are also intrinsically 
capable of handling both categorical features
(e.g., unique filenames accessed during execution) and continuous features
(e.g., filename mean randomness).
%
We also considered neural networks, but discarded them because
to achieve good performance they require larger training datasets 
(e.g., $\ge 400k$ samples in~\cite{malconv}).
It was not clear whether we could build a balanced family dataset of 
the required size. 
In addition, there exist many potential neural architectures to evaluate and
their training times are longer,
which is critical given the large number of models we evaluate.
%
%We tune the Random Forest and XGBoost hyper-parameters
%(e.g., number of trees, learning rate, max tree depth, loss type) 
%on a validation set.
% The Random Forest models use 500 trees, gini impurity, and unlimited tree depth.
}

\paragraph{Dataset construction.} For the binary classification task, we
experiment with two ways of building our dataset, namely \emph{uniform} and
not \emph{nonUniform}.
% 
The \emph{uniform} approach builds datasets that balance the number of
goodware and malware, using a sampling-with-replacement approach, as
follows. We uniformly select from each family in $M_B$ a number of samples
so that the total number of malicious samples matches the size of the benign
dataset (i.e., each family in $M_B$ provides 24--25 samples for a total of
16,611 malware samples).  We repeat the process five times avoiding
repetitions (i.e., each time selecting a different set of malware samples
from each family in $M_B$), to completely cover all the malicious samples
in each family.  These steps produce 5 balanced datasets. 
%
Each dataset is split into 60\% of samples for training, 20\% for
validation (i.e., selecting the classifier hyper-parameters), 
and 20\% for testing.
%
To evaluate a model, for each of the five datasets, we perform a 10-fold
cross validation to ensure that all the samples equally contribute to the
training and testing datasets. We report average results across the five
rounds and their respective folds. Thus, obtaining the accuracy results from
one model requires us to train and test 50 times.

The \emph{nonUniform} approach replicates the unbalanced distribution of
samples per family in the Motif dataset~\cite{motif}.  The motivation for
this dataset is to study whether the family distribution in the training set
of a binary classification task (where family labels are not used) affects
the detection accuracy.
%
In Motif, 29\% of families have only one sample, 41\% have 2-5 samples, 12\%
6-10, 10\% 11-20, 4\% 21-30, 2\% 31-40, 1\% 41-50, and 1\% has over 142
samples.  We replicate this distribution on the 670 families in $M_B$.  For
example, we select one sample from 29\% (randomly-chosen without
replacement) of the 670 $M_B$ families and 142 samples from one
randomly-chosen family. 
%
The resulting dataset comprises all 16,611 benign samples and 4,821 samples
from 670 families that follow the per-family sample distribution in Motif.

\paragraph{Number of families and samples.} To measure the impact that the
number of families to classify and the available samples for each family
have on the results, we build multiple ML-based classifiers for the family
classification task by uniformly sampling 70, 170, 270, 370, 470 and 570
families from the total 670 families. For each of them, we also experiment
with a version trained and tested on 50, 60, 70, and 80 malware samples for
each family. As indicated above, we have 20\% samples used as the validation
data.  Therefore, at maximum, there are 80 malware samples for training and
testing use. 

%\paragraph{Research questions to explore} We organize the measurement study
%in this work to investigate the effectiveness of ML-based classifiers in
%the context of malware detection and family classification. The goal of the
%detection and family classification task is to distinguish goodware from
%malicious executables (detection) and differentiate various malware
%families
%carrying different malware features. We aim to explore the answer to the
%following research questions. 
%
%\noindent \rques{1} \textbf{How do the ML-driven classifiers perform with
%different types of malware features? } First of all, we evaluate and
%compare the performance of the ML-driven classifiers using respectively the
%static analysis-based, dynamic analysis-based and feature fusion both types
%of malware features. We measure the contribution of each type of malware
%features in both
%detection and family classification task. 
%
%\noindent\rques{2} \textbf{What are the impacting factors determining the
%performances of ML-driven malware classification?} Second, we quantify the
%impact of varying numbers of families and malware samples per family on the
%detection and family classification outcome. We also study the implications
%of non-uniform family coverage in the training data over the classification
%results. Furthermore, we unveil the negative effects of missing values in
%dynamic analysis-based behavioral features over the classification
%performances. 
%
%\noindent\rques{3} \textbf{How does the ML-driven malware classifier
%perform over malware samples previously unseen families ? } Finally, we
%evaluate the accuracy and uncertainity level of the ML-driven classifiers
%over malware samples of previously unseen families. We aim to compare the
%the uncertainity level of the classifier over malware families appearing in
%the training phase
%(\textit{in-distribution} training instances) and those beyond the training
%data (\textit{out-of-distribution} training instances). Theoretically,
%ML-driven classifiers tend to produce less confident output over the
%out-of-distribution testing samples. We organise the test to show how large
%the uncentainity gap may be encountered in larger-scale malware
%classification task
%s. 

% For example, the training dataset for the \emph{binary-dynamic-170} model
% comprises of 10,697 goodware and 10,200 malware samples that correspond to
% 60 samples for each of the 170 more complete families.  On the other side,
% the training set of the \emph{binary-dynamic-670} model contains the
% 16,611 benign samples and 24--25 samples from each of the 670 families.

%For the n-gram based low-level feature representation, we use three
%different classifiers for malware detection and classification test. They
%include \textit{Random Forests} composing 500 trees, \textit{Gradient
%Boosted Trees} of 120 cascaded trees and a 2-layered \textit{Neural
%Network} containing a linear fully connected layer with ReLU activation
%units and a softmax layer. Random
%Forest and Gradient Boosted Tree-based classifiers are two common
%architectures used in previous works of Machine Learning-driven malware
%analysis \cite{maldy,malinsight}. These two models are intrinsically
%capable of handling categorical features, such as the n-gram patterns. They
%are easy to deploy, yet have been demonstrated to achieve similar levels of
%classification accuracy
%compared to the classifiers with more heavy architectures, e.g. Deep Neural
%Networks. The 2-layered Neural Network-based classifier has been proposed
%in the MtNet work and used for malware classification over 98 families
%\cite{mtnet}.  For the time-series-based representation, we adopt the
%the\textit{Random Forest} with 500 trees as the classifier \cite{maldy}. 

