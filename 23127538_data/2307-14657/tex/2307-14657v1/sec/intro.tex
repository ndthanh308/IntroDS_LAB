\section{Introduction} \label{sec:introduction}

Modern Windows malware analysis has to cope with a large number of samples that
have been steadily increasing for two decades. 
In 2022, both the AV-TEST Institute and Kaspersky registered over 400,000 new malicious programs daily~\cite{kasperskyNewmalware,avtestNewmalware}. 
In order to counter such numbers, research and industry have begun to rely on Machine Learning (ML)-driven malware classification models. 
They can be applied over a large number of files and offer more flexible
classification mechanisms than signature-based methods.  Nevertheless, they have
to contend with human attackers' imagination, which consistently produces new
variants to fly under the radar.  At their core, ML techniques capture the
statistical correlation between training data and classification targets. As a
result, such statistics-based classification models lose their effectiveness
when going beyond the
knowledge encoded in the training data. Human attackers aware of this limitation
can thus always be one step ahead to choose attacks unseen in the training data,
in order to evade the detection of ML-based methods.  Moreover, ML-based
classification models are often performed in a pipeline~\cite{loi2021ITASEC,xiao2021csai,malinsight}. For example, given a
suspicious file, a typical ML pipeline should first figure out whether it is
malicious (\emph{binary classification}), and then find out whether it belongs to a
known family (\emph{family classification}). Even though these classification
tasks achieve high accuracy in previous literature~\cite{loi2021ITASEC,xiao2021csai,malinsight}, most of these works have
been carried out with unrealistic assumptions, mainly because of how the dataset
was constructed. 

In addition, a ground-truth of malware families is hard to obtain. Antivirus companies will not likely use the same name for the same
family. Although the CARO (Computer Antivirus Research
Organization) naming convention has been proposed to mitigate this issue, 
it still faces usage obstacles.
Scientific research tackled this problem and produced AVClass~\cite{avclass2}: 
given a list of AV labels (e.g., from a VirusTotal JSON report),
the tool returns the \emph{single most likely} family name.  However, even if AVClass
returns a single family name according to a consensus algorithm by default, it
can also output a ranking of all alternative family names. Thus, the problem is
that AVClass is often used to carry out studies using its default output as
ground truth, even though it is probabilistic in nature.

% Second, while it is straightforward to get a high number of samples for
% the most famous families,  it is challenging to get even 100 samples for the
% less common ones.  
% To give an idea, we had to monitor the VirusTotal feed for
% \xxx hours in order to obtain 100 samples for the most famous \xxx families,
% while we had to wait \xxx days to get the same number of samples for the less
% common \xxx families. 
% Consequently, given that the datasets used in this type
% of experiment are characterized by the number of families and the number of
% samples per family, it is arduous to balance the dataset appropriately.
Moreover, while it is straightforward to collect a high number of samples for
popular families, collecting a large diverse malware dataset remains difficult
and time-consuming~\cite{anderson2018ember,smith2020mind,tesseract,lee2021android}.
In this work, we collect PE malware executables from the VirusTotal (VT) feed~\cite{vtfeedapi},
a real-time stream of JSON-encoded reports of samples submitted to VirusTotal.
Despite the appearance of more than 44M VT reports over a period of 
nearly three months and the collection of 227k samples from 13.8k families, 
only 780 malware families of those contain at least 100 samples.

To further complicate the matter, malware authors often use off-the-shelf
packers and protectors~\cite{miramirkhani2017spotless,maffia2021longitudinal}. 
Both modify a program to hinder its analysis while still preserving 
its original behavior. Based on their design, different malware that undergo the
packing or protection procedures may generate executables that share a highly
similar structure. This easily makes a ML classifier trained over these malware samples
overfit the packed or protected file structure, rather than capturing
its true malicious component.

Therefore, in this work, we put considerable effort to create four
heterogeneous datasets for a total of 118,111 samples to perform a large-scale
measurement study. Three of them are composed of malicious samples with
varying numbers of families, while the fourth contains benign samples.  We devoted
particular attention during the construction of the datasets, trying both to
reproduce the datasets usually used in research, but also considering
real-world scenarios typical of malware analysis. Such datasets allowed us to
create well-controlled experiments for studying how the effectiveness of
ML-based binary and family classification change under different testing
scenarios. 

Finally, there is also another crucial aspect that influences ML algorithms
that we further explored: \emph{feature} extraction. The methods by which one
can analyze executable files fall into two main categories, depending on what
facets one wants to study, namely \emph{static} properties and \emph{dynamic}
behavior; nonetheless, the previous two can also be \emph{combined}.  Since we
wanted to study existing ML state-of-the-art solutions and \textbf{not} design
new ones, we build our static and dynamic feature extraction approaches on what
was described in recent papers~\cite{AonzoUsenix2022,aghakhani2020malware}.
Therefore, this means that we have statically analyzed and dynamically executed
in a sandbox more than a hundred thousand samples were used in this study.

\vspace{0.3em}

Our work contributes by answering the following research questions
for both binary and family classification tasks:

\noindent \rques{1} \textbf{How do static, dynamic, and combined models perform
on different malware families/classes in binary and family classification?}
% We first report overall results and compare the
% three feature classes. Then, we dive into the malware families that are the
% most difficult to classify in both binary and family classification tasks using
% different feature sources. We aim to figure out what could be the reason
% causing the difficulty of classification.

\noindent\rques{2} \textbf{On which families and classes of malware does each model fail to produce accurate classification? }
%\noindent\rques{2} \textbf{When do we need to launch dynamic
%analysis for ML-based malware classification ?} Running dynamic analysis is
%costly in practices. Therefore an efficient automated malware classification
%pipeline should decide when to introduce the dynamic analysis-based features for
%classification. We conduct the analysis of the classification results to show
%when we need to run dynamic analysis, for example, dynamic analysis could be
%launched when the classification confidence using static analysis-based features
%is below a specific threshold.  

\noindent\rques{3} \textbf{What is the contribution of static and dynamic feature
classes to the classification performance and does their contribution change when
joining the two sets?}

\noindent\rques{4} \textbf{Does the presence of off-the-shelf packers and protectors bring harm to classification accuracy? }
% For static analysis-based features, we investigate the relations between the
% classification accuracy of each malware family with the percentage of packed
% samples in the family. 

\noindent \rques{5} \textbf{Do missing feature values in the runtime behaviors
negatively impact the classification results?}
% For dynamic analysis-based features, there are some
% features valued as NA / Null, as they are absent in the execution traces. We
% check the impact of these missing feature values over the classification
% performances. 

\noindent\rques{6} \textbf{Is the AVClass2 confidence score correlated with
\mbox{ML-based} decisions?}

\noindent\rques{7} \textbf{How does the training dataset construction strategy
affect the model performance?}  
% We quantify the impact of varying numbers of families and malware
% samples per family on the binary and family classification outcome. We also
% study the implications of non-uniform family coverage in the training data over
% the classification results. In the real world, the models receive the samples
% they have to classify unevenly. For example, in the case of the binary task, an
% EDR (Endpoint Detection and Response) has to analyze many more goodware than
% malware, while in the family classification task there are some of families that
% are ubiquitous and polarize the ecosystem. Thus, we studied how the models are
% affected by the two ways of constructing the dataset, i.e., uniform and non
% uniform. Furthermore, we unveil the negative effects of missing values in
% dynamic analysis-based behavioural features over the classification
% performances. 

\noindent\rques{8} \textbf{How does the ML-driven malware classifier perform
over the families unseen in the training data?}
% Does the classification accuracy
% augment by combing the static analysis-based and dynamic-analysis based features
% together into a joint feature pool?}  

% We evaluate the accuracy and uncertainty
% level of the ML-driven classifiers over malware samples of previously unseen
% families. We aim to compare the uncertainty level of the classifier over testing
% malware samples belonging to the malware families in the training phase
% (\textit{in-distribution} testing instances) and those beyond families of the
% training data (\textit{out-of-distribution}, testing instances). 
% Theoretically, ML-driven classifiers tend to produce less confident output over the
% out-of-distribution testing samples. We organised the test to show how large the
% uncertainty gap may be encountered in larger-scale malware classification tasks. 
% The results show that performing a well-trained malware classifier over
% out-of-distribution samples produces much less confident decision. This raises a
% concern over the reliability of generalising a ML-based classifier
% in the practical malware classification use case.




%\vspace{0.3em}
%    \noindent In conclusion, the main contribution of this study is two-fold:
%\begin{itemize}[leftmargin=*]
%\item Among the datasets we used, one of them is of crucial importance for scientific research.
%This large-scale Windows malware dataset contains 67,000 malware samples from 670 malware families, with 100 samples per family. 
%Although we cannot share the samples due to legal restrictions, we share~\footnote{--- Anonymized for blind submission ---} the hashes with the corresponding family label.
%To date, it is the dataset with the largest number of families ever studied, and the one with the most samples in the last five years.
%    \item We present a comprehensive measurement study
%        on the effectiveness of ML-based malware classification under
%        different testing scenarios. Specifically, the experimental study
%        investigates the research questions raised above, which
%        deepens the understanding about how the accuracy of ML-based malware
%        classification should be evaluated and interpreted.  
%         \end{itemize}

		 
\begin{comment}	
	
%\noindent \rques{2} \textbf{Which types of features are the most important in
%the two classification tasks (e.g., n-grams, opcodes, network activity, and so
%on)? Does this change if we combine the important static analysis-based and
%dynamic analysis-based features together for classification? } We evaluate and
%compare the performance of the ML-driven classifiers using respectively the
%static-analysis based, dynamic-analysis based and combination of both types of
%malware features. We measure the contribution of each type of features in both
%detection and family classification task. 

%\simo{This was missing, and imho fits in this position} \noindent\rques{X}
%\textbf{How does the building of the training dataset affect the model?} In the
%real world, the models receive the samples they have to classify unevenly.  For
%example, in the case of the binary task, an EDR (Endpoint Detection and
%Response) has to analyze many more goodware than malware, while in the family
%task there are some of them that are ubiquitous and polarize the ecosystem.
%Thus, we studied how the models are affected by the two ways of constructing
%the dataset, i.e., uniform and non uniform.

%\sav{Third request together with discussing the impact of missing features for
%dynamic analysis. This should be linked to research question 2. So for example
%if you say that some families perform better with dynamic, can we spot any
%correlation between missing features and performance?} \noindent\rques{3}
%\textbf{Does the presence of off-the-shelf packers / protectors bring harm to
%the classification accuracy?}  We dive into the results of both classification
%tasks using the static analysis-based features. We investigate the relations
%between the classification accuracy of each malware family with the percentage
%of packed samples in the family. 


%\sav{I'd put this as last point together with the entropy discussion. We can
%also discuss here when one should or should not trust the classifier based on
%the entropy score (and here comment the last plots at the end of the paper)}
%\noindent\rques{5} \textbf{Does the family label provided by AVClass provide a
%good training class label of ML-driven family classification?} For family
%classification, we compare the confidence of AVClass with that of the ML-based
%classifier to unveil the strong statistical association between the two
%confidence measurement. The results confirm the use of AVClass's family
%attribution for training ML-based classifiers.

In the followings, Section.2 describe the dataset collection process. In
Section.3, we provide detailed descriptions about the research methodology
adopted in this study, which are designed to answer the 6 research questions.
Section.4 and 5 present the empirical results of detection and family
classification task using the collected dataset. Section.6 concludes the whole
paper. 


%In this work, we created a large-scale malware dataset to find a good trade-off
%between the number of families and samples, and through several analysis, we
%sought answers to the following research questions.  All these questions must
%be answered for the binary classification as well as family classification
%\begin{enumerate} \item Which kind of features are the most important in the
%tasks (ngrams, opcodes, dynamic and so on)? Does this change if we combine the
%static and dynamic?  \item Is it worth running Dynamic analysis (because it is
%expensive)?  And when (i.e., below which accuracy threshold of static
%analysis)?

%    \item Does the presence of off-the-shelf packers/protectors negatively
%    affect the accuracy of the models?
%\end{enumerate}

%These questions are just related to binary classification \begin{enumerate}
%\item Does training dataset diversity matter for binary classification?  \item
%Can the static classifier confidence be used to determine whether a sample
%should be dynamically run?  \end{enumerate}

%These questions are just related to family classification \begin{enumerate}
%\item How the number of families and samples influence classification results?
%(heatmap in the tasks) \item What happens if you provide singleton or unseen
%families?  \item How does the confidence of our classifiers relate to that of
%AVClass?  \end{enumerate}

%Here is the list of things we explore in this paper: \begin{itemize}

%\item We analyze the impact of the missing information in dynamic analysis
%reports on the binary and family classification problems.

%\end{itemize}



\end{comment}
