\subsection{Model Generalization}
\label{sec:singleton_and_unseen} 

\input{tbl/tbl_binary_singletons_unseen}

% Figure environment removed


%% Figure environment removed (e.g., those families do not reach 100 samples)

In this section, we test how well our models for binary and family
classification generalize on unseen data. To this extent, we validate the performance of
the previously-trained models on the singleton and unseen 
datasets introduced in Section~\ref{sec:maltest}, which include new families and have 
different distributions from the training data.
%We first use the malware samples from the 670 families and \emph{goodware} to train a
%Random Forest model for binary classification. We then apply this classifier
%over 1) unseen singleton malware samples that are not classified by AVClass in
%any family, and 2) malware samples from families not included in the training
%dataset.  The testing malware samples are collected from families different from
%those in the training set. 
This scenario is known as the "out-of-distribution" (OOD)
test~\cite{Liu2020nips}, where training and testing data have different
distributions in the feature space. 
The distribution gap between the training and testing data has been frequently
witnessed in malware analysis~\cite{Jordaney2017usenix}, as
malware families evolve rapidly over time. 
Theoretically, one should expect the performance of a ML model 
to drop drastically in this more realistic scenario, as OOD samples directly violate the IID assumption of ML techniques. 

%The classic generalisation theory of machine learning
%assumes training and testing data share the same distribution, i.e., the
%assumption of identical independent distribution (IID). In the
%out-of-distribution case, the classification performance of a machine learning
%model may drop and provide less certain decision outputs due to the distribution
%gap between training and testing samples. While the IID assumption holds
%practically in large-scale image classification tasks, the drift of data
%distribution is more frequently witnessed in malware analysis \cite{Jordaney2017usenix}. Malware families
%evolves rapidly over time due to change of attack techniques and update of
%security policies. 
%In this section, we organise this test to address \rques{4}
%and evaluate empirically the accuracy of malware classification over
%out-of-distribution malware samples.  


\paragraph{Binary Classification.} Table~\ref{tbl:binary_singletons_unseen}
summarizes the binary classification results over the singletons and unseen
families using the static, dynamic, and joint feature pool.
"Uniform" and "non-uniform" in the table denote training with the 670 families
with uniformly and non-uniform dataset construction methods (\S~\ref{sec:models})
The empirical measurements shown in Table~\ref{tbl:binary_singletons_unseen} can
be summarized around three main observations. 

First, the accuracy of binary classification using only
static or dynamic features deteriorates significantly over singleton and unseen family files. 
Using the combined feature set, the binary classification accuracy with the
uniform setting augments over the singleton samples, whereas it deteriorates over the unseen families. 
In the non-uniform setting, we can observe the same
tendency of accuracy drop over the OOD samples. The observations echo closely to the
out-of-distribution challenge of machine learning raised in
\cite{Liu2020nips}. %The accuracy improvement over the singleton samples can be caused by the fusion of the static
%and dynamic analysis features. Note the static analysis-based features perform
%relatively much more stably across the in-distribution (0.9569) and the
%singleton files (0.9428), than the dynamic-based features.

%For example, compared to the performance shown in Compared to
%training and testing using the subsets of the 670 families, the binary
%classification accuracy with the static analysis-based features drop from 0.9569
%to 0.9428 for singleton samples and 0.8105 for unseen family samples in the
%uniform case. Using dynamic features and with the uniform
%setting, the accuracy score drops from 0.9291 to 0.8052 for singleton samples
%and 0.8984 for unseen family samples. Combing the static and dynamic
%analysis-based features together, the binary classification accuracy with the
%uniform setting augments over the singleton samples (0.9569 v.s. 0.9852), while
%deteriorates over the unseen families (0.9569 v.s. 0.9085). The accuracy
%improvement over the singleton samples can be caused by the fusion of the static
%and dynamic analysis features. Note the static analysis-based features perform
%relatively much more stably across the in-distribution (0.9569) and the
%singleton files (0.9428), than the dynamic-based features. The static
%analysis-based features play a dominant role in the classification task of the
%singleton files. In this situation, injecting the other feature source can hence
%help correct the classification output over the difficult samples close to the
%classification boundary.  In the non-uniform setting, we can observe the same
%tendency that the binary classification accuracy decreases consistently over the
%singleton and unseen family samples. The observations echo closely to the
%out-of-distribution challenge of machine learning raised as in
%\cite{Liu2020nips}. 

Second, the accuracy deterioration over the
out-of-distribution samples is more significant in the non-uniform setting of training than
that in the uniform setting, regardless of the used features. 

%For example, we can find that the accuracy scores over the
%singleton and unseen families are generally 10\% to 70\% lower than those
%derived under the uniform setting. 
This is different from the results of the
in-distribution evaluation in Table~\ref{tbl:binary_accuracy}, where we observe no major
difference in accuracy between the uniform and non-uniform settings. These
results show an important point: classifiers built on very unbalanced datasets may perform equally well
when tested on samples with the same unbalanced distribution, but generalize more poorly to 
other testing datasets, likely because many families were underrepresented in the training
and thus the model failed to properly capture them.
%The uniform setting provides a more balanced coverage of the diversified feature
%representations over different malware families. As a result, the classifier
%learnt using the uniform setting can capture more accurately the variation of
%the malware features across different families, which helps better tolerate the
%distribution drift. 

%In our case, we aggregate the static analysis and
%dynamic analysis-based features together into a joint and richer feature
%representation. Theoretically, an enriched feature space can offer better
%classification performances according to the curse-of-dimensionality theory.
%Nevertheless, the 

Third, we can notice that static features generalize poorly to unseen families, while dynamic
features perform better in this scenario. This is due to the nature of the features
themselves: static information can precisely pinpoint only known samples, while
dynamic behavior can better generalize also to unknown ones.
Thus, compared to static features, dynamic features may provide more
rich information to capture new types of malicious behaviors that never
appear in the training phase.  

% we noticed again how feature-level fusion does not bring
% a clear benefit  a common way of information
% fusion in machine learning, the results suggest that the benefit of feature fusion is not necessarily guaranteed in
% practices~\cite{Trunk1979pami}.  One possible reason is dynamic features
% suffer from missing feature values. The fusion of both static and dynamic
% features can thus introduce noise, which harm the accuracy. 

We investigate this aspect in more detail by varying the number of families 
we used for training. In Figure~\ref{fig:binary_singletons_unseen_heatmap}, we can see that 
dynamic features perform
poorly when the number of malware families for training is low (as there was not enough
example of behaviors to learn from) but, with a sufficient number of families, they
offer better classification results than static features. 
Dynamic features usually have a high dimensional
and highly sparse feature representation. For example, some files or
processes only appear a few times in the training set for specific
malware families. A smaller number of families may aggravate the
curse of dimensionality, which results in an overfitting of the classifier.
Furthermore, we can observe the classification
accuracy over unseen samples improves as the number of families increases,
regardless of the features used in the test. 

%Whether it can improve the accuracy depends on whether the
%feature sources are complementary to each other \cite{Lip2012}. According to Table.\ref{tbl:binary_accuracy} and
%Table.\ref{tbl:binary_singletons_unseen}, the accuracy of binary classification
%using the combined feature set is generally lower than using the static
%analysis-based features only. One possible reason is dynamic
%features suffer from missing feature values. This reduces the
%descriptive power of dynamic features. Feature-level fusion hence doesn't help boost the
%performances. %Notably, in the non-uniform setting of
%Table.\ref{tbl:binary_singletons_unseen}, the accuracy obtained using the
%combined feature space is lower than those derived using either of the feature
%sources. This phenomenon can be attributed to the vulnerability of feature
%fusion to the potential noise in the combined feature sources. The accuracy of
%the static analysis-based features is much lower than that of the dynamic
%analysis-based features, i.e. 0.6532 v.s. 0.8557. In this case, simply combing
%the two features may introduce noise to the joint feature representation and
%reduce the classification confidence of the samples close to the classification
%boundary. As a result, the feature fusion strategy may harm the classification
%performance using a single feature source. 

%Figure~\ref{fig:binary_singleton_errors} shows the error distribution for the
%singleton detection results. 
%\yufei{I remove the CDF plot from the main body of the paper tentatively. I
%think it doesn't bring extra information compared to Table.10. Another thing is:
%should we move the results using the classifier trained using different number
%of families into a plot, instead of keeping them in a table? }


%For each model, we sample the training data 5 times and report the averaged
%detection accuracy.

% Figure environment removed

\paragraph{Family Classification.}
So far, we only tested the generalization of our models in a binary classification
scenario. We now apply our family classifier
trained using the 670 families over the singleton and unseen families as another
out-of-distribution test scenario. Achieving high or low classification accuracy
over these out-of-distribution samples is not interesting, as most of these
samples share no common families as the training data and we don't have the
ground truth family labels for these samples. Thus, the purpose of organizing this
test is only to study how the uncertainty level of the family classifier changes
over the out-of-distribution malware samples. 

%On one hand, theoretically as
%defined in \cite{Liu2020nips}, the distribution gap between the training and
%testing data can render the sharp increasing of decision uncertainty of machine
%learning algorithms. On the other hand, the out-of-distribution testing samples
%origin from different families from the training samples. The family classifier
%is hence expected to produce much less confident decision over the testing
%samples. 

To measure the uncertainty difference, we define the Relative Entropy Score
(\textit{RES}) of the classifier's output as $\frac {\sum_{k=1}^{C} p_{k}\log{p_{k}}} {T}$, 
%\begin{equation}\label{eq:RES} \small RES = \frac{\sum_{k=1}^{C} p_{k}\log{p_{k}}}{\sum_{k=1}^{C} 1/C\log{1/C}} \end{equation} 
where $T={
\sum_{k=1}^{C} 1/C\log{1/C}}$ and $C$ is the number of the
families covered by the training data building the classifier. In this
experiment, $C$ is therefore set to 670.  For an input sample, the output of the family classifier is a 670-dimensional
probability-valued vector $\{p_{k}\}$ (k=1,2,3,...,C=670). Each $p_{k}$ gives
the probabilistic confidence that the sample belongs to the corresponding
family.  By definition, the numerator $\sum_{k=1}^{C} p_{k}\log{p_{k}}$ provides
the entropy of the classifier's output. The denominator $\sum_{k=1}^{C}
1/C\log{1/C}$ denotes the maximum entropy that the classifier's classification
output may have. %More specifically, the denominator gives the entropy achieved
%if every class has uniformly the same decision confidence. 
As a result, the magnitude of \textit{RES} is strictly normalized between 0 and 1. Higher/Lower
\textit{RES} denotes that the classifier shows higher/lower uncertainty level
over the classification output. 


%we
%introduce \textit{RES} to reflect the relative uncertainty level of the
%classification output, compared to the uniformly random classification output.
%The value range of the vanilla entropy is unbounded and increases / decreases
%globally as the number of the families $C$ becomes bigger / smaller. The
%normalized value of \textit{RES} is thus used to show how close the uncertainty
%level of the classifiers is to the purely random family classification output. 

In Figure~\ref{fig:entropyComparison}, we demonstrate the empirical cumulative
distribution function (CDF) of RES-based uncertainty distribution of the family
classifier's output on the testing malware samples of the 670 families
(in-distribution samples) and those belonging to singleton / unseen families. 
Consistently with theoretical studies~\cite{Liu2020nips}, we can find that the uncertainty level of the family classification output over the singleton and malware samples of previously unseen families increases significantly, compared to those derived with the testing samples sharing the same families of the training data. 

%Using the
%features of dynamic analysis,  the average, the median and variance of the
%\textit{RES} scores over the singleton malware samples are 0.46, 0.49 and 0.18.
%On the malware samples of unseen families, the average, median and variance of
%the \textit{RES} scores are 0.56, 0.62 and 0.19 respectively. 

%$\dots$ \juan{Explain it}


%In Figure.\ref{fig:entropyVsCorrectness},
%Figure.\ref{fig:entropyVsMaxProbability} and
%Figure.\ref{fig:maxProbabilityVsCorrectness}, we compare the RES-based
%uncertainity level of the family classifier with the correct classification
%rate and the max probability score output from the classifier. \yufei{What is
%the setup of these two figures? Need explanation and discussions}
%% Figure environment removed

%% Figure environment removed

%% Figure environment removed
%\fix{I mask the max prob vs percentage of correct prediction plot fmt. Any implications we need to discuss here}


%We are unable to measure the family classification accuracy over the singletons
%and malware of unseen families, as no ground truth of the family classes are
%provided. These malware samples have no overlapping with the 670 malware
%families used to train the family classifier. We apply the built family
%classifier over these malware samples as a problem of out-of-distribution (OOD)
%classification. As defined in \cite{Liu2020nips}, a machine learning model sees
%an input that differs from its training data in an OOD classification task.
%Classically machine learning methods assumes the i.i.d. assumption between
%training and testing data. However, the i.i.d. assumption hardly holds in
%practices. The drift between the training and testing data can render the sharp
%increasing of decision uncertainty of machine learning algorithms.
%\yufei{should we derive the average, median and variance of RES using static
%features as well?}

%\juan{for unseen families we are not sure that those we have tested on are real
%families because we don't have analyzed them. Maybe we could train a classifier
%on a subset of the 670 families and test on the remaining ones so that we are
%sure that those are families.}

\summary{8}{Our experiments confirm a significant performance drop in binary classification
    over out-of-distribution samples, both in the case of singleton and unseen
    families. At the same time, the confidence of the ML-based
    classifier decreases significantly over these out-of-distribution
    samples. This implies that ML-based models tend to be less certain over
    malware samples drifted from the training samples. 
    Our results also show that models trained on a very unbalanced dataset generalize
    more poorly, and that dynamic features generalize better than static over
    new families. Overall, as the distribution
    gap between training and testing malware samples is common in
    practice, these results raise concern over the utility of ML-based
    malware classification for real-world scenarios. } 
