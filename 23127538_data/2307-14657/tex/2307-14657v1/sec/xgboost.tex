\input{tbl/tbl_xgboost}

\subsection{XGBoost results}\label{sec:xgboost}
\revision{As for \textit{Random Forest} models, we tuned \emph{XGBoost} hyper-parameters
(i.e., learning rate, max tree depth, and the loss type) on a validation set
obtained for each experiment by setting aside 20\% of the samples. Results are
obtained by employing 10-fold cross validation, and by averaging partial
performance scores. Table~\ref{tbl:overallResultsXGboost} summarizes classification results for all
the experiments with this architecture. On average, \emph{XGBoost} classifiers
show 4.4\% and 8.2\% lower F1-score respectively for binary and family
classification. One potential explanation is that
Gradient Boosted trees do not use ensemble voting to deliver the decision.
Their design is similar to AdaBoost or RealBoost~\cite{Friedman2001aos}, which use a cascade
structure to approximate the gradient descent process to a generalized
linear model~\cite{NIPS1999_96a93ba8}. In this way, \textit{XGBoost}
intrinsically achieves low variance in its decision by trading it with
larger bias. If some of the features used by the early layers of Gradient
Boosted trees contain noise or contain misleading information for
classification, this bias will be propagated to the decision stage of the
model, which in turn cause final misclassification.
On the contrary, \textit{Random Forest} models deliver the decision using
the averaged - majority vote of the composing trees. Theoretically, this
approach has a decision output with low bias and low bias, as discussed in
Chapter 9.2.3 of \cite{hastie01statisticallearning}. Intuitively, even if some of the component
trees use ambiguous or noisy features to reach incorrect decision, the voting
output can offer compensation to the misclassification.}

