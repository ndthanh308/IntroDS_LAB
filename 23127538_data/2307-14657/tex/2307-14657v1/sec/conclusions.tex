\section{Final Recommendations} 
\label{sec:conclusions}

The goal of this work was to understand the key factors that influence the
performance of ML models for malware detection and family
classification. 
Based on our experimental results,
we can draw some general recommendations on the use of ML for
malware classification:

\begin{enumerate}[leftmargin=0.4cm]
\item[1.] 
 \revision{Ideally, experiments on malware classification (both binary and family) should
    be performed on hundreds of different families, each containing a sufficient
    and balanced number of samples. However, this is often difficult to 
    achieve in the malware field. Thus, we believe the contribution of our paper
    is not to simply re-state this obvious finding, but to provide for the first time
    a quantitative assessment of the impact of the lack of these
    characteristics on the classification results. For instance, we show that
    classifiers trained on a few
    families (like the ones using the popular Microsoft dataset) can provide
    misleadingly high accuracy scores while experiments conducted on unbalanced datasets
    tend to generalize poorly when tested over different distributions.\\
    Our findings can also be used to better understand and compare results reported
    in previous studies. For example, our results show that a family
    classifier with a F1 score of 0.89 over 600 families is likely
    better than a classifier with a score of 0.93 on 30 families.}

    % Experiments on malware classification (both binary and family) should be 
   % performed on hundreds of different families. Classifiers trained on a few
   % families (like the ones on the popular Microsoft dataset) can provide
   % misleadingly high accuracy scores.

% \item[2.] Each family in the dataset should contain a sufficient number of samples. 
   % In particular, results from experiment performed on very unbalanced
   % datasets (e.g., where many families only contain a handful of samples)
   % tend to generalize poorly when tested over different distributions.

\item[2.] Static features dominate detection and classification of samples from
   \textit{known} families, by relying on signature-like information extracted
   from sequences of bytes and opcodes. Packing, in its current widespread implementation,
   does not seem to have a considerable negative effect on this.
   The addition of dynamic features, which are much more time-consuming and
   error-prone to extract, has only a marginal impact on the classification accuracy
   and therefore its use should be carefully considered if the goal is to detect
   known families.
   However, static features are unable to capture samples from
   \textit{unknown} families, where instead models based on dynamic
   behavior show a better ability to generalize.
   \revision{Therefore, our findings suggest that \emph{today} static features alone 
       are sufficient for family classification, but a combination of static and dynamic
       features is probably preferable for binary classification.}

\item[3.] The performance of \textit{all} ML models drop drastically when tested on OOD samples.
    \revision{While the feature completeness and the regular update of the training data to cover new malware families are key to obtaining good classification accuracy,
   both of them are difficult to achieve in the real world. It is due to the data-driven nature of ML-based classification mechanisms. 
   The quality and coverage of training data play a core role in determining the classification performance. 
Beyond improving the quality of training data, our experiments suggest that the inclusion of dynamic features into the classification task can be used to alleviate the impact of the OOD issue. 
More specifically, we show that using dynamic features still allows us to successfully flag suspicious previously-unseen malware samples, even if with less accuracy and higher false positive rates in binary and family classification tasks.
}
\end{enumerate}

\revision{Our work opens several directions for future work. 
For example, we would like to explore 
how to mitigate the impact of missing features in dynamic analysis, 
e.g., through feature selection.
We also plan to analyze the reasons behind hard-to-detect families, 
which could be due to 
custom packers, 
benign functionality in the malware, 
generic families that cover different malware, or 
other reasons.}  

% The goal of this work was to understand the key factors that influence the performance of machine-learning models for malware detection and classification. 
% The study collected the largest dataset of Windows malware families to date, and used state-of-the-art models to conduct several experiments.
% The results showed that models trained on static features performed better than those trained on dynamic features, and that a joint set of features only provided a marginal improvement. 
% Additionally, no correlation was found between packing and classification accuracy, and missing behaviors in dynamically-extracted features negatively impacted model performance. 
% The study also found that a lower number of families to distinguish facilitated the prediction task and that a higher number of samples increased ground-truth richness and model accuracy. 
% Finally, the study found that models trained with a uniform distribution of samples as families better generalized on unseen data. 
% Overall, the study aims to shed light on open questions in the field of malware detection and classification, and to provide guidance for future research in this area.
