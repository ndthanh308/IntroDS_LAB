\subsection{Impact of Training Dataset Construction}
\label{sec:dataset_construction} 

This section evaluates the effect of the construction of the training dataset
on classification accuracy. We specifically investigate the impact of the size
of the training dataset, the variety of malware families represented, and the
uniformity of the sample-family selection.
To the best of our knowledge, the question of how diversity in terms of
families impact binary classification has not been studied before. 

To study this aspect we plot a number of heatmaps. In each experiment,
as described in Section.\ref{sec:methodology},
we reserved randomly 20 samples in each 
family for validation (e.g., hyper-parameter tuning) and we choose
$p$ samples from the remaining 80 samples and use them for training and testing.
To study the impact of number of available samples, we vary $p$ from 50 to 80. 
To study instead the impact of the number of different families in the dataset, we progressively
vary the number of
families involved in both binary and family classification from 70 to
670. For each combination of number of families and number of samples per family, we
conduct a 10-fold cross validation test and report the averaged F1 score in
the corresponding cell of each heatmap. 


% Figure environment removed

% Figure environment removed

% Figure environment removed


% The most
%likely reason is that binary classification does not require the associated
%family of the malicious samples.
%However, it is also possible to use samples labeled with a family to 
%perform binary classification, e.g., those in MOTIF or in our $M_B$ dataset.
%This raises the question as to how should sample selection happen in this 
%case.

Figure~\ref{fig:binary_static_f1_heatmap} and 
Figure~\ref{fig:binary_dynamic_f1_heatmap}
present heatmaps of the F1 score for binary classification, 
using static features and dynamic features respectively.
%The number of families in the training dataset 
%is gradually increased from 70 up to all 670 families.
%The number of samples per family used for training and testing 
%varies from 50 to 80. 
%It does not reach 100 because 20\% of the samples are used for validation.
%Each F1 score in the heatmap is an average of multiple sampling rounds
%such that all samples in $M_B$ are eventually used in the training dataset. 
%
Figure~\ref{fig:binary_combined_f1_heatmap} shows the heatmap for the 
combined model, for brevity only showing the variation with the 
number of families.
%
Figure~\ref{fig:multiclass_static_f1_heatmap},
Figure~\ref{fig:multiclass_dynamic_f1_heatmap}, and
Figure~\ref{fig:multiclass_combined_f1_heatmap} 
are similar but for the family classification task.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Training dataset size
Overall, the results indicate that
%for both tasks and 
%for both static and dynamic features show that, 
as the number of samples per family 
increases, the classification accuracy also increases.
The exception is for the binary classification using static features, 
where increasing the samples per family may cause a decrease in overall 
accuracy. 
For example, when using 50 samples for each of the 670 families 
the F1 score is 0.960, but when using 80 samples it 
slightly decreases to 0.958. 
However, the trend is different if we consider more families. 
We consider these very small changes as fluctuations due to the randomness of the sample selection process.
%
%\juan{Need to speculate here}
% One potential explanation is randomly picking a small fraction of 
% malware families, e.g. 70 and 170 out the 670 families, 
% may introduce random fluctuations to the testing accuracy. 
% Indeed, we can observe that the variation tendency of F1 scores 
% becomes more stable when the number of families for training is above 270. 
% We can witness similar tendency in family classification. 
% For instance, by using all the samples per family and increasing the 
% number of families from 70 to 670, 
% the F1 score of family classification drops from 0.906 down to 0.848 
% using static analysis features, and 
% from 0.751 to 0.657 using dynamic analysis features.

% Training dataset diversity
With respect to family diversity, 
the results confirm that the more families in the training dataset 
the more difficult their classification is.
% There is one exception with 60 samples in binary static/dynamic
As expected, the decrease in classification accuracy is more marked for the 
family classification task, 
where intuitively the higher the number of classes the more difficult the 
classification becomes.
The decrease is also more marked for the dynamic features than for the 
static ones, likely due to their lower discriminatory power 
as discussed in Section~\ref{sec:classification-results}.
% For example, by using 80 samples per family and increasing the number of 
% families from 70 to 670, the F1 score in 
% Figure~\ref{fig:binary_dynamic_f1_heatmap} and 
% Figure~\ref{fig:binary_combined_f1_heatmap} 
% decreases from 0.940 to 0.926 and from 0.985 to 0.948 respectively. 
%
% In Figure~\ref{fig:binary_static_f1_heatmap}, 
% the performance of using static features is relatively more stable than 
% those with dynamic features and the joint feature pool. 
% Nevertheless, in most of the cases, 
% more families involved in the training set can lead to lower 
% binary classification accuracy.

%In both both classification tests, higher diversity of malware families increases the difficulty of learning of the classifier. Consequently, the classification accuracy to become lower as the number of malware families increase. In parallel, the accuracy decreases as the number of malware samples per family rises in the test. The reason is less number of malware samples used per family provide less rich information to cover the malware features in the given family. Besides, we also measure the correlation between the F1 scores using static and dynamic features under the same setting of training data.  The overall pearson correlation statistics is 0.66 and the p-value is \textcolor{red}{p-value}. The results indicate that the classification performances of two feature sets are strongly correlated. It confirms the consistent impact of training data construction over the ML-based classifiers, regardless of used features. %used for training the ML-based classifiers. 

% \paragraph{Feature-level fusion of the features with static and dynamic analysis.} Compared to using either of static or dynamic analysis, the F1 scores using the combined feature sets is improved for a few families (e.g., the number of families is less than 270). With the number of families larger than 170, combing with dynamic features does not bring accuracy improvement compared to  using only static features. One explanation can be: more families involved in the test introduce more dynamic features containing missing feature values marked as NA / Null. These missing feature values reduces the descriptive power of the malware features generated from dynamic analysis. Therefore, when the number of families increases, the classification output using dynamic features becomes less informative. Instead, the classifier using static features dominate the decision. 


\paragraph{Non-uniform sampling.}
We also evaluate the impact of a non-uniform downsampling strategy
for binary classification. 
For this purpose, we mimic the distribution of the recently-proposed MOTIF dataset~\cite{motif}, 
which contain 3,095 PE malware samples from 454 families with an unbalanced 
distribution 
(e.g., the median is 3 samples per family and 
29\% of families have a single sample). 
%
We create a new dataset by applying the MOTIF distribution 
to $M_B$.
This new MOTIF-like dataset comprises 4,821 samples from all 
670 families with the following distribution:
29\% of the families are singletons, 
41\% have 2-5 samples,
12\% 6-10,
10\% 11-20,
4\% 21-30,
2\% 31-40,
1\% 41-50,
and 1\% has over 50 samples (up to 100). 

% Compared to uniformly selecting the same number of malware samples from each family to form the training dataset, this new dataset follows a highly skewed malware sample distribution. 

We use this to compare two sampling approaches: 
the \emph{uniform} approach (which is the one we adopted so far in the paper) where we keep a balanced number
of samples for each family, versus a \emph{nonUniform} approach, where we consider a real-world case in which
the number of available samples varies from one family to another, as captured by the MOTIF-like dataset.
Table~\ref{tbl:binary_accuracy} shows the results for both approaches and 
different feature sets. 
We could not identify any significant difference between the two approaches,
thus suggesting that training a classifier with a non-uniform amount of samples
does not significantly impact its performance, under the important assumption that
the testing dataset also follows the same distribution.

% Clearly, whether or not malware samples are uniformly sampled 
% has much less impact on the performance, compared to varying the family numbers.
% The explanation behind can be: the intra-class difference among malware
% samples of the same family is much much less than the inter-class
% difference between different families. Therefore,  more malware families
% used for training results in more complex malware feature distribution,
% which makes it more difficult to fit using ML models. By comparison,
% non-uniformly sampling malware samples, yet with the same coverage of
% malware families, doesn't change much the malware feature distribution.
% The non-uniform sampling strategy thus rarely reduce the classification
% performances. 

\summary{7}{
Increasing the number of malware families in the training set
makes the classification more complex and generally results in lower accuracy. 
While not surprising, this is very important because previous studies were often 
performed on only a few dozens of families, with the risk of reporting inflated
results that do not generalize to larger and more realistic datasets.

%
Increasing the number of samples per family can help to increase the 
classification accuracy, in particular for models based on dynamic analysis.
%
Finally, the choice between a non-uniform and a uniform downsampling strategy 
does not significantly affect the binary classification accuracy.
}
\input{tbl/tbl_binary_classificationReport}
