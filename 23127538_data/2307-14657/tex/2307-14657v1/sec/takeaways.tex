\section{Final Recommendations} 

The goal of this work was to understand the key factors that influence the
performance of machine-learning models for malware detection and
classification. 
Based on the results of the individual experiments that we conducted,
we can draw some general recommendations on the use of ML for
malware classification:

\begin{enumerate}
\item \revision{Ideally, experiments on malware classification (both binary and family) should
    be performed on hundreds of different families, each containing a sufficient
    and balanced number of samples. However, this is often difficult to 
    achieve in the malware field. Thus, we believe the contribution of our paper
    is not to simply re-state this obvious finding, but to provide for the first time
    a quantitative assessment of the impact of the lack of these
    characteristics on the classification results. For instance, we show that
    classifiers trained on few
    families (like the ones using the popular Microsoft dataset) can provide
    misleadingly high accuracy scores and experiments conducted on unbalanced datasets
    tend to generalize poorly when tested over different distributions.
    This can also be used to better understand and compare results reported
    in previous studies. For example, our results show that a family
    classifier with a F1 score of 0.89 over 600 families is likely
    better than a classifier with a score of 0.93 on 30 families.}

% \item Experiments on malware classification (both binary and family) should be 
%    performed on hundreds of different families. Classifiers trained on a few
%    families (like the ones on the popular Microsoft dataset) can provide
%    misleadingly high accuracy scores.

% \item Each family in the dataset should contain a sufficient number of samples. 
%    In particular, results from experiment performed on very unbalanced
%    datasets (e.g., where many families only contain a handful of samples)
%    tend to generalize poorly when tested over different distributions.

\item Static features dominate detection and classification of samples from
   \textit{known} families, by relying on signature-like information extracted
   from sequences of bytes and opcodes. Packing, in its current widespread implementation,
   does not seem to have a considerable negative effect on this.
   The addition of dynamic features, which are much more time-consuming and
   error-prone to extract, has only a marginal impact on the classification accuracy
   and therefore its use should be carefully considered if the goal is to detect
   known families.
   However, static features are unable to capture samples from
   \textit{unknown} families, where instead models based on dynamic
   behavior show a better ability to generalize.

\item The performance of \textit{all} ML models drop drastically when tested on OOD samples.
   Therefore, the completeness and the regular update of the training data 
   (which are both difficult to achieve in the real world) are key to
   obtain good results. ML models, especially if integrating dynamic features,
   might still be used to flag suspicious previously-unseen samples, but with 
   much less accuracy and higher false positive rates.
\end{enumerate}

