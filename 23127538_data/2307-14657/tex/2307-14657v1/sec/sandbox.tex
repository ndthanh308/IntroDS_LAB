\subsection{Sandbox}
\label{sec:sandbox}

We created a Windows 10 Pro 32-bit virtual machine with 2 CPUs (Intel Xeon Platinum 8160 @ 2.10GHz) and 2 GiB of RAM.
We installed some most popular apps from the Microsoft Store (like VLC, 7-Zip, and Telegram), and we populated the file system with documents, pictures, and other common types of files to resemble a legitimate desktop workstation that malware may identify as a more valuable target~\cite{miramirkhani2017spotless}.

Recent findings~\cite{maffia2021longitudinal,galloro2022systematical} about evasive behaviors in Windows malware show that the prevalence of samples that resort to evasive techniques had a slight but steady increase over the years.
Some results of the abovementioned works are diverse because they consider a different number of techniques: Galloro et al.~\cite{galloro2022systematical} report that 80\% of modern malware use at least one evasive technique, while Maffia et al.~\cite{maffia2021longitudinal} the 40\%.
However, both percentages are not negligible, and therefore we have based our analysis on the (only available) open-source code of \emph{JuanLesPIN}~\cite{JLP} by the authors of~\cite{maffia2021longitudinal}, which in turn is based on Intel PIN~\cite{intelpin}, a dynamic binary instrumentation (DBI) framework.
This tool is able to detect and mitigate common evasive techniques, thereby increasing the likelihood that malware detonates.
Unfortunately it does not support executables for 64-bit version of Windows, and for this reason we have decided to focus our study on malware targeting the 32-bit version, preferring a more reliable dynamic analysis that tries to observe all the behavior of the specimen under analysis.
We modified their tool to also monitor the Windows APIs and their arguments responsible for interacting with the following system's components: network, processes, services, registry, mutexes, file system, and dynamic loading of DLLs.
We provide the complete list of the APIs in Appendix~\ref{sec:monitoredAPIs}.

Then, as some participants of the interview by Wong et al.~\cite{yong2021inside} mentioned, we tested our analysis environment with Al-Khaser~\cite{alkhaser}; it is an executable that provides a list of several evasion techniques that the system is susceptible to.
\emph{None} of the techniques implemented in Al-Khaser are able to detect our environment nor our PIN tool.

Our PIN tool introduces an overhead that must be considered; thus, we set up a similar test performed by the same authors of JuanLesPIN~\cite{maffia2021longitudinal}.
We randomly selected 1000 malware samples from our dataset among those who: I) terminate the execution, II) resort to at least one evasive technique, and III) call at least 50 Windows API (the authors of~\cite{kuechler2021} use the same threshold to determine whether a specimen has detonated).
Then, we ran each sample with and without our instrumentation, and we measured the time between the beginning and the end of the execution.
Compared to the non-instrumented version, we obtained a percentage increase of: $\mu=125$, $\sigma=31$, $min=26$, $med=106$, $max=206$.
These results are in line with~\cite{maffia2021longitudinal}, and in the worst case the instrumented sample ran for less than five minutes.

A recent study by Kuechler et al.~\cite{kuechler2021} shows that the amount of code executed by malware samples tends to plateau after only two minutes, and very little additional information can be gained by running samples for more extended periods of time.
Therefore, considering the overhead introduced by our tool and state-of-the-art works that employ a DBI for malware analysis~\cite{polino2017measuring, maffia2021longitudinal,galloro2022systematical}, we decided to run each sample for a maximum of five minutes.

Our infrastructure is composed of 50 clones of the virtual machine described above that we orchestrate using Proxmox VE~\cite{proxmoxve}.
We also took a snapshot of the VM already turned on, and after the user had logged into the system.
We restore the snapshot at the end of the analysis of each sample in order to start the analysis with a clean environment.
It is worth mentioning that we stored all the virtual disk images and the snapshots in a RAM disk, and this trick allowed us to greatly improve performance, especially when restoring a snapshot.

Finally, following the best practices for malware experiments~\cite{rossow2012prudent}, each machine runs on a separate local network with no other machine connected, and we granted full internet access through a dedicated ADSL line of our institution to the samples for communicating with their control servers or downloading external resources.


