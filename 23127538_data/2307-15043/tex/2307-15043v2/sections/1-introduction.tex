\section{Introduction}

% Figure environment removed

Large language models (LLMs) are typically trained on massive text corpora scraped from the internet, which are known to contain a substantial amount of objectionable content.  Owing to this, recent LLM developers have taken to ``aligning'' such models via various finetuning mechanisms\footnote{``Alignment'' can generically refer to many efforts to make AI systems better aligned with human values. Here we use it in the narrow sense adopted by the LLM community, that of ensuring that these models do not generate harmful content, although we believe our results will likely apply to other alignment objectives.}; there are different methods employed for this task \citep{ouyang2022training,bai2022constitutional,korbak2023pretraining,glaese2022improving}, but the overall goal of these approaches is to attempt ensure that these LLMs do not generate harmful or objectionable responses to user queries.  And at least on the surface, these attempts seem to succeed: public chatbots will not generate certain obviously-inappropriate content when asked directly.

In a largely separate line of work, there has also been a great deal of effort invested into identifying (and ideally preventing) \emph{adversarial attacks} on machine learning models \citep{szegedy2014intriguing,biggio2013evasion,papernot2016limitations,carlini2017towards}.  Most commonly raised in computer vision domains (though with some applications to other modalities, including text), it is well-established that adding small perturbations to the input of a machine learning model can drastically change its output.  To a certain extent, similar approaches are already known to work against LLMs: there exist a number of published ``jailbreaks'': carefully engineered prompts that result in aligned LLMs generating clearly objectionable content \citep{wei2023jailbroken}.  Unlike traditional adversarial examples, however, these jailbreaks are typically crafted through human ingenuity---carefully setting up scenarios that intuitively lead the models astray---rather than automated methods, and thus they require substantial manual effort.  Indeed, although there has been some work on automatic prompt-tuning for adversarial attacks on LLMs~\citep{shin2020autoprompt,wen2023hard,jones2023automatically}, this has traditionally proven to be a challenging task, with some papers explicitly mentioning that they had been unable to generate reliable attacks through automatic search methods \citep{carlini2023aligned}.  This owes largely to the fact that, unlike image models, LLMs operate on \emph{discrete} token inputs, which both substantially limits the effective input dimensionality, and seems to induce a computationally difficult search.

In this paper, however, we propose a new class of adversarial attacks that can in fact induce aligned language models to produce virtually any objectionable content.  Specifically, given a (potentially harmful) user query, our attack appends an adversarial \emph{suffix} to the query that attempts to induce negative behavior. that is, the user's original query is left intact, but we add additional tokens to attack the model.  To choose these adversarial suffix tokens, our attack consists of three key elements; these elements have indeed existed in very similar forms in the literature, but we find that it is their careful combination that leads to reliably successful attacks in practice.
\begin{enumerate}
    \item \textsf{\textbf{Initial affirmative responses.}} As identified in past work \citep{wei2023jailbroken,carlini2023aligned}, one way to induce objectionable behavior in language models is to force the model to give (just a few tokens of) an affirmative response to a harmful query.  As such, our attack targets the model to begin its response with ``Sure, here is (content of query)'' in response to a number of prompts eliciting undesirable behavior.  Similar to past work, we find that just targeting the \emph{start} of the response in this manner switches the model into a kind of ``mode'' where it then produces the objectionable content immediately after in its response.
    
    \item \textbsf{Combined greedy and gradient-based discrete optimization.} Optimizing over the adversarial suffix is challenging due to the fact that we need to optimize over \emph{discrete} tokens to maximize the log likelihood of the attack succeeding.  To accomplish this, we leverage gradients at the token level to identify a \emph{set} of promising single-token replacements, evaluate the loss of some number of candidates in this set, and select the best of the evaluated substitutions.  The method is, in fact, similar to the AutoPrompt \citep{shin2020autoprompt} approach, but with the (we find, practically quite important) difference that we search over \emph{all} possible tokens to replace at each step, rather than just a single one.

    \item \textbsf{Robust multi-prompt and multi-model attacks.}  Finally, in order to generate reliable attack suffixes, we find that it is important to create an attack that works not just for a single prompt on a single model, but for \emph{multiple} prompts across \emph{multiple} models.  In other words, we use our greedy gradient-based method to search for a \emph{single} suffix string that was able to induce negative behavior across multiple different user prompts, and across three different models (in our case, Vicuna-7B and 13b~\cite{zheng2023judging} and Guanoco-7B~\cite{dettmers2023qlora}, though this was done largely for simplicity, and using a combination of other models is possible as well).
\end{enumerate}




Putting these three elements together, we find that we can reliably create adversarial suffixes that circumvent the alignment of a target language model.  For example, running against a suite of benchmark objectionable behaviors, we find that we are able to generate 99 (out of 100) harmful behaviors in Vicuna, and generate 88 (out of 100) \emph{exact} matches with a target (potential harmful) string in its output.  Furthermore, we find that the prompts achieve up to 84\% success rates at attacking GPT-3.5 and GPT-4, and 66\% for PaLM-2; success rates for Claude are substantially lower (2.1\%), but notably the attacks still \emph{can} induce behavior that is otherwise never generated.  Illustrative examples are shown in Figure \ref{fig:splash}.  Futhermore, our results highlight the importance of our specific optimizer: previous optimizers, specifically PEZ \citep{wen2023hard} (a gradient-based approach) and GBDA \citep{guo2021gradient} (an approach using Gumbel-softmax reparameterization) are not able to achieve any exact output matches, whereas AutoPrompt~\citep{shin2020autoprompt} only achieves a 25\% success rate, compared to our attack success rate of 88\%.



Overall, this work substantially pushes forward the state of the art in demonstrated adversarial attacks against such LLMs. It thus also raises an important question: if adversarial attacks against aligned language models follow a similar pattern to those against vision systems, what does this mean for the overall agenda of this approach to alignment?  Specifically, in modern computer vision systems, adversarial attacks are still an omnipresent phenomenon.  State-of-the-art methods to prevent adversarial examples~\citep{madry2018towards,cohen2019certified,leino2021globally} are almost never used in practice, owing to 1) their computational inefficiency, 2) the fact that they lead to substantial performance drops (in terms of ``clean'' model performance) in the resulting systems,  and 3) the fact that they can only prevent attacks usually against a narrowly-defined attack model.  It remains to be seen how this ``arms race'' between adversarial attacks and defenses plays out in the LLM space, but historical precedent suggests that we should consider rigorous wholesale alternatives to current attempts, which aim at posthoc ``repair'' of underlying models that are already capable of generating harmful content.

\paragraph{Responsible Disclosure.} Prior to publishing this work, we shared preliminary results with \mbox{OpenAI}, Google, Meta, and Anthropic. We discuss ethical considerations and the broader impacts of this work further in Section~\ref{sec:ethics}.