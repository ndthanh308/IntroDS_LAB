\documentclass[11pt]{article}
\usepackage[margin=1in,letterpaper]{geometry}
\usepackage{graphicx} %
\usepackage[bf,sf]{titlesec}
\usepackage{setspace}
\usepackage{titling}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[table,dvipsnames]{xcolor}
\usepackage{algorithm}
\usepackage{comment}
\usepackage[noEnd=True]{algpseudocodex}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{fancybox}
\usepackage{fancyvrb}
\usepackage{multirow}
\usepackage[colorlinks]{hyperref}

\usepackage{listings}
\lstset{
backgroundcolor=\color{lightgray},
frame=lr,
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\hypersetup{
  colorlinks   = true, %
  urlcolor     = RoyalBlue, %
  linkcolor    = RoyalBlue, %
  citecolor   = RoyalBlue %
}



\makeatletter
\renewenvironment{abstract}{%
    \if@twocolumn
      \section*{\abstractname}%
    \else %
      \begin{center}%
        {\sffamily \bfseries \abstractname\vspace{\z@}}%
      \end{center}%
      \quotation
    \fi}
    {\if@twocolumn\else\endquotation\fi}
\makeatother

\definecolor{lightgray}{gray}{0.95} %
\newenvironment{FVerbatim}
{\VerbatimEnvironment
  \setlength{\fboxsep}{0.1in}
  \begin{Sbox}
    \begin{minipage}{0.9\columnwidth}
    \begin{Verbatim}[commandchars=\\\{\}]}
{\end{Verbatim}
  \end{minipage}
  \end{Sbox}
  \begin{center}
    \fcolorbox{black}{lightgray}{\TheSbox}
  \end{center}
}





\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicloop}{\textbf{repeat}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\andy}[1]{\textcolor{blue}{#1}}
\newcommand{\textbsf}[1]{\textsf{\textbf{#1}}}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\argmin}{argmin}


\title{\textbsf{Universal and Transferable Adversarial Attacks} \\ \textbsf{on Aligned Language Models}}
\author{Andy Zou$^{1,2}$, Zifan Wang$^2$, Nicholas Carlini$^{3}$, Milad Nasr$^{3}$, \\
J. Zico Kolter$^{1,4}$, Matt Fredrikson$^1$ \vspace{2pt} \\ 
$^1$Carnegie Mellon University, $^2$Center for AI Safety, \\ $^3$ Google DeepMind, $^4$Bosch Center for AI \vspace{2pt} \\
}
\date{}

\begin{document}
\maketitle

\begin{abstract}


Because ``out-of-the-box'' large language models are capable of generating a great deal of objectionable content, recent work has focused on \emph{aligning} these models in an attempt to prevent undesirable generation.  While there has been some success at circumventing these measures---so-called ``jailbreaks'' against LLMs---these attacks have required significant human ingenuity and are brittle in practice. Attempts at \emph{automatic} adversarial prompt generation have also achieved limited success.  In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors.  Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.

Surprisingly, we find that the adversarial prompts generated by our approach are highly \emph{transferable}, including to black-box, publicly released, production LLMs.  Specifically, we train an adversarial attack suffix on \emph{multiple} prompts (i.e., queries asking for many different types of objectionable content), as well as \emph{multiple} models (in our case, Vicuna-7B and 13B).  When doing so, \textbf{\emph{the resulting attack suffix induces objectionable content in the public interfaces to ChatGPT, Bard, and Claude}}, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others.  Interestingly, the success rate of this attack transfer is much higher against the GPT-based models, potentially owing to the fact that Vicuna itself is trained on outputs from ChatGPT.  In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. 
Code is available at \href{https://github.com/llm-attacks/llm-attacks}{\texttt{github.com/llm-attacks/llm-attacks}}.
\clearpage


\end{abstract}


\input{sections/1-introduction}
\input{sections/3b-method}
\input{sections/6-benchmark}
\input{sections/7-experiments}
\input{sections/2-background}
\input{sections/8-conclusion}

\section*{Acknowledgements}
We are grateful to the Center for AI Safety for generously providing computational resources needed to run many of the experiments in this paper.
This work was supported by DARPA and the Air Force Research Laboratory under agreement number FA8750-15-2-0277, the U.S. Army Research Office under MURI Grant W911NF-21-1-0317, and the National Science Foundation under Grant No. CNS-1943016.

\bibliography{main}
\bibliographystyle{plainnat}

\newpage
\appendix
\input{sections/9-appendix}

\end{document}
