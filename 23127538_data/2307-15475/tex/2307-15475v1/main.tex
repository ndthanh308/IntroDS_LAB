% Formatting guidelines here:
% https://facctconference.org/2023/cfp.html
% https://authors.acm.org/proceedings/production-information/preparing-your-article-with-latex
% Note the cap of 14 pages!

\documentclass[manuscript]{acmart}

% \usepackage{authblk}
\usepackage{booktabs} % For formal tables
\usepackage[ruled]{algorithm2e} % For algorithms
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes.multipart}
\usepackage{caption}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
\setlength{\bibsep}{0.1pt}

\usepackage[all]{nowidow}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     %\usepackage[preprint, nonatbib]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
    }
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{bbm}
\usepackage{tikz} 
\usepackage{wrapfig}
%\usepackage[demo]{graphicx}
% \usepackage{floatrow}
%\usepackage[para]{footmisc}
\usepackage{enumitem}
\usepackage{framed}         % Framing model logs

\usepackage[suppress]{color-edits}
% \usepackage{color-edits}
\addauthor{ub}{red}
\addauthor{vc}{blue}
\addauthor{jk}{orange}
\addauthor{sw}{blue}
\addauthor{dj}{teal}
\addauthor{mb}{green}
\addauthor{kc}{purple}

%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}

\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\vTheta}{\boldsymbol{\Theta}}
\newcommand{\vgamma}{\boldsymbol{\gamma}}
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\tocite}{{\bf [CITE]}}
\newcommand{\xhdr}[1]{\textbf{#1.}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{\fls{}: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

%\begin{document}

% \usepackage[symbol]{footmisc}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\newcommand{\fl}{\texttt{FeedbackLog}}
\newcommand{\fls}{\texttt{FeedbackLogs}}

\definecolor{pastelGreen}{RGB}{206, 232, 227}
\definecolor{pastelBlue}{RGB}{198, 211, 239}

\usepackage[framemethod=TikZ]{mdframed}
\newenvironment{topbox}
{\begin{mdframed}[roundcorner=5pt, bottomline=false, backgroundcolor=pastelBlue]}
{\end{mdframed}}

\newenvironment{midbox}
{\begin{mdframed}[backgroundcolor=pastelBlue]}
{\end{mdframed}}

\newenvironment{bottombox}
{\begin{mdframed}[roundcorner=5pt, topline=false, backgroundcolor=pastelBlue]}
{\end{mdframed}}

\newenvironment{onebox}
{\begin{mdframed}[roundcorner=5pt,  backgroundcolor=pastelGreen]}
{\end{mdframed}}

% For ragged right columns in tables.
% https://tex.stackexchange.com/questions/12703/how-to-create-fixed-width-table-columns-with-text-raggedright-centered-raggedlef
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\setcopyright{none}

\begin{document}

\author{Matthew Barker}
\email{mrlb3@cam.ac.uk}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}

\author{Emma Kallina}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}
\affiliation{%
  \institution{Responsible AI Institute}
  \country{United Kingdom}
}

\author{Dhananjay Ashok}
\affiliation{%
  \institution{Carnegie Mellon University}
  \country{USA}
}


\author{Katherine M. Collins}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}

\author{Ashley Casovan}
\affiliation{%
  \institution{Responsible AI Institute}
  \country{United Kingdom}
}

\author{Adrian Weller}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}
\affiliation{%
  \institution{The Alan Turing Institute}
  \country{United Kingdom}
}

\author{Ameet Talwalkar}
\affiliation{%
  \institution{Carnegie Mellon University}
  \country{USA}
}

\author{Valerie Chen}
\email{valeriechen@cmu.edu}
\authornote{Both last authors contributed and advised equally. Order was decided by a coin flip. Correspondence to: \href{mailto:valeriechen@cmu.edu}{valeriechen@cmu.edu}  and \href{mailto:usb20@cam.ac.uk}{usb20@cam.ac.uk}}
\affiliation{%
  \institution{Carnegie Mellon University}
  \country{USA}
}

\author{Umang Bhatt}
\email{usb20@cam.ac.uk}
\authornotemark[1]
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}
\affiliation{%
  \institution{The Alan Turing Institute}
  \country{United Kingdom}
}

\renewcommand{\shortauthors}{Barker, et al.}

\begin{abstract}
Even though machine learning (ML) pipelines affect an increasing array of stakeholders, there is little work on how input from stakeholders is recorded and incorporated. We propose \fls{}, addenda to existing documentation of ML pipelines, to track the input of multiple stakeholders. Each log records important details about the feedback collection process, the feedback itself, and how the feedback is used to update the ML pipeline. In this paper, we introduce and formalise a process for collecting a \fl{}. We also provide concrete use cases where \fls{} can be employed as evidence for algorithmic auditing and as a tool to record updates based on stakeholder feedback.
\end{abstract}

\maketitle

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003456.10003457.10003490.10003507.10003509</concept_id>
       <concept_desc>Social and professional topics~Technology audits</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011074</concept_id>
       <concept_desc>Software and its engineering~Software creation and management</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003130.10003233</concept_id>
       <concept_desc>Human-centered computing~Collaborative and social computing systems and tools</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Social and professional topics~Technology audits}
\ccsdesc[300]{Software and its engineering~Software creation and management}
\ccsdesc[100]{Human-centered computing~Collaborative and social computing systems and tools}

\section{Introduction}
Stakeholders, who interact with or are affected by machine learning (ML) models, should be involved in the model development process~\citep{fails2003interactive,amershi2014power,cui2021understanding}.
Their unique perspectives, however, may not be adequately accounted for by practitioners, who are responsible for developing and deploying models (e.g., ML engineers, data scientists, UX researchers)~\citep{cheng2021soliciting}. 
We notice a gap in the existing literature around documenting how stakeholder input was collected and incorporated in the ML pipeline, which we define as a model's end-to-end lifecycle, from data collection to model development to system deployment and ongoing usage. 
A lack of documentation can create difficulties when practitioners attempt to justify why certain design decisions were made through the pipeline: this may be important for compiling defensible evidence of compliance to governance practices~\citep{bennett2017information}, anticipating stakeholder needs~\citep{zamenopoulos2007towards}, or participating in the model auditing process~\citep{mokander2021ethics}.
While existing documentation literature (e.g., Model Cards~\citep{mitchell2019model} and FactSheets~\citep{arnold2019factsheets}) focuses on providing \emph{static} snapshots of an ML model, as shown in Figure~\ref{fig:summary} (Left), we propose \fls{}, a systematic way of recording the \emph{iterative} process of collecting and incorporating stakeholder feedback.

The \fl{} is constructed during the development and deployment of the ML pipeline, and updated as necessary throughout the model lifecycle. While the \fl{} contains a starting point and final summary to document the start and end of stakeholder involvement, the core of a \fl{} are the records that document practitioners' interactions with stakeholders.
Each record contains the content of the feedback provided by a particular stakeholder, as well as how it was incorporated into the ML pipeline.  
The process for adding records to a \fl{} is shown in purple in Figure~\ref{fig:summary} (Right). Over time, a \fl{} reflects how the ML pipeline has evolved as a result of these interactions between practitioners and stakeholders. 

To explore how \fls{} would be used in practice, we engaged directly with ML practitioners. Through interviews, we surveyed the perceived practicality of \fls{}. Furthermore, we collected three real-world examples of \fls{} from practitioners across different industries. Each example \fl{} was recorded at a different stage in the ML model development process, demonstrating the flexibility of \fls{} to account for feedback from various stakeholders. The examples show how \fls{} serve as a defensibility mechanism in algorithmic auditing and as a tool for recording updates based on stakeholder feedback.

In summary, the main contributions of this work are:
\begin{enumerate}
    \item A new documentation structure, \fls{}, that captures the iterative process of collecting and incorporating stakeholder feedback (Sections \ref{sec:features_log} and \ref{sec:records}).
    \item Findings from practitioner interviews on the benefits and challenges of implementing \fls{} in practice (Section \ref{sec:practitioner_perspectives}) and an interactive demo tool to make \fls{} more accessible and easy to use for practitioners (Section \ref{sec:demo}).
\end{enumerate}

% discuss common issues which may be encountered by practitioners when using \fls{} (Section \ref{sec:practitioner_perspectives}). Finally, we release an interactive demo tool to make \fls{} more accessible and easy to use for practitioners (Section \ref{sec:demo}).

% Figure environment removed

\section{Overview of \fls{}}
\subsection{Background}
Prior work has focused on documentation that provides a snapshot of the ML pipeline at a specific stage of the ML lifecycle  (Figure~\ref{fig:summary} (Left)). We discuss a few, non-exhaustive, examples of these documentation strategies below. 
Model Cards describe how a model was developed, including who trained the model, when it was trained, and what data was used in the learning procedure along with details of model development and performance of the model on various metrics~\cite{mitchell2019model}. 
Similarly, FactSheets describe relevant information at each phase of the model's development: pre-training, during training, and post-training~\cite{8843893}. 
Explainability Fact Sheets summarize key features that lead a model to be more explainable~\cite{sokol2020explainability}. Reward reports~\cite{gilbert2022reward} frame an ML system as a reinforcement learning model, and record the decisions taken to optimise the system.
Application-specific documentation aims to contextualise more general techniques for use within the domain of interest.  For example, \textit{Healthsheet}~\cite{rostamzadeh2022healthsheet} is a questionnaire adapted from \textit{datasheets for datasets}~\cite{gebru2021datasheets} to improve accountability for data collection and usage in the health domain.  Unlike prior forms of documentation, we propose \fls{} which provide information on the \emph{iterative} process of eliciting and incorporating multiple stakeholder feedback throughout the model's lifecycle (Figure~\ref{fig:summary} (Right)).
To the best of our knowledge, this is the first work that introduces a systematic way to record how stakeholder feedback has been incorporated into an ML pipeline. 
We note that \fls{} can be used alongside existing documentation tools, which we describe further in Section~\ref{sec:incorp}.

The rise of participatory ML~\cite{paml} has resulted in the incorporation of feedback from a diverse set of stakeholders. This raises issues such as ``participation washing''~\cite{sloane2022participation} and a lack of clarity as to what is expected from stakeholders~\cite{birhane2022power}. \fls{} aim to clarify exactly what is expected from stakeholders and the effect of their participation. In addition to documenting model development, previous work has argued for a comprehensive understanding of the usage of a system, including algorithmic auditing~\cite{costanza2022audits,paleyes2022challenges} and critical refusal~\cite{garcia2022no}. By tracking the reasons for decisions prompted by feedback, \fls{} address the \emph{accountability gap}~\cite{raji2020closing} in the development of ML systems that elicit feedback from numerous stakeholders. A \fl{} provides more information than a one-off certification~\cite{henriksen2021situated} and captures the iterative development process rather than a static snapshot~\cite{shergadwala2022human}.

% for purposes of auditing ML pipelines~\cite{raji2020closing,paleyes2022challenges}.
% Uniquely, \fls{} can help auditors track which stakeholders provided feedback to the practitioner and how the practitioner went about incorporating said feedback. 
% This provides more information than a one-off certification~\cite{henriksen2021situated} and than a static snapshot of a pipeline~\cite{shergadwala2022human}.
% In the future, practitioners could leverage such tools for demonstrating which stakeholders were consulted and how their feedback affected the pipeline. \ubcomment{Can condense the last three sentences, feels v repetitive}

% The issues of trust and legal regulation when deploying ML models are well-established~\cite{paleyes2022challenges}. Existing techniques using explainability are currently unable to address these issues ~\cite{Selbst2018TheIA, bhatt2020explainable}. \fls{} take a different approach to trust, i.e. they aim at establishing trust through explaining the \emph{process} of developing an ML system, rather than explaining the \emph{outputs} of the model.

\subsection{\fl{} Components} \label{sec:features_log}
To motivate the design of \fls{}, we set out three desiderata which can be used to evaluate their value added to the documentation process.
\begin{enumerate}
    \item \textit{Completeness:} \fls{} should provide comprehensive details about stakeholder feedback and subsequent practitioner updates. %by the practitioner 
    %to be comprehensive and to satisfy regulatory requirements.
    \item \textit{Flexibility:} \fls{} should be able to be integrated into the ML pipeline at any point. \fls{} should also be able to handle the variability in the types and amount of stakeholder feedback as well as the types of updates a practitioner may consider.
    % \fls{} should be interoperable with other documentation and be modular, capturing any portion of ML pipelines. 
    \item \textit{Ease of Use:} \fls{} should come with minimal overhead for practitioners to adopt.
    %ion with minimal overhead.
\end{enumerate}

% Since the three desiderata can have conflicting requirements, achieving a suitable balance between them is challenging \vccomment{are they really that much in conflict?}. 
% The three sections of a \fl{} are designed to satisfy the three desiderata. 
%

% Figure environment removed

We propose a template-like design for \fls{} with three distinct components (shown in Figure~\ref{fig:fl_sections}): a \textbf{starting point}, one or more \textbf{records}, and a \textbf{final summary}. We describe both the starting point and final summary now and the records in greater detail in the subsequent section. To illustrate how \fls{} can be instantiated, we provide practical examples that have been completed by practitioners in Section~\ref{sec:example_logs}.


The \textbf{starting point} describes the state of the ML pipeline before the practitioner reaches out to any relevant stakeholders. 
The starting point might contain information on the objectives, assumptions, and current plans of the practitioner. More generally, a starting point may consist of descriptions of the data, such as Data Sheets~\cite{gebru2021datasheets}; metrics used to evaluate the models; or policies regarding deployment of the system ~\cite{wan2020human}. 
This component provides \emph{flexibility} since the \fl{} can capture any arbitrary starting point in the development process. 
A proper starting point allows auditors and practitioners to understand when in the development process the gathered feedback was incorporated, and defensibly demonstrates how specific feedback led to changes in the metrics.
% \vcedit{The first phase in generating a \fl{} is documenting the existing ML pipeline. 
% Many projects start engaging experts and other stakeholders for feedback even before the pipeline is created~\cite{wan2020human}. 
% In such cases, 
% }
%; later starting points may require a more detailed description of the current system. T


The feedback from stakeholders is contained in the \textbf{records} section, which can house multiple records. A single record logs how the stakeholder was requested for feedback, the stakeholder's response, and how the practitioner used the stakeholder input to update the ML pipeline. Figure \ref{fig:fl_sections} shows the structure of one record, which contains the elicitation, feedback, incorporation and summary sections for one source of feedback.
% the prompt given to the stakeholder when asking for feedback, the content of the feedback and the measured effect of the feedback after it was taken into account. Every record also has information on which previous records are relevant to the problem it addresses.} \vccomment{text in blue needs to be updated..could just be shortened and punted to the next section} 
Each record conveys enough information to satisfy \emph{completeness} while not being excessively burdensome to hinder \emph{ease of use}. 

The \textbf{final summary} consists of the same questions as the starting points, i.e. which dataset(s) and models are used after the updates, as well as the metrics used to track model performance. This component provides \emph{completeness} by encapsulating the net effect as a result of feedback from all the relevant experts. 
Proper documentation of the finishing point of the \fl{} allows reviewers to clearly establish how the feedback documented leads to concrete and quantifiable changes within the ML pipeline.
% This template design of \fls{} facilitates creation, enhances consistency for ease of review, and can be seamlessly integrated with the development process.
% Figure~\ref{fig:fl_sections} shows the three components of a \fl{}, which contains records logged between a starting point and a final summary. 
% The starting point and final summary are explored below, followed by a more complete description of the records in Section \ref{sec:records}. 
% Sections \ref{sec:starting_point_summary} \& \ref{sec:records})

\section{Records}
\label{sec:records}
Each record in a \fl{} is a self-contained interaction between the practitioner and a relevant stakeholder. It consists of how the stakeholder was requested for feedback (\textbf{elicitation}), the stakeholder's response (\textbf{feedback}), and how the practitioner used the stakeholder input to update the ML pipeline (\textbf{incorporation}). 


\subsection{Elicitation}
Every record in a \fl{} begins with a practitioner's request for feedback. Tracking how the request was made gives vital context for deciding on how to act on the advice~\cite{stumpf2007toward}, and surface potential downstream issues, such as use of leading prompts, omission of key information, or other problems in the feedback collection process.     

\paragraph{Which stakeholder(s) are being consulted and why?} There are many stakeholders who can provide feedback to improve models~\cite{suresh2021beyond}. 
Stakeholders may be internal to a practitioner's organisation (e.g., senior leadership, compliance officers, account executives) or external (e.g., regulators, auditors, review boards, end users)~\cite{bhatt2020machine,deshpande2022responsible}.
Acknowledging the stakeholder who was consulted is important to document in the feedback procedure, since credit attribution is key to responsible innovation~\cite{solaiman2019release,intelligence2021responsible}. Crediting the source of feedback also helps stakeholders gauge if and when their comments are incorporated into the pipeline~\cite{ouyang2022training, liang2022holistic}. Additionally, it may be important to document \emph{why} the particular stakeholder is being asked for feedback. For example, experts from different fields may be consulted to see whether something noteworthy (e.g., fairness considerations in a specific jurisdiction) has been overlooked. When many stakeholders are consulted for the same reason, as is the case in participatory ML, it is up to the practitioner's discretion whether each stakeholder should be in a separate record, or combined into the same record.


%What prompted the request for feedback?
%This is important to consider as a request for feedback that is crafted deliberately while 
% Considering the intended use of the feedback is more likely to lead to more fruitful interactions between practitioners and stakeholders. 
% For example, the stakeholder could help diagnose a specific problem. Many real-world considerations are difficult to fit into standard model diagnostic and evaluation methods~\cite{stumpf2007toward}. Consulting a stakeholder could confirm the presence of a certain problem in the pipeline or model, or provide insights on how the evaluation method to identify the problem could be designed. It depends on the context whether it is preferable to explicitly ask the stakeholder whether this problem is visible, or whether to avoid confirmation bias. On the other hand, the stakeholder might offer a novel perspective on the pipeline. organisations may consult experts from different fields to see whether something noteworthy (e.g., fairness considerations in a specific jurisdiction) has been overlooked. In this case, it may be useful to specify the model more generally than in cases where there are specific questions for the stakeholder to answer.

\paragraph{How is the relevant model information presented to stakeholders?} While acquiring stakeholder feedback over a series of interactions~\cite{lee2022evaluatingInteractive}, practitioners will need to decide on what information about a model should be shown to the stakeholder. The information should help the stakeholder develop an appropriate understanding of the current pipeline 
% When deciding how to solicit feedback from stakeholders, practitioners ought to consider the stakeholder's technical expertise and context-specific knowledge. 
Approaches to communicate such information include socio-technical details~\cite{gebru2021datasheets,mitchell2019model}, performance metrics~\cite{hunton201021st,raji2019actionable}, model explanations~\cite{chen2022interpretable,dodge2019explaining}, and confidence estimates~\cite{bhatt2021uncertainty,kay2016ish}. 
The content and presentation of model information will affect the stakeholder's downstream feedback~\citep{schnabel2018improving}. 
% Open-ended feedback may be more challenging to use than feedback on a set of choices, insofar as the feedback may have little to no implication on the model. 



  
% \begin{itemize}
%     \item \textbf{The stakeholder could help in diagnosing a specific problem.} Many real-world considerations are difficult to fit into standard model diagnostic and evaluation methods~\cite{stumpf2007toward}. Consulting a stakeholder could confirm the presence of a certain problem in the pipeline or model, or provide insights on how the evaluation method to identify the problem could be designed. It depends on the context whether it is preferable to explicitly ask the stakeholder whether this problem is visible, or whether to avoid confirmation bias. 
%     \item \textbf{The stakeholder's preference regarding multiple options is of interest.} Several solutions to address feedback may be equivalent with respect to objective metrics like loss or performance~\cite{wan2020human}. To identify a beneficial solution, it might be helpful to consult a relevant stakeholder that is less biased through technical knowledge. 
    
%     \item \textbf{The stakeholder might offer a novel perspective.} Very often organisations will choose to consult with experts from different fields not because there is something specific they are expecting to hear, but rather to see whether there is anything of note that the expert will raise given a particular ML pipeline. In this case, it may be useful to specify the model more generally than in cases where there are specific questions for the stakeholder to answer.      
% \end{itemize}


\subsection{Feedback} 
The content of feedback elicited from stakeholders is tracked in each record. Different stakeholders may tend to provide different kinds of feedback, and we illustrate examples below:
\begin{itemize}
    \item \textbf{End Users} are individuals who may be affected by the pipeline. 
    % Capturing their input on relevant features and potential interaction constraints is fundamental to understanding the requirements of an ML pipeline \vccomment{what is this sentence saying?}.
    End users can provide feedback on desired model behaviour or feedback on the issues with existing model behaviour. 
    For example, they might specify the kinds of behavior that a model should not exhibit (e.g., a model should not be able to generate hate speech~\cite{brown2020language, markov_2022}). 
    \item \textbf{Regulators} include compliance officers, internal review boards, and independent evaluators. Their feedback may include how to be compliant with regulations~\cite{voigt2017eu,european2020white}, policies~\cite{stafford2018role,cihon2021ai}, or industry standards~\cite{cihon2021ai,nativi2021ai}. These pieces of feedback would need to be translated into concrete actionable updates, which we soon discuss.
    %These individuals are important to understanding the legal constraints and obligations, as an ML pipeline needs to be in accordance with before it can be safely used. For example, compliance officers may be responsible for ensuring GDPR compliance or EU AI Act compliance; as a result, they are an important stakeholder for any ML pipeline deployed in these areas. 
    \item \textbf{Domain Experts} are individuals with prior experience and knowledge about the context of the ML pipeline. They may give practitioners auxiliary information that can be used to inform model development (e.g., feature attributions~\cite{weinberger2020learning}, style factors~\cite{adel2018discovering}, semantically-meaningful concepts~\cite{koh2020concept}). 
    % the task are often essential in learning the nuances of the problem that are very hard to infer for practitioners who have no direct experience of the field. 
    % This could be a doctor sharing the best practices during patient doctor interaction to help improve a conversational AI agent, a film director explaining the nuances of cinematography to an automatic video capture AI etc. 
\end{itemize}

% After capturing which stakeholder was chosen and how they were prompted for feedback in the \fl{}, practitioners can detail the insights that the stakeholder provided. Feedback may concern specific properties to which the stakeholder would like the model to adhere, potentially including jurisdiction-specific fairness definitions, compliance-mandated robustness certificates, or regulation-tied transparency requirements~\cite{bhatt2020explainable,bhatt2020machine,paml}.
% Feedback may also cover details for how instance(s) ought to be treated. For example, stakeholders may give practitioners contextual information, such that every data point could be accompanied by auxiliary information that can be used during learning (e.g., feature attributions~\cite{weinberger2020learning}, style factors~\cite{adel2018discovering}, semantically meaningful concepts~\cite{koh2020concept}). 
% Stakeholders can also provide feedback on model behavior, including specifying the kinds of behavior that a model should not exhibit (e.g., a model should not be able to generate hate speech~\cite{brown2020language, markov_2022}). 


% There are many other feedback forms that are less intuitive to elicit from a non-technical stakeholder~\citep{schoeffer2021ranking,wang2020deontological} but still plausible. This includes suggestions for changing the learning algorithm (e.g., in differential privacy communities~\citep{song2013stochastic,dwork2014algorithmic}), selecting hyperparameters (e.g., in AutoML research~\citep{li2017hyperband, li2021intermittent}), or specifying the order of data points given to a learning algorithm (e.g., in machine teaching literature~\cite{simard2017machine,liu2017iterative,wang2020mathematical}).


\subsection{Incorporation}
\label{sec:incorp}
Once stakeholders have provided feedback, practitioners can leverage their input to improve the model.
It is imperative to document the update process as there are many different ways (i.e. types of updates) in which a single piece of stakeholder feedback could be incorporated. 
% The documentation relating to incorporation includes how, where, and when the feedback was be considered. Those decisions will impact how effective the chosen update(s) are. In the \fl{}, practitioners can systematically track the various options considered and document the reasons why the eventually selected update was chosen because one piece of stakeholder feedback can be incorporated in many ways (i.e. types of updates). 
These updates to the ML pipeline can be largely clustered into \emph{model updates} or \emph{ecosystem updates}, which we now describe in more detail. 
% In Table~\ref{tab:updates}, we highlight example areas of such updates. 

\subsubsection{Model updates} It is often feasible to incorporate targeted feedback by making direct changes to the ML model. We focus our discussion on the common supervised learning setting, where a practitioner minimises a loss function on a dataset to learn a model that has many parameters, and any one of these aspects of the model could be changed in response to feedback provided. Common model updates include dataset, loss function, and parameter space updates (a more extensive list can be found in~\citet{chen2023perspectives}): 
\begin{itemize}
    \item \textbf{Dataset updates:} Feedback can be incorporated by adjusting the dataset of a model, i.e. by adding, modifying or removing data~\cite{dao2019kernel,xu2018fairgan,wan2020human}. In addition to active data collection~\cite{irvin2019chexpert}, dataset updates may take place in an unsupervised way~\cite{ratner2017snorkel,hertwig2009description,littlestone1994weighted,hannan1957approximation}.
    
    \item \textbf{Loss function updates.} Feedback can also be used to  update the loss function, thus changing the optimisation objective of the model. It is possible to add constraints to the model which may capture normative notions, such as fairness or transparency~\cite{zafar2017fairness,lakkaraju2016interpretable}, as well as practical considerations, like resourcing or robustness~\cite{frankle2018lottery}.
    
    \item \textbf{Parameter space updates.} Feedback can be incorporated by changing the architecture or features of the model~\cite{correia2019human, roe2020feature}, which affects the model parameter space. These updates traditionally require more technical users, although there are user-friendly interfaces developed to allow even non-technical experts to edit the model in a more direct manner~\cite{wang2021gam,yang2019study}, even in models with many billions of parameters~\cite{meng2022locating, massEdit2022, locateLLMs2023}.

    % \kcedit{\noindent \textbf{Prompt updates.} 
    
    % \kcedit{Precision edits of LLMs, for instance, of factual knowledge, are increasingly available }
    % Katie: there is also increasing ability to directly edit the params of even billion param models like LLMs. 
\end{itemize}

Implementing such changes to a model requires the practitioner to translate stakeholder feedback into a concrete update, which can be challenging. Not all updates naturally fit in this decomposition. For instance, in large language models \cite{brown2020language, bommasani2021opportunities}, the structure and context of the \textit{prompts} used to elicit generations can have a substantial impact on the model's output~\cite{wei2022chain, zhou2022least, zhou2022large}. Prompts are not necessarily ``data,'' nor parameters; however, their updates are worth tracking nonetheless and naturally fit within the purview of \fl{}.

\subsubsection{Ecosystem updates.} In many practical settings, making model updates only may be insufficient or ineffective to account for a piece of feedback, requiring modifications to the broader ecosystem. Here, ecosystem refers to the socio-technical realm in which the ML pipeline lives. We now describe parts of the ecosystem that can be altered upon receiving feedback.
% For example, a stakeholder may remind a practitioner to adhere to industry-specific standards, e.g., necessary testing requirements and explicit documentation. While such feedback might trigger alterations to the model, it most certainly would require thorough documentation (e.g. through Model Cards) to demonstrate adherence~\cite{mitchell2019model}. Other potential ecosystem updates include changes to the model's interface, accountability structure, or the way the pipeline should be deployed, including use cases and recommended oversight.

\begin{itemize}
    \item \noindent\textbf{Documentation.} Feedback can increase the need for documentation. For instance, if the practitioners are made aware of audit requirements (e.g. as outlined in the drafts of the EU AI Act~\cite{EuAIAct} and the Canadian AI and Data Act~\cite{CanadaAIRegulation}), then practitioners might be required to log aspects of the model and its development that have not been considered before. Such aspects could be an additional metric to include in the Model Cards, properties of the dataset that should be in the Datasheet, or a set of specifications that must be reflected in policy documentation. 
    \item \textbf{Interface or UX Updates.} Feedback from end users is essential to ensure a smooth user experience (UX)~\cite{xu2019toward}. Insights into their perception and usability issues with the interface are required to tailor it to their needs. Changes may include considering the perceived trustworthiness of the model~\citep{qin2020understanding}, the required level of interpretability of how the model arrived at a specific decision~\citep{suresh2021beyond}, or even the emotional relationship with a model~\cite{schweitzer2019servant}. These aspects are often addressed via interface changes (e.g. providing forms of explanation~\cite{weitz2019} or recourse~\cite{schnabel2020impact} oranthropomorphizing the model~\cite{zhang2021motivation}). 
    %However, this can also lead to updates to the model or the development process, e.g. to increase trustworthiness via specific model features such as ease of explanation \cite{toreini2020relationship}.
   
    \item \textbf{Accountability Structure.} Stakeholders might provide insights into risks that are inherent to a pipeline's use case. Whilst it could be difficult to directly incorporate such feedback into an ML model~\cite{stumpf2007toward}, it might prompt practitioners to identify appropriate strategies to address these risks. For instance, they could establish monitoring processes that detect the manifestation of such risks early on, paired with an action plan with clearly defined responsibilities \cite{CanadaAIRegulation}. This increased awareness would ensure that the practitioners are aware of the risks and their role in preventing potential harms \citep{schiff2020, rakova2021}. 
    
    \item \textbf{Deployment Details.} It may be appropriate to update the intended usage and scope of the pipeline. This includes details of scenarios in which the model is expected to function appropriately, scenarios that should be avoided (e.g. due to data or model drift), or the recommended level of human oversight (and the required expertise of the monitoring individual)~\cite{brundage2018malicious}. This could, for instance, be realized in a guidance document that is issued with the model - similar to a manual - that details the best practices of pipeline implementation and usage, as recommended in \cite{EuAIAct,CanadaAIRegulation}. Such guidance could include where and why pipeline failures may occur with a higher likelihood, how to prevent such failures, what data can and cannot be used in certain circumstances, and generally how to ensure optimal model operation~\cite{shergadwala2022human}. By outlining the context of proper system operation, the operators can quickly establish best practices.
\end{itemize}

Model and ecosystem updates are not necessarily exclusive, since both forms of update may be suitable for a given source of feedback. For example, a practitioner may change both a dataset and loss function, while also adding further details regarding best practices of model use. 
We note that some types of feedback 
(e.g., subjective or qualitative feedback) may be more difficult to translate into updates, which should be noted in the record.
% In this way, different strategies of presenting information to the stakeholder have various tradeoffs that must be considered before the solicitation of feedback. \citet{chen2022perspectives} discuss such strategies in depth.
The incorporation section of a record also tracks the following two aspects of the implemented updates:


\paragraph{At which stage of the ML pipeline is the update located?} The feasibility of updates is partly dictated by the current stage of the ML pipeline. Thus, the documentation of where in the pipeline an update is located is part of the justification for the choice of update. Common updates for each of the stages are described further:
%\paragraph{At which stage of the ML pipeline is the update located?} Documenting when an update was made may help to justify the choice of update because the set of feasible updates is dictated by the stage of the ML pipeline that an update is made \vccomment{pls edit wording here}. Common updates for each of the stages are described further:
\begin{itemize}
    % \item \noindent\textbf{Dataset and Information}: Updates made to the ecosystem or dataset before a model is changed. Adding or removing datapoints is an example of a change with affects this section of the pipeline. \mbcomment{Do we need both this and pre-training Dj? Changing datatpoints definitely seems like a dataset update.}

    \item \noindent\textbf{Data Collection (pre-training)}:  This is typically when updates are made to the ecosystem or to the dataset (e.g., adding data from underrepresented groups). 
    % Updates are made to the model right before it is trained. 
    Other updates might also include feature engineering or model class selection~\cite{turner1999conceptual}.
    \item \noindent\textbf{Model Development (training)}: This includes updates made to the optimisation or learning process of a model (e.g., adding regularization~\cite{zafar2017fairness}, importance weighting~\cite{littlestone1994weighted}, specialized fine tuning~\cite{wortsman2022robust}). 
    \item \noindent\textbf{Model Deployment (post-training)}: Even after the model has been developed, ecosystem-level updates (e.g., interface updates and changes to deployment details) can still occur. We note that the lifecycle of the ML pipeline is not linear; it may be necessary to return to earlier stages and consider their relevant updates.
    % For example, interface  in the pipeline after models have been learned. For example, ensemble methods~\cite{gao2021improving}, model selection methods~\cite{jang2022group}, explanations~\cite{hind2019ted}, and safety-based filters \cite{brown2020language, markov_2022} often occur at this stage.
\end{itemize}


\paragraph{How do we measure the impact of the update?} The final part of this section is a description of how the update(s) affected downstream metrics of interest that were spelled out in the starting point. 
To the extent possible, practitioners should explore performing individual updates, rather than implementing multiple updates simultaneously, to disentangle the isolated effects of the individual updates. 
% Updates may affect predictions, specific metrics, model explainability, or other relevant factors. 
% Tracking this is essential to understand the effect of a given update on the ML pipeline. 
This measurement can be used in comparing multiple updates to explain the reasoning for selecting from a set of updates, thus demonstrating that other alternatives were considered and ruled out for legitimate reasons. The practitioners may choose to refrain from implementing potential updates, making the justification for inaction in the \fl{} even more important.

% \vccomment{the following text should be worked into this subsection..} \mbcomment{What do you mean by this?} The information provided in the Update section of the \fl{} allows practitioners to isolate and list all of the adjustments that affected a specific part of the ML pipeline, such as providing a concrete and complete list of all the feedback points that inspired changes affecting the models loss functions. It is important to acknowledge that the \fl{} is not able to isolate the effect of an individual update on the starting point of the model. This is largely due to the nature of adjusting models in ML where multiple sequential changes to a model can have a very different net effect compared to applying these changes independently~\cite{stumpf2007toward}. 

\subsection{Summary}
Each record contains a summary of the updates that describes what updates were considered and what their effect was on the metrics of interest. Since each record section may consider multiple \emph{potential} updates, it is important to state which updates are ultimately implemented. 
% Although the measurable impact of each update is logged in the update section, the updates may have overlapping effects which cannot be disentangled when performing several updates simultaneously. 
To enhance readability, the summary should capture the impact of updates, while minimizing the amount of technical detail present about the specific update details.

\section{Towards \fls{} in Practice}
\label{sec:use_in_practice}

We intend to make \fls{} effective for real-world projects. The following section describes three steps that we undertook to bring the \fl{} concept closer to practice as well as to uncover considerations which could affect implementation and usage in real scenarios. First, we collected practitioner perspectives on the concrete implementation of \fls{}. Second, we created an open-source \fl{} generator to make the concept accessible to practitioners, as well as to ease the collection of practitioner feedback. Third, we completed example \fls{} based on consultations with practitioners working on ML pipelines.

\subsection{Practitioner Perspectives \& Future Developments}
\label{sec:practitioner_perspectives}
% We emphasise that \fls{} as described are a preliminary version. Whilst usable in its current form, we plan to refine the present concept in collaboration with practitioners to ensure that it fits their team structure and working practice. 

We conducted semi-structured interviews with three practitioners to gain insight into how \fls{} could be implemented in practice (see Appendix \ref{sec:methods} for the interview guide and details of the method). The responses are summarised below.

\textbf{Responsibilities.} All practitioners expected that a single person would be responsible for the completion of \fls{} for a specific system, i.e. the \textit{\fl{} owner}. This person might be the UX researcher, product manager, analyst, or engineering manager, depending on the type of feedback and development stage. The \fl{} owner would frequently draw on the expertise of other roles to provide input, e.g. on developers to establish the feasibility of technical updates, or the UX designer to propose potential UI solutions. Thus, future versions of \fls{} should have the ability to assign the completion of \fl{} sections to a specific role or person. 

\textbf{Timing of \fl{} Completion.} The timings of when to complete a \fl{} evoked varied responses from the practitioners. For smaller, more confined rounds of feedback collection as in the image recognition example below (Figure~\ref{fig:log_image_recognition}), a post-hoc completion by the analyst was deemed sufficient by a practitioner. However, they agreed that for feedback loops requiring more participating parties, the \fl{} should be filled out alongside the stakeholder involvement process to provide a common point of reference for everyone involved.%This resulted in the feature recommendation of being able to save progress in an incomplete \fl{} whilst still being able to share this incomplete version. 

\textbf{Expected Benefits of Implementing \fls{}.} The practitioners confirmed many of the benefits of \fls{} mentioned in the previous sections, e.g. the predefined structure that allows for fast information gathering and the benefits regarding audits, accountability, and transparency. The practitioners also suggested that \fls{} might improve communication and knowledge-sharing within organisations. One practitioner mentioned that the product team around the ML model was working with a different information management software than the technical team. They mentioned that this was especially true for A/B tests: the technical team members often had no context around why specific versions were developed and compared, and even lost track of the different versions themselves due to distributed and contradictory information. This resulted in communication issues. \fls{} could serve as a single source of truth that includes links to the other, more specific software. 
Additionally, an interviewee named \fls{} as a repository of past mistakes, solutions, and best practices. If an issue emerged, it could be used to trace the source of the issue as well as to identify past reactions to similar issues and the (long-term) effect of these reactions.%This is especially useful after changes in team members. A future version of \fls{} could support this via a `marking' feature that can be used to mark a (sections of) a \fl{} with labels such as `best practice', `avoid', or custom labels. 

\textbf{Expected Challenges of Implementing \fls{}.} The practitioners anticipated several challenges during the practical implementation of \fls{} that are listed below.

\textit{Log Access.} It is essential to consider who would be able view a \fl{}, amend it, and who would be able to assign these access rights. Since one of the main benefits of \fls{} is that they can increase transparency and accountability, we propose maximum internal viewing access with minimum edit rights. However, this should be customizable to the specific needs of a team. Thus, we plan to incorporate the ability to assign and restrict access in further versions of the \fls{}.

\textit{Scalability: Search and Linking \fls{}}. \fls{} will be created by different \fl{} owners along the entire ML pipeline. Additionally, large organizations often have numerous teams working on various ML models, each of which might require input from many stakeholders. Two practitioners mentioned concerns around organising \fls{} and establishing a structure between the individual entries. 
Future versions of \fls{} could address this concern via the ability to link and search \fl{} entries. 
In many cases, linking \fls{} is essential to trace decisions: For example, initial exploratory user research often scopes product requirements first. These are refined with further user research as well as consultations of the technical team regarding feasibility, both resulting in further \fls{} with more detailed technical requirements. The \fls{} of these different steps should be linked, so it is clear which insights prompted which technical solution. %In our next iteration, we could enable this via the ability to link a \fl{} to previously saved \fls{}. To make this practical, \fls{} have to be searchable. Future versions of \fls{} could enable practitioners to filter \fl{} entries by ML system, author, stakeholder, product feature, feedback point, reaction, or other keyword. Together with the ability of labelling different entries as `best practices', this would be a strong feature to ensure usability at scale. 

\textit{Logistical Trade-offs.} Completing a \fl{} involves a compromise between detail (e.g. the number of different incorporation strategies considered or the level of description of the final update) and labour. Two practitioners mentioned that it might be a nuisance for the \fl{} owner to chase the different required inputs from several team members. However, they agreed that future auditing processes will require detailed process logs for many systems. The current version of \fls{} already offers a high degree of flexibility regarding the depth and detail provided, allowing practitioners to complete it following the depth-labour balance that they deem fit. We plan to maintain and further develop this flexibility in future \fl{} versions.  % intensity of this process. 

\subsubsection{Summary}
The collected practitioner perspectives offered valuable insights into aspects of the \fls{} that could be improved to increase its fit within existing ML pipelines. In addition to the concerns mentioned by the practitioners, we identified three further challenges for practical applications of the \fls{}, given in Appendix \ref{sec:additional_considerations}. To facilitate the collection of stakeholder insights, as well as to make \fls{} accessible for first practical use cases, we introduce an online demo that allows for the quick generation of a \fl{}.

\subsection{FeedbackLog Demo}
\label{sec:demo}
To ease and encourage the adoption of \fls{}, we provide an open-source \fl{} generator\footnote{\url{https://feedback-log.web.app/}}. We acknowledge that this demo is a prototype, solely meant to illustrate the components of \fls{} and to gather feedback on how they may be incorporated into existing workflows. Our tool consists of two components: a web interface for stakeholders, practitioners, and auditors to interact with; and a command-line interface (CLI), shown in Figure \ref{fig:cli}, to enable practitioners to track updates at the source code level\footnote{Code available at \url{https://github.com/barkermrl/feedback-log}}. The \fl{} generator addresses the three desiderata described in Section~\ref{sec:features_log}:
\begin{enumerate}
    \item \textit{Completeness:} The tool covers the spectrum of possible update types: all feedback and ecosystem-level updates are logged in the web interface, while model-level updates are tracked by the CLI. 
    % This covers a spectrum of possible update types and allows auditors to examine the \fl{} at a high level with the possibility of low-level investigation when required. 
    \item \textit{Flexibility:} The web interface is designed to be ecosystem-agnostic, providing a universal interface that can be used alone or with other logging methods. At the time of writing, the CLI only supports Python~\cite{van1995python}.
    \item \textit{Ease of Use:} The web interface contains prompts for expert feedback and structures a \fl{} automatically. To ensure all feedback is incorporated, the CLI has a built-in checklist that consists of the \fl{} components that can be integrated into a practitioner's existing workflow.
\end{enumerate}

In the future, our tool can be extended to a richer interface with which \textit{both} stakeholders and practitioners can interact. This would ease the creation of -- and updates to -- \fls{}, as stakeholders could provide feedback within the tool and practitioners would update the pipeline using our CLI integration. Such a tool would also reduce the burden of maintaining a \fl{}.





% \vccomment{work this in: Third parties could use the \fl{} as a roadmap for reproducing the ML pipeline over the course of its development, should this be required.}

\subsection{Example Logs}
\label{sec:example_logs}

%provide example logs created after consultations with practitioners working on ML pipelines.
%\ubedit{ We close with a demonstrative synthetic log that captures the flexibility of \fls{} to make technical updates.} %\vccomment{mention the demonstration log?}.
\input{asthma}
\input{image_recognition}

We now walk through concrete examples of \fls{}: three \fls{} obtained from industry practitioners on ML pipelines  still in development and one demonstration log using a real dataset and model that shows a completed pipeline.

\subsubsection{FeedbackLogs in Industry.}

We collected \fls{} from three practitioners for ML pipelines that they are working on or have recently worked on. They were provided with a blank \fl{} template that they completed in their own time. More details on the methods can be viewed in Appendix \ref{sec:methods}. 
Since the practitioners chose ongoing projects, we refrain from providing the Final Summary section. 
Additionally, to avoid sharing specific information about proprietary ML models, these \fls{} focused on higher level pieces of feedback that practitioners have encountered. 
As such, more complete \fls{} in practice may be much lengthier or messier than the examples that we provide.
We describe the projects and the learnings from each \fl{}.

%rom various organisations that collaborate closely with the authors of this paper, which falls within an ethics-reviewed project approved by the University's Ethics Committee.
% The example \fls{} were obtained by practitioners working along ML pipelines, recruited via the researchers' personal networks. Thus, 
%Each practitioner, whose role is either a UX researcher and designer or data scientist, was provided with a blank \fl{} and asked to complete it for ML pipelines and stakeholder input they have encountered in their professional careers.
% All of the collected \fls{} are based on actual occurrences of stakeholder input obtained along the development process \vccomment{get rid of this sentence but not sure where to put }.
% All practitioners were employed either as a UX Designer or Data Scientist.
%All of these projects are ongoing, hence we do not collect a Final Summary for any of them. %Due to concerns about data privacy and intellectual property, we are unable to disclose the exact organisations or identities of the stakeholders.
% \vccomment{umang: how does this fall within the IRB, we need to mention.}

% \subsection{Learnings from Example Logs}
% The \fl{} concisely tracks the feedback incorporation for each of these different projects, showcasing the flexibility of the \fl{} design. 
%While each log only contains a few records as a proof of concept, the principles still apply when collecting feedback from a larger set of stakeholders.
\smallbreak
\noindent\textit{Asthma Conversation Agent:} This \fl{} describes the project of a national healthcare body to develop a conversational agent for asthma patients, operating via WhatsApp. The aim is to help patients with the management of their condition, including the prediction of the onset of asthma attacks. The \fl{} (Figure~\ref{fig:log_asthma}) contains two records that demonstrate how practitioners can track the needs of stakeholders. At the starting point, there was no statistical metric was defined by the practitioners; however, the log provides evidence that the metrics  eventually used in the project are informed by consultations with clinicians, who are domain experts on asthma. In case of an audit, practitioners can demonstrate how alternate methods of incorporating the feedback were considered, herein adding details to metrics or fine-tuning the model. The summary captures why a particular update (i.e., metric details) was selected.

\smallbreak
\noindent\textit{Recommender Systems:} Next, the \fl{} describes a model developed by a large streaming platform, aiming at increasing the user engagement of subscribed users. The \fl{} (Figure~\ref{fig:log_reco}) shows how the structure of the log is capable of capturing end-user needs and translating this feedback into a concrete UX update. This update manifests as the addition of a ``like'' button to gauge user preferences over repeated interaction, and improves the click-through rate metric used to measure performance for this application. 


\smallbreak
\noindent\textit{Sexual Health:} This \fl{} concerns the healthcare domain, focusing on sexual health. A national healthcare provider developed a model to automatically offer treatment for patients suffering from chlamydia symptoms, based on their answers to a questionnaire. The aim of the described stakeholder involvement was to identify accessibility and usability issues for vulnerable demographic groups, risking inaccurate treatment allocation. While both the previous records document feedback provided to projects where the ML pipelines are already set up, this \fl{} (Figure~\ref{fig:log_std}) captures changes that occur in the data collection phase before a model is even trained. The feedback collected from patients and psychologists informed  practitioners that their dataset collection must better accommodate individuals from vulnerable demographic groups. This log could be used as evidence to demonstrate how the organisation took into account the conditions of  vulnerable patients, who now have an alternative method for having their data collected in a way that minimises the risk of unrepresentative data. 
\smallbreak
The example \fls{} provided useful insights in the template's ability to represent the feedback collection and model updating process. The \fls{} concisely tracked the incorporation of feedback for each project, showcasing the flexibility of the \fl{} to describe changes to the pipeline at various stages. 

\subsubsection{Demonstration of a Complete FeedbackLog.}

While the three industry examples demonstrate how \fls{} can be used in the real-world, industry practitioners are prevented from sharing proprietary information about the exact models that are being developed.
As such, we provide a demonstration of a \textit{complete} \fl{}, which uses a real dataset and model, and includes details of technical updates. We consider a hypothetical scenario wherein a practitioner is developing an image recognition model for automotive vehicles. 


\smallbreak
\noindent\textit{Image Recognition:} This \fl{} (Figure~\ref{fig:log_image_recognition}) shows records that track non-technical, ecosystem updates as well as technical, model updates. In this case, two updates (to the parameter space and dataset) needed to be used simultaneously since no individual update was sufficient to meet the metric requirements. 
However, we note that individual updates are still tracked.
% The record also tracks individual updates. 
% By tracking each update in the record separately, the effect of each update can provide motivation for how one update was selected after multiple updates were considered. \ubcomment{this prev sentence needs work} 
This \fl{} contains a final summary, as the updates per the second record satisfy specified metrics. %vccomment{check this is correct}.
% tangible changes to the ML pipeline which can be evaluated and recorded. 
% For the model to be used in production, the practitioner may need to seek approval from external regulators. 
% By recording the interaction with one such regulator, the \fl{} provides the practitioner with evidence that they have consulted relevant experts when developing the ML pipeline. 


\section{Conclusion}
Stakeholder engagement is important to consider when deploying ML pipelines. However, even when stakeholders are consulted by practitioners, their feedback is rarely tracked and incorporated in a systematic manner. 
% Coupled with the increasing prevalence of regulatory standards which mandate proof of that stakeholders have been consulted (and their feedback incorporated) -- there is a crucial need for better documentation practices around stakeholder-practitioner exchanges. 
In this work, we propose \fls{}: a tool for practitioners to document the process of collecting and incorporating stakeholder feedback into the ML pipeline. 
\fls{} are designed to be complete, flexible, and easy to use.
% The \fl{} has been designed with practical use in mind. It strives to be complete, providing a detailed account of the exchange between practitioners and stakeholders. 
Through real-world examples, we demonstrate how \fls{} can record a wide variety of stakeholder feedback and capture the resulting updates made to ML pipelines. 
We hope \fls{} usher in the development of extensible tools for practitioners to empower the voice of a diverse set of stakeholders.


% Thus, they encourage practitioners to engage with stakeholders and to ensure that their feedback is systematically incorporated into the ML pipeline. 
 % This can encourage practitioners not only to engage with stakeholders, but to ensure that the obtained feedback is systematically incorporated into the ML pipeline.
%We hope \fls{} usher in more easy-to-use tools for practitioners to collaborate with and empower the voice of a diverse set of stakeholders.
%We hope \fls{} lower the barrier for practioners to engage with stakeholders  .... something about supporting dialogue/exchange We hope \fls{} lower the barrier to effective dialogue between stakeholders and practitioners. 

% We hope that \fls{} not only offer a crucial tool for practitioners -- but open the ground towards a more critical discourse around stakeholder involvement in the ML community

% The streamlined nature of \fls{}  

% richer, better documented cross-cutting exchanges between stakeholders and practioners throughout the ML development cycle.

% can lead to a more critical discussion about stakeholder involvement in the ML pipeline and how to best track the plethora of ways practitioners can leverage stakeholder feedback.

% We aim to encourage practitioners to capture and justify the actions resulting from the stakeholder feedback. 


% stakeholders' input is often overlooked -- and even  are consulted, their feedback is rarely tracked in a systematic manner. 


% \vccomment{too wordy!!}
% Increasingly, a growing body of research suggests that increased stakeholder engagement is crucial to deploying ML pipelines in practical settings. To date, many stakeholders are overlooked and do not feel involved in the development process. Even if stakeholders are consulted, their insights are often dispersed, making it challenging to track the effectiveness of their feedback on the ML pipeline. organisations using ML pipelines are also increasingly required to pass stringent regulatory requirements, where auditors require proof that relevant stakeholders and experts have been consulted and their feedback has been incorporated. Thus, we propose \fls{}, a framework for tracking the interaction between practitioners and stakeholders. We aim to encourage practitioners to capture and justify the actions resulting from the stakeholder feedback. 
% The \fl{} has been designed with practical use in mind. It strives to be complete, providing a detailed account of the exchange between practitioners and stakeholders. As we show with our real-world examples, \fls{} can flexibly track a wide variety of updates made to an ML pipeline -- from changes to the internal algorithms to the administrative systems that use the trained models. We hope that \fls{} can lead to a more critical discussion about how to involve stakeholders in the ML pipeline and how to best track the ways practitioners leverage stakeholder feedback. 
% Finally, \fls{} are easy to use - we provide a demo and give real-world examples demonstrating how it can be used to create defensible and auditable machine learning ecosystems.

\bibliographystyle{ACM-Reference-Format}
\bibliography{acmart}

\newpage
\appendix

\section{Additional Example Logs}
\input{tv}

\newpage
\input{sexual_health}

\newpage
\section{Command Line Interface (CLI) Usage}
% Figure environment removed

\newpage
\section{Practitioner Engagement: Methods}
\label{sec:methods}
The following section provides details to the method of the practitioner engagement steps described in section \ref{sec:use_in_practice}, i.e. the semi-structured interviews (section \ref{sec:practitioner_perspectives}) and the example \fls{} (section \ref{sec:example_logs}). Both steps were approved by the Ethics Committee of the University of Cambridge.

\subsection{Semi-Structured Interviews}
The ML practitioners for the semi-structured interviews were recruited via the personal or professional networks of the researchers. 
Each interview lasted between 45 and 60 minutes and was conducted via a video call. The three practitioners had different roles along the ML pipeline, i.e. UX researcher, developer, and engineering manager with varying levels of experience (from one year to over five years). 
The interviews followed an interview guide in a semi-structured manner. The guide included sections on (1) the practitioners' experience and role within ML, (2) their awareness and practices around current stakeholder involvement and their perception of this, (3) high-level feedback regarding the idea and usefulness of \fls{}, and lastly (4), feedback on the timings, responsibilities, and challenges they foresee when applying \fls{} in a specific scenario. There was time for the participants to ask questions and add additional thoughts. The fourth section was supported with a Miro board that displayed an empty \fl{} template. This template was used to discuss the order in which the different sections would be completed in practice, the responsibilities for completing the different sections, and the agency of the different roles in determining the content of these sections. 
The interviews were recorded, summarised, and analysed using thematic analysis~\cite{braun2012thematic}.

\subsection{Example FeedbackLogs}
As for the semi-structured interviews, practitioners that were consulted for the example \fls{} were recruited via personal and professional networks. Two practitioners worked as UX researcher and designer, the third practitioner was a developer. They had between three and nine years of experience in their role. The practitioners were provided with an online document that included the sections of the \fls{} as headers with a short description of the content that such a section would entail. Then, they were asked to complete each section for an ML project they are working on or have recently worked on. This could be done asynchronously in their own  time. The completed documents are the core of the example \fls{}, with slight edits and cuts to increase conciseness. 

\section{Additional Considerations}
\label{sec:additional_considerations}
In addition to the concerns mentioned by the practitioners, we identified three further challenges for practical applications of the \fls{}.

%\textit{Sharing of Sensitive Information.} Organizations that seek to demonstrate a high level of transparency might provide \fls{} for external viewing, while some may choose to keep the log internal and only provide it to relevant authorities when explicitly requested. The visibility of a \fl{} may raise concerns for practitioners if records speak negatively about the pipeline or if a stakeholder suggestion could not be addressed. Indeed, organisations may want to keep some records of a \fl{} private, as they may reveal vulnerabilities about a pipeline already in deployment. % that could be exploited by those who are aware of this shortcoming. 

\textit{Measurability of Impact.} Assessing the impact of an update implemented in response to stakeholder feedback can be challenging. Some updates have effects which are hard to define empirically, such as trust or accessibility. In such cases, practitioners could consider expanding the tracked metrics to give a more holistic picture of the pipeline and its objectives.

% Some updates are difficult to quantify, for example the `memorability of content' or `trust in the system'. In such cases, practitioners could estimate the change of metrics upon an update, or consider expanding the tracked metrics to give a more holistic picture of the pipeline and its objectives. Via the linking feature, future versions of \fls{} would also enable practitioners to evaluate the effect of a reaction to stakeholder feedback through a new round of stakeholder input.

\textit{Reproducibility.} If third parties rely on \fls{} to reproduce models and replicate a development process, it is essential that practitioners meticulously create and maintain their \fls{} with sufficient detail. For some pipelines, this may include the need to track which how much of the pipeline was procured from third-party vendors.
%\fls{} their systems inherit from external pre-trained models which may be used in the ML pipeline. 
For instance, if a practitioner fine-tunes a procured large language model \cite{brown2020language, bommasani2021opportunities} for a particular task, they should denote this in the \fl{} but also request thorough documentation of the base model.%Future versions of \fls{} could support practitioners in this task, e.g. via the linking and labelling feature mentioned above. 

\textit{Privacy.} Logging stakeholder feedback may make it possible to identify the stakeholder who provided the feedback. Care should be taken to ensure that stakeholders who may be identifiable have explicitly consented. Stakeholders  who have not consented should have their feedback recorded in a way that does not compromise their anonymity.%This could be supported via an `anonymize' feature in the \fl{} template, potentially with the ability to anonymize for particular viewers only.

% \newpage
% \appendix
% \section{Example Logs}
% % Figure environment removed

% % Figure environment removed



\end{document}