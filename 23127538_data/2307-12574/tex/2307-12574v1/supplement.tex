\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{subcaption}
\usepackage{makecell, multirow, tabularx}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{color}
\usepackage{amsmath}
\definecolor{hollywoodcerise}{rgb}{0.96, 0.0, 0.63}
\definecolor{lasallegreen}{rgb}{0.03, 0.47, 0.19}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
%\usepackage{hyperref}
\usepackage{amsmath}
% Include other packages here, before hyperref.
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\definecolor{hollywoodcerise}{rgb}{0.96, 0.0, 0.63}
\definecolor{lasallegreen}{rgb}{0.03, 0.47, 0.19}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
% \usepackage{hyperref}
% \hypersetup{colorlinks,linkcolor={red},citecolor={hanpurple},urlcolor={red}}  
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation

-Supplementary-}

\author{Jinjing Zhu$^{1}$
\quad
Yunhao Luo$^{3}$
\quad
Xu Zheng$^{1}$
\quad
Hao Wang $^{4}$
\quad
Lin Wang$^{1,2}$ \thanks{Corresponding author}
\and
\affmark[1] AI Thrust, HKUST(GZ)\quad
\affmark[2] Dept. of CSE, HKUST \quad
\affmark[3] Brown University\quad
\affmark[4] Alibaba Cloud, Alibaba Group\\
\quad
{\tt\footnotesize zhujinjing.hkust@gmail.com, devinluo27@gmail.com, zhengxu128@gmail.com,
cashenry@126.com, linwang@ust.hk}}

% Remove page # from the first page of camera-ready.
\maketitle

\begin{abstract}
Due to the lack of space in the main paper, we provide more details of the proposed method and experimental results in the supplementary material. Sec.\ref{detials} introduces the details of the proposed method. Sec.\ref{parameters} provides the details of the encoders used in this work. Lastly, Sec.\ref{algorithm} provides pseudo algorithm of the proposed method. Sec.\ref{discussion} shows some discussions about our proposed method.
\end{abstract}

\section{Details of the Proposed Method}
\label{detials}

Tab. \ref{tab:size} shows the architecture of MobileNetV2, ResNet-50, MiT-B1, and MiT-B2, respectively. We take the collaborative learning between MobileNetV2 and MiT-B1 as an example and present the details of our proposed method. 
\subsection{Heterogeneous Feature Distillation (HFD)}

The first-layer feature $F_{1}^{V}$ size of MobileNetV2 is 24$\times$128$\times$128 and the first-stage feature $F_{1}^{V}$ size of MiT-B1 is 64$\times$128$\times$128. To match the sizes of features, we utilize the linear transformations ${\Gamma}^{C}_{1}$ and ${\Gamma}^{V}_{1}$ to reshape the sizes of $F_{1}^{C}$ and $F_{1}^{V}$ as 64$\times$128$\times$128 and 24$\times$128$\times$128, respectively. Then, we can use the transformed feature to calculate the HFD loss as follow:

\begin{equation}
\small
\begin{aligned}
  &\mathcal{L}_{\text{HFD}}^{C}=cos({\text{Attn}}((F_{1}^{\hat{C}})),F_{2}^{V}), \\
  &\mathcal{L}_{\text{HFD}}^{V}=cos({\text{MLP}}({F_{1}^{\hat{V}}}),F_{2}^{C}),
\end{aligned}
\end{equation}
where $F_{1}^{\hat{C}}$ and $F_{1}^{\hat{V}}$ is the transformed feature, the shapes of which are 64$\times$128$\times$128 and 24$\times$128$\times$128, respectively. 
\subsection{Region-wise Bidirectional Selective Distillation}
The last-layer feature $F_{l}^{C}$ size of MobileNetV2 is 96$\times$64$\times$64 and the last-stage feature $F_{l}^{V}$ is 512$\times$ 16$\times$16. To match the sizes of features, we exploit the linear transformations ${\Gamma}^{C}_{l}$ and ${\Gamma}^{V}_{l}$ to reshape the sizes of $F_{1}^{C}$ and $F_{1}^{V}$ as 96$\times$16$\times$16 and 96$\times$16$\times$16, respectively. The transformed features are donated as ${F^{\hat{C}}_{l}}$ and ${F^{\hat{V}}_{l}}$, separately.
It is worth noting that the shapes of the predictions are 512$\times$512. To match the sizes of transformed features ${F^{\hat{C}}_{l}}$ (or ${F^{\hat{V}}_{l}}$ )and predictions $P^{C}$ (or $P^{V}$), we divide the prediction map into $16 \times 16$ size. A $\frac{512}{16} \times \frac{512}{16}$ sized prediction map $P^{C}$ (or $P^{V}$) at the same location corresponds to one region in ${F^{\hat{C}}_{l}}$ (or ${F^{\hat{V}}_{l}}$ ). Then we use the sum of cross entropy loss of $\frac{512}{16} \times \frac{512}{16}$ sized prediction map to decide the transferred direction between two students' regions with the same location.
Finally, the region-wise BSD loss is defined as
\begin{equation}
\small
\begin{aligned}
&\mathcal{L}_{R}^{C} = \frac{1}{16 \times 16-\hat{M}} \sum_{\hat{h}=1}^{16} \sum_{\hat{w}=1}^{16} (1-\hat{m}_{(\hat{h},\hat{w})})S_{(\hat{h},\hat{w})},\\ 
&\mathcal{L}_{R}^{V} =  \frac{1}{\hat{M}}\sum_{\hat{h}=1}^{16}\sum_{\hat{w}=1}^{16}   m_{(\hat{h},\hat{w})}S_{(\hat{h},\hat{w})},
\end{aligned}
\end{equation}
where $\hat{M}$ decides the direction of KD for each region  and  calculate the cross-student region-wise similarity matrix $S_{(\hat{h},\hat{w})}$ is the similarity matrix (as introduced in main paper).

\section{Parameters of Encoder}
\label{parameters}
Tab. \ref{tab:parameter} shows the parameters of encoder for different methods. For CNN-based students, we adopt the famous segmentation architecture DeepLabV3+ with encoders of MobileNetV2 and ResNet-50; for ViT-based students, we utilize the lightweight SegFormer with encoders of MiT-B1 and MiT-B2, which have comparable or smaller parameters with their CNN counterparts, respectively.
\section{Algorithm}
\label{algorithm}
The pseudo algorithm of the proposed method is shown in Algorithm. \ref{alg}.


\begin{table*}[t]
  \renewcommand{\tabcolsep}{5pt}
    \centering
    \begin{tabular}{l|c|c|c|c}
    \toprule
    Layer of MobileNetV2&First-layer $F_{1}^{C}$&Second-layer $F_{2}^{C}$&Third-layer&Last-layer $F_{l}^{C}$\\
    Output Size&24$\times$128$\times$128&32$\times$64$\times$64&64$\times$64$\times$64&96$\times$64$\times$64\\
    \midrule
    Layer of ResNet-50&First-layer $F_{1}^{C}$&Second-layer $F_{2}^{C}$&Third-layer&Last-layer $F_{l}^{C}$\\
    Output Size&256$\times$128$\times$128&512$\times$64$\times$64&1024$\times$32$\times$32&2048$\times$32$\times$32\\
    \midrule
    Stage of MiT-B1&First-stage $F_{1}^{V}$&Second-stage $F_{2}^{V}$&Third-stage&Last-stage $F_{l}^{V}$\\
    Output Size&64$\times$128$\times$128&128$\times$64$\times$64&320$\times$32$\times$32&512$\times$ 16$\times$16\\
    \midrule
    Stage of MiT-B2&First-stage $F_{1}^{V}$&Second-stage $F_{2}^{V}$&Third-stage&Last-stage $F_{l}^{V}$\\
    Output Size&64$\times$128$\times$128&128$\times$64$\times$64&320$\times$32$\times$32&512$\times$ 16$\times$16\\
    \bottomrule
    
    \end{tabular}
    \caption{Output size of each layer (stage) of different encoders. }
    \label{tab:size}
\end{table*}
\begin{table}[t]
    \centering
    \begin{tabular}{c|c|c}
    \toprule
    $Method$&Encoder&Parameters(M)\\
    \midrule
    DeepLabV3+&MobileNetV2&15.4\\
    SegFormer&MiT-B1&13.7\\
    DeepLabV3+&ResNet-50&43.7\\
    SegFormer&MiT-B2&27.5\\
    \bottomrule
    \end{tabular}
    \caption{The Parameters of methods with different encoder.}
    \label{tab:parameter}
\end{table}

\begin{algorithm}[]
	\caption{The Proposed framework} 
	\label{alg} 
	\begin{algorithmic}[1]
	    \STATE \textbf{Input}: $\{X,Y\}$; max iterations: $T$
	    \\ model: $f(X,\theta^{C})$, $f(X,\theta^V)$;
	    \STATE  \textbf{Initialization}: Set $\theta^C$ and $\theta^V$;
	    \FOR{for t $\xleftarrow[]{}$ 1 to $T$}
    	    \STATE Attain the segmentation prediction maps and feature representations for each student, respectively:
    	    \small $ \left(P^{\mathrm{C}}, F^{\mathrm{C}}\right)=f\left(X ;\theta^{\mathrm{C}}\right), \quad\left(P^{\mathrm{V}}, F^{\mathrm{V}}\right)=f\left(X ; \theta^{\mathrm{V}}\right);$
    	    \STATE Compute the pixel-wise segmentation loss for each student:
    	    
    	    $\mathcal{L}_{CE}^{C}=\frac{1}{H \times W} \sum_{h=1}^{H} \sum_{w=1}^{W} CE\left(\sigma\left({P^{C}}_{(h, w)}\right), y_{(h, w)}\right)$, \\
            $\mathcal{L}_{CE}^{V}=\frac{1}{H \times W} \sum_{h=1}^{H} \sum_{w=1}^{W} C E\left(\sigma\left({P^{V}}_{(h, w)}\right), y_{(h, w)}\right);$
            \STATE Compute the HFD loss for each student:
            
            $\mathcal{L}_{\text{HFD}}^{C}=cos({Attn}((F_{1}^{\hat{C}})),F_{2}^{V})$, \\
            $\mathcal{L}_{\text{HFD}}^{V}=cos({MLP}({F_{1}^{\hat{V}}}),F_{2}^{C});$
            \STATE Compute the region-wise BSD loss for each student:
            
            $\mathcal{L}_{R}^{C} = \frac{1}{\hat{H} \times \hat{W}-\hat{M}} \sum_{\hat{h}=1}^{\hat{H}} \sum_{\hat{w}=1}^{\hat{W}} (1-\hat{m}_{(\hat{h},\hat{w})})S_{(\hat{h},\hat{w})}$,\\ 
           $\mathcal{L}_{R}^{V} =  \frac{1}{\hat{M}}\sum_{\hat{h}=1}^{\hat{H}}\sum_{\hat{w}=1}^{\hat{W}}   m_{(\hat{h},\hat{w})}S_{(\hat{h},\hat{w})}$;
           \STATE Compute the pixel-wise BSD loss for each student:
           $\mathcal{L}_{\text{BSD}}^{C} = \mathcal{L}_{R}^{C} + \alpha \mathcal{L}_{P}^{C}$ , \\
          $\mathcal{L}_{\text{BSD}}^{V} =\mathcal{L}_{R}^{V} + \alpha \mathcal{L}_{P}^{V};$
          \STATE Compute the total objective for each student:
          
          $\mathcal{L}^{C}=\mathcal{L}^{C}_{CE}+\beta \mathcal{L}^{C}_{\text{HFD}}+\gamma\mathcal{L}^{C}_{\text{BSD}},$ \\
         $\mathcal{L}^{V}=\mathcal{L}^{C}_{CE}+\beta \mathcal{L}^{V}_{\text{HFD}}+\gamma\mathcal{L}^{V}_{\text{BSD}}.$
         \STATE Back propagation for $\mathcal{L}^{C}$ and $\mathcal{L}^{V}$;
         \STATE Update the students $\theta^C$ and $\theta^{V}$ with $\mathcal{L}^{C}$ and $\mathcal{L}^{V}$, respectively.
    	 \ENDFOR
    % 	 \eindent
	    \STATE  \textbf{return}  $\theta^C$ and $\theta^{V}$.
	    \STATE  \textbf{End}.
	\end{algorithmic} 
\end{algorithm}



\section{Discussion}
\label{discussion}

\subsection{Intuition of BSD}
The design of BSD is one of the critical contributions of this paper as it facilitates the two students to collaboratively learn reliable knowledge from each other and the knowledge is transferred bidirectionally. Due to the different performance at different regions between the ViT and CNN students, we intend to dynamically select reliable knowledge between the two students in the feature space, so as to benefit each other. However, there is a challenging problem: ‘how to decide the directions of transferring knowledge fro different regions during training?’ To this end, we propose to manage the directions of KD via combining the predictions and GT labels, where we regard the directions of KD for different regions as a sequential decision making problem. Consequently, we propose a directional selective distillation (BSD) for enabling students to learn collaboratively. As the principle of collaborative learning requires bidirectional knowledge transfer, BSD should be “bidirectional” to enable CNNs to learn from ViT while ViT learns from CNNs. Our key idea is “selective” due to the considerable model size gap and learning capacity gap between CNNs and ViT. The reasons causing the gaps are 1): The discrepancies in features and predictions between CNNs and ViT caused by the distinct computing paradigms make it challenging to do online KD. 2): These methods only transfer the knowledge in logit space; however, there is more reliable and efficient knowledge in the features extracted by both models. 3) There are considerable model size gap and learning capacity gap between CNNs and ViT.
\begin{table*}[t]
  \renewcommand{\tabcolsep}{5pt}
  \captionsetup{font=small}
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
    Method&MobileNetV2&MiT-B2&$\Delta$&ResNet-50&MiT-B1&$\Delta$\\
    \hline\hline
    Vanilla&67.54&82.03&0.00&76.05&78.48&0.00\\
    \textbf{Ours}&$\textbf{69.21}_{\textcolor{red}{+1.67}}$&$\textbf{82.27}_{\textcolor{red}{+0.24}}$&\textcolor{red}{\textbf{+1.91}}&$\textbf{77.59}_{\textcolor{red}{+1.54}}$&$\textbf{79.56}_{\textcolor{red}{+1.08}}$&\textcolor{red}{\textbf{+2.62}}\\
    \hline
    \end{tabular}
    \vspace{-5pt}
    \caption{Comparison with the Vanilla  methods on the \textbf{PASCAL VOC 2012} dataset for our CNN-based (MobileNetV2 and ResNet-50) and ViT-based (MiT-B1 and MiT-B2) students.}
    \label{tab:coma}
    \vspace{-10pt}
\end{table*}
\subsection{Intuition of HFD}

We make students learn the heterogeneous features from each other in the first-layer feature space and align these features in the second layer. That is, we input the transformed features into the second layer and then align the outputs instead of directly aligning features of the first layer. This way, it can make both students learn the global and local features in the first-layer space.

\subsection{Selection of Layers}

We use the first-layer features as low-layer features of CNNs and ViT are less distinct and heterogeneous, making CNNs and ViT learn from each other more effectively. Moreover, due to the different computing paradigms and learning capacities of CNNs and ViT, aligning high-layer features is less approachable and practical. Lastly, aligning multiple low-layer features lead to an increase in the computation cost. Tab. \ref{tab:coma} in the paper shows the effectiveness of our proposed method between heterogeneous students with different performance abilities.

\subsection{About MLP or Attn in HFD Module}
MLP consisting of convolutional layers extracts the local semantic features, and Attn consisting of a self-attention module extracts the global semantic features. Therefore, after inputting the local features into Attn or inputting the global features into MLP, these output features are comparable. As such, we use cosine similarity to measure the similarity of these features and enable students to learn from each other in the low-layer space.

\subsection{About Operations in Eq.\textcolor{red}{2}}
Attn updates the first-layer features of CNNs, while MLP updates the first-layer features of ViT. However, if we apply Attn operation in `CNNs to ViT” and MLP in “ViT to CNNs', Attn operation can optimize the first two layers of ViT while MLP operation can optimize the first two layers of CNNs. Both approaches can facilitate collaborative learning between CNNs and ViT but optimizing the first two layers increases computation cost.

\subsection{About ViT-ViT setting}
ViT is not absolutely better while CNN still matters; therefore, we explore to take full advantage of CNN and ViT while compensating for their limitations. Moreover, in Tab.\ref{tab:vit-vit}, our method demonstrates superior performance compared to previous studies in ViT-ViT setting.

\subsection{Results on ADE-20K:}
The effectiveness of our method is further demonstrated by the results obtained on the more challenging ADE-20K dataset, as shown in Tab.~\ref{tab:vit-vit}. The results will be included in the final version. 

\subsection{Distillation on hybrid network:} 
We explore the potential of our framework between the CNN-based (ViT-based) and hybrid network-based students, to further demonstrate its effectiveness in Tab.~\ref{tab:Results_camvid_resnet50}. The significant improvements \textbf{+7.59\%} and \textbf{+5.45\%} underscore the effectiveness and practicality of employing our proposed methodology within hybrid network architectures. 

\begin{table*}[t]
    \centering
    % \footnotesize
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
    \hline
    Method&& MiT-B1&MiT-B2&$\Delta$&& MobileNet&MiT-B1&$\Delta$\\
    \hline\hline
    Vanilla &\multirow{5}{*}{\rotatebox{90}{\textbf{CamVid}}}& 76.26&77.76&0.00&\multirow{5}{*}{\rotatebox{90}{\textbf{ADE-20K}}}&22.53&40.07&0.00 \\
    DML &&75.84&77.40&-0.78&&22.02&40.12&-0.46\\
    KDCL&&76.61&77.55&+0.14&&22.16&41.62&+1.18\\
    IFVD&&76.43&77.45&-0.14&&21.42&40.64&-0.54\\
    Ours&&\textbf{77.89}&\textbf{78.01}&\textcolor{red}{\textbf{+1.88}}&&\textbf{26.47}&\textbf{42.28}&\textcolor{red}{\textbf{+6.15}}\\
    \hline
    \end{tabular}%
    \caption{Comparison on the \textbf{CamVid} for MiT-B2 and MiT-B2 students, and  \textbf{ADE-20K} for MobileNetV2 and MiT-B1 students.
}
    \label{tab:vit-vit}
\end{table*}

\begin{table*}[t]
    \centering
    \begin{tabular}{|l|c|c|c||c|c|c|c|}
    \hline
    Method& ResNet-50&MaxViT&$\Delta$& MiT-B2&MaxViT&$\Delta$\\
    \hline\hline
    Vanilla &58.12&61.89&0.00&77.76&61.89&0.00\\
    DML &59.07&63.80&+2.86&77.09&60.61&-1.95\\
    KDCL&58.64&61.61&+0.24&77.49&63.26&+1.10\\
    IFVD&59.69&62.01&+1.69&77.08&63.10&+0.53\\
    Ours&\textbf{62.13}&\textbf{65.47}&\textcolor{red}{\textbf{+7.59}}&\textbf{77.96}&\textbf{67.14}&\textcolor{red}{\textbf{+5.45}}\\
    \hline
    \end{tabular}
    \caption{Comparison on the \textbf{CamVid} for ResNet-50(MiT-B2) and MaxViT students.
}
    \label{tab:Results_camvid_resnet50}
\end{table*}
% Figure environment removed
\subsection{About the motivation}
We argue that CNN is undoubtedly necessary for our problem setting. \textcolor{red}{\textbf{\textcircled{1}}} As ViT is notoriously impeded by limitations, such as the lack of certain inductive biases and poor performance on small-scale datasets; while CNN excels at capturing local features although CNN may underperform ViT on large-scale datasets. Therefore,  ViT is not absolutely better while CNN still matters, and it is promising to take full advantage of CNN and ViT while compensating for their limitations. From this new perspective, prior arts [1,2] adopting the CNN for an auxiliary purpose, are less optimal and intuitive. So, our motivation is reasonable and novel. Our key idea is to simultaneously learn compact yet effective CNN-based and ViT-based models by selecting and exchanging reliable knowledge between them for semantic segmentation. 
 \textcolor{red}{\textbf{\textcircled{2}}} Although `ViT is shown to have higher upper bounds than CNN', we observe in Figs.~\textcolor{red}{1}(b) and ~\textcolor{red}{4} that ViTs may exhibit less accurate segmentation results in certain regions compared to CNNs within the same image. To address this, we introduce BSD to compensate for students' weaknesses in region-wise and pixel-wise levels. We further demonstrate the effectiveness of our proposed method in collaborative learning between CNN-based (or ViT-based) and hybrid network-based students by conducting experiments as shown in Tab.~\ref{tab:Results_camvid_resnet50}.

\subsection{About 'reliable` knowledge in BSD}

Here, `reliable` does not indicate `regions', but \textit{indicates better predictions with relatively higher segmentation accuracy} (See Fig.~\ref{fig:bsd_rel}). Predictions in region $R_{1}^V$ ($R_{2}^C$) of ViT (CNN) is more reliable compared with predictions in region $R_{1}^C$ ($R_{2}^V$) of CNN (ViT). Then we utilize BSD to enable $R_{1}^C$ ($R_{2}^V$) to learn from $R_{1}^V$ ($R_{2}^C$). Finally, we obtain more accurate region predictions $\hat{R}_{1}^C$ ($\hat{R}_{2}^V$). \textit{BSD enables students to learn collaboratively and guarantees the correctness and consistency of soft label}. Qualitative results are in Tabs. \textcolor{red}{4, 5, 7}, and \textcolor{red}{9} (in main paper), and visualized results in Fig.~\textcolor{red}{4} specifically highlight the effectiveness of BSD.
% \section{Code Implementation}
% \label{code}
% In this section, we provide some demo implementation codes of our proposed framework including HFD, Region-wise distillation, and pixel-wise distillation.
% The complete version will be publicly available upon acceptance.
% \begin{lstlisting}[language=Python,title={Core.py}]
% import torch
% import torch.nn.functional as F
% import torch.nn as nn
% from torch.autograd import Variable
% import numpy as np

% ####HFD

% eva = torch.ones((4))
% for i in range (4):
%     ce1 = criterion_sup((pred_cnn[i,:,:,:]).reshape((1,21,512,512)), (s_gt[i,:,:]).reshape((1,512,512))) 
%     ce2 = criterion_sup((pred_vit[i,:,:,:]).reshape((1,21,512,512)), (s_gt[i,:,:]).reshape((1,512,512)))
%     if ce1>ce2:
%         eva[i]=0
    
% eval = eva.cuda()

% if torch.sum(1-eval)==0:
%     M1_cnn_loss = 0
% else:
%     M1_cnn_loss =torch.sum(torch.mul(cosine(pred_cnn_tp[2],(pred_vit_tp[1][0]).detach()),1-eval))/(torch.sum(1-eval))
% if torch.sum(eval)==0:
%     M1_vit_loss=0
% else:
%     if resnet50:
%         M1_vit_loss = torch.sum(torch.mul(cosine(pred_vit_tp[2], (cnn_features[0]).detach()),eval))/(torch.sum(eval))
%     else:
%         M1_vit_loss = torch.sum(torch.mul(cosine(pred_vit_tp[2], (cnn_features[3]).detach()),eval))/(torch.sum(eval))
        
% ### Region-wise distillation
% k_size = 32
% nelem_patch = k_size * k_size

% cemap1 = ce_lossNoRd(pred_cnn, s_gt).unsqueeze(1) 
% cemap2 = ce_lossNoRd(pred_vit, s_gt).unsqueeze(1)
% s_gt_usq = s_gt.unsqueeze(1).float()
% s_gt_usq = F.unfold(s_gt_usq, kernel_size=(k_size, k_size), stride=(k_size, k_size)) 
% s_gt_usq = s_gt_usq.permute(0,2,1) 

% s_gt_usqIs255 = torch.sum( (s_gt_usq == 255), dim=2 )  
% s_gt_usqIs255 =  (s_gt_usqIs255 >= (nelem_patch-3)) 
% s_gt_usqNot255 = ~ s_gt_usqIs255
% cemap1_uf = F.unfold(cemap1, kernel_size=(k_size, k_size), stride=(k_size, k_size)) 
% cemap1_uf = cemap1_uf.permute(0,2,1) 
% cemap2_uf = F.unfold(cemap2, kernel_size=(k_size, k_size), stride=(k_size, k_size)) 
% cemap2_uf = cemap2_uf.permute(0,2,1) 

% cemap1_uf_sum = torch.sum(cemap1_uf, dim=2) 
% cemap2_uf_sum = torch.sum(cemap2_uf, dim=2)

% cnn_betterIs1 = (cemap1_uf_sum <= cemap2_uf_sum) * s_gt_usqNot255  
% vit_betterIs1 = (cemap1_uf_sum > cemap2_uf_sum) * s_gt_usqNot255
            
% cnn_align_vit = 1 - cos_m2(CNN_new, ViT_new.detach())
% vit_align_cnn = 1 - cos_m2(CNN_new.detach(), ViT_new) 

% M2_cnn_loss = torch.sum(cnn_align_vit * vit_betterIs1) / (torch.sum(vit_betterIs1).detach() + 1e-7)
% M2_vit_loss = torch.sum(vit_align_cnn * cnn_betterIs1) / (torch.sum(cnn_betterIs1).detach() + 1e-7)

% ### Pixel-wise distillation
% M3_cemap1 = ce_lossNoRd(pred_cnn, s_gt) 
% M3_cemap2 = ce_lossNoRd(pred_vit, s_gt)
% M3_s_gt_usq = s_gt.float()
% M3_s_gt_usqIs255 =(M3_s_gt_usq == 255) 
% M3_s_gt_usqNot255 = ~ M3_s_gt_usqIs255
% M3_cemap1_uf = M3_cemap1 
% M3_cemap2_uf = M3_cemap2 

% M3_cnn_betterIs1 = (M3_cemap1_uf <= M3_cemap2_uf) * M3_s_gt_usqNot255  
% M3_vit_betterIs1 = (M3_cemap1_uf > M3_cemap2_uf) * M3_s_gt_usqNot255

% cnn_logsfmax = nn.functional.log_softmax(pred_cnn, dim=1).permute(0, 2, 3, 1)
% vit_logsfmax = nn.functional.log_softmax(pred_vit, dim=1).permute(0, 2, 3, 1)

% loss_kd = torch.sum(kl_loss(cnn_logsfmax, vit_logsfmax.detach()),dim=-1)
% M3_cnn_loss = 1*torch.sum(M3_vit_betterIs1*loss_kd)/(torch.sum(M3_vit_betterIs1*torch.ones((4,512,512)).cuda())+1)
% loss_kd2 = torch.sum(kl_loss(vit_logsfmax, cnn_logsfmax.detach()),dim=-1)
% M3_vit_loss = 1*torch.sum(M3_cnn_betterIs1*loss_kd2)/(torch.sum(M3_cnn_betterIs1*torch.ones((4,512,512)).cuda())+1)
% ce_cnn = criterion_sup(pred_cnn, s_gt)
% ce_vit = criterion_sup(pred_vit, s_gt)

% loss_cnn = ce_cnn + 0.1*M1_cnn_loss+ 1*M2_cnn_loss+1*M3_cnn_loss
% loss_vit = ce_vit + 0.1*M1_vit_loss+ 1*M2_vit_loss+1*M3_vit_loss

% \end{lstlisting}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}
