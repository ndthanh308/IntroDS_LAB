{
  "title": "Safety Margins for Reinforcement Learning",
  "authors": [
    "Alexander Grushin",
    "Walt Woods",
    "Alvaro Velasquez",
    "Simon Khan"
  ],
  "submission_date": "2023-07-25T16:49:54+00:00",
  "revised_dates": [
    "2024-10-10T01:29:24+00:00"
  ],
  "abstract": "Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monitoring deployed agents allows for the real-time identification of potentially catastrophic situations.",
  "categories": [
    "cs.LG",
    "cs.AI",
    "eess.SY"
  ],
  "primary_category": "cs.LG",
  "doi": "10.1109/CAI54212.2023.00026",
  "journal_ref": null,
  "arxiv_id": "2307.13642",
  "pdf_url": null,
  "comment": "2 pages, 2 figures. Presented at the 2023 IEEE Conference on Artificial Intelligence (CAI), Santa Clara, CA",
  "num_versions": null,
  "size_before_bytes": 1429915,
  "size_after_bytes": 951345
}