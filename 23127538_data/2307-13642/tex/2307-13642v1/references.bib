% Shorter references; enable with \bstctlcite{IEEEexample:BSTcontrol} in document after \begin{document}
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
CTLuse_forced_etal       = "yes",
CTLmax_names_forced_etal = "1",
CTLnames_show_etal       = "1" }

% Normal bib file

@ARTICLE{lin2017,
       author = {{Lin}, Yen-Chen and {Hong}, Zhang-Wei and {Liao}, Yuan-Hong and {Shih}, Meng-Li and {Liu}, Ming-Yu and {Sun}, Min},
        title = "{Tactics of Adversarial Attack on Deep Reinforcement Learning Agents}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
         year = 2017,
        month = mar,
          eid = {arXiv:1703.06748},
        pages = {arXiv:1703.06748},
archivePrefix = {arXiv},
       eprint = {1703.06748},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170306748L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{kumar2021,
  author={Praveen Kumar, R and Niranjan Kumar, I and Sivasankaran, Sujith and Mohan Vamsi, A and Vijayaraghavan, Vineeth},
  booktitle={2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Critical State Detection for Adversarial Attacks in Deep Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1761-1766},
  doi={10.1109/ICMLA52953.2021.00279}}

@inproceedings{guo2021,
 author = {Guo, Wenbo and Wu, Xian and Khan, Usmann and Xing, Xinyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {12222--12236},
 publisher = {Curran Associates, Inc.},
 title = {EDGE: Explaining Deep Reinforcement Learning Policies},
 url = {https://proceedings.neurips.cc/paper/2021/file/65c89f5a9501a04c073b354f03791b1f-Paper.pdf},
 volume = {34},
 year = {2021}
}

@ARTICLE{sun2020,
       author = {{Sun}, Jianwen and {Zhang}, Tianwei and {Xie}, Xiaofei and {Ma}, Lei and {Zheng}, Yan and {Chen}, Kangjie and {Liu}, Yang},
        title = "{Stealthy and Efficient Adversarial Attacks against Deep Reinforcement Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2020,
        month = may,
          eid = {arXiv:2005.07099},
        pages = {arXiv:2005.07099},
archivePrefix = {arXiv},
       eprint = {2005.07099},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200507099S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{huang2017,
       author = {{Huang}, Sandy and {Papernot}, Nicolas and {Goodfellow}, Ian and {Duan}, Yan and {Abbeel}, Pieter},
        title = "{Adversarial Attacks on Neural Network Policies}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
         year = 2017,
        month = feb,
          eid = {arXiv:1702.02284},
        pages = {arXiv:1702.02284},
archivePrefix = {arXiv},
       eprint = {1702.02284},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170202284H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{driels2004,
author = {Driels, Morris and Shin, Young},
year = {2004},
month = {04},
title = {Determining the Number of Iterations for Monte Carlo Simulations of Weapon Effectiveness},
number = {NPS-MAE-04-005},
institution = {Department of Mechanical & Astronautical Engineering, Naval Postgraduate School},
address = {700 Dryer Rd, Monterey, CA 93943-5000},
adsurl = {https://apps.dtic.mil/sti/pdfs/ADA423541.pdf}
}

@ARTICLE{meyes2019,
       author = {{Meyes}, Richard and {Lu}, Melanie and {Waubert de Puiseau}, Constantin and {Meisen}, Tobias},
        title = "{Ablation Studies in Artificial Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
         year = 2019,
        month = jan,
          eid = {arXiv:1901.08644},
        pages = {arXiv:1901.08644},
archivePrefix = {arXiv},
       eprint = {1901.08644},
 primaryClass = {cs.NE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190108644M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{woods2019,
       author = {{Woods}, Walt and {Chen}, Jack and {Teuscher}, Christof},
        title = "{Adversarial Explanations for Understanding Image Classification Decisions and Improved Neural Network Robustness}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
         year = 2019,
        month = jun,
          eid = {arXiv:1906.02896},
        pages = {arXiv:1906.02896},
archivePrefix = {arXiv},
       eprint = {1906.02896},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190602896W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{scott1979,
    author = {SCOTT, DAVID W.},
    title = "{On optimal and data-based histograms}",
    journal = {Biometrika},
    volume = {66},
    number = {3},
    pages = {605-610},
    year = {1979},
    month = {12},
    abstract = "{In this paper the formula for the optimal histogram bin width is derived which asymptotically minimizes the integrated mean squared error. Monte Carlo methods are used to verify the usefulness of this formula for small samples. A data-based procedure for choosing the bin width parameter is proposed, which assumes a Gaussian reference standard and requires only the sample size and an estimate of the standard deviation. The sensitivity of the procedure is investigated using several probability models which violate the Gaussian assumption.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/66.3.605},
    url = {https://doi.org/10.1093/biomet/66.3.605},
    eprint = {https://academic.oup.com/biomet/article-pdf/66/3/605/632347/66-3-605.pdf},
}

@ARTICLE{spielberg2018,
       author = {{Spielberg}, Yitzhak and {Azaria}, Amos},
        title = "{The Concept of Criticality in Reinforcement Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Statistics - Machine Learning},
         year = 2018,
        month = oct,
          eid = {arXiv:1810.07254},
        pages = {arXiv:1810.07254},
          doi = {10.48550/arXiv.1810.07254},
archivePrefix = {arXiv},
       eprint = {1810.07254},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181007254S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{spielberg2022, title={Criticality-Based Advice in Reinforcement Learning (Student Abstract)}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/21665}, DOI={10.1609/aaai.v36i11.21665}, abstractNote={One of the ways to make reinforcement learning (RL) more efficient is by utilizing human advice. Because human advice is expensive, the central question in advice-based reinforcement learning is, how to decide in which states the agent should ask for advice. To approach this challenge, various advice strategies have been proposed. Although all of these strategies distribute advice more efficiently than naive strategies, they rely solely on the agent’s estimate of the action-value function, and therefore, are rather inefficient when this estimate is not accurate, in particular, in the early stages of the learning process. To address this weakness, we present an approach to advice-based RL, in which the human’s role is not limited to giving advice in chosen states, but also includes hinting a-priori, before the learning procedure, in which sub-domains of the state space the agent might require more advice. For this purpose we use the concept of critical: states in which choosing the proper action is more important than in other states.}, number={11}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Spielberg, Yitzhak and Azaria, Amos}, year={2022}, month={Jun.}, pages={13057-13058} }

@INPROCEEDINGS{xu2021,

  author={Xu, Linling and Wu, Fenghua and Zhou, Yuan and Hu, Hesuan and Ding, Zuohua and Liu, Yang},

  booktitle={2021 China Automation Congress (CAC)}, 

  title={Criticality-Guided Deep Reinforcement Learning for Motion Planning}, 

  year={2021},

  volume={},

  number={},

  pages={3378-3383},

  doi={10.1109/CAC53003.2021.9728277}}

@ARTICLE{huang2018,
       author = {{Huang}, Sandy and {Bhatia}, Kush and {Abbeel}, Pieter and {Dragan}, Anca D.},
        title = "{Establishing Appropriate Trust via Critical States}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2018,
        month = oct,
          eid = {arXiv:1810.08174},
        pages = {arXiv:1810.08174},
          doi = {10.48550/arXiv.1810.08174},
archivePrefix = {arXiv},
       eprint = {1810.08174},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181008174H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}