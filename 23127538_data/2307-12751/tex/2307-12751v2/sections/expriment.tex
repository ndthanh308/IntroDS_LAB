

\section{Experiments}\label{sec:experiment}
%
We first introduce training and evaluation configurations of the proposed ICF-SRSR framework.
%
Then we conduct comprehensive experiments, extensive quantitative and qualitative comparisons with the other methods, and an in-depth analysis of our proposed method.

\subsection{Training details}
%
\Paragraph{Dataset.}
%
We train and evaluate our method on two scenarios. 1) Synthetic datasets, where the training and testing LR images are synthesized by a uniform degradation process~(\eg, bicubic down-sampling) from HR images. 2) Real-world datasets, which provide paired LR-HR images from the real-world captured by adjusting the focal length of a camera.

To train our ICF-SRSR, we use $800$ bicubic LR images from the DIV2K~\cite{agustsson2017ntire} dataset.
%
For evaluation, we adopt five standard benchmarks: Set5~\cite{bevilacqua2012low}, Set14~\cite{zeyde2010single}, BSD100~\cite{martin2001database}, Urban100~\cite{huang2015single}, and Manga109~\cite{Manga109}.
%
We also use the high-quality DIV2K validation set for evaluation.
%

To train and evaluate our ICF-SRSR under real-world scenarios, we utilize real-world datasets~\cite{cai2019toward, wei2020component} for the SISR task.
%
RealSR-V3~\cite{cai2019toward} includes paired LR-HR images captured by two different cameras, Canon and Nikon.
%
For each camera, about $200$ training images are captured from different scenes for each scaling factor $\times2$, $\times3$, and $\times4$.
%
We use only the LR images with scaling factors $\times2$ and $\times4$ for training and evaluate our method on the $50$ test pairs for each scale. 
%
DRealSR~\cite{wei2020component} also contains images captured by five DSLR cameras.
%
We conduct our experiments using images for $\times 2$ and $\times 4$ SR, containing 884 and 840 LR images, respectively.
%
For evaluation, we use $83$ and $93$ test pairs in DRealSR for $\times 2$ and $\times 4$, respectively.
\input{tables/real_data}
%
\Paragraph{Hyperparameters.}
%
During the training, we extract random patches of size $48\times48$ from LR images of both synthetic and real-world datasets.
%
For all our experiments, we set the batch size to $16$, and  $\lambda_{\text{Color}}=0.2$.
%
Random flip and rotation augmentations are applied to the input images to increase the number of effective training samples. 
%
We train our model using ADAM~\cite{kingma2017adam} optimizer with the initial learning rate $1 \times 10^{-4}$, which decays by a factor $0.5$ after every $200$ epochs.
%
For quantitative comparisons, we adopt structural similarity~(SSIM)~\cite{measure_ssim} and peak signal-to-noise ratio~(PSNR) on the luminance channel for the experiments on synthetic datasets and real-world dataset DRealSR~\cite{wei2020component} and also on RGB channels for dataset RealSR-V3~\cite{cai2019toward}.
%
All experiments are done using PyTorch 1.8.1 and Quadro RTX 8000 GPUs.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation on synthetic datasets}\label{sec:ex_syn}
%
We train our ICF-SRSR on the DIV2K~\cite{agustsson2017ntire} dataset with EDSR-baseline~\cite{lim2017enhanced} and test it on the public benchmark datasets~\cite{bevilacqua2012low, zeyde2010single, martin2001database, huang2015single, Manga109} and also the validation set of DIV2K.
%
We note that the proposed method is trained in a self-supervised manner by targeting a certain scale $s$.
%
Specifically, we train $\left( \times 2, \times \nicefrac{1}{2} \right)$ ICF and $\left( \times 4, \times \nicefrac{1}{4} \right)$ ICF independently.
%
\cref{tab:benchmark} shows extensive comparisons between the proposed self-supervised approach and the other representative supervised/unsupervised SR methods with PSNR metric.
%
We demonstrate that our ICF-SRSR approach achieves superior performance compared to the SelfExSR~\cite{huang2015single} model and comparable performance to the other unsupervised and supervised methods.
%
We note that ground-truth HR images in Set5 and Set14 are relatively noisier than the other datasets, preventing our self-supervised framework from learning accurate scaling functions.
%
We will discuss more details about the noisy cases in our supplementary material.
%
Notably, ICF-SRSR outperforms the unsupervised method ZSSR~\cite{shocher2018zero} by $1.05$dB on scale $\times2$ of Urban100 dataset and the supervised methods~\cite{kim2016accurate, chen2021learning} on both scales of DIV2K validation set.

Moreover, we apply the trained ICF-SRSR to LR images from the DIV2K training dataset and get LLR-LR paired images.
%
Then, we train off-the-shelf EDSR on the synthesized paired data from scratch and evaluate it on the test datasets as shown in \cref{tab:benchmark}.
%
The results demonstrate that EDSR~(LLR, LR) trained on our generated pairs~(LLR, LR) achieves superior performance than ICF-SRSR, which illustrates the merit of our method to generate useful training image pairs.
%

\cref{fig:benchmark} further visualizes the qualitative results of ICF-SRSR on two validation images from the DIV2K~\cite{agustsson2017ntire} dataset.
%
Our method achieves comparable results to the supervised methods~\cite{lim2017enhanced,chen2021learning} while restoring more details compared to the unsupervised methods~\cite{shocher2018zero,soh2020meta}. 
%
We note that the results on ZSSR~\cite{shocher2018zero} show lost information and scratched texts, and on MZSR~\cite{soh2020meta} include severe artifacts and color shifting.
%
For an in-depth comparison, we also provide quantitative results with SSIM metric in our supplementary material.
%

\subsection{Evaluation on real-world datasets}\label{sec:ex_real}
%
We train and evaluate ICF-SRSR for each scale $\times 2$ and $\times 4$ independently on the LR images of each Canon and Nikon camera from the real-world dataset RealSR-V3~\cite{cai2019toward} separately and also on the LR images of the real-world dataset DRealSR~\cite{wei2020component} in a self-supervised manner.
%
We further train the model EDSR~\cite{lim2017enhanced} on our generated~(LLR, LR) image pairs. 
%
We compare our method with the supervised methods~\cite{lim2017enhanced, wang2018esrgan, zhang2018image, cai2019toward, guo2020closed} trained on real paired images, which serve as the upper bounds for the SR problem.
%
\input{sections/figures/realsr2}

On the other hand, we employ the pre-trained supervised models EDSR~\cite{lim2017enhanced}, RRDB~\cite{wang2018esrgan}, IKC~\cite{gu2019blind}, BlindSR~\cite{sr_blindsr} and DRN-S~\cite{guo2020closed} on the synthetic DIV2K~\cite{agustsson2017ntire} dataset to super-resolve the LR images in the testing sets of RealSR-V3~\cite{cai2019toward} and DRealSR~\cite{wei2020component}.
%
Moreover, we utilize Kernel-GAN~\cite{bell2019blind} to approximate the down-sampling kernel from a single LR image and use ZSSR~\cite{shocher2018zero} as a zero-shot SR to apply to real LR images.
%
Our extensive comparisons with the various methods trained on real and synthetic datasets are summarized in \cref{tab:real_data}.
%
We illustrate that our self-supervised method can achieve superior performance compared to the methods pre-trained on the synthetic datasets and unsupervised method ZSSR~\cite{shocher2018zero}+Kernel-GAN~\cite{bell2019blind} in terms of both PSNR and SSIM metrics, which emphasizes the fact that the trained models on synthetic datasets with known degradations cannot perform well on real-world scenarios.
%
We qualitatively compare our method with the various existing methods on the RealSR-V3 dataset and visualize the SR results and their corresponding error maps with respect to the GT~(HR) in \cref{fig:realsr}. 
%
We demonstrate that our self-supervised method can achieve comparable and sometimes better performance to the supervised method LP-KPN~\cite{cai2019toward} trained on real paired images. 
%
We note that our method is generally more suitable for restoring the texture and preserving color compared to supervised method IKC~\cite{gu2019blind} and unsupervised method ZSSR~\cite{shocher2018zero}+Kernel-GAN~\cite{bell2019blind} as evident in appearance and PSNR, SSIM, and mean absolute error~(MAE) metrics.
%
We show more qualitative results in the supplementary material.
%
\subsection{Ablation study}\label{sec:ablation}
%
We conduct various ablation studies on the model design, down-sampling operators, few-shot learning, augmentation, and the effect of loss functions to better analyze our method.

\Paragraph{Model design.}
%
We conduct an experiment to show the superiority of a developed baseline as a single conditional model compared to two independent models and also the effect of training our two-stage framework compared to training each Up-Down and Down-Up stage separately.
%
Our results on synthetic dataset DIV2K~\cite{agustsson2017ntire} and Canon and Nikon images from real-world dataset RealSR-V3~\cite{cai2019toward} for scale $\times2$ show that training with two independent models or using only one stage~(half) results in unsatisfactory performance, demonstrating the uniqueness of our method in using a single invertible scale-conditional model as shown in \cref{tab:reb-two-models}.

\input{tables/reb_two-models.tex}
%

\Paragraph{Evaluation of down-sampling.} 
%
Due to the invertibility attribute of ICF, our method can be interpreted as a learnable down-sampler.
%
Therefore, we analyze our model $f_{\theta}$ as a down-sampling operator in three aspects.
%

\Paragraph{First.} We train ICF-SRSR on HR images from RealSR-V3~\cite{cai2019toward} and evaluate the model on HR images of the test dataset to gather the generated down-sampled images. 
%
Then, we compare ground-truth LR images with our generated LR images, as well as LR images obtained by down-sampling functions \eg, Nearest, Bicubic, Gaussian+Nearest, and Gaussian+bicubic~($\sigma=0.4$).
%
\cref{tab:downsampling} provides a comparison of LR images for different down-sampling models based on PSNR.
%
The values show the superiority of our learnable down-sampling method in generating more realistic LR images compared to ones with other down-sampling operators. 

\input{tables/downsampling.tex}

\Paragraph{Second.} 
%
We further analyze our learnable down-sampling operator $f_{\theta}$ compared to non-learnable down-sampling approaches.
%
We use our learnable down-sampling operator $f_{\theta}$, bicubic down-sampling, and Gaussian~($\sigma=0.4$) filtering followed by different nearest and bicubic down-sampling operators to generate the LLR images from given input LR images on the training sets.
%
Then, we train the model EDSR on the generated paired images~(LLR, LR) to learn generating SR images given LR counterparts.
%
We summarize the results for scale $\times2$ of the benchmarks Set5~\cite{bevilacqua2012low} and Set14~\cite{zeyde2010single}, and Canon and Nikon sets of RealSR-V3~\cite{cai2019toward} dataset for both non-learnable and our learnable down-sampling operators in \cref{tab:downsampling2}.
%
The results indicate the effect of our learnable down-sampling operator to generate appropriate image pairs for training, which results in a significant improvement compared to known down-sampling operators.

\input{tables/downsampling2.tex}


\Paragraph{Third.}
%
By using different down-sampling methods, we first generate LR samples from the real training HR images and then train a vanilla EDSR model using the generated pairs, \ie, (LR, HR).
%
As shown in \cref{tab:reb-down}, our synthesized pairs can provide more suitable training data compared to ones by previous learnable down-sampling methods ADL~\cite{son2021toward} and DRN-S~\cite{guo2020closed} as the EDSR performs much better for the $\times 2$ SR tasks on real dataset RealSR-V3~\cite{cai2019toward}.

\input{tables/reb_down.tex}


\paragraph{Few-shot learning.}
%
We train and evaluate our method on small datasets to show the advantage of our method to learning from only a few images without requiring a large-scale training dataset.
%
Therefore, we train the model ICF-SRSR~(Small) on the test sets of synthetic datasets Set14~\cite{zeyde2010single}, BSD100~\cite{martin2001database} and Urban100~\cite{huang2015single} and also real-world datasets RealSR-V3~\cite{cai2019toward} and DRealSR~\cite{wei2020component} and show their results on the corresponding test datasets in \cref{tab:ab-few-shot}. 
%
We demonstrate that our method can achieve slightly lower performance even when trained on very small datasets compared to our model ICF-SRSR~(Large) trained on large-scale training datasets.
\input{tables/few-shot.tex}

\Paragraph{Multi-scale augmentation.}
%
As we mention in \cref{sec:net_arch}, augmented data with different scales can lead to performance improvement.
%
Therefore, when we train ICF-SRSR directly on the test samples, we adopt diverse scaling factors as well as their reciprocals to compensate for the limited number of training data.
%
In \cref{tab:ab-scale}, we show that increasing the number of inputs induced by various scaling factors, \eg, $\times2$, $\times4$, and $\times8$, and their inverses can lead to obtaining superior performance on the RealSR-V3~\cite{cai2019toward} dataset.
%
More details about our multi-scale augmentation strategy are described in our supplementary material.

\input{tables/ab_scale}


\Paragraph{Effects of loss functions.}
%
\textcolor{blue}
We also analyze the effect of each loss function discussed in \cref{sec:loss}.
%
As shown in \cref{tab:ab-loss}, our novel self-supervised consistency loss $\mathcal{L}^{\text{Cons}}$ can drastically improve the model performance when it is added to color preserving loss $\mathcal{L}^{\text{Color}}$ on both synthetic and real-world datasets.
%
In our supplementary material, we further discuss the effect of the weight $\lambda_{\text{Color}}$.

\input{tables/ab_loss.tex}


