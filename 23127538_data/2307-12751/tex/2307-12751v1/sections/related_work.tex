\section{Related Works}
\label{gen_inst}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure environment removed
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%In this section, we review the related works into two branches: supervised SR methods and unsupervised methods.
%
In this section, we review recent SR methods from the perspective of training supervision.
%
\subsection{Supervised image super-resolution}
%
Starting from Dong~\etal~\cite{dong2014learning}, CNNs~\cite{dong2016accelerating, shi2016realtime} have become a standard for SISR.
%
Following VDSR~\cite{kim2016accurate}, several methods such as LapSRN~\cite{lai2017deep}, EDSR~\cite{lim2017enhanced}, and SRGAN~\cite{ledig2017photo} have leveraged benefits of residual learning.
%
Advanced approaches utilize dense connections~\cite{wang2018esrgan, zhang2018residual}, channel attention~\cite{ zhang2018image, dai2019second, niu2020single}, and back-projection~\cite{haris2018deep, haris2019recurrent}, and even Transformers~\cite{dosovitskiy2020image, Mei_2021_CVPR, chen2021pre, wang2021uformer, zamir2021restormer, liang2021swinir} for high-performance SR architectures.
%
Furthermore, recent attempts extend the task toward continuous scaling factors~\cite{sr_meta, sr_arb, chen2021learning, inr_siren} and even arbitrary shapes~\cite{sr_warp}.
%
%Conventional SR models have been developed under a fully-supervised framework.
%where LR inputs are synthesized from a set of HR images using known degradation, \eg, bicubic.
%
%Therefore, designing powerful architecture~\cite{Magid_2021_ICCV, Zhang_2021_ICCV} directly results in improving the quality of super-resolved images.
%
%Since the training LR-HR image pairs can be synthesized using known degradation functions, \eg, bicubic, it is straightforward to increase the number of training data by collecting large-scale high-quality images.

\iffalse
Recently, Vision Transformer~\cite{dosovitskiy2020image} with large-scale data has achieved significant success.
%
Such a trend has facilitated non-local self-attention~\cite{Mei_2021_CVPR} and Transformer architectures in the image restoration domain.
%
IPT~\cite{chen2021pre} is pre-trained on large-scale ImageNet~\cite{data_imagenet} first and then fine-tuned to downstream tasks such as SR, denoising, and deraining.
%
Uformer~\cite{wang2021uformer} and Restormer~\cite{zamir2021restormer} adopt U-shaped Transformer architecture for efficient implementation.
%
SwinIR~\cite{liang2021swinir} utilizes shifted window~\cite{liu2021swin} in ViT to deal with both local interactions and long-range dependency.
\fi

Nevertheless, supervised methods are still vulnerable when a given LR image is degraded by an unknown down-sampling function~\cite{son2021toward} that is not seen during training.
%
Therefore, several methods~\cite{gu2019blind, sr_blindsr, huang2020unfolding} jointly estimate latent kernel parameters and SR images to alleviate the issue.
%
Rather than up-sampling LR images directly, Correction filter~\cite{hussein2020correction} first converts a given input to resemble a bicubic down-sampled image and applies off-the-shelf SR methods.
%
%Still, they require supervision from synthetic LR-HR image pairs for training, which prevents the methods from being applied to real-world inputs~\cite{son2021toward}.
Still, they require supervision from synthetic LR-HR pairs for training, which prevents their real-world applications.
%

\subsection{Unsupervised super-resolution}
%
To reduce biases from synthetic training data, zero-shot methods are trained on a given LR input only, without relying on supervision from large-scale data.
%
Ulyanov~\etal~\cite{ulyanov2018deep} has shown that the structure of CNNs can be prior for natural image representation which can be utilized for the SR task.
%
Based on internal patch recurrence~\cite{michaeli2013nonparametric}, ZSSR~\cite{shocher2018zero} is trained on numerous sub-patches of the given image to construct an input-specific SR model.
%
Later, there has been an attempt to integrate external and internal learning using model-agnostic meta-learning~\cite{finn2017model}.
%
MZSR~\cite{soh2020meta} is firstly trained on a large-scale paired dataset with multiple degradation parameters and then adopted to a given image during the inference time.


However, the zero-shot methods assume that the degradation pipeline for a given image is known, which is less practical.
%
%In other words, they can synthesize feasible training pairs using the knowledge even without supervision from large-scale data.
%
To implement fully-blind SR methods, internal patch recurrence properties have played a critical role~\cite{michaeli2013nonparametric}.
%
Based on such a background, KernelGAN~\cite{bell2019blind} predicts a kernel that matches the distribution of the down-sampled image and the original input in an unsupervised manner.
%
The estimated kernel can also be utilized for several SR models~\cite{shocher2018zero, sr_srmd} for more accurate reconstruction.
%
Rather than explicitly utilize the concept of image distribution, we construct self-supervised chains to learn the SR model without assuming a specific degradation model.


\subsection{Cyclic architectures for super-resolution}
%
On the other hand, a class of methods interprets SR as a domain transfer problem between LR and HR distributions.
%
They introduce cyclic architectures~\cite{isola2017image} with adversarial loss~\cite{goodfellow2014generative, radford2015unsupervised, zhu2017unpaired} to train consecutive down-sampling and SR networks.
%
%The concept of Generative Adversarial Networks (GANs)~\cite{radford2015unsupervised} provides a feasible solution for such a problem, \eg, unpaired image-to-image translation~\cite{isola2017image, yi2017dualgan, zhu2017unpaired}.
%
CinCGAN~\cite{yuan2018unsupervised} utilizes the concept of cycle consistency to train the model on unpaired LR-HR images.
%
Under the cyclic framework~\cite{sr_deg, sr_gandeg, sr_unpaired_pesudo, sr_unsupervised}, down-sampling models are trained to simulate the distribution of training LR images.
%
Then, the following SR network can learn to generalize on given LR images even if the corresponding HR pairs do not exist.
%
However, they are still biased toward handcrafted down-sampling functions~\cite{son2021toward} and lack generalization.
%
Without using adversarial loss, Guo~\etal~\cite{guo2020closed} combine paired and unpaired data to train a dual regression network with a loop.
%
In this paper, we further propose a self-supervised approach without requiring either paired/unpaired training data or a specific down-sampling operator.


\subsection{Real-world super-resolution}
%
To overcome the limitations of existing methods when handling real-world data, several approaches have captured paired LR-HR images in the wild.
%
While they are still limited due to scene diversity~\cite{chen2019camera}, accurate alignment~\cite{cai2019toward, wei2020component}, real-world datasets help generalization of existing SR models with more practical training data.
%
Zhang~\etal~\cite{sr_zllz} and Xu~\etal~\cite{sr_real_raw} leverage RAW and RGB images together to deliver better reconstruction quality.
%
Nevertheless, those pairs require careful alignment and complicated hardware setup, which are not scalable.
%
Recently, Real-ESRGAN~\cite{wang2021realesrgan} and BSRGAN~\cite{zhang2021designing} aim to synthesize more realistic and diverse LR images to improve the generalization ability of existing SR models. 
%
Still, they cannot leverage information from real-world images and heavily depend on such a synthesis process.
%
On the other hand, our fully self-supervised framework does not require synthetic or real-world pairs and can be trained on arbitrary LR images.

