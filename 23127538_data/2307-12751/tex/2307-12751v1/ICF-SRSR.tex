\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{iccv}  
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).


\usepackage{xcolor}         % colors
\definecolor{ForestGreen}{rgb}{0.0, 0.27, 0.13}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\parens}[1]{( #1 )}
\newcommand{\bracket}[1]{\left[ #1 \right]}
%\newcommand{\normtwo}[1]{\left\lVert #1 \right\rVert_2^2}
\newcommand{\normtwo}[1]{\left\lVert #1 \right\rVert}

\newcommand{\Paragraph}[1]{\noindent \textbf{#1}}
% Include other packages here, before hyperref.
\usepackage{appendix}
\usepackage{titling}
    \pretitle{\vspace*{-3pt}\begin{center}\Large \bf}
    \posttitle{\vspace*{10pt}\par\end{center}}
    \preauthor{\large\begin{center}\par}
    \postauthor{\vspace*{6pt}\par\end{center}}
    \predate{}
    \date{}
    \postdate{}
% Support for easy cross-referencing
%\crefname{section}{Sec.}{Secs.}
%\Crefname{section}{Section}{Sections}
%\Crefname{table}{Table}{Tables}
%\crefname{table}{Tab.}{Tabs.}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{xcolor}  % colors
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{enumerate}
\usepackage{comment}
\usepackage{stackengine}
%\usepackage{gensymb}
\usepackage{pifont}
\usepackage[accsupp]{axessibility}
\usepackage{tabularx}
\usepackage{multirow}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,citecolor=blue,bookmarks=false]{hyperref}
%\usepackage[pagebackref,breaklinks,colorlinks,]{hyperref}
\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,citecolor=blue,bookmarks=false]{hyperref}
\usepackage[capitalize]{cleveref}
\iccvfinalcopy % *** Uncomment this line for the final submission




%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\iccvPaperID{5616} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\fi

\begin{document}

\title{ICF-SRSR: Invertible scale-Conditional Function \textit{for}\\ Self-Supervised Real-world Single Image Super-Resolution}

\author{Reyhaneh Neshatavar$^{1}$\thanks{equal contribution} \qquad Mohsen Yavartanoo$^{1}$\samethanks \qquad Sanghyun Son$^{1}$ \qquad Kyoung Mu Lee$^{1,2}$ \\$^{1}$Dept. of ECE \& ASRI, $^{2}$IPAI, Seoul National University, Seoul, Korea\\
{\tt\small \{reyhanehneshat,myavartanoo,thstkdgus35,kyoungmu\}@snu.ac.kr}}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%





%\include{macro}

\maketitle
% Remove page # from the first page of camera-ready.
%\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\input{sections/abstract}
%%%%%%%%% BODY TEXT
\input{sections/introduction}
\input{sections/related_work}
\input{sections/method}
\input{sections/expriment}
\input{sections/conclusion}



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ICF-SRSR}
}

%\input{sections/appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendixpageoff
\appendixtitleoff

\begin{appendices}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Supplementary Material \textit{for} \\
ICF-SRSR: Invertible scale-Conditional Function \textit{for}\\ Self-Supervised Real-world Single Image Super-Resolution}

\author{Reyhaneh Neshatavar$^{1\ast}$ \qquad Mohsen Yavartanoo$^{1\ast}$ \qquad Sanghyun Son$^{1}$ \qquad Kyoung Mu Lee$^{1,2}$ 
\\$^{1}$Dept. of ECE \& ASRI, $^{2}$IPAI, Seoul National University, Seoul, Korea\\
{\tt\small \{reyhanehneshat,myavartanoo,thstkdgus35,kyoungmu\}@snu.ac.kr}}

\maketitle
%\renewcommand{\thetable}{S\arabic{table}}
%\renewcommand{\thefigure}{S\arabic{figure}}
%\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thesection}{S\arabic{section}}
\input{sections/figures/net_arch}



\input{sections/figures/multi}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Details of network architecture}
%
As described in Section~\textcolor{red}{3.4} of our main manuscript, our ICF-SRSR adopts EDSR~\cite{lim2017enhanced} as a baseline.
However, to handle both up-sampling and down-sampling operations with the same network, we slightly modify the tail part of the original EDSR architecture for each scaling factor, \eg, $\times2$ and $\times4$, and their inverses.
%
Figure~\ref{fig:supp_edsr} shows the original EDSR~(Figure~\ref{fig:supp_edsr}\subref{fig:supp_edsr_original}) and our modified EDSR~(Figure~\ref{fig:supp_edsr}\subref{fig:supp_edsr_developed}). 
%
We use the pixel-unshuffle operator to down-sample an input image and generate the corresponding LLR image.
%
For more stable optimization, we use the detach operator of PyTorch before passing the first outputs to the network again.


\section{Details of multi-scale augmentation strategy}
%
As we mention in Section~\textcolor{red}{4.4} of our main manuscript, we can generate images with various scaling factors, \eg, $\times2$, $\times4$, and $\times8$ and their corresponding inverses from a single LR input.
%
Figure~\ref{fig:supp_multi1} shows our multi-tail architecture, which introduces a tail for each of the scale conditions.
%by introducing a tail for each scale condition as shown in Figure~\ref{fig:supp_multi1}.
%
Then, we pass the generated output images of different scales to the model $f_\theta$ with their inverse scaling factors.
%
By doing so, we reconstruct the input LR image as shown in Figure~\ref{fig:supp_multi2}.
%
Accordingly, to train our model $f_\theta$ under such a configuration, we minimize the loss functions $\mathcal{L}^{\text{Cons}}$ and $\mathcal{L}^{\text{Color}}$ defined in Section~\textcolor{red}{3.3} of our main manuscript between the generated images and the input LR image.



\input{tables/supp_benchmark.tex}
\input{tables/supp_baseline.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Evaluation by SSIM}
We quantitatively show the results of our ICF-SRSR and EDSR~(LLR,LR) methods compared to other supervised and unsupervised methods trained on DIV2K~\cite{agustsson2017ntire} dataset and tested on the five standard benchmarks Set5~\cite{bevilacqua2012low}, Set14~\cite{zeyde2010single}, BSD100~\cite{martin2001database}, Urban100~\cite{huang2015single}, and Manga109~\cite{Manga109} by SSIM metric in Table~\ref{tab:supp-benchmark}.
%
According to the results, our method outperforms unsupervised method~\cite{huang2015single} on both scaling factors $\times2$ and $\times4$ and supervised method~\cite{chen2021learning} on scaling factor $\times2$ and is comparable with other methods.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation on baseline model}
We employ different models LIIF~\cite{chen2021learning}, EDSR~\cite{lim2017enhanced}, RDN~\cite{zhang2018residual}, and RCAN~\cite{zhang2018image} as the baseline of our ICF-SRSR framework.
%
In the case of EDSR, RDN, and RCAN, we develop the original network architecture to generate multi-scale images by applying a tail for each scaling factor $s$ and its inverse $\nicefrac{1}{s}$, individually.
%
In the case of LIIF, we leverage its continuous attribute to generate any scale of images by sub-sampling from the reconstructed continuous image.
%
Table~\ref{tab:supp-baseline} shows the results of our ICF-SRSR with different baselines.
%
We illustrate that our method is model-agnostic and can leverage different state-of-the-art~(SOTA) baseline models.
%
We note that our method can achieve better performance using advanced baselines except LIIF, which is not trained with continuous scales due to the limitation of the color loss $\mathcal{L}^{\text{Color}}$.
%
We select the model EDSR as our baseline due to its training time efficiency. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation on the hyperparameter $\lambda_{\text{Color}}$.} 


We conduct an ablation study to investigate the importance of our color loss $\mathcal{L}^\text{Color}$ defined in Section~\textcolor{red}{3.3} by changing its weight $\lambda_{\text{Color}}$.
%
Specifically, We increase the weight from 0.1 to 10 and report the performance of our ICF-SRSR trained on the scale $\times2$ of test sets of both real-world dataset RealSR~\cite{cai2019toward} and synthetic datasets Set5~\cite{bevilacqua2012low} and DIV2K~\cite{agustsson2017ntire} validation in Table~\ref{tab:ab-loss}.
%
The results indicate that $\lambda_{\text{Color}}=0.2$ achieves the best performance on different datasets.
\input{tables/ablation}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Noise-free results}
%
\input{sections/figures/supp/noisy}
In Section~\textcolor{red}{4.2} of our main manuscript, we note that the ground-truth images of Set5~\cite{bevilacqua2012low} and Set14~\cite{zeyde2010single} datasets are noisy while our SR images are noise-free.
%
We show the difference between our SR images and the noisy ground-truth images in Figure~\ref{fig:supp-noise}.
%
The results prove our claim and show that we can restore SR images without any noise.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Complicated down-sampling degradations}
%
As we show in Section~\textcolor{red}{4.3} of our main manuscript, the proposed method can learn from real-world datasets with unknown degradations~(real LR usually includes complicated degradations).
%
For example, we can train our model $f_\theta$ on images from RealSR-V3~\cite{cai2019toward} and DRealSR~\cite{wei2020component} datasets directly and achieve promising results. 
%
Furthermore, we train and test our method ICF-SRSR on a dataset with more complicated degradations generated by the Real-ESRGAN~\cite{wang2021realesrgan} down-sampling strategy.
%
We note that the generated LR images by the Real-ESRGAN~\cite{wang2021realesrgan} down-sampling model are synthesized by a sequence of classical degradations such as blur, resize, noise, JPEG compression, and artifacts to simulate more practical degradations.
%
Figure~\ref{fig:reb_degrade} demonstrates that our method ICF-SRSR can perform $\times 2$ SR faithfully even on images with mild noise and artifacts.
%
\input{sections/figures/rebuttal_degrade.tex}
%
%\vspace{-9.2mm}
\section{Visualization of the generated images}
%\vspace{1mm}
In Figure~\ref{fig:supp_scales2} and \ref{fig:supp_scales4}, we visualize the generated down-sampled~(LLR) and up-sampled~(SR) images by our ICF-SRSR framework for different scaling factors $\times2$ and $\times4$, respectively on various benchmark datasets Set14~\cite{zeyde2010single}, BSD100~\cite{martin2001database}, and Urban100~\cite{huang2015single} and also real-world dataset RealSR-V3~\cite{cai2019toward}.
%
We further restore the down-sampled LR images given HR images for scaling factor $\times2$ of Canon and Nikon sets from the RealSR-V3~\cite{cai2019toward} dataset as shown in Figure~\ref{fig:reb_LRHR}.
%
%We further compare the generated and real LR images in Figure~\ref{fig:reb_LRHR}.
%
The comparison demonstrates that the generated down-sampled LR images by our self-supervised method ICF-SRSR look similar to the real LR images, validating the ability of our method to synthesize realistic LR-HR image pairs.
%
Such generated paired images LR-HR are useful to train other off-the-shelf supervised methods, as evident in Table~\textcolor{red}{6} of our main manuscript.
\input{sections/figures/supp/scales2}
\input{sections/figures/supp/scales4}
\input{sections/figures/rebuttal_LRHR.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training on a single image}
\input{sections/figures/supp/single_image}
\input{sections/figures/supp/real_single}
In Section~\textcolor{red}{4.4} of our main manuscript, we show that our method ICF-SRSR can learn to restore SR images by training on a small dataset and even a single image as shown in Figure~\textcolor{red}{1}. 
%
We show more samples to illustrate the ability of our method to learn from only a single image.
%
Therefore, we train and evaluate our ICF-SRSR model on a single LR image from the test set of the RealSR-V3~\cite{cai2019toward} dataset captured by the Nikon camera for scaling factor $\times2$.
%
Our results in Figure~\ref{fig:supp_single} demonstrate that our method can restore an SR image by training the model on only the same image.
%
Furthermore, our result for the single-image case is not only on par with the multi-image case but also shows better performance for some samples in terms of PSNR metric and visual appearance.
%
This attribute makes our method more practical in real-world scenarios where there are not many sample images for training.
%
Moreover, we train and evaluate our self-supervised method ICF-SRSR on a single real-world smartphone photo and show the results in Figure~\ref{fig:supp_real_single}.

\end{appendices}

\end{document}