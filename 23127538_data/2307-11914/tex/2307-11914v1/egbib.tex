% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{bbding}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{multirow}
\usepackage{bm}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
 
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5260}%CVPR 
\def\confName{ICCV}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds}

\author{Ruisheng Wang, Shangfeng Huang, Hongxin Yang\\
University of Calgary, AB, Canada\\
%2500 University Dr NW, Calgary, AB, Canada\\
{\tt\small \{ruiswang, shangfeng.huang, hongxin.yang\}@ucalgary.ca}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Shangfeng Huang\\
%University of Calgary\\
%First line of institution2 address\\
%{\tt\small shangfeng.huang@ucalgary.ca}
}
% \maketitle

%%%%%%%%%%% Image 1
\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
\captionsetup{type=figure}
  \begin{subfigure}{0.55\linewidth}
  \centering
    % Figure removed
    \caption{Aerial LiDAR point clouds of Tallinn}
    \label{fig_1:a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.43\linewidth}
  \begin{subfigure}{1\linewidth}
  \begin{subfigure}{0.49\linewidth}
    \centering
    % Figure removed
    \caption{A building point cloud}
    \label{fig_1:b}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
    \centering
    % Figure removed
    \caption{A roof point cloud}
    \label{fig_1:c}
    \end{subfigure}
  \end{subfigure}
  
  \begin{subfigure}{1\linewidth}
  \begin{subfigure}{0.48\linewidth}
  \centering
    % Figure removed
    \caption{A mesh model}
    \label{fig_1:d}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
  \centering
    % Figure removed
    \caption{A wireframe model}
    \label{fig_1:e}
    \end{subfigure}
  \end{subfigure}
  
  \end{subfigure}
\caption{Illustration of Tallinn, one of the 16 cities in the Building3D dataset}
\label{fig_1}
\end{center}
}]
%%%%%%%%% ABSTRACT
\begin{abstract}
Urban modeling from LiDAR point clouds is an important topic in computer vision, computer graphics, photogrammetry and remote sensing. 3D city models have found a wide range of applications in smart cities, autonomous navigation, urban planning and mapping etc. However, existing datasets for 3D modeling mainly focus on common objects such as furniture or cars. Lack of building datasets has become a major obstacle for applying deep learning technology to specific domains such as urban modeling.   
%A key prerequisite for advancing supervised learning methods in urban modeling is availability of a large amount of high-quality labeled data with sufficient variety. However, lack of such datasets is preventing the development of learning-based 3D modeling algorithms, especially for buildings which have found a wide range applications in smart cities, digital twins, and urban planning etc. 
In this paper, we present an urban-scale dataset consisting of more than 160 thousands buildings along with corresponding point clouds, mesh and wireframe models, covering 16 cities in Estonia about 998 $Km^2$. 
%We also provide two new baselines for the benchmark test: a supervised and a self-supervised learning method. To our knowledge, the self-supervised learning pipeline based on a novel linear self-attention mechanism is the first attempt in this regard. 
We extensively evaluate performance of state-of-the-art algorithms including handcrafted and deep feature based methods. Experimental results indicate that Building3D has challenges of high intra-class variance, data imbalance and large-scale noises. The Building3D is the first and largest urban-scale building modeling benchmark, allowing a comparison of supervised and self-supervised learning methods. We believe that our Building3D will facilitate future research on urban modeling, aerial path planning, mesh simplification, and semantic/part segmentation etc.  

\end{abstract}
% Our benchmark is publicly available athttps://ucalgary.ca/labs/spatial-intelligence/building3d 
%In the area of 3D modeling, the annotated date are geometric models in varied forms such as mesh or wireframe.

%%%%%%%%% BODY TEXT

\section{Introduction}
\label{sec:intro}
Deep learning has achieved tremendous success on computer vision applications such as image classification and object detection\cite{NIPS2012_c399862d, brusilovsky:simonyan2014very, he2016residual, 47353}, semantic segmentation \cite{he2017mask,deeplabv3plus2018, ronneberger2015u} and human pose estimation \cite{8765346,wang2019deep}, to name a few. With the increased availability of 3D point clouds that found a wide range of applications in robotics, autonomous driving and urban modeling, a recent research focus has been shifted to deal with such massive 3D point clouds \cite{su15mvcnn,Maturana2015VoxNet,qi2017pointnet,qi2017pointnet++}. The majority of current work on 3D point cloud processing are focused on 3D shape classification \cite{su15mvcnn,thomas2019KPConv,Riegler2017OctNet, qi2017pointnet}, 3D object detection and tracking \cite{Maturana2015VoxNet,9652096,9156722}, 3D semantic and instance segmentation\cite{hu2019randla,thomas2019KPConv, qi2017pointnet++,Loic18}. Correspondingly, a large number of datasets including synthetic and real-world ones have been established to train and evaluate deep learning algorithms with respect to above mentioned applications \cite{chang2015shapenet,uy2019revisiting,6248074,9156412,semantic3D2017,isprs,hu2020towards, Chen_2022_BMVC}. The widely available datasets are prerequisite for rapid algorithm advancement in supervised learning based on neural networks. Supervised learning methods heavily rely on labeled data, which have been intensively studied. Due to the expensive cost of labeled data, self-supervised learning methods learning representations from unlabeled data, are receiving more and more attention. Started from self-supervised learning on 2D images, SimCLR \cite{pmlr-v119-chen20j} and CPC \cite{2018arXiv180703748V,DBLP:journals/corr/abs-1905-09272}
 have reached top performance on image classification benchmarks. The methods of self-supervised learning in 2D are being quickly adapted for 3D point clouds, such as the jigsaw puzzle pretext task \cite{sauder2019self}, estimating rotations \cite{kim1999robust}, contrastive learning \cite{xie2020pointcontrast}, point cloud completion \cite{wang2021unsupervised}. 
 
 %To our knowledge, none of these self-supervised learning methods have been adapted for 3D modeling, especially building roof modeling.  

%3D urban models are important infrastructure for urban planning, infrastructure asset management, smart cities, digital twins, and autonomous vehicles etc. 
Current deep learning methods on urban modeling have been restricted to small datasets or synthetic ones. However, urban modeling is different from existing object modeling work where small objects are collected under a well-controlled lab environment. Specifically, urban modeling deal with large-scale LiDAR scans containing more noisy and incomplete point clouds that represent complex real-world scenes. To advance urban modeling research in computer vision, we introduce an urban-scale dataset for 3D roof modeling from point clouds collected from the air. The dataset covering 16 cities in Estonian consist of 875.39 millions of aerial LiDAR point clouds and 161.91 thousands of 3D building models in both mesh and wireframe formats. A mesh model is a 3D model that is made up of small discrete cells. The commonly used 2D cell shapes are the triangle and the quadrilateral, which the triangular mesh is the one referred in this paper. A wireframe model is a 3D model that the polygonal faces have been removed to retain only the outlines of its component polygons. It is the least complex representation, namely a skeletal description of a 3D object consisting of vector points connected by lines. Man-made objects such as buildings are mostly polyhedral which can be represented by corners, edges and/or planar surfaces\cite{Liu2019ICLR}. Therefore, 
wireframes are particularly suitable for representing polyhedral objects such as buildings or furniture. Besides benefit of efficient storage and transmit, wireframe models are easy to edit and manipulate in CAD software which can help create CAD models for various applications such as quality inspection, metrology, rendering and animation\cite{Liu2019ICLR}. 

We convert mesh models into wireframe models. There is no wireframe models provided in current building modeling datasets \cite{chang2015shapenet,wichmann2018roofn3d,huang2022city3d,lin2021capturing,hu2020towards}. To our knowledge, we are the first to provide both mesh and wireframe building models along with corresponding LiDAR point clouds at urban scale. Fig.\ref{fig_1:a} shows the aerial LiDAR point clouds of Tallinn, one out of 16 cities in Building3D dataset. It contains 361.95 million points and 47.05 thousands of buildings, covering an area around 195 km$^2$. Figure \ref{fig_1:b}, \ref{fig_1:c} \ref{fig_1:d} \ref{fig_1:e} shows building and roof point clouds, as well as corresponding mesh and wireframe models, respectively. % The description of other 15 cities containing 513.44 millions points and 114.86 thousands buildings can be found in the Table 1.
Besides a urban-scale dataset, we also provide two new baselines of supervised and self-supervised learning, adopted and evaluated various supervised and self-supervised pipelines for 3D roof modeling. To our knowledge, we are the first to use and evaluate self-supervised learning method for 3D object reconstruction. Overall, our main contributions are in the following. 
\begin{itemize}
\item We present the first and largest urban-scale building modeling dataset consisting of aerial LiDAR point clouds, mesh and wireframe models. Besides urban modeling, the proposed dataset can be extended to support various downstream applications. The whole dataset is made available to the research community.
%\item An alternative end-to-end supervised learning method for large-scale building roof modeling is presented. 
\item We evaluate representative deep and handcrafted feature based methods including mainstream self-supervised learning methods, and establish new baselines and evaluation metric for future research. The new baselines achieve state-of-the-art performance compared with deep learning based methods. To our knowledge, we are the first to propose and adopt self-supervised pre-training methods for 3D building reconstruction. 

\end{itemize}
 
%------------------------------------------------------------------------
\section{Related Work}
\label{sec:formatting}

%-------------------------------------------------------------------------
\subsection{Related Datasets}

\begin{table*}[]
\centering
\setlength{\tabcolsep}{0.3mm}{
\begin{tabular}{@{}c|ccccccc@{}}
\toprule
Dataset   & Scene      &  Type      & Area($Km^2$)   & Diversity  & Point Clouds  & Mesh & Wireframe\\ \midrule
ShapeNetCore\cite{chang2015shapenet} & Object   & Synthetic   & --  & --  &  & \Checkmark  &  \\
KITTI\cite{geiger2012we}  & Street  & LiDAR  & --  &   39.2 Km street  &  \Checkmark &   &  \\
Simulated dataset\cite{li2022point2roof} & --  & Synthetic        & --               &  --  & \Checkmark & \Checkmark &    \\
RoofN3D by point2roof\cite{wichmann2018roofn3d} &  --       &     LiDAR &  --  & 1 city  &\Checkmark &\Checkmark &     \\
City3D\cite{huang2022city3d}  & Urban  & LiDAR  & --  & 3 scenes  &\Checkmark  &\Checkmark  &  \\
UrbanScene3D\cite{lin2021capturing} & Urban  & CAD\&MVS  & 136  &  \begin{tabular}[c]{@{}c@{}}6 cities and\\ 10 scenes \end{tabular}  &  & \Checkmark  &  \\
SensatUrban\cite{hu2020towards} & Urban & UAV Photogrammetry  & 7.64   & 3 cities &\Checkmark  & &\\
STPLS3D\cite{Chen_2022_BMVC} &Large scenes &Aerial Photogrammetry & 17 & 67 scenes & \Checkmark & \Checkmark &\\
DublinCity \cite{BMVC2019}  &Small Scenes  & LiDAR  &5.6 & 1 scene & \Checkmark & & \\
Building3D      & Urban       & LiDAR       &  998              &  16 cities  &  \Checkmark & \Checkmark  &\Checkmark \\
\bottomrule
\end{tabular}}
\caption{Comparison with existing 3D datasets }
 \label{tab:1}
\end{table*}

\begin{table*}[]
\centering
\setlength{\tabcolsep}{5mm}{
\begin{tabular}{@{}c|cccc@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Number}                \\
                        & Objects & Roof(avg.) & Corners (avg.) & Edges(avg.) \\ \midrule
Simulated dataset\cite{li2022point2roof}&  17.6 K       & 1409        & 8               & 10      \\
RoofN3D by point2roof\cite{wichmann2018roofn3d,li2022point2roof}& 0.5 K        & 1349      &  6              & 8      \\
Tallinn of Building3D      & 36.9 K       & 3292       &   16             & 18      \\ \bottomrule
\end{tabular}}
\caption{Quantitative comparison between Building3D and most relevant datasets}
 \label{tab:2}
\end{table*}

We only review pertinent 3D modeling datasets with a focus on point clouds based urban modeling datasets.  

\textbf{ShapeNet} \cite{chang2015shapenet} is currently one of the most popular 3D object datasets for part segmentation, point cloud completion and reconstruction. For 3D object reconstruction, ShapeNetCore, a subset of ShapeNet, covers 55 common object categories with about 51,300 unique 3D models but without corresponding point clouds.  
The methods \cite{park2019deepsdf,mescheder2019occupancy,peng2020convolutional} generate simulated point clouds as input, and use continuous signed distance field or 3D voxel grid with occupancy information for implicit reconstruction of 3D object surfaces.
\textbf{RoofN3D} \cite{wichmann2018roofn3d} is designed for 3D building reconstruction. 
%Although the dataset contains raw point clouds and corresponding roof mesh models, the point clouds are sparse and have numerous redundant points that don't belong to the roofs.
We found that the quality of mesh models are poor and a large amount of shape and vertex are mismatched between point clouds and mesh models.
%It indicates that the dataset is not suitable for training a robust neural network.
Based on RoofN3D dataset, Li et al., \cite{li2022point2roof} select 500 roof point clouds and manually create mesh models to construct a small real-world dataset on the top of a simulated dataset.
%including 17,600 point clouds and corresponding mesh models. 
However, both the real-world and simulated datasets consist of very few categories of primitive shape, which doesn't exhibit considerable variability.  
\textbf{City3D}\cite{huang2022city3d}, a large-scale building reconstruction dataset, consists of about 20 thousands of building mesh models and aerial LiDAR point clouds. However, all mesh models are generated by the proposed method, which means that the quality of mesh models depends on the method's capability. A major limitation is that their framework uses only planar primitives. Therefore their method can't deal with curved surfaces or non-planar roofs which commonly exit in the real world. 
%Another problem is inferring missing vertical planes of buildings from roof point clouds, which may not perfectly align with the ground-truth footprints. They recommend the use of high-quality footprint data for alleviating this problem. Computational bottlenecks for buildings with complex structures is another problem.
\textbf{UrbanScene3D} \cite{lin2021capturing} covers 10 synthetic and six real-world scenes. Specifically, 10 synthetic datasets consist of 13,352 textured mesh objects and corresponding aerial images for the sake of 3D reconstruction. The six real-world scenes provide 488 textured mesh objects and corresponding aerial images. Although point clouds are also provided, they are only allowed to be used for evaluating the quality of reconstruction results not for the training.
% Figure environment removed
%-------------------------------------------------------------------------
\subsection{Related Methods}
Building reconstruction from point clouds has been intensively studied in the past two decades \cite{poullis2009automatic, Nan2010, zhou20122, Vanegas2012, henn2013model, vosselman20013d, chen2017topologically, nan2017polyfit, PoullisPAMI2019, Minglei2016, li2022point2roof}. This problem remains unsolved due to complexity in roof structure and building style, sparsity, noise and possible missing data in point clouds. In general, traditional 3D roof reconstruction methods can be classified into three categories: data-driven \cite{poullis2009automatic, zhou20122}, model-driven \cite{henn2013model, vosselman20013d} and hybrid \cite{vosselman20013d}. Traditional methods normally require multiple steps of processing to generate 3D building models, and the errors introduced in each step will be accumulated. The quality of final building models largely rely on the quality resulted from previous steps. If an end-to-end deep learning method, which takes point clouds as input and outputs wireframe building models, is employed, the problem of accumulated errors can be eliminated to a large extent \cite{li2022point2roof}. However, there are very few deep learning methods to address this problem probably due to unavailability of a well-labeled and diversified dataset. 

The PC2WF \cite{Liu2019ICLR} is the first end-to-end deep learning approach to generate wireframe models from synthetic 3D point clouds. The problem of wireframe model generation is formulated as a problem of vertex and edge classification. Due to lack of a publicly available point cloud dataset, they constructed their own synthetic dataset. 
%: a subset from ABC \cite{Koch_2019_CVPR} and a Furniture dataset from the web. The point clouds are generated from a virtual scanner and the reconstructed objects are furniture-like small objects.
Along this line of work, the Point2Roof \cite{li2022point2roof}is the first trainable end-to-end deep neural network to generate 3D wireframe building models from LiDAR point clouds. Due to lack of a real-world dataset, they also used synthetic point clouds and models for the pre-training, and constructed a small amount of real world data  to fine tune the network. However, due to the limited training samples, their algorithm can only generate very limited types of building models. We believe that our urban-scale real-world dataset will help advance the state of art in the field of urban modeling.

%The data-driven methods first segment point clouds into adjacent planar facets then establish topological relations between each facet \cite{poullis2009automatic, zhou20122}. They adopt a bottom-up strategy, which segment or extract geometric primitives then use resultant primitives to form building models. Although these methods have advantages for reconstructing complex roofs, they can be unreliable when input data is incomplete or occluded. The model-driven methods employ a top-down strategy. Given a predefined model library such as gable or hip roofs  or a single parametrized roof model, the modeling is formulated as parameter estimation and/or model selection problems\cite{henn2013model, vosselman20013d}. However, these methods reply on the assumption that roofs can be represented by the predefined parametric shapes. In practice, it is difficult to define all the required models representing all types of buildings. The hybrid methods usually rely on ground plan \cite{vosselman20013d} or decompose complex roofs into simple primitives. Then these primitives can be matched to the predefined shape library. 
 
% % data-driven examples
% For example, Poullis et al., \cite{poullis2009automatic} reconstructed the simplified 3D polygonal models of different areas, such as downtown areas of Baltimore, Denver and Oakland and the city of Atlanta. However, the obvious drawback is that the whole scanned point cloud can not be processed directly, it needed to convert the 3D point cloud into a set of 2D XYZ maps. Zhou et al., \cite{zhou20122} introduced global regularities in 2.5D building models to recognize their intrinsic structures of building models. 

% % 
% For instance, Henn et al., \cite{henn2013model} utilized sparse point clouds with low number of ($\leq 20$) points of roof parts and building footprints to implement roof models. Such model-driven methods can guaofntee the reconstructed roof models are water-tight. 

% \textbf{Supervised Learning Methods}  Some researchers try to use deep network to reconstruct building models from images \cite{Mahmud_2020_CVPR,yu2021automatic}. There are also   
%  methods to generate 3D shape models from point clouds based on  signed distance fields\cite{park2019deepsdf} and occupancy voxels \cite{mescheder2019occupancy} etc. The PC2WF \cite{Liu2019ICLR} is the first end-to-end deep learning approach to generate wireframe models from synthetic 3D point clouds. The problem of wireframe
% model generation is formulated as a problem of vertex and edge classification. Due to lack of a publicly available point cloud dataset, they constructed their own synthetic datasets: a subset from ABC \cite{Koch_2019_CVPR} and a Furniture dataset from the web. The point clouds are generated from a virtual scanner and the reconstructed objects are furniture-like small objects. Along this line of work, the Point2Roof \cite{li2022point2roof} is the first trainable end-to-end deep neural network to generate 3D building wireframe models from LiDAR point clouds. Due to lack of a real-world dataset, they also used synthetic point clouds and models for the pre-training, and constructed a small amount of real world data (about 500 buildings) to fine tune the network. However, due to the limited training samples, their algorithm can only generate very limited types of building models. We believe that our urban-scale real-world dataset, consisting of more than 160 thousands buildings, will help advance the state of art in the field of urban modeling.

% \begin{table*}[h!]
% \begin{center}
% \setlength{\tabcolsep}{5mm}{
% \begin{tabular}{@{}c|cccccc@{}} \toprule
% City        & Area($m^2$) & Points(M)  & Objects(K) & \begin{tabular}[c]{@{}c@{}}Point size\\ (Kb/avg.)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Mesh\\ (Kb/avg.)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Wireframe\\ (Kb/avg.)\end{tabular} \\ \midrule
% Haapsalu    &  $40\times 10^6$    & 21.31         & 5.1        & 365.26         & 4.02              & 1.09  \\ 
% Harku       &  $166\times 10^6$        & 54.92    & 14.14      &338.89          & 3.67              & 1.03      \\ 
% Hiiumaa     &  $16\times10^6$        & 9.15      & 2.86      & 279.22          & 3.30               & 0.93        \\ 
% Keila       &  $17\times10^6$        & 15.78      & 2.95      & 469.50             & 4.28              & 1.16 \\ 
% Kohtla      &  $62\times10^6$        & 39.62    & 6.05      & 575.91             & 3.41              & 0.95 \\ 
% Loksa       &  $8\times10^6$        & 5.13      & 1.03      & 437.17               &  3.67             &1.02 \\ 
% Maardu      & $38\times10^6$     & 35.30    & 6.00    & 517.47                & 3.56              &0.98  \\ 
% Narva       &  $59\times10^6$        & 27.62  & 14.27  & 169.87               & 2.96               & 0.83\\ 
% Narva Joesuu & $66\times10^6$    & 4.00     &4.18  & 83.89                & 3.14                & 0.89\\
% Paide       & $18\times10^6$          & 14.74      &2.73    &474.77           & 3.93                 & 1.06 \\ 
% Parnu linn  & $45\times10^6$          &68 .80   &14.23  &423.23          & 4.35                 & 1.17\\ 
% Rakvere     & $19\times10^6$          & 25.78    &5.85    &386.31          & 4.18                 & 1.13\\
% Saue        & $144\times10^6$         & 50.80      &13.78   &323.93        & 3.15                 & 0.90\\ 
% Sillamae    & $20\times10^6$          &16.80     &2.05     &724.83         & 4.04                 & 1.10\\ 
% Tartu       & $ 85\times10^6$        & 123.69    &19.64    &551.96        & 5.07                 & 1.31 \\ 
% Tallinn      & $195\times10^6$        &361.95   &47.05     & 672.97      & 6.38                &  1.87      \\ \midrule
% Total        &   $998\times 10^6 $  & 875.39   &  161.91  & 408.14    &4.07                   & 1.16 \\ \bottomrule
% \end{tabular}}
% \end{center}
% \caption{Details of each city in the Building3D dataset}
%  \label{tab:10}
% \end{table*}

% \textbf{Self-supervised Learning Methods} 
% % #### notes:
% The self-supervised learning methods learn features from unlabeled data itself by solving a pretext task. The learned feature can be used for downstream tasks without requiring a large amount of labeled data. One of the pretext tasks \cite{sauder2019self} is to reconstruct point clouds whose parts have been randomly rearranged. The DepthContrast \cite{zhang2021self} uses single- or multi-view 3D scans and an instance discrimination framework to learn 3D representation from points or voxels. The Occlusion Completion (OcCo)  \cite{wang2021unsupervised}  recovers occluded point clouds and uses the learned encoder weights as initialization for downstream tasks such as point cloud segmentation and classification. The Point-BERT \cite{yu2022point} uses a Transformer-based pipeline to reconstruct masked parts from unmasked parts of each object for the pre-training.  However, it needs to train a DGCNN \cite{phan2018dgcnn} to discrete the divided point tokens, which will be randomly masked at some ratios. The masked and unmasked point tokens are then input into a standard Transformer to recover the masked tokens. Such operation causes masked tokens' location leakage. To solve this problem, Point-MAE \cite{pang2022masked} only input unmasked parts into encoder and use learned unmasked features to recover masked parts which extends the idea of MAE \cite{he2022masked}.

% \textbf{Self-attention Mechanism}
% The design of self-attention mechanism is important for efficient Transformers. The time complexity of Transformers with original self-attention mechanism is quadratic. Such transformer models are slow to train and deploy in practice. To remedy this drawback, Shen et al., \cite{shen2021efficient} propose an efficient self-attention mechanism. It regards the Key and Value vectors as single-channel feature map and multiplies matrices by aggregating the value features through weighted summation to form a global context vector. Then combines this global context vector with the Query for the final output. Along this line of work in natural language processing (NLP), Wang et al. \cite{wang2020linformer} add two linear projection matrices to project Key and Value matrices. Such operation reduces complexity of calculating scores between one and other tokens to linearity. However, it is only applied in the NLP field. 

\section{The Building3D Dataset}
%Building3D covers 16 cities with  aerial LiDAR point clouds, while UrbanScene3D \cite{lin2021capturing} is composed of a large amount of synthetic data and a small amount of real data. 
We process the raw data provided by land board of republic of Estonia to generate Building3D dataset (Building 3D model data: Estonian Land Board 2022). The Building3D dataset covers 16 cities about 998 $Km^2$. Fig.\ref{fig_5} shows overall statistics of the proposed dataset, which contains about 160 thousands building point clouds with corresponding mesh and wireframe models. Fig.\ref{fig_5}b shows histograms of point clouds and objects (i.e. buildings) in each city. The order of magnitude of points and objects are indicated by symbols M (million) and K (thousand) respectively. Fig.\ref{fig_5}c shows the area of each city. Fig.\ref{fig_5}d shows the average memory consumption of a building point cloud, mesh and wireframe model in each city respectively. The average memory consumption for a building point cloud is between 83.89KB and 672.97KB, while the corresponding mesh and wireframe model is less than 6.5KB and 2KB. The ratio of average sizes among building point clouds, mesh and wireframe models is approximately 400:4:1. An straightforward visualization of Building3D is shown in \url{http://building3d.ucalgary.ca/}.

 %It contains a large number of diversified buildings with complex structure, which potentially contribute to increased  robustness of deep learning methods trained on our dataset.
%In comparison to existing datasets, the Building3D dataset not only offers building point clouds and meshes but also provides wireframe models.

% The point clouds are imbalanced and noisy. Some buildings contain several hundreds points while others may have several thousands points. 
%The complexity of roofs are varied from easy to complex. Point clouds with ground-truth models can be used not only for benchmarking performance of urban modeling algorithms but also aerial path planing algorithms for efficient data acquisition of unmanned vehicles such as drones. The wireframe and mesh models will also facilitate fundamental research in mesh simplification and remeshing. 

\subsection{Data Description and Annotation} 
\textbf{Building point clouds}
The raw LiDAR point clouds are collected by a high-precision RIEGL VQ-1560i scanner at altitude 2600 meters then stored in LAZ format. Each LAZ file covers 1 Km$^2$ and consists of  terrain, water, trees, bushes, buildings, bridges etc. The relative accuracy of point clouds is 20 mm. The density of point clouds is 30.314 points per square meter and the point-to-point distance
is 0.1816m. For 3D building reconstruction, we remove irrelevant point clouds and only retain building point clouds. To this end, point clouds and corresponding mesh models are projected onto XY plane. Then all irrelevant point clouds outside projected regions of mesh models are removed to obtain building point clouds. To generate fine point clouds for 3D building reconstruction as shown in Fig.\ref{fig_1:b}, only points whose shortest distances to corresponding mesh models are within a given threshold are retained. Each building point clouds are stored in XYZ format including XYZ coordinates, RGB color, near infrared information, intensity and reflectance. 
% (https://felix.rohrba.ch/en/2015/point-densityand-
% point-spacing/)

\textbf{Roof point clouds} 
It is inevitable that building facade are incomplete due to use of airborne scanners. In practice, some building point clouds have relatively complete facade information, while others may have barely information on facade. However, it is not an essential problem for 3D roof reconstruction which doesn't involve much of facade information. We can remove all the points representing facades. Basically, almost all facades are vertical to the XY plane. We calculate normals for each mesh face and remove all mesh faces whose normals are parallel to the XY plane to generate roof mesh models. A point is classified as roof if its distance to the roof mesh model is within a given threshold. The roof point clouds are shown in Fig.\ref{fig_1:c}.

\textbf{Mesh models}
Building mesh models are created from aerial LiDAR point clouds and building footprints by using the Terrasolid software with manual editing. These mesh models are typical LoD2 models including detailed and realistic representation of roofs. Compared with mesh models with dense triangular facets provided by ShapeNet\cite{chang2015shapenet} and UrbanScene3D\cite{lin2021capturing}, our mesh models can be considered as simplified mesh models with an average of 50 faces for each model. In the Tallinn dataset, approximately 40$\%$ of mesh models are less than 30 faces.  A mesh model is shown in Fig.\ref{fig_1:d} which different colors indicate different triangular meshes. 

\textbf{Wireframe models}
wireframe models are designed to formulate the problem of 3D reconstruction as a problem of point and edge classification. Compared with mesh models, wireframe models have clearer and more well-defined structures and require less disk space. To generate wireframe models, we calculate normals for each mesh. If the normals of two adjacent triangular meshes are approximately parallel, it indicates that they are co-planar and the shared edge will be removed. After all shared edges are removed, we obtain coarse wireframe models with redundant vertices. The coarse wireframe models also contain short edges that can be merged into long edges. Finally, the fine wireframe models are generated by removing all redundant vertices and merging short edges, then reviewed and adjusted by professional technicians. A wireframe roof model in the Building3D dataset is shown in Fig.\ref{fig_1:e}. Root Mean Square Error (RMSE) represents displacement between original point clouds and corresponding mesh and wireframe models and  the average RMSE is 0.065m.

% Figure environment removed

\subsection{Building Roof Types}
The Building3D dataset encompasses more than 60 distinct roof types, surpassing coverage of all comparable datasets as shown in Table \ref{tab:1}. Table \ref{tab:2} shows differences compared with most relevant datasets \cite{li2022point2roof,wichmann2018roofn3d}. The number of corners and edges shows that our roof models are more complex and have more categories. More points in roof point clouds indicate that our LiDAR point clouds contain more detailed information and intricate structure.  Fig.\ref{fig:4} illustrates some of the representative roof types, including Flat, Dormer, Mansard, Pyramid Hip, Saltbox, Glambrel, Overlaid Hip, Hip and Valley, Cross Hipped, Combination, among others. In each category of Fig.\ref{fig:4}, columns from left to right show roof point cloud and wireframe (top-down view), roof point cloud and wireframe (side view), corresponding roof mesh and wireframe model, respectively. The wireframe models contain less facets than mesh models.  

%-------------------------------------------------------------------------
\section{Benchmarks}
%Current deep learning methods on 3D roof reconstruction from point clouds consists of two steps, roof corner detection and roof edge generation. The resultant wireframe roof models can be readily extruded to generate LoD2 building models. The challenge includes diversity of buildings, unbalanced point clouds, and incomplete data such as lack of roof corner point clouds. 

\subsection{Evaluation metrics}  
We use several metrics for evaluation, average corner offset (ACO), precision, recall and F1 score, 3D mesh IoU, and root mean square error (RMSE). ACO is the average offset between predicted corners and ground-truth corners. 
%The smaller offsets indicate better quality of generated models.
Corner precision (CP), edge precision (EP), corner recall (CR), and edge recall (ER) are calculated through confusion matrix to evaluate the accuracy of corner and edge classification. %larger CP and EP values indicate more precise classification of corners and edges, while larger CR and ER values indicate lower rates of missing detection.
%The CO formula is as follows:
%\begin{equation}
%    CO = Average\left ( \left \| C_{p} - C_{g}  \right \| _{2}^{2} \right )
% \end{equation}where $C_{p}$ and $C_{g}$ represent the predicted and ground-truth corner coordinates respectively. 
3D Mesh IoU is a metric for evaluation of the fit between generated mesh models and ground truth mesh models. Existing 3D IoU methods use minimum bounding boxes of mesh models which can't represent accuracy of generated mesh models. We develop a numerical solution to use mesh models for 3D IoU to represent fitting errors.
%A smaller 3D IoU indicates better accuracy of generated mesh models with respect to ground truth mesh models.
%Here, we partition  both meshes to be compared into extremely small voxels and then calculate the intersection and union volumes of these voxels. 
RMS distance is a metric for evaluation of the fit between input roof point clouds and generated mesh models.
%A smaller RMSE value indicates a better accuracy of generated models with respect to input roof point clouds. 

%Specifically,  we first calculate the shortest distance from each point to the mesh surface. This distance is not the vertical distance from the point to a mesh plane, but the shortest distance from the point to the surface area of the mesh.  Based on the shortest distance, we calculate the value of RMS to evaluate the fit between the mesh models and the point clouds. A smaller RMS value indicates a better match between the two.
 \subsection{Training}
We carefully selected about 37k data samples consisting of roof point clouds and wireframe models from Tallinn. We use around 33.3k ($90\%$) data for the training and the remaining 3.7k ($10\%$) for the testing. The number of each roof point cloud is either upsampled or downsampled to 3072 as input, where the input $P\in R^{3072\times 7} $ contains  XYZ coordinates, RGB color and near infrared spectrum values. All models are trained in a RTX A6000 GPU with 48GB. In addition, we also select approximately 5,600 for training and 550 for testing as entry-level Building3D dataset, which contains approximately 10 corners and 12 edges per building on average.

\subsection{Representative Baselines}
To our knowledge, there are only two deep learning methods \cite{Liu2019ICLR, li2022point2roof} available for wireframe reconstruction from point clouds. This is probably due to limited availability of real-world datasets. The PC2WF \cite{Liu2019ICLR} is designed to generate furniture models which requires very dense point clouds as input. It doesn't work with point clouds of lower density and fails on the Building3D dataset. We evaluate performance of Point2Roof and also propose a new supervised method. Besides evaluation of four representative self-supervisions and three supervisions, we also propose a new self-supervised feature extraction module. In total, we select ten representative methods as solid baselines to benchmark our Building3D dataset.  
%To enhance the reliability and enrich the baseline, we have replaced the point cloud feature extraction module of our supervised methods with an existing self-supervised feature extraction algorithm.

$\bullet$ PointNet\cite{qi2017pointnet} and PointNet++ \cite{qi2017pointnet++}: The most popular methods to extract 3D point cloud features.  

$\bullet$ RandLA-Net\cite{hu2019randla}: A method that can extract large-scale point cloud features by an effective local feature aggregation module.

$\bullet$ Point2Roof \cite{li2022point2roof}: It employs the PointNet++ \cite{qi2017pointnet++} as feature extractor to generate pointwise point features for roof corner prediction and edge classification. 
 
$\bullet$ Our Supervised: We use Point-Transformer \cite{zhao2021point} as feature extractor to generate pointwise point features for roof corner prediction and feature generation. Based on graph neural networks\cite{zhou2020graph},our method makes full use of node (corners) location and feature information to generate edges (refer to our supplementary material). 
 
$\bullet$ Point-BERT \cite{yu2022point}: A prior baseline that introduces a new pre-training approach using a masked point modeling pretext task for 3D point cloud Transformer.
 
$\bullet$ Point-MAE \cite{pang2022masked}: A solid baseline that introduces a self-supervised learning method for point clouds using masked auto-encoders.
 
$\bullet$ Point-M2AE \cite{zhang2022point}: It introduces a hierarchical pre-training method for point clouds using multi-scale masked auto-encoders.
 
$\bullet$ 3D-OAE \cite{zhou2022self}: It uses occlusions in the point clouds to train an auto-encoder and outperforms existing self-supervised methods for point cloud classification and segmentation tasks.

$\bullet$ {Our Self-supervised}: Based on Point-MAE, we designed a new linear self-attention mechanism to increase computational efficiency (refer to our supplementary material). 

%Hongxin write your differnece from exsiting methods and summarize other SS representative methods
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\setlength{\tabcolsep}{4mm}{
\begin{tabular}{@{}cc|c|ccc|ccc@{}}
\toprule
\multicolumn{2}{c|}{Methods}                  & ACO & CP & CR & F1 & EP & ER & F1 \\ \midrule
\multirow{5}{*}{Self-supervised} & Point-BERT\cite{yu2022point} & 0.25 & 0.88 & 0.69 & 0.77 & 0.90 & 0.29  & 0.44   \\
                                 & Point-MAE \cite{pang2022masked} & 0.27 & 0.85  & 0.69 & 0.76 & 0.86  & 0.22 & 0.35   \\
                                 & Point-M2AE \cite{zhang2022point}& 0.26  & 0.88 & 0.69 & 0.77  & 0.90  & 0.31 & 0.46  \\
                                 & 3D-OAE \cite{zhou2022self} & 0.27 &  0.86 & 0.68 & 0.76 & 0.79 & 0.32 & 0.46   \\
 & \begin{tabular}[c]{@{}c@{}}Our self-supervised\\ (Based on Linear Transformer)\end{tabular} & 0.26  & \textbf{0.87} &\textbf{0.69} &\textbf{0.77}  & 0.87 & \textbf{0.37}  & \textbf{0.52} \\ \midrule
\multirow{4}{*}{Supervised}      & PointNet\cite{qi2017pointnet}   & 0.36  & 0.71 & 0.50  & 0.59  &0.81  & 0.26  &0.39  \\
                                 & PointNet++ \cite{qi2017pointnet++}& 0.34     &0.79    & 0.52 & 0.63 & 0.84   &0.33   &0.47 \\
                                & RandLA-Net \cite{hu2019randla}  & 0.35     & 0.70  & 0.60 &0.65  &0.67  & 0.16   &  0.25  \\
                                &DGCNN \cite{phan2018dgcnn}   & 0.32 & 0.73 &0.58 &0.65 & 0.81 &0.30 &0.44\\ 
                                &PAConv \cite{xu2021paconv} & 0.33 & 0.75 &0.57 &0.65 & 0.85 &0.31 &0.45\\ 
                                &Stratified Transformer \cite{lai2022stratified} & 0.38 & 0.72 &0.51 &0.62 & 0.75 &0.22 &0.34\\ 
                                &FG-Net \cite{Liu2022FGNetAF}    &0.32  &0.77 &0.64  &0.70  &0.84 &0.38  &0.52 \\
                                 & Point2RooF\cite{li2022point2roof} &0.30     &0.66    &0.48  &0.56   &0.71    &0.26  &0.38  \\
 & \begin{tabular}[c]{@{}c@{}}Our supervised\\ (Based on Point Transformer)\end{tabular}       &\textbf{0.26}  & \textbf{0.89} & 0.66 &\textbf{0.76} &\textbf{0.91} &\textbf{0.46} & \textbf{0.61}\\ \bottomrule
\end{tabular}}
\caption{Quantitative results on the entry-level Building3D}
\label{table:3}
\end{table*}

\begin{table*}[]
\captionsetup{skip=2pt}
\centering
\setlength{\tabcolsep}{4mm}{
\begin{tabular}{@{}cc|c|ccc|ccc@{}}
\toprule
\multicolumn{2}{c|}{Methods}                  & ACO & CP & CR & F1 & EP & ER & F1 \\ \midrule
\multirow{5}{*}{Self-supervised} & Point-MAE (1\%)  &  0.49 & 0.21 & 0.07 & 0.11 & 0.00 &0.00  &0.00 \\
                                 & Point-MAE (10\%) & 0.38 & 0.69  & 0.42 & 0.52 & 0.00  & 0.00  & 0.00  \\
                                 & Point-MAE (20\%) & 0.36 & 0.71  & 0.44 & 0.54 & 0.47  & 0.09  & 0.15  \\
                                 & Point-MAE (40\%) & 0.34 & 0.73  & 0.46 & 0.56 & 0.49  & 0.10  & 0.17  \\
                                 & Point-MAE (50\%) & 0.33 & 0.75  & 0.47 & 0.58 & 0.52  & 0.12  & 0.20  \\ \midrule
\multirow{5}{*}{Self-supervised} & Point-M2AE (10\%)  & 0.38 & 0.69 & 0.52 & 0.59 & 0.42 & 0.02   & 0.04\\
& Point-M2AE (20\%)  & 0.35 & 0.73  & 0.55 & 0.63 & 0.39  & 0.05 & 0.09\\
& Point-M2AE (40\%) & 0.32 & 0.77  & 0.57  & 0.66 & 0.42  & 0.08 & 0.13\\
& Point-M2AE (50\%) & 0.32 &  0.79 & 0.58 & 0.67 & 0.50 & 0.07  & 0.12 \\ \midrule
\multirow{5}{*}{Self-supervised} & Our self-supervised (1\%) & 0.57  & 0.34 & 0.04 & 0.07  &0.13  & 0.00 &0.00  \\
& Our self-supervised (10\%)  & 0.39 & 0.71 & 0.46 & 0.56 & 0.60 & 0.01 & 0.02\\
& Our self-supervised (20\%)  & 0.37 & 0.76  & 0.49 & 0.64 & 0.78  & 0.12 & 0.21\\
& Our self-supervised (40\%)  & 0.32 & 0.79  & 0.51 & 0.62 & 0.82  & 0.14 & 0.24 \\
& Our self-supervised (50\%)& 0.30 & 0.84  & 0.53  &0.65 & 0.85  & 0.15 &0.26\\
& Our self-supervised (80\%) & \textbf{0.28} &  \textbf{0.87} & \textbf{0.55} & \textbf{0.67} & \textbf{0.89}& \textbf{0.16}  & \textbf{0.27}
 \\ \midrule
\multirow{4}{*}{Supervised} & Point2RooF\cite{li2022point2roof} & 0.39 & 0.65 & 0.30 &0.41 & 0.66 & 0.08  & 0.14 \\
 & \begin{tabular}[c]{@{}c@{}}Our supervised\\ (Based on Point Transformer)\end{tabular}       &  \textbf{0.29}    &\textbf{0.90}     & \textbf{0.53}    &\textbf{0.66}  &\textbf{0.88}    & \textbf{0.23}  &\textbf{0.36}\\ \bottomrule
\end{tabular}}
\caption{Quantitative results on the Tallinn Building3D}
\label{table:4}
\end{table*}


% \begin{table}[]
% \centering
% \setlength{\tabcolsep}{1.2mm}{
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% Method & Ratio & ACO & CP & CR & 
% EP & ER \\ \midrule
% \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Our Self-supervised\\  method\end{tabular}} 
%  & 10$\%$ & 0.39 & 0.71 & 0.46 & 0.60 & 0.01  \\
%  & 20$\%$ & 0.37 & 0.76  & 0.49  & 0.78  & 0.12  \\
%  & 50$\%$ & 0.30 & 0.84  & 0.53  & 0.85  & 0.15 \\ 
%  & 80$\%$ & 0.28 & 0.87  & 0.55  & 0.89  & 0.16 \\
%  & 100$\%$ & 0.30 & 0.87  & 0.56  & 0.87  & 0.19 \\ 
%  \bottomrule
% \end{tabular}}
% \caption{Performance of the self-supervised method at reduced data ratios}
%  \label{tab:5}
% \end{table}
% \begin{table}[]
% \centering
% \begin{tabular}{@{}cccclc@{}}
% \toprule
% Methods     & ACO  & CP   & CR   & EP  & ER  \\ \midrule
% Point2Roof\cite{li2022point2roof} & 0.39 & 0.65 & 0.30 & 0.66 & 0.08 \\
% Supervised &  0.29    & 0.90     & 0.53      & 0.88    & 0.23   \\ \bottomrule
% \end{tabular}
% \caption{Quantitative results on Tallinn dataset.}
% \label{table:4}
% \end{table}



% \begin{table*}[]
% \centering
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% Methods & Evaluation metrics & Building A & Buding B & Building C & Building D & Building E \\ \midrule
% \multirow{2}{*}{\begin{tabular} 22.5D Dual \\
% Contouring \cite{zhou20102}\end{tabular}} & Number of faces & 268 & 100 & 152 & 96 & 283 \\
%  & RMSE & 0.85  & 1.12  & 0.94 & 1.34  & 1.22  \\ 
% \multirow{2}{*}{PolyFit\cite{nan2017polyfit}} & Number of faces & 31  & 158  & 73  & 35 & failed \\
%  & RMSE &3.19  & 3.7 & 2.12  & 3.35 & - \\ 
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Topology Aware\\ Modeling\cite{chen2017topologically}\end{tabular}} & Number of faces & 3022 & 320  &355  & 1373  & 2714  \\
%  & RMSE & - & - & - & - & -  \\
% \multirow{2}{*}{Ground Truth} & Number of faces & 80  & 54  & 68  & 54 & 82 \\
%  & RMSE &  &  &  &  &  \\ 
% \multirow{2}{*}{Ours} & Number of faces & 19  & 13  & 17 & 13 & 27 \\
%  & RMSE &  &  &  &  &  \\ \bottomrule
% \end{tabular}
% \caption{The comparison results with traditional methods on different evaluation metrics}
%  \label{tab:3}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{@{}ccccc@{}}
% \toprule
% Method                  & Building A & Building B & Building C & Building D \\ \midrule
% 2.5D Dual Contouring\cite{zhou20122}    &  0.091  &  0.134   & 0.091    & 0.094     \\
% PolyFit \cite{nan2017polyfit}              & 0.080      & 0.111   & 0.055    &  0.122  \\
% Topology Aware Modeling \cite{chen2017topologically} &  0.114  &  0.346  &  0.40  &   0.22   \\
% Groud Truth             &   0.086         & 0.171    & 0.052           & 0.063          \\
% Ours                    &  0.490          &  0.263          &  0.133          & 0.096          \\ \bottomrule
% \end{tabular}
% \caption{The comparison results of RMS}
% \label{table:5}
% \end{table*}


% \begin{table*}[h!]
% \centering
% \begin{tabular}{@{}ccccc@{}}
% \toprule
% Method                  & Building A & Building B & Building C & Building D \\ \midrule
% 2.5D Dual Contouring\cite{zhou20122}    &  0.92     &  0.73   &   0.92     &  0.95  \\
% PolyFit \cite{nan2017polyfit}        &  0.97          &   0.36         & 0.97    &   0.63 \\
% Topology Aware Modeling\cite{chen2017topologically} &  -   & 0.56 & 0.73 & - \\
% Ours                    &    0.73        & 0.67  &  0.93   & 0.92    \\ \bottomrule
% \end{tabular}
% \caption{The comparison results of different methods in 3D mesh IOU. }
% \label{table_1}
% \end{table*}

% \begin{table*}[]
% \centering
% \begin{tabular}{@{}cccccc@{}}
% \toprule
% Methods  & Building A & Buding B & Building C & Building D & Building E \\ \midrule
% 2.5D Dual Contouring\cite{zhou20102}
% & 268 & 100 & 152 & 96 & 283 \\
% PolyFit\cite{nan2017polyfit}  & 31  & 158  & 73  & 35 & - \\
% Topology Aware Modeling \cite{chen2017topologically}  & 3022 & 320  &355  & 1373  & 2714  \\
% Ground Truth & 80  & 54  & 68  & 54 & 82 \\
% Ours  & 19  & 13  & 17 & 13 & 27 \\ \bottomrule
% \end{tabular}
% \caption{The comparison number of constructed faces results with traditional methods}
%  \label{tab:3}
% \end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\captionsetup{skip=2pt}
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{@{}ccllcllcllcll@{}}
\toprule
\multirow{2}{*}{Method} &
  \multicolumn{3}{c}{Building A} &
  \multicolumn{3}{c}{Building B} &
  \multicolumn{3}{c}{Building C} &
  \multicolumn{3}{c}{Building D} \\ \cmidrule(l){2-13} 
 &
  \multicolumn{3}{c}{RMSE $\downarrow$ / IoU $\uparrow$  / Faces$\downarrow$} &
  \multicolumn{3}{c}{RMSE $\downarrow$ / IoU $\uparrow$  / Faces$\downarrow$} &
  \multicolumn{3}{c}{RMSE $\downarrow$ / IoU $\uparrow$  / Faces$\downarrow$} &
  \multicolumn{3}{c}{RMSE $\downarrow$ / IoU $\uparrow$  / Faces$\downarrow$} \\ \midrule
\begin{tabular}[c]{@{}c@{}}2.5D Dual\\ Contouring\cite{zhou20122}\end{tabular}    & \multicolumn{3}{c}{0.091\hskip 0.1cm/ \hskip 0.1cm 0.92\hskip 0.1cm /\hskip 0.1cm 268} & \multicolumn{3}{c}{ 0.134 \hskip 0.1cm / \hskip 0.1cm \textbf{0.73} \hskip 0.1cm/\hskip 0.1cm 100} & \multicolumn{3}{c}{0.091\hskip 0.1cm/\hskip 0.1cm0.92\hskip 0.1cm/ \hskip 0.1cm96} & \multicolumn{3}{c}{\textbf{0.094}\hskip 0.1cm/\hskip 0.1cm\textbf{0.95}\hskip 0.1cm/\hskip 0.1cm 283 } \\
PolyFit \cite{nan2017polyfit}                  & \multicolumn{3}{c}{\textbf{0.080}\hskip 0.1cm/\hskip 0.1cm \textbf{0.97} \hskip 0.1cm/\hskip 0.1cm 31  } & \multicolumn{3}{c}{\textbf{0.111}\hskip 0.1cm/ \hskip 0.1cm0.36 \hskip 0.1cm/ 158} & \multicolumn{3}{c}{\textbf{0.055}\hskip 0.1cm/ \hskip 0.1cm\textbf{0.97}\hskip 0.1cm / \hskip 0.1cm73} & \multicolumn{3}{c}{0.122\hskip 0.1cm/\hskip 0.1cm0.63\hskip 0.1cm/\hskip 0.1cm35} \\
\begin{tabular}[c]{@{}c@{}}Topology Aware\\ Modeling\cite{chen2017topologically}\end{tabular} & \multicolumn{3}{c}{0.114\hskip 0.1cm/\hskip 0.2cm -\hskip 0.3cm /\hskip 0.1cm3022 } & \multicolumn{3}{c}{0.346\hskip 0.1cm /\hskip 0.2cm 0.56\hskip 0.1cm /\hskip 0.1cm320} & \multicolumn{3}{c}{0.40\hskip 0.1cm /\hskip 0.1cm 0.73\hskip 0.15cm /\hskip 0.1cm355 } & \multicolumn{3}{c}{0.22\hskip 0.1cm /\hskip 0.2cm - \hskip 0.2cm/\hskip 0.1cm1373} \\
Groud Truth             & \multicolumn{3}{c}{0.086\hskip 0.1cm /\hskip 0.2cm - \hskip 0.2cm/ 80} & \multicolumn{3}{c}{0.171\hskip 0.1cm /\hskip 0.2cm - \hskip 0.2cm /\hskip 0.1cm54} & \multicolumn{3}{c}{0.052\hskip 0.1cm /\hskip 0.2cm - \hskip 0.2cm/\hskip 0.1cm68} & \multicolumn{3}{c}{0.063\hskip 0.1cm / \hskip 0.2cm - \hskip 0.2cm/\hskip 0.1cm54} \\
Ours                    & \multicolumn{3}{c}{0.490\hskip 0.1cm/\hskip 0.1cm 0.73\hskip 0.1cm /\hskip 0.1cm\textbf{19}} & \multicolumn{3}{c}{0.263 \hskip 0.1cm/\hskip 0.1cm 0.67\hskip 0.1cm / \hskip 0.1cm\textbf{13}} & \multicolumn{3}{c}{0.133\hskip 0.1cm /\hskip 0.1cm0.93\hskip 0.1cm/ \hskip 0.1cm\textbf{17}} & \multicolumn{3}{c}{0.096\hskip 0.1cm / \hskip 0.1cm0.92\hskip 0.1cm /\hskip 0.1cm\textbf{27} } \\ \bottomrule
\end{tabular}}
\caption{Quantitative results with traditional methods}
\label{table:5}
\end{table*}
% Figure environment removed

% % Figure environment removed



%\textbf{Precision and Recall} The corner and edge precision (CoP and EP) show the false classification rate. The corner and edge recall (CoR and ER) represent the missed classification rate.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}


%\begin{equation}
%    \begin{split}
%     Precision &= \frac{TP}{TP + FP} \\
%     Recall &= \frac{TP}{TP + FN} 
%    \end{split}
% \end{equation}

\subsection{Benchmark Results}
\textbf{Baselines On Deep Learning Methods} Table \ref{table:3} shows quantitative results of supervised and self-supervised methods with different feature extractors on the entry-level Building3D dataset. Although these methods show good performance in CP and EP metrics, they tend to perform poorly in CR and ER metrics, particularly in ER metric. This indicates that these methods struggle to detect all edges in the wireframe models, resulting in missing edges. In general, our supervised method performs the best in terms of F1 score in the supervised learning category. Our self-supervised performs the best in corner F1 score. Table \ref{table:4} shows that our supervised method outperforms the only existing deep-learning method \cite{li2022point2roof} in roof reconstruction by a large margin on the Tallinn Building3D. This is probably because Point2Roof was designed without sufficient real-world data for training, which limits its applicability to real-world scenarios beyond simulated data. As shown in Table \ref{table:3} and Table \ref{table:4}, all methods don't perform well in the ER metric, probably because of data incompleteness such as missing corner point clouds and insufficient edge feature extraction. This is also one of the major challenges in the Building3D dataset. For the self-supervised baseline, we use the pre-trained self-supervised module to replace the supervised feature extractor\cite{zhao2021point}. Then use partial labeled data at a reduced ratio, such as 1\%, 10\%, 20\%, 50\% and 80\% to fine tune the network to test its performance. Table \ref{table:4} indicate that the performance of self-supervised method is increased when using more labeled data. Our method achieves generally better performance at the same reduced data ratios compare to the Point-MAE\cite{pang2022masked}. More experiments are provided in our supplementary material. 

% % Figure environment removed

{\textbf{Modeling Results on Handcrafted Features} We evaluate three representative methods, 2.5D Dual Contouring \cite{zhou20102}, PolyFit \cite{nan2017polyfit}, and Topology Aware Modeling \cite{chen2017topologically}. 
These traditional methods typically require intensive parameter tuning from experts who well understand the algorithms, and errors in each step are accumulated which affect the final reconstruction. Specifically, the PolyFit requires complete facade point cloud information for the reconstruction process.  The 2.5D Dual Contouring and Topology Aware Modeling require denser point clouds as input.
%Noted that compared with the our method inputting the open roof point clouds, the PolyFit requires complete facade information for the reconstruction process.  The 2.5D Dual Contouring and Topology Aware Modeling require denser point clouds as input 
On the contrary, our deep learning method is fed into open point clouds and can be used by non-professionals without any parameter tuning. 
% Table \ref{table:5} and show quantitative and visualization results. 
Table \ref{table:5} shows quantitative results compared with the traditional methods. 
Although our method doesn't surpass traditional methods in terms of RMSE and IoU, the generated models have fewer faces, resulting smoother models as shown in Fig.\ref{fig:4}. This is because the ground truth wireframe in the Building3D dataset serves as guide to generate smoother mesh models.  

%The visualization results of input point cloud, ground truth mesh model and three representative reconstruction methods are provided in the supplementary material. Meanwhile, we also calculate values of the number of faces and RMSE (root-mean-square error) on the selected medium difficulty five building models A, B, C, D and E as shown in table \ref{tab:3}. \textcolor{blue}{the explanation of indexs faces and RMSE.}

%ground truth evaluation it's challenging for previous method evulation since there is no such dataset. Now  the mesh models enable us to better evaluate previous building modeling methods. IoU metric  

%Firstly, the building point clouds are required to calculate the normal of each point. Particularly, the ground height needs to be provided to limit the the height of generated mesh models in 2.5D Dual Contouring method. The topology Aware modeling method firstly generates the intermediate shp file for input point clouds, and then reconstructs the corresponding mesh model from the file. It can lead to the accumulation of errors. The error accumulation is more obvious in the PolyFit method. In addition, the PolyFit method requires more strict point cloud input and professional manual intervention. Specifically, The PolyFit method is designed to reconstruct closed building point clouds. Thus, the input point clouds consist of roof point clouds and facade point clouds sampled from ground truth meshes without roof. Then, RANSC algorithm is used to extract planar groups, where it is extremely important to select a proper point threshold representing the minimum number of points for each planar according to professional background knowledge.

%the input of PC2WF is dense (about 200-300 thousand), if our proposed dataset adopted this data pre-processing techniques to train the model which will only 20\%-30\% dataset satisfy the training requirement. So, we think that it is unfair to compare these two methods.}
% % Figure environment removed

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}




%\subsection{Challenges} 
%-------------------------------------------------------------------------


%------------------------------------------------------------------------
\section{Multi-purpose of Building3D Dataset}
%The Building3D dataset can be extended to support various downstream tasks including building semantic \& part segmentation, mesh simplification, footprint detection, aerial path planing.

\textbf{Building Semantic \& Part Segmentation} 
Our building point clouds, roof point clouds and facade point clouds have been assigned unique labels when building the roof reconstruction dataset. Therefore, Building3D can be extended for evaluating semantic segmentation and part segmentation of buildings. Large-scale real-world point cloud processing are very challenging in terms of semantic segmentation and part segmentation tasks etc. 
 
\textbf{Mesh Simplification} 
 Building3D dataset provides a large collection of building point clouds that can be triangulated into continuous and intricate mesh models. These triangulated mesh models can be used as original input for mesh simplification. Moreover, the quality of mesh simplification can be evaluated by calculating 3D mesh IoU between ground truth mesh models provided by Building3D dataset and simplified mesh models. 

\textbf{Footprint Detection} 
In our dataset, building models are generated by using 2D building footprints and aerial LiDAR point clouds. The Building3D dataset provides 2D building footprints as ground truth labels, which can be used to train and evaluate building footprint extraction methods from aerial LiDAR point clouds. Building footprint extraction from aerial LiDAR point clouds or images is important for producing maps which can be used in various applications such as urban planning and navigation.

\textbf{Aerial Path Planning}  
 Aerial path planning is a process of planning a path from starting point to target point with minimum energy consumption and collision avoidance. It considers factors such as potential threats of collision and path length. Building3D dataset provides large-scale 3D urban models that can be used as obstacle factors to train drones for finding optimal paths.

\section{Conclusions and Future Work}
In this paper, we present an urban-scale dataset for building roof modeling from aerial LiDAR point clouds. It consists of more than 160 thousands buildings, covering about 998 $Km^2$ of urban landscape. Besides mesh models and real-world LiDAR point clouds, it is the first time to release wireframe models which transforms 3D building reconstruction into a classification problem. We also provide two new baselines, a supervised and a self-supervised learning method, allowing a fair comparison between two learning modalities. The evaluation results indicate that our dataset is challenging and creates new opportunities for urban modeling research. We believe that this work will help advance future research on several fundamental problems as well as common object modeling such as mesh simplification and remeshing. In the future work, we aim to add detailed building facade models to enable LoD3 modeling for photorealistic building model generation, and associate address data to each building for holistic 3D scene understanding. 
%The end-to-end deep learning solution reduces accumulated errors to some degree. We evaluate performance of state-of-the-art building modeling algorithms on Building3D. 




%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}


\end{document}
