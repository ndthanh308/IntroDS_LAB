 %%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
% \documentclass[sigconf,authordraft]{acmart}
\documentclass[sigconf]{acmart}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subfigure}
% \usepackage{subfig}
\usepackage{caption}
\usepackage{natbib}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{balance}

% \newcommand{\setParDis}{\setlength {\parskip} {0.3cm} }

%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


\copyrightyear{2023}
\acmYear{2023}
\setcopyright{acmlicensed}
\acmConference[CIKM '23] {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management}{October 21--25, 2023}{Birmingham, United Kingdom.}
\acmBooktitle{Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21--25, 2023, Birmingham, United Kingdom}
\acmPrice{15.00}
\acmISBN{979-8-4007-0124-5/23/10}
\acmDOI{10.1145/3583780.3615463}
% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.

\settopmatter{printacmref=true}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
% \acmSubmissionID{xxxx}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Continual Learning in Predictive Autoscaling}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\settopmatter{authorsperrow=4}

\author{Hongyan Hao}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{hongyanhao.hhy@alipay.com}

\author{Zhixuan Chu}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{chuzhixuan.czx@alipay.com}

\author{Shiyi Zhu}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{zhushiyi.zsy@antgroup.com}

\author{Gangwei Jiang}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{gangwei.jgw@antgroup.com}

\author{Yan Wang}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{luli.wy@antgroup.com}


\author{Caigao Jiang}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{caigao.jcg@antgroup.com}

\author{James Y Zhang}

\affiliation{%
  \institution{Ant Group}
  \country{USA}
}
\email{james.z@antgroup.com}

\author{Wei Jiang}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{shouzhi.jw@antgroup.com}

\author{Siqiao Xue}
\authornote{Corresponding author.}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{siqiao.xsq@antgroup.com}

\author{Jun Zhou}
\affiliation{%
  \institution{Ant Group}
  \country{China}
}
\email{jun.zhoujun@antgroup.com}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Hongyan Hao et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Predictive Autoscaling is used to forecast the workloads of servers and prepare the resources in advance to ensure service level objectives (SLOs) in dynamic cloud environments.
  However, in practice, its prediction task often suffers from performance degradation under abnormal traffics caused by external events (such as sales promotional activities and applications' re-configurations), for which a common solution is to re-train the model with data of a long historical period, but at the expense of high computational and storage costs.
  To better address this problem, we propose a replay-based continual learning method, i.e., \textbf{D}ensity-based \textbf{M}emory \textbf{S}election and \textbf{H}int-based Network Learning \textbf{M}odel (DMSHM), using only a small part of the historical log to achieve accurate predictions.
  First, we discover the phenomenon of sample overlap when applying replay-based continual learning in prediction tasks. In order to surmount this challenge and effectively integrate new sample distribution, we propose a density-based sample selection strategy that utilizes kernel density estimation to calculate sample density as a reference to compute sample weight and employs weight sampling to construct a new memory set.
  Then we implement hint-based network learning based on hint representation to optimize the parameters.
  Finally, we conduct experiments on public and industrial datasets to demonstrate that our proposed method outperforms state-of-the-art continual learning methods in terms of memory capacity and prediction accuracy. Furthermore, we demonstrate remarkable practicability of DMSHM in real industrial applications.
  
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003227.10010926</concept_id>
       <concept_desc>Information systems~Computing platforms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Computing platforms}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{continual learning, regression task, autoscaling}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   % Figure removed
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

% Figure environment removed


Predictive Autoscaling is a cloud computing method that automatically adjusts cloud services to ensure that resource utilization is maintained within a reasonable range.
This technology has been widely used by major cloud service providers,
such as Google Cloud \cite{google_book} and Azure \cite{azure_book}, helping consumers optimize their resource utilization and cost efficiencies. 
In this paper, we study the optimization of resource usage efficiency on the cloud of Alipay, the world-leading digital payment platform.
The pipeline of Predictive Autoscaling can be divided into 3 steps, i.e., workload forecasting, CPU utilization estimation, and scaling decision. We focus on the first two regression tasks \cite{DBLP:journals/tsc/AbdullahIBPC22,DBLP:conf/kdd/XueQSLZTMWWH0ZL22} in this paper. 

% Figure environment removed

In the above tasks, the data distribution of training samples is usually non-stationary due to external events (such as sales promotional activities and re-configurations of applications), as illustrated in Fig~\ref{fig:sample_dists}, whose data are obtained from an application of Alipay's ecosystem. 
The common solution, as shown in Fig.~\ref{fig:training_modes}, is periodically re-training the model from scratch with a long range of historical data, however, with high costs in terms of both computation and memory consumption.

For example, in order to store a training set of CPU utilization data and to train estimation models for one zone of one application on Alipay's ecosystem, about 119GB of data storage and the computation of 113 minutes of 16000 CPU cores are required. 
One solution is to fine-tune the previously-learned model and make it adaptable to the changes in the data distribution. 
However, this approach may result in catastrophic forgetting, where the new knowledge completely or partially replaces the old knowledge\cite{DBLP:conf/eccv/HayesKSAK20}.

In this paper, we focus on developing a more practical and effective approach for the regression tasks in Predictive Autoscaling.
Inspired by the application of continual learning (CL) \cite{chu2023continual,chu2023continuala}, which learns from a stream of incoming data while avoiding forgetting prior knowledge, we combine replay-based CL with a regression model to solve our problem. As shown in Fig.~\ref{fig:training_modes}, CL maintains a memory set to store informative historical samples and concatenates it with the current data set to serve as the training set. The storage of the memory set is much smaller than that of the single periodic data set. 
In our practical scenario, resource utilization is decreased by almost a quarter of the initial consumption,
as illustrated in Fig.~\ref{fig:resource_comparison}. 
However, after analyzing the properties in the autoscaling scenario, two specific challenges lie in our continual learning paradigm compared with existing works:
i) \textbf{Sample Overlap}: 
In practice,
data distribution of regression problem is usually imbalanced \cite{DBLP:conf/icml/YangZCWK21,DBLP:conf/icml/GongMT22}, 
and besides, sample distributions have certain similarities, leading to area overlapping in distributions between the memory set of replay-based continual learning and the current data set, namely, \textit{sample overlap};
ii) \textbf{Regression Task}: 
 Most continual learning methods are proposed based on the classification tasks, but they cannot be directly applied in a regression task, especially in the process of network training using knowledge distillation \cite{DBLP:journals/spm/ChengWZZ18,DBLP:conf/iccv/SaputraGAMT19}.

To address these challenges, we propose Density-based Memory Selection
and Hint-based network learning Model (DMSHM).
For sample overlap in continual learning, we design a density-based memory selection strategy, utilizing kernel density estimation to calculate sample density as a reference to compute sample weight, followed by constructing a new memory set using weight sampling, to achieve a balance between battling sample overlap and fusing new sample distribution.
To better preserve predicting performance, we apply the hint-based training strategy by storing previous representations and 
utilizing the prior model to produce an intermediate representation of the current sample, which can be regarded as a hint, to recall the prior knowledge.
The ``hint" fills the gap that ``dark knowledge" is inapplicable to regression problems \cite{DBLP:conf/iccv/SaputraGAMT19}. Our main contribution can thus be summarized as follows:
\begin{itemize}[leftmargin=*]
\item We propose a new continual learning method DMSHM for regression tasks in Predictive Autoscaling. We design sample density-based scores to endow with the weight of the sample to construct a memory set and adopt a hint-based network learning strategy to adjust parameters. 
\item We conduct experiments on public and industrial datasets, and demonstrate DMSHM has stronger performance than state-of-the-art continual learning methods on regression tasks.
\end{itemize}



\section{Background}

\subsection{Predictive Autoscaling}

Cloud service providers (CSPs) usually adopt a conservative approach to resource provisioning to satisfy their service level objectives (SLOs). In order to maintain the quality of their services, CSPs often set low CPU utilization targets, even when there are significant variations in the workload \cite{DBLP:conf/cloud/WangZLJRZY0L22}.
This paradigm is inefficient in terms of computation resources, energy consumption, as well as cost, which calls for improvement. 
Predictive Autoscaling forecasts the workload with the help of machine learning models and prepares proper resources in advance to satisfy SLOs with better efficiency \cite{DBLP:journals/tsc/AbdullahIBPC22, DBLP:journals/tsc/SotiriadisBAB19,DBLP:conf/icws/ZhangWPZY20,DBLP:conf/kdd/XueQSLZTMWWH0ZL22,xiao2023automatic}.  
In our industrial practices, we observe that the consumption of storage and computing resources is very high, due to a large number of applications and zones, incentivising further optimization of resource usage and prediction accuracy, for which we devise a new scheme for regression tasks based on continual learning in this paper.

\subsection{Continual Learning}
To ensure the accuracy of our regression model in practical scenarios, a common practice is to periodically retrain using data from the past month. The time span of training dataset is typically chosen so that the cyclical properties of the data can be learned. Continual learning enables us to maintain predictive accuracy on relatively short-term data with reduced resource costs.
However, a common challenge of continual learning is \textit{catastrophic forgetting}, i.e., as new tasks or domains are introduced, the previously acquired knowledge cannot be retained, resulting in performance degradation.
\citeauthor{DBLP:journals/pami/LangeAMPJLST22} \cite{DBLP:journals/pami/LangeAMPJLST22} conduct a critical implementation, compared against mainstream CL methods \cite{DBLP:conf/cvpr/RebuffiKSL17,article,DBLP:conf/iclr/ChaudhryRRE19,DBLP:conf/nips/RolnickASLW19}. A general continual learning (GCL) setting \cite{DBLP:conf/cvpr/AljundiKT19} is also proposed for real-world applications, appending an assumption that the boundaries of tasks are agnostic, based on which, several approaches \cite{DBLP:conf/nips/BuzzegaBPAC20,DBLP:conf/iclr/SunCHLT22} are proposed to make GCL more practical. \citeauthor{DBLP:journals/corr/abs-2101-00926} \cite{DBLP:journals/corr/abs-2101-00926} further propose CLeaR in the context of power forecasting, to address the regression task of GCL. Moreover, the gap of application of knowledge distillation between classification and regression problems are elaborated in \cite{DBLP:journals/spm/ChengWZZ18,DBLP:conf/mipr/TakamotoMI20,DBLP:conf/iccv/SaputraGAMT19}, and to address this problem, we leverage the idea of ``hint'' \cite{DBLP:journals/corr/RomeroBKCGB14} in this work.



% Figure environment removed



\section{Preliminary}
As mentioned in the previous section, we try to solve regression tasks (workload forecasting and CPU utilization estimation) in Predictive Autoscaling using continual learning. 
For the convenience of description, we define the $\textit{i}$-th $k-$dimensional input sample of the regression model as $x_{i} \in \mathbb{R}^{k}$ and the label as $y_{i}$, which is a one-dimensional scalar in CPU utilization estimation or a multidimensional vector in workload forecasting. 
In the $\textit{n}$-th period, we acquire dataset $S^{n} = (X^{n}, Y^{n}) = \{ (x_{i}, y_{i}) \}_{i=1}^{N^{n}}$, whose size is $N^{n}$, and the cumulative sample size before present period is $A^{n}=\sum_{i=1}^{n-1}N^{i}$.
In addition, we maintain a memory set $\mathcal{M}^{n}$ with fixed size $M$, which is updated every time a new dataset arrives. 
Using the setting of an ordinary supervised learning problem, we define the representation function as $h^{n}$, indicating an underlying mapping from the input sample to a lower dimension vector $z = h^{n}(x)$. We then define $g^{n}$ to map the representation vector to prediction result. We formulate the prediction function as $f^{n}(x) = g^{n}(h^{n}(x)) = g^{n}(z)$,
whose training parameters are $\theta_h$ and $\theta_g$ for the representation and linear function, respectively. We define the whole parameter set as $\theta$, and it is optimized by defined loss functions which are specified in Section~\ref{sec:param_train} later.



\section{Method}



In the following sections, we detail density-based memory selection and hint-based network learning to overcome catastrophic forgetting from the perspectives of sampling and training, respectively. The former aims to select the most informative samples to represent history to help the model recall the prior knowledge, while the latter uses the hint-based training strategy to train the model with a memory set and current dataset.

\subsection{Density-based Memory Selection}
\label{sec:dms}

Many replay-based methods use reservoir sampling to update the memory set from $\mathcal{M}^{n-1}$ and $S^{n}$, which ensures that each sample has the same probability of being selected into a new memory set $\mathcal{M}^{n}$.  However, data imbalance can result in some data drawn from the overlapping area of the distribution being sampled more than once. Our expectation for the memory set is that it can represent the distribution of historical samples, but the repetition in sample sets can lead to a more severe imbalance, which would impair the efficiency of the memory set.
To overcome this problem, we design a density-based memory selection (DMS) mechanism. 

As shown in Fig.~\ref{fig:framework}, our overall goal is to use weight sampling \cite{zhou2023ptse} to select samples from $\mathcal{M}^{n-1}$ and $S^{n}$ as the memory set of the next period, so we focus on the design of weights. In order to overcome the problem of sample overlap while incorporating the sample in the new distribution into the memory set, we designed two scoring functions, \textbf{DensityScore} and \textbf{ShiftLevelScore}. 

We devise some indicators based on sample density to determine whether the overlapping phenomenon or new distribution occurs. 
Specifically, considering the similarity among the sample distributions across different time steps, 
a smaller mean value difference of $\mathcal{M}^{n-1}$ and $S^{n}$ would result in a smaller variance of combined sample distribution, resulting in the overlap phenomenon.
In principle, the distribution of $\mathcal{M}^{n-1}$ can better represent the real distribution of the previous sample. 
We use kernel density estimation to fit the samples in $\mathcal{M}^{n-1}$, and obtain the mapping function $d(x) \in \mathbb{R}$, formulated as follows:
\begin{equation}
\begin{split}
    d(x) &= \frac{1}{|\mathcal{M}^{n-1}|b^{k}} \sum_{i=1}^{|\mathcal{M}^{n-1}|} K(\frac{x - x_{i}}{b}), \\ 
    K(z) &\geq 0, \quad \int K(z)\mathbf{d}z=1. \nonumber
\end{split}
\label{equation:probs_memory_set}
\end{equation}
We choose the Gaussian kernel function as $K(z)$, and we use maximum likelihood cross-validation to obtain the optimal bandwidth $b$.
Then we use $d(x)$ to calculate the density of each sample in $\mathcal{M}^{n-1} \cup S^{n}$, and samples with very small density are called \textit{border samples}. 
To avoid extreme cases of weight skewness caused by differences in density scales, we employ the $sigmoid$ function to smooth sample density as $q^{n}(x)=1/(1 + e^{-d(x)})$.
We define $q^{n}(x): \mathbb{R}^{k} \to \mathbb{R}_{+}$ as \textbf{DensityScore} function, which outputs a score to reflect the degree of sample aggregation.


\begin{algorithm}[t]
        \caption{DMS \textbf{SampleWeight}}
        \label{alg:sample_score}
        \begin{algorithmic}[1]
        \STATE \textbf{Input:} The memory set $\mathcal{M}^{n-1}$ with budget size $M$;
        The dataset $\mathrm{S}^{n}$ with size $N^{n}$; The cumulative sample size $A^{n}=\sum_{i=1}^{n-1}N^{i}$;
        The \textbf{DensityScore} function $q^{n}$;
        The balance factor $\gamma$.
        \FOR{$i = 1, \dots, (|\mathcal{M}^{n-1}|+N^{n}) $}
            \IF{$i \leq |\mathcal{M}^{n-1}|$}
                \STATE $ (x_{i},z_{i}, y_{i}) \leftarrow  \mathcal{M}^{n-1}_{i}$ \qquad\qquad \text{// $i$-th sample in $\mathcal{M}^{n-1}$}
                \STATE $w(x) = (1-\gamma) \times q^{n}(x_{i}) + \gamma \times \frac{A^{n}}{A^{n}+N^{n}}$
            \ELSE
                \STATE $ (x_{i},y_{i}) \leftarrow  \mathcal{S}^{n}_{i-|\mathcal{M}^{n-1}|}$ \qquad \text{// ($i$-$|\mathcal{M}^{n-1}|$)-th sample in ${S}^{n}$}
                \STATE $w(x) = (1-\gamma) \times q^{n}(x_{i}) + \gamma \times \frac{M}{A^{n}+N^{n}}$
            \ENDIF
        \ENDFOR
        \STATE $ W^{n} := \{w(x)| x \in \mathcal{M}^{n-1} \cup S^{n}\}$
        \RETURN The $n$-th sample weight set $W^{n}$
        \end{algorithmic}
\end{algorithm}


\begin{algorithm}[htp]
        \caption{Density-based Memory Selection}
        \label{alg:dms}
        \begin{algorithmic}[1]
        \STATE \textbf{Input:} The memory set $\mathcal{M}^{n-1}$ with the budget size $M$;
        The dataset $\mathrm{S}^{n}$ with size $N^{n}$; The cumulative sample size $A^{n}=\sum_{i=1}^{n-1}N^{i}$;
        The representation function $ h^{n}(x) $.

        \IF{n = 1}
            \STATE $W^{n} = $ \textbf{SampleWeight}($ \varnothing, M, S^{n}, N^{n}, 0, 0.5, 1$)
            \STATE $(X^{n}_{m}, Y^{n}_{m}) \leftarrow$ \textbf{WeightedSampling}$( S^{n}, W^{n} )$
        \ELSE
            \STATE $d(x) \leftarrow $ \textbf{KernelDensityEstimator}($\mathcal{M}^{n-1}$)
            \STATE $q^{n}(x) = \frac{1}{1 + e^{-d(x)}}$ \qquad\qquad\qquad\text{//} \textbf{DensityScore}
            \STATE $\gamma = $ \textbf{ShiftLevelScore} $( \{ q^{n}(x) | x \in \mathcal{M}^{n-1} \cup S^{n} \})$
            \STATE $W^{n} = $ \textbf{SampleWeight}($ \mathcal{M}^{n-1}, M, S^{n}, N^{n}, A^{n}, q^{n}, \gamma$)
            \STATE $(X^{n}_{m}, Y^{n}_{m}) \leftarrow$ \textbf{WeightedSampling}$( \mathcal{M}^{n-1} \cup S^{n}, W^{n} )$
        \ENDIF
        \STATE $Z^{n}_{m} = \{ z | z = h^{n}(x), x \in X^{n}_{m} \} $ 
        \STATE Update $\mathcal{M}^{n} \leftarrow (X^{n}_{m}, Z^{n}_{m}, Y^{n}_{m})$
        \RETURN The $n$-th memory set $\mathcal{M}^{n}$
        \end{algorithmic}
\end{algorithm}


From the design of the score above, we can infer that the score of samples in the distribution of $\mathcal{M}^{n-1}$ is larger than the rest.  A large number of border samples indicates that the distribution of the combined sample set is quite different from the previous ones, which is called \textit{distribution shift}.  However, sample selection solely relying on the density score can lead to excessive tendency to the samples of $\mathcal{M}^{n-1}$, voiding our attempt to incorporate the new distribution. To mitigate the problem, we devise \textbf{ShiftLevelScore} function that generates the indicator $\gamma$, to describe the level of distribution shift. We use the Gaussian mixture model with two components to fit the density scores of $\mathcal{M}^{n-1} \cup S^{n}$, and we formulate the absolute difference between the means of the two components as $\gamma$. A large $\gamma$ implies the occurrence of distribution shift, and border samples need to be attended, for which 
we introduce the reservoir sampling strategy.
For the batch data stream scenario, we employ two biased coefficients $\frac{A^{n}}{A^{n}+N^{n}}$ and $\frac{M}{A^{n}+N^{n}}$ on samples of $\mathcal{M}^{n-1}$ and $S^{n}$ to ensure that all samples are sampled with equal probability (see Sec.~\ref{sec:bias}).
To trade-off between avoiding distribution shift and mitigating overlap, the coefficient $\gamma$ is used to control reservoir sampling and score-based sampling. We define the procedure as \textbf{SampleWeight} function, which is described in Algorithm~\ref{alg:sample_score}.

After the above processing, we can obtain the sample weight set $W^{n}$, which is then used to select $M$ samples as $\mathcal{M}^{n}$. The detailed description of DMS is shown in Algorithm~\ref{alg:dms}.
Note that the memory set stores not only the selected samples and labels but also the outputs of intermediate representation from the current model, whose intention is explained in Sec.~\ref{sec:param_train} later.

\subsection{Proof of Biased Coefficients}
\label{sec:bias}
The objective of the reservoir sampling algorithm is to choose a fixed number of samples without replacement in the face of an uncertain total number of samples in a single pass while ensuring that each sample is selected with equal probability.
Unlike the traditional reservoir sampling method that draws one sample per period, we process a sample set $S^{n}$ containing $N^{n}$ samples in each period.
The sample set $\mathcal{M}^{n}$ consists of subsets of $\mathcal{M}^{n-1}$ and $S^{n}$ by sampling. Since the samples in $\mathcal{M}^{n-1}$ are sourced from the sample set prior to the $n$-th and $S^{n}$ is being sampled for the first time, it is necessary to assign sampling weights $w^{bias}(x|x \in \mathcal{M}^{n-1})=\frac{A^{n}}{A^{n}+N^{n}}$ and $w^{bias}(x|x \in S^{n})=\frac{M}{A^{n}+N^{n}}$ to the samples in $\mathcal{M}^{n-1}$ and $S^{n}$ to ensure that all samples $ \bigcup_{i=1}^{n}S^{i}$ are sampled with equal probability.  We term the above sampling weights as \textit{biased coefficients}, whose rationality is demonstrated in the mathematical proof below. 

\begin{proof}
The mathematical induction can be divided into the initial step and the inductive step.

\begin{itemize}[leftmargin=*]
    \item \textbf{Initial step}: For $n=1$, $A^{1}=0$ and $|\mathcal{M}^{0}|=0$, then
    \begin{align}
        w^{bias}(x|x \in \mathcal{M}^{0}) &=\frac{A^{1}}{A^{1}+N^{1}}=\frac{0}{0+N^{1}}=0 \nonumber \\
        w^{bias}(x|x \in S^{1}) &=\frac{M}{A^{1}+N^{1}}=\frac{M}{0+N^{1}}=\frac{M}{N^{1}}. \nonumber \\
        w(x|x \in S^{1}) &= w^{bias}(x|x \in S^{1}) = \frac{M}{A^{1}+N^{1}}  \nonumber
    \end{align}
    As of the $\text{1-st}$ period's end, all samples $ \bigcup_{i=1}^{1}S^{i}$ are sampled with equal probability.
    \item \textbf{Inductive step}:
    We assume that the proposition is true for $n=k$, i.e., after applying biased coefficients $w^{bias}(x|x \in \mathcal{M}^{k-1})=\frac{A^{k}}{A^{k}+N^{k}}$ and $w^{bias}(x|x \in S^{k})=\frac{M}{A^{k}+N^{k}}$, all samples $\bigcup_{i=1}^{k}S^{i}$ have the same weights:
    \begin{align}
        w(x|x \in \mathcal{M}^{k-1})=w(x|x \in S^{k})=\frac{M}{A^{k}+N^{k}} . \nonumber
    \end{align}
    For $n=k+1$, after applying biased coefficients, the sample weight of $\mathcal{M}^{k}$ and $S^{k+1}$ are
    \begin{align}
        w(x|x \in \mathcal{M}^{(k+1)-1}) &= w(x|x \in \mathcal{M}^{k-1}) \times w^{bias}(x|x \in \mathcal{M}^{(k+1)-1}) \nonumber \\ 
        &= \frac{M}{A^{k}+N^{k}} \times \frac{A^{k+1}}{A^{k+1}+N^{k+1}} \nonumber \\ 
        &= \frac{M}{A^{k+1}+N^{k+1}}, \nonumber
    \end{align}
    and 
    \begin{align}
        w(x|x \in {S}^{k+1}) &= \frac{M}{A^{k+1}+N^{k+1}}, \nonumber
    \end{align}
    i.e., the same weight of $\mathcal{M}^{k}$ and $S^{k+1}$ means the samples of $\bigcup_{i=1}^{k+1}S^{i}$ have the same probability of being sampled.
\end{itemize}
\end{proof}




\subsection{Hint-based Network Learning}
\label{sec:param_train}
To optimize parameters for mitigating catastrophic forgetting, most approaches leverage dark knowledge \cite{DBLP:journals/corr/HintonVD15} to retain the prior knowledge, where the main idea is to store softened logits output, which is then used to guide the optimization trajectory of the current model. However, this approach does not apply to the regression problem. Specifically, the output of the regression task is a continuous value, which has the same properties as the ground truth along with an unknown error distribution \cite{DBLP:conf/mipr/TakamotoMI20,DBLP:journals/spm/ChengWZZ18}, so that keeping the previous dark knowledge of samples is not a suitable solution for our problem.
Inspired by the theory of \citeauthor{DBLP:journals/corr/RomeroBKCGB14} \cite{DBLP:journals/corr/RomeroBKCGB14}, which suggests that intermediate representation provides ``hint'' for the current model to imitate the previous one, we devise our hint-based network learning strategy.

During the network training, we have memory set $\mathcal{M}^{n-1}$ and current dataset $S^{n}$. Before training in the current step, we utilize the representation network of the previous step $h^{n-1}(x)$ to generate additional hint $Z^{n}_{hint} = h^{n-1}(X^{n})$. 
We define the loss $\mathcal{L}_{hint}$, which is the error of intermediate output $\hat{Z}^{n}_{hint} = h^{n}(X^{n})$ and $Z^{n}_{hint}$, formulated as:
\begin{equation}
    \mathcal{L}_{hint} := \mathbb{E}_{(x,y) \sim S^{n}} \left[ l(h^{n-1} (x), h^{n} (x) ) \right] ,
    \nonumber
\end{equation}
where $l(h^{n-1} (x), h^{n} (x) )$ is the mean absolute error between $\hat{Z}^{n}_{hint}$ and $Z^{n}_{hint}$, and $l$ means the same for the rest of the paper.
The network $h^{n}$ regards $Z^{n}_{hint}$ as guidance for conducting training close to the previous optimization trajectory, which assists the network in recalling knowledge to alleviate catastrophic forgetting.

We also minimize the error between hints $Z^{n-1}_{m}$ and intermediate representation $\hat{Z}^{n-1}_{m} = h^{n}(X^{n-1}_{m})$ of memory samples, along with the error between their ground truth $Y^{n-1}_{m}$ and predictions $\hat{Y}^{n-1}_{m} = f^{n}(X^{n-1}_{m})$, and we refer to the loss as $\mathcal{L}_{memory}$:
\begin{equation}
\begin{split}
    \mathcal{L}_{memory} &:= \alpha 
\cdot \mathbb{E}_{(x,z,y) \sim \mathcal{M}^{n-1}} \left[ l(z, h^{n}(x)) \right] \nonumber \\
    &\quad \ + \beta \cdot 
 \mathbb{E}_{(x,z,y) \sim \mathcal{M}^{n-1}} \left[ l(y, f^{n}(x)) \right], \nonumber 
 \end{split}
\end{equation}
where the coefficients $\alpha$ and $\beta$ serve as weights to adjust the importance of following the optimization trajectory of the former model ($\theta^{n-1}$) and recalling from ground-truth labels $Y^{n-1}_{m}$. 

Furthermore, the model needs to acquire new knowledge from the current dataset $S^{n}=(X^{n}, Y^{n})$, and we set the network to minimize weighted mean absolute error, defined as:
\begin{equation}
    \mathcal{L}_{cur} := \mathbb{E}_{(x,y) \sim S^{n}} \left[ l(y, f^{n} (x) ) \right] ,
    \nonumber
\end{equation}
where the $\mathcal{L}_{cur}$ reflect the convergence process on new data set. 

To integrate the three losses $\mathcal{L}_{hint}$, $\mathcal{L}_{memory}$ and $\mathcal{L}_{cur}$, we sum the terms up with coefficients $\delta$ and $\xi$ to trade-off tendencies between recalling previous information and learning new knowledge, to form the total loss function ($\mathcal{L}$) as follows:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{memory} + \xi \cdot \mathcal{L}_{hint} + \delta \cdot \mathcal{L}_{cur} .
    \nonumber
\end{equation}

\section{Experiments}
\subsection{Datasets} 
To evaluate the effectiveness of our proposed method, we conduct experiments on ATEC's\footnote{https://github.com/TRaaSStack/Forecasting} public industrial dataset, which is for workload forecasting competition.  We select one traffic series and split all data into $8$ periodic datasets in chronological order to perform the evaluation of the workload forecasting task. Furthermore, we also perform the evaluation of CPU utilization estimation (CUE) on a zone of a main application whose data was collected from an industrial scenario. 
It includes $9$ types of traffic values and CPU usage per minute of 8 days, whose distributions are significantly different from each other, as shown in Fig.~\ref{fig:cpu_dist}. We need to model the mapping between traffic and CPU usage.

\subsection{Baselines and Metrics}
Fine-tuning \cite{DBLP:journals/pami/LangeAMPJLST22} is a naive baseline for our comparison of prediction performance and we further compare against DER++ \cite{DBLP:conf/nips/BuzzegaBPAC20} and CLeaR \cite{DBLP:journals/corr/abs-2101-00926}. The former is an excellent general continual learning method, and the latter applies continual learning on regression tasks. 
We use different networks as $h(x)$ for two regression tasks, i.e., an LSTM layer for ATEC task and an MLP layer for CUE task. 

To measure memorization and forecast performance of the model in a continual learning setup, we regard the first dataset as a historical set and the samples composed of random selection from all $8$ datasets as a future set.
After every period, we record the mean square error (MSE) of the model on the historical and future set. 
We define forgetting error (FE) as the average MSE of all steps on the historical set, which indicates the ability to memorize and overcome catastrophic forgetting.
In addition, we define the prediction error (PE) as the MSE of the final period on the future set, reflecting the performance of model generalization. A small PE value means better predictive power for samples from unknown distributions.


\subsection{Experiment Results}

\begin{table*}
  \vspace{-2mm}
  \caption{Mean and standard deviation (in bracket) of experiment results on ATEC and CUE datasets.}
  \vspace{-2mm}
  \label{tab:experiments}
  \begin{tabular}{ccccl}
    \toprule
    & \multicolumn{2}{c}{ATEC} & \multicolumn{2}{c}{CUE} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    Models & FE & PE & FE & PE\\
    \midrule
Finetune & 0.0686 (0.0312) & 0.0739 (0.0437) & 33.46 (5.39) & 23.59 (6.29) \\ 
CLeaR    & 0.0512 (0.0245) & 0.0405 (0.0276) & 20.57 (4.62) & 19.02 (4.38) \\ 
DER++    & 0.0450 (0.0367) & 0.0315 (0.0125) & 18.12 (2.84) & 17.05 (2.93) \\ 
\midrule
DMSHM (w/o DMS)    & 0.0445 (0.0129) & 0.0311 (0.0174) & 17.91 (2.83) & 16.99 (2.72) \\ 
DMSHM (w/o Hint)    & 0.0426 (0.0201) & 0.0301 (0.0214) & 16.82 (1.92) & 16.90 (2.84) \\ 
\textbf{DMSHM} (ours)    & \textbf{0.0404 (0.0167)} & \textbf{0.0282 (0.0186)} & \textbf{15.70 (2.14)} & \textbf{16.85 (3.46)} \\ 
  \bottomrule 	 
\end{tabular}
\end{table*}

% Figure environment removed


The experiment results are shown in Table~\ref{tab:experiments}, where the best results are highlighted in bold, where DMSHM obtains the best performance in terms of forgetting error and prediction error on both ATEC dataset and CPU utilization estimation task.
Finetune acquires the worst performance on all metrics due to the absence of capability to overcome catastrophic forgetting. To explore the performance of the model in each period, we demonstrate MSE of the four models on the historical set and future set in 8 days, as illustrated in Fig.~\ref{fig:mse_history_point} and Fig.~\ref{fig:mse_future_point}. The MSE on the historical set of all models appears to have a tendency to decrease before the fifth day due to the fact that the datasets of these days are of high similarity.  However, on the sixth day, when a configuration change takes place, the sample distribution has a somewhat larger change, as shown in Fig.~\ref{fig:cpu_dist}. 
DMSHM is the most robust to this disturbance due to the density-based selection strategy. As for the prediction error, DMSHM shows smaller MSEs, which indicates that the samples in the memory set are more similar to the overall distribution, and the model is capable of predicting data in the future set.


\subsection{Ablation Study}


In order to evaluate the effectiveness of the density-based memory selection strategy (DMS) and hint-based network learning method (Hint) of the proposed DMSHM model, we conduct two ablation studies of DMSHM(w/o DMS) and DMSHM(w/o Hint) on all datasets.  Our first study is DMSHM(w/o DMS), where the density-based memory selection method is replaced by a reservoir sampling strategy, which assigns equal weight to all samples in the memory set and current data set in \textbf{SampleWeight} function. Given the practical sample storage scheme of batch acquisition for a new dataset, we assign weights $\frac{A^{n}}{A^{n}+N^{n}}$ and $\frac{M}{A^{n}+N^{n}}$ to the samples in memory set and the new dataset to obtain uniform selection probability for each sample.
The second ablation study is DMSHM(w/o Hint), where the hint-based network learning is replaced by conventional parameter adjustment strategy with loss function $\mathcal{L}_{memory} + \delta \cdot \mathcal{L}_{cur}$, which invalidates the model's capability of maintaining historical knowledge through hint-based parameter learning. 

According to the results provided in Table~\ref{tab:experiments}, the performance worsens after removing either DMS or the hint-based module compared to the original DMSHM.  Specifically, after replacing the DMS with a reservoir sampling strategy, DMSHM(w/o DMS) suffers a dramatic decline in performance.  Besides, DMSHM(w/o DMS) exhibits a more drastic decrease, which indicates DMS is more critical for addressing catastrophic forgetting and maintaining the forecasting accuracy, compared to the hint parameter learning.

\section{Deployment}

Our method has been successfully deployed in Alipay Cloud to serve over 20,000 zones across more than 3,000 applications, as evidenced by its impressive effectiveness and efficiency. In one representative zone, the data time span is reduced from 113 minutes to just 36 minutes, while the storage size decreased from 119 GB to 37 GB. Moreover, our method achieves CPU utilization estimation with only one-third of the original resource usage, as illustrated in Fig.~\ref{fig:resource_comparison}.
As the dataset size increases, the degree of resource-saving needs to be calculated in conjunction with the size of the current data set and memory set that replaces a large training set. The existence of the memory set exponentially reduces the amount of data needed to be stored. By significantly reducing the capital cost of application operation, our method makes a valuable contribution towards the environmental sustainability of Alipay.

\section{Conclusion}
The focus of this paper is to address the problem of inefficient resource utilization in regression tasks within Predictive Autoscaling. To solve this challenge, we propose a novel continual learning approach called DMSHM. This method is designed to efficiently manage sample overlap through a density-based memory selection strategy that leverages reservoir sampling with batch stream samples. To support the rationality of two biased coefficients, we have included inductive mathematical proof. In addition, we have implemented a hint-based network learning strategy to bridge the gap between regression tasks and continual learning. Our model has been extensively tested, and the results demonstrate its effectiveness.

\clearpage
% ACM's consolidated article template, introduced in 2017, provides a
% consistent \LaTeX\ style for use across ACM publications, and
% incorporates accessibility and metadata-extraction functionality
% necessary for future Digital Library endeavors. Numerous ACM and
% SIG-specific \LaTeX\ templates have been examined, and their unique
% features incorporated into this single new template.

% If you are new to publishing with ACM, this document is a valuable
% guide to the process of preparing your work for publication. If you
% have published with ACM before, this document provides insight and
% instruction into more recent changes to the article template.

% The ``\verb|acmart|'' document class can be used to prepare articles
% for any ACM publication --- conference or journal, and for any stage
% of publication, from review to final ``camera-ready'' copy, to the
% author's own version, with {\itshape very} few changes to the source.

% \section{Template Overview}
% As noted in the introduction, the ``\verb|acmart|'' document class can
% be used to prepare many different kinds of documentation --- a
% double-blind initial submission of a full-length technical paper, a
% two-page SIGGRAPH Emerging Technologies abstract, a ``camera-ready''
% journal article, a SIGCHI Extended Abstract, and more --- all by
% selecting the appropriate {\itshape template style} and {\itshape
%   template parameters}.

% This document will explain the major features of the document
% class. For further information, the {\itshape \LaTeX\ User's Guide} is
% available from
% \url{https://www.acm.org/publications/proceedings-template}.

% \subsection{Template Styles}

% The primary parameter given to the ``\verb|acmart|'' document class is
% the {\itshape template style} which corresponds to the kind of publication
% or SIG publishing the work. This parameter is enclosed in square
% brackets and is a part of the {\verb|documentclass|} command:
% \begin{verbatim}
%   \documentclass[STYLE]{acmart}
% \end{verbatim}

% Journals use one of three template styles. All but three ACM journals
% use the {\verb|acmsmall|} template style:
% \begin{itemize}
% \item {\verb|acmsmall|}: The default journal template style.
% \item {\verb|acmlarge|}: Used by JOCCH and TAP.
% \item {\verb|acmtog|}: Used by TOG.
% \end{itemize}

% The majority of conference proceedings documentation will use the {\verb|acmconf|} template style.
% \begin{itemize}
% \item {\verb|acmconf|}: The default proceedings template style.
% \item{\verb|sigchi|}: Used for SIGCHI conference articles.
% \item{\verb|sigchi-a|}: Used for SIGCHI ``Extended Abstract'' articles.
% \item{\verb|sigplan|}: Used for SIGPLAN conference articles.
% \end{itemize}

% \subsection{Template Parameters}

% In addition to specifying the {\itshape template style} to be used in
% formatting your work, there are a number of {\itshape template parameters}
% which modify some part of the applied template style. A complete list
% of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}

% Frequently-used parameters, or combinations of parameters, include:
% \begin{itemize}
% \item {\verb|anonymous,review|}: Suitable for a ``double-blind''
%   conference submission. Anonymizes the work and includes line
%   numbers. Use with the \verb|\acmSubmissionID| command to print the
%   submission's unique ID on each page of the work.
% \item{\verb|authorversion|}: Produces a version of the work suitable
%   for posting by the author.
% \item{\verb|screen|}: Produces colored hyperlinks.
% \end{itemize}

% This document uses the following string as the first command in the
% source file:
% \begin{verbatim}
% \documentclass[sigconf,authordraft]{acmart}
% \end{verbatim}

% \section{Modifications}

% Modifying the template --- including but not limited to: adjusting
% margins, typeface sizes, line spacing, paragraph and list definitions,
% and the use of the \verb|\vspace| command to manually adjust the
% vertical spacing between elements of your work --- is not allowed.

% {\bfseries Your document will be returned to you for revision if
%   modifications are discovered.}

% \section{Typefaces}

% The ``\verb|acmart|'' document class requires the use of the
% ``Libertine'' typeface family. Your \TeX\ installation should include
% this set of packages. Please do not substitute other typefaces. The
% ``\verb|lmodern|'' and ``\verb|ltimes|'' packages should not be used,
% as they will override the built-in typeface families.

% \section{Title Information}

% The title of your work should use capital letters appropriately -
% \url{https://capitalizemytitle.com/} has useful rules for
% capitalization. Use the {\verb|title|} command to define the title of
% your work. If your work has a subtitle, define it with the
% {\verb|subtitle|} command.  Do not insert line breaks in your title.

% If your title is lengthy, you must define a short version to be used
% in the page headers, to prevent overlapping text. The \verb|title|
% command has a ``short title'' parameter:
% \begin{verbatim}
%   \title[short title]{full title}
% \end{verbatim}

% \section{Authors and Affiliations}

% Each author must be defined separately for accurate metadata
% identification. Multiple authors may share one affiliation. Authors'
% names should not be abbreviated; use full first names wherever
% possible. Include authors' e-mail addresses whenever possible.

% Grouping authors' names or e-mail addresses, or providing an ``e-mail
% alias,'' as shown below, is not acceptable:
% \begin{verbatim}
%   \author{Brooke Aster, David Mehldau}
%   \email{dave,judy,steve@university.edu}
%   \email{firstname.lastname@phillips.org}
% \end{verbatim}

% The \verb|authornote| and \verb|authornotemark| commands allow a note
% to apply to multiple authors --- for example, if the first two authors
% of an article contributed equally to the work.

% If your author list is lengthy, you must define a shortened version of
% the list of authors to be used in the page headers, to prevent
% overlapping text. The following command should be placed just after
% the last \verb|\author{}| definition:
% \begin{verbatim}
%   \renewcommand{\shortauthors}{McCartney, et al.}
% \end{verbatim}
% Omitting this command will force the use of a concatenated list of all
% of the authors' names, which may result in overlapping text in the
% page headers.

% The article template's documentation, available at
% \url{https://www.acm.org/publications/proceedings-template}, has a
% complete explanation of these commands and tips for their effective
% use.

% Note that authors' addresses are mandatory for journal articles.

% \section{Rights Information}

% Authors of any work published by ACM will need to complete a rights
% form. Depending on the kind of work, and the rights management choice
% made by the author, this may be copyright transfer, permission,
% license, or an OA (open access) agreement.

% Regardless of the rights management choice, the author will receive a
% copy of the completed rights form once it has been submitted. This
% form contains \LaTeX\ commands that must be copied into the source
% document. When the document source is compiled, these commands and
% their parameters add formatted text to several areas of the final
% document:
% \begin{itemize}
% \item the ``ACM Reference Format'' text on the first page.
% \item the ``rights management'' text on the first page.
% \item the conference information in the page header(s).
% \end{itemize}

% Rights information is unique to the work; if you are preparing several
% works for an event, make sure to use the correct set of commands with
% each of the works.

% The ACM Reference Format text is required for all articles over one
% page in length, and is optional for one-page articles (abstracts).

% \section{CCS Concepts and User-Defined Keywords}

% Two elements of the ``acmart'' document class provide powerful
% taxonomic tools for you to help readers find your work in an online
% search.

% The ACM Computing Classification System ---
% \url{https://www.acm.org/publications/class-2012} --- is a set of
% classifiers and concepts that describe the computing
% discipline. Authors can select entries from this classification
% system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
% commands to be included in the \LaTeX\ source.

% User-defined keywords are a comma-separated list of words and phrases
% of the authors' choosing, providing a more flexible way of describing
% the research being presented.

% CCS concepts and user-defined keywords are required for for all
% articles over two pages in length, and are optional for one- and
% two-page articles (or abstracts).

% \section{Sectioning Commands}

% Your work should use standard \LaTeX\ sectioning commands:
% \verb|section|, \verb|subsection|, \verb|subsubsection|, and
% \verb|paragraph|. They should be numbered; do not remove the numbering
% from the commands.

% Simulating a sectioning command by setting the first word or words of
% a paragraph in boldface or italicized text is {\bfseries not allowed.}

% \section{Tables}

% The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
% package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
% high-quality tables.

% Table captions are placed {\itshape above} the table.

% Because tables cannot be split across pages, the best placement for
% them is typically the top of the page nearest their initial cite.  To
% ensure this proper ``floating'' placement of tables, use the
% environment \textbf{table} to enclose the table's contents and the
% table caption.  The contents of the table itself must go in the
% \textbf{tabular} environment, to be aligned properly in rows and
% columns, with the desired horizontal and vertical rules.  Again,
% detailed instructions on \textbf{tabular} material are found in the
% \textit{\LaTeX\ User's Guide}.

% Immediately following this sentence is the point at which
% Table~\ref{tab:freq} is included in the input file; compare the
% placement of the table here with the table in the printed output of
% this document.

% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}

% To set a wider table, which takes up the whole width of the page's
% live area, use the environment \textbf{table*} to enclose the table's
% contents and the table caption.  As with a single-column table, this
% wide table will ``float'' to a location deemed more
% desirable. Immediately following this sentence is the point at which
% Table~\ref{tab:commands} is included in the input file; again, it is
% instructive to compare the placement of the table here with the table
% in the printed output of this document.

% \begin{table*}
%   \caption{Some Typical Commands}
%   \label{tab:commands}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

% Always use midrule to separate table header rows from data rows, and
% use it only for this purpose. This enables assistive technologies to
% recognise table headers and support their users in navigating tables
% more easily.

% \section{Math Equations}
% You may want to display math equations in three distinct styles:
% inline, numbered or non-numbered display.  Each of the three are
% discussed in the next sections.

% \subsection{Inline (In-text) Equations}
% A formula that appears in the running text is called an inline or
% in-text formula.  It is produced by the \textbf{math} environment,
% which can be invoked with the usual
% \texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
% the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
% and structures, from $\alpha$ to $\omega$, available in
% \LaTeX~ \cite{Lamport:LaTeX}; this section will simply show a few
% examples of in-text equations in context. Notice how this equation:
% \begin{math}
%   \lim_{n\rightarrow \infty}x=0
% \end{math},
% set here in in-line math style, looks slightly different when
% set in display style.  (See next section).

% \subsection{Display Equations}
% A numbered display equation---one set off by vertical space from the
% text and centered horizontally---is produced by the \textbf{equation}
% environment. An unnumbered display equation is produced by the
% \textbf{displaymath} environment.

% Again, in either environment, you can use any of the symbols and
% structures available in \LaTeX\@; this section will just give a couple
% of examples of display equations in context.  First, consider the
% equation, shown as an inline equation above:
% \begin{equation}
%   \lim_{n\rightarrow \infty}x=0
% \end{equation}
% Notice how it is formatted somewhat differently in
% the \textbf{displaymath}
% environment.  Now, we'll enter an unnumbered equation:
% \begin{displaymath}
%   \sum_{i=0}^{\infty} x + 1
% \end{displaymath}
% and follow it with another numbered equation:
% \begin{equation}
%   \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
% \end{equation}
% just to demonstrate \LaTeX's able handling of numbering.

% \section{Figures}

% The ``\verb|figure|'' environment should be used for figures. One or
% more images can be placed within a figure. If your figure contains
% third-party material, you must clearly identify it as such, as shown
% in the example below.
% % Figure environment removed

% Your figures should contain a caption which describes the figure to
% the reader.

% Figure captions are placed {\itshape below} the figure.

% Every figure should also have a figure description unless it is purely
% decorative. These descriptions convey whats in the image to someone
% who cannot see it. They are also used by search engine crawlers for
% indexing images, and when images cannot be loaded.

% A figure description must be unformatted plain text less than 2000
% characters long (including spaces).  {\bfseries Figure descriptions
%   should not repeat the figure caption  their purpose is to capture
%   important information that is not already provided in the caption or
%   the main text of the paper.} For figures that convey important and
% complex new information, a short text description may not be
% adequate. More complex alternative descriptions can be placed in an
% appendix and referenced in a short figure description. For example,
% provide a data table capturing the information in a bar chart, or a
% structured list representing a graph.  For additional information
% regarding how best to write figure descriptions and why doing this is
% so important, please see
% \url{https://www.acm.org/publications/taps/describing-figures/}.

% \subsection{The ``Teaser Figure''}

% A ``teaser figure'' is an image, or set of images in one figure, that
% are placed after all author and affiliation information, and before
% the body of the article, spanning the page. If you wish to have such a
% figure in your article, place the command immediately before the
% \verb|\maketitle| command:
% \begin{verbatim}
%   \begin{teaserfigure}
%     % Figure removed
%     \caption{figure caption}
%     \Description{figure description}
%   \end{teaserfigure}
% \end{verbatim}

% \section{Citations and Bibliographies}

% The use of \BibTeX\ for the preparation and formatting of one's
% references is strongly recommended. Authors' names should be complete
% --- use full first names (``Donald E. Knuth'') not initials
% (``D. E. Knuth'') --- and the salient identifying features of a
% reference should be included: title, year, volume, number, pages,
% article DOI, etc.

% The bibliography is included in your source document with these two
% commands, placed just before the \verb|\end{document}| command:
% \begin{verbatim}
%   \bibliographystyle{ACM-Reference-Format}
%   \bibliography{bibfile}
% \end{verbatim}
% where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
% suffix, of the \BibTeX\ file.

% Citations and references are numbered by default. A small number of
% ACM publications have citations and references formatted in the
% ``author year'' style; for these exceptions, please include this
% command in the {\bfseries preamble} (before the command
% ``\verb|\begin{document}|'') of your \LaTeX\ source:
% \begin{verbatim}
%   \citestyle{acmauthoryear}
% \end{verbatim}

%   Some examples.  A paginated journal article  \cite{Abril07}, an
%   enumerated journal article  \cite{Cohen07}, a reference to an entire
%   issue  \cite{JCohen96}, a monograph (whole book)  \cite{Kosiur01}, a
%   monograph/whole book in a series (see 2a in spec. document)
%    \cite{Harel79}, a divisible-book such as an anthology or compilation
%    \cite{Editor00} followed by the same example, however we only output
%   the series if the volume number is given  \cite{Editor00a} (so
%   Editor00a's series should NOT be present since it has no vol. no.),
%   a chapter in a divisible book  \cite{Spector90}, a chapter in a
%   divisible book in a series  \cite{Douglass98}, a multi-volume work as
%   book  \cite{Knuth97}, a couple of articles in a proceedings (of a
%   conference, symposium, workshop for example) (paginated proceedings
%   article)  \cite{Andler79, Hagerup1993}, a proceedings article with
%   all possible elements  \cite{Smith10}, an example of an enumerated
%   proceedings article  \cite{VanGundy07}, an informally published work
%    \cite{Harel78}, a couple of preprints  \cite{Bornmann2019,
%     AnzarootPBM14}, a doctoral dissertation  \cite{Clarkson85}, a
%   master's thesis:  \cite{anisi03}, an online document / world wide web
%   resource  \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
%   (Case 1)  \cite{Obama08} and (Case 2)  \cite{Novak03} and  \cite{Lee05}
%   and (Case 3) a patent  \cite{JoeScientist001}, work accepted for
%   publication  \cite{rous08}, 'YYYYb'-test for prolific author
%    \cite{SaeediMEJ10} and  \cite{SaeediJETC10}. Other cites might
%   contain 'duplicate' DOI and URLs (some SIAM articles)
%    \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
%   multi-volume works as books  \cite{MR781536} and  \cite{MR781537}. A
%   couple of citations with DOIs:
%    \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
%   citations:  \cite{TUGInstmem, Thornburg01, CTANacmart}. Artifacts:
%    \cite{R} and  \cite{UMassCitations}.

% \section{Acknowledgments}

% Identification of funding sources and other support, and thanks to
% individuals and groups that assisted in the research and the
% preparation of the work should be included in an acknowledgment
% section, which is placed just before the reference section in your
% document.

% This section has a special environment:
% \begin{verbatim}
%   \begin{acks}
%   ...
%   \end{acks}
% \end{verbatim}
% so that the information contained therein can be more easily collected
% during the article metadata extraction phase, and to ensure
% consistency in the spelling of the section heading.

% Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

% \section{Appendices}

% If your work needs an appendix, add it before the
% ``\verb|\end{document}|'' command at the conclusion of your source
% document.

% Start the appendix with the ``\verb|appendix|'' command:
% \begin{verbatim}
%   \appendix
% \end{verbatim}
% and note that in the appendix, sections are lettered, not
% numbered. This document has two appendices, demonstrating the section
% and subsection identification method.

% \section{Multi-language papers}

% Papers may be written in languages other than English or include
% titles, subtitles, keywords and abstracts in different languages (as a
% rule, a paper in a language other than English should include an
% English title and an English abstract).  Use \verb|language=...| for
% every language used in the paper.  The last language indicated is the
% main language of the paper.  For example, a French paper with
% additional titles and abstracts in English and German may start with
% the following command
% \begin{verbatim}
% \documentclass[sigconf, language=english, language=german,
%               language=french]{acmart}
% \end{verbatim}

% The title, subtitle, keywords and abstract will be typeset in the main
% language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
% begin title, subtitle and keywords, can be used to set these elements
% in the other languages.  The environment \verb|translatedabstract| is
% used to set the translation of the abstract.  These commands and
% environment have a mandatory first argument: the language of the
% second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
% of their usage.

% \section{SIGCHI Extended Abstracts}

% The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
% not in Word) produces a landscape-orientation formatted article, with
% a wide left margin. Three environments are available for use with the
% ``\verb|sigchi-a|'' template style, and produce formatted output in
% the margin:
% \begin{itemize}
% \item {\verb|sidebar|}:  Place formatted text in the margin.
% \item {\verb|marginfigure|}: Place a figure in the margin.
% \item {\verb|margintable|}: Place a table in the margin.
% \end{itemize}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
