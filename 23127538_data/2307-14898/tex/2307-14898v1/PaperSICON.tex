\documentclass[hidelinks,onefignum,onetabnum]{siamart220329}



% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.
\usepackage{amsfonts,amsmath}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{subfig,multicol,dashrule}
%\ifpdf
%\DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
%\else
%\DeclareGraphicsExtensions{.eps}
%\fi

% Add a serial/Oxford comma by default.
%\newcommand{\creflastconjunction}{, and~}

\definecolor{Green}{RGB}{53, 181, 53}
% \newcommand*{\an}{\textcolor{Green}}
% \newcommand*{\tm}{\textcolor{blue}}
% \newcommand{\ms}{\textcolor{red}}
% \newcommand*{\bn}{\textcolor{purple}}

% Used for creating new theorem and remark environments
\newsiamremark{remark}{Remark}
\newsiamremark{assumption}{Assumption}
%\newsiamremark{definition}{Definition}
%\newsiamremark{hypothesis}{Hypothesis}
%\crefname{hypothesis}{Hypothesis}{Hypotheses}
%\newsiamthm{claim}{Claim}

%\declaretheorem[style=plain, title=Theorem, refname={theorem}]{theorem}
%\declaretheorem[style=plain, title=Corollary, refname={corollary}]{corollary}
%\declaretheorem[style=plain, title=Assumption, refname={assumption}]{assumption}
%\declaretheorem[style=definition, title=Definition, refname={definition}, qed=$\circ$]{definition}
%\declaretheorem[style=plain, title=Proposition, refname={proposition}]{proposition}
%\declaretheorem[style=plain, title=Remark, 
%refname={remark}]{remark}
%\declaretheorem[style=plain, title=Lemma, refname={lemma}]{lemma}
%\declaretheorem[style=plain, title=Problem, refname={problem}]{problem}

%\DeclareMathOperator*{\argmin}{arg\,min}

%\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
%\makeatletter
%\let\oldnorm\norm
%\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
%\makeatother

% Sets running headers as well as PDF title and authors
\headers{Infinite-Horizon Discrete-Time Dynamic Games}{A. Monti, B. Nortmann, T. Mylvaganam, M. Sassano}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{Feedback and Open-Loop Nash Equilibria for LQ Infinite-Horizon Discrete-Time Dynamic Games
%\thanks{Submitted to the Editors on 16/06/2023.
\thanks{\funding{The work of B. Nortmann is supported by UKRI DTP 2019 grant number EP/R513052/1.
		The work of M. Sassano is partially supported by the Italian Ministry for Research in the framework of the 2020 Program for Research Projects of National Interest (PRIN). Grant No. 2020RTWES4.}}}

% Authors: full names plus addresses.
\author{Andrea Monti\thanks{Dipartimento di Ingengeria Civile e Ingegneria Informatica, Universit√† di Roma, ``Tor Vergata'' 
		(\email{andrea.monti@uniroma2.it, mario.sassano@uniroma2.it}).}
	\and Benita Nortmann\thanks{Department of Aeronautics, Imperial College London, London SW7 2AZ, UK 
		(\email{benita.nortmann15@imperial.ac.uk,t.mylvaganam@imperial.ac.uk}).}
	\and Thulasi Mylvaganam\footnotemark[3]
	\and Mario Sassano\footnotemark[2]
}

%\usepackage{amsopn}
%\DeclareMathOperator{\diag}{diag}

\begin{document}

\maketitle


\begin{abstract}
We consider dynamic games defined over an \emph{infinite horizon}, characterized by linear, discrete-time dynamics and quadratic cost functionals. Considering such linear-quadratic (LQ) dynamic games, we %provide an analysis of 
focus on their solutions in terms Nash equilibrium strategies. Both Feedback (F-NE) and Open-Loop (OL-NE) Nash equilibrium solutions are considered. The contributions of the paper are threefold. First, our detailed study reveals some interesting structural insights in relation to F-NE solutions. Second, as a stepping stone towards our consideration of OL-NE strategies, we consider a specific infinite-horizon discrete-time (single-player) optimal control problem, wherein the dynamics are influenced by a known exogenous input and draw connections between its solution obtained via Dynamic Programming and Pontryagin's Minimum Principle. Finally, we exploit the latter result to provide a characterization of OL-NE strategies of the class of infinite-horizon dynamic games. The  results and key observations made throughout the paper are illustrated via a numerical example.
\end{abstract}

% REQUIRED
\begin{keywords}
Infinite-horizon Dynamic Games; Dynamic Programming; Pontryagin's Minimum Principle.
\end{keywords}

% REQUIRED
% \begin{MSCcodes}
% 91A50, 90C39
% \end{MSCcodes}

\section{Introduction} \label{sec:intro}
Game theory provides a framework to study strategic interactions between multiple \emph{decision makers}, referred to as \emph{players}, which may (or may not) be in competition with one another.
\emph{Dynamic} games, in particular, concern such \emph{multi-player decisions} that evolve over time via a dynamical system whose evolution is \emph{steered} by inputs of a certain number of \emph{players}. %Such scenarios have gained interest in the last decades. %This is in part due to % 
Such problems have gained interest across the control engineering field in the last decades and find a wide range of applications, for instance, in the context of cyber-security (\cite{huang2020,basar2019}), economics (\cite{jorgensen2010,long2011,jaskiewicz2022}), epidemiology (\cite{kordonis2022,nadini2020}), robotics (\cite{Myl2017,Cappello2021, Li-Bur:19, Nor-Myl:ECC22}) and power systems
	(\cite{nagata2002,pratap2017}).  Notably, dynamic games have gained increasing interest in recent years, in part due to the widespread presence of \emph{multi-agent systems} in modern applications, namely systems consisting of individual decision makers (or players) that interact, potentially in a competitive way, with one another (see \emph{e.g} \cite{chen2019control} and references therein). 
The framework provided by dynamic games has also proved useful for a wider range of problems involving dynamical systems with multiple inputs and/or multiple (potentially conflicting) objectives, such as robust control, mixed $H_2/H_\infty$ control and distributed control  (\cite{sweriduk1994differential,limebeer1992mixed,basar2008h,jaskiewicz2011stochastic,Cappello2022, Wrz20}).
%{\color{red} add refs}. 

In this paper we study \emph{infinite-horizon, dynamic} games.
More precisely, we consider a discrete-time linear system, the evolution of which is influenced by the actions (\emph{i.e.} control inputs) of a set of players. Each player aims at minimizing, via an individually manipulated input, its own quadratic cost functional over an infinite-horizon. 
While dynamic games can be considered a generalization of optimal control problems (which can be interpreted as a single-player game),
in the case of games the outcome for one player is dependent on the strategies of all other players. Hence, several alternative solution concepts exist for dynamic games. In what follows we focus on one of the most common notions of solution, namely \emph{Nash Equilibrium} (NE) strategies. These can be further categorized according to the information pattern available to each player while implementing its own strategy: \emph{Feedback Nash Equilibrium} (F-NE) 
and \emph{Open-Loop Nash Equilibrium} (OL-NE). 
See, \emph{e.g.} \cite{basar1998, starr1969further, Starr:69}, 
for further details on dynamic games and their various solution concepts.
Even in the case of linear-quadratic (LQ) games considered herein, NE solutions may not be unique, and the equations characterizing solutions are generally challenging to solve.
Somewhat surprisingly, the class of \emph{infinite}-horizon LQ \emph{discrete-time} dynamic games considered herein has received relatively limited attention in the literature hitherto (see, \emph{e.g.} \cite{basar1998} and \cite{freiling1999discrete}) compared to their continuous-time counterpart, namely \emph{differential} games defined over an infinite horizon. In the continuous-time setting infinite-horizon \emph{differential} games have been studied extensively (see, \emph{e.g.} \cite{Engwerda2005} and references therein). In this setting it is known that the application of Dynamic Programming (DP) arguments leads to a set of coupled Algebraic Riccati equations (AREs) characterizing F-NEs, whereas
arguments borrowed from Pontryagin's Minimum Principle (PMP) lead to (deceptively similar) \emph{asymmetric} AREs characterizing OL-NEs. 

The main contribution of this work is a complete characterization of all OL-NE solutions of \emph{infinite-horizon}, \emph{discrete-time} LQ dynamic games by relying on insights gained from PMP. Moreover, we provide conditions in the form of asymmetric coupled matrix equations (via invariance arguments) under which OL-NE strategies admit a \emph{feedback synthesis}.
As such, one of the main contributions of this paper can be seen as the discrete-time counterpart of the results presented in Chapters 7 and 8 of\footnote{Note that the terminology used throughout this paper is deliberately consistent with that of \cite{Engwerda2005} and the interested reader is referred to \cite{Engwerda2005} for further insights on LQ \emph{differential} games.} \cite{Engwerda2005}. 
Obtaining these results requires in turn 
the study of a special class of discrete-time optimal control problems, over an infinite horizon, wherein the system dynamics are influenced by a \emph{known} exogenous input. While this problem has been addressed in \cite{engwerda_infinityHor} via DP arguments, we draw novel connections with PMP by relating the solution to a certain trajectory of the associated Hamiltonian dynamical system. These novel insights are instrumental towards obtaining the aforementioned results related to OL-NE solutions. %\hl{, which has not been considered previously in the literature. We should amend this!}
In addition, we provide a comprehensive characterization of F-NE in LQ dynamic games via DP arguments. Although similar conditions have appeared in the literature (see, \emph{e.g.} \cite[Sec. 6.2.3]{basar1998}), the specific matrix equations derived herein reveal an interesting structure, which allows comparison to both the equations characterizing OL-NE strategies and the continuous-time equivalent. Furthermore, we provide sufficient structural conditions (easily verifiable based solely on the problem data and more general than those in \cite[Sec. 6.2.3]{basar1998}) implying the asymptotic stability of the origin for the plant in closed loop with the F-NE strategies, provided they exist.
For ease of exposition, we present the analysis for two-player dynamic games.
However, the results are readily generalized to the $N$-player setting (at the cost of more cumbersome notation).

The remainder of the manuscript is organized as follows. In Section \ref{sec:problem} we recall some preliminaries on LQ dynamic games and their solutions in terms of NE strategies. F-NE strategies for infinite-horizon dynamic games are discussed in Section \ref{sec:F_NE}, while OL-NE strategies are derived %considered 
in Section \ref{sec:OL_NE}. Finally, an illustrative numerical example is presented in Section~\ref{sec:sim}, before some concluding remarks are provided in Section~\ref{sec:conc}. %\\[.2em]

\textbf{Notation} The set of real numbers is denoted by $\mathbb{R}$. 
Given a square matrix $A$, $\sigma(A)$ indicates the set of the eigenvalues of $A$, namely the \emph{spectrum} of $A$, while $A^{-\top}$ represents the transpose of the inverse of $A$, namely $A^\top A^{-\top}~=~I$, and $A^\mathsf{H}$ denotes the conjugate transpose of $A$.  Given a symmetric matrix $A$, $A\succ 0$ ($A \succeq$ 0) denotes that the matrix is positive definite (positive semi-definite). Given a non-square matrix $B$ of full column rank, $B^\dagger$ denotes its %the 
(left) pseudo-inverse.  The observability matrix of the pair of matrices $(A,C)$ is denoted by $\mathcal{O}(A,C)$. $\mathbb{C}_{d}$ denotes the set $\{ \lambda \in \mathbb{C} : |\lambda| < 1 \}$.
The space of square summable infinite sequences $w = (w_0, w_1, w_2, ...)$, \emph{i.e.} such that $\sum_{k = 0}^{\infty} |w_k|^2 < \infty$, is denoted by $\ell_2$.

\section{Problem statement and preliminaries}
\label{sec:problem}
The main objective of this section consists in briefly recalling a few definitions and results concerning the computation of NE strategies in the setting of infinite-horizon LQ dynamics games. 
Thus, consider a linear system described by the difference equation
\begin{equation}\label{eq:lti}
	x(k+1) = Ax(k) + B_1u_1(k) + B_2u_2(k),
\end{equation}
where $x : \mathbb{N} \rightarrow \mathbb{R}^{n}$ denotes the state of the system and $u_i : \mathbb{N} \rightarrow \mathbb{R}^{m_i}$, for $i = 1, 2$, denote the control actions of players 1 and 2, respectively, subject to the initial condition $x(0) = x_0 \in \mathbb{R}^{n}$. The aim of each player is to minimize an individual cost functional defined over an infinite horizon, described by
\begin{equation}
	\label{eq:cost_J_i}
	J_i(x_0, u_1(\cdot), u_2(\cdot)) = \frac{1}{2}\sum\limits_{k=0}^{\infty} x(k)^\top Q_ix(k) + u_i(k)^\top R_iu_i(k),
\end{equation}
with $Q_i = Q_i^{\top} \succeq 0$ and $R_i = R_i^{\top} \succ 0$ parameterized with respect to the initial condition $x_0$. Since the value of the cost functional of each player depends
on the action selected by its opponent, indirectly via the evolution of the state,  
the cost functionals
can seldom be  minimized simultaneously. Thus the players need to settle for \emph{equilibrium}, rather than \emph{optimal}, solutions. 
Two common solution concepts that reflect different underlying information structures, namely feedback Nash equilibria (F-NE) and open-loop Nash equilibria (OL-NE),
	are recalled below (see, \emph{e.g.} \cite{basar1998}
	for further details).

\smallskip

\begin{definition}
	\label{def:FNE}
	(F-NE) A pair of control laws $(u_1^{\star}, u_2^{\star}) = (K_1^{\star}x, K_2^{\star}x)$ is a \emph{Feedback Nash Equilibrium} (F-NE) strategy if the inequalities
	\begin{subequations}\label{eq:def_FNE}
		\begin{align}
			J_1(x_0, K_1^{\star}x, K_2^{\star}x) & \leq J_1(x_0, K_1 x, K_2^{\star}x) \label{eq:def_FNE_ply_1}\,, \\[3pt]
			J_2(x_0, K_1^{\star}x, K_2^{\star}x) & \leq J_2(x_0, K_1^{\star} x, K_2x) \label{eq:def_FNE_ply_2}\,,
		\end{align}
	\end{subequations}
	hold for all $x_0 \in \mathbb{R}^{n}$ and for any pair $(K_1, K_2^{\star})$ such that $\sigma(A+B_1 K_1 + B_2 K_2^{\star}) \subset \mathbb{C}_d$ in the case of \eqref{eq:def_FNE_ply_1}, and for any pair $(K_1^{\star}, K_2)$ such that $\sigma(A+B_1 K_1^{\star} + B_2 K_2) \subset \mathbb{C}_d$ in the case of \eqref{eq:def_FNE_ply_2}.
\end{definition}

An appealing property of the solution concept detailed in Definition~\ref{def:FNE} is that the pair of strategies\footnote{As is common in the context of LQ dynamic games (see \emph{e.g.} \cite{Engwerda2007}), we limit our attention to linear feedback strategies in the context of F-NE solutions.} $(K_1^\star x, K_2^\star x)$ constitutes a F-NE solution of the game defined by the system~\eqref{eq:lti} and the cost functionals \eqref{eq:cost_J_i}, for $i = 1,2$, for \emph{all} initial conditions $x_0 \in \mathbb{R}^n$. Similarly, if at any point in time $\bar k > 0$ the actual state $x(\bar k)$ of the system~\eqref{eq:lti} deviates for any reason from the state $x^\star(\bar k)$, induced by the initial condition and the equilibrium strategies $(u_1^\star, u_2^\star)$ played for $k = 0, \ldots, \bar k - 1$, then the pair of strategies $(K_1^\star x, K_2^\star x)$ still constitutes a F-NE solution of the (restricted) game on $k \geq \bar k$. However, the feedback strategies entail 
that the players must measure the entire state of the underlying plant at all times in order to implement a F-NE strategy.
In practice, it may be interesting to study also the setting in which each player has knowledge of the initial condition $x_0$ only, together with the elapsed time, as discussed in the following definition. To streamline the presentation, let $\mathcal{U}_1(x_0) := \{ (\phi_1, \phi_2^{\star}) \in \ell_2 \times \ell_2 : J_1(x_0, \phi_1, \phi_2^{\star}) < \infty, \lim_{k \rightarrow \infty} x(k)=0 \}$ and $\mathcal{U}_2(x_0) := \{ (\phi_1^{\star}, \phi_2) \in \ell_2 \times \ell_2 : J_2(x_0, \phi_1^{\star}, \phi_2) < \infty, \lim_{k \rightarrow \infty} x(k) = 0 \}$.

\medskip 

\begin{definition}
	\label{def:OLNE}
	(OL-NE) Let $x_0 \in \mathbb{R}^{n}$ be given.
	A pair of control laws $(u_1^{\star}, u_2^{\star}) = (\phi_1^{\star}(k,x_0), \phi_2^{\star}(k,x_0))$ is an \emph{Open-Loop Nash Equilibrium} (OL-NE) strategy if the inequalities
	\begin{subequations}
		\label{eq:def_OLNE}
		\begin{align}
		  %J_1(x_0, \phi_1^{\star}(\cdot), \phi_2^{\star}(\cdot))  & \leq J_1(x_0, \phi_1(\cdot), \phi_2^{\star}(\cdot))\,,
            J_1(x_0, \phi_1^{\star}, \phi_2^{\star})
            & \leq J_1(x_0, \phi_1, \phi_2^{\star})\,, \label{eq:def_OLNE_ply_1} \\[3pt]
			%J_2(x_0, \phi_1^{\star}(\cdot), \phi_2^{\star}(\cdot)) 
            J_2(x_0, \phi_1^{\star}, \phi_2^{\star})
            & \leq J_2(x_0, \phi_1^{\star}, \phi_2)\,, 
            %& \leq J_2(x_0, \phi_1^{\star}(\cdot), \phi_2(\cdot))\,, 
            \label{eq:def_OLNE_ply_2}
		\end{align}
	\end{subequations}
	hold for any $(\phi_1, \phi_2^{\star}) \in \mathcal{U}_1(x_0)$, % := \{ (\phi_1, \phi_2^{\star}) \in \ell_2 \times \ell_2 : J_1(x_0, \phi_1, \phi_2^{\star}) < \infty, \lim_{k \rightarrow \infty} x(k)=0 \}$ 
	in \eqref{eq:def_OLNE_ply_1} 
	and for any $(\phi_1^{\star}, \phi_2) \in \mathcal{U}_2(x_0)$ % := \{ (\phi_1^{\star}, \phi_2) \in \ell_2 \times \ell_2 : J_2(x_0, \phi_1^{\star}, \phi_2) < \infty, \lim_{k \rightarrow \infty} x(k) = 0 \}$ 
	in \eqref{eq:def_OLNE_ply_2}\,.
\end{definition}

\smallskip

\begin{remark}
	\label{rem:OL_vs_F}
	{\rm Apart from the structure of the underlying control law, it is worth observing that the distinction between F-NE and OL-NE consists also in the characterization of the subset of alternative control laws with which a \emph{candidate} NE should be compared (see the definitions of the feasible sets in Definitions \ref{def:FNE} and \ref{def:OLNE}, respectively). Furthermore, it is worth observing that such admissibility conditions in Definitions \ref{def:FNE} and \ref{def:OLNE}, namely the requirements following the inequalities \eqref{eq:def_FNE} and \eqref{eq:def_OLNE}, respectively, account for the unbounded horizon in the cost functional \eqref{eq:cost_J_i}. As such, they are specific to %peculiar of 
			the infinite-horizon setting and do not possess any counterpart in the finite-horizon formulation. Similar conditions appear in \cite[Sec. 7.4]{Engwerda2005} although in the case of continuous-time LQ games.}
	\hfill $\triangle$
\end{remark}

The objective of this manuscript is to provide a comprehensive analysis of the conditions characterizing both F-NE and OL-NE of the class of dynamic games described by \eqref{eq:lti} and \eqref{eq:cost_J_i}. %, hence both F-NE and OL-NE are discussed. 
While the study of the two concepts of solution may be carried out in a somewhat parallel way, throughout the paper several connections are established and explicitly discussed (see, \emph{e.g.} Remarks \ref{rem:notARE} and \ref{rem:comaprison} and Section \ref{sec:sim} below). 

\section{F-NE for infinite-horizon dynamic games}
\label{sec:F_NE}
Focusing first on Definition \ref{def:FNE}, in this section we discuss the characterization of F-NE solutions. 
Consider the following condition, necessary for the existence of a F-NE, that is assumed to hold throughout this section.

\vspace{.7em}

\begin{assumption}
	The pair $\left(A, \begin{bmatrix} B_1 & B_2 \end{bmatrix} \right)$ is stabilizable.
	\label{as:stabilizability_F-NE}
\end{assumption}

Assumption \ref{as:stabilizability_F-NE} is a necessary requirement to ensure that the feasible set in Definition \ref{def:FNE} is not empty. The verification of such a condition may be performed via a standard rank condition on the reachability matrix for the combined effect of the control actions, namely by interpreting the columns of the matrices $B_1$ and $B_2$ as those of a common input matrix $B$. The following statement provides a characterization of F-NE strategies for the dynamic game described by the system \eqref{eq:lti} and the cost functionals \eqref{eq:cost_J_i}, $i=1,2$, in terms of certain coupled algebraic equations.

\smallskip

\begin{theorem} \label{thm:F_NE}
	Consider the system \eqref{eq:lti} and the cost functionals \eqref{eq:cost_J_i}, for $i = 1,2$. Then the pair of control laws
	\begin{equation}\label{eq:u_F_NE}
		u_i^{F}(x) = K_i x\,,
	\end{equation}
	\noindent for $i = 1,2$, constitutes a F-NE if and only if $K_i \in \mathbb{R}^{m_i \times n}$ are such that
	\begin{equation} \label{eq:stability} 
		\sigma(A+B_1 K_1 + B_2 K_2) \subset \mathbb{C}_{d}\,. 
	\end{equation}
	\noindent and solve, together with some $P_i \in \mathbb{R}^{n \times n}$, $P_i = P_i^{\top} \succeq 0$, $i = 1,2$, the matrix equations
	\begin{subequations}\label{eq:ACE}
		\begin{gather}\label{eq:ARE_1}
			\begin{split}
				P_1 =Q_1 + K_1^\top R_1 K_1   +  (A+B_1K_1 + B_2K_2)^\top P_1 (A+B_1K_1 + B_2K_2)  \,, 
			\end{split} 
		\end{gather} 
		\vspace{-20pt} 
		\begin{gather}\label{eq:ARE_2}
			\begin{split} 
				P_2 = Q_2 + K_2^\top R_2 K_{2}  + (A+B_1K_1 + B_2K_2)^\top P_2 (A+B_1K_1 + B_2K_2)\,,
			\end{split}
		\end{gather} 
	\end{subequations}
	\noindent and
	\begin{subequations}\label{eq:K_M}
		\begin{gather}\label{eq:Ki_s}
			M \left[\begin{array}{c} K_1 \\[2pt] K_2 \end{array}\right] = -\left[\begin{array}{c} B_1^{\top}P_1 \\[2pt] B_2^{\top}P_2 \end{array} \right]A\,, 
		\end{gather}
		\noindent with
		\begin{gather}
			M = \left[\begin{array}{cc} R_1 + B_1^{\top}P_1 B_1 & B_1^{\top}P_1 B_2 \\[2pt] B_2^{\top}P_2 B_1 & R_2 + B_2^{\top}P_2 B_2 \end{array} \right]\,.
		\end{gather}
	\end{subequations}
	Furthermore,  the F-NE is such that   $J_i(x_0, u_1^{F},u_2^{F}) = \frac{1}{2} x_0^\top P_i x_0$,
	for $i=1,2$. 
	\hfill $\circ$
\end{theorem}

\smallskip

\textbf{Proof:} 
Consider first the sufficient implication. Suppose that player 2 adheres to the strategy $u_2 = u^F_2(x)$ and let $A_{cl,2} = A + B_2K_2$ with $K_2$ defined in \eqref{eq:ACE}, \eqref{eq:K_M}. Then, with the action of player 2 fixed, the best response of player~1 constitutes a (single-player) optimal control problem  described by the dynamics $x(k+1) = (A+B_2 K_2)x(k) + B_1 u_1(k)$ and the cost functional \eqref{eq:cost_J_i} with $i = 1$. The solution of the optimal control problem is given by 
\begin{equation}\label{eq:u_oc}
	u_1^{oc}(x) = - (R_1 + B_1^\top P_1^{oc} B_1)^{-1}B_1^\top P_1^{oc} A_{cl,2}x = K_1^{oc} x,
\end{equation}
where $P_1^{oc}$ denotes a stabilizing solution of the 
corresponding discrete-time ARE 
\begin{equation}\label{eq:DP1} \begin{split}
		P_{1}^{oc} = & A_{cl,2}^\top P_{1}^{oc} A_{cl,2}  + Q_1 \\& - A_{cl,2}^\top P_{1}^{oc}B_1(R_1 + B_1^\top P_{1}^{oc} B_1)^{-1} B_1^\top P_{1}^{oc} A_{cl,2}.
	\end{split} 
\end{equation}
Substituting the first row of \eqref{eq:K_M}, \emph{i.e.} $$K_1 = -(R_1 + B_1^\top P_1 B_1 )^{-1} B_1^\top P_1 A_{cl,2}\,,$$ 
into \eqref{eq:ARE_1} reveals  that
$P_1^{oc} = P_1$ constitutes a stabilizing solution (by \eqref{eq:stability}) of the ARE \eqref{eq:DP1}, that $u_1^{oc}(x) = u_1^F(x)$ and \eqref{eq:def_FNE_ply_1} follows. Moreover, it follows from standard Dynamic Programming arguments that $V_1(x) =\frac{1}{2} x^\top P_1 x$ constitutes the value function corresponding to the optimal control problem obtained when the strategy of the second player is fixed at $u_2= u_2^F(x)$ and, thus, it follows (by definition of the value function) that $J_1(x_0, u_1^F, u_2^F) = \frac{1}{2}x_0^\top P_1 x_0$.
Consider now the converse statement. 
Assume that the pair of linear strategies $(K_1^\star x,K_2^\star x)$ is a F-NE of the game \eqref{eq:lti}, \eqref{eq:cost_J_i}, $i=1,2$. By Definition~\ref{def:FNE}, \eqref{eq:def_FNE_ply_1} holds for all $K_2$ such that $\sigma(A+B_1 K_1^\star + B_2 K_2) \subset \mathbb{C}_{d}$. Hence, for fixed $K_2$, the feedback strategy $K_1^\star x$ is the unique stabilizing optimal control strategy \eqref{eq:u_oc} with $P_1^{oc}$ satisfying \eqref{eq:DP1}, which implies that there exists $P_1 = P_1^{oc}$ such that \eqref{eq:ARE_1} holds with $K_1=K_1^\star.$
The proof is concluded via a symmetric reasoning for player 2.
\hfill $\square$

Theorem~\ref{thm:F_NE} characterizes F-NE strategies via the set of \emph{asymptotically stabilizing} solutions of the coupled algebraic equations \eqref{eq:ACE}, \eqref{eq:K_M}. In the following statement we provide sufficient conditions under which a solution to \eqref{eq:ACE}, \eqref{eq:K_M} is indeed asymptotically stabilizing. 

\smallskip

\begin{corollary}
	\label{co:stab_sol_ARE}
	Let $K_1$, $K_2$, $P_1 \succeq 0$, $P_2 \succeq 0$ be a solution to \eqref{eq:ACE}, \eqref{eq:K_M}. If the pair $\left( A, Q_1+Q_2 \right)$ is detectable, then $K_1$, $K_2$ are such that \eqref{eq:stability} holds, \emph{i.e.} the strategies $(K_1x, K_2x)$ constitute a F-NE of the game \eqref{eq:lti}, \eqref{eq:cost_J_i}, $i=1,2$.
	\hfill $\circ$
\end{corollary}

\textbf{Proof:} 
The claim is proved by contradiction. Adding \eqref{eq:ARE_1} and \eqref{eq:ARE_2} gives
\begin{multline}
	(A+B_1 K_1 + B_2 K_2)^\top \tilde P (A+B_1 K_1 + B_2 K_2) -  \tilde P + \tilde Q + \sum_{i=1}^2 K_i^\top R_i K_i = 0,
	\label{eq:summed_ARE}
\end{multline}
where $\tilde P = P_1 + P_2$ and $\tilde Q = Q_1 + Q_2$. Assume there exists a solution $K_1$, $K_2$, $P_2 \succeq 0$, $P_1 \succeq 0$ of \eqref{eq:ACE}, \eqref{eq:K_M}, which is such that $\sigma(A+B_1 K_1 + B_2 K_2)\not\subset \mathbb{C}_d$ and let $\lambda\notin \mathbb{C}_d$, with corresponding eigenvector $v\neq0$, be an eigenvalue of the closed-loop equilibrium system that is not located in the interior of the unit circle. Then, pre- and postmultiplying \eqref{eq:summed_ARE} by $v^\mathsf{H}$ and $v$, respectively, gives 
\begin{equation}
	\left( \lambda ^\mathsf{H} \lambda - 1 \right) v^\mathsf{H} \tilde P v + v^\mathsf{H} \tilde Q v + v^\mathsf{H} \left(\sum_{i=1}^2 K_i^\top R_i K_i \right) v = 0.
	\label{eq:ARE_stability}
\end{equation}
Since by assumption $\left( \lambda^\mathsf{H} \lambda - 1 \right) \geq 0$, $\tilde Q \succeq 0$, $\tilde P \succeq 0$ and $K_i^\top R_i K_i \succeq 0$, for $i = 1,2$, \eqref{eq:ARE_stability} implies $K_i v = 0$ and $\tilde Q v= 0$, which contradicts the hypothesis that the pair $\left( A, Q_1+Q_2 \right)$ is detectable.
\hfill $\square$

\smallskip

\begin{remark}\label{rem:notARE}
	{\rm The equations \eqref{eq:ACE}, \eqref{eq:K_M} appear reminiscent of the classic AREs arising in (single-player) optimal control problems in discrete-time, with additional terms to account for the presence of the opponent. Note, however, that  the matrix algebraic equations arising in Theorem~\ref{thm:F_NE} are \emph{not quadratic} in the unknown variables. This feature is appreciated in \eqref{eq:ACE}, \eqref{eq:K_M} with respect to the unknowns $K_i$ and $P_i$, $i = 1,2$, and it becomes even more evident whenever $K_i$, $i = 1,2$, can be obtained from \eqref{eq:K_M} and replaced in \eqref{eq:ACE}, \emph{i.e.} provided the matrix $M$ is invertible, thus yielding cubic terms in the variables $P_i$. As such, \eqref{eq:ACE}, \eqref{eq:K_M} \emph{are not Riccati equations}. This is in stark contrast not only  to discrete-time optimal control problems (for which Dynamic Programming leads to quadratic matrix equations), but also to (continuous-time) LQ \emph{differential} games (for which F-NE solutions are characterised by coupled quadratic matrix equations). Interestingly, as shown in Theorem \ref{thm:OL_feedback_synth} below, the computation of OL-NE for discrete-time infinite-horizon games revolves around the solution of certain (asymmetric) Riccati equations.}~\hfill $\triangle$
\end{remark}


\section{OL-NE for infinite-horizon dynamic games}
\label{sec:OL_NE}
We now turn our attention to Definition \ref{def:OLNE} and the class of OL-NE solutions for infinite-horizon LQ dynamic games. The analysis of such solutions %of such a notion of solution 
	is performed according to the following structure. First -- by recognizing that at the core of the computation of a Nash equilibrium one finds a standard (single-player) optimal control problem when the opponent has \emph{fixed} a certain (feed-forward) control strategy -- we develop an infinite-horizon extension of the PMP  for discrete-time linear systems, driven by a known exogenous input in Section~\ref{sec:OC_infinite_hor}. This objective is in turn obtained in two steps: first we recall the optimal feedback obtained via Dynamic Programming arguments (Lemma~\ref{lemma:OC_DP}); then we show that such an optimal control law can be obtained, for a given initial condition of the state, as a specific trajectory of the Hamiltonian dynamics arising in PMP (Lemma~\ref{lemma:OC_PMP_equiv_DP}), thus yielding \emph{sufficient conditions} for optimality on the latter as an open-loop strategy. Finally, in Section \ref{sec:pmp}, such boundary conditions that allow to \emph{identify} the optimal trajectory of the Hamiltonian dynamics are translated into the context of dynamic games (Theorem~\ref{thm:OL_NE}), thus transforming PMP into a set of sufficient conditions for OL-NE solutions. 

\subsection{Infinite-horizon affine-quadratic optimal control}
\label{sec:OC_infinite_hor}
Consider a (single-player) optimal control problem described by the dynamics
\begin{equation}
	\label{eq:OC_sys}
	x(k+1) = A x(k) + B u(k) + w(k)\,, \hspace{0.4cm} x(0) = x_0\,,
\end{equation}
subject to the initial condition $x_0 \in \mathbb{R}^{n}$, where $x : \mathbb{N} \rightarrow \mathbb{R}^n$ denotes the state of the system, $u : \mathbb{N} \rightarrow \mathbb{R}^m$ denotes the control input and $w(\cdot) \in \ell_2$ denotes a \emph{known} signal, and the cost functional 
\begin{equation}
	\label{eq:OC_cost}
	J^{oc}(x_0, u(\cdot)) = \frac{1}{2} \displaystyle \sum_{k = 0}^{\infty} x(k)^{\top}Q x(k) + u(k)^{\top} R u(k)\,,
\end{equation}
with $Q = Q^{\top} \succeq 0$ and $R = R^{\top} \succ 0$.
The characterization of the \emph{open-loop} solution of the problem described by \eqref{eq:OC_sys}, \eqref{eq:OC_cost} is obtained below in two steps: first the \emph{feedback, time-varying} optimal control law is constructed via Dynamic Programming arguments; then \emph{boundary conditions} are obtained under which the induced time history of the optimal control can be equivalently generated by a specific trajectory of the underlying Hamiltonian dynamics. While the problem hinted at in the first step has already been tackled in \cite{engwerda_infinityHor}, the novel constructions of the second step allow to obtain an open-loop implementation of the former result, by establishing %the open-loop of the former task, thus establishing 
	a connection with a specific trajectory of the corresponding Hamiltonian dynamics. The boundary conditions associated with this specific trajectory will be instrumental as we turn our attention back to the dynamic game setting and OL-NE solutions in Section \ref{sec:pmp}. Furthermore, since an alternative proof is provided for the first step described above, a formal statement (\emph{i.e.} Lemma \ref{lemma:OC_DP}) is dedicated also to such results, which are akin to those in \cite{engwerda_infinityHor}.

\smallskip

\begin{assumption}
	\label{assu:OC_structural}
	Consider \eqref{eq:OC_sys} and \eqref{eq:OC_cost}. Suppose that \emph{(i)}
	the pair $(A,B)$ is reachable; \emph{(ii)} the pair $(A,Q)$ is detectable; \emph{(iii)} the matrix $A$ is non-singular; \emph{(iv)} given $x_0$ and $w(\cdot)$, there exists at least one control law $\hat{u}(\cdot)$ such that $J^{oc}(x_0,\hat{u}(\cdot))$ is finite.
	\hfill $\circ$
\end{assumption}

\smallskip

\begin{lemma}
	\label{lemma:OC_DP}
	Consider the system \eqref{eq:OC_sys} and the cost functional \eqref{eq:OC_cost}.	Suppose that Assumption \ref{assu:OC_structural} holds.
	Let $P \in \mathbb{R}^{n \times n}$, $P = P^{\top} \succeq 0$ be the stabilizing solution to the \emph{discrete-time Riccati equation}
	\begin{equation}
		\label{eq:OC_Riccati}
		P = Q + A^{\top}P A - A^{\top}P B (R+B^{\top}PB)^{-1} B^{\top}P A\,, 
	\end{equation}
	and let $b(\cdot)$ be defined as
	\begin{equation}
		\label{eq:OC_b_def}
		b(k) = \displaystyle \sum_{h = k}^{\infty} (A_{\rm cl}^{\top})^{h-k+1} P w(h)\,,
	\end{equation}
	with $A_{\rm cl}^{\top} = A^{\top}(I-B(R+B^{\top}PB)^{-1}B^{\top}P)^{\top}$.
	Then the optimal control law is given by 
	\begin{equation}
		\label{eq:OC_u_star}
		\begin{split}
			u^{\star}(k) = -(R+B^{\top}PB)^{-1}B^{\top}P A x(k) 
			- (R+B^{\top}P B)^{-1} B^{\top}(P w(k) + b(k+1))\,.
		\end{split}
	\end{equation}
	Moreover the minimum cost is given by
	\begin{equation}
		\label{eq:OC_optimal_cost}
		J^{oc}(x_0, u^{\star}(\cdot)) = \frac{1}{2}x_0^{\top}P x_0 + x_0^{\top}b(0) + c(0)\,,
	\end{equation}
	with $c(0) = \frac{1}{2}\sum_{k=0}^{\infty} w(k)^{\top}P w(k) + 2 w(k)^{\top}b(k+1) - (B^{\top}Pw(k)+B^{\top}b(k+1))^{\top}(R+B^{\top}PB)^{-1}(B^{\top}Pw(k)+B^{\top}b(k+1))$.
	\hfill $\circ$
\end{lemma}

\smallskip

\textbf{Proof:}
The claim follows from Dynamic Programming arguments. Let 
\begin{equation}
	\label{eq:OC_candidate_value_fct}
	V(k,x(k)) = \frac{1}{2}x(k)^{\top} P x(k) + x(k)^{\top}b(k) + c(k)
\end{equation}
be a \emph{candidate value function} and consider the corresponding Bellman equation
\begin{equation}
	\label{eq:OC_bellman_eq}
	\begin{split}
		V(k,x(k)) = & \displaystyle \min_{u(k)}\Big\{  \frac{1}{2}x(k)^{\top}Q x(k) + \frac{1}{2}u(k)^{\top}R u(k) \\[3pt] & + V(k+1,A x(k) + B u(k) + w(k)) \Big\}\,,
	\end{split}
\end{equation}
which must be verified for all $(k,x(k)) \in \mathbb{N} \times \mathbb{R}^{n}$.
Straightforward computations show that $u^{\star}(k)$ defined in \eqref{eq:OC_u_star} is the argument that attains the minimum of the right-hand side of \eqref{eq:OC_bellman_eq}.
Replacing  $u^{\star}$ into the latter equation then leads (via straightforward although tedious derivations) to a non-homogeneous quadratic, time-varying function of $x(k)$.
While the (matrix) coefficient of the quadratic terms is zeroed provided the discrete-time Riccati \eqref{eq:OC_Riccati} holds, the coefficient of the linear terms is equal to zero provided $b(\cdot)$ satisfies the difference equation
\begin{equation}
	\label{eq:OC_b_k_eq}
	b(k) = A_{\rm cl}^{\top}b(k+1) + A_{\rm cl}^{\top}P w(k)\,.
\end{equation}

% % Figure environment removed

Item (iv) of Assumption~\ref{assu:OC_structural} implies that
$\lim_{k \rightarrow \infty}u^{\star}(k) = 0$, by positive definiteness of $R$, and, together with item (ii) of Assumption~\ref{assu:OC_structural}, that $\lim_{k \rightarrow \infty} x(k) = 0$.
As a consequence, by inspecting the structure of $u^{\star}(k)$ in \eqref{eq:OC_u_star}, one has that $\lim_{k \rightarrow \infty}B^{\top}b(k) = 0$, hence, since $w(k)$ tends to zero 
%again by Assumption~\ref{assu:OC_structural}, 
by definition,
there exists a function $\pi : \mathbb{N} \rightarrow \mathbb{R}^{m}$ with the properties that $B^{\top}b(k) = \pi(k)$ for all $k$ and $\lim_{k \rightarrow \infty} \pi(k) = 0$.
Therefore, by rewriting \eqref{eq:OC_b_k_eq} as $b(k+1) = A_{\rm cl}^{-\top}b(k) - Pw(k)$ and iterating on the reasoning above for $n$ steps, it follows that %\eqref{eq:OC_moving_window_bk} (overleaf) 
\begin{equation*}
		%\label{eq:OC_moving_window_bk}
		\begin{split}
			\left[\begin{array}{c} 0 \\ 0 \\ \vdots \\ 0 \end{array}\right] & =   \left[\begin{array}{cccc} 0 & 0 & \ldots & 0 \\-B^{\top}P & 0 & \ldots & 0 \\ \vdots & \ddots &  & \vdots \\-B^{\top}(A_{\rm cl}^{-\top})^{n-2}P & -B^{\top}(A_{\rm cl}^{-\top})^{n-3}P & \ldots & -B^{\top}P \end{array} \right]\left[\begin{array}{c} w(k) \\ w(k+1) \\ \vdots \\ w(k+n-2) \end{array} \right] \\& \qquad +\left[\begin{array}{c} B^{\top} \\ B^{\top}(A_{\rm cl}^{-\top}) \\ \vdots \\B^{\top}(A_{\rm cl}^{-\top})^{n-1} \end{array}\right] b(k) - \left[\begin{array}{c} \pi(k) \\ \pi(k+1) \\ \vdots \\ \pi(k+n-1) \end{array} \right]
			\\[5pt] & =: \mathcal{O}(A_{\rm cl}^{-\top},B^{\top})b(k) + \mathcal{M} W(k) - \Pi(k)\,,
		\end{split}
	\end{equation*}
holds for any $k$. %, where $\mathcal{O}(A_{\rm cl}^{-\top},B^{\top})$ denotes the observability matrix of the pair $(A_{\rm cl}^{-\top},B^{\top})$.
%Note that, 
By the Woodbury matrix identity\footnote{Note that since $R\succ 0$ and $P\succeq 0$, $(R+B^\top PB) \succ 0$. Thus, the inverse in \eqref{eq:OC_identity} exists, since the both the identity matrix and $(R+B^\top PB)$ are invertible (see e.g. \cite[Chapter 2]{golub}).} 
\begin{equation}
	\label{eq:OC_identity}
	(I+B R^{-1} B^{\top}P )^{-1} = I - B(R+B^{\top}P B)^{-1} B^{\top} P\,. 
\end{equation}
Thus, it follows from item (iii) of Assumption \ref{assu:OC_structural} and the structure of $A_{cl}$ that $A_{cl}^{-1}$ exists.   Moreover, note that item (i) of Assumption \ref{assu:OC_structural}  implies reachability of the closed-loop (feedback) system $(A_{\rm cl},B)$ and this, in turn, implies reachability of the pair $(A_{\rm cl}^{-1},B)$.
By duality, the latter property ensures observability of the pair $(A_{\rm cl}^{-\top},B^{\top})$, and hence the matrix $\mathcal{O}(A_{\rm cl}^{-\top},B^{\top})$ is full column rank.
Therefore, $b(k)$ can be uniquely expressed as $b(k) = \mathcal{O}(A_{\rm cl}^{-\top},B^{\top})^{\dagger}(\Pi(k)-\mathcal{M} W(k))$,
and hence $\lim_{k \rightarrow \infty}b(k) = 0$ follows from the asymptotic properties of $w$ and $\pi$.
Therefore, as a result of the latter boundary condition and by considering \eqref{eq:OC_b_k_eq}, the explicit solution is yielded by \eqref{eq:OC_b_def}, 
% which is well defined for any $k$, since $\sigma(A_{\rm cl}^{\top}) \subset \mathbb{C}_d$ - by the property of the stabilizing solution $P$ of \eqref{eq:OC_Riccati} -  and 
by the fact that $w \in \ell_2$.
% Let's keep this reasoning for Reviewers if asked: each term of the sum can be bounded by a term of the form $\lambda^h w(h)$, which can be bounded, by using Young's inequality, by the sum of square of the two terms, which both lead to finite summations.
Finally, the last claim follows by observing that the constant term of \eqref{eq:OC_bellman_eq}, with $u = u^{\star}$, with respect to $x(k)$ is zeroed for all $k$ by the selection of $c(\cdot)$ that satisfies the difference equation $c(k) = c(k+1) + \frac{1}{2} w(k)^{\top}P w(k) + %2 
w(k)^{\top}b(k+1) - \frac{1}{2}(B^{\top}Pw(k)+B^{\top}b(k+1))^{\top}(R+B^{\top}PB)^{-1}(B^{\top}Pw(k)+B^{\top}b(k+1))$, with the boundary condition $\lim_{k \rightarrow \infty} c(k) = 0$ by definition of value function.
\hfill $\square$

\smallskip

\begin{remark}
	\label{rem:OC_u_star_k}
	{\rm The optimal control law $u^{\star}(\cdot)$ in \eqref{eq:OC_u_star} can be equivalently written as a function at the current time $k$ alone by exploiting the difference equation \eqref{eq:OC_b_k_eq}, namely
		\begin{equation}
			\label{eq:OC_u_star_k_alone}
			\begin{split}
				u^{\star}(k) =  -(R+B^{\top}PB)^{-1}B^{\top}P A x(k) - (R+B^{\top}P B)^{-1} B^{\top}A_{\rm cl}^{-\top} b(k)\,.
			\end{split}
		\end{equation}
		It is worth observing that the \emph{anticipative} nature of the control law with respect to the exogenous signal $w$ is preserved since the computation of $b(k)$ via \eqref{eq:OC_b_def} %\eqref{eq:OC_b_k_eq} 
		requires in turn knowledge of $w(h)$ for all $h \geq k$.}
	\hfill $\triangle$
\end{remark}

The objective of the following result is to relate the time history induced by the optimal control law \eqref{eq:OC_u_star} (computed via Dynamic Programming arguments) with a specific trajectory of the underlying Hamiltonian dynamics (arising when approaching the problem from the perspective of PMP).
To this end, %To provide a concise statement, 
define the Hamiltonian function for the optimal control problem \eqref{eq:OC_sys}, \eqref{eq:OC_cost},
\begin{equation}
	\label{eq:OC_hamilt_fct}
	\begin{split}
		&\mathcal{H}(x(k),\lambda(k+1),u(k),k) =  \frac{1}{2}x(k)^{\top}Q x(k) \\ & + \frac{1}{2}u(k)^{\top} R u(k) 
		+ \lambda(k+1)^{\top}(A x(k) + B u(k) + w(k))\,.
	\end{split}
\end{equation}
Utilizing PMP arguments results in the %together with the corresponding 
Hamiltonian dynamics
\begin{subequations}
	\label{eq:OC_hamilt_dyn}
	\begin{align}
		x(k+1) &  = Ax(k) - BR^{-1} B^{\top} \lambda(k+1) + w(k)\,, \label{eq:OC_hamilt_dyn_x} \\[3pt]
		\lambda(k) & = Q x(k) + A^{\top} \lambda(k+1)\,. \label{eq:OC_hamilt_dyn_lam} 
	\end{align}
\end{subequations}
%obtained by replacing $u$ with $u^{P}(k) := \arg \min_u \mathcal{H} = -R^{-1} B^{\top} \lambda(k+1)$.

\smallskip

\begin{lemma}
	\label{lemma:OC_PMP_equiv_DP}
	Consider the system \eqref{eq:OC_sys} together with the cost functional \eqref{eq:OC_cost}.
	Suppose that the hypotheses of Lemma \ref{lemma:OC_DP} hold.
	%Assumption \ref{assu:OC_structural} holds.
	Let $P \in \mathbb{R}^{n \times n}$, $P = P^{\top} \succeq 0$ be the stabilizing solution to \eqref{eq:OC_Riccati} and let $b(\cdot)$ be determined by \eqref{eq:OC_b_def}, and hence such that $\lim_{k \rightarrow \infty}b(k) = 0$.
	Then the set defined by $\lambda(k) = P x(k) + b(k)$ is invariant for \eqref{eq:OC_hamilt_dyn}, for all $k\geq 0$. Moreover,  the optimal control law is given by 
	\begin{equation}
		\label{eq:OC_equivalent_u}
		u^{P}(k) = -R^{-1}B^{\top}\lambda(k+1) = u^{\star}(k)\,,
	\end{equation}
	for all $k \geq 0$, 
	with $u^{\star}$ defined in \eqref{eq:OC_u_star}, where $\lambda(k)$ is the solution of \eqref{eq:OC_hamilt_dyn}, subject to the initial condition $x(0) = x_0$ and $\lambda(0) = Px_0 + b(0)$.
	\hfill $\circ$
\end{lemma}

\smallskip

\textbf{Proof:}
To begin with, letting\footnote{Note that in the proof of Lemma \ref{lemma:OC_PMP_equiv_DP} the superscripts $P$ and $D$ are used to indicate the state trajectory ensuing from \eqref{eq:OC_hamilt_dyn} (PMP) and \eqref{eq:OC_sys}, \eqref{eq:OC_u_star} (DP), respectively.}  $x^{P}(\cdot)$ denote the solution of \eqref{eq:OC_hamilt_dyn_x} with $\lambda(k+1)$ replaced by $P x^{P}(k+1) + b(k+1)$, it follows from \eqref{eq:OC_hamilt_dyn_x} that
% \begin{equation*}
	%     \begin{split}
		%     x^{P}(k+1) =  \, & A x^P(k) - BR^{-1}B^{\top}Px^{P}(k+1) \\[3pt] & - BR^{-1}B^{\top}b(k+1) + w(k)\,,
		%     \end{split}
	% \end{equation*}
% % \noindent
% hence the closed-loop system is described by
\begin{equation}
	\label{eq:OC_cl_sys_xP}
	\begin{split}
		x^{P}(k+1) = & \, (I + BR^{-1}B^{\top}P)^{-1}(A x^{P}(k) + w(k)) \\[3pt] & -(I + BR^{-1}B^{\top}P)^{-1}BR^{-1}B^{\top} b(k+1)\,.
	\end{split}
\end{equation}
Relying on \eqref{eq:OC_identity},
%the Woodbury matrix identity 
% \begin{equation}
	%     \label{eq:OC_identity}
	%     (I+B R^{-1} B^{\top}P )^{-1} = I - B(R+B^{\top}P B)^{-1} B^{\top} P\,,
	% \end{equation}
% \noindent
% it follows that the Riccati equation \eqref{eq:OC_Riccati} can be equivalently written as $0 = P-Q-A^{\top}P(I+B R^{-1} B^{\top}P)^{-1}A$.
the first claim is then shown by substituting \eqref{eq:OC_cl_sys_xP} into \eqref{eq:OC_hamilt_dyn_lam}, yielding
\begin{equation*}
	\begin{split}
		&  P x^{P}(k) + b(k) \, = Q x^{P}(k) + A^{\top}Px^{P}(k+1) + A^{\top}b(k+1) \\[3pt] & \, = Q x^{P}(k) + A^{\top}P(I+BR^{-1}B^{\top}P)^{-1}A x^{P}(k) \\[2pt] & \hspace{0.4cm} + A^{\top} P (I + B R^{-1} B^{\top}P)^{-1} w(k) + A^{\top}b(k+1) \\[2pt] & \hspace{0.4cm} -A^{\top}PB(R+B^{\top}PB)^{-1}B^{\top}b(k+1)\,,
	\end{split}
\end{equation*}
which is satisfied for any $x^{P}(k)$ by the discrete-time Riccati equation \eqref{eq:OC_Riccati} and the definition of the dynamics for $b(\cdot)$ given in \eqref{eq:OC_b_k_eq}. %\eqref{eq:OC_b_def}. 
The second claim is obtained by first considering the system \eqref{eq:OC_sys} in closed-loop with \eqref{eq:OC_u_star} described (utilizing ~\eqref{eq:OC_identity}) by the equation
\begin{equation}
	\label{eq:OC_closed_loop_DP}
	\begin{split}
		x^{D}(k+1) 
		& = (I+BR^{-1}B^{\top}P)^{-1}(Ax^{D}(k) + w(k)) \\[2pt] & \hspace{0.4cm} - B(R+B^{\top}PB)^{-1}B^{\top}b(k+1)\,,
	\end{split}
\end{equation}
Therefore, exploiting again~\eqref{eq:OC_identity}, it follows that  
\begin{equation*}
	\begin{split}
		(I +B R^{-1}  &B^{\top}P)x^{D}(k+1)  
		\\& = \, A x^{D}(k) + w(k)
		+BR^{-1}B^{\top}b(k+1)\,,
	\end{split}
\end{equation*}
% Therefore,   defining $\Omega := I+B R^{-1} B^{\top}P$ for compactness, it follows that
% \begin{equation*}
	%     \begin{split}
		%         \Omega x^{D}(k+1) & =  \, A x^{D}(k) + w(k) \\[2pt] & \hspace{0.4cm} - \Omega B (R+B^{\top}PB)^{-1}B^{\top} b(k+1) \\[3pt] 
		%         & = \, A x^{D}(k) + w(k) \\[2pt] & \hspace{0.4cm}
		%         - \Omega B (R+B^{\top}PB)^{-1}B^{\top} P P^{-1} b(k+1)
		%         \\[3pt] 
		%         & = \, A x^{D}(k) + w(k) \\[2pt] & \hspace{0.4cm}
		%         - \Omega (\tm{I -} \Omega^{-1}) P^{-1} b(k+1)
		%         \\[3pt] 
		%         & = \, A x^{D}(k) + w(k) \\[2pt] & \hspace{0.4cm}
		%         + (-I + BR^{-1}B^{\top}P + I) P^{-1} b(k+1)
		%         \\[3pt] 
		%         & = \, A x^{D}(k) + w(k)
		%         +BR^{-1}B^{\top}b(k+1)
		%     \end{split}
	% \end{equation*}
which, by inspecting \eqref{eq:OC_cl_sys_xP} and since $x^{D}(0) = x^{P}(0) = x_0$, implies that $u^{\star}(\cdot)$ and $u^{P}(\cdot)$ induce an identical (optimal) closed-loop trajectory.
\hfill $\square$


	\begin{remark}\label{rem:sufficient_cond}
		The %$deeper
		implications of \eqref{eq:OC_equivalent_u} are explicitly discussed here. While conditions derived via Dynamic Programming arguments (such as the ones of Lemma~\ref{lemma:OC_DP} in the context of \eqref{eq:OC_sys}, \eqref{eq:OC_cost}) provide necessary and sufficient conditions for optimality, those based instead on Pontryagin's principle typically yield only necessary requirements. However, the merit of Lemma~\ref{lemma:OC_PMP_equiv_DP} consists in determining certain \emph{boundary conditions} for the Hamiltonian dynamics~\eqref{eq:OC_hamilt_dyn} with the property that the ensuing output trajectory coincides \emph{for any time} with the optimal solution derived in Lemma~\ref{lemma:OC_DP}. As a consequence, the conditions derived via PMP become also sufficient at least for the given initial condition $x_0$ for which the boundary conditions have been determined. More precisely, as entailed by the statement of Lemma~\ref{lemma:OC_PMP_equiv_DP}, such boundary conditions are enforced by determining the costate trajectory that asymptotically converges to the origin, namely such that $\lim_{k \rightarrow \infty} \lambda(k) = 0$. In fact, the latter claim is derived by noting that 
			\begin{equation}\label{eq:lambda_invar_time_varying}
				\lambda(k) = P x(k) + b(k)
			\end{equation}
			\noindent for all $k$ and that $x(k)$ and $b(k)$ tend %s
            to zero as $k$ tends to infinity.
		\hfill $\triangle$
\end{remark}


\subsection{OL-NE via Riccati equations}\label{sec:pmp}

The key conclusion of            Lemma~\ref{lemma:OC_PMP_equiv_DP} and of Remark~\ref{rem:sufficient_cond} can be summarized as follows: for fixed $x_0 \in \mathbb{R}^n$, the Hamiltonian dynamics~\eqref{eq:OC_hamilt_dyn} yield sufficient conditions for optimality provided one selects the trajectory of~\eqref{eq:OC_hamilt_dyn} such that $\lim_{k\rightarrow\infty} \lambda(k) = 0$. This observation is instrumental as we
now turn our attention back to dynamic games and OL-NE strategies.
Consider the class of discrete-time systems characterized by a non-singular matrix $A$, and with certain structural properties as prescribed in the following assumptions.

\smallskip

\begin{assumption}
	\label{assu:A_nonsing}
	Consider the system \eqref{eq:lti}.
	The matrix $A$ is non-singular, \emph{i.e.} $\sigma(A) \cap \{0\} = \emptyset$.
	\hfill $\circ$    
\end{assumption}

\smallskip

\begin{assumption}
	\label{assu:structural_games}
	Consider the system \eqref{eq:lti} and the cost functionals \eqref{eq:cost_J_i}. The pairs $(A,B_i)$ and $(A,Q_i)$ are reachable and detectable, respectively, for $i = 1,2$.
	\hfill $\circ$ 
\end{assumption} 

The constructions of this section heavily rely on the properties of the matrix $H \in \mathbb{R}^{3n \times 3n}$ defined, under the hypothesis that Assumption~\ref{assu:A_nonsing} holds, by
\begin{equation}
	\label{eq:def_mat_H}
	H \! := \! \left[\begin{array}{ccc}
		A + S_1A^{-\top}Q_1 + S_2A^{-\top}Q_2 & -S_1A^{-\top} & -S_2A^{-\top} \\
		-A^{-\top}Q_1 & A^{-\top} & 0 \\
		-A^{-\top}Q_2 & 0 & A^{-\top}
	\end{array}\right]\!\!.
\end{equation}
with $S_i = B_i R_i^{-1} B_i^{\top}$, $i = 1,2$. 
The following property regarding the spectrum of $H$ is immediate.

\smallskip

% \begin{lemma}
	%     \label{lemma:H_nonsingular}
	%     Consider the matrix $H$ in \eqref{eq:def_mat_H} and suppose that Assumption \ref{assu:A_nonsing} holds.
	%     Then $\sigma(H) \cap \{0\} = \emptyset$.
	%     \hfill $\circ$
	% \end{lemma}

% \smallskip

% \textbf{Proof:}
%     Suppose by contradiction that there exists a non-zero vector $v = [v_1^{\top}, v_2^{\top}, v_3^{\top}]^{\top} \in \mathbb{R}^{3n}$, $v \neq 0$, with \linebreak
%     $v_i \in \mathbb{R}^{n}$, for $i = 1,2,3$, such that $H v = 0$.
%     By considering the partitioning of the matrix $H$ as in \eqref{eq:def_mat_H} the hypothesis above would imply
%     \begin{subequations}
	%         \label{eq:H_inv_sys}
	%         \begin{align}
		%             \begin{split}
			%                 0 & = (A+S_1 A^{-\top}Q_1 + S_2 A^{-\top}Q_2)v_1 \\[2pt] & \hspace{0.5cm} - S_1 A^{-\top}v_2 -
			%                 S_2 A^{-\top}v_3\,, \label{eq:H_inv_sys_1}
			%             \end{split} \\[4pt]
		%             0 & = -A^{-\top}Q_1 v_1 + A^{-\top}v_2\,, \label{eq:H_inv_sys_2} \\[4pt]
		%             0 & = -A^{-\top}Q_2 v_1 + A^{-\top}v_3\,. \label{eq:H_inv_sys_3}
		%         \end{align}
	%     \end{subequations}
%     The sum of \eqref{eq:H_inv_sys},  with \eqref{eq:H_inv_sys_2} and \eqref{eq:H_inv_sys_3} left-multiplied by $S_1$ and $S_2$, respectively, yields $A v_1 = 0$, hence $v_1 = 0$ by Assumption \ref{assu:A_nonsing}.
%     Replacing $v_1 = 0$ into \eqref{eq:H_inv_sys_2} and \eqref{eq:H_inv_sys_3} yields $v_2 = v_3 = 0$, which contradicts the hypothesis.
% \hfill $\square$

\smallskip

\begin{theorem}
	\label{thm:OL_NE}
	Consider the system \eqref{eq:lti} and the cost functionals \eqref{eq:cost_J_i}, for $i = 1, 2$.
	Suppose that Assumptions \ref{assu:A_nonsing} and \ref{assu:structural_games} hold and fix any $x_0 \in \mathbb{R}^{n}$.
	Suppose that the state/costate dynamics
	\begin{equation}
		\label{eq:state_costate_dyn}
		\left[\begin{array}{c} x(k+1) \\ \lambda_1(k+1) \\ \lambda_2(k+1)  \end{array} \right] = H \left[\begin{array}{c} x(k) \\ \lambda_1(k) \\ \lambda_2(k)  \end{array} \right]\,,
	\end{equation}
	\noindent admit a solution, denoted $(x^{OL}, \lambda_1^{OL}, \lambda_2^{OL})$, such that $x(0) = x_0$ and $\lim_{k \rightarrow \infty}\lambda_i(k) = 0$. 
	Then the underlying dynamic game possesses an  OL-NE described by
	\begin{equation}
		\label{eq:u_OL_NE}
		u^{OL}_i(k) = -R_i^{-1}B_i^{\top}\lambda_i^{OL}(k+1)\,,
	\end{equation}
	for $i = 1,2$. 
	\hfill $\circ$
\end{theorem}

\smallskip

\textbf{Proof:}
The claim is shown by borrowing, symmetrically for each player, the constructions of Lemma \ref{lemma:OC_PMP_equiv_DP} with respect to the Hamiltonian functions
\begin{equation}
	\label{eq:hamiltonian}
	\begin{split}
		\mathcal{H}_i = &\,\,\frac{1}{2}x(k)^\top Q_ix(k) + \frac{1}{2}u_{i}(k)^\top R_iu_{i}(k) \\
		&+ \lambda_{i}(k+1)^\top\left(Ax(k) + B_iu_{i}(k) + B_j\phi_j^{\star}(k,x_0)\right),
	\end{split}
\end{equation}
for $i = 1, 2$, for $j = 1, 2$ and with $i \neq j$, namely as in \eqref{eq:OC_hamilt_fct} with the role of $w(\cdot)$ played by $\phi_j^{\star}(\cdot,x_0)$. 
The conclusions of Lemmas \ref{lemma:OC_DP} and \ref{lemma:OC_PMP_equiv_DP}, together with the comments in Remark~\ref{rem:sufficient_cond}  imply that the above underlying (partial) optimal control problem has the unique solution described by \eqref{eq:u_OL_NE} for each player, with the individual costate variables satisfying
\begin{equation}
	\label{eq:costate}
	\lambda_i(k) = Q_ix(k) + A^\top\lambda_i(k+1)\,,
\end{equation}
provided the boundary condition $\lim_{k \rightarrow \infty} \lambda_i(k) = 0$ holds, consistently to the conclusions of Lemma \ref{lemma:OC_PMP_equiv_DP} in the case of a single-player. The latter is precisely the property that characterizes the process $(x^{OL}, \lambda_1^{OL}, \lambda_2^{OL})$ and hence it holds by assumption.
Hence, by replacing the control laws \eqref{eq:u_OL_NE} into \eqref{eq:lti} a composite description of the evolution of the individual costate variables is obtained as
\begin{equation}\label{eq:composite_ham_dyn_mixed_times}
	\left[\begin{array}{c}
		x(k+1) \\
		\lambda_1(k) \\
		\lambda_2(k)
	\end{array}\right] = \left[\begin{array}{ccc}
		A & -S_1 & -S_2 \\
		Q_1 & A^\top & 0 \\
		Q_2 & 0 & A^\top
	\end{array}\right]
	\left[\begin{array}{c}
		x(k) \\
		\lambda_1(k+1) \\
		\lambda_2(k+1)
	\end{array}\right].
\end{equation}
By arranging the equations in \eqref{eq:composite_ham_dyn_mixed_times} in order to induce consistent time instants on each side, one immediately obtains
\begin{equation}
	\hspace{-0.3cm}  \label{eq:composite_ham_dyn_pre_inverse}
	\left[\begin{array}{c}
		x(k+1) \\
		\lambda_1(k+1) \\
		\lambda_2(k+1)
	\end{array}\right] \! = \! \left[\begin{array}{ccc}
		I & S_1 & S_2 \\
		0 & -A^\top & 0 \\
		0 & 0 & -A^\top
	\end{array}\right]^{-1} \! \! \left[\begin{array}{ccc}
		A & 0 & 0 \\
		Q_1 & -I & 0 \\
		Q_2 & 0 & -I
	\end{array}\right] \! \!
	\left[\begin{array}{c}
		x(k) \\
		\lambda_1(k) \\
		\lambda_2(k)
	\end{array}\right]\!\!.
\end{equation}
% The proof is then concluded by observing that
% \begin{equation}
	%     \label{eq:compact_right_inv}
	%     \left[\begin{array}{ccc}
		%          I & S_1 & S_2 \\
		%          0 & -A^\top & 0 \\
		%          0 & 0 & -A^\top
		%     \end{array}\right]^{-1} = \left[\begin{array}{ccc}
		%          I & S_1A^{-\top} & S_2A^{-\top} \\
		%          0 & -A^{-\top} & 0 \\
		%          0 & 0 & -A^{-\top}
		%     \end{array}\right],
	% \end{equation}
% and hence
The proof is concluded by noting that, taking the matrix inverse,
the state/costate dynamics \eqref{eq:composite_ham_dyn_mixed_times} yields \eqref{eq:state_costate_dyn}, since
\begin{equation}
	\label{eq:H_alternative}
	H = \left[\begin{array}{ccc}
		I & S_1A^{-\top} & S_2A^{-\top} \\
		0 & -A^{-\top} & 0 \\
		0 & 0 & -A^{-\top}
	\end{array}\right]
	\left[\begin{array}{ccc}
		A & 0 & 0 \\
		Q_1 & -I & 0 \\
		Q_2 & 0 & -I
	\end{array}\right],
\end{equation}
coincides with the matrix defined in \eqref{eq:def_mat_H}. Therefore, since by the considered boundary conditions of the process $(x^{OL}, \lambda_1^{OL}, \lambda_2^{OL})$ the PMP conditions become sufficient, one has that \eqref{eq:u_OL_NE} \emph{is} the optimal solution of the underlying single-player optimal control problem for a fixed strategy of the opponent. Finally, since the same reasoning applies symmetrically to both players, it can be concluded that \eqref{eq:u_OL_NE}, for $i = 1,2$ constitute open-loop Nash equilibrium strategies.  
\hfill $\square$


The following assumption extends to the discrete-time framework conditions inspired by the \emph{strong stabilizability}  of \cite{Engwerda2007} for continuous-time LQ games.

\smallskip

\begin{assumption}
	\label{assu:spectrum_H}
	The matrix $H$ in \eqref{eq:def_mat_H} possesses $n$ eigenvalues, counted with their multiplicity, in $\mathbb{C}_d$ and $2n$ eigenvalues with modulus greater than or equal to one. Moreover, letting $\mathcal{V}^{-}$ denote the $n$-dimensional stable invariant subspace of $H$, the subspaces $\mathcal{V}^{-}$ and 
		$$ {\rm Im}\left(\left[\begin{array}{cc} 0 & 0 \\I_n & 0 \\0 & I_n \end{array} \right]\right) $$
		\noindent are complementary.
	
	\hfill $\circ$    
\end{assumption}

%As entailed by its statement, 
	Theorem~\ref{thm:OL_NE} provides sufficient conditions for the existence of an OL-NE \emph{for a fixed initial condition}. Therefore, the control law~\eqref{eq:u_OL_NE} does not in general satisfy the underlying Nash inequalities whenever its performance is compared with feedback strategies (see also the numerical simulations of Section \ref{sec:sim}), namely it does not constitute a F-NE. Nonetheless, it may be still desirable to express~\eqref{eq:u_OL_NE} as a feedback from the current state, \emph{i.e.} in providing the so-called \emph{feedback synthesis}\footnote{See \cite[Sec. 7.4]{Engwerda2005} for more detailed discussions.} of such OL-NE.
This section is concluded by showing that under %the %conditions required in 
	Assumption \ref{assu:spectrum_H} there exists a unique pair of OL-NE strategies \eqref{eq:u_OL_NE} permitting a feedback synthesis. This solution is
%permit a feedback synthesis of the OL-NE strategies \eqref{eq:u_OL_NE}, 
characterized in Theorem~\ref{thm:OL_feedback_synth},  %by solving
via the solution of a set of coupled Riccati equations. 

\smallskip

\begin{theorem}
	\label{thm:OL_feedback_synth}
	Consider the system \eqref{eq:lti} and the cost functionals \eqref{eq:cost_J_i}, for $i = 1, 2$.
	Suppose that the hypotheses of Theorem \ref{thm:OL_NE} and Assumption  \ref{assu:spectrum_H} hold and fix any $x_0 \in \mathbb{R}^{n}$.
	Then the underlying dynamic game possesses a unique OL-NE admitting a feedback synthesis, described by
	\begin{equation}
		\label{eq:u_OL_NE_feedback_synth}
		u^{OL}_i(k) = -R_i^{-1}B_i^{\top}A^{-\top}(-Q_i +P_i^p)x(k) =: K_i^{OL} x(k)\,,
	\end{equation}
	for $i = 1,2$, with $P_i^p \in \mathbb{R}^{n \times n}$ solving the coupled Riccati equations
	\begin{equation}
		\label{eq:OL_ARE}
		\begin{split}
			0 = & A^{-\top}Q_1 + P_1^p(A + S_1A^{-\top}Q_1 + S_2A^{-\top}Q_2) -A^{-\top}P_1^p \\ & 
            - P_1^p S_1 A^{-\top}P_1^p - P_1^pS_2A^{-\top}P_2^p\,, \\[4pt]
			0 = & A^{-\top}Q_2 + P_2^p(A + S_1A^{-\top}Q_1 + S_2A^{-\top}Q_2)-A^{-\top}P_2^p \\ & 
            - P_2^pS_1A^{-\top}P_1^p - P_2^pS_2A^{-\top}P_2^p.
		\end{split}
	\end{equation}
	\hfill $\circ$
\end{theorem}

\textbf{Proof:}
Since the costate variables should converge asymptotically to zero, their initial condition can be determined by restricting the dynamics to the stable invariant subspace, whose existence and dimension are ensured by Assumption \ref{assu:spectrum_H}.
One should determine matrices $P_1^p$, $P_2^p$ and $\Lambda$, not necessarily symmetric, such that
\begin{equation}\label{eq:invariance}
	H
	\left[\begin{array}{c}
		I \\
		P_1^p \\
		P_2^p
	\end{array}\right] = \left[\begin{array}{c}
		I \\
		P_1^p \\
		P_2^p
	\end{array}\right]\Lambda,
\end{equation}
with $\Lambda$ such that $\sigma(\Lambda) \subset \mathbb{C}_d$.
%By r
Replacing $H$ defined in \eqref{eq:def_mat_H} into \eqref{eq:invariance} and substituting the first row of the resulting system of equations into the bottom two gives  \eqref{eq:OL_ARE}.
% , the following system of equations 
% \begin{equation*}
	%     \begin{split}
		%         \Lambda & = A + S_1A^{-\top}Q_1 + S_2A^{-\top}Q_2 \\[2pt] & \hspace{0.4cm} -S_1A^{-\top}P_1^p - S_2A^{-\top}P_2^p, \\[2pt]
		%         P_1^p\Lambda & = -A^{-\top}Q_1 + A^{-\top}P_1^p, \\[2pt]
		%         P_2^p\Lambda & = -A^{-\top}Q_2 + A^{-\top}P_2^p\,,
		%     \end{split}
	% \end{equation*}
% is obtained, from which \eqref{eq:OL_ARE} is immediately derived.
Finally, the structure of \eqref{eq:u_OL_NE_feedback_synth} is obtained by restricting the evolution of the costate $\lambda_i(k+1)$ to the set defined by
\begin{equation}\label{eq:lambda_invar_static}
	\lambda_i(k) = P_i^p x(k)
\end{equation}
\noindent as ensured by the \emph{invariant} subspace characterized in \eqref{eq:invariance}, namely such that $\lambda_i(k+1) = -A^{-\top}Q_i x(k) + A^{-\top}P_i^p x(k)$, for $i = 1, 2$.
\hfill $\square$

\begin{remark}\label{rem:feedback_synthesis}
		{\rm The arguments employed in the proof of Theorem \ref{thm:OL_feedback_synth} show that the (somewhat more abstract) boundary conditions of Theorem \ref{thm:OL_NE} can be more effectively replaced by the initial condition $\lambda_i(0) = P_i^p x_0$, with $P_i^p$, $i = 1,2$, solutions of the asymmetric ARE \eqref{eq:OL_ARE}. Furthermore, it is worth relating the two different expressions for the costate variable, namely \eqref{eq:lambda_invar_time_varying} (although in the context of single-player optimal control) and \eqref{eq:lambda_invar_static}: the former is obtained in terms of a time-varying relation in which the underlying matrix $P$ solves conditions derived via Dynamic Programming arguments; the latter is a static relation between state and costate derived by invariance reasonings on the Hamiltonian dynamics arising in PMP. Therefore, although seemingly similar in their expressions, the two conditions are different in their essence (see also similar results in the context of nonlinear continuous-time differential games in \cite{sassano2022analysis}).}
		\hfill $\triangle$ 
\end{remark}

\smallskip

\begin{remark}\label{rem:relax_assumption_4}
	A milder version of Assumption \ref{assu:spectrum_H} may be stated by requiring, instead, that the spectrum of $H$ contains \emph{at least} $n$ eigenvalues with modulus smaller than one. The conclusions of Theorem \ref{thm:OL_feedback_synth} remain valid apart from \emph{uniqueness} of the OL-NE (a continuous-time counterpart of this aspect can be found in the discussions following Theorem 7.11 of \cite{Engwerda2005}).
	\hfill $\triangle$    
\end{remark}


\smallskip

\begin{remark}
	\label{rem:comparison_AbouKandil}
	{\rm A similar formulation of dynamic games has been addressed in \cite{freiling1999discrete}, in which the  focus is on an elegant characterization of the limiting behavior of the finite-horizon Riccati difference equations as the terminal time tends to infinity.
		Differently from \cite{freiling1999discrete}, %here 
		we provide sufficient conditions that are derived by tackling directly the infinite-horizon problem and this objective is achieved via a preliminary, and relevant \emph{per se}, consideration of PMP applied to a (single-player) optimal control problem with underlying dynamics influenced by a known exogenous input. 
		% extension to the infinite-horizon setting of the conditions arising with PMP.
		Moreover, the conditions \eqref{eq:OL_ARE} are more reminiscent of their continuous-time counterpart than the structure of the algebraic equations derived in \cite{freiling1999discrete}.
		This is obtained by relying on the state/costate extended dynamics \eqref{eq:state_costate_dyn}, with the property that the eigenvalues of the closed-loop equilibrium dynamics are a subset of $\sigma(H)$, instead of the set containing the \emph{inverse} of the eigenvalues of the dynamics matrix of the state/costate system used in \cite{freiling1999discrete}.}
	\hfill $\triangle$     
\end{remark}

\smallskip

\begin{remark}\label{rem:comaprison}
	{\rm Theorem~\ref{thm:OL_feedback_synth} provides conditions under which there exist constant matrices $K_i^{OL} \in \mathbb{R}^{m_i \times n}$ such that a pair of OL-NE strategies can be implemented as a feedback law, namely $\phi_i^{\star}(k,x_0) = K_i^{OL} x^{\star}(k)$, for $i = 1, 2$ and for all $k \in \mathbb{N}$, where $x^{\star}$ satisfies $x^{\star}(k+1) = (A + B_1 K_1^{OL} + B_2 K_2^{OL})x^{\star}(k)$, $x(0) = x_0$.
		However, recall from Remark~\ref{rem:OL_vs_F} that the strategies \eqref{eq:u_OL_NE_feedback_synth} do not constitute a F-NE solution. %In fact, the \emph{feedback structure} of the strategies $(K_1^{OL}x(\cdot), K_2^{OL}x(\cdot))$ \emph{does not prevent} the existence of a 
		Namely, there may exist a gain matrix $\hat{K}_1$ (similarly for the second player) such that
		\begin{equation}
			\label{eq:OL_F_counter_ex}
			J_1(x_0, K_1^{OL}\hat{x}(\cdot), K_2^{OL}\hat{x}(\cdot)) \geq J_1(x_0, \hat{K}_1 \hat{x}(\cdot), K_2^{OL}\hat{x}(\cdot))\,,
		\end{equation}
		where $\hat{x}$ satisfies $\hat{x}(k+1) = (A+B_1 \hat{K}_1 + B_2 K_2^{OL} )\hat{x}(k)$, $\hat{x}(0) = x_0$. This is, in fact, demonstrated in the numerical example considered in Section \ref{sec:sim}.
		Nonetheless, 
		it may still be desirable to compute a feedback synthesis of an OL-NE for several reasons, including a certain margin of robustness guaranteed by the feedback implementation and, more importantly, the possibility of simultaneously computing an OL-NE \emph{for all initial conditions}, without the need to resort to dedicated boundary conditions (see the comments in Remark \ref{rem:feedback_synthesis}).
		It is also worth comparing the coupled matrix equations \eqref{eq:u_OL_NE_feedback_synth}, \eqref{eq:OL_ARE} and \eqref{eq:ACE}, \eqref{eq:K_M} characterizing %F-NE 
		OL-NE and %OL-NE 
		F-NE solutions, respectively. While the former are quadratic in the (in general asymmetric) variables $P_i^p$, the latter are not quadratic in the symmetric variables $P_i$, $i =1,2$. Hence, depending on the dimension of the system \eqref{eq:lti}, OL-NE strategies admitting a feedback synthesis may be easier to compute and may hence %of 
		be of practical interest.}
	\hfill $\triangle$     
\end{remark}



\section{Numerical simulations}\label{sec:sim}
In this section the presented results are illustrated %a conceptual comparison between F-NE and OL-NE is provided 
via a numerical example. 
An example with scalar dynamics is considered to permit an immediate visualization of the results. Namely, consider \eqref{eq:lti} and  \eqref{eq:cost_J_i}, 
$i = 1, 2$, with $A~=~1, B_1~=~2, B_2~=~1, Q_1~=~1, Q_2~=~0.2, R_1~=~2$ and $R_2~=~0.5$.
For the given system and cost parameters there exists a unique F-NE. The corresponding strategies are described by \eqref{eq:u_F_NE} with $(K_1,K_2) = ( -0.3205,-0.1096)$, which satisfy \eqref{eq:ACE}, \eqref{eq:K_M} with $(P_1,P_2) = (1.2854,0.2197)$. Note that the present example is such that Assumption~\ref{assu:spectrum_H} holds. Hence, there exists a unique OL-NE admitting a feedback synthesis. The %equilibrium 
corresponding strategies are given by \eqref{eq:u_OL_NE_feedback_synth} with $(P_1, P_2) = (1.3165, 0.2633)$, $(K_1^{OL},K_2^{OL}) = (-0.3165,-0.1266)$ satisfying \eqref{eq:OL_ARE}.
%Finally, 
The time histories of the state, starting from $x(0) =$ 0.95751, corresponding to the F-NE ($u_i=u_i^F$, $i=1,2$, red line) and the OL-NE ($u_i=u_i^{OL}$, $i=1,2$, blue line) are shown in Figure~\ref{fig:x_comparison}. The time histories of the control inputs $u_1(k)$ (solid lines) and $u_2(k)$ (dashed lines) corresponding to the F-NE (red) and OL-NE (blue) strategies are shown in Figure \ref{fig:u_comparison}. The inspection of the graphs of Figures~\ref{fig:x_comparison} and \ref{fig:u_comparison} immediately reveals, as expected, that the two set of conditions lead to two different sets of equilibrium strategies, which in turn induce distinct time evolutions of the underlying plant.
% Figure environment removed
% Figure environment removed

A different set of numerical simulations is now considered,  with the objective of demonstrating the observations made in Remarks \ref{rem:OL_vs_F} and \ref{rem:comaprison} and graphically illustrating the difference between the Definitions \ref{def:FNE} and \ref{def:OLNE}. To this end, we focus on the pair of OL-NE strategies determined above. %The main purpose of this case study consists in graphically illustrating the difference between the Definitions \ref{def:FNE} and \ref{def:OLNE}, as also discussed in Remarks \ref{rem:OL_vs_F} and \ref{rem:comaprison}.
In Figure~\ref{fig:j1_f} the outcome ($J_1$) of player~1 is depicted, for different strategies of the form $u_1(k) = K_1x(k)$, with the action of player~2 fixed at $u_2(k) = K_2^{OL} x(k)$.
It can be clearly appreciated, as discussed in Remark~\ref{rem:OL_vs_F} and Remark \ref{rem:comaprison} %Section~\ref{sec:pmp}, %~\ref{rem:OC_u_star_k}, 
that the minimum of $J_1$ (indicated by the vertical red dashed line) is not, in general, attained by the OL-NE (indicated by the vertical black dotted line). A similar comparison is depicted in Figure~\ref{fig:j2_f}, by varying the strategy $u_2(k)= K_2x(k)$, with the action of player~1 fixed at $u_1(k) = K_1^{OL} x(k)$. This observation highlights the fact that the \emph{feedback synthesis of an OL-NE} does not yield a F-NE, since it does not in general enforce the inequalities~\eqref{eq:def_FNE} whenever the feasible set of alternative strategies includes control actions fed back from the current value of the state.
Nonetheless, the outcome of player~1 for different strategies $u_1(k)=K_1x(k)$ is shown in Figure~\ref{fig:j1_ol} when the strategy of player~2 is fixed at\footnote{Note that $(A_{cl}^{OL})^{k}x(0)$ corresponds to the trajectory $x(k)$ of the closed-loop system \eqref{eq:lti}, \eqref{eq:u_OL_NE_feedback_synth}, $i=1,2$.} $u_2(k) = K_2^{OL}(A_{cl}^{OL})^kx(0)$, where $A_{cl}^{OL}=\left(A + B_1K_1^{OL} + B_2K_2^{OL}\right)$. Then, the individual optimal control problem for the player 1 is solved by $u_1(k) = K_1^{OL}x(k)$, even if the admissible set includes feedback control laws, as can be clearly appreciated by the (intersecting) vertical red (indicating the minimum of $J_1$) and black dashed lines (indicating the outcome corresponding to $u_1(k) = K_1^{OL}x(k)$). The same observation can be made for player~2, as depicted in Figure~\ref{fig:j2_ol}. %Furthermore, 
Note that in Figures \ref{fig:j1_f}-\ref{fig:j2_ol} the red stars represent the minimum of the cost functional.
% Finally, the time histories of the state, corresponding to the OL-NE ($u_i=u_i^{OL}$, $i=1,2$, blue line)  and the F-NE ($u_i=u_i^F$, $i=1,2$, red line) are shown in Figure~\ref{fig:x_comparison}. The time histories of the control inputs $u_1(k)$ (solid lines) and $u_2(k)$ (dashed lines) corresponding to the OL-NE (blue) and F-NE (red) strategies are shown in Figure  \ref{fig:u_comparison}. 


% Figure environment removed

% Figure environment removed


% Figure environment removed
% Figure environment removed


\addtolength{\textheight}{-0.5cm}
\section{Conclusions}\label{sec:conc}
%In this work, 
%An %comprehensive 
%analysis of Nash equilibria for dynamic games over an infinite horizon has been provided in the presence of linear, discrete-time dynamics and quadratic cost functionals. 
Feedback and Open-Loop NE have been studied and discussed for dynamic games defined by linear, discrete-time dynamics and quadratic cost functionals over an infinite horizon. Leveraging on DP and PMP, respectively, for the former we provide a comprehensive characterization in terms of the stabilizing solutions of coupled algebraic matrix equations, %we have reinterpreted necessary and sufficient conditions in such a way to highlight interesting insights on their nature, 
whereas for the latter %equilibria 
we have derived %provided 
an entirely new characterization in terms of certain quadratic matrix equations. This result has been enabled by an extension of PMP arguments to the context of infinite-horizon optimal control problems, with the underlying dynamics influenced by a known exogenous input. % with affine dynamics. 
The analysis provides insights that are demonstrated and visualized on a numerical example.

\bibliographystyle{siamplain}
\bibliography{biblio}
\end{document}
