@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}


@generic{Gu2020,
   author = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
   title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
   year = {2020},
}
@generic{Hermann2015,
   author = {Karl Moritz Hermann and Tomáš Kočiský and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman and Phil Blunsom},
   title = {Teaching Machines to Read and Comprehend},
   year = {2015},
}
@inproceedings{,
   abstract = {We present COVID-QA, a Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19. To evaluate the dataset we compared a RoBERTa base model fine-tuned on SQuAD with the same model trained on SQuAD and our COVID-QA dataset. We found that the additional training on this domain-specific data leads to significant gains in performance. Both the trained model and the annotated dataset have been open-sourced at: https://github.com/deepset-ai/COVID-QA},
   author = {Timo Möller and Anthony Reina and Raghavan Jayakumar and Malte Pietsch},
   city = {Online},
   journal = {Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020},
   month = {7},
   publisher = {Association for Computational Linguistics},
   title = {COVID-QA: A Question Answering Dataset for COVID-19},
   url = {https://www.aclweb.org/anthology/2020.nlpcovid19-acl.18},
   year = {2020},
}
@inproceedings{Yang2015,
   author = {Yi Yang and Wen-tau Yih and Christopher Meek},
   city = {Lisbon, Portugal},
   doi = {10.18653/v1/D15-1237},
   journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
   month = {9},
   pages = {2013-2018},
   publisher = {Association for Computational Linguistics},
   title = {WikiQA: A Challenge Dataset for Open-Domain Question Answering},
   url = {https://www.aclweb.org/anthology/D15-1237},
   year = {2015},
}
@article{Nentidis_2020,
   title={Results of the Seventh Edition of the BioASQ Challenge},
   ISBN={9783030438876},
   ISSN={1865-0937},
   url={http://dx.doi.org/10.1007/978-3-030-43887-6_51},
   DOI={10.1007/978-3-030-43887-6_51},
   journal={Communications in Computer and Information Science},
   publisher={Springer International Publishing},
   author={Nentidis, Anastasios and Bougiatiotis, Konstantinos and Krithara, Anastasia and Paliouras, Georgios},
   year={2020},
   pages={553–568}
}
@generic{Clicr,
   author = {Simon Šuster and Walter Daelemans},
   title = {CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension},
   year = {2018},
}
@generic{Huang2019,
   author = {Lifu Huang and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
   title = {Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning},
   year = {2019},
}
@generic{Bajaj2018,
   author = {Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara and Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
   title = {MS MARCO: A Human Generated MAchine Reading COmprehension Dataset},
   year = {2018},
}
@inproceedings{Dua2019,
   author = {Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},
   journal = {Proc. of NAACL},
   title = {  DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
   year = {2019},
}
@generic{Yagcioglu2018,
   author = {Semih Yagcioglu and Aykut Erdem and Erkut Erdem and Nazli Ikizler-Cinbis},
   title = {RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes},
   year = {2018},
}
@generic{Zellers2018,
   author = {Rowan Zellers and Yonatan Bisk and Roy Schwartz and Yejin Choi},
   title = {SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference},
   year = {2018},
}
@generic{Lai2017,
   author = {Guokun Lai and Qizhe Xie and Hanxiao Liu and Yiming Yang and Eduard Hovy},
   title = {RACE: Large-scale ReAding Comprehension Dataset From Examinations},
   year = {2017},
}
@generic{Yang2018,
   author = {Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W Cohen and Ruslan Salakhutdinov and Christopher D Manning},
   title = {HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
   year = {2018},
}
@generic{Joshi2017,
   author = {Mandar Joshi and Eunsol Choi and Daniel S Weld and Luke Zettlemoyer},
   title = {TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
   year = {2017},
}
@misc{rajpurkar2018know,
      title={Know What You Don't Know: Unanswerable Questions for SQuAD}, 
      author={Pranav Rajpurkar and Robin Jia and Percy Liang},
      year={2018},
      eprint={1806.03822},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@generic{Trischler2017,
   author = {Adam Trischler and Tong Wang and Xingdi Yuan and Justin Harris and Alessandro Sordoni and Philip Bachman and Kaheer Suleman},
   title = {NewsQA: A Machine Comprehension Dataset},
   year = {2017},
}
@generic{Reddy2019,
   author = {Siva Reddy and Danqi Chen and Christopher D Manning},
   title = {CoQA: A Conversational Question Answering Challenge},
   year = {2019},
}
@article{Kwiatkowski2019,
   author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
   journal = {Transactions of the Association of Computational Linguistics},
   title = {Natural Questions: a Benchmark for Question Answering Research},
   year = {2019},
}
@generic{Clark2018,
   author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
   title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
   year = {2018},
}
@inproceedings{MEDIQA,
   abstract = {This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98\% in the NLI task, 74.9\% in the RQE task, and 78.3\% in the QA task. In this paper, we describe the tasks, the datasets, and the participants' approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain.},
   author = {Asma Ben Abacha and Chaitanya Shivade and Dina Demner-Fushman},
   city = {Florence, Italy},
   doi = {10.18653/v1/W19-5039},
   journal = {Proceedings of the 18th BioNLP Workshop and Shared Task},
   month = {8},
   pages = {370-379},
   publisher = {Association for Computational Linguistics},
   title = {Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering},
   url = {https://www.aclweb.org/anthology/W19-5039},
   year = {2019},
}
@generic{Zhang2018,
   author = {Xiao Zhang and Ji Wu and Zhiyang He and Xien Liu and Ying Su},
   title = {Medical Exam Question Answering with Large-scale Reading Comprehension},
   year = {2018},
}
@generic{Jin2019,
   author = {Qiao Jin and Bhuwan Dhingra and Zhengping Liu and William W Cohen and Xinghua Lu},
   title = {PubMedQA: A Dataset for Biomedical Research Question Answering},
   year = {2019},
}
@inproceedings{Vilares2019,
   abstract = {We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.},
   author = {David Vilares and Carlos Gomez-Rodr},
   city = {Florence, Italy},
   doi = {10.18653/v1/P19-1092},
   journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
   month = {7},
   pages = {960-966},
   publisher = {Association for Computational Linguistics},
   title = {HEAD-QA: A Healthcare Dataset for Complex Reasoning},
   url = {https://www.aclweb.org/anthology/P19-1092},
   year = {2019},
}
@generic{Chen2017,
   author = {Danqi Chen and Adam Fisch and Jason Weston and Antoine Bordes},
   title = {Reading Wikipedia to Answer Open-Domain Questions},
   year = {2017},
}
@inproceedings{Wolf2020,
   abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
   author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Remi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander Rush},
   city = {Online},
   doi = {10.18653/v1/2020.emnlp-demos.6},
   journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
   month = {10},
   pages = {38-45},
   publisher = {Association for Computational Linguistics},
   title = {Transformers: State-of-the-Art Natural Language Processing},
   url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
   year = {2020},
}
@inproceedings{Rajpurkar2016,
   author = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
   city = {Austin, Texas},
   doi = {10.18653/v1/D16-1264},
   journal = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
   month = {11},
   pages = {2383-2392},
   publisher = {Association for Computational Linguistics},
   title = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
   url = {https://www.aclweb.org/anthology/D16-1264},
   year = {2016},
}
@inproceedings{Beltagy2019,
   abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
   author = {Iz Beltagy and Kyle Lo and Arman Cohan},
   city = {Hong Kong, China},
   doi = {10.18653/v1/D19-1371},
   journal = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
   month = {11},
   pages = {3615-3620},
   publisher = {Association for Computational Linguistics},
   title = {SciBERT: A Pretrained Language Model for Scientific Text},
   url = {https://www.aclweb.org/anthology/D19-1371},
   year = {2019},
}
@inproceedings{Devlin2019,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
   city = {Minneapolis, Minnesota},
   doi = {10.18653/v1/N19-1423},
   journal = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
   month = {6},
   pages = {4171-4186},
   publisher = {Association for Computational Linguistics},
   title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
   url = {https://www.aclweb.org/anthology/N19-1423},
   year = {2019},
}
@article{c99d46c12d234e77957c3d847b64f5cf,
title = "BioBERT: A pre-trained biomedical language representation model for biomedical text mining",
abstract = "Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results: We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.",
author = "Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and So, {Chan Ho} and Jaewoo Kang",
year = "2020",
month = feb,
day = "15",
doi = "10.1093/bioinformatics/btz682",
language = "English",
volume = "36",
pages = "1234--1240",
journal = "Bioinformatics",
issn = "1367-4803",
publisher = "Oxford University Press",
number = "4",
}
@misc{karpukhin2020dense,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Touvron2023Llama2O,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Daniel M. Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony S. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. V. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year={2023}
}

@misc{Promptify2022,
  title = {Promptify: Structured Output from LLMs},
  author = {Pal, Ankit},
  year = {2022},
  howpublished = {\url{https://github.com/promptslab/Promptify}},
  note = {Prompt-Engineering components for NLP tasks in Python}
}