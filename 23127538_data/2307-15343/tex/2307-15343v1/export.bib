@generic{Gu2020,
   author = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
   title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
   year = {2020},
}
@generic{Hermann2015,
   author = {Karl Moritz Hermann and Tomáš Kočiský and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman and Phil Blunsom},
   title = {Teaching Machines to Read and Comprehend},
   year = {2015},
}
@article{Singhal2023TowardsEM,
  title={Towards Expert-Level Medical Question Answering with Large Language Models},
  author={K. Singhal and Tao Tu and Juraj Gottweis},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.09617}
}
@misc{Promptify2022,
  title = {Promptify: Structured Output from LLMs},
  author = {Pal, Ankit},
  year = {2022},
  howpublished = {\url{https://github.com/promptslab/Promptify}},
  note = {Prompt-Engineering components for NLP tasks in Python}
}
@inproceedings{,
   abstract = {We present COVID-QA, a Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19. To evaluate the dataset we compared a RoBERTa base model fine-tuned on SQuAD with the same model trained on SQuAD and our COVID-QA dataset. We found that the additional training on this domain-specific data leads to significant gains in performance. Both the trained model and the annotated dataset have been open-sourced at: https://github.com/deepset-ai/COVID-QA},
   author = {Timo Möller and Anthony Reina and Raghavan Jayakumar and Malte Pietsch},
   city = {Online},
   journal = {Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020},
   month = {7},
   publisher = {Association for Computational Linguistics},
   title = {COVID-QA: A Question Answering Dataset for COVID-19},
   url = {https://www.aclweb.org/anthology/2020.nlpcovid19-acl.18},
   year = {2020},
}
@inproceedings{Yang2015,
   author = {Yi Yang and Wen-tau Yih and Christopher Meek},
   city = {Lisbon, Portugal},
   doi = {10.18653/v1/D15-1237},
   journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
   month = {9},
   pages = {2013-2018},
   publisher = {Association for Computational Linguistics},
   title = {WikiQA: A Challenge Dataset for Open-Domain Question Answering},
   url = {https://www.aclweb.org/anthology/D15-1237},
   year = {2015},
}
@generic{clicr,
   author = {Simon Šuster and Walter Daelemans},
   title = {CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension},
   year = {2018},
}
@generic{Huang2019,
   author = {Lifu Huang and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
   title = {Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning},
   year = {2019},
}
@generic{Bajaj2018,
   author = {Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara and Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
   title = {MS MARCO: A Human Generated MAchine Reading COmprehension Dataset},
   year = {2018},
}
@inproceedings{Dua2019,
   author = {Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},
   journal = {Proc. of NAACL},
   title = {  DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
   year = {2019},
}
@generic{Yagcioglu2018,
   author = {Semih Yagcioglu and Aykut Erdem and Erkut Erdem and Nazli Ikizler-Cinbis},
   title = {RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes},
   year = {2018},
}
@generic{Zellers2018,
   author = {Rowan Zellers and Yonatan Bisk and Roy Schwartz and Yejin Choi},
   title = {SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference},
   year = {2018},
}
@generic{Lai2017,
   author = {Guokun Lai and Qizhe Xie and Hanxiao Liu and Yiming Yang and Eduard Hovy},
   title = {RACE: Large-scale ReAding Comprehension Dataset From Examinations},
   year = {2017},
}
@generic{Yang2018,
   author = {Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W Cohen and Ruslan Salakhutdinov and Christopher D Manning},
   title = {HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
   year = {2018},
}
@generic{Joshi2017,
   author = {Mandar Joshi and Eunsol Choi and Daniel S Weld and Luke Zettlemoyer},
   title = {TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
   year = {2017},
}

@generic{Trischler2017,
   author = {Adam Trischler and Tong Wang and Xingdi Yuan and Justin Harris and Alessandro Sordoni and Philip Bachman and Kaheer Suleman},
   title = {NewsQA: A Machine Comprehension Dataset},
   year = {2017},
}

@generic{Reddy2019,
   author = {Siva Reddy and Danqi Chen and Christopher D Manning},
   title = {CoQA: A Conversational Question Answering Challenge},
   year = {2019},
}

@article{Kwiatkowski2019,
   author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
   journal = {Transactions of the Association of Computational Linguistics},
   title = {Natural Questions: a Benchmark for Question Answering Research},
   year = {2019},
}

@article{Clark2018,
   author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
   title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
   year = {2018},}

@inproceedings{mediqa,
   abstract = {This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98% in the NLI task, 74.9% in the RQE task, and 78.3% in the QA task. In this paper, we describe the tasks, the datasets, and the participants' approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain.},
   author = {Asma Ben Abacha and Chaitanya Shivade and Dina Demner-Fushman},
   city = {Florence, Italy},
   doi = {10.18653/v1/W19-5039},
   journal = {Proceedings of the 18th BioNLP Workshop and Shared Task},
   month = {8},
   pages = {370-379},
   publisher = {Association for Computational Linguistics},
   title = {Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering},
   url = {https://www.aclweb.org/anthology/W19-5039},
   year = {2019},
}
@generic{Zhang2018,
   author = {Xiao Zhang and Ji Wu and Zhiyang He and Xien Liu and Ying Su},
   title = {Medical Exam Question Answering with Large-scale Reading Comprehension},
   year = {2018},
}
@generic{Jin2019,
   author = {Qiao Jin and Bhuwan Dhingra and Zhengping Liu and William W Cohen and Xinghua Lu},
   title = {PubMedQA: A Dataset for Biomedical Research Question Answering},
   year = {2019},
}
@article{Li2023HaluEvalAL,
  title={HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
  author={Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jianyun Nie and Ji-rong Wen},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.11747}
}
@article{Jin2020WhatDD,
  title={What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
  author={Di Jin and Eileen Pan and Nassim Oufattole and Wei-Hung Weng and Hanyi Fang and Peter Szolovits},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.13081}
}
@article{Vilares2019HEADQAAH,
  title={HEAD-QA: A Healthcare Dataset for Complex Reasoning},
  author={David Vilares and Carlos G{\'o}mez-Rodr{\'i}guez},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.04701}
}
@inproceedings{Vilares2019,
   abstract = {We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.},
   author = {David Vilares and Carlos Gómez-Rodr\'\iguez},
   city = {Florence, Italy},
   doi = {10.18653/v1/P19-1092},
   journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
   month = {7},
   pages = {960-966},
   publisher = {Association for Computational Linguistics},
   title = {HEAD-QA: A Healthcare Dataset for Complex Reasoning},
   url = {https://www.aclweb.org/anthology/P19-1092},
   year = {2019},
}
@generic{Chen2017,
   author = {Danqi Chen and Adam Fisch and Jason Weston and Antoine Bordes},
   title = {Reading Wikipedia to Answer Open-Domain Questions},
   year = {2017},
}
@inproceedings{Wolf2020,
   abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
   author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Remi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander Rush},
   city = {Online},
   doi = {10.18653/v1/2020.emnlp-demos.6},
   journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
   month = {10},
   pages = {38-45},
   publisher = {Association for Computational Linguistics},
   title = {Transformers: State-of-the-Art Natural Language Processing},
   url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
   year = {2020},
}
@inproceedings{Rajpurkar2016,
   author = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
   city = {Austin, Texas},
   doi = {10.18653/v1/D16-1264},
   journal = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
   month = {11},
   pages = {2383-2392},
   publisher = {Association for Computational Linguistics},
   title = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
   url = {https://www.aclweb.org/anthology/D16-1264},
   year = {2016},
}
@generic{Gu2020,
   author = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
   title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
   year = {2020},
}
@inproceedings{Beltagy2019,
   abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
   author = {Iz Beltagy and Kyle Lo and Arman Cohan},
   city = {Hong Kong, China},
   doi = {10.18653/v1/D19-1371},
   journal = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
   month = {11},
   pages = {3615-3620},
   publisher = {Association for Computational Linguistics},
   title = {SciBERT: A Pretrained Language Model for Scientific Text},
   url = {https://www.aclweb.org/anthology/D19-1371},
   year = {2019},
}
@inproceedings{Devlin2019,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
   city = {Minneapolis, Minnesota},
   doi = {10.18653/v1/N19-1423},
   journal = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
   month = {6},
   pages = {4171-4186},
   publisher = {Association for Computational Linguistics},
   title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
   url = {https://www.aclweb.org/anthology/N19-1423},
   year = {2019},
}

%------

@article{jin2020disease,
  title={What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={arXiv preprint arXiv:2009.13081},
  year={2020}
}


@InProceedings{pmlr-v174-pal22a,
  title = 	 {MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
  author =       {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle = 	 {Proceedings of the Conference on Health, Inference, and Learning},
  pages = 	 {248--260},
  year = 	 {2022},
  editor = 	 {Flores, Gerardo and Chen, George H and Pollard, Tom and Ho, Joyce C and Naumann, Tristan},
  volume = 	 {174},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--08 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v174/pal22a/pal22a.pdf},
  url = 	 {https://proceedings.mlr.press/v174/pal22a.html},
  abstract = 	 {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects & topics. A detailed explanation of the solution, along with the above information, is provided in this study.}
}

@article{shi2023large,
  title={Large Language Models Can Be Easily Distracted by Irrelevant Context},
  author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Schärli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2302.00093},
  year={2023}
}


@article{Singhal2022LargeLM,
  title={Large Language Models Encode Clinical Knowledge},
  author={K. Singhal and Shekoofeh Azizi and Tao Tu},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.13138}
}

@article{DBLP:journals/corr/abs-2104-07567,
  author       = {Kurt Shuster and
                  Spencer Poff and
                  Moya Chen and
                  Douwe Kiela and
                  Jason Weston},
  title        = {Retrieval Augmentation Reduces Hallucination in Conversation},
  journal      = {CoRR},
  volume       = {abs/2104.07567},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.07567},
  eprinttype    = {arXiv},
  eprint       = {2104.07567},
  timestamp    = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-07567.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{agrawal2022large,
      title={Large Language Models are Few-Shot Clinical Information Extractors}, 
      author={Monica Agrawal and Stefan Hegselmann and Hunter Lang and Yoon Kim and David Sontag},
      year={2022},
      eprint={2205.12689},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{Sezgin2022-cx,
  title     = "Operationalizing and implementing pretrained, large artificial
               intelligence linguistic models in the {US} health care system:
               Outlook of generative pretrained transformer 3 ({GPT-3}) as a
               service model",
  author    = "Sezgin, Emre and Sirrianni, Joseph and Linwood, Simon L",
  abstract  = "Generative pretrained transformer models have been popular
               recently due to their enhanced capabilities and performance. In
               contrast to many existing artificial intelligence models,
               generative pretrained transformer models can perform with very
               limited training data. Generative pretrained transformer 3
               (GPT-3) is one of the latest releases in this pipeline,
               demonstrating human-like logical and intellectual responses to
               prompts. Some examples include writing essays, answering complex
               questions, matching pronouns to their nouns, and conducting
               sentiment analyses. However, questions remain with regard to its
               implementation in health care, specifically in terms of
               operationalization and its use in clinical practice and
               research. In this viewpoint paper, we briefly introduce GPT-3
               and its capabilities and outline considerations for its
               implementation and operationalization in clinical practice
               through a use case. The implementation considerations include
               (1) processing needs and information systems infrastructure, (2)
               operating costs, (3) model biases, and (4) evaluation metrics.
               In addition, we outline the following three major operational
               factors that drive the adoption of GPT-3 in the US health care
               system: (1) ensuring Health Insurance Portability and
               Accountability Act compliance, (2) building trust with health
               care providers, and (3) establishing broader access to the GPT-3
               tools. This viewpoint can inform health care practitioners,
               developers, clinicians, and decision makers toward understanding
               the use of the powerful artificial intelligence tools integrated
               into hospital systems and health care.",
  journal   = "JMIR Med. Inform.",
  publisher = "JMIR Publications Inc.",
  volume    =  10,
  number    =  2,
  pages     = "e32875",
  month     =  feb,
  year      =  2022,
  keywords  = "artificial intelligence; chatbot; clinical informatics;
               generative pretrained transformer; natural language processing",
  language  = "en"
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  booktitle ={},
  year={2019}
}

@article{DBLP:journals/corr/abs-2109-01652,
  author       = {Jason Wei and
                  Maarten Bosma and
                  Vincent Y. Zhao and
                  Kelvin Guu and
                  Adams Wei Yu and
                  Brian Lester and
                  Nan Du and
                  Andrew M. Dai and
                  Quoc V. Le},
  title        = {Finetuned Language Models Are Zero-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2109.01652},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.01652},
  eprinttype    = {arXiv},
  eprint       = {2109.01652},
  timestamp    = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-01652.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{bai2022training,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2021token,
  title={A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation},
  author={Liu, Tianyu and Zhang, Yizhe and Brockett, Chris and Mao, Yi and Sui, Zhifang and Chen, Weizhu and Dolan, Bill},
  journal={arXiv preprint arXiv:2104.08704},
  year={2021}
}

@misc{penedo2023refinedweb,
      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, 
      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
      year={2023},
      eprint={2306.01116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{refinedweb,
  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
  journal={arXiv preprint arXiv:2306.01116},
  eprint={2306.01116},
  eprinttype = {arXiv},
  url={https://arxiv.org/abs/2306.01116},
  year={2023}
}
@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}
@article{Paszke2019PyTorchAI,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas K{\"o}pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.01703}
}

@online{MosaicML2023Introducing,
    author    = {MosaicML},
    title     = {Introducing MPT-30B: Raising the bar
for open-source foundation models},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-30b},
    note      = {Accessed: 2023-06-22},
    urldate   = {2023-06-22}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{li2023chatdoctor,
  title={ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge},
  author={Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
  journal={Cureus},
  volume={15},
  number={6},
  year={2023},
  publisher={Cureus}
}

@article{han2023medalpaca,
  title={MedAlpaca--An Open-Source Collection of Medical Conversational AI Models and Training Data},
  author={Han, Tianyu and Adams, Lisa C and Papaioannou, Jens-Michalis and Grundmann, Paul and Oberhauser, Tom and L{\"o}ser, Alexander and Truhn, Daniel and Bressem, Keno K},
  journal={arXiv preprint arXiv:2304.08247},
  year={2023}
}

@ARTICLE{Gilson2023-pg,
  title    = "How does {ChatGPT} perform on the United States medical licensing
              examination? The implications of large language models for
              medical education and knowledge assessment",
  author   = "Gilson, Aidan and Safranek, Conrad W and Huang, Thomas and
              Socrates, Vimig and Chi, Ling and Taylor, Richard Andrew and
              Chartash, David",
  abstract = "BACKGROUND: Chat Generative Pre-trained Transformer (ChatGPT) is
              a 175-billion-parameter natural language processing model that
              can generate conversation-style responses to user input.
              OBJECTIVE: This study aimed to evaluate the performance of
              ChatGPT on questions within the scope of the United States
              Medical Licensing Examination Step 1 and Step 2 exams, as well as
              to analyze responses for user interpretability. METHODS: We used
              2 sets of multiple-choice questions to evaluate ChatGPT's
              performance, each with questions pertaining to Step 1 and Step 2.
              The first set was derived from AMBOSS, a commonly used question
              bank for medical students, which also provides statistics on
              question difficulty and the performance on an exam relative to
              the user base. The second set was the National Board of Medical
              Examiners (NBME) free 120 questions. ChatGPT's performance was
              compared to 2 other large language models, GPT-3 and InstructGPT.
              The text output of each ChatGPT response was evaluated across 3
              qualitative metrics: logical justification of the answer
              selected, presence of information internal to the question, and
              presence of information external to the question. RESULTS: Of the
              4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and
              NBME-Free-Step2, ChatGPT achieved accuracies of 44\% (44/100),
              42\% (42/100), 64.4\% (56/87), and 57.8\% (59/102), respectively.
              ChatGPT outperformed InstructGPT by 8.15\% on average across all
              data sets, and GPT-3 performed similarly to random chance. The
              model demonstrated a significant decrease in performance as
              question difficulty increased (P=.01) within the AMBOSS-Step1
              data set. We found that logical justification for ChatGPT's
              answer selection was present in 100\% of outputs of the NBME data
              sets. Internal information to the question was present in 96.8\%
              (183/189) of all questions. The presence of information external
              to the question was 44.5\% and 27\% lower for incorrect answers
              relative to correct answers on the NBME-Free-Step1 (P<.001) and
              NBME-Free-Step2 (P=.001) data sets, respectively. CONCLUSIONS:
              ChatGPT marks a significant improvement in natural language
              processing models on the tasks of medical question answering. By
              performing at a greater than 60\% threshold on the
              NBME-Free-Step-1 data set, we show that the model achieves the
              equivalent of a passing score for a third-year medical student.
              Additionally, we highlight ChatGPT's capacity to provide logic
              and informational context across the majority of answers. These
              facts taken together make a compelling case for the potential
              applications of ChatGPT as an interactive medical education tool
              to support learning.",
  journal  = "JMIR Med. Educ.",
  volume   =  9,
  pages    = "e45312",
  month    =  feb,
  year     =  2023,
  keywords = "ChatGPT; GPT; MedQA; NLP; artificial intelligence; chatbot;
              conversational agent; education technology; generative
              pre-trained transformer; machine learning; medical education;
              natural language processing",
  language = "en"
}



@misc{https://doi.org/10.48550/arxiv.2207.08143,
  doi = {10.48550/ARXIV.2207.08143},
  url = {https://arxiv.org/abs/2207.08143},
  author = {Liévin, Valentin and Hother, Christoffer Egeberg and Winther, Ole},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.1; I.2.7},
  title = {Can large language models reason about medical questions?},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Bang2023AMM,
  title={A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity},
  author={Yejin Bang and Samuel Cahyawijaya and Nayeon Lee and Wenliang Dai and Dan Su and Bryan Wilie and Holy Lovenia and Ziwei Ji and Tiezheng Yu and Willy Chung and Quyet V. Do and Yan Xu and Pascale Fung},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.04023}
}

@article{Ji2022SurveyOH,
  title={Survey of Hallucination in Natural Language Generation},
  author={Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Yejin Bang and Wenliang Dai and Andrea Madotto and Pascale Fung},
  journal={ACM Computing Surveys},
  year={2022},
  volume={55},
  pages={1 - 38}
}

@ARTICLE{Lu2023-lc,
  title    = "Artificial intelligence in intensive care medicine: Toward a
              {ChatGPT/GPT-4} way?",
  author   = "Lu, Yanqiu and Wu, Haiyang and Qi, Shaoyan and Cheng, Kunming",
  abstract = "Although intensive care medicine (ICM) is a relatively young
              discipline, it has rapidly developed into a full-fledged and
              highly specialized specialty covering several fields of medicine.
              The COVID-19 pandemic led to a surge in intensive care unit
              demand and also bring unprecedented development opportunities for
              this area. Multiple new technologies such as artificial
              intelligence (AI) and machine learning (ML) were gradually being
              applied in this field. In this study, through an online survey,
              we have summarized the potential uses of ChatGPT/GPT-4 in ICM
              range from knowledge augmentation, device management, clinical
              decision-making support, early warning systems, and establishment
              of intensive care unit (ICU) database.",
  journal  = "Ann. Biomed. Eng.",
  month    =  may,
  year     =  2023,
  keywords = "Artificial intelligence; Biomedical engineering; Chatbot; GPT-4;
              New era",
  language = "en"
}

@misc{manyoso2023haltt4llm,
  author = {manyoso},
  title = {haltt4llm},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/manyoso/haltt4llm}
}

@misc{selfinstruct,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@misc{gudibande2023false,
      title={The False Promise of Imitating Proprietary LLMs}, 
      author={Arnav Gudibande and Eric Wallace and Charlie Snell and Xinyang Geng and Hao Liu and Pieter Abbeel and Sergey Levine and Dawn Song},
      year={2023},
      eprint={2305.15717},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-2005-14165,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1904-09751,
  author       = {Ari Holtzman and
                  Jan Buys and
                  Maxwell Forbes and
                  Yejin Choi},
  title        = {The Curious Case of Neural Text Degeneration},
  journal      = {CoRR},
  volume       = {abs/1904.09751},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.09751},
  eprinttype    = {arXiv},
  eprint       = {1904.09751},
  timestamp    = {Sat, 29 Apr 2023 10:09:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09751.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

