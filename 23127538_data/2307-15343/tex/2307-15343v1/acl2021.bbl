\begin{thebibliography}{26}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Agrawal et~al.(2022)Agrawal, Hegselmann, Lang, Kim, and
  Sontag}]{agrawal2022large}
Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag.
  2022.
\newblock \href {http://arxiv.org/abs/2205.12689} {Large language models are
  few-shot clinical information extractors}.

\bibitem[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain,
  Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, El-Showk,
  Elhage, Hatfield-Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda,
  Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and
  Kaplan}]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,
  Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,
  Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
  Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,
  Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022.
\newblock \href {http://arxiv.org/abs/2204.05862} {Training a helpful and
  harmless assistant with reinforcement learning from human feedback}.

\bibitem[{Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji,
  Yu, Chung, Do, Xu, and Fung}]{Bang2023AMM}
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie,
  Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet~V. Do, Yan Xu, and
  Pascale Fung. 2023.
\newblock A multitask, multilingual, multimodal evaluation of chatgpt on
  reasoning, hallucination, and interactivity.
\newblock \emph{ArXiv}, abs/2302.04023.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{DBLP:journals/corr/abs-2005-14165}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei. 2020.
\newblock \href {http://arxiv.org/abs/2005.14165} {Language models are few-shot
  learners}.
\newblock \emph{CoRR}, abs/2005.14165.

\bibitem[{Han et~al.(2023)Han, Adams, Papaioannou, Grundmann, Oberhauser,
  L{\"o}ser, Truhn, and Bressem}]{han2023medalpaca}
Tianyu Han, Lisa~C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom
  Oberhauser, Alexander L{\"o}ser, Daniel Truhn, and Keno~K Bressem. 2023.
\newblock Medalpaca--an open-source collection of medical conversational ai
  models and training data.
\newblock \emph{arXiv preprint arXiv:2304.08247}.

\bibitem[{Holtzman et~al.(2019)Holtzman, Buys, Forbes, and
  Choi}]{DBLP:journals/corr/abs-1904-09751}
Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019.
\newblock \href {http://arxiv.org/abs/1904.09751} {The curious case of neural
  text degeneration}.
\newblock \emph{CoRR}, abs/1904.09751.

\bibitem[{Ji et~al.(2022)Ji, Lee, Frieske, Yu, Su, Xu, Ishii, Bang, Dai,
  Madotto, and Fung}]{Ji2022SurveyOH}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
  Yejin Bang, Wenliang Dai, Andrea Madotto, and Pascale Fung. 2022.
\newblock Survey of hallucination in natural language generation.
\newblock \emph{ACM Computing Surveys}, 55:1 -- 38.

\bibitem[{Jin et~al.(2020)Jin, Pan, Oufattole, Weng, Fang, and
  Szolovits}]{Jin2020WhatDD}
Di~Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter
  Szolovits. 2020.
\newblock What disease does this patient have? a large-scale open domain
  question answering dataset from medical exams.
\newblock \emph{ArXiv}, abs/2009.13081.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Cheng, Zhao, Nie, and rong
  Wen}]{Li2023HaluEvalAL}
Junyi Li, Xiaoxue Cheng, Wayne~Xin Zhao, Jianyun Nie, and Ji~rong Wen.
  2023{\natexlab{a}}.
\newblock Halueval: A large-scale hallucination evaluation benchmark for large
  language models.
\newblock \emph{ArXiv}, abs/2305.11747.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Li, Zhang, Dan, Jiang, and
  Zhang}]{li2023chatdoctor}
Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang.
  2023{\natexlab{b}}.
\newblock Chatdoctor: A medical chat model fine-tuned on a large language model
  meta-ai (llama) using medical domain knowledge.
\newblock \emph{Cureus}, 15(6).

\bibitem[{Liu et~al.(2021)Liu, Zhang, Brockett, Mao, Sui, Chen, and
  Dolan}]{liu2021token}
Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi~Mao, Zhifang Sui, Weizhu Chen, and
  Bill Dolan. 2021.
\newblock A token-level reference-free hallucination detection benchmark for
  free-form text generation.
\newblock \emph{arXiv preprint arXiv:2104.08704}.

\bibitem[{MosaicML(2023)}]{MosaicML2023Introducing}
MosaicML. 2023.
\newblock \href {www.mosaicml.com/blog/mpt-30b} {Introducing mpt-30b: Raising
  the bar for open-source foundation models}.
\newblock Accessed: 2023-06-22.

\bibitem[{Pal(2022)}]{Promptify2022}
Ankit Pal. 2022.
\newblock Promptify: Structured output from llms.
\newblock \url{https://github.com/promptslab/Promptify}.
\newblock Prompt-Engineering components for NLP tasks in Python.

\bibitem[{Pal et~al.(2022)Pal, Umapathi, and Sankarasubbu}]{pmlr-v174-pal22a}
Ankit Pal, Logesh~Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.
\newblock \href {https://proceedings.mlr.press/v174/pal22a.html} {Medmcqa: A
  large-scale multi-subject multi-choice dataset for medical domain question
  answering}.
\newblock In \emph{Proceedings of the Conference on Health, Inference, and
  Learning}, volume 174 of \emph{Proceedings of Machine Learning Research},
  pages 248--260. PMLR.

\bibitem[{Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"o}pf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}]{Paszke2019PyTorchAI}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas K{\"o}pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala. 2019.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{ArXiv}, abs/1912.01703.

\bibitem[{Penedo et~al.(2023{\natexlab{a}})Penedo, Malartic, Hesslow, Cojocaru,
  Cappelli, Alobeidli, Pannier, Almazrouei, and Launay}]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
  Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
  and Julien Launay. 2023{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2306.01116} {The refinedweb dataset for
  falcon llm: Outperforming curated corpora with web data, and web data only}.

\bibitem[{Penedo et~al.(2023{\natexlab{b}})Penedo, Malartic, Hesslow, Cojocaru,
  Cappelli, Alobeidli, Pannier, Almazrouei, and Launay}]{refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
  Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
  and Julien Launay. 2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2306.01116} {The {R}efined{W}eb dataset
  for {F}alcon {LLM}: outperforming curated corpora with web data, and web data
  only}.
\newblock \emph{arXiv preprint arXiv:2306.01116}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{Radford2019LanguageMA}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Shuster et~al.(2021)Shuster, Poff, Chen, Kiela, and
  Weston}]{DBLP:journals/corr/abs-2104-07567}
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.
\newblock \href {http://arxiv.org/abs/2104.07567} {Retrieval augmentation
  reduces hallucination in conversation}.
\newblock \emph{CoRR}, abs/2104.07567.

\bibitem[{Singhal et~al.(2022)Singhal, Azizi, and Tu}]{Singhal2022LargeLM}
K.~Singhal, Shekoofeh Azizi, and Tao Tu. 2022.
\newblock Large language models encode clinical knowledge.
\newblock \emph{ArXiv}, abs/2212.13138.

\bibitem[{Singhal et~al.(2023)Singhal, Tu, and Gottweis}]{Singhal2023TowardsEM}
K.~Singhal, Tao Tu, and Juraj Gottweis. 2023.
\newblock Towards expert-level medical question answering with large language
  models.
\newblock \emph{ArXiv}, abs/2305.09617.

\bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,
  Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen,
  Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn,
  Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura,
  Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra,
  Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith,
  Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan,
  Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
  Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.
\newblock \href {http://arxiv.org/abs/2307.09288} {Llama 2: Open foundation and
  fine-tuned chat models}.

\bibitem[{Vilares and G{\'o}mez-Rodr{\'i}guez(2019)}]{Vilares2019HEADQAAH}
David Vilares and Carlos G{\'o}mez-Rodr{\'i}guez. 2019.
\newblock Head-qa: A healthcare dataset for complex reasoning.
\newblock \emph{ArXiv}, abs/1906.04701.

\bibitem[{Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and
  Hajishirzi}]{selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi. 2022.
\newblock Self-instruct: Aligning language model with self generated
  instructions.

\bibitem[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le}]{DBLP:journals/corr/abs-2109-01652}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le. 2021.
\newblock \href {http://arxiv.org/abs/2109.01652} {Finetuned language models
  are zero-shot learners}.
\newblock \emph{CoRR}, abs/2109.01652.

\bibitem[{Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, and Brew}]{Wolf2019HuggingFacesTS}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, and
  Jamie Brew. 2019.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{ArXiv}, abs/1910.03771.

\end{thebibliography}
