61\section{Experimental Results}
\subsection{Setup}
\noindent\textbf{Datasets.}
We utilize the AVSBench dataset~\cite{zhou_AVSBench_ECCV_2022}, which consists of 5,356 audio-video pairs with pixel-wise annotations.
% for conducting our experiments. 
Each audio-video pair in the dataset spans 5 seconds, and we trim the video to include five consecutive frames by extracting the video frame at the end of each second. 
The AVSBench dataset is further divided into two subsets: semi-supervised Single Sound Source Segmentation (S4), where only the first frame is labeled, and fully supervised Multiple Sound Source Segmentation (MS3), where all frames are labeled. 
The S4 subset contains 4,922 videos, while the MS3 subset contains 424 videos. 
For training and testing, we follow the conventional splitting from the AVSBench dataset~\cite{zhou_AVSBench_ECCV_2022} and perform training and testing with S4 and MS3, respectively.
% and We train our model using the training set and evaluate its performance on the testing set.



\noindent\textbf{Evaluation Metrics.}
We assess the audio-visual segmentation performance using the same evaluation metrics as AVSBench~\cite{zhou_AVSBench_ECCV_2022}, namely Mean Intersection over Union (mIoU) and F-score. 
The F-score is formulated as follows: $F_{\beta}=\frac{(1+\beta^2 \times \text{precision} \times \text{recall})}{\beta^2\times \text{precision} + \text{recall}}, \beta^2=0.3$. 
Here, both precision and recall are computed based on a binary segmentation map, which is obtained by applying 256 uniformly distributed binarization thresholds in the range $[0, 255]$.



\noindent\textbf{Compared Methods.} We compare our method with published AVS methods, including AVSBench~\cite{zhou_AVSBench_ECCV_2022}, AVS-BiGen~\cite{hao_aaai_2024_avsbg}, ECMVAE~\cite{mao_iccv_2023_ecmvae}, CATR~\cite{li_catr_acmmm_2023}, CMMS~\cite{liu_avs_acmmm_2023} \Fix{and AVSegFormer~\cite{gao2024avsegformer}}.
To strictly keep accordance with the settings in previous work~\cite{zhou_AVSBench_ECCV_2022}, we also compare the performance with related segmentation tasks, such as video foreground segmentation models (VOS)~\cite{mahadevan_3DC_VOS_2020, duke_sstvos_cvpr_2021}, RGB image based salient object detection models~\cite{mao_transformerSOD_2021, zhang_ebm_sod_nips_2021}.
We set up the comparison due to the binary video segmentation nature of AVS. 
Being consistent with AVSBench, we also use two backbones, ResNet50~\cite{he_resnet_cvpr_2016} and PVT~\cite{wang_Pvtv2_CVM_2022} initialized with ImageNet~\cite{deng_imagenet_cvpr_2009} per-trained weights, to demonstrate that our proposed model achieves consistent performance improvement under different backbones.
\Fixtwo{For a fair comparison, we establish consistent experimental protocols across methods. Regarding CATR, their paper presents two experimental settings: (1) a baseline setting that maintains consistency with the training protocol of AVSBench, and (2) an enhanced setting utilizing additional AOT-enhanced annotations~\cite{yang2021associating_aot}. To ensure fair comparison, we specifically reference their results from the baseline setting. Similarly, AVSegFormer presents two training configurations: (1) a standard setting with $224\times 224$ input resolution that aligns with the configuration of AVSBench, and (2) an enhanced setting with $512\times 512$ resolution that achieves better performance through increased input size. To maintain consistent experimental protocols, we specifically compare with their $224\times 224$ configuration results, ensuring architectural comparisons are conducted under equivalent conditions.}

\noindent\textbf{Implementation Details.}
Our proposed method is trained end-to-end using the Adam optimizer~\cite{Kingma_Adam_ICLR_2015} with default hyper-parameters for 15 and 30 epochs on the S4 and MS3 subsets. The learning rate is set to $10^{-4}$ and the batch size is 4. All the video frames are resized to the shape of $224\times 224$. 
For the latent diffusion model, we use the cosine noise schedule and the noise prediction objective in \equref{ddpm_loss} for all experiments. The diffusion steps $K$ is set as 20. To accelerate sampling, we use the DDIM~\cite{song_DDIM_ICLR_2020} with 10 sampling steps.


\begin{table}[t]
    \caption{\textbf{Quantitative results on the AVSBench dataset} in terms of mIOU and F-score under S4 and MS3 settings. 
    We both report the performance with R50 and PVT as a backbone for the results of comparison methods and Ours.
    \Fix{* denotes that the training datasets are supplemented annotation with AOT~\cite{yang2021associating_aot}.
    For AVSegFormer~\cite{gao2024avsegformer}, we only report the performance when trained at the common $224\times 224$ resolution.}}
    \label{tab:main_results_on_avsbench}
    \centering
    \small
    \setlength{\tabcolsep}{1.0mm}
      \renewcommand{\arraystretch}{1.3}
    {
        \begin{threeparttable}
        \begin{tabular}{cccccc}
        \toprule[1.1pt]
        & \multirow{2}{*}{Methods} & \multicolumn{2}{c}{S4}                      & \multicolumn{2}{c}{MS3}                     \\
        \cmidrule(r){3-4}  \cmidrule(r){5-6}
                     &         & mIoU                 & F-score              & mIoU                 & F-score   \\
        \midrule
        \multirow{2}{*}{VOS}& 3DC~\cite{mahadevan_3DC_VOS_2020}    & 57.10   & 0.759   & 36.92   & 0.503    \\
        & SST~\cite{duke_sstvos_cvpr_2021}                        & 66.29   & 0.801   & 42.57   & 0.572    \\
        \midrule
        \multirow{2}{*}{SOD}& iGAN~\cite{mao_transformerSOD_2021}   & 61.59   & 0.778   & 42.89   & 0.544    \\
        & LGVT~\cite{zhang_ebm_sod_nips_2021}                       & 74.94   & 0.873   & 40.71   & 0.593    \\
        \midrule
        & AVSBench (R50)~\cite{zhou_AVSBench_ECCV_2022}   & 72.79   & 0.848   & 47.88   & 0.578    \\
        & AVSBench (PVT)~\cite{zhou_AVSBench_ECCV_2022}   & 78.74   & 0.879   & 54.00   & 0.645    \\
        & AVS-BiGen (R50)~\cite{hao_aaai_2024_avsbg}   & 74.13   & 0.854   & 44.95   & 0.568 \\
        & AVS-BiGen (PVT)~\cite{hao_aaai_2024_avsbg}   & 81.71   & 0.904   & 55.10   & 0.668 \\
        & ECMVAE (R50)~\cite{mao_iccv_2023_ecmvae}     & 76.33   & 0.865   & 48.69   & 0.607  \\
        \multirow{2}{*}{AVS}& ECMVAE (PVT)~\cite{mao_iccv_2023_ecmvae}     & 81.74   & 0.901   & 57.84   & 0.708  \\
        & CATR (R50)~\cite{li_catr_acmmm_2023}         & 74.8    & 0.866   & 52.8    & 0.653           \\
        & CATR (PVT)~\cite{li_catr_acmmm_2023}         & 81.4    & 0.896   & 59.0    & 0.700  \\
        & \Fix{CATR (R50)*~\cite{li_catr_acmmm_2023}}  & \Fix{74.9}    & \Fix{0.871}   & \Fix{53.1}    & \Fix{0.656}           \\
        & \Fix{CATR (PVT)*~\cite{li_catr_acmmm_2023}}  & \Fix{84.4}    & \Fix{0.913}   & \Fix{62.7}    & \Fix{0.745}  \\
        & CMMS~\cite{liu_avs_acmmm_2023}               & 81.29   & 0.886   & 59.5    & 0.657           \\
        & \Fix{AVSegFormer (R50)~\cite{gao2024avsegformer}}  & \Fix{76.45} & \Fix{0.859} & \Fix{49.53} & \Fix{0.628}           \\
        & \Fix{AVSegFormer (PVT)~\cite{gao2024avsegformer}}  & \Fix{\textbf{82.06}} & \Fix{0.899} & \Fix{58.36} & \Fix{0.693}   \\ 
        & Ours (R50)               & 75.80          & 0.869          & 49.77          & 0.621           \\
        & Ours (PVT)               & 81.51 & \textbf{0.903} & \textbf{59.62} & \textbf{0.712}  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-5.0mm}
\end{table}

% Figure environment removed




\subsection{Performance Comparison}
% \Jing{I'm here}
\noindent\textbf{Quantitative Comparison.} Generally, we define our task as a multimodal binary segmentation task, where the input includes both visual and audio, and the output is a binary map showing the sound producer(s). We find a related and similar setting is salient object detection, where the output is also a binary map, localizing the foreground object(s) that attract human attention. In this way, to prepare the comparison methods, we also adapt the existing state-of-the-art (SOTA) salient object detection models to our multimodal binary segmentation task and show the performance of those models in \tabref{tab:main_results_on_avsbench}, where \enquote{VOS} contains video salient object detection models, and \enquote{SOD} lists the SOTA salient object detection models.
Based on the quantitative results obtained from \tabref{tab:main_results_on_avsbench}, we observe that direct adaptation of salient object detection models to AVS fails to achieve reasonable performance. The main reason is that although both salient object detection and AVS are categorized as binary segmentation, the former relies mainly on the visual input, while the latter depends greatly on the audio modality to localize the sound producer(s).

In the \enquote{AVS} section of \tabref{tab:main_results_on_avsbench}, we show performance comparison of various methods and ours on the AVSBench dataset under different settings (S4 and MS3).
\Fix{Our method consistently outperforms state-of-the-art AVS methods on both MS3 and S4 subsets, achieving notable improvements in mIoU (59.62) and F-score (0.712).}
There is a consistent performance improvement of our proposed method compared to CATR~\cite{li_catr_acmmm_2023}, regardless of whether \enquote{R50} or \enquote{PVT} is used as the backbone. In particular, 0.11 and 0.62 higher mIOU than CATR is obtained on the two subsets with the \enquote{PVT} backbone.
Moreover, the performance of our method significantly surpasses that of ECMVAE~\cite{mao_iccv_2023_ecmvae}, an AVS method based on generative models (VAE). This comparison highlights that, despite the fact that ECMVAE employs intricate strategies involving complex multimodal latent space factorization and constraints, its capacity to model the latent space falls short in comparison to our approach utilizing a conditional latent diffusion model.
It is worth noting that our \enquote{R50} based model slightly outperforms the LGVT~\cite{zhang_ebm_sod_nips_2021} under the S4 subset, despite LGVT using a swin transformer~\cite{liu_swin_iccv_2021} backbone, while AVSBench (R50) performs worse than LGVT. 
This suggests that exploring matching relationships between visual objects and sounds is more important than using a better visual backbone for AVS tasks.
\Fixtwo{Notably, our method demonstrates superior performance over AVSegFormer~\cite{gao2024avsegformer} in three out of four metrics across both datasets. This performance advantage stems from our latent diffusion architecture and contrastive loss design, which effectively model the correlation between video and audio modalities, leading to better audio-guided segmentation results. Specifically, on the S4 dataset, while achieving higher F-score due to our strength in sounding object localization, we observe slightly lower mIoU performance. This can be attributed to the single-source characteristic of S4 dataset, where mIoU primarily reflects the refinement of segmentation boundaries rather than the accuracy of sounding object localization, which is relatively straightforward in single-source scenarios.} Despite these achievements, our model maintains a lightweight architecture where $E_{\varphi}$, $E_{\varphi}$, $D_{\tau}$, and $\epsilon_\theta$ collectively contribute only 4M parameters, resulting in a total of 94.48M parameters when incorporating the PVT backbone. \Fixtwo{This parameter count is substantially more efficient compared to AVSegFormer's 186.05M parameters and CATR's 118.38M parameters while achieving better performance.}


\Fixtwo{We further compare the performance of our model with AVSSBench~\cite{zhou2024avss}, CATR~\cite{li_catr_acmmm_2023} and AVSegFormer~\cite{gao2024avsegformer}} on the AVSBench-semantic datasets (AVSS)~\cite{zhou2024avss} dataset. 
\Fixtwo{Compared to AVSegFormer, our model demonstrates consistent improvements with absolute margins of 1.4 and 1.3 in mIoU and F-score metrics, respectively. These performance gains are particularly pronounced on complex datasets containing multiple sounding targets and rich semantic information, as shown in~\tabref{tab:avss-comparison}. This superior performance can be attributed to our model's enhanced capability in modeling audio-visual correlations using the proposed diffusion framework, which becomes more evident when handling sophisticated scenarios with diverse audio sources and semantic contexts.}
The consistent performance across multiple datasets (AVSBench-S4, MS3, and now AVSS) provides substantial evidence for the robustness and adaptability of our approach. This additional experiment reinforces our claim that recasting AVS as a conditional generation task with audio guidance offers a generalizable framework for audio-visual segmentation challenges.

\begin{table}[t]
\caption{Quantitative comparisons on AVSBench-semantic datasets (AVSS)~\cite{zhou2024avss} in terms of mIoU and F-score.}
\label{tab:avss-comparison}
\centering
\small
\setlength{\tabcolsep}{1.5mm}
\renewcommand{\arraystretch}{1.3}
\begin{threeparttable}
\begin{tabular}{ccccc}
\toprule[1.1pt]
Task & Method & Backbone & mIoU & F-score \\
\midrule
\multirow{2}{*}{VOS} & 3DC~\cite{mahadevan_3DC_VOS_2020} & R18 & 17.3 & 0.210 \\
                     & AOT~\cite{yang2021associating_aot} & R50 & 25.4 & 0.310 \\
\midrule
\multirow{4}{*}{AVSS} & AVSSBench~\cite{zhou2024avss} & PVT & 29.8 & 0.352 \\
                      & CATR~\cite{li_catr_acmmm_2023}          & PVT & 32.8 & 0.385 \\
                      & AVSegFormer~\cite{gao2024avsegformer}   & PVT & 36.7 & 0.420 \\
                      & \textbf{Ours}                           & \textbf{PVT} & \textbf{38.1} & \textbf{0.430} \\
\bottomrule[1.1pt]
\end{tabular}
\end{threeparttable}
\end{table}


\noindent\textbf{Qualitative Comparison.}
In Fig.~3, we show the qualitative comparison of our method with
\Fixtwo{AVSBench~\cite{zhou_AVSBench_ECCV_2022}, ECMVAE~\cite{mao_iccv_2023_ecmvae} and AVSegFormer~\cite{gao2024avsegformer}.
Among them, AVSBench is the baseline model, ECMVAE is also a generative AVS model similar to ours. Furthermore, AVSegFormer is the most advanced model.}
The visualization samples in Fig.~\ref{fig:main_compare} are selected from the more challenging MS3 subset.
It can be observed that our method tends to output segmentation results with finer details, \ie~an accurate segmentation of the \emph{bow of the violin} and the \emph{piano-key} in the left sample in Fig.~\ref{fig:main_compare}. 
In addition, our method also has the ability to identify the true sound producer, such as the \emph{boy} in the right sample in Fig.~\ref{fig:main_compare}, indicating a better sound localization capability.
\Fixtwo{Compared to AVSegFormer, which adopts a transformer architecture, our model incorporates audio cues explicitly via a conditional latent diffusion process. This enables more accurate localization of sounding objects, especially in complex scenes. 
As a result, AVSegFormer tends to highlight visually salient regions, whereas our model focuses more accurately on sounding objects.}


% In Fig.~\ref{fig:main_compare}, we show the qualitative comparison of our method with \Fixtwo{AVSBench~\cite{zhou_AVSBench_ECCV_2022}, ECMVAE~\cite{mao_iccv_2023_ecmvae} and AVSegFormer~\cite{gao2024avsegformer}.
% Among them, AVSBench is the baseline model, ECMVAE is also a generative AVS model similar to ours. Furthermore, AVSegFormer is the most advanced model.}
% The visualization samples in Fig.~\ref{fig:main_compare} are selected from the more challenging MS3 subset.
% It can be observed that our method tends to output segmentation results with finer details, \ie~an accurate segmentation of the \emph{bow of the violin} and the \emph{piano-key} in the left sample in Fig.~\ref{fig:main_compare}. 
% In addition, our method also has the ability to identify the true sound producer, such as the \emph{boy} in the right sample in Fig.~\ref{fig:main_compare}, indicating a better sound localization capability. 
% While the compared methods segment the two salient foreground objects, ignoring the audio information as guidance.

% % We illustrate the qualitative results under the challenge MS3 subset in Fig.~\ref{fig:main_compare}. 



\subsection{Ablation Studies}
We conduct ablation studies to analyze the effectiveness of our proposed method. All variations of the experiments are trained with the PVT backbone.

\begin{table}[!htp]
    \caption{\textbf{Ablation on the latent diffusion model.} \enquote{E-D} indicates the deterministic \enquote{encoder-decoder} structure. \enquote{CVAE} denotes using CVAE to generate the latent code. \enquote{LDM} is our proposed latent diffusion model}
    % \vspace{-2.0mm}
    \label{tab:ablation_on_ldm}
    \centering
    \small
    \setlength{\tabcolsep}{2.5mm}
    \renewcommand{\arraystretch}{1.3}
    {
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        E-D      & 78.89      & 0.881      & 54.28      & 0.648       \\
        CVAE     & 79.97      & 0.888      & 55.21      & 0.661       \\
        % \midrule
        LDM (Ours)& \textbf{81.02} & \textbf{0.894} & \textbf{57.67} & \textbf{0.698}  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}

\noindent\textbf{Ablation on Latent Diffusion Model.}
As discussed in the introduction section (Sec.~\ref{sec:intro}), a likelihood conditional generative model exactly fits our current conditional generation setting, thus a conditional variational auto-encoder~\cite{structure_output,kingma2013auto} can be a straightforward solution. 
To verify the effectiveness of our latent diffusion model, we design two baselines and show the comparison results
% As the critical component of our proposed method, to verify the impact of the latent diffusion model, we provide two baseline models and show the results
in \tabref{tab:ablation_on_ldm}. Firstly, we design
% : \textbf{1}) 
a deterministic model with a simple encoder-decoder structure (\enquote{E-D}), where the input data encoding $\{\mathbf{G}\}_{l=1}^4$ is feed directly to the prediction decoder (see Fig.~\ref{fig:model_overview}). Note that \enquote{E-D} is the same as AVSBench~\cite{zhou_AVSBench_ECCV_2022}, and we retrain it in our framework and get similar performance as the original numbers reported in their paper. 
Secondly, to explain the superiority of the diffusion model compared with other likelihood based generative models, namely conditional variational auto-encoder~\cite{structure_output} in our scenario, we follow~\cite{ucnet_sal,mao_iccv_2023_ecmvae} and design an AVS model based on CVAE (\enquote{CVAE}).
The full pipeline of the \enquote{CVAE} for the audio-visual segmentation task can be shown in Fig.~\ref{fig:model_overview_vae}.
Note that this structure can be regarded as a simplified version of ECMVAE~\cite{mao_iccv_2023_ecmvae}, which removes the complex multimodal factorization and other latent space constraints.
% \textbf{2}) a conditional variational auto-encoder (\enquote{CVAE}) following~\cite{ucnet_sal}. The structure of \enquote{E-D} is the same as AVSBench~\cite{zhou_AVSBench_ECCV_2022}, we retrain it in our framework and get similar performance as the original report in their paper. 
% CVAE~\cite{structure_output} is introduced to RGB-Depth salient object detection in~\cite{ucnet_sal}, where early fusion is used for latent feature encoding. 
We follow a similar pipeline and perform latent feature encoding based on the fused feature $\{\mathbf{G}_l\}_{l=0}^4$ instead of the early fusion feature due to our audio-visual setting, which is different from the visual-visual setting in~\cite{ucnet_sal}.
Specifically, the CVAE~\cite{structure_output} pipeline for our AVS task consists of an inference process and a generative process, where the inference process infers the latent variable $\mathbf{z}$ by $p_\theta(\mathbf{z}|\mathbf{X})$, and the generative process
% builds a latent variable $\mathbf{z}$ by $p_\theta(\mathbf{z}|\mathbf{X})$ and obtains 
produces the output via $p_\theta(\mathbf{y}|\mathbf{X},\mathbf{z})$. 


% Figure environment removed

% \footnote{We will explain the detailed structure of CVAE for the AVS task in the supplementary material.}.
% based generative model. 
Results in~\tabref{tab:ablation_on_ldm} show that generative models can improve the performance of AVS by yielding more meaningful latent space compared with the deterministic models. 
Additionally, the latent diffusion model (LDM) exhibits a more powerful latent space modeling capability than our implemented CVAE counterpart. Note that, as no latent code is involved in \enquote{E-D}, we do not perform contrastive learning. For a fair comparison, the contrastive learning objective $\mathcal{L}_\text{contrastive}$ is not involved in \enquote{CVAE} or \enquote{LDM (Ours)} either.



\begin{table}[t!]
    \caption{\textbf{Ablation on the conditional variable,} where we remove the conditional variable (\enquote{None}), or replace the conditional variable with only audio or visual representation.
    % with  We remove the audio-visual condition, the audio condition, and the visual condition as three comparison variants.
    }
    % \vspace{-2.0mm}
    \label{tab:ablation_on_audio_visual_condition}
    \centering
    \small
    \setlength{\tabcolsep}{2.4mm}
    \renewcommand{\arraystretch}{1.3}
    {
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        None       & 80.04      & 0.889      & 56.12      & 0.671       \\
        Audio      & 80.29      & 0.892      & 56.59      & 0.680       \\
        Visual     & 80.68      & 0.892      & 57.21      & 0.688       \\
        % \midrule
        Audio-Visual (Ours) & \textbf{81.02} & \textbf{0.894} & \textbf{57.67} & \textbf{0.698}  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}


\begin{table}[t!]
    \caption{\textbf{Ablation of contrastive learning.} We perform experiments without the $\mathcal{L}_\text{contrastive}$ to show its effectiveness.}
    % \vspace{-2.0mm}
    \label{tab:ablation_on_contrastive_learning}
    \centering
    \small
    \setlength{\tabcolsep}{3.4mm}
    \renewcommand{\arraystretch}{1.3}
    {
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        w/o $\mathcal{L}_\text{contrastive}$    & 81.02 & 0.894 & 57.67 & 0.698  \\
        w   $\mathcal{L}_\text{contrastive}$    & \textbf{81.51} & \textbf{0.903} & \textbf{59.62} & \textbf{0.712} \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}
\noindent\textbf{Ablation on Audio-Visual Condition.}
To further investigate the effectiveness of the audio-visual conditioning in the training process of the latent diffusion model, we train three models by incorporating different conditional variables $\mathbf{c}$, and present their performance in Table~\ref{tab:ablation_on_audio_visual_condition}. 
Initially, we remove the conditional variable, leading to unconditional generation with $p_\theta(\mathbf{z}_{k-1}|\mathbf{z}_{k})$, which is represented as \enquote{None} in the table. 
Subsequently, we consider unimodal audio or visual as only one conditional variable. 
For this purpose, we simply use the feature of each individual modality before multimodal feature concatenation (refer to $E_\psi$ in Sec.\ref{subsec_conditional_latent_diffusion}), leading to audio/visual as conditional variable based models referred to as \enquote{Audio} and \enquote{Visual} in Table~\ref{tab:ablation_on_audio_visual_condition}.
% by removing the audio condition, the visual condition, and the audio-visual condition in the latent diffusion model. 
% As shown in Table~\ref{tab:ablation_on_audio_visual_condition}, we explore four variants of the different condition types. 
Compared to unconditional generation, conditional generation can provide performance improvements, with the best results achieved when using the audio-visual condition. Furthermore, we can also observe that the performance of using visual data as the conditional variable yields superior performance compared to using audio.
We attribute this observation to two main factors. Firstly, our dataset is small and less diverse, leading to less effective audio information exploration as we pre-trained our model on a large visual image dataset. 
Secondly, the audio encoder is smaller compared with the visual encoder. More investigation will be conducted to address and balance the distribution of data.
In order to ensure a fair comparison, we opted not to perform contrastive learning in the related experiments outlined in Table~\ref{tab:ablation_on_audio_visual_condition}, similar to the ablation on the latent diffusion model.





\noindent\textbf{Ablation on Contrastive Learning.}
We introduce contrastive learning to our framework to learn the discriminative conditional variable $\mathbf{c}$. We then train our model directly without contrastive learning and show its performance as \enquote{w/o $\mathcal{L}_\text{contrastive}$} in Table~\ref{tab:ablation_on_contrastive_learning}, where \enquote{w $\mathcal{L}_\text{contrastive}$} is our final performance in Table~\ref{tab:main_results_on_avsbench}. The improved performance of \enquote{w $\mathcal{L}_\text{contrastive}$} indicates the effectiveness of contrastive learning in our framework.
\begin{table}[t]
    \caption{\textbf{Ablation on the size of the latent space,} where we conduct experiments with different latent sizes.}
    % \vspace{-2.0mm}
    \label{tab:ablation_on_ldm_dimension}
    \centering
    \small
    \setlength{\tabcolsep}{3.8mm}
    \renewcommand{\arraystretch}{1.3}
    {
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Latent Size} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        $ {D}= 8 $  & 81.04          & 0.892          & 57.28          & 0.689           \\
        $ {D}=16 $  & 81.18          & 0.895          & 57.98          & 0.704           \\
        $ {D}=24 $  & \textbf{81.51} & \textbf{0.903} & \textbf{59.62} & \textbf{0.712}  \\
        $ {D}=32 $  & 80.78          & 0.891          & 57.01          & 0.687           \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}
Additionally, we observe that contrastive learning performs poorly with the naive encoder-decoder framework, especially with our limited computation configuration, where we cannot construct large enough positive/negative pools. 
However, we find the improvement is insignificant compared to using contrastive learning in other tasks~\cite{han2022expanding}. 
We argue the main reason for this lies in our dataset being less diverse to learn distinctive enough features. 
We will investigate self-supervised learning to further explore the effectiveness of contrastive learning in our framework.
% our model relies on the representation $z$ in this case. Large positive/negative pools can relax the necessity for semantic-correlated representation, as the large sample pools can guarantee the sample-wise distinction.


\begin{table}[t]
\caption{\Fix{\textbf{Ablation on the prediction decoder,} where we conduct experiments under the AVSegFormer architecture.}}
\label{tab:AVSegFormer_diffusion}
\small
\centering
\setlength{\tabcolsep}{1.0mm}
\renewcommand{\arraystretch}{1.3}
\begin{threeparttable}
\begin{tabular}{lcccc}
\toprule[1.1pt]
\multirow{2}{*}{Method} & \multicolumn{2}{c}{S4} & \multicolumn{2}{c}{MS3} \\
\cmidrule(r){2-3}  \cmidrule(r){4-5}
 & mIoU & F-score & mIoU & F-score \\
\midrule
AVSegFormer~\cite{gao2024avsegformer} & 82.06 & 0.899 & 58.36 & 0.693 \\
AVSegFormer~w.~Diffusion (Ours)       & \textbf{82.79} & \textbf{0.910} & \textbf{59.94} & \textbf{0.715} \\
\bottomrule[1.1pt]
\end{tabular}
\end{threeparttable}
\end{table}



\noindent\textbf{Ablation on Size of the Latent Space.} 
We conduct additional ablation experiments to investigate the impact of the latent space size. In the main experiment, we perform parameter tuning and determine that $D = 24$ yields the best results. Here, we proceed to conduct experiments with varied latent sizes and present the performance outcomes in Table~\ref{tab:ablation_on_ldm_dimension}. An obvious observation is that the size of the latent space should not exceed a certain threshold ($D=32$) for the diffusion model, as doing so can lead to significant performance degradation. Conversely, we find that relatively stable predictions are achieved within the latent code dimension range of $D\in [16, 24]$.


\Fix{\noindent\textbf{Ablation on Prediction Decoder.}
We replace the decoder of the model with the transformer decoder in AVSegFormer~\cite{gao2024avsegformer} to demonstrate the applicability of our proposed conditional generation framework under different model frameworks. 
The experimental results are shown in~\tabref{tab:AVSegFormer_diffusion}. 
This demonstrates that our method's contribution extends beyond a specific architecture and represents a general enhancement that can benefit various AVS base models. Note that although alternative decoders such as transformer-based structures (\eg, AVSegFormer) demonstrate strong performance, their higher computational overhead and larger parameter counts motivated us to adopt the more lightweight Panoptic-FPN decoder.}




\subsection{Analysis}
\noindent\textbf{Pre-training Strategy Analysis.} 
As discussed in~\cite{zhou_AVSBench_ECCV_2022}, we also train our model with the full parameters initialized by the weight per-trained on the S4 subset. The performance comparison is shown in \tabref{tab:results_for_pertrain}. 
% The pre-training strategy can facilitate the modeling of audio-visual correspondence by 
It is verified that an effective pre-training strategy is beneficial in all the settings with our proposed method, using \enquote{R50} or \enquote{PVT} as a backbone. We argue the main reason lies in the less diverse and small amount of dataset. In this case, effective transfer learning with suitable model tuning strategies can be a promising research direction to improve the effectiveness of our solution further, \eg~prompt tuning~\cite{lester-etal-2021-power,han2021ptr,li-liang-2021-prefix}.

\begin{table}[t]
    \caption{\textbf{Performance comparison with different initialization strategies} (train from scratch or pre-train on S4) under MS3 setting in terms of mIoU.
    We use the arrows with specific values to indicate the performance gain.
    % the performance mIOU after using the weights pre-trained on the S4 subset.
    }
    % \vspace{-2.0mm}
    \label{tab:results_for_pertrain}
    \centering
    \small
    \setlength{\tabcolsep}{0.8mm}{
        \begin{threeparttable}
        \begin{tabular}{cccc}
        \toprule[1.1pt]
        {Methods} & {From scratch}  &  & {Pre-trained on S4}         \\
        \midrule
        AVSBench (R50)~\cite{zhou_AVSBench_ECCV_2022}   & 47.88 & $\stackrel{+ 6.45}{\longrightarrow}$ & 54.33  \\
        AVSBench (PVT)~\cite{zhou_AVSBench_ECCV_2022}   & 54.00 & $\stackrel{+ 3.34}{\longrightarrow}$ & 57.34  \\
        ECMVAE (R50)~\cite{mao_iccv_2023_ecmvae}   & 48.69 & $\stackrel{+ 8.87}{\longrightarrow}$ & 57.56  \\
        ECMVAE (PVT)~\cite{mao_iccv_2023_ecmvae}   & 57.84 & $\stackrel{+ 2.97}{\longrightarrow}$ & 60.81  \\
        Ours (R50)                             & \textbf{49.77} & $\stackrel{+ 7.82}{\longrightarrow}$ & \textbf{57.59}  \\
        Ours (PVT)                             & \textbf{59.62} & $\stackrel{+ 2.32}{\longrightarrow}$ & \textbf{61.94}  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}

% Figure environment removed



\noindent\textbf{Performance with Different Denoising Steps.}
The denoising step in diffusion models is usually pre-defined empirically. We set the denoising step in this paper following the conventional practice.
We thus evaluate the effect of the re-spaced inference denoising steps driven by the DDIM scheduler~\cite{song_DDIM_ICLR_2020}.
The change in testing performance for our model across the MS3 and S4 datasets with varying denoising steps is presented in Fig.~\ref{fig:ddim_step}.
Although the model is trained with 50 DDPM steps, employing 10 steps during inference is sufficient to achieve accurate results.
As expected, increasing the number of denoising steps leads to improved performance.
We observe that the elbow point of marginal returns given more denoising steps depends on the dataset but is always under 10 steps.
Hence, we determine that a denoising step value of 10 strikes an optimal trade-off between sampling efficiency and sample quality.


% It can be seen that improved performance is achieved with increased sampling steps.
% And when denoising step is set to 10, a saturated performance can be obtained. 
% Therefore, for the trade-off of sampling efficiency and sample quality, the value 10 is optimal for denoising step.
% larger
% along with the change in the 
% denoising steps is indeed beneficial for our task. 
% 
% To achieve trade-off between sampling efficiency and sample quality, we set denosing step as 10, and  in this paper.
% , the performance of our model shows a gradual increase.


% Figure environment removed


\noindent\textbf{Failure Case Analysis.}
\Fixtwo{We conduct a failure case analysis on our proposed method, AVSBench~\cite{zhou_AVSBench_ECCV_2022} and AVSegFormer~\cite{gao2024avsegformer}.}
In Fig.~\ref{fig:Failure}, it can be observed that our method, AVSbench, and \Fixtwo{AVSegFormer can not handle the absence of segmented objects resulting from sound interruptions.}
This limitation arises from the fact that neither our method nor AVSBench considered the \enquote{timing discontinuity} of the sound during the modeling process. 
Nevertheless, our proposed method is still able to achieve accurate sound source localization and then deliver high-quality segmentation results.
We believe that modeling from a temporal perspective, \ie~an audio-visual temporal correlation latent space, is one way to think about this problem.



