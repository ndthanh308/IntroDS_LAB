\section{Conclusion}
% In this paper, we have 
We have proposed a conditional latent diffusion model with contrastive learning for audio-visual segmentation (AVS). 
We first define AVS as a guided binary segmentation task, where audio serves as the guidance for
% should be extensively explored to localize
segmenting the sound producer(s).
% in the visual data.
Based on the conditional setting, we have introduced a conditional latent diffusion model to
% As a conditional generation task, 
% we aim to 
maximize the conditional log-likelihood,
% which can be achieved with a conditional latent diffusion model, 
where the diffusion model is chosen to produce semantic correlated latent space.
Specifically, our latent diffusion model learns the conditional ground truth feature generation process, and the reverse diffusion process can then
% Especially, we have developed a latent diffusion model to perform the latent code of segmentation map generation conditioned on the audio-visual input, thus the reverse diffusion process can 
restore the ground-truth information during inference. 
Contrastive learning has been studied to further enhance the discriminativeness of the conditional variable, leading to mutual information maximization between the conditional variable and the final output. Quantitative and qualitative evaluations on the AVSBench dataset verify the effectiveness of our solution.