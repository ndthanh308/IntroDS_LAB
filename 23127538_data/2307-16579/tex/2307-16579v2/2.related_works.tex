\section{Related Work}
\noindent\textbf{Audio-Visual Segmentation.}
Audio-visual segmentation (AVS) is a challenging, newly proposed problem that predicts pixel-wise masks for the sound producer(s) in a video sequence given audio information. 
To tackle this issue, Zhou~\etal~\cite{zhou_AVSBench_ECCV_2022} propose an audio-visual
segmentation benchmark and provide pixel-level annotations. The dataset contains five-second videos and audio, and the binary mask is used to indicate the pixels of sounding objects for the corresponding audio.
Subsequently, they present a simple baseline, an encoder-decoder network based on temporal pixel-wise audio-visual interaction.
Building upon this work, CATR~\cite{li_catr_acmmm_2023} introduces a comprehensive approach that incorporates both spatial and temporal dependencies in an audio-visual combination.
CMMS~\cite{liu_avs_acmmm_2023} extends the AVS tasks to the instance level.
Hao~\etal~\cite{hao_aaai_2024_avsbg} present an audio-visual correlation module with a bidirectional generation consistency module to ensure audio-visual signal consistency.
However, this fusion strategy only considers correlations at the feature level and does not capture the intrinsic characteristic of AVS, namely, the guiding role of audio.
Considering the role of audio as guidance for guided
% uniqueness of this task is that audio serves as guidance, leading to guided 
multimodal binary segmentation, Mao~\etal~\cite{mao_iccv_2023_ecmvae} employ a multimodal VAE with latent space factorization to model the distribution of audio and visual, aiming to maximize the contribution of audio for AVS.



% Due to its binary segmentation nature, models for salient object detection (SOD) and video foreground segmentation (VOS)~\cite{mahadevan_3DC_VOS_2020,duke_sstvos_cvpr_2021,zhang_ebm_sod_nips_2021,mao_transformerSOD_2021} (segmenting the foreground attracts human attention) are usually treated as baselines. However, the uniqueness of this task is that audio serves as guidance, leading to guided multimodal binary segmentation.


\noindent\textbf{Diffusion Models for Segmentation.}
Diffusion model~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020, denoising_diffuion,mao2024tavgbench,li2024tri} is the most popular image-generation approach aiming to learn data distribution through the iterative forward noise-adding process and the reverse denoising process. In recent days, researchers have found that it is also an effective representative learning method to capture essential features or structures~\cite{diffusion_representation_learning,Preechakul_2022_CVPR,traub2022representation,kingma2021on,label_efficient_diffusion,zhu_CDCD_ICLR_2023}. For image segmentation,
% many works have studied how diffusion model extract strong features and help improve the performance. In  
\cite{baranchuk_label_efficient_DDPMSeg_2021}
% it first 
demonstrates that the feature representation learned by a pre-trained diffusion model can significantly benefit zero-shot image segmentation. Pix2Seq-D~\cite{chen2022generalist} extend the bit-diffusion~\cite{chen2022analog} for panoptic segmentation.
\cite{asiedu_DecoderPretrain_arxiv_2022} propose a decoder pre-training strategy to pre-train the decoder of the diffusion UNet for image segmentation.
\cite{lai2023ddps} use the diffusion model for the mask prior modeling.
\cite{segdiff_image_segmentation_with_diffusion, Rahman_2023_CVPR} utilize diffusion probabilistic model for medical image segmentation. 
Most of the mentioned works only study unimodal image segmentation. However, our work investigates representative features across multiple modalities and semantic connections between them.

% where we explore the diffusion model for a unique multimodal task, where one modality serves as guidance to achieve guided segmentation.

% The essential idea of diffusion model~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020} is to systematically and slowly destroy the structure in a data distribution through an iterative forward diffusion process. The reverse diffusion process then restores structures in data.
% Denoising Diffusion Probabilistic Model (DDPM)~\cite{denoising_diffuion} extends diffusion models to generate high quality samples. Especially, DDPM establishes a connection between diffusion models and denoising score matching, leading to a simplified, weighted variational bound objective for diffusion models.
% % The original goal of diffusion models is to generate high quality images, making super-resolution as a straightforward application~\cite{saharia_SRDiffusion_PAMI_2022}. 
% % Later, diffusion models have been explored for segmentation tasks~\cite{baranchuk_label_efficient_DDPMSeg_2021,wu2023promptunet}.
% Given its feature encoding nature, diffusion models have also been used for representation learning~\cite{diffusion_representation_learning,Preechakul_2022_CVPR,traub2022representation,kingma2021on,label_efficient_diffusion,zhu_CDCD_ICLR_2023}, classification and regression~\cite{han2022card}.
% For multimodal generation~\cite{weinbach_M_VADER_arxiv_2022,gopalakrishnan2022image}, each unimodal is treated equally, which is different from our setting that one modality serves as guidance for the conditional generation process. 

% Diffusion models have been studied in the image segmentation field.
% \cite{baranchuk_label_efficient_DDPMSeg_2021} use a well-trained diffusion model as a feature extractor, to achieve zero-shot image segmentation.
% Pix2Seq-D~\cite{chen2022generalist} extend the bit-diffusion~\cite{chen2022analog} for panoptic segmentation.
% \cite{asiedu_DecoderPretrain_arxiv_2022} propose a decoder pre-training strategy to per-train the decoder of the diffusion UNet for image segmentation.
% \cite{lai2023ddps} use the diffusion model for the mask prior modeling.
% \cite{segdiff_image_segmentation_with_diffusion, Rahman_2023_CVPR} utilize diffusion probabilistic model for medical image segmentation.
% Most of these works study unimodal image segmentation, where we explore the diffusion model for a unique multimodal task, where one modality serves as guidance to achieve guided segmentation.


% The essential idea of diffusion model~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020,song2021scorebased} is to systematically and slowly destroy the structure in a data distribution through an iterative forward diffusion process. The reverse diffusion process then restores structures in data. 
% \cite{diffusion_model_raw} claims that generative models suffer from a trade-off between tractability and flexibility. A tractable model can be easily analyzed and evaluated to fit the data. A flexible model can fit structure in arbitrary data. The diffusion model allows 1) extreme flexibility in the model structure; 2) exact sampling; 3) easy multiplication with other distributions; 4) model log-likelihood and individual states can be cheaply evaluated, achieving both tractable and flexible models.
% DDPM~\cite{denoising_diffuion} extends diffusion models~\cite{diffusion_model_raw} to generate high quality samples. Especially, DDPM~\cite{denoising_diffuion} establishes a connection between diffusion models and denoising score matching, leading to a simplified, weighted variational bound objective for diffusion models.
% The original goal of diffusion models is to generate high quality images, making super-resolution as a straightforward application~\cite{saharia_SRDiffusion_PAMI_2022}. Later, diffusion models have been explored for segmentation tasks~\cite{asiedu_DecoderPretrain_arxiv_2022,chen2022generalist,conditional_diffusion_iterative_seg,segdiff_image_segmentation_with_diffusion,baranchuk_label_efficient_DDPMSeg_2021,diffusion_implicit_image_segmentation_ensemble,Xu_2023_CVPR,Rahman_2023_CVPR,kim2022diffusion,ji2023ddp,zbinden2023stochastic,bogensperger2023score,chen2023berdiff,wu2023promptunet,brempongdecoder}.
% Given its feature encoding nature, diffusion models have also been used for representation learning~\cite{diffusion_representation_learning,Preechakul_2022_CVPR,traub2022representation,kingma2021on,label_efficient_diffusion,zhu_CDCD_ICLR_2023}, classification and regression~\cite{han2022card}.
% Diffusion models have also been extended in 3D vision~\cite{cheng_3DDiffusion_arxiv_2022}. For multimodal generation~\cite{weinbach_M_VADER_arxiv_2022,gopalakrishnan2022image}, each unimodal is treated equally, which is different from our setting that one modality serves as guidance for the conditional generation process. A comprehensive survey on diffusion models can be found at~\cite{yang2022diffusion}.



% \cite{song2021scorebased} present a stochastic differential equation (SDE) that smoothly transforms a data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that restores the data distribution from the prior distribution by slowly removing the noise. The reverse-time SDE depends only on the time-dependent gradient (score) of the noisy data, where the score can be accurately estimated, with which samples can be generated by using numerical SDE.
% M-VADER: A Model for Diffusion with Multimodal Context~\cite{weinbach_M_VADER_arxiv_2022}. \\
% % Image super-resolution via iterative refinement~\cite{saharia_SRDiffusion_PAMI_2022}. \\
% SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation~\cite{cheng_3DDiffusion_arxiv_2022}. \\
% \cite{zhu_CDCD_ICLR_2023}
% \noindent{Latent Diffusion Models:}

\noindent\textbf{Contrastive Learning for Representation Learning.}
Contrastive loss~\cite{chopra2005learning,dimension_reduction_lecun,chen2020simple} is introduced for distance metric learning to decide whether the pair of samples is similar or dissimilar.
% which takes pair of examples ($\mathbf{x}$ and $\mathbf{x}'$) as input and train the network ($E$) to predict whether they are similar (from the same class: $\mathbf{y}_\mathbf{x}=\mathbf{y}_{\mathbf{x}'}$) or dissimilar (from different classes: $\mathbf{y}_\mathbf{x}\neq \mathbf{y}_{\mathbf{x}'}$). 
% Taking a step further, triplet loss~\cite{Distance_Metric_Learning,large_scale_online_learning,facenet} achieves distance metric learning by using triplets, including a query sample ($\mathbf{x}$), it is a positive sample ($\mathbf{x}^{+}$) and a negative sample ($\mathbf{x}^{-}$). The goal of triplet loss is to push the difference of similarity between positive and negative samples to the query sample to be greater than a predefined margin parameter. 
Taking a step further, triplet loss~\cite{Distance_Metric_Learning,large_scale_online_learning,facenet} uses triplets to push the difference of similarity between positive ($\mathbf{x}^{+}$) and negative samples ($\mathbf{x}^{-}$) to the query sample ($\mathbf{x}$) to be greater than a predefined threshold and achieves better feature representation learning.
% However, the unbalanced problem will limit the performance due to the fact that it only learns from one negative samples while ignoring other negative samples.  
% achieves distance metric learning by using triplets, including a query sample ($\mathbf{x}$), it is a positive sample ($\mathbf{x}^{+}$) and a negative sample ($\mathbf{x}^{-}$). The goal of triplet loss is to push the difference of similarity between positive and negative samples to the query sample to be greater than a predefined margin parameter.
% By pulling similar concepts to be closer in the embedding space and pushing the dissimilar ones to be far apart, triplet loss achieves better feature representation learning.
% However, one of the main issues is that it only learns from one negative sample, ignoring the dissimilarity with all the other candidate negative samples, leading to unbalanced metric learning. 
% To solve this problem, 
Later,~\cite{npair_loss} introduces N-pair loss to learn from multiple negative samples.  
% for balanced metric learning.
% Contrastive loss, triplet loss, and N-pair loss produce one positive sample for the query sample $\mathbf{x}$, 
% SimCLR~\cite{chen2020simple} produces two noise versions of $\mathbf{x}$ via different data augmentation strategies, and then maximizes agreement between differently augmented views of the same sample via a contrastive loss in latent space. 
% Consider a sample $\mathbf{x}$ and its two correlated views $\tilde{\mathbf{x}}_i$ and $\tilde{\mathbf{x}}_j$, 
% % where $\tilde{\mathbf{x}}_i$ and $\tilde{\mathbf{x}}_j$ is defined 
% termed as the positive pair, \cite{chen2020simple} design a self-supervised feature representation framework with a base encoder $f(\cdot)$ (\eg~ResNet backbone) and a projection head $g(\cdot)$ (\eg~a MLP), where the former is for image backbone feature extraction, and the latter produces latent space representation, where contrastive loss is applied. 
% In this case, the contrastive prediction task aims to identify $\tilde{\mathbf{x}}_j$ from $\{\tilde{\mathbf{x}}_k\}_{k\neq i}$ for a given $\tilde{\mathbf{x}}_i$. 
The main strategy to achieve self-supervised contrastive learning is constructing positive/negative pairs via data augmentation~\cite{xie2021propagate,li2021dense,wang2021dense,van2021unsupervised,o2020unsupervised,chaitanya2020contrastive,xie2021detco}.
% Prototypical contrastive learning (PCL)~\cite{li2020prototypical,li2020prototypical,lin2022prototypical,Prototypical_Graph_Contrastive_Learning,Prototypical_Momentum_Contrastive_Learning,du2022weakly,mo2022siamese,Yue_2021_CVPR,toering2022self} aims to bridge contrastive learning with clustering, where prototypes are introduced as latent variables to find the maximum-likelihood estimation of model parameters. Specifically, prototypical contrastive learning optimize both instance discrimination and semantic structure similarity, where semantic structure of the data is also encoded into the embedding space for more fine-grained representation learning. 
Instead of model instance/image discrimination, dense contrastive learning~\cite{Wang_2022_CVPR_SimSet,wang2021exploring}, widely used in segmentation tasks, aims to explore pixel-level similarity. 
% \cite{wang2021exploring} introduces pixel-wise contrastive learning
% and applies it 
% to semantic segmentation. 
Specifically, 
% the positive/negative pairs, namely 
memory bank~\cite{wang2020xbm,chen2020improved,he2020momentum,chen2020simple,wang2022contrastive} stores historical samples from the same image or different images to make up the positive/negative pool, thereby improving the discriminative capabilities of the model. In this paper, we explore contrastive learning to extensively maximize the alignment of model output with the audio signal from a density ratio perspective, leading to both effective guided segmentation and distinction of our solution with existing techniques~\cite{sun2023learning}.

\noindent\textbf{Uniqueness of Our Solutions.}
Although diffusion models have been explored in segmentation tasks, our method aims to use a conditional latent diffusion model to learn an effective multimodal latent space.
Within the representation learning method, we learn an effective representation of the ground-truth segmentation maps, which is subsequently used to support in the segmentation results.
Instead of employing the diffusion model directly in a separate pipeline, we propose a strategy that utilizes contrastive learning to minimize the audio density ratio. This strategy imposes an explicit constraint on the latent space of the diffusion model and allows us to maximize the contribution of audio for localizing the sound source to achieve high quality segmentation.
% extract the audio information's contribution.

















% Contrastive loss~\cite{chopra2005learning,dimension_reduction_lecun} was introduced for distance metric learning to decide whether the pair of data is similar or dissimilar.
% Taking a step further, triplet loss~\cite{Distance_Metric_Learning,large_scale_online_learning,facenet} achieves distance metric learning by using triplets, including a query sample ($\mathbf{x}$), it is a positive sample ($\mathbf{x}^{+}$) and a negative sample ($\mathbf{x}^{-}$). The goal of triplet loss is to push the difference of similarity between positive and negative samples to the query sample to be greater than a predefined margin parameter. By pulling similar concepts to be closer in the embedding space and pushing the dissimilar ones to be far apart, triplet loss achieves better feature representation learning. However, one of the main issues is that it only learns from one negative sample, ignoring the dissimilarity with all the other candidate negative samples, leading to unbalanced metric learning. To solve this problem, \cite{npair_loss} introduces N-pair loss to learn from multiple negative samples for balanced metric learning.
% % Contrastive loss, triplet loss, and N-pair loss produce one positive sample for the query sample $\mathbf{x}$, 
% SimCLR~\cite{chen2020simple} produces two noise versions of $\mathbf{x}$ via different data augmentation strategies, and it then maximizes agreement between differently augmented views of the same sample via a contrastive loss in latent space. Consider a sample $\mathbf{x}$ and its two correlated views $\tilde{\mathbf{x}}_i$ and $\tilde{\mathbf{x}}_j$, where $\tilde{\mathbf{x}}_i$ and $\tilde{\mathbf{x}}_j$ is defined as the positive pair, \cite{chen2020simple} design a self-supervised feature representation framework with a base encoder $f(\cdot)$ (\eg~ResNet backbone) and a projection head $g(\cdot)$ (\eg~a MLP), where the former is for image backbone feature extraction, and the latter produces latent space representation, where contrastive loss is applied. In this case, the contrastive prediction task aims to identify $\tilde{\mathbf{x}}_j$ in $\{\tilde{\mathbf{x}}_k\}_{k\neq i}$ for a given $\tilde{\mathbf{x}}_i$. 
% The main strategy to achieve self-supervised contrastive learning is constructing positive/negative pairs via data augmentation techniques~\cite{xie2021propagate,li2021dense,wang2021dense,van2021unsupervised,o2020unsupervised,chaitanya2020contrastive,xie2021detco}.
% Instead of model instance/image discrimination, dense contrastive learning~\cite{Wang_2022_CVPR_SimSet,wang2021exploring} aims to explore pixel-level similarity.
% \cite{wang2021exploring} introduces pixel-wise contrastive learning, and applies it to semantic segmentation. Specifically, the positive/negative pairs, namely memory bank~\cite{wang2020xbm,chen2020improved,he2020momentum,chen2020simple,wang2022contrastive}, can be from the same image or different images, leading to intra/inter-image level pixel-wise contrastive learning.
