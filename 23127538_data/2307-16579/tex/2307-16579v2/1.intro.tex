\section{Introduction}
\label{sec:intro}

\IEEEPARstart{A}{udio-visual} segmentation (AVS)~\cite{zhou_AVSBench_ECCV_2022,hao_aaai_2024_avsbg,li_catr_acmmm_2023,liu_avs_acmmm_2023,mao_iccv_2023_ecmvae} aims to accurately segment the region in the image that produces the sound from the audio. 
Unlike semantic segmentation~\cite{chen_deeplab_pami_2017} or instance segmentation~\cite{he2017mask,wan2025instance}, AVS involves identifying the foreground object(s) responsible for producing the given sound in the audio. 
Due to the usage of multimodal data, \ie~audio and visual, \Fix{AVS typically relies on multimodal learning, where various fusion strategies are explored to integrate audio and visual data.}
Most of these methods rely on the cross modality attention layer~\cite{zhou_AVSBench_ECCV_2022} or the transformer module~\cite{liu_avs_acmmm_2023, li_catr_acmmm_2023} to implicitly fuse the audio-visual feature.
% However, such methods rely purely on fitting the discrete samples in the dataset.


\Fix{We argue that without using audio as guidance, the visual information alone is insufficient for training the AVS model through regression-based learning.}
This \enquote{guided} attribute also distinguishes AVS from other multimodal binary segmentation, \ie~RGB-Depth salient object detection~\cite{ucnet_sal}, where each unimodal data can achieve reasonable prediction. 
With the above understanding of AVS, we find it essential to ensure the audio contribution for AVS, or the model output should be correlated with the audio. In this paper, we aim to extensively explore the contribution of audio for AVS with better data alignment modeling.

Specifically, we define the task of AVS as a conditional generation task, which aims to extensively explore the correlation between audio-visual input (the conditional variable) and the segmentation of the sound producer(s) (target).
Conditional generation can be achieved via maximizing the conditional log-likelihood with likelihood based generative models, \ie~conditional variational auto-encoders (CVAE)~\cite{structure_output,kingma2013auto}, diffusion models~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020},~\etc
Building upon the CVAE, Mao~\etal~\cite{mao_iccv_2023_ecmvae} propose to maximize the likelihood via an evidence lower bound (ELBO) with a latent space factorization strategy, proving its general effectiveness. 
This approach demonstrates the utility of employing a generative model to represent a meaningful multimodal latent space and its effectiveness in enhancing the performance of AVS.
However, the latent space in CVAE contains less semantically related information, and it suffers from the posterior collapse issue~\cite{lucas2019understanding}. 
On the other hand, diffusion models are proven more effective in producing semantic correlated latent space~\cite{label_efficient_diffusion}. 
Therefore, we introduce the diffusion model to our AVS task to ensure the extraction of semantic information from the conditional variable.
In particular, we encode the ground-truth segmentation map and use it as the target of the diffusion model, which is destroyed and generated by the diffusion model via the forward and denoising process. Furthermore, we encode the audio-visual pair and use it as the condition, leading to a conditional generative process.


% Given the strong mapping ability from visual to the segmentation map~\cite{chen2017rethinking}, directly using diffusion models with the multimodal data, \eg~audio and visual, has a risk of achieving conditional likelihood maximization without effective modeling of the audio, especially with the less diverse training dataset~\cite{zhou_AVSBench_ECCV_2022} for AVS.


Based on the conditional diffusion modeling, we argue that besides the maximization of the multimodal conditional generation, extra constraints should be introduced, such that the model output is well-aligned with the audio signal.
The alignment is achieved via minimizing the density ratio $r(\mathbf{y},\mathbf{x}^v,\mathbf{x}^a)$ between the conditional probability of the multimodal data $p(\mathbf{y}|\mathbf{x}^v,\mathbf{x}^a)$ and the unimodal data $p(\mathbf{y}|\mathbf{x}^a)$, where $\mathbf{x}^v$ and $\mathbf{x}^a$ represent the visual and audio data respectively, and $\mathbf{y}$ is the segmentation map.
Additionally, $p(\mathbf{y}|\mathbf{x}^v,\mathbf{x}^a)$ is conditioned on the audio-visual data $\mathbf{x}^v$ and $\mathbf{x}^a$ respectively, while $p(\mathbf{y}|\mathbf{x}^a)$ is conditioned only on the audio data $\mathbf{x}^a$.
In this context, $\mathbf{y}$ represents the desired segmentation map indicating the sound producer(s).
Further, we claim that minimizing the density ratio can be achieved through contrastive learning.



% Given the strong mapping ability from visual to the segmentation map~\cite{chen2017rethinking}, using diffusion models directly with multimodal data, such as audio and visual, poses a risk of maximizing conditional likelihood without effectively modeling the audio, especially when using a less diverse training dataset~\cite{zhou_AVSBench_ECCV_2022}. 

% Therefore, we propose introducing additional constraints in addition to maximizing multimodal conditional generation. These constraints ensure that the model's output aligns well with the audio signal. This alignment is achieved by minimizing the density ratio $r(\mathbf{y},\mathbf{x}^v,\mathbf{x}^a)$ between the conditional probability of the multimodal data ($p(\mathbf{y}|\mathbf{x}^v,\mathbf{x}^a)$), conditioned on the audio-visual data ($\mathbf{x}^v$ and $\mathbf{x}^a$, respectively), and the conditional probability of the unimodal data ($p(\mathbf{y}|\mathbf{x}^a)$), conditioned only on the audio data ($\mathbf{x}^a$). Here, $\mathbf{y}$ represents the desired segmentation map indicating the sound producer(s). Additionally, we argue that contrastive learning methods can be employed to minimize the density ratio.





% Directly using the conditional diffusion model with rich semantic information can sufficiently accomplish conditional maximum likelihood learning. 
% However, due to the limited and less diverse training dataset of our AVS task~\cite{zhou_AVSBench_ECCV_2022}, ensuring the discriminativeness of the audio-based conditional variable becomes challenging.
% We want the conditional variable to be representative enough in feature space to distinguish the higher level audio-visual semantic difference.
% Therefore, we introduce an audio density ratio modeling process and claim the contribution of the audio data can be maximized via minimizing the audio density ratio.
% Further, we employ contrastive learning to acquire informative latent space representation.



Contrastive learning, which is initially introduced for metric learning~\cite{chopra2005learning,dimension_reduction_lecun}, serves the purpose of acquiring a discriminative feature representation. 
In the context of representation learning, contrastive loss~\cite{oord2018representation} is employed to ensure that positive samples outputted by the network are maximally similar, while negative samples are distinctly dissimilar. 
Traditionally, in the unimodal setting~\cite{wang2021dense,o2020unsupervised,chaitanya2020contrastive,xie2021detco}, data augmentation is utilized to construct positive/negative pairs.
However, for our specific multimodal task, we construct positive/negative samples based on paired/unpaired audio-visual latent variables. 
Subsequently, the contrastive learning solution is derived from a density ratio perspective, enhancing the contribution and semantic richness of the audio guidance. 
We establish the necessity of aligning the audio signal with the prediction by minimizing a density ratio, and contrastive learning emerges as an effective approach to imposing such alignment constraints.


% With the conditional latent diffusion model and contrastive learning based on density ratio minimizing strategy, our model can achieve more effective latent space modeling and better exploration of audio for AVS.
\Fix{Our conditional latent diffusion model, coupled with contrastive learning via density ratio minimization, effectively models latent space and enhances audio exploration for AVS.}
% the contribution of audio modes.
Extensive experimental results demonstrate that our proposed pipeline achieves state-of-the-art AVS performance, especially on the more challenging multiple sound source segmentation dataset.


% We are working on a special multi-modality task, where contrastive learning is used to enhance the contribution or the semantic richness of the guidance (the audio data in our case).
% We then construct positive/negative pairs based on correspondence between the visual sound producer and the audio, where the basic idea is that the higher level semantic information of the visual sound producer and the audio should be the same, \eg~the category of the sound. With the leverage of contrastive learning, we aim to generate features with richer semantic information and ensure their distinction in feature space, making it suitable to serve as a conditional variable for the proposed conditional latent diffusion model. As a latent space diffusion model, the feature space reverse diffusion process restores the ground truth distribution, leading to ground-truth aware inference when we perform the denoising process at the test stage.



% Contrastive learning~\cite{chopra2005learning,dimension_reduction_lecun} was introduced for metric learning, and the classical contrastive loss or InfoNCE loss~\cite{oord2018representation} is defined as:
% \begin{equation}
%     \begin{aligned}
%     \label{loss_NCE}
%     \mathcal{L}_{\text{InfoNCE}} = \frac{1}{|\mathbf{P}|} \sum_{\mathbf{z}^+\in \mathbf{P}} -{\rm{log}} \frac{{\rm{exp}}(s(\mathbf{z},\mathbf{z}^+) /\tau)}{\sum\limits_{\mathbf{z}^j\in \{\mathbf{z}^+,\mathbf{N}\}} {\rm{exp}}(s(\mathbf{z},\mathbf{z}^j) /\tau)},
%     \end{aligned}
% \end{equation}
% which takes a pair of examples ($\mathbf{x}$ and $\mathbf{x}'$) as input and trains the network to predict whether they are similar (from the same class) or dissimilar (from different classes).
% In \equref{loss_NCE}, $\mathbf{z}$ is the representation of the sample $\mathbf{x}$, $\mathbf{z}^+$ is the representation of the positive sample $\mathbf{x}^+$ of $\mathbf{x}$, $\mathbf{P}$ and $\mathbf{N}$ are the sets of positive and negative pairs corresponding to $\mathbf{x}$, respectively, $s(\cdot)$ computes the dot product between $\ell_2$ normalized feature embeddings, and $\tau$ is the temperature, which can be learned or pre-defined.

% Conventionally, contrastive learning is applied to unimodal setting~\cite{wang2021dense,o2020unsupervised,chaitanya2020contrastive,xie2021detco}, where data augmentation is the main strategy to construct positive/negative pairs. We are working on a special multimodal task, where contrastive learning is used to enhance the contribution or the semantic richness of the guidance (the audio data in our case).
% We then construct positive/negative pairs based on correspondence between the visual sound producer and the audio, where the basic idea is that the higher level semantic information of the visual sound producer and the audio should be the same, \eg~the category of the sound. With the leverage of contrastive learning, we aim to generate features with richer semantic information and ensure their distinction in feature space, making it suitable to serve as a conditional variable for the proposed conditional latent diffusion model. As a latent space diffusion model, the feature space reverse diffusion process restores the ground truth distribution, leading to ground-truth aware inference when we perform the denoising process at the test stage.


% Different from existing contrastive learning based sound source localization models~\cite{sun2023learning}, where contrastive learning is directly applied to guarantee that similar samples stay closer than the dissimilar samples, we derive the contrastive learning solution from a density ratio perspective. We explain the necessity of aligning the audio signal with the prediction by minimizing a density ratio, where contrastive learning is one effective solution to impose such alignment constraints.

% For our AVS task, we aim to extensively explore semantic information from the audio data to maximum the conditional log-likelihood. With contrastive learning, we aim to push the paired data to be closer than the unpaired ones~\cite{han2022expanding}.

% For the audio data, we define the paired ground-truth segmentation map as its positive sample and other segmentation maps of the same mini-batch as negative samples. With contrastive learning and the construction of the positive/negative pair, we aim to ensure the informativeness of the conditional variable, which can also be explained as maximizing the mutual information between the output and the conditional variable, \ie~the audio data in our scenario.
% The core of contrastive learning in Eq.~\ref{loss_NCE} is feature extraction, \ie~computing $\mathbf{z}$, and positive/negative pairs construction, \ie~preparing $\mathbf{P}$ and $\mathbf{N}$. 

% For feature representation learning, we want to model the correspondence between visual sound producer(s) and audio.
% especially the visual sound producer(s) corresponding to the audio, 
% One straight forward solution can be achieved by aligning the former with the latter in feature space with contrastive learning directly following an encoder-decoder pipeline. 
% simply multiply the visual data with the binary segmentation map, and extract feature for the masked image, which can be used to decide the correspondence between visual foreground and audio. 
% However, as we need feature for the masked image, directly feed it to visual encoder can be problematic as its visual distribution is different from the raw RGB image.
% With the SSL dataset and contrastive learning, we aim to obtain better representation for audio and visual. Taking a step further, 
% Alternatively, we can use the feature representation of the fused modalities should be related to the output, which can be achieved via auto-encoder, where the fused representation is fed to the segmentation decoder for AVS. 
% However, we find contrastive learning works poorly with the naive encoder-decoder framework, especially with our limited computation configuration, where we cannot construct large enough positive/negative pools. We argue the main reason is that the model relies too much on the representation $z$ in this case. Large positive/negative pools can relax the necessity for semantic-correlated representation, as the large sample pools can guarantee the sample-wise distinction.
% The typically auto-encoder framework maps the data to feature space, which is effective for data compression. However, without further regularization on the feature space, the learned latent space is not continuous, which cannot provide a rich semantic correlation of the data. For our AVS task, we aim to explore the latent code with rich semantic information, we resort to variational auto-encoder (VAE).
% To achieve semantic-correlated representation learning,
% The main drawback of VAE is the possible posterior collapses, where the posterior is collapsed to be the standard normal distribution, providing no data semantic correlation, \ie~the latent code show no information about the ground-truth (sound producer(s)) or audio is not encoded effectively. To prevent posterior collapse, 
% we introduce score-based diffusion model in latent space to our framework, achieving conditional generation, where audio data is defined as the conditional variable in our case. Note that although the latent space diffusion solution is introduced to achieve more informative latent space given limited computational resource, our strategy can also be used when computation is never an issue, as more discriminative feature can lead to better convergence of constrative learning.

% To construct positive/negative pairs \Jing{xxxx}
% on top of VAE with a second stage training, thus the latent code is informative in representing the ground-truth information, leading to extensively exploration of audio.
% Further, the model is trained by sampling from the posterior, while inferred by sampling from the prior. 
% The discrepancy between the posterior and prior leads to less accurate predictions. We then use the score-based diffusion models to bridge the gap between the prior and posterior.

% Further, we extend the evaluation setting of AVS by performing experiments on a large SSL testing dataset, \eg~bounding box as an object detection task with object detection evaluation strategies.



We summarize our main contributions as:
\begin{compactitem}
    \item We rethink audio-visual segmentation (AVS) as a supervised conditional generation task, to explore the semantic relationship between the guiding input (audio) and the resulting output (segmentation maps).
    \item  We introduce the latent diffusion model, and the maximum likelihood estimation objective to guarantee the ground-truth aware inference.
    \item  A density ratio is introduced to impose the alignment constraint between audio and model output via contrastive learning to maximize the contribution of audio for the desired output within our latent diffusion model.
    \item Experimental results demonstrate that our proposed method achieves state-of-the-art segmentation performance. Extensive ablation experiments further validate the effectiveness of each component in our approach.

    % \item We rethink AVS as a guided conditional generation task, aiming to extensively explore the semantic correlation between the guidance (the audio) and the final output (the segmentation maps).
    % \item  We introduce the latent diffusion model, and its maximum likelihood estimation objective guarantees
    % the ground-truth aware inference.
    % \item  We adopt contrastive learning to our framework to ensure the distinction of the audio representation to achieve an effective latent diffusion model.
    % \item Experiments show that our proposed method achieves state-of-the-art segmentation performance and extensive ablation experiments demonstrate the effectiveness of each component.
\end{compactitem}