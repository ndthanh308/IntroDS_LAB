\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
%\usepackage{algorithmic}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

% extra packages
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{csquotes}
\usepackage{paralist}
\usepackage{threeparttable}
\usepackage{colortbl}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\equref}[1]{Eq.~\eqref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}


\newcommand\eg{\emph{e.g.}} \newcommand\Eg{\emph{E.g.}}
\newcommand\ie{\emph{i.e.}} \newcommand\Ie{\emph{I.e.}}
\newcommand\ien{\emph{i.e. }} \newcommand\Ien{\emph{I.e. }}
\newcommand\cf{\emph{c.f.}} \newcommand\Cf{\emph{C.f.}}
\newcommand\etc{\emph{etc. }}
\newcommand\wrt{w.r.t.} \newcommand\dof{d.o.f.}
\newcommand\etal{\emph{et al.}}

% \newcommand{\secref}[1]{Sec. \ref{#1}}
\usepackage{algpseudocode}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\newcommand{\red}[1]{{\textcolor{red}{#1}}} % ranking the first
\newcommand{\blue}[1]{{\textcolor{blue}{#1}}} % ranking the second  

\def\Jing#1{{\color{magenta}{\bf [Jing:} {\it{#1}}{\bf ]}}}
\def\MYX#1{{\color{red}{\bf [Yuxin:} {\it{#1}}{\bf ]}}}
\def\YC#1{{\color{blue}{\bf [Yuchao:} {\it{#1}}{\bf ]}}}
\newcommand{\rev}[1]{{\textcolor{blue}{#1}}}

\begin{document}

\title{Contrastive Conditional Latent Diffusion for Audio-visual Segmentation}


\author{Yuxin Mao,~
Jing Zhang*,~
Mochu Xiang,~
Yunqiu Lv,~
Dong Li,~
Yiran Zhong,~
Yuchao Dai*\\
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Yuxin Mao, Jing Zhang, Mochu Xiang, Yunqiu Lv, and Yuchao Dai are with School of Electronics and Information, Northwestern Polytechnical University, and Shaanxi Key Laboratory of Information Acquisition and Processing, Xi'an, China.
% \IEEEcompsocthanksitem Jing Zhang is with School of Computing, the Australian National University, Canberra, Australia.
\IEEEcompsocthanksitem Dong Li and Yiran Zhong are with Shanghai AI Laboratory, China.
\IEEEcompsocthanksitem
% Y. Lv and J. Zhang contributed equally. J. Zhang (zjnwpu@gmail.com) and 
Jing Zhang (zjnwpu@gmail.com) and Yuchao Dai (daiyuchao@nwpu.edu.cn) are the corresponding authors. 
\IEEEcompsocthanksitem This research was supported in part by National Natural Science Foundation of China (62271410, 12150007) and by the Fundamental Research Funds for the Central Universities.
Yuxin Mao is sponsored by the Innovation Foundation for Doctoral Dissertation of Northwestern Polytechnical University (CX2024014).

% \IEEEcompsocthanksitem The source code and experimental results are publicly available via our project page: \url{xx}.
}
}

\newcommand{\toreviewer}[1]{\vspace{0.1em}\noindent \textcolor{blue}{\textbf{#1 \hspace{0.1em}}}}
\def\Fix#1{{\color{black}{{#1}}}}
\def\Fixtwo#1{{\color{black}{{#1}}}}


% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
% The task of AVS 
Audio-visual Segmentation (AVS) is conceptualized as a conditional generation task, where audio is considered as the conditional variable for segmenting the sound producer(s). In this case, audio should be extensively explored to maximize its contribution for the final segmentation task. We propose a contrastive conditional latent diffusion model for audio-visual segmentation (AVS) to thoroughly investigate the impact of audio, where the correlation between audio and the final segmentation map is modeled to guarantee the strong correlation between them.
To achieve semantic-correlated representation learning, our framework incorporates a latent diffusion model. 
The diffusion model learns the conditional generation process of the ground-truth segmentation map, resulting in ground-truth aware inference during the denoising process at the test stage. As our model is conditional, it is vital to ensure that the conditional variable contributes to the model output.
We thus extensively model the contribution of the audio signal by minimizing the density ratio between the conditional probability of the multimodal data, \eg~conditioned on the audio-visual data, and that of the unimodal data, \eg~conditioned on the audio data only.
In this way, our latent diffusion model via density ratio optimization explicitly maximizes the contribution of audio for AVS, which can then be achieved with contrastive learning as a constraint, where the diffusion part serves as the main objective to achieve maximum likelihood estimation, and the density ratio optimization part imposes the constraint. 
By adopting this latent diffusion model via contrastive learning, we effectively enhance the contribution of audio for AVS. The effectiveness of our solution is validated through experimental results on the benchmark dataset.
Code and results are online via our project page: \url{https://github.com/OpenNLPLab/DiffusionAVS}.



% We propose a latent diffusion model with contrastive learning for audio-visual segmentation (AVS) to thoroughly investigate the impact of audio. 
% The task of AVS is conceptualized as conditional generation, where audio is considered the conditional variable for segmenting sound producer(s). The correlation between audio and the final segmentation map is modeled to ensure its contribution to this new approach.
% To achieve semantic-correlated representation learning, our framework incorporates a latent diffusion model. This diffusion model learns the conditional generation process of the ground-truth segmentation map, resulting in ground-truth aware inference during the denoising process at the test stage. As our model is conditional, it is vital to ensure that the conditional variable contributes to the model output.
% Furthermore, contrastive learning is incorporated into our framework to capture audio-visual correspondence. This approach consistently maximizes the mutual information between model predictions and audio data.
% By adopting this latent diffusion model via contrastive learning, we effectively enhance the contribution of audio for AVS. The effectiveness of our solution is validated through experimental results on the benchmark dataset.
% Code and results are online via our project page: \url{https://github.com/OpenNLPLab/DiffusionAVS}.

\end{abstract}

\begin{IEEEkeywords}
Audio-visual segmentation, Conditional latent diffusion model, Contrastive learning.
\end{IEEEkeywords}


% \section{Reviews}
% \toreviewer{To Reviewer 1:}\\
% \noindent\textbf{{R1.1}: This framework is based on the baseline [1] but adds the diffusion module. The diffusion process is operated on the latent embedding of the ground truth mask, can the diffusion model be directly used on the ground truth image?}

% \noindent\textbf{{R1.2}: Can the contrastive loss also work with hat(z\_0)? This may constrain the denoised hat(z\_0) to be consistent with the audio-visual condition. The denoised hat(z\_0) is expected to restore the ground truth information, does the learned hat(z\_0) belonging to the same class in the S4 subset have a close/similar distribution in the feature space?}

% \noindent\textbf{{R1.3}: There are still some typos to be fixed, such as 'per-trained' at Line 623, and repeated encoder at Line 679.}

% \toreviewer{To Reviewer 2:}\\
% \noindent\textbf{{R2.1}: What is the application of AVS in real world? In addition to the definition of the task, it is necessary to explain the usefulness of the research. Also, in the case of applications that must operate in real-time, it is necessary to verify and analysis the amount of computational cost.}


% \noindent\textbf{{R2.2}: Diffusion models are generally slow than pixel-wise classification models including segmentation networks due to step-wise repetition in inference. Is it reasonable computation?}

% \noindent\textbf{{R2.3}: The diffusion-based segmentation methods should be clearly compared with the proposed method, in regard to the novelty, architecture design and so on.}

% \noindent\textbf{{R2.4}: What is the performance gap according to the denoise step?}

% \noindent\textbf{{R2.5}: It is interesting to learn the distance function of features through continuous learning, but the performance improvement is poor than expected.}


% \toreviewer{To Reviewer 3:}\\
% \noindent\textbf{{R3.1}: The idea of using diffusion models for segmentation is not quite new. Overall speaking, this paper just use the diffusion model to replace the previous segmentation backbones, which seems not that novel.}


% \noindent\textbf{{R3.2}: After I check the related work, I found [36] already studied diffusion + contrastive learning. Thus the contrastive part (Positive/Negative Pair Construction) could not be viewed as brand new. Besides, the performance improvement brought by the contrastive learning is very marginal.}



% \noindent\textbf{{R3.3}: The experiments are not sufficient to evaluate the proposed method. The authors only tested it on a small dataset (about 5,000 short videos). And the authors only compared to simple baselines (AVSBench proposed by the dataset authors).}

% \noindent\textbf{{R3.4}:  What is the GFLOPS of the proposed method? I believe this is not a fair comparison since the proposed model requires much more computation cost than the compared simple baselines.}

\input{1.intro}
\input{2.related_works}
\input{3.method}
\input{4.experimets}
\input{5.conclusion}











%%%%%%%%% REFERENCES
{
\bibliographystyle{ieeetr}
\bibliography{egbib}
}

%%%%%%%%% Biography
%\bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}

\end{document}


