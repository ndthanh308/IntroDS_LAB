\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
%\usepackage{algorithmic}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

% extra packages
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{csquotes}
\usepackage{paralist}
\usepackage{threeparttable}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\equref}[1]{Eq.~\eqref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

% \def\ie{\emph{i.e.}}
% \def\eg{\emph{e.g.}}
% \def\wrt{\emph{w.r.t.}}
\newcommand\eg{\emph{e.g.}} \newcommand\Eg{\emph{E.g.}}
\newcommand\ie{\emph{i.e.}} \newcommand\Ie{\emph{I.e.}}
\newcommand\cf{\emph{c.f.}} \newcommand\Cf{\emph{C.f.}}
\newcommand\etc{\emph{etc}}
\newcommand\wrt{w.r.t.} \newcommand\dof{d.o.f.}
\newcommand\etal{\emph{et al.}}
\newcommand\ien{\emph{i.e.}}

% \newcommand{\secref}[1]{Sec. \ref{#1}}
\usepackage{algpseudocode}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\newcommand{\red}[1]{{\textcolor{red}{#1}}} % ranking the first
\newcommand{\blue}[1]{{\textcolor{blue}{#1}}} % ranking the second  


%\def\Jing#1{{\color{magenta}{\bf [Jing:} {\it{#1}}{\bf ]}}}
\newcommand{\rev}[1]{{\textcolor{blue}{#1}}}

\begin{document}

\title{Contrastive Conditional Latent Diffusion for Audio-visual Segmentation}

% \author{
% Yi Zhang,
% % *\thanks{*Equal Contribution.}, 
% Jing Zhang\thanks{Jing Zhang is with Australian National University, Australia.}, Wassim Hamidouche and Olivier Deforges\thanks{Yi Zhang, Wassim Hamidouche and Olivier Deforges are with Univ Rennes, INSA Rennes, CNRS, IETR (UMR 6164), France.}
% %IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% %\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% %\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
% }

\author{Yuxin Mao,~
Jing Zhang,~
Mochu Xiang,~
Yunqiu Lv,~
Yiran Zhong,~
Yuchao Dai*\\
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Yuxin Mao, Mochu Xiang, Yunqiu Lv, Yuchao Dai are with School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China.
\IEEEcompsocthanksitem Jing Zhang is with School of Computing, Australian National University, Canberra, Australia.
\IEEEcompsocthanksitem Yiran Zhong is with Shanghai AI Laboratory, China.
% \IEEEcompsocthanksitem The source code and experimental results are publicly available via our project page: \url{xx}.
}
}

\newcommand{\toreviewer}[1]{\vspace{0.1em}\noindent \textcolor{blue}{\textbf{#1 \hspace{0.1em}}}}


% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
We propose a latent diffusion model with contrastive learning for audio-visual segmentation (AVS) to extensively explore the contribution of audio. We interpret AVS as a conditional generation task, where audio is defined as the conditional variable for sound producer(s) segmentation.
% \enquote{command} to the system to segment the sound producer(s), 
With our new interpretation, it is especially necessary to model the correlation between audio and the final segmentation map to ensure its contribution.
% Secondly,
% due to their different modalities, 
We introduce a latent diffusion model to our framework to achieve semantic-correlated representation learning. Specifically, our diffusion model learns the conditional generation process of the ground-truth segmentation map, leading to ground-truth aware inference when we perform the denoising process at the test stage. As a conditional diffusion model, we argue it is essential to ensure that the conditional variable contributes to model output.
% Due to the importance of the conditional variable for our denoising process, 
We then introduce contrastive learning to our framework to learn audio-visual correspondence, which is proven consistent with maximizing the mutual information between model prediction and the audio data.
In this way, our latent diffusion model via contrastive learning explicitly maximizes the contribution of audio for AVS. Experimental results on the benchmark dataset verify the effectiveness of our solution. Code and results are online via our project page: \url{https://github.com/OpenNLPLab/DiffusionAVS}.
% Code will be released.
% The code will be released publicly to ensure reproducibility.

% \YC{Diffusion, multi-model are not emphasized yet.}

% Specifically, we first achieve audio/visual correspondence learning with mutual information maximization via contrastive learning. The basic idea behind it is that conditioned on the final segmentation map, the audio feature and visual feature should be independent, which is true as AVS localizes sound producer(s), and background region is assumed to contain no sound producer(s).
% Secondly, to avoid trivial 
% , where each level of the reconstructed latent code within diffusion model is conditioned on the audio data,.
\end{abstract}

\begin{IEEEkeywords}
Audio-visual segmentation, Conditional latent diffusion model, Contrastive learning.
\end{IEEEkeywords}


% \section{Reviews}
% \toreviewer{To Reviewer 1:}\\
% \noindent\textbf{{R1.1}: This framework is based on the baseline [1] but adds the diffusion module. The diffusion process is operated on the latent embedding of the ground truth mask, can the diffusion model be directly used on the ground truth image?}

% \noindent\textbf{{R1.2}: Can the contrastive loss also work with hat(z\_0)? This may constrain the denoised hat(z\_0) to be consistent with the audio-visual condition. The denoised hat(z\_0) is expected to restore the ground truth information, does the learned hat(z\_0) belonging to the same class in S4 subset have a close/similar distribution in the feature space?}

% \noindent\textbf{{R1.3}: There are still some typos to be fixed, such as 'per-trained' at Line 623, and repeated encoder at Line 679.}

% \toreviewer{To Reviewer 2:}\\
% \noindent\textbf{{R2.1}: What is the application of AVS in real world? In addition to the definition of the task, it is necessary to explain the usefulness of the research. Also, in the case of applications that must operate in real-time, it is necessary to verify and analysis the amount of computational cost.}


% \noindent\textbf{{R2.2}: Diffusion models are generally slow than pixel-wise classification models including segmentation networks due to step-wise repetition in inference. Is it reasonable computation?}

% \noindent\textbf{{R2.3}: The diffusion-based segmentation methods should be clearly compared with the proposed method, in regard to the novelty, architecture design and so on.}

% \noindent\textbf{{R2.4}: What is the performance gap according to the denoise step?}

% \noindent\textbf{{R2.5}: It is interesting to learn the distance function of features through continuous learning, but the performance improvement is poor than expected.}


% \toreviewer{To Reviewer 3:}\\
% \noindent\textbf{{R3.1}: The idea of using diffusion models for segmentation is not quite new. Overall speaking, this paper just use the diffusion model to replace the previous segmentation backbones, which seems not that novel.}


% \noindent\textbf{{R3.2}: After I check the related work, I found [36] already studied diffusion + contrastive learning. Thus the contrastive part (Positive/Negative Pair Construction) could not be viewed as brand new. Besides, the performance improvement brought by the contrastive learning is very marginal.}



% \noindent\textbf{{R3.3}: The experiments are not sufficient to evaluate the proposed method. The authors only tested it on a small dataset (about 5,000 short videos). And the authors only compared to simple baselines (AVSBench proposed by the dataset authors).}

% \noindent\textbf{{R3.4}:  What is the GFLOPS of the proposed method? I believe this is not a fair comparison since the proposed model requires much more computation cost than the compared simple baselines.}

\section{Introduction}
\label{sec:intro}

\IEEEPARstart{A}{udio-visual} segmentation (AVS) aims to accurately segment the region in the image that produces the sound from the audio, which is also defined as the sound source segmentation task. 
Due to the usage of multimodal data, \ie~audio and visual, AVS is conventionally achieved via multimodal learning~\cite{zhou_AVSBench_ECCV_2022}, where fusion strategies are investigated to implicitly explore the contribution of each modality. As a segmentation task, AVS is different from others, \ie~semantic segmentation~\cite{chen_deeplab_pami_2017} or instance segmentation~\cite{he2017mask}, in that it identifies the foreground object(s) that produce the given sound in the audio, which can also be defined as a guided binary segmentation task. We argue that without audio as guidance, the visual itself in AVS is not enough to regress the AVS model. This \enquote{guided} attribute also makes AVS
% that as a \enquote{guided segmentation} As one modality within AVS serves as the guidance, and using the other modality alone is not  where the audio data serves as guidance, making AVS also 
different from other multimodal binary segmentation, \ie~RGB-Depth salient object detection~\cite{ucnet_sal}, where each unimodal data can achieve reasonable prediction. With the above understanding of AVS, we find it is essential to ensure the contribution of audio for AVS, or the model output should be correlated with the audio. In this paper, we aim to extensively explore the contribution of audio for AVS with better data alignment modeling. 
% , as different audio of the same visual should lead to different segmentation results (see Fig.~\ref{fig_model_prediction_with_different_audio} \enquote{Ours}). 
% However, existing techniques fail to produce output accurately corresponds to audio (see Fig.~\ref{fig_model_prediction_with_different_audio} \enquote{AVSBench}, where it produces almost similar outputs to the four pairs of data with various audio data.).

% A similar and related task is sound source localization (SSL), localizing sound sources that are visible in a video sequence without using per-pixel annotations. AVS is different from SSL in two main aspects: 1) AVS is a segmentation task while SSL is a localization task, where the former is typically achieved via supervised learning, and the latter is usually defined as unsupervised or self-supervised learning; 2) Due to the labeling burden, dataset for the AVS task is much smaller than that for the SSL, which relies only on audio-visual pairs.

% , while segmentation maps are needed for the former.
% Although closely related, the two tasks are not correlated in the past, and as labels are available, the existing AVS models are trained following the conventional multimodal learning pipeline.

% We find the more suitable setting should be conditional generation, where the audio data serves as a conditional variable or the guidance for the segmentation task.
% % Figure environment removed

% We work on AVS to segment the full scope of the sound producer(s), which is more precise and can provide more informative about the foreground, \ie~its structure or shape. 

% We find three main issues of the existing AVS models. Firstly, the dataset for AVS is too small and less diverse, making over-fitting unavoidable, where the model can directly fit the visual data without audio data as guidance; 2) although multimodal fusion solutions are used~\cite{zhou_AVSBench_ECCV_2022}, there exists no explicit constraints to guarantee 
% % methods are introduced to encourage 
% the contribution of audio;
% % modality, and it's not clear how the audio data contributes to the final segmentation model; 
% 3) the testing datasets are too small to evaluate model performance.


% The small and less diverse dataset might lead to model overfitting due to the discrepancy between the empirical training dataset and true data joint distribution.
% Given the training dataset 
% $D=\{x_i,y_i\}_{i=1}^N$, the objective of machine learning methods is to minimize the expected loss function ($\mathbb{E}$), or in practice the empirical loss function (also known as empirical risk minimization~\cite{empirical_risk}) as:
% \begin{equation}
% \begin{aligned}
% \min_\theta &\mathbb{E}_{x,y}\left[\mathcal{L}(f_\theta(x),y)\right]=\int_{x,y}\mathcal{L}(f_\theta(x),y)d p(x,y)\\
% &\approx\frac{1}{N}\sum_{i=1}^N\mathcal{L}(f_\theta(x_i),y_i), \quad (x_i,y_i)\sim p(x,y),
% \end{aligned}
% \label{uncertainty_formulation}
% \end{equation}
% where $x,y$ are input and output variables from the sampled training dataset $D$, $\theta$ is the learned model parameter set, $p(x,y)$ is the joint data distribution, $(x_i,y_i)$ represents a
% % \NB{s a} 
% sampled pair from the joint data distribution $p(x,y)$, and $\mathcal{L}(.,.)$ is the loss function. 
% Under the maximum likelihood estimation framework, we let $\mathcal{L}$ be the negative (log) likelihood.
% The basic requirement of Eq.~\ref{uncertainty_formulation} is $(x_i,y_i)\sim p(x,y)$, indicating the training dataset should be sampled from the true data distribution $p(x,y)$, which is in practice not accessible. In this case, the main solution is to obtain a large enough dataset to bridge the gap between the distribution of $D$ and $p(x,y)$.


% However, for our AVS task, ....
% Besides the small dataset issue, the multimodal fusion strategies are implicit, where fusion modules are designed, and audio is not well explored as the visual itself is enough to regress a reasonable model (see fig). Further, the current testing dataset is too small to evaluate the contribution of audio.
% With above understanding about AVS, we find it's essential to ensure the contribution of audio for AVS, or the model output should be correlated with the audio. In this paper, we aim to extensively explore the contribution of audio for AVS with better data alignment modeling. 
% We find the main reason for the above issue in Fig.~\ref{fig_model_prediction_with_different_audio} is the misalignment of output and audio,
% % of existing AVS models are less accurately aligned,
% which is caused by the imbalanced contribution of visual and audio, as the current AVS models can be trained by using only the visual data.
% We verify this with a simple experiment by fitting an AVS model with visual data only and finding a converged model with reasonable output (see Table\Jing{xx}). 
% However, the nature of AVS is localizing and segmenting the sound producer(s), indicating sound from the audio data should serve as the guidance or \enquote{command}. A model without using audio should produce random output.
% where visual data alone can already fit an AVS model, which should be totally avoided. 
% In this paper, we aim to extensively explore the contribution of audio for AVS with better data alignment modeling. 



% We argue that the less effective audio contribution exploration is due to the less effective audio-visual correspondence modeling, as our audio data is only an audio sequence, without extra audio related information, \eg~the category of the sound. 
% Without extra information, it's difficult to correlate the two modalities with limited data.
% We find the SSL data can be used in our task for audio-visual correspondence modeling due to its large and diverse dataset. Specifically, w
Specifically, as a conditional generation task, we aim to extensively explore the correlation of audio (the conditional variable) and the final sound producer(s) (the target), which can be achieved via maximizing the conditional log-likelihood with likelihood based generative models, \ie~conditional variational auto-encoders (CVAE)~\cite{structure_output,kingma2013auto}, diffusion models~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020}, \etc CVAE~\cite{structure_output} maximizes the likelihood via an evidence lower bound (ELBO), which is effective in general. However, it suffers from the posterior collapse issue~\cite{lucas2019understanding} where the latent code contains nothing semantically related information. Diffusion models~\cite{score_based_model_latent_space,song2021scorebased} are proven more effective in producing semantic correlated latent space~\cite{label_efficient_diffusion}. We then introduce the diffusion model to our AVS task to ensure semantic related information is extracted from the conditional variable.
Specifically, we encode the ground-truth segmentation map and use it as the latent code, which is destroyed and generated by the diffusion model via the forward and denoising process. Further, we encode the audio-visual pair and use it as the condition, leading to a conditional generative process.

% we introduce a diffu we want to align the visual region (sound producer(s)) with the sound and push apart the visual region that does not produce the sound. 
Directly using the diffusion model with conditional variables with rich semantic information can be enough to achieve conditional maximum likelihood learning. However, given the limited and less diverse training dataset~\cite{zhou_AVSBench_ECCV_2022} of our AVS task, it's difficult to guarantee the discriminativeness of the audio based conditional variable. We want the conditional variable to be representative enough in feature space to distinguish the higher level visual semantic difference.
% can be less effective once the conditional variable itself is less informative. 
We thus resort to contrastive learning to learn informative feature representation. 


Contrastive learning~\cite{chopra2005learning,dimension_reduction_lecun} was introduced for metric learning, and
% , which takes pair of examples ($\mathbf{x}$ and $\mathbf{x}'$) as input and trains the network to predict whether they are similar (from the same class) or dissimilar (from different classes).
% The 
the classical contrastive loss or InfoNCE loss~\cite{oord2018representation} is defined as:
\begin{equation}
    \begin{aligned}
    \label{loss_NCE}
    \mathcal{L}_{\text{InfoNCE}} = \frac{1}{|\mathbf{P}|} \sum_{\mathbf{z}^+\in \mathbf{P}} -{\rm{log}} \frac{{\rm{exp}}(s(\mathbf{z},\mathbf{z}^+) /\tau)}{\sum\limits_{\mathbf{z}^j\in \{\mathbf{z}^+,\mathbf{N}\}} {\rm{exp}}(s(\mathbf{z},\mathbf{z}^j) /\tau)},
    \end{aligned}
\end{equation}
which takes pair of examples ($\mathbf{x}$ and $\mathbf{x}'$) as input and trains the network to predict whether they are similar (from the same class) or dissimilar (from different classes).
In \equref{loss_NCE}, $\mathbf{z}$ is the representation of the sample $\mathbf{x}$, $\mathbf{z}^+$ is the representation of the positive sample $\mathbf{x}^+$ of $\mathbf{x}$, $\mathbf{P}$ and $\mathbf{N}$ are the sets of positive and negative pairs corresponding to $\mathbf{x}$, respectively, $s(\cdot)$ computes the dot product between $\ell_2$ normalized feature embeddings,
% of the involved feature embedding \Yunqiu{what does "involved feature embedding" mean?}, 
and $\tau$ is the temperature, which can be learned or pre-defined.

Conventionally, contrastive learning is applied to unimodal setting~\cite{wang2021dense,o2020unsupervised,chaitanya2020contrastive,xie2021detco}, where data augmentation is the main strategy to construct positive/negative pairs. We are working on a special multimodal task, where contrastive learning is used to enhance the contribution or the semantic richness of the guidance (the audio data in our case).
% , or the audio data in our task. 
We then construct positive/negative pairs based on correspondence between the visual sound producer and the audio, where the basic idea is that the higher level semantic information of the visual sound producer and the audio should be the same, \eg~the category of the sound. With the leverage of contrastive learning, we aim to generate features with richer semantic information and ensure their distinction in feature space, making it suitable to serve as a conditional variable for the proposed conditional latent diffusion model. As a latent space diffusion model, the feature space reverse diffusion process restores the ground truth distribution, leading to ground-truth aware inference when we perform the denoising process at the test stage.

% For our AVS task, we aim to extensively explore semantic information from the audio data to maximum the conditional log-likelihood. With contrastive learning, we aim to push the paired data to be closer than the unpaired ones~\cite{han2022expanding}.

% For the audio data, we define the paired ground-truth segmentation map as its positive sample and other segmentation maps of the same mini-batch as negative samples. With contrastive learning and the construction of the positive/negative pair, we aim to ensure the informativeness of the conditional variable, which can also be explained as maximizing the mutual information between the output and the conditional variable, \ie~the audio data in our scenario.
% The core of contrastive learning in Eq.~\ref{loss_NCE} is feature extraction, \ie~computing $\mathbf{z}$, and positive/negative pairs construction, \ie~preparing $\mathbf{P}$ and $\mathbf{N}$. 

% For feature representation learning, we want to model the correspondence between visual sound producer(s) and audio.
% especially the visual sound producer(s) corresponding to the audio, 
% One straight forward solution can be achieved by aligning the former with the latter in feature space with contrastive learning directly following an encoder-decoder pipeline. 
% simply multiply the visual data with the binary segmentation map, and extract feature for the masked image, which can be used to decide the correspondence between visual foreground and audio. 
% However, as we need feature for the masked image, directly feed it to visual encoder can be problematic as its visual distribution is different from the raw RGB image.
% With the SSL dataset and contrastive learning, we aim to obtain better representation for audio and visual. Taking a step further, 
% Alternatively, we can use the feature representation of the fused modalities should be related to the output, which can be achieved via auto-encoder, where the fused representation is fed to the segmentation decoder for AVS. 
% However, we find contrastive learning works poorly with the naive encoder-decoder framework, especially with our limited computation configuration, where we cannot construct large enough positive/negative pools. We argue the main reason is that the model relies too much on the representation $z$ in this case. Large positive/negative pools can relax the necessity for semantic-correlated representation, as the large sample pools can guarantee the sample-wise distinction.
% The typically auto-encoder framework maps the data to feature space, which is effective for data compression. However, without further regularization on the feature space, the learned latent space is not continuous, which cannot provide a rich semantic correlation of the data. For our AVS task, we aim to explore the latent code with rich semantic information, we resort to variational auto-encoder (VAE).
% To achieve semantic-correlated representation learning,
% The main drawback of VAE is the possible posterior collapses, where the posterior is collapsed to be the standard normal distribution, providing no data semantic correlation, \ie~the latent code show no information about the ground-truth (sound producer(s)) or audio is not encoded effectively. To prevent posterior collapse, 
% we introduce score-based diffusion model in latent space to our framework, achieving conditional generation, where audio data is defined as the conditional variable in our case. Note that although the latent space diffusion solution is introduced to achieve more informative latent space given limited computational resource, our strategy can also be used when computation is never an issue, as more discriminative feature can lead to better convergence of constrative learning.

% To construct positive/negative pairs \Jing{xxxx}
% on top of VAE with a second stage training, thus the latent code is informative in representing the ground-truth information, leading to extensively exploration of audio.
% Further, the model is trained by sampling from the posterior, while inferred by sampling from the prior. 
% The discrepancy between the posterior and prior leads to less accurate predictions. We then use the score-based diffusion models to bridge the gap between the prior and posterior.

% Further, we extend the evaluation setting of AVS by performing experiments on a large SSL testing dataset, \eg~bounding box as an object detection task with object detection evaluation strategies.

We summarize our main contributions as:
\begin{compactitem}
% \begin{enumerate}[1)]
    % \item We discover one key limitation of AVS models, where visual data can be used directly for sound producer(s) segmentation, which motivates our solution to explicitly explore the visual-sound correspondence.
    % , observe one main problem of AVS where audio is not well encoded, leading to consistent output with various audio as guidance for the same scene.
    \item We rethink AVS as a guided conditional generation task, aiming to extensively explore the semantic correlation between the guidance (the audio) and the final output (the segmentation maps).
    \item  We introduce the latent diffusion model, and its maximum likelihood estimation objective guarantees
    the ground-truth aware inference.
    % to the audio-visual segmentation task to extensively explore the semantic correlated information from the audio data.
    \item  We adopt contrastive learning to our framework to ensure the distinction of the audio representation to achieve an effective latent diffusion model.
    \item Experiments show that our proposed method achieves state-of-the-art segmentation performance and extensive ablation experiments demonstrate the effectiveness of each component.
\end{compactitem}
% \end{enumerate}

    % maximize the mutual information between the conditional variable and the ground truth representation, leading to representative feature
    % We introduce intra-sample correlation modeling and inter-sample relationship exploration to build the audio-visual correspondence via contrastive learning.
    % \item Positive/negative pairs\Jing{xxx}
    % We introduce score based diffusion model on top of a VAE framework to avoid posterior collapse. Further, the latent code of score based diffusion model is designed to condition on the audio data, leading to further audio contribution exploration.
    % We introduce the diffusion model to the AVS task for the first time and verified that the diffusion model gives a better latent space representation compared to the rest of the generative models.
    % \item We propose a contrastive diffusion mechanism to mine the correspondence between the paired audio and visual input.

\section{Related Work}
\noindent\textbf{Audio-Visual Segmentation.}
Audio-visual segmentation (AVS) is a challenging, newly proposed problem that predicts pixel-wise masks for the sound producer(s) in a video sequence given audio information. To tackle this issue, Zhou \etal~\cite{zhou_AVSBench_ECCV_2022} propose an audio-visual
segmentation benchmark and provide pixel-level annotations. The dataset contains five-second videos and audio, and the binary mask is used to indicate the pixels of sounding objects for the corresponding audio.
Subsequently, they proposed an encoder-decoder network based on the temporal pixel-wise audio-visual interaction as a simple baseline.
However, such a feature fusion strategy considers only feature-level correlations and does not model the AVS task from its essence, \ien it does not consider the guidance capability of the audio.
Due to its binary segmentation nature, models for salient object detection (SOD) and video foreground segmentation (VOS)~\cite{mahadevan_3DC_VOS_2020,duke_sstvos_cvpr_2021,zhang_ebm_sod_nips_2021,mao_transformerSOD_2021} (segmenting the foreground attracts human attention) are usually treated as baselines. However, the uniqueness of this task is that audio serves as guidance, leading to guided multimodal binary segmentation.
% , those tasks are different from ours, as we have audio as guidance to perform guided segmentation.
% Although multimodal binary segmentation tasks exists, \ie~RGB-D SOD, the audio data we used is completely different from the depth data used in RGB-D SOD. 

% A similar and related task is sound source localization (SSL), localizing sound sources that are visible in a video sequence without using per-pixel annotations. AVS is different from SSL in that
% % two main aspects: 1) 
% AVS is a segmentation task and SSL is a localization task, where the former is typically achieved via supervised learning, and the latter is usually defined as unsupervised or self-supervised learning. Although SSL has been extensively explored, limited attention has been  AVS is still at very early stage due to its difficulty in precisely localizing the full scope of the sound producer(s).
% ; 2) Due to the labeling burden, dataset for the AVS task is much smaller than that for the SSL, which relies only on audio-visual pairs.


% The Audio-Visual Segmentation (AVS) task is newly proposed, aiming to localize the sound producers with pixel-wise segmentation masks. Zhou \etal~\cite{zhou_AVSBench_ECCV_2022} propose an AVSBench dataset for audio-visual segmentation and provide a simple baseline based on temporal pixel-wise audio-visual interaction (TPAVI), which is a cross-modal attention~\cite{vaswani_attention_is_2017_NIPS} based fusion strategy.
% The other audio-visual collaboration tasks can be classified as audio-visual correspondence (AVC)~\cite{arandjelovic_look_listen_and_learn_CVPR_2017, arandjelovic_objects_that_sound_ECCV_2018}, event localization (AVEL)~\cite{lin_avel1cite_icassp_2019, lin_avel2cite_icassp_20120, tian_avel3cite_eccv_2018, zhou_avel4cite_cvpr_2021, xuan_avel_cross_atten1_aaai_2020, yu_avel_cross_atten1_mm2022}, event parsing (AVP)~\cite{tian_avpcite1_eccv_2020, wu2021_avpcite2_cvpr_2021, lin_avpcite3_2021exploring}, \etc These methods require the fusion of audio and visual signals. Such as audio-visual similarity modeling by computing the correlation matrix~\cite{arandjelovic_look_listen_and_learn_CVPR_2017, arandjelovic_objects_that_sound_ECCV_2018}, audio-visual cross attention~\cite{xuan_avel_cross_atten1_aaai_2020, yu_avel_cross_atten1_mm2022, Liang_2021_ICCV_Asynchronous_Multimodal}, audio-guided Grad-CAM~\cite{qian_audio_guided_cam_eccv_2020}, or using a multimodal transformer for modeling the long-range dependencies between elements across modalities directly~\cite{tsai2019MULT, attention_bottleneck_multimodal_fusion}.
% However, the challenge and uniqueness of the AVS task are how to map the audio signals to \emph{fine-grained} visual cues, \ie per-pixel segmentation maps. This will rely on reliable modeling of visual and audio signals, as well as more effective fusion strategies.
\noindent\textbf{Diffusion Models and the Applications.}
The essential idea of diffusion model~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020,song2021scorebased} is to systematically and slowly destroy the structure in a data distribution through an iterative forward diffusion process. The reverse diffusion process then restores structures in data. \cite{diffusion_model_raw} claims that generative models suffer from a trade-off between tractability and flexibility. A tractable model can be easily analyzed and evaluated to fit the data. A flexible model can fit structure in arbitrary data. The diffusion model allows 1) extreme flexibility in the model structure; 2) exact sampling; 3) easy multiplication with other distributions; 4) model log-likelihood and individual states can be cheaply evaluated, achieving both tractable and flexible models.
DDPM~\cite{denoising_diffuion} extends diffusion models~\cite{diffusion_model_raw} to generate high quality samples. Especially, DDPM~\cite{denoising_diffuion} establishes a connection between diffusion models and denoising score matching, leading to a simplified, weighted variational bound objective for diffusion models.
The original goal of diffusion models is to generate high quality images, making super-resolution as a straightforward application~\cite{saharia_SRDiffusion_PAMI_2022}. Later, diffusion models have been explored for segmentation tasks~\cite{baranchuk_label_efficient_DDPMSeg_2021,asiedu_DecoderPretrain_arxiv_2022,diffusion_implicit_image_segmentation_ensemble,chen2022generalist,conditional_diffusion_iterative_seg,segdiff_image_segmentation_with_diffusion,baranchuk_label_efficient_DDPMSeg_2021,asiedu_DecoderPretrain_arxiv_2022,diffusion_implicit_image_segmentation_ensemble,chen2022generalist,conditional_diffusion_iterative_seg,diffusion_implicit_image_segmentation_ensemble,segdiff_image_segmentation_with_diffusion,Xu_2023_CVPR,Rahman_2023_CVPR,chen2022generalist,kim2022diffusion,ji2023ddp,zbinden2023stochastic,bogensperger2023score,chen2023berdiff,wu2023promptunet,brempongdecoder}.
Given its feature encoding nature, diffusion models have also been used for representation learning~\cite{diffusion_representation_learning,Preechakul_2022_CVPR,traub2022representation,kingma2021on,label_efficient_diffusion,zhu_CDCD_ICLR_2023}, classification and regression~\cite{han2022card}.
Diffusion models have also been extended in 3D vision~\cite{cheng_3DDiffusion_arxiv_2022}. For multimodal generation~\cite{weinbach_M_VADER_arxiv_2022,gopalakrishnan2022image}, each unimodal is treated equally, which is different from our setting that one modality serves as guidance for the conditional generation process. A comprehensive survey on diffusion models can be found at~\cite{yang2022diffusion}.
% \cite{song2021scorebased} present a stochastic differential equation (SDE) that smoothly transforms a data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that restores the data distribution from the prior distribution by slowly removing the noise. The reverse-time SDE depends only on the time-dependent gradient (score) of the noisy data, where the score can be accurately estimated, with which samples can be generated by using numerical SDE.
% M-VADER: A Model for Diffusion with Multimodal Context~\cite{weinbach_M_VADER_arxiv_2022}. \\
% % Image super-resolution via iterative refinement~\cite{saharia_SRDiffusion_PAMI_2022}. \\
% SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation~\cite{cheng_3DDiffusion_arxiv_2022}. \\
% \cite{zhu_CDCD_ICLR_2023}
% \noindent{Latent Diffusion Models:}

% \noindent{Multimodal Diffusion Models:}
% Figure environment removed

% \noindent\textbf{Diffusion Models for Segmentation:}
% \cite{segdiff_image_segmentation_with_diffusion}: segdiff image segmentation with diffusion probabilistic models.

% \cite{baranchuk_label_efficient_DDPMSeg_2021,asiedu_DecoderPretrain_arxiv_2022,diffusion_implicit_image_segmentation_ensemble,chen2022generalist,conditional_diffusion_iterative_seg,diffusion_implicit_image_segmentation_ensemble,segdiff_image_segmentation_with_diffusion,Xu_2023_CVPR,Rahman_2023_CVPR,chen2022generalist,kim2022diffusion,ji2023ddp,zbinden2023stochastic,bogensperger2023score,chen2023berdiff,wu2023promptunet,brempongdecoder}

\noindent\textbf{Contrastive Learning for Representation Learning.} Contrastive loss~\cite{chopra2005learning,dimension_reduction_lecun} was introduced for distance metric learning to decide whether the pair of data is similar or dissimilar.
% which takes pair of examples ($\mathbf{x}$ and $\mathbf{x}'$) as input and train the network ($E$) to predict whether they are similar (from the same class: $\mathbf{y}_\mathbf{x}=\mathbf{y}_{\mathbf{x}'}$) or dissimilar (from different classes: $\mathbf{y}_\mathbf{x}\neq \mathbf{y}_{\mathbf{x}'}$). 
Taking a step further, triplet loss~\cite{Distance_Metric_Learning,large_scale_online_learning,facenet} achieves distance metric learning by using triplets, including a query sample ($\mathbf{x}$), it is a positive sample ($\mathbf{x}^{+}$) and a negative sample ($\mathbf{x}^{-}$). The goal of triplet loss is to push the difference of similarity between positive and negative samples to the query sample to be greater than a predefined margin parameter. By pulling similar concepts to be closer in the embedding space and pushing the dissimilar ones to be far apart, triplet loss achieves better feature representation learning. However, one of the main issues is that it only learns from one negative sample, ignoring the dissimilarity with all the other candidate negative samples, leading to unbalanced metric learning. To solve this problem, \cite{npair_loss} introduces N-pair loss to learn from multiple negative samples for balanced metric learning.
% Contrastive loss, triplet loss, and N-pair loss produce one positive sample for the query sample $\mathbf{x}$, 
SimCLR~\cite{chen2020simple} produces two noise versions of $\mathbf{x}$ via different data augmentation strategies, and it then maximizes agreement between differently augmented views of the same sample via a contrastive loss in latent space. Consider a sample $\mathbf{x}$ and its two correlated views $\tilde{\mathbf{x}}_i$ and $\tilde{\mathbf{x}}_j$, where $\tilde{\mathbf{x}}_i$ and $\tilde{\mathbf{x}}_j$ is defined as the positive pair, \cite{chen2020simple} design a self-supervised feature representation framework with a base encoder $f(\cdot)$ (\eg~ResNet backbone) and a projection head $g(\cdot)$ (\eg~a MLP), where the former is for image backbone feature extraction, and the latter produces latent space representation, where contrastive loss is applied. In this case, the contrastive prediction task aims to identify $\tilde{\mathbf{x}}_j$ in $\{\tilde{\mathbf{x}}_k\}_{k\neq i}$ for a given $\tilde{\mathbf{x}}_i$. 
The main strategy to achieve self-supervised contrastive learning is constructing positive/negative pairs via data augmentation techniques~\cite{xie2021propagate,li2021dense,wang2021dense,van2021unsupervised,o2020unsupervised,chaitanya2020contrastive,xie2021detco}.
% Prototypical contrastive learning (PCL)~\cite{li2020prototypical,li2020prototypical,lin2022prototypical,Prototypical_Graph_Contrastive_Learning,Prototypical_Momentum_Contrastive_Learning,du2022weakly,mo2022siamese,Yue_2021_CVPR,toering2022self} aims to bridge contrastive learning with clustering, where prototypes are introduced as latent variables to find the maximum-likelihood estimation of model parameters. Specifically, prototypical contrastive learning optimize both instance discrimination and semantic structure similarity, where semantic structure of the data is also encoded into the embedding space for more fine-grained representation learning. 
Instead of model instance/image discrimination, dense contrastive learning~\cite{Wang_2022_CVPR_SimSet,wang2021exploring} aims to explore pixel-level similarity.
\cite{wang2021exploring} introduces pixel-wise contrastive learning, and applies it to semantic segmentation. Specifically, the positive/negative pairs, namely memory bank~\cite{wang2020xbm,chen2020improved,he2020momentum,chen2020simple,wang2022contrastive}, can be from the same image or different images, leading to intra/inter-image level pixel-wise contrastive learning.


% \noindent\textbf{Uniqueness of Our Solution:}





\section{Method}
% \Jing{overview of the method with a figure maybe}
Given the training dataset $D=\{\mathbf{X}_i,\mathbf{y}_i\}_{i=1}^N$ with the input data $\mathbf{X}=\{\mathbf{x}^v,\mathbf{x}^a\}$ ($\mathbf{x}^v$ represents the input video with continuous frames~\cite{zhou_AVSBench_ECCV_2022} , $\mathbf{x}^a$ is the audio of the current clip) and output $\mathbf{y}$ (the ground-truth segmentation maps of the video clip), the goal of AVS is to segment the sound producer(s) from $\mathbf{x}^v$ with the guidance from $\mathbf{x}^a$. $i$ indexes the samples, which are omitted for clear presentation. As discussed in Sec.~\ref{sec:intro}, AVS is unique in that audio serves as guidance to achieve guided binary segmentation,
% Given that the same input video ($\mathbf{x}^v$) can lead to significantly different segmentation maps ($\mathbf{y}$) with different audio ($\mathbf{x}^a$) as input, we argue that audio in AVS serves as guidance or conditional variable, 
making it different from conventional multimodal learning~\cite{baltruvsaitis2018multimodal}, where each modal contributes nearly equally to the final output. With this understanding, we define AVS as a conditional generation task, and our goal is then to maximize the likelihood of the conditional distribution $p(\mathbf{y}|\mathbf{x}^v,\mathbf{x}^a)$.



% The overview of our proposed method can be seen in Fig.~\ref{fig:model_overview}.
% (see Fig.~\ref{fig:model_overview} for an overview
% % whole pipeline 
% of our method).
% Let $D=\{X_i,y_i\}_{i=1}^N$ to be the training dataset with $i$ as the index. $X=\{\{x^v_t\}_{t=1}^T,x^a\}$ denotes the input data, \ien~the visual $\{x^v_t\}_{t=1}^T$ for $T$ continuous frames, audio $x^a$ of the current clip. $y=\{y_t\}_{t=1}^T$ are the ground-truth segmentation map, \ien~the segmentation maps (we omit $t$ for clear presentation).
% The whole pipeline of our method is illustrated in Fig.~\ref{fig:model_overview}.

% We aim to segment objects that are producing the sound $x^a$ in a video $\{x^v_t\}_{t=1}^T$. 

 

A typical way to achieve this is via a variational auto-encoder (VAE)~\cite{kingma2013auto,structure_output}. However, a not well-designed VAE might suffer from the posterior collapse issue~\cite{lucas2019understanding}, where the latent variable fails to encode input related information, leading to less informative latent space. In our case, posterior collapse could lead to less extensive audio exploration. Recently, diffusion models~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020,song2021scorebased} show potentials in many tasks, which are proven capable of achieving more semantic related latent space~\cite{label_efficient_diffusion}. We thus resort to diffusion models for our AVS task (see Sec.~\ref{subsec_conditional_latent_diffusion}), aiming to model the distribution of $p(\mathbf{y}|\mathbf{x}^v,\mathbf{x}^a)$.
% , which will serve as guidance for our conditional generation process with another decoder. 
Further, we introduce contrastive learning (see Sec.~\ref{subsec_contrastive_learning}) to our framework to guarantee the effectiveness of the conditional variables by explicitly modeling the correspondence between visual objects and the audio. We present our pipeline in Sec.~\ref{subsec_objective_function} (see Fig.~\ref{fig:model_overview} for an overview of the proposed method).

% We present a latent diffusion model via contrastive learning to extensively explore the contribution of audio for effective audio-visual segmentation (see Fig.~\ref{fig:model_overview} for an overview of the proposed method).
% Toward this goal, we model the latent code over the segmentation maps under the audio-visual pair as a condition using the latent diffusion model, then feed the learned latent distribution into the decoder to achieve the segmentation. 
% Further, we perform contrastive learning under the positive latent code (encoded from the paired audio-visual data as the condition) and the negative latent code (encoded from the unpaired audio-visual data as the condition) to exploit the \enquote{paired information} between visual and audio inputs.

% Our network is composed of five main modules:
% 1) Latent Encoder that maps the ground-truth into a low dimensional latent code; 2) Latent Conditional Diffusion Model to learn a meaningful latent space; 3) Audio-Visual network to produce deterministic feature maps; 4) PredictionNet that employs stochastic features and deterministic features to produce the final segmentation results; 5) P/N pair construction and contrastive learning.
% We will introduce each module as follows.


\subsection{Conditional Latent Diffusion Model for AVS}
\label{subsec_conditional_latent_diffusion}
With a conditional latent diffusion model, we aim to model the conditional distribution $p(\mathbf{y}|\mathbf{x}^v,\mathbf{x}^a)$, where a ground-truth related diffusion model is learned to estimate the conditional ground-truth density function, achieving ground-truth aware inference.
% of the ground-truth distribution, which is proven 
% Our CLDM is composed of a ground-truth latent encoder that maps the ground-truth into a lower dimensional latent code, and a conditional latent diffusion module for conditional prediction generation.

% Our network is composed of five main modules:
% 1) Latent Encoder that maps the ground-truth into a low dimensional latent code; 2) Latent Conditional Diffusion Model to learn a meaningful latent space; 3) Audio-Visual network to produce deterministic feature maps; 4) PredictionNet that employs stochastic features and deterministic features to produce the final segmentation results; 5) P/N pair construction and contrastive learning.
% We will introduce each module as follows.

\noindent\textbf{Ground-Truth Latent Encoder.}
We employ an encoder $E_{\varphi}$ to map the ground-truth segmentation map into the latent space, \ie~$\mathbf{z}_0=E_{\varphi}(\mathbf{y})\in \mathbb{R}^{B\times D}$, where $B$ is the batch size and $D$ denotes the latent dimensions. Note that our ground-truth encoding process is similar to the posterior computation strategy in VAE~\cite{kingma2013auto}. However, the main difference is that the latent code in our case is explicitly modeled via a diffusion model, and it can follow any distributions, where the latent code in VAE is assumed to be Gaussian with the re-parameterization trick~\cite{kingma2013auto}.
The $E_{\varphi}$ consists of five convolutional layers followed
by leakyReLU and batch norm, and the output channels are $[16, 32, 64, 64, 64]$, respectively. Then two fully connected layers are used to generate the latent code
% encode the feature into latent space 
of size $D=24$. 
% Ablation on the size of the latent space can be found in the supplementary material.
% has no posterior collapse issue as in VAE.


\noindent\textbf{Latent Conditional Diffusion Model.}
%% 介绍Diffusion Model的损失函数，如何进行condition等等，参考“SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation”的3.2
Given the latent code $\mathbf{z_0}$, our latent conditional diffusion model aims to learn its distribution, thus we can restore the ground-truth information during testing.
We first review diffusion models~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020}. Then, we present our conditional diffusion model
to gradually diffuse $\mathbf{z}_0$ to $\mathbf{z}_K\sim\mathcal{N}(0,\mathbf{I})$, 
% of standard normal distribution 
and restore $\mathbf{z}_0$ back from $\mathbf{z}_K$ via conditional generation.
% with the audio-visual data as a conditional variable. 

\noindent\textit{\textbf{Diffusion models}}~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020,song2021scorebased} are built upon a generative Markov chain, which converts a simple known distribution, (\eg~a Gaussian) into a target distribution (\eg~any distribution). The essential idea of diffusion model~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020} is to systematically and slowly destroy the structure of a sample
% in a data distribution \Mochu{destroy the structure of a sample} 
through an iterative forward diffusion process. The reverse diffusion process then restores structures in the sample. 
% \Mochu{in the sample}.

% Let's define the conditional data distribution as $q(\mathbf{z}_0|\mathbf{f}^f)$, where $\mathbf{f}^f$ is the fused audio-visual feature.
Following the conventional diffusion process, a latent data representation $\mathbf{z}_0$ 
% $q(\mathbf{z}_0|\mathbf{f}^f)$ 
will be gradually converted into an analytically tractable distribution, \ie~$\pi(\mathbf{z})=\mathcal{N}(0,\mathbf{I})$, by iteratively applying a Markov diffusion kernel $T_\pi(\mathbf{z}|\mathbf{z}';\beta)$ with diffusion rate $\beta$ via:
\begin{equation}
    q(\mathbf{z}_k|\mathbf{z}_{k-1})=T_\pi(\mathbf{z}_k|\mathbf{z}_{k-1};\beta_k).
\end{equation}
% and the final analytically tractable distribution $\pi(z)$
% % , \ie~$\mathcal{N}(0,\mathbf{I})$,
% is defined as:
% \begin{equation}
%     \pi(z) = \int T_\mathcal{N}(\mathbf{z}|\mathbf{z}';\beta) \pi(\mathbf{z}') d\mathbf{z}'.
% \end{equation}
The forward trajectory is thus:
\begin{equation}
q(\mathbf{z}_{0,...,K})=q(\mathbf{z}_0)\prod_{k=1}^K q(\mathbf{z}_k|\mathbf{z}_{k-1}),
\end{equation}
where the diffusion kernel $q(\mathbf{z}_k|\mathbf{z}_{k-1})$ is defined as Gaussian in~\cite{diffusion_model_raw,ho_ddpm_NIPS_2020}
% either Gaussian diffusion 
with an identity-covariance:
\begin{equation}
    q(\mathbf{z}_k|\mathbf{z}_{k-1}) = \mathcal{N}(\mathbf{z}_k;\sqrt{1-\beta_k}\mathbf{z}_{k-1},\beta_k\mathbf{I}).
\end{equation}
A notable property of the forward diffusion process is that it admits sampling $\mathbf{z}_k$ at arbitrary timestep $k$ in closed form:
\begin{equation}
\label{eq_diffusion_process}
    q(\mathbf{z}_k|\mathbf{z}_0)=\mathcal{N}(\mathbf{z}_k;\sqrt{\bar{\alpha}_k}\,\mathbf{z}_0,(1-\bar{\alpha}_k)\mathbf{I}),
\end{equation}
where $\alpha_k=1-\beta_k$ and $\bar{\alpha}_k=\prod_{s=1}^k\alpha_s$. \equref{eq_diffusion_process} explains the stochastic diffusion process, where no learnable parameters are needed, and a pre-defined set of hyper-parameters, \ie~$\{\beta\}_{k=1}^K$, will lead to a set of latent variables $\{\mathbf{z}\}_{k=1}^K$.

The generative process or the denoising process is then to 
% The generative distribution is trained to 
% restore the data distribution \Mochu{restore the sample} 
restore the sample via:
\begin{equation}
p_\theta(\mathbf{z}_{0,...,K})=p(\mathbf{z}_{K})\prod_{k=1}^K p_\theta(\mathbf{z}_{k-1}|\mathbf{z}_{k}),
\end{equation}
where $p(\mathbf{z}_{K})=\pi(\mathbf{z})=\mathcal{N}(0,\mathbf{I})$ in our case.
For Gaussian diffusion, during learning, only the mean ($\mu$) and variance ($\Sigma$)
% for a Gaussian diffusion kernel 
are needed to be estimated, leading to:
\begin{equation}
    p_\theta(\mathbf{z}_{k-1}|\mathbf{z}_{k})=\mathcal{N}(\mathbf{z}_{k-1};\mu_\theta(\mathbf{z}_{k},k),\Sigma_\theta(\mathbf{z}_{k},k)),
\end{equation}
where $\theta$ represents model parameters. In practice, \cite{ho_ddpm_NIPS_2020} sets $\Sigma$ as hyper-parameters, \ie~$\Sigma_\theta(\mathbf{z}_{k},k)=\beta_k\mathbf{I}$, for stable training, thus only $\mu_\theta(\mathbf{z}_{k},k)$ is learned.   

\noindent\textit{\textbf{Conditional diffusion models for AVS.}}
% involve conditional variables to the generative process. 
For our AVS task, with the ground-truth latent encoder $\mathbf{z}_0=E_{\varphi}(\mathbf{y})$, \equref{eq_diffusion_process} provides the diffusion process by gradually destroying $\mathbf{z}_0$ to obtain $\mathbf{z}_K\sim\mathcal{N}(0,\mathbf{I})$. Our conditional generation process aims to restore $\mathbf{z}_0$ given the input data
% under the audio-visual condition 
$\mathbf{c}=E_{\psi}(\mathbf{x}^v, \mathbf{x}^a)$, where $\mathbf{c}$ is the feature embedding of our input, leading to the conditional generative process $p_\theta(\mathbf{z}_{k-1}|\mathbf{z}_{k},\mathbf{c})$ (Although we aim to correlate output with audio, as no appearance information is presented in the binary ground truth, we take the fused feature $\mathbf{c}$ instead of the audio feature to correlate \enquote{visual} sound producer(s) with the audio data).
% as no appearance information in encoded in the output feature $\mathbf{z}_0$. Our goal is to learn semantic rich latent space, where we aim to correlated \enquote{visual} sound producer(s) with audio, and the fused information in our case can provide \enquote{visual} guidance. 
We thus sample from $p_\theta(\mathbf{z}_0|\mathbf{c})$ via:
\begin{equation}
\begin{aligned}
    \label{eq_conditional_generation}
    &p_\theta(\mathbf{z}_0|\mathbf{c})=\int p_\theta(\mathbf{z}_{0,...,K}|\mathbf{c}) \text{d}\mathbf{z}_{1,...,K},\\
&p_\theta(\mathbf{z}_{0,...,K}|\mathbf{c})=p(\mathbf{z}_K)\prod_{k=1}^K p_\theta(\mathbf{z}_{k-1}|\mathbf{z}_k,\mathbf{c}).
\end{aligned}
\end{equation}

Following the simplified diffusion model objective~\cite{ho_ddpm_NIPS_2020}, with the re-parameterization trick~\cite{kingma2013auto}, a noise estimator $\epsilon_\theta$ is designed to regress the actual noise $\epsilon$ added to $\mathbf{z}_k$ via:
% Specifically, 
% % To train the denoising UNet,
% we adopt the simplified objective proposed by Ho \etal~\cite{ho_ddpm_NIPS_2020} to train our latent conditional diffusion model.
% of  and an encoder $E_{\psi}$ to get the audio-visual embedding as a condition. The latent code and the condition can be represented as:
% \begin{equation}
%     \begin{aligned}
%     \mathbf{z_0}=E_{\varphi}(\mathbf{y}), \quad\text{and}\quad \mathbf{c}=E_{\psi}(x^a,x^v)
%     \end{aligned}
% \end{equation}
\begin{equation}
    \begin{aligned}
    \label{ddpm_loss}
    \mathcal{L}_{\text{simple}}(\theta):=\mathbb{E}_{\mathbf{z},\mathbf{c}, \epsilon\sim \mathcal{N}(0,\mathbf{I}), k}\left[\left\|\epsilon-\epsilon_\theta\left(\mathbf{z}_k, \mathbf{c}, k\right)\right\|^2\right].
    \end{aligned}
\end{equation}
At inference time, given feature $\mathbf{c}$ of the audio-visual pair and random noise $\mathbf{z}_K\!\sim\!\mathcal{N}(0,\mathbf{I})$, our model samples $p_\theta(\mathbf{z}_0|\mathbf{c})$ via \equref{eq_conditional_generation} by gradually performing denoising.
% ~\footnote{Please refer to the supplementary material for detailed $\epsilon_\theta$.}.
% ~\footnote{Please refer the detailed structure of $E_{\varphi}$, $E_{\phi}$, and $\epsilon_\theta$ in the supplementary material.}.

\noindent\textit{Technical details.}
% Here we introduce the detailed structure of the involved modules. The $E_{\varphi}$ consists of five convolutional layers followed
% by leakyReLU and batch norm, and the output channels are $[16, 32, 64, 64, 64]$. Then two fully connected layers are used to encode the feature into a latent space of size 24. The $E_{\psi}$ can be divided into two branches: the visual branch and the audio branch. The visual branch involves five convolutional layers and two fully connected layers, which keep the same as $E_{\varphi}$. The audio branch involves two fully connected layers. Further, the visual features and the audio features are concatenated channel-wisely and another two fully connected layers are used to get final conditional embedding. 
Following the conventional practice in designing the diffusion models~\cite{ho_ddpm_NIPS_2020}, the noise estimator $\epsilon_\theta$ in our case is a UNet structure, which consists of eight fully connected layers followed by leakyReLU activation, the former four layers are \enquote{encoder} and the latter four layers are \enquote{decoder}.
% The core role of the latent decoder $D_\tau$ is to perform feature expanding, and the implementation details are reported in the above subsection.


The effectiveness of $p_\theta(\mathbf{z}_0|\mathbf{c})$ depends on the representativeness of the conditional variable $\mathbf{c}$, which can be addressed by contrastive learning~\cite{chopra2005learning,dimension_reduction_lecun,Distance_Metric_Learning,large_scale_online_learning,facenet,npair_loss,chen2020simple,oord2018representation}.
% Directly using the diffusion model with conditional variables with rich semantic information can be enough to achieve conditional maximum likelihood learning. However, g
% Given the limited and less diverse training dataset~\cite{zhou_AVSBench_ECCV_2022} of our AVS task, it's difficult to guarantee the discriminativeness of the conditional variable.
% An ideal $\mathbf{c}$ should be distinctive enough, serving as clear guidance.
% % We want the conditional variable to be representative enough in feature space to distinguish the higher level visual semantic difference.
% % can be less effective once the conditional variable itself is less informative. 
% We thus resort to contrastive learning to learn informative feature representation~\cite{chopra2005learning,dimension_reduction_lecun,Distance_Metric_Learning,large_scale_online_learning,facenet,npair_loss,chen2020simple,oord2018representation}. 

% The key challenge is then to obtain informative feature representation $\mathbf{c}$ for effective conditional generation $p_\theta(\mathbf{z}_0|\mathbf{c})$, which is achieved via contrastive learning~\cite{chopra2005learning,dimension_reduction_lecun,Distance_Metric_Learning,large_scale_online_learning,facenet,npair_loss,chen2020simple,oord2018representation}.

% \noindent\textit{Technical details:} As the latent code and the condition embedding are vectors with size $B\times D$.
% We use 8 MLP layers to construct the noise estimator $\epsilon_\theta$.

% \Jing{structure of $\epsilon_\theta\left(\mathbf{z}_k, \mathbf{f}^f, k\right)$ }
% conditions from audio-visual pairs, we sample $\mathbf{\hat{z}_0}$ by gradually denoising a noise variable sampled from the standard normal distribution $N(0,1)$.






\subsection{Contrastive Representation Learning}
\label{subsec_contrastive_learning}
As a conditional generation model, we argue that the representativeness of the conditional variable(s) plays an important role in the sample quality, especially for our specific multimodal task, where audio data serves as guidance for the visual data to achieve guided segmentation. We will first introduce our conditional variable generation process, \ie~$\mathbf{c}=E_{\psi}(\mathbf{x}^v, \mathbf{x}^a)$, and then 
% We want to have the conditional variable $\mathbf{c}$ to be discriminative enough to explain the alignment of audio and visual. We thus introduce contrastive representation learning to our framework. We will first introduce the conditional feature generation process
% % encoding 
% and then 
present our positive/negative pairs construction for contrastive learning.

% \Jing{briefly introduce how you adopt contrastive learning in your task -- two main directions: 1) feature representation; 2) positive negative pair construction.}



\noindent\textbf{Generating the Conditional Feature $\mathbf{c}$.}
We encode
% use 
visual-audio pairs
% as a condition and encode it 
into the conditional latent code $\mathbf{c}$. Specifically, $E_{\psi}$ can be divided into two branches, namely the visual branch and the audio branch. The visual branch consists of five convolutional layers and two fully connected layers, which share the same structure of $E_{\varphi}$. The audio branch involves two fully connected layers. Further, the visual features and the audio features are concatenated channel-wisely and another two fully connected layers are used to get the final conditional embedding $\mathbf{c}$.

% We perform late fusion on the audio features and five convolutional layers followed by leakyReLU encoded visual features to obtain the conditional feature $\mathbf{c}$ from the fused features.


\noindent\textbf{Positive/Negative Pair Construction.} One of the key challenges of contrastive learning is positive/negative pairs construction, based on which metric learning~\cite{metric_learning_survey} can be achieved. Metric learning~\cite{metric_learning_survey} is the proper choice to automate the distance function selection process, aiming to learn task-specific distance functions in a supervised manner. For our AVS task, we aim to learn a suitable distance function, thus the paired audio/visual sound producer(s) data should stay closer in feature space than the unpaired ones.

Besides the latent code $\mathbf{z}_0$, we have three variables involved in our framework, namely video $\mathbf{x}^v$, audio $\mathbf{x}^a$, and ground-truth segmentation map $\mathbf{y}$. As discussed in
Sec.~\ref{subsec_conditional_latent_diffusion}, we design a diffusion model to diffuse and restore the ground-truth information, thus we can sample from $p_\theta(\mathbf{z}_0|\mathbf{c})$ via \equref{eq_conditional_generation}. In that case, we claim that the conditional variable $\mathbf{c}$ should be discriminative enough to distinguish $\mathbf{z}_0$. In other words, given $\mathbf{c}$, the corresponding $\mathbf{z}_0$ should lead to a larger score than  $\mathbf{z}'_0$ of another sound producer(s).
% We then define our positive and negative pair based on $\mathbf{z}_0$.
Specifically, with
% Given $\mathbf{z}_0$ of $\mathbf{y}_i$, we define its corresponding paired
audio-visual feature $\mathbf{c}=E_{\psi}(\mathbf{x}^v_i, \mathbf{x}^a_i)$, we define its ground-truth encoding $\mathbf{z}_0=E_{\varphi}(\mathbf{y}_i)$ as its positive sample, and $\mathbf{y}'$ other than $\mathbf{y}_i$ in the mini-batch as the negative samples.
% We construct unpaired audio-visual data, \ie~$(\mathbf{x}^v_i, \mathbf{x}^a_j)$ by reversing the ordering of the audio in the mini-batch, and obtain $\mathbf{c}'=E_{\psi}(\mathbf{x}^v_i, \mathbf{x}^a_j)$ as the negative sample of $\mathbf{z}_0$.
With the above positive/negative samples, we obtain our contrastive loss as:
% by reversing the ordering of the audio in the mini-batch, leading to the negative pairs.
% We then 
% Given the visual data $\mathbf{x}^v$, we define the paired audio $\mathbf{x}^a$ as its positive sample, leading to $c$.
% % For the negative samples, w
% We construct unpaired audio-visual data by reversing the ordering of the audio in the mini-batch, leading to the negative pairs.
% For the positive samples, we use paired audio-visual data to do the above process and obtain $\mathbf{z}_T$.
% For the negative samples, we construct unpaired audio-visual data by reversing the ordering of the audio in the batch. 
% Then feed the unpaired audio-visual data into the diffusion model and obtain $\mathbf{z}_T^\text{pos}$.\Jing{not sure about this, will come back.}
% We define the unpaired audio-visual data as the negative sample. 
% Thus, t
% The contrastive loss can be defined as:
\begin{equation}
    \begin{aligned}
    \label{contrastive_loss}
    \mathcal{L}_{\text{contrastive}}(\psi)=-\mathbb{E}_\mathbf{\mathbf{z}_0}\left[\log \frac{f(\mathbf{z}_0,\mathbf{c})/\tau}{\sum_{\mathbf{z}_0'\in\{\mathbf{N},\mathbf{z}_0\}}f(\mathbf{z}'_0,\mathbf{c})/\tau}\right],
    \end{aligned}
\end{equation}
% \Yunqiu{The $f(z_0,c)$ in the denominator is $f(z'_0,c)$? where is $z'_0$?} 
where $\mathbf{z}_0$ is always paired with $\mathbf{c}$, and $\mathbf{N}$ represents the negative samples within the mini-batch, which includes all the samples except $\mathbf{z}_0$, $f(\mathbf{z}_0,\mathbf{c})=\exp(s(\mathbf{z}_0,\mathbf{c}))$ (see \equref{loss_NCE})  estimates the density ratio $\frac{p(\mathbf{z}_0|\mathbf{c})}{p(\mathbf{z}_0)}$. $\tau$ is a temperature parameter and we set $\tau\!=\!1$ in all experiments.

% \Yunqiu{Why the temperature parameter $\tau$ in \eqref{loss_NCE} is deleted here?}
\noindent\textbf{Mutual Information Maximization Analysis.} Mutual Information (MI) captures the nonlinear statistical dependencies between variables. Specifically, for random variables $\mathbf{z}_0$ and $\mathbf{c}$, their mutual information is defined as:
\begin{equation}
\begin{aligned}
    &I(\mathbf{z}_0;\mathbf{c})=\sum_{\mathbf{z}_0,\mathbf{c}}p(\mathbf{z}_0,\mathbf{c})\log\frac{p(\mathbf{z}_0,\mathbf{c})}{p(\mathbf{z}_0)p(\mathbf{c})}\\
    &=\sum_{\mathbf{z}_0,\mathbf{c}}p(\mathbf{z}_0,\mathbf{c})\log\frac{p(\mathbf{z}_0|\mathbf{c})}{p(\mathbf{z}_0)}\propto\sum_{\mathbf{z}_0,\mathbf{c}}p(\mathbf{z}_0,\mathbf{c})\log f(\mathbf{z}_0,\mathbf{c}).
\end{aligned}
\end{equation}

With the contrastive loss in \equref{contrastive_loss}, we aim to maximize the density ratio $f(\mathbf{z}_0,\mathbf{c})$, which in turn can be explained as achieving mutual information maximization between $\mathbf{z}_0$ and $\mathbf{c}$, thus our contrastive loss explicitly explores the contribution of $\mathbf{c}$ for the segmentation representation $\mathbf{z}_0$.
% The density ratio $f(\mathbf{z}_0,\mathbf{c})=\frac{p(\mathbf{z}_0|\mathbf{c})}{p(\mathbf{z}_0)}$
% \Jing{Similar to~\cite{zhu_CDCD_ICLR_2023}, we will analyze how our objective is consistent with mutual information maximization between visual (or visual sound producer(s) to be more precise) and audio. I will come back to this part}

% \begin{equation}
%     \begin{aligned}
%     \label{contrastive_loss}
%     \mathcal{L}_{\text{contrastive}}:=\mathbb{E}_q\left[-\log p_\theta\left(\mathbf{z}_0|\mathbf{z}_t, \mathbf{f}\right)\right]-\\
%     C\Sigma_{\mathbf{z}^j \in Z^{\prime}} \mathbb{E}_q\left[-\log p_\theta\left(\mathbf{z}_0^j|\mathbf{z}_t, \mathbf{f}\right)\right]
%     \end{aligned}
% \end{equation}

% \noindent\textbf{Feature representation via Latent diffusion}
% We use audio-visual data as a condition and get latent code z by the diffusion model.

% \noindent\textbf{P/N pair construction}
% For the positive samples, we use paired audio-visual data to do the above process and obtain $z_T$.
% For the negative samples, we construct unpaired audio-visual data by reversing the ordering of the audio in the batch. Then feed the unpaired audio-visual data into the diffusion model and obtain $z_T^\text{pos}$.
\subsection{Model Prediction Generation and Training}
\label{subsec_objective_function}
In Sec.~\ref{subsec_conditional_latent_diffusion} we present our conditional latent diffusion model to learn a conditional distribution $p_\theta(\mathbf{z}_0|\mathbf{c})$, restoring the ground truth information $\hat{\mathbf{z}}_0$ during inference, where the discriminativeness of the conditional variable and it's contribution to the final output is constrained via the contrastive learning pipeline (see Sec.~\ref{subsec_contrastive_learning}). As shown in Fig.~\ref{fig:model_overview}, the restored $\hat{\mathbf{z}}_0$ and the input data encoding are feed to the prediction decoder to generate our final prediction.

% given the  
% Our whole framework in Fig.~\ref{fig:model_overview} is composed of a fused feature extraction module for AVS, where TPAVI is used for cross-level multimodal fusion following~\cite{zhou_AVSBench_ECCV_2022}, a contrastive learning module for representative feature generation, a conditional diffusion module to explicitly model the conditional generation process $p(y|x^a,x^v)$, and a prediction decoder to generate our final segmentation map.



\noindent\textbf{Input Data Encoding.}
% \Jing{finish it, mention how to perform early fusion and the generation of $\mathbf{f}^f$ in Eq.~\ref{eq_conditional_generation}.}
% \noindent\textbf{Audio-Visual network.} %% 解释audio-visual backbone的详细结构，参考UCNet中的“SaliencyNet:”
We design an Audio-Visual network to produce a set of multi-scale deterministic feature maps from the input audio-visual pairs. 
% As described in 
Similar to AVSBench~\cite{zhou_AVSBench_ECCV_2022}, we encode the audio and visual features from two branches.
For the audio branch, we first process the audio waveform to a spectrogram via the short-time Fourier transform, and then fed it to the frozen VGGish~\cite{hershey_VGGish_icassp_2017} model, which is an audio classification network and per-trained on AudioSet~\cite{gemmeke_audioset_icassp_2017}. We denote the audio features as $\mathbf{A}\in \mathbb{R}^{T\times d}$, where $d=128$ is the feature dimension.
Given the video sequence $\mathbf{x}^v$, we encode visual features from ImageNet pre-trained ResNet50~\cite{he_resnet_cvpr_2016} (or PVTv2~\cite{wang_Pvtv2_CVM_2022}) backbone. Finally, we obtain the multi-scale visual features as $\mathbf{F}_l\in \mathbb{R}^{T\times c_l\times h_l\times w_l}$, where $c_l$ indicates the channel numbers and $(h_l,w_l)=(H,W)/2^{l+1}$. $(H,W)$ is the spatial size of the input video and $l\in [1,4]$ represents the feature levels.
For the ResNet50 backbone, the channel sizes of the four stages are $c_{1:4}=[256, 512, 1024, 2048]$. And for the PVTv2 backbone, $c_{1:4}=[64, 128, 320, 512]$.
Additionally, we use four convolutional layers to further post-process the visual features $\mathbf{F}_l$ to $\mathbf{V}_l\in \mathbb{R}^{T\times c\times h_l\times w_l}$, where $c=128$.
Further, given the audio-visual features, namely $\mathbf{A}$ and $\mathbf{V}_l$, we exploit the temporal pixel-wise audio-visual interaction
% (TPAVI) 
module~\cite{zhou_AVSBench_ECCV_2022} to perform audio-visual fusion in the feature space, which is a cross modal attention module to explore the audio-visual feature correlation.
Finally, we obtain the deterministic feature maps $\mathbf{G}_l\in \mathbb{R}^{T\times c\times h_l\times w_l}$, which contains audio-visual information.



\noindent\textbf{Prediction decoder.} %% 解释decoder的结构，参考UCNet中的“PredictionNet:”
Since the deterministic features $\mathbf{G}_l$ and stochastic representation $\mathbf{\hat{z}}_0$ are with different feature sizes, to fuse the two items, we perform a latent code expanding module $D_\tau$, which contains one $3\times 3$ convolutional layer, to achieve feature expanding of $\mathbf{\hat{z}}_0$. Specifically, we first expand $\mathbf{\hat{z}}_0$ to a 2D tensor and tile it to the same spatial size as $\mathbf{G}_4$. We define the new 2D feature map as $\mathbf{\hat{z}^\mathbf{e}}_0$. Given that the spatial size of $\mathbf{\hat{z}^\mathbf{e}}_0$ and $\mathbf{G}_4$ are the same, we perform cascaded channel-wise feature concatenation and one $3\times 3$ convolution to obtain $\mathbf{\hat{G}}_4$, which is the same size as $\mathbf{G}_4$.
% to obtain a new tensor of channel dimension 
% Specifically, we expand $\mathbf{\hat{z}}_0$ to the feature map of the same spatial size as $\mathbf{G}_4$ by defining $\varepsilon \in \mathcal{N}(0, \mathbf{I})$ as two-dimensional Gaussian noise map. Thus, we can obtain a $D$ (size of the latent space) channel stochastic feature $\mathbf{\hat{z}^\mathbf{e}}_0$ which is full of the segmentation ground-truth representation. Further, we mix $\mathbf{\hat{z}_0^\mathbf{e}}$ and $G_4$ channel-wise by concatenation, and obtain a fused $\hat{G}_4$ with $C+D$ channels.
We use the decoder of Panoptic-FPN~\cite{kirillov_panopticfpn_cvpr_2019} to decode the mixed features $\{\{\mathbf{G}_l\}_{l=1}^3,\mathbf{\hat{G}}_4\}$. 
The final output of the decoder is $\mathbf{M}\in \mathbb{R}^{T\times 1\times H\times W}$ and activated by a \texttt{sigmoid} function.
% Note that any valid decoder architecture could be used, we keep the same with AVSBench~\cite{zhou_AVSBench_ECCV_2022} 


% \noindent\textbf{Technical details.}
% Here we introduce the detailed structure of the involved modules. The $E_{\varphi}$ consists of five convolutional layers followed
% by leakyReLU and batch norm, and the output channels are $[16, 32, 64, 64, 64]$. Then two fully connected layers are used to encode the feature into a latent space of size 24. The $E_{\psi}$ can be divided into two branches: the visual branch and the audio branch. The visual branch involves five convolutional layers and two fully connected layers, which keep the same as $E_{\varphi}$. The audio branch involves two fully connected layers. Further, the visual features and the audio features are concatenated channel-wisely and another two fully connected layers are used to get final conditional embedding. The $\epsilon_\theta$ consists of eight fully connected layers followed by leakyReLU activation, the former four layers are \enquote{encoder} and the latter four layers are \enquote{decoder}. The core role of the latent decoder $D_\tau$ is to perform feature expanding, and the implementation details are reported in the above subsection.
% \Jing{introduce the structure of involved modules, $E_{\varphi}$, $\epsilon_\theta$, $D_\tau$, $E_{\phi}$. Especially, for the diffusion model, the step number, and how to achieve efficient sampling. Those details are needed}. 
\begin{table*}[!ht]
\small
\caption{\textbf{Quantitative comparison against
% AVSBench~\cite{zhou_AVSBench_ECCV_2022} and other methods of 
related models} on the S4 and MS3 subsets in terms of mIOU and F-score. We use two backbones (\enquote{R50} and \enquote{PVT}) to demonstrate that our method achieves a consistent performance improvement.}
% \vspace{-5mm}
\begin{center}
\begin{threeparttable}
\begin{tabular}{cp{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}}
  \toprule[1.1pt]
    \multirow{3}{*}{Setting} &\multirow{3}{*}{Metric} &\multicolumn{2}{c}{VOS} &\multicolumn{2}{c}{SOD} &\multicolumn{4}{c}{AVS}\\
    \cmidrule(r){3-4}\cmidrule(r){5-6}\cmidrule(r){7-10}
    &  & 3DC & SST & iGAN & LGVT & AVSBench & 
    AVSBench & Ours & \textbf{Ours}\\
    &  & \cite{mahadevan_3DC_VOS_2020} & \cite{duke_sstvos_cvpr_2021} & \cite{mao_transformerSOD_2021} & \cite{zhang_ebm_sod_nips_2021} & (R50)~\cite{zhou_AVSBench_ECCV_2022} & 
    (PVT)~\cite{zhou_AVSBench_ECCV_2022} & (R50) & \textbf{(PVT)}\\
    \midrule
    \multirow{2}{*}{S4}  & mIoU    & 57.10 & 66.29 & 61.59 & 74.94 & 72.79 & 78.74 & 75.80 & \textbf{81.38}  \\
                         & F-score & 0.759 & 0.801 & 0.778 & 0.873 & 0.848 & 0.879 & 0.869 & \textbf{0.902}  \\ 
    \midrule
    \multirow{2}{*}{MS3} & mIoU    & 36.92 & 42.57 & 42.89 & 40.71 & 47.88 & 54.00 & 49.77 & \textbf{58.18}  \\
                         & F-score & 0.503 & 0.572 & 0.544 & 0.593 & 0.578 & 0.645 & 0.621 & \textbf{0.709}  \\
    \bottomrule[1.1pt]
  \end{tabular}
\end{threeparttable}
\end{center}
\label{tab:main_results_on_avsbench}
% \vspace{-8mm}
\end{table*}

% Figure environment removed

\noindent\textbf{Objective function.} As a segmentation task, our model is trained with a cross-entropy loss with the ground-truth segmentation map as supervision. We also have a conditional diffusion module and a contrastive learning pipeline involved, leading to our final objective as:
\begin{equation}
    \label{eq_objective_function}
    \mathcal{L}=\mathcal{L}_{\text{seg}}+\lambda_1 \mathcal{L}_{\text{simple}}+\lambda_2 \mathcal{L}_{\text{contrastive}},
\end{equation}
where $\lambda_1$ and $\lambda_2$ are used to balance the two objectives, which are set empirically as 1 and 0.1, respectively.
% : $\{\lambda_1, \lambda_2\}=\{1, 0.1\}$.



\section{Experimental Results}
\subsection{Setup}
\noindent\textbf{Datasets.} We use the AVSBench~\cite{zhou_AVSBench_ECCV_2022}, a dataset with 5,356 audio-video pairs and pixel-wise annotations to perform our experiments.
Each audio-video pair in the dataset is 5 seconds and the video is trimmed to five consecutive frames by extracting the video frame at the end of each second.
The AVSBench is divided into semi-supervised Single Sound Source Segmentation (S4) with only the first frame labeled, and fully supervised Multiple Sound Source Segmentation (MS3) with all frames labeled.
And there are 4,922 videos in the S4 subset, and 424 videos in the MS3 subset.
We train our model on the training set and perform evaluations on the testing set.

\noindent\textbf{Evaluation Metrics.} Being the same as AVSBench~\cite{zhou_AVSBench_ECCV_2022}, we evaluate the audio-visual segmentation performance by using Mean Intersection over Union (mIoU) and F-score. The F-score is defined as: $F_{\beta}=\frac{(1+\beta^2 \times \text{precision} \times \text{recall})}{\beta^2\times \text{precision} + \text{recall}}, \beta^2=0.3$, where both precision and recall are based on binary segmentation map (with 256 uniformly distributed binarization thresholds in the range of $[0, 255]$).


\noindent\textbf{Compared Methods.} We compare our method with AVSBench~\cite{zhou_AVSBench_ECCV_2022}, and other related segmentation tasks, such as video foreground segmentation models (VOS)~\cite{mahadevan_3DC_VOS_2020, duke_sstvos_cvpr_2021}, RGB image based salient object detection models~\cite{mao_transformerSOD_2021, zhang_ebm_sod_nips_2021}, which is strictly in accordance with the settings in previous work~\cite{zhou_AVSBench_ECCV_2022}. We set up the comparison due to the binary video segmentation nature of AVS. Being consistent with AVSBench, we also use two backbones, ResNet50~\cite{he_resnet_cvpr_2016} and PVT~\cite{wang_Pvtv2_CVM_2022} initialized with ImageNet~\cite{deng_imagenet_cvpr_2009} per-trained weights, to demonstrate the effectiveness of our solution.
% that our proposed model achieves consistent performance improvement under different backbones.

\noindent\textbf{Implementation.}
Our proposed method is trained end-to-end using the Adam optimizer~\cite{Kingma_Adam_ICLR_2015} with default hyper-parameters for 15 and 30 epochs on the S4 and MS3 subsets. The learning rate is set to $10^{-4}$ and the batch size is 4. All the video frames are resized to the shape of $224\times 224$. 
For the latent diffusion model, we use the cosine noise schedule and the noise prediction objective in \equref{ddpm_loss} for all experiments. The diffusion steps $K$ is set as 20. To accelerate sampling, we use the DDIM~\cite{song_DDIM_ICLR_2020} with 10 sampling steps.
% ~\footnote{The code will be released publicly to ensure reproducibility.}

% \begin{table}[!htp]
%     \caption{Quantitative comparison against AVSBench~\cite{zhou_AVSBench_ECCV_2022} and other methods of related tasks on the S4 and MS3 subset in terms of mIOU and F-score. We use two backbones (\enquote{R50} and \enquote{PVT}) to demonstrate that our method achieves a consistent performance improvement.}
%     \label{tab:main_results_on_avsbench}
%     \centering
%     \small
%     \vspace{1.0mm}
%     \setlength{\tabcolsep}{1.2mm}{
%         \begin{threeparttable}
%         \begin{tabular}{cccccc}
%         \toprule %[1.5pt]
%         & \multirow{2}{*}{Methods} & \multicolumn{2}{c}{S4}                      & \multicolumn{2}{c}{MS3}                     \\
%         \cmidrule(r){3-4}  \cmidrule(r){5-6}
%                      &         & mIoU                & F-score              & mIoU                 & F-score   \\
%         \midrule
%         \multirow{2}{*}{VOS}& 3DC~\cite{mahadevan_3DC_VOS_2020}  & 57.10   & 0.759   & 36.92   & 0.503    \\
%         & SST~\cite{duke_sstvos_cvpr_2021}                       & 66.29   & 0.801   & 42.57   & 0.572    \\
%         \midrule
%         \multirow{2}{*}{SOD}& iGAN~\cite{mao_transformerSOD_2021}& 61.59   & 0.778   & 42.89   & 0.544    \\
%         & LGVT~\cite{zhang_ebm_sod_nips_2021}                    & 74.94   & 0.873   & 40.71   & 0.593    \\
%         \midrule
%         &AVSBench (R50)~\cite{zhou_AVSBench_ECCV_2022}   & 72.79   & 0.848   & 47.88   & 0.578    \\
%         \multirow{2}{*}{AVS}&AVSBench (PVT)~\cite{zhou_AVSBench_ECCV_2022}   & 78.74   & 0.879   & 54.00   & 0.645    \\
%          & Ours (R50)               & 78.80          & 0.861          & 49.77          & 0.621     \\
%          & Ours (PVT)               & \textbf{81.38} & \textbf{0.902} & \textbf{58.18} & \textbf{0.709}  \\
%         \bottomrule
%         \end{tabular}
%         \end{threeparttable}
%     }
%     \vspace{-5.0mm}
% \end{table}




\subsection{Performance Comparison}
\noindent\textbf{Quantitative Comparison.}
The quantitative results on the test set of S4 and MS3 are presented in \tabref{tab:main_results_on_avsbench}. 
We can observe that our method consistently achieves better performance on MS3 and S4 subsets in terms of mIoU and F-score, outperforming state-of-the-art AVS methods by a large margin.
There is a consistent performance improvement of our proposed method compared to AVSBench, regardless of whether \enquote{R50} or \enquote{PVT} is used as the backbone. Particularly, 4.18 and 2.58 higher mIOU than AVSbench~\cite{zhou_AVSBench_ECCV_2022} is obtained on the two subsets with \enquote{PVT} backbone. 
It is worth noting that our \enquote{R50} based model slightly outperforms the LGVT~\cite{zhang_ebm_sod_nips_2021} under the S4 subset, despite LGVT using a swin transformer~\cite{liu_swin_iccv_2021} backbone, while AVSBench (R50) performs worse than LGVT. 
This suggests that exploring matching relationships between visual objects and sounds is more important than using a better visual backbone for AVS tasks.
Note that the performance improvement benefits from the effective latent diffusion with the contrastive loss design itself, but does not come from more trainable parameters. Because the $E_{\varphi}$, $E_{\varphi}$, $D_{\tau}$, and $\epsilon_\theta$ are lightweight enough with total 4M parameters, thus the capacity of our model is comparable with AVSBench.

\noindent\textbf{Qualitative Comparison.}
In Fig.~\ref{fig:main_compare}, we show the qualitative comparison of our method
% the model predictions between our method 
% and 
with the existing method, namely AVSBench~\cite{zhou_AVSBench_ECCV_2022}. Our method tends to output segmentation results with finer details, \ien~an accurate segmentation of the \emph{bow of the violin} and the \emph{piano-key} in the left sample in Fig.~\ref{fig:main_compare}. Additionally, our method also has the ability to identify the true sound producer, such as the \emph{boy} in the right sample in Fig.~\ref{fig:main_compare}, indicating a better sound localization performance. While \cite{zhou_AVSBench_ECCV_2022} segments the two foreground objects, ignoring the audio information as guidance.

% \Jing{will come back}

% We illustrate the qualitative results under the challenge MS3 subset in Fig.~\ref{fig:main_compare}. 


\begin{table}[!htp]
    \caption{\textbf{Ablation on the latent diffusion model.} \enquote{E-D} indicates the deterministic encoder-decoder structure. \enquote{CVAE} denotes using CVAE to generate the latent code. \enquote{LDM} is our proposed latent diffusion model}
    % \vspace{-2.0mm}
    \label{tab:ablation_on_ldm}
    \centering
    \small
    \setlength{\tabcolsep}{1.5mm}{
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        E-D      & 78.89      & 0.881      & 54.28      & 0.648       \\
        CVAE     & 79.97      & 0.888      & 55.21      & 0.661       \\
        % \midrule
        LDM (Ours)& \textbf{81.02} & \textbf{0.894} & \textbf{57.67} & \textbf{0.698}  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}
\subsection{Experimental Analysis}


\noindent\textbf{Ablation on Latent Diffusion Model.}
As discussed in the introduction section (Sec.~\ref{sec:intro}), a likelihood conditional generative model exactly fits our current conditional generation setting, thus a conditional variational auto-encoder~\cite{structure_output,kingma2013auto} can be a straightforward solution. 
To verify the effectiveness of our latent diffusion model, we design two baselines and show the comparison results
% As the critical component of our proposed method, to verify the impact of the latent diffusion model, we provide two baseline models and show the results
in \tabref{tab:ablation_on_ldm}. Firstly, we design
% : \textbf{1}) 
a deterministic model with a simple encoder-decoder structure (\enquote{E-D}), where the input data encoding $\{\mathbf{G}\}_{l=1}^4$ is feed directly to the prediction decoder (see Fig.~\ref{fig:model_overview}). Note that \enquote{E-D} is the same as AVSBench~\cite{zhou_AVSBench_ECCV_2022}, and we retrain it in our framework and get similar performance as the original numbers reported in their paper. 
Secondly, to explain the superiority of the diffusion model compared with other likelihood based generative models, namely conditional variational auto-encoder~\cite{structure_output} in our scenario, we follow \cite{ucnet_sal} and design an AVS model based on CVAE (\enquote{CVAE}).
% \textbf{2}) a conditional variational auto-encoder (\enquote{CVAE}) following~\cite{ucnet_sal}. The structure of \enquote{E-D} is the same as AVSBench~\cite{zhou_AVSBench_ECCV_2022}, we retrain it in our framework and get similar performance as the original report in their paper. 
CVAE~\cite{structure_output} was introduced to RGB-Depth salient object detection in~\cite{ucnet_sal}, where early fusion is used for latent feature encoding. We follow a similar pipeline and perform latent feature encoding based on the fused feature $\{\mathbf{G}_l\}_{l=0}^4$ instead of the early fusion feature due to our audio-visual setting, which is different from the visual-visual setting in~\cite{ucnet_sal}.
Specifically, the CVAE~\cite{structure_output} pipeline for our AVS task consists of an inference process and a generative process, where the inference process infers the latent variable $\mathbf{z}$ by $p_\theta(\mathbf{z}|\mathbf{X})$, and the generative process
% builds a latent variable $\mathbf{z}$ by $p_\theta(\mathbf{z}|\mathbf{X})$ and obtains 
produces the output via $p_\theta(\mathbf{y}|\mathbf{X},\mathbf{z})$. 
The full pipeline of the CVAE for the audio-visual segmentation task can be shown in Fig.~\ref{fig:model_overview_vae}.

% Figure environment removed

% \footnote{We will explain the detailed structure of CVAE for the AVS task in the supplementary material.}.
% based generative model. 
Results in \tabref{tab:ablation_on_ldm} show that generative models can improve the performance of AVS by providing more meaningful latent space compared with the deterministic models. 
Further, the latent diffusion model provides a more powerful latent space modeling capability than our implemented CVAE counterpart. Note that, as no latent code is involved in \enquote{E-D}, we do not perform contrastive learning. For a fair comparison, the contrastive learning objective $\mathcal{L}_\text{contrastive}$ is not involved in \enquote{CVAE} or \enquote{LDM (Ours)} either.

\begin{table}[t!]
    \caption{\textbf{Ablation on the conditional variable,} where we remove it (\enquote{None}), or replace it with audio or visual representation.
    % with  We remove the audio-visual condition, the audio condition, and the visual condition as three comparison variants.
    }
    % \vspace{-2.0mm}
    \label{tab:ablation_on_audio_visual_condition}
    \centering
    \small
    \setlength{\tabcolsep}{1.5mm}{
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        None       & 80.04      & 0.889      & 56.12      & 0.671       \\
        Audio      & 80.29      & 0.892      & 56.59      & 0.680       \\
        Visual     & 80.68      & 0.892      & 57.21      & 0.688       \\
        % \midrule
        audio-visual (Ours) & \textbf{81.02} & \textbf{0.894} & \textbf{57.67} & \textbf{0.698}  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}


\begin{table}[t!]
    \caption{\textbf{Ablation of contrastive learning.} We perform experiments without the $\mathcal{L}_\text{contrastive}$ to show its effectiveness.}
    % \vspace{-2.0mm}
    \label{tab:ablation_on_contrastive_learning}
    \centering
    \small
    \setlength{\tabcolsep}{1.5mm}{
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        w/o $\mathcal{L}_\text{contrastive}$    & 81.02 & 0.894 & 57.67 & 0.698  \\
        w   $\mathcal{L}_\text{contrastive}$    & \textbf{81.38} & \textbf{0.902} & \textbf{58.18} & \textbf{0.709}  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}
\noindent\textbf{Ablation on Audio-Visual Condition.}
To further understand the effectiveness of the audio-visual conditioning in the training process of the latent diffusion model, we train three models with different conditional variables $\mathbf{c}$, and show performance in Table~\ref{tab:ablation_on_audio_visual_condition}. Firstly, we remove the conditional variable, leading to unconditional generation $p_\theta(\mathbf{z}_{k-1}|\mathbf{z}_{k})$, and show the performance as \enquote{None}. 
Then we define unimodal audio or visual as only one conditional variable. To achieve this, we simply use the feature of each individual modal before multimodal feature concatenation (see $E_\psi$ in Sec.\ref{subsec_contrastive_learning}), leading to audio/visual as conditional variable based models, namely \enquote{Audio} and \enquote{Visual} in Table~\ref{tab:ablation_on_audio_visual_condition}.
% by removing the audio condition, the visual condition, and the audio-visual condition in the latent diffusion model. 
% As shown in Table~\ref{tab:ablation_on_audio_visual_condition}, we explore four variants of the different condition types. 
Compared with the unconditional generation,
% not using the condition, 
the conditional generation
% using audio and visual for the condition 
can bring performance gains, and the best performance can be obtained by using the audio-visual condition. We can also observe that the performance of taking the visual data as conditioning
% performing the visual condition 
is better than taking
% performing the 
audio as the conditional variable. We believe this can be explained from two main aspects. Firstly, our dataset is small and less diverse, leading to less effective audio information exploration as we pretrained our model on a large visual image dataset. Secondly, the audio encoder is smaller compared with the visual encoder. More investigation will be conducted to balance the data distribution.
Similar to ablation on latent diffusion model, we do not perform contrastive learning in those related experiments in Table~\ref{tab:ablation_on_audio_visual_condition} for a fair comparison.
% this is due to visual data can provide more per-pixel information for a segmentation task.




\noindent\textbf{Ablation on Contrastive Learning.}
We introduce contrastive learning to our framework to learn the discriminative conditional variable $\mathbf{c}$. We then train our model directly without contrastive learning and show its performance as \enquote{w/o $\mathcal{L}_\text{contrastive}$} in Table~\ref{tab:ablation_on_contrastive_learning}, where \enquote{w   $\mathcal{L}_\text{contrastive}$} is our final performance in Table~\ref{tab:main_results_on_avsbench}. The improved performance of \enquote{w   $\mathcal{L}_\text{contrastive}$} indicates the effectiveness of contrastive learning in our framework.
% However, 
Apart from that, we find contrastive learning works poorly with the naive encoder-decoder framework, especially with our limited computation configuration, where we cannot construct large enough positive/negative pools. 
However, we find the improvement is not significant compared with using contrastive learning in other tasks~\cite{han2022expanding}. We argue the main reason is that our dataset is less diverse to learn distinctive enough features. We will investigate self-supervised learning to further explore contrastive learning in our framework.
% our model relies on the representation $z$ in this case. Large positive/negative pools can relax the necessity for semantic-correlated representation, as the large sample pools can guarantee the sample-wise distinction.



\noindent\textbf{Analysis of the Per-training Strategy.} As discussed in~\cite{zhou_AVSBench_ECCV_2022}, we also train our model with the full parameters initialized by the weight per-trained on the S4 subset. The performance comparison is shown in \tabref{tab:results_for_pertrain}. 
% The pre-training strategy can facilitate the modeling of audio-visual correspondence by 
It is verified that the pre-training strategy is beneficial in all the settings with our proposed method, using \enquote{R50} or \enquote{PVT} as a backbone. 

\begin{table}[t!]
    \caption{\textbf{Performance comparison with different initialization strategies} (train from scratch or pre-train on S4) under MS3 setting in terms of mIoU.
    We use the arrows with specific values to indicate the performance gain.
    % the performance mIOU after using the weights pre-trained on the S4 subset.
    }
    % \vspace{-2.0mm}
    \label{tab:results_for_pertrain}
    \centering
    \small
    \setlength{\tabcolsep}{0.8mm}{
        \begin{threeparttable}
        \begin{tabular}{cccc}
        \toprule[1.1pt]
        {Methods} & {From scratch}  &  & {Pre-trained on S4}         \\
        \midrule
        AVSBench (R50)~\cite{zhou_AVSBench_ECCV_2022}   & 47.88 & $\stackrel{+ 6.45}{\longrightarrow}$ & 54.33  \\
        AVSBench (PVT)~\cite{zhou_AVSBench_ECCV_2022}   & 54.00 & $\stackrel{+ 3.34}{\longrightarrow}$ & 57.34  \\
        Ours (R50)                             & \textbf{49.77} & $\stackrel{+ 7.82}{\longrightarrow}$ & \textbf{57.59}  \\
        Ours (PVT)                             & \textbf{58.18} & $\stackrel{+ 2.76}{\longrightarrow}$ & \textbf{60.94}  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}

\noindent\textbf{Ablation on Size of The Latent Space:} We perform extra ablation on the size of the latent space. In the main experiment, after parameter tuning, we find $D = 24$ works best. Here, we conduct experiments with different latent sizes, and show performance in Table~\ref{tab:ablation_on_ldm_dimension}. An obvious observation is that the size of the latent space should not be too large ($D=32$) for the diffusion model, which can have significant performance degradation, and we obtain relative stable predictions with the latent code dimension in the range of
% with 
$D\in [16, 24]$.

\begin{table}[!htp]
    \caption{\textbf{Ablation on the size of the latent space,} where we conduct experiments with different latent sizes.}
    \vspace{-2.0mm}
    \label{tab:ablation_on_ldm_dimension}
    \centering
    \small
    \setlength{\tabcolsep}{2.5mm}{
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Latent Size} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        $ {D}= 8 $  & 81.04          & 0.892          & 57.28          & 0.689           \\
        $ {D}=16 $  & 81.18          & 0.895          & 57.98          & 0.704           \\
        $ {D}=24 $  & \textbf{81.38} & \textbf{0.902} & \textbf{58.18} & \textbf{0.709}  \\
        $ {D}=32 $  & 80.78          & 0.891          & 57.01          & 0.687           \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    % \vspace{-2.0mm}
\end{table}

% Figure environment removed

\noindent\textbf{Failure Case Analysis:}
We perform failure case analysis on our proposed method and the AVSBench~\cite{zhou_AVSBench_ECCV_2022}. As shown in Fig.~\ref{fig:Failure}, both ours and AVSBench can not deal with the absenting of segmented objects due to sound interruptions. This is due to the fact that neither we nor AVSBench took into account the \enquote{timing discontinuity} of the sound in the modeling process. However, our proposed method is still able to provide a high-quality sound source localization result.
% \subsection{Sample-wise Diffusion \textit{vs.} Latent Diffusion}

% \subsection{Accuracy \wrt~Diffusion Process}



% \subsection{Efficient Sampling Strategies Analysis}

% Although diffusion models can lead to high quality samples~\cite{diffusion_model_beat_gans} and exact likelihood estimation by leveraging the connection to neural ordinary differential equations (ODEs)~\cite{song2021scorebased}, its slow sampling speed limits its real-time applications, making efficient samplers~\cite{song2021denoising,salimans2022progressive,xiao2022tackling,ma2022accelerating,ryu2022pyramidal,guth2022wavelet,lu2022dpm,zheng2022fast,ma2022accelerating,knowledge_distillation_iterative_generative,noise_estimation_generative_diffusion,improved_ddpm,bilateral_denoising_diffusion,learning_fast_samplers_differentiating_sample_quality,Gotta_Go_Fast_When_Generating,analytic_Dpm,lu2022dpmnips2022,ma2022accelerating} an urgent request for real-time deployment of diffusion models. The existing fast samplers for diffusion models can be roughly divided into training based samplers~\cite{salimans2022progressive,knowledge_distillation_iterative_generative,noise_estimation_generative_diffusion,bilateral_denoising_diffusion,improved_ddpm,learning_fast_samplers_differentiating_sample_quality,ma2022accelerating} and training-free samplers~\cite{song2021denoising,Gotta_Go_Fast_When_Generating,analytic_Dpm,lu2022dpmnips2022}. The former usually involves expensive training stages and their flexibility can be limited. The latter ones are based on the pre-trained diffusion models, which can be used as plug-and-play strategies, and are more flexible. 


% \noindent\textbf{Computational Analysis:}
\section{Conclusion}
% In this paper, we have 
We have proposed a conditional latent diffusion model with contrastive learning for audio-visual segmentation (AVS). 
We first define AVS as a guided binary segmentation task, where audio serves as the guidance for
% should be extensively explored to localize
sound producer(s) segmentation.
% in the visual data.
Based on the conditional setting, we have introduced a conditional latent diffusion model to
% As a conditional generation task, 
% we aim to 
maximize the conditional log-likelihood,
% which can be achieved with a conditional latent diffusion model, 
where the diffusion model is chosen to produce semantic correlated latent space.
Specifically, our latent diffusion model learns the conditional ground truth feature generation process, and the reverse diffusion process can then
% Especially, we have developed a latent diffusion model to perform the latent code of segmentation map generation conditioned on the audio-visual input, thus the reverse diffusion process can 
restore the ground-truth information during inference. 
Contrastive learning has been studied to further enhance the discriminativeness of the conditional variable, leading to mutual information maximization between the conditional variable and the final output. Quantitative and qualitative evaluations on the AVSBench dataset verify the effectiveness of our solution.

%%%%%%%%% REFERENCES
{
\bibliographystyle{IEEEtran}
\bibliography{egbib}
}

%%%%%%%%% Biography
%\bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}

\end{document}


