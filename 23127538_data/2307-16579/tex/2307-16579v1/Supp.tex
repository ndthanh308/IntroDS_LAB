\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{cite}
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{colortbl}
\usepackage{csquotes}
\usepackage[normalem]{ulem}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{paralist}
\usepackage{color}
\usepackage{enumerate}
\usepackage{colortbl}
\usepackage[normalem]{ulem}
\usepackage{bbding}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\usepackage{xcolor}

\def\Fixvv#1{{\color{blue}{}{#1}}}
\newcommand\ien{\emph{i.e.}}

\definecolor{mygray}{gray}{.9}
\definecolor{myred}{rgb}{0.8,0.2,0}
\definecolor{myblue}{rgb}{0,0,1.0}


\def\Jing#1{{\color{magenta}{\bf [Jing:} {\it{#1}}{\bf ]}}}
\def\MYX#1{{\color{red}{\bf [Yuxin:} {\it{#1}}{\bf ]}}}
\def\YC#1{{\color{blue}{\bf [Yuchao:} {\it{#1}}{\bf ]}}}
\def\ZYR#1{{\color{red}{\bf [Yiran:} {\it{#1}}{\bf ]}}}
\def\Mochu#1{{\color{blue}{\bf [Mochu:} {\it{#1}}{\bf ]}}}
\def\Yunqiu#1{{\color{red}{\bf [Yunqiu:} {\it{#1}}{\bf ]}}}


\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\equref}[1]{Eq.~\eqref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{7152} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
% \title{Cross-conditional Latent Diffusion Model for Audio-visual Segmentation}
% \title{Latent Diffusion Model via Contrastive Learning for Audio-visual Segmentation}
\title{Contrastive Conditional Latent Diffusion for Audio-visual Segmentation\\--Supplementary Materials --}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\section{CVAE for AVS}
We compare our approach with conditional variational auto-encode (CVAE)~\cite{structure_output} in our manuscript, and here we describe how CVAE can be applied to AVS tasks.

The CVAE consists of an inference process and a generative process, where the inference process infers the latent variable $\mathbf{z}$ by $p_\theta(\mathbf{z}|\mathbf{X})$, and the generative process
% builds a latent variable $\mathbf{z}$ by $p_\theta(\mathbf{z}|\mathbf{X})$ and obtains 
produces the output via $p_\theta(\mathbf{y}|\mathbf{X},\mathbf{z})$. 
The full pipeline of the CVAE for the audio-visual segmentation task can be shown in Fig.~\ref{fig:model_overview_vae}. And the quantitative comparison in Table 2 in the manuscript can show the superior performance gain between ours and the CVAE based model.
% Figure environment removed

\section{Ablation on Size of The Latent Space}
We perform extra ablation on the size of the latent space. In the manuscript, after parameter tuning, we find $D = 24$ works best. Here, we conduct experiments with different latent sizes. An obvious observation is that the size of the latent space should not be too large ($D=32$) for the diffusion model, which can have significant performance degradation, and we obtain relative stable predictions with the latent code dimension in the range of
% with 
$D\in [16, 24]$.

\begin{table}[!htp]
    \caption{\textbf{Ablation on the size of the latent space,} where we conduct experiments with different latent sizes.}
    \vspace{-2.0mm}
    \label{tab:ablation_on_ldm}
    \centering
    \small
    \setlength{\tabcolsep}{2.5mm}{
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
        \multirow{2}{*}{Latent Size} & \multicolumn{2}{c}{S4}    & \multicolumn{2}{c}{MS3}         \\
        \cmidrule(r){2-3}  \cmidrule(r){4-5}
                & mIoU    & F-score & mIoU    & F-score        \\
        \midrule
        $ {D}= 8 $  & 81.04          & 0.892          & 57.28          & 0.689           \\
        $ {D}=16 $  & 81.18          & 0.895          & 57.98          & 0.704           \\
        $ {D}=24 $  & \textbf{81.38} & \textbf{0.902} & \textbf{58.18} & \textbf{0.709}  \\
        $ {D}=32 $  & 80.78          & 0.891          & 57.01          & 0.687           \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
    \vspace{-2.0mm}
\end{table}

% Figure environment removed

\section{Failure case analysis}
We perform failure case analysis on our proposed method and the AVSBench~\cite{zhou_AVSBench_ECCV_2022}. As shown in Fig.~\ref{fig:Failure}, both ours and AVSBench can not deal with the absenting of segmented objects due to sound interruptions. This is due to the fact that neither we nor AVSBench took into account the \enquote{timing discontinuity} of the sound in the modeling process. However, our proposed method is still able to provide a high-quality sound source localization result.



{\small
\bibliographystyle{unsrt}
\bibliography{egbib}
}

\end{document}