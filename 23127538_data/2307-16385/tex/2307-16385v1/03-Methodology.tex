
The control framework architecture comprises a high-level controller in \MATLAB~that communicates with the low-level \Arduino~microprocessor to result in locomotion of the robot as summarized in \Fig \ref{Fig:ControlArchitecture}. Localization is achieved by analyzing the webcam output and tracking the center of the robot. In the offline state, the Gait Synthesizer uses the motion data from the Euler cycle experiments to build the Gait Library. The gaits therein are then experimentally validated to store the expected motion data to feed into the path planner. In the online state, the experiment world (which maps the obstacles and initial robot pose) and the gait library are used to initialize the path planner. Real-time control is then achieved by comparing the expected pose from the path planner and the instantaneous experiment pose to inform the low-level controller and re-planning is performed as necessary.
% 
\subsection{Localization}
\label{Subsec:Localization}
%The experimental setup consists of the the \MSoRo~on a rubber garage mat, and two overhead webcams. Low-level robot control is performed by the \Arduino and is integrated with high-level planning and localization that is performed in MATLAB\textregistered. The first webcam is used to capture HD video of the experiments. The second webcam has its properties (e.g., contrast, brightness, etc.) adjusted to allow for better image segmentation of the four neon markers on the robot hub to facilitate tracking. Similarly, the obstacles, and goal are identified. The real-time robot pose is estimated as described in \Sec \ref{Subsec:Localization}. The Gait Library described in \Sec \ref{Subsec:GaitSynthesizer} informs path planning described in Sec. \ref{Subsec:TrajectoryPlanning}.

The feedback to the robot controller plays a critical role in path re-planning. As can be seen in the experimental setup, \Fig \ref{Fig:ControlArchitecture}b, two overhead webcams are used. One webcam is used to record HD video; the other is used for localization and has its properties (e.g., contrast, brightness, etc.) adjusted to facilitate image segmentation of the four neon markers on the robot hub. Both webcam videos are processed in parallel using the \MATLAB~Parallel Processing Toolbox\textsuperscript{\texttrademark}.
%The experiment performs localization of the robot using two web-cameras that are processed in parallel using the \MATLAB Parallel Processing Toolbox. The web-cameras have specific tasks - one is dedicated for localization purposes, while the other records HD video. Each of the video stream is processed by separate cores on the microprocessor. 
The ``localization core" video is processed through a image mask that highlights the markers located on the robot (blue markers on orange robot), obstacles (pink) and the target (green cross), as seen in \Fig \ref{Fig:ControlArchitecture}. This video is stored and a pollable data queue accesses the data for localization at appropriate times. The pose estimation is performed using Arun's method \cite{arun1987least}, which finds the least-squares solution to the pose by using singular value decomposition. Occlusion of the markers by the tether is managed by identifying the marker in the next time frame using the nearest neighbor. The occluded markers are reconstructed using the estimated pose. 
\textadded{Camera capture occurs at $\sim 30$Hz. 
The robot pose estimation provides feedback at $6$Hz on an Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} E5-1650 v4, as dictated by the computational complexity and the robot's average motion profile.}
%every 5th frame out of the 30 frames per second is been processed for real time robotâ€™s pose estimation.}

% \begin{algorithm}[h]
% \caption{Robot pose estimation}\label{Alg:PoseEstimation}
% \KwData{Two sets of markers $\bm{p}_i,\bm{p}_i'$ related by\\$\displaystyle \bm{p}_i'=Rp_i+\bm{t}+\bm{n}_i,~\forall i=1,2,\cdots, N$}
% \KwResult{$R \in SO(3), ~\bm{t}\in\Re^{3\times 1}$}
% { $\displaystyle \bm{p} \gets {\frac{1}{N}\sum_{i=1}^N \bm{p}_i}, \quad \bm{p}' \gets {\frac{1}{N}\sum_{i=1}^N\bm{p}_i'}$\;
% $\displaystyle \bm{q}_i \gets {\left(\bm{p}_i-\bm{p}\right)}, \quad  \bm{q}_i' \gets {\left(\bm{p}_i'-\bm{p}'\right)}$\;
% $\displaystyle H \gets {\frac{1}{N}\sum_{i=1}^N \bm{q}_i \bm{q}_i'}$. 
% Find SVD of $H =U\Lambda V^T$\; 
% $R \gets {VU^T}, \quad \bm{t} \gets {\bm{p}'-R \bm{p}}$\;
% }
% \end{algorithm}
% Figure environment removed
%%%%%%%%%%%
\subsection{Gait Library}
\label{Subsec:GaitSynthesizer}
Gaits are synthesized as described in \Sec \ref{SubSec:GaitSynthesis}. For this research, five gaits are chosen - a rotation dominant gait $R$ and a translation dominant gait $T_1$ with its permutations $T_2$, $T_3$, and $T_4$. The limb actuation patterns are shown in \Fig \ref{Fig:GaitLibrary}a. %below:
% \begin{center}
% % Figure removed
% \end{center}
% The red indicate the actuated limb, while dotted black are the unactuated limbs. The numbers in bracket in front of each gait indicate the edge that is activated, e.g., `Gait G: [3 15 1]' implies that the simple cycle representing Gait G traverses edges $\{e_3,e_{15},e_1\}$ and only the $3,15,1$ elements of the gait vector $\mathbf{z}_G$ are non-zero. %
%%%%%%%%%

Each of the gaits are run for 120 gait cycles and the mean locomotion twist $\xi(\bm{p_i},\theta_i)$ is obtained that can be used by the path planner. The mean translation and rotation for each of these gaits is visually shown in \Fig \ref{Fig:GaitLibrary}. These plots highlight a key observation: the translation-dominant gaits $T_1$, $T_2$, $T_3$, and $T_4$ are offset from each other by 90 degrees as expected \textadded{(due to the limbs being positioned at 90 degree offsets)}. As the robot is fabricated to be rotationally symmetric, \textadded{we anticipated} that the twist magnitudes of these gaits would be identical; after all, they are the same gait \textadded{with actuation initiated by a different limb}. However, this is not true and highlights the sensitivity of soft robots to small manufacturing inaccuracies/non-uniformities. \textadded{While control parameters could potentially be adjusted to reduce these differences in behavior, it would be unlikely to eliminate them completely as the frictional effects appear to dominate.} Moreover, soft robots are sensitive to small changes in the environment (e.g., small bumps in the substrate) as suggested by the error bars in \Fig \ref{Fig:GaitLibrary}b. \textadded{The data-driven gait discovery and the path planning strategy accommodate these asymmetries and variations by treating each starting limb option as a distinct gait with individually (experimentally-) derived behaviors; feedback-based re-planning additionally serves to mitigate undesirable effects.}

%  % Figure environment removed
% % Figure environment removed


\subsection{Trajectory Planning}
\label{Subsec:TrajectoryPlanning}
\input{Planner}

\subsection{Path Recalculation}
As observed, the locomotion gaits have both rotation and translation associated with them. As the robot performs gait cycles and switches gaits, the pose error does not always monotonically increase. Generally, the position error reaches some maximum value and then decreases. This observation is illustrated in \Fig \ref{Fig:GatiSwitchingError}. Consequently, path recalculation should be performed when the position error exceeds a user-defined error threshold upon completion of a gait sequence and/or at user-defined intervals (i.e., every $n$ gait cycles). 
%Consequently, the path-recalculation is performed upon completion of a gait sequence and then comparing the position error with the user-defined threshold error.
% Figure environment removed


% % Figure environment removed
% % Figure environment removed
