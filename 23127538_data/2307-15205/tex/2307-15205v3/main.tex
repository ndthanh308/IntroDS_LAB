\documentclass[lineno]{biometrika}
% add \nolinenumbers after \begin{document} to remove line numbers
% also modify biometrika.cls to remove materials relate to Biometrika
% comment \markboth{P. Fearnhead et~al.}{Biometrika style} on line 91

\usepackage{amsmath,amssymb}
%\usepackage{graphics}

%% Please use the following statements for
%% managing the text and math fonts for your papers:
%\usepackage{times}
%\usepackage[cmbold]{mathtime}
%\usepackage{bm}

\usepackage{hyperref}
\usepackage{newtxtext}
\usepackage[subscriptcorrection]{newtxmath}
\usepackage{color}

%\usepackage{natbib}

\graphicspath{{./art/}}

\usepackage[ruled,noend]{algorithm2e}
\SetKwRepeat{Do}{do}{while}
\makeatletter
\renewcommand{\algocf@captiontext}[2]{#1\algocf@typo. \AlCapFnt{}#2} % text of caption
\renewcommand{\AlTitleFnt}[1]{#1\unskip}% default definition
\def\@algocf@capt@plain{top}
\renewcommand{\algocf@makecaption}[2]{%
  \addtolength{\hsize}{\algomargin}%
  \sbox\@tempboxa{\algocf@captiontext{#1}{#2}}%
  \ifdim\wd\@tempboxa >\hsize%     % if caption is longer than a line
    \hskip .5\algomargin%
    \parbox[t]{\hsize}{\algocf@captiontext{#1}{#2}}% then caption is not centered
  \else%
    \global\@minipagefalse%
    \hbox to\hsize{\box\@tempboxa}% else caption is centered
  \fi%
  \addtolength{\hsize}{-\algomargin}%
}
\makeatother

%%% User-defined macros should be placed here, but keep them to a minimum.
\def\Bka{{\it Biometrika}}
\def\AIC{\textsc{aic}}
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\v{{\varepsilon}}

%\addtolength\topmargin{35pt}
\DeclareMathOperator{\Thetabb}{\mathcal{C}}


\newtheorem{claim}{Claim}
\newcommand{\ep}{\textsf{E}}
\newcommand{\var}{\textsf{Var}}
\newcommand{\cov}{\textsf{Cov}}
\newcommand{\B}{\text{B}}
\renewcommand{\P}{\text{P}}
\newcommand{\iidsim}{\stackrel{\text{iid}}{\sim}}
\renewcommand{\d}{{\text{diff}}}
\renewcommand{\,}{{\text{, }}}
\newcommand{\covp}{\textsf{Cov}_\text{P}}
\newcommand{\epp}{\textsf{E}_\text{P}}
\newcommand{\varp}{\textsf{Var}_\text{P}}
\newcommand{\covb}{\textsf{Cov}_\text{B}}
\newcommand{\epb}{\textsf{E}_\text{B}}
\newcommand{\varb}{\textsf{Var}_\text{B}}



\begin{document}

\nolinenumbers

\jname{Biometrika}
%% The year, volume, and number are determined on publication
\jyear{2024}
\jvol{103}
\jnum{1}
\cyear{2024}
%% The \doi{...} and \accessdate commands are used by the production team
%\doi{10.1093/biomet/asm023}
\accessdate{Advance Access publication on 31 July 2023}

%% These dates are usually set by the production team
\received{2 January 2017}
\revised{1 August 2023}

%% The left and right page headers are defined here:
%\markboth{P. Fearnhead et~al.}{Biometrika style}

%% Here are the title, author names and addresses
\title{A new robust graph for graph-based methods}

\author{Yejiong Zhu}
\affil{Department of Statistics, University of California, Davis,\\ California, U.S.A
\email{yjzhu@ucdavis.edu}}

\author{Hao Chen}
\affil{Department of Statistics, University of California, Davis\\ California, U.S.A \email{hxchen@ucdavis.edu}}

\maketitle

\begin{abstract}
Graph-based two-sample tests and change-point detection are powerful tools for analyzing high-dimensional and non-Euclidean data, as they do not impose distributional assumptions and perform effectively across a wide range of scenarios. These methods utilize a similarity graph constructed from the observations, with $K$-nearest neighbor graphs or $K$-minimum spanning trees being the current state-of-the-art choices. However, in high-dimensional settings, these graphs tend to form hubs -- nodes with disproportionately large degrees -- and graph-based methods are sensitive to hubs. To address this issue, we propose a robust graph that is significantly less prone to forming hubs in high-dimensional settings. Incorporating this robust graph can substantially improve the power of graph-based methods across various scenarios. Furthermore, we establish a theoretical foundation for graph-based methods using the proposed robust graph, demonstrating its consistency under fixed alternatives in both low-dimensional and high-dimensional contexts.
\end{abstract}

\begin{keywords}
Nonparametrics; Curse of dimensionality; Two-sample testing; Change-point detection; Edge-count tests
\end{keywords}

\section{Introduction}\label{sec: 1}
Two-sample hypothesis testing is a fundamental task in statistics and have been extensively explored. Nowadays, the growing prevalence of complex data in various fields like genomics, finance, and social networks has led to a rising demand for methods capable of handling high-dimensional and non-Euclidean data \citep{bullmore2009complex,koboldt2012comprehensive,feigenson2014disorganization, beckmann2021downregulation}. Parametric approaches are limited in many ways when dealing with a large number of features and various data types as they are often confined by particular parametric families.

 In the nonparametric domain, there were numerous advancements over the years. \cite{friedman1979multivariate} proposed the first practical method that can be applied to data in an arbitrary dimension.
This method (we call it the \emph{original edge-count test} (OET) for easy reference) involved constructing the minimum spanning tree, which is a tree connecting all observations such that the sum of edge
lengths that are measured by the distance between two endpoints is minimized, and counting the number of edges connecting observations from different samples. Later, researchers applied this method to different similarity graphs, including the $K$-nearest neighbor graph ($K$-NNG) \citep{schilling1986multivariate,henze1988multivariate} and the cross-match graph \citep{rosenbaum2005exact}. More recently, \cite{chen2017new} renovated the test statistic by incorporating an important pattern caused by the curse of dimensionality, and proposed the $\emph{\text{generalized edge-count test}}$ (GET). They applied the GET on $K$-minimum spanning tree\footnote{$K$-MST: an undirected graph built as the union of the $1$st, $\cdots$ $K$th MSTs, where the 1st MST is the minimum spanning tree, and the $k$th $(k> 1)$ MST is a tree connecting all observations that minimizes the sum of distance across edges subject to the constraint that it does not contain any edges in the 1st, $\cdots$ $(k-1)$th MST(s).} ($K$-MST) and recommended using the $5$-MST for the test. GET on $5$-MST exhibits substantial power improvement over OET for a wide range of alternatives. Since then, two additional graph-based tests have been proposed: the weighted edge-count test (WET) \citep{chen2018weighted} and the max-type edge-count test (MET) \citep{chu2019asymptotic}. WET focuses on location alternatives, while MET performs similarly to GET and has some advantages under the change-point detection setting. Recently, \cite{bhattacharya2020asymptotic} studied the asymptotic distribution of OET on $K$-NNG or $K$-MST with a constant $K$ under general alternatives by using the theory of stabilizing geometric graphs in the Poissonized setting. \cite{zhu2024limiting} studied the asymptotic distributions of OET, GET and MET on graphs that range from sparse to dense, and found that using $K$-MST or $K$-NNG, where $K$ is proportional to $N^\beta$ ($N$ being the sample size, $\beta > 0$), significantly increases the power under general alternatives, compared to a constant $K$. Since all these tests (OET, GET, WET and MET) are based on a similarity graph, they are referred to as the \textit{graph-based tests}. 

Other nonparametric two-sample tests have also been proposed, including those based on Maximum Mean Discrepancy (MMD) \citep{gretton2008kernel, gretton2012kernel, gretton2012optimal}, Ball Divergence \citep{pan2018ball}, and measure transportation \citep{deb2021multivariate}.
Among these nonparametric approaches, the graph-based edge-count methods have an important niche given their good performance and easy type I error control \citep{zhu2024limiting}. Figure \ref{estimated_power_various_tests} shows the estimated power of GET on the 5-NNG (GET-5) and on the $\sqrt{N}$-NNG (GET-sqrtN), the cross match test (CM) \citep{rosenbaum2005exact}, the test based on MMD (MMD) \citep{gretton2012kernel}, the test based on the Ball Divergence (BD) \citep{pan2018ball}, and a mutivariate rank-based test (MT) \citep{deb2021multivariate} under the following scenarios: 
\begin{enumerate}
    \item[(i)] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{u}{\sqrt{d}}\mathbf{1}_d,$ $ \Sigma_d(0.5)+\frac{u}{\sqrt{d}}\mathbf{I}_d)$,
    
    \item[(ii)] $X_1,{\cdots},X_m \iidsim {\text{Lognormal}}(\mathbf{0}_d,\Sigma_d(0.6))$, $Y_1,{\cdots},Y_n \iidsim {\text{Lognormal}}(\mathbf{u}_1,\Sigma_d(0.2))$,
    
    \item[(iii)] $X_1,{\cdots},X_m\iidsim t_5(\mathbf{0}_d,\Sigma_d(0.6))$,$Y_1,{\cdots},Y_n \iidsim t_5(\mathbf{u}_2,\Sigma_d(0.6))$,
\end{enumerate}
where $\mathbf{0}_d$ is a $d$-dimensional vector with elements $0$, $\mathbf{1}_d$ is a $d$-dimensional vector with elements $1$, $\mathbf{u}_1$ is a $d$-dimensional vector with first $\sqrt{d}$ elements equal to $u$ and the remaining elements equal to $0$, $\mathbf{u}_2$ is a $d$-dimensional vector with first $d^{1/3}$ elements equal to $u$ and the remaining elements equal to $0$, $\mathbf{I}_d$ is a $d$-by-$d$ identity matrix, $\Sigma_d(r) = (r^{|i-j|})_{1\leq i,\space j\leq d}$, $m = n = 100$ and $d=500$. We can see that the GET test on the $K$-NNG  has good performance under location and scale differences for symmetric and asymmetric distributions, while other tests that work well under one setting could fail under some other settings.

% Figure environment removed

The good performance of GET on the $K$-NNG is attributed to the test statistic's responsiveness to patterns in $K$-NNG and $K$-MST that emerge due to the curse of dimensionality (see \cite{chen2017new}). While this statistic has been renovated to handle high-dimensional data in \cite{chen2017new}, the similarity graphs -- $K$-NNG and $K$-MST --  remain vulnerable to dimensional challenges, leading to properties that are not ideal for the graph-based framework (for details, see Section \ref{sec:1.2}). In this paper, we propose a robust graph that effectively mitigates these unfavorable characteristics in constructing the $K$-NNG.  Details of the robust graph are provided in Section \ref{sec:2}. We then study the asymptotic properties of the GET statistic on the robust graph under the null hypothesis and under the alternative hypotheses in Section \ref{sec:3}. The performance of GET on the robust graph is compared with leading existing graph-based methods and other popular methods through extensive numerical studies of two-sample testing and change-point detection problems in Section \ref{sec:4}. The new methods are illustrated through an analysis of the New York taxi dataset {in Section \ref{sec:real_example}}. The paper concludes with discussions in Section \ref{sec:conclusion}.

 %the fact that the statistic takes into account a pattern in $K$-NNG and $K$-MST caused by the curse of dimensionality \citep{chen2017new}. However, even though the statistic has been renovated for dealing with high-dimensional data, we notice that the similarity graphs, $K$-NNG and $K$-MST, are still affected by the curse of dimensionality (for details, see Section \ref{sec:1.2}). 

\section{Dimensionality effects on GET using $K$-NNG}\label{sec:1.2}

For graph-based methods, GET on the $K$-MST or the $K$-NNG is usually recommended due to their relatively high power across a broad range of alternatives \citep{chen2017new, chu2019asymptotic}.  For simplicity, in the following, we refer to this subset of methods as `graph-based methods' unless otherwise specified. Since using the $K$-MST or the $K$-NNG generally produce similar results, our main focus will be on GET with the $K$-NNG due to its relatively ease of constructions. Additional results for GET on the $K$-MST are provided in the Supplemental Material.% \citep{supplement}.

We are concerned with identifying which types of observations or graph structures could affect the performance of these methods. We begin by examining the effect of outliers, defined as observations that are far from the majority of the data. For exploration, we use a simple setting: $X_1, {\cdots}, X_{100} \iidsim N(\mathbf{0}_d, \mathbf{I}_d)$, $Y_1, {\cdots}, Y_{100} \iidsim N(\mathbf{0}_d, \sigma^2\mathbf{I}_d)$. We consider two types of perturbations:
\begin{itemize}
    \item[ (1)] Random perturbation: mislabel the sample labels of 5 randomly chosen observations, i.e. if a randomly chosen observation belongs to sample $X$, we label it as sample $Y$ and vice versa.
    \item[ (2)] Outlier perturbation: mislabel the sample labels of 5 observations that are furthest away from the center of the data.
\end{itemize}


% Figure environment removed

Figure \ref{Estimated power on 5NNG and 14NNG} plots the estimated power of GET on $5$-NNG and $14$-NNG ($14\approx \sqrt{100+100}$), with $\sigma^2$  ranging evenly from 1 to 1.02 and $d = 1,000$, under no perturbation, random perturbation and outlier perturbation.
We observe that, compared to random perturbation, mislabeling observations farthest from the center decreases the power of test slightly more. This indicates that outliers are generally a bit more influential than randomly selected observations. However, the decrease in power is not substantial. This result is expected because the number of edges in the $K$-NNG that connect to the far-away observations is relatively small as other observations typically do not consider these distant observations as their nearest neighbors. Therefore, the method is quite robust to outliers.

Then, following the same reasoning, if there are observations that connect to many other observations in the similarity graph, will they significantly affect the method? To investigate this, we examine another type of perturbation:
\begin{itemize}
    \item [(3)] Hub perturbation: mislabel the sample labels of 5 observations with the largest degrees in the graph.
\end{itemize}

% Figure environment removed

Figure \ref{Estimated power with inlier} plots the estimated power of GET under the same setting as in Figure \ref{Estimated power on 5NNG and 14NNG} but with hub perturbation. We observe that mislabeling the sample labels of 5 observations with the largest degrees can dramatically decrease the power of the test. While using a denser graph (right panel of Figure \ref{Estimated power with inlier}) may mitigate this effect to some extent, there is still a significant reduction in power under hub perturbation. The high influence of hubs on the performance of the method is due to the method's reliance on the number of edges; an observation with a large degree significantly affects the count, thereby exerting a substantial influence. 

To assess how large the degree of the hub compares to other observations, we plot the boxplots of average degrees of the selected observations under the three ways of perturbations (Figure \ref{degrees of mislabeled points}). We find that the average degree of five observations with the largest degrees in the $5$-NNG is around 50, while the average degree of five randomly selected observations is only about 10. On the other hand, the average degree of five observations farthest from the center is even smaller. A similar pattern is observed in the $14$-NNG.

% Figure environment removed


\subsection{Relationship between hub and dimensionality}\label{sec:hub}
The above example underscores the substantial impact of hubs on the method's performance. Notably, in moderate to high dimensions, the presence of hubs in the $K$-NNG is common. \cite{radovanovic2010hubs} investigated the phenomenon of hubness in the $K$-NNG for data from one distribution.  Their research showed that the degree distribution becomes significantly right-skewed with increasing dimensionality.  We observe the same phenomenon  for data from two distributions.

Figure \ref{degree_distribution_dim_5_500} plots the empirical degree distributions of the $5$-NNG with the data from the standard multivariate normal distribution (top panel) and the previous simple example with two distributions ($\sigma^2 = 1.02$) (bottom panel) under dimensions 5, 10 and 50. We see that both degree distributions -- whether under the standard multivariate normal distribution or the two-distribution setting -- exhibit a more pronounced right-skewed pattern as dimension increases.

% Figure environment removed

% Figure environment removed

The boxplot in Figure \ref{avg_deg_vs_dim} illustrates the average degree of  selected observations under the three perturbations across various dimensions, ranging from $d=5$ to $d=500$. At low dimensions $(d=5)$, the average degree of the five observations with the largest degrees marginally surpasses that of five randomly selected observations. However, as the dimension increases, the average degree of randomly selected observations remains relatively constant, while the average degree of the five observations with the largest degrees significantly increases, leading to the formation of hubs. The marked increase in degree becomes particularly evident when the dimension reaches around 50.

%  we see that the average degree of  five observations with the biggest degrees slightly exceeds that of 5 randomly selected observations when the dimension is low $(d=5)$. As the dimension increases, the average degree of the randomly selected nodes remains relatively stable, while the average degree of the five observations with the biggest degrees experiences a significant increase, forming hubs. This significant increase in degree occurs when the dimension is around 50.  


% Figure environment removed

Figure \ref{estiamted power with d} displays the estimated power of GET on both the $5$-NNG and the $14$-NNG  across various dimensions. The estimated power remains relatively stable across all dimensions for scenarios with  no perturbation and outlier perturbation. However, in the case of hub perturbation, the estimated power starts lower than that of no perturbation and outlier perturbation at low dimensions and shows a significant decline until the dimension reaches around 50, after which the decline becomes more gradual. This trend is consistent with the observed increase in the average degree of the five observations with the largest degrees, as depicted in Figure \ref{avg_deg_vs_dim}.

%slightly lower compared to other perturbations at low dimensions and exhibits a significant decline with a moderate increase in dimension.  Notably, the estimated power with hub perturbation experiences a pronounced decrease until it reaches dimension 50, after which the decline becomes more gradual.  This pattern is consistent with the observed increase in average degrees of five observations with the largest degrees illustrated in Figure \ref{avg_deg_vs_dim}.




\section{\label{sec:2} A Robust similarity graph}
Given that GET is sensitive to the presence of hubs, and hubs tend to form in the $K$-NNG (a similar issue can be observed with the $K$-MST as well, see Supplement S4), particularly when the dimension is moderate to high, we propose a modification to the $K$-NNG to prevent excessively large degrees within the graph. Specifically, we introduce a penalty on the summation of the squares of the degrees to manage this issue.  Let $Z_1,{\cdots}, Z_N$ represent the pooled observations ($N =n+m$), and let $D(\cdot,\cdot)$ be a distance metric. We define $R_i(Z_j)$ as the rank of the distance $D(Z_i,Z_j)$ among the set of distances $\{D(Z_i,Z_l): l \neq i\}$.  The traditional $K$-NNG seeks to minimize $\sum_{i=1}^N\sum_{x \in C_i}R_i(x)$ across all possible sets $C_i$, where each $C_i$ contains $K$ observations excluding $Z_i$. For a graph $G$, $|G|$ represents the number of edges, and $|G_i|$ indicates the degree of the $i$-th node, counting both in-degrees and out-degrees. Thus, for the $K$-NNG, $|G| = KN$ and $\sum_{i=1}^N|G_i|= 2KN$. We introduce the robust $K$-nearest neighbor graph (r$K$-NNG), which minimizes the following objective function over all sets $C_i$:
\begin{align}
\label{obj1}
    \sum_{i=1}^N\sum_{x \in C_i}R_i(x)+\lambda\sum_{i=1}^N|G_i|^2.
\end{align}
Here, $\lambda$ is a hyper-parameter whose selection is discussed in Section \ref{sec: choice of lambda}. Optimizing this objective function poses a combinatorial challenge, and finding the global optimum is typically difficult.  As a practical solution, we propose a greedy algorithm (Algorithm \ref{alg:penalized KNN}).  While this algorithm may not ensure a global optimum, we find it to be effective enough in practice.

% \begin{algo}
% Constructing the $K$-robust nearest neighbor graph.\label{alg:penalized KNN}
% %\vspace*{-12pt}
% \begin{tabbing}
%    \qquad \enspace Initialize $G$ with the $K$-NNG, and compute the value of the objective function (\ref{obj1}) on $G$ and store it as $L$\\
%    \qquad \enspace Randomly permute the order of nodes and indicate them to be $1,{\cdots},N$\\
%    \qquad \enspace For $i=1$ to $N$\\
%    \qquad \qquad Compute $W_{i}(j) = R_i(Z_j) + \lambda(|G_j^\star|+1)^2$, where $|G_j^\star| = |G_j|-1$ if node $j$ is one of neighbors of node $i$; otherwise $|G_j^\star| = |G_j|$\\
%    \qquad \qquad Find $K$ nodes with the $K$ smallest $W_{i}(j)$'s among $\{W_{i}(j)\}_{j= 1,{\cdots},i-1,i+1,{\cdots},N}$\\
%    \qquad \qquad Compute objective function (\ref{obj1}) with node $i$ connecting to these $K$ nodes found in Step 2.2  and denote it as $L^\star$\\
%    \qquad \qquad If $L^\star <L$, update the graph by pointing node $i$ to the $K$ nodes found Step 2.2 and Let $L = L^\star$; otherwise do not change the graph or the value of $L$\\
% \qquad \enspace Repeat Step 2 until no node can find neighbors with a lower $L^\star$. $s$
% \end{tabbing}
% \end{algo}

\begin{algorithm}
\caption{Constructing the robust $K$-nearest neighbor graph (r$K$-NNG)}\label{alg:penalized KNN}\
Initialize $G$ with the $K$-NNG, and compute the value of the objective function (\ref{obj1}) on $G$ and store it as $L$\;
Randomly permute the order of the $N$ observations and indicate them to be $1,{\cdots},N$\;
$i\gets 1$\;
\While{ $L$ still can be further decreased }{
\begin{itemize}
    \item[2.1] \For{$j = 1, j\leq N, j\neq i$}{Compute $W_{i}(j) = R_i(Z_j) + \lambda(|G_j^\star|+1)^2$, where $|G_j^\star| = |G_j|-1$ if node $j$ is one of neighbors of node $i$ in $G$; otherwise $|G_j^\star| = |G_j|$;}
    \item[2.2] Find $K$ nodes with the $K$ smallest $W_{i}(j)$'s among $\{W_{i}(j)\}_{j= 1,{\cdots},i-1,i+1,{\cdots},N}$;
    \item[2.3] Compute objective function (\ref{obj1}) on $G^\prime$, where $G^\prime$ contains the same edges in $G$ but the $K$ edges starting from node $i$ are connected to $K$ nodes found in Step 2.2, denote the value as $L^\prime$;
    \item[2.4] If $L^\prime <L$, update $G$ with $G^\prime$ and Let $L = L^\prime$; otherwise do not change the graph $G$ or the value of $L$.
    \item[2.5] $i\gets (i+1)$ $\mathbf{mod}$ $N$
\end{itemize}
}
\end{algorithm}


\begin{remark}
In the objective function (\ref{obj1}), the regularization term utilizes the total degree $|G_i|$, which is effectively equivalent to using only the in-degree, as the out-degree for each node is a constant $K$ in the r$K$-NNG.

% we use the total degree $|G_i|$ in the regularization term, which is equivalent to use the in-degree here because the out-degree and the graph size are fixed here to be $K$ and $KN$.
\end{remark}

\begin{remark}
The objective function (\ref{obj1}) is not limited to ranks. We could use the distance metric $D(\cdot,\cdot)$ directly, and the modified robust graph can be obtained by solving
\begin{align*}
    &\min_{C_i\text{'s}} \sum_{i=1}^N\sum_{x\in C_i}D(Z_i,x)+\lambda\sum_{i=1}^N|G_i|^2\\
    &\text{s.t. } Z_i\notin C_i, |C_i| = K.
\end{align*}
\end{remark}

\begin{remark}\label{robust K MST}
A similar idea can be used to extend the $K$-MST to the robust $K$-MST (r$K$-MST). Let $R(Z_i,Z_j)$ be the rank of distance $D(Z_i,Z_j)$ in the set of all pairwise distances.
The robust $K$-MST is a $K$-spanning tree that minimizes the objective function 
\begin{align*}\label{obj2}
    \sum_{(z_i,z_j) \in T}R(z_i,z_j)+\lambda\sum_{i=1}^N |T_i|^2,
\end{align*}
where $T$ is a $K$-spanning tree, and $|T_i|$ is the degree of node $i$ in $T$.
\end{remark}

\subsection{The performance of GET on the r$K$-NNG} \label{sec: mitigated effect of K-RNNG}
We apply GET on the r$K$-NNG to the same simulation settings as in Figures \ref{Estimated power on 5NNG and 14NNG}, \ref{Estimated power with inlier} and \ref{estiamted power with d}. The results are plotted in Figures \ref{estimated_power on K-RNNG} and \ref{power of KRNNG with d}. Notably, the power of the method significantly improves under hub perturbation when using the robust $K$-NNG, as illustrated by the comparison between the blue solid lines (robust $K$-NNG) and the blue dashed lines (standard $K$-NNG).



% Figure environment removed


% Figure environment removed

% Figure environment removed

Figure \ref{avg_deg_vs_dim_of K-RNNG} displays the boxplots of the average degree of perturbed observations in the robust $5$-NNG and $14$-NNG under a similar setting as that in Figure \ref{avg_deg_vs_dim} for dimensions 5, 10, 50, 100, 200, and 500. We see that the average degrees of the five observations with largest degrees in the robust 5-NNG and the robust 14-NNG are considerably smaller compared to their counterparts in the 5-NNG and the 14-NNG, as presented in Figure \ref{avg_deg_vs_dim}. For instance, when the dimension increases from 5 to 500, the average degree of the five largest degrees in the $5$-NNG rises from 17 to 50.  However, in the robust $5$-NNG, this average degree only experiences a modest increase, from 14 to approximately 22. 

Further analysis from Figures \ref{estimated_power on K-RNNG} and \ref{power of KRNNG with d} indicates that the power of GET on the r$K$-NNG also improves under other perturbations (outlier and random perturbation) and even with no perturbation, compared to the $K$-NNG.  We next investigate whether this improvement is universal. We employ the following settings:

\begin{itemize}
    \item[1.] $X_1, {\cdots}, X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, $ $\Sigma_d(0.5))$,
    \item[2.] $X_1,{\cdots}, X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, $ $\Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
    \item[3.] $X_1,{\cdots}, X_m \iidsim \text{Lognormal}(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Lognormal}(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5))$,
    \item[4.] $X_1,{\cdots}, X_m \iidsim \text{Multivariate t}_5(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_5(\frac{\delta}{\sqrt{d}}\mathbf{1}_d,\\ \Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
\end{itemize}
where $m=n=100$, $d = 500$. Setting 1 examines a mean shift in a multivariate normal distribution, while Settings 2, 3, and 4 include both mean and variance differences under various distributions. The estimated power for different signal strength ($\delta$) are plotted in Figure \ref{KRNNG_vs_KNNG}.

% Figure environment removed

We observe that when there is only a mean difference, the performance of GET on the robust $K$-NNG and the standard $K$-NNG is comparable. However, when there are variance differences, GET on the robust $K$-NNG generally outperforms that on the standard $K$-NNG.  To understand why this is the case, we look into the GET statistic: \begin{align*}
    S = \begin{pmatrix}R_1-\epp(R_1),& R_2-\epp(R_2) \end{pmatrix}
    \varp\binom{R_1}{R_2}^{-1} \begin{pmatrix}R_1-\epp(R_1) \\R_2-\epp(R_2)\end{pmatrix},
\end{align*}
where $\epp$, $\varp$ and $\covp$ are the expectation, variance and covariance under the permutation null distribution, which assigns equal probability to each selection of $m$ observations from  $N$  as sample $X$. 
\cite{chu2019asymptotic} showed that the statistic $S$ can be decomposed as  $S = (Z_w^\P)^2 + (Z_\d^\P)^2,$
where $$Z_w^\P = \frac{R_w-\epp(R_w)}{\varp(R_w)^{1/2}}, \quad Z_{\d}^\P = \frac{R_{\d}-\epp(R_\d)}{\varp(R_\d)^{1/2}},\quad \covp(Z_w^\P, Z_\d^\P) = 0,$$ and $R_w = R_1(n-1)/(N-2)+R_2(m-1)/(N-2)$, $R_\d= R_1-R_2$. Prior research has shown that $Z_w^\P$ focuses on detecting mean differences, while $Z_\d^\P$ detects variance differences \citep{chu2019asymptotic, song2022asymptotic, liu2022fast}.  From \cite{chu2019asymptotic}, we have that 
\begin{align*}
    &\epp(R_\d) = \frac{m-n}{N}|G|,\\
    &\varp(R_\d) = \frac{mn(m-1)(n-1)}{N(N-1)(N-2)(N-3)}\left(\frac{m-2}{n-1}+\frac{n-2}{m-1}+2\right)V_G,
\end{align*}
where $V_G = \sum_{i=1}^N(|G_i|-2|G|/N)^2$.  For the robust $K$-NNG, $\sum_{i=1}^N|G_i|^2$ or equivalently $V_G$ is penalized, and thus $\varp(R_\d)$ is reduced, making $Z_\d^\P$ more powerful.

%\subsection{How does r$K$-NNG affect GET?} \label{why rkNGG is better}



% Here, we look into the generalized edge-count test statistic to understand how adding a penalty on the node degrees would affect the performance of the method. 

%We next explore how the robust $K$-NNG affect the GET statistic.
%For any graph $G$ constructed from the pooled observations of the two samples, let $R_1$ be the number of edges in $G$ connecting observations from sample $X$, and $R_2$ be those connecting sample $Y$. The GET statistics is defined as 
%\begin{align*}
%    S = \begin{pmatrix}R_1-\epp(R_1),& R_2-\epp(R_2) \end{pmatrix}
%    \varp\binom{R_1}{R_2}^{-1} \begin{pmatrix}R_1-\epp(R_1) \\R_2-\epp(R_2)\end{pmatrix},
%\end{align*}
%where $\epp$, $\varp$ and $\covp$ are the expectation, variance and covariance under the permutation null distribution, which assigns equal probability to each selection of $m$ observations from  $N$  as sample $X$. 
%\cite{chu2019asymptotic} showed that the statistic $S$ can be decomposed as  $S = (Z_w^\P)^2 + (Z_\d^\P)^2,$
%where $$Z_w^\P = \frac{R_w-\epp(R_w)}{\varp(R_w)^{1/2}}, \quad Z_{\d}^\P = \frac{R_{\d}-\epp(R_\d)}{\varp(R_\d)^{1/2}},\quad \covp(Z_w^\P, Z_\d^\P) = 0,$$ and $R_w = R_1(n-1)/(N-2)+R_2(m-1)/(N-2)$, $R_\d= R_1-R_2$. Prior research has shown that $Z_w^\P$ focuses on detecting mean differences, while $Z_\d^\P$ detects variance differences \citep{chu2019asymptotic, song2022asymptotic, liu2022fast}. As illustrated in Figure \ref{KRNNG_vs_KNNG},  GET on the r$K$-NNG performs similarly to GET on the $K$-NNG when only a mean difference exists. Thus, we next focus on how r$K$-NNG affect $Z_\d^\P$. From Lemma S2 in the Supplemental Material, we have 
%\begin{align*}
%    &\epp(R_\d) = \frac{m-n}{N}|G|,\\
%    &\varp(R_\d) = \frac{mn(m-1)(n-1)}{N(N-1)(N-2)(N-3)}\left(\frac{m-2}{n-1}+\frac{n-2}{m-1}+2\right)V_G,
%\end{align*}
%where $V_G = \sum_{i=1}^N(|G_i|-2|G|/N)^2$  represents the variation of degrees. For the r$K$-NNG, $\sum_{i=1}^N|G_i|^2$ or equivalently $V_G$ is penalized, and thus $\varp(R_\d)$ is reduced.
%



%% Figure environment removed
%
%Figure \ref{ratio} visualizes how $|R_\d|$ and $\varp(R_\d)^{1/2}$ change as $\lambda$ increases from 0 to 1 on the robust 5-NNG across Settings 1-4, with parameters $m=n=100$, $d=500$ and $\delta=2$.  We see that, as $\lambda$ increases, both $|R_\d|$ and $\varp(R_\d)^{1/2}$ decreases, with $\varp(R_\d)^{1/2}$ reducing more swiftly except in Setting 1. This rapid decrease in $\varp(R_\d)^{1/2}$ enhances the power of $Z_\d$ on the r$K$-NNG for small positive values of $\lambda$.

 %The values of $\epp(R_\d)$ depend solely on $m$, $n$, and $|G|$, and since both the $K$-NNG and r$K$-NNG have the same value of $|G|$, these values are not influenced by r$K$-NNG. Then we examine how $R_\d$ and $\varp(R_\d)^{1/2}$ vary with respect to $\lambda$. Figure~\ref{ratio} illustrates the changes in $|R_\d|$ and $\varp(R_\d)^{1/2}$, each divided by their respective values under $K$-NNG ($\lambda=0$), as $\lambda$ increases from 0 to 1 in the robust $5$-NNG across settings 1-4 in Figure \ref{KRNNG_vs_KNNG} with parameters $m=n=100$, $d=500$, and $\delta=2$. Both $|R_\d|$ and $\varp(R_\d)^{1/2}$ decrease as $\lambda$ increases in high-dimensional data, with $\varp(R_\d)^{1/2}$ decreasing more rapidly except in Setting 1. This accelerated reduction boosts the power of $Z_\d$ on r$K$-NNG for small positive values of $\lambda$. In Setting 1, where there is only a mean difference, $|R_\d|$ and $\varp(R_\d)^{1/2}$ decrease at almost the same rate, resulting in an unchanged value of of $Z_\d$ (See Fig.\ref{Z_d under the rK-NN}).

%% Figure environment removed





\subsection{Choice of $\lambda$}\label{sec: choice of lambda}
Here, we explore how to choose $\lambda$. We consider the following scenarios including symmetric distribution, asymmetric distribution, and heavy-tailed distribution.

\begin{itemize}
    \item[(i)] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 200$, $n =100$, $d= 500$ and $\delta = 1.03$;
    \item[(ii)] $X_1,{\cdots},X_m \iidsim \text{lognormal}(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{lognormal}(\delta\mathbf{1}_d,\Sigma_d(0.5))$ with $m = 100$, $n =200$, $d= 1000$ and $\delta = 0.05$;
    \item[(iii)] $X_1,{\cdots},X_m \iidsim \text{lognormal}(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{lognormal}(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 100$, $n =100$, $d= 100$ and $\delta = 1.15$;
    \item[(iv)] $X_1,{\cdots},X_m \iidsim \text{t}_5(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{t}_5(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 100$, $n =100$, $d= 500$ and $\delta = 1.35$;
    \item[(v)] $X_1,{\cdots},X_m \iidsim \text{t}_5(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{t}_5(\delta\mathbf{1}_d, \Sigma_d(0.5))$ with $m = 300$, $n =100$, $d= 500$ and $\delta = 0.095$.
\end{itemize}
where $\delta$ is chosen so that the tests have moderate power when $\lambda$ is equal to zero.

We first examine how the power of the test changes with varying $\lambda$ (Figure \ref{lambda_selection}). There is a rapid increase in power as $\lambda$ rises from 0 to 0.3, regardless of the scenario. After 0.3,  the increase in power slows down, suggesting that $\lambda=0.3$ could be a reasonable choice.

Based on the analysis in Section \ref{sec: mitigated effect of K-RNNG}, the quantity $\sum_{i=1}^N|G_i|^2$ plays a vital role in controlling the variance of $R_\d$. Since $\sum_{i=1}^N |G_i|^2$ depends on $N$,  we plot $\sum_{i=1}^N(|G_i|-2K)^2/N$, which represents the variance of degrees (denoted as $\var(Q_N)$, where $Q_N$ is defined in Theorem \ref{th2}), over $\lambda$ for these scenarios (Figure \ref{lambda_variance}). We observe that $\var(Q_N)$ decreases quickly as $\lambda$ increases from 0 to 0.3, and then the decrease slows. This also supports chosing $\lambda = 0.3$.

In real applications, we are unable to estimate power, but computing $\var(Q_N)$ for different values of $\lambda$ can be easily done. Therefore, we can use the plot of $\var(Q_N)$ over $\lambda$ to choose $\lambda$ if we do not want to use its default value of 0.3.


% In Figure \ref{lambda_variance}, we plot the relationship between the maximum center degree and $\lambda$. It is evident that the maximum degree experiences a rapid decline when $\lambda$ increases from 0 to 0.3, followed by a slower rate of decrease. This observation aligns with the trend depicted in Figure \ref{lambda_selection}, which showcases the estimated power. Consequently, a practical data-driven approach to select an appropriate $\lambda$ is to plot the maximum degrees for various values of $\lambda$ and identify the point at which the maximum degree exhibits minimal changes.

% Figure environment removed

% Figure environment removed


\section{Asymptotic properties of the GET statistic on the r$K$-NNG} \label{sec:3}

\cite{zhu2024limiting} derived  the best sufficient conditions  for the validity of the asymptotic distribution of the GET statistic on undirected graphs. In this section, we extend their results to directed graphs and demonstrate that  robust $K$-NNG graphs meet these conditions with an appropriate choice of $\lambda$. Before presenting these results, we  define some essential notations.

Given a directed graph $G$, let the pair $(i,j)$ (where the order matters) represent a directed edge pointing from node $i$ to node $j$. For each node $i$, we define $G_i$ as the set of edges involving node $i$, $G_{i,2}$ as the set of edges sharing at least one node with an edge in $G_i$, $node_{G_i}$ as the set of nodes  connected in $G_i$ excluding  node $i$, and $node_{G_{i,2}}$ as the set of nodes  connected in $G_{i,2}$ excluding  node $i$. Let $\Tilde{d}_i$ be the centered degree of the $i$-th node, defined as $\Tilde{d}_i = |G_i| - 2|G|/N$. Let $V_G = \sum_{i=1}^N \Tilde{d}_i^2$ represent the variation of degrees. We further define $N_0$ as the number of edges whose reversed edge is also in $G$, i.e. $N_0 = \sum_{\left(i,j\right)\in G}1_{\{(j,i)\in G\}}$, and $N_{sq}$ to be the number of combinations of 4 edges that form a quadrilateral, such as $(i,j),(j,s),(s,t),(t,i)$.

%circle, \textcolor{red}{in which each node is shared by two edges, e.g. $(i,j),(j,s),(s,t),(t,i)$}.

% Given a directed graph $G$ built on the pooled observations $Z_1,{\cdots},Z_N$ $(Z_i = X_i,$ $i = 1,{\cdots},m;$ $Z_{j+m}= Y_j,$ $j = 1,{\cdots},n; N = m+n)$. We define $|G|$ to be the number of edges in the graph G. Let $\Tilde{d}_i$ be the center degree of $i$-th node, i.e. $\Tilde{d}_i = |G_i| - 2|G|/N$. Let $V_G = \sum_{i=1}^N \Tilde{d}_i^2$ representing the variation of degrees. Let $l_i$ be the sample group label of $i$-th node defined as
% \begin{align*}
%     l_i=\left\{
% \begin{aligned}
% 1 & \text{ if node $i$ is from sample X} \\
% 2 & \text{ if node $i$ is from sample Y.}
% \end{aligned}
% \right.
% \end{align*}
% Let $R_1$ and $R_2$ be the number of within-sample edges in sample $X$ and sample $Y$, respectively,
% \begin{align*}
%     R_1 = \sum_{(i,j)\in G} 1_{\{l_i=l_j=1\}},\quad R_2 = \sum_{(i,j)\in G} 1_{\{l_i=l_j=2\}}.
% \end{align*}
% Then, the GET statistic $S$ can be expressed

Additionally, we use $\xrightarrow{\mathcal{D}}$ to denote convergence in distribution, and use `the usual limit regime' to refer to $N\rightarrow \infty$ and $\lim_{N\rightarrow \infty}\frac{m}{N} = p\in (0,1)$. In the following, $a_n= o(b_n)$ or $a_n\prec b_n$ means that $a_n$ is dominated by $b_n$ asymptotically, i.e. $\lim_{n\rightarrow \infty} \frac{a_n}{b_n}=0$, $a_n\precsim b_n$ or $a_n = O(b_n)$ means $a_n$ is bounded above by $b_n$ (up to a constant factor) asymptotically, and $a_n= \Theta(b_n)$ or $a_n \asymp b_n$ means that $a_n$ is bounded both above and below by $b_n$ (up to constant factors) asymptotically. We use $a\wedge b$ for $\min\{a,b\}$. For two sets $S_1$ and $S_2$, $S_1\backslash S_2$ denotes the set  containing elements in $S_1$ but not in $S_2$.

\cite{zhu2024limiting} established the sufficient conditions for the asymptotic distribution of the test statistic using the `locSCBâ€™ approach.  This approach relies on the equivalence between the permutation null distribution and the conditional Bootstrap null distribution. The Bootstrap null distribution assigns each observation to either sample $X$ or sample $Y$ independently, with probabilities $\frac{m}{N}$ and $\frac{n}{N}$, respectively. Conditioning on the number of observations assigned to sample $X$ being $m$, the Bootstrap null distribution becomes the permutation null distribution. The authors applied the Stein's method that considers the first neighbor dependency under the Bootstrap null distribution to derive asymptotic multivariate normality. We  adopt a similar approach to derive the sufficient conditions for  directed graphs. A particular challenge with directed graphs is their allowance for more than one edge between two nodes, necessitating careful consideration of certain graph-related quantities. The sufficient conditions and proof are provided in Supplement Material and are applicable to general directed graphs. For the robust $K$-NNG, a more concise result can be obtained, as stated in Theorem \ref{th2}, with the proof provided in the Supplement S1.

% \footnote{The bootstrap null distribution assigns each observation to sample X with probability $\frac{m}{N}$ and to sample Y with probability $\frac{n}{N}$, independently from any other observations.}
%
% that utilizes the equivalence of the permutation null distribution and the conditional Bootstrap null distribution  conditioning on that the number of nodes labeled as being sample $X$ is $m$, and then applies the local Stein's method under the bootstrap null distribution to show asymptotic multivariate normality. The same idea can be used here to derive the sufficient conditions on the directed graph. Theorem \ref{th1} summarizes these sufficient conditions on the directed graph with the proof provided in Appendix \ref{proof of th1}.

% \begin{theorem}\label{th1}
% For a directed graph $G$ with $|G| = O(N^\alpha), 1\leq \alpha<2$, under conditions 
% \begin{align*}
%    &\sum_{i=1}^{N}\left|G_{i}\right|^{2}=o\left(|G|^{\frac{3}{2}}\right), \sum_{i=1}^N \left|\Tilde{d_i}\right|^3 = o(V_G^\frac{3}{2}), \sum_{i=1}^N \Tilde{d_i}^3 = o(V_G\sqrt{|G|}),\\
%    &\sum_{i=1}^N\sum_{(i,j) \text{ or }(j,i) \in G_i}^{(i,k) \text{ or }(k,i) \in G_i, j\neq k}\Tilde{d_j}\Tilde{d_k} = o(|G|V_G), \quad N_{sq} = o(|G|^2).
% \end{align*}
%  in the usual limit regime, we have $S\xrightarrow{\mathcal{D}} \chi_2^2$ under the permutation null distribution.
% \end{theorem}


\begin{theorem}\label{th2}
Let $Q_N$ be the random variable generated from the degree distribution of the robust $K$-NNG with $N$ nodes. Assume $K = \Theta(1)$. If $\lambda$ is chosen such that $\var(Q_N)>0$ and $ \max\{Q_N\}\precsim N^{\frac{1}{2} -\beta}$ for some $\beta > 0$, then $S\xrightarrow{\mathcal{D}} \chi_2^2$ under the permutation null distribution in the usual limit regime.

\end{theorem}

%\begin{remark}
%    The maximum degree of $K$-NNG is bounded when the dimension is fixed. As a result, the maximum degree of the $K$-RNNG is also bounded from Lemma \ref{lm6}, which leads to the bounded $\var(Q_N)$. This means that $\lambda$ can be selected as $\Theta(1)$ under the fixed dimension.
%\end{remark}

\begin{remark}
Theorem \ref{th2} requires that the variance of degree distribution of the robust $K$-NNG is asymptotically bounded away from zero when choosing $\lambda$.  This ensures that the GET statistic is well defined -- when $\var(Q_N)=0$, the degrees of all nodes are the same, and $\varp\binom{R_1}{R_2}$ becomes singular. This situation arises when an extremely large $\lambda$ is used. Additionally, large values of $\lambda$ diminish the utilization of the similarity information contained in the first term of the objective function \eqref{obj1}, making such choices of $\lambda$ less desirable. Therefore, we tend not to choose a very large $\lambda$ in practice.
\end{remark}


\begin{theorem}[Consistency under fixed dimensions] \label{th3}
For two samples generated 
from two continuous multivariate distributions in Euclidean space with a fixed dimension, if the graph is the r$K$-NNG with $K = \Theta(1)$ and $\lambda \geq 0$, GET is consistent against all alternatives in the usual limiting regime.
\end{theorem}

\begin{theorem}[Consistency under high dimensions]\label{th4}
Assume distributions $F_X$ and $F_Y$ satisfy Assumptions 1 and 2 in \citep{biswas2014distribution}, and that $\lim_{d\rightarrow \infty}\ep(||X-\ep(X)||_2^2)/d = \sigma_1^2$, $\lim_{d\rightarrow \infty}\ep(||Y-\ep(Y)||_2^2)/d = \sigma_2^2$ and $\lim_{d\rightarrow \infty}(||\ep(X)-\ep(Y)||_2^2)/d = v^2$, where $X\sim F_X$, $Y\sim F_Y$ and $d$ is the dimension. Without loss of generality,  assume that $\sigma_1^2 \geq \sigma_2^2$. Then, for GET on the r$K$-NNG with $0<\lambda < (\sqrt{8NK+4N-8K}-\sqrt{8NK})^2/16$ and $\min\{m,n\} > K+2\lambda+\sqrt{8\lambda KN}$, we have $\lim_{d\rightarrow \infty}P(S>\chi_2^2(1-\alpha)) = 1$, for any fixed $\alpha \in (0,1)$, when any of the following conditions hold:
\begin{itemize}
    \item [(1)] $|\sigma_1^2 - \sigma_2^2| < v^2$, $N > 2.5 + \frac{\xi}{K}+\sqrt{0.25+3\frac{\xi}{K} + \frac{\xi^2}{K^2}}$,
    \item [(2)]
    $\sigma_1^2 - \sigma_2^2 > v^2$, $N>\frac{n^2\xi^2}{2m^2K^2}\left(\sqrt{\frac{K}{\lambda}}+\sqrt{\frac{K}{\lambda}+\frac{2mK}{n\xi}(1+\frac{K}{2\lambda}+\frac{mK}{n\xi}-K)}\right)^2$, 
    \item[(3)] $\sigma_2^2 - \sigma_1^2 > v^2$, $N>\frac{m^2\xi^2}{2n^2K^2}\left(\sqrt{\frac{K}{\lambda}}+\sqrt{\frac{K}{\lambda}+\frac{2nK}{m\xi}(1+\frac{K}{2\lambda}+\frac{nK}{m\xi}-K)}\right)^2$, 
\end{itemize}
where $\xi =\chi_2^2(1-\alpha)$ is the $(1-\alpha)$ quantile of the $\chi_2^2$ distribution.
%$\frac{m}{n}K(N-1) > (\frac{K}{2\lambda}+1 + \sqrt{\frac{2NK}{\lambda}}-K)\chi_2^2(1-\alpha)$
\end{theorem}

Theorem \ref{th3} examines the consistency of GET on the r$K$-NNG under a fixed dimension as the sample size goes to infinity. Theorem \ref{th4} explores the consistency as the dimension goes to infinity. The proofs for these theorems are provided in the Supplemental Material. For Theorem \ref{th4}, since it is typically uncertain which case the parameters $\sigma_1^2$, $\sigma_2^2$ and $v^2$ satisfy, the largest value of $N$ among the three cases can be used as the required minimum sample size for implementation. For instance, with $\alpha = 0.05$, $\lambda = 0.3$, $m=n$ and $K = 5$, the requirement is $N \geq 69$. With $\alpha = 0.05$, $\lambda = 0.3$, $m/n = 2$ or $n/m = 2$ and $K = 5$,  the requirement is $N \geq 214$.

\section{Numerical studies}\label{sec:4} In this section, we evaluate the performance of GET on the r$K$-NNG by comparing it with other state-of-the-art methods in both  two-sample testing and change-point detection problems.

\subsection{Two-sample testing}
We consider GET on the robust 5-NNG (New), 5-MST,  $\sqrt{N}$-MST, and other popular tests: the cross-match test \citep{rosenbaum2005exact} (CM), the Ball divergence test \citep{pan2018ball} (BD), the mutivariate rank-based test \citep{deb2021multivariate} (MT), the Adaptable Regularized Hotelling's $\text{T}^2$ test \citep{li2020adaptable} (ARHT), the kernel test based on minimum mean discrepancy \citep{gretton2012kernel} (MMD), and the generalized kernel two-sample test \citep{song2023generalized} (gKernel) under the following simulation scenarios:
\begin{itemize}
    \item[1.] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d,$ $\Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
    \item[2.] $X_1,{\cdots},X_m \iidsim \text{lognormal}(\mathbf{0}_d,\Sigma_d(0.6))$, $Y_1,{\cdots},Y_n \iidsim \text{lognormal}(\mathbf{\Delta},\Sigma_d(0.2))$,
    \item[3.] $X_1,{\cdots},X_m \iidsim \text{Multivariate t}_2(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_2(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5)+\delta\mathbf{I}_d)$,
    \item[4.] $X_1,{\cdots},X_m \iidsim \text{Multivariate t}_1(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_1(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5)+\frac{\delta}{2}\mathbf{I}_d)$,
\end{itemize}
where $\mathbf{\Delta}$ a $d$-dimensional vector with the first $\sqrt{d}$ elements equal to $\delta$ and the remaining elements equal to $0$.

% Figure environment removed

In each scenario, we set $m=n=100$ and $d = 50, 500, 1000$. The estimated powers computed from 1000 repetitions are plotted in Figure \ref{two_sample_sim}. Firstly, we observe that the empirical sizes of the GET on the robust $5$-NNG are well controlled across different scenarios (at $\delta=0$ in Scenarios 1, 3 and 4). In Scenario 1, although the power of the new test is marginally inferior to the power of BD and gKernel, which demonstrate the maximum power in this scenario, it excels in Scenario 2, 3 and 4, and shows significant improvement over BD and gKernel in Scenario 2 and 4. Moreover, the new test consistently surpasses GET on the $5$-MST and GET on the $\sqrt{N}$-MST across all scenarios. These results underpin the advantages of employing the robust $K$-NNG.



\subsection{Change-point detection} \label{numeric study for change point detection}
For graph-based change-point detection, MET is often recommended over GET \citep{chu2019asymptotic,liu2022fast,song2022asymptotic}. In this section, we evaluate the performance of both MET on the robust $5$-NNG and GET on the robust $5$-NNG. We also include in the comparison the GET scan statistic on the $5$-MST, the MET scan statistic on the $5$-NNG, and the distance-based approach in \citep{matteson2014nonparametric, james2013ecp} (e.divisive).  We
consider the following simulation settings:

\begin{itemize}
    \item[1.]  $X_1,{\cdots},X_\tau \iidsim  N(\mathbf{0}_d, \Sigma_d(0.5))$, $X_{\tau + 1},{\cdots}, X_{N} \iidsim N(\frac{\delta}{\sqrt{d}} \mathbf{1}_d,$ $\Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}{\mathbf{I}_d})$;
    
    \item[2.] $X_1,{\cdots},X_\tau \iidsim N(\mathbf{0}_d, \mathbf{I}_d)$, $X_{\tau + 1},{\cdots}, X_{N} \iidsim N(\mathbf{0}_d, \Sigma_d(\delta))$;
    
    \item[3.] $X_1,{\cdots},X_\tau \iidsim \text{Multivariate t}_5(\mathbf{0}_d, \Sigma_d(0.5))$, $X_{\tau + 1},{\cdots}, $ $X_{N}\iidsim \text{Multivariate t}_5(\frac{\delta}{d}\mathbf{1}_d,\\ \delta\mathbf{I}_d + \Sigma_d(0.5))$;
    \item[4.] $X_1,{\cdots}, X_\tau \iidsim \text{lognormal}(\mathbf{0}_d, \Sigma_d(0.5))$, $X_{\tau + 1}, {\cdots}, X_N \iidsim \text{lognormal}(\frac{\delta}{d}\mathbf{1}_d, \Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$.
\end{itemize}


In each setting, we set $N$ to be 400, the true change-point $\tau$ to be at $100,200$ or $300$, and $d$ to be $100$ or $500$. The estimated power is computed as the proportion of trials with a significant $p$-value among 1000 trials, and the accuracy is computed as the proportion of trials with a significant $p$-value and estimated change-point $\hat \tau$ satisfying $|\hat \tau-\tau|\leq 10$, among 1000 trials. The estimated power and accuracy under Setting 1 are plotted in Figure \ref{cp_normal}, and the estimated power and accuracy under other settings are plotted in Supplement S5. We observe that the MET and GET scan statistics on the robust $5$-NNG exhibit good power and accuracy in all settings, while others may perform well under some settings but poorly in others.

% Figure environment removed

\section{Real-data examples}\label{sec:real_example}
We illustrate our new test using New York taxi data, available on the New York City Taxi \& Limousine Commission website \url{https://www1.
nyc.gov/site/tlc/about/tlc-trip-record-data.page}. This dataset includes various details, such as the latitude and longitude coordinates of pickup and drop-off locations, as well as the time of pickups and drop-offs. We analyzed data from the year 2014, examining travel patterns over time.

We preprocessed the data similarly to \citep{chu2019asymptotic} by defining the boundaries of New York City as latitudes ranging from 40.577 to 41.5 and longitudes from -74.2 to -73.6. We divided this geographical area into a 30Ã—30 grid with uniform cell sizes. For each cell in the grid, we counted the number of trips originating from that cell and terminating at Central Park each day. The differences between these daily trip count matrices were quantified using the Frobenius norm.

\subsection{Testing for differences in trip origins to Central Park across consecutive quarters}
We investigate whether there are differences in the origins of trips ending in  Central Park between the first quarter (January, February, March) and the second quarter (April, May, June).  We applied GET on the robust $5$-NNG (new), GET on the standard $5$-NNG, BD, MT, MMD and G-kernel to the data.  The results are presented in Table \ref{two-sample test on New York taxi}, show that GET on the robust 5-NNG, GET on the standard 5-NNG, MT, and G-kernel reject the null hypothesis of no difference at the 0.01 significance level, while BD and MMD have $p$-values slightly above 0.01.  This suggests a difference between these two samples.  

\begin{table}[htp]
	\tbl{$p$-values of various tests comparing trip origins to Central Park between the first and second quarters}
{	\begin{tabular}{crrrrrr}
		\hline
		&  robust $5$-NNG & 5-NNG  & BD& MT & MMD & G-kernel\\ 
		\hline
         $p$-value & $3.89*10^{-15}$ & $1.11*10^{-10}$&0.02 &$9.99*10^{-4}$  & 0.012 & 0.00431 \\
		\hline
	\end{tabular}}
	\label{two-sample test on New York taxi}
\end{table}

%on data from the first quarter (Jan, Feb, Mar) and the second quarter (Apr, May, Jun). Table \ref{two-sample test on New York taxi} records the p-value of these tests.  All the tests reject the null hypothesis at 0.05 significant level, indicating a difference of travel patterns between the two quarters.  Notably, GET on the robust 5-NNG has the lowest $p$-value among these tests. 

\begin{table}[!htp]
	\tbl{Estimated power of various tests on downsampled data}
{	\begin{tabular}{crrrrrr}
		\hline
		& robust $5$-NNG & 5-NNG & BD& MT & MMD & G-kernel \\  \hline
     $m=30$ &\textbf{0.631}& $0.573$ &0 &0.319 & 0.057&0.133\\
	%	\hline
         $m=50$ &\textbf{0.995}& $0.982$ &0 &$0.681$ & 0.110 &0.237 \\
		\hline
	\end{tabular}}
	\label{estimated power two-sample test on New York taxi}
\end{table}

We further downsampled the data by randomly selecting $m$ observations from each sample and performed a two-sample test.  This process was repeated 1000 times, and the estimated power is reported in Table \ref{estimated power two-sample test on New York taxi}.  We observe that GET on the robust $5$-NNG exhibits the highest power, followed by GET on the 5-NNG, and then MT, while the remaining methods exhibit low or no power.

% Further, we compare these methods on down-sampled data by randomly selecting $m$ observations from each group and conducting the test. Table \ref{estimated power two-sample test on New York taxi} shows the results under 1000 repetitions with $m=30,50$. GET on the $5$-NNG, GET on the robust $5$-NNG, and MT demonstrate significantly higher power than the other methods, with GET on the robust $5$-NNG achieving the maximum power.

% Under a significance level of 0.01, BD and MT fail to reject the null hypothesis, indicating an inability to distinguish differences in taxi pick-up patterns between the first and second quarters. Conversely, GET on the robust 5-NNG, GET on the standard 5-NNG, MT, and G-kernel reject the null hypothesis, demonstrating their efficacy in discerning differences. Notably, GET on the robust 5-NNG exhibits the lowest p-value among these tests.




\subsection{Change-point detection in trip origins to Central Park}

We investigate potential changes in the origins of trips ending in Central Park during the period from February 1st to April 30th.  Given that changes in travel patterns are not uncommon, we kept this sequence short to minimize the impact of the multiple testing problem arising from too many change-points, which is not the primary focus of this paper. We applied the GET scan statistic on the robust 5-NNG (new), the MET scan statistic on the robust 5-NNG (new), the GET scan statistic on the  5-NNG, the MET scan statistic on the  5-NNG,  and e.divisive.  For each method, we identified all change-points in the sequence using binary segmentation with a 0.01 significance level. This process involved continuing the detection on the subsequences $X_1,{\cdots}, X_t$ and $X_{t+1},{\cdots}, X_N$ if $t$ was identified as a significantly change-point.  Table \ref{change-point on New York taxi} summarizes the estimated significant change-points for these methods.

\begin{table}[htp]
	\tbl{Estimated change-points}
{	\begin{tabular}{crrrrrr}
		\hline
		 & GET on r5-NNG &MET on r5-NNG& GET on 5-MST & MET on 5-NNG & e.divisive\\ 
		\hline
         &49, 63&49, 61&NULL&49, 61& NULL \\ \hline
	\end{tabular}}
	\label{change-point on New York taxi}
\end{table}

% Figure environment removed

The table shows that the GET scan statistic on the robust 5-NNG,  the MET scan statistic on the robust 5-NNG, and the MET scan statistic on the  5-NNG all report two change-points, one at 49  (March 21st) and another at either 63 (April 4th) or 61 (April 2nd). In contrast, the GET scan statistic on the  5-NNG and e.divisive do not detect any change-points.  Figure \ref{heatmap} presents a heatmap of pairwise distances among observations, where a distinct difference between the region $[49, 63]$ and the rest of the data is evident.  This suggests that the change-points at 49 and 63  (or 61) are highly plausible.  A review of events in Central Park in 2014 reveals that this period aligns with the early spring bloom, a time when Central Park typically sees an increae in visitors. The robust 5-NNG enabled both the GET and MET scan statistics to detect these change-points, while the GET scan statistic on the standard 5-NNG did not.

\iffalse
We further verify the detected change points on March 21st and April 2nd by examining the total number of trips. Figure \ref{total_trips} illustrates a significant reduction in the number of trips during the two-week period within these detected change points. We also applied univariate change-point detection methods
AMOC and PELT to detect change-points for total number of trips. AMOC detected one change-point at $t = 63$ (April 4th) while PELT detected 89 change-points.
% Figure environment removed
\fi

%Firstly, we compare the results of two GET scan statistics. The GET scan method on the $5$-MST failed to detect significant change-points, with the p-value being 0.015. However, the GET scan method on the robust 5-NNG successfully identified two significant change-points at $t=49$ and $t=63$, with p-values of $6.6 \times 10^{-20}$ and $0.0078$, respectively, demonstrating the superiority of the robust graphs in the GET scan statistic. The two MET scan statistics detected the same significant change-points at $t=49$ and $t=61$. We then compared the p-values at these change-points. The MET scan statistic on the 5-NNG yielded a p-value of $0.0092$ at $t=49$ and $1.32 \times 10^{-8}$ at $t=63$, whereas the MET scan statistic on the robust 5-NNG produced a p-value of $0.0087$ at $t=49$ and $1.63 \times 10^{-9}$ at $t=63$. At both detected change-points, the MET method on the robust 5-NNG yielded lower p-values. Additionally, the e.divisive approach did not detect any significant change-points.
%
%To validate the detected change-points, we plotted them on a heatmap of the pairwise distances between observations. The change-points at $t=49$ and $t=61$ (or $t=63$) are clearly distinguishable, as evidenced by the distinct patterns in the pairwise distances during this period compared to others.







\section{Conclusion and discussion}\label{sec:conclusion}
In this paper, we propose a novel similarity graph to reduce the presence of hubs, which can negatively impact the performance of the graph-based methods. To achieve this, we penalize the sum of the degree squares. Numerical studies demonstrate that using this new graph can enhance the power of graph-based two-sample tests and graph-based offline change-point detection under various scenarios. Although our investigation focuses on offline change-point detection, it is likely that the new robust graph could also improve the power of graph-based online change-point detection. Since the current graph-based online change-point detection framework requires the graph to be the standard $K$-NNG \citep{chu2019asymptotic, chu2022sequential} and extending this framework to graphs other than the standard $K$-NNG would require substantial work, we defer this to future research.



















\section*{Acknowledgement}
The authors were supported in part by NSF awards DMS-1848579 and DMS-2311399.

\section*{Supplementary material}
\label{SM}
The Supplementary Material contains proofs of the main theorems presented in Section \ref{sec:3}. Additionally, it includes results from a numerical study investigating the effect of dimension on GET with the $K$-MST, as well as the estimated power and accuracy under Settings 2-4 in Section \ref{numeric study for change point detection}.

%Further instructions will be given when a paper is accepted.
%\vspace*{-10pt}

% \appendix

% \appendixone


\bibliographystyle{biometrika}
\bibliography{reference}



\end{document}
