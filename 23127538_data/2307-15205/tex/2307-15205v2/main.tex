\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{algorithm}
\usepackage{algpseudocode}


\bibliographystyle{abbrvnat}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newcommand{\ep}{\textsf{E}}
\newcommand{\var}{\textsf{Var}}
\newcommand{\cov}{\textsf{Cov}}
\newcommand{\B}{\text{B}}
\renewcommand{\P}{\text{P}}
\newcommand{\iidsim}{\stackrel{\text{iid}}{\sim}}
\renewcommand{\d}{{\text{diff}}}
\renewcommand{\,}{{\text{, }}}
\newcommand{\covp}{\textsf{Cov}_\text{P}}
\newcommand{\epp}{\textsf{E}_\text{P}}
\newcommand{\varp}{\textsf{Var}_\text{P}}
\newcommand{\covb}{\textsf{Cov}_\text{B}}
\newcommand{\epb}{\textsf{E}_\text{B}}
\newcommand{\varb}{\textsf{Var}_\text{B}}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%




\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Robust graph-based methods for overcoming the curse of dimensionality}
  \author{Yejiong Zhu\hspace{.2cm}\\
    Department of Statistics, University of California, Davis\\
    and \\
    Hao Chen \\
    Department of Statistics, University of California, Davis}
    \date{}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Robust graph-based methods for overcoming the curse of dimensionality}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Graph-based two-sample tests and graph-based change-point detection that utilize a similarity graph provide a powerful tool for analyzing high-dimensional and non-Euclidean data as these methods do not impose  distributional assumptions on data and have good performance across various scenarios. Current graph-based tests that deliver efficacy across a broad spectrum of alternatives typically reply on the $K$-nearest neighbor graph or the $K$-minimum spanning tree. However, these graphs can be vulnerable for high-dimensional data due to the curse of dimensionality. To mitigate this issue, we propose to use a robust graph that is considerably less influenced by the curse of dimensionality. We also establish a theoretical foundation for graph-based methods utilizing this proposed robust graph and demonstrate its consistency under fixed alternatives for both low-dimensional and high-dimensional data.
\end{abstract}

\noindent%
{\it Keywords:}  edge-count two-sample tests, curse of dimensionality, permutation null distribution
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!

\section{Introduction}\label{sec: 1}
Two-sample hypothesis testing is a fundamental task in statistics and have been extensively explored. Nowadays, the growing prevalence of complex data in various fields like genomics, finance, and social networks has led to a rising demand for methods capable of handling high-dimensional and non-Euclidean data \citep{bullmore2009complex,koboldt2012comprehensive,feigenson2014disorganization, beckmann2021downregulation}. Parametric approaches are limited in many ways when dealing with a large number of features and various data types as they are often confined by particular parametric families.

In the nonparametric domain, two-sample testing has numerous advancements over the years. \cite{friedman1979multivariate} proposed the first practical method that can be applied to data in an arbitrary dimension.
This method (we call it the original edge-count test (OET) for easy reference) involved constructing the minimum spanning tree, which is a tree connecting all observations such that the sum of edge
lengths that are measured by the distance between two endpoints is minimized, and counting the number of edges connecting observations from different samples. Later, researchers applied this method to different similarity graphs, including the $K$-nearest neighbor graph ($K$-NNG) \citep{schilling1986multivariate,henze1988multivariate} and the cross-match graph \citep{rosenbaum2005exact}. More recently, \cite{chen2017new} renovated the test statistic by incorporating an important pattern caused by the curse of dimensionality, and proposed the $\emph{\text{generalized edge-count test}}$ (GET). GET exhibits substantial power improvement over OET for a wide range of alternatives. Since then, two additional graph-based tests have been proposed: the weighted edge-count test (WET) \citep{chen2018weighted} and the max-type edge-count test (MET) \citep{chu2019asymptotic}. WET focuses on location alternatives, while MET performs similarly to GET and has some advantages under the change-point setting. Since all these tests are based on a similarity graph, they are referred to as the graph-based tests. 

Other nonparametric two-sample tests have also been proposed, including those based on Maximum Mean Discrepancy (MMD) \citep{gretton2008kernel, gretton2012kernel, gretton2012optimal}, Ball Divergence \citep{pan2018ball}, and measure transportation \citep{deb2021multivariate}.
Among these nonparametric approaches, the graph-based edge-count methods have an important niche given their good performance and easy type I error control \citep{zhu2021limiting}. We here compare GET on the 5-NNG (GET-5) and on the $\sqrt{N}$-NNG (GET-sqrtN) where $N$ is the total sample size, with the cross match test (CM) \citep{rosenbaum2005exact}, the test based on MMD (MMD) \citep{gretton2012kernel}, the test based on the Ball Divergence (BD) \citep{pan2018ball}, and a mutivariate rank-based test (MT) \citep{deb2021multivariate} under following scenarios. 
\begin{enumerate}
    \item[(i)] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{u}{\sqrt{d}}\mathbf{1}_d,$ $ \Sigma_d(0.5)+\frac{u}{\sqrt{d}}\mathbf{I}_d)$,
    
    \item[(ii)] $X_1,{\cdots},X_m \iidsim {Lognormal}(\mathbf{0}_d,\Sigma_d(0.6))$, $Y_1,{\cdots},Y_n \iidsim {Lognormal}(\mathbf{u}_1,\Sigma_d(0.2))$,
    
    \item[(iii)] $X_1,{\cdots},X_m\iidsim t_5(\mathbf{0}_d,\Sigma_d(0.6))$,$Y_1,{\cdots},Y_n \iidsim t_5(\mathbf{u}_2,\Sigma_d(0.6))$,
\end{enumerate}
where $\mathbf{0}_d$ is a $d$-dimensional vector with elements $0$, $\mathbf{1}_d$ is a $d$-dimensional vector with elements $1$, $\mathbf{u}_1$ is a $d$-dimensional vector with first $\sqrt{d}$ elements equal to $u$ and the remaining elements equal to $0$, $\mathbf{u}_2$ is a $d$-dimensional vector with first $d^{1/3}$ elements equal to $u$ and the remaining elements equal to $0$, $\mathbf{I}_d$ is a $d$-by-$d$ identity matrix, and $\Sigma_d(r) = (r^{|i-j|})_{1\leq i,\space j\leq d}$.

We set $m = n = 100$ and $d=500$. The estimated power of each test is computed through 1,000 simulation runs and plotted in Figure \ref{estimated_power_various_tests}. We can see that, under these location and scale differences for symmetric and asymmetric distributions including heavy-tailed distributions, the GET test on the $K$-NNG generally have satisfactory performance, while other tests that work well under one setting could fail under some other settings.


% Figure environment removed


For graph-based methods, GET or MET on the $K$-MST\footnote{$K$-MST: an undirected graph built as the union of the $1$st, $\cdots$ $K$th MSTs, where the 1st MST is the minimum spanning tree, and the $k$th $(k> 1)$ MST is a tree connecting all observations that minimizes the sum of distance across edges subject to the constraint that it does not contain any edges in the 1st, $\cdots$ $(k-1)$th MST(s).} or the $K$-NNG are usually recommended due to their relatively high power under a broad range of alternatives \citep{chen2017new, chu2019asymptotic}. For simplicity, for the remaining of the paper, we refer to this subset when saying graph-based methods unless otherwise specified. In addition, this subset perform similarity across various scenarios, so we focus on GET on $K$-NNG in the main context. Some results on GET on $K$-MST are provided in Appendix \ref{effect on MST}.


\subsection{What might affect the performance of graph-based methods?} \label{effect of inliers} We first check whether outliers, defined as observations that are far from other observations, affect the performance of the graph-based methods. Figure \ref{Estimated power on 5NNG and 14NNG} plots the estimated power of GET on $5$-NNG and $14$-NNG ($14\approx \sqrt{100+100}$) for a toy example: $X_1, {\cdots}, X_{100} \iidsim N(\mathbf{0}_d, \mathbf{I}_d)$, $Y_1, {\cdots}, Y_{100} \iidsim N(\mathbf{0}_d, \sigma\mathbf{I}_d)$, where $\sigma$ equally ranges from 1 to 1.02 with an increment of 0.001 and $d = 1,000$. We purposely perturb the data in two ways:
\begin{itemize}
    \item[(1)] Random perturbation: reverse the sample labels of 5 randomly chosen nodes.
    \item[(2)] Outlier perturbation: reverse the sample labels of 5 nodes that are furthest away from the center of the data.
\end{itemize}

% Figure environment removed


We see that, compared to random perturbation, mislabeling points farthest from the center decreases the power of test a bit more. However, the decrease is not too much. Hence, the method is quite robust to outliers. This is expected because the number of edges in the similarity graph that connect to the outliers is relatively small as the outliers are far away from the remaining observations and thus outliers have little effect on the method.

Then, in the same line of reasoning, if there are observations that connect to many other observations in the similarity graph, will they affect the method a lot? To check for this, we examine another type of perturbation:
\begin{itemize}
    \item [(3)] Hub perturbation: reverse the sample labels of 5 nodes with the largest degrees in the graph.
\end{itemize}

% Figure environment removed

Figure \ref{Estimated power with inlier} plots the estimated power of GET under the same setting as in Figure \ref{Estimated power on 5NNG and 14NNG} but with hub perturbation. We see that reversing sample labels of 5 points with the largest degrees could dramatically decrease the power of the test. While using a denser graph (right panel of Figure \ref{Estimated power with inlier}) may mitigate this effect, there is still a significant decrease in power. 
One explanation behind the high influence of hubs on the performance of the graph-based method is that the method relies on the number of edges  and a node with a large degree would affect the count more, leading to a high influence. Figure \ref{degrees of mislabeled points} plots boxplots of average degrees of perturbed points under the toy example with $\sigma=1.02$. We see that the average degree of hubs are much higher than that of other selected points.

% Figure environment removed


\subsection{Relationship between hub and dimensionality}\label{sec:hub}
The toy example in Section \ref{effect of inliers} clearly demonstrates the significant influence of hubs within the $K$-NNG on the performance of graph-based methods. Here, we further examine the relationship between hubs and data dimensionality. We utilize the same toy example, maintaining a fixed Fubini norm of the covariance matrix difference at 0.3, while varying the dimensionality from 5 to 1,000. Figure \ref{avg_deg_vs_dim} presents boxplots of the average degree of perturbed points in the $5$-NNG and the $14$-NNG for dimensions $5, 10, 50, 100, 200,$ and $500$. 

At low dimensions ($d=5$), we observe that the average degree of hubs slightly exceeds that of 5 randomly selected nodes. As the dimensionality increases, the average degree of the randomly selected nodes remains relatively stable, while the average degree of the hubs experiences a significant escalation.  This results in a pronounced overweighted influence of hubs, particularly when the dimension is not small ($d\geq 50$).  

Figure \ref{estiamted power with d} displays the estimated power of GET on the $5$-NNG and the $14$-NNG with perturbed data across different dimensions. The estimated power remains relatively stable across varying dimensions for both  no perturbation and outlier perturbation. However, in the case of hub perturbation, the estimated power is slightly lower compared to other perturbations at low dimensions and exhibits a significant decline with a moderate increase in dimension.  Notably, the estimated power with hub perturbation experiences a pronounced decrease until it reaches dimension 50, after which the decline becomes more gradual till dimension 1,000.  This pattern is consistent with the observed increase in average degrees of hubs illustrated in Figure \ref{avg_deg_vs_dim}.

% Figure environment removed

% Figure environment removed

% Figure environment removed


The presence of hubs in the $K$-NNG for moderate to high dimensions can be attributed to the \emph{curse of dimensionality}. \cite{radovanovic2010hubs} investigated the phenomenon of hubness in the $K$-NNG for data from one distribution.  They demonstrated that, under commonly employed assumptions, the degree distribution becomes significantly right-skewed as dimension increases. Figure \ref{degree_distribution_dim_5_500} plots the empirical degree distributions of the $5$-NNG with the data from the standard multivariate normal distribution (top panel) and the previous toy example with a fixed Fubini norm of the covariance matrix difference at 0.3 (bottom panel)  under dimensions 5, 10 and 50. We see that, as dimension increases, both degree distributions -- whether under the standard multivariate normal distribution or the toy example setting -- exhibit a more pronounced right-skewed pattern.


% explored the hubness phenomenon in the $K$-NNG for the data from identical distributions and showed that under commonly used assumptions the degree distribution becomes considerably skewed as dimensionality increases. They stated that it is an inherent property of data distributions in high-dimensional vector space. 


\subsection{Mitigate the effect of the curse of dimensionality for graph-based methods}

In terms of the test statistic, \cite{chen2017new} had renovated the OET statistic to the GET statistic to take into account the pattern caused by the curse of dimensionality, and thus making the test statistic more robust to the  curse of dimensionality. However we see from previous examples that the recommended graphs, $K$-NNG (in Section \ref{effect of inliers} and \ref{sec:hub}) and $K$-MST (in Appendix \ref{effect on MST}), are also affected by the curse of dimensionality.  In this paper, we focus on constructing similarity graphs that are robust to the curse of dimensionality.  In particular, since hubs emerge naturally as dimension increases and graph-based methods are susceptible to hubs, we propose to  construct robust graphs by penalizing the presence of hubs.  The detailed procedure for constructing these robust graphs is provided in  Section \ref{sec:2}.  By using the robust similarity graphs, we can significantly mitigate the impact of the curse of dimensionality.

Figure \ref{estimated_power on K-RNNG} displays the estimated power of GET on the $K$-robust nearest neighbor graph ($K$-RNNG) (solid lines) under the same setting as in Figures \ref{Estimated power on 5NNG and 14NNG} and \ref{Estimated power with inlier} (dotted lines).  We see that, even though the hub perturbation (blue lines) still cause some power decrease, the decrease is much less significant compared to that using the $K$-NNG (dashed blue lines).


% Figure environment removed

% Figure environment removed

Figure \ref{avg_deg_vs_dim_of K-RNNG} displays the boxplots of the average degree of perturbed points in the $5$-RNNG and $14$-RNNG under a similar setting as in Figure \ref{avg_deg_vs_dim} for dimensions 5, 10, 50, 100, 200, and 500. We see that the average degrees of the five largest degrees in the 5-RNNG and the 14-RNNG are considerably smaller compared to their counterparts in the 5-NNG and the 14-NNG, as presented in Figure \ref{avg_deg_vs_dim}. For instance, when the dimension increases from 5 to 1,000, the average degree of the five largest degrees in the $5$-NNG rises from 17 to 50.  However, in the $5$-RNNG, this average degree only experiences a modest increase, from 14 to approximately 22. 

% Figure environment removed

Figure \ref{power of KRNNG with d} displays the estimated power of GET on the $K$-RNNG across different dimensions. It is evident that the estimated power of GET on $K$-RNNG with hub perturbation
no longer decreases as dimension increases. % and the performance of GET on $K$-RNNG is better than that of GET on $K$-NNG generally.




\subsubsection{Power improvement without perturbation}
Besides enhancing the robustness of graph-based methods against hub perturbations, the robust graphs also improves the overall power of these methods. This improvement is evident in Figures \ref{estimated_power on K-RNNG} and \ref{power of KRNNG with d}, where the solid red lines surpass the dotted red lines that represent scenarios without perturbations.  To check that this effect is not coincidental, we further examine the power of GET on both the 5-RNNG and on the 5-NNG across various settings:

\begin{itemize}
    \item[1.] $X_1, {\cdots}, X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, \quad $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, $ $\Sigma_d(0.5))$,
    \item[2.] $X_1,{\cdots}, X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, $ $\Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
    \item[3.] $X_1,{\cdots}, X_m \iidsim \text{Lognormal}(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Lognormal}(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5))$,
    \item[4.] $X_1,{\cdots}, X_m \iidsim \text{Multivariate t}_5(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_5(\frac{\delta}{\sqrt{d}}\mathbf{1}_d,\\ \Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
\end{itemize}
where $m=n=100$, $d = 500$. Setting 1 involves mean shift under the multivariate normal distribution. Settings 2, 3, and 4 introduce both mean shift and scale difference under different distributions. 

% Figure environment removed

Figure \ref{KRNNG_vs_KNNG} illustrates the estimated power of GET on both the $5$-RNNG and the $5$-NNG.  In Setting 1 where only mean difference exists, GET on the $5$-RNNG and GET on $5$-NNG exhibit similar performance. However, in Settings 2, 3, and 4, which involve both mean shift and scale difference, GET on the $5$-RNNG exhibits substantially higher power compared to GET on the $5$-NNG. This finding suggests that the $5$-RNNG is particularly advantageous in scenarios where scale difference exists across distributions.

\subsection{Organization}
The rest of the paper is organized as follows. In Section \ref{sec:2}, we introduce the robust similarity graph in details, investigate the asymptotic properties of the GET statistic on the proposed robust graph, and explore the choice of hyper-parameter $\lambda$. Section \ref{sec:4} presents a comparative analysis of the performance of GET on the $K$-NNG, $K$-MST, and $K$-RNNG, along with other popular methods, in both two-sample testing and change-point detection problems through numeric studies.





\section{\label{sec:2} Robust similarity graphs}
Given $N$ observations, $Z_1,{\cdots}, Z_N$, and a distance metric $D(\cdot,\cdot)$, we define $R_i(Z_j)$ to be the rank of the distance $D(Z_i,Z_j)$ within the set of distances $\{D(Z_i,Z_l): l \neq i\}$.  Then, the $K$-NNG minimizes $\sum_{i=1}^N\sum_{x \in C_i}R_i(x)$ over all possible sets $C_i$, where $C_i$ contains $K$ observations excluding $Z_i$.   In a graph $G$, let $|G_i|$ be  the degree of the $i$-th node, taking into account  both the in-degree and out-degree.  We define the $K$-robust nearest neighbor graph ($K$-RNNG) as the graph that minimizes the objective function (\ref{obj1}) over all sets $C_i$ that contain $K$ observations excluding $Z_i$:
\begin{align}
\label{obj1}
    \sum_{i=1}^N\sum_{x \in C_i}R_i(x)+\lambda\sum_{i=1}^N|G_i|^2.
\end{align}
Here, $\lambda$ is a hyper-parameter and its choice is discussed in Section \ref{sec: choice of lambda}. Optimizing the objective function (\ref{obj1}) is a combinatorial problem and finding the global optimum is typically difficult. In this paper, we provide a greedy algorithm (Algorithm \ref{alg:penalized KNN}) as a practical approach.  While this algorithm may not guarantee the global optimum, we find it to be effective enough in practice.


\begin{algorithm}[h]
\caption{Constructing the $K$-robust nearest neighbor graph}\label{alg:penalized KNN}
\begin{algorithmic}[1]
\normalsize
  \item Initialize $G$ with the $K$-NNG, and compute the value of the objective function (\ref{obj1}) on $G$ and store it as $L$.
  \item Randomly permute the order of nodes and indicate them to be $1,{\cdots},N$. For $i$ from $1$ to $N$, 
  \begin{itemize}
      \item[2.1] Compute $W_{i}(j) = R_i(Z_j) + \lambda(|G_j^\star|+1)^2$, where $|G_j^\star| = |G_j|-1$ if node $j$ is one of neighbors of node $i$; otherwise $|G_j^\star| = |G_j|$;
      \item[2.2] Find $K$ nodes with the $K$ smallest $W_{i}(j)$'s among $\{W_{i}(j)\}_{j= 1,{\cdots},i-1,i+1,{\cdots},N}$;
      \item[2.3] Compute objective function (\ref{obj1}) with node $i$ connecting to these $K$ nodes found in Step 2.2  and denote it as $L^\star$;
      \item[2.4] If $L^\star <L$, update the graph by pointing node $i$ to the $K$ nodes found Step 2.2 and Let $L = L^\star$; otherwise do not change the graph or the value of $L$.
  \end{itemize}
  \item Repeat Step 2 until no node can find neighbors with a lower $L^\star$.
  \end{algorithmic}
\end{algorithm}

\begin{remark}
In the objective function (\ref{obj1}), the regularization term employs the total degree $|G_i|$, which is equally to use the in-degree as the out-degree for each node is fixed to be $K$.

% we use the total degree $|G_i|$ in the regularization term, which is equivalent to use the in-degree here because the out-degree and the graph size are fixed here to be $K$ and $KN$.
\end{remark}

\begin{remark}
The objective function (\ref{obj1}) is not limited to ranks. We could use a distance metric $D(\cdot,\cdot)$ directly, and the $K$-RNNG can be obtained by solving
\begin{align*}
    &\min_{C_i\text{'s}} \sum_{i=1}^N\sum_{x\in C_i}D(Z_i,x)+\lambda\sum_{i=1}^N|G_i|^2\\
    &\text{s.t. } Z_i\notin C_i, |C_i| = K.
\end{align*}
\end{remark}

The choice of using ranks in the objective function here that necessitates only the information of closeness brings a distance-free graph. It allows to study the theoretical property of the robust graph without considering the specific distance metric.

\begin{remark}\label{robust K MST}
A similar idea can be used to extend the $K$-MST to the robust $K$-MST. Let $R(Z_i,Z_j)$ be the rank of distance $D(Z_i,Z_j)$ in the set of all pairwise distances.
The robust $K$-MST is a $K$-spanning tree $T$, which minimize the objective function 
\begin{align*}\label{obj2}
    \sum_{(z_i,z_j) \in T}R(z_i,z_j)+\lambda\sum_{i=1}^N |G_i|^2.
\end{align*}
\end{remark}



\subsection{Asymptotic properties of the GET statistic on the $K$-RNNG} \label{sec:3}

\cite{zhu2021limiting} derived so far the best sufficient conditions on undirected graphs for the validity of the asymptotic distribution of the GET statistic. In this section, we extend their results to directed graphs and demonstrate that the $K$-RNNG graphs meet these conditions with an appropriate choice of $\lambda$. Before stating these results, we first define some essential notations.

Given a directed graph $G$ built on the pooled observations $Z_1,{\cdots},Z_N$ $(Z_i = X_i,$ $i = 1,{\cdots},m;$ $Z_{j+m}= Y_j,$ $j = 1,{\cdots},n; N = m+n)$. The pair $(i,j)$ (the order matters) represents a directed edge pointing from node $i$ to node $j$.  We define $|G|$ to be the number of directed edges in the graph $G$. For each node $i$, we define $G_i$ as the set of edges with one node $i$, $G_{i,2}$ as the set of edges sharing at least one node with an edge in $G_i$, $node_{G_i}$ as the set of nodes that are connected in $G_i$ excluding the node $i$, and $node_{G_{i,2}}$ as the set of nodes that are connected in $G_{i,2}$ excluding the node $i$. We further define $N_0$ to be the number of edges whose reversed edge is also in $G$, i.e. $N_0 = \sum_{\left(i,j\right)\in G}1_{\{(j,i)\in G\}}$, $\Tilde{d_i}$ to be the centered degree, i.e. $\Tilde{d_i} = |G_i|-\frac{2|G|}{N}$, and $N_{sq}$ to be the number of combinations of 4 edges that form a square. Let $V_G = \sum_{i=1}^N \Tilde{d_i}^2 = \sum_{i=1}^N|G_i|^2-\frac{4|G|^2}{N}$ representing the variation of degrees. 

Besides, we use $\xrightarrow{\mathcal{D}}$ to denote convergence in distribution, and use `the usual limit regime' to refer $N\rightarrow \infty$ and $\lim_{N\rightarrow \infty}\frac{m}{N} = p\in (0,1)$. In the following, $a_n= o(b_n)$ or $a_n\prec b_n$ means that $a_n$ is dominated by $b_n$ asymptotically, i.e. $\lim_{n\rightarrow \infty} \frac{a_n}{b_n}=0$, $a_n\precsim b_n$ or $a_n = O(b_n)$ means $a_n$ is bounded above by $b_n$ (up to a constant factor) asymptotically, and $a_n= \Theta(b_n)$ or $a_n \asymp b_n$ means that $a_n$ is bounded both above and below by $b_n$ (up to constant factors) asymptotically. We use $a\wedge b$ for $\min\{a,b\}$. For two sets $S_1$ and $S_2$, $S_1\backslash S_2$ is used for the set that contains elements in $S_1$ but not in $S_2$.

Let $l_i$ be the sample group label of $i$-th node defined as
\begin{eqnarray*}
    l_i=\left\{
\begin{aligned}
1 & \text{ if node $i$ is from sample X} \\
2 & \text{ if node $i$ is from sample Y.}
\end{aligned}
\right.
\end{eqnarray*}
Let $R_1$ and $R_2$ be the number of within-sample edges in sample $X$ and sample $Y$, respectively,
\begin{align*}
    R_1 = \sum_{(i,j)\in G} 1_{\{l_i=l_j=1\}},\quad R_2 = \sum_{(i,j)\in G} 1_{\{l_i=l_j=2\}}.
\end{align*}
Then, the GET statistic $S$ can be expressed as

\begin{align*}
    S = \begin{pmatrix}R_1-\ep_\P(R_1),& R_2-\ep_\P(R_2) \end{pmatrix} \times 
    \left(\varp\binom{R_1}{R_2} \right)^{-1} \begin{pmatrix}R_1-\ep_\P(R_1) \\R_2-\ep_\P(R_2)\end{pmatrix},
\end{align*}
where $\epp$, $\varp$ and $\covp$ are the expectation, variance and covariance under the permutation null distribution which places probability $1/ {N\choose m}$ on each selection of $m$ observations among all $N$ observations as sample X.

\cite{zhu2021limiting} established the sufficient conditions for the asymptotic distribution of the test statistic via the `locSCBâ€™ approach.  This approach relies on the equivalence between the permutation null distribution and the conditional Bootstrap null distribution. The Bootstrap null distribution assigns each observation to either sample $X$ or sample $Y$ independently, with probabilities $\frac{m}{N}$ and $\frac{n}{N}$, respectively. Conditioning on the number of observations assigned to sample $X$ being $m$, the Bootstrap null distribution becomes the permutation null distribution. The authors applied the Stein's method that considers the first neighbor dependency under the Bootstrap null distribution to derive asymptotic multivariate normality. We here adopt a similar idea to derive the sufficient conditions for the directed graph. A challenge with directed graphs is their allowance for multiple edges between two nodes, so it requires meticulous consideration of certain graph-related quantities. The conditions are provided in Theorem \ref{th1}, and the proof of the theorem is in Supplemental Material.

% \footnote{The bootstrap null distribution assigns each observation to sample X with probability $\frac{m}{N}$ and to sample Y with probability $\frac{n}{N}$, independently from any other observations.}
%
% that utilizes the equivalence of the permutation null distribution and the conditional Bootstrap null distribution  conditioning on that the number of nodes labeled as being sample $X$ is $m$, and then applies the local Stein's method under the bootstrap null distribution to show asymptotic multivariate normality. The same idea can be used here to derive the sufficient conditions on the directed graph. Theorem \ref{th1} summarizes these sufficient conditions on the directed graph with the proof provided in Appendix \ref{proof of th1}.

\begin{theorem}\label{th1}
For a directed graph $G$ with $|G| = O(N^\alpha), 1\leq \alpha<2$, under conditions 
\begin{align*}
   &\sum_{i=1}^{N}\left|G_{i}\right|^{2}=o\left(|G|^{\frac{3}{2}}\right), \sum_{i=1}^N \left|\Tilde{d_i}\right|^3 = o(V_G^\frac{3}{2}), \sum_{i=1}^N \Tilde{d_i}^3 = o(V_G\sqrt{|G|}),\\
   &\sum_{i=1}^N\sum_{(i,j) \text{ or }(j,i) \in G_i}^{(i,k) \text{ or }(k,i) \in G_i, j\neq k}\Tilde{d_j}\Tilde{d_k} = o(|G|V_G), \quad N_{sq} = o(|G|^2).
\end{align*}
 in the usual limit regime, we have $S\xrightarrow{\mathcal{D}} \chi_2^2$ under the permutation null distribution.
\end{theorem}

These sufficient conditions stated in Theorem \ref{th1} are applicable to any general directed graphs. For the $K$-RNNG, a more concise result can be obtained. By selecting an appropriate value for $\lambda$, all the sufficient conditions in Theorem \ref{th1} are satisfied for the $K$-RNNG. The main result is stated in Theorem \ref{th2} with the proof provided in Supplemental Material.

\begin{theorem}\label{th2}
Let $Q_N$ be the random variable generated from the degree distribution of the $K$-RNNG with $N$ nodes. Assume $K = \Theta(1)$ and if $\lambda$ is chosen such that $\var(Q_N)>0$ and $ \max\{Q_N\}\precsim N^{\frac{1}{2} -\beta}$ for some $\beta > 0$, we have $S\xrightarrow{\mathcal{D}} \chi_2^2$ under the permutation null distribution in the usual limit regime.

\end{theorem}

%\begin{remark}
%    The maximum degree of $K$-NNG is bounded when the dimension is fixed. As a result, the maximum degree of the $K$-RNNG is also bounded from Lemma \ref{lm6}, which leads to the bounded $\var(Q_N)$. This means that $\lambda$ can be selected as $\Theta(1)$ under the fixed dimension.
%\end{remark}

\begin{remark}
Theorem \ref{th2} requires that the variance of degree distribution of the $K$-RNNG is asymptotically bounded away from zero when choosing $\lambda$.  This is to ensure that the GET statistic is well defined -- when $\var(Q_N)=0$, the degrees of all nodes are the same and $\varp\binom{R_1}{R_2}$ becomes singular. This situation arises when an extremely large $\lambda$ is used. Additionally large values of $\lambda$ diminish the utilization of the similarity information contained in the first term of the objective function \eqref{obj1}, making such choices of $\lambda$ less desirable. Therefore, we tend not to choose a very large $\lambda$ in practice.
\end{remark}


\begin{theorem}[Consistency under fixed dimensions] \label{th3}
For two samples generated 
from two continuous multivariate distributions in Euclidean space with a fixed dimension, if the graph is the $K$-RNNG with $K = \Theta(1)$ and $\lambda \geq 0$, GET is consistent against all alternatives in the usual limiting regime.
\end{theorem}

\begin{theorem}[Consistency under high dimensions]\label{th4}
Assume distributions $F_X$ and $F_Y$ satisfy Assumptions 1 and 2 in \citep{biswas2014distribution}, and $\lim_{d\rightarrow \infty}\ep(||X-\ep(X)||_2^2)/d = \sigma_1^2$, $\lim_{d\rightarrow \infty}\ep(||Y-\ep(Y)||_2^2)/d = \sigma_2^2$ and $\lim_{d\rightarrow \infty}(||\ep(X)-\ep(Y)||_2^2)/d = v^2$, where $X\sim F_X$, $Y\sim F_Y$ and $d$ is the dimension. Without loss of generality, we assume that $\sigma_1^2 > \sigma_2^2$. Then, for GET on the $K$-RNNG with $0<\lambda < (\sqrt{8NK+4N-8K}-\sqrt{8NK})^2/16$ and $\min\{m,n\} > K+2\lambda+\sqrt{8\lambda KN}$, we have $\lim_{d\rightarrow \infty}P(S>\chi_2^2(1-\alpha)) = 1$, for any fixed $\alpha \in (0,1)$, when either of the following conditions hold:
\begin{itemize}
    \item [(1)] $|\sigma_1^2 - \sigma_2^2| < v^2$, $N > 2.5 + \frac{\xi}{K}+\sqrt{0.25+3\frac{\xi}{K} + \frac{\xi^2}{K^2}}$,
    \item [(2)]
    $N>\frac{n^2\xi^2}{2m^2K^2}\left(\sqrt{\frac{K}{\lambda}}+\sqrt{\frac{K}{\lambda}+\frac{2mK}{n\xi}(1+\frac{K}{2\lambda}+\frac{mK}{n\xi}-K)}\right)^2$, $\sigma_1^2 - \sigma_2^2 > v^2$,
    \item[(3)] $N>\frac{m^2\xi^2}{2n^2K^2}\left(\sqrt{\frac{K}{\lambda}}+\sqrt{\frac{K}{\lambda}+\frac{2nK}{m\xi}(1+\frac{K}{2\lambda}+\frac{nK}{m\xi}-K)}\right)^2$, $\sigma_2^2 - \sigma_1^2 > v^2$,
\end{itemize}
where $\xi =\chi_2^2(1-\alpha)$.
%$\frac{m}{n}K(N-1) > (\frac{K}{2\lambda}+1 + \sqrt{\frac{2NK}{\lambda}}-K)\chi_2^2(1-\alpha)$
\end{theorem}

Theorem \ref{th3} studies the consistency of GET on $K$-RNNG under the fixed dimension as the sample size goes to infinity. Theorem \ref{th4} studies the consistency as the dimension goes to infinity. The proofs of these theorems are in Supplemental Material. For Theorem \ref{th4}, although we usually don't know which case $\sigma_1^2$, $\sigma_2^2$ and $v^2$ satisfy, we can always choose the largest $N$ among three cases. For instance, with $\alpha = 0.05$, $\lambda = 0.3$, $m=n$ and $K = 5$, it requires $N \geq 69$. With $\alpha = 0.05$, $\lambda = 0.3$, $m/n = 2$ or $n/m = 2$ and $K = 5$, it requires $N \geq 214$.


\subsection{Choice of $\lambda$}\label{sec: choice of lambda}

To assess the impact of $\lambda$ on the power of GET on the 5-RNNG, we vary the value of $\lambda$ and look into the empirical power of the test. We consider the following scenarios including symmetric distribution, asymmetric distribution, and heavy-tailed distribution.

\begin{itemize}
    \item[(i)] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 200$, $n =100$, $d= 500$ and $\delta = 1.03$;
    \item[(ii)] $X_1,{\cdots},X_m \iidsim \text{lognormal}(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{lognormal}(\delta\mathbf{1}_d,\Sigma_d(0.5))$ with $m = 100$, $n =200$, $d= 1000$ and $\delta = 0.05$;
    \item[(iii)] $X_1,{\cdots},X_m \iidsim \text{lognormal}(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{lognormal}(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 100$, $n =100$, $d= 100$ and $\delta = 1.15$;
    \item[(iv)] $X_1,{\cdots},X_m \iidsim \text{t}_5(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{t}_5(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 100$, $n =100$, $d= 500$ and $\delta = 1.35$;
    \item[(v)] $X_1,{\cdots},X_m \iidsim \text{t}_5(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{t}_5(\delta\mathbf{1}_d, \Sigma_d(0.5))$ with $m = 300$, $n =100$, $d= 500$ and $\delta = 0.095$.
\end{itemize}
where $\delta$ is chosen so that the tests have moderate power when $\lambda$ is equal to zero.

Figure \ref{lambda_selection} presents the estimated power of the test across different values of $\lambda$.  The results indicate a rapid increase in power as $\lambda$ rises from 0 to 0.3, followed by a slower rate of improvement.  To determine a default value for $\lambda$, we employ the elbow method, which suggests a value of 0.3 would be appropriate.

In Figure \ref{lambda_variance}, we plot the relationship between the maximum center degree and $\lambda$. It is evident that the maximum degree experiences a rapid decline when $\lambda$ increases from 0 to 0.3, followed by a slower rate of decrease. This observation aligns with the trend depicted in Figure \ref{lambda_selection}, which showcases the estimated power. Consequently, a practical data-driven approach to select an appropriate $\lambda$ is to plot the maximum degrees for various values of $\lambda$ and identify the point at which the maximum degree exhibits minimal changes.

% Figure environment removed

% Figure environment removed





\section{Numerical studies}\label{sec:4} In this section, we evaluate the performance of GET on $K$-RNNG by comparing it with other state-of-the-art methods in both the two-sample testing and change-point detection problems.

\subsection{Two-sample testing}
We consider GET on 5-RNNG (New), 5-MST,  $\sqrt{N}$-MST, and other popular tests: the cross-match test \citep{rosenbaum2005exact} (CM), the Ball divergence test \citep{pan2018ball} (BD), the mutivariate rank-based test \citep{deb2021multivariate} (MT), the Adaptable Regularized Hotelling's $\text{T}^2$ test \citep{li2020adaptable} (ARHT) and the kernel test based on minimum mean discrepancy \citep{gretton2012kernel} (MMD), under the following simulation scenarios:
\begin{itemize}
    \item[1.] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d,$ $\Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
    \item[2.] $X_1,{\cdots},X_m \iidsim {Lognormal}(\mathbf{0}_d,\Sigma_d(0.6))$, $Y_1,{\cdots},Y_n \iidsim {Lognormal}(\mathbf{\Delta},\Sigma_d(0.2))$,
    \item[3.] $X_1,{\cdots},X_m \iidsim \text{Multivariate t}_2(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_2(\frac{\delta}{\sqrt{d}}\mathbf{1}_d,\\ \Sigma_d(0.5)+\delta\mathbf{I}_d)$,
    \item[4.] $X_1,{\cdots},X_m \iidsim \text{Multivariate t}_1(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_1(\frac{\delta}{\sqrt{d}}\mathbf{1}_d,\\ \Sigma_d(0.5)+\frac{\delta}{2}\mathbf{I}_d)$,
\end{itemize}
where $\mathbf{\Delta}$ a $d$-dimensional vector with first $\sqrt{d}$ elements equal to $\delta$ and the remaining elements equal to $0$.

% Figure environment removed

In each scenario, we set $m=n=100$ and $d = 50, 500, 1000$. The estimated powers computed from 1000 repetitions are plotted in Figure \ref{two_sample_sim}. Firstly, we observe that the empirical sizes of the GET on the $5$-RNNG are well controlled across different scenarios (at $\delta=0$ in Secenario 1, 3 and 4). In Scenario 1 and 2, although the power of the new test is marginally inferior to the BD's power, which demonstrates the maximum power in these scenarios, it excels in Scenario 3 and 4 and has a significant improvement over BD in scenario 4. Moreover, the new test consistently surpasses the GET on $5$-MST and the GET on $\sqrt{N}$-MST across all scenarios. This result underpins the advantages of employing $K$-RNNG, considering the tests only differ in their graph structures. 



\subsection{Change-point detection}
For graph-based change-point detection, MET is often recommended over GET \citep{chu2019asymptotic,liu2022fast,song2022asymptotic}, so we check the performance of both MET on $5$-RNNG and GET on $5$-RNNG in this section. We include in the comparison GET scan statistic on $5$-MST, MET scan statistic on $5$-NNG, and the distance-based approach in \citep{matteson2014nonparametric, james2013ecp} (e.divisive), and
consider the following simulation settings:

\begin{itemize}
    \item[1.]  $X_1,{\cdots},X_\tau \iidsim  N(\mathbf{0}_d, \Sigma_d(0.5))$, $X_{\tau + 1},{\cdots}, X_{N} \iidsim N(\frac{\delta}{\sqrt{d}} \mathbf{1}_d,$ $\Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}{\mathbf{I}_d})$,
    
    \item[2.] $X_1,\cdots,X_\tau \iidsim N(\mathbf{0}_d, \mathbf{I}_d)$, $X_{\tau + 1},{\cdots}, X_{N} \iidsim N(\mathbf{0}_d, \Sigma_d(\delta))$,
    
    \item[3.] $X_1,\cdots,X_\tau \iidsim \text{Multivariate t}_5(\mathbf{0}_d, \Sigma_d(0.5))$, $X_{\tau + 1},{\cdots}, $ $X_{N}\iidsim \text{Multivariate t}_5(\frac{\delta}{d}\mathbf{1}_d,\\ \delta\mathbf{I}_d + \Sigma_d(0.5))$.
\end{itemize}

% Figure environment removed

In each setting, we set $N$ to be 400, the true change-point $\tau$ to be at $100,200$ or $300$, and $d$ to be $100$ or $500$. The estimated power is computed as the proportion of trials with significant $p$-value among 1000 trials, and the accuracy is computed as the proportion of trials with significant $p$-value and estimated change-point $\hat \tau$ satisfying $|\hat \tau-\tau|\leq 10$, among 1000 trials. The estimated power and accuracy under Setting 1 are plotted in Figure \ref{cp_normal}, and the estimated power and accuracy under other settings are plotted in Figures \ref{cp_lognormal_diff} and \ref{cp_t1} in Appendix \ref{result in change-point}. We see that the MET and GET scan statistics on the $5$-RNNG have good power and accuracy in all settings, while others may perform well under some settings but poorly for some others.



\section{Conclusion and discussion}
In this paper, we propose a novel similarity graph to overcome the curse of dimensionality by imposing penalties on high-degree hubs, effectively reducing their impact.  Our empirical investigations demonstrate that incorporating this new graph can significantly enhance the effectiveness of graph-based methods in the domains of both two-sample testing and offline change-point detection problems.  However, the advantages of this robust similarity graph extend beyond these specific applications.  It holds the potential to elevate performance and alleviate the detrimental effects of the curse of dimensionality across various fields, including online change-point detection, independence testing, classifications, and clustering.  For instance, by adopting the new $K$-RNNG, the efficiency of methods like GET or MET in online change-point detection can be further amplified. Similarly, the performance of classification algorithms currently relying on the conventional $K$-NNG can experience substantial improvements through the incorporation of the new $K$-RNNG.

% that is less impacted by the hubs and the curse of dimensionality. The new graph reduce the influence of hubs by penalizing high degree of hubs. Our numeric studies illustrate that the utilization of this new graph can bolster the efficacy of graph-based tests in the realms of both two-sample testing and offline change-point detection problems. However, the benefits of this innovative robust similarity graph are not confined to these areas. It has the potential to improve the performance and mitigate the negative effect from the curse of dimensionality by replacing conventional similarity graphs in multiple fields, including online change-point detection, independence testing, classification, and clustering. For instances, the efficiency of the GET or MET with $K$-NNG in online change-point detection might be further augmented by using the new $K$-RNNG.
%The performance of classification methods that currently use the $K$-NNG might be further enhanced by the implementation of the new $K$-RNNG.

\if1\blind
{
\section*{Acknowledgment}

The authors were supported in part by NSF awards DMS-1848579 and DMS-2311399.
\fi

\appendix

\section{Effect of hubs on GET on $K$-MST} \label{effect on MST}

We apply GET on $K$-MST under the same setting in Section \ref{effect of inliers}. Figure \ref{effect_inlier_MST} shows the estimated power of GET on $K$-MST with and without perturbations, from which we can see that the performance of GET on $K$-MST is similar to that of GET on $K$-NNG and hubs have dominated effect on the power of GET on $K$-MST compared with other nodes.


We also investigate the effect of hubs and dimensionality in the $K$-MST. Figure \ref{degrees of mislabeled points in MST} depicts the average degree of perturbed points in $5$-MST and $14$-MST with $\sigma = 1.02$ and $d = 1000$. Figure \ref{avg_deg_vs_dim_MST} shows the average degrees of perturbed points $5$-MST and $14$-MST with ranging dimensions. Figure \ref{estiamted power with d in MST} shows the estimated power of GET on $5$-MST and $14$-MST with ranging dimensions.

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed

\newpage
\section{Estimated power and accuracy in the change-point detection analysis} \label{result in change-point}
Estimated power and accuracy of change-point detection numeric study under Setting 2 and 3 are plotted in Figure \ref{cp_lognormal_diff} and \ref{cp_t1}.


% Figure environment removed

% Figure environment removed







\bibliography{reference}
\end{document}
