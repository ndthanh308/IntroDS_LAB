% Template for the submission to:
%   The Annals of Statistics [AOS]
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% In this template, the places where you   %%
%% need to fill in your information are     %%
%% indicated by '???'.                      %%
%%                                          %%
%% Please do not use \input{...} to include %%
%% other tex files. Submit your LaTeX       %%
%% manuscript as one .tex document.         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[aos,preprint]{imsart}
\documentclass[preprint]{imsart}

%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
%\RequirePackage[numbers]{natbib}
\RequirePackage[authoryear]{natbib} %% uncomment this for author-year bibliography
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{graphicx}% uncomment this for including figures
\usepackage{enumerate}
\usepackage{wrapfig}
\usepackage{multicol}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{floatrow}
\usepackage{comment}
\usepackage{tablefootnote}
%\usepackage{xr}
\usepackage[symbol]{footmisc}
\floatsetup[table]{capposition=top}
\usepackage{array, makecell} %

% \usepackage{algorithm2e}
\usepackage{algorithm,float} 
\usepackage{algpseudocode}

\makeatletter



\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothezis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{example}{Example}
\newtheorem*{fact}{Fact}     
\newtheorem{remark}{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\ep}{\textsf{E}}
\newcommand{\var}{\textsf{Var}}
\newcommand{\cov}{\textsf{Cov}}
\newcommand{\B}{\text{B}}
%\newcommand{\iid}{\stackrel{\mathrm{iid}}{\sim}}
\renewcommand{\P}{\text{P}}
\newcommand{\iidsim}{\stackrel{\text{iid}}{\sim}}
\renewcommand{\d}{{\text{diff}}}
\renewcommand{\,}{{\text{, }}}
\newcommand{\covp}{\textsf{Cov}_\text{P}}
\newcommand{\epp}{\textsf{E}_\text{P}}
\newcommand{\varp}{\textsf{Var}_\text{P}}
\newcommand{\covb}{\textsf{Cov}_\text{B}}
\newcommand{\epb}{\textsf{E}_\text{B}}
\newcommand{\varb}{\textsf{Var}_\text{B}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

\graphicspath{{figures/}}


\endlocaldefs

\begin{document}

\begin{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Robust graphs for graph-based methods}
%\title{A sample article title with some additional note\thanksref{T1}}
\runtitle{Robust graphs for graph-based methods}
%\thankstext{T1}{A sample of additional note to the title.}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%Only one address is permitted per author. %%
%%Only division, organization and e-mail is %%
%%included in the address.                  %%
%%Additional information can be included in %%
%%the Acknowledgments section if necessary. %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{\fnms{Yejiong} \snm{Zhu}\ead[label=e1]{yjzhu@ucdavis.edu}}
\and
\author{\fnms{Hao} \snm{Chen}\ead[label=e2]{hxchen@ucdavis.edu}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address{University of California, Davis}
%\address{University of California, Davis, \printead{e1,e2}}

%\address[B]{University of California, Davis, \printead{e2}}
\end{aug}

\begin{abstract}
Graph-based two-sample tests and graph-based change-point detection that utilize similarity graphs provide powerful tools for analyzing high-dimensional and non-Euclidean data as these methods do not impose  distributional assumptions and have good performance across various scenarios. Current graph-based tests that deliver efficacy across a broad spectrum of alternatives typically reply on the $K$-nearest neighbor graph or the $K$-minimum spanning tree.  However, these graphs can be vulnerable for high-dimensional data due to the curse of dimensionality.  To mitigate this issue, we propose to use a robust graph that is considerably less influenced by the curse of dimensionality. We also establish a theoretical foundation for graph-based methods utilizing this proposed robust graph and demonstrate its consistency under fixed alternatives for both low-dimensional and high-dimensional data.

\end{abstract}

\begin{keyword}[class=MSC2010]
\kwd[Primary ]{62G10}
%\kwd{???}
\kwd[; secondary ]{62G20}
\end{keyword}

\begin{keyword}
\kwd{edge-count tests}
\kwd{curse of dimensionality}
\kwd{hypothesis testing}
\kwd{change-point detection}
\kwd{permutation null distribution}
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main text entry area:

\section{Introduction}\label{sec: 1}
Two-sample hypothesis testing is a fundamental task in statistics and have been extensively explored. Nowadays, the growing prevalence of complex data in various fields like genomics, finance, and social networks has led to a rising demand for methods capable of handling high-dimensional and non-Euclidean data \citep{bullmore2009complex,koboldt2012comprehensive,feigenson2014disorganization, beckmann2021downregulation}. Parametric approaches are limited in many ways when dealing with a large number of features and various data types as they are often confined by particular parametric families.

In the nonparametric domain, two-sample testing has numerous advancements over the years. \cite{friedman1979multivariate} proposed the first practical method that can be applied to data in an arbitrary dimension.
This method (we call it the original edge-count test (OET) for easy reference) involved constructing the minimum spanning tree, which is a tree connecting all observations such that the sum of edge
lengths that are measured by the distance between two endpoints is minimized, and counting the number of edges connecting observations from different samples. Later, researchers applied this method to different similarity graphs, including the $K$-nearest neighbor graph ($K$-NNG) \citep{schilling1986multivariate,henze1988multivariate} and the cross-match graph \citep{rosenbaum2005exact}. More recently, \citet{chen2017new} renovated the test statistic by incorporating an important pattern caused by the curse of dimensionality, and proposed the $\emph{\text{generalized edge-count test}}$ (GET). GET exhibits substantial power improvement over OET for a wide range of alternatives. Since then, two additional graph-based tests have been proposed: the weighted edge-count test (WET) \citep{chen2018weighted} and the max-type edge-count test (MET) \citep{chu2019asymptotic}. WET focuses on location alternatives, while MET performs similarly to GET and has some advantages under the change-point setting. Since all these tests are based on a similarity graph, they are referred to as the graph-based tests. 

Other nonparametric two-sample tests have also been proposed, including those based on Maximum Mean Discrepancy (MMD) \citep{gretton2008kernel, gretton2012kernel, gretton2012optimal}, Ball Divergence \citep{pan2018ball}, and measure transportation \citep{deb2021multivariate}.
Among these nonparametric approaches, the graph-based edge-count methods have an important niche given their good performance and easy type I error control \citep{zhu2021limiting}. We here compare GET on the 5-NNG (GET-5) and on the $\sqrt{N}$-NNG (GET-sqrtN) where $N$ is the total sample size, with the cross match test (CM) \citep{rosenbaum2005exact}, the test based on MMD (MMD) \citep{gretton2012kernel}, the test based on the Ball Divergence (BD) \citep{pan2018ball}, and a mutivariate rank-based test (MT) \citep{deb2021multivariate} under following scenarios: 
\begin{itemize}
    \item[(i)] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{u}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5)+\frac{u}{\sqrt{d}}\mathbf{I}_d)$,
    
    \item[(ii)] $X_1,{\cdots},X_m \iidsim \text{Lognormal}(\mathbf{0}_d,\Sigma_d(0.6))$, $Y_1,{\cdots},Y_n \iidsim \text{Lognormal}(\mathbf{u}_1,\Sigma_d(0.2))$,
    
    \item[(iii)] $X_1,{\cdots},X_m\iidsim t_5(\mathbf{0}_d,\Sigma_d(0.6))$, $Y_1,{\cdots},Y_n \iidsim t_5(\mathbf{u}_2,\Sigma_d(0.6))$,
\end{itemize}
where $\mathbf{0}_d$ is a $d$-dimensional vector of $0$'s, $\mathbf{1}_d$ is a $d$-dimensional vector $1$'s, $\mathbf{u}_1$ is a $d$-dimensional vector with the first $\sqrt{d}$ elements equal to $u$ and the remaining elements equal to $0$, $\mathbf{u}_2$ is a $d$-dimensional vector with the first $d^{1/3}$ elements equal to $u$ and the remaining elements equal to $0$, $\mathbf{I}_d$ is a $d$-dimensional identity matrix, and $\Sigma_d(r) = (r^{|i-j|})_{1\leq i,\space j\leq d}$.
We set $m = n = 100$ and $d=500$. The estimated power of each test is computed through 1,000 simulation runs and plotted in Figure \ref{estimated_power_various_tests}. We can see that, under these location and scale differences for symmetric and asymmetric distributions including heavy-tailed distributions, the GET test on the $K$-NNG generally have satisfactory performance, while other tests that work well under one setting could fail under some other settings.

% Figure environment removed

For graph-based methods, GET or MET on the $K$-MST\footnote{$K$-MST: an undirected graph built as the union of the $1$st, $\cdots$ $K$th MSTs, where the 1st MST is the minimum spanning tree, and the $k$th $(k> 1)$ MST is a tree connecting all observations that minimizes the sum of distance across edges subject to the constraint that it does not contain any edges in the 1st, $\cdots$ $(k-1)$th MST(s).} or the $K$-NNG are usually recommended due to their relatively high power under under a broad range of alternatives \citep{chen2017new, chu2019asymptotic, liu2022fast, chen2023graph}. For simplicity, for the remaining of the paper, we refer to this subset when saying graph-based methods unless otherwise specified.  In addition, this subset perform similarity across various scenarios, so we focus on GET on $K$-NNG in the main context.  Some results on GET on $K$-MST are provided in Appendix \ref{effect on MST}.

%In the nonparametric domain, famous nonparametric tests in the univariate setting include the Kolmogorov–Smirnov test \citep{smirnov1939estimation}, the Wald–Wolfowitz runs test \citep{wald1940test} and the Mann–Whitney rank-sum test \citep{mann1947test}. Later, \cite{weiss1960two} and \cite{bickel1969distribution} make extensions of these methods to multivariate data. However, these extensions are limited in the high dimensional setting. \cite{gretton2012kernel} developed the kernel-based method, and \cite{pan2018ball} proposed an test based on the ball divergence. Both of these methods are workable with high-dimensional data, and have satisfactory performance under the particular settings. For change-point detection analysis, \cite{desobry2005online} and \cite{harchaoui2013kernel} used kernel-based methods that heavily depend on the choice of the kernel function and its parameters, and the problem becomes severe with high-dimensional data. \cite{lung2011robust} proposed a nonparametric approach based on marginal rank statistics that still requires the restriction that the number of observations be larger than the dimension of the data. 

%Besides, Graph-based methods utilizing edge counts in similarity graphs such as $K$-nearest neighbor graph ($K$-NNG), $K$-minimum spanning tree ($K$-MST) or minimum non-bipartite matching become promising and popular in both two-sample testing and change-point detection analysis, especially in the high dimensional setting. \cite{pallotta2017context} detect structural changes in the evolving networks by using the graph-based method. \cite{dai2019discovering} applied the graph-based method to detect change points of patterns into functional connectivity across subjects represented a time-indexed sequence of symmetric, positive definite matrices. \cite{dong2020predicting} used the graph-based edge-count method to detect the change point in dynamic co-expression networks of genes on gene sets.


\subsection{What might affect the performance of graph-based methods?} \label{effect of inliers} We first check whether outliers, defined as observations that are far from other observations, affect the performance of the graph-based methods. Figure \ref{Estimated power on 5NNG and 14NNG} plots the estimated power of GET on $5$-NNG and $14$-NNG ($14\approx \sqrt{100+100}$) for a toy example: $X_1, {\cdots}, X_{100} \iidsim N(\mathbf{0}_d, \mathbf{I}_d)$, $Y_1, {\cdots}, Y_{100} \iidsim N(\mathbf{0}_d, \sigma\mathbf{I}_d)$, where $\sigma$ equally ranges from 1 to 1.02 with an increment of 0.001 and $d = 1,000$. We purposely perturb the data in two ways:
\begin{itemize}
    \item[(1)] Random perturbation: mislabel the sample labels of 5 random chosen nodes.
    \item[(2)] Outlier perturbation: mislabel the sample labels of 5 nodes that are furthest away from the center of the data.
\end{itemize}
We see that, compared to random perturbations, mislabeling points farthest from the center decreases the power of test a bit more. However, the decrease is not too much. Hence, the method is quite robust to outliers. This is expected because the number of edges in the similarity graph that connect to the outliers is relatively small as the outliers are far away from the remaining observations and thus outliers have little effect on the method.

% Figure environment removed

Then, in the same line of reasoning, if there are observations that connect to many other observations in the similarity graph, will they affect the method a lot? To check for this, we examine another type of perturbation:
\begin{itemize}
    \item [(3)] Hub perturbation: mislabel the sample labels of 5 nodes with the largest degrees in the graph.
\end{itemize}

% Figure environment removed

Figure \ref{Estimated power with inlier} plots the estimated power of GET under the same setting as in Figure \ref{Estimated power on 5NNG and 14NNG} but with hub perturbation. We see that mislabeling sample labels of 5 points with the largest degrees could dramatically decrease the power of the test. While using a denser graph (right panel of Figure \ref{Estimated power with inlier}) may mitigate this effect, there is still a significant decrease in power. 
One explanation behind the high influence of hubs on the performance of the graph-based method is that the method relies on the number of edges  and a node with a large degree would affect the count more, leading to a high influence. Figure \ref{degrees of mislabeled points} plots boxplots of average degrees of perturbed points under the toy example with $\sigma=1.02$. We see that the average degree of hubs are much higher than that of other selected points.

% Figure environment removed

\subsection{Relationship between hub and dimensionality}\label{sec:hub}
The toy example in Section \ref{effect of inliers} clearly demonstrates the significant influence of hubs within the $K$-NNG on the performance of graph-based methods.  Here, we further examine the relationship between hubs and data dimensionality.   We utilize the same toy example, maintaining a fixed Fubini norm of the covariance matrix difference at 0.3, while varying the dimensionality from 5 to 1,000.  Figure \ref{avg_deg_vs_dim} presents boxplots of the average degree of perturbed points in the $5$-NNG and the $14$-NNG for dimensions $5, 10, 50, 100, 200,$ and $500$. 

At low dimensions ($d=5$), we observe that the average degree of hubs slightly exceeds that of 5 randomly selected nodes.  As the dimensionality increases, the average degree of the randomly selected nodes remains relatively stable, while the average degree of the hubs experiences a significant escalation.  This results in a pronounced overweighted influence of hubs, particularly when the dimension is not too small ($d\geq 50$).  

Figure \ref{estiamted power with d} displays the estimated power of GET on the $5$-NNG and the $14$-NNG with perturbed data across different dimensions. The estimated power remains relatively stable across varying dimensions for both  no perturbation and outlier perturbation. However, in the case of hub perturbation, the estimated power is slightly lower compared to other perturbations at low dimensions and exhibits a significant decline with a moderate increase in dimension.  Notably, the estimated power with hub perturbation experiences a pronounced decrease until it reaches dimension 50, after which the decline becomes more gradual till dimension 1,000.  This pattern is consistent with the observed increase in average degrees of hubs illustrated in Figure \ref{avg_deg_vs_dim}.

%Furthermore, we investigate the performance of GET on $5$-NNG and $14$-NNG across varying dimensions. 

% We observe that, at a low dimension ($d=5$), the average degree of hubs surpasses that of both 5 randomly chosen nodes and 5 outliers, albeit marginally. As dimensionality increases, the average degree of the 5 randomly chosen nodes remains relatively constant, and the average degree of the 5 outliers drops to $5$ in the $5$-NNG and to $14$ in the $14$-NNG. Conversely, the average degree of the 5 hubs escalates dramatically. Consequently, as dimensionality grows, the difference between the average degree of the 5 hubs and that of other nodes increases substantially, leading to an overweighted influence of hubs when the dimension is not too small $(d=50)$. 

%as the dimension increases, the average degrees of hubs and the five mislabeled inner points increase dramatically leading an intensifying impact on the performance of the graph-based method. Meanwhile, the average degrees of five random nodes remain roughly constant, and the average degrees of outliers decrease until they reach the value of $K$. This is expected, as points on a ball's sphere become more likely to choose the ball's center as close neighbors with increasing dimension, unless there are enough points on the sphere, and outliers become less likely to be chosen as close neighbors.

%Additionally, we investigate the impact of dimensionality on degree distributions. Figure \ref{degree_distribution_dim_5_500} presents the degree distributions of nodes in $5$-NNG under the same settings, with dimensions set to 5 and 500. Notably, in higher-dimensional settings, the degree distribution exhibits a right-skewed pattern.

% Figure environment removed




% Figure environment removed


The presence of hubs in the $K$-NNG for moderate to high dimensions can be attributed to the \emph{curse of dimensionality}.  \cite{radovanovic2010hubs} investigated the phenomenon of hubness in the $K$-NNG for data from one distribution.  They demonstrated that, under commonly employed assumptions, the degree distribution becomes significantly right-skewed as dimension increases.  Figure \ref{degree_distribution_dim_5_500} plots the empirical degree distributions of the $5$-NNG with the data from the standard multivariate normal distribution (top panel) and the previous toy example with a fixed Fubini norm of the covariance matrix difference at 0.3 (bottom panel)  under dimensions 5, 10 and 50. We see that, as dimension increases, both degree distributions -- whether under the standard multivariate normal distribution or the toy example setting -- exhibit a more pronounced right-skewed pattern.


% explored the hubness phenomenon in the $K$-NNG for the data from identical distributions and showed that under commonly used assumptions the degree distribution becomes considerably skewed as dimensionality increases. They stated that it is an inherent property of data distributions in high-dimensional vector space. 

% Figure environment removed


\subsection{Mitigate the effect of the curse of dimensionality for graph-based methods}

In terms of the test statistic, \cite{chen2017new} had renovated the OET statistic to the GET statistic to take into account the pattern caused by the curse of dimensionality, and thus making the test statistic more robust to the  curse of dimensionality. However we see from previous examples that the recommended graphs, $K$-NNG (in Section \ref{effect of inliers} and \ref{sec:hub}) and $K$-MST (in Appendix \ref{effect on MST}), are also affected by the curse of dimensionality.  In this paper, we focus on constructing similarity graphs that are robust to the curse of dimensionality.  In particular, since hubs emerge naturally as dimension increases and graph-based methods are susceptible to hubs, we propose to  construct robust graphs by penalizing the presence of hubs.  The detailed procedure for constructing these robust graphs is provided in  Section \ref{sec:2}.  By using the robust similarity graphs, we can significantly mitigate the impact of the curse of dimensionality.

Figure \ref{estimated_power on K-RNNG} displays the estimated power of GET on the $K$-robust nearest neighbor graph ($K$-RNNG) (solid lines) under the same setting as in Figures \ref{Estimated power on 5NNG and 14NNG} and \ref{Estimated power with inlier} (dotted lines).  We see that, even though the hub perturbation (blue lines) still cause some power decrease, the decrease is much less significant compared to that using the $K$-NNG (dashed blue lines).

Figure \ref{avg_deg_vs_dim_of K-RNNG} displays the boxplots of the average degree of perturbed points in the $5$-RNNG and $14$-RNNG under a similar setting as in Figure \ref{avg_deg_vs_dim} for dimensions 5, 10, 50, 100, 200, and 500. We see that the average degrees of the five largest degrees in the 5-RNNG and the 14-RNNG are considerably smaller compared to their counterparts in the 5-NNG and the 14-NNG, as presented in Figure \ref{avg_deg_vs_dim}.  For instance, when the dimension increases from 5 to 1,000, the average degree of the five largest degrees in the $5$-NNG rises from 17 to 50.  However, in the $5$-RNNG, this average degree only experiences a modest increase, from 14 to approximately 22. 

% Figure environment removed

% Figure environment removed

Figure \ref{power of KRNNG with d} displays the estimated power of GET on the $K$-RNNG across different dimensions. It is evident that the estimated power of GET on $K$-RNNG with hub perturbation
no longer decreases as dimension increases. % and the performance of GET on $K$-RNNG is better than that of GET on $K$-NNG generally.

% Figure environment removed










%\subsubsection{Mitigate the curse of dimensionality}
%
%We first assess the performance of GET on $K$-RNNG by comparing it to GET on $K$-NNG under the same setting in Section \ref{effect of inliers}. Figure \ref{estimated_power on K-RNNG} presents the power curves for $K$-RNNG, with the power curves for $K$-NNG (from Figure \ref{Estimated power on 5NNG and 14NNG} and \ref{Estimated power with inlier}) shown as dotted lines for comparison. The result demonstrates that $K$-RNNG significantly mitigates the influence of hubs by the comparison of blue solid lines and blue dotted lines. 
%
%We also evaluate the performance of GET on $K$-RNNG's under the setting with varying dimensions. 

\subsubsection{Power improvement without perturbation}

Besides enhancing the robustness of graph-based methods against hub perturbations, the robust graphs also improves the overall power of these methods.  This improvement is evident in Figures \ref{estimated_power on K-RNNG} and \ref{power of KRNNG with d}, where the solid red lines surpass the dotted red lines representing scenarios without perturbations.  To check that this effect is not coincidental, we further examine the power of GET on both the 5-RNNG and on the 5-NNG across various settings:

%The $K$-RNNG not only makes the method more robust, but it also enhances the power of GET. Comparing the red solid lines with the red dotted lines in Figure \ref{estimated_power on K-RNNG}, it is clear that GET on $5$-RNNG has significantly higher power than GET on $5$-NNG under the toy example. We further explore the power improvement gained by applying $5$-RNNG through numerical studies under the following settings:

\begin{itemize}
    \item[1.] $X_1, {\cdots}, X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5))$,
    \item[2.] $X_1,{\cdots}, X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
    \item[3.] $X_1,{\cdots}, X_m \iidsim \text{Lognormal}(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Lognormal}(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5))$,
    \item[4.] $X_1,{\cdots}, X_m \iidsim \text{Multivariate t}_5(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_5(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
\end{itemize}
where $m=n=100$, $d = 500$. Setting 1 involves a mean shift under the multivariate normal distribution. Settings 2, 3, and 4 introduce both a mean shift and scale difference under different distributions. 

% Figure environment removed

Figure \ref{KRNNG_vs_KNNG} illustrates the estimated power of GET on both the $5$-RNNG and the $5$-NNG.  In Setting 1 where only a mean difference exists, GET on the $5$-RNNG and GET on $5$-NNG exhibit similar performance. However, in Settings 2, 3, and 4, which involve both a mean shift and scale difference are present, GET on the $5$-RNNG exhibits clearly higher power compared to GET on the $5$-NNG. This finding suggests that the $5$-RNNG is particularly advantageous in scenarios where a scale difference exists across distributions.  



%The hyperparameter $\lambda$ plays a crucial role in balancing the tradeoff between similarity and the degree distribution balance in the graph. The conventional $K$-nearest neighbor graph is achieved when $\lambda$ is set to zero. As $\lambda$ increases, the resulting graph will concentrate degrees but will have a larger sum of edge lengths. With an extremely large value of $\lambda$, all nodes will have degrees close to $2K$. Based on our experimental study in Section \ref{sec: choice of lambda}, there is a wide range of suitable values for $\lambda$. A recommended value for implementation with fixed dimensional data is $0.3$. Further research is needed to determine the optimal choice of $\lambda$.

%\cite{chen2017new} also studied the phenomenon that, when the dimension is moderate to high and the two distributions differ in scale, points in the outer layer commonly find themselves to be closer to points in the inner layer than other points in the outer layer unless the number of points in the outer layer is extremely large (exponential in $d$). The volume of a $d$-dimensional unit ball expands exponentially in $d$. When $d$ is large, we can put a huge number of points on the unit surface such that no pair of them is closer than 1. This phenomenon can also occur with data from the identical distribution. Figure \ref{degree_vs_dist2center} plots the degree of nodes with respect to its distance to the center with the data from the toy example with a fixed Fubini norm of the covariance matrix difference at 0.3 and with the data from the multivariate standard normal distribution under dimension 50. We can see that these points more close to the center of the data generally have higher degrees. \cite{chen2017new} then renovated the graph-based statistic from OET to GET to mitigate the issue caused by this phenomenon, resulting a dramatic power increase from OET for high-dimensional data under a wide range of alternatives. In this paper, we focus on renovating the similarity graph to make it less susceptible to the influence of hubs and the curse of dimensionality.
%
%% Figure environment removed
%To solve this issue, we propose a robust graph that produce a relatively balanced degree distribution called `Robust $K$-nearest neighbors graph ($K$-RNNG)', which is formally defined in Section \ref{sec:2}. The right panel of Figure \ref{fig2} is the degree distribution of the $5$-RNNG using the same data. The robust graph ensures that no few observations would have deterministic impact on the test results, making the GET on the robust 5-NNG more robust to influential inner nodes compared to GET on the 5-NNG. The estimated power of GET on the 5-RNNG using the same data can be seen in the bottom panels of Figure \ref{fig1}. Furthermore, our findings indicate that the robust graph has the potential to further improve the power of GET under various alternatives, as shown in Figure \ref{fig1}. The estimated power of GET on the 5-RNNG before mislabeling is significantly larger than the power of GET on the 5-NNG when the variance discrepancy $\sigma$ remains the same. We also applied GET on the $\sqrt{N}$-RNNG (14-RNNG for $N=200$) using the same data, but the estimated power did not show significant improvement with the use of a denser robust graph.

%The efficacy of the $K$-RNNG compared to the $K$-NNG can also be demonstrated through a real-data example, where GET on the 5-RNNG and GET on 5-NNG are applied to test the distribution discrepancy of microscopic biopsy images of benign and malignant breast tumors \citep{spanhol2015dataset}. The first 400 images, consisting of 371 benign and 29 malignant tumors, were selected for this study. Each image was represented by a $128\times128\times 3$ tensor and the dissimilarity between images was measured using the Euclidean distance. The results, including the asymptotic p-value and some other quantities (formally defined in Section \ref{sec:3}), of the two GETs are listed in Table 1. It can be observed that the p-value of GET on the 5-RNNG is much lower than that of GET on 5-NNG, indicating that GET on the 5-RNNG is more powerful in detecting the distribution difference.

\subsection{Organization}
The rest of the paper is organized as follows. In Section \ref{sec:2}, we introduce the robust similarity graph in details, investigate the asymptotic properties of the GET statistic on the proposed robust graph, and explore the choice of a hyper-parameter $\lambda$. Section \ref{sec:4} presents a comparative analysis of the performance of GET on the $K$-NNG, $K$-MST, and $K$-RNNG, along with other popular methods, in both two-sample testing and change-point detection problems through numeric studies.  The paper concludes with a discussion in Section  \ref{sec:conclusion}.

\section{\label{sec:2} Robust similarity graphs}
%\subsection{Robust nearest neighbor graph}
%Here we propose to construct a robust graph that penalize large degrees. 
Given $N$ observations, $Z_1,{\cdots}, Z_N$, and a distance metric $D(\cdot,\cdot)$, we define $R_i(Z_j)$ to be the rank of the distance $D(Z_i,Z_j)$ within the set of distances $\{D(Z_i,Z_l): l \neq i,j\}$.  Then, the $K$-NNG minimizes $\sum_{i=1}^N\sum_{x \in C_i}R_i(x)$ over all possible sets $C_i$, where $C_i$ contains $K$ observations excluding $Z_i$.   In a graph $G$, let $|G_i|$ be  the degree of the $i$-th node, taking into account  both the in-degree and out-degree.  We define the $K$-robust nearest neighbor graph ($K$-RNNG) as the graph that minimizes the objective function (\ref{obj1}) over all sets $C_i$'s:
\begin{align}
\label{obj1}
    \sum_{i=1}^N\sum_{x \in C_i}R_i(x)+\lambda\sum_{i=1}^N|G_i|^2.
\end{align}
% and is constructed by connecting each node $Z_i$ to nodes in $C_i$ with a directed arrow from $Z_i$. 
Here, $\lambda$ is a hyper-parameter and its choice is discussed in Section \ref{sec: choice of lambda}. Optimizing the objective function (\ref{obj1}) is a combinatorial problem and finding the global optimum is typically difficult. In this paper, we provide a greedy algorithm (Algorithm \ref{alg:penalized KNN}) as a practical approach.  While this algorithm may not guarantee the global optimum, we find it to be effective enough in practice.


\begin{algorithm}[H]
\caption{Constructing the $K$-robust nearest neighbor graph}\label{alg:penalized KNN}
\begin{algorithmic}[1]
\normalsize
  \item Initialize $G$ with the $K$-NNG, and compute the value of the objective function (\ref{obj1}) on $G$ and store it as $L$.
  \item Randomly permute the order of nodes and indicate them by $1,{\cdots},N$. For $i$ from $1$ to $N$, 
  \begin{itemize}
      \item Compute $W_{i}(j) = R_i(Z_j) + \lambda(|G_j^\star|+1)^2$, where $|G_j^\star| = |G_j|-1$ if node $j$ is one of neighbors of node $i$; otherwise $|G_j^\star| = |G_j|$;
      \item Find $K$ nodes with the $K$ smallest $W_{i}(j)$'s among $\{W_{i}(j)\}_{j= 1,{\cdots},i-1,i+1,{\cdots},N}$; compute objective function (\ref{obj1}) with node $i$ connecting to these $K$ nodes denote the value of \eqref{obj1} as $L^\star$; if $L^\star <L$, update the graph by pointing node $i$ to the $K$ nodes and set $L = L^\star$; otherwise do not change the graph or the value of $L$.
  \end{itemize}
  \item Repeat Step 2 until no node can find neighbors with a lower $L^\star$.
  \end{algorithmic}
\end{algorithm}





\begin{remark}
In the objective function (\ref{obj1}), the regularization term employs the total degree $|G_i|$, which is equivalent to using the in-degree as the out-degree for each node is always $K$.

% we use the total degree $|G_i|$ in the regularization term, which is equivalent to use the in-degree here because the out-degree and the graph size are fixed here to be $K$ and $KN$.
\end{remark}

\begin{remark}
The objective function (\ref{obj1}) is not limited to ranks. We could use a distance metric $D(\cdot,\cdot)$ directly, and the $K$-RNNG can be obtained by solving
\begin{align*}
    &\min_{C_i\text{'s}} \sum_{i=1}^N\sum_{x\in C_i}D(Z_i,x)+\lambda\sum_{i=1}^N|G_i|^2\\
    &\text{s.t. } Z_i\notin C_i, |C_i| = K.
\end{align*}
%The choice of using ranks in the objective function brings a distance-free graph. More importantly, it allows to study the theoretical property of the robust graph without considering the distance metric.
\end{remark}

\begin{remark}\label{robust K MST}
A similar idea can be used to extend the $K$-MST to the robust $K$-MST. Let $R(Z_i,Z_j)$ be the rank of distance $D(Z_i,Z_j)$ in the set of all pairwise distances.
The robust $K$-MST is a $K$-spanning tree $T$, which minimize the objective function 
\begin{align*}\label{obj2}
    \sum_{(z_i,z_j) \in T}R(z_i,z_j)+\lambda\sum_{i=1}^N |G_i|^2.
\end{align*}
\end{remark}

%\subsection{The merits of $K$-RNNG}


\subsection{Asymptotic properties of the GET statistic on the \texorpdfstring{$K$}{L}-RNNG} \label{sec:3}

\cite{zhu2021limiting} derived so far the best sufficient conditions on undirected graphs for the validity of the asymptotic distribution of the GET statistic. In this section, we extend their results to directed graphs and demonstrate that the $K$-RNNG graphs meet these conditions with an appropriate choice of $\lambda$. Before stating these results, we first define some essential notations.

Given a directed graph $G$ built on the pooled observations $Z_1,{\cdots},Z_N$ $(Z_i = X_i, i = 1,{\cdots},m; Z_{j+m}= Y_j, j = 1,{\cdots},n; N = m+n)$. The pair $(i,j)$ represents a directed edge pointing from node $i$ to node $j$.  We define $|G|$ to be the number of directed edges in the graph $G$. For each node $i$, we define $G_i$ as the set of edges with one node $i$, $G_{i,2}$ as the set of edges sharing at least one node with an edge in $G_i$, $node_{G_i}$ as the set of nodes that are connected in $G_i$ excluding the node $i$, and $node_{G_{i,2}}$ as the set of nodes that are connected in $G_{i,2}$ excluding the node $i$. We further define $N_0$ to be the number of edges whose reversed edge is also in $G$, i.e. $N_0 = \sum_{\left(i,j\right)\in G}1_{\{(j,i)\in G\}}$, $\Tilde{d_i}$ to be the centered degree, i.e. $\Tilde{d_i} = |G_i|-\frac{2|G|}{N}$, and $N_{sq}$ to be the number of combinations of 4 edges that form a square. Let $V_G = \sum_{i=1}^N \Tilde{d_i}^2 = \sum_{i=1}^N|G_i|^2-\frac{4|G|^2}{N}$ representing the variation of degrees. 

Besides, we use $\xrightarrow{\mathcal{D}}$ to denote convergence in distribution, and use `the usual limit regime' to refer $N\rightarrow \infty$ and $\lim_{N\rightarrow \infty}\frac{m}{N} = p\in (0,1)$. In the following, $a_n= o(b_n)$ means that $a_n$ is dominated by $b_n$ asymptotically, i.e. $\lim_{n\rightarrow \infty} \frac{a_n}{b_n}=0$, $a_n\precsim b_n$ means $a_n$ is bounded above by $b_n$ (up to a constant factor) asymptotically, and $a_n= \Theta(b_n)$ or $a_n \asymp b_n$ means that $a_n$ is bounded both above and below by $b_n$ (up to constant factors) asymptotically. We use $a\wedge b$ for $\min\{a,b\}$. For two sets $S_1$ and $S_2$, $S_1\backslash S_2$ is used for the set that contains elements in $S_1$ but not in $S_2$.

Let $l_i$ be the sample group label of $i$-th node defined as
\begin{eqnarray*}
    l_i=\left\{
\begin{aligned}
1 & \text{ if node $i$ is from sample X} \\
2 & \text{ if node $i$ is from sample Y.}
\end{aligned}
\right.
\end{eqnarray*}
Let $R_1$ and $R_2$ be the number of within-sample edges in sample $X$ and sample $Y$, respectively,
\begin{align*}
    R_1 = \sum_{(i,j)\in G} 1_{\{l_i=l_j=1\}},\quad R_2 = \sum_{(i,j)\in G} 1_{\{l_i=l_j=2\}}.
\end{align*}
Then, the GET statistic $S$ can be expressed as
$$ S = \begin{pmatrix}R_1-\ep_\P(R_1),& R_2-\ep_\P(R_2) \end{pmatrix}\left(\varp\binom{R_1}{R_2} \right)^{-1} \begin{pmatrix}R_1-\ep_\P(R_1) \\R_2-\ep_\P(R_2)\end{pmatrix},$$ 
%$$ S = \begin{pmatrix}R_1-\ep_\P(R_1),& R_2-\ep_\P(R_2) \end{pmatrix}\begin{pmatrix}\varp(R_1)&\covp(R_1,R_2)\\ \covp(R_1,R_2)&\varp(R_2)\end{pmatrix}^{-1} \begin{pmatrix}R_1-\ep_\P(R_1) \\R_2-\ep_\P(R_2)\end{pmatrix},$$ 
where $\epp$, $\varp$ and $\covp$ are the expectation, variance and covariance under the permutation null distribution which places probability $1/ {N\choose m}$ on each selection of $m$ observations among all $N$ observations as sample X.

\cite{zhu2021limiting} established the sufficient conditions for the asymptotic distribution of the test statistic via the `locSCB’ approach.  This approach relies on the equivalence between the permutation null distribution and the conditional Bootstrap distribution.  The Bootstrap null distribution assigns each observation to either sample $X$ or sample $Y$ independently, with probabilities $\frac{m}{N}$ and $\frac{n}{N}$, respectively.  Conditioning on the number of observations assigned to sample $X$ being $m$, the Bootstrap null distribution becomes the permutation null distribution.  The authors applied the Stein's method that considers the first neighbor of the dependency under the Bootstrap null distribution to derive asymptotic multivariate normality.  We here adopt a similar idea to derive the sufficient conditions for the directed graph.  The conditions are provided in Theorem \ref{th1}, and the proof of the theorem is in Supplement A. 

% \footnote{The bootstrap null distribution assigns each observation to sample X with probability $\frac{m}{N}$ and to sample Y with probability $\frac{n}{N}$, independently from any other observations.}
%
% that utilizes the equivalence of the permutation null distribution and the conditional Bootstrap null distribution  conditioning on that the number of nodes labeled as being sample $X$ is $m$, and then applies the local Stein's method under the bootstrap null distribution to show asymptotic multivariate normality. The same idea can be used here to derive the sufficient conditions on the directed graph. Theorem \ref{th1} summarizes these sufficient conditions on the directed graph with the proof provided in Appendix \ref{proof of th1}.

\begin{theorem}\label{th1}
For a directed graph $G$ with $|G| = O(N^\alpha), 1\leq \alpha<2$, under conditions 
\begin{align*}
   &\sum_{i=1}^{N}\left|G_{i}\right|^{2}=o\left(|G|^{\frac{3}{2}}\right), \sum_{i=1}^N \left|\Tilde{d_i}\right|^3 = o(V_G^\frac{3}{2}), \sum_{i=1}^N \Tilde{d_i}^3 = o(V_G\sqrt{|G|}),\\
   &\sum_{i=1}^N\sum_{(i,j) \text{ or }(j,i) \in G_i}^{(i,k) \text{ or }(k,i) \in G_i, j\neq k}\Tilde{d_j}\Tilde{d_k} = o(|G|V_G), \quad N_{sq} = o(|G|^2).
\end{align*}
 in the usual limit regime, we have $S\xrightarrow{\mathcal{D}} \chi_2^2$ under the permutation null distribution.
\end{theorem}

These sufficient conditions stated in Theorem \ref{th1} are applicable to any general directed graphs. For the $K$-RNNG, a more concise result can be obtained. By selecting an appropriate value for $\lambda$, all the sufficient conditions in Theorem \ref{th1} are satisfied for the $K$-RNNG. The main result is stated in Theorem \ref{th2} with the proof provided in Supplement A.

\begin{theorem}\label{th2}
Let $Q_N$ be the random variable generated from the degree distribution of the $K$-RNNG with $N$ nodes. Assume $K = \Theta(1)$ and if $\lambda$ is chosen such that $\var(Q_N)>0$ and $ \max\{Q_N\}\precsim N^{\frac{1}{2} -\beta}$ for some $\beta > 0$, we have $S\xrightarrow{\mathcal{D}} \chi_2^2$ under the permutation null distribution in the usual limit regime.

\end{theorem}

%\begin{remark}
%    The maximum degree of $K$-NNG is bounded when the dimension is fixed. As a result, the maximum degree of the $K$-RNNG is also bounded from Lemma \ref{lm6}, which leads to the bounded $\var(Q_N)$. This means that $\lambda$ can be selected as $\Theta(1)$ under the fixed dimension.
%\end{remark}

\begin{remark}
Theorem \ref{th2} requires that the variance of degree distribution of the $K$-RNNG is asymptotically bounded away from zero when choosing $\lambda$.  This is to ensure that the GET statistic is well defined -- when $\var(Q_N)=0$, the degrees of all nodes are the same and $\varp\binom{R_1}{R_2}$ becomes singular. This situation arises when an extremely large $\lambda$ is used. Additionally, large values of $\lambda$ diminish the utilization of the similarity information contained in the first term of the objective function \eqref{obj1}, making such choices of $\lambda$ less desirable. Therefore, we tend not to choose a very large $\lambda$ in practice.

%This is because, when the variance of the degree distribution goes to zero, meaning all degrees are almost equal, the covariance matrix in the GET statistic $S$ would be almost singular and thus $S$ would not be well-defined. This case would occur only with extremely large $\lambda$. However, under such large values of $\lambda$, the similarity information carried by the first term in the objective function (\ref{obj1}) is seldom utilized, making this choice of $\lambda$ not ideal.
\end{remark}


\begin{theorem}[Consistency under fixed dimensions] \label{th3}
For two samples generated from two continuous multivariate distributions in Euclidean space with a fixed dimension, if the graph is the $K$-RNNG with $K = \Theta(1)$ and $\lambda \geq 0$, GET is consistent against all alternatives in the usual limiting regime.
\end{theorem}


\begin{theorem}[Consistency under high dimensions]\label{th4}
Assume distributions $F_X$ and $F_Y$ satisfy Assumptions 1 and 2 in \cite{biswas2014distribution}, and $\lim_{d\rightarrow \infty}\ep(||X-\ep(X)||_2^2)/d = \sigma_1^2$, $\lim_{d\rightarrow \infty}\ep(||Y-\ep(Y)||_2^2)/d = \sigma_2^2$ and $\lim_{d\rightarrow \infty}(||\ep(X)-\ep(Y)||_2^2)/d = v^2$, where $X\sim F_X$, $Y\sim F_Y$ and $d$ is the dimension. Without loss of generality, we assume that $\sigma_1^2 > \sigma_2^2$. Then, for GET on the $K$-RNNG with $0<\lambda < (\sqrt{8NK+4N-8K}-\sqrt{8NK})^2/16$ and $\min\{m,n\} > K+2\lambda+\sqrt{8\lambda KN}$, we have $\lim_{d\rightarrow \infty}P(S>\chi_2^2(1-\alpha)) = 1$, for any fixed $\alpha \in (0,1)$, when either of the following conditions hold:
\begin{itemize}
    \item [(1)] $|\sigma_1^2 - \sigma_2^2| < v^2$, $N > 2.5 + \frac{\xi}{K}+\sqrt{0.25+3\frac{\xi}{K} + \frac{\xi^2}{K^2}}$,
    \item [(2)]
    $\sigma_1^2 - \sigma_2^2 > v^2$, $N>\frac{n^2\xi^2}{2m^2K^2}\left(\sqrt{\frac{K}{\lambda}}+\sqrt{\frac{K}{\lambda}+\frac{2mK}{n\xi}(1+\frac{K}{2\lambda}+\frac{mK}{n\xi}-K)}\right)^2$,
    \item[(3)]  $\sigma_2^2 - \sigma_1^2 > v^2$, $N>\frac{m^2\xi^2}{2n^2K^2}\left(\sqrt{\frac{K}{\lambda}}+\sqrt{\frac{K}{\lambda}+\frac{2nK}{m\xi}(1+\frac{K}{2\lambda}+\frac{nK}{m\xi}-K)}\right)^2$,
\end{itemize}
where $\xi =\chi_2^2(1-\alpha)$.
%$\frac{m}{n}K(N-1) > (\frac{K}{2\lambda}+1 + \sqrt{\frac{2NK}{\lambda}}-K)\chi_2^2(1-\alpha)$
\end{theorem}

Theorem \ref{th3} studies the consistency of GET on $K$-RNNG under  fixed dimensions as the sample size goes to infinity.  On the other hand, Theorem \ref{th4} studies the consistency as the dimension goes to infinity. The proofs of these theorems are in Supplement A. For Theorem \ref{th4}, although we usually don't know which case $\sigma_1^2$, $\sigma_2^2$ and $v^2$ satisfy, we can always choose the larger $N$ among all cases. For instance, with $\alpha = 0.05$, $\lambda = 0.3$, $m=n$ and $K = 5$, $N \geq 69$ would satisfy all cases;  and with $\alpha = 0.05$, $\lambda = 0.3$, $m/n = 2$ or $n/m = 2$ and $K = 5$, $N \geq 214$ satisfies all cases.


\subsection{Choice of \texorpdfstring{$\lambda$}{l}}\label{sec: choice of lambda}

To assess the impact of $\lambda$ on the power of GET on the 5-RNNG, we vary the value of $\lambda$ and check the empirical power of the test. We consider the following scenarios including symmetric distribution, asymmetric distribution, and heavy-tailed distribution.
% relationship between the power of graph-based method on the $5$-RNNG and the value of $\lambda$, we conducted an experimental study by applying GET on the $5$-RNNG with $\lambda$ equally ranging from $0$ to $2$ under the following distribution scenarios. 
\begin{itemize}
    \item[(i)] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 200$, $n =100$, $d= 500$ and $\delta = 1.03$.
    \item[(ii)] $X_1,{\cdots},X_m \iidsim \text{lognormal}(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{lognormal}(\delta\mathbf{1}_d,\Sigma_d(0.5))$ with $m = 100$, $n =200$, $d= 1000$ and $\delta = 0.05$.
    \item[(iii)] $X_1,{\cdots},X_m \iidsim \text{lognormal}(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{lognormal}(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 100$, $n =100$, $d= 100$ and $\delta = 1.15$.
    \item[(iv)] $X_1,{\cdots},X_m \iidsim \text{t}_5(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{t}_5(\mathbf{0}_d,\delta\Sigma_d(0.5))$ with $m = 100$, $n =100$, $d= 500$ and $\delta = 1.35$.
    \item[(v)] $X_1,{\cdots},X_m \iidsim \text{t}_5(\mathbf{0}_d,\Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{t}_5(\delta\mathbf{1}_d, \Sigma_d(0.5))$ with $m = 300$, $n =100$, $d= 500$ and $\delta = 0.095$.
\end{itemize}
Here, $\delta$ is chosen so that the tests have moderate power when $\lambda$ is equal to zero.

Figure \ref{lambda_selection} presents the estimated power of the test across different values of $\lambda$.  The results indicate a rapid increase in power as $\lambda$ rises from 0 to 0.3, followed by a slower rate of improvement.  To determine a default value for $\lambda$, we employ the elbow method, which suggests a value of 0.3 would be appropriate.


%The results indicate that a small increase in $\lambda$ leads to a substantial improvement in the estimated power and the estimated power increases slightly after $\lambda$ exceeds $0.5$. The values between 0.3 and 0.5 could be suitable.  %We recommend setting $\lambda$ to $0.3$ as the default value because a larger value of $\lambda$ will reduce the weight of similarity information in the objective function (\ref{obj1}).



%We also plot the maximum center degree against $\lambda$ in Figure \ref{lambda_variance}.  We can see that the maximum center degree decreases quickly as $\lambda$ increases from 0 to 0.3, followed by a much slower rate of decrease.  This  is consistent with the trend in the estimated power shown in Figure \ref{lambda_selection}. Therefore, one practical approach to selecting $\lambda$ is to plot the maximum degrees for different values of $\lambda$ and choose the value at which the maximum degree changes slowly.

%The graph demonstrates that a slight increase in $\lambda$ results in a rapid decrease in the maximum center degree. After $\lambda$ exceeds $0.5$, the maximum degree changes slowly, which is consistent with the trend in the estimated power shown in Figure \ref{lambda_selection}. Therefore, one practical approach to selecting $\lambda$ is to plot the maximum degrees for different values of $\lambda$ and choose the value at which the maximum degree changes slowly.


In Figure \ref{lambda_variance}, we plot the relationship between the maximum center degree and $\lambda$. It is evident that the maximum center degree experiences a rapid decline when $\lambda$ increases from 0 to 0.3, followed by a slower rate of decrease. This observation aligns with the trend depicted in Figure \ref{lambda_selection}, which showcases the estimated power. Consequently, a practical data-driven approach to select an appropriate $\lambda$ is to plot the maximum degrees for various values of $\lambda$ and identify the point at which the maximum degree exhibits minimal changes.

% Figure environment removed

% Figure environment removed

\section{Numerical studies}\label{sec:4} In this section, we evaluate the performance of GET on $K$-RNNG by comparing it with other state-of-the-art methods in both the two-sample testing and change-point detection problems.

\subsection{Two-sample testing}
We consider GET on 5-RNNG (New), 5-MST,  $\sqrt{N}$-MST ($N = m+n$ is the total sample size), and other popular tests: the cross-match test \citep{rosenbaum2005exact} (CM), the Ball divergence test \citep{pan2018ball} (BD), the mutivariate rank-based test \citep{deb2021multivariate} (MT), the Adaptable Regularized Hotelling's ${T}^2$ test \citep{li2020adaptable} (ARHT) and the kernel test based on minimum mean discrepancy \citep{gretton2012kernel} (MMD), under the following simulation scenarios:
\begin{itemize}
    \item[1.] $X_1,{\cdots},X_m \iidsim N(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim N(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}\mathbf{I}_d)$,
    \item[2.] $X_1,{\cdots},X_m \iidsim \text{Lognormal}(\mathbf{0}_d,\Sigma_d(0.6))$, $Y_1,{\cdots},Y_n \iidsim \text{Lognormal}(\mathbf{\Delta},\Sigma_d(0.2))$,
    \item[3.] $X_1,{\cdots},X_m \iidsim \text{Multivariate t}_2(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_2(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5)+\delta\mathbf{I}_d)$,
    \item[4.] $X_1,{\cdots},X_m \iidsim \text{Multivariate t}_1(\mathbf{0}_d, \Sigma_d(0.5))$, $Y_1,{\cdots},Y_n \iidsim \text{Multivariate t}_1(\frac{\delta}{\sqrt{d}}\mathbf{1}_d, \Sigma_d(0.5)+\frac{\delta}{2}\mathbf{I}_d)$,
\end{itemize}
where $\mathbf{\Delta}$ a $d$-dimensional vector with the first $\sqrt{d}$ elements equal to $\delta$ and the remaining elements equal to $0$.
%$\mathbf{0}_d$ is a $d$ dimensional vector with element $0$, $\mathbf{1}_d$ is a $d$ dimensional vector with element $1$, $\Sigma_d$ is a $d\times d$ matrix with $(i,j)$-th element $0.5^{|i-j|}$ and $\mathbf{I}_d$ is a $d\times d$ identity matrix.
In each scenario, we set $m=n=100$ and $d = 50, 500, 1000$. The estimated powers computed from 1000 repetitions are plotted in Figure \ref{two_sample_sim}. Firstly, we observe that the empirical sizes of the GET on the $5$-RNNG are well controlled for Scenario 1, 3 and 4 (at $\delta=0$ where the two distributions are the same). In Scenario 1,  the power of the new test is slightly worse than that of BD; however, the new test excels BD in other scenarios, especially in Scenario 2 and 4 where the improvement is significant.  Also, the new test consistently surpasses the GET on $5$-MST and the GET on $\sqrt{N}$-MST. This result underpins the advantages of employing $K$-RNNG, considering the tests only differ in their graph structures. 

\begin{comment}
\begin{table}
\centering
\caption{Quantities from two GETs}
\begin{tabular}{|c|c|c|} 
\hline
              & Robust 5-NNG & 5-NNG     \\ 
\hline
p-value       & 4.163e-14       & 0.3517    \\
$R_1$\tablefootnote{$R_1$: the number of edges with two nodes within the first sample (images of benign breast tumors); $R_2$: the number of edges with two nodes within the second sample (images of malignant breast tumors)}      & 1824            & 1850      \\
$R_2$      & 27              & 2         \\
$\epp(R_1)$   & 1720.175        & 1720.175  \\
$\epp(R_2)$   & 8.796992        & 8.796992  \\
$\varp(R_1)$ & 660.9824        & 9277.245  \\
$\varp(R_2)$ & 13.93628        & 60.57411  \\
$Z(R_1)$\tablefootnote{$Z(R_1): = (R_1 - \epp(R_1))/\sqrt{\varp(R_1)}$; $Z(R_2): = (R_2 - \epp(R_2))/\sqrt{\varp(R_2)}$}   & 4.038378        & 1.347     \\
$Z(R_2)$   & 4.876           & -0.8733   \\
\hline
\end{tabular}
\end{table}
\end{comment}


% Figure environment removed


\subsection{Change-point detection}
For graph-based change-point detection, MET is often recommended over GET \citep{chen2015graph, chu2019asymptotic,liu2022fast,song2022asymptotic}, so we check the performance of both MET on $5$-RNNG and GET on $5$-RNNG in this section. We include in the comparison GET scan statistic on $5$-MST, MET scan statistic on $5$-NNG, and the distance-based approach in \citep{matteson2014nonparametric, james2013ecp} (e.divisive), and
consider the following simulation settings:

%We compare the $5$-RNNG on the power and accuracy of the graph-based edge-count method in change-point detection with the $5$-MST and $5$-NNG. \cite{chu2019asymptotic} compared GET and MET scan statistics on $5$-MST with the method based on Hotelling's $\text{T}^2$ scan statistic, the method based on generalized likelihood ratio scan statistic, and the original edge-count scan statistic on $5$-MST \citep{chen2015graph}, and recommended the generalized edge-count scan statistic on $5$-MST. Later, \cite{liu2022fast} compared MET scan statistic on $5$-KNNG with three state-of-the-art
%methods: MET on $5$-NNG, the distance-based method \citep{matteson2014nonparametric} and the kernel-based method \citep{arlot2019kernel}. They recommended the MET scan statistic on $5$-KNNG. Here, we compare GET scan statistic on $5$-RNNG, MET scan statistic on $5$-RNNG, with GET scan statistic on $5$-MST, MET scan statistic on $5$-NNG, and the distance-based approach in \citep{matteson2014nonparametric, james2013ecp} (e.divisive). The hyperparameter $\lambda$ is set to be $0.3$ for building the $5$-RNNG. We consider following simulation settings:
\begin{itemize}
    \item[1.]  $X_1,{\cdots},X_\tau \iidsim  N(\mathbf{0}_d, \Sigma_d(0.5))$, $X_{\tau + 1},{\cdots}, X_{N} \iidsim N(\frac{\delta}{\sqrt{d}} \mathbf{1}_d,\Sigma_d(0.5)+\frac{\delta}{\sqrt{d}}{\mathbf{I}_d})$,
    
    \item[2.] $X_1,\cdots,X_\tau \iidsim N(\mathbf{0}_d, \mathbf{I}_d)$, $X_{\tau + 1},{\cdots}, X_{N} \iidsim N(\mathbf{0}_d, \Sigma_d(\delta))$,
    
    \item[3.] $X_1,\cdots,X_\tau \iidsim \text{Multivariate t}_5(\mathbf{0}_d, \Sigma_d(0.5))$, $X_{\tau + 1},{\cdots}, X_{N} \iidsim \text{Multivariate t}_5(\frac{\delta}{d}\mathbf{1}_d, \delta\mathbf{I}_d + \Sigma_d(0.5))$.
\end{itemize}

In each setting, we set $N$ to be 400, $\tau$ to be at $100,200$ or $300$, and $d$ to be $100$ or $500$. The estimated power is computed as the proportion of trials, among 1,000 trails, that the test reject the null hypothesis of homogeneity, and accuracy is computed as the proportion of trials the null hypothesis is rejected and the estimated change-point $\hat \tau$ satisfying $|\hat \tau-\tau|\leq 10$. The estimated power and accuracy under Setting 1 are plotted in Figure \ref{cp_normal}, and the estimated power and accuracy under other settings are plotted in \ref{cp_lognormal_diff} and \ref{cp_t1} in Appendix \ref{result in change-point}. In all settings, it is clear that the MET and GET scan statistics on the $5$-RNNG have higher power and accuracy than other tests, with the MET scan statistic on the 5-RNNG slightly better than the GET scan statistic on the 5-RNNG.

\section{Conclusion and discussion} \label{sec:conclusion}
In this paper, we propose a novel similarity graph that is less impacted by the curse of dimensionality. The new graphs reduce the influence of hubs by penalizing high degree of hubs. Numeric studies illustrate that the utilization of this new graph can bolster the efficacy of graph-based tests in the realms of both two-sample testing and offline change-point detection problems. However, the benefits of this innovative robust similarity graph are not confined to these areas. It has the potential to improve the performance and mitigate the effect from the curse of dimensionality by replacing conventional similarity graphs in multiple fields, including online change-point detection, independence testing, and clustering. For instances, the efficiency of the GET or MET with $K$-NNG in online change-point detection \citep{chen2019sequential, chu2022sequential} might be further augmented by using the new $K$-RNNG, and the performance of the high-dimensional clustering approach that utilize the $K$-NNG \citep{chen2023new} might be enhanced by the new $K$-RNNG as well.

% Figure environment removed


\begin{appendix}
\newpage 
%\section{Algorithm for constructing the \texorpdfstring{$K$}{K}-NNG}\label{alg1}

\section{Effect of hubs on GET on $K$-MST} \label{effect on MST}

We apply GET on $K$-MST under the same settings in Section \ref{effect of inliers}. Figure \ref{effect_inlier_MST} shows the estimated power of GET on $K$-MST with and without perturbations, from which we can see that the performance of GET on $K$-MST is similar to that of GET on $K$-NNG, and hubs have dominated effect on the power of GET on $K$-MST compared with other nodes.



We also investigate the effect of hubs and dimensionality in the $K$-MST. Figure \ref{degrees of mislabeled points in MST} depicts the average degree of perturbed points in $5$-MST and $14$-MST with $\sigma = 1.02$ and $d = 1000$.  We see a similar pattern as in Figure \ref{degrees of mislabeled points} that the average degree of hubs are much higher than that of other selected points.  Figure \ref{avg_deg_vs_dim_MST} shows the average degree of perturbed points $5$-MST and $14$-MST across different dimensions, and Figure \ref{estiamted power with d in MST} shows the estimated power of GET on $5$-MST and $14$-MST across different dimensions with the simulation setting the same as that in Figure \ref{estiamted power with d}.  We also see similar patterns as those of $K$-NNG in Figure \ref{avg_deg_vs_dim} and \ref{estiamted power with d}.

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed


\newpage 
\section{Estimated power and accuracy in the change-point detection analysis} \label{result in change-point}

% Figure environment removed

% Figure environment removed

\end{appendix}


\section*{Acknowledgements}

The authors was supported in part by NSF award DMS-1848579.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, if any, should   %%
%% be provided in {supplement} environment  %%
%% with title inside \textbf{} and short    %%
%% description below.                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{supplement}
\textbf{Supplemental Materials to ``Robust graphs for graph-based methods"}
\sdescription{This supplemental materials contain proof to the theorems.}
\end{supplement}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  imsart-???.bst  will be used to                        %%
%%  create a .BBL file for submission.                     %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%  MR numbers will be added by VTeX.                      %%
%%                                                         %%
%%  Use \cite{...} to cite references in text.             %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% if your bibliography is in bibtex format, uncomment commands:
%\newpage
\bibliographystyle{imsart-nameyear} % Style BST file (imsart-number.bst or imsart-nameyear.bst)
\bibliography{reference}      % Bibliography file (usually '*.bib')

%% or include bibliography directly:
% \begin{thebibliography}{}
% \bibitem{b1}
% \end{thebibliography}

\end{document}
