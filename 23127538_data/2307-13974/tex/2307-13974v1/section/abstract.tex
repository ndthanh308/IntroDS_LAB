\begin{abstract}
	Visual object tracking is a fundamental video task in computer vision.
	Recently, the notably increasing power of perception algorithms allows the unification of single/multi-object and box/mask-based tracking.
	Among them, the Segment Anything Model (SAM) attracts much attention.
	In this report, we propose HQTrack, a framework for \textbf{H}igh \textbf{Q}uality \textbf{Track}ing anything in videos. HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR).
	Given the object to be tracked in the initial frame of a video, VMOS propagates the object masks to the current frame.
	The mask results at this stage are not accurate enough since VMOS is trained on several close-set video object segmentation (VOS) datasets, which has limited ability to generalize to complex and corner scenes.
	To further improve the quality of tracking masks, a pre-trained MR model is employed to refine the tracking results.
	%Ultimately, 
	As a compelling testament to the effectiveness of our paradigm, 
	without employing any tricks such as test-time data augmentations and model ensemble,
	HQTrack ranks the 2$^{nd}$ place in the Visual Object Tracking and Segmentation (VOTS2023) challenge.
	Code and models are available at \href{https://github.com/jiawen-zhu/HQTrack}{https://github.com/jiawen-zhu/HQTrack}.
 \end{abstract}