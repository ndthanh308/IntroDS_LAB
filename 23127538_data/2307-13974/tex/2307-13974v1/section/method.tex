
% Figure environment removed


\section{Method}
In this section,  we present our HQTrack in detail. 
We first showcase the pipeline of our method. Subsequently, we introduce each component in our framework. Finally, we describe the training and inference details. 

\subsection{Pipeline}
\label{sec:pipline}

The pipeline of the proposed HQTrack is depicted in Figure~\ref{fig:method_overview}.
Given a video and the first frame reference (mask annotated), 
HQTrack first segments the target objects for each frame via VMOS.
The segmentation results of the current frame are from the propagation of the first frame along the temporal dimension, utilizing the modeling of appearance/identification information and long/short-term memory.
VMOS is a variant of DeAOT\cite{deaot} so that it can accomplish the modeling of multiple objects in a scene within a single propagation process.
Furthermore, we employ HQ-SAM\cite{sam_hq} as our MR to refine the segmentation masks of VMOS.
HQ-SAM is a variant of SAM\cite{sam}, it can handle objects with more complex structures than SAM.
We first perform bounding box extraction on the target masks predicted by VMOS, and they are fed into the HQ-SAM model as box prompts.
Last, we design a mask selector to select the final results from VMOS and MR.


\subsection{Video Multi-object Segmenter (VMOS))}
\label{sec:vmos}
VMOS is a variant of DeAOT\cite{deaot}, thereby in this subsection, we first provide a brief revisiting of DeAOT which is the baseline of our VMOS, then we delve into the design of our VMOS.

\noindent\textbf{DeAOT.}
AOT\cite{aot} proposes to incorporate an identification mechanism to associate multiple objects in a unified embedding space which enables it to handle multiple objects in a single propagation. 
DeAOT is a video object segmentation model with a AOT-like hierarchical propagation. To alleviate the loss of object-agnostic visual information in deep propagation layers, DeAOT proposes to decouple the propagation of visual and identification embeddings into a dual-branch gated propagation module (GPM).
GPM is an efficient module with single-head attention for constructing hierarchical propagation.

\noindent\textbf{VMOS.}
The video multiple object segmenter (VMOS) in HQTrack is a variant of DeAOT.
As shown in the left of Figure~\ref{fig:method_overview}, to improve the segmentation performance, especially perceiving tiny objects,
%in complex scenarios, 
we cascade a GPM with 8$\times$ scale and expand the propagation process to multiple scales.
The original DeAOT only performs propagating operation on the visual and identification features of 16$\times$ scale. 
At this scale, lots of detailed object clues are lost, especially for tiny objects, 16$\times$ scale features are insufficient for accurate video object segmentation.
In our VMOS, considering the memory usage and model efficiency, we only use up-sampling and linear projection to upscale the propagation features to 4$\times$ scale.
Multi-scale propagation features will be fed into the decoder along with multi-scale encoder features for mask prediction.
Decoder is a simple FPN~\cite{fpn}.
In addition, as a new large-scale CNN-based foundation model, Internimage~\cite{internimage} 
employs deformable convolution as the core operator, showing impressive performance on various representative tasks \eg, object detection and segmentation.
In VMOS, Intern-T is employed as our encoder to enhance object discrimination capabilities. 

\subsection{Mask Refiner (MR)}
\label{sec:mr}

MR is a pre-trained HQ-SAM~\cite{sam_hq}, in this section, we first revisit the HQ-SAM method which is a variant of SAM~\cite{sam}, then we provide the usage of HQ-SAM.

\noindent\textbf{SAM and HQ-SAM.}
Segment anything model (SAM) has recently attracted high-heat attention in the field of image segmentation, and researchers have utilized SAM to secondary a series of work (including but not limited to segmentation) with many stunning results.
SAM scales up segmentation models by training with a high-quality annotated dataset containing 1.1 billion masks.
In addition to the powerful zero-shot capabilities brought  by large-scale training, SAM also involves flexible human interaction mechanisms achieved by different prompt formats.  
However, when the processed image contains  objects with intricate structures, SAM's prediction masks tend to fall short.
To tackle such an issue as well as maintain  SAM's original promptable design, efficiency, and zero-shot generalizability, Ke~\etal propose HQ-SAM~\cite{sam_hq}.  HQ-SAM introduces a few additional parameters to the pre-trained SAM model. High-quality mask is obtained by injecting a learning output token into SAM's mask decoder.

\noindent\textbf{MR.} HQTrack employs the above HQ-SAM as our mask refiner.
As shown in the right of Figure~\ref{fig:method_overview}, we take the prediction mask from VMOS as the input of MR.
Since the VMOS model is trained on scale-limited close-set datasets, the first stage mask from VMOS probably with insufficient quality especially handling some complex scenarios.
Hence, employing a large-scale trained segmentation algorithm to refine the primary segmentation results will bring considerable performance improvement.
Specifically, we calculate the outer enclosing boxes of the predicted mask from VMOS as the box prompts and feed them into HQ-SAM together with the original image to obtain the refined masks. HQ-SAM here is a version with a ViT-H backbone.
Finally, the output mask of HQTrack is selected from the mask results from VMOS and HQ-SAM. 
Specifically, we find that for the same target object, the mask refined by HQ-SAM is sometimes completely different from the predicted mask of VMOS (very low IoU score) which instead harms the segmentation performance. This may be a result of the different understanding and definition of object between HQ-SAM and reference annotation. Hence, we set an IoU threshold $\tau$ (between masks from VMOS and HQ-SAM) to determine which mask will be used as the final output. In our case, when the IoU score is higher than $\tau$, we choose the refined mask. This process constrains HQ-SAM to focus on refining the current object mask rather than re-predicting another target object.


