

\section{Implementation Details}
% \label{sec:experiments}

In VMOS of HQTrack, InternImage-T~\cite{internimage} is employed as the backbone for the image encoder for the trade-off between accuracy and efficiency.
The layers number of the GMP for 16$\times$ and 8$\times$ scale is set to 3 and 1. The 4$\times$ scale propagation features are up-sampled and projection features from 8$\times$ scale.
The long and short-term memory is used in our segmenter to deal with object appearance changes in long-term video sequences.
To save memory usage, we use a fixed length of long-term memory of 8, excluding the initial frame, the early memory will be discarded. 

\noindent\textbf{Model Training.}
The training process comprises two stages, following previous methods~\cite{aot,deaot}. 
In the first phase, we pre-train VMOS on synthetic video sequences generated from static image datasets~\cite{cheng2014global,everingham2010pascal,hariharan2011semantic,lin2014microsoft,shi2015hierarchical}.  
In the second stage, VMOS uses multi-object segmentation datasets for training for a better understanding of the relationship between multiple objects. The training splits of DAVIS~\cite{davis}, YoutubeVOS~\cite{youtubevos}, VIPSeg~\cite{vipseg}, BURST~\cite{burst}, MOTS~\cite{mots}, and OVIS~\cite{ovis} are chosen for training our VMOS, in which OVIS is employed to improve the robustness of the tracker in handling occluded objects. 
We use 2 NVIDIA Tesla A100 GPUs with a global batch size of 16 to train our VMOS.
The pre-training stage uses an initial learning rate of $4 \times 10^{-4}$
for 100,000 steps.
The second stage uses an initial learning rate of $2 \times 10^{-4}$ for 150,000 steps. Learning rates gradually decay to $1 \times 10^{-5}$ in a polynomial manner~\cite{yang2020collaborative}.

\noindent\textbf{Inference.}
The inference process is as described in our pipeline. 
We do not use any test time augmentation (TTA) such as flipping, multi-scale testing, and model ensemble.
