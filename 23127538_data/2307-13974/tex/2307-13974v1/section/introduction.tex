\section{Introduction}
\label{sec:intro}
As a fundamental video task in computer vision, visual object tracking has become the cornerstone of many related areas, such as robot vision and autonomous driving. The task aims at consistently locating the specified object in a video sequence. As one of the most influential challenges in the tracking field, Visual Object Tracking (VOT) challenge \cite{vot21,vot22} attracted a lot of attention, and many SOTA algorithms participate to show their cutting-edge performance. The Visual Object Tracking and Segmentation challenge (VOTS2023\footnote{\url{https://www.votchallenge.net/vots2023}}) relaxes the constraints enforced in past VOT challenges for considering general object tracking in a broader context. Therefore, VOTS2023 merges short-term and long-term, single-target and multiple-target tracking with segmentation as the only target location specification. This poses more challenges, \eg, inter-object relationship understanding, multi-target trajectory tracking, accurate mask estimation, etc.

Visual object tracking has made great strides with deep learning techniques\cite{alexnet, resnet, vit}. 
Previous methods can be grouped into either online-update trackers\cite{eco, dimp} and Siamese trackers\cite{siamesefc, siamrcnn}. 
Recently, Transformer\cite{attention_is_all} sweeps in computer vision, the dominant tracking methods are Transformer-based trackers\cite{transt,stark,mixformer,ostrack}. TransT\cite{transt} proposes transformer-based ECA and CFA modules to replace the long-used correlation calculation.
Benefiting from Transformer's superior long-range modeling capability, TransT outperforms the previous correlation modules which are a capable class of linear modeling. 
More recently, some trackers\cite{mixformer, ostrack} introduce pure transformer architecture, and the feature extracting and template-search region interaction is completed in a single backbone, tracking performance is pushed to new records.
These trackers mainly focus on single object tracking and output the bounding box for performance evaluation.
Hence, merely employing SOT trackers is not well-suited to the VOTS2023 challenge.

Video object segmentation aims to segment out the specific objects of interest in a video sequence. Similar to VOT, semi-supervised video object segmentation also manually provides the annotated in the first frame. The main difference is that the VOS task provides a more fine-grained mask annotation. 
Early VOS methods propagate object masks over video frames via motion clues\cite{segflow,tsai2016video} or adopt online learning strategies\cite{caelles2017one,li2018video}.
Recently, Space-Temporal Memory (STM) network~\cite{oh2019video, wang2017learning} extracts the spatio-temporal context from a memory bank to handle the appearance changes and occlusions, offering a promising solution for semi-supervised video object segmentation.
For multi-object segmentation, these methods segment the objects one by one, the final results are merged masks by post ensemble.
AOT\cite{aot} proposes an identification mechanism that can encode, match, and segment multiple objects at the same time. Based on AOT\cite{aot}, DeAOT\cite{deaot} decouples the hierarchical propagation of object-agnostic and object-specific embeddings from previous frames to the current frame, further improving the VOS accuracy.

Although the above VOS methods can handle tracking task with multi-object and mask output, 
challenges in VOTS2023 benchmark remain. 
\textbf{\textit{(i)}} VOTS videos contain a large number of long-term sequences, the longest of which exceeds 10,000 frames, which requires the tracker to be able to discriminate the drastic changes in object appearance and adapt to variations in the environment.
At the same time, long-term video sequences also make some memory-based methods face memory bank space challenges.
\textbf{\textit{(ii)}} In VOTS videos, targets will leave the field of view and then returns.
Trackers require additional design to accommodate the disappearance and appearance of targets. 
%in long-term sequences.
\textbf{\textit{(iii)}} A series of challenges such as fast motion, frequent occlusion, distractors, and tiny objects also make this task more difficult. 

In this work, we propose Tracking Anything in High Quality (termed HQTrack), which mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). VMOS is an improved variant of DeAOT~\cite{deaot}, we cascade a 1/8 scale gated propagation module (GPM) for perceiving small objects in complex scenarios. Besides, Intern-T\cite{internimage} is employed as our feature extractor to enhance object discrimination capabilities.
To save memory usage, a fixed length of long-term memory is used in VMOS, excluding the initial frame, the memory of early frames will be discarded.
On the other hand, it should be beneficial to apply a large segmentation model to refine our tracking masks.
SAM\cite{sam} is prone to failure when predicting objects with complex structures\cite{sam_hq}, and these difficult cases are common in VOTS chanllenge.
To further improve the quality of tracking masks, a pre-trained HQ-SAM\cite{sam_hq} model is employed to refine  the tracking masks. 
We calculate the outer enclosing boxes of the predicted masks from VMOS as box prompts and feed them into HQ-SAM together with the original images to gain the refined masks, the final tracking results are selected from VMOS and MR.

Finally, HQTrack obtains an impressive 0.615 quality score on the VOTS2023 test set, achieving runner-up at the VOTS2023 challenge.
	
	
	
	