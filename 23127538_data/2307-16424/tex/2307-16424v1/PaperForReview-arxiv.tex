% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{wrapfig}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{color}
%\usepackage{subfigure}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning}

\author{Baoquan Zhang\\
Harbin Institute of Technology, Shenzhen\\
%Institution1 address\\
{\tt\small zhangbaoquan@stu.hit.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Demin Yu\\
Harbin Institute of Technology, Shenzhen\\
%First line of institution2 address\\
{\tt\small deminyu98@gmail.com}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
    Equipping a deep model the abaility of few-shot learning, i.e., learning quickly from only few examples, is a core challenge for artificial intelligence. Gradient-based meta-learning approaches effectively address the challenge by learning how to learn novel tasks. Its key idea is learning a deep model in a bi-level optimization manner, where the outer-loop process learns a shared gradient descent algorithm (i.e., its hyperparameters), while the inner-loop process leverage it to optimize a task-specific model by using only few labeled data. Although these existing methods have shown superior performance, the outer-loop process requires calculating second-order derivatives along the inner optimization path, which imposes considerable memory burdens and the risk of vanishing gradients. Drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be actually viewed as a reverse process (i.e., denoising) of diffusion where the target of denoising is model weights but the origin data. Based on this fact, in this paper, we propose to model the gradient descent optimizer as a diffusion model and then present a novel task-conditional diffusion-based meta-learning, called MetaDiff, that effectively models the optimization process of model weights from Gaussion noises to target weights in a denoising manner. Thanks to the training efficiency of diffusion models, our MetaDiff do not need to differentiate through the inner-loop path such that the memory burdens and the risk of vanishing gradients can be effectvely alleviated. Experiment results show that our MetaDiff outperforms the state-of-the-art gradient-based meta-learning family in few-shot learning tasks.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
With a large number of labeled data, deep learning techniques have shown superior performance and made breakthrough on various computer vision and nature language process tasks. However, collecting such much data may be impractical or diffcult on some applications such as drug screening \cite{Altae-TranRPP16} and cold-start recommendation \cite{VartakTMBL17}. Inspired by the fast learning abaility of humans, i.e., humans can quickly learn a new concept or task from only very few examples, few-shot learning (FSL) has been proposed to equip deep models with fast learning abaility and has gained wide attention. FSL aims to learn transferable knowledge from data-ubuntant base classes and then leverages it to assist novel class classification with few examples \cite{WangYKN20}.

% Figure environment removed

To address this FSL problem, meta-learning \cite{finn2017model, RaviL17, vinyals2016matching, snell2017prototypical, rusu2018meta} is proposed, which constructs a large number of base tasks from these base classes and then leverages it to learn task-agnostic meta knowledge for assisting novel class prediction. Among these methods, gradient-based meta-learning methods \cite{finn2017model, RaviL17, rusu2018meta, baik2020meta, baik2021meta} are gaining increased attention, with its potential on generalization capability. This type of methods aims to learn a sample-effcient gradient descent algorithm (called meta-optimizer) by directly modeling its initialization \cite{finn2017model}, learning rate \cite{baik2020meta}, or update rule \cite{RaviL17} as a shared meta parameter (i.e., meta knowledge) and then learning it in a bi-level (i.e., outer-loop and inner-loop) optimization manner. Here, the outer-loop process accounts for learning a task-agnostic meta parameter for meta-optimizer, while the inner-loop process leverages the meta-optimizer to learn a task-specific model with few gradient updates. Although these methods have shown superior performance, the outer-loop process requires backpropagation along inner-loop optimization path and calculating second-order derivatives for learning meta-optimizer such that significant memory overhead and the risk of vanishing gradient is imposed during training, which degrades meta-learning performance. Some works attempt to address the above issues but from the perspective of gradient approximate estimations \cite{rajeswaran2019meta, nichol2018first}, which would introduce an estimation error into the gradient (i.e., meta-gradient) for outer-loop optimization, and hamper its generalization ability. 

%Diffusion models is a popular type of deep generative models that modeling and learning the generation process of target data from random Gaussion noises in a forward diffusion and backward denoise manner. The forward diffusion step aims to use a Markov chain of diffusion steps to slowly add random noise to data, which is used for training difussion models, and then the backward denoise step account for reversing the diffusion process to generate target data in a denoise manner from the Gaussion noise. 
%With the superior properties of diffusion models, e.g., its stationary training objective and easy scalability, the diffusion model have been widely exploited on various tasks (e.g., high-quality image generation) [56, 59, 25] and achieved remarkable performance improvement. However, at present, there are very a few works exploring it on FSL. 

In this paper, we also focus on gradient-based meta-learning with its good generalization but present a new diffusion perspective to model gradient descent algorithm (i.e., meta-optimizer). This is inspired by diffusion models that effectively models the generation process of data in a reverse diffusion and forward denoise manner. As shown in Figures~\ref{fig1a} and \ref{fig1b}, we find that 1) the optimizaton process of gradient descent algorithm (see Figure~\ref{fig1a}) is actually equivalent to the denoising process of diffusion models from a Gaussion noise data to a target data (see Figure~\ref{fig1b}); and 2) the latter is a generalized version of the former with weight momentum update and uncertainty estimation (see Section~\ref{sec_4_1}). The key difference is that the denoising variable is model weights in gradient descent algorithm but origin image data in diffusion models. In other words, the optimization process of gradient descent algorithm can be described as: given a randomly initial weight, the target weight is finally obtained by gradually removing its noise. 

Based on this fact, we propose a novel meta-learning with conditioned diffusion, called MetaDiff. As shown in Figure~\ref{fig1c}, our idea is regarding model weights as denoising variables, and then modeling gradient descent algorithm as a diffusion model and learning it in a diffusion manner. The key challenge of achieveing the above idea is how to predict the diffused noise of model weights at each time step $t-1$ with few labeled samples for a base learner. To address this challenge, we take the few labeled samples as the conditions of diffusion models and carefully design a gradient-based task-conditional UNet for noise prediction. Different from previous gradient-based meta-learning methods that learning meta-optimizer in a bi-level optimization manner, our MetaDiff learns it in a diffusion manner. The advantage of such design is that it do not need to differentiate through the inner-loop path, such that the memory burden and the risk of vanishing gradients can be effectvely alleviated. 

Our main contributions can be summarized as follows:
\begin{itemize}
	\item We are the first to reveal the close connection between gradient descent algorithm and diffusion models. From workflow view, the optimization process of gradient descent can be viewed as a denoising process of diffusion models. From theoretical view, the denoising process of diffusion models can be regareded as a generalized gradient descent algorithm with weight momentum updates and uncertainty estimation. 
	
	\item We propose a novel diffusion-based meta-learning method for FSL, i.e., introducing the idea of diffusion to model gradient descent algorithm and learning it in a diffusion manner. In particular, a gradient-based task-conditional UNet is designed as our meta-learner for predicting diffusion noises. Thanks to the training efficiency of diffusion manner, the issue of memory burden and vanishing gradients of gradient-based meta-learning can be effectvely alleviated.
	
	\item We conduct comprehensive experiments on four FSL data sets, which verify the effectivenss of our method.
\end{itemize}


\section{Related Work}
\label{sec:relatedwork}

%-------------------------------------------------------------------------
\subsection{Meta-Learning}
Few-shot learning (FSL) is a challenging task, which aims to recognize novel classes with few labeled samples \cite{chen2021meta, ChenLKWH19}. To address this FSL problem, meta-learning (also called learning to learn) is proposed, which aims to learn to quickly learn novel tasks with few examples \cite{flennerhag2019meta, zhu2023transductive}. The core idea is learning task-agnostic meta knowledge from a large number of similar tasks and then levelrage it to assist the learning of novel tasks. From the type of meta-knowledge, these existing meta-learning methods can be roughly grouped into three groups. \emph{1) The metric-based methods} \cite{snell2017prototypical, vinyals2016matching, zhang2021prototype, zhang2022sgmnet, zhang2022metanode, zhang2022hyperbolic, zhang2023prototype} regard the metric space or metric strategy as meta knowledge and perform the novel class predictions by using a nearest-prototype manner. For example, %in \cite{zhang2022sgmnet},  a scene graph matching method is proposed to evaluate the similarity between two scene images for FSL.
Zhang et al. \cite{zhang2022hyperbolic} introduce a class hierarchy as prior and attempt to leverage it to learn a hyperbolic metric space such the metric knowledge can be well transfered to novel classes. \emph{2) The model-based methods} \cite{hou2019cross, zhmoginov2022hypertransformer, li2019lgm} regard the black-box model as meta knowledge, which aims to leverage it and few data to directly predict model weights or sample labels. For example, in \cite{zhmoginov2022hypertransformer}, Zhmoginov et al. propose a transformer-based hypernetwork for achieving fast adaptation of a convolutional neural network (CNN) in a generation manner of model weights. \emph{3) The gradient-based methods} \cite{rusu2018meta, finn2017model, lee2019meta, baik2020meta, rajeswaran2019meta, nichol2018first, baik2021meta, deleu2022continuous, von2021learning, RaghuRBV20, baik2020learning} regards the gradient-based optimization algorithm as meta knowledge, which learns to model its hyperparameters (i.e, learning rate \cite{baik2020meta}, loss function \cite{baik2021meta}, initialization \cite{finn2017model}, preconditioner \cite{kang2023meta}, or updata rules \cite{deleu2022continuous, RaviL17}) such that the base learner can be quickly learned with few gradient updates. For example, %in \cite{RaviL17}, Ravi propose to model the update rule of gradient-based optimization algorithm as a long short-term memory (LSTM) and then learn it in a gradient descient manner. 
in \cite{rajeswaran2019meta}, an approximate gradient estimation method is proposed to alleviate the isues of memory burdens and vanishing gradients for gradient-based meta-learning methods. %Although these approximate gradient estimation method are all effective, this would introduce an estimation error into the outer-loop gradient, which hampers its generalization ability. 


 %\emph{4) The representation-based methods} mainly focus on how to design a robust feature extractor or a superior loss function to learn  transferable representations for novel classes from data-abuntant base, such that the novel classes can be predicted by using a simple nearest-prototype or linear classfier. For instance, in [], . %\emph{5) The graph-based methods} regard the  black-box model as meta knowledge, which aims to leverage it and few data to directly predict model weights. For example, in [], . \emph{6) The external knowledge-based methods} regard the  black-box model as meta knowledge, which aims to leverage it and few data to directly predict model weights. For example, in [], .

In this paper, we focus on the gradient-based meta-learning due its good generalization. However, different from these existing meta-learning methods, we presents a new diffusion perspective to model meta-optimizer. Its advantage is that our MetaDiff do not need to differentiate through inner-loop path such that the issues of memory burdens and vanishing gradients can be effectvely alleviated. 

%\subsection{Few-Shot Learning}
%Few-shot learning (FSL) is a challenging task, which aims to recognize novel classes with few labeled samples. In addition to the above meta-learning methods, there is lots of non-meta learning methods for addressing FSL. These non-meta-learning methods can be roughly divided into two groups, i.e., representation learning methods and data augment methods. Specifically, the representation learning methods mainly focus on how to design a robust feature extractor or a superior loss function to learn  transferable representations for novel classes from data-abuntant base, such that the novel classes can be predicted by using a simple nearest-prototype or linear classfier. For instance, in [], . The data augment methods aims to directly alleviate the data scaricity issue in a data or feature generation manner. For example, . In this paper, different from the above FSL methods, we mainly focus on gradient-based meta-learning to address FSL.


\subsection{Diffusion Models}
Diffusion model \cite{ho2020denoising, nichol2021improved} is a popular type of deep generative models, which models and learns the generation process of target data from random Gaussion noises in a forward diffusion and backward denoise manner. %The forward diffusion step aims to use a Markov chain of diffusion steps to slowly add random noise to data, which is used for training diffusion models, and then the backward denoise step account for reversing the diffusion process to generate target data in a denoise manner from the Gaussion noise. 
With the superior properties of diffusion models, %e.g., its stationary training objective and easy scalability, 
the diffusion models have been widely exploited on various vision \cite{lugmayr2022repaint} and multi-modal tasks \cite{rombach2022high, kawar2023imagic, kumari2023multi} and achieved remarkable performance improvement. %For example, in \cite{rombach2022high}, Rombach et al. propose a conditional and latent diffusion models, which attempts to model the diffusion process in a latent space instead of oringin data space. 
However, at present, there are a few works exploring it on FSL. In \cite{roy2022diffalign}, Roy et al. introduce class names as prior and then leverage the prior and text2image diffusion models to generate more images for alleviating the data-scaricity issue. Instead of using text2image diffusion models, Hu et al. \cite{hu2023meta} employ a image2image diffusion models and then leverage it to generate more high-similarity pseudo-data for improving FSL.

In this paper, we also focus on exploring difussion models for FSL. However, different from these existing methods that directly regarding it as a component of data augment, we find that the optimization process of gradient descent is actually a denoise process, thus we propose to model the gradient descent algorithm as a diffusion model. We note that a concurrent working with our MetaDiff is ProtoDiff \cite{du2023protodiff}. However, different from ProtoDiff \cite{du2023protodiff} that focuses on metric-based meta-learning (i.e., rectifying prototype bias), we target at gradient-based meta-learning, and first reveal close connections between gradient descent algorithm and diffusion models and then present a new diffusion-based meta-optimizer for fast adaptation of base-learner.
  %Thanks to the training efficiency of diffusion models, our MetaDiff do not need to differentiate through inner-loop path such that the memory burdens and vanishing gradients of gradient-based meta-learning can be effectvely alleviated. 

\section{Problem Definition and Preliminaries}
\subsection{Problem Definition}
For a $N$-way $K$-shot FSL problem, it consists of two datasets, i.e., a base class dataset $\mathcal{D}_{base}$ and a novel class dataset $\mathcal{D}_{novel}$. The base class dataset $\mathcal{D}_{base}$ consists of abundant labeled data from base class $\mathcal{C}_{base}$, which is used for assisting the classifier learning for novel classes. The novel class dataset $\mathcal{D}_{novel}$ contains two sub datasets from novel classes $\mathcal{C}_{novel}$, i.e., a training set (call support set $\mathcal{S}$) that consists of $N$ classes and $K$ samples per class and a test set (call query set $\mathcal{Q}$) consisting of unlabeled samples.

Our goal is that leveraging the base class dataset $\mathcal{D}_{base}$ to learn a good meta-optimizer such that the classifier can be quickly learned from few labeled data (i.e., the support set $\mathcal{S}$) to perform the novel class prediction for query set $\mathcal{Q}$.

%-------------------------------------------------------------------------
\subsection{Preliminaries}
\label{sec_3_2}
\noindent{\bf Diffusion Models.} Diffusion models aim to model a probability transformation from a prior Gaussian distribution $p_{prior} \in \mathcal{N}(\mathbf{0}, \mathbf{I})$ to a target distribution $p_{target}$ as a markov chain model with learned Gaussian transitions, which consists of two processes: a diffusion (also called forward) process and a denoising (also called reverse) process. 

\emph{1) The diffusion process} aims to iteratively add a noise from a Gaussian distribution to a target data $x_{0} \sim p_{target}$ to transform $x_{0}$ into $x_{1},x_{2}, ..., x_{T}$. The final $x_{T}$ tends to become a sample point from the prior distribution $p_{prior} \in \mathcal{N}(\mathbf{0}, \mathbf{I})$ when the number of iterations $T$ tends to big enough. The diffusion process aims to learn a noise prediction model $\epsilon_{\theta}(x_{t}, t)$ for estimating the added noise at time $t-1$ from $x_{t}$, which is then used to recovery the target data in denoising process. The training object $L$ is as follows:
\begin{equation}
	\begin{aligned}
		L=\mathbb{E}_{x_{0} \sim p_{target}, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), t} [\|\epsilon-\epsilon_{\theta}(x_{t}, t)\|_2^2],
	\end{aligned}
	\label{eq_1}
\end{equation}where $\|\cdot\|_2^2$ denotes a mean squared error loss. It is worth noting that the above training object defined in Eq.~\eqref{eq_1} can be performed in any time step $t$ with the iterations of adding noise due to its good closed form at any time step $t$. That is,
\begin{equation}
	\begin{aligned}
		q(x_{t}|x{0}) &= \mathcal{N}(x_{t};\sqrt{\overline{\alpha}_{t}}x_{0}, (1-\overline{\alpha}_{t})\mathbf{I}), \\ &\alpha_{t}=1-\beta_t, \overline{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{t},
	\end{aligned}
	\label{eq_2}
\end{equation}where $\beta_t \in (0,1)$ is a variance hyperparameter, which is experimentally set in linear increasing manner from a small value (e.g., $10^{-4}$) to a large value (e.g., 0.02).

\emph{2) The denoising process} is reverse process of diffusion process. Based on the learned noise prediction model $\epsilon_{\theta}(x_{t}, t)$, given a start noise $x_T \sim p_{prior}$, we can iteratively remove the fraction of noise at each time $t$ and finally recovering the target data $x_{0}$ from the noisy data $x_T$. The overall denoising process can be expressed as:
\begin{equation}
	\begin{aligned}
		&p_{\theta}(x_{t-1}|x{t}) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_{t}, t), \sigma_{t}^2\mathbf{I}), \\ \mu_{\theta}&(x_{t}, t)=\frac{1}{\sqrt{\alpha_{t}}}(x_{t}-\frac{\beta_t}{\sqrt{(1-\overline{\alpha}_{t})}}\epsilon_{\theta}(x_{t}, t)).
	\end{aligned}
	\label{eq_3}
\end{equation}Based on the reparameter technique, the recovering data $x_{t-1}$ at time step $t$ can be represented as:
\begin{equation}
	\begin{aligned}
		x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}(x_{t}-&\frac{\beta_t}{\sqrt{(1-\overline{\alpha}_{t})}}\epsilon_{\theta}(x_{t}, t)) + \sigma_{t}z,\\ z \sim &\mathcal{N}(\mathbf{0}, \mathbf{I}).
	\end{aligned}
	\label{eq_4}
\end{equation}where $\sigma_{t}$ is a variance hyperparameter, which is experimentally set to $\sigma^2_{t}=\beta_t$ in most diffusion works \cite{ho2020denoising, nichol2021improved}.
 
\noindent{\bf Gradient Descent Algorithm (GDA).} GDA is a family of optimization algorithm, which aims to optimize model weights by following the opposite direction of gradient. Formally, let $w$ denotes the weights of a base learner $g_{w}(\cdot)$ and $L(w)$ be its differentiable loss function, and $\nabla L(w)$ be its weight gradient, during performing gradient descent algorithm. The overall optimization process of GDA can be summaried as iteratively performing Eq.~\eqref{eq_5}, that is,
\begin{equation}
	\begin{aligned}
		w_{t+1} = w_{t} - \eta (\nabla L(w_t)), t=0,1,,,T-1.
	\end{aligned}
	\label{eq_5}
\end{equation}where $w_0$ is an initial weight, i.e., a Gaussian noise in origin gradient descent algorithm; and $\eta$ denotes a learning rate. %More generally, the gradient descent algorithm with gradient momentum update can be further expressed as:
%\begin{equation}
%	\begin{aligned}
%		v_t &= \gamma v_{t-1} +  \eta (\nabla L(w_t)) \\
%		&w_{t+1} = w_{t} - v_t, t=0,1,,,T-1.
%	\end{aligned}
%	\label{eq_5_}
%\end{equation}where $\gamma$ is a hyperparameter, which controls the scale of momentum update. We can spread the term $v_t$ as follows:
%\begin{equation}
%	\begin{aligned}
%	v_t= \eta (\nabla L(w_t)) + \eta \gamma (\nabla L(w_{t-1})) + \eta \gamma^2 (\nabla L(w_{t-2})) ...
%	\end{aligned}
%\label{eq_5_}
%\end{equation} 
Due to the data scarcity issue in FSL, directly employing the Eq.~\eqref{eq_5} to learn a base learner $g_{w}(\cdot)$ would result in an overfitting issue. To address this issue, previous gradient-based meta-learning methods propose to learn a GDA (i.e., meta-optimizer) by modeling its hyperparameters (e.g., initial weight $w_0$, learning rate $\eta$, or  $l2$ regularization weight) as meta-knowledge to improve FSL performance. 

\section{Methodology}
\subsection{Connection: Diffusion Models vs GDA}
\label{sec_4_1}
As introduced in Section~\ref{sec_3_2}, we can respectively describe the process of denoising in diffusion model and gradient descent in GDA as follows: 1) given a noise data $x_{T}$, the denoising process iteratively performs Eq.~\eqref{eq_4} to obtain a latent sequence $x_{T-1}, x_{T-2},...,x_{0}$. As a result, a target data $x_{0}$ can be recoveried from a Gaussian noise $x_{T}$; and 2) given a randomly initial wight $w_{0}$, the gradient descent process of GDA iteratively performs Eq.~\eqref{eq_5} to a latent sequence $w_{1}, w_{2},...,w_{T}$. As a result, an optimizal weight $w_{T}$ can be obtained from a Gaussian noise weight. We can see that the denoising process of diffusion models and the gradient descent process of GDA are very similar in workflow. This inspires us to think about what is the connection between the two methods in theory. Let's take a closer look on Eqs.~\eqref{eq_4} and ~\eqref{eq_5}, Eq.~\eqref{eq_4} can be further simplifed as:
\begin{equation}
	\begin{aligned}
		x_{t-1}=\underbrace{\frac{1}{\sqrt{\alpha_{t}}}}_{Term 1}x_{t}-&\underbrace{\frac{\beta_t}{\sqrt{\alpha_{t}}\sqrt{(1-\overline{\alpha}_{t})}}}_{Term 2}\epsilon_{\theta}(x_{t}, t) + \underbrace{\sigma_{t}}_{Term 3}z,\\ z \sim &\mathcal{N}(\mathbf{0}, \mathbf{I}).
	\end{aligned}
	\label{eq_6}
\end{equation}Let $\gamma$ denotes the \emph{Term 1}, $\eta$ be the \emph{Term 2}, $\xi$ be the \emph{Term 3} of Eq.~\eqref{eq_6}, respectively. The Eq.~\eqref{eq_6} can be simplifed as:
\begin{equation}
	\begin{aligned}
		x_{t-1}=\gamma x_{t}-\eta \epsilon_{\theta}(x_{t}, t) + \xi z,\ z \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
	\end{aligned}
	\label{eq_7}
\end{equation}Due to $\gamma>1$ ($\alpha_{t}<1$), we can transform Eq.~\eqref{eq_7} as follows:
\begin{equation}
	\begin{aligned}
		x_{t-1}=\underbrace{x_{t}-\eta \epsilon_{\theta}(x_{t}, t)}_{Term 1}& + \underbrace{(\gamma - 1) x_{t}}_{Term 2}+\underbrace{\xi z}_{Term 3},\\ z \sim \mathcal{N}&(\mathbf{0}, \mathbf{I}).
	\end{aligned}
	\label{eq_7_}
\end{equation}where $Term 1$ is denoising term, $Term 2$ denotes a momentum update term with hyperparameter $\gamma-1$ (also called exponentially weighted moving average), and $Term 3$ is a uncertain term. Comparing Eqs.~\eqref{eq_5} and \eqref{eq_7_}, we can see that the gradient decent process defined in Eq.~\eqref{eq_5} is equivalent to the $Term 1$ of Eq.~\eqref{eq_7_}, which means that Eq.~\eqref{eq_5} is actually a special case of denoising process described in Eq.~\eqref{eq_7_} when the $\gamma$ is set to one (i.e., $\gamma=1$),  the $\eta$ is regarded as a hyperparameter, and the $\xi$ is set to zero. In particular, it is worth noting that, the predicted variable of noise prediction model $\epsilon_{\theta}(x_{t}, t)$ is actually the gradient (i.e., $\nabla L(w)$) of model weights. In other words, in fact, the denosing process defined in Eq.\eqref{eq_7_} can be viewed as a generalized gradient descent algorithm defined in Eq.\eqref{eq_5}, i.e., a gradent descent algorithm with weight momentum updates and uncertainty estimation where $\gamma$ controls the weight of momentum updates, $\eta$ is a learning rate, and $\xi$ is the degree of uncertainty.

\emph{Why set parameters ($\gamma$, $\eta$, and $\xi$) by following Eq.~\ref{eq_6}?} Instead of using manual setting or model learning manner like existing meta-optimizers to set hyperparameters $\gamma$, $\eta$, and $\xi$, respectively, the diffusion models unify the parameter settings by theoretical deduction, i.e., $\gamma=\frac{1}{\sqrt{\alpha_{t}}}$, $\eta=\frac{\beta_t}{\sqrt{\alpha_{t}}\sqrt{(1-\overline{\alpha}_{t})}}$, and $\xi=\sigma_{t}$ where $\alpha_{t}=1-\beta_t$, $\sigma^2_{t}=\beta_t$, and $\beta_t$ is experimentally set in linear decreasing manner from a small value (e.g., $10^{-4}$) to a large value (e.g., 0.02). The goal of such setting is to ensure that denoising and diffusion processes have approximately
the same functional form and the efficiency and robustness of diffusion training (i.e., the training object defined in Eq.~\eqref{eq_1} can be performed at any time step $t$ without the iterations from $t=0$ to $t$).



\subsection{Meta-Learning with Conditional Diffusion}
Inspired by the connection analysis between diffusion models and GDA, we find that the diffusion model is a generalized form of GDA with weight momentum updates and uncertainty estimation. Based on this and its training efficiency, we propose to leverage a diffusion model to model GDA and then present a new meta-optimizer, i.e., MetaDiff, for fast adaptation of base-learner. Next, we introduce our MetaDiff-based FSL framework and its key components.

% Figure environment removed

\noindent{\bf Overall Framework.} The overall framework of our MetaDif on FSL is presented in Figure~\ref{fig2}(a), which consists of an embedding network $f_{\varphi}(\cdot)$ with parameters $\varphi$, a base learner $g_{w}(\cdot)$ with parameters $w$, and a MetaDiff meta-optimizer $\epsilon_{\theta}(\cdot)$ with meta parameters $\theta$. Here, the embedding network $f_{\phi}(\cdot)$ aims to encode each support/query image as a $d$-dim feature vector. Inspired by prior meta-learning works \cite{deleu2022continuous, lee2019meta}, we assume that the embedding network $f_{\varphi}(\cdot)$ is shared across tasks, which can be obtained by using a simple pretraining manner on entire base class classification task \cite{chen2021meta, ChenLKWH19}. The base learner $g_{w}(\cdot)$ is a simple linear or cosine classifer (the cosine classifer is used in this paper due it good performance), which is a task-specific and needs to be adapted starting at some Gaussian initialization $w_{T}$. The MetaDiff meta-optimizer $\epsilon_{\theta}(\cdot)$ is a optimization algorithm, which takes the features and labels of all support samples $(u_i, y_i) \in \mathcal{S}$ as inputs and then aims to learn a target weights $w_{0}$ for the base learner $g_{w}(\cdot)$ from the initial weights $w_{T}$ in a denoising manner (see Figure~\ref{fig2}(b)).

Specifically, given a $N$-way $K$-shot FSL task, we first leverage the embedding network $f_{\varphi}(\cdot)$ to encode the  feature representation $f_{\varphi}(u_i)$ for each support/query image $u_i \in \mathcal{S} \cup \mathcal{Q}$. Then, we randomly initalize a weight $w_{T} \sim \mathbb{N}(\mathbf{0}, \mathbf{I})$ for the base learner $g_{w}(\cdot)$, and design a task-conditional UNet (i.e., the noise prediction model  $\epsilon_{\theta}(\cdot)$) that regards the features and labels of all support sample $(u_i, y_i) \in \mathcal{S}$ as task condition, to estimate the noise to be removed at time $t$. After that, we take the weight $w_{T}$ as the denoising variable and iteratively perform the denoising process, that is,
\begin{equation}
	\begin{aligned}
		w_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}(w_{t}-&\frac{\beta_t}{\sqrt{(1-\overline{\alpha}_{t})}}\epsilon_{\theta}(w_{t}, \mathcal{S}, t)).
	\end{aligned}
	\label{eq_8}
\end{equation}Note that we remove the uncertainty term (i.e., $\sigma_{t}z$) for deterministic estimation during inference. After iteratively perform $T$ step, the target weight $w_0$ can be obtained as the optimal weight $w$ for base learner $g_{w}(\cdot)$. Finally, we perform class prediction of each query image $u_i \in \mathcal{Q}$ by using the learned base learner $g_{w}(\cdot)$. That is,
\begin{equation}
	\begin{aligned}
		\hat{y}=g_{w}(f_{\varphi}(u_i)),\ w=w_{0},\ u_i \in \mathcal{Q}.
	\end{aligned}
	\label{eq_9}
\end{equation}Note that we only introduce the inference workflow of our MetaDiff framework, which is summaried in Algorithm~\ref{alg:sampling}. Next, we will introduce the design details of our key component, i.e., the task-conditional UNet $\epsilon_{\theta}(\cdot)$.

% Figure environment removed

\begin{table*}
	\caption{Experiment results on ImageNet derivatives. The best results are highlighted in bold. ``CH'' denotes classification head. }
	\vspace{-15pt}
	\begin{center}
		\smallskip\scalebox
		{0.9}{
			\begin{tabular}{l|c|c|c|c|c|c}
				\toprule
				\multicolumn{1}{l|}{\multirow{2}{*}{Method}}&\multicolumn{1}{c|}{\multirow{2}{*}{Adaptation Type}}&\multicolumn{1}{c|}{\multirow{2}{*}{Backbone}}& \multicolumn{2}{c|}{miniImagenet} & \multicolumn{2}{c}{tieredImagenet} \\ 
				\cline{4-7}
				& & & 5-way 1-shot & 5-way 5-shot & 5-way 1-shot & 5-way 5-shot \\
				\midrule
				%CTM \cite{li2019finding} & ResNet18 &  $62.05 \pm 0.55\%$  & $78.63 \pm 0.06\%$ & $64.78 \pm 0.11\%$  & $81.05 \pm 0.52\%$ \\
				%MetaLSTM \cite{RaviL17} & Conv4 & $48.70 \pm 1.84\%$  & $63.11 \pm 0.92\%$ & $51.67 \pm 1.81\%$  & $70.30 \pm 1.75\%$ \\
				MAML \cite{finn2017model} & All & Conv4 & $48.70 \pm 1.84\%$  & $63.11 \pm 0.92\%$ & $51.67 \pm 1.81\%$  & $70.30 \pm 1.75\%$ \\
				iMAML\cite{rajeswaran2019meta} & All & Conv4 & $49.30 \pm 1.88\%$  & $59.77 \pm 0.73\%$ & $38.54 \pm 1.37\%$  & $60.24 \pm 0.76\%$ \\
				%TAML\cite{JamalQ19} & All & Conv4 & $51.77 \pm 1.86\%$  & $65.60 \pm 0.93\%$ & $- \pm -\%$  & $- \pm -\%$ \\
				%CAML\cite{lee2019meta} & Conv4 & $59.23 \pm 0.99\%$  & $72.35 \pm 0.18\%$ & $- \pm -\%$  & $- \pm -\%$ \\
				Warp-MAML \cite{flennerhag2019meta} & All & Conv4 & $52.30 \pm 0.80\%$  & $68.40 \pm 0.60\%$ & $57.20 \pm 0.90\%$  & $71.40 \pm 0.70\%$ \\
				Sparse-MAML \cite{von2021learning} & All & Conv4 & $51.04 \pm 0.59\%$  & $67.03 \pm 0.74\%$ & $53.91 \pm 0.67\%$  & $69.92 \pm 0.21\%$ \\
				%ModGrad \cite{} & MLEN & Conv4 & $53.20 \pm 0.86\%$  & $69.17 \pm 0.69\%$ & $- \pm -\%$  & $- \pm -\%$ \\
				ALFA \cite{baik2020meta} & All & Conv4 & $50.58 \pm 0.51\%$  & $69.12 \pm 0.47\%$ & $53.16 \pm 0.49\%$  & $70.54 \pm 0.46\%$ \\
				MeTAL \cite{baik2021meta} & All & Conv4 & $52.63 \pm 0.37\%$  & $70.52 \pm 0.29\%$ & $54.34 \pm 0.31\%$  & $70.40 \pm 0.21\%$ \\
				%MetaSGD + SiMT \cite{} & All & Conv4 & $51.70 \pm 0.80\%$  & $69.13 \pm 1.40\%$ & $52.98 \pm 0.07\%$  & $71.46 \pm 0.12\%$ \\
				GAP \cite{kang2023meta} & All & Conv4 & $54.86 \pm 0.85\%$  & $71.55 \pm 0.61\%$ & $57.60 \pm 0.93\%$  & $74.90 \pm 0.68\%$ \\
				MetaOptNet\cite{lee2019meta} & only CH & Conv4 &  $52.87 \pm 0.57\%$  & $68.76 \pm 0.48\%$ & $54.71 \pm 0.67\%$  & $71.79 \pm 0.59\%$ \\
				ANIL \cite{RaghuRBV20} & only CH & Conv4 & $46.30 \pm 0.40\%$  & $61.00 \pm 0.60\%$ & $49.35 \pm 0.26\%$  & $65.82 \pm 0.12\%$ \\
				COMLN \cite{deleu2022continuous} & only CH & Conv4 & $53.01 \pm 0.62\%$  & $70.54 \pm 0.54\%$ & $54.30 \pm 0.69\%$  & $71.35 \pm 0.57\%$ \\ 
				%ClassifierBaseline \cite{chen2021meta} & only CH & Conv4 & $49.37 \pm 0.80\%$  & $66.43 \pm 0.63\%$ & $- \pm -\%$  & $- \pm -\%$ \\
				\midrule
				MetaDiff (ours) & only CH & Conv4 & \textbf{55.06} $\pm$ \textbf{0.81}$\%$ & \textbf{73.18} $\pm$ \textbf{0.64}$\%$ & \textbf{57.77} $\pm$ \textbf{0.90}$\%$ & \textbf{75.46} $\pm$ \textbf{0.69}$\%$  \\	
				\midrule
				MAML \cite{finn2017model} & All & ResNet12 & $58.37 \pm 0.49\%$  & $69.76 \pm 0.46\%$ & $58.58 \pm 0.49\%$  & $71.24 \pm 0.43\%$ \\
				%MTL\cite{SunLCS19} & ResNet12 & $61.20 \pm 1.80\%$  & $75.50 \pm 0.80\%$ & $65.62 \pm 1.80\%$  & $80.61 \pm 0.90\%$ \\
				%MetaOptNet-RR\cite{LeeMRS19} & ResNet12 &  $61.41 \pm 0.61\%$  & $77.88 \pm 0.46\%$ & $65.36 \pm 0.71\%$  & $81.34 \pm 0.52\%$ \\
				
				ALFA \cite{baik2020meta} & All & ResNet12 & $59.74 \pm 0.49\%$  & $77.96 \pm 0.41\%$ & $64.62 \pm 0.49\%$  & $82.48 \pm 0.38\%$ \\
				MeTAL \cite{baik2021meta} & All & ResNet12 & $59.64 \pm 0.38\%$  & $76.20 \pm 0.19\%$ & $63.89 \pm 0.43\%$  & $80.14 \pm 0.40\%$ \\
				%MetaNODE & ResNet12 & 66.07 $\pm$ 0.79$\%$ & 81.93 $\pm$ 0.55$\%$ & 72.72 $\pm$ 0.90$\%$ & 86.45 $\pm$ 0.62$\%$ \\
				MetaOptNet\cite{lee2019meta} & only CH & ResNet12 &  $62.64 \pm 0.61\%$  & $78.63 \pm 0.46\%$ & $65.99 \pm 0.72\%$  & $81.56 \pm 0.53\%$ \\
				LEO \cite{rusu2018meta} & only CH & WRN-28-10 & $61.76 \pm 0.08\%$  & $77.59 \pm 0.12\%$ & $66.33 \pm 0.05\%$  & $81.44 \pm 0.09\%$ \\
				Meta-Curvature\cite{park2019meta} & only CH & WRN-28-10 &  $61.85 \pm 0.10\%$  & $77.02 \pm 0.11\%$ & $67.21 \pm 0.10\%$  & $82.61 \pm 0.08\%$ \\
				ANIL\cite{RaghuRBV20} & only CH & ResNet12 &  $49.65 \pm 0.65\%$  & $59.51 \pm 0.56\%$ & $54.77 \pm 0.76\%$  & $69.28 \pm 0.67\%$ \\
				COMLN \cite{deleu2022continuous} & only CH & ResNet12 & 59.26 $\pm$ 0.65$\%$ & 77.26 $\pm$ 0.49$\%$ & 62.93 $\pm$ 0.71$\%$ & 81.13 $\pm$ 0.53$\%$ \\
				ClassifierBaseline \cite{chen2021meta} & only CH & ResNet12 & 61.22 $\pm$ 0.84$\%$ & 78.72 $\pm$ 0.60$\%$ & 69.71 $\pm$ 0.88$\%$ & 83.87 $\pm$ 0.64$\%$ \\
				\midrule
				MetaDiff (ours) & only CH & ResNet12 & \textbf{64.99} $\pm$ \textbf{0.77}$\%$ & \textbf{81.21} $\pm$ \textbf{0.56}$\%$ & \textbf{72.33} $\pm$ \textbf{0.92}$\%$ & \textbf{86.31} $\pm$ \textbf{0.62}$\%$ \\			
				\bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-15pt}
	\label{table1}
\end{table*} 


\noindent{\bf Task-Conditional UNet (TCUNet).} The task-conditional UNet $\epsilon_{\theta}(\cdot)$ (called TCUNet) is the key component in our MetaDiff meta-optimizer, which takes the features and labels of all support samples $(u_i, y_i) \in \mathcal{S}$, time step $t$, and the weight $w_t$ of base learner as inputs. It aims to estimate the noise to be remove for the weight $w_t$ of base learner at each time step $t$. We attempt to use a general conditional UNet like \cite{rombach2022high} for implementing TCUNet $\epsilon_{\theta}(\cdot)$. However, we find that such general conditional UNet does not work in our MetaDiff, which inspires us to think deeply the rationale of the noise prediction model $\epsilon_{\theta}(\cdot)$ in our MetaDiff meta-optimizer. As analyzed in Section~\ref{sec_4_1}, we can see that the goal of noise prediction model $\epsilon_{\theta}(\cdot)$ is actually to predict the gradient in meta-optimizer (see Eqs.~\eqref{eq_5} and \eqref{eq_7_}). 

Based on this find, as shown in Figure \ref{fig3}, we design a task-conditional UNet from the perspective of gradient estimation as the noise prediction model $\epsilon_{\theta}(\cdot)$ of our MetaDiff. The key idea is predicting noise in a gradient estimation manner instead of a general black-box manner like \cite{rombach2022high}.  Specifically, given a base learner weight $w_t$ at time $t$ and the features and labels of all support samples $(u_i, y_i) \in \mathcal{S}$, we first leverage the base learner $g_{w_t}(\cdot)$ with model weights $w_t$ to compute the class scores of all support samples $(u_i, y_i) \in \mathcal{S}$ and its cross-entropy loss $L_{w_t}(\mathcal{S})$. That is,
\begin{equation}
	\begin{aligned}
		L_{w_t}(\mathcal{S})=\frac{1}{|\mathcal{S}|} \sum_{(u_i, y_i) \in \mathcal{S}} CE(g_{w_t}(f_{\phi}(u_i)), y_i).
	\end{aligned}
	\label{eq_10}
\end{equation}where $|\cdot|$ denots the number of sample in the support set. Then, the gradient $\nabla L_{w_t}(\mathcal{S})$ regarding weights $w_t$ can be obtained for the base learner $g_{w_t}(\cdot)$ at time $t$. However, such gradient estimation $\nabla L_{w_t}(\mathcal{S})$ is inaccuracy, because the number of available labeled (support) samples per class (e.g., K=1 or 5) in $\mathcal{S}$ is far less than the expected amount. To obtain more accuracy gradient estimation, we take the estimation $\nabla L_{w_t}(\mathcal{S})$ as inputs and then design a simple UNet fusing time embedding $t$ to polish the gradient estimation. 

As shown in Figure \ref{fig3}, the UNet consists of two encoder blocks (EB), a bottle block (BB) and two decoder blocks (DB). At each encoder step, we halve the number of input features and then remain unchanged at bottle step, but the number of features is doubled at each decoder step. The details of each encoder, bottle, decoder block are all similar, which contains a feature transform layer, a time embedding layer, and a ReLU activation layer. Note that we remove the ReLU activation layer in the final decoder block for estimating gradients. At each block, its output is obtained by first feedding the output of previous block and time step $t$ into the feature transform and time embedding layers, respectively, and then fusing them in an element-by-element product manner, finally followed by a ReLU activation.

\algrenewcommand\algorithmicindent{0.5em}%
% Figure environment removed

\begin{table*}[h!]
	\caption{Experiment results on CIFAR derivatives. The best results are highlighted in bold. ``CH'' denotes classification head.}
	\vspace{-15pt}
	\begin{center}
		\smallskip\scalebox
		{0.9}{
			\begin{tabular}{l|c|c|c|c|c|c}
				\toprule
				\multicolumn{1}{l|}{\multirow{2}{*}{Method}}&\multicolumn{1}{c|}{\multirow{2}{*}{Adaptation Type}}&\multicolumn{1}{c|}{\multirow{2}{*}{Backbone}}& \multicolumn{2}{c|}{CIFAR-FS} & \multicolumn{2}{c}{FC100} \\ 
				\cline{4-7}
				& & & 5-way 1-shot & 5-way 5-shot & 5-way 1-shot & 5-way 5-shot \\
				\midrule
				MAML \cite{finn2017model}& All & Conv4 & $56.80 \pm 0.49\%$  & $74.94 \pm 0.43\%$ & $36.67 \pm 0.48\%$  & $49.38 \pm 0.49\%$ \\
				%iMAML\cite{RajeswaranFKL19} & Conv4 & $49.30 \pm 1.88\%$  & $59.77 \pm 0.73\%$ & $38.54 \pm 1.37\%$  & $60.24 \pm 0.76\%$ \\
				%TAML\cite{JamalQ19} & Conv4 & $51.77 \pm 1.86\%$  & $65.60 \pm 0.93\%$ & $- \pm -\%$  & $- \pm -\%$ \\
				%CAML\cite{lee2019meta} & Conv4 & $59.23 \pm 0.99\%$  & $72.35 \pm 0.18\%$ & $- \pm -\%$  & $- \pm -\%$ \\
				%Warp-MAML \cite{FlennerhagRPVYH20} & Conv4 & $52.30 \pm 0.80\%$  & $68.40 \pm 0.60\%$ & $57.20 \pm 0.90\%$  & $71.40 \pm 0.70\%$ \\
				%Sparse-MAML \cite{} & Conv4 & $51.04 \pm 0.59\%$  & $67.03 \pm 0.74\%$ & $53.91 \pm 0.67\%$  & $69.92 \pm 0.21\%$ \\
				L2F \cite{baik2020learning}& All & Conv4 & $60.35 \pm 0.48\%$  & $76.76 \pm 0.42\%$ & $38.96 \pm 0.49\%$  & $53.23 \pm 0.48\%$ \\
				%SIB \cite{}& All & Conv4 & $68.70 \pm 0.60\%$  & $77.10 \pm 0.40\%$ & $- \pm -\%$  & $- \pm -\%$ \\
				ALFA \cite{baik2020meta}& All & Conv4 & $60.56 \pm 0.49\%$  & $75.43 \pm 0.43\%$ & $38.20 \pm 0.49\%$  & $52.98 \pm 0.50\%$ \\
				
				MeTAL \cite{baik2021meta}& All & Conv4 & $59.19 \pm 0.56\%$  & $74.62 \pm 0.42\%$ & $37.46 \pm 0.39\%$  & $51.34 \pm 0.25\%$ \\
				%MetaSGD + SiMT \cite{} & Conv4 & $51.70 \pm 0.80\%$  & $69.13 \pm 1.40\%$ & $52.98 \pm 0.07\%$  & $71.46 \pm 0.12\%$ \\
				%GAP \cite{} & Conv4 & $54.86 \pm 0.85\%$  & $71.55 \pm 0.61\%$ & $57.60 \pm 0.93\%$  & $74.90 \pm 0.68\%$ \\
				%ClassifierBaseline \cite{Yinbo20}& Only CH & Conv4 & - $\pm$ -$\%$ & - $\pm$ -$\%$ & - $\pm$ -$\%$ & - $\pm$ -$\%$ \\
				\midrule
				MetaDiff (ours)& Only CH & Conv4 & \textbf{65.03} $\pm$ \textbf{0.90}$\%$ & \textbf{81.70} $\pm$ \textbf{0.67}$\%$ & \textbf{41.02} $\pm$ \textbf{0.73}$\%$ & \textbf{55.77} $\pm$ \textbf{0.76}$\%$ \\	
				\midrule
				MAML \cite{finn2017model}& All & ResNet12 & $64.33 \pm 0.48\%$  & $76.38 \pm 0.42\%$ & $37.92 \pm 0.48\%$  & $52.63 \pm 0.50\%$ \\
				%LEO \cite{RusuRSVPOH19} & WRN-28-10 & $61.76 \pm 0.08\%$  & $77.59 \pm 0.12\%$ & $66.33 \pm 0.05\%$  & $81.44 \pm 0.09\%$ \\
				%MTL\cite{SunLCS19} & ResNet12 & $61.20 \pm 1.80\%$  & $75.50 \pm 0.80\%$ & $65.62 \pm 1.80\%$  & $80.61 \pm 0.90\%$ \\
				%MetaOptNet-RR\cite{LeeMRS19} & ResNet12 &  $61.41 \pm 0.61\%$  & $77.88 \pm 0.46\%$ & $65.36 \pm 0.71\%$  & $81.34 \pm 0.52\%$ \\
				%Meta-Curvature\cite{} & WRN-28-10 &  $61.85 \pm 0.10\%$  & $77.02 \pm 0.11\%$ & $67.21 \pm 0.10\%$  & $82.61 \pm 0.08\%$ \\
				%ANIL\cite{} & ResNet12 &  $49.65 \pm 0.65\%$  & $59.51 \pm 0.56\%$ & $54.77 \pm 0.76\%$  & $69.28 \pm 0.67\%$ \\
				L2F \cite{baik2020learning}& All & ResNet12 & $67.48 \pm 0.46\%$  & $82.79 \pm 0.38\%$ & $41.89 \pm 0.47\%$  & $54.68 \pm 0.50\%$ \\
				ALFA \cite{baik2020meta}& All & ResNet12 & $64.14 \pm 0.48\%$  & $78.11 \pm 0.41\%$ & $40.57 \pm 0.49\%$  & $53.19 \pm 0.50\%$ \\
				MeTAL \cite{baik2021meta}& All & ResNet12 & $67.97 \pm 0.47\%$  & $82.17 \pm 0.38\%$ & $39.98 \pm 0.39\%$  & $53.85 \pm 0.36\%$ \\
				%MetaNODE & ResNet12 & 66.07 $\pm$ 0.79$\%$ & 81.93 $\pm$ 0.55$\%$ & 72.72 $\pm$ 0.90$\%$ & 86.45 $\pm$ 0.62$\%$ \\
				%COMLN & ResNet12 & 59.26 $\pm$ 0.65$\%$ & 77.26 $\pm$ 0.49$\%$ & 62.93 $\pm$ 0.71$\%$ & 81.13 $\pm$ 0.53$\%$ \\
				MetaOptNet\cite{lee2019meta}& Only CH & ResNet12 &  $72.00 \pm 0.70\%$  & $84.20 \pm 0.50\%$ & $41.10 \pm 0.60\%$  & $55.50 \pm 0.60\%$ \\
				%ClassifierBaseline \cite{Yinbo20}& Only CH & ResNet12 & - $\pm$ -$\%$ & - $\pm$ -$\%$ & - $\pm$ -$\%$ & - $\pm$ -$\%$ \\
				\midrule
				MetaDiff (ours)& Only CH & ResNet12 & \textbf{73.60} $\pm$ \textbf{0.85}$\%$ & \textbf{87.43} $\pm$ \textbf{0.60}$\%$ & \textbf{44.42} $\pm$ \textbf{0.77}$\%$ & \textbf{61.02} $\pm$ \textbf{0.75}$\%$ \\			
				\bottomrule
		\end{tabular}}
	\end{center}
	\vspace{-15pt}
	\label{table2}
\end{table*}

\noindent{\bf Meta-Learning Objective.}  Different from previous gradient based meta-learning methods that learns meta-optimizer in a bi-level optimization manner, as shown in Figure~\ref{fig2}(b), we employ a diffusion process to train our MetaDiff meta-optimizer. However, unlike existing diffusion models  \cite{ho2020denoising}  where the target data $x_0$ is known (i.e., origin images), the target variable of our MetaDiff is model weights (i.e., $w_{0}$) of base learner $g_{w}(\cdot)$ which is unknown. Thus, a key challenge of training our MetaDiff is how to obtain a large number of target weight $w_{0}$ for base learner $g_{w}(\cdot)$. 

To this end, we follow episodic training strategy \cite{vinyals2016matching} and contruct a large number of $N$-way $K$-shot tasks from base class dataset $\mathcal{D}_{base}$. Then, given a contructed $N$-way $K$-shot tasks $\tau$, based on its origin label $k'$ of each class $k=0,1,..,N-1$ in the base classes $\mathcal{C}_{base}$, we extract all samples that belongs to the origin label $k'$ of each class $k=0,1,..,N-1$ from the base class dataset $\mathcal{D}_{base}$, as the auxiliary dataset $\mathcal{D}_{base}^{\tau}$. The labeled data is very sufficient in the auxiliary dataset $\mathcal{D}_{base}^{\tau}$ because it contains all labeled data belonging to class $k'$ in $\mathcal{D}_{base}$, thus we can leverage it to train a base learner $g_{w}(\cdot)$ by using gradient descent algorithm suth that the target weight $w_0$ can be obtained for each task $\tau$. Finally, we leverage the target weight $w_0$ to train our MetaDiff meta-optimizer in a diffusion manner. That is,
\begin{equation}
	\begin{aligned}
		\mathop{min}_{\theta} \mathbb{E}_{(\mathcal{S}, w_0) \sim \mathbb{T}, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), t \sim [1,T]} \|\epsilon-\epsilon_{\theta}(w_{t}, \mathcal{S}, t)\|_2^2.
	\end{aligned}
	\label{eq_11}
\end{equation}During training, our MetaDiff does not require backpropagation along inner-loop optimization path and calculating second-order derivatives for learning meta-optimizer such that the memory overhead and the risk of vanishing gradient can be effectively alleviated.  The complete diffusion training procedure is summaried in Algorithm~\ref{alg:training}.

\section{Experiments}
\subsection{Datasets and Settings}
\noindent \textbf{MiniImagenet.} The dataset is a subset from ImageNet, which contains 100 classes and 600 images per class. Following \cite{lee2019meta}, we split it into three class sets, i.e., 64, 16, and 20 classes for training, validation, and test, respectively. 

\noindent \textbf{TieredImagenet.}
The dataset is also a subset from ImageNet but larger, which has 608 classes and 1200 images per class. Following \cite{lee2019meta}, it is splited into 20, 6, and 8 high-level classes for training, validation, and test, respectively. % in high-level semantic categories. 

\noindent \textbf{CIFAR-FS.} Similar to MiniImagenet, the dataset also contains 100 classes and 600 images per class, but is a subset from CIFAR.  Following \cite{lee2019meta}, we split it into 64, 16, and 20 classes for training, validation, and test, respectively. 

\noindent \textbf{FC100.} The dataset is also a subset from CIFAR. However, different from CIFAR-FS, following \cite{lee2019meta}, we split this dataset into 20, 6, and 8 classes at high-level semantic categories for training, validation, and test, respectively. % in high-level semantic categories. 

%\noindent \textbf{CUB-200-2011.} The dataset is a fine-grained dataset with 200 bird classes. It contains about 11,788 images. Following \cite{ChenLKWH19}, we split the data set into 100 classes, 50 classes, and 50 classes for training, validation, and test, respectively. %In this paper, the dataset is used for evaluating the performance of few-shot fine-grained classification tasks.

\subsection{Implementation Details}

\noindent \textbf{Network Details.}
We use Conv4 and ResNet12 as the embedding network $f_{\phi}(\cdot)$, which are same to previous gradient-based meta-learning methods \cite{kang2023meta, lee2019meta, deleu2022continuous} and delivers 128/512-dim vector to encode each image, respectively. In our task-conditional UNet, %we employ two encoder, a bottle, and two decoder blocks to estimate the noise to be remove at time $t$. 
for the encoder blocks, we use a linear layer with 512/256-dim inputs and 256/128-dim outputs to implement its feature transform layer, and a linear layer with 32-dim inputs and 256/128-dim outputs as its time embedding layer. For the bottle blocks, we use a linear layer with 128-dim inputs and outputs to implement its feature transform layer, and a linear layer with 32-dim inputs and 128-dim outputs as its time embedding layer. For the decoder blocks, we use a linear layer with 128/256-dim inputs and 256/512-dim outputs to implement its feature transform layer, and a linear layer with 32-dim inputs and 256/512-dim outputs as its time embedding layer.

\noindent \textbf{Training details.}
During diffusion training, we train our MetaDiff meta-optimizer 50 epochs (10000 iterations per epoch) using Adam with a learning rate of 0.0001 and a weight decay of 0.0005. Following the standard setting of diffusion models in \cite{ho2020denoising}, we set the number of denoising iterations to 1000 (i.e., $T=1000$ is used).


\noindent \textbf{Evaluation.} Following  \cite{kang2023meta, lee2019meta, deleu2022continuous}, we evaluate our MetaDiff and baseline methods on 600 randomly sampled 5-way 1/5-shot tasks from novel classes and report the mean accuracy together with the 95\% confidence interval. In each task, 15 images per class are randomly sampleed as query set.

\subsection{Experimental Results}
Our MetaDiff falls into the type of gradient-based meta-learning methods, thus we mainly select various state-of-the-art gradient-based meta learning methods as our comparsion baselines. Amoung them, MetaOptNet \cite{lee2019meta}, ANIL \cite{RaghuRBV20}, COMLN \cite{deleu2022continuous}, and ClassifierBaseline \cite{chen2021meta} are our key competitor, which also focus on the adaptation of classification head. We evaluate our MetaDiff and these baseline methods on two derivative datasets, i.e., ImageNet and CIFAR. The experimental results are shown in Tables~\ref{table1} and~\ref{table2}. 

\noindent {\bf Performance on ImageNet derivatives.} Table~\ref{table1} shows the results of various gradient-based meta-learning methods on miniImagenet and tieredImagenet. From these results, we find that (\romannumeral1) our MetaDiff achieves superior or comparable performance on all task settings, which exceeds most state-of-the-art gradient-based meta-learning methods by around 1\% $\sim$ 5\%. This verifies the effectiveness of our MetaDiff; (\romannumeral2) Our MetaDiff achieves consistent improvement on Conv4-based and ResNet12-based backbones for all tasks, which is reasonable because our MetaDiff mainly focus on the adaptation of classification head. This also verifies the universality of our MetaDiff on variour backbone types. %than 5-shot tasks. %because the labeled samples are more scarce.

\noindent {\bf Performance on CIFAR derivatives.} The experimental results on CIFAR-FS and FC100 are shown in Table~\ref{table2}. From these results, we can observe similar conclusions to Table~\ref{table1}, i.e., (\romannumeral1) significant performance improvement over state-of-the-art methods (around 1\% $\sim$ 6\%); (\romannumeral3) consistent performance improvement on Conv4 and ResNet12. This further verifies the effectiveness and robuseness of our MetaDiff.

\begin{table}[t]
	\centering
	\caption{Analysis of our MetaDiff on miniImagenet.
	}\smallskip
	\vspace{-15pt}
	\smallskip\scalebox
	{0.75}{
		\smallskip\begin{tabular}{c|l|c|c}
			\hline
			%\cline{2-2}
			& Method &  5-way 1-shot & 5-way 5-shot \\
			\hline \hline
			(\romannumeral1) & Baseline (Standard GDA) & 60.53 $\pm$ 0.86$\%$ & $72.43 \pm 0.66\%$  \\
			(\romannumeral2) & Replacing by Momentum GDA & 62.03 $\pm$ 0.82$\%$ & 78.28 $\pm$ 0.56$\%$ \\
			(\romannumeral3) & Replacing by ANIL \cite{RaghuRBV20} & 60.77 $\pm$ 0.82$\%$ & 77.34 $\pm$ 0.64$\%$ \\
			(\romannumeral4) & Replacing by MetaLSTM \cite{RaviL17} & 63.56 $\pm$ 0.81$\%$ & 79.90 $\pm$ 0.59$\%$\\
			(\romannumeral5) & Replacing by ALFA \cite{baik2020meta} & 63.92 $\pm$ 0.82$\%$ &80.01 $\pm$ 0.61$\%$ \\
			%(\romannumeral6) & Replacing by COMLN & 77.92 $\pm$ 0.99$\%$ & $85.13 \pm 0.62\%$ \\
			(\romannumeral6) & Replacing by Our MetaDiff & 64.99 $\pm$ 0.77$\%$ & $81.21 \pm 0.56\%$ \\
			\hline
	\end{tabular}}
	\label{table3}
\end{table}

\begin{table}[t]
	\centering
	\caption{Analysis of our task-conditional UNet on miniImagenet.}\smallskip
	\vspace{-15pt}
	\smallskip\scalebox
	{0.85}{
		\smallskip\begin{tabular}{c|c|c|c}
			\hline
			%\cline{2-2}
			& Method & 5-way 1-shot & 5-way 5-shot \\
			\hline \hline
			(\romannumeral1) & TCUNet & 64.99 $\pm$ 0.77$\%$ & 81.21 $\pm$ 0.56$\%$ \\
			%1-shot & 0.93$\%$ & 0.97$\%$ & $\%$ & \\
			%(\romannumeral2) & w/o $\nabla L_{w_t}(\mathcal{S})$  & 75.34 $\pm$ 1.10$\%$ & 84.00 $\pm$ 0.53$\%$ \\
			(\romannumeral2) & w/o UNet  & 62.72 $\pm$ 0.84$\%$ & 80.62 $\pm$ 0.60$\%$ \\
			%5-way 5-shot &0.57$\%$ & $\%$ & $\%$ & \\
			\hline
	\end{tabular}}
	
	\label{table4}
\end{table}

\begin{table}[t]
	\centering
	\caption{Classifier analysis of our MetaDiff on miniImagenet .}\smallskip
	\vspace{-15pt}
	\smallskip\scalebox
	{0.80}{
		\smallskip\begin{tabular}{c|l|c|c}
			\hline
			%\cline{2-2}
			& Method & 5-way 1-shot & 5-way 5-shot \\
			\hline \hline
			\multicolumn{1}{l|}{\multirow{2}{*}{(\romannumeral1)}} & Cosine Classfier (ALFA) & 63.92 $\pm$ 0.82$\%$ & 80.01 $\pm$ 0.61$\%$ \\
			 & Cosine Classfier (MetaDiff) & 64.99 $\pm$ 0.77$\%$ & 81.21 $\pm$ 0.56$\%$ \\
			%1-shot & 0.93$\%$ & 0.97$\%$ & $\%$ & \\
			\hline
			\multicolumn{1}{l|}{\multirow{2}{*}{(\romannumeral2)}} & Linear Classfier (ALFA) & 62.09 $\pm$ 0.84$\%$ & 78.13 $\pm$ 0.59$\%$ \\
			 & Linear Classfier (MetaDiff) & 62.72 $\pm$ 0.89$\%$ & 80.19 $\pm$ 0.57$\%$ \\
			%(\romannumeral3) & w/o residual gate  & 76.02 $\pm$ 1.17$\%$ & 84.16 $\pm$ 0.64$\%$ \\
			%5-way 5-shot &0.57$\%$ & $\%$ & $\%$ & \\
			\hline
	\end{tabular}}
	
	\label{table5}
\end{table}
 
\subsection{Ablation Study}
\label{section_5_4}
%In this section, we conduct ablation studies on miniImageNet, to analyze the effectiveness of MetaNODE.

\noindent {\bf Is our MetaDiff effective?} In Table~\ref{table3}, we analyze the effectiveness of our MetaDiff. Specifically, (\romannumeral1) we implement the adaptation of base learner (i.e., cosine classfier) by using a standard GDA (i.e., Eq.~\eqref{eq_5}) on the support set $\mathcal{S}$; (\romannumeral2) we replace the standard GDA (i.e., Eq.~\eqref{eq_5}) by a GDA with gradient momentum updates on (\romannumeral1); (\romannumeral3) replacing by the ANIL \cite{RaghuRBV20} on (\romannumeral1); (\romannumeral4) replacing by the MetaLSTM \cite{RaviL17} on (\romannumeral1); (\romannumeral5) replacing by the ALFA \cite{baik2020meta} on (\romannumeral1); and (\romannumeral6) replacing by our MetaDiff. From the results of (\romannumeral1) $\sim$ (\romannumeral7), we observe that: 1) the performance of (\romannumeral2) $\sim$ (\romannumeral6) exceeds (\romannumeral1) around 1\% $\sim$ 5\%, which means that it is helpful to learn a meta-optimizer to optimize task-specific base-learner; 2) the performance of (\romannumeral7) exceeds (\romannumeral2) $\sim$ (\romannumeral6) around 1\% $\sim$ 4\%, which shows the superiority of our MetaDiff. %This is because MetaNODE regards the gradient flow as meta-knowledge%, instead of hyperparameters like weight decay. %Finally, comparing the results of (\romannumeral4) with (\romannumeral5), we find that using samples can significantly enhance MetaNODE.

%\noindent {\bf Are our task-conditional UNet effective?} In Table~\ref{table4}, (\romannumeral1) we evaluate TCUNet on miniImagenet; (\romannumeral2) we estimate noise to be remove in a general conditional UNet like \cite{rombach2022high} instead of gradient-based conditional UNet; (\romannumeral3) we remove the UNet on  (\romannumeral1). We find that our MetaDiff would not work when using the general conditional UNet like \cite{rombach2022high}. This implies that leveraging the idea of gradient-based UNet to estimate noise is very beneficial. 

\noindent {\bf Are our task-conditional UNet effective?} In Table~\ref{table4}, (\romannumeral1) we evaluate TCUNet on miniImagenet; (\romannumeral2) we remove the UNet on  (\romannumeral1). From results, we can see that the performance of our TCUNet descreases by around 1\% $\sim$ 3\%. This implies that  leveraging the idea of gradient-based UNet to estimate noise is very beneficial for our TCUNet. 

\noindent {\bf Can our MetaDiff be applied to other classifiers?} To verify the universality of our MetaDiff on other classifiers, in Table~\ref{table5}, we evaluate our MetaDiff and ANIL on cosine classifiers and linear classifiers. We find that our MetaDiff all achieves superior performacne on these two classifier and cosine classifier performs better. This result implies that our MetaDiff is very universal for different classifier.

% Figure environment removed

%\begin{table}[t]
%	\centering
%	\caption{Cost analysis of our MetaDiff on miniImagenet.}\smallskip
%	\vspace{-15.4pt}
%	\smallskip\scalebox
%	{0.79}{
%		\smallskip\begin{tabular}{c|c|c|c|c|c}
%			\hline
			%\cline{2-2}
%			& \multicolumn{1}{l|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{5-way 1-shot} & \multicolumn{2}{c}{5-way 5-shot} \\
%			\cline{3-6}
%			& & Memory (G) & Time (s) & Memory (G) & Time (s) \\
%			\hline \hline
			%(\romannumeral1) & MAML &  & &  &\\
			%1-shot & 0.93$\%$ & 0.97$\%$ & $\%$ & \\
%			(\romannumeral2) & MetaSGD  & & & & \\
%			(\romannumeral3) & ALFA  & & & & \\
%			(\romannumeral4) & ANIL & & & & \\
%			(\romannumeral5) & MetaDiff & & & & \\
%			%5-way 5-shot &0.57$\%$ & $\%$ & $\%$ & \\
%			\hline
%	\end{tabular}}
%	
%	\label{table7}
%\end{table}

% Figure environment removed


\subsection{Statistical Analysis}
\label{section_5_5}

\noindent {\bf How much does our MetaDiff take GPU memory?} In Figure~\ref{fig4}, we select a latest gradient-based meta-learning (i.e., MetaLSTM \cite{RaviL17} and ALFA \cite{baik2020meta}) as baseline and report the GPU memory during training by varing the number of inner-loop number. From Figure~\ref{fig4}, we can see that 1) the cost of GPU memory keep increase linearly as the number of inner-loop step increase; however 2) our MetaDiff keep constant. This is reasonable because our MetaDiff is trained in a diffusion manner, which is irrelevant to inner-loop.

\noindent {\bf Can our MetaDiff converge?} We randomly select 600 5-way 1-shot tasks from the test set of miniImageNet, and then report their test accuracy and loss of entire denoising process, which are shown in Figure~\ref{fig5}. From results, we observe that our MetaDiff can converge without overfitting within a finite number of steps (i.e., around 450 steps).

%\noindent {\bf How does our MetaDiff work?} In Figure 4(b), we visualize the denosing process of target weights by using TSNE on miniImagenet. From Figure~\ref{}, we can see that a random weight marked by squares can be gradually denoised to target weights marked by stars. This indicates that our MetaDiff is effective, which well models the optimization of target base-learner from an random initialization.



\section{Conclusion}
In this paper, we present a novel meta-learning based conditional diffusion for few-shot learning, called MetaDiff. In particular, we find that the diffusion model actually is a generalization of gradient descent, a gradient descent algorithm with momentum updates and uncertainty estimation, and then design a task-conditional UNet from the perspective of gradient estimation to estimation the nosie for denoising target weight in our MetaDiff. Experimental results verify the effectiveness of our MetaDiff method. %We also conduct extensive statistical experiments and ablation studies, which further verify the effectiveness of our method.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
