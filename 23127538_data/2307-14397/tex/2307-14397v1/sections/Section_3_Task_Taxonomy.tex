% For aligning Sec 3.1 item (2)
\newenvironment{myquote}
  {\begin{list}{}{%
     \setlength{\leftmargin}{1.6em}%   adjust this to change the left margin
     \setlength{\rightmargin}{0pt}% adjust this for the right margin, 0 keeps it at default
     }
   \item\relax}
  {\end{list}}

\section{Generative Modeling under Data Constraint: Task Taxonomy, Challenges}
\label{sec:taxonomy}
In this section, first, we present our proposed taxonomy on different GM-DC tasks (Sec.~\ref{ssec:tasks}) highlighting their relationships and differences based on their attributes, e.g. unconditional or conditional  generation. 
Then, we present the unique challenges of GM-DC (Sec.~\ref{ssec:challenges}), including new insights such as domain proximity, and incompatible knowledge transfer.
Later, in Sec.~\ref{sec:comprehensive_review}, we present our proposed taxonomy on approaches for GM-DC, with a detailed review of individual work organized under our proposed taxonomy.



\subsection{Generative Modeling under Data Constraint: A Taxonomy on Tasks}
\label{ssec:tasks}


The goal of GM-DC is to learn to generate diverse and high-quality samples given only a small number of training samples. A number of GM-DC setups have been studied in different works
(Fig.~\ref{fig:timeline}). In this section, we propose a {\bf GM-DC task taxonomy} to categorize  setups in different works.  Tab. \ref{tab:tasktaxonomy} tabulates our GM-DC task taxonomy.

\begin{enumerate}

\item {\bf Unconditional generative modeling under data constraint (uGM-1).}
\vspace{-0.3em}
\begin{definition}[uGM-1]
{\em Given $K$ samples from domain $\D$, learn to generate diverse and high-quality samples from $\D$.}
\end{definition}
\vspace{-0.3em}
Without leveraging other side information, existing work has studied uGM-1 under limited samples ranging from 100 to several thousands. uGM-1 is an important task, especially for a domain that is distant from common domains, e.g. medical images which are distant from common personal photos in terms of content and characteristics.
In such scenarios, leveraging from common domains would not  provide any advantage. 

\item {\bf Unconditional generative modeling under data constraint with pre-trained generator  and cross-domain adaptation} (uGM-2).
\vspace{-0.3em}
\begin{definition}[uGM-2]
{\em Given a pre-trained generator on a source domain $\Ds$  (with numerous and diverse samples) and $K$ samples from a target domain $\Dt$, learn to generate diverse and high-quality samples from $\Dt$.}
\end{definition}
\vspace{-0.3em}
uGM-2 is similar to uGM-1, except that 
a pre-trained generator on another source domain $\Ds$ is additionally given.
uGM-2 is a major task in GM-DC and has been studied in many works.
In most works, close proximity in semantic 
between $\Ds$ and $\Dt$ is assumed, \eg $\Ds$ is photos of human faces, $\Dt$ is sketches of human faces.
For uGM-2, 
transfer learning  has been a popular approach to tackle this task driving GM-DC into the few-shot regime, \eg only 10 samples from $\Dt$ are given \cite{li2020ewc} (See Sec.~\ref{sec:comprehensive_review}
for the taxonomy of GM-DC approaches).
Recent work has started to look into the challenging setup when $\Ds$ and $\Dt$ are more semantically apart \cite{zhao2022adam}, \eg $\Ds$ is photos of human faces, $\Dt$ is photos of cat faces.  See Sec.~\ref{ssec:challenges} for further discussion on domain proximity in GM-DC.

\item {\bf Unconditional generative modeling under data constraint with pre-trained generator  and cross-domain adaptation, using text prompt (uGM-3).}
\vspace{-0.3em}
\begin{definition}[uGM-3]
{\em Given a pre-trained generator on a source domain $\Ds$  (with numerous and diverse samples) and a text prompt describing  a target domain $\Dt$, learn to generate diverse and high-quality samples from $\Dt$.}
\end{definition}
\vspace{-0.3em}
uGM-3 is similar to uGM-2, except that a text prompt is provided to describe $\Dt$ instead of samples from $\Dt$.
Particularly, this task requires generating samples  from $\Dt$ without seeing any sample from that domain, \ie 
 zero-shot domain adaptation.
Important work to tackle this task leverages recent large vision-language models to provide textual direction to guide the adaptation of the pre-trained generator to $\Dt$ \cite{gal2022stylegannada}.

\item {\bf Conditional generative modeling under data constraint (cGM-1).}
\vspace{-0.3em}
\begin{definition}[cGM-1]
{\em Given $K$ samples with class labels from a domain $\D$, learn to generate diverse and high-quality samples  conditioning on the class labels  from $\D$.}
\end{definition}
\vspace{-0.3em}
cGM-1 is similar to uGM-1 but focuses on conditional generation, \ie inputs to the generator include a random latent vector and a class label.
Conditional generative models such as BigGAN \cite{brock2019biggan} could achieve high-quality image generation 
when they are trained on large-scale datasets \eg ImageNet. However, under limited data, 
it is 
challenging to 
 achieve diverse and high-quality conditional sample generation.
As a natural extension of uGM-1, 
data augmentation has been studied for 
cGM-1 among other approaches, see Sec.~\ref{sec:comprehensive_review}.
 
\item {\bf Conditional generative modeling under data constraint with pre-trained generator (cGM-2).}
\vspace{-0.3em}
\begin{definition}[cGM-2]
{\em Given a pre-trained generator on the seen classes $\Cs$ of a domain $\D$, and
$K$ samples with class labels from unseen classes $\Cu$ of $\D$, learn to generate diverse and high-quality samples conditioning on the class labels for $\Cu$ from $\D$.}
\end{definition}
\vspace{-0.3em}
cGM-2 is similar to cGM-1, except that a pre-trained generator on the seen classes $\Cs$ is additionally given.
Note that in cGM-2, $\Cs$ and $\Cu$  contain disjoint classes, but both of them are from the same domain $\D$.
For example, \cite{shahbazi2021efficient} 
studies the setup when CIFAR100
\cite{krizhevsky2009cifar100}
is partitioned into 80 seen classes for the pre-trained generator and 20 unseen classes as the target, 
with 100 samples per unseen class given for training.
Meta-learning and transfer learning (regularizer-based fine-tuning, etc.) have been effective approaches for cGM-2, see Sec.~\ref{sec:comprehensive_review}.

\item {\bf Conditional Generative Modeling under data constraint with pre-trained generator and cross-domain adaptation (cGM-3).} 
\vspace{-0.3em}
\begin{definition}[cGM-3]
{\em Given a pre-trained generator on a source domain $\Ds$ (with numerous and diverse samples) and $K$ samples with class labels from a target domain $\Dt$, learn to generate diverse and high-quality samples conditioning on the class labels from $\Dt$.}
\end{definition}
\vspace{-0.3em}
\end{enumerate}

%--------------------------------------------------------------------
%                    Table 3: Our proposed taxonomy
%--------------------------------------------------------------------
\clearpage 
\begin{spacing}{0.88}
\fontsize{7pt}{7pt}
\selectfont
\setlength\tabcolsep{2pt}
\begin{longtable}{cc}
\caption{
{\bf Our proposed taxonomy for approaches in GM-DC.}
For each  approach,
the addressed GM-DC tasks (see Tab. \ref{tab:tasktaxonomy} for task definitions) and the data constraints 
are indicated. 
A detailed list of 
methods under 
each sub-category is also tabulated (some 
methods are under  multiple categories).
\xmark/\cmark ~denotes the absence/presence of the tasks commonly addressed by each approach, and  
the data constraints  
usually considered: {\bf LD}: \underline{L}imited-\underline{D}ata, {\bf FS}: \underline{F}ew-\underline{S}hot and {\bf ZS}: \underline{Z}ero-\underline{S}hot. 
}
\label{tab:approaches}
%
\\
\toprule
%\midrule
%==============Transfer Learning=====================
%Header----------------------
\rowcolor{gray!10}\multicolumn{2}{c}{{\bf \normalsize Transfer Learning} (Sec.~\ref{ssec:review_transferlearning})}
\\[2pt]
%Description----------------------
{\bf \small Description:}& \parbox[l]{0.90\linewidth}
{
Improve
GM-DC on target domain by knowledge of a generator pre-trained on source domain (with numerous and diverse samples).} 
\\[2pt]
{\bf \small Task:} & \parbox[l]{0.85\linewidth}
{
uGM-1 \xmark \hspace{0pt} %Task 1
uGM-2 \cmark \hspace{0pt} %Task 2
uGM-3 \cmark \hspace{0pt} %Task 3
cGM-1 \xmark \hspace{0pt} %Task 4
cGM-2 \cmark \hspace{0pt} %Task 5
cGM-3 \cmark \hspace{0pt}%Task 6
IGM \xmark \hspace{0pt} %Task 7
SGM \cmark %Task 8
\hspace{2pt} 
{\bf \small Data constraint:} \hspace{0pt}
LD \cmark \hspace{0pt}
FS \cmark \hspace{0pt}
ZS \cmark 
}
\\
\arrayrulecolor{black!50}
\midrule
%TF.1.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{1) Regularizer-based Fine-Tuning:} 
    Explore regularizers to preserve source generators' knowledge.}
    }
 \\[2pt]
%TF.1.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} TGAN\cite{wang2018tgan}, BSA\cite{noguchi2019bsa}, FreezeD\cite{mo2020freezed}, EWC\cite{li2020ewc}, CDC\cite{ojha2021cdc}, cGANTransfer\cite{shahbazi2021efficient}, W\textsuperscript{3}\cite{grigoryev2022when}, C\textsuperscript{3}\cite{Lee2021C3}, DCL\cite{zhao2022dcl}, RSSA\cite{xiao2022rssa}, fairTL\cite{teo2023fairtl}, GenOS\cite{zhang2022generalizedoneshot}
           , SVD\cite{robb2020svd}, 
           D\textsuperscript{3}-TGAN\cite{wu2023d3tgan}, JoJoGAN\cite{chong2022jojogan}, 
           KDFSIG\cite{hou2022exploitingkd}, CtlGAN\cite{wang2022ctlgan}, 
           ICGAN\cite{casanova2021icgan}, 
           MaskD\cite{zhu2022few}, 
           F\textsuperscript{3}\cite{yuichi2023_fewshot},
           ICGAN \cite{casanova2021icgan},
           DDPM-PA \cite{zhu2022few_dm},
           DWSC \cite{hou_dynamic}, 
           CSR \cite{gou2023csr},
           ProSC \cite{moon2023prosc}
       }
       }
\\
\midrule
%TF.2,1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{2) Latent Space:} 
   Explore latent space of source generator to identify suitable knowledge for adaptation.}
    }
\\[2pt]
%TF.2.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
     MineGAN\cite{wang2020minegan}, MineGAN++\cite{wang2021minegan++}, LCL\cite{mondal2023lcl}, WeditGAN\cite{duan2023weditgan}, GenDA\cite{yang2021genda},
     %, AGE\cite{ding2022age}, SAGE\cite{ding2023sage}, LSO\cite{zheng2023lso}
     SiSTA \cite{thopalli2023targetaware}, MultiDiffusion \cite{bar2023multidiffusion}
       }
       }
 \\
\midrule
%TF.3.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{3) Modulation:} 
   Leverage trainable modulation weights on top of frozen weights of the source generator.}
    }
 \\[2pt]
%TF.3.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
     AdaFMGAN\cite{zhao2020leveraging}, GAN-Memory \cite{cong2020ganmemory}, CAM-GAN\cite{varshney2021camgan}, AdAM\cite{zhao2022adam}, DynaGAN\cite{kim2022dynagan}, HyperDomainNet\cite{alanov2022hyperdomainnet}
       }
       }
\\
\midrule
%TF.4.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{4) Natural Language-guided:} 
      Use the feedback of vision-language models to adapt the source generator with text prompts.}
    }
 \\[2pt]
%TF.4.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
       StyleGAN-NADA\cite{gal2022stylegannada}, MTG\cite{zhu2022mindthegap}, HyperDomainNet\cite{alanov2022hyperdomainnet}, DiFa\cite{zhang2022difa}, OneCLIP\cite{kwon2022oneclip}, IPL\cite{guo2023ipl},
       SINE\cite{zhang2023sine}, DreamBooth\cite{ruiz2023dreambooth},
       MCC\cite{kumari2023mcc}, 
       \\ Textual-Inversion\cite{gal2022textualinversion}, SpecialistDiffusion\cite{lu2023specialistdiffusion},
       BLIP-Diffusion\cite{li2023blipdiffusion}
       }
       }
\\
\midrule
%TF.5.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{5) Adaptation-Aware:} 
      Preserve the source generator's knowledge that is important to the adaptation task.}
    }
 \\[2pt]
%TF.5.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
        AdAM\cite{zhao2022adam}, RICK\cite{zhao2023rick}
       }
       }
\\
\midrule
%TF.6.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{6) Prompt Tuning:} 
      Freeze the source generator and add/ generate visual prompts to guide generation for the target domain.}
    }
 \\[2pt]
%TF.6.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
       VPT \cite{sohn2023vpt}
       }
       }
\\
\arrayrulecolor{black!100} \midrule %\bottomrule
%==============Data Augmentation-=====================
%Header----------------------
\rowcolor{gray!10}\multicolumn{2}{c}{{\bf \normalsize Data Augmentation}
(Sec.~\ref{ssec:review_dataaugmentation})}
\\[2pt]
%Description----------------------
{\bf \small Description:}& \parbox[l]{0.90\linewidth}{
Improve GM-DC by increasing coverage of the data distribution by applying various transformations on the given samples.}

\\[2pt]
%Buttons----------------------
{\bf \small Task:} & \parbox[l]{0.85\linewidth}
{
uGM-1 \cmark \hspace{0pt} %Task 1
uGM-2 \xmark \hspace{0pt} %Task 2
uGM-3 \xmark \hspace{0pt} %Task 3
cGM-1 \xmark \hspace{0pt} %Task 4
cGM-2 \xmark \hspace{0pt} %Task 5
cGM-3 \xmark \hspace{0pt} %Task 6
IGM \xmark \hspace{0pt} %Task 7
SGM \xmark %Task 8
\hspace{2pt}
{\bf \small Data constraint:} \hspace{0pt} 
LD \cmark \hspace{0pt}
FS \xmark \hspace{0pt}
ZS \xmark 
}
\\
\arrayrulecolor{black!50}
\midrule
%DA.1.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{1) Image-Level Augmentation:} 
      Apply data transformations 
      %$\{ T_k \}$ 
      on image space.}
    }
 \\[2pt]
%DA.1.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
        ADA \cite{karras2020ada}, DiffAugment\cite{zhao2020diffaug}, IAG\cite{zhao2020imageaugmentation}, DiffusionGAN\cite{wang2023diffusiongan}, bCR\cite{zhao2021improved}, CR-GAN\cite{Zhang2020Consistency}, APA \cite{jiang2021deceive}, PatchDiffusion\cite{wang2023patchdiffusion}
       }
       }
\\
\midrule
%DA.2.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{2) Feature-Level Augmentation:} 
     Apply data transformations 
     %$\{ T_k \}$ 
     on the feature space.
    }
    }
 \\[2pt]
%DA.2.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
        AdvAug\cite{chen2021advaug}, AFI\cite{dai2021implicit}
       }
       }
\\
\midrule
%DA.3.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{3) Transformation-Driven Design:} 
    Leverage the information of 
    individual transformations
    to design an efficient learning mechanism.
    }
}
 \\[2pt]
%DA.3.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
        DAG\cite{tran2021dag}, SSGAN-LA\cite{hou2021labelaugmentation}
       }
       }
\\
\arrayrulecolor{black!100}
\midrule
%==============Network Architecture-=====================
%Header----------------------
\rowcolor{gray!10}\multicolumn{2}{c}{{\bf \normalsize Network Architectures} {(Sec.~\ref{ssec:review_networkarchitecture})}}
\\[2pt]
%Description----------------------
{\bf \small Description:}& \parbox[l]{0.90\linewidth}{Design specific architecture for the generator to improve its learning under data constraints.} 
\\[2pt]
%Challenges----------------------
%Buttons----------------------
{\bf \small Task:} & \parbox[l]{0.85\linewidth}
{
uGM-1 \cmark \hspace{0pt} %Task 1
uGM-2 \xmark \hspace{0pt} %Task 2
uGM-3 \xmark \hspace{0pt} %Task 3
cGM-1 \cmark \hspace{0pt} %Task 4
cGM-2 \xmark \hspace{0pt} %Task 5
cGM-3 \xmark \hspace{0pt} %Task 6
IGM \xmark \hspace{0pt} %Task 7
SGM \xmark %Task 8
\hspace{2pt} 
{\bf \small Data constraint:} \hspace{3pt} 
LD \cmark \hspace{1pt}
FS \xmark \hspace{1pt}
ZS \xmark 
}
\\
\arrayrulecolor{black!50}
\midrule
%NA.1.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{1) Feature Enhancement:} 
      Design additional modules/ layers to enhance/ retain the feature maps of the generator for better generative modeling.}
    }
 \\[2pt]
%NA.1.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
    FastGAN\cite{liu2021fastgan}, MoCA\cite{li2022moca}, DFSGAN\cite{yang2023dfsgan},
    SCHA-VAE \cite{pmlr-v162-giannone22a}
       }
       }
\\
\midrule
%NA.2.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{2) Ensemble Large Pre-trained Vision Models:} 
    Improve architecture by integrating pre-trained vision models to enable more accurate GM-DC.}
      %under limited data}
    }
 \\[2pt]
%NA.2.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
    Vision-aided GAN\cite{kumari2022ensembling}, ProjectedGAN \cite{sauer2021projectedgan}
       }
       }
\\
\midrule
%NA.3.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{3) Dynamic Network Architecture:} 
     Improve generative learning with limited data by evolving the generator architecture during training.
    }
    }
 \\[2pt]
%NA.3.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
    CbC\cite{shahbazi2022collapse}, DynamicD\cite{yang2022dynamicd}, AdvAug\cite{chen2021advaug}, Re-GAN\cite{saxena2023regan},
    AutoInfoGAN \cite{shi2023autoinfogan}
       }
       }
\\
\arrayrulecolor{black!100}
\midrule
%==============Multi-Task Objectives-=====================
%Header----------------------
\rowcolor{gray!10}\multicolumn{2}{c}{{\bf \normalsize 
Multi-Task Objectives} 
{(Sec.~\ref{ssec:review_trainingtechniques})}}
\\[2pt]
%Description----------------------
{\bf \small Description:}& \parbox[l]{0.90\linewidth}{
Introduce additional task(s) to extract generalizable representations that are useful for all tasks, to reduce overfitting under data constraints.

} 
\\[2pt]
{\bf \small Task:} & \parbox[l]{0.85\linewidth}
{
uGM-1 \cmark \hspace{0pt} %Task 1
uGM-2 \cmark \hspace{0pt} %Task 2
uGM-3 \xmark \hspace{0pt} %Task 3
cGM-1 \cmark \hspace{0pt} %Task 4
cGM-2 \xmark \hspace{0pt} %Task 5
cGM-3 \xmark \hspace{0pt} %Task 6
IGM \xmark \hspace{0pt} %Task 7
SGM \xmark %Task 8
\hspace{2pt} 
{\bf \small Data constraint:} \hspace{3pt} 
LD \cmark \hspace{1pt}
FS \cmark \hspace{1pt}
ZS \xmark 
}
\\
\arrayrulecolor{black!50}
\midrule
%TT.1.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{1) Regularizer:} 
    Add an additional task objective as a regularizer to prevent an undesirable behaviour during training generative model.
    }
    }
 \\[2pt]
%TT.1.1 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
         LeCam\cite{tseng2021lecam}, DigGAN\cite{fang2022diggan}, MDL\cite{kong2022mdl}, RegLA\cite{hou2023regularizing}
       }
       }
\\
\midrule

%TT.2.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{2) Contrastive Learning:} 
    Introduce a pretext task to enhance the learning process of the generative model.
    }
    }
 \\[2pt]
%TT.2.1 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
     InsGen\cite{yang2021insgen}, FakeCLR\cite{li2022fakeclr}, DCL\cite{zhao2022dcl}, C\textsuperscript{3}\cite{Lee2021C3}, ctlGAN\cite{wang2022ctlgan}, IAG\cite{zhao2020imageaugmentation}, CML-GAN\cite{phaphuangwittayakul2022cmlgan}
       }
       }
       \\
\midrule
%TT.4.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{3) Masking:} 
    Mask a part of the image/ information to increase the task hardness and prevent learning the trivial solutions.
    }
    }
 \\[2pt]
%TT.4.1 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
     MaskedGAN\cite{huang2022maskedgan}, MaskD\cite{zhu2022few},
     DMD \cite{zhang2023dmd}
       }
       }
\\
\midrule
\pagebreak
%TT.5.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{4) Knowledge Distillation:} 
    Add a task objective that enforces the generator to follow a strong teacher.
    }
    }
\\[2pt]
%TT.5.1 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
    KD-DLGAN\cite{cui2023kddlgan}, KDFSIG\cite{hou2022exploitingkd}
       }
       }
\\
\midrule
%TT.6.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{5) Prototype Learning:} 
    Emphasize learning prototypes for samples/ concepts within the distribution as an additional task objective.
    }
    }
 \\[2pt]
%TT.6.1 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
     ProtoGAN\cite{yang2023protogan}, MoCA\cite{li2022moca}
       }
       }
\\
\midrule

%TT.3.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{6) Other Multi-Task Objectives:} 
    Apply other
    types of multi-task objectives
    including co-training, patch-level learning, and diffusion.
    }
    }
 \\[2pt]
%TT.3.1 (Works)--------------------------------------------
\multicolumn{2}{l}{    
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
     GenCo\cite{cui2022genco}, PatchDiffusion\cite{wang2023patchdiffusion}, AnyRes-GAN\cite{chai2022anyresolution} , DiffusionGAN\cite{wang2023diffusiongan}, D2C\cite{sinha2021d2c},
     AdaptiveIMLE \cite{aghabozorgi2023adaptiveimle},
     FSDM \cite{giannone2022fsdm}
       }
       }
\\
\arrayrulecolor{black!100}
\midrule
%==============Exploiting Frequency Components=====================
%Header----------------------
\rowcolor{gray!10}\multicolumn{2}{c}{{\bf \normalsize Exploiting Frequency Components} {(Sec.~\ref{ssec:review_exploitingfrequency})}}
\\[2pt]
%Description----------------------
{\bf \small Description:}& \parbox[l]{0.90\linewidth}{
Exploit frequency components to improve learning the generative model by reducing frequency bias.} 
\\[2pt]
%Buttons----------------------
{\bf \small Task:} & \parbox[l]{0.85\linewidth}
{
uGM-1 \cmark \hspace{0pt} %Task 1
uGM-2 \xmark \hspace{0pt} %Task 2
uGM-3 \xmark \hspace{0pt} %Task 3
cGM-1 \xmark \hspace{0pt} %Task 4
cGM-2 \cmark \hspace{0pt} %Task 5
cGM-3 \xmark \hspace{0pt} %Task 6
IGM \xmark \hspace{0pt} %Task 7
SGM \xmark %Task 8
\hspace{2pt} 
{\bf \small Data constraint:} \hspace{3pt} 
LD \cmark \hspace{1pt}
FS \cmark \hspace{1pt}
ZS \xmark 
}
\\
\arrayrulecolor{black!50}
\midrule
%EFC.1.1 (Definition)--------------------------------------------
% No Subcategory
%EFC.1.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
     FreGAN\cite{yang2022fregan}, WaveGAN\cite{yang2022wavegan}, MaskedGAN\cite{huang2022maskedgan}, Gen-co\cite{cui2022genco}
       }
       }
\\
\arrayrulecolor{black!100}
\midrule
%==============Meta-Learning=====================
%Header----------------------
\rowcolor{gray!10}\multicolumn{2}{c}{{\bf \normalsize Meta-Learning} {(Sec.~\ref{ssec:review_metalearning})}}
\\[2pt]
%Description----------------------
{\bf \small Description:}& \parbox[l]{0.90\linewidth}{

Learn meta-knowledge from seen classes to improve generator learning for unseen classes.
} 
\\[2pt]

%Buttons----------------------
{\bf \small Task:} & \parbox[l]{0.85\linewidth}
{
uGM-1 \xmark \hspace{0pt} %Task 1
uGM-2 \xmark \hspace{0pt} %Task 2
uGM-3 \xmark \hspace{0pt} %Task 3
cGM-1 \xmark \hspace{0pt} %Task 4
cGM-2 \cmark \hspace{0pt} %Task 5
cGM-3 \xmark \hspace{0pt} %Task 6
IGM \xmark \hspace{0pt} %Task 7
SGM \xmark %Task 8
\hspace{2pt} 
{\bf \small Data constraint:} \hspace{3pt} 
LD \xmark \hspace{1pt}
FS \cmark \hspace{1pt}
ZS \xmark 
}
\\
\arrayrulecolor{black!50}
\midrule
%ML.1.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{1) Optimization:}
    Learn initialization weights from the seen classes as meta-knowledge
    to enable quick adaptation to unseen classes.
    }
    }
 \\[2pt]
%ML.1.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
    GMN\cite{bartunov2018few}, FIGR\cite{clouatre2019figr}, Dawson\cite{liang2020dawson}, FAML\cite{phaphuangwittayakul2021faml}, CML-GAN\cite{phaphuangwittayakul2022cmlgan}
       }
       }
\\
\midrule
%ML.2.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{2) Transformation:} 
    Learn sample transformations from the seen classes as meta-knowledge and use them for sample generation for unseen classes.
    }
    }
 \\[2pt]
%ML.2.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:}
    DAGAN\cite{antoniou2017dagan}, DeltaGAN\cite{hong2022deltagan}, Disco\cite{hong2022disco}, AGE\cite{ding2022age}, SAGE\cite{ding2023sage}, HAE\cite{li2022hae}, LSO \cite{zheng2023lso}
       }
       }
\\
\midrule
%ML.3.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{3) Fusion:} 
    Learn to fuse the samples of the seen classes as meta-knowledge, and apply learned meta-knowledge to generation for unseen classes.
    }
    }
 \\[2pt]
%ML.3.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:} 
    MatchingGAN\cite{hong2020matchinggan}, F2GAN\cite{hong2020f2gan}, LofGAN\cite{gu2021lofgan}, WaveGAN\cite{yang2022wavegan}, AMMGAN\cite{li2023ammgan}
       }
       }
\\
\arrayrulecolor{black!100}
\midrule
%==============Modeling Internal Patch Distribution=====================
%Header----------------------
\rowcolor{gray!10}\multicolumn{2}{c}{{\bf \normalsize Modeling Internal Patch Distribution} {(Sec.~\ref{ssec:review_internalpatch})}}
\\[2pt]
%Description----------------------
{\bf \small Description:}& \parbox[l]{0.90\linewidth}{Learn the internal patch distribution within one image to generate diverse samples
%that carry the 
with the
same visual content (patch distribution).} 
\\[2pt]
%Buttons----------------------
{\bf \small Task:} & \parbox[l]{0.85\linewidth}
{
uGM-1 \xmark \hspace{0pt} %Task 1
uGM-2 \xmark \hspace{0pt} %Task 2
uGM-3 \xmark \hspace{0pt} %Task 3
cGM-1 \xmark \hspace{0pt} %Task 4
cGM-2 \xmark \hspace{0pt} %Task 5
cGM-3 \xmark \hspace{0pt} %Task 6
IGM \cmark \hspace{0pt} %Task 7
SGM \xmark %Task 8
\hspace{2pt} 
{\bf \small Data constraint:} \hspace{3pt} 
LD \xmark \hspace{1pt}
FS \cmark \hspace{1pt}
ZS \xmark 
}
\\
\arrayrulecolor{black!50}
\midrule
%MIPD.1.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{1) Progressive Training:} 
    Train a generative model progressively %Progressively train a generative model
    to learn the patch distribution at different scales/ noise levels.
    }
    }
 \\[2pt]
%MIPD.1.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:}
    SinDiffusion\cite{wang2022sindiffusion}, SinDDM\cite{kulikov2023sinddm}, Deff-GAN\cite{kumar2023deffGAN}, BlendGAN\cite{kligvasser2022blendgan}, SinGAN\cite{shaham2019singan}, ConSinGAN\cite{hinz2021consingan}
       }
       }
\\
\midrule
%MIPD.2.1 (Definition)--------------------------------------------
\multicolumn{2}{l}{
    \cellcolor{gray!0}\parbox[l]{\linewidth}{
    \textbf{2) Non-progressive Training:} 
    %Training takes place 
    Train a generative model
    on the same scale/ noise but with changes to the modelâ€™s architecture.
    }
    }
 \\[2pt]
%MIPD.2.2 (Works)--------------------------------------------
\multicolumn{2}{l}{
    \parbox[l]{\linewidth}{
    \emph{Methods:}
        SinFusion\cite{nikankin2022sinfusion}, One-Shot GAN\cite{sushko2021oneshotgan}
       }
       }
\\
\arrayrulecolor{black!100}
\bottomrule
\end{longtable}
\end{spacing}
% }

% continue \item 2


\begin{enumerate}\setcounter{enumi}{6}
\item[]{cGM-3 is similar to uGM-2 as cross-domain adaptation is required in both tasks, but cGM-3 focuses on conditional generation while uGM-2 focuses on unconditional generation.
Furthermore, cGM-3 is similar to cGM-2, but seen classes and unseen classes are from different domains in cGM-3.
For example, \cite{shahbazi2021efficient} has studied the setup when a pre-trained generator on ImageNet is adapted to generate samples for several classes from Places365 \cite{zhou2017places}.
Transfer learning is one of the effective approaches for cGM-3, see Sec.~\ref{sec:comprehensive_review}.}
\item {\bf Internal patch distribution Generative Modeling  (IGM).}
\vspace{-0.3em}
\begin{definition}[IGM]
{\em Given $K$ samples and assuming rich internal distribution for patches within these samples, learn to generate diverse and high-quality samples with the same internal patch distribution.}
\end{definition}
\vspace{-0.3em}
IGM aims to  capture the internal distribution of
patches within the samples. 
With the model capturing the samples' patch statistics, it is then possible  to generate high
quality, diverse samples 
with the same content as the given training samples.
In most works, $K = 1$, and IGM focuses on images \cite{shaham2019singan},
learning to generate new images with 
significant variability while maintaining
both the global structure and fine textures of the training image.



\item {\bf Subject-driven Generative Modeling  (SGM).}
\vspace{-0.3em}
\begin{definition}[SGM]
{\em Given $K$ samples of a particular subject and a text prompt, learn to generate diverse and high-quality samples containing the same subject.}
\end{definition}
\vspace{-0.3em}
SGM is a 
recent GM-DC task introduced in \cite{ruiz2023dreambooth}.
Given  a few images (3-5 in most cases) of a subject
and leveraging a large text-to-image generative model, \cite{ruiz2023dreambooth} learns to generate diverse images of the subject in different contexts with the guidance of text prompts. 
The goals are: i) to achieve  natural interactions between the subject and diverse new contexts, and ii)  to maintain high fidelity to the key visual features of the subject. 
In \cite{ruiz2023dreambooth}, a natural language-guided transfer learning approach and a new prior preservation loss have been proposed to achieve SGM.

\end{enumerate}
