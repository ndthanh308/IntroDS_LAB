\section{Comprehensive Review}
\label{sec:comprehensive_review}


In this section, 
first, we  will present our proposed taxonomy of approaches for GM-DC which systematically categorizes and organizes GM-DC methods under seven approaches (Tab.~\ref{tab:approaches}) based on the principal ideas of these methods.
Then, we will discuss individual GM-DC methods organized under our proposed taxonomy.


\noindent
{\bf Our Proposed Taxonomy of Approaches for GM-DC} categorizes  GM-DC methods into seven groups:
\begin{enumerate}
    \vspace{0.1cm}
    \item {\bf Transfer Learning:} 
    In GM-DC, transfer learning (TL) aims to improve the learning of the generator for the target domain using the knowledge of a generator pre-trained on a source domain (with numerous and diverse samples). For example, some methods under this category use the knowledge of a StyleGAN2 pre-trained on the large FFHQ \cite{karras2019style} to improve  the learning of generation for face paintings by an artist, given only a few images of the artist's paintings \cite{ojha2021cdc,zhao2022dcl,xiao2022rssa}. Major challenges for TL-based GM-DC are to identity, select and preserve suitable knowledge of the source generator for the target generator.
    Along this line, there are six subcategories: i) {\em Regularization-based Fine-tuning}, explores regularizers to preserve suitable source generator's knowledge to improve learning target generator;  
    ii) {\em Latent Space}, explores transformation/ manipulation of the source generator latent space;
    iii) {\em Modulation},  freezes and transfers weights of the source generator to the target generator and adds trainable modulation weights on top of frozen weights to increase the adaptation capability to the target domain; iv) {\em Natural Language-guided}, uses natural language prompt and supervision signal from language-vision models to adapt source generator to target domain; v) {\em Adaptation-Aware}, identifies and preserves the source generator's knowledge that is important to the adaptation task; vi) {\em Prompt Tuning}, is an emerging idea that freezes the weights of the source generator and learns to generate visual prompts (tokens) to guide generation for the target domain.
    \vspace{0.1cm}
    \item {\bf Data Augmentation:} Augmentation aims to improve GM-DC by increasing coverage of the  data distribution with  applying various transformations $\{ T_k \}_{k=1}^{K}$ to available data. 
    For example, within this category, some works augment the available limited data to train an unconditional StyleGAN2 \cite{karras2020analyzing} using the 100-shot Obama dataset or train a conditional BigGAN \cite{brock2019biggan} with only 10\% of the CIFAR-100 dataset.
    A major challenge of these approaches is augmentation leakage, 
    where
    the generator learns the augmented distribution, \eg, generating rotated/ noisy samples.
    There are three representative categories: 
    i) {\em Image-Level Augmentation}, applies the transformations on the image space; 
    ii) {\em Feature-Level Augmentation}, applies the transformations on the feature space; 
    iii) {\em Transformation-Driven Design}, leverages the information on each individual transformation $T_k$
    for
    an efficient learning mechanism.
    \vspace{0.1cm}
    \item {\bf Network Architectures:} These approaches design specific architectures for the generators to improve their learning under data constraints. 
    Some works in this category design shallow/ sparse generators to prevent overfitting to training data due to over-parameterization.
    The primary challenge is that when endeavoring to design a new architecture, the process of discovering the optimal hyperparameters can be laborious.
    There are three major types of architectural designs for GM-DC: 
    i) {\em Feature Enhancement}, introduces additional modules to enhance/ retain the knowledge within feature maps; 
    ii) {\em Ensemble Large Pre-trained Vision Models}, leverages large pre-trained vision models to aid more accurate generative modeling, iii) {\em Dynamic Network Architecture}, evolves the architecture of the generative model during training to compensate for data constraints.
    \vspace{0.1cm}
    \item {\bf 
    Multi-Task Objectives:} 
    These approaches modify the learning objective of the generative model by introducing additional task(s) to extract generalizable representations and reduce overfitting under data constraints.
    As an example, some works define a pretext task based on contrastive learning \cite{he2020momentum} to pull the positive samples and push away negative ones in addition to the original generative learning task to prevent overfitting under limited available data.
    The efficient integration of the new objective with the generative learning objective could be challenging under data constraints.
    These works can be categorized into several kinds of approaches: 
    i) {\em Regularizer}, adds an additional learning objective as a regularizer to prevent an undesirable behavior during training generative model under data constraint. Note that this category is different from regularizer-based fine-tuning as the latter aims to preserve source knowledge, but the former is for training without a source generator; 
    ii) {\em Contrastive Learning}, adds the learning objective related to a pretext task to enhance the learning process of the generative model using additional supervision signal from solving this pretext task; 
    iii) {\em Masking},
    introduces alternative learning objective by masking a part of the image/ information to improve generative modeling by increasing the task hardness and preventing learning the trivial solutions; 
    iv) {\em Knowledge Distillation}, introduces an additional learning objective that enforces the generator to follow a strong teacher; 
    v) {\em Prototype Learning}, emphasizes learning prototypes for samples/ concepts within the distribution as an additional objective; 
    vi) {\em Other Multi-Task Objectives}, include co-training, patch-level learning, and using diffusion to enhance 
    generation.
    \vspace{0.1cm}
    \item {\bf Exploiting Frequency Components:} Deep generative models exhibit frequency bias tending to ignore high-frequency signals as they are hard to generate \cite{schwarz2021frequency}. Data constraints can exacerbate this problem \cite{yang2022fregan}. 
    The approaches in this category aim to improve frequency awareness of the generative models by leveraging frequency components during training. 
    For instance, certain approaches employ Haar Wavelet transform to extract high-frequency components from the samples. 
    These frequency components are then fed into various layers using skip connections, aiming to alleviate the challenges associated with generating high-frequency details.
    Despite its effectiveness, utilizing frequency components for GM-DC has not been thoroughly investigated. The performance can be enhanced by incorporating more advanced techniques for extracting frequency components.
    \vspace{0.1cm}
    \item {\bf Meta-Learning:} These approaches create sample generation tasks with data constraints for the seen classes, and learn the meta-knowledge ---knowledge that is shared between all tasks--- 
    across these tasks during meta-training. This meta-knowledge is then used in improving generative modeling for the unseen classes with data constraints.
    For instance, some studies, as meta-knowledge, learn to fuse the samples from 
    the seen categories $C_{seen}$ of the Flowers  dataset \cite{nilsback2008oxford_flower} for sample generation.
    This meta-knowledge enables the model to generate new samples from unseen classes $C_{unseen}$ within the same dataset ($C_{seen} \cap C_{unseen} = \emptyset$) by fusing only 3 samples from each class.
    Note that as these works employ episodic learning within a generative framework, the training stability can be impacted.
    Approaches within this line can be classified into three categories: 
    i) {\em Optimization}, initializes the generative model with weights learned on the seen classes as meta-knowledge, to enable quick adaptation to unseen classes with limited steps of the optimization;
    ii) {\em Transformation}, learns cross-category transformations from the samples of the seen classes as meta-knowledge and applies them to available samples of the unseen classes to generate new samples; 
    iii) {\em Fusion}, learns to fuse the samples of the seen classes as meta-knowledge, and applies learned meta-knowledge to sample generation by fusing samples of the unseen classes.
    \vspace{0.1cm}
    \item {\bf Modeling Internal Patch Distribution:} These approaches aim to learn the internal patch distribution within one image (in some cases a few images), and then generate diverse samples that carry the same visual content (patch distribution) with an arbitrary size, and aspect ratio. 
    As an example, some works train a Diffusion Model using a single image of the ``Marina Bay Sands'', and after training, the Diffusion Model can generate similar images, but include additional towers topped by the similar ``Sands Skypark''.
    However, a major limitation of these methods lies in the fact that for every single image, usually a separate generative model is trained from scratch, 
    neglecting the potential for efficient training through knowledge transfer in this context.
    Approaches proposed along this line can be categorized into two major groups: i) {\em Progressive Training}, progressively trains a generative model to learn the patch-distribution at different scales or noise levels; ii) {\em Non-Progressive Training}, learns a generative model at a single scale by implementing additional sampling techniques or new model architectures.  
\end{enumerate}
In what follows we delve into detailed descriptions of the approaches within each category.
\vspace{-0.1cm}

\subsection{Transfer Learning}
\label{ssec:review_transferlearning}
Transfer Learning (TL) is a major approach for GM-DC. 
Given a source generator $G_s$ (for GANs both $G_s$ and $D_s$) pre-trained on a large and diverse source domain $\mathcal{D}_s$, these approaches aim to learn an adapted generator to the target domain $G_t$ by initializing the weights to that of the source generator. 



\subsubsection{Regularizer-based Fine-Tuning}

TGAN \cite{wang2018tgan} is the first systematic study to evaluate transfer learning in GANs. 
TGAN shows that transfer learning reduces the convergence time and improves generative modeling under limited data. 
The knowledge transfer is performed by using the source GAN for initializing the weights of the target GAN, followed by fine-tuning the weights on target data.
TGAN \cite{wang2018tgan} demonstrates that: i) transferring $D$ is more important than $G$, while transferring both $G$ and $D$ gives the best results; ii) transfer learning performance degrades by increasing the distance between source and target domains or decreasing the number of samples from target domain; iii) to select a pre-trained GAN for a target domain, in addition to a smaller distance, more dense source domains are preferable. 
As an example, for the Flower \cite{nilsback2008oxford_flower} target domain, surprisingly, a GAN pre-trained on semantically unrelated LSUN Bedrooms \cite{yu2015lsun} is shown to be among the best sources \cite{wang2018tgan}.

W\textsuperscript{3} \cite{grigoryev2022when} revisits the transfer learning in GANs with a modern structure ---StyleGAN2-ADA \cite{karras2020analyzing, karras2020ada} instead of WGAN-GP \cite{gulrajani2017improved} used in TGAN.
Results in \cite{grigoryev2022when} suggest that for SOTA GANs, it is beneficial to transfer the knowledge from sparse and diverse sources (pre-trained StyleGAN2 on ImageNet) rather than dense and less diverse ones.
One major limitation of TGAN is that under limited data simply fine-tuning the whole generator destroys a considerable portion of the general knowledge obtained on the source domain, and results in overfitting. Almost all of the following works aim to address this by different approaches to preserve the knowledge of the source generator.

%BSA
BSA \cite{noguchi2019bsa} limits scale and shift parameter updates during fine-tuning for batch normalization (BN) layers.
%FreezD
FreezeD \cite{mo2020freezed}, hypothesizes that as $D$ performs the discriminative task during training a GAN, based on common knowledge in discriminative learning \cite{yosinski2014transferable}, its early layers encode general knowledge which is shared between source and target domains. 
Therefore, this general knowledge is preserved during adaptation by freezing early layers of $D$.
% cGANTransfer
cGANTransfer \cite{shahbazi2021efficient} assumes that the pre-trained $G$ is conditioned on class labels using BN parameters, \ie each class has its own BN parameters \cite{brock2019biggan}. Then, explicit knowledge propagation from seen classes to unseen classes is enforced by defining the BN parameters of the unseen classes to be the weighted average of the BN parameters of seen classes.
%SVD
SVD \cite{robb2020svd} applies singular value decomposition \cite{van1976generalizing}, and only updates the singular values that are related to changing entanglement between different attributes within data to constrain the parameter update.

%EWC
EWC \cite{li2020ewc} aims to preserve the diversity of the source GAN during adapting to a target domain with only a few samples, \eg, 10-shot. The importance of each parameter in source GAN is measured by Fisher Information (FI), and the change on each parameter during adaption is penalized based on its importance, \eg, change over important parameters is penalized more.
% CDC
CDC \cite{ojha2021cdc} aims to keep the diversity of the generated samples using a cross-domain correspondence loss.
Specifically, first, a batch of $N+1$ latent codes are sampled for image generation: $\{ G(z_0), ..., G(z_N) \}$.
Then, using $G(z_0)$ as a reference, the similarity of generated samples to reference is measured for the generator before and after adaptation, resulting in two $N-way$ probability vectors. The diversity is preserved by adding the KL divergence between these two probability vectors to the standard loss as a regularizer.
% MaskD
MaskD~\cite{zhu2022few} applies random masks to extracted features of $D$, on top of CDC \cite{ojha2021cdc}, to prevent overfitting. 
% DDPM-PA
DDPM-PA \cite{zhu2022few_dm} uses a pairwise adaptation method similar to CDC for adapting diffusion models to the new domain.
% RSSA
RSSA \cite{xiao2022rssa} extends the cross-domain consistency idea of the CDC \cite{ojha2021cdc} to a more constrained form by preserving the structural similarity of the samples before and after adaptation.
% ProSC
ProSC \cite{moon2023prosc} extends RRSA by performing a progressive adaptation to the target domain in $N$ iterations instead of a single adaptation to reduce the gap between pair of domains.
% CSR
CSR \cite{gou2023csr} uses a similar idea to CDC but applies semantic loss directly to the spatial space, \ie, generated images with $G_s$ and $G_t$.

% C3 - DCL - CtlGAN
$C^3$ \cite{Lee2021C3}, DCL \cite{zhao2022dcl}, and CtlGAN \cite{wang2022ctlgan} aim to preserve the diversity by applying contrastive learning. 
Assuming $G_s(z_i)$ as an anchor point, the generated sample for the same latent code with the adapted generator ($G_t(z_i)$) is considered a positive pair, and the generated samples with the adapted generator for other latent code values ($G_t(z_j)$, $i \neq j$) are considered as negative pairs. 
Additionally, DCL applies similar contrastive learning to the $D$.

%JoJoGAN
JoJoGAN \cite{chong2022jojogan} addresses one-shot image generation using the style space of StyleGAN2.
First, GAN inversion is used to find the corresponding style code of the reference image. Then, style mixing is used to generate a set of style codes, and generated images with these styles are used for GAN adaptation.
%GenOS
GenOS \cite{zhang2022generalizedoneshot} includes entity transfer with some related entity mask using an auxiliary network.
%D3-TGAN
D\textsuperscript{3}-TGAN
\cite{wu2023d3tgan} first inverts each target sample into the latent code space of the source GAN.
Then, the maximum mean discrepancy between the features of the source $G$ for inverted code and features of the adapted GAN for a random latent code is used as a regularizer.


% FairTL
FairTL \cite{teo2023fairtl} adopts transfer learning in GANs to train a fair generative model \wrt a sensitive attribute (SA) using a limited fair dataset.
% IC-GAN
To model complex distributions like ImageNet, IC-GAN \cite{casanova2021icgan} learns data distribution as a mixture of conditional distributions.
This enables IC-GAN to generate images from unseen distributions, by just changing the conditioning instances on the target samples.
%KDFSIG
KDFSIG \cite{hou2022exploitingkd} exploits the knowledge distillation idea by treating the source model as a teacher and the target model as a student.
% F3
F\textsuperscript{3} \cite{yuichi2023_fewshot} proposes a faster method to generate face images with features of a specific group. First, a GAN Inversion of target images is applied and then PCA is leveraged as a feature extraction strategy to render features of target group.
% DWSC
DWSC \cite{hou_dynamic} proposes the dynamic weighted semantic correspondence between the source and target generator during adaption to preserve the diversity.



%-----------Latent Space---------------%
\subsubsection{Latent Space} 

% MineGAN
MineGAN \cite{wang2020minegan} trains a miner network $M$ during adaptation, to map the latent space $z$ of the source GAN to another space $u = M(z)$ more appropriate for the target domain. 
MineGAN++\cite{wang2021minegan++} extends MineGAN by only updating important parameters.
% GenDA
GenDA \cite{yang2021genda} proposes a lightweight attribute adaptor in the form of scaling and shifting latent codes
to adapt the latent space of the source GAN to the target GAN.
LCL \cite{mondal2023lcl} freezes $G$ and learns a network to map the latent codes from the $\mathcal{Z}$ space to the extended intermediate space $\mathcal{W}^+$ of a pre-trained StyleGAN2 during adapting GAN. 
%WeditGAN 
WeditGAN \cite{duan2023weditgan} proposes to learn a constant offset parameter ($\Delta w$) for the target domain in the intermediate latent space of StyleGAN2 to relocate source latent codes to the target domain.
%SiSTA
After fine-tuning the generator to a target domain, SiSTA \cite{thopalli2023targetaware} perturbs latent representations of the fine-tuned generator that falls below a threshold, either by replacing them with zero or reverting them back to the pre-trained generator's weights.
% MultiDiffusion
MultiDiffusion \cite{bar2023multidiffusion} freezes the whole parameters of the source diffusion model and optimizes the latent code as a post-processing method to generate the desired output based on a conditioned input.



%----------------------Modulation------------------%
\subsubsection{Modulation}
In signal processing literature, modulation varies some key attributes of a signal to add the desired information to it \cite{oppenheim1997signals}. 
Similarly, in deep neural networks, modulation is used to add some desired information to a base network by adding modulation parameters to the parameters/ features of the base network.
% AdaFM
AdaFMGAN \cite{zhao2020leveraging} shows that layers closer to the sample (earlier layers in $D$, and later layers in $G$) encoder general knowledge.
This general knowledge is conceptually shared between source and target domains and aimed to be preserved by Adaptive Filter Modulation which trains a scale and shift parameter for each $k \times k$ kernel.
GAN-Memory \cite{cong2020ganmemory}, and CAM-GAN \cite{varshney2021camgan} use similar modulation ideas to modulate a pre-trained GAN for generative continual learning.
AdAM \cite{zhao2022adam} uses kernel modulation \cite{milad2021revisit} for few-shot generative modeling by aiming to preserve the important wights of a pre-trained GAN during a few-shot adaptation to a target domain.
HyperDomainNet \cite{alanov2022hyperdomainnet} adds an additional modulation to StyleGAN2 \cite{karras2020analyzing} for adapting to a new domain, while optimizing only $6K$ parameters.

%------------------Adaptation-Aware-------------------%
\subsubsection{Adaptation-Aware}
Adaptation-aware transfer learning approaches propose that different parts of the knowledge encoded on a pre-trained generative model could be important based on the target domain. 
AdAM \cite{zhao2022adam} proposes a probing step before the main adaptation, where the importance of each kernel for adapting a source GAN to the target domain using a few samples is measured using FI.
Then, during the main adaptation, the important kernels are preserved using modulation, and the other kernels are simply fine-tuned.
RICK \cite{zhao2023rick} shows that incompatible knowledge from a source domain to a target domain is related to the kernels with the least importance to this adaptation, and this knowledge can not be removed by simple fine-tuning. Therefore, RICK proposes a dynamic probing schedule during adaptation where it gradually prunes the kernels with the least importance.

%--------------Natural Language-Guided----------------%
\subsubsection{Natural Language-Guided}
Vision-Language models like CLIP \cite{radford2021CLIP} are usually trained on large-scale image-text pairs and learn to encapsulate the generic information by combining image and text modalities.
This generic information is shown to be helpful in various downstream tasks including zero-shot and few-shot image generation.
StyleGAN-NADA (NADA) \cite{gal2022stylegannada} is the pioneering work on utilizing CLIP for zero-shot image generation.
NADA \cite{gal2022stylegannada} uses a text prompt $T_t$ --which describes the target domain-- as input and uses feedback to adapt a pre-trained StyleGAN2 to the target domain.
Specifically, assuming a text prompt $T_s$ that describes the source domain (\eg, "Photo" for a StyleGAN2 pre-trained on the FFHQ), and a given $T_t$ (\eg, "Fernando Botero Painting"), CLIP's text encoder $E_T$ is used to find the update direction in the embedding space: $\Delta T = E_T(T_t) - E_T(T_s)$. 
Similarly, the direction of the update/ change for the images can be computed using generated images with source and target generators: $\Delta I = E_I(G_t(z)) - E_I(G_s(z))$, where $E_I$ denotes CLIP's image encoder.
Since the image and corresponding texts are aligned in the CLIP space, NADA \cite{gal2022stylegannada} proposes to update the generator's parameters in a way to match $\Delta I$ and $\Delta T$ leading to the directional loss $\mathcal{L}_{directional}$:
\begin{equation}
    \label{eq:nada_loss}
    \mathcal{L}_{directional} = 1 - \frac{\Delta I(G_s(z),G_t(z)).\Delta T(T_s,T_t)}{|\Delta I(G_s(z),G_t(z))||\Delta T(T_s,T_t)|}
\end{equation}
This idea can be easily extended to one-shot image generation, by replacing $\Delta T$ with the direction obtained by target image $I_t$ and a batch of generated images by the source generator: $\Delta I' = E_I(I_t) - \mathbb{E}_i \{E_I(G_s(z_i))\}$, where $\mathbb{E}_i \{E_I(G_s(z_i))\}$ denotes the mean of the CLIP embedding for a batch of generated images.
% Mind The GAP (MTG)
MTG \cite{zhu2022mindthegap} extends the idea for one-shot image generation by replacing the mean embedding with the projection of the target image on the source generator denoted as $I^*_s$. Specifically, MTG uses GAN inversion to get the corresponding $z^{ref}$ for $I_t$, and uses it to generate the projected image: $I^*_s=G_s(z^{ref})$. 
% HyperDomainNet
HyperDomainNet improves the performance of the NADA and MTG by freezing the weights of the source generator and training modulation weights for the synthesis part inside the generator.
% DiFa
DiFa \cite{zhang2022difa} adds an attentive style loss to directional loss of NADA \cite {gal2022stylegannada} as a local-level adaptation which aligns the intermediate tokens of the generated image with source and pre-trained GAN.
% OneCLIP
OneCLIP \cite{kwon2022oneclip} exploits the CLIP embedding for three major modules in one-shot learning: i) inverting sample into latent space, ii) preserving the diversity of the GAN during adaptation, and iii) 
% during
a patch-wise contrastive learning approach for preserving local consistency.

% IPL
IPL \cite{guo2023ipl} mentions that NADA \cite{gal2022stylegannada} uses a fixed update direction for a target domain 
% because of using 
due to
manual text prompts being used for describing source and target domains.
To address this, \cite{guo2023ipl} learns a specific prompt for each generated image, \eg, "{\em Elder glass man} photo" $\rightarrow$ "{\em Elder glass man} Fernando Botero Painting".
A mapper function $F$ is used to learn prompts for the generated images $G_s(z_i)$:
\begin{equation}
\label{eq:ipl_prompt}
    F(z_i) = [V]_1^i [V]_2^i ... [V]_m^i
\end{equation}
where $[V]_j^i$ represents the $j^{th}$ prompt vector. Then, the domain embedding $[Y_s]$ is added to this prompt to represent both the prompt and the domain: $M_s^i =F(z_i)[Y_s]$.
The mapper function is trained by contrastive loss in the CLIP space.
After training $F$, the same adaptation process as NADA \cite{gal2022stylegannada} can be used but using $T_s(z_i)=[V]_1^i [V]_2^i ... [V]_m^i [Y_s]$. 

%DreamBooth
DreamBooth \cite{ruiz2023dreambooth} addresses subject-driven sample generation by fine-tuning a text-to-image diffusion model  e.g. Imagen \cite{saharia2022imagen}, DALL-E2 \cite{ramesh2022dalle2}. 
Input images are paired with a text prompt that contains a unique identifier and the subject class (e.g., ``A [V] dog''), and the pair is used to fine-tune the model. 
They further propose a class-specific prior preservation regularizer to encourage diversity and to mitigate {\em language drift}, i.e., the model progressively loses syntactic and semantic knowledge during fine-tuning.
% Textual Inversion
Textual-Inversion \cite{gal2022textualinversion} optimizes a word vector for the new subject given a few images and uses that word vector for the subject-driven generation. 
% MCC
MCC \cite{kumari2023mcc} extends DreamBooth by the capability of adding multiple subjects and improves convergence and performance by restricting fine-tuning to a subset of cross-attention layer parameters in DM.
%BLIP-DIffusion
BLIP-Diffusion \cite{li2023blipdiffusion} uses BLIP-2 \cite{li2023blip} multimodal encoder to extract a more text-aligned representation for each subject, and then a subject representation learning step is performed to enable DM to leverage such representation for fast and high-fidelity subject-driven sample generation. 
%SINE
SINE \cite{zhang2023sine} uses a similar fine-tuning of text-to-image diffusion model with a unique identifier, but at the patch level.
In addition, the mixing of the latent code is also used to edit the subject and put it in a new context. 
%SpecialistDiffusion
SpecialistDiffusion \cite{lu2023specialistdiffusion} addresses adapting the text-to-image models to an unseen style given a few samples from that style using augmenting both text and image, and sparse diffusion for computation efficiency.

%--------------Prompt Tuning---------------%

\subsubsection{Prompt Tuning}
VQ-VAEs (Sec.~\ref{ssec:generative_models}) can be broadly categorized into two types from the perspective of predicting the latent prior of visual tokens. AutoRegressive (AR) approaches like DALL$\cdot$E \cite{ramesh2021dalle} and VQ-GAN \cite{esser2021vqgan}, learn an AR predictor that follows a raster scan order and predicts the visual tokens from left to right, line-by-line. 
Non-AutoRegressive (NAR) approaches like DALL$\cdot$E2 \cite{ramesh2022dalle2}, MaskGIT \cite{chang2022maskgit}, Latent Diffusion \cite{rombach2022latentdiffusion}, or Imagen \cite{saharia2022imagen} usually resort to masking techniques \cite{devlin2019bert} to predict the visual tokens in a series of refinement or denoising steps.
VPT \cite{sohn2023vpt} is the first work that adopts the prompt tuning idea for image generation with generative knowledge transfer.
It uses a VQ-VAE framework where a MaskGIT\cite{chang2022maskgit}/ VQ-GAN\cite{esser2021vqgan} on the ImageNet dataset (as an example of NAR/ AR approach) is used as a pre-trained network.
Then, during adaptation, all the parameters of the VQ Encoder, VQ decoder, and transformer are frozen, and a generator is learned to minimize the adaptation loss by generating and appending a set of visual tokens to the predicted prior.
These visual tokens guide the generation process for the target domain by helping the transformer to predict proper tokens to the VQ decoder.


%%%%%%%%%%%%%%-----AUGMENTATION-----%%%%%%%%%%%%%%
\subsection{Data Augmentation}
\label{ssec:review_dataaugmentation}
Data augmentation increases the quantity and diversity of the training data  
which is shown to be beneficial for GM-DC.
However, if it is not deployed correctly, augmentations can leak into the generator resulting in generating samples with similar augmentations, \eg noisy or rotated images, which is undesirable.


%--------------Image-Level Augmentation--------------
\subsubsection{Image-Level Augmentation}
CR-GAN \cite{Zhang2020Consistency} and bCR \cite{zhao2021improved} apply various transformations on images and enforce the output of the generator to be the same for original and transformed images.
Even though not developed for GM-DC, experimental results in \cite{karras2020ada} show that CR-GAN and bCR are beneficial for limited data scenarios.
ADA proposes applying the transformations to real and fake images but with a probability $p < 1$.
The central design in ADA \cite{karras2020ada} is that the strength of the augmentation ($p$) is being adapted based on the training dynamics.
Specifically, the portion of the real images that get positive output from the discriminator, \ie, $r = \mathbb{E}[Sign(D)]$, is used as an indicator of the discriminator overfitting ($r=0$ no overfitting, and $r=1$ complete overfitting).
Then, during training, $p$ is adjusted to keep $r$ low.
DiffAugment \cite{zhao2020diffaug}, and IAG \cite{zhao2020imageaugmentation} use a similar idea to ADA, but without the adaptive component ($p=1$).
APA \cite{jiang2021deceive} uses the same adaptive augmentation mechanism in ADA, but instead of using transformations like rotation, it randomly labels generated images as pseudo-real ones to prevent an overconfident discriminator.

%DiffusionGAN
DiffusionGAN \cite{wang2023diffusiongan} applies the gradual diffusion process on real and generated images during training GAN. 
Training starts with real and generated images, and each diffusion step is applied after a certain number of training epochs making the bi-classification task harder for the discriminator to prevent its overfitting.
% PatchDiffusion
PatchDiffusion \cite{wang2023patchdiffusion} 
augments the data during training diffusion models by sampling patches with random locations and random sizes alongside the full image and conditioning the denoising score function \cite{karras2022elucidating} on the patch size and the location information. 


%--------------Feature-Level Augmentation--------------
\subsubsection{Feature-Level Augmentation}
AdvAug \cite{chen2021advaug} computes the adversarial perturbation $\delta$ for the feature maps of the discriminator and generator using the projected gradient descent \cite{madry2018towards}. Denoting the discriminator as $D = D_2 \circ D_1$, the adversarial augmentation is applied on the intermediate feature maps ($D_1$) of both real and generated images, resulting in $D_{1}(x) + {\delta}$, and $D_{1}(G(z)) + {\delta}$. The adversarial loss is then added to the loss function of $D$ during GAN training to maximize the score of the perturbed real image and minimize the score of the perturbed generated image:
\begin{equation}
    \mathcal{L}_{D}^{adv} \coloneqq \max_{\| \delta \|_{\infty} < \epsilon} \mathbb{E}_{x \sim p_{data}} [f_{D}(-D_2(D_1(x)+\delta))] 
    +
    \max_{\| \hat{\delta} \|_{\infty} < \epsilon} \mathbb{E}_{z \sim p_{z}} [f_{D}(D_2(D_1(G(z))+\hat{\delta}))] 
\end{equation}
As AdvAug is performed on the feature level, it is shown to be complementary to image-level augmentations like ADA \cite{karras2020ada} and DiffAug \cite{zhao2020diffaug}.
AFI \cite{dai2021implicit} observes a flattening effect in discriminators with multiple output neurons, and takes advantage of this observation by proposing feature interpolation as implicit data augmentation.

%--------------Transformation Information--------------
\subsubsection{Transformation-Driven Design}
DAG \cite{tran2021dag} uses a separate discriminator $D_k$ for discriminating real and fake images that are augmented by transformation $T_k$. 
A weight-sharing mechanism between all discriminators is used to prevent overfitting.
Additionally, DAG provides a theoretical ground for training convergence under augmentation. As mentioned in \cite{goodfellow2014GANs}, for an optimal discriminator $D^*$, optimizing $G$ is equivalent to minimizing the Jensen-Shannon (JS) divergence between the real data distribution $P_{data}$ and generated data distribution $P_{model}$, \ie, $JS(P_{data}, P_{model})$. 
Denoting $P_{data}^{T}$, and $P_{model}^{T}$ as the distribution of the real and generated data under augmentation $T$, \cite{tran2021dag} shows that JS divergence between two distributions is invariant under differentiable and invertible transformations:
\begin{equation}
    \label{eq:JS_invariant}
    JS(P_{data}, P_{g}) = JS(P_{data}^{T}, P_{g}^{T})
\end{equation}
This means that as long as the augmentation is differentiable and invertible, training convergence is guaranteed.
SSGAN-LA \cite{hou2021labelaugmentation} extends DAG by merging all discriminators to a single discriminator and augmenting the label space of the discriminator, \ie, asking $D$ to detect the type of augmentation in addition to conventional real/ fake detection.

%%%%%%%%%%%%%%-----Network Architecture-----%%%%%%%%%%%%%%
\subsection{Network Architectures}
\label{ssec:review_networkarchitecture}

\subsubsection{Feature Enhancement}
FastGAN \cite{liu2021fastgan} proposes a light-weight GAN structure ---shallower $G$ and $D$ compared to SOTA GANs like StyleGAN2--- to decrease the risk of overfitting.
Inspired by skip connections \cite{he2016resnet}, and squeeze-and-excitation module \cite{hu2018squeeze}, FastGAN fuses features with different resolutions in $G$ through proposed skip-layer excitation modules.
An additional reconstruction task is defined for $D$.
MoCA \cite{li2022moca} learns some prototypes for each semantic concept within a domain, \eg, railroad, or sky in a photo of a train.
Then, by attending to these prototypes during image generation, some residual feature maps are produced to improve image generation.
DFSGAN \cite{yang2023dfsgan} proposes to preserve the content and layout information in intermediate layers of $G$ by extracting channel-wise and pixel-wise information and using them to scale corresponding feature maps.
SCHA-VAE \cite{pmlr-v162-giannone22a} extend  latent variable models for sets to a fully hierarchical approach and propose Set-Context-Hierarchical-Aggregation VAE for few-shot generation. 


\subsubsection{Ensemble Pre-trained Vision Models}
ProjectedGAN \cite{sauer2021projectedgan} proposes to project real and generated images into the feature space of a pre-trained vision model to enhance $D$'s performance in discriminating real and fake images by adding two modules.
First, the output from multiple layers is used with separate discriminators.
Then a {\em random projection} is used to dilute the features and encourage the discriminator to focus on a subset of the features.
Vision-aided GAN \cite{kumari2022ensembling} uses an ensemble of the original discriminator $D$ and additional discriminators $\{ \hat{D}_n \}_{n=1}^{N}$ to perform the classification task.
The additional discriminators $\{ \hat{D}_n \}_{n=1}^{N}$ have a set of pre-trained feature extractors $\mathcal{F} = \{ F_n \}_{n=1}^{N}$ (extracted form pre-trained vision models) with a small trainable head $C_n$ added on top: $\hat{D}_n =  F_n \circ C_n$. 


\subsubsection{Dynamic Network Architecture}
CbC \cite{shahbazi2022collapse} shows that under data constraints, 
where an unconditional GAN can generate satisfactory performance, training the 
conditional GANs (cGANs) result in mode collapse. 
To mitigate this issue, CbC \cite{shahbazi2022collapse} starts training from an unconditional GAN and slowly transitions to a cGAN using a transition function $0 \leq \lambda_t \leq 1$.
Considering the conditioning variable as $c$, this transition is implemented in $G$ as $G(z,c,\lambda_t)=G(S(z)+\lambda_t.E(c))$, with $S$ and $E$ as neural networks that transform the latent code and the conditioning variable.
DynamicD \cite{yang2022dynamicd} dynamically reduces the capacity of $D$ by randomly sampling a subset of channels of $D$ during each training iteration to prevent overfitting.
Inspired by the lottery ticket hypothesis \cite{frankle2019lotteryticket}, AdvAug \cite{chen2021advaug} and Re-GAN \cite{saxena2023regan} have shown that a much sparse subnetwork of the original generator can be useful for GM-DC.
AutoInfoGAN \cite{shi2023autoinfogan} applies a reinforcement learning-based neural architecture search to find the best network architecture for the generator.


%%%%%%%%%%%%%%-----Multi-Task Objectives-----%%%%%%%%%%%%%%

\subsection{
Multi-Task Objectives
}
\label{ssec:review_trainingtechniques}

\subsubsection{Regularizer} 
LeCam \cite{tseng2021lecam} uses two moving average values to track $D$'s prediction for real and generated images, denoted by $\alpha_R$ and $\alpha_F$, respectively.
Then the distance between the $D$'s prediction for real (fake) images and $\alpha_F$ ($\alpha_R$) is decreased by adding a regularizer to prevent overfitting. 
Analysis in \cite{tseng2021lecam} shows that this regularizer enforces WGAN \cite{arjovsky2017wasserstein}/ BigGAN\cite{brock2019biggan} to minimize the LeCam-divergence which is beneficial for GM-DC.
Reg-LA \cite{hou2023regularizing} uses a similar idea to regularize the label-augmented GANs discussed in Sec. \ref{ssec:review_dataaugmentation}.
DIG \cite{fang2022diggan} shows that the discriminator gradient gap between real and generated images increases when training GANs with limited data, and adds this gap as a regularizer to prevent this behavior.
MDL \cite{kong2022mdl} addresses the pre-training free few-shot image generation by adding a regularizer that aims to keep the similarities between the latent codes in $\mathcal{Z}$ space and corresponding generated images in image space.

\subsubsection{Contrastive Learning}
InsGen \cite{yang2021insgen} uses contrastive learning to improve learning $D$ by introducing a pretext task.
The pretext task is defined as instance discrimination, meaning that each sample should be mapped to a separate class. This is done by constructing the query and key from the same sample as positive pair, and all remaining images as negative pair. 
FakeCLR \cite{li2022fakeclr}  
analyze three different contrastive learning strategies, namely instance-real,  instance-fake, and instance-perturbation. It is shown that instance-perturbation contributes the most improvement in quality and can effectively alleviate the issue of latent space discontinuity.
Note that contrastive learning is also combined with other approaches as discussed before (C\textsuperscript{3} \cite{Lee2021C3}, DCL \cite{zhao2022dcl}, CtlGAN \cite{wang2022ctlgan}, CML-GAN \cite{phaphuangwittayakul2022cmlgan}, and IAG \cite{zhao2020imageaugmentation}).

\subsubsection{Masking}
MaskedGAN \cite{huang2022maskedgan} utilizes a masking idea for training GANs under limited data by masking both spatial and spectral information.
For spatial masking, they use a patch-based mask to enable random masking of all spatial parts. For spectral masking, they mask each frequency channel (extracted by Fourier transform) based on the amount of information, \ie, channels with more information are more probable to be masked.
MaskD \cite{zhu2022few} randomly masking feature maps extracted by $D$ for a few-shot setup.
DMD \cite{zhang2023dmd} detects that the discriminator slows down learning and applies random masking to its features adaptively to balance its learning pace with the generator.


\subsubsection{Knowledge Distillation}
KD-DLGAN \cite{cui2023kddlgan} proposes a knowledge distillation (KD) \cite{hinton-distill,chandrasegaran2022revisiting} approach by leveraging CLIP \cite{radford2021CLIP} as the teacher model  to distill text-image knowledge  to the discriminator.
They propose two designs: aggregated generative knowledge designs a harder learning task, and correlated generative knowledge distillation improves the generation diversity by distilling and preserving the diverse
image-text correlation from CLIP.
As discussed before, KDFSIG \cite{hou2022exploitingkd} also uses KD in the context of transfer learning for few-shot image generation.

\subsubsection{Prototype Learning}
Inspired by the recent success of learning prototypes in few-shot classification, ProtoGAN \cite{yang2023protogan}, aims to improve the fidelity and diversity of the FastGAN under limited data\cite{snell2017prototypical}.
ProtoGAN has two main modules: prototype alignment for increasing the fidelity of the generated images, and diversity loss to improve the generation diversity.
MoCA \cite{li2022moca} also learns prototypes but for different semantic concepts through an attend and replace mechanism on the extracted feature maps of $G$.

\subsubsection{Other Multi-Task Objectives}
Gen-Co \cite{cui2022genco} uses multiple discriminators to extract diverse and complementary information from samples.
This {\em co-training} has two major modules: weight-discrepancy co-training which trains separate 
$D$s with different weights, and data-discrepancy co-training 
which in addition to training separate $D$s also uses different information as inputs, \ie, spatial or frequency information.
AdaptiveIMLE \cite{aghabozorgi2023adaptiveimle} proposes an adaptive version of implicit maximum likelihood estimation \cite{li2018imle} to improve the mode coverage by assigning different boundary radii for each sample. 
PathcDiffusion \cite{wang2023patchdiffusion} and AnyResGAN \cite{chai2022anyresolution} show the effectiveness of {\em Patch-Level} learning of the generators.
Diffusion-GAN \cite{wang2023diffusiongan} leverages the {\em diffusion process} to improve training GANs by gradually increasing the task hardness for $D$.  D2C \cite{sinha2021d2c} uses a DM to improve the sampling process of VAEs by denoising the latent codes and feeding VAE with a clean latent code for sample generation.
FSDM \cite{giannone2022fsdm} uses an attentive conditioning mechanism and aggregates image patch information using a vision transformer for image generation for unseen classes.


\subsection{Exploiting Frequency Components}
\label{ssec:review_exploitingfrequency}
Approaches in this category aim to improve frequency awareness to improve GM-DC.
FreGAN \cite{yang2022fregan} extracts high-frequency information 
($HF$)
of images (related to details in images) using Haar Wavelet transform \cite{porwik2004haar} and uses three different modules to emphasize learning high-frequency information: high-frequency discriminator uses $HF$ as an additional signal to perform real/fake classification, frequency skip connection feeds the $HF$ information of each feature map to the next one in $G$ to prevent frequency loss, and a frequency alignment loss is used to make sure $G$ and $D$ are learning frequency information in the same pace.
WaveGAN \cite{yang2022wavegan} uses similar idea, but in a different setup to address task cGM-2.
Gen-Co extracts some frequency information of the image and feeds it to a separate $D$ in addition to using original real and fake images.
MaskedGAN \cite{huang2022maskedgan} masks out some frequency bands of the input during training to enforce the generative model to focus more on under-represented frequency bands.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%Meta-Learning%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Meta-Learning}
\label{ssec:review_metalearning}
Meta-learning shifts the learning paradigm from data level to task level to capture across-task knowledge as {\em meta-knowledge}, and then adapt this meta-knowledge to improve the learning process of unseen tasks in the future. 
An abundant of recent works adopt meta-learning to tackle few-shot classification \cite{finn2017maml, snell2017prototypical, vinyals2016matching, sung2018relationnet, milad2021revisit} and few-shot semantic segmentation \cite{wang2019panet}.
These works usually follow the {\em episodic learning} setup which matches the way that model is trained and tested.
Considering task distribution $P_{\mathcal{T}}$, a set of training tasks are constructed from seen classes $\mathcal{T}^{train} = \{ \mathcal{T}^{train}_i \}$, where $\mathcal{T}^{train}_i$ denotes $i^{th}$ training (meta-training) task. 
The model is trained on the meta-training tasks and later tested on the meta-test tasks $\mathcal{T}^{test} = \{ \mathcal{T}^{test}_j \}$ constructed from unseen classes. Usually meta-training and meta-testing tasks follow the same distribution $P_{\mathcal{T}}$.
Similarly, the approaches in this category use meta-learning to address image generation: train a generative model on a set of few-shot image generation tasks constructed from seen classes of a domain, then, test it on the few-shot image generation tasks from unseen classes of the same domain.

\subsubsection{Optimization} 
Optimization-based meta-learning algorithms are used in these approaches for learning meta-knowledge.  
Generative Matching Network (GMN) proposes a similar attention mechanism used in Matching Networks \cite{vinyals2016matching} for few-shot image generation with variational inference. 
FIGR \cite{clouatre2019figr} meta-trains a GAN using Reptile \cite{nichol2018reptile}.
Training has an inner loop that adapts the GAN weights based on a few-shot image generation task and an outer loop that updates the meta-knowledge using Reptile.
Dawson \cite{liang2020dawson} modifies the inner loop training to directly get the gradients for the generator from evaluation data.
FAML \cite{phaphuangwittayakul2021faml} uses a similar idea to FIGR, but instead of using the standard GAN structure, it uses an encoder-decoder architecture for the generator.
CML-GAN \cite{phaphuangwittayakul2022cmlgan} extends FAML \cite{phaphuangwittayakul2021faml} by leveraging contrastive learning to learn quality representations.


\subsubsection{Fusion} 
MatchingGAN \cite{hong2020matchinggan} learns to generate new images for a category by fusing the available images of that category.
A set of encoders are used to estimate the similarity between the embedding of the latent code and input images. Then, these similarities are used as interpolation coefficients by an auto-encoder to extract the embeddings of the training images and fuse them for generating new images.
F2GAN \cite{hong2020f2gan} uses random coefficients for general information, and attention module for details. 
The attention module takes the weighted average of the real image features and the corresponding features from the decoder to produce the image details.
LoFGAN \cite{gu2021lofgan} focuses on local features in the fusion process. 
Given a batch of images, one sample is selected as a base while the remaining are utilized as a reference set. This set acts as a feature bank for the fusing process.
WaveGAN \cite{yang2022wavegan} adds frequency awareness to LofGAN by extracting and feeding the frequency components of feature maps to later layers of the generator.
AMMGAN \cite{li2023ammgan} utilizes an adaptive fusion mechanism for learning pixel-wise metric coefficients during the fusion.

\subsubsection{Transformation}
DAGAN \cite{antoniou2017dagan} leverages the task of the learning augmentation manifold in the learning process of the GAN.
This is modeled as some transformations on the input, and these transformations are applied to the new sample from the unseen classes for sample generation. 
DeltaGAN \cite{hong2022deltagan} learns the difference between images (delta) in the feature space, and then uses this delta concept for diverse sample generation. 
Disco \cite{hong2022disco} learns a dictionary based on seen images to encode input images into visual tokens.
These tokens are then fed into the decoder with the style embedding of seen images to generate images from unseen classes.
AGE \cite{ding2022age} uses GAN inversion to invert the samples of a category to $\mathcal{W}^+$ space of StyleGAN2 \cite{karras2020analyzing}. The mean latent code for all samples of a category is used as a prototype and all differences are considered as general attributes.
These attributes are then used to diversify sample generation for unseen classes.
SAGE \cite{ding2023sage} addresses the class inconsistency in AGE by taking all given samples from unseen classes into account during inference.
HAE \cite{li2022hae} uses a similar idea to AGE \cite{ding2022age}, but in the Hyperbolic space instead of using Euclidian distance which allows more semantic diversity control.
LSO \cite{zheng2023lso} finds a prototype for each class similar to AGE \cite{ding2022age}. Then it adjusts GAN to produce similar images to target samples using latent samples from the neighborhood of the prototype, followed by updating the prototype in latent space using the adapted GAN.



%%%%%%%%%%%%%%%%%%%%INTERNAL PATCH DISTRIBUTION%%%%%%%%%%%%%%%%%%
\subsection{Modeling Internal Patch Distribution}
\label{ssec:review_internalpatch}

\subsubsection{Progressive Training}
SinGAN \cite{shaham2019singan} is the pioneering work that makes use of the internal distribution of the patches within an image to train a generative model.
It trains a pyramid of generators $\{G_0, \dots, G_N \}$ against a pyramid of real images $\{x_0, \dots, x_N \}$, where $x_n$ is a downsampled version of input image $x$ by a factor of $r^n$.
The generator at scale $n$ uses random noise $z_n$ and upsampled version of the generated image from the lower resolution $\tilde{x}_{n+1}$ as input:
$\tilde{x}_{n} = G_n(z_n, (\tilde{x}_{n+1})\uparrow^{r_{n}})$.
Similarly, a pyramid of discriminators is used where $D_n$ compares the $\tilde{x}_{n}$ and $x_{n}$ in patch-level for real-fake classification.
SinDDM \cite{kulikov2023sinddm} applies the same idea but uses diffusion models with a fully convolutional lightweight denoiser.
ConSinGAN \cite{hinz2021consingan} stacks the new layers for a bigger scale on top of the previous layers used for a smaller scale instead of using separate generators for each scale. 
BlendGAN \cite{kligvasser2022blendgan} and DEff-GAN \cite{kumar2023deffGAN} extend previous approaches for learning the internal distribution for $k$ images, thereby allowing for the potential mixing of different image semantics and improving diversity.
SinDiffusion \cite{wang2022sindiffusion} addresses artifacts in SinGAN due to progressive resolution growth by applying progressive denoising using a diffusion model architecture.


\subsubsection{Non-Progressive Training}
One-Shot GAN \cite{sushko2021oneshotgan} uses a standard generator (single-scale), but multiple paths for the discriminator to enforce learning objects' appearance and how to combine them.
Within the discriminator the low-level loss is defined on low-level features and two different losses are defined to learn the content and the layout in image patches.
SinFusion \cite{nikankin2022sinfusion}  explores learning the internal patch distribution from both a single image and video. 
SinFusion extends on DPPM \cite{ho2020denoising} and reduces the size of the receptive fields by first removing attention layers, then adopting ConvNext \cite{liu2022convnet} blocks in the U-Net \cite{ronneberger2015unet} architecture. 
To reconstruct videos, a series of images are fed into a series of 3 identical models. The first model predicts the next frame; the second model denoises and removes small artifacts from the generated images; the last model interpolates between the different frames.





