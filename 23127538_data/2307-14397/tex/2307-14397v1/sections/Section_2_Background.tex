\section{Background}
\label{sec:background}

In this section, we first define `domain' and `generative modeling', then we  discuss common approaches of generative modeling and  data constraints studied in 
GM-DC.


\noindent
{\bf Domain.} In this survey, a {\em domain} consists of two components: i) a sample space $\mathcal{X}$, and ii) a marginal probability distribution $P_{data}$, which models the probability of samples from $\mathcal{X}$ \cite{pan2009yang-qiang-transfer}.
This is written as $\mathcal{D}=\{\mathcal{X},P_{data} \}$,
and $x\sim P_{data} \in \mathcal{X}$ denoting a sample in this space. 
An example of a domain is the domain of image of human faces: $\mathcal{D}^{h}=\{\mathcal{X},P_{data}^{h} \}$.
Here $\mathcal{X}$ is the sample space of images, and $P_{data}^{h}$ is the probability distribution of human faces.
\vspace{0.1cm}


\noindent
{\bf Generative Modeling.}
Given a set of training sample $x$ of 
a domain 
$\mathcal{D}=\{\mathcal{X},P_{data} \}$, i.e., 
with an underlying probability distribution $P_{data}$, 
generative modeling  aims to learn to  capture $P_{data}$ ---sometimes also denoted as $P(x)$ in literature.
The result of generative modeling is a 
{\em generative model} $G$ encoding 
a probability distribution $P_{model}$.
The learning objective is to have $P_{model}$ similar to  $P_{data}$ statistically.
After the training, $G$ can generate samples following 
$P_{model}$.
For example, generative modeling with a training set of  human face images 
aims to learn to capture $P_{data}^{h}$, thereby the resulting $G^{h}$ can generate human face images
that are statistically similar to samples from $P_{data}^{h}$.
We also refer to the domain of training samples as {\em target domain}.


\vspace{0.1cm}
\noindent
{\bf Conditional vs Unconditional Sample Generation.}
After learning the underlying distribution of data $P_{data}$, the generative model can generate new samples by sampling from the learned distribution $P_{model}$.
Typically, generation starts with sampling a random vector $z$ ---also called latent code--- as input. Then, this input is passed into the generative model $G$ to transform the latent code into a new sample $G(z) \sim P_{model}$.
Ideally, a good generator is able to capture  the characteristics, quality and diversity of the training dataset, \ie, $P_{model}$ is similar to  $P_{data}$ statistically.
If an additional condition $c$ (like a class label or attribute) is used alongside with the latent code to steer the sample generation towards $c$, the sample generation is called conditional generation:  $G(z,c)$.



% Figure environment removed
%\vspace{-0.3cm}

\subsection{Approaches for Generative Modeling}
\label{ssec:generative_models}


Earlier works on generative models  
study
Gaussian Mixture Models \cite{reynolds2009gaussian}, Hidden Markov Models 
\cite{phung2005topic}, 
Latent Dirichlet Allocation 
\cite{chauhan2021topic}
and Boltzmann Machines \cite{ackley1985learning}.
With the introduction of deep neural networks, recent works study  powerful generative models,
particularly those for image generation, which most GM-DC works focus on.

\vspace{0.1cm}
\noindent
{\bf Variational Auto Encoders (VAE)} 
\cite{pouyanfar2018survey}.
VAE is a variant of Auto-Encoder (AE) \cite{zhai2018autoencoder}, where both consist of the encoder and decoder networks. AE focuses on dimensional reduction.
The encoder in AE learns to map an input $x$ into a latent (compressed) representation, $z=E(x)$.
Then, the decoder aims to reconstruct the image from that latent representation, $\hat{x}=D(z)$. 
Model parameters are optimized with the following reconstruction loss:
\begin{equation}
    \mathcal{L}_{rec}=||x-D(z)||_2
\end{equation}
AEs are notorious for latent space irregularity making them improper for sample generation \cite{kingma2019introduction}. VAE aims to address this problem by enforcing $E$ to return a normal distribution over latent space.
Assuming a distribution $z\sim \mathcal{N}(\mu,\sigma^2)$ for latent space, this is done by adding the KL-divergence term to the loss function:
\begin{equation}
    \mathcal{L}=||x-D(z)||_2 + KL(\mathcal{N}(\mu,\sigma^2), \mathcal{N}(0,I))
\end{equation} 
Due to the challenges of direct maximization of the likelihood in pixel space, Vector-Quantized VAE (VQ-VAE) 
proposes {\em tokenization} where a codebook $\mathbf{e}_k$, $k \in 1, \dots, K$ is used to quantize the embeddings $E(x)$ into visual tokens (indices), acting like a lookup table.
In addition, a latent prior of the visual tokens is predicted (usually using a transformer), and the decoder is modified to map the visual tokens into the image space.

\begin{table}[t]
    \centering
    \fontsize{7pt}{7pt}
    \selectfont
    \vspace{-0.4cm}
    \caption{List of common datasets used in GM-DC works. 
    Number of samples (\# Samples) refers to the sample size of the entire dataset. In GM-DC experiments, usually, only a subset of the dataset is used.
    We remark that \xmark/\cmark denotes the absence/presence of the dataset under the data constraint settings: {\bf LD}: \underline{L}imited-\underline{D}ata, {\bf FS}: \underline{F}ew-\underline{S}hot and {\bf ZS}: \underline{Z}ero-\underline{S}hot, and Labels indicate if training labels are available (but not necessarily used).
    }
    \label{tab:datasets}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cc c@{\hspace{3pt}}c @{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}c}
    \toprule
        {\bf Dataset} &  {\bf Description} & 
        {\bf \# Samples} &
        {\bf Resolution} &
        {\bf LD}  & {\bf FS} & {\bf ZS}
        & {\bf Labels} \\
       \midrule
        \arrayrulecolor{black!10}
        %1--------------------------------------------------------------
        \makecell[c]{Flickr-Faces-HQ\\ (FFHQ) \cite{karras2019style}} & 
        \parbox[l]{0.45\linewidth}
        {Images with human faces, containing variation in terms of age, ethnicity, and image background.}
        & 70K
        & 1024$\times$1024
        &
        \cmark & \cmark & \cmark &        
        \xmark
        \\ \midrule
         %2--------------------------------------------------------------
        \makecell[c]{Large-scale Scene\\ Understanding (LSUN) \cite{yu2015lsun}  }       &
        \parbox[l]{0.45\linewidth}
          {
        Images with large-scale scene containing 10 scene and 20 object categories.
        }
        &
        3M
        &
        256$\times$256 &
        \cmark & \cmark & \xmark
        &
        \cmark
        \\ \midrule
        %3--------------------------------------------------------------
        MetFace \cite{karras2020ada}            & 
        \parbox[l]{0.45\linewidth}
        {Images depicting paintings, drawings, and statues of human faces 
        } 
        &
        1336 & 1024$\times$1024 &
        \cmark & \cmark & \xmark
        &
        \xmark
        \\ \midrule
        %4--------------------------------------------------------------
        BreCaHAD \cite{aksac2019brecahad}           &
        \parbox[l]{0.45\linewidth}
        {Images of breast cancer histopathology.
        }
        &
        162 & 1360$\times$1024 &
        \cmark & \xmark & \xmark
        &
        \xmark
        \\ \midrule
        %5--------------------------------------------------------------
        \makecell[c]{Animal FacesHQ\\(AFHQ) \cite{choi2020starganv2}} &
        \parbox[l]{0.45\linewidth}
        {Images of animal faces in the domains of cat, dog, and wildlife.}
        &
        15K & 512$\times$512 &
        \cmark & \cmark & \xmark &
        \xmark
        \\ \midrule
        %6--------------------------------------------------------------
        CIFAR-10 \cite{krizhevsky2009cifar100}           &
        \parbox[l]{0.45\linewidth}
        {
        Images including objects and animals.
        }
        &
        60K & 32$\times$32 &
        \cmark & \xmark & \xmark &
        \cmark
        \\ \midrule
        %7--------------------------------------------
        CIFAR-100 \cite{krizhevsky2009cifar100}            & 
        \parbox[l]{0.45\linewidth}{
        A dataset similar to CIFAR-10, but with 100 classes}
        &
        60K & 32$\times$32 &
        \cmark & \xmark & \xmark 
        &
        \cmark
        \\ \midrule
        %8--------------------------------------------
        \makecell[c]{100-shot Obama/\\Gumpy Cat/Panda \cite{zhao2020diffaug}}      &
        \parbox[l]{0.45\linewidth}{Colored images of Obama/Gumpy Cat/Panda}
        &
        100 & 256$\times$256 &
        \cmark & \xmark & \xmark 
        &
        \xmark
        \\\midrule
        %11--------------------------------------------
        Sketches \cite{wang2008faceSketches}            & 
        \parbox[l]{0.45\linewidth}
        {Face sketches in frontal pose, normal lighting, and neutral expressions}
        &
        606 & 256$\times$256 &
        \xmark & \cmark & \xmark 
        &
        \xmark
        \\\midrule
        %12--------------------------------------------
        Sunglasses \cite{ojha2021cdc}         &
        \parbox[l]{0.45\linewidth}
        {Images of human faces wearing sunglasses.
        }
        & 2700 & 256$\times$256 &
        \xmark & \cmark & \xmark 
        &
        \xmark
        \\\midrule
        %13--------------------------------------------
        Babies \cite{ojha2021cdc}             &
        \parbox[l]{0.45\linewidth}
        {Images of baby faces.}
        &
        2500 & 256$\times$256 &
        \xmark & \cmark & \xmark 
        &
        \xmark
        \\\midrule
        %14--------------------------------------------
        \makecell[l]{Artistic-Faces \cite{yaniv2019face}} &
        \parbox[l]{0.45\linewidth}
        {Images containing 160 artistic portraits of 16 different artists.}
        &
        160 & 256$\times$256  &
        \xmark & \cmark & \xmark 
        &
        \xmark
        \\\midrule
        %15--------------------------------------------
        Haunted houses \cite{yu2015lsun}       &
        \parbox[l]{0.45\linewidth}
        {Images of haunted houses}
        &
        1K & 256$\times$256 &
        \xmark & \cmark & \xmark 
        &
        \xmark
        \\\midrule
        %16--------------------------------------------
        Wrecked cars \cite{yu2015lsun}       &
        \parbox[l]{0.45\linewidth}
        {Images of wrecked cars}
        &
        1K & 256$\times$256 &
        \xmark & \cmark & \xmark 
        &
        \xmark
        \\
        \arrayrulecolor{black!100}
        \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
\end{table}




\begin{table}[!htbp]
    \centering
%\small
\fontsize{7pt}{7pt}
\selectfont
\caption{{\bf Our proposed taxonomy for tasks in GM-DC}. For each task, we extract their key characteristics.  
[Attributes] {\bf C}: \underline{C}onditional generation, {\bf P}: \underline{P}re-trained generator given, {\bf I}: \underline{I}mages (as input), {\bf TP}: \underline{T}ext-\underline{P}rompt (as input), {\bf X}: \underline{X}(Cross)-domain adaptation; [Data Constraint] {\bf LD}: \underline{L}imited-\underline{D}ata, {\bf FS}: \underline{F}ew-\underline{S}hot, {\bf ZS}: \underline{Z}ero-\underline{S}hot.
\xmark/\cmark denotes the absence/presence, respectively.
Best viewed in color.
}
\label{tab:tasktaxonomy}
\resizebox{\linewidth}{!}{
\begin{tabular}{
c
c@{\hspace{15pt}}c@{\hspace{15pt}}c@{\hspace{15pt}}c@{\hspace{15pt}}c 
@{\hspace{25pt}}
c@{\hspace{15pt}}c@{\hspace{15pt}}c
c}
\toprule 
\makecell[c]{\bf Task} & \multicolumn{5}{c}{\bf Attributes} & \multicolumn{3}{c}{\bf Data Constraint} & \makecell[c]{\bf Task Illustration}\\
\cmidrule(lr){1-1}\cmidrule(lr){2-6}\cmidrule(lr){7-9}\cmidrule(lr){10-10}
%Headers
 %Attributes (Names)
     &{\bf C}
     &{\bf P}
     &{\bf I}
     &{\bf TP}
     &{\bf X}
 %Context
    &{\bf LD}
    &{\bf FS}
    &{\bf ZS}
 & \\
    \midrule
%Task 1
    %Row 1-----------------------------
        %Task Name
        \multirow{3}{*}{uGM-1} 
        &
        %Attributes (row 1)
        \xmark & \xmark & \cmark & \xmark & \xmark &
        \cmark &\xmark & \xmark
        &
        %Example (Fig)
        \multirow{3}{*}{
        \includegraphics[width=0.48\textwidth, clip, 
        trim={88pt 865pt 1099pt 60pt},]
        {figures/Tasks_v8_1.pdf}
        }
    \\
    \cmidrule(lr){2-9}
    \vspace{3pt}
    %Row 2-----------------------------
    & 
    %Description
    \multicolumn{8}{p{6cm}}{
    \textbf{Description:} Given $K$ samples from a domain $\D$,
    learn to generate diverse and  high-quality samples
    from $\D$} 
    \\
    %Row 3-----------------------------
    &
    %Examole
    \multicolumn{8}{p{6cm}}{
    \textbf{Example:} ADA \cite{karras2020ada} learns a StyleGAN2 using 1k
    images from AFHQ-Dog}  
    \vspace{3pt}
    \\
    \midrule
%Task 2
    %Row 1-----------------------------
        %Task Name
        \multirow{3}{*}{uGM-2}
        &
        %Attributes (row 1)
        \xmark & \cmark & \cmark & \xmark &\cmark &
        \cmark & \cmark & \xmark &
        %Example (Fig)]
    %Row 2-----------------------------
        \multirow{3}{*}{
        \includegraphics[width=0.48\textwidth, clip, 
        trim={88pt 535pt 1099pt 338pt},]
        {figures/Tasks_v8_1.pdf}
        }
    \vspace{1pt}
    \\
    \cmidrule(lr){2-9}
    \vspace{1pt}
    %Row 2-----------------------------
    & 
    %Description
    \multicolumn{8}{p{6cm}}{
    \textbf{Description:} Given a pre-trained generator on a 
    source domain $\Ds$ and $K$ samples from a target
    domain $\Dt$, learn to generate diverse and
    high-quality samples from $\Dt$
    }
    \\
    %\cmidrule(lr){2-9}
    %Row 3-----------------------------
    &
    %Example
    \multicolumn{8}{p{6cm}}{
    \textbf{Example:}
    CDC \cite{ojha2021cdc} adapts a pre-trained 
    GAN on FFHQ to Sketches using 10 samples}
    \vspace{2pt}
    \\
    \midrule
%Task 3
    %Row 1-----------------------------
        %Task Name
        \multirow{3}{*}{uGM-3} 
        &
        %Attributes (row 1)
        \xmark & \cmark & \xmark & \cmark & \cmark &
        \xmark & \xmark & \cmark &
        %Example (Fig)
        \multirow{3}{*}{
        \includegraphics[width=0.48\textwidth, clip, 
        trim={88pt 113pt 1099pt 698pt},]
        {figures/Tasks_v8_1.pdf}
        }
    \\
    \cmidrule(lr){2-9}
    \vspace{1pt}
    %Row 2-----------------------------
    & 
    %Description
    \multicolumn{8}{p{6cm}}{
    \textbf{Description:}  Given a pre-trained 
    generator on a 
    source domain $\Ds$
    and a text prompt 
    describing a target 
    domain $\Dt$, learn 
    to generate diverse 
    and high-quality 
    samples from $\Dt$
    }
    \vspace{2pt}
    \\
    %\cmidrule(lr){2-9}
    %Row 3-----------------------------
    &
    %Example
    \multicolumn{8}{p{6cm}}{
    %\textit{E.g.,} 
    \textbf{Example:}
    NADA \cite{gal2022stylegannada}
    adapts pre-trained GAN on 
    FFHQ to the 
    painting domain 
    using {\em `Fernando 
    Botero
    Painting'} as input}
    \\
    \midrule
%Task 4
    %Row 1-----------------------------
        %Task Name
        \multirow{3}{*}{cGM-1} 
        &
        %Attributes (row 1)
        \cmark & \xmark & \cmark & \xmark & \xmark
        & \cmark & \xmark & \xmark &
        %Example (Fig)
        \multirow{3}{*}{
        \includegraphics[width=0.48\textwidth, clip, 
        trim={88pt 834pt 1099pt 68pt},]
        {figures/Tasks_v8_2}
        }
    \\
    \cmidrule(lr){2-9}
    \vspace{1pt}
    %Row 2-----------------------------
    & 
    %Description
    \multicolumn{8}{p{6cm}}{
    \textbf{Description:} 
    Given $K$ samples 
    with class labels  
    from a domain $\D$,
    learn to generate 
    diverse and 
    high-quality 
    samples 
    conditioning 
    on the class 
    labels from $\D$
    }
    \\
    %Row 3-----------------------------
    &
    %Example
    \multicolumn{8}{p{6cm}}{
    \textbf{Example:} CbC \cite{shahbazi2022collapse} trains conditional generator on 20 classes of ImageNet Carnivores using 100 images per class
    }
    \vspace{2pt}
    \\
    \midrule
%Task 5
    %Row 1-----------------------------
        %Task Name
        \multirow{3}{*}{cGM-2} 
        &
        %Attributes (row 1)
        \cmark & \cmark & \cmark & \xmark & \xmark
      & \xmark & \cmark & \xmark &
        %Example (Fig)
        \multirow{3}{*}{
        \includegraphics[width=0.48\textwidth, clip, 
        trim={88pt 492pt 1099pt 342pt},]
        {figures/Tasks_v8_2}
        }
    \\
    \cmidrule(lr){2-9}
    \vspace{3pt}
    %Row 2-----------------------------
    & 
    %Description
    \multicolumn{8}{p{6cm}}{
    \textbf{Description:} 
    Given a pre-trained 
    generator on the
    seen classes $\Cs$
    of a domain $\D$ and
    $K$ samples with class
    labels from unseen
    classes $\Cu$ of
    $\D$, learn to generate
    diverse and
    high-quality samples
    conditioning on
    the class labels
    for $\Cu$ from $\D$
    }
    \\
    %\cmidrule(lr){2-9}
    %Row 3-----------------------------
    &
    %Example
    \multicolumn{8}{p{6cm}}{
    %\textit{E.g.,} 
    \textbf{Example:} LoftGAN \cite{gu2021lofgan} 
    learns from 85 classes of Flowers to generate 
    images for an unseen class with only 3 samples
    }
    \vspace{3pt}
    \\
    \midrule
%Task 6
    %Row 1-----------------------------
        %Task Name
        \multirow{3}{*}{cGM-3} 
        &
        %Attributes (row 1)
         \cmark & \cmark & \cmark & \xmark & \cmark &
    \cmark & \cmark & \xmark &
        %Example (Fig)
        \multirow{3}{*}{
        \includegraphics[width=0.48\textwidth, clip, 
        trim={88pt 113pt 1099pt 698pt},]
        {figures/Tasks_v8_2.pdf}
        }
    \vspace{2pt}
    \\
    \cmidrule(lr){2-9}
    \vspace{3pt}
    %Row 2-----------------------------
    & 
    %Description
    \multicolumn{8}{p{6cm}}{
    \textbf{Description:} 
    Given a pre-trained 
    generator on a 
    source domain $\Ds$
    and $K$ samples 
    with class labels 
    from a target
    domain $\Dt$ , learn 
    to generate diverse 
    and high-quality 
    samples conditioning
    on the class 
    labels from $\Dt$
    }
    \\
    %\cmidrule(lr){2-9}
    %Row 3-----------------------------
    &
    %Example
    \multicolumn{8}{p{6cm}}{
    %\textit{E.g.,} 
    \textbf{Example:}
     VPT \cite{sohn2023vpt} adapts
    a pre-trained 
    conditional 
    generator on
    ImageNet 
    to Places365 
    with 500 images per class
    }
    \vspace{3pt}
    \\
    \midrule
%Task 7
    %Row 1-----------------------------
        %Task Name
        \multirow{3}{*}{IGM} 
        &
        %Attributes (row 1)
         \xmark & \xmark & \cmark & \xmark & \xmark &
    \xmark & \cmark & \xmark &
        %Example (Fig)
        \multirow{3}{*}{
        \raisebox{-1.0cm}{
        \includegraphics[width=0.48\textwidth, clip, 
        trim={88pt 849pt 1099pt 20pt},]
        {figures/Tasks_v8_3.pdf}
        }
        }
    \\
    \cmidrule(lr){2-9}
    %Row 2-----------------------------
    & 
    %Description
    \multicolumn{8}{p{6cm}}{
    \textbf{Description:} 
     Given $K$ samples 
    (usually $K=1$) 
    and assuming rich internal
    distribution for
    patches within
    these samples,
    learn to generate
    diverse and
    high-quality
    samples with
    the same internal
    patch distribution
    }
    \\
    %\cmidrule(lr){2-9}
    %Row 3-----------------------------
    &
    %Example
    \multicolumn{8}{p{6cm}}{
    %\textit{E.g.,} 
    \textbf{Example:}
    SinDDM \cite{kulikov2023sinddm}
    trains a 
    generator using 
    a single image 
    of
    Marina Bay Sands, and
    generates
    %high-quality
    variants of it
    }
    \\
    \midrule
    %\pagebreak
%Task 8
    %Row 1-----------------------------
        %Task Name
        \multirow{3}{*}{SGM} 
        &
        %Attributes (row 1)
        \xmark & \cmark & \cmark & \cmark & \xmark
        & \xmark & \cmark & \xmark &
        %Example (Fig)
        \multirow{3}{*}{
        \includegraphics[width=0.48\textwidth, clip, 
        trim={90pt 440pt 1099pt 365pt},]
        {figures/Tasks_v8_3.pdf}
        }
        \vspace{3pt}
    \\
    \cmidrule(lr){2-9}
    %Row 2-----------------------------
    & 
    %Description
    \multicolumn{8}{p{6cm}}{
    \textbf{Description:} 
    Given a pre-trained generator, $K$ samples
    of a particular
    subject, and a 
    text prompt,
    learn to generate
    diverse and
    high-quality
    samples containing
    the same subject
    }
    \\
    %\cmidrule(lr){2-9}
    %Row 3-----------------------------
    &
    %Example
    \multicolumn{8}{p{6cm}}{
    %\textit{E.g.,} 
    \textbf{Example:}
    DreamBooth \cite{ruiz2023dreambooth}
    trains a  generator using 4 images of
    a particular
    backpack and
    adapts it with
    a text-prompt
    to be in the {\em `grand canyon'}
    }
    \\
    \midrule
\end{tabular}
}
\end{table}









% GANs
\vspace{0.1cm}
\noindent
{\bf Generative Adversarial Models (GAN)} \cite{saxena2021generative,jabbar2021survey}.
GAN applies an adversarial approach to learn the distribution of data $P_{data}$.
It consists of a generator $G$ and a discriminator $D$ playing a min-max game. 
Specifically, given the latent code $z$, the $G$ learns to generate the images $G(z)$, $z\sim P_z$, where $P_z$ is usually a Gaussian distribution.
Then, $D$ learns to distinguish the real images $x \sim P_{data}$ from the generated ones $G(z) \sim P_{model}$.
The $D$ and $G$ are optimized by respectively maximizing and minimizing the following value function:
\begin{equation}
    \mathcal{V}(D,G)= \mathbb{E}_{x\sim p_{data}} [ \log D(x)] + \mathbb{E}_{z\sim p_z} [\log (1-D(G(z)))] 
\end{equation}


% Normalizing Flow
\vspace{0.1cm}
\noindent
{\bf Flow-based Models} \cite{ho2019flow++}.
The flow-based model includes  a series of invertible yet differentiable functions $f$, between latent distribution $P_z$, and data distribution $P_{data}$.
The following log-likelihood function is maximized to train $f(.|\theta)$:
\begin{equation}
    \max_\theta \textstyle \sum_{i=1}^K \log P_{z} (f(x^{(i)}|\theta))+ \log |\det Df(x^{(i)}|\theta)|
\end{equation}
For ease of discussion, we simplify the model as a single flow and denote the training samples with $\{x^{(i)}\}^K_{i=1}$, and the Jacobian of $f(x)$ as $Df(x)$.
We remark that, unlike VAEs that estimate the lower bounds of the log-likelihood, flow-based models evaluate the exact log-likelihood in their loss function.

\vspace{0.1cm}
\noindent
{\bf Diffusion Models (DM)} \cite{kingma2021variational}. 
DM leverages the concept of the diffusion process from stochastic calculus and consists of forward diffusion and reverse diffusion processes.
In the forward diffusion process, based on the foundations of Markov chains, the noise $\epsilon \sim \mathcal{N}(0,I)$ is iteratively added to data samples until it approaches an isotropic Gaussian distribution.
Then, in the backward process, the DM learns to denoise the noisy vector $x_T$ and reconstruct the data samples $x_0$.
This is done by learning the noise estimation model $\epsilon_\theta$ with minimizing the following loss function \cite{ho2020denoising}:
\begin{equation}
    \mathcal{L}=\mathbb{E}_{t,x_0,\epsilon} [||\epsilon - \epsilon_{\theta}(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)||_2]
    \label{eqn:DMLoss}
\end{equation}
Then, during the generation process, DM first samples a noise $x_T\sim \mathcal{N}(0,I)$, 
and utilizes the learned noise function $\epsilon_\theta$ to iteratively apply the following denoising process \cite{ho2020denoising}:
\begin{equation}
    x_{t-1}=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}}}\epsilon_\theta(x_t,t)) +\sqrt{\beta_t}\epsilon, \quad t\in[0,T]
    \label{eqn:DMRec}
\end{equation}
Here, $x_t$ is the generated sample at step $T-t$, $\beta_t$ is variance scheduler, $\alpha_t=1-\beta_t$ and
$\bar{\alpha_t}=\prod^t_{s=1}\alpha_s$.

\vspace{0.1cm}
\noindent
{\bf Remark.} We remark that among discussed models, only GANs, DMs, and VAEs are adopted in the context of GM-DC.









\subsection{Data Constraints and Commonly Used Datasets}


In GM-DC, three data constraints have been considered in most works:
(i) \emph{Limited data (LD)}, 
when 50 to 5,000 training samples are given;
(ii) \emph{Few-Shot (FS)},
when 1 to 50 training samples are given;
(iii) \emph{Zero-Shot (ZS)},
when no training samples are given.
Training under these data constraints often results in various problems \eg over-fitting. We remark that these ranges are the typically used values as there are no fixed definitions in the literature.
Tab.~\ref{tab:datasets} lists the most common datasets used in GM-DC with related details.



