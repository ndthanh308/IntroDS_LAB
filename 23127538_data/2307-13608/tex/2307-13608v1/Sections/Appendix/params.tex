\section{\label{sec:hyperparams} Hyper-parameters}

After the hyperparameter search, we found that the best learning rates were:
$10^{-3}$ for {\sc EPMP} and {\sc PiNet}, 
$10^{-2}$ for {\sc $E(n)$-EPMP}, 
$5*10^{-3}$ for {\sc DiffNet}.
We trained all the models for 200 epochs and kept the weights that performed the best on the validation metrics during training.

The surface generated by PyMOL are composed of around 14k points. To ease and fast the training procudere we subsampled the surface considering only 2k points. In the case of point clouds we usa a random subsampling during training, while for the mesh we used a simplification method base on quadric error metrics.

\subsection{Layer dimensions}
For the {\sc EPMP$_{xyz}$} model, we use a graph convolution layer with inner dimension 31 and two GAT layers with inner dimension 62. In contrast, for the {\sc $E(n)$-EPMP}, we use one $E(n)$-invariant layer with an inner dimension of 28 and two GAT layers with inner dimension 56.

For all the O-GEP models, the geometric module comprises two layers with dimensions 64 and 128, while the segmentation module is composed of two layers with dimensions 64 and 32.