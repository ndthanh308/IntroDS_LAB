\section{Method}
In our experiments, we considered two scenarios: a protein represented through its inner structure (\textit{I-GEP}) and outer structure (\textit{O-GEP}). In both cases, we leverage the geometric information to improve the performance of epitope and paratope prediction methods.

% Figure environment removed

\subsection{\label{sec:IGEP} I-GEP}
Our I-GEP model is a method for predicting epitopes and paratopes using a graph-based approach that captures the inner structure of a protein. Each residue is represented as a node in a graph, and edges are created between the 15 closest neighbouring residues within 10 \AA. The I-GEP model has two main components: a structural module that computes an embedding for each residue using the graph structure and a graph attention network (GAT) that combines information from both the antigen and antibody residues. The network then predicts both epitope and paratope residues simultaneously using a fully connected layer, as shown in Fig. \ref{fig:OGEP}.

To improve the accuracy of our predictions, we integrate geometric information into the I-GEP model using two different approaches.
In the first approach, EPMP$_{xyz}$, we use graph convolutional network layers in the structural module as in EPMP \cite{del2021neural}, but we include the centred 3D coordinates of residues in the input features. 
The second approach, \textit{$E(n)$-EPMP}, uses the $E(n)$ invariant layer encoder from EGNN \cite{satorras2021n} instead of graph convolutional networks. This approach considers only the distances between residues, making it invariant to translations, rotations, and reflections on the residue positions in each molecule. 
%EGNN has been previously used for predicting small molecule properties, achieving competitive results.


% Figure environment removed

\subsection{\label{sec:OGEP} O-GEP}
Our O-GEP model operates on the protein's surface and includes a geometric module that uses the surface's geometry to spread information across it. This process generates features that are then combined and shared between the antibody and antigen through fully connected layers (segmentation module), resulting in an interaction probability for each point on the surface, as shown in Fig. \ref{fig:OGEP}.

We explore two different models for the geometric module. As a baseline, we use PointNet \cite{qi2017pointnet} to recreate the architecture proposed in PiNet \cite{PiNet}. The second model employs diffusion layers from DiffNet \cite{sharp2022diffnet} to propagate features on the surface. This makes our model robust against surface perturbations and suitable for handling meshes and point clouds with fewer points.

We further examine the impact of using the Heat Kernel Signature (HKS) as an extra geometric descriptor input. The HKS \cite{sun2009hks} is a concise point-wise spectral signature which summarizes local and global information about the intrinsic geometry of a shape by capturing the properties of the heat diffusion process on the surface. 
One of the key benefits of using HKS is that it remains stable even under minor surface perturbations, thus enabling it to withstand even conformational rearrangements of the proteins. 
To utilize the HKS descriptor, we concatenate it with the input features at each point on the surface and then pass the concatenated data through the geometric module.

To transfer the binding probabilities from the protein's surface to the residues, we utilized the average of all the points on the surface that correspond to the same residues. This method ensures that the binding probabilities are accurately represented in the residue space, enabling us to make reliable predictions about epitope and paratope locations.


\subsection{Training}

To handle imbalanced binary classification tasks, the networks were trained using the class-weighted binary cross-entropy loss and the Adam SGD optimizer. 
% During the training, the losses from both tasks, paratope and epitope prediction, were combined. 
For parameter tuning, we performed a hyperparameter search on the validation set. 
We train each model with five random seeds, and for each run, we keep the models' weights that performed the best on the validation set. 
During training, we also randomly rotate instances of the dataset to increase the robustness of the models.
See Appendix \ref{sec:hyperparams} for more details.

\subsection{Evaluation}

Given the significant disparity in class sizes, we utilize Matthew's correlation coefficient (MCC) between the residues' classification as our main benchmarking metric for model evaluation. %This aligns with evaluation methods in similar studies such as \cite{cia2023critical,krawczyk2014improving}. 
We also report the area under the receiver operating characteristic curve (AUC ROC) and the area under the precision-recall curve (AUC PR) as used in \cite{PiNet,del2021neural}. 
All reported values are aggregated across five random seeds to ensure the robustness of our findings.



