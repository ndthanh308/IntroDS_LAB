\documentclass[aps,pra,reprint,amsmath,amssymb,superscriptaddress,onecolumn,longbibliography, notitlepage,nofootinbib]{revtex4-2}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage[hidelinks]{hyperref}
\usepackage{float}

\usepackage{physics}


\usepackage{bbm}
\newcommand{\I}{\mathbbm{1}}
\usepackage[labelfont=bf, justification=RaggedRight, figurename={Figure }, tablename={Table}, labelsep=period]{caption}
\renewcommand{\thefigure}{\arabic{figure}}

% Definitions for comments from the authors
\usepackage{xcolor}
\def\fromplm#1{{\color{orange}\small{\bf PLM:} {\em #1}}}
\def\fromsym#1{{\color{red}\small{\bf SYM:} {\em #1}}}
\def\fromtw#1{{\color{blue}\small{\bf TW:} {\em #1}}}
\def\fromjl#1{{\color{cyan}\small{\bf JL:} {\em #1}}}

\begin{document}


\title{Quantum-noise-limited optical neural networks operating at a few quanta per activation}

\author{Shi-Yuan~Ma}
\email{sm2725@cornell.edu}
\affiliation{School of Applied and Engineering Physics, Cornell University, Ithaca, NY 14853, USA}

\author{Tianyu~Wang}
\affiliation{School of Applied and Engineering Physics, Cornell University, Ithaca, NY 14853, USA}

\author{Jérémie~Laydevant}
\affiliation{School of Applied and Engineering Physics, Cornell University, Ithaca, NY 14853, USA}
\affiliation{USRA Research Institute for Advanced Computer Science, Mountain View, CA 94035, USA}

\author{Logan~G.~Wright} 
\thanks{Present address: Department of Applied Physics, Yale University, New Haven, CT 06511, USA}
\affiliation{School of Applied and Engineering Physics, Cornell University, Ithaca, NY 14853, USA}
\affiliation{NTT Physics and Informatics Laboratories, NTT Research, Inc., Sunnyvale, CA 94085, USA}

\author{Peter~L.~McMahon}
\email{pmcmahon@cornell.edu}
\affiliation{School of Applied and Engineering Physics, Cornell University, Ithaca, NY 14853, USA}
\affiliation{Kavli Institute at Cornell for Nanoscale Science, Cornell University, Ithaca, NY 14853, USA}

\begin{abstract}

Analog physical neural networks, which hold promise for improved energy efficiency and speed compared to digital electronic neural networks, are nevertheless typically operated in a relatively high-power regime so that the signal-to-noise ratio (SNR) is large (\textgreater10). What happens if an analog system is instead operated in an ultra-low-power regime, in which the behavior of the system becomes highly stochastic and the noise is no longer a small perturbation on the signal? In this paper we study this question in the setting of optical neural networks operated in the limit where some layers use only a single photon to cause a neuron activation. Neuron activations in this limit are dominated by quantum noise from the fundamentally probabilistic nature of single-photon detection of weak optical signals. We show that it is possible to train stochastic optical neural networks to perform deterministic image-classification tasks with high accuracy in spite of the extremely high noise (SNR $\sim$ 1) by using a training procedure that directly models the stochastic behavior of photodetection. We experimentally demonstrated MNIST handwritten-digit classification with a test accuracy of $98\%$ using an optical neural network with a hidden layer operating in the single-photon regime; the optical energy used to perform the classification corresponds to 0.008 photons per multiply--accumulate (MAC) operation, which is equivalent to 0.003 attojoules of optical energy per MAC. Our experiment used \textgreater40$\times$  fewer photons per inference than previous state-of-the-art low-optical-energy demonstrations, to achieve the same accuracy of \textgreater90\%. Our work shows that some extremely stochastic analog systems, including those operating in the limit where quantum noise dominates, can nevertheless be used as layers in neural networks that deterministically perform classification tasks with high accuracy if they are appropriately trained.

\end{abstract}

\maketitle


\section{Introduction}
\label{sec:intro}

The development and widespread use of very large neural networks for artificial intelligence \cite{lecun2015deep,canziani2016analysis,thompson2020computational} has motivated the exploration of alternative computing paradigms---including analog processing---in the hope of improving both energy efficiency and speed \cite{markovi2020physics,christensen20222022}. Photonic implementations of neural networks using analog optical systems have experienced a resurgence of interest over the past several years \cite{shen2017deep,lin2018all,rios2019memory,wetzstein2020inference,xu202111,feldmann2021parallel,zhou2021large,wang2022optical,davis2022frequency,ashtiani2022chip,sludds2022delocalized,bandyopadhyay2022single}. However, analog processors---including those constructed using optics---inevitably have noise and typically also suffer from imperfect calibration and drift. These imperfections can result in degraded accuracy for neural-network inference performed using them \cite{shen2017deep,moon2019enhancing,joshi2020accurate,semenova2022understanding}. To mitigate the impact of noise, noise-aware training schemes have been developed \cite{klachko2019improving,zhou2020noisy,yang2022tolerating,wright2022deep,wu2022harnessing,borras2022walking,semenova2022noise,anderson2023optical,jiang2023physical}. These schemes treat the noise as a relatively small perturbation to an otherwise deterministic computation, either by explicitly modeling the noise as the addition of random variables to the processor's output or by modeling the processor as having finite bit precision. Recent demonstrations of ultra-low optical energy usage in optical neural networks (ONNs) \cite{wang2022optical,sludds2022delocalized} were in this regime of noise as a small perturbation and used hundreds to thousands of photons to represent the average neuron pre-activation signal prior to photodetection. In Ref.~\cite{wang2022optical}, we reported achieving $90\%$ accuracy on MNIST handwritten-digit classification using slightly less than 1 photon per scalar weight multiplication (i.e., per MAC)---which is already counterintuitively small---and one might be tempted to think that it's not possible to push the number of photons per MAC much lower while preserving accuracy. More typically, millions of photons per activation are used \cite{feldmann2021parallel,zhou2021large,sludds2022delocalized,bernstein2023single}. In this paper we address the following question: what happens if we use such weak optical signals in a ONN that each photodetector in a neural-network layer receives at most just one, or perhaps two or three, photons?

Physical systems are subject to various sources of noise. While some noise can be reduced through improvements to the hardware, some noise is fundamentally unavoidable, especially when the system is operated with very little power---which is an engineering goal for neural-network processors. Shot noise is a fundamental noise that arises from the quantized, i.e., discrete, nature of information carriers: the discreteness of energy in the case of photons in optics, and of discreteness of charge in the case of electrons in electronics \cite{beenakker2003quantum}. A shot-noise-limited measurement of a signal encoded with an average of $N_\textrm{p}$ photons (quanta) will have an SNR that scales as $\sqrt{N_\textrm{p}}$ \cite{agarwal2012quantum}.\footnote{The \textit{shot-noise limit}, which is sometimes also referred to as the \textit{standard quantum limit} \cite{machida1987observation}, can be evaded if, instead of encoding the signal in a thermal or coherent state of light, a quantum state---such as an intensity-squeezed state or a Fock state---is used. In this paper we consider only the case of \textit{classical} states of light for which shot noise is present and the shot-noise limit applies.} To achieve a suitably high SNR, ONNs typically use a large number of quanta for each detected signal. In situations where the optical signal is limited to just a few photons, photodetectors measure and can count individual quanta. Single-photon detectors (SPDs) are highly sensitive detectors that---in the typical \textit{click detector} setting---report, with high fidelity, the absence of a photon (\textit{no click}) or presence of one or more photons (\textit{click}) during a given measurement period \cite{hadfield2009single}. In the quantum-noise-dominated regime of an optical signal with an average photon number of about 1 impinging on an SPD, the measurement outcome will be highly stochastic, resulting in a very low SNR (of about 1).\footnote{Again, this is under the assumption that the optical signal is encoded in an optical state that is subject to the shot-noise limit---which is the case for classical states of light.} Conventional noise-aware-training algorithms are not able to achieve high accuracy with this level of noise. \textbf{Is it possible to operate ONNs in this very stochastic regime and still achieve high accuracy in deterministic classification tasks?} The answer is \textit{yes}, and in this work we will show how.


% Figure environment removed

The stochastic operation of neural networks has been extensively studied in computer science as part of the broader field of stochastic computing \cite{alaghi2013survey}. In the field of machine learning, binary stochastic neurons (BSNs) have been used to construct stochastic neural networks \cite{ackley1985learning,neal1990learning,neal1992connectionist,bengio2013estimating,tang2013learning,raiko2014techniques,hubara2016binarized}, with training being a major focus of study. Investigations of hardware implementations of stochastic computing neural networks, such as those in Refs.~\cite{ji2015hardware,lee2017energy} (with many more surveyed in Ref.~\cite{liu2020survey}), have typically been for deterministic complementary metal--oxide--semiconductor (CMOS) electronics, with the stochasticity introduced by random-number generators. While many studies of binary stochastic neural networks have been conducted with standard digital CMOS processors, there have also been proposals to construct them from beyond-CMOS hardware, motivated by the desire to minimize power consumption: direct implementation of binary stochastic neurons using bistable systems that are noisy by design---such as low-barrier magnetic tunnel junctions (MTJs)---has been explored \cite{vodenicarevic2017low,hassan2019low,chowdhury2023full}, and there have also been proposals to realize hardware stochastic elements for neural networks that could be constructed with noisy CMOS electronics or other physical substrates \cite{hylton2021vision,coles2023thermodynamic}. ONNs in which noise has been intentionally added \cite{wu2022harnessing,wu2022photonic,ma2023stochastic} have also been studied. Our work with low-photon-count optics is related but distinct from many of the studies cited here in its motivating assumption: instead of desiring noise and stochastic behavior---and purposefully designing devices to have them, we are concerned with situations in which physical devices have large and unavoidable noise but where we would like to nevertheless construct deterministic classifiers using these devices because of their potential for low-energy computing (Figure \ref{fig:intro}).

The \textbf{key idea} in our work is that when ONNs are operated in the approximately-1-photon-per-neuron-activation regime and the detectors are SPDs, it is natural to consider the neurons as binary stochastic neurons: the output of an SPD is binary (\textit{click} or \textit{no click}) and fundamentally stochastic. Instead of trying to train the ONN as a deterministic neural network that has very poor numerical precision, one can instead train it as a binary stochastic neural network, adapting some of the methods from the last decade of machine-learning research on stochastic neural networks \cite{bengio2013estimating,tang2013learning,raiko2014techniques,gu2015muprop,hubara2016binarized,liu2018stochastic,liu2020survey} and using a physics-based model of the stochastic single-photon detection (SPD) process during training. We call this \textit{physics-aware stochastic training}.

We experimentally implemented a stochastic ONN using as a building block an optical matrix-vector multiplier \cite{wang2022optical} modified to have SPDs at its output: we call this a \textit{single-photon-detection neural network} (SPDNN). We present results showing that high classification accuracy can be achieved even when the number of photons per neuron activation is approximately 1, and even without averaging over multiple shots. We also studied in simulation how larger, more sophisticated stochastic ONNs could be constructed and what their performance on CIFAR-10 image classification would be.

% Figure environment removed

\section{Single-photon-detection neural networks: optical neural networks with stochastic activation from single-photon detection}
\label{subsec:spd_actv}

We consider ONNs in which one or more layers are each constructed from an optical matrix-vector multiplier followed by an array of SPDs (Figure \ref{fig:diag}a--c), and in which the optical powers used are sufficiently low that in each execution of the layer, each SPD has at most only a few photons impinging on it, leading to stochastic measurement outcomes of \textit{no click} or \textit{click}.

In our setting, we aim to perform \textit{inference} using the SPDNN---with its implementation in physical hardware---(Figure \ref{fig:diag}d) and to perform \textit{training} of the SPDNN \textit{in silico} (Figure \ref{fig:diag}e--f). That is, training is performed entirely using standard digital electronic computing.\footnote{It is not required that the training be done \textit{in silico} for it to succeed but is just a choice we made in this work. \textit{Hardware-in-the-loop} training, such as used in Ref.~\cite{wright2022deep}, is a natural alternative to purely \textit{in silico} training that even can make training easier by relaxing the requirements on how accurate the \textit{in silico} model of the physical hardware process needs to be.}

\subsection{Physics-aware stochastic training}

To train an SPDNN, we perform gradient descent using backpropagation, which involves a forward pass, to compute the current error (or loss) of the network, and a backward pass, which is used to compute the gradient of the loss with respect to the network parameters; our procedure is inspired by backpropagation-based training of stochastic and binary neural networks \cite{bengio2013estimating,hubara2016binarized}. We model the forward pass (upper part of Figure \ref{fig:diag}e) through the network as a stochastic process that captures the key physics of SPD of optical signals having Poissonian photon statistics \cite{gerry2005introductory}: the measurement outcome of SPD is a binary random variable (\textit{no click} or \textit{click}) that is drawn from the Bernoulli distribution with a probability that depends on the mean photon number of the light impinging on the detector. However, during the backward pass (lower part of Figure \ref{fig:diag}e), we employ a deterministic mean-field estimator to compute the gradients. This approach avoids the stochasticity and binarization of the SPD process, which typically pose difficulties for gradient estimation.

We now give a brief technical description of our forward and backward passes for training; for full details see Methods and Supplementary Notes 1A and 2A. We denote the neuron pre-activations of the $l$th stochastic layer of an SPDNN as $\textbf{z}^{(l)}=W^{(l)}\textbf{a}^{(l-1)}$, where $\textbf{a}^{(l-1)}$ is the activation vector from the previous layer ($\textbf{a}^{(0)}$ denotes the input vector $\textbf{x}$ of the data to be classified). In the physical realization of an SPDNN, $\textbf{z}^{(l)}$ is encoded optically (for example, in optical intensity) following an optical matrix-vector multiplier (optical MVM, which computes the product between the matrix $W^{(l)}$ and the vector $\textbf{a}^{(l-1)}$) but before the light impinges on an array of SPDs. We model the action of an SPD with a stochastic activation function, $f_{\text{SPD}}$ (Figure \ref{fig:diag}b; Eq.~\ref{eq:f_spd}). The stochastic output of the $l$th layer is then $\textbf{a}^{(l)}=f_{\text{SPD}}(\textbf{z}^{(l)})$.

For an optical signal having mean photon number $\lambda$ and that obeys Poissonian photon statistics, the probability of a \textit{click} event by an SPD is $P_{\text{SPD}}(\lambda) = 1-e^\lambda$ (Figure \ref{fig:diag}c). We define the stochastic activation function $f_{\text{SPD}}$ as follows:

\begin{equation}
\label{eq:f_spd}
  f_{\text{SPD}}(z) \coloneqq
  \begin{cases}
    1 & \text{with probability $p=P_{\text{SPD}}(\lambda(z))$}, \\
    0 & \text{with probability $1-p$,}
  \end{cases}
\end{equation}

\noindent where $\lambda(z)$ is a function mapping a single neuron's pre-activation value to a mean photon number. For an incoherent optical setup where the information is directly encoded in intensity, $\lambda(z) = z$; for a coherent optical setup where the information is encoded in field amplitude and the SPD directly measures the intensity, $\lambda(z) = |z|^2$. In general, the form of $\lambda(z)$ is determined by the signal encoding used in the optical MVM, and the detection scheme following the MVM. We use $f_{\text{SPD}}$ in modeling the stochastic behavior of an SPDNN layer in the forward pass. However, during the backward pass, we make a deterministic mean-field approximation of the network: instead of evaluating the stochastic function $f_{\text{SPD}}$, we evaluate $P_{\text{SPD}}(\lambda(z))$ when computing the activations of a layer: $\textbf{a}^{(l)}=P_{\text{SPD}}(\lambda(\textbf{z}^{(l)}))$ (Figure \ref{fig:diag}b). This is an adaptation of a standard machine-learning method for computing gradients of stochastic neural networks \cite{bengio2013estimating}.

\subsection{Inference}

When performing inference (Figure \ref{fig:diag}d), we can run just a single shot of a stochastic layer or we can choose to take the average of multiple shots---trading greater energy and/or time usage for reduced stochasticity. For a single shot, a neuron activation takes on the value $a^{[1]}=a\in\{0,1\}$; for $K$ shots, $a^{[K]}=\frac{1}{K}\sum^K_{k=1} a_k\in\{0,1/K,2/K,\ldots,1\}$. In the limit of infinitely many shots, $K\rightarrow\infty$, the activation $a^{[\infty]}$ would converge to the expectation value, $a^{[\infty]}=\mathbb{E}[a]=P_{\text{SPD}}(\lambda(z))$. In this work we focus on the single-shot ($K=1$) and few-shot $K \leq 5$ regime, since the high-shot $K \gg 100$ regime is very similar to the high-photon-count-per-shot regime that has already been studied in the ONN literature (e.g., in Ref.~\cite{wang2022optical}). An important practical point is that averaging for $K>1$ shots can be achieved by counting the clicks from each SPD, which is what we did in the experiments we report. We can think of $K$ as a discrete integration time, so averaging need not involve any data reloading or sophisticated control.

% Figure environment removed


\section{MNIST handwritten-digit classification with a single-photon-detection multilayer perceptron}

We evaluated the performance---both in numerical simulations and in optical experiments---of SPDNNs on the MNIST handwritten-digit-classification benchmark task with a simple, $784\rightarrow N \rightarrow10$ multilayer perceptron (MLP) architecture (Figure \ref{fig:incoh}a).
The activation values in the hidden layer were computed by SPDs. The optical power was chosen so that the SNR of the SPD measurements was $\sim 1$, falling in the low-SNR regime (Figure \ref{fig:intro}b). The output layer was implemented either with full numerical precision on a digital electronic computer, or optically with an integration time set so that the measured signal comprised enough photons that a high SNR (Figure \ref{fig:intro}b) was achieved, as in conventional ONNs. Our use of a full-precision output layer is consistent with other works on binary neural networks \cite{hubara2016binarized,rastegari2016xnor,zhou2016dorefa}. In a shallow neural network, executing the output layer at high SNR substantially limits the overall energy efficiency gains from using small photon budgets in earlier layers, but in larger models, the relatively high energy cost of a high-SNR output layer is amortized. Nevertheless, as we will see, even with just a single-hidden-layer network, efficiency gains of \textgreater40$\times$ are possible by performing the hidden layer in the low-SNR regime. 

The models we report on in this section used non-negative weights in the hidden layers and real-valued weights in the output layers. This allows the hidden layers to be straightforwardly realized with optical MVMs using incoherent light.\footnote{A high-SNR layer with real-valued weights can be realized with an incoherent optical MVM if some digital-electronic postprocessing is allowed \cite{hayasaki1992optical,wang2022optical}---which is the approach we take for the optical output layer executions in our experiments. However, the postprocessing strategy doesn't directly apply in the low-SNR regime because readout becomes inseparable from the application of a nonlinear activation function, so we are constrained to non-negative weights and activations in the hidden layers.} In Section \ref{sec:coh} and Supplementary Note 2, we report on extensions to the case of real-valued weights in coherent optical processors.

\subsection{Simulation results}
\label{subsec:classif}

First, we digitally simulated the SPDNN models shown in Figure \ref{fig:incoh}a. We report the simulated test accuracies in Figure \ref{fig:incoh}b for the full test dataset of 10,000 images, as a function of the number of hidden neurons $N$ and the number of shots $K$ of binary SPD measurements integrated to compute each activation.

Due to the stochastic nature of the model, the classification output for a fixed input varies from run to run. We repeated inferences on fixed inputs from the test set 100 times; we report the mean and standard deviation of the test accuracy as data points and error bars, respectively. The standard deviations of the test accuracies are around $0.1\%$.

The accuracy achieved by the SPDNN is substantially higher than for linear models ($<93\%$ classification accuracy on MNIST \cite{lecun1998gradient}). This both shows that despite the hidden layer being stochastic, high-accuracy determistic classification is possible, and that the SPD activation function serves as a suitable nonlinearity in the neural network. 
The sizes of the models we simulated (in number of neurons $N$) are similar to those of traditional deterministic neural networks for MNIST classification \cite{hamerly2019large}, so the high accuracies achieved are not a simple consequence of averaging over many noisy neurons \cite{laydevant2021training}.

If we integrated an infinite number of SPD measurements for each activation ($K \rightarrow \infty$)---which is infeasible in experiment, but can be simulated---then the SPDNN output would become deterministic. The test accuracy achieved in this limit can be considered as an upper bound, as the classification accuracy improves monotonically with $K$.
Notably, even with just a single SPD measurement ($K=1$) for each activation, the mean test accuracy is around $97\%$.
The accuracy is substantially improved with just a few more shots of averaging, and approaches the deterministic upper bound when $K\gtrsim5$. The mean single-photon-detection probability, averaged over all neurons, is $\approx 0.5$, so the simulated number of detected photons per shot is very small: $\approx 0.5 N$. As we will quantify in the next section reporting the results of optical experiments, this means high accuracy can be achieved using much less optical energy than in conventional ONNs.

\subsection{Optical experimental results}

In our experimental demonstrations, we based our SPDNN on a free-space optical matrix-vector multiplier (MVM) that we had previously constructed for high-SNR experiments \cite{wang2022optical}, and replaced the detectors with SPDs so that we could operate it with ultra-low photon budgets (see Methods). The experiments we report were, in part, enabled by the availability of cameras comprising large arrays of pixels capable of detecting single photons with low noise \cite{dhimitri2022scientific}.
We encoded neuron values in the intensity of incoherent light; as a result, the weights and input vectors were constrained to be non-negative. However, this is not a fundamental feature of SPDNNs---in the next section (Section~\ref{sec:coh}), we present simulations of coherent implementations that lift this restriction.
A single-photon-detecting camera measured the photons transmitted through the optical MVM, producing the stochastic activations as electronic signals that were input to the following neural-network layer (see Methods and Supplementary Note 3 and 4).

In our first set of optical experiments, the hidden layer was realized optically and the output layer was realized \textit{in silico} (Figure \ref{fig:incoh}c): the output of the SPD measurements after the optical MVM was passed through a linear classifier executed with full numerical precision on a digital electronic computer. We tested using both $K=1$ (no averaging) and $K=2$ shots of averaging the stochastic binary activations in the hidden layer. The results agree well with simulations, which differ from the simulation results shown in Figure \ref{fig:incoh}b because they additionally modeled imperfections in our experimental optical-MVM setup (see Methods, Supplementary Note 7). The test accuracies were calculated using 100 test images, with inference for each image repeated 30 times. The hidden layer (the one computed optically in these experiments) used approximately 0.0008 detected photons per MAC, which is $\geq 6$ orders of magnitude lower than is typical in ONN implementations \cite{feldmann2021parallel,zhou2021large,sludds2022delocalized,bernstein2023single} and $\geq 3$ orders of magnitude lower than the lowest photons-per-MAC numbers reported to date \cite{wang2022optical,sludds2022delocalized}.

We then performed experiments in which both the hidden layer and the output layer were computed optically (Figure \ref{fig:incoh}d). In these experiments, we implemented a neural network with 400 hidden neurons and used 5 shots per inference ($N=400$, $K=5$). The total optical energy was varied by changing the number of photons used in the output layer; the number of photons used in the hidden layer was kept fixed.

The average value of the stochastic binary activations $a_i$ in the hidden layer was $\approx 0.522$. This corresponds to a total of $0.522 \times N \times K = 1044$ photons being detected in the hidden layer per inference. The total detected optical energy per inference comprises the sum of the detected optical energy in the hidden ($784\rightarrow 400$) layer and in the output ($400\rightarrow 10$) layer (see Methods, Supplementary Table 6 and Supplementary Note 9).

The results show that even though the output layer was operated in the high-SNR regime (Figure \ref{fig:intro}b), the full inference computation achieved high accuracy yet used only a few femtojoules of optical energy in total (equivalent to a few thousand photons). By dividing the optical energy by the number of MACs performed in a single inference, we can infer the per-MAC optical energy efficiency achieved: with an average detected optical energy per MAC of approximately 0.001 attojoules (0.003 attojoules), equivalent to 0.003 photons (0.008 photons), the test accuracy was $92.0\pm 2.3\%$ ($98.0\pm 1.3\%$).

% Figure environment removed

\section{Simulation study of possible future deeper, coherent single-photon-detection neural networks}
\label{sec:coh}

We have successfully experimentally demonstrated a two-layer SPDNN, but can SPDNNs be used to implement deeper and more sophisticated models? One of the limitations of our experimental apparatus was that it used an intensity encoding with incoherent light and as a result could natively only perform operations with non-negative numbers. In this section we will show that SPDNNs capable of implementing signed numbers can be used to realize multilayer models (with up to 6 layers), including models with more sophisticated architectures than multilayer perceptrons---such as models with convolutional layers.

ONNs based on coherent light can naturally encode sign information in the phase of the light and have been realized in many different physical platforms \cite{shen2017deep,chang2018hybrid,lin2018all,spall2020fully,miscuglio2020massively,feldmann2021parallel,xu202111}. We propose---and study in simulation---SPDNNs using coherent light. Neuron values are encoded in optical amplitudes that are constrained to have phases that are either $0$ (positive values) or $\pi$ (negative values). With this encoding, detection by an SPD---which measures intensity and is hence insensitive to phase---results in a stochastic nonlinear activation function that is symmetric about zero (Figure \ref{fig:coh}a; see Methods). Alternative detection schemes could be employed that would modify the activation function, but we have focused on demonstrating the capabilities of this straightforward case, avoiding introducing additional experimental complexity.

We performed two sets of simulation experiments: one on coherent SPDNNs trained to perform MNIST handwritten-digit classification, and one on coherent SPDNNs trained to performed CIFAR-10 image classification. Figures \ref{fig:coh}d shows the architectures tested and simulation results for the MNIST benchmark (see Methods, Supplementary Note 2B). The accuracy achieved by MLPs with either one or two hidden layers was higher than that of the single-hidden-layer MLP simulated for the incoherent case (Figure \ref{fig:incoh}b), and an architecture with a single convolutional layer followed by two linear layers achieved \textgreater99\% accuracy even in the single-shot ($K=1$) regime.

Figure \ref{fig:coh}e shows the results of simulating variants of a 6-layer convolutional SPDNN (comprising 4 convolutional layers and 2 fully connected, linear layers) on CIFAR-10 image classification. All these simulation results were obtained in the single-shot ($K=1$) regime. The number of channels in each convolution layer was varied, which affects the total number of MACs used to perform an inference. We observed that the test accuracy increased with the size of the SPDNN, with accuracies approaching those of conventional convolutional neural networks of comparable size \cite{lee2016generalizing}, as well as of binarized convolutional neural networks \cite{hubara2016binarized,esser2016convolutional,qin2020binary}. 
In the models we simulated that only used SPD as the activation function (i.e., the ones in which there are no `Digital ReLU' blocks), the high-SNR linear output layer had only 4000 MAC operations, so the number of MACs in the high-SNR layer comprises less than 0.01\% of the total MACs performed during an inference. The models we simulated are thus sufficiently large that the total optical energy cost would be dominated by the (low-SNR) layers prior to the (high-SNR) output layer. Equivalently, the optical energy cost per MAC would be predominantly determined by the cost of the low-SNR layers.
These simulation results illustrate the ability of SPDNNs to scale to larger and deeper models, enabling them to perform more challenging tasks. The symmetric stochastic activation function that is realized by SPD of coherently encoded real values yields good accuracies on both MNIST and CIFAR-10 benchmarks and is straightforward to implement experimentally.

\section{Discussion}
\label{sec:discussion}  

In this paper we have shown that it is possible to construct an optical neural network (ONN) in which one or more layers use single-photon detection (SPD) of weak optical signals to perform stochastic neuron activation, and---despite the exceptionally low signal-to-noise ratio (SNR) of around 1 in the low-optical-power layers---such single-photon-detection neural networks (SPDNNs) can achieve high accuracy in deterministic classification tasks. This is enabled by physics-aware stochastic training, in which an ONN is trained as a stochastic neural network using a model that incorporates knowledge of the physics of photodetection of optical signals with average photon number around 1 that are subject to Poissonian photon statistics. We experimentally demonstrated a two-layer ONN in which the (large) hidden layer was operated in the low-optical-power, quantum-noise-limited, highly stochastic regime (SNR $\sim$ 1) and the (small) output layer was operated in the higher-optical-power, low-noise regime (SNR $\gtrsim$ 10). This ONN (when run with $N=50$ hidden neurons and $K=5$ shots of SPD measurements per activation; see Supplementary Figure 20) achieved a test accuracy of $90.6\%$ on MNIST handwritten-digit recognition while using only an average of $1390$ detected photons per inference (corresponding to $\sim$\SI{0.5}{fJ} of detected optical energy per inference), which is a large improvement over recent state-of-the-art low-optical-power ONN experiments in the following metric: $1390$ photons per inference is \textgreater40$\times$ less than used by the ONNs in Refs.~\cite{wang2022optical,sludds2022delocalized} to achieve the same accuracy (\textgreater90\%) on the same task (MNIST classification). \footnote{We could also very favorably compare the number of photons per MAC used in our experiments versus in the experiments reported in Refs.~\cite{wang2022optical,sludds2022delocalized}, but we don't wish to emphasize this metric here for two reasons. Firstly, and most importantly, we see energy per inference as a more important metric to focus on than energy per MAC, even though picking metrics is not necessarily straightforward \cite{sze2020evaluate}. Secondly, dot products computed for the hidden layer in our optical experiments are read out stochastically by single-photon detectors that output just 1 bit of information, whereas the dot products computed in the experiments reported by Refs.~\cite{wang2022optical,sludds2022delocalized} are read out with more bits of precision. This difference in the nature and precision of the readout means a MAC operation in our experiments is arguably not quite the same as a MAC operation in the experiments of Refs.~\cite{wang2022optical,sludds2022delocalized}, and so careful interpretation is needed when comparing their costs.}

While we have demonstrated a fundamental point---that ONNs can be successfully operated in the few-photon-per-activation regime in which quantum shot noise causes very low SNR---an important practical consideration for the construction of ONNs is that the energy used by optical signals within the ONN are only part of the ONN's total energy consumption, and it is the total energy per inference that is generally what one wants to optimize for \cite{nahmias2019photonic,hamerly2019large,anderson2023optical}. A practical limitation of our experiments is that they were conducted with a relatively slow\footnote{19.8~kHz maximum frame rate.} single-photon-detector array, limiting the speed at which a single execution of a layer could be carried out, and the detector array was not optimized for energy efficiency. For our fundamental approach and methods to be applied to make ONNs that offer a practical advantage over state-of-the-art electronic processors as generic neural-network accelerators, there remains important work to be done in engineering an overall system that operates sufficiently fast while minimizing total energy cost. Recent progress in the development of large, fast arrays of single-photon detectors coupled with digital logic \cite{bruschini2023linospad2} suggest that there is a path towards this goal. Ref.~\cite{shainline2017superconducting} has also pointed out the possibility of using fast superconducting-nanowire single-photon detectors for realizing spiking neural networks. Furthermore, there is a complementary path toward utility in the nearer term: if instead of aiming to use ONNs to entirely replace electronic processors, one uses ONNs as a pre-processor for input data that is already optical \cite{wetzstein2020inference,wang2023image,huang2023photonic}, operating the ONN with single-photon detectors is a natural match with scenarios in which the optical input is very weak---for example, in low-light-imaging applications.

Our approach is not tied to a specific architecture of ONN---the free-space matrix-vector multiplier used in our experiments is just one of many possible choices of architecture. Other ONNs could be adapted to use our approach by replacing the photodetectors typically used for readout of neurons at the end of a layer with single-photon detectors. ONNs based on diffractive optics \cite{lin2018all,chang2018hybrid,zhou2021large}, Mach-Zehnder interferometer (MZI) meshes \cite{carolan2015universal,shen2017deep,bogaerts2020programmable}, and other on-chip approaches to matrix-vector multiplication \cite{tait2015demonstration,xu202111,feldmann2021parallel} all appear compatible.

In our optical experiments, we used single-photon detectors that output an electronic signal when a photon is detected. However, in multilayer ONNs, the input to each layer is optical. One can convert an electronic detector output to an optical input by modulating an optical source---which is what we did and what is often done in ONNs more generally \cite{wetzstein2020inference}---but an alternative is to construct a device that performs SPD with high efficiency and gives the measurement result as an \textit{optical} signal that can be directly used as an input to the next layer in the ONN. Designing and demonstrating such a device is an interesting potential avenue for future work in applied quantum nonlinear optics \cite{mazets2007multiatom,pinotsi2008single,sotier2009femtosecond,kiilerich2019input,li2023single,roques2023biasing}, and could lead to both lower electronic energy consumption and higher speed for single-photon-detection ONNs.

We trained our demonstration SPDNN \textit{in silico} using backpropagation, but if SPDNNs with high overall energy efficiency are built, it would be a boon use this efficient hardware not only for inference but also for training. To this end, it could be interesting to study how to adapt \textit{in situ} training \cite{zhou2020insitu,guo2021backpropagation,bandyopadhyay2022single,pai2023experimentally}, including backpropagation-free (e.g., Refs.~\cite{bengio2015towards,lillicrap2020backpropagation,hinton2023mortal,stern2023learning}), methods for SPDNNs. An open question related to training is whether it is possible to make SPDNNs that do not involve a final high-SNR layer while preserving task accuracy; this could help to reduce the overall energy per inference. Other future work could explore the extension of our research to neural networks with larger sizes (wider and more layers, which could both improve the capability of the neural network and further amortize the energy cost of the final, high-SNR layer, if used), more sophisticated classification tasks (beyond MNIST and CIFAR-10 image classification---such as has been shown with conventional binary neural networks \cite{rastegari2016xnor, bulat2019xnor, bulat2019matrix}), and generative or other probabilistic tasks---for which the stochasticity can be harnessed rather than merely tolerated. Beyond machine-learning tasks, an SPDNN layer could be used as the core of a single-photon-regime photonic Ising machine \cite{mohseni2022ising} for heuristically solving combinatorial-optimization problems, realizing an optical version of p-bit computing \cite{chowdhury2023full}.

Our research is an example of realizing a neural network using a stochastic physical system. Beyond optics, our work is related and complementary to recent investigations in electronic, spintronic, and quantum neuromorphic computing \cite{torrejon2017neuromorphic,grollier2020neuromorphic,cai2020power,harabi2023memristor,markovi2020physics,islam2023hardware,markovic2020quantum,cerezo2022challenges}, including in training physical systems to perform neural-network inference \cite{prezioso2015training,hughes2019wave,mitarai2018quantum,cramer2022surrogate,chen2020classification,wright2022deep}. Noise is a fundamental feature and the ultimate limit to energy efficiency in computing with all analog physical systems. It has long been realized that noise is not always detrimental: not only does it not necessarily prevent accurate computation, but can in some cases even enable fundamentally new and more efficient algorithms or types of computation. Our work shows that using a quantum physical model of a particular hardware's noise at the software level can enable surprisingly large gains in energy efficiency. 

The phenomena observed in our work seemingly relies on two key physical ingredients. First, the system's available states are effectively quantized, as in the photonic quantization of energy in our ONN demonstration, or the binarization that occurs in low-barrier, stochastic magnetic tunnel junctions \cite{grollier2020neuromorphic}. Second, the noise in the system results in the quantized outputs of the system being stochastic. This suggests that ultra-low-SNR physical neural networks should be possible in many physical hardware platforms beyond photonics. Systems in which shot noise dominates are natural matches with our approach and methods. Our approach could also be relevant to systems in which thermal (Johnson) noise dominates---as is typically the case in room-temperature electronics---but this will depend on not just the noise but also the system's dynamics. Which hardware platforms and system architectures can yield an overall energy benefit by being operated in a stochastic regime while maintaining computational accuracy is an important open question.

While there are many reasons computer science has traditionally favored the abstraction of hardware from software, our work is part of a broad trend, spanning many different physical platforms \cite{berggren2020roadmap,christensen20222022,finocchio2023roadmap}, in which researchers engineer computations in a physics-aware manner. By short-circuiting the abstraction hierarchy---in our case, going from a physics-aware software description of a stochastic neural network directly to a physical optical realization of the constituent operations---it is possible to achieve orders-of-magnitude improvements in energy efficiency \cite{wetzstein2020inference,anderson2023optical} versus conventional CMOS computing. \textit{Physics-aware software}, in which software directly incorporates knowledge of the physics of the underlying computing hardware---such as in the \textit{physics-aware stochastic training} we used in this work---is understudied compared to purely software-level or hardware-level innovations (i.e., ``at the top'' or ``at the bottom'' of the hierarchy \cite{leiserson2020there}). It is thus ripe for exploration: within the domain of neural networks, there are a multitude of emerging physical platforms that could be more fully harnessed if the physical devices were not forced to conform to the standard abstractions in modern computer architecture \cite{wright2022deep}. Beyond neural-network accelerators, communities such as computational imaging \cite{kellman2020physics} have embraced the opportunity to improve system performance through co-optimizing hardware and software in a physics-aware manner. We believe there is an opportunity to make gains in even more areas and applications of computing technology by collapsing abstractions and implementing physics-aware software with physical hardware that could be orders of magnitude faster or more energy efficient than current digital CMOS approaches but that doesn't admit a clean, digital, deterministic abstraction.

\section*{Data and code availability}

All the simulation and experimental data presented in the paper, demonstration data for data gathering, as well as training data for the SPDNN models, are available at \url{https://doi.org/10.5281/zenodo.8188270}.
An expandable demonstration code to train SPDNNs as well as other stochastic physical systems is available at \url{https://github.com/mcmahon-lab/Single-Photon-Detection-Neural-Networks}. 

\section*{Acknowledgements}

We wish to thank NTT Research for their financial and technical support (S.-Y.M., P.L.M., T.W. and L.G.W.). Portions of this work were supported by the National Science Foundation (award no. CCF-1918549; J.L., P.L.M. and T.W.), a Kavli Institute at Cornell instrumentation grant (P.L.M. and T.W.), and a David and Lucile Packard Foundation Fellowship (P.L.M.). P.L.M. acknowledges membership of the CIFAR Quantum Information Science Program as an Azrieli Global Scholar. T.W. acknowledges partial support from an Eric and Wendy Schmidt AI in Science Postdoctoral Fellowship. We acknowledge valuable discussions with M.~Anderson, F.~Chen, R.~Hamerly, T.~Onodera, S.~Prabhu, M.\,M.~Sohoni and R.~Yanagimoto. We also acknowledge Z.~Eslami, V.~Kremenetski, F.~Presutti, C.~Wan and F.~Wu for helpful suggestions regarding the manuscript.

\section*{Author Contributions}

S.-Y.M., L.G.W., T.W., and P.L.M. conceived the project. S.-Y.M. and T.W. designed the experiments and built the experimental setup. S.-Y.M. and J.L. performed the neural-network training. S.-Y.M. performed the experiments, the data analysis, and the numerical simulations. All authors contributed to preparing the manuscript. T.W., L.G.W. and P.L.M. supervised the project.


\bibliographystyle{mcmahonlab}
\bibliography{references}

\newpage
\setcounter{equation}{0}

\section*{Methods}
\label{methods}
\subsection*{Stochastic optical neural networks using single-photon detection as the activation function}
In the single-photon-detection neural networks (SPDNNs), the activation function is directly determined by the stochastic physical process of single-photon detection (SPD). The specific form of the activation function is dictated by the detection process on a single-photon detector (SPD). Each SPD measurement produces a binary output of either 0 or 1, with probabilities determined by the incident light intensity. Consequently, each SPD neuron activation, which corresponds to an SPD measurement in experiments, is considered as a binary stochastic process \cite{neal1990learning,lee2017energy,liu2018stochastic}.

Following the Poisson distribution, the probability of an SPD detecting a photon click is given by $P_{\text{SPD}}(\lambda)=1-e^{-\lambda}$ when exposed to an incident intensity of $\lambda$ photons per detection. Note that this photon statistics may vary based on the state of light (e.g. squeezed light), but here we only consider the Poissonian light.
Therefore, the SPD process can be viewed as a Bernoulli sampling of that probability, expressed as $f_{\text{SPD}}(z) = \textbf{1}_{t<P_{\text{SPD}}(\lambda(z))}$, where $t$ is a uniform random variable $t\sim U[0,1]$ and $\textbf{1}_x$ is the indicator function that evaluates to 1 if $x$ is true. This derivation leads to Equation \ref{eq:f_spd} in the main text.
In our approach, the pre-activation value $z$ is considered as the direct output from an optical matrix-vector multiplier (MVM) that encodes the information of a dot product result. For the $i$th pre-activation value in layer $l$, denoted as $z^{(l)}_i$, the expression is given by:
\begin{equation}
z^{(l)}_i=\sum_{j=1}^{N_{l-1}} w^{(l)}_{ij}\cdot a^{(l-1)}_{j},
\end{equation}
where $N_{l-1}$ is the number of neurons in layer $l-1$, $w^{(l)}_{ij}$ is the weight between the $i$th neuron in layer $l$ and the $j$th neuron in layer $l-1$, $a^{(l-1)}_j$ is the activation of the $j$th neuron in layer $l$. The intensity $\lambda(z)$ is a function of $z$ that depends on the detection scheme employed in the optical MVM.
In optical setups using incoherent light, the information is directly encoded in the intensity, resulting in $\lambda=z$. 
If coherent light were used in a setup, where 0 and $\pi$ phases represent the sign of the amplitude, the intensity is determined by squaring the real-number amplitude if directly measured, resulting in $\lambda=z^2$. 
While more sophisticated detection schemes can be designed to modify the function of $\lambda(z)$, we focused on the simplest cases to illustrate the versatility of SPDNNs.
 
During the inference of a trained model, in order to regulate the level of uncertainty inherent in stochastic neural networks, we can opt to conduct multiple shots of SPD measurements during a single forward propagation. In the case of a $K$-shot inference, each SPD measurement is repeated $K$ times, with the neuron's final activation value $a^{[K]}$ being derived from the average of these $K$ independent stochastic binary values. Consequently, for a single shot, $a^{[1]}=a\in\{0,1\}$; for $K$ shots, $a^{[K]}=\frac{1}{K}\sum^K_{k=1} a_k\in \{0,1/K,2/K,\ldots,1\}$. By utilizing this method, we can mitigate the model's stochasticity, enhancing the precision of output values. Ideally, with an infinite number of shots ($K\rightarrow\infty$), the activation $a^{[\infty]}$ would equate to the expected value without any stochasticity, that is, $a^{[\infty]}=\mathbb{E}[a]=P_{\text{SPD}}(\lambda(z))$. The detailed process of an inference of SPDNNs is described in Algorithm 2 in Supplementary Note 1A.

The training of our stochastic neuron models takes inspiration from recent developments in training stochastic neural networks. We have created an effective estimator that trains our SPDNNs while accounting for the stochastic activation determined by the physical SPD process.
To train our SPDNNs, we initially adopted the idea of the ``straight-through estimator'' (STE) \cite{hinton2012,yin2019understanding}, which enables us to bypass the stochasticity and discretization during neural network training. However, directly applying STE to bypass the entire SPD process led to subpar training performance. To address this, we adopted a more nuanced approach by breaking down the activation function and treating different parts differently.
The SPD process can be conceptually divided into two parts: the deterministic probability function $P_{\text{SPD}}$ and the stochasticity introduced by the Bernoulli sampling. For a Bernoulli distribution, the expectation value is equal to the probability, making $P_{\text{SPD}}$ the expectation of the activation. Instead of applying the ``straight-through'' method to the entire process, we chose to bypass only the Bernoulli sampling process. At the same time, we incorporate the gradients induced by the probability function, aligning them with the expectation values of the random variable. In this way, we obtained an unbiased estimator \cite{williams1992simple} for gradient estimation, thereby enhancing the training of our SPDNNs.

In the backward propagation of the $l$th layer, the gradients of the pre-activation $z^{(l)}$ can be computed as (the gradient with respect to any parameter $x$ is defined as $g_x=\partial C/\partial x$ where $C$ is the cost function): 
\begin{equation}
    g_{z^{(l)}} 
    = \frac{\partial a^{(l)}}{\partial \lambda^{(l)}}\circ \frac{\partial\lambda^{(l)}}{\partial z^{(l)}} \circ  g_{a^{(l)}}
    = P_{\text{SPD}}'(\lambda^{(l)})\circ \frac{\partial\lambda^{(l)}}{\partial z^{(l)}} \circ  g_{a^{(l)}},
\end{equation}
where $a^{(l)}=f_{\text{SPD}}(z^{(l)}) = \textbf{1}_{t<P_{\text{SPD}}(\lambda(z^{(l)}))}$ and the gradients $g_{a^{(l)}}$ is calculated from the next layer (previous layer in the backward propagation).
Using this equation, we can evaluate the gradients of the weights $W^{(l)}$ as 
    $g_{W^{(l)}} = g_{z^{(l)}}^\top a^{(l-1)}$,
where $a^{(l-1)}$ is the activation values from the previous layer. 
By employing this approach, SPDNNs can be effectively trained using gradient-based algorithms (such as SGD \cite{bottou2012stochastic} or AdamW \cite{loshchilov2017decoupled}), regardless of the stochastic nature of the neuron activations.

For detailed training procedures, please refer to Algorithm 1 and 3 in Supplementary Notes 1A and 2A, respectively.

\subsection*{Simulation of incoherent SPDNNs for deterministic classification tasks}
The benchmark MNIST (Modified National Institute of Standards and Technology database) \cite{de2015comparison} handwritten digit dataset consists of 60,000 training images and 10,000 testing images. Each image is a grayscale image with $28 \times 28 = 784$ pixels. To adhere to the non-negative encoding required by incoherent light, the input images are normalized so that pixel values range from 0 to 1.

To assess the performance of the SPD activation function, we investigated the training of the MLP-SPDNN models with the structure of $784\xrightarrow{W^{(1)}} N \xrightarrow{W^{(2)}} 10$, where $N$ represents the number of neurons in the hidden layer, $W^{(1)}$ ($W^{(2)}$) represents the weight matrices of the hidden (output) layer. The SPD activation function is applied to the $N$ hidden neurons, and the resulting activations are passed to the output layer to generate output vectors (Figure \ref{fig:incoh}a).
To simplify the experimental implementation, biases within the linear operations were disabled, as the precise control of adding or subtracting a few photons poses significant experimental challenges. We have observed that this omission has minimal impact on the model's performance.

In addition, after each weight update, we clamped the elements of $W^{(1)}$ in the positive range in order to comply with the constraint of non-negative weights of an incoherent optical setup. Because SPD is not required at the output layer, the constraints on the last layer operation are less stringent. Although our simulations indicate that the final performance is only marginally affected by whether the elements in the last layer are also restricted to be non-negative, we found that utilizing real-valued weights in the output layer provided increased robustness against noise and errors during optical implementation. As a result, we chose to use real-valued weights in $W^{(2)}$.

During the training process, we employed the LogSoftmax function on the output vectors and used cross-entropy loss to formulate the loss function. Gradients were estimated using the unbiased estimator described in the previous section and Algorithm 1.

For model optimization, we found that utilizing the SGD optimizer with small learning rates yields better accuracy compared to other optimizers such as AdamW, albeit at the cost of slower optimization speed. Despite the longer total training time, the SGD optimizer leads to a better optimized model. The models were trained with a batch size of 128, a learning rate of 0.001 for the hidden layer and 0.01 for the output layer, over 10,000 epochs to achieve optimized parameters. To prevent gradient vanishing in the plateau of the probability function $P_{\text{SPD}}$, pre-activations were clamped at $\lambda_{\text{max}}=3$ photons. 

It should be noted that due to the inherent stochasticity of the neural networks, each forward propagation generates varying output values even with identical weights and inputs. 
However, we only used one forward propagation in each step. This approach effectively utilized the inherent stochasticity in each forward propagation as an additional source of random search for the optimizer. Given the small learning rate and the significant noise in the model, the number of epochs exceeded what is typically required for conventional neural network training processes.
The training is performed on a GPU (Tesla V100-PCIE-32GB) and takes approximately eight hours for each model.

We trained incoherent SPDNNs with a varying number of hidden neurons $N$ ranging from 10 to 400. The test accuracy of the models improved as the number of hidden neurons increased (see Supplementary Note 1B for more details). During inference, we adjusted the number of shots per SPD activation $K$ to tune the SNR of the activations within the models.

For each model configuration with $N$ hidden neurons and $K$ shots of SPD readouts per activation, we repeated the inference process 100 times to observe the distribution of stochastic output accuracies. Each repetition of inference on the test set, which comprises 10,000 images, yielded a different test accuracy. The mean values and standard deviations of these 100 repetitions of test accuracy are plotted in Figure \ref{fig:incoh}b (see Supplementary Table 1 for more details). It was observed that increasing either $N$ or $K$ led to higher mean values of test accuracy and reduced standard deviations.

\subsection*{Experimental implementation of SPDNNs}
The optical matrix-vector multiplier setup utilized in this work is based on the design presented in \cite{wang2022optical}. The setup comprises an array of light sources, a zoom lens imaging system, an light intensity modulator, and a photon-counting camera. For encoding input vectors, we employed an organic light-emitting diode (OLED) display from a commercial smartphone (Google Pixel 2016 version). The OLED display features a $1920 \times 1080$ pixel array, with individually controllable intensity for each pixel. In our experiment, only the green pixels of the display were used, arranged in a square lattice with a pixel pitch of \SI{57.5}{\micro m}.
To perform intensity modulation as weight multiplication, we combined a reflective liquid-crystal spatial light modulator (SLM, P1920-500-1100-HDMI, Meadowlark Optics) with a half-wave plate (HWP, WPH10ME-532, Thorlabs) and a polarizing beamsplitter (PBS, CCM1-PBS251, Thorlabs). The SLM has a pixel array of dimensions $1920 \times 1152$, with individually controllable transmission for each pixel measuring $9.2\times9.2$ \SI{}{\micro m}. The OLED display was imaged onto the SLM panel using a zoom lens system (Resolv4K, Navitar).
The intensity-modulated light field reflected from the SLM underwent further de-magnification and was focused onto the detector using a telescope formed by the rear adapter of the zoom lens (1-81102, Navitar) and an objective lens (XLFLUOR4x/340, Olympus). 

We decompose a matrix-vector multiplication in a batch of vector-vector dot products that are computed optically, either by spatial multiplexing (parallel processing) or temporal multiplexing (sequential processing). To ensure a more accurate experimental implementation, we chose to perform the vector-vector dot products in sequence in most of the data collection. 
For the computation of an optical vector-vector dot product,  the value of each element in either vector is encoded in the intensity of the light emitted by a pixel on the OLED and the transmission of an SLM pixel. 
The imaging system aligned each pixel on the OLED display with its corresponding pixel on the SLM, where element-wise multiplication occurred via intensity modulation. The modulated light intensity from pixels in the same vector was then focused on the detector to sum up the element-wise multiplication values, yielding the vector-vector dot product result. 
Since the light is incoherent, only non-negative values can be allowed in both of the vectors.
For more details for the incoherent optical MVM, please refer to Supplementary Note 3. The calibration of the vector-vector dot products on the optical MVM is detailed in Supplementary Note 5.

In this experiment, we used a scientific CMOS camera (Hamamatsu ORCA-Quest qCMOS Camera C15550-20UP) \cite{dhimitri2022scientific} to measure both conventional light intensity measurement and SPD. 
This camera, with $4096 \times 2304$ effective pixels of $4.6\times4.6$ \SI{}{\micro m} each, can perform SPD with ultra-low readout noise in its photon counting mode.
This scientific CMOS camera is capable of carrying out the SPD process with ultra-low readout noise. 
When utilized as an SPD in the photon-counting mode, the camera exhibits an effective photon detection efficiency of $68\%$ and a dark count rate of approximately 0.01 photoelectrons per second per pixel (Supplementary Note 4). 
We typically operate with an exposure time in the millisecond range for a single shot of SPD readout.
For conventional intensity measurement that integrates higher optical energy for the output layer implementation, we chose another operation mode that used it as a common CMOS camera.

Further details on validating the stochastic SPD activation function measured on this camera are available in Supplementary Note 6. We also adapted our SPDNNs training methods to conform to the real-world constraints of our setup, ensuring successful experimental implementation (see Supplementary Note 7).
 
First, we conducted the implementation of the hidden layers and collect the SPD activations experimentally by the photon-counting camera as an SPD array. Then we performed the output layer operations digitally on a computer. 
This aims to verify the fidelity of collecting SPD activations from experimental setups. 
Supplementary Figure 16 provides a visual representation of the distribution of some of the output vectors.
For the experiments with 1 shot per activation ($K=1$), we collected 30 camera frames from the setup for each fixed input images and weight matrix, which are regarded as 30 independent repetitions of inference. They were then used to compute 30 different test accuracies by performing the output linear layer on a digital computer.
For the experiments with 2 shots per activation ($K=2$), we divided the 30 camera frames into 15 groups, with each group containing 2 frames. The average value of the 2 frames within each group serves as the activations, which are used to compute 15 test accuracies. For additional results and details, please refer to Supplementary Note 8.

Second, to achieve the complete optical implementation of the entire neural networks, we utilized our optical matrix-vector multiplier again to carry out the last layer operations. 
For example, we first focused on the data from the model with 400 hidden neurons and performed 5 shots per inference.
In this case, for the 30 binary SPD readouts obtained from 30 frames, we performed an averaging operation on every 5 frames, resulting in 6 independent repetitions of the inference. These activation values were then displayed on the SLM as the input for the last layer implementation. For the 5-shot activations, the possible values included 0, 0.2, 0.4, 0.6, 0.8, and 1.
When the linear operation were performed on a computer with full precision, the mean test accuracy was approximately $99.17\%$. 
To realize the linear operation with real-valued weight elements on our incoherent optical setup, we divided the weight elements into positive and negative parts. Subsequently, we projected these two parts of the weights onto the OLED display separately and performed them as two different operations. The final output value was obtained by subtracting the results of the negative weights from those of the positive weights.
This approach requires at least double the photon requirement for the output layer and offers room for optimization to achieve higher energy efficiency. Nevertheless, even with these non-optimized settings, we demonstrated a photon budget that is lower than any other ONN implementations known to us for the same task and accuracy.
For additional data and details, please refer to Supplementary Note 9.

\subsection*{Deeper SPDNNs operating with coherent light}
Optical processors with coherent light have the ability to preserve the phase information of light and have the potential to encode complex numbers using arbitrary phase values. In this work, we focused on coherent optical computing utilizing real-number operations. In this approach, positive and negative values are encoded in the light amplitudes corresponding to phase 0 and $\pi$, respectively.

As the intensity of light is the square of the amplitude, direct detection of the light amplitude, where the information is encoded, would involve an additional square operation, i.e., $\lambda(z)=|z|^2$.
This leads to a ``V-shape'' SPD probability function with respect to the pre-activation $z$, as depicted in Figure \ref{fig:coh}a. We chose to focus on the most straightforward detection case to avoid any additional changes to the experimental setup. Our objective is to demonstrate the adaptability and scalability of SPDNN models in practical optical implementations without the need for complex modifications to the existing setup.

\subsubsection*{Coherent SPDNNs for MNIST classification}

\paragraph*{MLP-SPDNNs}
Classifying MNIST using coherent MLP-SPDNNs was simulated utilizing similar configurations as with incoherent SPDNNs. The only difference was the inclusion of the coherent SPD activation function and the use of real-valued weights.
Contrary to the prior scenario with incoherent light, the input values and weights do not need to be non-negative.
The models were trained using the SGD optimizer \cite{bottou2012stochastic} with a learning rate of 0.01 for the hidden layers and 0.001 for the last linear layer, over a period of 10,000 epochs.

\paragraph*{Convolutional SPDNNs}
The convolutional SPDNN model used for MNIST digit classification, illustrated in Figure \ref{fig:coh}b, consists of a convolutional layer with 16 output channels, a kernel size of $5 \times 5$, a stride size of 1, and padding of 2. The SPD activation function was applied immediately after the convolutional layer, followed by average pooling of $2 \times 2$. The feature map of $14\times 14 \times 16=3136$ was then flattened into a vector of size 3136. After that, the convolutional layers were followed by a linear model of $3136 \rightarrow 400 \rightarrow 10$, with the SPD activation function applied at each of the 400 neurons in the first linear layer.

The detailed simulation results of the MNIST test accuracies of the coherent SPDNNs can be found in Supplementary Table 2 with varying model structures and shots per activation $K$. For additional information, see Supplementary Note 2B.


\subsubsection*{Coherent convolutional SPDNNs for Cifar-10 classification}
The CIFAR-10 dataset \cite{krizhevsky2009learning} has 60,000 images, each having $3\times 32\times 32$ pixels with 3 color channels, that belong to 10 different categories, representing airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships and trucks. The dataset is partitioned into a training set with 50,000 images and a test set with 10,000 images. The pixel values have been normalized using the mean value of $(0.4914, 0.4822, 0.4465)$ and standard deviation of $(0.2471, 0.2435, 0.2616)$ for each of the color channels.
To boost performance, data augmentation techniques including random horizontal flips (50\% probability) and random $32\times 32$ crops (with 4-pixel padding) were implemented during training.

The convolutional SPDNN models for Cifar-10 classification have deeper structures. 
Same as the convolutional models trained for MNIST, the convolutional layers use a kernel size of $5 \times 5$, a stride size of 1 and padding of 2. Each convolutional layer is followed by the SPD activation function, average pooling of $2 \times 2$, as well as batch normalization.
After $N_\text{conv}$ convolutional layers ($N_\text{conv}=4$ in Figure \ref{fig:coh}e) with the number of output channels of the last one to be $N_\text{chan}^\text{last}$, the feature map of $(32/2^{N_\text{conv}})^2\times N_\text{chan}^\text{last}$ is flattened to a vector, followed by two linear layers of $(32/2^{N_\text{conv}})^2 N_\text{chan}^\text{last} \rightarrow 400 \rightarrow 10$.
In the first linear layer, either SPD or ReLU \cite{agarap2018deep} activation function were used for each of the 400 neurons, as depicted in Figure \ref{fig:coh}e.
We vary the number of convolutional layers and number of output channels of them to change the different model size (Figure \ref{fig:coh}e and Supplementary Figure 5). 
In these results, we only used a single shot of SPD measurement ($K=1$) to compute the SPD activations in the models, including the convolutional and linear layers.
For additional information, please refer to Supplementary Note 2C.


\end{document}
