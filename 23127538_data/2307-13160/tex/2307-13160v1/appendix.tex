
\section{Supplementary Material for Section~\ref{sec:prelim}}\label{app:prelim}

\subsection{Basics of Probability Theory}


A \emph{measurable space} is a pair $(U,\Sigma_U)$, where $U$ is a nonempty set and $\Sigma_U$ is a $\sigma$-algebra on $U$, i.e., a family of subsets of $U$ such that $\Sigma_U\subseteq \mathcal{P}(U)$ contains $\emptyset$ and is closed under complementation and countable union. Elements of $\Sigma_U$ are called \emph{measurable} sets. A function $f$ from a measurable space $(U_1,\Sigma_{U_1})$ to another measurable space $(U_2,\Sigma_{U_2})$ is \emph{measurable} if $f^{-1}(A)\in\Sigma_{U_1}$ for all $A\in\Sigma_{U_2}$.

A \emph{measure} $\mu$ on a measurable space $(U,\Sigma_U)$ is a mapping from $\Sigma_U$ to $[0,\infty]$ such that (i) $\mu(\emptyset)=0$ and (ii) $\mu$ 
%satisfies the
is countably additive:
%condition: 
for every pairwise-disjoint set sequence $\{A_n\}_{n\in\Nset}$ in $\Sigma_U$, it holds that $\mu(\bigcup_{n\in\Nset}A_n)=\sum_{n\in\Nset}\mu(A_n)$. We call the triple $(U,\Sigma_U,\mu)$ a \emph{measure space}. 
%If $\mu(U)\le 1$, we call $\mu$ a \emph{subprobability measure}. 
If $\mu(U)=1$, we call $\mu$ a \emph{probability measure}, and $(U,\Sigma_U,\mu)$ a \emph{probability space}.
The Lebesgue measure $\lambda$ is the unique measure on $(\Rset,\Sigma_{\Rset})$ satisfying $\lambda([a,b))=b-a$ for all valid intervals $[a,b)$ in $\Sigma_{\Rset}$. For each $n\in\Nset$, we have a measurable space $(\Rset^n,\Sigma_{\Rset^n})$ 
%such that there exists 
and
a unique product measure $\lambda_n$ on $\Rset^n$ satisfying $\lambda_n(\prod_{i=1}^n A_i)=\prod_{i=1}^n \lambda(A_i)$ for all $A_i\in\Sigma_{\Rset}$.


The \emph{Lebesgue} integral operator $\int$ is a partial operator that maps a measure $\mu$ on $(U,\Sigma_U)$ and a real-valued function $f$ on the same space $(U,\Sigma_U)$ to a real number or infinity, which is denoted by $\int f \mathrm{d}\mu$ or $\int f(x)\mu(\mathrm{d}x)$. 
The detailed definition of Lebesgue integral is somewhat technical, see \cite{rankin1968real,rudin1976principles} for more details. 
Given a measurable set $A\in\Sigma_U$, the integral of $f$ over $A$ is defined by $\int_A f(x)\mu(\mathrm{d} x):=\int f(x) \cdot [x\in A] \mu(\mathrm{d}x)$
%\begin{align*}
%\textstyle\int_A f(x)\mu(\mathrm{d} x):=\int f(x) \cdot [x\in A] \mu(\mathrm{d}x)
%\end{align*} 
where $[-]$ is the Iverson bracket such that $[\phi]=1$ if 
%the predicate 
$\phi$ is true, and $0$ otherwise. If $\mu$ is a probability measure, then we call the integral as the \emph{expectation} of $f$, denoted by $\expectdist{x\sim\mu;A}{f}$, or $\expv[f]$ when the scope is clear from the context.

For a measure $v$ on $(U,\Sigma_U)$, a measurable function $f:U\to \Rset_{\ge 0}$ is the \emph{density} of $v$ with respect to $\mu$ if $v(A)=\int f(x)\cdot [x\in A] \mu(\mathrm{d} x)$ for all measurable $A\in\Sigma_U$, and $\mu$ is called the \emph{reference measure} (most often $\mu$ is the Lebesgue measure). Common families of probability distributions on the reals, e.g., uniform, normal distributions, are measures on $(\Rset,\Sigma_{\Rset})$. Most often these are defined in terms of probability density functions with respect to the Lebesgue measure. That is, for each $\mu_D$ there is a measurable function $\text{pdf}_D:\Rset\to\Rset_{\ge 0}$ that determines it: $\mu_D(A):=\int_A \text{pdf}_D (\mathrm{d}\lambda) $. As we will see, density functions such as $\text{pdf}_D$ play an important role in Bayesian inference.

Given a probability space $\pspace$, a \emph{random variable} is an $\mathcal{F}$-measurable function $X: \Omega \rightarrow \Rset \cup \{+\infty,-\infty\}$. The expectation of a random variable $X$, denoted by $\expv(X)$, is the Lebesgue integral of $X$ w.r.t. $\probm$, i.e., $\int X\,\mathrm{d}\probm$. A \emph{filtration} of $\pspace$ is an infinite sequence $\{ \mathcal{F}_n \}_{n=0}^{\infty}$ such that for every $n\ge 0$, the triple $(\Omega, \mathcal{F}_n, \probm)$ is a probability space and $\mathcal{F}_n \subseteq \mathcal{F}_{n+1} \subseteq \mathcal{F}$. A \emph{stopping time} w.r.t. $\{ \mathcal{F}_n \}_{n=0}^{\infty}$ is a random variable $T: \Omega \rightarrow \Nset \cup \{0, \infty\}$ such that for every $n \geq 0$, the event \{$T \leq n$\} is in $\mathcal{F}_n$. 

A \emph{discrete-time stochastic process} is a sequence $\Gamma = \{X_n\}_{n=0}^\infty$ of random variables in $\pspace$. The process $\Gamma$ is \emph{adapted} to a filtration $\{ \mathcal{F}_n \}_{n=0}^{\infty}$, if for all $n \geq 0$, $X_n$ is a random variable in $(\Omega, \mathcal{F}_n, \probm)$. A discrete-time stochastic process $\Gamma=\{X_n\}_{n=0}^\infty$ adapted to a filtration $\{\mathcal{F}_n\}_{n=0}^\infty$ is a \emph{martingale} (resp. \emph{supermartingale}, \emph{submartingale})
if for all $n \geq 0$, $\expv(|X_n|)<\infty$ and it holds almost surely (i.e.,~with probability $1$) that
$\condexpv{X_{n+1}}{\mathcal{F}_n}=X_n$ (\mbox{resp. } $\condexpv{X_{n+1}}{\mathcal{F}_n}\le X_n$, $\condexpv{X_{n+1}}{\mathcal{F}_n}\ge X_n$).
See~\cite{williams1991probability} for details.
%Intuitively, a martingale is a discrete-time stochastic process, in which at any time $n$, the expected value $\condexpv{X_{n+1}}{\mathcal{F}_n}$ in the next step, given all previous values, is equal to the current value $X_n$. In a supermartingale, this expected value is less than or equal to the current value and a submartingale is defined conversely.
Applying martingales to qualitative and quantitative analysis of probabilistic programs is a well-studied technique~\cite{SriramCAV,ChatterjeeFG16,ChatterjeeNZ2017}.

\subsection{Details for WPTS Semantics}\label{app:wpts-semantics}

\changed[fu]{We denote by $\Lambda$ the set of all states, by $\Delta$ the set of all weighted states, and by $\Sigma_\Delta$ the product $\sigma$-algebra (on $\Delta$) among the discrete $\sigma$-algebra $(L, 2^{L})$ for locations, the $\sigma$-algebra $\Sigma_{\mathbb{R}^{|\pvars|}}$ for program valuations, and the $\sigma$-algebra $\Sigma_{\mathbb{R}}$ for the multiplicative likelihood weight. 
We define $\Sigma^n_\Delta$ (for $n\ge 1$) as the set $\{A_1\times \dots \times A_n\mid \forall 1\le i\le n. (A_i\in \Sigma_{\Delta})\}$, and $\Delta^\infty$ as the set of all infinite sequences of weighted states. }

The probability space for the WPTS $\Pi$ is defined such that its sample space is the set of all program runs, 
%denoted as $\Omega$, 
its $\sigma$-algebra 
is generated by the countable union $\bigcup_{n\ge 1} \{B\times \Delta^\infty\mid B\in \Sigma^n_\Delta\}$), and its probability measure $\probm$ is the unique one such that (i) $\probm(A\times \Lambda^\infty)=\mu_\mathrm{init}(\{\pv\mid (\lin, \pv, 1)\in A\})$ for all $A\in\Sigma_\Delta$, and 
%for which we abuse the notation so that $\mu_\mathrm{init}$ also means the distribution $\mu$ such that 
(ii) $\probm(A\times B\times \Delta^\infty)$ (for every $A\in\Sigma_\Delta, B\in \Sigma^n_\Delta$ ($n\ge 1$)) equals the probability w.r.t the sampling of $\mu_\mathrm{init}$ (for the initial program valuation) and $\rdvarjdis$ (for a sampling valuation in each step until the $(n+1)$-th step) that a program run $\{\Theta_n\}_{n\ge 0}$ is subject to $\Theta_0\in A$ and $(\Theta_1,\dots,\Theta_{n+1})\in B$. 
%first $n+1$ weighted states 
%:=\mathrm{Pr}_{\mu_\mathrm{init}\otimes \rdvarjdis^n}(A\times B)$ 
%, where $\rdvarjdis^n$ is the product measure of $n$ copies of $\rdvarjdis$. 
For each program valuation $\pv$, we denote by $\mathbb{P}_\pv$ the probability measure of $\Pi$ when the initial distribution is changed to the Dirac distribution at $\pv$.

\subsection{Sampling-based Semantics}\label{app:sampling-semantcis}
 We recall one prominent semantics in the literature, i.e., the sampling-based semantics~\cite{borgstrom2016lambda,DBLP:conf/lics/StatonYWHK16}. We show that the transition-based semantics in our work is equivalent to the widely-used sampling-based semantics in Bayesian statistical probabilistic programming. 
 
 The sampling-based semantics by Borgstr{\"{o}}m et al.~\cite{borgstrom2016lambda} interprets a probabilistic program as
 a deterministic program parameterized by a sequence of random draws sampled during the execution of the program. 

 A \emph{sampling trace} is a finite sequence $\tr= \langle r_1, \dots, r_n \rangle$ of real numbers, and we define $\mathcal{T}:=\bigcup_{n\in\Nset} \Rset^n$ as the set of all sampling traces. 
 Given a probabilistic program $P$, a \emph{configuration} $\sigma$ under the semantics is a tuple $\langle \pv,S,w,\tr \rangle$ where $\pv\in \val{\pvars}$ 
 , $S$ is the statement to be executed, $w\in [0,\infty)$ is the global weight variable whose value expresses how well the current computation matches the observations, and $\tr$ is a sampling trace. We denote by $\Sigma$ the set of all configurations. 
 
 The semantics operates on the configurations, where an execution of the program is initialized with $\sigma_0=\tuple{\pv_0,P, 1,\tr}$, and the termination configurations have the form of $\tuple{\_,\textbf{skip}, \_, []}$, for which $\_$ is a ``wildcard'' character that matches everything and $[]$ is an empty set. 
 \cref{fig:OperationalSemantics} shows the corresponding one-step reduction relation $\to$ (note that $\Downarrow$ is the
 usual big-step semantics for deterministic Boolean and arithmetic expressions, so we omit it here).  
 
 
 Let $\to^*$ be the reflexive transitive closure of the one-step reduction $\to$ in \cref{fig:OperationalSemantics}. Given a probabilistic program $P$, we call a sampling trace $\tr$ \emph{terminating} if $\tuple{\pv,P,1,\tr}\to^* \tuple{\pv',\text{skip},w,[]}$ for some valuations $\pv, \pv'\in \val{\pvars}$ and weight $w\in \Rset_{\ge 0}$, i.e.,~the program $P$ terminates under the samples drawn as in $\tr$.	
 
 
 % Figure environment removed
 

 Below we define the notion of posterior distributions by the sampling-based semantics. From the one-step reduction rules (in \cref{fig:OperationalSemantics}), we can reason about the global behavior of probabilistic programs in terms of the sampling traces they produce. 
 That is, given a probabilistic program $P$, and a terminating trace $\tr$ such that $\tuple{\pv,P,1,\tr}\to^* \tuple{\pv',\text{skip},w,[]}$ for valuations $\pv, \pv'\in\val{\pvars}$ and weight $w\in \Rset_{\ge 0}$, we define the \emph{value function} $\valueSem P$
 and the \emph{weight function} $\weightSem P$ as follows: 
 \begin{align}
 \valueSem P(\pv,\tr) &= \begin{cases}
 \pv' &\text{if }\tuple{\pv,P,1,\tr}\to^* \tuple{\pv',\text{skip},w,[]}\\
 \text{unspecified} &\text{otherwise},
 \end{cases}\\
 \weightSem P(\pv,\tr) &= \begin{cases}
 w &\text{if }\tuple{\pv,P,1,\tr}\to^* \tuple{\pv',\text{skip},w,[]}\\
 0 &\text{otherwise}.
 \end{cases}
 \end{align}
 Moreover, we denote the return variable by $\valueSem{P,\mathit{ret}}(\pv,\tr)$, i.e.,~ $\valueSem{P,\mathit{ret}}(\pv,\tr):=\pv'[\mathit{ret}]$.
 We also consider the measure space $(\mathcal{T},\Sigma_{\mathcal{T}},\mu_{\mathcal{T}})$ where $\mathcal{T}=\bigcup_{n\in\Nset} \Rset^n$ (as mentioned previously), $\Sigma_{\trans}:=\{\bigcup_{n\in\Nset} U_n\mid U_n\in \Sigma_{\Rset^n}\}$  and $\mu_{\trans}(U):=\sum_{n\in\Nset}\lambda_n(U\cap \Rset^n)$. 
 By definition, the measure space $(\mathcal{T},\Sigma_{\mathcal{T}},\mu_{\mathcal{T}})$ specifies the probability values for sets of sampling traces.
 

 \paragraph{Posterior Distributions.} Given a probabilistic program $P$, an initial program valuation $\pv\in \val{\pvars}$ and a measurable set $U\in\Sigma_{\Rset}$, we define the set of terminating traces where the value of the return variable falls into $U$ as
 
 \[
 \mathcal{T}_{P, \pv, U} := \{\tr\in\mathcal{T} \mid \tuple{\pv,P,1,\tr}\to^*\tuple{\pv',\text{skip},w,[]}, \pv'[\mathit{ret}]\in U\}
 \]
 and the set of all terminating traces as
 \[
 \mathcal{T}_{P, \pv} := \{\tr\in\mathcal{T} \mid \tuple{\pv,P,1,\tr}\to^*\tuple{\pv',\text{skip},w,[]}\}.
 \]
 Note that $\mathcal{T}_{P, \pv} = \mathcal{T}_{P, \pv, \Rset}$. Therefore, we can define the \emph{unnormalised density} w.r.t $P,\pv,U$ as
 %
 \begin{align}\label{eq:unnormalised-posterior}
 %\measureSem{P}_{\pv}(U) := \int_{\valueSem{P,\mathit{ret}}^{-1}(U)} \weightSem P(\pv,\tr)  \,\mu_\trans(d \tr).
 \measureSem{P}_{\pv}(U) := \int_{\mathcal{T}_{P, \pv, U}} \weightSem P(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr).
 \end{align}
 That is, the integral takes all traces $\tr$ on which $P$ starts from $\pv$ and evaluates to a value in $U$, weighting each $\tr$ with the weight $\weightSem P(\pv,\tr)$ of the corresponding execution. The \emph{normalising constant} is thus defined by %$Z_P:=\measureSem{P}_{\pv}(\Rset)$.
 \begin{align}\label{eq:normalising-constant}
 Z_{P,\pv} := \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr).
 \end{align}
 Therefore, the normalised \emph{posterior distribution} is defined as $\posterior_P (\pv,U):=\frac{\measureSem{P}_\pv(U)}{Z_{P,\pv}}$. 
 %\end{defn}
  
 We call a program $P$ \emph{integrable} if its normalised constant is finite, i.e,~$0<Z_{P,\pv}<\infty$ for any $\pv\in\val{\pvars}$. 
 Given an integrable program, we are interested in deriving lower and upper bounds on the posterior distribution.
 
 \begin{definition}[Interval Bounds]
 	Given an integrable probabilistic program $P$, a program valuation $\pv\in\val{\pvars}$, and a measurable set $U\in\Sigma_\Rset$, we call $[l,u]$ an \emph{interval bound} of $\posterior_P (\pv,U)$ if $l\le \posterior_P(\pv,U) \le u$ for two reals $0\le l\le u\le 1$.
 	
 \end{definition}
 


\subsection{Equivalence between Posterior Distributions and Expected Weights }
In this subsection, we introduce the equivalence between posterior distributions (in sample-based semantics) and expected weights (in transition-based semantics: WPTS).

% Lemma~\ref{lem:density-composition}
\begin{lemma}\label{lem:density-composition}
	For all non-negative bounded measurable function $g:\Rset^{|\pvars|}\to \Rset$, and a program state $\Xi=(\loc,\pv)$, and a statement $P:=S_1;S_2$, we have that	
	\begin{align*}
	&\int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
	& {}=\int_{\mathcal{T}_{S_1, \pv}} \weightSem{S_1}(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr)   \Big([\valueSem{S_1}(\pv,\tr)=\pv']\\ 
	& \quad {}\cdot \int_{\mathcal{T}_{S_2, \pv'}} \weightSem{S_2}(\pv',\tr') \cdot g(\valueSem{S_2}(\pv',\tr')) \,\mu_\trans(\mathrm{d} \tr') \Big)\\
	\end{align*}
\end{lemma}
\begin{proposition}\label{prop:posterior-weight}
	For all non-negative bounded measurable function $g:\Rset^{|\pvars|}\to \Rset$, a probabilistic program $P$ and an initial program state $\Xi = (\loc,\pv)\in\Lambda$, we have that
	\begin{align*}
		\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)}= \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr).
	\end{align*}
\end{proposition}



\begin{proof}
	We prove by induction on the structure of statements.
	
	\begin{itemize}
		\item Case $P\equiv ``\textbf{skip}"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=&   g(\pv)  \\
			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv) \,\mu_\trans(\mathrm{d} \tr) \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		\item Case $P\equiv``x:=E"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=&   g(\pv[x\mapsto \llbracket E\rrbracket(\pv)])  \\
			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv[x\mapsto \llbracket E\rrbracket(\pv)]) \,\mu_\trans(\mathrm{d} \tr)  \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		\item Case $P\equiv``x:=\textbf{sample}\ D"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=&  \int  g(\pv[x\mapsto r]) \mu_D (\mathrm{d}r) \\
			%			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv[x\mapsto r]) \,\mu_\trans(\mathrm{d} \tr)  \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		
		\item Case $P\equiv``\textbf{observe}(x,D)"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=& W_\loc(\pv) \cdot g(\pv) \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		\item Case $P\equiv``\textbf{return}\ x"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=&    g(\pv)  \\
			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv) \,\mu_\trans(\mathrm{d} \tr)  \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		
		\item Case $P\equiv``\textbf{if}\ B\ \textbf{then}\ S_1\ \textbf{else}\ S_2\ \textbf{fi}"$. Assume the next state corresponding to the \textbf{then}-branch (resp. \textbf{else}-branch) is $\Xi_1=(\loc_1,\pv)$ (resp. $\Xi_2=(\loc_2,\pv)$). Then we obtain that
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} 
			%&=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\pv}(\mathrm{d}\omega)  \\
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} +  [\llbracket B \rrbracket(\pv)=false]\cdot \expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} \\
			%			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv) \,\mu_\trans(\mathrm{d} \tr)  \\
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \int_{\mathcal{T}_{S_1, \pv}} \weightSem{S_1}(\pv,\tr) \cdot g(\valueSem{S_1}(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
			& &	+[\llbracket B \rrbracket(\pv)=false] \cdot \int_{\mathcal{T}_{S_2, \pv}} \weightSem{S_2}(\pv,\tr) \cdot g(\valueSem{S_2}(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr)\\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		
		
		\item Case $P\equiv``S_1;S_2"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)}
			%	&=&    g(\pv)  \\
			&=& \int \widehat{w}_T(\omega) \probm_{\Xi}(\mathrm{d}\omega) \int [\omega_T=\Xi'] \cdot \widehat{w}_T(\omega')\cdot g(\widehat{\pv}_T(\omega')) \probm_{\Xi'}(\mathrm{d}\omega') \\
			&=& \int \widehat{w}_T(\omega) \probm_{\Xi}(\mathrm{d}\omega) \left([\omega_T=\Xi']\cdot \int  \cdot \widehat{w}_T(\omega')\cdot g(\widehat{\pv}_T(\omega')) \probm_{\Xi'}(\mathrm{d}\omega')\right) \\
			&=&  \int \widehat{w}_T(\omega) \probm_{\Xi}(\mathrm{d}\omega) \left([\omega_T=\Xi']\cdot \int_{\mathcal{T}_{S_2, \pv'}} \weightSem{S_2}(\pv',\tr') \cdot g(\valueSem{S_2}(\pv',\tr')) \,\mu_\trans(\mathrm{d} \tr') \right)\\
			%			&=&  \int \widehat{w}_T(\omega) \probm_{\Xi}(\mathrm{d}\omega) \left([\omega_T=\Xi'] \cdot \int_{\mathcal{T}_{S_2, \pv'}} \weightSem{S_2}(\pv',\tr') \cdot g(\valueSem{S_2}(\pv',\tr')) \,\mu_\trans(\mathrm{d} \tr') \right)\\
			&=&  \int_{\mathcal{T}_{S_1, \pv}} \weightSem{S_1}(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr)     \Big([\valueSem{S_1}(\pv,\tr)=\pv']\\ 
			&& \cdot \int_{\mathcal{T}_{S_2, \pv'}} \weightSem{S_2}(\pv',\tr') \cdot g(\valueSem{S_2}(\pv',\tr')) \,\mu_\trans(\mathrm{d} \tr') \Big)\\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		Here $\omega_i$ is the $i$-th element of the sequence $\omega=\{\Xi_n\}_{n\in\Nset}$, i.e., $\omega_i:=\Xi_i$. $\omega_T$ is the last element of $\omega$, and $\Xi'=(\loc',\pv')$. The third and fourth equalities follow from the induction hypothesis, and the last equality from \cref{lem:density-composition}.
		\item Case $P\equiv``\textbf{while}\ B\ \textbf{do}\ S\ \textbf{od}"$. Assume the next state corresponding to the entry of the loop (resp. the exit of the loop) is $\Xi_1=(\loc_1,\pv)$ (resp. $\Xi_2=(\loc_2,\pv)$). Then we obtain that
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} 
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \expectdist{\Xi_1}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} +  [\llbracket B \rrbracket(\pv)=false]\cdot \expectdist{\Xi_2}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} \\
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T)\probm_{\Xi_1}(\mathrm{d}\omega) +[\llbracket B \rrbracket(\pv)=false] \cdot g(\pv)  \\
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \int_{\mathcal{T}_{S;P, \pv}} \weightSem{S;P}(\pv,\tr) \cdot g(\valueSem{S;P}(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
			& &	+[\llbracket B \rrbracket(\pv)=false] \cdot  \int_{\mathcal{T}_{\textbf{skip}, \pv}} [\tr=[]] \cdot g(\valueSem{\textbf{skip}}(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
	\end{itemize}
\end{proof} 


\begin{theorem}\label{thm:posterior-weight}
	Given a probabilistic program $P$, an initial program state $\Xi=(\loc,\pv)\in\Lambda$ and a measurable set $U\in\Sigma_{\Rset}$, it holds that $\expectdist{\Xi}{\widehat{w}_T\cdot [\widehat{\pv}_T\in \calU]}=\measureSem{P}_{\pv}(\calU)$. Moreover, the expected weight $\expectdist{\Xi}{\widehat{w}_T}$ is equivalent to the normalising constant $Z_P$.
\end{theorem}

\begin{proof}
	We instantiate \cref{prop:posterior-weight} with $g(x)=[x\in \calU]$. For any initial program state $\Xi=(\loc,\pv)$, we have that
	\begin{eqnarray*}
		\expectdist{\pv}{\widehat{w}_T\cdot [\widehat{\pv}_T\in \calU]} 
		&=&  \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot [\valueSem P(\pv,\tr)\in \calU] \,\mu_\trans(\mathrm{d} \tr)  \\
		&=&  \int_{\mathcal{T}_{P, \pv, U}} \weightSem P(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr) \\
		&=& \measureSem{P}_{\pv}(\calU)
	\end{eqnarray*}
	We prove $\expectdist{\pv}{\widehat{w}_T}=Z_P$ by setting $\calU=\Rset$.
\end{proof}

By \cref{thm:posterior-weight}, we show the equivalence  between posterior distributions under sampling-based semantics and expected weights under transition-based semantics. In the following, we will focus on developing approach es to infer interval bounds on expected weights.

\subsection{Proof for \cref{prop:unnorm-norm}}\label{app:sec2-prop}
\changed[pw]{
\begin{proposition}\label{prop:norm}
  Given a WPTS in the form of \eqref{eq:wpts}, the interval-bound analysis of $Z_\Pi$ can be reduced to the interval-bound analysis of the expected weight $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$ for all $\pv\in \calV$.
\end{proposition}
}
\begin{proof}
Fix a WPTS $\Pi$ in the form of \eqref{eq:wpts}.
% and a measurable set $\calU\in\Sigma_{\Rset^{|\pvars|}}$,
By \cref{def:npd}, the normalising constant $Z_\Pi=\measureSem{\Pi}(\Rset^{|\pvars|})=\int_{\calV} \measureSem{\Pi}_{\pv}(\Rset^{\pvars})\cdot \mu_{\mathrm{init}}(\mathrm{d} \pv)$ is the integral of the expected weight function $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$ over the finite set $\calV=\supp{\mu_{\mathrm{init}}}$. Assume there exist two integrable bound functions $Up,Lw$ such that $Lw(\pv)\le \llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})\le Up(\pv)$ for all $\pv\in\calV$. 
Then the interval bounds $l_Z,u_Z$ for $Z_\Pi$ can be derived by the integrals of $Up,Lw$ over $\calV$, that is, 
\begin{align*}
l_Z:= \int_{\calV} Lw(\pv) \cdot \mu_{\mathrm{init}}(\mathrm{d} \pv) \le \measureSem{\Pi}(\Rset^{|\pvars|})=\int_{\calV} \measureSem{\Pi}_{\pv}(\Rset^{|\pvars|})\cdot \mu_{\mathrm{init}}(\mathrm{d} \pv)\le \int_{\calV} Up(\pv) \cdot \mu_{\mathrm{init}}(\mathrm{d} \pv)=:u_Z .
\end{align*}
\end{proof}



\changed[pw]{
\begin{proposition}\label{prop:unnorm}
  Given a WPTS in the form of \eqref{eq:wpts} and any measurable set $\calU\subsetneqq \Rset^{|\pvars|}$, the interval-bound analysis of $\llbracket \Pi\rrbracket (\calU)$ can be reduced to the interval-bound analysis of the expected weight $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$ for all $\pv\in \calV'\subseteq \calV$. 
  %or $\pv\in \calU(ret)$ where $\calU(ret)$ is the interval.
\end{proposition}
}
\begin{proof}
Fix a WPTS $\Pi$ in the form of \eqref{eq:wpts} and a measurable set $\calU\subsetneqq \Rset^{|\pvars|}$. By \cref{def:npd}, $\llbracket \Pi\rrbracket (\calU)=\int_{\calV} \llbracket \Pi\rrbracket_{\pv}(\calU)\cdot\mu_{\mathrm{init}}(\mathrm{d}\pv)$. By \cref{def:exp-wt}, $\llbracket \Pi\rrbracket_{\pv}(\calU)=\expectdist{\pv}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T}$ for any initial program valuation $\pv$. As $\widehat{\pv}_T$ is the random (vector) variable of the program valuation at termination and $\calU\neq \Rset^{|\pvars|}$, it is not possible to track the valuation of $\widehat{\pv}_T$ for each program run starting from $\pv$, which makes it challenging to compute interval bounds for $\llbracket \Pi\rrbracket_{\pv}(\calU)$ directly. However, in practice, we only care about the posterior distribution that whether the return variable falls into some target sets of interest.\footnote{ For the sake of simplicity, we consider that the WPTS has only one return variable $ret\in\pvars$ of interest, but it can be straightforwardly extended to general cases.} That is, the set $\calU$ is defined such that $[\widehat{\pv}_T\in \calU]=1$ iff $\widehat{\pv}_T[ret]\in\calU(ret)$ where $\calU(ret)$ is the element in $\calU$ that corresponds to the set w.r.t. the return variable. Below we distinguish the computation into two cases.
\begin{itemize}
	\item[Case 1.] If the value of $ret$ is determined once and unaffected by loop iterations, then $[\widehat{\pv}_T[ret]\in\calU(ret)]$ is equivalent to $[\valin[ret]\in\calU(ret)]$. We can conclude that $\llbracket \Pi\rrbracket_{\valin}(\calU)=\\ \expectdist{\valin}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T}= \expectdist{\valin}{\widehat{w}_T}=\llbracket \Pi\rrbracket_{\valin}(\Rset^{|\pvars|})$ if $\valin[ret]\in\calU(ret)$, and $0$ otherwise.
	\item[Case 2.] If the value of $ret$ is affected during loop iterations, then we can construct a WPTS $\Pi_\calU$ by adding a conditional branch of the form ``\textbf{if} $\pv_T\notin\calU$ \textbf{then} \textbf{score}($0$) \textbf{fi}'' immediately before the termination of $\Pi$. And we can prove that $\llbracket \Pi\rrbracket_{\valin}(\calU)$ is equivalent to $\llbracket \Pi_\calU\rrbracket_{\valin}(\Rset^{|\pvars|})$ for all $\valin\in\calV$.
\end{itemize}
The correctness of Case 1 above is straightforward to be proved, while the correctness of Case 2 follows from the proposition below.
\end{proof}




\begin{proposition}
	Given a WPTS $\Pi$ in the form of \eqref{eq:wpts} and a measurable set $\calU\subsetneqq \Rset^{|\pvars|}$, for any initial program valuation $\valin\in\calV$, the expected weight $\measureSem{\Pi}_{\valin}(\calU)$ is equivalent to $\measureSem{\Pi_\calU}_{\valin}(\Rset^{|\pvars|})$ where $\Pi_\calU$ is constructed by the method described in Case 2 above.
\end{proposition}

\begin{proof}
Fix a WPTS $\Pi$ in the form of \eqref{eq:wpts}, a measurable set $\calU\subsetneqq \Rset^{|\pvars|}$ and a new WPTS $\Pi_\calU$ constructed by the method described in Case 2. Let the termination location of $\Pi$ be $\lout$ and the termination location of $\Pi_\calU$ be $\lout'$. Define a predicate $\Phi_\calU$ over $\pvars$  such that $\calU=\{\pv\mid \pv\models\Phi_\calU  \}$. Then $\Pi_\calU$ has two additional transitions than $\Pi$, i.e., $\langle \lout, \Phi_\calU, F_{\top}\rangle$ with $F_\top:=\langle \lout',1, \mbox{\sl id},\overline{0} \rangle$ where $\mbox{\sl id}$ is the identity function and $\overline{0}$ is the constant function that always takes the value $0$, and $\langle \lout, \neg\Phi_\calU, F_{\bot}\rangle$ with $F_\bot:=\langle \lout',1, \mbox{\sl id},\overline{1} \rangle$ where $\overline{1}$ is the constant function that always takes the value $1$. Let the termination time of $\Pi$ be $T$ and the termination time of $\Pi_\calU$ be $T'$.
%, we can have that the termination time of $\Pi_\calU$ is $T'=T+1$.
Therefore, for the random variable of the multiplicative likelihood weight at termination, we have that $\widehat{w}_{T'}=[\widehat{\pv}_{T'}\in \calU]\cdot \widehat{w}_{T}+[\widehat{\pv}_{T'}\notin \calU]\cdot 0$.
By \cref{def:exp-wt},
	\begin{align*}
	    \llbracket \Pi\rrbracket_{\valin}(\calU)&=\expectdist{\valin}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T} \\
     &= \expectdist{\valin}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T+[\widehat{\pv}_T\notin \calU]\cdot 0}  \\
     &=\expectdist{\valin}{[\widehat{\pv}_{T'}\in \calU]\cdot \widehat{w}_{T}+[\widehat{\pv}_{T'}\notin \calU]\cdot 0} \\
     &=\expectdist{\valin}{\widehat{w}_{T'}} \\
     &=\expectdist{\valin}{[\widehat{\pv}_{T'}\in\Rset^{|\pvars|}]\cdot\widehat{w}_{T'}}\\
     &= \llbracket \Pi_\calU \rrbracket_{\valin}(\Rset^{|\pvars|})
	\end{align*}
where the third equality is derived from the fact that $\widehat{\pv}_{T'}=\widehat{\pv}_{T}$ and the fourth equality is obtained from the fact that $[\widehat{\pv}_{T'}\in\Rset^{|\pvars|}]\equiv 1$.
\end{proof}



Based on \cref{prop:norm} and \cref{prop:unnorm}, in the following (\cref{sec:math}) we develop approaches to obtain interval bounds for expected weights in the form $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$ where $\pv$ is an initial program valuation.


\section{Supplementary Material for Section~\ref{sec:math}}\label{app:sec4}

\subsection{Basics of Fixed Point Theory}\label{app:fixed-point-materials}

%We first recall several basic concepts and theorems from fixed-point theory. 

Given a partial order $\sle$ on a set $K$ and a subset $K' \subseteq K,$ an \emph{upper bound} of $K'$ is an element $u \in K$ that is no smaller than every element of $K'$, i.e.,~$\forall k' \in K'.~k' \sle u.$ Similarly, a \emph{lower bound} for $K'$ is an element $l$ that is no greater than every element of $K',$ i.e.~$\forall k' \in K'.~l \sle k'.$ The \emph{supremum} of $K',$ denoted by $\bigsqcup K'$, is an element $u^* \in K$ such that $u^*$ is an upper-bound of $K'$ and for every upper bound $u$ of $K',$ we have $u^* \sle u.$ Similarly, the \emph{infimum} $\bigsqcap K'$ is a lower bound $l^*$ of $K'$ such that for every lower-bound $l$ of $K',$ we have $l \sle l^*.$ We define $\bot\!:=\!\bigsqcap K$ and $\top\!:=\!\bigsqcup K.$ In general, suprema and infima may not exist.

A partial order $(K, \sle)$ is called a \emph{complete lattice} if every subset $K'\subseteq K$ has a supremum and an infimum.
Given a partial order $(K, \sle)$, a function $f: K \to K$ is called \textit{monotone} if for every $k_1 \sle k_2$ in $K$, we have $f(k_1) \sle f(k_2).$

Given a complete lattice $(K, \sle),$ a function $f: K \to K$ is called \emph{continuous} if for every increasing chain $k_0 \sle k_1 \sle \ldots$ in $K,$ we have $f(\bigsqcup \{k_n\}_{n=0}^\infty) = \bigsqcup \{f(k_n)\}_{n=0}^\infty,$ and \emph{cocontinuous} if for every decreasing chain $k_0 \sge k_1 \sge \ldots$ of elements of $K,$ we have $f(\bigsqcap \{k_n\}_{n=0}^\infty) = \bigsqcap \{f(k_n)\}_{n=0}^\infty.$

Given a complete lattice $(K, \sle)$ and a function $f: K \to K,$ an element $k \in K$ is called a \emph{fixed-point} if $f(k) = k.$ Moreover, $k$ is a \emph{pre fixed-point} if $f(k) \sle k$ and a \emph{post fixed-point} if $k\sle f(k)$. The \emph{least fixed-point} of $f$, denoted by $\lfp f,$ is the fixed-point that is no greater than every fixed-point under $\sle.$ Analogously, the \emph{greatest fixed-point} of $f$, denoted by $\gfp f$, is the fixed-point that is no smaller than all fixed-points.


\begin{theorem}[\textit{Kleene}~\cite{Sangiorgibook}]
\label{thm:kleene}
Let $(K, \sle)$ be a complete lattice and $f: K \to K$ be an continuous function. Then, we have
	$$\textstyle \lfp\ f = {\textstyle \mathop{\bigsqcup}_{i \ge 0}} \left\{f^{(i)}(\bot)\right\}.$$
Analogously, if $f$ is cocontinuous, then we have
	$$\textstyle \gfp\ f = {\textstyle \mathop{\bigsqcap}_{i \ge 0}} \left\{f^{(i)}(\top)\right\}.$$
\end{theorem}


\subsection{Proofs for Our Fixed-Point Approach}\label{app:fixedpoint}

%\noindent \textbf{Theorem}~\ref{thm:lfp}.
\begin{theorem}
 The expected-weight function $\mbox{\sl ew}$ is the least fixed point of the expected-weight transformer $\ewt$. 	
\end{theorem}


\begin{proof}
Define the step-bounded weight random variable $\widehat{w}^n_{(\loc, \pv)}$ starting from any program state $\Xi=(\loc,\pv)$  for a step bound $n\in\mathbb{N}$ by 

\[
\widehat{w}^n_{(\loc, \pv)}(\omega)=\begin{cases} \widehat{w}_{(\loc, \pv)}(\omega) & \mbox{if $T(\omega)\le n$} \\ 0 & \mbox{otherwise}\end{cases}\enskip.
\]
Since we always assume that the underlying WPTS is almost-surely terminating, 
it follows that the sequence of random variables $\{\widehat{w}^n\}_{n\in\Nset}$ converges non-decreasingly to $W$. 

Given any program state $\Xi=(\loc,\pv)$ with a unique transition $\tau = \langle \loc, \phi_\tau, f_\tau \rangle$ satisfying $\pv\models\phi_{\tau}$, define the step-bounded expected-weight function $\mbox{\sl ew}^n$ by $\mbox{\sl ew}^n(\loc,\pv)=\expectdist{(\loc,\pv)}{\widehat{w}^n}$. 
\changed[ts]{Without loss of generality, we assume there is only one fork $f_\tau$ in this transition. }Assume the next sampling valuation from $\Xi$ is $\rv_0$ and the next program state is $\Xi'=(\loc',\pv')$, i.e., $\pv' = f_\tau(\pv,\rv_0)$. \changed[ts]{Following the symbols in \cref{sec:prelim}, we denote the probability space of the WPTS $\Pi$ starting from $(\loc, \pv)$, i.e., the program runs starting from $\Xi=(\loc, \pv)$ as $\pspace_{\Xi}$}. By Tonelli-Fubini Theorem, we have that for all $n\ge 0$,
\begin{eqnarray*}
\mbox{\sl ew}^{n+1}(\loc,\pv) & = & \int \widehat{w}^{n+1}_{(\loc, \pv)} \,\mathrm{d}\probm_{\Xi} \\
&=& \int \widehat{w}^{n+1}_{(\loc, \pv)} \,\mathrm{d}(\mathcal{D}_{\rv_0} \times \probm_{\Xi'}) \\
&=& \int \wet \cdot W^n_{(\loc', \pv')}(\omega)\,\mathrm{d}(\mathcal{D}_{\rv_0} \times \probm_{\Xi'}) \\
&=& \int_{\rv_0}\int_{\omega_{\Xi'}} \wet\cdot W^n_{(\loc', \pv')}(\omega)\,\mathrm{d}\probm_{\Xi'}\,\mathrm{d}\mathcal{D}_{\rv_0}\\
&=& \int_{\rv_0}\wet\cdot\left(\int_{\omega_{\Xi'}}\widehat{w}^n_{(\loc', \pv')}(\omega)\,\mathrm{d}\probm_{\Xi'}\right)\,\mathrm{d}\mathcal{D}_{\rv_0}\\
&=& \int_{\rv_0}\wet\cdot \mbox{\sl ew}^n(\loc', \pv') \,\mathrm{d}\mathcal{D}_{\rv_0}\\
&=& \expectdist{\rv_0}{ \wet\cdot \mbox{\sl ew}^n(\loc', \pv')} \\
&=& \ewt(\mbox{\sl ew}^n)(\loc,\pv) 
\end{eqnarray*}
By applying MCT to the both sides of the equality above, we have that 
\[
\mbox{\sl ew}(\loc,\pv)=\ewt(\mbox{\sl ew})(\loc,\pv).
\]
This shows that $\mbox{\sl ew}$ is a fixed point of $\ewt$. Furthermore, given any fixed point $h$ of 
$\ewt$, since (i) $\mbox{\sl ew}^0\le h$ and (ii) $\mbox{\sl ew}^n\le h$ implies $\mbox{\sl ew}^{n+1}=\ewt(\mbox{\sl ew}^n)\le \ewt(h)=h$, one can prove by a straightforward induction on $n$ that $\mbox{\sl ew}^n\le h$ for all $n\ge 0$. It follows from $\mbox{\sl ew}=\lim\limits_{n\rightarrow\infty}\mbox{\sl ew}^n$ that $\mbox{\sl ew}$ is the least fixed point of $\ewt$. 
\end{proof}


In order to show the uniqueness of the fixed point, we first prove that $\ewt$ is both continuous and cocontinuous. 

\begin{proposition}\label{prop:continuity} If $M\in [0,\infty)$, then the expected-weight transformer $\ewt:\mathcal{K}_M\to \mathcal{K}_M$ is both continuous and cocontinous.
\end{proposition}

\begin{proof}
	We first prove that $\ewt$ is well-defined. Given an arbitrary $h\in \mathcal{K}_M$, for any $\Xi=(\loc,\pv)\in\Lambda$,
	\begin{itemize}
		\item When $\loc=\lout$, $\ewt(h)(\loc,\pv)=1$.
		\item When $\loc\neq \lout$, for a unique transition $\tau = \langle \loc, \phi_{\tau}, f_{\tau}, \loc' \rangle$ such that $\pv\models\phi_{\tau}$,
		\begin{align*}
		\ewt(h)(\loc,\pv)&=\expectdist{\rv}{h(\loc',f_{\tau}(\pv,\rv))\cdot W(\loc,\pv)} \\
		&\le M\cdot maxscore \\
		&< \infty  \\
		\end{align*}
	\end{itemize}
where $maxscore$ is the maximum of $W$ given any state $\Xi$.
As $W$ is a non-negative function, we can prove that $\ewt(h)(\loc,\pv)\ge 0$. Thus, $\ewt$ is well defined.
Next, we prove that $\ewt$ is monotone. Given any two functions $h_1,h_2\in \mathcal{K}_M$ such that $h_1\le h_2$, by case analysis on $(\loc,\pv)$,
\begin{itemize}
	\item If $\loc=\lout$, $\ewt(h_1)(\loc,\pv)=1=\ewt(h_2)(\loc,\pv)$.
	\item If $\loc\neq\lout$, given a unique transition $\tau = \langle \loc, \phi_{\tau}, f_{\tau}, \loc' \rangle$ such that $\pv\models\phi_{\tau}$,
	\begin{align*}
	\ewt(h_1)(\loc,\pv)&= \expectdist{\rv}{h_1(\loc',f_{\tau}(\pv,\rv))\cdot W(\loc,\pv)} \\&\le \expectdist{\rv}{h_2(\loc',f_{\tau}(\pv,\rv))\cdot W(\loc,\pv)} \\
	&= \ewt(h_2)(\loc,\pv)
	\end{align*}
\end{itemize}
Therefore, $\ewt(h_1)\le \ewt(h_2)$, hence it is monotone.
Then we prove upper continuity of $\ewt$. Choose any increasing chain $h_0\sle h_1\sle h_2\sle \cdots$ and do another case analysis on $(\loc,\pv)$:
\begin{itemize}
	\item If $\loc = \lout$, then 
	\[
	\ewt(\mathop{\bigsqcup}\limits_{n\ge 0}\left\{h_n\right\})(\loc,\pv)=1=\mathop{\bigsqcup}\limits_{n\ge 0}\left\{\ewt(h_n)\right\}(\loc,\pv).
	\]
	\item Otherwise, for a unique transition $\tau = \langle \loc, \phi_{\tau}, f_{\tau} \rangle$ such that $\pv\models\phi_{\tau}$:
	\begin{align*}
	&\ewt(\mathop{\bigsqcup}\limits_{n\ge 0}\left\{h_n\right\})(\loc,\pv) \\
	=& \expectdist{\rv}{ \wet_j(\pv,\rv) \cdot (\mathop{\bigsqcup}\limits_{n\ge 0}\left\{h_n\right\})(\loc',f_{\tau}(\pv,\rv))} \\
	=& \expectdist{\rv}{\mathop{\text{sup}}\limits_{n\ge 0}\left\{h_n (\loc',f_{\tau}(\pv,\rv))\right\}} \\
	=& \expectdist{\rv}{\mathop{\text{lim}}\limits_{n\to \infty}\left\{h_n (\loc',f_{\tau}(\pv,\rv))\right\}} \\
	\overset{\mathrm{MCT}}{=} &\mathop{\text{lim}}\limits_{n\to \infty} \expectdist{\rv}{h_n (\loc',f_{\tau}(\pv,\rv))} \\
	=& \mathop{\text{lim}}\limits_{n\to \infty} \ewt(h_n)(\loc,\pv)\\
	= &\sup_{n\ge 0}\left\{\ewt(h_n)(\loc,\pv)\right\}\\
	= &\mathop{\bigsqcup}\limits_{n\ge 0}\left\{\ewt(h_n)\right\}(\loc,\pv)
	\end{align*}
\end{itemize}
The ``MCT'' above denotes the monotone convergence theorem. A similar argument establishes cocontinuity for integrable $h_0$ and decreasing chains.
\end{proof}

Then the uniqueness follows from Theorem~\ref{thm:kleene}. 

\noindent \textbf{Theorem}~\ref{thm:lfp}.
%\begin{theorem}\label{thm:lfp}
Let $\Pi$ be a non-score-recursive WPTS whose weights are bounded in $[-M,M]$ for a finite $M\ge 1$. Then the expected-weight function $\mbox{\sl ew}$ is the least fixed point of the expected-weight transformer $\ewt$ in the complete lattice $(\mathcal{K}_M,\le)$. 
%when $M\ge 1$. 
Furthermore, if the WPTS $\Pi$ is AST, then the function $\mbox{\sl ew}$ is the unique fixed point of the higher-order function $\ewt$ in $(\mathcal{K}_M,\le)$ when $M\ge 1$.
%\end{theorem}

\begin{proof}
The proof follows similar arguments in \cite[Theorem 4.4]{DBLP:conf/pldi/WangS0CG21}. By Proposition~\ref{prop:continuity}, we have that for every state $\Xi=(\loc,\pv)$, 

\begin{itemize}
\item $\lfp\ \ewt(\loc, \pv) = \lim\limits_{n\rightarrow\infty} \ewt^n(\bot)(\loc, \pv)$, and 
\item $\gfp\ \ewt(\loc, \pv) = \lim\limits_{n\rightarrow\infty} \ewt^n(\top)(\loc, \pv)$. 
\end{itemize}

By the definition of $\ewt_M^n$ and Proposition~\ref{prop:continuity}, we have that 

\begin{itemize}
\item $\ewt^n(\bot)(\loc, \pv) = \expectdist{\Xi}{W\cdot [T\le n]}-M\cdot\mathbb{P}(T>n)$, and 
\item $\ewt^n(\top)(\loc, \pv) = \expectdist{\Xi}{W\cdot [T\le n]}+M\cdot \mathbb{P}(T>n)$. 
\end{itemize}

Recall that we assume the underlying PTS to be almost-surely terminating. Hence, $\lim\limits_{n\rightarrow\infty} \mathbb{P}(T >n) = \mathbb{P}(T=\infty)= 0$. 
It follows that $\lfp\ \ewt(\loc, \pv) = \gfp\ \ewt(\loc, \pv)$, i.e., the fixed point is unique. 
\end{proof}

\subsection{Proof for the OST Variant}\label{app:ost-variant-proof}

Theorem~\ref{thm:ost-variant} (OST Variant)
Let $\{X_n\}_{n=0}^\infty$ be a supermartingale adapted to a filtration $\mathcal{F} = \{\mathcal{F}_n\}_{n=0}^\infty$, and $\kappa$ be a stopping time w.r.t. the filtration $\mathcal{F}$. 
Then the following condition $(\mho)$ is sufficient to ensure that $\expv\left(|X_\kappa|\right)<\infty$ and %$\expv\left(X_\kappa\right) = \expv(X_0)$ 
$\expv\left(X_\kappa\right)\le\expv(X_0)$:
\begin{itemize}
\item[$(\mho)$] There exist integers $b_1,b_2>0$ and real numbers $c_1>0,c_2>c_3> 0$ such that (i) $\probm(\kappa>n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for sufficiently large $n \in \Nset$, and (ii) for all $n \in \Nset$, $\left\vert X_{n+1}-X_n \right\vert \le b_1\cdot n^{b_2}\cdot e^{c_3\cdot n}$ holds almost surely.
\end{itemize}



\begin{proof}
% \changed[ts]{delete: We only prove the ``$\le$'' case, the ``$=$'' case is similar. }
 For every $n\in\Nset_0$,
	\begin{eqnarray*}
		\left|X_{\kappa\wedge n}\right|&=& \left|X_0+\sum_{k=0}^{\kappa\wedge n-1} \left(X_{k+1}-X_k\right)\right| \\
		&=& \left|X_0+\sum_{k=0}^\infty \left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k\wedge n>k}\right|\\
		&\le& \left|X_0\right|+\sum_{k=0}^\infty \left|\left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k\wedge n>k}\right| \\
		&\le& \left|X_0\right|+\sum_{k=0}^\infty \left|\left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k}\right|\enskip. \\
	\end{eqnarray*}
	
	Then
	\begin{eqnarray*}
		& &   \expv\left(\left|X_0\right|+\sum_{k=0}^\infty \left|\left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k}\right|\right) \\
		&=& \mbox{(By Monotone Convergence Theorem)} \\
		& & \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty \expv\left(\left|\left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k}\right|\right) \\
		&=& \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty \expv\left(\left|X_{k+1}-X_k\right|\cdot \mathbf{1}_{\kappa>k}\right) \\
		&\le & \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty \expv\left( \lambda\cdot k^d\cdot e^{c_3\cdot k}\cdot \mathbf{1}_{\kappa>k}\right) \\
		&=& \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty \lambda \cdot k^d \cdot e^{c_3\cdot k}\cdot \probm\left(\kappa>k\right) \\
		&\le & \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty \lambda \cdot k^d \cdot e^{c_3\cdot k} \cdot c_{1}\cdot e^{-c_{2}\cdot k} \\
		&=& \expv\left(\left|X_0\right|\right)+\lambda\cdot c_1 \cdot \sum_{k=0}^\infty   k^d\cdot e^{-(c_2-c_3)\cdot k} \\
		&<& \infty\enskip.
	\end{eqnarray*}
	where the first inequality is obtained by Condition (ii), and the second inequality is derived from Condition (i).
	
	
	Therefore, by Dominated Convergence Theorem
	and the fact that $X_\kappa=\lim\limits_{n\rightarrow\infty} X_{\kappa\wedge n}$ a.s.,
	\[
	\expv\left(X_\kappa\right)=\expv\left(\lim\limits_{n\rightarrow\infty} X_{\kappa\wedge n}\right)=\lim\limits_{n\rightarrow\infty}\expv\left(X_{\kappa\wedge n}\right)\enskip.
	\]
	Finally, the result follows from properties for the stopped process $\{X_{\kappa\wedge n}\}_{n\in\Nset_0}$ that 
	\[
	\expv\left(X_\kappa\right) \le \expv\left(X_0\right)\enskip.
	\]
\end{proof}



\subsection{Proofs for Our OST-Based Approach}\label{app:ost}

\noindent\textbf{Theorem}~\ref{thm:puwf-normalizing}. [Upper Bounds on Expected Weights] 
Let $\Pi$ be a score-recursive WPTS that has the bounded update property. Suppose that there exist real numbers $c_1>0$ and $c_2>c_3>0$ such that 

\begin{itemize}
\item $\probm(\kappa >n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for sufficiently large $n\in\Nset$, and 
\item for each score function $\wet$ in $\Pi$ we have $|\wet|\le e^{c_3}$. 
\end{itemize}
Then for any PUWF $h$ over $\Pi$, we have that $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\le h(\lin,\valin)$ for any initial state $(\lin,\valin)$.

\begin{proof}
	Define the stochastic process $\{X_n\}_{n=0}^\infty$ as $X_n:=h(\loc_n,\pv_n)$ where $(\loc_n,\pv_n)$ is the program state at the $n-$th step of a program run. Then construct a stochastic process $\{Y_n\}_{n=0}^\infty$ such that $Y_n:=X_n\cdot \prod_{i=0}^{n-1} W_i$ where $W_i$ is the weight at the $i-$th step of the program run. According to Condition (C1), we have that $\expect{X_{n+1}\cdot W_n|\mathcal{F}_n}\le X_n$. Therefore, by the “take out what is known” property of conditional
	expectation (see \cite{williams1991probability}), it follows that
	
	\begin{eqnarray*}
		& &\expect{X_{n+1}\cdot \prod_{i=0}^{n}W_i|\mathcal{F}_n} \le X_n\cdot \prod_{i=0}^{n-1} W_i \\
		&\Leftrightarrow& \expect{Y_{n+1}|\mathcal{F}_n}\le \expect{Y_n}, 
	\end{eqnarray*}
	which means that $\expect{Y_{n+1}}\le \expect{Y_n}$ from the basic property of conditional expectation. By an easy induction on $n$, we have that $\expect{Y_n}\le \expect{Y_0}<\infty$ for all $n\ge 0$, thus the conditional expectation is also taken in the normal sense as each $Y_n$ is indeed integrable. Hence, $\{Y_n\}_{n=0}^\infty$ is a supermartingale. Moreover, we have from the bounded-update property that $|X_{n+1}|\le \zeta \cdot (n+1)^d$ for a real number $\zeta>0$. By definition, we obtain that for sufficiently large $n$,
	\begin{eqnarray*}
		\left|Y_{n+1}-Y_n\right|&=&  \left|X_{n+1}\cdot \prod_{i=0}^{n} W_i-X_n\cdot \prod_{i=0}^{n-1} W_i \right|  \\
		&\le& \left\vert X_{n+1}\cdot \prod_{i=0}^{n} W_i\right\vert+ \left|X_n\cdot \prod_{i=0}^{n-1} W_i\right|\\
		&<& e^{c_3\cdot n}\cdot (|X_{n+1}|+|X_n|) \\
		&\le& e^{c_3\cdot n}\cdot [\zeta \cdot (n+1)^d+\zeta \cdot n^d] \\
		&\le& \lambda \cdot n^d\cdot e^{c_3\cdot n}
	\end{eqnarray*}
	where the first inequality is induced by the triangle inequality, and the second inequality is derived from the bounded stepwise weight condition such that each $W_i\in [0,e^{c_3}]$ and the fact $W_0=1$.
	By applying the OST variant (Theorem \ref{thm:ost-variant}), we obtain that $\expect{Y_T}\le \expect{Y_0}$. By definition and Condition (C2),
	\begin{eqnarray*}
		Y_T&=&h(\loc_T,\pv_T)\cdot \prod_{i=0}^{T-1} W_i \\
		&=& h(\lout,\pv_T)\cdot  \prod_{i=0}^{T-1} W_i \\
		&=& \prod_{i=0}^{T-1} W_i \\
	\end{eqnarray*}  
	Finally, we have that $\expectdist{\Xi}{W_{\infty}}=\expect{\prod_{i=0}^{T-1} W_i}\le \expect{Y_0}=h(\lin,\valin)$.
\end{proof}

\noindent\textbf{Theorem}~\ref{thm:puwf-normalizing}. [Lower Bounds on Expected Weights] 
Let $\Pi$ be a score-recursive WPTS that has the bounded update property. Suppose that there exist real numbers $c_1>0$ and $c_2>c_3>0$ such that 

\begin{itemize}
\item $\probm(\kappa >n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for sufficiently large $n\in\Nset$, and 
\item for each score function $\wet$ in $\Pi$ we have $|\wet|\le e^{c_3}$. 
\end{itemize}
Then for any PLWF $h$ over $\Pi$, we have that $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\ge h(\lin,\valin)$ for any initial state $(\lin,\valin)$.



\begin{proof}
	Define the stochastic process $\{X_n\}_{n=0}^\infty$ as $X_n:=h(\loc_n,\pv_n)$ where $(\loc_n,\pv_n)$ is the program state at the $n-$th step of a program run. Then construct a stochastic process $\{-Y_n\}_{n=0}^\infty$ such that $-Y_n:=-X_n\cdot \prod_{i=0}^{n-1} W_i$ where $W_i$ is the weight at the $i-$th step of the program run. According to Condition (C1'), we have that $\expect{-X_{n+1}\cdot W_n|\mathcal{F}_n}\le -X_n$. Therefore, by the “take out what is known” property of conditional
	expectation (see \cite{williams1991probability}), it follows that
	
	\begin{eqnarray*}
		& &\expect{-X_{n+1}\cdot \prod_{i=0}^{n}W_i|\mathcal{F}_n} \le -X_n\cdot \prod_{i=0}^{n-1} W_i \\
		&\Leftrightarrow& \expect{-Y_{n+1}|\mathcal{F}_n}\le \expect{-Y_n}, 
	\end{eqnarray*}
	which means that $\expect{-Y_{n+1}}\le \expect{-Y_n}$ from the basic property of conditional expectation. By an easy induction on $n$, we have that $\expect{-Y_n}\le \expect{-Y_0}<\infty$ for all $n\ge 0$, thus the conditional expectation is also taken in the normal sense as each $Y_n$ is indeed integrable. Hence, $\{-Y_n\}_{n=0}^\infty$ is a supermartingale. Moreover, we have from the bounded-update property that $|X_{n+1}|\le \zeta \cdot (n+1)^d$ for a real number $\zeta>0$. By definition, we obtain that for sufficiently large $n$,
	\begin{eqnarray*}
		\left|-Y_{n+1}-(-Y_n)\right| &=& \left|X_{n+1}\cdot \prod_{i=0}^{n} W_i-X_n\cdot \prod_{i=0}^{n-1} W_i \right|  \\
		&\le& \left\vert X_{n+1}\cdot \prod_{i=0}^{n} W_i\right\vert+ \left|X_n\cdot \prod_{i=0}^{n-1} W_i\right|\\
		&<& e^{c_3\cdot n}\cdot (|X_{n+1}|+|X_n|) \\
		&\le& e^{c_3\cdot n}\cdot [\zeta \cdot (n+1)^d+\zeta \cdot n^d] \\
		&\le& \lambda \cdot n^d\cdot e^{c_3\cdot n}
	\end{eqnarray*}
	where the first inequality is induced by the triangle inequality, and the second inequality is derived from the bounded stepwise weight condition such that each $W_i\in [0,e^{c_3}]$ and the fact $W_0=1$.
	By applying the variant of Optional Stopping Theorem (Theorem \ref{thm:ost-variant}), we obtain that $\expect{-Y_T}\le \expect{-Y_0}$, so $\expect{Y_T}\ge \expect{Y_0}$. By definition and Condition (C2'),
	\begin{eqnarray*}
		-Y_T&=&-h(\loc_T,\pv_T)\cdot \prod_{i=0}^{T-1} W_i \\
		&=& -h(\lout,\pv_T)\cdot  \prod_{i=0}^{T-1} W_i \\
		&=& -\prod_{i=0}^{T-1} W_i \\
	\end{eqnarray*}  
	Finally, we have that $\expectdist{\Xi}{W_{\infty}}=\expect{\prod_{i=0}^{T-1} W_i}\ge \expect{Y_0}=h(\lin,\valin)$.
\end{proof}




\subsection{Correctness of Truncation}\label{app:truncation-correctness}
\smallskip\noindent\textbf{Theorem}~\ref{thm:upperlower}. 
\changed[fu]{Let 
%$\Pi = (\pvars, \rvars, \mathcal{D}, L,\transset, \lin,\valin, \lout,\win)$ 
$\Pi$ be a WPTS in the form of \eqref{eq:wpts}, $\trunc$ a truncation function and $\calM$ an approximation function.}
Suppose that the following condition ($\ast$) holds:
\begin{itemize}
\item[($\ast$)] for each fork $F^{M,\sharp}=\langle \sharp, p, \mbox{\sl upd}, \calM\rangle$ in the truncated WPTS $\Pi_{\trunc,\calM}$ that is derived from
some fork $F=\langle \loc', p, \mbox{\sl upd}, \wet\rangle$ with the source location $\loc$ in the original WPTS (see sentence (b) in Definition~\ref{def:truncation}),  
%$\langle \loc, \phi, F_1,\dots, F_k \rangle\in\transset$ 
%from the construction of $\Pi_{\trunc,M}$, 
we have that $\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\le \calM(\pv)$ for all $\pv$ such that the state $(\loc,\pv)$ is reachable and \changed[fu]{$\pv\not\models\Phi_\trunc$}. 
\end{itemize}
Then $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\le \llbracket \Pi_{\trunc,\calM}\rrbracket_{\valin}(\Rset^{|\pvars|})$ for all initial program valuations $\valin$. 
Analogously, if it holds the condition ($\star$) which is almost the same as ($\ast$) except for that ``$\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\le \calM(\pv)$'' is replaced with ``$\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\ge \calM(\pv)$'', then we have $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\ge \llbracket \Pi_{\trunc,\calM}\rrbracket_{\valin}(\Rset^{|\pvars|})$ for all initial program valuations $\valin$.
  
\begin{proof}

We first prove that when every score function $\mathcal{M}$ in a $F^{\mathcal{M},\sharp}$ derived from a transition with source location $\loc$ is equal to the function $\ewt(\loc,-)$, we have that $\Pi_{\trunc,\mathcal{M}}$ is equal to $\Pi$. 
By \cref{thm:lfp}, the expected weight functions $\mbox{\sl ew}_\Pi$, $\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$
%of $\Pi$ and $\Pi_B$ 
are the least fixed point of the higher-order operator $\ewt$ defined in \cref{def:ewt}. 
We prove that both $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ and  $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ holds. Note that since we choose the scoring function to be the exact expected weight function of $\Pi$, it holds that 
$\mbox{\sl ew}_\Pi(-,\pv)=\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}(-,\pv)$ for all program valuations outside $B$. Thus, the nontrivial part is to consider program valuations inside the truncated range. 

\begin{itemize}
	\item $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$: To show that $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$, it suffices to observe that $\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ satisfies $\ewt_{\Pi}(\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}})=\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
 %the higher-order equation 
 %of the original WPTS $\Pi$. 
 Since $\mbox{\sl ew}_{\Pi}$ is the least fixed point of the higher order equation, we directly obtain that $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
	\item $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$: To show that $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$, it suffices to observe that 
	$\mbox{\sl ew}_{\Pi}$ (extended with the $\sharp$ location whose score function is $1$) satisfies the higher-order equation of $\Pi_{\trunc,\mathcal{M}}$. Thus, we directly have that $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
\end{itemize}

Then we prove the theorem. We only prove the upper-bound case, since the lower-bound case can be proved similarly. The proof follows from \cref{thm:lfp}. Denote $\bot$ as the bottom element of the complete lattice 
$(\mathcal{K}_M, \le)$. Then by \cref{thm:lfp}, we have that $\lim\limits_{n\rightarrow \infty} \ewt_\Pi^n(\bot)=\mbox{\sl ew}_{\Pi}$ and $\lim\limits_{n\rightarrow \infty} \ewt_{\Pi_{\trunc,\mathcal{M}}}^{n}(\bot)=\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
Since $\mbox{\sl ew}_{\Pi}(\pv)\le \mathcal{M}(\pv)$ for all $\pv\in \mbox{\sl exit}(\Pi)$ where $\mbox{\sl exit}(\Pi)$ is the set of all program valuations $\pv$ such that the state $(\lout,\pv)$ is reachable, one can perform a straightforward induction on $n$ that $\mbox{\sl ew}^{n,\mbox{\sl ew}_{\Pi}}_{\Pi_B}\le  \mbox{\sl ew}^{n,f}_{\Pi_B}$ for all $n$. 
%Hence, the result follows from Proposition~\ref{?}. 
\end{proof}


\section{Supplementary Material for Section~\ref{sec:algorithm}}\label{app:sec5}

\subsection{Error Analysis for Polynomial Approximation}


\begin{theorem}\label{app:score-error}
Let $\Pi$ be a non-score-recursive WPTS with score functions $g_1,\dots,g_k$ on the transitions to the termination location $\lout$. Suppose we have a non-negative real number $\epsilon$ and polynomials $g'_1,\dots,g'_k$ such that for all $x\in \mbox{\sl exit}(\Pi)$ and $1\le j\le k$, $|g'_j(x)-g_j(x)|\le \epsilon$. Then we have that $| \llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|}) - \llbracket \Pi' \rrbracket_{\pv}(\Rset^{|\pvars|}) | \le \epsilon$ for all initial program valuation $\pv$, where $\Pi'$ is obtained from $\Pi$ by replacing each $g_j$ ($1\le j\le k$) with $g'_j$. 
\end{theorem}

\begin{proof} 
By \cref{thm:lfp}, we have that 
$\lim\limits_{n\rightarrow \infty} \mbox{\sl ewt}^n_{\Pi}(\bot)=\mbox{\sl ew}_{\Pi}$ and 
$\lim\limits_{n\rightarrow \infty} \mbox{\sl ewt}^{n}_{\Pi'}=\mbox{\sl ew}_{\Pi'}$. 
Since $|g'_j-g_j|\le \epsilon$ for every $j$, one can perform a straightforward induction on $n$ to prove that for all $n\ge 0$, it holds that that $|\mbox{\sl ewt}^n_{\Pi}(\bot) - \mbox{\sl ewt}^n_{\Pi'}(\bot)|\le  \epsilon$. By the definition of $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$, we have that $| \llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|}) - \llbracket \Pi' \rrbracket_{\pv}(\Rset^{|\pvars|}) | \le \epsilon$.


\end{proof}


%\subsection{Application of  Positivstellensatz's}
\vspace{2em}

In \textbf{Step A4} of our algorithm, constraints are established in the form $\forall\pv\in P.(g(\pv)\ge 0)$ where $P$ is a polyhedron over program variables $\pvars$ and are grouped conjunctively. Thus, the key point is how to tackle the each such constraint. In our algorithm, we follow the exact treatment through the application of Putinar's and Handelman's Positivstellensatz in~\cite{putinar,handelman1988representing}. Below we describe the detailed application. 

\subsection{Application of Putinar's Positivstellensatz}\label{app:putinar}

We recall Putinar's Positivstellensatz below.  


\begin{theorem}[Putinar's Positivstellensatz~\cite{putinar}] \label{thm:putinar} Let $V$ be a finite set of real-valued variables and $g, g_1, \ldots, g_m \in \mathbb{R}[V]$ be polynomials over $V$ with real coefficients. Consider the set $\mathcal{S}:=\{\mathbf{x} \in \mathbb{R}^V\,\mid\, g_i(\mathbf{x}) \geq 0  \mbox{ for all }1\le i\le m \}$ which is the set of all real vectors at which every $g_i$ is non-negative. If (i)~there exists some $g_k$ such that the set $\{ \mathbf{x} \in \mathbb{R}^V ~\mid~ g_k(\mathbf{x}) \geq 0  \}$ is compact and (ii)~$g(\mathbf{x})>0$ for all $\mathbf{x} \in \mathcal{S}$, then we have that 
	\begin{equation} \label{eq:putinar}
	\textstyle g = f_0 + \sum_{i=1}^m f_i \cdot g_i
	\end{equation}
	for some polynomials $f_0,f_1\dots, f_m\in \mathbb{R}[V]$ such that each polynomial $f_i$ is the  a sum of squares (of polynomials in $\mathbb{R}[V]$), i.e.~$f_i = \sum_{j=0}^{k} q_{i,j}^2$ for polynomials $q_{i,j}$'s in $\mathbb{R}[V]$.
\end{theorem}

In this work, we utilize the sound form in \eqref{eq:putinar} for witnessing a polynomial $g$ to be non-negative over a polyhedron $P$  for each constraint $\forall \pv\in P.(g(\pv)\ge 0)$ from \textbf{Step A5} of our algorithm. Let $\forall \pv\in P.(g(\pv)\ge 0)$ be such a constraint for which the polyhedron $P$ is defined by the linear inequalities $g_1\ge 0,\dots,g_m\ge 0$. Let $\pvars= \{ v_1, v_2, \ldots, v_t\}$ be the set of program variables and define $\monomials_d(\pvars)= \{m_1, m_2, \ldots, m_r\}$ as the set of all monomials of degree at most $d$ over $\pvars$, i.e.~$\monomials_d(\pvars) :=  \{ \prod_{i=1}^t v_i^{\alpha_i} ~\mid~ \forall i~~\alpha_i\in \mathbb{N} ~\wedge~ \sum_{i=1}^t \alpha_i \leq d \}$. 
%For each location $\loc_i\in\locs$, our algorithm generates a template $h(\loc_i):=\sum_{j=1}^r a_{ij}\cdot m_r$ where the coefficients $a_{ij}$'s are unknown parameters to be resolved. 
The application of Putinar's to $\forall \pv\in P.(g(\pv)\ge 0)$ has the following steps. 
%our algorithm has the following sub-steps.
\begin{itemize}
		\item First, represent each $f_i$ in \cref{eq:putinar} as the positive semidefinite form $f_i=\mathbf{v}^\mathrm{T} \mathbf{Q}_i \mathbf{v}$ subject to the positive semidefinite constraint where each $\mathbf{Q}_i$ is a real matrix whose every entry is an unknown parameter. 
        \item Second, compute an equation in the form \eqref{eq:putinar} whose coefficients are affine expressions in the unknown coefficients from our templates and the unknown entries in the matrices $\mathbf{Q}_i$'s. 
        \item Third, establish the affine constraints between the unknown coefficients in the templates and the unknown entries in the matrices $Q_i$'s by matching the coefficients at the LHS and the RHS of the equation obtained from the previous step. 
\end{itemize}
The overall application processes all such constraints from \textbf{Step A5} of our algorithm by (i) collecting all the affine and the semidefinite constraints from the first and the third steps above and (ii) solve them by
semidefinite programming. 



\subsection{Application of Handelman's Positivstellensatz}

To present Handelman's Positivstellensatz, we need the notion of monoid as follows. 
Below we consider an arbitrary finite collection $\Gamma=\{g_1,\dots, g_k\}$ ($k\ge 1$) of linear functions (i.e., degree-$1$ polynomials) in the program variables.  

\begin{definition}[Monoid]
The \emph{monoid} of $\Gamma$ is defined by:
\[
\mbox{\sl Monoid}(\Gamma):=\left\{\prod_{i=1}^k h_i \mid k\in\Nset_0\mbox{ and }h_1,\dots,h_k\in\Gamma\right\}~~.
\]
\end{definition}

Then in our context, Handelman's Positivstellensatz can be formulated as follows. 

\begin{theorem}[Handelman's Positivstellensatz~\cite{handelman1988representing}]
\label{thm:handelman}
Let $g$ be a polynomial in the program variables such that $g(\pv)>0$ for all program valuations $\pv\in 
P:=\{\pv'\in \Rset^{|\pvars|}\mid g_1(\pv')\ge 0, \dots, g_k(\pv')\ge 0\}$. 
%\SAT{\Gamma}$. 
If $P$ is compact, then we have  
\begin{equation}\label{eq:handelman} 
g=\sum_{i=1}^d a_i\cdot u_i 
\end{equation}
for some $d\in\Nset$, real numbers $a_1,\dots,a_d\ge 0$ and $u_1,\dots,u_d\in\mbox{\sl Monoid}(\Gamma)$. 
\end{theorem}

To apply Handelman's Positivstellensatz, we consider a natural number which serves as a bound on the number of multiplicands allowed to form an element in $\mbox{\sl Monoid}(\Gamma)$.
Then~\cref{eq:handelman} results in a system of linear equalities that involves $a_1,\dots,a_d$ and the coefficents of $g$. The application of Handelman's Positivstellensatz to each $\forall \pv\in P.(g(\pv)\ge 0)$
is simpler than that of Putinar's Positivstellensatz, and is as follows. 

\begin{itemize}
        \item First, compute an equation in the form \eqref{eq:handelman} whose coefficients are affine expressions in the unknown coefficients from our templates and the fresh variables $a_1,\dots,a_d$ from \cref{eq:handelman}. 
        \item Second, establish the affine constraints between the unknown coefficients in the templates and the fresh variables $a_1,\dots,a_d$ from \cref{eq:handelman} by matching the coefficients at the LHS and the RHS of the equation obtained from the previous step. 
\end{itemize}
The overall application processes all such constraints from \textbf{Step A5} of our algorithm by (i) collecting all the affine constraints from the second steps above and (ii) solve them by
linear programming. 

\section{Supplementary Materials for Section~\ref{sec:experiment}}\label{app:experiments}


\subsection{Possible Approaches for Computing $M_{\mathrm{up}}$ and $M_{\mathrm{low}}$ of Score-recursive WPTS's}

Fix a score-recursive WPTS $\Pi$, and assume it has (1) the concentration property, i.e., $\probm(T>n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for $c_1,c_2>0$, (2) the bounded-update property, and (3) the stepwise weight is bounded by $e^{c_3}$ for $0<c_3<c_2$. Then given the bounded ranges $B$ and $B'$ as computed in \cref{sec:algorithm}, we derive the upper bound $M_{\mathrm{up}}$ and the lower bound $M_{\mathrm{low}}$ for the expected weight from $B'\backslash B$ as follows.

For any $\pv\in B'\backslash B$,
\begin{eqnarray*}
\llbracket \Pi\rrbracket (\pv) &=& \expectdist{\pv}{w_T}   \\
&=& \sum_{n=1}^{\infty} \probm(T=n)\cdot w_n\\
&\le & \sum_{n=0}^{\infty} \probm(T>n)\cdot w_n \\
&\le& 1+\sum_{n=1}^{\infty} \probm(T>n)\cdot w_n \\
&=& 1+\sum_{n=1}^{n^*-1} \probm(T>n)\cdot w_n +\sum_{n=n^*}^{\infty} \probm(T>n)\cdot w_n \\
&\le& 1+M+\sum_{n=n^*}^{\infty} \probm(T>n)\cdot w_n \\
&=& M'+\sum_{n=n^*}^{\infty} \probm(T>n)\cdot w_n \\
&\le& M'+\sum_{n=n^*}^{\infty} c_1\cdot e^{-c_2\cdot n} \cdot w_n \\
&\le& M'+\sum_{n=n^*}^{\infty} c_1\cdot e^{-c_2\cdot n} \cdot e^{c_3\cdot n} \\
&=& M'+c_1\cdot \sum_{n=n^*}^{\infty}(e^{c_3-c_2})^n \\
&=&M'+ c_1\cdot \frac{a}{1-q} \\
&=& M_{\mathrm{up}} \\
\end{eqnarray*}
where 
\[
\sum_{n=1}^{n^*-1} \probm(T>n)\cdot w_n\le \sum_{n=1}^{n^*-1} w_n\le \sum_{n=1}^{n^*-1} (e^{c_3})^n = \frac{a'\cdot (1-(q')^{(n^*-1)})}{1-q'}=:M
\]
and $a'=e^{c_3}$, $q'=e^{c_3}$.
The first inequality is obtained from the fact that
\[
\probm(T>n)=\probm(T\ge n+1)=\probm(T=n+1)+\probm(T=n+2)+\dots,
\]
thus,
\[
\probm(T=n+1)\le \probm(T>n).
\]
The second inequality is derived by the fact that $\probm(T>0)\le 1$ and $w_0=\win=1$. The third inequality is obtained by the definition of $M$ above. The fourth inequality is obtained by the concentration property, while the fifth inequality is derived by the bounded stepwise weight condition.

For $M_{\mathrm{low}}$, we trivially set $M_{\mathrm{low}}=0$. We can refine it heuristically, e.g., according to the monotonicity of the scoring function.

\subsection{Overapproximation via Polynomial Interpolations}
Given a non-polynomial function $f(x)$ over the interval $I=[a,b]$, we aim to approximate $f(x)$ by polynomials $p(x)$'s. The correctness of approximation is based on a classical theorem called Weierstrass' Theorem \cite{jeffreys1988weierstrass}.

\begin{theorem}[Weierstrass' Theorem]\label{thm:Wierstrass}
Let $f(x)$ be a continuous function on the (closed) interval $[a,b]$. Then there is a sequence of polynomials $p_n(x)$ (of degree $n$) such that
\[
\mathop{lim}_{n\to\infty} ||f-p_n||_{\infty}=0.
\]
	
\end{theorem}

We also need the following theorem to measure the derived polynomials. The property of Lipschitz continuity supports the following theorem easily.
\begin{theorem}\label{thm:errorbound}
	Suppose $r(x)$ is a continuous and differentiable function on a compact convex set $\Psi\subseteq \Rset$. Assume that a collection of points $\{x_1,x_2,\dots,x_k\}$ are sampled uniformly from $\Psi$ and $s\in\Rset_{>0}$ is the sampling spacing. Let $r_0=\mathop{max}\{|r(x_1)|,|r(x_2)|,\dots,|r(x_k)|\}$, and $\beta=\mathop{sup}_{x\in\Psi} ||\nabla r(x)||$, then 
	\begin{equation}
	|r(x)|\le \beta\cdot s+ r_0,\ \forall\ x\in \Psi.
	\end{equation}
\end{theorem}


Then our scheme is as follows.
\begin{itemize}
	\item Split the interval $I=[a,b]$ uniformly into $m$ partitions, i.e., $I_1=[a_1,b_1],I_2=[a_2,b_2],\dots,I_m=[a_m,b_m]$.
	\item For each partition $I_i=[a_i,b_i]$, define a $n$-degree polynomial $p^i_n(x):=\sum_{j=0}^n c_{ij}\cdot x^j$.
	\begin{enumerate}
		\item Pick a non-negative integer $k>n$ and sample $k$ points uniformly from $f$ over $I_i$. That is,  \[
		D=\{(x_1,f(x_1)),(x_2,f(x_2)),\dots,(x_k,f(x_k))\}
		\]
		where $x_l\in I_i$ for all $1\le l\le k$.
		\item Let $p_n^i(x_l)=f(x_l)$ for all $1\le l\le k$, then we have a linear system $\mathbf{V}\cdot \mathbf{c}=\mathbf{f}$ where 
		\begin{equation*}
		\mathbf{V}=
		\begin{bmatrix}
		   1 & x_1 & x_1^2& \cdots &x_1^n \\
		   1 & x_2 & x_2^2& \cdots &x_2^n \\
		   \vdots & \vdots & \vdots&   &\vdots \\
		   1 & x_k & x_k^2& \cdots &x_k^n \\
		\end{bmatrix},
		\end{equation*}
		$\mathbf{c}=[c_{i0},c_{i1},\dots,c_{in}]^T$ and $\mathbf{f}=[f(x_1),f(x_2),\dots,f(x_k)]^T$.
		\item By solving the above overdetermined system, we obtain $p_n^i(x)$ as the approximation of $f(x)$ over the interval $I_i=[a_i,b_i]$. 
		\item Having $p_n^i(x)$, evaluate an error bound $\gamma_i$ such that
		\begin{equation}
		\forall\ x\in I_i,\ |f(x)-p_n^i(x)|\le\gamma_i .
		\end{equation} 
		Let $r(x)=f(x)-p_n^i(x)$ and $\Psi=I_i$, then we obtain $r_0=\mathop{max}\{|r(x_1)|,|r(x_2)|,\dots,|r(x_k)|\}$ by \cref{thm:errorbound}. To derive the Lipschitz constant $\beta$ of $r(x)$ over the interval $I_i$, we pick a non-negative integer $q=10k$, and sample $q$ points uniformly from $f$, i.e., we have another collection of points $\{x'_1,x'_2,\dots,x'_q\}$. Let $\beta=\mathop{max}\{|\nabla r(x'_1)|,\dots,|\nabla r(x'_q)|\}$, then
		\[
		\gamma_i:=\beta\cdot s+r_0
		\]
		where $s$ is the corresponding sampling spacing of the $q$ points.
	\end{enumerate}
	\item Now we have a set $D_p$ of tuples of intervals, polynomials and error bounds, i.e., 
	\begin{equation}\label{eq:D_p}
	D_p=\{(I_1,p_n^1(x),\gamma_1),\dots,(I_m,p_n^m(x),\gamma_m)\}
	\end{equation}
 
\end{itemize}

The approximation error bounds $\gamma_i$'s are taken into account when we synthesize the polynomial template $h$. Given a non-polynomial function $f(x)$ such that ${\tt score}(f(x))$ occurs in the program, we obtain a set $D_p$ in the form of \eqref{eq:D_p}.  For each interval $I_i$, we introduce a new variable $r_i$ and approximate $f(x)$ over $I_i$ as $p_n^i(x)+r_i$ with $r_i\in [-\gamma_i,\gamma_i]$. 
That is, for $1\le i\le m$, we have
\begin{equation}
\forall\ x\in I_i,\ f(x)\approx p_n^i(x)+r_i\ \text{with }r_i\in [-\gamma_i,\gamma_i].
\end{equation}


For a state $(\loc,\pv)$ such that $\loc$ is the location before the command ${\tt score}(f(x))$, there is the unique transition $\tau=\langle \loc, true, F  \rangle$ such that $F=\langle \loc',1, \textbf{1}, f \rangle$ and $\loc'$ is the location that follows the command ${\tt score}(f(x))$. Then for all valuations $\pv\in I(\loc) \wedge \Phi_B$ and $1\le i \le m$, it should hold that 
\begin{itemize}
	\item  for all $\pv[x] \in I_i$ and $r_i\in [-\gamma_i,\gamma_i]$, we have that $\ewt(h)(\loc,\pv) \le h(\loc, \pv)$ (for upper bounds) and $\ewt(h)(\loc,\pv) \ge h(\loc,\pv)$ (for lower bounds) where
	\[
	\ewt(h)(\loc,\pv)=(p_n^i(x)+r_i)\cdot h(\loc',\pv).
	\]
\end{itemize}


\subsection{Other Experimental Results}

% Figure environment removed


% Figure environment removed



% Figure environment removed

% Figure environment removed



