
\subsection{The OST Approach}\label{sec:ostapproach}


As stated in Remark~\ref{rmk:maxvalue}, our fixed-point approach cannot handle score-recursive Bayesian probabilistic programs. To tackle score-recursive programs, we consider the adaption of Optional Stopping Theorem (OST) to our case. OST is a classical theorem in martingale theory that characterizes the relationship between the expected values initially and at a stopping time in a %discrete-time 
supermartingale. Below we first present the classical form of OST.


\begin{theorem}[Optional Stopping Theorem (OST) \cite{williams1991probability}]
Let $\{X_n\}_{n=0}^\infty$ be a supermartingale adapted to a filtration $\mathcal{F}=\{\mathcal{F}_n\}_{n=0}^\infty$, and $\kappa$ be a stopping time w.r.t. the filtration $\mathcal{F}$. 
%$\{\mathcal{F}_n\}_{n=0}^\infty$. 
Then the following condition is sufficient to ensure that $\expv\left(|X_\kappa|\right)<\infty$ and %$\expv\left(X_\kappa\right) = \expv(X_0)$ 
%(resp. 
$\expv\left(X_\kappa\right)\le\expv(X_0)$:
\begin{itemize}
\item  $\expv(\kappa)<\infty$, and
\item (\emph{bounded difference}) there exists a constant $C>0$ such that for all $n\ge 0$, $|X_{n+1}-X_n|\le C$ holds almost surely.
\end{itemize}	
	
\end{theorem}


In our NPD problem, as the score statements accumulate weights in a multiplicative fashion, the classical OST cannot be applied since the bounded difference condition may be violated. To address this difficulty, we propose a novel variant of OST that tackles the multiplicative feature from score statements.



\begin{theorem}[OST Variant]\label{thm:ost-variant}
Let $\{X_n\}_{n=0}^\infty$ be a %martingale 
supermartingale 
adapted to a filtration 
$\mathcal{F}=\{\mathcal{F}_n\}_{n=0}^\infty$, and $\kappa$ be a stopping time w.r.t. the filtration $\mathcal{F}$. 
%$\{\mathcal{F}_n\}_{n=0}^\infty$. 
Then the following condition $(\mho)$ is sufficient to ensure that $\expv\left(|X_\kappa|\right)<\infty$ and %$\expv\left(X_\kappa\right) = \expv(X_0)$ 
$\expv\left(X_\kappa\right)\le\expv(X_0)$:
\begin{itemize}
\item[$(\mho)$] There exist integers $b_1,b_2>0$ and real numbers $c_1>0,c_2>c_3> 0$ such that (i) $\probm(\kappa>n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for sufficiently large $n \in \Nset$, and (ii) for all $n \in \Nset$, $\left\vert X_{n+1}-X_n \right\vert \le b_1\cdot n^{b_2}\cdot e^{c_3\cdot n}$ holds almost surely.
\end{itemize}
\end{theorem}



Our OST variant extends the classical OST with the relaxation that we allow the magnitude of the next random variable $X_{n+1}$ to be bounded by that of $X_n$ with a multiplicative factor $e^{c_3}$, which corresponds to the multiplicative feature from score statements. The intuition of the theorem is that 
to cancel the effect of the multiplicative factor, we require in extra the exponential decrease in $\probm(\kappa>n) \leq c_1 \cdot e^{-c_2 \cdot n}$. The proof resembles \cite[Theorem 5.2]{cost2019wang} and is relegated to \cref{app:ost-variant-proof}. 




Below we show how our OST variant can be applied to handle score-recursive WPTS's. Fix a WPTS $\Pi$ in the form of \eqref{eq:wpts}. 
%The key concepts in the application of the OST variant are \emph{potential weight functions} over the WPTS $\Pi$ as follows. 
In the rest of this subsection, we reuse the expected-weight transformer $\ewt$ defined in \cref{def:ewt} and  potential weight functions given in \cref{def:puwf}. The slight difference is that for the expected-weight transformer in the context of a score-recursive WPTS, we consider that the weight function $\wet_j$ before termination may not be constantly $1$. 



To apply our multiplicative OST variant, we impose a bounded update requirement as in \cite{cost2019wang}. A WPTS $\Pi$ has the \emph{bounded update} property 
%over its program variables, 
if there exists a constant $\varkappa>0$ such that for every reachable state $(\loc,\pv)$, if $\tau=\langle \loc,\phi, F_1,\dots,F_k \rangle$ is the unique transition with each fork $F_j=\langle \loc'_j,p_j,\upd_j,\wet_j \rangle$ such that $\pv\models \phi$, then we have that 
	%\[
	$\forall \rv\in\supp{\rdvarjdis}\,~\forall x\in \pvars,~~ |\upd_j(\pv,\rv)(x)-\pv(x)|\le \varkappa$.

We apply our OST variant (Theorem~\ref{thm:ost-variant}) in a way similar to \cite[Theorem 6.10 and Theorem 6.12]{cost2019wang} to obtain the main theorem for our OST-based approach. To be more precise, we construct a stochastic process from a PUWF (or the negative of a PLWF) and show that the stochastic process is a supermartingale and the condition $(\mho)$ is fulfilled. The statement of the theorem is as follows.  

\begin{theorem}[OST Approach]\label{thm:puwf-normalizing}
Let $\Pi$ be a score-recursive WPTS that has the bounded update property. Suppose that there exist real numbers $c_1>0$ and $c_2>c_3>0$ such that 
\begin{itemize}
\item[(E1)] $\probm(T>n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for sufficiently large $n\in\Nset$, and 
\item[(E2)] for each score function $\wet$ in $\Pi$ we have $|\wet|\le e^{c_3}$. 
\end{itemize}
 Then for any PUWF (resp. PLWF) $h$ over $\Pi$, we have that $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\le h(\lin,\valin)$ (resp. $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\ge h(\lin,\valin)$) for any initial state $(\lin,\valin)$, respectively.  
\end{theorem}

\begin{proof}[Proof Sketch.]
For the upper bounds, we define the stochastic process $\{X_n\}_{n=0}^\infty$ as $X_n:=h(\loc_n,\pv_n)$ where $(\loc_n,\pv_n)$ is the program state at the $n$th step of a program run. Then we construct a stochastic process $\{Y_n\}_{n=0}^\infty$ such that $Y_n:=X_n\cdot \prod_{i=0}^{n-1} W_i$ where $W_i$ is the weight at the $i$th step of the program run. We consider the termination time $T$ of $\Pi$ and prove that $\{Y_n\}_{n=0}^\infty$ satisfies the prerequisites of our OST variant (\cref{thm:ost-variant}). This proof depends on the assumption that $\Pi$ has concentration and bounded update properties, and the score functions in $\Pi$ are also bounded (Item 2, \cref{thm:puwf-normalizing})). Then by applying \cref{thm:ost-variant}, we obtain that $\expect{Y_T}\le \expect{Y_0}$. By (C2) in \cref{def:puwf}, we have that $Y_T=h(\loc_T,\pv_T)\cdot \prod_{i=0}^{T-1} W_i=\widehat{w}_T$. Thus, we have that $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})=\expectdist{\valin}{\widehat{w}_T}=\expect{\prod_{i=0}^{T-1} W_i}\le \expect{Y_0}=h(\lin,\valin)$. For the lower bounds, the proof is similar. The more detailed proof is relegated to~\cref{app:ost}.
\end{proof}



