We first review some basic concepts from probability theory (see standard textbooks such as \cite{pollard2002user,williams1991probability} for a detailed treatment), 
%the background of Bayesian inference, and finally 
%We first review some basic concepts from probability theory, 
and then present the Bayesian probabilistic programming language and the normalised posterior distribution (NPD) problem.
%we consider in this work. 
Throughout the paper,
we denote by $\Nset$, $\Zset$ and $\Rset$ the sets of all natural numbers (including zero), integers, and real numbers, respectively.

\vspace{-1.5ex}
\subsection{Basics of Probability Theory}
%We assume familiarity with basic probability theory (see \cref{app:prelim} for details). 

A \emph{measurable space} is a pair $(U,\Sigma_U)$, where $U$ is a nonempty set and $\Sigma_U$ is a $\sigma$-algebra on $U$, i.e., a family of subsets of $U$ such that $\Sigma_U\subseteq \mathcal{P}(U)$ contains $\emptyset$ and is closed under complementation and countable union. Elements of $\Sigma_U$ are called \emph{measurable} sets. A function $f$ from a measurable space $(U_1,\Sigma_{U_1})$ to another measurable space $(U_2,\Sigma_{U_2})$ is \emph{measurable} if $f^{-1}(A)\in\Sigma_{U_1}$ for all $A\in\Sigma_{U_2}$.

A \emph{measure} $\mu$ on a measurable space $(U,\Sigma_U)$ is a mapping from $\Sigma_U$ to $[0,\infty]$ such that (i) $\mu(\emptyset)=0$ and (ii) $\mu$ 
%satisfies the
is countably additive:
%condition: 
for every pairwise-disjoint set sequence $\{A_n\}_{n\in\Nset}$ in $\Sigma_U$, it holds that $\mu(\bigcup_{n\in\Nset}A_n)=\sum_{n\in\Nset}\mu(A_n)$. We call the triple $(U,\Sigma_U,\mu)$ a \emph{measure space}. 
%If $\mu(U)\le 1$, we call $\mu$ a \emph{subprobability measure}. 
If $\mu(U)=1$, we call $\mu$ a \emph{probability measure}, and $(U,\Sigma_U,\mu)$ a \emph{probability space}.
The Lebesgue measure $\lambda$ is the unique measure on $(\Rset,\Sigma_{\Rset})$ satisfying $\lambda([a,b))=b-a$ for all valid intervals $[a,b)$ in $\Sigma_{\Rset}$. For each $n\in\Nset$, we have a measurable space $(\Rset^n,\Sigma_{\Rset^n})$ 
%such that there exists 
and
a unique product measure $\lambda_n$ on $\Rset^n$ satisfying $\lambda_n(\prod_{i=1}^n A_i)=\prod_{i=1}^n \lambda(A_i)$ for all $A_i\in\Sigma_{\Rset}$.


The \emph{Lebesgue} integral operator $\int$ is a partial operator that maps a measure $\mu$ on $(U,\Sigma_U)$ and a real-valued function $f$ on the same space $(U,\Sigma_U)$ to a real number or infinity, which is denoted by $\int f \mathrm{d}\mu$ or $\int f(x)\mu(\mathrm{d}x)$. 
The detailed definition of Lebesgue integral is somewhat technical, see \cite{rankin1968real,rudin1976principles} for more details. 
Given a measurable set $A\in\Sigma_U$, the integral of $f$ over $A$ is defined by $\int_A f(x)\mu(\mathrm{d} x):=\int f(x) \cdot [x\in A] \mu(\mathrm{d}x)$
%\begin{align*}
%\textstyle\int_A f(x)\mu(\mathrm{d} x):=\int f(x) \cdot [x\in A] \mu(\mathrm{d}x)
%\end{align*} 
where $[-]$ is the Iverson bracket such that $[\phi]=1$ if 
%the predicate 
$\phi$ is true, and $0$ otherwise. If $\mu$ is a probability measure, then we call the integral as the \emph{expectation} of $f$, denoted by $\expectdist{x\sim\mu;A}{f}$, or $\expv[f]$ when the scope is clear from the context.

For a measure $v$ on $(U,\Sigma_U)$, a measurable function $f:U\to \Rset_{\ge 0}$ is the \emph{density} of $v$ with respect to $\mu$ if $v(A)=\int f(x)\cdot [x\in A] \mu(\mathrm{d} x)$ for all measurable $A\in\Sigma_U$, and $\mu$ is called the \emph{reference measure} (most often $\mu$ is the Lebesgue measure). Common families of probability distributions on the reals, e.g., uniform, normal distributions, are measures on $(\Rset,\Sigma_{\Rset})$. Most often these are defined in terms of probability density functions with respect to the Lebesgue measure. That is, for each $\mu_D$ there is a measurable function $\text{pdf}_D:\Rset\to\Rset_{\ge 0}$ that determines it: $\mu_D(A):=\int_A \text{pdf}_D (\mathrm{d}\lambda) $. As we will see, density functions such as $\text{pdf}_D$ play an important role in Bayesian inference.

Given a probability space $\pspace$, a \emph{random variable} is an $\mathcal{F}$-measurable function $X: \Omega \rightarrow \Rset \cup \{+\infty,-\infty\}$. The expectation of a random variable $X$, denoted by $\expv(X)$, is the Lebesgue integral of $X$ w.r.t. $\probm$, i.e., $\int X\,\mathrm{d}\probm$. A \emph{filtration} of $\pspace$ is an infinite sequence $\{ \mathcal{F}_n \}_{n=0}^{\infty}$ such that for every $n\ge 0$, the triple $(\Omega, \mathcal{F}_n, \probm)$ is a probability space and $\mathcal{F}_n \subseteq \mathcal{F}_{n+1} \subseteq \mathcal{F}$. A \emph{stopping time} w.r.t. $\{ \mathcal{F}_n \}_{n=0}^{\infty}$ is a random variable $T: \Omega \rightarrow \Nset \cup \{0, \infty\}$ such that for every $n \geq 0$, the event \{$T \leq n$\} is in $\mathcal{F}_n$. 

A \emph{discrete-time stochastic process} is a sequence $\Gamma = \{X_n\}_{n=0}^\infty$ of random variables in $\pspace$. The process $\Gamma$ is \emph{adapted} to a filtration $\{ \mathcal{F}_n \}_{n=0}^{\infty}$, if for all $n \geq 0$, $X_n$ is a random variable in $(\Omega, \mathcal{F}_n, \probm)$. A discrete-time stochastic process $\Gamma=\{X_n\}_{n=0}^\infty$ adapted to a filtration $\{\mathcal{F}_n\}_{n=0}^\infty$ is a \emph{martingale} (resp. \emph{supermartingale}, \emph{submartingale})
if for all $n \geq 0$, $\expv(|X_n|)<\infty$ and it holds almost surely (i.e.,~with probability $1$) that
$\condexpv{X_{n+1}}{\mathcal{F}_n}=X_n$ (\mbox{resp. } $\condexpv{X_{n+1}}{\mathcal{F}_n}\le X_n$, $\condexpv{X_{n+1}}{\mathcal{F}_n}\ge X_n$).
See~\cite{williams1991probability} for details.
%Intuitively, a martingale is a discrete-time stochastic process, in which at any time $n$, the expected value $\condexpv{X_{n+1}}{\mathcal{F}_n}$ in the next step, given all previous values, is equal to the current value $X_n$. In a supermartingale, this expected value is less than or equal to the current value and a submartingale is defined conversely.
Applying martingales to qualitative and quantitative analysis of probabilistic programs is a well-studied technique~\cite{SriramCAV,ChatterjeeFG16,ChatterjeeNZ2017}.


\subsection{Bayesian Probabilistic Programming Language}

%We consider an imperative arithmetic probabilistic programming language. 
The syntax of our probabilistic programming language (PPL) is given in \cref{fig:syntax}, where the metavariables $S$, $B$ and $E$ stand for statements, boolean expressions and arithmetic expressions, respectively.   
Our PPL is imperative with the usual conditional and loop structures (i.e.,~\textbf{if} and \textbf{while}), as well as the following new structures: (a)~sample constructs of the form ``$\textbf{sample}\  D$'' that sample a value from a prescribed distribution $D$ over $\mathbb{R}$ and then assign this value to a sampling variable $r$; (b)~score statements of the form ``\textbf{score}($EW$)'' that weight the current execution with a value expressed by $EW$ (note that $\textit{pdf}(D,x)$ means the value of a probability density function w.r.t. $D$ at $x$);
%\footnote{Instead of the hard conditioning that refutes the execution when the observation mismatches the value of the sampling variable, we use the more general soft conditioning and assume the existence of a global weight variable initialized  to $1$.}
%for each program
(c)~probabilistic branching statements of the form
``$\textbf{if}\ \textbf{prob}(p)\dots$'' that lead to the then part with probability
$p\in (0,1]$ and to the else part with probability $1-p$. We also have sequential compositions (i.e., ";") and support return statements (i.e., \textbf{return}) that 
return the value of the program variable of interest. %The set of all statements is denoted by $Stmt$.
Note that $c,c_1,c_2\in\Rset$ are constants, and our language supports any distributions with continuous density functions and infinite supports, 
including but not limited to uniform and normal distributions. 



% Figure environment removed





Given a probabilistic program in our language, we distinguish two disjoint sets of variables in the program: (i) the set $\pvars$ of \emph{program variables} whose values are determined by assignments in the program (i.e., the expressions at the LHS of ``:="); (ii)~the set $\rvars$ of \emph{sampling variables} whose values are independently sampled from prescribed probability distributions each time they are accessed (i.e., each ``$\textbf{sample}\ D$" can be regarded as a sampling variable $r$). 




\begin{example}\label{ex:pedestrian-program}

%Consider the pedestrian random walk example~\cite{DBLP:conf/esop/MakOPW21}, a pedestrian is lost on a road, and she only knows that she is away from her house at most $3$ km. Thus, she starts to repeatedly walk a uniformly random distance of at most $1$ km in either direction, until reaching her house. Upon she arrives, an  odometer tells that she has walked $1.1$ km totally. However, this odometer was once broken and the measured distance is normally distributed around the true distance with standard deviation $0.1$ km. 
\cref{fig:pedestrian-program} shows a Bayesian probabilistic program written in our PPL language. In this program, the set of program variables is $\pvars=\{start,pos,dis,step\}$, and the set of sampling variables is $\rvars=\{ \textbf{sample uniform}(0,1)\}$. Each time $\textbf{sample uniform}(0,1)$ is executed, it samples a value uniformly from $[0,1]$ and then assigns the value to the variable $step$. 
%Thus, $step$ is associated with the probability distribution $\textbf{uniform}(0,1)$.
\qed


	
% Figure environment removed
\end{example}

\subsection{The Semantics of Our Programming Language}

%To relate variables with their values, we introduce the notion of valuations. 
Let $V$ be a finite set of variables with an implicit linear order over its elements. A \emph{valuation} on $V$ is a function $\pv: V \rightarrow \Rset$ that assigns a real value to each variable in $V$. We denote the set of all valuations on $V$ by $\val{V}$. For each $1\le i\le |V|$, we denote the value of the $i$-th variable (in the implicit linear order) in $\pv$ by $\pv[i]$, so that we can view each valuation as a real vector on $V$. A \emph{program} (resp. \emph{sampling}) valuation is a valuation on $\pvars$ (resp. $\rvars$), respectively. 
For the sake of convenience, we fix the notations in the following way, i.e., we always use $\pv\in\val{\pvars}$ to denote a program valuation, and $\rv\in\val{\rvars}$ to denote a sampling valuation; we also write $\pv[\mathit{ret}]$ for the value of the return variable in $\pv$. 



Below we present the semantics for our programming language. Existing semantics in the literature are either measure-\cite{DBLP:conf/lics/StatonYWHK16,LeeYRY20} or sampling-based  \cite{DBLP:conf/esop/MakOPW21,Beutner2022b}. To facilitate the development of our algorithm, we consider the \emph{transition-based} semantics~\cite{DBLP:conf/cav/ChakarovS13,DBLP:conf/popl/ChatterjeeFNH16} to our language and 
%To apply template-based algorithmic approaches to NPD problems, we consider  that 
treat each probabilistic program as a \emph{weighted probabilistic transition system} (WPTS). A WPTS extends a PTS  ~\cite{DBLP:conf/cav/ChakarovS13,DBLP:conf/popl/ChatterjeeFNH16} with weights and an initial probability distribution. 





%Below we present a variant of probabilistic transition systems \cite{DBLP:conf/cav/ChakarovS13}.
\begin{definition}
%[Weighted Probabilistic Transition Systems]
[WPTS]\label{def:wpts}
	A \emph{weighted probabilistic transition system} (WPTS) $\Pi$
	is a tuple
\begin{equation}\label{eq:wpts} 
\tag{\dag}
\Pi = (\pvars, \rvars,  L,\lin,\lout,\mu_{\mathrm{init}}, \rdvarjdis,\transset)%\win)
\end{equation}
for which:
	\begin{itemize}
		\item
		$\pvars$ and $\rvars$ are finite disjoint sets of \emph{program} and resp. \emph{sampling} variables.
%  (variables}) 
%  such that $\pvars\cap \rvars=\emptyset$.
    \item $\locs$ is a finite set of \emph{locations} 
  %or \emph{program counters} 
  with special locations $\lin,\lout\in \locs$. Informally, $\lin$ is the initial location and $\lout$ represents program termination. 
		\item
		$\mu_{\mathrm{init}}$ is the \emph{initial probability distribution} over $\mathbb{R}^{\pvars}$ with a finite support (denoted by $\supp{\mu_{\mathrm{init}}}$), 
  %from which the initial program valuation %$\valin$ is sampled, 
  while $\rdvarjdis$ is a function that assigns a probability distribution $\rdvarjdis(r)$ to each 
  %sampling variable 
  $r \in \rvars$. We call each $\pv\in\supp{\mu_{\mathrm{init}}}$ an \emph{initial program valuation}, and abuse the notation so that $\rdvarjdis$ also denotes the independent joint distribution of all $\rdvarjdis(r)$'s ($r\in \rvars$).
		\item 
		$\transset$ is a finite set of \emph{transitions} where
		each transition $\tau \in \transset$ is a tuple $\langle \loc, \phi, F_1,\dots,F_k \rangle$ such that 
(i) $\loc\in L$ is the \emph{source location} of the transition, 
%\item 
(ii) $\phi$ is the \emph{guard condition} which is a predicate over variables $\pvars$, %which serves as the \emph{guard condition}, 
and (iii) each $F_j:=\langle \loc'_j, p_j, \upd_j,\wet_j \rangle$ is called a \emph{weighted fork} for which (a) $\loc'_j\in L$ is the \emph{destination location} of the fork, (b) $p_j\in (0,1]$ is the probability of this fork, (c) $\upd_j:\Rset^{|\pvars|} \times \Rset^{|\rvars|} \rightarrow \Rset^{|\pvars|}$ is an {\em update function} that takes as inputs the current program and sampling valuations  and returns an updated program valuation in the next step, and (d) $\wet_j:\Rset^{|\pvars|} \times \Rset^{|\rvars|}\to [0,\infty)$ is a \emph{score function} that gives the likelihood weight of this fork depending on the current program and sampling valuations.	
\end{itemize}
\end{definition}


In a WPTS, we use update and score functions to model the update on the program variables and resp. the likelihood weight when running a basic block of statements in a program, respectively.  
%and use score functions to model  caused by the execution of the score statements (if exists) in this block. 
If there is no score statement in the block, then the score function is constantly $1$. 
We always assume that a WPTS $\Pi$ is \emph{deterministic} and \emph{total}, i.e., (i) there is no program valuation that simultaneously satisfies the guard conditions of two distinct transitions from the same source location, and (ii) the disjunction of the guard conditions of all the transitions from any source location is a tautology. 
The transformation from a probabilistic program into its WPTS can be done in a straightforward way (see e.g.~\cite{DBLP:journals/toplas/ChatterjeeFNH18,DBLP:conf/cav/ChakarovS13}). 

\begin{example}\label{ex:pedestrian-semantics} 
\cref{fig:pedestrian-wpts} shows the WPTS of the program in \cref{fig:pedestrian-program} which has two locations $\lin,\lout$. 
 %In the WPTS, 
The circle nodes represent locations and square nodes model the forking behavior of transitions. An edge entering a square node is labeled with the condition of its respective transition, while an edge entering a circle node stands for a fork, which is associated with its probability, update functions and score functions that marked by $w$.\footnote{Here we omit the update functions if the values of program variables are unchanged.} The value of $step$ is initialised to $0$. An the initial probability distribution $\mu_{\mathrm{init}}$ is determined by the joint distribution of $(start,pos,dis,step)$ where $start\sim uniform(0,3)$ and $pos,dis,step$ observe the Dirac measures $Dirac(\{start\})$, $Dirac(\{0\})$ and $Dirac(\{0\})$, respectively, e.g., the probability of the event ``$dis\in\{0\}$'' equals $1$. As $step$ simply receives values from a sampling variable, we neglect it in the WPTS.\qed
\end{example}

%\paragraph{Score-recursive WPTS.} 

We say that a WPTS is \emph{non-score-recursive} if for all transitions $\tau=\langle \loc, \phi, F_1,  F_2,\dots,F_k \rangle$ in the WPTS with each fork $F_j=\langle \loc'_j, p_j, \upd_j,\wet_j \rangle$ ($1\le j\le k$), we have that each score function $\wet_j$ is constantly $1$ (i.e., the multiplicative weight does not change) for every $\loc'_j\ne \lout$. Otherwise, the WPTS is \emph{score-recursive}.
Informally, a non-score-recursive WPTS has non-trivial score functions only on the transitions to the termination of a program, while a score-recursive WPTS has {\tt score} statements in the execution of the program. 
For example, the WPTS of the program in~\cref{sec3:pedestrian} is non-score-recursive as the nontrivial (i.e., score values not equal to $1$) {\tt score} statement only appears to the termination, while the WPTS of the program in \cref{sec3:phylogenetic} is score recursive since it has {\tt score} statements inside the loop body.
In the case of a non-score-recursive WPTS, we say that the WPTS is \emph{score-bounded} by a positive real $M>0$ if for every $\tau=\langle \loc, \phi, F_1, F_2,\dots,F_k \rangle$ in the WPTS with $F_j=\langle \loc'_j, p_j, \upd_j,\wet_j \rangle$ ($1\le j\le k$), we have that 
$|\wet_j|\le M$ whenever $\loc'_j=\lout$.


Given a program valuation $\mathbf{v}$ and a predicate $\phi$ over variables $\pvars$, we say that $\mathbf{v}$ \emph{satisfies} $\phi$ (written as $\mathbf{v}\models\phi$) if $\phi$ holds when the variables in $\phi$ are substituted by their values in $\mathbf{v}$. 
A \emph{state} 
%of the WPTS $\Pi$ 
is a pair $\Xi=(\loc, \pv)$ where $\loc \in L$ (resp. $\pv \in \Rset^{|\pvars|}$) represents the current location (resp. program valuation), respectively, while a \emph{weighted state} is a triple 
%$\Xi^w:=(\loc, \pv,w)$ 
$\Theta=(\loc, \pv, w)$ 
where $(\loc, \pv)$ is a state and $w\in [0,\infty)$ represents the multiplicative likelihood weight accumulated so far. 


 
%\paragraph{Semantics.} 
Below we specify the semantics of a WPTS. Consider a WPTS $\Pi$ in the form of \eqref{eq:wpts}. The semantics of $\Pi$ is formalized by the infinite sequence $\Gamma=\{\widehat{\Theta}_n=(\widehat{\loc}_n,\widehat{\pv}_n,\widehat{w}_n)\}_{n\ge 0}$ 
%of \emph{random weighted states} 
where each $(\widehat{\loc}_n,\widehat{\pv}_n,\widehat{w}_n)$ is the random weighted state at the $n$th execution step of the WPTS such that $\widehat{\loc}_n$ (resp. $\widehat{\pv}_n$, $\widehat{w}_n$) is the random variable for the location (resp. the random vector 
%of random variables 
for the program valuation, the random variable for the multiplicative likelihood weight) at the $n$th step, respectively. %The initial random state $\widehat{\Theta}_0$ is constant and equals $(\lin,\valin,\win)$. 
%its corresponding stochastic process $\Gamma:=\{\hat{\Xi}_n\}_{n\ge 0}$ on states.
The sequence $\Gamma$ starts with the initial random weighted state 
$\widehat{\Theta}_0=(\widehat{\loc}_0,\widehat{\pv}_0,\widehat{w}_0)$ such that $\widehat{\loc}_0$ is constantly $\lin$, $\widehat{\pv}_0\in \supp{\mu_\mathrm{init}}$ is sampled from the initial distribution $\mu_\mathrm{init}$ and the initial weight $\widehat{w}_0$ is constantly set to $1$\footnote{This follows the traditional setting in e.g.~\cite{Beutner2022b}.}. 
Then, given the current random weighted state $\widehat{\Theta}_n=(\widehat{\loc}_n,\widehat{\pv}_n,\widehat{w}_n)$ at the $n$th step, the next random weighted state $\widehat{\Theta}_{n+1}=(\widehat{\loc}_{n+1},\widehat{\pv}_{n+1},\widehat{w}_{n+1})$ is determined by:
(a) If $\widehat{\loc}_n=\lout$, then $(\widehat{\loc}_{n+1}, \widehat{\pv}_{n+1},\widehat{w}_{n+1})$ takes the same weighted state as $(\widehat{\loc}_n,\widehat{\pv}_n,\widehat{w}_n)$ (i.e., the next weighted state stays at the termination location $\lout$);
(b) Otherwise, $\widehat{\Theta}_{n+1}$ is determined by the following procedure:
\begin{itemize}
\item First, since the WPTS $\Pi$ is deterministic and total, we take the unique transition $\tau=\langle \hat{\loc}_n,\phi,F_1,\dots, F_k \rangle$ such that $\hat{\pv}_n\models\phi$. 
\item Second, we choose a fork $F_j=\langle \loc_j, p_j,\upd_j,\wet_j\rangle$ with probability $p_j$.
\item 
Third, we obtain a sampling valuation $\rv\in \supp{\rdvarjdis}$ 
%over the sampling variables $\rvars$ 
by sampling each $r \in \rvars$ independently w.r.t the probability distribution $\rdvarjdis(r).$
\item Finally, the value of the next random weighted state $(\widehat{\loc}_{n+1}, \widehat{\pv}_{n+1},\widehat{w}_{n+1})$ is determined as that of 
$(\loc'_j, \upd_j(\hat{\pv}_n,\rv),\widehat{w}_n\cdot \wet_j(\widehat{\pv}_n,\rv))$. 
\end{itemize}


Given the semantics, a \emph{program run} of the WPTS $\Pi$ is a concrete instance of $\Gamma$, i.e., an infinite sequence $\omega=\{\Theta_n\}_{n\ge 0}$ of weighted states where each $\Theta_n=(\loc_n,\pv_n,w_n)$ is the concrete weighted state at the $n$th step in this program run with location $\loc_n$, program valuation $\pv_n$ and multiplicative likelihood weight $w_n$. A state $(\loc,\pv)$ is called \emph{reachable} if there exists a program run $\omega=\{\Theta_n\}_{n\ge 0}$ such that $\Theta_n=(\loc,\pv,w_n)$ for some $n$. 


 
\begin{example}\label{ex:pedestrian-run}
Consider the WPTS in \cref{ex:pedestrian-semantics}. Consider an initial program valuation $(1,1,0)$ which means that the initial values of $start,pos,dis$ are $1,1,0$, respectively. Then starting from the initial weighted state $(\lin,(1,1,0),1)$, a program run w.r.t the WPTS semantics above could be 
\[
(\lin,(1,1,0),1)\to (\lin,(1,0.5,0.5),1)\to (\lin,(1,-0.1,1.1),1)\to (\lout,(1,-0.1,1.1),3.9894).\qed
\]
\end{example}

Given an initial program valuation $\valin$ of a WPTS, one could construct a probability space over the program runs by their probabilistic evolution described above and standard constructions such as general state space Markov chains~\cite{meyn2012markov}. We denote the probability measure in the probability space by $\probm_{\valin}(-)$ and the expectation operator by $\expectdist{\valin}{-}$.  



\subsection{Normalised Posterior Distribution}\label{sec2:NPD}


Before presenting the central problem of Bayesian probabilistic programming, i.e., analyzing normalised posterior distribution with our WPTS models, we introduce some technical concepts.

%\paragraph{Termination.}
\begin{definition}[Termination]
The \emph{termination time} of a WPTS
%The \emph{termination time} of the WPTS 
$\Pi$ 
%is a random variable $T$ defined on programs runs given 
is the random variable $T$ given by
%a program run  $\omega=\{\Xi_n=(\loc_n,\pv_n,w_n)\}_{n\in\Nset}$,
%\begin{align*}	
$T(\omega):=\text{min}\{n\in\Nset\mid \loc_n=\lout\}$ for every program run  $\omega=\{(\loc_n,\pv_n,w_n)\}_{n\ge 0}$
%\end{align*}
where $\text{min}\,\emptyset:=\infty$. That is, $T(\omega)$ is the number of steps a program run $\omega$ takes to reach the termination location $\lout$. A WPTS $\Pi$ is \emph{almost-surely terminating} (AST) if $\probm_{\valin}(T<\infty)=1$ for all initial program valuations $\valin\in \supp{\mu_{\mathrm{init}}}$.  
%in the case that the program run never terminates. 
\end{definition}




\begin{definition}[Expected Weights]\label{def:exp-wt}
 Given a WPTS $\Pi$ in the form of \eqref{eq:wpts}, a designated initial program valuation $\valin$ and a measurable subset $\calU\in\Sigma_{\Rset^{|\pvars|}}$, the \emph{expected weight} $\measureSem{\Pi}_{\valin}(\calU)$ 
%$\measureSem{\Pi}(\valin)$ 
%of $\Pi$ w.r.t $\pv$ 
is defined as
%$\measureSem{\Pi}_\calU(\valin):=\expectdist{\valin}{\widehat{w}_T}$. 
$\measureSem{\Pi}_{\valin}(\calU):=\expectdist{\valin}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T}$. 
\end{definition}

By definition, we have that $\widehat{\pv}_T$ (resp. $\widehat{w}_T$) is the random vector (resp. variable) of the program valuation (resp. the multiplicative likelihood weight) at termination, respectively. Thus, $\measureSem{\Pi}_{\valin}(\calU)$ is the expectation of $\widehat{w}_T$ 
%over all program runs 
that start from the state $(\lin,\valin,1)$ and end with $\widehat{\pv}_T\in\calU$. If $\calU=\Rset^{|\pvars|}$, the restriction of $\widehat{\pv}_T\in\calU$ can be removed.

Below we define the normalised posterior distribution (NPD) problem. %under our WPTS semantics. 

 
\begin{definition}[Normalised Posterior Distribution]\label{def:npd}
Given a WPTS $\Pi$ in the form of \eqref{eq:wpts},
%We write $\measureSem{\Pi}(\valin)$ iff $\calU=\Rset^{|\pvars|}$.)
%Then given a probability distribution $\mu$ over initial program valuations, 
the \emph{normalised posterior distribution} (NPD) $\posterior_\Pi$ of $\Pi$ 
%over $U$ 
is defined by:
\begin{align*}
\posterior_{\Pi}(\calU):=\measureSem{\Pi}(\calU)/Z_\Pi\mbox{ for all measurable subsets } \calU\in \Sigma_{\Rset^{|\pvars|}},   
\end{align*}	
where 
$\measureSem{\Pi}(\calU):=\int_{\calV} \measureSem{\Pi}_{\pv}(\calU)\cdot \mu_{\mathrm{init}}(\mathrm{d} \pv)$ is the \emph{unnormalised posterior distribution} w.r.t. $\calU$, $\calV:=\supp{\mu_{\mathrm{init}}}$, %is the support of $\mu_{\mathrm{init}}$
%is the integral of all expected weights with an initial program valuation $\pv\in U$, 
and $Z_\Pi:=\measureSem{\Pi}(\Rset^{|\pvars|})$ is the \emph{normalising constant}.  
The WPTS $\Pi$ is called \emph{integrable} 
%w.r.t a probability distribution (for initial program valuations) 
if we have $0<Z_{\Pi}<\infty$. 
%\pw{Shall we mention that $\measureSem{\Pi}_{\pv}(\calU)$ is an integrable function here?}
\end{definition}

%We call a WPTS $\Pi$ \emph{integrable} 
%w.r.t a probability distribution (for initial program valuations) 
%if the normalising constant is finite, i.e., ~$0<Z_{\Pi}<\infty$. %for any $\pv\in\val{\pvars}$. 
%Given an integrable program, we are interested in deriving lower and upper bounds on the normalised posterior distribution over some measurable set $U\in \Sigma_\Rset$.
\paragraph{Interval Bounds for NPD.} In this work, we consider the automated interval-bound analysis for NPD of a WPTS. Formally, we aim to derive an interval $[l,u]\subseteq [0,\infty)$ 
for an integrable WPTS $\Pi$ and any measurable sets $\calU\in\Sigma_{\Rset^{|\pvars|}}$ as tight as possible such that $l\le \posterior_{\Pi}(\calU) \le u$. 
%$l,u$ are called \emph{interval bounds} for the NPD $\posterior_{\Pi}(\calU)$. 
%To achieve this, in the following (\cref{sec:math}) we develop approaches to obtain interval bounds for expected weights as $\measureSem{\Pi}(\calU)$ and $Z_\Pi$ are integrations of expected weights over $\calV$. 
 



To achieve interval bounds for NPD, below we introduce the construction of a new WPTS $\Pi_\calU$ based on the original WPTS $\Pi$ and a measurable set $\calU\in \Sigma_{\Rset^{|\pvars|}}$.  

\paragraph{Construction of $\Pi_\calU$.} Consider a probabilistic program $P$ and its WPTS $\Pi$, given a measurable set $\calU\in\Sigma_{\Rset^{|\pvars|}}$, we construct a new program $P_\calU$ by adding a conditional branch of the form ``\textbf{if} $\pv_T\notin\calU$ \textbf{then} \textbf{score}($0$) \textbf{fi}'' immediately after the termination of $P$ and obtain the WPTS $\Pi_\calU$ of $P_\calU$. Therefore, $\Pi$ and $\Pi_\calU$ have the same initial probability distribution $\mu_{\mathrm{init}}$ and the same finite support $\calV=\supp{\mu_{\mathrm{init}}}$. The following proposition shows that interval-bound analysis for NPD can be reduced to interval-bound analysis for expected weights in the form $\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})$. 

\begin{proposition}\label{prop:unnorm-norm}
   Given a WPTS $\Pi$ in the form of \eqref{eq:wpts}, a measurable set $\calU\in\Sigma_{\Rset^{|\pvars|}}$ and the WPTS $\Pi_\calU$ constructed as above, we have that $\llbracket \Pi \rrbracket_{\pv}(\calU)=\llbracket \Pi_\calU\rrbracket_{\pv}(\Rset^{|\pvars|})$ for any $\pv\in\calV=\supp{\mu_{\mathrm{init}}}$. Furthermore,
   if there exist intervals $[l_1,u_1],[l_2,u_2]\subseteq [0,\infty)$ such that $\llbracket \Pi_\calU\rrbracket_{\pv}(\Rset^{|\pvars|})\in [l_1,u_1]$ and $\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\in [l_2,u_2 ]$ for any $\pv\in\calV$, then we have two intervals $[l_\calU,u_\calU],[l_Z,u_Z]\subseteq [0,\infty)$ such that the unnormalised posterior distribution $\llbracket \Pi\rrbracket (\calU)\in [l_\calU,u_\calU]$ and the normalising constant $Z_\Pi\in [l_Z,u_Z]$. Moreover, if $\Pi$ is integrable, i.e., $[l_Z,u_Z]\subseteq (0,\infty)$, then we can obtain the NPD $\posterior_{\Pi}(\calU)\in [\frac{l_\calU}{u_Z},\frac{u_\calU}{l_Z}]$.\footnote{The interval bounds derived in this manner may be loose, but they are definitely correct.}  Note that by \cref{def:npd}, $l_\calU=\int_\calV l_1 \cdot\mu_{\mathrm{init}}(\mathrm{d} \pv)$, $u_\calU=\int_\calV u_1 \cdot\mu_{\mathrm{init}}(\mathrm{d} \pv)$, $l_Z=\int_\calV l_2 \cdot\mu_{\mathrm{init}}(\mathrm{d} \pv)$ and $u_Z=\int_\calV u_1 \cdot\mu_{\mathrm{init}}(\mathrm{d} \pv)$.

\end{proposition}

The proof of \cref{prop:unnorm-norm} is relegated to \cref{app:sec2-prop}. In the following, we will develop approaches to obtain interval bounds for expected weights.
%in the form $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$ where $\pv$ is an initial program valuation.










