\section{Supplementary Material for Section~\ref{sec:prelim}}\label{app:prelim}

\subsection{Basics of Probability Theory}
A \emph{measurable space} is a pair $(U,\Sigma_U)$ where $U$ is a nonempty set and $\Sigma_U$ is a $\sigma$-algebra on $U$, i.e., a family of subsets of $U$ such that $\Sigma_U\subseteq \mathcal{P}(U)$ contains $\emptyset$ and is closed under complementation and countable union. Elements of $\Sigma_U$ are called \emph{measurable} sets. A function $f$ from a measurable space $(U_1,\Sigma_{U_1})$ to another measurable space $(U_2,\Sigma_{U_2})$ is \emph{measurable} if $f^{-1}(A)\in\Sigma_{U_1}$ for all $A\in\Sigma_{U_2}$.

A \emph{measure} $\mu$ on a measurable space $(U,\Sigma_U)$ is a mapping from $\Sigma_U$ to $[0,\infty]$ such that (i) $\mu(\emptyset)=0$ and (ii) $\mu$ is countably additive:
for every pairwise-disjoint set sequence $\{A_n\}_{n\in\Nset}$ in $\Sigma_U$, it holds that $\mu(\bigcup_{n\in\Nset}A_n)=\sum_{n\in\Nset}\mu(A_n)$. We call the triple $(U,\Sigma_U,\mu)$ a \emph{measure space}. 
If $\mu(U)=1$, we call $\mu$ a \emph{probability measure}, and $(U,\Sigma_U,\mu)$ a \emph{probability space}.
The Lebesgue measure $\lambda$ is the unique measure on $(\Rset,\Sigma_{\Rset})$ satisfying $\lambda([a,b))=b-a$ for all valid intervals $[a,b)$ in $\Sigma_{\Rset}$. For each $n\in\Nset$, we have a measurable space $(\Rset^n,\Sigma_{\Rset^n})$ 
and
a unique product measure $\lambda_n$ on $\Rset^n$ satisfying $\lambda_n(\prod_{i=1}^n A_i)=\prod_{i=1}^n \lambda(A_i)$ for all $A_i\in\Sigma_{\Rset}$.

The \emph{Lebesgue} integral operator $\int$ is a partial operator that maps a measure $\mu$ on $(U,\Sigma_U)$ and a real-valued function $f$ on the same space $(U,\Sigma_U)$ to a real number or infinity, which is denoted by $\int f \mathrm{d}\mu$ or $\int f(x)\mu(\mathrm{d}x)$. 
The detailed definition of Lebesgue integral is somewhat technical, see \cite{rankin1968real,rudin1976principles} for more details. 
Given a measurable set $A\in\Sigma_U$, the integral of $f$ over $A$ is defined by $\int_A f(x)\mu(\mathrm{d} x):=\int f(x) \cdot [x\in A] \mu(\mathrm{d}x)$ where $[-]$ is the Iverson bracket such that $[\phi]=1$ if $\phi$ is true, and $0$ otherwise. If $\mu$ is a probability measure, then we call the integral as the \emph{expectation} of $f$, denoted by $\expectdist{x\sim\mu;A}{f}$, or $\expv[f]$ when the scope is clear from the context.

For a measure $v$ on $(U,\Sigma_U)$, a measurable function $f:U\to \Rset_{\ge 0}$ is the \emph{density} of $v$ with respect to $\mu$ if $v(A)=\int f(x)\cdot [x\in A] \mu(\mathrm{d} x)$ for all measurable $A\in\Sigma_U$, and $\mu$ is called the \emph{reference measure} (most often $\mu$ is the Lebesgue measure). Common families of probability distributions on the reals, e.g., uniform, normal distributions, are measures on $(\Rset,\Sigma_{\Rset})$. Most often these are defined in terms of probability density functions with respect to the Lebesgue measure. That is, for each $\mu_D$ there is a measurable function $\text{pdf}_D:\Rset\to\Rset_{\ge 0}$ that determines it: $\mu_D(A):=\int_A \text{pdf}_D (\mathrm{d}\lambda) $. As we will see, density functions such as $\text{pdf}_D$ play an important role in Bayesian inference.

Given a probability space $\pspace$, a \emph{random variable} is an $\mathcal{F}$-measurable function $X: \Omega \rightarrow \Rset \cup \{+\infty,-\infty\}$. The expectation of a random variable $X$, denoted by $\expv(X)$, is the Lebesgue integral of $X$ w.r.t. $\probm$, i.e., $\int X\,\mathrm{d}\probm$. A \emph{filtration} of $\pspace$ is an infinite sequence $\{ \mathcal{F}_n \}_{n=0}^{\infty}$ such that for every $n\ge 0$, the triple $(\Omega, \mathcal{F}_n, \probm)$ is a probability space and $\mathcal{F}_n \subseteq \mathcal{F}_{n+1} \subseteq \mathcal{F}$. A \emph{stopping time} w.r.t. $\{ \mathcal{F}_n \}_{n=0}^{\infty}$ is a random variable $T: \Omega \rightarrow \Nset \cup \{0, \infty\}$ such that for every $n \geq 0$, the event \{$T \leq n$\} is in $\mathcal{F}_n$. 

A \emph{discrete-time stochastic process} is a sequence $\Gamma = \{X_n\}_{n=0}^\infty$ of random variables in $\pspace$. The process $\Gamma$ is \emph{adapted} to a filtration $\{ \mathcal{F}_n \}_{n=0}^{\infty}$, if for all $n \geq 0$, $X_n$ is a random variable in $(\Omega, \mathcal{F}_n, \probm)$. A discrete-time stochastic process $\Gamma=\{X_n\}_{n=0}^\infty$ adapted to a filtration $\{\mathcal{F}_n\}_{n=0}^\infty$ is a \emph{martingale} (resp. \emph{supermartingale}, \emph{submartingale})
if for all $n \geq 0$, $\expv(|X_n|)<\infty$ and it holds almost surely (i.e.,~with probability $1$) that
$\condexpv{X_{n+1}}{\mathcal{F}_n}=X_n$ (\mbox{resp. } $\condexpv{X_{n+1}}{\mathcal{F}_n}\le X_n$, $\condexpv{X_{n+1}}{\mathcal{F}_n}\ge X_n$).
See~\cite{williams1991probability} for details.
Applying martingales to qualitative and quantitative analysis of probabilistic programs is a well-studied technique~\cite{SriramCAV,ChatterjeeFG16,ChatterjeeNZ2017}.

\subsection{Details for WPTS Semantics}\label{app:wpts-semantics}
We denote by $\Lambda$ the set of all states, by $\Delta$ the set of all weighted states, and by $\Sigma_\Delta$ the product $\sigma$-algebra (on $\Delta$) among the discrete $\sigma$-algebra $(L, 2^{L})$ for locations, the $\sigma$-algebra $\Sigma_{\mathbb{R}^{|\pvars|}}$ for program valuations, and the $\sigma$-algebra $\Sigma_{\mathbb{R}}$ for the multiplicative likelihood weight. 
We define $\Sigma^n_\Delta$ (for $n\ge 1$) as the set $\{A_1\times \dots \times A_n\mid \forall 1\le i\le n. (A_i\in \Sigma_{\Delta})\}$, and $\Delta^\infty$ as the set of all infinite sequences of weighted states.

The probability space for the WPTS $\Pi$ is defined such that its sample space is the set of all program runs, its $\sigma$-algebra is generated by the countable union $\bigcup_{n\ge 1} \{B\times \Delta^\infty\mid B\in \Sigma^n_\Delta\}$), and its probability measure $\probm$ is the unique one such that (i) $\probm(A\times \Lambda^\infty)=\mu_\mathrm{init}(\{\pv\mid (\lin, \pv, 1)\in A\})$ for all $A\in\Sigma_\Delta$, and (ii) $\probm(A\times B\times \Delta^\infty)$ (for every $A\in\Sigma_\Delta, B\in \Sigma^n_\Delta$ ($n\ge 1$)) equals the probability w.r.t the sampling of $\mu_\mathrm{init}$ (for the initial program valuation) and $\rdvarjdis$ (for a sampling valuation in each step until the $(n+1)$-th step) that a program run $\{\Theta_n\}_{n\ge 0}$ is subject to $\Theta_0\in A$ and $(\Theta_1,\dots,\Theta_{n+1})\in B$. 
For each program valuation $\pv$, we denote by $\mathbb{P}_\pv$ the probability measure of $\Pi$ when the initial distribution is changed to the Dirac distribution at $\pv$.

\subsection{Sampling-based Semantics}\label{app:sampling-semantcis}
We recall one prominent semantics in the literature, i.e., the sampling-based semantics~\cite{borgstrom2016lambda,DBLP:conf/lics/StatonYWHK16}. In the next section, we will show that the transition-based semantics in our work is equivalent to the widely-used sampling-based semantics in Bayesian statistical probabilistic programming. 

The sampling-based semantics by Borgstr{\"{o}}m et al.~\cite{borgstrom2016lambda,Beutner2022b} interprets a probabilistic program as a deterministic program parameterized by a sequence of random draws sampled during the execution of the program. 
A \emph{sampling trace} is a finite sequence $\tr= \langle r_1, \dots, r_n \rangle$ of real numbers, and we define $\mathcal{T}:=\bigcup_{n\in\Nset} \Rset^n$ as the set of all sampling traces. 
Given a probabilistic program $P$, a \emph{configuration} $\sigma$ under the semantics is a tuple $\langle \pv,S,w,\tr \rangle$ where $\pv\in \val{\mathrm{p}}$ 
, $S$ is the statement to be executed, $w\in [0,\infty)$ is the multiplicative likelihood weight variable whose value expresses how well the current computation matches the observations, and $\tr$ is a sampling trace. We denote by $\Sigma$ the set of all configurations. 

The semantics operates on the configurations, where an execution of the program is initialized with $\sigma_0=\tuple{\pv_0,P, 1,\tr}$, and the termination configurations have the form of $\tuple{\_,\textbf{skip}, \_, []}$, for which $\_$ is a ``wildcard'' character that matches everything and $[]$ represents an empty set. 
\cref{fig:OperationalSemantics} shows the corresponding one-step reduction relation $\to$ (note that $\Downarrow$ is the usual big-step semantics for deterministic Boolean and arithmetic expressions, so we omit it here). We do not give the reduction rule for the probabilistic branching statement in our PPL (see~\cref{fig:syntax}) as it can be represented by a sequential composition of a boolean statement and a conditional statement.
 
 
Let $\to^*$ be the reflexive transitive closure of the one-step reduction $\to$ in \cref{fig:OperationalSemantics}. Given a probabilistic program $P$, we call a sampling trace $\tr$ \emph{terminating} if $\tuple{\pv,P,1,\tr}\to^* \tuple{\pv',\text{skip},w,[]}$ for some valuations $\pv, \pv'\in \val{\mathrm{p}}$ and weight $w\in \Rset_{\ge 0}$, i.e.,~the program $P$ terminates under the samples drawn as in $\tr$.	
 
 
% Figure environment removed

Then the notion of normalised posterior distributions by the sampling-based semantics is given as follows. 
From the one-step reduction rules (in \cref{fig:OperationalSemantics}), we can reason about the global behavior of probabilistic programs in terms of the sampling traces they produce. 
That is, given a probabilistic program $P$, and a terminating trace $\tr$ such that $\tuple{\pv,P,1,\tr}\to^* \tuple{\pv',\text{skip},w,[]}$ for valuations $\pv, \pv'\in\val{\mathrm{p}}$ and weight $w\in \Rset_{\ge 0}$, we define the \emph{value function} $\valueSem P$
and the \emph{weight function} $\weightSem P$ as follows: 
\begin{align}
 \valueSem P(\pv,\tr) &= \begin{cases}
 \pv' &\text{if }\tuple{\pv,P,1,\tr}\to^* \tuple{\pv',\text{skip},w,[]}\\
 \text{unspecified} &\text{otherwise},
 \end{cases}\\
 \weightSem P(\pv,\tr) &= \begin{cases}
 w &\text{if }\tuple{\pv,P,1,\tr}\to^* \tuple{\pv',\text{skip},w,[]}\\
 0 &\text{otherwise}.
 \end{cases}
 \end{align}
 Moreover, we denote the return variable by $\valueSem{P,\mathit{ret}}(\pv,\tr)$, i.e.,~ $\valueSem{P,\mathit{ret}}(\pv,\tr):=\pv'[\mathit{ret}]$.
We also consider the measure space $(\mathcal{T},\Sigma_{\mathcal{T}},\mu_{\mathcal{T}})$ where $\mathcal{T}=\bigcup_{n\in\Nset} \Rset^n$ (as mentioned previously), $\Sigma_{\trans}:=\{\bigcup_{n\in\Nset} U_n\mid U_n\in \Sigma_{\Rset^n}\}$  and $\mu_{\trans}(U):=\sum_{n\in\Nset}\lambda_n(U\cap \Rset^n)$. 
By definition, the measure space $(\mathcal{T},\Sigma_{\mathcal{T}},\mu_{\mathcal{T}})$ specifies the probability values for sets of sampling traces.
 
\paragraph{Posterior Distributions.} Given a probabilistic program $P$, an initial program valuation $\pv\in \val{\mathrm{p}}$ and a measurable set $U\in\Sigma_{\Rset^{|\pvars|}}$, we define the set of terminating traces where the value of the return variable falls into $U$ as
 
 \[
 \mathcal{T}_{P, \pv, U} := \{\tr\in\mathcal{T} \mid \tuple{\pv,P,1,\tr}\to^*\tuple{\pv',\text{skip},w,[]}, \pv'\in U\}
 \]
 and the set of all terminating traces as
 \[
 \mathcal{T}_{P, \pv} := \{\tr\in\mathcal{T} \mid \tuple{\pv,P,1,\tr}\to^*\tuple{\pv',\text{skip},w,[]}\}.
 \]
Note that $\mathcal{T}_{P, \pv} = \mathcal{T}_{P, \pv, \Rset^{|\pvars|}}$. Therefore, we can define the \emph{unnormalised density} w.r.t $P,\pv,U$ as

\begin{align}\label{eq:unnormalised-posterior}
\measureSem{P}_{\pv}(U) := \int_{\mathcal{T}_{P, \pv, U}} \weightSem P(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr).
\end{align}
That is, the integral takes all traces $\tr$ on which $P$ starts from $\pv$ and evaluates to a valuation in $U$, weighting each $\tr$ with the weight $\weightSem P(\pv,\tr)$ of the corresponding execution. The \emph{normalising constant} is thus defined by 
\begin{align}\label{eq:normalising-constant}
 Z_{P,\pv} := \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr).
\end{align}
 Therefore, the  \emph{normalised posterior distribution} is defined as $\posterior_P (\pv,U):=\frac{\measureSem{P}_\pv(U)}{Z_{P,\pv}}$. 

 
We call a program $P$ \emph{integrable} if its normalised constant is finite, i.e,~$0<Z_{P,\pv}<\infty$ for any $\pv\in\val{\mathrm{p}}$. 
Given an integrable program, we are interested in deriving lower and upper bounds on the posterior distribution.


\subsection{Equivalence between The Two Semantics}
In this subsection, we prove the equivalence of the sample-based semantic and our WPTS transition-based semantics over posterior distributions. Fix a probabilistic program $P$ and its WPTS $\Pi$.

\begin{lemma}\label{lem:density-composition}
	For all non-negative bounded measurable function $g:\Rset^{|\pvars|}\to \Rset$, and a program state $\Xi=(\loc,\pv)$, and a statement $P:=S_1;S_2$, we have that	
	\begin{align*}
	&\int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
	& {}=\int_{\mathcal{T}_{S_1, \pv}} \weightSem{S_1}(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr)   \Big([\valueSem{S_1}(\pv,\tr)=\pv']\\ 
	& \quad {}\cdot \int_{\mathcal{T}_{S_2, \pv'}} \weightSem{S_2}(\pv',\tr') \cdot g(\valueSem{S_2}(\pv',\tr')) \,\mu_\trans(\mathrm{d} \tr') \Big)\\
	\end{align*}
\end{lemma}

The proof is straightforward and resembles that of \cite{LeeYRY20}[Lemma 4.8].

\begin{proposition}\label{prop:posterior-weight}
	For all non-negative bounded measurable function $g:\Rset^{|\pvars|}\to \Rset$, a probabilistic program $P$ and an initial program state $\Xi = (\loc,\pv)\in\Lambda$, we have that
	\begin{align*}
		\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)}= \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr).
	\end{align*}
\end{proposition}


\begin{proof}
	We prove by induction on the structure of statements.
	
	\begin{itemize}
		\item Case $P\equiv ``\textbf{skip}"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=&   g(\pv)  \\
			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv) \,\mu_\trans(\mathrm{d} \tr) \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		\item Case $P\equiv``x:=E"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=&   g(\pv[x\mapsto \llbracket E\rrbracket(\pv)])  \\
			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv[x\mapsto \llbracket E\rrbracket(\pv)]) \,\mu_\trans(\mathrm{d} \tr)  \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		\item Case $P\equiv``x:=\textbf{sample}\ D"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=&  \int  g(\pv[x\mapsto r]) \mu_D (\mathrm{d}r) \\
			%			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv[x\mapsto r]) \,\mu_\trans(\mathrm{d} \tr)  \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		
		\item Case $P\equiv``\textbf{score}(pdf(D,x))"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=& W_\loc(\pv) \cdot g(\pv) \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		\item Case $P\equiv``\textbf{return}\ x"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} &=& \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T(\omega)) \probm_{\Xi}(\mathrm{d}\omega)  \\
			&=&    g(\pv)  \\
			&=&  \int_{\mathcal{T}_{P, \pv}} [\tr=[]] \cdot g(\pv) \,\mu_\trans(\mathrm{d} \tr)  \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		
		\item Case $P\equiv``\textbf{if}\ B\ \textbf{then}\ S_1\ \textbf{else}\ S_2\ \textbf{fi}"$. Assume the next state corresponding to the \textbf{then}-branch (resp. \textbf{else}-branch) is $\Xi_1=(\loc_1,\pv)$ (resp. $\Xi_2=(\loc_2,\pv)$). Then we obtain that
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} 
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} +  [\llbracket B \rrbracket(\pv)=false]\cdot \expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} \\
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \int_{\mathcal{T}_{S_1, \pv}} \weightSem{S_1}(\pv,\tr) \cdot g(\valueSem{S_1}(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
			& &	+[\llbracket B \rrbracket(\pv)=false] \cdot \int_{\mathcal{T}_{S_2, \pv}} \weightSem{S_2}(\pv,\tr) \cdot g(\valueSem{S_2}(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr)\\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		
		
		\item Case $P\equiv``S_1;S_2"$.
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)}
			&=& \int \widehat{w}_T(\omega) \probm_{\Xi}(\mathrm{d}\omega) \int [\omega_T=\Xi'] \cdot \widehat{w}_T(\omega')\cdot g(\widehat{\pv}_T(\omega')) \probm_{\Xi'}(\mathrm{d}\omega') \\
			&=& \int \widehat{w}_T(\omega) \probm_{\Xi}(\mathrm{d}\omega) \left([\omega_T=\Xi']\cdot \int  \cdot \widehat{w}_T(\omega')\cdot g(\widehat{\pv}_T(\omega')) \probm_{\Xi'}(\mathrm{d}\omega')\right) \\
			&=&  \int \widehat{w}_T(\omega) \probm_{\Xi}(\mathrm{d}\omega) \left([\omega_T=\Xi']\cdot \int_{\mathcal{T}_{S_2, \pv'}} \weightSem{S_2}(\pv',\tr') \cdot g(\valueSem{S_2}(\pv',\tr')) \,\mu_\trans(\mathrm{d} \tr') \right)\\
			&=&  \int_{\mathcal{T}_{S_1, \pv}} \weightSem{S_1}(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr)     \Big([\valueSem{S_1}(\pv,\tr)=\pv']\\ 
			&& \cdot \int_{\mathcal{T}_{S_2, \pv'}} \weightSem{S_2}(\pv',\tr') \cdot g(\valueSem{S_2}(\pv',\tr')) \,\mu_\trans(\mathrm{d} \tr') \Big)\\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
		Here $\omega_i$ is the $i$-th element of the sequence $\omega=\{\Xi_n\}_{n\in\Nset}$, i.e., $\omega_i:=\Xi_i$. $\omega_T$ is the last element of $\omega$, and $\Xi'=(\loc',\pv')$. The third and fourth equalities follow from the induction hypothesis, and the last equality from \cref{lem:density-composition}.
		\item Case $P\equiv``\textbf{while}\ B\ \textbf{do}\ S\ \textbf{od}"$. Assume the next state corresponding to the entry of the loop (resp. the exit of the loop) is $\Xi_1=(\loc_1,\pv)$ (resp. $\Xi_2=(\loc_2,\pv)$). Then we obtain that
		\begin{eqnarray*}
			\expectdist{\pv}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} 
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \expectdist{\Xi_1}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} +  [\llbracket B \rrbracket(\pv)=false]\cdot \expectdist{\Xi_2}{\widehat{w}_T\cdot g(\widehat{\pv}_T)} \\
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \int \widehat{w}_T(\omega)\cdot g(\widehat{\pv}_T)\probm_{\Xi_1}(\mathrm{d}\omega) +[\llbracket B \rrbracket(\pv)=false] \cdot g(\pv)  \\
			&=&  [\llbracket B \rrbracket(\pv)=true] \cdot \int_{\mathcal{T}_{S;P, \pv}} \weightSem{S;P}(\pv,\tr) \cdot g(\valueSem{S;P}(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
			& &	+[\llbracket B \rrbracket(\pv)=false] \cdot  \int_{\mathcal{T}_{\textbf{skip}, \pv}} [\tr=[]] \cdot g(\valueSem{\textbf{skip}}(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
			&=&   \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot g(\valueSem P(\pv,\tr)) \,\mu_\trans(\mathrm{d} \tr) \\
		\end{eqnarray*}
	\end{itemize}
\end{proof} 


\begin{theorem}\label{thm:posterior-weight}
	Given a probabilistic program $P$ and its WPTS $\Pi$, an initial program state $\Xi=(\loc,\pv)\in\Lambda$ and a measurable set $U\in\Sigma_{\Rset}$, it holds that $\expectdist{\pv}{\widehat{w}_T\cdot [\widehat{\pv}_T\in \calU]}=\llbracket \Pi \rrbracket_{\pv}(\calU) = \measureSem{P}_{\pv}(\calU)$. Moreover, the expected weight $\expectdist{\pv}{\widehat{w}_T}$ is equivalent to the normalising constant $Z_{P,\pv}$.
\end{theorem}

\begin{proof}
	We instantiate \cref{prop:posterior-weight} with $g(x)=[x\in \calU]$. For any initial program state $\Xi=(\loc,\pv)$, we have that
	\begin{eqnarray*}
		\expectdist{\pv}{\widehat{w}_T\cdot [\widehat{\pv}_T\in \calU]} 
		&=&  \int_{\mathcal{T}_{P, \pv}} \weightSem P(\pv,\tr) \cdot [\valueSem P(\pv,\tr)\in \calU] \,\mu_\trans(\mathrm{d} \tr)  \\
		&=&  \int_{\mathcal{T}_{P, \pv, U}} \weightSem P(\pv,\tr)  \,\mu_\trans(\mathrm{d} \tr) \\
		&=& \measureSem{P}_{\pv}(\calU)
	\end{eqnarray*}
	We prove $\expectdist{\pv}{\widehat{w}_T}=Z_{P,\pv}$ by setting $\calU=\Rset^{|\pvars|}$.
\end{proof}

By \cref{thm:posterior-weight}, we show the equivalence of sampling-based semantics and transition-based semantics over posterior distributions. 

\subsection{Supplementary Details for $\llbracket \Pi \rrbracket(\calU)$ }\label{app:sec2-prop}
\begin{proposition}\label{prop:unnorm-norm}
   Given a WPTS $\Pi$ in the form of \eqref{eq:wpts}, a measurable set $\calU\in\Sigma_{\Rset^{|\pvars|}}$ and the WPTS $\Pi_\calU$ constructed as above, we have that $\llbracket \Pi \rrbracket_{\pv}(\calU)=\llbracket \Pi_\calU\rrbracket_{\pv}(\Rset^{|\pvars|})$ for any $\pv\in\calV=\supp{\mu_{\mathrm{init}}}$. Furthermore,
   if there exist intervals $[l_1,u_1],[l_2,u_2]\subseteq [0,\infty)$ such that $\llbracket \Pi_\calU\rrbracket_{\pv}(\Rset^{|\pvars|})\in [l_1,u_1]$ and $\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\in [l_2,u_2 ]$ for any $\pv\in\calV$, then we have two intervals $[l_\calU,u_\calU],[l_Z,u_Z]\subseteq [0,\infty)$ such that the unnormalised posterior distribution $\llbracket \Pi\rrbracket (\calU)\in [l_\calU,u_\calU]$ and the normalising constant $Z_\Pi\in [l_Z,u_Z]$. Moreover, if $\Pi$ is integrable, i.e., $[l_Z,u_Z]\subseteq (0,\infty)$, then we can obtain the NPD $\posterior_{\Pi}(\calU)\in [\frac{l_\calU}{u_Z},\frac{u_\calU}{l_Z}]$.
\end{proposition}

We prove the proposition by dividing it to the following sub-propositions and complete the proofs.

\begin{proposition}\label{prop:norm}
  Given a WPTS in the form of \eqref{eq:wpts}, the interval-bound analysis of $Z_\Pi$ can be reduced to the interval-bound analysis of the expected weight $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$ for all $\pv\in \calV$.
\end{proposition}
\begin{proof}
Fix a WPTS $\Pi$ in the form of \eqref{eq:wpts}.
By \cref{def:npd}, the normalising constant $Z_\Pi=\measureSem{\Pi}(\Rset^{|\pvars|})=\int_{\calV} \measureSem{\Pi}_{\pv}(\Rset^{\pvars})\cdot \mu_{\mathrm{init}}(\mathrm{d} \pv)$ is the integral of the expected weight function $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$ over the finite set $\calV=\supp{\mu_{\mathrm{init}}}$. Assume there exist two integrable bound functions $Up,Lw$ such that $Lw(\pv)\le \llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})\le Up(\pv)$ for all $\pv\in\calV$. 
Then the interval bounds $l_Z,u_Z$ for $Z_\Pi$ can be derived by the integrals of $Up,Lw$ over $\calV$, that is, 
\begin{align*}
l_Z:= \int_{\calV} Lw(\pv) \cdot \mu_{\mathrm{init}}(\mathrm{d} \pv) \le \measureSem{\Pi}(\Rset^{|\pvars|})=\int_{\calV} \measureSem{\Pi}_{\pv}(\Rset^{|\pvars|})\cdot \mu_{\mathrm{init}}(\mathrm{d} \pv)\le \int_{\calV} Up(\pv) \cdot \mu_{\mathrm{init}}(\mathrm{d} \pv)=:u_Z .
\end{align*}
\end{proof}

\begin{proposition}\label{prop:unnorm}
  Given a WPTS in the form of \eqref{eq:wpts} and any measurable set $\calU\subsetneqq \Rset^{|\pvars|}$, the interval-bound analysis of $\llbracket \Pi\rrbracket (\calU)$ can be reduced to the interval-bound analysis of the expected weight $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$ for all $\pv\in \calV'\subseteq \calV$. 
\end{proposition}
\begin{proof}
Fix a WPTS $\Pi$ in the form of \eqref{eq:wpts} and a measurable set $\calU\subsetneqq \Rset^{|\pvars|}$. By~\cref{def:npd}, $\llbracket \Pi\rrbracket (\calU)=\int_{\calV} \llbracket \Pi\rrbracket_{\pv}(\calU)\cdot\mu_{\mathrm{init}}(\mathrm{d}\pv)$. By the definition of expected weights in \cref{sec2:NPD}, $\llbracket \Pi\rrbracket_{\pv}(\calU)=\expectdist{\pv}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T}$ for any initial program valuation $\pv$. As $\widehat{\pv}_T$ is the random (vector) variable of the program valuation at termination and $\calU\neq \Rset^{|\pvars|}$, it is not possible to track the valuation of $\widehat{\pv}_T$ for each program run starting from $\pv$, which makes it challenging to compute interval bounds for $\llbracket \Pi\rrbracket_{\pv}(\calU)$ directly. However, in practice, we only care about the posterior distribution that whether the return variable falls into some target sets of interest.\footnote{ For the sake of simplicity, we consider that the WPTS has only one return variable $ret\in\pvars$ of interest, but it can be straightforwardly extended to general cases.} That is, the set $\calU$ is defined such that $[\widehat{\pv}_T\in \calU]=1$ iff $\widehat{\pv}_T[ret]\in\calU(ret)$ where $\calU(ret)$ is the element in $\calU$ that corresponds to the set w.r.t. the return variable. Below we distinguish the computation into two cases.
\begin{itemize}
	\item[Case 1.] If the value of $ret$ is determined once and unaffected by loop iterations, then $[\widehat{\pv}_T[ret]\in\calU(ret)]$ is equivalent to $[\valin[ret]\in\calU(ret)]$. We can conclude that $\llbracket \Pi\rrbracket_{\valin}(\calU)=\\ \expectdist{\valin}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T}= \expectdist{\valin}{\widehat{w}_T}=\llbracket \Pi\rrbracket_{\valin}(\Rset^{|\pvars|})$ if $\valin[ret]\in\calU(ret)$, and $0$ otherwise.
	\item[Case 2.] If the value of $ret$ is affected during loop iterations, then we can construct a WPTS $\Pi_\calU$ by adding a conditional branch of the form ``\textbf{if} $\pv_T\notin\calU$ \textbf{then} \textbf{score}($0$) \textbf{fi}'' immediately before the termination of $\Pi$. And we can prove that $\llbracket \Pi\rrbracket_{\valin}(\calU)$ is equivalent to $\llbracket \Pi_\calU\rrbracket_{\valin}(\Rset^{|\pvars|})$ for all $\valin\in\calV$.
\end{itemize}
The correctness of Case 1 above is straightforward to be proved, while the correctness of Case 2 follows from the proposition below.
\end{proof}

\begin{proposition}
	Given a WPTS $\Pi$ in the form of \eqref{eq:wpts} and a measurable set $\calU\subsetneqq \Rset^{|\pvars|}$, for any initial program valuation $\valin\in\calV$, the expected weight $\measureSem{\Pi}_{\valin}(\calU)$ is equivalent to $\measureSem{\Pi_\calU}_{\valin}(\Rset^{|\pvars|})$ where $\Pi_\calU$ is constructed by the method described in Case 2 above.
\end{proposition}

\begin{proof}
Fix a WPTS $\Pi$ in the form of \eqref{eq:wpts}, a measurable set $\calU\subsetneqq \Rset^{|\pvars|}$ and a new WPTS $\Pi_\calU$ constructed by the method described in Case 2. Let the termination location of $\Pi$ be $\lout$ and the termination location of $\Pi_\calU$ be $\lout'$. Define a predicate $\Phi_\calU$ over $\pvars$  such that $\calU=\{\pv\mid \pv\models\Phi_\calU  \}$. Then $\Pi_\calU$ has two additional transitions than $\Pi$, i.e., $\langle \lout, \Phi_\calU, F_{\top}\rangle$ with $F_\top:=\langle \lout',1, \mbox{\sl id},\overline{0} \rangle$ where $\mbox{\sl id}$ is the identity function and $\overline{0}$ is the constant function that always takes the value $0$, and $\langle \lout, \neg\Phi_\calU, F_{\bot}\rangle$ with $F_\bot:=\langle \lout',1, \mbox{\sl id},\overline{1} \rangle$ where $\overline{1}$ is the constant function that always takes the value $1$. Let the termination time of $\Pi$ be $T$ and the termination time of $\Pi_\calU$ be $T'$.
Therefore, for the random variable of the multiplicative likelihood weight at termination, we have that $\widehat{w}_{T'}=[\widehat{\pv}_{T'}\in \calU]\cdot \widehat{w}_{T}+[\widehat{\pv}_{T'}\notin \calU]\cdot 0$.
By the definition of expected weights,
	\begin{align*}
	    \llbracket \Pi\rrbracket_{\valin}(\calU)&=\expectdist{\valin}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T} \\
     &= \expectdist{\valin}{[\widehat{\pv}_T\in \calU]\cdot\widehat{w}_T+[\widehat{\pv}_T\notin \calU]\cdot 0}  \\
     &=\expectdist{\valin}{[\widehat{\pv}_{T'}\in \calU]\cdot \widehat{w}_{T}+[\widehat{\pv}_{T'}\notin \calU]\cdot 0} \\
     &=\expectdist{\valin}{\widehat{w}_{T'}} \\
     &=\expectdist{\valin}{[\widehat{\pv}_{T'}\in\Rset^{|\pvars|}]\cdot\widehat{w}_{T'}}\\
     &= \llbracket \Pi_\calU \rrbracket_{\valin}(\Rset^{|\pvars|})
	\end{align*}
where the third equality is derived from the fact that $\widehat{\pv}_{T'}=\widehat{\pv}_{T}$ and the fourth equality is obtained from the fact that $[\widehat{\pv}_{T'}\in\Rset^{|\pvars|}]\equiv 1$.
\end{proof}

\section{Supplementary Material for Section~\ref{sec:math}}\label{app:sec4}

\subsection{Basics of Fixed Point Theory}\label{app:fixed-point-materials}

We complement some basic concepts of lattice theory here. Given a partial order $\sle$ on a set $K$ and a subset $K' \subseteq K,$ an \emph{upper bound} of $K'$ is an element $u \in K$ that is no smaller than every element of $K'$, i.e.,~$\forall k' \in K'.~k' \sle u.$ Similarly, a \emph{lower bound} for $K'$ is an element $l$ that is no greater than every element of $K',$ i.e.~$\forall k' \in K'.~l \sle k'.$ The \emph{supremum} of $K',$ denoted by $\bigsqcup K'$, is an element $u^* \in K$ such that $u^*$ is an upper-bound of $K'$ and for every upper bound $u$ of $K',$ we have $u^* \sle u.$ Similarly, the \emph{infimum} $\bigsqcap K'$ is a lower bound $l^*$ of $K'$ such that for every lower-bound $l$ of $K',$ we have $l \sle l^*.$ We define $\bot\!:=\!\bigsqcap K$ and $\top\!:=\!\bigsqcup K.$ In general, suprema and infima may not exist.

A partially-ordered set $(K, \sle)$ is called a \emph{complete lattice} if every subset $K'\subseteq K$ has a supremum and an infimum.
Given a partial order $(K, \sle)$, a function $f: K \to K$ is called \textit{monotone} if for every $k_1 \sle k_2$ in $K$, we have $f(k_1) \sle f(k_2).$

Given a complete lattice $(K, \sle),$ a function $f: K \to K$ is called \emph{continuous} if for every increasing chain $k_0 \sle k_1 \sle \ldots$ in $K,$ we have $f(\bigsqcup \{k_n\}_{n=0}^\infty) = \bigsqcup \{f(k_n)\}_{n=0}^\infty,$ and \emph{cocontinuous} if for every decreasing chain $k_0 \sge k_1 \sge \ldots$ of elements of $K,$ we have $f(\bigsqcap \{k_n\}_{n=0}^\infty) = \bigsqcap \{f(k_n)\}_{n=0}^\infty.$ An element $k \in K$ is called a \emph{fixed-point} if $f(k) = k.$ Moreover, $k$ is a \emph{pre fixed-point} if $f(k) \sle k$ and a \emph{post fixed-point} if $k\sle f(k)$. The \emph{least fixed-point} of $f$, denoted by $\lfp f,$ is the fixed-point that is no greater than every fixed-point under $\sle.$ Analogously, the \emph{greatest fixed-point} of $f$, denoted by $\gfp f$, is the fixed-point that is no smaller than all fixed-points.

\begin{theorem}[\textit{Kleene}~\cite{Sangiorgibook}]
\label{thm:kleene}
Let $(K, \sle)$ be a complete lattice and $f: K \to K$ be a continuous function. Then, we have
	$$\textstyle \lfp\ f = {\textstyle \mathop{\bigsqcup}_{i \ge 0}} \left\{f^{(i)}(\bot)\right\}.$$
Analogously, if $f$ is cocontinuous, then we have
	$$\textstyle \gfp\ f = {\textstyle \mathop{\bigsqcap}_{i \ge 0}} \left\{f^{(i)}(\top)\right\}.$$
\end{theorem}

\subsection{Proofs for Our Fixed-Point Approach}\label{app:fixedpoint}

Recall that $\mbox{\sl ew}_\Pi(\lin,\valin)=\llbracket \Pi \rrbracket_{\valin}(\Rset^{|\pvars|})$ (see the definition of $\mbox{\sl ew}_\Pi$ in~\cref{sec:fixed-point}).
We first show that the expected weight function $\mbox{\sl ew}_\Pi$ is a fixed point of $\ewt_\Pi$ in the complete lattice $(\mathcal{K}_M,\le)$ when $M>1$ and the WPTS $\Pi$ is score bounded by $M$. 

\begin{proposition}\label{prop:fixedpoint}
    The expected-weight function $\mbox{\sl ew}$ is a fixed point of the expected-weight transformer $\ewt$. 
\end{proposition}


\begin{proof}
Define the step-bounded weight random variable $\widehat{w}^n_{(\loc, \pv)}$ starting from any program state $\Xi=(\loc,\pv)$  for a step bound $n\in\mathbb{N}$ by 

\[
\widehat{w}^n_{(\loc, \pv)}(\omega)=\begin{cases} \widehat{w}_{(\loc, \pv)}(\omega) & \mbox{if $T(\omega)\le n$} \\ 0 & \mbox{otherwise}\end{cases}\enskip.
\]
Since we always assume that the underlying WPTS is almost-surely terminating, 
it follows that the sequence of random variables $\{\widehat{w}^n\}_{n\in\Nset}$ converges non-decreasingly to $W$. 

Given any program state $\Xi=(\loc,\pv)$ with a unique transition $\tau = \langle \loc, \phi_\tau, f_\tau \rangle$ satisfying $\pv\models\phi_{\tau}$, define the step-bounded expected-weight function $\mbox{\sl ew}^n$ by $\mbox{\sl ew}^n(\loc,\pv)=\expectdist{(\loc,\pv)}{\widehat{w}^n}$. 
Without loss of generality, we assume there is only one fork $f_\tau$ in this transition. Assume the next sampling valuation from $\Xi$ is $\rv_0$ and the next program state is $\Xi'=(\loc',\pv')$, i.e., $\pv' = f_\tau(\pv,\rv_0)$. Following the symbols in \cref{sec:prelim}, we denote the probability space of the WPTS $\Pi$ starting from $(\loc, \pv)$, i.e., the program runs starting from $\Xi=(\loc, \pv)$ as $\pspace_{\Xi}$. By Tonelli-Fubini Theorem, we have that for all $n\ge 0$,
\begin{eqnarray*}
\mbox{\sl ew}^{n+1}(\loc,\pv) & = & \int \widehat{w}^{n+1}_{(\loc, \pv)} \,\mathrm{d}\probm_{\Xi} \\
&=& \int \widehat{w}^{n+1}_{(\loc, \pv)} \,\mathrm{d}(\mathcal{D}_{\rv_0} \times \probm_{\Xi'}) \\
&=& \int \wet \cdot W^n_{(\loc', \pv')}(\omega)\,\mathrm{d}(\mathcal{D}_{\rv_0} \times \probm_{\Xi'}) \\
&=& \int_{\rv_0}\int_{\omega_{\Xi'}} \wet\cdot W^n_{(\loc', \pv')}(\omega)\,\mathrm{d}\probm_{\Xi'}\,\mathrm{d}\mathcal{D}_{\rv_0}\\
&=& \int_{\rv_0}\wet\cdot\left(\int_{\omega_{\Xi'}}\widehat{w}^n_{(\loc', \pv')}(\omega)\,\mathrm{d}\probm_{\Xi'}\right)\,\mathrm{d}\mathcal{D}_{\rv_0}\\
&=& \int_{\rv_0}\wet\cdot \mbox{\sl ew}^n(\loc', \pv') \,\mathrm{d}\mathcal{D}_{\rv_0}\\
&=& \expectdist{\rv_0}{ \wet\cdot \mbox{\sl ew}^n(\loc', \pv')} \\
&=& \ewt(\mbox{\sl ew}^n)(\loc,\pv) 
\end{eqnarray*}
By applying MCT to the both sides of the equality above, we have that 
\[
\mbox{\sl ew}(\loc,\pv)=\ewt(\mbox{\sl ew})(\loc,\pv).
\]
\end{proof}


In order to further show the uniqueness of the fixed point, we prove that $\ewt$ is both continuous and cocontinuous. 

\begin{proposition}\label{prop:continuity} If $M\in [0,\infty)$, then the expected-weight transformer $\ewt:\mathcal{K}_M\to \mathcal{K}_M$ is both continuous and cocontinous.
\end{proposition}

\begin{proof}
	We first prove that $\ewt$ is well-defined. Given an arbitrary $h\in \mathcal{K}_M$, for any $\Xi=(\loc,\pv)\in\Lambda$,
	\begin{itemize}
		\item When $\loc=\lout$, $\ewt(h)(\loc,\pv)=1$.
		\item When $\loc\neq \lout$, for a unique transition $\tau = \langle \loc, \phi_{\tau}, f_{\tau}, \loc' \rangle$ such that $\pv\models\phi_{\tau}$,
		\begin{align*}
		\ewt(h)(\loc,\pv)&=\expectdist{\rv}{h(\loc',f_{\tau}(\pv,\rv))\cdot W(\loc,\pv)} \\
		&\le M\cdot maxscore \\
		&< \infty  \\
		\end{align*}
	\end{itemize}
where $maxscore$ is the maximum of $W$ given any state $\Xi$.
As $W$ is a non-negative function, we can prove that $\ewt(h)(\loc,\pv)\ge 0$. Thus, $\ewt$ is well defined.
Next, we prove that $\ewt$ is monotone. Given any two functions $h_1,h_2\in \mathcal{K}_M$ such that $h_1\le h_2$, by case analysis on $(\loc,\pv)$,
\begin{itemize}
	\item If $\loc=\lout$, $\ewt(h_1)(\loc,\pv)=1=\ewt(h_2)(\loc,\pv)$.
	\item If $\loc\neq\lout$, given a unique transition $\tau = \langle \loc, \phi_{\tau}, f_{\tau}, \loc' \rangle$ such that $\pv\models\phi_{\tau}$,
	\begin{align*}
	\ewt(h_1)(\loc,\pv)&= \expectdist{\rv}{h_1(\loc',f_{\tau}(\pv,\rv))\cdot W(\loc,\pv)} \\&\le \expectdist{\rv}{h_2(\loc',f_{\tau}(\pv,\rv))\cdot W(\loc,\pv)} \\
	&= \ewt(h_2)(\loc,\pv)
	\end{align*}
\end{itemize}
Therefore, $\ewt(h_1)\le \ewt(h_2)$, hence it is monotone.
Then we prove upper continuity of $\ewt$. Choose any increasing chain $h_0\sle h_1\sle h_2\sle \cdots$ and do another case analysis on $(\loc,\pv)$:
\begin{itemize}
	\item If $\loc = \lout$, then 
	\[
	\ewt(\mathop{\bigsqcup}\limits_{n\ge 0}\left\{h_n\right\})(\loc,\pv)=1=\mathop{\bigsqcup}\limits_{n\ge 0}\left\{\ewt(h_n)\right\}(\loc,\pv).
	\]
	\item Otherwise, for a unique transition $\tau = \langle \loc, \phi_{\tau}, f_{\tau} \rangle$ such that $\pv\models\phi_{\tau}$:
	\begin{align*}
	&\ewt(\mathop{\bigsqcup}\limits_{n\ge 0}\left\{h_n\right\})(\loc,\pv) \\
	=& \expectdist{\rv}{ \wet_j(\pv,\rv) \cdot (\mathop{\bigsqcup}\limits_{n\ge 0}\left\{h_n\right\})(\loc',f_{\tau}(\pv,\rv))} \\
	=& \expectdist{\rv}{\mathop{\text{sup}}\limits_{n\ge 0}\left\{h_n (\loc',f_{\tau}(\pv,\rv))\right\}} \\
	=& \expectdist{\rv}{\mathop{\text{lim}}\limits_{n\to \infty}\left\{h_n (\loc',f_{\tau}(\pv,\rv))\right\}} \\
	\overset{\mathrm{MCT}}{=} &\mathop{\text{lim}}\limits_{n\to \infty} \expectdist{\rv}{h_n (\loc',f_{\tau}(\pv,\rv))} \\
	=& \mathop{\text{lim}}\limits_{n\to \infty} \ewt(h_n)(\loc,\pv)\\
	= &\sup_{n\ge 0}\left\{\ewt(h_n)(\loc,\pv)\right\}\\
	= &\mathop{\bigsqcup}\limits_{n\ge 0}\left\{\ewt(h_n)\right\}(\loc,\pv)
	\end{align*}
\end{itemize}
The ``MCT'' above denotes the monotone convergence theorem. A similar argument establishes cocontinuity for integrable $h_0$ and decreasing chains.
\end{proof}

Now the uniqueness follows from Theorem~\ref{thm:kleene}. 

\begin{theorem}\label{thm:lfp}
Let $\Pi$ be a score-at-end WPTS that is score-bounded by a positive real $M>1$.Then the expected-weight function $\mbox{\sl ew}$ is the unique fixed-point of the higher-order function $\ewt$ on the complete lattice $(\mathcal{K}_{M},\le)$.  

\end{theorem}

\begin{proof}
The proof follows similar arguments in \cite[Theorem 4.4]{DBLP:conf/pldi/WangS0CG21}. By Proposition~\ref{prop:continuity}, we have that for every state $\Xi=(\loc,\pv)$, 

\begin{itemize}
\item $\lfp\ \ewt(\loc, \pv) = \lim\limits_{n\rightarrow\infty} \ewt^n(\bot)(\loc, \pv)$, and 
\item $\gfp\ \ewt(\loc, \pv) = \lim\limits_{n\rightarrow\infty} \ewt^n(\top)(\loc, \pv)$. 
\end{itemize}

By the definition of $\ewt_M^n$ and Proposition~\ref{prop:continuity}, we have that 

\begin{itemize}
\item $\ewt^n(\bot)(\loc, \pv) = \expectdist{\Xi}{W\cdot [T\le n]}-M\cdot\mathbb{P}(T>n)$, and 
\item $\ewt^n(\top)(\loc, \pv) = \expectdist{\Xi}{W\cdot [T\le n]}+M\cdot \mathbb{P}(T>n)$. 
\end{itemize}

Recall that we assume the underlying PTS to be almost-surely terminating. Hence, $\lim\limits_{n\rightarrow\infty} \mathbb{P}(T >n) = \mathbb{P}(T=\infty)= 0$. 
It follows that $\lfp\ \ewt(\loc, \pv) = \gfp\ \ewt(\loc, \pv)$, i.e., the fixed point is unique. 
\end{proof}


Theorem~\ref{thm:fix-point-bounds} (Fixed-Point Approach)

$\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\le h(\lin,\valin)$ (resp. $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\ge h(\lin,\valin)$) for any bounded PUWF (resp. PLWF) $h$ over $\Pi$ and initial state $(\lin,\valin)$.

\begin{proof}
Let $M>1$ be a bound for a PUWF $h$. By \cref{thm:lfp}, the expected weight function $\mbox{\sl ew}_\Pi$ is the unique fixed point of $\ewt_\Pi$. Note that $\mbox{\sl ew}_\Pi(\lin,\valin)=\llbracket \Pi \rrbracket_{\valin}(\Rset^{|\pvars|})$. Then by applying Tarski's Fixed Point Theorem (\cref{thm:tarski}) and the definition of potential weight functions (\cref{def:puwf}), the PUWF satisfying the prefixed-point conditions can serve as the upper bound for the expected weight $\llbracket \Pi \rrbracket_{\valin}(\Rset^{|\pvars|})$ for any initial state $(\lin,\valin)$. The proof for a PLWF is completely dual. 
\end{proof}


\subsection{Classical OST}~\label{classical_OST}
Optional Stopping Theorem (OST) is a classical theorem in martingale theory that characterizes the relationship between the expected values initially and at a stopping time in a supermartingale. Below we present the classical form of OST.

\begin{theorem}[Optional Stopping Theorem (OST) \cite{williams1991probability}]
Let $\{X_n\}_{n=0}^\infty$ be a supermartingale adapted to a filtration $\mathcal{F}=\{\mathcal{F}_n\}_{n=0}^\infty$, and $\kappa$ be a stopping time w.r.t. the filtration $\mathcal{F}$. 
Then the following condition is sufficient to ensure that $\expv\left(|X_\kappa|\right)<\infty$ and  
$\expv\left(X_\kappa\right)\le\expv(X_0)$:
\begin{itemize}
\item (\emph{almost-sure termination}) $\expv(\kappa)<\infty$, and
\item (\emph{bounded difference}) there exists a constant $C>0$ such that for all $n\ge 0$, $|X_{n+1}-X_n|\le C$ holds almost surely.
\end{itemize}	
	
\end{theorem}

\subsection{Proof for the OST Variant}\label{app:ost-variant-proof}

Theorem~\ref{thm:ost-variant} (OST Variant)
Let $\{X_n\}_{n=0}^\infty$ be a supermartingale adapted to a filtration $\mathcal{F}=\{\mathcal{F}_n\}_{n=0}^\infty$, and $\kappa$ be a stopping time w.r.t. the filtration $\mathcal{F}$. 
Suppose that there exist positive real numbers $b_1,b_2,c_1,c_2,c_3$ such that $c_2>c_3$ and
\begin{itemize}
\item[(A1)] $\probm(\kappa>n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for sufficiently large $n \in \Nset$, and
\item[(A2)] for all $n \in \Nset$, $\left\vert X_{n+1}-X_n \right\vert \le b_1\cdot n^{b_2}\cdot e^{c_3\cdot n}$ holds almost surely. 
\end{itemize}
Then we have that $\expv\left(|X_\kappa|\right)<\infty$ and $\expv\left(X_\kappa\right)\le\expv(X_0)$.


\begin{proof}
 For every $n\in\Nset_0$,
	\begin{eqnarray*}
		\left|X_{\kappa\wedge n}\right|&=& \left|X_0+\sum_{k=0}^{\kappa\wedge n-1} \left(X_{k+1}-X_k\right)\right| \\
		&=& \left|X_0+\sum_{k=0}^\infty \left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k\wedge n>k}\right|\\
		&\le& \left|X_0\right|+\sum_{k=0}^\infty \left|\left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k\wedge n>k}\right| \\
		&\le& \left|X_0\right|+\sum_{k=0}^\infty \left|\left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k}\right|\enskip. \\
	\end{eqnarray*}
	
	Then
	\begin{eqnarray*}
		& &   \expv\left(\left|X_0\right|+\sum_{k=0}^\infty \left|\left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k}\right|\right) \\
		&=& \mbox{(By Monotone Convergence Theorem)} \\
		& & \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty \expv\left(\left|\left(X_{k+1}-X_k\right)\cdot \mathbf{1}_{\kappa>k}\right|\right) \\
		&=& \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty \expv\left(\left|X_{k+1}-X_k\right|\cdot \mathbf{1}_{\kappa>k}\right) \\
		&\le & \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty \expv\left( b_1\cdot k^{b_2}\cdot e^{c_3\cdot k}\cdot \mathbf{1}_{\kappa>k}\right) \\
		&=& \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty b_1 \cdot k^{b_2} \cdot e^{c_3\cdot k}\cdot \probm\left(\kappa>k\right) \\
		&\le & \expv\left(\left|X_0\right|\right)+\sum_{k=0}^\infty b_1 \cdot k^{b_2}\cdot e^{c_3\cdot k} \cdot c_{1}\cdot e^{-c_{2}\cdot k} \\
		&=& \expv\left(\left|X_0\right|\right)+ b_1 \cdot c_1 \cdot \sum_{k=0}^\infty   k^{b_2}\cdot e^{-(c_2-c_3)\cdot k} \\
		&<& \infty\enskip.
	\end{eqnarray*}
	where the first inequality is obtained by Condition (A2), the second inequality is derived from Condition (A1), and the third inequality stems from the fact that $c_2>c_3$ in the definition.
	
	
	Therefore, by Dominated Convergence Theorem
	and the fact that $X_\kappa=\lim\limits_{n\rightarrow\infty} X_{\kappa\wedge n}$ a.s.,
	\[
	\expv\left(X_\kappa\right)=\expv\left(\lim\limits_{n\rightarrow\infty} X_{\kappa\wedge n}\right)=\lim\limits_{n\rightarrow\infty}\expv\left(X_{\kappa\wedge n}\right)\enskip.
	\]
	Finally, the result follows from properties for the stopped process $\{X_{\kappa\wedge n}\}_{n\in\Nset_0}$ that 
	\[
	\expv\left(X_\kappa\right) \le \expv\left(X_0\right)\enskip.
	\]
\end{proof}



\subsection{Proofs for Our OST-Based Approach}\label{app:ost}

\noindent\textbf{Theorem}~\ref{thm:puwf-normalizing}. (OST Approach)
Let $\Pi$ be a bounded-update score-recursive WPTS. 
Suppose that there exist real numbers $c_1>0$ and $c_2>c_3>0$ such that 
\begin{itemize}
\item[(E1)] $\probm(T>n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for sufficiently large $n\in\Nset$, and 
\item[(E2)] for each score function $\wet$ in $\Pi$, we have $|\wet|\le e^{c_3}$. 
\end{itemize}
 Then for any polynomial PUWF (resp. PLWF) $h$ over $\Pi$, we have that $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\le h(\lin,\valin)$ (resp. $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\ge h(\lin,\valin)$) for any initial state $(\lin,\valin)$, respectively.  

\begin{proof}
\textbf{Part 1. Upper bounds.}  Consider the WPTS $\Pi$ has a $d$-degree polynomial PUWF $h$ (see~\cref{def:puwf}).
	Define the stochastic process $\{X_n\}_{n=0}^\infty$ as $X_n:=h(\loc_n,\pv_n)$ where $(\loc_n,\pv_n)$ is the program state at the $n-$th step of a program run. Then construct a stochastic process $\{Y_n\}_{n=0}^\infty$ such that $Y_n:=X_n\cdot \prod_{i=0}^{n-1} W_i$ where $W_i$ is the weight at the $i-$th step of the program run. According to Condition (C1), we have that $\expect{X_{n+1}\cdot W_n|\mathcal{F}_n}\le X_n$. Therefore, by the “take out what is known” property of conditional
	expectation (see \cite{williams1991probability}), it follows that
	
	\begin{eqnarray*}
		& &\expect{X_{n+1}\cdot \prod_{i=0}^{n}W_i|\mathcal{F}_n} \le X_n\cdot \prod_{i=0}^{n-1} W_i \\
		&\Leftrightarrow& \expect{Y_{n+1}|\mathcal{F}_n}\le \expect{Y_n}, 
	\end{eqnarray*}
	which means that $\expect{Y_{n+1}}\le \expect{Y_n}$ from the basic property of conditional expectation. By an easy induction on $n$, we have that $\expect{Y_n}\le \expect{Y_0}<\infty$ for all $n\ge 0$, thus the conditional expectation is also taken in the normal sense as each $Y_n$ is indeed integrable. Hence, $\{Y_n\}_{n=0}^\infty$ is a supermartingale. Moreover, we have from the bounded-update property that $|X_{n+1}|\le \zeta \cdot (n+1)^d$ for a real number $\zeta>0$. By definition, we obtain that for sufficiently large $n$,
	\begin{eqnarray*}
		\left|Y_{n+1}-Y_n\right|&=&  \left|X_{n+1}\cdot \prod_{i=0}^{n} W_i-X_n\cdot \prod_{i=0}^{n-1} W_i \right|  \\
		&\le& \left\vert X_{n+1}\cdot \prod_{i=0}^{n} W_i\right\vert+ \left|X_n\cdot \prod_{i=0}^{n-1} W_i\right|\\
		&<& e^{c_3\cdot n}\cdot (|X_{n+1}|+|X_n|) \\
		&\le& e^{c_3\cdot n}\cdot [\zeta \cdot (n+1)^d+\zeta \cdot n^d] \\
		&\le& \lambda \cdot n^d\cdot e^{c_3\cdot n}
	\end{eqnarray*}
	where the first inequality is induced by the triangle inequality, and the second inequality is derived from the bounded stepwise weight condition such that each $W_i\in [0,e^{c_3}]$ and the fact $W_0=1$.
	By applying the OST variant (Theorem \ref{thm:ost-variant}), we obtain that $\expect{Y_T}\le \expect{Y_0}$. By definition and Condition (C2) in \cref{def:puwf},
	\begin{eqnarray*}
		Y_T&=&h(\loc_T,\pv_T)\cdot \prod_{i=0}^{T-1} W_i \\
		&=& h(\lout,\pv_T)\cdot  \prod_{i=0}^{T-1} W_i \\
		&=& \prod_{i=0}^{T-1} W_i \\
	\end{eqnarray*}  
	Finally, we have that $\llbracket \Pi \rrbracket_{\valin}(\Rset^{|\pvars|}) =\expect{\prod_{i=0}^{T-1} W_i}\le \expect{Y_0}=h(\lin,\valin)$.


\textbf{Part 2. Lower bounds.}  Consider the WPTS $\Pi$ has a $d$-degree polynomial PLWF $h$ (see~\cref{def:puwf}).
	Define the stochastic process $\{X_n\}_{n=0}^\infty$ as $X_n:=h(\loc_n,\pv_n)$ where $(\loc_n,\pv_n)$ is the program state at the $n-$th step of a program run. Then construct a stochastic process $\{-Y_n\}_{n=0}^\infty$ such that $-Y_n:=-X_n\cdot \prod_{i=0}^{n-1} W_i$ where $W_i$ is the weight at the $i-$th step of the program run. According to Condition (C1'), we have that $\expect{-X_{n+1}\cdot W_n|\mathcal{F}_n}\le -X_n$. Therefore, by the “take out what is known” property of conditional
	expectation (see \cite{williams1991probability}), it follows that
	
	\begin{eqnarray*}
		& &\expect{-X_{n+1}\cdot \prod_{i=0}^{n}W_i|\mathcal{F}_n} \le -X_n\cdot \prod_{i=0}^{n-1} W_i \\
		&\Leftrightarrow& \expect{-Y_{n+1}|\mathcal{F}_n}\le \expect{-Y_n}, 
	\end{eqnarray*}
	which means that $\expect{-Y_{n+1}}\le \expect{-Y_n}$ from the basic property of conditional expectation. By an easy induction on $n$, we have that $\expect{-Y_n}\le \expect{-Y_0}<\infty$ for all $n\ge 0$, thus the conditional expectation is also taken in the normal sense as each $Y_n$ is indeed integrable. Hence, $\{-Y_n\}_{n=0}^\infty$ is a supermartingale. Moreover, we have from the bounded-update property that $|X_{n+1}|\le \zeta \cdot (n+1)^d$ for a real number $\zeta>0$. By definition, we obtain that for sufficiently large $n$,
	\begin{eqnarray*}
		\left|-Y_{n+1}-(-Y_n)\right| &=& \left|X_{n+1}\cdot \prod_{i=0}^{n} W_i-X_n\cdot \prod_{i=0}^{n-1} W_i \right|  \\
		&\le& \left\vert X_{n+1}\cdot \prod_{i=0}^{n} W_i\right\vert+ \left|X_n\cdot \prod_{i=0}^{n-1} W_i\right|\\
		&<& e^{c_3\cdot n}\cdot (|X_{n+1}|+|X_n|) \\
		&\le& e^{c_3\cdot n}\cdot [\zeta \cdot (n+1)^d+\zeta \cdot n^d] \\
		&\le& \lambda \cdot n^d\cdot e^{c_3\cdot n}
	\end{eqnarray*}
	where the first inequality is induced by the triangle inequality, and the second inequality is derived from the bounded stepwise weight condition such that each $W_i\in [0,e^{c_3}]$ and the fact $W_0=1$.
	By applying the variant of Optional Stopping Theorem (Theorem \ref{thm:ost-variant}), we obtain that $\expect{-Y_T}\le \expect{-Y_0}$, so $\expect{Y_T}\ge \expect{Y_0}$. By definition and Condition (C2') in \cref{def:puwf},
	\begin{eqnarray*}
		-Y_T&=&-h(\loc_T,\pv_T)\cdot \prod_{i=0}^{T-1} W_i \\
		&=& -h(\lout,\pv_T)\cdot  \prod_{i=0}^{T-1} W_i \\
		&=& -\prod_{i=0}^{T-1} W_i \\
	\end{eqnarray*}  
	Finally, we have that $\llbracket \Pi \rrbracket_{\valin}(\Rset^{|\pvars|})=\expect{\prod_{i=0}^{T-1} W_i}\ge \expect{Y_0}=h(\lin,\valin)$.
\end{proof}

\section{Supplementary Material for Section~\ref{sec:algorithm}}\label{app:sec5}

\subsection{Error Analysis for Polynomial Approximation}\label{app:error-analysis}

\begin{theorem}\label{app:score-error}
Let $\Pi$ be a score-at-end WPTS with score functions $g_1,\dots,g_k$ on the transitions to the termination location $\lout$. Suppose we have a non-negative real number $\epsilon$ and polynomials $g'_1,\dots,g'_k$ such that for all $x\in \mbox{\sl exit}(\Pi)$ and $1\le j\le k$, $|g'_j(x)-g_j(x)|\le \epsilon$. Then we have that $| \llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|}) - \llbracket \Pi' \rrbracket_{\pv}(\Rset^{|\pvars|}) | \le \epsilon$ for all initial program valuation $\pv$, where $\Pi'$ is obtained from $\Pi$ by replacing each $g_j$ ($1\le j\le k$) with $g'_j$. 
\end{theorem}

\begin{proof}
By \cref{thm:lfp}, we have that 
$\lim\limits_{n\rightarrow \infty} \mbox{\sl ewt}^n_{\Pi}(\bot)=\mbox{\sl ew}_{\Pi}$ and 
$\lim\limits_{n\rightarrow \infty} \mbox{\sl ewt}^{n}_{\Pi'}=\mbox{\sl ew}_{\Pi'}$. 
Since $|g'_j-g_j|\le \epsilon$ for every $j$, one can perform a straightforward induction on $n$ to prove that for all $n\ge 0$, it holds that that $|\mbox{\sl ewt}^n_{\Pi}(\bot) - \mbox{\sl ewt}^n_{\Pi'}(\bot)|\le  \epsilon$. By the definition of $\llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|})$, we have that $| \llbracket \Pi \rrbracket_{\pv}(\Rset^{|\pvars|}) - \llbracket \Pi' \rrbracket_{\pv}(\Rset^{|\pvars|}) | \le \epsilon$. 
\end{proof}

\subsection{Truncation over WPTS's}\label{app:truncaion}
In the following, We formally define the truncation operation and supplement some descriptions and proofs for \textbf{Stage 3}.

A truncation operation for a WPTS is to restrict the value of every program variable in the WPTS to a prescribed bounded range. We consider that a bounded range for a program variable could be either $[-R,R]$ ($R> 0$), or $[0,R], [-R,0]$ if the value of the program variable is guaranteed to be non-negative or non-positive. 

To present our truncation operation, we define the technical notions of truncation function and truncation approximations.   
A \emph{truncation function} 
$\trunc$ is a function that maps every program variable $x\in\pvars$ to a bounded interval $\trunc(x)$ in $\Rset$ that specifies the bounded range of the variable $x$. We denote by $\Phi_\trunc$ the formula $\bigwedge_{x\in\pvars} x\in \trunc(x)$ for a truncation function $\trunc$. 
A \emph{truncation approximation} is a function $\calM:\mathbb{R}^{|\pvars|}\to [0,\infty)$ such that each $\calM(\pv)$ ($\pv\in \mathbb{R}^{|\pvars|}$) is intended to be an over- or under-approximation of the expected weight $\llbracket \Pi\rrbracket_{\pv} (\Rset^{|\pvars|})$ outside the bounded range specified by $\Phi_\trunc$. The truncation operation is given by the following definition. 

Our main theorem shows that by choosing an appropriate truncation approximation $\calM$ in the truncation, one can obtain upper/lower approximation of the original WPTS. 

\begin{theorem}\label{thm:upperlower}
Let $\Pi$ be a WPTS in the form of \eqref{eq:wpts}, $\trunc$ a truncation function and $\calM$ a bounded truncation approximation.
Suppose that the following condition ($\ast$) holds:
\begin{itemize}
\item[($\ast$)] for each fork $F^{M,\sharp}=\langle \sharp, p, \mbox{\sl upd}, \calM\rangle$ in the truncated WPTS $\Pi_{\trunc,\calM}$ that is derived from
some fork $F=\langle \loc', p, \mbox{\sl upd}, \wet\rangle$ with the source location $\loc$ in the original WPTS (see sentence (b) in Definition~\ref{def:truncation}),  we have that $\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\le \calM(\pv)$ for all $\pv$ such that the state $(\loc,\pv)$ is reachable and $\pv\not\models\Phi_\trunc$. 
\end{itemize}
Then $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\le \llbracket \Pi_{\trunc,\calM}\rrbracket_{\valin}(\Rset^{|\pvars|})$ for all initial program valuations $\valin$. 
Analogously, if it holds the condition ($\star$) which is almost the same as ($\ast$) except for that ``$\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\le \calM(\pv)$'' is replaced with ``$\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\ge \calM(\pv)$'', then we have $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\ge \llbracket \Pi_{\trunc,\calM}\rrbracket_{\valin}(\Rset^{|\pvars|})$ for all initial program valuations $\valin$.
\end{theorem}

\begin{proof}
We first prove that when every score function $\mathcal{M}$ in a $F^{\mathcal{M},\sharp}$ derived from a transition with source location $\loc$ is equal to the function $\ewt(\loc,-)$, we have that $\Pi_{\trunc,\mathcal{M}}$ is equal to $\Pi$. 
By \cref{thm:lfp}, the expected weight functions $\mbox{\sl ew}_\Pi$, $\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ are the least fixed point of the higher-order operator $\ewt$ defined in \cref{def:ewt}. 
We prove that both $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ and  $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ holds. Note that since we choose the scoring function to be the exact expected weight function of $\Pi$, it holds that 
$\mbox{\sl ew}_\Pi(-,\pv)=\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}(-,\pv)$ for all program valuations outside $B$. Thus, the nontrivial part is to consider program valuations inside the truncated range. 

\begin{itemize}
	\item $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$: To show that $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$, it suffices to observe that $\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ satisfies $\ewt_{\Pi}(\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}})=\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
    Since $\mbox{\sl ew}_{\Pi}$ is the least fixed point of the higher order equation, we directly obtain that $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
	\item $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$: To show that $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$, it suffices to observe that 
	$\mbox{\sl ew}_{\Pi}$ (extended with the $\sharp$ location whose score function is $1$) satisfies the higher-order equation of $\Pi_{\trunc,\mathcal{M}}$. Thus, we directly have that $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
\end{itemize}

Then we prove the theorem. We only prove the upper-bound case, since the lower-bound case can be proved similarly. The proof follows from \cref{thm:lfp}. Denote $\bot$ as the bottom element of the complete lattice 
$(\mathcal{K}_M, \le)$. Then by \cref{thm:lfp}, we have that $\lim\limits_{n\rightarrow \infty} \ewt_\Pi^n(\bot)=\mbox{\sl ew}_{\Pi}$ and $\lim\limits_{n\rightarrow \infty} \ewt_{\Pi_{\trunc,\mathcal{M}}}^{n}(\bot)=\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
Since $\mbox{\sl ew}_{\Pi}(\pv)\le \mathcal{M}(\pv)$ for all $\pv\in \mbox{\sl exit}(\Pi)$ , one can perform a straightforward induction on $n$ that $\mbox{\sl ew}^{n,\mbox{\sl ew}_{\Pi}}_{\Pi_B}\le  \mbox{\sl ew}^{n,f}_{\Pi_B}$ for all $n$. 

\end{proof}


\vspace{2em}



In the case of OST, the proof is as follows.

\begin{theorem}\label{thm:ostupperlower}
Let 
$\Pi$ be a WPTS in the form of \eqref{eq:wpts}, $\trunc$ a truncation function and $\calM$ a polynomial truncation approximation.
Suppose that condition ($\ast$) as in Theorem~\ref{thm:upperlower} holds. 
Furthermore, suppose that the expected weight functions of the original and the truncated WPTS are both bounded by a polynomial in the program variables, and the WPTS has exponentially-decreasing concentration property for its termination time.
Then $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\le \llbracket \Pi_{\trunc,\calM}\rrbracket_{\valin}(\Rset^{|\pvars|})$ for all initial program valuations $\valin$. 
Analogously, if it holds the condition ($\star$) which is almost the same as ($\ast$) except for that ``$\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\le \calM(\pv)$'' is replaced with ``$\llbracket \Pi\rrbracket_{\pv}(\Rset^{|\pvars|})\ge \calM(\pv)$'', then we have $\llbracket \Pi\rrbracket_{\valin} (\Rset^{|\pvars|})\ge \llbracket \Pi_{\trunc,\calM}\rrbracket_{\valin}(\Rset^{|\pvars|})$ for all initial program valuations $\valin$.
\end{theorem}

\begin{proof}
We first prove that when every score function $\mathcal{M}$ in a $F^{\mathcal{M},\sharp}$ derived from a transition with source location $\loc$ is equal to the function $\ewt(\loc,-)$, we have that $\Pi_{\trunc,\mathcal{M}}$ is equal to $\Pi$. 
By \cref{prop:fixedpoint}, the expected weight functions $\mbox{\sl ew}_\Pi$, $\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$
is a fixed point of the higher-order operator $\ewt$ defined in \cref{def:ewt}. 
We prove that both $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ and  $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ holds. Note that since we choose the scoring function to be the exact expected weight function of $\Pi$, it holds that 
$\mbox{\sl ew}_\Pi(-,\pv)=\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}(-,\pv)$ for all program valuations outside $B$. Thus, the nontrivial part is to consider program valuations inside the truncated range. 

\begin{itemize}
	\item $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$: To show that $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$, it suffices to observe that $\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ satisfies $\ewt_{\Pi}(\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}})=\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
By applying \cref{thm:puwf-normalizing} with given polynomial bounds for $\mbox{\sl ew}_\Pi,\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$, we obtain that $\mbox{\sl ew}_\Pi\le \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
	\item $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$: To show that $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$, it suffices to observe that 
	$\mbox{\sl ew}_{\Pi}$ (extended with the $\sharp$ location whose score function is $1$) satisfies the higher-order equation of $\Pi_{\trunc,\mathcal{M}}$. Thus, we directly have that $\mbox{\sl ew}_\Pi\ge \mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$ by \cref{thm:puwf-normalizing} with given polynomial bounds for $\mbox{\sl ew}_\Pi,\mbox{\sl ew}_{\Pi_{\trunc,\mathcal{M}}}$. 
\end{itemize}

Now similar to the proof for Theorem~\ref{thm:upperlower}, it suffices to show that $\lim\limits_{n\rightarrow \infty} \ewt_\Pi^n(0)=\mbox{\sl ew}_{\Pi}$. First, one observes that $\lim\limits_{n\rightarrow \infty} \ewt_\Pi^n(0)=\mathbb{E}_{\pv}(W\cdot [T\le n])$. Hence, $\mbox{\sl ew}_{\Pi}-\ewt_\Pi^n(0)=\mathbb{E}_{\pv}(W\cdot [T> n])$. By the bounded update property, we have that $\pv'$ is linearly bounded by $n$ for program valuations $\pv'$ reachable after exactly $n$ steps of execution of 
the underlying WPTS. Hence, since the truncation approximation $f$ is polynomial, we have that $\mathbb{E}_{\pv}(W\cdot [T> n])\le f(n)\cdot \mathbb{P}(T>n)$. As we have the exponential-decreasing concentration property for $\mathbb{P}(T>n)$, it follows that 
$\lim\limits_{n\rightarrow\infty}\mathbb{E}_{\pv}(W\cdot [T> n])=0$. 
\end{proof}

The theorem above states that if the truncation approximation gives correct bounds for the expected weights of the original WPTS outside the bounded range, then the bounds for the expected weights of the truncated WPTS are also correct bounds for the expected weights of the original WPTS.

\subsection{Application of Putinar's Positivstellensatz}\label{app:putinar}

We recall Putinar's Positivstellensatz below.  


\begin{theorem}[Putinar's Positivstellensatz~\cite{putinar}] \label{thm:putinar} Let $V$ be a finite set of real-valued variables and $g, g_1, \ldots, g_m \in \mathbb{R}[V]$ be polynomials over $V$ with real coefficients. Consider the set $\mathcal{S}:=\{\mathbf{x} \in \mathbb{R}^V\,\mid\, g_i(\mathbf{x}) \geq 0  \mbox{ for all }1\le i\le m \}$ which is the set of all real vectors at which every $g_i$ is non-negative. If (i)~there exists some $g_k$ such that the set $\{ \mathbf{x} \in \mathbb{R}^V ~\mid~ g_k(\mathbf{x}) \geq 0  \}$ is compact and (ii)~$g(\mathbf{x})>0$ for all $\mathbf{x} \in \mathcal{S}$, then we have that 
	\begin{equation} \label{eq:putinar}
	\textstyle g = f_0 + \sum_{i=1}^m f_i \cdot g_i
	\end{equation}
	for some polynomials $f_0,f_1\dots, f_m\in \mathbb{R}[V]$ such that each polynomial $f_i$ is the  a sum of squares (of polynomials in $\mathbb{R}[V]$), i.e.~$f_i = \sum_{j=0}^{k} q_{i,j}^2$ for polynomials $q_{i,j}$'s in $\mathbb{R}[V]$.
\end{theorem}

In this work, we utilize the sound form in \eqref{eq:putinar} for witnessing a polynomial $g$ to be non-negative over a polyhedron $P$  for each constraint $\forall \pv\in P.(g(\pv)\ge 0)$ from \textbf{Step A5} of our algorithm. Let $\forall \pv\in P.(g(\pv)\ge 0)$ be such a constraint for which the polyhedron $P$ is defined by the linear inequalities $g_1\ge 0,\dots,g_m\ge 0$. Let $\pvars= \{ v_1, v_2, \ldots, v_t\}$ be the set of program variables and define $\monomials_d(\pvars)= \{m_1, m_2, \ldots, m_r\}$ as the set of all monomials of degree at most $d$ over $\pvars$, i.e.~$\monomials_d(\pvars) :=  \{ \prod_{i=1}^t v_i^{\alpha_i} ~\mid~ \forall i~~\alpha_i\in \mathbb{N} ~\wedge~ \sum_{i=1}^t \alpha_i \leq d \}$. The application of Putinar's to $\forall \pv\in P.(g(\pv)\ge 0)$ has the following steps. 

\begin{itemize}
		\item First, represent each $f_i$ in \cref{eq:putinar} as the positive semidefinite form $f_i=\mathbf{v}^\mathrm{T} \mathbf{Q}_i \mathbf{v}$ subject to the positive semidefinite constraint where each $\mathbf{Q}_i$ is a real matrix whose every entry is an unknown parameter. 
        \item Second, compute an equation in the form \eqref{eq:putinar} whose coefficients are affine expressions in the unknown coefficients from our templates and the unknown entries in the matrices $\mathbf{Q}_i$'s. 
        \item Third, establish the affine constraints between the unknown coefficients in the templates and the unknown entries in the matrices $Q_i$'s by matching the coefficients at the LHS and the RHS of the equation obtained from the previous step. 
\end{itemize}
The overall application processes all such constraints from \textbf{Step A5} of our algorithm by (i) collecting all the affine and the semidefinite constraints from the first and the third steps above and (ii) solve them by semidefinite programming. 

\subsection{Application of Handelman's Positivstellensatz}\label{app:Handelman}

To present Handelman's Positivstellensatz, we need the notion of monoid as follows. 
Below we consider an arbitrary finite collection $\Gamma=\{g_1,\dots, g_k\}$ ($k\ge 1$) of linear functions (i.e., degree-$1$ polynomials) in the program variables.  

\begin{definition}[Monoid]
The \emph{monoid} of $\Gamma$ is defined by:
\[
\mbox{\sl Monoid}(\Gamma):=\left\{\prod_{i=1}^k h_i \mid k\in\Nset_0\mbox{ and }h_1,\dots,h_k\in\Gamma\right\}~~.
\]
\end{definition}

Then in our context, Handelman's Positivstellensatz can be formulated as follows. 

\begin{theorem}[Handelman's Positivstellensatz~\cite{handelman1988representing}]
\label{thm:handelman}
Let $g$ be a polynomial in the program variables such that $g(\pv)>0$ for all program valuations $\pv\in 
P:=\{\pv'\in \Rset^{|\pvars|}\mid g_1(\pv')\ge 0, \dots, g_k(\pv')\ge 0\}$. 
If $P$ is compact, then we have  
\begin{equation}\label{eq:handelman} 
g=\sum_{i=1}^d a_i\cdot u_i 
\end{equation}
for some $d\in\Nset$, real numbers $a_1,\dots,a_d\ge 0$ and $u_1,\dots,u_d\in\mbox{\sl Monoid}(\Gamma)$. 
\end{theorem}

To apply Handelman's Positivstellensatz, we consider a natural number which serves as a bound on the number of multiplicands allowed to form an element in $\mbox{\sl Monoid}(\Gamma)$.
Then~\cref{eq:handelman} results in a system of linear equalities that involves $a_1,\dots,a_d$ and the coefficents of $g$. The application of Handelman's Positivstellensatz to each $\forall \pv\in P.(g(\pv)\ge 0)$
is simpler than that of Putinar's Positivstellensatz, and is as follows. 

\begin{itemize}
        \item First, compute an equation in the form \eqref{eq:handelman} whose coefficients are affine expressions in the unknown coefficients from our templates and the fresh variables $a_1,\dots,a_d$ from \cref{eq:handelman}. 
        \item Second, establish the affine constraints between the unknown coefficients in the templates and the fresh variables $a_1,\dots,a_d$ from \cref{eq:handelman} by matching the coefficients at the LHS and the RHS of the equation obtained from the previous step. 
\end{itemize}
The overall application processes all such constraints from \textbf{Step A5} of our algorithm by (i) collecting all the affine constraints from the second steps above and (ii) solve them by linear programming. 

\section{Supplementary Materials for Section~\ref{sec:experiment}}\label{app:experiments}



\subsection{Possible Approaches for Computing $M_{\mathrm{up}}$ and $M_{\mathrm{low}}$ of Score-recursive WPTS's}

Fix a score-recursive WPTS $\Pi$, and assume it has (1) the concentration property, i.e., $\probm(T>n) \leq c_1 \cdot e^{-c_2 \cdot n}$ for $c_1,c_2>0$, (2) the bounded-update property, and (3) the stepwise weight is bounded by $e^{c_3}$ for $0<c_3<c_2$. Then given the bounded ranges $B$ and $B'$ as computed in \cref{sec:algorithm}, we derive the upper bound $M_{\mathrm{up}}$ and the lower bound $M_{\mathrm{low}}$ for the expected weight from $B'\backslash B$ as follows.

For any $\pv\in B'\backslash B$,
\begin{eqnarray*}
\llbracket \Pi\rrbracket (\pv) &=& \expectdist{\pv}{w_T}   \\
&=& \sum_{n=1}^{\infty} \probm(T=n)\cdot w_n\\
&\le & \sum_{n=0}^{\infty} \probm(T>n)\cdot w_n \\
&\le& 1+\sum_{n=1}^{\infty} \probm(T>n)\cdot w_n \\
&=& 1+\sum_{n=1}^{n^*-1} \probm(T>n)\cdot w_n +\sum_{n=n^*}^{\infty} \probm(T>n)\cdot w_n \\
&\le& 1+M+\sum_{n=n^*}^{\infty} \probm(T>n)\cdot w_n \\
&=& M'+\sum_{n=n^*}^{\infty} \probm(T>n)\cdot w_n \\
&\le& M'+\sum_{n=n^*}^{\infty} c_1\cdot e^{-c_2\cdot n} \cdot w_n \\
&\le& M'+\sum_{n=n^*}^{\infty} c_1\cdot e^{-c_2\cdot n} \cdot e^{c_3\cdot n} \\
&=& M'+c_1\cdot \sum_{n=n^*}^{\infty}(e^{c_3-c_2})^n \\
&=&M'+ c_1\cdot \frac{a}{1-q} \\
&=& M_{\mathrm{up}} \\
\end{eqnarray*}
where 
\[
\sum_{n=1}^{n^*-1} \probm(T>n)\cdot w_n\le \sum_{n=1}^{n^*-1} w_n\le \sum_{n=1}^{n^*-1} (e^{c_3})^n = \frac{a'\cdot (1-(q')^{(n^*-1)})}{1-q'}=:M
\]
and $a'=e^{c_3}$, $q'=e^{c_3}$.
The first inequality is obtained from the fact that
\[
\probm(T>n)=\probm(T\ge n+1)=\probm(T=n+1)+\probm(T=n+2)+\dots,
\]
thus,
\[
\probm(T=n+1)\le \probm(T>n).
\]
The second inequality is derived by the fact that $\probm(T>0)\le 1$ and $w_0=\win=1$. The third inequality is obtained by the definition of $M$ above. The fourth inequality is obtained by the concentration property, while the fifth inequality is derived by the bounded stepwise weight condition.

For $M_{\mathrm{low}}$, we trivially set $M_{\mathrm{low}}=0$. We can refine it heuristically, e.g., according to the monotonicity of the scoring function.

\subsection{Overapproximation via Polynomial Interpolations}
Given a non-polynomial function $f(x)$ over the interval $I=[a,b]$, we aim to approximate $f(x)$ by polynomials $p(x)$'s. The correctness of approximation is based on a classical theorem called Weierstrass' Theorem \cite{jeffreys1988weierstrass}.

\begin{theorem}[Weierstrass' Theorem]\label{thm:Wierstrass}
Let $f(x)$ be a continuous function on the (closed) interval $[a,b]$. Then there is a sequence of polynomials $p_n(x)$ (of degree $n$) such that
\[
\mathop{lim}_{n\to\infty} ||f-p_n||_{\infty}=0.
\]
	
\end{theorem}

We also need the following theorem to measure the derived polynomials. The property of Lipschitz continuity supports the following theorem easily.
\begin{theorem}\label{thm:errorbound}
	Suppose $r(x)$ is a continuous and differentiable function on a compact convex set $\Psi\subseteq \Rset$. Assume that a collection of points $\{x_1,x_2,\dots,x_k\}$ are sampled uniformly from $\Psi$ and $s\in\Rset_{>0}$ is the sampling spacing. Let $r_0=\mathop{max}\{|r(x_1)|,|r(x_2)|,\dots,|r(x_k)|\}$, and $\beta=\mathop{sup}_{x\in\Psi} ||\nabla r(x)||$, then 
	\begin{equation}
	|r(x)|\le \beta\cdot s+ r_0,\ \forall\ x\in \Psi.
	\end{equation}
\end{theorem}


Then our scheme is as follows.
\begin{itemize}
	\item Split the interval $I=[a,b]$ uniformly into $m$ partitions, i.e., $I_1=[a_1,b_1],I_2=[a_2,b_2],\dots,I_m=[a_m,b_m]$.
	\item For each partition $I_i=[a_i,b_i]$, define a $n$-degree polynomial $p^i_n(x):=\sum_{j=0}^n c_{ij}\cdot x^j$.
	\begin{enumerate}
		\item Pick a non-negative integer $k>n$ and sample $k$ points uniformly from $f$ over $I_i$. That is,  \[
		D=\{(x_1,f(x_1)),(x_2,f(x_2)),\dots,(x_k,f(x_k))\}
		\]
		where $x_l\in I_i$ for all $1\le l\le k$.
		\item Let $p_n^i(x_l)=f(x_l)$ for all $1\le l\le k$, then we have a linear system $\mathbf{V}\cdot \mathbf{c}=\mathbf{f}$ where 
		\begin{equation*}
		\mathbf{V}=
		\begin{bmatrix}
		   1 & x_1 & x_1^2& \cdots &x_1^n \\
		   1 & x_2 & x_2^2& \cdots &x_2^n \\
		   \vdots & \vdots & \vdots&   &\vdots \\
		   1 & x_k & x_k^2& \cdots &x_k^n \\
		\end{bmatrix},
		\end{equation*}
		$\mathbf{c}=[c_{i0},c_{i1},\dots,c_{in}]^T$ and $\mathbf{f}=[f(x_1),f(x_2),\dots,f(x_k)]^T$.
		\item By solving the above overdetermined system, we obtain $p_n^i(x)$ as the approximation of $f(x)$ over the interval $I_i=[a_i,b_i]$. 
		\item Having $p_n^i(x)$, evaluate an error bound $\gamma_i$ such that
		\begin{equation}
		\forall\ x\in I_i,\ |f(x)-p_n^i(x)|\le\gamma_i .
		\end{equation} 
		Let $r(x)=f(x)-p_n^i(x)$ and $\Psi=I_i$, then we obtain $r_0=\mathop{max}\{|r(x_1)|,|r(x_2)|,\dots,|r(x_k)|\}$ by \cref{thm:errorbound}. To derive the Lipschitz constant $\beta$ of $r(x)$ over the interval $I_i$, we pick a non-negative integer $q=10k$, and sample $q$ points uniformly from $f$, i.e., we have another collection of points $\{x'_1,x'_2,\dots,x'_q\}$. Let $\beta=\mathop{max}\{|\nabla r(x'_1)|,\dots,|\nabla r(x'_q)|\}$, then
		\[
		\gamma_i:=\beta\cdot s+r_0
		\]
		where $s$ is the corresponding sampling spacing of the $q$ points.
	\end{enumerate}
	\item Now we have a set $D_p$ of tuples of intervals, polynomials and error bounds, i.e., 
	\begin{equation}\label{eq:D_p}
	D_p=\{(I_1,p_n^1(x),\gamma_1),\dots,(I_m,p_n^m(x),\gamma_m)\}
	\end{equation}
 
\end{itemize}

The approximation error bounds $\gamma_i$'s are taken into account when we synthesize the polynomial template $h$. Given a non-polynomial function $f(x)$ such that ${\tt score}(f(x))$ occurs in the program, we obtain a set $D_p$ in the form of \eqref{eq:D_p}.  For each interval $I_i$, we introduce a new variable $r_i$ and approximate $f(x)$ over $I_i$ as $p_n^i(x)+r_i$ with $r_i\in [-\gamma_i,\gamma_i]$. That is, for $1\le i\le m$, we have
\begin{equation}
\forall\ x\in I_i,\ f(x)\approx p_n^i(x)+r_i\ \text{with }r_i\in [-\gamma_i,\gamma_i].
\end{equation}

For a state $(\loc,\pv)$ such that $\loc$ is the location before the command ${\tt score}(f(x))$, there is the unique transition $\tau=\langle \loc, true, F  \rangle$ such that $F=\langle \loc',1, \textbf{1}, f \rangle$ and $\loc'$ is the location that follows the command ${\tt score}(f(x))$. Then for all valuations $\pv\in I(\loc) \wedge \Phi_B$ and $1\le i \le m$, it should hold that 
\begin{itemize}
	\item  for all $\pv[x] \in I_i$ and $r_i\in [-\gamma_i,\gamma_i]$, we have that $\ewt(h)(\loc,\pv) \le h(\loc, \pv)$ (for upper bounds) and $\ewt(h)(\loc,\pv) \ge h(\loc,\pv)$ (for lower bounds) where
	\[
	\ewt(h)(\loc,\pv)=(p_n^i(x)+r_i)\cdot h(\loc',\pv).
	\]
\end{itemize}


\subsection{Other Experimental Results}

% Figure environment removed


% Figure environment removed


% Figure environment removed

% Figure environment removed
