Bayesian probabilistic programming~\cite{DBLP:journals/corr/abs-1809-10756,rainforth2017automating} is a programming paradigm that incorporates
Bayesian reasoning into programming languages, and aims at first modelling probabilistic models as probabilistic programs and then analyzing the models through their program representations. Compared with traditional approaches \cite{McIverM04,McIverM05,SriramCAV,ChatterjeeFG16} that specify an ad-hoc programming language, probabilistic programming languages (PPLs) \cite{DBLP:journals/corr/abs-1809-10756} provide a universal framework to perform Bayesian inference. PPLs have two specific constructs: {\tt sample} and {\tt score} \cite{borgstrom2016lambda}.\footnote{Sometimes {\tt observe} is used instead of {\tt score}~\cite{gordon2014probabilistic}, which has the same implicit effect.} The {\tt sample} construct describes the prior probabilities, allowing to draw samples from a (prior) distribution. The {\tt score} construct records the likelihood of observed data in the form of ``{\tt score}(weight)'',\footnote{The argument ``weight'' corresponds to the likelihood each time the data is observed.} and is typically used to weight the current execution in Monte Carlo simulation. 
Nowadays, Bayesian probabilistic programming has become an active research subject in statistics, machine learning and programming language communities, for which typical Bayesian programming languages include Pyro~\cite{bingham2019pyro}, WebPPL~\cite{dippl}, Anglican~\citep{DBLP:conf/pkdd/TolpinMW15}, Church~\cite{DBLP:conf/uai/GoodmanMRBT08}, etc. 

In this work, we consider the analysis of the normalised posterior distribution (NPD) in Bayesian probabilistic programs. The general statement of the problem is that: given a prior distribution $p(z)$ 
over the latent variables $z\in\Rset^n$ of interest, and a probabilistic model represented by a probabilistic program whose distribution $p(x,z)$ is obtained by observing the event $x\in\Rset^m$ with the likelihood $p({x}{\mid}{z})$, the target is to calculate the NPD $p({z}{\mid}{x})$ by Bayes' rule. 
There are two mainstream variants of the NPD conditioning. The first is soft conditioning~\cite{DBLP:conf/lics/StatonYWHK16} that 
assigns a non-negative weight to the program based on the probability (density) of
a given event occurring. The second is hard conditioning that restricts the weights to be either $0$ or $1$. 
Hard and soft conditioning are incomparable in general, as in some situations ``hard conditioning is a particular case of soft conditioning''~\cite[Page 42]{rainforth2017automating}, while in other cases hard conditioning is more general.

In the literature, there are two classes of approaches to address the 
NPD problem. The first is approximate approaches that estimate the NPD by random simulation, while the second is formal approaches that aim at deriving guaranteed bounds for NPD. 
In approximate approaches, two dominant methods are Markov chain Monte Carlo~\cite{gamerman2006markov} and variational inference~\cite{blei2017variational}. Although approximate approaches can produce approximate results efficiently, they cannot provide formal guarantee within a finite time limit. 
Moreover, as shown in ~\citet{Beutner2022b}, approximate approaches may produce inconsistent results between different simulation methods, which led the machine learning community to develop new variants~\cite{DBLP:conf/icml/MakZO22}. 
In formal approaches, there is a large amount of existing works such as $(\lambda)$PSI~\cite{DBLP:conf/cav/GehrMV16,DBLP:conf/pldi/GehrSV20}, AQUA~\cite{DBLP:conf/atva/HuangDM21}, Hakaru~\cite{DBLP:conf/flops/NarayananCRSZ16} and SPPL~\cite{DBLP:conf/pldi/SaadRM21}, aiming to derive exact inference for NPD. However, these methods are restricted to specific kinds of programs, e.g., programs with closed-form solutions to NPD or without continuous distributions, and none of them can handle probabilistic programs with unbounded while-loops/recursion. 
In recent works, \citet{DBLP:conf/nips/ZaiserMO23} and~\citet{oopsla24} used probability generating functions (PGF) to do exact inference for NPD. However, the former work cannot handle loopy programs, and both of them require a closed-form solution to NPD and only work for discrete observations. The recent work by~\citet{Beutner2022b} infers guaranteed bounds for NPD and handles unbounded recursion and continuous distributions. 
This approach relies on recursion unrolling and hence suffers from the path explosion problem.

\vspace{1ex}
\noindent{\em Challenges and gaps.} In this work, we focus on developing formal approaches to derive guaranteed bounds for NPD over loopy probabilistic programs in the setting of soft conditioning. From the literature, a main challenge is to develop new techniques that circumvent the path explosion problem from the approach~\cite{Beutner2022b}. Another challenge (and gap) is that existing approaches cannot handle the situation where {\tt score} statements with weights greater than $1$ appear inside a loop (which we refer to as \emph{score-recursive} programs), which has received significant attention in statistical phylogenetics~\cite{ronquist2021universal,treeflow}. 

 
For score-recursive programs, the following example shows that {\tt score} inside a loop may cause an integrability issue and thus requires careful treatment. Consider a simple loop ``\textbf{while true do if prob}($0.5$) \textbf{then break else score}($3$) \textbf{fi od}''.
In each loop iteration, the loop terminates directly with probability $\frac{1}{2}$, and continues to execute a score command ``{\tt score}$(3)$'' with the same probability. 
It follows that the normalising constant in NPD is equal to $\sum_{n=1}^{\infty} \probm(T=n)\cdot 3^n=\sum_{n=1}^{\infty} (\frac{3}{2})^n=\infty$,
so that the infinity makes the posterior distribution invalid. 
This is noted in e.g., 
~\citet{DBLP:conf/lics/StatonYWHK16}  
that
unbounded weights may introduce the possibility of ``infinite model evidence errors''. To circumvent the drawback, previous results (e.g.,~\citet{borgstrom2016lambda}) allow only $1$-bounded weights. 

In probabilistic program analysis, polynomial solving~\cite{cost2019wang,DBLP:conf/pldi/WangS0CG21,DBLP:conf/cav/ChakarovS13,DBLP:journals/toplas/ChatterjeeFNH18,ChatterjeeFG16} is a well-established technique and 
naturally avoids the path explosion problem in the approach of \citet{Beutner2022b}.   
In this work, we leverage polynomial solving to address the NPD problem. 
Note that simply applying well-known polynomial solving techniques does not suffice 
for the following reasons:
(a) Polynomial solving is tight usually over a bounded region (see e.g., Weierstrass Approximation Theorem~\cite{jeffreys1988weierstrass}), and in general is not accurate if the region is unbounded;
(b) Polynomial solving synthesizes a single bound. 
However, having a single bound is not enough to get tight bounds for NPD, as one needs different bounds for different program inputs to achieve tightness in the normalisation. 

We address the challenges and gaps mentioned above. Our contributions are as follows.

\paragraph{Our contributions.}  In this work, we present the following contributions: 
\begin{itemize}
\item First, we establish a fixed-point theorem and a multiplicative variant of Optional Stopping Theorem (OST)~\cite{doob1971martingale,williams1991probability}.  
Our fixed-point theorem targets Bayesian probabilistic programs that have almost-sure termination and a single score statement at the end of the programs with a bounded score function (referred to as \emph{score-at-end} Bayesian programs), which is a wide class of Bayesian programs in the literature~\cite{DBLP:conf/cav/GehrMV16,DBLP:conf/pldi/GehrSV20,Beutner2022b}. Our OST variant targets 
\emph{score-recursive} Bayesian programs and 
addresses the integrability issue in these  programs.
\item Second, we apply polynomial solving techniques with our fixed-point theorem and OST variant. In addition to existing polynomial solving techniques, our approach improves the accuracy of the derived NPD bounds by the following: First, we propose a novel truncation operation that truncates a probabilistic program into a bounded range of program values.
Second, we devise our algorithm to synthesize multiple bounds for various program inputs. 
\end{itemize}
Experimental results show that our approach can handle a wide range of benchmarks including non-parametric examples such as Pedestrian~\cite{Beutner2022b} and score-recursive examples such as phylogenetic models~\cite{ronquist2021universal}. 
Compared with the previous approach~\cite{Beutner2022b} over score-at-end benchmarks, our approach reduces the runtime by up to $15$ times, while deriving comparable or even tighter bounds for NPD. 



\paragraph{Limitations.} Our approach has the combinatorial explosion in the degree of polynomial solving. However, by our experimental results, a moderate choice of the degree (e.g., $\le 10$) suffices. Moreover, our synthesis of multiple bounds for various inputs  mitigates the combinatorial explosion. Another limitation is that in our polynomial solving, we utilize linear and semidefinite programming solvers, which may produce unsound results due to numerical errors.
