Below we compare our results with the most related work in the literature.

\paragraph{Approximate methods.} Statistical approaches such as MCMC~\cite{rubinstein2016simulation,gamerman2006markov},  variational inference~\cite{blei2017variational} and 
Hausdorff measure~\cite{DBLP:conf/aistats/RadulA21}
cannot provide formal guarantees for the outcomes w.r.t. posterior distributions in a finite time limit. 
The work \cite{DBLP:journals/corr/abs-2101-01502} proposes a novel sampling framework by combining control-data separation and logical condition propagation, which is actually approximate methods. 
In contrast, our approach has formal guarantees on the derived NPD bounds.


\paragraph{Guaranteed NPD inference.}  Most works on guaranteed inference (such as  $(\lambda)$PSI~\cite{DBLP:conf/cav/GehrMV16,DBLP:conf/pldi/GehrSV20}, AQUA~\cite{DBLP:conf/atva/HuangDM21}, Hakaru~\cite{DBLP:conf/flops/NarayananCRSZ16}, SPPL~\cite{DBLP:conf/pldi/SaadRM21}, etc.) are restricted to specific kinds of programs, e.g., programs with closed-form solutions to NPD or without continuous distributions, and none of them can handle unbounded while-loops/recursion.
The most relevant work ~\cite{Beutner2022b} infers the NPD bounds by recursion unrolling. Our approach circumvents the path explosion problem from recursion unrolling by polynomial solving, and outperforms this approach over various benchmarks.  
Several recent works~\cite{oopsla24,DBLP:conf/nips/ZaiserMO23} consider efficient posterior inference in Bayesian Probabilistic Programming. Note that PGF-based inference methods~\cite{oopsla24,DBLP:conf/nips/ZaiserMO23} can only be applied in discrete probabilistic programs. \cite{oopsla24} presents an extended denotational semantics for discrete Bayesian probabilistic while loops and performs exact inference for loop-free programs and a syntactic class of AST programs, while~\cite{DBLP:conf/nips/ZaiserMO23}  calculates the PGFs in Bayesian probabilistic programs efficiently using approximation and automatic differentiation, but cannot handle while loop. Our methods can handle continuous Bayesian probabilistic programs and derive tight bounds for posterior distributions. 



\paragraph{Static analysis of probabilistic programs.} In recent years, there have been an abundance of works on the static analysis of probabilistic programs. Most of them address fundamental aspects such as 
termination~\cite{DBLP:conf/cav/ChakarovS13,DBLP:conf/popl/ChatterjeeFNH16,DBLP:conf/vmcai/FuC19}, sensitivity~\cite{DBLP:journals/pacmpl/BartheEGHS18,DBLP:journals/pacmpl/WangFCDX20}, expectation~\cite{DBLP:conf/pldi/NgoC018,cost2019wang,DBLP:conf/tacas/BatzCJKKM23}, tail bounds~\cite{kura2019tail,DBLP:conf/pldi/Wang0R21,wang2022tail}, assertion probability~\cite{DBLP:conf/pldi/SankaranarayananCG13,DBLP:conf/pldi/WangS0CG21}, etc. Compared with these results, we have that: 
(a) Our work focuses on normalised posterior distribution in Bayesian probabilistic programming, and hence is an orthogonal objective. 
(b) Although our algorithms follow the previous works on polynomial template solving~\cite{DBLP:conf/cav/ChakarovS13,cost2019wang,DBLP:journals/toplas/ChatterjeeFNH18,ChatterjeeFG16}, we have a truncation operation to increase the accuracy which to our best knowledge is novel.
(c) Our approach extends the classical OST as the previous works~\cite{cost2019wang,DBLP:conf/pldi/Wang0R21} do, but we consider a multiplicative variant, while the work ~\cite{cost2019wang} considers only additive variants. 


