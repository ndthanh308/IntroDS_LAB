% --------------------------------------------------------------------------
% Template for WASPAA-2023 paper; to be used with:
%          waspaa23.sty  - WASPAA 2023 LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
%
% --------------------------------------------------------------------------

\documentclass{article}
\usepackage{waspaa23,amsmath,graphicx,url,times}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsfonts, amssymb}
\usepackage{url}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{array,eqparbox,collcell}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{balance}
\providecommand{\yw}[1]{\textcolor{blue}{{#1}}}
\providecommand{\YW}[1]{\textcolor{blue}{[{\bf #1}]}}
\providecommand{\nz}[1]{\textcolor{magenta}{{#1}}}
\providecommand{\NZ}[1]{\textcolor{magenta}{[{\bf #1}]}}
\providecommand{\zd}[1]{\textcolor{red}{{#1}}}
\providecommand{\ZD}[1]{\textcolor{red}{[{\bf #1}]}}

% Example definitions.
% --------------------
\def\defeqn{\stackrel{\triangle}{=}}
\newcommand{\symvec}[1]{{\mbox{\boldmath $#1$}}}
\newcommand{\symmat}[1]{{\mbox{\boldmath $#1$}}}

% Title.
% --------------------
\title{Mitigating Cross-Database Differences for Learning Unified HRTF Representation}

%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Retain this section as-is for your initial submission   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \name{Author 1,$^{1}$
%       Author 2,$^{2}$
%       Author 3,$^{3}$
%       Author 4$^{2}$}
% \address{$^1$ Author names and affiliations omitted for double-blind review.\\
%          $^2$ Please do not make changes to this section.\\
%          $^3$ Author information may be added after paper acceptance.\\
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Please use the commands below to include author          %%
%%  information for the camera-ready submission after your   %%
%%  paper has been accepted.                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Single addresses (uncomment and modify for single-address case).
%% --------------------
\name{Yutong Wen, You Zhang, Zhiyao Duan \thanks{This work is partially supported by the Goergen Institute for Data Science at the University of Rochester, a New York State Center of Excellence in Data Science award, and synergistic activities funded by the National Science Foundation (NSF) under grant DGE-1922591. 
}}
\address{University of Rochester, Rochester, NY, USA
% \\
% \href{mailto:you.zhang@rochester.edu}{you.zhang@rochester.edu}
}

% \email{you.zhang@rochester.edu}
%
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
%\address{Author Affiliation(s)}
%%
%% For example:
%% ------------
%%\address{School\\
%%       Department\\
%%       Address}

% Two addresses
% --------------------
%\twoauthors
%  {John Doe\sthanks{Thanks to ABC agency for funding.}}
%    {Fictional University\\
%Computer Science Dept., 2133 Long Road\\
%     Gotham, NY 10027, USA \\
%     john@fictional.edu}
%  {Maria Ortega\sthanks{Thanks to XYZ agency for funding.}}
%    {University of the Imagination \\
%     Big Engineering Building, 8765 Dream Blvd. \\
%     New Chicago, IL 60626, USA \\
%     maria@imagination.edu}

% Many authors with many addresses
% --------------------
% \name{John Doe,$^{1}$\sthanks{Thanks to ABC agency for funding.}
%       Maria Ortega,$^{2}$\sthanks{Thanks to XYZ agency for funding.}
%       Third Author,$^{3}$ \sthanks{Also many thanks.}
%       Fourth Author$^{2}$}
% \address{$^1$ Fictional University, Computer Science Dept., 2133 Long Road, Gotham, NY 10027, USA\\ john@fictional.edu\\
%          $^2$ University Imagination, Engineering Building, 8765 Dream Blvd., New Chicago, IL 60626, USA\\
%           maria@imagination.edu, fourthAuthor@imagination.edu\\
%          $^3$ Important Laboratory, 123 Street, City, Country, thirdAuthor@test.edu\\
% }

\begin{document}

\ninept
\maketitle

\begin{sloppy}

\begin{abstract}
  Individualized head-related transfer functions (HRTFs) are crucial for accurate sound positioning in virtual auditory displays. As the acoustic measurement of HRTFs is resource-intensive, predicting individualized HRTFs using machine learning models is a promising approach at scale. Training such models require a unified HRTF representation across multiple databases to utilize their respectively limited samples. However, in addition to differences on the spatial sampling locations, recent studies have shown that, even for the common location, HRTFs across databases manifest consistent differences that make it trivial to tell which databases they come from. This poses a significant challenge for learning a unified HRTF representation across databases.
  %\nz{This leads to that existing datasets only contain dozens of subjects each, and they use different spatial sampling schemes due to different loudspeakers setup, making cross-database HRTF modeling not trivial.}
  % This leads to that each existing measured dataset only contains dozens of subjects. However, machine learning methods are usually data-hungry when learning HRTF representation.
    %This leads to that each existing measured dataset comprises only dozens of subjects, a sample size that may be inadequate for machine learning algorithms when learning HRTF representations. As such, it is desirable for machine learning approaches to leverage merged datasets to enhance the available sample size for optimal model training.
  % that target solving HRTF-related tasks to engage in training on merged datasets. 
  % The datasets use different loudspeaker setups, causing different spatial sampling schemes. % due to , making cross-database HRTF modeling not trivial.
  % Recently, machine learning-based approaches leveraging numerical simulations and anthropometric data have been introduced to enhance the efficiency of individualized HRTF estimation, and mix-dataset training has been introduced to machine learning techniques. 
  % \nz{One previous work ``HRTF Field'' have unified the HRTF magnitude representation using neural fields, which enables cross-database training and demonstrates the effectiveness.}
  %One recent study has pointed out that
  % shown that training machine learning models on 
  %naively combined datasets can potentially impair the model's efficacy, as evidenced by the manifestation of linearly separable discrepancies observed in their frequency responses.
  % \nz{One recent study} have shown pitfalls in the \nz{potential} use of mix-dataset training methodologies \nz{by demonstrating the linearly separable difference across their frequency responses}. 
  In this work, we first identify the possible causes of these cross-database differences, attributing them to variations in the measurement setup. Then, we propose a novel approach to normalize the frequency responses of HRTFs across databases. We show that HRTFs from different databases cannot be classified by their database after normalization. We further show that these normalized HRTFs can be used to learn a more unified HRTF representation across databases than the prior art. We believe that this normalization approach paves the road to many data-intensive tasks on HRTF modeling. %Finally, we demonstrate the effectiveness of our proposed method by testing it through an SVM classifier and applying it to an existing mix dataset training method, resulting in improved performance. Our proposed method effectively alleviates the limitations of current mix-dataset training methodologies.
  % \nz{Our work improves unified HRTF representation by identifying and mitigating the cross-database difference. }
  % Our work contributes to the development of efficient and accurate individualized HRTF estimation methods for virtual auditory displays.
\end{abstract}

\begin{keywords}
Head-related transfer function, cross-database difference, normalization, representation learning, spatial audio
\end{keywords}

\section{Introduction}
\label{sec:intro}



% introduce HRTF and the necessity of personalizing HRTF
Head-related transfer functions (HRTFs) represent the spectral changes that occur as a sound wave travels from a source to the listener's ears, taking into account the listener's unique ear, head, and torso geometry. They serve as a crucial cue for human auditory systems to localize sound sources in a 3D space. HRTFs are fundamental in virtual auditory displays, where they are used to create 
%realistic spatialization and immersion in virtual environments
more accurate localization in virtual environments~\cite{Xie2014HRTF}. 

HRTFs are different from person to person due to the geometry differences. To achieve natural auditory displays, individualized HRTFs are preferred over generic ones since the latter often cause 
% a higher occurrence of 
front-back confusions, distance errors, and other related issues, particularly at locations with a lack of interaural differences~\cite{møller1996binaural}. 
% As such, individual variations in head, ear, and torso geometry play a crucial role in providing spatial cues for accurate sound localization.
% explain it's hard to record personalized HRTFs
The acoustic measurement method, which is the conventional approach for obtaining individualized HRTFs, is resource-intensive in terms of both time and equipment. A typical measurement setup, situated within an anechoic room, includes a vertical array of loudspeakers arranged around an arc and a robotic arm for controlling the horizontal rotation of the arc~\cite{bili}. Several research labs have conducted measurements to obtain individualized HRTFs and published HRTF databases~\cite{3d3a, bili,cipic,hutubs,ita,riec,sadie}, where each of them contains dozens of subjects.
% The  and requires approximately 1.75 hours to complete. 
% \NZ{Add a brief intro to setup and talk about so what}

% introduce recent works and mention the problem of small-scale data
% introduce recent works are on small scale data and are limited, and hrtf field can solve the spatial sampling scheme issue 
% \nz{As the measurement of HRTFs is time-consuming and equipment-intensive, i

In recent years, researchers have been developing machine learning methods to estimate individualized HRTFs from geometry information~\cite{ZIEGELWANGER2016threeDmesh, chun2017deep, Lee2018P-HRTF, wang2021global, wang2022predicting}.
% In recent years, novel approaches have been introduced to enhance the efficiency of individualized HRTF estimation. These techniques encompass numerical simulations, which leverage 3D geometrical scans of individuals' heads to infer their respective HRTFs\cite{ZIEGELWANGER2016threeDmesh}\cite{wang2022predicting}, and HRTF estimation methods relying on anthropometric data\cite{wang2021global}\cite{chun2017deep} and ear images\cite{Lee2018P-HRTF}. 
These methods are trained and evaluated on a single database, which usually comprises only several dozen subjects, thereby limiting models' accuracy and generality to a larger population. A natural strategy to improve the performance of these methods is to enlarge the training set by combining multiple HRTF databases. %To make the method more scalable, it is promising to leverage data across databases to enable mix-database training. 
However, different databases often adopt different spatial sampling schemes, and   %this requires a unified representation across database. 
% It is worth noting that several of these techniques harness machine learning algorithms, wherein data scale significantly impacts the accuracy of the outcome. Nevertheless, it is important to acknowledge that these methods primarily rely on small-scale data sets, usually comprising only a few dozen subjects, thereby limiting their generality and applicability to a larger population. 
mix-database training requires a unified representation across different spatial sampling schemes. Such representation has been attempted by Zhang et al.~\cite{Zhang2022HRTFfield}, where \textit{HRTF field} was proposed as a neural field representation learned from different HRTF databases with different spatial sampling schemes. %across subjects. %Only Zhang's method of HRTF field introduces mix-dataset training, which unifies ten distinct HRTF databases for the first time\cite{Zhang2022HRTFfield}.

%Their work alleviates the issue of spatial sampling scheme differences. Nevertheless, other cross-database differences, such as those resulting from variations in the recording environment and loudspeaker setups, may still persist.

% introduce current limitation of mix-data training
% in addition to spatial sampling scheme issue

In addition to different spatial sampling schemes, a recent study by Pauwels et al.~\cite{pauwels2022relevance} has revealed that there are other significant discrepancies across HRTF databases; these discrepancies are so significant that HRTFs from different databases from the same source locations can be easily classified by a support vector machine (SVM) classifier.
% The researchers pre-processed the recorded HRIRs and retaining only those from common sound source directions across databases. They then demonstrated that a basic machine learning classifier can accurately identify the database from which subjects' HRTFs were derived. 
This finding suggests that there are systematic differences beyond spatial sampling schemes in HRTF data across databases, and these differences are likely to hinder the training of more accurate and generalizable machine learning models for HRTFs on merged databases. 
%hinder the development of a unified representation learning approach across databases by introducing unwanted noise when merging them.
% Zhang's work has demonstrated significant merit, however, recent studies have indicated potential pitfalls in the use of mix-dataset training in HRTF field methodologies. \NZ{Explain how they find the pitfall} Notably, distinct HRTF databases employ varying setups for their measurements, including different equipment, environments, and source positions, thus raising concerns that machine learning models utilized in mix-data training may be susceptible to generalizing across dissimilar measurement setups\cite{pauwels2022relevance}. \NZ{Definition of far-field, } However, the precise underlying factors contributing to the challenge of merging disparate databases have yet to be fully explained, and therefore warrant further investigation through in-depth analysis of the distinct databases.

% summary
In our work, we start by analyzing the potential factors for systematic cross-database differences and illustrate position-dependent differences on average-person HRTF frequency responses. %examining the characteristics of the selected databases that could potentially contribute to the misalignment across databases. 
Then, we propose a novel normalization strategy to reduce the cross-database differences in HRTF frequency responses; We show that this normalization strategy makes HRTFs from different databases hard to distinguish by a kernel support vector machine (SVM) classifier. Finally, we apply the normalization strategy to \textit{HRTF field}\cite{Zhang2022HRTFfield}, a state-of-the-art method for learning a unified HRTF representation across databases; We show that the proposed normalization strategy improves the unified representation learning, resulting in prominent improvement on HRTF reconstruction accuracy on a cross-database reconstruction task.
% demonstrate its benefits on a cross-database HRTF reconstruction task. We show that the proposed normalization strategy significantly improves the reconstruction accuracy overdemonstrate the effectiveness of our proposed method by employing it to train , highlighting the resultant improvement in performance.}
Our code is available at: \url{https://github.com/YutongWen/HRTF_field_norm}.

\begin{table*}[t]
\centering
\begin{tabular}{c|cccccccccc}
\hline
Database Information & ARI  & ITA   & Listen & Crossmod & SADIE II & BiLi & HUTUBS & CIPIC & 3D3A & RIEC \\ \hline
\# Subjects         & 97   & 48    & 50     & 24       & 18       & 52   & 96     & 45    & 38   & 105  \\
\# Positions        & 1550 & 2304* & 187    & 651      & 2818*    & 1680 & 440    & 1250  & 648  & 865  \\
Source Distance (m)  & 1.2  & 1.2   & 1.95   & 1.0      & 1.2      & 2.06 & 1.47   & 1.0   & 0.76 & 1.5  \\ \hline
\end{tabular}
\caption{Database information. Position number with an asterisk * signifies that there are variations for some subjects in that database.} 
% in the number of measured positions.}
\label{tab:dataset_info}
\end{table*}

% % Figure environment removed

% \NZ{Mention the mapping earlier}
\section{Investigating cross-database differences 
% the characteristics of different datasets
}
\label{sec:dataset_characteristics}

%In this section, we aim to conduct a thorough examination of the distinctive characteristics of each database, and compare the differences in the HRTFs across them. Our goal is to identify the underlying factors that contribute to the differences discovered in~\cite{pauwels2022relevance}.

% To develop a normalization technique that can effectively integrate all datasets, it is crucial to thoroughly examine the unique features of each dataset and determine the underlying factors that contribute to their dissimilarity. Thus, in this section, we will expound upon the properties of the 

% \nz{We choose eight measured HRTF databases analyzed in~\cite{pauwels2022relevance}XXXXXX}.
% ten selected datasets (3D3A~\cite{3d3a}, ITA~\cite{ita}, SADIE II~\cite{sadie}, ARI, RIEC~\cite{riec}, BiLi~\cite{bili}, HUTUBS~\cite{hutubs}, Listen, Crossmod, CIPIC~\cite{cipic}) by analyzing their average person HRTF \nz{at different locations}. Frequency above 18kHz is not being analyzed following~\cite{pauwels2022relevance}). The information of each dataset is shown in Table~\ref{tab:dataset_info}. 

% introduce how to get system's response
% \subsection{Average person HRTF across datasets}

%% what is true HRTF and why it is different from the measured HRTF.
HRTFs describe the spatial filtering effect of the listener's head, torso, and outer ears on sound arriving from different directions. However, measured HRTFs often contain effects from the measurement equipment and recording environment as well, which may be the main contributors to the cross-database differences discovered in~\cite{pauwels2022relevance}. We present ten selected databases: 3D3A~\cite{3d3a}, ARI, BiLi~\cite{bili}, CIPIC~\cite{cipic},  Crossmod, HUTUBS~\cite{hutubs}, ITA~\cite{ita}, Listen, RIEC~\cite{riec}, SADIE II~\cite{sadie},       and their information is shown in Table~\ref{tab:dataset_info}.
% We assume that the true HRTFs should be of no difference across databases.

%One factor is the physical limitations of the measurement equipment, which may not be able to capture all of the details of the HRTF, so we analyze only the frequency range of 200 Hz to 18kHz following~\cite{}. Another factor is the measurement setup itself, which may not accurately capture the natural listening environment. 
%Hence, we hypothesize that the cross-database differences is caused by the recording environment and equipment.

% Figure environment removed

% Despite the variability in measurement setups across datasets, 
Figure~\ref{fig:signal_flow} illustrates the top view of a general measure setup and the signal flow from the sound source to in-ear microphones. At a particular location denoted by azimuth $\theta$ and elevation $\phi$, a source signal $x(t)$ is played back by a loudspeaker. The resulting sound wave propagates through the recording environment (typically an anechoic chamber) and reaches the ear canals of a listener. The sound wave is then recorded by the in-ear microphones as $y_l(t)$ and $y_r(t)$. HRTFs are then computed by deconvolving the received signals $y_l(t)$ and $y_r(t)$ with the source signal $x(t)$, assuming that the signal path is a linear time-invariant system.
% as the differences between the source signal $x(t)$ and the received signals $y_l(t)$ and $y_r(t)$, assuming that the signal path is a linear time-invariant system.

One systematic difference across HRTF databases is caused by the frequency response of the loudspeaker. While high-quality loudspeakers are often used in HRTF measurement, the frequency responses of different loudspeakers can be quite different. In addition, loudspeakers used in HRTF measurement are directional loudspeakers, meaning that their frequency responses vary with the direction between the receiver and the loudspeakers. %As the in-ear microphone may not always be in the axial direction of the loudspeaker as the loudspeaker moves in the space, the frequency response of the loudspeaker onto the recorded signal may vary with the location of the loudspeaker. 
This variation can be significant when the loudspeaker is close to the listener. 

Another difference may arise from the frequency responses of microphones. Different microphones used in different databases are likely to have different frequency responses. In addition, different microphones are placed at different depths into the ear canal, causing further variations in frequency responses.

Regarding the recording space, while it is typically intended to be anechoic, the reflections may still cause changes to frequency response
%it may still impose additional frequency responses to the signal 
due to the geometry and material of measurement equipment (e.g., mechanic arm, soundproofing foam, chair, and floor) that makes the space not entirely anechoic. This response may also be dependent on the source location.

%\nz{We further abstract it with block diagram in Figure~\ref{fig:signal_flow}(b).} 
% the input signal denoted by $x(t)$ represents the source reference signal (e.g. sine sweep, burst signals etc.), and $X$ denotes the Laplace Transform of $x(t)$. 

%\zd{For a particular spherical location denoted by azimuth $\theta$ and elevation $\phi$, let $\textit{HRTF}(\theta, \phi)$ be the true HRTF for one ear, and let $A(\theta, \phi)$ be the combined effects from the loudspeaker, space and microphone. Assuming a linear process, then the signal flow can be mathematically described as}
%\begin{equation}
%    Y(\theta, \phi) = X \cdot A(\theta, \phi) \cdot \textit{HRTF}(\theta, \phi),
%\end{equation}
%\zd{where $X$ and $Y(\theta, \phi)$ are the frequency responses of the source signal and the in-ear microphone signal, respectively.}

% where
% \begin{equation}
%     System(\theta, \phi, ear) = L(\theta, \phi) \cdot E(\theta, \phi) \cdot M(\theta, \phi, ear) \label{system_def}
% \end{equation}
%\yw{Please not that the $A(\theta, \phi)$ includes the frequency response of the loudspeaker deployed in generating the sound signal at a given position specified by the spherical coordinates $\theta$ and $\phi$, the microphone response corresponding to the source position $(\theta, \phi)$, and the recording space frequency response which may varies from space to space even though the recordings all take place in anechoic chambers. It is imperative to acknowledge that each position in the acoustic environment is associated with a unique loudspeaker response, since in certain cases, a speaker array is employed in data collection to enhance the efficiency of the measurement process. Similarly, it should be noted that the microphone response also varies with the directionality, although the microphone remains unchanged throughout the measurement process.}
%Please note that the $L(\theta, \phi)$ represents the response characteristic of the loudspeaker deployed in generating the sound signal at a given position specified by the spherical coordinates $\theta$ and $\phi$. It is imperative to acknowledge that each position in the acoustic environment is associated with a unique loudspeaker response, since in certain cases, a speaker array is employed in data collection to enhance the efficiency of the measurement process. Similarly, it should be noted that the microphone response also varies with the directionality, although the microphone remains unchanged throughout the measurement process. Neglecting the potential variations in microphone response that arise from changes in the source position is inappropriate. 

% % Figure environment removed

% % Figure environment removed

%Furthermore, it should be noted that some dataset utilizes a distinct measuring distance, ranging from less than one meter to over two meters. This variance in measuring distance leads to a significant divergence in the frequencies that conform to the far field assumption and those that do not, which further intensifies the misalignment in HRTFs across different datasets. It is worth mentioning that due to the existence of frequencies that violate the far field assumption, the HRTF is not entirely independent of the distance. Thus, it is more appropriate to denote HRTF with $HRTF(\theta, \phi, distance)$ instead of $HRTF(\theta, \phi)$. While our research aims to mitigate the system responses of datasets, which are the spectral inconsistencies that are unrelated to the subjects, the misalignment caused by the variations in distance is beyond the scope of our study. 

% Figure environment removed

% To examine the difference among system responses of all datasets, we average out the variations in the system response resulting from different subjects and source positions. The system response of a dataset comprised of M subjects measured over K position can be approximated by
To empirically investigate the differences in system responses across different databases, we conducted an analysis of the average HRTFs across individuals within each database at specific source positions. We pick the common locations across all databases to remove the effects of the different spatial schemes. To maximize the usage of data and reduce the variance of average, we then map the right ear HRTF at location $(\theta, \phi)$ to its corresponding left ear HRTF at location $(2\pi - \theta, \phi)$. This is to assume the symmetry between the left and the right ear HRTFs of each ``augmented'' subject. %, so that we only need to consider the left ear from each subject. 
This process allows us to double our sample size for the left ear. As such, the following discussions of HRTFs are all about the left ears. % and therefore the HRTFs are not ear dependent in the following analysis. 
This mapping strategy has also been applied to the other experiments in this study. %

%We aggregated HRTF measurements obtained from different subjects at those positions and computed their mean. We hypothesized that, with the availability of a sufficient number of subjects and measured source positions, the true average person HRTF for each dataset at the same source position (i.e., the HRTF without the system response) should exhibit similarity since they conform to the same distribution. Additionally, we analyzed the directionality of the system response by providing the average person HRTF at another source position. If the difference in system responses across different datasets varies from position to position, then the system response is not source position independent. The magnitude results of the average person HRTF at source positions (0, 0) and (90, 0) from eight datasets are presented in Figure~\ref{fig:system_response}.}
%\nz{From the figure, we show that the signal flow for measuring the HRTFs of the two ears are equivalent.} \yw{To increase the size of our datasets, we employed methodology of mapping a given right ear $HRTF(\theta, \phi)$ to its corresponding left ear $HRTF(2\pi - \theta, \phi)$, under the assumption that another individual's left ear HRTF is identical to the right ear HRTF of our study's participants. This process allowed us to double our sample size, and therefore the HRTFs are not ear dependent in the following analysis. This mapping strategy has also been applied to the other experiments in this study.}

% Figure environment removed

% The average person HRTF across all source positions of a dataset comprised of M subjects measured over K position can be approximated by

% \begin{equation}
%     HRTF_{avg} = \frac{1}{MK}  \sum_{m} \sum_{k} Y_{m}(\theta_k, \phi_k, 0) % + Y_{m}(\theta_k, \phi_k, 1)
% \end{equation}
% where $Y_{m}(\theta_k, \phi_k, 0)$ and $Y_{m}(\theta_k, \phi_k, 1)$ denotes the left ear and right ear HRTF for the $m$-th subject measured at the $k$-th source position. The magnitude result of the ten datasets is shown in Figure~\ref{fig:system_response}

Figure~\ref{fig:system_response} shows the average HRTF frequency responses of eight different databases at two common source locations: front (0,0) and right (90,0). A frequency range from 200Hz to 18kHz is displayed in the log-frequency scale.
We can see that different databases show discernible discrepancies in the average HRTF frequency responses at both source positions, suggesting systematic differences in the measurement system responses in these databases. In addition, the difference in the system responses across databases does not remain unchanged when the source position varies, suggesting that the system response is position dependent. We argue that these position-dependent systematic differences in measurement system responses need to be removed or reduced if one would like to combine different HRTF databases to train machine learning models. In the next section, we will propose a strategy to reduce these differences. %Our subsequent investigation aims to establish that by mitigating the variations in system response, we can improve the performance of the machine learning model.
% ITD of different datasets
% \subsection{Interaural Time Difference Across Datasets}
% Further 

% In the next section, we will propose a normalization strategy to reduce these differences.



% 2.1 get the frequency response of the recording system used in a dataset
% Method:
% We suppose the signal flow of a measurement can be characterized by following:
% Measurement signal * loudspeaker loc * environment loc * true HRTF loc * microphone loc = recorded signal at loc
% We summarize:
% Measurement signal * system transfer function loc * true HRTF loc = recorded signal…
% system transfer function = loudspeaker * environment * microphone
% …description of getting the system characteristics (e.g. frequency response)
% Show that datasets have different frequency response (especially different loc has one different freq response), and the plot
% \newline\newline
% 2.2 ITD 
% Description of getting the ITD
% Need a plot to show ITD of different loc in different dataset?
% Maybe state no need to normalize ITD
% \newline\newline


% % Figure environment removed

% \begin{table*}[t]
% \centering
% \begin{tabular}{c|cccccccccc}
% \hline
% Dataset Information & ARI  & ITA   & Listen & Crossmod & SADIE II & BiLi & HUTUBS & CIPIC & 3D3A & RIEC \\ \hline
% \# Subjects         & 97   & 48    & 50     & 24       & 18       & 52   & 96     & 45    & 38   & 105  \\
% \# Positions        & 1550 & 2304* & 187    & 651      & 2818*    & 1680 & 440    & 1250  & 648  & 865  \\
% Source Distance (m)  & 1.2  & 1.2   & 1.95   & 1.0      & 1.2      & 2.06 & 1.47   & 1.0   & 0.76 & 1.5  \\ \hline
% \end{tabular}
% \caption{Dataset information, position number with a * signifies that there are variations in the number of measured positions}
% \label{tab:dataset_info}
% \end{table*}


\section{Mitigating the Spectral Discrepancy across Different Databases}
\label{sec:mitigate_discrepancy}
Based on the findings in the previous section, spectral variations exist among databases and across different source positions, which constitute potential factors impairing the generalization and accuracy of machine learning models. In this section, we propose a normalization approach motivated by the previous section to address systematic spectral differences across different databases. The effectiveness of our method is evaluated by comparing the performance of a kernel SVM classifier trained on the normalized data against that of the original (unprocessed) data.



%\subsection{Positional spectral variation}
%The preceding section omitted the spectral variations arising from different source positions to demonstrate a comprehensive spectral difference across the chosen datasets. However, when striving to mitigate frequency response discrepancies across diverse datasets, it is crucial to incorporate these minor spectral variations, as they constitute crucial dataset-specific features. We herein present evidence of the non-trivial nature of the spectral alterations resulting from changes in the source position.

%The frequency response of a given dataset comprised of $M$ subjects at the source position $\theta, \phi$ for $ear$ (i.e. left or right ear) is then estimated by
%\begin{equation}
%    System(\theta, \phi, ear) \approx \frac{1}{M}  \sum_{m} Y_{m}(\theta, \phi, ear)
%\end{equation}

%Dataset hutubs' frequency response at twelve source positions on the equator (i.e. zero degree elevation) averaging over all subjects' both ears is shown in the Figure \ref{fig:hutubs_FR_pos}. 

%As discernible from Figure \ref{fig:hutubs_FR_pos}, the frequency response of the twelve equatorial positions exhibits significant disparities, underscoring the necessity of normalizing each position's frequency response individually rather than amalgamating them into a single response. 

% \subsection{Data spectral normalization}
% In light of the observations from the last section, we present our normalization approach. 
For a given subject's HRTF $Y(\theta, \phi)$ measured from the source position $\theta$ and $\phi$ in compliance with the database's average person HRTF measured from the same source position $\textit{HRTF}_\mathrm{avg}(\theta, \phi)$, we propose to normalize its HRTF by 
\begin{equation}
    \textit{HRTF}_{\mathrm{normalized}}(\theta, \phi) = \frac{Y(\theta, \phi)}{\textit{HRTF}_{\mathrm{avg}}(\theta, \phi)},
\end{equation} 
where $\textit{HRTF}_{\mathrm{normalized}}(\theta, \phi)$ denotes the normalized HRTF.

% explain why convolved with another HRTFavg
% It is important to note that in this context, a reference system's average person HRTF across all positions is utilized instead of solely normalizing out the system's response of the HRTF. The rationale behind this approach is that the assumption underlying the averaging of different subjects is that the sole common frequency response among HRTFs is the system response. However, it is possible that the distribution of HRTFs is not random, and that they share some common frequency response which is also factored out when removing the system response. As such, it is necessary to incorporate the reference system's response in order to recover common information across HRTFs. In terms of selecting the reference system, we suggest using the system response that contains the largest number of subjects as the reference system response. Furthermore, it should be noted that the reference response is not ear-specific nor source position specific. This decision is motivated by our objective of achieving uniformity in the frequency response of both left and right microphones, and its response is independent of source positions, thus creating the illusion of a single microphone recording the signals from both ears, thereby minimizing dataset-specific features. 

%However, we preserve the directionality information, (i.e. the $\theta$ and $\phi$ in $System'$),\YW{$\theta$ and $\phi$ might not even exist in $System'$. still need to re-think this. The code currently use the reference system response independent to the position} as it would be fallacious to presume that the HRTF distributions recorded at varying positions conform to a single distribution.

% \subsection{Classification of HRTFs}
To assess the effectiveness of our normalization results, we trained a kernel support vector machine (SVM) following \cite{pauwels2022relevance} to classify which database a given HRTF originates from. This particular selection was based on the consideration that kernel SVM classifiers are less susceptible to overfitting in comparison to complex models such as multilayer perceptrons with limited training data.

Following the methodology described in~\cite{pauwels2022relevance},
%In accordance with 
our study proceeded to train the kernel SVM on the subset of twelve positions which were found to be shared across eight of the selected databases; the remaining two databases do not contain these positions in common. In this task, a total of 144 subjects were utilized, with 18 subjects (the smallest database size) extracted from each database. This sampling methodology was adopted to reduce bias toward any database. The total number of HRTFs in this experiment is thus 12 (positions) $\times$ 18 (subjects) $\times$ 2 (ears) = 432 for each database.
% , corresponding to 12 common source directions, 18 subjects, and two ears. 
We perform five-fold cross-validation and report the mean accuracy and confusion matrix in Figure~\ref{fig:cm_results}.

First, we present the SVM classification results on the unnormalized data shown in Figure~\ref{fig:cm_results}(a) as the baseline. The results are close to what is reported in~\cite{pauwels2022relevance} as a sanity check. The classification accuracy is a little lower than their reported 61.2\%. This is because we discarded the two simulated databases they used, which achieved much higher ($>$90\%) accuracy and hence increased their overall classification accuracy. Then, in Figure~\ref{fig:cm_results}(b), we provide the outcomes obtained from position-independent normalization, computed by normalizing the mean average-person HRTFs across 12 shared positions. Finally, we show the results derived from our proposed normalization method in Figure~\ref{fig:cm_results}(c), which is conducted on an individual location and per ear basis.


Comparing the results depicted in Figure~\ref{fig:cm_results} (b) (c) to (a), both accuracy from normalizing by averaging shared positions and by individual position is lower than the accuracy from the unnormalized data, suggesting that both normalization methods are able to confuse the classifier, hence effective in reducing systematic differences across databases. Further, comparing the results in Figure~\ref{fig:cm_results} (b) to (c), position-dependent normalization accuracy is much lower than the position-independent normalization accuracy, reinforcing our analysis in Section~\ref{sec:dataset_characteristics} that the systematic cross-database differences are more likely to be dependent on source position. 
Note that differences due to the overall normalization of the datasets are ruled out as a cause.
The normalization technique applied to individual locations, with results shown in Figure~\ref{fig:cm_results}(c) yields very low classification accuracy, where the accuracy is close to a random guess of classifying into one of the databases. This suggests that the cross-database differences are highly reduced to the extent that the SVM classifier becomes confused. This shows the success of our proposed normalization on an individual position basis.

\section{Mix-database training with neural fields}
\label{sec:mix_data_training}



% Figure environment removed

% To provide a more comprehensive demonstration of the effectiveness of our normalization approach, it is beneficial to compare the results obtained from our methodology versus those which didn't. To enable a valid comparison, we select the HRTF field result \cite{Zhang2022HRTFfield} as the baseline, given that only this work utilizes mix-data training. Consequently, we compare the results achieved through our normalization technique with \cite{Zhang2022HRTFfield}'s original method.

To further evaluate the benefits of our proposed normalization method, we apply it to training machine learning models for learning a unified HRTF representation across databases.

% \subsection{Mix-dataset training and cross-dataset reconstruction}

\subsection{Experimental setup}

%% introduce HRTF Field
\textbf{Model}: \textit{HRTF field}~\cite{Zhang2022HRTFfield} has shown successful mix-databases training since they learned a unified representation across different spatial sampling schemes used by different databases. 
% It applies a generative model for neural field representation, deep neural networks that maps the spherical coordinates to frequency magnitudes. 
The model represents each person's HRTF with $G(\theta, \phi, z)$, where $z$ represents one person's identity. The training and evaluation both involve two steps, as illustrated in Figure~\ref{fig:mix_training}. The first step is to infer a latent vector $z$ from the gradient of the mean squared error (MSE) between the ground truth and setting $z$ at $z_0$. The second step is to update the generator with the MSE loss. After convergence, the generator would learn a unified HRTF representation.

\textbf{Task}: Instead of the interpolation and the generation tasks in the original paper~\cite{Zhang2022HRTFfield}, we propose cross-database reconstruction since we focus on mitigating the cross-database differences. We train the generator with multiple databases combined. During inference, for a given HRTF from another database (different from databases used in training), we employ the trained model to infer $z$ and reconstruct this HRTF. 

\textbf{Evaluation}: The log-spectral distortion (LSD) is utilized as the evaluation metric, and it is defined as
\begin{equation}
\textit{LSD}(H, H') = \sqrt{\frac{1}{PN}\sum_{\theta, \phi}\sum_{n}\left (20 \log_{10}\left | \frac{H(\theta, \phi, n)}{H'(\theta, \phi, n)}  \right | \right  )^2  },
\label{eq: lsd}
\end{equation}
where $H(\theta, \phi, n)$ and $H'(\theta, \phi, n)$ denote the $n$-th frequency bin ($N$ in total) of the ground-truth and predicted HRTFs measured at the source position $(\theta, \phi)$, respectively. $P$ represents the number of total source positions within the database.
% and $N$ 
% and the number of frequency bins, respectively.

As the model is trained on normalized HRTFs, the predicted HRTFs need to be de-normalized using the same normalization factors before computing the LSD with the ground truth. 
However, as the normalization is a division by the average-person HRTF and LSD is in the dB scale, LSD can also be equivalently computed between the predicted HRTFs and the normalized ground truth.
 %the model learns the representation for the normalized HRTFs, and we de-normalized the predicted HRTFs to compute the LSD, so that the results are valid. 

\textbf{Baseline}: In the original paper of HRTF field~\cite{Zhang2022HRTFfield}, the authors applied normalization of the equator energy, but we argue that it is insufficient as it is not frequency dependent nor position dependent. In the following sections, we will compare the performance of our proposed normalization techniques with the original ones.

% \NZ{Introduce HRTF Field a little bit}

% \NZ{Introduce the proposed cross-database construction task, to be continued}

% \NZ{Explain the original normalization from HRTF field: the equator energy}

\subsection{Cross-database reconstruction performance of different normalization methods}
To more effectively demonstrate our improvement, we intentionally construct 5 experiments to incorporate different cases and present the comparison between the obtained results using our normalization method with the baseline in Table \ref{tab:lsd_table}.

In order to evaluate the generalization capability of our proposed method, we first select several databases (BiLi, Crossmod, Listen, SADIE II) that exhibit similar system frequency responses in Figure~\ref{fig:system_response} for training, and a database (ARI) with very different system frequency responses for testing.
% the first training mix-database combination was selected to comprise the databases. The test database is selected to have a  from that of the training databases, 
This would pose a significant challenge to the model's generality. The results are shown under experiment 1 in Table~\ref{tab:lsd_table}. The results show that our proposed approach outperformed the baseline LSD of 7.47 dB, achieving a markedly lower LSD value of 4.69 dB. These results provide empirical support for the effectiveness of our proposed methodology in enhancing the model's generality.

The second experiment is designed to assess the model's capacity to cope with varying system frequency responses in training and testing databases. The databases are selected to ensure that no two databases exhibit similar system frequency responses. Results are shown under experiment 2 in Table~\ref{tab:lsd_table}, showing that our proposed methodology effectively mitigated the model's challenges in dealing with distinct system frequency responses, yielding a notable improvement over the baseline LSD of 5.54 dB, with an LSD of 4.82 dB. This underscores the potential of our approach to enhance the model's capacity to handle variability in system frequency responses across databases.

The third experiment is devised such that
%to examine the effectiveness of our proposed method in enhancing the model's performance in cases where 
the training and testing databases manifest highly similar system frequency responses. The result confirmed the effectiveness of our proposed method in enhancing the model's performance,
%Despite the high degree of similarity in system frequency responses, our approach was found to be effective in improving the model's performance, 
with the LSD improving from 4.31 dB to 3.89 dB. This provides compelling evidence of the versatility of our proposed approach, and its capacity to enhance the model's performance across different scenarios. In the next two experiments (experiments 4 and 5 in Table~\ref{tab:lsd_table}), we use one database randomly chosen for evaluation and the other nine for training. In addition, we do 10-fold cross-validation on this setup, and the average LSD for our result is 4.24 dB and for the baseline is 4.83 dB.

\begin{table}[t]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
    Experiments       & 1                & 2                & 3                & 4                & 5                \\ \hline
ARI        & $\bigcirc$       & $\bigtriangleup$ &                  & $\bigtriangleup$ & $\bigtriangleup$ \\
ITA        &                  &                  &                  & $\bigtriangleup$ & $\bigtriangleup$ \\
Listen     & $\bigtriangleup$ &                  & $\bigcirc$       & $\bigtriangleup$ & $\bigtriangleup$ \\
Crossmod                                                                   & $\bigtriangleup$ & $\bigtriangleup$ & $\bigtriangleup$ & $\bigtriangleup$ & $\bigtriangleup$ \\
SADIE II   & $\bigtriangleup$ &                  & $\bigtriangleup$ & $\bigtriangleup$ & $\bigtriangleup$ \\
BiLi                                                                       & $\bigtriangleup$ & $\bigtriangleup$ & $\bigtriangleup$ & $\bigtriangleup$ & $\bigtriangleup$ \\
HUTUBS     &                  & $\bigtriangleup$ &                  & $\bigtriangleup$ & $\bigcirc$       \\
CIPIC      &                  &                  &                  & $\bigtriangleup$ & $\bigtriangleup$ \\
3D3A       &                  &                  &                  & $\bigtriangleup$ & $\bigtriangleup$ \\
RIEC       &                  & $\bigcirc$       &                  & $\bigcirc$       & $\bigtriangleup$ \\ \hline
HRTF field~\cite{Zhang2022HRTFfield} & 7.47            & 5.54             & 4.31            & 4.43            & 5.01            \\
\textbf{Our proposed}       & \textbf{4.69}   & \textbf{4.82}   & \textbf{3.89}   & \textbf{3.73}   & \textbf{4.04}   \\ 
\hline
% \begin{tabular}[c]{@{}c@{}}Mix Positions\\ and Ears\end{tabular}     
w/o position dependency & 5.61            & 5.32            & 4.32            & 4.00            & 4.89            \\
% \begin{tabular}[c]{@{}c@{}}Individual Position\\ and Mix Ears\end{tabular} 
w/o ear dependency & 5.11             & 5.11            & 3.98            & 3.94            & 4.67            \\
% Ours       & \textbf{4.694}   & \textbf{4.819}   & \textbf{3.893}   & \textbf{3.729}   & \textbf{4.039}   \\ 
\hline
\end{tabular}
\caption{Log-spectral distortion (LSD) in dB scale of cross-database HRTF reconstruction with five different experiments (columns) using different mix-databases. For each experiment, training sets are denoted with $\bigtriangleup$, evaluation sets are denoted with $\bigcirc$, and databases not used are left as blank.}
\label{tab:lsd_table}
\end{table}


% \subsection{Ablation study on our proposed normalized method}
\textbf{Ablation study}: We perform a comprehensive analysis of the significance of each step in our proposed methodology. This was accomplished through a comparison of results obtained from HRTF field \cite{Zhang2022HRTFfield} trained on mix-databases using common position normalization, common microphone normalization (wherein the left and right mics are treated as having identical frequency responses), and our proposed normalization approach. The experimental findings are presented in Table \ref{tab:lsd_table}.
As shown in the table, our proposed normalization method has the best result in each experimental setup, suggesting that each step in our normalization method is necessary. 

% \NZ{Limitation: what we are dividing, recover the for spatial rendering, explain the equivalence of using/or not using denormalization}



\section{Conclusion}
\label{sec:conclusion}
In this study, we identified the different position-dependent system frequency responses as a key factor causing the systematic differences across databases. We proposed a novel normalization strategy that effectively mitigates the influence of this factor. By employing our normalization method, we effectively confused the kernel SVM classifier in performing the task of distinguishing which database a given HRTF originates from. Additionally, we showed that our method improved the unified HRTF representation learning across different HRTF databases, resulting in notable improvement on HRTF reconstruction accuracy. 
% Although our method cannot address all sources of misalignment, 
% such as distance, and subtle position information is not able to be recovered, 
We believe our method represents a promising step towards integrating existing HRTF databases and can benefit future machine learning approaches to tackle challenges on data-intensive HRTF modeling. Overall, our findings contribute to a better understanding of the factors affecting HRTF database misalignment and provide a practical solution for enhancing the accuracy and generalization of HRTF representation learning. Future work includes HRTF normalization and de-normalization strategies for novel source positions.
% so that the last information from the current normalization method can be recovered for any source positions. 





% -------------------------------------------------------------------------
% Either list references using the bibliography style file IEEEtran.bst
\balance
\bibliographystyle{IEEEtran}
\bibliography{main}

%
% or list them by yourself
% \begin{thebibliography}{9}
%
% \bibitem{waspaaweb}
%   \url{http://www.waspaa.com}.
%
% \bibitem{IEEEPDFSpec}
%   {PDF} specification for {IEEE} {X}plore$^{\textregistered}$,
%   \url{http://www.ieee.org/portal/cms_docs/pubs/confstandards/pdfs/IEEE-PDF-SpecV401.pdf}.
%
% \bibitem{PDFOpenSourceTools}
%   Creating high resolution {PDF} files for book production with
%   open source tools,
%   \url{http://www.grassbook.org/neteler/highres_pdf.html}.
%
% \bibitem{eWilliams1999}
% E. Williams, \emph{Fourier Acoustics: Sound Radiation and Nearfield Acoustic
%   Holography}. London, UK: Academic Press, 1999.
%
% \bibitem{ieeecopyright}
%   \url{http://www.ieee.org/web/publications/rights/copyrightmain.html}.
%
% \bibitem{cJones2003}
% C. Jones, A. Smith, and E. Roberts, ``A sample paper in conference
%   proceedings,'' in \emph{Proc. IEEE ICASSP}, vol. II, 2003, pp. 803--806.
%
% \bibitem{aSmith2000}
% A. Smith, C. Jones, and E. Roberts, ``A sample paper in journals,''
%   \emph{IEEE Trans. Signal Process.}, vol. 62, pp. 291--294, Jan. 2000.
%
% \end{thebibliography}


\end{sloppy}
\end{document}
