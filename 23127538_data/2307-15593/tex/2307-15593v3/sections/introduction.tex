\section{Introduction}\label{sec:intro}
% The provenance of a text is valuable context for its consumers: 
% for example, proper source attribution gives credibility to journalism and academic writing.
% \pl{why did we drop the misattribution?  that's more salient motivation;
% our watermark isn't really about source attribution, so don't want to mislead}
% \pl{in general, could cut the first sentence or add misattribution to balance it out}
% conversely, misattribution \pl{or no attribution?} begets misinformation \pl{does it?} and discord \john{is ``discord'' too flowerly language?} \pl{I think it doesn't add much beyond misinformation}. [cite multiple] 
% for example, intelligence agencies 
% widely acknowledge that Russian agents influenced the 2016 U.S. presidential 
% election by operating a bot farm of fake profiles on various social media platforms.
The ability of language models to mass produce human-like text
creates an acute, renewed emphasis on the importance of provenance of generated content.
% An estimated $x$\% \pl{I'm skeptical of how accurate this number can be} of high school students use language models to complete homework assignments,
% often without attribution or permission from their teachers.
For example, the website StackOverflow has banned users from posting answers using OpenAI's
ChatGPT model to mitigate the spread of misinformation on the platform \cite{vincent23stackoverflow}.
A reliable forensic tool for attributing text to a particular language model would empower 
individuals---such as platform moderators and teachers---to 
enact and enforce policies on language model usage; it would also  
better enable model providers to track the (mis)use of their models, e.g.,
to scrub synthetic text from the training data of future language models. %\thc{Somewhat high level note - we're emphasizing the more high stakes stuff with stronger adversaries (misinformation, misuse) but i think the most realistically useful angle for watermarking is either unsophisticated adversaries or raising the cost for attacks}

% \pl{I think this is overall well-written and an inspiring introduction;
% I think I'd shorten it (especially the first paragraph) and add citations}

% a policy stance towards synthetic 
% text---such as a ban, or mandating attribution, or simply scrubbing synthetic text 
% from the training data of future language models---is hopeless.

% \pl{I'd introduce the 3-step setting here and point to the figure; this needs to be grounded out more}

To achieve provenance,
a \textit{watermark} is a signal embedded within some 
generated content---in our case, text from a language model---that encodes the source of the content.
% \pl{I wouldn't use 'data' - can be confused with training data, especially in the context of talking about provenance}
We consider a setting where an untrusted third party user queries a language model (LM) by sending prompts 
to a trusted provider (Figure~\ref{fig:main-top}):
the LM provider generates text from their language model with a watermark so that a detector may later 
identify the source of the text if the user publishes it.
% In particular, the user sends a prompt to an LM provider, which generates text with a watermark;
% the user can then adversarially perturb the text (to avoid detection) before posting or submitting it;
% a detector finds the perturbed text and seeks to verify that it indeed came from the API.
The ideal watermark should satisfy at least the following three desiderata:
\begin{itemize}
    \item[1.] \textbf{distortion-free}---the watermark should preserve the original text distribution; 
    \item[2.] \textbf{agnostic}---it should be detectable without the language model and/or prompt;
    \item[3.] \textbf{robust}---it should withstand perturbations of the watermarked text.
\end{itemize}
Existing watermarks either distort the model's sampling distribution, thus altering the API functionality \cite{kirchenbauer2023watermark,aaronson2023openai}, 
or are not robust to editing or cropping the text \cite{christ2023undetectable}.
Meanwhile, classical steganographic techniques for covertly encoding messages within samples of text from a language model
are neither agnostic nor robust \cite{ziegler2019neural}.
We develop the first watermarks for attributing text to a language model that achieve all three desiderata.
% \rck{citations TBD}

% Figure environment removed

% \pl{more explicitly relate the 4 steps above to the two components - we need to specify the two algorithmic steps that are not controlled by the user}
Our methodology consists of two components, which the LM provider and detector respectively use to execute 
the two steps of the protocol in Figure~\ref{fig:main-top} under their control: 
a $\generate$ method that deterministically maps a sequence $\xi$ of random numbers encoded by a
% \pl{maybe put a footnote that it's not important for the key to be secret; this is not cryptography, which saying secret key would suggest} 
watermark key---which we call the watermark key sequence---to a sample from the language model, and a $\detect$ method that aligns a putative watermarked text
with the watermark key sequence using the shared key.
Informally, our watermarks are \emph{distortion-free} in the sense that---marginalizing over the watermark key sequence---each 
call to $\generate$ is equal in distribution to a sample from the original language model, i.e., the distribution
\begin{align*}
    P(\textbf{text}) = \int_{\xi} \indic{\textbf{text} = \generate(\xi,\textbf{prompt})} d \nu(\xi)
\end{align*}
is equal to the original language model's sampling distribution. %\pl{new sentence, detectability is a different point; define $\phi$ before using it - I find the '[claim] where [definitions]' constructions harder to process}
% \john{can we be more explicit? $p(text) = \int_\xi p(text|\xi)dp(\xi)$? And provide some more intuition re: correlating the watermark $\xi$ with the text.}.


The challenge of detecting watermarked text is that the detector cannot simply recompute $\generate$ and compare its 
output against the text since they do not necessarily know the prompt which produced the text: in practice, users often 
crop the prompt when publishing text from a language model.
% \pl{not obvious why we don't have the prompt - give a 6 word explanation?}
Our watermarks are \emph{agnostic} in the sense that they are easily detectable 
with a suitable model-agnostic and prompt-agnostic test statistic $\phi$ such that 
$\phi(\generate(\xi,\textbf{prompt}),\xi) \ll \phi(\textbf{text},\xi)$
for any $\textbf{text}$ that is independent of the watermark key sequence.
The idea here is that the detector may use $\phi$ within $\detect$ to compute a $p$-value with respect to the null 
hypothesis that the text is independent of the watermark key sequence, i.e.,
that the text is not watermarked.

To ensure $\detect$ is \emph{robust} to edits of the watermarked text, 
the core idea underpinning the design of each test statistic $\phi$ is to leverage techniques 
for robust sequence alignment to align a putative watermarked text with the watermark key sequence;
we quantify the quality of the alignment using an ``alignment cost" specific to each watermark.
The sequence alignment procedure ensures the watermark is detectable from even a 
small, corrupted block of watermarked text planted within some other larger text.
Of course, a sufficiently motivated and/or sophisticated user can still evade detection 
by simply rewriting the text from scratch themselves (or, using another language model to generate the text);
the point of a robust watermark is simply that the amount of effort and/or resources a user requires to produce text 
that evades watermark detection should be commensurate to what they would have expended had they not had access 
to the watermarked language model in the first place.

Whereas $\generate$ is a deterministic function, if our watermark 
produced the same text every time for each prompt it would not be very useful.
We resolve this limitation by designing a wrapper $\shuffle$ around $\generate$ that calls $\generate$ using a randomly chosen subsequence of $\xi$
instead of generating tokens from the same starting point each time.
For the same reasons that $\detect$ is robust to editing and cropping watermarked text,
calling $\generate$ in this fashion does not affect watermark detectability.
In practice, the statistical power of our watermarks improves exponentially with respect 
to the length of the putative watermarked text and diminishes only linearly with the 
length of the random number sequence;
thus, by increasing the length of the random number sequence, we can reduce the probability 
of reusing the same random subsequence while still ensuring our watermark has good statistical power
(i.e., that it yields low $p$-values for watermarked text).
So long as we do not reuse an element of the key sequence, successive calls to $\shuffle$ will 
be jointly indistinguishable from regular calls to the language model.

To remark briefly on the work most closely related to ours, we contrast the distortion-free property of our watermarks
with the hashing-based watermarks of \citet{kirchenbauer2023watermark} and \citet{aaronson2023openai}
that bias the distribution of watermarked text towards certain $k$-grams by hashing 
a sliding window of the previous $k-1$ tokens to determine the next token pseudorandomly.
% even if $\generate$ produces the $k$-gram twice (e.g., if a user asks for a list words 
% that start with the same prefix), %\pl{not sure how this example works - what's $k$, why 5, why aa? I was thinking for the example, consider $k=2$, you couldn't even generate, "a, b, c" because after the comma need to sample from half of the distribution}),
% the resulting text output will still be indistinguishable from a regular sample from the language model.
We give examples of prompts (e.g., ``Give me a list of 20 movies.") for which the bias due to hashing is clearly noticeable in our experiments.
% \pl{give a concrete example of this - I think this is an important point and let's make it visceral}
% \pl{I still would recommend having a simple half-example to make real - e.g., ``generate a list of movies'' even if we're linking to the figure}
\citet{christ2023undetectable} propose a variation of hashing in which the window size changes based on the entropy of the generated tokens to avoid hash collisions with high probability.
Their motivation is similar to ours in that they focus on preserving 
the original text distribution; however, like \citet{kirchenbauer2023watermark} and \citet{aaronson2023openai}, using larger window sizes
hurts robustness as an adversary can break the watermark by replacing a single token in each window.
Our watermark is not only distortion-free but also robust to substantial corruption of the text,
which is crucial in practice.
% \pl{also, do they require knowledge of model / prompt? if so, that's even stronger weakness}
% \pl{should pull 'robust' out into a separate paragraph?}
We defer a more thorough discussion of related work to the next section (Section~\ref{sec:related-work}).

% \pl{should say something about the computational complexity of implementing generate and detect}

% \pl{
%   Overall, I think we can do a better job making more points in a more modular way and anchor with the following phrases:
%   distortion-free,
%   robust,
%   agnostic,
%   restoring entropy;
%   each point should probably be a separate paragraph (right now, various things are kind of mixed together)
% }

% \pl{v2 comments: overall this is better, but could still use more modularity, especially in the last paragraph}

% % Figure environment removed

We describe the details of our methodology in Section~\ref{sec:results}, 
wherein we give two instantiations of watermarks---using inverse transform sampling 
and exponential minimum sampling---and provide analyses of their statistical power.
We experimentally validate the power
and robustness of our watermarks using the OPT-1.3B, LLaMA-7B and Alpaca-7B 
language models in Section~\ref{sec:experiments}.
Across all models, we find the second instantiation using exponential minimum sampling to be 
the most powerful.
For both the OPT-1.3B and LLaMA-7B models, using this watermark we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens 
even after corrupting between $40$-$50$\% of the tokens via random edits (i.e., substitutions,
insertions or deletions);
the watermark also remains detectable from $50$ tokens even after paraphrasing 
the text by translating to French/Russian and back.
For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses 
to typical user instructions. Due to the lower entropy of the responses, detection is more difficult:
around $25\%$ of the responses---whose median length is around $100$ tokens---are detectable with $p \leq 0.01$, 
and the watermark is also less robust to paraphrasing.
We release code for implementing the watermark and reproducing the experiments in this paper, as well as additional supplementary material including an in-browser demo of the watermark detector.\footnote{For assets and supplemental material, see: \url{https://github.com/jthickstun/watermark}.}
% \pl{some of the experimental highlights from the abstract can be put here}

% \pl{in related work, we mention Kirchenbauer and Aaronson by name; we should probably do that up here too}

\subsection{Related work}\label{sec:related-work}
% \rck{Can arguably scrap the entire first paragraph if space constrained}
% \rck{(TODO) change format from numbers to actual names}
Text watermarking is a special case of linguistic steganography, 
in that the goal is to convey a hidden message---the watermark---within a passage of text.
% \john{Not sure if you were planning to add this commented stuff on edit-based watermarks back in. I think we ought to mention this line of work to draw the contrast with more recent generative methods. This is helpful to orient a reader familiar with older work in this space (and strategic; this folks might be reviewers and we want them to be allies)}
Existing approaches to linguistic steganography fall under two broad categories: 
\emph{edit-based} methods that modify a pre-existing text,
and \emph{generative} methods that construct a distribution over cover text \citep{ueoka2021frustratingly}. 
Crucially, in contrast to steganography, the literature on digital watermarking has historically foregrounded 
robustness to corruption as a key attribute of a good watermark \citep{katzenbeisser2000digital,atallah2001natural}.
In this light, a text watermark should be able to withstand some perturbations of the text,
thus precluding the direct application of many existing techniques for linguistic steganography \citep{dai2019towards,ziegler2019neural,shen2020near}.

% \pl{steganography is okay if you're watermarking data before posting so you can catch people who train on your data}

% \john{this makes it soundlike robust watermarking is a special case, 
% when in fact robustness has been part of the definition of a digital 
% watermark since the earliest work. 
% Maybe we should cite an early paper for a problem statement, 
% e.g., Natural Language Watermarking and Tamperproofing (Atallah et. al., 2002) 
% or Natural Language Watermarking (Topkara et. al., 2005).}

% ---namely, 
% the watermark should withstand some paraphrasing of the text---thus 
% precluding the direct application of many existing steganographic methods [cite ArithDec/multiple]. 

Older work on text watermarking considers editing a pre-existing text to include 
a watermark \citep{rizzo2019fine,abdelnabi2021adversarial,yang2022tracing}; 
for a survey of edit-based watermarks, see \citet{kamaruddin2018review}. In contrast, 
we are interested in generating watermarked text while preserving the distribution over the text from a language model.
Work on generative watermarking is nascent, underwritten by recent advances in open-ended text generation \citep{brown2020language}. 
Pioneering work by \citet{venugopal2011watermarking} proposed a generative watermark for the output of a machine translation system, biasing the system towards translations with particular features that can later be detected using a hypothesis test.

Our work is most closely related to \citet{kirchenbauer2023watermark}, 
who watermark text from a language model by reweighting the token log-probabilities 
from the model at inference time as a function (i.e., hash) of the previous $k-1$ tokens, where $k \in \N$ is a hyperparameter.
In ongoing unpublished work concurrent to ours, \citet{aaronson2023openai} describes a technique for watermarking language models 
using exponential minimum sampling (a close relative of the Gumbel trick \cite{papandreou2011perturbandmaprf}) 
to sample from the model, where the inputs to the sampling mechanism
are also a hash of the previous $k-1$ tokens.
Neither watermark is distortion-free, and in fact we show in our experiments that the 
distortions manifest noticeably in practice (e.g., excessive repetition of certain tokens).
Specifically, both \citet{kirchenbauer2023watermark} and \citet{aaronson2023openai}
bias the distribution toward a subset of $k$-grams.
% \pl{I think the first order bit is to say that their method isn't distortion free and this creates artifacts (and forward reference experiments section)}
Increasing $k$ makes the bias less noticeable but hurts the robustness of both watermarks:
an adversary can break the signal from a particular token by
replacing any one of the previous $k-1$ tokens.

Also concurrent to our work, 
\citet{christ2023undetectable} propose watermarking blocks of text from a language model
by hashing each block to seed a sampler for the next block.
\citet{christ2023undetectable} vary their block sizes---which are analogous to the hyperparameter $k$ of \citet{kirchenbauer2023watermark} 
and \citet{aaronson2023openai}---as a function of the empirical entropy of the constituent tokens to avoid using the same seed twice with high probability.
Their work is similar to ours in that they preserve the original language model's sampling distribution;
however, the resulting watermark is not robust since in order to mitigate the distortion induced by hashing
the block sizes must be sufficiently large to avoid hash collisions with high probability over all 
blocks and---similar to \citet{kirchenbauer2023watermark} and \citet{aaronson2023openai}---replacing 
any token in the previous block breaks the watermark in the next block.
Whereas \citet{christ2023undetectable}---who do not run experiments---choose their block sizes to be sufficiently 
large to minimize distortion, \citet{kirchenbauer2023watermark} and \citet{aaronson2023openai} 
recommend choosing $k$ to be a small constant in practice, which ensures a moderate amount of robustness 
by introducing some distortion.
Finally, whereas our definition distortion-freeness implies exact equality in distribution of watermarked text to unwatermarked text for a single query 
to the language model, \citet{christ2023undetectable} propose a definition of ``undetectability'' that implies approximate equality in distribution, i.e., 
approximate distortion-freeness, over multiple queries.
Using $\shuffle$, we also achieve approximate distortion-freeness in the multiple query setting, though the runtime of our watermark detection procedure must grow with the number of queries; 
we discuss these trade-offs in more detail in Section~\ref{sec:discussion}.
% \pl{more objective to say they don't have experiments?} 

An alternative approach for detecting synthetic text is to 
learn a classifier between synthetic and human text \citep{jawahar2020automatic,mitchell2023detectgpt}.
A key advantage of such methods over watermarking is that they do not require coordination with the original producer of the text 
(i.e., the LM provider); however, their effectiveness is distribution dependent and they do not provide a priori (distribution-free)
guarantees on the significance level of detection (i.e., Type I errors).
% \thc{I think accuracy is not the right thing here - these systems might be accurate, but they are distribution dependent, and dont provide Type-I error guarantees}
% \pl{I agree accuracy isn't right; I think is that we provide guarantees, and there are no a priori (distribution-free) guarantees;
% I think this contrast is important and most people are more familiar with the detection strategies;
% so I'd spend a bit more time on this;
% say that work is empirical, whereas ours is by construction;
% not to say that one is better than the other, but just giving pros and cons
% }

Finally, we note that our setting is different from the literature on planting watermarks in the training data of machine learning models, 
e.g., to infer the model's training set or otherwise influence the model's output \citep{he2022protecting,he2022cater,zhao2023protecting}.
Such watermarks are not distortion-free by design, since the point is to plant some learnable signal in the training data that influences 
the behavior of models which train on the watermarked data.
% \pl{last sentence should be new paragraph - this is a completely different idea;
% need to explain more what this is; not clear what 'orthogonal' means}

% \pl{I think given distortion-free is the main point, can we show an example where it matters (Kirchenbauer versus us)?}

% \pl{In general, I think there could be more examples of generated text from the watermark throughout the paper}
