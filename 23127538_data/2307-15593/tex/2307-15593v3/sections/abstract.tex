\begin{abstract}
%   \thc{consider putting robust into the title?}
    We propose a methodology for planting watermarks in text from an 
    autoregressive language model that are robust to perturbations without 
    changing the distribution over text up to a certain maximum generation budget.
    % \thc{IMO `without distorting' doesnt get across the strenght of our result, which is that it does not affect the distribution of generated texts *at all*}
    We generate watermarked text by mapping a sequence of random numbers---which 
    we compute using a randomized watermark key---to a sample from the language model.
    To detect watermarked text, any party who knows the key can align the 
    text to the random number sequence.
    % Our watermark is undetectable by anyone who does not know the watermark key so long as
    % the total number of generated tokens is less than the length of the random number sequence;
    % while in principle reusing elements of the sequence to generate text induces mild correlation across samples,
    % empirically it is not an acute problem.
    % \pl{say induces mild correlation across samples, but empirically is not an acute problem (works better)}
    We instantiate our watermark methodology with two sampling schemes: 
    inverse transform sampling and exponential minimum sampling.
    We apply these watermarks to three language models---OPT-1.3B, LLaMA-7B and Alpaca-7B---to 
    experimentally validate their statistical power and robustness to various paraphrasing attacks.
    Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens 
    even after corrupting between $40$-$50$\% of the tokens via random edits (i.e., substitutions,
    insertions or deletions). 
    % the watermark also remains detectable from $50$ tokens even after paraphrasing 
    % the text by translating to French/Russian and back.
    For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses 
    to typical user instructions. Due to the lower entropy of the responses, detection is more difficult:
    around $25\%$ of the responses---whose median length is around $100$ tokens---are detectable with $p \leq 0.01$, 
    and the watermark is also less robust to certain automated paraphrasing attacks we implement.\footnote{
        We release all code publicly at \url{https://github.com/jthickstun/watermark}.
    }
    % \pl{mention we have code / demo with link - can be a footnote}
\end{abstract}
