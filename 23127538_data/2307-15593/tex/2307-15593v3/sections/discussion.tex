\section{Discussion}\label{sec:discussion}
In this paper, we give the first distortion-free watermarking strategies for language models that 
are robust to editing and/or cropping.
The key idea underpinning our approach is to leverage methods for robust sequence alignment 
to align a putative watermarked text to a watermark key sequence which the LM
provider uses to generate watermarked text.
The statistical power of our watermarks improves exponentially with respect to the length of the 
text and diminishes only linearly with respect to the length of the watermark key sequence.

The core assumption underlying watermarking
is that the LM provider and the watermark detector coordinate 
by sharing information in advance, e.g., a watermark key.
Indeed, the main inherent limitation of watermarking is that the detector must trust the LM provider to 
faithfully apply the watermark when generating text.
A second limitation, 
which is not inherent to watermarking language models but does presently apply to all known watermarks,
is that the LM provider cannot release the model weights, since then users could simply 
query the model directly instead of through the LM provider.
Planting robust watermarks directly into the weights of a language model without 
degrading the quality of the model is an important direction for future work.

\subsection{Trade-offs among watermarks}
Hashing-based watermarks \citep{aaronson2023openai,kirchenbauer2023watermark,christ2023undetectable}
incur a direct trade-off between the degree of distortion versus robustness: larger hash windows reduce distortion but 
hurt robustness.
We avoid sacrificing distortion-freeness for robustness by choosing to formulate watermark detection as a sequence alignment problem; 
however, this design choice introduces a new trade-off: the computational complexity of our watermark detection algorithms grows linearly with
the length of the watermark key sequence.
In contrast, the complexities of the watermark detection algorithms of both \citet{christ2023undetectable}
and also \citet{aaronson2023openai} and \citet{kirchenbauer2023watermark} depend (in essence) only on 
the length of the input text.
Just as the watermark key length imposes a cap on both
the total number of distortion-free watermarked tokens the LM provider may generate for a single query as well as 
the expected total they can generate across multiple queries before reusing a part of the key sequence, the
the window size imposes a cap on the number of (distortion-free) tokens one can expect to generate using a hashing-based 
watermark without incurring a hash collision.
Whether this apparent tension between detection complexity, robustness and (approximate) distortion-freeness 
is due to fundamental trade-offs is an interesting open question.

% \rck{I've ballpark measured most of these numbers already but need to double check and fill stuff in.}
To illustrate how these trade-offs manifest in practice,
suppose an LM provider responds with $m = 100$ tokens to 
a sequence of $T = 10$ user queries (which may be adaptively chosen).
Recall from Section~\ref{sec:multi-query} that
we must set the key length $n = \omega(mT^2)$ to achieve approximate distortion-freeness using our watermarks,
where here $mT^2 = 10000$;
for $n = 10000$, the runtimes\footnote{
    We report average runtimes and the associated standard deviations across $5$ calls on an Apple M2 Macbook Pro Laptop. We include benchmarking scripts
    with our code release.
} of our implementation of the test statistics for 
{\its}, {\itsedit}, {\gumb}, {\gumbedit} are $0.004 \pm 0.0002$, $0.60 \pm 0.01$, $2.20 \pm 0.01$ and $3.21 \pm 0.01$ seconds respectively.\footnote{
    In principle, running $\detect$ requires recomputing the test statistic for each resampled watermark key in order 
    to obtain an exact $p$-value. However, as we discuss in Section~\ref{app:experiments} and Appendix~\ref{app:experiments-pvals}, we can avoid this recomputation 
    and still obtain approximate $p$-values with respect to some reference distribution of unwatermarked text,
    in which case we need only compute the single test statistic once (using the original watermark key) during watermark detection (Algorithm~\ref{algorithm:modified-detect}).
}

In order to achieve approximate distortion-freeness using a hashing-based watermark in the same setting, the LM provider must set the window size $k$ to be sufficiently large
as a function of $m$ and $T$.
For example, \citet{christ2023undetectable} argue that the probability of incurring the same sequence of tokens twice decays exponentially with the observed 
entropy of the token sequence (which will depend on the previous tokens); thus, they dynamically adjust the window size $k$ during generation to ensure the observed entropy of the constituent 
tokens in the window is sufficiently large such that the probability of a hash collision with any of the other windows is negligible across all $mT$ tokens.
Specifically, letting $h$ denote the expected observed per-token entropy (i.e., log-probability),
the typical window size for a hashing-based watermark would need to be at least $k \approx \frac{\log m T}{h}$ in order to ensure approximate distortion-freeness in our setting.
% \footnote{
%     See line 9 of Algorithm 5 in their paper
% and the subsequent analysis of undetectability in Theorem 9.
% }
% of a hash collision using a hashing-based watermark with window size $k$ decays at most inverse exponentially with $k h$
% and grows roughly linearly with the total number of tokens generated (or more precisely, the total number of windows).\footnote{
%     Here we treat each window as independent, ignoring the fact that some (i.e., adjacent) windows will overlap.
%     Thus, our analysis arguably underestimates the robustness-distortion trade-off of hashing-based watermarks (i.e., we overestimate the robustness achievable by 
%     hashing-based watermarks at a given level of distortion). \citet{christ2023undetectable} control the probability of hash collisions with their watermark
%     by using non-overlapping windows.
% }
% Thus, deploying a hashing-based watermark in the same setting (i.e., $m = 100$, $T = 10$) would require a typical window size of $k = \omega(\frac{\log m T}{h})$ to ensure negligible distortion.
Concretely, in the setting of Section~\ref{sec:exp-instruct}, we have $h \approx 0.70$ for Alpaca-7B,
in which case such a hashing-based watermark would not be robust to replacing  
more than roughly $10\%$ of watermarked tokens.

\subsection{Recommendations in practice and combining watermarks}
We conclude with some salient recommendations for practitioners aiming to watermark their deployed language models.
First, though in principle the length of the watermark key sequence $n$---which recall imposes a cap on the total 
number of distortion-free watermarked tokens the LM provider can generate---can grow (nearly) exponentially 
in the block size $k$ of the test statistic while still enabling watermark detection from as few as $k$ tokens,
in practice we find that using a fairly small watermark key sequence (e.g., $n = 256$) 
does not noticeably affect the quality of watermarked text (i.e., even when generating more than $n$ tokens total)
while allowing for fast detection and improved robustness.
In settings where robustness is important (e.g., discouraging students from using a language model for homework assistance) 
we recommend practitioners use our {\gumbedit} watermark, as it is by far the most robust watermark 
of those we tested.
Meanwhile, in settings where throughput of detection is important (e.g., scrubbing synthetic text from a large training corpus),
we recommend practitioners use our {\its} watermark: its detection is essentially an instance of maximum inner-product search, a problem 
for which there exist various fast implementations and indexing structures (e.g., via vector databases).

% Finally, we remark that it is possible to combine our watermark with the hashing-based watermark of \citet{kirchenbauer2023watermark}, which 
% recall biases the distribution of the next token by upweighting certain logits over others, by using our watermark sampling schemes to sample tokens from the 
% distribution induced by \citet{kirchenbauer2023watermark}; one can then determine which detection procedure to run 
% depending on whether throughput versus robustness is a higher priority.
Finally, we remark that for certain hashing-based watermarks we can combine our watermark with the hashing-based watermark 
to generate watermarked text that is detectable
using either of the two corresponding watermark detection algorithms.
For example, we can use the hashing-based watermark of Kirchenbauer et al. (2023), 
which biases the distribution of the next token by upweighting certain logits over others,
to determine the distribution of the next token and then use our watermarks to sample from this next token distribution.
One can then later determine which detection procedure to run for a collection of putative watermarked text depending on whether throughput or robustness is a higher priority.
Similarly, we can combine our watermarks with the watermark of \citet{christ2023undetectable} by alternating tokens between 
the watermarks (and only hashing the alternate tokens).
The resulting watermarked text will be approximately distortion-free since both watermarks
are approximately distortion-free.
One can choose between the two detection procedures to optimize precision versus recall in adversarial settings; 
in particular, the watermark of \citet{christ2023undetectable} is hard to spoof (due to its multi-query undetectability guarantee) while 
our watermarks are harder to remove (due to our robustness guarantees).
Exploring such combinations of watermarks with complementary strengths is an exciting direction for future work.

% That said, our watermark detection procedures (i.e., both $\detect$ and the test statistic therein from Algorithm~\ref{algorithm:test})
% are easily parallelizeable, so we expect even with a very large watermark key sequence (e.g., $n = 100000$) the computational demands of watermark 
% detection will not be a significant bottleneck---though 
% we caveat this speculation by noting that we did not ever run such large $n$ with our implementation.


% \pl{can we talk more about other future directions - like actually getting this
% watermarking strategy implemented and deployed - seems urgent right now given
% these models are used;
% can we talk about how easy it is to use, that we have a library;
% what other open problems are there?
% }
% \rck{re: deployment, seems better to talk about in the intro}
% \rck{elaborated a bit on future work}

\section*{Acknowledgement}
We thank Saminul Haque, Gary Cheng and Padma Kuditipudi for pointing out errors in preliminary drafts of this work
and for their helpful feedback in general.
% The members of this research team belong to the Stanford Center for Research on Foundation Models (CRFM) 
% and the Stanford Natural Language Processing Group. 
This work is supported by an Open Philanthropy Project Award (OpenPhil) and an NSF Frontier Award (NSF Grant no. 1805310).