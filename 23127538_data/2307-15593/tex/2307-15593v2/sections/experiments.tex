\section{Experimental results}\label{sec:experiments}
We empirically validate the statistical power of our watermarking strategies 
(i.e., {\its}, {\itsedit}, {\gumb}, and {\gumbedit})
via experiments with the OPT-1.3B \citep{zhang2022opt} and LLaMA-7B \citep{touvron2023llama} models.\footnote{
    We will also at times collectively refer to {\its} and {\itsedit} as the ITS watermarks and/or strategies and 
    {\gumb} and {\gumbedit} as the EXP watermarks and/or strategies.
}
We run experiments using $\generate$ rather than $\shuffle$, mainly for the sake of reproducibility;
recall however that this choice has no impact on the $p$-values we report.
We test for all watermarks using a block size $k$ (in Algorithm~\ref{algorithm:test}) 
equal to the length $m$ of the text.
Following the methodology of \citet{kirchenbauer2023watermark}, 
we generate watermarked text continuations of prompts 
sampled from the news-like subset of the C4 dataset \citep{raffel2020exploring}.
We vary the generation length $m$ (Experiment 1) and the random number sequence length $n$ (Experiment 2),
and we report median $p$-values of watermarked text over $500$ samples.\footnote{
    The median $p$-value corresponds to the significance level (i.e., Type I error rate) 
    at which the power of our watermark detector is at least $0.5$.
}
% \thc{is it clear that this is evaluating power? power should be the detection rate at a fixed p-value, not the size of the p-value itself}.
% \pl{I also don't see in what sense this is a proxy? why not just say what we're doing instead of power?}

We also evaluate robustness to four kinds of paraphrasing attacks:
randomly substituting a fraction of the generated tokens 
with tokens chosen uniformly at random from the vocabulary (Experiment 3);
randomly inserting a fraction of tokens among the generated tokens (Experiment 4);
randomly deleting a fraction of the generated tokens (Experiment 5);
using another language model to translate the text from English to French 
and back (Experiment 6). The first three attacks allow us to systematically vary the level of corruption,
while the last attack is an example of an attack we might encounter in the wild.
We defer the details of the translation procedures to Appendix~\ref{app:experiments-roundtrip}.

Finally, using the Alpaca-7B model and evaluation dataset \cite{taori23alpaca}, 
we conduct a case-study on the feasibility of watermarking 
the responses of a performant instruction-tuned language model to user queries.
We also show for certain kinds of instructions that hashing-based watermarks
produce noticeably worse responses than our distortion-free watermarks, thus underlining 
the importance of the distortion-free property in practice.

In all our experiments---except for Experiment 2, where the control variable $n$ is a hyperparameter that is unique 
to our watermarks---we also replicate the watermark of \citet{kirchenbauer2023watermark} as a baseline, 
setting the greenlist fraction $\gamma = 0.25$ and varying the logit bias $\delta \in \{1.0,2.0\}$. %and sampling tokens without beam search.
We respectively refer to these versions of their watermark as {\kgwone} and {\kgwtwo} after the first three authors' last names.
We emphasize their watermark is not directly comparable to our watermarks as it is not distortion-free
(e.g., \citet{kirchenbauer2023watermark} report that even the weakest version we employ with $\delta = 1.0$ and $\gamma = 0.25$
typically increases perplexity by 5--10\%).

In their work, \citet{kirchenbauer2023watermark} report approximate $p$-values, which they obtain from 
computing the $z$-score of a certain test statistic.
To ensure a fair comparison, we use $\detect$ (with $T = 5000$) to report $p$-values for all watermarks;\footnote{
    This setting of $T$ means we never report $p$-values less than $1/5000$ (i.e., $0.0002$) in any of our experiments. 
} %\thc{is this fair to kirchenbauer? im not sure what krichenbauer's test is, but if its a t-test or something, wont they get more power out of the parametric assumption?};
in the case of {\kgwone} and {\kgwtwo}, we run $\detect$ using the original inexact $p$-values they report as the test statistic.
% \pl{what does this mean? elaborate?} \rck{elaborated a little, not sure if enough}
We report error bars for the median $p$-value based on a bootstrapped estimate of the standard deviation
using $1000$ resamples.
% \pl{provide more details?} \rck{added number of resamples}
% \thc{Is this discreteness an issue in just on experiment? if so, i think its probably cleaner to move this comment there.} 

Instead of recomputing the test statistic $T$ times for each prompt---as we originally prescribe 
in $\detect$---to save computation we simply sample $T$ prompts and compute the test statistic once 
for each ground-truth length $m$ completion;
we then use the empirical distribution of these test statistics as the reference distribution 
within $\detect$, which gives a proper $p$-value with respect to the null hypothesis that 
the text is an original completion from the dataset.
For reference, we include the full pseudocode for this modified version of $\detect$ in Appendix~\ref{app:experiments-pvals},
and we also plot the full distributions of $p$-values for nonwatermarked generations (i.e.,
regular samples from the language models) to verify they are indeed roughly uniform over the interval $[0,1]$.

We defer further details regarding our experimental protocol to Appendix~\ref{app:experiments}.

\subsection{Varying text and watermark key length}
We vary the length $m$ of watermarked text in Figure~\ref{fig:experiment-1}, fixing the watermark key length 
$n = 256$ for each of our watermarks and setting $\gamma = 0.4$
for {\itsedit} and $\gamma = 0.0$ for {\gumbedit} 
(see Appendix~\ref{app:experiments-hyperparam} for the details of tuning $\gamma$).
Our ITS watermarks slightly outperform {\kgwone} while our EXP watermarks slightly outperform {\kgwtwo},
despite the fact that {\kgwone} and {\kgwtwo} both distort the text distribution. 
% \thc{both here and earlier when you talk about distortion of kgwone and kgwtwo, can we dump some samples from each watermark and shove it into the appdix? hopefully the distortions are strong enough to visibly see. this would make the paper a bit more self contained without doing things like sample quality evals.}
The EXP watermarks are notably more powerful than the ITS watermarks, requiring roughly two to three times
fewer tokens to achieve a comparably low median $p$-value.
One conceivable advantage %\pl{is this really an advantage? I guess mostly for the detector, since the generator has to hold the parameters anyway} 
of the ITS watermarks over the EXP watermarks is that they have comparatively less overhead:
the watermark key for {\gumb} and {\gumbedit} is a sequence of $n$ vectors in $[0,1]^N$, where recall $N$ is the size 
of the vocabulary, while for {\its} and {\itsedit} it is simply a sequence of $n$ numbers in $[0,1]$.
All watermarking strategies perform worse on LLaMA-7B than OPT-1.3B, due to the fact that LLaMA-7B typically produces lower entropy text than OPT-1.3B.
Due to the discrete nature of the test statistic of \citet{kirchenbauer2023watermark}, i.e., the number of tokens 
in the text belonging to a ``greenlist" versus a ``redlist'', the median $p$-values 
for the {\kgwone} and {\kgwtwo} watermarks are occasionally unstable, particularly for small values of $m$.

% Figure environment removed
  
We vary the length $n$ of the watermark key sequence $\xi$ in Figures~\ref{fig:experiment-2-tr} and \ref{fig:experiment-2-gu}
for different lengths $m$ of watermarked text from the ITS and EXP watermarks respectively.
Recall $n$ corresponds to the total number of tokens we can generate while maintaining our 
distortion-free guarantee.
As our theory predicts, the $p$-values of watermarked text grow linearly with $n$.
The rate of growth is fairly mild and decreases rapidly with $m$;
even for $n = 4096$, which is larger than the maximum generation length of both the OPT-1.3B and LLaMA-7B models,
slightly increasing the number of tokens (by 4--8 tokens in the case of EXP, and 10--20 tokens in the case of ITS)
suffices to distinguish watermarked text with roughly the same statistical power as $n = 64$.

% \pl{clarify that there's no shift going on here?
% wonder if we should have the shift as the default?
% }

% Figure environment removed

% Figure environment removed

\subsection{Robustness to corruption and paraphrasing}
We now proceed to evaluate the robustness of our watermark strategies to various forms of corruption and paraphrasing.
We focus on comparing our strongest watermarks ({\gumb} and {\gumbedit}) 
against {\kgwtwo}, deferring results for all other watermarks to Appendix~\ref{app:experiments-defer}.
As larger $n$ increases the computational overhead of computing our test statistics 
and the effect of larger $n$ on statistical power is mild (as shown in Figure~\ref{fig:experiment-2-gu}),
we run all experiments with $n = 256$, which in any case is sufficiently large to ensure the watermarked text across all experiments is distortion-free.
% \thc{why is this gamma discussion here? doesnt it fit better in the robustness section?}
Decreasing the insertion/deletion penalty $\gamma$ improves robustness (at least up to a point) but hurts the statistical power of the {\itsedit}  and {\gumbedit} watermarks for larger $n$, 
since reducing the penalizer for edits effectively increases the number of candidate alignments under consideration.
We run {\itsedit} and {\gumbedit} with the same choices of $\gamma$ as in the previous section.
We defer the details of tuning $\gamma$ to Appendix~\ref{app:experiments-hyperparam}.

We vary the fraction of substituted tokens in Figure~\ref{fig:experiment-3},
and we vary the fraction of inserted and deleted tokens in Figures~\ref{fig:experiment-4} and \ref{fig:experiment-5} respectively.
For the insertion experiment, we pass only the first $m$ tokens to the detector; similarly, for the deletion experiment,
we initially generate more than $m$ watermarked tokens so that even after deleting a fraction thereof, there are at least $m$ tokens remaining.
The {\gumb} and {\gumbedit} watermarks are comparably robust to substitution errors, but the latter is far more robust 
to insertion and deletion errors.
% Notably, Figure~\ref{fig:experiment-4} suggests the {\gumbedit} watermark would be robust to a version of the ``emoji attack'' \citet{kirchenbauer2023watermark}
% devise to break their watermark, wherein to evade watermark detection the user prompts the language model to insert an emoji before each token and then deletes 
% the emojis before sending the text to the watermark detector (i.e., the user deletes half the generated tokens).\footnote{
%     While one could certainly strengthen the emoji attack by inserting more than one emoji before each token 
%     (\citet{kirchenbauer2023watermark} use two emojis separated from the other tokens by spaces), this may hurt text quality.
%     Also, the robustness of {\gumbedit} to insertions improves as we increase the number of tokens $m$.
% }

We compare our watermarks against the most robust version of {\kgwtwo}, in the sense that we hash only the previous token 
to determine the next token distribution and thus bias the distribution towards some subset of bigrams.
If instead we hash the previous $k$ tokens for $k > 1$, then substituting any one of the previous $k$ tokens will break 
the watermark signal in a particular token, and thus the statistical power of their watermark will be worse 
than what we report in our experiments.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% We vary the fraction of inserted and deleted tokens in Figure~\ref{fig:experiment-3}.
% KGW+ outperforms our {\its} watermark, while {\gumb} is still by far the most powerful.
% Unlike with all previous experiments, we find $k = m$ is no longer necessarily the optimal 
% block size.
% In particular, without incorporating Levenshtein distance into our test statistic,
% a single insertion and/or deletion within a block of tokens can significantly degrade the alignment 
% with the random number sequence; setting $k < m$ increases the likelihood that at least a few blocks are 
% preserved in their entirety.
% Fortunately, incorporating Levenshtein distance largely eliminates this trade-off. 
% In particular, ITS-$\gamma$ and {\gumb} with the optimal $k = m$ each respectively 
% outperform {\its} and {\gumb} with any choice of $k$.

Finally, in Figures~\ref{fig:experiment-6-french} and \ref{fig:experiment-6-russian} we implement a ``roundtrip translation'' attack, wherein we attempt to paraphrase watermarked texts of varying lengths
by translating the (English) texts into another language (i.e., French and Russian respectively) and back again using 
a machine translation model (details in Appendix~\ref{app:experiments-roundtrip}).
We include a representative example of the original and (re-)translated texts in Figure~\ref{fig:french-example}.
Using Russian is a noticeably more effective attack than French: none of the watermarks aside from {\gumbedit}
are able to reliably detect watermarked text with $p < 0.05$ irrespective of $m$.

In many cases, both using French and Russian, the roundtrip translation still preserves large chunks of
the original text, which suffices for watermark detection even using {\gumb}, which is substantially less robust 
to insertion and deletion errors than {\gumbedit}.
Aside from inspecting a few examples, we did not verify that the roundtrip translations preserve the basic semantics of the original text;
thus, it is possible our results provide an overly pessimistic view of the robustness of our watermarks to these attacks, 
since in practice users would presumably not publish such examples.
It is also possible that using different machine translation models---or more generally, different forms 
of automated paraphrasing---might be far more effective in evading watermark 
detection than those we employed.
We publish the full set of watermarked generations for each watermarking strategy, 
along with their (roundtrip) translations, as part of our code release.

% \pl{how much of this is because the translation is bad versus the translation just moves to a farther paraphrase?
% One general thing we should state more clearly from the very beginning is what the threat model here is;
% what can the user perturb (what's the cost model)?
% there should be some constraints about preserving the semantics;
% and given that we don't guarantee this in the experiemnts,
% how should we interpret the results - as a lower/upper bound?
% If we can still detect the watermark, then having a stronger perturbation is fine - a weaker perturbation will clearly be detectable;
% but if our conclusion is that we can't detect the watermark, then this says little
% because it might because the text was too corrupted anyway for it to be useful
% }

% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{Case study: instruction following}
In the wild, most users interact with language models by prompting the model with instructions 
(e.g., ``give me code for...''), and the most widely-used language models (e.g., ChatGPT) 
are specifically fine-tuned to follow such instructions.
Thus, using the instruction fine-tuned Alpaca-7B model,
we presently conduct a case study on the effectiveness of watermarking a performant instruction following model.
In particular, we sample $200$ instructions from the Alpaca-7B evaluation dataset and generate watermarked responses 
of at most $200$ tokens for each.
We then compute conditionally valid $p$-values for each response using the original version of $\detect$
with $T = 500$.
% \pl{why even mention this? I would assume that you just use the same procedure as before - might be confusing to mention what we don't do}
We also replicate the roundtrip translation attack from Experiment 6.
We publish the full set of watermarked generations for each method, 
along with their (roundtrip) translations, and the instruction prompts as part of our code release.

We plot the distribution of $p$-values for the {\gumbedit} and {\kgwtwo} watermarks in Figure~\ref{fig:experiment-7},
as well as the $p$-values versus the watermark potential of the watermarked text in Figure~\ref{fig:experiment-7-potential}.
In general, the Alpaca-7B responses have considerably lower per-token watermark potential than both the OPT-1.3B and LLaMA-7B
models, and thus the statistical power of our watermark is worse despite the responses typically being longer than in 
the previous experiments (i.e., Experiments 1 and 6).
In particular, based on the same random sample of $200$ prompts (from the Alpaca evaluation set in the case of Alpaca-7B, and from the 
news-like subset of the C4 dataset in the cases of LLaMA-7B and OPT-1.3B),
the average per-token watermark potential of text from Alpaca-7B is $0.28$, compared to $0.59$
for LLaMA-7B and $0.67$ for OPT-1.3B.
Unlike the previous experiments, {\kgwtwo} noticeably outperforms the {\gumbedit} watermark. 
% \thc{I know we cap the max length, but is KGW consistently longer than our watermark, or goes into weird gibberish that has higher watermark potential?}
% \rck{after we fixed eos issue lengths seem fairly similar---in any case, dumping samples to appendix should address this (see other TODO)}
Figure~\ref{fig:experiment-7-potential} indicates this difference in performance 
is largely due to the fact {\kgwtwo} distorts the distribution of the text and produces
responses with noticeably larger watermark potential than regular responses from the model.
For responses whose unnormalized watermark potential (i.e., watermark potential multiplied by the number of tokens in the response, to 
account for the varying lengths of the responses) exceeds roughly $60$, both watermarks tend to yield $p$-values close to zero.
Paraphrasing the responses via roundtrip translation attacks into both French and Russian degrades 
the statistical power of both watermarks, as we show in Figures~\ref{fig:experiment-7-french} 
and \ref{fig:experiment-7-russian}.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% \thc{can you dump samples from both methods, along with whether the samples were flagged at the 0.05 level into the appendix? IMO if we dont do sample quality evals, having a random sample sheet is pretty useful to a careful reader}
% \rck{TODO}
Finally, recall the main distinguishing feature of our watermark compared to \citet{kirchenbauer2023watermark} and \citet{aaronson2023openai} is that we do not hash previous tokens to determine 
the distribution of the next token.
To demonstrate the pitfalls of hashing, we implement a version of the watermark \citet{aaronson2023openai} proposes by modifying the $\generate$ 
method of {\gumb} to obtain the vector $\xi_i \in [0,1]^N$ from seeding a random number generator using the previous $k$ tokens
instead of using the watermark key;
we call this version {\gumbhash}.
We then prompt Alpaca-7B with requests for various kinds of lists.
Because Alpaca-7B tends to separate items in lists
by the same recurring token, e.g., a comma or a newline character, 
and because this recurring token determines the next token, for $k = 1$ the lists degenerate into repetition (Figure~\ref{fig:experiment-7-hashing}).\footnote{
    The authors would like to pat themselves on the back by drawing the reader's attention to the fact that the title of this paper is not among those suggested by Alpaca-7B.
}

From inspection, hashing with $k > 1$ substantially improves the quality of samples;
however, even using $k = 4$ can sometimes produce noticeably repetitive text.
We reiterate that while increasing $k$ may improve sample quality by making the distortions of watermarked text less noticeable,
doing so harms the robustness of the watermark (e.g., replacing just $20\%$ of the tokens would suffice to evade detection for $k = 4$).
Moreover, using a more robust hash function does not avoid this trade-off between robustness and distortion-freeness, 
as there is a direct trade-off between the likelihood of a hash collision and the robustness of the hash.
In addition to Figure~\ref{fig:experiment-7-hashing}, 
we include more examples (for both $k = 1$ and $k = 4$) and different prompts in Appendix~\ref{app:experiments-instruct} and our code release.
% \pl{use Alpaca-7B rather than Alpaca everywhere}

% Figure environment removed


