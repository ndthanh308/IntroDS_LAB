\section{Discussion}\label{sec:discussion}
In this paper, we give the first distortion-free watermarking strategies for language models that 
are robust to editing and/or cropping.
The key idea underpinning our approach is to leverage methods for robust sequence alignment 
to align a putative watermarked text to a watermark key sequence which the LM
provider uses to generate watermarked text.
The statistical power of our watermarks improves exponentially with respect to the length of the 
text and diminishes only linearly with respect to the length of the watermark key sequence.

The computational complexity of our watermark detection algorithms grows linearly with
the length of the watermark key sequence, which is also 
the total number of distortion-free watermarked tokens the LM provider may generate.
In contrast, the complexities of the watermark detection algorithms of both \citet{christ2023undetectable}
and also \citet{aaronson2023openai} and \citet{kirchenbauer2023watermark} depend only on 
the length of the input text; however, the former watermark is not robust to corruption and 
the latter two watermarks are not distortion-free.
Whether this apparent trade-off between computational complexity, robustness and distortion-freeness 
is a fundamental trade-off is an interesting open question.

The underlying assumption behind all of the above watermarking strategies including ours
is that the LM provider and the watermark detector coordinate 
by sharing information in advance, e.g., a watermark key.
Indeed, the main inherent limitation of watermarking is that the detector must trust the LM provider to 
faithfully apply the watermark when generating text.
A second limitation, 
which is not inherent but does presently apply to all known watermarks,
is that the LM provider cannot release the model weights, since then users could simply 
query the model directly instead of through the LM provider.
Planting robust watermarks directly into the weights of a language model without 
degrading the quality of the model is an important direction for future work.
% \pl{I think we speed too great of a fraction of the discussion talking about the one limitation
% that we can just turn off the watermark;
% I'd make sure we sum up the main takeaways first (to John's point below);
% }

Recently, several major language model providers (among others: OpenAI, Anthropic, Google and Meta) have pledged 
to watermark the text from their models \cite{bartz23pledge}.
Thus, we conclude with some salient recommendations for practitioners.
First, we recommend practitioners use our {\gumbedit} watermark, as it is by far the most robust watermark 
of those we tested.
Second, though in principle the length of the watermark key sequence $n$---which recall imposes a cap on the total 
number of distortion-free watermarked tokens the LM provider can generate---can grow (nearly) exponentially 
in the block size $k$ of the test statistic while still enabling watermark detection from as few as $k$ tokens,
in practice we find that using a fairly small watermark key sequence (e.g., $n = 256$) 
does not noticeably affect the quality of watermarked text (i.e., even when generating more than $n$ tokens total).
Our watermark detection procedures (i.e., both $\detect$ and the test statistic therein from Algorithm~\ref{algorithm:test})
are easily parallizeable, so we expect even with a very large watermark key sequence (e.g., $n = 100000$) the computational demands of watermark 
detection will not be a significant bottleneck---though 
we caveat this speculation by noting that we did not ever run such large $n$ with our implementation.


% \pl{can we talk more about other future directions - like actually getting this
% watermarking strategy implemented and deployed - seems urgent right now given
% these models are used;
% can we talk about how easy it is to use, that we have a library;
% what other open problems are there?
% }
% \rck{re: deployment, seems better to talk about in the intro}
% \rck{elaborated a bit on future work}

\section*{Acknowledgement}
We thank Saminul Haque, Gary Cheng and Padma Kuditipudi for pointing out errors in preliminary drafts of this work
and for their helpful feedback in general.
% The members of this research team belong to the Stanford Center for Research on Foundation Models (CRFM) 
% and the Stanford Natural Language Processing Group. 
This work is supported by an Open Philanthropy Project Award (OpenPhil) and an NSF Frontier Award (NSF Grant no. 1805310).