\section{Analysis of inverse transform sampling}\label{app:transform}
To prove the main theorems, we introduce the following supporting lemma.
Recall $C_0 = \txt{Var}(\eta(\txt{Unif}([N])))$.

\begin{lemma}\label{lemma:core-result-cov}
    Let $\mu \in \Delta([N])$. Let $(U,\pi) \sim \txt{Unif}([0,1]) \times \txt{Unif}(\Pi)$ and
    $Y = \decode((U,\pi),\mu)$. 
    Then $\frac{1}{C_0}\txt{Cov}(U,\eta(\pi(Y)) \mid Y) =  1-\mu(Y)$ almost surely.
\end{lemma}
\begin{proof}
    We first characterize the conditional distribution of $\pi$ given $Y$
    and the conditional distribution of $U$ given both $\pi$ and $Y$, 
    where recall $\pi$ and $Y$ are discrete.
    Applying Bayes' formula and Theorem~\ref{thm:stealth}, we have
    \begin{align}
        \P(\pi \mid Y) = \frac{\P(Y \mid \pi) \P(\pi)}{\P(Y)}
        \stackrel{(\star)}{=} \frac{\mu(Y) \P(\pi)}{\P(Y)}
        = \P(\pi). \label{eqn:helper-display-1}
    \end{align}
    Also, defining the interval
    \begin{align*}
        I(Y,\pi) \defeq \left[P(\{y : \pi(y) < \pi(Y)\}),\mu(\{y : \pi(y) \leq \pi(Y)\})\right],
    \end{align*}
    for any interval $I \subset [0,1]$ we have
    \begin{align}
        \P(U \in I \mid Y,\pi)
        \stackrel{(a)}{=} \frac{\P(Y \mid U \in I,\pi)\P(U \in I)\P(\pi)}{\mu(Y)\P(\pi)}
        \stackrel{(b)}{=} \frac{\abs{I \cap I(Y,\pi)}}{\mu(Y)}
        \stackrel{(c)}{=} \frac{\abs{I \cap I(Y,\pi)}}{\abs{I(Y,\pi)}}, \label{eqn:helper-display-2}
    \end{align}
    where ($a$) follows from Bayes' formula and the independence of $U$ and $\pi$;
    ($b$) follows from the definition~\eqref{eqn:rho-defn} of the decoder $\decode$; 
    and ($c$) follows from $I(Y,\pi) \subset [0,1]$ having width equal to
    $\mu(Y)$.
    The displays~\eqref{eqn:helper-display-1} and~\eqref{eqn:helper-display-2} respectively imply $\pi \mid Y \sim \txt{Unif}(\Pi)$
    and $U \mid \pi,Y \sim \txt{Unif}(I(Y,\pi))$, from which it follows that
    \begin{align*}
        \Ep\left[U \mid Y,\pi(Y)\right] & = \Ep\left[\mu(\{y:\pi(y)<\pi(Y)\})  + \frac{\abs{I(Y,\pi)}}{2} \;\middle|\; Y,\pi(Y)\right] \\
        & = \frac{\left(\pi(Y)-1\right)\left(1-\mu(Y)\right)}{n-1} + \frac{\mu(Y)}{2} \\
        % & = \frac{1}{2} + \left(\frac{\pi(Y)-1}{n-1}-\frac{1}{2}\right)\left(1-P(Y)\right) \\
        & = 1/2 + \left(\eta(\pi(Y))-1/2\right)\left(1-\mu(Y)\right).
    \end{align*}
    By symmetry, we have $\Ep[U] = \Ep[\eta(\pi(Y))] = 1/2$, the former 
    because $\P(Y \mid U) = \P(Y \mid 1-U)$ for any $U$ and 
    the latter because recall $\pi \mid Y$ is uniform over $\Pi$.
    Thus, marginalizing the preceding display over $\pi(Y)$ gives
    \begin{align*}
        \txt{Cov}(U,\eta(\pi(Y)) \mid Y) & = \Ep \left[\left(U-1/2\right)\left(\eta(\pi(Y)) - 1/2\right) \mid Y\right] \\
        & = (1 - \mu(Y)) \txt{Var}(\eta(\pi(Y)) \mid Y),
    \end{align*}
    from which the desired result follows immediately from recalling
    $\pi(Y) \mid Y \sim \txt{Unif}([N])$ and the definition of the constant $C_0$.
\end{proof}
\subsection{Proof of Lemma~\ref{lemma:sound-transform}}
\begin{proof}
    Recall by definition
    \begin{align*}
        d(Y_i,\xi_i) = -(U_i - 1/2) \cdot (\eta(\pi_i(Y_i)) - 1/2),
    \end{align*}
    where (as in the proof of Lemma~\ref{lemma:core-result-cov}) we have $\Ep[U_i \mid Y] = \Ep[\eta(\pi_i(Y_i)) \mid Y] = 1/2$.
    Lemma~\ref{lemma:core-result-cov} thus implies $\Ep[d(Y_i,\xi_i) \mid Y] = -C_0 \cdot (1 - p(Y_i \mid Y_{:i-1}))$,
    while trivially $\Ep[d(Y_i,\xi_j') \mid Y] = 0$ as $Y$ and $\xi'$ are independent.
    The result follows immediately.
\end{proof}
\subsection{Proof of Lemma~\ref{lemma:perm-test}}
We prove the following more general result, from which Lemma~\ref{lemma:perm-test} follows as a corollary.
\begin{lemma}
    Let $m,n \in \N$ with $n \geq m$, where $m$ is the generation length and $n$ is the watermark key length. 
    Define the decoder $\decode$ by equation~\eqref{eqn:rho-defn}, alignment score $d$ by equation~\eqref{eqn:d-defn},
    and $\phi$ by Algorithm~\ref{algorithm:test} with block size $k \leq m$.
    Let $\xi,\xi' \iid \txt{Unif}(\Xi^n)$ with $Y = \generate(\xi;n,p,\decode)$.
    Let $\wt{Y}$ be a substring of $Y$ of length at least $k$ that is conditionally independent of 
    $\xi$ and $\xi'$ given $Y$, i.e., $\wt{Y} = Y_{\tau+1:\tau+\ell}$ for $\ell \geq k$.
    Then for $\est{\alpha} \defeq 1 - \frac{1}{k}\sum_{i=\tau+1}^{\tau+k} p(Y_i \mid Y_{:i-1})$, almost surely
    \begin{align*}
        \P(\phi(\wt{Y},\xi') \leq \phi(\wt{Y},\xi) \mid \wt{Y},Y) \leq 2n\exp\left(-k C_0^2\est{\alpha}^2/2\right).
    \end{align*}
\end{lemma}
\begin{proof}
    % Recall by assumption $\wt{Y} = Y_{j+1:j+m}$ for some 
    % $j \leq n - m$ that is conditionally independent of $\xi$ given $Y$. Without loss of generality, 
    % suppose $j = 0$, i.e., we have $\wt{Y}_i = Y_i = \decode((U_i,\pi_i),p(Y_{:i-1}))$ for $j \in [m]$.
    % For convenience, define $\est{\alpha} \defeq 1 - \frac{1}{k}\sum_{i=\tau+1}^{\tau+k} p(Y_i \mid Y_{:i-1})$.
    Recall by definition
    \begin{align}
        d(y,(u,\pi)) = -\sum_{i=1}^{\len{y}} (u_i - 1/2) \cdot (\eta(\pi_i(y_i)) - 1/2), \label{eqn:d-defn-review}
    \end{align}
    Lemma~\ref{lemma:sound-transform} and the conditional independence of $\tau$ and $\xi$ given $Y$ 
    imply for any $j \in [n]$ that 
    \begin{align*}
        \Ep[d(\wt{Y}_{1:k},\xi_{(j+1:j+k)\% n}') \mid Y,\wt{Y}] - \Ep[d(\wt{Y}_{1:k},\xi_{\tau+1:\tau+k})\mid Y,\wt{Y}] = k C_0 \est{\alpha}.
    \end{align*}
    
    Each summand in equation~\eqref{eqn:d-defn-review} lies between $-1/4$ and $1/4$,
    and also $(U_i,\pi_i)$ is conditionally independent of $U_{-i}$ and $\pi_{-i}$ given $Y$.
    Thus, Hoeffding's inequality \cite[Proposition 2.5]{wainwright19} implies for $j \in [n]$ that 
    \begin{align*}
        & \P\left(d(\wt{Y},\xi_{(j+1:j+k)\% n}') \leq d(\wt{Y},\xi_{\tau+1:\tau+k}) \mid Y,\wt{Y}\right) \\
        & \leq \P\left(d(\wt{Y},\xi_{1:m}) - \Ep[d(\wt{Y},\xi_{1:m})] \geq kC_0\est{\alpha}/2 \mid Y,\wt{Y}\right) \\
        &  \qquad + \P\left(\Ep[d(\wt{Y},\xi_{j+1:j+m}')] - d(\wt{Y},\xi_{j+1:j+m}') \geq k C_0 \est{\alpha}/2\mid Y,\wt{Y}\right) \\
        & \leq 2\exp\left(-m C_0^2\est{\alpha}^2/2\right).
    \end{align*}
    Recalling the definition of the test statistic $\phi$ via Algorithm~\ref{algorithm:test},
    the main claim then follows from taking a union bound over all $j \in [n]$.
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:perm-test-subs}}
\begin{proof}
    We begin with the following observation for a single token.
    \begin{observation}\label{obs:robust-perm-test}
        Let $P \in \Delta([N])$. Let $(U,\pi) \sim \txt{Unif}([0,1]) \times \txt{Unif}(\Pi)$ and
        $Y = \decode((U,\pi),P)$. Let $\wt{Y} \in [N]$ be conditionally independent of $(U,\pi)$ given $Y$.
        If $\wt{Y} \neq Y$, then almost surely
        \begin{align*}
            \txt{Cov}(U,\eta(\pi(\wt{Y})) \mid Y,\wt{Y}) = -\frac{1}{N-1}\txt{Cov}(U,\eta(\pi(Y)) \mid Y,\wt{Y}).
        \end{align*}
    \end{observation}
    \begin{proof}[Proof of Observation~\ref{obs:robust-perm-test}]
        Observe the conditional distribution of $\pi(\wt{Y})$ given $Y$ is uniform over 
        $[N] \setminus \{\pi(Y)\}$.
        Let $X$ be a random variable that is equal to $\eta(\pi(Y))$
        with probability $1/N$ and otherwise equal to $\eta(\pi(\wt{Y}))$.
        Observe $X$ is independent of $Y$ and thus also $U$ by assumption---in particular, $(N-1)X + 1 \mid Y \sim \txt{Unif}([N])$
        irrespective of the value of $Y$.
        The claim thus follows from rearranging terms in the equality
        \begin{align*}
            0 = \txt{Cov}(U,X \mid Y,\wt{Y}) = \frac{1}{N} \txt{Cov}(U,\eta(\pi(Y)) \mid Y,\wt{Y}) + \frac{N-1}{N}\txt{Cov}(U,\eta(\pi(\wt{Y})) \mid Y,\wt{Y}).
        \end{align*}
        % we have
        % \begin{align*}
        %     \txt{Cov}(U,\eta(\pi(\wt{Y})) \mid Y) \geq -\frac{1}{N-1}\txt{Cov}(U,\eta(\pi(Y)) \mid Y).
        % \end{align*}
        % The claim then follows immediately as both $U$ and $\eta(\cdot)$ are bounded in $[0,1]$.
    \end{proof}

    % Proceeding, let $\xi =: \{(U_i,\pi_i)\}_{i=1}^n$ and $\xi' =: \{(U_i',\pi_i')\}_{i=1}^n$.
    Lemma~\ref{lemma:sound-transform} and Observation~\ref{obs:robust-perm-test} 
    together imply for any $j \in [n]$ that 
    \begin{align*}
        \Ep[d(\wt{Y},\xi_{j+1:j+m}') \mid \wt{Y},Y] - \Ep[d(\wt{Y},\xi_{1:m}) \mid \wt{Y},Y] = m C_0\wt{\alpha}(Y,\wt{Y}),
    \end{align*}
    i.e., by adding the two results together using Observation~\ref{obs:robust-perm-test} to account for 
    the influence of each substituted token on the expectation.
    Using the same concentration argument as in the proof of Theorem~\ref{lemma:perm-test},
    we then have 
    \begin{align*}
        & \P\left(d(\wt{Y},\xi_{j+1:j+m}') \leq d(\wt{Y},\xi_{1:m})\mid \wt{Y},Y\right) \\
        & \leq \P\left(d(\wt{Y},\xi_{1:m}) - \Ep[d(\wt{Y},\xi_{1:m})] \geq m\wt{\alpha}(Y,\wt{Y})/2 \mid \wt{Y},Y\right) \\
        &  \qquad + \P\left(\Ep[d(\wt{Y},\xi_{j+1:j+m}')] - d(\wt{Y},\xi_{j+1:j+m}') \geq m \wt{\alpha}(Y,\wt{Y})/2\mid \wt{Y},Y\right) \\
        & \leq 2\exp\left(-m C_0^2\wt{\alpha}(Y,\wt{Y})^2/2\right).
    \end{align*}
    Recalling the definition of the test statistic $\phi$ via Algorithm~\ref{algorithm:test},
    the main claim then follows from taking a union bound over all $j \in [n]$ and recalling $k = m$ by assumption.
\end{proof}
\subsection{Proof of Lemma~\ref{lemma:perm-test-indel}}
\begin{proof}
    We begin with the following useful facts about edit distance. Throughout, let 
    $\mc{S}(y)$ denote the set of substrings of a string $y \in \mc{V}^*$, including the empty string.
    \begin{observation}\label{obs:edit-dist-basic}
        Let $y,\wt{y} \in \mc{V}^*$. Then $d_\txt{edit}(y,\wt{y})$ is the length of the smallest 
        sequence of insertion and/or deletion operations to obtain $\wt{y}$ from $y$. 
    \end{observation}
    \begin{proof}[Proof of Observation~\ref{obs:edit-dist-basic}]
        We proceed via induction on the sum $\len{y} + \len{\wt{y}}$. The base case 
        where $y$ and $\wt{y}$ are both empty is trivial.
        Now suppose the claim holds all strings whose lengths sum to at most 
        $\len{y} + \len{\wt{y}} - 1$.
        Recalling the definition of $d_\txt{edit}$ (Definition~\ref{defn:edit-dist}),
        there are three cases.

        First, suppose $d_\txt{edit}(y,\wt{y}) = d_\txt{edit}(y_{2:},\wt{y}_{2:})$. Then by induction 
        there exists a sequence of $d_\txt{edit}(y,\wt{y})$ insertion and/or deletion operations to obtain 
        $\wt{y}_{2:}$ from $y_{2:}$. Because $y_{1} = \wt{y}_{1}$, the same sequence suffices to obtain $\wt{y}$
        from $y$ and thus the claim follows.

        Second, suppose $d_\txt{edit}(y,\wt{y}) = 1 + d_\txt{edit}(y_{2:},\wt{y})$. Again by induction,
        there exists a sequence of $d_\txt{edit}(y,\wt{y}) - 1$ insertion and/or deletion operations to obtain 
        $\wt{y}$ from $y_{2:}$.
        It follows immediately (i.e., by first deleting $y_1$) there exists a sequence of $d_\txt{edit}(y,\wt{y})$ such operations 
        to obtain $\wt{y}$ from $y$, and so the claim holds.

        The third case follows by symmetry with the second case.
    \end{proof}
    \begin{observation}\label{obs:edit-dist-0}
        Let $y,\wt{y} \in \mc{V}^*$. Then for any $\tau < \len{y}$, we have
        \begin{align*}
            d_\txt{edit}(y,\wt{y}) \geq 
            \min_{y' \in \mc{S}(\wt{y})} d_\txt{edit}(y_{:\tau},y') + \min_{y' \in \mc{S}(\wt{y})} d_\txt{edit}(y_{\tau+1:},y').
        \end{align*}
    \end{observation}
    \begin{proof}[Proof of Observation~\ref{obs:edit-dist-0}]
        Observation~\ref{obs:edit-dist-basic} implies there exists a sequence of $d_\txt{edit}(y,\wt{y})$ insertion 
        and/or deletion operations to obtain $\wt{y}$ from $y$.
        We may partition this sequence of operations into sequences based respectively on whether they occur on $y_{:\tau}$ or $y_{\tau+1:}$.
        Let $\wt{y}_\txt{pre}$ be the result of performing the first sequence of operations on $y_{:\tau}$ and let 
        $\wt{y}_\txt{suf}$ be the result of performing the second sequence of operations on $y_{\tau+1:}$.
        Then $\wt{y}$ is the concatenation of $\wt{y}_\txt{pre}$ and $\wt{y}_\txt{suf}$, and so the claim follows from the fact that
        \begin{align*}
            d_\txt{edit}(y,\wt{y}) & = d_\txt{edit}(y_{:\tau},\wt{y}_\txt{pre}) + d_\txt{edit}(y_{\tau+1:},\wt{y}_\txt{suf}) \\
            & \geq \min_{y' \in \mc{S}(\wt{y})} d_\txt{edit}(y_{:\tau},y') + \min_{y' \in \mc{S}(\wt{y})} d_\txt{edit}(y_{\tau+1:},y').
        \end{align*}
    \end{proof}
    \begin{observation}\label{obs:edit-dist-1}
        Let $y,\wt{y} \in \mc{V}^*$ and $\xi \in \Xi^*$. Then 
        $d_\gamma(y,\xi) \leq \gamma d_\txt{edit}(y,\wt{y}) + d_\gamma(\wt{y},\xi)$.
    \end{observation}
    \begin{proof}[Proof of Observation~\ref{obs:edit-dist-1}]
        The case $d_\txt{edit}(y,\wt{y}) = 0$ is trivial as we then have $y = \wt{y}$.
        Now suppose $d_\txt{edit}(y,\wt{y}) = 1$, and let $i$ be the first index such that 
        $y_i \neq \wt{y}_i$.
        Then, unrolling the recursive definition of $d_\gamma(\wt{y}_{i:},\xi_{j:})$, there must exist 
        $c \in \R$ and an index $j$ such that both
        $d_\gamma(\wt{y},\xi) = c + d_\gamma(\wt{y}_{i:},\xi_{j:})$ and 
        $d_\gamma(y,\xi) \leq c + d_\gamma(y_{i:},\xi_{j:})$.
        Moreover, from the definition of edit distance, either ${y}_{i+1:} = \wt{y}_{i:}$
        or vice versa.

        We claim $d_\gamma(y_{i:},\xi_{j:}) \leq d_\gamma(\wt{y}_{i:},\xi_{j:}) + \gamma$.
        If $y_{i+1:} = \wt{y}_{i:}$, then the claim obtains as 
        \begin{align*}
            d_\gamma(y_{i:},\xi_{j:}) 
            & \leq d_\gamma(y_{i+1:},\xi_{j:}) + \min_{\xi' \in \Xi} d_0(y_i,\xi') + \gamma \\
            & \stackrel{(\star)}{\leq} d_\gamma(y_{i+1:},\xi_{j:}) + \gamma \\
            & = d_\gamma(\wt{y}_{i:},\xi_{j:}) + \gamma,
        \end{align*}
        with $(\star)$ following from the fact that $d_0(y_i,\xi') = 0$ for $\xi' = (1/2,\pi)$ irrespective of 
        $y_i$ and $\pi$.

        Otherwise, if $y_{i:} = \wt{y}_{i+1:}$, then from unrolling the recursive definition of $d_\gamma(\wt{y}_{i:},\xi_{j:})$
        there must exist some index $j' \geq j$ such that either
        \begin{align*}
            d_\gamma(\wt{y}_{i:},\xi_{j:}) = d_\gamma(\wt{y}_{i+1:},\xi_{j':}) + \gamma + \min_{\xi' \in \Xi} d_0(\wt{y}_i,\xi') + \sum_{j \leq \ell < j'} \gamma + \min_{y' \in \mc{V}} d_0(y',\xi_\ell)
        \end{align*}
        or 
        \begin{align*}
            d_\gamma(\wt{y}_{i:},\xi_{j:}) = d_\gamma(\wt{y}_{i+1:},\xi_{j'+1:}) + d_0(\wt{y}_i,\xi_{j'}) + \sum_{j \leq \ell < j'} \gamma + \min_{y' \in \mc{V}} d_0(y',\xi_\ell).
        \end{align*}
        In the first case, we have $\gamma + \min_{\xi' \in \Xi} d_0(\wt{y}_i,\xi') > 0$ since $\gamma > 1/2$ by assumption, and so the claim follows as 
        \begin{align*}
            d_\gamma(y_{i:},\xi_{j:}) & \leq d_\gamma(y_{i:},\xi_{j':}) + \sum_{j \leq \ell < j'} \gamma + \min_{y' \in \mc{V}} d_0(y',\xi_\ell) \\
            & = d_\gamma(\wt{y}_{i+1},\xi_{j':}) + \sum_{j \leq \ell < j'} \gamma + \min_{y' \in \mc{V}} d_0(y',\xi_\ell) \\
            & < d_\gamma(\wt{y}_{i:},\xi_{j:}).
        \end{align*}
        In the second case, we have $d_0(\wt{y}_j)$ the claim follows as 
        \begin{align*}
            d_\gamma(y_{i:},\xi_{j:}) & \leq d_\gamma(y_{i:},\xi_{j'+1:}) + \sum_{j \leq \ell < j'+1} \gamma + \min_{y' \in \mc{V}} d_0(y',\xi_\ell) \\
            & = d_\gamma(\wt{y}_{i+1},\xi_{j'+1:}) + \sum_{j \leq \ell < j'+1} \gamma + \min_{y' \in \mc{V}} d_0(y',\xi_\ell) \\
            & \leq d_\gamma(\wt{y}_{i:},\xi_{j:}) + \gamma.
        \end{align*}
        
        Thus, assuming $d_\txt{edit}(y,\wt{y}) \leq 1$, we have shown $d_\gamma(y_{i:},\xi_{j:}) \leq d_\gamma(\wt{y}_{i:},\xi_{j:}) + \gamma$,
        from which it follows that $d_\gamma(y,\xi) \leq d_\gamma(\wt{y},\xi) + \gamma$.
        The general result follows immediately by applying Observation~\ref{obs:edit-dist-basic} and 
        summing the bound for a single edit over the (smallest) sequence of 
        edits to obtain $\wt{y}$ from $y$. 
    \end{proof}
Proceeding with the main proof, define for convenience the quantity 
\begin{align*}
    \est{\alpha}_\tau \defeq \frac{1}{k}\sum_{i=1}^k p(Y_{\tau+i} \mid Y_{:\tau+i-1}).
\end{align*}
Observe 
\begin{align}
    \alpha(Y) = \frac{k}{m}\sum_{\tau = 0}^{m/k-1} \est{\alpha}_{k \tau}, \label{eqn:pigeon-1}
\end{align}
while Observation~\ref{obs:edit-dist-0} together with our assumption that $d_\txt{edit}(Y,\wt{Y}) \leq \eps m$ implies 
\begin{align}
    \frac{k}{m}\sum_{\tau = 0}^{m/k-1} \min_{Y' \in \mc{S}(\wt{Y})} d_\txt{edit}(Y_{k\tau+1:k\tau+k},Y') \leq k \eps. \label{eqn:pigeon-2}
\end{align}
The displays~\eqref{eqn:pigeon-1} and \eqref{eqn:pigeon-2} together imply there exists an index 
$\tau$ and $Y' \in \mc{S}(\wt{Y})$ such that $\est{\alpha}_\tau - \frac{1}{k}\min_{Y' \in \mc{S}(\wt{Y})} d_\txt{edit}(Y_{\tau+1:\tau+k},Y') \geq \alpha(Y) - \eps$.
% there must exist a length $k$ substring $Y_{\tau+1:\tau+k}$ of $Y$ and a substring $Y'$ of $\wt{Y}$
% such that $d_\txt{edit}(Y,Y') \leq \eps k$.
% \begin{align*}
%     \Ep[d(Y_{\tau+1:\tau+k},\xi_{\tau+1:\tau+k})] = - C_0 \frac{1}{k}\sum_{i=1}^k p(Y_{\tau+i} \mid Y_{:\tau+i-1}).
% \end{align*}
% Defining for convenience the quantity
% \begin{align*}
%     \est{\alpha} \defeq \frac{1}{k}\sum_{i=1}^k p(Y_{\tau+i} \mid Y_{:\tau+i-1}).
% \end{align*}
% Recalling by assumption
Reusing the same concentration argument as in the proof of Theorem~\ref{lemma:perm-test}, for $t \geq 0$ we have
\begin{align*}
    \P\left(d_0(Y_{\tau+1:\tau+k},\xi_{\tau+1:\tau+k}) \geq -k \left(C_0\est{\alpha}_\tau + t\right)\mid Y\right)
    % & \leq \P\left(d_0(Y,\xi_{1:m}) \geq -m C_0\alpha(Y)/2 \mid Y\right) \\
    & \leq \exp\left(-2 k t^2\right),
\end{align*}
and thus from Observation~\ref{obs:edit-dist-1} it follows that
\begin{align*}
    \P\left(d_\gamma(Y',\xi_{\tau+1:\tau+k}) \geq -k \left(C_0\alpha(Y) - \gamma \eps + t \right)\mid \wt{Y},Y\right) \leq \exp\left(-2 k t^2\right).
\end{align*}
Letting $t = (C_0 \alpha - \gamma \eps)/2$ and recalling the definition of the test statistic, we have
\begin{align}
    \P\left(\phi(\wt{Y},\xi) \geq -k \left(C_0\alpha(Y) - \gamma \eps\right)/2 \mid \wt{Y},Y\right) \leq \exp\left(-k (C_0\alpha(Y) - \gamma \eps)_{+}^2/2\right). \label{eqn:indel-main-1}
\end{align}

All that remains to bound the probability of $\phi(\wt{Y},\xi')$ exceeding the 
threshold from the above display.
To this end, define the set-valued map $\mc{N}_\beta(y) \defeq \{y' \ : \ d_\txt{edit}(y,y') \leq \beta/(4\gamma - 1)\}$.
Then we make the following observation.
\begin{observation}\label{obs:edit-dist-2}
    For any $y \in \mc{V}^*$ and $\xi \in \Xi^*$, there exists $y' \in \mc{N}_{\len{\xi}}(y)$ 
    such that 
    \begin{align*}
        d_\gamma(y,\xi) = \gamma \cdot d_\txt{edit}(y,y') + d_0(y',\xi).
    \end{align*}
\end{observation}
\begin{proof}
    We proceed via induction. The base case where $y$ and $\xi$ both have 
    length $1$ follows trivially by taking $y' = y$; in particular,
    $\gamma > 1/2$ implies $d(y,\xi) \leq \gamma + \min_{y'} d(y',\xi)$ and
    likewise $d(y,\xi) \leq \gamma + \min_{\xi'} d(y,\xi')$.
    Now suppose the result holds so long as $\len{y} + \len{\xi} \leq n-1$.
    We claim that the result must then also hold if the lengths sum to at most $n$.

    We prove this inductive claim by considering three exhaustive cases.
    First, suppose that $d_\gamma(y,\xi) = d_\gamma(y_{2:},\xi_{2:}) + d(y_1,\xi_1)$.
    By our induction hypothesis, there exists
    $\hat{y} \in \mc{N}_{\len{\xi}-1}(y_{2:})$ such that 
    $d_\gamma(y_{2:},\xi_{2:}) = \gamma \cdot d_\txt{edit}(y_{2:},\hat{y}) + d(\hat{y},\xi_{2:})$.
    The desired result then obtains with $y'$ as the concatenation of
    $y_1$ and $\hat{y}$.
    Second, suppose $d_\gamma(y,\xi) = d_\gamma(y,\xi_{2:}) + \min_{\xi' \in \Xi} d(y_1,\xi') + \gamma$.
    By our induction hypothesis, there exists $\hat{y} \in \mc{N}_{\len{\xi}=1}(y)$
    such that $d_\gamma(y_{2:},\xi) = \gamma \cdot d_\txt{edit}(y_{2:},\hat{y}) + d(\hat{y},\xi_{2:})$.
    The result obtains with $y' = \hat{y}$.
    Finally, suppose $d_\gamma(y,\xi) = d_\gamma(y_{2:},\xi) + d(y'',\xi_1) + \gamma$ for some $y'' \in \mc{V}$.
    By our induction hypothesis, there exists $\hat{y} \in \mc{N}_{\len{\xi}-1}(y)$
    such that $d_\gamma(y_{2:},\xi) = \gamma \cdot d_\txt{edit}(y_{2:},\hat{y}) + d(\hat{y},\xi)$.
    The result then obtains by concatenating $y''$ with $\est{y}$.
\end{proof}
Let $\mc{I}_j \defeq \{(j+i)\%n\}_{i=1}^k$.
For any $0 \leq i \leq \len{\wt{Y}} - k$ and $j \in [n]$,
Observations~\ref{obs:edit-dist-1} and~\ref{obs:edit-dist-2} together imply that 
\begin{align}
    d_\gamma(\wt{Y}_{i+1:i+k},\xi_{\mc{I}_j}') 
    & = \min_{y \in \mc{N}_k(\wt{Y}_{i+1:i+k})} \gamma \cdot d_\txt{edit}(\wt{Y}_{i+1:i+k},y) + d_0(y,\xi_{\mc{I}_j}') \label{eqn:edit-min}\\
    & \stackrel{(\star)}{=} \min_{y \in \mc{N}_{k/4(\gamma-1)}(\wt{Y}_{i+1:i+k})} \gamma \cdot d_\txt{edit}(y,\wt{Y}_{i+1:i+k}) + d_0(y,\xi_{\mc{I}_j}'), \label{eqn:edit-reduce}
\end{align}
where ($\star$) follows from the fact that $d_\txt{edit}(\wt{Y}_{i+1:i+k},y) > k/4(\gamma-1)$ implies 
\begin{align*}
    \gamma \cdot d_\txt{edit}(\wt{Y}_{i+1:i+k},y) + d_0(y,\xi_{\mc{I}_j}') \geq k/4 > d_0(\wt{Y}_{i+1:i+k},\xi_{\mc{I}_j}'),
\end{align*}
and therefore the minimizer in equation~\eqref{eqn:edit-min} must be an element of $\mc{N}_{k/4(\gamma-1)}(\wt{Y}_{i+1:i+k})$.

By construction, $\mc{N}_\beta(y)$ consists of the set of strings obtainable from $y$ by a sequence of 
at most $\beta$ insertion and/or deletion operations.
Now define another set-valued map $\mc{N}_{\beta,-}(y)$ as the restriction of $\mc{N}_\beta(y)$
such that we may only insert a particular token into $y$ (which token is immaterial).
As the specific identity of each token we insert into $y$ can only influence the value of $d_\gamma$ by $\pm 1/2$,
for any $\beta$ it follows that 
\begin{align*}
    \min_{y \in \mc{N}_{\beta}(\wt{Y}_{i+1:i+k})} \gamma \cdot d_\txt{edit}(y,\wt{Y}_{i+1:i+k}) + d_0(y,\xi_{\mc{I}_j}')
    \geq \min_{y \in \mc{N}_{\beta,-}(\wt{Y})} d_0(y,\xi_{\mc{I}_j}'),
\end{align*}
and so, letting $\beta = k/4(\gamma-1)$, from equation~\eqref{eqn:edit-reduce} we have 
\begin{align*}
    d_\gamma(\wt{Y}_{i+1:i+k},\xi_{\mc{I}_j}') \geq \min_{y \in \mc{N}_{\beta,-}(\wt{Y}_{i+1:i+k})} d_0(y,\xi_{\mc{I}_j}')
\end{align*}
Let $\wt{Y}(i,\ell)$ denote the $\ell$-th element of $\mc{N}_{\beta,-}(\wt{Y}_{i+1:i+k})$ for some $\wt{Y}$-measurable indexing.
From the independence of $\wt{Y}$ and $\xi'$, we have $\Ep[d_0(\wt{Y}(i,\ell),\xi_{\mc{I}_j}) \mid \wt{Y}] = 0$
for any $\ell$ and $j$.
The cardinality of $\mc{N}_{\beta,-}(\wt{Y}_{i+1:i+k})$ is equal to the number of possible combinations of locations for $\beta$ 
insertion and/or deletion operations on $\wt{Y}$, of which there are at most $(k+\beta)^\beta \leq (2k)^\beta$.
Thus, applying the same concentration argument as in the proof of Theorem~\ref{lemma:perm-test}
and taking a union bound over all $i \leq m - k$, $j \leq n$ and $\ell \leq (2k)^\beta$, we have
\begin{align}
    \P(\phi(\wt{Y},\xi') \leq -\alpha(Y)/2 + \gamma \eps \mid \wt{Y},Y) \leq m n(2k)^{k/(4\gamma-1)}\exp(-k C_0^2 (\alpha(Y) - \gamma\eps)_+^2/2). \label{eqn:indel-main-2}
\end{align}
Combining the displays~\eqref{eqn:indel-main-1} and~\eqref{eqn:indel-main-2} via another union bound 
gives the desired result.
\end{proof}
