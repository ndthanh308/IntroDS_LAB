@article{reynolds2002improved,
  title={An improved in situ and satellite SST analysis for climate},
  author={Reynolds, Richard W and Rayner, Nick A and Smith, Thomas M and Stokes, Diane C and Wang, Wanqiu},
  journal={Journal of climate},
  volume={15},
  number={13},
  pages={1609--1625},
  year={2002},
  publisher={American Meteorological Society}
}
@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}
@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}
@article{williams2022data,
  title={Data-driven sensor placement with shallow decoder networks},
  author={Williams, Jan and Zahn, Olivia and Kutz, J Nathan},
  journal={arXiv preprint arXiv:2202.05330},
  year={2022}
}

@article{manohar2018data,
  title={Data-driven sparse sensor placement for reconstruction: Demonstrating the benefits of exploiting known patterns},
  author={Manohar, Krithika and Brunton, Bingni W and Kutz, J Nathan and Brunton, Steven L},
  journal={IEEE Control Systems Magazine},
  volume={38},
  number={3},
  pages={63--86},
  year={2018},
  publisher={IEEE}
}

@article{gardner2006james,
  title={The james webb space telescope},
  author={Gardner, Jonathan P and Mather, John C and Clampin, Mark and Doyon, Rene and Greenhouse, Matthew A and Hammel, Heidi B and Hutchings, John B and Jakobsen, Peter and Lilly, Simon J and Long, Knox S and others},
  journal={Space Science Reviews},
  volume={123},
  pages={485--606},
  year={2006},
  publisher={Springer}
}

@article{stevenson2011advances,
  title={How advances in neural recording affect data analysis},
  author={Stevenson, Ian H and Kording, Konrad P},
  journal={Nature neuroscience},
  volume={14},
  number={2},
  pages={139--142},
  year={2011},
  publisher={Nature Publishing Group US New York}
}

@article{medsker2001recurrent,
  title={Recurrent neural networks},
  author={Medsker, Larry R and Jain, LC},
  journal={Design and Applications},
  volume={5},
  number={64-67},
  pages={2},
  year={2001}
}

@article{salehinejad2017recent,
  title={Recent advances in recurrent neural networks},
  author={Salehinejad, Hojjat and Sankar, Sharan and Barfett, Joseph and Colak, Errol and Valaee, Shahrokh},
  journal={arXiv preprint arXiv:1801.01078},
  year={2017}
}

@article{zaremba2014recurrent,
  title={Recurrent neural network regularization},
  author={Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1409.2329},
  year={2014}
}

@article{erichson2020shallow,
  title={Shallow neural networks for fluid flow reconstruction with limited sensors},
  author={Erichson, N Benjamin and Mathelin, Lionel and Yao, Zhewei and Brunton, Steven L and Mahoney, Michael W and Kutz, J Nathan},
  journal={Proceedings of the Royal Society A},
  volume={476},
  number={2238},
  pages={20200097},
  year={2020},
  publisher={The Royal Society Publishing}
}

@article{williams2023sensing,
  title={Sensing with shallow recurrent decoder networks},
  author={Williams, Jan P and Zahn, Olivia and Kutz, J Nathan},
  journal={arXiv preprint arXiv:2301.12011},
  year={2023}
}

@article{rosenberg2020predicting,
  title={Predicting walking response to ankle exoskeletons using data-driven models},
  author={Rosenberg, Michael C and Banjanin, Bora S and Burden, Samuel A and Steele, Katherine M},
  journal={Journal of the Royal Society Interface},
  volume={17},
  number={171},
  pages={20200487},
  year={2020},
  publisher={The Royal Society}
}

@article{hicks2015my,
  title={Is my model good enough? Best practices for verification and validation of musculoskeletal models and simulations of movement},
  author={Hicks, Jennifer L and Uchida, Thomas K and Seth, Ajay and Rajagopal, Apoorva and Delp, Scott L},
  journal={Journal of biomechanical engineering},
  volume={137},
  number={2},
  pages={020905},
  year={2015},
  publisher={American Society of Mechanical Engineers}
}

@article{delp2007opensim,
  title={OpenSim: open-source software to create and analyze dynamic simulations of movement},
  author={Delp, Scott L and Anderson, Frank C and Arnold, Allison S and Loan, Peter and Habib, Ayman and John, Chand T and Guendelman, Eran and Thelen, Darryl G},
  journal={IEEE transactions on biomedical engineering},
  volume={54},
  number={11},
  pages={1940--1950},
  year={2007},
  publisher={IEEE}
}

@book{brunton2019data,
  title={Data-driven science and engineering: Machine learning, dynamical systems, and control},
  author={Brunton, Steven L and Kutz, J Nathan},
  year={2019},
  publisher={Cambridge University Press}
}

@book{kutz2013data,
  title={Data-driven modeling \& scientific computation: methods for complex systems \& big data},
  author={Kutz, J Nathan},
  year={2013},
  publisher={OUP Oxford}
}

@article{everson_karhunenloeve_1995,
	title = {Karhunen–{Loève} procedure for gappy data},
	volume = {12},
	copyright = {© 1995 Optical Society of America},
	issn = {1520-8532},
	url = {https://opg.optica.org/josaa/abstract.cfm?uri=josaa-12-8-1657},
	doi = {10.1364/JOSAA.12.001657},
	abstract = {The problem of using the Karhunen–Loève transform with partial data is addressed. Given a set of empirical eigenfunctions, we show how to recover the modal coefficients for each gappy snapshot by a least-squares procedure. This method gives an unbiased estimate of the data that lie in the gaps and permits gaps to be filled in a reasonable manner. In addition, a scheme is advanced for finding empirical eigenfunctions from gappy data. It is shown numerically that this procedure obtains spectra and eigenfunctions that are close to those obtained from unmarred data.},
	language = {EN},
	number = {8},
	urldate = {2022-11-16},
	journal = {JOSA A},
	author = {Everson, R. and Sirovich, L.},
	month = aug,
	year = {1995},
	note = {Publisher: Optica Publishing Group},
	keywords = {Face recognition, Image analysis, Image compression, Projection systems, Turbulence},
	pages = {1657--1664},
}

@article{drmac_new_2016,
	title = {A {New} {Selection} {Operator} for the {Discrete} {Empirical} {Interpolation} {Method}---{Improved} {A} {Priori} {Error} {Bound} and {Extensions}},
	volume = {38},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/15M1019271},
	doi = {10.1137/15M1019271},
	abstract = {This paper introduces a new framework for constructing the discrete empirical interpolation method ({\textbackslash}sf DEIM) projection operator. The interpolation node selection procedure is formulated using the QR factorization with column pivoting, and it enjoys a sharper error bound for the {\textbackslash}sf DEIM projection error. Furthermore, for a subspace 
U
 given as the range of an orthonormal ��
U
, the {\textbackslash}sf DEIM projection does not change if ��
U
 is replaced by ��Ω
U
Ω
 with arbitrary unitary matrix Ω
Ω
. In a large-scale setting, the new approach allows modifications that use only randomly sampled rows of ��
U
, but with the potential of producing good approximations with corresponding probabilistic error bounds. Another salient feature of the new framework is that robust and efficient software implementation is easily developed, based on readily available high performance linear algebra packages.},
	number = {2},
	urldate = {2022-11-16},
	journal = {SIAM Journal on Scientific Computing},
	author = {Drmač, Zlatko and Gugercin, Serkan},
	month = jan,
	year = {2016},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {nonlinear model reduction, proper orthogonal decomposition, 15A12, 15A23, 65F35, 65M20, 65M22, 93B40, 93C15, empirical interpolation, projections, QR factorization, randomized sampling, rank revealing factorization},
	pages = {A631--A648},
	file = {Submitted Version:/Users/janwilliams/Zotero/storage/7UF7LXGH/Drmač and Gugercin - 2016 - A New Selection Operator for the Discrete Empirica.pdf:application/pdf},
}

@article{barrault_empirical_2004,
	title = {An ‘empirical interpolation’ method: application to efficient reduced-basis discretization of partial differential equations},
	volume = {339},
	issn = {1631-073X},
	shorttitle = {An ‘empirical interpolation’ method},
	url = {https://www.sciencedirect.com/science/article/pii/S1631073X04004248},
	doi = {10.1016/j.crma.2004.08.006},
	abstract = {We present an efficient reduced-basis discretization procedure for partial differential equations with nonaffine parameter dependence. The method replaces nonaffine coefficient functions with a collateral reduced-basis expansion which then permits an (effectively affine) offline–online computational decomposition. The essential components of the approach are (i) a good collateral reduced-basis approximation space, (ii) a stable and inexpensive interpolation procedure, and (iii) an effective a posteriori estimator to quantify the newly introduced errors. Theoretical and numerical results respectively anticipate and confirm the good behavior of the technique. To cite this article: M. Barrault et al., C. R. Acad. Sci. Paris, Ser. I 339 (2004).
Résumé
Nous présentons dans cette Note une méthode rapide de base réduite pour la résolution d'équations aux dérivées partielles ayant une dépendance non affine en ses paramètres. L'approche propose de remplacer le calcul des fonctionelles non affines par un développement en base réduite annexe qui conduit à une évaluation en ligne effectivement affine. Les points essentiels de cette approche sont (i) un bon système de base réduite annexe, (ii) une méthode stable et peu coûteuse d'interpolation dans cette base, et (iii) un estimateur a posteriori pertinent pour quantifier les nouvelles erreurs introduites. Des résultats théoriques et numériques viennent anticiper puis confirmer le bon comportement de cette technique. Pour citer cet article : M. Barrault et al., C. R. Acad. Sci. Paris, Ser. I 339 (2004).},
	language = {en},
	number = {9},
	urldate = {2022-11-16},
	journal = {Comptes Rendus Mathematique},
	author = {Barrault, Maxime and Maday, Yvon and Nguyen, Ngoc Cuong and Patera, Anthony T.},
	month = nov,
	year = {2004},
	pages = {667--672},
	file = {ScienceDirect Full Text PDF:/Users/janwilliams/Zotero/storage/AL6XU23K/Barrault et al. - 2004 - An ‘empirical interpolation’ method application t.pdf:application/pdf;ScienceDirect Snapshot:/Users/janwilliams/Zotero/storage/3ZC3FPWN/S1631073X04004248.html:text/html},
}

@article{chaturantabut_nonlinear_2010,
	title = {Nonlinear {Model} {Reduction} via {Discrete} {Empirical} {Interpolation}},
	volume = {32},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/090766498},
	doi = {10.1137/090766498},
	abstract = {A dimension reduction method called discrete empirical interpolation is proposed and shown to dramatically reduce the computational complexity of the popular proper orthogonal decomposition (POD) method for constructing reduced-order models for time dependent and/or parametrized nonlinear partial differential equations (PDEs). In the presence of a general nonlinearity, the standard POD-Galerkin technique reduces dimension in the sense that far fewer variables are present, but the complexity of evaluating the nonlinear term remains that of the original problem. The original empirical interpolation method (EIM) is a modification of POD that reduces the complexity of evaluating the nonlinear term of the reduced model to a cost proportional to the number of reduced variables obtained by POD. We propose a discrete empirical interpolation method (DEIM), a variant that is suitable for reducing the dimension of systems of ordinary differential equations (ODEs) of a certain type. As presented here, it is applicable to ODEs arising from finite difference discretization of time dependent PDEs and/or parametrically dependent steady state problems. However, the approach extends to arbitrary systems of nonlinear ODEs with minor modification. Our contribution is a greatly simplified description of the EIM in a finite-dimensional setting that possesses an error bound on the quality of approximation. An application of DEIM to a finite difference discretization of the one-dimensional FitzHugh–Nagumo equations is shown to reduce the dimension from 1024 to order 5 variables with negligible error over a long-time integration that fully captures nonlinear limit cycle behavior. We also demonstrate applicability in higher spatial dimensions with similar state space dimension reduction and accuracy results.},
	number = {5},
	urldate = {2022-11-16},
	journal = {SIAM Journal on Scientific Computing},
	author = {Chaturantabut, Saifon and Sorensen, Danny C.},
	month = jan,
	year = {2010},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65L02, 65M02, empirical interpolation methods, nonlinear model reduction, nonlinear partial differential equations, proper orthogonal decomposition},
	pages = {2737--2764},
	file = {Submitted Version:/Users/janwilliams/Zotero/storage/MWNKG6DH/Chaturantabut and Sorensen - 2010 - Nonlinear Model Reduction via Discrete Empirical I.pdf:application/pdf},
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{joshi_sensor_2009,
	title = {Sensor {Selection} via {Convex} {Optimization}},
	volume = {57},
	issn = {1941-0476},
	doi = {10.1109/TSP.2008.2007095},
	abstract = {We consider the problem of choosing a set of k sensor measurements, from a set of m possible or potential sensor measurements, that minimizes the error in estimating some parameters. Solving this problem by evaluating the performance for each of the (m k) possible choices of sensor measurements is not practical unless m and k are small. In this paper, we describe a heuristic, based on convex optimization, for approximately solving this problem. Our heuristic gives a subset selection as well as a bound on the best performance that can be achieved by any selection of k sensor measurements. There is no guarantee that the gap between the performance of the chosen subset and the performance bound is always small; but numerical experiments suggest that the gap is small in many cases. Our heuristic method requires on the order of m 3 operations; for m= 1000 possible sensors, we can carry out sensor selection in a few seconds on a 2-GHz personal computer.},
	number = {2},
	journal = {IEEE Transactions on Signal Processing},
	author = {Joshi, Siddharth and Boyd, Stephen},
	month = feb,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Signal Processing},
	keywords = {Sensor phenomena and characterization, Sensor systems, Additive noise, Chemical sensors, Convex optimization, Covariance matrix, Estimation error, experiment design, Microcomputers, Optimization methods, Parameter estimation, sensor selection, Wireless sensor networks},
	pages = {451--462},
	file = {IEEE Xplore Abstract Record:/Users/janwilliams/Zotero/storage/ZFUAXD3A/4663892.html:text/html},
}

@article{caselton_optimal_1984,
	title = {Optimal monitoring network designs},
	volume = {2},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/0167715284900208},
	doi = {10.1016/0167-7152(84)90020-8},
	abstract = {The selection of a monitoring network is formulated as a decision problem whose solutions would then be optimal. The theory is applied where the underlying field has a multivariate normal probability structure.},
	language = {en},
	number = {4},
	urldate = {2022-11-16},
	journal = {Statistics \& Probability Letters},
	author = {Caselton, W. F. and Zidek, J. V.},
	month = aug,
	year = {1984},
	keywords = {decision analysis, information theory, monitoring networks, network design, proper local utility},
	pages = {223--227},
	file = {ScienceDirect Snapshot:/Users/janwilliams/Zotero/storage/3AFLGZYM/0167715284900208.html:text/html},
}

@article{krause_near-optimal_2008,
	title = {Near-{Optimal} {Sensor} {Placements} in {Gaussian} {Processes}: {Theory}, {Efficient} {Algorithms} and {Empirical} {Studies}},
	volume = {9},
	issn = {1532-4435},
	shorttitle = {Near-{Optimal} {Sensor} {Placements} in {Gaussian} {Processes}},
	abstract = {When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.},
	journal = {The Journal of Machine Learning Research},
	author = {Krause, Andreas and Singh, Ajit and Guestrin, Carlos},
	month = jun,
	year = {2008},
	pages = {235--284},
	file = {Full Text PDF:/Users/janwilliams/Zotero/storage/SE3547A6/Krause et al. - 2008 - Near-Optimal Sensor Placements in Gaussian Process.pdf:application/pdf},
}

@article{lindley_measure_1956,
	title = {On a {Measure} of the {Information} {Provided} by an {Experiment}},
	volume = {27},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2237191},
	abstract = {A measure is introduced of the information provided by an experiment. The measure is derived from the work of Shannon [10] and involves the knowledge prior to performing the experiment, expressed through a prior probability distribution over the parameter space. The measure is used to compare some pairs of experiments without reference to prior distributions; this method of comparison is contrasted with the methods discussed by Blackwell. Finally, the measure is applied to provide a solution to some problems of experimental design, where the object of experimentation is not to reach decisions but rather to gain knowledge about the world.},
	number = {4},
	urldate = {2022-11-16},
	journal = {The Annals of Mathematical Statistics},
	author = {Lindley, D. V.},
	year = {1956},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {986--1005},
}
@article{sebastiani_maximum_2000,
	title = {Maximum {Entropy} {Sampling} and {Optimal} {Bayesian} {Experimental} {Design}},
	volume = {62},
	issn = {1369-7412},
	url = {https://www.jstor.org/stable/2680683},
	abstract = {When Shannon entropy is used as a criterion in the optimal design of experiments, advantage can be taken of the classical identity representing the joint entropy of parameters and observations as the sum of the marginal entropy of the observations and the preposterior conditional entropy of the parameters. Following previous work in which this idea was used in spatial sampling, the method is applied to standard parameterized Bayesian optimal experimental design. Under suitable conditions, which include non-linear as well as linear regression models, it is shown in a few steps that maximizing the marginal entropy of the sample is equivalent to minimizing the pre-posterior entropy, the usual Bayesian criterion, thus avoiding the use of conditional distributions. It is shown using this marginal formulation that under normality assumptions every standard model which has a two-point prior distribution on the parameters gives an optimal design supported on a single point. Other results include a new asymptotic formula which applies as the error variance is large and bounds on support size.},
	number = {1},
	urldate = {2022-11-16},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Sebastiani, Paola and Wynn, Henry P.},
	year = {2000},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {145--157},
}

@article{paninski_asymptotic_2005,
	title = {Asymptotic theory of information-theoretic experimental design},
	volume = {17},
	issn = {0899-7667},
	doi = {10.1162/0899766053723032},
	abstract = {We discuss an idea for collecting data in a relatively efficient manner. Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials). We prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design. In particular, we calculate the asymptotic efficiency of the information-maximization strategy and demonstrate that this method is in a well-defined sense never less efficient--and is generically more efficient--than the nonadaptive strategy. For example, we are able to explicitly calculate the asymptotic relative efficiency of the staircase method widely employed in psychophysics research and to demonstrate the dependence of this efficiency on the form of the psychometric function underlying the output responses.},
	language = {eng},
	number = {7},
	journal = {Neural Computation},
	author = {Paninski, Liam},
	month = jul,
	year = {2005},
	pmid = {15901405},
	pages = {1480--1507},
}

@book{law_data-2015,
	edition = {1st edition},
	title = {Data Assimilation: A Mathematical Introduction},
	isbn = {978-3-319-20324-9},
	shorttitle = {Data Assimilation},

	publisher = {Springer Cham},
	author = {Law, Kody and Stuart, Andrew and Zygalakis, Konstantinos},

	year = {2015},
}

@article{li_public_2008,
	title = {A public turbulence database cluster and applications to study {Lagrangian} evolution of velocity increments in turbulence},
	volume = {9},
	url = {https://doi.org/10.1080/14685240802376389},
	doi = {10.1080/14685240802376389},
	abstract = {A public database system archiving a direct numerical simulation (DNS) data set of isotropic, forced turbulence is described in this paper. The data set consists of the DNS output on 10243 spatial points and 1024 time samples spanning about one large-scale turnover time. This complete 10244 spacetime history of turbulence is accessible to users remotely through an interface that is based on the Web-services model. Users may write and execute analysis programs on their host computers, while the programs make subroutine-like calls that request desired parts of the data over the network. The users are thus able to perform numerical experiments by accessing the 27 terabytes (TB) of DNS data using regular platforms such as laptops. The architecture of the database is explained, as are some of the locally defined functions, such as differentiation and interpolation. Test calculations are performed to illustrate the usage of the system and to verify the accuracy of the methods. The database is then used to analyze a dynamical model for small-scale intermittency in turbulence. Specifically, the dynamical effects of pressure and viscous terms on the Lagrangian evolution of velocity increments are evaluated using conditional averages calculated from the DNS data in the database. It is shown that these effects differ considerably among themselves and thus require different modeling strategies in Lagrangian models of velocity increments and intermittency.},
	urldate = {2022-11-16},
	journal = {Journal of Turbulence},
	author = {Li, Yi and Perlman, Eric and Wan, Minping and Yang, Yunke and Meneveau, Charles and Burns, Randal and Chen, Shiyi and Szalay, Alexander and Eyink, Gregory},
	month = jan,
	year = {2008},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/14685240802376389},
	keywords = {Turbulence, database, intermittency, simulation},
	pages = {N31},
	file = {Submitted Version:/Users/janwilliams/Zotero/storage/MJ7XE8U3/Li et al. - 2008 - A public turbulence database cluster and applicati.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2022-12-01},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Full Text PDF:/Users/janwilliams/Zotero/storage/3C3PFA3U/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf;Snapshot:/Users/janwilliams/Zotero/storage/PPWWSEVW/Long-Short-Term-Memory.html:text/html},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:/Users/janwilliams/Zotero/storage/B2EP8HRU/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/janwilliams/Zotero/storage/3FBJSAMS/1412.html:text/html},
}

@article{peherstorfer2020stability,
  title={Stability of discrete empirical interpolation and gappy proper orthogonal decomposition with randomized and deterministic sampling points},
  author={Peherstorfer, Benjamin and Drmac, Zlatko and Gugercin, Serkan},
  journal={SIAM Journal on Scientific Computing},
  volume={42},
  number={5},
  pages={A2837--A2864},
  year={2020},
  publisher={SIAM}
}

@article{gunnarson2021learning,
  title={Learning efficient navigation in vortical flow fields},
  author={Gunnarson, Peter and Mandralis, Ioannis and Novati, Guido and Koumoutsakos, Petros and Dabiri, John O},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={7143},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{krishna2022finite,
  title={Finite-horizon, energy-efficient trajectories in unsteady flows},
  author={Krishna, Kartik and Song, Zhuoyuan and Brunton, Steven L},
  journal={Proceedings of the Royal Society A},
  volume={478},
  number={2258},
  pages={20210255},
  year={2022},
  publisher={The Royal Society}
}

@article{biferale2019zermelo,
  title={Zermelo’s problem: optimal point-to-point navigation in 2D turbulent flows using reinforcement learning},
  author={Biferale, Luca and Bonaccorso, Fabio and Buzzicotti, Michele and Clark Di Leoni, Patricio and Gustavsson, Kristian},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={29},
  number={10},
  year={2019},
  publisher={AIP Publishing}
}

@inproceedings{buzzicotti2020optimal,
  title={Optimal control of point-to-point navigation in turbulent time dependent flows using reinforcement learning},
  author={Buzzicotti, Michele and Biferale, Luca and Bonaccorso, Fabio and Clark di Leoni, Patricio and Gustavsson, Kristian},
  booktitle={International Conference of the Italian Association for Artificial Intelligence},
  pages={223--234},
  year={2020},
  organization={Springer}
}

@article{madridano2021trajectory,
  title={Trajectory planning for multi-robot systems: Methods and applications},
  author={Madridano, Angel and Al-Kaff, Abdulla and Mart{\'\i}n, David and De La Escalera, Arturo},
  journal={Expert Systems with Applications},
  volume={173},
  pages={114660},
  year={2021},
  publisher={Elsevier}
}

@article{mei2022mobile,
  title={Mobile Sensor Path Planning for Kalman Filter Spatiotemporal Estimation},
  author={Mei, Jiazhong and Brunton, Steven L and Kutz, J Nathan},
  journal={arXiv preprint arXiv:2212.08280},
  year={2022}
}

@inproceedings{shriwastav2022dynamic,
  title={Dynamic compressed sensing of unsteady flows with a mobile robot},
  author={Shriwastav, Sachin and Snyder, Gregory and Song, Zhuoyuan},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={11910--11915},
  year={2022},
  organization={IEEE}
}

@article{lynch2008decentralized,
  title={Decentralized environmental modeling by mobile sensor networks},
  author={Lynch, Kevin M and Schwartz, Ira B and Yang, Peng and Freeman, Randy A},
  journal={IEEE transactions on robotics},
  volume={24},
  number={3},
  pages={710--724},
  year={2008},
  publisher={IEEE}
}

@article{leonard2007collective,
  title={Collective motion, sensor networks, and ocean sampling},
  author={Leonard, Naomi Ehrich and Paley, Derek A and Lekien, Francois and Sepulchre, Rodolphe and Fratantoni, David M and Davis, Russ E},
  journal={Proceedings of the IEEE},
  volume={95},
  number={1},
  pages={48--74},
  year={2007},
  publisher={IEEE}
}

@article{devries2013observability,
  title={Observability-based optimization of coordinated sampling trajectories for recursive estimation of a strong, spatially varying flowfield},
  author={DeVries, Levi and Majumdar, Sharanya J and Paley, Derek A},
  journal={Journal of intelligent \& robotic systems},
  volume={70},
  number={1-4},
  pages={527--544},
  year={2013},
  publisher={Springer}
}

@article{ogren2004cooperative,
  title={Cooperative control of mobile sensor networks: Adaptive gradient climbing in a distributed environment},
  author={Ogren, Petter and Fiorelli, Edward and Leonard, Naomi Ehrich},
  journal={IEEE Transactions on Automatic control},
  volume={49},
  number={8},
  pages={1292--1302},
  year={2004},
  publisher={IEEE}
}

@article{zhang2010cooperative,
  title={Cooperative filters and control for cooperative exploration},
  author={Zhang, Fumin and Leonard, Naomi Ehrich},
  journal={IEEE Transactions on Automatic Control},
  volume={55},
  number={3},
  pages={650--663},
  year={2010},
  publisher={IEEE}
}

@article{paley2020mobile,
  title={Mobile sensor networks and control: Adaptive sampling of spatiotemporal processes},
  author={Paley, Derek A and Wolek, Artur},
  journal={Annual Review of Control, Robotics, and Autonomous Systems},
  volume={3},
  pages={91--114},
  year={2020},
  publisher={Annual Reviews}
}

@article{peng2014dynamic,
  title={Dynamic data driven application system for plume estimation using UAVs},
  author={Peng, Liqian and Lipinski, Doug and Mohseni, Kamran},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={74},
  number={1-2},
  pages={421--436},
  year={2014},
  publisher={Springer}
}

@article{schmitz2014accuracy,
  title={Accuracy and repeatability of joint angles measured using a single camera markerless motion capture system},
  author={Schmitz, Anne and Ye, Mao and Shapiro, Robert and Yang, Ruigang and Noehren, Brian},
  journal={Journal of biomechanics},
  volume={47},
  number={2},
  pages={587--591},
  year={2014},
  publisher={Elsevier}
}
