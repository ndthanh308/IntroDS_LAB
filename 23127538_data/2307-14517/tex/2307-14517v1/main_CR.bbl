\begin{thebibliography}{10}

\bibitem{adebayo_sanity_2018}
J.~Adebayo, J.~Gilmer, M.~Muelly, I.~Goodfellow, M.~Hardt, and B.~Kim.
\newblock Sanity checks for saliency maps.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems}, 2018.

\bibitem{barredo_arrieta_explainable_2020}
A.~Barredo~Arrieta, N.~Díaz-Rodríguez, J.~Del~Ser, A.~Bennetot, S.~Tabik,
  A.~Barbado, S.~Garcia, S.~Gil-Lopez, D.~Molina, R.~Benjamins, R.~Chatila, and
  F.~Herrera.
\newblock Explainable {Artificial} {Intelligence} ({XAI}): {Concepts},
  taxonomies, opportunities and challenges toward responsible {AI}.
\newblock {\em Information Fusion}, 58, 2020.

\bibitem{biederman1987recognition}
I.~Biederman.
\newblock Recognition-by-components: a theory of human image understanding.
\newblock {\em Psychological review}, 94(2):115, 1987.

\bibitem{iclr/BorowskiZSGWBB21}
J.~Borowski, R.~S. Zimmermann, J.~Schepers, R.~Geirhos, T.~S.~A. Wallis,
  M.~Bethge, and W.~Brendel.
\newblock {Exemplary Natural Images Explain {CNN} Activations Better than
  State-of-the-Art Feature Visualization}.
\newblock In {\em Proc. Intl. Conference on Learning Representations}, 2021.

\bibitem{bagnet_iclr}
W.~Brendel and M.~Bethge.
\newblock {Approximating CNNs with Bag-of-local-Features models works
  surprisingly well on ImageNet}.
\newblock In {\em Proc. Intl. Conference on Learning Representations}, 2019.

\bibitem{bruckert2020next}
S.~Bruckert, B.~Finzel, and U.~Schmid.
\newblock The next generation of medical decision support: a roadmap toward
  transparent expert companions.
\newblock {\em Frontiers in artificial intelligence}, 3:507973, 2020.

\bibitem{carloni_2022}
G.~Carloni, A.~Berti, M.~Pascali, and S.~Colantonio.
\newblock On the applicability of prototypical part learning in medical images:
  Breast masses classification using protopnet.
\newblock In {\em Proceedings International Conference on Pattern Recognition
  (ICPR) - 2nd International Workshop on Artificial Intelligence for Healthcare
  Applications}, 08 2022.

\bibitem{nips/ChenLTBRS19}
C.~Chen, O.~Li, D.~Tao, A.~Barnett, C.~Rudin, and J.~Su.
\newblock This looks like that: Deep learning for interpretable image
  recognition.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, {\em Advances in
  Neural Information Processing Systems}, pages 8928--8939, 2019.

\bibitem{clement2023}
T.~Clement, N.~Kemmerzell, M.~Abdelaal, and M.~Amberg.
\newblock Xair: A systematic metareview of explainable ai (xai) aligned to the
  software development process.
\newblock {\em Machine Learning and Knowledge Extraction}, 5(1):78--108, 2023.

\bibitem{colin_what_2022}
J.~Colin, T.~Fel, R.~Cadene, and T.~Serre.
\newblock What {I} {Cannot} {Predict}, {I} {Do} {Not} {Understand}: {A}
  {Human}-{Centered} {Evaluation} {Framework} for {Explainability} {Methods}.
\newblock In {\em Advances in Neural Information Processing Systems}, Oct.
  2022.

\bibitem{icdm/DasXDER20}
S.~Das, P.~Xu, Z.~Dai, A.~Endert, and L.~Ren.
\newblock {Interpreting Deep Neural Networks through Prototype Factorization}.
\newblock In G.~D. Fatta, V.~S. Sheng, A.~Cuzzocrea, C.~Zaniolo, and X.~Wu,
  editors, {\em Proc. Intl. Conference on Data Mining Workshops, {ICDM}
  Workshops}, pages 448--457. {IEEE}, 2020.

\bibitem{doshi-velez_considerations_2018}
F.~Doshi-Velez and B.~Kim.
\newblock Considerations for {Evaluation} and {Generalization} in
  {Interpretable} {Machine} {Learning}.
\newblock In {\em Explainable and {Interpretable} {Models} in {Computer}
  {Vision} and {Machine} {Learning}}. Springer, Cham, 2018.

\bibitem{ehsan2021explainable}
U.~Ehsan, S.~Passi, Q.~V. Liao, L.~Chan, I.~Lee, M.~Muller, M.~O. Riedl, et~al.
\newblock {The Who in explainable AI: How AI background shapes perceptions of
  AI explanations}.
\newblock {\em arXiv preprint arXiv:2107.13509}, 2021.

\bibitem{cvpr/FongV18}
R.~Fong and A.~Vedaldi.
\newblock {Net2Vec: Quantifying and Explaining How Concepts Are Encoded by
  Filters in Deep Neural Networks}.
\newblock In {\em Proc. {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 8730--8738. {IEEE} Computer Society, 2018.

\bibitem{gautam_prp_2023}
S.~Gautam, M.~M.-C. Höhne, S.~Hansen, R.~Jenssen, and M.~Kampffmeyer.
\newblock {This looks More Like that: Enhancing Self-Explaining Models by
  Prototypical Relevance Propagation}.
\newblock {\em Pattern Recognition}, 136:109172, 2023.

\bibitem{nips/GhorbaniWZK19}
A.~Ghorbani, J.~Wexler, J.~Y. Zou, and B.~Kim.
\newblock {Towards Automatic Concept-based Explanations}.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, {\em Advances in
  Neural Information Processing Systems}, pages 9273--9282, 2019.

\bibitem{goodman2017european}
B.~Goodman and S.~Flaxman.
\newblock European union regulations on algorithmic decision-making and a
  “right to explanation”.
\newblock {\em AI Magazine}, 38(3):50--57, 2017.

\bibitem{goyal2019explaining}
Y.~Goyal, A.~Feder, U.~Shalit, and B.~Kim.
\newblock Explaining classifiers with causal concept effect (cace).
\newblock {\em arXiv preprint arXiv:1907.07165}, 2019.

\bibitem{icml/GoyalWEBPL19}
Y.~Goyal, Z.~Wu, J.~Ernst, D.~Batra, D.~Parikh, and S.~Lee.
\newblock Counterfactual visual explanations.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, {\em Proc. Intl.
  Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach,
  California, {USA}}, volume~97, pages 2376--2384. {PMLR}, 2019.

\bibitem{guidotti2018survey}
R.~Guidotti, A.~Monreale, S.~Ruggieri, F.~Turini, F.~Giannotti, and
  D.~Pedreschi.
\newblock A survey of methods for explaining black box models.
\newblock {\em ACM computing surveys (CSUR)}, 51(5):1--42, 2018.

\bibitem{guo_calibration_2017}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In D.~Precup and Y.~W. Teh, editors, {\em Proc. International
  Conference on Machine Learning}, volume~70, pages 1321--1330. PMLR, 06--11
  Aug 2017.

\bibitem{acl/HaseB20}
P.~Hase and M.~Bansal.
\newblock Evaluating explainable {AI:} which algorithmic explanations help
  users predict model behavior?
\newblock In D.~Jurafsky, J.~Chai, N.~Schluter, and J.~R. Tetreault, editors,
  {\em Proc. Annual Meeting of the Association for Computational Linguistics,
  {ACL} 2020, Online, July 5-10, 2020}, pages 5540--5552. Association for
  Computational Linguistics, 2020.

\bibitem{hase_nips_21_ood_xai}
P.~Hase, H.~Xie, and M.~Bansal.
\newblock The out-of-distribution problem in explainability and search methods
  for feature importance explanations.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 3650--3666. Curran Associates, Inc., 2021.

\bibitem{he2021towards_nico}
Y.~He, Z.~Shen, and P.~Cui.
\newblock Towards non-iid image classification: A dataset and baselines.
\newblock {\em Pattern Recognition}, 110:107383, 2021.

\bibitem{hoffman_metrics_2019}
R.~R. Hoffman, S.~T. Mueller, G.~Klein, and J.~Litman.
\newblock Metrics for {Explainable} {AI}: {Challenges} and {Prospects}.
\newblock {\em arXiv:1812.04608 [cs]}, Feb. 2019.
\newblock arXiv: 1812.04608.

\bibitem{hoffmann2021looks}
A.~Hoffmann, C.~Fanconi, R.~Rade, and J.~Kohler.
\newblock This looks like that... does it? shortcomings of latent space
  prototype interpretability in deep networks.
\newblock {\em arXiv preprint arXiv:2105.02968}, 2021.

\bibitem{huysmans_empirical_2011}
J.~Huysmans, K.~Dejaeger, C.~Mues, J.~Vanthienen, and B.~Baesens.
\newblock An empirical evaluation of the comprehensibility of decision table,
  tree and rule based predictive models.
\newblock {\em Decision Support Systems}, 51(1), 2011.

\bibitem{acl/JacoviG20}
A.~Jacovi and Y.~Goldberg.
\newblock Towards faithfully interpretable {NLP} systems: How should we define
  and evaluate faithfulness?
\newblock In D.~Jurafsky, J.~Chai, N.~Schluter, and J.~R. Tetreault, editors,
  {\em Proc. Annual Meeting of the Association for Computational Linguistics,
  {ACL} 2020, Online, July 5-10, 2020}, pages 4198--4205. Association for
  Computational Linguistics, 2020.

\bibitem{nips/JeyakumarNCGS20}
J.~V. Jeyakumar, J.~Noor, Y.~Cheng, L.~Garcia, and M.~B. Srivastava.
\newblock How can {I} explain this to you? an empirical study of deep neural
  network explanation methods.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{kaur_interpreting_2020}
H.~Kaur, H.~Nori, S.~Jenkins, R.~Caruana, H.~Wallach, and J.~Wortman~Vaughan.
\newblock Interpreting interpretability: Understanding data scientists' use of
  interpretability tools for machine learning.
\newblock In {\em Proc. CHI Conference on Human Factors in Computing Systems},
  page 1–14, New York, NY, USA, 2020. Association for Computing Machinery.

\bibitem{Kim2022HIVE}
S.~S.~Y. Kim, N.~Meister, V.~V. Ramaswamy, R.~Fong, and O.~Russakovsky.
\newblock {HIVE}: Evaluating the human interpretability of visual explanations.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2022.

\bibitem{kulesza_principles_2015}
T.~Kulesza, M.~Burnett, W.-K. Wong, and S.~Stumpf.
\newblock Principles of explanatory debugging to personalize interactive
  machine learning.
\newblock In {\em Proc. Intl. Conference on Intelligent User Interfaces}, page
  126–137, New York, NY, USA, 2015. Association for Computing Machinery.

\bibitem{nips/LakkarajuL16}
H.~Lakkaraju and J.~Leskovec.
\newblock Confusions over time: An interpretable bayesian model to characterize
  trends in decision making.
\newblock In D.~D. Lee, M.~Sugiyama, U.~von Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems}, pages
  3261--3269, 2016.

\bibitem{lampert_animals_attributes}
C.~H. Lampert, H.~Nickisch, and S.~Harmeling.
\newblock Learning to detect unseen object classes by between-class attribute
  transfer.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 951--958, 2009.

\bibitem{liu2021synthetic}
Y.~Liu, S.~Khandagale, C.~White, and W.~Neiswanger.
\newblock Synthetic benchmarks for scientific research in explainable machine
  learning.
\newblock In {\em NeurIPS Datasets and Benchmarks Track}, 2021.

\bibitem{michel2022survey}
A.~Michel, S.~K. Jha, and R.~Ewetz.
\newblock A survey on the vulnerability of deep neural networks against
  adversarial attacks.
\newblock {\em Progress in Artificial Intelligence}, pages 1--11, 2022.

\bibitem{mohammadjafari2021using}
S.~Mohammadjafari, M.~Cevik, M.~Thanabalasingam, and A.~Basar.
\newblock Using protopnet for interpretable alzheimer's disease classification.
\newblock In {\em Canadian Conference on AI}, 2021.

\bibitem{montavon_gradient-based_2019}
G.~Montavon.
\newblock Gradient-based vs. propagation-based explanations: An axiomatic
  comparison.
\newblock In {\em Explainable AI: Interpreting, Explaining and Visualizing Deep
  Learning}. Springer, 2019.

\bibitem{nakka_robust_2020}
K.~K. Nakka and M.~Salzmann.
\newblock Towards robust fine-grained recognition by maximal separation of
  discriminative features.
\newblock In H.~Ishikawa, C.-L. Liu, T.~Pajdla, and J.~Shi, editors, {\em
  Computer Vision -- ACCV 2020}, pages 391--408, Cham, 2021. Springer
  International Publishing.

\bibitem{nauta_exproto_2021}
M.~Nauta, A.~Jutte, J.~Provoost, and C.~Seifert.
\newblock This looks like that, because ... explaining prototypes
  for interpretable image recognition.
\newblock In {\em Machine Learning and Principles and Practice of Knowledge
  Discovery in Databases}, pages 441--456, Cham, 2021. Springer International
  Publishing.

\bibitem{nauta_pipnet}
M.~Nauta, J.~Schl\"otterer, M.~van Keulen, and C.~Seifert.
\newblock Pip-net: Patch-based intuitive prototypes for interpretable image
  classification.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 2744--2753, June 2023.

\bibitem{Nauta2023_csur_evaluating-xai-survey}
M.~Nauta, J.~Trienes, S.~Pathak, E.~Nguyen, M.~Peters, Y.~Schmitt,
  J.~Schlötterer, M.~van Keulen, and C.~Seifert.
\newblock {From Anecdotal Evidence to Quantitative Evaluation Methods: A
  Systematic Review on Evaluating Explainable AI}.
\newblock {\em ACM Comput. Surv.}, 2023.

\bibitem{Nauta_2021_CVPR_ProtoTree}
M.~Nauta, R.~van Bree, and C.~Seifert.
\newblock Neural prototype trees for interpretable fine-grained image
  recognition.
\newblock In {\em Proc. IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 14933--14943, June 2021.

\bibitem{nguyen_effectiveness_2021}
G.~Nguyen, D.~Kim, and A.~Nguyen.
\newblock The effectiveness of feature attribution methods and its correlation
  with automatic evaluation scores.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 26422--26436. Curran Associates, Inc., 2021.

\bibitem{papenmeier_complicated_2022}
A.~Papenmeier, D.~Kern, G.~Englebienne, and C.~Seifert.
\newblock {It’s Complicated: The Relationship between User Trust, Model
  Accuracy and Explanations in AI}.
\newblock {\em ACM Trans. Comput.-Hum. Interact.}, 29(4), 2022.

\bibitem{pearce2021understanding}
T.~Pearce, A.~Brintrup, and J.~Zhu.
\newblock Understanding softmax confidence and uncertainty.
\newblock {\em arXiv preprint arXiv:2106.04972}, 2021.

\bibitem{pmlr-v162-rong22a}
Y.~Rong, T.~Leemann, V.~Borisov, G.~Kasneci, and E.~Kasneci.
\newblock {A Consistent and Efficient Evaluation Strategy for Attribution
  Methods}.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and
  S.~Sabato, editors, {\em Proc. Intl. Conference on Machine Learning}, volume
  162, pages 18770--18795. PMLR, 17--23 Jul 2022.

\bibitem{rudin2019stop}
C.~Rudin.
\newblock Stop explaining black box machine learning models for high stakes
  decisions and use interpretable models instead.
\newblock {\em Nature Machine Intelligence}, 1(5):206--215, 2019.

\bibitem{rymarczyk_2022_protopool}
D.~Rymarczyk, {\L}.~Struski, M.~G{\'o}rszczak, K.~Lewandowska, J.~Tabor, and
  B.~Zieli{\'{n}}ski.
\newblock Interpretable image classification with differentiable prototypes
  assignment.
\newblock In S.~Avidan, G.~Brostow, M.~Ciss{\'e}, G.~M. Farinella, and
  T.~Hassner, editors, {\em Computer Vision -- ECCV 2022}, pages 351--368,
  Cham, 2022. Springer Nature Switzerland.

\bibitem{protopshare_rymarczyk_2021}
D.~Rymarczyk, L.~Struski, J.~Tabor, and B.~Zieli\'{n}ski.
\newblock {ProtoPShare: Prototypical Parts Sharing for Similarity Discovery in
  Interpretable Image Classification}.
\newblock In {\em Proc. ACM SIGKDD Conference on Knowledge Discovery \& Data
  Mining}, page 1420–1430, New York, NY, USA, 2021. Association for Computing
  Machinery.

\bibitem{iccv/SelvarajuCDVPB17}
R.~R. Selvaraju, M.~Cogswell, A.~Das, R.~Vedantam, D.~Parikh, and D.~Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In {\em {IEEE} International Conference on Computer Vision, {ICCV}},
  pages 618--626. {IEEE} Computer Society, 2017.

\bibitem{Singh2022-proto-on-chest_ctscan}
G.~Singh.
\newblock Think positive: An interpretable neural network for image
  recognition.
\newblock {\em Neural Netw.}, 151:178--189, July 2022.

\bibitem{Singh2021-proto-on-chest-xray}
G.~Singh and K.-C. Yow.
\newblock An interpretable deep learning model for covid-19 detection with
  chest x-ray images.
\newblock {\em IEEE Access}, 9:85198--85208, 2021.

\bibitem{sinhamahapatra2022towards}
P.~Sinhamahapatra, L.~Heidemann, M.~Monnet, and K.~Roscher.
\newblock Towards human-interpretable prototypes for visual assessment of image
  classification models.
\newblock {\em arXiv preprint arXiv:2211.12173}, 2022.

\bibitem{summers_nondeterminism_21}
C.~Summers and M.~J. Dinneen.
\newblock Nondeterminism and instability in neural network optimization.
\newblock In M.~Meila and T.~Zhang, editors, {\em Proc. Intl. Conference on
  Machine Learning}, volume 139, pages 9913--9922. PMLR, 18--24 Jul 2021.

\bibitem{vilone2021notions}
G.~Vilone and L.~Longo.
\newblock Notions of explainability and evaluation approaches for explainable
  artificial intelligence.
\newblock {\em Information Fusion}, 76, 2021.

\bibitem{wang2022knowledge}
C.~Wang, Y.~Chen, Y.~Liu, Y.~Tian, F.~Liu, D.~J. McCarthy, M.~Elliott,
  H.~Frazer, and G.~Carneiro.
\newblock Knowledge distillation to ensemble global and interpretable
  prototype-based mammogram classification models.
\newblock In {\em Medical Image Computing and Computer Assisted
  Intervention--MICCAI 2022: 25th International Conference, Singapore,
  Proceedings, Part III}, pages 14--24. Springer, 2022.

\bibitem{Wang_2021_ICCV_tesnet}
J.~Wang, H.~Liu, X.~Wang, and L.~Jing.
\newblock Interpretable image recognition by constructing transparent embedding
  space.
\newblock In {\em Proc. IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 895--904, October 2021.

\bibitem{williams2017measuring}
P.~A. Williams, J.~Jenkins, J.~Valacich, and M.~D. Byrd.
\newblock {Measuring actual behaviors in HCI research--a call to action and an
  example}.
\newblock {\em AIS Transactions on Human-Computer Interaction}, 9(4):339--352,
  2017.

\bibitem{xie2022vit}
W.~Xie, X.-H. Li, C.~C. Cao, and N.~L. Zhang.
\newblock {ViT-CX: Causal Explanation of Vision Transformers}.
\newblock {\em arXiv preprint arXiv:2211.03064}, 2022.

\bibitem{xudarme_romain_sanity_2023}
R.~Xu-Darme, G.~Qu{\'e}not, Z.~Chihani, and M.-C. Rousset.
\newblock {Sanity checks and improvements for patch visualisation in
  prototype-based image classification}.
\newblock working paper or preprint, Jan. 2023.

\bibitem{xue2022protopformer}
M.~Xue, Q.~Huang, H.~Zhang, L.~Cheng, J.~Song, M.~Wu, and M.~Song.
\newblock Protopformer: Concentrating on prototypical parts in vision
  transformers for interpretable image recognition.
\newblock {\em arXiv preprint arXiv:2208.10431}, 2022.

\bibitem{BAM2019}
M.~Yang and B.~Kim.
\newblock {Benchmarking Attribution Methods with Relative Feature Importance}.
\newblock {\em CoRR}, abs/1907.09701, 2019.

\bibitem{nips/YehKALPR20}
C.~Yeh, B.~Kim, S.~{\"{O}}. Arik, C.~Li, T.~Pfister, and P.~Ravikumar.
\newblock On completeness-aware concept-based explanations in deep neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{zhou_evaluating_2021}
J.~Zhou, A.~H. Gandomi, F.~Chen, and A.~Holzinger.
\newblock Evaluating the quality of machine learning explanations: A survey on
  methods and metrics.
\newblock {\em Electronics}, 10(5), 2021.

\bibitem{zhuang_randomness_22}
D.~Zhuang, X.~Zhang, S.~Song, and S.~Hooker.
\newblock Randomness in neural network training: Characterizing the impact of
  tooling.
\newblock In D.~Marculescu, Y.~Chi, and C.~Wu, editors, {\em Proc. Machine
  Learning and Systems}, volume~4, pages 316--336, 2022.

\end{thebibliography}
