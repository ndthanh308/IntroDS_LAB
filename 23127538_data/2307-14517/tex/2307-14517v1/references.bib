@inproceedings{colin_what_2022,
	title = {What {I} {Cannot} {Predict}, {I} {Do} {Not} {Understand}: {A} {Human}-{Centered} {Evaluation} {Framework} for {Explainability} {Methods}},
	shorttitle = {What {I} {Cannot} {Predict}, {I} {Do} {Not} {Understand}},
    booktitle = {Advances in Neural Information Processing Systems},
	language = {en},
	urldate = {2022-12-20},
	author = {Colin, Julien and Fel, Thomas and Cadene, Remi and Serre, Thomas},
	month = oct,
	year = {2022},
}

@article{hoffmann2021looks,
  title={This looks like that... does it? Shortcomings of latent space prototype interpretability in deep networks},
  author={Hoffmann, Adrian and Fanconi, Claudio and Rade, Rahul and Kohler, Jonas},
  journal={arXiv preprint arXiv:2105.02968},
  year={2021}
}


@inproceedings{nips/JeyakumarNCGS20,
  author    = {Jeya Vikranth Jeyakumar and
               Joseph Noor and
               Yu{-}Hsi Cheng and
               Luis Garcia and
               Mani B. Srivastava},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {How Can {I} Explain This to You? An Empirical Study of Deep Neural
               Network Explanation Methods},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/2c29d89cc56cdb191c60db2f0bae796b-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/JeyakumarNCGS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sinhamahapatra2022towards,
  title={Towards Human-Interpretable Prototypes for Visual Assessment of Image Classification Models},
  author={Sinhamahapatra, Poulami and Heidemann, Lena and Monnet, Maureen and Roscher, Karsten},
  journal={arXiv preprint arXiv:2211.12173},
  year={2022}
}


@InProceedings{pmlr-v162-rong22a,
  title = 	 {{A Consistent and Efficient Evaluation Strategy for Attribution Methods}},
  author =       {Rong, Yao and Leemann, Tobias and Borisov, Vadim and Kasneci, Gjergji and Kasneci, Enkelejda},
  booktitle = 	 {Proc. Intl. Conference on Machine Learning},
  pages = 	 {18770--18795},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/rong22a/rong22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/rong22a.html},
}

@article{xie2022vit,
  title={{ViT-CX: Causal Explanation of Vision Transformers}},
  author={Xie, Weiyan and Li, Xiao-Hui and Cao, Caleb Chen and Zhang, Nevin L},
  journal={arXiv preprint arXiv:2211.03064},
  year={2022}
}

@inproceedings{hase_nips_21_ood_xai,
 author = {Hase, Peter and Xie, Harry and Bansal, Mohit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {3650--3666},
 publisher = {Curran Associates, Inc.},
 title = {The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations},
 url = {https://proceedings.neurips.cc/paper/2021/file/1def1713ebf17722cbe300cfc1c88558-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{kaur_interpreting_2020,
author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},
title = {Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy2.utwente.nl/10.1145/3313831.3376219},
doi = {10.1145/3313831.3376219},
abstract = {Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretability tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scientists' use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visualizations output by these tools. We highlight qualitative themes for data scientists' mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our findings in the social science literature.},
booktitle = {Proc. CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {machine learning, user-centric evaluation, interpretability},
location = {Honolulu, HI, USA},
}

@article{papenmeier_complicated_2022,
author = {Papenmeier, Andrea and Kern, Dagmar and Englebienne, Gwenn and Seifert, Christin},
title = {{It’s Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI}},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3495013},
doi = {10.1145/3495013},
abstract = {Automated decision-making systems become increasingly powerful due to higher model complexity. While powerful in prediction accuracy, Deep Learning models are black boxes by nature, preventing users from making informed judgments about the correctness and fairness of such an automated system. Explanations have been proposed as a general remedy to the black box problem. However, it remains unclear if effects of explanations on user trust generalise over varying accuracy levels. In an online user study with 959 participants, we examined the practical consequences of adding explanations for user trust: We evaluated trust for three explanation types on three classifiers of varying accuracy. We find that the influence of our explanations on trust differs depending on the classifier’s accuracy. Thus, the interplay between trust and explanations is more complex than previously reported. Our findings also reveal discrepancies between self-reported and behavioural trust, showing that the choice of trust measure impacts the results.},
journal = {ACM Trans. Comput.-Hum. Interact.},
articleno = {35},
numpages = {33},
keywords = {user trust, machine learning, Explainable AI, explanation fidelity, minimum explanations}
}

@inproceedings{nguyen_effectiveness_2021,
 author = {Nguyen, Giang and Kim, Daeyoung and Nguyen, Anh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26422--26436},
 publisher = {Curran Associates, Inc.},
 title = {The effectiveness of feature attribution methods and its correlation with automatic evaluation scores},
 url = {https://proceedings.neurips.cc/paper/2021/file/de043a5e421240eb846da8effe472ff1-Paper.pdf},
 volume = {34},
 year = {2021}
}

@unpublished{xudarme_romain_sanity_2023,
  TITLE = {{Sanity checks and improvements for patch visualisation in prototype-based image classification}},
  AUTHOR = {Xu-Darme, Romain and Qu{\'e}not, Georges and Chihani, Zakaria and Rousset, Marie-Christine},
  URL = {https://hal-cea.archives-ouvertes.fr/cea-03943308},
  NOTE = {working paper or preprint},
  YEAR = {2023},
  MONTH = Jan,
  KEYWORDS = {Case-based Reasoning ; Saliency Map ; Evaluation metrics ; Interpretability of AI},
  PDF = {https://hal-cea.archives-ouvertes.fr/cea-03943308/file/Sanity_checks_and_improvements_for_patch_visualisation_in_prototype_based_image_classification.pdf},
  HAL_ID = {cea-03943308},
  HAL_VERSION = {v1},
}

@INPROCEEDINGS{lampert_animals_attributes,
  author={Lampert, Christoph H. and Nickisch, Hannes and Harmeling, Stefan},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Learning to detect unseen object classes by between-class attribute transfer}, 
  year={2009},
  volume={},
  number={},
  pages={951-958},
  doi={10.1109/CVPR.2009.5206594}}

@Article{BAM2019,
  title = {{Benchmarking Attribution Methods with Relative Feature Importance}},
  author = {Yang, Mengjiao and Kim, Been},
  journal   = {CoRR},
  volume    = {abs/1907.09701},
  year = {2019}
}

@article{he2021towards_nico,
  title={Towards non-iid image classification: A dataset and baselines},
  author={He, Yue and Shen, Zheyan and Cui, Peng},
  journal={Pattern Recognition},
  volume={110},
  pages={107383},
  year={2021},
  publisher={Elsevier}
}


@InProceedings{guo_calibration_2017,
  title = 	 {On Calibration of Modern Neural Networks},
  author =       {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = 	 {Proc. International Conference on Machine Learning},
  pages = 	 {1321--1330},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/guo17a/guo17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/guo17a.html},
  abstract = 	 {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}
}

@article{pearce2021understanding,
  title={Understanding softmax confidence and uncertainty},
  author={Pearce, Tim and Brintrup, Alexandra and Zhu, Jun},
  journal={arXiv preprint arXiv:2106.04972},
  year={2021}
}

@article{Zhou_Booth_Ribeiro_Shah_2022, 
title={{Do Feature Attribution Methods Correctly Attribute Features?}}, 
volume={36}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/21196}, DOI={10.1609/aaai.v36i9.21196}, abstractNote={Feature attribution methods are popular in interpretable machine learning. These methods compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of &quot;attribution&quot;, leading to many competing methods with little systematic evaluation, complicated in particular by the lack of ground truth attribution. To address this, we propose a dataset modification procedure to induce such ground truth. Using this procedure, we evaluate three common methods: saliency maps, rationales, and attentions. We identify several deficiencies and add new perspectives to the growing body of evidence questioning the correctness and reliability of these methods applied on datasets in the wild. We further discuss possible avenues for remedy and recommend new attribution methods to be tested against ground truth before deployment. The code and appendix are available at https://yilunzhou.github.io/feature-attribution-evaluation/.}, number={9}, 
journal={Proc. AAAI Conference on Artificial Intelligence}, author={Zhou, Yilun and Booth, Serena and Ribeiro, Marco Tulio and Shah, Julie}, year={2022}, 
pages={9623-9633} }

@article{ehsan2021explainable,
  title={{The Who in explainable AI: How AI background shapes perceptions of AI explanations}},
  author={Ehsan, Upol and Passi, Samir and Liao, Q Vera and Chan, Larry and Lee, I and Muller, Michael and Riedl, Mark O and others},
  journal={arXiv preprint arXiv:2107.13509},
  year={2021}
}

@InProceedings{nakka_robust_2020,
author="Nakka, Krishna Kanth
and Salzmann, Mathieu",
editor="Ishikawa, Hiroshi
and Liu, Cheng-Lin
and Pajdla, Tomas
and Shi, Jianbo",
title="Towards Robust Fine-Grained Recognition by Maximal Separation of Discriminative Features",
booktitle="Computer Vision -- ACCV 2020",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="391--408",
abstract="Adversarial attacks have been widely studied for general classification tasks, but remain unexplored in the context of fine-grained recognition, where the inter-class similarities facilitate the attacker's task. In this paper, we identify the proximity of the latent representations of local regions of different classes in fine-grained recognition networks as a key factor to the success of adversarial attacks. We therefore introduce an attention-based regularization mechanism that maximally separates the latent features of discriminative regions of different classes while minimizing the contribution of the non-discriminative regions to the final class prediction. As evidenced by our experiments, this allows us to significantly improve robustness to adversarial attacks, to the point of matching or even surpassing that of adversarial training, but without requiring access to adversarial samples. Further, our formulation also improves detection AUROC of adversarial samples over baselines on adversarially trained models.",
isbn="978-3-030-69544-6"
}

@article{williams2017measuring,
  title={{Measuring actual behaviors in HCI research--a call to action and an example}},
  author={Williams, Parker A and Jenkins, Jeffrey and Valacich, Joseph and Byrd, Michael D},
  journal={AIS Transactions on Human-Computer Interaction},
  volume={9},
  number={4},
  pages={339--352},
  year={2017}
}

@inproceedings{zhuang_randomness_22,
 author = {Zhuang, Donglin and Zhang, Xingyao and Song, Shuaiwen and Hooker, Sara},
 booktitle = {Proc. Machine Learning and Systems},
 editor = {D. Marculescu and Y. Chi and C. Wu},
 pages = {316--336},
 title = {Randomness in Neural Network Training: Characterizing the Impact of Tooling},
 url = {https://proceedings.mlsys.org/paper/2022/file/757b505cfd34c64c85ca5b5690ee5293-Paper.pdf},
 volume = {4},
 year = {2022}
}


@InProceedings{summers_nondeterminism_21,
  title = 	 {Nondeterminism and Instability in Neural Network Optimization},
  author =       {Summers, Cecilia and Dinneen, Michael J.},
  booktitle = 	 {Proc. Intl. Conference on Machine Learning},
  pages = 	 {9913--9922},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/summers21a/summers21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/summers21a.html},
  abstract = 	 {Nondeterminism in neural network optimization produces uncertainty in performance, making small improvements difficult to discern from run-to-run variability. While uncertainty can be reduced by training multiple model copies, doing so is time-consuming, costly, and harms reproducibility. In this work, we establish an experimental protocol for understanding the effect of optimization nondeterminism on model diversity, allowing us to isolate the effects of a variety of sources of nondeterminism. Surprisingly, we find that all sources of nondeterminism have similar effects on measures of model diversity. To explain this intriguing fact, we identify the instability of model training, taken as an end-to-end procedure, as the key determinant. We show that even one-bit changes in initial parameters result in models converging to vastly different values. Last, we propose two approaches for reducing the effects of instability on run-to-run variability.}
}

@article{michel2022survey,
  title={A survey on the vulnerability of deep neural networks against adversarial attacks},
  author={Michel, Andy and Jha, Sumit Kumar and Ewetz, Rickard},
  journal={Progress in Artificial Intelligence},
  pages={1--11},
  year={2022},
  publisher={Springer}
}


@InProceedings{goyal_counterfactual_2019,
  title = 	 {Counterfactual Visual Explanations},
  author =       {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle = 	 {Proc. Intl. Conference on Machine Learning},
  pages = 	 {2376--2384},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/goyal19a/goyal19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/goyal19a.html},
  abstract = 	 {In this work, we develop a technique to produce counterfactual visual explanations. Given a ‘query’ image $I$ for which a vision system predicts class $c$, a counterfactual visual explanation identifies how $I$ could change such that the system would output a different specified class $c’$. To do this, we select a ‘distractor’ image $I’$ that the system predicts as class $c’$ and identify spatial regions in $I$ and $I’$ such that replacing the identified region in $I$ with the identified region in $I’$ would push the system towards classifying $I$ as $c’$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.}
}

@article{Nauta2023_csur_evaluating-xai-survey,
author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schlötterer, Jörg and van Keulen, Maurice and Seifert, Christin},
title = {{From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI}},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583558},
doi = {10.1145/3583558},
journal = {ACM Comput. Surv.},
comment   = {https://utwente-dmb.github.io/xai-papers/},
}

@inproceedings{Kim2022HIVE,
      author = {Sunnie S. Y. Kim and Nicole Meister and Vikram V. Ramaswamy and Ruth Fong and Olga Russakovsky},
      title = {{HIVE}: Evaluating the Human Interpretability of Visual Explanations},
      booktitle = {European Conference on Computer Vision (ECCV)},
      year = {2022}
    }

@InProceedings{nauta_pipnet,
    author    = {Nauta, Meike and Schl\"otterer, J\"org and van Keulen, Maurice and Seifert, Christin},
    title     = {PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {2744-2753}
}

@InProceedings{Nauta_2021_CVPR_ProtoTree,
    author    = {Nauta, Meike and van Bree, Ron and Seifert, Christin},
    title     = {Neural Prototype Trees for Interpretable Fine-Grained Image Recognition},
    booktitle = {Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {14933-14943}
}

@InProceedings{rymarczyk_2022_protopool,
author="Rymarczyk, Dawid
and Struski, {\L}ukasz
and G{\'o}rszczak, Micha{\l}
and Lewandowska, Koryna
and Tabor, Jacek
and Zieli{\'{n}}ski, Bartosz",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Interpretable Image Classification with Differentiable Prototypes Assignment",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="351--368",
abstract="Existing prototypical-based models address the black-box nature of deep learning. However, they are sub-optimal as they often assume separate prototypes for each class, require multi-step optimization, make decisions based on prototype absence (so-called negative reasoning process), and derive vague prototypes. To address those shortcomings, we introduce ProtoPool, an interpretable prototype-based model with positive reasoning and three main novelties. Firstly, we reuse prototypes in classes, which significantly decreases their number. Secondly, we allow automatic, fully differentiable assignment of prototypes to classes, which substantially simplifies the training process. Finally, we propose a new focal similarity function that contrasts the prototype from the background and consequently concentrates on more salient visual features. We show that ProtoPool obtains state-of-the-art accuracy on the CUB-200-2011 and the Stanford Cars datasets, substantially reducing the number of prototypes. We provide a theoretical analysis of the method and a user study to show that our prototypes capture more salient features than those obtained with competitive methods. We made the code available at https://github.com/gmum/ProtoPool.",
isbn="978-3-031-19775-8"
}

@inproceedings{nips/ChenLTBRS19,
  author    = {Chaofan Chen and
               Oscar Li and
               Daniel Tao and
               Alina Barnett and
               Cynthia Rudin and
               Jonathan Su},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {This Looks Like That: Deep Learning for Interpretable Image Recognition},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {8928--8939},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:19 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/ChenLTBRS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gautam_prp_2023,
title = {{This looks More Like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation}},
journal = {Pattern Recognition},
volume = {136},
pages = {109172},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109172},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322006513},
author = {Srishti Gautam and Marina M.-C. Höhne and Stine Hansen and Robert Jenssen and Michael Kampffmeyer},
keywords = {Self-explaining models, Explainable AI, Deep learning, Spurious Correlation Detection},
}

@inproceedings{acl/HaseB20,
  author    = {Peter Hase and
               Mohit Bansal},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  title     = {Evaluating Explainable {AI:} Which Algorithmic Explanations Help Users
               Predict Model Behavior?},
  booktitle = {Proc. Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages     = {5540--5552},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.acl-main.491},
  doi       = {10.18653/v1/2020.acl-main.491},
  timestamp = {Fri, 08 Jan 2021 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/conf/acl/HaseB20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{iccv/SelvarajuCDVPB17,
  author    = {Ramprasaath R. Selvaraju and
               Michael Cogswell and
               Abhishek Das and
               Ramakrishna Vedantam and
               Devi Parikh and
               Dhruv Batra},
  title     = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based
               Localization},
  booktitle = {{IEEE} International Conference on Computer Vision, {ICCV}},
  pages     = {618--626},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/ICCV.2017.74},
  doi       = {10.1109/ICCV.2017.74},
  timestamp = {Fri, 25 Dec 2020 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/SelvarajuCDVPB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{protopshare_rymarczyk_2021,
author = {Rymarczyk, Dawid and Struski, \L{}ukasz and Tabor, Jacek and Zieli\'{n}ski, Bartosz},
title = {{ProtoPShare: Prototypical Parts Sharing for Similarity Discovery in Interpretable Image Classification}},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467245},
doi = {10.1145/3447548.3467245},
booktitle = {Proc.  ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages = {1420–1430},
numpages = {11},
keywords = {interpretability, neural networks, prototypical parts, explainability},
location = {Virtual Event, Singapore},
}

@inproceedings{adebayo_sanity_2018,
 author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 title = {Sanity Checks for Saliency Maps},
 url = {https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf},
 year = {2018}
}

@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group}
}

@inproceedings{icml/GoyalWEBPL19,
  author    = {Yash Goyal and
               Ziyan Wu and
               Jan Ernst and
               Dhruv Batra and
               Devi Parikh and
               Stefan Lee},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Counterfactual Visual Explanations},
  booktitle = {Proc. Intl. Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume    = {97},
  pages     = {2376--2384},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/goyal19a.html},
  timestamp = {Tue, 11 Jun 2019 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/GoyalWEBPL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nips/LakkarajuL16,
  author    = {Himabindu Lakkaraju and
               Jure Leskovec},
  editor    = {Daniel D. Lee and
               Masashi Sugiyama and
               Ulrike von Luxburg and
               Isabelle Guyon and
               Roman Garnett},
  title     = {Confusions over Time: An Interpretable Bayesian Model to Characterize
               Trends in Decision Making},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {3261--3269},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/hash/97d0145823aeb8ed80617be62e08bdcc-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/LakkarajuL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cvpr/FongV18,
  author    = {Ruth Fong and
               Andrea Vedaldi},
  title     = {{Net2Vec: Quantifying and Explaining How Concepts Are Encoded by Filters
               in Deep Neural Networks}},
  booktitle = {Proc. {IEEE} Conference on Computer Vision and Pattern Recognition},
  pages     = {8730--8738},
  publisher = {{IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Fong\_Net2Vec\_Quantifying\_and\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00910},
  timestamp = {Fri, 25 Dec 2020 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/FongV18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nips/GhorbaniWZK19,
  author    = {Amirata Ghorbani and
               James Wexler and
               James Y. Zou and
               Been Kim},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {{Towards Automatic Concept-based Explanations}},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {9273--9282},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/GhorbaniWZK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{icdm/DasXDER20,
  author    = {Subhajit Das and
               Panpan Xu and
               Zeng Dai and
               Alex Endert and
               Liu Ren},
  editor    = {Giuseppe Di Fatta and
               Victor S. Sheng and
               Alfredo Cuzzocrea and
               Carlo Zaniolo and
               Xindong Wu},
  title     = {{Interpreting Deep Neural Networks through Prototype Factorization}},
  booktitle = {Proc. Intl. Conference on Data Mining Workshops, {ICDM} Workshops},
  pages     = {448--457},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/ICDMW51313.2020.00068},
  doi       = {10.1109/ICDMW51313.2020.00068},
  timestamp = {Sun, 21 Feb 2021 16:22:10 +0100},
  biburl    = {https://dblp.org/rec/conf/icdm/DasXDER20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{xue2022protopformer,
  title={ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition},
  author={Xue, Mengqi and Huang, Qihan and Zhang, Haofei and Cheng, Lechao and Song, Jie and Wu, Minghui and Song, Mingli},
  journal={arXiv preprint arXiv:2208.10431},
  year={2022}
}

@inproceedings{acl/JacoviG20,
  author    = {Alon Jacovi and
               Yoav Goldberg},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  title     = {Towards Faithfully Interpretable {NLP} Systems: How Should We Define
               and Evaluate Faithfulness?},
  booktitle = {Proc. Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages     = {4198--4205},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.acl-main.386},
  doi       = {10.18653/v1/2020.acl-main.386},
  timestamp = {Fri, 08 Jan 2021 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/conf/acl/JacoviG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{iclr/BorowskiZSGWBB21,
  author    = {Judy Borowski and
               Roland Simon Zimmermann and
               Judith Schepers and
               Robert Geirhos and
               Thomas S. A. Wallis and
               Matthias Bethge and
               Wieland Brendel},
  title     = {{Exemplary Natural Images Explain {CNN} Activations Better than State-of-the-Art
               Feature Visualization}},
  booktitle = {Proc. Intl. Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=QO9-y8also-},
  timestamp = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BorowskiZSGWBB21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @inproceedings{bagnet_iclr,
  author    = {Wieland Brendel and
               Matthias Bethge},
  title     = {{Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet}},
  booktitle = {Proc. Intl. Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=SkfMWhAqYQ},
  timestamp = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BrendelB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{huysmans_empirical_2011,
title = {An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models},
journal = {Decision Support Systems},
volume = {51},
number = {1},
year = {2011},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2010.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167923610002368},
author = {Johan Huysmans and Karel Dejaeger and Christophe Mues and Jan Vanthienen and Bart Baesens},
keywords = {Data mining, Classification, Knowledge representation, Comprehensibility, Decision tables},
abstract = {An important objective of data mining is the development of predictive models. Based on a number of observations, a model is constructed that allows the analysts to provide classifications or predictions for new observations. Currently, most research focuses on improving the accuracy or precision of these models and comparatively little research has been undertaken to increase their comprehensibility to the analyst or end-user. This is mainly due to the subjective nature of ‘comprehensibility’, which depends on many factors outside the model, such as the user's experience and his/her prior knowledge. Despite this influence of the observer, some representation formats are generally considered to be more easily interpretable than others. In this paper, an empirical study is presented which investigates the suitability of a number of alternative representation formats for classification when interpretability is a key requirement. The formats under consideration are decision tables, (binary) decision trees, propositional rules, and oblique rules. An end-user experiment was designed to test the accuracy, response time, and answer confidence for a set of problem-solving tasks involving the former representations. Analysis of the results reveals that decision tables perform significantly better on all three criteria, while post-test voting also reveals a clear preference of users for decision tables in terms of ease of use.}
}

@article{goyal2019explaining,
  title={Explaining classifiers with causal concept effect (cace)},
  author={Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been},
  journal={arXiv preprint arXiv:1907.07165},
  year={2019}
}

@incollection{doshi-velez_considerations_2018,
	address = {Cham},
	title = {Considerations for {Evaluation} and {Generalization} in {Interpretable} {Machine} {Learning}},
	isbn = {978-3-319-98130-7 978-3-319-98131-4},
	url = {http://link.springer.com/10.1007/978-3-319-98131-4_1},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a deﬁnitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
	urldate = {2019-02-20},
	booktitle = {Explainable and {Interpretable} {Models} in {Computer} {Vision} and {Machine} {Learning}},
	publisher = {Springer},
	author = {Doshi-Velez, Finale and Kim, Been},
	year = {2018},
	doi = {10.1007/978-3-319-98131-4_1},
	keywords = {definition}
}

@article{hoffman_metrics_2019,
	title = {Metrics for {Explainable} {AI}: {Challenges} and {Prospects}},
	shorttitle = {Metrics for {Explainable} {AI}},
	url = {http://arxiv.org/abs/1812.04608},
	abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
	urldate = {2020-04-17},
	journal = {arXiv:1812.04608 [cs]},
	author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
	month = feb,
	year = {2019},
	note = {arXiv: 1812.04608},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{kulesza_principles_2015,
author = {Kulesza, Todd and Burnett, Margaret and Wong, Weng-Keen and Stumpf, Simone},
title = {Principles of Explanatory Debugging to Personalize Interactive Machine Learning},
year = {2015},
isbn = {9781450333061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678025.2701399},
doi = {10.1145/2678025.2701399},
abstract = {How can end users efficiently influence the predictions that machine learning systems make on their behalf? This paper presents Explanatory Debugging, an approach in which the system explains to users how it made each of its predictions, and the user then explains any necessary corrections back to the learning system. We present the principles underlying this approach and a prototype instantiating it. An empirical evaluation shows that Explanatory Debugging increased participants' understanding of the learning system by 52\% and allowed participants to correct its mistakes up to twice as efficiently as participants using a traditional learning system.},
booktitle = {Proc. Intl. Conference on Intelligent User Interfaces},
pages = {126–137},
numpages = {12},
keywords = {interactive machine learning, end user programming},
location = {Atlanta, Georgia, USA},
}

@InProceedings{nauta_exproto_2021,
author="Nauta, Meike
and Jutte, Annemarie
and Provoost, Jesper
and Seifert, Christin",
title="This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition",
booktitle="Machine Learning and Principles and Practice of Knowledge Discovery in Databases",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="441--456",
isbn="978-3-030-93736-2"
}

@InProceedings{Wang_2021_ICCV_tesnet,
    author    = {Wang, Jiaqi and Liu, Huafeng and Wang, Xinyue and Jing, Liping},
    title     = {Interpretable Image Recognition by Constructing Transparent Embedding Space},
    booktitle = {Proc. IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {895-904}
}

@Incollection{montavon_gradient-based_2019,
author="Montavon, Gr{\'e}goire",
title="Gradient-Based Vs. Propagation-Based Explanations: An Axiomatic Comparison",
bookTitle="Explainable AI: Interpreting, Explaining and Visualizing Deep Learning",
year="2019",
publisher="Springer",
abstract="Deep neural networks, once considered to be inscrutable black-boxes, are now supplemented with techniques that can explain how these models decide. This raises the question whether the produced explanations are reliable. In this chapter, we consider two popular explanation techniques, one based on gradient computation and one based on a propagation mechanism. We evaluate them using three ``axiomatic'' properties: conservation, continuity, and implementation invariance. These properties are tested on the overall explanation, but also at intermediate layers, where our analysis brings further insights on how the explanation is being formed.",
isbn="978-3-030-28954-6",
doi="10.1007/978-3-030-28954-6_13",
url="https://doi.org/10.1007/978-3-030-28954-6_13"
}

@inproceedings{liu2021synthetic,
title={Synthetic Benchmarks for Scientific Research in Explainable Machine Learning},
author={Yang Liu and Sujay Khandagale and Colin White and Willie Neiswanger},
booktitle={NeurIPS Datasets and Benchmarks Track},
year={2021},
}

@inproceedings{nips/YehKALPR20,
  author    = {Chih{-}Kuan Yeh and
               Been Kim and
               Sercan {\"{O}}mer Arik and
               Chun{-}Liang Li and
               Tomas Pfister and
               Pradeep Ravikumar},
  title     = {On Completeness-aware Concept-Based Explanations in Deep Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/YehKALPR20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{barredo_arrieta_explainable_2020,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {1566-2535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {http://www.sciencedirect.com/science/article/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	language = {en},
	urldate = {2020-02-06},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	year = {2020},
	keywords = {Comprehensibility, Deep Learning, Machine Learning, Interpretability, Explainable Artificial Intelligence, Accountability, Data Fusion, Fairness, Privacy, Responsible Artificial Intelligence, Transparency},
}

@article{guidotti2018survey,
  title={A survey of methods for explaining black box models},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  journal={ACM computing surveys (CSUR)},
  volume={51},
  number={5},
  pages={1--42},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{biederman1987recognition,
  title={Recognition-by-components: a theory of human image understanding.},
  author={Biederman, Irving},
  journal={Psychological review},
  volume={94},
  number={2},
  pages={115},
  year={1987},
  publisher={American Psychological Association}
}

@ARTICLE{Singh2021-proto-on-chest-xray,
  author={Singh, Gurmail and Yow, Kin-Choong},
  journal={IEEE Access}, 
  title={An Interpretable Deep Learning Model for Covid-19 Detection With Chest X-Ray Images}, 
  year={2021},
  volume={9},
  number={},
  pages={85198-85208},
  doi={10.1109/ACCESS.2021.3087583}}

@ARTICLE{Singh2022-proto-on-chest_ctscan,
  title     = "Think positive: An interpretable neural network for image
               recognition",
  author    = "Singh, Gurmail",
  journal   = "Neural Netw.",
  publisher = "Elsevier BV",
  volume    =  151,
  pages     = "178--189",
  month     =  jul,
  year      =  2022,
  keywords  = "COVID-19; CT-scan; Interpretable; Pneumonia; Prototypes",
  language  = "en"
}

@article{bruckert2020next,
  title={The next generation of medical decision support: a roadmap toward transparent expert companions},
  author={Bruckert, Sebastian and Finzel, Bettina and Schmid, Ute},
  journal={Frontiers in artificial intelligence},
  volume={3},
  pages={507973},
  year={2020},
  publisher={Frontiers Media SA}
}

@inproceedings{wang2022knowledge,
  title={Knowledge Distillation to Ensemble Global and Interpretable Prototype-Based Mammogram Classification Models},
  author={Wang, Chong and Chen, Yuanhong and Liu, Yuyuan and Tian, Yu and Liu, Fengbei and McCarthy, Davis J and Elliott, Michael and Frazer, Helen and Carneiro, Gustavo},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2022: 25th International Conference, Singapore, Proceedings, Part III},
  pages={14--24},
  year={2022},
  organization={Springer}
}

@inproceedings{carloni_2022,
author={Carloni, Gianluca and Berti, Andrea and Pascali, Maria and Colantonio, Sara},
year={2022},
booktitle={Proceedings International Conference on Pattern Recognition (ICPR) - 2nd International Workshop on Artificial Intelligence for Healthcare Applications},
month={08},
pages={},
title={On the Applicability of Prototypical Part Learning in Medical Images: Breast Masses Classification Using ProtoPNet}
}

@inproceedings{mohammadjafari2021using,
  title={Using ProtoPNet for Interpretable Alzheimer's Disease Classification},
  author={Mohammadjafari, Sanaz and Cevik, Mucahit and Thanabalasingam, Mathusan and Basar, Ayse},
  booktitle={Canadian Conference on AI},
  year={2021}
}

@article{goodman2017european,
  title={European Union regulations on algorithmic decision-making and a “right to explanation”},
  author={Goodman, Bryce and Flaxman, Seth},
  journal={AI Magazine},
  volume={38},
  number={3},
  pages={50--57},
  year={2017}
}

@Article{zhou_evaluating_2021,
AUTHOR = {Zhou, Jianlong and Gandomi, Amir H. and Chen, Fang and Holzinger, Andreas},
TITLE = {Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {593},
ISSN = {2079-9292},
ABSTRACT = {The most successful Machine Learning (ML) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of ML explanation has experienced a surge in interest from the research community to application domains. While numerous explanation methods have been explored, there is a need for evaluations to quantify the quality of explanation methods to determine whether and to what extent the offered explainability achieves the defined objective, and compare available explanation methods and suggest the best explanation from the comparison for a specific task. This survey paper presents a comprehensive overview of methods proposed in the current literature for the evaluation of ML explanations. We identify properties of explainability from the review of definitions of explainability. The identified properties of explainability are used as objectives that evaluation metrics should achieve. The survey found that the quantitative metrics for both model-based and example-based explanations are primarily used to evaluate the parsimony/simplicity of interpretability, while the quantitative metrics for attribution-based explanations are primarily used to evaluate the soundness of fidelity of explainability. The survey also demonstrated that subjective measures, such as trust and confidence, have been embraced as the focal point for the human-centered evaluation of explainable systems. The paper concludes that the evaluation of ML explanations is a multidisciplinary research topic. It is also not possible to define an implementation of evaluation metrics, which can be applied to all explanation methods.},
DOI = {10.3390/electronics10050593}
}

@article{vilone2021notions,
title = {Notions of explainability and evaluation approaches for explainable artificial intelligence},
journal = {Information Fusion},
volume = {76},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001093},
author = {Giulia Vilone and Luca Longo},
keywords = {Explainable artificial intelligence, Notions of explainability, Evaluation methods}
}


@Article{clement2023,
AUTHOR = {Clement, Tobias and Kemmerzell, Nils and Abdelaal, Mohamed and Amberg, Michael},
TITLE = {XAIR: A Systematic Metareview of Explainable AI (XAI) Aligned to the Software Development Process},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {5},
YEAR = {2023},
NUMBER = {1},
PAGES = {78--108},
URL = {https://www.mdpi.com/2504-4990/5/1/6},
ISSN = {2504-4990},
ABSTRACT = {Currently, explainability represents a major barrier that Artificial Intelligence (AI) is facing in regard to its practical implementation in various application domains. To combat the lack of understanding of AI-based systems, Explainable AI (XAI) aims to make black-box AI models more transparent and comprehensible for humans. Fortunately, plenty of XAI methods have been introduced to tackle the explainability problem from different perspectives. However, due to the vast search space, it is challenging for ML practitioners and data scientists to start with the development of XAI software and to optimally select the most suitable XAI methods. To tackle this challenge, we introduce XAIR, a novel systematic metareview of the most promising XAI methods and tools. XAIR differentiates itself from existing reviews by aligning its results to the five steps of the software development process, including requirement analysis, design, implementation, evaluation, and deployment. Through this mapping, we aim to create a better understanding of the individual steps of developing XAI software and to foster the creation of real-world AI applications that incorporate explainability. Finally, we conclude with highlighting new directions for future research.},
DOI = {10.3390/make5010006}
}



