\documentclass[
        % lettersize,
        journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[caption=false,font=footnotesize,labelfont=rm,textfont=rm]{subfig}
% ======
\usepackage{color}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{stfloats}
\usepackage{amssymb}
\DeclareMathOperator{\st}{s.t.}
\usepackage{cleveref}

\usepackage{titlesec}

% 定义段落样式
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries}{}{0pt}{}[.\hspace{0.5em}\hspace{0.5em}]
\titlespacing{\paragraph}{0pt}{0pt}{0pt}
% \setlength{\parskip}{0pt}

% ======
% \hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{HeightFormer: Explicit Height Modeling \\
without Extra Data for Camera-only \\
3D Object Detection in Bird’s Eye View}


\author{
  
       Yiming~Wu, Ruixiang~Li, Zequn~Qin$^{*}$, Xinhai~Zhao, Xi~Li$^{*}$

        % ~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
% This code is offered as-is without any warranty either expressed or


\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem
Yiming~~Wu is with Polytechnic Institute, Zhejiang University, Hangzhou 310015, China.
Ruixiang~Li, Zequn~Qin, and Xi~Li are with College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China.
Xinhai~Zhao is with Noah's Ark Lab, Huawei Technologies, Shanghai 201206, China.
% J.~Gao is also with the School of Automation and Software Engineering, Shanxi University, Taiyuan 030013, China. 
E-mail: \{nolva,ruixli\}@zju.edu.cn, zequnqin@gmail.com, zhaoxinhai1@huawei.com, xilizju@zju.edu.cn.
\protect
}%see http://www.michaelshell.org/contact.html
%\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
\thanks{(Corresponding authors: Zequn~Qin and Xi~Li.)}}

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\input{chaps/0abstract.tex}


\begin{IEEEkeywords}
3D object detection, BEV perception, Height modeling.
\end{IEEEkeywords}


\input{chaps/1intro.tex}

\input{chaps/2related.tex}
\input{chaps/3method.tex}
\input{chaps/4exp.tex}
\input{chaps/5conclusion.tex}

{\appendix

\section*{Detailed derivation of $\delta_{d,max}$}

According to the analysis of depth prediction error in \cref{sec:equivalence}, if an object is correctly detected, its projected position $(x,z)$ in the BEV space should fall in the $\epsilon$-neighbourhood of its ground truth position $(x_{gt},z_{gt})$. That is to say:
    \begin{align}
        |x-x_{gt}|&+|z-z_{gt}| \le \epsilon, \label{eq:d1}\\
        \st 
        % \begin{aligned}
            \left[
                \begin{array}{c}x \\ y \\ z \end{array}
            \right] &=K^{-1}\left[
                \begin{array}{c}u_{gt}\cdot d \\ v_{gt} \cdot d \\ d \end{array}
            \right], \label{eq:d2}\\
            \left[
                \begin{array}{c}x_{gt} \\ y_{gt} \\ z_{gt} \end{array}
            \right]   &=K^{-1}\left[
                \begin{array}{c}u_{gt}\cdot d_{gt} \\ v_{gt} \cdot d_{gt} \\ d_{gt} \end{array}
            \right], \label{eq:d3} \\
            K &= \left[
        \begin{array}{ccc}
            f_x & 0   & u_0 \\
            0   & f_y & v_0 \\
            0   & 0   & 1
        \end{array}
        \right],
        % \end{aligned}
        \label{eq:d4}
    \end{align}
where $(u_{gt}, v_{gt})$ is the position of the object in the image.
$d$ is the given depth at $(u_{gt}, v_{gt})$. The feature at $(u_{gt}, v_{gt})$ is gathered into the BEV grid at $(x,z)$. $K$ is the camera's intrinsic matrix which does the transformation between the image frame and the BEV frame.$\footnote{Theoretically, there is a camera frame other than the BEV frame. Without losing generality, this manuscript does not distinguish between the camera frame and the BEV frame, because we assume that the extrinsic parameter matrix %which does transformation between the two frames 
is an identity matrix.}$
Expanding \cref{eq:d2}, \cref{eq:d3} and \cref{eq:d4}, and we get
\begin{align}
    x-x_{gt} &= \frac{1}{f_x}(u_{gt}-u_0)(d-d_{gt}),                      \\
    z-z_{gt} &= d-d_{gt}.
\end{align}
Substituting them into \cref{eq:d1}, and we can solve out the upper bound of $|d-d_{gt}|$ as follows:
\begin{equation}
    \delta_{d,max}=\epsilon \cdot \frac{f_x}{|u_{gt}-u_0|+f_x},
\end{equation}
which is described in the \cref{eq:deptherror}.


\section*{Detailed derivation of $\delta_{y,max}$}

\label{sec:intro}
According to the analysis of height prediction error in \cref{sec:equivalence}, if an object is correctly detected, the sampling locations should cover the ground truth position of the object's feature. This is concluded as:
\begin{equation}
    \begin{aligned}
        (u_{gt},&v_{gt})^T\in S_{\epsilon},                                  \\
        \st\; S_{\epsilon}\triangleq\Bigg\{ (u,v)^T&=\frac{1}{z}\left[
            \begin{array}{ccc}
                f_x & 0   & u_0 \\
                0   & f_y & v_0 \\
            \end{array}
        \right]\cdot         
        \left[ \begin{array}{c}x \\
                       y% y_{gt}\pm \delta_{y}(x,z) 
                       \\
                       z\end{array}\right] \\
        & \bigg|                \; |x-x_{gt}|+|z-z_{gt}| \le \epsilon \Bigg\}.
    \end{aligned}
    \label{eq:prob}
\end{equation}
The above problem is described in \cref{eq:hpe}. $(u_{gt},v_{gt})$ is the ground truth position of an object in the image frame, while $(x_{gt},y_{gt},z_{gt})$ is the ground truth position of the object in the BEV frame.  $S_\epsilon$ is the sampling location set corresponding to the $\epsilon$-neighbourhood of $(x_{gt},z_{gt})$. $y$ is the predicted height at $(x,z)$. Expanding \cref{eq:prob}, we get two equations and one inequality:
\begin{align}
    u_{gt}z=    & f_x x+u_0 z,                      \\
    v_{gt}z=    & f_y y+v_0 z,                      \\
    |x-x_{gt}|+ & |z-z_{gt}|\le\epsilon,\label{ieq}
\end{align}
where $y$ is an abbreviation for $y(x,z)$. According to the transformation between the image frame and the BEV frame:
\begin{align}
    u_{gt}z_{gt}=f_x x_{gt}+u_0 z_{gt}, \\
    v_{gt}z_{gt}=f_y y_{gt}+v_0 z_{gt},
\end{align} we can conclude the relationships among $x-x_{gt}$, $y-y_{gt}$, and $z-z_{gt}$. They follow:
\begin{align}
    (u_{gt}-u_0)(z-z_{gt})=f_x(x-x_{gt}),\label{e1} \\
    (v_{gt}-v_0)(z-z_{gt})=f_y(y-y_{gt}).\label{e2}
\end{align}
Substituting \cref{e1} and \cref{e2} into \cref{ieq}, we can get an inequality about $y$:
\begin{equation}
    \frac{f_y}{f_x}\cdot\frac{|u_{gt}-u_0|}{|v_{gt}-v_0|}|y-y_{gt}|+\frac{f_y}{|v_{gt}-v_0|}|y-y_{gt}| \le \epsilon.
    \label{e3}
\end{equation}
Substituting $y=y_{gt}\pm \delta_y$ into \cref{e3}, we can get the inequality about the height prediction error:
\begin{equation}
    \delta_y\le\epsilon\cdot\frac{|v_{gt}-v_0|}{f_y}\cdot \frac{f_x}{|u_{gt}-u_0|+f_x}.
\end{equation}
As a result, we get the upper bound of the height prediction error:
\begin{equation}
    \delta_{y,max}=\epsilon\cdot\frac{|v_{gt}-v_0|}{f_y}\cdot \frac{f_x}{|u_{gt}-u_0|+f_x},
\end{equation}
which is described in \cref{eq:heighterror}.

}

\bibliographystyle{IEEEtran}
\bibliography{main}




\end{document}


