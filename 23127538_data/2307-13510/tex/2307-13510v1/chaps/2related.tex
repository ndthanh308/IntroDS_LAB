\section{Related work}

% start of Related work
\paragraph{BEV-based camera-only 3D object detection} Most recent works in camera-only 3D object detection operate in 3D space. And there are two main methods of mapping 2D features into 3D space, which are pseudo-lidar methods\cite{wang2019pseudo, you2019pseudo, ma2020rethinking} and voxel-based BEV methods\cite{roddick2018orthographic, lang2019pointpillars, yin2021center}. Due to the efficient representation of voxels, BEV methods are commonly used now. 
%An intuitive method is to use a geometry-based method: Inverse Perspective Mapping(IPM)\cite{mallot1991inverse}.% 
The key challenge of BEV-based methods is to construct the BEV space with multi-view images. OFT\cite{roddick2018orthographic} utilizes orthographic transformation to map monocular 2D features into BEV space. LSS\cite{philion2020lift} predicts depth distribution in image space and lifts 2D features to 3D space by outer product. Following LSS, CaDDN\cite{reading2021categorical} and BEVDet\cite{huang2021bevdet} utilize the depth distribution to construct the BEV representation. 

Due to the success of transformer\cite{vaswani2017attention,brown2020language,liu2021swin}, mainstream works deal with detection tasks with a transformer-like pipeline. DETR3D\cite{wang2022detr3d} follows DETR\cite{carion2020end} to generate 3D reference points from queries. To simplify the feature sampling process in DETR3D, PETR\cite{liu2022petr,liu2022petrv2} proposes 3D positional embedding and adds the temporal information to 3D PE to align different frames. 
BEVDet\cite{huang2021bevdet} uses LSS\cite{philion2020lift} method in the BEV encoder and proposes specific data augmentation and scale-NMS tricks to improve the performance. BEVFormer\cite{li2022bevformer} introduces spatial cross-attention and temporal self-attention to generate spatiotemporal grid features from history BEV information and multi-view scenes. 
To fit the nature of the ego car's perspective, PolarFormer\cite{jiang2022polarformer} advocates the exploitation of the polar coordinate system. SOLOFusion\cite{park2022time} designs an efficient but strong temporal multi-view 3D detector to leverage long-term temporal information. TBP-Former\cite{fang2023tbp} proposes a temporal BEV pyramid transformer for spatial-temporal synchronization and BEV states prediction.


\paragraph{Depth estimation} Depth estimation is essential for 3D object detection. Due to the high similarity between depth estimation and height estimation, we learn from the experiences in depth estimation. Early works focus on geometry-based methods for stereo images\cite{scharstein2002taxonomy, flynn2016deepstereo}. In the monocular situation, there are two mainstream ways to estimate depth, which are direct regression by a well-designed network\cite{eigen2014depth, fu2018deep, xu2018multi, ding2020learning} and geometry depth derived from the pinhole imaging model\cite{cai2020monocular}. 

For the first way, many works\cite{chen2020monopair, qin2022monoground, wang2022probabilistic} adopt uncertainty to get accurate depth. Following the practice of MonoPair\cite{chen2020monopair}, most works assume that the depth follows a Laplacian distribution, and they regress it with L1 loss. 
For example, MonoFlex\cite{zhang2021objects} designs an adaptive ensemble of estimators to predict the depth of paired diagonal key points. 
% MonoRUn\cite{chen2021monorun} uses a mixed KL loss to mitigate the issue that L1 loss is not differentiable at some points.
In multi-view task, BEVDepth\cite{li2022bevdepth} proposes that the final detection loss establishes an implicit depth supervision and utilizes an explicit depth supervision with LiDAR points to enable a trustworthy depth estimation.

For the second way, due to the geometric relationship between heights and depths, many works try to model heights to estimate depths. GUPNet\cite{lu2021geometry} models the height distribution and uses 3D height $h_{3d}$ and 2D height $h_{2d}$ to express depth, which is easier than regressing depth directly. MonoRCNN\cite{shi2021geometry} decomposes distances into physical heights and the reciprocals of projected visual heights.

% Height estimation in BEV is similar to depth estimation in images, so some designs in this work are inspired by depth estimation. As the first work to
% % solve the ill-posed 2D to 3D mapping problem by explicitly modeling heights in BEV, 
% explicitly model heights in BEV, 

% % end of related work
{{\color{blue}}
\paragraph{Height modeling} Compared to depth modeling, constructing BEV features via height modeling is adopted by fewer works. BEVFormer\cite{li2022bevformer} and PolarFormer\cite{chen2022polar} obtain BEV queries by sampling 3D points around fixed heights and assigning to them different attention weights, which is an implicit way of height modeling. BEVHeight\cite{yang2023bevheight} proposes to model object heights by predicting heights for image pixels and then lifts image features to BEV space via geometric transformation, which leverages the high camera height and few occlusions in roadside situations. 

Different from previous works, we explicitly estimate heights in BEV space and focus on car-side situations, in which cameras are mounted on the car roof and occlusion is common. This work will also prove the equivalence between the height-based solution and the depth-based solution for constructing the BEV representation.}