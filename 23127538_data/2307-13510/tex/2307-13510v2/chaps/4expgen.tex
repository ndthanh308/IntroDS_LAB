\subsection{LiDAR supervision}
% Figure environment removed

 Ground truth heights are critical for the HeightFormer module. However, with the bounding boxes of objects, we can only obtain the heights of grids that have objects, which limits the performance of the proposed method. To show how much further improvement can be achieved with extra LiDAR supervision, we introduce LiDAR supervision in this part.

First, LiDAR points are projected into BEV grids with the height range meshed into 16 intervals. For each BEV grid, its height $y_{xz}$ is the height of the interval containing the most points. Its lower bound is the height of the lowest point, which formulates $y_{xz} - h_{xz} / 2$. Furthermore, heights from LiDAR points and heights from bounding boxes are fused to formulate the ground truth heights. For each BEV gird, the height from ground truth bounding boxes is preferred. The fused heights serving as the ground truth heights are shown in \cref{fig:fused}.

With the extra supervision of LiDAR information, the performance is improved by 0.3 percentage points. The ablation study is shown in \cref{tab:explidar}. 

\begin{table}[htb]
    \centering
    \caption{Ablation study on LiDAR supervision. The models here take ResNet50 as the backbone and the BEV space is meshed into 200 grids by 200 grids. }
    \begin{tabular}{lccc}
        \toprule
        Model     & Height           & NDS$\uparrow$ & mAP$\uparrow$ \\
        \midrule
        BEVFormer & implicit         & 0.403         & 0.288         \\
        % \midrule
        HeightFormer & explicit         & 0.421         & 0.299         \\
        HeightFormer & explicit + lidar & {\bf 0.428}         & {\bf 0.307}         \\
        \bottomrule
    \end{tabular}
    \label{tab:explidar}
\end{table}

We can note the extra LiDAR supervision brings little improvement to the performance of the proposed method compared with the improvement brought by explicit height modeling. \uwave{A potential reason is that LiDAR points are sparse and occluded positions are not covered. This might yield inconsistent supervision information. This is different from the situation of depth-modeling, in which most pixels can be paired with one or more LiDAR points. However, the advantage of our method is that, it is a cost-free approach for improving the performance and requires no extra data.}

\subsection{Generalization ability}
In this part, we will show that the proposed HeightFormer can also serve as a plugin to refine other types of BEV representations. The pipeline is shown in \cref{fig:refine}.

% Figure environment removed


Taking BEVDepth as an example, it lifts image features with predicted depth distribution into voxel features and splats voxel features into BEV features with voxel pooling. Here we make HeightFormer a plugin of BEVDepth and improve the performance of BEVDepth with the plugin. The results are shown in \cref{tab:expgen}.

\begin{table}[htb]
    \centering
    \caption{Performance of height-based BEV feature refinement. The baseline models both take ResNet50 as the backbone.}
    \begin{tabular}{lccc}
        \toprule
        Model     & Refinement & NDS$\uparrow$ & mAP$\uparrow$ \\
        \midrule
        BEVFormer &            & 0.403         & 0.288         \\
        BEVFormer & 3 layers & {\bf 0.421}         & {\bf 0.299}         \\
        \midrule
        BEVDepth  &            & 0.366         & {\bf 0.273}             \\
        % \midrule
        BEVDepth  & 1 layer & {\bf 0.369}         & {\bf 0.273}             \\
        \bottomrule
    \end{tabular}
    \label{tab:expgen}
\end{table}

We first train a BEVDepth model without BEV data augmentation for 20 epochs. Its best performance is 0.366 in NDS. We insert a single layer of the HeightFormer plugin into BEVDepth and train the new model with the same settings. The new model's best performance is 0.369 in NDS. The NDS is improved by 0.3 percentage points, which shows that the HeightFormer plugin effectively improves the BEV representations and thus improves the performance.