\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{ulem}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[caption=false,font=footnotesize,labelfont=rm,textfont=rm]{subfig}
% ======
\usepackage{color}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{stfloats}
\usepackage{amssymb}
\DeclareMathOperator{\st}{s.t.}
\usepackage{cleveref}

\usepackage{titlesec}
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries}{}{0pt}{}[.\hspace{0.5em}\hspace{0.5em}]
\titlespacing{\paragraph}{0pt}{0pt}{0pt}
% \setlength{\parskip}{0pt}

% ======
% updated with editorial comments 8/9/2021
\renewcommand{\uwave}{}
\begin{document}
\title{HeightFormer: Explicit Height Modeling \\
without Extra Data for Camera-only \\
3D Object Detection in Bird’s Eye View}


\author{
  
       Yiming~Wu$^\dag$, Ruixiang~Li$^\dag$, Zequn~Qin$^{*}$, Xinhai~Zhao, Xi~Li$^{*}$

\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem
Yiming~~Wu is with Polytechnic Institute, Zhejiang University, Hangzhou 310015, China.
Ruixiang~Li, Zequn~Qin, and Xi~Li are with College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China.
Xinhai~Zhao is with Noah's Ark Lab, Huawei Technologies, Shanghai 201206, China.
E-mail: \{nolva,ruixli\}@zju.edu.cn, zequnqin@gmail.com, zhaoxinhai1@huawei.com, xilizju@zju.edu.cn.
\protect
}

\thanks{($\dag$: Equal contributions. Corresponding authors: Zequn~Qin and Xi~Li.)}}

\maketitle

% \markboth{IEEE TRANSACTIONS ON IMAGE PROCESSING}{HeightFormer: Explicit Height Modeling \\
% without Extra Data for Camera-only \\
% 3D Object Detection in Bird’s Eye View}

\input{chaps/0abstract.tex}


\begin{IEEEkeywords}
3D object detection, BEV perception, Height modeling.
\end{IEEEkeywords}


\input{chaps/1intro.tex}

\input{chaps/2related.tex}
\input{chaps/3method.tex}
\input{chaps/4exp.tex}
\input{chaps/5conclusion.tex}

{\appendix

\section*{Detailed derivation of $\delta_{d,max}$}

According to the analysis of depth prediction error in \cref{sec:equivalence}, if an object is correctly detected, its projected position $(x,z)$ in the BEV space should fall in the $\epsilon$-neighbourhood of its ground truth position $(x_{gt},z_{gt})$. That is to say:
    \begin{align}
        |x-x_{gt}|&+|z-z_{gt}| \le \epsilon, \label{eq:d1}\\
        \st 
        % \begin{aligned}
            \left[
                \begin{array}{c}x \\ y \\ z \end{array}
            \right] &=K^{-1}\left[
                \begin{array}{c}u_{gt}\cdot d \\ v_{gt} \cdot d \\ d \end{array}
            \right], \label{eq:d2}\\
            \left[
                \begin{array}{c}x_{gt} \\ y_{gt} \\ z_{gt} \end{array}
            \right]   &=K^{-1}\left[
                \begin{array}{c}u_{gt}\cdot d_{gt} \\ v_{gt} \cdot d_{gt} \\ d_{gt} \end{array}
            \right] \label{eq:d3} \\
        %     K &= \left[
        % \begin{array}{ccc}
        %     f_x & 0   & u_0 \\
        %     0   & f_y & v_0 \\
        %     0   & 0   & 1
        % \end{array}
        % \right],
        % \end{aligned}
        \label{eq:d4}
    \end{align}
where $(u_{gt}, v_{gt})$ is the position of the object in the image.
$d$ is the given depth at $(u_{gt}, v_{gt})$. The feature at $(u_{gt}, v_{gt})$ is gathered into the BEV grid at $(x,z)$. $K$ is the camera's intrinsic matrix which does the transformation between the image frame and the BEV frame as stated in \cref{eq:intrinsic}.$\footnote{Theoretically, there is a camera frame other than the BEV frame. Without losing generality, this manuscript does not distinguish between the camera frame and the BEV frame, because we assume that the extrinsic parameter matrix %which does transformation between the two frames 
is an identity matrix.}$
Expanding \cref{eq:d2}, \cref{eq:d3} and \cref{eq:d4}, and we get
\begin{align}
    x-x_{gt} &= \frac{1}{f_x}(u_{gt}-u_0)(d-d_{gt}),                      \\
    z-z_{gt} &= d-d_{gt}.
\end{align}
Substituting them into \cref{eq:d1}, and we can solve out the upper bound of $|d-d_{gt}|$ as follows:
\begin{equation}
    \delta_{d,max}=\epsilon \cdot \frac{f_x}{|u_{gt}-u_0|+f_x},
\end{equation}
which is described in the \cref{eq:deptherror}.


\section*{Detailed derivation of $\delta_{y,max}$}

\label{sec:intro}
According to the analysis of height prediction error in \cref{sec:equivalence}, if an object is correctly detected, the sampling locations should cover the ground truth position of the object's feature. This is concluded as:
\begin{equation}
    \begin{aligned}
        (u_{gt},&v_{gt})^T\in S_{\epsilon},                                  \\
        \st\; S_{\epsilon}\triangleq\Bigg\{ (u,v)^T&=\frac{1}{z}\left[
            \begin{array}{ccc}
                f_x & 0   & u_0 \\
                0   & f_y & v_0 \\
            \end{array}
        \right]\cdot         
        \left[ \begin{array}{c}x \\
                       y% y_{gt}\pm \delta_{y}(x,z) 
                       \\
                       z\end{array}\right] \\
        & \bigg|                \; |x-x_{gt}|+|z-z_{gt}| \le \epsilon \Bigg\}.
    \end{aligned}
    \label{eq:prob}
\end{equation}
The above problem is described in \cref{eq:hpe}. $(u_{gt},v_{gt})$ is the ground truth position of an object in the image frame, while $(x_{gt},y_{gt},z_{gt})$ is the ground truth position of the object in the BEV frame.  $S_\epsilon$ is the sampling location set corresponding to the $\epsilon$-neighbourhood of $(x_{gt},z_{gt})$. $y$ is the predicted height at $(x,z)$. Expanding \cref{eq:prob}, we get two equations and one inequality:
\begin{align}
    u_{gt}z=    & f_x x+u_0 z,                      \\
    v_{gt}z=    & f_y y+v_0 z,                      \\
    |x-x_{gt}|+ & |z-z_{gt}|\le\epsilon,\label{ieq}
\end{align}
where $y$ is an abbreviation for $y(x,z)$. According to the transformation between the image frame and the BEV frame:
\begin{align}
    u_{gt}z_{gt}=f_x x_{gt}+u_0 z_{gt}, \\
    v_{gt}z_{gt}=f_y y_{gt}+v_0 z_{gt},
\end{align} we can conclude the relationships among $x-x_{gt}$, $y-y_{gt}$, and $z-z_{gt}$. They follow:
\begin{align}
    (u_{gt}-u_0)(z-z_{gt})=f_x(x-x_{gt}),\label{e1} \\
    (v_{gt}-v_0)(z-z_{gt})=f_y(y-y_{gt}).\label{e2}
\end{align}
Substituting \cref{e1} and \cref{e2} into \cref{ieq}, we can get an inequality about $y$:
\begin{equation}
    \frac{f_y}{f_x}\cdot\frac{|u_{gt}-u_0|}{|v_{gt}-v_0|}|y-y_{gt}|+\frac{f_y}{|v_{gt}-v_0|}|y-y_{gt}| \le \epsilon.
    \label{e3}
\end{equation}
Substituting $y=y_{gt}\pm \delta_y$ into \cref{e3}, we can get the inequality about the height prediction error:
\begin{equation}
    \delta_y\le\epsilon\cdot\frac{|v_{gt}-v_0|}{f_y}\cdot \frac{f_x}{|u_{gt}-u_0|+f_x}.
\end{equation}
As a result, we get the upper bound of the height prediction error:
\begin{equation}
    \delta_{y,max}=\epsilon\cdot\frac{|v_{gt}-v_0|}{f_y}\cdot \frac{f_x}{|u_{gt}-u_0|+f_x},
\end{equation}
which is described in \cref{eq:heighterror}.

}

\section*{Comparison to BEVHeight}

In this appendix, we provide a detailed comparison between the proposed method and BEVHeight\cite{yang2023bevheight}, which also constructs BEV features via height modeling.

% Figure environment removed

\begin{enumerate}
    \item {\bf Modeling:} BEVHeight models s for each image pixel, and then projects image features into the BEV space. This is {\bf LSS-style}\cite{philion2020lift} modeling, and it can be turned into depth modeling by mapping height bins into depth bins. In contrast, HeightFormer adopts {\bf OFT-style}\cite{roddick2018orthographic} modeling. BEV features are gathered from image features by projecting reference points into images.
    
    \item {\bf Scenario:} BEVHeight is primarily designed for roadside 3D object detection tasks, whereas HeightFormer is tailored for vehicle-mounted scenarios. BEVHeight operates optimally with {cameras with high installation}, as highlighted in its paper. The proposed method has no such requirements as it models heights with BEV features which have historical information about objects.

    \item {\bf Height:} BEVHeight focuses on modeling {\bf surface heights} relative to the { flat ground}, while HeightFormer characterizes both {\bf the center and the range} of an object along the height axis.

    \item {\bf Implementation:} BEVHeight employs a strategy of dividing heights into {\bf discrete} bins and performing classifications for each image pixel. In contrast, HeightFormer utilizes Laplacian priors to model heights in {\bf contiguous} space and conducts height regression for each BEV grid.
\end{enumerate}


\begin{table}[h]
    \centering
    \caption{Comparative experiments with BEVHeight. $\dag$: Official results, single frame model. $*$: Explicit height modeling without other designs or tricks.}
    \begin{tabular}{ccc}
    \toprule
     Model    &   NDS & mAP  \\
     \midrule
      BEVFormer & 0.403 & 0.288  \\
      BEVHeight$^\dag$ & 0.342 & 0.291  \\
      HeightFormer$^*$  & 0.421 & 0.299 \\
      \bottomrule
    \end{tabular}
    \label{tab:vs}
\end{table}

Furthermore, we compare the performance. The \cref{tab:vs} shows the comparative experiments with BEVHeight. Because BEVHeight does not use history frames and its code does not contain training configurations on NuScenes, we compare mAP only. The proposed method outperformed BEVHeight by 0.8 percentage points.


% \section*{Acknowledgment}
% This work is supported by National Science Foundation for Distinguished Young Scholars under Grant 62225605, Zhejiang Provincial Natural Science Foundation of China under Grant LD24F020016, National Natural Science Foundation of China under Grant U20A20222, the Ningbo Science and Technology Innovation Project (No.2024Z294), Zhejiang Key Research and Development Program under Grant 2023C03196.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,main}

% \input{chaps/6profile.tex}
\end{document}


