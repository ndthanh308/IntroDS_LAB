% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{lu2021graph}
P.~Lu, S.~Xu, and H.~Peng, ``Graph-embedded lane detection,'' \emph{IEEE TIP},
  vol.~30, pp. 2977--2988, 2021.

\bibitem{xie2023x}
L.~Xie, G.~Xu, D.~Cai, and X.~He, ``X-view: non-egocentric multi-view 3d object
  detector,'' \emph{IEEE TIP}, vol.~32, pp. 1488--1497, 2023.

\bibitem{yang2023bevformer}
C.~Yang, Y.~Chen, H.~Tian, C.~Tao, X.~Zhu, Z.~Zhang, G.~Huang, H.~Li, Y.~Qiao,
  L.~Lu \emph{et~al.}, ``Bevformer v2: Adapting modern image backbones to
  bird's-eye-view recognition via perspective supervision,'' in \emph{CVPR},
  2023, pp. 17\,830--17\,839.

\bibitem{bartoccioni2023lara}
F.~Bartoccioni, {\'E}.~Zablocki, A.~Bursuc, P.~P{\'e}rez, M.~Cord, and
  K.~Alahari, ``Lara: Latents and rays for multi-camera bird's-eye-view
  semantic segmentation,'' in \emph{CoRL}, 2023, pp. 1663--1672.

\bibitem{lin2022sparse4d}
X.~Lin, T.~Lin, Z.~Pei, L.~Huang, and Z.~Su, ``Sparse4d: Multi-view 3d object
  detection with sparse spatial-temporal fusion,'' \emph{arXiv preprint
  arXiv:2211.10581}, 2022.

\bibitem{10557672}
Y.~Li, B.~Huang, Z.~Chen, Y.~Cui, F.~Liang, M.~Shen, F.~Liu, E.~Xie, L.~Sheng,
  W.~Ouyang, and J.~Shao, ``Fast-bev: A fast and strong bird's-eye view
  perception baseline,'' \emph{IEEE TPAMI}, pp. 1--14, 2024.

\bibitem{10473113}
W.~Liu, Q.~Li, W.~Yang, J.~Cai, Y.~Yu, Y.~Ma, S.~He, and J.~Pan, ``Monocular
  bev perception of road scenes via front-to-top view projection,'' \emph{IEEE
  TPAMI}, pp. 1--17, 2024.

\bibitem{philion2020lift}
J.~Philion and S.~Fidler, ``Lift, splat, shoot: Encoding images from arbitrary
  camera rigs by implicitly unprojecting to 3d,'' in \emph{ECCV}, 2020, pp.
  194--210.

\bibitem{roddick2018orthographic}
T.~Roddick, A.~Kendall, and R.~Cipolla, ``Orthographic feature transform for
  monocular 3d object detection,'' \emph{arXiv preprint arXiv:1811.08188},
  2018.

\bibitem{park2021pseudo}
D.~Park, R.~Ambrus, V.~Guizilini, J.~Li, and A.~Gaidon, ``Is pseudo-lidar
  needed for monocular 3d object detection?'' in \emph{ICCV}, 2021, pp.
  3142--3152.

\bibitem{li2022bevdepth}
Y.~Li, Z.~Ge, G.~Yu, J.~Yang, Z.~Wang, Y.~Shi, J.~Sun, and Z.~Li, ``Bevdepth:
  Acquisition of reliable depth for multi-view 3d object detection,'' in
  \emph{AAAI}, vol.~37, no.~2, 2023, pp. 1477--1485.

\bibitem{nuscenes2019}
H.~Caesar, V.~Bankiti, A.~H. Lang, S.~Vora, V.~E. Liong, Q.~Xu, A.~Krishnan,
  Y.~Pan, G.~Baldan, and O.~Beijbom, ``nuscenes: A multimodal dataset for
  autonomous driving,'' \emph{arXiv preprint arXiv:1903.11027}, 2019.

\bibitem{wang2019pseudo}
Y.~Wang, W.-L. Chao, D.~Garg, B.~Hariharan, M.~Campbell, and K.~Q. Weinberger,
  ``Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object
  detection for autonomous driving,'' in \emph{CVPR}, 2019, pp. 8445--8453.

\bibitem{you2019pseudo}
Y.~You, Y.~Wang, W.-L. Chao, D.~Garg, G.~Pleiss, B.~Hariharan, M.~Campbell, and
  K.~Q. Weinberger, ``Pseudo-lidar++: Accurate depth for 3d object detection in
  autonomous driving,'' in \emph{ICLR}, 2020.

\bibitem{ma2020rethinking}
X.~Ma, S.~Liu, Z.~Xia, H.~Zhang, X.~Zeng, and W.~Ouyang, ``Rethinking
  pseudo-lidar representation,'' in \emph{ECCV}, 2020, pp. 311--327.

\bibitem{lang2019pointpillars}
A.~H. Lang, S.~Vora, H.~Caesar, L.~Zhou, J.~Yang, and O.~Beijbom,
  ``Pointpillars: Fast encoders for object detection from point clouds,'' in
  \emph{CVPR}, 2019, pp. 12\,697--12\,705.

\bibitem{yin2021center}
T.~Yin, X.~Zhou, and P.~Krahenbuhl, ``Center-based 3d object detection and
  tracking,'' in \emph{CVPR}, 2021, pp. 11\,784--11\,793.

\bibitem{reading2021categorical}
C.~Reading, A.~Harakeh, J.~Chae, and S.~L. Waslander, ``Categorical depth
  distribution network for monocular 3d object detection,'' in \emph{CVPR},
  2021, pp. 8555--8564.

\bibitem{huang2021bevdet}
J.~Huang, G.~Huang, Z.~Zhu, and D.~Du, ``Bevdet: High-performance multi-camera
  3d object detection in bird-eye-view,'' \emph{arXiv preprint
  arXiv:2112.11790}, 2021.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{NeurIPS}, vol.~30, 2017.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{NeurIPS}, vol.~33, pp. 1877--1901,
  2020.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in
  \emph{ICCV}, 2021, pp. 10\,012--10\,022.

\bibitem{wang2022detr3d}
Y.~Wang, V.~C. Guizilini, T.~Zhang, Y.~Wang, H.~Zhao, and J.~Solomon, ``Detr3d:
  3d object detection from multi-view images via 3d-to-2d queries,'' in
  \emph{CoRL}, 2022, pp. 180--191.

\bibitem{carion2020end}
N.~Carion, F.~Massa, G.~Synnaeve, N.~Usunier, A.~Kirillov, and S.~Zagoruyko,
  ``End-to-end object detection with transformers,'' in \emph{ECCV}, 2020, pp.
  213--229.

\bibitem{liu2022petr}
Y.~Liu, T.~Wang, X.~Zhang, and J.~Sun, ``Petr: Position embedding
  transformation for multi-view 3d object detection,'' in \emph{ECCV}, 2022.

\bibitem{liu2022petrv2}
Y.~Liu, J.~Yan, F.~Jia, S.~Li, A.~Gao, T.~Wang, and X.~Zhang, ``Petrv2: A
  unified framework for 3d perception from multi-camera images,'' in
  \emph{ICCV}, 2023, pp. 3262--3272.

\bibitem{li2022bevformer}
Z.~Li, W.~Wang, H.~Li, E.~Xie, C.~Sima, T.~Lu, Y.~Qiao, and J.~Dai,
  ``Bevformer: Learning birdâ€™s-eye-view representation from multi-camera
  images via spatiotemporal transformers,'' in \emph{ECCV}, 2022.

\bibitem{jiang2022polarformer}
Y.~Jiang, L.~Zhang, Z.~Miao, X.~Zhu, J.~Gao, W.~Hu, and Y.-G. Jiang,
  ``Polarformer: Multi-camera 3d object detection with polar transformer,'' in
  \emph{AAAI}, vol.~37, no.~1, 2023, pp. 1042--1050.

\bibitem{park2022time}
J.~Park, C.~Xu, S.~Yang, K.~Keutzer, K.~Kitani, M.~Tomizuka, and W.~Zhan,
  ``Time will tell: New outlooks and a baseline for temporal multi-view 3d
  object detection,'' \emph{arXiv preprint arXiv:2210.02443}, 2022.

\bibitem{fang2023tbp}
S.~Fang, Z.~Wang, Y.~Zhong, J.~Ge, and S.~Chen, ``Tbp-former: Learning temporal
  bird's-eye-view pyramid for joint perception and prediction in vision-centric
  autonomous driving,'' in \emph{CVPR}, 2023, pp. 1368--1378.

\bibitem{scharstein2002taxonomy}
D.~Scharstein and R.~Szeliski, ``A taxonomy and evaluation of dense two-frame
  stereo correspondence algorithms,'' \emph{IJCV}, vol.~47, no.~1, pp. 7--42,
  2002.

\bibitem{flynn2016deepstereo}
J.~Flynn, I.~Neulander, J.~Philbin, and N.~Snavely, ``Deepstereo: Learning to
  predict new views from the world's imagery,'' in \emph{CVPR}, 2016, pp.
  5515--5524.

\bibitem{eigen2014depth}
D.~Eigen, C.~Puhrsch, and R.~Fergus, ``Depth map prediction from a single image
  using a multi-scale deep network,'' \emph{NeurIPS}, vol.~27, 2014.

\bibitem{fu2018deep}
H.~Fu, M.~Gong, C.~Wang, K.~Batmanghelich, and D.~Tao, ``Deep ordinal
  regression network for monocular depth estimation,'' in \emph{CVPR}, 2018,
  pp. 2002--2011.

\bibitem{xu2018multi}
B.~Xu and Z.~Chen, ``Multi-level fusion based 3d object detection from
  monocular images,'' in \emph{CVPR}, 2018, pp. 2345--2353.

\bibitem{ding2020learning}
M.~Ding, Y.~Huo, H.~Yi, Z.~Wang, J.~Shi, Z.~Lu, and P.~Luo, ``Learning
  depth-guided convolutions for monocular 3d object detection,'' in
  \emph{CVPRW}, 2020, pp. 1000--1001.

\bibitem{cai2020monocular}
Y.~Cai, B.~Li, Z.~Jiao, H.~Li, X.~Zeng, and X.~Wang, ``Monocular 3d object
  detection with decoupled structured polygon estimation and height-guided
  depth estimation,'' in \emph{AAAI}, vol.~34, 2020, pp. 10\,478--10\,485.

\bibitem{chen2020monopair}
Y.~Chen, L.~Tai, K.~Sun, and M.~Li, ``Monopair: Monocular 3d object detection
  using pairwise spatial relationships,'' in \emph{CVPR}, 2020, pp.
  12\,093--12\,102.

\bibitem{qin2022monoground}
Z.~Qin and X.~Li, ``Monoground: Detecting monocular 3d objects from the
  ground,'' in \emph{CVPR}, 2022, pp. 3793--3802.

\bibitem{wang2022probabilistic}
T.~Wang, X.~Zhu, J.~Pang, and D.~Lin, ``Probabilistic and geometric depth:
  Detecting objects in perspective,'' in \emph{CoRL}, 2022, pp. 1475--1485.

\bibitem{zhang2021objects}
Y.~Zhang, J.~Lu, and J.~Zhou, ``Objects are different: Flexible monocular 3d
  object detection,'' in \emph{CVPR}, 2021, pp. 3289--3298.

\bibitem{lu2021geometry}
Y.~Lu, X.~Ma, L.~Yang, T.~Zhang, Y.~Liu, Q.~Chu, J.~Yan, and W.~Ouyang,
  ``Geometry uncertainty projection network for monocular 3d object
  detection,'' in \emph{ICCV}, 2021, pp. 3111--3121.

\bibitem{shi2021geometry}
X.~Shi, Q.~Ye, X.~Chen, C.~Chen, Z.~Chen, and T.-K. Kim, ``Geometry-based
  distance decomposition for monocular 3d object detection,'' in \emph{ICCV},
  2021, pp. 15\,172--15\,181.

\bibitem{chen2022polar}
S.~Chen, X.~Wang, T.~Cheng, Q.~Zhang, C.~Huang, and W.~Liu, ``Polar
  parametrization for vision-based surround-view 3d detection,'' \emph{arXiv
  preprint arXiv:2206.10965}, 2022.

\bibitem{yang2023bevheight}
L.~Yang, K.~Yu, T.~Tang, J.~Li, K.~Yuan, L.~Wang, X.~Zhang, and P.~Chen,
  ``Bevheight: A robust framework for vision-based roadside 3d object
  detection,'' \emph{arXiv preprint arXiv:2303.08498}, 2023.

\bibitem{kendall2017uncertainties}
A.~Kendall and Y.~Gal, ``What uncertainties do we need in bayesian deep
  learning for computer vision?'' \emph{NeurIPS}, vol.~30, 2017.

\bibitem{lin2017focal}
T.-Y. Lin, P.~Goyal, R.~Girshick, K.~He, and P.~Doll{\'a}r, ``Focal loss for
  dense object detection,'' in \emph{ICCV}, 2017, pp. 2980--2988.

\bibitem{xie2022m}
E.~Xie, Z.~Yu, D.~Zhou, J.~Philion, A.~Anandkumar, S.~Fidler, P.~Luo, and J.~M.
  Alvarez, ``M\^{} 2bev: Multi-camera joint 3d detection and segmentation with
  unified birds-eye view representation,'' \emph{arXiv preprint
  arXiv:2204.05088}, 2022.

\bibitem{zhu2019class}
B.~Zhu, Z.~Jiang, X.~Zhou, Z.~Li, and G.~Yu, ``Class-balanced grouping and
  sampling for point cloud 3d object detection,'' \emph{arXiv preprint
  arXiv:1908.09492}, 2019.

\bibitem{wang2021fcos3d}
T.~Wang, X.~Zhu, J.~Pang, and D.~Lin, ``Fcos3d: Fully convolutional one-stage
  monocular 3d object detection,'' in \emph{ICCV}, 2021, pp. 913--922.

\bibitem{li2022unifying}
Y.~Li, Y.~Chen, X.~Qi, Z.~Li, J.~Sun, and J.~Jia, ``Unifying voxel-based
  representation with transformer for 3d object detection,'' in \emph{NeurIPS},
  2022.

\bibitem{lu2022learning}
J.~Lu, Z.~Zhou, X.~Zhu, H.~Xu, and L.~Zhang, ``Learning ego 3d representation
  as ray tracing,'' in \emph{ECCV}, 2022.

\bibitem{lee2019energy}
Y.~Lee, J.-w. Hwang, S.~Lee, Y.~Bae, and J.~Park, ``An energy and
  gpu-computation efficient backbone network for real-time object detection,''
  in \emph{CVPRW}, 2019.

\end{thebibliography}
