\section{Experiments}


\subsection{Experimental details}
\paragraph{Dataset} The proposed method is evaluated on the NuScenes\cite{nuscenes2019} detection dataset.
As a challenging dataset for autonomous driving, NuScenes provides data of 1k scenes, from a sensor suite that consists of 6 cameras, 1 LiDAR, 5 RADAR, 1 GPS, and 1 IMU. The {\it train} set contains 28,130 samples and the \uwave{{\it validation} set} contains 6,019 samples.

\paragraph{Metrics} The main metrics that reflect the performance are mAP and NDS. mAP is widely used in detection tasks. NDS stands for NuScenes Detection Score, which consists of six sub-metrics: mAP, mATE, mASE, mAOE, mAVE, and mAAE. NDS is computed as:
\begin{equation}
    \text{NDS}=\cfrac{1}{10}[5\text{mAP}+\sum_{\text{mTP}\in \mathbb{TP}}[1-\min(1, \text{mTP})],
    \label{eq:nds}
\end{equation}
where $\mathbb{TP}$ is the set of sub-metrics expect mAP. mATE, mASE, and mAOE measure the localization error, the scale error, and the orientation error respectively. mAVE measures velocity prediction error. mAAE measures the error of object classification. For NDS and mAP, higher is better. For other sub-metrics, the opposite is true.

\paragraph{Settings} As our model is built based on BEVFormer\cite{li2022bevformer}, we share most settings with BEVFormer. \uwave{For time fusion module, 4 history BEV queries are used in the {\it base} setting and 3 BEV queries are used in the {\it tiny} setting.} The BEV space is divided into 200$\times$200 grids in both {\it base} and {\it tiny} settings, with each grid standing for an area of 0.512 meters by 0.512 meters. We supervise the height predictor of the last encoder layer with the loss function stated in \cref{eq:loss0}. The extra segmentation map is supervised with binary focal loss\cite{lin2017focal}. A position-aware loss weight called BEV centerness\cite{xie2022m} which pays more attention to distant areas is also used to improve the height estimation accuracy of distant grids.
% It is calculated as 
% \begin{equation}
%     C(x,z) = 1 + \sqrt{\frac{x^2+z^2}{X^2+Z^2}}.
%     \label{eq:bevc}
% \end{equation}
% Here, $X$ and $Z$ stand for the range of BEV space, and they are both equal to 51.2 meters.


\subsection{Ablation study}


\paragraph{Performance upper bound} We first replace the height predictors with ground truth heights to evaluate whether accurate height estimation could improve the performance of detection, and we call this method HeightFormer-gt. For BEV girds containing objects, predicted heights are replaced with ground truth heights. For others, predicted heights are replaced with $y_{xz}=0.5$ and $h_{xz}=1.0$ (normalized), which degrades into the situation of BEVFormer. Results are shown in \cref{tab:expa0}.

\begin{table}[hbt]
    \centering
    \caption{The upper bound of performance. The anchor heights of HeightFormer-gt are uniformly distributed in the range of $[y_{gt} - h_{gt}/2, y_{gt} + h_{gt}/2 ]$. *: The experiments about Lift-Splat\cite{philion2020lift} is done by BEVDepth\cite{li2022bevdepth}.}
    \begin{tabular}{lccc}
        \toprule
        Model             & Condition & NDS$\uparrow$ & mAP$\uparrow$ \\
        \midrule
        Lift-Splat$^*$    & -         & 0.327         & 0.282         \\
        Lift-Splat-gt$^*$ & depth     & {\bf 0.515}         & {\bf 0.470}         \\
        \midrule
        BEVFormer         & -         & 0.517         & 0.416         \\
        HeightFormer-gt      & height    & {\bf 0.725}   & {\bf 0.789}   \\
        \bottomrule
    \end{tabular}
    \label{tab:expa0}
\end{table}


As shown in \cref{tab:expa0}, HeightFormer-gt exceeds BEVFormer by 0.208 in NDS and 0.374 in mAP, which can be considered as the performance upper bound. 
\uwave{However, in reality, height or depth estimation error could be amplified when considering the final detection error. As a result, it is not feasible to achieve this theoretical upper bound.}
In the meanwhile, Lift-Splat\cite{philion2020lift} in \cref{tab:expa0} is a method that constructs the BEV space with depth modeling and according to BEVDepth\cite{li2022bevdepth} introducing ground truth depth improves mAP of the Lift-Splat style detector by 0.188. The experiments in \cref{tab:expa0} show that there is room for improvement whether in depth modeling or height modeling. 


Besides, it should be noted that the two types of methods, LSS and BEVFormer, cannot be compared directly as they have different network architectures. \uwave{We list these results here to show the potential of height-based methods.}

\paragraph{Effectiveness of explicit height modeling} We present the vanilla HeightFormer which turns fixed reference points into adaptive reference points. The adaptive reference points are generated with predicted anchor heights. In this way, height information is explicitly learned by the height predictor. To show the effectiveness of explicit height modeling, it is compared with BEVFormer which encodes height information into attention weights of spatial cross-attention in an implicit way. The vanilla HeightFormer simply has a multilayer perceptron as the height predictor. Results are shown in \cref{tab:expa1}. \uwave{Explicit height modeling brings a gain of 0.5 percentage points in both NDS and mAP for the ``base'' model. For the ``tiny'' model, the improvements come to 1.8 and 1.1 percentage points.
% We also list the results of BEVHeight, which is also a height-based method. The proposed style of height modeling outperforms it on mAP by 0.8 percentage points.
}


% For the supervision of height prediction, we do not take the . One only supervises the height predictions of the last encoder layer, and the other supervises all encoder layers by averaging height prediction losses of them. The result is listed in \cref{tab:expa1}. Supervising the last layer works better here. A potential reason is that it is difficult for shallow layers to estimate heights, and thus these layers might not learn well. In all the following experiments, we will adopt the practice of only supervising the height predictor of the last encoder layer. %Besides NDS and mAP, we notice that there is an improvement of 4.5\% in mAVE, which measures the performance of velocity estimation. Theoretically, explicit height supervision of two successive frames indirectly provides velocity information, which helps the model learn velocity estimation.

\begin{table}[hbt]
    \centering
    \caption{Ablation study on height modeling. *: Vanilla HeightFormer which predicts heights in a standalone way. $\dag$: Official results, no history frames.}
    \begin{tabular}{lcccc}
        \toprule
        Model    & Config  & Height   & NDS$\uparrow$ & mAP$\uparrow$ \\
        \midrule
        BEVFormer & Base & implicit & 0.517         & 0.416         \\
        % \midrule
        HeightFormer* & Base & explicit & {\bf 0.522}         & {\bf 0.421}     \\
        % \,+self-recursive & explicit & {\bf 0.525}   & {\bf 0.422}   \\
        \midrule
        BEVFormer     & Tiny & implicit & 0.403         & 0.288         \\
        % BEVHeight$^\dag$     & ResNet50 & explicit & 0.342 & 0.291 \\
        HeightFormer* & Tiny & explicit & {\bf 0.421}   & {\bf 0.299}     \\
        \bottomrule
    \end{tabular}
    \label{tab:expa1}
\end{table}


\paragraph{Effectiveness of the network design}
Based on the vanilla HeightFormer which predicts heights standalone in each layer, height embeddings are added to formulate self-recursive predictors. For BEV grids that do not cover any object, the query mask is applied. Two types of masks are designed for comparison. The uncertainty-based mask filters BEV queries that have high uncertainties of the predicted heights. The segmentation-based mask filters BEV queries that are less likely to have objects. Results are shown in \cref{tab:expa3}.

The self-recursive way of predicting heights brings an improvement of 0.3 percentage points in NDS. Besides, we observe that there is a giant gap between the outputs of successive predictors when they are standalone, and refining the heights layer by layer in a self-recursive way mitigates this issue. 

The two types of masks both improve the NDS, while the segmentation-based mask brings a gain of 0.5  percentage points in mAP. The improvement mainly comes from filtering irrelevant features and introducing segmentation as an auxiliary task.
In the proposed method, there is no proper way to define the heights of BEV grids that do not cover any object. As a result, the predicted heights in these grids are not explainable and the sampled features are from irrelevant background areas. Applying a mask mitigates this issue.


\begin{table}[hbt]
    \centering
    \caption{Ablation study on the effectiveness of the network design. SR: a self-recursive way of predicting heights. Unc.M: uncertainty-based mask. Seg.M: segmentation-based mask.}
    \begin{tabular}{ccc|cc}
        \toprule
        SR         & Unc.M      & Seg.M      & NDS$\uparrow$ & mAP$\uparrow$ \\
        \midrule
                   &            &            & 0.522         & 0.421         \\
        \checkmark &            &            & 0.525         & 0.422         \\
        \checkmark & \checkmark &            & {\bf 0.528}   & 0.424         \\
        \checkmark &            & \checkmark & 0.527         & {\bf 0.427}   \\
        \bottomrule
    \end{tabular}
    \label{tab:expa3}
\end{table}


\paragraph{Robustness of height modeling} 
% Compared with depth modeling in the image view, modeling heights can fit arbitrary camera rigs and types. In the NuScenes dataset, there is a great manufacturing difference between the back camera and all other cameras. We introduce ``mAP-cam'' which evaluates the detection performance in a single camera. It can be calculated by filtering the ground truth and predicted bounding boxes whose centers are not seen by the camera. We choose BEVDepth\cite{li2022bevdepth} for comparison. BEVDepth takes camera parameters as the input of its network. Results are shown in \cref{tab:expa4}. HeightFormer-tiny shows better performance on the back camera compared to BEVDepth-r50, although HeightFormer-tiny has a lower overall mAP. This demonstrates the advantage of height modeling in fitting different cameras.
As described in Introduction, although both depth and height modeling can provide an extra condition for 2D to 3D mapping, height modeling has a unique advantage in that it can process any camera rig. This is because estimating depth in the image space might be influenced by the camera rigs, but estimating height is simple and robust since all information has been mapped to the unified BEV space. \uwave{To verify this property, we test our method with depth modeling specifically with the back camera, which has a different focal length from other cameras.} We use BEVDepth\cite{li2022bevdepth} as the depth modeling method. Results are shown in \cref{tab:expa4}. 


\begin{table}[htb]
    \centering
    \caption{Ablation study on the robustness of height modeling. Although the depth modeling method has higher overall performance (due to extra LIDAR data), our method still achieves higher performance on the back camera which has a different focal length and camera configuration. This shows the robustness of height modeling.}
    \begin{tabular}{c|cc|cc}
        \toprule
        \multirow{2}{*}{Extra condition}
                  & \multicolumn{2}{c|}{Overall} & \multicolumn{2}{c}{Back Cam}  \\
         & mAP & NDS & mAP & NDS  \\
        \midrule
        Depth modeling   & {\bf 0.330}  &  {\bf  0.436}   & 0.273  &   0.398             \\
        Height modeling & 0.299     & 0.421 & {\bf 0.279}   &  {\bf  0.413}  \\
        \bottomrule
    \end{tabular}
    \label{tab:expa4}
\end{table}


From \cref{tab:expa4} we can see that although BEVDepth has higher overall performance \uwave{(due to extra LiDAR data training and different model architectures)}, height modeling's performance on the back camera is higher, which highlighted a great performance gap between the back camera and other cameras for BEVDepth. This means height modeling is more robust to different camera rigs, even when the cameras' focal lengths are different.

\input{chaps/4expgen.tex}

\input{chaps/4expqual.tex}


\input{chaps/4expbench.tex}