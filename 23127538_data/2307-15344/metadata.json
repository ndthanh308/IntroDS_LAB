{
  "title": "Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions",
  "authors": [
    "Yifei Xin",
    "Yuexian Zou"
  ],
  "submission_date": "2023-07-28T06:46:30+00:00",
  "revised_dates": [
    "2025-05-03T14:12:10+00:00"
  ],
  "abstract": "Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical cross-modal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets.",
  "categories": [
    "cs.SD",
    "eess.AS"
  ],
  "primary_category": "cs.SD",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15344",
  "pdf_url": "https://arxiv.org/pdf/2307.15344v2",
  "comment": "Accepted by Interspeech2023",
  "num_versions": null,
  "size_before_bytes": 4738677,
  "size_after_bytes": 351693
}