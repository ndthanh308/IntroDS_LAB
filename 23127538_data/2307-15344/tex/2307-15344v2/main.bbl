% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{koepke2022audio}
A.~S. Koepke, A.-M. Oncescu, J.~Henriques, Z.~Akata, and S.~Albanie, ``Audio retrieval with natural language queries: A benchmark study,'' \emph{IEEE Transactions on Multimedia}, 2022.

\bibitem{lou2022audio}
S.~Lou, X.~Xu, M.~Wu, and K.~Yu, ``Audio-text retrieval in context,'' in \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 4793--4797.

\bibitem{xin2024diffatr}
Y.~Xin, X.~Cheng, Z.~Zhu, X.~Yang, and Y.~Zou, ``Diffatr: Diffusion-based generative modeling for audio-text retrieval,'' \emph{arXiv preprint arXiv:2409.10025}, 2024.

\bibitem{gemmeke2017audio}
J.~F. Gemmeke, D.~P. Ellis, D.~Freedman, A.~Jansen, W.~Lawrence, R.~C. Moore, M.~Plakal, and M.~Ritter, ``Audio set: An ontology and human-labeled dataset for audio events,'' in \emph{2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 776--780.

\bibitem{kong2020panns}
Q.~Kong, Y.~Cao, T.~Iqbal, Y.~Wang, W.~Wang, and M.~D. Plumbley, ``Panns: Large-scale pretrained audio neural networks for audio pattern recognition,'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, vol.~28, pp. 2880--2894, 2020.

\bibitem{xu2021investigating}
X.~Xu, H.~Dinkel, M.~Wu, Z.~Xie, and K.~Yu, ``Investigating local and global information for automated audio captioning with transfer learning,'' in \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 905--909.

\bibitem{mesaros2021sound}
A.~Mesaros, T.~Heittola, T.~Virtanen, and M.~D. Plumbley, ``Sound event detection: A tutorial,'' \emph{IEEE Signal Processing Magazine}, vol.~38, no.~5, pp. 67--83, 2021.

\bibitem{xin2023background}
Y.~Xin, D.~Yang, and Y.~Zou, ``Background-aware modeling for weakly supervised sound event detection,'' in \emph{Proc. ISCA Annu. Conf. Int. Speech Commun. Assoc}, 2023, pp. 1199--1203.

\bibitem{xin2023enhancement}
Y.~Xin, X.~Peng, and Y.~Lu, ``Improving speech enhancement via event-based query,'' in \emph{ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 1--5.

\bibitem{mei2022metric}
X.~Mei, X.~Liu, J.~Sun, M.~D. Plumbley, and W.~Wang, ``On metric learning for audio-text cross-modal retrieval,'' \emph{arXiv preprint arXiv:2203.15537}, 2022.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for contrastive learning of visual representations,'' in \emph{International conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2020, pp. 1597--1607.

\bibitem{xie2022dcase}
H.~Xie, S.~Lipping, and T.~Virtanen, ``Dcase 2022 challenge task 6b: Language-based audio retrieval,'' \emph{arXiv e-prints}, pp. arXiv--2206, 2022.

\bibitem{lamorttake}
T.~L. de~Gail and D.~Kicinski, ``Take it easy: Relaxing contrastive ranking loss with cider,'' DCASE2022 Challenge, Tech. Rep, Tech. Rep., 2022.

\bibitem{touvron2021augmenting}
H.~Touvron, M.~Cord, A.~El-Nouby, P.~Bojanowski, A.~Joulin, G.~Synnaeve, and H.~J{\'e}gou, ``Augmenting convolutional networks with attention-based aggregation,'' \emph{arXiv preprint arXiv:2112.13692}, 2021.

\bibitem{wang2019comparison}
Y.~Wang, J.~Li, and F.~Metze, ``A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling,'' in \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 31--35.

\bibitem{jiang2022tencent}
J.~Jiang, S.~Min, W.~Kong, H.~Wang, Z.~Li, and W.~Liu, ``Tencent text-video retrieval: Hierarchical cross-modal interactions with multi-level representations,'' \emph{IEEE Access}, 2022.

\bibitem{mei2022language}
X.~Mei, X.~Liu, H.~Liu, J.~Sun, M.~D. Plumbley, and W.~Wang, ``Language-based audio retrieval with pre-trained models,'' DCASE2022 Challenge, Tech. Rep, Tech. Rep., 2022.

\bibitem{zhang2022caption}
Y.~Zhang, H.~Yu, R.~Du, Z.~Ma, and Y.~Dong, ``Caption feature space regularization for audio captioning,'' \emph{arXiv preprint arXiv:2204.08409}, 2022.

\bibitem{mei2022automated}
X.~Mei, X.~Liu, M.~D. Plumbley, and W.~Wang, ``Automated audio captioning: An overview of recent progress and new challenges,'' \emph{EURASIP journal on audio, speech, and music processing}, vol. 2022, no.~1, pp. 1--18, 2022.

\bibitem{zhou2022can}
Z.~Zhou, Z.~Zhang, X.~Xu, Z.~Xie, M.~Wu, and K.~Q. Zhu, ``Can audio captions be evaluated with image caption metrics?'' in \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 981--985.

\bibitem{kenton2019bert}
J.~D. M.-W.~C. Kenton and L.~K. Toutanova, ``Bert: Pre-training of deep bidirectional transformers for language understanding,'' in \emph{Proceedings of naacL-HLT}, 2019, pp. 4171--4186.

\bibitem{lu2019vilbert}
J.~Lu, D.~Batra, D.~Parikh, and S.~Lee, ``Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,'' \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{chen2021multimodal}
R.~J. Chen, M.~Y. Lu, W.-H. Weng, T.~Y. Chen, D.~F. Williamson, T.~Manz, M.~Shady, and F.~Mahmood, ``Multimodal co-attention transformer for survival prediction in gigapixel whole slide images,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2021, pp. 4015--4025.

\bibitem{wu2022cap4video}
W.~Wu, H.~Luo, B.~Fang, J.~Wang, and W.~Ouyang, ``Cap4video: What can auxiliary captions do for text-video retrieval?'' \emph{arXiv preprint arXiv:2301.00184}, 2022.

\bibitem{xin2024audio}
Y.~Xin, Z.~Zhu, X.~Cheng, X.~Yang, and Y.~Zou, ``Audio-text retrieval with transformer-based hierarchical alignment and disentangled cross-modal representation,'' \emph{arXiv preprint arXiv:2409.09256}, 2024.

\bibitem{kim2019audiocaps}
C.~D. Kim, B.~Kim, H.~Lee, and G.~Kim, ``Audiocaps: Generating captions for audios in the wild,'' in \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, 2019, pp. 119--132.

\bibitem{drossos2020clotho}
K.~Drossos, S.~Lipping, and T.~Virtanen, ``Clotho: An audio captioning dataset,'' in \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 736--740.

\bibitem{xin2023cooperative}
Y.~Xin, B.~Wang, and L.~Shang, ``Cooperative game modeling with weighted token-level alignment for audio-text retrieval,'' \emph{IEEE Signal Processing Letters}, vol.~30, pp. 1317--1321, 2023.

\bibitem{zhao2024mint}
H.~Zhao, Y.~Xin, Z.~Yu, B.~Zhu, L.~Lu, and Z.~Ma, ``Mint: Boosting audio-language model via multi-target pre-training and instruction tuning,'' \emph{arXiv preprint arXiv:2402.07485}, 2024.

\bibitem{cheng2023s}
X.~Cheng, Z.~Zhu, H.~Li, Y.~Li, and Y.~Zou, ``Ssvmr: Saliency-based self-training for video-music retrieval,'' in \emph{Proc. of ICASSP}, 2023.

\end{thebibliography}
