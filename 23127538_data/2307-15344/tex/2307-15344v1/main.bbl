% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{koepke2022audio}
A.~S. Koepke, A.-M. Oncescu, J.~Henriques, Z.~Akata, and S.~Albanie, ``Audio
  retrieval with natural language queries: A benchmark study,'' \emph{IEEE
  Transactions on Multimedia}, 2022.

\bibitem{lou2022audio}
S.~Lou, X.~Xu, M.~Wu, and K.~Yu, ``Audio-text retrieval in context,'' in
  \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2022, pp. 4793--4797.

\bibitem{xin2023improving}
Y.~Xin, D.~Yang, and Y.~Zou, ``Improving text-audio retrieval by text-aware
  attention pooling and prior matrix revised loss,'' in \emph{ICASSP 2023-2023
  IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 1--5.

\bibitem{gemmeke2017audio}
J.~F. Gemmeke, D.~P. Ellis, D.~Freedman, A.~Jansen, W.~Lawrence, R.~C. Moore,
  M.~Plakal, and M.~Ritter, ``Audio set: An ontology and human-labeled dataset
  for audio events,'' in \emph{2017 IEEE international conference on acoustics,
  speech and signal processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2017, pp. 776--780.

\bibitem{kong2020panns}
Q.~Kong, Y.~Cao, T.~Iqbal, Y.~Wang, W.~Wang, and M.~D. Plumbley, ``Panns:
  Large-scale pretrained audio neural networks for audio pattern recognition,''
  \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  vol.~28, pp. 2880--2894, 2020.

\bibitem{xu2021investigating}
X.~Xu, H.~Dinkel, M.~Wu, Z.~Xie, and K.~Yu, ``Investigating local and global
  information for automated audio captioning with transfer learning,'' in
  \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2021, pp. 905--909.

\bibitem{kenton2019bert}
J.~D. M.-W.~C. Kenton and L.~K. Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' in \emph{Proceedings
  of naacL-HLT}, 2019, pp. 4171--4186.

\bibitem{cheng2023acl}
X.~Cheng, B.~Cao, Q.~Ye, Z.~Zhu, H.~Li, and Y.~Zou, ``Ml-lmcl: Mutual learning
  and large-margin contrastive learning for improving asr robustness in spoken
  language understanding,'' in \emph{Proc. of ACL Findings}, 2023.

\bibitem{chen2022hts}
K.~Chen, X.~Du, B.~Zhu, Z.~Ma, T.~Berg-Kirkpatrick, and S.~Dubnov, ``Hts-at: A
  hierarchical token-semantic audio transformer for sound classification and
  detection,'' in \emph{ICASSP 2022-2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2022, pp. 646--650.

\bibitem{xin2022audio}
Y.~Xin, D.~Yang, and Y.~Zou, ``Audio pyramid transformer with domain adaption
  for weakly supervised sound event detection and audio classification,''
  \emph{Proc. Interspeech 2022}, pp. 1546--1550, 2022.

\bibitem{mesaros2021sound}
A.~Mesaros, T.~Heittola, T.~Virtanen, and M.~D. Plumbley, ``Sound event
  detection: A tutorial,'' \emph{IEEE Signal Processing Magazine}, vol.~38,
  no.~5, pp. 67--83, 2021.

\bibitem{xin2023causal}
Y.~Xin, D.~Yang, F.~Cui, Y.~Wang, and Y.~Zou, ``Improving weakly supervised
  sound event detection with causal intervention,'' in \emph{ICASSP 2023-2023
  IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 1--5.

\bibitem{mei2022metric}
X.~Mei, X.~Liu, J.~Sun, M.~D. Plumbley, and W.~Wang, ``On metric learning for
  audio-text cross-modal retrieval,'' \emph{arXiv preprint arXiv:2203.15537},
  2022.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for
  contrastive learning of visual representations,'' in \emph{International
  conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2020, pp. 1597--1607.

\bibitem{xie2022dcase}
H.~Xie, S.~Lipping, and T.~Virtanen, ``Dcase 2022 challenge task 6b:
  Language-based audio retrieval,'' \emph{arXiv e-prints}, pp. arXiv--2206,
  2022.

\bibitem{lamorttake}
T.~L. de~Gail and D.~Kicinski, ``Take it easy: Relaxing contrastive ranking
  loss with cider,'' DCASE2022 Challenge, Tech. Rep, Tech. Rep., 2022.

\bibitem{touvron2021augmenting}
H.~Touvron, M.~Cord, A.~El-Nouby, P.~Bojanowski, A.~Joulin, G.~Synnaeve, and
  H.~J{\'e}gou, ``Augmenting convolutional networks with attention-based
  aggregation,'' \emph{arXiv preprint arXiv:2112.13692}, 2021.

\bibitem{wang2019comparison}
Y.~Wang, J.~Li, and F.~Metze, ``A comparison of five multiple instance learning
  pooling functions for sound event detection with weak labeling,'' in
  \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2019, pp. 31--35.

\bibitem{jiang2022tencent}
J.~Jiang, S.~Min, W.~Kong, H.~Wang, Z.~Li, and W.~Liu, ``Tencent text-video
  retrieval: Hierarchical cross-modal interactions with multi-level
  representations,'' \emph{IEEE Access}, 2022.

\bibitem{mei2022language}
X.~Mei, X.~Liu, H.~Liu, J.~Sun, M.~D. Plumbley, and W.~Wang, ``Language-based
  audio retrieval with pre-trained models,'' DCASE2022 Challenge, Tech. Rep,
  Tech. Rep., 2022.

\bibitem{zhang2022caption}
Y.~Zhang, H.~Yu, R.~Du, Z.~Ma, and Y.~Dong, ``Caption feature space
  regularization for audio captioning,'' \emph{arXiv preprint
  arXiv:2204.08409}, 2022.

\bibitem{mei2022automated}
X.~Mei, X.~Liu, M.~D. Plumbley, and W.~Wang, ``Automated audio captioning: An
  overview of recent progress and new challenges,'' \emph{EURASIP journal on
  audio, speech, and music processing}, vol. 2022, no.~1, pp. 1--18, 2022.

\bibitem{zhou2022can}
Z.~Zhou, Z.~Zhang, X.~Xu, Z.~Xie, M.~Wu, and K.~Q. Zhu, ``Can audio captions be
  evaluated with image caption metrics?'' in \emph{ICASSP 2022-2022 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 981--985.

\bibitem{lu2019vilbert}
J.~Lu, D.~Batra, D.~Parikh, and S.~Lee, ``Vilbert: Pretraining task-agnostic
  visiolinguistic representations for vision-and-language tasks,''
  \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{chen2021multimodal}
R.~J. Chen, M.~Y. Lu, W.-H. Weng, T.~Y. Chen, D.~F. Williamson, T.~Manz,
  M.~Shady, and F.~Mahmood, ``Multimodal co-attention transformer for survival
  prediction in gigapixel whole slide images,'' in \emph{Proceedings of the
  IEEE/CVF International Conference on Computer Vision}, 2021, pp. 4015--4025.

\bibitem{wu2022cap4video}
W.~Wu, H.~Luo, B.~Fang, J.~Wang, and W.~Ouyang, ``Cap4video: What can auxiliary
  captions do for text-video retrieval?'' \emph{arXiv preprint
  arXiv:2301.00184}, 2022.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{cheng2023m}
X.~Cheng, Q.~Dong, F.~Yue, T.~Ko, M.~Wang, and Y.~Zou, ``M 3 st: Mix at three
  levels for speech translation,'' in \emph{Proc. of ICASSP}, 2023.

\bibitem{kim2019audiocaps}
C.~D. Kim, B.~Kim, H.~Lee, and G.~Kim, ``Audiocaps: Generating captions for
  audios in the wild,'' in \emph{Proceedings of the 2019 Conference of the
  North American Chapter of the Association for Computational Linguistics:
  Human Language Technologies, Volume 1 (Long and Short Papers)}, 2019, pp.
  119--132.

\bibitem{drossos2020clotho}
K.~Drossos, S.~Lipping, and T.~Virtanen, ``Clotho: An audio captioning
  dataset,'' in \emph{ICASSP 2020-2020 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2020, pp. 736--740.

\bibitem{cheng2023s}
X.~Cheng, Z.~Zhu, H.~Li, Y.~Li, and Y.~Zou, ``Ssvmr: Saliency-based
  self-training for video-music retrieval,'' in \emph{Proc. of ICASSP}, 2023.

\end{thebibliography}
