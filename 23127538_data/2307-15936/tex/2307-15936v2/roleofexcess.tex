\section{(Mis)understanding, Excess entropy, and Cloze Questions} %A Clarifying example}
\label{sec:excessentropy}

Thinking about emergence and Scaling Laws, it is possible to get confused as follows: {\em ``When we increase $D$ from $10^{11}$ to $10^{12}$ then according to~(\ref{eqn:scaling})  this  changes cross-entropy by a tiny amount. Why does it lead to big changes in macroscopic behavior?''} 
The flaw in this reasoning is that most of the loss captures merely the inherent entropy of language (the $A$ term in (\ref{eqn:scaling})). We argue now that the model's mistakes on downstream tasks (i.e., its  misunderstandings) are captured by the  {\em excess} entropy, which as noted in Section~\ref{subsec:excessce} reduces by a constant factor each time the model is scaled up by an order of magnitude\footnote{A recent empirical study~\cite{Xiatrajectories23} also concludes with the  finding that ``language modeling perplexity correlates well with few-shot in-context learning performance along the trajectory, regardless of model sizes.'' At the same time, it is known that two models with the same cross-entropy can differ somewhat in their performance on language tasks.} 

We illustrate using a classic example  from~\cite{winograd1971procedures}, which later inspired the {\em Winograd Schema Challenge(WSC)}~\cite{levesque2012winograd}:

\noindent {\tt The city councilmen refused the demonstrators a permit because they feared violence.} 

Here the pronoun {\tt they} is ambiguous--- grammar rules allow it to refer to either {\tt demonstrators} or {\tt city councilmen}. Winograd pointed out that disambiguating it (i.e., anaphora resolution) requires world knowledge that is
 unavailable in the  text itself, namely that demonstrations can get violent, and city councilmen don't like violence. 
%The WSC contains many such examples from varied contexts, thus testing the model's world knowledge and common sense. 

A key idea in designing test-beds for language understanding such as WSC is  the {\bf Cloze Procedure}\footnote{Cloze questions are multiple choice, which allows testing most language skills~\cite{brown2020language}. Some skills such as understanding of irony  don't lend themselves well to cloze-based testing since one of the  multiple choices already explains the joke.  See \cite{saunshiexplore20} for earlier use of Cloze prompts in developing a theory of LLMs.}, popular also for testing language development in children~\cite{brownassessment}. To test the model's understanding of {\tt they} in this sentence, 
 we can append a {\em prompt}: {\tt Q. Who feared violence?}. This is followed by either a blank, or a choice of multiple answers:  {\tt A. city councilmen. B. demonstrators.}   For WSC examples, even though a human would be hundred percent sure of the answer, language models circa 2016 were roughly $50/50$ confused between the two options. 

 % (blank)} or,  
%\label{eqn:WSC}
%\end{eqnarray*}

In the above example, the human is $100\%$ certain of the answer, which implies their entropy here is $\log 1$, namely $0$. However if the model is split $50$-$50$ between the two options this implies it has cross-entropy  $\log 2$, all of which is {\em excess entropy}! Given the frequency of ambiguous pronouns in usual English, one concludes that a model that has not learned pronoun disambiguation will display huge excess entropy at many places in surrounding text. Thus reductions in excess entropy (which happen naturally due to scaling) will tend to squeeze out such errors. The rest of the paper tries to make this intuition mathematically precise.

Of course, text corpora  do not normally contain such artificial cloze questions. But one could imagine that the model's basic misunderstanding of the above type could, often, lead to prediction mistakes in neighboring text. Our theory in Section~\ref{sec:basictheory} will assume that cloze questions can closely capture the  model's misunderstanding.

