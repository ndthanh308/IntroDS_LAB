\section{Preliminaries} 
\label{sec:preliminaries}
Deep-learning based language models follows a statistical paradigm: occurrences of linguistic units (say, words, sentences, paragraphs) are assumed to fit a statistical profile, 
and thus pieces of text from a sufficiently large and diverse corpus are assumed to be samples from a probability distribution \cite{bengio2000neural}. 
Language models are trained to solve {
\em next-word-prediction} tasks: Given, say, the past 500 words\footnote{Actual training involves breaking up words into smaller  {\em tokens}, which allows  a single model to handle all human languages,  math formulae, computer code,  etc. For simplicity, our discussion will refer to ``words.''}  in the text, predict the next word. Faced with such a task,  humans may be able to give many completions, and the modeling assumes that frequencies of these completions can be modeled via probability. 
Thus the model $M$ computes a probability $\Pr_M[w_{i+1}~|~w_{1}w_2\ldots w_{i}]$ for all possible words $w_{i+1}$'s. 
The goodness of a model is computed by its {\em cross entropy loss}, which for a sequence of words $w_1w_2\ldots w_t$ is:
\begin{equation} \label{eqn:CEloss}
\ell(M) =   - \sum_{i} \log \Pr_M[w_{i+1}~|~w_{1}w_2\ldots w_{i}] \qquad \mbox{(Cross Entropy)}
\end{equation}
Models are trained by minimizing (via gradient descent) this training loss  on a text corpus, and their goodness is computed by their {\em test loss}---evaluating the same loss expression on
a held-out text from the same corpus. Often the training corpus is so large that the model trains only once (or a few times) on each piece of text, and by the end, 
the test loss on held-out text is almost the same as the training loss.  %Furthermore, many language tasks can also be solved (possibly after adding a suitable {\em prompt}) using fill-in-the blank capability. 

\paragraph{Scaling laws:}  These empirically-derived expressions describe how test cross entropy loss on held-out data scales (in experiments) with number of model parameters (N)  and size of the dataset (D) \cite{cortes1993learning, hestness2017deep, kaplan2020scaling, bahri2021explaining}. For Chinchilla models~\cite{hoffmann2022training} the law is as follows:
\begin{equation}
L(N, D) = A + \frac{B}{N^{0.34}} + \frac{C}{D^{0.28}} \qquad  A = 1.61 \quad B = 406.4 \quad C = 410.7 \label{eqn:scaling}.
\end{equation}
%describes test loss (and also training loss, if the model is trained with one pass over the dataset) as a 
Here the constants $A, B, C$ in \ref{eqn:scaling}  hold only for the  specific architecture and training strategy ---even the constant  $A$  depends upon the tokenization. This description of macro behavior using two basic parameters ---reminiscent of 2nd Law of Thermodynamics--- will help us circumvent the need for mechanistic understanding of training. Our theory will only rely upon the general form of the equations, specifically, that the dependence is inverse polynomial in $N, D$. So it applies to other frameworks of training (e.g., overtrained models~\cite{muennighoff2023scaling}) where scaling laws have also been found. 
%The scaling law is a description of macro behavior that abstracts away micro details of parameter updates etc. 
%Although not as universally applicable as the physical laws invoked by the creators of the term, it is qualitatively indeed like, say, the  2nd Law of Thermodynamic,  which also gives macro descriptions of matter and energy independent of the underlying chemistry.

\paragraph{Emergence:}  Emergence refers to an interesting empirical phenomenon that as $D, N $ are increased together then the 
model's performance (zero shot or few-shot) on a {\em broad range} of language tasks improves in a correlated way. The improvement can appear as a quick transition when $D, N$ are plotted on a log scale (which is often the case) but it is now generally accepted that for most tasks the performance improves gradually when $D, N$ are scaled up. Thus the term {\em slow emergence} is more correct.
Furthermore,  it is known that emergence happens at different rates for different tasks, and is often quickest for tasks where the text is plausibly close to text found in training data \cite{wei2022emergent}.
Plenty of tasks are known that stump current models, and they usually tend to be very different from what one would  find in usual text corpora.  See \citep{wei2022emergent,srivastava2022beyond,schaeffermirage23} for experimental results on emergence rates of the broad range of language tasks. 
One might thus posit, with some justification from the above-mentioned studies,  that the emergence of skills arises from training on related tasks that were implicitly solved while solving next-word prediction in the training dataset. This is indeed our starting point. 

\paragraph{Poverty of Stimulus with respect to skill combinations:} Any expert who has conversed with popular chat agents quickly discovers that at the very least they seem to flexibly solve tasks that require {\em combinations} of simple skills.
%For instance, {\sc put an example here}
%(see Figure~\ref{fig:GPT4reply}). 
However, the number of $k'$-wise combinations of elementary skills feels too large (say, with $k' =4$ and the number of elementary skills is in the tens of thousands) for all of the combinations to even be present in the training dataset.  In other words, if the model indeed acquires ability to solve tasks involving $t$-tuples of skills, we are looking at the familiar {\em Poverty of the Stimulus} issue\footnote{Chomsky coined the phrase {\em Poverty of the Stimulus}  to emphasize that babies do not have enough data to learn language, and concluded that evolution must have led to a ``universal grammar'' that humans must be be born with.
%a specialized   
In our  framework, the language model (whose learning is initialized using Gaussian noise) also seems to defy paucity of stimulus with respect to skill combinations.} which we return to in Section~\ref{subsec:ktuples}.

\subsection{Cross-Entropy, Entropy, and Excess entropy}
\label{subsec:excessce}
\newcommand{\Ex}{\mathbb{E}}
The conceptual framework underlying  cross-entropy loss (\ref{eqn:CEloss}) is that there is a ground-truth (i.e., humans') distribution for generating the next word, which assigns probability $p_i(w~|~w_1w_2\ldots w_i)$ to the event that the $(i+1)$th word is $w$ given that the previous words were $w_1w_2\ldots w_i$. In interest of compact notation we shorten $p_i(w~|~w_1w_2\ldots w_i)$  to $p_i(w)$,
Thus the {\em entropy} of the $(i+1)$th word 
\iffalse  which simplifies the expression for the probability of a corpus $\vec{w}= w_1w_2\ldots w_M$  to $\prod_{i=0}^{M-1} p_i(w_{i+1})$. Denote the probability distribution just defined as ${\mathcal P}$. The {\em entropy} of this distribution is $\Ex_{\vec{w}}[-\log (\prod_{i=0}^{M-1} p_i(w_{i+1})]$, which by a simple induction has the expectation
\fi 
\begin{equation}
     \sum_w p_i(w) \log \frac{1}{p_i(w)} ~~~~(\text{\sc entropy})
\end{equation}
This entropy is an inherent property of language, due to existence of many possible choices human writers can make for the next word. Given sequence $w_1 w_2 \ldots w_i$ the model has a probability distribution $q(w|w_1 w_2 \ldots w_i)$ for the next word $w$.  Extending our compact notation, we use $q_i(w)$ as a shorthand for this. The  cross-entropy loss of the model on the $i+1$th word is $\log \frac{1}{q(w_{i+1})}$, which should be seen as an empirical estimate of 
\begin{equation} E_{w \sim p_i()}[\log \frac{1}{q(w)}]~~ \text{\sc (expected c-e loss)} \label{eqn:empiricalloss}
\end{equation} 
\iffalse 
The probability assigned by the model to the held out corpus  $w_1 w_2 \ldots w_M$  is $\prod_{i\leq M} q_i(w)$. Let ${\mathcal Q}$ denote this distribution over word sequences.  
\begin{equation} KL({\mathcal P}||{\mathcal Q}) = \Ex_{\vec{w} \sim {\mathcal P}}[\log \prod_i p_i(w_{i+1}) - \log \prod_i q_i(w_{i+1})] = \sum_i KL(p_i|| q_i) \quad (\text{\sc excess entropy}) \label{eqn:KLloss}
 \end{equation}
\fi {\em KL divergence}, also sometimes called {\em excess entropy}, is  non-negative and defined as 
\begin{equation} KL(p_i|| q_i) = E_{w \sim p_i()}[\log \frac{p_i(w)}{q_i(w)}] ~~~\text{{\sc  excess entropy}}
\end{equation}
 Thus on a per-word basis we have:
 \begin{equation} \label{eqn:basicequation}
\text{{\sc expected c-e loss}}  =\text{{\sc entropy}} + \text{{\sc  excess entropy}}
\end{equation} 
 %$i$th term is itself the expectation of a random variable that is conditional on the previous words.
%Finally, we need to consider how (\ref{eqn:empiricalloss}) relates to its expectation over the population. O
Summing over the entire held out corpus, one obtains a similar estimate for the entire corpus.  One can make mild assumptions to the effect that the conditional probabilities $p_i(), q_i()$ only  depend only on (say) the previous $10^3$ words, whereas the corpus size $M$ is much bigger, e.g., $M \gg 10^7$. So the corpus consists of a random walk of sorts, where every $10^4$ words or so it switches to a different  portion of the language distribution. Under such assumptions  the above relationship, which holds in expectation at the word level, should  hold fairly precisely at the corpus level.
\iffalse Under this scenario the empirical hold-out loss (per-word) should be close to its expectation, namely. 
\begin{equation}
     \frac{1}{M}\sum_i \sum_{w}p_i(w)\log \frac{1}{q_i(w)} ~~ \text{\sc (population  c-e loss)} \label{eqn:populationloss}
\end{equation}
Thus examining the above expressions and dividing by $M$ where needed, we arrive at the following intuitive fact which holds on average  (i.e., per word) for the quantities above:
\fi 




\paragraph{Understanding the Scaling Law in terms of excess entropy:}
In (\ref{eqn:scaling}) the $A$ term captures the entropy of language~\footnote{Here we're assuming that as the model and data set size tend to infinity in tandem, the model will perfectly learn the language distribution.}. No model, however good, can achieve lower cross-entropy loss than $A$ for large corpora. The second and third terms of (\ref{eqn:scaling}) capture {\em excess entropy}, and they decrease polynomially with $N$ and $D$. For example when $N, D$ are increased by a factor of $10$ it reduces by roughly $(10)^{0.28} \approx 2$. 

Section~\ref{sec:excessentropy} will argue that reductions in excess entropy lead to improvements in model capabilities. But note that there is no way to compute excess entropy using just the corpus. Relationship~(\ref{eqn:basicequation}) shows that estimating excess entropy requires knowing the inherent entropy of text, which requires humans in the picture.  It is not possible to look at the model's cross-entropy loss on the $i$th word  according to (\ref{eqn:CEloss}) and know---without asking humans--- how much is due to inherent cross-entropy and how much is excess.

\iffalse % Figure environment removed
\fi 
 


    










\iffalse 
\begin{definition}[Performance Curve for $\theta, k$]
The boundary of $(\alpha, \beta)$ pairs for the region  in (\ref{eqn:mix2}) is called the {\em performance curve}.
\end{definition}

    

\begin{lemma}[Mixing Lemma] \label{lem:mix}
For every positive integer $k$ and $\theta \in [0,1]$ there are $\alpha, \beta >0$ such that the following statement is true with probability almost $1$ for degree-$k$ random bipartite graphs.  For every $T \subseteq V_1$ of size $\theta N_1$  and $S \subseteq V_2$ of size at least $\alpha  |N_2|$, the fraction of edges that are incident to  $S$ and have the other end point in $T$ is at least $\beta \theta$.
%In the same setting as Lemma~\ref{lem:wellspread} with probability almost $1$ the following is true for every $\theta >0.5$ and some small constants $\alpha, \beta >0$ that depend upon $\theta$ (see Figure ?? in the appendix for details of this dependence). 
% how $\alpha, \beta$
\end{lemma}
\begin{proof}
    
\end{proof}
Figure~\ref{fig:mixing_lemma} shows for example that when $\theta = 0.8$ we can use $\alpha =0.25, \beta = 0.82$. 

\begin{lemma}[Well spread-out edges] \label{lem:wellspread} There is a fixed constant $\alpha >0$ s.t. the following is true with probability almost $1$ when $N_2$ is sufficiently large: for each $S_2 \subseteq V_2$ of size  $\alpha N_2$, the fraction of edges that are incident to lies in $[\alpha/(1+ \alpha), \alpha(1+\alpha)]$.

Consequently, for any subset $S_2$ of size $\beta |V_2|$ where  $\beta = t \alpha$, the fraction of edges incident to it lies in $[\beta/(1+\alpha), \beta(1+\alpha)]$
\end{lemma}
\fi 

\iffalse 
The next Lemma is a key part of how emergence happens. 

The following lemma extends the above calculations to the setting where there is a measure on vertices in $V_1, V_2$. This will be actually the version we use in the emergence analysis. 
Specifically,  $\mu_i()$ is a measure on vertices in $V_i$. The $k$ edges incident to $v_1 \in V_1$ are picked by picking endpoints in $V_2$ iid using measure $\mu_2()$. \footnote{The theory could be reworked with a different sampling schemes, which is omitted in interest of brevity.} 

\begin{lemma}[Version with Measure] \label{lem:measure}
In the setting of the preceding paragraph, an analogous statement holds for any fixed $\theta$, except 
the cardinalities of $Z, Y$ are replaced by their measure. 
%total measure associated with the vertices in $V_1$ that appear in these edges.
\end{lemma}
\begin{proof}(Sketch)
Putting a measure $\mu_2()$ on $V_2$ does not affect anything because it can be seen as replacing each vertex $v_2 \in V_2$ by a collection of vertices whose size is proportional to $\mu_2(v_2)$.  To handle the measure on $V_1$, discretize $\mu_1$ using powers of $(1+ \delta)$ for an arbitrarily small $\delta$, which allows the measure to be approximated by a sum of uniform measures. 
\end{proof}
%\subsection{Cross Entropy versus Excess Cross Entropy}

\fi 

%Returning to our Winograd schema example (\ref{eqn:WSC}), the human's prediction for the missing word is ``councilmen,'' and therefore humans have cross-entropy loss $\log 1$ ---i.e., zero--- at this location. However, a model that is totally confused between two options has loss $\log 2$ here, which is significantly higher than the average excess of $1/D^c$. Thus 






