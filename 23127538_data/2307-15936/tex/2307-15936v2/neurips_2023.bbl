\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alon and Spencer(2016)]{alonspencer}
N~Alon and J~Spencer.
\newblock \emph{The Probabilistic Method (4th Ed)}.
\newblock Wiley, 2016.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and Shmitchel]{stochparrots}
Emily Bender, Timnit Gebru, Andrea McMillan-Major, and Shmargaret Shmitchel.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock \emph{FACCT}, 2021.

\bibitem[Bengio et~al.(2000)Bengio, Ducharme, and Vincent]{bengio2000neural}
Yoshua Bengio, R{\'e}jean Ducharme, and Pascal Vincent.
\newblock A neural probabilistic language model.
\newblock \emph{Advances in neural information processing systems}, 13, 2000.

\bibitem[Brown(2018)]{brownassessment}
HD~Brown.
\newblock \emph{Language Assessment: Principles and Classroom Practice (6th Ed)}.
\newblock Pearson Education, 2018.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chomsky(1957)]{chomsky57}
N~Chomsky.
\newblock \emph{Syntactic Structures}.
\newblock Mouton \& Co., 1957.

\bibitem[Chomsky(1988)]{chomsky88}
N~Chomsky.
\newblock \emph{Language and the problems of knowledge}.
\newblock MIT Press, 1988.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Coecke et~al.(2010)Coecke, Sadrzadeh, and Clark]{coeckeetal2010}
B~Coecke, M~Sadrzadeh, and S~Clark.
\newblock Mathematical foundations for a compositional distributional model of meaning.
\newblock \emph{Lambek Festschrift. Linguistic Analysis}, 36:\penalty0 345â€“384, 2010.

\bibitem[Cortes et~al.(1993)Cortes, Jackel, Solla, Vapnik, and Denker]{cortes1993learning}
Corinna Cortes, Lawrence~D Jackel, Sara Solla, Vladimir Vapnik, and John Denker.
\newblock Learning curves: Asymptotic values and rate of convergence.
\newblock \emph{Advances in neural information processing systems}, 6, 1993.

\bibitem[Ganguli et~al.(2022)Ganguli, Hernandez, Lovitt, Askell, Bai, Chen, Conerly, Dassarma, Drain, Elhage, et~al.]{ganguli2022predictability}
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et~al.
\newblock Predictability and surprise in large generative models.
\newblock In \emph{2022 ACM Conference on Fairness, Accountability, and Transparency}, pages 1747--1764, 2022.

\bibitem[Grice(1975)]{grice75}
H.~P. Grice.
\newblock \emph{Logic and conversation}.
\newblock Cole and Morgan, 1975.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Ali, Yang, and Zhou]{hestness2017deep}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock \emph{arXiv preprint arXiv:1712.00409}, 2017.

\bibitem[Hippisley and Stump(2017)]{morphologyhandbook}
A~Hippisley and G~Stump.
\newblock \emph{Cambridge Handbook of Morphology}.
\newblock Cambridge University Press, 2017.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and Morgenstern]{levesque2012winograd}
Hector Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In \emph{Thirteenth international conference on the principles of knowledge representation and reasoning}, 2012.

\bibitem[Muennighoff et~al.(2023)Muennighoff, Rush, Barak, Scao, Piktus, Tazi, Pyysalo, Wolf, and Raffel]{muennighoff2023scaling}
Niklas Muennighoff, Alexander~M. Rush, Boaz Barak, Teven~Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
\newblock Scaling data-constrained language models, 2023.

\bibitem[OpenAI(2023)]{DBLP:journals/corr/abs-2303-08774}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023.
\newblock \doi{10.48550/arXiv.2303.08774}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.08774}.

\bibitem[Saunshi et~al.(2021)Saunshi, Malladi, and Arora]{saunshiexplore20}
Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora.
\newblock A mathematical exploration of why language models help solve downstream tasks.
\newblock In \emph{ICLR 2021}, 2021.

\bibitem[Schaeffer et~al.(2023)Schaeffer, Miranda, and Koyejo]{schaeffermirage23}
Ryan Schaeffer, Brando Miranda, and Sanmi Koyejo.
\newblock Are emergent abilities of large language models a mirage?
\newblock \emph{ArXiv e-prints}, April 2023.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Steedman(1996)]{steedman96}
Mark Steedman.
\newblock \emph{Surface Structure and Interpretation}.
\newblock MIT Press, 1996.

\bibitem[Tannen~(ed)(1994)]{tannen94}
D~Tannen~(ed).
\newblock \emph{Framing in Discourse}.
\newblock Oxford University Press, 1994.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem[Winograd(1971)]{winograd1971procedures}
Terry Winograd.
\newblock Procedures as a representation for data in a computer program for understanding natural language.
\newblock Technical report, MASSACHUSETTS INST OF TECH CAMBRIDGE PROJECT MAC, 1971.

\bibitem[Xia et~al.(2023)Xia, Artetxe, Zhou, Lin, Pasunuru, Chen, Zettlemoyer, and Stoyanov]{Xiatrajectories23}
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi~Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov.
\newblock Training trajectories of language models across scales.
\newblock \emph{ArXiv e-prints}, May 2023.

\end{thebibliography}
