\section{Mathematical Framework}
\label{sec:basictheory}

We give a mathematical framework for thinking about skills and how they might relate to language comprehension tasks such as pronoun disambiguation. First, it is assumed that language comprehension involves a set of skills, though the theory will not need to know a precise list.  (Scholars have  discovered and named thousands of skills. Well-trained transformers have   undoubtedly discovered many more that remain unnamed.)
Next, the theory will assume scaling laws such as (\ref{eqn:scaling}) and thus not need to reason about  training and generalization. Instead, it can reason directly about the model's behavior on the test distribution, i.e., the distribution from which the training data was drawn. We assume this test distribution is structured as a  long unordered list of text-pieces, each with an associated measure\footnote{Text-pieces should be thought of as having a size between a paragraph to a few pages, drawn from a longer corpus. To allow good prediction for the model, the text-piece could include ancillary text that preceded it the longer corpus. The model need not do predictions for the words in this ancillary text but can use it to make predictions on the text-piece.}  Traditional cross-entropy loss is averaged using this associated measure. 
%Comprehending individual text-pieces  will be assumed to require a small subset of all skills. 
%Comprehending a particular text-piece may require a set of skills. 
%Now we  make some mild  assumptions about the test pieces and the underlying skills. 

\begin{definition}[Text piece] The test corpus for the model is viewed as being divided into  {\em text-pieces}, each consisting of $C_{test}$ tokens.  There is also a measure $\mu_2()$ on  these text-pieces, with $\mu_2(t)$ denoting the measure of text-piece $t$. The usual cross-entropy loss is computed by weighting text-pieces with respect to this measure. 
\end{definition}

Now we make some assumptions. We assume that the model's ``comprehension'' of a text piece is  testable via suitable cloze questions analogous to the Winograd example in Section~\ref{sec:excessentropy}. Specifically, we assume that an (unknown) process {\sc cloze} has been used to add such  cloze questions  to the text pieces at test time. These are clearly-marked multiple-choice questions in simple English that the model has to answer. Note that the training corpus did not contain such cloze questions, so this is a simple form of distribution shift at test time. The prediction loss on cloze questions does not require  predicting the location or contents of the cloze question ---it only requires  selecting the  correct answer to the  multiple-choice cloze question. 

We allow the process {\sc cloze} to tailor the questions to the model being tested. Thus the next assumption is reasonable. 

\begin{assumption}\label{assum:proportionalloss}[Cloze Sufficiency Assumption:]
{\em  The pre-trained model's average (multiclass) prediction loss on Cloze questions --- where the average is taken over the distribution of text pieces-- closely tracks (within a small multiplicative factor like 1.1)  the excess cross-entropy of the model on classical next-word prediction.} 
\end{assumption}
\noindent{\bf Note:} As discussed in Section~\ref{sec:excessentropy}, if the cloze question is assumed to be perfectly answerable by a human then any incorrect answers  by the model  can be interpreted analogously excess cross entropy. Our assumption  amounts to saying that mistakes on cloze questions closely capture the excess entropy of the model as defined in (\ref{eqn:CEloss}).  The next theorem, shows that there {\em exists} a set of cloze questions (albeit fairly artificial) where the excess cross-entropy  of the model's answer tracks the overall excess cross-entropy on next-word prediction. 

\begin{theorem}
If a model's  excess entropy at the $i$th place in text is $\epsilon$ then there is a cloze question with binary answer such that the probability that the model answers it incorrectly is at most $\sqrt{2\epsilon}$.
\end{theorem}
\begin{proof}
    The proof involves Pinsker's Inequality (wikipedia version) which relates variation distance and KL divergence. As in Section~\ref{sec:excessentropy} let $p_i()$ be the humans' probability distribution for the $i+1$th word in the text piece and $q_i()$ be the model's distribution. 
    The probability that the  human and the model give different answers is the variation distance    between the two distributions, which is the maximum (over all subsets $A$ of words) of  $\sum_{w \in A} (p_i(w) - q_i(w))$. 
    Let  $A_{i+1}$ denote the subset for which the previous expression is maximised. The cloze question consists of replacing word $w_{i+1}$ in the text with  the question: {\em Is the next word among the words listed in option (a) or in option (b)}, where option (a) lists words in $A_{i+1}$ and (b) lists words in $\overline{{A}_{i+1}}$. 
   The theorem now follows from Pinsker's inequality.
    %Suppose $w_{i+1}$ is the actual next word in the text. Then the {\sc cloze} procedure 
    %tosses a biased coin with $\Pr[\text{heads}] =p_i(w_{i+1})$ and if it comes up heads inserts the following cloze question here ``{\em What is the next word? (A) $w_{i+1}$ (B) something else''.} Of course, that both human and model are quite likely to choose $B$ but the expected excess cross-entropy of the model in this situation is $p_i(w_{i+1})\log p_i(w_{i+1})/q_i(w_{i+1})$. 
\end{proof}

\subsection{Skills: A Statistical View}

Language is assumed to have an underlying set  $S$ of {\em skills}. Every text-piece $t$ has an associated set of skills that are required for comprehending it. The theory allows this set of skills to be quite large ---it only needs to be (a fair bit) smaller than the number of text-pieces in the distribution (an enormous number).  

\begin{definition}[skill graph]  
A {\em skill graph} is a bipartite graph $(S, T, E)$ where nodes in $S$ correspond to skills, 
nodes in $T$ correspond to text-pieces,  and  $(s, t)$ is in the edge set $ E$ if ``comprehending'' text-piece $t$  (i.e., answering its associated cloze questions) requires using skill $s$. (See Figure~\ref{fig:skillgraph})
\end{definition}

It is important to realize that  we are interested in  quantifying the model's {\em competence} on a skill. For example, while the above definition assumes there the distribution of text-pieces includes those whose comprehension requires the skill ``anaphora resolution,''    a language model (or even human individuals!) will in general be unable to apply the  skill correctly in all text pieces. Thus ``competence on anaphora resolution'' is not $0/1$ ---instead it is quantified as the fraction of text-pieces associated with this skill whose cloze questions were correctly answered by the model. Quantifying the success rate of this (in other words, the model's capabilities) is the goal of the rest of the paper.


\iffalse \begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    % Figure removed
    \caption{Skill Graph.}
    \label{fig:skillgraph}
\vspace{-3mm}
\end{wrapfigure}
\fi 


%In this full generality no theory seems possible,  since the full skill set and the skill graph are unknown to us humans. So 
% text-piece.

%Second,  we make an assumption that the skill graph has random edges. 

The final element of our theory is that the skill-graph has  random edges, as made precise in Definition~\ref{def:nature}. To understand why this makes sense, we recall Winograd's example: {\em The city councilmen refused the demonstrators a permit because they feared violence}.  Winograd implicitly assumes that the trickiest skill needed here is pronoun/anaphora  resolution, but of course, applying that skill in this context requires other skills: understanding of causality (i.e., interpretation of ``because'') as well as world knowledge about ``city councilmen,''  ``permit,'' ``demonstrators,'' etc.  This example highlights the fact that if we were to look at random  text-pieces that require pronoun disambiguation, we would encounter random real-world scenarios, whose comprehension requires very different set of skills. Moreover, the scenarios (and hence the relevant skills) could have different probabilities of occurring in the corpus. 
%\footnote{While our framework is suggested as a plausible way to think about a diverse text corpus, it seems a particularly good match for productions of a chat agent, which has to generate short pieces of text in response to a user query.}. 

For simplicity we assume that each text-piece requires exactly $k$ skills for some $k$, and this set was drawn by iid sampling from an underlying measure on the set of skills.
(Thinking of  $k$  as a random variable is natural but will not be considered here.) 
The next definition  formalizes the above framework in form of a {\em skill cluster}. 


\begin{definition}[Degree-$k$ skill cluster] \label{def:nature} This is a skill graph $(S, T, E)$ where the collection of text pieces is generated by ``nature'' by applying the following process: pick a subset of $k$ skills via iid sampling from an underlying measure $\mu_1$ on skills, and then use a procedure {\sc gen} to create a text-piece $t$ whose comprehension requires these skills, as well as a measure $\mu_2(t)$ associated\footnote{Note that the measure on text-pieces has to have the correct marginals  e.g., the $\mu_2$-measure of all text-pieces containing a skill $s$ is $\mu_1(s)$. There are many measures satisfying this weak condition, since the number of text pieces is way larger than the number of skills.} with this text piece $t$.  Then nature uses process {\sc cloze} to add cloze prompts to test comprehension on $t$. The {\em prediction loss}  on the text-piece is the cross-entropy loss on predicting the answers to the cloze questions in it. The average prediction loss over all text-pieces is computed with respect to the measure $\mu_2()$. %with the generated text-piece.
We call the skill-graph thus produced  a {\em degree-$k$ skill cluster}.
%associated with is the skill graph  produces by this process. \qed
\end{definition}
%\noindent{\bf Note:}  
Now we formalize a simple model of what the full text corpus looks like. More complicated extensions of this framework (e.g., considering a hierarchy among corpora) are left for future work. 
\begin{definition}(Text corpus) The text corpus consists of many skill clusters (e.g., math, newspapers, science, coding, etc.) $(S, T_1, E_1), (S, T_2, E_2),\ldots$  which share the same underlying set of skills $S$ but have disjoint sets of text-pieces $T_1, T_2, \ldots$ that are generated as in Definition~\ref{def:nature}.
\end{definition} 
 





%, which have the advantage 


Definition~\ref{def:nature} allows us to define ``competence on a skill'' in the more familiar setting of statistical learning theory, specifically by letting us associate a statistical task with it. The task involves predicting answers to cloze questions in a sub-distribution of text pieces that contain that skill.  Our emergence theory will apply to the family of tasks of the next definition.

\begin{definition}[Competence on Skills] \label{defn:statsviewskill}
In the setting of  Definition~\ref{def:nature}, for each skill cluster and each skill $s \in S$ {\em statistical task  $\tau_{s}$ corresponding to $s$ and this cluster} is defined as follows. The  learner is given a text-piece  created by sampling $s_1, \ldots, s_{k-1}$  via iid sampling $(k-1)$ times from measure $\mu_1$, and applying  {\sc gen} and {\sc cloze} to the skill-tuple $(s, s_1, \ldots, s_{k-1})$ to convert it into a text piece $t$ with an associated measure $\mu_2(t)$ (but the measure is re-scaled so that the total measure of the inputs to this task $\tau_s$ is $1$). 
The {\em error rate} of the model at the statistical tasks is the  expected prediction loss  on text-pieces drawn from the above distribution.  Since error rate is between $0$ and $1$, the {\em competence} refers to $(1 - \text{error rate})$.

For every  $k'$-tuple of skills $(s_1, s_2,\ldots, s_{k'})$ (where $k' \leq k$)  the  statistical task 
$\tau_{s_1, s_2,\ldots, s_{k'}}$ corresponding to that $k$'-tuple is similarly defined. The inputs to the task are generated by completing the $k'$-tuple to a $k$-tuple $\vec{s}$ by iid sampling  of $k- k'$ additional skills from $\mu_1$  and then using {\sc gen} and {\sc cloze} to convert it into a text-piece.

Competence on the $k'$-tuple is defined just as above.
\end{definition}
\noindent{\bf Note:} The definition involves the $k$-tuple being picked by iid sampling from $\mu_1$ which, in principle, allows a skill to be picked twice. However, the probability of picking the same skill twice scales as $O(1/|S|)$. Since the set of  skills $S$ is assumed to be large, the distribution is almost the same as sampling distinct $k$-tuples of skills. The small difference of $O(1/|S|)$ between the two methods will not affect any of the random graph theory calculations. 

To illustrate with an example, if comprehending a text-piece involves $5$ skills, then that text-piece will appear in $5$ statistical tasks corresponding to individual skills, ${5 \choose 2}$ tasks corresponding to pairs of skills, and so on.  However, our method of measuring the loss incurred on these statistical tasks implicitly assumes that if the model incorrectly answered this cloze question (i.e., it assigned significant probability to the wrong answer), then that loss was incurred in {\em all} these statistical tasks. This accounting is conservative ---it ignores the possibility that a model could have perfect on skills $1$ to $4$ but still have incorrectly answered the cloze question because of, say, shaky   understanding of skill $5$. But this conservative accounting has the significant benefit of obviating the need for a mathematical formulation of what skills are, and what  it means to combine skills ---which is unformulated, as earlier noted. In summary, Definition~\ref{defn:statsviewskill}  can be thought of as a lower bound on the model's true ``competence''  individual skills. Note this notion of competence also does not capture out-of-distribution generalization (i.e. predict well when the distribution of text pieces changes). 



\section{Analysis of Emergence (uniform cluster)}
\label{subsec:emergence}

Having set up a framework for modeling skills and (via Assumption~\ref{assum:proportionalloss}) connecting them to the cross-entropy loss of the model, we have  arrived at a core mathematical issue around emergence: {\em As the  model's excess cross entropy goes down (due to scaling), this improves the model's performance on cloze tasks inserted in the test stream. 
How does this improve  competence on the skills as well as on tuples of skills --in other words, performance on the associated  cloze questions?}


This section analyzes a simple setting where the test-stream consists of a single degree-$k$ skill cluster, and the skills are uniformly distributed and so are the text-pieces---in other words, the  distributions $\mu_1$ and $\mu_2$ in Definition~\ref{def:nature} are uniform. Section~\ref{subsec:measure} will extend the analysis to the general setting.  
The calculations below only require the total number of skills to be much less than the support size of the distribution of text---in other words, the set of skills can be extremely large.

\paragraph{Key Hurdle:}  We point out the naive but incorrect way to reason about this.  Since each text piece is connected to a random $k$-tuple  of skills, say $\vec{s}$, one is tempted to reason about emergence via linearity of expectations, specifically, the following relation about prediction loss, where ``expectation'' is just average over text-pieces/skills with respect to their measure: 
\begin{equation}\label{eqn:incorrectreln}
    k \cdot E_t[\text{loss}(t)] = E_s[\text{failure rate of statistical task}~\tau_{s}].~~~ (\text{\bf Incorrect!})
\end{equation}
To see that this is incorrect,  let $Y$ be the subset of such text pieces where the model makes mistakes on cloze questions. This $Y$ depends upon the skill graph, and the unknown processes {\sc gen} and {\sc cloze} of Definition~\ref{def:nature}, which assign measure to text pieces in an unknown way that may introduce arbitrary correlations. Since the model ``saw'' part of the test stream (namely, the portion corresponding to training data) it has picked some  information about the skill cluster. Thus at the end of training, locations of errors in the test stream  --i.e., the set $Y$--- depend upon the skill-cluster, and since we lack understanding of $Y$ the analysis has to treat it as arbitrary. In other words, our analysis is allowed to assume an upper bound on the test loss, but the text-pieces on which this loss occurs form an arbitrary subset that depends upon the graph structure. In particular, (\ref{eqn:incorrectreln}) cannot be inferred. This is the key mathematical hurdle and our proof will surmount it using  random graph theory.



Let's say the model {\em makes a  mistake} on a text-piece if the total prediction loss on all the  cloze-questions of that text-piece is at least $1/2$ (which is the kind of error incurred if the incorrect answer is chosen with noticeable probability  on even a  single cloze question). If the average cross-entropy loss for the text-pieces is $\delta$ we conclude $Y$ consists of at most $2\delta$ fraction of text pieces.  The following result guarantees that statistical tasks corresponding to most skills do not assign significant probability to text pieces in $Y$ --in other words, the model has good performance on statistical tasks connected with these skills. The theorem follows from (and is a simple rephrasing of) Lemma~\ref{lem:mixing} in the appendix. 
\begin{theorem}[Basic]\label{corr:emerge1}
 Let $\alpha, \beta, \theta >0, \beta >1, \alpha \beta <1, \theta <1$ satisfy
\begin{equation} \label{eqn:mix2a}
    H(\theta) + k\theta \left( H(\beta \alpha)  - \beta \alpha  \log \frac{1}{\alpha} - (1- \beta \alpha)  \log (\frac{1}{1-\alpha})\right)<0
\end{equation} and  the distribution on skills and text pieces be uniform in the skill-cluster. Then irrespective of the details of {\sc gen} and {\sc cloze} processes, the following property holds for every subset $Y$ of text pieces  that contains at least $\theta$ fraction of text pieces:  at least $1-\alpha$ fraction of skills have at most $\beta \theta  k N_1/N_2$ edges to $Y$ (in other words, at  most $\beta$ times the number of edges a skill would be {\em expected} to have to text-pieces in $Y$).
 \end{theorem}
 Note that as the model is scaled up, $\theta$ will go down and the set $Y$ containing erroneous answers on cloze questions will shrink.  Our analysis kicks in only once  $\theta$ drops below $1$. In terms of the emergence phenomenon, this corresponds to first signs of improvement on downstream tasks once the model's loss drops below some threshold.
 
  Since edges between a skill node $s$ and set $Y$ correspond to errors in the statistical task $\tau_s$,  Theorem~\ref{corr:emerge1} is giving an upper bound on the prediction error in statistical tasks corresponding to $(1-\alpha)$ fraction of skills. 

 \begin{definition}[performance curve]
The contour plot (i.e., the boundary) of the region of $\alpha, \beta$ combinations satisfying Theorem~\ref{corr:emerge1} is called a {\em performance curve} and denoted $C_{(k,\theta)}$. A performance curve $C$ is {\em better} than another curve $C'$ if for every $\alpha, \beta$ on $C$ there is a corresponding point $(\alpha, \beta')$ on $C'$ for $\beta' >\beta$.
 \end{definition}


 Figure~\ref{fig:sub1} gives {\em performance curves}, i.e., the contour plot of the set of
$\alpha, \beta$ combinations satisfying Theorem~\ref{corr:emerge1} for a given $\theta, k$.     The horizontal axis plots $(1-\alpha)$ and the vertical axis plots $\beta \theta$, so  point $(0.8, 0.16)$ on a curve    means at least $0.8$ fraction of skills have at most $0.16$ fraction of their edges in the ``error set'' $Y$ (hence $0.84$ fraction of their edges are outside the error set). The emergence curves shift down noticeably (i.e., imply emergence of more skills) as we increase $k$. The next lemma shows this trend always holds; follows from the fact that 
$H(\theta)/\theta$ is a decreasing function in the interval $(0, 1)$. 

\begin{lemma}[Monotonicity] If $\theta' < \theta$ then the performance curve for $\theta', k$ lies below that for $\theta, k$. 

If $k' > k$ then  the performance curve of $\theta, k'$ lies  below that for $k, \theta$.
\end{lemma}


% Figure environment removed

\subsection{The tensorization argument}
While the above method yields performance curves,  better curves can be derived via a tensorization argument. 
Consider the following {\em $k'$-wise recombination} operation on the test stream. First randomly partition the test stream into subsets of size $k'$, and then concatenate  the $k'$ text pieces within each subset to create a larger piece of text that we refer to as a ``$k'$-piece,'' and whose measure  is the sum of the measures of the component test-pieces. All cloze questions for the old test-pieces are retained and no new cloze questions are inserted. Clearly, if the error of the model per average text-piece was $\delta$, then the  error per average $b$-piece is $k'\delta$. 
However, each $k'$-piece is now using  a random $k'k$-tuple of skills. Importantly, this set of $k'k$ skills consists of iid draws from the skill distribution. In other words, Theorem~\ref{corr:emerge1} now becomes the following.


\begin{corollary}[tensorization] \label{corr:emerge2} In the same setting as Theorem~\ref{corr:emerge1}, for integer $k' \in [2, 1/\theta]$ the conclusion of that theorem holds also for $\alpha, \beta$ pairs satisfying
\begin{equation} \label{eqn: tensor1}
 H(k'\theta) + kk'\theta \left( H(\beta \alpha)  - \beta \alpha  \log \frac{1}{\alpha} - (1- \beta \alpha)  \log (\frac{1}{1-\alpha})\right) < 0
\end{equation}   
Furthermore, if $H(k'\theta) < k'H(\theta) $ the emergence curve from this expression dominates that derived from Theorem~\ref{corr:emerge1}.
\end{corollary}

\subsubsection{Emergence for $k'$-tuples of skills}
\label{subsec:ktuples}
Now we estimate the model's emergence curve for statistical tasks corresponding to $k'$-tuples for $k'\leq k$.
The basic idea is to consider $k'$-tuples of skills as `composite-skills,' and then re-do the calculation.


\noindent{\bf 2nd estimate (better):} Consider the following {\em $k'$-wise recombination} operation on the test stream. First randomly partition the test stream into subsets of size $k'$, and then concatenate  the $k'$ text pieces within each subset to create a larger piece of text that we refer to as a ``$k'$-piece.''  All cloze questions for the old test-pieces are retained and no new cloze questions are inserted. Clearly, if the error of the model per average text-piece was $\delta$, then the  error per average $b$-piece is $k'\delta$. 
However, each $k'$-piece is now using  a random $k'k$-tuple of skills,   which we can alternatively view as $k$  random $k'$-tuples. Thus viewing $k'$-tuples of skills as `composite skills' we can use this as the skill set in  the setting of Theorem~\ref{corr:emerge1}, which gives us an easy corollary quantifying the   performance on tasks corresponding to $k'$-tuples of skills. % Thus the same proof yields the following.
%we have the following Corollary of Lemma~\ref{lem:mixing}.

\begin{lemma}[Emergence for $k'$-tuples of skills] \label{corr:emerge2}
     Consider the skill-graph $(S', T', E)$ where $S'$ consists of all $k'$-tuples of skills, $T'$ consists of $k'$-pieces, and $E$ consists of $(s', t')$ where $s'$ is a $k'$-tuple of skills and $t'$ is a $k'$-piece where this tuple of skills is used. Let $Y$ consist of $\theta$ fraction of $k'$-pieces. Then for any $\alpha, \beta >0, \beta >1, \alpha \beta <1$ satisfying (\ref{eqn:mix1}) there are at least $1-\alpha$ fraction of $k'$-tuples of skills that have at most $\alpha \beta \theta \theta N_1$ $\beta \theta$ fraction of their edges connected to $Y$.
\end{lemma}

The next corollary presents a somewhat surprising general principle that's also hinted at in caption of Figure~\ref{fig:mixing_lemma}. Assume (for simplicity) a Chinchilla-like scaling law that $10$x up-scaling leads to  factor $2$ reduction in excess entropy. If a model is considered to have reasonable performance on individual skills at current scaling, then after further up-scaling of $10x$ one would see similar reasonable performance on skill-pairs, and scaling up by yet another $10$x after that will yield similar reasonable performance on $4$-tuples of skills, etc. 
Note that these are {\em provable lower bounds} on performance gains---actual gains could  be higher. 
Figure~\ref{fig:mixing_lemma} illustrates the phenomenon.
%from scaling may  stronger boost on performance. 
%The alludes to this Corollary. 

\begin{corollary} \label{corr:emerge3} When the model $M_1$ with loss $\delta$ is scaled up (e.g., as per equation~(\ref{eqn:scaling}))  so that the new model $M_2$ has loss  $\delta/k'$,  then the performance curve inferred by our method for $k'$-tuples of skills using $M_2$ is identical to the curve inferred for individual skills on model $M_1$.
\end{corollary}  
\begin{proof}
    As noted above, a loss of $\delta$ still allows the model to make significant mistakes on $2\delta$ fraction of test pieces, which we denote by $\theta$. Thus Theorem~\ref{corr:emerge1} describes the performance curve for skills. 
    Making the loss drop to $\delta/k'$ but creating $k'$-pieces makes the fraction of errors $\theta =2\delta$ again. (Note that ``error'' now means an erroneous answer  on {\em any} cloze question in the entire $k'$-piece ---again, this is a conservative definition of error.) Applying Lemma~\ref{corr:emerge2} we get the same emergence curve as Theorem~\ref{corr:emerge1}.
\end{proof}


%See also Section~\ref{sec:digits} for a toy example.











\vspace{-2mm}
\section{Emergence analysis with general  measure on text and skills}
\label{subsec:measure}
\vspace{-2mm}

Now we turn to analysis of the general setting of Definition~\ref{def:nature} where text piece $t$ has measure $\mu_2(t)$ and  skill $s$ has measure $\mu_1(s)$.  In this setup, our lemma statements (e.g., Lemma~\ref{lem:mixing} as well as the ones in Sections~\ref{subsec:emergence} and \ref{subsec:ktuples}) hold -----the claim is the same but with cardinalities replaced by measure!

\begin{theorem}[Emergence of skills and $k$'-tuples of skills] \label{thm:genmeasure} Let $Y$ be any subset of text pieces consisting of text pieces with total measure $\theta$, and every text-piece has measure substantially less than $\theta$.  Let $\alpha, \beta >0, \beta >1, \alpha \beta <1$ satisfy
\begin{equation} \label{eqn:mix2a}
    H(\theta) + k\theta (H(\beta \alpha)  - \beta \alpha  \log \frac{1}{\alpha} - (1- \beta \alpha)  \log (\frac{1}{1-\alpha}))<0
\end{equation}
 Then the measure of skills that have at most $\beta \theta$ fraction of their edges connected to $Y$ is at least $1-\alpha$.

 For $k'$-tuples of skills the statement of Lemma~\ref{corr:emerge2}  holds with the same modification of cardinality to ``measure.''
 \end{theorem}
\begin{proof}
  The measure $\mu_1$ on skills is trivial to reason about  by just replacing each skill $s$ by a  number of copies that is proportional to $\mu_1(s)$. This converts the measure to a uniform measure ---specifically, $k$ iid draws  from this uniform measure are equivalent to $k$ iid  draws from the  $\mu_1$.
  
For the measure $\mu_2(\cdot)$ on texts, the above trick doesn't work. Recall that a text-piece is connected in the skill graph to a random $k$-tuple of skills. If we try to replace $\mu_2()$ with a uniform measure by replacing the text piece with identical copies, then these copies must still all connect to the {\em same}  subset of $k$ skills ---meaning these connections are correlated and not random. We need a more subtle argument. The key part in the proof of Lemma~\ref{lem:mixing} is where we
choose  random subset of text-pieces, $Y$  whose size is $\theta |T|$ and subset $Z$ of skills of size $\alpha |S|$, and then upper bound by (\label{eqn:mix1}) the expectation of the event that the latter has more than $\alpha \beta \theta k$ fraction of its edges going to
$Y$.  In presence of measure $\mu_2()$ let's pick $Y$ as follows: Independently pick text-pieces, choosing $t$ with probability $\theta \mu_2(t)$. (Note: $|Y|$ is  tightly concentrated around $\theta |T|$.) We still pick $Z$ randomly as before. Then we apply Jensen's Inquality on the same calculation to end up with the same upper bound as before. See Lemma~\ref{lem:mixing+measure} in the Appendix.
\end{proof}










\vspace{-2mm}
\subsection{Extending theory to multiple clusters}
\label{subsec:multipleclusters}
\vspace{-2mm}
Above we assumed a single skill cluster in the language. Real-life text  might contain multiple skill clusters. For example,  standard corpora must contain a large skill cluster involving pieces of  ``everyday'' text pieces 
and a set of basic language skills and  world knowledge needed to comprehend them. Smaller clusters may correspond to specialized topics, e.g., finance, science, mathematical reasoning, etc.  We assume each piece of text appears in only one cluster but skills may appear in different clusters. When each text-piece appears in a single cluster, the  analysis of Section~\ref{sec:slingshot}) continues to apply.  The overall loss is the weighted sum of measure of text in the individual clusters. Thus overall reduction in loss will drive emergence within individual clusters. But lacking any mechanistic insight, our theory cannot predict the rate at which loss decrease (and hence emergence) happens within clusters. This pertains to the point made earlier in the paper about lack of detailed study of scaling laws for different kinds of corpora, as well as for training on mixes of corpora.


%We later discuss what it might mean for a skill to appear in different clusters. 

%\begin{lemma}[Skill emergence with multiple clusters]
 %   TBD.  As loss goes down, it may go down at different rates in different clusters. Loss decrease within each cluster drives emergence of skills within that cluster.
%\end{lemma}

We leave a more fine-grained analysis, including possibly allowing hierarchical structure in clusters, for future work. As usual, simpler settings probably give the main insight. 
\iffalse 

\section{Toy Illustration} \label{sec:digits}



Our theory can explain the surprising phenomenon that the inductive bias of pretraining implies that combinations of skills emerge as naturally as the individual skills. Now we give a simple experiment illustrating such a phenomenon involving vision tasks on pretrained ViT models.

%We conducted an experiment to test the theory of emergence using a pre-trained Vision Transformer (ViT-CLIP) model on a custom dataset. The dataset was designed to assess the emergence of combinations of skills in vision tasks.

\begin{wrapfigure}{r}{0.5\textwidth}
    %\centering
    \vspace{1mm}
    % Figure removed
    \caption{Composite image setup. Number of underlying skills is $10$ and the model learns to apply all $4$ skills needed for the composite image.}
    \label{fig:4tuple}
    \vspace{-2mm}
\end{wrapfigure}



Our labeled dataset consisted of $10^3$ composite images created from random selecting four images from the MNIST dataset of handwritten digit images and putting each in one quadrant of the composite image. The composite image was assigned a fixed label, which is the label of one of the four digits, randomly selected.  This is the ``target'' label, while the remaining three digits were considered ``background'' labels not made available during training.



Supervised training on this dataset used a linear probe on top of input  embeddings of the composite images output by a pre-trained Vision Transformer (ViT-CLIP) model (pre-trained on a custom image dataset). The training used mini-batch SGD. Testing was done using held out images, which were composites constructed from  MNIST images that had not been used to create the training set. This also ensured the model faced novel composite images during evaluation. 
%The goal was to predict the label of the target digit when presented with individual composite images. A subset of the images was held out for testing to evaluate the model's generalization ability.

 %This required the model to generalize its learning to correctly identify the target digit in previously unseen images.

 


The final classifier, being softmax, can  be used to output top-$4$ labels just as easily as as top-$1$. Doing so achieved approximately $92$\% accuracy in classifying the four-digit tuples even though trained using only $10^3$ examples labeled with a single digit.  Full fine-tuning of the model yielded around $95$\% accuracy with $10^3$ labeled examples.

To understand why this happened, note that since the provided label is fixed by  randomly picking one of the four digits in the image, the  optimum softmax output should learn to give   equal logit values to labels of all four digits present in the image.
Figure~\ref{fig:4tuple} describes the skill-cluster implicit here.


\fi 


\iffalse 


Below, for $S \subseteq V_2$ we define $\rho(S) = \frac{|S|}{|V_2|}$ and for $T \subseteq V_2$ we define $\rho(T) = \frac{|T|}{|V_1|}$.
Also $\Gamma(s)$ denotes the neighbor set of $s$ in $V_1$. 

{\sc also need to write down asymptotics wrt $|V_1|, |V_2|$}

\subsection{Deriving Emergence} 

The important thing about the next lemma is that it holds for {\em every} $T$ and not just the average or most $T$ (which would not suffice to derive emergence). 
\begin{lemma} For any fixed $\beta$, and every $T \subseteq V_1$ with $\rho(T) =\beta$ the following is true for $1- \epsilon_1(\beta)$ fraction of $s \in V_2$:
\begin{equation}
    \rho(\Gamma(s) \cap T) \geq \beta - \epsilon_2(\beta)
\end{equation}
where $\epsilon_1(), \epsilon_2()$ are functions of $\beta$ as well as $|V_1|, |V_2|$ that go to $0$ as  the sizes of $V_1, V_2$ are increased. 
\end{lemma}
\begin{proof}
Mixing lemma for random bipartite graphs.
\end{proof}
\fi 




%{\sc sketch} Let $\mu(t) =$ excess cross-entropy on piece of text $t$. Scaling forces $\mu(V_2)$ down. Now apply the corollary to conclude that for almost all skills the excess cross-entropy goes down.


\iffalse \section{Statistical Formalization of Skills and Explanation of Emergence}
\label{sec:basictheory}

Our theory relies on scaling laws to ignore issues of training and generalization, and thus has the luxury of reasoning directly about test distribution, i.e., the distribution from which the training data was drawn.   Furthermore, this distribution is assumed to consist of long stream of text-pieces with piece $t$ having an associated measure $\mu(t)$. 
%(which do not need to be semantically related).  % to perform its  {\em test task} on it. 

\begin{definition}[Test stream and skill graph] Test data  consists of an arbitrarily long stream of {\em text-pieces}, each consisting of $C_{test}$ tokens, and an associated {\em test task}. Language has an underlying set $V_2$ of {\em skills}. Performing well on the test task associated with a text-piece $t$ requires using a subset of $V_2$.

The {\em skill graph} is a bipartite graph $(V_1, V_2, E)$ where nodes in $V_1$ correspond to text-pieces in the test stream,  nodes in $V_2$ correspond to skills, and $(v_1, v_2) \in E$ if solving the task in text-piece $v_1$ uses skill $v_2$. \end{definition}
Note that skill set $V_2$ is long and presumably hard to catalog  even for humans. We are interested here in skills as they might be defined using language models, of which we know even less. 



% LLMs are thought of as  can now be thought of as a bipartite graph.
%\begin{definition}[Skill graph]     \end{definition}
In this full generality (with skill set and the graph being unknown), no theory seems possible. So we make a (mild)  assumption about how the  test stream and skill graph are related. To motivate the assumptions, we return to Winograd's example: {\em The city councilmen refused the demonstrators a permit because they feared violence}. The Winograd challenge implicitly assumes that the trickiest skill needed here is pronoun/anaphora  resolution, but of course, applying that skill requires other skills: understanding of causality (i.e., interpretation of ``because'') as well as world knowledge about ``citycouncilmen,''  ``permit,'' ``demonstrators,'' etc.  We quickly realize, as Winograd did, that skills are intertwined and mixed up when they get used in a piece of text. This random mixing of skills is an essential feature captured in our model. 


  In the next definition note that we make no assumptions about process ${\mathcal N}$, nor about the measures $\mu_1, \mu_2$. For simplicity of notation,   our exposition treats a distribution on a finite set as a multiset.

\begin{definition}[Text generation assumption and degree-$k$ skill cluster] \label{def:nature} There is an underlying distribution $\mu_1$ on skills and a distribution $\mu_2$ on text pieces. There is an underlying process (``nature'') ${\mathcal N}$ that, given a subset $s$ of skills, can generate a text piece ${\mathcal N}(s)$ whose associated task requires the skills in the given subset, and also associates a measure $\mu_2(t)$ with the generated text-piece.

A {\em degree-$k$ skill cluster} is generated as follows.  Nature draws a subset of skills $s$ by sampling $k$ times independently from measure $\mu_1$. Then it produces a text-piece $t$  from the process ${\mathcal N}(v_2)$, and associates a measure $\mu_2(t)$ with this text-piece. Then the edges connecting $t$ to each of the $k$ skills in $v_2$ are added to the graph. (In particular,  every node in $V_1$ has degree $k$.)
\end{definition}

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    % Figure removed
    \caption{Skill Graph.}
    \label{fig:skillgraph}
\vspace{-3mm}
\end{wrapfigure}

Now we see that ``skill'' can be brought down to familiar setting of statistical learning theory:  a skill involves a distribution on text pieces, and ability to compute a desired answer for each text-piece drawn from the distribution. 

\begin{definition}[Statistical view of Skill]
In the degree-$k$ cluster of  Definition~\ref{def:nature}, define for each skill $v_2 \in V_2$ the {\em statistical task  corresponding to $v_2$} as follows. Randomly pick a  
$k$-tuple $s$ of skills  that contains $v_2$ (i.e., by iid sampling $(k-1)$ times from $\mu_1$), and then solve the prediction task corresponding to $t ={\mathcal N}(s)$).

For each  $k'$-tuple of skills for  $k' \leq k$ the  statistical task corresponding to that $k$'-tuple is simimlarly defined  ---sample a text-piece that relies upon those $k'$ skills, and do the statistical task on it. 
%{\em Failure rate} of the model on a skill-tuple is the average cross-entropy loss on the corresponding statistical task.
\end{definition}


%A {\em skill cluster} is a set of skills and set of pieces of text where each text involves a random combination of some subset of skills in the cluster. Thus the corresponding skill graph is a random bipartite graph. 
\iffalse 
\begin{definition}[Degree-$k$ Skill Cluster] A {\em degree-$k$ skill cluster}  consists of a skill graph $(V_1, V_2, E)$ where $V_2$ correspond to  subsets of $k$ skills appearing with some multiplicity, and for each $s \in V_2$ the text-piece in $V_2$  
\end{definition}
\fi 


%We point out that the above framework has in effect recast ``skills'' as a statistical task. 

\subsection{Connecting skills, cross-entropy, and emergence}

While the framework above is fairly general and could apply in many settings, we focus from now on on emergence phenomena in LLMs trained in the standard setting with a text corpus. 
%We first do a simplified presentation of our theory of Emergence, and flesh it out with more realistic assumptions in Sections~\ref{subsec:ktuples}~\ref{subsec:multipleclusters} and~\ref{subsec:measure}.  
Emergence is usually quantified by performance on various tasks. Our theory will concern skills that are testable using Cloze prompts introduced in Section~\ref{sec:excessentropy} (and used already in theory of LLMs~\cite{saunshiexplore20}). Many language comprehension skills --such as the Winograd task---are testable via Cloze prompts and testing using cloze prompts is also common in evaluations of language development in children~\cite{brown2020language}.%, which have the advantage 

\noindent{\bf Assumptions in our Theory:} {\em (1) Tasks associated with text-pieces in the test stream involve answering cloze questions. (2) The model's error on these tasks, when measured using cross-entropy loss (in context of prediction), tracks the overall excess cross-entropy of the model.} 

The second assumption is in effect saying that the model's gaps of understanding ---quantified as excess cross-entropy---can be extracted out via suitable cloze prompts. 

%Two questions arise: {\em (Question 1)} What are skills and who decides the complete list of skills being used in a piece of text? {\em (Question 2)} Who adds suitable cloze prompts to test the model's  capabilities at these skills? 
  Thus to understand emergence we arrive at the core mathematical issue: as scaling reduces the model's loss, how does this improve  performance on skills as well as on tuples of skills? But some technical issues arise. 
  
  First: {\em if a text-piece involves $k$ skills and the model gives the wrong answer to the cloze prompt for that text-piece, then which of the $k$ skills did it fail at?} For our theory, it will suffice to only consider the case when the model succeeded in {\em correctly} answering the prompt, in which case we assume it correctly applied  {\em all} skills that were needed in this sentence. (In other words, when the model fails,  our calculation will not need to assign blame to individual skills.) 


 Second: {\em Cloze prompts do not naturally appear in text. Who is inserting them and which skills are being tested?}  Formally, we address the above formal hurdles via  the concept of an {\em Omniscient Rater}, who is assumed to know the full catalog of skills as well as where they appear in the text.  The rater is allowed to add cloze prompts in appropriate places in the test data where those skills appear. Answering the prompts adds no excess cross-entropy for the human. The model is not penalized for cross-entropy loss on words in the cloze prompts  but it incurs cross-entropy loss for its prediction in the blank slot/multiple choice occuring in the cloze prompt. The next definition makes this precise.%is made clear in the following definition.

{\sc stopped edits here}


\begin{definition}[Test stream and Model's Failure Rate] Test data  consists of an arbitrarily long stream of {\em text-pieces}, each consisting of $C_{test}$ tokens. (The theory considers different $C_{test}$ values.)  The ominiscient rater has inserted  cloze prompts  at suitable places.  The {\em failure rate} of the model is its average cross-entropy loss at the cloze prompts.

Language is assumed to have an underlying measure $\mu()$, meaning each text-piece $t$  has an associated probability $\mu(t)$.
\end{definition}
\noindent{\bf Note:} For simplicity  our exposition below assumes a uniform measure on text; Section~\ref{subsec:measure} sketches how to  extend the analysis when a general measure exists on text-pieces and skills. 

Intuitively, the failure rate of the model measures how well it has picked up language skills that occur in natural text.

\begin{assumption}[Excess cross-entropy vs failure rate] \label{assum:proportionalloss} The average cross-entropy loss of the model per cloze question scales roughly in proportion to excess per-token cross-entropy loss on the full text.     
\end{assumption} 
\noindent{\bf Note:} This assumption intuitively says that the additional text in the cloze prompt is simple and thus unambiguous for a model that is minimally competent. The model cannot predict the cloze questions themselves,  but after it reads them it is able to understand them.  It is penalized only for its answer to the cloze questions,  which are assumed to collectively condense out the overall difficulty of understanding the text-piece. 

%\end{asummption}

%\begin{definition}[Failure rate on skill]
%Competence on a $k'$-tuple of skills is measured analogously.
%\end{definition}

\subsection{Deriving Emergence}
\label{subsec:emergence}

 By Assumption~\ref{assum:proportionalloss}, the average cross-entropy loss on the cloze questions tracks the excess cross entropy of the language model. Thus as the model is scaled up, the loss on cloze-questions will go down.  Since each text piece is connected to a random $k$-tuple of skills, one is tempted to reason about emergence via linearity of expectations, specifically, the following relation: 
\begin{equation}\label{eqn:incorrectreln}
    k \cdot E_t[\text{loss}(t)] = E_s[\text{failure rate of}~s].~~~ (\text{\bf Incorrect!})
\end{equation}
This is incorrect!  Let $Y$ be the subset of such text pieces where the model makes mistakes on cloze questions. This $Y$ depends upon the skill-cluster graph, which the model ``saw''  during training (at least, the portion corresponding to training data).  Thus the graph cannot be treated as random after $Y$ has been revealed and (\ref{eqn:incorrectreln}) cannot be infered. Our proof will surmount this mathematical hurdle using random graph theory. 

Let's say the model {\em makes a  mistake} on a text-piece if the cross-entropy loss on the included cloze-questions is at least $1/2$ (which is the kind of error associated with incorrect answer being chosen with noticeable probability  on a multiple-choice question). Since average cross-entropy loss for the average text-piece is $\delta$ we conclude $Y$ consists of at most $2\delta$ fraction of text pieces. The following result guarantees that many individual skills do not get used too often in text pieces in $Y$ --in other words, the model has good performance on statistical tasks connected with these skills.

\begin{corollary}(to Lemma~\ref{lem:mixing}) \label{corr:emerge1}
   Viewing the skill-cluster as a degree-$k$ bipartite graph, let $Y$ consist of $\theta$ fraction of text pieces.  For any $\alpha, \beta >0, \beta >1, \alpha \beta <1$ satisfying (\ref{eqn:mix1}) there are at least $1-\alpha$ fraction of skills that have at most $\beta \theta$ fraction of their edges connected to $Y$.
 \end{corollary}
Figure~\ref{fig:sub1} gives performance curves, i.e. 
$\alpha, \beta$ combinations satisfying Corollary~\ref{corr:emerge1}.  The horizontal axis plots $(1-\alpha)$ and the vertical axis plots $\beta \theta$, so  point $(0.8, 0.16)$ on a curve    means at least $0.8$ fraction of skills have at most $0.16$ fraction of their edges in the ``error set'' $Y$ (hence $0.84$ fraction of their edges are outside the error set). 
% Figure environment removed

\iffalse
\begin{wrapfigure}{r}{0.5\textwidth}
  \centering
  \vspace{-10mm}
  % Figure removed
  \caption{Performance Curves: We give $\alpha, \beta$ combinations satisfying Corollary~\ref{corr:emerge1} for $k=8$ when $\theta=0.05, 0.1, 0.2$.  The horizontal axis plots $(1-\alpha)$ and the vertical axis plots $\beta \theta$. For example, the point $(0.8, 0.16)$ on the red curve means at least $0.8$ fraction of skills have at most $0.16$ fraction of their edges in the ``error set'' $Y$.  Section~\ref{subsec:ktuples} clarifies that these curves also describe the model's performance curve for $t$-tuples of skills for for $\theta =0.05$ and $t=1, 2, 4$ respectively . Thus for example the blue curve describes performance on $4$-tuples of skills when $\theta =0.05$.}
  \label{fig:mixing_lemma}
  \vspace{-4mm}
\end{wrapfigure}
\fi

\subsection{Emergence for $k'$-tuples of skills}
\label{subsec:ktuples}
Now we estimate the model's failure rate on statistical tasks corresponding to $k'$-tuples for $k'\leq k$.

\noindent{\bf Naive estimate:} This consists of observing that a random $k$-tuple is a union of $k/k'$ random $k'$ tuples. Considering $k'$-tuples as `new-skills,' we get a skill-graph  with degree $k/k'$, and Corollary~\ref{corr:emerge1} can let us derive performance curves for $k'$-tuples. However, this is a weak and sometimes trivial estimate.

\noindent{\bf 2nd estimate (better):} Consider a {\em $t$-wise recombination} operation on the test stream. First it randomly divides the test stream into subsets of size $t$, and then it concatenates  the $t$ text pieces within each subset to create a larger piece of text that we refer to as a ``$t$-piece.''  All cloze questions for the old test-pieces are retained and no new cloze questions are inserted. Clearly, if the error of the model per average text-piece was $\delta$, then the  error per average $t$-piece is $t\delta$. 
However, each $t$-piece is now using  a random $tk$-tuple of skills,   which we can alternatively view as $k$  random $t$-tuples. Thus we can use the random graph theory to derive performance on $t$-tuples of skills. 
%we have the following Corollary of Lemma~\ref{lem:mixing}.

\begin{corollary}(Emergence for $t$-tuples of skills) \label{corr:emerge2}
     In setting of Lemma~\ref{lem:mixing} consider the bipartite random graph of degree $k$ where $V_1$ consists of $t$-pieces and  $V_2$ consists of $t$-tuples of skills.  Let $Y$ consist of $\theta$ fraction of $t$-pieces. Then for any $\alpha, \beta >0, \beta >1, \alpha \beta <1$ satisfying (\ref{eqn:mix1}) there are at least $1-\alpha$ fraction of $t$-tuples of skills that have at most $\beta \theta$ fraction of their edges connected to $Y$.
\end{corollary}

The next corollary presents a somewhat surprising general principle (alluded to in caption of Figure~\ref{fig:mixing_lemma}): if the model is considered to have reasonable performance on individual skills, then scaling it up by one order of magnitude will yield same reasonable performance on skill-pairs, and scaling up by yet another order of magnitude will yield same performance on $4$-tuples of skills, etc. (Recall that the Chinchilla law predicts roughly a factor $2$ reduction in excess cross-entropy with one magnitude of up-scaling.)
%another scaling by one order of magnitude will yield reasonable performance on quadruples, and so on. We caution 
Note that these are {\em provable bounds} on performance gains---actual gains could  be higher.
%from scaling may  stronger boost on performance. 
%The alludes to this Corollary. 

\begin{corollary} \label{corr:emerge2} When the model $M_1$ with loss $\delta$ is scaled up (e.g., as per equation~(\ref{eqn:scaling})) so that its loss drops from $\delta$ to $\delta/t$ then the performance curve inferred by our method for $t$-tuples of skills using $M_2$ is identical to the curve inferred for individual skills on $M_1$.
\end{corollary}  
\begin{proof}
    As noted above, a loss of $\delta$ still allows the model to make significant mistakes on $2\delta$ fraction of test pieces, which we denote by $\theta$. Thus Corollary~\ref{corr:emerge1} describes the performance curve for skills. 
    Making the loss drop to $\delta/t$ but creating $t$-pieces makes the fraction of errors $\theta =2\delta$ again. (Note that ``error'' now means an erroneous answer  on {\em any} cloze question in the entire $t$-piece.) Applying Corollary~\ref{corr:emerge2} we get the same emergence curve. 
\end{proof}




\iffalse 
\vspace{-4mm}
\textbf{Discussion of Paucity of stimulus.} Figure~\ref{fig:mixing_lemma} illustrates that our theory predicts interesting performance curves for statistical tasks involving a constant fraction of 4-tuples, although they are slightly behind the performance on individual skills. Moreover, according to the Scaling Law and Corollary~\ref{corr:emerge2}, after scaling by two orders of magnitude, the performance on 4-tuples of skills catches up with that of individual skills. This substantial improvement in performance on most 4-tuples of skills highlights the strong inductive bias inherent in the scaling laws. Notably, this improvement occurred with a constant factor increase in the size of the dataset, while the number of $4$-tuples of skills grows as the $4$th power of the number of skills. The model learns combinations of skills because it only observed skills being applied within random subsets of other skills. See also Section~\ref{sec:digits}.
\fi 

%Figure~\ref{fig:mixing_lemma} shows that our theory implies nontrivial performance curves on  statistical tasks corresponding to a constant fraction of $4$-tuples, while lagging somewhat behind the performance on individual skills. Furthermore,  the Scaling Law and Corollary~\ref{corr:emerge2} imply that after two orders of magnitude of scaling,  the performance on $4$-tuples of skills catches up with individual skills, . This significant better performance on many/most $4$-tuples of skills highlights the strong inductive bias implicit in the scaling laws. The improvement happened with a constant factor increase in data-set size  whereas the number of skill combinations scales as the $4$th power of number of skills.  The model learns combinations of skills  because it only sees skills being applied in context of  random subsets of other skills, and the  model has no option but to learn this ability. See Section~\ref{sec:digits} as well.

 \iffalse 
The following easy lemma shows that failure on the average skill in the cluster tracks the average cross-entropy loss on text pieces.
\fi 


%{\sc need a different term than competence since this quantity goes up with loss.}



\iffalse 

\subsection{Deriving Emergence for skills}




This section assumes  the corpus consists of a single Skill Cluster; see Section~\ref{subsec:multipleclusters} for the  case with multiple clusters. Our analysis will proceed by tracking the excess cross-entropy loss on the test stream, which by Assumption~\ref{assum:proportionalloss} gets reduced as the  model size and dataset size are scaled up. Note that Lemma~\ref{lem:lossvsfailure} already implies that average failure rate of the model improves as we scale up. But improvement in average performance in principle does not rule out large holes in the model's capabilities on the skills. For example if the excess failure rate per text-piece is $0.5$,  this does not rule out high failure rate on half the skills.  We now show that improvement happens across a broad set of skills. This uses random graph theory from Section~\ref{subsec:randomgraphs}. The hurdle is that the reduction of loss happens on an arbitrary subset of the test stream, and Lemma~\ref{lem:mix} applies to every  $Y$ and every $Z$.



%The following lemma shows that almost all skills in the cluster improve in the process. This relies upon Lemma~\ref{lem:measure}.

{\sc Lemma to be rewritten} 
 \begin{lemma} When failure rate in a skill cluster is less than $\delta$ then there is a set of size $(1-\alpha)$ fraction of skills whose failure rate is at most $(1-\beta(1- 2\delta)$,  where $(\alpha, \beta)$ satisfy (\ref{eqn:mix2}) for $\theta = 1-2\delta$.
 \end{lemma}

Now we argue that emergence happens for many $k$-tuples of skills, which for exposition we refer to as a ``meta-skill.''  (Emergence of $k'$-tuples for $k' <k$ is similarly derived.) It again relies upon Lemma~\ref{lem:mix}.  The skill-graph for $k$-tuples is completely analogous, except the  ``new skills'' now correspond to $k$-tuples of the skills. Now each text piece
only depends on a {\em single} randomly-chosen ``new skill.'' In other words, this is just the subcase $k=1$ of the skill-cluster, and Lemma~\ref{lem:measure} continues to apply. 


\begin{lemma} \label{lem:ktuple}
When the cross-entropy loss on the test stream for the skill cluster is $\theta$ then for  at least 
$1-\alpha$ fraction of $k$-tuples of skills, the failure on that $k$-tuple  is at least $\beta\theta$,
where where $(\alpha, \beta)$ satisfy (\ref{eqn:mix2}) upon setting $k=1$.
\end{lemma}

\noindent{\bf Discussion:} The theory predicts equally fast emergence on $k$-tuples and individual skills. However, part of the reason is our decision to blame the error made on a cloze task equally to all skills. 
\fi 

\vspace{-2mm}
\subsection{Allowing a measure on text and another measure on skills}
\label{subsec:measure}
\vspace{-2mm}

The above setup involved a uniform distribution on skills and on text-pieces. In practice, text pieces have different probabilities and very likely the skills too. We denote by $\mu(t)$ the probability of text-piece $t$, and by $\rho(s)$ the probability of skill $s$. Each text-piece has edges to $k$ skills, where the $k$-tuple of skills is chosen via $k$ independent draws\footnote{Concretely, this corresponds to a generative process for text where nature generates a text-piece by first randomly picking a $k$-tuple of skills via iid sampling on $\rho()$ and then generates a text piece $t$ that uses that $k$-tuple of skills, and then assigns it the measure $\mu(t)$.} according to measure $\rho(\cdot)$.  In this setup, our lemma statements (e.g., Lemma~\ref{lem:mixing} as well as the ones in Sections~\ref{subsec:emergence} and \ref{subsec:ktuples}) hold but instead of cardinality of sets we need to use their measure. 

\noindent{\bf Allowing measure on skills.} The measure $\rho$ on skills is trivial to reason about  by just replacing each skill $s$ by a  number of copies that is proportional to $\rho(s)$. This converts the measure to a uniform measure ---specifically, random draw from this uniform measure is equivalent to random draw from measure $\rho$.   

\noindent{\bf Allowing measure on texts.} For the measure $\mu(t)$ on texts, the above trick doesn't work. Recall that a text-piece is connected in the skill graph to a random $k$-tuple of skills. If we try to replace $\mu()$ with a uniform measure by replacing the text piece with identical copies, then these copies must still all connect to the {\em same}  subset of $k$ skills ---meaning we no longer have a random graph. We need a more subtle argument. 

The key part is the proof of Lemma~\ref{lem:mixing} where 
choose  random subset $Y \subseteq V_1$ of size $\theta V_1$ and subset $Z \subseteq V_2$ of size $\alpha N_2$, and then upper bound by (\label{eqn:mix1}) the expectation of the event that the latter has more than $\alpha \beta \theta k$ fraction of its edges going to
$Y$.  In presence of measure $\mu()$ let's pick $Y \subseteq V_1$ as follows: Independently pick text-pieces, choosing $t$ with probability $\theta \mu(t)$. (Note: $|Y|$ is  tightly concentrated around $\theta N_1$.) We still pick $Z$ randomly as before. Then we apply Jensen's Inquality on the same calculation to end up with the same upper bound as before. See Appendix.

\iffalse 
\begin{lemma}
If ${\mathcal T}$ is the set of all text-pieces, then each measure $\mu(t)$ on text pieces can be $(1+\epsilon)$-approximated by a weighted sum $\alpha_i \mu_i(t)$ where each $\mu_i$ is the uniform measure on some subset of ${\mathcal T}$.
\end{lemma}
\fi 
\vspace{-2mm}
\subsection{Extending theory to multiple clusters}
\label{subsec:multipleclusters}
\vspace{-2mm}
Above we assumed a single skill cluster in the language. Real-life text  might contain multiple skill clusters. For example,  standard corpora must contain a large skill cluster involving pieces of  ``everyday'' text pieces 
and a set of basic language skills and  world knowledge needed to comprehend them. Smaller clusters may correspond to specialized topics, e.g., finance, science, mathematical reasoning, etc.  We assume each piece of text appears in only one cluster but skills may appear in different clusters. The disjoint clusters of text is reminiscent of the setup considered in Section~\ref{sec:slingshot}). The overall loss is the weighted sum of measure of text in the individual clusters. Thus overall reduction in loss will drive emergence within individual clusters, but lacking any mechanistic insight, our theory cannot predict the rate at which loss decrease (and hence emergence) happens within clusters. 


%We later discuss what it might mean for a skill to appear in different clusters. 

%\begin{lemma}[Skill emergence with multiple clusters]
 %   TBD.  As loss goes down, it may go down at different rates in different clusters. Loss decrease within each cluster drives emergence of skills within that cluster.
%\end{lemma}

We leave a more fine-grained analysis, including possibly hierarchical structure in clusters, for future work. We do not attempt it here because the basic insights about emergence of skills are clearer in the simple settings discussed above. 


\section{Toy Illustration} \label{sec:digits}
Our theory can explain the surprising phenomenon that the inductive bias of pretraining implies that combinations of skills emerge as naturally as the individual skills. Now we give a simple experiment illustrating such a phenomenon involving vision tasks on pretrained ViT models.

%We conducted an experiment to test the theory of emergence using a pre-trained Vision Transformer (ViT-CLIP) model on a custom dataset. The dataset was designed to assess the emergence of combinations of skills in vision tasks.





Our labeled dataset consisted of $10^3$ composite images created from random selecting four images from the MNIST dataset of handwritten digit images and putting each in one quadrant of the composite image. The composite image was assigned a fixed label, which is the label of one of the four digits, randomly selected.  This is the ``target'' label, while the remaining three digits were considered ``background'' labels not made available during training.

\begin{wrapfigure}{r}{0.5\textwidth}
    %\centering
    \vspace{-4mm}
    % Figure removed
    \caption{Composite image setup. Number of underlying skills is $10$ and the model learns to apply all $4$ skills needed for the composite image.}
    \label{fig:4tuple}
    \vspace{-2mm}
\end{wrapfigure}

Supervised training on this dataset used a linear probe on top of input  embeddings of the composite images output by a pre-trained Vision Transformer (ViT-CLIP) model (pre-trained on a custom image dataset). The training used mini-batch SGD. Testing was done using held out images, which were composites constructed from  MNIST images that had not been used to create the training set. This also ensured the model faced novel composite images during evaluation. 
%The goal was to predict the label of the target digit when presented with individual composite images. A subset of the images was held out for testing to evaluate the model's generalization ability.

 %This required the model to generalize its learning to correctly identify the target digit in previously unseen images.

The final classifier, being softmax, can  be used to output top-$4$ labels just as easily as as top-$1$. Doing so achieved approximately $92$\% accuracy in classifying the four-digit tuples even though trained using only $10^3$ examples labeled with a single digit.  Full fine-tuning of the model yielded around $95$\% accuracy with $10^3$ labeled examples.

To understand why this happened, note that since the provided label is fixed by  randomly picking one of the four digits in the image, the  optimum softmax output should learn to give   equal logit values to labels of all four digits present in the image.
Figure~\ref{fig:4tuple} describes the skill-cluster implicit here.



\iffalse 


Below, for $S \subseteq V_2$ we define $\rho(S) = \frac{|S|}{|V_2|}$ and for $T \subseteq V_2$ we define $\rho(T) = \frac{|T|}{|V_1|}$.
Also $\Gamma(s)$ denotes the neighbor set of $s$ in $V_1$. 

{\sc also need to write down asymptotics wrt $|V_1|, |V_2|$}

\subsection{Deriving Emergence} 

The important thing about the next lemma is that it holds for {\em every} $T$ and not just the average or most $T$ (which would not suffice to derive emergence). 
\begin{lemma} For any fixed $\beta$, and every $T \subseteq V_1$ with $\rho(T) =\beta$ the following is true for $1- \epsilon_1(\beta)$ fraction of $s \in V_2$:
\begin{equation}
    \rho(\Gamma(s) \cap T) \geq \beta - \epsilon_2(\beta)
\end{equation}
where $\epsilon_1(), \epsilon_2()$ are functions of $\beta$ as well as $|V_1|, |V_2|$ that go to $0$ as  the sizes of $V_1, V_2$ are increased. 
\end{lemma}
\begin{proof}
Mixing lemma for random bipartite graphs.
\end{proof}
\fi 




%{\sc sketch} Let $\mu(t) =$ excess cross-entropy on piece of text $t$. Scaling forces $\mu(V_2)$ down. Now apply the corollary to conclude that for almost all skills the excess cross-entropy goes down.
\fi 


