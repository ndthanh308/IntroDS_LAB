\vspace{-4mm}
\section{Introduction}
	\label{sec:intro}
As language models scale up, via an increase in both the number of parameters and the size of the training datasets, they exhibit remarkable new behaviors \cite{brown2020language, ganguli2022predictability, srivastava2022beyond, wei2022emergent} ---this phenomenon is often termed {\em emergence}. Some emergent properties were noticed by early model designers and have since been confirmed through experiments with substantially larger models such as GPT \cite{brown2020language, DBLP:journals/corr/abs-2303-08774}, PaLM \cite{chowdhery2022palm} and PaLM-2 \citep{anil2023palm}. Ultimate forms of emergence are {\em in-context Learning}~\cite{brown2020language} and {\em zero-shot learning}, whereby the model can understand task instructions given as part of its input and solve the task. Attempting to explain this range of phenomena solely through a mathematical analysis of gradient-based training appears challenging, and appears to call for new thinking.

The model exhibiting new ``behaviors'' upon deployment is also of obvious interest in discussions about AI safety and alignment.  A contrarian view downplaying such concerns is that  all learned behaviors were already present  somewhere in the (massive) training corpus ---the so-called ``stochastic parrots'' view~\cite{stochparrots}.

The current paper introduces new frameworks to derive mathematical understanding of such phenomena. But first we recall (well-known) challenges  in such a line of inquiry.

\begin{enumerate}
\item The phenomenon is not well-defined! Quantifying emergence of new ``skills'' requires formulating what ``language skills'' are, which is  tricky. Formalizations using { \em Probabilistic Context-Free Grammars}, {\em Boolean logic}, {\em Combinatorial Categorial Grammars},  {\em Dependency Grammars}, {\em Gricean theories},   {\em Frame Theory} and {\em Category Theory}~\cite{chomsky57,morphologyhandbook,grice75, steedman96, coeckeetal2010,tannen94} capture essential aspects of this. But it seems difficult (perhaps impossible) to integrate all of these into a single framework and connect it to statistical frameworks undelying LLMs, namely, next-word prediction using cross-entropy loss.
    
    \item Multiple skills tend to emerge roughly {\em together}. The theory has to explain how they are connected. 
    
    \item Finally, the model appears capable of flexibly combining its various capabilities during in-context learning. (See Appendix \ref{appendix:gpt_examples} for examples.)  Prior attempts to formalize ``combinations'' got highly technical, e.g. involving category theory~\cite{}.  
\end{enumerate}

\subsection{Our contributions}  \label{subsec:contributions}

We introduce new types of mathematically rigorous ---yet elementary---frameworks and analyses that stay recognizably close to current statistical frameworks, and yet offer insights into above phenomena. Since analysis of training and generalization has proved difficult, our theory  assumes  {\em LLM Scaling Laws}, which are empirically relationships describing reduction in language modeling loss with  model scale. The ``language distribution'' is conceptualized as a distribution over finite pieces of text, called {\em text-pieces}. The following are the key elements of the theory: (i) {\em A framework for conceptualizing skills.} The theory assumes there {\em exists} a set of ground-truth skills and that  text-pieces are generated by an unknown process that takes random tuples of skills and converts them into a text-piece whose comprehension requires those skills. This is best visualized as a bipartite graph  (``skill graph'') with skills on one side and text-pieces on the other. An edge $(s, t)$ means that understanding of the text-piece $t$ requires applying skill $s$. (See Figure~\ref{fig:skillgraph}.)
The framework assumes that ``understanding'' of a text-piece is testable by simple multiple-choice (``cloze'') questions (see Section ~\ref{sec:excessentropy}) inserted (by another unknown process) into the text at test time. The model does not need to predict the content of the questions ---it  only needs to predict the answer, and it is penalized for any incorrect guesses. (ii) {\em Statistical tasks associated with skills and skill-tuples:} Having cast language understanding as ability to answer cloze questions, we can define statistical tasks corresponding to  ``skills'' and ``skill $k'$-tuples'' as follows.   ``Competence'' on the skill is a fraction between $0$ and $1$ corresponding to the model's success rate when presented with all cloze questions from text-piece selected randomly among all text-pieces adjacent to the particular skill node. Competence in $k'$-tuple of skills where $k' \geq 1$ is the ability to correctly answer cloze questions in a randomly selected text-piece that is connected to all skills in the $k'$-tuple. (iii) {\em  How competencies evolve with scaling.}  Random graph theory is used to give rates at which   competencies in skills and skill-tuples improve with scaling. It is shown that competence in skill-tuples improves almost as fast as that in individual skills. (Theorem~\ref{thm:genmeasure}.)


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    % Figure removed
    \caption{Skill Graph.}
    \label{fig:skillgraph}
\vspace{-3mm}
\end{wrapfigure}

 


\noindent{\bf {\em Poverty of the stimulus} angle:} Reasonable competence at $k'$-tuples of skills is a simple example of {\em compositional generalization}.  The phrase {\em Poverty of the Stimulus}  is often used (\cite{chomsky88})  to highlight that humans apply linguistic structures in new combinations. (See Section~\ref{sec:preliminaries}.) A similar issue also arises in our setting.  Our analysis allows the set of skills to be almost as large as the training corpus, which implies all individual skills were likely to have been seen during training. However,  the number of $k'$-tuples of skills scales as the $k'$th power of the  number of skills, which is much larger than the size of the training corpus.  Therefore if the model displays competency on even 
$10\%$ of the $k'$-tuples of skills then it must have somehow acquired competence in $k'$-tuples that were not seen during training. If we think of the $k'$-tuple of skills as a ``complex skills'' then we must conclude that the model (through some internal combinational process) has acquired many such complex skills despite not having seen that combination in training data.

\noindent{\bf Potential conceptual frameworks for next-generation AI models:} The ongoing reworking of  Language Models   into AI agents is accompanied by a shift away from the old paradigm of simply training a model to predict the next word   in a text corpus. Instead, the ``corpus'' is a carefully weighted/curated combination of  data, which could include code, math/logical reasoning, images, synthetic text, etc. Training could involve new kinds of losses.  Our conceptual framework seems applicable  to these new settings, since it is agnostic about what ``skills'' and ``text'' are, and how they compose. The analysis is also adaptable to prediction losses  other than cross-entropy.  For simplicity, we choose to describe the framework in context of vanilla language modeling.

\paragraph{Paper organization:} Section \ref{sec:preliminaries} provides a brief introduction to scaling laws, emergence, and random bipartite graphs. Section \ref{sec:excessentropy} explores the connection between the reduction in excess cross-entropy and learning.  
Sections~\ref{subsec:emergence} and~\ref{subsec:measure} give concrete results about  emergence of skills as a result of scaling. 





