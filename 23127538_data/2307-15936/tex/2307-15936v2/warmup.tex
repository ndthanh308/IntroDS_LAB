\section{Scaling Law implies Slingshot Generalization}
\label{sec:slingshot}
This section is a warmup for our theory, highlighting that the Scaling Law  implies a strong inductive bias ---learning capability that can seem shockingly high from naive application of learning theory. We use a simplistic thought experiment that assumes the Scaling Law continues to hold for text data derived from multiple sources. (This seems roughly true  in practice.) 

 Assume there are $S$ elementary skills in language, and each piece of text applies exactly one of these skills. % they are  viewed as a disjoint union of $k$ equal-sized sub-languages, where the text in the $i$th language is representative of application of the $i$'th skill. 
Then  training and test datasets consists of $S$ (roughly) equal portions, one per sub-language. 
If $D$ is the total dataset size then the size per sub-language is $D/S$. Section~\ref{sec:excessentropy} suggests that excess cross entropy roughly captures the rate of learning, so the average error on applying the {\em union} of $S$ skills on test tasks (involving any of the $S$ skills) is the per-word excess cross-entropy of the model trained on the full language, which scales as $1/D^{0.28}$ according to the Scaling Law. (While this is the average per word, for at least $1/2$ of the the sub-languages the per-word cross-entropy is at most twice of this quantity.) 
By contrast, if we were to train the model  using just data from a single skill, the scaling would be $S^{0.28}/D^{0.28}$, which is worse by a multiplicative
factor $S^{0.28}$, provided the model size is the same in all cases.   Since $S$ is presumably large, training on the full dataset gives hugely better performance on individual skills than if we trained separate models on individual skills.
We call this effect  {\em  Slingshot Generalization}\footnote{Economists might call this phenomenon {\em increasing returns to scale}.} and it requires
no mechanistic understanding of gradient descent or transformers. 

The above effect is closely related to  {\em Effective Data Transfer}, a concept used to empirically quantify efficacy of pre-training. 
Suppose we use a small labeled dataset of size $n$ for supervised  training of a classifier by fine-tuning a pretrained LLM. This usually yields final accuracy much better than training the classifier using vanilla supervised training on  the labeled dataset. The improvement can be thought of as an effective transfer of knowledge (= additional equivalent labeled data) from the model's pretraining. Our discussion here  begins to make this precise as a form of inductive bias. 

\textbf{View from Learning theory:}  Naive application of classical learning theory implies that if the model is trained on a single skill (i.e., sub-language) then its  error on downstream tasks on that skill should be at least the reciprocal of the square root of dataset size, which is $1/\sqrt{D/S}$. However, the above analysis suggests that when the model is trained on the {\em union} of $S$ sub-languages, Scaling Law makes the error on the average skill decrease as  $1/D^{0.28}$. When $S \gg D^{0.44}$, we have $1/\sqrt{D/S} \gg 1/D^{0.28}$, which suggests at first sight that the Scaling Law must violate learning theory, at least in this toy example. This inference is erroneous, however.  Learning theory carves out an exception for inductive bias of the model --- the error estimate of
$1/\sqrt{D/S}$ for training on a single sub-language assumes no inductive bias\footnote{To give a trivial example, if we were to commence training with a model that already has minimum training and test loss, then for sure its error is zero, not $1/\sqrt{D/S}.$}.
The correct conclusion is that training using the union of  $S$ datasets provides a strong inductive bias for learning on each individual dataset. 
This inductive bias presumably arises from the model's architecture and the fact that its parameters can aggregate structural information from all $S$ sub-languages, which improves learning on individual languages. (This effect is intuitively clear to humans: learning your $6$th language is easier than learning your $2$nd.)



