\section{Cross-Entropy vs Excess Cross-Entropy: A Clarifying Example} %A Clarifying example}
\label{sec:excessentropy}

Thinking about emergence and Scaling Laws, researchers sometimes get confused as follows: {\em ``The loss decreases by a tiny amount when we increase $D$ from $10^{11}$ to $10^{12}$. According to~(\ref{eqn:scaling})  this  changes cross-entropy by a tiny amount. Why does it lead to big changes in macroscopic behavior?''} 
Let us understand the flaw in this reasoning.
%begins the theory. % actually 

Language has an inherent (i.e., irreducible) cross-entropy  ---arising from existence of many possible correct choices for the next word in an average place in text---and this
is captured  by the $A$ term \footnote{Here we're assuming that as the model and dataset size tend to infinity in tandem, the model will perfectly learn the language distribution.} in (\ref{eqn:scaling}). No model, however good, can achieve lower cross-entropy than $A$. Below, we argue that rate at which the model makes (arising from its misunderstandings of the presented text) are connected to {\em excess} cross-entropy over this minimum amount, which is captured by the second and third terms of (\ref{eqn:scaling}). This excess does reduce noticeably from scaling: e.g., when $N, D$ are increased by a factor of $10$ it reduces by roughly $(10)^{0.28} \approx 2$  . %Thus scaling leads to significant reductions in excess cross-entropy!

\paragraph{Excess Cross Entropy Drives Learning:} 
 We present a key idea in our theory:  reduction in excess cross-entropy drives improvement on language tasks. 
We illustrate using a classic example  from~\cite{winograd1971procedures} that inspired the {\em Winograd Schema Challenge(WSC)}~\cite{levesque2012winograd}:

\noindent {\tt The city councilmen refused the demonstrators a permit because they feared violence.} 

Here the pronoun {\tt they} is ambiguous--- grammar rules allow it to refer to either {\tt demonstrators} or {\tt city councilmen}. Winograd pointed out that disambiguating it (i.e., anaphora resolution) requires world knowledge that is
 unavailable in the  text itself, namely that demonstrations can get violent, and city councilmen don't like violence. 
The WSC contains many such examples from varied contexts, thus testing the model's world knowledge and common sense. 

A key idea in designing testbeds for language understanding such as WSC is  the {\bf Cloze Procedure}\footnote{Although cloze procedures allow testing most language skills, they get a bit artificial for testing quality of language productions, or testing understanding of irony, because the setup requires presenting multiple choices, one of which already explains the joke to the model.}, popular also for testing language development in children~\cite{brownassessment}. To test the model's understanding of {\tt they} in this sentence, 
 we can append a {\em prompt}: {\tt Q. Who feared violence?}. This is followed by either a blank, or a choice of multiple answers:  {\tt A. city councilmen. B. demonstrators.}   For WSC examples, even though a human would be hundred percent sure of the answer, language models circa 2016 were roughly $50/50$ confused between the two options. 

 % (blank)} or,  
%\label{eqn:WSC}
%\end{eqnarray*}

In the above example, the human is $100\%$ certain of the answer, which implies their cross-entropy here is $\log 1$, namely $0$. However if the model is split $50$-$50$ between the two options this implies it has cross-entropy  $\log 2$, all of which is {\em excess cross entropy}! Given the frequency of ambiguous pronouns in usual English, one concludes that a model that has not learned pronoun disambiguation will display huge excess cross-entropy at many places in text\footnote{Note that quantifying ``excess'' cross-entropy requires  humans in the picture. It is not possible to look at the model's cross-entropy loss on the $i$th word  according to (\ref{eqn:CEloss}) and know---without asking humans--- whether it is inherent cross-entropy or excess. } and thus reductions in excess cross-entropy will tend to squeeze out such errors. 

Of course, text corpora  do not normally contain such artificial cloze questions. But one could imagine that the model's basic misunderstanding of the above type could, often, lead to prediction mistakes in neighboring text. Our theory will make this precise in Section~\ref{sec:basictheory}.

