\section{Preliminaries} 
\label{sec:preliminaries}
Language modeling follows a statistical paradigm: occurences of linguistic units (say, words, sentences, paragraphs) are assumed to fit a statistical profile, 
and thus pieces of text from a sufficiently large and diverse corpus are assumed to be samples from a probability distribution \cite{bengio2000neural}. 
Language models are trained to solve {
\em Next-word-prediction} tasks: Given, say, the past 500 words\footnote{Actual training involves breaking up words into smaller  {\em tokens}, which allows  a single model to handle all human languages,  math formulae, computer code,  etc. For simplicity, our discussion will refer to ``words.''}  in the text, predict the next word. Faced with such a task,  humans may be able to give many completions, and the modeling assumes that frequencies of these completions can be modeled via probability. 
Thus the model $M$ computes a probability $\Pr_M[w_{i+1}~|~w_{1}w_2\ldots w_{i}]$ for all possible words $w_{i+1}$'s. 
The goodness of a model is computed by its {\em cross entropy loss}, which for a sequence of words $w_1w_2\ldots w_t$ is:
\begin{equation} \label{eqn:CEloss}
\ell(M) =   - \sum_{i} \log \Pr_M[w_{i+1}~|~w_{1}w_2\ldots w_{i}] \qquad \mbox{(Cross Entropy)}
\end{equation}
Models are trained by minimizing (via gradient descent) this training loss  on a text corpus, and their goodness is computed by their {\em test loss}---evaluating the same loss expression on
a held-out text from the same corpus. Often the training corpus is so large that the model trains only once (or a few times) on each piece of text, and by the end, 
the test loss on held-out text is almost the same as the training loss.  %Furthermore, many language tasks can also be solved (possibly after adding a suitable {\em prompt}) using fill-in-the blank capability. 

\paragraph{Scaling laws:}  These empirically-derived expressions describe how test cross entropy loss on held-out data scales (in experiments) with number of model parameters (N)  and size of the dataset (D) \cite{cortes1993learning, hestness2017deep, kaplan2020scaling, bahri2021explaining}. For Chinchilla models~\cite{hoffmann2022training} the law is as follows (precise constants of  \ref{eqn:scaling}  hold only for the  specific architecture and training strategy ---even the constant  $A$  depends upon the tokenization):
\begin{equation}
L(N, D) = A + \frac{B}{N^{0.34}} + \frac{C}{D^{0.28}} \qquad  A = 1.61 \quad B = 406.4 \quad C = 410.7 \label{eqn:scaling}.
\end{equation}
%describes test loss (and also training loss, if the model is trained with one pass over the dataset) as a 
This description of macro behavior using two basic parameters ---reminiscent of 2nd Law of Thermodynamics--- will help us circumvent the need for mechanistic understanding of training. Our theory will only rely upon the general form of the equations, specifically, that the dependence is inverse polynomial in $N, D$. So it applies to other frameworks of training (e.g., overtrained models) where scaling laws have also been found. 
%The scaling law is a description of macro behavior that abstracts away micro details of parameter updates etc. 
%Although not as universally applicable as the physical laws invoked by the creators of the term, it is qualitatively indeed like, say, the  2nd Law of Thermodynamic,  which also gives macro descriptions of matter and energy independent of the underlying chemistry.

\paragraph{Emergence:}  Emergence refers to an interesting empirical phenomenon that as $D, N $ are increased together then the 
model's performance (zero shot or few-shot) on a {\em broad range} of language tasks improves in a correlated way. The improvement can appear as a quick transition when $D, N$ are plotted on a log scale (which is often the case) but it is now generally accepted that for most tasks the performance improves gradually when $D, N$ are scaled up. Thus the term {\em slow emergence} is more correct. 
Furthermore,  it is known that emergence happens at different rates for different tasks, and is often quickest for tasks where the text is plausibly close to text found in training data \cite{wei2022emergent}.
Plenty of tasks are known that stump current models, and they usually tend to be very different from what one would  find in usual text corpora.  See \citep{wei2022emergent,srivastava2022beyond} for a discussion of emergence rates of the broad range of language tasks in Big-Bench dataset. 
One might thus posit, with some justification from the above-mentioned studies,  that the emergence of skills arises from training on similar tasks that were implicitly solved while solving next-word prediction in the training dataset. This is indeed our starting point.

\paragraph{Poverty of Stimulus with respect to skill combinations:} Any expert who has conversed with popular chat agents quickly discovers that at the very least they seem to flexibly solve tasks that require {\em combinations} of simple skills.
%For instance, {\sc put an example here}
%(see Figure~\ref{fig:GPT4reply}). 
However, the number of $t$-wise combinations of elementary skills feels too large (say, with $t =4$ and the number of elementary skills is in the tens of thousands) for all of the combinations to even be present in the training dataset.  In other words, if the model indeed acquires ability to solve tasks involving $t$-tuples of skills, we are looking at the familiar {\em Poverty of the Stimulus} issue\footnote{Chomsky coined the phrase {\em Poverty of the Stimulus}  to emphasize that babies do not have enough data to learn language, and concluded that evolution must have led to a ``universal grammar'' that humans must be be born with.
%a specialized   
In our  framework, the language model (whose learning is initialized using Gaussian noise) also seems to defy paucity of stimulus with respect to skill combinations. The driver of this counterintuitive fact is the Scaling Laws; see Section~\ref{sec:slingshot}.} which we return to in Section~\ref{subsec:ktuples}.


\iffalse % Figure environment removed
\fi 
 


    










\iffalse 
\begin{definition}[Performance Curve for $\theta, k$]
The boundary of $(\alpha, \beta)$ pairs for the region  in (\ref{eqn:mix2}) is called the {\em performance curve}.
\end{definition}

    

\begin{lemma}[Mixing Lemma] \label{lem:mix}
For every positive integer $k$ and $\theta \in [0,1]$ there are $\alpha, \beta >0$ such that the following statement is true with probability almost $1$ for degree-$k$ random bipartite graphs.  For every $T \subseteq V_1$ of size $\theta N_1$  and $S \subseteq V_2$ of size at least $\alpha  |N_2|$, the fraction of edges that are incident to  $S$ and have the other end point in $T$ is at least $\beta \theta$.
%In the same setting as Lemma~\ref{lem:wellspread} with probability almost $1$ the following is true for every $\theta >0.5$ and some small constants $\alpha, \beta >0$ that depend upon $\theta$ (see Figure ?? in the appendix for details of this dependence). 
% how $\alpha, \beta$
\end{lemma}
\begin{proof}
    
\end{proof}
Figure~\ref{fig:mixing_lemma} shows for example that when $\theta = 0.8$ we can use $\alpha =0.25, \beta = 0.82$. 

\begin{lemma}[Well spread-out edges] \label{lem:wellspread} There is a fixed constant $\alpha >0$ s.t. the following is true with probability almost $1$ when $N_2$ is sufficiently large: for each $S_2 \subseteq V_2$ of size  $\alpha N_2$, the fraction of edges that are incident to lies in $[\alpha/(1+ \alpha), \alpha(1+\alpha)]$.

Consequently, for any subset $S_2$ of size $\beta |V_2|$ where  $\beta = t \alpha$, the fraction of edges incident to it lies in $[\beta/(1+\alpha), \beta(1+\alpha)]$
\end{lemma}
\fi 

\iffalse 
The next Lemma is a key part of how emergence happens. 

The following lemma extends the above calculations to the setting where there is a measure on vertices in $V_1, V_2$. This will be actually the version we use in the emergence analysis. 
Specifically,  $\mu_i()$ is a measure on vertices in $V_i$. The $k$ edges incident to $v_1 \in V_1$ are picked by picking endpoints in $V_2$ iid using measure $\mu_2()$. \footnote{The theory could be reworked with a different sampling schemes, which is omitted in interest of brevity.} 

\begin{lemma}[Version with Measure] \label{lem:measure}
In the setting of the preceding paragraph, an analogous statement holds for any fixed $\theta$, except 
the cardinalities of $Z, Y$ are replaced by their measure. 
%total measure associated with the vertices in $V_1$ that appear in these edges.
\end{lemma}
\begin{proof}(Sketch)
Putting a measure $\mu_2()$ on $V_2$ does not affect anything because it can be seen as replacing each vertex $v_2 \in V_2$ by a collection of vertices whose size is proportional to $\mu_2(v_2)$.  To handle the measure on $V_1$, discretize $\mu_1$ using powers of $(1+ \delta)$ for an arbitrarily small $\delta$, which allows the measure to be approximated by a sum of uniform measures. 
\end{proof}
%\subsection{Cross Entropy versus Excess Cross Entropy}

\fi 

%Returning to our Winograd schema example (\ref{eqn:WSC}), the human's prediction for the missing word is ``councilmen,'' and therefore humans have cross-entropy loss $\log 1$ ---i.e., zero--- at this location. However, a model that is totally confused between two options has loss $\log 2$ here, which is significantly higher than the average excess of $1/D^c$. Thus 






