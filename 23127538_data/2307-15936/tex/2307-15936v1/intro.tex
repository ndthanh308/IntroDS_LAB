\vspace{-4mm}
\section{Introduction}
	\label{sec:intro}
\begin{quotation}
{\em Teach a man to read and write, and you have put into his hands the great keys of the wisdom box.} \qquad {\bf Thomas Huxley}

    \end{quotation}
 
As language models scale up, via an increase in both the number of parameters and the size of the training datasets, they exhibit remarkable new behaviors \cite{brown2020language, ganguli2022predictability, srivastava2022beyond, wei2022emergent}.
{\em Emergence} is a term used to describe this fascinating phenomenon.
%gained significant traction in the field of Artificial Intelligence (AI) as a term used to .  
Some emergent properties were noticed by early model designers and have since been confirmed through experiments with substantially larger models such as GPT \cite{brown2020language, DBLP:journals/corr/abs-2303-08774}, PaLM \cite{chowdhery2022palm} and PaLM-2 \citep{anil2023palm}.





%(Give examples, maybe a chart of progress. Some behaviors emerge and some don't. Evidence that emergence is not a sudden phase transition.)

From a scientific perspective, ``emergence'' appears to encompass a range of phenomena rather than a singular phenomenon. Attempting to explain it solely through a mathematical analysis of gradient-based training proves to be challenging, and there have been suggestions that such a mechanistic approach may fall short ---i.e., new scientific theories may be needed. A related question --of interest in discussions about AI safety and Alignment---concerns the extent of the machine's ability to  exhibit ``new behaviors'' that it did not encounter in the training data. If all learned behaviors were already present in the (massive) training corpus ---the so-called ``stochastic parrots'' critique~\cite{??}--- then the AI safety issue becomes less pressing.

The current paper seeks better mathematical understanding of such phenomena. There are well-known challenges  with such a line of inquiry.


 



\iffalse 
% Figure environment removed
\fi 
 

\begin{enumerate}
    \item The phenomenon is not well-defined! We lack a  mathematical formulation for the set of language tasks of interest, and also a full catalog of the skills and abilities that the theory should apply to. Linguistics research suggests useful mathematical formalizations such as { \em Probabilistic Context-Free Grammars}, {\em Dependency Grammars}, {\em Gricean theories}, and {\em Frame Theory}. But it is a challenge to integrate these into a single framework and connect it to statistical learning, which underlies the cross-entropy loss used in language models.
    
    \item Explaining the co-emergence of skills: why do multiple skills tend to emerge roughly {\em together}. What connects them? 
    
    \item Finally, a theory has to explain how the ability to solve tasks combining natural skills  (whatever ``combination'' means ---note that prior attempts to formalize compositions get highly technical, e.g. involving category theory)  emerges just as naturally from the theory\footnote{For example popular chat agents can give reasonable responses to questions  meant to test this ability to combine skills, e.g., {\em Please give me a a couple lines of text that illustrate all of the following language understanding skills: Anaphora resolution, simple logical reasoning, simple understanding of physics, and understanding of sentiment.} See Appendix \ref{appendix:gpt_examples} for more examples.}. One mystery here is that the number of skill combinations seems too numerous for all of them to have been seen in the training data (this is a concrete example of the old {\em Poverty of the Stimulus} question; see Section~\ref{sec:preliminaries}).
\end{enumerate}
The current paper introduces new types of mathematically rigorous ---yet elementary---analyses that stay recognizably close to frameworks of statistical learning, and yet offer plausible  explanations for some of the above phenomena.  Mathematically, the theory imagines a bipartite graph with skills on one side and text-pieces on the other. Text-pieces are generated by an unknown process that takes random tuples of skills and converts them into a text-piece whose comprehension requires those skills, and  an unknown process that inserts multiple-choice prompts in text-pieces to test that understanding. A key assumption is that prediction loss on the cloze prompts captures average cross-entropy loss of the model. Thus ``understanding'' gets connected to prediction in a statistical task, except the statistical tasks corresponding to skills (and tuples of skills) have overlaps created by the random mixing of skills in text. 

 Even though the framework feels so general and unspecified (with several unknown processes in the picture)  the very fact of text-pieces having been produced using random combinations of skills is powerful enough (as captured by our lemmas about random graphs) to imply that reduction in cross-entropy manifests itself as improvement in competence on the basic skills (measured by success at answering cloze questions) as well as competence on {\em combinations} of basic skills. The rate of emergence is tightly governed by random graph theory, which we easily adapted to allow arbitrary measures on text and skills.
%Notably, our framework addresses the observation that emergence of skills across multiple tasks appears to be correlated, and that the emergence of competence in {\em combinations} of skills is nearly as effortless as competence in individual skills. 


\paragraph{Potential conceptual frameworks for next-generation AI models:} The ongoing reworking of  Language Models   into AI agents is accompanied by a shift away from the old paradigm of simply training a model to predict the next word   in a text corpus. Instead, the ``corpus'' is a carefully weighted/curated combination of  data, which could include code, math/logical reasoning, images,  etc. Training could involve new kinds of losses.  Our conceptual framework seems applicable  to these new settings, since it is agnostic about what ``skills'' and ``text'' are, and how they compose. The analysis is also adaptable to prediction losses  other than cross-entropy.  For simplicity, we choose to describe the framework in context of vanilla language modeling.

\paragraph{Paper organization:} Section \ref{sec:preliminaries} provides a brief introduction to scaling laws, emergence, and random bipartite graphs. Section \ref{sec:excessentropy} explores the connection between the reduction in excess cross-entropy and learning.  Section \ref{sec:slingshot} presents a key insight about scaling laws, namely that they imply a robust inductive bias for the emergence of complex skills. Section \ref{sec:basictheory} presents our conceptual (and statistical) framework for measuring skills via next-word prediction. Sections~\ref{subsec:emergence} and~\ref{subsec:measure} give concrete results about  emergence of skills as a result of scaling. Section~\ref{sec:digits} presents a brief experiment highlighting the (predicted)  phenomenon of ease of learning skill combinations that have not been seen in the training data. 



%Besides illuminating what kind of new science ---if any---might be needed, this study may also have implications for discussions on AI alignment and safety, since it provides a rough guide to model properties that should be considered predictable, and those that are surprising/unexpected.  Conceivably, it gives insight into skills emergence that could inform the next generation of training protocols for LLMs.





%This is a simple illstration of a quantitative version of Huxley's observation: expending  effort to learn to read and write  also turbocharges their ability to learn new tasks.
%happened to be included (in some instances, explicitly included by experimenters) in the training corpus. 

%It has been commonly assumed that understanding such  phenomena await development of new theory. The goal of the current article is to show that existing theoretical frameworks can begin to explain the Slingshot phenomenon in its broad outlines. 




