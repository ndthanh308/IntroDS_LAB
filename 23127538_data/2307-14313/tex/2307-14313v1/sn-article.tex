%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2022}%
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
% \usepackage[polish]{babel}
\usepackage[english]{babel}

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\usepackage{caption}
\usepackage{subcaption}

\usepackage[inline]{trackchanges}
\usepackage{hyperref}

% \usepackage[xcolor={divpdf,grey},authormarkup=none]{changes}
%\usepackage[final,xcolor={divpdf,grey},authormarkup=none]{changes}

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[LiDAR-based drone navigation with reinforcement learning]{LiDAR-based drone navigation with reinforcement learning}
%EN Control of an autonomous unmanned aerial vehicle using reinforcement learning
%\titleEN[DVS object counting]{Fast object counting with Dynamic Vision Sensor}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author{\fnm{Paweł} \sur{Miera}} \email{miera@student.agh.edu.pl}
\author{\fnm{Hubert} \sur{Szolc} \href{https://orcid.org/0000-0003-3018-5731}{% Figure removed}} \email{szolc@agh.edu.pl}
\author*{\fnm{Tomasz} \sur{Kryjak}* \href{https://orcid.org/0000-0001-6798-4444}{% Figure removed}} \email{tomasz.kryjak@agh.edu.pl}

%\affil{\orgdiv{Zespół Wbudowanych Systemów Wizyjnych, Laboratorium Systemów Wizyjnych, \\ Wydział Elektrotechniki, Automatyki, Informatyki i Inżynierii Biomedycznej}, \\ \orgname{Akademia Górniczo-Hutnicza im. S. Staszica}, \orgaddress{\street{al. Mickiewicza 30}, \city{Kraków}, \postcode{30-059}, \country{Polska}}}
\affil{\orgdiv{Embedded Vision Systems Group, Computer Vision Laboratory, Department of Automatic Control and Robotics}, \orgname{AGH University of Science and Technology}, \orgaddress{\street{al. Mickiewicza 30}, \city{Krakow}, \postcode{30-059}, \country{Poland}}}


%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%
%\selectlanguage{polish}
\abstract%{Uczenie \add{przez} wzmacnianie ma coraz większe znaczenie w~zagadnieniu sterowania robotami, a~symulacja odgrywa w~tym procesie kluczową rolę.
%W~dziedzinie bezzałogowych statków powietrznych (BSP, \remove{tak{\.z}e} dronów) obserwujemy również wzrost liczby publikowanych prac naukowych \change{zajmuj{\k a}cych si{\k e} tym zagadnieniem}{wykorzystuj{\k a}cych to podej{\' s}cie}.
%W~niniejszej pracy przygotowano system autonomicznego sterowania dronem, który ma za zadanie lecieć do przodu \add{(zgodnie z~przyj{\k e}tym uk{\l}adem odniesienia)} i~omijać napotykane w~lesie drzewa na podstawie odczytów z~obrotowego sensora LiDAR.
%Do jego przygotowania wykorzystano algorytm \textit{Proximal Policy Optimization} (PPO), stanowiący przykład uczenia przez wzmacnianie (ang. \textit{reinforcement learning}, RL).
%Do realizacji tego celu opracowano własny symulator w~języku Python.
%Przy testach uzyskanego algorytmu sterowania wykorzystano również środowisko Gazebo, zintegrowane z~Robot Operating System (ROS).
%Finalnie, przygotowane rozwiązanie zaimplementowano w~układzie eGPU Nvidia Jetson Nano i~przeprowadzono testy w~rzeczywistości.
%Podczas nich dron skutecznie zrealizował postawione zadania i~był w~stanie w~powtarzalny sposób omijać drzewa i~przelatywać przez las.}%
%\selectlanguage{english}
{Reinforcement learning is of increasing importance in the field of robot control and simulation plays a~key role in this process.
In the unmanned aerial vehicles (UAVs, drones), there is also an increase in the number of published scientific papers involving this approach.
In this work, an autonomous drone control system was prepared to fly forward (according to its coordinates system) and pass the trees encountered in the forest based on the data from a~rotating LiDAR sensor.
The Proximal Policy Optimization (PPO) algorithm, an example of reinforcement learning (RL), was used to prepare it.
A~custom simulator in the Python language was developed for this purpose.
The Gazebo environment, integrated with the Robot Operating System (ROS), was also used to test the resulting control algorithm.
Finally, the prepared solution was implemented in the Nvidia Jetson Nano eGPU and verified in the real tests scenarios.
During them, the drone successfully completed the set task and was able to repeatably avoid trees and fly through the forest.
}


%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

%\keywords{Uczenie przez wzmacnianie, Drony, Autonomiczne sterowanie, ROS, Gazebo}
\kewwords{LiDAR, Reinforcement learning, RL, Drones, Automatic control, ROS, Gazebo}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle
%\selectlanguage{polish}

\section{Introduction}\label{sec:wprowadzenie}

% Bezzałogowe statki powietrzne (BSP, ang. \textit{Unmanned Aerial Vehicle} -- UAV), zwane również dronami, można najczęściej zobaczyć w formie samolotów lub wielowirnikowców.
% Charakteryzują się one niezwykle szeroką gamą potencjalnych zastosowań:
% \begin{itemize}
%     \item inspekcja niedostępnych miejsc \citep{MANDIROLA2022102824},
%     \item transport drobnych przesyłek, na przykład szybkie dostarczanie krwi lub dostawy zaopatrzenia medycznego w~trudno dostępne tereny \cite{8701196},
%     \item monitorowanie i analiza terenu na podstawie ortofotomapy stworzonej ze zdjęć z kamery umieszczonej na dronie  \cite{CARABASSA2020110717},
%     \item monitorowanie natężenia ruchu ulicznego w~mieście \cite{8730677}.
% \end{itemize}
Unmanned aerial vehicles (UAVs), also known as drones, can most often be seen in the form of aircrafts or multi-rotors.
They are characterised by an extremely wide range of potential applications:
\begin{itemize}
    \item inspection of inaccessible areas  \cite{MANDIROLA2022102824},
    \item transport of small consignments, for example rapid delivery of blood or delivery of medical supplies to hard-to-reach areas \cite{8701196},
    \item monitoring and analysis of terrain based on an orthophoto created from images from a~camera placed on a~drone \cite{CARABASSA2020110717},
    \item monitoring of traffic congestion in the city \cite{8730677}.
\end{itemize}

% Większość z~tych przypadków przynajmniej w~pewnym stopniu wykorzystuje automatyczny system sterowania.
% W~najprostszym ujęciu polega on na locie po kolei do zadanych wcześniej punktów, których współrzędne składają się z: długości i~szerokości geograficznej, kąta obrotu oraz wysokości. 
% Wykonanie takiej misji wymaga ciągłego dostępu sterownika lotu do sygnału GPS (Global Positioning System), poprzez który możliwe jest uzyskanie globalnej pozycji pojazdu za pomocą informacji dostarczanych przez satelity orbitujące wokół Ziemi.
% W~ogólności dron może jednak znajdować się w~różnego typu zamkniętych pomieszczeniach, takich jak jaskinia \cite{DBLP}, tunel czy też magazyn lub hala.
% Wówczas nie ma on dostępu do wspomnianego sygnału GPS.
% W~takich miejscach można jednak zastosować odpowiednie sensory do określania położenia. 
% Pozwalają one na uzyskanie aktualnej pozycji względem punktu startowego, która jest obliczana na podstawie fuzji danych z czujnika inercyjnego i wizyjnego.
Most of these cases make at least some use of an automatic control system.
In the simplest view, it involves flying sequentially to preset points whose coordinates consist of: longitude and latitude, angle of rotation and altitude. 
Carrying out such a~mission requires the flight controller to have continuous access to the GPS (Global Positioning System) signal, through which it is possible to obtain the vehicle's global position using information provided by satellites orbiting the Earth.
In general, however, the drone can perform a~mission in various types of enclosed spaces, such as a~cave \cite{DBLP}, a~tunnel as well as a~warehouse or hall.
Then it does not have access to the aforementioned GPS signal.
In such places, suitable sensors can be used to determine the position. 
These allow to obtain the current position relative to the starting point, which is calculated from the fusion of data from the inertial and vision sensor.

% Obecnie w~wielu pracach naukowych rozpatruje się sterowanie dronami za pomocą uczenia przez wzmacnianie.
% Takie podejście polega na szkoleniu tak zwanego ,,agenta'' do wykonywania określonego zadania. 
% Trening ma na celu maksymalizację otrzymywanego zwrotu za wykonywanie działań w~środowisku (najczęściej symulacyjnym). 
% Wyszkolona polityka agenta może być następnie przeniesiona do rzeczywistości i~odpowiadać na przykład za sterowanie prędkościami drona w~osiach X, Y~i~Z, także w~warunkach braku sygnału GPS.
Currently, many research papers consider drone control by using reinforcement learning.
This approach involves training a~so-called 'agent' to perform a~specific task. 
The training (usually simulation-based) aims to maximise the return received for performing actions in the environment. 
The trained agent's policy can then be transferred to the real world and be responsible, for example, for controlling the drone's speeds in the X, Y and Z axes, also in the absence of a~GPS signal.

% W niniejszym artykule przedstawiamy wykorzystanie uczenia przez wzmacnianie do zadania nawigowania dronem na podstawie danych zwracanych przez obrotowy czujnik LiDAR.
% Do tego celu zastosowaliśmy algorytm Proximal Policy Optimization (PPO).
% Przypadek testowy to przelot w linii prostej (w zadanym układzie odniesienia) przez las, tak aby nie dochodziło do kolizji z drzewami.
% Agenta nauczyliśmy w specjalnie przygotowanym, prostym symulatorze, po czym zaimplementowaliśmy go w układzie Jetson Nano firmy Nvidia i przeprowadziliśmy testy w rzeczywistości.
In this paper, we present the use of reinforcement learning for the task of navigating a~drone based on data returned by a~rotating LiDAR sensor.
For this purpose, we applied the Proximal Policy Optimization (PPO) algorithm.
The test case is to fly in a~straight line (in a~given reference system) through a~forest, so that there are no collisions with trees.
We trained the agent in a~custom-built simple simulator, then implemented it in Nvidia's Jetson Nano chip and conducted real-world tests.

% Efekty przeprowadzonych badań należy uznać za satysfakcjonujące.
% Zaproponowana przez nas metoda umożliwia bowiem otrzymanie skutecznego algorytmu sterowania dronem w środowisku leśnym, charakteryzującym się znacznym nagromadzeniem przeszkód w postaci drzew.
% Co więcej, jest ona relatywnie tania i przystępna, gdyż nie wymaga wykorzystania skomplikowanych środowisk symulacyjnych ani zaawansowanych algorytmów fuzji danych z różnych czujników.  
The results of the conducted research should be considered satisfactory.
Our proposed method makes it possible to obtain an effective drone control algorithm in a~forest environment characterised by a~significant accumulation of tree obstacles.
Moreover, it is relatively cheap and accessible, as it does not require the use of complex simulation environments or advanced algorithms for the fusion of data from various sensors.

%Dalsza część artykułu została zorganizowana w~następujący sposób.
% W~rozdziale \ref{sec:literatura}~przedstawiono przegląd literatury naukowej związanej z~poruszaną tematyką.
% Szczególną uwagę zwrócono na prace, w których wykorzystano uczenie przez wzmacnianie.
% W~rozdziale \ref{sec:metoda}~omówiono zaproponowaną metodę sterowania dronem.
% Zawarto w~nim opis użytego środowiska symulacyjnego, szczegóły opracowanych agentów oraz specyfikację systemu sprzętowego, który wykorzystano do testów w~rzeczywistości.
% Rozdział \ref{sec:rezultaty}~prezentuje rezultaty uzyskane za pomocą zaproponowanej metody sterowania.
% Opisano w~nim zarówno wyniki uczenia w~symulacji, jak i~efekty przeniesienia systemu do rzeczywistości.
% Ostatni rozdział \ref{sec:podsumowanie}~podsumowuje całość artykułu i~wskazuje plany dalszego rozwoju pracy.
The rest of the paper is organised as follows.
Section \ref{sec:literatura} provides an overview of the scientific literature related to the addressed subject.
We paid particular attention to works that used reinforcement learning.
Section \ref{sec:metoda} includes a~description of our proposed drone control method.
We include there details of the used simulation environment, the developed agents and the specification of the hardware system we used for the real-world tests.
Section \ref{sec:rezultaty} presents the results obtained with the proposed control method.
It describes both the outcome of training in simulation and the effects of transferring the system to reality.
The final Section \ref{sec:podsumowanie} summarises the paper and indicates plans for further development of the work.

% ========================================================================================
\section{Related work}\label{sec:literatura}
% Wykorzystanie uczenia przez wzmacnianie do zadania sterowania różnymi robotami stanowi obecnie bardzo popularne podejście.
% Jest to odzwierciedlone relatywnie dużą liczbą nowych prac naukowych dotyczących tego zagadnienia.
The use of reinforcement learning for the task of controlling different robots is currently a~very popular approach.
This is reflected in the relatively large number of new research papers on the subject.

% W artykule \citep{slm_lab} przedstawiono porównanie wyników poszczególnych algorytmów uczenia przez wzmacnianie w różnych środowiskach. 
% Zostały one podzielone na takie, w których występowało sterowanie dyskretne (wektor akcji może przyjmować tylko określone wartości) i ciągłe (wartości wektora akcji mogą być dowolne). 
% W obu przypadkach algorytm PPO osiągał bardzo wysokie wyniki, ustępując tylko SAC (Soft Actor Critic) w środowiskach ciągłych, który to jednak potrzebował dłuższego czasu treningu w celu osiągnięcia dobrych rezultatów.
The article \cite{slm_lab} presents a~comparison of the results of different reinforcement learning algorithms in various environments. 
They were divided into those involving discrete control (the action vector can only have certain values) and continuous control (the values of the action vector can be arbitrary). 
In both cases, the PPO algorithm performed very well, second only to SAC (Soft Actor Critic) in continuous environments, which, however, needed longer training times to achieve good results.

% W artykule \citep{9537063} opisano natomiast zastosowanie uczenia przez wzmacnianie do zadania bezpiecznej zmiany pasa przez samochód w sytuacji ruchu wielu pojazdów na drodze. 
% W tym celu przygotowano symulację w narzędziu Unity oraz wyszkolono algorytmy PPO i SAC. 
% W ostatecznym porównaniu, skuteczność obu algorytmów wynosiła ponad 90\%, ale czas szkolenia pierwszego z nich był znacznie krótszy.
The paper \cite{9537063} describes the application of reinforcement learning to the task of safe lane changing by a~car in a~multi-vehicle traffic situation on the road. 
For this purpose, a~simulation was prepared in the Unity tool and the PPO and SAC algorithms were trained. 
In the final comparison, the effectiveness of both algorithms was more than 90\%, but the training time of the former was much shorter.

% Uczenie przez wzmacnianie ma również coraz szersze zastosowania w dziedzinie dronów, co opisano w artykule przeglądowym \citep{przeglad}.
% Autorzy wykazują w nim wzrost zainteresowania wykorzystaniem tych algorytmów do sterowania prędkością i wysokością lotu wielowirnikowców.
% Zwracają oni jednak również uwagę na problemy związane z przenoszeniem takich systemów z symulacji do rzeczywistości. 
% Zaliczają się do nich ograniczenia sprzętowe oraz różnice pomiędzy tymi dwoma środowiskami (sim-to-real gap).
Reinforcement learning also has increasing number of applications in the field of drones, as described in the review article \cite{przeglad}.
Its authors show a~growing interest in using these algorithms to control the speed and altitude of a~multi-rotor aircraft.
However, they also highlight the problems associated with transferring such systems from simulation to reality. 
These include hardware limitations and the differences between the two environments (sim-to-real gap).

% Artykuł \citep{ladowanie} przedstawia przygotowanie systemu, którego misją było wylądowanie dronem na ruchomej platformie. 
% Zadanie to zostało wykonane poprzez użycie agenta RL, dla którego obserwacjami były aktualne stany pojazdu. 
% Po przeprowadzeniu treningu w symulacji, został on uruchomiony na stacji naziemnej, a sygnały sterujące były przesyłane poprzez sieć Wi-Fi. 
% Skuteczność działania została potwierdzona zarówno w środowisku symulacyjnym, jak i w rzeczywistości.
The paper \cite{ladowanie} presents the development of a~system whose mission was to land a~drone on a~moving platform. 
This task was accomplished by using an RL agent for which the observations were the current states of the vehicle. 
After training in the simulation, it was launched at the ground station and control signals were transmitted via Wi-Fi network. 
The effectiveness of the operation was confirmed both in the simulation environment and in reality.

% Kolejnym przykładem wykorzystania uczenia przez wzmacnianie do sterowania dronami jest praca przedstawiona w artykule \citep{racing}, w którym autorzy zaprezentowali sposób wyznaczania trajektorii przelotu drona przez bramki w zmieniającym się środowisku. 
% Zadanie to zostało wykonane poprzez trening agenta algorytmem PPO w symulatorze Fligthmare. 
% Polityka była następnie testowana na tysiącu przelotów, a ostateczny współczynnik nieudanych prób dla najlepszej sieci wynosił 0.6\%.
% Jedna z wygenerowanych trajektorii została również uruchomiona na rzeczywistym dronie, jednak w trakcie przelotu wystąpiły duże błędy w śledzeniu pozycji.
Another example of the use of reinforcement learning for UAV control is the work presented in paper \cite{racing}, in which the authors demonstrated how to determine the trajectory of a~drone's flight through gates in a~changing environment. 
This task was performed by training an agent with the PPO algorithm in the Fligthmare simulator. 
The policy was then tested on one thousand flights, and the final failure rate for the best network was 0.6\%.
One of the generated trajectories was also run on an actual drone, but there were large errors in position tracking during the flight.

% Analiza wyżej wymienionych artykułów pokazała, że algorytm PPO charakteryzuje się dobrymi wynikami oraz akceptowalnym czasem uczenia.
% Dlatego też zdecydowano się go wykorzystać w~prowadzonych badaniach.
Analysis of the above-mentioned articles showed that the PPO algorithm has good performance and acceptable learning time.
Therefore, we decided to use it in our research.

% ========================================================================================
\section{Proposed method}\label{sec:metoda}
% Jak już wspomniano, niniejsza praca koncentruje się na wykorzystaniu uczenia przez wzmacnianie do zadania sterowania dronem.
% Rozpatrzony problem dotyczy przelotu przez uprzednio nieznane środowisko przestrzenne w postaci lasu.
% Zaproponowaną metodę postępowania można podzielić na trzy etapy:
% \begin{enumerate}
%     \item Przygotowanie środowiska symulacyjnego.
%     \item Opracowanie agentów realizujących postawione zadanie.
%     \item Implementacja uzyskanego algorytmu sterowania w~systemie sprzętowym.
% \end{enumerate}
This paper focuses on the use of reinforcement learning for the task of controlling a~drone.
The considered problem concerns the flight through a~previously unknown spatial environment in the form of a~forest.
The proposed approach can be divided into three steps:
\begin{enumerate}
    \item Preparation of the simulation environment.
    \item Development of the agents performing the stated task.
    \item Implementation of the obtained control algorithm in the hardware system.
\end{enumerate}

% Szczegóły realizacji każdego z nich przedstawiamy w dalszej części niniejszego rozdziału.
% Przy realizacji systemu założyliśmy, że system dron mający omijać drzewa w lesie będzie otrzymywał informacje o otoczeniu za pomocą sensora RPLIDAR A2 \citep{RPLIDAR}.
% Jest to urządzenie, które wykorzystuje czujnik laserowy na obrotowej podstawie do mierzenia odległości. 
% Wynikiem jego działania jest konturowy skan środowiska, składający się z odległości do najbliższych obiektów dla określonych kątów obrotu.
% Zasięg wynosi 16 metrów, zaś rozdzielczość kątowa 0.225\textdegree{}.
We provide details of the realisation of each of these later in this section.
For the implementation of the system, we assumed that the drone receives information about its surroundings using the RPLIDAR A2 sensor \cite{RPLIDAR}.
This is a~device that uses a~laser sensor on a~rotating base to measure distance. 
Its result is a~contour scan of the environment, consisting of distances to the nearest objects for specific angles of rotation.
The range is 16 metres, while the angular resolution is 0.225\textdegree{}.

\subsection{Simulation environment}\label{subsec:symulator}

% W celu jak najlepszego wyszkolenia agentów, zdecydowaliśmy się na wykorzystanie dwóch symulatorów.
% Pierwszy z nich, przygotowany od podstaw w języku Python, miał na celu wsparcie procesu uczenia.
% Drugi symulator wykorzystaliśmy natomiast do wprowadzenia korekt algorytmu, niezbędnych przy przeniesieniu systemu do układu sprzętowego.
% Do tego celu zastosowaliśmy gotowe środowisko Gazebo Simulator.
In order to train the agents in a~best possible way, we decided to use two simulators.
The first one, prepared from scratch in Python, was intended to support the learning process.
The second simulator was used to make the adjustments to the algorithm needed to transfer the system to a~hardware chip.
For this, we used the off-the-shelf Gazebo Simulator environment.

% Implementację pierwszego symulatora rozpoczęliśmy od stworzenia klasy drona jako obiektu poruszającego się ruchem zmiennym. 
% Metodę obliczenia przyspieszenia przedstawiamy we wzorze \eqref{eq:drone_acc} -- jego wartość ograniczyliśmy do zakresu $[-max\_acc, max\_acc]$. 
% Aktualną prędkość i pozycję drona obliczamy odpowiednio ze wzorów \eqref{eq:drone_speed} i \eqref{eq:drone_pos} (analogicznie dla każdej osi prostokątnego układu współrzędnych).
We started the implementation of the first simulator by creating a~drone class as an object that moves in a~changing manner. 
We present the method for calculating acceleration in Equation \eqref{eq:drone_acc} -- we restrict its value to the range $[-max\_acc, max\_acc]$. 
We calculate the current speed and position of the drone from Equations \eqref{eq:drone_speed} and \eqref{eq:drone_pos}, respectively (similarly for each axis of the rectangular coordinate system).

\begin{equation}
    \label{eq:drone_acc}
    a = \frac{v_{req} - v_0}{dt}
\end{equation}

\begin{equation}
    \label{eq:drone_speed}
    v = v_0 + a * dt
\end{equation}

\begin{equation}
    \label{eq:drone_pos}
    x = x_0 + v * dt + \frac{a * dt^2}{2}
\end{equation}
where: $a$ -- acceleration [m/s$^{2}$], $v_{req}$ -- target velocity [m/s], $v_0$ -- velocity in the previous simulation step [m/s], $dt$ -- time of one simulation step [s], $v$ -- actual velocity [m/s], $x$ -- actual position [m], $x_0$ -- position in the previous simulations step [m].

% Współczynniki w równaniach \eqref{eq:drone_speed} i \eqref{eq:drone_pos} dobraliśmy tak, aby przy takim samym wymuszeniu zmiana pozycji i prędkości w opracowanym symulatorze drona była jak najbardziej podobna do wartości otrzymanych z Gazebo. 
% Przykład uzyskanych w ten sposób rezultatów na wykresie dla osi Y przedstawiamy na rysunkach \ref{img:p_my_gaz} i \ref{img:v_my_gaz}.
% Pomimo zauważalnych różnic, uzyskana dokładność okazała się wystarczająca do dalszych eksperymentów.
We selected the coefficients in Equations \eqref{eq:drone_speed} and \eqref{eq:drone_pos} so that, for the same enforcing, the change in position and velocity in the developed drone simulator is as similar as possible to the values obtained from Gazebo. 
An example of the results obtained in this way on the Y-axis is shown on the graph in Figures \ref{img:p_my_gaz} and \ref{img:v_my_gaz}.
Despite the noticeable differences, the obtained accuracy proved sufficient for further experiments.

% Figure environment removed

% Istotną funkcjonalność opracowanego przez nas symulatora stanowi możliwość generowania wirtualnego lasu.
% Na rysunku \ref{img:grid} przedstawiamy losową mapę z naniesioną siatką o boku długości 16 metrów. 
% Do każdej z komórek zostały przypisane znajdujące się w niej drzewa.
% Ich atrybuty stanowią współrzędne położenia środka oraz promień konara.
% Zastosowaliśmy podział na odrębne sektory, aby w trakcie szukania drzew znajdujących się w zasięgu lasera, uwzględnić tylko te należące do aktualnej komórki drona (kolor czarny) oraz komórek sąsiadujących (kolor niebieski). 
% Pozostała część siatki (w kolorze czerwonym) nie jest uwzględniana w danym kroku obliczeń, co pozwala na ich przyspieszenie.
An important functionality of our simulator is the ability to generate a~virtual forest.
In Figure \ref{img:grid}, we show a~random map with an applied grid with a~side length of 16 metres. 
The trees within it were assigned to each cell.
Their attributes are the coordinates of the centre position and the radius of the branch.
We applied the division into separate sectors so that, when searching for trees within the range of the laser, only those belonging to the current drone cell (black) and neighbouring cells (blue) are taken into account. 
The rest of the grid (in red) is not taken into account in a~given calculation step, thus speeding up the computation.

% Figure environment removed

% Uruchomienie symulacji rozpoczyna się od inicjalizacji zmiennych i stworzenia obiektu drona z zerowymi wartościami pozycji i prędkości. 
% Następnie w każdej komórce siatki losowo generowane są drzewa, z uwzględnieniem następujących parametrów:
Simulation launch starts with the initialisation of variables and the creation of a~drone object with zero values for position and velocity. 
Trees are then randomly generated in each grid cell, taking into account the following parameters:
\begin{itemize}
    \item \texttt{trees\_per\_grid} -- specifies the number of trees that can be located in one grid cell (a~value between 30 and 60, drawn after each iteration);
    \item \texttt{tree\_radius\_range} -- defines the range of tree radii in metres;
    \item \texttt{trees\_min\_distance} -- determines the minimum distance between the generated trees.
\end{itemize}
% Wartości współczynników dobrano w~taki sposób, aby symulacja przypominała jak najbardziej rzeczywisty las.
% Jeśli oprogramowanie nie potrafiło znaleźć nowej lokalizacji dla kolejnej przeszkody przez 1000 prób, pętla była przerywana, przez co w~aktualnej siatce znajdowało się mniej drzew.
We selected the values of the coefficients in such a~way that the simulation resembled the real forest as much as possible.
If the software could not find a~new location for the next obstacle for 1000 trials, the loop was interrupted, resulting in fewer trees in the current grid.

% Przygotowaliśmy również symulację urządzenia RPLIDAR.
% Otrzymywane w jej ramach pomiary uzyskaliśmy poprzez znalezienie punktów wspólnych równania prostej i okręgu.
% Do tego celu wykorzystaliśmy fakt przechodzenia prostej przez punkt $P_0$, którego współrzędne były równe (co do wartości) różnicy pomiędzy pozycją drona i środka drzewa.
% Opracowana implementacja wspomnianych równań w języku Python odrzuca również niechciane rozwiązania wynikające z przecięcia okręgu w dwóch punktach oraz tej samej wartości tangensa dla kątów przesuniętych o wielokrotność $\pi$.
We also prepared a~simulation of the RPLIDAR device.
We obtained the resulting measurements by finding the common points of the equations of the straight line and the circle.
For this purpose, we used the fact that the straight line passes through the point $P_0$, whose coordinates were equal (in value) to the difference between the position of the drone and the centre of the tree.
The developed Python implementation of the aforementioned equations also rejects unwanted solutions resulting from the intersection of the circle at two points and the same tangent value for angles shifted by multiples of $P_0$.

% Finalny wygląd okna symulacji przedstawiono na rysunku \ref{img:drone_game}.
% Przygotowany symulator został wykorzystany do wyszkolenia agentów RL (podrozdział \ref{subsec:RL}).
% Jego kod źródłowy został także udostępniony w~repozytorium Github\footnote{\url{https://github.com/vision-agh/python_drone_game_laser_scan}}.
The final design of the simulation window is shown in Figure \ref{img:drone_game}.
We used the prepared simulator to train RL agents (Section \ref{subsec:RL}).
We also made its source code available in the Github repository\footnote{\url{https://github.com/vision-agh/python_drone_game_laser_scan}}.

% Figure environment removed

% Przy przenoszeniu algorytmów na platformę sprzętową wykorzystaliśmy również środowisko Gazebo.
% Sterowanie dronem odbywa się w nim poprzez komunikację z trybem SITL (software-in-the-loop) oprogramowania PX4.
% Dzięki temu kod przygotowany w symulacji może później działać bez zmian na rzeczywistym dronie. 
% Całość jest również zintegrowana z systemem ROS (Robot Operating System), co ułatwia komunikację między wszystkimi dodanymi sensorami.
We also used the Gazebo environment when transferring the algorithms to the hardware platform.
The drone is controlled there by communicating with the SITL (software-in-the-loop) mode of the PX4 software.
This allows the code prepared in the simulation to then run unchanged on the actual drone. 
The whole system is also integrated into the ROS (Robot Operating System), which facilitates communication between all the added sensors.

% W celu wykorzystania symulatora Gazebo w omawianym problemie, przygotowaliśmy specjalną mapę. 
% Drzewa są na niej generowane jako wysokie walce o różnych promieniach.
% W przypadku drona i czujnika RPLIDAR, wykorzystaliśmy natomiast gotowe modele, dostępne wraz z oprogramowaniem PX4.
% W domyślnym pliku konfiguracyjnym zmieniliśmy jednak niektóre parametry, takie jak rozdzielczość, częstotliwość próbkowania, wysokość względem podstawy oraz zasięg lasera.
% Przygotowane okno symulacji przedstawiamy na rysunku \ref{img:gazebo_lidar}.
In order to use the Gazebo simulator in the discussed problem, we prepared a~special map. 
The trees are generated on it as tall cylinders with different radii.
For the drone and the RPLIDAR sensor, we used the ready-to-use models available with the PX4 software.
However, we changed some parameters in the default configuration file, such as resolution, sampling frequency, height relative to the base and laser range.
The prepared simulation window is shown in Figure \ref{img:gazebo_lidar}.

% Figure environment removed

% ========================================================================================

\subsection{Developed RL agent}
\label{subsec:RL}

% Do sterowania dronem wykorzystaliśmy algorytmy uczenia przez wzmacnianie. 
% Ogólnie takie podejście polega na tym, że tak zwany agent wykonuje pewne działanie w~środowisku na podstawie otrzymywanych obserwacji i wartości nagrody.
% Równocześnie zmienia w ten sposób stan środowiska, co wpływa na jego kolejne decyzje.
% Agent podejmuje je za pomocą polityki, czyli funkcji, która na podstawie obserwacji zwraca określoną akcję. 
% Implementuje się ją często jako sieć neuronową, której parametry są zmieniane w trakcie treningu. 
% Jej architektura może składać się z kilku warstw w pełni połączonych, jednak w niektórych środowiskach zastosowanie mają również warstwy rekurencyjne (posiadają one pamięć, przez co wcześniejsze obliczenia mają wpływ na aktualny wynik). 
% W przypadku, gdy obserwacją jest obraz, model sieci neuronowej może zawierać w sobie również warstwy konwolucyjne oraz pooling.
We used reinforcement learning algorithms to control the drone. 
In general, this approach involves a~so-called agent performing some action in the environment based on the observations it receives and the value of the reward.
At the same time, it changes the state of the environment in this way, which influences its subsequent decisions.
The agent takes these decisions by means of a~policy, i.e. a~function that returns a~certain action based on the observations. 
It is often implemented as a~neural network whose parameters are changed during training. 
Its architecture may consist of several layers that are fully connected, but in some environments recursive layers are also applicable (they have memory, so that previous calculations affect the current result). 
When the observation is an image, the neural network model may also include convolutional and pooling layers.

% Najprostszym algorytmem uczenia przez wzmacnianie jest Q-learning.
% Wybór kolejnych wykonywanych akcji odbywa się w nim przy wykorzystaniu tak zwanej Q-Table.
% Znajduje się w niej zwrot (zdyskontowana suma nagród), który zostanie potencjalnie otrzymany po wykonaniu danej akcji w konkretnym stanie i podejmowaniu dalszych decyzji zgodnie z wykorzystywaną polityką.
% Jest to tzw. action-value function.
% Wartości komórek Q-table są aktualizowane iteracyjnie za pomocą równania Bellmana \cite{bellman}. 
% Q-learning oddziałuje na środowisko za pomocą innej polityki niż ta, która jest przez niego optymalizowana. 
% Pewną modyfikację tego podejścia stanowi algorytm SARSA (State–Action–Reward–State–Action).
% W nim jedna i ta sama polityka jest zarówno aktualizowana, jak i wykorzystywana do wykonywania akcji.
% Na podstawie omówionych podejść powstały popularne współcześnie algorytmy uczenia przez wzmacnianie:
The simplest algorithm for reinforcement learning is Q-learning.
The selection of the next action to be performed in it is done using a~so-called Q-Table.
This contains the return (discounted sum of rewards) that will potentially be received after performing a~particular action in a~particular state and making further decisions according to the used policy.
This is known as the action-value function.
The values of the Q-Table cells are updated iteratively using the Bellman equation \cite{bellman}. 
Q-learning interacts with the environment using a~different policy to the one it is optimising. 
A~certain modification of this approach is the SARSA (State-Action-Reward-State-Action) algorithm.
One and the same policy is both updated and used in it to perform an action.
Popular modern reinforcement learning algorithms have been developed on the basis of the discussed approaches:
\begin{itemize}
    \item Deep Q~Network (DQN),
    \item Proximal Policy Optimization (PPO),
    \item Soft Actor Critic (SAC),
    \item Trust Region Policy Optimization (TPRO),
    \item Advantage Actor Critic (A2C).
\end{itemize}

% Na podstawie przeglądu literatury (rozdział \ref{sec:literatura}), w niniejszej pracy zdecydowaliśmy się wykorzystać algorytm PPO. 
% Na początku założyliśmy, że celem drona jest przelecenie 30 metrów przez las wzdłuż osi X (w~zadanym układzie odniesienia), bez zderzenia się z żadnym drzewem. 
% Jedynymi informacjami, które otrzymywał agent PPO były odczyty z obrotowego sensora LiDAR. 
% Obserwacje nie zawierały więc w sobie żadnych danych o aktualnej prędkości ani pozycji drona. 
% Były one przetwarzane przez politykę agenta, którą zaimplementowaliśmy poprzez sieć neuronową.
% W celu dopasowania obserwacji do przyjmowanego przez nią zakresu liczb $<-1; 1>$, zdecydowaliśmy się na normalizację zgodnie ze wzorem \eqref{eq:normalization}:
Based on a~literature review (Section \ref{sec:literatura}), in this work we decided to use the PPO algorithm. 
To begin with, we assumed that the goal of the drone was to fly 30 metres through the forest along the X-axis (in a~given coordinate system), without colliding with any tree. 
% The only information the PPO agent received was readings from the rotating LiDAR sensor.
The PPO agent received only readings from the rotating LiDAR sensor.
Therefore, the observations did not include any data on the drone's current speed or position. 
They were processed by the agent's policy, which we implemented through a~neural network.
In order to fit the observations into the range $<-1; 1>$ that it accepts, we decided to normalise them according to Equation \eqref{eq:normalisation}:
\begin{equation}
    \label{eq:normalisation}
    r = (r - h_2) / h_2
\end{equation}
where: $r$ -- distances from the rotating LiDAR [m], $h_2$ -- half of the maximum LiDAR range [m].

% Opracowaliśmy również funkcję nagrody, która stanowiła sumę następujących składników:
We also developed a~cost function that was the sum of the following components:
\begin{itemize}
    \item penalty for approaching a~tree at a~distance of less than 0.15 m --- value: $-0.25$;
    \item penalty for colliding with a~tree --- value: $-1.5$;
    \item penalty for moving away from the centre-line of the environment (relative to the Y-axis), the value of which was $-0.1 * \|p_y\|$, where $p_y$ is actual drone's Y~coordinate;
    \item reward for the high speed in the X~axis ($v_x$), calculated as $0.8 * v_x$ --- in case the drone was flying in the wrong direction, its value was multiplied by $-3$.
\end{itemize}
% W roli zarówno polityki, jak i funkcji wartości, wykorzystaliśmy multilayer perceptron (MLP).
% W obu przypadkach składał się on z dwóch warstw ukrytych z aktywacją ReLU (rectified linear unit).
% Ich rozmiary wynosiły 128 neuronów dla polityki oraz 256 dla funkcji wartości.
In the role of both policy and value function, we used a~multilayer perceptron (MLP).
In both cases, it consisted of two hidden layers with ReLU (rectified linear unit) activation.
Their sizes were 128 neurons for the policy and 256 for the value function.

% Szkolenie agenta PPO realizowaliśmy poprzez skrypt napisany w języku Python. 
% Za jego pomocą najpierw inicjalizowaliśmy przygotowany symulator (sekcja \ref{subsec:symulator}) oraz model agenta RL, a następnie uruchamialiśmy trening dla zadanej liczby epizodów.
We implemented the training of the PPO agent through a~script written in Python. 
We first initialised the prepared simulator (Section \ref{subsec:symulator}) and the RL agent model with it, and then ran the training for a~set number of episodes.

\subsection{Hardware system}

% Po przeprowadzeniu procesu uczenia, gotowego agenta RL zaimplementowaliśmy w układzie sprzętowym Jetson Nano, który pełnił rolę komputera pokładowego.
% Komunikował się on z kontrolerem lotu Black Cube, do którego wgraliśmy oprogramowanie PX4.
% Wymiana danych pomiędzy tymi urządzeniami odbywała się poprzez port szeregowy z użyciem konwertera USB-UART.
% Wykorzystaliśmy ponadto wspominany RPLIDAR A2, a także kamerę Intel Realsense T265.
% Ostatnie z wymienionych urządzeń miało dostarczać informację o aktualnej lokalizacji pojazdu wymaganą przez kontroler lotu Black Cube.
% Wszystkie komponenty komunikujące się z komputerem pokładowym prezentujemy na rysunku \ref{img:components}.
After the training process, we implemented the final RL agent in a~Jetson Nano hardware chip, which acted as the flight computer.
It communicated with the Black Cube flight controller, to which we uploaded the PX4 software.
Data exchange between these devices took place via a~serial port using a~USB-UART converter.
In addition, we used the aforementioned RPLIDAR A2, as well as the Intel RealSense T265 camera.
The latter device was used to provide information on the current location of the vehicle required by the Black Cube flight controller.
All the components communicating with the on-board computer are shown in Figure \ref{img:components}.

% Figure environment removed

% Podczas implementacji algorytmów sterowania wykorzystaliśmy systemy operacyjne Ubuntu 18.04 oraz ROS Noetic (który uruchomiliśmy w~specjalnie przygotowanym kontenerze Docker).
% Dzięki temu zapewniliśmy wysoki stopień podobieństwa środowiska programistycznego na docelowej platformie sprzętowej względem wykorzystywanego symulatora Gazebo.
% Przyczyniło się to do ułatwienia całego procesu migracji.
During the implementation of the control algorithms, we used the Ubuntu 18.04 operating systems and ROS Noetic (which we ran in a~specially prepared Docker container).
This ensured a~high degree of similarity between the development environment on the target hardware platform and the Gazebo simulator used and contributed to facilitating the entire migration process.

\section{Results}\label{sec:rezultaty}

% Zaproponowana przez nas metoda wykorzystania uczenia przez wzmacnianie do zadania sterowania dronem (rozdział \ref{sec:metoda}) implikuje dwustopniowy proces testowania.
% W pierwszej kolejności ma on miejsce na etapie symulacji, w trakcie uczenia agenta.
% Następnie przygotowany system jest weryfikowany w rzeczywistym środowisku operacyjnym.
% Rezultaty obu wspomnianych etapów przedstawiamy poniżej.
Our method of using reinforcement learning for the drone control task (Section \ref{sec:metoda}) implies a~two-stage testing process.
First, it takes place at the simulation phase, during the training of the agent.
Then, the prepared system is verified in a~real operational environment.
The results of the two stages mentioned are presented below.

\subsection{Agent's training}

% Przy realizacji procesu treningu skorzystaliśmy z implementacji algorytmu PPO w bibliotece \textit{stable\_baselines3} \citep{stable-baselines3}.
% Obliczenia prowadziliśmy na komputerach klasy PC, z~których każdy wyposażony był w układ GPU Nvidia GeForce RTX 3060.
% Szkoleni agenci RL podlegali ewaluacji polegającej na wykonaniu 100 misji przelotu przez las wygenerowany za pomocą przygotowanego przez nas symulatora (rozdział \ref{subsec:symulator}).
% Za każdym razem rozmieszczenie oraz wielkość drzew były losowane od nowa, aby sprawdzić algorytm sterowania na możliwie dużej liczbie przypadków.
% Lot uznawaliśmy za udany, gdy dron przebył zakładaną odległość 30 m, unikając po drodze kolizji z innymi obiektami.
% Najwyższa uzyskana skuteczność (zgodnie z przedstawionym kryterium) wyniosła \textbf{91\%}.
In executing the training process, we used the implementation of the PPO algorithm in the library \textit{stable-baselines3} \cite{stable-baselines3}.
We ran the calculations on PCs, each equipped with an Nvidia GeForce RTX 3060 GPU.
The trained RL agents were subjected to the evaluation consisting of 100 flight missions through a~forest generated using a~simulator that we prepared (Section \ref{subsec:symulator}).
Each time, the placement and size of the trees were drawn again to test the control algorithm on as many cases as possible.
We considered a~flight successful when the drone flew an assumed distance of 30~m, avoiding collisions with other objects along the way.
The highest achieved success rate (according to the presented criterion) was \textbf{91\%}.

% Następnie przeprowadziliśmy również analogiczne eksperymenty przy wykorzystaniu symulatora Gazebo.
% W tym przypadku badaliśmy przede wszystkim, czy agent zachowuje się poprawnie w środowisku programowym zbliżonym do docelowego.
% Ze względu na statyczny charakter mapy ograniczyliśmy się do wykonania 5 przelotów testowych.
% Wyniki eksperymentów potwierdziły poprawność przeprowadzonej implementacji -- agent w sposób powtarzalny unikał kolizji z drzewami.
We then also conducted similar experiments using the Gazebo simulator.
In this case, we mainly investigated whether the agent behaves correctly in a~software environment close to the target.
Due to the static nature of the map, we decided to perform 5~test flights.
The experimental results confirmed the correctness of the implementation -- the agent reproducibly avoided collisions with trees.

\subsection{Real-world flights}

% Do realizacji lotów w rzeczywistości wykorzystaliśmy czterowirnikowiec z ramą wykonaną z drewna. 
% Cała konstrukcja charakteryzowała się względnie dużą wagą, co skutkowało niskim stosunkiem ciągu do masy oraz krótkimi czasami lotów. 
% Dron w konfiguracji startowej prezentujemy na rysunku \ref{img:drone}.
For the real-world flights, we used a~quad-rotor with a~frame made of wood. 
The whole design had a~relatively high weight, resulting in a~low thrust-to-weight ratio and short flight times. 
We present the drone in its launch configuration in Figure \ref{img:drone}.

% Figure environment removed

% Loty testowe przeprowadziliśmy w lesie, w miejscu jak najbardziej podobnym do warunków założonych w symulacji. 
% Dron na swojej trasie mógł głównie napotkać drzewa z małą liczbą niskich gałęzi, a średniej wysokości krzewiny nie występowały zbyt często.
% Misje przeprowadzaliśmy z różnych punktów startowych w ten sposób, aby pojazd zawsze był kierowany w podobny obszar lasu. 
% Celem misji było przelecenie zakładanej odległości wzdłuż osi X (w zadanym układzie współrzędnych) bez uderzenia w żadne drzewo. 
% Wszystkie testy dodatkowo nadzorowaliśmy poprzez kontroler radiowy, co umożliwiało manualne przejęcie kontroli w sytuacji awaryjnej. 
We conducted the test flights in a~forest, in a~location as similar as possible to the conditions assumed in the simulation. 
The drone was likely to encounter mainly trees with a~small number of low branches on its route, while medium-height shrubs were not very common.
We conducted the missions from different starting points in such a~way that the vehicle was always directed into a~similar area of the forest. 
The goal of the mission was to fly a~preset distance along the X-axis (in a~given coordinate system) without hitting a~single tree. 
We additionally supervised all tests via a~radio controller, which made it possible to take over manually in case of emergency. 

% Łącznie zrealizowliśmy 25 przelotów testowych.
% Ich wyniki przedstawiamy w tabeli \ref{tab:rezultaty rzeczywistosc}.
% Zdecydowana większość przelotów (\textbf{80\%}) zakończyła się sukcesem.
% Dron z powodzeniem przeleciał zadany dystans przez las, unikając po drodze większości napotykanych drzew.
% Należy przy tym podkreślić, że w żadnym przypadku jego konstrukcja nie uległa uszkodzeniom.
In total, we carried out 25 test flights.
Their results are presented in Table \ref{tab:rezultaty rzeczywistosc}.
The vast majority of the test flights (\textbf{80\%}) were successful.
The drone flew the preset distance through the forest, avoiding most of the trees encountered along the way.
It should be emphasised that its construction wasn't damaged in any case.
%\renewcommand{\tablename}{Tabela}
\begin{table}[!t]
    \centering
    \caption{Results of the real-world test scenarios.}
    \label{tab:rezultaty rzeczywistosc}
    \begin{tabular}{|l|c|c|} 
    \hline
    Total flights & 25 & 100\% \\
    \hline
    Without hitting a~tree & 13 & 52\% \\
    \hline
    Continued after hitting a~tree & 7 & 28\% \\
    \hline
    Ended after hitting a~tree & 5 & 20\% \\
    \hline
    \end{tabular}
\end{table}

% Sama skuteczność przelotów jest jednak zauważalnie niższa, niż uzyskana uprzednio w symulatorze.
% Stanowi to przykład problemu określanego w literaturze jako \textit{sim-to-real gap}.
% Należy pamiętać, że w omawianym przypadku podczas symulacji wykorzystano proste figury geometryczne o regularnym kształcie, które można uznać za niezwykle uproszczony model drzewa.
% Pomimo tego, agent RL był w stanie na tej podstawie zbudować wystarczająco generyczny obraz środowiska, który umożliwił wielokrotne skuteczne przeloty przez las.
% W tym świetle uzyskany rezultat należy uznać za satysfakcjonujący na obecnym etapie badań.
However, the flight efficiency itself is noticeably lower than that previously obtained in the simulator.
This exemplifies the problem referred to in the literature as the \textit{sim-to-real gap}.
Note that in the discussed case, the simulation used simple geometric figures with a~regular shape, which can be considered an extremely simplified tree model.
Despite this, the RL agent was able to build a~sufficiently generic view of the environment on this basis, which enabled multiple successful flights through the forest.
In this context, the obtained result should be considered satisfactory at this stage of the research.

% Na rysunku \ref{img:forest} przedstawiamy zdjęcie z jednego z przeprowadzonych przelotów w rzeczywistości.
% Film przedstawiający podsumowanie działania systemu można natomiast zobaczyć w~serwisie YouTube\footnote{\url{https://www.youtube.com/watch?v=JqosupMgu7g}}.
Figure \ref{img:forest} shows a~photo of one of the real-world flights.
A~video showing a~summary of the system's operation can be seen on YouTube\footnote{\url{https://www.youtube.com/watch?v=JqosupMgu7g}}.

% Figure environment removed

\section{Summary}\label{sec:podsumowanie}

% W niniejszym artykule przedstawiliśmy wykorzystanie algorytmów uczenia przez wzmacnianie do realizacji systemu sterowania dronem, który miał za zadanie autonomicznie lecieć do przodu i omijać napotykane drzewa w lesie na podstawie odczytów z obrotowego sensora LiDAR. 
% Przy szkoleniu agenta RL wykorzystaliśmy algorytm PPO.
% W tym celu przygotowaliśmy własny symulator napisany w języku Python, który w uproszczony sposób odwzorowywał środowisko leśne.
% Pozwalał on również na generowanie losowych map, składających się z różnie rozmieszczonych drzew o odmiennych średnicach konarów.
% Politykę agenta oraz funkcję wartości zaimplementowaliśmy jako wielowarstwowe perceptrony.
% Opracowana funkcja nagrody umożliwiła pomyślny trening, dzięki czemu agent uzyskał najlepszą skuteczność przelotu na poziomie 91\%. 
In this paper, we presented the use of reinforcement learning algorithms to implement a~drone control system that was intended to autonomously fly forward and avoid encountered trees in the forest based on measurements from a~rotating LiDAR sensor. 
When training the RL agent, we used a~PPO algorithm.
To do this, we developed our own simulator written in Python, which mapped the forest environment in a~simplified way.
It also allowed us to generate random maps, consisting of differently spaced trees with various branch diameters.
We implemented the agent policy and the value function as multilayer perceptrons.
The developed cost function enabled successful training, giving the agent the best flight efficiency of 91\%. 

% Przy przenoszeniu systemu na rzeczywistą platformę sprzętową wykorzystaliśmy również środowisko Gazebo.
% Przygotowaliśmy w nim imitację drzew w postaci wysokich walców o różnych promieniach. 
% W tak opracowanym środowisku uruchomiliśmy agenta, który był w stanie wykonywać skuteczne przeloty w sposób powtarzalny.
% Finalnie, opracowany algorytm sterowania zaimplementowaliśmy w układzie Nvidia Jetson Nano i przeprowadziliśmy testy na rzeczywistym dronie.
% W zdecydowanej większości prób pojazd skutecznie przeleciał przez las, niejednokrotnie unikając jakiegokolwiek uderzenia w drzewo.
When transferring the system to a~real hardware platform, we also used the Gazebo environment.
We prepared an imitation of trees in it in the form of tall cylinders with different radii. 
In this developed environment, we ran an agent that was able to perform effective flights in a~repeatable manner.
Finally, we implemented the developed control algorithm in an Nvidia Jetson Nano chip and conducted tests on the actual drone.
In the vast majority of tests, the vehicle successfully flew through the forest, repeatedly avoiding any impact with a~tree.

% Należy podkreślić, że prezentowane rezultaty stanowią jedynie pewien etap prac związanych z wykorzystaniem algorytmów uczenia przez wzmacnianie do zadania sterowania dronami.
% Tym niemniej są one obiecujące i uzasadniają kontynuację obranego kierunku badań.
% Dalszy rozwój tego podejścia zakłada przede wszystkim wykorzystanie bardziej zaawansowanych graficznie symulacji 3D.
% Ponadto planujemy również zastosowanie sensorów innego typu, w szczególności kamer RGB lub głębi, a także czujników zdarzeniowych (neuromorficznych).
% Wśród potencjalnych usprawnień należy także wskazać możliwość wprowadzenia algorytmów ,,douczania'' agenta już w trakcie realizacji misji w rzeczywistości oraz implementację całego systemu na innych platformach sprzętowych (np. heterogenicznych układach SoC FPGA) w celu przyspieszenia obliczeń i zmniejszenia zużycia energii.
It should be emphasised that the results presented in this paper represent only a~certain stage of work related to the application of reinforcement learning algorithms to the task of drone control.
Nevertheless, they are promising and justify the continuation of the chosen research direction.
Further development of this approach primarily involves the use of more graphically advanced 3D simulations.
In addition, we also plan to use sensors of other types, in particular RGB or depth cameras, as well as event (neuromorphic) sensors.
Potential improvements also include the possibility to introduce learning algorithms for the agent during real-world missions and the implementation of the entire system on other hardware platforms (e.g. heterogeneous SoC FPGAs) to speed up calculations and reduce energy consumption.

\subsection*{Acknowledgements} The work presented in this paper was supported by the AGH University of Krakow project no. 16.16.120.773.


% The Introduction section, of referenced text \cite{bib1} expands on the background of the work (some overlap with the Abstract is acceptable). The introduction should not include subheadings.

% Springer Nature does not impose a strict layout as standard however authors are advised to check the individual requirements for the journal they are planning to submit to as there may be journal-level preferences. When preparing your text please also be aware that some stylistic choices are not supported in full text XML (publication version), including coloured font. These will not be replicated in the typeset article if it is accepted. 

% \section{Results}\label{sec2}

% \section{This is an example for first level head---section head}\label{sec3}

% \subsection{This is an example for second level head---subsection head}\label{subsec2}

% \subsubsection{This is an example for third level head---subsubsection head}\label{subsubsec2}


% \section{Equations}\label{sec4}

% Equations in \LaTeX\ can either be inline or on-a-line by itself (``display equations''). For
% inline equations use the \verb+$...$+ commands. E.g.: The equation
% $H\psi = E \psi$ is written via the command \verb+$H \psi = E \psi$+.

% For display equations (with auto generated equation numbers)
% one can use the equation or align environments:
% \begin{equation}
% \|\tilde{X}(k)\|^2 \leq\frac{\sum\limits_{i=1}^{p}\left\|\tilde{Y}_i(k)\right\|^2+\sum\limits_{j=1}^{q}\left\|\tilde{Z}_j(k)\right\|^2 }{p+q}.\label{eq1}
% \end{equation}
% where,
% \begin{align}
% D_\mu &=  \partial_\mu - ig \frac{\lambda^a}{2} A^a_\mu \nonumber \\
% F^a_{\mu\nu} &= \partial_\mu A^a_\nu - \partial_\nu A^a_\mu + g f^{abc} A^b_\mu A^a_\nu \label{eq2}
% \end{align}
% Notice the use of \verb+\nonumber+ in the align environment at the end
% of each line, except the last, so as not to produce equation numbers on
% lines where no equation numbers are required. The \verb+\label{}+ command
% should only be used at the last line of an align environment where
% \verb+\nonumber+ is not used.
% \begin{equation}
% Y_\infty = \left( \frac{m}{\textrm{GeV}} \right)^{-3}
%     \left[ 1 + \frac{3 \ln(m/\textrm{GeV})}{15}
%     + \frac{\ln(c_2/5)}{15} \right]
% \end{equation}
% The class file also supports the use of \verb+\mathbb{}+, \verb+\mathscr{}+ and
% \verb+\mathcal{}+ commands. As such \verb+\mathbb{R}+, \verb+\mathscr{R}+
% and \verb+\mathcal{R}+ produces $\mathbb{R}$, $\mathscr{R}$ and $\mathcal{R}$
% respectively (refer Subsubsection~\ref{subsubsec2}).

% \section{Tables}\label{sec5}

% Tables can be inserted via the normal table and tabular environment. To put
% footnotes inside tables you should use \verb+\footnotetext[]{...}+ tag.
% The footnote appears just below the table itself (refer Tables~\ref{tab1} and \ref{tab2}). 
% For the corresponding footnotemark use \verb+\footnotemark[...]+

% \begin{table}[h]
% \begin{center}
% \begin{minipage}{174pt}
% \caption{Caption text}\label{tab1}%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Column 1 & Column 2  & Column 3 & Column 4\\
% \midrule
% row 1    & data 1   & data 2  & data 3  \\
% row 2    & data 4   & data 5\footnotemark[1]  & data 6  \\
% row 3    & data 7   & data 8  & data 9\footnotemark[2]  \\
% \botrule
% \end{tabular}
% \footnotetext{Source: This is an example of table footnote. This is an example of table footnote.}
% \footnotetext[1]{Example for a first table footnote. This is an example of table footnote.}
% \footnotetext[2]{Example for a second table footnote. This is an example of table footnote.}
% \end{minipage}
% \end{center}
% \end{table}

% \noindent
% The input format for the above table is as follows:

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{verbatim}
% \begin{table}[<placement-specifier>]
% \begin{center}
% \begin{minipage}{<preferred-table-width>}
% \caption{<table-caption>}\label{<table-label>}%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Column 1 & Column 2 & Column 3 & Column 4\\
% \midrule
% row 1 & data 1 & data 2	 & data 3 \\
% row 2 & data 4 & data 5\footnotemark[1] & data 6 \\
% row 3 & data 7 & data 8	 & data 9\footnotemark[2]\\
% \botrule
% \end{tabular}
% \footnotetext{Source: This is an example of table footnote. 
% This is an example of table footnote.}
% \footnotetext[1]{Example for a first table footnote.
% This is an example of table footnote.}
% \footnotetext[2]{Example for a second table footnote. 
% This is an example of table footnote.}
% \end{minipage}
% \end{center}
% \end{table}
% \end{verbatim}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%

% \begin{table}[h]
% \begin{center}
% \begin{minipage}{\textwidth}
% \caption{Example of a lengthy table which is set to full textwidth}\label{tab2}
% \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc@{\extracolsep{\fill}}}
% \toprule%
% & \multicolumn{3}{@{}c@{}}{Element 1\footnotemark[1]} & \multicolumn{3}{@{}c@{}}{Element 2\footnotemark[2]} \\\cmidrule{2-4}\cmidrule{5-7}%
% Project & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ \\
% \midrule
% Element 3  & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$\\
% Element 4  & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$\\
% \botrule
% \end{tabular*}
% \footnotetext{Note: This is an example of table footnote. This is an example of table footnote this is an example of table footnote this is an example of~table footnote this is an example of table footnote.}
% \footnotetext[1]{Example for a first table footnote.}
% \footnotetext[2]{Example for a second table footnote.}
% \end{minipage}
% \end{center}
% \end{table}

% In case of double column layout, tables which do not fit in single column width should be set to full text width. For this, you need to use \verb+\begin{table*}+ \verb+...+ \verb+\end{table*}+ instead of \verb+\begin{table}+ \verb+...+ \verb+\end{table}+ environment. Lengthy tables which do not fit in textwidth should be set as rotated table. For this, you need to use \verb+\begin{sidewaystable}+ \verb+...+ \verb+\end{sidewaystable}+ instead of \verb+\begin{table*}+ \verb+...+ \verb+\end{table*}+ environment. This environment puts tables rotated to single column width. For tables rotated to double column width, use \verb+\begin{sidewaystable*}+ \verb+...+ \verb+\end{sidewaystable*}+.

% \begin{sidewaystable}
% \sidewaystablefn%
% \begin{center}
% \begin{minipage}{\textheight}
% \caption{Tables which are too long to fit, should be written using the ``sidewaystable'' environment as shown here}\label{tab3}
% \begin{tabular*}{\textheight}{@{\extracolsep{\fill}}lcccccc@{\extracolsep{\fill}}}
% \toprule%
% & \multicolumn{3}{@{}c@{}}{Element 1\footnotemark[1]}& \multicolumn{3}{@{}c@{}}{Element\footnotemark[2]} \\\cmidrule{2-4}\cmidrule{5-7}%
% Projectile & Energy	& $\sigma_{calc}$ & $\sigma_{expt}$ & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ \\
% \midrule
% Element 3 & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$ \\
% Element 4 & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$ \\
% Element 5 & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$ \\
% Element 6 & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$ \\
% \botrule
% \end{tabular*}
% \footnotetext{Note: This is an example of table footnote this is an example of table footnote this is an example of table footnote this is an example of~table footnote this is an example of table footnote.}
% \footnotetext[1]{This is an example of table footnote.}
% \end{minipage}
% \end{center}
% \end{sidewaystable}

% \section{Figures}\label{sec6}

% As per the \LaTeX\ standards you need to use eps images for \LaTeX\ compilation and \verb+pdf/jpg/png+ images for \verb+PDFLaTeX+ compilation. This is one of the major difference between \LaTeX\ and \verb+PDFLaTeX+. Each image should be from a single input .eps/vector image file. Avoid using subfigures. The command for inserting images for \LaTeX\ and \verb+PDFLaTeX+ can be generalized. The package used to insert images in \verb+LaTeX/PDFLaTeX+ is the graphicx package. Figures can be inserted via the normal figure environment as shown in the below example:

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{verbatim}
% % Figure environment removed
% \end{verbatim}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%

% % Figure environment removed

% In case of double column layout, the above format puts figure captions/images to single column width. To get spanned images, we need to provide \verb+% Figure environment removed+.

% For sample purpose, we have included the width of images in the optional argument of \verb+\includegraphics+ tag. Please ignore this. 

% \section{Algorithms, Program codes and Listings}\label{sec7}

% Packages \verb+algorithm+, \verb+algorithmicx+ and \verb+algpseudocode+ are used for setting algorithms in \LaTeX\ using the format:

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{verbatim}
% \begin{algorithm}
% \caption{<alg-caption>}\label{<alg-label>}
% \begin{algorithmic}[1]
% . . .
% \end{algorithmic}
% \end{algorithm}
% \end{verbatim}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%

% You may refer above listed package documentations for more details before setting \verb+algorithm+ environment. For program codes, the ``program'' package is required and the command to be used is \verb+\begin{program}+ \verb+...+ \verb+\end{program}+. A fast exponentiation procedure:

% \begin{program}
% \BEGIN \\ %
%   \FOR i:=1 \TO 10 \STEP 1 \DO
%      |expt|(2,i); \\ |newline|() \OD %
% \rcomment{Comments will be set flush to the right margin}
% \WHERE
% \PROC |expt|(x,n) \BODY
%           z:=1;
%           \DO \IF n=0 \THEN \EXIT \FI;
%              \DO \IF |odd|(n) \THEN \EXIT \FI;
% \COMMENT{This is a comment statement};
%                 n:=n/2; x:=x*x \OD;
%              \{ n>0 \};
%              n:=n-1; z:=z*x \OD;
%           |print|(z) \ENDPROC
% \END
% \end{program}


% \begin{algorithm}
% \caption{Calculate $y = x^n$}\label{algo1}
% \begin{algorithmic}[1]
% \Require $n \geq 0 \vee x \neq 0$
% \Ensure $y = x^n$ 
% \State $y \Leftarrow 1$
% \If{$n < 0$}\label{algln2}
%         \State $X \Leftarrow 1 / x$
%         \State $N \Leftarrow -n$
% \Else
%         \State $X \Leftarrow x$
%         \State $N \Leftarrow n$
% \EndIf
% \While{$N \neq 0$}
%         \If{$N$ is even}
%             \State $X \Leftarrow X \times X$
%             \State $N \Leftarrow N / 2$
%         \Else[$N$ is odd]
%             \State $y \Leftarrow y \times X$
%             \State $N \Leftarrow N - 1$
%         \EndIf
% \EndWhile
% \end{algorithmic}
% \end{algorithm}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%

% Similarly, for \verb+listings+, use the \verb+listings+ package. \verb+\begin{lstlisting}+ \verb+...+ \verb+\end{lstlisting}+ is used to set environments similar to \verb+verbatim+ environment. Refer to the \verb+lstlisting+ package documentation for more details.

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{minipage}{\hsize}%
% \lstset{frame=single,framexleftmargin=-1pt,framexrightmargin=-17pt,framesep=12pt,linewidth=0.98\textwidth,language=pascal}% Set your language (you can change the language for each code-block optionally)
% %%% Start your code-block
% \begin{lstlisting}
% for i:=maxint to 0 do
% begin
% { do nothing }
% end;
% Write('Case insensitive ');
% Write('Pascal keywords.');
% \end{lstlisting}
% \end{minipage}

% \section{Cross referencing}\label{sec8}

% Environments such as figure, table, equation and align can have a label
% declared via the \verb+\label{#label}+ command. For figures and table
% environments use the \verb+\label{}+ command inside or just
% below the \verb+\caption{}+ command. You can then use the
% \verb+\ref{#label}+ command to cross-reference them. As an example, consider
% the label declared for Figure~\ref{fig1} which is
% \verb+\label{fig1}+. To cross-reference it, use the command 
% \verb+Figure \ref{fig1}+, for which it comes up as
% ``Figure~\ref{fig1}''. 

% To reference line numbers in an algorithm, consider the label declared for the line number 2 of Algorithm~\ref{algo1} is \verb+\label{algln2}+. To cross-reference it, use the command \verb+\ref{algln2}+ for which it comes up as line~\ref{algln2} of Algorithm~\ref{algo1}.

% \subsection{Details on reference citations}\label{subsec7}

% Standard \LaTeX\ permits only numerical citations. To support both numerical and author-year citations this template uses \verb+natbib+ \LaTeX\ package. For style guidance please refer to the template user manual.

% Here is an example for \verb+\cite{...}+: \cite{bib1}. Another example for \verb+\citep{...}+: \citep{bib2}. For author-year citation mode, \verb+\cite{...}+ prints Jones et al. (1990) and \verb+\citep{...}+ prints (Jones et al., 1990).

% All cited bib entries are printed at the end of this article: \cite{bib3}, \cite{bib4}, \cite{bib5}, \cite{bib6}, \cite{bib7}, \cite{bib8}, \cite{bib9}, \cite{bib10}, \cite{bib11} and \cite{bib12}.

% \section{Examples for theorem like environments}\label{sec10}

% For theorem like environments, we require \verb+amsthm+ package. There are three types of predefined theorem styles exists---\verb+thmstyleone+, \verb+thmstyletwo+ and \verb+thmstylethree+ 

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{tabular}{|l|p{19pc}|}
% \hline
% \verb+thmstyleone+ & Numbered, theorem head in bold font and theorem text in italic style \\\hline
% \verb+thmstyletwo+ & Numbered, theorem head in roman font and theorem text in italic style \\\hline
% \verb+thmstylethree+ & Numbered, theorem head in bold font and theorem text in roman style \\\hline
% \end{tabular}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. please ignore this.       %%
% %%=============================================%%

% For mathematics journals, theorem styles can be included as shown in the following examples:

% \begin{theorem}[Theorem subhead]\label{thm1}
% Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
% Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
% Example theorem text. 
% \end{theorem}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{proposition}
% Example proposition text. Example proposition text. Example proposition text. Example proposition text. Example proposition text. 
% Example proposition text. Example proposition text. Example proposition text. Example proposition text. Example proposition text. 
% \end{proposition}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{example}
% Phasellus adipiscing semper elit. Proin fermentum massa
% ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend
% at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. 
% \end{example}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{remark}
% Phasellus adipiscing semper elit. Proin fermentum massa
% ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend
% at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. 
% \end{remark}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{definition}[Definition sub head]
% Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. 
% \end{definition}

% Additionally a predefined ``proof'' environment is available: \verb+\begin{proof}+ \verb+...+ \verb+\end{proof}+. This prints a ``Proof'' head in italic font style and the ``body text'' in roman font style with an open square at the end of each proof environment. 

% \begin{proof}
% Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. 
% \end{proof}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{proof}[Proof of Theorem~{\upshape\ref{thm1}}]
% Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. 
% \end{proof}

% \noindent
% For a quote environment, use \verb+\begin{quote}...\end{quote}+
% \begin{quote}
% Quoted text example. Aliquam porttitor quam a lacus. Praesent vel arcu ut tortor cursus volutpat. In vitae pede quis diam bibendum placerat. Fusce elementum
% convallis neque. Sed dolor orci, scelerisque ac, dapibus nec, ultricies ut, mi. Duis nec dui quis leo sagittis commodo.
% \end{quote}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text (refer Figure~\ref{fig1}). Sample body text. Sample body text. Sample body text (refer Table~\ref{tab3}). 

% \section{Methods}\label{sec11}

% Topical subheadings are allowed. Authors must ensure that their Methods section includes adequate experimental and characterization data necessary for others in the field to reproduce their work. Authors are encouraged to include RIIDs where appropriate. 

% \textbf{Ethical approval declarations} (only required where applicable) Any article reporting experiment/s carried out on (i)~live vertebrate (or higher invertebrates), (ii)~humans or (iii)~human samples must include an unambiguous statement within the methods section that meets the following requirements: 

% \begin{enumerate}[1.]
% \item Approval: a statement which confirms that all experimental protocols were approved by a named institutional and/or licensing committee. Please identify the approving body in the methods section

% \item Accordance: a statement explicitly saying that the methods were carried out in accordance with the relevant guidelines and regulations

% \item Informed consent (for experiments involving humans or human tissue samples): include a statement confirming that informed consent was obtained from all participants and/or their legal guardian/s
% \end{enumerate}

% If your manuscript includes potentially identifying patient/participant information, or if it describes human transplantation research, or if it reports results of a clinical trial then  additional information will be required. Please visit (\url{https://www.nature.com/nature-research/editorial-policies}) for Nature Portfolio journals, (\url{https://www.springer.com/gp/authors-editors/journal-author/journal-author-helpdesk/publishing-ethics/14214}) for Springer Nature journals, or (\url{https://www.biomedcentral.com/getpublished/editorial-policies\#ethics+and+consent}) for BMC.

% \section{Discussion}\label{sec12}

% Discussions should be brief and focused. In some disciplines use of Discussion or `Conclusion' is interchangeable. It is not mandatory to use both. Some journals prefer a section `Results and Discussion' followed by a section `Conclusion'. Please refer to Journal-level guidance for any specific requirements. 

% \section{Conclusion}\label{sec13}

% Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

% In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

% \backmatter

% \bmhead{Supplementary information}

% If your article has accompanying supplementary file/s please state so here. 

% Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

% Please refer to Journal-level guidance for any specific requirements.

% \bmhead{Acknowledgments}

% Acknowledgments are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

% Please refer to Journal-level guidance for any specific requirements.

% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
% \item Ethics approval 
% \item Consent to participate
% \item Consent for publication
% \item Availability of data and materials
% \item Code availability 
% \item Authors' contributions
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}

% \begin{appendices}

% \section{Section title of first appendix}\label{secA1}

% An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

% %%=============================================%%
% %% For submissions to Nature Portfolio Journals %%
% %% please use the heading ``Extended Data''.   %%
% %%=============================================%%

% %%=============================================================%%
% %% Sample for another appendix section			       %%
% %%=============================================================%%

% %% \section{Example of another appendix section}\label{secA2}%
% %% Appendices may be used for helpful, supporting or essential material that would otherwise 
% %% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
% %% tables and equations etc.

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%
\renewcommand\refname{References}
%\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%% Default %%
\input sn-sample-bib.tex%

\end{document}
