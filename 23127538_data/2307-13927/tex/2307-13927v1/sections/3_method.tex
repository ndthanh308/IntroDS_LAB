\section{Method}
As illustrated in Fig. \ref{fig:pipeline}, DFR-Net generates a proposal image using a Proposal Image Generator (PIG), which is a simple U-Net, to facilitate density-awareness. Besides, DFR-Net consists of two primary parts: Global Branch (GB) and Local Branch (LB). The GB and LB are responsible for learning and refining global and local haze density features, respectively, and generate a pseudo result each. In the end, an adaptive fusion is used to obtain the final dehazing result. This section presents a detailed introduction of our method's main ideas.

% Figure environment removed

\subsection{Proposal Image for Density-awareness}

In DFR-Net, we first generate a proposal image (\textbf{P}) that provides information about the difference in haze density. \textbf{P} exhibits lower overall density than the input image (\textbf{I}), and inconsistent dehazing performance in some local areas. Taking these characteristics into account, we are motivated to extract and refine haze density features in the subsequent network.

To generate \textbf{P}, we employ a Proposal Image Generator (PIG), which is a simple U-net comprising multiple Residual Blocks (ResBlocks). We pre-train PIG with paired images and incorporate it into subsequent branches as an end-to-end network.

\subsection{Global Branch}

\subsubsection{Overview} The Global Branch (GB) is designed to extract and refine global density features using the overall haze density difference. As depicted in Fig. \ref{fig:GB} (top), GB consists of a 7-stage U-Net with 7 global blocks, where each block has a Siamese structure with $N_{GB}^{i}(i\in[1,...,7])$ ResBlocks for feature extraction and a GDFR module for global density feature refinement. The inputs to GB are \textbf{I} and \textbf{P} $\in \mathbb{R}^{H\times W\times 3}$, which are embedded to $F^{0}_{I\_GB}$, $F^{0}_P$ $\in \mathbb{R}^{H\times W\times C}$ using a convolutional layer. Upsampling or downsampling is performed between every two blocks. To obtain the predicted dehazing residual, the output features of the last global block are fed to a restore block, and the predicted dehazing residual is added to \textbf{I} to obtain a pseudo result, $\hat{J}_{GB}$.

We design the global block as a basic unit in GB, taking the haze density relationship between \textbf{I} and \textbf{P} into consideration. Specifically, the $i$-th global block is composed of a Siamese structure with $N_{GB}^{i}$ ResBlocks for feature extraction and a GDFR module for global density feature refinement. For the $i$-th global block, the inputs are the outputs of the previous block, $F^{i-1}_{I\_GB}$ and $F^{i-1}_P$, the outputs are $F^{i}_{I\_GB}$, $F^{i}_P$ and the refined global density feature, $F^{i}_G$.

\subsubsection{Siamese ResBlocks for global feature extraction} Siamese structure shares weights and can measure similarity \cite{bertinetto2016fully,he2018twofold,guo2017learning} or dissimilarity \cite{koch2015siamese,wu2017face} between samples effectively. So we utilize a Siamese structure to extract image features from both \textbf{P} and \textbf{I}, which enables the network to establish a relationship between images with different haze densities. Moreover, compared to a single hazy input structure, the Siamese structure enables the features extracted by the network to better perceive the variation in haze density.

\subsubsection{GDFR for global feature refinement} To further refine the extracted features, we propose the Global Density Feature Refinement (GDFR) module, which aims to highlight the features that better describe global density information and thus pull apart the features of images with different densities. For the implementation, we compute the difference between the feature maps of \textbf{I} and \textbf{P}, and then square each element in the difference map. This subtractive operation filters out non-density information, and the resulting feature channels with larger differences can better describe density information. We perform global average pooling (GAP) and sigmoid operation on the feature differences to obtain density-related channel weights, denoted as $W_c$. We then multiply these weights with $F^{i}_{I\_GB}$ channel-wise: 
\begin{equation}
  \Tilde{F}_I^i=F_{I\_GB}^i\otimes\sigma(GAP(POW((F_P^i-F_{I\_GB}^i),2))) 
\end{equation}
where $GAP(\cdot)$ stands for global average pooling, $\sigma(\cdot)$ represents sigmoid operation, $\otimes$ stands for channel-wise multiply and $POW(a,b)$ represents a power of exponent $b$ for each element of $a$.

In spatial dimension, we first multiply the channel weights by the squared feature difference. Next, we apply channel-wise average pooling (CAP) and sigmoid operations to obtain a 2-D weight map that represents the spatial density difference $W_s$. Higher value in $W_s$ indicates that haze has been more effectively removed here. Therefore, $1-W_s$ can represent an attention map that guides the network's focus. Here $1$ denots a 2-D tensor of ones with the same shape of $W_s$. We then multiply $1-W_s$ and $\Tilde{F}_I^i$ and add the result to $F^{i}_{I\_GB}$ to obtain the finally refined global density feature ${F}_G^i$:

\begin{equation}
  F_G^i=(1-\sigma(CAP(POW((F_P^i-F_{I\_GB}^i),2))))\odot\Tilde{F}_I^i+F^{i}_{I\_GB}
\end{equation}
where $CAP(\cdot)$ stands for channel-wise average pooling.

After the last block, we concatenate $F^{7}_P$ and $F^{7}_G$ and input the concatenated feature to a restore block, which is composed of four ResBlocks and a convolutional layer, to obtain the dehazing residual.
\begin{equation}
  res_{GB}=RB_{GB}(cat(F_P^{7},F_G^{7})) 
\end{equation}
where $RB_{GB}(\cdot)$ denotes the calculation of the restore block in GB and $cat(\cdot)$ denotes the channel-wise concatenation operation. And the pseudo-clear image of GB can be obtained by: $\hat{J}_{GB} = I + res_{GB}$.

\subsection{Local Branch}

\subsubsection{Overview} LB extracts local density features from the dehazing residual of \textbf{P} and \textbf{I} , which contains local density differences and indicates hard dehazing or high haze density areas. To refine these local features, we propose the IDRF module that gradually adjusts the features to match those of clear images.

The main body of LB is a 7-stage U-net, as illustrated in Fig. \ref{fig:DAFF} (top). Each stage is composed of $N_{LB}^i (i\in[1,...,7])$ ResBlocks. LB takes \textbf{I} and \textbf{res}$\in \mathbb{R}^{H\times W\times 3}$ as inputs and embeds them to shallow features $F^{0}_{I\_LB} \in \mathbb{R}^{H\times W\times C}$, $F^{0}_L\in \mathbb{R}^{H\times W\times C_L}$ using a normal convolutional layer and a multi-scale convolutional layer, respectively. In the encoder stages, local and image features are extracted by a split and merge way, and in the decoder stages, DAFF fuses shallow image features, deep image features, local features, and refined global features from GB. The dehazing residual can be obtained by a restore block, and the pseudo result $\hat{J}_{LB}$ can be obtained by summing it and \textbf{I}.

\subsubsection{S\&M for local feature extraction} The local features are extracted in an S\&M way. Firstly, $F^{0}_{I\_LB}$ and $F^{0}_L$ are merged by concatenation and fed into stage-1, resulting in an output feature with ($C+C_L$) channels. The output feature is then processed by the S\&M module. As shown in Fig. \ref{fig:DAFF} (b), the input feature is split into an image feature and a local density feature. Notably, the number of channels for the image feature varies from stage to stage, while the number of channels for the local feature is consistently fixed at $C_L$. To utilize the global density information obtained from GB, the image feature is concatenated with the refined global feature from GB. Afterwards, the concatenated feature and local feature are downsampled by $1\times 1$ convolutional layers and pixel-unshuffle, respectively. The downscaled features are then concatenated and fed into the next stage. This design allows local features to fully interact with hazy image features, thus enhancing the reliability of local features.

\subsubsection{DAFF for feature fusion}\label{sec:method} Several methods introduce skip connection to aggregate shallow and deep features and simply concatenate or add them together \cite{zamir2022restormer,yin2021attentive}, which might result in a loss of information. To fully utilize the information relevant to density, we employ Density Aware Feature Fusion (DAFF). Specifically, given a set of features: $\{F^i_{LB\_out}, F^{7-i}_{LB\_out}, F^{7-i}_G\} (i\in[1, 2, 3])$, which represent the output feature of the $i$-th, $(7-i)$-th stage of LB and the refined global feature of the $(7-i)$-th block in GB, we first align them to a same shape and split the image features and local density features. Then we perform convolutions and LeakyReLU (Conv+LReLu) on $F^{7-i}_G$ and the results are concatenated with $F^i_{I\_LB}$ and $F^{7-i}_{I\_LB}$ respectively and input to Channel-Spatial Density Attention (CSDA) modules, as illustrated in Fig. \ref{fig:DAFF} (c). Moreover, the local feature $F^{7-i}_L$ is projected to a 2-D local attention map ($\mathcal{M}_{local}$) by Conv+LReLu and $\mathcal{M}_{local}$ is fed to CSDAs. Finally, the outputs of CSDAs are concatenated and compressed to the channel number of the subsequent stage by a $1\times 1$ convolutional layer. 

% Figure environment removed


\begin{table*}[ht]
    \caption{Quantitative comparison of DFR-Net with the state-of-the-art image dehazing methods on different datasets (PSNR (DB)/SSIM). Best results are \textbf{bolded} and second best results are \underline{underlined}. Cells where results are not available are replaced by "-"}
    \begin{center}
    \begin{tabular}{c c c c c c c c c c}
    \toprule[1pt]
    \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{RESIDE-outdoor} & \multicolumn{2}{c}{Haze4K} & \multicolumn{2}{c}{NH-HAZE} & \multicolumn{2}{c}{Dense-Haze}\\ \cmidrule(l){3-4} \cmidrule(l){5-6} \cmidrule(l){7-8} \cmidrule(l){9-10}
    \multicolumn{2}{c}{} & PSNR(dB) & SSIM & PSNR(dB) & SSIM & PSNR(dB) & SSIM & PSNR(dB) & SSIM\\
    \midrule
    \multirow{13}{*}{Non-Density-Aware} & DCP \cite{he2010single} (TPAMI' 10) & 19.13 &0.815 & 14.01 & 0.760 & 10.57 & 0.522 & 11.01 & 0.416\\
    & DehazeNet \cite{cai2016dehazenet} (TIP' 16) & 24.75 & 0.927 & 19.12 & 0.840 & 12.86 & 0.545 & 9.48 & 0.438\\
    & AOD-Net \cite{li2017aod} (ICCV' 17) & 24.14 & 0.920 & 17.15 & 0.830 & 15.40 & 0.571 & 12.82 & 0.468\\
    & GDNet \cite{liu2019griddehazenet} (ICCV' 19) & 30.86 & 0.982 & 23.29 & 0.930 & 18.33 & 0.667 & 14.96 & 0.530\\
    & MSBDN \cite{dong2020multi} (CVPR' 20) & 33.48 & 0.982 & 22.99 & 0.850 & 19.23 & 0.713 & 15.13 & 0.555\\
    & FFA-Net \cite{qin2020ffa} (AAAI' 20) & 33.57 & 0.984 & 26.96 & 0.950 & 19.87 & 0.694 & 12.22 & 0.444\\
    & AECR-Net \cite{wu2021contrastive} (CVPR' 21) & - & - & - & - & 19.88 &0.722 & 15.80 & 0.466 \\ 
    & CEEF \cite{liu2021joint} (TMM' 22) & 19.13 & 0.792 & -& -& -& - &- &-\\
    & SGID-PFF \cite{bai2022self} (TIP' 22) & 30.20 & 0.975 &  - & - &  - & - & - & -\\ 
    & UDN \cite{hong2022uncertainty} (AAAI' 22) & 34.92 & 0.987 & - & - &  - & - & - & - \\ 
    & QCNN-H \cite{frants2023qcnn} (TC' 23) & 28.74 & 0.964 & - & - & - & - & - & - \\
    & MFINEA \cite{sun2023multi} (NN' 23) & 33.88 & 0.981 & - & - & - & - & \underline{18.34} & \underline{0.609} \\
    & DehazeFormer-B \cite{song2023vision} (TIP' 23) & 34.95 & 0.984 & 30.29 & \underline{0.985} & 17.37 & 0.725 & - & - \\ \midrule
    % & C2PNet \cite{zheng2023curricular} (CVPR' 23) & \textbf{36.68} & \underline{0.990} & \underline{34.20} & \underline{0.990} & 20.20 & \underline{0.749} & \underline{16.88} &  \underline{0.573} \\ 

    \multirow{4}{*}{Density-Aware} & HDDNet \cite{zhang2021hierarchical} (TC' 22) & 22.52 & 0.910 & - & -&  - & - & - & -\\
    & DeHamer \cite{guo2022image} (CVPR' 22) & \underline{35.18} & 0.986 & - & -& \underline{20.66}&0.684 & 16.62 & 0.560 \\
    & PMNet \cite{yeperceiving} (ECCV' 22) & 34.74 & \underline{0.990} & \underline{33.49} & 0.980 & 20.42 & \underline{0.731} & 16.79 & 0.510 \\
    \cmidrule{2-10}
    & DFR-Net (ours) & \textbf{35.34} & \textbf{0.993} & \textbf{34.63} & \textbf{0.993} & \textbf{21.21} & \textbf{0.810} & \textbf{18.85} & \textbf{0.674}\\
    \bottomrule
    \end{tabular}
    \end{center}
    \label{tab:result}
    \end{table*}
    
% Figure environment removed

% % Figure environment removed


% Figure environment removed
% % Figure environment removed




\subsubsection{IDRF for local feature refinement} To refine the local density feature, we further introduce IDRF module. As shown in Fig. \ref{fig:IDRF}, IDRF takes $F^i_{I\_LB}$ as input to obtain intermediate dehazing residual $res_{inter}^i$ by an intermediate restore block (IRB) composed of two ResBlocks and a convolutional layer. Then the $res_{inter}^i$ is projected to a $C_L$-channel feature embedding $F_L^{'}$ which is subsequently concatenated with local features as shown in Fig. \ref{fig:DAFF} (a) and (b). With this process, local density features can be updated by the current dehazing residual. To optimize this module and pull local features in towards clear image features, we employ a local density refinement loss ($\mathcal{L}_{LDR}$) which will be introduced in Sec. \ref{section:loss}.

Similar to GB, LB predicts a pseudo-clear result: $\hat{J}_{LB} = I + res_{LB}$.
% \begin{equation}
%     \hat{J}_{LB} = I + res_{LB} 
% \end{equation}
Finally we fuse the pseudo-results of the two branches with a learnable parameter $\alpha $: $\hat{J} = \alpha\times\hat{J}_{GB}+(1-\alpha)\times\hat{J}_{LB}$.

\subsection{Loss Function} \label{section:loss}
The overall loss fuction of our DFR-Net can be formulated as: $\mathcal{L}=\mathcal{L}_{Rec}+\lambda_1\mathcal{L}_P+\lambda_2\mathcal{L}_{RD}+\lambda_3\mathcal{L}_{LDR}$, where $\mathcal{L}_{Rec}$ and $\mathcal{L}_P$ denote L1 loss and perceptual loss \cite{DBLP:journals/corr/JohnsonAL16} between the predicted haze-free output $\hat{J}$ and ground truth $J$, $\mathcal{L}_{RD}$ and $\mathcal{L}_{LDR}$ represent representation dissimilarity loss and local density refinement loss, and $\lambda_1$, $\lambda_2$ and $\lambda_3$ are hyper-parameters for loss regulation.

\subsubsection{Representation Dissimilarity Loss}
In this paper, we introduce a Siamese structure to learn density-related features from \textbf{I} and \textbf{P}. To motivate the Siamese structure to learn more information about the differences between the inputs, we design representation dissimilarity loss:
\begin{equation}
    % \mathcal{L}_{RD}=\sum_{i=1}^{n}CosSim(F_P^i, F_{I\_GB}^i)
    \mathcal{L}_{RD}=\sum_{i=1}^{n}\langle F_P^i, F_{I\_GB}^i\rangle 
\end{equation}
where $\langle a, b\rangle $ represents the calculation of the cosine similarity between $a$ and $b$. We compute cosine similarities between the intermediate features of \textbf{I} and \textbf{P} and minimize them to let the Siamese structure learn more representation about the difference between the two inputs.

\subsubsection{Local Density Refinement Loss}
To achieve the pulling of local features from haze images to clear images, we introduce local density refinement loss.
\begin{equation}
    \mathcal{L}_{LDR}=\frac{1}{k}\sum_{j=1}^{k}\Vert \hat{J}^j_{inter}(x)-Down^j(J(x))\Vert_1
\end{equation}
where $k$ denotes the number of IDRF modules which are applied, $\hat{J}_{inter}^j(x)$ is the $j$-th intermediate predicted clear output, and $Down^j(\cdot)$ is the operation that downsamples ground truth to the size of the corresponding intermediate output.
