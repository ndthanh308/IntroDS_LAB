\section{Introduction}

\IEEEPARstart{H}{aze}
 is a common atmospheric phenomenon caused by the accumulation of aerosol particles. It can cause severe quality degradation of images, which can affect subsequent computer vision tasks. Therefore, developing effective techniques for haze removal is essential to improve the quality of images and ensure accurate results of downstream tasks \cite{lin2022msaff,li2019pdr, liu2021joint}. 

After decades of study, researchers \cite{drake1985mie,mccartney1976optics} model this atmospheric phenomenon as:
\begin{equation}\label{eq:1}
    \textbf{I}(x)=\textbf{J}(x)t(x)+\textbf{A}(1-t(x))
\end{equation}
where $\textbf{I}(x)$ represents the hazy image; $\textbf{J}(x)$ represents the clear image; $t(x)$ and $\textbf{A}$ stand for the transmission map (T-map) and the global atmospheric light separately. And this model is commonly called the atmospheric scattering model (ASM).

% Figure environment removed


% For better fusion of density information, a Density Aware Feature Fusion (DAFF) module is used to fuse multiple source features in the decoder stages.

To improve image quality and highlight image details captured in hazy weather, numerous image dehazing methods have been proposed. Prior-based methods \cite{he2010single, zhu2015fast, fattal2014dehazing, berman2016non, berman2018single} rely on statistical analysis of haze images and handcrafted priors to recover haze-free images. However, these methods have limitations in their robustness due to their reliance on specific assumptions, which may not hold in different scenes. 

With the success of deep neural networks in high-level tasks, data-driven dehazing methods \cite{ren2016single, li2017aod, berman2018single, chen2019gated, wu2021contrastive, yeperceiving} have become mainstream. Compared to prior-based methods, deep learning methods demonstrate stronger capabilities in feature extraction and image restoration. However, early deep learning methods neglect the uneven distribution of haze, resulting in redundancy in network design and inefficiency in feature extraction \cite{yeperceiving}. One idea to improve these methods is to enable the network to learn features about the haze density.

Research works \cite{yeperceiving,zhang2021hierarchical,deng2020hardgan,yang2022self,lou2020integrating} have proposed density-aware dehazing methods. Haze density describes the distribution of haze and impacts the effectiveness of a dehazing method. Several methods \cite{zhang2021hierarchical, yang2022self, lou2020integrating} estimate the T-map to obtain haze density information, which is inversely proportional to the T-map \cite{zhang2021hierarchical}. However, these methods require T-map labels, which can be difficult to obtain. To avoid this problem, methods \cite{deng2020hardgan, yeperceiving} directly extract density-related features. Nonetheless, these methods still have shortcomings: they lack a comparator to measure density, and the learning process of density features lacks interpretability.

As image dehazing is an ill-posed problem, estimating a clear image directly from a hazy input using a feedforward network is challenging \cite{bai2022self}. Additionally, distinguishing haze densities from a single hazy image is also difficult, and a comparable image that helps the network learn to perceive different haze densities is necessary for density-awareness. While some research works \cite{bai2022self,yeperceiving,chen2021desmokenet} have proposed methods that generate intermediate images to alleviate this problem, they do not take the density differences between the intermediate results and the hazy inputs into much consideration, limiting the network's ability to fully perceive haze density.
% Their lack of design for the relationship between the two images also constrains the network's capability to extract the haze information, especially the difference information between the two images. 

To address this issue, we propose a Density Feature Refinement Network (DFR-Net) that achieves density awareness by utilizing density difference. To fully explore haze density feature, we first generate a dehazing proposal image ($\textbf{P}$, as shown in Fig. \ref{fig:head}), that can provide a comparison of density information from both global and local perspectives. Our proposal image is generated by a simple U-net structure and has an overall lower haze density than the haze input ($\textbf{I}$), which brings global density differences. The different dehazing performance of different areas reflected in the dehazing residuals ($\textbf{res}$) leads to differences in local density and hints to areas with haze removal challenges. Subsequently, we propose a 2-branch structure consisting Global Branch (GB) and Local Branch (LB) \emph{based on the density difference information between} \emph{\textbf{P}} \emph{and} \emph{\textbf{I}} to comprehensively extract density features and refine them in different ways.

In detail, the scenes are the same between $\textbf{P}$ and $\textbf{I}$, differing only in haze density. Therefore, we propose to use Siamese structures for feature extraction for $\textbf{P}$ and $\textbf{I}$, enabling the network to understand both the different densities in one forward process. In addition, considering that the two features contain part of the same information, to push them farther away and highlight the density information, we design a Global Density Feature Refinement (GDFR) module to refine the features. Locally, the dehazing residual between $\textbf{P}$ and $\textbf{I}$ contains hints of the local density information. The dehazing residual represents the performance of dehazing and areas with small residual values tend to be more heavy or hard dehazing regions. Hence, we propose to learn local density features from the dehazing residual ($\textbf{P}-\textbf{I}$) in a split and merge (S\&M) way. To refine local features, several Intermediate Dehazing Residual Feedforward (IDRF) modules are used, which can pull local features closer to clear image features. The refinement illustration is exemplified in Fig.\ref{fig:head}. Both the two branches give a predicted haze-free image and we perform an adaptive fusion on them to gain the final dehazing result. 

Compared to other density-aware methods, on the one hand, our DFR-Net does not rely on additional T-map annotations and does not predict the T-map, thus reducing the manual workload and the possible loss of information due to the T-map prediction process \cite{jin2023dnf}. On the other hand, unlike the way other methods extract density features, e.g., PMNet extracts density features from the splicing of hazy inputs and pseudo-haze-free images via an SHA module, we analyze the relationship of haze density between $\textbf{P}$ and $\textbf{I}$, and design a network with interpretability to extract and refine density features from the density difference. This makes our DFR-Net contain density-related prior knowledge.

Overall, the main contributions of our work are as follows:

\begin{itemize}
  % \vspace*{-2pt}
    \item We propose to learn and refine the haze density feature of a hazy image by utilizing the difference information between a generated proposal image ($\textbf{P}$) and a hazy input ($\textbf{I}$) and an end-to-end method named DFR-Net is designed to achieve density-aware dehazing.
    \item We extract haze density features both globally and locally. In GB, a Global Block is introduced, which can explore the image features of $\textbf{P}$ and $\textbf{I}$. To highlight the features that can better describe the density information, a GDFR module is proposed. In LB, local density features are extracted from the dehazing residual between $\textbf{P}$ and $\textbf{I}$. Additionally, a IDRF module is presented to refine local density features stage by stage.
    \item Sufficient experiments are conducted on our DFR-Net and demonstrate that it can achieve better results over the existing state-of-the-art (SOTA) methods on multiple commonly used datasets.
\end{itemize}
% \vspace{0.25 cm}


% Overall, our contributions can be summarized as:
% \begin{itemize}
%     \item We propose RescaleNorm to replace the LayerNorm to minimize the negative effects of normalization. And we also use ReLU to replace the GELU to facilitate the decoder to decode the features into image signals.
%     \item We use reflection padding to achieve a constant window size for shifted window partitioning. We also add a convolution parallel to attention, which complements attention to improve the capability of aggregate information.
%     % \item We propose a better image reconstruction module than global residuals, and a more lightweight feature map fusion module.
%     \item Together with other minor architectural modifications, we build multiple DehazeFormer variants and surpass the state-of-the-art methods in various experimental setups.
%     \item We collect a new large-scale realistic remote sensing image dehazing dataset for evaluating the method's dehazing capability for non-homogeneous haze.
% \end{itemize}
