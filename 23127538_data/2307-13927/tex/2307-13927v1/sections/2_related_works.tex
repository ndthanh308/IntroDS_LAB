% Figure environment removed

\section{Related Works}
\subsection{Density-aware Dehazing Methods}

In recent years, several methods \cite{zhang2021hierarchical,deng2020hardgan,guo2022image,yang2022self,wang2021haze,yi2022two,yeperceiving} have attempted to improve the dehazing performance by enabling the network to perceive haze density.

\subsubsection{Density-awareness via estimating T-map} Haze density is influenced by several factors and is inversely proportional to T-map, so some methods learn density information by estimating T-map. Lou et al. \cite{lou2020integrating} predict a T-map first for nighttime image dehazing. Zhang et al. \cite{zhang2021hierarchical} estimate a low-resolution T-map and then jointly input the feature map and the estimated T-map to a Laplacian pyramid decoder to achieve a restored image. Yang et al. \cite{yang2022self} propose a semi-supervised method that does not require paired data. The method estimates T-map, scattering coefficient, and depth to reconstruction hazy images and restores clear images. However, these methods require additional labeled data and might be inaccurate due to the complexity of practical scenes \cite{li2017aod}.
% The Haze-Aware Feature Distillation (HARD) module is designed in \cite{deng2020hardgan}, which introduces two dual-channel maps to describe the atmospheric brightness and pixel-wise spatial information of each feature channel respectively, and calculates a haze aware map through an InstanceNorm followed by a sigmoid layer. Finally it fuses the above three factors.

% Figure environment removed

\subsubsection{Density-awareness via extracting density features directly} Research works \cite{deng2020hardgan,chen2020unsupervised,yeperceiving} directly learn haze density information without estimating a T-map. Deng et al. \cite{deng2020hardgan} design a Haze-Aware Representation Distillation (HARD) module to extract global brightness and a haze-aware map. Chen et al. \cite{chen2020unsupervised} propose an attention mechanism based on dark channel prior to describe haze concentration. However, not estimating the T-map would result in a lack of a comparator to measure density. Generating intermediate results and using the information contained therein can address this issue.

\subsection{Dehazing Methods with Intermediate Results}
Considering the difficulty of recovering images directly from the haze input, dehazing methods \cite{bai2022self,chen2021desmokenet,yeperceiving,hong2022uncertainty} which generate intermediate results (or one result) inside the network to facilitate the dehazing process are proposed. Bai et al. \cite{bai2022self} first generate a reference image by a deep pre-dehazer, and then develop a progressive feature fusion module to fuse the hazy and reference features, which achieves high metrics on several datasets. Chen et al. \cite{chen2021desmokenet} first remove light and thick smoke by a Smoke Remove Network (SRN) to gain a coarse output, which is concatenated with the original input and fed to a Pixel Compensation Network (PCN) to recover the missing pixels in the thick smoke. Hong et al. \cite{hong2022uncertainty} propose an Uncertainty-Driven Dehazing Network. In this method, intermediate results are together generated with uncertainty maps for uncertainty features extraction. Ye et al. \cite{yeperceiving} also pre-generate a pseudo-haze-free image. The hazy input and the pseudo-haze-free image are concatenated to estimate a Density Encoding Matrix describing the relationship between haze density and absolute position and mixed up to the following deep layers.

Despite the above methods extracting feature from intermediate results, they do not fully consider the differences between these results and the haze inputs, especially the differences in haze density. Simple concatenation \cite{chen2021desmokenet,bai2022self} or linear summation \cite{yeperceiving} might lead the networks to rely on the uncertain learning process and lose the capture of information about the differences between the two images. In addition, the lack of a targeted design that addresses the relationship between the intermediate results and the original input leads the extracted features not fine enough and limits the dehazing performance.

Our DFR-Net improves on the aforementioned methods by exploring and refining density features through the utilization of density differences between a generated proposal image and the hazy input, thereby achieving an awareness of haze density and superior dehazing performance.

