% This section aims to assess the performance and effectiveness of the proposed method by conducting comprehensive experiments and comparing the results with state-of-the-art approaches.
The present section endeavors to appraise the performance and efficacy of the proposed methodology through a series of challenging experiments, and subsequently compare the results against those achieved by current state-of-the-art techniques.

\subsection{Datasets} \label{sec:dataset}
\BfPara{Training datasets} 
In accordance with recent developments in the field of deepfake detection \cite{li2021frequency, li2020face, liu2021spatial, luo2021generalizing, khormali2022dfdt}, this study endeavors to evaluate the efficacy and performance of the proposed self-supervised graph Transformer model against different real-world settings through a comprehensive set of experiments. To this end, the model is trained and evaluated using several well-established and challenging deepfake forensics datasets, including FaceForensics++ \cite{rossler2019faceforensics++}, {Celeb-DF (V2)} \cite{Celeb_DF_cvpr20}, and {WildDeepfake} \cite{zi2020wilddeepfake}. A concise overview of these datasets is provided below.
% , while a summary of their statistical details is presented in \autoref{tab:CelebDFV2}. 

\BfPara{FaceForensics++} The FaceForensics++ dataset encompasses a diverse range of four distinct types of image forgeries, specifically Deepfakes~\cite{Faceswap}, FaceSwap~\cite{Faceswap1}, Face2Face~\cite{thies2016face2face}, and NeuralTextures~\cite{thies2019deferred}. The FaceForensics++ dataset is thoughtfully curated and available in three varying compression rates, namely heavily compressed (LQ), slightly compressed (HQ), and uncompressed (Raw). While the Raw dataset exhibits relatively facile identification of deepfakes, the process becomes considerably more challenging with higher compression rates. In this manuscript, the HQ version of the dataset is predominantly utilized, unless specified otherwise.
\BfPara{Celeb-DF (V2)} The Celeb-DF (V2) dataset represents a significant advancement in synthetic video generation due to its superior face-swapping strategy that elevates its visual quality above previous efforts. The dataset encompasses an extensive collection of 5639 videos, all of which exhibit a high level of visual quality, making it an ideal resource for training and testing deepfake detection models. 
\BfPara{WildDeepfake} The WildDeepfake is a collection of fabricated videos sourced entirely from the Internet, making it a distinct and demanding real-world dataset. Unlike other AI-generated deepfakes, such as Celeb-DF (V2) and FaceForensics++, the WildDeepfake dataset incorporates multiple facial expressions and encompasses a wider range of scenarios, with numerous individuals appearing in each scene.


\BfPara{Testing datasets} In addition to the aforementioned datasets, the official test set of several additional benchmarks are utilized to investigate the generalizability of the proposed model. Specifically, these benchmarks are the DeeperForensics~\cite{jiang2020deeperforensics}, FaceShifter~\cite{li2019faceshifter}, and DeepFake Detection Challenge (DFDC) ~\cite{dolhansky2019deepfake} datasets. The DeeperForensics dataset contains over 11,000 synthetic videos generated by leveraging a specific Variational Auto-Encoder (VAE) on the real videos from FaceForensics++. Similarly, the FaceShifter dataset is a collection of videos created by applying advanced face-swapping techniques to the real videos from the same dataset. This presents a valuable opportunity to assess the robustness and effectiveness of the proposed self-supervised graph Transformer model in detecting deepfake videos across a wide range of scenarios, including those that incorporate different sophisticated manipulation techniques based on the same underlying source videos. In addition, the DFDC dataset, comprising over 4,000 manipulated videos created using various GAN-based and non-learned techniques from 1,000 real videos, provides a complementary benchmark against which the proposed method can be evaluated.

 

\subsection{Implementation Specifics}
\textbf{Deepfake Encoder Training.}
The present study adopts an architecture based on vision Transformers \cite{dosovitskiy2020image} as the fundamental building block of the feature extractor $f$, which is trained through the utilization of the previously mentioned contrastive learning approach expounded upon in~\textsection\ref{sec:SSL}. Specifically, the feature extractor model undergoes pre-training and fine-tuning on images of size $320 \times 320$, with patch sizes of $20 \times 20$ yielding a total of 256 patch tokens. The projection head $h$, which plays a central role in feature extraction, is established by means of a multi-layer perceptron comprising three layers. Importantly, to foster effective information sharing between the $\mathrm{CLS}$ and the patch tokens, the projection head parameters are shared, notably, ${{h}_{s}}^{[\mathrm{CLS}]} = {{h}_{s}}^{\mathrm{patch}}$ and ${{h}_{t}}^{[\mathrm{CLS}]} = {{h}_{t}}^{\mathrm{patch}}$. This enables the feature extractor model to capitalize on both the global and local context provided by the $\mathrm{CLS}$ and patch tokens, respectively. 
% Moreover, a masked image modeling approach wherein the prediction ratio $r$ is randomly set for utilization. With an equal probability of 50\%, $r$ is either set to zero or uniformly sampled from the interval of 0.1 to 0.5, thereby generating distorted images. 
The training regimen is executed for 800 epochs with a learning rate that exhibits a linearly scaled progression in accordance with the batch size, as determined by the expression: $\operatorname{lr}=5 e^{-4} \times {batch_{size}} / 256$.

% The model is pre-trained and fine-tuned using $320 \times 320$ size images and patch sizes of $20 \times 20$, yielding to 256 patch tokens in total. 
% A multi-layer perceptron with three layers constructs the projection head $h$, which shares same parameters between $[\mathrm{CLS}]$ and ${\mathrm{patch}}$ tokens, \ie ${{h}}_{s}^{[\mathrm{CLS}]} = {{h}}_{s}^{\mathrm{patch}}$ and ${{h}}_{t}^{[\mathrm{CLS}]} = {{h}}_{t}^{\mathrm{patch}}$. 
The preeminent implementation specifications at hand entail the pre-training of a self-supervised feature extractor on a conflation of training sets procured from three distinct benchmarks explicated in~\textsection\ref{sec:dataset}. 

Upon the conclusion of training, the parameters of the feature extractor are fixed and employed as a pre-trained feature extractor for the purpose of node feature vector extraction, which in turn facilitates the representation of the deepfake images as a graph.
The graph Transformer classifier is comprised of a graph convolutional layer, and three blocks of Transformer layers, each of which comprises 8 self-attention heads, a dimensional parameter of D=256, and an MLP size of 512 (Equation \ref{eq:attention}). This construction is indicative of the sophistication that underlies the proposed approach.


The experimental evaluation of the proposed framework was carried out on a Lambda Quad deep learning workstation machine equipped with 4 NVIDIA GeForce GTX 1080 Ti Graphics Processing Units (GPUs), 64 GB DDR4 RAM, an Intel Coreâ„¢ i7-6850K CPU, and running the Ubuntu 20.04.3 LTS operating system. This machine configuration facilitated the efficient training and evaluation of the models, thanks to its high-performance computing capabilities. The performance evaluation of the proposed framework was conducted using accuracy and area under the receiver operating characteristic curve (AUC) metrics in different experimental settings. Comprehensive assessment of the proposed framework's performance was enabled through the use of these stringent evaluation metrics, which were employed to compare it with state-of-the-art methods and to provide valuable insights into the practical applicability of the approach.










