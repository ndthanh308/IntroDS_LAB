
% Figure environment removed





\subsection{Deepfake Detection}
Recent years have witnessed a growing body of research aimed at addressing the pressing challenges posed by deepfakes and developing robust detection models capable of thwarting their nefarious effects on society. A variety of techniques have been proposed in the literature to this end, with a rich diversity of approaches being explored \cite{agarwal2020detecting, haliassos2021lips, khormali2021add, khormali2022dfdt, li2020face, liu2021spatial, wang2022m2tr, zhao2021multi}.

One popular strategy is to leverage the implicit visual artifacts of deepfakes' generation process to construct a reliable detection framework \cite{li2020face, matern2019exploiting}. These approaches capitalize on the inconsistencies and discrepancies that inevitably arise during the creation of deepfakes, such as blending artifacts or anomalies in the frequency spectrum. Examples of such approaches include Patch-wise Consistency Learning \cite{zhao2021learning} and Face X-ray \cite{li2020face}, which exploit image blending inconsistency traces between the manipulated face region and the background. These approaches exemplify the diverse range of methods used to detect deepfakes, with some leveraging specific visual artifacts while many mainstream deepfake detection methodologies rely heavily on the versatility of convolutional neural networks. For instance, \cite{bonettini2021video, khormali2021add, marra2019gans, rossler2019faceforensics++} rely on the exceptional ability of convolutional neural networks to capture complex spatial features and relationships as the primary building block of their proposed deepfake detection framework. 


Additionally, recent research has explored alternative avenues, such as analyzing the temporal dynamics of videos or using frequency spectrum analysis to detect deepfakes based on unique spectral signatures. For instance, the temporal dynamics of videos have been explored as a means to recongnize fake videos \cite{dang2020detection, sabir2019recurrent, zhao2023istvt, yin2023dynamic}. This approach involves examining the directional changes in the video's temporal characteristics, thereby detecting any inconsistencies or abnormal patterns indicative of deepfakes. Furthermore, frequency spectrum analysis is gaining significant traction in research interest \cite{durall2020watch, frank2020leveraging, li2021frequency, liu2021spatial, luo2021generalizing, masi2020two, qian2020thinking}. This technique capitalizes on leveraging the identification of spectral signatures exhibited by deepfakes, such as the existence of high-frequency noise or low-pass filtering artifacts, enabling the detection of manipulated content.



% Over the last few years, different studies have suggested various approaches to detect deepfakes and mitigate their societal threats \cite{agarwal2020detecting, haliassos2021lips, li2020face, khormali2022dfdt, khormali2021add, liu2021spatial, wang2022m2tr, zhao2021multi}. 
% One approach is to exploit specific visual artifacts inherent to the generation process to build a deepfake detection model \cite{li2020face, matern2019exploiting, mccloskey2019detecting}. For example, the image blending inconsistency traces between the manipulated face region and the background were used in Patch-wise Consistency Learning \cite{zhao2021learning} and Face X-ray \cite{li2020face}. Convolutional neural networks are a common theme of mainstream deepfake detection methodologies  \cite{bonettini2021video, marra2019gans, rossler2019faceforensics++, yu2019attributing}. Investigating the temporal directional changes in videos is another approach to building detection models \cite{guera2018deepfake, dang2020detection, sabir2019recurrent}. Furthermore, frequency spectrum analysis is among the other categories of deepfake detection approaches \cite{qian2020thinking, durall2020watch, frank2020leveraging, liu2021spatial, li2021frequency, luo2021generalizing, masi2020two, qian2020thinking}. 


In light of recent advancements in deepfake generation techniques that have reduced the visual anomalies, the detection methods have shifted focus towards more sophisticated approaches such as attention mechanisms \cite{dang2020detection, wang2021representative, zhao2021multi, khormali2021add, yang2023masked} and vision Transformers \cite{khormali2022dfdt, wang2022m2tr, wang2022adt}. For example, Wang et al. \cite{wang2022m2tr} proposed a multi-regional attention mechanism to enhance deepfake detection performance. Additionally, vision Transformers have been employed to establish an end-to-end deepfake detection framework \cite{khormali2022dfdt, wang2022lisiam}. Furthermore, recent research has utilized pre-trained networks and developed Lipforensics \cite{haliassos2021lips} for analyzing lip prints in lip-reading tasks. This work highlights that while a robust pre-trained lip feature extractor could have far-reaching implications, acquiring a large-scale and well-annotated dataset could be a daunting task. Given the potential limitations of relying on annotated data, this study proposes a self-supervised vision Transformer for pretraining the feature extractor without the need for annotation or labels. The resulting framework is scalable and achieves good performance.

% More recently, with advancements in deepfake generation techniques that leave fewer visual anomalies, the detection techniques have focused on more advanced approaches, including attention mechanisms \cite{dang2020detection, wang2021representative, zhao2021multi, khormali2021add} and vision Transformers \cite{khormali2022dfdt, wang2022m2tr, wang2022adt}. For instance, a multi-regional attention mechanism was introduced in \cite{wang2022m2tr} to improve deepfake detection performance. Moreover, vision Transformers are employed to implement an end-to-end deepfake detection approach \cite{khormali2022dfdt}. Recent studies used a pre-trained network and proposed Lipforensics \cite{haliassos2021lips} to analyze lip prints for lip-reading tasks. This work shows that although a robust pre-trained lip feature extractor could have far-reaching implications, providing a large-scale well-annotated dataset might be extortionate. 
% Although such a robust pre-trained lip feature extractor could have far-reaching implications, providing a large-scale well-annotated dataset might be extortionate. Therefore, this study proposes a self-supervised vision Transformer for feature extractor pretraining without requiring annotation or labels that obtain good scalability.

% \textcolor{red}{Recently, researchers find that high-level semantic features show excellent robustness in dealing with both crossdata tests and post-processing operations.  Lipforensics [18] extract the representations of lip movement by using a pretrained network. The lip feature extractor is pre-trained in a supervised manner on Lipreading in the Wild(LRW) [10] dataset, which is commonly used for the lipreading tasks. The network takes a multi-branch temporal convolution network(MSTCN) [16] as backend and fine-tuned on deepfake datasets. It achieves state-of-the-art generalization ability.However, to pre-train such a robust lip feature extractor requires a large-scale well-annotated dataset, which is extremely costly. In this paper, we propose a self-supervised Transformer for pre-training, in this way, we can significantly reduce the annotation cost for pre-training data and obtain good scalability.}


\subsection{Self-Supervised Contrastive Learning}
A meaningful visual representation of a given input image could be learned through self-supervised contrastive learning methods without relying on labeled data \cite{oord2018representation}. Recently, self-supervised contrastive learning approaches have gained tremendous attention in building pre-trained models that can be used for fine-tuning different downstream tasks. Contrastive learning maximizes the similarity between positive pairs while repelling features of negative pairs \cite{chen2020simple}. Although contrastive learning approaches have been utilized for dense visual representation learning tasks \cite{o2020unsupervised, wang2021dense}, recent studies suggest that removing negative pairs yields better performances \cite{grill2020bootstrap, caron2020unsupervised, caron2021emerging, chen2021exploring}. While Random augmentations have been widely used to generate diverse views of the same image \cite{chen2020simple, chen2021empirical, fung2021deepfakeucl}, this work, inspired by  \cite{bao2022beit}, proposes a masked image modeling approach to extract more robust semantic features through a contrastive learning methodology. In contrast to previous work that only focused on mouth regions for lip-forensics~\cite{zhao2022self}, this work leverages the entire facial region to improve the detection performance. The proposed masked image modeling approach was utilized to train a pre-trained model, with a vision Transformer serving as its backbone. Through this approach, complex semantic features from input images were captured by the model, resulting in improved deepfake detection performance.
