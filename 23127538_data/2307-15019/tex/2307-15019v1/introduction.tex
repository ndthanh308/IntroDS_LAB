
The rise of Artificial Intelligence (AI) empowered face manipulation/generation technologies have enabled individuals' expressions or appearances to be realistically altered with minimal expert knowledge \cite{dolhansky2019deepfake, jiang2020deeperforensics, li2020advancing, Celeb_DF_cvpr20, rossler2019faceforensics++, zi2020wilddeepfake}. This technology poses a severe and large-scale societal threat, as it facilitates the creation and dissemination of malicious content, such as digital kidnapping, ransomware, and other forms of criminal activity \cite{chen2021defakehop, khormali2021add} that are among the most insidious forms of misinformation \cite{tran2021high}. Consequently, the development of reliable deepfake detection methods has become an urgent need and has garnered significant attention in recent years. Although several efforts have been made to defend against the growing threat of forged digital media, the effectiveness of these approaches have primarily been limited to in-dataset settings \cite{agarwal2020detecting, haliassos2021lips, li2020face, liu2021spatial, masi2020two, qian2020thinking, rossler2019faceforensics++, zhao2021multi, khormali2022dfdt}. Therefore, the need to develop more robust and effective deepfake detection methods that can operate in real-world scenarios irrespective of the forgery type, appearance, or quality has become increasingly pressing.

In the domain of deepfake detection, previous investigations have predominantly relied upon low-level texture features to capture common artifacts that are inherent to the generation process. Nonetheless, such approaches are susceptible to severe performance degradation when applied to novel types of forgeries, rendering the detection of real-world deepfakes a challenging task, especially when the differences between genuine and manipulated media are nuanced. As a result, an effective deepfake detection model ought to be impartial towards forgery type, appearance, and quality, in order to ensure applicability in real-world scenarios. To this end, several techniques have been proposed to enhance the performance and generalization of deepfake detectors, such as targeting the blending boundary between the background and the altered face \cite{li2020face}, utilizing 3D decomposition \cite{zhu2021face}, truncating classifiers \cite{chai2020makes}, amplifying multi-band frequencies \cite{masi2020two}, and augmenting data \cite{wang2020cnn}. Despite their effectiveness in cross-data evaluations, low-level texture cues may be vulnerable to degradation through standard post-processing procedures, such as video compression \cite{haliassos2021lips}. Consequently, it is imperative to develop novel high-level features that are invariant to image manipulations and exhibit resilience to post-processing manipulations to enhance the current state-of-the-art in deepfake detection. 

Moreover, The widely-used convolutional neural network and Transformer treat the image as a grid or sequence structure, which is not flexible to capture irregular and complex facial landmarks. Instead of the regular grid or sequence representation, this work processes the deepfake image in a more flexible way. Human facial images can be viewed as a composition of parts, such as eyes, ears, nose, etc. These facial regions naturally form a graph structure. This graph representation of deepfakes offers a better understanding of local pixels and their interconnection with other parts.  Moreover, the graph is a generalized data structure that grid and sequence can be viewed as a special case of the graph. Viewing an image as a graph is more flexible and effective for visual perception.


Given the limitations of existing methods, building a detection model that capitalizes on high-level visual representations, which tend to be more resilient to post-processing perturbations, could potentially mitigate the dependence on dataset-specific patterns, leading to improved recognition performance across various forgery types. In light of this, this study first proposes a self-supervised pre-training approach for deepfake facial representation learning. This approach utilizes a contrastive learning framework based on masked image modeling to extract high-level visual representations of facial landmarks that are invariant to variations in lighting, pose, and other factors that are irrelevant to the identity of the object or scene. The contrastive learning method maximizes the agreement between two different augmentations of the same image using a contrastive loss in the latent space \cite{chen2020simple, he2020momentum, tschannen2020self}. This means that the learned representation is not biased towards any particular class or label, but rather captures the underlying structure of the input data.  

Moreover, the present work offers an innovative approach that integrates the unique characteristics of Graph Convolutional Networks (GCNs) and Transformer architectures to capture complex dependencies between distinct regions of an image and acquire more informative representations for deepfake detection. In the context of deepfake detection, the input data can be projected as a graph, wherein nodes denote various regions of the image, such as eyes or ears, and edges represent the association between these regions. This is particularly useful in detecting deepfakes, where only certain regions of the image may be manipulated to create realistic but false identities. The utilization of GCNs allows for modeling local relationships between nodes in the input graph, providing the ability to learn complex features that capture the spatial correlations between different facial landmarks. In contrast, Transformers exhibit exceptional efficacy in encoding long-range correlations and global interdependencies between pixels, rendering them especially appropriate for deepfake detection tasks. Given that adversarial entities can manipulate multiple regions of an image concurrently, the ability to model such complex relationships is crucial for reliable detection. Finally, this work introduces a graph Transformer relevancy map that contributes valuable insights into the model's explainability. By generating a saliency map that highlights the importance of individual regions towards the output class label, this component facilitates a more fine-grained analysis of the model's decision-making process and allows for a better understanding of the specific features and regions of the image that are crucial for deepfake detection. \autoref{fig:SSLGT} shows the general framework of the proposed deepfake detection model using a self-supervised contrastive learning approach and graph Transformer architecture. 


\BfPara{Contributions}
Together, these contributions offer a promising solution to the urgent need for robust and effective deepfake detection methods that can operate irrespective of the forgery types, appearances, or qualities in real-world scenarios. The proposed framework's use of high-level visual representations that are invariant to post-processing perturbations, combined with the ability to capture complex interdependencies between regions of an image, provides a comprehensive solution that outperforms existing methods. Below are the key contributions of this study:

\begin{itemize}

    \item The present study introduces a self-supervised pre-training approach based on contrastive learning to extract more high-level visual representations that are invariant to variations in compression, blur, and other factors irrelevant to the subject's identity. This approach significantly enhances the generalizability of deepfake detection models, thereby reducing their dependence on dataset-specific patterns. 

    \item Furthermore, this paper proposes an innovative deepfake detection approach by leveraging the expressive power of graph convolutional networks and Transformers to capture intricate interdependencies among different regions of an image and acquire more informative representations. Unlike traditional convolutional neural networks, which are limited in their ability to model non-local relationships between pixels, the proposed graph Transformer can encode both local and global dependencies, rendering them particularly suited to deepfake detection tasks.
    
    \item  By providing detailed and granular insights into the underlying reasoning behind the model's predictions, the proposed graph Transformer relevancy map offers a more thorough and rigorous understanding of the complex interdependencies and salient features that drive the detection process, while suppressing irrelevant or redundant information. Thus, facilitating a rigorous and accurate examination of deepfakes, contributing to enhanced detection performance and increased reliability.
       
    \item The proposed framework's efficacy and generalizability were rigorously evaluated via a comprehensive set of experiments, encompassing a diverse range of challenging scenarios spanning both in-distribution and out-of-distribution settings. The experimental results unequivocally establish the framework's superiority, with exceptional in-dataset detection accuracy being achieved. Moreover, the proposed self-supervised pre-training feature extractor constitutes a significant contribution to the field, having markedly improved the framework's ability to generalize across multiple datasets while simultaneously enhancing its resilience to post-processing perturbations, such as compression and blur \etc
    
\end{itemize}


\BfPara{Organization} The rest of the paper is structured as follows. 
In \textsection\ref{sec:RelatedWork}, a review of recent research at the intersection of deepfake detection and self-supervised contrastive learning is provided. The proposed self-supervised graph Transformer deepfake detection approach, including its key components and the rationale for their selection, is outlined in \textsection\ref{sec:Methodology}.  The evaluation settings, including datasets, implementation specifics, and evaluation metrics, are described in~\textsection\ref{sec:EvaluationSettings}. In~\textsection\ref{sec:Results}, the obtained deepfake detection results and their implications are presented and discussed in comparison with existing methods. Finally, in~\textsection\ref{sec:Conclusion}, concluding remarks are drawn to summarize the contributions of this study and outline avenues for future research.