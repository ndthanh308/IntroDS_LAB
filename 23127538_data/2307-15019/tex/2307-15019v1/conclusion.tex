

This work presents a self-supervised graph Transformer framework for deepfake detection that is comprised of three main components. The proposed framework leverages the expressive power of self-supervised contrastive learning to learn high-level representations of deepfakes. Unlike existing supervised approaches, which depend on low-level visual fingerprints, the extracted high-level visual representations via this approach significantly enhance the generalizability of deepfake detection models, thereby reducing their dependence on dataset-specific patterns. Furthermore, the current work leverages the expressive power of graph convolutional networks and Transformer blocks to capture intricate interdependencies among both local and global regions of an image, rendering them particularly suited to deepfake detection tasks. In addition, the proposed graph Transformer relevancy map provides a comprehensive understanding of the complex interdependencies and salient features which leads to enhanced transparency and accountability of the detection model. Finally, the efficacy and generalizability of the proposed framework were rigorously evaluated via a comprehensive set of experiments, encompassing a diverse range of challenging scenarios spanning both in-distribution and out-of-distribution settings. The experimental results unequivocally establish the framework's superiority, with exceptional in-dataset detection accuracy being achieved. Moreover, the proposed self-supervised pre-training feature extractor constitutes a significant contribution to the field, having markedly improved the framework's ability to generalize across multiple datasets while simultaneously enhancing its resilience to post-processing perturbations, such as compression and blur.


