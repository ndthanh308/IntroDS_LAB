% The proposed deepfake detection approach consists of four main steps: graph construction, self-supervised contrastive learning, graph Transformer, and class activation map. 

The proposed deepfake detection method entails four fundamental components, as illustrated in \autoref{fig:SSLGT}, which are deepfake graph construction, deepfake encoder based on vision Transformer architecture that is pre-trained using a self-supervised contrastive learning approach, graph Transformer classifier, and graph Transformer relevancy map. This section is dedicated to elaborating on each of these components in a more comprehensive manner.


\subsection{Deepfake Graph Representation}
% With the advent of deepfake techniques, traditional approaches such as commonly-used grid or sequence structures for image manipulation detection have become inadequate. 
The irregularity of facial regions' shape constitutes a major obstacle in employing conventional techniques, such as grid-based architectures in CNNs or sequence-based structures in Transformers, to process facial images. Such approaches are characterized by redundancy and inflexibility, rendering them incapable of efficiently addressing the complex structural nuances of facial regions. As a consequence, there is a need for more sophisticated and adaptive techniques that can account for the varying geometries and topologies of these regions. In this regard, the present study advocates the adoption of graph representation of deepfakes as a highly promising method to tackle the issue of deepfake detection, owing to its capacity to account for the varying topologies and geometries of the facial regions. By leveraging this approach to represent deepfake images as graphs, it becomes possible to capture the underlying structural and relational features of key facial landmarks, such as the eyes, lips, and ears. This capability, in turn, enables the identification of potentially manipulated regions and their relationship within the image, thereby capturing more complex features for enhanced and robust deepfake detection. 


The construction of an undirected graph from an image input is a pivotal step in the proposed deepfake detection methodology. In essence, the process of representing a given input image $I$ as a graph $G = (V, E)$ involves partitioning it into $N$ patches, where $V = \left\{ v_{1}, v_{2}, \dots, v_{N} \right\}$ signifies the nodes of the graph and $E$ denotes its corresponding edges. This entails representing feature embeddings extracted from image patches as graph nodes, denoted by $v_i \in \mathbb{R}^D$, where $D$ is the dimensionality of the embedding vector. The adjacency relationship among these patches constitutes the graph edges $E$. Notably, edges within the graph are established by means of a spatial proximity criterion that determines the association of each patch with its neighboring counterparts, allowing for at least one, and up to $K$ connections between $K$ nearest neighboring patches. To encode the adjacency relationship between patches, an adjacency matrix, $A = [A_{ij}]$, is formulated, is formulated, where $A_{ij} = A_{ji} = 1$ if the nodes $v_{i}$ and $v_{j}$ are in the neighborhoods, and $A_{ij} = A_{ji} = 0$ otherwise, that is if $(v_i, v_j) \notin E$. By modeling deepfake images in this way, the structural relationships and dependencies among the various facial components can be captured and analyzed in a more comprehensive manner. Furthermore, the efficacy of the proposed methodology depends crucially on the feature vector's ability to provide a potent and resilient representation of the node in question. To achieve this, the present study utilizes self-supervised contrastive learning and a vision Transformer structure to extract high-level visual representations of deepfake images. This enables the detection system to capture the salient attributes of the underlying image regions, resulting in more informative and discriminative features. As high-level features are less vulnerable to manipulations, this approach maximizes the model's potential to generalize to new data and perform well on downstream tasks.


% Figure environment removed


\subsection{Self-Supervised Contrastive Learning} \label{sec:SSL}


Existing deepfake detection methodologies heavily rely on weakly supervised learning approaches for visual feature extraction. While such approaches exhibit high detection performance on in-distribution samples, their ability to generalize to unseen samples is limited. This generalizability issue poses a significant challenge to real-world applications where the model must be robust to unseen scenarios. To overcome this issue, this study proposes the utilization of self-supervised contrastive learning for robust visual feature extraction without the need for explicit labels. Specifically, the proposed approach employs masked image modeling, which applies a random mask to an input image with $N$ token sequences and a prediction ratio of $r$, to train a vision Transformer for generating patch representations. The model maximizes the concordance between two distinct views of the same image through contrastive loss in a latent space. This approach is inspired by recent studies on self-supervised contrastive learning, including \cite{chen2020simple}, and builds on the successful application of masked image modeling in \cite{bao2022beit, zhou2022image}. Figure \ref{fig:SSL} illustrates the proposed self-supervised contrastive learning framework for deepfake representation learning. The resulting visual features are expected to be more informative and discriminative, improving the model's ability to generalize to unseen data and achieve high performance on downstream tasks.


The proposed training methodology commences with the random selection of an image $x$ from the training set, followed by the application of random augmentations to generate two distorted views of the same image, denoted as $\boldsymbol{u}$ and $\boldsymbol{v}$. To enhance robustness, blockwise masking \cite{bao2022beit} is then applied to produce masked views of the augmented images, denoted as $\hat{\boldsymbol{u}}$ and $\hat{\boldsymbol{v}}$. Subsequently, the masked views undergo processing by a student-teacher network to yield predictive categorical distributions. Both the student and teacher networks share an identical architectural configuration and are parameterized by $\boldsymbol{\theta}$ and $\boldsymbol{\theta}^{\prime}$, respectively. This framework enables the self-supervised training of the student network, which endeavors to reconstruct the masked tokens by leveraging the guidance of the teacher network's output. 


In self-supervised frameworks, it is customary for the student network to acquire distilled knowledge from the teacher network by minimizing the cross-entropy loss \cite{he2020momentum, grill2020bootstrap, caron2021emerging}. However, in the present study, the deepfake images' visual representations are acquired through the minimization of two concurrent loss functions, namely: 1) $\mathcal{L}{[\mathrm{CLS}]}$, which represents the self-distillation between cross-view $[\mathrm{CLS}]$ tokens, and 2) $\mathcal{L}{[\mathrm{MIM}]}$, which represents the self-distillation between in-view patch tokens. Specifically, the self-distillation process regarding the $[\mathrm{CLS}]$ tokens of cross-view images, denoted by $\boldsymbol{u}$ and $\boldsymbol{v}$, can be formulated as a symmetric loss, as shown in Equation \ref{eq:losscls}.



\begin{equation}\label{eq:losscls}
\begin{aligned}
\mathcal{L}_{[\mathrm{CLS}]} &=\frac{1}{2} \left(-P_{\boldsymbol{\theta}^{\prime}}^{[\mathrm{CLS}]}(\boldsymbol{v})^{\mathrm{T}} \log P_{\boldsymbol{\theta}}^{[\mathrm{CLS}]}(\boldsymbol{u}) \right) + \\
&\frac{1}{2} \left(-P_{\boldsymbol{\theta}^{\prime}}^{[\mathrm{CLS}]}(\boldsymbol{u})^{\mathrm{T}} \log P_{\boldsymbol{\theta}}^{[\mathrm{CLS}]}(\boldsymbol{v}) \right)
\end{aligned}
\end{equation}

where  ${\boldsymbol{u}}_{s}^{\mathrm{CLS}}=P_{\boldsymbol{\theta}}^{\mathrm{CLS}}({\boldsymbol{u}})$, ${\boldsymbol{v}}_{s}^{\mathrm{CLS}}=P_{\boldsymbol{\theta}}^{\mathrm{CLS}}({\boldsymbol{v}})$, $\boldsymbol{u}_{t}^{\mathrm{CLS}}=P_{\boldsymbol{\theta}^{\prime}}^{\mathrm{CLS}}(\boldsymbol{u})$, and $\boldsymbol{v}_{t}^{\mathrm{CLS}}=P_{\boldsymbol{\theta}^{\prime}}^{\mathrm{CLS}}(\boldsymbol{v})$ are student and teacher network outputs for cross-view images, \ie $\boldsymbol{u}$ and $\boldsymbol{v}$, of [CLS] token.  


The student and teacher network outputs for the masked view with $m_i$ masking ratio for  $\hat{\boldsymbol{u}}$,  $\hat{\boldsymbol{v}}$ and non-masked view $\boldsymbol{u}$, $\boldsymbol{v}$ projections of patch tokens are claculated as $\hat{\boldsymbol{u}}_{s}^{\mathrm{patch}}=P_{\boldsymbol{\theta}}^{\mathrm{patch}}(\hat{\boldsymbol{u}})$,  $\hat{\boldsymbol{v}}_{s}^{\mathrm{patch}}=P_{\boldsymbol{\theta}}^{\mathrm{patch}}(\hat{\boldsymbol{v}})$ and  $\boldsymbol{u}_{t}^{\mathrm{patch}}=P_{\boldsymbol{\theta}^{\prime}}^{\mathrm{patch}}(\boldsymbol{u})$,  $\boldsymbol{v}_{t}^{\mathrm{patch}}=P_{\boldsymbol{\theta}^{\prime}}^{\mathrm{patch}}(\boldsymbol{v})$, respectively. Similarly, the self-distillation between in-view patch tokens can be formulated as a symmetric cross-entropy loss as \autoref{eq:lossmim}.
\begin{equation}\label{eq:lossmim}
\begin{aligned}
\mathcal{L}_{\mathrm{MIM}}= &=\frac{1}{2} \left(-\sum_{i=1}^{N} m_{i} \cdot P_{\boldsymbol{\theta}^{\prime}}^{\mathrm{patch}}\left(\boldsymbol{u}_{i}\right)^{\mathrm{T}} \log P_{\boldsymbol{\theta}}^{\mathrm{patch}}\left(\hat{\boldsymbol{u}}_{i}\right) \right)+ \\
&\frac{1}{2} \left( -\sum_{i=1}^{N} m_{i} \cdot P_{\boldsymbol{\theta}^{\prime}}^{\mathrm{patch}}\left(\boldsymbol{v}_{i}\right)^{\mathrm{T}} \log P_{\boldsymbol{\theta}}^{\mathrm{patch}}\left(\hat{\boldsymbol{v}}_{i}\right)\right)
\end{aligned}
\end{equation}


The overall loss function is expressed as a combination of two individual loss functions, $\mathcal{L}_{[\mathrm{CLS}]}$ and $\mathcal{L}_{\mathrm{MIM}}$, as presented in \autoref{eq:loss2}. The loss function $\mathcal{L}_{[\mathrm{CLS}]}$ involves self-distillation between cross-view $[\mathrm{CLS}]$ tokens, whereas $\mathcal{L}_{\mathrm{MIM}}$ is related to self-distillation among in-view patch tokens. 
% The total loss function is computed by summing these two individual loss functions, i.e., $\mathcal{L}= \mathcal{L}_{[\mathrm{CLS}]}+ \mathcal{L}_{\mathrm{MIM}}$. 
The proposed approach leverages the collective information obtained from both loss functions to effectively learn the deepfake image representations.

\begin{equation}\label{eq:loss2}
\mathcal{L}= \mathcal{L}_{[\mathrm{CLS}]}+ \mathcal{L}_{\mathrm{MIM}}
\end{equation}


\subsection{Graph Transformer Classifier}


The methodology employed in this study involves the use of a vision Transformer as a feature extractor, which has been trained using a self-supervised contrastive learning approach, as previously mentioned in \autoref{sec:SSL}. This approach is effective in computing the visual representation of each node in the graph. An image representation matrix $\mathcal{F} \in \mathbb{R}^{N \times D}$ is constructed by subjecting all $N$ nodes within the image to the aforementioned pre-trained ViT feature extractor. Specifically, each node $v_{i}$ is associated with a feature vector $f_{i} \in \mathbb{R}^{D}$, where $D$ denotes the dimensionality of the feature vector. The resultant matrix is defined as $F = \left\{ f_{1}, f_{2},\dots, f_{N} \right\}$. It is noteworthy that the combination of $F$ with the adjacency matrix $\mathcal{A}$ facilitates the construction of a graph representation of deepfake images. The resulting graph-structured data can be analyzed using a multi-layer graph convolutional network $f(\mathcal{F}, \mathcal{A})$ \cite{kipf2016semi}, which utilizes a layer-wise propagation rule as specified in \autoref{eq:gclayer}.

\begin{equation}\label{eq:gclayer}
\begin{array}{l}
H_{l+1}=R e L U\left(\hat{A} H_{l} W_{l}\right), \quad l=1,2, . ., L \\
\hat{A}=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}
\end{array}
\end{equation}

here, $L$ represents the total number of graph convolution layers, and the activation matrix of the $l^{th}$ graph convolutional layer is denoted as $H_l$. The activation matrix $H_l$ captures the learned representations at each layer, with the initialization of $H_0$ as the input feature matrix $F$. The weight matrix $W_l$ is specific to each layer and is learned during training. Moreover, the graph's adjacency matrix with added self-connections to each node is represented by $\tilde{A}$ and is defined as the sum of the adjacency matrix $A$ and the identity matrix $I$. A diagonal matrix $\tilde{D}$ is used to normalize the adjacency matrix, where $\tilde{D}_{ii}$ is defined as the sum of the entries in row $i$ of the normalized adjacency matrix $\tilde{A}$. The resulting matrix $\hat{A}$ is the symmetric normalized adjacency matrix of $A$.



Although graph convolutional layers have been successful in learning node-level features and have been widely used in graph neural networks, they have limitations in learning hierarchical visual features that capture the global context of the graph. In contrast, the attention mechanism in Transformer models has shown remarkable success in natural language processing tasks by capturing the long-range dependencies between tokens and their relative importance for the final prediction \cite{vaswani2017attention}. This ability is particularly important in graph-structured deepfake images, where the relationships between nodes can be complex and far-reaching. Therefore, the present work proposes to leverage attention mechanisms to analyze graph-structured data by treating the feature nodes as tokens in a sequence, and adjacency matrices as positional encodings to preserve their spatial relationship. By synergizing the attention mechanism with graph convolutional layers, the proposed model proficiently assimilates both localized and global features, and adeptly captures intricate inter-nodal relationships within the graph. Notably, the transformation of graph space data into Transformer space is enacted via a Transformer layer, as specified in \autoref{eq:graphattention1} to \autoref{eq:graphattention4}.


% \begin{table*}[t]
%     \centering
%     \caption{ The statistical specifications pertaining to the three benchmarks employed in this investigation are as follows. The selected benchmarks are characterized by their capacity to encompass a diverse range of real-world scenes, exhibit minimal visual artifacts, offer superlative levels of realism and surreptitiousness~\cite{Celeb_DF_cvpr20, zi2020wilddeepfake}, and span a wide spectrum of manipulation techniques~\cite{rossler2019faceforensics++}. } 
%     \label{tab:CelebDFV2}
%     \begin{tabular}{cccccccc}
%     \toprule
%     \multirow{2}{*}{ \textbf{Dataset} } & & \multicolumn{4}{c}{ \textbf{Statistics} } & \multirow{2}{*}{ \textbf{Source} } \\
%     \cline { 3 - 6 } & & \textbf{Videos} & \textbf{Train} & \textbf{Test} & \textbf{Val.} & \\ \midrule
%      \multirow{2}{*}{ FaceForensics++ \cite{rossler2019faceforensics++} } & Real & 1000 & 720 & 140 & 140 & YouTube \\
%     & Deepfake & 4000  & 2880 & 480 & 480 & DF \\
%      \multirow{2}{*}{ Celeb-DF (V2) \cite{Celeb_DF_cvpr20} } & Real & 590 + 300 & 632 & 62 & 196 & YouTube \\
%     & Deepfake & 5639 & 4736 & 536 & 340 & DF \\
%      \multirow{2}{*}{ WildDeepfake \cite{zi2020wilddeepfake} } & Real & 3805  & 3044 & 380 & 381 & Internet \\
%     & Deepfake & 3509 & 2807 & 350 & 351 & Internet \\
%     \bottomrule
%     \end{tabular}
% \end{table*}






\begin{equation}\label{eq:graphattention1}
\mathbf{z}_{0} =\left[x_{\mathrm{class}} ; h^{(1)} ; h^{(2)} ; \ldots ; h^{(N)}\right], \quad h^{(i)} \in H
\end{equation}
\begin{equation}
\mathbf{z}_{\ell}^{\prime}=\operatorname{MSA}\left(\mathrm{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1}, \quad \quad \ell=1 \ldots L
\end{equation}
\begin{equation}
\mathbf{z}_{\ell}=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, \quad \ell=1 \ldots L
\end{equation}
\begin{equation}\label{eq:graphattention4}
\mathbf{y}=\mathrm{LN}\left(\mathbf{z}_{L}^{0}\right)
\end{equation}

% \begin{equation}\label{eq:graphattention}
% \begin{aligned}
% t_{0} &=\left[x_{\mathrm{class}} ; h^{(1)} ; h^{(2)} ; \ldots ; h^{(N)}\right], \quad h^{(i)} \in H \\
% t_{l}^{\prime} &=\operatorname{MSA}\left(\mathrm{LN}\left(t_{l-1}\right)\right)+t_{l-1}, \quad l=1 \ldots L \\
% t_{l} &=\operatorname{MLP}\left(\mathrm{LN}\left(t_{l}^{\prime}\right)\right)+t_{l}^{\prime}, \quad l=1 \ldots L
% \end{aligned}
% \end{equation}

Here, the model architecture entails a multihead self-attention mechanism $\operatorname{MSA}$, featuring $k$ Self-Attention (SA) heads, as delineated in Equation \ref{eq:attention} and \ref{eq:MSA}, respectively. Furthermore, $\operatorname{MLP}$ represents a Multilayer Perceptron, and Layer Norm is indicated by $\mathrm{LN}$. Here, $L$ corresponds to the number of Transformer blocks, and $\mathbf{y}$ signifies the class label \cite{dosovitskiy2020image}. The Transformer model utilizes the graph feature embeddings, as outlined in Equation \ref{eq:gclayer}, to facilitate self-attention operation $\text{SA}(Q, K, V)$, based on Transformer architecture as presented in Equation \ref{eq:attention}.

% where $\operatorname{MSA}$ is a multihead self-attention with $k$ Self-Attention (SA) heads defined in \autoref{eq:attention} and \autoref{eq:MSA}, repectively, $\operatorname{MLP}$ represents a Multilayer Perceptron, $\mathrm{LN}$ denotes Layer Norm, $L$ shows number of Transformer blocks, and $\mathbf{y}$ is the class label \cite{dosovitskiy2020image}. The Transformer model utilizes the graph feature embeddings \autoref{eq:gclayer} to perform self-attention operation $\text {SA}(Q, K, V)$ based on Transformer architecture as \autoref{eq:attention}. 

\begin{equation}\label{eq:MSA}
\begin{aligned}
\text { MSA }(Q, K, V) &=\text { Concat }\left(\operatorname{SA}_{1}, \ldots, \text { SA}_{\mathrm{k}}\right) W^{O} \\
\text { where SA}_i &=\text { SA }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}
\end{equation}

\begin{equation}\label{eq:attention}
\text { SA }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\end{equation}

where, $N$ represents the number of patches, $D$ is the dimension of patch embeddings, and $\text{SA}(Q, K, V)$ denotes the pairwise similarity of two nodes based on their corresponding Query $(Q_{in})$, Key $(K)$, and Values $(V)$. Notably, the projections are parameter matrices $W_{i}^{Q} \in \mathbb{R}^{D \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{D \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{D \times d_{v}}$ and $W^{O} \in \mathbb{R}^{kd_{v} \times D}$ \cite{vaswani2017attention, dosovitskiy2020image}.

Finally, it is imperative to acknowledge that the conventional self-attention mechanism computes attention scores for all possible pairs of nodes, leading to an exponential increase in memory and time complexity as the number of nodes increases, with a complexity of $O(n^2)$. Thus, a technique that reduces the number of nodes while retaining local information is indispensable. In this investigation, a learnable pooling layer, referred to as the min-cut pooling layer \cite{bianchi2020spectral}, has been employed on the output of the final graph convolutional layer to effectively reduce the number of nodes.

% Figure environment removed

\subsection{Graph Transformer Relevancy Map}
To pinpoint areas within a given image that are likely to have been manipulated, and which exhibit a significant correlation with deepfake class labels, the present study builds upon previous research in this field \cite{chefer2021transformer} and presents a graph Transformer relevancy map. This innovative approach involves computing the class activation maps from the output class and then propagating them to the inner graph space through a propagation procedure. The current study shows that the attention relevancy of the $\ell^{th}$ Transformer block  $\bar{\mathbf{A}}^{(\ell)}$ can be obtained by representing the attention map of its Transformer block as $A^{\ell}$, which facilitates the calculation of the layer relevance score $R^{n_\ell}$ and gradient $\nabla A^{(\ell)}$ values pertaining to class $t$. It should be noted that ${n_\ell}$ corresponds to the layer that aligns with the $softmax$ operation in the $\ell^{th}$ Transformer block. Finally, as depicted in \autoref{fig:graphcam} the Transformer's final relevancy maps are represented by a weighted attention relevance, as described by \autoref{eq:cam}. 

\begin{equation}\label{eq:cam}
\begin{aligned}
\bar{\mathbf{A}}^{(\ell)} &=\mathbb{E}_{h}\left(\nabla \mathbf{A}^{(\ell)} \odot R^{\left(n_{l}\right)}\right) + I\\
\mathbf{C_{t}} &=\bar{\mathbf{A}}^{(1)} \cdot \bar{\mathbf{A}}^{(2)} \cdot \ldots \cdot \bar{\mathbf{A}}^{(L)}
\end{aligned}
\end{equation}

% where $\mathbb{E}_{h}$ denotes the average across the transformer's self-attention heads, and $\odot$ represents the Hadamard product. The identity matrix $I$ is added to prevent self-inhibition for each node. Finally, the transformer relevance map $C_{t}$ is mapped to each node in the graph based on the min-cut dense learned assignments. 

Here, $\mathbb{E}_{h}$ signifies the mean value calculated across the dimension of the self-attention heads of the Transformer model, whereas $\odot$ denotes the Hadamard product. To prevent self-inhibition for each node, the identity matrix $I$ is added. In the final step, the Transformer relevance map $C_{t}$ is mapped to each node in the graph, based on the min-cut dense learned assignments \cite{bianchi2020spectral}. This min-cut approach ensures that the most relevant nodes are assigned the highest relevance scores, thereby facilitating more accurate identification of manipulated regions within an image. The proposed methodology represents a significant advancement in deepfake detection and image forensics, by incorporating a highly effective graph-based approach with Transformer architectures that improves accuracy and enhances practical utility.

