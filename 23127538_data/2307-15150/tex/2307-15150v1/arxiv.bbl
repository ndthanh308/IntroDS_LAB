\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2020)Aghajanyan, Shrivastava, Gupta, Goyal,
  Zettlemoyer, and Gupta]{aghajanyan2020better}
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke
  Zettlemoyer, and Sonal Gupta.
\newblock Better fine-tuning by reducing representational collapse.
\newblock \emph{arXiv preprint arXiv:2008.03156}, 2020.

\bibitem[Allen-Zhu \& Li(2020)Allen-Zhu and Li]{allen2020towards}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Towards understanding ensemble, knowledge distillation and
  self-distillation in deep learning.
\newblock \emph{arXiv preprint arXiv:2012.09816}, 2020.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Ba \& Frey(2013)Ba and Frey]{ba2013adaptive}
Lei~Jimmy Ba and Brendan Frey.
\newblock Adaptive dropout for training deep neural networks.
\newblock In \emph{Proceedings of the 26th International Conference on Neural
  Information Processing Systems-Volume 2}, pp.\  3084--3092, 2013.

\bibitem[Baevski \& Auli(2018)Baevski and Auli]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Clark et~al.(2019)Clark, Luong, Le, and Manning]{clark2019electra}
Kevin Clark, Minh-Thang Luong, Quoc~V Le, and Christopher~D Manning.
\newblock Electra: Pre-training text encoders as discriminators rather than
  generators.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186, 2019.

\bibitem[DeVries \& Taylor(2017)DeVries and Taylor]{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Erhan et~al.(2009)Erhan, Manzagol, Bengio, Bengio, and
  Vincent]{erhan2009difficulty}
Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, and Pascal
  Vincent.
\newblock The difficulty of training deep architectures and the effect of
  unsupervised pre-training.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  153--160.
  PMLR, 2009.

\bibitem[Fang et~al.(2021)Fang, Wang, Wang, Zhang, Yang, and Liu]{fang2021seed}
Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng
  Liu.
\newblock Seed: Self-supervised distillation for visual representation.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{arXiv preprint arXiv:2101.03961}, 2021.

\bibitem[Furlanello et~al.(2018)Furlanello, Lipton, Tschannen, Itti, and
  Anandkumar]{furlanello2018born}
Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima
  Anandkumar.
\newblock Born again neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1607--1616. PMLR, 2018.

\bibitem[Gal \& Ghahramani(2016{\natexlab{a}})Gal and
  Ghahramani]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pp.\
  1050--1059. PMLR, 2016{\natexlab{a}}.

\bibitem[Gal \& Ghahramani(2016{\natexlab{b}})Gal and
  Ghahramani]{gal2016theoretically}
Yarin Gal and Zoubin Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 1019--1027, 2016{\natexlab{b}}.

\bibitem[Gao et~al.(2021)Gao, Yao, and Chen]{gao2021simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{arXiv preprint arXiv:2104.08821}, 2021.

\bibitem[Gotmare et~al.(2019)Gotmare, Keskar, Xiong, and
  Socher]{gotmare2018closer}
Akhilesh Gotmare, Nitish~Shirish Keskar, Caiming Xiong, and Richard Socher.
\newblock A closer look at deep learning heuristics: Learning rate restarts,
  warmup and distillation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hahn \& Choi(2019)Hahn and Choi]{hahn2019self}
Sangchul Hahn and Heeyoul Choi.
\newblock Self-knowledge distillation in natural language processing.
\newblock In \emph{Proceedings of the International Conference on Recent
  Advances in Natural Language Processing (RANLP 2019)}, pp.\  423--430, 2019.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Hermann et~al.(2015)Hermann, Ko{\v{c}}isk{\`y}, Grefenstette,
  Espeholt, Kay, Suleyman, and Blunsom]{hermann2015teaching}
Karl~Moritz Hermann, Tom{\'a}{\v{s}} Ko{\v{c}}isk{\`y}, Edward Grefenstette,
  Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock \emph{arXiv preprint arXiv:1506.03340}, 2015.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012improving}
Geoffrey~E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan~R Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv preprint arXiv:1207.0580}, 2012.

\bibitem[Hochreiter \& Schmidhuber(1995)Hochreiter and
  Schmidhuber]{hochreiter1995simplifying}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Simplifying neural nets by discovering flat minima.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  529--536, 1995.

\bibitem[Huang et~al.(2018)Huang, Liu, Lang, Yu, Wang, and
  Li]{huang2018orthogonal}
Lei Huang, Xianglong Liu, Bo~Lang, Adams Yu, Yongliang Wang, and Bo~Li.
\newblock Orthogonal weight normalization: Solution to optimization over
  multiple dependent stiefel manifolds in deep neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456. PMLR, 2015.

\bibitem[Kang et~al.(2016)Kang, Li, and Tao]{kang2016shakeout}
Guoliang Kang, Jun Li, and Dacheng Tao.
\newblock Shakeout: A new regularized deep neural network training scheme.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kolesnikov et~al.(2019)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2019big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock \emph{arXiv preprint arXiv:1912.11370}, 6\penalty0 (2):\penalty0 8,
  2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krogh \& Hertz(1992)Krogh and Hertz]{krogh1992simple}
Anders Krogh and John~A Hertz.
\newblock A simple weight decay can improve generalization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  950--957, 1992.

\bibitem[Labach et~al.(2019)Labach, Salehinejad, and Valaee]{labach2019survey}
Alex Labach, Hojjat Salehinejad, and Shahrokh Valaee.
\newblock Survey of dropout methods for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1904.13310}, 2019.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2020bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  7871--7880, 2020.

\bibitem[Liang et~al.(2021)Liang, Hao, Shen, Zhou, Chen, Chen, and
  Carin]{liang2021mixkd}
Kevin~J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou
  Chen, and Lawrence Carin.
\newblock Mixkd: Towards efficient distillation of large-scale language models.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lin \& Hovy(2002)Lin and Hovy]{lin2002manual}
Chin-Yew Lin and Eduard Hovy.
\newblock Manual and automatic evaluation of summaries.
\newblock In \emph{Proceedings of the ACL-02 Workshop on Automatic
  Summarization}, pp.\  45--51, 2002.

\bibitem[Liu et~al.(2020)Liu, Duh, Liu, and Gao]{liu2020very}
Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao.
\newblock Very deep transformers for neural machine translation.
\newblock \emph{arXiv preprint arXiv:2008.07772}, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock \emph{arXiv preprint arXiv:2103.14030}, 2021.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2018regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing lstm language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mobahi et~al.(2020)Mobahi, Farajtabar, and Bartlett]{mobahi2020self}
Hossein Mobahi, Mehrdad Farajtabar, and Peter~L Bartlett.
\newblock Self-distillation amplifies regularization in hilbert space.
\newblock \emph{arXiv preprint arXiv:2002.05715}, 2020.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2498--2507. PMLR, 2017.

\bibitem[Moradi et~al.(2020)Moradi, Berangi, and Minaei]{moradi2020survey}
Reza Moradi, Reza Berangi, and Behrouz Minaei.
\newblock A survey of regularization strategies for deep models.
\newblock \emph{Artificial Intelligence Review}, 53\penalty0 (6):\penalty0
  3947--3986, 2020.

\bibitem[Neklyudov et~al.(2017)Neklyudov, Molchanov, Ashukha, and
  Vetrov]{neklyudov2017structured}
Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Structured bayesian pruning via log-normal multiplicative noise.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  6778--6787, 2017.

\bibitem[Nguyen et~al.(2020)Nguyen, Joty, Wu, and Aw]{nguyen2020data}
Xuan-Phi Nguyen, Shafiq Joty, Kui Wu, and Ai~Ti Aw.
\newblock Data diversification: A simple strategy for neural machine
  translation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10018--10029, 2020.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli.
\newblock Scaling neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pp.\  1--9, 2018.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{NAACL-HLT (Demonstrations)}, 2019.

\bibitem[Pham \& Le(2021)Pham and Le]{pham2021autodropout}
Hieu Pham and Quoc~V Le.
\newblock Autodropout: Learning dropout patterns to regularize deep networks.
\newblock \emph{arXiv preprint arXiv:2101.01761}, 2021.

\bibitem[Poole et~al.(2014)Poole, Sohl-Dickstein, and
  Ganguli]{poole2014analyzing}
Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli.
\newblock Analyzing noise in autoencoders and deep networks.
\newblock \emph{arXiv preprint arXiv:1406.1831}, 2014.

\bibitem[Post(2018)]{post2018call}
Matt Post.
\newblock A call for clarity in reporting bleu scores.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}. Association for Computational Linguistics, 2018.

\bibitem[Qi et~al.(2020)Qi, Yan, Gong, Liu, Duan, Chen, Zhang, and
  Zhou]{qi2020prophetnet}
Weizhen Qi, Yu~Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
  Zhang, and Ming Zhou.
\newblock Prophetnet: Predicting future n-gram for sequence-to-sequence
  pre-training.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: Findings}, pp.\  2401--2410, 2020.

\bibitem[Radford et~al.()Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Tim Salimans and Diederik~P Kingma.
\newblock Weight normalization: a simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pp.\  901--909, 2016.

\bibitem[Semeniuta et~al.(2016)Semeniuta, Severyn, and
  Barth]{semeniuta2016recurrent}
Stanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth.
\newblock Recurrent dropout without memory loss.
\newblock In \emph{Proceedings of COLING 2016, the 26th International
  Conference on Computational Linguistics: Technical Papers}, pp.\  1757--1766,
  2016.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{sennrich2016neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1715--1725,
  2016.

\bibitem[Shen et~al.(2020)Shen, Zheng, Shen, Qu, and Chen]{shen2020simple}
Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen.
\newblock A simple but tough-to-beat data augmentation approach for natural
  language understanding and generation.
\newblock \emph{arXiv preprint arXiv:2009.13818}, 2020.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2818--2826, 2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  6000--6010, 2017.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Le~Cun, and
  Fergus]{wan2013regularization}
Li~Wan, Matthew Zeiler, Sixin Zhang, Yann Le~Cun, and Rob Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In \emph{International conference on machine learning}, pp.\
  1058--1066. PMLR, 2013.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
  Analyzing and Interpreting Neural Networks for NLP}, pp.\  353--355, 2018.

\bibitem[Wang \& Manning(2013)Wang and Manning]{wang2013fast}
Sida Wang and Christopher Manning.
\newblock Fast dropout training.
\newblock In \emph{international conference on machine learning}, pp.\
  118--126. PMLR, 2013.

\bibitem[Wang et~al.(2020)Wang, Liu, and Ma]{wang2020scale}
Yue Wang, Yuting Liu, and Zhi-Ming Ma.
\newblock The scale-invariant space for attention layer in neural network.
\newblock \emph{Neurocomputing}, 392:\penalty0 1--10, 2020.

\bibitem[Wei et~al.(2020)Wei, Kakade, and Ma]{wei2020implicit}
Colin Wei, Sham Kakade, and Tengyu Ma.
\newblock The implicit and explicit regularization effects of dropout.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10181--10192. PMLR, 2020.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pp.\  2082--2090, 2016.

\bibitem[Wu \& Gu(2015)Wu and Gu]{wu2015towards}
Haibing Wu and Xiaodong Gu.
\newblock Towards dropout training for convolutional neural networks.
\newblock \emph{Neural Networks}, 71:\penalty0 1--10, 2015.

\bibitem[Wu et~al.(2021)Wu, Zhao, and Zhang]{wu2021not}
Hongqiu Wu, Hai Zhao, and Min Zhang.
\newblock Not all attention is all you need.
\newblock \emph{arXiv preprint arXiv:2104.04692}, 2021.

\bibitem[Wu et~al.(2019)Wu, Wang, Xia, Tian, Gao, Qin, Lai, and
  Liu]{wu2019depth}
Lijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao, Tao Qin, Jianhuang Lai,
  and Tie-Yan Liu.
\newblock Depth growing for neural machine translation.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  5558--5563, 2019.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  3--19, 2018.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 5753--5763, 2019.

\bibitem[Zehui et~al.(2019)Zehui, Liu, Huang, Chen, Qiu, and
  Huang]{zehui2019dropattention}
Lin Zehui, Pengfei Liu, Luyao Huang, Junkun Chen, Xipeng Qiu, and Xuanjing
  Huang.
\newblock Dropattention: A regularization method for fully-connected
  self-attention networks.
\newblock \emph{arXiv preprint arXiv:1907.11065}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Zhao, Saleh, and Liu]{zhang2020pegasus}
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu.
\newblock Pegasus: Pre-training with extracted gap-sentences for abstractive
  summarization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11328--11339. PMLR, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Song, Gao, Chen, Bao, and Ma]{zhang2019your}
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng
  Ma.
\newblock Be your own teacher: Improve the performance of convolutional neural
  networks via self distillation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  3713--3722, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Xiang, Hospedales, and Lu]{zhang2018deep}
Ying Zhang, Tao Xiang, Timothy~M Hospedales, and Huchuan Lu.
\newblock Deep mutual learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4320--4328, 2018.

\bibitem[Zhao et~al.(2019)Zhao, Sun, Xu, Zhang, and Luo]{zhao2019muse}
Guangxiang Zhao, Xu~Sun, Jingjing Xu, Zhiyuan Zhang, and Liangchen Luo.
\newblock Muse: Parallel multi-scale attention for sequence to sequence
  learning.
\newblock \emph{arXiv preprint arXiv:1911.09483}, 2019.

\bibitem[Zhou et~al.(2021)Zhou, Song, Chen, Zhou, Wang, Yuan, and
  Zhang]{zhou2021rethinking}
Helong Zhou, Liangchen Song, Jiajie Chen, Ye~Zhou, Guoli Wang, Junsong Yuan,
  and Qian Zhang.
\newblock Rethinking soft labels for knowledge distillation: A bias-variance
  tradeoff perspective.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Ge, Wei, Zhou, and Xu]{zhou2020scheduled}
Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, and Ke~Xu.
\newblock Scheduled drophead: A regularization method for transformer models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: Findings}, pp.\  1971--1980, 2020.

\bibitem[Zhu et~al.(2019)Zhu, Xia, Wu, He, Qin, Zhou, Li, and
  Liu]{zhu2019incorporating}
Jinhua Zhu, Yingce Xia, Lijun Wu, Di~He, Tao Qin, Wengang Zhou, Houqiang Li,
  and Tieyan Liu.
\newblock Incorporating bert into neural machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2019.





\bibitem[Srivastava et~al.(2014)Srivastava, Hinton,Krizhevsky,Sutskever and Salakhutdinov]{Srivastava2014DropoutAS}
Nitish Srivastava and Geoffrey E. Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{J. Mach. Learn. Res.}, 15:\penalty0 1929--1958, 2014.

\bibitem[Ba \& Frey(2013)Ba and Frey]{ba2013adaptive}
Lei~Jimmy Ba and Brendan Frey.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock In \emph{Proceedings of the 26th International Conference on Neural
  Information Processing Systems-Volume 2}, pp.\  3084--3092, 2014.

\bibitem[Ba et~al.(2016)]{Ba2016LayerN}
Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton.
\newblock Incorporating bert into neural machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wu et~al.(2019)Wu, Wang, Xia, Tian, Gao, Qin, Lai, and
  Liu]{wu2019depth}
Lijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao, Tao Qin, Jianhuang Lai,
  and Tie-Yan Liu.
\newblock Depth growing for neural machine translation.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  5558--5563, 2019.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  3--19, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  6000--6010, 2017.

\end{thebibliography}
