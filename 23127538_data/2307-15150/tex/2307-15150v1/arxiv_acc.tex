\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
% \usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow} % Required for multirows
\usepackage{enumerate}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{subcaption}
% \usepackage{subfigure}
\usepackage{diagbox}
\usepackage{wrapfig,lipsum}
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\usepackage{authblk}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}

\title{R-Block: Regularized block of Dropout for convolutional networks}

\author{%
	\textbf{Liqi Wang}\textsuperscript{\normalfont 1} \quad
	\textbf{Qiya Hu}\textsuperscript{\normalfont 1}\thanks{Corresponding author.}
    
    \textsuperscript{1}Academy of Mathematics and Systems Science, Chinese Academy of Sciences
}



%\footnote{ \textbf{Funding}: The work of author was supported by the National Natural Science Foundation of China grant G12071469.}
%\footnote{Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China.}


\begin{document}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\setcounter{footnote}{-1}
\footnote{ \textbf{Funding}: The work of the author was supported by the National Natural Science Foundation of China grant G12071469. 
}

\maketitle


\begin{abstract}
Dropout as a regularization technique is widely used in fully connected layers while is less effective in convolutional layers. Therefore more structured forms of dropout have been proposed to regularize convolutional networks. The disadvantage of these methods is that the randomness introduced causes inconsistency between training and inference. In this paper, we apply a mutual learning training strategy for convolutional layer regularization, namely R-Block, which forces two outputs of the generated difference maximizing sub models to be consistent with each other. Concretely, R-Block minimizes the losses between the output distributions of two sub models with different drop regions for each sample in the training dataset. We design two approaches to construct such sub models. Our experiments demonstrate that R-Block achieves better performance than other existing structured dropout variants. We also demonstrate that our approaches to construct sub models outperforms others.

\end{abstract}


\section{Introduction}
Convolutional Neural Networks (CNNs) are powerful tools in the field of computer vision. Regularization~\cite{Ba2016LayerN,Hinton2015DistillingTK,Ioffe2015BatchNA,Szegedy2015RethinkingTI,Wu2018GroupN,Zhang2019BeYO,Li2022ASO} is needed during CNN training to prevent overfitting and improve the generalization ability of the model. Among them, dropout ~\cite{Srivastava2014DropoutAS} is a well-known regularization technique, which can perform implicit ensemble and reduce interdependence of neurons (co-adaptation)~\cite{Li2022ASO} by randomly dropping units in hidden layers during training. Although dropout works well in fully connected layers, it is nearly ineffective in convolutional layers where features are spatially correlated~\cite{Ghiasi2018DropBlockAR}. To address this dropout problem, many structured dropout methods have gradually been proposed~\cite{Tompson2014EfficientOL,Park2016AnalysisOT,Wu2015TowardsDT,Khan2017RegularizationOD,Cai2019EffectiveAE,Hou2019WeightedCD,Zeng2021CorrelationbasedSD,Lu2021LocalDropAH,Pham2021AutoDropoutLD,Chen2020DropClusterAS}. A drawback of these methods is that the randomly sampled sub models during training are different from the full model during inference~\cite{Ma2016DropoutWE,Zolna2017FraternalD}.



In this paper, we adopt a simple and effective strategy to regularize above inconsistency in CNNs, named as R-Block. Specifically, in each mini-batch training, the same sample goes through two sub models with completely different random drop regions and obtains the similar prediction outputs. R-Block minimizes the bidirectional losses between the two output distributions, that is, it adds deep mutual learning~\cite{zhang2018deep} of the two sub models with difference maximization, which can reduce the inconsistency between training and inference phases of model~\cite{Liang2021RDropRD}. According to the structure of feature maps, we design two approaches to construct sub models with different drop regions. 



From the perspective of convolutional neural network regularization, our proposed R-Block can be regarded as a new structured form of dropout. In our experiments, R-Block is much better than other existing structured dropout variants. Our results show that our approachs to construct sub models perform better than the alternatives.


\noindent{\bf Related Work.} For CNNs with special structures, structured variants of dropout have been proposed such as
SpatialDropout~\cite{Tompson2014EfficientOL,Park2016AnalysisOT}, Max-pooling Dropout~\cite{Wu2015TowardsDT}, DropBlock~\cite{Ghiasi2018DropBlockAR}, Spectral Dropout~\cite{Khan2017RegularizationOD}, Drop-Conv2d~\cite{Cai2019EffectiveAE}, Weighted Channel Dropout~\cite{Hou2019WeightedCD}, DropCluster~\cite{Chen2020DropClusterAS}, CorrDrop~\cite{Zeng2021CorrelationbasedSD}, LocalDrop~\cite{Lu2021LocalDropAH}, AutoDropout~\cite{Pham2021AutoDropoutLD}, etc. Our method, R-Block, is closely related to SpatialDropout and DropBlock. SpatialDropout randomly masks out entire channels of a feature map and DropBlock randomly drops square regions of a feature map. Since selecting contiguous blocks exploits the spatial correlation in feature maps, these two methods both have good regularization effects in CNNs. We design BDropDML and SDropDML by splitting channels and regions respectively to construct sub models with different dropping units. R-Block enforces the sub models to be consistent with each other to alleviate the training inconsistency. It is inspired by R-Drop~\cite{Liang2021RDropRD} and generalizes the idea of "dropout twice"~\cite{Gao2021SimCSESC} from dropout to structured dropout variants. Our experiments show that R-Block is more effective than R-Drop in CNNs.



%\noindent{\bf Consistency Training.}
%Besides regularization methods, our work also relates to a few works of consistency training on dropout models or data augmentation.
%Among them, the most representative methods are ELD~\citep{ma2016dropout}, FD~\citep{zolna2017fraternal}, and Cutoff~\citep{shen2020simple}.
%As discussed in Section~\ref{sec:discuss}, ELD only focuses on the inconsistency between the sub model with dropout (train) and the expected full-model without dropout (inference), while FD works between the sub models only (consistence between two sub models).





\section{R-Block Regularization}
\label{sec:R-Block regularization}

This section introduces our proposed regularization method, R-Block, in detail.
Given the training dataset $\mathcal{D} = \{(x, y)\}$, where $x\in\mathcal{R}^{m\times n\times c} $ and $y\in \{1,2,3,...,\kappa\}$ correspond to the image and its label in the dataset for the image classification task, $m\times n\times c$ is the height $\times$ width $\times$ channel dimension of a feature tensor and $\kappa$ is the number of distinctive classes. The goal of the training is to learn a model $F_\Theta$, where $\Theta$ represents all parameters in a CNN.

Given the input data $(x,y)$ at each training step, we feed $x$ to the forward pass of two sub models $F_\Theta^1$ and $F_\Theta^2$ with different drop regions. Therefore, we can obtain two distributions $p_1$  and $p_2$ of the model predictions, expressed as:
\begin{equation}
	\label{eqn:predictions}
	p^k_i(x)=\dfrac{exp([F^i_\Theta(x)]_k/T)}{\sum_jexp([F^i_\Theta(x)]_j/T)},
\end{equation}
where $p_i^k$  represents the probability of the $k$th class for the $i(i=1,2)$ sub model $F_\Theta^i$, and $T>0$ is the temperature scaling parameter.

The conventional supervised loss between the predicted values of the sub model $F_\Theta^i$  and the correct labels $y$ is the standard cross-entropy loss $J_{ce}$:
\begin{equation}
	\label{eqn:cross-entropy loss}
	J_{ce}(F_\Theta^i(x),y)=-\ln\dfrac{exp([F^i_\Theta(x)]_y)}{\sum_j exp([F^i_\Theta(x)]_j)}.
\end{equation}
We use the Kullback Leibler (KL) Divergence to quantify the match of the output distributions $p_1$  and $p_2$ of two sub models. The KL distance $J_{KL}$  from $p_i$  and $p_j$ is computed as:
\begin{equation}
	\label{eqn:KL loss}
	J_{KL}(p_j,p_i)=\sum_k[p_j]_k\ln\dfrac{[p_j]_k}{[p_i]_k}.
\end{equation}
The overall loss functions $J_i$  for the sub model $F_\Theta^i$ is:
\begin{equation}
	\label{eqn:sub model loss}
	J_i(F_\Theta^i(x),y)=(1-\alpha)J_{ce}(F_\Theta^i(x),y)+\alpha T^2\sum_{j\neq i}J_{KL}(p_j,p_i),
\end{equation}
where $\alpha>0$ is the coefficient weight. Note that we multiply the square of the temperature $T^2$ by following the original KD~\cite{Hinton2015DistillingTK}.

The total training loss $J_{R-Block}$ is defined as follows:
\begin{equation}
	\label{eqn:total loss}
	J_{R-Block}=\dfrac{1}{|\mathcal{D}|}\sum_{(x,y)\in \mathcal{D}}\sum_iJ_i(F_\Theta^i(x),y),
\end{equation}
The overall framework of R-Block is shown in Figure~\ref{fig:framework}. 
% Figure environment removed

\section{Sub models}
\label{sec:sub models}
In this section, we design two approaches, BDropDML and SDropDML, to construct sub models with completely different drop regions. There are two main parameters: $b_{size}$ and $p$. $b_{size}$ is the size of the block to be dropped, and $p$ is the dropout probability for each activation unit.

We reference the mask sampling in DropBlock as shown in Figure~\ref{fig:block}: first, we compute the dropout probability $\gamma$ of block center by the dropout probability $p$ of each activation unit, and sample a mask $M$ with probability $\gamma$ on each feature map; second, every $1$ entry on $M$ is expanded to $1$ block of $b_{size}\times b_{size}$. In order to position and maintain symmetry, $b_{size}$ is usually set to an odd number. Here we denote $b_{size}=2k+1$.

% Figure environment removed

\subsection{BDropDML}
\label{sec:BDropDML}
We propose a strategy for constructing two sub models with complementary drop regions on feature channels, coined Block Dropout Deep Mutual Learning (BDropDML). Specifically, two sub models share the same DropBlock mask on each feature channel and then perform complementary mask division on channels with a probability of $0.5$. The overall framework of BDropDML is shown in Figure~\ref{fig:BDropDML}, where the yellow Blocks represent drop regions.

% Figure environment removed

Given one input $x\in\mathcal{R}^{m\times n\times c}$, the size $b_{size}$ of the block to be dropped and the dropout probability $p$ for each activation unit, the dropout probability $\gamma$ of block center can be computed as:
\begin{equation}
	\label{eqn:gamma1}
	\gamma = \frac{1}{b_{size}^2}p,
\end{equation}
or:
\begin{equation}
	\label{eqn:gamma2}
	\gamma = \frac{mn}{b_{size}^2(m-b_{size}+1)(n-b_{size}+1)}p.
\end{equation}
The details of BDropDML are given in Algorithm~\ref{al:BDropDML}.

\begin{algorithm}[htbp]%[!t]
	\setstretch{1.28}
	\caption{BDropDML}
	\label{al:BDropDML}
	\textbf{Input}: $x\in\mathcal{R}^{m\times n\times c}$,  $b_{size}$, $p$. \\
	\textbf{Output}: Maskes sampling $M^1$, $M^2$.
	\begin{algorithmic}[1]
		\STATE Calculate $\gamma$ by Eq.~(\ref{eqn:gamma1}) or Eq.~(\ref{eqn:gamma2}), randomly sample mask $r\in \mathcal{R}^{m\times n}$: \\
		$r_{i,j}\sim\textit{Bernoulli}(1-\gamma)$, $\forall i\in \{1,2,\ldots,m\}$, $j\in \{1,2,\ldots,n\}$.
		\STATE  For each $1$ position $r_{i,j}$, create a spatial square mask $M\in\mathcal{R}^{m\times n}$ with the center being $r_{i,j}$, the width, height being $b_{size}$ and set all the values of $r$ in the square to be $1$.
		\STATE Randomly sample mask $l^1\in\mathcal{R}^c$: $l_k^1\sim\textit{Bernoulli}(0.5)$, $\forall k\in \{1,2,\ldots,c\}$.\\
		Calculate $l^2\in\mathcal{R}^c$: $l^2=\textbf{1}_{l^1}-l^1$.
		\STATE Expand $M$ to $\hat{M}\in\mathcal{R}^{m\times n\times c}$: $\hat{M}_{i,j,k}=M_{i,j}$.\\
		Expand $l^1$, $l^2$ to $\hat{l}^1$, $\hat{l}^2\in\mathcal{R}^{m\times n\times c}$: $\hat{l}^1_{i,j,k}=l_k^1$,  $\hat{l}^2_{i,j,k}=l_k^2$,\\
		$\forall i\in \{1,2,\ldots,m\}$, $j\in \{1,2,\ldots,n\}$, $k\in \{1,2,\ldots,c\}$.
		\STATE Calculate: $\bar{M}^i=\textbf{1}_x-\hat{M}\odot\hat{l}^i (i=1,2)$.
		\STATE Normalize the features: $M^i=\bar{M}^i/s^i$, where $s^i$ is the proportion of 1 in $\bar{M}^i (i=1,2)$.
		\STATE Return $M^1$, $M^2$.
	\end{algorithmic}
\end{algorithm}

%Note: $\odot$is Hadamard product, $1_x$ represent all 1 tensor with the same type as $x$.


\subsection{SDropDML}
\label{sec:SDropDML}
We propose a strategy for constructing two sub models with complementary drop regions on the same channel, coined Spatial Dropout Deep Mutual Learning (SDropDML). Specifically, two sub models randomly drops the same channels of a feature map and then perform complementary DropBlock mask division with a probability of $0.5$ on each dropped channel. The overall framework of SDropDML is shown in Figure~\ref{fig:SDropDML}, where the yellow Blocks represent drop regions.

% Figure environment removed


When calculating the complementary DropBlock mask, we need to first estimate the dropout probability $\gamma$ of block center based on the dropout probability $p=0.5$. Given one input $x\in\mathcal{R}^{m\times n\times c}$ and the size $b_{size}=2k+1$, when $m,n>2b_{size}$, the relationship between $p$ and $\gamma$ is as follows(detailed proof can be found in Sec.~\ref{sec:relationship}):%given in the following theorem.%(detailed proof can be found in Appendix A).
\begin{align}
	\label{eqn:relationship}
	p 
	&= (1-\frac{2k}{m})(1-\frac{2k}{n})\left( 1-(1-\gamma)^{(2k+1)^2}\right) \\ \nonumber
	&+4\left[ \frac{k^2}{mn}-\frac{1}{mn}\sum_{1\leq i,j\leq k}(1-\gamma)^{(k+i)(k+j)}\right] \\ \nonumber
	&+2\left( \frac{1}{m}+\frac{1}{n}-\frac{4k}{mn}\right) \left[ k-(1-\gamma)^{(2k+1)(k+1)}\frac{1-(1-\gamma)^{(2k+1)k}}{1-(1-\gamma)^{(2k+1)}}\right].\nonumber
\end{align}	

The details of SDropDML are given in Algorithm~\ref{al:SDropDML}.


\begin{algorithm}[htbp]%[!t]
	\setstretch{1.28}
	\caption{SDropDML}
	\label{al:SDropDML}
	\textbf{Input}: $x\in\mathcal{R}^{m\times n\times c}$,  $b_{size}$, $p$. \\
	\textbf{Output}: Maskes sampling $M^1$, $M^2$.
	\begin{algorithmic}[1]	
		\STATE Randomly sample mask $l\in \mathcal{R}^c$: $l_k\sim\textit{Bernoulli}(1-p)$, $\forall k\in \{1,2,\ldots,c\}$.
		\STATE Estimate $\gamma$ by Eq.~(\ref{eqn:relationship}) with $p=0.5$, randomly sample mask $r\in \mathcal{R}^{m\times n}$: \\
			$r_{i,j}\sim\textit{Bernoulli}(1-\gamma)$, $\forall i\in \{1,2,\ldots,m\}$, $j\in\{1,2,\ldots,n\}$.
		\STATE  For each $1$ position $r_{i,j}$, create a spatial square mask $M^1\in\mathcal{R}^{m\times n}$ with the center being $r_{i,j}$, the width, height being $b_{size}$ and set all the values of $r$ in the square to be $1$.\\
			Calculate $M^2\in\mathcal{R}^{m\times n}$: $M^2=\textbf{1}_{M^1}-M^1$.
		\STATE Expand $M^1$, $M^2$ to $\hat{M}^1$, $\hat{M}^2\in\mathcal{R}^{m\times n\times c}$: $\hat{M}^1_{i,j,k}=M^1_{i,j}$, $\hat{M}^2_{i,j,k}=M^2_{i,j}$.\\
			Expand $l$ to $\hat{l}\in\mathcal{R}^{m\times n\times c}$: $\hat{l}_{i,j,k}=l_k$, \\
			$\forall i\in \{1,2,\ldots,m\}$, $j\in \{1,2,\ldots,n\}$, $k\in \{1,2,\ldots,c\}$.
		\STATE Calculate: $\bar{M}^i=\textbf{1}_x-\hat{l}\odot\hat{M}^i (i=1,2)$.
		\STATE Normalize the features: $M^i=\bar{M}^i/s^i$, where $s^i$ is the proportion of $1$ in $\bar{M}^i (i=1,2)$.
		\STATE Return $M^1$, $M^2$.
	\end{algorithmic}
\end{algorithm}

	%Note: $\odot$is Hadamard product, $1_x$ represent all 1 tensor with the same type as $x$.


\subsection{A the operational relationship between $p$ and $\gamma$}
\label{sec:relationship}
In this section, we provide the derivation of Eq.~\ref{eqn:gamma1} -~\ref{eqn:relationship} about the operational relationship between the dropout probability $p$ of each activation unit and the dropout probability $\gamma$ of block center.

\begin{theorem}%
	Given one input $x\in\mathcal{R}^{m\times n\times c}$ and the size $b_{size}=2k+1$ of the block to be dropped, when $m,n>2b_{size}$, the operational relationship between the dropout probability $p$ of each activation unit and the dropout probability $\gamma$ of block center can be described as:
	\begin{align}
		p \nonumber
		&= (1-\frac{2k}{m})(1-\frac{2k}{n})\left( 1-(1-\gamma)^{(2k+1)^2}\right) \\ \nonumber
		&+4\left[ \frac{k^2}{mn}-\frac{1}{mn}\sum_{1\leq i,j\leq k}(1-\gamma)^{(k+i)(k+j)}\right] \\ \nonumber
		&+2\left( \frac{1}{m}+\frac{1}{n}-\frac{4k}{mn}\right) \left[ k-(1-\gamma)^{(2k+1)(k+1)}\frac{1-(1-\gamma)^{(2k+1)k}}{1-(1-\gamma)^{(2k+1)}}\right]\nonumber
	\end{align}	
\end{theorem}

Proof: Since the expansion principle of mask sampling in each channel is the same, we only consider the relationship between $p$ and $\gamma$ on $x\in\mathcal{R}^{m\times n}$.

Randomly sample mask  $r\in\mathcal{R}^{m\times n}$: $r_{i,j}\sim\textit{Bernoulli}(1-\gamma)$. For each $1$ position $r_{i,j}$, create a spatial square mask $M\in\mathcal{R}^{m\times n}$ with the center being $r_{i,j}$, the width, height being $b_{size}$ and set all the values of $r$ in the square to be $1$. 

Here, the dropout probability for each activation unit in $M$ is $p$:
\begin{equation}
	p([M]_{i,j}=1\mid1\leq i\leq m,1\leq j\leq n)=p,
\end{equation}
We can calculate the expectation of the number of dropped activation units in $M$:
\begin{equation}
	\label{eqn:pmn}
	\mathbb{E}\left( \sum_{i,j}[M]_{i,j}\right) =\sum_{1\leq i\leq m,1\leq j\leq n}p([M]_{i,j}=1)=pmn,
\end{equation}
In order to express the dropout probability of $M$ with $\gamma$, we divide the mask $M$ into $3$ parts as shown in Figure~\ref{fig:partition}. 

In part \uppercase\expandafter{\romannumeral1}, the probability that the value of each activation unit equal to $1$ can be expressed as:
\begin{align}
	\label{eqn:p1}
	& p\left([M]_{i,j}=1\mid k+1\leq i\leq m-k,k+1\leq j\leq n-k\right)  \\  \nonumber
	= & p\left( \max_{-k+1\leq t,q\leq k+1}r_{i-1+t,j-1+q}=1\mid k+1\leq i\leq m-k,k+1\leq j\leq n-k\right)   \\ \nonumber
	= & 1-p\left( r_{i-1+t,j-1+q}=0\mid \forall -k+1\leq t,q\leq k+1\right)  \\ \nonumber
	= & 1-(1-\gamma)^{(2k+1)^2}. \nonumber
\end{align}
According to Eq.\ref{eqn:p1}, we have:
\begin{align}
	\label{eqn:e1}
	& \mathbb{E}\left(\sum_{i,j}[M]_{i,j}\mid [M]_{i,j} \in  \rm{\uppercase\expandafter{\romannumeral1}} \right)  \\  \nonumber
	= & \sum_{k+1\leq i\leq m-k,k+1\leq j\leq n-k}p\left([M]_{i,j}=1\right)   \\ \nonumber
	= & \sum_{k+1\leq i\leq m-k,k+1\leq j\leq n-k}\left(1-(1-\gamma)^{(2k+1)^2} \right)   \\ \nonumber
	= & (m-2k)(n-2k)\left( 1-(1-\gamma)^{(2k+1)^2}\right). \nonumber
\end{align}

% Figure environment removed

In part \uppercase\expandafter{\romannumeral2}, due to symmetry, only the region $1\leq i,j\leq k$ needs to be considered. Then we have:
\begin{align}
	\label{eqn:p2}
	& p\left([M]_{i,j}=1\mid 1\leq i,j\leq k\right)  \\  \nonumber
	= & p\left( \max_{-k+1\leq t,q\leq k+1}r_{i-1+t,j-1+q}=1\mid 1\leq i,j\leq k\right)   \\ \nonumber
	= & 1-p\left( r_{i-1+t,j-1+q}=0\mid \forall 2-i\leq t\leq k+1, 2-j\leq q\leq k+1\right)  \\ \nonumber
	= & 1-(1-\gamma)^{(k+i)(k+j)}. \nonumber
\end{align}
The expectation of the number of dropped activation units in part \uppercase\expandafter{\romannumeral2} can be calculated as:
\begin{align}
	\label{eqn:e2}
	& \mathbb{E}\left(\sum_{i,j}[M]_{i,j}\mid [M]_{i,j} \in  \rm{\uppercase\expandafter{\romannumeral2}} \right)  \\  \nonumber
	= & 4\sum_{1\leq i,j\leq k}p\left([M]_{i,j}=1\right)   \\ \nonumber
	= & 4\sum_{1\leq i,j\leq k}\left(1-(1-\gamma)^{(k+i)(k+j)} \right)   \\ \nonumber
	= & 4\left[k^2- \sum_{1\leq i,j\leq k}(1-\gamma)^{(k+i)(k+j)} \right]. \nonumber
\end{align}
In part \uppercase\expandafter{\romannumeral3}, due to symmetry, only the regions $k+1\leq i\leq m-k$, $1\leq j\leq k$ and $1\leq i\leq k$, $k+1\leq j\leq n-k$ needs to be considered. The probability that the value of each activation unit in the regions $k+1\leq i\leq m-k$, $1\leq j\leq k$ equal to $1$ can be expressed as:
\begin{align}
	\label{eqn:p31}
	& p\left([M]_{i,j}=1\mid k+1\leq i\leq m-k,1\leq j\leq k\right)  \\  \nonumber
	= & p\left( \max_{-k+1\leq t,q\leq k+1}r_{i-1+t,j-1+q}=1\mid k+1\leq i\leq m-k,1\leq j\leq k\right)   \\ \nonumber
	= & 1-p\left( r_{i-1+t,j-1+q}=0\mid \forall -k+1\leq t\leq k+1, 2-j\leq q\leq k+1\right)  \\ \nonumber
	= & 1-(1-\gamma)^{(2k+1)(k+j)}. \nonumber
\end{align}
According to Eq~\ref{eqn:p31}, we have:
\begin{align}
	\nonumber
	& \mathbb{E}\left(\sum_{i,j}[M]_{i,j}\mid k+1\leq i\leq m-k,1\leq j\leq k \right)  \\  \nonumber
	= & \sum_{k+1\leq i\leq m-k,1\leq j\leq k}p\left([M]_{i,j}=1\right)   \\ \nonumber
	= &\sum_{k+1\leq i\leq m-k,1\leq j\leq k}\left(1-(1-\gamma)^{(2k+1)(k+j)} \right)   \\ \nonumber
	= & (m-2k)\sum_{1\leq j\leq k}\left(1-(1-\gamma)^{(2k+1)(k+j)} \right)\\ \nonumber 
	= & (m-2k)\left[ k-(1-\gamma)^{(2k+1)(k+1)}\frac{1-(1-\gamma)^{(2k+1)k}}{1-(1-\gamma)^{(2k+1)}}\right]. \nonumber
\end{align}
In the same way, the probability that the value of each activation unit in the regions $1\leq i\leq k$, $k+1\leq j\leq n-k$ equal to $1$ can be expressed as:
\begin{align}
	\label{eqn:p32}
	& p\left([M]_{i,j}=1\mid 1\leq i\leq k,k+1\leq j\leq n-k\right)  \\  \nonumber
	= & p\left( \max_{-k+1\leq t,q\leq k+1}r_{i-1+t,j-1+q}=1\mid 1\leq i\leq k,k+1\leq j\leq n-k\right)   \\ \nonumber
	= & 1-p\left( r_{i-1+t,j-1+q}=0\mid \forall 2-i\leq t\leq k+1,-k+1\leq q\leq k+1\right)  \\ \nonumber
	= & 1-(1-\gamma)^{(k+i)(2k+1)}. \nonumber
\end{align}
The corresponding expectation can be calculated as:
\begin{align}
	\nonumber
	& \mathbb{E}\left(\sum_{i,j}[M]_{i,j}\mid 1\leq i\leq k,k+1\leq j\leq n-k \right)  \\  \nonumber
	= & \sum_{1\leq i\leq k,k+1\leq j\leq n-k}p\left([M]_{i,j}=1\right)   \\ \nonumber
	= &\sum_{1\leq i\leq k,k+1\leq j\leq n-k}\left(1-(1-\gamma)^{(k+i)(2k+1)} \right)   \\ \nonumber
	= & (n-2k)\left[ k-(1-\gamma)^{(2k+1)(k+1)}\frac{1-(1-\gamma)^{(2k+1)k}}{1-(1-\gamma)^{(2k+1)}}\right]. \nonumber
\end{align}
The expectation of the number of dropped activation units in part \uppercase\expandafter{\romannumeral3} can be calculated as:
\begin{align}
	\label{eqn:e3}
	& \mathbb{E}\left(\sum_{i,j}[M]_{i,j}\mid [M]_{i,j} \in  \rm{\uppercase\expandafter{\romannumeral3}} \right)  \\  \nonumber
	= & 2\mathbb{E}\left(\sum_{i,j}[M]_{i,j}\mid k+1\leq i\leq m-k,1\leq j\leq k \right)  \\  \nonumber
	+ &2\mathbb{E}\left(\sum_{i,j}[M]_{i,j}\mid 1\leq i\leq k,k+1\leq j\leq n-k \right)  \\  \nonumber
	= & 2(m+n-4k)\left[ k-(1-\gamma)^{(2k+1)(k+1)}\frac{1-(1-\gamma)^{(2k+1)k}}{1-(1-\gamma)^{(2k+1)}}\right]. \nonumber
\end{align}
Combined with Eq.~\ref{eqn:pmn}, Eq.~\ref{eqn:e1}, Eq.~\ref{eqn:e2} and Eq.~\ref{eqn:e3}, we have:
	\begin{align}
	pmn \nonumber
	&= (m-2k)(n-2k)\left( 1-(1-\gamma)^{(2k+1)^2}\right) \\ \nonumber
	&+4\left[k^2-\sum_{1\leq i,j\leq k}(1-\gamma)^{(k+i)(k+j)}\right] \\ \nonumber
	&+2\left(m+n-4k\right) \left[ k-(1-\gamma)^{(2k+1)(k+1)}\frac{1-(1-\gamma)^{(2k+1)k}}{1-(1-\gamma)^{(2k+1)}}\right].\nonumber
\end{align}	
Divided by $mn$, we can get the result. $\hfill\square$

Particularly, when we use a sample mask $r$ to generate a spatial square mask $M$ without considering marginal loss, we have:
\begin{equation}
	\label{eqn:pmax}
	p=1-(1-\gamma)^{b_{size}^2}.
\end{equation}
If we only sample mask $r$ in part \uppercase\expandafter{\romannumeral3} in Figure~\ref{fig:partition}, we have:
\begin{equation} 
	\nonumber
	pmn=(m-b_{size}+1)(n-b_{size}+1)\left( 1-(1-\gamma)^{b_{size}^2}\right),
\end{equation}
that is: 
\begin{equation}
	\label{eqn:pmin}
	p=\frac{(m-b_{size}+1)(n-b_{size}+1)\left( 1-(1-\gamma)^{b_{size}^2}\right)}{mn}.
\end{equation}
For the same $\gamma$, we denote the probabilities calculated by Eq.~\ref{eqn:pmax} and Eq.~\ref{eqn:pmin} as $p_1$ and $p_1$ respectively, and then $p$ obtained by Eq.~\ref{eqn:relationship} satisfies:
\begin{equation} 
	\label{eqn:order}
	p_2\leq p\leq p_1.
\end{equation}
Suppose the value of $\gamma$ is small, according to the McLaughlin expansion, we have:
\begin{equation} 
	1-(1-\gamma)^{b_{size}^2}\sim b_{size}^2\gamma.
\end{equation}
Combined with Eq.~\ref{eqn:pmax} and Eq.~\ref{eqn:pmin}, we can easily get Eq.~\ref{eqn:gamma1} and Eq.~\ref{eqn:gamma2}.

In addition, we need to estimate $\gamma$ by Eq.~\ref{eqn:relationship} with $p=0.5$ in Algorithm~\ref{al:SDropDML}. In fact, $\gamma$ increases monotonously with the increase of $p$ from $0$ to $1$. Here, we calculate the initial values of $\gamma$ through Eq.~\ref{eqn:gamma1} and Eq.~\ref{eqn:gamma2} with $p=0.5$ and then adjustment $\gamma$ according to Eq.~\ref{eqn:relationship} and Monotonicity Eq.~\ref{eqn:order}.

\section{Experiments}
\label{sec:experiments}
	
\subsection{Experimental setup}
\noindent{\bf Datasets.} We demonstrate the effectiveness of R-Block on various  image classification datasets: CIFAR-10, CIFAR-100~\cite{Krizhevsky2009LearningML} and TinyImageNet~\cite{Deng2009ImageNetAL}. The CIFAR-10 and CIFAR-100 datasets consist of $32 \times 32$ color images containing objects from $10$ and $100$ classes respectively. Both are split into a 50,000-image train set and a 10,000-image test set. The TinyImageNet dataset contains 120,000 $64 \times 64$ colour images of $200$ object classes. 


\noindent{\bf Network architecture.} We demonstrate our method on ResNet-18, ResNet-34~\cite{He2015DeepRL} and VGG16~\cite{Simonyan2014VeryDC}. We modify the first convolutional layer of ResNet-18 with kernel size $3 \times 3$, strides $1$ and padding $1$, instead of the kernel size $7 \times 7$, strides $2$ and padding $3$~\cite{He2016IdentityMI}.


\noindent{\bf Implementation Details.} We use stochastic gradient descents (SGD) with a momentum of $0.9$, an initial rate of $0.1$, weight decay of $5e-4$ and batch size of $128$. The learning rate is decayed by the factor of $0.1$ at $75$, $130$ and $180$ epochs for all datasets. The total epoch is set as $200$, the temperature $T$ is $3$ and the coefficient weight $\alpha$ is $0.1$. We set the dropout probability parameter $p$ as $0.2$ and the block size $b_{size}$ as $3$. Since baselines are usually overfitted for the longer training scheme and have lower validation accuracy at the end of training, we report the highest validation accuracy over the full training course for fair comparison.

\subsection{Classification accuracy}
We measure the top-1 ($\%$) accuracy of R-Block by comparing with Dropout, SpatialDropout and DropBlock on various datasets and model architectures. The results are presented in Table~\ref{tab:accuracy1} and Table~\ref{tab:accuracy2}. These results imply that R-Block induces better classification performance than others. For example, R-Block(BDropDML) improves the top-1 ($\%$) accuracy of Baseline from $71.28\%$ to $72.35\%$ under the CIFAR-100 dataset and ResNet-18. We also observe that the top-1 ($\%$) accuracy of Dropout with greater dropout probability tends to be worse than Baseline, which perhaps caused by insufficient training due to too many dropped activation unit.
 \begin{table}[!t]
%	\begin{wraptable}{r}{10cm}
%		\vspace{-0.2cm} %调整图片与上文的垂直距
		\centering
		\renewcommand\arraystretch{1.1} 
		\scalebox{0.92}{
			\begin{tabular}{l l l c c c}
				\toprule
			\textbf{Model} & \textbf{Method} & \textbf{\textit{p}} & \textbf{CIFAR-10} & \textbf{CIFAR-100}  \\
				\midrule
				\multirow{6}{*}{ResNet-18} 
				& Baseline & 0 & 92.98 & 71.28 \\
				& Dropout & 0.5 & 92.71 & 70.47 \\
				& SpatialDropout & 0.1 & 93.02 & 71.30 \\
				& DropBlock & 0.1 & 93.02 & 71.38 \\
				& \textbf{R-Block(BDropDML)} & 0.2 & \textbf{\textcolor{blue}{93.49}} & \textbf{72.35} \\
				& \textbf{R-Block(SDropDML)} & 0.2 & \textbf{93.56} & \textbf{\textcolor{blue}{72.08}} \\
				\midrule
				\multirow{6}{*}{VGG16} 
				& Baseline & 0 & 93.64 & 73.20 \\
				& Dropout & 0.5 & 93.56 & 72.93 \\
				& SpatialDropout & 0.1 & 93.75 & 73.08 \\
				& DropBlock & 0.1 & 93.71 & 73.27 \\
				& \textbf{R-Block(BDropDML)} & 0.2 & \textbf{94.24} & \textbf{\textcolor{blue}{73.80}}\\
				& \textbf{R-Block(SDropDML)} & 0.2 & \textbf{\textcolor{blue}{94.21}} & \textbf{73.98} \\
				\bottomrule
		\end{tabular}}
		\\[0.2cm]
		\caption{
			Top-1 ($\%$) accuracy on the CIFAR datasets and different model architectures. The best and second-best results are indicated in black bold and blue bold respectively.
		}
		\label{tab:accuracy1}
%		\vspace{-0.3cm}
%	\end{wraptable}
 \end{table}

\begin{table}[!t]
	%	\begin{wraptable}{r}{10cm}
%		\vspace{-0.2cm} %调整图片与上文的垂直距
		\centering
		\renewcommand\arraystretch{1.1} 
		\scalebox{0.92}{
		\begin{tabular}{l l c}
			\toprule
			\textbf{Method} & \textbf{\textit{p}} & \textbf{TinyImageNet}  \\
			\midrule
			 Baseline & 0 & 42.96 \\
			 SpatialDropout & 0-0.1 & 43.02 \\
			 DropBlock & 0-0.1 & 43.07 \\
			 \textbf{R-Block(BDropDML)} & 0-0.2 & \textbf{43.37} \\
			 \textbf{R-Block(SDropDML)} & 0-0.2 & \textbf{\textcolor{blue}{43.35}} \\
			\bottomrule
		\end{tabular}}
		\\[0.2cm]
		\caption{
			Top-1 ($\%$) accuracy on theTinyImageNet dataset and ResNet-34. The best and second-best results are indicated in black bold and blue bold respectively. In this experiments, we use a linear scheme of increasing the value of dropout probability. This linear scheme is similar to ScheduledDropPath~\cite{Zoph2017LearningTA}.
		}
		\label{tab:accuracy2}
%				\vspace{-0.3cm}
		%	\end{wraptable}
\end{table}


\subsection{Methods with different sub models}

% Figure environment removed

We provide six approaches to construct sub models as shown in Figure~\ref{fig:submodels} (a)-(f). The details of these approaches are listed below:

\begin{enumerate}[(a)]
	\item This approach is to use Dropout in fully connected layers twice randomly, corresponding to R-Drop~\cite{Liang2021RDropRD}.
	\item This approach is inspired by R-Drop and two sub models have complementary drop regions in fully connected layers. We denote it as C-Drop.
	\item This approach is to use SpatialDropout twice randomly and we denote it as R-SpatialDropout.
	\item This approach is to use DropBlock twice randomly and we denote it as R-DropBlock.
	\item This approach is BDropDML in Sec.~\ref{sec:BDropDML}, corresponding to ours method R-Block(BDropDML).
	\item This approach is SDropDML in Sec.~\ref{sec:SDropDML}, corresponding to ours method R-Block(SDropDML).
\end{enumerate}

 \begin{table}[!t]
	%	\begin{wraptable}{r}{10cm}
%		\vspace{-0.2cm} %调整图片与上文的垂直距
		\centering
		\renewcommand\arraystretch{1.1} 
		\scalebox{0.92}{
		\begin{tabular}{l l | c c c c c}
			\toprule
			\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{\textit{p}}} & \multicolumn{5}{c}{\textbf{Training stages}} \\
			&  & \textbf{20$\%$} & \textbf{40$\%$} & \textbf{60$\%$} & \textbf{80$\%$} & \textbf{100$\%$} \\
			\midrule
			 R-Drop & 0.5 & 54.93 & 68.88 & 68.99 & 70.81 & 71.17 \\
			 C-Drop & 0.5 & 54.60 & 69.50 & 69.70 & 71.03 & 71.30 \\
			 R-SpatialDropout & 0.1 & 55.80 & 69.83 & 69.98 & 71.41 & 71.86 \\
			 R-DropBlock & 0.1 & 56.20 & 69.81 & 70.30 & 71.15 & 71.60 \\
			 \textbf{R-Block(BDropDML)} & 0.2 & \textbf{\textcolor{blue}{56.83}} & \textbf{70.21} & \textbf{70.83} & \textbf{71.98}& \textbf{72.35}\\
			 \textbf{R-Block(SDropDML)} & 0.2 & \textbf{58.19} & \textbf{\textcolor{blue}{70.15}} & \textbf{\textcolor{blue}{70.31}} & \textbf{\textcolor{blue}{71.49}} & \textbf{\textcolor{blue}{72.08}} \\
			\bottomrule
		\end{tabular}}
		\\[0.2cm]
		\caption{
			Top-1 ($\%$) accuracy of various methods with different sub models at different training stages on CIFAR-100 and ResNet-18. Each stage adopts the optimal model of the current stage. The best and second-best results are indicated in black bold and blue bold respectively.
		}
		\label{tab:accuracy3}
%		\vspace{-0.3cm}
		%	\end{wraptable}
\end{table}

We report the performance of R-Drop, C-Drop, R-SpatialDropout, R-DropBlock, R-Block(BDropDML) and R-Block(SDropDML) corresponding to (a)-(f) respectively at different training stages in Table~\ref{tab:accuracy3}. The results show that our R-Block performs the best. These also show that methods using sub models with different drop regions outperform methods using sub models with random drop regions. Sub models with complementary drop regions enable each activation unit to update during training. At the same time, since the drop regions of two sub models in BDropDML and SDropDML are completely different, the difference in semantic information of the feature map is extracted to the greatest extent, which greatly improves the regularization efficiency of the sub models.

\section{Conclusion and Future Work}

In this paper, we propose R-Block to regularize training CNNs. R-Block is a form of structured dropout and can reduce the inconsistency of the model structure between training and inference phases by minimizing the mutual learning losses on the outputs of two sub models for each data sample in training. We introduce BDropDML and SDropDML to construct sub models with different drop regions. Our experiments on CIFAR and TinyImageNet demonstrate that R-Block performs better than other existing structured dropout variants and methods using sub models with different drop regions outperform methods using sub models with random drop regions. Our next step is to consider adding an attention mechanism on feature maps and dynamically segment the drop regions to construct sub models more reasonably.


\bibliographystyle{mybst}%mybst splncs04 IEEEtran
\bibliography{neurips_2021}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
%\appendix

%\section{Appendix}
%\label{sec:appendix}



\end{document} 