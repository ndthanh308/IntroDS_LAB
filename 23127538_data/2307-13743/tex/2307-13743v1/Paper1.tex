% LaTeX template for Models of Computation assessed coursework
% Most of the required packages are standard and should be provided by most TeX installations
% The exception is mathpartir, which is provided alongside this document

\documentclass{article}
\usepackage{jcappub}


\usepackage{jcappub}
\usepackage{lmodern}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{subcaption}
\hypersetup{colorlinks = true, urlcolor = blue, linkcolor = blue, frenchlinks = true, pdfborder = {0 0 0}}

%\usepackage{fullpage}
%\usepackage{rotating}
%\usepackage{amsmath, amssymb, amsthm}
%\usepackage{stmaryrd}
%\usepackage{proof}
%\usepackage{mathpartir}
%\usepackage{tikz}
%\usepackage{verbatim}
%\usepackage{url}
%\usepackage{multicol}
%\usepackage{xr-hyper}

%\usepackage[utf8]{inputenc}
%\usepackage{multicol}
%\usepackage{xcolor}
%\usepackage{graphicx}
%\usepackage{amsmath}
%\usepackage{amsfonts}

%\usepackage{caption}






\begin{document}
\title{Can the dark energy puzzle be explained by Causal set?}
\author{XXX}
\affiliation{Imperial College London,\\Exhibition Rd, South Kensington, London SW7 2BX}
\emailAdd{xxx@imperial.ac.uk}
\abstract{
Probably Not!


} 
\maketitle
\section{Introduction}

The standard model of cosmology or the $\Lambda$CDM model is the most widely accepted model of cosmology. It is not only effective at the background level but also robust at linear perturbations, giving accurate predictions for the matter power spectrum and the micro-K level fluctuations in the photon temperature observed in CMB. However, a major drawback of the standard $\Lambda$CDM model is that it requires dark matter and dark energy to explain the observational results, none of which are properly understood yet. Earlier, people assumed that dark matter is the particles emerging from supersymmetry theory~\citep{MARTIN_1998,Abazov_2008}. However, the lack of evidence of these particles from the Large Hadron Collider (LHC) strengthens the proposition of other candidates, such as Axions, ultra-light scalar field dark matter, etc~\citep{PhysRevLett.40.279,PhysRevD.53.2236}. A further mysterious puzzle is dark energy because that requires to produce a repulsive gravitation force. Cosmological constant or $\Lambda$-term provides an excellent solution for this. Quantum field theory gives a value of the $\Lambda$-term which is about $10^{120}$ times higher than the observed cosmological constant~\cite{carroll2001cosmological}.  

Apart from the cosmological constant  a plethora of alternative dark energy models have been explored in past. A series of these models are based on fluid model, %of for dark energy, without modifying GR. These models includes different $\omega$CDM models
where instead of taking dark energy equation of state (EoS) $w_\Lambda = 1$, other values have been explored, such as~\cite{CHEVALLIER_2001,PhysRevLett.90.091301,Jassal_2004,Barboza_2008,Feng_2012,Sendra_2012}. Other fluid dark energy models include Chaplygin-Like, Fluid~\cite{POURHASSAN_2013,2003GReGr..35.2063B,PhysRevD.70.083519}, different Viscous fluid models~\cite{Brevik_2005,Cruz_2017,Hern_ndez_Almada_2019},  interacting Viscous Models~\cite{Cruz_2017}, Phenomenological Emergent Dark Energy Models~\cite{Li_2019} etc. On the other hand there are models where authors try to modify General relativity and the theoretical gravity model. Such models include  Constant Brane Tension~\cite{PhysRevLett.83.3370, PhysRevLett.83.4690},  variable brane tension~\cite{Garc_a_Aspeitia_2018} which is a natural extension of the constant brane tension. There are others models based on $f(r)$ gravity~\cite{2015PhRvD..91h4060T,2014JCAP...09..036A}. Unimodular Gravity gives a remarkable proposition to tackle the dark energy problem by restricting the field equations at only nine linear independent equations~\cite{Garc_a_Aspeitia_2019}. Einstein-Gauss-Bonet (EGB) is a recent proposition that modifies the geometrical part of the field equations, maintaining the continuity equation in its original form~\cite{EGB}. However, several flaws of the model has also been found out by researchers~\cite{Ai_2020,G_rses_2020}. 

Causal set theory (CST) is a discrete theory of gravity that is proposed to solve the problem of quantum gravity. In 1991 the CST group proposed a model of cosmological constant where the cosmological constant term fluctuates over space and time~\cite{sorkin1991spacetime,sorkin1994role}. This arose from the basis tenets of CST. In 2004  \cite{Ahmed_2004} proposed a cosmological model named as Everpresent-$\Lambda$ based on causal sets. It relies on emphasizing on some of the key aspects in Causal Set Theory (CST) and Unimodular Gravity (UG). It has later been tested by others~\cite{Ahmed_2013,Zwane_2018}. In this paper we have discussed the Everpresent-$\Lambda$ model and then test the model against the observed data. Our analysis showed that no such model can explain or even resemble the observations.

The paper is organized as follows. In section 2 we briefly discuss about causal sets, Unimodular gravity and the Everpresent lambda model. In section 3 we discuss about the phenomenological implementation of the  Everpresent Lambda model. In section 4 we discuss about the supernova data and the next section is about the CMB data fitting. Final section is about the discussion and conclusion.  


%Causal set theory based model, Everpresent-$\Lambda$ is another prominent model of  dark energy. Infact Cosal set theory group actually predicted the values of Dark energy more then a decade before the discovery of supernova. However, this is a stochastic model of dark energy which itself has its own drawbacks. A detail discussion of all the dark energy models are presented in \cite{Di_Valentino_2021}

%However, as the observations get more precise, multiple inconsistencies come to light, such as discrepancies in Hubble parameters measured from CMB and supernova~\citep{Di_Valentino_2021,Joudaki_2016,Hildebrandt_2016,Riess_2019}, discrepancies in $\sigma 8$ of density perturbations at the scale of $8h^{-1}$Mpc inferred from cluster number counts and the value inferred from the CMB alone~\citep{Ade2016pap16}, discrepancies in the amount of lensing in the CMB angular power
%spectrum, in the curvature $\Omega_k$ and other specific parameters~\citep{Addison_2016,Kitching_2016,Couchot_2017}. All these make the dark energy puzzle even more cumbersome. 

%{\color{red} In the introduction give the history. Explain the issues with dark energy. How causal set can solve that problem etc. Cite the literature in this field. Try not to put any maths in the introduction.}

%The idea of Everpresent Lambda was first introduced in \cite{ahmed2004everpresent}\cite{sorkin1997forks}.
%Here we review some of the basic principles that are involved in developing the model. For a more comprehensive review, refer to [][][][][].\\

\section{Theoretical understanding of the model}
\subsection{About causal sets}

In the causal set theory (CST), we assume that space-time is discrete. So instead of the infinite number of points of a continuous manifold, we consider only a finite subset of 
points and induce the causal order of the initial manifold on our finite set. The resulting finite set along with the causal relation on its elements is called a causal set, and the claim of CST is that a causal set is a more fundamental description of spacetime than a continuous GR manifold.~\cite{bombelli1987space,meyer1988dimension}. Given a Lorentzian manifold, we can obtain a corresponding underlying causal set; This process is called sprinkling. We want it to satisfy two basic properties: 

1. On average, the number of sprinkled points
in a region of spacetime should be proportional to the 4-volume of the region. This is called the number-to-volume correspondence. The proportionality factor is denoted by $\rho_c=1/l_c^4$, where $l_c$ is called the discreteness length, and is supposedly close to the Planck length. 

2. The sprinkled points should preserve the Lorentz symmetry.\\
%It is not the metric that plays an essential role, but the causal relation between spacetime events.
A Poisson Sprinkling, i.e. sampling points from the manifold by a Poisson process, satisfies both of the above conditions, and has the smallest variance from number-to-volume correspondence, ~\cite{saravani2014causal,bombelli2009discreteness,dowker2020symmetry}. More specifically, if we take a region in spacetime with volume $V$, then the probability of having $n$ elements in this region is equal to 
\begin{equation}
    P(N)=\frac{1}{N!}e^{-\rho_c V}(\rho_c V)^N.
\end{equation}
Importantly, $\langle N\rangle=\rho_cV$ which is the statement of number-to-volume correspondence, and $\mathrm{std}(N)=\sqrt{\rho_cV}$.


On the other hand, given a causal set $\mathcal{C}$, a Lorentzian manifold is said to be a good continuous approximation of it if $\mathcal{C}$ is a typical outcome of the Poisson sprinkling on the manifold. The message to be taken from here is that the Lorentzian manifolds that approximate $\mathcal{C}$ have the total volume $V=\frac{1}{\rho_c}(N\pm\sqrt{N})$, so
\begin{equation}
\label{std}
    \delta V=\frac{\sqrt{N} }{\rho_c}
\end{equation}
The uncertainty rises from the fact that we don't have direct access to the detailed partial order structure of the underlying causal set. 
%, and hence will always have an error in our knowledge of the total number of events in any region of spacetime.

So far, we only discussed pure kinematic aspects of the correspondence between manifolds and causal sets. Of great importance is the question of evolution. Dynamically, a new causal set element is born in some causal relationship with the previously existing elements and in this way the causal set grows. This model is known as the classical sequential growth (CSG) of causal sets ~\cite{rideout1999classical,zalel2021discrete}.
Therefore, the total number of causal set elements can be used as an external time parameter. The clock ticks once every time a new element is born. For the purpose of the present paper, the main takeaway lesson is that a proper time variable for quantum gravity would be the total number of causal set elements, which corresponds to spacetime volume in the continuum (number-to-volume correspondence).\\

 %This causal set is mimicking the GR manifold. 
%Therefore, in CST, there doesn't exist any preferred frame or spacelike hypersurface that is evolving through time. Neither the whole causal set exist at once as a block universe that can be approximated by a GR manifold. 

%According to the fundamental conjecture of CST, this causal structure of a Poisson sprinkled causal set is enough to recover the differential structure of the continuous manifold down to the discreteness scale~\citep{Bombelli_2000,Bombelli2012,Luca_Bombelli_2004, Noldus_2004}.



%Each of these spacetime elements has some four volume, let's say $l_c^4$, where $l_c$ is the 

%The main principles are causality and discreteness, which are implemented at the kinematic level of the theory. 
%Unlike some other approaches to quantum gravity where time is either non-existent [?][?] or its passage requires a foliation of spacetime [?][?], in CST it is very much an objective notion that corresponds to the birth of spacetime events in a partial order.  


%We believe that CSG models present a novel way for thinking about the dynamics of a physical system, without relying on any action principle or equation of motion. 



%The second aspect of causal sets which we need for our discussion of Everpresent Lambda is the notion of a Poisson sprinkling. Assume that the reality of spacetime is more closely represented by a causal set rather than a Lorentzian manifold. Then one would expect that in a description of physics using the language of continuous manifolds, some stochastic contribution would start to pop out somewhere in the theory due to the random distribution of the physical causal set elements on the artificial manifold. The distribution of causal set elements is taken to be random rather than regular to maintain Lorentz invariance, and among all random distributions, Poisson has the smallest deviation from number-to-volume correspondence. More specifically, for us Poisson sprinkling means that if we take a regular region in spaccetime with volume $V$, then the probability of having $n$ elements in this region is equal to 
%\begin{equation}
%    P(n)=\frac{1}{n!}e^{-\rho V}(\rho V)^n
%\end{equation}
%where $\rho$ is a density, which supposedly must be close to the Planck density. The message to be taken from here is  that $std(P)=\sqrt{\rho V}$. Therefore, if we're at $T=N$, where $T$ is the external gauge time we talked about, then the Lorentzian manifolds that approximate the causal have, on average, total volume $\langle V\rangle=\frac{1}{\rho}N$, but then there is a standard deviation which is classically stochastic and is equal to 
%\begin{equation}
%    \label{std}
%    \mathrm{std}(\rho V)=\sqrt{N}
%\end{equation}
%This rises from the fact that we don't have direct access to the detailed partial order structure of the underlying causal set, and hence will always have an error in our knowledge of the total number of events in any region of spacetime, and of the most interest, our past light-cone. \\




Given a causal set $\mathcal{C}$, we can define the action $S_{BDG}[\mathcal{C}]$, %is introduced 
such that in the continuum limit, i.e. $\lim_{\rho_c\rightarrow\infty}S_{BDG}[\mathcal{C}]$, it tends to the Einstein-Hilbert action, $S_{EH}$.  This action was first proposed by \cite{benincasa2010scalar}, and is known as Benincasa-Dowker-Glaser action\citep{dowker2013causal,surya2019causal}. 


%The causal set action is known, \cite{benincasa2010scalar, dowker2013causal}. For a causal set $\mathcal{C}$, the action $S[\mathcal{C}]$ is introduced such that in the continuum limit, $\lim_{\rho_c\rightarrow\infty}$, it tends to the Einstein-Hilbert action. 
 

 
 %Now imagine we want to perform a path integral for CST.
 
We can use this action to write the path integral for causal sets by replacing the integral over geometries by the sum over all causal sets with a given number of elements, \citep{loomis2017suppression}
\begin{equation}
\label{carlip}
    \mathcal{Z}(N)=\sum_{|\mathcal{C}|=N}e^{iS_{BDG}[\mathcal{C}]}\,.
\end{equation}
%Setting a fixed $N$ as the total number of elements when summing over different causal sets is analogous to fixing two time slices when we perform path integral for a single particle. %
%So in fact, different partial orders are being summed over, while the number of elements is kept fixed. 
%{\color{red}In the IR limit, we expect to have a theory of continuum where in its path integral, the total volume of the spacetime is kept fixed. In such a theory of gravity, if we are to sum over all topologies and geometries, we have to be careful to keep the total volume constant. This is the global version of Unimodular Gravity.} - This is not correct.
This form of the path integral for causal sets will be our motivation to introduce Unimodular Gravity in the next subsection. 

\subsection{A brief overview of Unimodular Gravity}
Unimodular gravity (UG) was originally proposed as a mild modification of GR with the potential to solve the old cosmological constant problem, namely to address why the vacuum energy of field theories does not seem to gravitate \cite{buchmuller1989gauge}\cite{weinberg1989cosmological}\cite{sorkin1997forks}. Motivated by \ref{carlip}, to write a path integral for gravity in the continuum limit, we have to keep the total volume equal to some constant $V$. So in the continuum, the limit of \ref{carlip} would be
\begin{equation}
\label{Zglobal1}
    \mathcal{Z}_{UG}^{global}(V)=\int_{Vol(\mathcal{M})=V} \frac{\mathcal{D}g_{\mu\nu}}{Vol[\text{VDiff}]}\ e^{iS_{EH}[g]}
\end{equation}
 By VDiff we mean the group of diffeomorphisms that keep the total volume of spacetime unaltered. The superscript “global" is because only the total volume of the spacetime $\mathcal{M}$ is constrained. In contrast to the global version, there is the local version of the theory where the volume measure $d^4x\sqrt{-g}$ is constrained to be locally equal to some background 4-form $\omega$ and is not dynamical. This is the formulation of UG that is mostly discussed in the literature. It posits a seemingly stronger constraint on the metric, and reduces one degree of freedom in the metric at each spacetime point. At the classical level of local UG, it is well known (see \cite{weinberg1989cosmological,herrero2020non}) that the variational principle for the constrained metric only gives the traceless Einstein equations. We shall see that the same is true for global UG, and then we conclude that at the classical level, local and global UG are equivalent.% $\Lambda$ with $\lambda$ in the Einstein equations,  for the same argument as above in the case of local UG.
%In its global version, one would only sum over those geometries in the path integral that keep the total volume of spacetime equal to some constant $V$, in accord with our discussion on causal sets. In the analysis of its saddle point, we should be careful not to perform a variational principle with respect to $\delta g_{\mu\nu}$ because it is (globally) constrained:

In order to find the stationary point of the action in \ref{Zglobal1}, let's do a variation $g_{\mu\nu}\rightarrow g_{\mu\nu}+\delta g_{\mu\nu}$. The constraint in the path integral gives
\begin{equation}
\label{constraintG}
    \int d^4x \sqrt{-g}=V\ \ \Rightarrow\ \ \int d^4x\sqrt{-g}\ \delta g_{\mu\nu}g^{\mu\nu}=0
\end{equation}
Now the stationary-action principle requires $\delta S=0$, where we also allow $S$ to include matter fields and a cosmological constant. On the other hand:
\begin{equation}
    \delta S\propto \int d^4x\sqrt{-g}\ \delta g_{\mu\nu}E^{\mu\nu}
\end{equation}
where $E_{\mu\nu}=G_{\mu\nu}+\Lambda g_{\mu\nu}-8\pi GT_{\mu\nu}$. 
However, we can no longer infer the full Einstein equations $E_{\mu\nu}=0$ because $\delta g_{\mu\nu}$ should also satisfy \ref{constraintG}. So we should write the metric $g_{\mu\nu}$ in terms of an unconstrained metric $\hat{g}_{\mu\nu}$ as follows
\begin{equation}
\label{gghat}
    g_{\mu\nu}(x)=\sqrt{\frac{V}{\int d^4y \sqrt{-\hat{g}(y)}}}\ \hat{g}_{\mu\nu}(x)
\end{equation}
The condition \ref{constraintG} on $g_{\mu\nu}$ follows naturally from this redefinition for any arbitrary $\hat{g}_{\mu\nu}$. Performing a variation on \ref{gghat} gives
\begin{equation}
    \delta g_{\mu\nu}(x)=\frac{V}{\sqrt{\int d^4 y\sqrt{-\hat{g}(y)}}}\left(\delta\hat{g}_{\mu\nu}(x)-\frac{1}{4}\frac{\int d^4y\sqrt{-\hat{g}(y)}\ \delta\hat{g}_{\alpha\beta}(y)g^{\alpha\beta}(y)}{\int d^4 y\sqrt{-\hat{g}(y)}}g_{\mu\nu}(x)\right)
\end{equation}
%We replace this inside the variation of the Einstein-Hilbert (plus cosmological constant and some matter fields) action

Then for finding the classical equations of motion, we merely set the coefficient of $\delta\hat{g}_{\mu\nu}(x)$ in $\delta S$ equal to zero. A bit of algebra reveals 
\begin{equation}
\label{EEE}
    E_{\mu\nu}(x)-\frac{1}{4}g_{\mu\nu}(x)\langle E\rangle=0
\end{equation}
Here $\langle E\rangle$ is the spacetime average of the trace of the tensor $E_{\mu\nu}$:
\begin{equation}
    \langle E\rangle=\frac{\int d^4y\sqrt{-g}\ E(y)}{\int d^4y\sqrt{-g}}
\end{equation}
Taking the trace of \ref{EEE} reveals that $E=\langle E\rangle$; i.e. $E$ must be a constant:
\begin{equation}
\label{cons}
    -R-8\pi G T+4\Lambda= const.
\end{equation}
In particular, \ref{EEE} can be rewritten as
\begin{equation}
\label{EE}
    E_{\mu\nu}(x)-\frac{1}{4}g_{\mu\nu}(x)E=0\ \ \rightarrow \ \ G_{\mu\nu}-8\pi G T_{\mu\nu}+\frac{1}{4}g_{\mu\nu}(R+8\pi GT)=0
\end{equation}
which is the trace-free form of the Einstein equations. The full Einstein equations can be recovered by taking the covariant derivative of \ref{EE}, as it's done in \cite{weinberg1989cosmological} for local UG. Bianchi identity and the conservation of energy-momentum tensor force $R+8\pi GT$ to be a constant as well, which we call $4\lambda$. Then 
\begin{equation}
    G_{\mu\nu}+\lambda g_{\mu\nu}=8\pi GT_{\mu\nu}
\end{equation}
This $\lambda$ is just an integration constant that should be set as initial conditions for the universe, and is totally unrelated to any coupling constant in the action or the vacuum energy (Note that we initially separate any vacuum energy from $T_{\mu\nu}$ and add its contribution to $\Lambda$, so that $T$ doesn't include vacuum energy.) This is why UG solves the old cosmological constant problem.

As it's evident from the above discussion, UG doesn't provide any insight on the new cosmological constant problem or on the coincidence problem. In the context of UG, why the observed dark energy density is of the order of the present critical density remains an open problem. The Everpresent $\Lambda$ model adds insights from CST to UG in order to fill this gap and provide an explanation for the observed value of dark energy density.\\

%There is also the \textit{local} version of UG, that is mostly discussed in the literature. It posits a seemingly stronger constraint on the metric, by imposing that the local volume element of the spacetime should be held fixed in a path integral approach, not only the total volume.
%\begin{equation}
%    \sqrt{g}\ d^4x=\omega
%\end{equation}
%$\omega$ is taken to be a background 4-form. However, at least at the classical level, the local and the global versions give identical perspective on replacing $\Lambda$ with $\lambda$ in the Einstein equations, see \cite{herrero2020non} for the same argument as above in the case of local UG.\\

%There is an ongoing debate on the quantum version of UG and the differences in the quantization procedure between GR and UG, see \cite{herrero2020non}\cite{smolin2009quantization}\cite{bufalo2015unimodular}\cite{de2021can}\cite{kugo2021brst}\cite{kugo2022covariant}. In \cite{smolin2009quantization}, it is argued that the Hamiltonian path integral of the Henneaux-Teitelboim formulation of UG (\cite{henneaux1989cosmological}) preserves the unimodular condition at the quantum effective action level. This is essentially a consequence of $\delta(\sqrt{-g}-\omega)$ appearing in the path integral. In \cite{bufalo2015unimodular}, it is found that the same Hamiltonian analysis on the Lagrange multiplier formulation of UG ([?]) introduces $\delta(\int_{\Sigma_t}(\sqrt{-g}-\omega))$ instead of $\delta(\sqrt{-g}-\omega)$ in the path integral. This only enforces an \textit{averaged} unimodular condition at each spatial hypersurface, but its relation with the global UG is unclear. In \cite{de2021can} it is argued on rather formal grounds that the path integral of the global and local versions of UG are equivalent (in the paper, the authors use  the term “GR with fixed volume" for what we call the global UG.) However, it is not obvious how they implemented the global unimodular condition, and it seems that their argument is not altered if this condition is removed. Then their claim would be that the path integral of UG is equivalent to GR in unimodular gauge. Some subtleties in such a comparison is pointed out in \cite{kugo2021brst}\cite{kugo2022covariant}, where it's shown that although the factor $$\delta(\sqrt{-g}-\omega)=\int d\lambda\ e^{i\int d^4x\ \lambda(\sqrt{-g}-\omega)}$$ might look the same in both cases, the BRST structure is quite different. In GR in unimodular gauge, the field $\lambda$ falls into a BRST quartet and is the BRST daughter of a ghost, and must have vanishing VEV. Therefore, it cannot be used to cancel the cosmological constant. Even if we shift it as an integration variable inside the path integral and get rid of $\Lambda$, the new field $\lambda$ will have a VEV equal to $\Lambda$ that will pop up inside the Einstein equation and can carry the vacuum energy. On the other hand, in local UG, $\lambda$ is a BRST singlet and there is no restriction on its VEV. We can shift it as an integration variable inside the path integral and get rid of $\Lambda$. 

%It is not known to us whether the local and global versions of UG are equivalent quantum mechanically.

\section{The Everpresent $\Lambda$ model}\label{EvLSec}
The original argument for the Everpresent $\Lambda$ goes as follows. The path integral of global UG, equation \ref{Zglobal1}, can be rewritten by replacing the constraint by a delta function
\begin{equation}
\label{Zglobal}
    \mathcal{Z}_{UG}^{global}(V)=\int \frac{\mathcal{D}g_{\mu\nu}}{Vol[\text{VDiff}]}\ \delta(V-\int d^4x\sqrt{-g})\ e^{iS[g]}
\end{equation}
where $S$ is some action for gravity without the cosmological constant.% By VDiff we mean the group of diffeomorphisms that keep the total volume of spacetime unaltered.% For infinitesimal gauge transformations $\delta g_{\mu\nu}=\nabla_{\mu}\xi_\nu+\nabla_\nu\xi_\mu$ this is the condition that
%\begin{equation}
%    \int d^4x\sqrt{-g}\ \nabla^\mu\xi_\mu=0
%\end{equation}

The above partition function depends explicitly on $V$, the total volume of spacetime. Now instead of working in the “volume space", we can perform a Fourier transform and go to the conjugate space.
\begin{equation}
    \mathcal{Z}_{UG}^{global}(\Lambda)=\int dV\ e^{-i\Lambda V}\mathcal{Z}_{UG}^{global}(V)
\end{equation}
There is no need to worry about the physical meaning of the negative values for $V$ inside the integral because \ref{Zglobal} has a delta function at a positive value. Performing this delta function integration, and redefining $\Lambda$ with a factor $8\pi G$, we find
\begin{equation}
\label{fouri}
    \mathcal{Z}_{UG}^{global}(\Lambda)=\int \frac{\mathcal{D}g_{\mu\nu}}{Vol[\text{VDiff}]}\ \exp\left(iS[g]-\frac{2i\Lambda}{16\pi G}\int d^4x\sqrt{-g}\right)
\end{equation}
This is just the usual path integral with a cosmological constant $\Lambda$. We conclude from this calculation that $V$ and $\Lambda$ are quantum mechanically conjugate. This is not a rigorous derivation, as is required from a full theory of quantum gravity. However, we see it as a well motivated conjecture to expect from a quantum unimodular gravity. The Heisenberg uncertainty principle for these conjugate variables is
\begin{equation}
    \frac{\delta\Lambda}{8\pi G}\ .\ \delta V\geq\frac{\hbar}{2}
\end{equation}
Naturally one would ask what $\delta V$ means.
Having the causal set picture in mind, the total causal set elements are fixed at each instance, so $V$ has some standard deviation around $N$, with $N$ being the total number of causal set elements. But then if we insist on a continuum picture, heuristically we have to describe the universe as being in a superposition of different volumes. The distribution of $V$ should be a Poisson with mean $N$. For $1\ll N$ (which is indeed the case for all cases of interest), the distribution is very well approximated by a normal distribution $\mathcal{N}(N,\sqrt{N})$. We reinterpret this distribution, which comes from a classically stochastic process, as a quantum wave function distribution. The deeper nature of this correspondence is yet to be understood. Nevertheless, it hints toward a possibility to be explored.  Therefore, if our universe is in a Gaussian state in $V$-space, it should also be described with a Gaussian wave function in $\Lambda$-space. Gaussian wave functions saturate the Heisenberg uncertainty principle. Therefore we expect the cosmological state of the universe to be in a superposition of states that each have a definite $\Lambda$, with a spread equal to $\delta\Lambda$ so that
\begin{equation}
    \frac{\delta\Lambda}{8\pi G}\ .\ \delta V=\frac{\hbar}{2}
\end{equation}
And finally using \ref{std}, we find
\begin{equation}
\label{Heis}
    \delta\Lambda=\left(\frac{l_p}{l_c}\right) ^2 \frac{1}{\sqrt{V}}
\end{equation}
We use $l_p=\sqrt{4\pi G\hbar}$ as the rationalized Planck length and $l_c=\rho^{-1/4}$ for the discreteness scale that appears in CST. Some remarks are in order.\\

\begin{itemize}
\item
    Based on dimensional analysis, the volume of the universe should be of order $\sim H^{-4}$. In our background solutions for the Friedmann equations, we'll see that this is indeed the case. So roughly
    \begin{equation}
        \delta \Lambda\sim H^2
    \end{equation}
    Now if we accept that UG solves the problem of vacuum energy, the most \textit{natural} value for $\langle\Lambda\rangle$ is zero, see \cite{hawking1984cosmological}. Therefore, $\Lambda=\langle\Lambda\rangle\pm \delta \Lambda\sim H^2$, and this is indeed a correct estimate for the current value of $\Lambda$. More importantly, this is telling us that in our picture, $\Lambda$ is not temporally constant. It fluctuates such that at any time in the expansion history of the universe, its standard deviation is comparable to the critical energy density at that time, hence the name Everpresent Lambda.
\item 
Perhaps the closest analogue to the $\delta\Lambda\delta V$ uncertainty relation is the energy-time uncertainty in quantum mechanics. It has been suggested (\cite{sorkin1994role,gielen2022quantum}) that in quantum gravity, $V$ could serve as a \textit{time} variable to treat the seemingly timeless nature of GR and Wheeler-DeWitt equation. In this case, $\Lambda$ would be the analogue of energy.

\item
One issue is that we started with a global unimodular condition, setting $\Lambda$ to be a global variable. However, we 



--Mention whether the same can be done in any gauge theory for any Nakanishi Lautrup field-- lambda conservation difficulty

\end{itemize}
\subsection{A Phenomenological Implementation- Model 1}

The $\delta\Lambda\delta V$ uncertainty relation does not provide a practical model for how to evolve $\Lambda$. This is again in the same spirit that $\delta E\delta t$ uncertainty in quantum mechanics alone does not provide any dynamical model for short time energy non-conservation. In \cite{ahmed2004everpresent} the authors suggested a phenomenological model for the dynamics of $\Lambda$ that shows the expected fluctuations. The idea goes as follows. Instead of having to deal with a quantum superposition of states for the universe, each with a definite $\Lambda$, we want to build a classically stochastic model for the fluctuations in $\Lambda$. So we have to work within a \textit{single} universe, and promote $\Lambda$ as a constant to a scalar function $\Lambda(x)$, or else \ref{Heis} will cease to have any meaning because $V$ in any sense depends on $x$. From a causal set point of view, and due to the number-to-volume correspondence, \ref{Heis} is equivalent to
\begin{equation}
\label{Heis2}
    \delta\Lambda=\frac{\alpha}{l_c^2} \frac{1}{\sqrt{N}}
\end{equation}
where we defined
\begin{equation}
    \alpha=\left(\frac{l_p}{l_c}\right) ^2
\end{equation}
Now we extend this to be true even for one single element in the causal set, in the sense that each causal set element would contribute a random variable as its $\Lambda$. The standard deviation of this random variable should be equal to $\frac{\alpha}{l_c^2}$. The simplest scenario is to have a Bernoulli random variable:
\begin{equation}
    \Lambda|_{\text{1 element}}=\Lambda|_x=\begin{cases}
    \frac{\alpha}{l_c^2}\ \ \ p=\frac{1}{2}\\
    \frac{-\alpha}{l_c^2}\ \ p=\frac{1}{2}
    \end{cases}
\end{equation}
A normal distribution works as well for our purposes. In fact, any distribution works, as long as its mean is 0 and its standard deviation is $\frac{\alpha}{l_c^2}$.
Now the cosmological constant part of the action should be obtained by summing over all the above stochastic variables in a given causal set:
\begin{equation}
\label{Slam1}
    S_\Lambda=\frac{l_c^4}{2l_p^2}\sum_{x\in C}\Lambda|_x
\end{equation}
We have replaced the integral $\int d^4x\sqrt{-g}$ with the discrete sun $l_c^4\sum_{x\in C}$. Now whatever the distribution of $\Lambda|_x$ is, $S_\Lambda$ will follow a normal distribution $\mathcal{N}(0,\frac{\sqrt{N}}{2})$ according to the central limit theorem.
We interpret $\Lambda(x)$ (which is quite different from $\Lambda|_x$) that appears in \ref{fouri} and \ref{Heis2} as the effective global cosmological constant that is ascribed to the past light cone of the point $x$ such that
\begin{equation}
\label{Slam2}
    S_\Lambda|_{\text{past of $x$}}=\frac{1}{8\pi G}\Lambda (x) V(x)= \frac{l_c^2}{2\alpha}\Lambda(x)N(x)
\end{equation}
$V(x)$ is the volume of the past light cone at $x$, and $N(x)$ is the corresponding number. Therefore, the distribution of $\Lambda(x)$ turns out to be $$\mathcal{N}(0,\frac{\alpha}{l_c^2}\frac{1}{\sqrt{N}})$$ as desired. It seems that we have re-derived what we already know, but in fact we have provided a stochastic dynamics that satisfies \ref{Heis}. Now we can easily give a recursive formula for $\Lambda(x_n)$. Imagine we have found $S_\Lambda|_{\text{past of $x_{n-1}$}}$ and $\Lambda(x_{n-1})$ for some point $x_{n-1}$ in the past of $x_n$, close enough to $x_n$. The additional contribution to $S_\Lambda|_{\text{past of $x_n$}}$ compared to $S_\Lambda|_{\text{past of $x_{n-1}$}}$ is from all random variables $\Lambda|_z$ such that $z$ is in the past of $x_n$ but not in the past of $x_{n-1}$. The number of such points is proportional to the volume they occupy $\delta V_n= V(x_n)-V(x_{n-1})$, and the sum of their contributions to $S_\Lambda$ is, again thanks to the central limit theorem, a random variable $\sim\frac{\sqrt{\delta V}}{2 l_c^2}\mathcal{N}(0,1)$. So if we generate a standard normal random variable $\xi_n$, we can write
\begin{equation}
    \Lambda(x_n)=\frac{2l_p^2 S_\Lambda|_{\text{past of $x_{n-1}$}}+\alpha\sqrt{\delta V_n}\xi_n}{V(x_n)}=\frac{\Lambda(x_{n-1})V(x_{n-1})+\alpha\sqrt{\delta V_n}\xi_n}{V(x_n)}
\end{equation}
Summing up this recursive relation back to some initial point $x_0$ in the far past of $x_n$, we find
\begin{equation}
\label{stoch}
    \Lambda(x_n)=\ \frac{S_0+\alpha\sum_{i=1}^{n}\sqrt{\delta V_n}\xi_n}{V(x_n)}
\end{equation}
where $S_0=\Lambda(x_{0})V(x_{0})$.\\
Our notation so far has been slightly different from \cite{ahmed2004everpresent} and \cite{Zwane_2018}. In their notation, it is not entirely clear why $\alpha$ should show up and whether its subsequent numerical values are sensible. They only mention that $\alpha$ is needed to control the magnitude of fluctuations in $\Lambda$. We have explicitly kept track of $l_p$ and $l_c$, so that now we see $\alpha$ as merely the square of the ratio of the Planck length to the discreteness length. The smaller the discreteness scale, the larger the fluctuations in $\Lambda$. This is a curious property, since naively one would expect that in a perfect continuum, $\l_c\rightarrow 0$, the fluctuations would die off. However, in fact it is the opposite. Plus this shows that since it is quite reasonable that the actual discreteness scale would be one order of magnitude different from the Planck length, any value for $\alpha$ from $\sim 0.01$ to $\sim 100$ can be regarded as \textit{natural}. Indeed it turns out $\alpha$ should be $\sim 0.01$.

% To motivate it, let's repeat the same argument as the previous section, but this time with the local version of UG.

%\begin{equation}
%\label{Zlocal}
%    \mathcal{Z}_{UG}^{local}(V)=\int \frac{\mathcal{D}g_{\mu\nu}}{Vol[\text{TDiff}]}\ \delta(\omega-\sqrt{-g})\ e^{iS[g]}
%\end{equation}
%TDiff is the gauge group of local UG, which consists of gauge transformations that satisfy
%Doing the (functional) Fourier transform gives
%\begin{equation}
%    \mathcal{Z}_{UG}^{local}(\lambda)=\int \frac{\mathcal{D}g_{\mu\nu}}{Vol[\text{TDiff}]}\ \exp\left(iS[g]-\frac{i}{16\pi G}\int d^4x\ 2\sqrt{-g}\lambda\right)
%\end{equation}
 

\section{A brief overview of dark energy}


Observational samples are required not only to investigate the consistency of any theoretical model prediction but also to discern between different models. There are different observational data available and many new experiments are coming up in near future. The most important dataset that are used for testing the the dark energy and expansion history of the universe can be listed as follows. All type 1a Supernova, originated by white dwarf accreting matter from a companion star. When the white dwarf reached Chandrashakher limit (~1.4 mass of sun) it collapse and explode in type 1a supernova. All the type 1a supernova have almost same luminosity and hence it can be used as standard candle. The latest analysis on different model can be found in~\cite{Riess_2019,Aspeitia2007,Scolnic_2018}. Another observation that can be used for fitting parameters is the baryon acoustic oscillation (BAO) observation. Most recent compilation of BAO data can be found in~\cite{Nunes_2020, PhysRevD.93.023530,Jailson2016,de_Carvalho_2018}. Cosmic microwave background is one of the most important probe to model the dark energy due to the very accurate measurements of the CMB data~\cite{Hinshaw_2013,Ade2016}. However, dark energy mainly changes the the hubble parameter and most of the the other parameters on CMB remain almost unchanged due to different dark energy model which is proved by multiple authors~\cite{Wang_2006,Wang_2012}.The Hubble parameter is estimated directly by using the differential age methodology, i.e. measuring the age between pairs of passive evolving galaxies with similar metallicity and separated by a small redshift interval~\cite{Jimenez_2002}. Strong gravitational lens systems (SLS) offer a unique opportunity to study the $\Omega_m - w$ plane because their
confidence regions are almost orthogonal to those of standard rulers like BAO and CMB. Researchers used strong gravitational lensing to constrain cosmological parameters with different method~\cite{Chae_2002,Biesiada_2010,Cao_2012,Cao_2015}.  Authors also argued that the correlation between the measured luminosity $L$ and the
inferred velocity dispersion $\sigma$ of the ionized gas in extreme starburst galaxies (i.e. containing a population of O and/or B stars) may be used as a cosmological tracer to constrain cosmological model parameters~\cite{Ch_vez_2012}. Apart from these other measurements of hubble parameter comes from observation of tip of the red-giant branch (TRGB)~\cite{M_ller_2018,1999IAUS..183...48S}, Cepheid Amplitudes\cite{Riess_2020,Sabour2014} etc. Intensity mapping is a net field in astonomy which promises to provide 3D map of the universe at different redshifts. 


\section{{\color{red} Discuss about model 1}}

The equations governing the background evolution of the FRW metric are the first Friedmann equation
\begin{equation}
\label{FR}
    H^2=\frac{8\pi G}{3}\rho\ + \frac{\Lambda}{3},
\end{equation}
the continuity equation for the total energy density $\rho$, and the stochastic equation for the dynamics of $\Lambda$, equation \ref{stoch}. We also need to know how to compute the volume of the past light cone at some time $t$. The metric in conformal time is 
\begin{equation}
    ds^2=a(\eta)^2(-d\eta^2+dr^2+r^2d\Omega^2)
\end{equation}
so the volume element is $dV=a^4r^2\sin\theta\ d\eta dr d\theta d\phi$. This should be integrated on the past light cone of a given point $x$ at conformal time $\eta$. The 3d integration at a fixed time slice $\eta'$ gives $d\eta\ \frac{4\pi}{3}a^4r(\eta')^3$, where $r(\eta')=\eta'$ is the comoving distance of $x$ from that hypersurface. Therefore
\begin{equation}
    V(x)=\frac{4\pi}{3}\int_{\eta_i}^{\eta}d\eta'\ a(\eta')^4\eta'^3=\frac{4\pi}{3}\int_{t_i}^{t}dt'\ a(t')^3\left(\int_{t'}^{t} dt''\ \frac{1}{a(t'')}\right)^3
\end{equation}\\
(There is a mistake in the limits of the integral in \cite{Zwane_2018}.)\\
Having all the necessary equations at hand, we can perform simulations of the background cosmology. The statements we can make are no longer deterministic. We should instead look at the final distribution of various physical quantities after performing many realizations of the stochastic process. The free parameters for the background are $(\Omega_m^0h^2,\alpha)$. %We can not use $\Omega_m^0h^2$ as a parameter because the present fractional density of the matter essentially depends on each particular realization of the random process. The total matter density, on the other hand, can be consistently used as a parameter since we know its evolution.

{\color{red} 
Plots:
\begin{itemize}
    \item Discussion about the time steps and how is it affecting the crashing. 
    \item Plot showing that rholam tracks matter. rholam (or log of abs) versus a, and comparison with matter. (Santanu)
    \item Discussion about crashing. (Santanu)
    \item Typical Model 1 plot. (Arad)
    \item There is a relation between $\Omega_m$ and $\alpha$.  (Arad)
    \item Add auto-correlation time (Arad)
    \item omegamh2 should always be close to .14
    \item Discussion about the initial volume change. 
    \item LCDM values in all the plots.
    \item 3 sets with different alphas (0.005, 0.01, 0.013). Plot LCDM values in RED. Make plot sizes equal. X-axis same, number of beans same. Remove count from y axis.  
    \item Fluid model for such expansion history is very unstable. $w$ is going to infinity.  (Arad)
    \item Discuss why both the Friedmann equation not giving correct results. Why shall we use only one of the equations and why to ignore the other equation?   (Santanu)
    \item How did we work with Peeble-Yu equation.
    \item exponential correlation and model 3 (Arad)
    \item figure 1c wrong caption
    \item if probability of crashing is $p$, then having passed through $N$ hubble times, the probability of crashing is $p^N$. So starting earlier causes more crashings.
\end{itemize}
}

% Figure environment removed

%% Figure environment removed



{\color{red} This paragraph is moved from the next section. Need to rewrite properly. }
As it is apparent, although $(\Omega_m^0h^2,\alpha)$ are independent parameters of the model, $(\Omega_m,\alpha)$ are perfectly correlated. This is merely an artifact of having fixed the seed number for every chain. The reason is that if you keep the seed and $\alpha$ constant, and then multiply $\rho_m^0$ by some constant $c_0$, then all $\Lambda$, $H^2$, and $\frac{1}{\sqrt{V}}$ also get multiplied by $c_0$ (assuming that $S_0=0$). In particular, this is saying that the ratio $\frac{\rho_m}{\Lambda}$ and hence $\Omega_m$ remain constant if we keep $\alpha$ and the series of random numbers fixed. Therefore, for a fixed seed number, one will be moving along some fixed curve in the $(\alpha,\Omega_m)$ plane.
{\color{red} Need to explain it properly.}

% Figure environment removed

\subsection{Auto-Correlation Time}
An important property of Everpresent Lambda is that the auto-correlation time of the model is of the order of the Hubble time. Intuitively, this means that roughly it takes a Hubble time for $\Lambda$ to become independent of its past values and to cross zero or change sign. By one Hubble time, we mean the time it takes for the scale factor to roughly double.\\
To see why this is true, we'll calculate $\langle\Omega_\Lambda(a_1)\Omega_\Lambda(a_2)\rangle$. Combining \ref{Slam1} and \ref{Slam2} gives
\begin{equation}
    \Lambda(a_1)=\frac{l_p^2}{V(a_1)}\sum_{e_i\in\mathcal{V}(a_1)} b(e_i)
\end{equation}
The summation is performed on all the causal set elements $e_i$ that are in the past light cone of a point at $a_1$, and $b$ is a Bernoulli random number:
\begin{equation}
    b(e_i)=\begin{cases}
    1\ \ \ \ p=\frac{1}{2}\\
    -1\ \ p=\frac{1}{2}
    \end{cases}
\end{equation}
We have $\langle b(e_i)b(e_j)\rangle=\delta_{ij}$; therefore, if $a_1<a_2$ we have
\begin{equation}
    \langle V_1 V_2\Lambda(a_1)\Lambda(a_2)\rangle=l_p^4\sum_{e_i\in\mathcal{V}(a_1)} 1= \frac{l_p^4}{l_c^4}\langle V_1\rangle=\alpha^2\langle V_1\rangle
\end{equation}
Here we cannot proceed any further without simplifying assumptions. We assume that we can take $V$ and $H$ in and out of the expectation values. This is certainly not correct, but for the purpose of a rough understanding of the correlation time, it suffices. Under this assumption, we can write
\begin{equation}
    \langle\Omega_\Lambda(a_1)\Omega_\Lambda(a_2)\rangle=\frac{\alpha^2}{\langle H_1^2\rangle\langle H_2^2\rangle\langle V_2\rangle}
\end{equation}
Now we write $\langle V(a)\rangle=\frac{\beta}{\langle H(a)^2\rangle^2}$. According to figure \ref{figstat}, we expect $\beta\sim\mathcal{O}(10^{-2})$. So we find
\begin{equation}
    \langle\Omega_\Lambda(a_1)\Omega_\Lambda(a_2)\rangle=\frac{\alpha^2}{\beta}\frac{\langle H_2^2\rangle}{\langle H_1^2\rangle}
\end{equation}
Now using the Friedmann equation \ref{FR} and noting that $\langle\Lambda\rangle=0$, we find that in matter dominated era
\begin{equation}
    \langle\Omega_\Lambda(a_1)\Omega_\Lambda(a_2)\rangle=\frac{\alpha^2}{\beta}\left(\frac{a_1}{a_2}\right)^3
\end{equation}
Changing the time variable from scale factor $a$ to $\lambda=\ln a$ we have shown that
\begin{equation}
\label{autocor}
    \langle\Omega_\Lambda(\lambda_1)\Omega_\Lambda(\lambda_2)\rangle=\frac{\alpha^2}{\beta}e^{-3|\lambda_1-\lambda_2|}
\end{equation}
which confirms that the auto-correlation time is of order of one Hubble time.

We have made some naive assumptions in deriving \ref{autocor}, so we don't expect the auto-correlation function to be exactly an exponential function. Indeed, in figure \ref{autocorfig}, one can see that the auto correlation function is not an exponential. Nevertheless, the dependency on $\Delta\lambda$ confirms that when the scale factor is multiplied by an $\mathcal{O}(1)$ number, the correlation of $\Lambda$ with its previous values gets highly suppressed.

% Figure environment removed
\subsection{Model 2 and Beyond}
In \cite{Zwane_2018}, the authors came up with a “Model 2" that also tries to incorporate the idea of Everpresent Lambda as explained in section \ref{EvLSec}. To make it computationally fast, they do not involve the volume calculation. Instead, they take the fluctuating dark energy and Hubble time auto-correlation as the main premises that they want to include in the model. They also make a couple of assumptions. First, they assume that $|\Omega_\Lambda(\lambda)|\leq1$. In fact, $\Omega_\Lambda(\lambda)\leq1$ is always true since the other components of the universe have positive energy density, but $\Omega_\Lambda(\lambda)\geq-1$ is just an assumption that they make to be able to define $\hat{\Omega}_\Lambda(\lambda)=\tanh^{-1}(\Omega_\Lambda(\lambda))$. They need this for their second assumption; that the values of $\hat{\Omega}_\Lambda(\lambda)$ for different $\lambda$ form a multivariate random Gaussian variable. Hence, they can instantly make a history of $\Omega_\Lambda(\lambda)$ just by knowing the covariance matrix of $\hat{\Omega}_\Lambda(\lambda)$. Here they make their third assumption. They assume the auto-correlation function to be the following
\begin{equation}
\label{model2}
    \langle\hat{\Omega}_\Lambda(\lambda_1)\hat{\Omega}_\Lambda(\lambda_2)\rangle=\Tilde{\alpha}^2e^{-\frac{\mu}{2}(\lambda_1-\lambda_2)^2}
\end{equation}
with $\Tilde{\alpha}$ and $\mu$ being the free parameters of the model. For $\mu\sim\mathcal{O}(1)$, the auto-correlation time of the fluctuations in dark energy are of the order of one Hubble time. However, one essential difference between the realizations of Model 1 and those of Model 2 is that the histories of $\Omega_\Lambda$ in Model 1 are more rugged at smaller time scales, see Figure 1 in \cite{Zwane_2018}. The reason is that an exponential auto-correlation function drops faster and has a smaller power close to $\Delta\lambda=0$.\\
Taking these considerations into account, more models can be built based on the Everpresent Lambda idea that are computationally as fast as Model 2 but better resemble the realizations of Model 1. For this purpose, \ref{model2} should be replaced with an exponential function, or even with the numerical blue curve from figure \ref{autocorfig}.

\subsection{On the Fluid Interpretation}
As mentioned earlier, if we insist on the Everpresent Lambda being indeed a “Cosmological Constant", in the sense that $p_\Lambda=-\rho_\Lambda$, then the Friedmann equations and the continuity equation taken together are incompatible with a fluctuating $\Lambda$. One way out of this dilemma that was first suggested in \cite{Ahmed_2013} is to relax the condition $p_\Lambda=-\rho_\Lambda$ and allow the equation-of-state parameter $w_\Lambda$ to be realization-dependent.\\
Here is how the idea works. If we require both of the Friedmann equations to be valid, then we would have the continuity equation $\Dot{\rho}_\Lambda+3H(\rho_\Lambda+p_\Lambda)=0$. Now there are two schemes for a fluid interpretation of the Everpresent Lambda.
\begin{itemize}
    \item Scheme 1: Set $\rho_\Lambda=\frac{1}{8\pi G}\Lambda$. Then consistency implies that \begin{equation}
        p_\Lambda=-\frac{1}{8\pi G}\left(\frac{\Dot{\Lambda}}{3H}+\Lambda\right)
    \end{equation}
    \item Scheme 2: Set $p_\Lambda=-\frac{1}{8\pi G}\Lambda$. Then consistency implies that $\rho_\Lambda$ should be the solution to the first order equation
    \begin{equation}
    \label{sch2}
        \frac{\Dot{\rho}_\Lambda}{3H}+\rho_\Lambda=\frac{\Lambda}{8\pi G}
    \end{equation}
\end{itemize}
The advantage of the fluid interpretation is that there is no need to alter the Einstein equations, and so the perturbation theory for the Everpresent Lambda cosmology would be more straightforward as well. However, we shall now see that the Scheme 1 is problematic.

Whenever $\Lambda$ crosses zero, we have $\rho_\Lambda=0$ in Scheme 1. However, $\Dot{\Lambda}$ being non-zero says that $p_\Lambda\neq0$. Therefore, the equation-of-state parameter $w_\Lambda$ diverges at these zero-crossing instants. This might seem OK if we only work with $\rho_\Lambda$ and $p_\Lambda$, but problems arise at the level of perturbations. The second order equation governing the density contrast of the dark energy fluid is (see \cite{gavela2009dark})
\begin{equation}
\begin{aligned}[b]
    \partial_a^2\delta_\Lambda+&\frac{1}{2a}\left(5+3\frac{p_t}{\rho_t}-6w_\Lambda\right)\partial_a\delta_\Lambda+\frac{9(c_\Lambda^2-w_\Lambda)}{2a^2}\delta_\Lambda\\& =\left(1+w_\Lambda\right)\left(\frac{1}{a}\left(1+\frac{3p_t}{\rho_t}-3c_\Lambda^2\right)\partial_a\delta_c+\frac{3}{2a^2}\sum_{i\neq\Lambda}\left(1+3\frac{\delta p_i}{\delta\rho_i}\right)\Omega_i\delta_i\right)
\end{aligned}
\end{equation}
The equation is written in synchronous gauge and in the rest frame of dark matter. $\delta_\Lambda=\frac{\delta\rho_\Lambda}{\rho_\Lambda}$ is the density contrast of Everpresent Lambda, and $\delta_c$ is that of cold dark matter. $p_t$ and $\rho_t$ are the total background pressure and energy density.\\ Now according to this equation, a divergence in $w_\Lambda$ is detrimental to the perturbation variable $\delta_\Lambda$. Every time $\Lambda$ crosses zero, the whole perturbation theory of the Scheme 1 fluid Everpresent Lambda breaks down.
% Figure environment removed
Scheme 2 is problematic in a different way. As shown in \cite{Ahmed_2013}, solving for the first Friedmann equation with $\rho_\Lambda$ from \ref{sch2} is like solving the second Friedmann equation or the acceleration equation. The issue with that is numerical instability. Small initial numerical errors grow hugely and render the whole numerical integration unreliable.
\section{Fitting the model to Supernovae}
To gain some understanding of the basic behaviors of the model, it is useful to try to fit the parameters of the model to the Supernovae Ia data, \cite{riess2022comprehensive}.\\
The parameters used in the MCMC are $(\Omega_m^0h^2,\alpha)$. However, these two parameters do not uniquely determine a background history due to the stochastic nature of the model. A unique history is obtained only after the selection of a series $(\xi_1,...,\xi_n)$ of independent, standard normal random numbers. In the code, this can be reduced to the selection of a \textit{seed} number $s$. Then the natural question is whether $s$ should be taken as a parameter in MCMC. In \cite{Zwane_2018} the authors used $s$ as a parameter in their MCMC search, and they claimed that although the likelihood is not a continuous function of $s$, the Markov chain turns out to converge for CMB data.\\
As the likelihood surface is, in principle, stochastic along the axis $s$, we take a more conservative approach. We take $s$ to be constant for each Markov chain, and in this way we guarantee that it will converge. Then we have to run MCMC for many seeds independently, and check how often we can get a likelihood as good as $\Lambda \text{CDM}$. In Appendix A, we expand on how these independent Markov chains for different seeds should be combined to obtain a Bayesian analysis of such a stochastic model.\\

Now we run the MCMC chain for 20000 different seeds. The aim is to find seeds that, after convergence, give better fits to data than $\Lambda$CDM. Out of that number, 27 seeds gave a likelihood in the range $[0.01L_{\Lambda\text{CDM}},245L_{\Lambda\text{CDM}}]$, and 3 seeds gave better likelihood than $\Lambda$CDM. In figure \ref{sneIadists} you can see some of the statistics of the aforementioned 27 seeds.

% Figure environment removed
Although $H_0$ is well constrained, there is almost no constraint on $\Omega_m$ from SNe Ia data. We can interpret, from figure \ref{supaaa}, that for any value of $\Omega_m$, we can find a seed number for which Model 1 gives a reasonably good fit to SNe Ia.

In figure \ref{sneIacorner}, you can see the chain corresponding to a particular seed in more details.


% Figure environment removed



Having the above results, how can we judge if the model is doing good or not? What do the success rates $27/20000$ or $3/20000$ have to tell us? More basically, how should one decide about the plausibility of a stochastic model? At first sight, these figures seem discouraging. However, if in a simplified setting, we are given a realization of a random walk and we are to check if it is actually an outcome of a random walk model we have, we would find that only a very small fraction of our seed numbers give a good fit to the data.\\
In \cite{Zwane_2018}, the authors used a $\chi^2_{\text{red}}$ to evaluate how likely it is that a best fit (found by MCMC) would actually be a realization of Model 1. Simply put, their measure doesn't have anything to do with the relation between the model and the data. It only measures how likely it is that a given history for $\Lambda$ would be an outcome of the generation of random numbers $(\xi_1,...,\xi_n)$ according to Model 1. Hence, not very surprisingly, they found that their best fit (in their case to CMB data) is a very typical outcome of their model, while $\Lambda$CDM is not. Of course it is expected that, since the best fit has been found by the model, the corresponding sequence of random numbers would be a typical outcome of $n$ gaussian random numbers. Therefore, we believe that another criterion should be used.

{\color{red}

\begin{itemize}
    \item Discussion about Mock supernova data
    \item Discussion about CMB analysis with model 1 and model 2. 
\end{itemize}
}

\section{Analysis with CMB data}



\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|}
\hline  \multicolumn{4}{|c|}{Model 1 }  \\
\hline
\hline &Model 1 & Model 1 with LSS suppression ($\Delta z = 100$) & Model 1 with LSS suppression ($\Delta z = 200$)\\
\hline $\#seed_{-2 \log \mathcal{L}<3000.0}$ & $91$ & $248$  & $539$ \\
\hline$-2 \log \mathcal{L}$ & $2495.6$ & $ $2360.5$ &  $2358.5$\\
\hline
\end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}

\section{Understanding the CMB data}
Another and probably the most important data for testing any cosmological model is the cosmic microwave background (CMB) data. To understand the effect of the Everpresent Lambda model on the CMB power spectra, here we have shown how the cosmological power spectra behaves. Here we have just consider the effects on the scalar power spectra. No tensor modes are consider for our analysis. If we don't consider the CMB lensing then, the CMB scalar power spectrum can be written as

\begin{equation}
C_l^{X X}=(4 \pi)^2 \int k^2 \mathrm{~d} k P^s(k)\left[\Delta_{X l}^s(k)\right]^2\,, \quad C_l^{T E}=(4 \pi)^2 \int k^2 \mathrm{~d} k P^s(k) \Delta_{T l}^s \Delta_{E l}^s\,,
\end{equation}

\noindent here $X\in (T,E)$. $P^s(k)$ is the primordial power spectra from inflation and $\Delta_{X l}^s(k)$ are the brightness fluctuation functions for the temperature and the $E$-mode polarization power spectra and are given by  

\begin{equation}
\Delta_{T l}(k)=\int_0^{\tau_0} \mathrm{~d} \tau S_T(k, \tau) j_l(k(\tau_0-\tau)), \quad \Delta_{E l}(k)=\sqrt{\frac{(l+2) !}{(l-2) !}} \int_0^{\tau_0} \mathrm{~d} \tau S_P(k, \tau) j_l(k(\tau_0-\tau))\,.
\end{equation}


%% Figure environment removed

% Figure environment removed

\noindent $j_l(x)$ are the spherical bessel functions. $S_T(k, \tau)$ and $S_E(k, \tau)$ are known as the temperature and the polarization source terms and these are the terms that are dependent on the cosmological perturbation and the expansion history of the universe at different epoch. The source terms for the temperature and the $E$-mode polarization power spectra are given by 
\begin{eqnarray}
&S_T(k, \tau)=-g\left(\Delta_{T 0}+2 \ddot{\alpha}+\frac{\dot{\theta}_b}{k^2}+\frac{\Pi}{4}+\frac{3 \ddot{\Pi}}{4 k^2}\right)+e^{-\kappa}(\dot{\eta}+\ddot{\alpha})+\dot{g}\left(\frac{\theta_b}{k^2}+\frac{3 \dot{\Pi}}{4 k^2}\right)+\frac{3 \ddot{g} \Pi}{4 k^2} \label{Tsource}\\
&S_P(k, \tau)=\frac{3 g \Pi(\tau, k)}{4 k^2\left(\tau_0-\tau\right)^2} \label{Esource}.
\end{eqnarray}

In the temperature source term, consists of 4  terms. The first term is known as the Sachs Wolf term, the third term is the velocity term. The second term is the integrated Sachs Wolf term. The last is the a contribution from anisotropic stress and its significantly small for stndard $\Lambda$CDM. Here we can see that except the ISW term all other terms depends on visibility function given by $g$, where $(g=\dot{\kappa} \exp (-\kappa))$. $\kappa=\int_\tau^{\tau_0} a n_e \sigma_T \mathrm{~d}\tau$, is the optical depth at the time $\tau$. $\tau$ is the conformal time. All the derivative represented by over dot $\dot{(\;)}$ represents the derivative with respect to conformal time.
$\alpha=(\dot{h}+6 \dot{\eta}) / 2 k^2$. Here all the equations are in synchronous gauge. $\Pi$ is the anisotropic stress term and is given by 

\begin{equation}
\Pi=\Delta_{T 2}+\Delta_{P 2}+\Delta_{P 0}\,.
\end{equation}

Rest of the variables are the pertubation variables for different component of the universe. 

We consider that the universe consists of 5 components \-- Cold dark matter, baryon (which includes leptons except neutrinos), neutrino, photons and dark energy. Neutrinos, dark matter and the dark energy does not couple to anything except through gravity.  In our universe neutrinos were coupled with baryons through the weak interaction before neutrino decoupling. However, our domain of interest for CMB calculation is much later than neutrino decoupling, which allows neutrinos to freestream through the medium. Initially photons and the baryons were strongly couple before the last scattering surface. However, they got decoupled and freestream after the last scattering. 

\paragraph{Metric Perturbation}
For the perturbation calculations it is assumed that the perturbations are isotropic in space and therefore there is no angular dependence. In synchronous gauge for scalar only perturbation the line element is given by
$ds^2 = a^2(\tau)\left\{\mathrm{d} \tau^2 - \left(\delta_{i j}+h_{i j}\right) \mathrm{d} x^i \mathrm{~d} x^j\right\}$, where $h_{i j}$ is  scalar part metric perturbation. $\delta_{i j}$ is the Kronecker delta and $h_{i j}$ is the scalar perturbation. In the Fourier space we can represent these perturbations using two variables, $h(k,\tau)$ and $\eta(k,\tau)$ which are used in Eq.~\ref{Tsource}. The relation is given by

\begin{equation}
h_{i j}(\vec{x}, \tau)=\int \mathrm{d}^3 k e^{i \vec{k} \cdot \vec{x}}\left\{\hat{k}_i \hat{k}_j h(\vec{k}, \tau)+\left(\hat{k}_i \hat{k}_j-\frac{1}{3} \delta_{i j}\right) 6 \eta(\vec{k}, \tau)\right\}
\end{equation}

Of course we consider a isotropic case and hence no angular dependence in $h$ and $\eta$ are considered. 

\begin{eqnarray}
\frac{\dot{a}}{a}\dot{h} & = & 2k^{2}\eta+\frac{8\pi G}{c^{2}}\left[\rho_{cr}\left(\Omega_{c}\frac{\delta_{c}}{a}+\Omega_{b}\frac{\delta_{b}}{a}+\Omega_{d}\delta_{d}a^2\right)\right.\nonumber\\
 &  &+\left(\frac{4\sigma_{B}T^{4}}{c^{3}}\right) \left(\frac{\delta_{\gamma}}{a^{2}}\right.+N_{\text{eff}}\left(\frac{7}{8}\right)\left(  \left(\frac{4}{11}\right)^{4/3}\frac{\delta_{\nu}}{a^{2}}\right.
 \left.\left.\left.+\left(\frac{43}{11}\frac{1}{g_{A}}\right)^{4/3}\frac{\delta_{\nu_m}}{a^{2}}\right)\right)\right]\,, \\
2k^{2}\dot{\eta}&=&\frac{8\pi G}{c^{2}}\left[\rho_{cr}\left(\Omega_{b}\frac{\theta_{b}}{a}+\Omega_{d}\delta_{d}a^2\right)
+\left(\frac{4\sigma_{B}T^{4}}{c^{3}}\right)\left(\frac{\theta_{\gamma}}{a^{2}}+N_{\text{eff}}\left(\frac{7}{8}\right)\left(\left(\frac{4}{11}\right)^{4/3}\frac{\theta_{\nu}}{a^{2}} \right.\right.\right.\nonumber\\ 
& &+\left(\frac{43}{11}\frac{1}{g_{A}}\right)^{4/3}  \left.\left.\left.\times\left(\frac{\theta_{\nu_m}}{a^{2}}\right)\right)\right)\right]\,,
\end{eqnarray}

Here $\delta_x$ and $\theta_x$ are the perturbation variables. $\delta_x = \frac{\delta \rho_x}{\rho_x}$  and $\theta_x = \vec{k}.\vec{v}_x$, i.e. the divergence of the velocity in Fourier space and $x\in (c,b,\gamma,\nu)$.

\paragraph{Baryons and Cold Dark matter}

The equation for the perturbation variable for baryon can be written as 
\begin{eqnarray}
\dot{\delta}_{b} & = & -\theta_{b}-\frac{1}{2}\dot{h}\,,\nonumber \\
\dot{\theta}_{b} & = & -\frac{\dot{a}}{a}\theta_{b}+c_{s}^{2}k^{2}\delta_{b}+\frac{4\rho_{\gamma0}}{3\rho_{b0}}an_{e}\sigma_{T}(\theta_{\gamma}-\theta_{b})\,,
\end{eqnarray}

Here the last therm in the $\theta$ expression is the coupling term. At the end of the recombination era, when there is not coupling between the baryon and the photons, this term will be very small.

For cold dark matter the perturbation equations will be similar. However as there is no interaction the last two terms are $0$. 

\paragraph{Photons and neutrinos}
Photons and the neutrinos has extra pressure. 

\begin{eqnarray}
\dot{\Delta}_{T\,0} & = & \dot{\delta}_{\gamma} = -\frac{4}{3}\theta_{\gamma}-\frac{2}{3}\dot{h}\,,\nonumber \\
\frac{4k}{3}\dot{\Delta}_{T\,1} & = & \dot{\theta}_{\gamma} = k^{2}\left(\frac{1}{4}\delta_{\gamma}-\sigma_{\gamma}\right)+an_{e}\sigma_{T}(\theta_{b}-\theta_{\gamma})\,,\nonumber \\
\dot{\Delta}_{T\,2} & = & 2\dot{\sigma}_{\gamma}=\frac{8}{15}\theta_{\gamma}-\frac{3}{5}k\Delta_{T\,3}+\frac{4}{15}\dot{h}+\frac{8}{5}\dot{\eta}-\frac{9}{5}an_{e}\sigma_{T}\sigma_{\gamma}+\frac{1}{10}an_{e}\sigma_{T}\left(\Delta_{P\,0}+\Delta_{P\,2}\right)\,,\nonumber\\
\dot{\Delta}_{T\, l^{\gamma}} & = & \frac{k}{2l^{\gamma}+1}\left[l^{\gamma}\Delta_{T\,(l^{\gamma}-1)}-(l^{\gamma}+1)\Delta_{T\,(l^{\gamma}+1)}\right]-an_{e}\sigma_{T}\Delta_{T\, l^{\gamma}}\,,\quad l^{\gamma}\geq3\,,\nonumber \\
\dot{\Delta}_{P\, l^{\gamma}} & = & \frac{k}{2l^{\gamma}+1}\left[l^{\gamma}\Delta_{P\,(l^{\gamma}-1)}-(l^{\gamma}+1)\Delta_{P\,(l^{\gamma}+1)}\right]+an_{e}\sigma_{T}\left[-\Delta_{P\, l^{\gamma}}\right. \label{eq:k}\\
&&+\left.\frac{1}{2}\left(\Delta_{T\,2}+\Delta_{P\,0}+\Delta_{P\,2}\right)\left(\delta_{l^{\gamma}0}+\frac{\delta_{l^{\gamma}2}}{5}\right)\right]\,,\nonumber 
\label{photonmomentum}
\end{eqnarray}


\noindent where $\sigma_{T}$ is the Thomson scattering cross-section, and $n_{e}$ is the free electron number density. 

For neutrinos the perturbation equations will be same. Except that the neutrinos are Fermions. Therefore, the polarization terms will not be there. Also there is not coupling. Therefore, the coupling terms will be zero for neutrinos. 

\paragraph{Tight coupling era}
Before recombination the opacity $\dot{\mu}=an_{e}\sigma_{T}$
was very large. Hence, the photons and baryons evolve together during
the tight coupling approximation. The conformal time scale for the
photon baryon interaction is $\tau_{c}=\left(an_{e}\sigma_{T}\right)^{-1}$.
In the tight coupling era, this interaction time scale is much smaller than the timescale on
which the modes in the super-Hubble scale evolve, i.e. $\tau_{H}=a/\dot{a}$
and the modes for the sub-Hubble scale i.e. $\tau_{k}=1/k$. The standard
numerical integration is not efficient to integrate the baryon and
photon perturbation equations independently in this era. Therefore,
Peebles and Yu, \citep{Ma1995, Peebles1970} developed a new set of equations
for the baryon perturbations which is valid in the tight coupling
regime i.e. where $\tau_{c}/\tau_{H}$ or $\tau_{c}/\tau_{k}\ll 1$. Instead of using the standard baryon and photon equations, the idea is to integrate a coupled form of differential
equation, given by 

\begin{equation}
(1+R)\dot{\theta}_{b}+\frac{\dot{a}}{a}\theta_{b}-c_{s}^{2}k^{2}\delta_{b}-k^{2}R\left(\frac{1}{4}\delta_{\gamma}-\sigma_{\gamma}\right)+R(\dot{\theta}_{\gamma}-\dot{\theta}_{b})=0\,,\label{momentum}
\end{equation}


\noindent where $R=(4/3)\rho_{\gamma0}/\rho_{b0}$, and $\left(\dot{\theta}_{\gamma}-\dot{\theta}_{b}\right)$
is given by the following equation

\begin{eqnarray}
\dot{\theta}_{b}-\dot{\theta}_{\gamma} &=& \frac{2R}{1+R}\frac{\dot{a}}{a}(\theta_{b}-\theta_{\gamma})+\frac{\tau_{c}}{1+R}\left[-\frac{\ddot{a}}{a}\theta_{b}-\frac{\dot{a}}{a}k^{2}\left(\frac{1}{2}\delta_{\gamma}\right)\right. \nonumber\\
& &+\left. k^{2}\left(c_{s}^{2}\dot{\delta}_{b}-\frac{1}{4}\dot{\delta}_{\gamma}\right)\right]+O(\tau_{c}^{2}).\label{sliprate}
\end{eqnarray}

\noindent For obtaining $\dot{\theta}_\gamma$, we can use the following equation~\citep{Ma1995}

\begin{equation}
    \dot{\theta}_\gamma = - R^{-1}\Bigg(\dot{\theta}_b + \frac{\dot{a}}{a}\theta_b - c_s^2k^2\delta_b\Bigg)+k^2\Bigg(\frac{1}{4}\delta_\gamma-\sigma_\gamma\Bigg)
\end{equation}

\noindent In the tight coupling limit the higher multipoles of the photon distribution, i.e. $F_{\gamma 3}$, $F_{\gamma 4}$, ... and $G_{\gamma 0}$, $G_{\gamma 1}$, $G_{\gamma 2}$, ... can be taken as $0$. We also consider $\dot{\sigma}_\gamma = 0$, which from Eq.~\ref{photonmomentum} gives

\begin{equation}
    \sigma_\gamma = \frac{\tau_c
}{9}\Bigg(\frac{8}{3}\theta_\gamma +\frac{4}{3}\dot{h} + 8\dot{\eta}\Bigg) \end{equation}

\paragraph{Dark energy}
For any dark energy if it is assumed to be a fluid then the equations for dark energy perturbation variables are given by

\begin{eqnarray}
\label{fluid}
    \dot{\delta} &=& - (1+w) \left(\theta+{\dot{h}\over 2}\right)
      - 3{\dot{a}\over a} \left({\delta P \over \delta\rho} - w
      \right)\delta  \,,\nonumber\\
    \dot{\theta} &=& - {\dot{a}\over a} (1-3w)\theta - {\dot{w}\over
         1+w}\theta + {\delta P/\delta\rho \over 1+w}\,k^2\delta
         - k^2 \sigma\,, \label{matter_prtb}
\end{eqnarray}

\subsection{Everpresent Lambda and evaluation of cosmological perturbation}

\subsection{Recombination}
In Eq.~\ref{Tsource} and Eq.~\ref{Esource} we can see that all the terms excapt the ISW term are multiplied with the visibility function, $g$ or its derivatives, $\dot{g}$ or $\ddot{g}$. The visibility function mainly depends on the background expansion history during recombination. However, the perturbation params inside the bracket of the SW, velocity and the polarization depends on the perturbation and the nature of dark energy long before the last scattering surface. Therefore, there is no reason to believe that for a particular $k$ mode the changes in the perturbations inside the bracket can get cancelled by the change $g$.  The variation of $g$ is not correlated with the perturbation equations. 

If we change dark energy during the recombination era it will change the expansion history during the recombination era, then it will change the rate of recombination and hence $g$, $\dot{g}$ and $\ddot{g}$. Both their heights and the spread. 

To provide a semi-analytical estimate we can use the Peebles recombination equation 

\begin{equation}
\frac {\mathrm{d}x_{\text{e}}}{\mathrm{d}\tau}=-aC\left[\alpha^{(2)} (T_{b})n_{\text{p}}x_{e}-4(1-x_{\text{e}})\beta(T_{b})e^{-E_{21}/T}\right] = -aC \mathcal{Z}
\end{equation}

$\mathcal{Z}$ is the  term inside the bracket. The terms inside the square bracket are independent on $\dot{a}$ and just a function of $a$. Therefore, if there is any change in $\dot{a}$ then it will enter the equation through $C$ or through the derivative in the left hand side of the equations. 

\begin{equation}
C = \frac{\Lambda_\alpha + \Lambda_{2s\rightarrow1s}}{\Lambda_\alpha + \Lambda_{2s\rightarrow1s} +\beta^{(2)}(T_b)} 
\end{equation}

\noindent where 

\begin{equation}
\beta^{(2)}(T_b) = \beta(T_b)e^{+hc/\lambda_\alpha k_BT_b}\;,   \;\;\;\;\;\;\;\;\;\;\;\;\;\; \Lambda_\alpha = \frac{8\pi\dot{a}}{a^2\lambda^3_\alpha n_{1s}}\,.
\end{equation}

% Figure environment removed

% Figure environment removed

The only term in $C$ that gets affected by expansion history is $\Lambda_\alpha$. So we can write 

\begin{eqnarray}
    C + \delta C &=& \frac{\Lambda_\alpha + \Lambda_{2s\rightarrow1s} + \delta\Lambda_\alpha}{\Lambda_\alpha + \Lambda_{2s\rightarrow1s} +\beta^{(2)}(T_b) + \delta\Lambda_\alpha} \noindent \\
    &=& \frac{\Lambda_\alpha + \Lambda_{2s\rightarrow1s}}{\Lambda_\alpha + \Lambda_{2s\rightarrow1s} +\beta^{(2)}(T_b)}\times \left[1+\frac{\delta\Lambda_\alpha}{\Lambda_\alpha + \Lambda_{2s\rightarrow1s}}-\frac{\delta\Lambda_\alpha}{\Lambda_\alpha + \Lambda_{2s\rightarrow1s} +\beta^{(2)}(T_b)} \right]
\end{eqnarray}

Also we can write 

\begin{eqnarray}
\frac{dx_e}{da} = \frac{1}{\dot{a}+\delta\dot{a}}\frac{dx_e}{d\tau} = \frac{1}{\dot{a}}\frac{dx_e}{d\tau}\left(1 - \frac{\delta\dot{a}}{\dot{a}}\right) \\
=-aC\mathcal{Z} \left[1+\frac{\delta\dot{a}}{\dot{a}}\left(\frac{\Lambda_\alpha}{\Lambda_\alpha + \Lambda_{2s\rightarrow1s}}-\frac{\Lambda_\alpha}{\Lambda_\alpha + \Lambda_{2s\rightarrow1s} +\beta^{(2)}(T_b)} - 1 \right) \right]
\end{eqnarray}
TWe can consider that the first two terms are small and the sum will be smaller than $1$. Therefore, $-1$ will be the dominating term and if we add more dark energy then $\frac{\dot{a}}{\dot{a}}$ will be positive. Therefore, $\frac{dx_e}{da}$ will increase. This will increase the value of $g$ and the spread of $g$ will be smaller. Therefore, it will change the CMB powerspectrum. Firstly, if the height of the peak of $g$ becomes smaller then it will change the power in $C_l$, which will change the $A_s$ parameter. If the redshift of the peak of $g$ changes then it will move the CMB powetrspectrum to higher or smaller $l$. If the spread of $g$ changes then it will change the ratio of the peaks as it will add different amount of power to the SW and the velocity terms. 

% Figure environment removed
% Figure environment removed

\subsection{Understanding the ISW effect}

\begin{equation}
\ddot{\alpha}=-\frac{3 \dot{\sigma}}{2 k^2}+\dot{\eta}-2\left(\frac{\dot{a}}{a}\right) \dot{\alpha}-2 \frac{d}{d \tau}\left(\frac{\dot{a}}{a}\right) \alpha .
\end{equation}

In the above expression, $\sigma$ is the shear term its contribution is significantly small after LSS. Therefore, the first two terms can be neglected and the remaining part can be written as
\begin{equation}
\ddot{\alpha}=-2 \frac{d}{d \tau}\left(\frac{\dot{a}}{a} \alpha\right) .
\end{equation}

Now we using the separation of variable to break $\alpha$ apart into a $\tau$ dependent part and a $k$ dependent part, can write $\alpha$ as
\begin{equation}
\alpha(k, \tau)=\alpha_\tau(\tau) \alpha_k(k) .
\end{equation}

The equation in the conformal time domain becomes,
\begin{equation}
\ddot{\alpha}_\tau=-2 \frac{d}{d \tau}\left(\frac{\dot{a}}{a} \alpha_\tau\right) \quad\;
\dot{\alpha}_\tau+2 \frac{\dot{a}}{a} \alpha_\tau = K
\end{equation}

So the powerspectrum for the ISW effect is 

\begin{equation}
S_T(k, \tau)=e^{-\kappa}(\dot{\eta}+\ddot{\alpha}) \quad 
\label{ISWTsource}
\end{equation}
As $\dot{\eta}$ is negligable in comparison to the other term
\begin{equation}
C_l^{T T, ISW}=(4 \pi)^2 \int k^2 \mathrm{~d} k P^s(k)\left[\int_0^{\tau_0} \mathrm{~d} \tau e^{-\kappa}\ddot{\alpha} j_l(k(\tau_0-\tau))\right]^2 
\end{equation}

Now for any $l$ the spherical bessel function $j_l(x)$ has its first peak near $x = \frac{\pi}{2}l$. Now, lets consider a particular $l$. We don't want to Now suppose we add some additional DE at time $\tau_1$ then it will increase the amplitude of the brightness function at about $k \approx \frac{\pi l}{2 (\tau_0-\tau)}$. As we want to keep the power at $C_l$ exact same we need to area under the brightness function same. To do that I need to decrease the amplitude of the brightness fluctuation function at some other $k$ So we need to add some negative DE at a $\tau \approx \tau_0 - \frac{\pi l}{2 k}$. In this way we can preserve the power at the particular $C_l$. However, when we preserve the power at a particular $l$, it does not guarantee that the power will remain unchanged at other multi-poles. 




% Figure environment removed
% Figure environment removed
\noindent 
\begin{table}
    \centering
\begin{tabular}{|c|c|c||c|c|c|c|}
\hline  \multicolumn{3}{|c||}{Modified DE after LSS } & \multicolumn{3}{c|}{ Modified DE before LSS  } \\
\hline
\hline & Average & Best Fit & & Average & Best Fit \\
\hline$\Omega_m h^2$ & $0.1415 \pm 0.0015$ &$0.1415$& $\Omega_m h^2$  & $0.1426 \pm 0.0021$ & 0.1430\\
\hline$\Omega_b h^2$ & $0.0224 \pm 0.00016$ &$0.0224$& $\Omega_b h^2$  & $0.0227 \pm 0.00026$ & $0.0226$ \\
\hline$h$ & $0.6370 \pm 0.0384$ & $0.6275$ & $h$ &$0.6910 \pm 0.0121$ & $0.6866$ \\
\hline$\tau$ & $0.1238 \pm 0.0295$ & $0.1231$ & $\tau$ & $0.1303 \pm 0.0287$ & 0.1245 \\
\hline$n_s$ & $0.9650 \pm 0.0045$ & $0.9655$ & $n_s$  & $0.9758 \pm 0.01$ & 0.9713\\
\hline $\log \left(10^{10} A_s\right)$ & $3.17 \pm 0.0571$ & $3.1340$  & $\log \left(10^{10} A_s\right)$ & $3.1910 \pm 0.0550$ & $3.1811$ \\
\hline$f_{z=1}$ & $0.0248 \pm 0.028$ & $0.0467$ &$f_{z=2140}$ & $0.00037 \pm 0.00078$ & $0.00029$ \\
\hline$f_{z=10}$ & $0.0018 \pm 0.0289$ & $-0.0249$ &$f_{z=4570}$ & $0.0019 \pm 0.0018$ & $0.0016$ \\
\hline$f_{z=50}$ & $0.0034 \pm 0.0201$ & $-0.0015$ & $f_{z=10000}$ & $0.0026 \pm 0.0028$ & $0.0021$ \\
\hline$f_{z=100}$ & $0.0133 \pm 0.0160$ & $0.0224$ & $f_{z=21380}$ & $0.0030 \pm 0.0045$ & $0.0026$ \\
\hline$f_{z=500}$ & $0.00075 \pm 0.0057$ & $-0.0028$ & $f_{z=45710}$ & $0.0010 \pm 0.0084$ & $0.0009$ \\
\hline$-2\Delta \log \mathcal{L}$ & & $5.68$ & $-2\Delta\log \mathcal{L}$ &  & $9.02$\\
\hline
\end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}
\section{Appendix A}
\subsection{Issue with the mcmc analysis}
The true physical parameters of the model (at least those relevant for supernova) are $(\alpha, \Omega_m h^2)=\theta$. There is also the auxiliary parameter $seed=s$, which we need for calculating likelihoods.
$$\begin{aligned}L\left( \Omega _{m}h^{2},\alpha \right) =L\left( \theta \right) =P\left(  D| \theta \right) =\sum _{seed}P\left(  D| \theta ,seed\right) P\left( seed\right) \\
=\dfrac{1}{N}\sum _{s}P\left(  D| \theta ,s\right) =\dfrac{1}{N}\sum _{s}L\left( \theta ,s\right) \end{aligned}$$
By $D$ I mean data, and N is the number of seeds. So the likelihood on $\theta$ is an average over $L(\theta,s)\propto \exp(-1/2\chi^2(\theta,s))$.\\
Therefore, for a correct MCMC in $\theta$ space, we have to use $$\dfrac{L\left( \theta _{i+1}\right) }{L\left( \theta i\right) }=\dfrac{\sum _{s}\exp \left( -\dfrac{1}{2}\chi ^{2}\left( \theta _{i+1},s\right) \right) }{\sum _{s'}\exp \left( -\dfrac{1}{2}\chi ^{2}\left( \theta _{i},s'\right) \right) }.$$ This suggests the following recipe: Take each step of MCMC only in $(\alpha, \Omega_m h^2)=\theta$ space; for each $\theta$, take 1000 seeds, and compute the $\chi^2$ for those 1000 seeds in \boldsymbol{parallel} in the cluster, then sum over likelihoods to find the transition probabilities. We will be sampling the posterior: $P(\theta\sdorange{|D})=\frac{L(\theta)}{Z}$.\\
One might want to work things out in 3d $(\theta,s)$ parameter space instead. 2 ways come to mind:\\
1. Change seed at each step. The ratio of transition probabilities would be:
$$\dfrac{T\left(  \theta _{i+1},s_{i+1}| \theta _{i},s_{i}\right) }{T\left(  \theta _{i},s_{i}| \theta _{i+1},s_{i+1}\right) }=\dfrac{L\left( \theta _{i+1},s_{i+1}\right) }{L\left( \theta _{i},s_{i}\right) }=\dfrac{\exp \left( -\dfrac{1}{2}\chi^{2}\left( \theta _{i+1},s_{i+1}\right) \right) }{\exp \left( -\dfrac{1}{2}\chi^{2}\left( \theta _{i},s_{i}\right) \right) }.$$
By updating $s$ at each step, we will be sampling the posterior $P_1(\theta,s|D)=\frac{1}{N}\frac{L(\theta,s)}{Z}$ ($\frac{1}{N}$ is due to the prior on seeds); summing over $seed$ will give us the desired 2d posterior: $P(\theta|D)=\sum_{s} P_1(\theta,s|D)$. The downside is that this should be done in series in the cluster which takes too long.
\\2. Keep the seed constant at each run of MCMC. So take a constant $s$ and update $\theta$. The transition probabilities are:
$$\dfrac{T\left(  \theta _{i+1},s| \theta _{i},s\right) }{T\left(  \theta _{i},s| \theta _{i+1},s\right) }=\dfrac{L\left( \theta _{i+1},s\right) }{L\left( \theta _{i},s\right) }=\dfrac{\exp \left( -\dfrac{1}{2}\chi^{2}\left( \theta _{i+1},s\right) \right) }{\exp \left( -\dfrac{1}{2}\chi ^{2}\left( \theta _{i},s\right) \right) }$$
After one run, we will sample the posterior $P_2(\theta|s,D)=\frac{L(\theta,s)}{Z_2(s)}$ for a constant $s$, which this time satisfies: $\int d\theta P_{2}\left( \theta |s,D\right) =1$; therefore $Z_{2}\left( s\right) =\int d\theta L\left( \theta ,s\right) $. So $$\Rightarrow P_{2}\left( \theta |s,D\right) =\dfrac{L\left( \theta ,s\right) }{\int d\theta 'L\left( \theta ',s\right) }$$Hence, unlike $P_1$, here $P(\theta|D)\neq\sum_{s} P_2(\theta|s,D)$. In fact there is no simple algebraic relation between our desired posterior $P$ and this $P_2$ (\sdorange{Wrong. There exists}).(Note that in the previous method, since we were also searching in seed numbers, we had $\sum_s\int d\theta P_{1}\left( \theta ,s\right) =1$ and so $Z =\frac{1}{N}\sum_s\int d\theta L\left( \theta ,s\right)$ which is s-independent.)

\sdorange{I think what you point out is correct. However, I think it can be solved easily. We can each of the  chains independently and then combine those.} 

\sdorange{$Z = \int d\theta' L(\theta)=\int d\theta' P(D|\theta)=P(D) = 1$ ... as data is given}

\sdorange{$Z = \int d\theta' L(\theta,s)=\int d\theta' P(D|\theta,s)=P(D|s)$}

\sdorange{$P(D|s)$ is the probability of data given a particular seed. When all the chains are running for equal number of chain points the number of accepted data points are giving the same thing. When you are taking all the samples of equal size we are calculating the same probability. I mean for each seed P(D|s) is just $\sum exp(-\chi^2/2)$}

\sdorange{Now $P(\theta|D) = \sum P(\theta, s |D) P(D|s) = \sum L(\theta,s) = L(\theta)$ as prior on $s$ is constant.} 

Update on method 2:\\
After finding $P_2$, we can do a summation with proper coefficients to find the real posterior $P$. Just note that:
$$P(\theta|D)=\sum_{s} P_2(\theta|s,D)P(s|D)$$
On the other hand $P(s|D)=\frac{P(D|s)\pi(s)}{\sum_{s'}P(D|s')\pi(s')}=\frac{\int d\theta P(D|s,\theta)\pi(\theta)}{\sum_{s'}\int d\theta' P(D|s',\theta')\pi(\theta ')}=\frac{\int d\theta L(\theta,s)}{\sum_{s'}\int d\theta' L(\theta',s')}$

\section{Appendix B: Other issue with the model}



%\bibliographystyle{unsrt}
\bibliographystyle{plainnat}
\bibliography{biblio.bib}

\end{document}




\end{document}