\begin{thebibliography}{10}

\bibitem{giraud}
C.~Giraud, {\em Introduction to High-Dimensional Statistics}, vol.~139 of {\em
  Monographs on Statistics and Applied Probability}.
\newblock Chapman and Hall/CRC, Boca Raton, FL, 2014.

\bibitem{nguyen2010estimating}
X.~Nguyen, M.~J. Wainwright, and M.~I. Jordan, ``Estimating divergence
  functionals and the likelihood ratio by convex risk minimization,'' {\em IEEE
  Tr. on Inf. Theory}, vol.~56, no.~11, pp.~5847--5861, 2010.

\bibitem{GAN}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio, ``Generative adversarial networks,'' {\em
  Communications of the ACM}, vol.~63, no.~11, pp.~139--144, 2020.

\bibitem{GANOverviewIEEE}
A.~Creswell, T.~White, V.~Dumoulin, K.~Arulkumaran, B.~Sengupta, and A.~A.
  Bharath, ``Generative adversarial networks: An overview,'' {\em IEEE Signal
  Processing Magazine}, vol.~35, no.~1, pp.~53--65, 2018.

\bibitem{creswell2018denoising}
A.~Creswell and A.~A. Bharath, ``Denoising adversarial autoencoders,'' {\em
  IEEE transactions on neural networks and learning systems}, vol.~30, no.~4,
  pp.~968--984, 2018.

\bibitem{belghazi2018mutual}
M.~I. Belghazi, A.~Baratin, S.~Rajeshwar, S.~Ozair, Y.~Bengio, A.~Courville,
  and D.~Hjelm, ``Mutual information neural estimation,'' in {\em International
  conference on machine learning}, pp.~531--540, PMLR, 2018.

\bibitem{gretton2012kernel}
A.~Gretton, K.~M. Borgwardt, M.~J. Rasch, B.~Sch{\"o}lkopf, and A.~Smola, ``A
  kernel two-sample test,'' {\em J. of Machine Learning Research}, vol.~13,
  no.~1, pp.~723--773, 2012.

\bibitem{durkan2020contrastive}
C.~Durkan, I.~Murray, and G.~Papamakarios, ``On contrastive learning for
  likelihood-free inference,'' in {\em Int. conf. on machine learning},
  pp.~2771--2781, PMLR, 2020.

\bibitem{hermans2020likelihood}
J.~Hermans, V.~Begy, and G.~Louppe, ``Likelihood-free {MCMC} with amortized
  approximate ratio estimators,'' in {\em Int. conf. on Mach. learn.},
  pp.~4239--4248, PMLR, 2020.

\bibitem{thomas2022likelihood}
O.~Thomas, R.~Dutta, J.~Corander, S.~Kaski, and M.~U. Gutmann,
  ``Likelihood-free inference by ratio estimation,'' {\em Bayesian Analysis},
  vol.~17, no.~1, pp.~1--31, 2022.

\bibitem{DBLP:journals/corr/abs-2108-00490}
F.~Llorente, L.~Martino, J.~Read, and D.~Delgado, ``A survey of monte carlo
  methods for noisy and costly densities with application to reinforcement
  learning,'' {\em CoRR}, vol.~abs/2108.00490, 2021.

\bibitem{sisson2018handbook}
S.~A. Sisson, Y.~Fan, and M.~Beaumont, {\em Handbook of approximate Bayesian
  computation}.
\newblock CRC Press, 2018.

\bibitem{VAE}
D.~P. Kingma and M.~Welling, ``Stochastic gradient {VB} and the variational
  auto-encoder,'' in {\em Second Int. Conf. on Learning Representations, ICLR},
  vol.~19, p.~121, 2014.

\bibitem{chen2018continuous}
C.~Chen, C.~Li, L.~Chen, W.~Wang, Y.~Pu, and L.~C. Duke, ``Continuous-time
  flows for efficient inference and density estimation,'' in {\em International
  Conference on Machine Learning}, pp.~824--833, PMLR, 2018.

\bibitem{song2020score}
Y.~Song, J.~Sohl-Dickstein, D.~P. Kingma, A.~Kumar, S.~Ermon, and B.~Poole,
  ``Score-based generative modeling through stochastic differential
  equations,'' {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{ContinuouslyIndexedFlowGenerative}
R.~Cornish, A.~Caterini, G.~Deligiannidis, and A.~Doucet, ``Relaxing
  bijectivity constraints with continuously indexed normalising flows,'' in
  {\em Proceedings of the 37th ICML}, vol.~119 of {\em Proceedings of Machine
  Learning Research}, pp.~2133--2143, PMLR, 13--18 Jul 2020.

\bibitem{VIwithContinuouslyIndexedFlow}
A.~Caterini, R.~Cornish, D.~Sejdinovic, and A.~Doucet, ``{{Variational
  Inference with Continuously-Indexed Normalizing Flows}},'' in {\em
  Uncertainty in Artificial Intelligence (UAI)}, 2021.

\bibitem{bartoli-delmoral-2001}
N.~Bartoli and P.~del Moral, {\em Simulation et algorithmes stochastiques}.
\newblock Cépaduès éditions, 2001.

\bibitem{gentle-2004}
J.~Gentle, {\em Random Number Generation and Monte Carlo Methods}.
\newblock Springer, 2004.

\bibitem{robert-casella}
C.~Robert and G.~Casella, {\em {M}onte {C}arlo Statistical Methods (Springer
  Texts in Statistics)}.
\newblock Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2005.

\bibitem{Chib1995}
S.~Chib and E.~Greenberg, ``Understanding the {M}etropolis-{H}astings
  algorithm,'' {\em The American Statistician}, vol.~49, no.~4, pp.~327--335,
  1995.

\bibitem{mcbook}
A.~B. Owen, {\em Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem{martino2018independent}
L.~Martino, D.~Luengo, and J.~M{\'\i}guez, {\em Independent random sampling
  methods}.
\newblock Springer, 2018.

\bibitem{hammersley-handscomb1964}
J.~M. Hammersley and D.~C. Handscomb, {\em {M}onte {C}arlo Methods}.
\newblock London: Methuen and Co., 1964.

\bibitem{castella:hal-00830124}
M.~Castella, S.~Rafi, P.~Comon, and W.~Pieczynski, ``{Separation of
  instantaneous mixtures of dependent sources using classical ICA methods},''
  {\em {EURASIP Journal on Advances in Signal Processing}}, p.~n/c, Mar. 2013.

\bibitem{Kahn1953}
H.~Kahn and A.~W. Marshall, ``Methods of reducing sample size in {M}onte
  {C}arlo computations,'' {\em J. of the Op. Res. Soc. of Amer.}, vol.~1,
  no.~5, pp.~263--278, 1953.

\bibitem{marshall1956}
A.~W. Marshall, ``The use of multi-stage sampling schemes in {M}onte {C}arlo
  computations,'' in {\em Symposium on Monte Carlo Methods} (M.~Meyer, ed.),
  (New York), pp.~123--140, 1956.

\bibitem{Geweke1989}
J.~Geweke, ``{B}ayesian inference in econometric models using {M}onte {C}arlo
  integration,'' {\em Econometrica}, vol.~57, no.~6, pp.~1317--39, 1989.

\bibitem{rubin1988}
D.~B. Rubin, ``Using the {SIR} algorithm to simulate posterior distributions,''
  in {\em Bayesian Statistics III} (M.~H. Bernardo, K.~M. Degroot, D.~V.
  Lindley, and A.~F.~M. Smith, eds.), Oxford: Oxford University Press, 1988.

\bibitem{gelfand-smith}
A.~E. Gelfand and A.~F.~M. Smith, ``Sampling based approaches to calculating
  marginal densities,'' {\em J. of the Amer. Stat. Ass.}, vol.~85, no.~410,
  pp.~398--409, 1990.

\bibitem{smith-gelfand}
A.~F.~M. Smith and A.~E. Gelfand, ``{B}ayesian statistics without tears : a
  sampling-resampling perspective,'' {\em The American Statistician}, vol.~46,
  no.~2, pp.~84--87, 1992.

\bibitem{Cappeetal}
O.~Capp{\'e}, {\'E}.~Moulines, and T.~Ryd{\'e}n, {\em Inference in Hidden
  {M}arkov Models}.
\newblock Springer-Verlag, 2005.

\bibitem{bebis1994feed}
G.~Bebis and M.~Georgiopoulos, ``Feed-forward neural networks,'' {\em IEEE
  Potentials}, vol.~13, no.~4, pp.~27--31, 1994.

\bibitem{zhai2016deep}
S.~Zhai, Y.~Cheng, W.~Lu, and Z.~Zhang, ``Deep structured energy based models
  for anomaly detection,'' in {\em Int. Conf. on Machine learning},
  pp.~1100--1109, PMLR, 2016.

\bibitem{carreira2005contrastive}
M.~A. Carreira-Perpinan and G.~Hinton, ``On contrastive divergence learning,''
  in {\em International workshop on artificial intelligence and statistics},
  pp.~33--40, PMLR, 2005.

\bibitem{hinton2012practical}
G.~E. Hinton, ``A practical guide to training restricted boltzmann machines,''
  in {\em Neural Networks: Tricks of the Trade: Second Edition}, pp.~599--619,
  Springer, 2012.

\end{thebibliography}
