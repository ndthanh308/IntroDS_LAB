\documentclass{article}
\usepackage{xr}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{algorithm2e}
\usepackage{xcolor}
\usepackage{tikz} 
\usetikzlibrary{positioning,arrows.meta,quotes,bayesnet,matrix}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{capt-of}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{stackrel}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage[numbers]{natbib}
\newcommand{\paren}[1]{\mathopen{}\left({#1}_{{}_{}}\,\negthickspace\right)\mathclose{}}
\newcommand{\bracket}[1]{\mathopen{}\left[ {#1}_{{}_{}}\,\negthickspace\right]\mathclose{}}
\newcommand{\set}[1]{\mathopen{}\left\{ {#1}_{{}_{}}\,\negthickspace\right\}\mathclose{}}
\newcommand{\abs}[1]{\mathopen{}\left| {#1}_{{}_{}}\,\negthickspace\right|\mathclose{}}
\newcommand{\Var}{\mathbbm{V}\textrm{ar}}
\newcommand{\E}{\mathbbm{E}}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\author{
  Elouan Argouarc'h, Fran\c{c}ois Desbouvries, \\
  SAMOVAR \\
  Télécom SudParis, Institut Polytechnique de Paris \\
   91120 Palaiseau France\\
  \texttt{\{elouan.argouarch, francois.desbouvries\}@telecom-sudparis.eu} \\
}
\begin{document}

\title{Neural classifiers based Monte Carlo simulation}
\maketitle

\begin{abstract}
Acceptance-rejection (AR), 
Independent Metropolis Hastings (IMH)
or importance sampling (IS) Monte Carlo (MC) simulation algorithms 
all involve computing {\sl ratios} of 
probability density functions (pdfs). 
On the other hand, 
classifiers discriminate labellized samples 
produced by a mixture density model,
i.e., a convex linear combination of two pdfs, 
and can thus be used for approximating the ratio of these two densities.
This bridge between simulation
and classification techniques 
enables us to propose (approximate)
pdf-ratios-based simulation algorithms 
which are built only  
from a labellized training data set.
\end{abstract}
\section{Introduction}
\label{intro}

If $a$ and $b$ are two positive numbers,
\begin{eqnarray}\label{fondamental}
r = \frac{a}{a+b} \in (0,1) \Leftrightarrow \frac{r}{1-r} = \frac{a}{b} > 0.
\end{eqnarray}
This humble algebraic identity has interesting consequences in Bayesian classification, machine learning and stochastic simulation.

Indeed, if $a$ and $b$ are probabilities of a sample in a binary mixture context, then ratio $\frac{a}{a+b}$ is the classical formula for the posterior probability. This posterior provides with the class probabilities given a sample, and can be approximated by a parametric classifier 
$r^{\star} \approx r$ 
trained to distinguish between the two probability distributions. On the other hand, 
positive ratios $\frac{a}{b}$ play a key role in 
AR, IMH or 
sampling importance resampling (SIR) techniques.
The r.h.s. of \eqref{fondamental} 
relates $r$ to such positive ratios,
and tells us that {\sl ratio $\frac{a}{b}$ can be computed exactly from $r$,
or, in practice, 
approximately from $r^{\star}$,
without necessarily knowing $a$ nor $b$}.
This simple observation will enable us 
to propose versions 
of these algorithms 
which rely on relaxed weaker hypotheses.

Let $\lambda \in (0,1)$ and $(1-\lambda)$ 
be the prior probabilities of two categories $k=1,0$,
distributed resp. 
$\sim$
$p_1$ and $p_0$.
Binary classification tries to distinguish samples 
from  mixture $\lambda P_1 + (1-\lambda)P_0$ 
by identifying the 
pdf which 
generated them. 
The appropriate way to classify 
relies on the posterior probability: 
$x$ is a sample $\sim$
$P_1$ rather than $\sim$
$P_0$ with probability
\begin{equation}\label{optimal_lambda}
    \mathbbm{p}(k=1|x,\lambda, P_0, P_1) = \frac{\lambda p_1(x)}{\lambda p_1(x) + (1-\lambda)p_0(x)}.
\end{equation}
As is well known 
(see e.g. \cite[Chap. 11]{giraud}), 
assigning a sample to the label with highest posterior probability is the optimal decision rule in the sense that it minimizes the probability of misclassification.

To compute this posterior probability, 
one needs to evaluate the 
pdfs
$p_1,p_0$ associated with $P_1$ and $P_0$ and 
know
the prior probability $\lambda$. 
Unfortunately, 
$\lambda$ is often unknown so \eqref{optimal_lambda} is untractable. If however we dispose of a set 
$\mathcal{D}=\{(x_i, k_i)\}_{i=1}^{N_0 + N_1}$ 
of labelled observations,
$\lambda$ can be estimated by $\frac{N_1}{N_1+N_0}$ where $N_1$ and $N_0$ are respectively the number of samples from $P_1$ and $P_0$. This leads to the (approximate) probability:
\begin{equation}\label{optimal_sans_lambda}
    \mathbbm{p}(k=1|x, \mathcal{D}, P_0, P_1) = \frac{N_1p_1(x)}{N_1p_1(x) + N_0p_0(x)}. 
\end{equation}

However in most cases, 
$p_0$ and $p_1$ are unknown too,
so \eqref{optimal_sans_lambda} cannot be computed either. 
When we only dispose of $\mathcal{D}$, 
we can make use of a parametric classifier
(in this paper we call 
classifier
any parameterized function $r_\phi(x)$
which mimics the unknown posterior pdf).
So let us assume that 
we have at our disposal a function $r_{\phi}$ 
such that
\begin{equation}
\label{approx-classifier1}
    r_{\phi}(x) \approx \frac{N_1p_1(x)}{N_1p_1(x) + N_0p_0(x)}.
\end{equation}
Our paper is based on the observation that 
\eqref{approx-classifier1}
is equivalent to
\begin{equation}
\label{pdf_ratio_approx1}
    \frac{N_0}{N_1}\frac{r_{\phi}(x)}{1-r_{\phi}(x)} 
    \approx
    \frac{p_1(x)}{p_0(x)},
\end{equation}
which implies that (typically neural network based) classifiers can also be used for approximating 
{\sl density ratios}.

Equation 
\eqref{pdf_ratio_approx1}
has already been observed, 
and exploited in contexts where estimating a ratio 
of pdfs is relevant.
First, classifiers are at the core of adversarial training techniques in which divergence measures 
involving a ratio 
are replaced by an approximation based on a 
classifier \citep{nguyen2010estimating}. 
This enables to learn implicit generative models 
(i.e., with untractable pdfs) 
\citep{GAN} \cite{GANOverviewIEEE} \citep{creswell2018denoising}. 
Moreover, classifier based density ratio approximation have been applied to estimation of such metrics  as Mutual Information \citep{belghazi2018mutual}. 
Finally, 
classifiers based ratios have been applied successfully 
in statistical hypothesis testing procedures \citep{gretton2012kernel}, 
which heavily rely on likelihood-ratio tests.

If now $p_0$ is an instrumental distribution, 
then \eqref{pdf_ratio_approx1} can be turned into an (approximately normalized) approximation of target pdf $p_1$. 
So classifiers can be used for 
density estimation,
conditional density estimation, or likelihood-to-evidence ratio estimation, 
making them 
especially relevant 
in a likelihood-free inference setting \citep{durkan2020contrastive}\citep{hermans2020likelihood}\citep{thomas2022likelihood}.

However,
the question of sampling the corresponding model 
remains open,
and this is precisely the point we discuss in this paper. 
We realize that pdf ratios also play a key role in such simulation techniques as
the AR or 
Markov Chain Monte Carlo (MCMC) methods,
in which an acceptance probability needs to be computed, 
or in importance sampling,
which
corrects samples from a surrogate distribution
by a weight function which is also a ratio of two densities.
This establishes a connection between
classification and MC sampling,
and will enable us to relax the assumptions underlying those sampling algorithms, 
at the price of approximate sampling.

The rest of this paper is organized as follows.
In \S
\ref{tutorial-simu}
we recall classical ratio-based stochastic simulation algorithms, i.e. 
the AR, IMH and IS techniques.
In \S \ref{standard_binary_classif}
we show that classifiers computed via the Binary Cross Entropy (BCE) criterion
indeed provide with a suitable approximation of the posterior
\eqref{optimal_sans_lambda}.
Finally in \S \ref{connection simu-classifier}
we propose classification based sampling methods,
and illustrate our method via simulations. 
We end the paper with a conclusion.

\section{Classical ratio-based 
sampling algorithms}
\label{tutorial-simu}

Stochastic simulation includes 
a variety of techniques,
see e.g.
\cite{bartoli-delmoral-2001}-\nocite{gentle-2004}\nocite{robert-casella}\nocite{Chib1995}\nocite{owen2013monte}\cite{hammersley-handscomb1964}.
In this section we focus on
AR, IMH and IS which share in common 
that they all compute a ratio of pdfs. 

\subsection{The AR algorithm}
\label{classical-AR}

\subsubsection{A brief reminder of AR Sampling}\label{rejection_sampling}

AR Sampling 
\cite[chap. 2]{robert-casella}
is a 
simulation algorithm 
that yields samples distributed according to a target distribution $P$ 
via samples from a proposal distribution $Q$,
which are accepted or rejected as valid samples from $P$ 
via some acceptance probability.
More precisely, 
let the support of $P$ be inside that of $Q$. 
This means that there exists a constant $C\geq1$ 
s.t. for all $y\in \mathbbm{R}^d, p(y) \leq Cq(y)$. Let $Y \sim Q$, and let $k$ a Bernoulli random variable (r.v.) with parameter $\alpha(y) = \frac{p(y)}{Cq(y)}$. 
AR sampling is based on the fact that $Y|k=1$ is distributed according to $P$.

Note that $\mathbbm{p}(k=1) = \frac{1}{C}$, 
so the lower the value of $C$, the higher the acceptance rate.

In order to use the algorithm in practice,
we thus need to know pdf $p$, 
and
build $Q$ s.t. one can sample easily from $Q$, pdf $q$ is available, 
there exists $C$ such that $p(y) \leq Cq(y)$ for all $y$, we can compute one such value of $C$,
and $C$ is as small as possible.
Note finally that
the algorithm can easily be relaxed 
to the cases where $p$ and/or $q$
are known up to a (non necessarily common) constant,
see e.g. \cite[Th. 4.5]{owen2013monte}.

\subsubsection{Revisiting AR sampling 
as optimal binary classification}
\label{AR-classifier}

As we shall now see, 
AR sampling is indeed nothing but a binary classification procedure (see also \citep[\S 6]{castella:hal-00830124} for an application of this principle in the context of a mixture model for blind source separation).
Starting from the target pdf $p(x)$,
we find an easy-to-sample distribution $Q$ 
and constant $C>1$
s.t. $C q(x)$ envelopes $p(x)$.
Since 
$C q(x) - p(x)$ 
is non negative,
we write $C q(x)$ as $p(x)$ 
plus a positive reminder %$C q(x) - p(x)$
which,
up to a constant, is also a pdf;
so enveloping $p(x)$ with $C q(x)$
(or indeed $\frac{1}{C} p(x)$ with $q(x)$)
is nothing but building the implicit binary mixture distribution (see also figure \ref{fig:AR enveloppe} below) 
\begin{equation}
\label{AR-as-a-mixture}
\underbrace{q(x)}_{\rm proposal}= \frac{1}{C} \; \underbrace{p(x)}_{\rm target} 
\;+\; 
(1-\frac{1}{C})\;
\underbrace{
\frac{q(x)-\frac{1}{C}p(x)}{1-\frac{1}{C}}
}_{\rm reminder}\;
\end{equation}
with a priori classes probabilities $\frac{1}{C}$ and $1 - \frac{1}{C}$.
The first component of the mixture (that of interest)
is the target density $p$, 
and the second one is the law of the rejected samples.
% Figure environment removed
The magic of the AR algorithm consists 
in drawing samples 
from mixture  $Q$
{\sl without} needing to sample from its two components
(see the r.h.s. of 
\eqref{AR-as-a-mixture}).
Accepting (or rejecting) a sample 
depending on the ratio probability 
\begin{equation}
\label{eqratio}
\alpha(x) = \frac{p(x)}{C q(x)} =
\frac
{\frac{1}{C} p(x)}
{
\frac{1}{C} p(x)
+
(1-\frac{1}{C}) \frac{q(x)-\frac{1}{C}p(x)}{1-\frac{1}{C}}
}
\end{equation}
then amounts to classifying the samples 
with the (optimal \cite[Chap. 11]{giraud}) posterior pdf
(compare 
\eqref{eqratio} to 
\eqref{optimal_lambda}).

\subsection{IMH}\label{IMH}

MCMC algorithms 
build a Markov chain whose invariant distribution is  
the target distribution $P$; 
so simulating the chain 
yields samples asymptotically distributed 
$\sim$ $P$. 
The Metropolis-Hastings (MH) algorithm
\cite{robert-casella}
\cite{Chib1995}
is a particular MCMC method 
which constructs the Markov Chain $x_t$ as a two-step procedure: 
given a current state 
$x_t$, the algorithm draws 
a candidate $x^*$ from a proposal distribution $q(.|x_t)$, 
and then calculates the acceptance probability $\alpha(x^{*}, x_t)=\min(1, \frac{p(x^*)q(x_t|x^*)}{p(x_t)q(x^*|x_t)})$. 
$x^*$ is accepted as the new state $x_{t+1}$ with probability $\alpha(x^{*}, x_t)$; 
if $x^*$ is rejected then the chain remains in the current state $x_t$.
In practice, 
$q(.|x_t)$ plays a crucial role in the performance of the MH algorithm: 
if 
not well-tuned, the acceptance rate may be too low, leading to slow mixing of the chain, or too high, leading to poor exploration of the target distribution.

The IMH algorithm is a simplified version of MH which considers an independent transition. The new point $x^*$ is hence proposed independently of the current state $x_t$, according to an independent proposal $q(.)$. 
In this case, the acceptance probability simplifies to $\alpha(x^{*}, x_t)=\min(1, \frac{p(x^*)q(x_t)}{p(x_t)q(x^*)})$.

\subsection{IS}
\label{reminder-is}

In many signal processing problems
we want to compute 
the expectation of some function $f$ with respect to density $p$:
$\mu = \int f(x)p(x){\rm d}x = \E_{P}\bracket{f(x)}. 
$
In practice %\eqref{eq:mu}
$\mu$
can be very difficult to compute,
so one often needs to resort to approximations.
IS is a variance reduction technique
for integral MC estimates 
which can be traced back to the 1950's
\cite{Kahn1953}
\cite{marshall1956}
\cite[\S 5.4]{hammersley-handscomb1964}. 

The crude MC estimate of $\mu$
reads
$\hat{\mu}^{\rm MC} = \frac{1}{N} \sum_{i=1}^N f(x_i)$ with 
$x_i \stackrel{\rm iid}{\sim} P$.
However it is generally difficult 
to sample directly from $P$,
moreover 
$\hat{\mu}^{\rm MC}$ can 
be a poor estimate,
particularly when the regions where $p$ is large 
do not coincide with those where $f$ is large.
Rewriting $\mu$ 
as
$
\mu = \int f(x) \frac{p(x)}{q(x)} q(x){\rm d}x,
$
where $Q$ is some importance distribution, 
leads to the IS estimator 
$
\hat{\mu}^{\rm IS}(q) =
\frac{1}{N} \sum_{i=1}^N \frac{p(x_i)}{q(x_i)} f(x_i), \,\,\, 
x_i \stackrel{\rm iid.}{\sim} Q.
$
As far as variance reduction is concerned,
one can easily show that
the importance pdf  
which minimizes 
$\Var(\hat{\mu}^{\rm IS}(q))$
is 
$q_{\rm opt}^{\rm IS}(x) \propto |f(x)| p(x)$.
Even if in practice 
$\hat{\mu}^{\rm IS}(q_{\rm opt}^{\rm IS})$ cannot be computed,
this tells us that the regions where it is important to sample from
(whence the term "{\sl importance} distribution")
are not those where $p$ is large, 
but rather those
where $|f|p$ is large.
Note that 
$\hat{\mu}^{\rm IS}$
can be computed only if $p$ and $q$ are known exactly, 
or
known up to a common constant; if this is not the case one can resort to self-normalized IS \cite{Geweke1989}.

Besides being a variance reduction technique, IS can also be seen as a two step sampling procedure for producing samples 
(approximatively) drawn from $p$,
out of samples originally drawn from $q$.
The technique is known as Rubin's
%sampling importance resampling (SIR)
SIR mechanism
\cite{rubin1988},
\cite{gelfand-smith},
\cite{smith-gelfand},
\cite[\S 9.2]{Cappeetal}:
Let
$\{ x_i \}_{i=1}^N$ 
be $N$ iid. samples from $q(x)$,
and given $\{x_i\}_{i=1}^N$,
let $\{ \tilde{x}^i \}_{i=1}^M$ be $M$
iid. samples from 
$
\sum_{i=1}^N
\frac
{
p(x_i)/q(x_i)
}
{
\sum_{i=1}^N 
p(x_i)/q(x_i)
}
\delta_{x_i}({\rm d}x)$
(in other words, we draw samples from $q$,
weight each of them with weight $w_i$ proportional to $p(x_i)/q(x_i)$,
and {\sl resample} $M$ iid. points from this random discrete mass probability function).
Then $\{ \tilde{x}^i \}_{i=1}^M$ are dependent and are not $p$-distributed,
but become iid. samples from 
$p({x})$
if $N \rightarrow \infty$.

\section{Parametric classifier by minimizing the BCE}
\label{standard_binary_classif}

From now on
we consider the setting where $\lambda$, $p_1$ and $p_0$
are  unknown,
and we only have a set 
${\cal D}$ of samples from
$P_0$ and $P_1$,
see \S \ref{intro}.
In this context, 
we should build a parametric function $r_{\phi}(x)$
that approximates the posterior pdf 
from the recorded samples. 
The aim of this section is to show that minimizing a BCE criterion indeed yields such a suitable approximation,
since the BCE, up to constants, is nothing but an MC approximation of a Kullback-Leibler Divergence ($D_{\mathrm{KL}}$) between the classifier and the unavailable posterior pdf.

To see this,
let us first recall the BCE criterion:
\begin{equation}\label{BCE}
    \mathcal{L}_{\mathrm{BCE}}(\phi) = -\sum_{i=1}^{N_1} \log(r_\phi(x^{(1)}_i)) -  \sum_{i=1}^{N_0} \log(1 - r_\phi(x^{(0)}_i)),
\end{equation}
where $r_\phi(x) = \mathbbm{p}_\phi(k=1|x)$ is the probability (under model $\phi$)
that the label associated to an observation $x$ is $1$. 
Let $h(x,k)$ be the joint distribution over observations and labels: 
\begin{align}
    \label{joint}
    &h(x,k)  \!=\! \underbrace{\frac{N_1^k N_0^{1-k}}{N_1+N_0}}_{h(k)}\underbrace{p_1(x)^kp_0(x)^{1-k}}_{h(x|k)}, x\in\mathbbm{R}^d, k=0,1.
\end{align}
On the other hand, using $r_\phi(x) \in \bracket{0,1}$, 
we construct another joint probability distribution 
$
h_\phi(x,k) 
 = h(x) r_{\phi}(x)^{k}(1-r_{\phi}(x))^{1-k}
$,
 where $h(x)$ is the $x$-marginal in \eqref{joint}. 
 As the name Cross-Entropy suggests, the BCE loss is, up to additive and multiplicative constants, nothing but an MC approximation of $D_\mathrm{KL}\paren{h(x,k)||h_\phi(x,k)} = \E_{h(x)}(D_\mathrm{KL}(h(k|x)||r_{\phi}(x)^{k}(1-r_{\phi}(x))^{1-k})$, 
see appendix.

The interest of this interpretation 
is that,
as is well known, 
a $D_{\mathrm{KL}}$ reaches zero if and only if the two joint distributions are equal almost surely.
So minimizing 
$D_\mathrm{KL}\paren{h(x,k)||h_\phi(x,k)}$
(over an arbitrary function
$r_{\phi}$)
would ensure that $\mathbbm{p}_\phi(k|x) = h(k|x)$ for all $x\in\mathbbm{R}^d$ and for $k=1,0$,
i.e. that the classifier reaches the target posterior pdf.

Of course, in practice, 
minimizing the BCE does not ensure that
this  $D_\mathrm{KL}$ decreases to zero. 
First, since we only dispose of a finite number of observations and associated labels, minimizing an MC approximation of the $D_{\mathrm{KL}}$ does not minimize the $D_{\mathrm{KL}}$ itself.
Next, the parametric family of classifiers $\set{r_\phi|\phi\in\Phi}$ may not contain the optimal $r^{*}$, 
and therefore we may only reach a positive minimum of the $D_{\mathrm{KL}}$. 
Lastly, even if the family of classifiers contained $r^{*}$, standard optimization techniques would only guarantee convergence to a local minimum of the $D_{\mathrm{KL}}$. 
Finally  in practice,
minimizing the BCE loss only provides with $\phi$ such that $r_{\phi}$ 
satisfies
\eqref{approx-classifier1},
and thus approximates the unknown $r^{*}$.

\section{Using a binary classifier for (approximate) Sampling}\label{parametric_approx}
\label{connection simu-classifier}

We now come to the heart of this paper.
If $P_1$ is a distribution of interest in 
an MC sampling setting, and $P_0$ a suitable 
easy-to sample instrumental distribution
- be it the proposal distribution in AR, the independent Markov transition Kernel in IMH, or the importance distribution in IS, then the three sampling algorithms involve the density ratio $\frac{p_1(x)}{p_0(x)}$. 
As explained in section
\ref{standard_binary_classif},
a parametric binary classifier
trained from a set of recorded labeled observations 
computes an approximation 
of the unknown posterior distribution.
However,
remember that 
\eqref{approx-classifier1}
is equivalent to
\eqref{pdf_ratio_approx1};
we thus see that classifiers can also be used for approximating {\sl density ratios} of interest,
which enables us to propose
approximate versions of the sampling algorithms based on this classifier-ratio approximation,
and thus to relax the tractable densities requirement, but at the cost of approximate sampling.
Of course, the closer $P_0$ is to $P_1$, the more efficient the algorithms will be. However, here $P_0$ is supposed to be given and hence, our problem is not (as usual) to adjust $P_0$ from a given $P_1$,
but to make the most of dataset $\mathcal{D}$ 
for fixed $P_0$  and $P_1$.

\begin{flushleft}
{\sl Assumptions.}
\end{flushleft}
$P_1$ is the distribution of interest
and $P_0$ a fixed instrumental distribution. 
We neither know $p_1(x)$ nor $p_0(x)$,
even up to a constant,
but we dispose of a dataset 
$\mathcal{D}$ 
made of samples 
$x^{(0)}_1,..., x^{(0)}_{N_0} \sim P_0$ and $x^{(1)}_1,..., x^{(1)}_{N_1} \sim P_1$,
and assume 
that we can train a binary classification model $r_{\phi}$ which minimizes \eqref{BCE}.

\subsection{A classifier-based AR algorithm}

Remember from \S \ref{rejection_sampling}
that the AR algorithm 
is all the more efficient that $\frac{1}{C}P_1$ is close to $P_0$,
and constant $C$ (which is $\geq 1$) is as small as possible.

Indeed the minimum value of admissible $C$ is 
$C_{\rm min} = \sup_{y\in\mathbbm{R}^d}\frac{p_1(y)}{p_0(y)}$.
Now, a  key ingredient 
in the algorithm is the acceptance probability
$
\alpha(x) = \frac{1}{C} \times \frac{p_1(x)}{p_0(x)} ,
$
but in our setting $p_1(x)$ and $p_0(x)$ are unavailable,
so we cannot compute neither ratio $\frac{p_1(x)}{p_0(x)}$,
nor its supremum $C_{\rm min}$.

Following the idea expressed 
in 
\eqref{pdf_ratio_approx1},
we can however make use of a classifier 
for approximating ratio 
$\frac{p_1(x)}{p_0(x)}$, 
and approximate its supremum over an uncountable set by using the data at hand:
\begin{equation}\label{ar_acceptance_prob_approx}
    \alpha(x) \leftarrow 
    \alpha_\phi(x)
\end{equation}
\begin{equation}
    \alpha_\phi(x) =
    \frac{1}{\widetilde{C}} 
    \frac{r_{\phi}(x)}{1-r_{\phi}(x)} \text{ where }
    \widetilde{C} = \max_{y\in \mathcal{D}}
    %\set{x,x^{(0)}_1,..., x^{(0)}_{N_0},x^{(1)}_1,..., x^{(1)}_{N_1}}}
    \frac{r_{\phi}(y)}{1-r_{\phi}(y)}.
\end{equation}
The supremum is only defined 
on the finite set of samples in $\mathcal{D}$ 
and is therefore easy to compute.
Note that the approximation $\widetilde{C}$ can also be updated effortlessly
each time we propose a new sample from $P_0$. 

Finally,
if moreover proposal $P_0$ 
has a tractable pdf $p_0(x)$, 
then the algorithm provides
as a byproduct
an estimate $p_\phi$ 
(up to a normalizing constant)
of the pdf of the accepted samples:
\begin{equation}\label{p_phi}
    p_\phi(x) = \frac{p_0(x)\alpha_\phi(x) }{\int_{\mathbbm{R}^d} p_0(z)\alpha_\phi(z)\mathrm{d}z} = \frac{p_0(x)\frac{r_{\phi}(x)}{1-r_{\phi}(x)} }{\int_{\mathbbm{R}^d} p_0(z)\frac{r_{\phi}(z)}{1-r_{\phi}(z)}\mathrm{d}z}.
\end{equation}
If required by the application, 
the normalizing constant
can be estimated via MC, i.e.
by replacing the denominator with
$\frac{1}{M}\sum_{i=1}^M \frac{r_{\phi}(z_i)}{1-r_{\phi}(z_i)}, z_i\sim P_0$.

\subsection{A classifier based IMH algorithm}

Remember from section \ref{IMH} that the IMH algorithm requires 
computing the acceptance probability 
\begin{equation}
    \alpha(x^{*}, x_t)=\min\paren{1, \frac{p_1(x^*)/p_0(x^*)}{p_1(x_t)/p_0(x_t)}},
\end{equation}
but in our setting we cannot compute $\alpha(x^{*}, x_t)$ since $p_1$ and $p_0$ are unavailable. 
However, using 
\eqref{pdf_ratio_approx1}
we can make use of the classifier in order to compute an approximated acceptance probability:
\begin{align}\label{imh_acceptance_prob_approx}
    \alpha(x^*, x_t) & \leftarrow 
    \alpha_\phi(x^*, x_t), \\
    \alpha_\phi(x^*, x_t) &= \min\paren{1, \frac{r_\phi(x^*)}{(1-r_\phi(x^*))}\frac{(1-r_\phi(x_t))}{r_\phi(x_t)}}.
\end{align}

\subsection{A classifier-based IS algorithm}
\label{classifier-IS}

Remember from section \ref{reminder-is}
that SIR relies on the normalized importance weights 
$
w(x^{(0)}_i) = 
w^{\rm u}(x^{(0)}_i) /
(\sum_{j=1}^N w^{\rm u}(x^{(0)}_j)),
$
where $\{x^{(0)}_j\}_{j=1}^N$ are samples from the importance pdf $P_0$, 
and in which the unnormalized weights read
$p_1(x^{(0)}_i)/p_0(x^{(0)}_i)$. 
Here we cannot compute 
$w^{\rm u}(x^{(0)}_i)$ since 
$p_1$ and $p_0$ are unavailable.
However, using 
\eqref{pdf_ratio_approx1}
we can make use of
the classifier in order to compute an approximated unormalized weight:
\begin{align}
\label{importance_weight_approx}
    w^{\rm u}(x^{(0)}_i)
    \leftarrow w^{\rm u}_\phi(x^{(0)}_i) 
    \; \text{where} 
    \; w^{\rm u}_\phi(x^{(0)}_i) =
    \frac
    {r_\phi(x^{(0)}_i)}{1-r_\phi(x^{(0)}_i)}.
\end{align}

\subsection{Illustrating examples} 
% Figure environment removed

We illustrate our approach (see fig. \ref{illustration} below) on reference 2D examples in order to illustrate the mechanism of (i) obtaining an approximate of the density ratio from samples, and (ii) sampling from the target distribution via that density ratio. 
The instrumental $Q$ was set to be Gaussian with mean and covariance estimated from the samples from $P$ (even though it can be computed, pdf $q$ was not used during the procedure). 
Notice that the classifier based sampler is indeed compatible with multimodal data, 
so long as $Q$ covers the different regions of mass of $P$.

\section{Conclusion}
In this paper we showed that 
making use of (typically neural network based) classifiers 
enables to relax the assumptions 
underlying some classical,
pdfs-ratio-based
MC samplers.
The main interest of 
the versions of the AR, IMH and SIR 
samplers which we propose 
is that they only require 
a labellized training data set;
in particular,
the knowledge of the two pdfs
(possibly up to a constant)
involved in these samplers 
is no longer necessary.
On the other hand,
our algorithms produce samples which are only approximately drawn 
from the target density.
Our methodology is confirmed by simulations.

\appendix
\vspace{-15pt}
\begin{eqnarray}
\nonumber
D_\mathrm{KL}(h(x,k)||h_\phi(x,k))
&\!\!\!=\!\!\!&
-\mathbbm{H}[(x,k)]
-\E_{h(\!k,x)}[\log\paren{h(x)}] \\
\label{lastterm}
&&
-\E_{h(k,x)}[\log\paren{\mathbbm{p}_\phi(k|x)}],
\end{eqnarray}
in which 
$\mathbbm{H}[(x,k)] = \E_{h(x,k)}[-\log(h(x,k))]$ is the entropy of the joint r.v. $(x,k)$. 
In \eqref{lastterm}, only the last term depends on 
$\phi$. 
We get the BCE loss by computing an MC approximation of that last term 
(or, equivalently, 
replacing the expectation with one computed on the empirical joint distribution):
\vspace{-5pt}
\begin{align*}
    &\E_{h(k,x)}[\log(\mathbbm{p}_\phi(k|x))] \!=\! \sum_{k = 0}^1
    h(k)
    \!\!\!
    \int_{\mathbbm{R}^d}
    \!\!\!\!\!\!
    \log(\mathbbm{p}_\phi(k|x))h(x|k) \mathrm{d}x \\
    \stackrel{\eqref{joint}}{=} 
    &\sum_{k = 0}^1\frac{N_1^kN_0^{1-k}}{N_1+N_0}\int_{\mathbbm{R}^d}\log(\mathbbm{p}_\phi(k|x))h(x|k) \mathrm{d}x \\
    \approx & 
    \sum_{k = 0}^1\frac{N_1^kN_0^{1-k}}{N_1+N_0} [\frac{1}{N_k} \sum_{i=1}^{N_k}
    \log(\mathbbm{p}_\phi(k|x_i^{(k)}))]\\
    = & 
    \frac{1}{N_1+N_0}
    (\sum_{i=1}^{N_1}\log(r_\phi(x^{(1)}_i)) +  \sum_{i=1}^{N_0}\log(1- r_\phi(x^{(0)}_i))) \\
    = & -\frac{1}{N_1+N_0}\mathcal{L}_{\mathrm{BCE}}(\phi).
\end{align*}
So
$
D_\mathrm{KL}\paren{h(x,k)||h_\phi(x,k)}\approx
A +
\frac{1}{N_1+N_0}\mathcal{L}_{\mathrm{BCE}}(\phi),
$
in which $A$
does not depend on  $\phi$,
and consequently
$
\arg\min_{\phi} D_\mathrm{KL}\paren{h(x,k)||h_\phi(x,k)} 
$
$\approx$
$\arg\min_{\phi}\mathcal{L}_{\mathrm{BCE}}(\phi).
$

\bibliographystyle{ieeetr}
\bibliography{bibliography}

\end{document}



