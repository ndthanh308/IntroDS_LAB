In this section, we provide the details of the experiment's setup, the results, and the discussion. We describe how the features introduced in Section \ref{sec:method} are combined and selected to achieve the best performance, as well as the evaluation metrics used including confusion matrices, precision, recall, and accuracy.

% The simulation results are discussed in this section. The explanation and discussion are composed of 2 parts.  The first part is to explain how these extracted features should be combined to achieve the best performance. The second part is to discuss the performance of the following metrics: confusion matrices, precision, recall and F-score.
\subsection{Experiment Setup}
We use 5-fold cross validation to quantitatively evaluate the proposed methods. For each subset of the data, we select $1/5$ for testing, and the rest for training. We report the performance by calculating the mean and variance accuracy among 100 runs of the 5-fold cross validation. Note that the augmented data are only used in training.
%\textcolor{red}{You need to describe how the experiments are set up. How the datasets are splitted for training, testing (and validation if any), did you do cross-validation, etc.}

\begin{table}[t]
    \caption{Mean and Variance for 5-Fold Average Classification Accuracy}
    \centering
    \begin{tabular}{|c|c|c|c|} 
        \hline 
        & Mean Accuracy & Variance Accuracy  \\ \hline
        Original & 90.82\% & 0.0012\% \\ \hline
        Augmented & 91.46\% & 0.0004\% \\ \hline
    \end{tabular}
    \label{tab:Accuracy}
\end{table}

\begin{table}[t]
\centering
\small
\caption{Precision, Recall and F1-Scores}
\begin{tabular}{|c|c|c|c|c|}           \hline 
\multirow{1}{*}{Setting} &\multirow{1}{*}{Class Type}    & {Precision} & {Recall} & {F1-Score}               \\   \hline 
\multirow{2}{*}{Original} & Acceptable& 0.92 & 0.973 & 0.944   \\   \cline{2-5}  
 & Unacceptable & 0.82 & 0.593 & 0.688           \\   \hline 
\multirow{2}{*}{Augmented} & Acceptable& 0.982 & 0.907 & 0.943   \\   \cline{2-5}  
 & Unacceptable & 0.67 & 0.92 & 0.775          \\   \hline 
% \multicolumn{2}{|c|}{Average}   & 76.42 & 66.52 &69.8 \\ \hline 
 \end{tabular}
 \label{tab:PR}
 \end{table}
\subsection{Classification Results}
Table \ref{tab:Accuracy} shows the results of classifying raster images in the scanned document as acceptable or unacceptable with and without augmented data in training. We observe that including the augmented data in training slightly improves the overall accuracy. However, when we take a closer look at the two settings as shown in Table \ref{tab:PR}, we noticed that the recall for unacceptable class under the original data setting is much lower, \textit{i.e.}, 0.593, compared to \textit{i.e.}, 0.92 when augmented data is included in the training. This is important since our goal is to determine whether or not a low resolution setting can be used for scanning the document. If we wrongly predict an image with unacceptable resolution as acceptable, the image would be stored at a resolution that is too low to be usable. On the other hand, if we predict an image with a relatively high resolution as ``Unacceptable", we may select a higher resolution and store the image at an increased file size. However, the image is still usable. 
% with data augmentation, the prediction for the ``Unacceptable"  images is improved significantly for the true negative rate, which increased from 0.593 to 0.922 when using the augmented dataset for training. 
The trade-off is that the true positive rate (recall for acceptable) dropped from 0.973 to 0.907 when augmented data is used. This implies that the model trained with augmented data tends to predict more images as unacceptable, including those images that were previously predicted as acceptable without the augmented data. 


% We use standard metrics such as \textit{precision}, \textit{recall}, and \textit{accuracy} to evaluate the performance of our proposed method to classify raster images in the scanned document as acceptable or unacceptable for a given resolution. 
% Note that all 9 features are used here for fair comparison. 
% % \subsubsection{Original Dataset vs. Augmented Dataset}
% % For the performance metrics under the original dataset, the SVM classifier is only trained on the original dataset. The average accuracy is reported in Table \ref{tab:Accuracy}.  
% When we take a closer 
% To better understand the effect of including augmented data in training, 

% From Table \ref{tab:PR}, is shows that the precision and recall are 0.92 and 0.97, respectively. However, the true negative rate is much lower, \textit{i.e.}, 0.593. The poor performance is largely due to the data imbalance as more images are rated as ``Acceptable" compared to ``Unacceptable", as discussed in Section \ref{sec:dataanaysis}. Since our goal is to determine whether or not a low resolution setting can be used for scanned documents, true negative rate is more important. If we wrongly predict an image with unacceptable resolution as acceptable, the image would be stored at a resolution that is too low to be usable. On the other hand, if we predict an image with a relatively high resolution as ``Unacceptable", we may select a higher resolution and store the image at an increased file size. However, the image is still usable.
\begin{table}[t]
    \caption{Normalized Confusion Matrix for the Original Dataset and with augmented data training (in parentheses).}
    \centering
    \begin{tabular}{@{}cc cc@{}}
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted} \\ 
        %\cmidrule(lr){3-4}
        \multicolumn{1}{c}{} & 
        \multicolumn{1}{c}{} & 
        \multicolumn{1}{c}{Acceptable} & 
        \multicolumn{1}{c}{Unacceptable} \\ 
        \cline{2-4}
        \multirow[c]{2}{*}{\rotatebox[origin=tr]{90}{Actual}}
        & Acceptable  & 0.973 (0.907) & 0.027 (0.093)  \\[1.5ex]
        & Unacceptable  & 0.407 (0.078)  & 0.593 (0.922) \\ 
        \cline{2-4}
    \end{tabular}
    \label{tab:Original_Conf_Matrix}
\end{table}

% \begin{table}[t]
%     \caption{Normalized Confusion Matrix for the Augmented Dataset.}
%     \centering
%     \begin{tabular}{@{}cc cc@{}}
%         \multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted} \\ 
%         %\cmidrule(lr){3-4}
%         \multicolumn{1}{c}{} & 
%         \multicolumn{1}{c}{} & 
%         \multicolumn{1}{c}{Acceptable} & 
%         \multicolumn{1}{c}{Unacceptable} \\ 
%         \cline{2-4}
%         \multirow[c]{2}{*}{\rotatebox[origin=tr]{90}{Actual}}
%         & Acceptable  & 0.907 & 0.093   \\[1.5ex]
%         & Unacceptable  & 0.078   & 0.922 \\ 
%         \cline{2-4}
%     \end{tabular}
%     \label{tab:Augmented_Conf_Matrix}
% \end{table}
%\begin{table}[t]
%    \centering
%    \begin{tabular}{|c|c|c|c|} 
%        \hline 
%        & Precision & Recall & F-Score\\ \hline
%        Acceptable & 0.92 & 0.97 & 0.94\\ \hline
%        Unacceptable & 0.81 & 0.58 & 0.68\\ \hline
%    \end{tabular}
%    \caption{Precision, Recall and $F_1$-score for Original Dataset}
%    \label{tab:Ori_perform}
%\end{table}

% As shown in Table \ref{tab:Original_Conf_Matrix}, \ref{tab:Augmented_Conf_Matrix}, and \ref{tab:PR}, 



%\begin{table}[t]
%\centering
%    \begin{tabular}{|c|c|c|c|} 
%        \hline 
%        & Precision & Recall & F-Score\\ \hline
%        Acceptable & 0.97 & 0.91 & 0.94\\ \hline
%        Unacceptable & 0.66 & 0.89 & 0.76\\ \hline
%    \end{tabular}
%    \caption{Precision, Recall and $F_1$-score for Augmented Dataset}
%    \label{tab:Aug_perform}
%\end{table}

% Under the augmented dataset, the precision and recall can be found in Table \ref{tab:Aug_perform}.  Considering ``Acceptable’’ as positive, we have precision and recall equal to 0.90 and 0.93 respectively.  The performance is good although they are a little bit worse (very subtle difference) than the ones of original dataset.  However, when considering ``Unacceptable’’ as positive, the precision and recall are 0.68 and 0.89 respectively.  As we discussed in the above, the recall under ``unacceptable – positive’’ condition is a more important parameter.  The recall rate improved from 0.58 to 0.89 after adding augmentation data. This improvement shows that the data augmentation approach provides help training a better SVM model for the document image quality assessment for scanned raster images.

% Finally, let's compare the difference between confusion matrices under both original and augmented datasets shown in Figure \ref{fig:conf}.  We can easily see that the original dataset has a data imbalance problem and the True Negative rate is too low for a meaningful application. and after data augmentation, we can easily see that True Negative rate is comparable with True Positive rate.  An improvement on True Negative rate is achieved.
\subsection{Feature Combinations}
% In this section, we discuss how the features are combined to optimize classification accuracy.  
As described in Section \ref{sec:method}, we use the sequential floating forward selection (SFFS) method to determine the set of most relevant features. Two different experiments are performed to determine the best combination. We first search the optimal combination of the features based on the original dataset, we then apply the same method to the augmented dataset.  

\subsubsection{Original Dataset vs. Augmented Dataset}
Table \ref{tab:feature_importance} shows the rankings of importance for the features using the SFFS method \cite{SFFS} on the original and augmented datasets in decreasing order. Suppose we use Table \ref{tab:feature_importance} to illustrate the different feature combinations. If we can only select one feature, then the Differential Spatial Activity (DSA) would be the choice because it gives the best performance with just a single feature.  The remaining features should be selected based on the order of their importance.  

% If we can select four features as the feature vectors for the classifier, then a combination of DSA, standard deviation of power spectrum difference, mean of power spectrum difference and standard deviation of edge density should be selected.

When comparing the difference between original dataset feature rankings and that with augmented dataset in Table \ref{tab:feature_importance} , we observe that edge related features, \textit{i.e.}, Differential Spatial Activity, Mean and StdDev of Edge Density, ranked lower when the augmented dataset is used, while the MSE-related features become more important. Such difference is likely contributed by the added noisy images in the augmented dataset, which leads to increased importance of the residual error features, especially those related to MSE. Also, by adding noise to an image, the edges become less sharp, causing the decrease in rankings for edge-related features.  

% the noisy images in the augmented dataset added to balance the dataset, especially the additive noise degraded dataset, such as the Gaussian noise and salt\& pepper noise, which is directly adding noise to the images. This will emphasize the importance of the residual error features, especially the MSE-related features. Also, since adding noise to an image will make the sharp edges not obvious, this causes the decrease in rankings for edge-related features.  
\begin{table}[!t]
\small
    \caption{Ranking of the Features According to Their Importance For the Original Dataset in Decreasing Order.}
    \centering
    \begin{tabular}{|c|c|c|c|} 
        \hline 
        Feature Name& Category & Original & Aug \\ \hline
        Differential Spatial Activity& Edge &1 &2\\ \hline
        StdDev of PSD& Residual Error & 2 &5\\ \hline
        Mean of PSD& Residual Error &3&7\\ \hline
        StdDev of Edge Density& Edge &4&8\\ \hline
        Mean of Edge Density& Edge &5&9\\ \hline
        StdDev of Tile-SSIM& Image Structure & 6&6\\ \hline
        StdDev of MSE& Residual Error & 7&3\\ \hline
        Mean of Tile-SSIM& Image Structure & 8&4\\ \hline
        Mean of MSE& Residual Error & 9&1\\ \hline
    \end{tabular}
    \label{tab:feature_importance}
\end{table}

%From the ranking of the importance of these features, we observe that edge related features are the most important (average ranking 3.3), \textit{i.e.}, Differential Spatial Activity, Mean and StdDev of Edge Density. The next group includes residual error related features (average ranking 5) such as Mean and StdDev of Power Spectrum Density (PSD) and Mean Square Error. Lastly are the image structure related features (average ranking 7.5), \textit{i.e.}, Mean and StdDev of Tile-SSIM. 

% Under the original dataset, if the top index is used to determine which category is more important, then the answer is that the edge related features are the most important, then the residual error related features are the second important and the image structure related features are the third.  Please note that the ranking of the importance does not imply that the features ranked at the third place should be dropped. It just indicates their relative importance. These features of these 3 categories are all required for achieving a good result.

% Table \ref{tab:feature_importance_aug} shows the ranking of importance for the features using the SFFS method on the augmented dataset in decreasing order.

% \begin{table}[H]
% \caption{Ranking of the Features According to Their Importance Using the Augmented Dataset in Decreasing Order.}
% \centering
%     \begin{tabular}{|c|c|c|} 
%         \hline 
%         Rank& Feature Name& Category\\ \hline
%         1& Mean of MSE& Residual Error \\ \hline
%         2& Differential Spatial Activity& Edge\\ \hline
%         3& StdDev of MSE& Residual Error\\ \hline
%         4& Mean of Tile-SSIM& Image Structure\\ \hline
%         5& StdDev of Power Spectrum Difference& Residual Error\\ \hline
%         6& StdDev of Tile-SSIM& Image Structure\\ \hline
%         7& Mean of Power Spectrum Difference& Residual Error\\ \hline
%         8& StdDev of Edge Density& Edge\\ \hline
%         9& Mean of Edge Density& Edge\\ \hline
%     \end{tabular}
%     \label{tab:feature_importance_aug}
% \end{table}

%Similarly, under augmented dataset, Table \ref{tab:feature_importance_aug} shows the ranking of importance for the features using the SFFS method on the augmented dataset in decreasing order. From the ranking order of these feature, we observed that after adding augmented data into the training process, residual features are the most important (average ranking 3.5). The next group includes the image structure related features (average ranking 6). Lastly are the edge related features (average ranking 6.3). However, we want to emphasize that the above discussion does not imply that image structure related features are not important.  The ranking is only for the relative importance.



\subsubsection{Classification Performance for Selected Features}
\label{classificationsubsec}
Figure \ref{fig:fs_results} shows the classification accuracy for different number of features used in the original dataset and the augmented dataset. 
% The average ranking for importance of features for each dataset can be found at Table \ref{tab:feature_importance} and Table \ref{tab:feature_importance_aug}.
We observe that a minimum of 3 features are needed to reach a classification accuracy above 90\% for the original dataset and 4 features for the augmented datasets. For the original dataset, the best performance is achieved when 5 features are selected, after which the performance drops slightly.  For the augmented dataset, the performance is quite stable when 4 or more features are selected. However, because the number of unacceptable images from the subjective ratings is very limited in the original dataset, it is necessary to include the additional noisy images in the augmented dataset to improve the performance of unacceptable images as discussed in Section \ref{sec:dataanaysis}. % and Section \ref{sec:result}.  
% Figure environment removed
% The accuracy performances under acceptable cases are quite close for both situations of using original and augmented datasets. Actually, the main purpose of introducing augmented data is to improve the accuracy performance under unacceptable cases where we will see later that using augmented dataset is necessary.  For original dataset, the best accuracy performance occurs when the number of selected features is 5. It seems that there is a trend that the accuracy performance will drop if more than 5 features are used.  For augmented dataset, the accuracy performance presents a stable performance if the number of selected features is more than 2.  

%\newpage


%% Figure environment removed

%% Figure environment removed



