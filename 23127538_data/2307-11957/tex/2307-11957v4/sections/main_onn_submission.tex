
\section{Introduction}

Optical computing leverages the properties of light waves to facilitate high-speed data processing while reducing the energy cost~\cite{marechal1953filtre, cutrona1960optical,o1956spatial, ambs2010optical, wetzstein2020inference, lugt1964signal, zhou2019optical}. Recent advance in automatic differentiation has enabled in silico training of large-scale optical computing weights, giving rise to the realizations of diffractive neural networks~\cite{chang2018hybrid, lin2018all}, optical reservoir computing~\cite{rafayelyan2020large, verstraeten2007experimental}, and coherent nanophotonic circuits~\cite{shen2017deep}. 

Training optical computing systems presents two challenges: an intensive computational process and a performance disparity between simulation and reality when implementing pre-trained weights onto real-world systems~\cite{buckley2023photonic, lin2018all, rumelhart1986learning}.
Typically, the optical computing systems are trained in silico using differentiable simulators rooted in the first principle of optics, an approach known as simulator-based training (SBT). While SBT has proven effective within the confines of the simulator, the performance in real systems is largely contingent upon the simulator's fidelity. Factors such as misalignment and aberration, often omitted in simulations, cause significant performance degradation when optical computing weights trained exclusively within the simulator are applied to real-world systems.

To bridge the simulation-to-reality gap, physics-aware training (PAT) and hybrid training (HBT) have been introduced ~\cite{wright2022deep, spall2022hybrid}. Both training strategies include conducting the forward pass in the real-world system and back-propagating the loss from the system to its weights through the simulator. These in situ approaches allow the training process to access the optical computing system during the forward pass, which leads to more accurate weight updates than in silico training ~\cite{buckley2023photonic}.

Despite these recent advances, there is a continued reliance on a physics-based simulator during the back-propagation process in current in situ training methods. Such a setting brings three drawbacks: (1) the biased simulator prohibits the above training process from achieving optimal results; (2) the in silico simulation requires large memory and computation, limiting the aforementioned methods from in situ training in edge devices with limited computing resources ~\cite{sludds2022delocalized}; and (3) the model-based training strategies need high-fidelity images of the input object, which are costly to acquire in terms of time and memory.

% #Medium
\begin{table}[ht]
\arrayrulecolor{black}
\fontsize{8pt}{8pt}\selectfont
    \centering
    \begin{tabular}{c|p{1.3cm}|p{1.3cm}|p{1.5cm}}
    \toprule
    &\textbf{SBT~\cite{lin2018all}} & \hfil\textbf{HBT}~\cite{spall2022hybrid} & \hfil\textbf {MFO (ours)}  \\\midrule
    % Physics engine~ & \checkmark \\
    In situ & \hfil\cellcolor{red!20} No & \hfil\cellcolor{green!20}Yes & 
    \hfil\cellcolor{green!20}Yes \\
    % Learned physics~ & & \checkmark & \checkmark \\
    % \hline
 Computation overhead & \hfil\cellcolor{red!20} High & \hfil\cellcolor{red!20} High &  \hfil\cellcolor{green!20} Low \\ 
 % \hline
    Model-free & \hfil\cellcolor{red!20} No & \hfil\cellcolor{red!20} No & \hfil\cellcolor{green!20} Yes  \\
    \bottomrule
    \end{tabular}
    \caption{{\textbf{Comparison of strategies on training optical computing systems} along the axes of in situ training capability (in situ), in silico computation overhead (computation overhead), and requirements on a physics-based simulator and knowledge of input objects (model-free).}}
    % The AFM method is slow in speed but has high lateral resolution and exhibits $2.5D$ imaging capability. Our work chooses AFM as the tool for characterization as the latter two are vital factors for the sub-diffraction-limited characterization of high-resolution OPD map in nano-prints.
    \label{tab: comparison of training methodlogies}
\vspace{-1em}
\end{table}

Here, we propose an in situ model-free optimization (MFO) method that does not require back-propagating errors through the simulator. Instead, we use a score gradient estimation algorithm~\cite{wierstra2014natural, williams1992simple} to solely use the forward outputs from the real system to get gradients for updating the weights of the optical computing system. As shown in Fig.~\ref{fig:mfo}, our method treats the optical system as a black box and back-propagates task-specific negative-loss as rewards to the source weight distributions (Fig.~\ref{fig:mfo}a). This process only requires knowledge of the weights and forward outputs of the system, unlike the SBT and HBT methods that require a simulator and the images of the input objects (Fig.~\ref{fig:mfo}b). A tabular comparison of our MFO method, SBT, and HBT is in Tab.~\ref{tab: comparison of training methodlogies}. 

We demonstrate our method on a single-layer diffractive optical computing system. Experimental results show that MFO outperforms hybrid training on the commonly used MNIST and FMNIST datasets~\cite{lin2018all, wright2022deep, spall2022hybrid} (Sec.~\ref{sec: results}\ref{subsec: results comparison on MNIST}). 
As a proof-of-concept demonstration, we experimentally show that our MFO-trained optical computing system can classify four types of white blood cells from their phase maps with a testing accuracy of $73.8\%$ (Sec.~\ref{sec: results}\ref{subsec: classify WBC.}), making it a promising approach for image-free and high-speed cell analysis. Lastly, we show the MFO training process only consumes $\sim 0.1\%$ of the GPU time and memory compared with the HBT in situ method by avoiding the computation-intensive modeling of the wave propagation process (Sec.~\ref{sec: results}\ref{subsec: source efficient training by MFO.}).

% Figure environment removed 

\section{Methodology}\label{sec: method}
In what follows, we detail the problem setup on training the optical computing system and our solution. We introduce problem formation in Subsec.~\ref{subsec: problem setup} and the conventional solution of using simulator-based training (SBT) in Subsec.~\ref{subsec: simulator-based design}. We then illustrate our solution to the problem, the model-free optimization (MFO) for training the optical computing system in Subsec.~\ref{subsec: mfo methodology}. Finally, we illustrate the optical computing system and its simulator in Subsec.~\ref{subsec: system description}, which we will use to demonstrate the performance of our method. 


\subsection{Problem setup} \label{subsec: problem setup}
% \gy{$x_i\in \mathbb{R}^{n_1\times n_1}$}
 We are interested in learning the optimal optical computing weight $w\in\mathbb{R}^{H}$ for the optical computing system on a desired task with a training dataset $\mathcal{D} = \{x_i, y_i\}_{i=1}^N$, where $N$ is the size of the dataset, $H$ is the number of trainable parameters in $w$, and $x$ and $y$ denote the input and target of interest, respectively. A function $f_{sys}(\cdot;w)$ maps $x\rightarrow y$ through this optical computing system with $w$. Specifically, in the image classification task based on the diffractive optical computing system we work on, $f_{sys}$ denotes the optical mapping from the input image $x$ to the output label $y$, and $w$ is the phase-valued optical computing weight. 
 % shown in Fig.~\ref{fig:mfo}.
 % We detail more about our system in subsection~\ref{subsec: system description}.
 
During training, we minimize the cost function $J(w)$ as the mean task-specific loss across the entire training data set $\mathcal{D}$:

\begin{subequations}
\begin{align}
          \operatorname*{arg\,min}_w J(w): &=\mathbb{E}[\mathcal{L}(\mathcal{D}, w)], \\
                    &= \frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(f_{sys}(x_i; w), y_i), \label{eq:objective function}
\end{align}
\end{subequations}
% \begin{subequations}
where $\mathcal{L}$ is the task-specific loss function, we use cross-entropy loss~\cite{brier1950verification} since we deal with image classification tasks throughout this paper. 

We use gradient descent-based search to find the optimal $w$ to minimize the objective function $J(w)$:
\begin{equation} \label{eq:grad descent}
    w = w - \alpha \nabla_{w} J(w),
\end{equation}
where $\nabla_{w}$ represents the gradient operator that collects all the partial derivatives of a function concerning parameters in $w$, and $\alpha$ is the learning rate. 

It is straightforward to use the backpropagation method~\cite{rumelhart1986learning} to take the gradient through $f_{sys}$ and finds the gradient $\nabla_{w} J(w)$ as:
\begin{equation}
    \nabla_w J(w)= \frac{1}{N}\sum_{i=1}^N \nabla_w \mathcal{L}({f_{sys}(x_i, w)}, y_i),
    \label{eq: FOBG}
\end{equation}
when we have an accurate differentiable modeling of $f_{sys}$.
However, this is the case for training digital neural networks, but not when we train a real-world optical computing system. \textbf{Thus, this paper's critical aim is finding an accurate gradient estimation of $\nabla_w J(w)$ to update optical computing weight $w$ in a real-world optical system}.

% Note that we use the whole dataset for the gradient update to simplify the illustration in Eq.~\ref{eq: FOBG}. In actual implementation, We use stochastic gradient descent (SGD)~\cite{lecun2015deep} and iterate through the whole dataset with the batch-gradient-based update.


% Figure environment removed

\subsection{In silico simulator-based training (SBT)} \label{subsec: simulator-based design}



\textbf{Back-propagation through the simulator $\hat{f}_{sys}$.} In a real-world optical computing system, since we do not have an exact functional expression of $f_{sys}$, the \textbf{simulator-based training} (SBT) builds a simulator $\hat{f}_{sys}$ as the differentiable approximation of $f_{sys}$ (Fig.~\ref{fig:mfo}b). 
A naive training strategy is substituting $f_{sys}$ in Eq.~\ref{eq: FOBG} with the simulator $\hat{f}_{sys}$ and applying in silico training on the simulator:

\begin{equation}
    \nabla_w J(w)= \frac{1}{N}\sum_{i=1}^N \nabla_w \mathcal{L}({\hat{f}_{sys}(x_i, w)}, y_i).
    \label{eq: sbt}
\end{equation}

The resulting $\nabla_w J(w)$ is used in Eq.~\ref{eq:grad descent} to update $w$.
After training, the optimized $w$ is uploaded to the real optical computing system $f_{sys}$ to test the performance. 


\textbf{Simulation-to-reality gap.} The aforementioned simulator-based training is based on backpropagation through the simulator $\hat{f}_{sys}$. The "sim2real" gap is low (i.e., the gradient $\nabla_w J(w)$ is an accurate estimation) when the simulator $\hat{f}_{sys}$ is similar to $f_{sys}$. However, this assumption does not hold in many prototypes of optical computing systems where the inadequate modeling and misalignment between the optical elements decay the performance of the SBT during the "sim2real" transfer. We use a simulator described in Subsec.~\ref{subsec: system description} to assess the adverse effect of misalignment on the image classification results. We imposes various misalignments to a well-trained ideal optical computing system and measure drops in classification accuracy. For instance, we show in Fig.~\ref{fig: misalignment degrades the performance} that slightly laterally misaligning the optical computing layer by $41.1 \: \mu m$ reduces the classification accuracy by $31.2\%$.  
% Thus, \textbf{we aim to demonstrate a training strategy that does not suffer from the sim2real transfer and has the comparable imaging classification accuracy we have in the simulator}.

% The cost function $J$:

% \begin{equation}
% % \mathcal{F}(\distParams) := \int \dist(\vx; \distParams) \cost(\vx; \costParams) \intd\vx =
% % \expect{\dist(\vx; \distParams)}{\cost(\vx; \costParams)} .
% J(\theta) = \mathbb{E}_{\theta}[l_{\theta}(z)] = \int l(z) \pi(z\|\theta) dz
% \label{eq:expectation_function}
% \end{equation}


% \begin{equation}
%     r = R(\mathbf{y}) = \frac{\bar{\mathbf{y}}_{focus}}{\bar{\mathbf{y}}_{bg}},
% \end{equation}


\subsection{In situ model-free optimization (MFO)}\label{subsec: mfo methodology}
Our solution for solving the aforementioned "sim2real" gap issue in Subsec.~\ref{subsec: simulator-based design} is: in situ learning the optical computing weight $w$ with the model-free optimization (MFO). In situ learning enables us to access the output of $f_{sys}$, and is feasible on the hardware side because we can use programmable devices such as spatial light modulators to update $w$. The challenging part is designing a training strategy that efficiently uses the output of actual system $f_{sys}$ to construct an unbiased gradient estimator. Here, we use the score gradient estimator to calculate the gradient~\cite{schulman2015gradient, mohamed2020monte} for the backward update of parameters in $w$ while circumventing the construction of $\hat{f}_{sys}$, a biased and resource-intensive numerical modeling of $f_{sys}$. 

\textbf{Back-propagation through the weights distribution $p$.} In our score gradient estimator for model-free optimization, we model optical computing weight $w$ as a random variable that follows a 
probability distribution: $w\sim p(w|\theta)$ and rewrite the cost function $J(w)$ in  Eq.~\ref{eq:objective function} as a probability cost function $J(\theta)$:
\begin{subequations}
\begin{align}
        \operatorname*{arg\,min}_{\theta} J(\theta): 
        % &= \frac{1}{N}\sum_{i=1}^{N}l(f_{sys}(x_i, w), y_i)\\
          &= \frac{1}{N}\sum_{i=1}^{N} \mathbb{E}_{p(w|\theta)}[\mathcal{L}(f_{sys}(x_i, w), y_i)],\\
          &= \frac{1}{N}\sum_{i=1}^{N} \int {p(w|\theta)}\mathcal{L}(f_{sys}(x_i, w), y_i) \diff w.
\end{align}\label{eq: prob obj}
\end{subequations}

The probability distribution $p(w|\theta)$ is continuous in its domain and differentiable concerning its distribution parameter $\theta$. 
Accordingly, the original goal of optimizing $w$ is reformulated as finding a most likely distribution $p(w|\theta)$ that minimizes the cost function $J(\theta)$ in Eq.~\ref{eq: prob obj}. 
Specifically, we model the distribution $p$ as a multivariate normal distribution with $\theta = \{\mu, \sigma^2\}$ and $p(w|\theta) = \mathcal{N}(w;\mu, \sigma^2)$ for optimizing the continuous phase-valued weight to be uploaded onto the SLM in our work. 


To update distribution parameter $\theta$ with the gradient descent Eq.~\ref{eq:grad descent}, we take the gradient of the cost function $J(\theta)$ in Eq.~\ref{eq: prob obj}:
\begin{subequations}
\begin{align}
        \nabla_{\theta} J(\theta) 
        &= \nabla_{\theta} \frac{1}{N}\sum_{i=1}^{N} \int {p(w|\theta)}\mathcal{L}(f_{sys}(x_i, w), y_i) \diff w ,\\
        &= \frac{1}{N}\sum_{i=1}^{N} \int \mathcal{L}(f_{sys}(x_i, w), y_i) \nabla_{\theta}{p(w|\theta)} \diff w .
\end{align}
\end{subequations}
Applying log derivative trick, we have:
\begin{equation}
        \nabla_{\theta} J(\theta) 
        = \frac{1}{N}\sum_{i=1}^{N} \int p(w|\theta) \mathcal{L}(f_{sys}(x_i, w), y_i) \nabla_{\theta} \log{p(w|\theta)} \diff w .\label{eq: prob obj int}
\end{equation}
We apply Monte Carlo integration to approximate the integral value in Eq.~\ref{eq: prob obj int} by first drawing $M$ independent samples $\{w_j\}_{j=1}^M$ from the distribution $p(w_j|\theta)$ and then computing the average function value evaluated in these samples:
        \begin{subequations}
        \begin{align}
        \nabla_{\theta} J(\theta) 
        &=\frac{1}{N}\sum_{i=1}^{N} \frac{1}{M}\sum_{j=1}^{M} \mathcal{L}(f_{sys}(x_i, w_j), y_i) \nabla_{\theta} \log{p(w_j|\theta)} \label{eq: prob obj monte carlo int},\\
        &=\frac{1}{M}\sum_{j=1}^{M} [\frac{1}{N}\sum_{i=1}^{N}  \mathcal{L}(f_{sys}(x_i, w_j), y_i)] \nabla_{\theta} \log{{p(w_j|\theta)}}, \label{eq: prob grad}
\end{align}
\end{subequations}
%where $M$ is the number of samples we draw from the distribution $p(w|\theta)$. 
We define $r(w_j)=\frac{1}{N}\sum_{i=1}^{N} \mathcal{L}(f_{sys}(x_i, w_j), y_i)$ as the negative reward corresponding to each weight $w_j$ sampled from the distribution $p(w_j|\theta)$. We use Equation~\ref{eq: prob grad} as the score gradient estimator for MFO, 
where the score function is $\nabla_{\theta} \log{p(w_j|\theta)}$, which has been widely used in other areas, such as policy gradient algorithms in reinforcement learning~\cite{williams1992simple} and diffusion models~\cite{song2019generative}. 


% \sx{we do not do so}  \gy{write down how we did} 

\textit{Variance reduction.} The main risk of using the score gradient estimator is the high variance that comes from the Monte Carlo integration step that transits Eq.~\ref{eq: prob obj int} to Eq.~\ref{eq: prob obj monte carlo int}. Such a sampling-based integration step has high variance because different sets of random samples may lead to significantly different integral estimates. We reduce the variance by subtracting the $r(w_j)$ with baseline value $\bar{r} = \frac{1}{M} \sum_{j=1}^M r(w_j)$ while keeping the bias of gradient unchanged~\cite{mei2023role}:
\begin{equation}
    \nabla_{\theta} J(\theta) = 
    \frac{1}{M}\sum_{j=1}^{M} (r(w_j)- \bar{r}) \nabla_{\theta} \log{{p(w_j|\theta)}}. \label{eq: prob grad simplified}
\end{equation}

\textbf{Training recipe of MFO.} 
During training, we sample a group of phase-valued optical computing weights $\{w_j\}_{j=1}^M$ from the distribution: $w_j \sim p(w_j|\theta)$. Then we upload the sampled weights onto the optical computing layer and test the weights with inputs from dataset $\mathcal{D}$. After that, we calculate the negative rewards $r(w_j)$ and update the distribution parameter $\theta$ through Eqs.~\ref{eq:grad descent} and~\ref{eq: prob grad simplified}. 
The algorithm iterates these steps until convergence. 
After minimizing the objective function Eq.~\ref{eq: prob obj}, we export $w_j$ with the smallest $r(w_j)$ as the output weight $w^{\star}$. In practice, this performs better than setting $w^{\star}$ as the sampled mean $\mu$. 
The algorithmic overview of the training recipe is shown in Algorithm~\ref{alg:mfo}.

\begin{algorithm}
\caption{Algorithmic overview of MFO.}\label{alg:mfo}
\begin{algorithmic}[1]
\State \textbf{Input: }  {Classification dataset $\mathcal{D}=\{x_i, y_i\}_{i=1}^N$, learning rate $\alpha$, number of sampled weights $M$, optical computing system $f_{sys}$, distribution parameter $\theta=\{\mu, \sigma^2\}$, loss function $\mathcal{L}$, epochs $K$.}
\State \textbf{Output: }{Optimized optical computing weight $w^{\star}$.}

\For{$k$ in range $K$}
    \State{Sample $\{w_j\}_{j=1}^M$ from distribution $p(w_j|\theta)$.}
    % \State{ $\{w_j\}_{j=1}^M$}
    \State{\emph{$\triangleright$ in situ evaluate $\{w_j\}_{j=1}^M$.}}
    \For{$j$ in range $M$}\\
        \State{$r(w_j)\gets \frac{1}{N}\sum_{i=1}^{N} \mathcal{L}(f_{sys}(x_i, w_j), y_i).$}
    \EndFor
    \State{\emph{$\triangleright$ in silico update $\theta$.}}
    \State{Calculate $\nabla_{\theta} J(\theta)$ via Eq.~\ref{eq: prob grad simplified}.}
    \State{$\theta \gets \theta -\alpha\nabla_{\theta} J(\theta).$ }
\EndFor
% \State $w^{\star}\gets \mu \text{ from } \theta$.
\State $w^{\star}\gets w_j$ with the smallest $r(w_j)$.
\end{algorithmic}
\end{algorithm}

% Figure environment removed


\subsection{Experiment and simulation detail of our diffractive optical computing system} \label{subsec: system description}

\textbf{Experimental setup of the real optical computing system $f_{sys}$.} We built a single-layer optical computing system to validate the effectiveness of the proposed MFO-based training strategy (see Fig.~\ref{fig: system skectch}). In the experimental setup, a laser field $u_{laser}$ propagates onto the input layer (SLM1), where the phase-valued object $u_{obj}$ is displayed. The light field reflects from the input layer onto the optical computing layer $u_w$ (SLM2). A camera then detects the diffraction light at the output layer. The length $d_{IC}$ and $d_{CO}$ for $Prop_{IC}$ and $Prop_{CO}$ in Fig.~\ref{fig: system skectch} are $215.1 mm$ and $201.6 mm$, respectively. More details of the optical computing system are in Supplement 1 Sec. S.1. As there are multiple components in the system (e.g., SLMs and cameras), it is difficult to align them with high accuracy manually. To compensate for the alignment errors, we developed a digital alignment method based on the homography technique~\cite{riba2020kornia} using SLMs. Details of our digital alignment method are in Supplement 1 Sec. S.5.  



 
\textbf{Differentiable physics-based simulator $\hat{f}_{sys}$.} We construct an ideal physics-based simulator $\hat{f}_{sys}$ corresponding to the aforementioned optical computing system $f_{sys}$ as the sandbox to test different training algorithms. This simulator is also used inside the design loop of SBT and HBT, which serve as the baseline methods to compare with. Since our system only includes free-space wave propagation $\textcolor{red}{\hat{f}_{prop}}$, wavefront modulation $\textcolor{Colorfmod}{\hat{f}_{mod}}$, and sensor detection $\textcolor{Colorfdet}{\hat{f}_{det}}$, we build the optical computing simulator by stacking these three optical modules as building blocks.
The functions of optical modules are:
\begin{subequations} \label{eq: diff modules of system}
   \begin{align}
        &\textcolor{red}{\hat{f}_{prop}}(u_{in},z): u_{out} = \mathcal{F}^{-1} (\mathcal F(u_{in}) \times \mathcal F(h_{prop}(z))), \label{eq: free-space prop} \\
        &\textcolor{Colorfmod}{\hat{f}_{mod}}(u_{in}, u_{element}): u_{out} = u_{in} * u_{element},\\
        % &\textcolor{Colorfdet}{\hat{f}_{det}}(u_{in}):I_{out} = Imnoise(|u_{in}|^2), \label{eq: camera detection.}
        &\textcolor{Colorfdet}{\hat{f}_{det}}(u_{in}):I_{out} = |u_{in}|^2, \label{eq: camera detection.}
    \end{align}
\vspace{0.2cm}
\end{subequations}
where $h_{prop}(z) = \frac {e^{jkz'}}{j\lambda z'}e^{\frac{jk}{2z}(x'^2+y'^2)}$ is the propagation kernel under Fresnel approximation~\cite{born2013principles} with a propagation distance $z'$, wavelength $\lambda$ and angular wave number $k$, $u_{element}$ denotes the wavefront modulation from the programmable optical devices, $\mathcal{F}$ denotes the Fourier transform. 
% $Imnoise$ denotes the process of imposing camera noise. 

% Figure environment removed


Based on the modules in Eq.~\ref{eq: diff modules of system}, the simulator $\hat{f}_{sys} = \{\textcolor{Colorfmod}{\hat{f}_{mod}}, \textcolor{red}{\hat{f}_{prop}}, \textcolor{Colorfmod}{\hat{f}_{mod}}, \textcolor{red}{\hat{f}_{prop}}, \textcolor{Colorfdet}{\hat{f}_{det}}\}$ is constructed by chaining the building blocks ${1-5}$:
\begin{subequations}\label{eq: simuator}
    \begin{align}
        & 1. u_{out} = \textcolor{Colorfmod}{\hat{f}_{mod}}(u_{laser}, u_{obj}), \\
        & 2. u_{out} = \textcolor{red}{\hat{f}_{prop}}(u_{out}, d_{IC}), \\
        & 3. u_{out} = \textcolor{Colorfmod}{\hat{f}_{mod}}(u_{out}, u_{w}), \\
        & 4. u_{out} = \textcolor{red}{\hat{f}_{prop}}(u_{out}, d_{CO}), \\
        & 5. I_{cam} = \textcolor{Colorfdet}{\hat{f}_{det}}(u_{out}),
    \end{align} 
\end{subequations}
The training details of experiment and simulation are in Supplement 1 Sec. S.3.
%where $d_{IC}$ denotes the distance between the input plane and the optical computing layer, $d_{CO}$ denotes the distance between the optical computing layer and the output plane. %$u_{laser}$, $u_{w}$, $u_{obj}$ correspond to the complex fields of the incident laser, the optical computing layer with weight $w$, and the input layer with object $x$ displayed to classify, respectively.


% \subsection{Training details.}

% The parameters of simulators are also calibrated



% % \begin{align}
%     \left \textcolor{red}{Propagator: }f_{prop}(u_{in},z): u_{out} = \mathcal{F}^{-1} (\mathcal F(u_{in}) \times \mathcal F(h_{prop}(z))), \right \\
%     \left \text{where } 
%     h(x,y; z) = \frac {e^{jkz}}{j\lambda z}e^{\frac{jk}{2z}(x^2+y^2)};\right \\
%     \left \textcolor{red}{Modulator: } f_{mod}(u_{in}, u_{element}): u_{out} = u_{in} * u_{elment}.\right
% % \end{align}
% \end{subequations}


\section{Results}\label{sec: results}

We experimentally evaluate the performance of our MFO method on the open-source MNIST and FMNIST datasets in Subsec.~\ref{subsec: problem setup}. We also demonstrate the MFO method on a novel application of stain-free classifying white blood cells in Subsec.~\ref{subsec: classify WBC.}. We then illustrate MFO's advantage of memory- and computation-efficient training in Subsec.~\ref{subsec: source efficient training by MFO.}.
% Lastly, we analyze the influence of sampling size $M$ on our method's final results and optimizable degree of freedom (DOF) $H$ in Subsection~\ref{subsec: MFO exhibits curse of dimensionality.}. 

% our solution to the problem, the model-free optimization for training the optical computing system in subsection~\ref{subsec: mfo}. Finally, we illustrate the optical computing system and its simulator in subsection~\ref{subsec: system description}, which we will use to demonstrate the performance of our method. 

% # and analysis of the training time cost\

\vspace{-0.1cm}
\subsection{MFO outperforms hybrid training (HBT) in the real systems on the MNIST and FMNIST datasets}\label{subsec: results comparison on MNIST}

We conduct experiments on training the single-layer optical computing system (described in Fig.~\ref{fig: system skectch}) on two classical image classification datasets: MNIST~\cite{deng2012mnist} and FMNIST~\cite{xiao2017fashion}. We include in silico SBT and in situ HBT methods as the comparison baselines. Tab.~\ref{tab: MNIST and FMNIST result} quantitatively shows that our method achieves higher classification accuracy than the HBT and SBT methods on both datasets in the experiments. The SBT method performs poorly due to the reality gap between the simulator and the real system. 
The HBT method suffers from the bias between $\hat{f}_{sys}$ and ${f}_{sys}$ in the backward process, while the MFO bypasses the bias-sensitive modeling and updates gradients solely with $f_{sys}$. 
We further show MFO's capability to fine-tune the HBT result. The last row of Tab.~\ref{tab: MNIST and FMNIST result} show that the unbiased MFO method further improves the results of HBT. Moreover, we also empirically find that fine-tuning outperforms MFO only. Fig.~\ref{fig: MFO result} visualizes some experimental outputs and confusion matrices using the MFO method, respectively. 

\begin{table}[h!]
\newcolumntype {C}[1]{>{\centering\arraybackslash}m{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{C{0.16\linewidth} | C{0.072\linewidth} | C{0.072\linewidth}| C{0.072\linewidth}| C{0.072\linewidth}| C{0.072\linewidth}| C{0.072\linewidth}}
\hline
             &\multicolumn{3}{c|}{MNIST}          & \multicolumn{3}{c}{FMNIST} \\ \cline{2-7}
             & Train      & Val      & Test       & Train         & Val         & Test \\\hline\hline
\textcolor{gray}{Ideal}        & \textcolor{gray}{$92.7\%$}          & \textcolor{gray}{$84.3\%$}            & \textcolor{gray}{$82.2\%$}            & \textcolor{gray}{$85.6\%$}         & \textcolor{gray}{$79.9\%$}           & \textcolor{gray}{$76.4\%$} \\
\hline
\hline
SBT          & $81.9\%$         & $74.4\%$           & $69.3\%$         & $68.3\%$          & $64.1\%$           & $60.9\%$ \\
HBT          & $81.9\%$         & $75.5\%$           & $72.8\%$         & $68.3\%$          & $68.7\%$           & $65.8\%$ \\
MFO (Ours)         & $\textbf{83.1\%}$         & $\textbf{77.8\%}$           &$\textbf{73.6\%}$         & $\textbf{74.0\%}$          & $\textbf{71.1\%}$           & $\textbf{70.4\%}$ \\ 
HBT+MFO (Ours)      & $\textbf{87.0\%}$& $\textbf{80.2\%}$  & $\textbf{77.7\%}$ &$\textbf{75.0\%}$  & $\textbf{71.7\%}$   & $\textbf{70.2\%}$  \\
\hline
\end{tabular}
\caption{\textbf{Experimental performance comparison on MNIST and FMNIST datasets.} Results of the ideal mode are from the simulator, whose parameters are determined from experiments while we impose no misalignment on the simulator. The lower four rows are experimental results from SBT, HBT, MFO, {and HBT+MFO (MFO fine-tuning upon the result from HBT)}. 
% The SBT method results in very low experimental accuracy due to misalignment. 
Our method outperforms the SBT and HBT methods on the MNIST and FMNIST datasets in experiments.
} 
\label{tab: MNIST and FMNIST result}
\vspace{-0.2cm}
\end{table}

See Supplement 1 Sec. S.2 for the details of the MNIST and FMNIST datasets used in our work. The detailed description and result visualization of the HBT method are in Supplement 1 Sec. S.6. 
% Implementations of SBT and HBT methods have homography-based calibration in the loop.




% Figure environment removed

\subsection{Application: all-optical classification on cellular dataset.}\label{subsec: classify WBC.}

For the first time, we demonstrate the capability of an optical computing system for stain-free cell analysis, trained by our MFO algorithm (Fig.~\ref{fig: all-optical cell classfier}). We work on the white blood cells (WBC), whose abnormal subtype percentages indicate the immune system’s malfunction or infectious disease \cite{wbcfunction1, wbcfunction2}. We include details of the WBC phase map dataset in Supplement 1 Sec. S.2. Previously, researchers used machine learning methods to classify WBC subtypes, including monocyte, granulocyte, B cell, and T cell, by their morphology in a stain-free way~\cite{wbcbrightfield2019Nassae,wbcphase2021shu}. However, the analysis process is computationally heavy and time-consuming. 
Here, we accelerate the stain-free cell analysis process via computing with light. Our MFO method strikes a training/validation/testing classification accuracy of $72.1\%/73.3\%/73.8\%$ when classifying $4$ types of WBC, exceeding that of the HBT method (Fig.~\ref{fig: all-optical cell classfier}c). Furthermore, Fig.~\ref{fig: all-optical cell classfier}d shows that the inference enabled by optical computing is almost instantaneous ($\frac{d_{IO}+d_{OC}}{c}=1.4 \: ns$, where $c$ is the speed of light), compared to the $1.7\: ms$ of ResNet10, the electronic machine learning model used in \cite{wbcphase2021shu}. We currently need $1$ more millisecond of in silico computing of region intensities corresponding to different classes to obtain the prediction. Such a step can be skipped if we use single-photon avalanche diode (SPAD)~\cite{bruschini2019single} point detectors to count the corresponding regions' cumulative signals. Though for now, the performance of our single-layer linear optical computing system is not on par with the electronic neural network, which hits a testing classification accuracy of $90.5\%$ \cite{wbcphase2021shu}, the potential of having ultra-high inference speed and $>70\%$ classification accuracy here point out an exciting direction on further increasing the complexity of our optical computing system to improve the absolute classification accuracy on classifying cells.  
% \gy{@sx: still needs comparison of inference time and also include the comparison table (against HBT )in the supplement.}\gy{and also mention here.}
% \gy{shuxin, also add quantitative results for the test-time overhead compared to the deep learning-based work.} \sx{will finish on May 17} 
% Fig.~\ref{fig: all-optical cell classfier} further shows the classification results of the optical computing system using the MFO method for the WBC dataset.   
% Figure \ref{fig:false-color} shows an example figure. 



\subsection{Advantage: memory- and computation-efficient training enabled by MFO.}\label{subsec: source efficient training by MFO.}

% Figure environment removed

Our MFO method has an advantage over other training algorithms regarding GPU time and memory efficiency, besides the predicting accuracy discussed in the previous subsection. 
The SBT and HBT methods compute $\hat{f}_{sys}(x, w)$ and $\nabla_{w}\hat{f}_{sys}$ (Fig.~\ref{fig:mfo}b) for each input $x$ in silico, which requires a lot of in silico computation resources. 
In contrast, our MFO method executes the calculation of $f_{sys}(x, w)$ in the light-speed real optical computing system, but not in the computation- and memory-heavy simulator $\hat{f}_{sys}(x,w)$.
% the calculation of gradient in our MFO method relies on execution in the light-speed real optical computing system $f_{sys}$ but not in the computation- and memory-heavy simulator $\hat{f}_{sys}$. \sx{i do not think it is the calculation of gradient}
The only step of our MFO method that consumes in silico computational resources is the one described in Eq.~\ref{eq: prob grad simplified}, where we calculate the score gradient. The in silico computational resources consumption in this step is low because it only scales with the dimension of $w$ and has no relation to the complexity of the system's light transport $\hat{f}_{sys}$.

We compare our MFO method with HBT in GPU memory and time usage in Fig.~\ref{fig: resource consuming}. Our MFO method requires far less GPU time and memory than the HBT method during the training.
%\gy{shuxin: should have table show that MFO saves GPU time and memory.}
% The comparison does not include the memory and time used to transfer the stored image from CPU to GPU.
% The GPU handles all \textit{in silico} computation during training except for image storage from the camera.
% which consumes far less memory and computation than the HBT's backpropagation through the simulator. 




% \subsection{Optical computing with scattering media.}
% One key advantage of MFO training is that it does not rely on the modeling of the model. The computation and modeling overhead does not scale with the system's complexity. We, on the one hand, show such properties, and on the other hand, show the sharp feature in the scattering media will help improve the performance of the optical computing systems. 




% Fig.~\ref{fig: curse of dimensionality}(a) shows that a larger M leads to a higher MFO accuracy.







% % Figure environment removed








\section{Discussion}



\subsection{Limitation: MFO exhibits the curse of dimensionality}\label{subsec: MFO exhibits curse of dimensionality.}
Our method is not without its limits. Our MFO training relies on Monte Carlo integration. It thus inherits the \textit{curse of dimensionality} from the Monte Carlo integration~\cite{bellman1959adaptive}. That is, the number of samples $M$ needed to estimate the integration in Eq.~\ref{eq: prob obj int} with a given level of accuracy grows exponentially with respect to the $H$, the number of trainable parameters (i.e., dimensionality) of the function.
This is also discussed and alleviated with \textit{variance reduction} in the previous Sec.~\ref{sec: method}C. However, the MFO strategy presented in
this paper is still sample-inefficient, though unbiased and memory-efficient. 
We need to either limit the number of trainable parameters $H$, or sample a large number of varied optical computing weights $\{w_j\}^M_{j=1}$ from distribution $p(w_j|\theta)$ in every iteration to make MFO's gradient less noisy. 
The former limits the design DOF of our method, while the latter requires more executions on the real system, which prolongs the training time. 
% The training speed is heavily limited by the refreshing rate of the SLM performs as the optical computing layer (see further in Sec.\ref{}).

% Figure environment removed


We quantitatively investigate the influence of this limitation in Fig.~\ref{fig: curse of dimensionality} by simulation in MNIST dataset, where we visualize how $M$ and $H$ impact the training performance MFO. 
% we conduct simulations with varying $M$ across different trials ( Fig.~\ref{fig: curse of dimensionality}a). we choose $M=128$ in our experiments to have a balance between training time and accuracy.
% We limit the training dataset to $200$ samples from $4$ FMNIST classes, as the MFO training in simulation takes a long time. 
% We only evaluate the training accuracy because we have limited training data, and our primary concern is the MFO's performance in finding $w^{\star} = \operatorname*{arg\, min}_w J(w)$ but not the generalization capability. 
Shown in Fig.~\ref{fig: curse of dimensionality}(a), MFO requires a $M>=128$ to achieve a training accuracy of $>90\%$ given $H=128^2$. 
Moreover, we show in Fig.~\ref{fig: curse of dimensionality}(b) our MFO method fails when increasing $H$ beyond $128^2$ while keeping the sampling size $M$ fixed to $128$. 
% {Note that the training accuracy in this simulation is higher than the experiment in Table~\ref{tab: MNIST and FMNIST result}
% because we use different stopping criteria (see Supplement 1 Sec. S.3.4). 


% As a result, we train for more epochs (~200 for most cases) in the simulation, while the number of epochs is no more than 35 in the experiment. Training for nearly 200 epochs ($\sim$17 days) in the MFO experiment is not feasible with our current system, due to time constraints. We discuss this limitation in Supplement 1 Sec. S.4.}

% In this work, we choose $M=128$ and $H=128^2$.


% Increasing the sampling size $M$ allows for higher accuracy. However, this is at the cost of requiring more time for refreshing weights $w$ of the optical computing layer. We analyze the training time bottleneck induced by the refreshing rate of SLM in Supplement Sec.~\ref{subsec: time bottleneck from refreshing time}. 
% 


% \paragraph{Limitations}

% \begin{itemize}
%     \item Curse of dimensionality. Given the 'curse of dimensionality,' the existing implementation is efficient within a limited searching space (subsection~\ref{}). 
%     \item Our training algorithm Mask uploading time. This limitation coins with the previous one. In such 
% \end{itemize}

% % Figure environment removed


% \paragraph{Limitations} 

% As a model-free method, MFO gets stuck in a local minimum. Table.~\ref{tab: MNIST and FMNIST simulation result} shows that when the exact simulator $\hat{f}_{sys}$ is known in simulation, the simulator-based training (SBT) achieves slightly higher accuracy than our MFO method. However, in a real experiment, we cannot access $f_{sys}$, so our MFO method is the better solution.

% Though an unbiased and memory-efficient in situ training method, the model-free training strategy presented in this paper is intrinsically sample in-efficient. That is the  Monte-Carlo gradient estimator for updating the weights $w$  suffers from the curse of dimensionality. We must limit the search space size or increase the number of Monte Carlo samples in each iteration for an accurate gradient. The former limits the dimension of weights $w$ in the optical computing system, while the latter requires the training process to take many more in situ evaluations $M \times N$ in a real system, as we also have shown in Fig.~\ref{fig: curse of dimensionality}(b); the optimization fails when we increase the size of $w$ to be above 10000. This is solved in various ways in the research black-box optimization~\cite{}, and future research will enhance the sample efficiency. 

% Our method 

% Moreover, the current implementation of the optical computing system relies on off-the-shelf programmable optical elements such as SLMs. The form factor and cost of such devices hinder the deployment of optical computing systems. In the future, we envision that cost-effective and miniaturized programmable devices will make programmable optical computing systems more accessible. 

% \paragraph{Conclusion} 

% \paragraph{Conclusion}

\subsection{Future directions.}

\subsubsection{Further apply MFO to more complex optical computing systems.}
Exploring the scalability of the optical computing system to more complex optical structures, along with integrating more layers and non-linear activation functions, could potentially enhance absolute performance. 

\subsubsection{Further improve the performance of MFO.}
Future research could also consider employing more advanced techniques related to Monte Carlo integration to reduce the training variance discussed in the previous Subsec.~\ref{subsec: MFO exhibits curse of dimensionality.}, which we anticipate could substantially broaden the viable search space, thus further empowering the MFO approach. These include using more advanced sampling strategies~\cite{caflisch1998monte} or integrating MFO with the SBT methods~\cite{kurenkov2021guiding}. The latter makes a trade-off between the model bias and sampling variance.

\subsubsection{Expanding the application of MFO to additional computational optics tasks.}
The concept of MFO presents promising avenues for application in other areas of computational optics, such as computer-generated holography~\cite{zhao2022model} and lens design~\cite{sitzmann2018end}. The inherent model-free and resource-efficient characteristics of MFO position it as a viable alternative to prevalent model-based methods~\cite{blinder2022state, sitzmann2018end}. Future research could focus on leveraging MFO to these domains, potentially enhancing computational efficiency and performance.


\subsection{Conclusion}
To conclude, our study underscores the effectiveness of a model-free strategy in training optical computing systems in situ, manifesting considerable potential in computational efficiency and reducing the simulation-to-reality performance gap. Although the study does not focus entirely on absolute image classification accuracy as it is based on a simple single-layer diffractive optical computing system, it shows relative improvements compared to the existing training strategies, indicating that our strategy is a potentially valuable approach. The model-agnostic nature of our technique may become even more beneficial when implemented in intricate optical systems, representing a robust and versatile alternative to current strategies. It promises a strong foundation for exploring and practically implementing optical computing in real-world applications such as high-speed cell analysis. 
% This research opens the way to many photonic applications involving a contribution from the optical magnetic field, such as chiral light–matter interactions [12], photochemistry [16], manipulation of magnetic processes [38], and new schemes in quantum computing [39] or nonlinear processes [15], among others.

% Our MFO method is $0_{th}$ order method, which can be further combined with the $1_{st}$ order HBT 


% \section{Backmatter}
% Backmatter sections should be listed in the order Funding/Acknowledgment/Disclosures/Data Availability Statement/Supplemental Document section. An example of backmatter with each of these sections included is shown below.

\vspace{2pt}

\begin{backmatter}
\bmsection{Funding} 
Hong Kong General Research Fund (14209521); Hong Kong Innovation and Technology Fund (ITS/178/20FP \& ITS/148/20); Croucher Foundation (CM/CT/CF/CIA/0688/19ay).



\bmsection{Acknowledgments} 
We thank \href{https://zcshinee.github.io/}{Cheng Zheng} for the discussions in the early stages of the work.
% Acknowledgments should be included at the end of the document. The section title should not follow the numbering scheme of the body of the paper. Additional information crediting individuals who contributed to the work being reported, clarifying who received funding from a particular source, or other information that does not fit the criteria for the funding block may also be included; for example, ``K. Flockhart thanks the National Science Foundation for help identifying collaborators for this work.''
\paragraph{Author Contributions.} G.Z. conceived the project, derived the formulation, and built the backbone code and system. X.S. helped with the code writing and system setup and collected the simulation and experiment results. G.Z. and X.S. wrote the manuscript.
% R.Z. supervised the project. G.Z. and X.S. wrote the manuscript with comments and edits from R.Z..

\bmsection{Data availability} 
% The MNIST data used in this study are available in the MNIST dataset [http://yann.lecun.com/exdb/mnist/]. 
Raw data underlying the results presented in this paper are not publicly available but can be obtained from the authors upon request.

\bmsection{Code availability} 
% The MNIST data used in this study are available in the MNIST dataset [http://yann.lecun.com/exdb/mnist/]. 
The code regarding this research will be released upon publication.



\bmsection{Disclosures} 
The authors declare no conflicts of interest.
% Disclosures should be listed in a separate section at the end of the manuscript. List the Disclosures codes identified on the \href{https://opg.optica.org/submit/review/conflicts-interest-policy.cfm}{Conflict of Interest policy page}. If there are no disclosures, then list ``The authors declare no conflicts of interest.''

\smallskip


\bmsection{Supplemental document}
See Supplement 1 for supporting content. 

\end{backmatter}






% \section{References}

% Note that \emph{Optics Letters} and \emph{Optica} short articles use an abbreviated reference style. Citations to journal articles should omit the article title and final page number; this abbreviated reference style is produced automatically when the \emph{Optics Letters} journal option is selected in the template if you are using a .bib file for your references.

% However, full references (to aid the editor and reviewers) must be included as well on a fifth informational page that will not count against page length; again this will be produced automatically if you are using a .bib file.

% \bigskip
% \noindent Add citations manually or use BibTeX. See \cite{Zhang:14,OSA,FORSTER2007,testthesis,manga_rao_single_2007}.

% Bibliography
\bibliography{main_onn}


