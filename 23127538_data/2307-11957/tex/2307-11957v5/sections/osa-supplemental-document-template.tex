


% \captionsetup{justification=justified}



% \title{Towards high-performance real-world optical computing by in situ model-free optimization: supplemental document}


% \begin{abstract}
% The supplement document contains details to reproduce the hardware experiments, the dataset and more results.
% \end{abstract}

% \setboolean{displaycopyright}{false} %copyright statement should not display in the  supplemental document


% \maketitle
% \addcontentsline{toc}{section}{Supplements}
\section{Supplements}
\renewcommand{\thefigure}{S.\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{S.\arabic{table}}
\setcounter{subsection}{0}
\renewcommand{\thesubsection}{S.\arabic{subsection}}
\setcounter{table}{0}
\setcounter{equation}{0}
\setcounter{algorithm}{0}
\renewcommand{\theequation}{S.\arabic{equation}}
\renewcommand{\thealgorithm}{S.\arabic{algorithm}}

% \vspace{0.3cm}
% \localtableofcontents
% \vspace{0.5cm}

\subsection{Reproducibility of the hardware system} \label{subsec: Reproducibility}
\subsubsection{Hardware details of the optical computing system.}

% \gy{Shuxin: Also have a distance of propagators included.}

% Figure environment removed

Figure.~\ref{fig:real_system} shows our home-built optical computing systems. Table.~\ref{tab:device_type} lists models of critical components used in our home-built optical computing systems. Table.~\ref{tab:device_param} further provides dimensions and frame rates of programmable optical devices (i.e., SLM1, SLM2 and DMD) and cameras involved in this work. The definition of the modulation area is shown in Fig.~\ref{fig:effective_shape}.

\vspace{0.3cm}
\subsubsection{Parameters of the optical computing system}
We obtain the classification prediction of the optical computing system by comparing the intensities of different regions in the output layer~\cite{reconfigurable_onn, lin2018all}, as illustrated in Fig.~\ref{fig:apply mask}. 

\begin{table}[h!]
\begin{threeparttable}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{P{0.22\linewidth} P{0.3\linewidth} P{0.28\linewidth}}
\hline
\textbf{Device} & \textbf{Part number} & \textbf{Manufacturer}\\
\hline
\rowcolor{gray!10}  SLM1        & PLUTO                & Holoeye\\
\rowcolor{white!10} SLM2        & GAEA                 & Holoeye\\
\rowcolor{gray!10}  DMD         & DLPLCR9000EVM       & TI\\
\rowcolor{white!10} Camera          & BFS-U3-13Y3C-C       & Teledyne FLIR\\
\rowcolor{gray!10}  BS              & HBS11-025-50-VIS     & Hengyang Optics\\
\rowcolor{white!10} Polarizer       & CCM5-PBS201/M        & Thorlabs\\
\hline
\end{tabular}
\caption{\textbf{Models of key components used in our optical computing system.}}
\label{tab:device_type}
\end{threeparttable}
\end{table}


\begin{table}[h!]
\begin{threeparttable}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular} {P{0.12\linewidth} P{0.05\linewidth} P{0.19\linewidth} P{0.19\linewidth} P{0.15\linewidth}}
\hline
\textbf{Device} & \textbf{Pitch size ($\mu m$)} & \textbf{Full shape (pixels)} & \textbf{Modulation area shape (pixels)} & \textbf{Refreshing rate (FPS)} \\ \hline
\rowcolor{gray!10}  SLM1  & $8$       & $1080\times1920$     & $512\times512$      & $25$\tnote{*}\\
\rowcolor{white!10} SLM2   & $3.74$    & $2160\times3840$     & $1248\times1248$    & $30$\tnote{*}\\
\rowcolor{gray!10}  DMD        & $7.6$     & $1600\times2560$     & $512\times512$      & $125$\tnote{**}\\
\rowcolor{white!10} Camera     & $8$       & $1024\times1280$     & $512\times512$      & Max: $170$\\
\hline
\end{tabular}
\begin{tablenotes}
\item[*] We experimentally get 25 or 30 FPS in our self-written Python programs.
\item[**] The refresh rate of the DMD is limited by the camera's exposure time which is 4000 Âµs.
\end{tablenotes}
\caption{\textbf{Dimensions and frame rates of our experiments' programmable optical devices and cameras.}}
\label{tab:device_param}
\end{threeparttable}
\end{table}

% Figure environment removed
% Figure environment removed


\subsection{WBC dataset details}\label{sec: dataset}
% \subsubsection{MNIST and FMNIST} 
% We randomly split $2000$ samples ($200$ per class) from the MNIST or FMNIST dataset into training and validation sets. There are 1000 samples for training and 1000 samples for validation. We additionally use $1000$ samples (100 per class) for testing.
% \subsubsection{WBC dataset}
The WBC phase image dataset \cite{wbcphase2021shu} has four classes of cells: granulocyte, monocyte, B cell, and T cell. Table~\ref{tab: WBC dataset} gives the number of cells in each class. 
% We use size and dry mass \cite{drymass} features to classify WBCs with an support vector machine (SVM) \cite{svm} and achieve an accuracy of $85.5\%$.

\begin{table}[h!]
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\renewcommand{\arraystretch}{1.0}
\begin{tabular} {P{0.3\linewidth} P{0.15\linewidth} P{0.15\linewidth} P{0.15\linewidth}}
\hline
                & \textbf{Training  set} & \textbf{Validation set} &\textbf{Testing set}\\ \hline
\rowcolor{gray!10}  Granulocyte     & $465$      & $100$           & $100$\\
\rowcolor{white!10} Monocyte        & $513$      & $100$           & $100$\\
\rowcolor{gray!10}  B cell          & $437$      & $100$           & $100$\\
\rowcolor{white!10} T cell          & $330$      & $100$           & $100$\\
\rowcolor{gray!10}  Total           & $1745$     & $400$           & $400$\\
\hline
\end{tabular}
\caption{\textbf{Number of cells of each class in the WBC dataset.}}
\label{tab: WBC dataset}
\end{table}


\subsection{Training details} \label{subsec: training process}

\subsubsection{Computing resources.}
We evaluate our method both in simulation and in experiments. The simulation runs on a high-end Linux server. Physical experiments are conducted with a Windows desktop. 
% because the server lacks an interactive interface to calibrate the optical computing system. 
Table~\ref{tab:computer_configuration} shows the Windows desktop and Linux server configurations. 



\begin{table}[h!]
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
% \centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{P{0.3\linewidth} P{0.29\linewidth} P{0.27\linewidth}}
\hline
\toprule
                                     & \textbf{Server (Simulation)}                     & \textbf{Desktop (Experiments)}\\ \hline
\rowcolor{gray!10} CPU               & Intel Xeon Silver 4210R [$\times$2] & Intel Core i7-7700\\
\rowcolor{white!10}GPU               & Nvidia A6000 [$\times$2]            & Nvidia TITAN XP\\ 
\rowcolor{gray!10}GPU memory         & $48GB$ [$\times$2]                    & $12GB$\\ 
\rowcolor{white!10}RAM               & $192GB$                               & $32GB$\\ 
\rowcolor{gray!10}Operating system   & Ubuntu 18.04                        & Windows 10\\ 
\rowcolor{white!10}Graphic interface & No                                  & Yes\\
\bottomrule
\end{tabular}
\caption{\textbf{Configurations of our server and desktop.}}
\label{tab:computer_configuration}
\end{table}
\subsubsection{Training parameters and time of simulations and experiments}

The training parameters and time of simulations and experiments are shown in Table.~\ref{tab: simulation and experiment parameters}. Unless otherwise noted, we stop the training process when the validation loss does not decrease for 10 epochs in both simulations and real system experiments. We choose the checkpoint with the lowest validation loss and use it to evaluate the test set. 

In Sec. 4.5 of the main text, we assess the G-MFO method's potential for overfitting using a single-layer optical computing system simulator. Thus, we stop the training process at the 200th epoch or after 50 epochs of no decrease in the training loss. We take the training accuracy in the checkpoint with the lowest training loss as the result for each simulation in Sec. 4.5. 

Due to the extensive time required for G-MFO simulations and experiments involving 10,000 training data on the two-layer optical computing system, we stop these training processes at the $10th$ epoch and $9th$ epoch, respectively.

% Training using MFO is much slower than training using SBT since MFO requires $M=128$ sampled variants, and thus the forward computing in the simulator takes time. 
% We need $32\times 128$ times to process a batch of $B=32$ inputs and complete one training batch.

\begin{table}[h!]
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{threeparttable}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular} {P{0.1\linewidth} P{0.08\linewidth} P{0.05\linewidth} P{0.05\linewidth} P{0.05\linewidth} P{0.05\linewidth} P{0.1\linewidth} P{0.1\linewidth} P{0.08\linewidth} P{0.1\linewidth}}
\hline
    & Number of trainable parameters & Sampling size & Batch size  & Optimizer & Learning rate & \textit{In silico} computation time\tnote{*} & Device waiting time for getting $f_{sys}(x,w)$\tnote{*} & Total training time\tnote{*} & Stop criteria \\ \hline
\rowcolor{gray!10} \multicolumn{10}{c}{MNIST and FMNIST simulations on the two-layer optical computing system $\hat{f}_{sys-2-layer}$}\\
\rowcolor{gray!10} \multicolumn{10}{c}{(\textit{$10,000$ data points each for training, validation and testing).}}\\
Ideal & $128^2$ & None  & $32$ & SGD & $10$ & $1m47s$ & None & $1m47s$ & ES-Val-10\\
HBT   & $128^2$ & None  & $32$ & SGD & $10$ & $2m25s$  & None & $2m25s$ & ES-Val-10\\
G-MFO   & $128^2$ & $128$ & $32$ & SGD & $20$ & $6h2m25s$ & None & $6h2m25s$ & $10th$ epoch\\
\rowcolor{gray!10} \multicolumn{10}{c}{MNIST and FMNIST experiments on the two-layer optical computing system $f_{sys-2-layer}$}\\
\rowcolor{gray!10} \multicolumn{10}{c}{(\textit{$10,000$ data points each for training, validation and testing).}}\\
G-MFO   & $128^2$ & $128$  & $32$ & SGD & $20$ & $24m53s$ & $5h38m20s$ & $6h3m13s$ & $9th$ epoch\\
\rowcolor{gray!10} \multicolumn{10}{c}{MNIST and FMNIST simulations on the single-layer optical computing system $\hat{f}_{sys-1-layer}$}\\
\rowcolor{gray!10} \multicolumn{10}{c}{(\textit{$1,000$ data points each for training, validation and testing).}}\\
Ideal \& SBT & $128^2$ & None & $32$ & Adam & $0.01$ & $17s$ & None & $17s$ & ES-Val-10\\
G-MFO          & $128^2$ & $128$& $32$ & SGD  & $15$ & $11m58s$ & None & $11m58s$ & ES-Train-50 or $200th$ epoch\\
\rowcolor{gray!10} \multicolumn{10}{c}{MNIST and FMNIST experiments on the single-layer optical computing system $f_{sys-1-layer}$}\\
\rowcolor{gray!10} \multicolumn{10}{c}{(\textit{$1,000$ data points each for training, validation and testing).}}\\
HBT   & $128^2$ & None  & $4$ & Adam & $0.01$ & $2m16s$ & $2m18s$ & $4m34s$ & ES-Val-10\\
G-MFO   & $128^2$ & $128$ & $32$ & SGD & $100$ & $6s$ & $1h56m1s$ & $1h56m7s$ & ES-Val-10\\
HBT+G-MFO   & $128^2$ & $128$ & $32$ & SGD & $10$ & $6s$ & $1h56m1s$ & $1h56m7s$ & ES-Val-10\\
\rowcolor{gray!10} \multicolumn{10}{c}{WBC experiments on the single-layer optical computing system $f_{sys-1-layer}$}\\
% \rowcolor{gray!10} \multicolumn{10}{c}{(\textit{$1,000$ data points each for training, validation and testing).}}\\
HBT   & $128^2$ & None  & $4$ & Adam & $0.005$ & $4m4s$ & $2m6s$ & $6m15s$ & ES-Val-10\\
G-MFO   & $128^2$ & $64$ & $32$ & SGD & $50$ & $1m11s$ & $1h42m17s$ & $1h43m28s$ & ES-Val-10\\
\hline
\end{tabular}
\begin{tablenotes}
\item[*] Time per epoch.
\end{tablenotes}
\caption{\textbf{Training parameters and time for experiments and simulations.} ES-Val-10 denotes stopping training after 10 epochs without a decrease in validation loss, while ES-Train-50 denotes stopping training following 50 epochs of no decrease in training loss.}
\label{tab: simulation and experiment parameters}
\end{threeparttable}
\end{table}



% \begin{table}[h!]
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
% \arrayrulecolor{black}
% \centering
% \renewcommand{\arraystretch}{1.0}
% \begin{tabular} {P{0.5\linewidth} P{0.2\linewidth} P{0.2\linewidth}}
% \hline
%                                 & \textbf{SBT{ \&Ideal}} & \textbf{MFO}  \\ \hline
% \rowcolor{white!10} Number of trainable parameters $H$ & $128^2$       & $128^2$\\
% \rowcolor{gray!10}  Sampling size $M$& NAN           & $128$ \\
% \rowcolor{white!10} Parallel computing batch size   & $32$             & $1$  \\
% \rowcolor{gray!10}  Batch size $B$                  & $32$             & $32$ \\
% \rowcolor{white!10} Optimizer                       & Adam             & SGD\\
% \rowcolor{gray!10}  Learning rate                   & $0.01$           & $15$  \\
% \rowcolor{white!10} Per-epoch training time         & $17s$            & {$11m58s$}\\
% % \rowcolor{gray!10}  GPU memory usage                & 10.3GB         & 35.3GB\\
% \hline
% \end{tabular}
% \caption{\textbf{Training parameters and time for simulation.} MFO is slow in simulation as compared to the experiments as in simulation we use computational-intensive simulations to substitute the light-speed forward optical computation.}
% \label{tab:simulation_training_parameter}
% \end{table}

\subsubsection{Training process for experiments}

The training process for G-MFO and HBT experiments consists of getting optical computing system output $f_{sys}(x,w)$. Experiment workflow.~\ref{workflow: HBT} shows how to get $\{f_{sys}(x_i, w)\}_{i=1}^B$ in the HBT experiment. 
In the G-MFO experiment, we use different experiment workflows to get $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$ depending on the specific devices used as input and optical computing layers. In experiments on the single-layer optical computing system ($responsetime_I>responsetime_O$), we use Experiment workflow~\ref{workflow: MFO mask} for the sake of a shorter waiting time. In experiments on the two-layer optical computing system  ($responsetime_I<responsetime_O$), we use Experiment workflow~\ref{workflow: MFO input}. $B$ is the batch size, $M$ is the sampling size, $responsetime_I$ is the input layer device response time and $responsetime_O$ is the optical computing layer device response time. 



\begin{algorithm}[h!]
\floatname{algorithm}{Experiment workflow}
\caption{Acquiring $\{f_{sys}(x_i, w)\}_{i=1}^B$ in HBT experiment.}\label{workflow: HBT}
\begin{algorithmic}[1]
\State \textbf{Input: }  {A batch of classification dataset $\mathcal{D}_b=\{x_i, y_i\}_{i=1}^B$ with batch size $B$, optical computing weight $w$, optical computing system $f_{sys}$, input layer device response time $responsetime_I$, optical computing layer device response time $responsetime_O$.}
\State \textbf{Output: }{Optical computing system output $\{f_{sys}(x_i, w)\}_{i=1}^B$. 
}
\State{Refresh $w$ on the optical computing layer.}
\State{Wait $responsetime_O$.}  
\For{$i$ in range $B$}
    \State{Display $x_i$ on input layer.}
    \State{Wait $responsetime_I$.}
    \State{Capture $f_{sys}(x_i, w)$ with camera.}
\EndFor
\State{\textbf{Total waiting time}: $responsetime_O + B \times responsetime_I$.}
\end{algorithmic}
\vspace{-0.1cm}
\end{algorithm}


\begin{algorithm}[h!]
\floatname{algorithm}{Experiment workflow}
\caption{Acquiring $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$ in G-MFO experiment when optical computing layer device has a short response time.}\label{workflow: MFO mask}
\begin{algorithmic}[1]
\State \textbf{Input:} {A batch of classification dataset $\mathcal{D}_b=\{x_i, y_i\}_{i=1}^B$ with batch size $B$, a group of sampled optical computing weights $\{w_j\}_{j=1}^M$, optical computing system $f_{sys}$, input layer device response time $responsetime_I$, optical computing layer device response time $responsetime_O$.}
\State \textbf{Output:} {Optical computing system output $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$.}
\For{$i$ in range $B$}
    \State{Display $x_i$ on input layer.}
    \State{Wait $responsetime_I$.}
    \For{$j$ in range $M$}
        \State{Refresh $w_j$ on the optical computing layer.}
        \State{Wait $responsetime_O$.}        
        \State{Capture $f_{sys}(x_i, w_j)$ with camera.}
    \EndFor
\EndFor
\State{\textbf{Total waiting time}: $B \times M \times responsetime_O + B \times responsetime_I$}
\end{algorithmic}
\vspace{-0.1cm}
\end{algorithm}


\begin{algorithm}[!h]
\floatname{algorithm}{Experiment workflow}
\caption{Acquiring $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$ in G-MFO experiment when input layer device has a short response time.} \label{workflow: MFO input}
\begin{algorithmic}[1]
\State \textbf{Input: }  {A batch of classification dataset $\mathcal{D}_b=\{x_i, y_i\}_{i=1}^B$ with batch size $B$, a group of sampled optical computing weights $\{w_j\}_{j=1}^M$, optical computing system $f_{sys}$, input layer device response time $responsetime_I$, optical computing layer device response time $responsetime_O$.}
\State \textbf{Output: }{Optical computing system output $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$.}
\For{$j$ in range $M$}
    \State{Refresh $w_j$ on the optical computing layer}
    \State{Wait $responsetime_O$}
    \For{$i$ in range $B$}
        \State{Display $x_i$ on input layer}
        \State{Wait $responsetime_I$}        
        \State{Capture $f_{sys}(x_i, w_j)$ with camera}
    \EndFor
\EndFor
\State{\textbf{Total waiting time}: $M \times responsetime_O + B \times M \times responsetime_I$}
\end{algorithmic}
\vspace{-0.1cm}
\end{algorithm}

% The training parameters and time for MFO and HBT are shown in Table~\ref{tab:experiment-training-time}. We fine-tune MFO upon the result from HBT by initializing MFO with the best HBT weight we got in the HBT experiment. We use a learning rate of 10 and keep all other settings same as in MFO training.

% \begin{table}[h!]
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
% \centering
% \begin{threeparttable}
% \renewcommand{\arraystretch}{1.0}
% \begin{tabular} {P{0.4\linewidth} P{0.16\linewidth} P{0.16\linewidth}}
% \hline
% % estimate at DMD frame rate of 1440Hz, and use a high speed camera
% % HBT accu is 70%, in another exp, although accu is 65, but the time for in silicon is only 2m9s
% & \textbf{MFO}      &\textbf{HBT} \\ \hline
% \rowcolor{gray!10} Number of trainable parameters $H$ & $128^2$       & $128^2$\\
% \rowcolor{white!10}  Batch size $B$                                        & $32$           & $4$\\
% \rowcolor{gray!10}   Sampling size $M$                       & $128$          & None\\
% \rowcolor{white!10}  Optimizer                                          & SGD          & Adam\\
% \rowcolor{gray!10}   Learning rate                                      & $100$          & $0.01$\\
% \rowcolor{white!10}  \textit{In silico} computation time\tnote{*}   & $6s$           & $2m16s$\\
% \rowcolor{gray!10}    Device waiting time for getting $f_{sys}(x,w)$\tnote{*}         & $1h56m01s$     & $2m18s$\\ 
% \rowcolor{white!10}  Total training time\tnote{*}                      & $1h56m07s$     & $4m34s$\\
% \hline
% \end{tabular}
% \begin{tablenotes}
% \item[*] Time per epoch.
% \end{tablenotes}
% \end{threeparttable}
% \caption{\textbf{Training parameters and training time for experiments. 
% } As the optical computing time is negligible, the device waiting time for getting $f_{sys}(x,w)$ consists mainly of SLM pattern refreshing time and camera exposure time.}
% \label{tab:experiment-training-time}
% \end{table}

% {\subsubsection{When we stop training?}}





% \begin{table}[htp]
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
% \arrayrulecolor{black}
% \centering
% \renewcommand{\arraystretch}{1.0}
% \begin{tabular}{P{0.08\linewidth} P{0.15\linewidth} P{0.15\linewidth} P{0.25\linewidth}}
% \hline
%                         & Sample batch size (L)   & GPU memory usage    & GPU time\\\hline
% \rowcolor{gray!10}  SBT & 4        & 10.8GB                 & 254s  \\
% \rowcolor{white!10} HBT & 4        & 10.8GB                 & 254s \\
% \rowcolor{gray!10}  MFO & 32       & 8.3GB                  & 153s  \\
% \hline
% \end{tabular}
% \caption{GPU memory usage and GPU time.}
% \label{tab: memory and computation efficient training with MFO}
% \end{table}



\subsection{Compare the overall training time of G-MFO and HBT}~\label{subsec: time bottleneck from refreshing time}
The maximum frame rate for the phase-SLM is approximately 30 FPS, while the DMD can reach approximately 125 FPS which is limited by the camera's exposure time (refer to Table~\ref{tab:device_param}). These low refresh rates more adversely affects the training duration of the G-MFO compared to the HBT, with the impact scaling in relation to the G-MFO's sampling size (refer to the device waiting time for computing $f_{sys}(x,w)$ in Table~\ref{tab: simulation and experiment parameters}).
This time bottleneck can be alleviated when substituting the conventional phase-SLM with the fast ones, such as the Heavily-quantized Spatial Light Modulators~\cite{choi2022time} or enhance the light intensity to reduce the camera's exposure time, thereby increasing the DMD frame rate. 
% In such a way, we can significantly reduce the overall training time of our method (Tab.~\ref{tab:experiment-training-time-dmd}). 






% \begin{table}[h!]
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
% \arrayrulecolor{black}
% \centering
% \renewcommand{\arraystretch}{1.0}
% \begin{threeparttable}
% \begin{tabular} {P{0.35\linewidth} P{0.16\linewidth} P{0.16\linewidth}}
% \hline
% % estimate at DMD frame rate of 1440Hz, and use a high speed camera
% % HBT accu is 70%, in another exp, although accu is 65, but the time for in silicon is only 2m9s
% & \textbf{MFO}      &\textbf{HBT} \\ \hline
% \rowcolor{gray!10}  \textit{In silico} computation time\tnote{*}      & $6s$           & $2m16s$\\
% \rowcolor{white!10}  Device waiting time for getting $f_{sys}(x,w)$\tnote{*}       & $3m43s$           & $2s$\\ 
% \rowcolor{gray!10}   Total training time\tnote{*}                  & $3m49s$         & $2m18s$\\
% \hline
% \end{tabular}
% \begin{tablenotes}
% \item[*] Time for one epoch.
% \end{tablenotes}
% \end{threeparttable}
% \caption{\textbf{Using Digital Micromirror Device (DMD) as the input layer can speed up the MFO training. }
% We only estimate the training time by calculation when using DMD as the input layer device and SLM2 as the optical computing layer device. With training dataset size $N=1000$, $B=32$, and $M=128$, we use the same \textit{in silico} time as in Tab.~\ref{tab:experiment-training-time}. While getting $f_{sys}(x,w)$, $responsetime_I = 1000 / 1440Hz = 0.69ms$, $responsetime_O = 1000 / 30Hz = 33ms$. We use Experiment workflow.~\ref{workflow: HBT} in the HBT experiment and Experiment workflow.~\ref{workflow: MFO input} in the MFO experiment. The total device waiting time for each batch is $33ms+32\times0.69ms=55.08ms$ in the HBT experiment and $128\times33ms+32\times128\times0.69=7050.24ms$ in the MFO experiment, except for the last batch with $8$ samples, which has a device waiting time of $33ms+8*0.69ms=38.52ms$ and $128\times33ms+8\times128\times0.69=4930.56ms$, respectively. There are $32$ batches in total. The total per-epoch device waiting time for getting $f_{sys}(x,w)$ in the HBT experiment is about $31\times55.08ms+38.52ms\approx2s$, while in the MFO experiment it is about $31\times7050.24ms+4930.56ms\approx3m43s$.}
% \label{tab:experiment-training-time-dmd}
% \end{table}



% % Figure environment removed




\subsection{Calibration of the system} \label{subsec: calibration}
The calibration of the optical computing system is critical for reproducing baseline methods that rely on the simulator $\hat{f}_{sys}$ as tiny misalignment between multiple planes will hugely degrade the performance, as we have shown in Fig. 2 in the main text. 
% For example,  we can not tell how the light reflecting from the first SLM falls onto the second SLM with our naked eyes.
Our digital alignment for a single-layer optical computing system consists of estimating $2$ sets of parameters: lengths $\{d_{IO}, d_{CO}\}$ and homography transformations $\{H_{IO}, H_{CO}\}$. 
Specifically, $H_{IO}$ is the projective transformation between the input layer and output layer, $d_1+d_2$ is the corresponding distance, $H_{CO}$ is the projective transformation between the optical computing layer and output layer, and $d_2$ is the corresponding distance. 



% Robustness to lighting and
% viewpoint change is improved with a feature-metric loss based on CNN features. In our
% outer loop, we train a CNN to produce robust features for image alignment.
\subsubsection{Calibrate propagation distance z using holography.}
We first use free-space holography~\cite{blinder2022state} to calibrate the distance $d$ between planes. We concatenate the building blocks Eq. 10a and 10c in the main text and construct a free-space holography simulator $\hat{f}_{holo}=\{\hat{f}_{prop}, \hat{f}_{det}\}$. 
The input to the holography simulator is $u_{in}=e^{j\phi_{holo}}$ and $d$, where $\phi_{holo}$ is the pre-calculated phase that generates the hologram, while $d$ is the distance to optimize. 
Calculation of $\phi_{holo}$ is achieved by extracting the phase of Fourier transform of the target obj $I_{target}$:
\begin{equation}
    \phi_{holo} = \text{angle}(\mathcal{F}(\sqrt{I_{target}}))
\end{equation}
Then we upload the $\phi_{holo}$ onto the real SLM with the Holoeye SLM Pattern Generator software, add various defocus phase masks corresponding to various $d$, visually compare the quality of the output hologram and select the best fit $d$ as the calibrated distance.  

\subsubsection{Homography estimation among multiple planes.}
Homography is a linear transformation between corresponding points in
two images with an $8$ degree of freedom~\cite{riba2020kornia}. It is solved by minimizing dense photometric loss or by feature matching. 
Our experiment has two SLMs and a camera (Fig. 4(a) in the main text). Since directly calibrating $H_{IC}$, the transformation between the two SLMs, is infeasible as we cannot put the camera on any plane of these two SLMs; we calibrate their transformations to the output plane (the camera plane), respectively. Take the estimation  of photometric-based homography between the input layer and the output layer as an example. The objective function is:
\begin{equation}
    % H_{IO}^{\star} =  \operatorname*{arg\,min}_{H_{IO}} \|\sqrt{I_{target}}-warp(|\hat{f}_{prop}(u_{in}, d_{IO}, H_{IO})\|^2,
    H_{IO}^{\star} =  \operatorname*{arg\,min}_{H_{IO}} \mathcal{L}_{homo}(\sqrt{I_{target}}-warp(|\hat{f}_{prop}(u_{in}, d_{IO})|),
\end{equation}
where $warp$ is the homography warpper given the homography estimation $H_{IO}$. We solve this objective function with auto-differentiation~\cite{riba2020kornia}. 


% Briefly, the planar homography relates the transformation between two planes (up to a scale factor $s$)~\cite{opencv_library}:
% \begin{equation}
%     s \begin{bmatrix}
%         x'\\
%         y'\\
%         1
%     \end{bmatrix} = 
%     \begin{bmatrix}
%     h_{11} & h_{12} & h_{13}\\
%     h_{21} & h_{22} & h_{23}\\
%     h_{31} & h_{32} & 1
%     \end{bmatrix}
%     \begin{bmatrix}
%         x \\
%         y\\
%         1
%     \end{bmatrix}
%     = H 
%     \begin{bmatrix}
%         x \\
%         y \\
%         1
%     \end{bmatrix}
% \end{equation}




% The SLMs are denoted as pluto $P$ and gaea SLM $G$ respectively.  
% The lightfield passes through the two SLMs and the camera sequentially. 
% Normally, the matrix transformation between the spatial light modulator

% \begin{equation}
%     a = 
%     \begin{bmatrix}
%     1 & 2 & 3\\
%     a & b & c
%     \end{bmatrix} \times \begin{bmatrix}
%         x \\
%         y\\
%         1
%     \end{bmatrix}
% \end{equation}

% \begin{equation}
%     \begin{bmatrix}
%         x'\\ \theta'        
%     \end{bmatrix}
%     = 
%         \begin{bmatrix}
%         1& d\\
%         0 & 1
%     \end{bmatrix} \times 
%     \begin{bmatrix}
%         x\\ \theta        
%     \end{bmatrix}
% \end{equation}


% \begin{equation}
%     \begin{bmatrix}
%         x'\\ \theta'        
%     \end{bmatrix}
%     = 
%         \begin{bmatrix}
%         1& d\\
%         0 & 1
%     \end{bmatrix} 
%     \times 
%     \begin{bmatrix}
%         x\\ \theta        
%     \end{bmatrix}
% \end{equation}

% \subsection{Implementation of hybrid training.} Since the original training



\subsubsection{Rebuild the simulator with homography estimation in the loop.} 
In the previous subsection, we discuss estimating the homography transformation between planes in the real system.

To incorporate the calibrated homography matrices into our system simulator $\hat{f}_{sys}$, we rebuild the simulator with three virtual propagators instead of two. The latter is discussed in the main text and depicted in Fig~\ref{fig: three_virtual_prop}(a). We use a 3-layer simulator because the homography wrapper $warp$ and the wave propagator $\hat{f}_{prop}$ are not commutative. In other words:
\begin{equation}
    warp(\hat{f}_{prop}(u_{in}, d_{IO}), H_{IO}) \neq \hat{f}_{prop}(warp(u_{in}, H_{IO}), d_{IO}). 
\end{equation}

A sketch of the rebuilt simulator is in Fig.~\ref{fig: three_virtual_prop}(b). We introduce a virtual output plane $O_{virtual}$ into the simulator so that we can incorporate the homography estimations $H_{IO}$ and $H_{CO}$ into the simulation loop. Since the homography matrix is estimated at the output planes of the optical propagator, the settings in Fig.~\ref{fig: three_virtual_prop}(b) enable the use of homography matrix at the output planes of the propagator, which cannot be done via using the original simulator in Fig.~\ref{fig: three_virtual_prop}(a). 

% Figure environment removed


\subsection{Baseline method -- Hybrid training.} \label{subsec: baseline}
% We include two baseline methods in the paper. The first one is hybrid training (HBT)/physics-aware training. The other one is the genetic algorithm, another model-free algorithm that is commonly used in wavefront engineering work. Note that for the HBT method, we need to do digital calibration in the above subsection beforehand as otherwise, the performance would drop a lot. 

We include the hybrid training (HBT)/physics-aware training as one of the baseline methods to compare~\cite{spall2022hybrid, wright2022deep}.
% \paragraph{Hybrid training.}
% The forward pass of training utilizes the computing from the real pass. 
The philosophy of hybrid training is simple: when updating the optical computing system, we use the real system to do the forward pass to get the value of the objective function $J(w)$:
% The forward pass:
\begin{equation}
    J(w) = \frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(f_{sys}(x_i; w), y_i).
\end{equation}

In the backward update pass, we substitute the $f_{sys}$ with its differentiable simulator $\hat{f}_{sys}$ and update the weights $w$ via:
\begin{equation}
    w =  w + \alpha \frac{\partial J(w)}{\partial \hat{f}_{sys}} \frac{\partial \hat{f}_{sys}}{w}.
\end{equation}

This trick enables the backward pass in the biased while differentiable simulator $\hat{f}_{sys}$. The critical difference between the hybrid training (HBT) method and the simulator-based training (SBT) is that the former does the forward pass in the real system ${f}_{sys}$ while the latter conducts both passes in the simulator ${\hat{f}}_{sys}$.

We visualize some experimental outputs and confusion matrices using HBT on a single-layer optical computing system in Fig.~\ref{fig: HBT result}.

% Figure environment removed

