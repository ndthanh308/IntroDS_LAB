\documentclass[9pt,twocolumn,twoside]{osajnl}
%% Please use 11pt if submitting to AOP
% \documentclass[11pt,twocolumn,twoside]{osajnl}

\journal{optica} % Choose journal (ao,jocn,josaa,josab,ol,optica,pr)

%See template introduciton for guidance on setting shortarticle option
\setboolean{shortarticle}{False}
% true = letter/tutorial
% false = research/review article
% (depending on journal)


%-------------------------------------------------------
%------ self-defined commands
\newcommand{\gy}[1]{\textcolor{magenta}{gy: #1}}
\newcommand{\sx}[1]{\textcolor{orange}{sx: #1}}
\newcommand{\rz}[1]{\textcolor{red}{rz: #1}}
% \newcommand{\cheng}[1]{\textcolor{red}{cheng: #1}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}


\usepackage{lineno}
\usepackage{threeparttable}
\usepackage{etoc}
\usepackage{multirow}
\usepackage{float}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{caption}

% \usepackage{xr}
% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
%   \typeout{(#1)}
%   \@addtofilelist{#1}
%   \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
% \makeatother

% \newcommand*{\myexternaldocument}[1]{%
%     \externaldocument{#1}%
%     \addFileDependency{#1.tex}%
%     \addFileDependency{#1.aux}%
% }
% \myexternaldocument{supplement}

\captionsetup{justification=justified}



% https://www.overleaf.com/learn/latex/Using_colours_in_LaTeX
\usepackage{xcolor} 
\definecolor{Colorfmod}{RGB}{100, 131, 245} % blue
\definecolor{Colorfdet}{RGB}{251, 161, 60} % orange
\definecolor{Colormfo}{RGB}{211, 82, 48} % orange
\definecolor{CustomBlue2}{RGB}{91, 129, 200}
\definecolor{CustomGreen}{RGB}{7, 179, 88}
% \usepackage{annotated_eq}
% \usepackage{flushend}
%------------------------------------------------------------

\linenumbers
\title{High-performance real-world optical computing trained by in situ model-free optimization}

% Optical computing through model-free in-situ training

\author[1,2,3]{Guangyuan Zhao}
\author[1,2]{Xin Shu}
\author[1,4]{Renjie Zhou}
% \author[1]{Mouse}


\affil[1]{Department of Biomedical Engineering, the Chinese University of Hong Kong,
Hong Kong, China}
% \affil[2]{School of Science, University of Technology, 2000 J St. NW, Washington DC, 20036}
% \affil[3]{School of Optics, University of Technology, 2000 J St. NW, Washington DC, 20036}
\affil[2]{Equal contribution}
\affil[3]{zhaoguangyuan@link.cuhk.edu.hk}
\affil[4]{rjzhou@cuhk.edu.hk}

\begin{abstract}

Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.

% Optical computing systems provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and the simulation-to-reality gap.
% % due to the simulator-based training process. 
% Recently, in situ training methods have been proposed to improve real-world performance by substituting the physics-based simulator with the physical systems for forward error calculation. However, they still encounter difficulties with model mismatch and resource-intensive simulations because of using a physics-based simulator to enable backward updates of parameters. 
% We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score-gradient estimation algorithm. 
% Our approach uniquely treats the optical computing system as a black box, back-propagates the loss directly to the probabilistic distributions of optical weights, and circumvents the need to construct a simulator of the system and acquire images for the input to the optical computing system in previous methods. 
% To prove the effectiveness, we experiment with a single-layer diffractive optical computing system. Our method experimentally achieves higher classification accuracy on the MNIST and FMNIST classification datasets and reduces the computation burden by avoiding resource-intensive wave propagation modeling. Moreover, we experimentally show the use of optical computing to enable the classification of white blood cells in a stain-free and image-free manner, which reveals the possibility of using optical computing for image-free and high-speed cell analysis. We believe that the proposed methods' simplicity, low computation resource requirement, and robustness  will facilitate the deployment of optical computing to real-world applications.
% Our method offers a versatile and efficient alternative for optimizing optical computing systems by emphasizing back-propagating optimization loss to the probabilistic distribution parameters of optical computing weights. 
% \gy{add application scope of MFO}
% Besides classfication datasets such as MNIST and FMNIST, we show. 
% The code of our work is available at xxx.github.com.
% Optical computing systems are promising machine learning hardware capable of energy-efficient, parallel computation. Today’s optical computation systems are mainly developed to perform optical inference after in silico training on digital simulators. However, various physical imperfections that cannot be accurately modeled may lead to the notorious “reality gap” between the digital simulator and the physical system.

% We present using the score gradient estimating algorithm as a robust model-free method to optimize the optical computing system without requiring correct knowledge of the system’s light transport. Our method can train the diffractive optical computing system and achieve high-accuracy classification results. Since our method does not require a model of actual light transport, it not only bypasses correct modeling of the system's light transport but also avoids the heavy computation of light transport.
\end{abstract}

\setboolean{displaycopyright}{False}

\begin{document}
\nolinenumbers

\maketitle

\section{Introduction}

Optical computing leverages the properties of light waves to facilitate high-speed data processing while reduces the energy cost~\cite{marechal1953filtre, cutrona1960optical,o1956spatial, ambs2010optical, wetzstein2020inference}, such as the optical spatial correlators~\cite{lugt1964signal, o1956spatial}, the optical edge detector~\cite{zhou2019optical}. 
 Recent advance in automatic differentiation has enabled in silico training of large-scale optical computing weights, giving rise to the realizations of diffractive neural networks~\cite{chang2018hybrid, lin2018all}, optical reservoir computing~\cite{rafayelyan2020large, verstraeten2007experimental} and coherent nanophotonic circuits~\cite{shen2017deep}. 
 
 % ~\cite{caulfield2010future, baydin2018automatic, chang2018hybrid, lin2018all, rafayelyan2020large, verstraeten2007experimental, shen2017deep, zhou2021large}. 
% Of particular relevance to our work is the diffractive optical computing systems, which offer large-scale parallel processing capability~\cite{lin2018all}.


Training optical computing systems presents two significant challenges: an intensive computational process and a performance disparity between simulation and reality when implementing pre-trained weights onto real-world systems~\cite{buckley2023photonic, lin2018all, rumelhart1986learning}.
The systems are typically trained in silico using differentiable simulators rooted in the first principle of optics, an approach known as simulator-based training (SBT). While SBT has proven effective within the confines of the simulator, the performance in real systems is largely contingent upon the simulator's fidelity. Factors such as misalignment and aberration, often omitted in simulations, can cause significant performance degradation when weights trained exclusively within the simulator are applied to real-world systems.
To bridge the simulation-to-reality gap, physics-aware training (PAT) and hybrid training (HBT) have been introduced ~\cite{wright2022deep, spall2022hybrid}. Both training strategies include conducting the forward pass in the real system and back-propagating loss through the simulator. 
%This in situ approach enables the training process to access the optical computing system during the forward computation. 
Consequently, the error measured in the real system during the forward pass refines weight optimization more accurately than strictly in silico training~\cite{buckley2023photonic}.

Despite these advances, there is a continued reliance on a physics-based simulator during the backward process in the current in situ training methods. Such a setting brings three drawbacks: Firstly, the biased simulator prohibits the above training process from achieving optimal results; Secondly, the in silico simulation requires large memory and computation, limiting the aforementioned methods from in situ training in edge devices with limited computing resources~\cite{sludds2022delocalized}; Lastly, the model-based training strategies require the input object to be fully visible during the training. Capturing the high-fidelity image of this object would impose burdens on the experiment.  
% #Medium
\begin{table}[ht]
\arrayrulecolor{black}
\fontsize{8pt}{8pt}\selectfont
    \centering
    \begin{tabular}{c|p{1.3cm}|p{1.3cm}|p{1.5cm}}
    \toprule
    &\textbf{SBT~\cite{lin2018all}} & \hfil\textbf{HBT}~\cite{spall2022hybrid} & \hfil\textbf {MFO (ours)}  \\\midrule
    % Physics engine~ & \checkmark \\
    In situ & \hfil\cellcolor{red!20} No & \hfil\cellcolor{green!20}Yes & 
    \hfil\cellcolor{green!20}Yes \\
    % Learned physics~ & & \checkmark & \checkmark \\
    % \hline
 Computation overhead & \hfil\cellcolor{red!20} High & \hfil\cellcolor{red!20} High &  \hfil\cellcolor{green!20} Low \\ 
 % \hline
    Model-free & \hfil\cellcolor{red!20} No & \hfil\cellcolor{red!20} No & \hfil\cellcolor{green!20} Yes  \\
    \bottomrule
    \end{tabular}
    \caption{{\textbf{Comparison of strategies on training optical computing systems} along the axes of in situ training capability (in situ), in silico computation overhead (computation overhead), and requirements on a physics-based simulator and knowledge of input objects (model-free).}}
    % The AFM method is slow in speed but has high lateral resolution and exhibits $2.5D$ imaging capability. Our work chooses AFM as the tool for characterization as the latter two are vital factors for the sub-diffraction-limited characterization of high-resolution OPD map in nano-prints.
    \label{tab: comparison of training methodlogies}
\vspace{-1em}
\end{table}



Here we propose an alternative solution to the above solutions requiring back-propagating errors through the simulator: an in situ, model-free optimization (MFO) method using a score gradient estimation algorithm~\cite{wierstra2014natural, williams1992simple} to solely use the forward outputs from the real system to get gradient for updating the weights of the optical computing system. As shown in Fig.~\ref{fig:mfo}, our method treats the optical system as a black box and back-propagates task-specific negative-loss as rewards to the source weight distributions (Fig.~\ref{fig:mfo}b). This process only requires knowledge of the weights and forward outputs of the optical computing system, unlike the SBT and HBT methods that require a model and the images of the input objects (Fig.~\ref{fig:mfo}a). A tabular comparison of our work and the prior methodologies is in Tab.~\ref{tab: comparison of training methodlogies}. 

We demonstrate our method on a home-built single-layer diffractive optical computing system. Experimental results show that our MFO method outperforms the result of hybrid training on the commonly used MNIST and FMNIST datasets~\cite{lin2018all, wright2022deep, spall2022hybrid} (Sec.~\ref{sec: results}\ref{subsec: results comparison on MNIST}). 
As a proof-of-concept demonstration, we experimentally show that our MFO-trained optical computing system can achieve classification of white blood cell phase maps with a testing accuracy of $73.8\%$ (Sec.~\ref{sec: results}\ref{subsec: classify WBC.}), making it a promising method for image-free and high-speed cell analysis. Lastly, we show our pipeline only consumes $\sim \frac{1}{1000}$ computing resources as compared to the HBT in situ methods by avoiding computation-intensive modeling of the wave propagation process (Sec.~\ref{sec: results}\ref{subsec: source efficient training by MFO.}).
% \vspace{-0.2cm}

% \vspace{-0.4cm}
% The history of optical computing is traced back to the early 1960s when the discovery of the laser spiked massive interest in exploiting Fourier transform relationships inherent in coherent optical imaging systems~\cite{ambs2010optical, wetzstein2020inference}. 
% Optical computing system uses light waves for data processing and exhibits advantages such as high-speed, low-energy cost compared to pure electrical computing~\cite{caulfield2010future}. 
% Recently, advance in the auto-differentiation~\cite{baydin2018automatic} has enabled in silico training of large-scale optical computing units, which spikes a revival of research in optical computing with various demonstrations, including the diffractive neural networks~\cite{chang2018hybrid, lin2018all}, reservoir computing~\cite{rafayelyan2020large, verstraeten2007experimental} and coherent nanophotonic circuits~\cite{shen2017deep}. 

% Though the inference speed of an optical computing system is fast, the training process of an optical system is computationally demanding and vulnerable to the sim2real gap\footnote{Sim2real gap was commonly used in robotics to describe the discrepancy between the performance of a robotic system in simulation (sim) and in real-world (real)~\cite{tobin2017domain}. 
% Here we use sim2real gap to illustrate the similar discrepancy in optical computing.} during deployment.
% Existing optical computing systems are often trained digitally on the computer~\cite{buckley2023photonic}. Take the diffractive optical computing system as an example~\cite{lin2018all}. 
% Before training, a differentiable simulator based on the first-principles theoretical knowledge from wave optics is set up. During in silico training on the computer, a single update includes forward propagating through the simulator, back-propagating through it, calculating gradients, and updating the weight matrices. After training, the optimized weights are uploaded to programmable optical devices such as spatial light modulators (SLM) for real-world testing. The in silico process shares the same philosophy as the standard "backpropagation" algorithm used in deep learning frameworks~\cite{rumelhart1986learning}. 
% The reliability of the fully used in silico training process hugely relies on the fidelity of the simulated optical system. 
% However, the physics-based simulator only models the system's primary components, such as modulation, free-space propagator, and detection. The misalignment, the aberrations, which exist in the real system while ignored in the simulation, degenerates the performance when we deploy the in silico trained weights to the real system. 

% Recently, Wright~\textit{et al.}~\cite{wright2022deep} and Spall~\textit{et al.}~\cite{spall2022hybrid} have proposed physics-aware training (PAT) and hybrid training for in situ training for the optical computing system, respectively. 
% Both works share the same philosophy of making the forward pass of optical computing conducted in a real system. At the same time, the loss is back-propagated through the physics-based simulator during the backward update process. 
% Such training strategy has achieved comparable accuracy as the expected result from simulation regarding the image classification tasks on relatively simple optical computing environments. 
% % This results in a bias-variance tradeoff between the biased first-order estimator and the often high-variance, yet unbiased zeroth-order estimator.

% However, the hybrid training relies on a physical simulation model biased version of the in the backward pass. The model mismatch between the simulations and experiments is not eliminated during backpropagation. The performance drops radically when the model mismatch between the simulations and experiments is significant. Moreover, the backward pass requires in silico simulation, where the complicated modeling of the physics process is both memory- and computation-demanding. For instance, modeling a single layer wave-propagation usually takes $\times10.8GB$ memory when we want a batchsize of 4, as we will also show in this paper
% % \gy{shuxin: also show this in the supplement} \sx{add in Table \ref{tab:simulation_training_parameter}, is it necessary to show the exact position in the text?}
% . This hinders the application when we want to train the optical computing system on edge devices~\cite{sludds2022delocalized}. 

% This work proposes using the score gradient estimating algorithm~\cite{wierstra2014natural, williams1992simple} as a lightweight model-free solution to optimize the optical computing system in situ. Our method treats the optical computing system as a black-box where only the learnable optical computing weights and the input/output pairs of the system are visible. Our method models the learnable optical weights as multivariate normal distributions, calculates the task-specific negative-loss as rewards, back-propagating directly from the rewards to the source weights distributions, and updates the distribution parameter in a gradient ascent manner.

% % Our results show that our method can generate an optimized hologram at a search space size of $96\times96$ and reconstruct a high-fidelity image  on the target plane. 
% Both simulation and experiment results show that the proposed method achieves comparable classification accuracy as the result we have in the ideal mode in simulation, where we assume that we have accurate modeling of the optical system. 
% Furthermore, we show that model-free optimization (MFO) is used to fine-tune the result of hybrid training. 
% Significantly, our model-free method eases the computation burden by circumventing the computation-heavy and memory-in-efficient modeling of wave propagation in our diffraction-based optical computing system. 
% Such feature is particularly useful when we want to deploy optical computing system on the edge devices.


% The calculated gradient is un-biased as all the forward and backward parameters are from 



% The update of the weight matrices is computed from the combined data obtained in these two processes. Because any physical system exhibits certain experimental imaccurateions that are hard to accurately model, ONNs trained in this way usually perform worse than expected [6,9,19]. To narrow this reality gap, one can incorporate simulated noise into the in silico training [7].


% However, one bottleneck of the current implementation of diffractive is 
% See \href{https://opg.optica.org/submit/templates/default.cfm}{Style Guide} and \href{https://opg.optica.org/submit/templates/default.cfm}{Manuscript Templates} pages for more details. Please select the appropriate journal abbreviation in the document preamble.

% If you have a question while using this template on {Overleaf}, please use the help menu (``?'') on the top bar to search for help or ask us a question using our \href{https://www.overleaf.com/contact}{contact form}.

% The system has been limited for has xxx. Training or simulation 
% Here we directly training the optical computing system with. We show such training is useful when we. 

% \section{Examples of Article Components}
% \label{sec:examples}

% The sections below show examples of different article components.

% \section{Related work}
% \subsection{In situ training of real optical computing system.} As we show in this paper. 
% \subsection{}

% \paragraph{Natural Evolutionary Strategy} On simple tasks. But not focus after the scattering media deal with the algorithm, most does also in the model-free way, such as the evolutionary algorithm. PSO SA, but they mostly restrict their result to be focusing a single point etc. Here 
% Mostly focus on a single point. Our work presents the result of large-scale holography. We show how the algorithms is extended to a evne larger. The closest related work to us is the work, we show the 

% % Figure removed
% Figure environment removed 

\section{Methodology}\label{sec: method}
In what follows, we detail the problem setup of training the optical computing system and our solution. We introduce background and problem formation in Subsec.~\ref{subsec: problem setup} and the conventional solution of using simulator-based training (SBT) in Subsec.~\ref{subsec: simulator-based design}. We then illustrate our solution to the problem, the model-free optimization (MFO) for training the optical computing system in Subsec.~\ref{subsec: mfo methodology}. Finally, we illustrate the optical computing system and its simulator in Subsect.~\ref{subsec: system description}, which we will use to demonstrate the performance of our method. 


\subsection{Problem setup} \label{subsec: problem setup}
% \gy{$x_i\in \mathbb{R}^{n_1\times n_1}$}
 We are interested in learning the optimal optical weights $w\in\mathbb{R}^{H}$ for the optical computing system on desired task with training dataset $\mathcal{D} = \{x_i, y_i\}_{i=1}^N$, where $N$ is the size of the dataset, $H$ is the number of trainable parameters in $w$, and $x$ and $y$ denote the input and target of interest, respectively. A function $f_{sys}(\cdot;w)$ maps $x\rightarrow y$ through this optical computing system with $w$. Specifically, in the image classification task based on the diffractive optical computing system we work on, $f_{sys}$ denotes the optical mapping from the input image $x$ to the output label $y$, and $w$ is the phase-valued optical computing weight. 
 % shown in Fig.~\ref{fig:mfo}.
 % We detail more about our system in subsection~\ref{subsec: system description}.
 
During training, we minimize the cost function $J(w)$ as the expected value across the entire training data set $\mathcal{D}$:

\begin{subequations}
\begin{align}
          w^{\star} &= \operatorname*{arg\,min}_w J(w), \\
          &= \operatorname*{arg\,min}_w  \mathbb{E}[\mathcal{L}(\mathcal{D}, w)], \\
          &= \operatorname*{arg\,min}_w \frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(f_{sys}(x_i; w), y_i), \label{eq:objective function}
\end{align}
\end{subequations}
% \begin{subequations}
where $\mathcal{L}$ is the task-specific loss function, we use cross-entropy loss~\cite{brier1950verification} since we deal with image classification tasks throughout this paper. 

We use gradient descent-based search to find optimal $w$ to minimize the objective function $J(w)$:
\begin{equation} \label{eq:grad descent}
    w = w - \alpha \nabla_{w} J(w),
\end{equation}
where $\nabla_{w}$ represents the gradient operator that collects all the partial derivatives of a function concerning parameters in $w$, and $\alpha$ is the learning rate. 

It is straightforward to use
the backpropagation method~\cite{rumelhart1986learning} to take the gradient through $f_{sys}$ and finds the gradient $\nabla_{w} J(w)$ as:
\begin{equation}
    \nabla_w J(w)= \frac{1}{N}\sum_{i=1}^N \nabla_w \mathcal{L}({f_{sys}(x_i, w)}, y_i),
    \label{eq: FOBG}
\end{equation}
when we have an accurate differentiable modeling of $f_{sys}$.
However, this is the case for digital neural networks, but not when we train a real-world optical computing system. \textbf{Thus, this paper's critical aim is finding an accurate gradient estimation of $\nabla_w J(w)$ to update optical computing weights $w$ in a real-world system}.

% Note that we use the whole dataset for the gradient update to simplify the illustration in Eq.~\ref{eq: FOBG}. In actual implementation, We use stochastic gradient descent (SGD)~\cite{lecun2015deep} and iterate through the whole dataset with the batch-gradient-based update.


% Figure environment removed

\subsection{In silico simulator-based training (SBT)} \label{subsec: simulator-based design}



\textbf{Back-propagate through the simulator $\hat{f}_{sys}$.} In a real-world optical computing system, since we do not have an exact functional expression of $f_{sys}$, the \textbf{simulator-based training} builds a simulator $\hat{f}_{sys}$ as the differentiable approximation of $f_{sys}$ for the simulation-based training (Fig.~\ref{fig:mfo}a). 
The naive training strategy is substituting $f_{sys}$ in Eq.~\ref{eq: FOBG} with the simulator $\hat{f}_{sys}$, applying in silico training on the simulator:

\begin{equation}
    \nabla_w J(w)= \frac{1}{N}\sum_{i=1}^N \nabla_w \mathcal{L}({\hat{f}_{sys}(x_i, w)}, y_i).
    \label{eq: sbt}
\end{equation}

Result of $\nabla_w J(w)$ is used in Eq.~\ref{eq:grad descent} to update the parameter $w$.
After training, the optimized $w$ is uploaded to the real optical computing system $f_{sys}$ to test the performance. 


\textbf{Simulation-to-reality gap.} The aforementioned simulator-based training is based on backpropagation through the simulator $\hat{f}_{sys}$. The "sim2real" gap is low (i.e., the gradient $\nabla_w J(w)$ is an accurate estimation) when the simulator $\hat{f}_{sys}$ is similar to $f_{sys}$. However, this assumption does not hold in many prototypes of optical computing systems where the inadequate modeling and misalignment between the optical elements decay the performance of the SBT during the "sim2real" transfer. We use a simulator described in Subsec.~\ref{subsec: system description} to assess the adverse effect of misalignment on the image classification results by measuring drops in classification accuracy when introducing various misalignments to a well-trained ideal optical computing system. For instance, we show with simulations in Fig.~\ref{fig: misalignment degrades the performance} that slightly laterally misaligning the optical computing layer by $41.1 \: \mu m$ reduces the classification accuracy by $31.2\%$.  
% Thus, \textbf{we aim to demonstrate a training strategy that does not suffer from the sim2real transfer and has the comparable imaging classification accuracy we have in the simulator}.

% The cost function $J$:

% \begin{equation}
% % \mathcal{F}(\distParams) := \int \dist(\vx; \distParams) \cost(\vx; \costParams) \intd\vx =
% % \expect{\dist(\vx; \distParams)}{\cost(\vx; \costParams)} .
% J(\theta) = \mathbb{E}_{\theta}[l_{\theta}(z)] = \int l(z) \pi(z\|\theta) dz
% \label{eq:expectation_function}
% \end{equation}


% \begin{equation}
%     r = R(\mathbf{y}) = \frac{\bar{\mathbf{y}}_{focus}}{\bar{\mathbf{y}}_{bg}},
% \end{equation}


\subsection{In situ model-free optimization (MFO)}\label{subsec: mfo methodology}
Our solution for solving the aforementioned "sim2real" gap issue in Subsec.~\ref{subsec: simulator-based design} is: in situ learning the optical weights $w$ with the model-free optimization. In situ learning, on the one hand, enables us to access the output of $f_{sys}$ and, on the other hand, in principle, feasible on the hardware requirement side as we can use spatial light modulators as programmable devices to update optical weights $w$. The challenging part is designing a training strategy that efficiently uses the actual system function $f_{sys}$ to construct an unbiased gradient estimator. Here, we use the score gradient estimator to calculate the gradient~\cite{schulman2015gradient, mohamed2020monte} for the backward update of parameters in $w$ while circumventing the construction of $\hat{f}_{sys}$, a biased and resource-intensive numerical modeling of $f_{sys}$. 

\textbf{Back-propagate through the weights distribution $p$.} In our score gradient estimation for model-free optimization, we model optical computing weights $w$ as a random variable that follows a parameterized distribution: $w\sim p(w|\theta)$ and rewrite the objective function in  Eq.~\ref{eq:objective function} as a probabilistic objective function:
\begin{subequations}
\begin{align}
        \operatorname*{arg\,min}_{\theta} J(\theta) 
        % &= \frac{1}{N}\sum_{i=1}^{N}l(f_{sys}(x_i, w), y_i)\\
          &= \operatorname*{arg\,min}_{\theta} \frac{1}{N}\sum_{i=1}^{N} \mathbb{E}_{p(w|\theta)}[\mathcal{L}(f_{sys}(x_i, w), y_i)],\\
          &= \operatorname*{arg\,min}_{\theta} \frac{1}{N}\sum_{i=1}^{N} \int {p(w|\theta)}\mathcal{L}(f_{sys}(x_i, w), y_i) \diff w.
\end{align}\label{eq: prob obj}
\end{subequations}

The probabilistic distribution $p(w|\theta)$ is continuous in its domain and differentiable concerning its distributional parameters $\theta$. 
Accordingly, the original goal of optimizing $w$ is reformulated as finding a most likely distribution $p(w|\theta)$ that minimizes the objective function in Eq.~\ref{eq: prob obj}. 
Specifically, in our work, we model the distribution $p$ as a multivariate normal distribution with $\theta = \{\mu, \sigma^2\}$ and $p(w|\theta) = \mathcal{N}(w;\mu, \sigma^2)$ as we optimize the continuous phase map to be uploaded onto the SLM. 


To update distribution parameter $\theta$ with the gradient descent Eq.~\ref{eq:grad descent}, we take the gradient on Eq.~\ref{eq: prob obj}:
\begin{subequations}
\begin{align}
        \nabla_{\theta} J(\theta) 
        &= \nabla_{\theta} \frac{1}{N}\sum_{i=1}^{N} \int {p(w|\theta)}\mathcal{L}(f_{sys}(x_i, w), y_i) \diff w ,\\
        &= \frac{1}{N}\sum_{i=1}^{N} \int \mathcal{L}(f_{sys}(x_i, w), y_i) \nabla_{\theta}{p(w|\theta)} \diff w ,\\
        &= \frac{1}{N}\sum_{i=1}^{N} \int p(w|\theta) \mathcal{L}(f_{sys}(x_i, w), y_i) \nabla_{\theta} \log{p(w|\theta)} \diff w \label{eq: prob obj int},\\ 
        &=\frac{1}{N}\sum_{i=1}^{N} \frac{1}{M}\sum_{j=1}^{M} \mathcal{L}(f_{sys}(x_i, w_j), y_i) \nabla_{\theta} \log{p(w_j|\theta)} \label{eq: prob obj monte carlo int},\\
        &=\frac{1}{M}\sum_{j=1}^{M} [\frac{1}{N}\sum_{i=1}^{N}  \mathcal{L}(f_{sys}(x_i, w_j), y_i)] \nabla_{\theta} \log{{p(w_j|\theta)}}, \label{eq: prob grad}
\end{align}
\end{subequations}
where $M$ is the number of samples we draw from the distribution $p(w|\theta)$. We get $r(w_j)=\frac{1}{N}\sum_{i=1}^{N} \mathcal{L}(f_{sys}(x_i, w_j), y_i)$ as the negative reward corresponding to each weight $w_j$ sampled from the distribution $p(w_j|\theta)$. Equation~\ref{eq: prob grad} is the score gradient estimator, 
where the score function is $\nabla_{\theta} \log{p(w_j|\theta)}$, which has been widely used in other areas, such as policy gradient algorithms in reinforcement learning~\cite{williams1992simple} and diffusion models~\cite{song2019generative}. 


% \sx{we do not do so}  \gy{write down how we did} 

\textit{Variance reduction.} The main risk of using such a gradient estimator is the high variance that comes from the Monte Carlo integration step that transits Eq.~\ref{eq: prob obj int} to Eq.~\ref{eq: prob obj monte carlo int}. The Monte Carlo integration step in Eq~\ref{eq: prob obj monte carlo int} approximates the integral value in Eq.~\ref{eq: prob obj int} by first drawing $M$ independent samples $\{w_j\}_{j=1}^M$ from the distribution $p(w_j|\theta)$ and then computing the average function evaluated in these samples. Such a sampling-based integration step has high variance because different sets of random samples may lead to significantly different integral estimates. We reduce the variance by subtracting the $r(w_j)$ with baseline value $\bar{r} = \frac{1}{M} \sum_{j=1}^M r(w_j)$:
\begin{equation}
    \nabla_{\theta} J(\theta) = 
    \frac{1}{M}\sum_{j=1}^{M} (r(w_j)- \bar{r}) \nabla_{\theta} \log{{p(w_j|\theta)}}. \label{eq: prob grad simplified}
\end{equation}

The subtraction of baseline value in Eq.~\ref{eq: prob grad simplified} reduces the variance of gradient estimation while keeping the bias of gradient unchanged~\cite{mei2023role}.

\textbf{Training recipe of MFO.} 
During training, we sample a batch of phase-valued optical computing weights $\{w_j\}_{j=1}^M$ from the distribution: $w_j \sim p(w_j|\theta)$. Then we upload the sampled weights onto the optical computing layer and test the weights with inputs from the dataset $\mathcal{D}$. After that, we calculate the rewards and update the distribution parameter $\theta$ through Eqs.~\ref{eq:grad descent} and~\ref{eq: prob grad simplified}. 
The algorithm iterates these steps until convergence. 
After minimizing the objective function Eq.~\ref{eq: prob obj}, we export $w_j$ with the smallest $r(w_j)$ tested on the validation set as the output result of $w^{\star}$. In practice, this performs better on training and validation sets than setting $w^{\star}$ as the sampled mean from the last batch of samples. 
The algorithmic overview of the training recipe is shown in Algorithm~\ref{alg:mfo}.

\begin{algorithm}
\caption{Algorithmic overview of MFO.}\label{alg:mfo}
\begin{algorithmic}[1]
\State \textbf{Input: }  {Classification dataset $\mathcal{D}=\{x_i, y_i\}_{i=1}^N$, learning rate $\alpha$, number of sampled weights $M$, optical computing system $f_{sys}$, distribution parameter $\theta=\{\mu, \sigma^2\}$, loss function $\mathcal{L}$, epochs $K$.}
\State \textbf{Output: }{Optimized optical computing weight $w^{\star}$.}

\For{$k$ in range $K$}
    \State{Sample $\{w_j\}_{j=1}^M$ from distribution $p(w_j|\theta)$.}
    % \State{ $\{w_j\}_{j=1}^M$}
    \State{\emph{$\triangleright$ in situ evaluate $\{w_j\}_{j=1}^M$.}}
    \For{$j$ in range $M$}\\
        \State{$r(w_j)\gets \frac{1}{N}\sum_{i=1}^{N} \mathcal{L}(f_{sys}(x_i, w_j), y_i).$}
    \EndFor
    \State{\emph{$\triangleright$ in silico update $\theta$.}}
    \State{Calculate $\Delta \theta$ via Eq.~\ref{eq: prob grad simplified}.}
    \State{$\theta \gets \theta -\alpha \Delta \theta.$ }
\EndFor
% \State $w^{\star}\gets \mu \text{ from } \theta$.
\State $w^{\star}\gets w_j$ with the smallest $r(w_j)$.
\end{algorithmic}
\end{algorithm}

% Figure environment removed


\subsection{Experiment and simulation detail of our diffractive optical computing system} \label{subsec: system description}

\textbf{Experimental setup of the real optical computing system $f_{sys}$.} We build our home-built single-layer optical computing system to validate the effectiveness of the proposed training strategy (see Fig.~\ref{fig: system skectch}). In the experimental setup, we have a laser field $u_{laser}$ incident onto the input layer with the object field $u_{obj}$. The light field reflects from the input layer onto the optical computing layer $u_w$ and is then detected by the sensor in the output layer. The object and optical computing weights are fulfilled using spatial light modulators (SLMs). Specifically, the length $d_{IC}$ and $d_{CO}$ for $Prop_{IC}$ and $Prop_{CO}$ in Fig.~\ref{fig: system skectch} are $215.1 mm$ and $201.6 mm$, respectively. The reproducibility of the hardware system is in Supplement Sec.~\ref{subsec: Reproducibility}. Since it is difficult to manually align multiple components with high accuracy and we have programmable optical devices, we use digital homography-enabled registration as compensation for manual alignment. More details of digital alignment are given in Supplement Sec.~\ref{subsec: calibration}.  

 
\textbf{Differentiable physics-based simulator $\hat{f}_{sys}$.} We construct an ideal physics-based simulator $\hat{f}_{sys}$ corresponding to the aforementioned optical computing system $f_{sys}$ as the sandbox to test different training algorithms. This simulator is also used inside the design loop of SBT and HBT, which serve as the baseline methods to compare with. Since our system only includes free-space wave propagation $\textcolor{red}{\hat{f}_{prop}}$, wavefront modulation $\textcolor{Colorfmod}{\hat{f}_{mod}}$, and sensor detection $\textcolor{Colorfdet}{\hat{f}_{det}}$, we build the optical computing simulator by stacking these three optical modules as building blocks.
The functions of optical modules are:
\begin{subequations} \label{eq: diff modules of system}
   \begin{align}
        &\textcolor{red}{\hat{f}_{prop}}(u_{in},z): u_{out} = \mathcal{F}^{-1} (\mathcal F(u_{in}) \times \mathcal F(h_{prop}(z))), \label{eq: free-space prop} \\
        &\textcolor{Colorfmod}{\hat{f}_{mod}}(u_{in}, u_{element}): u_{out} = u_{in} * u_{element},\\
        % &\textcolor{Colorfdet}{\hat{f}_{det}}(u_{in}):I_{out} = Imnoise(|u_{in}|^2), \label{eq: camera detection.}
        &\textcolor{Colorfdet}{\hat{f}_{det}}(u_{in}):I_{out} = |u_{in}|^2, \label{eq: camera detection.}
    \end{align}
\vspace{0.2cm}
\end{subequations}
where $h_{prop}(z) = \frac {e^{jkz'}}{j\lambda z'}e^{\frac{jk}{2z}(x'^2+y'^2)}$ is the propagation kernel under Fresnel approximation~\cite{born2013principles} with a propagation distance $z'$, wavelength $\lambda$ and angular wave number $k$, $u_{element}$ denotes the wavefront modulation from the programmable optical devices, $\mathcal{F}$ denotes the Fourier transform. 
% $Imnoise$ denotes the process of imposing camera noise. 

% Figure environment removed


Based on the modules in Eq.~\ref{eq: diff modules of system}, the simulator $\hat{f}_{sys} = \{\textcolor{Colorfmod}{\hat{f}_{mod}}, \textcolor{red}{\hat{f}_{prop}}, \textcolor{Colorfmod}{\hat{f}_{mod}}, \textcolor{red}{\hat{f}_{prop}}, \textcolor{Colorfdet}{\hat{f}_{det}}\}$ is constructed by chaining the building blocks ${1-5}$:
\begin{subequations}\label{eq: simuator}
    \begin{align}
        & 1. u_{out} = \textcolor{Colorfmod}{\hat{f}_{mod}}(u_{laser}, u_{obj}), \\
        & 2. u_{out} = \textcolor{red}{\hat{f}_{prop}}(u_{out}, d_{IC}), \\
        & 3. u_{out} = \textcolor{Colorfmod}{\hat{f}_{mod}}(u_{out}, u_{w}), \\
        & 4. u_{out} = \textcolor{red}{\hat{f}_{prop}}(u_{out}, d_{CO}), \\
        & 5. I_{cam} = \textcolor{Colorfdet}{\hat{f}_{det}}(u_{out}),
    \end{align} 
\end{subequations}
%where $d_{IC}$ denotes the distance between the input plane and the optical computing layer, $d_{CO}$ denotes the distance between the optical computing layer and the output plane. %$u_{laser}$, $u_{w}$, $u_{obj}$ correspond to the complex fields of the incident laser, the optical computing layer with weight $w$, and the input layer with object $x$ displayed to classify, respectively.


% \subsection{Training details.}

% The parameters of simulators are also calibrated



% % \begin{align}
%     \left \textcolor{red}{Propagator: }f_{prop}(u_{in},z): u_{out} = \mathcal{F}^{-1} (\mathcal F(u_{in}) \times \mathcal F(h_{prop}(z))), \right \\
%     \left \text{where } 
%     h(x,y; z) = \frac {e^{jkz}}{j\lambda z}e^{\frac{jk}{2z}(x^2+y^2)};\right \\
%     \left \textcolor{red}{Modulator: } f_{mod}(u_{in}, u_{element}): u_{out} = u_{in} * u_{elment}.\right
% % \end{align}
% \end{subequations}


\section{Results}\label{sec: results}

We experimentally evaluate the performance of our MFO method on the open-source MNIST and FMNIST datasets in Subsec.~\ref{subsec: problem setup}. We also demonstrate the MFO method on a novel application of stain-free classifying white blood cells in Subsec.~\ref{subsec: classify WBC.}. We then illustrate MFO's advantage of memory- and computation-efficient training in Subsec.~\ref{subsec: source efficient training by MFO.}.
% Lastly, we analyze the influence of sampling size $M$ on our method's final results and optimizable degree of freedom (DOF) $H$ in Subsection~\ref{subsec: MFO exhibits curse of dimensionality.}. 

% our solution to the problem, the model-free optimization for training the optical computing system in subsection~\ref{subsec: mfo}. Finally, we illustrate the optical computing system and its simulator in subsection~\ref{subsec: system description}, which we will use to demonstrate the performance of our method. 

% # and analysis of the training time cost\

\vspace{-0.1cm}
\subsection{MFO outperforms hybrid training (HBT) in the real systems on the MNIST and FMNIST datasets}\label{subsec: results comparison on MNIST}

We conduct experiments on training the single-layer optical computing system (described in Fig.~\ref{fig: system skectch}) on two classical image classification datasets: MNIST~\cite{deng2012mnist} and FMNIST~\cite{xiao2017fashion}.  We include in silico SBT and in situ HBT methods as the comparison baselines.  Table~\ref{tab: MNIST and FMNIST result} quantitatively shows that our method achieves higher classification accuracy than the HBT and SBT methods on both datasets in the experiments. The SBT method performs poorly due to the gap between the simulator and the real system. 
The HBT method suffers from the bias between $\hat{f}_{sys}$ and ${f}_{sys}$ in the backward process, while the MFO bypasses the bias-sensitive modeling and updates gradients solely with $f_{sys}$.
Figure~\ref{fig: MFO result} and Fig.~\ref{fig: HBT result} visualize some experimental outputs and confusion matrices using the MFO and HBT methods, respectively. 

\begin{table}[h!]
\newcolumntype {C}[1]{>{\centering\arraybackslash}m{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{C{0.16\linewidth} | C{0.072\linewidth} | C{0.072\linewidth}| C{0.072\linewidth}| C{0.072\linewidth}| C{0.072\linewidth}| C{0.072\linewidth}}
\hline
             &\multicolumn{3}{c|}{MNIST}          & \multicolumn{3}{c}{FMNIST} \\ \cline{2-7}
             & Train      & Val      & Test       & Train         & Val         & Test \\\hline\hline
\textcolor{gray}{Ideal}        & \textcolor{gray}{$92.7\%$}          & \textcolor{gray}{$84.3\%$}            & \textcolor{gray}{$82.2\%$}            & \textcolor{gray}{$85.6\%$}         & \textcolor{gray}{$79.9\%$}           & \textcolor{gray}{$76.4\%$} \\
\hline
\hline
SBT          & $81.9\%$         & $74.4\%$           & $69.3\%$         & $68.3\%$          & $64.1\%$           & $60.9\%$ \\
HBT          & $81.9\%$         & $75.5\%$           & $72.8\%$         & $68.3\%$          & $68.7\%$           & $65.8\%$ \\
MFO (Ours)         & $\textbf{83.3\%}$         & $\textbf{77.8\%}$           &$\textbf{73.6\%}$         & $\textbf{74.0\%}$          & $\textbf{71.1\%}$           & $\textbf{70.4\%}$ \\ 
% HBT+MFO      & $\textbf{87.0\%}$& $\textbf{80.2\%}$  & $\textbf{77.7\%}$ &$\textbf{xx.x\%}$  & $\textbf{xx.x\%}$   & $\textbf{xx.x\%}$  \\
\hline
\end{tabular}
\caption{\textbf{Performance comparison on MNIST and FMNIST datasets.} Results of the ideal mode are from the simulator, whose parameters are determined from experiments while we impose no misalignment on the simulator. The lower three rows are experimental results from SBT, HBT, and MFO. 
% The SBT method results in very low experimental accuracy due to misalignment. 
Our method outperforms the SBT and HBT methods on the MNIST and FMNIST datasets in experiments.
} 
\label{tab: MNIST and FMNIST result}
\vspace{-0.2cm}
\end{table}

See Supplement Sec.~\ref{sec: dataset} for the details of the MNIST and FMNIST datasets. Training details are in Supplement Sec.~\ref{subsec: training process}. The detailed description of the HBT method is in Supplement Sec.~\ref{subsec: baseline}. 
% Implementations of SBT and HBT methods have homography-based calibration in the loop.




% Figure environment removed

\subsection{Application: all-optical classification on cellular dataset.}\label{subsec: classify WBC.}

For the first time, we demonstrate the capability of an optical computing system for stain-free cell analysis, trained by our MFO algorithm (Fig.~\ref{fig: all-optical cell classfier}). We work on the white blood cells (WBC), whose abnormal subtype percentages indicate the immune system’s malfunction or infectious disease \cite{wbcfunction1, wbcfunction2}. We include details of the WBC phase map dataset in Supplement Sec.~\ref{sec: dataset}. Previously, researchers used machine learning methods to classify WBC subtypes, including monocyte, granulocyte, B cell, and T cell, by their morphology in a stain-free way~\cite{wbcbrightfield2019Nassae,wbcphase2021shu}. However, the analysis process is computationally heavy and time-consuming. 
Here, we accelerate the stain-free cell analysis process via computing with light. Our MFO method strikes a training/validation/testing classification accuracy of $72.1\%/73.3\%/75.8\%$ when classifying $4$ types of WBC, exceeding that of the HBT method (Fig.~\ref{fig: all-optical cell classfier}c). Furthermore, Fig.~\ref{fig: all-optical cell classfier}d shows that the inference enabled by optical computing is almost instantaneous ($\frac{d_{IO}+d_{OC}}{c}=1.4 \: ns$, where $c$ is the speed of light), compared to the $1.7\: ms$ of ResNet10, the electronic machine learning model used in \cite{wbcphase2021shu}. We currently need $1$ more milliseconds of in silico computing of region intensities corresponding to different classes to obtain the prediction. Such a step can be skipped if we use single-photon avalanche diode (SPAD)~\cite{bruschini2019single} point detectors to count the corresponding regions' cumulative signals. Though for now, the performance of our single-layer linear optical computing system is not on par with the electronic neural network, which hits a testing classification accuracy of $90.5\%$ \cite{wbcphase2021shu}, the potential of having ultra-high inference speed and $>70\%$ classification accuracy here point out an exciting direction on further increasing the complexity of our optical computing system to improve the absolute classification accuracy on classifying the cells.  
% \gy{@sx: still needs comparison of inference time and also include the comparison table (against HBT )in the supplement.}\gy{and also mention here.}
% \gy{shuxin, also add quantitative results for the test-time overhead compared to the deep learning-based work.} \sx{will finish on May 17} 
% Fig.~\ref{fig: all-optical cell classfier} further shows the classification results of the optical computing system using the MFO method for the WBC dataset.   
% Figure \ref{fig:false-color} shows an example figure. 



\subsection{Advantage: memory- and computation-efficient training enabled by MFO.}\label{subsec: source efficient training by MFO.}

% Figure environment removed

Our MFO method has an advantage over other training algorithms regarding GPU time and memory efficiency, besides the predicting accuracy discussed in the previous subsection. 
The SBT and HBT methods compute $\hat{f}_{sys}(x, w)$ and $\nabla_{w}\hat{f}_{sys}$ (Fig.~\ref{fig:mfo}a) for each input $x$ in silico, which requires a lot of in silico computation resources. 
In contrast, our MFO method executes the calculation of $f_{sys}(x, w)$ in the light-speed real optical computing system, but not in the computation- and memory-heavy simulator $\hat{f}_{sys}(x,w)$.
% the calculation of gradient in our MFO method relies on execution in the light-speed real optical computing system $f_{sys}$ but not in the computation- and memory-heavy simulator $\hat{f}_{sys}$. \sx{i do not think it is the calculation of gradient}
The only step of our MFO method that consumes in silico computational resources is the one described in Eq.~\ref{eq: prob grad simplified}, where we calculate the score gradient. The in silico computational resources consumption in this step is low because it only scales with the dimension of $w$ and has no relation to the complexity of the system's light transport $\hat{f}_{sys}$.

We compare our MFO method with HBT in GPU memory and time usage in Fig.~\ref{fig: resource consuming}. Our MFO method requires far less GPU time and memory than the HBT method during the training.
%\gy{shuxin: should have table show that MFO saves GPU time and memory.}
% The comparison does not include the memory and time used to transfer the stored image from CPU to GPU.
% The GPU handles all \textit{in silico} computation during training except for image storage from the camera.
% which consumes far less memory and computation than the HBT's backpropagation through the simulator. 




% \subsection{Optical computing with scattering media.}
% One key advantage of MFO training is that it does not rely on the modeling of the model. The computation and modeling overhead does not scale with the system's complexity. We, on the one hand, show such properties, and on the other hand, show the sharp feature in the scattering media will help improve the performance of the optical computing systems. 




% Fig.~\ref{fig: curse of dimensionality}(a) shows that a larger M leads to a higher MFO accuracy.







% % Figure environment removed








\section{Discussion}



\subsection{Limitation: MFO exhibits the curse of dimensionality}\label{subsec: MFO exhibits curse of dimensionality.}
Our method is not without its limits. Our MFO training relies on Monte Carlo integration. It thus inherits the \textit{curse of dimensionality} from the Monte Carlo integration~\cite{bellman1959adaptive}. That is, the number of samples $M$ needed to estimate the integration in Eq.~\ref{eq: prob obj int} with a given level of accuracy grows exponentially with respect to the $H$, the number of input variables (i.e., dimensionality) of the function.
This is also discussed and alleviated with \textit{variance reduction} in the previous Sec.~\ref{sec: method}C. However, the MFO strategy presented in
this paper is still sample-inefficient, though unbiased and memory-efficient. 
We need to either limit the number of trainable parameters $H$, which is also the search space size $H$ or sample a large number of varied optical computing weights $\{w\}^M_{j=1}$ from distribution $p(w_j|\theta)$ in every iteration to make MFO's gradient less noisy. 
The former limits the design DOF of our method, while the latter requires more executions on the real system, which prolongs the training time. 
% The training speed is heavily limited by the refreshing rate of the SLM performs as the optical computing layer (see further in Sec.\ref{}).

% Figure environment removed


We quantitatively investigate the influence of this limitation in Fig.~\ref{fig: curse of dimensionality} with a small dataset, where we visualize how $M$ and $H$ impact the training performance MFO. In the investigation, we limit the training dataset to $200$ samples from $4$ FMNIST classes, as the MFO training in the simulator takes a long time.  
% we conduct simulations with varying $M$ across different trials ( Fig.~\ref{fig: curse of dimensionality}a). we choose $M=128$ in our experiments to have a balance between training time and accuracy.
% We limit the training dataset to $200$ samples from $4$ FMNIST classes, as the MFO training in simulation takes a long time. 
% We only evaluate the training accuracy because we have limited training data, and our primary concern is the MFO's performance in finding $w^{\star} = \operatorname*{arg\, min}_w J(w)$ but not the generalization capability. 
As shown in Fig.~\ref{fig: curse of dimensionality}, MFO requires a $M>=128$ to achieve a training accuracy of $94\%$ given a search space size $H=128^2$. 
Moreover, our MFO method catastrophically fails when increasing the search space size $H$ beyond $128^2$ when keeping the sampling size $M$ fixed to $128$. 
% In this work, we choose $M=128$ and $H=128^2$.


% Increasing the sampling size $M$ allows for higher accuracy. However, this is at the cost of requiring more time for refreshing weights $w$ of the optical computing layer. We analyze the training time bottleneck induced by the refreshing rate of SLM in Supplement Sec.~\ref{subsec: time bottleneck from refreshing time}. 
% 


% \paragraph{Limitations}

% \begin{itemize}
%     \item Curse of dimensionality. Given the 'curse of dimensionality,' the existing implementation is efficient within a limited searching space (subsection~\ref{}). 
%     \item Our training algorithm Mask uploading time. This limitation coins with the previous one. In such 
% \end{itemize}

% % Figure environment removed


% \paragraph{Limitations} 

% As a model-free method, MFO gets stuck in a local minimum. Table.~\ref{tab: MNIST and FMNIST simulation result} shows that when the exact simulator $\hat{f}_{sys}$ is known in simulation, the simulator-based training (SBT) achieves slightly higher accuracy than our MFO method. However, in a real experiment, we cannot access $f_{sys}$, so our MFO method is the better solution.

% Though an unbiased and memory-efficient in situ training method, the model-free training strategy presented in this paper is intrinsically sample in-efficient. That is the  Monte-Carlo gradient estimator for updating the weights $w$  suffers from the curse of dimensionality. We must limit the search space size or increase the number of Monte Carlo samples in each iteration for an accurate gradient. The former limits the dimension of weights $w$ in the optical computing system, while the latter requires the training process to take many more in situ evaluations $M \times N$ in a real system, as we also have shown in Fig.~\ref{fig: curse of dimensionality}(b); the optimization fails when we increase the size of $w$ to be above 10000. This is solved in various ways in the research black-box optimization~\cite{}, and future research will enhance the sample efficiency. 

% Our method 

% Moreover, the current implementation of the optical computing system relies on off-the-shelf programmable optical elements such as SLMs. The form factor and cost of such devices hinder the deployment of optical computing systems. In the future, we envision that cost-effective and miniaturized programmable devices will make programmable optical computing systems more accessible. 

% \paragraph{Conclusion} 

% \paragraph{Conclusion}

\subsection{Future directions.}

Exploring the scalability of the optical computing system to more complex optical structures, along with integrating more layers and non-linear activation functions, could potentially enhance absolute performance. 

Future research could also consider employing more advanced techniques related to Monte Carlo integration to reduce the training variance discussed in the previous Subsec.~\ref{subsec: MFO exhibits curse of dimensionality.}, which we anticipate could substantially broaden the viable search space, thus further empowering the MFO approach. These include using more advanced sampling strategies~\cite{caflisch1998monte} or integrating MFO with the SBT methods~\cite{kurenkov2021guiding}. The latter makes a trade-off between the model bias and sampling variance.
 

\subsection{Conclusion}
To conclude, our study underscores the effectiveness of a model-free strategy in training optical computing systems in situ, manifesting considerable potential in computational efficiency and reducing the simulation-to-reality performance gap. Although the study does not focus entirely on absolute image classification accuracy as it is based on a simple single-layer diffractive optical computing system, it shows relative improvements compared to the existing training strategies, indicating that our strategy is a potentially valuable approach. The model-agnostic nature of our technique may become even more beneficial when implemented in intricate optical systems, representing a robust and versatile alternative to current strategies. It promises a strong foundation for exploring and practically implementing optical computing in real-world applications such as high-speed cell analysis. 
% This research opens the way to many photonic applications involving a contribution from the optical magnetic field, such as chiral light–matter interactions [12], photochemistry [16], manipulation of magnetic processes [38], and new schemes in quantum computing [39] or nonlinear processes [15], among others.

% Our MFO method is $0_{th}$ order method, which can be further combined with the $1_{st}$ order HBT 


% \section{Backmatter}
% Backmatter sections should be listed in the order Funding/Acknowledgment/Disclosures/Data Availability Statement/Supplemental Document section. An example of backmatter with each of these sections included is shown below.

\vspace{2pt}

\begin{backmatter}
\bmsection{Funding} 
Hong Kong General Research Fund (14209521); Hong Kong Innovation and Technology Fund (ITS/178/20FP \& ITS/148/20); Croucher Foundation (CM/CT/CF/CIA/0688/19ay).



\bmsection{Acknowledgments} 
We thank Cheng Zheng for the discussions in the early stage of the work.
% Acknowledgments should be included at the end of the document. The section title should not follow the numbering scheme of the body of the paper. Additional information crediting individuals who contributed to the work being reported, clarifying who received funding from a particular source, or other information that does not fit the criteria for the funding block may also be included; for example, ``K. Flockhart thanks the National Science Foundation for help identifying collaborators for this work.''
\paragraph{Author Contributions.} G.Z. conceived the project, derived the formulation, and built the backbone code and system. X.S. helped with the code writing and system setup and collected the simulation and experiment results. R.Z. supervised the project. G.Z. and X.S. wrote the manuscript with comments and edits from R.Z..

\bmsection{Data availability} 
% The MNIST data used in this study are available in the MNIST dataset [http://yann.lecun.com/exdb/mnist/]. 
Raw data underlying the results presented in this paper are not publicly available but can be obtained from the authors upon request.

\bmsection{Code availability} 
% The MNIST data used in this study are available in the MNIST dataset [http://yann.lecun.com/exdb/mnist/]. 
The code regarding this research will be released upon publication.



\bmsection{Disclosures} 
The authors declare no conflict of interest.
% Disclosures should be listed in a separate section at the end of the manuscript. List the Disclosures codes identified on the \href{https://opg.optica.org/submit/review/conflicts-interest-policy.cfm}{Conflict of Interest policy page}. If there are no disclosures, then list ``The authors declare no conflicts of interest.''

\smallskip


\bmsection{Supplemental document}
See Supplement 1 for supporting content. 

\end{backmatter}






% \section{References}

% Note that \emph{Optics Letters} and \emph{Optica} short articles use an abbreviated reference style. Citations to journal articles should omit the article title and final page number; this abbreviated reference style is produced automatically when the \emph{Optics Letters} journal option is selected in the template if you are using a .bib file for your references.

% However, full references (to aid the editor and reviewers) must be included as well on a fifth informational page that will not count against page length; again this will be produced automatically if you are using a .bib file.

% \bigskip
% \noindent Add citations manually or use BibTeX. See \cite{Zhang:14,OSA,FORSTER2007,testthesis,manga_rao_single_2007}.

% Bibliography
\bibliography{main_onn}


\clearpage

\newpage
\section*{Supplements}
\addcontentsline{toc}{section}{Supplements}
\renewcommand{\thefigure}{S.\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{S.\arabic{table}}
\renewcommand{\thesubsection}{S.\arabic{subsection}}
\setcounter{table}{0}
\setcounter{equation}{0}
\setcounter{subsection}{0}
\setcounter{algorithm}{0}
\renewcommand{\theequation}{S.\arabic{equation}}
\renewcommand{\thealgorithm}{S.\arabic{algorithm}}

\localtableofcontents

\subsection{Reproducibility of the hardware system} \label{subsec: Reproducibility}

\subsubsection{Hardware details of the optical computing system.}

% \gy{Shuxin: Also have a distance of propagators included.}

Table.~\ref{tab:device_type} lists models of critical components used in our home-built optical computing system. Table.~\ref{tab:device_param} further provides dimensions and frame rates of programmable optical devices (i.e., SLM1, SLM2 and DMD) and cameras involved in this work. The definition of the modulation area is shown in Fig.~\ref{fig:effective_shape}.


\begin{table}[h!]
\begin{threeparttable}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{P{0.22\linewidth} P{0.3\linewidth} P{0.28\linewidth}}
\hline
\textbf{Device} & \textbf{Part number} & \textbf{Manufacturer}\\
\hline
\rowcolor{gray!10}  SLM1        & PLUTO                & Holoeye\\
\rowcolor{white!10} SLM2        & GAEA                 & Holoeye\\
\rowcolor{gray!10}  DMD*        & DLPLCR6500EVM or DLP7000       & TI\\
\rowcolor{white!10} Camera          & BFS-U3-13Y3C-C       & Teledyne FLIR\\
\rowcolor{gray!10}  BS              & HBS11-025-50-VIS     & Hengyang Optics\\
\rowcolor{white!10} Polarizer       & CCM5-PBS201/M        & Thorlabs\\
\hline
\end{tabular}
\begin{tablenotes}
\item[*] The DMD is not experimentally used. We use its parameters to compare the MFO training time.    
\end{tablenotes}
\caption{\textbf{Models of key components used in our optical computing system.}}
\label{tab:device_type}
\end{threeparttable}
\end{table}

\begin{table}[h!]
\begin{threeparttable}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular} {P{0.12\linewidth} P{0.05\linewidth} P{0.19\linewidth} P{0.19\linewidth} P{0.15\linewidth}}
\hline
\textbf{Device} & \textbf{Pitch size ($\mu m$)} & \textbf{Full shape (pixels)} & \textbf{Modulation area shape (pixels)} & \textbf{Refreshing rate (FPS)} \\ \hline
\rowcolor{gray!10}  SLM1  & $8$       & $1080\times1920$     & $512\times512$      & $25$\tnote{*}\\
\rowcolor{white!10} SLM2   & $3.74$    & $2160\times3840$     & $1248\times1248$    & $30$\tnote{*}\\
\rowcolor{gray!10}  DMD        & $7.6$     & $1080\times1920$     & $512\times512$      & $1440$ \cite{reconfigurable_onn}\\
\rowcolor{white!10} Camera     & $8$       & $1024\times1280$     & $512\times512$      & $170$\\
\hline
\end{tabular}
\begin{tablenotes}
\item[*] We experimentally get 25 or 30 FPS in our self-written Python programs.
\end{tablenotes}
\caption{\textbf{Dimensions and frame rates of our experiments' programmable optical devices and cameras.}}
\label{tab:device_param}
\end{threeparttable}
\end{table}

% Figure environment removed

\subsubsection{Parameters of the optical computing system}
The optical computing weight $w$ comprises $H=128^2$ trainable parameters, and we use sampling size $M=128$ throughout the paper if not specifically specified. We obtain the classification prediction of the optical computing system by comparing the intensities of different regions in the output layer~\cite{reconfigurable_onn, lin2018all}, as illustrated in Fig.~\ref{fig:apply mask}. 

% Figure environment removed

\subsection{Training details} \label{subsec: training process}

\subsubsection{Computing resources.}
We evaluate our method both in simulation and in experiments. The simulation runs on a high-end Linux server. Physical experiments are conducted with a Windows desktop. 
% because the server lacks an interactive interface to calibrate the optical computing system. 
Table~\ref{tab:computer_configuration} shows the Windows desktop and Linux server configurations. 

\begin{table}[h!]
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{P{0.3\linewidth} P{0.29\linewidth} P{0.27\linewidth}}
\hline
\toprule
                                     & \textbf{Server (Simulation)}                     & \textbf{Desktop (Experiments)}\\ \hline
\rowcolor{gray!10} CPU               & Intel Xeon Silver 4210R [$\times$2] & Intel Core i7-7700\\
\rowcolor{white!10}GPU               & Nvidia A6000 [$\times$2]            & Nvidia TITAN XP\\ 
\rowcolor{gray!10}GPU memory         & $48GB$ [$\times$2]                    & $12GB$\\ 
\rowcolor{white!10}RAM               & $192GB$                               & $32GB$\\ 
\rowcolor{gray!10}Operating system   & Ubuntu 18.04                        & Windows 10\\ 
\rowcolor{white!10}Graphic interface & No                                  & Yes\\
\bottomrule
\end{tabular}
\caption{\textbf{Configurations of our server and desktop.}}
\label{tab:computer_configuration}
\end{table}
\subsubsection{Training parameters and time of simulation}

The training parameters and time of simulation are shown in Table.~\ref{tab:simulation_training_parameter}. In MFO simulation with a sampling size of optical computing weights as $M=128$, we can only process one input at a time due to GPU memory limitation. Training MFO is much slower than training the SBT since MFO requires $M=128$ sampled variants, and thus the forward computing in the simulator takes time. 
% We need $32\times 128$ times to process a batch of $B=32$ inputs and complete one training batch.

\begin{table}[h!]
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular} {P{0.55\linewidth} P{0.12\linewidth} P{0.12\linewidth}}
\hline
                                & \textbf{SBT} & \textbf{MFO}  \\ \hline
\rowcolor{gray!10}  Sampling size $M$& NAN           & $128$ \\
\rowcolor{white!10} Parallel computing batch size   & $32$             & $1$  \\
\rowcolor{gray!10}  Batch size $B$                  & $32$             & $32$ \\
\rowcolor{white!10} Optimizer                       & Adam             & SGD\\
\rowcolor{gray!10}  Learning rate                   & $0.01$           & $2$  \\
\rowcolor{white!10} Per-epoch training time         & $17s$            & $4m47s$\\
% \rowcolor{gray!10}  GPU memory usage                & 10.3GB         & 35.3GB\\
\hline
\end{tabular}
\caption{\textbf{Training parameters and time for simulation.} MFO is slow in simulation as the computational-intensive simulations substitute the light-speed forward optical computation and sampling.}
\label{tab:simulation_training_parameter}
\end{table}


\subsubsection{Training process for experiments}

The training process for MFO and HBT experiments consists of getting optical computing system output $f_{sys}(x,w)$ and performing in silico gradient calculation and parameter update. Experiment workflow.~\ref{workflow: HBT} shows how to get $\{f_{sys}(x_i, w)\}_{i=1}^B$ in the HBT experiment. 
In the MFO experiment, we use different experiment workflows to get $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$ depending on the specific devices used as input and optical computing layers. If we use SLM1 and SLM2 ($responsetime_I>responsetime_O$), we use Experiment workflow~\ref{workflow: MFO mask} for the sake of a shorter waiting time. If we use DMD and SLM2 ($responsetime_I>responsetime_O$), we use Experiment workflow~\ref{workflow: MFO input}. $B$ is the batch size, $M$ is the sampling size, $responsetime_I$ is the input layer device response time and $responsetime_O$ is the optical computing layer device response time. The training parameters and time for MFO and HBT are shown in Table~\ref{tab:experiment-training-time}.
% We use two workflows for MFO experiments: one updates the phase masks on the optical computing layer continuously (workflow.~\ref{workflow: MFO mask}), and the other updates the inputs continuously (workflow.~\ref{workflow: MFO input}). These two workflows have different waiting times when $responsetime_I \neq responsetime_O$, where $responsetime_I$ is the input layer device sleep time and $responsetime_O$ is the optical computing layer device sleep time. 
%The hardware response time is long due to the low SLM frame rate, especially for MFO. We can speed up the training by using a DMD with a $1440 fps$ frame rate as the input layer and following Workflow~\ref{workflow: MFO input} to get the hardware response. Table~\ref{tab:experiment-training-time-dmd} shows the estimated training time.


\begin{algorithm}[h!]
\floatname{algorithm}{Experiment workflow}
\caption{Acquiring $\{f_{sys}(x_i, w)\}_{i=1}^B$ in HBT experiment.}\label{workflow: HBT}
\begin{algorithmic}[1]
\State \textbf{Input: }  {A batch of classification dataset $\mathcal{D}_b=\{x_i, y_i\}_{i=1}^B$ with batch size $B$, optical computing weight $w$, optical computing system $f_{sys}$, input layer device response time $responsetime_I$, optical computing layer device response time $responsetime_O$.}
\State \textbf{Output: }{Optical computing system output $\{f_{sys}(x_i, w)\}_{i=1}^B$. 
}
\State{Refresh $w$ on the optical computing layer.}
\State{$\triangleright$ Wait $responsetime_O$.}  
\For{$i$ in range $B$}
    \State{Display $x_i$ on input layer.}
    \State{$\triangleright$ Wait $responsetime_I$.}
    \State{Capture $f_{sys}(x_i, w)$ with camera.}
\EndFor
\State{\textbf{Total waiting time}: $responsetime_O + B \times responsetime_I$.}
\end{algorithmic}
\vspace{-0.1cm}
\end{algorithm}

\vspace{-0.5cm}

\begin{algorithm}[h!]
\floatname{algorithm}{Experiment workflow}
\caption{Acquiring $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$ in MFO experiment when optical computing layer device has a short response time.}\label{workflow: MFO mask}
\begin{algorithmic}[1]
\State \textbf{Input:} {A batch of classification dataset $\mathcal{D}_b=\{x_i, y_i\}_{i=1}^B$ with batch size $B$, a group of sampled optical computing weights $\{w_j\}_{j=1}^M$, optical computing system $f_{sys}$, input layer device response time $responsetime_I$, optical computing layer device response time $responsetime_O$.}
\State \textbf{Output:} {Optical computing system output $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$.}
\For{$i$ in range $L$}
    \State{Display $x_i$ on input layer.}
    \State{$\triangleright$ Wait $responsetime_I$.}
    \For{$j$ in range $M$}
        \State{Refresh $w_j$ on the optical computing layer.}
        \State{$\triangleright$ Wait $responsetime_O$.}        
        \State{Capture $f_{sys}(x_i, w_j)$ with camera.}
    \EndFor
\EndFor
\State{\textbf{Total waiting time}: $B \times M \times responsetime_O + B \times responsetime_I$}
\end{algorithmic}
\vspace{-0.1cm}
\end{algorithm}

\vspace{-0.2cm}

\begin{algorithm}[!h]
\floatname{algorithm}{Experiment workflow}
\caption{Acquiring $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$ in MFO experiment when input layer device has a short response time.} \label{workflow: MFO input}
\begin{algorithmic}[1]
\State \textbf{Input: }  {A batch of classification dataset $\mathcal{D}_b=\{x_i, y_i\}_{i=1}^B$ with batch size $B$, a group of sampled optical computing weights $\{w_j\}_{j=1}^M$, optical computing system $f_{sys}$, input layer device response time $responsetime_I$, optical computing layer device response time $responsetime_O$.}
\State \textbf{Output: }{Optical computing system output $\{f_{sys}(x_i, w_j)\}_{i=1, j=1}^{B, M}$.}
\For{$j$ in range $M$}
    \State{Refresh $w_j$ on the optical computing layer}
    \State{$\triangleright$ Wait $responsetime_O$}
    \For{$i$ in range $L$}
        \State{Load $x_i$ to input layer}
        \State{$\triangleright$ Wait $responsetime_I$}        
        \State{Capture $f_{sys}(x_i, w_j)$ with camera}
    \EndFor
\EndFor
\State{\textbf{Total waiting time}: $M \times responsetime_O + B \times M \times responsetime_I$}
\end{algorithmic}
\vspace{-0.1cm}
\end{algorithm}

% Figure environment removed

We stop the training process when the validation loss does not decrease for $10$ epochs in both simulation and real system experiments. We choose the checkpoint with the lowest validation loss and use it to evaluate the test set. In Sec.4~\ref{subsec: MFO exhibits curse of dimensionality.}, however, we apply a different rule to make the model overfit on the training dataset. We stop the training process after 50 epochs of no decrease in the training loss when testing the effect of sampling size and after 10 epochs when testing the effect of number of trainable parameters, due to training time constraints. 


% \begin{table}[htp]
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
% \arrayrulecolor{black}
% \centering
% \renewcommand{\arraystretch}{1.0}
% \begin{tabular}{P{0.08\linewidth} P{0.15\linewidth} P{0.15\linewidth} P{0.25\linewidth}}
% \hline
%                         & Sample batch size (L)   & GPU memory usage    & GPU time\\\hline
% \rowcolor{gray!10}  SBT & 4        & 10.8GB                 & 254s  \\
% \rowcolor{white!10} HBT & 4        & 10.8GB                 & 254s \\
% \rowcolor{gray!10}  MFO & 32       & 8.3GB                  & 153s  \\
% \hline
% \end{tabular}
% \caption{GPU memory usage and GPU time.}
% \label{tab: memory and computation efficient training with MFO}
% \end{table}



\subsection{Compare the overall training time of MFO and HBT}~\label{subsec: time bottleneck from refreshing time}
We use the optical computing layer based on the conventional phase-SLM, with an experimentally achievable refreshing rate of about $30$ FPS (see Tab.~\ref{tab:device_param}). 
For now, refreshing weights $w$ on the optical computing layer consumes far more time than the light-speed computation of $f_{sys}(x, w)$. 
Given the current refreshing rate of optical computing layer, the overall training time of the MFO is larger than that of the HBT (see Tab.~\ref{tab:experiment-training-time}).
This time bottleneck can be alleviated when substituting the conventional phase-SLM with the fast ones, such as the Heavily-quantized Spatial Light Modulators~\cite{choi2022time} or the digital micromirror devices~\cite{ayoub2021high}, which exhibit thousands of FPS. In such a way, we can significantly reduce the overall training time of our method (Tab.~\ref{tab:experiment-training-time-dmd}). 


\begin{table}[h!]
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\centering
\begin{threeparttable}
\renewcommand{\arraystretch}{1.0}
\begin{tabular} {P{0.4\linewidth} P{0.16\linewidth} P{0.16\linewidth}}
\hline
% estimate at DMD frame rate of 1440Hz, and use a high speed camera
% HBT accu is 70%, in another exp, although accu is 65, but the time for in silicon is only 2m9s
& \textbf{MFO}      &\textbf{HBT} \\ \hline
\rowcolor{white!10}  Batch size $L$                                        & $32$           & $4$\\
\rowcolor{gray!10}   Sampling size $M$                       & $128$          & None\\
\rowcolor{white!10}  Optimizer                                          & SGD          & Adam\\
\rowcolor{gray!10}   Learning rate                                      & $100$          & $0.01$\\
\rowcolor{white!10}  \textit{In silico} computation time\tnote{*}   & $6s$           & $2m16s$\\
\rowcolor{gray!10}    Device waiting time for getting $f_{sys}(x,w)$\tnote{*}         & $1h56m01s$     & $2m18s$\\ 
\rowcolor{white!10}  Total training time\tnote{*}                      & $1h56m07s$     & $4m34s$\\
\hline
\end{tabular}
\begin{tablenotes}
\item[*] Time per epoch.
\end{tablenotes}
\end{threeparttable}
\caption{\textbf{Training parameters and training time for experiments. 
} As the optical computing time is negligible, the time here consists mainly of SLM pattern refreshing time and camera exposure time. MFO requires evaluating a lot of masks in the real system and thus cannot 
}
\label{tab:experiment-training-time}
\end{table}



\begin{table}[h!]
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{threeparttable}
\begin{tabular} {P{0.35\linewidth} P{0.16\linewidth} P{0.16\linewidth}}
\hline
% estimate at DMD frame rate of 1440Hz, and use a high speed camera
% HBT accu is 70%, in another exp, although accu is 65, but the time for in silicon is only 2m9s
& \textbf{MFO}      &\textbf{HBT} \\ \hline
\rowcolor{gray!10}  \textit{In silico} computation time\tnote{*}      & $6s$           & $2m16s$\\
\rowcolor{white!10}  Device waiting time for getting $f_{sys}(x,w)$\tnote{*}       & $3m43s$           & $2s$\\ 
\rowcolor{gray!10}   Total training time\tnote{*}                  & $3m49s$         & $2m18s$\\
\hline
\end{tabular}
\begin{tablenotes}
\item[*] Time for one epoch.
\end{tablenotes}
\end{threeparttable}
\caption{\textbf{Using Digital Micromirror Device (DMD) as the input layer can speed up the MFO training. } 
We only estimate the training time by calculation when using DMD as the input layer device and SLM2 as the optical computing layer device. With $N=1000$, $B=32$, and $M=128$, we use the same \textit{in silico} time as in Tab.~\ref{tab:experiment-training-time}. While getting $f_{sys}(x,w)$, $responsetime_I = 1000 / 1440Hz = 0.69ms$, $responsetime_O = 1000 / 30Hz = 33ms$. We use Experiment workflow.~\ref{workflow: HBT} in the HBT experiment and Experiment workflow.~\ref{workflow: MFO input} in the MFO experiment. The total device waiting time for each batch is $33ms+32\times0.69ms=55.08ms$ in the HBT experiment and $128\times33ms+32\times128\times0.69=7050.24ms$ in the MFO experiment, except for the last batch with $8$ samples, which has a device waiting time of $33ms+8*0.69ms=38.52ms$ and $128\times33ms+8\times128\times0.69=4930.56ms$, respectively. There are $32$ batches in total. The total per-epoch device waiting time for getting $f_{sys}(x,w)$ in the HBT experiment is about $31\times55.08ms+38.52ms\approx2s$, while in the MFO experiment it is about $31\times7050.24ms+4930.56ms\approx3m43s$.}
\label{tab:experiment-training-time-dmd}
\end{table}



\subsection{Dataset details}\label{sec: dataset}
\paragraph{MNIST and FMNIST} 
We randomly split $2000$ digits ($200$ per class) from the MNIST or FMNIST dataset into training and validation sets of equal size. We additionally use $1000$ digits (100 per class) for testing.
\paragraph{WBC dataset}
The WBC phase image dataset \cite{wbcphase2021shu} has four classes of cells: granulocyte, monocyte, B lymphocyte, and T lymphocyte. Table~\ref{tab: WBC dataset} gives the number of cells in each class. 
% We use size and dry mass \cite{drymass} features to classify WBCs with an support vector machine (SVM) \cite{svm} and achieve an accuracy of $85.5\%$.

\begin{table}[htp]
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\arrayrulecolor{black}
\centering
\renewcommand{\arraystretch}{1.0}
\begin{tabular} {P{0.3\linewidth} P{0.15\linewidth} P{0.15\linewidth} P{0.15\linewidth}}
\hline
                & \textbf{Training  set} & \textbf{Validation set} &\textbf{Testing set}\\ \hline
\rowcolor{gray!10}  Granulocyte     & $465$      & $100$           & $100$\\
\rowcolor{white!10} Monocyte        & $513$      & $100$           & $100$\\
\rowcolor{gray!10}  B lymphocyte    & $437$      & $100$           & $100$\\
\rowcolor{white!10} T lymphocyte    & $330$      & $100$           & $100$\\
\rowcolor{gray!10}  Total           & $1745$     & $400$           & $400$\\
\hline
\end{tabular}
\caption{\textbf{Number of cells of each class in the WBC dataset.}}
\label{tab: WBC dataset}
\end{table}



% % Figure environment removed




\subsection{Calibration of the system} \label{subsec: calibration}
We calibrate the optical computing system shown in Fig.~\ref{fig: system skectch}. The calibration is critical for reproducing baseline methods that rely on the model $\hat{f}_{sys}$ as tiny misalignment between multiple planes will hugely degrade the performance, as we have shown in Fig.~\ref{fig: misalignment degrades the performance}. 
% For example,  we can not tell how the light reflecting from the first SLM falls onto the second SLM with our naked eyes.
Our digital alignment consists of estimating $2$ sets of parameters: lengths $\{d_{IO}, d_{CO}\}$ and homography transformations $\{H_{IO}, H_{CO}\}$. 
Specifically, $H_{IO}$ is the projective transformation between the input layer and output layer, $d_1+d_2$ is the corresponding distance, $H_{CO}$ is the projective transformation between the optical computing layer and output layer, and $d_2$ is the corresponding distance. 



% Robustness to lighting and
% viewpoint change is improved with a feature-metric loss based on CNN features. In our
% outer loop, we train a CNN to produce robust features for image alignment.
\subsubsection{Calibrate propagation distance z using holography.}
We first use free-space holography~\cite{blinder2022state} to calibrate the distance $d$ between planes. We concatenate the building blocks Eq.~\ref{eq: free-space prop} and~\ref{eq: camera detection.} and construct a free-space holography simulator $\hat{f}_{holo}=\{\hat{f}_{prop}, \hat{f}_{det}\}$. 
The input to the holography simulator is $u_{in}=e^{j\phi_{holo}}$ and $d$, where $\phi_{holo}$ is the pre-calculated phase that generates the hologram, while $d$ is the distance to optimize. 
Calculation of $\phi_{holo}$ is achieved by extracting the phase of Fourier transform of the target obj $I_{target}$:
\begin{equation}
    \phi_{holo} = \text{angle}(\mathcal{F}(\sqrt{I_{target}}))
\end{equation}
Then we upload the $\phi_{holo}$ onto the real SLM with the Holoeye Slideshow software, add various defocus phase masks corresponding to various $d$, visually compare the quality of the output hologram and select the best fit $d$ as the calibrated distance.  

\subsubsection{Homography estimation among multiple planes.}
Homography is a linear transformation between corresponding points in
two images with an $8$ degree of freedom~\cite{riba2020kornia}. It is solved by minimizing dense photometric loss or by feature matching. 
Our experiment has two SLMs and a camera (Fig.~\ref{fig: system skectch}). Since directly calibrating $H_{IC}$, the transformation between the two SLMs, is infeasible as we cannot put the camera on any plane of these two SLMs; we calibrate their transformations to the output plane (the camera plane), respectively. Take the estimation  of photometric-based homography between the input layer and the output layer as an example. The objective function is:
\begin{equation}
    % H_{IO}^{\star} =  \operatorname*{arg\,min}_{H_{IO}} \|\sqrt{I_{target}}-warp(|\hat{f}_{prop}(u_{in}, d_{IO}, H_{IO})\|^2,
    H_{IO}^{\star} =  \operatorname*{arg\,min}_{H_{IO}} \mathcal{L}_{homo}(\sqrt{I_{target}}-warp(|\hat{f}_{prop}(u_{in}, d_{IO})|),
\end{equation}
where $warp$ is the homography warpper given the homography estimation $H_{IO}$. We solve this objective function with auto-differentiation~\cite{riba2020kornia}. 


% Briefly, the planar homography relates the transformation between two planes (up to a scale factor $s$)~\cite{opencv_library}:
% \begin{equation}
%     s \begin{bmatrix}
%         x'\\
%         y'\\
%         1
%     \end{bmatrix} = 
%     \begin{bmatrix}
%     h_{11} & h_{12} & h_{13}\\
%     h_{21} & h_{22} & h_{23}\\
%     h_{31} & h_{32} & 1
%     \end{bmatrix}
%     \begin{bmatrix}
%         x \\
%         y\\
%         1
%     \end{bmatrix}
%     = H 
%     \begin{bmatrix}
%         x \\
%         y \\
%         1
%     \end{bmatrix}
% \end{equation}




% The SLMs are denoted as pluto $P$ and gaea SLM $G$ respectively.  
% The lightfield passes through the two SLMs and the camera sequentially. 
% Normally, the matrix transformation between the spatial light modulator

% \begin{equation}
%     a = 
%     \begin{bmatrix}
%     1 & 2 & 3\\
%     a & b & c
%     \end{bmatrix} \times \begin{bmatrix}
%         x \\
%         y\\
%         1
%     \end{bmatrix}
% \end{equation}

% \begin{equation}
%     \begin{bmatrix}
%         x'\\ \theta'        
%     \end{bmatrix}
%     = 
%         \begin{bmatrix}
%         1& d\\
%         0 & 1
%     \end{bmatrix} \times 
%     \begin{bmatrix}
%         x\\ \theta        
%     \end{bmatrix}
% \end{equation}


% \begin{equation}
%     \begin{bmatrix}
%         x'\\ \theta'        
%     \end{bmatrix}
%     = 
%         \begin{bmatrix}
%         1& d\\
%         0 & 1
%     \end{bmatrix} 
%     \times 
%     \begin{bmatrix}
%         x\\ \theta        
%     \end{bmatrix}
% \end{equation}

% \subsection{Implementation of hybrid training.} Since the original training



\subsubsection{Rebuild the simulator with homography estimation in the loop.} 
In the previous subsection, we discuss estimating the homography transformation between planes in the real system.

To incorporate the calibrated homography matrices into our system simulator $\hat{f}_{sys}$, we rebuild the simulator with three virtual propagators instead of two. The latter is discussed in the main text and depicted in Fig~\ref{fig: three_virtual_prop}(a). We use a 3-layer simulator because the homography wrapper $warp$ and the wave propagator $\hat{f}_{prop}$ are not commutative. In other words:
\begin{equation}
    warp(\hat{f}_{prop}(u_{in}, d_{IO}), H_{IO}) \neq \hat{f}_{prop}(warp(u_{in}, H_{IO}), d_{IO}). 
\end{equation}

A sketch of the rebuilt simulator is in Fig.~\ref{fig: three_virtual_prop}(b). We introduce a virtual output plane $O_{virtual}$ into the simulator so that we can incorporate the homography estimations $H_{IO}$ and $H_{CO}$ into the simulation loop. Since the homography matrix is estimated at the output planes of the optical propagator, the settings in Fig.~\ref{fig: three_virtual_prop}(b) enable the use of homography matrix at the output planes of the propagator, which cannot be done via using the original simulator in Fig.~\ref{fig: three_virtual_prop}(a). 

% Figure environment removed


\subsection{Baseline method -- Hybrid training.} \label{subsec: baseline}
% We include two baseline methods in the paper. The first one is hybrid training (HBT)/physics-aware training. The other one is the genetic algorithm, another model-free algorithm that is commonly used in wavefront engineering work. Note that for the HBT method, we need to do digital calibration in the above subsection beforehand as otherwise, the performance would drop a lot. 

We include the hybrid training (HBT)/physics-aware training as one of the baseline methods to compare~\cite{spall2022hybrid, wright2022deep}.
% \paragraph{Hybrid training.}
% The forward pass of training utilizes the computing from the real pass. 
The philosophy of hybrid training is simple: when updating the optical computing system, we use the real system to do the forward pass to get the value of the objective function $J(w)$:
% The forward pass:
\begin{equation}
    J(w) = \frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(f_{sys}(x_i; w), y_i).
\end{equation}

In the backward update pass, we substitute the $f_{sys}$ with its differentiable simulator $\hat{f}_{sys}$ and update the weights $w$ via:
\begin{equation}
    w =  w + \alpha \frac{\partial J(w)}{\partial \hat{f}_{sys}} \frac{\partial \hat{f}_{sys}}{w}.
\end{equation}



This trick enables the backward pass in the biased while differentiable simulator $\hat{f}_{sys}$. The critical difference between the hybrid training (HBT) method and the simulator-based training (SBT) is that the former does the forward pass in the real system ${f}_{sys}$ while the latter conducts both passes in the simulator ${\hat{f}}_{sys}$.

We visualize some experimental outputs and confusion matrices using HBT in Fig.~\ref{fig: HBT result}.

% \subsection{Simulation result for MNIST and FMNIST datasets}

% \begin{table}[h!]
% \newcolumntype {C}[1]{>{\centering\arraybackslash}m{#1}}
% \arrayrulecolor{black}
% \centering
% \renewcommand{\arraystretch}{1.0}
% \begin{tabular}{C{0.08\linewidth} | C{0.08\linewidth}  C{0.08\linewidth} C{0.08\linewidth} | C{0.08\linewidth} C{0.08\linewidth} C{0.08\linewidth}}
% \hline
%        &\multicolumn{3}{c|}{\textbf{MNIST}}          & \multicolumn{3}{c}{\textbf{FMNIST}} \\ \cline{2-7}
%        & Train      & Val         & Test           & Train          & Val         & Test \\\hline
%  \textcolor{gray}{Ideal}    & \textcolor{gray}{91.5\%}     & \textcolor{gray}{84.9\%}      & \textcolor{gray}{81.7\%}         & \textcolor{gray}{85.6\%}         & \textcolor{gray}{79.9\%}      & \textcolor{gray}{76.4\%} \\
% \rowcolor{white!10} MFO    & 76.9\%      & 71.2\%     & 66.7\%         & 71.3\%         & 68.3\%      & 65.0\%  \\\hline
% \end{tabular}
% \caption{\textbf{Classification accuracy in simulation.} In the simulation, we compare the performance of our method against simulator-based training (SBT). Not knowing the simulator $\hat{f}_{sys}$, MFO works a little worse than the SBT method.\sx{mfo result need update}}
% \label{tab: MNIST and FMNIST simulation result}
% \end{table}



% Notably, in this work, we use a $1$ line implementation:

% \subsection{WBC classification experiment results}
% \begin{table}[h!]
% \newcolumntype {C}[1]{>{\centering\arraybackslash}m{#1}}
% \arrayrulecolor{black}
% \centering
% \renewcommand{\arraystretch}{1}
% \begin{tabular}{C{0.25\linewidth}  C{0.14\linewidth}  C{0.14\linewidth}  C{0.14\linewidth}  C{0.14\linewidth}}
% \hline
% \textbf{Method} & \textbf{Train accuracy} & \textbf{Val accuracy} & \textbf{Test accuracy}\\
% \hline
% \textcolor{gray}{Ideal}  & \textcolor{gray}{$82.1\%$}            & \textcolor{gray}{$82.8\%$}           & \textcolor{gray}{$81.5\%$}\\
% % \rowcolor{white!10} Sim  & MFO  & 79.9\%            & 80.5\%            & 77.0\% \\
% %Real & SBT  & xx.x\%      & xx.x\%  & xx.x\%\\
% HBT  & $59.5\%$            & $63.3\%$            & $62.0\%$\\
% MFO  & $\textbf{72.1\%}$   & $\textbf{73.3\%}$   & $\textbf{73.8\%}$\\\hline
% \end{tabular}
% \caption{\textbf{WBC classification accuracy in experiments.}}
% \label{tab: wbc result}
% \end{table}


% \subsection{WBC classification inference time}

% Figure~\ref{fig: inference time} shows that the optical computing system has a lower inference time ($1ms$) than ResNet10~\cite{wbcphase2021shu, he2016resnet} ($1.7ms$) on the WBC dataset. The inference time only includes the calculation time and excludes the image-capturing time. 

% % Figure environment removed


% \paragraph{Genetic algorithm.}

% Population-based
% algorithms have been
% successfully used.
% We also compare our method against the genetic algorithm, another well-known population-based model-free algorithm that has been used in  
% model-free optimization methods for wavefront engineering. Though the demonstration of the genetic algorithms on optical computing is sparse, we include in this work for comparison to demonstrate of the efficiency of using a zeroth-order gradient for optimization.


% \subsection{Details of derivation}



% The trick of log-likelihood:
% \begin{equation}
%     \nabla_{\theta} \log p(w|\theta) = \frac{\nabla_{\theta} p(w|\theta)}{p(w|\theta)}
% \end{equation}


% The ZoBG is also referred to as the REINFORCE~\cite{schulman2015gradient, mohamed2020monte}, score function, or the likelihood-ratio gradient.

% black-box optimization (BBO) {To clarify, the black box in terms of optimization is different from the black-box in terms of modeling, which is often referred to in deep learning works. the former consider the case that the structure model of the underlying environment is unknown; the latter considers the case that the structure of the environment is represented by models that can not be interpreted.} considers the design and analysis of algorithms for problems where the structure of the objective function and/or the constraints defining the set is unknown, unexploitable, or non-existent.


% The black-box optimization method is 0-th order gradient~\cite{mohamed2020monte}. 
% The optical system
% We model the desired mask $\mathbf{x}_{\phi}$ as multivariate normal distribution and use \textit{gradient-based} distribution search $\pi_{\phi}$ to find the optimal distribution that maximizes the reward $r$.
% The update is achieved through gradient ascent on the partial derivative of score function w.r.t. $\phi$~\cite{schulman2015gradient}:







% \begin{equation}\label{eq:pg}
%     \Delta \phi = \frac{1}{K}\sum_{i=1}^K \alpha (r_i-\bar{r}) \frac{\partial \log(p(x_i|\pi_{\phi}))}{\partial \phi},
% \end{equation}
% where $\phi$ is the optimizable parameter corresponding to the distribution of the mask, $\alpha$ is the step size, $K$ is the number of masks of the batch, $r$ is the reward by taking $x_i$ as the input and $p(x_i|\pi_\phi))$ is the probability of taking mask $x_i$ given policy $\pi_\phi$. The algorithm subtracts every reward $r_i$ with a baseline $\bar{r} = \frac{1}{K} \sum_{i=1}^K r_i$ to reduce the variance of gradient estimation while keeping the bias of gradient unchanged.

\end{document}
