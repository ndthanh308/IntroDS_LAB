% BIBTEX format file for all bibliography entries

%%% URL sources
@misc{hlegAI,
	title        = {Ethics guidelines for trustworthy AI},
	author       = {AI HLEG},
	year         = 2019,
	url          = {https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai},
	urldate      = {22.03.2023}
}
%%%%% PSYCHOLOGY PAPERS %%%%%
@book{.2001,
	title        = {Trust in Cyber-societies},
	year         = 2001,
	publisher    = {{Springer, Berlin, Heidelberg}}
}
@misc{Lacker.2020,
	title        = {Giving GPT-3 a Turing Test},
	author       = {Lacker, Kevin},
	year         = 2020,
	url          = {https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html},
	urldate      = {18.12.2022}
}
@article{rudin2019stop,
	title        = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	author       = {Rudin, Cynthia},
	year         = 2019,
	journal      = {Nature Machine Intelligence},
	publisher    = {Nature Publishing Group},
	volume       = 1,
	number       = 5,
	pages        = {206--215}
}
@article{Fang.2015,
	title        = {Multi-faceted trust and distrust prediction for recommender systems},
	author       = {Fang, Hui and Guo, Guibing and Zhang, Jie},
	year         = 2015,
	journal      = {Decision Support Systems},
	volume       = 71,
	pages        = {37--47},
	doi          = {10.1016/j.                  dss.2015.01.005},
	issn         = {0167-9236}
}
@article{kaplan2023trust,
	title        = {Trust in artificial intelligence: Meta-analytic findings},
	author       = {Kaplan, Alexandra D and Kessler, Theresa T and Brill, J Christopher and Hancock, PA},
	year         = 2023,
	journal      = {Human factors},
	publisher    = {SAGE Publications Sage CA: Los Angeles, CA},
	volume       = 65,
	number       = 2,
	pages        = {337--359}
}
@article{Kruger.2021,
	title        = {TVA in the wild: Applying the theory of visual attention to game-like and less controlled experiments},
	author       = {Kr{\"u}ger, Alexander and T{\"u}nnermann, Jan and Stratmann, Lukas and Briese, Lucas and Dressler, Falko and Scharlau, Ingrid},
	year         = 2021,
	journal      = {Open Psychology},
	volume       = 3,
	number       = 1,
	pages        = {1--46},
	doi          = {10.1515/psych-2021-0001},
	issn         = {2543-8883},
}
@article{Lehtonen.2019,
	title        = {The multiple faces of trust in statistics and indicators: A case for healthy mistrust and distrust},
	author       = {Lehtonen, Markku},
	year         = 2019,
	journal      = {Statistical Journal of the IAOS},
	volume       = 35,
	number       = 4,
	pages        = {539--548},
	doi          = {10.3233/SJI-190579},
	issn         = {1874-7655},
	url          = {https://content.iospress.com/articles/statistical-journal-of-the-iaos/sji190579}
}
@article{Lewicki.1998,
	title        = {Trust And Distrust: New Relationships and Realities},
	author       = {Lewicki, Roy J. and McAllister, Daniel J. and Bies, Robert J.},
	year         = 1998,
	journal      = {Academy of Management Review},
	volume       = 23,
	number       = 3,
	pages        = {438--458},
	doi          = {10.5465/amr.1998.926620},
	issn         = {0363-7425}
}
@article{Mayer.2011,
	title        = {Suspicious spirits, flexible minds: when distrust enhances creativity},
	author       = {Mayer, Jennifer and Mussweiler, Thomas},
	year         = 2011,
	journal      = {Journal of Personality and Social Psychology},
	volume       = 101,
	number       = 6,
	pages        = {1262--1277},
	doi          = {10.1037/a0024407},
	issn         = {1939-1315},
}
@article{Mayer.1995,
	title        = {An Integrative Model Of Organizational Trust},
	author       = {Mayer, Roger C. and Davis, James H. and Schoorman, F. David},
	year         = 1995,
	journal      = {Academy of Management Review},
	volume       = 20,
	number       = 3,
	pages        = {709--734},
	doi          = {10.5465/amr.1995.9508080335},
	issn         = {0363-7425},
}
@article{Mayo.2015,
	title        = {Cognition is a matter of trust: Distrust tunes cognitive processes},
	author       = {Mayo, Ruth},
	year         = 2015,
	journal      = {European Review of Social Psychology},
	volume       = 26,
	number       = 1,
	pages        = {283--327},
	doi          = {10.1080/10463283.2015.1117249},
}
@article{McKnight.2004,
	title        = {Dispositional Trust and Distrust Distinctions in Predicting High- and Low-Risk Internet Expert Advice Site Perceptions},
	author       = {McKnight and Kacmar and Choudhury},
	year         = 2004,
	journal      = {e-Service Journal},
	volume       = 3,
	number       = 2,
	pages        = 35,
	doi          = {10.2979/esj.2004.3.2.35},
	issn         = {1528-8226}
}
@article{Kruger.2017,
	title        = {Measuring and modeling salience with the theory of visual attention},
	author       = {Kr{\"u}ger, Alexander and T{\"u}nnermann, Jan and Scharlau, Ingrid},
	year         = 2017,
	journal      = {Attention, Perception, {\&} Psychophysics},
	volume       = 79,
	number       = 6,
	pages        = {1593--1614},
	doi          = {10.3758/s13414-017-1325-6},
	issn         = {1943-393X},
	url          = {https://link.springer.com/article/10.3758/s13414-017-1325-6}
}
@article{Ou.2010,
	title        = {Consumer trust and distrust: An issue of website design},
	author       = {Ou, Carol Xiaojuan and Sia, Choon Ling},
	year         = 2010,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 68,
	number       = 12,
	pages        = {913--934},
	doi          = {10.1016/j.         ijhcs.2010.08.003},
	issn         = {1071-5819}
}
@article{Schul.2004,
	title        = {Encoding under trust and distrust: the spontaneous activation of incongruent cognitions},
	author       = {Schul, Yaacov and Mayo, Ruth and Burnstein, Eugene},
	year         = 2004,
	journal      = {Journal of Personality and Social Psychology},
	volume       = 86,
	number       = 5,
	pages        = {668--679},
	doi          = {10.1037/0022-3514.86.5.668},
	issn         = {1939-1315}
}
@article{Schul.2008,
	title        = {The Value of Distrust},
	author       = {Schul, Yaacov and Mayo, Ruth and Burnstein, Eugene},
	year         = 2008,
	journal      = {Journal of Experimental Social Psychology},
	volume       = 44,
	number       = 5,
	pages        = {1293--1302},
	doi          = {10.1016/j.     jesp.2008.05.003},
	issn         = {0022-1031}
}
@article{Schoorman.2007,
	title        = {An Integrative Model of Organizational Trust: Past, Present, and Future},
	author       = {Schoorman, F. David and Mayer, Roger C. and Davis, James H.},
	year         = 2007,
	journal      = {Academy of Management Review},
	volume       = 32,
	number       = 2,
	pages        = {344--354},
	doi          = {10.5465/amr.2007.24348410},
	issn         = {0363-7425}
}
@book{luhmann2009a,
	title        = {Vertrauen : ein Mechanismus der Reduktion sozialer Komplexität},
	author       = {Luhmann, Niklas},
	year         = 2009,
	publisher    = {Stuttgart : Lucius \& Lucius},
	series       = {UTB : 2185},
	isbn         = {9783825221850},
	edition      = {4. Aufl., Nachdr.},
	keywords     = {Vertrauen, Soziologie}
}
@book{Schweer.2009,
	title        = {Zur Funktionalit{\"a}t und Dysfunktionalit{\"a}t von Misstrauen in virtuellen Organisationen},
	author       = {Schweer, Martin and Vaske, Christian and Vaske, Ann-Kathrin},
	year         = 2009,
	url          = {https://dl.gi.de/handle/20.500.12116/35191},
}
@article{Seckler.2015,
	title        = {Trust and distrust on the web: User experiences and website characteristics},
	author       = {Seckler, Mirjam and Heinz, Silvia and Forde, Seamus and Tuch, Alexandre N. and Opwis, Klaus},
	year         = 2015,
	journal      = {Computers in Human Behavior},
	volume       = 45,
	pages        = {39--50},
	doi          = {10.1016/j.   chb.2014.11.064},
	issn         = {0747-5632}
}
@incollection{Shahrdar.2018,
	title        = {A Survey on Trust in Autonomous Systems},
	author       = {Shahrdar, Shervin and Menezes, Luiza and Nojoumian, Mehrdad},
	year         = 2018,
	booktitle    = {Advances in Intelligent Systems and Computing},
	publisher    = {{Springer International Publishing}},
	address      = {Cham},
	pages        = {368--386},
	doi          = {10.1007/978-3-030-01177-2      {\textunderscore }27},
	isbn         = {9783030011765}
}
@article{Sitkin.1993,
	title        = {Explaining the Limited Effectiveness of Legalistic ``Remedies'' for Trust/Distrust},
	author       = {Sitkin, Sim B. and Roth, Nancy L.},
	year         = 1993,
	journal      = {Organization Science},
	volume       = 4,
	number       = 3,
	pages        = {367--392},
	doi          = {10.1287/orsc.4.3.367},
	issn         = {1047-7039}
}
@article{Spain.2008,
	title        = {Towards an Empirically Developed Scale for System Trust: Take Two},
	author       = {Spain, Randall D. and Bustamante, Ernesto A. and Bliss, James P.},
	year         = 2008,
	journal      = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	volume       = 52,
	number       = 19,
	pages        = {1335--1339},
	doi          = {10.1177/154193120805201907},
}
@article{Sperber.2010,
	title        = {Epistemic Vigilance},
	author       = {Sperber, D. A.N. and Cl{\'e}ment, Fabrice and Heintz, Christophe and Mascaro, Olivier and Mercier, Hugo and Origgi, Gloria and Wilson, Deirdre},
	year         = 2010,
	journal      = {Mind {\&} Language},
	volume       = 25,
	number       = 4,
	pages        = {359--393},
	doi          = {10.1111/j.1468-0017.2010.01394.x},
	issn         = {1468-0017},
}
@article{Mercier.2017,
	title        = {How Gullible are We? A Review of the Evidence from Psychology and Social Science},
	author       = {Mercier, Hugo},
	year         = 2017,
	journal      = {Review of General Psychology},
	volume       = 21,
	number       = 2,
	pages        = {103--122},
	doi          = {10.1037/gpr0000111},
}
@article{Thiebes.2021,
	title        = {Trustworthy artificial intelligence},
	author       = {Thiebes, Scott and Lins, Sebastian and Sunyaev, Ali},
	year         = 2021,
	journal      = {Electronic Markets},
	volume       = 31,
	number       = 2,
	pages        = {447--464},
	doi          = {10.1007/s12525-020-00441-4},
	issn         = {1422-8890},
}
@article{Thielsch.2018,
	title        = {Trust and distrust in information systems at the workplace},
	author       = {Thielsch, Meinald T. and Mee{\ss}en, Sarah M. and Hertel, Guido},
	year         = 2018,
	journal      = {PeerJ},
	volume       = 6,
	pages        = {e5483},
	doi          = {10.7717/peerj.5483},
	issn         = {2167-8359},
}
@article{Tunnermann.2018,
	title        = {Stuck on a Plateau? A Model-Based Approach to Fundamental Issues in Visual Temporal-Order Judgments},
	author       = {T{\"u}nnermann, Jan and Scharlau, Ingrid},
	year         = 2018,
	journal      = {Vision},
	volume       = 2,
	number       = 3,
	pages        = 29,
	doi          = {10.3390/vision2030029},
	issn         = {2411-5150},
	url          = {https://www.mdpi.com/316332},
}
@article{Poortinga.2003,
	title        = {Exploring the dimensionality of trust in risk regulation},
	author       = {Poortinga, Wouter and Pidgeon, Nick F.},
	year         = 2003,
	journal      = {Risk analysis : an official publication of the Society for Risk Analysis},
	volume       = 23,
	number       = 5,
	pages        = {961--972},
	doi          = {10.1111/1539-6924.00373}
}
@article{Posten.2021,
	title        = {How trust and distrust shape perception and memory},
	author       = {Posten, Ann-Christin and Gino, Francesca},
	year         = 2021,
	journal      = {Journal of Personality and Social Psychology},
	volume       = 121,
	number       = 1,
	pages        = {43--58},
	doi          = {10.1037/pspa0000269},
	issn         = {1939-1315},
}
@article{vandeWalle.2014,
	title        = {Trust and Distrust as Distinct Concepts: Why Studying Distrust in Institutions is Important},
	author       = {{van de Walle}, Steven and Six, Fr{\'e}d{\'e}rique},
	year         = 2014,
	journal      = {Journal of Comparative Policy Analysis: Research and Practice},
	volume       = 16,
	number       = 2,
	pages        = {158--174},
	doi          = {10.1080/13876988.2013.785146},
	issn         = {1387-6988},
}
@book{Vaske.2016,
	title        = {Misstrauen und Vertrauen},
	author       = {Vaske, C.},
	year         = 2016,
	publisher    = {Universit{\"a}t Vechta}
}
@article{Gaube.2021,
	title        = {Do as AI say: susceptibility in deployment of clinical decision-aids},
	author       = {Gaube, Susanne and Suresh, Harini and Raue, Martina and Merritt, Alexander and Berkowitz, Seth J and Lermer, Eva and Coughlin, Joseph F and Guttag, John V and Colak, Errol and Ghassemi, Marzyeh},
	year         = 2021,
	journal      = {NPJ digital medicine},
	publisher    = {Nature Publishing Group UK London},
	volume       = 4,
	number       = 1,
	pages        = 31
}
@article{Stanton.2021,
	title        = {Trust and Artificial Intelligence},
	author       = {Stanton, Brian and Jensen, Theodore},
	year         = 2021,
	doi          = {10.6028/nist.ir.8332-draft},
}
@inproceedings{Kastner.2021,
	title        = {On the Relation of Trust and Explainability: Why to Engineer for Trustworthiness},
	author       = {Kastner, Lena and Langer, Markus and Lazar, Veronika and Schomacker, Astrid and Speith, Timo and Sterz, Sarah},
	year         = 2021,
	booktitle    = {Proceedings, 29th IEEE International Requirements Engineering Conference Workshops : REW 2021 : September 20-24 2021, online event},
	publisher    = {{IEEE Computer Society, Conference Publishing Services}},
	address      = {Los Alamitos, California},
	pages        = {169--175},
	doi          = {10.1109/REW53955.2021.00031},
	isbn         = {978-1-6654-1898-0},
}
@article{Kramer.1999,
	title        = {Trust and distrust in organizations: emerging perspectives, enduring questions},
	author       = {Kramer, R. M.},
	year         = 1999,
	journal      = {Annual Review of Psychology},
	volume       = 50,
	number       = 1,
	pages        = {569--598},
	doi          = {10.1146/annurev.psych.50.1.569},
	issn         = {0066-4308},
}
@article{Jian.2000,
	title        = {Foundations for an Empirically Determined Scale of Trust in Automated Systems},
	author       = {Jian, Jiun-Yin and Bisantz, Ann M. and Drury, Colin G.},
	year         = 2000,
	journal      = {International Journal of Cognitive Ergonomics},
	volume       = 4,
	number       = 1,
	pages        = {53--71},
	doi          = {10.1207/S15327566                        IJCE0401{\textunderscore }04},
}
@article{Hoff.2015,
	title        = {Trust in automation: integrating empirical evidence on factors that influence trust},
	author       = {Hoff, Kevin Anthony and Bashir, Masooda},
	year         = 2015,
	journal      = {Human Factors},
	volume       = 57,
	number       = 3,
	pages        = {407--434},
	doi          = {10.1177/0018720814547570},
}
@book{Hoffman.2018,
	title        = {Measuring Trust in the XAI Context: Technical Report, DARPA Explainable AI Program.},
	author       = {Hoffman, Robert and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
	year         = 2018,
	doi          = {10.31234/osf.io/e3kv9},
}
@book{gigerenzer2021klick,
	title        = {Klick: Wie wir in einer digitalen Welt die Kontrolle behalten und die richtigen Entscheidungen treffen-Vom Autor des Bestsellers{\guillemotright} Bauchentscheidungen {\guillemotleft}},
	author       = {Gigerenzer, Gerd},
	year         = 2021,
	publisher    = {C. Bertelsmann Verlag}
}
@article{He.2019,
	title        = {AttGAN: Facial Attribute Editing by Only Changing What You Want},
	author       = {He, Zhenliang and Zuo, Wangmeng and Kan, Meina and Shan, Shiguang and Chen, Xilin},
	year         = 2019,
	journal      = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
	volume       = 28,
	number       = 11,
	pages        = {5464--5478},
	doi          = {10.1109/tip.2019.2916751},
}
@article{Pinhanez.2021,
	title        = {Expose Uncertainty, Instill Distrust, Avoid Explanations: Towards Ethical Guidelines for AI},
	author       = {Pinhanez, Claudio S.},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2112.01281},
	abstract     = {
		In this position paper, I argue that the best way to help and protect humans using AI technology is to make them aware of the intrinsic limitations and problems of AI algorithms. To accomplish this, I suggest three ethical guidelines to be used in the presentation of results, mandating AI systems to expose uncertainty, to instill distrust, and, contrary to traditional views, to avoid explanations. The paper does a preliminary discussion of the guidelines and provides some arguments for their adoption, aiming to start a debate in the community about AI ethics in practice.

		Presented in the NeurIPS 2021 workshop on Human-Centered AI. December 13th 2021
	},
}
@article{Roberts.2021,
	title        = {Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans},
	year         = 2021,
	journal      = {Nature Machine Intelligence},
	volume       = 3,
	number       = 3,
	pages        = {199--217},
	doi          = {10.1038/s42256-021-00307-0},
	issn         = {2522-5839},
	url          = {https://www.nature.com/articles/s42256-021-00307-0},
}
@misc{Bianchi.07.11.2022,
	title        = {Easily Accessible Text-to-Image Generation Amplifies Demographic  Stereotypes at Large Scale},
	author       = {Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and Ladhak, Faisal and Cheng, Myra and Nozza, Debora and Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and Caliskan, Aylin},
	url          = {https://arxiv.org/pdf/2211.03759},
	abstract     = {Machine learning models are now able to convert user-written text descriptions into naturalistic images. These models are available to anyone online and are being used to generate millions of images a day. We investigate these models and find that they amplify dangerous and complex stereotypes. Moreover, we find that the amplified stereotypes are difficult to predict and not easily mitigated by users or model owners. The extent to which these image-generation models perpetuate and amplify stereotypes and their mass deployment is cause for serious concern.},
	date         = {07.11.2022},
}
@proceedings{.2006,
	title        = {Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS'06)},
	year         = 2006,
	publisher    = {IEEE}
}
@book{.2018,
	title        = {Advances in Intelligent Systems and Computing},
	year         = 2018,
	publisher    = {{Springer International Publishing}},
	address      = {Cham},
	isbn         = {9783030011765}
}
@proceedings{.2020,
	title        = {International Conference on Human-Computer Interaction},
	year         = 2020,
	publisher    = {{Springer, Cham}}
}
@proceedings{.2021,
	title        = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	year         = 2021,
	publisher    = {{Association for Computing Machinery}},
	address      = {New York,NY,United States},
	series       = {ACM Digital Library},
	doi          = {10.1145/3442188},
	isbn         = {9781450383097}
}
@proceedings{.2021b,
	title        = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	year         = 2021,
	publisher    = {ACM},
	address      = {New York, NY, USA}
}
@proceedings{.2021c,
	title        = {Proceedings, 29th IEEE International Requirements Engineering Conference Workshops : REW 2021 : September 20-24 2021, online event},
	year         = 2021,
	publisher    = {{IEEE Computer Society, Conference Publishing Services}},
	address      = {Los Alamitos, California},
	isbn         = {978-1-6654-1898-0}
}
@inproceedings{Bansal.2021,
	title        = {Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance},
	author       = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	doi          = {10.1145/3411764.3445717},
}
@inproceedings{Jacovi.2021,
	title        = {Formalizing Trust in Artificial Intelligence},
	author       = {Jacovi, Alon and Marasovi{\'c}, Ana and Miller, Tim and Goldberg, Yoav},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	publisher    = {{Association for Computing Machinery}},
	address      = {New York,NY,United States},
	series       = {ACM Digital Library},
	pages        = {624--635},
	doi          = {10.1145/3442188.3445923},
	isbn         = {9781450383097}
}
@article{BarredoArrieta.2020,
	title        = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
	author       = {{Barredo Arrieta}, Alejandro and D{\'i}az-Rodr{\'i}guez, Natalia and {Del Ser}, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	year         = 2020,
	journal      = {Information Fusion},
	volume       = 58,
	pages        = {82--115},
	doi          = {10.1016/j.                 inffus.2019.12.012},
	issn         = {1566-2535},
}
@proceedings{Brewster.2019,
	title        = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
	year         = 2019,
	publisher    = {{Association for Computing Machinery}},
	address      = {New York,NY,United States},
	series       = {ACM Digital Library},
	doi          = {10.1145/3290605},
	isbn         = {9781450359702},
	editor       = {Brewster, Stephen},
	institution  = {{ACM Special Interest Group on Computer-Human Interaction}}
}
@article{Bundesen.1990,
	title        = {A theory of visual attention},
	author       = {Bundesen, C.},
	year         = 1990,
	journal      = {Psychological Review},
	volume       = 97,
	number       = 4,
	pages        = {523--547},
	doi          = {10.1037/0033-295x.97.4.523},
	issn         = {1939-1471}
}
@article{Cho.2006,
	title        = {The mechanism of trust and distrust formation and their relational outcomes},
	author       = {Cho, J.},
	year         = 2006,
	journal      = {Journal of Retailing},
	volume       = 82,
	number       = 1,
	pages        = {25--35},
	doi          = {10.1016/j.          jretai.2005.11.002},
	issn         = {0022-4359}
}
@inproceedings{Ehsan.2020,
	title        = {Human-Centered Explainable AI: Towards a Reflective Sociotechnical Approach},
	author       = {Ehsan, Upol and Riedl, Mark O.},
	year         = 2020,
	booktitle    = {International Conference on Human-Computer Interaction},
	publisher    = {{Springer, Cham}},
	pages        = {449--466},
	  doi          = {10.1007/978-3-030-60117-1{\textunderscore}33}
}
@article{Fein.1996,
	title        = {Effects of suspicion on attributional thinking and the correspondence bias},
	author       = {Fein, Steven},
	year         = 1996,
	journal      = {Journal of Personality and Social Psychology},
	volume       = 70,
	number       = 6,
	pages        = {1164--1184},
	doi          = {10.1037/0022-3514.70.6.1164}
}
@inproceedings{Frison.2019,
	title        = {In UX We Trust},
	author       = {Frison, Anna-Katharina and Wintersberger, Philipp and Riener, Andreas and Schartm{\"u}ller, Clemens and Boyle, Linda Ng and Miller, Erika and Weigl, Klemens},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
	publisher    = {{Association for Computing Machinery}},
	address      = {New York,NY,United States},
	series       = {ACM Digital Library},
	pages        = {1--13},
	doi          = {10.1145/3290605.3300374},
	isbn         = {9781450359702},
	editor       = {Brewster, Stephen}
}
@article{Glikson.2020,
	title        = {Human Trust in Artificial Intelligence: Review of Empirical Research},
	author       = {Glikson, Ella and Woolley, Anita Williams},
	year         = 2020,
	journal      = {Academy of Management Annals},
	volume       = 14,
	number       = 2,
	pages        = {627--660},
	doi          = {10.5465/annals.2018.0057},
	abstract     = {Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into...},
}
@article{Gunning.2019,
	title        = {DARPA's Explainable Artificial Intelligence (XAI) Program},
	author       = {Gunning, David and Aha, David},
	year         = 2019,
	journal      = {AI Magazine},
	volume       = 40,
	number       = 2,
	pages        = {44--58},
	doi          = {10.1609/aimag.v40i2.2850},
}
@article{Guo.2017,
	title        = {Revisiting the Foundations of Organizational Distrust},
	author       = {Guo, Shiau-Ling and Lumineau, Fabrice and Lewicki, Roy J.},
	year         = 2017,
	journal      = {Foundations and Trends{\circledR} in Management},
	volume       = 1,
	number       = 1,
	pages        = {1--88},
	doi          = {10.1561/3400000001},
	issn         = {2475-6946},
}
@article{Guo.2021,
	title        = {Modeling and Predicting Trust Dynamics in Human--Robot Teaming: A Bayesian Inference Approach},
	author       = {Guo, Yaohui and Yang, X. Jessie},
	year         = 2021,
	journal      = {International Journal of Social Robotics},
	volume       = 13,
	number       = 8,
	pages        = {1899--1909},
	doi          = {10.1007/s12369-020-00703-3},
	issn         = {1875-4805},
	url          = {https://link.springer.com/article/10.1007/s12369-020-00703-3}     ,
}
@article{Kohn.2021,
	title        = {Measurement of Trust in Automation: A Narrative Review and Reference Guide},
	author       = {Kohn, Spencer C. and de Visser, Ewart J. and Wiese, Eva and Lee, Yi-Ching and Shaw, Tyler H.},
	year         = 2021,
	journal      = {Frontiers in psychology},
	volume       = 12,
	pages        = 604977,
	doi          = {10.3389/fpsyg.2021.604977},
	issn         = {1664-1078},
}
@article{Obermeyer.2019,
	title        = {Dissecting racial bias in an algorithm used to manage the health of populations},
	author       = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	year         = 2019,
	journal      = {Science},
	volume       = 366,
	number       = 6464,
	pages        = {447--453},
	doi          = {10.1126/science.         aax2342},
	abstract     = {Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5{\%}. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
}
@incollection{HarrisonMcKnight.2001,
	title        = {Trust and Distrust Definitions: One Bite at a Time},
	author       = {{Harrison McKnight}, D. and Chervany, Norman L.},
	year         = 2001,
	booktitle    = {Trust in Cyber-societies},
	publisher    = {{Springer, Berlin, Heidelberg}},
	pages        = {27--54},
	doi          = {10.1007/3-540- 45547-7{\textunderscore }3},
}
@inproceedings{Benamati.2006,
	title        = {Are Trust and Distrust Distinct Constructs? An Empirical Study of the Effects of Trust and Distrust among Online Banking Users},
	author       = {Benamati, J. and Serva, M. A. and Fuller, M. A.},
	year         = 2006,
	booktitle    = {Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS'06)},
	publisher    = {IEEE},
	doi          = {10.1109/hicss.2006.63}
}
@article{Bobko.2014,
	title        = {The construct of state-level suspicion: a model and research agenda for automated and information technology (IT) contexts},
	author       = {Bobko, Philip and Barelka, Alex J. and Hirshfield, Leanne M.},
	year         = 2014,
	journal      = {Human Factors},
	volume       = 56,
	number       = 3,
	pages        = {489--508},
	doi          = {10.1177/0018720813497052},
}
@article{Visser.2020,
	title        = {Towards a Theory of Longitudinal Trust Calibration in Human--Robot Teams},
	author       = {de Visser, Ewart J. and Peeters, Marieke M. M. and Jung, Malte F. and Kohn, Spencer and Shaw, Tyler H. and Pak, Richard and Neerincx, Mark A.},
	year         = 2020,
	journal      = {International Journal of Social Robotics},
	volume       = 12,
	number       = 2,
	pages        = {459--478},
	doi          = {10.1007/s12369-019-00596-x},
	issn         = {1875-4805},
}
@article{Wachter.2017,
	title        = {Why a right to explanation of automated decision-making does not exist in the general data protection regulation},
	author       = {Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
	year         = 2017,
	journal      = {International Data Privacy Law},
	publisher    = {Oxford University Press},
	volume       = 7,
	number       = 2,
	pages        = {76--99}
}
@article{Wagner.2018,
	title        = {Overtrust in the robotic age},
	author       = {Wagner, Alan R. and Borenstein, Jason and Howard, Ayanna},
	year         = 2018,
	journal      = {Communications of the ACM},
	volume       = 61,
	number       = 9,
	pages        = {22--24},
	doi          = {10.1145/3241365},
	issn         = {0001-0782}
}
@article{Weitz.2022,
	title        = {Towards Human-Centered AI: Psychological concepts as foundation for empirical XAI research},
	author       = {Weitz, Katharina},
	year         = 2022,
	journal      = {it - Information Technology},
	volume       = 64,
	number       = {1-2},
	pages        = {71--75},
	doi          = {10.1515/itit-2021-0047},
	issn         = {1611-2776}
}
@article{Lewis.1985,
	title        = {Trust as a social reality},
	author       = {Lewis, J David and Weigert, Andrew},
	year         = 1985,
	journal      = {Social forces},
	publisher    = {Oxford University Press},
	volume       = 63,
	number       = 4,
	pages        = {967--985}
}
%%%%% MACHINE LEARNING PAPERS %%%%%
@inproceedings{ferrario2022,
	title        = {How explainability contributes to trust in AI},
	author       = {Ferrario, Andrea and Loi, Michele},
	year         = 2022,
	booktitle    = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {1457--1466}
}

@inproceedings{Toreini2020,
	title        = {The relationship between trust in AI and trustworthy machine learning technologies},
	author       = {Toreini, Ehsan and Aitken, Mhairi and Coopamootoo, Kovila and Elliott, Karen and Zelaya, Carlos Gonzalez and Van Moorsel, Aad},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 conference on fairness, accountability, and transparency},
	pages        = {272--283}
}
@article{Schaefer2016,
	title        = {A Meta-Analysis of Factors Influencing the Development of Trust in Automation},
	author       = {Kristin E. Schaefer and Jessie Y. C. Chen and James L. Szalma and P. A. Hancock},
	year         = 2016,
	month        = {mar},
	journal      = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	volume       = 58,
	number       = 3,
	pages        = {377--400},
	doi          = {10.1177/0018720816634228}
}
@inproceedings{Siau2018,
	title        = {Building Trust in Artificial Intelligence,Machine Learning, and Robotics},
	author       = {Keng Siau and Weiyu Wang},
	year         = 2018
}
@article{Riegelsberger2005,
	title        = {The mechanics of trust: A framework for research and design},
	author       = {Jens Riegelsberger and M. Angela Sasse and John D. McCarthy},
	year         = 2005,
	month        = {mar},
	journal      = {International Journal of Human-Computer Studies},
	publisher    = {Elsevier {BV}},
	volume       = 62,
	number       = 3,
	pages        = {381--422},
	doi          = {10.1016/j.         ijhcs.2005.01.001},
	keywords     = {Trust, Social capital, Dis-embedding, Interpersonal cues, Human computer interaction, Computer mediated communication, Computer supported collaborative work, Decision-making, Game theory, E-commerce}
}
%%%%%%%%%%%%%%
@inproceedings{MingYin2019,
	title        = {Understanding the Effect of Accuracy on Trust in Machine Learning Models},
	author       = {Ming Yin, Jennifer Wortman Vaughan, Hanna Wallach},
	year         = 2019,
	doi          = {.org/10.1145/3290605.3300509}    ,
	keywords     = {may misunderstand or mistrust its predictions [6, 16, 25]. Machine learning, trust, human-subject experiments Prompted by these challenges, as well as growing concerns}
}
@inproceedings{Honeycutt2020,
	title        = {Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy},
	author       = {Donald R. Honeycutt, Mahsan Nourani, Eric D. Ragan},
	year         = 2020,
	booktitle    = {Proceedings of the Eighth AAAI Conference on Human Computation and Crowdsourcing (HCOMP-20)},
	keywords     = {Full}
}
@article{Parasuraman1997,
	title        = {Humans and automation: Use, misuse, disuse, abuse},
	author       = {Parasuraman, Raja and Riley, Victor},
	year         = 1997,
	journal      = {Human factors},
	publisher    = {SAGE Publications Sage CA: Los Angeles, CA},
	volume       = 39,
	number       = 2,
	pages        = {230--253},
	doi          = {10.1518/001872097778543886}
}

@article{hoffman2013trust,
	title        = {Trust in automation},
	author       = {Hoffman, Robert R and Johnson, Matthew and Bradshaw, Jeffrey M and Underbrink, Al},
	year         = 2013,
	journal      = {IEEE Intelligent Systems},
	publisher    = {IEEE},
	volume       = 28,
	number       = 1,
	pages        = {84--88}
}
@inproceedings{nourani2019effects,
	title        = {The Effects of Meaningful and Meaningless Explanations on Trust and Perceived System Accuracy in Intelligent Systems},
	author       = {Nourani, Mahsan and Kabir, Samia and Mohseni, Sina and Ragan, Eric D},
	year         = 2019,
	booktitle    = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	volume       = 7,
	pages        = {97--105},
	url          = {https://ojs.aaai.org/index.php/HCOMP/article/view/5284}
}
@inproceedings{Visser2018,
	title        = {From ‘automation’ to ‘autonomy’: the importance of trust repair in human–machine interaction},
	author       = {De Visser, Ewart J and Pak, Richard and Shaw, Tyler H},
	year         = 2018,
	doi          = {.org/10.1080/00140139.2018.1457725}   ,
	url          = {https://www.tandfonline.com/doi/abs/10.1080/00140139.2018.1457725}  ,
	keywords     = {Trust repair; autonomy; automation; humanness; human–machine teaming}
}
@inproceedings{kulesza2013too,
	title        = {Too much, too little, or just right? Ways explanations impact end users' mental models},
	author       = {Kulesza, Todd and Stumpf, Simone and Burnett, Margaret and Yang, Sherry and Kwan, Irwin and Wong, Weng-Keen},
	year         = 2013,
	booktitle    = {2013 IEEE Symposium on visual languages and human centric computing},
	pages        = {3--10},
	doi          = {.org/10.1109/VLHCC.2013.6645235},
	organization = {IEEE}
}
@incollection{Gurney2022,
	title        = {Measuring and~Predicting Human Trust in~Recommendations from~an~{AI} Teammate},
	author       = {Nikolos Gurney and David V. Pynadath and Ning Wang},
	year         = 2022,
	booktitle    = {Artificial Intelligence in {HCI}},
	publisher    = {Springer International Publishing},
	series       = {LNAI},
	volume       = 13336,
	pages        = {22--34},
	doi          = {10.1007/978-3-031-05643-7_2},
	editor       = {©c The Author(s) and under exclusive license to Springer Nature Switzerland AG 2022 H. Degen and S. Ntoa},
	keywords     = {Trust in AI, Explainable machine learning, AI compliance}
}
@article{Dzindolet2003,
	title        = {The role of trust in automation reliance},
	author       = {Mary T. Dzindolet and Scott A. Peterson and Regina A. Pomranky and Linda G. Pierce and Hall P. Beck},
	year         = 2003,
	month        = {jun},
	journal      = {International Journal of Human-Computer Studies},
	publisher    = {Elsevier {BV}},
	volume       = 58,
	number       = 6,
	pages        = {697--718},
	doi          = {10.1016/s1071-5819(03)00038-7},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581903000387},
	keywords     = {Automation trust, Automation reliance, Misuse, Disuse}
}


@article{Dellermann2021,
	title        = {The future of human-AI collaboration: a taxonomy of design knowledge for hybrid intelligence systems},
	author       = {Dellermann, Dominik and Calma, Adrian and Lipusch, Nikolaus and Weber, Thorsten and Weigel, Sascha and Ebel, Philipp},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2105.03354}    ,
	url          = {https://arxiv.org/abs/2105.03354},
	keywords     = {The Future of Human-AI Collaboration: A Taxonomy of Design Knowledge for Hybrid Intelligence Systems.}
}
@inproceedings{Omeiza2021,
	title        = {Why Not Explain? Effects of Explanations on Human Perceptions of Autonomous Driving},
	author       = {Daniel Omeiza and Konrad Kollnig and Helena Web and Marina Jirotka and Lars Kunze},
	year         = 2021,
	month        = {jul},
	booktitle    = {2021 {IEEE} International Conference on Advanced Robotics and Its Social Impacts ({ARSO})},
	publisher    = {{IEEE}},
	doi          = {10.1109/arso51874.2021.9542835}
}
@article{Pieters2010,
	title        = {Explanation and trust: what to tell the user in security and {AI}?},
	author       = {Wolter Pieters},
	year         = 2010,
	month        = {nov},
	day          = 3,
	journal      = {Ethics and Information Technology},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 13,
	number       = 1,
	pages        = {53--64},
	doi          = {10.1007/s10676-010-9253-3},
	date         = {2010-11-03},
	keywords     = {Actor-network theory, Confidence, Expert systems, Explanation, Information security, Informed consent, Systems theory, Trust}
}
@inproceedings{korber2018theoretical,
	title        = {Theoretical considerations and development of a questionnaire to measure trust in automation},
	author       = {K{\"o}rber, Moritz},
	year         = 2018,
	booktitle    = {Congress of the International Ergonomics Association},
	pages        = {13--30},
	organization = {Springer}
}
@inproceedings{Onnasch2014,
	title        = {Operators׳ adaptation to imperfect automation – Impact of miss-prone alarm systems on attention allocation and performance},
	author       = {Onnasch, Linda and Ruff, Stefan and Manzey, Dietrich},
	year         = 2014,
	publisher    = {Technische Universität Berlin},
	doi          = {10.14279/DEPOSITONCE-10989},
	url          = {https://www.sciencedirect.com/science/article/abs/pii/S1071581914000688},
	keywords     = {150 Psychologie, alarm systems, reliability, miss-prone automation, attention allocation, adaptive behaviour}
}
@article{Lee1994,
	title        = {Trust, self-confidence, and operators' adaptation to automation},
	author       = {John D. Lee and Neville Moray},
	year         = 1994,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 40,
	number       = 1,
	pages        = {153--184},
	doi          = {https://doi.org/10.1006/ijhc.1994.1007}   ,
	issn         = {1071-5819}
}
@techreport{Parasuraman2000,
	title        = {A model for types and levels of human interaction with automation},
	author       = {R. Parasuraman and T.B. Sheridan and C.D. Wickens},
	year         = 2000,
	publisher    = {IEEE},
	number       = {automa-}
}
@article{Parasuraman2010,
	title        = {Complacency and Bias in Human Use of Automation: An Attentional Integration},
	author       = {Raja Parasuraman and Dietrich H. Manzey},
	year         = 2010,
	month        = {jun},
	journal      = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	publisher    = {{SAGE} Publications},
	volume       = 52,
	number       = 3,
	pages        = {381--410},
	doi          = {10.1177/0018720810376055},
	keywords     = {attention, automation-related complacency, automation bias, decision making, human-computer interaction, trust}
}
@article{ENDSLEY1999,
	title        = {Level of automation effects on performance, situation awareness and workload in a dynamic control task},
	author       = {MICA R. ENDSLEY and DAVID B. KABER},
	year         = 1999,
	month        = {mar},
	journal      = {Ergonomics},
	publisher    = {Informa {UK} Limited},
	volume       = 42,
	number       = 3,
	pages        = {462--492},
	doi          = {10.1080/001401399185595},
	keywords     = {Level of automation (LOA), Human-centred automation, Situation awareness (SA), Workload, Out-of-the-loop performance}
}
@inproceedings{Guesmi2021,
	title        = {Input or Output: Effects of Explanation Focus on the Perception of Explainable Recommendation with Varying Level of Details.},
	author       = {Guesmi, Mouadh and Chatti, Mohamed Amine and Vorgerd, Laura and Joarder, Shoeb Ahmed and Ain, Qurat Ul and Ngo, Thao and Zumor, Shadi and Sun, Yiqi and Ji, Fangzheng and Muslim, Arham},
	year         = 2021,
	booktitle    = {IntRS@ RecSys},
	pages        = {55--72},
	keywords     = {recommender system, explainable recommendation, personalized explanation, explanation design choices}
}
@article{hoff2015trust,
	title        = {Trust in automation: Integrating empirical evidence on factors that influence trust},
	author       = {Hoff, Kevin Anthony and Bashir, Masooda},
	year         = 2015,
	journal      = {Human factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 57,
	number       = 3,
	pages        = {407--434},
	doi          = {10.1177/0018720814547570}
}
@article{Parasuraman2008,
	title        = {Situation Awareness, Mental Workload, and Trust in Automation: Viable, Empirically Supported Cognitive Engineering Constructs},
	author       = {Raja Parasuraman and Thomas B. Sheridan and Christopher D. Wickens},
	year         = 2008,
	month        = {jun},
	journal      = {Journal of Cognitive Engineering and Decision Making},
	publisher    = {{SAGE} Publications},
	volume       = 2,
	number       = 2,
	pages        = {140--160},
	doi          = {10.1518/155534308x284417}
}
@article{Miller2016,
	title        = {Behavioral Measurement of Trust in Automation},
	author       = {David Miller and Mishel Johns and Brian Mok and Nikhil Gowda and David Sirkin and Key Lee and Wendy Ju},
	year         = 2016,
	month        = {sep},
	journal      = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	publisher    = {{SAGE} Publications},
	volume       = 60,
	number       = 1,
	pages        = {1849--1853},
	doi          = {10.1177/1541931213601422}
}
%

@article{Jiang2022,
	title        = {Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty},
	author       = {Jinglu Jiang and Surinder Kahai and Ming Yang},
	year         = 2022,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 165,
	pages        = 102839,
	doi          = {https://doi.org/10.1016/j.ijhcs.2022.102839}       ,
	issn         = {1071-5819},
	keywords     = {AI explainability, AI advice acceptance, Medical AI, Human-AI interaction, Experiment}
}
@article{Shin2021,
	title        = {The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI},
	author       = {Donghee Shin},
	year         = 2021,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 146,
	pages        = 102551,
	doi          = {https://doi.org/10.1016/j          .ijhcs.2020.102551},
	issn         = {1071-5819},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581920301531},
	keywords     = {Explainable Ai, Causability, Human-ai interaction, Explanatorycues, Interpretability, Understandability, Trust, Glassbox, Human-centeredAI}
}
@article{Dikmen2022,
	title        = {The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending},
	author       = {Murat Dikmen and Catherine Burns},
	year         = 2022,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 162,
	pages        = 102792,
	doi          = {https://doi.org/10.1016/j     .ijhcs.2022.102792},
	issn         = {1071-5819},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581922000210},
	keywords     = {Explainable AI, Human-AI interaction, Domain knowledge, Trust in AI}
}
@inproceedings{Wanner2020,
	title        = {White, Grey, Black: Effects of XAI Augmentation on the Confidence in AI-based Decision Support Systems Short Paper},
	author       = {Wanner, Jonas and Herm, Lukas-Valentin and Heinrich, Kai and Janiesch, Christian and Zschech, Patrick},
	year         = 2020,
	keywords     = {Explainable AI, Confidence, Decision Support Systems, User-study, Maintenance}
}
@inproceedings{branley2020user,
	title        = {User trust and understanding of explainable AI: exploring algorithm visualisations and user biases},
	author       = {Branley-Bell, Dawn and Whitworth, Rebecca and Coventry, Lynne},
	year         = 2020,
	booktitle    = {International Conference on Human-Computer Interaction},
	pages        = {382--399},
	url          = {https://link.springer.com/chapter/10.1007/978-3-030-49065-2_27}   ,
	organization = {Springer}
}
@inproceedings{Ferreira2021,
	title        = {The human-AI relationship in decision-making: AI explanation to support people on justifying their decisions},
	author       = {Ferreira, Juliana and Monteiro, Mateus},
	year         = 2021,
	keywords     = {eXplainable AI, AI design, decision-making, high-stakes decision-making, human-AI relationship}
}

@inproceedings{Wang2021,
	title        = {Are Explanations Helpful? A Comparative Study of the Effects of Explanations in {AI}-Assisted Decision-Making},
	author       = {Xinru Wang and Ming Yin},
	year         = 2021,
	month        = {apr},
	day          = 16,
	booktitle    = {26th International Conference on Intelligent User Interfaces},
	publisher    = {{ACM}},
	doi          = {10.1145/3397481.3450650},
	keywords     = {interpretable machine learning, explainable AI, trust, trust calibration, human-subject experiments},
	language     = {en},
	source       = {iui21-16.tex}
}
@inproceedings{Papenmeier1907,
	title        = {How model accuracy and explanation fidelity influence user trust in AI},
	author       = {Papenmeier, Andrea and Englebienne, Gwenn and Seifert, Christin},
	year         = 1907
}
@article{Papenmeier2022,
	title        = {It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in {AI}},
	author       = {Andrea Papenmeier and Dagmar Kern and Gwenn Englebienne and Christin Seifert},
	year         = 2022,
	month        = {aug},
	journal      = {{ACM} Transactions on Computer-Human Interaction},
	publisher    = {Association for Computing Machinery ({ACM})},
	volume       = 29,
	number       = 4,
	pages        = {1--33},
	doi          = {10.1145/3495013},
	keywords     = {Explainable AI, machine learning, minimum explanations, user trust, explanation fidelity This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike International 4.0 License}
}
@inproceedings{Lakkaraju2020,
	title        = {"How do I fool you?" Manipulating User Trust via Misleading Black Box Explanations},
	author       = {Lakkaraju, Himabindu and Bastani, Osbert},
	year         = 2020,
	month        = {feb},
	booktitle    = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	publisher    = {{ACM}},
	pages        = {79--85},
	doi          = {10.1145/3375627.3375833}
}
@article{hoffman2018metrics,
	title        = {Metrics for explainable AI: Challenges and prospects},
	author       = {Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1812.04608},
	url          = {https://arxiv.org/abs/1812.04608}
}
@article{adadi2018peeking,
	title        = {Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)},
	author       = {Adadi, Amina and Berrada, Mohammed},
	year         = 2018,
	journal      = {IEEE access},
	publisher    = {IEEE},
	volume       = 6,
	pages        = {52138--52160},
	doi          = {10.1109/ACCESS.2018.2870052},
}
@inproceedings{dovsilovic2018explainable,
	title        = {Explainable artificial intelligence: A survey},
	author       = {Do{\v{s}}ilovi{\'c}, Filip Karlo and Br{\v{c}}i{\'c}, Mario and Hlupi{\'c}, Nikica},
	year         = 2018,
	booktitle    = {2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO)},
	pages        = {0210--0215},
	url          = {https://ieeexplore.ieee.org/abstract/document/8400040},
	organization = {IEEE},
}
@inproceedings{Abdul2018,
	title        = {Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda},
	author       = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	location     = {Montreal QC, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '18},
	pages        = {1–18},
	doi          = {10.1145/3173574.3174156},
	isbn         = {9781450356206},
	url          = {https://doi.org/10.1145/3173574.3174156}  ,
	abstract     = {Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorith-mic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research to-wards this goal.},
	keywords     = {intelligibility, explanations, interpretable machine learning, explainable artificial intelli-gence},
	numpages     = 18
}
@article{Survey2019,
	title        = {A Survey and on Neural and Network Interpretability},
	author       = {Yu Zhang and Peter Tiňo and Aleš Leonardis and Ke Tang},
	year         = 2019,
	journal      = {IEEE TRANSACTIONS},
	publisher    = {IEEE},
	abstract     = {Along with the great success of deep neural net- “adversarial example”. Nguyen et al. [16] showed another way works, there is also growing concern about their black-box to produce completely unrecognizable images (e.g., look like nature. The interpretability issue affects people’s trust on deep white noise), which are, however, recognized as certain objects learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a by DNNs with 99.99% confidence. These observations suggest desired property for deep networks to become powerful tools that even though DNNs can achieve superior performance on in other research fields, e.g., drug discovery and genomics. In many tasks, their underlying mechanisms may be very different this survey, we conduct a comprehensive review of the neural from those of humans’ and have not yet been well-understood. network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and A. An (Extended) Definition of Interpretability propose a novel taxonomy organized along three dimensions: To open the black-boxes of deep networks, many researchers type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global started to focus on the model interpretability. Although this interpretability). This taxonomy provides a meaningful 3D view theme has been explored in various papers, no clear consensus of distribution of papers from the relevant literature as two on the definition of interpretability has been reached. Most of the dimensions are not simply categorical but allow ordinal previous works skimmed over the clarification issue and left it subcategories. Finally, we summarize the existing interpretability as “you will know it when you see it”. If we take a closer look, evaluation methods and suggest possible research directions inspired by our new taxonomy. the suggested definitions and motivations for interpretability},
	readstatus   = {skimmed}
}
@incollection{Samek2019,
	title        = {Towards Explainable Artificial Intelligence},
	author       = {Wojciech Samek and Klaus-Robert Müller},
	year         = 2019,
	month        = 9,
	day          = 26,
	booktitle    = {Explainable {AI}: Interpreting, Explaining and Visualizing Deep Learning},
	publisher    = {Springer International Publishing},
	series       = {Explaining and Visualizing Deep Learning. Lecture Notes in Computer},
	volume       = {Science},
	pages        = {5--22},
	doi          = {10.1007/978-3-030-28954-6_1},
	date         = {2019-09-26},
	eprint       = {arXiv:1909.12072v1 [cs.AI]},
	keywords     = {Explainable Artificial Intelligence, Model Transparency, Deep Learning, Neural Networks, Interpretability}
}
@article{Guidotti2019,
	title        = {A Survey of Methods for Explaining Black Box Models},
	author       = {Riccardo Guidotti and Anna Monreale and Salvatore Ruggieri and Franco Turini and Fosca Giannotti and Dino Pedreschi},
	year         = 2019,
	month        = {sep},
	journal      = {{ACM} Computing Surveys},
	publisher    = {Association for Computing Machinery ({ACM})},
	volume       = 51,
	number       = 5,
	pages        = {1--42},
	doi          = {10.1145/3236009},
	abstract     = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	keywords     = {CCS Concepts: • Information systems → Decision support systems, Data analytics, Data mining, Open the black box, explanations, interpretability, transparent models},
	priority     = {prio1}
}
@article{Tjoa2021,
	title        = {A Survey on Explainable Artificial Intelligence ({XAI}): Toward Medical {XAI}},
	author       = {Erico Tjoa and Cuntai Guan},
	year         = 2020,
	journal      = {{IEEE} Transactions on Neural Networks and Learning Systems},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 32,
	number       = 11,
	pages        = {4793--4813},
	doi          = {10.1109/tnnls.2020.3027314},
	abstract     = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide "obviously" interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
	keywords     = {Explainable artificial intelligence (XAI), interpretability, machine learning (ML), medical information system, survey}
}
@inproceedings{Arrieta2020,
	title        = {Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
	author       = {Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	year         = 2020,
	month        = 12,
	day          = 30,
	url          = {https://www.sciencedirect.com/science/article/abs/pii/S1566253519308103},
	abstract     = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	date         = {2019-12-30},
	eprint       = {arXiv:1910.10045v2   [cs.AI]},
	keywords     = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
	priority     = {prio1},
	ranking      = {rank5}
}
@article{Vilone2020,
	title        = {Explainable Artificial Intelligence: a Systematic Review},
	author       = {Vilone, Giulia and Longo, Luca},
	year         = 2020,
	month        = 10,
	day          = 13,
	abstract     = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
	date         = {2020-10-13},
	eprint       = {arXiv:2006.00093v4[cs.AI]},
	keywords     = {Explainable artificial intelligence, method classification, survey, systematic literature review}
}
@article{Linardatos2020,
	title        = {Explainable {AI}: A Review of Machine Learning Interpretability Methods},
	author       = {Pantelis Linardatos and Vasilis Papastefanopoulos and Sotiris Kotsiantis},
	year         = 2020,
	month        = {dec},
	day          = 25,
	journal      = {Entropy},
	publisher    = {{MDPI} {AG}},
	volume       = 23,
	number       = 1,
	pages        = 18,
	doi          = {10.3390/e23010018},
	abstract     = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into "black box" approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
	date         = {2020-12-25},
	keywords     = {Linardatos, P., Papastefanopoulos, V., Kotsiantis, S xai, machine learning, explainability, interpretability, fairness, sensitivity, black-box}
}
@inproceedings{Jeyakumar2020,
	title        = {How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods},
	author       = {Jeyakumar, Jeya Vikranth and Noor, Joseph and Cheng, Yu-Hsi and Garcia, Luis and Srivastava, Mani},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {4211--4222},
	url          = {https://proceedings.neurips.cc/paper/2020/file/2c29d89cc56cdb191c60db2f0bae796b-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
}
@inproceedings{Srinivasan2020,
	title        = {Explanation Perspectives from the Cognitive Sciences - A Survey.},
	author       = {Srinivasan, Ramya and Chander, Ajay},
	year         = 2020,
	booktitle    = {IJCAI},
	pages        = {4812--4818},
}
@article{Samek2021,
	title        = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
	author       = {Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
	year         = 2021,
	journal      = {Proceedings of the IEEE},
	volume       = 109,
	number       = 3,
	pages        = {247--278},
	doi          = {10.1109/JPROC.2021.3060483},
}
@article{Kee1970,
	title        = {Conceptual and methodological considerations in the study of trust and suspicion},
	author       = {Kee, Herbert W and Knox, Robert E},
	year         = 1970,
	journal      = {Journal of conflict resolution},
	publisher    = {Sage Publications Sage CA: Thousand Oaks, CA},
	volume       = 14,
	number       = 3,
	pages        = {357--366},
}
@inproceedings{Papenmeier1907,
	title        = {How model accuracy and explanation fidelity influence user trust in AI},
	author       = {Papenmeier, Andrea and Englebienne, Gwenn and Seifert, Christin},
	year         = 1907,
	abstract     = {Machine learning systems have become popular in fields such as marketing, financing, or data mining. While they are highly accurate, complex machine learning systems pose challenges for engineers and users. Their inherent complexity makes it impossible to easily judge their fairness and the correctness of statistically learned relations between variables and classes. Explainable AI aims to solve this challenge by modelling explanations alongside with the classifiers, potentially improving user trust and acceptance. However, users should not be fooled by persuasive, yet untruthful explanations. We therefore conduct a user study in which we investigate the effects of model accuracy and explanation fidelity, i.e. how truthfully the explanation represents the underlying model, on user trust. Our findings show that accuracy is more important for user trust than explainability. Adding an explanation for a classification result can potentially harm trust, e.g. when adding nonsensical explanations. We also found that users cannot be tricked by high-fidelity explanations into having trust for a bad classifier. Furthermore, we found a mismatch between observed (implicit) and self-reported (explicit) trust.},
}
@article{Cramer2008,
	title        = {The effects of transparency on trust in and acceptance of a content-based art recommender},
	author       = {Cramer, Henriette and Evers, Vanessa and Ramlal, Satyan and Van Someren, Maarten and Rutledge, Lloyd and Stash, Natalia and Aroyo, Lora and Wielinga, Bob},
	year         = 2008,
	journal      = {User Modeling and User-adapted interaction},
	publisher    = {Springer},
	volume       = 18,
	number       = 5,
	pages        = {455--496},
	url          = {https://link.springer.com/article/10.1007/s11257-008-9051-3},
}
@inproceedings{Lim2009a,
	title        = {Why and Why Not Explanations Improve the Intelligibility of Context-Aware Intelligent Systems},
	author       = {Lim, Brian Y. and Dey, Anind K. and Avrahami, Daniel},
	year         = 2009,
	booktitle    = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	location     = {Boston, MA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '09},
	pages        = {2119–2128},
	doi          = {10.1145/1518701.1519023},
	isbn         = {9781605582467},
	url          = {https://doi.org/10.1145/1518701.1519023} ,
	abstract     = {Context-aware intelligent systems employ implicit inputs, and make decisions based on complex rules and machine learning models that are rarely clear to users. Such lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance of these systems. However, automatically providing explanations about a system's decision process can help mitigate this problem. In this paper we present results from a controlled study with over 200 participants in which the effectiveness of different types of explanations was examined. Participants were shown examples of a system's operation along with various automatically generated explanations, and then tested on their understanding of the system. We show, for example, that explanations describing why the system behaved a certain way resulted in better understanding and stronger feelings of trust. Explanations describing why the system did not behave a certain way, resulted in lower understanding yet adequate performance. We discuss implications for the use of our findings in real-world context-aware applications.},
	keywords     = {explanations, context-aware, intelligibility},
	numpages     = 10
}
@inproceedings{kulesza2013too,
	title        = {Too much, too little, or just right? Ways explanations impact end users' mental models},
	author       = {Kulesza, Todd and Stumpf, Simone and Burnett, Margaret and Yang, Sherry and Kwan, Irwin and Wong, Weng-Keen},
	year         = 2013,
	booktitle    = {2013 IEEE Symposium on visual languages and human centric computing},
	pages        = {3--10},
	doi          = {.org/10.1109/VLHCC.2013.6645235}       ,
	organization = {IEEE},
}
@inproceedings{Bussone2015,
	title        = {The Role of Explanations on Trust and Reliance in Clinical Decision Support Systems},
	author       = {Bussone, Adrian and Stumpf, Simone and O'Sullivan, Dympna},
	year         = 2015,
	booktitle    = {2015 International Conference on Healthcare Informatics},
	pages        = {160--169},
	doi          = {10.1109/ICHI.2015.26},
}
@inproceedings{Ribeiro2016,
	title        = {"Why should i trust you?" Explaining the predictions of any classifier},
	author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year         = 2016,
	booktitle    = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
	pages        = {1135--1144},
	priority     = {prio2},
	readstatus   = {skimmed}
}
@inproceedings{Eslami2018,
	title        = {Communicating Algorithmic Process in Online Behavioral Advertising},
	author       = {Eslami, Motahhare and Krishna Kumaran, Sneha R. and Sandvig, Christian and Karahalios, Karrie},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	location     = {Montreal QC, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '18},
	pages        = {1–13},
	doi          = {10.1145/3173574.3174006},
	isbn         = {9781450356206},
	url          = {https://doi.org/10.1145/3173574.3174006}     ,
	abstract     = {Advertisers develop algorithms to select the most relevant advertisements for users. However, the opacity of these algorithms, along with their potential for violating user privacy, has decreased user trust and preference in behavioral advertising. To mitigate this, advertisers have started to communicate algorithmic processes in behavioral advertising. However, how revealing parts of the algorithmic process affects users' perceptions towards ads and platforms is still an open question. To investigate this, we exposed 32 users to why an ad is shown to them, what advertising algorithms infer about them, and how advertisers use this information. Users preferred interpretable, non-creepy explanations about why an ad is presented, along with a recognizable link to their identity. We further found that exposing users to their algorithmically-derived attributes led to algorithm disillusionment---users found that advertising algorithms they thought were perfect were far from it. We propose design implications to effectively communicate information about advertising algorithms.},
	keywords     = {process communication, advertising algorithms, ad explanation, algorithmic authority, algorithmic transparency},
	numpages     = 13
}
@inproceedings{MingYin2019,
	title        = {Understanding the Effect of Accuracy on Trust in Machine Learning Models},
	author       = {Ming Yin, Jennifer Wortman Vaughan, Hanna Wallach},
	year         = 2019,
	doi          = {.org/10.1145/3290605.3300509} ,
	abstract     = {-  Human-centered computing  ->  -1Empirical studies in HCI.-  Computing methodologies  ->  -1Machine learning.},
	keywords     = {may misunderstand or mistrust its predictions [6, 16, 25]. Machine learning, trust, human-subject experiments Prompted by these challenges, as well as growing concerns}
}
@inproceedings{nourani2019effects,
	title        = {The Effects of Meaningful and Meaningless Explanations on Trust and Perceived System Accuracy in Intelligent Systems},
	author       = {Nourani, Mahsan and Kabir, Samia and Mohseni, Sina and Ragan, Eric D},
	year         = 2019,
	booktitle    = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	volume       = 7,
	pages        = {97--105},
	url          = {https://ojs.aaai.org/index.php/HCOMP/article/view/5284},
}
@inproceedings{Honeycutt2020,
	title        = {Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy},
	author       = {Donald R. Honeycutt, Mahsan Nourani, Eric D. Ragan},
	year         = 2020,
	booktitle    = {Proceedings of the Eighth AAAI Conference on Human Computation and Crowdsourcing (HCOMP-20)},
	abstract     = {Proceedings of the Eighth AAAI Conference on Human Computation and Crowdsourcing (HCOMP-20)},
	keywords     = {Full},
	readstatus   = {skimmed}
}
@inproceedings{Zhang2020,
	title        = {Effect of confidence and explanation on accuracy and trust calibration in {AI}-assisted decision making},
	author       = {Yunfeng Zhang and Q. Vera Liao and Rachel K. E. Bellamy},
	year         = 2020,
	month        = {jan},
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	publisher    = {{ACM}},
	doi          = {10.1145/3351095.3372852},
	abstract     = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
	keywords     = {decision support, trust, confidence, explainable AI}
}
@inproceedings{Wanner2020,
	title        = {White, Grey, Black: Effects of XAI Augmentation on the Confidence in AI-based Decision Support Systems Short Paper},
	author       = {Wanner, Jonas and Herm, Lukas-Valentin and Heinrich, Kai and Janiesch, Christian and Zschech, Patrick},
	year         = 2020,
	abstract     = {AI-based decision support systems (DSS) have become increasingly popular for solving a variety of tasks in both, low-stake, and high-stake situations. However, due to their complexity, they often lack transparency into their decision process. Therefore, the field of explainable AI (XAI) has emerged to provide explanations for these black-box systems. While XAI research assumes an increase in confidence when using their augmented greybox systems, test designs for this proposition are scarce. Therefore, we propose an empirical study to test the effect of black-box, grey-box, and white-box explanations on a domain expert's confidence in the system, and subsequently on the effectiveness of the overall decision process. For this purpose, we derive hypotheses from theory and implement AI-based DSS with XAI augmentations for low-stake and high-stake situations. Further, we provide detailed information on a future survey-based study, which we will conduct to complete this research-in-progress.},
	keywords     = {Explainable AI, Confidence, Decision Support Systems, User-study, Maintenance}
}
@inproceedings{Ehsan2020,
	title        = {Human-centered explainable ai: Towards a reflective sociotechnical approach},
	author       = {Ehsan, Upol and Riedl, Mark O},
	year         = 2020,
	booktitle    = {International Conference on Human-Computer Interaction},
	pages        = {449--466},
	organization = {Springer},
}
@inproceedings{Omeiza2021,
	title        = {Why Not Explain? Effects of Explanations on Human Perceptions of Autonomous Driving},
	author       = {Daniel Omeiza and Konrad Kollnig and Helena Web and Marina Jirotka and Lars Kunze},
	year         = 2021,
	month        = {jul},
	booktitle    = {2021 {IEEE} International Conference on Advanced Robotics and Its Social Impacts ({ARSO})},
	publisher    = {{IEEE}},
	doi          = {10.1109/arso51874.2021.9542835},
	abstract     = {Autonomous vehicles (AVs) have the potential to change the way we commute, travel, and transport our goods. The deployment of AVs in society, however, requires that people understand, accept, and trust them. Intelligible explanations can help different AV stakeholders to assess AVs' behaviours, and in turn, increase their confidence and foster trust. In a user study (N = 101), we examined different explanation types (based on investigatory queries) provided by an AV and their effect on people using the trust determinant factors. Our quantitative and qualitative analysis shows that explanations with causal attributions improved task performance and understanding when assessing driving events but did not directly improve perceived trust. This underlines the potential need for additional measures and research to enhance trust in AVs.},
}
@inproceedings{Guesmi2021,
	title        = {Input or Output: Effects of Explanation Focus on the Perception of Explainable Recommendation with Varying Level of Details.},
	author       = {Guesmi, Mouadh and Chatti, Mohamed Amine and Vorgerd, Laura and Joarder, Shoeb Ahmed and Ain, Qurat Ul and Ngo, Thao and Zumor, Shadi and Sun, Yiqi and Ji, Fangzheng and Muslim, Arham},
	year         = 2021,
	booktitle    = {IntRS@ RecSys},
	pages        = {55--72},
	abstract     = {In this paper, we shed light on two important design choices in explainable recommender systems (RS) namely, explanation focus and explanation level of detail. We developed a transparent Recommendation and Interest Modeling Application (RIMA) that provides on-demand personalized explanations of the input (user model) and output (recommendations), with three levels of detail (basic, intermediate, advanced) to meet the demands of different types of end-users. We conducted a within-subject study to investigate the relationship between explanation focus and the explanation level of detail, and the effects of these two variables on the perception of the explainable RS with regard to different explanation aims. Our results show that the perception of explainable RS with different levels of detail is affected to different degrees by the explanation focus. Consequently, we provided some suggestions to support the effective design of explanations in RS.},
	keywords     = {recommender system, explainable recommendation, personalized explanation, explanation design choices}
}
@article{Shin2021,
	title        = {The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI},
	author       = {Donghee Shin},
	year         = 2021,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 146,
	pages        = 102551,
	doi          = {https://doi.org/10.1016/j  .ijhcs.2020.102551},
	issn         = {1071-5819},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581920301531},
	abstract     = {Artificial intelligence and algorithmic decision-making processes are increasingly criticized for their black-box nature. Explainable AI approaches to trace human-interpretable decision processes from algorithms have been explored. Yet, little is known about algorithmic explainability from a human factors’ perspective. From the perspective of user interpretability and understandability, this study examines the effect of explainability in AI on user trust and attitudes toward AI. It conceptualizes causability as an antecedent of explainability and as a key cue of an algorithm and examines them in relation to trust by testing how they affect user perceived performance of AI-driven services. The results show the dual roles of causability and explainability in terms of its underlying links to trust and subsequent user behaviors. Explanations of why certain news articles are recommended generate users trust whereas causability of to what extent they can understand the explanations affords users emotional confidence. Causability lends the justification for what and how should be explained as it determines the relative importance of the properties of explainability. The results have implications for the inclusion of causability and explanatory cues in AI systems, which help to increase trust and help users to assess the quality of explanations. Causable explainable AI will help people understand the decision-making process of AI algorithms by bringing transparency and accountability into AI systems.},
	keywords     = {Explainable Ai, Causability, Human-ai interaction, Explanatorycues, Interpretability, Understandability, Trust, Glassbox, Human-centeredAI}
}
@inproceedings{Wang2021,
	title        = {Are Explanations Helpful? A Comparative Study of the Effects of Explanations in {AI}-Assisted Decision-Making},
	author       = {Xinru Wang and Ming Yin},
	year         = 2021,
	month        = {apr},
	day          = 16,
	booktitle    = {26th International Conference on Intelligent User Interfaces},
	publisher    = {{ACM}},
	doi          = {10.1145/3397481.3450650},
	abstract     = {This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy-improve people's understanding of the AI model, help people recognize the model uncertainty, and support people's calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making. CCS CONCEPTS • Human-centered computing → Empirical studies in HCI; • Computing methodologies → Machine learning.},
	keywords     = {interpretable machine learning, explainable AI, trust, trust calibration, human-subject experiments},
	language     = {en},
	source       = {iui21-16.tex}
}
@article{Dodge2021,
	title        = {From “no clear winner” to an effective Explainable Artificial Intelligence process: An empirical journey},
	author       = {Dodge, Jonathan and Anderson, Andrew and Khanna, Roli and Irvine, Jed and Dikkala, Rupika and Lam, Kin-Ho and Tabatabai, Delyar and Ruangrotsakun, Anita and Shureih, Zeyad and Kahng, Minsuk and Fern, Alan and Burnett, Margaret},
	year         = 2021,
	journal      = {Applied AI Letters},
	volume       = 2,
	number       = 4,
	pages        = {e36},
	doi          = {https://doi.org/10.1002/ail2.36},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.36},
	abstract     = {Abstract “In what circumstances would you want this AI to make decisions on your behalf?” We have been investigating how to enable a user of an Artificial Intelligence-powered system to answer questions like this through a series of empirical studies, a group of which we summarize here. We began the series by (a) comparing four explanation configurations of saliency explanations and/or reward explanations. From this study we learned that, although some configurations had significant strengths, no one configuration was a clear “winner.” This result led us to hypothesize that one reason for the low success rates Explainable AI (XAI) research has in enabling users to create a coherent mental model is that the AI itself does not have a coherent model. This hypothesis led us to (b) build a model-based agent, to compare explaining it with explaining a model-free agent. Our results were encouraging, but we then realized that participants' cognitive energy was being sapped by having to create not only a mental model, but also a process by which to create that mental model. This realization led us to (c) create such a process (which we term After-Action Review for AI or “AAR/AI”) for them, integrate it into the explanation environment, and compare participants' success with AAR/AI scaffolding vs without it. Our AAR/AI studies' results showed that AAR/AI participants were more effective assessing the AI than non-AAR/AI participants, with significantly better precision and significantly better recall at finding the AI's reasoning flaws.},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ail2.36},
	keywords     = {after-action review for AI, empirical studies, explainable AI, human-computer interaction}
}
@article{Riveiro2021,
	title        = {“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems},
	author       = {Maria Riveiro and Serge Thill},
	year         = 2021,
	journal      = {Artificial Intelligence},
	volume       = 298,
	pages        = 103507,
	doi          = {https://doi.org/10.1016/j .artint.2021.103507},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370221000588},
	abstract     = {Research in the social sciences has shown that expectations are an important factor in explanations as used between humans: rather than explaining the cause of an event per se, the explainer will often address another event that did not occur but that the explainee might have expected. For AI-powered systems, this finding suggests that explanation-generating systems may need to identify such end user expectations. In general, this is a challenging task, not the least because users often keep them implicit; there is thus a need to investigate the importance of such an ability. In this paper, we report an empirical study with 181 participants who were shown outputs from a text classifier system along with an explanation of why the system chose a particular class for each text. Explanations were both factual, explaining why the system produced a certain output or counterfactual, explaining why the system produced one output instead of another. Our main hypothesis was explanations should align with end user expectations; that is, a factual explanation should be given when the system's output is in line with end user expectations, and a counterfactual explanation when it is not. We find that factual explanations are indeed appropriate when expectations and output match. When they do not, neither factual nor counterfactual explanations appear appropriate, although we do find indications that our counterfactual explanations contained at least some necessary elements. Overall, this suggests that it is important for systems that create explanations of AI systems to infer what outputs the end user expected so that factual explanations can be generated at the appropriate moments. At the same time, this information is, by itself, not sufficient to also create appropriate explanations when the output and user expectations do not match. This is somewhat surprising given investigations of explanations in the social sciences, and will need more scrutiny in future studies.},
	keywords     = {Expectations, Explanations, Factual, Counterfactual, Contrastive, Explainable AI, Mental models, Machine behaviour, Human-AI interaction}
}
@article{Ehsan2021,
	title        = {The who in explainable ai: How ai background shapes perceptions of ai explanations},
	author       = {Ehsan, Upol and Passi, Samir and Liao, Q Vera and Chan, Larry and Lee, I and Muller, Michael and Riedl, Mark O and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2107.13509},
}
@inproceedings{Ehsan2021a,
	title        = {Expanding Explainability: Towards Social Transparency in AI Systems},
	author       = {Ehsan, Upol and Liao, Q. Vera and Muller, Michael and Riedl, Mark O. and Weisz, Justin D.},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	location     = {Yokohama, Japan},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '21},
	doi          = {10.1145/3411764.3445188},
	isbn         = {9781450380966},
	url          = {https://doi.org/10.1145/3411764.3445188},
	abstract     = {As AI-powered systems increasingly mediate consequential decision-making, their explainability is critical for end-users to take informed and accountable actions. Explanations in human-human interactions are socially-situated. AI systems are often socio-organizationally embedded. However, Explainable AI (XAI) approaches have been predominantly algorithm-centered. We take a developmental step towards socially-situated XAI by introducing and exploring Social Transparency (ST), a sociotechnically informed perspective that incorporates the socio-organizational context into explaining AI-mediated decision-making. To explore ST conceptually, we conducted interviews with 29 AI users and practitioners grounded in a speculative design scenario. We suggested constitutive design elements of ST and developed a conceptual framework to unpack ST’s effect and implications at the technical, decision-making, and organizational level. The framework showcases how ST can potentially calibrate trust in AI, improve decision-making, facilitate organizational collective actions, and cultivate holistic explainability. Our work contributes to the discourse of Human-Centered XAI by expanding the design space of XAI.},
	articleno    = 82,
	keywords     = {explanations, Explainable AI, socio-organizational context, social transparency, Artificial Intelligence, human-AI interaction, sociotechnical},
	numpages     = 19
}
@inproceedings{Chromik2021,
	title        = {I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI},
	author       = {Chromik, Michael and Eiband, Malin and Buchner, Felicitas and Kr\"{u}ger, Adrian and Butz, Andreas},
	year         = 2021,
	booktitle    = {26th International Conference on Intelligent User Interfaces},
	location     = {College Station, TX, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {IUI '21},
	pages        = {307–317},
	doi          = {10.1145/3397481.3450644},
	isbn         = {9781450380171},
	url          = {https://doi.org/10.1145/3397481.3450644},
	abstract     = {Unintended consequences of deployed AI systems fueled the call for more interpretability in AI systems. Often explainable AI (XAI) systems provide users with simplifying local explanations for individual predictions but leave it up to them to construct a global understanding of the model behavior. In this work, we examine if non-technical users of XAI fall for an illusion of explanatory depth when interpreting additive local explanations. We applied a mixed methods approach consisting of a moderated study with 40 participants and an unmoderated study with 107 crowd workers using a spreadsheet-like explanation interface based on the SHAP framework. We observed what non-technical users do to form their mental models of global AI model behavior from local explanations and how their perception of understanding decreases when it is examined.},
	keywords     = {explainable AI, understanding, Shapley explanation, cognitive bias},
	numpages     = 11
}
@article{Linder2021,
	title        = {How level of explanation detail affects human performance in interpretable intelligent systems: A study on explainable fact checking},
	author       = {Linder, Rhema and Mohseni, Sina and Yang, Fan and Pentyala, Shiva K. and Ragan, Eric D. and Hu, Xia Ben},
	year         = 2021,
	journal      = {Applied AI Letters},
	volume       = 2,
	number       = 4,
	pages        = {e49},
	doi          = {https://doi.org/10.1002/ail2.49},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.49},
	abstract     = {Abstract Explainable artificial intelligence (XAI) systems aim to provide users with information to help them better understand computational models and reason about why outputs were generated. However, there are many different ways an XAI interface might present explanations, which makes designing an appropriate and effective interface an important and challenging task. Our work investigates how different types and amounts of explanatory information affect user ability to utilize explanations to understand system behavior and improve task performance. The presented research employs a system for detecting the truthfulness of news statements. In a controlled experiment, participants were tasked with using the system to assess news statements as well as to learn to predict the output of the AI. Our experiment compares various levels of explanatory information to contribute empirical data about how explanation detail can influence utility. The results show that more explanation information improves participant understanding of AI models, but the benefits come at the cost of time and attention needed to make sense of the explanation.},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ail2.49}   ,
	keywords     = {explainable artificial intelligence, human-computer interaction, machine learning, transparency}
}
@inproceedings{Leffrang2021,
	title        = {Should I Follow this Model? The Effect of Uncertainty Visualization on the Acceptance of Time Series Forecasts},
	author       = {Leffrang, Dirk and Müller, Oliver},
	year         = 2021,
	booktitle    = {2021 IEEE Workshop on TRust and EXpertise in Visual Analytics (TREX)},
	pages        = {20--26},
	doi          = {10.1109/TREX53765.2021.00009},
}
@article{kuhl2022keep,
	title        = {Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting},
	author       = {Kuhl, Ulrike and Artelt, Andr{\'e} and Hammer, Barbara},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.05515}    ,
	keywords     = {XAI, counterfactual explanations, quantitative user evaluation, Data and Algorithm Evaluation, Human Factors Anonymous Author(s). 2022. Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather}
}

@article{Dikmen2022,
	title        = {The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending},
	author       = {Murat Dikmen and Catherine Burns},
	year         = 2022,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 162,
	pages        = 102792,
	doi          = {https://doi.org/10.1016/j.ijhcs.2022.102792}      ,
	issn         = {1071-5819},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581922000210},
	abstract     = {Increasingly, artificial intelligence (AI) is being used to assist complex decision-making such as financial investing. However, there are concerns regarding the black-box nature of AI algorithms. The field of explainable AI (XAI) has emerged to address these concerns. XAI techniques can reveal how an AI decision is formed and can be used to understand and appropriately trust an AI system. However, XAI techniques still may not be human-centred and may not support human decision-making adequately. In this work, we explored how domain knowledge, identified by expert decision makers, can be used to achieve a more human-centred approach to AI. We measured the effect of domain knowledge on trust in AI, reliance on AI, and task performance in an AI-assisted complex decision-making environment. In a peer-to-peer lending simulator, non-expert participants made financial investments using an AI assistant. The presence or absence of domain knowledge was manipulated. The results showed that participants who had access to domain knowledge relied less on the AI assistant when the AI assistant was incorrect and indicated less trust in AI assistant. However, overall investing performance was not affected. These results suggest that providing domain knowledge can influence how non-expert users use AI and could be a powerful tool to help these users develop appropriate levels of trust and reliance.},
	keywords     = {Explainable AI, Human-AI interaction, Domain knowledge, Trust in AI}
}
@article{Wang2022,
	title        = {Effects of Explanations in {AI}-Assisted Decision Making: Principles and Comparisons},
	author       = {Xinru Wang and Ming Yin},
	year         = 2022,
	month        = {apr},
	journal      = {{ACM} Transactions on Interactive Intelligent Systems},
	publisher    = {Association for Computing Machinery ({ACM})},
	doi          = {10.1145/3519266},
	abstract     = {Recent years have witnessed the growing literature in empirical evaluation of explainable AI (XAI) methods. This study contributes to this ongoing conversation by presenting a comparison on the efects of a set of established XAI methods in AI-assisted decision making. Based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy Ð improve people's understanding of the AI model, help people recognize the model uncertainty, and support people's calibrated trust in the model. Through three randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of AI models of varying levels of complexity, and in two kinds of decision making contexts where people perceive themselves as having diferent levels of domain expertise. Our results demonstrate that many AI explanations do not satisfy any of the desirable properties when used on decision making tasks that people have little domain expertise in. On decision making tasks that people are more knowledgeable, the feature contribution explanation is shown to satisfy more desiderata of AI explanations, even when the AI model is inherently complex. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making, and for advancing more rigorous empirical evaluation of XAI methods. CCS Concepts: • Human-centered computing → Empirical studies in HCI; • Computing methodologies → Machine learning.},
	keywords     = {interpretable machine learning, explainable AI, trust, trust calibration, human-subject experiments}
}
@article{Papenmeier2022,
	title        = {It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in {AI}},
	author       = {Andrea Papenmeier and Dagmar Kern and Gwenn Englebienne and Christin Seifert},
	year         = 2022,
	month        = {aug},
	journal      = {{ACM} Transactions on Computer-Human Interaction},
	publisher    = {Association for Computing Machinery ({ACM})},
	volume       = 29,
	number       = 4,
	pages        = {1--33},
	doi          = {10.1145/3495013},
	abstract     = {Automated decision-making systems become increasingly powerful due to higher model complexity. While powerful in prediction accuracy, Deep Learning models are black boxes by nature, preventing users from making informed judgments about the correctness and fairness of such an automated system. Explanations have been proposed as a general remedy to the black box problem. However, it remains unclear if effects of explanations on user trust generalise over varying accuracy levels. In an online user study with 959 participants, we examined the practical consequences of adding explanations for user trust: We evaluated trust for three explanation types on three classifiers of varying accuracy. We find that the influence of our explanations on trust differs depending on the classifier's accuracy. Thus, the interplay between trust and explanations is more complex than previously reported. Our findings also reveal discrepancies between self-reported and behavioural trust, showing that the choice of trust measure impacts the results. CCS Concepts: • Human-centered computing → Empirical studies in HCI; Human computer interaction (HCI);},
	keywords     = {Explainable AI, machine learning, minimum explanations, user trust, explanation fidelity This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike International 4.0 License}
}
@article{Riefle2022,
	title        = {On the Influence of Cognitive Styles on Users' Understanding of Explanations},
	author       = {Lara Riefle and Patrick Hemmer and Carina Benz and Michael Vossing and Jannik Pries},
	year         = 2022,
	journal      = {ArXiv},
	volume       = {abs/2210.02123},
}
@article{Kim2022a,
	title        = {"Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction},
	author       = {Sunnie Kim and Elizabeth Anne Watkins and Olga Russakovsky and Ruth C. Fong and A. Monroy-Hern{\'a}ndez},
	year         = 2022,
	journal      = {ArXiv},
	volume       = {abs/2210.03735},
}

@article{Lewandowsky2000,
	title        = {The dynamics of trust: comparing humans to automation.},
	author       = {Stephan Lewandowsky and Max Mundy and G P Tan},
	year         = 2000,
	journal      = {Journal of experimental psychology. Applied},
	volume       = {6 2},
	pages        = {104--23},
	doi          = {10.1037//1076-898x.6.2.104},
	url          = {https://pubmed.ncbi.nlm.nih.gov/10937315/},
}
@article{Dzindolet2003,
	title        = {The role of trust in automation reliance},
	author       = {Mary T. Dzindolet and Scott A. Peterson and Regina A. Pomranky and Linda G. Pierce and Hall P. Beck},
	year         = 2003,
	month        = {jun},
	journal      = {International Journal of Human-Computer Studies},
	publisher    = {Elsevier {BV}},
	volume       = 58,
	number       = 6,
	pages        = {697--718},
	doi          = {10.1016/s1071-5819(03)00038-7},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581903000387},
	abstract     = {A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.},
	keywords     = {Automation trust, Automation reliance, Misuse, Disuse},
	priority     = {prio2}
}
@article{lee2004trust,
	title        = {Trust in automation: Designing for appropriate reliance},
	author       = {Lee, John D and See, Katrina A},
	year         = 2004,
	journal      = {Human factors},
	publisher    = {SAGE Publications Sage UK: London, England},
	volume       = 46,
	number       = 1,
	pages        = {50--80},
	url          = {https://journals.sagepub.com/doi/abs/10.1518/hfes.46.1.50_30392}  ,
}
@article{Parasuraman2008,
	title        = {Situation Awareness, Mental Workload, and Trust in Automation: Viable, Empirically Supported Cognitive Engineering Constructs},
	author       = {Raja Parasuraman and Thomas B. Sheridan and Christopher D. Wickens},
	year         = 2008,
	month        = {jun},
	journal      = {Journal of Cognitive Engineering and Decision Making},
	publisher    = {{SAGE} Publications},
	volume       = 2,
	number       = 2,
	pages        = {140--160},
	doi          = {10.1518/155534308x284417},
	abstract     = {Cognitive engineering needs viable constructs and principles to promote better understanding and prediction of human performance in complex systems. Three human cognition and performance constructs that have been the subjects of much attention in research and practice over the past three decades are situation awareness (SA), mental workload, and trust in automation. Recently, Dekker and Woods (2002) and Dekker and Hollnagel (2004; henceforth DWH) argued that these constructs represent "folk models" without strong empirical foundations and lacking scientific status. We counter this view by presenting a brief description of the large science base of empirical studies on these constructs. We show that the constructs can be operationalized using behavioral, physiological, and subjective measures, supplemented by computational modeling, but that the constructs are also distinct from human performance. DWH also caricatured as "abracadabra" a framework suggested by us to address the problem of the design of automated systems (Parasuraman, Sheridan, & Wickens, 2000). We point to several factual and conceptual errors in their description of our approach. Finally, we rebut DWH's view that SA, mental workload, and trust represent folk concepts that are not falsifiable. We conclude that SA, mental workload, and trust are viable constructs that are valuable in understanding and predicting human-system performance in complex systems.},
}
@inproceedings{Jenkins2010,
	title        = {Measuring Trust and Application of Eye Tracking in Human Robotic Interaction},
	author       = {Jenkins, Quaneisha and Jiang, Xiaochun},
	year         = 2010,
	booktitle    = {IIEAnnual Conference and Expo},
	abstract     = {The interaction between operator and robot in the context of Urban Search and Rescue (USAR) missions provides the opportunity to further study operator trust in human-robot interfaces. USAR missions are characterized by high uncertainty and risk, lending them to be situations where operator trust is critical to mission performance. Prior research using subjective questions to measure trust has indicated that there is a difference in operator trust measurements for different interfaces. This research presented here further analyzes trust in human-robot interaction (HRI) by examining trust and eye movements such as duration, sequence, and frequency of fixations for common mission tasks.},
	keywords     = {Trust, human-robot interaction, eye tracking}
}
@article{hoffman2013trust,
	title        = {Trust in automation},
	author       = {Hoffman, Robert R and Johnson, Matthew and Bradshaw, Jeffrey M and Underbrink, Al},
	year         = 2013,
	journal      = {IEEE Intelligent Systems},
	publisher    = {IEEE},
	volume       = 28,
	number       = 1,
	pages        = {84--88},
}
@article{hoff2015trust,
	title        = {Trust in automation: Integrating empirical evidence on factors that influence trust},
	author       = {Hoff, Kevin Anthony and Bashir, Masooda},
	year         = 2015,
	journal      = {Human factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 57,
	number       = 3,
	pages        = {407--434},
	doi          = {10.1177/0018720814547570},
	abstract     = {We systematically review recent empirical research on factors that influence trust in automation to present a three-layered trust model that synthesizes existing knowledge. Background: Much of the existing research on factors that guide human-automation interaction is centered around trust, a variable that often determines the willingness of human operators to rely on automation. Studies have utilized a variety of different automated systems in diverse experimental paradigms to identify factors that impact operators' trust. Method: We performed a systematic review of empirical research on trust in automation from January 2002 to June 2013. Papers were deemed eligible only if they reported the results of a human-subjects experiment in which humans interacted with an automated system in order to achieve a goal. Additionally, a relationship between trust (or a trust-related behavior) and another variable had to be measured. All together, 101 total papers, containing 127 eligible studies, were included in the review. Results: Our analysis revealed three layers of variability in human-automation trust (dispositional trust, situational trust, and learned trust), which we organize into a model. We propose design recommendations for creating trustworthy automation and identify environmental conditions that can affect the strength of the relationship between trust and reliance. Future research directions are also discussed for each layer of trust. Conclusion: Our three-layered trust model provides a new lens for conceptualizing the variability of trust in automation. Its structure can be applied to help guide future research and develop training interventions and design procedures that encourage appropriate trust.},
}
@inproceedings{Chien2016,
	title        = {Influence of Cultural Factors in Dynamic Trust in Automation},
	author       = {Chien, Shih-Yi and Lewis, Michael and Sycara, Katia},
	year         = 2016,
	booktitle    = {2016 IEEE International Conference on Systems, Man, and Cybernetics' SMC 20161 October 9-12,2016' Budapest, Hungary},
	publisher    = {IEEE},
	url          = {https://ieeexplore.ieee.org/abstract/document/7844677},
	abstract     = {The use of autonomous systems has been rapidly increasing in recent decades. To improve human-automation interaction, trust has been closely studied. Research shows trust is critical in the development of appropriate reliance on automation. To examine how trust mediates the human-automation relationships across cultures, the present study investigated the influences of cultural factors on trust in automation. Theoretically guided empirical studies were conducted in the U.S., Taiwan and Turkey to examine how cultural dynamics affect various aspects of trust in automation. The results found significant cultural differences in human trust attitude in automation.},
	keywords     = {Trust in Automation, Human Factors, Culture, Human computer Interaction, Reliability, Workload, User Study},
	readstatus   = {read}
}
@article{Miller2016,
	title        = {Behavioral Measurement of Trust in Automation},
	author       = {David Miller and Mishel Johns and Brian Mok and Nikhil Gowda and David Sirkin and Key Lee and Wendy Ju},
	year         = 2016,
	month        = {sep},
	journal      = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	publisher    = {{SAGE} Publications},
	volume       = 60,
	number       = 1,
	pages        = {1849--1853},
	doi          = {10.1177/1541931213601422},
	abstract     = {Stating that one trusts a system is markedly different from demonstrating that trust. To investigate trust in automation, we introduce the trust fall: a two-stage behavioral test of trust. In the trust fall paradigm, first the one learns the capabilities of the system, and in the second phase, the 'fall,' one's choices demonstrate trust or distrust. Our first studies using this method suggest the value of measuring behaviors that demonstrate trust, compared with self-reports of one's trust. Designing interfaces that encourage appropriate trust in automation will be critical for the safe and successful deployment of partially automated vehicles, and this will rely on a solid understanding of whether these interfaces actually inspire trust and encourage supervision.},
}
@inproceedings{korber2018theoretical,
	title        = {Theoretical considerations and development of a questionnaire to measure trust in automation},
	author       = {K{\"o}rber, Moritz},
	year         = 2018,
	booktitle    = {Congress of the International Ergonomics Association},
	pages        = {13--30},
	organization = {Springer},
}
@article{Kohn2020,
	title        = {A Brief Review of Frequently Used Self-Report Measures of Trust in Automation},
	author       = {Spencer C. Kohn and Molly Kluck and Tyler Shaw},
	year         = 2020,
	month        = {dec},
	journal      = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	publisher    = {{SAGE} Publications},
	volume       = 64,
	number       = 1,
	pages        = {1436--1440},
	doi          = {10.1177/1071181320641342},
	abstract     = {With the growing popularity of trust in automation research, there is a strong need for a review of which methods are used to measure trust in automation. This review catalogues the self-report methods commonly used to measure trust in automation, via publications found in the PsychINFO and ACM databases. The frequency of commonly deployed self-report scales are reported and the state of self-report measurement of trust in automation is discussed in brief.},
	keywords     = {and Databases of exclusion began by removing works based on un}
}
@incollection{Gurney2022,
	title        = {Measuring and~Predicting Human Trust in~Recommendations from~an~{AI} Teammate},
	author       = {Nikolos Gurney and David V. Pynadath and Ning Wang},
	year         = 2022,
	booktitle    = {Artificial Intelligence in {HCI}},
	publisher    = {Springer International Publishing},
	series       = {LNAI},
	volume       = 13336,
	pages        = {22--34},
	doi          = {10.1007/978-3-031-05643-7_2},
	editor       = {©c The Author(s) and under exclusive license to Springer Nature Switzerland AG 2022 H. Degen and S. Ntoa},
	abstract     = {Predicting compliance with AI recommendations and knowing when to intervene are critical facets of human-AI teaming. AIs are typically deployed in settings where their abilities to evaluate decision variables far exceed the abilities of their human counterparts. However, even though AIs excel at weighing multiple issues and computing near optimal solutions with speed and accuracy beyond that of any human, they still make mistakes. Thus, perfect compliance may be undesirable. This means, just as individuals must know when to follow the advice of other people, it is critical for them to know when to adopt the recommendations from their AI. Well-calibrated trust is thought to be a fundamental aspect of this type of knowledge. We compare the ability of a common trust inventory and the ability of a behavioral measure of trust to predict compliance and success in a reconnaissance mission. We interpret the experimental results to suggest that the behavioral measure is a better predictor of overall mission compliance and success. We discuss how this measure could possibly be used in compliance interventions and related open questions.},
	keywords     = {Trust in AI, Explainable machine learning, AI compliance}
}
@inproceedings{Papenmeier1907,
	title        = {How model accuracy and explanation fidelity influence user trust in AI},
	author       = {Papenmeier, Andrea and Englebienne, Gwenn and Seifert, Christin},
	year         = 1907,
	abstract     = {Machine learning systems have become popular in fields such as marketing, financing, or data mining. While they are highly accurate, complex machine learning systems pose challenges for engineers and users. Their inherent complexity makes it impossible to easily judge their fairness and the correctness of statistically learned relations between variables and classes. Explainable AI aims to solve this challenge by modelling explanations alongside with the classifiers, potentially improving user trust and acceptance. However, users should not be fooled by persuasive, yet untruthful explanations. We therefore conduct a user study in which we investigate the effects of model accuracy and explanation fidelity, i.e. how truthfully the explanation represents the underlying model, on user trust. Our findings show that accuracy is more important for user trust than explainability. Adding an explanation for a classification result can potentially harm trust, e.g. when adding nonsensical explanations. We also found that users cannot be tricked by high-fidelity explanations into having trust for a bad classifier. Furthermore, we found a mismatch between observed (implicit) and self-reported (explicit) trust.},
}
@article{ENDSLEY1999,
	title        = {Level of automation effects on performance, situation awareness and workload in a dynamic control task},
	author       = {MICA R. ENDSLEY and DAVID B. KABER},
	year         = 1999,
	month        = {mar},
	journal      = {Ergonomics},
	publisher    = {Informa {UK} Limited},
	volume       = 42,
	number       = 3,
	pages        = {462--492},
	doi          = {10.1080/001401399185595},
	keywords     = {Level of automation (LOA), Human-centred automation, Situation awareness (SA), Workload, Out-of-the-loop performance}
}
@techreport{Parasuraman2000,
	title        = {A model for types and levels of human interaction with automation},
	author       = {R. Parasuraman and T.B. Sheridan and C.D. Wickens},
	year         = 2000,
	publisher    = {IEEE},
	number       = {automa-},
	url          = {https://ieeexplore.ieee.org/abstract/document/844354},
	abstract     = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans;2000;30;3;10.1109/3468.844354},
}
@article{Cramer2008,
	title        = {The effects of transparency on trust in and acceptance of a content-based art recommender},
	author       = {Cramer, Henriette and Evers, Vanessa and Ramlal, Satyan and Van Someren, Maarten and Rutledge, Lloyd and Stash, Natalia and Aroyo, Lora and Wielinga, Bob},
	year         = 2008,
	journal      = {User Modeling and User-adapted interaction},
	publisher    = {Springer},
	volume       = 18,
	number       = 5,
	pages        = {455--496},
	url          = {https://link.springer.com/article/10.1007/s11257-008-9051-3},
}
@inproceedings{Lim2009,
	title        = {Assessing Demand for Intelligibility in Context-Aware Applications},
	author       = {Lim, Brian Y. and Dey, Anind K.},
	year         = 2009,
	booktitle    = {Proceedings of the 11th International Conference on Ubiquitous Computing},
	location     = {Orlando, Florida, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {UbiComp '09},
	pages        = {195–204},
	doi          = {10.1145/1620545.1620576},
	isbn         = {9781605584317},
	url          = {https://doi.org/10.1145/1620545.1620576},
	abstract     = {Intelligibility can help expose the inner workings and inputs of context-aware applications that tend to be opaque to users due to their implicit sensing and actions. However, users may not be interested in all the information that the applications can produce. Using scenarios of four real-world applications that span the design space of context-aware computing, we conducted two experiments to discover what information users are interested in. In the first experiment, we elicit types of information demands that users have and under what moderating circumstances they have them. In the second experiment, we verify the findings by soliciting users about which types they would want to know and establish whether receiving such information would satisfy them. We discuss why users demand certain types of information, and provide design implications on how to provide different intelligibility types to make context-aware applications intelligible and acceptable to users.},
	keywords     = {context-aware, satisfaction, intelligibility, explanations},
	numpages     = 10
}
@inproceedings{Lim2009a,
	title        = {Why and Why Not Explanations Improve the Intelligibility of Context-Aware Intelligent Systems},
	author       = {Lim, Brian Y. and Dey, Anind K. and Avrahami, Daniel},
	year         = 2009,
	booktitle    = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	location     = {Boston, MA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '09},
	pages        = {2119–2128},
	doi          = {10.1145/1518701.1519023},
	isbn         = {9781605582467},
	url          = {https://doi.org/10.1145/1518701.1519023},
	abstract     = {Context-aware intelligent systems employ implicit inputs, and make decisions based on complex rules and machine learning models that are rarely clear to users. Such lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance of these systems. However, automatically providing explanations about a system's decision process can help mitigate this problem. In this paper we present results from a controlled study with over 200 participants in which the effectiveness of different types of explanations was examined. Participants were shown examples of a system's operation along with various automatically generated explanations, and then tested on their understanding of the system. We show, for example, that explanations describing why the system behaved a certain way resulted in better understanding and stronger feelings of trust. Explanations describing why the system did not behave a certain way, resulted in lower understanding yet adequate performance. We discuss implications for the use of our findings in real-world context-aware applications.},
	keywords     = {explanations, context-aware, intelligibility},
	numpages     = 10
}
@article{Parasuraman2010,
	title        = {Complacency and Bias in Human Use of Automation: An Attentional Integration},
	author       = {Raja Parasuraman and Dietrich H. Manzey},
	year         = 2010,
	month        = {jun},
	journal      = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	publisher    = {{SAGE} Publications},
	volume       = 52,
	number       = 3,
	pages        = {381--410},
	doi          = {10.1177/0018720810376055},
	abstract     = {Objective: Our aim was to review empirical studies of complacency and bias in human interaction with automated and decision support systems and provide an integrated theoretical model for their explanation. Background: Automation-related complacency and automation bias have typically been considered separately and independently. Methods: Studies on complacency and automation bias were analyzed with respect to the cognitive processes involved. Results: Automation complacency occurs under conditions of multiple-task load, when manual tasks compete with the automated task for the operator's attention. Automation complacency is found in both naive and expert participants and cannot be overcome with simple practice. Automation bias results in making both omission and commission errors when decision aids are imperfect. Automation bias occurs in both naive and expert participants, cannot be prevented by training or instructions, and can affect decision making in individuals as well as in teams. While automation bias has been conceived of as a special case of decision bias, our analysis suggests that it also depends on attentional processes similar to those involved in automation-related complacency. Conclusion: Complacency and automation bias represent different manifestations of overlapping automationinduced phenomena, with attention playing a central role. An integrated model of complacency and automation bias shows that they result from the dynamic interaction of personal, situational, and automation-related characteristics. Application: The integrated model and attentional synthesis provides a heuristic framework for further research on complacency and automation bias and design options for mitigating such effects in automated and decision support systems.},
	keywords     = {attention, automation-related complacency, automation bias, decision making, human-computer interaction, trust}
}
@inproceedings{Jenkins2010,
	title        = {Measuring Trust and Application of Eye Tracking in Human Robotic Interaction},
	author       = {Jenkins, Quaneisha and Jiang, Xiaochun},
	year         = 2010,
	booktitle    = {IIEAnnual Conference and Expo},
	abstract     = {The interaction between operator and robot in the context of Urban Search and Rescue (USAR) missions provides the opportunity to further study operator trust in human-robot interfaces. USAR missions are characterized by high uncertainty and risk, lending them to be situations where operator trust is critical to mission performance. Prior research using subjective questions to measure trust has indicated that there is a difference in operator trust measurements for different interfaces. This research presented here further analyzes trust in human-robot interaction (HRI) by examining trust and eye movements such as duration, sequence, and frequency of fixations for common mission tasks.},
	keywords     = {Trust, human-robot interaction, eye tracking}
}
@inproceedings{Onnasch2014,
	title        = {Operators׳ adaptation to imperfect automation – Impact of miss-prone alarm systems on attention allocation and performance},
	author       = {Onnasch, Linda and Ruff, Stefan and Manzey, Dietrich},
	year         = 2014,
	publisher    = {Technische Universität Berlin},
	doi          = {10.14279/DEPOSITONCE-10989},
	url          = {https://www.sciencedirect.com/science/article/abs/pii/S1071581914000688},
	copyright    = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	abstract     = {Operators in complex environments are often supported by alarm systems that indicate when to shift attention to certain tasks. As alarms are not perfectly reliable, operators have to select appropriate strategies of attention allocation to compensate for unreliability and to maintain overall performance. This study explores how humans adapt to differing alarm reliabilities. Within a multi-task simulation consisting of a monitoring task and two other concurrent tasks, participants were assigned to one of five groups. In the manual control group none of the tasks was supported by an alarm system, whereas the four experimental groups were supported in the monitoring task by a miss-prone alarm system differing in reliability, i.e. 68.75%, 75%, 87.5%, 93.75%. Compared to the manual control group, all experimental groups benefited from the support by alarms, with best performance for the highest reliability condition. However, for the lowest reliability group the benefit was associated with an increased attentional effort, a more demanding attention allocation strategy, and a declined relative performance in a concurrent task. Results are discussed in the context of recent automation research.},
	keywords     = {150 Psychologie, alarm systems, reliability, miss-prone automation, attention allocation, adaptive behaviour}
}
@inproceedings{Visser2018,
	title        = {From ‘automation’ to ‘autonomy’: the importance of trust repair in human–machine interaction},
	author       = {De Visser, Ewart J and Pak, Richard and Shaw, Tyler H},
	year         = 2018,
	doi          = {.org/10.1080/00140139.2018.1457725},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/00140139.2018.1457725},
	abstract     = {Ergonomics, 2018. doi:10.1080/00140139.2018.1457725},
	keywords     = {Trust repair; autonomy; automation; humanness; human–machine teaming}
}
@inbook{Zhou2018,
	title        = {Revealing User Confidence in Machine Learning-Based Decision Making},
	author       = {Zhou, Jianlong and Yu, Kun and Chen, Fang},
	year         = 2018,
	booktitle    = {Human and Machine Learning: Visible, Explainable, Trustworthy and Transparent},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {225--244},
	doi          = {10.1007/978-3-319-90403-0_11},
	isbn         = {978-3-319-90403-0},
	url          = {https://doi.org/10.1007/978-3-319-90403-0_11},
	editor       = {Zhou, Jianlong and Chen, Fang},
	abstract     = {This chapter demonstrates the link between human cognition states and Machine Learning (ML) with a multimodal interface. A framework of informed decision making called DecisionMind is proposed to show how human's behaviour and physiological signals are used to reveal human cognition states in ML-based decision making. The chapter takes the revealing of user confidence in ML-based decision making as an example to demonstrate the effectiveness of the proposed approach. Based on the revealing of human cognition states during ML-based decision making, the chapter presents a concept of adaptive measurable decision making to show how the revealing of human cognition states are integrated into ML-based decision making to make ML transparent. On the one hand, human cognition states could help understand to what degree humans accept innovative technologies. On the other hand, through understanding human cognition states during ML-based decision making, ML-based decision attributes/factors and even ML models can be adaptively refined in order to make ML transparent.},
}
@inproceedings{Eslami2018,
	title        = {Communicating Algorithmic Process in Online Behavioral Advertising},
	author       = {Eslami, Motahhare and Krishna Kumaran, Sneha R. and Sandvig, Christian and Karahalios, Karrie},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	location     = {Montreal QC, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '18},
	pages        = {1–13},
	doi          = {10.1145/3173574.3174006},
	isbn         = {9781450356206},
	url          = {https://doi.org/10.1145/3173574.3174006},
	abstract     = {Advertisers develop algorithms to select the most relevant advertisements for users. However, the opacity of these algorithms, along with their potential for violating user privacy, has decreased user trust and preference in behavioral advertising. To mitigate this, advertisers have started to communicate algorithmic processes in behavioral advertising. However, how revealing parts of the algorithmic process affects users' perceptions towards ads and platforms is still an open question. To investigate this, we exposed 32 users to why an ad is shown to them, what advertising algorithms infer about them, and how advertisers use this information. Users preferred interpretable, non-creepy explanations about why an ad is presented, along with a recognizable link to their identity. We further found that exposing users to their algorithmically-derived attributes led to algorithm disillusionment---users found that advertising algorithms they thought were perfect were far from it. We propose design implications to effectively communicate information about advertising algorithms.},
	keywords     = {process communication, advertising algorithms, ad explanation, algorithmic authority, algorithmic transparency},
	numpages     = 13
}
@inproceedings{Abdul2018,
	title        = {Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda},
	author       = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	location     = {Montreal QC, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '18},
	pages        = {1–18},
	doi          = {10.1145/3173574.3174156},
	isbn         = {9781450356206},
	url          = {https://doi.org/10.1145/3173574.3174156},
	abstract     = {Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorith-mic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research to-wards this goal.},
	keywords     = {intelligibility, explanations, interpretable machine learning, explainable artificial intelli-gence},
	numpages     = 18
}
@inproceedings{MingYin2019,
	title        = {Understanding the Effect of Accuracy on Trust in Machine Learning Models},
	author       = {Ming Yin, Jennifer Wortman Vaughan, Hanna Wallach},
	year         = 2019,
	doi          = {.org/10.1145/3290605.3300509},
	abstract     = {-  Human-centered computing  ->  -1Empirical studies in HCI.-  Computing methodologies  ->  -1Machine learning.},
	keywords     = {may misunderstand or mistrust its predictions [6, 16, 25]. Machine learning, trust, human-subject experiments Prompted by these challenges, as well as growing concerns}
}
@inproceedings{Ribera2019,
	title        = {Can we do better explanations? A proposal of user-centered explainable AI.},
	author       = {Ribera, Mireia and Lapedriza, Agata},
	year         = 2019,
	booktitle    = {IUI Workshops},
	volume       = 2327,
	pages        = 38,
}
@inproceedings{Rutjes2019,
	title        = {Considerations on explainable AI and users' mental models},
	author       = {Heleen Rutjes and Martijn Willemsen and Wijnand IJsselsteijn},
	year         = 2019,
	month        = may,
	day          = 4,
	booktitle    = {Where is the Human? Bridging the Gap Between AI and HCI},
	publisher    = {Association for Computing Machinery, Inc},
	address      = {United States},
	note         = {CHI 2019 Workshop : Where is the Human? Bridging the Gap Between AI and HCI ; Conference date: 04-05-2019 Through 04-05-2019},
	abstract     = {As the aim of explaining is understanding, XAI is successful when the user has a good understanding of the AI system. This paper shows, using theories from the social sciences and HCI, that appropriately capturing and accounting for the user{\textquoteright}s mental model while explaining is key to successful XAI.},
	language     = {English}
}
@inproceedings{branley2020user,
	title        = {User trust and understanding of explainable AI: exploring algorithm visualisations and user biases},
	author       = {Branley-Bell, Dawn and Whitworth, Rebecca and Coventry, Lynne},
	year         = 2020,
	booktitle    = {International Conference on Human-Computer Interaction},
	pages        = {382--399},
	url          = {https://link.springer.com/chapter/10.1007/978-3-030-49065-2_27},
	organization = {Springer},
}
@inproceedings{Brennen2020,
	title        = {What Do People Really Want When They Say They Want "Explainable AI?" We Asked 60 Stakeholders.},
	author       = {Brennen, Andrea},
	year         = 2020,
	booktitle    = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
	location     = {Honolulu, HI, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI EA '20},
	pages        = {1–7},
	doi          = {10.1145/3334480.3383047},
	isbn         = {9781450368193},
	url          = {https://doi.org/10.1145/3334480.3383047},
	abstract     = {This paper summarizes findings from a qualitative research effort aimed at understanding how various stakeholders characterize the problem of Explainable Artificial Intelligence (Explainable AI or XAI). During a nine-month period, the author conducted 40 interviews and 2 focus groups. An analysis of data gathered led to two significant initial findings: (1) current discourse on Explainable AI is hindered by a lack of consistent terminology; and (2) there are multiple distinct use cases for Explainable AI, including: debugging models, understanding bias, and building trust. These uses cases assume different user personas, will likely require different explanation strategies, and are not evenly addressed by current XAI tools. This stakeholder research supports a broad characterization of the problem of Explainable AI and can provide important context to inform the design of future capabilities.},
	eprint       = {https://dl.acm.org/doi/abs/10.1145/3334480.3383047},
	keywords     = {data science, machine learning, user research, UI/UX design, interface design, explainable AI},
	numpages     = 7
}
@inproceedings{Ehsan2020,
	title        = {Human-centered explainable ai: Towards a reflective sociotechnical approach},
	author       = {Ehsan, Upol and Riedl, Mark O},
	year         = 2020,
	booktitle    = {International Conference on Human-Computer Interaction},
	pages        = {449--466},
	organization = {Springer},
}
@inproceedings{Ferreira2020,
	title        = {What are people doing about XAI user experience? A survey on AI explainability research and practice},
	author       = {Ferreira, Juliana J and Monteiro, Mateus S},
	year         = 2020,
	booktitle    = {International Conference on Human-Computer Interaction},
	pages        = {56--73},
	url          = {https://link.springer.com/chapter/10.1007/978-3-030-49760-6_4},
	organization = {Springer},
}
@article{Dellermann2021,
	title        = {The future of human-AI collaboration: a taxonomy of design knowledge for hybrid intelligence systems},
	author       = {Dellermann, Dominik and Calma, Adrian and Lipusch, Nikolaus and Weber, Thorsten and Weigel, Sascha and Ebel, Philipp},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2105.03354},
	url          = {https://arxiv.org/abs/2105.03354},
	abstract     = {Recent technological advances, especially in the field of machine learning, provide astonishing progress on the road towards artificial general intelligence. However, tasks in current real-world business applications cannot yet be solved by machines alone. We, therefore, identify the need for developing socio-technological ensembles of humans and machines. Such systems possess the ability to accomplish complex goals by combining human and artificial intelligence to collectively achieve superior results and continuously improve by learning from each other. Thus, the need for structured design knowledge for those systems arises. Following a taxonomy development method, this article provides three main contributions: First, we present a structured overview of interdisciplinary research on the role of humans in the machine learning pipeline. Second, we envision hybrid intelligence systems and conceptualize the relevant dimensions for system design for the first time. Finally, we offer useful guidance for system developers during the implementation of such applications.},
	keywords     = {The Future of Human-AI Collaboration: A Taxonomy of Design Knowledge for Hybrid Intelligence Systems.}
}
@article{Shin2021,
	title        = {The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI},
	author       = {Donghee Shin},
	year         = 2021,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 146,
	pages        = 102551,
	doi          = {https://doi.org/10.1016/j.ijhcs.2020.102551},
	issn         = {1071-5819},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581920301531},
	abstract     = {Artificial intelligence and algorithmic decision-making processes are increasingly criticized for their black-box nature. Explainable AI approaches to trace human-interpretable decision processes from algorithms have been explored. Yet, little is known about algorithmic explainability from a human factors’ perspective. From the perspective of user interpretability and understandability, this study examines the effect of explainability in AI on user trust and attitudes toward AI. It conceptualizes causability as an antecedent of explainability and as a key cue of an algorithm and examines them in relation to trust by testing how they affect user perceived performance of AI-driven services. The results show the dual roles of causability and explainability in terms of its underlying links to trust and subsequent user behaviors. Explanations of why certain news articles are recommended generate users trust whereas causability of to what extent they can understand the explanations affords users emotional confidence. Causability lends the justification for what and how should be explained as it determines the relative importance of the properties of explainability. The results have implications for the inclusion of causability and explanatory cues in AI systems, which help to increase trust and help users to assess the quality of explanations. Causable explainable AI will help people understand the decision-making process of AI algorithms by bringing transparency and accountability into AI systems.},
	keywords     = {Explainable Ai, Causability, Human-ai interaction, Explanatorycues, Interpretability, Understandability, Trust, Glassbox, Human-centeredAI}
}
@inproceedings{Ferreira2021,
	title        = {The human-AI relationship in decision-making: AI explanation to support people on justifying their decisions},
	author       = {Ferreira, Juliana and Monteiro, Mateus},
	year         = 2021,
	abstract     = {The explanation dimension of Artificial Intelligence (AI) based system has been a hot topic for the past years. Different communities have raised concerns about the increasing presence of AI in people's everyday tasks and how it can affect people's lives. There is a lot of research addressing the interpretability and transparency concepts of explainable AI (XAI), which are usually related to algorithms and Machine Learning (ML) models. But in decision-making scenarios, people need more awareness of how AI works and its outcomes to build a relationship with that system. Decision-makers usually need to justify their decision to others in different domains. If that decision is somehow based on or influenced by an AI-system outcome, the explanation about how the AI reached that result is key to building trust between AI and humans in decision-making scenarios. In this position paper, we discuss the role of XAI in decision-making scenarios, our vision of Decision-Making with AI-system in the loop, and explore one case from the literature about how XAI can impact people justifying their decisions, considering the importance of building the human-AI relationship for those scenarios. Human-centered computing • Human-computer interaction (HCI)• HCI theory, concepts and models},
	keywords     = {eXplainable AI, AI design, decision-making, high-stakes decision-making, human-AI relationship}
}
@inproceedings{Wang2021,
	title        = {Are Explanations Helpful? A Comparative Study of the Effects of Explanations in {AI}-Assisted Decision-Making},
	author       = {Xinru Wang and Ming Yin},
	year         = 2021,
	month        = {apr},
	day          = 16,
	booktitle    = {26th International Conference on Intelligent User Interfaces},
	publisher    = {{ACM}},
	doi          = {10.1145/3397481.3450650},
	abstract     = {This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy-improve people's understanding of the AI model, help people recognize the model uncertainty, and support people's calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making. CCS CONCEPTS • Human-centered computing → Empirical studies in HCI; • Computing methodologies → Machine learning.},
	keywords     = {interpretable machine learning, explainable AI, trust, trust calibration, human-subject experiments},
	language     = {en},
	source       = {iui21-16.tex}
}
@article{Ehsan2021,
	title        = {The who in explainable ai: How ai background shapes perceptions of ai explanations},
	author       = {Ehsan, Upol and Passi, Samir and Liao, Q Vera and Chan, Larry and Lee, I and Muller, Michael and Riedl, Mark O and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2107.13509},
}
@inproceedings{Ehsan2021a,
	title        = {Expanding Explainability: Towards Social Transparency in AI Systems},
	author       = {Ehsan, Upol and Liao, Q. Vera and Muller, Michael and Riedl, Mark O. and Weisz, Justin D.},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	location     = {Yokohama, Japan},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '21},
	doi          = {10.1145/3411764.3445188},
	isbn         = {9781450380966},
	url          = {https://doi.org/10.1145/3411764.3445188},
	abstract     = {As AI-powered systems increasingly mediate consequential decision-making, their explainability is critical for end-users to take informed and accountable actions. Explanations in human-human interactions are socially-situated. AI systems are often socio-organizationally embedded. However, Explainable AI (XAI) approaches have been predominantly algorithm-centered. We take a developmental step towards socially-situated XAI by introducing and exploring Social Transparency (ST), a sociotechnically informed perspective that incorporates the socio-organizational context into explaining AI-mediated decision-making. To explore ST conceptually, we conducted interviews with 29 AI users and practitioners grounded in a speculative design scenario. We suggested constitutive design elements of ST and developed a conceptual framework to unpack ST’s effect and implications at the technical, decision-making, and organizational level. The framework showcases how ST can potentially calibrate trust in AI, improve decision-making, facilitate organizational collective actions, and cultivate holistic explainability. Our work contributes to the discourse of Human-Centered XAI by expanding the design space of XAI.},
	articleno    = 82,
	keywords     = {explanations, Explainable AI, socio-organizational context, social transparency, Artificial Intelligence, human-AI interaction, sociotechnical},
	numpages     = 19
}
@inproceedings{Dhanorkar2021,
	title        = {Who Needs to Know What, When?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle},
	author       = {Dhanorkar, Shipi and Wolf, Christine T. and Qian, Kun and Xu, Anbang and Popa, Lucian and Li, Yunyao},
	year         = 2021,
	booktitle    = {Designing Interactive Systems Conference 2021},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {DIS '21},
	pages        = {1591–1602},
	doi          = {10.1145/3461778.3462131},
	isbn         = {9781450384766},
	url          = {https://doi.org/10.1145/3461778.3462131},
	abstract     = {The interpretability or explainability of AI systems (XAI) has been a topic gaining renewed attention in recent years across AI and HCI communities. Recent work has drawn attention to the emergent explainability requirements of in situ, applied projects, yet further exploratory work is needed to more fully understand this space. This paper investigates applied AI projects and reports on a qualitative interview study of individuals working on AI projects at a large technology and consulting company. Presenting an empirical understanding of the range of stakeholders in industrial AI projects, this paper also draws out the emergent explainability practices that arise as these projects unfold, highlighting the range of explanation audiences (who), as well as how their explainability needs evolve across the AI project lifecycle (when). We discuss the importance of adopting a sociotechnical lens in designing AI systems, noting how the “AI lifecycle” can serve as a design metaphor to further the XAI design field.},
	keywords     = {Explainable AI, Interviews, Work Practices},
	numpages     = 12
}
@inproceedings{Chromik2021,
	title        = {I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI},
	author       = {Chromik, Michael and Eiband, Malin and Buchner, Felicitas and Kr\"{u}ger, Adrian and Butz, Andreas},
	year         = 2021,
	booktitle    = {26th International Conference on Intelligent User Interfaces},
	location     = {College Station, TX, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {IUI '21},
	pages        = {307–317},
	doi          = {10.1145/3397481.3450644},
	isbn         = {9781450380171},
	url          = {https://doi.org/10.1145/3397481.3450644},
	abstract     = {Unintended consequences of deployed AI systems fueled the call for more interpretability in AI systems. Often explainable AI (XAI) systems provide users with simplifying local explanations for individual predictions but leave it up to them to construct a global understanding of the model behavior. In this work, we examine if non-technical users of XAI fall for an illusion of explanatory depth when interpreting additive local explanations. We applied a mixed methods approach consisting of a moderated study with 40 participants and an unmoderated study with 107 crowd workers using a spreadsheet-like explanation interface based on the SHAP framework. We observed what non-technical users do to form their mental models of global AI model behavior from local explanations and how their perception of understanding decreases when it is examined.},
	keywords     = {explainable AI, understanding, Shapley explanation, cognitive bias},
	numpages     = 11
}
@inproceedings{Ehsan2021b,
	title        = {Operationalizing Human-Centered Perspectives in Explainable AI},
	author       = {Ehsan, Upol and Wintersberger, Philipp and Liao, Q. Vera and Mara, Martina and Streit, Marc and Wachter, Sandra and Riener, Andreas and Riedl, Mark O.},
	year         = 2021,
	booktitle    = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
	location     = {Yokohama, Japan},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI EA '21},
	doi          = {10.1145/3411763.3441342},
	isbn         = {9781450380959},
	url          = {https://doi.org/10.1145/3411763.3441342},
	abstract     = {The realm of Artificial Intelligence (AI)’s impact on our lives is far reaching – with AI systems proliferating high-stakes domains such as healthcare, finance, mobility, law, etc., these systems must be able to explain their decision to diverse end-users comprehensibly. Yet the discourse of Explainable AI (XAI) has been predominantly focused on algorithm-centered approaches, suffering from gaps in meeting user needs and exacerbating issues of algorithmic opacity. To address these issues, researchers have called for human-centered approaches to XAI. There is a need to chart the domain and shape the discourse of XAI with reflective discussions from diverse stakeholders. The goal of this workshop is to examine how human-centered perspectives in XAI can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we put an emphasis on “operationalizing”, aiming to produce actionable frameworks, transferable evaluation methods, concrete design guidelines, and articulate a coordinated research agenda for XAI.},
	articleno    = 94,
	keywords     = {Interpretability, Algorithmic Fairness, Trust in Automation, Explainable Artificial Intelligence, Human-centered Computing, Interpretable Machine Learning, Artificial Intelligence, Critical Technical Practice},
	numpages     = 6
}
@article{langer2021,
	title        = {What do we want from Explainable Artificial Intelligence (XAI)?--A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research},
	author       = {Langer, Markus and Oster, Daniel and Speith, Timo and Hermanns, Holger and K{\"a}stner, Lena and Schmidt, Eva and Sesing, Andreas and Baum, Kevin},
	year         = 2021,
	journal      = {Artificial Intelligence},
	publisher    = {Elsevier},
	volume       = 296,
	pages        = 103473
}
@article{Linder2021,
	title        = {How level of explanation detail affects human performance in interpretable intelligent systems: A study on explainable fact checking},
	author       = {Linder, Rhema and Mohseni, Sina and Yang, Fan and Pentyala, Shiva K. and Ragan, Eric D. and Hu, Xia Ben},
	year         = 2021,
	journal      = {Applied AI Letters},
	volume       = 2,
	number       = 4,
	pages        = {e49},
	doi          = {https://doi.org/10.1002/ail2.49},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.49},
	abstract     = {Abstract Explainable artificial intelligence (XAI) systems aim to provide users with information to help them better understand computational models and reason about why outputs were generated. However, there are many different ways an XAI interface might present explanations, which makes designing an appropriate and effective interface an important and challenging task. Our work investigates how different types and amounts of explanatory information affect user ability to utilize explanations to understand system behavior and improve task performance. The presented research employs a system for detecting the truthfulness of news statements. In a controlled experiment, participants were tasked with using the system to assess news statements as well as to learn to predict the output of the AI. Our experiment compares various levels of explanatory information to contribute empirical data about how explanation detail can influence utility. The results show that more explanation information improves participant understanding of AI models, but the benefits come at the cost of time and attention needed to make sense of the explanation.},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ail2.49} ,
	keywords     = {explainable artificial intelligence, human-computer interaction, machine learning, transparency}
}
@inproceedings{Leffrang2021,
	title        = {Should I Follow this Model? The Effect of Uncertainty Visualization on the Acceptance of Time Series Forecasts},
	author       = {Leffrang, Dirk and Müller, Oliver},
	year         = 2021,
	booktitle    = {2021 IEEE Workshop on TRust and EXpertise in Visual Analytics (TREX)},
	pages        = {20--26},
	doi          = {10.1109/TREX53765.2021.00009},
}
@article{Dikmen2022,
	title        = {The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending},
	author       = {Murat Dikmen and Catherine Burns},
	year         = 2022,
	journal      = {International Journal of Human-Computer Studies},
	volume       = 162,
	pages        = 102792,
	doi          = {https://doi.org/10.1016/j         .ijhcs.2022.102792},
	issn         = {1071-5819},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581922000210},
	abstract     = {Increasingly, artificial intelligence (AI) is being used to assist complex decision-making such as financial investing. However, there are concerns regarding the black-box nature of AI algorithms. The field of explainable AI (XAI) has emerged to address these concerns. XAI techniques can reveal how an AI decision is formed and can be used to understand and appropriately trust an AI system. However, XAI techniques still may not be human-centred and may not support human decision-making adequately. In this work, we explored how domain knowledge, identified by expert decision makers, can be used to achieve a more human-centred approach to AI. We measured the effect of domain knowledge on trust in AI, reliance on AI, and task performance in an AI-assisted complex decision-making environment. In a peer-to-peer lending simulator, non-expert participants made financial investments using an AI assistant. The presence or absence of domain knowledge was manipulated. The results showed that participants who had access to domain knowledge relied less on the AI assistant when the AI assistant was incorrect and indicated less trust in AI assistant. However, overall investing performance was not affected. These results suggest that providing domain knowledge can influence how non-expert users use AI and could be a powerful tool to help these users develop appropriate levels of trust and reliance.},
	keywords     = {Explainable AI, Human-AI interaction, Domain knowledge, Trust in AI}
}

@article{Papenmeier2022,
	title        = {It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in {AI}},
	author       = {Andrea Papenmeier and Dagmar Kern and Gwenn Englebienne and Christin Seifert},
	year         = 2022,
	month        = {aug},
	journal      = {{ACM} Transactions on Computer-Human Interaction},
	publisher    = {Association for Computing Machinery ({ACM})},
	volume       = 29,
	number       = 4,
	pages        = {1--33},
	doi          = {10.1145/3495013},
	abstract     = {Automated decision-making systems become increasingly powerful due to higher model complexity. While powerful in prediction accuracy, Deep Learning models are black boxes by nature, preventing users from making informed judgments about the correctness and fairness of such an automated system. Explanations have been proposed as a general remedy to the black box problem. However, it remains unclear if effects of explanations on user trust generalise over varying accuracy levels. In an online user study with 959 participants, we examined the practical consequences of adding explanations for user trust: We evaluated trust for three explanation types on three classifiers of varying accuracy. We find that the influence of our explanations on trust differs depending on the classifier's accuracy. Thus, the interplay between trust and explanations is more complex than previously reported. Our findings also reveal discrepancies between self-reported and behavioural trust, showing that the choice of trust measure impacts the results. CCS Concepts: • Human-centered computing → Empirical studies in HCI; Human computer interaction (HCI);},
	keywords     = {Explainable AI, machine learning, minimum explanations, user trust, explanation fidelity This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike International 4.0 License}
}
@article{Riefle2022,
	title        = {On the Influence of Cognitive Styles on Users' Understanding of Explanations},
	author       = {Lara Riefle and Patrick Hemmer and Carina Benz and Michael Vossing and Jannik Pries},
	year         = 2022,
	journal      = {ArXiv},
	volume       = {abs/2210.02123},
}
@article{Kim2022a,
	title        = {"Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction},
	author       = {Sunnie Kim and Elizabeth Anne Watkins and Olga Russakovsky and Ruth C. Fong and A. Monroy-Hern{\'a}ndez},
	year         = 2022,
	journal      = {ArXiv},
	volume       = {abs/2210.03735},
}
@article{Chien2015,
	title        = {Cross-Country Validation of a Cultural Scale in Measuring Trust in Automation},
	author       = {Shih-Yi Chien and Michael Lewis and Sebastian Hergeth and Zhaleh Semnani-Azad and Katia Sycara},
	year         = 2015,
	month        = {sep},
	journal      = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	publisher    = {{SAGE} Publications},
	volume       = 59,
	number       = 1,
	pages        = {686--690},
	doi          = {10.1177/1541931215591149},
	abstract     = {Human automation interaction is a complex process. How autonomous assistance impacts trust in automation as well as how trust affects human calibration and use of automation has been investigated for both dynamic contexts, including the internal variables (e.g., cultural characteristics) and external factors (e.g., system settings). Having standardized measures to capture trust and its antecedents is particularly critical to understanding how factors associated with the human operators and autonomous applications affects the way they are used. This paper reports the development of a trust instrument and several rounds of crosscountry validation, including U.S., German, Taiwanese, and Turkish populations. The results confirm that the instrument which was developed reliably measured human trust in automation across cultures.},
}
@inproceedings{Bayer2021,
	title        = {The role of domain expertise in trusting and following explainable AI decision support systems},
	author       = {Sarah Bayer, Henner Gimpel, Moritz Markgraf},
	year         = 2021,
	doi          = {.org/10.1080/12460125.2021.1958505},
	abstract     = {Journal of Decision Systems, 2021. doi:10.1080/12460125.2021.1958505},
	keywords     = {Explainable artificial intelligence; trust; decision support systems; user expertise; online experiment}
}
@inproceedings{Kizilcec2016,
	title        = {How Much Information? Effects of Transparency on Trust in an Algorithmic Interface},
	author       = {Ren{\'{e}} F. Kizilcec},
	year         = 2016,
	month        = {may},
	booktitle    = {Proceedings of the 2016 {CHI} Conference on Human Factors in Computing Systems},
	publisher    = {{ACM}},
	doi          = {10.1145/2858036.2858402},
	abstract     = {The rising prevalence of algorithmic interfaces, such as curated feeds in online news, raises new questions for designers, scholars, and critics of media. This work focuses on how transparent design of algorithmic interfaces can promote awareness and foster trust. A two-stage process of how transparency affects trust was hypothesized drawing on theories of information processing and procedural justice. In an online field experiment, three levels of system transparency were tested in the high-stakes context of peer assessment. Individuals whose expectations were violated (by receiving a lower grade than expected) trusted the system less, unless the grading algorithm was made more transparent through explanation. However, providing too much information eroded this trust. Attitudes of individuals whose expectations were met did not vary with transparency. Results are discussed in terms of a dual process model of attitude change and the depth of justification of perceived inconsistency. Designing for trust requires balanced interface transparency-not too little and not too much.},
	keywords     = {H.5.2. Information Interfaces and Presentation (e.g. HCI): User Interfaces, K.3.1. Computers and Education: Computer Uses in Education Interface Design, Algorithm Awareness, Attitude Change, Transparency, Trust, Peer Assessment}
}
@inproceedings{Kulms2019,
	title        = {More Human-Likeness, More Trust? The Effect of Anthropomorphism on Self-Reported and Behavioral Trust in Continued and Interdependent Human-Agent Cooperation},
	author       = {Kulms, Philipp and Kopp, Stefan},
	year         = 2019,
	booktitle    = {Proceedings of Mensch Und Computer 2019},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	pages        = {31–42},
	doi          = {10.1145/3340764.3340793},
	isbn         = {9781450371988},
	url          = {https://doi.org/10.1145/3340764.3340793},
	abstract     = {Computer agents are increasingly endowed with anthropomorphic characteristics and autonomous behavior to improve their capabilities for problem-solving and make interactions with humans more natural. This poses new challenges for human users who need to make trust-based decisions in dynamic and complex environments. It remains unclear if people trust agents like other humans and thus apply the same social rules to human--computer interaction (HCI), or rather, if interactions with computers are characterized by idiosyncratic attributions and responses. To this ongoing and crucial debate we contribute an experiment on the impact of anthropomorphic cues on trust and trust-related attributions in a cooperative human--agent setting, permitting the investigation of interdependent, continued, and coordinated decision-making toward a joint goal. Our results reveal an incongruence between self-reported and behavioral trust measures. First, the varying degree of agent anthropomorphism (computer vs. virtual vs. human agent) did not affect people's decision to behaviorally trust the agent by adopting task-specific advice. Behavioral trust was affected by advice quality only. Second, subjective ratings indicate that anthropomorphism did increase self-reported trust.},
	keywords     = {anthropomorphism, Trust, virtual agents, human--agent cooperation, human–agent cooperation},
	numpages     = 12
}
@article{lopes2022xai,
	title        = {XAI Systems Evaluation: A Review of Human and Computer-Centred Methods},
	author       = {Lopes, Pedro and Silva, Eduardo and Braga, Cristiana and Oliveira, Tiago and Rosado, Lu{\'\i}s},
	year         = 2022,
	journal      = {Applied Sciences},
	publisher    = {MDPI},
	volume       = 12,
	number       = 19,
	pages        = 9423
}
@article{Mohseni2021,
	title        = {A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems},
	author       = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	year         = 2021,
	month        = {sep},
	journal      = {ACM Trans. Interact. Intell. Syst.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 11,
	number       = {3–4},
	doi          = {10.1145/3387166},
	issn         = {2160-6455},
	abstract     = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence (AI) applications used in everyday life. Explainable AI (XAI) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	articleno    = 24,
	issue_date   = {December 2021},
	keywords     = {transparency, machine learning, explanation, Explainable artificial intelligence (XAI), human-computer interaction (HCI)},
	numpages     = 45
}
@article{Ji2023,
	title        = {Survey of Hallucination in Natural Language Generation},
	author       = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	year         = 2023,
	month        = {mar},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 55,
	number       = 12,
	doi          = {10.1145/3571730},
	issn         = {0360-0300},
	articleno    = 248,
	issue_date   = {December 2023},
	keywords     = {consistency in NLG, faithfulness in NLG, factuality in NLG, intrinsic hallucination, extrinsic hallucination, Hallucination},
	numpages     = 38
}
@article{McBride2010,
	title        = {Trust calibration for automated decision aids},
	author       = {McBride, Maranda and Morgan, Shona},
	year         = 2010,
	journal      = {Institute for Homeland Security Solutions},
	pages        = {1--11},
}
@article{McGuirl2006,
	title        = {Supporting trust calibration and the effective use of decision aids by presenting dynamic system confidence information},
	author       = {McGuirl, John M and Sarter, Nadine B},
	year         = 2006,
	journal      = {Human factors},
	publisher    = {SAGE Publications Sage CA: Los Angeles, CA},
	volume       = 48,
	number       = 4,
	pages        = {656--665},
}
@article{Muir1996,
	title        = {Trust in automation. Part II. Experimental studies of trust and human intervention in a process control simulation},
	author       = {Muir, Bonnie M and Moray, Neville},
	year         = 1996,
	journal      = {Ergonomics},
	publisher    = {Taylor \& Francis},
	volume       = 39,
	number       = 3,
	pages        = {429--460},
}
