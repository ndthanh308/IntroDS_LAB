% LaTeX template for MLSP papers. To be used with:
%   * mlspconf.sty - ICASSP/ICIP LaTeX style file adapted for MLSP, and
%   * IEEEbib.bst - IEEE bibliography style file.
\documentclass{article}
\usepackage{amsmath, bm, graphicx, mlspconf, glossaries}
\usepackage{blindtext}
\setlength\parindent{0pt}
\usepackage{hyperref}
\usepackage{afterpage}
\usepackage{multicol}
\usepackage{float}
\usepackage{epstopdf}
\usepackage{xcolor}


\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{\z@}%
  {-2.0ex \@plus -0.5ex \@minus -.2ex}%
  {1.50ex \@plus.2ex \@minus-.2ex}%
  {\normalfont\large\bfseries\centering}}
  
\renewcommand{\subsection}{\@startsection{subsection}{2}{\z@}%
  {-1.50ex\@plus -1ex \@minus -.2ex}%
  {0.50ex \@plus .2ex}%
  {\normalfont\normalsize\bfseries}}
\makeatother

% Copyright notices.
% ------------------

% * For papers in which all authors are employed by the European Union:
% \copyrightnotice{979-8-3503-2411-2/23/\$31.00 {\copyright}2023 European Union}

% * For all other papers:
\copyrightnotice{979-8-3503-2411-2/23/\$31.00 {\copyright}2023 IEEE}

% Header
\toappear{2023 IEEE International Workshop on Machine Learning for Signal Processing, Sept.\ 17--20, 2023, Rome, Italy}

% Title.
% ------
\title{Concept-based explainability for an EEG transformer model}

% Double-blind peer review.
% -------------------------
% Anonymize your paper for the double-blind peer-review process using the following author and affiliation.


% Single address.
% ---------------
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
%\address{Author Affiliation(s)}

% For example:
% ------------
%\address{%
%    School \\
%    Department \\
%    Address
%}
%
% Two addresses.
% --------------
%\twoauthors{%
%    A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}
%}{%
%    School A-B \\
%    Department A-B \\
%    Address A-B \\
%    Email A-B
%}{%
%   C. Author-three, D. Author-four\sthanks{The fourth author performed the work while at ...}
%}{%
%    School C-D \\
%    Department C-D \\
%    Address C-D \\
%    Email C-D
%}
% 
% Two or more addresses (alternative form).
% -----------------------------------------
% If you need to list more than 2 authors or the option for two options above 
% produces a poor author block, please use the following structure:

%\name{Anonymous\thanks{Anonymous.}}
%\address{Anonymous}

\name{%
    \fontsize{10.5pt}{11pt}\selectfont
    \begin{tabular}{c}
    Anders Gjølbye Madsen$^{\star\dagger}$ \qquad William Theodor Lehn-Schiøler$^{\star\dagger}$ \\
    Áshildur Jónsdóttir$^{\star}$ \qquad Bergdís Arnardóttir$^{\star}$ \qquad Lars Kai Hansen$^{\star}$
    \end{tabular}
    \thanks{This work is supported by The Pioneer Centre for AI, DNRF grant number P1, The Novo Nordisk Foundation grant NNF22OC0076907 "Cognitive spaces - Next generation explainability", and travel grants from The Danish Data Science Academy awarded to AGM and WLS.}
    \vspace{-3pt}
}
\address{%
    \small
    \begin{tabular}{c}
    \begin{tabular}{@{}c@{}}
    $^{\star}$Technical University of Denmark \\
    Department of Applied Mathematics and Computer Science \\
    2800 Kgs. Lyngby, Denmark
    \end{tabular}
    \qquad \qquad
    \begin{tabular}{@{}c@{}}
    $^{\dagger}$BrainCapture \\
    2800 Kgs. Lyngby, Denmark
    \end{tabular}
    \end{tabular}
    \vspace{-11pt}
}

\begin{document}
\ninept

\maketitle

\begin{abstract}
Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing.
In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavour involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation: the use of externally labelled EEG datasets, and the application of anatomically defined concepts. The former approach is a straightforward generalization of methods used in image classification, while the latter is novel and specific to EEG.
We present evidence that both approaches to concept formation yield valuable insights into the representations learned by deep EEG models.
\end{abstract}

\begin{keywords}
Explainable AI, EEG Concepts, TCAV, BENDR
\end{keywords}

\input{Sections/1_Introduction}
\input{Sections/2_Theory}
\input{Sections/3_Methodology}
\input{Sections/4_Results}
\input{Sections/5_Conclusion}


\bibliographystyle{IEEEbib}
\bibliography{main}

%\input{template}

\end{document}
