\section{Methods} \label{sec:methods}
\subsection{Data} \label{ssec:data}

EEG is a non-invasive technique to record the brain's electrical activity. EEG data in this paper refers to these measurements, used often in research and healthcare to identify neurological conditions. In this work, we use five publicly accessible datasets, namely TUH EEG Corpus \cite{tuheeg}, TUH EEG Artifact (TUAR) Corpus, TUH EEG Events (TUEV) Corpus, TUH EEG Seizure (TUSZ) Corpus \cite{tuhz} and the EEG Motor Movement/Imagery (MMIDB) Dataset \cite{mmidb}.

\quad The TUH EEG Corpus contains 69,652 clinical and unlabeled EEG recordings obtained from Temple University Hospital (TUH). The TUH EEG Artifact Corpus, a labeled subset of the TUH EEG Corpus, includes annotations for five distinct artifacts including eye movement artifact (\emph{eyem}). The TUEV is a subset of the TUH EEG Corpus and includes annotations of event-based EEG segments. There are numerous categories, but we primarily focus on five key classes: (1) technical artifacts (\textit{artf}), (2) background (\textit{bckg}), (3) generalized periodic epileptiform discharge (\textit{gped}), (4) periodic lateralized epileptiform discharge (\textit{pled}), and (5) spike and slow wave (\textit{spsw}). The TUSZ contains EEG signals with manually annotated data for seizure events.

\quad The MMIDB EEG dataset consists of data from 109 participants who are performing or imagining specific motor tasks; our main interest is the moments when subjects either close or imagine closing their left or right fist following a visual cue. We are excluding participants S088, S090, S092, and S100 due to missing data, resulting in 105 participants.

%\subsubsection{Resting-State Dataset} \label{sssec:resting}
\quad In the construction of brain anatomy concepts, it is imperative to obtain an extensive collection of resting-state EEG data. Due to the limited availability of public datasets with the requisite size and reliability, we utilized The TUH EEG Corpus and source localization to develop a dedicated anatomically labeled resting-state dataset. A set of predefined criteria were employed, including the number of EEG channels, minimum duration, minimum sampling frequency, scaling, and the exclusion of extreme values, which led to the elimination of approximately 90\% of the initial EEG recordings. Following this, a manual examination of a part of the remaining data was performed, ultimately yielding 200 human-verified resting-state EEG recordings, corresponding to an aggregate of about 70 hours of EEG data.

%\subsection{Preprocessing of data}
\quad In the process of downstream fine-tuning and concept formation, we employ 19 EEG channels, namely $\textit{Fp1}$, $\textit{Fp2}$, $\textit{F7}$, $\textit{F3}$, $\textit{Fz}$, $\textit{F4}$, $\textit{F8}$, $\textit{T7}$, $\textit{C3}$, $\textit{Cz}$, $\textit{C4}$, $\textit{T8}$, $\textit{T5}$, $\textit{P3}$, $\textit{Pz}$, $\textit{P4}$, $\textit{T6}$, $\textit{O1}$, and $\textit{O2}$ (see the MNE documentation \cite{doi:10.1098/rsta.2011.0081} for more information). These channels originate from the initial pre-training of BENDR using The TUH EEG Corpus. In instances where the datasets lack these channels, we establish the following mapping: $T3 \mapsto T7$, $T4 \mapsto T8$, $P7 \mapsto T5$, and $P8 \mapsto T6$. We also resample the corresponding EEG data to a 256 Hz sampling frequency and apply a high-pass FIRWIN filter with a 0.1 Hz cutoff, a low-pass FIRWIN filter with a 100.0 Hz cutoff, and a 60 Hz FIRWIN notch filter to eliminate powerline noise. In situations where preprocessing cannot be performed, the EEG is excluded. Finally, we scale each trial to the range $[-1, 1]$ and append a relative amplitude channel, see \cite{BENDR}, resulting in a total of 20 channels.

%\subsection{Training of BENDR}

\subsection{Training}
Pre-training of BENDR is based on the large set of unlabelled EEG data from The TUH EEG Corpus. The pre-training procedure is largely based on \verb|wav2vec 2.0| and involves two main stages: The convolutional stage and the transformer stage. The convolutional stage generates a sequence of representations (BENDRs) that summarize the original input. This sequence is then fed into the transformer stage, which adjusts its output to be most similar to the encoded representation at each position. The layers affected during pre-training are the feature encoder and the transformer. Kostas et al.\ \cite{BENDR} kindly made the pre-trained weights of the encoder and contextualizer publicly available, and this is the model that we have employed here.
%\footnote{Pre-trained model weights:\\ \url{https://github.com/SPOClab-ca/BENDR/releases/tag/v0.1-alpha}}.

%\subsubsection{Downstream fine-tuning}
\quad The LHB model architecture described in Figure \ref{fig:linear_head_bendr} is used for downstream fine-tuning. We aim to optimize the model for two distinct binary classification objectives. First, the model is fine-tuned for the differentiation between \emph{seizure} and \emph{non-seizure} events, using the TUSZ Corpus with 60-second window segments. The hyperparameters are determined using Bayesian optimization to maximize the validation $F_1$-score. The fine-tuning employs a batch size of 80, a learning rate of $1 \times 10^{-4}$, and $30$ epochs. This results in a model with a balanced accuracy of $0.73 \pm 0.07$. 

\quad In our second fine-tuning example, the model is adapted for the differentiation between \emph{Left Fist Movement} versus \emph{Right Fist Movement}, using the MMIDB EEG Dataset with 4-second window segments. We are using both the imaginary and performed task data from the 105 participants. We train the model for $7$ epochs with a batch size of $4$ and a learning rate of $1 \times 10^{-5}$. The hyperparameters were chosen based on the best validation balanced accuracy from leave-one-subject-out cross-validation where the model was trained for 50 epochs and the best model was retained. The specific hyperparameter configuration aligns with the optimal hyperparameters found by the original authors \cite{BENDR} and we find a similar balanced accuracy of $0.83 \pm 0.02$.


\subsection{Constructing Concepts}
\label{subsec:explanatory_concepts}

To construct human-aligned explanatory EEG concepts, a number of initial investigations were conducted. The data processing involved follows the methodology previously mentioned. In this section, we provide a general pipeline overview and discuss several choices made throughout the process.

\vspace{0.5em}
{\bf Concepts from Labeled EEG Data}: Using the labeled EEG data from the TUAR and TUEV Corpus and the MMIDB EEG Dataset, we create concepts representing activities within specific time windows. Each annotated segment of the EEG data is divided into windows of predetermined length and assigned the corresponding label.

\quad In the TUEV Corpus, we define concepts for the spike/short wave (\textit{spsw}), periodic lateralized epileptic discharge (\textit{pled}), general period epileptic discharge (\textit{gped}), technical artifact (\textit{artf}), and background (bckg) with 60-second windows. This approach aligns with the length of the \textit{seizure} classifier.

\quad Lastly, we examine the eye movement (\textit{eyem}) from the TUAR Corpus and \textit{Left Fist Movement} and \textit{Right Fist Movement} from the MMIDB EEG Dataset, both using 4-second windows. These different-sized windows then constitute examples of concepts defined based on their labels.


\vspace{0.5em}
{\bf Anatomical Concepts from Unlabeled EEG Data}:
The objective is to identify concepts representing specific frequency bands within distinct areas of the cortex, e.g. \emph{alpha activity in pre-motor cortex} or \emph{gamma activity in early visual cortex}. To obtain a non-task-specific representation of each cortical area, we utilize resting-state EEG data, as it spontaneously generates activity throughout the cortex. For this purpose, we use a subset of The TUH EEG Corpus, as described above.

\quad To define anatomical concepts, EEG data is segmented into 4-second windows, with the first and last 5 seconds of each sequence  excluded to minimize artifact contamination. The data is then divided into five frequency bands with a FIRWIN bandpass filter: \emph{delta} (0.5-4Hz), \emph{theta} (4-8Hz), \emph{alpha} (8-12Hz), \emph{beta} (12-30Hz), and \emph{gamma} (30-70Hz). The inverse operator for the forward model is computed using eLORETA \cite{doi:10.1098/rsta.2011.0081} via the MNE Python library. Since the spatial resolution is not critical, minimal regularization of $1 \times 10^{-4}$ is applied.

\quad Using the combined version of the multi-modal parcellation of the human cerebral cortex, HCPMMP1 \cite{f8095709e11547daa07262682e1545f2} and the inverse operator, the average power of electrical activity in 23 cortical areas for each hemisphere is determined.

\quad Our interest lies in cortical areas exhibiting the greatest deviation from typical activity within a specific frequency band. However, cortical areas are not equidistant from the scalp or consistent in baseline activity across bands. To normalize for these differences in the distribution of cortical activity, we compute the mean and standard deviation of the power in each cortical area for each frequency band on an EEG session level, which will be employed in various ways. We call these the baseline mean and the baseline standard deviation.

\quad We explore possible approaches to how the baseline means and standard deviation for each EEG session could be used to normalize the power of 4-second windows within that session. The options include dividing by the baseline standard deviation to account for scalp source variation, subtracting or dividing by the baseline mean to identify the cortical area with the greatest deviation, taking the absolute difference or not, and selecting a single cortical area across all frequency bands or only within a specific band.

\quad Identifying a single frequency and cortical area for each 4-second window of EEG data is a challenging task without prior work to guide the process, and each method presents its own limitations. We specifically look for \emph{alpha} desynchronization in the cerebral cortex during imagined or actual movement and closed or open eyes in the MMIDB EEG dataset, i.e., that \emph{alpha} activity in cortical areas decreases when activated.
Using a paired t-test to examine the presence of lateralization in cortical activities for different methods, we found that the preferred approach is to choose the area which maximizes the absolute difference between the given time window's power and the baseline mean, divided by the baseline standard deviation, only within specific frequency bands.

\vspace{0.5em}
{\bf Random Concepts:} Construction of CAVs calls for data examples that are considered random with respect to the concept of interest. In all experiments, random concepts consisting of 4-second or 60-second windows were randomly sampled from resting-state data obtained from the subset of the TUH EEG Corpus and unannotated sections of the TUAR dataset. 

 \subsection{Experiments} \label{subsec:experiments}
We investigate two approaches for defining explanatory concepts in EEG data. The TCAV method is then employed to evaluate whether the LHB model uses specifically defined human-aligned concepts of EEG data. For all concepts, the resulting activation vectors for all five bottlenecks in the LHB model architecture are examined to determine if they significantly align with the latent representations of class data in the model. We conduct the following experiments:
% Figure environment removed
%
\begin{enumerate}
\item \textbf{Sanity Checks:} We verify the TCAV method and construction of concepts function as intended through a series of sanity checks when classifying \textit{Left Fist Movement}.
\item \textbf{Event-based Concepts:} We assess whether the LHB model leverages specific EEG events in the classification of \textit{seizure}.
\item \textbf{Anatomy/Frequency-based Concepts:} We investigate if the LHB model employs lateralization in cortical activity in the \emph{alpha} band for classifying \textit{Left Fist Movement}. The chosen cortical areas are based on their relevance to the classification task.
\end{enumerate}
%
In the experiments, we use the TCAV method with a regularized linear model and stochastic gradient descent (SGD) learning, setting the regularization parameter $\alpha = 0.1$ to learn the decision boundary between explanatory and random concepts. We employ 50 random concepts and a maximum of 40 examples per concept. These parameters were chosen to increase statistical power. The mean TCAV scores for the target concept examples and the random examples are compared using the non-parametric Mann-Whitney U Rank test, as opposed to the t-test used in the original TCAV method, as we observed a clear violation of the normality assumption for the TCAV scores. To mitigate Type I errors, the p-values are corrected for each experiment employing the conservative Bonferroni method, after which we claim significance if the corrected p-value is below $0.05$.

