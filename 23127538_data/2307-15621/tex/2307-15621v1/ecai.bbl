\begin{thebibliography}{10}

\bibitem{abdelfattah2021zero}
Mohamed~S Abdelfattah, Abhinav Mehrotra, {\L}ukasz Dudziak, and Nicholas~D
  Lane, `Zero-cost proxies for lightweight {NAS}', {\em arXiv preprint
  arXiv:2101.08134}, (2021).

\bibitem{ainsworth2022git}
Samuel~K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa, `Git re-basin:
  Merging models modulo permutation symmetries', {\em arXiv preprint
  arXiv:2209.04836}, (2022).

\bibitem{ash2020warm}
Jordan Ash and Ryan~P Adams, `On warm-starting neural network training', {\em
  Advances in Neural Information Processing Systems}, (2020).

\bibitem{bergstra2012random}
James Bergstra and Yoshua Bengio, `Random search for hyper-parameter
  optimization.', {\em JMLR}, {\bf 13}(2), (2012).

\bibitem{bjorck2021towards}
Nils Bjorck, Carla~P Gomes, and Kilian~Q Weinberger, `Towards deeper deep
  reinforcement learning with spectral normalization', {\em Advances in Neural
  Information Processing Systems}, {\bf 34},  8242--8255, (2021).

\bibitem{cai2019once}
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han, `Once for all:
  Train one network and specialize it for efficient deployment', in {\em
  International Conference on Learning Representations}, (2020).

\bibitem{chen2015net2net}
Tianqi Chen, Ian Goodfellow, and Jonathon Shlens, `Net2net: Accelerating
  learning via knowledge transfer', {\em arXiv:1511.05641}, (2015).

\bibitem{chen2021progressive}
Xin Chen, Lingxi Xie, Jun Wu, and Qi~Tian, `Progressive {DARTS}: Bridging the
  optimization gap for nas in the wild', {\em International Journal of Computer
  Vision}, {\bf 129},  638--655, (2021).

\bibitem{coates2011analysis}
Adam Coates, Andrew Ng, and Honglak Lee, `An analysis of single-layer networks
  in unsupervised feature learning', in {\em Proceedings of AISTATS}. JMLR
  Workshop and Conference Proceedings, (2011).

\bibitem{dalibard2021faster}
Valentin Dalibard and Max Jaderberg, `Faster improvement rate population based
  training', {\em arXiv preprint arXiv:2109.13800}, (2021).

\bibitem{dunn1961multiple}
Olive~Jean Dunn, `Multiple comparisons among means', {\em Journal of the
  American statistical association}, {\bf 56}(293),  52--64, (1961).

\bibitem{elsken2018efficient}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter, `Efficient multi-objective
  neural architecture search via lamarckian evolution', {\em arXiv preprint
  arXiv:1804.09081}, (2018).

\bibitem{falkner2018bohb}
Stefan Falkner, Aaron Klein, and Frank Hutter, `Bohb: Robust and efficient
  hyperparameter optimization at scale', in {\em International conference on
  machine learning}, pp. 1437--1446. PMLR, (2018).

\bibitem{franke2020sample}
J{\"o}rg~KH Franke, Gregor K{\"o}hler, Andr{\'e} Biedenkapp, and Frank Hutter,
  `Sample-efficient automated deep reinforcement learning', {\em arXiv preprint
  arXiv:2009.01555}, (2020).

\bibitem{gao2020adversarialnas}
Chen Gao, Yunpeng Chen, Si~Liu, Zhenxiong Tan, and Shuicheng Yan,
  `{AdversarialNAS}: Adversarial neural architecture search for {GANs}', in
  {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp. 5680--5689, (2020).

\bibitem{gong2019autogan}
Xinyu Gong, Shiyu Chang, Yifan Jiang, and Zhangyang Wang, `{AutoGAN}: Neural
  architecture search for generative adversarial networks', in {\em Proceedings
  of the IEEE/CVF International Conference on Computer Vision}, pp. 3224--3234,
  (2019).

\bibitem{gui2021review}
Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye, `A review on
  generative adversarial networks: Algorithms, theory, and applications', {\em
  IEEE Transactions on Knowledge and Data Engineering}, (2021).

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, `Deep residual learning
  for image recognition', in {\em Proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition}, pp. 770--778, (2016).

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter, `{GANs} trained by a two time-scale update rule converge to a
  local nash equilibrium', {\em Advances in Neural Information Processing
  Systems}, {\bf 30}, (2017).

\bibitem{hu2020dsnas}
Shoukang Hu, Sirui Xie, Hehui Zheng, Chunxiao Liu, Jianping Shi, Xunying Liu,
  and Dahua Lin, `{DSNAS}: Direct neural architecture search without parameter
  retraining', in {\em Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, (2020).

\bibitem{hutter2011sequential}
Frank Hutter, Holger~H Hoos, and Kevin Leyton-Brown, `Sequential model-based
  optimization for general algorithm configuration', in {\em Learning and
  Intelligent Optimization: 5th International Conference}, pp. 507--523.
  Springer, (2011).

\bibitem{jaderberg2017population}
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech~M Czarnecki, Jeff
  Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan,
  et~al., `Population based training of neural networks', {\em arXiv preprint
  arXiv:1711.09846}, (2017).

\bibitem{jin2019auto}
Haifeng Jin, Qingquan Song, and Xia Hu, `Auto-{K}eras: An efficient neural
  architecture search system', in {\em Proceedings of the 25th ACM SIGKDD
  International Conference on Knowledge Discovery \& Data Mining}, pp.
  1946--1956, (2019).

\bibitem{klyuchnikov2022bench}
Nikita Klyuchnikov, Ilya Trofimov, Ekaterina Artemova, Mikhail Salnikov, Maxim
  Fedorov, Alexander Filippov, and Evgeny Burnaev, `{NAS-Bench-NLP}: neural
  architecture search benchmark for natural language processing', {\em IEEE
  Access}, {\bf 10},  45736--45747, (2022).

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton, `Learning multiple layers of features from
  tiny images', {\em Master's thesis, University of Toronto}, (2009).

\bibitem{li2020random}
Liam Li and Ameet Talwalkar, `Random search and reproducibility for neural
  architecture search', in {\em Uncertainty in Artificial Intelligence}, pp.
  367--377. PMLR, (2020).

\bibitem{liang2021regularized}
Jason Liang, Santiago Gonzalez, Hormoz Shahrzad, and Risto Miikkulainen,
  `Regularized evolutionary population-based training', in {\em Proceedings of
  the Genetic and Evolutionary Computation Conference}, pp. 323--331, (2021).

\bibitem{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang, `{DARTS}: Differentiable
  architecture search', {\em arXiv preprint arXiv:1806.09055}, (2018).

\bibitem{loshchilov2016cma}
Ilya Loshchilov and Frank Hutter, `{CMA-ES} for hyperparameter optimization of
  deep neural networks', {\em arXiv:1604.07269}, (2016).

\bibitem{lu2019nsga}
Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb, Erik
  Goodman, and Wolfgang Banzhaf, `{NSGA-Net}: neural architecture search using
  multi-objective genetic algorithm', in {\em Proceedings of the Genetic and
  Evolutionary Computation Conference}, (2019).

\bibitem{mellor2021neural}
Joe Mellor, Jack Turner, Amos Storkey, and Elliot~J Crowley, `Neural
  architecture search without training', in {\em International Conference on
  Machine Learning}, pp. 7588--7598. PMLR, (2021).

\bibitem{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida, `Spectral
  normalization for generative adversarial networks', {\em arXiv preprint
  arXiv:1802.05957}, (2018).

\bibitem{moritz2018ray}
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw,
  Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael~I Jordan,
  et~al., `Ray: A distributed framework for emerging {AI} applications', in
  {\em 13th {USENIX} Symposium on Operating Systems Design and Implementation},
  pp. 561--577, (2018).

\bibitem{pham2018efficient}
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean, `Efficient neural
  architecture search via parameters sharing', in {\em International Conference
  on Machine Learning}, pp. 4095--4104. PMLR, (2018).

\bibitem{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen, `Improved techniques for training {GANs}', {\em Advances in neural
  information processing systems}, {\bf 29}, (2016).

\bibitem{sharma2020alphanet}
Rishab Sharma, Rahul Deora, and Anirudha Vishvakarma, `{AlphaNet}: An attention
  guided deep network for automatic image matting', in {\em 2020 International
  Conference on Omni-layer Intelligent Systems}, (2020).

\bibitem{shim2021core}
Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang, `Core-set sampling for efficient
  neural architecture search', {\em arXiv preprint arXiv:2107.06869}, (2021).

\bibitem{sinha2020d2rl}
Samarth Sinha, Homanga Bharadhwaj, Aravind Srinivas, and Animesh Garg, `D2rl:
  Deep dense architectures in reinforcement learning', {\em arXiv preprint
  arXiv:2010.09163}, (2020).

\bibitem{syswerda1989uniform}
Gilbert Syswerda et~al., `Uniform crossover in genetic algorithms.', in {\em
  ICGA}, volume~3, pp. 2--9, (1989).

\bibitem{tassa2018deepmind}
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de~Las
  Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et~al.,
  `Deepmind control suite', {\em arXiv preprint arXiv:1801.00690}, (2018).

\bibitem{uriot2020safe}
Thomas Uriot and Dario Izzo, `Safe crossover of neural networks through neuron
  alignment', in {\em Proceedings of the 2020 Genetic and Evolutionary
  Computation Conference}, pp. 435--443, (2020).

\bibitem{wan2022bayesian}
Xingchen Wan, Cong Lu, Jack Parker-Holder, Philip~J Ball, Vu~Nguyen, Binxin Ru,
  and Michael Osborne, `Bayesian generational population-based training', in
  {\em International Conference on Automated Machine Learning}, pp. 14--1.
  PMLR, (2022).

\bibitem{wang2021attentivenas}
Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra, `Attentivenas: Improving
  neural architecture search via attentive sampling', in {\em Proceedings of
  the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.
  6418--6427, (2021).

\bibitem{wei2016network}
Tao Wei, Changhu Wang, Yong Rui, and Chang~Wen Chen, `Network morphism', in
  {\em International conference on Machine Learning}, pp. 564--572. PMLR,
  (2016).

\bibitem{wen2020autogrow}
Wei Wen, Feng Yan, Yiran Chen, and Hai Li, `Autogrow: Automatic layer growing
  in deep convolutional networks', in {\em Proceedings of the 26th ACM SIGKDD
  International Conference on Knowledge Discovery \& Data Mining}, pp.
  833--841, (2020).

\bibitem{wilcoxon1992individual}
Frank Wilcoxon, {\em Individual comparisons by ranking methods}, Springer,
  1992.

\bibitem{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Ya Gadre, Rebecca Roelofs, Raphael
  Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,
  Simon Kornblith, et~al., `Model soups: averaging weights of multiple
  fine-tuned models improves accuracy without increasing inference time', in
  {\em International Conference on Machine Learning}, pp. 23965--23998. PMLR,
  (2022).

\bibitem{xu2020pc}
Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi~Tian, and
  Hongkai Xiong, `{PC-DARTS}: Partial channel connections for memory-efficient
  architecture search', in {\em International Conference on Learning
  Representations}, (2020).

\bibitem{yangevaluation}
Antoine Yang, Pedro~M Esperan{\c{c}}a, and Fabio~M Carlucci, `{NAS} evaluation
  is frustratingly hard', in {\em International Conference on Learning
  Representations}, (2020).

\bibitem{yarats2022mastering}
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto, `Mastering
  visual continuous control: Improved data-augmented reinforcement learning',
  in {\em International Conference on Learning Representations}, (2022).

\bibitem{yuevaluating}
Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu
  Salzmann, `Evaluating the search phase of neural architecture search', in
  {\em International Conference on Learning Representations}, (2020).

\bibitem{Zaidi_Berariu_Kim_Bornschein_Clopath_Teh_Pascanu_2022}
Sheheryar Zaidi, Tudor Berariu, Hyunjik Kim, JÃ¶rg Bornschein, Claudia Clopath,
  Yee~Whye Teh, and Razvan Pascanu, `When does re-initialization work?',
  (arXiv:2206.10011), (Jun 2022).

\bibitem{zoph2016neural}
Barret Zoph and Quoc~V Le, `Neural architecture search with reinforcement
  learning', {\em arXiv preprint arXiv:1611.01578}, (2016).

\end{thebibliography}
