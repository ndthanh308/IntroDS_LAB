\section{Related work}
\label{relwork}

\subsection{Neural Architecture Search} \label{rel:nas}

NAS is the automatic process of finding a well-performing neural network architecture for a specific task. Already in early NAS work~\cite{zoph2016neural}, efficiency concerns played a role: candidate architectures were trained for only a few epochs. Similar low-fidelity search methods save compute by using fewer layers~\cite{lu2019nsga}, or only a subset of the data~\cite{shim2021core}. Another way to save compute is by utilizing a training-free metric to perform NAS without any training~\cite{mellor2021neural, abdelfattah2021zero}.
% ~\cite{mellor2021neural, abdelfattah2021zero}. saveref

ENAS~\cite{pham2018efficient} introduced the idea of weight sharing: all candidate architectures are viewed as subsets of a supernetwork, with the weights of the common parts reused across the architectures. The final architecture is scaled up and trained from scratch. This approach greatly decreased cost of the search to just several GPU-days. DARTS~\cite{liu2018darts} further increased efficiency by continuously relaxing the problem. Many approaches build upon DARTS by e.g., reducing memory usage~\cite{xu2020pc} or improving performance~\cite{chen2021progressive}.
AdversarialNAS~\cite{gao2020adversarialnas} extends the approach to GAN training, outperforming previous algorithms~\cite{gong2019autogan}.

In OnceForAll~\cite{cai2019once}, a supernetwork is pretrained such that subnetworks would perform well without retraining, in AttentiveNAS~\cite{wang2021attentivenas}
and AlphaNet~\cite{sharma2020alphanet}
performance is further improved. 
These approaches are a good fit for multi-objective NAS (where in contrast to single-objective NAS, multiple architectures with different trade-offs between objectives such as performance and latency are searched). 
However, the costs for the proposed pretraining reach thousands of GPU-hours. Additionally, in any supernetwork approach, the diversity and size of the architectures are restricted by the supernetwork.

Our approach of exchanging layers and weights between different networks is distinct from the supernetwork-based weight sharing. The weights in the supernetwork are constrained to perform well in a variety of subnetworks, while in our approach, after the weights have been copied to the network with a novel architecture, they can be freely adapted to it, independently of what happens to their original version in the parent network.

% Training-free approaches propose various metrics for estimating architecture quality without any training~\cite{abdelfattah2021zero, mellor2021neural}. Additionally, recent work reported many of the training-free approaches being on par with the simple baseline of counting the number of parameters~\cite{yang2022revisiting}.

The general idea of creating new architectures by modifying existing ones and reusing the weights has been explored in NAS approaches~\cite{elsken2018efficient, jin2019auto} relying on network morphisms\cite{chen2015net2net, wei2016network}.
Network morphisms are operators that change the architecture of a neural network without influencing its functionality. Although \cite{elsken2018efficient, jin2019auto} successfully used morphisms, the idea was later challenged~\cite{wen2020autogrow} with experiments  demonstrating that random initialization of new layers is superior to morphisms. Morphisms are different from our work: while they create a new architecture by modifying one existing architecture, we seek to mix two distinct architectures and reuse their weights.

\subsection{Population Based Training} \label{rel:hptune}

In hyperparameter optimization, hyperparameters of neural network training, such as learning rate or weight decay, are optimized. Bayesian optimization algorithms~\cite{hutter2011sequential, falkner2018bohb} 
are commonly used for sequentially evaluating promising hyperparameter configurations. 
% ASHA~\cite{li2020system} stands out for its asynchronous nature, which lends itself well to parallel execution. 
Other approaches include Evolutionary Algorithms~\cite{loshchilov2016cma, liang2021regularized}, and random search~\cite{bergstra2012random}, a simple but reasonably good baseline.

In contrast to the approaches that train weights for each hyperparameter configuration from scratch, PBT~\cite{jaderberg2017population} reuses partly-trained weights when exploring hyperparameters (see Section~\ref{intro} for short description and~\cite{jaderberg2017population} for details). 
% PBT trains a population of networks in parallel; every several epochs of training, a network is evaluated, and if it is in the bottom percentile, it is replaced by a copy of a network from the top percentile (including both hyperparameters and weights). A subset of copied hyperparameters is randomly modified, after which the training proceeds from the existing weights but with the new hyperparameter configuration. 

To the best of our knowledge, two algorithms were proposed for including architecture search into PBT: SEARL~\cite{franke2020sample} and BG-PBT~\cite{wan2022bayesian}. In SEARL, the architecture is modified by mutation, which can add a linear layer, add neurons to an existing layer, change an activation function, or add noise to the weights. We use a SEARL-like mutation as a baseline. 
In BG-PBT, there are multiple generations; in each generation, network architectures are sampled, initialized with random weights, and their training is sped up via distillation from the best network of the previous generation. This approach adds complexity in the form of multiple generations (the number of which must be manually determined) and using distillation (that would require adaptation to each setting, e.g., GAN training). In addition, sequential generations decrease parallelizability. 
Both SEARL and BG-PBT were proposed exclusively for RL tasks, while we construct PBT-NAS to be a general NAS algorithm.

% \subsection{Evolutionary Algorithms}

% An EA is an optimization algorithm that develops \textit{a population} of potential solutions by varying them, selecting superior ones, discarding the rest, and then repeating this process. 

% The simplest way to change a solution is via random mutation, i.e. replacing a part of the solution with random values. This, however, may not be enough to solve complicated problems~\cite{}. For such problems, crossover may be essential~\cite{}, that is, modifying a solution by replacing a part of it with the corresponding part of another solution in the population. This allows for information transfer between different solutions. If different solutions discover different valuable "building blocks" of the solution, crossover allows for combining them, removing the necessity of discovering all of them indpependently in each solution by random mutation.

% EAs achieve SOTA results on a variety of benchmark and real-world problems~\cite{chiong2012variants, petchrompo2022pruning, bouter2019gpu}.

% \subsection{Shrink-perturb} \label{rel:shpe}

% Shrink-perturb~\cite{ash2020warm} was motivated by the observation that in online learning, continuing training from an existing checkpoint when new data comes in can be worse than retraining from scratch using all the available data.

% Shrink-pertrub consists of modifying the weights of a neural network by \emph{shrinking} (multiplying by a constant $\lambda$) and \emph{perturbing} them (adding noise multiplied by a constant $\gamma$; following~\cite{pass}, we use fresh initialization of the network architecture as the source of noise). This procedure allows to both use the information already present in the weights, and add enough randomness for the network to be able to adapt to new data.

\subsection{Combining several neural networks into one} \label{rel:combine}

Neural networks can be combined in various ways. In evolutionary NAS~\cite{lu2019nsga} where weights are trained from scratch for each considered architecture, crossover is performed between encodings of architectures. Alternatively, there exist methods combining only the weights of networks that have the same architecture~\cite{uriot2020safe, ainsworth2022git}. Naively averaging the weights leads to a large loss in performance~\cite{ainsworth2022git}, which motivated these approaches to align neurons so that they would represent similar features. Averaging weights without alignment is possible if the weights of the networks are closely related. 
% In SWA~\cite{izmailov2018averaging}, checkpoints of the same model at different training stages are averaged for better performance. % saveref
The idea of model soups~\cite{wortsman2022model} is to start with a pretrained model, fine-tune it with different sets of hyperparameters, and greedily search which of the fine-tuned models to average.

In our approach, we mix different architectures \emph{together with the weights} during training, in contrast to evolutionary NAS algorithms combining only architecture encodings, and training the weights from scratch. We also avoid the additional complexity of aligning neurons, instead we continue to train the created network, and allow the gradient descent procedure to adapt the neurons to each other (which is facilitated by shrink-perturb~\cite{ash2020warm}, see Section~\ref{method:motivation}).