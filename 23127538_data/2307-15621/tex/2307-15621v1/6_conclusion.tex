\vspace{-5pt}
\section{Conclusion}
\label{conclusion}

In this paper we designed and evaluated PBT-NAS, a novel way to search for an architecture by mixing different architectures while they are being trained. 
% We find that this can be facilitated by adapting weights with the shrink-perturb technique. 
% We find that this can be advantageous compared to training multiple architectures without mixing if the weights are adapted with the shrink-perturb technique. 
We find that adapting the weights with the shrink-perturb technique during mixing is advantageous compared to copying or randomly reinitializing them.

PBT-NAS is shown to be effective on challenging tasks (GAN training, RL), where it outperformed considered baselines. At the same time, it is efficient, requiring training of only tens of networks to explore large search spaces. The algorithm is straightforward, parallelizes and scales well, and has few hyperparameters.

While in this work only NAS was considered, in the future, PBT-NAS could be adapted to simultaneously search for hyperparameters of neural network training, and of the algorithm itself, both of which would be necessary in order to fully automate the process of neural network training.