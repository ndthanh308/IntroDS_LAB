\section{Introduction}
\label{intro}

Neural Architecture Search (NAS) is the process of automatically finding a neural network architecture that performs well on a target task (such as image classification~\cite{liu2018darts}, natural language processing~\cite{klyuchnikov2022bench}, image generation~\cite{gao2020adversarialnas}). One of the key questions for NAS is the question of efficiency, since evaluating every promising architecture by fully training it would require an extremely large amount of computational resources.

Many approaches have been proposed for increasing the search efficiency: low-fidelity evaluation~\cite{zoph2016neural, shim2021core}, using weight sharing via a supernetwork~\cite{pham2018efficient, cai2019once}, estimating architecture quality via training-free metrics~\cite{mellor2021neural, abdelfattah2021zero}.
Typically, each approach has two stages: first, finding an architecture efficiently, then, training it (or its scaled-up version) from scratch. 
This final training usually requires a manual intervention (e.g., if an architecture of a cell is searched, determining how many of these cells should be used), which diminishes the benefit of an automatic approach (potentially, this could also be automated, but we are not aware of such studies in the literature). Ideally, an architecture itself (not its proxy version) should be searched on the target problem, with the search result being immediately usable after the search (such single-stage NAS approaches exist but are limited: e.g., they restrict potential search spaces~\cite{hu2020dsnas} or require costly pretraining~\cite{cai2019once, wang2021attentivenas}).

For the task of hyperparameter optimization (which is closely related to NAS), effective and efficient single-stage algorithms exist in the form of Population Based Training (PBT)~\cite{jaderberg2017population} and its extensions~\cite{liang2021regularized, dalibard2021faster}. The key idea of PBT is to train many networks with different hyperparameters (a population) in parallel: as the training progresses, worse networks are replaced by copies of better ones (including the weights), with hyperparameter values explored via random perturbation. PBT is highly efficient due to the weight reuse, and due to its parallel nature: given a sufficient amount of computational resources, running PBT takes approximately the same wall-clock time as training just one network.

Determining the best way to adapt PBT to NAS is an open research question~\cite{dalibard2021faster}: if a network architecture has been perturbed, the partly-trained weights cannot be reused (because, e.g., weights of a convolutional layer cannot be used in a linear one). The naive approach of initializing them randomly does not work well (see Section~\ref{abl:shpe_crucial}), and existing algorithms extending PBT to NAS~\cite{franke2020sample, wan2022bayesian} sidestep the issue at the cost of parallelizability or performance (see Section~\ref{rel:hptune}).

% Figure environment removed

We propose to adapt PBT to NAS by modifying the search to rely not on random perturbations but on mixing layers of the networks in the population. An example of this principle is combining an encoder and a decoder from two different autoencoder networks, ultimately obtaining a better-performing network. 
In this setting, the source of the weights for the changed layers is natural: they can be copied from the parent networks. 
Furthermore, we explore if additionally adapting the copied weights with the shrink-perturb technique~\cite{ash2020warm} (reducing weight magnitude and adding noise) is helpful for achieving a successful transfer of a layer from one network to another. 

For many standard tasks (such as image classification), single-objective NAS algorithms are  matched by (or show only a small improvement over) the simple baseline of random search~\cite{li2020random, yuevaluating}.
In order to make the potential benefit of PBT-NAS clear, 
experiments in this paper are conducted in two challenging settings: Generative Adversarial Network (GAN) training, and Reinforcement Learning (RL) for visual continuous control. We further advocate for harder tasks and search spaces in Section~\ref{discussion}.

While our approach could potentially be extended to include hyperparameter optimization, this paper is focused on architecture search.

The contributions of this work are threefold:
\begin{enumerate}
    \item We propose to conduct NAS by training a population of different architectures and mixing them on-the-fly to create better ones (while inheriting the weights).
    \item We investigate if applying shrink-perturb~\cite{ash2020warm} to the weights is a superior technique for weight inheritance compared to copying or random reinitialization.
    \item Integrating these ideas, we introduce PBT-NAS, an efficient and general NAS algorithm, and evaluate it on challenging NAS search spaces and tasks. 
\end{enumerate}