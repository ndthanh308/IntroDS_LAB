%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \clearpage
\appendix
% % \onecolumn % this is optional
% % \section{Appendix}
% \section*{APPENDIX}

\section{Visualizing search progress} \label{app:viz}

In order to visualize the search process, we track the origin of all the layers in the population. Initially, the layers of the $i$-th population member are specified to have origin $i$. When a new network is created by mixing different networks, its layers will have different origins. Over time, as worse-performing networks are replaced with the results of mixing better-performing ones, the successful layers constitute an increasing proportion of all layers in the population.

We visualize the experiments on the Quadruped Run task that achieved the best (Figure~\ref{fig:vis_progress:a}) and the worst (Figure~\ref{fig:vis_progress:b}) scores (out of the three seeds).
In the experiment in Figure~\ref{fig:vis_progress:a}, many architectures are successfully mixed, with the final population containing layers coming from several initial architectures. On the contrary, in the experiment in Figure~\ref{fig:vis_progress:b}, one architecture largely takes over, with a small part inherited from a second architecture. 

% Figure environment removed


\section{Additional Bayesian Optimization baseline} \label{app:bohb}
In this section we compare PBT-NAS to an additional baseline, Bayesian Optimization Hyperband (BOHB)~\cite{falkner2018bohb}. BOHB is a well-established, efficient, and parallel Bayesian Optimization (BO) algorithm designed for hyperparameter optimization and NAS.

BO algorithms can be applied to almost any NAS search space, but typically have limited parallelization capability, and can struggle in high-dimensional spaces, especially if there are too few evaluations available. Our experiments in the main text were conducted in exactly such a scenario: the search spaces were high-dimensional (the \texttt{GanHard} space has 72 variables, the RL space has 36), and the number of total evaluations is smaller than the number of variables (24 evaluations for \texttt{GanHard}, 12 for RL). In addition, PBT-NAS and the baselines considered in the main text are fully parallelizable (unlike BO algorithms).

We report the performance of BOHB in two settings (in which all our ablations were done). BOHB is run with the same budget as PBT-NAS both for Reinforcement Learning (the Quadruped Run task, total number of epochs equal to fully training 12 architectures) and GAN training (\texttt{GanHard} space, CIFAR-10 dataset, total number of epochs equal to fully training 24 architectures). Same as in the main text, every experiment is repeated three times, we report the mean and standard deviation of the performance of the best architecture.

\begin{table}
    \centering
    \caption{Results of running BOHB. The best value in each column is in bold.}
    \begin{NiceTabular}{@{}lcc@{}} \toprule
    Algorithm & FID $\downarrow$ & Score $\uparrow$ \\
     & (CIFAR-10, \texttt{GanHard}) & (Quadruped Run) \\
    \midrule
    PBT-NAS & $\mathbf{13.25}_{\pm 1.64}$ & $\mathbf{801}_{\pm 70}$ \\
    BOHB & $18.95_{\pm 1.48}$ & $621_{\pm28}$ \\
    
    \bottomrule
    \end{NiceTabular}
    \label{tab:bohb}
\end{table}

As can be seen in Table~\ref{tab:bohb}, BOHB  is outperformed by PBT-NAS in both settings. The BOHB results for GAN training are particularly poor, we speculate that BOHB suffers from excessive greediness in this setting (it early-stops two-thirds of solutions at each fidelity). PBT-NAS, while also greedy, has lower selection pressure, so it is affected less.

\section{Statistical testing} \label{app:stats}

Table~\ref{tab:pvalues} lists p-values for one-sided Wilcoxon pairwise rank tests~\cite{wilcoxon1992individual} with Bonferroni correction~\cite{dunn1961multiple}. In the GAN setting, the null hypothesis is that Algorithm 1 has a higher FID than Algorithm 2. In the RL setting the null hypothesis is that Algorithm 1 has a lower score than Algorithm 2.

In the GAN setting, results for CIFAR-10 using \texttt{Gan}, \texttt{GanHard}, and STL-10 using \texttt{GanHard} are tested together to increase sample size (total sample size is 9). Similarly to increase sample size, in the RL setting all the tasks (Quadruped Run, Walker Run, Humanoid Run) are tested together (total sample size is 9).
P-values below the significance threshold of 0.0125 are highlighted (target $p$-value=0.05, 4 tests, corrected $p$=0.0125). 

\begin{table}[htbp]
\centering
  \caption{P-values of conducted experiments.}
  \label{tab:pvalues}
  \begin{tabular}{cccl}
    \toprule
    Algorithm 1 & Algorithm 2 & Setting & P-value\\
    \midrule
    PBT-NAS & Random search & GAN & \cellcolor{mycolor} 0.001953125 \\
    PBT-NAS & SEARL-like mutation & GAN & \cellcolor{mycolor} 0.00390625 \\
    PBT-NAS & Random search & RL & \cellcolor{mycolor} 0.001953125 \\
    PBT-NAS & SEARL-like mutation & RL & \cellcolor{mycolor} 0.005859375 \\
  \bottomrule
\end{tabular}
\end{table}


\section{Reproducibility issues with \texttt{Gan} on STL-10} \label{app:stl10}

In~\cite{gao2020adversarialnas}, results for STL-10 are reported on a single seed, we ran the experiment 5 times using the official implementation. The resulting FID for STL-10 increases by about 10 points, which is a substantial drop in performance (see Table~\ref{tab:perf_stl_gan}). The immediate cause is the divergence of the training procedure before good results could be achieved. However, as the official implementation was used, it is unclear why this divergence appeared consistently for all seeds and why none of the five seeds achieved the performance reported in~\cite{gao2020adversarialnas}. The authors did not respond when we notified them of the problem. We are also aware of an independent reproduction attempt running into the same issue.

Since our code relies on the AdversarialNAS codebase, whatever the issue is, it influenced all our experiments with \texttt{Gan} on STL-10, bringing their validity into question. Nonetheless, for transparency, our results (for three seeds) are reported in Table~\ref{tab:perf_stl_gan}. PBT-NAS achieves the best performance out of the approaches tested by us.

Note that we did not face the reproducibility issue with \texttt{Gan} on CIFAR-10, as the reproduced result (FID $12.29_{\pm 0.80}$) was close to the reported one (FID $10.87$~\cite{gao2020adversarialnas}). Also of note is that with \texttt{GanHard} on STL-10, results were even better than in~\cite{gao2020adversarialnas} (see Table 1 in the main text): FID of $25.11_{\pm 0.94}$. Since the same training code was used for \texttt{Gan} and \texttt{GanHard}, this implies that architectures could be the root cause of the problem, with architectures from \texttt{Gan} being a poor fit for STL-10, in contrast to architectures from \texttt{GanHard}. This is not consistent with the results from the main text for CIFAR-10, where, controlled for the amount of computational effort, better architectures could be found in \texttt{Gan} than in \texttt{GanHard}.

We would also like to note that \texttt{GanHard} was designed before any experiments with STL-10 were run, precluding the possibility that better STL-10 results with \texttt{GanHard} (compared to \texttt{Gan}) were achieved by deliberate search space adaptation to the dataset. STL-10 was our test dataset, the results for which did not influence any design or hyperparameter choices made in the paper.

\begin{table}[tp]
    \centering
    \caption{Additional results for GAN training (mean $\pm$ st. dev.).}
    \begin{NiceTabular}{@{}lcc@{}} \toprule
    \Block{3-1}{Algorithm} & \Block{1-2}{STL-10} & \\
    \cmidrule{2-3} & \Block{1-2}{\texttt{Gan}} & \\
    \cmidrule{2-3}
    & FID $\downarrow$ & IS $\uparrow$ \\
    \midrule
    AdversarialNAS (reported in~\cite{gao2020adversarialnas}) & $26.98$ & $9.63$ \\
    AdversarialNAS (reproduced) & $36.87_{\pm 3.62}$ & $8.90_{\pm 0.32}$ \\
    Random search & $32.54_{\pm 3.20}$ & $9.15_{\pm 0.08}$ \\
    SEARL-like mutation~\cite{franke2020sample} & $33.98_{\pm 4.36}$ & $8.98_{\pm 0.19}$ \\
    PBT-NAS & $29.51_{\pm 0.91}$ & $9.19_{\pm 0.05}$\\
    \bottomrule
    \end{NiceTabular}
    \label{tab:perf_stl_gan}
\end{table}

\section{Illustrations of GAN search spaces} \label{app:gan_ss}

% Figure environment removed

% Figure environment removed

\section{Implementation details} \label{app:impl}

The experiments were run on servers equipped with three Nvidia A5000 GPUs each. Each server is equipped with 2 Intel(R) Xeon(R) Bronze 3206R CPUs, and 96 GB of RAM. The used OS is Fedora Linux 36. The Ray~\cite{moritz2018ray} framework was used to run the experiments in a distributed fashion. The configuration files specifying versions of all software libraries are included in the source code.

The experiment costs in total GPU hours vary by the setting (CIFAR-10|\texttt{Gan}: 200, CIFAR-10|\texttt{GanHard}: 270, STL-10|\texttt{Gan}: 720, STL-10|\texttt{GanHard}: 1200, Quadruped Run: 210, Walker Run: 330, Humanoid Run: 1300). 