% % Figure environment removed

\section{Study Design}
\label{sec:approach}
In this section, we present the details of our study design. We first present our research questions and then introduce our dataset and the ChatGPT model. 

\subsection{Research Question}
In this empirical study, we aim to answer the following research questions.

\textbf{RQ1.} \textit{How effective is ChatGPT on code generation for programming tasks?} Despite informally receiving positive feedback from the community, there is a lack of comprehensive study on the performance of ChatGPT in code generation. This research question aims to measure how well ChatGPT could generate code for programming tasks and to analyze the factors that impact its performance, including task complexity, difficulty, and time that tasks are introduced.

\vspace{0mm}

\textbf{RQ2.} \textit{What are the common issues in ChatGPT-generated code?}  This research question aims to analyze issues in ChatGPT-generated code using popular static analysis tools and categorize them into common categories. 

\vspace{0mm}

\textbf{RQ3.} \textit{Can ChatGPT fix the code quality issues with prompting?} Conversational AI models such as ChatGPT allow users to provide feedback to allow ChatGPT to revise its output. This research question aims to investigate whether ChatGPT can correct coding issues based on runtime errors, feedback from the compiler, and static analysis tools.

% Figure~\ref{fig:overview} shows the overall workflow of our study. \thanh{TODO: ...}

\subsection{Constructing Benchmark Dataset}

Existing benchmarks for evaluating AI-based code generation are often limited and outdated. 
For example, prior work~\cite{chen2021evaluating, chen2022codet} has relied on the HumanEval dataset~\cite{chen2021evaluating}, which only includes 164 Python programming tasks before 2021. 
Such small and outdated datasets, however, are not ideal for evaluating modern AI models, such as ChatGPT, since they lack diversity and may have been used in the training data of modern AI models.
To address this issue, Fan~\ea~\cite{fan2022automated} introduce a new dataset, LMDefects, that contains 113 Java programming tasks released after Jun 2021. The dataset was collected from LeetCode, a well-known online platform that offers a variety of coding challenges to help programmers enhance their abilities and prepare for technical interviews. The dataset, however, is still relatively small and focused solely on Java programming tasks. 

In this study, we extend LMDefects by collecting all accessible programming tasks and the relevant official public test suites in LeetCode, and investigate ChatGPT's ability in generating code in both Java and Python.
% To address this issue, we collect our dataset from LeetCode~\cite{leetcode} a popular online platform that provides a collection of coding challenges to help programmers improve their skills and prepare for technical interviews.
At the time of data collection (March 2023), there were 2,617 task problems available on LeetCode. 
These problems cover various topics, including data structures, algorithms, databases, and concurrency.
For our dataset, we focused on the problems that were designed specifically for Java and Python, as these two languages are widely used and have a large community of developers.
Additionally, in order to provide a fair and accessible dataset, we filtered out the premium tasks that require a subscription to access.
After this filtering process, we successfully collected 2,033 programming tasks from LeetCode.
Out of the 2,033 tasks in our dataset, we found that 501 were classified as easy, 1,064 as medium, and 468 as hard.

\vspace{0mm}

\subsection{The ChatGPT Model}
% % Figure environment removed
ChatGPT is a large language model that can provide detailed responses given natural language instructions. We use the model that has been fine-tuned from the GPT3.5 models~\cite{gpt35} using Reinforcement Learning from Human Feedback~\cite{stiennon2020learning}. In our study, we used the ChatGPT-March-23 version~\cite{chatgptrelease}, which was trained on data up to 2021. To instruct ChatGPT, we followed the zero-shot prompt setting which does not include examples of code generation in the prompt as follows: "\textit{Please provide a code implementation of the following description + $<$description of a programming task$>$ + Provide a valid $<$programming language$>$ code with this template $<$solution template provided containing the input and output specifications$>$}".

% where \texttt{<description>} is the description for a programming task including task description and a few input-output examples while \texttt{<template>} is the code template provided by LeetCode which includes the input and output specifications. An example of our prompt can be seen in Figure~\ref{fig:prompt_example}.

% \subsubsection{Code Evaluation} 
% Our evaluation of the quality of ChatGPT-generated code involves a two-stage process, which includes automated and manual evaluation. In the first stage, we test ChatGPT-generated code against test suites provided by LeetCode. If the code fails on any test cases, we consider it to have quality issues and collected runtime errors and compiler feedbacks. However, as the code may still face quality issues even if it passes all the test cases, we then conduct a manual evaluation for the remaining code. Specifically, one author manually analyze the code to identify any potential code issues, and the other authors then review the manual evaluation to ensure its quality.

% \subsubsection{Open Card Sorting Discussion}

% \subsubsection{Code Fixing}

% \subsection{Experimental Setup}

% \subsubsection{Evaluation Metrics}

% \subsubsection{Implimentation Details}




