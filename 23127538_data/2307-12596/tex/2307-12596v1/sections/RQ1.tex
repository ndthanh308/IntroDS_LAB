\input{tables/tab1_overall_leetcode}
\section{RQ1: Performance}
\label{sec:RQ1}
In this section, we present the results for RQ1, which investigates the effectiveness of ChatGPT in code generation. 
To mitigate the randomness of ChatGPT, we make ChatGPT deterministic by setting the temperature to 0 and running the model once for each task, using the first generated output for evaluation.
ChatGPT's performance is measured with zero-shot pass-rate (\textit{pass@1}), which measures whether the model produces a correct solution on the first attempt.
For example, if ChatGPT generates code snippets for 10 tasks and 7 of them pass the test cases in the first attempt, the pass@1 accuracy would be 0.70.
We also conducted the Mann-Whitney U rank test~\cite{mann1947test} to measure the statistical significance of the performance differences by ChatGPT across factors. 
The Mann-Whitney U rank test is a non-parametric statistical test used to compare two independent samples to determine whether there is a significant difference between the two distributions, while the Cliff's Delta~\cite{macbeth2011cliff} effect size measures the magnitude of the difference between the samples.
% A lower p-value (typically, below 0.05) indicates a statistically significant difference between the samples. The effect size (Cliff's Delta~\cite{macbeth2011cliff}) measures the magnitude of the difference between the samples. An effect size greater than 0.5 is considered large, between 0.3 and 0.5 is medium, between 0.1 and 0.3 is small, and below 0.1 is negligible~\cite{macbeth2011cliff}.


Table~\ref{tab:rq1acc_leetcode} presents the pass rate of ChatGPT for LeetCode tasks with different difficulties in both Python and Java. 
It can be seen that ChatGPT performs better on easy tasks than on medium and hard tasks. 
For Python, the model achieves a pass@1 accuracy of 0.890 for easy tasks,  indicating that ChatGPT can handle 89\% of easy tasks in one attempt. 
However, the performance drops to 0.674 for medium tasks and further decreases to 0.400 for hard tasks.  
Similarly, for Java, the model attains a pass@1 accuracy of 0.860 for easy tasks, 0.710 for medium tasks, and 0.468 for hard tasks. 
These findings suggest that the difficulty level of tasks has a significant impact on the performance of ChatGPT in code generation.
Table~\ref{tab:rq1acc_leetcode} also shows the results from the Mann-Whitney U test on performance differences between Python and Java.
Although ChatGPT performs slightly better in Java for medium ($\uparrow 5.3\%$) and hard tasks ($\uparrow 17\%$), their difference in performance is not significant with a p-value of at least 0.53 and an effect size value less than 0.02~\cite{macbeth2011cliff}. 
% Although the difference in performance between Python and Java is not substantial,
% Similarly, the Mann-Whitney U test results (P-value) indicate that the difference in performance between Python and Java is not statistically significant at the conventional 0.05 level. Additionally, the effect size (Cliff's Delta) is small, suggesting that the difference in performance is negligible.


% This could be attributed to the model's training data, which might consist of more Java-based examples, or due to the inherent structure of Java programming that makes it easier for the model to generate code.

% \find{
% \textbf{Finding 2:} The performance of ChatGPT may vary across programming languages.
% }

\input{tables/RQ1_comparism}

% Figure environment removed


As ChatGPT (GPT-3.5-turbo) is trained solely on data until September 2021~\cite{chatgpt}, it's also important to measure its performance changes as new challenges arise.
% Since ChatGPT (GPT-3.5-turbo) is trained only on data available up to September 2021~\cite{chatgpt}, it is also important to understand how its capability evolves with the emergence of new problems.
Figure~\ref{fig:rq1time} illustrates the pass rates of ChatGPT across different difficulty levels (easy, medium, and hard) and programming languages (Python and Java) over five distinct time periods. 
The chart shows that the performance of ChatGPT declines over time for both Python and Java.
Specifically, ChatGPT can solve more than half of the hard-level code tasks before June 2021, but its performance reduces drastically to nearly 0.1 for the subsequent time periods.
The decline in performance is not as pronounced for easy-level tasks, which indicates that ChatGPT still maintains some level of proficiency when dealing with simpler problems, even as time progresses. 
As shown in Table~\ref{tab:rq1_comparism_pass_fail}, the Mann-Whitney U test indicates that the time period when tasks are introduced has a statistically significant difference between passed code and failed code (p-value \textless~0.001) with a large Cliff's delta effect size.
However, this observation also highlights the model's limitations in adapting to the intricacies and nuances of more complex, newer programming challenges.
Moreover, the drop in performance of ChatGPT could be explained by a data leakage issue in which the LeetCode problem may be contained in ChatGPT's training data. Therefore, the performance of ChatGPT on old programming tasks which is published before December 2021 may only reflect the memorization capability~\cite{carlini2023quantifying} of ChatGPT instead of its real performance. Therefore, the results also highlight the need to evaluate the model on the newly-introduced dataset after September 2021 for fair evaluations. 

% \find{
% \textbf{Finding 3:} The performance of ChatGPT significantly decreases, up to five times, on new programming tasks, highlighting that the model's limitations in adapting to new challenges.
% }

% Figure environment removed

In addition to difficulty levels and time periods, another factor that may impact the performance of ChatGPT is the length of the generated code. 
Figure~\ref{fig:rq1leght} presents the pass rates of ChatGPT for both Python and Java programming languages, grouped by the number of lines in the generated code. 
It is worth noting that the distribution of code lengths is not uniform, with the majority of generated code snippets falling into the 10-20 lines range for Python and the 20-30 lines range for Java. 
This discrepancy highlights the differences in verbosity and structure between the two programming languages, which might also contribute to the variations in ChatGPT's performance across different length categories.
In Figure~\ref{fig:rq1leght} there is a clear trend of decreasing the pass rate for both Python and Java, as the length of the generated code increases.
For Python, the pass@1 rate starts at 0.872 for code snippets with less than 10 lines and gradually decreases to 0.265 for code snippets with more than 50 lines. 
For Java, the pass@1 rate gradually decreases from 0.838 for code snippets with 10--20 lines to 0.478 for code snippets with more than 50 lines. 
This trend suggests that ChatGPT's ability to generate correct and bug-free code is inversely proportional to the size of the generated code. 
This could be due to the increased complexity, and the greater number of potential interactions between code elements as the code size grows, making it harder for the model to generate a correct and complete solution.
As shown in Table~\ref{tab:rq1_comparism_pass_fail}, the Mann-Whitney U test confirms the significance of the differences (p-value $<$ 0.01) with a small to medium effect size.
Overall, these findings suggest that improving the model's ability to generate longer and more complex code snippets is a valuable direction for future research and development.

% \find{
% \textbf{Finding 4:} The performance of ChatGPT is remarkably affected by the program size, decreasing up to 3 times in larger code snippets.
% }

%In summary, our analysis of ChatGPT's performance in code generation tasks (RQ1) reveals that its effectiveness is influenced by several factors, including the difficulty level of the task, the programming language, the time period of the problem, and the length of the generated code.

In summary, our results indicate that the model's performance declines over the difficulty level and time period of code tasks.
Furthermore, the model's ability to generate correct and bug-free code is inversely proportional to the size of the generated code, suggesting that the increased complexity of longer code snippets poses a significant challenge for the model.
Based on these findings, it is recommended that future research and development efforts focus on improving the model's ability to handle more complex tasks, adapt to new programming challenges, and generate longer and more intricate code snippets. 
%In light of these findings, future research and development efforts should focus on improving the model's ability to handle more complex tasks, adapt to new programming challenges, and generate longer and more intricate code snippets. 

\find{
\textbf{Finding 1:} The performance of ChatGPT is significantly and substantially affected by task difficulty,  time that tasks are introduced, program size and programming languages.
}

