\section{RQ2: Bugs and Issues}
\label{sec:RQ2}

\subsection{Static Analysis}
To address RQ2, our first step is to gather output from LeetCode for ChatGPT-generated code.
If the generated code does not pass the tests, we label it as ``Wrong Outputs'' to indicate failure to meet the problem requirements. 
However, passing test cases alone does not guarantee that the code is free from quality issues.  
Therefore, to further investigate the code quality and identify potential bugs, style issues, and other concerns that might impact the overall quality, we employ static analysis tools tailored for each programming language.

For Java code samples, we use PMD~\cite{copeland2005pmd} and Checkstyle~\cite{burn2003checkstyle}.
PMD is a well-known static analysis tool that inspects Java source code to identify potential problems and provides suggestions for improvements~\cite{wattanakriengkrai2020predicting}.
Checkstyle, on the other hand, statically checks Java code against a specified set of coding conventions.
Both tools evaluate the Java source code against a set of rules, reporting warnings for any violations, their priority, and the corresponding lines in the file.
Similarly, for Python code samples, we utilize Pylint~\cite{thenault2001pylint} and flake8~\cite{cordasco2010flake8}.
Pylint is a popular and comprehensive static analysis tool that enforces coding standards and detects various types of issues in Python code~\cite{siddiq2022empirical,vassallo2020configuration}. Flake8 is another widely-used tool for Python, which combines PyFlakes, pycodestyle, and McCabe to check for syntax errors, style issues, and code complexity, respectively.
These tools enable us to assess the code quality from multiple dimensions, beyond just functional correctness.

% Figure environment removed

After running the static analysis tools, we gather issues found for each ChatGPT-generated program identified by the compilers and the static analysis tools.
In order to simplify the analysis and reduce the impact of false positives, we focus on more significant aspects of code quality and functionality. 
Therefore, we choose to ignore messages related to style issues such as whitespace, newline, and invalid naming conventions, which is consistent with the approach taken in prior work~\cite{siddiq2022empirical}.
Figure~\ref{fig:rq2_overall_distribution} presents the distribution of code quality based on the difficulty levels and programming languages for both passed and failed tasks.
The figure highlights the proportion of clean code, which refers to the code snippets without issues identified by the static analysis tools, and the code with issues.

Figure~\ref{fig:rq2_overall_distribution} shows that the proportion of clean code is generally higher for passed tasks compared to failed tasks.
For Python, 63\% of the passed tasks have clean code, while only 56\% of the failed tasks are clean. 
In the case of Java, 47\% of the passed tasks have clean code, as opposed to 39\% for failed tasks.
Additionally, it is evident that the percentage of clean code decreases as the difficulty level increases for both Python and Java. 
For example, the percentage of clean Java code decreases from 54\% for easy tasks to 45\% for medium tasks, and further drops to 33\% for hard tasks.
These findings underscore the importance of addressing code quality concerns in tandem with functional correctness to better support developers in handling complex programming tasks across different languages and domains.

\find{
\textbf{Finding 2:} Code quality issues commonly happen in both code that pass or failed test cases, highlighting the need for characterizing and addressing these concerns alongside functional correctness.
}

\subsection{Open Card Sorting Discussion}

To gain a deeper understanding of the common issues and patterns found in the ChatGPT-generated code, we conducted a qualitative analysis using open card sorting. 
The open card sorting process has been used in many previous studies~\cite{wan2017bug,bao2016android,lo2015practitioners} to generate categories or taxonomy from data.
In this study, we follow the card sorting process highlighted by previous studies~\cite{spencer2009card,lo2015practitioners}, which mainly consisted of two phases.
First, the preparation phase where we created cards for each programming task. This card is filled with the title of the programming task, generated code by ChatGPT, the test results, and the static analysis tools result. The second phase is the execution phase where we choose the representative random sample of 154 programming tasks for both Java and Python from a total of 2,033 programming tasks (with a 99\% confidence level and 10\% margin of error).
We analyzed and discussed each card, and iteratively sort them into groups based on their issues. During the card sorting process, we found that many of the cards could be placed into several different categories, as there can be more than one issue coming up in a given code. Once all the cards were placed into categories, we created category names based on the issue patterns that we observed.

%\yue{Ratna, please help me add more details for why and how we do open card sorting}


After a thorough analysis and discussion of the card sorting process, the experts identify four different categories of issues in the ChatGPT-generated code: %, which can be broadly classified into the following categories:

\textbf{Compilation and Runtime Errors:} 
Compilation and Runtime errors encompass issues that prevent the correct execution of the code.
These errors can arise due to various factors, such as incorrect use of a programming language's syntax, improper handling of data structures, invalid input, or exceeding the bounds of an array. 
The errors often lead to failures during compilation or runtime, and they need to be resolved before the program can function as intended.
Code~\ref{code:syntax_example} demonstrates a compilation error that occurs when ChatGPT attempts to use the \textasciicircum (bitwise XOR) operator with incompatible operand types, causing a compilation error.

% Syntax errors are caused by incorrect use of a programming language's syntax, leading to failures during compilation or interpretation. 
% Examples include missing parentheses, incorrect indentation, or undeclared variables. 
% Syntax errors prevent the code from executing and need to be resolved before the program can run.
% Code~\ref{code:syntax_example} demonstrates a syntax error that occurs when ChatGPT attempts to use the \textasciicircum (bitwise XOR) operator with incompatible operand types, causing a compilation error.

\begin{lstlisting}[language=java, caption=An example of compliation error (LeetCode Problem 2564 - Java), label=code:syntax_example]
if (prefix[mid] ^ (left == 0 ? 0 : prefix[left - 1]) > queries[i][1]) {
    r = mid - 1;
} else {
    l = mid;
}
//Compiler: Solution.java:1: error: bad operand types for binary operator '^'
\end{lstlisting}

% \textbf{Runtime Errors:} Runtime errors are errors that occur during the execution of the program. 
% These errors can be caused by a variety of factors, such as invalid input, incorrect use of data structures, or exceeding the bounds of an array.  

\textbf{Wrong Outputs: } Wrong Outputs represent issues in the code that cause it to produce incorrect results or fail to meet the problem requirements. 
These errors can stem from incorrect algorithms, improper handling of edge cases, or other inaccuracies in the desired logic. 
These errors can occur even when the code is syntactically correct and free from any runtime errors.
Code~\ref{code:incorrect_example} presents an example where ChatGPT provided an inaccurate solution to LeetCode Problem 746, ``Min Cost Climbing Stairs".
The issue arises due to the incorrect construction of the loop and final return statement. 
In this specific example, the input is [10, 15, 20]. 
The expected output is 15, achieved by climbing the steps with costs of 10 and 20, while skipping the step with a cost of 15. 
However, the ChatGPT-generated code produces an output of 25. 
This error occurs because the loop iterates one step more than necessary, causing the last step's cost to be included in the calculation even when it should not be.

\begin{lstlisting}[language=python, caption=A example of Wrong Outputs (LeetCode Problem 746 - Python), label=code:incorrect_example]
def minCostClimbingStairs(self, cost: List[int]) -> int:
    n = len(cost)
    dp = [0] * (n+1)
    dp[0] = cost[0]
    dp[1] = cost[1]
    for i in range(2, n+1):
        dp[i] = min(dp[i-1], dp[i-2]) + cost[i-1]
    return min(dp[n-1], dp[n])
#Compiler: AssertionError: Input: cost = [10, _15_ ,20]  Expected output: 15
\end{lstlisting}


\textbf{Code Style and Maintainability:} This category encompasses issues related to the code's formatting, organization, and adherence to established style guides.
Poorly styled or organized code is difficult to read, understand, and maintain.
Examples include inconsistent indentation, overly long lines, or lack of meaningful variable names.
Code 5 presents an example where ChatGPT provided an smelly solution to LeetCode Problem 1297, â€œMaximum Number of Occurrences of a Substring".
In this case, the static analysis tool PMD identified an issue on line 1: the variable `maxSize` is declared but never used.

\begin{lstlisting}[language=python, caption=A example of Code Style and Maintainability (LeetCode Problem 1297 - Python), label=code:code-smell]
def maxFreq(self, s: str, maxLetters: int, minSize: int, maxSize: int) -> int:
    count = defaultdict(int)
    res = 0

    for i in range(len(s) - minSize + 1):
        substring = s[i: i + minSize]
        if len(set(substring)) <= maxLetters:
            count[substring] += 1
            res = max(res, count[substring])

    return res
#PMD: 1297-maximum-number-of-occurrences-of-a-substring.py:1:61: W0613: Unused argument 'maxSize' (unused-argument)
\end{lstlisting}

\textbf{Performance and Efficiency:} 
Performance and efficiency issues arise when the code is not optimized for execution speed or resource usage.
These issues can lead to slow or resource-intensive programs, which can negatively impact the user experience or system stability. 
Examples include inefficient algorithms, unnecessary memory allocations, or redundant calculations. 
For example, when using ChatGPT to solve LeetCode Problem 1982, titled "Find Array Given Subset Sums", the compiler outputs a ``TIMEOUT" error due to inefficient loop control within the generated code. 



\find{
\textbf{Finding 3:} Issues in ChatGPT-generated code can be categorized into four categories:  Compilation \& Runtime Errors, Wrong Outputs, Code Style \& Maintainability, Performance \& Efficiency.
}

\subsection{Quantitative Analysis}
In order to gain a comprehensive understanding of the issues present in the ChatGPT-generated code, we perform a quantitative analysis on the categorized issues identified in the open card sorting discussion. 
This analysis aims to provide insights into the frequency, distribution, and nature of the issues across different difficulty levels and programming languages.
From the card sorting results, we derive rules to classify the issues in the generated code, allowing us to identify areas where ChatGPT performs well and aspects that require improvement.
It is important to note that one generated code snippet may contain multiple issues, which can further affect the analysis.
By highlighting these issues, our analysis can guide future research and development efforts to enhance the code generation capabilities of ChatGPT and similar AI models.
% \rw{should we also highlight that one generated code can have more than 1 issue in this subsection?}
% \rw{Could we maybe add explicitly that from the card sorting results we can take rules and classify the others? I think it is not clear from this part. (Feel free to not add it too)}




% \textbf{\underline{.}} 
\subsubsection{Overall Analysis}
Table~\ref{tab:rq2_issues_distribution} presents the distribution of the four issues across task difficulty levels and programming languages. 
From the table, it is evident that Compilation \& Runtime Errors and Performance \& Efficiency issues are relatively less frequent, indicating that ChatGPT is generally successful in generating syntactically correct and efficient code.
However, Wrong Output and Code Style \& Maintainability issues are more prevalent and tend to be the most common challenges faced by the generated code.
Specifically, 1,081 out of 4,066 generated code snippets (i.e., 26.6\%) exhibit wrong output, while 1,933 out of 4,066 (i.e., 47.5\%) encounter issues related to code style and maintainability.
Furthermore, as the difficulty level of the tasks increases, the prevalence of these issues also tends to rise. 
For example, 7 out of 501 generated code snippets (i.e., 1.4\%) for easy Python tasks exhibit compilation and runtime errors, while the number of execution errors increases to 46 out of 468 (i.e., 9.8\%) for hard Python tasks.
These findings indicate that ChatGPT, while powerful, has room for improvement in automated code generation to deliver more reliable and effective AI-generated code.

\input{tables/table_rq2_distribution}

\find{
\textbf{Finding 4:} Wrong Outputs and Code Style \& Maintainability issues are the most common challenges faced by the ChatGPT-generated code while Compilation \& Runtime Errors and Performance \& Efficiency issues are relatively less prevalent.
}

\input{tables/tab_rq2-1-bug_details}

\subsubsection{Analysis on Compilation \& Runtime Errors}
% \textbf{\underline{Analysis on Execution Errors.}}
Table~\ref{tab:rq2_bug_details} presents a comparison of common compilation and runtime errors error categories in Java and Python programs (i.e., 80 Python and 97 Java programs with the errors). 
From this table, we can observe that ChatGPT generates code containing a diverse range of errors across multiple categories, indicating the need for improvement in various aspects of code generation. 
Additionally, a significant portion of common compilation and runtime errors are relevant to the semantics of the generated program.
For example, these errors may contain illegal values (e.g., division by zero or invalid indices) and wrong access (e.g., concurrent modification, null references, and empty collection access). 
These observations can be explained by the probabilistic nature of the ChatGPT model, which predicts subsequent tokens based on preceding ones.  
This nature enables ChatGPT to understand the semantics of common programs that appear frequently in the training set.
However, the model captures the semantics implicitly from the training data, leading to misunderstandings of program semantics and subsequently resulting in semantically-related compilation and runtime errors. 
These findings indicate that incorporating semantic information into ChatGPT could potentially improve the quality of the generated code, indicating a promising direction for future research.
\find{
\textbf{Finding 5:} ChatGPT-generated code contains various types of execution errors, primarily due to misunderstandings of program semantics. 
}

We also notice that Illegal Index errors are quite prevalent in both languages, particularly in Java.
In fact, out of the 97 compilation and runtime errors encountered in Java, 45 of them (46.4\%) are attributed to using an invalid index. 
Meanwhile, Type Mismatch errors are more prevalent in Python than in Java, with 27 occurrences in Python compared to 6 in Java.
This observation could be due to Python's dynamic typing system, which allows for more flexibility in variable types, but can also lead to unexpected type-related issues at runtime.
Overall, these findings suggest that different languages may have distinct compilation and runtime error patterns and that improvements in code generation should take these language-specific characteristics into account. 
Additionally, the presence of various errors highlights the need for more effective debugging and error detection tools tailored to each language, ultimately leading to more robust and efficient code generation.

\find{
\textbf{Finding 6:} Different languages may have distinct compilation and runtime error patterns.
}

\input{tables/tab_rq2_python}
\input{tables/tab_rq2_java}

\subsubsection{Analysis on Code Style \& Maintainability}
% \textbf{\underline{Analysis on Code Style and Maintainability.}}
Tables~\ref{tab:rq2_python_smell} and~\ref{tab:rq2_java_smell} present the top 10 issues affecting code style and maintainability in Python and Java programs generated by ChatGPT, respectively. From these tables, we can see various types of code styles and maintainability issues in the ChatGPT-generated code. 

In Python, the top three issues are ConsiderUsingEnumerate (213 out of 2033 programs, 10.5\%), NoElseReturn (161 out of 2033 programs, 7.9\%), and UnusedVariable (103 out of 2033 programs, 5.1\%). Interestingly, 5.1\% of ChatGPT-generated code has unused variables, which is considered a bad smell in code quality. Meanwhile, MultipleVariableDeclarations, AvoidingReassigningParameters, ForLoopCanBeForEach, and RedundantModifier are the most frequent issues happening in Java code generated by ChatGPT, accounting for more than 36\% ((334+176+114+112)/2033) of the generated code. The presence of these issues indicates that the code quality of ChatGPT-generated code for both Python and Java is not perfect and could be improved. 

We further compare code style and maintainability issues in Java and Python. Our results show that there are no overlapping top-10 issues in Python and Java. The possible reason is that Python and Java have very different code styles and common practices. These results highlight the need for language-specific techniques to address the issues. Finally, by analyzing the issues detected by different static analysis tools, we can see that there is only one common issue in Python that can be detected by both Pylint and Flake8. Similarly, there is no overlap between CheckStyle and PMD. Thus, using multiple static analysis tools can provide a more comprehensive analysis of code style and maintainability in ChatGPT-generated code.

\find{
\textbf{Finding 7:} ChatGPT-generated code contains various types of code style and maintainability issues. Their common issues are specific to the language and tool being used.  
}
% \input{tables/tab_rq2_pylint}

% \textbf{\underline{Analysis on Code Style and Maintainability.}}
% We use four different static analysis tools to analyze code quality issues of ChatGPT-generated code. 
% Table~\ref{tab:rq2_pylint_smells} presents the top 10 Pylint issues affecting code style and maintainability in Python programs generated by ChatGPT. 
% From this table, we can observe different types of code style and maintainability issues.
% The top three issues are: "consider-using-enumerate" (213 out of 2033 programs, 10.5\%), "no-else-return" (161 out of 2033 programs, 7.9\%), and "unused-variable" (103 out of 2033 programs, 5.1\%).
% Interestingly, 5.1\% of ChatGPT-generated programs have unused variables, which is considered a bad smell in code quality. 
% The presence of these and other issues suggests that the code quality of ChatGPT-generated code is not perfect and could be improved.

% \yue{ adding a analysis for the whole distribution from different static analysis tools}