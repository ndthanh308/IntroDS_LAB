\section{RQ3: Repair with Prompting}
\label{sec:RQ3}

Sections~\ref{sec:RQ1} and~\ref{sec:RQ2} have demonstrated that ChatGPT is capable of generating functional code for various code generation tasks. 
However, the generated code sometimes suffers from different code quality issues, such as execution errors, wrong outputs, and maintainability problems.
Addressing these issues is vital to ensure the reliability and efficiency of the generated solutions.
Unlike traditional code generation tools, ChatGPT has the potential to learn from user interactions and refine its outputs based on the feedback it receives. 
This interactive process can lead to more accurate and high-quality code generation.
In this section, we investigate the self-debugging capabilities of ChatGPT in addressing the code quality issues identified in the generated code. 
We focus on providing effective feedback and exploring various strategies to enhance the performance of the model.
To investigate the impact of user feedback on the code quality of ChatGPT-generated solutions, we employ two types of feedback: (1) Simple Feedback and (2) Feedback with Static Analysis.

\textbf{Simple Feedback:} This type of feedback involves providing ChatGPT with basic information about the issues in the generated code. For example, if a code quality issue is detected in the code, we provide feedback to ChatGPT as follows: "\textit{The generated code has quality issues. Please provide a better code implementation as expected by the task description.}"

\textbf{Feedback with Static Analysis and Runtime Errors:} In this method, we utilize the insights from static analysis tools and runtime errors (as discussed in Section~\ref{sec:RQ2}) to offer more precise and detailed feedback to ChatGPT. Thus, we augment the simple feedback with additional information derived from static analysis reports and runtime error messages. For example, if a static analysis tool pinpoints a specific error or poor coding practice, we supply ChatGPT with feedback that directly addresses the particular issue as follows: "\textit{The generated code contains the following quality issues: + $<$details from static analysis tools$>$ + Please provide a better code implementation as expected by the task description.}"
% \david{Not sure why this prompt ... Also what + means?}

We use both types of feedback to prompt ChatGPT to refine and improve its generated code. Then, we compare the revised code with the original version to evaluate the effectiveness of the feedback in addressing the identified code quality issues.

% Figure environment removed

% \subsection{Overall Results}
Figure~\ref{fig:rq3_fix_rates} presents the fixed rates for different feedback types and code quality issues for both Java and Python. 
The fixed rate is defined as the proportion of code quality issues that were successfully addressed and resolved by ChatGPT if the issue no longer happens, measured as a percentage (i.e., $\text{Fix Rate} = \frac{\text{Number of Issues Resolved}}{\text{Total Number of Issues}} $).
Overall, Figure~\ref{fig:rq3_fix_rates} presents that ChatGPT can successfully repair from about 20\% to 60\% code quality issues itself. Particularly, ChatGPT can resolve more than 60\% code style and maintainability issues in Python code with feedback from static analysis and runtime errors while more than 60\% performance and efficiency issues in Java code can be addressed with a simple feedback.

\find{
\textbf{Finding 8:} 
ChatGPT shows great promise in self-mitigating code quality issues, achieving a fixed rate of 20\% to 60\%.
}

In our comparison of two prompt designs, we observed that feedback with static analysis and runtime errors is more effective in fixing code style and maintainability while simple feedback performs better in the remaining quality issues in both Java and Python. 
This is because feedback from static analysis tools provides detailed information about code quality issues, guiding ChatGPT to self-mitigate these problems. 
For example, static analysis tools raise a warning that 
\begin{lstlisting}
    Solution.java:12: ForLoopCanBeForeach: This for loop can be replaced by a foreach loop
\end{lstlisting}
for the initial solution in lines 1 in Code~\ref{code:code-smell}. The warning provides detailed information about the code style and maintainability issue in line 12 including location and even solution. Therefore, ChatGPT can easily mitigate the issue. 
Meanwhile, feedback with runtime errors for remaining issues, such as execution errors or performance and efficiency, tends to be less specific and more ambiguous.
For example, in most of the performance and efficiency, we only obtain a ``TIMEOUT" message, which does not reveal any details or root cause of a given issue. Similarly, for solution inaccuracies, the runtime errors also usually only contain an \texttt{AssertionError}. For example, in Code~\ref{code:incorrect_example}, ChatGPT has only received the following information from runtime errors:
\begin{lstlisting}
    AssertionError : Input : cost = [10 , _15_ ,20] Expected output : 15
\end{lstlisting}
Although the \texttt{AssertionError} points out the incorrect input-output examples, it remains abstract and does not provide precise guidance. As a result of such limited feedback, it is not surprising that ChatGPT shows lower performance in self-debugging issues. Interestingly, we found that simple feedback is more effective than static analysis feedback or runtime errors in resolving these issues. 
This is possibly due to the introduction of noise by static analysis and runtime error feedback, which can confuse ChatGPT and lead to incorrect patches.
\find{
\textbf{Finding 9:} 
Prompts with detailed feedback can effectively assist ChatGPT in self-mitigating code quality issues, whereas ambiguous feedback may have a negative impact on ChatGPT's performance.
}
% We can see that the effectiveness of the provided feedback in addressing code quality issues varies depending on the feedback type, the programming language, and the specific code issue. 
% For instance, in Java, simple feedback appears to be more effective in fixing execution errors and wrong outputs, while static feedback is better suited for addressing performance and efficiency issues, as well as code style and maintainability problems. 
% In Python, however, static feedback seems to be more effective in resolving code style and maintainability issues, while simple feedback works better for addressing execution errors, wrong outputs, and performance and efficiency issues. 
% These findings highlight the importance of tailoring feedback to the specific context and code quality issues at hand in order to maximize the efficiency of ChatGPT's self-debugging capabilities.


% Figure environment removed

While the aforementioned feedback are quite effective in guiding ChatGPT to self-mitigate its code quality issues, we still observe that new code quality issues are introduced in the fixed version. Figure~\ref{fig:rq3_new_isssues_during_fixing} presents the number of newly introduced quality issues after providing feedback for both Java and Python. 
The results demonstrate that, in some cases, addressing certain code quality issues may inadvertently introduce new issues in the generated code. 
It can be seen that providing simple feedback results in a higher number of new issues compared to static feedback and runtime errors. 
This is because simple feedback is quite general so ChatGPT is sometimes misguided, resulting in introducing new problems though plausibly solving the original issues. We observed that ChatGPT sometimes even generates a new code, which is totally different from the original version. 
\begin{lstlisting}[language=java, caption=A example of newly-introduced issues in ChatGPT-generated fix using simple feedback for LeetCode Problem 1323-maximum-69-number, label=code:incorrect_simple_fix_example]
class Solution {
    //Initial solution with issues
    public int maximum69Number (int num) {
        int[] arr = String.valueOf(num).chars().map(c -> c-'0').toArray();
        for(int i=0; i<arr.length; i++){
            if(arr[i] == 6){
                arr[i] = 9;
                break;
            }
        }
        int res = 0;
        for(int i=0; i<arr.length; i++){
            res = res*10 + arr[i];
        }
        return res;
    }
    //Fixed by simple feedback
    public int maximum69Number (int num) {
        int digits = (int) Math.log10(num) + 1;
        int firstSixDigit = digits - 1;
        int temp = num;
        while (temp > 0) {
            if (temp % 10 == 6) {
                break;
            }
            temp = temp / 10;
            firstSixDigit--;
        }
        return num + (int) Math.pow(10, firstSixDigit) * 3;
    }
    //Fixed by static analysis feedback
    public int maximum69Number (int num) {
        int[] arr = Integer.toString(num).chars().map(c -> c - '0').toArray();
        for (int i = 0; i < arr.length; i++) {
            if (arr[i] == 6) {
                arr[i] = 9;
                break;
            }
        }
        int res = 0;
        for (int digit : arr) {
            res = res * 10 + digit;
        }
        return res;
    }
}
\end{lstlisting}
For example, lines 18-30 in Code~\ref{code:incorrect_simple_fix_example} show a fix generated by ChatGPT for the initial solution in lines 3-16. Unfortunately, instead of fixing the issue, ChatGPT generated a new solution (lines 18-30), which implement an incorrect solution, resulting in failing test cases. Static feedback and runtime errors, on the other hand, provide detailed information, i.e., leading to a correct fix in line 41 which change the for-loop to foreach-loop. The results show that providing more accurate feedback about code quality issues could lead to improvement in the quality of fixed programs by ChatGPT. These findings emphasize the importance of advanced feedback mechanisms and strategies that improve ChatGPT's self-debugging capabilities by reducing the introduction of new issues while resolving existing code quality problems.
\find{
\textbf{Finding 10:} Despite being effective in self-mitigating code quality issues, ChatGPT still introduces new code quality issues in the generated fixes. More precise feedback could help mitigate the issues. 
}

% This phenomenon is observed across different feedback types, programming languages, and code quality issues. 
% In Python, providing simple feedback results in a higher number of new issues compared to static feedback. 
% This suggests that while simple feedback can be more effective in addressing some issues as shown in Figure~\ref{fig:rq3_fix_rates}, it may also introduce additional problems that were not present in the original code. 
% Similarly, we observe a comparable pattern where certain feedback types may resolve some issues while introducing new ones. 
% These findings emphasize the need for developing more sophisticated feedback mechanisms and strategies that minimize the introduction of new issues while addressing existing code quality problems, in order to enhance the overall effectiveness of ChatGPT's self-debugging capabilities.

% \find{
% \textbf{Finding 11:}  The effectiveness of ChatGPT's self-debugging depends on the feedback type, programming language, and code issues, highlighting the importance of context-aware feedback strategies. 
% }


% \subsection{Manual Analysis}
% In this section,  we manually analyze case studies to gain a better understanding about self-debugging capability of ChatGPT.




