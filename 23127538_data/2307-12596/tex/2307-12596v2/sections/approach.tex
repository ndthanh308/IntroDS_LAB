% Figure environment removed

\section{Study Setup}
\label{sec:approach}
In this section, we present the comprehensive setup of our empirical study.
We describe the research questions, illustrate the workflow of our study design, and provide an in-depth description of the benchmark dataset construction and analysis. Furthermore, we detail the characteristics of the ChatGPT model employed in this study.

\subsection{Research Design}
In this empirical study, we aim to answer the following research questions.

\vspace{1mm}

\noindent\textbf{RQ1.} \textit{How effective is ChatGPT on code generation for programming tasks?} Despite informally receiving positive feedback from the community, there is a lack of comprehensive study on the performance of ChatGPT in code generation. This research question aims to measure how well ChatGPT could generate code for programming tasks and to analyze the factors that impact its performance, including task complexity, difficulty, and time that tasks are introduced.

\noindent\textbf{RQ2.} \textit{What are the common issues in ChatGPT-generated code?}  This research question aims to analyze issues in ChatGPT-generated code using popular static analysis tools and categorize them into common categories. 


\noindent\textbf{RQ3.} \textit{Can ChatGPT fix the code quality issues with prompting?} Conversational AI models such as ChatGPT allow users to provide feedback to allow ChatGPT to revise its output. This research question aims to investigate whether ChatGPT can correct coding issues based on runtime errors, feedback from the compiler, and static analysis tools.

\vspace{1mm}

Figure~\ref{fig:overview} presents the comprehensive workflow of our study, outlining the steps taken to answer the above research questions. 
Our approach starts with a data collection stage, where we collect 2,033 programming tasks from LeetCode.
These tasks, including task descriptions, code templates, and public test cases, serve as the foundation for our research.
Subsequently, ChatGPT is prompted to generate code solutions in Java and Python for these tasks. 
The generated code is then evaluated for performance based on task-specific test cases to address RQ1. 
This evaluation allows us to assess the effectiveness of ChatGPT in code generation, considering various dimensions such as task complexity and programming language types.
For all the generated code, we also employ automated static analysis tools including PMD~\cite{copeland2005pmd} and Checkstyle~\cite{burn2003checkstyle} for Java, and Pylint~\cite{thenault2001pylint} and Flake8~\cite{cordasco2010flake8} for Python. 
These tools enable us to identify and categorize code quality issues systematically. 
Combining the static analysis results with runtime information provided by compilers, we engage in a discussion using open card sorting. 
Through classifying identified bugs and issues, this systematic approach provides comprehensive answers to RQ2.
The final stage involves the repair of code quality issues (RQ3), where ChatGPT, upon receiving targeted prompts, attempts to repair the faults. These prompts are based on feedback from both static analysis tools and runtime error messages. This stage is important in determining ChatGPT's ability to self-repair and improve the code based on conversational AI feedback mechanisms. It provides insights into the practical application of ChatGPT in real-world coding scenarios, where iterative feedback and correction play a significant role.


\subsection{Constructing Benchmark Dataset}
Existing benchmarks for evaluating AI-based code generation are often limited and outdated.
Specifically, popular benchmarks like HumanEval~\cite{chen2021evaluating} encompassing 164 Python tasks, and MBPP~\cite{austin2021program} containing 974 programming tasks, have been widely used by prior research~\cite{chen2021evaluating, chen2022codet, liu2023your, cassano2023multipl}.
However, they were released prior to 2021 and lack detailed temporal metadata for the tasks.
Therefore, such small and outdated datasets are not ideal for evaluating modern generative models like ChatGPT, since they lack diversity and may have been used in the training data of modern AI models, thus providing unrealistic performance evaluation for these models.
To address this issue, Fan~\ea~\cite{fan2022automated} introduce a new dataset, LMDefects, that contains 113 Java programming tasks released after Jun 2021. The dataset was collected from LeetCode, a well-known online platform that offers a variety of coding challenges to help programmers enhance their abilities and prepare for technical interviews. 
The dataset, however, is still relatively small and focused solely on Java programming tasks. 


% Figure environment removed

In this study, we extend LMDefects by collecting all accessible programming tasks and the relevant official public test suites in LeetCode, and investigate ChatGPT's ability in generating code in both Java and Python.
At the time of data collection (March 2023), there were 2,617 task problems available on LeetCode. 
These problems cover various topics, including data structures, algorithms, databases, and concurrency.
For our dataset, we focused on the problems that were designed specifically for Java and Python, as these two languages are widely used and have a large community of developers.
Additionally, in order to provide a fair and accessible dataset, we filtered out the premium tasks that require a subscription to access.
After this filtering process, we successfully collected 2,033 programming tasks from LeetCode.
For each task listed on LeetCode, we collected a comprehensive set of data including the task description, example test cases, constraints, and predefined code templates for both Python and Java.
Figure~\ref{fig:task_dis_time} and Figure~\ref{fig:task_accross_diff} present the distribution of tasks across time and difficulty levels classified by LeetCode.
As shown in Figure~\ref{fig:task_dis_time}, while most tasks are from before 2021, there are still more than 400 test cases for evaluating ChatGPT's code generation capabilities. 
This temporal diversity is important for a fair evaluation of the model's performance over different periods.
Figure~\ref{fig:task_accross_diff} shows that out of the 2,033 tasks in our dataset, we found that 501 were classified as easy, 1,064 as medium, and 468 as hard.



\subsection{The ChatGPT Model}
ChatGPT is a large language model that can provide detailed responses given natural language instructions. We use the model that has been fine-tuned from the GPT3.5 models~\cite{gpt35} using Reinforcement Learning from Human Feedback~\cite{stiennon2020learning}. In our study, we used the ChatGPT-March-23 version~\cite{chatgptrelease}, which was trained on data up to 2021. To instruct ChatGPT, we followed the zero-shot prompt setting which does not include examples of code generation in the prompt as illustrated in Code~\ref{fig:prompt}. To mitigate the randomness of ChatGPT, we make ChatGPT deterministic by setting the temperature to 0 and running the model once for each task, using the first generated output for evaluation.

\begin{lstlisting}[ caption=Prompt Template, label=fig:prompt]
Please provide a code implementation of the following description:
<description of a programming task>
Provide a valid <programming language> code with this template:
<solution template provided containing the input and output specifications>
\end{lstlisting}

% "\textit{Please provide a code implementation of the following description + $<$description of a programming task$>$ + Provide a valid $<$programming language$>$ code with this template $<$solution template provided containing the input and output specifications$>$}".




% % Figure environment removed

% where \texttt{<description>} is the description for a programming task including task description and a few input-output examples while \texttt{<template>} is the code template provided by LeetCode which includes the input and output specifications. An example of our prompt can be seen in Figure~\ref{fig:prompt_example}.

% \subsubsection{Code Evaluation} 
% Our evaluation of the quality of ChatGPT-generated code involves a two-stage process, which includes automated and manual evaluation. In the first stage, we test ChatGPT-generated code against test suites provided by LeetCode. If the code fails on any test cases, we consider it to have quality issues and collected runtime errors and compiler feedbacks. However, as the code may still face quality issues even if it passes all the test cases, we then conduct a manual evaluation for the remaining code. Specifically, one author manually analyze the code to identify any potential code issues, and the other authors then review the manual evaluation to ensure its quality.

% \subsubsection{Open Card Sorting Discussion}

% \subsubsection{Code Fixing}

% \subsection{Experimental Setup}

% \subsubsection{Evaluation Metrics}

% \subsubsection{Implimentation Details}




