\section{Introduction}

Since launching in November 2022, ChatGPT~\cite{chatgpt}, an AI-powered chatbot developed by OpenAI, has rapidly gained popularity. Within just two months, ChatGPT had reached 100 million unique users, surpassing even the fastest-growing social network, TikTok, in user acquisition~\cite{chatgptrising}. Due to its remarkable ability in language understanding and human-like answering, ChatGPT has shown great promise in revolutionizing various research fields, including code generation due to it being trained on extensive repositories of source code~\cite{chatgpt}. Interestingly, users without any coding experience can use the model to generate code snippets from natural language requirements. Although ChatGPT's ability on code generation tasks has been \emph{informally} receiving positive feedback from the community, there exists no study that \emph{formally} investigates the reliability and quality of code generated by ChatGPT.

Despite the great promise of ChatGPT in code generation, formally and thoroughly studying the reliability and quality of code generated by ChatGPT is becoming increasingly critical. This is due to ChatGPT now being used not only by professional developers but also by novice programmers and individuals with no coding experience. 
Code quality issues in ChatGPT-generated code, if not properly identified and addressed, may unduly affect code comprehension, introduce bugs, or create security vulnerabilities in users' projects~\cite{she2023pitfalls}. 
Consequently, the widespread adoption of ChatGPT for code generation could potentially lead to a decline in the overall quality of software systems. 
Therefore, it is crucial to examine and address the common code quality issues that may arise from using ChatGPT-generated code.

In this paper, motivated by the above challenges, we are the first to formally study the reliability and quality of ChatGPT-generated code. Our objectives are (1) to analyze the correctness of ChatGPT-generated code, (2) to identify and characterize code quality issues that may arise, and (3) to examine different prompts that leverage feedback from static analysis tools and runtime errors to guide ChatGPT in mitigating code quality issues. Through experiments addressing the following three research questions, our work provides valuable insights that help increase awareness within the community regarding code quality issues in ChatGPT-driven code generation. 
%Moreover, our initial attempt at automatically mitigating these issues could provide informative suggestions for future research.
%To achieve this goal, our study addresses the following specific research questions:
\begin{itemize}
    \item \textbf{RQ1:} \textit{\textbf{(Performance)} How effective is ChatGPT on code generation for programming tasks}?
    \item \textbf{RQ2:} \textit{\textbf{(Bugs and Issues)} What are the common issues in ChatGPT-generated code?}
    \item \textbf{RQ3:} \textit{\textbf{(Repair with Prompting)} Can ChatGPT fix the code quality issues with prompting? }
\end{itemize}

To answer these questions, we first construct a benchmark dataset containing a total of 2,033 programming tasks from LeetCode, with 501 classified as easy, 1,064 as medium, and 468 as hard. 
We then evaluate the ChatGPT-generated code for these programming tasks against LeetCode's test suite to evaluate ChatGPT's performance on code generation. Next, we employ static analysis tools including Pylint~\cite{thenault2001pylint}, Flake8~\cite{cordasco2010flake8}, PMD~\cite{copeland2005pmd}, and CheckStyle~\cite{burn2003checkstyle} to examine ChatGPT-generated code. Based on feedback from static analysis tools and runtime errors, we conduct an open card sort discussion~\cite{spencer2009card} to characterize common code quality issues including compilation and runtime errors, wrong outputs, code style and maintainability, and performance and efficiency. Finally, we attempt to mitigate the identified code quality issues by using several fixing-prompts, i.e., prompts that request ChatGPT to fix issues. To do so, we experiment with fixing-prompts with and without feedback from static analysis tools and runtime errors.

Our experimental results lead to the following findings:
(1). On various code generation tasks on Python and Java, 66\% and 69\% of Python and Java programs generated by ChatGPT are functionally-correct programs, i.e., programs that pass all given test cases. We observed that the performance is attributed to various factors such as task difficulty, the time when tasks are introduced, and program size. Especially, ChatGPT's performance drops up to five times on new programming tasks introduced after January 2022, highlighting the model's limitations in adapting to new programming tasks. 
(2). We also identified that the generated code commonly suffers from different code quality issues, such as compilation and runtime errors, wrong outputs, code style and maintainability issues.
For instance, among ChatGPT-generated code that passed the test cases, 53\% of the Java code and 37\% of the Python code exhibited code style and maintainability issues. 
This highlights the importance of addressing such problems to ensure the long-term success of AI-driven code generation. In other words, developers and users still need to take appropriate measures to improve the overall quality of the ChatGPT-generated code.
(3). Our study on ChatGPT's self-repairing capabilities revealed that ChatGPT can partially fix code quality issues in the generated code with feedback from static analysis tools and runtime errors. Moreover, the effectiveness of ChatGPT in addressing code quality issues varies depending on the feedback information, programming languages, and code quality issues.

To summarize, our paper makes the following contributions:

\begin{itemize}
    \item Conduct a comprehensive study to evaluate the reliability and quality of ChatGPT-generated code;
    \item Identify and characterize common code quality issues in ChatGPT-generated code;
    \item Introduce a new time-sensitive dataset comprising 2033 programming tasks and 4066 ChatGPT-generated code snippets implemented in two popular programming languages: Java and Python, with 2,553 code with quality issues;
    \item Conduct an exploration study on ChatGPT's self-repairing capability for code quality issues.
\end{itemize}


To support the open science initiative, we publish the studies dataset and a replication package, which is publicly available at \url{ https://github.com/yueyueL/ChatGPT-CodeGenAnalysis}


% The rest of the paper is organized as follows. Section \ref{sec:background} provides background information on Large Language Models and presents motivating examples for our study. 
% Section \ref{sec:approach} details our study design, including research questions, benchmark dataset, and ChatGPT model.
% Section~\ref{sec:RQ1}, \ref{sec:RQ2} and \ref{sec:RQ3} describe our experimental settings and findings, while Section~\ref{sec:discussion} further discusses the lessons learned from these findings and addresses the threats to validity. 
% Section~\ref{sec:relatedwork} presents the related work. 
% Finally, Section~\ref{sec:conclusion} draws conclusions and presents future work.