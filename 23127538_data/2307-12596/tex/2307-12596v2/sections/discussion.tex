\section{Discussion}
\label{sec:discussion}
% In this section, we present the lessons learned from our experiments and discuss limitations and threats to validity.


\subsection{Enhancing ChatGPT’s Code Generation and Self-Repair Capabilities}
In this subsection, we delve into strategies to potentially enhance ChatGPT's code generation and self-repair capabilities in the real-world scenario.
\subsubsection{Prompt Optimization}
Recent research highlights the crucial role of prompt engineering in enhancing the performance of large language models (LLMs) like ChatGPT for software engineering tasks~\cite{hou2023large}.
This process, which involves the careful design of specialized prompts, is a fundamental technique for improving interactions with LLMs such as ChatGPT~\cite{dong2023self}.
For example, Gao~\ea~\cite{gao2023constructing} demonstrated that incorporating additional examples into the prompt could potentially enhance performance in bug fixing tasks.
Meanwhile, Ahmed~\ea~\cite{ahmed2022few} showed that in code summarization tasks, augmenting a prompt with explicit semantic facts can significantly improve performance.
Our findings, presented in Section~\ref{sec:RQ3}, corroborate these previous studies, indicating that the effectiveness of ChatGPT in self-repairing code issues is significantly influenced by the quality and specificity of prompts.
We found that for code style and maintainability issues (where static analysis tools like Pylint provide precise guidance including location and even solution), prompts that self-repair using static and runtime information achieve better performance than simple prompts that merely identify a quality issue.
However, for errors related to wrong outputs or efficiency, simpler prompts worked better in single-round feedback, likely because the prompts collected from the compiler feedback might be ambiguous or unhelpful.
These findings suggest that highly specific and well-crafted prompts can significantly enhance the performance of LLMs like ChatGPT in software engineering contexts.
Therefore, this highlights the importance of continued research in prompt engineering to fully harness the potential of LLMs in software engineering.

\subsubsection{Iterative Interactions}
In Section~\ref{sec:RQ3}, we demonstrated that iterative repairing is effective, with code quality improving as the iteration rounds progress.
In real-world usage, interactions with ChatGPT can be iterative, whereby the user provides feedback or additional information to ChatGPT after each prompt.
Xia et al.~\cite{xia2023conversational} observed that ChatGPT’s performance in generating correct patches improves notably as the number of iterations increases, with a significant improvement observed around three iterations.
Our research further proves that feedback from detailed static analysis tools and compilers can effectively enhance ChatGPT’s code repair capability over iterative interactions.
This evidence spotlights how repeated interactions enable ChatGPT to refine its understanding, adjust based on user feedback, and converge towards more accurate solutions.
However, our results show performance stabilizing in later rounds, indicating potential upper bounds to iterative gains.
Therefore, future work should establish interaction design patterns and benchmarks to systematically advance the efficiency and efficacy of conversational code generation.



\subsection{Lessons Learned}
In this section, we highlight key lessons learned through our experiments and analysis that can drive future research in the field.

\textbf{Code quality issues are prevalent in AI-generated code:} Our study revealed that ChatGPT-generated code is prone to various code quality issues, including compilation and runtime errors, wrong outputs, and maintainability problems.
This finding emphasizes the importance of addressing these issues to ensure the long-term success of AI-driven code generation and to maintain high-quality software systems.

\textbf{Task difficulty, time that tasks are introduced, and program size impact automated code generation performance:} We found that the performance of ChatGPT on code generation tasks is significantly influenced by factors such as task difficulty, task-established time, and program size. This suggests that improvements in AI models should consider these factors to better adapt to different types of code generation tasks.

\textbf{Tailored feedback and prompt engineering are crucial for effective self-repairing and code generation quality:} Our results suggest that the effectiveness of ChatGPT's self-mitigating capabilities depends on the type of feedback provided, the programming language, and the specific code quality issue. For instance, static analysis feedback works better for code style and maintainability issues, while simple feedback is more effective for addressing execution errors and wrong outputs. This finding highlights the importance of providing tailored feedback to maximize the efficacy of ChatGPT's self-mitigating capabilities. Moreover, the quality of ChatGPT-generated code can be heavily affected by the choice of prompts. Future work could explore optimizing prompts to improve the accuracy and reliability of ChatGPT-generated code, further enhancing the overall effectiveness of AI-driven code generation models.

% \textbf{AI-driven code generation requires continuous improvement and monitoring:} As AI models like ChatGPT evolve and become more advanced, it is crucial to continuously monitor and assess the quality of the generated code. This ongoing process will help identify new issues and opportunities for improvement, ensuring the long-term success of AI-driven code generation in real-world applications.
% \subsection{Limitations}

\subsection{Threats to Validity}
Below, we discuss threats that may impact the results of our study.

\subsubsection{External validity} 
Threats to external validity concern the generalizability of our findings. 
Our study is based on a dataset of 2,033 programming tasks from LeetCode, which may not represent all possible code generation tasks encountered in real-world software development. 
Additionally, we focus on Java and Python, two popular programming languages; however, our findings may not be directly applicable to other programming languages. 
To mitigate these threats, future work could expand the dataset by incorporating tasks from various sources and diverse programming languages, and by considering different types of software projects, such as web applications, mobile apps, and embedded systems.


\subsubsection{Internal validity}
Threats to internal validity refer to possible errors in our experiments. One such threat relates to bugs happening in our code. To mitigate this risk, we have carefully checked our code and made our code publicly available~\cite{replication}. Another possible threat may be introduced from our manual analysis and categorization. To eliminate the potential bias, we conducted a sorting discussion among three annotators. We also release our analysis and categorization results for public verification.   
In addition, to minimize the non-deterministic nature of ChatGPT, we set the temperature parameter to 0 in our experiments. This approach ensures that ChatGPT produces consistent outputs for the same input, thereby reducing variability and enhancing the internal validity of our results.

\subsubsection{Construct validity} 
Threats to construct validity relate to the suitability of our evaluation. In our study, we use the pass@1 metric, in which a program is considered as functionally correct if it passes all the test cases. A possible threat arises from the incompleteness of the test suite, which could potentially result in missed program bugs. In our experiments, we use the original test suite from LeetCode, which is carefully designed and widely recognized. Thus, we believe this risk is minimal. Another potential threat to construct validity comes from the variability in ChatGPT-generated code due to different prompts. To address this concern, we followed the similar methodology used by Fan~\ea~\cite{fan2022automated} and Tian~\ea~\cite{tian2023chatgpt}, which ensures that our results are reliable by using a consistent set of prompts across different tasks. However, it is important to note that prompt engineering can significantly influence the quality of the generated code. Future work could focus on optimizing the prompts to improve the accuracy and reliability of ChatGPT-generated code, thus enhancing the overall effectiveness of AI-driven code generation models.

% \kla{suggest to mention that different prompts may produce different code. this is one of the threats, but what have we done to ensure that the results are reliable? Probably, say we follow Fan et al [11]? Then, future work can focus on the prompt engineering may produce more accurate results, etc..}\yue{Added}

%\rw{maybe we can also mention that the ChatGPT evolves rapidly and we use the latest one?}