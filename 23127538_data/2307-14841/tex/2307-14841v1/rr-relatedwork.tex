Many projects developed in code hosting platforms are public, thus allowing anyone to explore their activity, which include access to commits, issues, pull requests and comments, among others.
This large amount of public data has enabled researchers to easily collect and analyze such data.
As a result, many empirical studies have been conducted in the last years, in particular, mostly relying on the \gh platform~\cite{DBLP:conf/msr/DemeyerMWL13,DBLP:journals/access/CosentinoIC17,MSRSampling}.
 
However, the potential perils of empirical studies on this public software data are also relevant~\cite{DBLP:conf/msr/KalliamvakouGBSGD14,DBLP:conf/msr/HowisonC04,DBLP:journals/ese/KalliamvakouGBS16,DBLP:journals/ese/FlintC022}.
Perils could involve the quality of the project's data, the scarce use of the platform's features or the purpose of the project, etc.
This situation may affect to the quality of empirical studies, but also may raise concerns about the replicability of the results~\cite{DBLP:conf/msr/Robles10}.

In the last years, the number of ML-based projects has been increasing and motivated the SE community to understand the differences to traditional software development~\cite{DBLP:conf/msr/Gonzalez0N20}.
Also, platforms such as HFH have appeared to facilitate the development of ML-based projects and host their projects, models and data. Therefore, HFH could be regarded as a key platform for new studies covering the new breed of ML-based projects.
However, as any other code hosting platform, empirical studies on HFH could also bring its own set of perils and promises.
To the best of our knowledge, no study has been conducted to evaluate the quality and quantity of the data available in HFH, and the potential of the platform to be used in empirical studies. 
Starting this discussion is the purpose of this report.