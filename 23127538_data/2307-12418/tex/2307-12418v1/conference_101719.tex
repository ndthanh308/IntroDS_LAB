\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\title{Testing Hateful Speeches against Policies
}

\author{Jiangrui Zheng, Xueqing Liu, Girish Budhrani, Wei Yang, Ravishka Rathnasuriya \\
Stevens Institute of Technology \\
jzheng36,xliu127,gbudhran@stevens.edu\\ ravishka.rathnasuriya,wei.yang@utdallas.edu }

\thispagestyle{plain}
\pagestyle{plain}
  
\begin{document}



\maketitle


\begin{abstract}

In the recent years, many software systems have adopted AI techniques, especially deep learning techniques. Due to their black-box nature, AI-based systems brought challenges to traceability, because AI system behaviors are based on models and data, whereas the requirements or policies are rules in the form of natural or programming language. To the best of our knowledge, there is a limited amount of studies on how AI and deep neural network-based systems behave against rule-based requirements/policies. 

This experience paper examines deep neural network behaviors against rule-based requirements described in natural language policies. In particular, we focus on a case study to check AI-based content moderation software against content moderation policies. First, using crowdsourcing, we collect natural language test cases which match each moderation policy, we name this dataset HateModerate; second, using the test cases in HateModerate, we test the failure rates of state-of-the-art hate speech detection software, and we find that these models have high failure rates for certain policies; finally, since manual labeling is costly, we further proposed an automated approach to augument HateModerate by finetuning OpenAI's large language models to automatically match new examples to policies. The dataset and code of this work can be found on our anonymous website: \url{https://sites.google.com/view/content-moderation-project}.


\end{abstract}

\begin{IEEEkeywords}
software requirements, traceability, artificial intelligence, software testing
\end{IEEEkeywords}

\section{Introduction}

% Figure environment removed

With the rapid development of AI technology, major software companies have built AI systems to serve users across a wide-range of real-world applications, e.g., self-driving car and social media content moderation~\cite{facebook_moderation_report}. Among these applications, deep neural networks are often used as the workhorse behind the scenes. This change has brought challenges to software requirements management and traceability~\cite{barzamini2022cade,al2022resam,abualhaija2022automated}. As deep neural networks are known as black boxes whose behaviors mostly depend on the training data and the model, whereas requirements are formal rule-based specifications, there exists a natural gap between the results returned by these systems and the software requirements. For example, in the medical domain, a system for cancer tumor recognition must successfully detect cancerous tumors while excluding benign tumors~\cite{barzamini2022cade}. To evaluate AI-based software systems on requirements, one often build test collections which cover different cases satisfying the requirement~\cite{pei2017deepxplore,barzamini2022cade}. 

% Figure environment removed

In the recent years, AI regulations have been a hot topic and many software companies have published regulation policies of how AI software should behave, driven by law enforcement~\cite{eu_gdpr,eu_digital_services_act,section_230} or to improve user experience. For example, in the area of online hateful content moderation (which is largely operated by AI now~\cite{facebook_moderation_report}), many social media companies released their policies on what user-generated content must be removed~\cite{fb_standard,ig_standard,yt_standard}. Figure~\ref{fig:intro} shows one policy in Facebook's community standards guidelines on Nov 23, 2022, which contains three tiers of 41 policies in total. 



Despite these progress, there is a lack of study on how deep neural network-based AI systems are consistent with these policies. For the same example of hate speech moderation, although deep neural network-based hate speech detectors achieved impressive overall score\footnote{For example, Facebook reported in Q4 of 2020 that their automated hate speech detectors achieved a recall of 97.1\%~\cite{facebook_moderation_report}.}, they may not be able to detect the hate speech specified by an individual policy, as neural networks often suffer from training bias~\cite{poliak2018hypothesis} and have difficulty generalizing to unseen examples~\cite{ribeiro2020beyond}. 



To gain deeper understanding of this problem, this experience paper focuses on a case study of social media content moderation, which now largely relies on AI-based systems~\cite{facebook_moderation_report}. In particular, we use a list of 41 content moderation policies by Facebook published on Nov 23, 2022~\cite{fb_standard} and examine the behaviors of state-of-the-art AI-based content moderation software (i.e., Google's Perspective API~\cite{perspective_api}, Facebook's RoBERTa model finetuned on hate speech datasets~\cite{fb_moderation_api}, and OpenAI's Moderation API~\cite{openai_moderation}). It is important to note that although we focus on Facebook's policy only, our framework is general enough to be extended to any policies on content moderation specified by natural language requirements. 

In this experience paper, we are driven by the following research questions:

\textbf{RQ1: How to examine deep neural network-based systems against natural language policies?} 

To answer this question, for the first step, we explore crowdsourcing to build test suites for each policy. More exactly, we hire annotators to manually extract a dataset named HateModerate of hate speech organized as 41 policies. In this step, we provide the annotators with 8 existing hate speech datasets~\cite{vidgen2020learning,pavlopoulos-etal-2021-semeval,davidson2017automated,rottger2020hatecheck,twitter_hate,mollas2020ethos,ljubevsic2019frenk,ziems2020racism}. We first assign the 41 policies to 7 annotators, and ask them to find (as many as possible) hate speech examples from the 8 datasets matching each policy. After extracting the initial dataset, we hire 2 additional annotators to examine whether each example indeed matches the specification of the corresponding policy, and remove the ones which were falsely matched. 

\textbf{RQ2: How do deep neural network-based systems perform against natural language policies?} 

To answer this question, for the second step, we use the hate speech examples extracted in the first step to test the accuracies of the content moderation API to be tested. We find that the failure rates are high, especially in OpenAI's Moderation API, which is popular among the machine learning communities~\cite{alpaca}. Both Google and Facebook's APIs fail to detect more implicit hate speeches. 

Since manually labeling the examples require huge costs, we ask the third research question:

\textbf{RQ3: What is the promise for automatically matching test cases to policy requirements?} 

To answer this question, for the third step, we finetune OpenAI's large language model (LLM) to automatically match any hate speech sentence to one of the policies, given the complete description of the policy. That is, after seeing the sentence "\emph{Traaans should be killed.}" and 41 policies including "\emph{Do not post: Violent speech or support in written or visual form}", the LLM can automatically match the sentence to one of the 41 policies. The accuracy of the automatic matching is 78.4\%, we further discuss ways to improve this accuracy.

% Our contributions of this work are of the following folds:

% \begin{itemize}
%     \item We propose a study to examine the behavior of deep neural network-based content moderation software against moderation policies, to the best of our knowledge, there is a lack of literature on this topic;
%     \item Using crowdsourcing, we collect a dataset of hate speech examples matching each policy, the dataset can be used to evaluate AI-based moderation software;
%     \item We further explore automatically matching a hate speech sentence to a any content moderation policy in natural language, by exploring different hyperparameters, we achieve a 78.4\% accuracy;
% \end{itemize}

We hope our study in this experience paper can bring more attention to testing the behaviors of AI-based software systems against rule-based policy requirements. 

The rest of the paper is organized as follows. Section~\ref{sec:relwork} summarizes the background and related work; Section~\ref{sec:step1} describes the process of manual extraction of the test collection; Section~\ref{sec:step2} presents the study results of using our test collection to test content moderation software; Section~\ref{sec:step3} explores the automated matching; Section~\ref{sec:limitation} discusses the limitation of this work; finally, Section~\ref{sec:conclusion} draws conclusions and Section~\ref{sec:future} discusses the future work. 

\section{Background on Content Moderation}

Today's social media companies are the de-facto parties for regulating unwanted contents online such as hateful speeches, yet such process is a double-edged sword. On the one hand, moderation can help improve the quality of content and protect users from unwanted harms. On the other hand, the potential risk of excessive-censorship may hurt user satisfaction and freedom of speech~\cite{excessive_censorship_harm}. To make the moderation process more transparent, many social media companies released policies for what content cannot be published on their platforms, e.g., Facebook~\cite{fb_standard}, Instagram~\cite{ig_standard}, and YouTube~\cite{yt_standard}. These unwanted contents include hate speech, violence, intellectual property violation, etc. In particular, as of Nov 23, 2022, Facebook has published 41 policies of hate speeches that cannot be posted~\cite{fb_standard}. The 41 policies are classified as 4 Tiers: Tier 1 (14 policies) includes the most hateful content, e.g., dehumanization and violence; Tier 2 (19 policies), Tier 3 (5 policies), and Tier 4 (3 policies) are milder speeches, e.g., stereotyping and contempt. Each policy is explained in abstractive level English description, sometimes with exemplifications. An example of Facebook's community standards guidelines is shown in Figure~\ref{fig:intro}, which is a Tier 1 criterion that specifies that violent speeches cannot be posted on Facebook. 

\section{Step I: Manually Matching Hate Speech to Policies with Crowdsourcing}
\label{sec:step1}

In this first step, we answer the first research question:

\textbf{RQ1: How to examine deep neural network-based systems against natural language policies?} 

To answer this question, we explore using crowdsourcing to manually label a dataset named HateModerate, which are test cases that match each policy in Facebook's community standards guidelines. 

\subsection{Data Collection Process}
\label{sec:collection}



\noindent \textbf{Human Annotators}. HateModerate was annotated by a group of 9 master students. The student annotators were either in first year (8) or second year (1) of master majored in Computer Science. All annotators had bachelor's degrees and were fluent English speakers. By the time of the annotation, all annotators had taken courses in natural language processing, therefore they understood the fundamentals of machine learning and how data annotation works. The team includes 9 males, 5 were from Indian and 4 were from China. The annotation process took approximate 4 weeks. Compensations were awarded in the format of gift cards upon successful completion of the annotation tasks. 

\noindent \textbf{Data Collection Method}. Starting from a set of natural language policies, there are three methods to add new example that match each policy. First, we can reuse examples from existing datasets. Second, we can ask human annotators to come up with new test cases. Third, we can leverage natural language generation techniques to generate synthetic examples. Since the second approach often require domain expertise while there are quality and diversity concern with the third approach, we mostly focused on the first approach. That is, we provided the policies and existing datasets to human annotators, and ask them to find example matching each policy from these datasets. 

\noindent \textbf{Data Sources}. The datasets we search from include the following: DynaHate~\cite{vidgen2020learning}, Toxic Spans\cite{pavlopoulos-etal-2021-semeval}, Hate Offensive~\cite{davidson2017automated}, HateCheck~\cite{rottger2020hatecheck}, Twitter Hate Speech~\cite{twitter_hate}, Ethos~\cite{mollas2020ethos}, FRENK~\cite{ljubevsic2019frenk}, and COVID Hate and Counter Speech~\cite{ziems2020racism}. The hate/nonhate labels are available in all datasets. None of the datasets contain existing links to the policies. 



\noindent\textbf{Step 1.1: Matching Examples to Policies}. In this step, each policy was assigned to an annotator. The annotation leveraged a Google spreadsheet (The interface used in Step 1.1 can be found in Figure~\ref{fig:interface}), every tab corresponded to 1 of the 41 policies. During the annotation, the description of each policy was displayed on top of each tab. 7 annotators participated, 6 annotators were assigned 6 policies and 1 annotator was assigned 5 policies. 

For each policy, each annotator was instructed to use keywords to find as many hateful examples as possible (at least 200) to match the description of that policy: they should first search within the 8 existing datasets, and if they could not find enough examples, they were allowed to augment the examples by either manually creating one example matching that policy, or querying chatGPT/GPT-3 to generate synthetic examples. During this process, the annotators reported that synonyms were frequently used, and regular expressions were used to improve the search efficiency. For example, for the Tier 1 policy on filth, the annotator used the regular expression "\emph{.*(filth$\vert$dirt).*}". Multiple annotators also reported that they needed to seek help from Google and other team members to correctly understand the policy. For example, one policy in Tier 1 was on hate crime: "\emph{Advocating, indicating, or expressing support for hate crimes against marginalized groups}". The annotator assigned with this policy first struggled to find enough examples because the exact word "\emph{hate crime}" rarely appeared in any example, after a discussion, the team members suggested him to search for concrete hate crime keywords including \emph{lynching} and \emph{holocaust}. He was able to add more examples as a result. Google and chatGPT were also frequently used to help with this process. 

\noindent \textbf{Problem with the Initial Matching}. After the initial search process, 6,939 examples were collected. However, not all examples matched the corresponding policy. The main reasons are the following. First, if an annotator only searched by keywords but did not carefully check the sentence, they could easily introduce false matches. Second, the annotator may have a different understanding of the criterion of the policy. For example, one policy in Tier 2 is on curse involving sexual activities : "\emph{Terms or phrases calling for engagement in sexual activity, or contact with genitalia, anus, feces or urine, including but not limited to: **** my ****, kiss my ***, eat ****.}". The sentences initially found by the annotator only contained the curse words themselves but were not calling for sexual activities, e.g., "\emph{They make me so pissed off these immigs!!}". As a result, the sentence was a false match. Third, when two policies are close, it is easy to confuse between policies. For example, one policy in Tier 1 is on denying existence: "\emph{Statements denying existence (including but not limited to: "[protected characteristic(s) or quasi-protected characteristic] do not exist}" which is close to another policy in Tier 2: "\emph{Expressions that a protected characteristic shouldn't exist.}". The team discussed and resolved these confusing policies. 


\noindent\textbf{Step 1.2: Validating Correctness}. Due to the problems with the initial matching, we include a second round of labeling where we asked annotators to remove invalid examples from Step 1.2. The annotators were asked to examine whether each example matched the policy description, and label it as 1 (valid) and 0 (invalid). During this step, we provided a few examples of falsely matched examples for each policy, e.g., for the aforementioned Tier 2 policy on curse involving sexual activities, we showed the annotator that "\emph{They make me so pissed off these immigs!!}" was a false match because it did not involve sexual activities. The agreement rate between the two annotators was 87\%. The interface and details of Step 1.2 can be found in Section~\ref{sec:collection}, Figure~\ref{fig:interface}. 

% Figure environment removed

Step 1.2 was critical to improving the quality of HateModerate. We removed all the examples for which at least 1 annotator labeled it as invalid. We further removed all examples which were non-hateful. That is, our dataset only contains hateful examples. 


\noindent \textbf{Step 1.3}. After Step 1.2, about 5,000 examples remained, and some policies contained too few examples. To augment these policies, 1 additional annotator was asked to add more examples until no other examples could be found. 



\subsection{Dataset Statistics}



Our final HateModerate dataset includes 5,430 hateful examples from the following 8 datasets: DynaHate (3,450), HateCheck (452), Toxic Span (134), GPT (809), manual (260), COVID hate (160), Hate Offensive (91), Ethos (11), Twitter Hate (36), and FRENK (27). 

Figure~\ref{fig:stats} shows the statistics of HateModerate. Among the 41 policies, the most frequent policy contains 289 examples whereas the least frequent policy contains 31 examples, most policies contain 50 to 200 examples, and the majority policies contain more than 100 examples. 



% \subsection{Comparison with Existing Hate Speech Datasets}

% Our dataset is the first hate speech dataset based on the requirements specified by content moderation policies. Each example in our dataset is associated with 1 out of 41 fine-grained hate speech labels. In contrast, existing work has only provided coarser-grained hate speech labels or no labels at all regarding what type of hate speech. One dataset with coarser grained labels is DynaHate~\cite{vidgen2020learning}, where each example is associated with 1 out of 5 hate labels including dehumanization, threatening and derogation. Other datasets such as HateCheck~\cite{rottger2020hatecheck} has provided 1 out of 7 labels for protected group such as women and Muslims. 

\subsection{Data Format}

HateModerate can be found on \url{https://sites.google.com/view/content-moderation-project}. We explain the format of the dataset below. 

% Figure environment removed

Our dataset consists of two files:

% Figure environment removed

\begin{itemize}
    \item \texttt{all\_examples.csv}: the file which stores all the hate speech examples in our dataset. Every row of this file contains 5 columns: \texttt{example\_id}: the example index in HateModerate; \texttt{sentence}: the hate speech example; \texttt{dataset}: the original dataset this example is from (i.e., 1 of the 8 datasets as in Section~\ref{sec:collection}); \texttt{index}: the index of the example in the original dataset; and \texttt{category}: the short name of each policy category. For example, "\emph{Advocating, indicating, or expressing support for hate crimes against marginalized groups}" is abbreviated as "\emph{hate crime}". 
    \item \texttt{cate2policies.csv}: the file containing the full description of each policy category. Each row of this file contains 3 columns: \texttt{tier}: the Tier of this policy (1,2,3, or 4); \texttt{category}: the category of the policy; and \texttt{description}: the full description of the policy as in Facebook's page for the community standards policys~\cite{fb_standard}.
\end{itemize}



The screenshot of our dataset can be found in Figure~\ref{fig:dataset}. 

\subsection{The Interface for Annotation}
\label{sec:annotate_interface}



Our annotation leverages Google spreadsheets (Section~\ref{sec:collection}). Initially, every annotator was assigned with a tab in the spreadsheet. A screenshot of the tab can be found in Figure~\ref{fig:interface}. The format of every spreadsheet tabs were kept in the same format to facilitate data processing. From the beginning of the annotation process, the complete description of the policy was displayed in cell B3. In Step 1.1, every of 7 annotators was assigned with 5-6 tabs, their names were registered in cell A1. 

In Step 1.2, for each tab, 2 annotators inspected the tab to remove invalid examples. First, 1 annotator inspected the tab without examples, but the quality of the initial inspection was low since the annotators failed to identify many falsely matched examples. To improve the qualify of the data, we repeated Step 1.2 by showing a few examples of falsely matched examples for each policy (Figure~\ref{fig:app2}), red means the example was falsely matched). Using examples, more falsely matched examples were identified. We further checked the validity by randomly sampling a small set of examples for each policy and confirmed the falsely matched examples were rare. In Step 1.2, we enforced the spreadsheet to be read-only, so we could avoid the risk for annotators to copy others' labels. 

\noindent \textbf{Finding Summary of RQ1}. Given a list of natural language policies, we find that we can build a set of high quality test cases with non expert annotators using a two-step process of keywords search in existing datasets and validation. 

\section{Step 2: Examining AI-based Content Moderation Software against Policies}
\label{sec:step2}

% Figure environment removed

In this section, we answer the second research question:

\textbf{RQ2: How do deep neural network-based systems perform against natural language policies?} 

To answer this question, we use HateModerate for evaluating automated content moderation models and observe how they perform for each policy. Since all examples in HateModerate are hateful, we run a model and observe the failure rates, i.e., the percentage of hateful speech not detected by the model. If the model fails in the majority of cases for a policy, it indicates the model's performance fail to meet the requirement of the community standards guideline. 

We apply HateModerate on three existing APIs for hate speech detection:

\begin{itemize}
    \item Facebook's RoBERTa model fine-tuned on the DynaHate dataset's Round 4 (Section~\ref{sec:relwork}): We evaluate Facebook's RoBERTa model fine-tuned on the DynaHate dataset~\cite{fb_moderation_api}. This model was released by Facebook's official account on the HuggingFace model hub\footnote{The HuggingFace Hub is a platform for model developers to publish models and datasets for training models using the Transformers library: \url{https://huggingface.co/}}, it has 34,345 downloads as of April of 2023;
    \item Google's Perspective API: We evaluate Google's Perspective API~\cite{perspective_api}, which is a popular model for content moderation. The Perspective API has the longest history and is frequently evaluated in existing work~\cite{rottger2020hatecheck};
    \item OpenAI's Moderation API: We also evaluate OpenAI's Moderation API~\cite{openai_moderation,markov2022holistic}, which is based on the GPT model and it is a more recent API for content moderation~\cite{markov2022holistic}. Recently, the Moderation API became popular among the machine learning community~\cite{alpaca};
\end{itemize}



\subsection{Overall Analysis}

We evaluate the three API's performances on the 41 policies and report their failure rates in Figure~\ref{fig:app1}. From Figure~\ref{fig:app1} we can make the following observations. First, failure rates of Facebook's model are much lower than the other two models, which is somewhat expected, as the Facebook model was finetuned on the DynaHate dataset, and the majority of HateModerate come from the DynaHate dataset. Second, Perspective and Moderation both fail in multiple policies. Perspective has an overall lower failure rate (avg: 0.28, std: 0.25) than Moderation (avg: 0.37, std: 0.19). The t-test score between Perspective and Moderation is 0.64, p-value = 5.5e-6. Therefore, the OpenAI model has the worst performance. For the OpenAI API, the model we tested was moderation-latest. OpenAI has another content moderation model moderation-stable. However, our experiments show that its performance was even worse than moderation-latest. 

To remove the impacts by the DynaHate dataset, we further test Facebook's model on test cases by excluding the DynaHate dataset, the results are shown in Figure~\ref{fig:app1}. After excluding DynaHate, some policies do not contain enough cases, we always plot 0 failure rate and use a star to denote these policies. Among the remaining policies, we can see that the failure rates increased. However, even after excluding DynaHate, Facebook's model is still more consistent to the policies than Perspective and Moderation. 

\subsection{Analyzing Failed Policies}

The highest failure rates are detected for the following policies:

% Figure environment removed

\begin{itemize}
\item Tier 1: "\emph{Dehumanizing speech as certain objects (women as household objects or property or objects in general; Black people as farm equipment; transgender or non-binary people as “it”)}": We find that Moderation has a 81\% failure rate in this policy. In particular, Moderation fails to detect hate speeches such as "\emph{MAKE WOMEN PROPERTY AGAIN!!!!}". 

\item Tier 2: "\emph{Expressions of contempt, self-admission to intolerance on the basis of a protected characteristics}". Perspective has a 83\% failure rate, e.g., Perspective fail to detect "\emph{I am racist because I'm a white person grown up in white American systems.}". 

\item Tier 3: "\emph{Economic exclusion, which means denying access to economic entitlements and limiting participation in the labour market}". We find that both Perspective and Moderation fail in the majority of this policy, e.g., "\emph{Pregnant women should not get any promotion at work as they will end up in maternity leave}". 

\item Tier 4: "\emph{Content explicitly providing or offering to provide products or services that aim to change people’s sexual orientation or gender identity}". We find that both Perspective and Moderation fail in the majority of this policy, e.g., "\emph{With the right therapy and support gays can be cured and live much more fulfilling lives}".

\end{itemize}



\subsection{Comparative Studies} 

Among the three tiers, the two models perform the best in Tier 1 (avg failure rate = 0.28), then in Tier 2 (avg failure rate = 0.35), and the worst in Tier 3 (avg failure rate = 0.53) and Tier 4 (avg failure rate = 0.54). This result also meets our expectation, since Tier 1 hate speech are considered more hateful than the other two tiers. 

By comparing the different policies, we further observe that Perspective has a very low failure rate for categories involving profane and curse words (e.g., Tier 2 "\emph{Profane terms or phrases with the intent to insult, including but not limited to: ****, ****, mother******})". While this result seems to show that Perspective can effectively detect hate speech in these categories, it may be associated with the fact that Perspective tends to be oversensitive to profane words~\cite{rottger2020hatecheck}. Meanwhile, hate speeches with the more implicit policies remain difficult to detect. 


\noindent\textbf{Finding Summary of RQ2}. By examining state-of-the-art AI-based models for content moderation, we find that even widely used models by big companies may have high failure rates on test cases for certain policies. Among three companies (i.e., Google, Facebook, OpenAI), we identify that Facebook's model has the lowest failure rate for the tested policy, such conclusion stays true even after it is tested on the cases excluding its training data. The OpenAI model has the highest failure rates. Despite this model has grown more and more popular, it failed to detect examples which are clearly hateful. 

\section{Step 3: Automatically Matching Examples to Policies}

\label{sec:step3}

\begin{table}[h]
\centering
\caption{The test accuracies of Step 3: automatically matching examples to policies.\label{tab:app2_results}}
\resizebox{\columnwidth}{!}{
% \begin{tabular}{cccc}
% \hline
% Model & I/O data-format & train-10 & train-all \\ \hline
% ada & short-cate & \textbf{0.512} & \textbf{0.887}\\
% ada & short-desc & 0.217 & 0.305 \\
% ada & long-cate & 0.117 & 0.690\\
% davinci& short-cate & 0.504 & 0.701\\
% davinci & short-desc & 0.158 & 0.214\\
% davinci & long-cate & 0.056 & 0.057\\\hline

% \end{tabular}
% \skip
\begin{tabular}{ccc}
\hline
Model & Output data-format  & Accuracy \\\hline
ada & label  & 0.733$\pm$0.002 \\
ada & label + description & 0.776$\pm$0.005 \\
curie& label  & 0.728$\pm$0.037  \\
curie & label + description  & \textbf{0.784$\pm$0.005} \\\hline

\end{tabular}
}
% \skip
% \skip
% \resizebox{1.8\columnwidth}{!}{\begin{tabular}{ccc}
% \hline
%  & Input & Output\\ \hline
% label & "\emph{[sentence]}" & "\emph{[short label]}"\\ 
% label + description & "\emph{[sentence]}"  & "\emph{[short label] : [long description]}"\\\hline
% \end{tabular}
\end{table}

Motivated by saving the manual labeling cost for collecting the HateModerate dataset, in this section, we answer the following research question:

\textbf{RQ3: What is the promise for automatically matching test cases to policy requirements?} 

To answer this question, we propose Step 3 in our study, which explores the performance of automatically matching hate speech examples to policies, given the natural language description of each policy. Notice this problem is different from a regular 41-class classification problem, as in the latter problem, the only available information is the \emph{categorical} class label, whereas HateModerate contains textual descriptions beyond the class labels to represent each policy: the original long description of the policy, e.g., "\emph{Certain objects (women as household objects or property or objects in general; Black people as farm equipment; transgender or non-binary people as 'it')}", as well as a short class label, e.g., "\emph{certain objects}". With more information, we can provide more contexts to the classifier to help with the prediction, especially for cases where the training data is limited. 

\subsection{Model Setup} As described above, we should leverage the rich textual information in the policy description, to leverage such information, we propose to use OpenAI models to generate natural language policies, which are then used for matching the class instead of applying the regular classifier directly~\cite{openai_finetune}. The OpenAI's model casts all classification problems as a text generation problem, therefore instead of predicting "0" and "1" labels, it predicts a natural language sentence as the output. For example, if we want the model to predict the policy "\emph{Violence speech or support in written and visual form.}" for "\emph{Traaans should be killed.}", we can finetune the model with training data where the inputs are violent speeches, and the outputs are also sentences which are copies of this policy. Among all models for finetuning OpenAI, we choose to use the Ada (350M parameters) and the Curie model (6.7B parameters)~\cite{openai_models}. The former is OpenAI's smallest model, whereas the latter is its largest low-cost model. Generally speaking, the larger the model size is, the better performance. We fix the temperature to 0 for all predictions since it generally gives better performance for classification tasks~\cite{liang2022holistic}. 


\subsection{Dataset Setup} 



To answer RQ3, we divide the 5,430 examples in HateModerate into training and testing folds. We randomly shuffle the examples and leave 50\% (2,715 examples) for testing and the others for training. For each record in the finetuning dataset, the input is the original sentence in HateModerate, the output can be one of two options: the first option contains only the short description (avg 5 words) of the policy, e.g, "\emph{certain objects}", whereas the second option concatenates the short description with the first 10 words of the long description of the policy, e.g., "\emph{certain objects: Dehumanizing speech as certain objects (women as household objects or}". The reason we prune the long description to 10 words is to keep different policies to about the same length to make the prediction easier. During prediction, we match the predicted sequence with the closest short description using the difflib library in Python~\cite{difflib}.

% Figure environment removed

\subsection{Hyperparameters Setup} 

We explore different hyperparameters to understand the performance of automated matching. These hyperparameters follow OpenAI's recommendation for finetuning models~\cite{openai_finetune}: first, we vary the learning rate multiplier between 0.1 and 0.2; second, we vary the batch size between 4 and 6, as OpenAI suggests the batch size to be 0.2\% times the training data size (2715); third, we vary the number of epochs between 4 (the default one) and 5; finally, we vary the output format between using the short description only, or the short + long description. 



\subsection{Overall Analysis} 

We run the hyperparameter swipe following the settings above, and report the overall best score in Table~\ref{tab:app2_results}. We can see that the classification accuracies are all above 0.7; the best score of curie is better than Ada, although Curie performs slightly worse than Ada using the short label only; and adding the description to the output helps improve the accuracy by a marging of 4.3\%$\sim$5.6\%. 

\subsection{Analysis of Individual Hyperparameters}



To understand the impacts of individual hyperparameter, in Figure~\ref{fig:hp}, we show bar plots by varying different hyperparameters. From left to right of Figure~\ref{fig:hp}, we vary the description setting, number of epochs, learning rate multiplier, and batch size. We can observe that adding description generally contributes to a better performance, the improvement is more obvious for the larger sized curie; a larger number of epochs does not always improve the performance for ada but is helpful for curie; for learning rate, curie also benefit from the larger learning rate; finally, the batch size 4 works better than batch size 6. 

\subsection{Discussion on Improvement}

In Section~\ref{sec:erroranalysis}, we analysis the errors of the automated matching. We find that some policies are easily confused between each other, which is an important cause for the lower accuracy. One potential way to improve the score is to add more boundary examples for these policies. Another potential direction is to keep increasing the length of long description included in the output. 



\subsection{Error Analysis}
\label{sec:erroranalysis}

To further analyze the errors, we take a closer look at the confusion matrix of fine-tuning using ada with short description in Figure~\ref{fig:app2} (learning rate multiplier = 0.1, batch size = 4, number of epochs = 4). The confusion matrix shows the number of error cases which belong to one policy but are falsely matched to another policy. By observing this matrix, we find that the classification error often happens between classes that are similar. For example, the Tier 1 policy "\emph{Dehumanizing speech: Feces (including but not limited to: s**t, crap)}" is often confused with the Tier 2 policy "\emph{Profane terms or phrases with the intent to insult, including but not limited to: f**k, b***h, m**********r.}". Such results meet our expectation since even for human it is more difficult to distinguish between the two policies.  By observing the confusion matrix, we believe the results can be improved by including more examples for similar classes. 

\noindent \textbf{Finding Summary of RQ3}. By finetuning OpenAI's large language model (including curie and ada) on HateModerate dataset, we find that the accuracy of automatically matching a test case to policies is around 78.4\%; the performance is improved by using larger model, including more information in the output, and carefully tuning hyperparameters. 

\section{Related Work}
\label{sec:relwork}

In this section, we review the related work. 

\noindent\textbf{Requirements for AI-based Systems}. As AI-based systems become prevalent, recent work has focused on eliciting requirements for these systems~\cite{heyn2021requirement,vogelsang2019requirements,al2022resam,barzamini2022cade}. For example, Vogelsang and Borg interviewed data scientists for how they handled requirements in their work, including data, privacy requirements and explainability~\cite{vogelsang2019requirements}. Heyn et al. posed several challenge research questions on studying the requirements of AI-based systems~\cite{heyn2021requirement}, including data quality, performance metrics and human factors. \cite{al2022resam} elicited requirements for deep-learning anomaly models with applications to UAV flight controllers. In the area of natural language processing, an open-source tool called CheckList developed functional requirements for three NLP applications: sentiment classification, paraphrase detection, and question answering~\cite{ribeiro2020beyond}. For each application, they proposed rule-based specification in the format of natural language templates. For example, one requirement description for sentiment classification models is "\emph{negated negative sentences}", which tests whether the model~\cite{ribeiro2020beyond} understands negations. The test checks whether the model always outputs the positive label for the following input: "\emph{The food is not [NEGATIVE\_WORD].}". They examined the sentiment classification APIs from companies such as Google and Microsoft as well as state-of-the-art open source models. They concluded that even though these models achieved more than 90\% overall accuracy, their accuries on individual requirements drop to below 50\%. A later work improved upon the template-based test cases and proposed to use natural language generation to diversify the test cases. By diversifying the test cases, their work also achieved higher failure rates~\cite{yang2022testaug}. For hateful speech, HateCheck~\cite{rottger2020hatecheck} proposed test cases for 30 functional requirements for hate speech detection. One example of hateful requirements is: "\emph{These [IDENTITY] should shut the
f*ck up once and for all.}", another example of non-hateful requirement is: "\emph{No [IDENTITY] deserves to die.}". However, HateCheck's test suites are highly repetitive, thus they suffer from low coverage and may not effectively detect the model errors. In addition, they make an unrealistic assumption that all hate speech are the same for different identity groups such as gay, women, Muslim, etc. In this work, we propose the first dataset for testing hate speech detectors based on Facebook's community standards guideline policies. 



\noindent\textbf{Content Moderation}. The removal of online content is an important process which is enforced by law~\cite{eu_digital_services_act,section_230}. The Digital Services Acts of European Union specified that any platform must report on how they use automated content moderation tools, the tools’ error rates, etc.~\cite{eu_digital_services_act}. Content moderation also purifies the content online and improves user experience. Existing literatures have studied the societal impacts of content moderation~\cite{arsht2018human,steiger2021psychological}. On the other hand, the machine learning community study how to improve content moderation tools~\cite{waseem2016you,waseem2016hateful,davidson2017automated,golbeck2017large,founta2018large,hartvigsen2022toxigen,vidgen2020learning}. For example, training the model to better recognize implicit hate~\cite{hartvigsen2022toxigen}, improving the explanation~\cite{mathew2021hatexplain}, and improving detection on a multi-lingual context~\cite{roy2021leveraging}. To the best of our knowledge, we are not aware of work that focus on testing the behavior of AI-based content moderation tools against moderation policies. 

In Step 1 (Section~\ref{sec:step1}), we reuse the examples in 8 datasets to construct HateModerate. Among these datasests, DynaHate~\cite{vidgen2020learning} and the aforementioned HateCheck~\cite{rottger2020hatecheck} contributed to the most examples. DynaHate is a dataset of hate speech collected using the dynamic benchmarking method~\cite{kiela2021dynabench}. This method started with one dataset, then iteratively trained a model using existing dataset, and asked human annotators to come up with adversarial examples failed by the trained model, then added the adversarial examples to the dataset, etc. The process was repeated for 4 rounds. 

\section{Limitations}
\label{sec:limitation}

\noindent \textbf{Cost of Manual Annotation}. HateModerate is built based on Facebook's content moderation policy on Nov 23, 2022~\cite{fb_standard}. When applying our work on different policies (e.g., for a different platform), we must hire new human annotators to repeat Step I. This process induces a cost every time thus is one limitation of our work.

\noindent \textbf{Coverage of Test Cases in Each Policy}. As explained in related work, to examine whether certain requirements are satisfied, sufficient test cases must be included to cover all examples that match the requirement~\cite{barzamini2022cade}. The majority of HateModerate are from the DynaHate and HateCheck dataset, therefore HateModerate can be biased towards these datasets and do not have sufficient coverage on the other datasets. 

\noindent \textbf{Comprehensiveness of Policy Requirements}. Although Facebook's content moderation policies on hate speech are relatively comprehensive, the 41 policies may not completely cover all hate speeches. 

\noindent \textbf{Contexts and User Expectation of Hate Speech}. Our study focuses on checking AI-based content moderation software' behavior against policies. When evaluating the moderation software, we have not considered the context. However, whether a sentence is hateful or not may depends on the context; the same sentence may sounds hateful in one context but not in another. Moreover, the rules in content moderation policies may not exactly match user's expectation. In this work, we have not focused on studying human subjects on their perception of hate speech, which can be done in future work. 

\section{Conclusions and Discussions}
\label{sec:conclusion}

\noindent \textbf{Conclusions}. In this experience paper, we propose the first study on examining the behavior of deep learning-based systems against policies where we focus on the application in online content moderation. First, we construct test cases matching each policy. We explore using crowdsourcing to collect the test dataset, HateModerate. By using keywords search from existing hate speech datasets followed by manual validation, we are able to construct a quality dataset for test cases matching each policy requirement in Facebook's community standards guidelines for hate speech detection. Second, to examine the behaviors of deep learning systems, we test state-of-the-art models for content moderation by major tech companies, we find that the moderation software of two companies (e.g., OpenAI and Google) frequently make mistakes for certain policies. Finally, we explore the promise of automatically matching a hate speech sentence to policies. By finetuning state-of-the-art large language model, we achieved 78.4\% accuracy, we further find that the model size, the completeness of output information are both important factors that determine the classification accuracy. 

\noindent \textbf{Discussion on the Generality}. Besides hate speech, content moderation also includes categories such as nudity and graphic content, intellectual property, misinformation, phishing information, etc. For example, Facebook has also published 19 policies on removing misinformation under their community standards guidelines~\cite{fb_standard_misinformation}. Our work (Figure~\ref{fig:workflow}) is not limited to hate speech. Given sufficient amount of datasets and data, the framework and can also be extended to other categories of content moderation. 



\section{Future Work} 
\label{sec:future}



For future work, we plan the work in the following directions:

\noindent \textbf{Improving the Coverage of Test Cases in Each Policy}. One direction of future work is to fix the deficiency of coverage (Section~\ref{sec:limitation}) by adding more examples from the other dataset. For example, by searching from more hate speech datasets or cover more examples from the existing datasets. We may define a metric for helping measuring to what extent a requirement is covered by the test cases, e.g., using neuron coverage~\cite{pei2017deepxplore,sekhon2022white}. 

% % Figure environment removed


\noindent \textbf{Discovering More Policy Requirements}. Following the limitation discussion in Section~\ref{sec:limitation}, another potential future direction is to identify new policy requirements which are not covered by Facebook's policy. For example, such requirements may be found among forum discussions. 



\noindent \textbf{Improve the Classification Accuracy of Step 3}. The classification accuracies of Step 3 are low, so there still exists room for improving the accuracy of Step 3. Another future work direction is to improve the accuracy so we are one step closer to automatically matching hate speech sentences to policy requirements. For example, by adding more examples which are easily confused, or by increasing the length of output to include more information. 

\noindent \textbf{Human-AI Collaboration}. Finally, it may be promising to leverage Human-AI collaboration to improve the efficiency of constructing HateModerate. For example, we can use the finetuned model in Section~\ref{sec:step3} to recommend sentences to human annotators, and ask them to only label the recommended ones instead of having to manually search examples from different datasets. 

\section{Acknowledgement}

We thank the following students for their help with labeling: Zhuo Zhang, Vikrant Gajare, Zhekang Xu, Ishaan Patel, Zhicheng Yang, Akshay Atam, Sachin Devanga, Daniel Achacon, Murad Aleskerov, Shubhankar Deol, and Xing Qian

\newpage


\bibliographystyle{IEEEtran}
\bibliography{hatemoderate}

\end{document}
