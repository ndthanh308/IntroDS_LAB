{
  "title": "Reinforcement Learning for Sequential Decoding of Generalized LDPC Codes",
  "authors": [
    "Salman Habib",
    "David G. M. Mitchell"
  ],
  "submission_date": "2023-07-26T02:05:34+00:00",
  "revised_dates": [],
  "abstract": "In this work, we propose reinforcement learning (RL) for sequential decoding of moderate length generalized low-density parity-check (GLDPC) codes. Here, sequential decoding refers to scheduling all the generalized constraint nodes (GCNs) and single parity-check nodes (SPCNs) of a GLDPC code serially in each iteration. A GLDPC decoding environment is modeled as a finite Markov decision process (MDP) in which the state-space comprises of all possible sequences of hard-decision values of the variables nodes (VNs) connected to the scheduled GCN or SPCN, and the action-space of the MDP consists of all possible actions (GCN and SPCN scheduling). The goal of RL is to determine an optimized scheduling policy, i.e., one that results in a decoded codeword by minimizing the complexity of the belief propagation (BP) decoder. For training, we consider the proportion of correct bits at the output of the GCN or SPCN as a reward once it is scheduled. The expected rewards for scheduling all the GCNs/SPCNs in the code's Tanner graph are earned via BP decoding during the RL phase. The proposed RL-based decoding scheme is shown to significantly outperform the standard BP flooding decoder, as well as a sequential decoder in which the GCNs/SPCNs are scheduled randomly.",
  "categories": [
    "cs.IT"
  ],
  "primary_category": "cs.IT",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13905",
  "pdf_url": null,
  "comment": "accepted for publication at ISTC 2023. arXiv admin note: text overlap with arXiv:2112.13934",
  "num_versions": null,
  "size_before_bytes": 2239676,
  "size_after_bytes": 560815
}