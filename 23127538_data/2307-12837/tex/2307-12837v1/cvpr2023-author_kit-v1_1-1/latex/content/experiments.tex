
\subsection{Implementation Details}

\begin{table*}
    \centering
    \small
    \begin{tabular}{ccccc|ccc}
    \toprule
        Sequence len $>$ 1 & $\mathcal{L}_{MS}$ & $\mathcal{L}_{DC}$ & LM & $\mathcal{M}_{CO}$ & Verb  & Noun  & Action \\
        \midrule
         \xmark & \xmark & \xmark & \xmark & \xmark &  51.17 & 28.88 & 20.93 \\ 
         \cmark & \xmark & \xmark & \xmark & \xmark &  51.27 & 30.48 & 21.59 \\ 
         \cmark & \cmark & \xmark & \xmark & \xmark &  53.22 & 32.68 & 23.40 \\ 
         \cmark & \cmark & \cmark & \xmark & \xmark &  54.30 & 33.31 & 24.58 \\ 
         \cmark & \cmark & \cmark & \cmark & \xmark &  \textbf{55.22} & 34.07 & 24.99 \\ 
         \cmark & \cmark & \cmark & \cmark & \cmark &  54.95 & \textbf{34.15} & \textbf{25.46} \\ 
         \bottomrule
    \end{tabular}
    \caption{Ablation study of the different components of our architecture on TBN features. The table shows the Top-1 accuracies starting from the baseline, e.g. each action sample is classified individually, and adding sequence predictions with a temporal window of 5 actions, mixing with the target data ($\mathcal{L}_{MS}$), adversarial alignment using a domain classifier ($\mathcal{L}_{DC}$), predictions refinement using a language model (LM) and pruning of the unlikely verb and noun pairs using a Matrix of Co-Occurrences ($\mathcal{M}_{CO}$). %Each component provides incremental improvements to our architecture.%, where the best results belong to the complete architecture with all components.
    }
    \vspace{-10pt}
    \label{tab:tbn}
\end{table*}


\textbf{Sequence Predictor Architecture. }
To train the mixed sequence model, we use the pre-extracted Temporal Binding Network (TBN)~\cite{kazakos2019TBN} features from the Unsupervised Domain Adaptation (UDA) splits of the EPIC-Kitchens dataset.
These features were available for three modalities: RGB, Flow, and Audio. Each modality comprises 25 clips with features size 1024 and we use TRN~\cite{zhou2018temporal} to temporally aggregate the sample clips.

The model is trained for 100 epochs using a SGD optimizer, with learning rate 0.005. Pseudo-labels for the target data are generated with a confidence threshold of $\lambda$ = 0.75. During each epoch, we mix actions between the source and target domains. Specifically, we replace one action from the source domain with a corresponding action from the target domain using a temporal window of length 5.

\textbf{Language Model. }
The language model is trained for an additional 100 epochs using the Adam optimizer, with a loss value set to 0.001, and fine-tuned on the source training set labels. The training process incorporates a temporal window ($w$) of size 5. The weight coefficient ($\beta$) that combines the predictions of the language model and sequence predictor is set to 0.25. To score sequences, the top-5 actions predicted by the model are taken into account.


\subsection{Results}

Table~\ref{tab:tbn} presents an ablation study of our method, using three modalities and TBN features. The sequence attention mechanism from MTCN~\cite{kazakos_temporal2021} enhances the accuracy compared to the baseline, highlighting the significance of temporal context reasoning even in the presence of domain shift. Furthermore, the inclusion of target and source mixing further boosts the accuracy. 
Additionally, the introduction of a domain classifier aids in better integration of target information into our model, resulting in further improvement. 
Subsequently, we apply to filter improbable predictions using the language model, leading to enhanced action accuracy, and demonstrate the role of temporal context in alleviating the domain gap. Lastly, the inclusion of the co-occurrence matrix yields a final improvement to the model's performance, especially on the \textit{action} metric as it weakens the probability of the unlikely verb and noun pairs.

\subsection{Ablations}
\textbf{Sequence length. }
Table~\ref{tab:sequence_length} shows the effect of the number of actions $w$ in the temporal window on verb and noun classification accuracy, without mixing actions between source and target. Best results on action category are obtained by using a sequence of length $w=5$. %A larger temporal window improves the accuracy up to $w=7$, showcasing that temporal context can provide useful cues for action recognition, although this effect saturates as the size of the time window increases.
\begin{table}
    \centering
    \small
    \begin{tabular}{c|ccc}
        \toprule
        Sequence length & Verb  & Noun  & Action \\
        \midrule
        1 & 51.17 & 28.88 & 20.93 \\
        \hline
         3 &  50.97 & 30.23 & 21.18 \\ 
         \hline
         5 &  51.27 & \textbf{30.48} & \textbf{21.59} \\ 
         \hline
        %7 & \textbf{51.54}  & \textbf{30.74} & \textbf{21.75}  \\ 
         
         9 & \textbf{51.31} & 30.34 & 21.41\\
         \bottomrule
    \end{tabular}
    \caption{Top-1 accuracy using different numbers of actions in the temporal window $w$ on the EPIC-Kitchens-100 validation set.}
    \label{tab:sequence_length}
\end{table}


\textbf{Number of replacements. } 
Table~\ref{tab:mixing_level} shows the effect of replacing one or more actions in the sequence with the target before the sequence is sent to the transformer. Improvements in accuracy on all metrics are observed when at least one sample is substituted, while the substitution of more samples only improves on individual categories.
\begin{table}
    \centering
    \small
    \begin{tabular}{c|ccc}
        \toprule
        \# Replacements & Verb  & Noun  & Action \\
        \midrule
         0 &  51.27 & 30.48 & 21.59 \\ 
        \hline
         1 &  53.22 & 32.68 & 23.40 \\ 
        \hline
         2 & 53.13 & 32.07 & \textbf{23.73}\\
        \hline
         3 & \textbf{53.64} & \textbf{32.70} & {23.59}\\
        \bottomrule
    \end{tabular}
    \caption{Top-1 accuracy using a different number of target replacements results within a sequence of $w=5$ actions. Results reported on the EPIC-Kitchens-100 validation set.}
    \label{tab:mixing_level}
\end{table}

\subsection{Model Ensemble}
For the final submission, we ensemble our technique with different backbones, using SlowFast~\cite{fan2020pyslowfast} and Temporal Shift Module~\cite{lin2019tsm} trained on EPIC-Kitchens-55 with ResNet50.
The performances of the individual backbones and the ensemble are presented in Table~\ref{tab:ensemble}.
\begin{table}
    \centering
    \small
    \begin{tabular}{c|ccc}
        \toprule
        Backbone & Verb  & Noun  & Action \\
        \midrule
         TBN~\cite{kazakos2019TBN} &  54.30 & 33.31 & 24.58 \\ 
         \hline
         TSM~\cite{lin2019tsm} &  54.13 & 33.25 & 24.53 \\ 
         \hline
         SlowFast~\cite{fan2020pyslowfast} & 54.44  & 30.74 & 23.38  \\ 
         \hline
         Ensemble(E) & 56.97 & 35.64 & 26.50\\
         \hline
         E + LM & \textbf{57.46} & \textbf{36.44} & 27.25\\
         \hline
         E + LM + $\mathcal{M}_{CO}$ & 57.24 & 36.42 & \textbf{27.63}\\
         \bottomrule
    \end{tabular}
    \caption{Top-1 accuracy using our domain adaptation method with different backbones on the EPIC-Kitchens-100 validation set. LM: Language Model. $\mathcal{M}_{CO}$: Matrix of Co-Occurrences.}
    \vspace{-10pt}
    \label{tab:ensemble}
\end{table}
Our approach is visible on the official leaderboard and shown in Table~\ref{tab:leaderboard}, along with the top five teams' performance.
\begin{table}
    \centering
    \small
    \begin{tabular}{c|c|ccc}
        \toprule
        Rank & Method & Verb  & Noun  & Action \\
        \midrule
         1&Ns-LLM &  \textbf{58.22} & 40.33 & \textbf{30.14} \\ 
         \hline
         2&VI-I2R &  57.89 & 40.07 & 30.12 \\ 
         \hline
         3&Audio-Adaptive-CVPR2022 &  52.95 & \textbf{42.26} & 28.06 \\ 
        \hline
        \rowcolor{lightgray}
        \textbf{4} & \textbf{sshayan} &  58.11 & 35.89 & 27.72 \\ 
        \hline
        5&plnet & 55.51 & 35.86 & 25.25\\
        \bottomrule
    \end{tabular}
    \caption{Top-1 accuracy on the official leaderboard of the UDA challenge EPIC-Kitchens-100. Our submission is highligthed.}
    \vspace{-8pt}
    \label{tab:leaderboard}
\end{table}

