\section{Our Approach}

In this section, we describe our method in details. 
We introduce our approach to model temporal dependencies between actions and to adapt action sequences across different domains. %, then normalize the result using the language model, and finally prune the unacceptable combination of verbs and nouns.
Finally, we refine the predictions of our model using a language model and we reduce the likelihood of actions not seen during training.
Figure~\ref{fig:method} presents the different components of our method.
%Each action sample $i$ of sequence $X$ has three input modalities: $X_r^{(i)}$ (RGB), $X_f^{(i)}$ (optical flow), and $X_a^{(i)}$ (audio).

%------------------------------------------------------------------------
\subsection{Mixed Sequence Generation}
The main objective of our method is to augment action sequences from the source domain with samples from the target sharing the same verb and noun labels.
As labels are not available in the target domain, we assign \textit{pseudo-labels} to the target data, based on a confidence threshold $\lambda$. 
%These pseudo-labels are based on the highest probability for both the verb and noun categories. 
For each target sample, we define the confidence of the model predictions as $(p_v + p_n) / 2$, where $p_v$ and $p_n$ represent the probabilities of the top verb and noun predictions respectively. 
%This threshold is used to determine the pseudo-labels. 
Samples whose prediction confidence falls below the threshold are not considered in the mixing process.

%Once the target data is labeled, we proceed with the learning process by integrating multiple modalities within a sequence of actions from a video as input. %Specifically, we consider three modalities: . 
Given a sequence consisting of $w$ actions, our goal is to predict the central action $i$ within the temporal window $(i - w/2 < i < i + w/2)$, while exploiting the context provided by both preceding and subsequent actions.% to make predictions for the selected action.

To incorporate target domain information, we introduce a domain adaptation approach for replacing action clips from the source domain with action clips from target distribution representing the same action. More specifically, for each sequence of actions $\mathcal{S}_i=\{s_{-w/2}, \dots, s_0, \dots s_{w/2}\}$ from the source domain, we randomly select one or more item ($s_i$) - excluding the central action - and replace them with an equivalent $t_i$ sampled from the target domain, with the constraint that $t_i$ pseudo-labels match verb and noun labels of $s_i$. The model is trained to predict the verb and noun labels of the central action of the mixed sequence $\mathcal{\Tilde{S}}_i=\{s_{-w/2}, ..., t_i, ..., s_{w/2}\}$.

\subsection{Sequence Predictor }
To integrate sequences in the action recognition process, we take inspiration from~\cite{kazakos_temporal2021} to model temporal relations within the actions window using a transformer model.

\textbf{Positional Embedding}
In the first step, we concatenate input features of different modalities and project the output to a lower dimension $D$. 
Then, we apply learnt positional embedding to tag the position of each action in the temporal window. Finally, we also append to the sequence two separate tokens for verb and noun prediction of central action. The resulting sequence is indicated by $X_e$. %, and final results will be given to the transformer.


\textbf{Transformers Encoding}
%After the addition of positional embeddings
Once temporal consistency is encoded in positional embeddings, the resulting sequence is fed to a transformer encoder $f(.)$. The attention layers in the transformer allow actions inside the sequence to attend to all the surrounding actions, possibly exchanging relevant clues for the classification of the central action. This operation can be expressed as $Z = f(X_e)$, where $Z$ represents the output of the transformer module, and $X_e$ is the output of positional embedding. %To promote information sharing and improve efficiency, we share weights among the layers of the transformer encoder.

%\subsubsection{Domain Classifier}
%To encourage the network to learn more domain agnostic representations, we also include a domain adversarial alignment module based on Gradient Reversal Layer~\cite{Ganin2015}. %This classifier aims to guide the feature extractor in learning domain-irrelevant features for accurate action prediction in a new domain.

\textbf{Action Classifier}
Finally, samples in the sequence are classified using two heads, to predict the verb and noun labels separately. 
%and the network is trained using the standard cross entropy loss. 
As in~\cite{kazakos_temporal2021}, we define two classification losses. 
The primary loss function is defined as the cross entropy of the central action of the sequence, computed separately for verb and noun predictions.
%Based on their predictions, the loss is calculated to drive the training process.
%\subsubsection{Loss}
%Our objective is to classify the middle action within a sequence. 
%To achieve this, we compute the center action cross-entropy loss as the primary loss. 
%Then, to favour the information transfer from the other actions in the sequence, we use an auxiliary classification loss for all the actions in the sequence. 
Then, to further encourage the network to model temporal relations between all neighbouring actions in the sequence, we also ask the model to predict the labels for all the actions in the sequence through standard cross entropy loss, which we refer to as the mixed sequence prediction loss ($\mathcal{L}_{MS}$).
%These individual auxiliary losses are later combined to form the sequence prediction loss. 

To encourage the network to learn domain agnostic representations, we also include a domain adversarial module with Gradient Reversal Layer~\cite{Ganin2015} to produce a domain classification loss $\mathcal{L}_{DC}$ for all the actions in the sequence. 
The resulting loss is thus determined by the summation of the domain discriminator loss $\mathcal{L}_{DC}$ and the sequence prediction loss $\mathcal{L}_{MS}$.

\subsection{Language Model}
%To maximize the integration of contextual information and ensure the continuity of actions while considering the relationship between two domains.
Inspired by MTCN~\cite{kazakos_temporal2021}, we introduce a Language Model (LM) to filter out unlikely action sequences. %, we trained the LM on the source dataset and utilized it to normalize the final predictions.
We adopt the Masked Language Model (MLM) approach as in~\cite{kazakos_temporal2021}, in which the model is trained to predict the masked actions within a sequence from the source data, thus encouraging the model to derive high-level dependencies between the actions that make up the sequence. 
%This enabled us to calculate the high prior probability of each verb and noun sequence within a window of size $w$.
At inference time, to integrate the LM into our framework, we generate all possible sequences of size $w$ using the top-$k$ predictions of the mixed sequence model for each action and select the most probable sequence according to the LM. 
%Based on the scores obtained by the language model for these provided sequences, we selected the highest-scoring sequence. 
The final prediction of the model is a linear combination of the predictions from the mixed sequence $y_{MS}$ and the output of the language model for the most probable sequence $y_{LM}$:
%Finally, we compute a convex combination of the predictions of the multi-modal model, applying a degree of integration as $\beta$ as in Equation~\ref{eq:LM}, where the $P_{LM}$ is the indicator of LM score for action and $P_{ML}$ is the indicator of the Multi-modal model score.
\begin{equation}
y_{final} = (1 - \beta)y_{MS} + \beta y_{LM}.
\label{eq:LM}
\end{equation}

\subsection{Co-Occurrence}
To further refine the model predictions, we compute a co-occurrence matrix $\mathcal{M}_{CO}$ of verbs and nouns labels in the source domain, as introduced in~\cite{cheng2022team}.
Each entry $\mathcal{M}_{CO_{i,j}}$ stores the occurrences of verb class $i$ and noun class $j$ together in the source data.
% Assuming that $P_i^n$ represents the noun probability vector for action $i$ and $P_i^v$ represents the verb probability vector for action $i$, we calculated the combination of all pairs of noun and verb probabilities for action i as $P_A = P_i^n * P_i^{vT}$. 
%If a specific noun-verb pair is not observed in the source data, e.g. its corresponding entry in the $M$ is zero, its probability of occurrence in the target set is assumed to be low. 
At test time, the probability of predicting verb and noun pairs which are not present in the source data reduces by a factor of $0.01$, i.e. we use the simple assumption that actions not present in the source data are unlikely to be found in the target data.
