
\section{Conclusions}
\label{sec:conclusion}

In this paper, we introduced \textit{TransNet}, a two-stage pipeline for category-level transparent object pose estimation that outperformed the
baselines. Ablation studies were performed to understand the benefits of using multi-modal input, feature embedding modules, and cross-task consistency. The efficacy of TransNet was demonstrated by enabling a robot system to perform transparent object manipulation tasks with high success rates. 
In the future, we could further improve the pose estimation accuracy as there is still a large gap between TransNet and state-of-the-art opaque object datasets. One direction is to consider objects' material information, as we observed a performance gap between perceiving glass and plastic objects.

% Built upon category-level opaque object poses estimation, our method completed the missing depth from state-of-art depth completion work, TransCG. More than that, an additional surface normal estimation does improve the performance. Our experiments also guided choosing useful modalities as input , a suitable embedding method for transparent objects. For our future work, one direction could be either improve the performance of category-level opaque object pose estimation (second stage) or transparent object depth completion, surface normal estimation (first stage). Another direction could be designing better module between this two stages, like we could have more strategies for generalized point cloud sampling within masks and we could have a module to avoid the noisy depth serving directly as the translation prior.