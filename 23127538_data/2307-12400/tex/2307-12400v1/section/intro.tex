\section{Introduction}
\label{sec:intro}
From glass doors and windows to kitchenware and all kinds of containers, objects with transparent materials are prevalent throughout daily life.
Thus, perceiving the pose (position and orientation) of transparent objects is a crucial capability for autonomous perception systems seeking to interact with their environment.
However, transparent objects present unique perception challenges both in the RGB and depth domains.

For RGB, the color appearance of transparent objects is highly dependent on the background, viewing angle, material, and lighting condition, due to light reflection and refraction effects.
For depth, common commercially available depth sensors record mostly invalid or inaccurate depth values within the transparent region.
Such visual challenges, especially missing detection in the depth domain, pose severe challenges for autonomous object manipulation and obstacle avoidance tasks.
This paper addresses these challenges by studying how category-level transparent object pose estimation may be achieved using end-to-end learning.

Recent works have shown promising results on grasping transparent objects by completing the missing depth values followed by the use of a geometry-based grasp engine~\cite{sajjan2020clear,ichnowski2021dex,fang2022transcg}, transfer learning from RGB-based grasping neural networks~\cite{weng2020multi}, light-field feature learning~\cite{zhou2019glassloc}, or domain-randomized depth noise simulation~\cite{dai2022domain}.
For more advanced manipulation tasks such as rigid body pick-and-place or liquid pouring, geometry-based estimations, such as symmetrical axes, edges~\cite{phillips2016seeing} or object poses~\cite{lysenkov2013recognition, xu20206dof, dai2022domain}, are required to model the manipulation trajectories.
Instance-level transparent object poses could be estimated from keypoints on stereo RGB images~\cite{liu2020keypose,liu2021stereobj}, a light-field camera~\cite{zhou2018plenoptic, zhou2020lit}, or directly from a single RGB-D image~\cite{xu20206dof} with support plane assumptions.
Recently emerged large-scale transparent object datasets~\cite{sajjan2020clear,xu2021seeing,liu2021stereobj,fang2022transcg,chen2022clearpose} pave the way for addressing the problem using deep learning.

In this paper, we set out to extend the frontier of 3D transparent object perception by building upon recent work. We introduce \textit{TransNet}, a category-level pose estimation pipeline for transparent objects that outperforms a state-of-the-art baseline. We further explore the effect of input modalities, feature embedding methods, and depth-normal consistency through the learning process to provide insights for future research. Moreover, we build an autonomous transparent object manipulation system using TransNet and demonstrate its efficacy in pick-place and pouring tasks.

% In this work, we aim to extend the frontier of 3D transparent object perception with three primary contributions.
% \begin{itemize}
%     \item We introduce \textit{TransNet}, a category-level pose estimation pipeline for transparent objects that outperform the accuracy of baselines.
%     \item We conclude insights in multiple aspects of the transparent pose estimation task that could direct future explorations, including input modalities, feature embedding method, and depth-normal consistency learning.
%     \item We build an autonomous transparent object manipulation system based on TransNet and demonstrate its efficacy in pick-and-place and pouring tasks.
% \end{itemize}

 



%Both modalities have been discussed in previous works on depth completion and instance pose estimation~\cite{sajjan2020clear,ichnowski2021dex,fang2022transcg,xu20206dof,jiang2022uni6d}, however, the characteristics of depth and surface normal have not been deeply analyzed or compared.
%\cxt{I also saw the cited works have done ablation study about normal and depth (their depth is perfect anyway)} 
%\cxt{To me the 2nd and 3rd points should not be noted as `aspects'. They are more like algorithmic details that improve the final accuracy/feature learning efficiency of deep learning approaches}
%\ao{I think the ablation studies can be a contribution since their results can guide future work, but I don't think we should be so specific about the embedding methods}

% % Figure environment removed    