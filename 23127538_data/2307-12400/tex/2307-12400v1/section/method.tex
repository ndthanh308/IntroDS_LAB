\section{TransNet}
\label{sec:method}

% Figure environment removed    

Given an input RGB-D pair ($\mathcal{I}$, $\mathcal{D}$) viewing a scene that includes transparent objects, our goal is to predict the objects' 6D rigid body transformations $[\textbf{R}|\textbf{t}]$, 3D scales $\textbf{s}$ in the camera coordinate frame and object classes within a predefined category set, where $\textbf{R} \in SO(3), \textbf{t} \in \mathbb{R}^{3}$ and $\textbf{s} \in \mathbb{R}^{3}_{+}$. 
In this problem, inaccurate or invalid depth readings exist within the image region of transparent objects (represented as a binary mask).
To solve the category-level pose estimation problem, we propose a novel two-stage deep neural network pipeline, \textit{TransNet}.

\subsection{Architecture Overview}

% Following recent work in object pose estimation~\cite{wang2019densefusion,chen2021fs,di2022gpv}, we first apply a pre-trained instance segmentation module (Mask R-CNN \cite{he2017mask}) that has been fine-tuned on the pose estimation dataset to extract the objects' bounding box patches, masks, and category labels to separate the objects of interest from the entire image. 
% Following recent works in object pose estimation~\cite{wang2019densefusion,chen2021fs,di2022gpv}, w
TransNet first applies a fine-tuned instance segmentation module to extract individual object patches from the overall image. As shown in Figure~\ref{fig:arch}, in the first stage, TransNet takes the patches as input and extracts multi-modal features corresponding to each transparent object instance. Depth completion and surface normal estimation are applied on RGB-D patches to obtain depth-normal pairs through cross-task consistency learning. The estimated depth-normal pairs, RGB, and ray direction patches are concatenated to feature patches, followed by a random sampling strategy within the instance masks to generate features referred to as the \textit{generalized point cloud}. In the second stage, the generalized point cloud is processed through Pointformer \cite{zou20216d} to produce embedded feature vectors. The pose is then separately estimated in four decoder modules for object translation, $x$-axis, $z$-axis, and scale, respectively.
%The estimated rotation matrix can be recovered using the estimated two axes. 
% Each component is discussed in more detail in the following sections.

\subsection{Object Instance Detection and Segmentation}

Similar to other category-level pose estimation work \cite{di2022gpv}, we fine-tune a Mask R-CNN \cite{he2017mask} model to obtain the object's bounding box $\mathcal{B}$, segmentation mask $\mathcal{M}$ and category label $\mathcal{P}_c$.
Patches of ray direction $\mathcal{R}_{\mathcal{B}}$, RGB $\mathcal{I}_{\mathcal{B}}$ and raw depth $\mathcal{D}_{\mathcal{B}}$ are extracted according to the bounding box $\mathcal{B}$ and serve as input to the first stage of TransNet. The UV mapping itself is an important cue when estimating poses from patches~\cite{jiang2022uni6d}, as it provides information about the relative position and size of the patches within the overall image.
We use ray direction instead of UV mapping because it also contains camera intrinsic information. Here the ray direction $\mathcal{R}$ represents the direction from camera origin to each pixel in the camera plane under the camera frame. For each pixel $(u,v)$:
\begin{align}
\begin{split}
    p &= \begin{bmatrix}u & v & 1\end{bmatrix}^T \\
    \mathcal{R} &= \frac{K^{-1} p}{\left\lVert K^{-1} p\right\rVert^2}
\end{split}
\end{align}
where $p$ is the homogeneous UV coordinate in the image plane and $K$ is the camera intrinsic matrix.


% \subsection{Transparent object depth completion}

% Due to light reflection and refraction on transparent material, the depth of transparent objects is very noisy.
% Therefore, depth completion is necessary to reduce the sensor noise.
% Given the raw RGB-D patch ($\mathcal{I}_{\mathcal{B}}$, $\mathcal{D}_{\mathcal{B}}$) pair and transparent mask $\mathcal{M}_t$ (a intersection of transparent objects' masks within bounding box $\mathcal{B}$), transparent object depth completion $\mathcal{F}_{D}$ is applied to obtain the completed depth of the transparent region $\{\hat{\mathcal{D}}_{(i, j)}|(i, j)\in\mathcal{M}_t \}$.

% Inspired by one state-of-the-art depth completion method, TransCG \cite{fang2022transcg}, we incorporate a similar multi-scale depth completion architecture into TransNet.
% \begin{equation}
% \label{eq:dc}
%     \hat{\mathcal{D}}_\mathcal{B} = \mathcal{F}_{D}\left(\mathcal{I}_\mathcal{B}, \mathcal{D}_\mathcal{B}\right)
% \end{equation}

% We use the same training loss as TransCG:
% \begin{align}
% \begin{split}
%     &\mathcal{L} = \mathcal{L}_d + \lambda_{smooth} \mathcal{L}_s \\
%     &\mathcal{L}_d = \frac{1}{N_p}\sum_{p\in\mathcal{M}_t \bigcap \mathcal{B}}\left\lVert\hat{\mathcal{D}}_p -  \mathcal{D}^{*}_p\right\rVert^2 \\
%     &\mathcal{L}_s = \frac{1}{N_p}\sum_{p\in\mathcal{M}_t \bigcap \mathcal{B}}\left(1 - \text{cos}\left\langle\mathcal{N}(\hat{\mathcal{D}}_p), \mathcal{N}(\mathcal{D}^{*}_p)\right\rangle\right)
% \end{split}
% \end{align}

% where $\mathcal{D}^{*}$ is the ground truth depth image patch, $p\in\mathcal{M}_t \bigcap \mathcal{B}$ represents the transparent region in the patch, $\left\langle\boldsymbol{\cdot \; , \; \cdot}\right\rangle$ denotes the dot product operator and $\mathcal{N}(\boldsymbol{\cdot})$ denotes the operator to calculate surface normal from depth. $\mathcal{L}_d$ is $L_2$ distance between estimated and ground truth depth within the transparency mask. $\mathcal{L}_s$ is the cosine similarity between surface normal calculated from estimated and ground truth depth. $\lambda_{smooth}$ is the weight between the two losses. 

% \subsection{Transparent object surface normal estimation}

% Surface normal estimation $\mathcal{F}_{SN}$ estimates surface normal $\mathcal{S}_{\mathcal{B}}$ from RGB image $\mathcal{I}_{\mathcal{B}}$. Although previous category-level pose estimation works \cite{di2022gpv,chen2021fs} show that depth is enough to obtain opaque objects' pose, experiments in Section \ref{exp:generalized point cloud} demonstrate that surface normal is not a redundant input for transparent object pose estimation. Here, we slightly modify U-Net \cite{ronneberger2015u} to perform the surface normal estimation. 
% \begin{equation}
% \label{eq:sne}
%     \hat{\mathcal{S}}_\mathcal{B} = \mathcal{F}_{SN}\left(\mathcal{I}_\mathcal{B}\right)
% \end{equation}

% We use the cosine similarity loss:
% \begin{align}
% \begin{split}
%     &\mathcal{L} = \frac{1}{N_p}\sum_{p\in\mathcal{B}}\left(1 - \text{cos}\left\langle\hat{\mathcal{S}}_p, \mathcal{S}^{*}_p\right\rangle\right)
% \end{split}
% \end{align}

% where $p\in \mathcal{B}$ means the loss is applied for all pixels in the bounding box $\mathcal{B}$. 

\subsection{Cross-task Consistency for Depth-Normal Pairs}

% Given the raw RGB-D patch ($\mathcal{I}_{\mathcal{B}}$, $\mathcal{D}_{\mathcal{B}}$) pair and transparent mask $\mathcal{M}_t$ (a intersection of transparent objects' masks within bounding box $\mathcal{B}$), we simultaneously applied transparent object depth completion $\mathcal{F}_{\mathcal{D}}$ and surface normal estimation $\mathcal{F}_{\mathcal{SN}}$ to obtain the completed depth $\{\hat{\mathcal{D}}_{(i, j)}|(i, j)\in\mathcal{M}_t \}$ and estimated surface normal $\{\hat{\mathcal{S}}_{(i, j)}|(i, j)\in\mathcal{M}_t \}$ of the transparent region. 

% Inspired by one state-of-the-art depth completion method, TransCG \cite{fang2022transcg}, we incorporate a similar multi-scale depth completion architecture into TransNet. Here, we slightly modify U-Net \cite{ronneberger2015u} to perform the surface normal estimation. 

We apply a recent state-of-the-art approach, TransCG~\cite{fang2022transcg} for depth completion ($\mathcal{F}_{D}$) and U-Net~\cite{ronneberger2015u} for surface normal estimation ($\mathcal{F}_{SN}$).

\begin{align}
\begin{split}
\label{eq:dc&sne}
    \hat{\mathcal{D}}_\mathcal{B} &= \mathcal{F}_{D}\left(\mathcal{I}_\mathcal{B}, \mathcal{D}_\mathcal{B}\right)\\
    \hat{\mathcal{S}}_\mathcal{B} &= \mathcal{F}_{SN}\left(\hat{\mathcal{D}}_\mathcal{B}\right)
\end{split}
\end{align}
where the depth estimation $\hat{\mathcal{D}}_\mathcal{B}$ is used as input to the surface normal estimation module.
The two models are first trained separately with depth and surface normal losses, $\mathcal{L}_d$ and $\mathcal{L}_s$, and then trained together using cross-task consistency. Following \cite{zamir2020robust}, we design a consistency loss $\mathcal{L}_{con}$ to train both networks. Both $\mathcal{L}_d$ and $\mathcal{L}_s$ are implemented using $L_2$ loss for depth completion and surface normal estimation within transparent mask $\mathcal{M}$. 
% $L_{con}$ is the consistent loss. \hjz{need more to describe consistent loss}. $\lambda_{s}, \lambda_{con}$ are weights for loss:
\begin{align}
\begin{split}
\label{eq:loss}
    % \mathcal{L} &= \mathcal{L}_d + \lambda_{s} \mathcal{L}_s + \lambda_{con} \mathcal{L}_{con}\\
    % \mathcal{L}_d &= \frac{1}{N_p}\sum_{p\in\mathcal{M}_t \bigcap \mathcal{B}}\left\lVert\hat{\mathcal{D}}_p -  \mathcal{D}^{*}_p\right\rVert^2 \\
    \mathcal{L}_d &= \frac{1}{N_p}\sum_{p\in\mathcal{M}}\left\lVert\hat{\mathcal{D}}_p -  \mathcal{D}^{*}_p\right\rVert^2 \\
    \mathcal{L}_s &= \frac{1}{N_p}\sum_{p\in\mathcal{M}}\left\lVert\hat{\mathcal{S}}_p -  \mathcal{S}^{*}_p\right\rVert^2 \\
    % \mathcal{L}_{con} &= \frac{1}{N_p}\sum_{p\in\mathcal{M}_t \bigcap \mathcal{B}}\left\lVert \hat{\mathcal{S}}_p - \mathcal{F}_{SN}(\mathcal{D}^{*}_\mathcal{B})_p \right\rVert^2
    \mathcal{L}_{con} &= \frac{1}{N_p}\sum_{p\in\mathcal{M}}\left\lVert \hat{\mathcal{S}}_p - \mathcal{F}_{SN}(\mathcal{D}^{*}_\mathcal{B})_p \right\rVert^2
\end{split}
\end{align}
where $\mathcal{D}^{*}$ and $\mathcal{S}^{*}$ are the ground truth depth and surface normal images, and $N_p$ refers to the number of masked pixels, respectively. During training, these losses are used according to the perceptual loss training strategy described by Zamir et al.~\cite{zamir2020robust}.
Following cross-task consistency training, the depth completion and surface normal estimation networks are frozen and used to generate input for the second stage of TransNet.


\subsection{Generalized Point Cloud}
The generalized point cloud is implemented as a concatenation of multi-modal input features: $\mathcal{I}_\mathcal{B}, \;\mathcal{R}_\mathcal{B}, \;\hat{\mathcal{D}}_\mathcal{B},$ and $\hat{\mathcal{S}}_\mathcal{B}$. Specifically, we randomly sample $N$ pixels within the transparent mask $\mathcal{M}$ of the feature patch to obtain a generalized point cloud $\mathcal{P}\in \mathbb{R}^{N\times 10}$. The experiment described in Section \ref{exp:generalized point cloud} explores the best choice of features for use in the generalized point cloud.
% As input to the second stage, generalized point cloud $\mathcal{P}\in \mathbb{R}^{N\times d}$ is a stack of $d$-dimensional features from the first stage taken at $N$ sample points, inspired from \cite{xu20206dof}. To be more specific, $d=10$ in our work. Given the completed depth $\hat{\mathcal{D}}_\mathcal{B}$ and predicted surface normal $\hat{\mathcal{S}}_\mathcal{B}$ from Equation \eqref{eq:dc&sne}, together with RGB patch $\mathcal{I}_\mathcal{B}$ and ray direction patch $\mathcal{R}_\mathcal{B}$, a concatenated feature patch is given as $\left[\mathcal{I}_\mathcal{B},  \hat{\mathcal{D}}_\mathcal{B},  \hat{\mathcal{S}}_\mathcal{B},  \mathcal{R}_\mathcal{B}\right] \in \mathbb{R}^{H \times W \times 10}$.

% Here the ray direction $\mathcal{R}$ represents the direction from camera origin to each pixel in the camera frame. For each pixel $(u,v)$:
% \begin{align}
% \begin{split}
%     p &= \begin{bmatrix}u & v & 1\end{bmatrix}^T \\
%     \mathcal{R} &= \frac{K^{-1} p}{\left\lVert K^{-1} p\right\rVert^2}
% \end{split}
% \end{align}

% where $p$ is the homogeneous UV coordinate in the image plane and $K$ is the camera intrinsic.
\subsection{Transformer Feature embedding}

Given a generalized point cloud $\mathcal{P}$ corresponding to a detected object, TransNet employs Pointformer~\cite{zou20216d}, a multi-stage transformer-based point cloud embedding method to encode object features $\mathcal{P}_{emb} \in \mathbb{R}^{N\times d_{emb}}$.
\begin{equation}
\label{eq:emb}
    \mathcal{P}_{emb} = \mathcal{F}_{PF}\left(\mathcal{P}\right)
\end{equation}
Experiments described in Section~\ref{exp:embedding} evaluate the effectiveness of using alternative point cloud embedding methods, such as 3D-GCN~\cite{Lin_2020_3dgcn}, for use in the transparent object context with noisy depth data.
% During feature aggregation for each point, they use the nearest neighbor algorithm to search nearby points within coordinate space, then calculate new features as a weighted sum of the features within surrounding points. Due to the noisy input $\hat{D}$ from Equation \eqref{eq:dc&sne}, the nearest neighbor may become unreliable by producing noisy feature embeddings. On the other hand, Pointformer aggregates feature by a transformer-based method. The gradient back-propagates through the whole point cloud. More comparisons and discussions in Section \ref{exp:embedding} demonstrate that transformer-based embedding methods are more stable than nearest neighbor-based methods when both are trained on noisy depth data.

Next, a Point Pooling layer (a multilayer perceptron (MLP) with max-pooling) is applied to extract global features $\mathcal{P}_{global}$. The global features are then concatenated with local features $\mathcal{P}_{emb}$ and a one-hot category $\mathcal{P}_c$ label from the instance segmentation network. The concatenations result in a feature vector to be used as input to the decoder: %\cxt{I assume this is provided as GT during training; explain how this label is obtained during inference}. 
\begin{align}
\begin{split}
\label{eq:concat}
    &\mathcal{P}_{global} = \text{MaxPool}\left(\text{MLP}\left(\mathcal{P}_{emb}\right)\right) \\
    &\mathcal{P}_{concat} = \left[\mathcal{P}_{emb}, \mathcal{P}_{global}, \mathcal{P}_c\right]
\end{split}
\end{align}

\subsection{Pose and Scale Estimation}

After the feature embeddings are extracted based on multi-modal input, four separate decoders are applied to estimate the object's translation, $x$-axis, $z$-axis, and scale. Similar to \cite{chen2021fs}.

\noindent\textbf{Translation Residual Estimation}
% \subsubsection{Translation Residual Estimation}
%As demonstrated in , residual estimation achieves better performance than direct regression by learning the distribution of the residual between the prior and actual value. 
The translation decoder $\mathcal{F}_{t}$ learns a 3D translation residual from the object translation prior $t_{prior}$ calculated as the average of predicted 3D coordinate over the sampled pixels in $\mathcal{P}$. %To be more specific:
\begin{align}
\begin{split}
\label{eq:trans}
    t_{prior} &= \frac{1}{N_p}\sum_{p\in N} K ^{-1} \left[u_p \  v_p \  1\right]^T \hat{\mathcal{D}_p} \\
    \hat{t} &= t_{prior} + \mathcal{F}_{t}\left(\left[\mathcal{P}_{concat}, \mathcal{P}\right]\right) \\
\end{split}
\end{align}
where $u_p$, $v_p$ are the 2D pixel coordinate for the selected pixel. We use the $L_1$ loss between the ground truth and estimated position:
\begin{equation}
\label{eq:trans_loss}
    \mathcal{L}_t = \left\lvert\hat{t} - t^*\right\rvert
\end{equation}

\noindent\textbf{Rotation Estimation} %Similar to \cite{chen2021fs}, rather than directly regress the rotation matrix $R$, it is more effective to 
We decouple the 3D rotation matrix $R$ into two orthogonal axes, $x$-axis $a_x$ and $z$-axis $a_z$, and estimate them separately.
% As shown in Figure \ref{fig:arch}, we decouple $R$ into the $z$-axis $a_z$ (red axis) and $x$-axis $a_x$ (green axis).
%Following the strategy of confidence learning in \cite{di2022gpv}, 
The network learns confidence values to deal with the problem that the regressed two axes are not orthogonal:
\begin{align}
\begin{split}
\label{eq:rot}
    \left[\hat{a}_i, c_i\right] &= \mathcal{F}_i\left(\mathcal{P}_{concat}\right), \  i\in \left\{x, z\right\} \\
    \theta_z &= \frac{c_x}{c_x + c_z}\left(\theta - \frac{\pi}{2}\right)\\
    \theta_x &= \frac{c_z}{c_x + c_z}\left(\theta - \frac{\pi}{2}\right)
\end{split}
\end{align}
where $c_x, c_z$ denotes the confidence for the learned axes. $\theta$ represents the angle between $a_x$ and $a_z$.
$\theta_x, \theta_z$ are obtained by solving an optimization problem and then used to rotate $a_x$ and $a_z$ within their common plane.
%More details can be found in \cite{di2022gpv}.
For the training loss, we use $L_1$ plus cosine similarity loss $\mathcal{L}_{r_x}, \mathcal{L}_{r_z}$ for each individual axis. Angular loss, $\mathcal{L}_a$, is also used to constrain a perpendicular relationship between the two axes and confidence loss $\mathcal{L}_{con_x},\mathcal{L}_{con_z}$ to learn axis confidence:
\begin{align}
\begin{split}
\label{eq:rot_loss}
    \mathcal{L}_{r_i} &= \left\lvert\hat{a}_i - a^*_i\right\rvert + 1 - \left\langle\hat{a}_i, a^*_i\right\rangle, \  i\in \left\{x, z\right\} \\
    \mathcal{L}_a &= \left\langle\hat{a}_x, \hat{a}_z\right\rangle\\
    \mathcal{L}_{con_i} &= \left\lvert c_i - \text{exp}\left(\alpha \left\lVert \hat{a}_i - a^*_i \right\rVert_2\right)\right\rvert, \  i\in \left\{x, z\right\} 
\end{split}
\end{align}
where $\alpha$ is a constant to scale the distance. Thus the overall loss for the second stage is:
\begin{align}
\begin{split}
\label{eq:all_loss}
    \mathcal{L} = &\lambda_s\mathcal{L}_s + \lambda_t\mathcal{L}_t + \lambda_{r_x}\mathcal{L}_{r_x} + \lambda_{r_z}\mathcal{L}_{r_z} + \\
    &\lambda_{r_a}\mathcal{L}_{a} + \lambda_{con_x}\mathcal{L}_{con_x} + \lambda_{con_z}\mathcal{L}_{con_z}
\end{split}
\end{align}

where $\lambda_{r_x}, \lambda_{r_z}, \lambda_{r_a}, \lambda_{t}, \lambda_{s}, \lambda_{con_x}, \lambda_{con_z}$ are weights for each loss.

% To deal with object symmetry, we apply specific treatments for different symmetry types.
% For axial symmetric objects (those that remain the same shape when rotating around one axis), we ignore the loss for the $x$-axis, $i.e., \mathcal{L}_{con_x}, \mathcal{L}_{r_x}$.
% For planar symmetric objects (those that remain the same shape when mirrored about one or more planes)\iffalse\cxt{define concisely here}\fi, we generate all candidate $x$-axis rotations. 
% For example, for an object symmetric about the $x-z$ plane and $y-z$ plane, rotating the $x$-axis about the $z$-axis by $\pi$ radians will not affect the object's shape.
% The new $x$-axis is denoted as $a_{x_{\pi}}$ and the loss for the $x$-axis is defined as the minimum loss of both candidates: 
% \begin{align}
% \begin{split}
% \label{eq:rot_plannar}
%     \mathcal{L}_x = \text{min}\left(\mathcal{L}_x(a_x), \mathcal{L}_x(a_{x_{\pi}})\right)
% \end{split}
% \end{align}

\noindent\textbf{Scale Residual Estimation}  
Similar to the translation decoder, we define the scale prior $s_{prior}$ as the average of scales of all object 3D CAD models within each category. Then the scale of a given instance is calculated as follows:
\begin{align}
\begin{split}
\label{eq:scale}
    &\hat{s} = s_{prior} + \mathcal{F}_{s}\left(\mathcal{P}_{concat}\right) \\
\end{split}
\end{align}

 The loss function is defined as the $L_1$ loss between the ground truth scale and estimated scale:

\begin{equation}
\label{eq:scale_loss}
    \mathcal{L}_s = \left\lvert\hat{s} - s^*\right\rvert
\end{equation}