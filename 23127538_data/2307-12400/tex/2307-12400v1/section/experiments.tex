\section{Experiments}
\label{sec:result}

% \subsection{Dataset and Evaluation Metrics}

\noindent\textbf{Dataset} We evaluated TransNet and baseline models on the Clearpose Dataset \cite{chen2022clearpose} for depth completion, surface normal estimation, and categorical transparent object pose estimation tasks.
% The Clearpose Dataset contains over 350K real-world labeled RGB-D frames in 51 scenes, 9 sets, and around 5M instance annotations covering 63 household objects.
As shown in Figure~\ref{fig:objects}, 33 object instances from 3 ClearPose categories (\textit{bowl, water\_cup,} and \textit{wine\_cup}) were selected for evaluating the relevant models on each task. 25 of the objects are used in the training set, totaling 190K RGB-D images. Hence, 8 objects not seen during training are used for testing, with 60K test images available. In practice, we train the models using the full 190K image training set and uniformly sample images of the test object to form a representative 5K image test set. For the robot manipulation experiments, we used 1 \textit{bowl} object and 1 \textit{wine\_cup} object from the test set along with another 1 new \textit{wine\_cup} and 2 new \textit{water\_cups} purchased from the market that fit the robot parallel gripper.
%\cxt{Do we need an object model illustration here?}
%We used all the scenes in set2, set4, set5, and set6 for training and scenes in set3 and set7 for validation and testing.
% The division guaranteed that there were some unseen objects for testing within each category.
%For training depth completion and surface normal estimation, we used the same dataset split.

% Figure environment removed

\noindent\textbf{Implementation Details} Our model was trained in several stages. For experiments included in the ablation study and baseline comparison, ground truth instance segmentations are used to generate input patches, while for the robot experiments, a Mask R-CNN~\cite{he2017mask} model fine-tuned on the Clearpose dataset is used for generating input patches. 
%\cxt{why don't we train a Mask R-CNN?}
The image patches were generated from object bounding boxes and re-scaled to a fixed shape of $256\times256$ pixels. 
Cross-task consistency training is performed using the AdamW optimizer~\cite{loshchilov2017decoupled} with learning rate of $1e^{-3}$. The pretrained surface normal estimation model provided by Zamir~\textit{et al.}~\cite{zamir2020robust} is used for initial network parameters.
Cross-task training hyperparameters follow the perceptual loss strategy described in~\cite{zamir2020robust} until convergence using a batch size of 8.
For TransCG, the AdamW optimizer~\cite{loshchilov2017decoupled} is used for training with $\lambda_{smooth} = 0.001$ and the overall learning rate is $0.001$ to train the model until converge. 
For U-Net, we used the Adam optimizer~\cite{kingma2014adam} with a learning rate of $1e^{-4}$ to train the model until convergence.
For both surface normal estimation and depth completion, the batch size was set to 24.
%The surface normal estimation and depth completion model were frozen during the training of the second stage.
For the second stage, the training hyperparameters of Pointformer and pose and scale estimation were selected following~\cite{zou20216d, di2022gpv}.
%Data augmentation for RGB features and instance masks was used. %for sampling generalized point cloud.
%A batch size of 18 was used. To balance sampling distribution across categories, 3 instance samples were selected randomly for each of 6 categories.
The learning rate for all loss terms were kept the same during training, $\left\{\lambda_{r_x}, \lambda_{r_z}, \lambda_{r_a}, \lambda_{t}, \lambda_{s}, \lambda_{con_x}, \lambda_{con_z}\right\} = \left\{8, 8, 4, 8, 8, 1, 1\right\} e^{-4}$.
The Ranger optimizer \cite{liu2019variance,yong2020gradient,zhang2019lookahead} was used with a linear warm-up for the first 1000 iterations, then a cosine annealing method at the 0.72 anneal point was used.
All the experiments for pose estimation were trained on a 16G RTX3080 GPU until loss convergence.

% \begin{table}
% \begin{center}
% \caption{Setting for Pixelformer}
% \label{table:pixelformer_setting}
% \hjz{Copy from 6D-ViT}
% \begin{tabular}{c|cccccc}
% \hline\noalign{\smallskip}
% Experiment type & RMSE$\downarrow$ & REL$\downarrow$ & MAE$\downarrow$ & $\delta_{1.05}\uparrow$ & $\delta_{1.10}\uparrow$ & $\delta_{1.25}\uparrow$\\
% \noalign{\smallskip}
% \hline
% \noalign{\smallskip}
% normal & &\\
% priority & & \\
% consistent & & \\
% \hline
% consistent + priority & & \\
% \end{tabular}
% \end{center}
% \end{table}




\noindent\textbf{Evaluation metrics} For category-level pose estimation, this study follows \cite{di2022gpv,chen2021fs} in using 3D intersection over union (IoU) between the ground truth and estimated 3D bounding box 
% (we used the estimated scale and pose to draw an estimated 3D bounding box) 
at 25\%, 50\% and 75\% thresholds. Additionally, $5^{\circ}5cm$, $10^{\circ}5cm$, and $10^{\circ}10cm$ are used as metrics. The reported measurements for each of these three metrics represent the percentage of a model's pose estimates with error less than the specified metric's degree and distance thresholds. %For section \ref{exp:depth-surfacenormal}, we also used separated translation and rotation metrics: $2cm$, $5cm$, $10cm$, $5^{\circ}$, $10^{\circ}$ that calculate percentage with respect to one factor. 

For depth completion evaluation, the root of mean squared error (RMSE), absolute relative error (REL) and mean absolute error (MAE) are used as metrics along with $\delta_{1.05}$, $\delta_{1.10}$, $\delta_{1.25}$, where $\delta_n$ is calculated as:

\begin{equation}
    \delta_n = \frac{1}{N_p}\sum_{p}\textbf{I}\left(\text{max}\left(\frac{\hat{\mathcal{D}}_p}{\mathcal{D}^*_p}, \frac{\mathcal{D}^*_p}{\hat{\mathcal{D}}_p}\right) < n\right)
\end{equation}

\noindent where $\textbf{I}(\boldsymbol{\cdot})$ represents the indicator function. $\hat{\mathcal{D}_p}$ and $\mathcal{D}^*_p$ represent estimated and ground truth depth for each pixel $p$.

For surface normal estimation, RMSE and MAE errors of normalized vectors are used as metrics with thresholds of $11.25^{\circ}$, $22.5^{\circ}$, and $30^{\circ}$. Here, measurements reported for the $11.25^{\circ}$ metric represent the percentage of estimates with an angular distance less than $11.25^{\circ}$ from the ground truth surface normals. The mean angular error in degrees (MEAN) is also reported.



\subsection{Comparison with Baseline}
\label{exp:baseline}
\setlength{\tabcolsep}{1.4pt}

We chose one state-of-the-art categorical opaque object pose estimation model (GPV-Pose \cite{di2022gpv}) as a baseline, which was trained with estimated depth from TransCG \cite{fang2022transcg} for a fair comparison. 
From Table \ref{table:baseline}, TransNet outperformed the baseline in most of the metrics on the Clearpose dataset. The $3\text{D}_{25}$ metric is relatively easy to achieve strong performance, so there is no substantial difference between the two models' corresponding $3\text{D}_{25}$ performance. 
When compared to the baseline on the remaining metrics, TransNet achieved around a 3 $\times$ improvement on $3\text{D}_{75}$, 3.5 $\times$ on $10^{\circ}5\text{cm}, 10^{\circ}10\text{cm}$ and 8 $\times$ on $5^{\circ}5\text{cm}$. Qualitative results are shown in Figure \ref{fig:visual} for TransNet. 

\setlength{\tabcolsep}{1pt}
\begin{table}[h]
% \vspace{0.5cm}
\begin{center}
\caption{Comparison with the baseline on the Clearpose Dataset.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|ccccccc}
\hline
\noalign{\smallskip}
Method & $\text{3D}_{25}\uparrow$ & $\text{3D}_{50}\uparrow$ & $\text{3D}_{75}\uparrow$ & $5^{\circ}5\text{cm}\uparrow$ & $10^{\circ}5\text{cm}\uparrow$ & $10^{\circ}10\text{cm}\uparrow$\\
\noalign{\smallskip} 
\hline
% \noalign{\smallskip}
GPV-Pose & 95.4 & 65.1 & 13.2 & 2.7 & 12.5 & 15.5 \\
\hline
TransNet & \textbf{97.3} & \textbf{80.3} & \textbf{39.9} & \textbf{22.7} & \textbf{45.4} & \textbf{50.6}   \\
\hline
\end{tabular}
}
\label{table:baseline}
\end{center}
\vspace{-0.7cm}
\end{table}



\subsection{Ablation Study}

\label{exp:ablation}

In Table \ref{table:ablation}, we compared the performance of TransNet to that of modified versions of the network architecture. The results of this study informed Trial 6 as the final architecture of TransNet and the one that was used in robot experiments. Results from the ablation study may also be used to guide future work in the area of transparent object pose estimation. 

% Figure environment removed

\begin{table*}
\vspace{0.2cm}
\begin{center}
\caption{Ablation study for TransNet}
% \begin{adjustbox}{width=\columnwidth,center}%
% \resizebox{\textwidth}{!}{%
\begin{tabular}{c|cc|cccc|c|c|ccccccc}
\hline
\multirow{2}{*}{Trial} &  \multicolumn{2}{c|}{Embedding} & \multicolumn{4}{c|}{Generalized Point Cloud} & \multirow{2}{*}{Consistency} & \multirow{2}{*}{Separate Category} & \multirow{2}{*}{$3D_{25}\uparrow$} & \multirow{2}{*}{$3D_{50}\uparrow$} & \multirow{2}{*}{$3D_{75}\uparrow$} & \multirow{2}{*}{$5^{\circ}5cm\uparrow$} & \multirow{2}{*}{$10^{\circ}5cm\uparrow$} & \multirow{2}{*}{$10^{\circ}10cm$$\uparrow$} \\ \cline{2-7}
& \multicolumn{1}{c}{PF}  & \multicolumn{1}{c|}{3D-GCN} & \multicolumn{1}{c}{RGB} & \multicolumn{1}{c}{Depth} & \multicolumn{1}{c}{Normal} & \multicolumn{1}{c|}{Ray-direction}  & & & & & & & & \\ \hline

\multicolumn{1}{c|}{1}&  & \checkmark & \checkmark & \checkmark &  & \checkmark & & & 98.3 & 72.7 & 17.6  & 6.6 & 22.5 & 28.0 \\ 

\multicolumn{1}{c|}{2} & \checkmark &   & \checkmark & \checkmark &  & \checkmark & &  & 97.9 & 76.9 & 28.4  & 9.4 & 8.0 & 29.6 \\ 

\multicolumn{1}{c|}{3}& \checkmark &  & \checkmark & \checkmark & \checkmark & & & &  95.4 & 61.6 & 11.6  & 3.6 & 15.3 & 31.3 \\ 

\multicolumn{1}{c|}{4}& \checkmark &   & \checkmark & \checkmark & \checkmark & \checkmark  & & & \textbf{98.4} & 81.6 & 34.4  & 13.0 & 38.6 & 45.1 \\ 

\multicolumn{1}{c|}{5}& \checkmark &  & \checkmark & \checkmark & \checkmark & \checkmark  & \checkmark & & 98.1 & \textbf{81.8} & 39.2  & 15.8 & 41.4 & 46.1 \\ 

\hline

\multicolumn{1}{c|}{6}& \checkmark &  & \checkmark & \checkmark & \checkmark & \checkmark  & \checkmark & \checkmark & 97.3 & 80.3 & \textbf{39.9} & \textbf{22.7} & \textbf{45.4} & \textbf{50.6} \\ 

\hline
\end{tabular}
% }
\label{table:ablation}
\end{center}
\vspace{-0.5cm}
\end{table*}

% \subsubsection{Embedding Method} 
\noindent\textbf{Embedding Method}
\label{exp:embedding}
Between trial 1 and trial 2, we compared the effect of the embedding method, 3D-GCN \cite{Lin_2020_3dgcn} and Pointformer \cite{zou20216d}, on TransNet accuracy. The use of Pointformer resulted in higher accuracy for TransNet on most metrics compared to when 3D-GCN was used. More details are shown in Table \ref{table:emb_ab}. Crucially, when the ground truth depth was changed to estimated depth (modeling the change from opaque to transparent setting), Pointformer retained substantially more accuracy than 3D-GCN. A possible explanation for this result is based on how 3D-GCN propagates information between nearest neighbors. While this is an efficient method for propagating information locally, it suffered when given the noisy completed depth as input. Pointformer, on the other hand, shares information across the whole point cloud, possibly contributing to increased robustness when input data is noisy. Therefore, given depth with large uncertainty, the transformer-based embedding method might be more powerful than embedding methods using nearest neighbors. 

\setlength{\tabcolsep}{1pt}
\begin{table}
\begin{center}
\caption{Comparison between different embedding methods}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|ccccccc}
\hline
% \noalign{\smallskip}
Depth type & Embedding& $\text{3D}_{25}$$\uparrow$ & $\text{3D}_{50}$$\uparrow$ & $\text{3D}_{75}$$\uparrow$ & $5^{\circ}5\text{cm}$$\uparrow$ & $10^{\circ}5\text{cm}$$\uparrow$ & $10^{\circ}10\text{cm}$$\uparrow$\\
% \noalign{\smallskip} 
\hline
% \noalign{\smallskip}
\multirow{2}{*}{Ground truth} 
& 3D-GCN & 99.8 & 96.6 & 55.4 & 70.9 & 86.0 & 90.3 \\
& Pointformer & 99.7 & 96.6 & 74.5 & 61.9 & 83.7 & 86.5 \\
\hline
\multirow{2}{*}{Estimation} 
& 3D-GCN & 98.3  & 72.7 & 17.6 & 6.6 & 22.5 & 28.0 \\
& Pointformer &97.9 & 76.9 & 28.4 & 9.4 & 29.6 & 35.3 \\
\hline
\end{tabular}
}
\label{table:emb_ab}
\end{center}
\vspace{-0.3cm}
\end{table}

% \subsubsection{Modalities for Generalized Point Cloud} 
\noindent\textbf{Modalities for Generalized Point Cloud}
\label{exp:generalized point cloud}
The comparison of trials 2â€”4 was used to identify the optimal combination of feature inputs for the generalized point cloud in TransNet. For trials 2 and 4, we compared the effect of adding the estimated surface normal to the generalized point cloud. 
Results from this comparison imply that surface normals serve as a good complement when depth estimation is not accurate, as in the transparent setting.
For trials 3 and 4, we compared the potential benefit of including ray-direction. 
%\hjz{Crucially, the ray-direction input contains both camera intrinsic and pixel positional information, helping the network learn 3D information}. 
Notably, the ray-direction input contains both camera intrinsic and pixel positional information, which we hypothesize is critical for the network to be able to model accurate 3D spatial relationships.
All the metrics showed that including both surface normal and ray-direction results in improved pose estimation accuracy. 





\setlength{\tabcolsep}{1pt}
\begin{table}
\begin{center}
\caption{Accuracy for depth-normal pair estimation}
% \begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{c|cccccc}
\hline
% \noalign{\smallskip}
Depth Metrics &RMSE$\downarrow$ & REL$\downarrow$ & MAE$\downarrow$ & $\delta_{1.05}$$\uparrow$ & $\delta_{1.10}$$\uparrow$ & $\delta_{1.25}$$\uparrow$\\
% \noalign{\smallskip}
\hline
% \noalign{\smallskip}
w/o consistency & \textbf{0.056} & \textbf{0.044} & \textbf{0.041} & 68.61 & 89.35 & \textbf{98.89}\\
w consistency  & 0.057 & \textbf{0.044} & \textbf{0.041} & \textbf{70.49} & \textbf{89.47} & 98.57 \\
\hline\hline
Surface Normal Metrics & RMSE$\downarrow$ & MAE$\downarrow$ & MEAN$\downarrow$ & $11.25^{\circ}$$\uparrow$ & $22.5^{\circ}$$\uparrow$ & $30^{\circ}$$\uparrow$\\
\hline
% \noalign{\smallskip}
w/o consistency  & 0.19 & 0.13 & 11.43 & 56.75 & 88.45 & \textbf{96.64} \\
w consistency  & \textbf{0.12} & \textbf{0.08} & \textbf{8.96} & \textbf{73.62} & \textbf{92.56} & 96.58 \\
\hline
\end{tabular}
\label{table:d-s}
\end{center}
\vspace{-0.7cm}
\end{table}

% \subsubsection{Consistent Learning} 
\noindent\textbf{Consistent Learning}
For trials 4 and 5 in Table \ref{table:ablation}, TransNet is trained without and with cross-task consistency, respectively. For trial 4, both surface normal estimation and depth completion networks in TransNet's first stage are trained separately and without the $\mathcal{L}_{con}$ loss. Results from this comparison show that adding cross-task consistency training improved TransNet performance on high accuracy metrics: $5^{\circ}5\text{cm}$, $3\text{D}_{75}$. From Table \ref{table:d-s}, adding a cross-task consistency loss resulted in improved accuracy for depth-normal pair estimation, particularly for the surface normal estimation network. Therefore, cross-task consistent training improves the performance of category-level transparent object pose estimation and is included in TransNet. 

\noindent\textbf{Separate Category}
%\hjz
{As shown in Table \ref{table:ablation} when comparing results in trial 5 and trial 6, it was found that TransNet's accuracy improves when a single instantiation of the model is trained per category as opposed to being trained on multiple categories. Notably, this result is most apparent for the high accuracy metric $5^{\circ}5\text{cm}$}

\noindent\textbf{Why TransNet Outperforms GPV-Pose}
%\hjz
{ Comparing GPV-Pose with the trial 1 model leads to an observation that both models use 3D-GCN and a similar network architecture. The only difference is that instead of using the generalized point cloud as input, GPV-Pose directly uses a point cloud (the multiplication of depth and ray direction) as input. Yet, trial 1 outperforms GPV-Pose on pose estimation tasks. We hypothesize that the camera intrinsic and pixel positional information contained in ray-direction degrades in accuracy after being multiplied directly with noisy depth data to form the point cloud input to GPV-Pose. In contrast, TransNet retains ray-direction and depth information as distinct feature channels in the generalized point cloud with learned transformations to combine each feature modality. These learned transformations potentially allow TransNet to be more robust in the face of noisy depth data.}

% Figure environment removed  

\subsection{Robot Experiment}

We used a Fetch robot for manipulation and an Intel RealSense L515 camera to take RGB-D images as used in the ClearPose dataset. The robot-camera extrinsic calibration is done using AprilTags~\cite{krogius2019iros}. Inspired from \cite{chen2020manipulation}, we designed two robot manipulation tasks: pouring and pick-and-place to evaluate the efficacy of TransNet to support robot manipulation of transparent objects using 3 \textit{water\_cups}, 2 \textit{wine\_cups}, and 1 \textit{bowl} not appearing in the training set. 
%\hjz
{For grasping the object, we calculate the approaching direction as orthogonal to the object's symmetrical axis and the grasp location as the object's center point.}

\noindent\textbf{Pick-and-place} 
This task is composed of a pick action and a place action. For the pick action, the robot is to grasp a cup (either \textit{water\_cup} or \textit{wine\_cup}) on the table-top with its parallel gripper based on its pose estimate from TransNet. For the place action, the robot is to place the cup upright in a pre-defined location. The target poses for the robot's gripper are solved by aligning the object's estimated bottom surface parallel to the target location surface with a 2 cm offset.

% \subsubsection{Pouring} 
\noindent\textbf{Pouring} 
This task is composed of a pick action and a pouring action. The pick action follows the same protocol as \textit{pick-and-place}. For the pouring action, the robot moved the cup above the bowl based on the bowl's estimated location while also keeping the cup's $z$ axis upright to prevent spilling. Then the content in the cup is poured into the bowl by tilting the cup's $z$ axis.

% $Obj_1$ was set as objects within the wine cup and water cup categories and $Obj_2$ as objects from the bowl category. 
% We trained these categories separately (one category for one model). 
% Three novel wine cups and water cups were chosen for the robot experiment. 
% \subsubsection{Result}



\noindent\textbf{Result}
The poses and scales used by the robot are output directly from TransNet without additional post-processing. Quantitative results measuring the robot's success rate are included in Table~\ref{table:robotgrasp}. 
% The number of Place and Pouring is equal to the successful cases of Pick. Failure of pour could be caused by an inaccurate estimation of $Obj_2$'s pose or an opposite opening direction of $Obj_1$. Failure of place could be caused by inaccurate estimation of poses and scales of $Obj_1$. 
TransNet enabled the robot to perform pour and pick-place actions with promising success rates. Visualization examples are included in Figure \ref{fig:robotgrasping}.




% \begin{table}
% \centering
% \caption{Robot experiment success rate}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{c|cc|cc|cc}
% \hline
% Task  & $Obj_1$ & $Obj_2$ & \#Pick & Success Rate & \#Place(Pour) & Success Rate \\ \hline
% \multirow{2}{*}{Pick-and-place} & water\_cup & - & 16/20 & 80\% & 12/16 & 75\%  \\ 
%   & wine\_cup & - & 13/20 & 65\% & 12/13 & 92.3\% \\ \hline
% \multicolumn{1}{c|}{\multirow{2}{*}{Pouring}} & water\_cup & bowl & 15/20 & 75\% & 10/15 & 66.7\% \\ 
% \multicolumn{1}{c|}{} & wine\_cup & bowl & 14/20 & 70\% & 11/14 & 78.6\% \\ \hline
% \end{tabular}}
% \label{table:robotgrasp}
% \vspace{-0.5cm}
% \end{table}


\begin{table}
\centering
\caption{Robot experiment success rate}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|cc|cc}
\hline
Task & Objects   & \#Pick & Success Rate & \#Place(Pour) & Success Rate \\ \hline
\multirow{2}{*}{Pick \& Place} & water\_cup         & 16/20  & 80\%         & 12/16         & 75\%         \\
                               & wine\_cup          & 13/20  & 65\%         & 12/13         & 92.3\%       \\ \hline
\multirow{2}{*}{Pouring}       & water\_cup \& bowl & 15/20  & 75\%         & 10/15         & 66.7\%       \\
                               & wine\_cup \& bowl  & 14/20  & 70\%         & 11/14         & 78.6\%       \\ \hline
\end{tabular}}
\label{table:robotgrasp}
\vspace{-0.5cm}
\end{table}

%===============================================================================