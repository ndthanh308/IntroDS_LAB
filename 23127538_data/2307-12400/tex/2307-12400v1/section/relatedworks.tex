\section{Related Work}
\label{sec:related_works}

\subsection{Transparent Object Visual Perception for Manipulation}

Transparent objects must be perceived before they can be acted on.
Lai~\textit{et al.}~\cite{lai2015transparent} and Khaing~\textit{et al.}~\cite{khaing2018transparent} developed CNN models to detect transparent objects from RGB images.
Xie~\textit{et al.}~\cite{xie2020segmenting} proposed a deep segmentation model that achieved state-of-the-art accuracy.
ClearGrasp~\cite{sajjan2020clear} employed depth completion for use with pose estimation on robotic grasping tasks, where they trained three DeepLabv3+~\cite{chen2018encoder} models to learn transparency mask, surface normal, and boundary, respectively.
Follow-on studies developed different approaches for depth completion, including implicit functions~\cite{zhu2021rgb}, NeRF reconstruction~\cite{ichnowski2021dex}, combined point cloud and depth features~\cite{xu2021seeing}, adversarial learning~\cite{tang2021depthgrasp}, multi-view geometry~\cite{chang2021ghostpose}, RGB image completion~\cite{fang2022transcg}, and sim2real transfer~\cite{dai2022domain}.
Weng~\textit{et al.}~\cite{weng2020multi} used transfer learning from the RGB to the depth sensor domain without completing raw depth.
For instance-level pose estimation, Xu~\textit{et al.}~\cite{xu20206dof} utilized segmentation, surface normal, and image coordinate UV-map as input to a network similar to~\cite{tian2020robust} that can estimate 6 DOF object pose.
Keypose~\cite{liu2020keypose} proposed to estimate 2D keypoints and regress object poses from stereo images using triangulation.
For other special sensors, Xu~\textit{et al.}~\cite{xu2015transcut} used light-field images to perform segmentation using a graph-cut-based approach.
Kalra \textit{et al.}~\cite{kalra2020deep} trained Mask R-CNN~\cite{he2017mask} using polarization images as input to outperform the baseline that was trained on only RGB images by a large margin.
Zhou \textit{et al.}~\cite{zhou2018plenoptic,zhou2019glassloc,zhou2020lit} employed light-field images to learn features for robotic grasping and object pose estimation.
Along with the proposed methods, massive datasets, across different sensors and both synthetic and real-world domains, have been collected and made public for various related tasks~\cite{xie2020segmenting,sajjan2020clear,liu2020keypose,zhou2020lit,kalra2020deep,liu2021stereobj,zhu2021rgb,xu2021seeing,fang2022transcg,chen2022clearpose}.
Compared with these previous works, and to the best of the authors' knowledge, we propose the first category-level pose (6D pose + 3D scale) estimation approach developed specifically for transparent objects.
% Notably, the proposed approach provides reliable 6D pose and scale estimates across instances with similar shapes.

\subsection{Opaque Object Category-level Pose Estimation}

Category-level object pose estimation aims to estimate unseen objects' 6D pose within seen categories and their scale or canonical shape.
% To the best of our knowledge, there is not currently any category-level pose estimation works focusing on transparent objects, and the works mentioned below mostly consider opaque objects. They won't work well for transparency due to their dependence on accurate depth.
Wang \textit{et al.}~\cite{wang2019normalized} introduced the Normalized Object Coordinate Space (NOCS) for dense 3D correspondence learning and solved for object pose and scale using the learned NOCS map and masked depth~\cite{umeyama1991least}.
More recent works explored different aspects to improve pose estimation accuracy.
A category-level shape prior is found to be beneficial for pose estimation accuracy in~\cite{tian2020shape} and further improved in~\cite{chen2021sgpa,chen2021fs, di2022gpv}.
DualPoseNet~\cite{lin2021dualposenet}, 6D-ViT~\cite{zou20216d}, ACR-Pose~\cite{fan2021acr}, and CPPF~\cite{you2022cppf} proposed to incorporate rotation-invariant embedding, Transformer networks, Generative Adversarial Networks, and deep point-pair-feature, respectively.
However, these techniques require high-quality depth input provided by opaque objects with Lambertian light reflectance.
% % They also contributed both a synthetic and a real dataset used extensively by the following works for benchmarking.
% Later, Li \textit{et al.}~\cite{li2020category} extended the idea towards articulated objects.
% To simultaneously reconstruct the canonical point cloud and estimate the pose, Chen \textit{et al.}~\cite{chen2020learning} proposed a method based on canonical shape space (CASS).
% % Tian \textit{et al.}~\cite{tian2020shape} learned category-specific shape priors from an autoencoder, and demonstrated its power for pose estimation and shape completion.
% Tian \textit{et al.}~\cite{tian2020shape} learned category-specific shape priors from an autoencoder.
% 6D-ViT~\cite{zou20216d} and ACR-Pose~\cite{fan2021acr} extended this idea by utilizing pyramid visual transformer (PVT) and generative adversarial network (GAN)~\cite{goodfellow2014generative} respectively. %PVT is also integrated into TransNet. \cxt{I would not state this in the middle of the paragraph.}
% Structure-guided prior adaptation (SGPA)~\cite{chen2021sgpa} utilized a transformer architecture for a dynamic shape prior adaptation.
% Other than learning a dense correspondence, FS-Net~\cite{chen2021fs} regressed the pose parameters directly, and it proposed to learn two orthogonal axes for 3D orientation.
% % Also, it contributed to an efficient data augmentation process for depth-only approaches.
% GPV-Pose~\cite{di2022gpv} further improved FS-Net by adding a geometric consistency loss between 3D bounding boxes, reconstruction, and pose. %Our TransNet uses a similar strategy for object axes regression. \cxt{I would not state this in the middle of the paragraph.}
% Also with depth as the only input, category-level point pair feature (CPPF)~\cite{you2022cppf} could reduce the sim-to-real gap by learning deep point pairs features.
% DualPoseNet~\cite{lin2021dualposenet} benefited from rotation-invariant embedding for category-level pose estimation.
% % Differing from other works using segmentation networks to crop image patches as the first stage, 
% CenterSnap~\cite{irshad2022centersnap} presented a single-stage approach for the prediction of 3D shape, 6D pose, and size.
One recent work by Dai~\textit{et al.}~\cite{dai2022domain} proposed a data generation system that simulates the noise on non-Lambertian surfaces of active stereo depth cameras and demonstrated its usage in category-level pose estimation and robotic grasping.

% Compared with opaque objects, we find the main challenge to perceive transparent objects is the poor quality of input depth.
% Thus, the proposed TransNet takes inspiration from the above category-level pose estimation works regarding feature embedding and architecture design.
% More specifically, TransNet leverages both Pointformer from PVT and the pose decoder from FS-Net and GPV-Pose.
% In the following section, the TransNet architecture is described, focusing on how to integrate the single-view depth completion module and utilize imperfect depth predictions to learn pose estimates of transparent objects.
Compared with these techniques, TransNet takes advantage of recent advances in transparent depth completion approaches and directly learns category-level pose estimation from imperfect depth predictions, carefully considering multi-modal input, feature embedding architecture, and depth-normal consistency.