%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{graphicx}
% DO NOT USE \usepackage{times}, it will be removed by typesetters
%\usepackage{times}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage{orcidlink}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{threeparttable}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage{dblfloatfix}
% \usepackage{cite}[nocondense]
\long\def\cxt#1{\textcolor{magenta}{\bf \small cxt: **#1**}}

\long\def\hjz#1{\textcolor{red}{\bf \small hjz: **#1**}}

\long\def\ao#1{\textcolor{blue}{\bf \small ao: **#1**}}

\long\def\zjy#1{\textcolor{violet}{\bf \small zjy: **#1**}}


\title{\LARGE \bf
TransNet: Transparent Object Manipulation\\ Through Category-Level Pose Estimation
}


\author{
Huijie Zhang$^{1}$
\and
Anthony Opipari$^{1}$
\and
Xiaotong Chen$^{1}$
\and
Jiyue Zhu$^{1}$
\and
Zeren Yu$^{2}$
\and
Odest Chadwicke Jenkins$^{1}$
\thanks{$^{1}$ H. Zhang, A. Opipari, X. Chen, J. Zhu, and O. Jenkins are with the Electrical Engineering and Computer Science, Mechanical Engineering, and Robotics departments, University of Michigan, Ann Arbor, MI, USA.
}
\thanks{$^{2}$ Z. Yu is with Department of Computer Science, University of Southern California, Los Angeles, CA, USA.}
}
\begin{document}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}
% % Figure environment removed

% \begin{strip}
% \centering
% % Figure removed
% \captionof{figure}{TransNet is a pipeline for category-level transparent object pose estimation. Given instance-level segmentation masks as input, TransNet estimates the 6 degrees of freedom pose and scale for each transparent object in the image, which could support robotic pick-place and pouring tasks. Internally, TransNet uses surface normal estimation, depth completion, and a transformer-based architecture for accurate pose estimation despite noisy sensor data.
% \label{fig:teaser}}
% \end{strip}

\begin{strip}
\vspace{-2.5cm}
\begin{center}
    % Figure removed
    \captionof{figure}{TransNet is a pipeline for category-level transparent object pose estimation and manipulation. Given an RGB-D image of a scene containing transparent objects, TransNet estimates the 6D pose and 3D scale of each object in the scene for use in downstream manipulation tasks such as grasping and pouring.}
    \label{fig:teaser}
\end{center}
\vspace{-0.6cm}
\end{strip}

\begin{abstract}
Transparent objects present multiple distinct challenges to visual perception systems.
First, their lack of distinguishing visual features makes transparent objects harder to detect and localize than opaque objects. Even humans find certain transparent surfaces with little specular reflection or refraction, like glass doors, difficult to perceive. 
A second challenge is that depth sensors typically used for opaque object perception cannot obtain accurate depth measurements on transparent surfaces due to their unique reflective properties. 
Stemming from these challenges, we observe that transparent object instances within the same category, such as cups, look more similar to each other than to ordinary opaque objects of that same category. 
Given this observation, the present paper explores the possibility of category-level transparent object pose estimation rather than instance-level pose estimation. We propose \textit{\textbf{TransNet}}, a two-stage pipeline that estimates category-level transparent object pose using localized depth completion and surface normal estimation. TransNet is evaluated in terms of pose estimation accuracy on a large-scale transparent object dataset and compared to a state-of-the-art category-level pose estimation approach. Results from this comparison demonstrate that TransNet achieves improved pose estimation accuracy on transparent objects. Moreover, we use TransNet to build an autonomous transparent object manipulation system for robotic pick-and-place and pouring tasks. The project webpage is available at \href{https://progress.eecs.umich.edu/projects/transnet/}{https://progress.eecs.umich.edu/projects/transnet/}.


% \keywords{Transparent Objects. Category-level Object Pose Estimation. Depth Completion. Surface Normal Estimation.}
% The results are evaluated on a recent large-scale transparent object dataset and will be made open-source soon.
\end{abstract}

\vspace{-0.2cm}

\input{section/intro}
\input{section/relatedworks}
\input{section/method}
\input{section/experiments}
\input{section/conclusions}


\bibliographystyle{IEEEtran}
\bibliography{egbib}




\end{document}
