\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Mixed-Prompt:}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

1. Big models gain significant performance boost on image related tasks. (\ie transformer-based architecture) Compare to CNN, it reaches much higher results (Benefit from large param capacity and attention mechanism for global dependence). However, with the increasing of model sizes, it becomes harder for training and fine-tuning hyperparameters. Unbenefit to researchers, making it not sustainable for AI development;


2. Current transfer learning methdos, full-fine tune, involves updating all the backbone params (making it data inefficient and energy costy during transfer learning.) Other methods, ..., can not provide competitive performance to fully fine-tune, limit the usages during transfer leanring. Inspired by prompting techniques in NLP, VPT brings powerful potentials on vision transformation task. However, it does not take serious attention on QKV mechanism on transformer structure, ignoring the possible inner attention on transformer-based architecture. The following questions naturally arise: 1. digging structure benefit in self-attention mechanism? 2. pursuing the extreme of visual prompt tuning with as least params as possible.

3. These two issues motivate us to revisit the task from a more systematic view. With this perspective, in \S, we first answer question 1 by pointing out current visual prompt tuning method, only introducing learnable params in cls token and linear head, cannot satisfy the transfer learning needs. We thus provide further contribution on key and value pairs to add more learnable params. Here compared to VPT, we introduced more learnable params, hence question 2 becomes more fundamental: 3. can we further prune unnecessary (even harmful) learnable params during training and make the learnbale param space more compactable? 

4. Driven by 3, we develop mixed-prompt to pursue the extreme of visual transfer learning tasks. The contributions are threefolds: 1. by prompting less than 1\% param (compare to fully fine-tune), we largely outperform VPT and reach SOTA? performance on visual transfer learning tasks; 2. By introducing pruning techniques, we significantly reduce the params load and maintain total learnable params lower or on-par to VPT; 3. We further investigate data-distribution on hyperbolic space and show robust performance of our methods on various datasets.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
