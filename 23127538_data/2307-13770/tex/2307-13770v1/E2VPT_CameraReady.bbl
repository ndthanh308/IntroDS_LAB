\begin{thebibliography}{100}\itemsep=-1pt

\bibitem{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c},
  and Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock In {\em ICCV}, 2021.

\bibitem{atigh2022hyperbolic}
Mina~Ghadimi Atigh, Julian Schoep, Erman Acar, Nanne Van~Noord, and Pascal
  Mettes.
\newblock Hyperbolic image segmentation.
\newblock In {\em CVPR}, 2022.

\bibitem{bao2021beit}
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock In {\em ICLR}, 2022.

\bibitem{beal2020toward}
Josh Beal, Eric Kim, Eric Tzeng, Dong~Huk Park, Andrew Zhai, and Dmitry
  Kislyuk.
\newblock Toward transformer-based object detection.
\newblock {\em arXiv preprint arXiv:2012.09958}, 2020.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock In {\em NeurIPS}, 2020.

\bibitem{cai2020tinytl}
Han Cai, Chuang Gan, Ligeng Zhu, and Song Han.
\newblock Tinytl: Reduce memory, not parameters for efficient on-device
  learning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em ECCV}, 2020.

\bibitem{chen2021crossvit}
Chun-Fu~Richard Chen, Quanfu Fan, and Rameswar Panda.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image
  classification.
\newblock In {\em ICCV}, 2021.

\bibitem{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock In {\em CVPR}, 2020.

\bibitem{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL-HLT}, 2018.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2021.

\bibitem{diabetic-retinopathy-detection}
Will~Cukierski Emma~Dugas, Jared~Jorge.
\newblock Diabetic retinopathy detection, 2015.

\bibitem{ermolov2022hyperbolic}
Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, and Ivan
  Oseledets.
\newblock Hyperbolic vision transformers: Combining improvements in metric
  learning.
\newblock In {\em CVPR}, 2022.

\bibitem{fournier2021practical}
Quentin Fournier, Ga{\'e}tan~Marceau Caron, and Daniel Aloise.
\newblock A practical survey on faster and lighter transformers.
\newblock {\em arXiv preprint arXiv:2103.14636}, 2021.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em ICLR}, 2019.

\bibitem{ganea2018hyperbolic}
Octavian Ganea, Gary B{\'e}cigneul, and Thomas Hofmann.
\newblock Hyperbolic neural networks.
\newblock In {\em NeurIPS}, 2018.

\bibitem{gao2022visual}
Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li,
  and Dimitris~N Metaxas.
\newblock Visual prompt tuning for test-time domain adaptation.
\newblock {\em arXiv preprint arXiv:2210.04831}, 2022.

\bibitem{gebru2017fine}
Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li
  Fei-Fei.
\newblock Fine-grained car detection for visual census estimation.
\newblock In {\em AAAI}, 2017.

\bibitem{guo2020parameter}
Demi Guo, Alexander~M Rush, and Yoon Kim.
\newblock Parameter-efficient transfer learning with diff pruning.
\newblock In {\em ICML}, 2021.

\bibitem{han2022survey}
Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu,
  Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et~al.
\newblock A survey on vision transformer.
\newblock {\em IEEE TPAMI}, 2022.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock {\em NeurIPS}, 2015.

\bibitem{hassibi1992second}
Babak Hassibi and David Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In {\em NeurIPS}, 1992.

\bibitem{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em CVPR}, 2022.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em ICCV}, 2015.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{he2022parameter}
Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin~Eric Wang.
\newblock Parameter-efficient fine-tuning for vision transformers.
\newblock {\em arXiv preprint arXiv:2203.16329}, 2022.

\bibitem{he2022hyperprompt}
Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao,
  YaGuang Li, Zhao Chen, Donald Metzler, et~al.
\newblock Hyperprompt: Prompt-based task-conditioning of transformers.
\newblock In {\em ICML}, 2022.

\bibitem{huang2020hand}
Lin Huang, Jianchao Tan, Ji Liu, and Junsong Yuan.
\newblock Hand-transformer: non-autoregressive structured modeling for 3d hand
  pose estimation.
\newblock In {\em ECCV}, 2020.

\bibitem{huang2020hot}
Lin Huang, Jianchao Tan, Jingjing Meng, Ji Liu, and Junsong Yuan.
\newblock Hot-net: Non-autoregressive transformer for 3d hand-object pose
  estimation.
\newblock In {\em ACMMM}, 2020.

\bibitem{innes2019differentiable}
Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckas, Elliot Saba, Viral~B
  Shah, and Will Tebbutt.
\newblock A differentiable programming system to bridge machine learning and
  scientific computing.
\newblock {\em arXiv preprint arXiv:1907.07587}, 2019.

\bibitem{iofinova2022well}
Eugenia Iofinova, Alexandra Peste, Mark Kurtz, and Dan Alistarh.
\newblock How well do sparse imagenet models transfer?
\newblock In {\em CVPR}, 2022.

\bibitem{islam2022recent}
Khawar Islam.
\newblock Recent advances in vision transformer: A survey and outlook of recent
  work.
\newblock {\em arXiv preprint arXiv:2203.01536}, 2022.

\bibitem{jia2022visual}
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath
  Hariharan, and Ser-Nam Lim.
\newblock Visual prompt tuning.
\newblock In {\em ECCV}, 2022.

\bibitem{jia2021exploring}
Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie, Serge Belongie, and
  Ser-Nam Lim.
\newblock Exploring visual engagement signals for representation learning.
\newblock In {\em ICCV}, 2021.

\bibitem{ju2022prompting}
Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie.
\newblock Prompting visual-language models for efficient video understanding.
\newblock In {\em ECCV}, 2022.

\bibitem{kaiser2017one}
Lukasz Kaiser, Aidan~N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion
  Jones, and Jakob Uszkoreit.
\newblock One model to learn them all.
\newblock In {\em ICML}, 2017.

\bibitem{khan2022transformers}
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed~Waqas Zamir, Fahad~Shahbaz
  Khan, and Mubarak Shah.
\newblock Transformers in vision: A survey.
\newblock {\em ACM Computing Surveys}, 54(10s):1--41, 2022.

\bibitem{khosla2011novel}
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li.
\newblock Novel dataset for fine-grained image categorization: Stanford dogs.
\newblock In {\em CVPR Workshop}, 2011.

\bibitem{khrulkov2020hyperbolic}
Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan Oseledets, and
  Victor Lempitsky.
\newblock Hyperbolic image embeddings.
\newblock In {\em CVPR}, 2020.

\bibitem{lecun1989optimal}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock In {\em NeurIPS}, 1989.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In {\em EMNLP}, 2021.

\bibitem{li2022automated}
Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang, and Yi
  Yang.
\newblock Automated progressive learning for efficient training of vision
  transformers.
\newblock In {\em CVPR}, 2022.

\bibitem{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock {\em arXiv preprint arXiv:1608.08710}, 2016.

\bibitem{li2021improved}
Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Mvitv2: Improved multiscale vision transformers for classification
  and detection.
\newblock In {\em CVPR}, 2022.

\bibitem{liang2023clustseg}
James Liang, Tianfei Zhou, Dongfang Liu, and Wenguan Wang.
\newblock Clustseg: Clustering for universal segmentation.
\newblock {\em arXiv preprint arXiv:2305.02187}, 2023.

\bibitem{lin2021traceability}
Jinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang.
\newblock Traceability transformed: Generating more accurate links with
  pre-trained bert models.
\newblock In {\em ICSE}, 2021.

\bibitem{lin2021end}
Kevin Lin, Lijuan Wang, and Zicheng Liu.
\newblock End-to-end human pose and mesh reconstruction with transformers.
\newblock In {\em CVPR}, 2021.

\bibitem{lin2022survey}
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.
\newblock A survey of transformers.
\newblock {\em AI Open}, 2022.

\bibitem{liu2020video}
Dongfang Liu, Yiming Cui, Yingjie Chen, Jiyong Zhang, and Bin Fan.
\newblock Video object detection for autonomous driving: Motion-aid feature
  calibration.
\newblock {\em Neurocomputing}, 409:1--11, 2020.

\bibitem{liu2021sg}
Dongfang Liu, Yiming Cui, Wenbo Tan, and Yingjie Chen.
\newblock Sg-net: Spatial granularity network for one-stage video instance
  segmentation.
\newblock In {\em CVPR}, 2021.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock In {\em ICLR}, 2020.

\bibitem{liu2022swin}
Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li Dong, et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock In {\em CVPR}, 2022.

\bibitem{liu2021swin}
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em ICCV}, 2021.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em ICML}, 2017.

\bibitem{lu2023transflow}
Yawen Lu, Qifan Wang, Siqi Ma, Tong Geng, Yingjie~Victor Chen, Huaijin Chen,
  and Dongfang Liu.
\newblock Transflow: Transformer as flow learner.
\newblock In {\em CVPR}, 2023.

\bibitem{ma2022xprompt}
Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun Quan,
  and Dawei Song.
\newblock Xprompt: Exploring the extreme of prompt tuning.
\newblock In {\em EMNLP}, 2022.

\bibitem{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In {\em ECCV}, 2018.

\bibitem{mcinnes2018umap}
Leland McInnes, John Healy, and James Melville.
\newblock Umap: Uniform manifold approximation and projection for dimension
  reduction.
\newblock {\em arXiv preprint arXiv:1802.03426}, 2018.

\bibitem{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock In {\em NeurIPS}, 2019.

\bibitem{narkhede2022review}
Meenal~V Narkhede, Prashant~P Bartakke, and Mukul~S Sutaone.
\newblock A review on weight initialization strategies for neural networks.
\newblock {\em Artificial Intelligence Review}, 2022.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em Indian Conference on Computer Vision, Graphics \& Image
  Processing}, 2008.

\bibitem{nishant2020artificial}
Rohit Nishant, Mike Kennedy, and Jacqueline Corbett.
\newblock Artificial intelligence for sustainability: Challenges,
  opportunities, and a research agenda.
\newblock {\em International Journal of Information Management}, 2020.

\bibitem{noroozi2016unsupervised}
Mehdi Noroozi and Paolo Favaro.
\newblock Unsupervised learning of visual representations by solving jigsaw
  puzzles.
\newblock In {\em ECCV}, 2016.

\bibitem{pan20213d}
Xuran Pan, Zhuofan Xia, Shiji Song, Li~Erran Li, and Gao Huang.
\newblock 3d object detection with pointformer.
\newblock In {\em CVPR}, 2021.

\bibitem{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em NeurIPS}, 2019.

\bibitem{peng2021hyperbolic}
Wei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, and Guoying Zhao.
\newblock Hyperbolic deep neural networks: A survey.
\newblock {\em IEEE TPAMI}, 2021.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em The Journal of Machine Learning Research}, 2020.

\bibitem{rebuffi2017learning}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock In {\em NeurIPS}, 2017.

\bibitem{ren2023prepare}
Yi Ren, Shangmin Guo, Wonho Bae, and Danica~J Sutherland.
\newblock How to prepare your task head for finetuning.
\newblock In {\em ICLR}, 2023.

\bibitem{riquelme2021scaling}
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe
  Jenatton, Andr{\'e} Susano~Pinto, Daniel Keysers, and Neil Houlsby.
\newblock Scaling vision with sparse mixture of experts.
\newblock In {\em NeurIPS}, 2021.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{strudel2021segmenter}
Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock In {\em ICCV}, 2021.

\bibitem{tajbakhsh2016convolutional}
Nima Tajbakhsh, Jae~Y Shin, Suryakanth~R Gurudu, R~Todd Hurst, Christopher~B
  Kendall, Michael~B Gotway, and Jianming Liang.
\newblock Convolutional neural networks for medical image analysis: Full
  training or fine tuning?
\newblock {\em IEEE Transactions on Medical Imaging}, 2016.

\bibitem{van2015building}
Grant Van~Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos
  Ipeirotis, Pietro Perona, and Serge Belongie.
\newblock Building a bird recognition app and large scale dataset with citizen
  scientists: The fine print in fine-grained dataset collection.
\newblock In {\em CVPR}, 2015.

\bibitem{nabirds}
Grant Van~Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos
  Ipeirotis, Pietro Perona, and Serge Belongie.
\newblock Building a bird recognition app and large scale dataset with citizen
  scientists: The fine print in fine-grained dataset collection.
\newblock In {\em CVPR}, 2015.

\bibitem{van2021sustainable}
Aimee Van~Wynsberghe.
\newblock Sustainable ai: Ai for sustainability and the sustainability of ai.
\newblock {\em AI and Ethics}, 2021.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, 2017.

\bibitem{vinuesa2020role}
Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia
  Dignum, Sami Domisch, Anna Fell{\"a}nder, Simone~Daniela Langhans, Max
  Tegmark, and Francesco Fuso~Nerini.
\newblock The role of artificial intelligence in achieving the sustainable
  development goals.
\newblock {\em Nature Communications}, 2020.

\bibitem{wah2011caltech}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge
  Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem{wang2021max}
Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.
\newblock Max-deeplab: End-to-end panoptic segmentation with mask transformers.
\newblock In {\em CVPR}, 2021.

\bibitem{wang2022visual}
Wenguan Wang, Cheng Han, Tianfei Zhou, and Dongfang Liu.
\newblock Visual recognition with deep nearest centroids.
\newblock In {\em ICLR}, 2022.

\bibitem{wang2022learning}
Wenguan Wang, James Liang, and Dongfang Liu.
\newblock Learning equivariant segmentation with instance-unique querying.
\newblock In {\em NeurIPS}, 2022.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In {\em ICCV}, 2021.

\bibitem{wang2021end}
Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen,
  and Huaxia Xia.
\newblock End-to-end video instance segmentation with transformers.
\newblock In {\em CVPR}, 2021.

\bibitem{wu2022sustainable}
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
  Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et~al.
\newblock Sustainable ai: Environmental implications, challenges and
  opportunities.
\newblock {\em Proceedings of Machine Learning and Systems}, 2022.

\bibitem{xing2022class}
Yinghui Xing, Qirui Wu, De Cheng, Shizhou Zhang, Guoqiang Liang, and Yanning
  Zhang.
\newblock Class-aware visual prompt tuning for vision-language pre-trained
  model.
\newblock {\em arXiv preprint arXiv:2208.08340}, 2022.

\bibitem{yang2023mixpave}
Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian
  Khabsa, Sinong Wang, Zenglin Xu, and Dongfang Liu.
\newblock Mixpave: Mix-prompt tuning for few-shot product attribute value
  extraction.
\newblock In {\em ACL}, 2023.

\bibitem{yang2021transpose}
Sen Yang, Zhibin Quan, Mu Nie, and Wankou Yang.
\newblock Transpose: Keypoint localization via transformer.
\newblock In {\em ICCV}, 2021.

\bibitem{yosinski2014transferable}
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
\newblock How transferable are features in deep neural networks?
\newblock In {\em NeurIPS}, 2014.

\bibitem{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In {\em ICLR}, 2020.

\bibitem{yuan2021temporal}
Zhenxun Yuan, Xiao Song, Lei Bai, Zhe Wang, and Wanli Ouyang.
\newblock Temporal-channel transformer for 3d lidar-based video object
  detection for autonomous driving.
\newblock {\em IEEE Transactions on Circuits and Systems for Video Technology},
  2021.

\bibitem{zang2022unified}
Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen~Change Loy.
\newblock Unified vision and language prompt learning.
\newblock {\em arXiv preprint arXiv:2210.07225}, 2022.

\bibitem{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In {\em CVPR}, 2022.

\bibitem{zhai2019large}
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos
  Riquelme, Mario Lucic, Josip Djolonga, Andre~Susano Pinto, Maxim Neumann,
  Alexey Dosovitskiy, et~al.
\newblock A large-scale study of representation learning with the visual task
  adaptation benchmark.
\newblock {\em arXiv preprint arXiv:1910.04867}, 2019.

\bibitem{zhang2022patchformer}
Cheng Zhang, Haocheng Wan, Xinyi Shen, and Zizhao Wu.
\newblock Patchformer: An efficient point transformer with patch attention.
\newblock In {\em CVPR}, 2022.

\bibitem{zhang2020side}
Jeffrey~O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra
  Malik.
\newblock Side-tuning: a baseline for network adaptation via additive side
  networks.
\newblock In {\em ECCV}, 2020.

\bibitem{zhang2016colorful}
Richard Zhang, Phillip Isola, and Alexei~A Efros.
\newblock Colorful image colorization.
\newblock In {\em ECCV}, 2016.

\bibitem{zheng2021rethinking}
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,
  Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip~HS Torr, et~al.
\newblock Rethinking semantic segmentation from a sequence-to-sequence
  perspective with transformers.
\newblock In {\em CVPR}, 2021.

\bibitem{zhu2020deformable}
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
\newblock Deformable detr: Deformable transformers for end-to-end object
  detection.
\newblock In {\em ICLR}, 2021.

\bibitem{zhuang2023survey}
Bohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, and Chunhua Shen.
\newblock A survey on efficient training of transformers.
\newblock {\em arXiv preprint arXiv:2302.01107}, 2023.

\end{thebibliography}
