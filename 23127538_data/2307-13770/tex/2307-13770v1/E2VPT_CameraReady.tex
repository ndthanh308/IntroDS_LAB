    \documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{color,xcolor}
\definecolor{fired}{RGB}{222,82,57}
\definecolor{iceblue}{RGB}{33,102,200}
\usepackage[export]{adjustbox}
% \usepackage{ulem}
\definecolor{mygray}{gray}{.9}

\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{6523} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{E$^2$VPT: An Effective and Efficient Approach for Visual Prompt Tuning 
% for Parameter-Efficient Learning
}
\author{$\text{Cheng Han}^{1}$, $\text{Qifan Wang}^{2}$, $\text{Yiming Cui}^{3}$, $\text{Zhiwen Cao}^{4}$, $\text{Wenguan Wang}^{5}$, $\text{Siyuan Qi}^{6}$, $\text{Dongfang Liu}^{1}$\thanks{Corresponding author} \\
$\text{Rochester Institute of Technology}^{1}$, $\text{Meta AI}^{2}$, $\text{University of Florida}^{3}$, $\text{Purdue University}^{4}$ \\
$\text{Zhejiang University}^{5}$, $\text{BIGAI}^{6}$\thanks{National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence} \\
{{\tt\small \{ch7858,~dongfang.liu\}@rit.edu, wqfcr@fb.com, cuiyiming@ufl.edu, cao270@purdue.edu}} \\
{{\tt\small wenguanwang.ai@gmail.com, syqi@bigai.ai}}
}
% {{\tt\small \{ch7858, dongfang.liu\}@rit.edu, \tt\small wqfcr@fb.com, \tt\small wqfcr@fb.com }}
% \author{Cheng Han\\
% % Rochester Institute of Technology\\
% % Institution1 address\\
% % {\tt\small ch7858@rit.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Qifan Wang\\
% % Institution2\\
% % Meta AI \\
% % {\tt\small wqfcr@fb.com}
% \and
% Yiming Cui \\
% % University of Florida \\
% \and
% Zhiwen Cao \\
% % Purdue University \\
% \and
% Wenguan Wang \\
% % Zhejiang University \\
% \and 
% Siyuan Qi \\
% % UCLA \\
% \and 
% Dongfang Liu
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
% As high-performance models become increasingly large, model adaptation becomes more energy-intensive
%As transformer-based, large-scale models continue to grow in size, model adaptation becomes increasingly parameter-intensive. 
% Consequently, there is a growing need to optimize the parameter usage of these models, which has emerged as a critical research area. 
% To address this challenge, prevalent parameter-efficiency solutions 
% can be generally categorized threefold: partial tuning, extra module, and prompt tuning methods, which
% prioritize reducing the number of finetuning parameters under the \textit{pretrain-then-finetune} paradigm and seldom consider the network architectures in the algorithmic design. 
%To address this challenge, many prevalent parameter-efficiency solutions aim to reduce the number of tuning parameters under the pretrain-then-finetune paradigm, without considering the network architectures in algorithmic design.
% that can accommodate the complexities of large models. 
% These methods are therefore insufficient for achieving optimal performance. 
% A more holistic approach is necessary to fully leverage the benefits of large models under \textit{pretrain-then-finetune} paradigm. 
% Prevalent parameter-efficiency solutions for large model adaptation can be generally categorized threefold: partial tuning, extra module, and prompt tuning methods. These solutions reduce learnable parameters in a heuristic manner when compared to full fine-tuning. However, they are primarily based on convolutional neural networks and do not take into account the inner structure of transformer-based architectures. 
% In light of this view, this study uncovers the limitations of such methods, and proposes Generalized Prompt Tuning to carefully consider the inner design of transformer-based architectures. 
% We further explore the acme of parameter-efficiency and suggest a pruning and rewinding stage to significantly reduce the parameter usage in input prompts.
% (\ie, $vs$ to Full fine-tuning). 
%As transformer-based, large-scale models continue to grow in size, model adaptation becomes increasingly parameter-intensive. 
%To address this challenge, many prevalent parameter-efficiency solutions aim to reduce the number of tuning parameters under the pretrain-then-finetune paradigm, without considering the network architectures in algorithmic design.
%without considering the network architectures in algorithmic design.
%In contrast to the prior arts, our insight of this work is to strategically design a tuning scheme towards transformer-based, large-scale vision architectures by adding learnable vectors into self-attention layers. We propose an effective and efficient visual prompt tuning that (1) injects prompts to both inputs and backbones to effectively boost the performance, and (2) efficiently prunes the systemic parameters while preserving performance gains. 
%by introducing learnable vectors into self-attention layers. Our approach effectively and efficiently tunes visual prompts, injecting them into both inputs and backbones to boost performance while pruning systemic parameters and preserving performance gains.
% As the size of transformer-based models continues to increase, fine-tuning these large-scale pretrained models to new tasks has become increasingly parameter-intensive.
% To address this challenge, several prevalent parameter-efficient solutions have aimed to reduce the number of tunable parameters during fine-tuning and achieved promising results.
% However, there are two main limitations in existing parameter-efficient methods. 
As the size of transformer-based models continues to grow, fine-tuning these large-scale pretrained vision models for new tasks has become increasingly parameter-intensive.
Parameter-efficient learning has been developed to reduce the number of tunable parameters during fine-tuning. Although these methods show promising results, there is still a significant performance gap compared to full fine-tuning.
To address this challenge, 
% several prevalent parameter-efficient solutions have aimed to reduce the number of tunable parameters during fine-tuning. Despite achieving promising results, there are two main limitations in existing parameter-efficient methods.
% Firstly, they either fine-tune a partial backbone (such as the top transformer layer) or insert a side module into the backbone but ignore the transformer architectures in algorithmic design, resulting in a large performance gap with full fine-tuning. Secondly, they usually need to fine-tune a relatively large number of parameters to achieve reasonable performance and fail to explore the extremes of parameter efficiency.
% In this paper, 
we propose an Effective and Efficient Visual Prompt Tuning (E$^2$VPT) approach for large-scale transformer-based model adaptation. Specifically, we introduce a set of learnable key-value prompts and visual prompts into self-attention and input layers, respectively, to improve the effectiveness of model fine-tuning. Moreover, we design a prompt pruning procedure to systematically prune low importance prompts while preserving model performance, which largely enhances the model's efficiency.
Empirical results demonstrate that our approach outperforms several state-of-the-art baselines on two benchmarks, with considerably low parameter usage (\eg, 0.32\% of model parameters on VTAB-1k). Our code is available at \href{https://github.com/ChengHan111/E2VPT}{https://github.com/ChengHan111/E2VPT}.
% We anticipate that this work will inspire further exploration within the pretrain-then-finetune paradigm for large-scale models.
%The empirical results demonstrate the superior performance of our approach over several state-of-the-art baselines on two benchmarks (\ie, FGVC, VTAB-1k) using different backbones (\ie, ViT, Swin), with considerably low parameter usage (averagely 0.29\% of model parameters).
%We anticipate that this work will inspire further exploration within the pretrain-then-finetune paradigm, and we will make our code publicly available.



\end{abstract}


%%%%%%%%% BODY TEXT
\vspace{-1em}
\section{Introduction}
\label{sec:introduction}
%The development of artificial intelligence (AI) should not only focus on performance advances but also prioritize sustainable deployment~\cite{nishant2020artificial,van2021sustainable,vinuesa2020role,wu2022sustainable}. Given the captivating pursuit of performance improvements in visual-related tasks, the size of present models become increasingly large and training for such models is energy-costy and computational-expensive~\cite{innes2019differentiable, sanh2019distilbert, you2019large}. 
The development of artificial intelligence (AI) should not only prioritize performance advances, but also emphasize sustainable deployment~\cite{nishant2020artificial,van2021sustainable,vinuesa2020role,wu2022sustainable}. Despite the captivating pursuit of performance improvements in visual-related tasks, the size of present models has been rapidly increasing, resulting in energy-intensive and computationally expensive training~\cite{innes2019differentiable,sanh2019distilbert,you2019large}. 
%Therefore, sustainable development in AI should also include reducing the carbon footprint and energy consumption of AI models, alongside performance advancements.
%It is essential to stress the importance of parameter efficiency and fast development. 
% Currently, visual-related models are dominated by transformer-based architectures, \eg, ViT-Huge~\cite{dosovitskiy2020image}~(632M parameters) and Swin-Large~\cite{liu2021swin}~(197M parameters). They contain significantly larger parameters when compared to Convolutional Neural Network (CNN) counterparts, \eg, ResNet~\cite{he2016deep}~(25M parameters). Training such a large model \textit{from scratch} faces challenges such as limited data~\cite{tajbakhsh2016convolutional, brown2020language, guo2020parameter} and slow convergence at low accuracy~\cite{lin2021traceability, kaiser2017one}).
% A common paradigm to overcome these challenges is \textit{pretrain-then-finetune}, which reduces the need to recall vast quantities of training data and enables speedy processing of various visual tasks. However, the most common method under this paradigm, \textit{full fine-tuning}, stores and deploys a complete copy of the backbone parameter to every single task~\cite{jia2022visual}. It is still parameter and computational expensive and not conducive to fast deployment. 
Transformer-based architectures currently dominate visual-related models, such as ViT-Huge~\cite{dosovitskiy2020image} (632M) and Swin-Large~\cite{liu2021swin} (197M), with significantly more parameters than the Convolutional Neural Networks (CNN) like ResNet~\cite{he2016deep} (25M). 
Training such large models from scratch presents challenges such as limited data~\cite{brown2020language,guo2020parameter,tajbakhsh2016convolutional} and slow convergence at low accuracy~\cite{kaiser2017one,lin2021traceability}. 
A common paradigm to overcome these challenges is \textit{pretrain-then-finetune}, which reduces the need for vast amounts of training data and speeds up processing of various visual tasks. However, the traditional \textit{full fine-tuning} involves storing and deploying a complete copy of the backbone parameters for every single task~\cite{jia2022visual}, which remains computationally expensive and not suitable for fast model deployment.
% It is a common paradigm to adapt the \textit{pretrain-then-finetune} to leverage the gap between different tasks. The motivation is simple yet effective since it reduces the need and effort to recall vast quantities of training data, and enables speedy processing of various visual tasks. 
% \textcolor{red}{In practical terms, the aforementioned paradigm is not sustainable due to inherent issues.} 
% The most common method under this paradigm is full \textit{fine-tuning}. It stores and deploys a complete copy of the backbone parameter to every single task~\cite{jia2022visual}, which is still parameter and computational expensive, and against the intention for fast deployment. 

% Current pursue on performance makes the model becoming large (\ie, compared to early convolutional network...., current big model, most of the current large model on visual related tasks are transformer-based architectures, such as vit, swin (transformer-based models)...) and energy costy (\ie, ) While training such a large model for specific tasks from scratch faces several challenges (far less data, slow training on a new task, lower accuracy.), it is a common sense to adapt the pretrained then fine-tuning paradigm to leverage the gap between different tasks. The motivation is to reduce the need and effort to recollect the massive amounts of training data. In practice, however, this paradigm gets its own challenges, making it unsustainable. A common example on pretrained then fine-tuning paradigm is fully fine-tuning of the pretrained model tasks in an end-to-end manner. However, this strategy requires one to store and deploy a separate copy of the backbone parameters for every single task, which is param expensive and against the original intention of ... for fast deployment. 

% Figure environment removed

% To reduce the number of costly learnable parameters, previous efforts under this paradigm try to train only limited parts of the network,
To address this issue, various approaches have been developed, which can be divided into three main categories (see Fig.~\ref{fig:plot1}): partial tuning, extra module, and prompt tuning methods.
% partial tuning, extra module and prompt tuning methods. 
\textit{Partial tuning} methods~\cite{chen2021empirical,jia2021exploring,mahajan2018exploring} only fine-tune part of the backbone, such as the classifier head or last few layers, while freezing the others. \textit{Extra module} methods insert learnable bias term~\cite{cai2020tinytl} or additional adapters~\cite{rebuffi2017learning, zhang2020side} to the network for adaptation. \textit{Prompt tuning} methods add prompt tokens~\cite{jia2022visual, ju2022prompting, zang2022unified} to the input layer of the transformer without changing or fine-tuning the backbone itself.
% To address this issue, various approaches have been developed and can be categorized based on their tuning scope~\cite{jia2022visual}. Three main approaches are commonly used (as shown in Fig.\ref{fig:plot1}): \textit{partial tuning} methods, \textit{extra module} methods, and \textit{prompt tuning} methods. \textit{Partial tuning} methods freeze all other parameters except for the classifier head (\eg, \cite{mahajan2018exploring, jia2021exploring, chen2021empirical}). \textit{Extra module} methods insert a learnable bias term\cite{cai2020tinytl} or additional adapters~\cite{rebuffi2017learning, zhang2020side} into the network for adaptation. \textit{Prompt tuning} methods modify the prompt tokens \cite{jia2022visual, ju2022prompting, zang2022unified} as input to the transformer without changing or fine-tuning the transformer itself.
 % , and  introduces learnable bias term, join additional adapter, or .
%  Noticing the blossoming methods,
% % the existing various methods on reducing leanrable parameter under \textit{pretrained then fine-tuning} paradigm, 
% a question naturally arises: \textbf{\ding{182}} \textit{What are the relations and limitations of the current bandwagon?} Answering question \textbf{\ding{182}} can bring fundamental insights into system design, and motivate us to rethink the task from a sustainable prospective. 
%As detailed in \S\ref{subsec:prior_arts}, 
% to answer question \textbf{\ding{182}}, 
% These methods all fall under the \textit{pretrain-then-finetune} paradigm which reduces learnable parameters when compared to full fine-tuning~\cite{mahajan2018exploring, jia2021exploring, chen2021empirical, rebuffi2017learning, zhang2020side}. However, current partial tuning and extra module methods do not generally outperform full fine-tuning in accuracy. Prompt tuning methods~\cite{jia2022visual, ju2022prompting, zang2022unified}, on the other hand, shows its potential as it is the first approach to achieve exceeding accuracy while using lower parameters than the full fine-tuning strategy. While other methods on transformer are derived from prior designs for CNNs, prompt tuning methods reexamine and take the architecture of transformers into consideration by altering its input, which is intentionally crafted to suit the transformer-based architecture. While being simple and superior to prior arts, it does not scrutinize the core design of the transformer --- self-attention mechanism.
All of these methods operate within the \textit{pretrain-then-finetune} paradigm, which reduces the number of learnable parameters compared to full fine-tuning~\cite{chen2021empirical,jia2021exploring,mahajan2018exploring,rebuffi2017learning, zhang2020side}. 
However, despite achieving promising results, there are two main limitations in existing parameter-efficient methods.
\textbf{Firstly}, they do not scrutinize the core architecture of the transformer's self-attention mechanism, resulting in a large performance gap with full fine-tuning. \textbf{Secondly}, they usually need to fine-tune a relatively large number of parameters to achieve reasonable performance and fail to explore the extremes of parameter efficiency.

%However, current partial tuning and extra module methods generally have large performance gap with full fine-tuning. On the other hand, recent \textit{prompt tuning} methods~\cite{jia2022visual, ju2022prompting, zang2022unified} have shown on par or even superior performance than full fine-tuning while using fewer parameters. However, they do not scrutinize the core architecture of the transformer's self-attention mechanism.

% Unlike other methods for transformers that are adapted from designs for CNNs, prompt tuning methods reconsider the architecture of transformers by modifying its input, which is specifically designed to suit the transformer-based architecture. Despite being simple and superior to previous approaches, prompt tuning methods do not scrutinize the core design of the transformer's self-attention mechanism.


The perspective outlined above leads to two fundamental questions: \textit{\textbf{\ding{182}} How can we establish the \textbf{effectiveness} of prompt tuning for large-scale transformer-based vision models? \textbf{\ding{183}} How can we explore the extremes of parameter \textbf{efficiency} to reduce the number of tunable parameters?}
These two questions are the foundation of our work. 
%We argue that it is possible to achieve superior performance while using fewer parameters. 
The intuition is that instead of solely focusing on modifying inputs, as in previous prompt tuning methods, we should explicitly investigate the potential of improving the self-attention mechanism during fine-tuning, and explore the extremes of parameter efficiency.

%These two questions form the basis of our work. Our argument is that it is possible to achieve superior performance while utilizing fewer parameters. Instead of solely focusing on modifying inputs as done in previous prompt tuning methods, we should intentionally investigate the potential of the self-attention mechanism during fine-tuning.

% In the light of this view, a question naturally arises: \textbf{\ding{182}} 
% % \textit{Can we systematically consider the relation between \textbf{prompt tuning} methods and self-attention mechanism for transformer-based models?}
% \textit{
% % Considering the superior performance of transformer-based architectures, 
% How to tailor an effective tuning strategy for large-scale, transformer-based vision models?}
% % one of the primary goals for 
% %  \textit{pretrain-then-finetune} 
% % is 
% In order to minimize the fine-tuning parameters,
% we also want to ask: \textbf{\ding{183}} \textit{
% How to explore the extreme of  parameter efficiency during fine-tuning?
% % Is there a strategy to limit parameter usage even further?
% }$_{\!}$ The two epistemological inquiries drive our work.
% % we first revisit transformer and its self-attention mechanism in \S\ref{subsec:attention_anatomy}.$_{\!}$ 
% \textcolor{red}{Our assertion is that there is plenty of room for achieving superior performance (effectivenesss) with lower parameter usage (efficiency). Rather than exclusively concentrating on altering inputs like previous prompt tuning approaches, we contend that the potent of self-attention mechanism deserves a deliberate exploration during fine-tuning.}
% Therefore, it is crucial to give due consideration to the potential of the self-attention mechanism during fine-tuning, as this informs our approach to design.
% We contend that the potent of self-attention mechanism should be carefully considered during fine-tuning, which guides our design.
% With these perspectives, we ask two more questions on current proposed methods


% Our epistemology for this work is driven by the two core research questions.
% and provide fundamental insights on pretrained then fine-tuning paradigm.

In response to question \textbf{\ding{182}}, we discuss and analyze the self-attention mechanism of the transformer, which is crucial in capturing long-range token dependencies within a global context~\cite{han2022survey,khan2022transformers,lin2022survey}. In additional to the input visual prompts, we introduce learnable key-value prompts and integrate them into the Key and Value matrices in the self-attention layers. The key-value prompts are jointly learned with the input visual prompts during fine-tuning. This approach effectively leverages the well-designed prompt architecture of the transformer, resulting in significant performance improvements. Moreover, it provides a generic plug-and-play prompt module for current transformer architectures, and its fine-tuning solution is conceptually different from all aforementioned arts in the vision domain.

%we elaborately design learnable key-value prompts and prepend them to the Key and Value matrices in the self-attention layers, which can be jointly learned with the input visual prompts (\S\ref{subsec:effective_prompting}). 
%This approach leverages the well-designed prompt architecture of the transformer and leads to substantial performance gains. Furthermore, it presents a generic plug-and-play prompt module for current transformer-based architectures, and its fine-tuning solution is conceptually different from all aforementioned arts in the vision realm.
% of the multihead self-attention (MSA) layer at each transformer block 
% (\S\ref{subsec:kv_pairs}), which
% various prior arts~\cite{xu2021human, ahmad2020transformer, zhu2021long, guo2022visual} prove that self-attention mechanism is known to be central and indispensable to transformer-based models. The \textbf{Q}uery-\textbf{K}ey-\textbf{V}alue dot product attention provides long-range dependencies~\cite{mao2022towards, zhu2021long, yang2021transformer} and a robust inductive bias~\cite{tay2021synthesizer, han2022survey}. 
% Being highly successful and having strong representation capabilities, it becomes intuitive to combine prompt tuning technique in self-attention patterns under \textit{pretrain-then-finetune} paradigm. 
% we intuitively combine current \textbf{prompt tuning} methods and self-attention patterns. 
% we argue that current \textbf{prompt tuning} methods are not sufficient enough to unleash transformers' potential. 
% We therefore consider carefully on the transformer-based input, backbone as well as head, and identify our method as a \textbf{global-oriented} method. 
% By prepending task specific learnable vectors to the Key-Value pairs of the
% multihead self-attention layer at each transformer block, 
% The task-specific attention 
% presenting a general plug-and-play prompt tuning solution to current transformer-based architectures.
% We define the new paradigm as \textit{global-oriented}. 
% We further discuss in \S\ref{subsec:kv_pairs}.

Motivated by \textbf{\ding{183}}, we propose a pruning strategy to further reduce the number of parameters while maintaining the model performance. 
Our approach draws inspiration from the lottery ticket hypothesis (LTH)~\cite{frankle2018lottery, zhuang2023survey}, which posits that for a given task, there exists a sub-network that can match the test accuracy of the original over-parameterized network without the unnecessary weights~\cite{han2015learning,hassibi1992second,lecun1989optimal,li2022automated,li2016pruning}. Building on this paradigm, we revisit the core design of prompt tuning methods and further reduce the number of learnable parameters. Specifically, we aim to retain the prompt tokens that contribute significantly to the performance, while pruning the prompt tokens that are redundant or unnecessary during fine-tuning. By pruning these unnecessary prompts, we can significantly improve the prompt tuning efficiency while maintaining the performance.

%Our inspiration for this approach comes from the lottery ticket hypothesis (LTH)~\cite{frankle2018lottery, zhuang2023survey},  which suggests that for a given task, there exists a subnetwork that can match the test accuracy of the original over-parameterized network without the unnecessary weights (pruning) ~\cite{lecun1989optimal, hassibi1992second, han2015learning, li2016pruning, li2022automated}. The subnetwork is instantiated as a lottery ticket and the accumulation of tickets is named as winning tickets~\cite{liang2021super}. Under this paradigm, we revisit the core design of fine-tuning methods --- reduce learnable parameters if at all feasible. We define winning tickets as the collection of prompt tokens that bring fundamental performance gains, while losing tickets are the prompt tokens that are unnecessary during training. By pruning these unnecessary learnable parameters during training, we achieve a considerably lower number of parameters.
% Specifically, building upon the idea of 

To answer question \textbf{\ding{182}}-\textbf{\ding{183}}, we propose \textbf{E$^{2}$VPT}, namely \textbf{E}ffective and \textbf{E}fficient \textbf{V}isual \textbf{P}rompt \textbf{T}uning. ${\rm E^{2}VPT}$ is a novel prompt tuning framework that is both architecture-aware and pruning-anchored (see Fig.~\ref{fig:plot1}). 
% we propose Generalized Prompt Tuning and formalize our method as a pruning based prompt tuning framework considering transformers' architectural design
% (see Fig.\ref{fig:plot2}). 
In \S\ref{sec:related_work}, we conduct a literature review and discuss relevant works. Our proposed approach is presented in \S\ref{sec:mixed_prompt}, where we describe in detail how we design visual and key-value prompts to achieve superior performance with fewer parameters.$_{\!}$  In~\S\ref{sec:Experiment}, we$_{\!}$  present$_{\!}$  compelling$_{\!}$  experimental$_{\!}$  results$_{\!}$  on$_{\!}$  various$_{\!}$  benchmarks,$_{\!}$  backbones,$_{\!}$  and$_{\!}$  different$_{\!}$  pretraining$_{\!}$  objectives.$_{\!}$  Specifically, our$_{\!}$  approach$_{\!}$  achieves$_{\!}$  an$_{\!}$  average$_{\!}$  improvement$_{\!}$  of \textbf{5.85\%} in accuracy on VTAB-1k compared$_{\!}$  to$_{\!}$  full$_{\!}$  fine-tuning, and \textbf{1.99\%} compared to VPT~\cite{jia2022visual}. Moreover,$_{\!}$  our$_{\!}$  approach$_{\!}$  uses$_{\!}$  considerably$_{\!}$  fewer$_{\!}$  learnable$_{\!}$  parameters$_{\!}$  than$_{\!}$  existing$_{\!}$  methods,$_{\!}$  accounting$_{\!}$  for$_{\!}$  an$_{\!}$  average of only \textbf{0.32\%} of the backbone parameters on VTAB-1k, whereas VPT on average requires 0.68\% (see Fig.~\ref{fig:plot1}).$_{\!}$  We$_{\!}$  further$_{\!}$  demonstrate$_{\!}$  and$_{\!}$  explain$_{\!}$  the$_{\!}$  superiority$_{\!}$  of$_{\!}$  our$_{\!}$  approach$_{\!}$  over$_{\!}$  VPT$_{\!}$  with$_{\!}$  hyperbolic$_{\!}$  visualization.$_{\!}$  Finally,$_{\!}$  we$_{\!}$  demonstrate$_{\!}$  the$_{\!}$  strong$_{\!}$  algorithmic$_{\!}$  generalization$_{\!}$  of$_{\!}$  our$_{\!}$  approach$_{\!}$  to$_{\!}$  the$_{\!}$  language$_{\!}$  domain$_{\!}$  in$_{\!}$  the$_{\!}$ Appendix. $_{\!}$We$_{\!}$ trust$_{\!}$ that$_{\!}$ this$_{\!}$ work$_{\!}$ provides$_{\!}$ valuable$_{\!}$ insights$_{\!}$ into$_{\!}$ related$_{\!}$ fields.
% In section \S\ref{sec:related_work}, we conduct a literature review and discuss relevant works. Our proposed approach is presented in section \S\ref{sec:mixed_prompt}.
% In section \S\ref{sec:Experiment}, we present compelling experimental results (\ie, \textbf{4.92\%} averagely higher in accuracy on VTAB-1K $vs$ full fine-tuning, \textbf{1.92\%} $vs$ VPT~\cite{jia2022visual}) on various benchmarks, backbones, and different pretraining objectives. Additionally, our approach uses considerably fewer learnable parameters than existing methods (\ie, \textbf{0.29\%} averagely of backbone parameters on VTAB-1k, while VPT in average takes 0.65\%, see Fig.~\ref{fig:plot1}). In \S\ref{subsec:Hyperbolic}, we further demonstrate our systemic superiority to a strong baseline \cite{jia2022visual} over visual evidence via hyperbolic feature embedding. Finally, we demonstrate the strong algorithmic generalization to the language domain in Appendix. We feel this work brings fundamental insights into related fields.
% These results are particularly impressive, considering the self-attention and prompt tuning nature of Generalized Prompt Tuning. 
 
% We further present per-tasks results for Table \ref{table:fgvc_vtab_main}, \ref{table:swin}, \ref{table:mae_moco} and \ref{table:before_after}, and extend to language-related tasks in Appendix for reference. 
% Our code will be released.

% Hence the following 3 questions naturally arise: 1. Can be further improve the performance of prompt tuning on transformer-based models in visual related tasks by digging from a systematic view?, 2. can we further decrease the parameter usage of prompt tuning techniques and 3. current prompt tuning techniques take decent performance gain, can we further improve its explanability by showing its intrinsic data structure?
% how to effectively adapting these large trained models to 

% Tackling these three issues can provide insights into prompt tuning model design, and motivate us to rethink
% the task from an intuitive systemic view.

% 1. Big models gain significant performance boost on image related tasks. (\ie transformer-based architecture) Compare to CNN, it reaches much higher results (Benefit from large param capacity and attention mechanism for global dependence). However, with the increasing of model sizes, it becomes harder for training and fine-tuning hyperparameters. Unbenefit to researchers, making it not sustainable for AI development;

% % why transfer learning

% 3. These two issues motivate us to revisit the task from a more systematic view. With this perspective, in \S, we first answer question 1 by pointing out current visual prompt tuning method, only introducing learnable params in cls token and linear head, cannot satisfy the transfer learning needs. We thus provide further contribution on key and value pairs to add more learnable params. Here compared to VPT, we introduced more possible learnable params, hence question 2 becomes more fundamental: 3. can we further prune unnecessary (even harmful) learnable params during training and make the learnbale param space more compactable? 

% 4. Driven by 3, we develop Generalized Prompt Tuning to pursue the extreme of visual transfer learning tasks. The contributions are threefolds: 1. by prompting less than 1\% param (compare to fully fine-tuning), we largely outperform VPT and reach SOTA? performance on visual transfer learning tasks; 2. By introducing pruning techniques, we significantly reduce the params load and maintain total learnable params lower or on-par to VPT; 3. We further investigate data-distribution on hyperbolic space and show robust performance of our methods on various benchmarks.

\vspace{-0.5em}
\section{Related Work}
\label{sec:related_work}
% In this section, we review relevant work on large-scale vision models based on transformer and efficient training.

\vspace{-0.1em}
\subsection{Vision Transformers}
\label{subsec:Vision_transformer}
\vspace{-0.1em}

Inspired by the remarkable success of transformers in natural language processing (NLP)~\cite{brown2020language,devlin2018bert,liu2019roberta, raffel2020exploring,vaswani2017attention, wang2022visual}, researchers have extended the transformer architecture to various supervised vision tasks, including image classification~\cite{dosovitskiy2020image,liu2022swin,liu2021swin, lu2023transflow}, image segmentation~\cite{liang2023clustseg, liu2021sg, strudel2021segmenter,wang2021max, wang2022learning, wang2021end,zheng2021rethinking}, object detection~\cite{beal2020toward,carion2020end,liu2020video,pan20213d,yuan2021temporal,zhu2020deformable} and pose estimation~\cite{huang2020hand, huang2020hot, lin2021end, yang2021transpose}). 
Self-supervised pretraining paradigms~\cite{bao2021beit, chen2021empirical, he2022masked} has also been explored, leading to state-of-the-art results.
transformers dominate in visual-related disciplines due to their superior performance and scalability compared to convolutional neural networks (CNNs)~\cite{he2022parameter, jia2022visual}.  However, the significant computational and parameter overhead required to adapt transformers to various vision tasks cannot be ignored~\cite{fournier2021practical, islam2022recent, zhang2022patchformer}.
For instance, recent transformer-based models such as MViTv2-Large~\cite{li2021improved} (218M), ViT-G~\cite{zhai2022scaling} (1.8B), SwinV2-G~\cite{liu2022swin} (3.0B), and V-MoE~\cite{riquelme2021scaling} (14.7B) incur substantial computational costs. Therefore, we propose ${\rm E^{2}VPT}$, which is designed to reduce the computational cost of transformer-based architectures while maintaining high performance in the \textit{pretrain-then-finetune} paradigm.

% \label{sec:transformer}
% We first briefly review the self-attention mechanism in \S\ref{sec:introduction}, and then discuss question \textbf{\ding{182}} in \S\ref{subsec:prior_arts}.
% \subsection{Efficient Training}
% \label{subsec:efficient_training}

 % \textit{I. Partial tuning} methods learn single classifier heads with other parameters frozen from the pretrained models. Despite being straightforward and simple to implement~\cite{mahajan2018exploring, jia2021exploring, chen2021empirical}, these methods do not perform as well as full fine-tuning. \cite{yosinski2014transferable, zhang2016colorful, noroozi2016unsupervised, he2022masked} consider adapting the last $k$ layers of backbone while freezing the others, by redefining the boundary between backbone and classification head, can also be categorized under partial tuning. 
% one can consider the last $k$ layers of the backbone as head while freeze the above layers, which are categorized into partial tuning methods
% \textit{II. Extra module} methods such as \cite{zhang2020side} add a lightweight "side" network to a pretrained network and fuse it with the fixed part via summation. Other approaches such as \cite{rebuffi2017learning} insert additional residual units (or lite residual learning~\cite{cai2020tinytl}) to the network, which is designed for ResNet-related~\cite{he2016deep} CNN architectures~\cite{sandler2018mobilenetv2}. These methods have provided insights into CNNs' \textit{pretrain-then-finetune} paradigm. However, these methods fall short of accuracy under transformer-based architectures.
% \textit{III. Prompt tuning} methods are initially introduced to Pretrained Language Models (PLMs) by altering the input of transformer-based architectures and only updating these learnable input vectors during training (\ie, prompt tuning). Many works \cite{davison2019commonsense, gong2021prompt, radford2019language, wang2021transprompt, khashabi2020unifiedqa, ma2022xprompt, he2022hyperprompt} prove the competitive performance and parameter-efficiency across various language tasks. Visual-related prompting, inspired by NLP approaches, is relatively new but has shown impressive performance ~\cite{jia2022visual, xing2022class, gao2022visual} and competitiveness with full fine-tuning.  Nevertheless, current methods disregard the inner design of transformer-based architectures. In contrast, our approach is mindful of the architecture and anchored on pruning, which conceptually sets it apart from the methods discussed above.
\subsection{Parameter-efficient Fine-tuning} 
\label{subsec:prior_arts}
Efficient model training has drawn much attention in the vision community, particularly with the rise of Vision Transformers~\cite{arnab2021vivit, chen2021crossvit,dosovitskiy2020image,liu2021swin, wang2021pyramid}. 
However, despite their effectiveness and widespread use, these models are often too large for practical deployment and adaptation. As a result, the \textit{pretrain-then-finetune} paradigm is commonly employed. While full fine-tuning ensures strong performance, it is an expensive approach that involves updating all network parameters~\cite{he2022parameter, tajbakhsh2016convolutional}. To overcome this challenge, researchers are exploring alternatives that balance parameter-efficiency and robust performance, which can be broadly categorized into three groups: \textit{partial tuning}, \textit{extra module} and \textit{prompt tuning} methods.

%While these models are effective and prevalent, they are prohibitively large for deployment and adaptation. The application of \textit{pretrain-then-finetune} paradigm is thus extensively adopted. Full fine-tuning, although it guarantees strong performance, is an expensive strategy that requires updating all parameters in the network~\cite{he2022parameter, tajbakhsh2016convolutional}. Researchers are thus pursuing parameter-efficiency and robust performance alternatives, which can be categorized into three groups.

% As discussed in \S\ref{sec:introduction}, parameter-efficient tuning methods under the \textit{pretrain-then-finetune} paradigm can be categorized into the following three types. 
% We cover more discussion in \S\ref{subsec:efficient_training}.

\textit{Partial tuning} methods are widely used for parameter-efficient fine-tuning. These methods involve freezing most of the backbone and only fine-tune a small portion of the parameters, such as linear~\cite{iofinova2022well} or MLP heads~\cite{chen2020improved}, or a few blocks/layers of the backbone~\cite{he2022masked, noroozi2016unsupervised, yosinski2014transferable, zhang2016colorful}.
% only fine-tune part of the backbone, such as the classifier heador last few
% layers, while freezing the other parameters. These models can be written as:
% \begin{equation}
% % \begin{aligned}
% \label{eq:head_oriented}
% \mathcal{M}_{partial}(x) = H(B(x; \omega); \theta^{h}),
% % \end{aligned}
% \end{equation}
% where $B$ represents the backbone network, $H$ is the classification heads, which might be linear heads~\cite{iofinova2022well}, MLP heads~\cite{chen2020improved} or few blocks of backbone~\cite{yosinski2014transferable, zhang2016colorful, noroozi2016unsupervised, he2022masked}. $\theta^{h}$ is the learnable part in the network head, and $\omega$ represents the fixed weights of the backbone during tuning. 
While these methods are straightforward and simple to implement~\cite{chen2021empirical, jia2021exploring, mahajan2018exploring}, they often have a large performance gap compared to full fine-tuning. 
%\cite{yosinski2014transferable, zhang2016colorful, noroozi2016unsupervised, he2022masked} consider adapting the last $k$ layers of backbone while freezing the others, by redefining the boundary between backbone and classification head, can also be categorized under partial tuning. 
% Here we use a transformer $T$ to represent the backbone, but the tuning methods are not restricted to transformer-based models.
% (\ie, by redefine the boundary between backbone and classification head, one can consider the last $k$ layers of the backbone as head while freeze the above layers, which are categorized into partial tuning methods).
\textit{Extra module} methods design additional learnable plug-in architecture for fine-tuning. For example, the work in \cite{zhang2020side} introduces a side structure alternatively while freezing the original network. The works in~\cite{cai2020tinytl, rebuffi2017learning} insert additional residual units into the backbone.
However, one drawback of these methods is that the inserted modules are often customized for specific architectures and might not be generalized to others. Additionally, these modules usually consume even more parameters compared to partial tuning methods.
% Specifically, \cite{zhang2020side} adapts a pretrained network by training a lightweight ``side” network alternatively. The training ``side" is then fused with fixed part via summation. \cite{rebuffi2017learning} insert addition residual units (or lite residual learning~\cite{cai2020tinytl}) to the network, which is expressly designed for ResNet-related~\cite{he2016deep} CNN architectures~\cite{sandler2018mobilenetv2}. 
% These approaches can be formulated as:
% \begin{equation}
% % \begin{aligned}
% \label{eq:backbone_oriented}
% \mathcal{M}_{extra}(x) = H(B(x;\omega, \theta^{b});\theta^{h}),
% % \end{aligned}
% \end{equation}
% where $\theta^{b}$ represents the additional learnable architecture for fine-tuning, and a corresponding learnable head $\theta^{h}$ is still required for adapting to a different number of classes. 
%One drawback of these methods is that the inserted modules are customized for specific architectures and might not be generalized to others. Moreover, these modules usually consume even more parameters compared to partial tuning methods.
%have provided insights into CNNs' \textit{pretrain-then-finetune} paradigm, but they generally fall short of accuracy under transformer-based architectures. 
%has been originally proposed for fast model adaptation in few-shot or zero-shot settings \cite{DBLP:conf/nips/BrownMRSKDNSSAA20}, which prepends text instruction to the input text on downstream tasks. Recent prompt tuning works \cite{LesterAC21,PrefixTuning,Hyperprompt,xprompt} propose to treat the prompts as task-specific continuous vectors, and directly learn them during fine-tuning. Different from full fine-tuning, they achieve comparable performance but with much less parameter storage. However, most of these methods only simply add prompts to the input layer, which greatly limited their performances.
\textit{Prompt tuning} or prompting~\cite{he2022hyperprompt, lester2021power,ma2022xprompt, yang2023mixpave} has been originally proposed for fast model adaptation in the language domain. These methods prepend a set of learnable vectors to the input of the backbone and only update these task-specific prompts during fine-tuning.
% They can be formulated as:
% \begin{equation}
% % \begin{aligned}
% \label{eq:input_oriented}
% \mathcal{M}_{prompt}(x) = H(B(x,\theta^{i}; \omega);\theta^{h}),
% % \end{aligned}
% \end{equation}
% where $\theta^{i}$
% % $\theta^{i} \in \left\{\theta^{i_{1}},..., \theta^{i}_{l},...,\theta^{i}_{l}\right\}$ 
% is the learnable input vectors during fine-tuning. 
% % Notably, it is different from Eq. \ref{eq:backbone_oriented} as they do not alter the structure of the backbone. 
% For transformer-based models (\ie, $B(\cdot)$ is a transformer), the input prompts can be prepended at every layer. For example, VPT~\cite{jia2022visual} prepends prompts to all $L$ layers, and VPT-SHALLOW~\cite{jia2022visual} only inserts prompts into the first transformer layer.
Recently, visual-related prompting~\cite{gao2022visual, jia2022visual, xing2022class} is introduced in vision domain, which designs visual prompts in the input sequence and shows competitive performance with full fine-tuning. However, current methods do not consider the inner design of transformer-based architectures, resulting in less effective prompting solutions. In contrast, our approach is mindful of architecture and anchored on pruning, which conceptually sets it apart from the methods discussed above.

% Although the aforementioned methods reduce a significant proportion of parameters to full fine-tuning, they 
% are designed specifically for CNN architectures (\ie, extra module) or 
% disregard the inner design of the transformer architecture, 
% (\ie, partial tuning and prompt tuning), 
% which hinder their potential for performance gain on transformer-based architecture.

% Tuning methods under the \textit{pretrain-then-finetune} paradigm has been extensively studied for visual-related tasks. Full fine-tuning, though guarantees strong performance, is an expensive strategy which needs to update all parameters for adaptation~\cite{he2022parameter, tajbakhsh2016convolutional}. Intuitively, researchers thus pursue parameter-efficiency and robust performance alternatives. Existing approaches may be structured into three categories. \textbf{I. Partial tuning} methods learn single classifier heads with other parameter freezing from pretrained models. Despite being straightforward and simple to implement, \cite{mahajan2018exploring, jia2021exploring, chen2021empirical} perform unmatched performance to Full fine-tuning. \cite{yosinski2014transferable, zhang2016colorful, noroozi2016unsupervised, he2022masked} consider adapting the last $k$ layers of backbone while freezing the others, by redefining the boundary between backbone and classification head, they can be categorized into \textbf{partial tuning} as well. \textbf{II. Extra module} methods, on the other hand, design plug-in learning architecture for tuning.  Specifically, \cite{zhang2020side} adapts a pretrained network by training a lightweight ``side” network alternatively. The training ``side" is then fused with fixed part via summation. \cite{rebuffi2017learning} insert addition residual units (or lite residual learning~\cite{cai2020tinytl}) to the network, which is expressly designed for ResNet-related~\cite{he2016deep} CNN architectures~\cite{sandler2018mobilenetv2}. These methods, though provide insights on CNNs' \textit{pretrain-then-finetune} paradigm, fall short on accuracy under transformer-based architectures.
% \textbf{III. Prompt tuning} methods are initially introduced to Pretrained Language Models (PLMs) by prepending learnable prompt vectors to the input and only updating these vectors during training, namely, prompt tuning. Many works \cite{davison2019commonsense, gong2021prompt, radford2019language, wang2021transprompt, khashabi2020unifiedqa, ma2022xprompt, he2022hyperprompt} prove the competitive performance and parameter-efficiency in various language tasks. Visual-related prompting are inspired by NLP approaches and relatively new, yet their performance are impressive~\cite{jia2022visual, xing2022class, gao2022visual} and competitive to Full fine-tuning.  Nevertheless, current methods disregard the inner design of transformer-based architectures. We are consequently committed to explore general and effective strategies for prompt tuning.

% Initially start from convolutional neural networks (CNN), methods such as side tuning~\cite{zhang2020side}, bias tuning~ and ~\cite{cai2020tinytl} are introduced. transformer-based architectures, 
% though highly successful, are not receiving enough attention under this paradigm and current CNN-based methods can hardly reach satisfying performance. Prompt tuning, with the development of GPT-3~\cite{}, receive 

% sustainable AI... (pretrained and finetune methods)
% NLP--Visual
% from a unified view of parametric prototype learning.


% Figure environment removed

% \vspace{-0.5em}
\section{Our \textbf{E$^{2}$VPT} Approach}
\label{sec:mixed_prompt}
\vspace{-0.3em}
In this section, we introduce ${\rm E^{2}VPT}$, a novel visual prompt tuning approach for effective and efficient large-scale transformer-based model fine-tuning. We first define the problem and notations in \S\ref{subsec:attention_anatomy}. The effective prompt tuning with the designing of visual and key-value prompts is presented in \S\ref{subsec:effective_prompting}, followed by the efficient prompt pruning in \S\ref{subsec:Pruning_Rewind}. The overall framework is shown in Fig. \ref{fig:plot2}. 

% in additional modifications to the prompts. ${\rm E^{2}VPT}$ \textbf{\ding{172}} adds learnable input prompts to transformer layers and task-specific learnable vectors into the self-attention module, resulting in improved performance over current fine-tuning methods (\S\ref{subsec:effective_prompting}), and \textbf{\ding{173}} employs the lottery ticket hypothesis (LTH) to reduce parameter usage in the input prompts (\S\ref{subsec:Pruning_Rewind}).
% % it \textbf{\ding{172}} adds learnable input prompts to transformer layers (\S\ref{subsec:inputs_token}), \textbf{\ding{173}} prepending task-specific learnable vectors into self-attention module for performance gain over current fine-tuning methods (\S\ref{subsec:kv_pairs}), and \textbf{\ding{174}} using the lottery ticket hypothes (LTH) to reduce the parameter usage in input prompts (\S\ref{subsec:Pruning_Rewind}).
% Following the discussion in \S\ref{subsec:prior_arts}, we formulate ${\rm E^{2}VPT}$ as:
% \begin{equation}
% % \begin{aligned}
% \label{eq:global_oriented_0}
% \mathcal{M}_{global}(x) = H(T(x, \zeta \cdot \theta^{i}; \omega, \hat{\theta});\theta^{h}),
% % \end{aligned}
% \end{equation}
% where $T$ is a transformer backbone, $\hat{\theta} = \left\{\hat{\theta}_{1},..., \hat{\theta}_{l},...,\hat{\theta}_{L}\right\}$ represent the task-specific learnable vectors in all $L$ self-attention layers. $\hat{\theta}$ is jointly updated with learnable input prompts and classification head during fine-tuning. $\zeta$ is the pruning factor applied to input prompts. With the inclusion of learnable tokens into the core of transformer architectures, it becomes essential to revisit transformer and its inner design in
% % the internal structure of the transformer in 
% \S\ref{subsec:attention_anatomy}.

\subsection{Problem Definition}
\label{subsec:attention_anatomy}
\vspace{-0.2em}
In this section, we define the problem of ${\rm E^{2}VPT}$ and provide the notations.
Assuming we have a backbone vision transformer model $\textbf{T}$, which is pretrained on a large set of data and tasks. The input to the vision transformer is a sequence of image patches $I$ = $\{I_1, I_2,\dots,I_m\}$, where $m$ is the total number of image patches. Each patch is then projected into a $d$-dimensional embedding with positional encoding, \ie, $E$ = $\{E_j | 1\le j\le m\}$ with $E_j$ = ${\rm Emb}(I_j)$. The vision transformer $\textbf{T}$ consists of $N$ identical transformer layers, represented as:
\begin{equation}
\begin{aligned}
    &Z^1 = {L_1}(E) \\
    &Z^i = {L_i}(Z^{i-1}) \ \ \ \ \ i = 2, 3,\dots,N
\end{aligned}
\end{equation}
here each transformer layer is a stack of multi-head self-attention (\text{MSA}) and feed-forward network (\text{FFN}):
\begin{equation}
L(\cdot) = \text{FFN} \ ( \ \text{MSA}\ (\cdot)\ )
\end{equation}
Given a new vision task, the objective is to fine-tune a model $\hat{\textbf{T}}$ that can deliver good performance on the task, while only tuning a small amount of parameters. 
In the context of visual prompt tuning, $\hat{\textbf{T}}$=$\{\textbf{\textcolor{iceblue}{T}}, \textbf{\textcolor{fired}{P}}\}$ which includes a \textcolor{iceblue}{frozen} backbone \textbf{\textcolor{iceblue}{T}}, and \textcolor{fired}{trainable} prompts \textbf{\textcolor{fired}{P}} with very few tunable parameters.


% Let $x \in \mathbb{R}^{N \times F}$ denote the input sequence with $N$ feature vectors of dimension $F$. A transformer is a function $T:\mathbb{R}^{N \times F} \rightarrow \mathbb{R}^{N \times F}$, where $T = \left\{ T_{1}(\cdot),..., T_{l}(\cdot),...,T_{L}(\cdot)\right\}$ is a composition of $L$ layers. For any $T_{l}(\cdot)$ with $x \in \mathbb{R}^{N \times F}$ as input: 
% \begin{equation}
% % \begin{aligned}
% \label{eq:Layer}
%    T_{l}(x)=f_{l}(A_{l}(x)+x),
% % \end{aligned}
% \end{equation}
% where $f_{l}(\cdot)$ transforms each feature individually from others, usually implemented with a two-layer feedforward network (FFN)~\cite{katharopoulos2020transformers, dosovitskiy2020image}. $A_{l}(\cdot)$ represents the self-attention function, which captures long-range token dependencies in a global context (\ie, global interaction between images patches~\cite{han2022survey, lin2022survey, khan2022transformers}). It computes a weighted average of all feature representations, using a weight proportional to the similarity score between each representation. Specifically, the input sequence is first projected to three matrices $Q$ (``queries"), $K$ (``keys") and $V$ (``values") by:
% \begin{equation}
% % \begin{aligned} 
% \label{eq:QKV}
% Q = xM_{Q}, 
% K = xM_{K}, 
% V = xM_{V},
% % \end{aligned}
% \end{equation}
% where $M_{Q} \in \mathbb{R}^{F \times D}$, $M_{K} \in \mathbb{R}^{F \times D}$, and $M_{V} \in \mathbb{R}^{F \times M}$ are the projection matrices. The corresponding output $A_{l}(x)$ for all positions is:
% % As the key component of transformer, the self-attention layer enables global interaction between images patches~\cite{han2022survey, lin2022survey, khan2022transformers}. 
% % In the self-attention layer, the input vector is first transformed into three vectors. These vectors derived from different inputs are then packed together into three metrics, namely, \textbf{Q}, \textbf{K} and \textbf{V}~\cite{han2022survey}.  This process is unified into Eq.~\ref{eq:Attention}:
% % This step can be acknowledged as 
% %with dimension 
% \begin{small}
% \begin{equation}
% % \begin{aligned}
% \label{eq:Attention}
% A_{l}(x) = {\rm Attention}(Q, K, V)={\rm softmax}(\frac{Q \cdot K^{T}}{\sqrt{D}}) \cdot {V}.
% % \end{aligned}
% \end{equation}
% \end{small}

% % Subsequently, in Eq.~\ref{eq:Attention}, it first computes scores between each pair of different vectors: $S=Q \cdot K^{T}$. After normalization and softmax, each value vector is multiplied by the sum of the probabilities. Vectors with larger probabilities receive additional focus from the following layers. Intuitively, $S$ varies accordingly with learnable $Q$ and $K$. Changes in $K$ can therefore participate in the calculation of attention feature map $A_{F}={\rm softmax}(\frac{Q \cdot K^{T}}{\sqrt{D}})$. $V$ is cooperated with $A_{F}$ for the weighted-sum of vectors.
% Our method is inspired by the design of the self-attention layer described in Eq. \ref{eq:Attention}, with learnable vectors prepending with $K$ and $V$, one can introduce attention feature shift to quickly adapt to various visual tasks under the \textit{pretrain-then-finetune} paradigm as discussed in \S\ref{subsec:effective_prompting}.

% \subsection{Input Prompt} 
% \label{subsec:inputs_token}
\subsection{Effective Prompting}
\label{subsec:effective_prompting}
Most existing prompt tuning approaches focus on tuning a set of visual prompts by prepending them to the input sequence in transformer layers, without considering the internal design of transformer architectures. However, to enhance the effectiveness of prompt tuning and achieve optimal fine-tuning performance, we propose a new approach that incorporates a set of key-value prompts (\textbf{{P$_K$}} and \textbf{{P$_V$}}) in addition to input visual prompts (\textbf{{P$_I$}}) within our visual prompt tuning framework.
Intuitively, the input visual prompts are inserted to the input sequence of each encoder layer, which learn to represent of the new task. The key-value prompts are concatenated with the key and value parameter matrices in the self-attention module, which learn to capture the new attention pattern from the data.

\noindent \textbf{Visual Prompts.}
Visual prompts are a set of $d$-dimensional embedding vectors that have the same dimensionality with the input visual tokens. They are prepended to the input sequence of each transformer encoder layer and interact with all the input tokens. Visual prompts play a similar role to those prompt tokens in traditional prompt tuning methods~\cite{jia2022visual,lester2021power}, which learn task-specific embeddings to guide the model performing on the new task.

Formally, these visual prompts are defined as P$_I$ = $\{P_I^1, P_I^2, \dots, P_I^N\}$, where $P_I^i$ denotes the learnable visual prompts in the $i_{th}$ encoder layer, and $N$ is the total number of layers. Then the encoder layers are represented as:
\begin{equation}
\begin{aligned}
    &Z^1 = \textcolor{iceblue}{L_1}(\textcolor{fired}{P_I^1}, \ \textcolor{iceblue}{E}) \\
    &Z^i = \textcolor{iceblue}{L_i}(\textcolor{fired}{P_I^i}, \ Z^{i-1}) \ \ \ \ \ i = 2, 3,\dots,N
    %&y \ = {\rm \textcolor{fired}{Head}}(Z^N)
\end{aligned}
\end{equation}
where $Z^i$ represents the contextual embeddings computed by the $i_{th}$ encoder layer. The different colors indicate \textcolor{fired}{trainable} and \textcolor{iceblue}{frozen} parameters, respectively. For the embeddings of the input image patches $E$, they are initialized with frozen ${\rm Emb}$ projection from the backbone.
% Based on the evidence of performance achieved through \textit{prompt tuning} methods (\S\ref{subsec:prior_arts}), 
% our approach involves incorporating prompts into the input space of the transformer layer and updating them during fine-tuning. To achieve this, we adopt the design of VPT~\cite{jia2022visual} by including input prompts for every transformer layer. 
% For any transformer layer with input prompts, we have $T_{l}(x, \theta^{i}_{l}) = T_{l}(P)$,
% % \begin{equation}
% % % \begin{aligned}
% % \label{eq:input_oriented_specific}
% % % \mathcal{M}_{input} = H(T(x;\theta^{i});\theta^{h})
% % T_{l}(x, \theta^{i}^{l})=f_{l}(A_{l}(P)+P),
% % % \end{aligned}
% % \end{equation}
% where $P = {\rm concat}(x[0,:], \theta^{i}_{l}, x[1:,:])$, which is a concatenation of learnable vectors to inputs for the transformer layers. $\theta^{i}_{l}$ with length $N$ is inserted between \texttt{[CLS]} token and the collection of image patch embeddings, It is considered as extra learnable prompts that contribute to adaptation.

% Similar to \textit{prompt tuning} methods discussed in \S\ref{subsec:prior_arts},
% our initial approach involves introducing prompts to the input space of the transformer layer and keep them updated during fine-tuning. We follow VPT's~\cite{jia2022visual} design to include prompts for every transformer layer. \textcolor{red}{Instead of focusing exhaustively on prepending input prompts~\cite{jia2022visual}, we meticulously evaluate transformer architectures to achieve advanced performance.}
% Our approach significantly outperforms VPT-SHALLOW~\cite{jia2022visual} in accuracy, while achieving a comparable cost by minimal parameter usage as described in \S\ref{subsec:Pruning_Rewind}.

% should be task-specific or not? here
% \subsection{Task-specific Learnable K-V Prompt}
% \label{subsec:kv_pairs}

% Previous tuning methods on transformers (\S\ref{subsec:prior_arts}) have shown potential for adaptation with fewer tuning parameters than full fine-tuning. However, they do not consider the internal design of transformer-based architectures. We aim to design general and effective strategies for transformer-based fine-tuning for higher performance gains under the \textit{pretrain-then-finetune} paradigm.
\noindent \textbf{Key-Value Prompts.}
Visual prompts are useful in learning knowledge about new tasks. However, they are insufficient in guiding information interaction within transformer encoder layers. The reason is that when fine-tuning on new data, the image distribution may significantly differ from those in the image examples used for pretraining the backbone model. As a result, it is crucial to enhance the model's capability to capture new information from the fine-tuning data and conduct more effective attention among input tokens to learn new patterns.

To this end, we propose a new approach by introducing a novel set of key-value prompts, P$_K$ and P$_V$, which are incorporated into the attention module within each encoder layer (as shown in Fig. \ref{fig:plot2}(a). These key-value prompts are small matrices that have only a few columns but share the same number of rows as the key and value matrices in the original attention module. To perform the new attention computations, the key and value matrices are concatenated with their corresponding P$_K$ and P$_V$ prompts, respectively. This process is defined as follows:
\begin{equation}
\begin{aligned}
    L(\cdot) &= \textcolor{iceblue}{\text{FFN}} \ (\textcolor{fired}{\text{MSA}} \ (\cdot)) \\
    \text{MSA} (\cdot)  &= {\rm concat}({\rm softmax}(\frac{\textcolor{iceblue}{Q_h}\textcolor{fired}{K_h^{'}}^T}{\sqrt{d}})\textcolor{fired}{V_h^{'}})
\end{aligned}
\end{equation}
where \text{FFN} is the feed-forward network and \text{MSA} is the multi-head attention inside the encoder layer. $h$ represents the $h_{th}$ head. $\textcolor{fired}{K^{'}}$ and $\textcolor{fired}{V^{'}}$ are the new key and value embedding matrices defined as:
\begin{equation}
\label{eq:kv_promt}
\begin{aligned}
    K^{'} = {\rm concat}(\textcolor{iceblue}{K}, \ \textcolor{fired}{P_K}), \ \ \ V^{'} = {\rm concat}(\textcolor{iceblue}{V}, \ \textcolor{fired}{P_V})
\end{aligned}
\end{equation}
where $K$ and $V$ represent the original key and value matrices in the backbone. In this way, the key-value prompts can help guide the model adaptation to the new data. 
In our implementation, we take it a step further by enabling parameter sharing of the P$_K$ and P$_V$ prompts within each transformer layer instead of tuning separate learnable vectors.
Our motivation is twofold: First, our experimental results show that with the shared prompts, the fine-tuning performance consistently improves across instances; Second, using shared prompt vectors reduces the parameter usage in the learnable transformer part by half, making it more parameter-efficient. We provide discussion on exploring the prompt locations (i.e., before or after $K$ and $V$) in \S\ref{subsec:Diagnostic_Experiment}. 

%Instead of solely relying on prepending input prompts as in \cite{jia2022visual}, we examine the transformer architecture to enhance performance. Given this perspective, we thus suggest integrating learnable parameters into the key design of the transformer --- the self-attention layer as: 
% Generally, we can formulate it as Eq. \ref{eq:global_oriented_0}.
% :  
% \begin{equation}
% % \begin{aligned}
% \label{eq:global_oriented}
% % \mathcal{M}_{global} = H(T(x, \theta^{i}; \hat{\theta});\theta^{h}),
% \hat{T}(x) = T(x, \theta^{i}; \omega, \hat{\theta}),
% % \end{aligned}
% \end{equation}
% where $\hat{T}$ represents the combined version of fixed-weight self-attention with additional learnable parameters $\hat{\theta}$. %They are then combined in MSA layer 
% where $\hat{\theta} = \left\{\hat{\theta}_{1},..., \hat{\theta}_{l},...,\hat{\theta}_{L}\right\}$ represent the task-specific learnable vectors in self-attention layers, which are jointly updated with learnable input prompts and classification head during fine-tuning. 

% The idea of prepending learnable vectors to the network for fine-tuning is first explored by NLP prompt tuning methods~\cite{he2022hyperprompt, li2021prefix, lester2021power, liu2021gpt}. We are introducing and expanding this idea to visual-related adaptation tasks for the first time. Specifically, we incorporate a collection of learnable vectors to $K$ and $V$.
% % denotes as $G \in $. 
% Different from \cite{he2022hyperprompt}, we create a shared vector within each transformer layer instead of initializing separate learnable vectors for the key and the value. Our motivation is twofold: \ding{228} We experimentally show that with separate learnable vectors created, the performance drop consistently across instances (\eg, 88.5\% $vs$ 86.0\% on VTAB-1k \textit{Natural} Pets~\cite{parkhi2012cats}); \ding{228} With additional initialized vectors, the parameter usage in the learnable transformer part actually doubles, which is untenable given the parameter-sensitive nature of our method. Thus we prepend the share-weight task-specific learnable vectors to the $K$ and $V$ of the self-attention layer at every transformer block as:
% \begin{equation}
% % \begin{aligned}
% \label{eq:global_oriented_specific}
% % \mathcal{M}_{global} = H(T(x;\theta^{i}, \hat{\theta});\theta^{h}).
% \hat{T}_{l}(x, \theta^{i}_{l}; \omega, \hat{\theta}_{l})=f_{l}(A_{l}(P; \omega, \hat{\theta}_{l})+P),
% % \end{aligned}
% \end{equation}
% where $A_{l}(P; \hat{\theta})$ can be further expressed as:
% \begin{small}
% \begin{equation}
% % \begin{aligned}
% \label{eq:global_oriented_specific_softmax}
% % \mathcal{M}_{global} = H(T(x;\theta^{i}, \hat{\theta});\theta^{h}).
% A_{l}(P; \omega, \hat{\theta}_{l})={\rm softmax}(\frac{Q \cdot {\rm concat}(\hat{\theta}_{l}, K)^{T}}{\sqrt{D}}) \cdot {\rm concat}(\hat{\theta}_{l}, V),
% % \end{aligned}
% \end{equation}
% \end{small}
% where the share-weight vectors are prepended at the front of $K$ and $V$ in our setting. 
It is worth noting that the query matrix $Q$ is another critical element in the self-attention mechanism. However, additional prompting on $Q$ is not desired for two reasons: First, prompting on $Q$ is similar to prepending on $K$ for computing attention scores between each pair of $Q$ and $K$ Therefore, prompting on both $Q$ and $K$ is unnecessary; Second, changes in $Q$ affect the output shape of the attention map, necessitating an additional linear projection for unmatched dimensions in the following layer. This is not affordable under the parameter-efficient design. More experiments and discussions will be provided in the Appendix.
% would xxxx. \S\ref{xxx} offers empirical evidence to substantiate this argument.}



% Following \S\ref{subsec:prior_arts}, we can then formulate our modified 

 % for them to further reduce computational overhead. 

\subsection{Efficient Prompting}
\label{subsec:Pruning_Rewind}
%Our effective prompting aims at improving the effectiveness of the fine-tuned model. A natural question to ask is: can we further reduce the tunable prompts while maintaining the model performance? The lottery ticket hypothesis (LTH)~\cite{frankle2018lottery, zhuang2023survey} states that for a given task, there exists a sub-network that can match the test performance of the original over-parameterized network without the unnecessary weights. Motivated by this hypothesis, we conduct an experiment of masking different visual prompts and find out that different prompts have different influence to the model performance, while certain prompts even have negative impact. This finding is consistent with the observations in~\cite{li2022automated,ma2022xprompt}.
%Therefore, we propose a prompt pruning method on the visual prompts to retain the most influential prompts, while eliminating redundant or unnecessary ones. By pruning these less important prompts, we can significantly improve the prompt tuning efficiency while maintaining the performance.

Our approach to effective prompting aims to enhance the performance of the fine-tuned model. However, a natural question arises: Can we reduce the number of tunable prompts without sacrificing model performance? The lottery ticket hypothesis (LTH)~\cite{frankle2018lottery, zhuang2023survey} states that there exists a sub-network that can achieve the same test performance as the original over-parameterized network for a given task, without the need for unnecessary weights. Motivated by this hypothesis, we conducted an experiment in which we masked different visual prompts and found that various prompts have varying effects on the model performance, with some even having a negative impact. This observation is consistent with previous research~\cite{li2022automated,ma2022xprompt}.

% for visual tasks for the first time. This approach operates under the \textit{pretrain-then-finetune} paradigm and removes negative prompt tokens from updated input prompts (discussed in \S\ref{subsec:effective_prompting}), creating soft filtered input prompts (Fig. \ref{fig:plot2}(d)).$_{\!}$ Our pruning stage
Based on our findings, we propose a prompt pruning method on visual prompts. 
The primary objective of this method is to retain the most influential prompts while eliminating redundant or unnecessary ones. By removing less important prompts, we can significantly improve the efficiency of prompt tuning while maintaining performance.
To achieve this goal, we design a cascade pruning strategy that operates at two levels of granularity, namely token-wise pruning and segment-wise pruning, as illustrated in Fig.~\ref{fig:plot2}(d). Token-wise pruning initially identifies and removes the least important visual prompts. After this step, segment-wise pruning divides each remaining prompt into multiple segments and filters out negative segments. By jointly reducing the parameter usage in learnable visual prompts, our two-level pruning approach creates soft filtered prompts that can be re-trained in the rewinding stage.
% This method aims to retain the most influential prompts while eliminating redundant or unnecessary ones. By removing less important prompts, we can significantly improve the efficiency of prompt tuning while maintaining performance. Specifically, we design a cascade pruning strategy which operates pruning at two levels of granularity, namely token-wise pruning and segment-wise pruning as shown in Fig.~\ref{fig:plot2}(d).
% Token-wise pruning first identifies and removes visual prompts that are of least importance. Within the remaining visual prompts, segment-wise pruning then divides each prompt into multiple segments, and filters out negative ones. By jointly reducing parameter usage in learnable visual prompts, our two-level pruning approach creates soft filtered prompts for re-training in the rewinding stage. 
% This is the first time such an approach has been developed for visual tasks.
% \ding{228} We apply token-wise pruning to identify learnable input prompts that have a negative impact on performance; \ding{228} Within the remaining learnable input prompts, we check for negative segments, and apply segment-wise pruning to filter them out. Two-level pruning jointly reduces parameter usage in learnable input prompts, and form soft filtered input prompts for re-training, defined as the rewinding stage.

% The design benefits are twofold: \ding{228} A subnetwork that can achieve comparable performance to the original network with fewer parameters is significant due to the parameter-sensitive nature of our method; \ding{228} While we introduce token-wise pruning to search the input prompts that contribute the most to the network, it may not be sufficient enough since there still exists negative prompt segments in the filtered embedding space.
% Token-wise and segment-wise pruning therefore work jointly in learnable input prompts and generate the soft filtered input prompts that are used for re-training, referred to as the rewinding stage.

\noindent \textbf{Token-wise Pruning.} We introduce a learnable mask variable $\rho=\left\{\rho_{1},\rho_{2},\dots,\rho_{M}\right\}$ ($M$ is the length of visual prompts) and associate it with the input visual prompts in each transformer layer. Here $\rho_{k} \in \left\{0,1\right\}$, where 0 means the corresponding learnable input prompt is pruned. Then the masked version of the visual prompts becomes $\widetilde{P_k}$ = $\rho_{k} \cdot P_k$.
% $\theta^{i}_{l}$, defined as $\widetilde{\theta}_{i_{l}}=\rho_{l} \cdot \theta^{i}_{l}$, is then prepended into the input prompt:
% \begin{equation}
% % \begin{aligned}
% \label{eq:pruning_1}
% \widetilde{P} = {\rm concat}((x[0,:], \widetilde{\theta}_{i_{l}}, x[1:, :])).
% % \end{aligned}
% \end{equation}
% % where $\rho=\left\{\rho_{1},..., \rho_{n},...,\rho_{N}\right\}$, $\rho_{n} \in \left\{0,1\right\}$, 
To determine the pruning position, we  
% follow~\cite{ma2022xprompt,frankle2018lottery, michel2019sixteen} and 
calculate the importance score~\cite{frankle2018lottery, ma2022xprompt} of each prompt token and eliminate those positions with lowest scores.
The importance score is defined as the expected sensitivity of the model to the mask variables $\rho_{k}$ \cite{michel2019sixteen}:
\begin{equation}
% \begin{aligned}
\label{eq:pruning_2}
% S_{\rho_{n}} = \mathop{\mathbb{E}_{\theta^{i}_{l}}}
S_{P_k} = \mathbb{E}_{x \sim \mathcal{D}_{x}}\left| \frac{\partial \mathcal{L}(x)}{\partial \rho_{k}}\right|
% \end{aligned}
\end{equation}
where $\mathcal{L}$ is the loss function, and $\mathcal{D}_{x}$ is the training data distribution~\cite{michel2019sixteen}. 
The importance score assigned to each visual prompt reflects its contribution to the fine-tuning performance. A low importance score indicates that the prompt has a minor or even negative contribution to the fine-tuning process. Conversely, a high importance score suggests that the prompt is a meaningful and useful one that significantly contributes to the fine-tuning process.

\noindent \textbf{Segment-wise Pruning.}  
We further investigate the segment-wise pruning to preclude the negative prompt segments within each prompt. The embedding of each prompt token is first equally divided into $R$ parts. Each part is treated as an isolated unit which can be optimized jointly. Similar to the token-wise pruning, we then assign a mask variable to each segment inside the prompt token and filter out those segments with low importance scores.
% \begin{equation}
% % \begin{aligned}
% \label{eq:pruning_3}
% % \hat{\mathbb{E}}\rho_{l_{n}}=\sigma\cdot\mathbb{E}\rho_{l_{n}}
% \widetilde{\delta}=\sigma\cdot\delta,
% % \end{aligned}
% \end{equation}
% where $\sigma=\left\{\sigma_{1},..., \sigma_{m},..., \sigma_{M}\right\}, \sigma_{m} \in \left\{0, 1\right\}$, where 0 indicates the corresponding segment is pruned.

% The importance score ${\rm S}(\delta_m)$ of each segment for every prompt token embedding is:
% \begin{equation}
% % \begin{aligned}
% \label{eq:pruning_4}
% % S_{\rho_{n}} = \mathop{\mathbb{E}_{\theta^{i}_{l}}}
% {\rm S}(\delta_m) = \mathbb{E}_{x \sim \mathcal{D}_{x}}\left| \frac{\partial \mathcal{L}(x)}{\partial \sigma_{m}}\right|.
% % \end{aligned}
% \end{equation}
%Token-wise and segment-wise pruning are conducted jointly for the pruning stage and form soft filtered prompt tokens.

\noindent \textbf{Rewinding.} 
% As discussed in \S\ref{sec:introduction}, the LTH~\cite{frankle2018lottery, li2022automated, zhuang2023survey} argue that a sparse subnetwork (\ie, network after pruning) can be updated and trained individually to the same accuracy as the original network (\ie, network without pruning stage). 
%The weight rewinding stage involves re-trains the soft filtered prompt tokens after the two-level cascade pruning.
%Specifically, we rank the importance scores for each layer in pruning stage and set the corresponding mask variables to 0 when their importance scores are relatively low. We then re-train the soft filtered input prompts with other learnable parameters using the original learning rate and weight decay combination during fine-tuning.
After performing the two-level cascade pruning, the weight rewinding stage focuses on re-training the soft filtered prompt tokens. This process involves ranking the importance scores for each layer during the pruning stage and setting the corresponding mask variables to 0 when their importance scores are relatively low. Next, the soft filtered input prompts are re-trained along with other learnable parameters using the original combination of learning rate and weight decay during fine-tuning.

\begin{table*}[t]
% \vspace{-0.5cm}
\caption{\textbf{Image classification accuracy for ViT-Base/16~\cite{dosovitskiy2020image}} pretrained on supervised ImageNet-21k. Following \cite{jia2022visual}, we report the average test accuracy (three runs) on FGVC~\cite{jia2022visual} and VTAB-1k~\cite{zhai2019large} benchmarks, and ``Number of Wins" in [$\cdot$] compared to full fine-tuning (Full)~\cite{iofinova2022well}. ``Tuned/Total" is the average percentage of tuned parameters required by 24 tasks. ``Scope” indicates the tuning scope of each method. ``Additional parameters" is the existence of parameters in addition to the pretrained backbone and linear head. The highest accuracy among all approaches except FULL are shown in \textbf{bold}. ${\rm E^{2}VPT}$ outperforms the full fine-tuning in \textbf{19 of 24} instances with far fewer trainable parameters. More impressively, we further report ``Number of Wins to VPT" in $\left\{\cdot\right\}$. Our method beats VPT in \textbf{21 of 24} cases with considerably lower parameters. Per-task results are available in Appendix. Same for Table \ref{table:swin} and \ref{table:mae_moco}.}
\vspace{5pt}
\label{table:fgvc_vtab_main}
% \begin{center}
% \begin{small}
%\tabcolsep=0.10cm
% \resizebox{0.90\textwidth}{!}{
\begin{adjustbox}{width=0.83\width,center}
\begin{tabular}{c||r|cc|c|r|rrr} 
\hline \thickhline
\rowcolor{mygray}
ViT-Base/16~\cite{dosovitskiy2020image}     & Tuned/ &\multicolumn{2}{c|}{Scope}   & Extra    &   &  \multicolumn{3}{c}{VTAB-1k~\cite{zhai2019large} [19]}  \\ 
\rowcolor{mygray}
\rowcolor{mygray}
  (85.8M)   & Total & Input & Backbone  & params  &  \multirow{-2}{*}{FGVC~\cite{jia2022visual} [5]}    &  \textit{Natural} [7] & \textit{Specialized} [4] & \textit{Structured} [8]   \\ 
\hline \hline
Full \textcolor{lightgray}{\scriptsize{[CVPR22]}}\cite{iofinova2022well} & 100.00\% & & \checkmark & & 88.54\% & 75.88\% & 83.36\% & 47.64\% \\
\hline
Linear \textcolor{lightgray}{\scriptsize{[CVPR22]}}\cite{iofinova2022well} & 0.08\% & & & & 79.32\% [0] & 68.93\% [1] & 77.16\% [1] & 26.84\% [0] \\
Partial-1 \textcolor{lightgray}{\scriptsize{[NeurIPS14]}}\cite{yosinski2014transferable} & 8.34\% & & & & 82.63\% [0] & 69.44\% [2] & 78.53\% [0] & 34.17\% [0] \\
MLP-3 \textcolor{lightgray}{\scriptsize{[CVPR20]}}\cite{chen2020improved} & 1.44\% & & & \checkmark & 79.80\% [0] & 67.80\% [2] & 72.83\% [0] & 30.62\% [0]\\
\hline
Sidetune \textcolor{lightgray}{\scriptsize{[ECCV20]}}\cite{zhang2020side} & 10.08\% & & \checkmark & \checkmark & 78.35\% [0] & 58.21\% [0] & 68.12\% [0] & 23.41\% [0] \\
Bias \textcolor{lightgray}{\scriptsize{[NeurIPS17]}}\cite{rebuffi2017learning} & 0.80\% & & \checkmark & & 88.41\% [3] & 73.30\% [3] & 78.25\% [0] & 44.09\% [2]\\
Adapter \textcolor{lightgray}{\scriptsize{[NeurIPS20]}}\cite{cai2020tinytl} & 1.02\% & & \checkmark & \checkmark & 85.66\% [2] & 70.39\% [4] & 77.11\% [0] & 33.43\% [0]\\
\hline
% VPT-SHALLOW~\cite{jia2022visual} & 1.04 $\times$ & & & & 84.62 [1] & 76.81 [4] & 79.66 [0] & 46.98 [4]\\
% \multirow{-2}{*}{
VPT \textcolor{lightgray}{\scriptsize{[ECCV22]}}\cite{jia2022visual} & 0.73\% & \checkmark &  & \checkmark & 89.11\% [4] & 78.48\% [6] & 82.43\% [2] & 54.98\% [8]\\
\rowcolor{mygray}
Ours & 0.39\% & \checkmark & \checkmark & \checkmark & \textbf{89.22\%} [4] $\left\{4\right\}$ & \textbf{80.01\%} [6] $\left\{5\right\}$ & \textbf{84.43\%} [3] $\left\{4\right\}$ & \textbf{57.39\%} [8] $\left\{7\right\}$ \\
\hline
\end{tabular}
\end{adjustbox}
% \end{small}
% \end{center}
\vspace{-1em}
\end{table*}


\begin{table}[t]
% \vspace{-0.5cm}
\caption{\textbf{Image classification accuracy for Swin-Base~\cite{liu2021swin}} pretrained  on supervised ImageNet-21k. }
\vspace{-15pt}
\label{table:swin}
\begin{center}
\begin{small}
\tabcolsep=0.10cm
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{c||r|rrr} 
\hline \thickhline
\rowcolor{mygray}
Swin-Base~\cite{liu2021swin}  & Tuned/ &  \multicolumn{3}{c}{VTAB-1k~\cite{zhai2019large} [19]}  \\ 
\rowcolor{mygray}
\rowcolor{mygray}
  (86.7M)   & Total &  \textit{Natural} [7] & \textit{Specialized} [4] & \textit{Structured} [8]   \\ 
\hline \hline
Full \textcolor{lightgray}{\scriptsize{[ICLR23]}}\cite{ren2023prepare} & 100.00\% &  79.10\% & 86.21\% & 59.65\% \\
\hline
Linear \textcolor{lightgray}{\scriptsize{[ICLR23]}}\cite{ren2023prepare} & 0.06\% & 73.52\% [5] & 80.77\% [0] & 33.52\% [0]  \\
Partial-1 \textcolor{lightgray}{\scriptsize{[NeurIPS14]}}\cite{yosinski2014transferable} & 14.58\% & 73.11\% [4] & 81.70\% [0] & 34.96\% [0]  \\
MLP-3 \textcolor{lightgray}{\scriptsize{[CVPR20]}}\cite{chen2020improved} & 2.42\% &  73.56\% [5] & 75.21\% [0] & 35.69\% [0] \\
\hline
Bias \textcolor{lightgray}{\scriptsize{[NeurIPS17]}}\cite{rebuffi2017learning} & 0.29\% &  74.19\% [2] & 80.14\% [0] & 42.42\% [0]  \\
\hline
% VPT-SHALLOW~\cite{jia2022visual} & 1.01 $\times$ & 79.85[6] & 82.45 [0] & 37.75 [0]  \\
VPT \textcolor{lightgray}{\scriptsize{[ECCV22]}}\cite{jia2022visual} & 0.25\% & 76.78\% [6] & 83.33\% [0] & 51.85\% [0]  \\
\rowcolor{mygray}
Ours &  0.21\% &  \textbf{83.31\%} [6] $\left\{6\right\}$ & \textbf{84.95\%} [2] $\left\{3\right\}$ & \textbf{57.35\%} [3] $\left\{7\right\}$ \\
\hline
\end{tabular}
}
\end{small}
\end{center}
\vspace{-1.5em}
\end{table}
 



\begin{table*}[t]
% \vspace{-0.5cm}
\caption{\textbf{Image Classification accuracy for different pretrained objectives} --- MAE~\cite{he2022masked} and MoCo v3~\cite{chen2021empirical} with ViT-Base~\cite{dosovitskiy2020image} as backbone. Our method enjoys significant performance gains to VPT~\cite{jia2022visual} while having lower parameter usage.}
\vspace{-15pt}
\label{table:mae_moco}
\begin{center}
\begin{small}
\tabcolsep=0.10cm
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c||r|rrr||r|rrr} 
\hline \thickhline
\rowcolor{mygray}
Pretrained objectives & \multicolumn{4}{c||}{MAE~\cite{he2022masked}} &  \multicolumn{4}{c}{MoCo v3~\cite{chen2021empirical}} \\
\hline 
\rowcolor{mygray}
& Tuned/ &  \multicolumn{3}{c||}{VTAB-1k~\cite{zhai2019large} [19]} & Tuned/ &  \multicolumn{3}{c}{VTAB-1k~\cite{zhai2019large} [19]}  \\ 
\rowcolor{mygray}
\rowcolor{mygray}
% \diagbox{1}{2} 
  \multirow{-2}{*}{\diagbox{Methods~}{Parms \& Data}} & Total &  \textit{Natural} [7] & \textit{Specialized} [4] & \textit{Structured} [8] & Total &  Natural [7] & Specialized [4] & Structured [8]  \\ 
\hline \hline
Full \textcolor{lightgray}{\scriptsize{[CVPR22]}}\cite{iofinova2022well} & 100.00\% &  59.31\% & 79.68\% & 53.82\% & 100.00\% & 71.95\% & 84.72\% & 51.98\% \\
\hline
Linear \textcolor{lightgray}{\scriptsize{[CVPR22]}}\cite{iofinova2022well} & 0.04\% &  18.87\% [0] & 53.72\% [0] & 23.70\% [0] & 0.04\% &  67.46\% [4] & 81.08\% [0] & 30.33\% [0] \\
Partial-1 \textcolor{lightgray}{\scriptsize{[NeurIPS14]}}\cite{yosinski2014transferable} & 8.30\% & 58.44\% [5] & \textbf{78.28\%} [1] & 47.64\% [1] & 8.30\% &  72.31\% [5] & 84.58\% [2] & 47.89\% [1] \\
\hline
Bias \textcolor{lightgray}{\scriptsize{[NeurIPS17]}}\cite{rebuffi2017learning} & 0.16\% &  54.55\% [1] & 75.68\% [1] & \textbf{47.70\%} [0] & 0.16\% & 72.89\% [3] & 81.14\% [0] & 53.43\% [4] \\
Adapter \textcolor{lightgray}{\scriptsize{[NeurIPS20]}}\cite{cai2020tinytl} & 0.87\% &  54.90\% [3] & 75.19\% [1] & 38.98\% [0] & 1.12\% & 74.19\% [4] & 82.66\% [1] & 47.69\% [2] \\
\hline
% VPT-SHALLOW~\cite{jia2022visual} & 1.01$\times$ &  39.96 [1] & 69.65 [0] & 27.50 [0] & 1.01$\times$ & 67.34 [3] & 82.26 [0] & 37.55 [0]  \\
VPT \textcolor{lightgray}{\scriptsize{[ECCV22]}}\cite{jia2022visual} & 0.10\% &  36.02\% [0] & 60.61\% [1] & 26.57\% [0] & 0.06\% &  70.27\% [4] & 83.04\% [0] & 42.38\% [0]\\
\rowcolor{mygray}
Ours &  0.07\% & \textbf{59.52\% [4]} $\left\{6\right\}$ & 77.80\% [1] $\left\{2\right\}$ & 44.65\% [3] $\left\{8\right\}$ & 0.13\% & \textbf{76.47\%} [4] $\left\{7\right\}$ & \textbf{87.28\%} [2] $\left\{4\right\}$  & \textbf{54.91\%} [6] $\left\{8\right\}$ \\
\hline
\end{tabular}
}
\end{small}
\end{center}
\vspace{-1.5em}
\end{table*}



% With the growth of the existing networks, efficient training has been extensively studied for visual-related tasks on CNNs, including side tuning~\cite{}, residual adapter~\cite{}

% \vspace{-0.1em}
\section{Experiment}
\label{sec:Experiment}

% Following~\cite{jia2022visual}, we evaluate Generalized Prompt Tuning for a wide range of recognition tasks with pretrained transformer-based backbones.

\subsection{Experimental Setup}
\label{subsec:Experimental_Setup}

\noindent \textbf{Datasets}.$_{\!}$ Our$_{\!}$ experiments$_{\!}$ are$_{\!}$ carried$_{\!}$ out$_{\!}$ on$_{\!}$ two$_{\!}$ image classification benchmarks. 
\textbf{VTAB-1k}~\cite{zhai2019large} collects 19 benchmarked Visual Task Adaptation, categorized into three groups: (1) \textit{Natural} contains natural images captured by standard cameras, (2) \textit{Specialized} includes images taken by specialized equipment, and (3) \textit{Structured} covers tasks requiring geometric comprehension (\ie, counting, distance). Each task of VTAB-1k contains 1000 training examples. Following~\cite{jia2022visual, zhai2019large}, we apply the 800-200 split for training set on hyperparameter tuning. The final evaluation is run on the full training data.
\textbf{FGVC} contains 5 benchmarked Fine-Grained Visual Classification, including CUB-200-2011~\cite{wah2011caltech}, NABirds~\cite{van2015building}, Oxford Flowers~\cite{nilsback2008automated}, Stanford Dogs~\cite{khosla2011novel} and Stanford Cars~\cite{gebru2017fine}. Following~\cite{jia2022visual}, the training set is randomly split into 90\% \texttt{train} and 10\% \texttt{val}. We use \texttt{val} for hyperparameter tuning.

% \noindent \textbf{T}

\noindent \textbf{Baselines.}
For fair comparison, we follow \cite{jia2022visual} and compare ${\rm E^{2}VPT}$ with other widely applied parameter-efficient fine-tuning methods. Results of two vision transformer architectures, Vision transformer~\cite{dosovitskiy2020image} (ViT) and Swin transformer~\cite{liu2021swin} (Swin), on image classification are discussed in \S\ref{subsec:Comparison_SOTA}. We also apply ${\rm E^{2}VPT}$ to two self-supervised objectives: MAE~\cite{he2022masked} and MoCo v3~\cite{chen2021empirical}. 
%Our model is compared with eight state-of-the-art baselines, including full fine-tune over the backbone~\cite{iofinova2022well,ren2023prepare}, three parameter-efficient partial tuning methods, Linear \cite{iofinova2022well}, Partial-1 \cite{yosinski2014transferable} and MLP-3 \cite{chen2020improved}, three extra module methods, Sidetune \cite{zhang2020side}, Bias \cite{rebuffi2017learning} and Adapter \cite{cai2020tinytl}, and one most recent visual prompt tuning method, VPT \cite{jia2022visual}. For Partial-1, we only fine-tune the top layer of the backbone. For MLP-3, we use 3-layer MLP.  Results of two vision transformer architectures, Vision transformer~\cite{dosovitskiy2020image} (ViT) and Swin transformer~\cite{liu2021swin} (Swin), are presented. We also apply ${\rm E^{2}VPT}$ to two self-supervised objectives: MAE~\cite{he2022masked} and MoCo v3~\cite{chen2021empirical}.

\noindent \textbf{Training.} 
Following \cite{jia2022visual, mahajan2018exploring}, we conduct grid search to match the best tuning hyperparameters, learning rate (\ie, [50, 25, 10, 5, 2.5, 1, 0.5, 0.25, 0.1, 0.05]), and weight decay (\ie, [0.01, 0.001, 0.0001, 0.0]) on \texttt{val} set for each task. Notably, ${\rm E^{2}VPT}$ \textbf{does not require} specific-designed large learning rate in \cite{jia2022visual}. For all models, the learning rate is scheduled following a cosine decay policy and trained for 100 epochs (including 10 warm-up epochs). We follow the same batch size setting in \cite{jia2022visual}: 64/128 for ViT-Base/16 and 80 for Swin-Base, respectively. The number of segments for each token (\S\ref{subsec:Pruning_Rewind}) is set to 8. The percentages for the pruning stage are searched linearly between 10\% and 90\% with 10\% intervals. The rewinding stage applies once to re-train the pruned input prompts. 

\noindent \textbf{Reproducibility.} ${\rm E^{2}VPT}$ is implemented in Pytorch~\cite{NEURIPS2019_9015}. Experiments are conducted on NVIDIA A100-40GB GPUs. To guarantee reproducibility, our full implementation will be publicly released.
% To provide a comprehensive explanation of our implementations, we will release our code.
% implementations, our code and data are publicly released at

\subsection{Comparison with State-of-the-Arts}
\label{subsec:Comparison_SOTA} 
We respectively examine the performance and robustness of ${\rm E^{2}VPT}$ on ViT~\cite{dosovitskiy2020image}, Swin~\cite{liu2021swin}, and two self-supervised objectives --- MAE~\cite{he2022masked} and MoCo v3~\cite{chen2021empirical}. For reference, we provide the individual per-task results for Table~\ref{table:fgvc_vtab_main}, \ref{table:swin} and \ref{table:mae_moco} in Appendix.

\noindent \textbf{\textbf{E$^{2}$VPT} on ViT.} We report the average accuracy score on VTAB-1k and FGVC benchmarks across four diverse task groups for three runs in Table~\ref{table:fgvc_vtab_main}, considering ${\rm E^{2}VPT}$ to the other eight tuning protocols under \textit{pretrain-then-finetune} paradigm. Specifically, Full~\cite{iofinova2022well} updates both backbone and classification head; Linear~\cite{iofinova2022well}, Parital-$1$~\cite{yosinski2014transferable} (top layer) and MLP-$3$~\cite{chen2020improved} (3 MLP layers) are partial tuning methods that only update partial parameters. Sidetune~\cite{zhang2020side}, Bias~\cite{rebuffi2017learning} and Adapter~\cite{cai2020tinytl} are extra module methods which add new trainable parameters to backbone for adaptation; VPT~\cite{jia2022visual} is a most recent visual prompt tuning method.
There are several key observations from these results. \textbf{First}, ${\rm E^{2}VPT}$ is able to outperform the full fine-tuning method in most cases, 21 out of 24 tasks. For example, our model achieves \textbf{0.68\%} improvement on FGVC and \textbf{9.75\%} improvements on VTAB-1k Structured respectively. This observation demonstrates the effectiveness of our approach for fast large-scale vision model adaptation. On the other hand, our model only trains \textbf{0.39\%} parameters in the backbone, which is much more parameter efficient than the full fine-tuned model. \textbf{Second}, it is not surprising to see that the prompt tuning based approaches generally outperform the other parameter efficient methods, such as partial fine-tuning (Partial-1) and extra module (Adapter), indicating the superior adaptability of prompt tuning methods on large-scale vision models. Again, the number of tunable parameters in prompt tuning methods is also smaller compared to the other methods. \textbf{Third}, our approach consistently outperforms the strong VPT model with less tunable prompts, demonstrating the effective design of the key-value prompting and the efficient prompt pruning. The reason is that VPT only focus on design input visual prompts, which fail to capture the accurate interactions between image patches in the new data. In contrast, the key-value prompts in ${\rm E^{2}VPT}$ effectively bridge this gap.

\begin{table*}[t]
% \vspace{-0.5cm}
\caption{\textbf{Impact of different components} in ${\rm E^{2}VPT}$ on two instances: VTAB-1k \textit{Natural} SVHN~\cite{netzer2011reading} and FGVC NABirds~\cite{nabirds}.}
% \vspace{-10pt}
\label{table:ablative_components}
\begin{center}
\begin{small}
\tabcolsep=0.20cm
\resizebox{0.96\textwidth}{!}{
\begin{tabular}{c|c|c||r|r|r||r|r|r} 
\hline \thickhline
\rowcolor{mygray}
\multicolumn{3}{c||}{Fine-tuning Techniques}  & \multicolumn{3}{c||}{VTAB-1k \textit{Natural} SVHN~\cite{netzer2011reading}} & \multicolumn{3}{c}{FGVC NABirds~\cite{nabirds}}\\ 
% Input & Inner & Pruning Rewinding & & VTAB-1k
  % \multicolumn{2}{c||}{(85.8M)}     &  \textit{Natural} [7] & \textit{Specialized} [4] & \textit{Structured} [8]   \\ 
\hline 
% \cline{1-3} 
% \cline{6-7}
\rowcolor{mygray}
\rowcolor{mygray}
% Input   & Inner  & Pruning \& Rewinding & Pruning & Tuned / Total & Accuracy &  Pruning & Tuned / Total & Accuracy \\
Visual Prompts  & Key-Value Prompts & Pruning \& Rewinding & Pruning & Tuned / Total  & Accuracy &  Pruning & Tuned / Total  & Accuracy  \\
% \rowcolor{mygray}
% (\S\ref{subsec:inputs_token}) & (\S\ref{subsec:kv_pairs}) & (\S\ref{subsec:Pruning_Rewind}) & (\%) & (\%) & (\%) &  (\%) & (\%) & (\%) \\
\hline \hline
% \multirow{-2}{*}{(a)}& Before &  \textbf{80.01} [6] & \textbf{84.23} [3]  & \textbf{57.39} [8] \\
% \hline
\checkmark & & & 0.0\% & 0.54\% & 78.1\% & 0.0\% & 1.02\% & 84.2\% \\
\checkmark & \checkmark & & 0.0\% & 0.55\% & 83.8\% & 0.0\% & 1.05\% & 84.5\% \\
\checkmark & & \checkmark & 56.3\% & 0.42\% & 79.0\% & 34.4\% & 0.63\% & 84.2\%\\
\checkmark & \checkmark & \checkmark & 62.5\% & 0.43\% & \textbf{85.3\%} & 40.0\% & 0.65\%  & \textbf{84.6\%} \\
%  & \textit{Trunc. Norm.}~\cite{NEURIPS2019_9015} &    &   \\
% \multirow{-2}{*}{(b)}& \textit{He}~\cite{he2015delving} &   \textbf{80.01} [6] & \textbf{84.23} [3] & \textbf{57.39} [8]  \\
\hline
\end{tabular}
}
\end{small}
\end{center}
\vspace{-2.5em}
\end{table*}

% Our ${\rm E^{2}VPT}$ consistently
% % defined as a global-oriented method, 
% outperforms full fine-tuning on both benchmarks, with \textbf{0.68\%} improvement on FGVC, \textbf{4.13\%}, \textbf{0.87\%} and \textbf{9.75\%} improvements on VTAB-1k \textit{Natural}, \textit{Specialized} and \textit{Structured}, respectively, while using significantly fewer total model parameters (\ie, \textbf{0.39\%} of model parameters). 
% % reach superior performance over all 8 tuning protocols averagely on two benchmarks 
% Notably, ${\rm E^{2}VPT}$ exceeds all the other parameter-efficient methods, including VPT with less parameters, and superior results (\ie, \textbf{0.11\%} over VPT on FGVC, \textbf{1.53\%}, \textbf{2.00\%} and \textbf{2.41\%} over VPT on VTAB-1k \textit{Natural}, \textit{Specialized} and \textit{Structured}, respectively), and establish a new state-of-the-art under \textit{pretrain-then-finetune} paradigm on both FGVC and VTAB-1k benchmarks.
% The individual per-task results are available in Appendix.

% (\ie, \% over VPT on FGVC, \%,\% and \% over FULL on VTAB-1k \textit{Natural}, \textit{Specialized} and \textit{Structured}, respectively)

\noindent \textbf{\textbf{E$^{2}$VPT} on Hierarchical Transformer.} To prove the effectiveness and generalization of our architectural design, we further extend ${\rm E^{2}VPT}$ to a hierarchical transformer --- Swin~\cite{liu2021swin}, where the MSA layer is employed in local shifted windows and patch embeddings are merged at deeper layers. For generality, we follow the same settings in ViT~\cite{dosovitskiy2020image} architecture to prepend K-V learnable pairs and \cite{jia2022visual} for altering input vectors (\ie, these learnable vectors are attended within the local windows and ignored during patch merging). For pruning, we notice performance drop when incorporating within the deeper local windows. We therefore assign pruning stage only to the first stage. As Swin does not use \texttt{[CLS]} and apply the global pooling as input for classification head~\cite{jia2022visual, liu2021swin}, we follow this design when adapting our method. The exclusive experiments are deployed on the ImageNet-21k supervised pretrained Swin-Base~\cite{liu2021swin}. ${\rm E^{2}VPT}$ consistently outperform all the other parameter-efficient methods on all three VTAB-1k problem classes and for the first time surpasses full fine-tuning on VTAB-1k \textit{Specialized} and \textit{Structured} using significantly fewer parameters (\ie, \textbf{0.21\%}).

% To prove the effectiveness of Generalized Prompt Tuning, we further extend to a hierarchical transformer --- Swin~\cite{liu2021swin}, whose representation is computed with shifted windows. The standard multi-head self attention (MSA) module is replaced by shifted window based self attention. For generality, we follow the same settings in ViT~\cite{dosovitskiy2020image} architecture to prepend K-V learnable pairs. For pruning stage, we only apply to the fist layer since we find significant performance loss to apply pruning strategy in shifted window base self attention, which suggests further investigation. However, we still achieve superior performance against \cite{jia2022visual} in Table~\ref{table:swin}. 


\noindent \textbf{Different Pretraining Methods.}
We conducted experiments with two self-supervised objectives, MAE~\cite{he2022masked} and MoCo v3~\cite{chen2021empirical}, on backbones pretrained without labeled data, following the approach of VPT~\cite{jia2022visual}. While VPT yielded inconclusive results on these objectives, our proposed method, ${\rm E^{2}VPT}$, outperformed other methods and achieved competitive performance to full fine-tuning (\textbf{8 of 19} instances under MAE, and \textbf{12 of 19} instances under MoCo v3), using significantly fewer model parameters (\textbf{0.07\%} on MAE and \textbf{0.13\%} on MoCo v3). Our method also outperformed VPT by a large margin (\textbf{59.52\%} vs. 36.02\% under MAE on VTAB-1k \textit{Natural}). We leveraged the gap discussed in VPT, which indicates that self-supervised ViTs are fundamentally different from the supervised ones, and demonstrated the generality of our method to both pretraining objectives.
%Considering the backbones pretrained without labeled data, we follow \cite{jia2022visual} and experiment with two self-supervised objectives --- MAE~\cite{he2022masked} and MoCo v3~\cite{chen2021empirical} in Table~\ref{table:mae_moco}. While VPT gets less conclusive results on self-supervised objectives, ${\rm E^{2}VPT}$ for the first time, outperforms other methods and reach competitive performance to full fine-tuning (\ie, \textbf{8 of 19} instances to Full under MAE, and \textbf{12 of 19} instances under MoCo v3) with significantly lower parameter usage (\ie, \textbf{0.07\%} of model parameters on MAE, \textbf{0.13\%} on MoCo v3). Our method also outperforms VPT with large margins (\eg, 36.02\% $vs$ \textbf{59.52}\% using MAE on VTAB-1k \textit{Natural}). We leverage the gap discussed in VPT that self-supervised ViTs are fundamentally different from the supervised one, and show generality to both pretraining objectives.
% Out method generally show robustness and strong performance


\subsection{Diagnostic Experiments}
\label{subsec:Diagnostic_Experiment}

% \noindent \textbf{Prompt Only on K-V Pairs.} We first look at the impact of training only on prompting V-K pairs (without additional tuning on \texttt{[CLS]} token). Severe
% performance drop is observed (\eg, 28.9\% lower in accuracy on VTAB-1k~\cite{zhai2019large} \textit{Natural} Caltech101). 

% \noindent \textbf{Prompt without Pruning.}

\noindent \textbf{Impact of Different Components.} 
%We first study the impact of different components in ${\rm E^{2}VPT}$, including visual prompts, key-value prompts, and pruning and rewinding. As shown in Table~\ref{table:ablative_components}, we propose two example instances in FGVC~\cite{jia2022visual} and VTAB-1k~\cite{zhai2019large} benchmarks respectively for reference. For SVHN~\cite{netzer2011reading}, the model with visual prompts alone achieves 78.1\% in accuracy. Adding key-value prompts or pruning and rewinding technique brings additional gains (\ie, \textbf{5.7\%}/\textbf{0.9\%}), validating the effectiveness of our prompt tuning technique in self-attention module as well as the pruning mechanism. It is not surprising to see that combing all the components together leads to the best performance, yielding 85.3\% in accuracy. We find similar trends on FGVC NABirds~\cite{nabirds}.
To investigate the impact of different components in ${\rm E^{2}VPT}$, including visual prompts, key-value prompts, and pruning and rewinding, we conducted experiments on two tasks in the benchmarks. The results are summarized in Table~\ref{table:ablative_components}. For SVHN~\cite{netzer2011reading}, we found that the model with visual prompts alone achieved an accuracy of 78.1\%. Adding key-value prompts and applying pruning and rewinding techniques individually led to additional gains (\textbf{5.7\%} and \textbf{0.9\%}), demonstrating the effectiveness of our key-value prompt tuning technique in the self-attention module as well as the pruning mechanism. Finally, combining all components together yielded the best performance, with an accuracy of 85.3\%. We observed similar trends on FGVC NABirds~\cite{nabirds}.


\noindent \textbf{Prompt Location.} An fundamental distinction between ${\rm E^{2}VPT}$ and other methods is the learnable key-value prompts introduced to self-attention. In our implementation, we prepend the key-value prompts to the sequence of Key and Value matrices. Further investigation is required to determine the appropriate placement of the learnable prompts. We provide ablation results on VTAB-1k exhaustively in Table~\ref{table:before_after}(a). We show that both prepending learnable prompts before or after Key and Value matrices show competitive results, validating the robustness of our approach on prompt locations. We choose ``Before" as our baseline method in all our experiments since it achieves slightly better results on average (\ie, 73.94\% $vs$ 73.91\%).

\begin{table}[t]
% \vspace{-0.5cm}
\caption{\textbf{Prompt location and Initialization} results on VTAB-1k~\cite{zhai2019large} in three runs. Per-task results are available in Appendix.}
\vspace{-15pt}
\label{table:before_after}
\begin{center}
\begin{small}
\tabcolsep=0.10cm
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{rr||ccc} 
\hline \thickhline
\rowcolor{mygray}
\multicolumn{2}{c||}{ViT-Base/16~\cite{dosovitskiy2020image}}   &  \multicolumn{3}{c}{VTAB-1k~\cite{zhai2019large} [19]}  \\ 
\rowcolor{mygray}
\rowcolor{mygray}
  \multicolumn{2}{c||}{(85.8M)}     &  \textit{Natural} [7] & \textit{Specialized} [4] & \textit{Structured} [8]   \\ 
\hline \hline
 & After & \textbf{80.67\%} [6] & 84.30\% [3] & 56.76\% [8] \\
\multirow{-2}{*}{(a)}& Before &  80.01\% [6] & \textbf{84.43\%} [3]  & \textbf{57.39\%} [8] \\
\hline
 & \textit{Trunc. Norm.}~\cite{NEURIPS2019_9015} &  79.77\% [6]  &  84.30\% [3] & 56.36\% [8] \\
\multirow{-2}{*}{(b)}& \textit{He}~\cite{he2015delving} &   \textbf{80.01\%} [6] & \textbf{84.43\%} [3] & \textbf{57.39\%} [8]  \\
\hline
\end{tabular}
}
\end{small}
\end{center}
\vspace{-2.5em}
\end{table}

% Figure environment removed
% Figure environment removed

\noindent \textbf{Initialization.} Table~\ref{table:before_after}(b) reports the performance of our approach with respect to two widely adopted initialization methods: 
% \textit{Xavier}~\cite{glorot2010understanding} and 
  \textit{truncated normal}~\cite{narkhede2022review, NEURIPS2019_9015} and \textit{He initialization}~\cite{he2015delving} on VTAB-1k benchmark. The results show that \textit{He initialization} generally provides more stable and preferable performances on average, though we observe that in some specific tasks (\ie, \textit{truncated normal} is 1.1\% higher in accuracy over \textit{He} on VTAB-1k \textit{Specialized} Diabetic Retinopathy Detection~\cite{diabetic-retinopathy-detection}) \textit{truncated normal} gets slightly better results. In conclusion, ${\rm E^{2}VPT}$ shows robustness on different initialization methods and is able to achieve consistent performance with full fine-tuning.

\noindent \textbf{Prompt Length.} 
Prompt length is the only hyper-parameter needed to tune in ${\rm E^{2}VPT}$. To further analyze the impact of different prompt lengths on the model performance, we conducted a comprehensive study on the lengths of visual prompts and key-value prompts for a better understanding of their characteristics on VTAB-1k \textit{Natural} SVHN~\cite{netzer2011reading}. The length of visual prompts is typically limited to [5, 10, 20, 30, 50], while the length of key-value prompts is restricted to [1, 5, 10, 50], which is a standard configuration for most datasets. The model performance results on different prompt length combinations are reported in Fig.~\ref{fig:prompt_length}. It can be seen that, when using 50 visual prompts, a relative shorter key-value prompt can benefit performance notably (\ie, 84.7\% when introducing one key-value prompt $vs$ 78.1\% without key-value prompts), while further increasing the length of the key-value prompt yields a small performance gain (\ie, 85.3\% when using 5 key-value prompts). We also notice that using a large number of key-value prompts lead to subpar results (\ie, 80.2\% with 20 key-value prompts). Similar patterns are observed with other visual prompt lengths. 
We argue that a heavy parameter engineering in self-attention layer might distort the original attention map and does harm to adaptation.

% It is clear that there is no universal optimal prompt length that can achieve the best performance across all attributes. For example, on `Boot Style', MixPAVE with prompt length 4 obtains the highest F1 score, while our model with prompt length 96 gains the best performance on `Compatibility'. Our hypothesis is that different attributes contain different data distributions, where attribute value extraction is more difficult on certain attributes than others. These ``hard'' attributes usually require longer prompts in order to better capture the patterns and knowledge from the data, with the cost of more trainable parameters.

% In Fig.~\ref{fig:prompt_length}, we conducted a comprehensive study on the lengths of input prompts and key-value prompts for a better understanding of their characteristics on VTAB-1k \textit{Natural} SVHN~\cite{netzer2011reading}. The length of input prompt is typically limited to [5, 10, 20, 30, 50], while the length of key-value prompt is restricted to [1, 5, 10, 50], which is a standard configuration for most datasets. We find that a relative shorter key-value prompt can benefit performance notably (\ie, 84.7\% $vs$ 78.1\% when introducing length=1 key-value prompt), while further increasing the length of the key-value prompt yields a small performance gain (\ie, 85.3\% $vs$ 84.7\%). We also notice that that using a key-value prompt that is too lengthy can lead to subpar results (\ie, 80.2\% $vs$ 85.3\%). \textcolor{red}{We argue that a heavy parameter engineering in self-attention layer might distort the original attention map and does harm to adaptation.}

\subsection{Visualization}
\label{subsec:Hyperbolic}
Following \cite{atigh2022hyperbolic, ermolov2022hyperbolic, ganea2018hyperbolic, khrulkov2020hyperbolic, peng2021hyperbolic}, we show hyperbolic visualizations results on training set for VPT and ours on three tasks in FGVC (\ie, CUB-200-2011~\cite{wah2011caltech}, Oxford Flowers~\cite{nilsback2008automated}, and Stanford Dogs~\cite{khosla2011novel}). Hyperbolic space, to be specific, is a Riemannian manifold of constant negative curvature. While there are several isometric models of hyperbolic space, we follow previous work~\cite{ermolov2022hyperbolic, ganea2018hyperbolic} and stick to the Poincar\'e ball model.
Similar to~\cite{ermolov2022hyperbolic}, we use UMAP~\cite{mcinnes2018umap} with the ``hyperboloid" distance metric to reduce the dimensionality to 2D. ViT-Base plays as an encoder with two types of pretraining (\ie, tuned models under VPT, and ours after rewinding, respectively). We freeze the models during fine-tuning and output embeddings are mapped to hyperbolic space. Adam optimizer~\cite{loshchilov2017decoupled} with a learning rate of $3 \times 10^{-5}$ is applied to all settings. The weight decay is 0.01 with batch size equals to 900. All models are trained for 50 steps for fair comparison, with a gradient clip by norm 3.

Fig.~\ref{fig:visulization} illustrates how learned embeddings are arranged on the Poincar\'e disk. 
We can see that in ${\rm E^{2}VPT}$, samples are clustered according to labels, and each cluster is pushed closer to the border of the disk, indicating that the encoder separates class well. On the other hand, we observe in VPT that some of the samples move towards the center and intermix~\cite{ermolov2022hyperbolic}, indicating possible confusion during projection. We also follow~\cite{ermolov2022hyperbolic, peng2021hyperbolic, khrulkov2020hyperbolic} and present the Recall@K metric in Appendix for reference. These visualization results further validate the effectiveness of the proposed ${\rm E^{2}VPT}$ approach in generating separatable embeddings from the input images in the new tasks.

\vspace{-1em}
\section{Conclusion and Discussion} \label{sec:conclusion_discussion}
\vspace{-0.3em}
The vast majority of current efforts under the \textit{pretrain-then-finetune} paradigm seek to reduce parameter usage while overlooking the inner design of transformer-based architecture. In light of this view, 
we present ${\rm E^{2}VPT}$, a new parameter-efficient visual prompt tuning approach to model the transformer architecture during adaptation. It enjoys several advantages: \textbf{i)} consider self-attention mechanism during tuning for superior performance to current parameter-efficient fine-tuning; and \textbf{ii)} apply pruning and rewinding stages to reduce parameter usage in input visual prompts. The systemic merits enable an  effective yet efficient algorithm. As a whole, we conclude that the outcomes elucidated in this paper impart essential understandings and necessitate further exploration within this realm.

\noindent \textbf{Acknowledgements.} 
This research was supported by the National Science Foundation under Grant No. 2242243.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}