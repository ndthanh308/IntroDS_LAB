{
  "title": "E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning",
  "authors": [
    "Cheng Han",
    "Qifan Wang",
    "Yiming Cui",
    "Zhiwen Cao",
    "Wenguan Wang",
    "Siyuan Qi",
    "Dongfang Liu"
  ],
  "submission_date": "2023-07-25T19:03:21+00:00",
  "revised_dates": [],
  "abstract": "As the size of transformer-based models continues to grow, fine-tuning these large-scale pretrained vision models for new tasks has become increasingly parameter-intensive. Parameter-efficient learning has been developed to reduce the number of tunable parameters during fine-tuning. Although these methods show promising results, there is still a significant performance gap compared to full fine-tuning. To address this challenge, we propose an Effective and Efficient Visual Prompt Tuning (E^2VPT) approach for large-scale transformer-based model adaptation. Specifically, we introduce a set of learnable key-value prompts and visual prompts into self-attention and input layers, respectively, to improve the effectiveness of model fine-tuning. Moreover, we design a prompt pruning procedure to systematically prune low importance prompts while preserving model performance, which largely enhances the model's efficiency. Empirical results demonstrate that our approach outperforms several state-of-the-art baselines on two benchmarks, with considerably low parameter usage (e.g., 0.32% of model parameters on VTAB-1k). Our code is available at https://github.com/ChengHan111/E2VPT.",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13770",
  "pdf_url": null,
  "comment": "12 pages, 4 figures",
  "num_versions": null,
  "size_before_bytes": 3722142,
  "size_after_bytes": 1135391
}