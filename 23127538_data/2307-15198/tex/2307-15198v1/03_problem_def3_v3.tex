\section{PRELIMINARIES} 

In this section, we begin by introducing the relevant concepts and notations. We then proceed to formally define the problem of one-shot joint brain extraction, registration, and segmentation.

\subsection{Notations and Definitions}

\noindent \textbf{Definition 1 (Source, target and target segmentation mask).}
Suppose we are given a training dataset $\mathcal{D} = \left\{\{\mathbf{S}_i\}_{i=1}^{Z}, (\mathbf{T}, \mathbf{B})\right\}$ that consists of $Z$ source images $\mathbf{S}_{i} \in \mathbb{R}^{W \times H \times D}$, and a pair of target image $\mathbf{T} \in \mathbb{R}^{W \times H \times D}$ and its corresponding segmentation mask $\mathbf{B} \in \{0, 1\}^{C \times W \times H \times D}$. Here, the source image $\mathbf{S}_i$ is the raw MRI scan of a patient's head, the target $\mathbf{T}$ is a standard template of the brain, and the segmentation mask $\mathbf{B}$ is the one-hot encoding of the target $\mathbf{T}$ segmentation. $W$, $H$, and $D$ denote the width, height and depth dimensions of the 3D images, $C$ denotes the number of anatomical labels (\eg the number of labelled brain regions). 
To ensure simplicity, we make the assumption that the source and target images are resized to the same dimension, denoted as $W \times H \times D$. Next, we omit the subscript $i$ of $\mathbf{S}_{i}$ for ease of notation. 


%For simplicity, we assume that the source and target images are resized to the same dimension, \ie $W \times H \times D$. 
%In the following discussion, we omit the subscript $i$ of $\mathbf{S}_{i}$ for ease of notation. 

\noindent \textbf{Definition 2 (Brain extraction mask).}
Brain extraction mask $\mathbf{M} \in \{0,1\}^{W \times H \times D}$ is a binary tensor of identical dimensions to the source image $\mathbf{S}$.
It represents cerebral tissues in $\mathbf{S}$ with a value of 1 and non-cerebral tissues with 0.
The extracted image $\mathbf{E} = \mathbf{S} \circ \mathbf{M}$ is obtained by applying the $\mathbf{M}$ on $\mathbf{S}$ via a element-wise product $\circ$.

\noindent \textbf{Definition 3 (Affine transformation and warped image).}
In order to maintain generality, we consider the transformation in the registration task to be affine-based. 
%Without loss of generality, we consider that the transformation in the registration task is affine-based. 
Extending this work to encompass other types of registration, such as nonlinear/deformable registration, is straightforward.
%Extending this work to other types of registration, such as nonlinear/deformable ones is straightforward.
The affine transformation parameters $\mathbf{a} \in \mathbb{R}^{12} $ is a vector used to parameterized an 3D affine transformation matrix $\mathbf{A} \in \mathbb{R}^{4 \times 4} $. The warped image $\mathbf{W} = \mathcal{T}\left(\mathbf{E},\mathbf{a}\right)$ results from applying the affine transformation on the extracted image $\mathbf{E}$, where $\mathcal{T}(\cdot, \cdot)$ denotes the affine transformation operator. 
At the voxel level, the relationship between $\mathbf{W}$ and $\mathbf{E}$ can be expressed as:
%The following relationship holds for $\mathbf{W}$ and $\mathbf{E}$ on the voxel level: 
\begin{equation}
\label{equ:voxel_value}
\mathbf{W}_{xyz} = \mathbf{E}_{x'y'z'},
\end{equation}
where the correspondences between coordinates $x,y,z$ and $x',y',z'$ are determined on the affine transformation matrix $\mathbf{A}$:
\begin{equation}
\begin{bmatrix}
x'\\
y'\\
z' \\
1
\end{bmatrix} = \mathbf{A}\begin{bmatrix}
x\\
y\\
z \\
1
\end{bmatrix} = \begin{bmatrix}
a_{1} & a_{2} & a_{3} & a_{4}\\
a_{5} & a_{6} & a_{7} & a_{8}\\
a_{9} & a_{10} & a_{11} & a_{12}\\
0 & 0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
x\\
y\\
z \\
1
\end{bmatrix}.
\end{equation}

\noindent \textbf{Definition 4 (Source segmentation mask).}
Brain segmentation mask $\mathbf{R} \in \{0,1\}^{C \times W \times H \times D}$ is a binary tensor with the first dimension being the number of anatomical labels and the rest dimensions identical to the source image $\mathbf{S}$. Every point in the source image $\mathbf{S}$ is densely labeled according to its anatomical structure, encoded in a one-hot vector at the corresponding coordinate of $\mathbf{R}$.

\input{fig_formulation}
\subsection{Problem Formulation}

The goal of joint brain extraction, registration, and segmentation is to collectively learn the extraction function $f_{\theta}: \mathbb{R}^{W \times H \times D} \rightarrow \mathbb{R}^{W \times H \times D}$, the registration function $g_{\phi}: \mathbb{R}^{W \times H \times D}\times \mathbb{R}^{W \times H \times D} \rightarrow \mathbb{R}^{12}$, and the segmentation function $h_{\psi}: \mathbb{R}^{W \times H \times D} \rightarrow \mathbb{R}^{C \times W \times H \times D}$, as shown in Figure~\ref{fig:formulation}. Specifically, the extraction function $f_{\theta}(\cdot)$ utilizes the source image $\mathbf{S}$ as input to predicts a brain extraction mask $\hat{\mathbf{M}} = f_{\theta}(\mathbf{S})$. The registration function $g_{\phi}(\cdot, \cdot)$ takes the extracted brain image $\hat{\mathbf{E}} = \hat{\mathbf{M}} \circ \mathbf{S}$ and the target image $\mathbf{T}$ to predict the affine transformation parameter $\hat{\mathbf{a}} = g_{\phi}(\hat{\mathbf{E}},\mathbf{T})$, then obtaining the warped image $\hat{\mathbf{W}} = \mathcal{T}(\hat{\mathbf{E}},\hat{\mathbf{a}})$. 
The warped segmentation mask $\hat{\mathbf{V}} = \mathcal{T}(\mathbf{B},\hat{\mathbf{a}}^{-1})$ is generated by warping the target segmentation mask $\mathbf{B}$ using inversed affine transformation $\hat{\mathbf{a}}^{-1}$. Finally, the segmentation function  $h_{\psi}(\cdot)$ takes the source image $\mathbf{S}$ as input to predict a source brain segmentation mask $\hat{\mathbf{R}} = h_{\psi}(\mathbf{S})$.
The optimal parameter $\theta^*$, $\phi^*$ and $\psi^*$ can be found by solving the following optimization problem:
\begin{equation}
\begin{split}
\label{equ:goal_training}
\theta^{*},\phi^{*},\psi^{*} &=\underset{\theta,  \phi,\psi}{\arg \min } \hspace{-3pt} \sum_{\left(\mathbf{S},  \mathbf{T}, \mathbf{B}\right)\in \mathcal{D}}\left[ \mathcal{L}_{sim} \left( \hat{\mathbf{W}}, \mathbf{T} \right) + \lambda \mathcal{L}_{seg} \left( \hat{\mathbf{R}}, \hat{\mathbf{V}}  \right)\right],
\end{split}
\end{equation}
where the image pair $(\mathbf{S},\mathbf{T},\mathbf{B})$ is sampled from the training dataset $\mathcal{D}$. $\mathcal{L}_{sim}(\cdot, \cdot)$ is image dissimilarity criteria, \eg mean square error and $\mathcal{L}_{seg}(\cdot, \cdot)$ is segmentation dissimilarity criteria, \eg cross entropy error. These two criteria guide a joint optimization of extraction, registration and segmentation functions, allowing feedback among them.

To the best of our knowledge, this work is the first endeavor in finding an optimal solution for the one-shot joint brain image extraction, registration, and segmentation problem. Our approach eliminates the necessity of labeling the brain extraction masks, transformation, and segmentation masks of the source image. We only require one pair of a target image and its corresponding segmentation mask to guide the training, as opposed to other fully supervised methods~\cite{kleesiek2016deep,lucena2019convolutional,sokooti2017nonrigid, dai2020dual, akkus2017deep, chen2019learning, kamnitsas2017efficient}.

