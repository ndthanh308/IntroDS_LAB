\input{fig_network2}
\section{Our Approach}
\label{sec:method}
\noindent\textbf{Overview.} Figure~\ref{fig:network} presents an overview of the proposed JERS framework for the one-shot joint brain extraction, registration, and segmentation problem. Our method is an end-to-end deep neural network consisting of three main branches: 1) \emph{Extraction Module} takes the raw source image $\mathbf{S}$ as input, and progressively produces the extracted brain image $\mathbf{E}^{M}$ after $M$ stages of extraction; 2) \emph{Registration Module} takes the extracted brain image $\mathbf{E}^{M}$, target image $\mathbf{T}$ and target segmentation mask $\mathbf{B}$ as inputs, and incrementally aligns $\mathbf{E}^{M}$ with $\mathbf{T}$ through $N$ stages of registration. Then, it generates the warped segmentation mask $\mathbf{V}$ (\ie segmentation mask in the source image space) by conducting inverse transformation on the target segmentation mask $\mathbf{B}$; 3) \emph{Segmentation Module} takes the raw source image $\mathbf{S}$ as inputs, and output the brain segmentation mask $\mathbf{R}$. The final output of our network is the extracted brain image $\mathbf{E}^{M}$ constituting only cerebral tissues, the warped image $\mathbf{W}^{N}$ aligning with the target image, and the brain segmentation mask $\mathbf{R}$ indicating anatomical regions of the source image.

\noindent \textbf{Key Insight.}  Considering the highly correlated characteristics of the extraction, registration, and segmentation tasks, the design of our method revolves around the following four positive mechanisms among the three modules:

\textbullet  \  The unalignable non-brain tissue revealed in the registration module guides the refinement process in the extraction module.

\textbullet  \  The registration module benefits from accurate brain extraction generated in the extraction module.

\textbullet  \  The anatomical structure estimated by the segmentation module can provide auxiliary information for the extraction module to remove non-cerebral tissues and for the registration module to find regional correspondences. 

\textbullet  \  The segmentation module relies on the extraction and registration modules to generate the corresponding segmentation mask in the source image space.


Hence, we encode the above four positive mechanisms in our loss function design and train our network in an end-to-end fashion. Specifically, our loss function includes two perceptual losses: the image similarity term $\mathcal{L}_{sim}( \mathbf{W}^{N}, \mathbf{T})$ and the segmentation loss term $\mathcal{L}_{seg}( \mathbf{R}, \mathbf{V})$.
The image similarity loss $\mathcal{L}_{sim}(\cdot, \cdot)$ captures the first and second positive mechanisms, which is affected by extraction and registration modules. 
The segmentation loss $\mathcal{L}_{seg}(\cdot, \cdot)$ embodies the third and fourth positive mechanisms, which is related to all modules.
We jointly optimize the two loss terms to guide the learning of extraction, registration, and segmentation modules.
Next, we introduce the details of each module and the training process.

\subsection{Extraction Module}
\label{sec: Extration Net}
For the extraction module, we have adopted the multi-stage design paradigm, a strategy that has proven effective in previous works~\cite{su2022ernet, zhao2019recursive, feng2021recurrent,tang2019multi,su2022abn}. This design allows for a progressive refinement in the removal of non-cerebral tissues, culminating in an image with only cerebral tissues at the final stage.
Following the multi-stage design of~\cite{su2022ernet}, the extraction module incorporates $M$ extraction stages, and each stage $j$ contains two key components as follows.


\subsubsection{Extraction Network: $f_{e}$}
\label{sec: fe}
The extraction network $f_{e}(\cdot)$ acts as an eliminator, intended to sequentially eradicate non-cerebral elements from source image $\mathbf{S}$, aiming to retain only brain tissue in the image obtained at the final stage. With each stage $j$, it forms an extraction mask $\mathbf{M}^{j}$ , which is utilized to eliminate non-cerebral tissues in the preceding extracted brain image $\mathbf{E}^{j-1}$. 
Specifically, we employ the 3D U-Net \cite{ronneberger2015u} as the base network to learn $f_{e}(\cdot)$.
When conducting inference, the output $\mathbf{M}^{j}$ is binarized by a Heaviside step function.
To facilitate effective gradient backpropagation during the training phase, we employ a Sigmoid function with a large slope parameter to approximate the Heaviside step function. 
$f_{e}(\cdot)$ employs a shared-weight scheme, which means $f_{e}(\cdot)$ is repeatedly applied across all stages with the same set of parameters.
The process can be formally expressed as:

\begin{equation}
\mathbf{M}^{j}=f_{e}\left(\mathbf{E}^{j-1}\right),
\end{equation}
where $\mathbf{M}^{j}$ is the outputted brain mask of the $j$-th stage for $j = [1, \cdots, M]$ and $\mathbf{E}^{0} = \mathbf{S}$.

\subsubsection{Overlay Layer: $OL$}
The overlay layer serves to eradicate any residual non-cerebral tissues by applying the current brain mask $\mathbf{M}^{j}$ to the preceding extracted image $\mathbf{E}^{j-1}$. The updated extraction is $ \mathbf{E}^{j} = \mathbf{E}^{j-1}\circ \mathbf{M}^{j}$, where $\circ$ denotes the operation of element-wise multiplication.


\subsection{Registration Module}
Similar to the extraction module introduced in Section \ref{sec: Extration Net}, we execute a multi-stage paradigm~\cite{su2022ernet, zhao2019recursive, feng2021recurrent,tang2019multi,su2022abn} to tackle the registration task. 
Following the multi-stage
settings of~\cite{su2022ernet}, the module is composed of $N$ cascading stages, with each stage $k$ holding the key components as follows.


\subsubsection{Registration Network: $f_{r}$}
The purpose of the registration network $f_{r}(\cdot, \cdot)$ is to slowly adapt the extracted brain image to better match the target image. Each stage $k$ generates a current affine transformation $\mathbf{A}_\text{i}^{k}$ based solely on the prior warped image $\mathbf{W}^{k-1}$ and the target image $\mathbf{T}$.
Similar to the extraction network $f_e$ in Section~\ref{sec: fe}, a 3D CNN-based encoder is used to learn $f_{r}(\cdot, \cdot)$ and a shared weight design is employed across stages using identical parameters. As a formal expression,
\begin{equation}
\mathbf{A}_\text{i}^{k}=f_{r}\left(\mathbf{W}^{k-1}, \mathbf{T}\right),
\end{equation}
where $\mathbf{A}_\text{i}^{k}$ represents the output affine transformation of the $k$-th stage for $k = [1, \cdots, N]$ and $\mathbf{W}^{0} = \mathbf{E}^{M}$.


\subsubsection{Composition Layer: $\text{COMP}$}
For each stage $k$, upon determining the current affine transformation $\mathbf{A}_\text{i}^{k}$ through $f_{r}(\cdot, \cdot)$, we combine all preceding transformations: $\mathbf{A}_\text{c}^{k}=\mathbf{A}_\text{i}^{k} \cdot \mathbf{A}_\text{c}^{k-1}$, where $\cdot$ denoting the matrix multiplication operation. 
Thus, the combined transformation $\mathbf{A}_\text{c}^{k}$ can always be performed on the extracted image $\mathbf{E}^{M}$ to preserve the sharpness of the warped images~\cite{su2022abn}.
For $k=1$, the initial affine transformation $\mathbf{A}_\text{c}^{0}$ is designated as an identity matrix, indicating no displacement.


\subsubsection{Spatial Transformation Layer: $\text{STL}$}
\label{sec: STL}
A crucial process in image registration involves reconstructing the warped image $\mathbf{W}^{k}$ from the extracted brain image $\mathbf{E}^{M}$ using the affine transformation operator. Utilizing the combined transformation $\mathbf{A}_\text{c}^{k}$, we introduce a spatial transformation layer (STL) that resamples voxels from the extracted image to produce the warped image through $\mathbf{W}^{k} = \mathcal{T}(\mathbf{E}^{M}, \mathbf{A}_\text{c}^{k})$.
Given the affine transformation operator depicted in Eq.~(\ref{equ:voxel_value}), we hold
\begin{equation}
     \mathbf{W}_{xyz}^{k} = \mathbf{E}_{x'y'z'}^{M} \hspace{1pt},
     \label{equ:voxel_value_k}
\end{equation}
where $[x', y', z', 1]^\top = \mathbf{A}_\text{c}^{k}[x, y, z, 1]^\top $. To assure successful gradient propagation during this procedure, we adopt a differentiable transformation founded on trilinear interpolation proposed by~\cite{jaderberg2015spatial}.


\subsection{Inverse Warping and Segmentation Module}
This section introduces the two key designs to solve the segmentation task for the input source image $\mathbf{S}$: 1) \emph{Inverse Warping} takes the target segmentation mask $\mathbf{B}$ and the composed affine transformation $\mathbf{A}_\text{c}^{N}$ as inputs, and generates a segmentation mask $\mathbf{V}$ in the source image space; 2) \emph{Segmentation Network} takes the source image $\mathbf{S}$ as input, and predicts the brain segmentation mask $\mathbf{R}$ for $\mathbf{S}$.  

\subsubsection{Inverse Warping}
In the training dataset, only the target image $\mathbf{T}$ is labeled with its segmentation mask $\mathbf{B}$. In order to generate the segmentation label for guiding the segmentation network, we warp the target image segmentation make $\mathbf{B}$ into the source image space by applying the inverse of $\mathbf{A}_\text{c}^{N}$:
\begin{equation}
    \mathbf{V}_{cxyz} = \mathbf{B}_{cx'y'z'}, \forall  c \in \{0, 1 ,\ldots, C-1\}
\end{equation}
where $[x', y', z', 1]^\top = (\mathbf{A}_\text{c}^{N})^{-1}[x, y, z, 1]^\top $, $c$ is the index for anatomical class. 
Same as the $\text{STL}$ in Section~\ref{sec: STL}, we apply a differentiable transformation based on trilinear interpolation. 

\subsubsection{Segmentation Network: $f_s$}
The segmentation network is tasked to predict a segmentation mask for the source image $\mathbf{S}$ that matches the synthesized segmentation mask $\mathbf{V}$. Similar to the extraction network discussed in Section \ref{sec: fe}, we employ the popular 3D U-Net as the base network to learn $f_s(\cdot)$. Formally, we have:
\begin{equation}
    \mathbf{R} = f_s(\mathbf{S}).
\end{equation}

\subsection{End-to-End Training}
\label{section:end-to-end training}

We train our JERS model by minimizing the following objective function
\begin{equation}
\label{eq:loss}
\theta^*, \phi^*, \psi^* = \underset{\theta, \phi, \psi}{\min} \mathcal{L}_{sim}(\mathbf{W}^N, \mathbf{T}) + \lambda\mathcal{L}_{seg}(\mathbf{R}, \mathbf{V})  + \eta \sum_{j=1}^{M} \mathcal{R}(\mathbf{M}^{j}),
\end{equation}
where $\theta, \phi, \psi$ are the parameters for the extraction, registration, and segmentation networks, respectively. $\mathcal{L}_{sim}(\cdot, \cdot)$ is the image  similarity loss, $\mathcal{L}_{seg}(\cdot, \cdot)$ is the segmentation loss, $\mathcal{R}$ is the extraction regularization term, and $\lambda, \eta$ are positive trade-off weights. 

The image similarity loss $ \mathcal{L}_{sim}(\cdot, \cdot)$ is a loss function measuring the similarity between the final warped image $\mathbf{W}^{N}$ and the target image $\mathbf{T}$:
\begin{equation}
    \mathcal{L}_{sim}\big(\mathbf{W}^{N}, \mathbf{T}\big) =  \mathcal{L}_{sim}\Big(\mathcal{T}\big(\mathbf{E}^{M}, \mathbf{A}_\text{c}^{N}\big), \mathbf{T}\Big). 
\end{equation}
We employ  the prevailing negative local cross-correlation loss for $\mathcal{L}_{sim}(\cdot, \cdot)$, which is resistant to variations in voxel intensity typically encountered across different scans and datasets~\cite{balakrishnan2018unsupervised, xu2019deepatlas, zhao2019recursive}.
The image similarity loss can guide the learning of both the extraction and registration modules. 


The segmentation loss $\mathcal{L}_{seg}(\cdot, \cdot)$ is a loss function measuring the similarity between the predicted segmentation mask $\mathbf{R}$ and the warped segmentation label $\mathbf{V}$:
\begin{equation}
\mathcal{L}_{seg}(\mathbf{R}, \mathbf{V}) = \mathcal{L}_{seg}\left(f_s\big(\mathbf{S}\big), \mathcal{T}\Big(\mathbf{B}, \big(\mathbf{A}_\text{c}^{N}\big)^{-1}\Big)\right)
\end{equation}
Here we use the cross-entropy loss function. The segmentation loss serves multiple purposes and can guide the learning of the extraction, registration, and segmentation modules. 

Finally, to suppress the occurrence of other connected regions apart from the brain in the extraction masks, we define the extraction regularization loss:
\begin{equation}
    \mathcal{R}(\mathbf{M}^{j}) = \textstyle\sum_{x=1}^{W} \sum_{y=1}^{H}  \sum_{z=1}^{D}\|\nabla \mathbf{M}_{xyz}^{j}\|^{2}.
\end{equation}
This regularization term quantifies the edge strength of the predicted extraction mask $\mathbf{M}^{j}$, \ie the likelihood of a voxel being an edge voxel. Through the minimization of this term, we can suppress the occurrence of edges, subsequently producing a smooth extraction mask. In this scenario, we apply the $\ell_2$-norm of the first-order derivative of $\mathbf{M}^{j}$ as the regularization term.


By leveraging the differentiability in each component of this design, our model can be jointly and progressively optimized in an end-to-end fashion. This training approach empowers us to unearth a joint optimal solution for the aggregate tasks of brain extraction, registration, and segmentation. Thus our work stood in marked contrast to other works \cite{su2022ernet, xu2019deepatlas} that resort to alternative training of individual modules and can only reach a sub-optimal solution. 


