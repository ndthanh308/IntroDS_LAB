%\section{Experiment and Analysis}
\section{Experiments}

\subsection{Datasets} \label{section:Dataset}

The effectiveness of our proposed method is evaluated across three public real-world 3D brain MRI datasets: 1) \emph{LPBA40}~\cite{shattuck2008construction} includes 40 raw T1-weighted 3D MRI scans, coupled with brain masks and 56 anatomical structures as segmentation ground truths; 2) \emph{CC359}~\cite{souza2018open} contains 359 raw T1-weighted 3D brain MRI scans and the brain masks. It also includes labeled white matter as the segmentation ground truth; 3) \emph{IBSR}~\cite{rohlfing2011image} consists of 18 scans with manual segmentation labels. But due to its small size, it serves only for the evaluation of the model trained on CC359. Brain masks and anatomical labels facilitate the accuracy evaluation for extraction and segmentation, respectively. Additional details can be found in Appendix~\ref{section: appendix Data Preprocessing}. 


\iffalse
\noindent\textbullet\ \textit{LONI Probabilistic Brain Atlas (LPBA40)}~\cite{shattuck2008construction}: 
This dataset consists of 40 raw T1-weighted 3D brain MRI scans along with their brain masks. It also provides the corresponding segmentation ground truth of 56 anatomical structures. The brain mask and anatomical segmentations are used to evaluate the accuracy of extraction and registration, respectively. Same to~\cite{balakrishnan2018unsupervised, zhao2019recursive}, we focus on atlas-based registration, where the first scan is the target image and the remaining scans need to align with it. Among the 39 scans, we use 30, 5, and 4 scans for training, validation, and testing, respectively. All scans are resized to $96\times96\times96$ after cropping.


\noindent\textbullet\ \textit{Calgary-Campinas-359 (CC359)}~\cite{souza2018open}: 
This dataset consists of 359 raw T1-weighted 3D brain MRI scans and the corresponding brain masks. It also contains the labeled white matter as the ground truth. We use brain masks and white-matter masks to evaluate the accuracy of extraction and registration, respectively. Like LPBA40, we concentrate on atlas-based registration and split CC359 into 298, 30, and 30 scans for training, validation, and test sets. All scans are resized to $96\times96\times96$ after cropping.


\noindent\textbullet\ \textit{Internet Brain Segmentation Repository (IBSR)} \cite{rohlfing2011image}: 
This dataset provides 18 raw T1-weighted 3D brain MRI scans along with the corresponding manual segmentation results. 
Due to the small sample size, We use this dataset only to test the model trained on CC359. 
Thus, all 18 scans need to align with the first scan of CC359. All scans are resized to $96\times96\times96$ after cropping.
\fi


\subsection{Compared Methods}
\label{section:Compared Methods}

We outline the comparison of our JERS with several representative methodologies in brain extraction, registration and segmentation, as illustrated in Table~\ref{tab:methods}. Notably, there are no existing solutions that can seamlessly learn brain extraction, registration, and segmentation in an end-to-end framework. Hence, we designed three-stage pipeline for comparison, combining various extraction, registration, and segmentation methods. Baselines detailed in Appendix~\ref{section: appendix baselines settings}. 


\noindent\textbullet\ \textit{Brain Extraction Tool (BET)} \cite{smith2002fast}: 
This technique within the FSL package uses deformable models for skull stripping.


\noindent\textbullet\ \textit{3dSkullStrip (3dSS)} \cite{cox1996afni}: This is a BET variant method within the AFNI package. It uses a spherical surface to perform skull stripping.

\noindent\textbullet\ \textit{Brain Surface Extractor (BSE)} \cite{shattuck2002brainsuite}: This method uses morphological operations and edge detection for brain extraction, which leverages anisotropic diffusion filtering and a Marr Hildreth edge detector to identify the brain boundary.

\noindent\textbullet\ \textit{FMRIB's Linear Image Registration Tool (FLIRT)} \cite{jenkinson2001global}: This is an automatic affine brain image registration tool in the FSL package.

\noindent\textbullet\ \textit{Advanced Normalization Tools (ANTs)} \cite{avants2009advanced}: This is a state-of-the-art medical image registration toolbox. We set the registration type and optimization as affine and cross-correlation metrics.


\noindent\textbullet\ \textit{VoxelMorph (VM)} \cite{balakrishnan2018unsupervised}: This is an unsupervised image registration method that uses a neural network to predict the transformation.

\noindent\textbullet\ \textit{Cascaded Registration Networks (CRN)} \cite{zhao2019recursive}:
This is an unsupervised multi-stage image registration method, which incrementally aligns the source image to the target image.  

\noindent\textbullet\ \textit{Directly Warping (DW)~\cite{jaderberg2015spatial}}:
It is an operation used to generate the segmentation mask by registration. The segmentation mask of the target image can be directly warped to the source image space by the completed registration.

\noindent\textbullet\ \textit{DeepAtlas \cite{xu2019deepatlas}}: This is a joint learning network for image registration and segmentation tasks.

\noindent\textbullet\ \textit{ERNet \cite{su2022ernet}}: This is an unsupervised learning method for joint extraction and registration.


\input{tab_methods_v4}
\input{tab_main_res_v2}


\subsection{Experimental Results}
Our JERS is compared with baseline methods on extraction, registration and registration accuracy. We measure the performance of each task with its corresponding metrics, and also record the time taken for each baseline method. Based on the experimental results, we find that JERS not only consistently outperforms other alternatives in terms of extraction, registration, and segmentation, but also exhibits superior time efficiency and robustness.


\subsubsection{Evaluation Metrics.}



To evaluate the extraction and segmentation accuracy, we measure the volume overlap between the predicted and ground-truth masks. To evaluate the registration performance, we calculate the mutual information between the warped image and the target image. Details can be found in Appendix~\ref{section: appendix evaluation Metrics}.  





\subsubsection{Experiment Setting.}
We divide the datasets into training, validation and test sets. The training set is utilized for parameter learning of the model, while the validation set is employed to evaluate the performance of hyperparameter settings (\eg the weight of the segmentation loss term). The test set is used only once to report the final evaluation results for each model. It should be noted that the IBSR dataset is exclusively used for testing purposes. We describe the detail of the data processing, JERS settings and baseline settings in Appendix~\ref{section: appendix Data Preprocessing}, \ref{section: appendix jers settings} and \ref{section: appendix baselines settings}.
The source code is available at~\url{https://github.com/Anonymous4545/JERS}.



\input{fig_main_res_v2}

\subsubsection{Extraction, Registration and Segmentation Results.} Table~\ref{tab:main res} presents the results for the compared methods as well as the proposed JERS in extraction, registration, and segmentation tasks. Through a comprehensive evaluation across three datasets, JERS outperforms existing methods in all metrics.

For the extraction task, we observed that the joint-based extraction methods (JERS and ERNet) outperform other single-stage extraction methods, especially on the
CC359 dataset. Specifically, we observed a gain in extraction dice score up to $10.4\%$ compared to the best single-stage extraction method 3dSkullStrip. 
Furthermore, joint-based extraction methods prove to be more robust compared to other alternatives, given their steady performance and achievement of the lowest standard deviation across all datasets.
%Besides, joint-based extraction methods are more robust in the extraction task than other alternatives, as it performs consistently well and obtains the smallest standard deviation across all datasets.

When observing registration performance, joint-based registration methods (JERS and ERNet) also outperform all other methods across all datasets.
Significantly, our findings suggest that the registration performance of most methods is constrained by the result of its corresponding extraction method. This underscores the fact that the precision of extraction considerably influences the subsequent registration task's quality. Joint-based registration methods leverage this characteristic to yield enhanced results through collaborative learning.
%Notably, we find that the registration result of almost every method is bounded by the result of its corresponding extraction method.
%This proves that the accuracy of extraction significantly impacts the quality of the subsequent registration task.
%Joint-based registration methods capture this property to deliver an improved result via collective learning.

For the segmentation task, once again, we find that joint-based segmentation methods (JERS and DeepAtlas) are superior to DW. This demonstrates that joint learning with the registration task can help the segmentation task to boost its performance.

Overall, joint-based methods (JERS, ERNet and DeepAtlas) outperform other pipeline-based methods in their respective joint tasks. However, the partially joint methods perform poorly on their stand-alone task. ERNet performs well on extraction and registration but inferior on the segmentation task. Similarly, although DeepAtlas achieves good results on the segmentation task, the stand-alone extraction method limits its extraction and registration performance. Benefiting from fully end-to-end joint learning, only JERS can perform well in all tasks.

\subsubsection{Qualitative Analysis.}
In Figure~\ref{fig:main result}, 
we visually compare the performance of our JERS and other approaches on the LPBA test set.
Upon observation, it is evident that JERS achieves more accurate brain extraction compared to BET, 3dSkullStrip, and BSE. The brain extraction mask predicted by JERS overlaps closely with the ground truth extraction mask, while the masks predicted by other extraction methods include noticeable non-brain tissues.  Regarding registration results, JERS also outperforms the other methods. The final registered image of JERS exhibits a higher resemblance to the target image compared to the alternatives. Importantly, the inaccurate extraction results with non-brain tissue further impact the subsequent registration results and ultimately affect the overall performance. This supports our assertion that failed extraction can propagate errors to the subsequent registration task, resulting in irreparable consequences. In terms of the segmentation task, JERS produces results that closely overlap to the ground truth segmentation mask. Overall, we can see that better extraction leads to better registration, and better registration yields better segmentation.


\subsubsection{Ablation Study.} To demonstrate the effectiveness of our JERS, we compared five variants of JERS in Table~\ref{tab: ablation}. We first freeze the extraction module, the registration module, and the segmentation module of JERS respectively. JERS w/o Ext consistently produces the extraction mask with all values of 1 (\ie no pattern be removed from the source image). JERS w/o Reg only outputs the identity affine matrix representing no displacement applied to the image. JERS w/o Seg directly warps the segmentation mask to the source image space (\ie no segmentation network exists). The results show that the extraction module, the registration module, and the segmentation module are essential to JERS, and removing any of them degrades the performance of all tasks. We then evaluate the effects of multi-stage and extraction mask smoothing design. The results show that they significantly boost the performance of all tasks. Since all tasks are learned in a collective manner, the performance boost of one module is shared by all other modules.

\subsubsection{Running Efficiency.}
We measure the efficiency of JERS by comparing its running time with other baselines. The measurement is made on the same device with an Intel$^{\circledR}$ Xeon$^{\circledR}$ E5-2667 v4 CPU and an NVIDIA Tesla A100 GPU.
As indicated in Table~\ref{tab: time_res}, all joint-based methods are faster than existing three-stage pipeline-based methods. This is because they can efficiently perform their corresponding extraction, registration and segmentation tasks end-to-end on the same device. 
GPU implementations for BET, 3dSkullStrip, BSE, FLIRT, and ANTs are not available~\cite{smith2002fast,cox1996afni,shattuck2002brainsuite,jenkinson2001global,avants2009advanced}.


\subsubsection{Influence of Parameters.}
We study two crucial hyperparameters of our JERS: the number of stages for extraction and registration modules and the value of segmentation loss weight~$\lambda$. 
In our multi-stage design, the number of stages in the network represents the depth of the model and the number of iterations for the extraction and registration tasks. Essentially, increasing the number of stages allows for more refinements in the extraction and registration processes.
As illustrated in Figure~\ref{fig:stage dice}(a, b, c), we adjust the number of stages of extraction and registration to study their influence.
The outcomes show that increasing the stages boosts the performance of all tasks, validating the notion that a multi-stage framework results in superior overall performance in a joint learning system.
%The results indicate that the performance of all tasks improves with additional stages.
%This supports the idea that a multi-stage design yields improved overall performance in a joint learning system.

As mentioned in Section~\ref{section:end-to-end training}, we introduce a segmentation loss term to learn a better segmentation network.
To show the effectiveness of the loss term, we vary different values of the loss weight $\lambda$ as shown in Figure~\ref{fig:stage dice}(d).
As the weight of the loss term gradually increases, the segmentation dice score grows as well.
This indicates that our JERS benefits from the segmentation loss term. 



