\appendix

\section{Appendix}
\label{sec:appendix}

This section provides more details of evaluation metrics and experiment settings to support the reproducibility of the results in this paper. Our code and data have been made publicly available at~\url{https://github.com/Anonymous4545/JERS}.


\subsection{Evaluation Metrics}
\label{section: appendix evaluation Metrics}
Our defined problem aims to solve the brain extraction, registration and segmentation tasks simultaneously: 1) identify the brain region (\ie whole cerebral tissue) within the source image; 2) align the extracted cerebral tissues to the target image; 3) identify the anatomical segmentation within the source image. Thus, we evaluate the accuracy of extraction, registration and segmentation to show the performance of our proposed method and compared methods.

\subsubsection{Extraction Performance.}
The brain MRI datasets incorporate the ground truth of the brain mask, representing the labeling of brain tissue in the source image. To assess the accuracy of extraction, we evaluate the volume overlap of brain masks using the Dice score, which can be expressed as:
%The brain MRI datasets contain the brain mask ground truth, which is the label of brain tissue in the source image. To evaluate the extraction accuracy, we measure the volume overlap of brain masks by Dice score, which can be formulated as:
\begin{equation}
\text{Dice}_{\mathrm{ext}}=2 \cdot \frac{|\mathbf{\hat{M}} \cap \mathbf{M}|}{|\mathbf{\hat{M}}|+|\mathbf{M}|},
\end{equation}
where $\mathbf{\hat{M}}$ denotes the predicted brain mask and $\mathbf{M}$ is the corresponding ground truth. If $\mathbf{\hat{M}}$ signifies a precise extraction, we anticipate a high degree of overlap between the non-zero areas in $\mathbf{\hat{M}}$ and $\mathbf{M}$.


\subsubsection{Registration Performance.} 
To evaluate the registration performance, we calculate the mutual information \cite{maes2003medical, thevenaz2000optimization,pluim2000image,563664} between the warped image (\ie registered) $\mathbf{W}$ and the target image~$\mathbf{T}$:
\begin{equation}
    MI(\mathbf{W}, \mathbf{T}) = \sum_{w, t} p_{\mathbf{W} \mathbf{T}}(w, t)\log \frac{p_{\mathbf{W} \mathbf{T}}(w, t)}{p_{\mathbf{W}}(w) \cdot p_{\mathbf{T}}(t)}
\end{equation}
where $p_{\mathbf{W}}(w)$ and $p_{\mathbf{T}}(t)$ are the marginal probability distributions of image $\mathbf{W}$ and $\mathbf{T}$, respectively. $p_{\mathbf{W} \mathbf{T}}(w, t)$ is the joint probability distribution. The mutual information measures the mutual dependence between $\mathbf{W}$ and $\mathbf{T}$. If the warped image $\mathbf{W}$ and the target image $\mathbf{T}$ are geometrically aligned, we expect the mutual information to be maximal.

\subsubsection{Segmentation Performance.}
We evaluate the segmentation accuracy by measuring the volume overlap of anatomical segmentation, which are the location labels of different tissues in the brain MRI image. If the segmentation task performs well,  the predicted segmentation mask should overlap with the ground truth. Similar to the extraction evaluation, we use Dice score to evaluate the overlap of the segmentation masks. A Dice score of 1 signifies that the corresponding structures overlap with the ground truth, whereas a score of 0 denotes the complete absence of overlap. If the image includes multiple labeled anatomical structures, the final score is calculated as the average of the Dice scores for each structure.


\subsection{Details of Data Preprocessing}
\label{section: appendix Data Preprocessing}
The proposed method and baselines are evaluated on three different public brain MRI datasets, LPBA40, CC-359 and IBSR. 

\noindent\textbullet\ \textit{LONI Probabilistic Brain Atlas (LPBA40)}~\cite{shattuck2008construction}: 
The dataset comprises 40 raw T1-weighted 3D brain MRI scans, each from a different patient. It includes corresponding brain masks and segmentation ground truth for 56 anatomical structures. The brain mask is used for evaluating the extraction accuracy, while the anatomical segmentations are used for evaluating the segmentation accuracy. Similar to~\cite{balakrishnan2018unsupervised, zhao2019recursive}, our focus is on atlas-based registration, where the first scan serves as the target image and the remaining scans are aligned to it. Out of the 39 remaining scans, 30 are used for training, 5 for validation, and 4 for testing. All scans are cropped and resized to $96\times96\times96$ dimensions.


\noindent\textbullet\ \textit{Calgary-Campinas-359 (CC-359)}~\cite{souza2018open}: 
The dataset consists of 359 raw T1-weighted 3D brain MRI scans from 359 different patients. Additionally, it includes corresponding brain masks and labeled white matter as ground truth. The brain masks are used to evaluate the accuracy of extraction, while the white matter masks are used for segmentation evaluation. Similar to the LPBA40 dataset, our focus is on atlas-based registration. For CC359, we divide the dataset into training, validation, and test sets, consisting of 298, 30, and 30 scans, respectively. All scans are cropped and resized to $96\times96\times96$.


\noindent\textbullet\ \textit{Internet Brain Segmentation Repository (IBSR)} \cite{rohlfing2011image}: 
The dataset consists of 18 raw T1-weighted 3D brain MRI scans from 18 different patients, accompanied by corresponding segmentation results. The segmentation results are merged to create the brain mask. Given the limited sample size, this dataset is exclusively used for testing the model trained on CC359. Consequently, all 18 scans are aligned with the first scan of CC359. After cropping, all scans are resized to $96\times96\times96$ dimensions.


\subsection{Details Settings of JERS}
\label{section: appendix jers settings}
\textbf{Training settings of JERS.} 
Our experiments are conducted on Red Hat Enterprise Linux 7.3, utilizing an Intel$^{\circledR}$ Xeon$^{\circledR}$ E5-2667 v4 CPU and an NVIDIA Tesla A100 GPU.
The code is implemented in Python 3.7.6, and the neural networks are built using PyTorch 1.7.1.
The implementation also makes use of Numpy 1.21.6, SimpleITK 2.0.2, and Nibabel 3.1.1. 
To overcome GPU memory limitations, we employ batch gradient descent, with each training batch consisting of one image pair. The models are optimized using the Adam optimizer, with a learning rate of $1 \times 10^{-6}$.
We also apply image augmentation techniques, including random translation, rotation, and scaling, to the source images during training. For more details, please refer to Table~\ref{tab:transformation}.


\input{tab_transformation}

\noindent\textbf{Parameters settings of JERS.} The extraction and registration stages are set to 5 in this work. The segmentation loss parameter $\lambda$ and extraction mask smooth parameter $\eta$ in Eq.~(\ref{eq:loss}) are 0.1 and 1, respectively.
The extraction network contains 10 convolutional layers with 16, 32, 32, 64, 64, 64, 32, 32, 32 and 16 filters. 
The registration network adopt 3D CNNs and fully-connected layers to map the input to the dimension of $1\times12$. It contains 6 convolutional layers with 16, 32, 64, 128, 256 and 512 filters. 
The segmentation network contains 10 convolutional layers with 128, 256, 256, 512, 512, 512, 256, 256, 256 and 128 filters.

\newcommand{\code}[1]{\texttt{#1}}

\subsection{Settings of Baselines}
\label{section: appendix baselines settings}
The settings of baselines are followed by~\cite{su2022ernet} for a fair comparison.
\noindent\textbf{Brain Extraction Tool (BET)} \cite{smith2002fast}: 
This skull stripping method is a component of the FSL (FMRIB Software Library) package. It employs a deformable approach to accurately fit the brain surface by utilizing locally adaptive set models.
The command we use for BET is \code{bet <input> <output> -f 0.5 -g 0 -m}, where \texttt{f} and \texttt{g} are fractional intensity threshold and gradient in fractional intensity threshold, respectively. We set them to default values.

\noindent\textbf{3dSkullStrip}~\cite{cox1996afni}: This modified version of BET (Brain Extraction Tool) is integrated into the AFNI (Analysis of Functional NeuroImages) package. It performs skull stripping by employing the expansion paradigm of the spherical surface. The command we use for 3dSkullStrip is \code{3dSkullStrip -input <input>  -prefix <output> -mask\_vol -fac 1000}. \code{fac} is set to the default value.

\noindent\textbf{Brain Surface Extractor (BSE)}~\cite{shattuck2002brainsuite}: This method extracts the brain region by utilizing morphological operations and edge detection techniques. It incorporates anisotropic diffusion filtering to enhance image quality and a Marr Hildreth edge detector to accurately identify the boundaries of the brain. The command we use for BSE is \code{bse -i <input> -o <output> --mask <mask> -p --trim --auto --timer }. Hyperparameters are set to default values.

\noindent\textbf{FMRIB's Linear Image Registration Tool (FLIRT)}~\cite{jenkinson2001global}: This is a fully automated affine brain image registration tool included in the FSL (FMRIB Software Library) package. It performs the registration process without requiring manual intervention, allowing for the alignment of brain images based on affine transformations. The command we use for FLIRT is \code{flirt -in <source> -ref <target> -out <output> -omat <output parameter> -bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 -searchrz -90 90 -dof 12  -interp trilinear}.

\noindent\textbf{Advanced Normalization Tools (ANTs)} \cite{avants2009advanced}: It is a cutting-edge medical image registration toolkit widely used in the field. In our approach, we employ the affine transformation model and cross-correlation metric provided by ANTs for the registration process. 

\noindent\textbf{VoxelMorph (VM)} \cite{balakrishnan2018unsupervised}: This unsupervised image registration method utilizes a neural network to predict the transformation between images. In order to ensure a fair comparison, we re-implemented the method using an affine transformation. The network architecture consists of 6 convolutional layers with filter sizes of 16, 32, 64, 128, 256, and 512. The deformation regularization ratio is set to 10, ensuring smooth and controlled transformations during the registration process.

\noindent\textbf{Directly Warping (DW)}~\cite{jaderberg2015spatial}:
This operation refers to generating a segmentation mask through the process of registration. Once the registration is completed, the segmentation mask of the target image can be directly warped and transformed into the source image space.

\noindent\textbf{Cascaded Registration Networks (CRN)} \cite{zhao2019recursive}:
This is an unsupervised multi-stage registration method that involves iteratively transforming the source image to align with a target image. 
Same to JERS, the number of stages is set to 5.
Within each stage, we configure the network architecture with 6 convolutional layers using filter sizes of 16, 32, 64, 128, 256, and 512. 


\noindent\textbf{DeepAtlas} \cite{xu2019deepatlas}: This is an unsupervised learning method for joint registration and segmentation. For a fair comparison, we configure 6 convolutional layers with 16, 32, 64, 128, 256 and 512 filters for the registration module, and the segmentation network contains 10 convolutional layers with 128, 256, 256, 512, 512, 512, 256, 256, 256 and 128 filters.


\noindent\textbf{ERNet} \cite{su2022ernet}: This is an unsupervised learning method for joint extraction and registration. For a fair comparison, the number of stages is set to 5, and we configure 6 convolutional layers with 16, 32, 64, 128, 256 and 512 filters, and the registration network contains 6 convolutional layers with 16, 32, 64, 128, 256 and 512 filters. 