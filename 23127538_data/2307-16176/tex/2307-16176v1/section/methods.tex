\section{Methods}

\subsection{Dataset}
\label{sec:dataset}
Our dataset can be divided into three parts, which are visualization images,
data images and QR Code images.

\noindent\textbf{Visualization Images} Visualization images are commonly composed of 
homogeneous regions, which results in a significant difference from natural 
images and therefore leads to different image features. Hence, it is not suitable to 
use a natural image dataset to train our model. Instead, we used a subset of images
from VIS30K \cite{Chen:2021:VCF}, which is a dataset containing visualizations 
from various sources, as our visualization image dataset. Specifically, we selected 
a total of 1500 images and split them into 1200 images as training set and the rest 
300 images as testing set.

\noindent\textbf{Data Images} We trained our model using data images as input instead 
of raw chart data. We built our data image dataset with two kinds of data, discrete data 
and continuous data. For discrete data, we randomly constructed 300 sets of 2-dimensional 
scatter data. And for continuous data, we first constructed 300 Perlin noise 
images \cite{perlin1985image}, which are spatial-continuous. To make our dataset more 
comprehensive, we also selected 380 sets of data from the Earthdata website 
\cite{EarthScienceDataSystems}, which include wind field data, ocean current data and 
other kinds of data with continuity. All types of data mentioned above were transferred 
into data images using the DTOI module to be discussed in \ref{sec: dtoi}. We
divided these data images into training and testing sets in a 4:1 ratio.

\noindent\textbf{QR Code Images} For the purpose of invertible visualization,
the chart information is required to be restored from the encoded image with no error.
Thus, it is necessary to use a reliable coding scheme to encode this kind of information.
There are many coding schemes with error correction (ECC) ability, like Polynomial Code
\cite{Moore2017PolynomialCO}, BCH Code \cite{Bose1960OnAC}, etc. However, embedding such kind 
of binary data can cause perceptible artifacts in the encoded image and also lead to insufficient 
data recovery accuracy, which is not suitable for the invertible visualization problem.
As a result, we leverage QR Code \cite{qrcodeweb}, which is a coding scheme with 
error correction. Although the encoding capacity of QR Code is 
less than the abovementioned coding schemes,
it is easier to embed and can achieve higer data recovery accuracy.

QR Code has different symbol versions (Version 1 to Version 40), and 
it provides four ECC levels, higher ECC level means higher error correction rate.
For more details, we suggest that the reader refer to the specification of 
ISO/IEC 18004 \cite{qrspec}.
In our implementation, we utilize QR Code Version 40 to encode chart information, 
whose information capacity and error correction capability for different ECC levels is shown in 
\autoref{tab:qrecc}. In addition, we choose the `H' ECC level, which means about 
30\% of the error can be fixed. 
For the training data, we constructed 500 random strings whose length 
range from 1 to 1273 and converted them into QR Code images.\vspace{-5pt}

% \begin{table}[bp]
%     \newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
%     % \renewcommand\arraystretch{1.2}
%     \caption{
%         Properties of QR Code Version 40
%     }\vspace{-7pt}
%     \centering
%     \small
    
%     \begin{tabular}{@{}M{2.6cm}M{2.6cm}M{2.6cm}@{}}
%     % \begin{tabular}{|c|c|ccc|}
%     % \begin{tabular}{@{}*{5}c@{}}
%     % \begin{tabular}{@{}SSSSS@{}}
    
%         \toprule
%         ECC Level & Information Capacity & ECC Capability \\
%         \midrule
%         `L' (`Low') & 2953 characters & About 7\%    \\
%         \midrule
%         `M' (`Medium') & 2331 characters & About 15\%    \\
%         \midrule
%         `Q' (`Quality') & 1663 characters & About 25\%    \\
%         \midrule
%         `H' (`High') & 1273 characters & About 30\%    \\
%         \bottomrule
%     \end{tabular}
% \label{tab:qrecc}
% \end{table}

\begin{table}[htb]
    \caption{Properties of QR Code Version 40}
    \vspace{-7pt}
    \newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
    \renewcommand\arraystretch{1.2}
    \centering
    \small
    \begin{tabular}{@{}M{2.6cm}M|M{2.6cm}M{2.6cm}|@{}}
    \bottomrule
    \multicolumn{2}{|c|}{ECC Level} & {Information Capacity} & {ECC Capability} \\ [0.5pt]
    \hline
    \multicolumn{2}{|c|}{`L' (`Low')} & {2953 characters} & {About 7\%}  \\
    \multicolumn{2}{|c|}{`M' (`Medium')} & {2331 characters} & {About 15\%}  \\
    \multicolumn{2}{|c|}{`Q' (`Quality')} & {1663 characters} & {About 25\%}\\
    \multicolumn{2}{|c|}{`H' (`High')} & {1273 characters} & {About 30\%}\\ [0.5pt]
    \toprule
    \end{tabular}\vspace{-10pt}
    \label{tab:qrecc}
\end{table}




\subsection{Data-to-Image Module}
\label{sec: dtoi}
Previous research has proposed various methods to hide data in images. 
\cite{zhang2020viscode, wengrowski2019light} hid QR Codes into images, while 
\cite{tancik2020stegastamp, fu2020chartem, fu2022chartstamp,zhu2018hidden}
directly embedded binary data into images.
However, these methods can only achieve limited data embedding capacity. 
To solve this problem, we propose a data-to-image (DTOI) module, which is a new data coding 
scheme. The data images output by the DTOI module can be embedded into the carrier 
image with a larger capacity while bringing less perceptual distortion than 
previous methods. 

In terms of data visualization, most data can be classified into two types: 
continuous data and discrete data. Our DTOI module performs different operations on
these two kinds of data, this process will be described in detail in the following subsections.

% Figure environment removed


\subsubsection{DTOI for Continuous Data}
Within the realm of data visualization, there are many kinds of continuous data that 
exhibit spatial continuity in some dimensions, for example, density maps. Although some 
methods obtain density maps by performing kernel density estimation (KDE) on scatter 
data, for some data that contains large amounts of scatter points, this is quite 
time-consuming. In such cases, directly storing the entire density map as a grayscale
image can also be a choice.
There are many other types of continuous data, vector field data such as wind field and 
flow field data have spatial continuity in the components of the vectors in each dimension.
Volumetric data is another example, it has spatial continuity on every cross-section.

Formally, a sequence of continuous data can be represented as $P=\{ p_i \}^{N}_{i=1}$,
where $p_i$ is a 2-dimensional plane of spatial-continuous data and $N$ is the number 
of such planes. For instance, $N$ is 1 for a density map and 2 for 2-dimensional wind 
field data. Due to the varying distribution and range of 
different chart data, we normalize $P$ as follows:

\vspace{-5pt}
\begin{equation}\label{eq: norm}
    \hat{P} = \{ \frac{p_i - m_i}{M_i - m_i} \}^{N}_{i=1}
    ,\vspace{-5pt}
\end{equation}
where $\hat{P}$ is the normalized $P$ while $m_i$ and $M_i$ are the minimum and maximum 
values in $p_i$, respectively. After that, we convert each element in $\hat{P}$ to a 
single-channel grayscale image and obtain $N$ data images. In the case when $N$ is large 
(e.g., volumetric data), these data images will be stitched horizontally 
and vertically to minimize the number of data images, while ensuring that the size of 
the stitched image does not exceed that of the carrier image.
Note that all $m_i$ and $M_i$, together with the data image stitching information are encoded 
into QR codes, just like chart information, since the normalized data needs to be 
de-normalized during the data restoration process.

\autoref{fig:di_c} shows some continuous data images output by the DTOI module.
\autoref{fig:di_c}(a) is a heatmap visualization and its corresponding data image, 
which is a density map. \autoref{fig:di_c}(b) shows a volumetric data visualization,
whose data images are stitched together to reduce their number.


\subsubsection{DTOI for Discrete Data}\label{sec:dtoi_discrete}
Discrete data is also very common in the field of information visualization, e.g., 2-dimensional scatter data. 
For some other kinds of data with higher dimensions, methods such as t-SNE \cite{van2008visualizing} 
or UMAP \cite{2018arXivUMAP} are often used to reduce their dimensionality for easier visualization. 
Most of this kind of data becomes 2-dimensional after dimensionality reduction, thus, 
the visualization of 2-dimensional discrete data is very common and important. In this 
paper, we focus on 2-dimensional discrete data.
This kind of data generally lacks continuity, either 
within or between dimensions. If we treat it the same way as we treat continuous data, 
the result data images will also lack continuity and features, making it difficult for the network to learn 
feature representations, which in turn leads to unsatisfactory results in data concealing 
and revealing. As a result, we utilize a novel method to transform discrete data into relatively 
continuous and smooth data images to solve this problem.

Given a set of 2-dimensional discrete data $S = \{(x_i, y_i)\}^N_{i = 1}$ where 
$N$ is the number of tuples in $S$, we first sort it in ascending order with the first 
dimension as key, which is, for $(x_i, y_i)$ and $(x_j, y_j)$ in $S$, the former is considered 
to be larger then the latter if $x_i > x_j$. Then, we divide $S$ into groups and sort each group 
locally with the second dimension as key. This promises that the first dimension data of $S$ are 
close to each other and the second dimension data are in ascending order in each group. This endows 
the originally discrete and irregular data with a certain continuity.
$S$ is then normalized and reshaped to obtain 2 grayscale images.
To further enhance the continuity of the data images, we insert $K$ additional pixels 
between every two adjacent pixels and use linear interpolation to assign values to them.
Specifically, for every pair of adjacent pixels $p_a$ and $p_b$, the inserted pixel values are:

\vspace{-5pt}
\begin{equation}\label{eq:interp}
    p_i = \frac{K - i + 1}{K + 1}p_a + \frac{i}{K + 1}p_b, \ i = 1, 2 ... K
    ,\vspace{-5pt}
\end{equation}
where $p_i$ is the value of the $i$\textsuperscript{th} inserted pixel and $K$ 
is set to 3 in this paper.
Larger $K$ can enhance the continuity of data images but decrease the steganography capacity.
We also conduct an experiment on how the value of $K$ affects the 
data embedding quality, this will be discussed in \ref{sec:steg_q}.
This algorithm is explained in detail in Algorithm \ref{alg:dtoi_d}. In addition, if the
data volume is quite large, the data can be divided into parts and apply this algorithm 
respectively to generate several data images. These images can then be stitched
together to reduce the total channel number of the final data images. \autoref{fig:dtoi_d_scatter} shows
the 2 data images generated with our algorithm, as we can see, the data images are smooth and continuous.
In the data restoring procedure, with the revealed data images output by the revealing 
network, the restored data can be obtained by selecting and denormalizing those non-inserted pixel values. 

% Figure environment removed

As shown in \autoref{fig:dtoi_d_compare}, we first embed data images obtained using the above algorithm 
into a chart image and result in \autoref{fig:dtoi_d_compare}(a). For comparison, we also embed data 
images obtained without sorting and pixel insertion operation into the same chart image and result in 
\autoref{fig:dtoi_d_compare}(b). As we can see, the data images generated with our algorithm can be embedded
into images with less distortion.

\begin{algorithm}[htb]
    \caption{DTOI Algorithm for Discrete Data}
    \label{alg:dtoi_d}
    \begin{algorithmic}[1]
        \Require
            $S = \{(x_i, y_i)\}^N_{i = 1}$: a set of 2-dimensional discrete data containing $N$ elements; 
            $(H_s, W_s)$: two integers close to each other and $H_s \times W_s \ge N$;
            $sort(s, i)$: a function that sort the array $s$ in ascending order with its 
            $i$\textsuperscript{th} dimension as key; $K$: the number of additional pixels inserted between two adjacent pixels;
        \Ensure
            $I_{data}$: a set of result data images, where
            the size of each image is $((K + 1) \times H_s, (K + 1) \times W_s)$;

        \State sort($S$, 1)
        \State $s_p \leftarrow the \ last \ element \ of \ S$
        \While {$S$.size < $H_s \times W_s$}
        \State $S$.append($s_p$)
        \EndWhile
        \State $S \leftarrow divide \ into \ H_s \ groups \ equally$
        \For {$s_{group}$ \textbf{in} $S$}
        \State sort($s_{gruop}$, 2)
        \EndFor
        \State $\hat{S} \leftarrow normalize \ S \ with \ \autoref{eq: norm}$
        \State $I_{data} \leftarrow reshape \ \hat{S} \ to \ (2, H_s, W_s) \ and \ convert \ 
        to \ 2 \ grayscale \ images$
        \For {$I_d$ \textbf{in} $I_{data}$}
        \For {\textbf{each} $row$ \textbf{in} $I_d$}
        \State $row \leftarrow insert \ K \ pixels \ with \ \autoref{eq:interp}$
        \EndFor
        \For {\textbf{each} $column$ \textbf{in} $I_d$}
        \State $column \leftarrow insert \ K \ pixels \ with \ \autoref{eq:interp}$
        \EndFor
        \EndFor
        \State\Return $I_{data}$
    \end{algorithmic}
\end{algorithm}


% Figure environment removed\vspace{-2pt}

\subsection{Concealing and Revealing Network}
\autoref{fig:flowmodel} shows our concealing and revealing network, which is a 
symmetric architecture. Both of these two networks are composed of a feature 
fusion block (FFB) and an invertible steganography network (ISN). In the concealing 
process, the input carrier image $I_{carrier}$ and secret images $I_{secret}$ which 
consist of QR Code image $I_{qr}$ and data images $I_{data}$, are fed into the FFB.
To minimize the impact of QR Code image on the distortion of encoded images, we multiply the QR
code image by a coefficient $m_{qr}=0.15$ before feeding it into the network, and the 
recovered QR code image is divided by $m_{qr}$ during the decoding procedure.
The output of FFB, i.e. $I^{*}_{carrier}$ and $I^{*}_{secret}$, undergo discrete wavelet
transform (DWT) and reshaping separately, they are then concatenated as the input of ISN.
The ISN processes its input and results in an encoded image $I_{enc}$ after performing
the inverse wavelet transform (IWT) and quantization operation. Since the input and output 
sizes of ISN are the same, it also outputs a matrix $I_l$. 
In the decoding process, an encoded image $I^{'}_{enc}$, together 
with a constant matrix $I_z$ whose size is the same as that of $I_l$, goes through the ISN
and FFB in sequence to produce the restored data images $I^{'}_{data}$ and QR Code image 
$I^{'}_{qr}$.  Note that in our implementation, the maximum number of the channel that hides
information is 4 (3 channels for data images and 1 channel for QR Code image), but it is 
quite easy to change by slightly modifying the network.

\subsubsection{Feature Fusion Block}
Previous studies \cite{guan2022deepmih,jing2021hinet,cheng2021iicnet,lu2021large} have 
proposed some methods to hide natural images in each other. The secret images and
carrier images in these methods generally have similar natural image features. However, 
in our InvVis, the secret images to be embedded are data images and QR Code image, they 
have different data distributions and features from the carrier image. Hence, a different
method is needed to reduce the perceptual deviation of the encoded image. To address 
this issue, we propose the feature fusion block (FFB), attempting to optimize the 
perceptual quality by blending the features of the carrier image and secret images before
the steganography process.

The architecture of our FFB is shown in \autoref{fig:ffb}, which is composed of several
dense blocks \cite{wang2018esrgan} and common convolutional layers. The input of FFB
comprises a total of 7 channels, and its output has the same size as the input. In the
concealing process, the FFB projects the input into a high-dimensional space and 
then maps it back to a low-dimensional space, achieving the effect of feature blending.
The output of FFB is easier to result in an encoded image with less visual distortion in 
the following steganography network. The ablation experiment on FFB will be discussed 
in \ref{sec:steg_q}. In the decoding process, we also use an FFB to process the output of 
ISN and obtain the final restored images.


\subsubsection{Wavelet Domain Steganography}
Spacial-domain steganography can lead to texture-copying artifacts 
\cite{weng2019high}, which affects the perceptual quality and is easy to be detected.
To improve the concealing performance, we adopt an approach similar to HiNet
\cite{jing2021hinet} and DeepMIH \cite{guan2022deepmih}, which is introducing discrete 
wavelet transform (DWT). Transferring the image to the wavelet domain before the ISN can 
guide the network to learn a better embedding strategy that hides more secret information 
in the high-frequency part of the image rather than the low-frequency part.

Specifically, we perform DWT on $I^*_{carrier}$, which is part of the output of FFB 
corresponding to the initial carrier image $I_{carrier}$. Assume that the size of 
$I^*_{carrier}$ is $(C_c, H, W)$ where $C_c$, $H$, $W$ indicate the channel number, 
height and width of the image, DWT transfers its size to $(4C_c, H / 2, W / 2)$. 
In contrast, we do not perform this operation on $I^*_{secret}$ whose size is 
$(C_s, H, W)$, instead, we simply reshape it to $(4C_s, H / 2, W / 2)$ for the purpose 
of size matching. After the hiding process performed by ISN, we use inverse wavelet
transform (IWT) to transform the image back to spatial domain and obtain the encoded image.
Similarly, In the revealing procedure, a series of symmetric operations are performed 
to restore the secret images. We conduct the ablation experiment and the result shows that this
method can lead to better steganography quality, this will be discussed in \ref{sec:steg_q}.

% Figure environment removed


\subsubsection{Invertible Steganography Network}
Our invertible steganography network (ISN) is based on the invertible neural network
(INN), which was first proposed by Dinh et al. \cite{Dinh2014NICENI}, it allows the
secret information to be hidden and revealed in the same network. To enhance the 
representation ability of the network, we also leverage the invertible 1 $\times$ 1 
convolution block proposed by Kingma et al. \cite{kingma2018glow}. Formally, given $c$ 
and $s$, which are the input of ISN corresponding to the carrier image and secret images, 
the encoded image $e$ can be obtained with $e = f_{\theta}(c, s)$ where $f(\cdot)$ is the 
hiding process and $\theta$ indicates the network parameters. For the revealing 
procedure, an inverse process $f^{-1}_{\theta}(\cdot)$ which shares the same parameters 
$\theta$ as $f_{\theta}(\cdot)$ can output the restored images $c'$ and $s'$ with 
$(c', s') = f^{-1}_{\theta}(e)$.

As shown in \autoref{fig:flowmodel}, our ISN is composed of several affine coupling blocks
(ACBs). Each ACB shares the same parameters in the hiding and revealing process but has
an opposite data flow direction. 
Assume that the input of the $i$\textsuperscript{th} ACB is $a^i_{carrier}$ and
$a^i_{secret}$ indicating the carrier image and secret image channels, the invertible 
convolution block outputs $x^i_{carrier}$ and $x^i_{secret}$ with their size not changed:
\begin{equation}\label{eq:iconv}
    (x^i_{carrier}, x^i_{secret}) = iConv(a^i_{carrier}, a^i_{secret})
    ,
\end{equation}
where $iConv(\cdot)$ indicates the forward procedure of invertible convolution.
After that, the output of this ACB can be formulated as:
\begin{equation}\label{eq:acb}
    \begin{split}
        a^{i + 1}_{carrier} &= x^i_{carrier} + \phi(x^i_{secret}), \\
        a^{i + 1}_{secret} &= x^i_{secret} \odot exp(\rho(a^{i + 1}_{carrier})) + \
        \eta(a^{i + 1}_{carrier})
        ,
    \end{split}
\end{equation}
in which $exp(\cdot)$ is the exponential function, $\odot$ is the Hadamard product and 
$\phi(\cdot)$, $\rho(\cdot)$, $\eta(\cdot)$ are functions learned by convolution blocks. 
There can be a variety of choices for the type of these convolution blocks, in this paper,
we use dense block \cite{wang2018esrgan} due to its powerful representation ability. 
In the revealing procedure, the output of the 
$i$\textsuperscript{th} ACB can be obtained by reformulating \autoref{eq:iconv} and \autoref{eq:acb} like:
\begin{equation}
    (a^i_{carrier}, a^i_{secret}) = iConv^{-1}(x^i_{carrier}, x^i_{secret})
    ,
\end{equation}\vspace{-5pt}
and
\begin{equation}
    \begin{split}
        x^i_{secret} &= (a^{i + 1}_{secret} - \eta(a^{i + 1}_{carrier})) \odot 
        exp(-\rho(a^{i + 1}_{carrier})), \\
        x^i_{carrier} &= a^{i + 1}_{carrier} - \phi(x^i_{secret})
        ,
    \end{split}
\end{equation}

where $iConv^{-1}(\cdot)$ is the inverse operation of $iConv(\cdot)$.

Since the input and output of ISN are the same, we only extract the first 12 channels 
from the output and perform IWT to obtain the final encoded image. Similarly, in the
revealing process, we perform DWT on the encoded image and pad it with a constant matrix
$I_z$ to match its size to that of the input of ISN.

\autoref{fig:encode_residual} demonstrates a comparison of the image quality before and 
after data embedding. \autoref{fig:encode_residual}(c) shows the difference between
the original chart image and the encoded image. We enhance the residual by 2 times to 
more clearly show the difference. As we can see, our method can embed data 
into images while preserving the perceptual quality of the image.

% Figure environment removed
    


\subsubsection{Loss Function}
The optimization of our network can be divided into two parts, which are the perceptual
quality of the encoded image and the restoration accuracy of secret images. We utilize hybrid
loss functions to train our model in order to achieve better performance.

Given a carrier image $I_{carrier}$ and an encoded image $I_{enc}$, we define our 
encoding loss $\mathcal{L}_{enc}$ as:
\begin{equation}
    \mathcal{L}_{enc} = \mathcal{L}_{mse} + \alpha\mathcal{L}_{freq}
    ,
\end{equation}
where $\alpha$ is a weight coefficient and $\mathcal{L}_{mse}$ measures the mean squared error (MSE) of pixel values between 
$I_{carrier}$ and $I_{enc}$. Inspired by Jing et al. \cite{jing2021hinet}, we introduce 
$\mathcal{L}_{freq}$ to measure and optimize the low-frequency sub-bands difference after wavelet 
decomposition. Assume that $W(\cdot)_l$ output the low-frequency sub-bands of image, 
$\mathcal{L}_{freq}$ can be defined as:
\begin{equation}
    \mathcal{L}_{freq} = \mathcal{L}_{mse}(W(I_{carrier})_l, W(I_{enc})_l)
    .
\end{equation}

For restoration accuracy, we use both L1 and MSE loss function to guide the training 
process, the restoration loss $\mathcal{L}_{res}$ of $N$ secret images can be defined as:
\begin{equation}
    \mathcal{L}_{res} = \sum_{i=1}^{N}(\mathcal{L}_{L1}(I_{secret}^{'i}, I_{secret}^i) + 
    \mathcal{L}_{mse}(I_{secret}^{'i}, I_{secret}^i))
    ,
\end{equation}
where $I_{secret}^i$ and $I_{secret}^{'i}$ denotes the $i$\textsuperscript{th} secret image and 
its corresponding restored image respectively.
In summary, the total loss function of our network can be formulated as:
\begin{equation}\label{eq:totloss}
    \mathcal{L}_{total} = \mathcal{L}_{enc} + \beta\mathcal{L}_{res} = 
    \mathcal{L}_{mse} + \alpha\mathcal{L}_{freq} + \beta\mathcal{L}_{res}
    ,
\end{equation}
where $\beta$ is another weight coefficient just like $\alpha$.
