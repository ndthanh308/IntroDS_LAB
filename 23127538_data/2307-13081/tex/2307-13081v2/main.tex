
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{multirow}
%\usepackage[numbers, sort]{natbib}
\usepackage{graphicx}  
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{bbm}
\usepackage{amssymb}
\makeatletter
\newcommand{\updated}{}
\newcommand{\patrik}{\color{blue}}
%\newcommand{\todo}{\color{red}}
% make numeric styles use name format
\patchcmd{\NAT@test}{\else \NAT@nm}{\else \NAT@nmfmt{\NAT@nm}}{}{}
 
% define \citepos just like \citet
\DeclareRobustCommand\citepos
  {\begingroup
   \let\NAT@nmfmt\NAT@posfmt% ...except with a different name format
   \NAT@swafalse\let\NAT@ctype\z@\NAT@partrue
   \@ifstar{\NAT@fulltrue\NAT@citetp}{\NAT@fullfalse\NAT@citetp}}

\let\NAT@orig@nmfmt\NAT@nmfmt
\def\NAT@posfmt#1{\NAT@orig@nmfmt{#1's}}

\newcommand{\VanilaFairness}{Vanilla (with fairness)}
\newcommand{\VanilaNoFairness}{Vanilla (without fairness)}  
\newcommand{\VanilaProxyFairness}{Proxy-DNN}  
\newcommand{\KSMOTE}{KSMOTE}   
\newcommand{\FairRF}{FairRF} 
\newcommand{\FairDA}{FairDA} 
\newcommand{\CVarDRO}{CVarDRO} 
\newcommand{\DRO}{DRO} 
\newcommand{\ARL}{ARL~} 
\newcommand{\CGL}{CGL} 
\newcommand{\Done}{$\mathcal{D}_1$} 
\newcommand{\Dtwo}{$\mathcal{D}_2$} 
 

\title{Fairness Under Demographic Scarce Regime}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Patrik Joslin Kenfack\email patrik-joslin.kenfack.1@ens.etsmtl.ca \\
      \addr \'ETS Montr\'eal, Mila
      \AND
      \name Samira Ebrahimi Kahou \email Samira.Ebrahimi.Kahou@gmail.com\\
      \addr University of Calgary, Mila\\ Canada CIFAR AI Chair
      \AND
      \name Ulrich Aïvodji \email Ulrich.Aivodji@etsmtl.ca \\
      \addr \'ETS Montr\'eal, Mila
    }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{09}  % Insert correct month for camera-ready version
\def\year{2024} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=TB18G0w6Ld}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or for privacy reasons. This setting is known as \textit{demographic scarce regime}. Prior research has shown that training an attribute classifier to replace the missing sensitive attributes (\textit{proxy}) can still improve fairness. However, using proxy-sensitive attributes worsens fairness-accuracy tradeoffs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy tradeoffs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty.  We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes can negatively impact the fairness-accuracy tradeoff. Our experiments on five datasets showed that the proposed framework yields models with significantly better fairness-accuracy tradeoffs than classic attribute classifiers.  Surprisingly, our framework can outperform models trained with fairness constraints on the true sensitive attributes in most benchmarks. We also show that these findings are consistent with other uncertainty measures such as conformal prediction. The source code is available at \href{https://github.com/patrikken/fair-dsr}{https://github.com/patrikken/fair-dsr}.
\end{abstract}


\section{Introduction}
Mitigating machine learning bias against certain demographic groups becomes challenging when demographic information is wholly or partially missing. Demographic information can be missing for various reasons, e.g., due to legal restrictions, prohibiting the collection of sensitive information of individuals, or voluntary disclosure of such information. As people are more concerned about privacy, reluctant users will not provide sensitive information. As such, demographic information is available only for a few users. A \textit{demographic scarce} regime was the term used by~\citet{awasthi2021evaluating} to describe this particular setting. The data in this setting can be divided into two sets: $\mathcal{D}_1$ and \Dtwo. The dataset $\mathcal{D}_1$ does not contain demographic information, while \Dtwo~contains both sensitive and non-sensitive information.  The goal is to train a fair classifier with respect to different (unobserved) demographic groups in $\mathcal{D}_1$. Without demographic information in $\mathcal{D}_1$, it is more challenging to enforce group fairness notions such as \textit{statistical parity}~\citep{dwork2012fairness} and \textit{equalized odds}~\citep{hardt2016equality}. Algorithms to enforce these notions require access to sensitive attributes to quantify and mitigate the model's disparities across different groups~\citep{hardt2016equality, agarwal2018reductions}. However, having access to another dataset where sensitive attributes are available gives room to train a sensitive attribute classifier that can serve as a \textit{proxy} for the missing ones. We are interested in understanding what level of fairness/accuracy one can achieve if proxy-sensitive attributes are used to replace the true sensitive attributes and properties of the sensitive attribute classifier and the data distribution that influences the fairness-accuracy tradeoff.

In their study, \citet{awasthi2021evaluating} demonstrated a counter-intuitive finding: when using proxy-sensitive attributes, neither the highest accuracy nor an equal error rate of the sensitive attribute classifier has an impact on the accuracy of the bias estimation. Although~\citet{gupta2018proxy} showed that improving fairness for the \textit{proxy} demographic group can improve fairness with respect to the true demographic group; it remains unclear how existing fairness mechanisms would perform in the presence of proxy-sensitive attributes and how the fairness-accuracy tradeoff is impacted. We show that existing fairness-enhancing methods~\citep{hardt2016equality, agarwal2018reductions} can be robust to noise introduced in the sensitive attribute space by the proxy attribute classifier, i.e., unfairness can be mitigated when proxy attributes are used instead of the sensitive attribute. However, the fairness-accuracy tradeoff worsens when fairness constraints are imposed on these proxy-sensitive attributes. %\textit{How does sensitive attribute imputation impact fairness-accuracy tradeoffs when demographic information is missing? What is the optimal way for practitioners to design sensitive attribute classifiers and integrate them with existing fairness-enhancing methods to achieve a better tradeoff between accuracy and fairness? } %Can we design a better strategy to improve the tradeoffs?. 
We aim to provide insights into the distribution of sensitive attributes that can yield better fairness and accuracy performances. We hypothesize that the uncertainty of the sensitive attribute classifier plays a critical role in improving fairness-accuracy tradeoffs on downstream tasks. Specifically, we argue that samples with \textit{reliable} demographic information should be used to fit fairness constraints, backed up by the intuition that these samples are \textit{easier} to discriminate against, while samples with uncertain demographic information are already hard to discriminate against. Validating this hypothesis requires effective uncertainty estimation in the sensitive attribute space. 
%Our results show that samples whose sensitive attribute values are predicted by the attribute classifier with high uncertainty are \textit{detrimental} to the fairness-accuracy tradeoff on downstream tasks. As such, we show empirically that existing fairness-enhancing methods achieve better fairness-accuracy tradeoffs when fairness constraints are enforced only on samples whose sensitive attribute values are predicted with low uncertainty.
%\paragraph{Hypothesis.} We hypothesize that samples with \textit{reliable} demographic information should be used to fit fairness constraints, backed up by the intuition that these samples are \textit{easier} to discriminate against, while samples with uncertain demographic information are already hard to discriminate against.


{\updated In this paper, we propose a framework to handle \textbf{Fair}ness under \textbf{D}emographic \textbf{S}carce \textbf{R}egime (FairDSR). FairDSR consists of two phases. In the first phase, we construct an uncertainty-aware Deep Neural Network  (DNN) model to predict demographic information. The training is semi-supervised using  self-ensembling~\citep{tarvainen2017weight}, and the uncertainty of the predictions is measured and improved during the training using Monte Carlo dropout~\citep{gal2016dropout}. The first phase outputs for each data point, the predicted sensitive attribute, and the uncertainty of the prediction. In the second phase, the classifier for the target task is trained with fairness constraints w.r.t to the predicted sensitive attributes. However, fairness constraints are imposed only on samples whose sensitive attribute values are predicted with low uncertainty. } 
Our results rely heavily on dropout regularization and the effectiveness of the uncertainty measure. Existing uncertainty measures compute prediction uncertainty using the model in a certain epoch, e.g., in the last training epoch. However, the uncertainty measure can vary for different models in different training epochs due to different perturbations, such as network dropout, input noise, and data ordering. Hence, we adopt self-ensembling to compute more consistent uncertainty by considering the training dynamic. %Aggregating the outputs of an ensemble of models to make predictions is likely to provide correct labels compared to evaluating a single model. 
In self-ensembling learning, the model ensemble is obtained using an exponential running average of model snapshots. In the semi-supervised setup, an unsupervised loss term ensures the model's output remains consistent with different perturbations. For example, given a set of labeled or unlabeled samples, the unsupervised loss enforces the model's output at the current epoch to be similar to the output aggregated from previous training epochs under different perturbations. The aggregated outputs can be obtained using the moving average of the model's output~\cite{lainetemporal} or the model's weights~\cite{tarvainen2017weight} throughout the training. %This unsupervised loss term can significantly improve the performance during the training by enforcing the model to be more robust to noise in the label space. 
In the first phase of FairDSR, we use a similar approach to train the attribute classifier with a reliable uncertainty measure in the data without demographic information. In addition to supervised loss, we add an unsupervised loss term that ensures the model's uncertainty remains consistent within epochs.  

On the other hand, considering the other aspect of our hypothesis,  the uncertainty estimated by our method is used to derive a subset of the dataset having higher uncertainty in the demographic information prediction. We demonstrate that using this subset to train a model without fairness constraints can yield fairer outcomes. {\updated Furthermore, we show how other uncertainty measures such as conformal prediction~\citep{angelopoulos2023conformal} can be integrated with our framework while achieving similar performance gains regarding tradeoffs in fairness and accuracy.}  
Our main contributions are summarized as follows:
\begin{itemize}
    \item We show that the fairness-accuracy tradeoff is suboptimal when using proxy-sensitive attributes instead of true-sensitive attributes.
    Specifically, when the sensitive attribute is partially available, replacing missing values using data imputation techniques based on k-nearest neighbor (KNN) or DNN models can yield a reasonably fair model but a worse fairness-accuracy tradeoff.     
    \item We propose a simple yet effective framework to improve the tradeoff by exploiting the uncertainty of the sensitive attribute predictions, which we show can play an important role in achieving better accuracy-fairness tradeoffs. We hypothesize that a better fairness-accuracy tradeoff can be achieved when fairness constraints are imposed on samples whose sensitive attribute values can be predicted with low uncertainty. We also show that a model trained without fairness constraints but using data with higher uncertainty in the predictions of sensitive attributes tends to be fairer across different fairness metrics.  
    \item We perform extensive experiments on a wide range of real-world datasets to demonstrate the effectiveness of the proposed framework compared to existing methods. Our results also show that the FairDSR can significantly outperform a model trained with fairness constraints on observed sensitive attributes. Applying our method in settings where demographic information is fully available can yield better fairness-accuracy tradeoffs.
\end{itemize}
\section{Related Work.}
Various metrics have been proposed in the literature to measure unfairness in classification, as well as numerous methods to enforce fairness as per these metrics. The most popular fairness metrics include demographic parity~\citep{dwork2012fairness}, equalized odds, and equal opportunity~\citep{hardt2016equality}. Demographic parity enforces the models' positive outcome to be independent of the sensitive attributes, while equalized odds aim at equalizing models' true positive and false positive rates across different demographic groups. Fairness-enhancing methods are categorized into three groups: pre-processing~\citep{zemel2013learning, kamiran2012data}, in-processing~\citep{agarwal2018reductions, zhang2018mitigating}, and post-processing~\citep{hardt2016equality}, depending on whether the fairness constraint is enforced before, during, or after model training respectively. However, enforcing these fairness notions often requires access to demographic information. There are fairness notions that do not require demographic information to be achieved, such as the \textit{Rawlsian Max-Min} fairness notion~\citep{rawls2020theory}, which aims at maximizing the utility of the worst-case (unknown) group~\citep{hashimoto2018fairness, lahoti2020fairness,liu2021just, levy2020large}. Specifically, these methods focus on maximizing the accuracy of the unknown worst-case group. However, they often fall short in effectively targeting the specific disadvantaged demographic groups or improving group fairness metrics~\citep{franke2021rawls, lahoti2020fairness}. In contrast, we aim to achieve group fairness through proxy-attributes using limited demographic information.     
Recent efforts have explored bias mitigation when demographic information is noisy~\citep{wang2020robust, chen2022fair}. Noise can be introduced in the sensitive feature space due to human annotation, privacy mechanisms, or inference~\citep{chen2022fairness}. \citet{chen2022fair} aims to correct the noise in the sensitive attribute space before using them in fairness-enhancing algorithms. Another line of work focuses on alleviating privacy issues in collecting and using sensitive attributes. This group of methods aims to train fair models under privacy-preservation of the sensitive attributes. They design fair models using privacy-preserving mechanisms such as trusted third party~\citep{veale2017fairer}, secure multiparty computation~\citep{kilbertus2018blind}, and differential privacy~\citep{jagielski2019differentially}.          

{\updated The most related work includes methods relying on proxy-sensitive attributes to enforce fairness when partial demographic information is available. \citet{gupta2018proxy} used non-protected features to infer proxy demographic information to replace the unobserved real ones. They showed empirically that enforcing fairness with respect to proxy groups generalizes well to the real protected groups and can be effective in practice. While they focus on post-processing techniques, we are interested in in-processing methods.  \citet{coston2019fair, liang2023fair} assumed sensitive attribute is available either in a source domain or the target domain and used domain adaptation-like techniques to enforce fairness in the domain with missing sensitive attributes.  Moreover, while these methods can improve fairness regarding true sensitive attributes, they result in a worse trade-off fairness accuracy than the method using the true sensitive attributes. \citet{diana2022multiaccurate} showed that training proxy classifier under multi-accuracy constraints can be a good substitute for the ground truth sensitive attributes when the latter is missing; however, the resulting proxy sensitive attributes also yield worse fairness-accuracy tradeoff.  \citet{liang2023fair} also considers the uncertainty in sensitive space and proposes to replace uncertain predictions with random sampling from the empirical conditional distribution of the sensitive attributes given the non-sensitive attributes $P(A|X, Y)$. However, the random sampling of sensitive attributes is sensitive to noise in the empirical distribution, which can negatively impact the fairness accuracy tradeoff. Moreover, this work provides a comprehensible analysis of the impact of uncertainty in sensitive attribute space over the fairness-accuracy tradeoff. On the other hand, \citet{awasthi2021evaluating} proposed an active sampling approach to improve bias assessment using predicted sensitive attributes. While their method focuses on bias assessment using proxy attributes, we want to improve the tradeoff between fairness and accuracy, and bias assessment under missing sensitive attributes is out of the scope of the paper.} In sum, related work relying on proxy-sensitive attributes mostly focuses on assessing what level of fairness can be achieved when proxy-sensitive attributes are used~\citep{coston2019fair}, properties of the sensitive attribute classifier~\citep{diana2022multiaccurate, coston2019fair}, and bias assessment via proxy sensitive features~\citep{awasthi2021evaluating}. Our proposed method focuses on reducing the accuracy-fairness tradeoff when proxy attributes are used.
\section{Problem Setting and Preliminaries.}
\label{sec:fair_metric}
\paragraph{Problem formulation.} We consider a dataset $\mathcal{D}_1=\{\mathcal{X}, \mathcal{Y}\}$ where $\mathcal{X} = \{x_i\}_{i=1}^{M}$ represents the non-sensitive input feature space and $\mathcal{Y}=\{0, 1\}$ represents the target variable. The goal is to build a classifier, $f:\mathcal{X} \rightarrow \mathcal{Y}$, that can predict $\mathcal{Y}$ while ensuring fair outcomes for samples from different demographic groups. 
However, demographic information of samples in $\mathcal{D}_1$ is unknown. We assume the existance of another dataset $\mathcal{D}_2=\{\mathcal{X}, \mathcal{A}\}$ sharing the same input feature space as $\mathcal{D}_1$ and for which demographic information is available, i.e., $\mathcal{A}=\{0, 1\}$. We assume binary demographic groups for simplicity. Therefore, the dataset $\mathcal{D}_1$ contains label information and  \Dtwo~contains demographic information. Our goal is to leverage \Dtwo~to train an attribute classifier $h:\mathcal{X} \rightarrow \mathcal{A}$ that can serve as a proxy to the sensitive attributes for samples in  $\mathcal{D}_1$, for which a fairness metric can be enforced in a way to improve fairness with respect to the true sensitive attributes. Attribute classifiers have been used in health~\citep{brown2016using, fremont2005use} and finance~\citep{zhang2018assessing, silva2019developing} to infer missing sensitive attributes, in particular when users or patients self-report their protected information.
To be able to estimate the true disparities in the label classifier $f$, we assume there exists a small set of samples drawn from the joint distribution $\mathcal{X} \times \mathcal{Y} \times \mathcal{A}$, i.e., samples that jointly have label and demographic information. If this subset is not available, one can consider using the active sampling technique proposed by~\citet{awasthi2021evaluating} to approximate bias with respect to the predicted sensitive attributes. This estimation is beyond the scope of this work. We aim to effectively assess fairness without being overly concerned about bias overestimation or underestimation. 

Reducing the tradeoff between fairness and accuracy is a significant challenge within the fair machine-learning community~\citep{dutta2020there, kenfack2021impact}. Our primary goal is to design a method that effectively leverages proxy features to achieve similar or better fairness-accuracy tradeoffs compared to settings where the true sensitive attributes are available. To this end, we considered a different range of fairness metrics and fairness-enhancing techniques.     

\paragraph{Fairness Metrics.} In this work, we consider three popular group fairness metrics: demographic parity~\citep{dwork2012fairness}, equalized odds, and equal opportunity~\citep{hardt2016equality}. These metrics aim to equalize the model's performance across different demographic groups; we provide more details in Appendix~\ref{app:fair_metric}. 
 
\paragraph{Fairness Mechanism.} We focus on in-processing techniques to improve the models' fairness. These methods introduce constraints in the classification problem to satisfy a given fairness metric. Our study focuses on state-of-the-art techniques in this category, i.e., exponentiated gradient~\citep{agarwal2018reductions} and adversarial debiasing~\citep{zhang2018mitigating}. We considered these methods as they allow better control over fairness and accuracy. In general, the optimization problem contains a parameter $\lambda$ that controls the balance between fairness and accuracy, i.e., a higher value of $\lambda$ would force the model to achieve higher fairness (respectively lower accuracy) while a smaller yields higher accuracy (respectively lower fairness). We aim to design a sensitive attribute predictor that achieves a better fairness-accuracy tradeoff, i.e., for the same value of $\lambda$ build a model that provides higher accuracy and lower unfairness.  
 
  

%\subsection{Characteristic of the Attribute Classifier}

\section{Method}
\label{sec:method}
This section presents the methodology and all components involved in the framework FairDSR. Figure~\ref{fig:overall-model} presents an overview of the stages in our framework and the interactions between and within each stage. The first stage consists of \textit{training the attribute classifier} with uncertainty awareness. This step outputs for each sample with missing sensitive attribute, its predicted sensitive attribute (proxy), and the prediction's uncertainty. In the second stage, \textit{the label classifier is trained with fairness constraints enforced using the predicted sensitive attributes}. To validate our hypothesis, fairness is enforced only on samples with the lowest uncertainty in the sensitive attribute prediction, i.e., samples with an uncertainty lower than a predefined uncertainty threshold~$H$.
% Figure environment removed

\subsection{Uncertainty-Aware Attribute Prediction}
We build the sensitive attribute classifier using a semi-supervised learning approach that accounts for the uncertainty of the predictions of samples with missing sensitive attributes, similar to~\cite{yu2019uncertainty,tarvainen2017weight, lainetemporal}. Motivated by the uncertainty estimation in Bayesian networks, we estimate the uncertainty with the Monte Carlo Dropout~\citep{gal2016dropout}. However, measuring the uncertainty of the predictions using the model in a given training step (e.g., the last epoch) might result in an unreliable uncertainty measure due to different perturbations, such as network dropout, input noise, and data ordering~\citep{he2023large}. In addition, we are interested in designing an attribute classifier that minimizes the uncertainty of the predictions in the data with missing sensitive attributes. 

To overcome this challenge, we adopt self-ensemble learning to ensure consistent uncertainty measurement by maintaining the moving average of the attribute classifier's weights during the training~\citep{tarvainen2017weight}. More specifically, the model maintains a moving average of itself during the training to provide a stable learning signal. We refer to the model at the current training epoch as the \textit{student model} and its moving average (self-ensembling) from previous epochs as the \textit{teacher model}.%or Mean Teacher as introduced by ~\cite{tarvainen2017weight}. 
%\footnote{Although the method uses a student-teacher framework, its functioning is different from transfer learning.}.

\paragraph{Student Model.} The student model is implemented as a neural network and is trained on \Dtwo~(samples with sensitive attributes) to predict sensitive attributes. Under a semi-supervised setup, the attribute classifier is optimized to minimize a double loss: the \textit{classification loss} ($\mathcal{L}_s$), i.e., the cross-entropy loss, and the \textit{consistency loss} ($\mathcal{L}_c$)~\citep{yu2019uncertainty}. The consistency loss (or unsupervised loss) forces the student model to focus on samples with low uncertainty in the sensitive attributes guided by the uncertainty estimation from the teacher model.
{\updated This loss is defined as the mean squared difference between the teacher's and student outputs (logits) on samples whose uncertainty does not exceed a threshold $R$. }

Overall, the attribute classifier is trained to minimize the following loss: 

\begin{equation}
    \label{eq:loss}
    \min_{f \in \mathcal{F}} \; \underset{{(x,a) \sim \mathcal{D}_2 \times \mathcal{A}}}{\mathbb{E}} \; \mathcal{L}_s (f(x), a) + \lambda \underset{x\sim \mathcal{D}_1 + \mathcal{D}_2}{\mathbb{E}} \; \mathcal{L}_c(f(x), h(x)) 
\end{equation}

where $f(\cdot)$ is the student model, $h(\cdot)$ the teacher model, and $\lambda$ a parameter controlling the consistency loss. The empirical loss minimized is defined by the following equations for classification ($\mathcal{L}_{s}$) and consistency loss $\mathcal{L}_{c}$:   
\begin{equation} 
\mathcal{L}_{s} = \frac{1}{|\mathcal{D}_2|} \sum_{x, a \in \mathcal{D}_2, A} a \cdot \mathrm{log}(f(x)) + (1-a) \cdot \mathrm{log}(1-f(x)) 
\end{equation}
\begin{equation} 
\mathcal{L}_{c} = \frac{1}{|\mathcal{D}_2|+|\mathcal{D}_1|} \sum_{x | u_{x} \leq R} \parallel f(x) - h(x) \parallel^2
\end{equation}

The consistency loss is applied only on samples, $x$, whose uncertainty, $u_{x}$, is lower than the threshold $R$. Following~\citet{srivastava2014dropout, baldi2013understanding}, $R$ and $\lambda$ are updated using a Gaussian warmup function to prevent the model from diverging at the beginning of the training. The motivation behind the consistency loss is the focus on our primary goal for the attribute classifier, which is to find the missing sensitive attributes in $\mathcal{D}_1$ with low uncertainty. In the experiments (Section~\ref{app:ablation-consistency-loss}), we show that training the model without consistency loss results in worse fairness-accuracy tradeoffs.

\paragraph{Teacher Model.} The teacher model maintains the moving average of the student's weights and is used for uncertainty estimation. Specifically, the teacher weights are updated within each training epoch, $t$, using the exponential moving average (EMA) of student weights as follows: 
\begin{equation}
    \omega_t = \alpha \omega_{t-1} + (1-\alpha)\theta,
\end{equation}
where $\theta$ and $\omega$ denote the respective weights of student and teacher and $\alpha$ controls the moving decay.  
The use of EMA to update the teacher model is motivated by previous studies~\citep{lainetemporal, yu2019uncertainty} that have shown that averaging model parameters at different training epochs can provide more reliable predictions than using the most recent model weights in the last epoch.  
The teacher model gets as input both labeled and unlabeled samples ($\mathcal{D}_1$ and \Dtwo) and computes the uncertainty of their predictions using Monte Carlo (MC) dropout~\citep{gal2016dropout}. %Therefore, the student and teacher networks have dropout layers between hidden network layers.  

\paragraph{Uncertainty Estimation.} MC dropout is an approximation of a Bayesian neural network widely used to interpret the parameters of neural networks~\citep{abdar2021review}. MC dropout can effectively capture various sources of uncertainties (Aleatoric and Epistemic uncertainty). It uses dropout at test time to compute prediction uncertainty from different sub-networks that can be derived from the whole original neural network.  Dropout is generally used to improve the generalization of DNNs. During training, the dropout layer randomly removes a unit with probability $p$. Therefore, each forward and backpropagation pass is done on a different model (sub-network), forming an ensemble of models that are aggregated together to form a final model with lower variance~\citep{srivastava2014dropout, baldi2013understanding}. The uncertainty of each sample is computed using $T$ stochastic forward passes on the teacher model to output $T$ independent and identically distributed predictions, i.e., $\{h_1(x),h_2(x), \cdots, h_T(x) \}$. The softmax probability of the output set is calculated. The uncertainty of the prediction ($u_x$) is quantified using the resulting entropy: $u_x = -\sum_a p_a(x) \log (p_a(x))$, where $p_a(x)$ is the probability that sample $x$ belongs to demographic group $a$ estimated over $T$ stochastic forward passes, i.e., $p_a(x) = \frac{1}{T}\sum_{t=1}^{T} h^a_t(x)$. In the experiments (Section~\ref{app:ablation-confidence-inter}), we compare with another uncertainty measure based on the confidence interval of the prediction probability and show the limitation of this measure in identifying highly uncertain samples. {\updated Furthermore, we also show that using reliable uncertainty measures such as conformal prediction~\citep{angelopoulos2023conformal} can also improve the fairness-accuracy tradeoff.}

%\paragraph{Why the student-teacher framework?}
%We design the student-teacher framework as a semi-supervised learning approach of the sensitive attributes with uncertainty awareness. The teacher model can be seen as a “clone” of the student model; the main difference is that the teacher is not trained to minimize a loss function, but its weights are updated using the moving average of the student’s weights. The teachers’ weights can be seen as the weighted average of an ensemble of networks (students) throughout different training epochs. \cite{lainetemporal} has adopted a similar process for semi-supervised learning. Using the consistency loss, our goal is to enforce the student model to focus on samples with low uncertainty. We show in the experiments (Appendix~\ref{app:ablation-consistency-loss}) that training the student model without consistency loss results in worse fairness-accuracy tradeoffs.

\subsection{Enforcing Fairness w.r.t Reliable Proxy Sensitive Attributes} 
\label{sec:step2}
After the first phase, the attribute classifier can produce for every sample in $\mathcal{D}_1$, i.e., samples with missing sensitive attributes, their predicted sensitive attribute (proxy)~$\hat{A}=\{h(x_i)_{x_i \in \mathcal{D}_1}\}$, and the uncertainty of the prediction~$U = \{u_{x_i}\}_{x_i \in \mathcal{D}_1}$. To validate our hypothesis, we define a confidence threshold $H$ for samples used to train the label classifier with fairness constraints, i.e., the label classifier with fairness constraints is trained on a subset $\mathcal{D}_1' \subset \mathcal{D}_1$ defined as follows:  
\begin{equation}
\label{eq:D_1}
    \mathcal{D}_1' = \{(x, y, f(x)) | u_x \leq H \}
\end{equation}
{\updated Note that the threshold $R$ in the previous step is only used to train the attributes classifier while ensuring the student and teacher remain consistent for samples with uncertainty lower than $R$. The threshold $H$ in this step is used at test time to select samples with low uncertainty, and its value can be tuned over a validation set. 
}
The hypothesis of enforcing fairness on samples whose sensitive attributes are reliably predicted stems from the fact that the model can confidently distinguish these samples based on their sensitive attributes in the latent space. In contrast, the label classifier is inherently fairer if an attribute classifier cannot reliably predict sensitive attributes from training data~\citep{kenfacklearning}. We further support this in section~\ref{sec:results} by comparing the sensitive attribute uncertainty in the New Adult dataset~\citep{ding2021retiring} and the old version of the dataset~\citep{asuncion2007uci}. The fairness constraints on samples with unreliable sensitive attributes could push the model's decision boundary in ways that penalize accuracy and/or fairness. We support these arguments in the experiments. In addition to this method, we considered two other approaches built around our hypothesis: the weighted approach--- FairDSR (weighted) and the uncertain approach--- FairDSR (uncertain)
%Therefore, enforcing fairness constraints on samples with the most reliable proxy attributes would be more useful in achieving better accuracy-fairness tradeoffs than considering samples for which the sensitive attributes are not distinguishable in the latent space. 

\textbf{FairDSR (weighted)}. Given that downsampling can reduce accuracy when uncertainty is low, instead of pruning out samples based on the uncertainty of their sensitive attributes, we considered a training process in the second step that enforces fairness constraints on all samples but weighted proportionally to the uncertainty of their sensitive attributes, i.e., samples with less uncertainty receive higher weights. We show in the experiments that this weighted approach can better preserve accuracy. 

\textbf{FairDSR (uncertain)}. Considering the ethical risks of inferring missing sensitive information, we also considered a variant in the second step where the model is trained without fairness constraints, i.e., without using sensitive information. Instead, we train the model using samples with higher uncertainty in their sensitive attributes for a given uncertainty threshold. We show in the experiments that this variant can significantly improve fairness, although no fairness constraints are applied.


 
\section{Experiments}
\label{sec:experiements}
In this section, we demonstrate the effectiveness of our framework on five datasets and compare it to different baselines.
\subsection{Experimental Setup}

\paragraph{Datasets.} 

We validate our method on five real-world benchmarks widely used for bias assessment: Adult Income~\citep{asuncion2007uci}\footnote{https://archive.ics.uci.edu/ml/datasets/Adult}, Compas~\citep{compas}, Law school (LSAC), CelebA~\citep{liu2018large}~\citep{wightman1998lsac}, and the New Adult~\citep{ding2021retiring} dataset. We use 20\% of each dataset as the group-labeled dataset (\Dtwo) and 80\% as the dataset without sensitive attributes (\Done). Supplementary~\ref{app:ablation-dataset-size} shows that our results hold even with a smaller group-labeled ratio, e.g., $5\%$. More details about the datasets appear in Supplementary~\ref{sec:datatets}.


%We validate our method on three real-world benchmarks widely used for bias assessment: Adult Income~\citep{asuncion2007uci}, Compas~\citep{compas}, and LSAC~\citep{wightman1998lsac} dataset (Cf. Table~\ref{tab:datasets}). 
%In the Adult dataset, the goal is to predict if an individual's annual income is greater or less than \$50k per year. The dataset comes divided into training and testing sets of around 32000 and 16000 samples respectively\footnote{https://archive.ics.uci.edu/ml/datasets/Adult}. We train the label classifier on the training set ($\mathcal{D}_1$) and the sensitive attributes on the testing set (\Dtwo). We used gender as the sensitive attribute, which was categorized in the dataset as Male or Female. We also considered the recent version of the Adult dataset (New Adult) for the year 2018 across different states in US~\citep{ding2021retiring}. The New Adult dataset contains around 1.6M  samples described with 10 features. 
%The Compas dataset contains around 6,000 samples described with 10 features. The goal is to predict whether a defendant will recidivate within two years. We use race as the sensitive attribute and 20\% of the samples are randomly sampled to train the sensitive attribute classifier (\Dtwo). For all the datasets, all the features are used to train the attribute classifier except for the target variable.

%\paragraph{Metrics}   

\paragraph{Attribute classifier.}  The student and teacher models were implemented as feed-forward Multi-layer Perceptrons (MLPs) with Pytorch~\citep{paszke2019pytorch}, and the loss function~\ref{eq:loss} is minimized using the Adam optimizer~\citep{kingma2014adam} with learning rate $0.001$ and batch size $256$. Following~\citet{yu2019uncertainty, lainetemporal}, we used $\alpha = 0.99$ for the EMA parameter for updating the teacher weights using the student's weights across epochs. The uncertainty threshold is finetuned over the interval $[0.1, 0.7]$ using 10\% of the training data. The best-performing threshold is used for the second step's threshold to obtain $\mathcal{D'}_1$. The uncertainty threshold that achieved the best results are $0.30$, $0.60$, $0.66$, and $0.45$ for the Adult, Compas, LSAC, and CelebA datasets, respectively.
 
\paragraph{Baselines.} 
For fairness-enhancing mechanisms, we considered the Fairlean~\citep{bird2020fairlearn} implementation of the exponentiated gradient~\citep{agarwal2018reductions}. 
We considered the three variants of our approach described in section~\ref{sec:step2}: FairDSR (weighted), FairDSR (uncertain), and our base method, FairDSR (certain).   %(1) a variant where the model is trained without fairness constraints but using samples with higher uncertainty in the sensitive attribute predictions --- FairDSR (uncertain); (2)  a variant where only samples with reliable (certain) attributes are used to train the label classifier with fairness constraints using the exponentiated gradient --- FairDSR (certain); (3) and a variant where fairness constraints are enforced on samples weighted based on the uncertainty of their sensitive attributes --- FairDSR (weighted). 

For comparison, we considered methods aiming to improve fairness without (full) demographic information. We compare with the following methods: 
\begin{itemize}
\setlength\itemsep{0.05em}
      %This represents the ideal situation where all the assumptions about the availability of demographic information are satisfied. This baseline is expected to achieve the best tradeoffs. 
    \item \FairRF~\citep{zhao2022towards}: This method assumes that non-sensitive features correlating with sensitive attributes are known. It leverages these related features to improve fairness w.r.t the unknown sensitive attributes.
    \item \FairDA~\citep{liang2023fair}: Similar to our setting, this method assumes the sensitive information is available in a \textit{source domain} (dataset \Dtwo~in our setting). It uses a domain adaptation-based approach to transfer demographic information from the source domain to improve fairness in the target using an adversarial approach. 
    \item \CGL~\citep{jung2022learning}: With partial access to sensitive attributes, this method also uses an attribute classifier and replaces uncertain predictions with random sampling from the empirical conditional distribution of the sensitive attributes given the non-sensitive attributes $P(A|X, Y)$.
    \item \ARL \citep{lahoti2020fairness}: The method uses an adversarial approach to upweight samples in regions hard to learn, i.e., regions where the model makes the most mistakes.
    \item Distributionally Robust Optimization~(DRO) \citep{hashimoto2018fairness}: It optimizes for the worst-case distribution around the empirical distribution. Similar to ARL, the goal is to improve the accuracy of the worst-case group.  
    \item \CVarDRO~\citep{levy2020large}: It is an improved variant of DRO.
    \item \KSMOTE~\citep{yan2020fair} performs clustering to obtain pseudo groups and use them as substitutes to oversample the minority groups. 
\end{itemize}
 

%In addition to these baselines, we considered other methods that enforce fairness when the sensitive attribute is (partially) missing~\citep{lahoti2020fairness, hashimoto2018fairness, levy2020large, yan2020fair, zhao2022towards}. The results of the comparison are presented in the appendix~(\ref{app:comparison}). 
For each method considered for comparison, we used the code provided by the authors\footnote{We implemented \FairDA~and reproduced using the instructions in the paper~\citep{liang2023fair}.} along with the recommended hyperparameters. We considered two \textit{vanilla} baselines: a baseline where the model is trained without fairness constraints (\VanilaNoFairness) and a baseline model trained with fairness constraints over the true sensitive attributes (\VanilaFairness). We also considered baselines trained with fairness constraints directly using predicted sensitive attributes obtained by data imputation using our student network (\VanilaProxyFairness) and imputation using  K-nearest neighbor (Proxy-KNN). 
  %using exponentiated gradient~\citep{agarwal2018reductions}.

For comparison, in addition to the accuracy, we consider the three fairness metrics described in the appendix~\ref{app:fair_metric}, i.e., equalized odds ($\Delta_{\text{EOD}}$), equal opportunity ($\Delta_{\text{EOP}}$), and demographic parity ($\Delta_{\text{DP}}$).  All the baselines are trained on 70\% of $\mathcal{D}_1$, and fairness and accuracy are evaluated on the 30\% as the test set. We assume the sensitive attribute is observed in the test set to report the true fairness violation. We trained each baseline 7 times and averaged the results. We use logistic regression\footnote{Appendix~\ref{app:additional-results-mlp} shows a comparison with an MLP model.
} as the base classifier for all the baselines and train each baseline to achieve minimal fairness violation.

\begin{table}%[ht]
  \centering
  %\fontsize{8.5pt}{7.5pt}\selectfont 
  %\resizebox{.5\textwidth}{!}{%
  \begin{tabular}{lcclll}
    \toprule 
    Dataset     & Mean uncertainty ($\downarrow$)     & Accuracy sensitive attribute ($\uparrow$) & $\Delta_{\text{DP}}$ & $\Delta_{\text{EOD}}$ & $\Delta_{\text{EOP}}$\\
    \midrule 
    Adult     & 0.15 & 85\% & 0.18 &  0.20 & 0.13 \\
    New Adult     & 0.42  & 68\% & 0.06 & 0.05 & 0.04\\
    Compas    & 0.39  & 72\% & 0.28 & 0.32 & 0.32\\
    CelebA & 0.21 & 83\% & 0.17 & 0.19 & 0.19 \\ 
    LSAC & 0.66 & 55\% & 0.014 & 0.005 & 0.049 \\ 
    \bottomrule
  \end{tabular}
 % }
   \caption{Average uncertainty of the attribute classifier and fairness of a label classifier on the dataset with missing sensitive attributes.}
  \label{tab:data-info}
\end{table} 
   
\subsection{Results and Discussion}
\label{sec:results}

\begin{table*}[ht]
  \def\arraystretch{1.1}
  %\fontsize{6.5pt}{6.5pt}\selectfont 
  \centering
  \resizebox{14cm}{!}{%
\begin{tabular}{l|llll} 
\hline
Method & Accuracy & $\Delta_{\text{DP}}$ &   $\Delta_{\text{EOP}}$  & $\Delta_{\text{EOD}}$ \\ \hline
\VanilaNoFairness   &    0.851 $\pm$ 0.008      &     0.171 $\pm$ 0.004 &  0.088 $\pm$ 0.033   &   0.091 $\pm$  0.030    \\ 
\VanilaFairness    &    0.829 $\pm$ 0.002      &    0.005 $\pm$  0.004        &  0.021 $\pm$ 0.014      &  0.017 $\pm$ 0.007     \\
\VanilaProxyFairness    &    0.829 $\pm$ 0.002      &    0.009 $\pm$  0.005        &  0.024 $\pm$ 0.014  &  0.019 $\pm$ 0.017     \\\cline{1-5} 
\FairRF  &   0.838 $\pm$ 0.002    &   0.162 $\pm$  0.015 &  0.063  $\pm$ 0.027    &  0.072 $\pm$ 0.019      \\

\FairDA  &  0.809  $\pm$  0.009   &   0.087   $\pm$ 0.028       &  0.071  $\pm$ 0.046 &  	0.078  $\pm$    0.039  \\
\CGL  &  0.834  $\pm$  0.002   &   0.009   $\pm$ 0.006       &  0.027  $\pm$ 0.020 &  	0.026  $\pm$    0.016 \\
\ARL     &    \textbf{0.850} $\pm$ 0.002      &     0.173 $\pm$ 0.013 &  0.028 $\pm$ 0.090   &   0.097 $\pm$  0.031    \\ 
\CVarDRO    &    0.820 $\pm$ 0.012       &     0.200 $\pm$ 0.005    &  0.160 $\pm$ 0.030      &   0.100 $\pm$ 0.027    \\
                                          % & \FairDA  &          &               &        &       \\

\KSMOTE  &   0.814 $\pm$ 0.003        &    0.302 $\pm$ 0.007          &   0.160  $\pm$ 0.021    &  0.196 $\pm$ 0.003      \\ 
\DRO  &  0.823  $\pm$  0.003     &   0.184    $\pm$   0.042    &   0.092 $\pm$  0.041   &   0.105 $\pm$ 0.041 \\ \cline{1-5}
FairDSR (weighted)    &    0.846 $\pm$ 0.003    &    0.032 $\pm$ 0.013          &  0.050 $\pm$ 0.033     &  0.027 $\pm$ 0.019    \\
FairDSR (uncertain)    &    0.825 $\pm$ 0.013    &    0.106 $\pm$ 0.036          &  0.065 $\pm$ 0.047     &  0.068 $\pm$ 0.032    \\ 

FairDSR (certain)   &    0.830 $\pm$ 0.004     &    \textbf{0.007} $\pm$ 0.005          &   \textbf{0.015} $\pm$ 0.010     &  \textbf{0.018} $\pm$ 0.016    \\ 
\hline    
                                          
\end{tabular}  
}
\caption{\textbf{Results on the Adult dataset.} Bolded values represent the best-performing baselines among the fairness-enhancing methods without (full) demographic information. All the baselines are trained on the dataset without the sensitive attributes (\Done). Each experiment is conducted 7 times, and the fairness and accuracy are averaged.
}
  \label{tab:data-baseline-adult}
\end{table*} 


\begin{table*}[ht] 
  \def\arraystretch{1.1}
  \centering 
  %\fontsize{6.5pt}{6.5pt}\selectfont 
    \resizebox{14cm}{!}{%
\begin{tabular}{l|llll} 
\hline
Method & Accuracy & $\Delta_{\text{DP}}$ &   $\Delta_{\text{EOP}}$  & $\Delta_{\text{EOD}}$ \\ \hline
\VanilaNoFairness     &    0.681 $\pm$ 0.011      &     0.285 $\pm$ 0.026 &  0.325 $\pm$ 0.029   &   0.325 $\pm$  0.029    \\ 
\VanilaFairness     &   0.634 $\pm$ 0.009       &   0.032 $\pm$ 0.011       &  0.039 $\pm$ 0.024    & 0.041 $\pm$ 0.016    \\  
\VanilaProxyFairness     &   0.634 $\pm$ 0.005       &   0.049 $\pm$ 0.008       &  0.099 $\pm$ 0.036    & 0.080 $\pm$ 0.022    \\  \cline{1-5}  
\FairRF  &   0.669 $\pm$  0.001   &   0.289   $\pm$  0.003      &  0.319  $\pm$ 0.004    &  	0.319  $\pm$ 0.004     \\

\FairDA  &  0.668  $\pm$ 0.019    &   0.229   $\pm$ 0.018       &  0.265  $\pm$  0.024   &  0.265  $\pm$  0.024     \\
\CGL  &  0.612  $\pm$ 0.019    &   0.032   $\pm$ 0.014       &  0.065  $\pm$  0.019   &  \textbf{0.065}  $\pm$  0.027     \\
\ARL      &     0.672 $\pm$ 0.009     &     0.290 $\pm$ 0.016         &   0.310  $\pm$ 0.010   &     0.320 $\pm$ 0.010 \\
\CVarDRO    &     0.668 $\pm$ 0.008     &      0.279 $\pm$ 0.018        &    0.300 $\pm$ 0.010     &  0.287 $\pm$ 0.015     \\
                                          % & \FairDA  &          &               &        &       \\
\KSMOTE  &   0.670 $\pm$ 0.012     &      0.286  $\pm$ 0.028       &   0.321 $\pm$ 0.028    &   0.321 $\pm$ 0.028    \\

\DRO  &   0.672 $\pm$ 0.010     &     0.282   $\pm$ 0.026      &   0.296 $\pm$ 0.017    &  0.296  $\pm$ 0.017     \\ \cline{1-5}
FairDSR (weighted)    &     0.672 $\pm$ 0.009    &      \textbf{0.027}  $\pm$ 0.016      & 0.069  $\pm$ 0.038    &  0.083  $\pm$ 0.035  \\
FairDSR (uncertain)    &     0.671 $\pm$ 0.009    &      0.272  $\pm$ 0.016      & 0.300  $\pm$ 0.039    &  0.300  $\pm$ 0.034  \\

FairDSR (certain)   &     \textbf{0.676} $\pm$ 0.009    &      0.085  $\pm$ 0.016      & \textbf{0.067}  $\pm$ 0.039    &  0.074  $\pm$ 0.034 \\

\hline       
\end{tabular} 
}
\caption{Results on the Compas dataset.}
  \label{tab:data-baseline-compas}
\end{table*}






\subsubsection{Uncertainty of the sensitive attribute and fairness.} We first analyze the relation between the sensitive attribute's uncertainty and the fairness of downstream models. Table~\ref{tab:data-info} showcases the average uncertainty of the sensitive attribute prediction estimated by our method and different fairness measures of a logistic regression model trained without fairness constraints on the dataset $\mathcal{D}_1$. These results show the correlation between the uncertainty of the sensitive attribute prediction and the fairness of the model. For example, we observe that the uncertainty in the Adult dataset is lower than in the New Adult dataset, while the unfairness in the Adult dataset is higher.
On the other hand, the LSAC dataset has the highest uncertainty of the sensitive attribute ($0.66$), and the model without fairness constraints has the smallest fairness violation across all fairness metrics. In sum, we can observe that unfairness is higher for datasets with a lower uncertainty in the sensitive attributes, e.g., the Adult and CelebA datasets. These results support our hypothesis that a model can hardly discriminate against samples with uncertain demographic information.  Furthermore, we investigate the relation between the level of uncertainty of the sensitive attributes in the training data and the fairness of downstream classifiers trained without fairness constraints. Our study reveals that a model without fairness constraints tends to be fairer as the uncertainty of the sensitive attribute in the training data increases.

%\section{Using the Uncertainty of Sensitive Attributes to Train Fair Models without Fairness Constraints.} 
%\label{sec:fairness-unaware-models}
Specifically, we trained different classifiers (Logistic Regression and Random Forest) without fairness constraints but using training data with higher uncertainty in the sensitive attributes. For different uncertainty thresholds $H \in \{0.0, 0.1, ..., 0.6\}$, we prune out samples whose uncertainty is lower than $H$ and train the model without fairness constraints using the remaining training data, i.e., $\{ (x,y) \in D_1 | u_x \geq H \}$, where $u_x$ is the estimated uncertainty provided by our method. In particular, when $H =0$, all the data points are used for the training, and in other cases, we only maintain samples with uncertainty higher than $H$, i.e., the model is trained on samples with more uncertain sensitive attributes. We train the model seven times with different random seeds for each uncertainty threshold and report fairness and accuracy in the testing set containing sensitive attributes. 

% Figure environment removed 

Figure~\ref{fig:no_fairness_constraints} shows the fairness and accuracy of a Random Forest classifier trained without fairness constraints. In the figure, each column represents the results on each dataset, and the first and the second rows provide the plots of fairness and accuracy for different uncertainty thresholds, respectively.
Across different datasets, the results show that unfairness decreases as the uncertainty threshold increases. We observe that the improvement in fairness is consistent for different fairness metrics considered, i.e., demographic parity, equal opportunity, and equalized odd.  We also observe a decrease in the accuracy, which is justified by reduced dataset size and improved fairness (tradeoff). On the LSAC dataset, fairness and accuracy remain almost constant as the average uncertainty in predicting the sensitive attribute on this dataset is 0.66, i.e., most of the samples already have the highest uncertainty. We observed similar results for a Logistic Regression classifier presented in the Supplementary (Figure~\ref{fig:lr_no_fairness_constraints}). However, this method incurs a higher drop in accuracy and does not necessarily guarantee that an adversary cannot reconstruct the sensitive attributes from the trained model~\citep{ferry2023exploiting}.    


\subsubsection{Fairness-accuracy tradeoffs.} 
% Figure environment removed

Table~\ref{tab:data-baseline-adult}, and~\ref{tab:data-baseline-compas} show the effectiveness of the FairDSR compared to other baselines on the Adult and Compas datasets, respectively. Results for the CelebA and LSAC appear in Appendix (Table~\ref{tab:data-baseline-celebA} and ~\ref{tab:data-baseline-lsac}, respectively). It is important to note that methods aiming to improve worst-case group accuracy (ARL, DRO, CVarDRO) do not necessarily improve fairness in terms of demographic party or equalized odds. In particular, results across different datasets show that \ARL can improve the Equal Opportunity metric but fails to improve demographic parity. It also yields the most accurate classifier as this method does not have a tradeoff with accuracy. On the other hand, \FairDA~\citep{liang2023fair}~and \CGL~\citep{jung2022learning}, which also exploit limited demographic information, show an improvement in fairness compared to other baselines. However, it incurs a higher drop in accuracy while our method using fairness constraints on samples with reliable sensitive attributes significantly outperforms them across all datasets. In other words, the results show that our method with fairness constraints on samples with reliable sensitive attributes provides Pareto dominant points in terms of fairness and accuracy. 

On the other hand, the variant using a model trained without fairness constraints (without using sensitive attributes) provides better fairness-accuracy tradeoffs compared to other baselines on the Adult and the CelebA datasets while providing comparable results on datasets with higher uncertainty (LSAC and Compas). For example, the LSAC dataset has an average uncertainty of 0.66, meaning most samples already have uncertain sensitive information, and the unfairness is already low. As no fairness constraints are enforced in FairDSR (uncertain), it less impacts fairness and accuracy as most data samples are preserved due to high overall uncertainty. On the other hand, we observe that FairDSR (weighted) can outperform other baselines and provide better accuracy than FairDSR (certain) at the cost of higher unfairness. The improved accuracy is explained by the use of all data points in the weighted approach. At the same time, FairDSR (certain) can strengthen fairness when only samples with reliable sensitive attributes are used. FairDSR (certain) also provides better Pareto dominant points than \CGL~and \FairDA. While \CGL, which also accounts for the uncertainty of the sensitive attributes, can outperform other baselines, especially those directly using predicted proxy-sensitive attributes. These results support our hypothesis that using data points with certain sensitive attributes is more beneficial to fairness and accuracy. % While  and comparable tradeoffs on the Compas and Lsac datasets.   
Figure~\ref{fig:expgrad-rf-compas-adult} shows that imputation methods can yield reasonably good fairness-accuracy tradeoffs on the Adult and Compas datasets but are suboptimal compared to the model using the true sensitive attributes.
We observed similar results with different base classifiers (logistic regression and gradient-boosted trees) and fairness mechanisms. These results appear Appendix~\ref{app:comparison}.
\subsubsection{Ablation experiments}

\paragraph{Impact of the uncertainty threshold.} Figure~\ref{fig:ablation-impact-uncertainty} showcases the impact of the uncertainty threshold on the fairness-accuracy threshold. %The Figure depicts the Pareto front for different fairness violations by varying the parameter $\epsilon \in [0, 1]$ controlling the balance between fairness and accuracy. The model is trained with 7 seeds for each value of $\epsilon$.  
When the feature space encodes much information about the sensitive attribute as in the Adult dataset (Figure~\ref{fig:ablation-impact-uncertainty}a) with 85\% accuracy of predicting the sensitive attributes, the results show that the more we enforce fairness w.r.t. samples with the lower uncertainty, the better the fairness-accuracy tradeoffs. In this regime, enforcing unfairness helps the model maintain a better accuracy level (Figure~\ref{fig:ablation-impact-uncertainty}a). In contrast, in a low bias regime, i.e., when the feature space does not encode enough information about the sensitive attributes, such as on the Compas and the New Adult dataset, the model achieves better fairness-accuracy tradeoffs when a higher uncertainty threshold is used. In this regime, most of the samples have higher uncertainty in the sensitive attribute prediction, and fairness violation is smaller (see Table~\ref{tab:data-info}), as can be observed in Figure~\ref{fig:ablation-impact-uncertainty}b, the use of a lower uncertainty threshold leads to a decrease in accuracy while fairness is improved. We observe similar results in the New Adult, CelebA, and LSAC datasets (Fig ~\ref{fig:ablation-impact-uncertainty-rf} in Supplementary). The accuracy drops since more and more samples were pruned out from the datasets, and this suggests that the feature space is more informative for the target task than the demographic information.  
In the appendix (\ref{app:uncertainty_disp}), we show that while under-represented demographic groups can have higher uncertainty on average than well-represented groups, minority groups are still consistently represented when a lower threshold is used. %As such, the uncertainty threshold should be carefully tuned in order to achieve a better tradeoff, in particular in datasets with low bias.   


% Figure environment removed

\paragraph{Importance of the consistency loss.}
\label{app:ablation-consistency-loss}
In the proposed framework, we use trained attribute classifier with a double loss: the cross-entropy loss (supervised loss) and the consistency loss (unsupervised loss). The parameter $\lambda$ controlling the consistency loss is updated using a Gaussian ramp-up function starting from zero at the beginning of the training. The consistency loss pushes the student to focus on data points with highly certain sensitive attributes. Without this second loss term, the student model minimizes the classification error without explicit consideration of the uncertainty in the dataset without sensitive attributes. 
To demonstrate the importance of consistency loss (and thus the importance of self-ensembling with the teacher), we experiment with the student model trained without the consistency loss, i.e., we set $\lambda=0$ in Equation \ref{eq:loss}. We applied the same procedure of the second step of FairDSR (certain) to derive data points with sensitive attributes predicted with low uncertainty.   

% Figure environment removed

Figure~\ref{fig:ablation-consistency-loss}  showcases the Pareto front of our FairDSR trained without (w/) and without (w/o) the consistency loss. The base classifier is Random Forest with exponentiated gradient as the fairness mechanism. The Figure also shows the Pareto front of the model trained with the ground truth sensitive attribute and the predicted sensitive attribute (Proxy-DNN). We observe that training the model with the consistency loss provides Pareto dominant points across different fairness metrics. This demonstrates the benefit of the consistency loss in effectively identifying samples with sensitive attributes predicted with low uncertainty in the dataset without demographics. We also observed that the mean uncertainty for the model without the consistency loss is around $0.19 \pm 0.08$, while the model that uses the consistency loss is $0.15 \pm 0.12$. This suggests that the consistency loss effectively enforces the student model focus on data points with low uncertainty.  Moreover, we also observe that the model without the consistency loss still outperforms the model that directly uses the sensitive attributes (Proxy-DNN) and the model that uses true sensitive attributes. This demonstrates that our hypothesis still holds even if the uncertainty measure is less effective, and a better uncertainty measure can provide even better results.

\paragraph{Experiment with uncertainty measure based on confidence intervals.}
\label{app:ablation-confidence-inter}
We adopted Monte Carlo dropout because it efficiently captures various sources of uncertainty, both in the data and the model. Another classic approach to measuring uncertainty is the model's prediction confidence. In this experiment, we evaluate the effect of using confidence intervals as the uncertainty measure on fairness-accuracy tradeoffs. {\updated Given a trained attribute classifier, recall we obtain the predicted group label by \textit{thresholding} the prediction probability, i.e., $f(x) = \mathbbm{1}(P(\hat{A} = a | X = x) \geq 0.5) $.

We construct the confidence interval using a threshold $\tau \in [0.5, 1]$, such that samples with higher uncertainty have their prediction probability closer to $0.5$. A given data point $x$ is in the high uncertainty set if  $ 1-\tau < P(\hat{A} = a|X =x) < \tau$, while $x$ is in the low uncertainty set otherwise. More specifically, in this experiment, we derive the subset of data points with low uncertainty of the sensitive attribute prediction as follows :

\begin{equation} 
     (x, y, f(x)) \in \mathcal{D}_1'  \; \operatorname{if} \; \begin{cases}
             P(\hat{A} = a|X =x) \in [0, 1- \tau], & \\
            \;\;\;\;\;\;\;\;\;\;\;\;\; \text{or}  & 
        \forall a \in \{0,1\}\\
            P(\hat{A} = a|X =x) \in [\tau, 1], &
        \end{cases} 
\end{equation}

%the confidence interval is defined by $[\tau, 1]$, and the set of data points with high confidence contains samples whose prediction probability is within the confidence interval, i.e., $P(\hat{A} = a|X =x) \in [\tau, 1]$. In contrast, low-confidence samples will have their prediction probability within $(1-\tau, \tau)$. 

% Figure environment removed

We experiment with different confidence thresholds $\tau$ on the Adult dataset. Specifically, for each confidence threshold $\tau \in \{ 0.6, 0.7, 0.8, 0.9\}$, we trained the Random Forest model using data points with high confidence. Setting the confidence threshold to $0.5$ is similar to using all data points to train the model.} As shown in Figure~\ref{fig:ablation-confidence-threshold}, models trained using confidence intervals as uncertainty measures yield worse Pareto points. Specifically, we observe a significant drop in accuracy while fairness performances significantly improve.  We also observe that a higher confidence threshold tends to provide better fairness for a similar level of accuracy. While this observation also supports our hypothesis, the drop in accuracy suggests that the confidence interval is less effective in identifying data points with low uncertainty in sensitive attributes. {\updated We suspect this results from the unreliability of the model probability due to overconfidence or underconfidence in wrong predictions~\citep{guo2017calibration}.} As the confidence interval is obtained using a single model, low-confidence prediction could be solely caused by poor calibration or randomness in the algorithm. {\updated We validate this in the next ablation study by calibrating the attribute classifier and using conformal prediction to measure uncertainty}. %On the other hand, Monte Carlo dropout computes the uncertainty using multiple  (independent)  predictions from different models, and the consistency loss minimizes the uncertainty of data with missing sensitive attributes.  
%Exploring the performance of other uncertainty measures represents an interesting direction for future work.   

{\updated \paragraph{Ablation on uncertainty measure based on conformal predictions.}  



% Figure environment removed

In this experiment, we investigate the validity of our hypothesis under different and reliable uncertainty measures, such as conformal prediction~\citep{shafer2008tutorial}.  Conformal prediction, a machine learning framework, constructs prediction sets containing possible labels. These sets are guaranteed to include the true label with a probability of $1 - \epsilon$, where $\epsilon \in [0,1]$ is a user-defined error rate. For instance, setting $\epsilon=0.1$ ensures the prediction set has at least a $90\%$ chance of containing the correct label. The size of the set reflects the model's confidence in its prediction. Ideally, the set would contain only one label, indicating high confidence. Larger sets suggest greater uncertainty about the predicted label. To construct the prediction set of the sensitive attribute, with the $1 - \epsilon$ guarantee, we implement \textit{split conformal prediction} as described by ~\citet{angelopoulos2023conformal}, which is the widely-used version of conformal prediction. Split conformal prediction takes the pretrained attribute classifier and calibration set to compute the conformal prediction threshold\footnote{See ~\citet{angelopoulos2023conformal} for more details on how prediction sets are constructed} used to include labels in the prediction set of the test set. We employ the Model Agnostic Prediction Interval Estimator (MAPIE) library to train a conformal classifier and measure the uncertainties in the attribute classier~\citep{cordier2023flexible}. The classifier wraps the original attribute classifier and produces conformal prediction sets with the guaranteed marginal coverage rate $1-\epsilon$. We used $10\%$ of the dataset with sensitive attributes as the calibration set and to train the calibrated attribute classifier. 
 
In the second step of our framework, we generate prediction sets of the sensitive attributes in the dataset where demographic information is missing (\(\mathcal{D}_1\)). Since the sensitive attributes are binary, there are four possible prediction sets for each data point: \(\{0\}\), \(\{0,1\}\), \(\{1\}\), and the empty set \{\(\varnothing\)\}. This means each prediction set for every sample consists of either a single attribute value, both values or no value at all~\citep{gupta2020distribution}. Samples with prediction sets containing exactly one attribute value are grouped into subsets of samples with low uncertainty (\(\mathcal{D}_1'\)). On the other hand, samples with empty prediction sets or sets containing both attribute values indicate higher uncertainty regarding the sensitive attributes. As we used a score-based method to compute non-conformity scores, the prediction set for uncertainty predictions might be empty if a set size smaller than 1 is necessary to ensure $1-\epsilon$ coverage~\citep{cordier2023flexible}.

In our experiments on the Adult dataset, we varied \(\epsilon\) across values \(\{0.05, 0.1, 0.2\}\). This means for \(\epsilon = 0.05\), \(\epsilon = 0.1\), and \(\epsilon = 0.2\), the prediction sets are guaranteed to include the true sensitive attribute with probabilities of \(95\%\), \(90\%\) and \(80\%\), respectively. Our model training incorporated fairness constraints specifically on data points where the prediction set contained a single attribute value, indicating higher confidence in the prediction for smaller values of $\epsilon$. Following the previous evaluation, we trained the Random Forest classifier for each $\epsilon$ and sweeping range of fairness coefficients $\lambda$, taking the median of 7 runs and computing the Pareto front.   Figure~\ref{fig:ablation-conformal} showcases the Pareto front of different models with fairness constraints on samples with greater certainty of the sensitive attributes (samples with single-valued prediction sets) for different values of $\epsilon$. As can be seen in Fig~\ref{fig:ablation-conformal}, all models trained using samples with low uncertainty in the sensitive attributes achieve better Pareto front than the model using all the predicted sensitive values (Proxy-DNN) and even ground truth sensitive attributes (clean). This demonstrates the benefit of applying fairness constraints over data points with low uncertainty in the sensitive on-fairness accuracy tradeoff. Moreover, the figure shows that smaller values of $\epsilon$ result in better Pareto dominant points, i.e., better fairness-accuracy tradeoff. This is justified by the fact that smaller values of $\epsilon$ increase the certainty of samples with a single value in their prediction set. Specifically for $\epsilon = 0.05$, the Pareto front is closer to our baseline method using the dropout-based uncertainty measure. We observed a similar trend on other datasets and with different base classifiers. These results also support our hypothesis that imposing fairness constraints on the sample with low uncertainty in the sensitive attribute can better improve fairness and accuracy. 

%% Figure environment removed
 

\begin{table}%[h!]
\centering
\begin{tabular}{cccccc}
\toprule
\textbf{Sensitive attributes} & \textbf{Coverage $\epsilon$} & \textbf{Accuracy} & $\Delta_{\text{DP}}$ &   $\Delta_{\text{EOP}}$  & $\Delta_{\text{EOD}}$ \\
\midrule
Uncertain + Certain & N/A & 0.82 & 0.08 & 0.07 & 0.07 \\
\midrule
\multirow{3}{*}{Certain} & 0.05 & 0.84 & 0.123 & 0.089 & 0.118 \\
 & 0.1 & 0.83 & 0.109 & 0.119 & 0.100 \\
 & 0.2 & 0.83 & 0.110 & 0.081 & 0.109 \\ 
\midrule
\multirow{3}{*}{Uncertain} & 0.05 & 0.77 & 0.009 & 0.012 & 0.007 \\
 & 0.1 & 0.77 & 0.006 & 0.017 & 0.009 \\
 & 0.2 & 0.76 & 0.003 & 0.005 & 0.008 \\
\bottomrule
\end{tabular}
\caption{Study of models without fairness constraints using different levels of uncertainty measured with conformal predictions. %Models trained without fairness constraints using samples with greater certainty of the sensitive attributes tend to exaggerate unfairness. Using samples with higher uncertainty yields fairer models.
}
\label{tab:ablation-conformal-unfair}
\end{table}

%\begin{table}[h!]
%\centering
%\begin{tabular}{cccccccccccccc}
%\toprule
%\textbf{Epsilon} & \multicolumn{4}{c}{\textbf{Certain}} & \multicolumn{4}{c}{\textbf{Uncertain}} & \multicolumn{4}{c}{\textbf{Uncertain + Certain}} \\
%\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
% & Acc. & DP & EOD & EOP & Acc. & DP & EOD & EOP & Acc. & DP & EOD & EOP \\
%\midrule
%0.05 & 0.84 & 0.123 & 0.089 & 0.118 & 0.77 & 0.009 & 0.012 & 0.007 & \multirow{3}{*}{0.82} & \multirow{3}{*}{0.08} & \multirow{3}{*}{0.07} & \multirow{3}{*}{0.07} \\
%0.1 & 0.83 & 0.109 & 0.119 & 0.100 & 0.76 & 0.006 & 0.017 & 0.009 & & & & \\
%0.2 & 0.83 & 0.110 & 0.081 & 0.109 & 0.76 & 0.003 & 0.005 & 0.008 & & & & \\
%\bottomrule
%\end{tabular}
%\caption{tudy of models without fairness constraints using different levels of uncertainty measured with conformal predictions.}
%\label{tab:fairness_accuracy}
%\end{table}


On the other hand,  using conformal predictions to measure uncertainty, we also evaluated the variant of our hypothesis where the model is trained without fairness constraints using samples with greater uncertainty in the predicted sensitive attributes. This means we trained the model without fairness constraints but using samples whose prediction sets are empty or contain two values. We also trained models without fairness constraints using samples with greater certainty of the sensitive attributes, i.e., samples with single values in their prediction sets. Table~\ref{tab:ablation-conformal-unfair} shows the fairness and accuracy of Random Forest models trained without fairness constraints using data with different uncertainty levels on the Adult dataset. More specifically, we considered three cases: 
\begin{itemize}
    \item \textit{Certain + Uncertain}: The model is trained using all data points, i.e., the data contain samples with certain and uncertain sensitive attributes.
    \item \textit{Certain}: The model is trained using only samples with greater certainty of the sensitive attribute. This means the training data contains only samples with a single in the prediction sets.
    \item \textit{Uncertain}: The model is trained using only samples with greater uncertainty of the sensitive attribute. This means the training data contains only samples whose prediction sets are empty or contain two values.
\end{itemize}

The results depicted in Table~\ref{tab:ablation-conformal-unfair} show that, without fairness constraints, the models using samples with greater certainty of sensitive attributes have better accuracy but tend to exacerbate unfairness, while models using data with lower uncertainty significantly improve fairness at the expense of accuracy. More specifically, for $\epsilon =0.05$, the model using only data with uncertain sensitive attributes achieves fairness in terms of demographic parity of $0.009$ compared to $0.12$ for the model using data with certain sensitive attributes. This corresponds to a $92\%$ improvement in fairness with a smaller drop of $8\%$ for accuracy.
This shows models hardly discriminate against samples with uncertain sensitive attributes, and the fairness-accuracy tradeoffs can be controlled by uncertainty in sensitive attribute space. This also supports our hypothesis under different and reliable uncertainty measures.
}

\section{Conclusion}
In this work, we introduced FairDSR, a framework to improve the fairness-accuracy tradeoff when only limited demographic information is available. Our method introduces uncertainty awareness in the sensitive attributes classifier. We showed that uncertainty in the attribute classifier plays an important role in the fairness-accuracy tradeoffs achieved in the downstream model with fairness constraints. We showed that enforcing fairness on samples whose sensitive attributes are predicted with low uncertainty can yield models with better fairness-accuracy tradeoffs. Subsequently, we considered different variations around our hypothesis and demonstrated the benefit of uncertainty quantification in the sensitive attribute space for designing fair models under unknown demographic information. 

%Our method consistently achieved a better tradeoff than existing methods and in most cases even better tradeoffs than the use of the true sensitive attributes. However, in a low-bias regime, most samples have uncertain sensitive attributes leading to a decrease in the accuracy. In future work, we plan to introduce weighted empirical risk minimization in the fairness-enhancing model where the samples' weights are defined based on the uncertainty of the attribute classifier.

%  Our method consistently achieved a better tradeoff than other classic attribute classifiers and in most cases even better tradeoffs than the use of the true sensitive attributes. However, in a low-bias regime, most samples have uncertain sensitive attributes leading to a decrease in the accuracy. In future work, we plan to introduce weighted empirical risk minimization in the fairness-enhancing model where the samples' weights are defined based on the uncertainty of the attribute classifier.


\section*{Impact Statement}
The proposed framework shows evidence that models can hardly discriminate against samples with high uncertainty in sensitive attribute prediction. However, our base model relies on the predicted missing sensitive information. Inferring sensitive information can raise ethical concerns and face legal restrictions, especially when individuals do not choose to disclose their sensitive information.
The line of methods relying upon proxy attributes or inferring sensitive attributes faces this limitation~\citep{diana2022multiaccurate, awasthi2021evaluating, coston2019fair, liang2023fair}.
For this reason, we proposed a variant of our model (FairDSR (uncertain), Sec. \ref{sec:step2}) that derives a dataset with higher uncertainty in sensitive attributes without using predicted information.{\updated In addition, verifying that no group of users is left out when training the model due to the lower uncertainty of their sensitive attributes is important. While we analyzed group representation and showed that groups are still relatively well represented even with a higher uncertainty threshold (Fig. \ref{fig:representation_rate_samples} in Supplementary), there are still a risks of disparate impact.}

We emphasize that the inference of the sensitive attributes using our proposed method should not be used for any purpose other than bias assessment and mitigation.    

\subsubsection*{Acknowledgments}
SEK is supported by CIFAR and NSERC DG (2021-4086) and UA by NSERC DG (2022-04006). We thank the anonymous reviewers for the feedback, which has helped improve the paper.

\bibliography{main}
\bibliographystyle{tmlr}

\appendix
\input{supplementary_material}

\end{document}
