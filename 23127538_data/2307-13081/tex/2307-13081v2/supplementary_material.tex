\newpage
\section*{Supplementary Material}
\label{sec:appendix}

\section{Limitations}

%Moreover, we showed that with a variant of our method (\textit{FairDSR (uncertain)}), it is possible to train a fairer model without fairness constraints but using samples with high uncertainty in the sensitive attribute predictions. This variant can operate without (predicted) sensitive information, thereby alleviating potential legal or ethical concerns associated with the prediction of sensitive information. We show in Appendix~\ref{sec:fairness-unaware-models} the impact of the uncertainty threshold on the fairness of a model trained without fairness constraints.  %that builds fair models without fairness constraints---meaning, it operates without (predicted) sensitive attributes. %Instead, the model is trained on samples with high uncertainty in the sensitive attribute prediction (see Appendix~\ref{sec:fairness-unaware-models}). 

Our method is evaluated mainly on one fairness-enhancing algorithm (i.e., Exponentiated Gradient). It will be interesting to explore if our hypothesis applies to pre-processing and post-processing techniques and with different fairness-enhancing algorithms. Finally, our assumption that the true sensitive attributes are available in the test dataset for fairness evaluation might not be true in several practical scenarios. This might require evaluation using proxy-sensitive attributes. These proxies are likely noisy and might require evaluations using bias assessment methods that effectively quantify fairness violation w.r.t to true sensitive attribute~\citep{chen2019fairness, awasthi2021evaluating}. On the other hand, it will be interesting to back our empirical findings with theoretical results by characterizing fairness violation bounds based on the uncertainty quantification of the sensitive attribute predictions. 
 
\section{Fairness Metrics \& Datasets}
\label{app:fair_metric}
In the main paper, we considered the three popular group fairness metrics: demographic parity, equalized odds, and equal opportunity. These metrics aim to equalize different models' performances across different demographic groups. Samples belong to the same demographic group if they share the same demographic information, $A$, e.g., gender and race.    
\subsection{Fairness Metrics}
\paragraph{Demographic Parity}
Also known as statistical parity, this measure requires that the positive prediction ($f(X) = 1$) of the model be the same regardless of the demographic group to which the user belongs~\citep{dwork2012fairness}. More formally the classifier $f$ achieves demographic parity if $P(f(X) = 1 | A=a) = P(f(X)=1)$
In other words, the model's outcome should be independent of sensitive attributes. In practice, this metric is measured as follows:
 \begin{equation}
\label{eq:dp}
    \Delta_{ \text{DP} } (f) = \left| \underset{x|A=0}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] - \underset{x|A=1}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] \right|
\end{equation}
Where $\mathbb{I}(\cdot)$ is the indicator function.
\paragraph{Equalized Odds}
This metric enforces the True Positive Rate (TPR) and False Positive Rate (FPR) for different demographic groups to be the same $ P(f(X) = 1 | A=0, Y=y) = P(f(X) = 1 | A=1, Y=y), :; \forall y \in \{0,1\}$.  The metric is measured as follows:

\begin{equation}
\label{eq:eod}
    \Delta_{\text{EOD}} = \alpha_0 + \alpha_1 
\end{equation}

Where,

\begin{equation}
\label{eq:alph_}
    \alpha_j =  \left| \underset{x|A=0, Y=j}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] - \underset{x|A=1, Y=j}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] \right|
\end{equation}

\paragraph{Equal Opportunity}
In certain situations, bias in positive outcomes can be more harmful. Therefore, the Equal Opportunity metric enforces the same TPR across demographic~\citep{hardt2016equality} and is measured using $\alpha_1$ (Eq.~\ref{eq:alph_}).

\subsection{Datasets} 
\label{sec:datatets}
In the Adult dataset, the task is to predict if an individual's annual income is greater or less than \$50k per year. %We train the label classifier on the training set ($\mathcal{D}_1$) and the attribute classifier on the provided testing set ($\mathcal{D}_2$).  The dataset is divided into training and testing sets of around 32000 and 16000 samples respectively. 
We also considered the recent version of the Adult dataset (New Adult) for 2018 across different states in US~\citep{ding2021retiring}.  
For the Compas dataset, the task is to predict whether a defendant will recidivate within two years. The LSAC dataset contains admission records to law school. The task is to predict whether a candidate will pass the bar exam. The CelebA\footnote{http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html} dataset contains 40 facial attributes of humaine annotated images. We consider the task of predicting \textit{attractiveness} using gender as a sensitive attribute.  We randomly sample 20\% of the data to train the sensitive attribute classifier ($\mathcal{D}_2$). All the input features are used to train the attribute classifier except for the target variable. Table~\ref{tab:datasets} provides more details about each dataset and sensitive attributes used. 

\begin{table}%{r}{7.5cm}
    \centering
    \resizebox{12cm}{!}{
    \begin{tabular}{l l c l p{1.5cm} }
    \toprule
        Dataset & Size & \#features & Domain & Sensitive attribute \\
        \hline
        Adult Income & 48,842 & 15 & Finance &  Gender\\
        \midrule
        Compas & 6,000 & 11 & Criminal justice & Race \\
         \midrule
        LSAC & 20,798 & 12 & Education & Gender \\
         \midrule
        New Adult & 1.6M & 10 & Finance & Gender \\ \midrule
        CelebA & 202,600 & 40 & Image & Gender \\
         \bottomrule
    \end{tabular}
      }
    \caption{Datasets}
    \label{tab:datasets}
  
\end{table}
%\section{Using the Uncertainty of Sensitive Attributes to Train Fair Models without Fairness Constraints.} 
%\label{sec:fairness-unaware-models}
%In this set of experiments, we trained different classifiers (Logistic Region and Random Forest) without fairness constraints but using training data with higher uncertainty in the sensitive attributes. For different uncertainty thresholds $H \in \{0.0, 0.1, ..., 0.6\}$, we prune out samples whose uncertainty is lower than $H$ and train the model without fairness constraints using the remaining training data, i.e., $\{ (x,y) \in D_1 | u_x \geq H \}$, where $u_x$ is the estimated uncertainty provided by our method. In particular, when $H =0$, all the data points are used for the training, and in other cases, samples with uncertainty lower than $H$ are removed, and the model is trained on samples with more uncertain sensitive attributes. We train the model seven times for each uncertainty threshold with different seeds and report fairness and accuracy in the testing set, which contains sensitive attributes. Results in Table~\ref{tab:data-info} show that a model trained without fairness constraints tends to be fairer when the average uncertainty in predicting the sensitive attribute is high. This suggests that training a model on samples with uncertain sensitive attributes can yield fairer outcomes.  

%Figures ~\ref{fig:no_fairness_constraints}(a) and ~\ref{fig:no_fairness_constraints}(b) show the fairness and accuracy of the Logistic Regression and Random Forest classifiers, respectively. In the figures, each column represents the results on each dataset, and the first and the second rows provide the plots for fairness and accuracy, respectively.
%Across different datasets, the results show that unfairness decreases as the uncertainty threshold increases. We observe that the improvement in fairness is consistent for different fairness metrics considered, i.e., demographic parity, equal opportunity, and equalized odd.  We also observe a decrease in the accuracy, which is justified by reduced dataset size and, consequently, the tradeoff with fairness. On the LSAC dataset, fairness and accuracy remain almost constant as the average uncertainty in predicting the sensitive attribute on this dataset is 0.66, i.e., most of the samples already have the highest uncertainty. However, this method incurs a higher drop in accuracy and does not necessarily guarantee that an adversary cannot reconstruct the sensitive attributes from the trained model~\citep{ferry2023exploiting}.    

% Figure environment removed 

 
\section{Additional Results}

\subsection{Impact of the group-labeled ratio.}
\label{app:ablation-dataset-size}
In all the previous experiments, we considered the ratio of the group-labeled dataset (\Dtwo) as 20\% of the original dataset. In many real-world scenarios, data labeling is costly, especially given the rising concerns surrounding privacy. As these concerns intensify, fewer users may be inclined to disclose their sensitive attributes. It is, therefore, important to study the impact of the group-labeled ratio on the performance of our method. In this ablation study, we consider the attribute classifier trained with different ratios of \Dtwo, i.e., 3\%, 5\%, 10\%, 15\%, and 20\% of the original dataset. We experiment on the Adult, wherein 3\% represents around 1,465 data points among the 48,842 data points available.   

% Figure environment removed

Figure~\ref{fig:ablation-dataset-size} shows the Pareto front for different group-labeled ratios. Our findings remain consistent even in the most demographic scare regime. While we observe a slight drop in accuracy for smaller group-labeled ratios, the fairness performances are maintained. The Pareto fronts still dominate the model directly using the predicted sensitive attributes. These results suggest that our hypothesis holds even when very limited demographic information is available, as long as the dataset is predictive for sensitive attributes. On the other hand,  even if the dataset is not predictive enough for the sensitive attributes, our results show that training a model without fairness constraints will exhibit less disparities across the unknown demographic groups.  

 \subsection{Results on Nonlinear Classifier.}
 \label{app:additional-results-mlp}
 Table~\ref{tab:data-baseline-celebA} compares with other baselines on the CelebA dataset with logistic regression as the base classifier.  In the main paper, we compared existing methods using Logistic Regression as the base classifier.  We performed experiments with a more complex non-linear model to analyze its impact on the performance of different methods. We considered a Multi-Layer Perceptron (MLP) with one hidden layer of 32 units and Relu as the activation function for all the baselines. On the Adult dataset, Table~\ref{tab:data-baseline-mlp-adult} shows that when using a more complex model, our method (FairDSR (certain)) still provides Pareto dominants points in terms of fairness and accuracy compared to other baselines. At the same time, we observed an improvement in the accuracy of other methods due to the increased model capacity.   

\begin{table}[ht] 
  \def\arraystretch{1.1}
  \centering 
\begin{tabular}{l|llll} 
\hline
Method & Accuracy & $\Delta_{\text{DP}}$ &   $\Delta_{\text{EOP}}$  & $\Delta_{\text{EOD}}$ \\ \hline

\VanilaNoFairness     &  0.803  $\pm$  0.002     &  0.176 $\pm$  0.010 & 0.183 $\pm$  0.015 &  0.183 $\pm$ 0.015  \\
\VanilaFairness     & 0.782 $\pm$ 0.001  & 0.008 $\pm$ 0.005  &  0.018 $\pm$ 0.014  & 0.017 $\pm$   0.014 \\
\VanilaProxyFairness     & 0.800 $\pm$ 0.001  & 0.008 $\pm$ 0.036  &  0.011 $\pm$ 0.022  & 0.036 $\pm$   0.011
\\  \cline{1-5}  

%\FairRF  & $\pm$   &    $\pm$     &  $\pm$  &   $\pm$     \\

\FairDA  &  0.802 $\pm$ 0.002   & 0.155 $\pm$ 0.010 &  0.165 $\pm$ 0.018 &  0.165 $\pm$  0.018   \\
\CGL  &  0.782 $\pm$ 0.002   & 0.008 $\pm$ 0.005 &  0.019 $\pm$ 0.011 &  0.020 $\pm$  0.009   \\
\ARL      & \textbf{0.803}  $\pm$ 0.002  &  0.157 $\pm$ 0.010 &  0.157 $\pm$ 0.010 &  0.166 $\pm$ 0.016     \\
\CVarDRO    & 0.781 $\pm$ 0.002 & 0.155 $\pm$ 0.010 & 0.162 $\pm$ 0.016 &  0.162 $\pm$  0.016     \\
\KSMOTE  & 0.773 $\pm$ 0.008  & 0.020 $\pm$ 0.067 & 0.110 $\pm$ 0.082 & 0.144 $\pm$ 0.068   \\

\DRO     & 0.796 $\pm$ 0.006 & 0.142 $\pm$ 0.020 & 0.152 $\pm$ 0.020 & 0.129 $\pm$ 0.028   \\ \cline{1-5}
FairDSR (weighted) & 0.799 $\pm$ 0.004  &  0.038 $\pm$ 0.010 & 0.020 $\pm$ 0.012 & 0.033 $\pm$ 0.013\\

FairDSR (uncertain) & 0.782 $\pm$ 0.004  &  0.071 $\pm$ 0.046 & 0.107 $\pm$ 0.033 & 0.107 $\pm$ 0.033\\

FairDSR (certain) & 0.793 $\pm$ 0.000%8 
&  \textbf{0.003} $\pm$ 0.000%8 
& \textbf{0.001} $\pm$ 0.001 & \textbf{0.007} $\pm$ 0.002\\ 

\hline       
                                          
\end{tabular}  
\caption{Results on the CelebA dataset.} %\textit{ FairDSR (uncertain)} represents the variant of our approach where the model is trained without fairness constraints but using samples with higher uncertainty in the sensitive attribute predictions. And \textit{FairDSR (certain)} the variant where only samples with reliable sensitive attributes are used to train the label classifier with fairness constraints using the exponentiated gradient.
  \label{tab:data-baseline-celebA}
\end{table}


\begin{table*}[ht] 
  \def\arraystretch{1.1}
  \centering 
  %\fontsize{6.5pt}{6.5pt}\selectfont 
   %\resizebox{13cm}{!}{%
\begin{tabular}{l|llll} 
\hline
Method & Accuracy & $\Delta_{\text{DP}}$ &   $\Delta_{\text{EOP}}$  & $\Delta_{\text{EOD}}$ \\ \hline

\VanilaNoFairness     &    0.793 $\pm$ 0.007      &     0.014 $\pm$ 0.005 &  0.005 $\pm$ 0.005   &   0.049 $\pm$  0.026    \\
\VanilaFairness    &    0.796 $\pm$ 0.009      &     0.004 $\pm$ 0.004 &  0.002 $\pm$ 0.001   &   0.025 $\pm$  0.016    \\  
\VanilaProxyFairness   &    0.792 $\pm$ 0.009      &     0.018 $\pm$ 0.009 &  0.004 $\pm$ 0.001   &   0.040 $\pm$  0.005    \\\cline{1-5}  

\FairRF  &   0.753 $\pm$  0.120   &     0.021   $\pm$   0.013     &  0.016  $\pm$  0.017   &  0.044  $\pm$ 0.015    \\

\FairDA  &  0.716  $\pm$  0.210   &    \textbf{0.001}  $\pm$ 0.000%9
&  0.000%6  
$\pm$   0.005  &  	0.003  $\pm$  0.004    \\
\CGL  &  0.765  $\pm$  0.022   &    0.005  $\pm$ 0.004%9
&  0.003 $\pm$   0.001  &  	0.035  $\pm$  0.017    \\
\ARL      &   \textbf{0.807} $\pm$ 0.024       &     0.014 $\pm$ 0.015          &    0.009 $\pm$ 0.014     &  0.037 $\pm$ 0.022     \\
\CVarDRO    &    0.776  $\pm$ 0.052    &       0.024 $\pm$ 0.010       &   0.019 $\pm$ 0.014     &  0.045 $\pm$ 0.015      \\
                                         %  & \FairDA  &          &               &        &       \\
\KSMOTE  &   0.655 $\pm$ 0.055      &     0.022   $\pm$ 0.034      &   0.030 $\pm$ 0.022    &  0.060  $\pm$ 0.018     \\
\DRO  &   0.580 $\pm$ 0.220      &     0.023   $\pm$ 0.014      &   0.021 $\pm$ 0.017    &  0.038  $\pm$ 0.020     \\\cline{1-5}
FairDSR (weighted)    &  0.806 $\pm$ 0.001        &  0.008  $\pm$ 0.005        &   0.004 $\pm$ 0.002     &   0.035 $\pm$ 0.018  \\
FairDSR (uncertain)    &  0.794 $\pm$ 0.001        &  0.015  $\pm$ 0.002         &   0.006 $\pm$ 0.001     &   0.055 $\pm$ 0.000  \\
FairDSR (certain)    &  0.805 $\pm$ 0.001        &  \textbf{0.001}  $\pm$ 0.002         &   \textbf{0.000%5
} $\pm$ 0.001     &   \textbf{0.002} $\pm$ 0.000 
  \\
\hline       
                                          
\end{tabular}  
%}
\caption{Results on the LSAC dataset.}
  \label{tab:data-baseline-lsac}
\end{table*}

\begin{table}%[ht]
  \def\arraystretch{1.1}
  %\fontsize{6.5pt}{6.5pt}\selectfont 
  \centering
  %\resizebox{13.2cm}{!}{%
\begin{tabular}{l|llll} 
\hline
Method & Accuracy & $\Delta_{\text{DP}}$ &   $\Delta_{\text{EOP}}$  & $\Delta_{\text{EOD}}$ \\ \hline
\VanilaNoFairness   &    0.853 $\pm$ 0.004      &     0.183 $\pm$ 0.019 &  0.100 $\pm$ 0.025   &   0.102 $\pm$  0.023    \\ 
\VanilaFairness    &    0.801 $\pm$ 0.009      &    0.006 $\pm$  0.004        &  0.049 $\pm$ 0.011      &  0.017 $\pm$ 0.007     \\
\VanilaProxyFairness    &    0.795 $\pm$ 0.009      &    0.029 $\pm$  0.004        &  0.067 $\pm$ 0.020      &  0.042 $\pm$ 0.011     \\\cline{1-5} 
\FairRF  & \textbf{0.853} $\pm$ 0.002  &  0.164  $\pm$  0.009    &   0.077   $\pm$  0.026  &  0.091  $\pm$ 0.013 \\

\FairDA  & 0.813  $\pm$  0.014  &   0.118 $\pm$  0.023 &   0.091 $\pm$ 0.050  &  0.099 $\pm$ 0.037    \\

\CGL  & 0.800  $\pm$  0.014  &   0.009 $\pm$  0.002 &   0.017 $\pm$ 0.021  &  0.032 $\pm$ 0.010    \\

\ARL      &   0.851  $\pm$  0.003   &   0.166   $\pm$  0.015  & 0.087  $\pm$ 0.019    & 0.090 $\pm$  0.016  \\
\CVarDRO    &  0.850   $\pm$  0.003   &      0.183 $\pm$ 0.018 &  
0.095 $\pm$  0.027 &  0.101 $\pm$ 0.026 \\
                                          % & FairDA  &          &               &        &       \\
\KSMOTE  &  0.814 $\pm$ 0.020  &  0.201  $\pm$ 0.055    &  0.120 $\pm$ 0.021   & 0.130 $\pm$ 0.023   \\

\DRO   &  0.837  $\pm$  0.016   &   0.232  $\pm$ 0.057  &  0.110 $\pm$ 0.057   & 0.140 $\pm$  0.045   \\ \cline{1-5}
FairDSR (weighted)    & 0.849 $\pm$ 0.027  &   0.018 $\pm$ 0.010   & 0.061 $\pm$ 0.034 &  0.047 $\pm$ 0.032 \\
FairDSR (uncertain)    &  0.801 $\pm$ 0.027  &   0.110 $\pm$ 0.022   & 0.067 $\pm$ 0.027  &  0.059 $\pm$ 0.024 \\

FairDSR (certain)   &   0.818  $\pm$ 0.004   &    \textbf{0.009} $\pm$ 0.008     &  \textbf{0.028} $\pm$ 0.020  &  \textbf{0.027}  $\pm$ 0.017 \\  \hline 
                                          
\end{tabular} 
%}
\caption{Results on the Adult dataset using an MLP with a hidden layer of 34 units as base classifier. }%Bolded values represent the best-performing baselines among the fairness-enhancing methods without (full) demographic information.}  %All the baselines are trained on the dataset without the sensitive attributes ($D_1$). Each experiment is conducted 7 times and the fairness and accuracy are averaged. 
  \label{tab:data-baseline-mlp-adult}
\end{table} 


\subsection{Comparison with Other Baselines}
\label{app:comparison}




To assess the effect of the attribute classifier over the performances of downstream classifiers with fairness constraints w.r.t the proxy, we also performed extensive comparisons with different methods of obtaining the missing sensitive attributes: 

\begin{itemize}
    \item \textbf{Ground truth sensitive attribute}. We considered the case where the sensitive attribute is fully available and trained the model with fairness constraints w.r.t the ground truth. This represents the ideal situation where all the assumptions about the availability of demographic information are satisfied. This baseline is expected to achieve the best trade-offs.  %This baseline can therefore be considered as an oracle method, i.e. an upper-bound on the performance we might expect from methods that do not use the real sensitive attribute.
    \item \textbf{Proxy-KNN}. Here the missing sensitive attributes are handled by data imputation using the k-nearest neighbors (KNN) of samples with missing sensitive attributes. %Data imputation is a widely used technique for handling missing data. 
    \item \textbf{Proxy-DNN}. For this baseline, an MLP is trained on $\mathcal{D}_2$ to predict the sensitive attributes without uncertainty awareness. The network architecture and the hyperparameter are the same as for the student in our model. 

    \item \CGL~\citep{jung2022learning}: Similar to our setup, this method also uses an attribute classifier and replaces uncertain predictions with random sampling from the empirical conditional distribution of the sensitive attributes given the non-sensitive attributes $P(A|X, Y)$.
\end{itemize}


% Figure environment removed

%\paragraph{Fairness-accuracy trade-offs.} 
For fairness-enhancing mechanisms, we considered the Fairlean~\citep{bird2020fairlearn} implementation of the exponentiated gradient~\citep{agarwal2018reductions} and adversarial debiasing~\citep{zhang2018mitigating} (Section~\ref{sec:fair_metric}). We used various base classifiers for the exponentiated gradient, including logistic regression, random forest, and gradient-boosted trees. Random forest was initialized with a maximum depth of 5 and minimum sample leaf of 10, and default parameters were used for logistic regression without hyperparameter tuning. The same models and hyperparameters were used across all the datasets.  Adversarial debiasing works for demographic parity and equalized odds. The architecture of the classifier, the adversary, and other hyperparameters used is the same as recommended by the original paper ~\citep{zhang2018mitigating}.      
We evaluate the fairness-accuracy trade-off of every baseline by analyzing the accuracy achieved in different fairness regimes, i.e., by varying the parameter $\epsilon \in [0, 1]$, controlling the balance between fairness and accuracy. For a value of $\epsilon$  close to 0, the label classifier is enforced to achieve higher accuracy, while for a value close to $1$, it is encouraged to achieve lower unfairness. For each value of $\epsilon$, we trained each baseline 7 times on a random subset of $\mathcal{D}_1$ (70\%) using their predicted sensitive attributes, and the accuracy and fairness are measured on the remaining subset (30\%), where we assumed that the joint distribution $(X, Y, A)$ is available for fairness evaluation. The results are averaged, and the Pareto front is computed.    

Figure~\ref{fig:expgrad-rf} shows the Pareto front of the exponentiated gradient method on the Adult, Compas, and LSAC datasets using Random Forests as the base classifier. The figure shows the trade-off between fairness and accuracy for the different methods of inferring the missing sensitive attributes. From the results, we observe on all datasets and across all fairness metrics that data imputation can be an effective strategy for handling missing sensitive attributes, %the exponentiated gradient is robust to proxy-sensitive attributes
i.e., this fairness mechanism can efficiently improve the model's fairness with respect to the true sensitive attributes, although fairness constraints were enforced on proxy-sensitive attributes. However, we observe a difference in the fairness-accuracy trade-off for each attribute classifier. The KNN-based attribute classifier has the worst fairness-accuracy trade-off on all datasets and fairness metrics. This shows that assigning sensitive attributes based on the nearest neighbors does not produce sensitive attributes useful for achieving a trade-off close to the ground truth. While the DNN-based attribute classifier produces a better trade-off, it is still suboptimal compared to the ground truth-sensitive attributes. We observed similar results with different baseline models, such as logistic regression and gradient-booted trees, and for adversarial debiasing as the fairness mechanism. %The results are presented in the appendix~\ref{app:additional_results}.
In contrast, our method consistently achieves a better trade-off on all datasets and the fairness metrics considered. Similar results are obtained on the exponentiated gradient with logistic regression and gradient-boosted trees as base classifiers and adversarial debiasing~(see Section~\ref{sec:appendix}).  The uncertainty threshold's choice depends on the dataset's bias level, i.e., the level of information about the sensitive attribute encoded in the feature space.

%\footnotetext{\textsuperscript{1}Model trained without fairness constraints}
%\footnotetext{\textsuperscript{2}Model trained with fairness constraints w.r.t the ground truth sensitive attributes}

% Figure environment removed


%\section{Additional results}
%\label{app:additional_results}


Figures~\ref{fig:expgrad-lr}(d), ~\ref{fig:expgrad-rf}(d), and ~\ref{fig:expgrad-bgm}(d), depict the Pareto front of various baselines on the CelebA dataset. It shows that models trained with imputed sensitive attributes via KNN consistently achieve comparable tradeoffs to models trained with fairness constraints based on the true sensitive attribute. This could be explained by the fact that gender clusters are perfectly defined in the latent space. We observed that KNN-based imputation achieved 95\% accuracy in predicting gender. Conversely, the figures illustrate that our method outperforms baselines using ground truth-sensitive attributes and imputation methods, yielding more Pareto-dominant points. This highlights the advantages of applying fairness constraints to samples with low uncertainty in the sensitive attributes. Furthermore, Figure~\ref{fig:ablation-impact-uncertainty-gbm} (c) shows decreasing the uncertainty threshold further improves fairness while preserving the accuracy. We note the CelebA dataset can raise ethical concerns and is used only for evaluation purposes. For instance, predicting the attractiveness of a photo using other facial attributes as sensitive attributes can still harm individuals even if the model predicting attractiveness is not \textit{biased}.   

\paragraph{Exponentiated gradient with different baseline classifiers}
Figure~\ref{fig:expgrad-lr} and \ref{fig:expgrad-bgm} show fairness-accuracy trade-offs achieved by the exponentiated gradient with logistic regression and gradient-boosted trees, respectively. Similar to the results presented in the main paper, our method achieves better fairness-accuracy trade-offs.    


% Figure environment removed

Figure~\ref{fig:ablation-impact-uncertainty-gbm} shows the accuracy-fairness trade-off Exponentiated gradient using gradient-boosted trees as the base classifier for various uncertainty thresholds, the true sensitive attributes, and the predicted sensitive attributes with DNN. The results obtained are similar to random forests as the base classifier. The smaller uncertainty threshold produced the best trade-off in a high-bias regime like the Adult dataset. On datasets that do not encode much information about the sensitive attributes (most samples have high uncertainty), such as the New Adult and LSAC datasets, the accuracy decreases as the uncertainty threshold reduces while fairness is improved or maintained. On the LSAC dataset (Figure~\ref{fig:ablation-impact-uncertainty-gbm}(d)), we observe that increasing the uncertainty threshold results in a much higher drop in accuracy. This is explained by the high average uncertainty ($0.66$), and using a smaller threshold removes most of the data.  


% Figure environment removed

% Figure environment removed

 
 
\paragraph{Experiments with adversarial debaising}
Figure~\ref{fig:avd_debiasing} shows the trade-offs for adversarial debiasing. Our methods achieve a better trade-off on the Adult datasets, while for the Compas dataset, the ground-truth sensitive achieves a better trade-off. It is worth noting that adversarial debiasing is unstable to train.

% Figure environment removed  

\section{Uncertainty Estimation of Different Demographic Groups}
\label{app:uncertainty_disp}
In the main paper, we showed that when the dataset does not encode enough information about the sensitive attributes, the attribute classifier has, on average, greater uncertainty in the predictions of sensitive attributes. This encourages a choice of a higher uncertainty threshold to keep enough samples to maintain the accuracy, i.e., to prune out only the most uncertain samples. Figure~\ref{fig:representation_rate_samples} shows that the gap between demographic groups can increase as a smaller uncertainty threshold is used. This is explained by the fact that the model is more confident about samples from well-represented groups than samples from under-represented groups. While this gap between demographic groups can increase, our results show there are still enough samples from the disadvantaged group with reliable sensitive attributes. Thus, tuning the uncertainty threshold can result in a model that achieves a better trade-off between accuracy and various fairness metrics. Note that we observed the same trend for the LSAC dataset. The average uncertainty is 0.66, and the minimum uncertainty is 0.62. We also observed that group representation remains consistent (35\% difference) when using the average uncertainty value. 
% Figure environment removed



%\newpage
% ============== FROM REBUTAL
