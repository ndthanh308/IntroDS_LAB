\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
%\usepackage{neurips_2023}
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
%\usepackage[numbers, sort]{natbib}
\usepackage{graphicx}
\makeatletter
% make numeric styles use name format
\patchcmd{\NAT@test}{\else \NAT@nm}{\else \NAT@nmfmt{\NAT@nm}}{}{}

% define \citepos just like \citet
\DeclareRobustCommand\citepos
  {\begingroup
   \let\NAT@nmfmt\NAT@posfmt% ...except with a different name format
   \NAT@swafalse\let\NAT@ctype\z@\NAT@partrue
   \@ifstar{\NAT@fulltrue\NAT@citetp}{\NAT@fullfalse\NAT@citetp}}

\let\NAT@orig@nmfmt\NAT@nmfmt
\def\NAT@posfmt#1{\NAT@orig@nmfmt{#1's}}
\makeatother

\title{Fairness Under Demographic Scarce Regime}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

%\author{
%Patrik Joslin Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji\\
%\'Ecole de technologie sup\'erieure \'ETS\\
%Mila - Quebec Artificial Intelligence Institute
%}


\author{%
  Patrik Joslin Kenfack\\
 ÉTS Montréal, Mila\\ 
  %\texttt{patrik-joslin.kenfack-1@etsmtl.ens.ca} \\
  %\texttt{\{samira.ebrahimi-Kahou, ulrich.aivodji\}@etsmtl.ca} \\
  % examples of more authors
  \And
  Samira Ebrahimi Kahou\\
  ÉTS Montréal, Mila, CIFAR\\
   %Address \\
   %\texttt{samira.ebrahimi.Kahou@gmail.com} \\
\AND
 Ulrich Aïvodji\\
 ÉTS Montréal, Mila\\
  %Address \\
  %\texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\  
 % ETS, Mila\\
 % \texttt{ulrich.aivodji@etsmtl.ca}
  %\{\texttt{patrik-joslin.kenfack, Ulrich.Aivodji, Samira.Ebrahimi-Kahou}\}@etsmtl.ca École de technologie supérieure, Québec
}


\begin{document}


\maketitle


\begin{abstract}
  %The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  %both the left- and right-hand margins. Use 10~point type, with a vertical
  %spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  %%bold, and in point size 12. Two line spaces precede the abstract. The abstract
  %must be limited to one paragraph.
  Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as \textit{demographic scarce regime}. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (\textit{proxy}) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty.  We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detrimental to fairness and accuracy. Our experiments on two datasets showed that the proposed framework yields models with significantly better fairness-accuracy trade-offs compared to classic attribute classifiers. Surprisingly, our framework outperforms models trained with constraints on the true sensitive attributes.    

  %TLDR ==>> To achieve fairness in a limited demographic information regime, we propose a framework that leverages the uncertainty of proxy-sensitive attributes to yield models with a significantly better fairness-accuracy tradeoff.
\end{abstract}


\section{Introduction}
Mitigating machine learning bias against certain demographic groups becomes challenging when demographic information is wholly or partially missing. Demographic information can be missing for various reasons, e.g. due to legal restrictions, prohibiting the collection of sensitive information of individuals, or due to the disclosure of such information being voluntary. As people are more concerned about their privacy, reluctant users will not provide their sensitive information. As such, demographic information is available only for a few users. A \textit{demographic scarce} regime was the term used by~\citet{awasthi2021evaluating} to describe this particular setting. The data in this setting can be divided into two different sets $D_1$ and $D_2$. The dataset $D_1$ does not contain demographic information while $D_2$ contains both sensitive and non-sensitive information.  The goal is to train a classifier that is fair with respect to different (unobserved) demographic groups in $D_1$. Without demographic information in $D_1$, it is impossible to enforce group fairness notions such as \textit{statistical parity}~\citep{dwork2012fairness} and \textit{equalized odds}~\citep{hardt2016equality}. Algorithms to enforce these notions require access to sensitive attributes in order to quantify and mitigate the model's disparities across different groups~\citep{hardt2016equality, agarwal2018reductions, kenfack2021adversarial}. However, having access to another dataset where sensitive attributes are available gives room to train a sensitive attribute classifier that can serve as a \textit{proxy} for the missing ones. We are interested in understanding what level of fairness/accuracy one can achieve if proxy-sensitive attributes are used in replacement of the true sensitive attributes as well as properties of the sensitive attribute classifier and the data distribution that influence the fairness-accuracy trade-off.

In their study, \citet{awasthi2021evaluating} demonstrated a counter-intuitive finding: when using proxy-sensitive attributes, neither the highest accuracy nor an equal error rate of the sensitive attribute classifier has an impact on the accuracy of the bias estimation. Although~\citet{gupta2018proxy} showed that improving fairness for the \textit{proxy} demographic group can improve fairness with respect to the true demographic group, it remains unclear how existing fairness mechanisms would perform in the presence of proxy-sensitive attributes and what fairness-accuracy level they can achieve compared to the use of true sensitive attributes when the latter is not available. \textit{What is the optimal way for practitioners to design a sensitive attribute classifier and integrate them into existing fairness-enhancing methods in a way to achieve a better trade-off between accuracy and fairness?}. We aim to answer these questions and provide insights into the characteristics of the data distribution and the attribute classifier that can yield better performances in terms of fairness and accuracy. 

In this paper, we show that existing fairness-enhancing methods are robust to noise introduced in the sensitive attribute space by the proxy attribute classifier, i.e., there is no significant gap between fairness-accuracy trade-off achieved by fairness algorithms considered when proxy attributes are used in replacement to the sensitive attribute. We hypothesize that the uncertainty of the sensitive attribute classifier plays a critical role in improving fairness-accuracy tradeoffs on downstream tasks. We show that samples whose sensitive attribute values are predicted by the attribute classifier with high uncertainty are \textit{detrimental} to the fairness-accuracy trade-off on downstream tasks. As such, we show empirically that existing fairness-enhancing methods achieve better fairness-accuracy trade-offs when fairness constraints are enforced only on samples whose sensitive attribute values are predicted with low uncertainty.

To demonstrate this, we propose a framework that consists of two phases. During the first phases, we construct an uncertainty-aware deep neural network model (DNN) to predict demographic information. The uncertainty is measured and improved during the training using Monte Carlo dropout~\citep{gal2016dropout}. The first phase outputs for each data point, the predicted sensitive attribute, and the uncertainty of the prediction. During the second phase, the classifier for the target task is trained with fairness constrained w.r.t to predicted sensitive attributes. However, fairness constraints are imposed only on samples whose sensitive attribute values are predicted with low uncertainty. Our main contributions are summarized below:
\begin{itemize}
    \item We show that existing bias mitigation techniques are robust to noise introduced in the sensitive attribute space by imputation, i.e., when the sensitive attribute is missing for some samples, replacing them using imputation techniques based on a nearest neighbor or DNN model can still yield a reasonably fair model. However, the fairness-accuracy tradeoff achieved is worst compared to the model trained with the true sensitive attributes.     
    \item We propose a framework that demonstrates that accounting for the uncertainty of sensitive attribute predictions can play an important role in achieving better accuracy-fairness trade-offs. We hypothesize that a better fairness-accuracy trade-off can be achieved when fairness constraints are imposed on samples whose sensitive attribute values are predicted with high confidence by a DNN.  
    \item We perform experiments on a wide range of real-world datasets to demonstrate the effectiveness of the proposed framework. In essence, our results showed that the proposed method even significantly outperforms a model trained with fairness constraints on observed sensitive attributes. This suggests that applying our method in settings where demographic information is fully available can yield better fairness-accuracy trade-offs.
\end{itemize}

%Please read the instructions below carefully and follow them faithfully. \textbf{Important:} This year the checklist will be submitted separately from the main paper in OpenReview, please review it well ahead of the submission deadline: \url{https://neurips.cc/public/guides/PaperChecklist}.

\section{Related Work}
Various metrics have been proposed in the literature to measure unfairness in classification, as well as numerous methods to enforce fairness as per these metrics. The most popular fairness metrics include demographic parity~\citep{dwork2012fairness}, equalized odds, and equal opportunity~\citep{hardt2016equality}. Demographic parity enforces the models' positive outcome to be independent of the sensitive attributes, while equalized odds aim at equalizing models' true positive and false positive rates across different demographic groups. Fairness-enhancing methods are categorized into three groups: pre-processing~\citep{zemel2013learning, kamiran2012data}, in-processing~\citep{agarwal2018reductions, zhang2018mitigating}, and post-processing~\citep{hardt2016equality}, depending on whether the fairness constraint is enforced before, during, or after model training respectively. However, enforcing these fairness notions often requires access to demographic information. There are fairness notions that do not require demographic information to be achieved, such as the \textit{Rawlsian Max-Min} fairness notion~\citep{rawls2020theory} which aims at maximizing the utility of the worst-case (unknown) group~\citep{hashimoto2018fairness, lahoti2020fairness,liu2021just}. Specifically, these methods focus on maximizing the accuracy of the unknown worst-case group. However, they often fall short in effectively targeting the specific disadvantaged demographic groups or improving group fairness metrics~\citep{franke2021rawls, lahoti2020fairness}. In contrast, we are interested in achieving group fairness notions via proxy using limited demographic information.     
Recent efforts have explored bias mitigation when demographic information is noisy~\citep{wang2020robust, chen2022fair}. Noise can be introduced in the sensitive feature space due to human annotation, privacy mechanisms, or inference. \citet{chen2022fair} aims to correct the noise in the sensitive attribute space before using them in fairness-enhancing algorithms.             

The most related work includes methods relying on proxy-sensitive attributes to enforce fairness when demographic information is partially available. \citet{coston2019fair} assumed sensitive attribute is available either in a source domain or the target domain, and use domain adaptation-like techniques to enforce fairness in the domain with missing sensitive attributes. \citet{diana2022multiaccurate} showed that training a model to predict the sensitive attributes can serve as a good substitute for the ground truth sensitive attributes when the latter is missing. \citet{awasthi2021evaluating} showed that one can leverage samples with sensitive attribute values to create a sensitive attribute predictor that can then infer the missing sensitive attribute values. They then proposed an active sampling approach to improve bias assessment when predicted sensitive attributes are used. \citet{gupta2018proxy} used non-protected features to infer proxy demographic information in replacement to the unobserved real ones. They showed empirically that enforcing fairness with respect to proxy groups generalizes well to the real protected groups and can be effective in practice. While they focus on post-processing techniques, we are interested in in-processing methods. 

Related work relying on proxy-sensitive attributes mostly focuses on assessing what level of fairness can be achieved when proxy-sensitive attributes are used~\citep{coston2019fair}, properties of the sensitive attribute classifier~\citep{diana2022multiaccurate, coston2019fair}, and bias assessment via proxy sensitive features~\citep{awasthi2021evaluating}.     
Our proposed method focuses on reducing accuracy-fairness trade-offs yield by models using proxy attributes in replacement to true sensitive attributes.

\section{Problem Setting and Preliminaries}
\paragraph{Problem formulation} We consider the dataset $\mathcal{D}_1=\{\mathcal{X}, \mathcal{Y}\}$ where $\mathcal{X} = \{x_i\}_{i=1}^{M}$ represents the non-sensitive input feature space and $\mathcal{Y}=\{0, 1\}$ represents the target variable. The goal is to build a classifier, $f:\mathcal{X} \rightarrow \mathcal{Y}$, that can predict $\mathcal{Y}$ while ensuring fair outcomes for samples from different demographic groups. However, demographic information of samples in $\mathcal{D}_1$ is unknown. Furthermore, we assume the existance of another dataset $\mathcal{D}_2=\{\mathcal{X}, \mathcal{A}\}$ sharing the same input feature space as $\mathcal{D}_1$ and for which demographic information is available, i.e., $\mathcal{A}=\{0, 1\}$. We assume binary demographic groups for simplicity. Therefore, the dataset $\mathcal{D}_1$ contains label information and  $\mathcal{D}_2$ contains demographic information. Our goal is to leverage $\mathcal{D}_2$ to train an attribute classifier $g:\mathcal{X} \rightarrow \mathcal{A}$ that can serve as a proxy to the sensitive attributes for samples in  $\mathcal{D}_1$, for which a fairness metric can be enforced in a way to improve fairness with respect to the true sensitive attributes.  However, to be able to estimate bias in label classifier $f$, we assume there exists a small set of samples drawn from the joint distribution $\mathcal{X} \times \mathcal{Y} \times \mathcal{A}$, i.e., samples that jointly have label and demographic information. If this subset is not available, one can consider using the active sampling technique proposed by \citet{awasthi2021evaluating} in order to approximate bias with respect to the predicted sensitive attributes. However, this estimation is beyond the scope of this work. Our goal is to effectively assess the level of fairness our method can achieve without being overly concerned about potential bias overestimation or underestimation.

Reducing the trade-off between fairness and accuracy is a significant challenge within the fair machine-learning community~\citep{dutta2020there}. Our primary goal is to design a method that effectively leverages proxy features to achieve similar or better fairness-accuracy trade-offs compared to settings where the true sensitive attributes are available. To this end, we considered a different range of fairness metrics along with various (in-processing) fairness-enhancing techniques.    

\subsection{Fairness Metrics}
\label{sec:fair_metric}
In this work, we consider three popular group fairness metrics: demographic parity, equalized odds, and equal opportunity. These metrics aim to equalize different models' performances across different demographic groups. Samples belong to the same demographic group if they share the same demographic information, $A$, e.g., gender and race.    

\subsubsection{Demographic Parity}
Also known as statistical parity, this measure requires that the positive prediction ($f(X) = 1$) of the model be the same regardless of the demographic group to which the user belongs~\citep{dwork2012fairness}. More formally the classifier $f$ achieves demographic parity if $P(f(X) = 1 | A=a) = P(f(X)=1)$
In other words, the outcome of the model should be independent of sensitive attributes. In practice, this metric is measured as follows:
 \begin{equation}
\label{eq:dp}
    \Delta_{ \text{DP} } (f) = \left| \underset{x|A=0}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] - \underset{x|A=1}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] \right|
\end{equation}
Where $\mathbb{I}(\cdot)$ is the indicator function.
\subsubsection{Equalized Odds}
This metric enforces the True Positive Rate (TPR) and False Positive Rate (FPR) for different demographic groups to be the same $ P(f(X) = 1 | A=0, Y=y) = P(f(X) = 1 | A=1, Y=y), :; \forall y \in \{0,1\}$.  The metric is measured as follows:

\begin{equation}
\label{eq:eod}
    \Delta_{\text{EOD}} = \alpha_0 + \alpha_1 
\end{equation}

Where,

\begin{equation}
\label{eq:alph_}
    \alpha_j =  \left| \underset{x|A=0, Y=j}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] - \underset{x|A=1, Y=j}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] \right|
\end{equation}

\subsubsection{Equal Opportunity}
In certain situations, bias in positive outcomes can be more harmful. Therefore, Equal Opportunity metric enforces the same TPR across demographic~\citep{hardt2016equality} and is measured using $\alpha_1$ (Eq.~\ref{eq:alph_}).

%\begin{equation}
%\label{eq:eop}
%    \Delta_{\text{EOP}} =  \left| \underset{x|A=0, Y=1}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] %- \underset{x|A=1, Y=1}{\mathbb{E}}[\mathbb{I}\{ f(x) = 1 \}] \right|
%\end{equation}

\subsection{Fairness Mechanism}
\label{sec:fair_mechanism}
In this work, we focus on in-processing techniques to improve the models' fairness. These methods introduce constraints in the classification problem in order to satisfy a given fairness metric. Our study focuses on state-of-the-art techniques in this category, i.e., exponential gradient~\citep{agarwal2018reductions} and adversarial debiasing~\citep{zhang2018mitigating}. We considered these methods as they allow control over fairness and accuracy. In general, the optimization problem contains a parameter $\lambda$ that controls the balance between fairness and accuracy, i.e., a higher value of $\lambda$ would force the model to achieve higher fairness (respectively lower accuracy) while a smaller yields higher accuracy (respectively lower fairness score). Our goal is to design a sensitive attribute predictor that achieves a better fairness-accuracy trade-off, i.e., for the same value of $\lambda$ build a model that provides higher accuracy and lower unfairness compared to other baselines.  
 
  

%\subsection{Characteristic of the Attribute Classifier}

\section{Methodology}
In this section, we present the methodology and all components involved in our proposed method. Figure~\ref{fig:overall-model} presents an overview of the stages in our framework and the interactions between and within each stage. The first stage consists of \textit{training the attribute classifier} and outputs for each sample with missing sensitive attribute, its predicted sensitive attribute (proxy) along with the uncertainty of the prediction. In the second step, \textit{the label classifier is trained with fairness constrained enforced using the predicted sensitive attributes}. Fairness is enforced only on samples with the lowest uncertainty in the sensitive attribute prediction, i.e., samples with an uncertainty lower than a predefined uncertainty threshold $H$.
% Figure environment removed

\subsection{Uncertainty-Aware Attribute Prediction}
We build the sensitive attribute classifier using a student-teacher framework in a semi-supervised learning approach similar to~\citet{yu2019uncertainty, lainetemporal}, which accounts for the uncertainty of the predictions of samples with missing sensitive attributes.

\paragraph{Student model} The student model is implemented as a neural network and is trained on $D_2$ (samples with sensitive attributes) to predict sensitive attributes. The attribute classifier is optimized to minimize a double loss function: the \textit{classification loss} ($\mathcal{L}_s$), i.e., the cross-entropy loss, and the \textit{consistency loss} ($\mathcal{L}_c$)~\citep{yu2019uncertainty}. The consistency loss (or unsupervised loss) enforces the student model to rely mostly on samples with confident sensitive attributes guided by the uncertainty estimation from the teacher model.
This loss is defined as the mean squared difference between the outputs (logits) of the student and the teacher on samples for which the uncertainty does not exceed a predefined threshold $R$. The motivation behind the consistency loss is the focus on the primary goal of the attribute classifier, which is to find the missing sensitive attributes in $D_1$ with high confidence. 

Overall, the attribute classifier is trained to minimize the following loss: 

\begin{equation}
    \label{eq:loss}
    \min_{f \in \mathcal{F}} \; \underset{{(x,a) \sim \mathcal{D}_2 \times \mathcal{A}}}{\mathbb{E}} \; \mathcal{L}_s (f(x), a) + \lambda \underset{x\sim \mathcal{D}_1 + \mathcal{D}_2}{\mathbb{E}} \; \mathcal{L}_c(f(x), h(x)) 
\end{equation}

where $f(\cdot)$ is the student model, $h(\cdot)$ the teacher model, and $\lambda$ a parameter controlling the consistency loss. The empirical loss minimized is defined by the following equations for classification ($\mathcal{L}_{s}$) and consistency loss $\mathcal{L}_{c}$:   
\begin{equation} 
\mathcal{L}_{s} = \frac{1}{|D_2|} \sum_{x, a \in D_2, A} a \cdot \mathrm{log}(f(x)) + (1-a) \cdot \mathrm{log}(1-f(x)) 
\end{equation}
\begin{equation} 
\mathcal{L}_{c} = \frac{1}{|D_2|+|D_1|} \sum_{x | u_{x} \leq R} \parallel f(x) - h(x) \parallel^2
\end{equation}

The consistency loss is applied only on samples, $x$, whose uncertainty, $u_{x}$, is lower than the predefined threshold $R$. Following~\citet{srivastava2014dropout, baldi2013understanding}, $R$ and $\lambda$ are updated using a Gaussian warmup function to avoid the model diverging from the beginning of the training.

\paragraph{Teacher model} The teacher model is implemented using the same network architecture as the student, and is used for uncertainty estimation. The teacher weights are updated in training epoch $t$ with the exponential moving average (EMA) of student weights: 
\begin{equation}
    \omega_t = \alpha \omega_{t-1} + (1-\alpha)\theta,
\end{equation}
where $\theta$ and $\omega$ denote the respective weights of student and teacher and $\alpha$ controls the moving decay.  
The use of EMA to update the teacher model is motivated by previous studies~\citep{lainetemporal, yu2019uncertainty} that have shown that averaging model parameters at different training epochs can provide better predictions than using the most recent model weights in the last epoch.  
The teacher model gets as input both samples with or without sensitive attributes, i.e. samples from both $D_1$ and $D_2$, and computes the uncertainty of their predictions using Monte Carlo (MC) dropout~\citep{gal2016dropout}. As such, both the student and teacher networks have dropout layers between hidden layers of the network. 

MC dropout is an approximation of a Bayesian neural network widely used to interpret the parameters of neural networks~\citep{abdar2021review}. MC dropout uses dropout at test time in order to compute prediction uncertainty from different sub-networks that can be derived from the whole original neural network.  Dropout is a regularization technique used in DNNs. During training, the dropout layer randomly removes a unit with probability $p$. Therefore, each forward and backpropagation pass is done on a different model (sub-network) forming an ensemble of models that are aggregated together to form a final model with lower variance~\citep{srivastava2014dropout, baldi2013understanding}. The uncertainty of each sample is computed using $T$ stochastic forward passes on the teacher model to output $T$ independent and identically distributed predictions, i.e., $\{h_1(x),h_2(x), \cdots, h_T(x) \}$. The softmax probability of the output set is calculated and the uncertainty of the prediction ($u_x$) is quantified using the resulting entropy: $u_x = -\sum_a p_a(x) \log (p_a(x))$, where $p_a(x)$ is the probability that sample $x$ belongs to demographic group $a$ estimated over $T$ stochastic forward passes, i.e., $p_a(x) = \frac{1}{T}\sum_{t=1}^{T} h_t(x)$.

\subsection{Enforcing Fairness w.r.t Reliable Proxy Sensitive Attributes} 
After the first phase, the attribute classifier can produce for every sample in $D_1$, i.e., samples with missing sensitive attributes, their predicted sensitive attribute (proxy)~$\hat{A}=\{h(x_i)_{x_i \in D_1}\}$, and the uncertainty of the prediction~$U = \{u_{x_i}\}_{x_i \in D_1}$. To validate our hypothesis, we define a confidence threshold $H$ for samples used to train the label classifier with fairness constraints, i.e., the label classifier with fairness constraints is trained on a subset $D_1' \subset D_1$ defined as follows:  
\begin{equation}
    D_1' = \{(x, y, f(x)) | u_x \leq H \}
\end{equation}

The hypothesis of enforcing fairness on samples whose sensitive attributes are reliably predicted stems from the fact that the model is confidently able to distinguish these samples based on their sensitive attributes in the latent space. In contrast, the label classifier is inherently fairer if an attribute classifier cannot reliably predict sensitive attributes from training data~\citep{kenfacklearning}. We further support this in section~\ref{sec:results} by comparing the new Adult dataset~\citep{ding2021retiring} and the old version of the dataset~\citep{asuncion2007uci}. 
Therefore, enforcing fairness constraints on samples with the most reliable proxy attributes would be more useful in achieving better accuracy-fairness trade-offs than considering samples for which the sensitive attributes are not distinguishable in the latent space. The fairness constraints on samples with unreliable sensitive attributes could push the model's decision boundary in ways that penalize accuracy. We support these arguments in the experiments.

 
\section{Experiments}
\label{sec:experiements}
In this section, we demonstrate the effectiveness of our framework on several datasets and compared to different baselines. The source code is available at \href{https://github.com/patrikken/fair-dsr}{https://github.com/patrikken/fair-dsr}.
\subsection{Experimental Setup}

\paragraph{Datasets} We validate our method on two real-world benchmarks widely used for bias assessment: Adult Income~\citep{asuncion2007uci} and Compas datasets~\citep{compas}. The Adult dataset contains 14 features and the goal is to predict if an individual's annual income is greater or less than \$50k per year. The dataset comes divided into training and test set of around 36000 and 16000 samples respectively\footnote{https://archive.ics.uci.edu/ml/datasets/Adult}. We train the label classifier on the training set ($D_1$) and the sensitive attributes on the testing set ($D_2$). We used gender as the sensitive attribute, which was categorized in the dataset as Male or Female. We also considered the recent version of the Adult dataset (New Adult) for the year 2018 across different states in US~\citep{ding2021retiring}. The New Adult dataset contains around 1.6M  samples described with 10 features. The Compas dataset contains around 6,000 samples described with 10 features. The goal is to predict whether a defendant will recidivate within two years. We use race as the sensitive attribute and 20\% of the samples are randomly sampled to train the sensitive attribute classifier ($D_2$). For all the datasets, all the features are used to train the attribute classifier except for the target variable.

%\paragraph{Metrics}   

\paragraph{Attribute classifier.}  The student and teacher models were implemented as feed-forward Multi-layer Perceptrons (MLPs) with Pytorch~\citep{paszke2019pytorch}, and the loss function~\ref{eq:loss} is minimized using the Adam optimizer with learning rate $0.001$ and batch size $256$. Following~\citet{yu2019uncertainty, lainetemporal}, we used $\alpha = 0.99$ for the EMA parameter for updating the teacher weights with the student's weights across epochs.    
 
\paragraph{Baselines}
For fairness-enhancing mechanisms, we considered the Fairlean~\citep{bird2020fairlearn} implementation of the exponential gradient~\citep{agarwal2018reductions} and adversarial debiasing~\citep{zhang2018mitigating} (Section~\ref{sec:fair_mechanism}). For the exponential gradient, we used various base classifiers including logistic regression, random forest, and gradient-boosted trees~\citep{ding2021retiring}. Random forest was initialized with maximum depth 5 and minimum samples leaf 10, and default parameters were used for logistic regression without hyperparameter tuning. The same models and hyperparameters were used across all the datasets.  Adversarial debiasing works for demographic parity and equalized odds. The architecture of the classifier, the adversary, as well as other parameters used are the same as recommended by the original paper~\citep{zhang2018mitigating}.  

To assess the effect of the attribute classifier over the performances of downstream classifiers with fairness constraints w.r.t the proxy, we considered different methods of obtaining the missing sensitive attributes as baselines. 

\begin{itemize}
    \item \textbf{Ground truth sensitive attribute}. We considered the case where the sensitive attribute is fully available and trained the model with fairness constraints w.r.t the ground truth. This represents the ideal situation where all the assumptions about the availability of demographic information are satisfied.  This baseline can therefore be considered as an oracle method, i.e. an  upper-bound on the performance we might expect from methods that do not use the real sensitive attribute.
    \item \textbf{Proxy-KNN}. Here the missing sensitive attributes are derived using the k-nearest neighbors (KNN) of samples with missing sensitive attributes. Data imputation is a widely used technique for handling missing data. 
    \item \textbf{Proxy-DNN}. For this baseline, an MLP is trained on $D_2$ to predict the sensitive attributes without uncertainty awareness. The network architecture used and the hyperparameter is the same as for the student in our model.  
\end{itemize}
We compare these baselines to our method where only samples with reliable sensitive attributes are used to train the label classifier with fairness constraints. The uncertainty threshold that defines the reliability is tuned over the interval $[0.1, 0.7]$. For comparison, in addition to the accuracy, we consider the three fairness metrics described in Section~\ref{sec:fair_metric}, i.e., equalized odds ($\Delta_{EOD}$), equal opportunity ($\Delta_{EOP}$), and demographic parity ($\Delta_{DP}$).

\subsection{Results and Discussion}
\label{sec:results}
% Figure environment removed

% Figure environment removed

We evaluate the fairness-accuracy trade-off of every baseline by analyzing the accuracy achieved in different fairness regimes, i.e., by varying the parameter $\epsilon \in [0, 1]$ controlling the balance between fairness and accuracy. For a value of $\epsilon$  close to 0, the label classifier is enforced to achieve higher accuracy while for a value close to $1$ it is encouraged to achieve lower unfairness. For each value of $\epsilon$, we trained each baseline 7 times on a random subset of $D_1$ (70\%) using their predicted sensitive attributes, and the accuracy and fairness are measured on the remaining subset (30\%), where we assumed that the joint distribution $(X, Y, A)$ is available for fairness evaluation. The results are averaged and the Pareto front is computed.     
Figure~\ref{fig:expgrad-rf} shows the Pareto front of the exponential gradient method on the Adult and Compas datasets using Random Forests as the base classifier. The figure shows the trade-off between fairness and accuracy of the different methods of inferring the missing sensitive attributes. From the results, we observe on both datasets and across all fairness metrics that the exponential gradient is robust to proxy-sensitive attributes, i.e., this fairness mechanism can efficiently improve the fairness of the model with respect to the true sensitive attributes although fairness constraints were enforced on proxy-sensitive attributes. However, we observe a difference in the fairness-accuracy trade-off for each attribute classifier.

Overall, the KNN-based attribute classifier has the worst fairness-accuracy trade-off on all datasets and fairness metrics. This shows that assigning sensitive attributes based on the nearest neighbors does not produce sensitive attributes useful for achieving a trade-off close to the ground truth. While the DNN-based attribute classifier produces a better trade-off but is still worse than the ground truth sensitive attributes.

In contrast, we see that our method consistently achieves a better trade-off on the Adult and Compas datasets across all the fairness metrics considered. Similar results are obtained on the exponential gradient with logistic regression and gradient-boosted trees as base classifiers and with adversarial debiasing~(see Section~\ref{sec:appendix}). The uncertainty threshold that achieved these results are $0.3$ and $0.6$ for the Adult and the Compas datasets, respectively. The choice of the uncertainty threshold depends on the level of bias in the dataset, i.e. the level of information about the sensitive attribute encoded in feature space. As shown in table~\ref{tab:data-info} the average uncertainty of the sensitive attribute prediction is lower on the Adult dataset compared to the Compas and the New adult datasets, $0.15$, $0.38$, $0.42$, respectively. The table also shows different fairness measures of a logistic regression model trained without fairness constrained on the unlabelled datasets. These results show that there is a correlation between the uncertainty of the sensitive attribute prediction and the fairness of the model, i.e. the least biased dataset (New Adult) has the highest uncertainty and lowest accuracy in the attribute prediction.

Figure~\ref{fig:ablation-impact-uncertainty} showcases the impact of the uncertainty threshold on the fairness-accuracy threshold. When the dataset encodes much information about the sensitive attribute as in the Adult dataset (Figure~\ref{fig:ablation-impact-uncertainty}a) with 85\% accuracy of predicting the sensitive attributes, results show that the more we enforce fairness w.r.t. samples with lowest uncertainty, the better the fairness-accuracy trade-offs. In this regime, enforcing unfairness helps the model in maintaining a better accuracy level. In contrast, in a low bias regime, i.e. when the feature space does not encode enough information about the sensitive attributes such as on the Compas and the New Adult dataset, the model achieves better fairness-accuracy trade-offs when a higher uncertainty threshold is used. In this regime, most of the samples have higher uncertainty in the sensitive attribute prediction (see Table~\ref{tab:data-info}), as such, Figure~\ref{fig:ablation-impact-uncertainty}b and Figure~\ref{fig:ablation-impact-uncertainty}c show that using a lower uncertainty threshold leads to a decrease in accuracy while a certain level of fairness is maintained. The drop in accuracy is due to the fact that more and more samples were pruned out from the datasets, and this suggests that most of the samples were more useful for the target task. 

In the appendix, we show that under-represented demographic groups can have higher uncertainty on average than well-represented groups and the uncertainty threshold should be carefully tuned in order to achieve a better trade-off, in particular in datasets with low bias.   

\begin{table}
  \caption{Average uncertainty and accuracy of the attribute classifier on the dataset with missing sensitive attributes.}
  \label{tab:data-info}
  \centering
  \begin{tabular}{lccccc}
    \toprule 
    Dataset     & Mean uncertainty ($\downarrow$)     & Accuracy sensitive attribute ($\uparrow$) & $\Delta_{DP}$ & $\Delta_{EOD}$ & $\Delta_{EOP}$\\
    \midrule 
    Adult     & 0.15 & 85\% & 0.18 &  0.20 & 0.13 \\
    Compas    & 0.39  & 72\% & 0.32 & 0.55 & 0.34\\
    New Adult     & 0.42  & 68\% & 0.06 & 0.05 & 0.04\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Conclusion}
In this work, we introduced a framework to improve the fairness-accuracy trade-off when only limited demographic information is available. Our method introduces uncertainty awareness in the sensitive attributes classifier. %We showed that uncertainty in the attribute classifier plays an important role in the fairness accuracy achieved in the downstream model with fairness constraints. 
We showed that enforcing fairness on samples whose sensitive attributes are predicted with low uncertainty can yield models with better fairness-accuracy trade-offs. Our method consistently achieved a better trade-off than other classic attribute classifiers and in most cases even better trade-offs than the use of the true sensitive attributes. However, in a low-bias regime, most samples have uncertain sensitive attributes. In future work, we plan to introduce weighted empirical risk minimization in the fairness-enhancing model where the samples' weights are defined based on the uncertainty of the attribute classifier.

\begin{ack}
 The authors thank the Digital Research Alliance of Canada for compute resources. SEK is supported by CIFAR and NSERC DG (2021-4086) and UA by NSERC DG (2022-04006).
\end{ack}

%\section*{References}
\bibliographystyle{apalike}
\bibliography{main}

\appendix
\input{suplementary_material}
\end{document}