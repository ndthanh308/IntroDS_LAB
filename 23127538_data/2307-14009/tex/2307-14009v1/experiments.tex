\section{EXPERIMENTS}

We construct our datasets \textbf{CarPatch3D} using three different sources: the KITTI multi-object tracking (KITTI-MOT) and object detection (KITTI-DET) tracks \cite{kitti}, and the DVM-Cars dataset \cite{dvm}. The characteristics of each dataset are shown in Table \ref{tab:dataset}. Our work addresses three challenging tasks: single-view supervision for zero-shot learning, multi-view supervision for novel scene synthesis, and patch reconstruction with test-time optimization. Furthermore, we demonstrate Car-Studio's ability to perform controllable scene editing. 

\textbf{Baselines}. We evaluate our proposed method against the following baselines: CodeNeRF \cite{codenerf}, PixelNeRF \cite{PixelNeRF}, and AutoRF \cite{autorf}. Note that CodeNeRF does not support zero-shot learning, and therefore, we only use it for the multi-view supervision few-shot learning task. Moreover, to ensure a fair comparison since our problem is category-specific, we convert PixelNeRF to the canonical coordinate system.

\textbf{Test-time Optimization}. For the auto-decoder model \cite{codenerf}, we optimize the latent vectors directly in the test dataset. However, for the encoder-decoder model \cite{PixelNeRF, autorf} and our model, we first obtain the latent vectors using the encoder and then perform joint optimization of the latent vectors and decoder during the test-time optimization stage.

\textbf{Runtime Environment Details}.  All experiments are conducted on an Nvidia RTX 3090 Ti graphics card. We optimize all NeRF models for 500,000 steps with a pixel sampler with a batch size of 3,072, which takes approximately two days to complete. Our pipeline is built upon NerfStudio \cite{nerfstudio}. We also reproduce the CodeNeRF \cite{codenerf}, Canonical PixelNeRF-ResNet\cite{PixelNeRF}, Canonical PixelNeRF-MLP with an MLP backbone, and AutoRF\cite{autorf} based on a community implementation\cite{autorf-pytorch} for reusing the data loader. We use the RAdam optimizer \cite{radam} with hyperparameters $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\textup{lr} = 10^{-3}$, and $\epsilon = 10^{-8}$. We use the exponential decay scheduler with $\textup{lr}_{\textup{final}} = 10^{-4}$ and $\textup{step}_{\max} = 200,000$. All other configurations follow the original setups.

\textbf{Evaluation and Metrics}. We evaluate the models on novel view synthesis. In Figure \ref{fig:single-view}, suppose we have two images of the same car instance captured at different poses $p_1$ and $p_2$. If we feed the image captured at $p_1$ to the encoder to obtain the latents, we can then render the image at the view $p_1$ to compute the loss between the rendered image and the ground truth image captured at $p_1$.This approach is called single-view supervision because it relies solely on information from a single view.If we were to use the image captured at $p_1$ and compute the loss using the rendered image at $p_2$, it would be considered multi-view supervision. The quantitative results are PSNR, SSIM \cite{ssim}, and LPIPS \cite{lpips}.

\subsection{Single-view Supervision for Zero-shot Learning} \label{sec:single-view}
\input{Figures/3_single_view}
\input{Figures/4_zero_shot_qualitative}
\input{Tables/single_view_zero_shot}
In Figure \ref{fig:zero_shot_qualitative}, we present qualitative comparisons with the baselines. In this experiment, all testing instances are unseen in the training set, and no test-time optimization is performed. The experiment uses a uniform 9:1 train-test split on all valid KITTI-DET-derived patches. We observe that the default architecture of PixelNeRF-ResNet degenerates to an empty output. Incorporating the segmentation loss $\mathcal{L}_s$ into our approach results in sharper contours compared to the baselines. Additionally, our architecture design reduces the presence of floaters around the target car.

Table \ref{tab:single_view_zero_shot} presents the quantitative results of single-view supervision zero-shot novel view synthesis. PixelNeRF \cite{PixelNeRF} uses a ResNet-based backbone by default. However, during the training process, PixelNeRF-ResNet encountered degradation. As a result, we also compare the performance of an MLP-based version of PixelNeRF. The results show that Car-NeRF with mask segmentation loss achieves the best SSIM and LPIPS performance, while Car-NeRF without mask segmentation loss performs best in terms of PSNR. 

The quantitative results suggest that incorporating mask segmentation loss can improve the quality of rendered images in terms of structural similarity and perceptual quality. This finding is consistent with the intuition of using mask segmentation to guide the training of neural rendering models. However, it is important to note that Car-NeRF without mask segmentation loss performs better in terms of peak signal-to-noise ratio. This indicates that relying solely on PSNR may not provide a comprehensive evaluation of image quality. Therefore, it may be necessary to consider various metrics, such as structural similarity and perceptual quality, in addition to PSNR, to obtain a more complete assessment of the performance of neural rendering models.

\subsection{Muti-view Supervision Novel View Synthesis}

\input{Tables/multi_view} 

In this experiment, we compared our method with other approaches to assess the effectiveness of single-view supervision versus multi-view supervision. While the baselines in Section \ref{sec:single-view} can obtain latent vectors by inputting an arbitrary image to the encoder, the auto-decoder architecture does not have this ability. To provide a fair comparison with these approaches, we use a more comprehensive dataset with instance labels across frames, which requires additional labor but provides more ideal evaluation conditions for multi-view supervision approaches. 

We train the models on patches derived from the KITTI-MOT dataset, using the cross-frame instance label as the ground truth for multi-view supervision. To ensure a fair comparison, we only consider instances that have multiple images available for training. This is because CodeNeRF \cite{codenerf} uses global latent vectors as input, and in the case where an instance is not seen in the training set, using an average initialization without any instance-specific information may introduce potential unfairness to CodeNeRF.
Similar to the experiment in Section \ref{sec:single-view}, we do not apply any test-time optimization to the models in this experiment.

\input{Figures/5_psnr}
\input{Figures/6_test_time_optimization}
\input{Figures/7_rot}

The comparison results are presented in Table \ref{tab:multi_view}, which demonstrate that our Car-NeRF model achieves competitive performance with PixelNeRF-MLP \cite{PixelNeRF} and AutoRF \cite{autorf} in the multi-view supervision setting. Notably, our method that employs only single-view supervision outperforms all of the multi-view supervision methods. This observation could be attributed to the fact that the rough estimation of camera parameters introduces more inconsistent noise to the multi-view supervision framework. In contrast, using single-view supervision avoids this inconsistency by relying on a single viewpoint for training. Furthermore, CodeNeRF outperforms the encoder-decoder architecture approaches (all methods except CodeNeRF in this experiment) in the multi-view setting. This may be because encoder-decoder models use a global encoder to extract the latent vectors, which are optimized across all instances, while auto-decoder models optimize the latent vectors instance-specifically. Therefore, we further investigate the potential benefits of instance-specific optimization.



\subsection{Foreground Reconstruction with Test-Time Optimization}



To further leverage the advantages of independently optimizing the latent vector, akin to auto-decoder approaches, we implement a test-time optimization on the latent vector decoupled from the encoder. In the first stage, we train the Car-NeRF model using the KITTI-DET-derived dataset. We then detach the latent code from the encoder and proceed to the second stage of training. In the second stage, we perform a test-time optimization on the Car-NeRF's field and the latent codes using the KITTI-MOT-derived dataset. Figure \ref{fig:psnr} shows the quantitative learning curves for test-time optimization. Figure \ref{fig:test_time_optimization} presents qualitative comparisons between the results with and without test-time optimization. We also compare our approach with a no-prior method, taking NSG \cite{neuralscenegraphs} as an example in Figure \ref{fig:rot}. These comparisons demonstrate that our Car-NeRF model is scalable for test-time optimization while retaining the ability to synthesize unseen views.


\input{Figures/8_transforms}
\subsection{Applications for Autonomous Driving}

Our new pipeline enables foreground editing in an autonomous driving simulator, including instance insertion, spatial transformation, deletion, and replacement of instances, as demonstrated in Figure \ref{fig:manipulation} and similar to NSG \cite{neuralscenegraphs}. However, our approach performs better on unseen views, as shown in Figure \ref{fig:rot}.
\input{Figures/9_appearance}

Furthermore, our model's ability to use larger datasets enables new applications.  We can now controllably and separately edit the appearance of objects without modifying their geometry features. This is demonstrated in Figure \ref{fig:appearance}, where a transparent color mask is applied to the original patch to achieve the desired appearance. The figure shows that our model enables controllable appearance editing, and this ability improves as the dataset scale increases.