\section{RELATED WORK}

{\bf Urban NeRF for autonoumous driving}. Urban NeRF has recently attracted considerable attention, with numerous methods and models developed to address the challenges posed by foreground objects, particularly cars. While some methods, such as BlockNerf \cite{BlockNeRF}, employs foreground masks to ignore the foreground objects and focus on large-scale background reconstruction, our work focuses on the foreground cars and their interactions with the environment. Existing methods \cite{neuralscenegraphs,snerf,PNF,SUDS,StreetSurf,DynamicViewSynthesis} use the same auto-decoder architecture as the seminal NeRF \cite{Nerf}. While these methods are effective for representing different instances, they lack the ability for zero-shot learning and controllable appearance editing. Recent approaches\cite{discoscene, urbangiraffe, gina3d,nerfdiff,3DiM,Zero1to3,deceptivenerf,Magic123} have utilized generative models to learn foreground models. In contrast, our approach adopts the decoder-encoder architecture used in previous works such as PixelNerf \cite{PixelNeRF} and AutoRF \cite{autorf}, without relying on explicit generators. Moreover, our approach extends the processing of images to include pose-free and even camera intrinsics-free in-the-wild images and introduces a controllable appearance editing approach to enable more flexible editing capabilities.


{\bf Single view to 3D NeRF}. Inferring 3D shapes from 2D images is a long-standing challenge in computer vision and graphics. Existing methods present good results with different approaches. Deformation-based methods\cite{CMR, pixel2mesh,deformation,nvdiffrec} learn the 3D shape by mesh deformation based on a primitive mesh. Unsup3D\cite{unsup3d}  assumes symmetric and deformable shapes to learn 3D shapes. Mesh R-CNN \cite{meshrcnn} predicts the 3D mesh by a 3D voxel prediction followed by a mesh refine branch. PiFu \cite{pifu} learns the occupancy field in 3D directly by the occupancy  implicit functions.  Some works learn shape codes for different types of object priors like CAD model\cite{3D-RCNN, CADSim}, SDF-based Database\cite{singleshotsr}, activation code library\cite{ObjectCompositinalNeRF}.  Recently, single-view to 3D NeRF has emerged as a promising approach to tackling this problem, with impressive results achieved by methods such as MVSNerf \cite{MVSNerf} and PixelNerf \cite{PixelNeRF} using multi-view supervision, obtaining multi-view images is often more expensive and impractical compared to single-view images, especially in real-world scenarios. In contrast, our proposed method can process both multi-view and single-view image input, making it more versatile and practical for a wider range of applications. Recent works have explored the use of transformer-based architectures rather than MLPs to develop methods for view synthesis from a single input image, offering more fine-grained control over the reconstruction process \cite{visionNerf, SRT}. CodeNeRF \cite{codenerf} initializes the implicit representation with a pre-trained neural network and optimizes it using a differentiable renderer. Our proposed method differs in that ours involve two stages: an encoder that extracts features from the input image and uses them to initialize the implicit representation, followed by a similar optimization process as CodeNeRF to refine it further. This enables our model to extract individual instance latent codes from images with backgrounds, while CodeNeRF is restricted to test optimization only.
