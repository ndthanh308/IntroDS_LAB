\section{INTRODUCTION}

NeRF\cite{Nerf} has emerged as a powerful art for generating novel views in both computer vision and computer graphics, and has recently garnered attention for its potential in autonomous driving\cite{autorf, snerf, PNF, SUDS, discoscene, neuralscenegraphs, gina3d, geosim, ners, StreetSurf, BlockNeRF, FEGR, urbangiraffe}. By modeling the color and density of every point within the relevant 3D space, NeRF can create highly realistic images of a scene from any viewpoint. This capability is particularly useful in the context of autonomous vehicle testing\cite{airsim, carla}, as it enables the replay of a scene multiple times with different viewpoints even though it's not recorded before. By doing so, NeRF can help improve the safety and reliability of autonomous vehicles by enabling more thorough testing and evaluation. 

To create a photorealistic and user-friendly autonomous driving testing platform based on recorded data, it is necessary first to reconstruct editable urban scenes. However, reconstructing editable urban scenes in the wild poses several challenges. One major challenge is the inconsistent movement over time between the background and foreground. Previous works, such as \cite{autorf, snerf, PNF, SUDS, discoscene, neuralscenegraphs, gina3d, geosim, ners, StreetSurf, FEGR}, have addressed this issue by decomposing the scene into background and foreground and learning foreground nodes based on categories, speed, distance, or a combination of these factors. In this letter, we focus on the challenge of reconstructing editable car instances and address it by parsing the foregrounds based on categories to achieve flexible editing.

Another challenge in reconstructing editable urban scenes is constrained views. Prior works discussed in the previous paragraph were trained on autonomous driving street datasets, where photos are captured from egocentric video streams that mostly show the front or rear of the car facing the camera. Consequently, these models may not perform well when the target is viewed from an unseen angle, such as the side view of a car. Some existing works have introduced pre-training generative branches \cite{discoscene, gina3d, urbangiraffe} to generate unseen views, but these indirect approaches do not address the fundamental problem of a shortage of side view data. To overcome the challenge of constrained views, we propose leveraging online photos captured in the wild, specifically infinite Internet images. However, using these images presents a new set of challenges, such as unknown illuminations, environments, camera intrinsic parameters, 3D poses, and dimensions. Therefore, we propose a novel toolchain that leverages in-the-wild 2D photos for favorable 3D neural radiance field training. Our approach aims to address the fundamental shortage of side view data and overcome the challenges of using online photos for 3D object reconstruction.


\input{Tables/datasets}


Our toolchain, named \textbf{Car-Studio} (as depicted in Fig \ref{fig:teaser}), is designed to utilize in-the-wild car images to train a category-based NeRF \cite{Nerf} model for cars. To generate a car instances dataset, we use three in-the-wild datasets: KITTI dataset multi-object tracking track (KITTI-MOT)\cite{kitti}, KITTI dataset object detection track (KITTI-DET), and DVM-Cars\cite{dvm} dataset. We provide a comparison of these datasets in Tab \ref{tab:dataset}. While using in-the-wild data presents challenges such as messy objects with backgrounds, no camera parameters, unknown car poses, and single-view only instances without multi-view supervision, it also provides benefits.  For example, it offers different types of car instances with various orientations that supplement the urban benchmark such as \cite{kitti}, which lacks side-views.  

Based on our analysis of the combined datasets, we observed that the 2D car patches exhibit variations in size and possess boundaries that are challenging to discern, particularly in scenarios where the cars are in areas of low light or have hues that are similar to the surrounding environment. To overcome these challenges, we developed \textbf{Car-Nerf}, which leverages the anti-aliasing advantages of mip-NeRF \cite{mipnerf} to enable continuous scaling editing from far to near in simulation, and a segmentation mask that supervises the rendered accumulated radiance field weights for sharp contours. Additionally, we employ a canonical car-centric coordinate system to learn 3D spatial features from single-view only instances. Our proposed approach produces plausible rendering results in novel views and enables controllable spatial and appearance editing during scenes. 

Our contributions can be summarized as follows:

\begin{itemize}
    \item A curated dataset \textbf{CarPatch3D} of hundreds of thousands of 2D images and 3D spatial information. This dataset provides favorable information for training a category-based NeRF model for cars. Its availability enables the development of more effective urban NeRF foreground models.
    \item We developed \textbf{Car-NeRF}, which conforms to the characteristics of autonomous driving environments and achieves state-of-the-art performance in image reconstruction and novel view synthesis tasks.
    \item  We designed a pipeline called \textbf{Car-Studio} that can learn from single-view in-the-wild car images to generate 3D surrounding views and enable plausible controllable spatial and appearance editing.
\end{itemize}
