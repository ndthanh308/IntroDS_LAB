\section{Methodology}
\subsection{\methodname Pipeline Overview}\label{sec:overview}

We propose \textbf{\methodname}, a three-phase pipeline designed to learn a general car neural rendering model from online car photos, as shown in Fig. \ref{fig:teaser}. In the first phase, \textbf{Input}, we use a pre-trained 2D detector \cite{wu2019detectron2} to identify patches of interest. We filter out patches that are too small or whose classification results indicate a non-vehicle object to ensure high-quality data. In the subsequent \textbf{Preprocessing} phase, we refine the data further to prepare it for use in our model. We describe the preprocessing phase in detail in Section \ref{sec:preprocessing}. In the third phase, \textbf{Training}, we use our \textbf{Car-NeRF} model, discussed in Section \ref{sec:car-nerf}, to learn from the preprocessed data.



\subsection{Preprocessing}\label{sec:preprocessing}

The NeRF model \cite{Nerf} requires images, camera parameters, and camera poses as input. Obtaining accurate camera parameters from a single image can be challenging when there is no known size calibration target in the field of view. Therefore, we use a rough focal length based on \cite{kitti} and the optical center equal to half of the image shape size, and pass them to a 3D detection model \cite{dd3d} to solve for the camera pose.  To ensure high-quality patches, we apply a filtering process that consists of the following steps:

\begin{enumerate}
\item We filter out images with a low intersection ratio between the 2D detector output and the minimum 2D envelope box for reprojecting the 3D box, as this indicates inconsistencies between the outputs of the 2D and 3D detectors.
\item We filter out images with a low pixel count ratio between the result of the 2D segmentor Segment Anything \cite{SAM} pixel-level segmentation and the 2D detector's result, which indicates high occlusion rate.
\item We filter out images with a low prediction confidence from the 2D detector, 3D detector, and 2D segmentor.
\item We manually filter out 3D prediction results that are inconsistent with the image.
\end{enumerate}

We carefully followed the aforementioned steps to ensure high-quality preprocessing of the data, making it suitable for use in our pipeline. This effort resulted in a new dataset named \textbf{CarPatch3D}. CarPatch3D comprises a set of car patches, selected using the semi-automatic method described above, along with binary masks generated by the SAM \cite{SAM}, roughly estimated camera parameters, and 3D position and orientation of the vehicles generated by DD3D \cite{dd3d}. This rich set of data makes CarPatch3D highly suitable for NeRF training of cars in autonomous driving.

\subsection{Car-NeRF Model}\label{sec:car-nerf}

\input{Figures/2_main_graph}


The architecture of our model is shown in Fig \ref{fig:architecrure}. We learn the global latent codes $\bm{z}$ using a global encoder, which is a ResNet-34\cite{resnet} pre-trained on ImageNet \cite{imagenet}. We then decouple the global latent codes into a shape component $\bm{z}_{shape}$ and a texture component $\bm{z}_{texture}$ using two one-layer independent MLPs.

\textbf{Normalization to car-centric canonical coordinate}.To learn 3D shapes from single-view image supervision, we convert the camera-centric coordinates to canonical coordinates centered at the car's centroid and oriented along its principal axes. We obtain the dimensions of the car (length $l$, height $h$, and width $w$) from the preprocessing 3D detection module's result. The scaled [-1, 1] camera pose with respect to the camera coordinate $T^{scaled}_{cam}$ is given by:
\begin{equation}
    T^{scaled}_{cam} = ([\frac{2}{l}, \frac{2}{h}, \frac{2}{w}, 1]^T \mathbf{1}_{1\times4}) \circ \mathbf{I}_{4\times4}
\end{equation}, where $\circ$ is the Hadamard product.

The camera pose with respect to the canonical coordinate system is obtained by the scaled camera pose with respect to the camera coordinate system ($T_{cam}^{scaled}$), and the car pose with respect to the camera coordinate system ($T_{cam}^{car}$). The resulting camera pose with respect to the canonical coordinate system $T^{cam}_{canon}$ is given by:
\begin{equation}
T^{cam}_{canon} = T^{scaled}_{cam}(T_{cam}^{car})^{-1} T^{scaled}_{canon}
\end{equation}, where $T^{scaled}_{canon}$ is scaling matrices that map the  canonical coordinates to camera-centric scaled coordinates whose diagonal values are (1, -1, -1, 1) according to Fig \ref{fig:architecrure}.

\textbf{Cone tracing and IPE}. In our architecture, the encoder takes Internet images processed by a 2D detector as input. Since the patch size of the input images can vary greatly, the region that each pixel represents can also vary greatly. To address this, we use cone tracing instead of ray tracing for a continuous scaling that provides anti-aliasing \cite{mipnerf}. A cone at apex $\bm{o}$ with direction $\bm{d}$ is represented by a multivariate Gaussian distribution with mean $\bm{\mu}(\bm{o}, \bm{d})$ and variance $\mathbf{\Sigma}(\bm{d})$.

To build a continuous scaling along the axis of cones, we use the integrated positional encoding (IPE) \cite{mipnerf} instead of positional encoding (PE) \cite{Nerf}. The IPE incorporates the multivariate Gaussian representation into the encoding process, resulting in improved performance. The IPE of positions is denoted by $\gamma_{pos}$, while the IPE of directions is denoted by $\gamma_{dir}$.

\textbf{Sampling and model architecture}. We adopt a coarse-to-fine pixel sampling strategy and a shared model architecture similar to mip-NeRF \cite{mipnerf}. The model $f_\theta$ consists of a shape component $f_{\theta, shape}$ and a texture component $f_{\theta, texture}$. To embed the latent vectors into the model, we introduce the following modifications:
\begin{equation}
\mathbf{f}_{out}, \sigma = f_{\theta, shape} (\gamma_{pos}(\bm{\mu}(\bm{o}, \bm{d}), \mathbf{\Sigma}(\bm{d})) + \bm{z}_{shape} )
\end{equation}

Here, $\bm{z}_{shape}$ is the shape latent vector, $\sigma$ is the density, and $\mathbf{f}_{out}$ is the output feature of the shape component. The RGB color, denoted by $c$, can be calculated using the texture latent vector $\bm{z}_{texture}$ as follows:
\begin{equation}
\textup{RGB} = c = f_{\theta, texture}(\mathbf{f}_{out} + \gamma_{dir}(\bm{d}) + \bm{z}_{texture})
\end{equation}

The accumulated weights $\int \bm{\omega}$ from near $t_n$ to far $t_f$ along the axis $\bm{r} = \bm{o} + t\bm{d}$ of the cone is given by:
\begin{equation}
    \smallint \bm{\omega} (\bm{r}) = \int_{t_n}^{t_f}\exp(-\int_{t_n}^t\sigma(\bm{r}(s)ds))\sigma(\bm{r}(t))dt
\end{equation}

\textbf{Volume Rendering and Loss Functions}. We employ the volume rendering technique described in \cite{Nerf} to compute the estimated rendered color $\hat{\textup{RGB}}$ and the estimated accumulated weights $\hat{\int \bm{\omega}}$ along the cone axis defined by the apex $\bm{o}$ and the direction $\bm{d}$ at the given pixel location. At this pixel location, the ground truth color is denoted as $\textup{RGB}$, and the corresponding binary mask obtained from the 2D segmentor described in Section \ref{sec:preprocessing} is denoted as $\alpha$. The photometric L2 render loss $\mathcal{L}_r$ is defined as follows:
\begin{equation}
    \mathcal{L}_{r} = \alpha(|| \hat{\textup{RGB}}_{f} - \textup{RGB}||_2^2 + \lambda_{c}|| \hat{\textup{RGB}}_{c} - \textup{RGB}||_2^2)
\end{equation}

Here, the estimated rendered color obtained from fine sampling is denoted as $\hat{\textup{RGB}}_{f}$, while that obtained from coarse sampling is denoted as $\hat{\textup{RGB}}_{c}$. The hyperparameter $\lambda_{coarse}$ is used to balance the loss between two sampling stages. To prevent our model from learning the background color or shapes, we incorporate the binary mask $\alpha$ into the loss function. Similarly, the segmentation loss $\mathcal{L}_s$ is:
\begin{equation}
    \mathcal{L}_s = ||\hat{\smallint \bm{\omega}}_f - \alpha||^2_2 + \lambda_c||\hat{\smallint \bm{\omega}}_c - \alpha||^2_2
\end{equation}

The total loss is weighted by a segmentation balancing coefficient $\lambda_s$:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{r} + \lambda_s \mathcal{L}_{s}
\end{equation}
