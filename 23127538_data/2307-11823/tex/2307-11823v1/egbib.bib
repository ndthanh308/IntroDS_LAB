@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}


@article{hendrycks2019augmix,
  title={Augmix: A simple data processing method to improve robustness and uncertainty},
  author={Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02781},
  year={2019}
}

@article{yang2021generalized,
  title={Generalized out-of-distribution detection: A survey},
  author={Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2110.11334},
  year={2021}
}

@article{rosenberg2021adversarial,
  title={Adversarial machine learning attacks and defense methods in the cyber security domain},
  author={Rosenberg, Ishai and Shabtai, Asaf and Elovici, Yuval and Rokach, Lior},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={5},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{deng2020analysis,
  title={An analysis of adversarial attacks and defenses on autonomous driving models},
  author={Deng, Yao and Zheng, Xi and Zhang, Tianyi and Chen, Chen and Lou, Guannan and Kim, Miryung},
  booktitle={2020 IEEE international conference on pervasive computing and communications (PerCom)},
  pages={1--10},
  year={2020},
  organization={IEEE}
}

@InProceedings{Benz_2021_WACV,
    author    = {Benz, Philipp and Zhang, Chaoning and Karjauv, Adil and Kweon, In So},
    title     = {Revisiting Batch Normalization for Improving Corruption Robustness},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2021},
    pages     = {494-503}
}

@InProceedings{Yeo_2021_ICCV,
    author    = {Yeo, Teresa and Kar, O\u{g}uzhan Fatih and Zamir, Amir},
    title     = {Robustness via Cross-Domain Ensembles},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {12189-12199}
}

@InProceedings{Saikia_2021_ICCV,
    author    = {Saikia, Tonmoy and Schmid, Cordelia and Brox, Thomas},
    title     = {Improving Robustness Against Common Corruptions With Frequency Biased Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10211-10220}
}


@inproceedings{
wang2021augmax,
title={AugMax: Adversarial Composition of Random Augmentations for Robust Training},
author={Haotao Wang and Chaowei Xiao and Jean Kossaifi and Zhiding Yu and Anima Anandkumar and Zhangyang Wang},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=P5MtdcVdFZ4}
}

@InProceedings{Lee_2020_CVPR_Workshops,
author = {Lee, Jin-Ha and Zaheer, Muhammad Zaigham and Astrid, Marcella and Lee, Seung-Ik},
title = {SmoothMix: A Simple Yet Effective Data Augmentation to Train Robust Classifiers},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2020}
}

@inproceedings{chen2021amplitude,
  title={Amplitude-phase recombination: Rethinking robustness of convolutional neural networks in frequency domain},
  author={Chen, Guangyao and Peng, Peixi and Ma, Li and Li, Jia and Du, Lin and Tian, Yonghong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={458--467},
  year={2021}
}

@InProceedings{prime_aug,
author="Modas, Apostolos
and Rade, Rahul
and Ortiz-Jim{\'e}nez, Guillermo
and Moosavi-Dezfooli, Seyed-Mohsen
and Frossard, Pascal",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="PRIME: A Few Primitives Can Boost Robustness to Common Corruptions",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="623--640",
abstract="Despite their impressive performance on image classification tasks, deep networks have a hard time generalizing to unforeseen corruptions of their data. To fix this vulnerability, prior works have built complex data augmentation strategies, combining multiple methods to enrich the training data. However, introducing intricate design choices or heuristics makes it hard to understand which elements of these methods are indeed crucial for improving robustness. In this work, we take a step back and follow a principled approach to achieve robustness to common corruptions. We propose PRIME, a general data augmentation scheme that relies on simple yet rich families of max-entropy image transformations. PRIME outperforms the prior art in terms of corruption robustness, while its simplicity and plug-and-play nature enable combination with other methods to further boost their robustness. We analyze PRIME to shed light on the importance of the mixing strategy on synthesizing corrupted images, and to reveal the robustness-accuracy trade-offs arising in the context of common corruptions. Finally, we show that the computational efficiency of our method allows it to be easily used in both on-line and off-line data augmentation schemes. Our code is available at https://github.com/amodas/PRIME-augmentations.",
isbn="978-3-031-19806-9"
}

@inproceedings{
calian2022defending,
title={Defending Against Image Corruptions Through Adversarial Augmentations},
author={Dan Andrei Calian and Florian Stimberg and Olivia Wiles and Sylvestre-Alvise Rebuffi and Andr{\'a}s Gy{\"o}rgy and Timothy A Mann and Sven Gowal},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=jJOjjiZHy3h}
}

@InProceedings{Hendrycks_2022_CVPR,
    author    = {Hendrycks, Dan and Zou, Andy and Mazeika, Mantas and Tang, Leonard and Li, Bo and Song, Dawn and Steinhardt, Jacob},
    title     = {PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16783-16792}
}

@inproceedings{wang2020high,
  title={High-frequency component helps explain the generalization of convolutional neural networks},
  author={Wang, Haohan and Wu, Xindi and Huang, Zeyi and Xing, Eric P},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8684--8694},
  year={2020}
}

@inproceedings{long2022frequency,
  title={Frequency Domain Model Augmentation for Adversarial Attack},
  author={Long, Yuyang and Zhang, Qilong and Zeng, Boheng and Gao, Lianli and Liu, Xianglong and Zhang, Jian and Song, Jingkuan},
  booktitle={European Conference on Computer Vision},
  pages={549--566},
  year={2022},
  organization={Springer}
}

@InProceedings{frequencyaug_competitor_method,
author="Sun, Jiachen
and Mehra, Akshay
and Kailkhura, Bhavya
and Chen, Pin-Yu
and Hendrycks, Dan
and Hamm, Jihun
and Mao, Z. Morley",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="A Spectral View of Randomized Smoothing Under Common Corruptions: Benchmarking and Improving Certified Robustness",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="654--671",
abstract="Certified robustness guarantee gauges a model's resistance to test-time attacks and can assess the model's readiness for deployment in the real world. In this work, we explore a new problem setting to critically examine how the adversarial robustness guarantees change when state-of-the-art randomized smoothing-based certifications encounter common corruptions of the test data. Our analysis demonstrates a previously unknown vulnerability of these certifiably robust models to low-frequency corruptions such as weather changes, rendering these models unfit for deployment in the wild. To alleviate this issue, we propose a novel data augmentation scheme, FourierMix, that produces augmentations to improve the spectral coverage of the training data. Furthermore, we propose a new regularizer that encourages consistent predictions on noise perturbations of the augmented data to improve the quality of the smoothed models. We show that FourierMix helps eliminate the spectral bias of certifiably robust models, enabling them to achieve significantly better certified robustness on a range of corruption benchmarks. Our evaluation also uncovers the inability of current corruption benchmarks to highlight the spectral biases of the models. To this end, we propose a comprehensive benchmarking suite that contains corruptions from different regions in the spectral domain. Evaluation of models trained with popular augmentation methods on the proposed suite unveils their spectral biases. It also establishes the superiority of FourierMix trained models in achieving stronger certified robustness guarantees under corruptions over the entire frequency spectrum.",
isbn="978-3-031-19772-7"
}

@InProceedings{eccv2022_freqpaper,
author="Long, Yuyang
and Zhang, Qilong
and Zeng, Boheng
and Gao, Lianli
and Liu, Xianglong
and Zhang, Jian
and Song, Jingkuan",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Frequency Domain Model Augmentation for Adversarial Attack",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="549--566",
abstract="For black-box attacks, the gap between the substitute model and the victim model is usually large, which manifests as a weak attack performance. Motivated by the observation that the transferability of adversarial examples can be improved by attacking diverse models simultaneously, model augmentation methods which simulate different models by using transformed images are proposed. However, existing transformations for spatial domain do not translate to significantly diverse augmented models. To tackle this issue, we propose a novel spectrum simulation attack to craft more transferable adversarial examples against both normally trained and defense models. Specifically, we apply a spectrum transformation to the input and thus perform the model augmentation in the frequency domain. We theoretically prove that the transformation derived from frequency domain leads to a diverse spectrum saliency map, an indicator we proposed to reflect the diversity of substitute models. Notably, our method can be generally combined with existing attacks. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our method, e.g., attacking nine state-of-the-art defense models with an average success rate of 95.4{\%}. Our code is available in https://github.com/yuyang-long/SSA.",
isbn="978-3-031-19772-7"
}

@article{oliva2006hybrid, title={Hybrid images}, author={Oliva, Aude and Torralba, Antonio and Schyns, Philippe G}, journal={ACM Transactions on Graphics (TOG)}, volume={25}, number={3}, pages={527--532}, year={2006}, publisher={ACM New York, NY, USA} }

@inproceedings{zhang2021universal,
  title={Universal adversarial perturbations through the lens of deep steganography: Towards a fourier perspective},
  author={Zhang, Chaoning and Benz, Philipp and Karjauv, Adil and Kweon, In So},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={4},
  pages={3296--3304},
  year={2021}
}

@article{wang2020towards,
  title={Towards frequency-based explanation for robust cnn},
  author={Wang, Zifan and Yang, Yilin and Shrivastava, Ankit and Rawal, Varun and Ding, Zihao},
  journal={arXiv preprint arXiv:2005.03141},
  year={2020}
}

@article{li2022robust,
  title={Robust deep learning object recognition models rely on low frequency information in natural images},
  author={Li, Zhe and Caro, Josue Ortega and Rusak, Evgenia and Brendel, Wieland and Bethge, Matthias and Anselmi, Fabio and Patel, Ankit B and Tolias, Andreas S and Pitkow, Xaq},
  journal={bioRxiv},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@inproceedings{moosavi2016deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016}
}

@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2017 ieee symposium on security and privacy (sp)},
  pages={39--57},
  year={2017},
  organization={Ieee}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@inproceedings{croce2020reliable,
  title={Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
  author={Croce, Francesco and Hein, Matthias},
  booktitle={International conference on machine learning},
  pages={2206--2216},
  year={2020},
  organization={PMLR}
}

@article{shaham2018defending,
  title={Defending against adversarial images using basis functions transformations},
  author={Shaham, Uri and Garritano, James and Yamada, Yutaro and Weinberger, Ethan and Cloninger, Alex and Cheng, Xiuyuan and Stanton, Kelly and Kluger, Yuval},
  journal={arXiv preprint arXiv:1803.10840},
  year={2018}
}

@inproceedings{borkar2020defending,
  title={Defending against universal attacks through selective feature regeneration},
  author={Borkar, Tejas and Heide, Felix and Karam, Lina},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={709--719},
  year={2020}
}

@inproceedings{lu2017safetynet,
  title={Safetynet: Detecting and rejecting adversarial examples robustly},
  author={Lu, Jiajun and Issaranon, Theerasit and Forsyth, David},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={446--454},
  year={2017}
}

@article{zhang2020geometry,
  title={Geometry-aware instance-reweighted adversarial training},
  author={Zhang, Jingfeng and Zhu, Jianing and Niu, Gang and Han, Bo and Sugiyama, Masashi and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2010.01736},
  year={2020}
}

@article{xu2017feature,
  title={Feature squeezing: Detecting adversarial examples in deep neural networks},
  author={Xu, Weilin and Evans, David and Qi, Yanjun},
  journal={arXiv preprint arXiv:1704.01155},
  year={2017}
}

@inproceedings{meng2017magnet,
  title={Magnet: a two-pronged defense against adversarial examples},
  author={Meng, Dongyu and Chen, Hao},
  booktitle={Proceedings of the 2017 ACM SIGSAC conference on computer and communications security},
  pages={135--147},
  year={2017}
}

@article{mu2019mnist,
  title={Mnist-c: A robustness benchmark for computer vision},
  author={Mu, Norman and Gilmer, Justin},
  journal={arXiv preprint arXiv:1906.02337},
  year={2019}
}

@inproceedings{kar20223d,
  title={3D Common Corruptions and Data Augmentation},
  author={Kar, O{\u{g}}uzhan Fatih and Yeo, Teresa and Atanov, Andrei and Zamir, Amir},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18963--18974},
  year={2022}
}

@inproceedings{mok2021advrush,
  title={AdvRush: Searching for Adversarially Robust Neural Architectures},
  author={Mok, Jisoo and Na, Byunggook and Choe, Hyeokjun and Yoon, Sungroh},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12322--12332},
  year={2021}
}

@misc{subnet_enhance,
  doi = {10.48550/ARXIV.2201.12765},
  
  url = {https://arxiv.org/abs/2201.12765},
  
  author = {Guo, Yong and Stutz, David and Schiele, Bernt},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Improving Robustness by Enhancing Weak Subnets},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{li2020wavelet,
  title={Wavelet integrated CNNs for noise-robust image classification},
  author={Li, Qiufu and Shen, Linlin and Guo, Sheng and Lai, Zhihui},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7245--7254},
  year={2020}
}

@InProceedings{adversarialtraining_corruption,
  title = 	 {On the effectiveness of adversarial training against common corruptions},
  author =       {Kireev, Klim and Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1012--1021},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/kireev22a/kireev22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/kireev22a.html},
  abstract = 	 {The literature on robustness towards common corruptions shows no consensus on whether adversarial training can improve the performance in this setting. First, we show that, when used with an appropriately selected perturbation radius, Lp adversarial training can serve as a strong baseline against common corruptions improving both accuracy and calibration. Then we explain why adversarial training performs better than data augmentation with simple Gaussian noise which has been observed to be a meaningful baseline on common corruptions. Related to this, we identify the sigma-overfitting phenomenon when Gaussian augmentation overfits to a particular standard deviation used for training which has a significant detrimental effect on common corruption accuracy. We discuss how to alleviate this problem and then how to further enhance Lp adversarial training by introducing an efficient relaxation of adversarial training with learned perceptual image patch similarity as the distance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that our approach does not only improve the Lp adversarial training baseline but also has cumulative gains with data augmentation methods such as AugMix, DeepAugment, ANT, and SIN, leading to state-of-the-art performance on common corruptions.}
}

@InProceedings{ant_corruption,
author="Rusak, Evgenia
and Schott, Lukas
and Zimmermann, Roland S.
and Bitterwolf, Julian
and Bringmann, Oliver
and Bethge, Matthias
and Brendel, Wieland",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="A Simple Way to Make Neural Networks Robust Against Diverse Image Corruptions",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="53--69",
abstract="The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against locally correlated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement.",
isbn="978-3-030-58580-8"
}

@article{yin2019fourier,
  title={A fourier perspective on model robustness in computer vision},
  author={Yin, Dong and Gontijo Lopes, Raphael and Shlens, Jon and Cubuk, Ekin Dogus and Gilmer, Justin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@misc{shapebias_corruption,
  doi = {10.48550/ARXIV.2206.05846},
  
  url = {https://arxiv.org/abs/2206.05846},
  
  author = {Gowda, Shruthi and Zonooz, Bahram and Arani, Elahe},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {InBiaseD: Inductive Bias Distillation to Improve Generalization and Robustness through Shape-awareness},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@InProceedings{Sun_2021_ICCV,
    author    = {Sun, Mingjie and Li, Zichao and Xiao, Chaowei and Qiu, Haonan and Kailkhura, Bhavya and Liu, Mingyan and Li, Bo},
    title     = {Can Shape Structure Features Improve Model Robustness Under Diverse Adversarial Settings?},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {7526-7535}
}


@article{geirhos2018imagenet,
  title={ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
  author={Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
  journal={arXiv preprint arXiv:1811.12231},
  year={2018}
}


@article{chan2022does,
  title={How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations?},
  author={Chan, Alvin and Ong, Yew-Soon and Tan, Clement},
  journal={arXiv preprint arXiv:2205.04533},
  year={2022}
}

@inproceedings{mukai2022improving,
  title={Improving Robustness to out-of-Distribution Data by Frequency-Based Augmentation},
  author={Mukai, Koki and Kumano, Soichiro and Yamasaki, Toshihiko},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)},
  pages={3116--3120},
  year={2022},
  organization={IEEE}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}
@article{yu2015lsun,
  title={Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop},
  author={Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
  journal={arXiv preprint arXiv:1506.03365},
  year={2015}
}

@article{krizhevsky2017imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@article{springenberg2014striving,
  title={Striving for simplicity: The all convolutional net},
  author={Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1412.6806},
  year={2014}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@article{tack2020csi,
  title={Csi: Novelty detection via contrastive learning on distributionally shifted instances},
  author={Tack, Jihoon and Mo, Sangwoo and Jeong, Jongheon and Shin, Jinwoo},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11839--11852},
  year={2020}
}

@article{khosla2020supervised,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18661--18673},
  year={2020}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@article{yucel2022robust,
  title={How robust are discriminatively trained zero-shot learning models?},
  author={Yucel, Mehmet Kerim and Cinbis, Ramazan Gokberk and Duygulu, Pinar},
  journal={Image and Vision Computing},
  volume={119},
  pages={104392},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{yucel2020deep,
  title={A deep dive into adversarial robustness in zero-shot learning},
  author={Yucel, Mehmet Kerim and Cinbis, Ramazan Gokberk and Duygulu, Pinar},
  booktitle={European Conference on Computer Vision},
  pages={3--21},
  year={2020},
  organization={Springer}
}

@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8340--8349},
  year={2021}
}

@article{mintun2021interaction,
  title={On interaction between augmentations and corruptions in natural corruption robustness},
  author={Mintun, Eric and Kirillov, Alexander and Xie, Saining},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3571--3583},
  year={2021}
}

@article{wong2020fast,
  title={Fast is better than free: Revisiting adversarial training},
  author={Wong, Eric and Rice, Leslie and Kolter, J Zico},
  journal={arXiv preprint arXiv:2001.03994},
  year={2020}
}

@inproceedings{zhang2019making,
  title={Making convolutional networks shift-invariant again},
  author={Zhang, Richard},
  booktitle={International conference on machine learning},
  pages={7324--7334},
  year={2019},
  organization={PMLR}
}

@inproceedings{benz2021robustness,
  title={Robustness comparison of vision transformer and MLP-Mixer to CNNs},
  author={Benz, Philipp and Zhang, Chaoning and Ham, Soomin and Karjauv, Adil and Kweon, In So},
  booktitle={CVPR 2021 Workshop on Adversarial Machine Learning in Real-World Computer Vision Systems and Online Challenges (AML-CV)},
  volume={7},
  year={2021}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}


@article{salman2020adversarially,
  title={Do adversarially robust imagenet models transfer better?},
  author={Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3533--3545},
  year={2020}
}