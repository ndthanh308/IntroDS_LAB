\section{Experimental Results} \label{sec:exp_hybridaugment}
In this section, we first describe our experimental setup, including the datasets, metrics, architectures and implementation details. We then present a discussion of the Gaussian kernel details, an important detail of the proposed schemes. We thoroughly evaluate the effectiveness of $\mathcal{HA}$ and $\mathcal{HA^{++}}$ in terms of three distribution shifts; common image corruptions, adversarial attacks, and out-of-distribution detection. We finalize with additional results and a discussion of the potential limitations.

\subsection{Experimental setup}

\noindent \textbf{Datasets.} We use CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning} and ImageNet \cite{deng2009imagenet} for training. Both CIFAR datasets are formed of 50.000 training images with a size of 32$\times$32. ImageNet dataset contains around 1.2 million images of 1000 different classes. Corruption robustness is evaluated on the corrupted versions of the test splits of these datasets, which are CIFAR-10-C, CIFAR-100-C and ImageNet-C \cite{hendrycks2019benchmarking}. For each dataset, corruptions are simulated for 4 categories (noise, blur, weather, digital) with 15 corruption types, each with 5 severity levels.  For adversarial robustness, we use AutoAttack \cite{croce2020reliable} on CIFAR-10 test set. Out-of-distribution detection is evaluated on SVHN \cite{netzer2011reading}, LSUN \cite{yu2015lsun}, ImageNet and CIFAR-100, and their fixed versions \cite{tack2020csi}. 

\noindent \textbf{Evaluation metrics.} We report top-1 classification as clean accuracy. Adversarial robustness is evaluated with robust accuracy, which is the top-1 classification on adversarially attacked test sets. Corruption robustness is evaluated with Corruption Error (CE) $CE=\sum^{5}_{1}E_{c,s}^{F} / \sum^{5}_{1}E_{c,s}^{AlexNet}$. CE calculates the average error of the model $F$ on a corruption type, normalized by the corruption error of AlexNet \cite{krizhevsky2017imagenet}. CE is calculated for all 15 corruption types, and their average Mean Corruption Error (mCE) is used as the final robustness metric. Out-of-distribution detection is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUROC) metric \cite{hendrycks2016baseline}. 



% Figure environment removed

\noindent \textbf{Architectures.} We use architectures commonly used in the literature for a fair comparison; ResNeXT \cite{xie2017aggregated}, All-Convolutional Network \cite{springenberg2014striving}, DenseNet \cite{huang2017densely}, WideResNet \cite{zagoruyko2016wide} and ResNet18 \cite{he2016deep} are used in CIFAR-10 and CIFAR-100, whereas ResNet50 is used for ImageNet.

\noindent \textbf{Implementation details.} For CIFAR experiments, all architectures are trained for 200 epochs with SGD, where initial learning rate of 0.1 decays after every 60 epochs. We use the last checkpoints for evaluation and do not perform any hyperparameter tuning. Paired and single variants of $\mathcal{HA}$ and $\mathcal{HA^{++}}$ are applied in each iteration with probabilities 0.6 and 0.5, respectively. Standard training augmentations are random horizontal flips and cropping. When a single-image augmentation is used, the input image is augmented with $Aug$ randomly sampled among [\textit{rasterize, autocontrast, equalize, rotate, solarize, shear, translate}]. Note that these do not overlap with test corruptions. On ImageNet, we train for 100 epochs with SGD, where an initial learning rate of 0.1 is decayed every 30 epochs. Data augmentations and their probabilities are the same as above.

We use the same checkpoints for all evaluations; we do not train separate models for corruption and out-of-distribution detection. In adversarial analysis, for a fair comparison with \cite{chen2021amplitude}, we train our model with $\mathcal{HA}$ \& $\mathcal{HA^{++}}$ and FGSM adversarial training. We note that we use the labels of the low-frequency image as the ground-truth labels. We have tried using the high-frequency image labels instead, but this leads to severe degradation in overall performance, as expected. All models are trained with the cross-entropy loss, where the original and the augmented (with our method) batches are used to calculate the loss. 



\subsection{Understanding the cut-off frequency} 

A key design choice is the cut-off frequency that defines $\mathcal{HF}$ and $\mathcal{LF}$ in Equation \ref{eq:cutoff}. Since we essentially define the cut-off frequency with a Gaussian blur operation, we have two hyperparameters; the size of the Gaussian kernel $\text{K}$ and its standard deviation $\text{S}$. Note that increasing both the kernel size and the standard deviation increases the blur strength, which eliminates increasingly higher frequencies (\ie higher cut-off frequency). We now evaluate the effects of these hyperparameters on both clean accuracy and mean corruption errors using $\mathcal{HA^{++}_{PS}}$, on both CIFAR-10 and CIFAR-100 using the ResNet18 architecture.

\noindent \textbf{Fixed standard deviation.} The effect of different $\text{K}$ values with fixed $\text{S}=0.5$ is shown in Figure \ref{fig:blur_ablation} top plot. $\text{K}=3$ provides the best trade-off here; it has the best clean accuracy and mCE on CIFAR100, whereas it shares the best mCE and has competitive clean accuracy on CIFAR10. 

\noindent \textbf{Fixed kernel size.} $\text{K}=3$ with different standard deviation values are shown in Figure \ref{fig:blur_ablation} bottom plot. The robustness-accuracy trade-off becomes more visible here; lower sigma values (\ie lower cut-off frequency) preserve more high-frequency content, and therefore have increasingly higher clean accuracy, but at the expense of degrading mCE. Note that further increasing the value $\text{S}$ is in contrast with this phenomena; if our method had only done frequency swapping (\ie $\mathcal{HA}$), then we could have expected a consistent trend, as shown in the literature \cite{li2022robust,wang2020towards}. However, $\mathcal{HA^{++}}$ also emphasizes the phase components, which results into a favourable behaviour where best results in mCE and clean accuracy can be obtained in the same cut-off frequency.

\noindent \textbf{The takeaway.} The results show that our hypothesis holds; we can find a sweet spot in the frequency spectrum where we can obtain favourable performance on both corrupted and clean images, given a careful selection of $\text{K}$ and $\text{S}$. A sound argument is that the optimality of these hyperparameters depends on the data; this is probably a correct assumption and can help tune the results further on other datasets. However, we use $K=3, S=0.5$ on all experiments across all architectures and datasets (including ImageNet), and show that we get solid improvements without any dataset-specific tuning.



\input{sections/table1.tex}  % corruption robustness
\input{sections/table2.tex}  % clean accuracy
\input{sections/table3.tex}  % corruption & clean on imagenet

\input{sections/table4.tex}  % AutoAttack



\subsection{Corruption robustness}
\noindent  As mentioned in Section \ref{method_hybridaugment}, we have three augmentation options; $\mathcal{APR}$ \cite{chen2021amplitude}, $\mathcal{HA}$ and $\mathcal{HA^{++}}$. We can apply them using image pairs, a single image or we can do both. This leads to quite a few potential combinations. We now evaluate all these combinations on CIFAR-10 and CIFAR-100, both for clean accuracy and corruption robustness (mCE). 

\noindent \textbf{Comparison against RFC~\cite{mukai2022improving}.} We implement and compare against RFC, which also performs hybrid-image based augmentation. RFC operates on paired-images of same-class samples, therefore we first compare it against $\mathcal{HA_{P}}$ and $\mathcal{HA^{++}_{P}}$. In mCE, we comfortably outperform it while staying competitive in clean accuracy. This shows the value of lifting the limitation of class-based sampling, which RFC does. Note that since we also propose single-image variants, both single-image augmentations ($\mathcal{HA_{S}}$ and $\mathcal{HA^{++}_{S}}$) and combined ones ($\mathcal{HA_{PS}}$ and $\mathcal{HA^{++}_{PS}}$) significantly outperform RFC on all architectures, datasets and metrics. 

\noindent \textbf{Corruption robustness.} The corruption results are shown in Table \ref{tab:cifar10_100_corruption}. The take-away message is crystal clear; $\mathcal{HA^{++}}$ is the best on all datasets, all architectures and all groups. The best results are obtained when we use $\mathcal{HA^{++}}$ both in pairs and single images, further cementing its effectiveness. Note that $\mathcal{HA}$ is competitive or better than $\mathcal{APR}$. 





% Figure environment removed




\noindent \textbf{Clean Accuracy.} The clean accuracy values of the models shown in Table \ref{tab:cifar10_100_corruption} are given in Table \ref{tab:cifar10_100_clean_accuracy}.  The results show us that both $\mathcal{HA}$ and $\mathcal{HA^{++}}$ achieve a good spot in robustness-accuracy trade-off; except two cases, both of them improve clean accuracy over the original models. The results are not as \textit{clean-cut} as those of Table \ref{tab:cifar10_100_corruption}, but in each group, the best ones mostly include $\mathcal{HA}$ or $\mathcal{HA^{++}}$. Furthermore, the best results on CIFAR-10 and CIFAR-100 have $\mathcal{HA_{S}}$ as the single-image augmentation. Although it does not perform the best, $\mathcal{HA^{++}_{PS}}$ still outperforms the baseline and is highly competitive against others. 

\noindent \textbf{The takeaway.} The results show us that $\mathcal{HA}$ and $\mathcal{HA^{++}}$ are superior to other frequency-based methods, and they comfortably improve robustness and clean accuracy performance across multiple datasets and architectures. See supplementary material for comparison with the state-of-the-art on CIFAR-10 and CIFAR-100. \textit{Hint to readers}: \textit{we achieve the state-of-the-art on all architectures on both datasets}.  





\vspace{-3mm}
\subsubsection{Scaling to ImageNet}
\noindent We now assess whether our methods can scale to ImageNet. Since we do not use extra data or ensembles during training or inference, we choose methods with similar characteristics, such as SIN \cite{ant_corruption}, PatchUniform, AutoAugment (AA), Random AA \cite{cubuk2018autoaugment}, MaxBlurPool and AugMix \cite{hendrycks2019augmix}. The results are shown in Table \ref{tab:imagenet_results}. Note that we use pretrained weights for alternative methods if available, otherwise we use the values reported in \cite{chen2021amplitude}.



The results show that all of our variants produce higher clean accuracy compared to $\mathcal{APR}$, showing the value of our method in improving model accuracy. $\mathcal{HA}$ results are competitive in corruption accuracy, but $\mathcal{HA^{++}}$ outperforms both $\mathcal{APR}$ and others in corruption accuracy, while being 0.5 shy of our best clean accuracy. Furthermore, our approach works well with extra data and other augmentations; we apply $\mathcal{HA^{++}_{PS}}$ with DeepAugment \cite{hendrycks2021many} and AugMix \cite{hendrycks2019augmix}, which leads to significant improvements in mCE ($\mathcal{\sim}$11 points) over both DeepAugment and $\mathcal{HA^{++}_{PS}}$.  Note that we are better than $\mathcal{APR}$, even when both methods train with DeepAugment. We also outperform PixMix \cite{Hendrycks_2022_CVPR}, which uses extra training data. Finally, we provide results of $\mathcal{HA^{++}_{PS}}$ with higher cut-off frequency (see experiments with $\dagger$ in Table \ref{tab:imagenet_results}); we see the expected trend where the elimination of higher frequencies make our models more robust in average, at the expense of lowered clean accuracy.






\noindent \textbf{Qualitative results.} We provide GradCam visualizations of $\mathcal{HA^{++}}$ against various corruptions in Figure \ref{fig:gradcam}. We sample corruptions from each category; noise, motion blur, fog, pixelate and contrast corruptions are shown from top to bottom. In the first four rows, it is apparent that corruptions lead to the standard model focusing on the wrong areas, leading to misclassifications. Note that this is the case for $\mathcal{APR}$ as well; it can not withstand these corruptions whereas $\mathcal{HA^{++}}$ still focuses on where matters, and manages to predict correctly. The fifth row shows another failure mode; despite the corruption, standard model manages to predict correctly but $\mathcal{APR}$ loses its focus and leads to misprediction. $\mathcal{HA^{++}}$ does not break what works; this case visualizes the ability of $\mathcal{HA^{++}}$ to improve clean accuracy. 


\input{sections/table5.tex}  % out-of-distribution



\subsection{Adversarial Robustness}
We present our results on adversarial robustness in Table \ref{tab:adversarial_cifar10}. For a fair comparison, we train models from scratch if official code is available. If not, we use pretrained models or use the results reported in \cite{chen2021amplitude}. We compare against APR, Cutout and FGSM adversarial training \cite{madry2017towards}. 


Our results show that there is no clear winner; with $\mathcal{HA_{S}}$ we obtain the best clean accuracy and with $\mathcal{HA^{++}_{PS}}$ we obtain the best robust accuracy. All our variants are better than the widely accepted adversarial training (AT) baseline in nearly all cases, which shows the effectiveness of our method. Our variants do quite well in clean accuracy and outperform others in nearly all cases. $\mathcal{HA^{++}_{S}}$ offers arguably the best trade-off; it ties with $\mathcal{APR_{PS}}$ on robust accuracy, and outperforms it on clean accuracy.


\subsection{Out-of-Distribution Detection}
For OOD detection, we use a ResNet18 model trained on CIFAR-10 and compare against several configurations, such as training with cross-entropy, SupCLR  \cite{khosla2020supervised} and CSI \cite{tack2020csi}, and augmentation methods as Cutout, Mixup and APR. 

First of all, all our variants comfortably beat the baseline OOD detection (CE), which shows that our proposed method is indeed useful. Furthermore, we see that our proposed methods are highly competitive, and they perform as good as the alternative methods. $\mathcal{HA^{++}_{P} + APR_S}$ outperforms all other methods on LSUN and ImageNet datasets, and produces competitive results on others. Mean AUROC across all datasets show that it ties with the best model $\mathcal{APR_{PS}}$, showing its efficiency. The broader framework we propose leads to many variants with various performance profiles across different datasets, highlighting the flexibility and usefulness of our unification of frequency-centric augmentations. Note that the clean accuracy on CIFAR-10 are provided in Table \ref{tab:cifar10_100_clean_accuracy}, and shows that we perform the same or better than the other methods.

\subsection{Additional results and potential limitations}
\noindent \textbf{ImageNet-$\overline{\text{C}}$.} We also assess our models on ImageNet-$\overline{\text{C}}$ \cite{mintun2021interaction}. The results, given in Table \ref{tab:imagenet_c_bar}, show key insights: we significantly improve over the original standard model and we are just 0.1 shy of $\mathcal{APR_{PS}}$. Training with additional data \cite{hendrycks2021many} helps, and actually puts us ahead of $\mathcal{APR_{PS}}$. An interesting observation is that with higher cut-off frequency (i.e. stronger blur), the performance becomes worse; in ImageNet-C, we observe the opposite. This is potentially due to the different dominant frequency bands in corruptions of ImageNet-C and ImageNet-$\overline{\text{C}}$. 


\noindent \textbf{What about transformers?} We also train a Swin-Tiny \cite{liu2021swin} on ImageNet with and without $\mathcal{HA^{++}_{PS}}$; ImageNet-C results show improvements (59.5 vs 54.8 mCE), but at the expense of slight degradation on clean accuracy (81.2 vs 80.6 top-1). Despite the fundamental differences between transformers and CNNs, especially regarding the frequency bands of the features they tend to learn \cite{benz2021robustness}, it is encouraging to see our methods also work well for transformers. We leave further analyses on transformers for future work.




\input{sections/table6} 

