\section{Method} \label{method_hybridaugment}
In this section, we formally define the problem, motivate our work and then present our proposed techniques.


\subsection{Preliminaries}
Let $\mathcal{F}(x;W)$ be an image classification CNN trained on the training set $\mathcal{T}_\text{train} = (x_{i}, y_{i})^{N}_{i=1}$  with $N$ samples, where $x$ and $y$ correspond to images and labels. The clean accuracy (CA) of $\mathcal{F}(x;W)$ is formally defined as its accuracy over a clean test set $\mathcal{T}_\text{test} = (x_{j}, y_{j})^{M}_{j=1}$. Assume two operators ${A}(\cdot)$ and ${C}(c, s)$ that adversarially attacks or corrupts a given set of images with the corruption category $c$ and severity $s$, respectively.  Let $A\mathcal{T}_\text{test}$ and $C\mathcal{T}_\text{test}$ be the adversarially attacked and corrupted versions of $\mathcal{T}_\text{test}$, and let $\mathcal{F}(x;W)$ have a robust accuracy (RA) on $A\mathcal{T}_\text{test}$ and a corruption accuracy (CRA) on $C\mathcal{T}_\text{test}$. 
The aim is to fit $\mathcal{F}(x;W)$ such that the model gains robustness (\ie. increased RA and CRA compared its the baseline version), while retaining (or improving) the clean accuracy of its baseline version trained without robustness concerns.


\noindent \textbf{What we know.} Our work builds on the following crucial observations: i) CNNs favour high-frequency content \cite{wang2020high}, ii) adversaries and corruptions often reside in high-frequency \cite{wang2020towards}, iii) images are dominated by low-frequency \cite{Saikia_2021_ICCV} and iv) models relying on low-frequency components are more robust \cite{li2022robust,wang2020towards}. The robustness-accuracy trade-off is visible; low-frequency reliant models are more robust, but tend to miss out on clean accuracy brought by the high-frequency components. 

\subsection{HybridAugment}
We hypothesize that a \textit{sweet spot} in the robustness-accuracy trade-off can be found. Unlike the \textit{hard} approaches that completely rule out the reliance on high-frequency components (i.e. low-pass filters), we propose to \textit{reduce} the reliance on them. To this end, we adopt a data augmentation approach that aims to diversify $\mathcal{T}_\text{train}$ by an operation $\mathcal{HA(\cdot)}$. Keeping the strong relation intact between labels and low-frequency content (i.e. labels come from low-frequency-component image), we propose to swap high and low-frequency components of images in a batch on-the-fly. Unlike \cite{mukai2022improving}, we \textit{do not} restrict the images to belong to the same class; this diversifies the training distribution even further while preserving the image semantics. We call this basic version of our approach \textit{HybridAugment}, which corresponds to: 
%
\begin{equation} \label{hybrid_augment_paired}
    \mathcal{HA_{P}}(x_{i}, x_{j}) = \mathcal{LF}(x_{i}) + \mathcal{HF}(x_{j})
\end{equation}
%
where $x_{i}$ is the input image and $x_{j}$ is a randomly sampled image from the whole training set, which we simply sample from the mini batch at each training iteration in practice. $\mathcal{HF}$ and $\mathcal{LF}$ operators select the high and low-frequency components of an input image, for which we use:
%
\begin{equation} \label{eq:cutoff}
\begin{split}
    \mathcal{LF}(x) = GaussBlur(x) \\
    \mathcal{HF}(x) = x - \mathcal{LF}(x)
    \end{split}
\end{equation}
%
where $GaussBlur$ is used as a low-pass filter. Note that a similar outcome is possible by using Discrete Fourier Transforms (DFT), swapping the frequency bands and then applying Inverse DFT (IDFT). We find the gaussian blur operation to be faster and better in practice. 


Inspired from \cite{chen2021amplitude}, in addition to the image-pair scheme in Eq.~\ref{hybrid_augment_paired}, we propose a single image variant of \textit{HybridAugment}. In the single image variant, instead of combining two images, $x_i$ and $x_{j}$ are obtained by applying randomly sampled augmentations to a single image. The single image variant $\mathcal{HA_{S}}$ can therefore be defined as 
%
\begin{equation} \label{hybrid_augment_single}
    \mathcal{HA_{S}}(x_{i}) = \mathcal{LF}(Aug(x_{i})) + \mathcal{HF}(\hat{Aug}(x_{i}))
\end{equation}
%
where $Aug$ and $\hat{Aug}$ correspond to two sets of randomly sampled augmentation operations. Note that paired and single versions can work in tandem ($\mathcal{HA_{PS}}$), and actually outperform single or paired image versions. 


\subsection{HybridAugment++}


The frequency analysis is a vast literature, however, two core aspects often stand out; frequency-band analysis (i.e. low, high) and the decomposition of signals into amplitude and phase. \textit{HybridAugment} covers the former and shows competitive results in various benchmarks (see Section \ref{sec:exp_hybridaugment}). The latter is investigated in $\mathcal{APR}$ \cite{chen2021amplitude}, where phase is shown to be the more relevant component for correct classification, and training models based on their phase labels and swapping amplitude components of images randomly lead to more robust models. Note that frequency-band and phase/amplitude discussions are arguably orthogonal, since frequency, phase and amplitude provide distinct characterizations of a signal: intuitively speaking, frequency, phase and amplitude can be seen as the separation of visual patterns in terms of scale, location and significance. 


We hypothesize these two approaches can be complementary; a model reliant on low-frequency and spatial information (i.e. phase) can further improve robustness. Inspired by the successes of cascaded augmentation methods \cite{hendrycks2019augmix,wang2021augmax,calian2022defending}, we unify these two core aspects into a single, hierarchical augmentation method. We refer to this method as \textit{HybridAugment++} and define its paired version as:
%
\begin{equation}
  \mathcal{HA_{P}}^{++}(x_{i}, x_{j}, x_{z}) = \mathcal{APR_{P}}(\mathcal{LF}(x_{i}), x_{z}) + \mathcal{HF}(x_{j})
\end{equation}
%
where $x_{i}$, $x_{j}$ and $x_{z}$ are images sampled from the same batch. Here, $\mathcal{APR_{P}}$~\cite{chen2021amplitude} is defined as
\begin{equation}
    \mathcal{APR_{P}}(x_{i}, x_{z}) = \mathcal{IDFT}(A_{x_{z}} \otimes e^{i. P_{x_{i}}}) \\
\end{equation}
%
where $\otimes$ is element-wise multiplication, $A$ is the amplitude and $P$ is the phase component. Similar to $\mathcal{HA}$ and $\mathcal{APR}$, we also define a single-image version of \textit{HybridAugment++} as
%
\begin{equation}
 \mathcal{HA_{S}}^{++}(x_{i}) = \mathcal{APR_{S}}(\mathcal{LF}(Aug(x_{i}))) + \mathcal{HF}(\hat{Aug}(x_{i}))
\end{equation}
%
where $\mathcal{APR_{S}}$~\cite{chen2021amplitude} is defined as
%
\begin{equation}
\mathcal{APR_{S}}(x_{i}) = \mathcal{IDFT}\left(A_{\bar{Aug}(x_{i})} \otimes e^{i. P_{\overline{Aug}\left(x_{i}\right)}}\right)    
\end{equation}
%
where $Aug$, $\hat{Aug}$, $\bar{Aug}$ and $\overline{Aug}$ are different sets of randomly sampled augmentation operations. Note that we essentially propose a framework; one can use different single and paired image augmentations, either individually or together, and can still achieve competitive results (see ablations in Section \ref{sec:exp_hybridaugment}). There are also other alternatives, such as swapping phase/amplitude first and then performing $\mathcal{HA}$, but we observe poor performance in practice; dividing the phase component into frequency-bands is not interpretable as frequencies of the phase component are not well defined. The pseudo-code of our methods can be found in the supplementary material.



