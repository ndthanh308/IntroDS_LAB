\section{Related Work} \label{relwork_hybridaugment}

\noindent \textbf{Robust Generalization - Adversarial.} Adversarial ML has been studied intensively \cite{szegedy2013intriguing,yucel2020deep}, resulting into numerous attack \cite{szegedy2013intriguing,moosavi2016deepfool,goodfellow2014explaining} and defense \cite{madry2017towards,shaham2018defending,borkar2020defending,lu2017safetynet} methods borne out of an arms race that is still very much active. Notable attacks include FGSM \cite{goodfellow2014explaining}, DeepFool \cite{moosavi2016deepfool}, C\&W \cite{carlini2017towards} where AutoAttack \cite{croce2020reliable} is now a widely used attack for adversarial evaluation. The defense methods mainly diversify the training distribution with attacked images \cite{madry2017towards,zhang2020geometry}, purify the adversarial examples \cite{shaham2018defending,meng2017magnet} or detect whether an image is adversary or not \cite{xu2017feature,lu2017safetynet}.  

\noindent \textbf{Robust Generalization - Corruptions.} Common image corruptions might have various causes, and they occur more frequently than adversaries in practice. Numerous datasets simulating these effects have been released to facilitate standard evaluations \cite{hendrycks2019augmix,mu2019mnist,kar20223d,yucel2022robust}. The methods addressing corruption robustness can be largely divided into two; architecture-centric and data-centric methods. Architecture-centric methods include neural architecture search for robust architectures \cite{mok2021advrush}, focusing on subnets \cite{subnet_enhance}, rectifying batch normalization \cite{Benz_2021_WACV}, wavelet based layers \cite{li2020wavelet} and forming ensembles \cite{Saikia_2021_ICCV,Yeo_2021_ICCV}. The data-centric methods are arguably more prominent in the literature; adversarial training \cite{madry2017towards,adversarialtraining_corruption}, cascade augmentations \cite{hendrycks2019augmix,wang2021augmax}, augmentation networks \cite{ant_corruption,calian2022defending}, learned augmentation policies \cite{yin2019fourier}, shape-bias injection \cite{shapebias_corruption,Sun_2021_ICCV}, style augmentation \cite{geirhos2018imagenet}, fractals \cite{Hendrycks_2022_CVPR}, soft-edge driven image blending \cite{Lee_2020_CVPR_Workshops} and max-entropy image transformations \cite{prime_aug} are all shown to improve corruption robustness at varying degrees.


\noindent \textbf{Robust Generalization - Frequency Aspect.} The generalization of neural networks have been analysed extensively. Specificially, several frequency-centric studies show that CNNs tend to rely on high-frequency information ignored by human vision \cite{wang2020high}, or rely more on amplitude component than phase component humans tend to favour \cite{chen2021amplitude}. Models trained on high-pass filtered images are shown to have higher accuracy than the models trained on low-pass filtered images, although high-pass filtered images are just random noise to humans \cite{yin2019fourier}. Multiple studies confirm that models reliant on low-frequency components are more robust \cite{wang2020towards,li2022robust}. Interestingly, frequency analyses presents a different interpretation of the robustness-accuracy trade-off; many methods that improve clean accuracy force networks to rely on high-frequency components, which might sacrifice robustness \cite{wang2020high}. 

\noindent \textbf{Robust Generalization - Frequency-Centric Methods.} A trade-off in frequency-based data augmentations is that one should not sacrifice the other; training on high-frequency augmentations can improve robustness to high-frequency corruptions, but tend to sacrifice the low-frequency corruption robustness or the clean accuracy \cite{Saikia_2021_ICCV,yin2019fourier,chan2022does}. Frequency-centric methods include biasing Jacobians \cite{chan2022does}, swapping phase and amplitude of random images \cite{chen2021amplitude}, perturbing phase and amplitude spectra along with consistency regularization \cite{frequencyaug_competitor_method}, frequency-band expert ensembles \cite{Saikia_2021_ICCV}, frequency-component swapping of same-class samples \cite{mukai2022improving} and wavelet-denoising layers \cite{li2020wavelet}. Note that there is a considerable literature on frequency-centric adversarial attacks, but we primarily focus on methods improving robustness.

A similar work is \cite{mukai2022improving}, where hybrid-image based augmentation is proposed. We have, however, several key advantages; we i) lift the restriction of sampling from same classes for augmentation, ii) propose both single and paired variants, leading to a significantly more diverse training distribution, iii) present \textit{HybridAugment++} that performs phase/amplitude swap specifically in low-frequency components and iv) report improvements on corruption and adversarial robustness, as well as clean accuracy on multiple benchmark datasets (CIFAR-10/100, ImageNet). Note that other methods either train with ImageNet-C corruptions \cite{Saikia_2021_ICCV}, report only corruption results \cite{frequencyaug_competitor_method}, rely on external data \cite{Hendrycks_2022_CVPR} or models \cite{calian2022defending}. Our methods, on the other hand, require no external models or data, and they can be plugged into existing pipelines easily due to their simplicity.




