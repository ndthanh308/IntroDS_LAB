\section{Supplementary Material}
\input{sections/suppmat_sections/table_cifar_1}
\input{sections/suppmat_sections/table_cifar_2}

\subsection{State of the art comparison on CIFAR-C}

In the main text, we provide detailed ablations on CIFAR10/100-C in the form of corruption robustness evaluation. Due to space limitations, we could not provide state-of-the-art results there; we provide these results here. We compare ourselves with methods which share our characteristics; no additional data or models to be used. We choose CutOut \cite{devries2017improved}, Mixup \cite{zhang2017mixup}, CutMix \cite{yun2019cutmix}, adversarial training (AT) \cite{madry2017towards}, AutoAugment (AA) \cite{cubuk2018autoaugment}, Augmix \cite{hendrycks2019augmix} and APR \cite{chen2021amplitude}. We take the results of these methods from \cite{chen2021amplitude}; we do not include CIFAR-100 clean accuracy results or ResNet18 results here since they are not available.

\noindent \textbf{Corruption Robustness.} Table \ref{tab:supmat_table1} shows mCE values of other methods, as well as the best results provided in Table 1 of the main text. The inclusion of the state-of-the-art methods do not change the takeaway message; $\mathcal{HA^{++}_{PS}}$ comfortably outperforms others on all datasets and architectures. Note that all variants of $\mathcal{HA}$ and $\mathcal{HA^{++}}$ either outperform or are competitive to all state-of-the-art methods.

\noindent \textbf{Clean Accuracy.} Table \ref{tab:suppmat_tab_2} shows clean accuracy values of other methods, as well as the best results provided in Table 2 of the main text. $\mathcal{HA^{++}_{PS}}$ outperforms all other state-of-the-art methods, and the best CIFAR-10 result comes with $\mathcal{APR_{P}} + \mathcal{HA^{++}_{S}}$.  Note that the best result on CIFAR-100 comes with $\mathcal{APR_{P}} + \mathcal{HA_{S}}$, which shows the effectiveness of our proposed methods.


\subsection{More on $\mathcal{HA}$ and $\mathcal{HA^{++}}$}
We provide the pseudo-code of $\mathcal{HA^{++}_{P}}$ and $\mathcal{HA_{P}}$ in Algorithm \ref{fig:pseudo_code}. Also provided is the pseudo-code for $\mathcal{HA^{++}_{S}}$ and $\mathcal{HA_{S}}$ in Algorithm \ref{fig:pseudo_code_2}. Our code and pretrained models will be made publicly available. 



Note that in Algorithm \ref{fig:pseudo_code_2}, we decompose into low and high frequency bands both augmented images (lines 22-23 and 25-26), and also amplitude-phase swap low-frequency bands (\verb|lfc_f| and \verb|lfc_s|) of both augmented images (lines 42 and 56). We then randomize the selection of which low/high frequency components will come from which image for the final result (lines 58 to 63). Figure 1 of the main text shows a simplified version of this, where only the execution of line 61 is shown. In practice, we use the implementation provided in Algorithm \ref{fig:pseudo_code_2}.



\subsection{Detailed results - transformer}
We provide the detailed results of our corruption robustness experiments with Swin-Tiny \cite{liu2021swin}. The result in Table \ref{tab:suppmat_swin}
 shows that $HA^{++}_{PS}$ consistently improves on all types of corruptions, regardless of their frequency characteristics. 

\clearpage 

\input{sections/algorithm_2}
\input{sections/algorithm}
\input{sections/suppmat_sections/table_swin_imagenet}


\subsection{Related work continued}
The robustness literature is vast, and it is difficult to cover all methods, therefore in the main text we opted to cover and compare ours against the most relevant ones (i.e. frequency-centric augmentations). Here, we discuss additional, more recent methods.

We focus on recent methods, such as \cite{calian2022defending,wang2021augmax,Saikia_2021_ICCV,prime_aug}. \cite{calian2022defending} uses an extra model to generate new training samples, which makes the method significantly more complex than ours. Despite this added complexity, we outperform it on ImageNet-C without extra data (75.03 vs 65.8 mCE) and with extra data (62.9 vs 58.9 mCE), even though they use additional augmentations (i.e. AugMix). \cite{wang2021augmax} extends AugMix by making parts of the cascade augmentation pipeline learnable. We outperform it on CIFAR-10/100-C on all architectures. Note that we could not compare against them  on ImageNet-C as they use a different architecture (i.e. ResNet18). \cite{Saikia_2021_ICCV} outperforms us on ImageNet, but it uses model ensembles during training, which are finetuned on some of the test-time corruptions of ImageNet-C (i.e. noise and blur finetuning for high-frequency model, contrast finetuning for low-frequency model). We believe this violates the assumption of not using test-time corruptions in training. PRIME \cite{prime_aug} mixes several max-entropy transforms to augment the training distribution. We outperform it on CIFAR-10/100, are competitive on ImageNet-$\bar{C}$ but behind on ImageNet-C. Despite its results, PRIME has three key disadvantages compared to our method; it i) requires per-dataset hyperparameter tuning for its transforms, ii) manual tuning of these parameters are required to preserve semantics after augmentation and iii) shows that their augmented images look similar to test-time corruptions, which might be (inadvertently) violating the assumption of not using test-time corruptions in training.


\subsection{Adversarial robustness on ImageNet}
We evaluate ResNet-50 models trained with $\mathcal{HA^{++}_{PS}}$, $\mathcal{APR_{PS}}$ and standard training.  We use the model checkpoints shown in Table \ref{tab:imagenet_results}; we do not train new models. Table \ref{tab:adv_imagenet} shows $\mathcal{HA^{++}_{PS}}$ improves robust and clean accuracy (RA, CA) on ImageNet, and comfortably outperforms our baseline. Note that we use a smaller $\epsilon=1/255$ value, as higher epsilon evaluation would require adversarial (re)training. 

\begin{table}[h]
\centering
\resizebox{0.2\textwidth}{!}{%
\setlength\tabcolsep{1.5pt} 
\begin{tabular}{c|c|c|c}
 & Orig. &  $\mathcal{APR_{PS}}$ & $\mathcal{HA^{++}_{PS}}$   \\  \hline
CA & 76.10 & 75.60  & \textbf{76.30} \\
RA & 51.02 &  54.22  & \textbf{56.44} 
\end{tabular}%
}
\caption{AutoAttack results.}
\label{tab:adv_imagenet}
\end{table}



\subsection{Transfer learning performance}

As reported in \cite{salman2020adversarially}, robust models tend to transfer better to downstream tasks. In the same vein, we perform a wide range of finetuning experiments, where a standard ResNet50 and $\mathcal{HA^{++}_{PS}}$-trained ResNet50 are finetuned on various datasets by changing the final layer. Note that we do not train new models; we use the model checkpoints shown in Table \ref{tab:imagenet_results}. Table \ref{tab:transfer_learning} shows we comfortably outperform standard training on majority of other classification tasks. This shows the transferability of the features learned by our augmentation schemes.
  
\vspace{7mm}
\begin{table}[h]
\centering
\resizebox{0.45\textwidth}{!}{%

\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
 \begin{rotate}{60} CIFAR10 \end{rotate} 
 & \begin{rotate}{60} CIFAR100 \end{rotate}  
 & \begin{rotate}{60} Aircraft \end{rotate} 
 & \begin{rotate}{60} CTech101 \end{rotate} 
 & \begin{rotate}{60} DTD \end{rotate} 
 & \begin{rotate}{60} Flowers \end{rotate} 
 & \begin{rotate}{60} Pets \end{rotate} 
 & \begin{rotate}{60} CTech256 \end{rotate}  
 & \begin{rotate}{60} Birds \end{rotate} 
 & \begin{rotate}{60} Cars \end{rotate}  
 & \begin{rotate}{60} SUN \end{rotate}  
 & \begin{rotate}{60} Food \end{rotate} \\ \hline

 96.8 & 83.4 & \textbf{86.6} & \textbf{94.0} & 74.1 & 96.3 & 93.2 & 81.5 & 73.6 & 90.9 & 62.1 & \textbf{87.5} \\ 
 \textbf{97.4} & \textbf{84.9} & 84.5 & 92.7 & \textbf{75.1} & \textbf{96.8} & \textbf{93.3} & \textbf{83.0} & \textbf{73.7} & \textbf{91.0} & \textbf{63.3} & 87.4
\end{tabular}%
}
\caption{Transfer learning acc. (top-1) of standard  ResNet50 (top) and $\mathcal{HA^{++}_{PS}}$ (bottom) on 12 other classification datasets. }
\label{tab:transfer_learning}
\vspace{-1mm}
\end{table}
% \end{wraptable}