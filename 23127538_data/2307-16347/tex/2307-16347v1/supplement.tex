%\documentclass[aps,pra,superscriptaddress,showpacs,onecolumn,draft]{revtex4}
\documentclass[aps,prl,onecolumn,superscriptaddress]{revtex4}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{epstopdf}
%\usepackage{subfigure}
%\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{theorem}
\usepackage{bm}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{csquotes}
\MakeOuterQuote{"}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\thealgorithm}{\!\!:}
\usepackage{dcolumn}
\usepackage{color}
%\usepackage[colorlinks,citecolor=blue]{hyperref}
%\usepackage{authblk}
%\usepackage{cite}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\usepackage{graphicx}


\newcommand{\red}{\color{red}}
\newcommand{\blu}{\color{blue}}
\newcommand{\blk}{\color{black}}
\definecolor{ngreen}{rgb}{0.2,0.6,0.2}
\newcommand{\grn}{\color{ngreen}}
\newcommand{\hmw}[1]{{\color{ngreen} \bf [[{#1}]]}}
\newcommand{\dwb}[1]{{\color{blue} \bf [[{#1}]]}}
\definecolor{ngold}{rgb}{0.7,0.6,0.2}
\newcommand{\gold}{\color{ngold}}
\newcommand{\blh}[1]{{\color{ngold} \bf [[{#1}]]}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% define mathematical words via abbreviations.

\def\vec#1{\mathbf{#1}} %% overiding the original command
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\Tr}{\operatorname{Tr}}
\providecommand{\det}{\operatorname{det}}
\newcommand{\Det}{\operatorname{Det}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\ad}{\operatorname{ad}}
\newcommand{\rep}{\mathrel{\widehat{=}}}
\newcommand{\rmi}{\mathrm{i}}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\rmE}{\mathrm{E}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\rmT}{\mathrm{T}}
\newcommand{\imply}{\mathrel{\Rightarrow}}
\newcommand{\equi}{\mathrel{\Leftrightarrow}}

\newcommand{\one}{\overline{1}}
\newcommand{\zero}{\overline{0}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\be}{\begin{equation}}
	\newcommand{\ee}{\end{equation}}
\newcommand{\ba}{\begin{align}}
	\newcommand{\ea}{\end{align}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\<{\langle}  %% overiding the original command \<
\def\>{\rangle}  %% overiding the original command \>
\newcommand{\ket}[1]{| #1\>}
\newcommand{\bra}[1]{\< #1|}
\newcommand{\dket}[1]{| #1\>\!\>}
\newcommand{\Dket}[1]{\Bigl| #1\Bigr\>\!\Bigr\>}
\newcommand{\dbra}[1]{\<\!\< #1|}
\newcommand{\Dbra}[1]{\Bigl\<\!\Bigl\< #1\Bigr|}
\newcommand{\inner}[2]{\<#1|#2\>}
\def\outer#1#2{|#1\>\<#2|}       %% overiding the original command \outer
\newcommand{\dinner}[2]{\<\!\< #1| #2\>\!\>}
\newcommand{\Dinner}[2]{\Bigl\<\!\Bigl\< #1\Bigl| #2\Bigr\>\!\Bigr\>}
\newcommand{\douter}[2]{| #1\>\!\>\<\!\< #2|}
\newcommand{\Douter}[2]{\Bigl| #1\Bigr\>\!\Bigr\>\Bigl\<\!\Bigl\< #2\Bigr|}
\newcommand{\norm}[1]{\parallel\!#1\!\parallel}
\newcommand{\nfrac}[2]{\genfrac{}{}{0pt}{}{#1}{#2}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abbreviations used in quantum estimation theory
\newcommand{\mse}{\mathcal{E}}
\newcommand{\mhs}{\mathcal{E}_{\mathrm{HS}}}
\newcommand{\msh}{\mathcal{E}_{\mathrm{SH}}}
\newcommand{\msb}{\mathcal{E}_{\mathrm{SB}}}
\newcommand{\mtr}{\mathcal{E}_{\tr}}
\newcommand{\barcal}[1]{\bar{\mathcal{#1}}}
\newcommand{\bt}{\bar{t}}
\newcommand{\bid}{\bar{\mathbf{I}}}
\newcommand{\cF}{\mathcal{F}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abbreviations used in cross references and citations

%\def\eqref#1{(\ref{#1})}    %% overiding the original command \eqref
%\newcommand{\eref}[1]{Eq.~(\ref{#1})}
%\newcommand{\Eref}[1]{Equation~(\ref{#1})}
%\newcommand{\esref}[1]{Eqs.~(\ref{#1})}
%\newcommand{\Esref}[1]{Equations~(\ref{#1})}

%\def\eqref#1{\textup{(}\ref{#1}\textup{)}}  %% overiding the original command \eqref
%\newcommand{\eref}[1]{Eq.~\textup{(}\ref{#1}\textup{)}}
%\newcommand{\Eref}[1]{Equation~\textup{(}\ref{#1}\textup{)}}
%\newcommand{\esref}[1]{Eqs.~\textup{(}\ref{#1}\textup{)}}
%\newcommand{\Esref}[1]{Equations~\textup{(}\ref{#1}\textup{)}}


\def\eqref#1{\textup{(\ref{#1})}}  %% overiding the original command \eqref
\newcommand{\eref}[1]{Eq.~\textup{(\ref{#1})}}
\newcommand{\Eref}[1]{Equation~\textup{(\ref{#1})}}
\newcommand{\esref}[1]{Eqs.~\textup{(\ref{#1})}}
\newcommand{\Esref}[1]{Equations~\textup{(\ref{#1})}}


\newcommand{\fref}[1]{Fig.~\ref{#1}}
\newcommand{\Fref}[1]{Figure~\ref{#1}}
\newcommand{\fsref}[1]{Figs.~\ref{#1}}
\newcommand{\Fsref}[1]{Figures~\ref{#1}}

\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}
\newcommand{\tsref}[1]{Tables~\ref{#1}}
\newcommand{\Tsref}[1]{Tables~\ref{#1}}

\newcommand{\sref}[1]{Sec.~\ref{#1}}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newcommand{\ssref}[1]{Secs.~\ref{#1}}
\newcommand{\Ssref}[1]{Sections~\ref{#1}}


\newcommand{\thref}[1]{Theorem~\ref{#1}}    
\newcommand{\Thref}[1]{Theorem~\ref{#1}}
\newcommand{\thsref}[1]{Theorems~\ref{#1}}
\newcommand{\Thsref}[1]{Theorems~\ref{#1}}

\newcommand{\lref}[1]{Lemma~\ref{#1}}
\newcommand{\Lref}[1]{Lemma~\ref{#1}}
\newcommand{\lsref}[1]{Lemmas~\ref{#1}}
\newcommand{\Lsref}[1]{Lemmas~\ref{#1}}

\newcommand{\crref}[1]{Corollary~\ref{#1}}
\newcommand{\Crref}[1]{Corollary~\ref{#1}}
\newcommand{\crsref}[1]{Corollaries~\ref{#1}}
\newcommand{\Crsref}[1]{Corollaries~\ref{#1}}


\newcommand{\cref}[1]{Conjecture~\ref{#1}}
\newcommand{\Cref}[1]{Conjecture~\ref{#1}}
\newcommand{\csref}[1]{Conjectures~\ref{#1}}
\newcommand{\Csref}[1]{Conjectures~\ref{#1}}

\newcommand{\chref}[1]{Chapter~\ref{#1}}
\newcommand{\Chref}[1]{Chapter~\ref{#1}}
\newcommand{\chsref}[1]{Chapters~\ref{#1}}
\newcommand{\Chsref}[1]{Chapters~\ref{#1}}

\newcommand{\aref}[1]{Appendix~\ref{#1}}
\newcommand{\asref}[1]{Appendices~\ref{#1}}
\newcommand{\Aref}[1]{Appendix~\ref{#1}}
\newcommand{\Asref}[1]{Appendices~\ref{#1}}

\newcommand{\rcite}[1]{Ref.~\cite{#1}}
\newcommand{\rscite}[1]{Refs.~\cite{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Supplumental Material}
\maketitle 
In the supplumental material we will discuss about following things:

1.The iterative method to find the global optimal adaptive strategy.

2.The POVM in GOAL strategy has no more than three elements. 

3.The analytical expression of GOAL strategy for pure-states discrimination .
\section{1. The iterative method to find the optimal local adaptive strategy}
In the main text, we have defined the function array which satisfies the iterative relationship:
\begin{equation}
N_{i+1}(q)=\left\{\begin{array}{c}
0 \quad \text { if } \min \left(q, 1-q\right) \leq \varepsilon\\
\min \limits_{n,\left\{M_k\right\} \in D_n}[n+\sum\limits_k P_k N_i\left(q_k\right)] \text { otherwise }
\end{array}\right.
\end{equation}
And $N_1\left(q\right)$ should be a self-set and realizable function. Then we have $\forall q \in[0,1], N_1\left(q\right) \geq N_{\mathrm{GOA}}\left(q\right)$ and $N_1\left(q\right)$ is bounded on $[0,1]$. As $N_{\mathrm{GOA}}\left(q\right)$ is the minimum average consumption, we have $\forall q \in[0,1], \forall N \in \mathrm{Z}^{+}, N_i\left(q\right) \geq$ $N_{\mathrm{GOA}}\left(q\right)$. There is at least one strategy to achieve $N_{\mathrm{GOA}}\left(q\right)$, and we can set one of them as 
$\left\{M_k\right\}_{\mathrm{GOA}}(q)$. Then we define a function array $\left\{N_i^{\prime}\left(q\right)\right\}$ which satisfies
	$N_1^{\prime}\left(q\right)=N_1\left(q\right)$  and 
\begin{equation}
 N_{i+1}^{\prime}\left(q\right)=\left\{\begin{array}{c}
0 \quad \text { if } \min \left(q, 1-q\right) \leq \varepsilon\\
\sum\limits_{\left\{M_k\right\}=\left\{M_k\right\}_{\mathrm{GOA}}(q),k}\left[ n+P_k N_i^{\prime}\left(q_k\right)\right] \text { otherwise }
\end{array}\right.
\end{equation}
The difference between Eq.1 and Eq.2 is that in Eq.1 every round we choose the measurement which can minimize $n+\sum\limits_k P_k N_i\left(q_k\right)$, while in Eq.2 every round we choose the measurement which can minimize $n+\sum\limits_k P_k N_{\mathrm{GOA}}(q_k)$.Thus we have $\forall i \in \mathrm{Z}^{+}, \forall q \in[0,1], N_i^{\prime}\left(q\right) \geq N_i\left(q\right)$. 

Due to $N_i^{\prime}\left(q\right)$ means the average copy consumption realized by doing the global optimal adaptive strategy in the first $i-1$ steps and if the first $i-1$ steps don't achieve the error rate requirement, doing the strategy corresponding to $N_1\left(q\right)$, we have
\begin{equation}
	\lim _{i \rightarrow \infty} N_i^{\prime}\left(q\right)=N_{\mathrm{GOA}}\left(q\right)
\end{equation}
As $\forall i \in \mathrm{Z}^{+}, \forall q \in[0,1], \quad N_{\mathrm{GOA}}\left(q\right) \leq N_i\left(q\right) \leq N_i^{\prime}\left(q\right)$ and $\lim\limits_{i \rightarrow \infty} N_i^{\prime}\left(q\right)=N_{\mathrm{GOA}}\left(q\right)$, finally we have 
\begin{equation}
\lim\limits_{i \rightarrow \infty} N_i\left(q\right)=N_{\mathrm{GOA}}\left(q\right)
\end{equation}
\section{2. The POVM in GOAL strategy has no more than three elements}
When searching the global optimal adaptive local strategy, our task is to calculate the function array $\{N_i\}$ which satisfy following iterative relationship:
\begin{equation}
			N_{\mathrm{i+1}}\left(q\right)=\left\{\begin{array}{c}
				0 \qquad \text { if } \min \left(q, 1-q\right) \leq \varepsilon \\
				1+\min\limits _{S \in \mathcal{D}} \int_0^\pi P_\theta N_{i}\left(q_\theta\right) d\theta \ \text {otherwise}
			\end{array}\right.
		\end{equation}
and the key problem of the calculation is how to find the strategy to minimize the integral shown in Eq.5:
\begin{equation}
	I=\int_0^\pi P_\theta N_{i}\left(q_\theta\right) d\theta\equiv \int_0^\pi S\left(\theta, q\right) \operatorname{tr}\left\{\left|\psi_\theta\right\rangle\left\langle\psi_\theta\right|\left[q \rho_0+\left(1-q\right) \rho_1\right]\right\} N_i\left(q_\theta\right) \mathrm{d} \theta
\end{equation}
Where $\left|\psi_\theta\right\rangle \equiv \cos \theta|0\rangle+\sin \theta|1\rangle$.

Here $q$ is given, so we can define $f(\theta) \equiv S\left(\theta, q\right)$ and $g(\theta) \equiv \operatorname{tr}\left\{\left|\psi_\theta\right\rangle\left\langle\psi_\theta\right|\left[q \rho_0+\left(1-q\right) \rho_1\right]\right\} N_i\left(q_\theta\right), \theta \in[0, \pi)$. Then $I=\int_0^\pi f(\theta) g(\theta) d \theta$. Due to the constraint $\int_0^\pi f(\theta) \cos (2 \theta) d \theta=0 \quad$ and $\int_0^\pi f(\theta) \sin (2 \theta) d \theta=0, \forall a, b \in \mathrm{R}$, we have $\int_0^\pi f(\theta) g(\theta) d \theta=\int_0^\pi f(\theta)[g(\theta)+a\cos(2 \theta)+b \sin (2 \theta)] d \theta$. Thus, if we can find $a$ and $b$ which satisfy one of two following conditions:\\
Condition 1: $g(\theta)+a \cos (2 \theta)+b \sin (2 \theta)$ take the minimum value at two points $\theta_1$ and $\theta_2$ and these two points satisfy $\left|\theta_1-\theta_2\right|=\frac{\pi}{2}$.\\
Condition 2: $g(\theta)+a \cos (2 \theta)+b \sin (2 \theta)$ take the minimum value at three points $\theta_1, \theta_2$ and $\theta_3\left(\theta_1<\theta_2<\right.$ $\theta_3$ ) and these three points satisfy $\theta_1>\theta_2-\frac{\pi}{2}$ and $\theta_2>\theta_3-\frac{\pi}{2}$ and $\theta_3>\theta_1+\frac{\pi}{2}$.\\
We can immediately obtain the $f(\theta)$ which minimizes the integral :\\
$$f(\theta)=\left\{\begin{array}{cl}
	\delta\left(\theta-\theta_1\right)+\delta\left(\theta-\theta_2\right) & \text {if} \ a \ \text{and} \ b \text { satisfy \ Condition \ } 1\\
	\frac{2 \sin \left[2\left(\theta_2-\theta_1\right)\right] \delta\left(\theta-\theta_3\right)+2 \sin \left[2\left(\theta_3-\theta_2\right)\right] \delta\left(\theta-\theta_1\right)+2 \sin \left[2\left(\theta_1-\theta_3\right)\right] \delta\left(\theta-\theta_2\right)}{\sin \left[2\left(\theta_2-\theta_1\right)\right]+\sin \left[2\left(\theta_3-\theta_2\right)\right]+\sin \left[2\left(\theta_1-\theta_3\right)\right]} & \text {if} \ a \ \text{and} \ b \text { satisfy \ Condition \ } 2
\end{array}\right.$$\\

for all the actual local measurement devices in the world, no matter how precise, strictly speaking, their measurement angle $\theta$ can only be taken discretely. Thus we can uniformly discretizate the function $g(\theta)$ into an array with very short step distance for $\theta$ and then we can use following method to find $a$ and $b$:
\begin{enumerate}
\item Step1:Let $g(\theta)+a \cos (2 \theta)+b \sin (2 \theta)$ take the minimum value at two points $\alpha_1$ and $\alpha_2$.\\
Set $a_0=g(\frac{\pi}{2})-g(0)$ and set $g_0(\theta)=g(\theta)+\frac{a_0}{2}\cos(2\theta)$. Then we have $g_0(0)=g_0(\frac{\pi}{2})$.\\
Now  we can set $g_1(\theta)=g_0(\theta)+b_1\sin(2\theta)$. And we can found that when $b\rightarrow-\infty$ the minimum value point of $g_1$ must in the range $[0, \frac{\pi}{2})$, and when $b\rightarrow+\infty$ the minimum value point of $g_1$ must in the range $[\frac{\pi}{2},\pi)$. As $g_1(0)=g_1(\frac{\pi}{2})$,it is easy to prove that there must exist $\xi\in R$, when $b_1=\xi$, $g_1(\theta)$ will have at least one minimum value point in the range $[0,\frac{\pi}{2})$ and will have at least one another minimum value point in the range $[\frac{\pi}{2},\pi)$. Now we can set $b_1=\xi$ and $g_1(\theta)=g_0(\theta)+b_1\sin(2\theta)$ and then finish Step1. 

\item Step2:In the Step1, we have generate $g_1(\theta)$ which has at least 2 minimum value points. If we can directly find the minimum value points which satisfy Condition 1 or Condition 2, we can finish the search.Otherwise,

(1) if $g_1(\theta)$ has only two minimum value points, set these two points as $\theta_1$ and $\theta_2 .\left(\theta_2>\theta_1\right)$

(2)If $g_1(\theta)$ has more than two minimum value points, then select $\theta_1$ and $\theta_2$ in the following order:

A. If there exist $\alpha_i$ and $\alpha_j$ which are the minimum value points of $g_1(\theta)$ and satisfy $\left|\alpha_i-\alpha_j\right|>\frac{\pi}{2}$ we select $\theta_1$ and $\theta_2$ which are the minimum value points and satisfy $\theta_2-\theta_1=\min \limits_{\alpha_i, \alpha_j,\left|\alpha_i-\alpha_j\right|>\frac{\pi}{2}}\left|\alpha_i-\alpha_j\right|$.

B. If the judgement in A is false, select $\theta_1$ and $\theta_2$ which are the minimum value points and satisfy
$$
\theta_2-\theta_1=\max _{\alpha_i, \alpha_j}\left|\alpha_i-\alpha_j\right|
$$

\item Step3:
After Step 2, if $\theta_2-\theta_1<\frac{\pi}{2}$, we do as follow:
Conduct transformation $g_1(\theta) \rightarrow g_1(\theta)+A \cdot \cos [2\left(\theta-\frac{\theta_2+\theta_1}{2}\right)]$, where $A=\min\limits_{\theta\in [0,\theta_1)\cup (\theta_2,\pi)}\frac{g(\theta)-g(\theta_1)}{-\cos (\theta-\theta_1-\theta_2)+cos(\theta_1-\theta_2)}$.
As $g(\theta)$ has been discretization into an array, we can always find a positive $A$.
This transformation will generate at least one more minimum value point $\theta_3$ in the regine $[0,\theta_1)\cup (\theta_2,\pi)$,while $\theta_1$ and $\theta_2$ are still minimum value points. Then we should do as follow:

A. If there exists minimum value points which can satisfy Condition 1 or Condition 2, we can finish the search.

B. If all the minimum value points can't satisfy Condition 1 or Condition 2, do as follow:

B.(1) If there exist two minimum value points $\theta_i$ and $\theta_j$ which satisfy $\theta_i-\theta_j>\frac{\pi}{2}$, we select $\theta_1^{\prime}$ and $\theta_2^{\prime}$ which are the minimum value points and satisfy $\theta_2^{\prime}-\theta_1^{\prime}=\min \limits_{\theta_i ,\theta_j,\left|\theta_i-\theta_j\right|>\frac{\pi}{2}}\left|\theta_i-\theta_j\right|$. Then replace $\theta_1$ and $\theta_2$ by $\theta_1^{\prime}$ and $\theta_2^{\prime}$, finish Step 3.

B.(2) If the judgement in B.(1) is false, select $\theta_1^{\prime}$ and $\theta_2^{\prime}$ which are the minimum value points and satisfy
$\theta_2^{\prime}-\theta_1^{\prime}=\max \limits_{\theta_i, \theta_j}\left|\theta_i-\theta_j\right|$. Then replace $\theta_1$ and $\theta_2$ by $\theta_1^{\prime}$ and $\theta_2^{\prime}$, repeat Step 3.
As every time after repeating Step3, if the judgement in B.(1) is false, the value of $\theta_2-\theta_1$ will increase and be closer to $\frac{\pi}{2}$, Step3 can always finish with a finite number of repetitions.

\item Step4: After doing Step1,2 and 3, either we have realized the search or $\theta_2-\theta_1>\frac{\pi}{2}$. If the second situation happens, we should do as following:
Conduct transformation $g_1(\theta) \rightarrow g_1(\theta)-A \cdot \cos [ 2\left(\theta-\frac{\theta_2+\theta_1}{2}\right)]$, where $A=\min\limits_{\theta\in(\theta_1,\theta_2)}\frac{g(\theta)-g(\theta_1)}{\cos (\theta-\theta_1-\theta_2)-cos(\theta_1-\theta_2)}$.
This transformation will generate at least one more minimum value point $\theta_3$ in the regine $[0,\theta_1)\cup (\theta_2,\pi)$,,while $\theta_1$ and $\theta_2$ are still minimum value points, then we should do as follow:

A. If these minimum value points satisfy Condition 1 or Condition 2 , we can finish the search.

B. If the judgement in A is false, for all the minimum value points $\{\theta_i \}$ we select $\theta_1^{\prime}$ and $\theta_2^{\prime}$ which are the minimum value points and satisfy  $\theta_2^{\prime}-\theta_1^{\prime}=\min \limits_{\theta_i, \theta_j,\left|\theta_i-\theta_j\right|>\frac{\pi}{2}}\left|\theta_i-\theta_j\right|$.\text Then replace $\theta_1$  and $\theta_2$ by $\theta_1^{\prime} $ and $\theta_2^{\prime}$ , repeat Step 4.

As every time after repeating Step3 and the judgement in A is false, the value of $\theta_2-\theta_1$ will reduce and be closer to  $\frac{\pi}{2}$, Step3 can always finish with a finite number of repetitions. When Step4 finish, we will search the minimum value points which satisfy Condition 1 or Condition 2.
\end{enumerate}

It is easy to use the above methods to search the global optimal adaptive local strategy through numerical calculation by the computer. And this method prove that the global optimal adaptive local strategy which can achieve the local bound only need three element POVM and projective measurements.

\section{3. The analytical expression of GOAL strategy for pure-states discrimination}
Through fitting the global optimal adaptive local strategies for pure state discrimination which are calculated by iterative method, we found the analytic expression of the global optimal adaptive local strategies for pure state discrimination. Here the two possible pure states are described as
\begin{equation}
\left|r_0\right\rangle=\cos x|0\rangle+\sin x|1\rangle,\left|r_1\right\rangle=\cos x|0\rangle-\sin x|1\rangle
\end{equation}
The global optimal adaptive local strategy is a three-element POVM with following three angles $\theta_1(q),\theta_2 (q)$ and $\theta_3(q) $ or a projective measurement.

		\begin{scriptsize}
	\begin{equation}
		\begin{aligned}
			& \theta_1=\pi-\frac{1}{2} \arcsin \frac{q-\varepsilon}{\sqrt{\left[\cos (2 x) \varepsilon\left(1-q\right)-(1-\varepsilon) q\right]^2+\left[\varepsilon\left(1-q\right) \sin (2 x)\right]^2}}\\
			& -\frac{1}{2} \arcsin \frac{(1-\varepsilon) q-\cos (2 x) \varepsilon\left(1-q\right)}{\sqrt{\left[\cos (2 x) \varepsilon\left(1-q\right)-(1-\varepsilon) q\right]^2+\left[\varepsilon\left(1-q\right) \sin (2 x)\right]^2}}+\frac{x}{2}, \\
			& \theta_2=-\frac{x}{2}+\frac{1}{2} \arcsin \frac{(1-\varepsilon)\left(1-q\right)-\varepsilon q \cos (2 x)}{\sqrt{\left[(1-\varepsilon)\left(1-q\right)-\varepsilon q \cos (2 x)\right]^2+\left[\varepsilon q \sin (2 x)\right]^2}} \\
			& +\frac{1}{2} \arcsin \frac{1-\varepsilon-q}{\sqrt{\left[(1-\varepsilon)\left(1-q\right)-\varepsilon q \cos (2 x)\right]^2+\left[\varepsilon q \sin (2 x)\right]^2}} ,\\
			& \theta_3=\frac{1}{2} \arcsin \frac{\frac{1}{2}-q}{\sqrt{q^2 \cos ^2 x+\frac{1}{4}-q \cos ^2 x}}\\
			&+\frac{1}{2} \arcsin \frac{\left(\frac{1}{2}-q\right) \cos x}{\sqrt{q^2 \cos ^2 x+\frac{1}{4}-q \cos ^2 x}} 
		\end{aligned}
	\end{equation}
        \end{scriptsize}
The process of the optimal strategy is as follow:

case 1:If $q=0.5$, the measurement will be a three elements fixed measurement with three angles $\theta_1,\theta_2 \ and \ \theta_3$. Here $\theta_3=0$. For every copy, if the measurement result is the element with the angle $\theta_3$, the posterior probability is equal to the priori probability. If the measurement result is the element with the angle $\theta_1$ or $\theta_2$, we can make the judgement with the error rate which is equal to $\varepsilon$ and finish the measurement.

case 2:If $0.5>q>\frac{1-\sqrt{1-4 \varepsilon(1-\varepsilon) \cos ^{-2} x}}{2}\ and\ \theta_1(q)-\frac{\pi}{2}>\theta_3(q)$, we should do adaptive three elements POVM on the first copy with three angles $\theta_1,\theta_2 \ and \ \theta_3$. If the measurement result is the element with the angle $\theta_3$, the posterior probability will be equal to $0.5$ and the situation turns to case 1 . If the measurement result is the element with the angle $\theta_1$ or $\theta_2$, we can make the judgement with the error rate which is equal to $\varepsilon$ and finish the measurement.

case 3:If $0.5>q>\frac{1-\sqrt{1-4 \varepsilon(1-\varepsilon) \cos ^{-2} x}}{2}\ and\ \theta_1(q)-\frac{\pi}{2}\leq\theta_3(q)$, we should do adaptive projective measurement with two angles $\theta_1 \ and\ \theta_1-\frac{\pi}{2}$ on the first copy. 
If the measurement result is the element with the angle $\theta_1$, we can make the judgement with the error rate which is equal to $\varepsilon$ and finish the measurement. If the measurement result is the element with the angle $\theta_1-\frac{\pi}{2}$, the updated posterior probability will increase. If the updated posterior probability still satisfy the condition of case3 we should do adaptive projective measurement on the next copy, otherwise, the situation turns to case 2.

case 4:If $\varepsilon<q\leq\frac{1-\sqrt{1-4 \varepsilon(1-\varepsilon) \cos ^{-2} x}}{2}$, do projective measurement with two eigenvectors of $[q\rho_0-(1-q)\rho_1]$ and then whatever the result is, we can make the judgement and finish the measurement.

case 5:If $q>0.5$, the measurement form is central symmetry about $\left(\frac{1}{2}, \frac{\pi}{2}\right)$ with the part where $q<0.5$.

Then the minimum average consumption can be expressed as follow:
\begin{equation}
	N_{\mathrm{GOAL}}(q)=\left\{\begin{array}{c}
		\frac{2}{\lambda \operatorname{tr}\left[|0\rangle\langle 0|\left(\rho_0+\rho_1\right)\right]}, case1 \\
		1+\lambda \operatorname{tr}\left(\left|\psi_{\theta_3}\right\rangle\left\langle\psi_{\theta_3}\right| \rho\right) N_{\mathrm{GOAL}}(0.5), case2 \\
		1+\operatorname{tr}\left(\left|\psi_{\theta_4}\right\rangle\left\langle\psi_{\theta_4}\right| \rho\right) N_{\mathrm{GOAL}}\left(q_{\theta_4}\right), case3 \\
		1, case4\\
		N_{\mathrm{GOAL}}(1-q), case5
	\end{array}\right.
\end{equation}
where $\theta_4\equiv\theta_1-\frac{\pi}{2}$, $\quad\left|\psi_{\theta_j}\right\rangle \equiv \cos \theta_j|0\rangle+\sin\theta_j|1\rangle$,$\rho=q\left|r_0\right\rangle\left\langle r_0\left|+\left(1-q\right)\right| r_1\right\rangle\left\langle r_1\right|$,

$\qquad\lambda=\frac{2 \sin \left[2\left(\theta_2-\theta_1\right)\right]}{\sin \left[2\left(\theta_2-\theta_1\right)\right]+\sin \left[2\left(\theta_3-\theta_2\right)\right]+\sin \left[2\left(\theta_1-\theta_3\right)\right]},\rho_0=\left|r_0\right\rangle\left\langle r_0\left|, \rho_1=\right| r_1\right\rangle\left\langle r_1\right|
\text { and }\quad q_{\theta_4}=\frac{q \operatorname{tr}\left(\left|\psi_{\theta_4}\right\rangle\left\langle\psi_{\theta_4}\right| \rho_1\right)}{\operatorname{tr}\left(\left|\psi_{\theta_1}\right\rangle\left\langle\psi_{\theta_4}\right| \rho\right)}$

Specially, if $\varepsilon=0$, there is one critical point $p_c=\frac{\cos^2x}{1+\cos^2x}$. When $q<p_c$, we should do projective measurement with two angles $\theta_1=\frac{x}{2}+\frac{\pi}{2}$ and 
$\theta_1-\frac{\pi}{2}$.When $q\ge p_c$, we should do three-element POVM with three angles $\theta_1=\frac{x}{2}+\frac{\pi}{2}$,
$\theta_2=-\frac{x}{2}+\frac{\pi}{2}$, and $\theta_3$.For pure states, the minimum average copy consumption is a finite value no more than $\frac{1}{1-\cos x}$ to achieve the perfect discrimination (this conclusion is the same with [10]), but even if $q \rightarrow 0^{+}$, the average copy consumption is no less than $\frac{2}{\sin ^2 x}$. 

Though in principle for any given error rate requirement we can rewrite Eq.7 as an elementary piecewise function, this function is too complex to prove through symbolic operations strictly that it is the solution to Eq.2 of the main text. Therefore, we did not strictly prove that this is the optimal one.

The reason we infer that it is the global optimal adaptive local strategy is that through numerical calculation, we found for any error rate requirement, any possible states and any prior probability, the average copy consumption function calculated by Eq.7 and the corresponding measurement angles satisfy Condtion 1 or Condition 2 which are shown in the second part. That means, numerically, if we define$\ \left.g(\theta)=\operatorname{tr}\left|\psi_\theta\right\rangle\left\langle\psi_\theta\right|\left[q\left|r_0\right\rangle\left\langle r_0|+(1-q)| r_1\right\rangle\left\langle r_1\right|\right]\right\} N_{\mathrm{GOAL}}\left(q_\theta\right), \theta \in[0, \pi)$, where $N_{\mathrm{GOAL}}$ is calaulated by Eq.9, we can find the suitable value of $a$ and $b$ so that for case 3 and case 4, $g(\theta)+a \cos (2 \theta)+b \sin (2 \theta)$ take the minimum value at two points $\theta_1$ and $\theta_4$ and they satisfy condition 1. And for case 1 and case 2, $g(\theta)+a \cos (2 \theta)+b \sin (2 \theta)$ take the minimum value at three points $\theta_1,\theta_2$ and $\theta_3$ and they satisfy Condition 2. Thus according to the second part, Eq.9 is the analytic expression of the average copy consumption realized by GOAL. And how to provide our statement based on numerical calculation evidence with a strict proof is an open problem.
\begin{thebibliography}{20}
	\bibitem{ref1}C. W. Helstrom, Quantum Detection and Estimation Theory
	(Academic Press, New York, 1976).
	\bibitem{ref2} B.L.Higgins,B.M.Booth,A.C.Doherty,S.D.Bartlett,\\ H.M.Wiseman,andG.J.Pryde,Phys.Rev.Lett.103, 220503 (2009).
	\bibitem{ref3} A. Peres andW. K.Wootters, Phys. Rev. Lett. 66, 1119 (1991).
	\bibitem{ref4}J. Calsamiglia, J. de Vicente, R. Muñoz-Tapia, and E.
	Bagan, Local Discrimination of Mixed States, Phys. Rev.
	Lett. 105, 080504 (2010).
	\bibitem{ref5}B. L. Higgins, A. C. Doherty, S. D. Bartlett, G. J. Pryde, and H. M. Wiseman, Multiple-copy state discrimination: Thinking globally, acting locally, Phys. Rev. A 83, 052314 (2011).
	\bibitem{ref6} H. M. Wiseman and G. J. Milburn, Quantum Measurement
	and Control (Cambridge University Press, Cambridge,
	England, 2009).
	\bibitem{ref7} A. Acín, E. Bagan, M. Baig, L. Masanes, and R. Munoz Tapia, Phys. Rev. A 71, 032338 (2005).
	\bibitem{ref8} D. Brody and B. Meister, Phys. Rev. Lett. 76, 1 (1996).
	\bibitem{ref9}] S. Slussarenko, M. M. Weston, J.-G. Li, N. Campbell, H. M.
	Wiseman, and G. J. Pryde, Phys. Rev. Lett. 118, 030502
	(2017).
	\bibitem{ref10}Esteban Martínez Vargas, Christoph Hirche, Gael Sentís, Michalis Skotiniotis, Marta Carrizo, Ramon Muñoz-Tapia, and John Calsamiglia, Phys. Rev. Lett. 126, 180502 (2021).
	\bibitem{ref11}Li, Y., Tan, V.Y.F. and Tomamichel, M. Optimal Adaptive Strategies for Sequential Quantum Hypothesis Testing. Commun. Math. Phys. 392, 993–1027 (2022).
\end{thebibliography}
\end{document}