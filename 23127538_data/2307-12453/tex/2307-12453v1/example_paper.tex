%%%%%%%% Syns \& ML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables



% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{synsml2023} with \usepackage[nohyperref]{synsml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% Use the following line for the initial blind version submitted for review:
%\usepackage{synsml2023}

% If accepted, instead use the following line for the camera-ready submission:
 \usepackage[accepted]{synsml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{commath}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{etoolbox}

%added by Minghan
\usepackage{xcolor}
\usepackage{hyperref}



% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \synsmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\synsmltitlerunning{Submission and Formatting Instructions for Syns \& ML at ICML 2023}

\begin{document}

\twocolumn[
\synsmltitle{Multi-Fidelity Data Assimilation For Physics Inspired Machine Learning In Uncertainty Quantification Of Fluid Turbulence Simulations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the synsml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\synsmlsetsymbol{equal}{*}

\begin{synsmlauthorlist}

\synsmlauthor{Minghan Chu}{equal,yyy}
\synsmlauthor{Weicheng Qian}{equal,comp}

\synsmlauthor{Department of Mechanical and Materials Engineering}{yyy}
\synsmlauthor{Department of Computer Science}{comp}

\synsmlauthor{Queen's University}{yyy}
\synsmlauthor{University of Saskatchewwan}{comp}

\synsmlauthor{\texttt{17MC93@queensu.ca}}{yyy}
\synsmlauthor{\texttt{weicheng.qian@usask.ca}}{comp}

%\synsmlauthor{Firstname1 Lastname1}{equal,yyy}
%\synsmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
%\synsmlauthor{Firstname3 Lastname3}{comp}
%\synsmlauthor{Firstname4 Lastname4}{sch}
%\synsmlauthor{Firstname5 Lastname5}{yyy}
%\synsmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\synsmlauthor{Firstname7 Lastname7}{comp}
%%\synsmlauthor{}{sch}
%\synsmlauthor{Firstname8 Lastname8}{sch}
%\synsmlauthor{Firstname8 Lastname8}{yyy,comp}
%%\synsmlauthor{}{sch}
%%\synsmlauthor{}{sch}
\end{synsmlauthorlist}

%\synsmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\synsmlaffiliation{comp}{Company Name, Location, Country}
%\synsmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

%\synsmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
%\synsmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

%% You may provide any keywords that you
%% find helpful for describing your paper; these are used to populate
%% the "keywords" metadata in the PDF but will not be shown in the document
%\synsmlkeywords{Machine Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \synsmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\synsmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Reliable prediction of turbulent flows is an important necessity across different fields of science and engineering. In Computational Fluid Dynamics (CFD) simulations, the most common type of models are eddy viscosity models that are computationally inexpensive but introduce a high degree of epistemic error. The Eigenspace Perturbation Method (EPM) attempts to quantify this predictive uncertainty via physics based perturbation in the spectral representation of the predictions. While the EPM outlines how to perturb, it does not address how much or even where to perturb. We address this need by introducing machine learning models to predict the details of the perturbation, thus creating a physics inspired machine learning (ML) based perturbation framework. In our choice of ML models, we focus on incorporating physics based inductive biases while retaining computational economy. Specifically we use a Convolutional Neural Network (CNN) to learn the correction between turbulence model predictions and true results. This physics inspired machine learning based perturbation approach is able to modulate the intensity and location of perturbations and leads to improved estimates of turbulence model errors. 
\end{abstract}

\section{Introduction}
\label{submission}
Turbulent fluid flows are common across different fields of science and engineering. The most commonly used models are Eddy Viscosity Models (EVMs) that are computationally inexpensive but introduce a high degree of epistemic error or model-form uncertainty, due to simplifications like the Boussinesq Turbulent Viscosity Hypothesis (TVH). The quantification of these model-form uncertainty \cite{duraisamy2017status} ensures reliable designs and analysis. The Eigenspace Perturbation Method (EPM) \cite{emory2013modeling,iaccarino2017eigenspace} is a physics based framework to quantify model-form uncertainty introduced in EVMs. This method has shown reliable uncertainty quantification with low computational cost, and has been applied to a variety of engineering problems including virtual certification of aircraft designs \cite{mukhopadhaya2020multi, nigam2021toolset}, design of urban structures\cite{gorle2019epistemic}, aerospace design and analysis\cite{mishra2019uncertainty, mishra2017rans, mishra2019estimating, mishra2017uncertainty}, application to design under uncertainty (DUU) \cite{demir2023robust, cook2019optimization, mishra2020design, righi2023uncertainties}, etc . 

The EPM only outlines how to perturb, while it does not address how much to perturb. As an illustration the EPM uses the maximal physics permissible perturbation all over the flow domain. The strength of perturbation should reflect the degree of discrepancy between EVMs and the truth, as the discrepancy actually varies spatially in the flow domain. We address this need by developing a function to predict the degree of perturbation at different locations. While there aren't physics based precepts to completely determine this function to predict the spatial variation of perturbations, it can be estimated reliably from data. Machine Learning based approaches are becoming more used in fluid mechanics applications \cite{duraisamy2019turbulence, chung2021data, brunton2020machine}. Some prior investigators have tried to use ML to improve turbulence model UQ \cite{xiao2016quantifying,wu2018physics,heyse2021estimating,heyse2021data,zeng2022adaptive}. These studies have focused on more complex models that necessitate large incorporation of labeled data and limit generalizability. These studies used polynomial regression \cite{chu2022model}, Probabilistic Graphical Models (PGMs), Random Forests, etc. These do not reflect the non-locality of turbulence physics where the evolution of a turbulent flow at a location is affected by far off points in the flow domain too. 

We utilize a new solution which tries to compensate for the absent non-local physics in turbulence models via the inductive biases of Convolutional Neural Networks (CNNs). We use this to formulate a function that predicts the degree of perturbation and improving the EPM. 

\section{Methodology}

% Figure environment removed

The turbulence kinetic energy $k$ plays an important role in the estimates of the model-form uncertainty introduced in RANS models using the Eigenspace Perturbation Method \cite{emory2013modeling,iaccarino2017eigenspace}. We employed a one-dimensional convolutional neural network (1D-CNN) to learn the function to correct RANS prediction towards DNS prediction, namely, correction function. Our approach enables the use of learned correction function for the computationally cheaper RANS prediction to replace the rather expensive high-fidelity DNS prediction. 

For both the RANS and DNS simulation, we can summarize their results as the function of the perturbed turbulence kinetic energy $k^{*} = f(x,y)$, where $x$ and $y$ are coordinates in a two-dimensional computational domain, and $f$ is the mapping from every coordinate $(x, y)$ to $k^{*}$, embedded in triples $(x, y, k^{*})$ from simulation results.

%For sake of simplicity, we will use k to represent k^{*} throughout the following sections. 
Without assuming a specific form, the correction function for RANS is a mapping between two triples $\zeta: (x, y, k^{\mathrm{RANS}}) \rightarrow (x, y, k^{\mathrm{DNS}})$. Consider the model error for RANS and DNS in terms of kinetic energy, we have
\begin{equation}
   p^{\text {RANS }}\left(K_g \mid x, y\right)=p\left(k_g=k^{\text {RANS }} \mid x, y\right),
\end{equation}

\begin{equation}
    p^{\text {DNS }}\left(K_g \mid x, y\right)=p\left(k_g=k^{\text {DNS }} \mid x, y\right),
\end{equation}

where $K_g$ is the unknown ground truth of kinetic energy at $(x, y)$.
Turbulence kinetic energy that results from DNS simulation, i.e., $p^{\mathrm{RANS}}$, can be estimated using RANS-based turbulence kinetic energy $p^{\mathrm{DNS}}$ and its correction function $g$ as

\begin{equation}
p^{\mathrm{DNS}}\left(K_g \mid x, y\right)=g\left(k^{\mathrm{RANS}}, x, y\right) p\left(k^{\mathrm{RANS}} \mid x, y\right).
\end{equation}

Because $k^{\mathrm{DNS}} = f^{\mathrm{DNS}}(x, y)$ and $k^{\mathrm{RANS}} = f^{\mathrm{RANS}}(x, y)$,
at each $x$, we have $k_x^{\mathrm{DNS}} = f_x^{\mathrm{DNS}}(y)$ and $k_x^{\mathrm{RANS}} = f_x^{\mathrm{RANS}}(y)$, assuming both $f_x^{\mathrm{RANS}}$ and $f_x^{\mathrm{DNS}}$ are continuous, that is, $\forall \epsilon > 0,\allowbreak\, \exists \delta > 0,\allowbreak\, s.t.\, \forall \abs{d} < \delta, \allowbreak \, \abs{f_x(y + d) - f_x(y)} < \epsilon$. We can approximate $g(k^{\mathrm{RANS}}, x, y)$ with $\hat{g}(\mathbf{k}_{x,y,\delta}^{\mathrm{RANS}})$, where $\mathbf{k}_{x,y,\delta}^{\mathrm{RANS}} = [k_{x, y_0}^{\mathrm{RANS}}, k_{x, y_1}^{\mathrm{RANS}}, \cdots]^\top$ and $y_0, y_1, \dots \in [y - \delta, y + \delta]$. In other words, we can learn $\hat{g}$ with paired $(\mathbf{k}_{x,y,\delta}^{\mathrm{RANS}}, \mathbf{k}_{x,y,\delta}^{\mathrm{DNS}})$.

Our one-dimensional convolutional neural network (1D-CNN) learns the correction function $\hat{g}$ from paired RANS and DNS simulation estimated turbulence kinetic energy $(\mathbf{k}_{x,y,\delta}^{\mathrm{RANS}}, \mathbf{k}_{x,y,\delta}^{\mathrm{DNS}})$. Because our approximated correction function $\hat{g}$ only depends on the neighbor of $k^{\mathrm{RANS}}$, and coordinates $(x, y)$ are only used to group neighbors of $k^{\mathrm{RANS}}$, we grouped simulation data by $x$ and transformed $(y, k)$ at $x$ into $\mathbf{k}_{x,y,\delta}^{\mathrm{RANS}}$ via a rolling window parameterized by window size. Our 1D-CNN has four-layers and in total 86 parameters: a single model for all zones at any $x$ to correct RANS towards DNS.

% % Figure environment removed

\section{Experiments Setup and Data Sources}
We proposed a lightweight 1D-CNN approach to approximate the correction function that corrects RANS simulation towards DNS simulation. We experimented our lightweight CNN-based approach to approximate the correction function for RANS on two datasets: the in-house RANS/DNS \cite{zhang2021turbulent,chu2022model} dataset and the public RANS/DNS dataset \cite{voet2021hybrid}. The in-house RANS/DNS dataset \cite{chu2022model,zhang2021turbulent} was obtained by considering the flow around an SD7003 airfoil. The public RANS/DNS dataset \cite{voet2021hybrid} was generated from the two-dimensional channel flow over periodically arranged hills. Both flow cases experience adverse pressure gradient, which causes the complex flow features of separation and reattachment. It is challenging both to understand and to model.

From \cref{fig:data-flow.pdf}, we split $x$-coordinate grouped pairs of $(\mathbf{k}_{x,y,\delta}^{\mathrm{RANS}}, \mathbf{k}_{x,y,\delta}^{\mathrm{DNS}})$ into training set and validating set by their group key $x$. For both the in-house and the public dataset, we choose $x$ at only three positions on the geometry from the beginning, the middle, and the end of all paired $x$ values. For the in-house dataset based on the SD7003 airfoil geometry, $x/c = 0.4, 0.56, 0.58$; for the public dataset based on the two-dimensional periodically arranged hills, $x/c = 0, 0.046, 0.116, 0.128$, where $c$ is a reference length used for normalization. For each dataset, we use a 80\%--20\% split as training--testing dataset. 



For both datasets, we validated our trained 1D-CNN by comparing the L1 loss of RANS, denoted $L^1_c(\texttt{rans}) = \abs{ CF^{\mathrm{RANS}}_{k} - CF^{\mathrm{DNS}}_{k} }$, with the L1 loss of 1D-CNN corrected RANS, denoted $L^1_c(\texttt{pred}) = \abs{ CF^{\mathrm{CNN}}_{k} - CF^{\mathrm{DNS}}_{k} }$.

%Furthermore, we examined the generalizability of our lightweight CNN-based correction function trained on the public RANS/DNS dataset \cite{voet2021hybrid} by applying the correction function for other cases. 

\section{Results and discussion}
Our CNN-based correction function is validated at all paired $x$ locations. \Cref{fig:cnn-corrected-rans-zhang.pdf,fig:cnn-corrected-rans-voet.pdf} show the results for SD7003 and 2D periodically arranged hills, respectively, at four $x$ locations within the region of separated flow. From \cref{fig:cnn-corrected-rans-zhang.pdf}, the series of CNN predicted DNS profiles in the first row are then smoothed with the moving average (the average of points in a sliding window) with a window size of six consecutive estimations. Our CNN-based prediction for the $k$ profile resemble the ground truth DNS despite being trained with only a few pairs of RANS and DNS results. 

For both datasets, the CNN-based correction function is trained on paired RANS-DNS simulated turbulence kinetic energy using less than $20\%$ positions along $x$-axis, while the CNN-based function is still effective for the remaining $80\%$ positions. In other words, our CNN-based correction can be used to predict RANS-DNS simulated turbulence kinetic energy at any $x$ coordinate with only a fraction of the whole $x$ coordinates. Furthermore, our lightweight CNN model uses the $y$ coordinates for grouping RANS-simulated turbulence kinetic energy within a neighbor. The results of our CNN-based correction function suggests that RANS results might be improved by leveraging information embedded in the positions within a close neighbor, which is independent of the absolute coordinates $(x, y)$. The lightweight CNN-based correction function trained on one case can still help smooth and reduce the error of RANS-based results for other cases.

For both the in-house and the public flow cases, our CNN-based prediction for $k$ lies closer to the DNS data at any $x$ location, i.e., the discrepancy in general reduces as the flow proceeds further downstream. The RANS results deviate from the DNS data in both flow scenarios and our CNN-based correction function can significantly reduce the L1 error of RANS- from DNS-simulations, as shown in the second row of \cref{fig:cnn-corrected-rans-zhang.pdf,fig:cnn-corrected-rans-voet.pdf}, i.e., a $L^1_c(\texttt{pred})$ drop of two orders compared to $L^1_c(\texttt{rans})$. 

From \Cref{fig:cnn-corrected-rans-zhang.pdf,fig:cnn-corrected-rans-voet.pdf}, it is interesting to note that the CNN-based prediction for $k$ tends to approach closer to the DNS profile as the flow proceeds further downstream. This indicates that the CNN-based correction function tends to become more trustworthy within the region of fully turbulent flow where flow features are less complex than that within region of separated flow where rather complex flow features evolve.  

%  From the \cref{fig:cnn-corrected-rans-zhang.pdf}, the second row shows the computed L1 error of the RANS-based prediction and the CNN-based prediction. It is clear that the L1 error for CNN-based correction function can significantly reduce the L1 error in magnitude compared to that for the original RANS. 
%In comparison to the regression-based polynomials, it is clear that the CNN predicted DNS profiles more closely resemble the DNS data for the $ab$ and $cd$ zone. 

%From \cref{fig:rans-predicted-dns-voet-case1-with-model-from-case2-main4-viz-loss.pdf}, the CNN-based correction function is trained on one of the cases of the public RANS/DNS dataset \cite{voet2021hybrid} to correct RANS for a different case. The first row of \cref{fig:rans-predicted-dns-voet-case1-with-model-from-case2-main4-viz-loss.pdf} shows that the CNN-based prediction for $k$ profile is qualitatively similar to that of the DNS profile, except for an over-prediction at beginning of the region of separated flow, i.e., $x/c =0.0$. In the second row of \cref{fig:rans-predicted-dns-voet-case1-with-model-from-case2-main4-viz-loss.pdf}, the loss $L^1_c(\texttt{rans})$ and $L^1_c(\texttt{pred})$ are comparable, and hence, the CNN-based prediction for $k$ profile is comparable with that of the RANS-based prediction. Results for other places can be found in \cref{fig:voet-case3-rans-predicted-dns.pdf,fig:voet-case4-rans-predicted-dns.pdf}. 



%The CNN predicted DNS profiles for $k$ show overall good resemblance to the DNS data, although an over-prediction exists at the beginning of the $ab$ zone.  In \cref{fig:rans-predicted-dns-voet-case1-with-model-from-case2-main4-viz-loss.pdf}, we selected four interesting $x/c$ points to demonstrate the lightweight CNN corrected RANS with the DNS and the error reduction of the lightweight CNN-based approach comparing with the uncorrected RANS. Results for other places can be found in \cref{fig:voet-case3-rans-predicted-dns.pdf,fig:voet-case4-rans-predicted-dns.pdf}.


  



% Figure environment removed
% Figure environment removed


%\section{Discussion}
%We proposed a CNN approach to approximate the correction function that corrects RANS simulation towards DNS simulation. We further examined our method on two datasets: 1) one flow being considered is over a SD7003 airfoil at $8^\circ$ angle of attack and the Reynolds number based on the cord length of $Re_{c} = 60000$ \cite{zhang2021turbulent}. A laminar separation bubble evolves on the suction side of the airfoil whereby the flow undergoes transition to turbulence, 2) another is generated from the DNS two-dimensional channel flow over periodically arranged hills \cite{voet2021hybrid}. The flow experiences adverse pressure gradient when encountering the curved surface of the hill. It should be noted that a separation bubble occurs for both datasets. The RANS results deviate from the DNS data in both flow scenarios and our CNN-based correction function can significantly reduce the L1 error of RANS- from DNS-simulations.

% 1. Our lightweight CNN model uses only the RANS-simulated kinetic energy at neighbor on the $y$-axis to corrected RANS towards DNS simulation results. The results shown great improvement compared to the DNS data. 
% 2. Our light-weight model trained on the in-house DNS dataset is directly used to yield corrected RANS predictions for the periodically arranged hills, and show a clear improvement. A separated region in both flow cases might partly explain this improvement. 
% 3. Also similar cross neighbor features might contribute this improvement. 

%For both datasets, the CNN-based correction function is trained on paired RANS-DNS simulated turbulence kinetic energy using less than $20\%$ positions along $x$-axis, while the CNN-based function is still effective for the remaining $80\%$ positions. In other words, our CNN-based correction can be used to predict RANS-DNS simulated turbulence kinetic energy at any $x$ coordinate with only a fraction of the whole $x$ coordinates. Furthermore, our lightweight CNN model uses the $y$ coordinates for grouping RANS-simulated turbulence kinetic energy within a neighbor. The results of our CNN-based correction function suggests that RANS results might be improved by leveraging information embedded in the positions within a close neighbor, which is independent of the absolute coordinates $(x, y)$. The lightweight CNN-based correction function trained on one case can still help smooth and reduce the error of RANS-based results for other cases \cref{fig:rans-predicted-dns-voet-case1-with-model-from-case2-main4-viz-loss.pdf}.


%There are relatively few studies for correcting the perturbed turbulence kinetic energy. Very recently, the study of Chu \textit{et al.} \cite{chu2022model} assessed the effect of polynomial regression on the estimation of the perturbed turbulence kinetic energy. Our CNN-based correction method has readily implications on practical applications, such as, to be coupled to the eigenspace perturbation approach of Emory \textit{et al.} \cite{emory2013modeling}. The eigenspace perturbation approach has been implemented within the OpenFOAM framework to construct a marker function for the perturbed turbulence kinetic energy \cite{chu2022model}. Our CNN-based correction method can be used as a new marker function to predict the perturbed turbulence kinetic energy. 

%\subsection{Application of the lightweight CNN-based correction function on UQ for an SD 7003 airfoil}
%Our CNN-based correction function method can be applied to different flow cases to correct RANS towards DNS. In this section, the CNN-based correction function is applied to the SD7003 airfoil case to predict the perturbed turbulence kinetic energy. 


% % Figure environment removed

%The CNN corrected RANS and ground truth profiles for the turbulence kinetic energy normalized with the freestream velocity squared, $k^{*}/U_{\infty}^2$ and $k/U_{\infty}^2$ are shown in Figs. \ref{fig:CNN_DNS.pdf} (a) and (b), respectively. The $k^{*}/U_{\infty}^2$ and $k/U_{\infty}^2$ profiles are equally spaced for the $ab$ and $cd$ zone with $x/c = 0.01$, and a uniform spacing of $x/c = 0.02$ is used for the $ef$ zone. It is clear that the $k^{*}/U_{\infty}^2$ and $k/U_{\infty}^2$ profiles are more densely packed for the $ab$ and $cd$ zone, within which the flow features are complex due to the presence of separation and reattachment. 

%From Figs. \ref{fig:CNN_DNS.pdf} (a) and (b), the CNN corrected DNS profiles in general exhibit a similar trend as that for the ground truth dataset, as both profiles show a gradual increase in the $ab$ and $cd$ zone. Then a reduction of the profile is observed further downstream in the $ef$ zone. 

%In Fig. \ref{fig:CNN_DNS.pdf} (a), CNN corrected RANS profiles in general increase in magnitude as the flow moves further downstream, which is qualitatively similar to the ground truth profiles. Further, it should be noted that the CNN corrected RANS profiles increase in a somewhat larger magnitude than that for the ground truth in the $ab$ zone. The discrepancy is more than $50\%$ at the beginning of the $ab$ zone and gradually reduces as the flow moves further downstream, which indicates that a better accuracy of our CNN model is yielded further downstream. This behavior becomes more clear for the $cd$ and $ef$ zone. In the region where the end of the $cd$ zone meets the beginning of the $ef$ zone, the ground truth profiles are clustered due to the complex flow feature of the reattachment \cite{chu2022quantification}, as shown in Figs \ref{fig:CNN_DNS.pdf} (b). This clustering behavior is successfully captured by our CNN model, as shown in Fig. \ref{fig:CNN_DNS.pdf} (a). In the $ef$ zone, our CNN model gives overall accurate predictions for the $k^{*}/U_{\infty}^{2}$ profiles, i.e., the CNN corrected RANS profiles and the ground truth profiles are almost identical.      



% think in the journal, we might be able to do a comparison

%I just applied the CNN-based correction function trained with case2 on to the data of case1. It looks like even our CNN-based correction is not trained on case1, it can still correct Voet's RANS model on case1.






\section{Conclusion}
\label{sec:Conclusion}


To the best of our knowledge, we are among the first to examine the projection from RANS to DNS using the CNN approach. Our experiment results suggest that the CNN-based correction function corrects the RANS predictions for perturbed turbulence kinetic energy towards both the in-house and the public DNS data. A projection that can approximate the in-house DNS data reasonably well from RANS might exist independent of $x$. Our methodology can be easily extended to analyze flows over different types of airfoils.

Our findings are subject to following limitations: our CNN model was trained with only two datasets. Future work may include validation with more datasets on different flow cases, e.g., different types of airfoils.  In addition, our CNN-based correction function will be integrated with the eigenspace perturbation framework to result in accurate perturbations and hence improved estimation of RANS UQ.

\section{Impact statement}
Separation, reattachment, and separation bubbles are so frequently encountered in real life engineering applications. However, those flow features are complex to both understand and model. LES and DNS give high-fidelity results, but the calculations are too expensive for practical engineering applications. Therefore, knowing the uncertainty associated with the low-cost RANS simulations are of great interest as it builds confidence when using RANS. An effective physics-based eigenspace perturbation method only presupposes the worst case scenario where maximum strength (turbulence kinetic energy) of perturbation is assumed. Thus it gives overly conservative uncertainty estimates. Our physics inspired machine learning based perturbation approach is able to spatially modulate the strength and location of the physics based perturbations and leads to improved estimates of turbulence model errors. 



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{synsml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%\appendix
%\onecolumn
%\section{You \emph{can} have an appendix here.}

%You can have as much text here as you want. The main body must be at most $4$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one, even using the one-column format.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
