\vspace*{-3pt}
\section{Related Work}
\label{sec:related_work}

Starting with foundational attempts to link the electronic properties of different substituents to the reactivity \citep{hammett1937effect} and bioactivity \citep{hansch1962correlation} of benzoic acid derivatives, the problem of predicting the properties of a molecule from its structure has long received considerable attention \citep{cherkasov2014QSAR}.
%
While simpler algorithms such as support vector machines \citep{cortes1995support} and random forests \citep{breiman2001random} remain a popular choice for such quantitative structure-activity relationship (\textsc{QSAR}) models, recent years have seen substantial interest in applying modern deep learning algorithms to this task \citep{ma2015deep,gawehn2016deep,zhang2017machine}, including important attempts to improve their performance in low-data and out-of-distribution regimes.

\vspace*{-3pt}
\paragraph{Self-supervised pre-training techniques.}
To this end, \citet{hu2019strategies} and \citet{rong2020self} have introduced a range of self-supervised objectives to pre-train graph neural networks and graph transformers on a set of unlabeled molecular structures to generate initializations that can be efficiently fine-tuned on downstream tasks.
However, the out-of-distribution generalization of their approaches was only assessed on scaffold splits---a setting that may underestimate of covariate and label shift encountered in many practical applications \citep{wallach2018most}.

%
%

%

%
%

\vspace*{-3pt}
\paragraph{Domain adaptation techniques.}
Building on the fact that biases in the data collection process are often known at training time, domain adaptation and generalization techniques \citep{ganin2016domain,sun2016deep,sagawa2019distributionally,arjovsky2019invariant} aim to improve the performance of deep learning algorithms in out-of-distribution settings by leveraging pre-specified domain indicators. However, these methods---originally developed for image data---have been found to provide limited benefits in the context of molecular property prediction~\citep{ji2022drugood}.

\paragraph{Bayesian inference-based techniques.}
Bayesian Neural Networks (\textsc{BNNs};~\citet{neal1996Bayesian}) provide a principled probabilistic framework for posterior inference over neural networks parameters and have long been explored in the context of drug discovery~\citep{burden1999robust,burden2000use}.
Even though they conceptually guarantee robustness in low-data regimes, their empirical performance often falls short of ensembling techniques or even standard stochastic gradient descent~\citep{ovadia2019uncertainty, foong2019inbetween, farquhar2020radial}, including in the context of molecular property prediction~\citep{ryu2019bayesian,zhang2019bayesian}.

While these approaches may improve the robustness of deep learning algorithms in some settings, they are limited in the extent to which they can encode problem-specific modeling preferences that, for example, encourage high predictive uncertainty away from the training data or specify prior knowledge of synthetic accessibility and patentability.
For instance, the standard parameter-space formulation of \bnns precludes the specification of semantically meaningful prior information due to the highly non-linear and complex relationship between a neural network's parameters and the functions they encode.
%

Building on recent work that aims to address the shortcomings of \bnns (e.g., in specifying meaningful prior distributions and providing reliable uncertainty quantification) via function-space variational inference~\citep{sun2019fbnn, rudner2021tractable, Rudner2022sfsvi}, we reframe \textsc{QSAR} modeling as inferring a posterior distribution over functions.
We do so by specifying a prior distribution over function mappings along with a prior distribution over function evaluation points and performing variational inference in this probabilistic model, which allows us to explicitly encode prior beliefs about the distribution over functions as well as about the structure of the input space into neural network training.


%
%

%
%
%


%