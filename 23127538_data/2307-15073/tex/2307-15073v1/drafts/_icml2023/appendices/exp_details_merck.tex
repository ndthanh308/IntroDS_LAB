
\clearpage


\section{Additional Experimental Details for the Merck Molecular Activity Challenge Data}
\label{sec:app_merck}

In order to assess the practical utility of our method in real-world production settings, an evaluation on the Merck Molecular Activity Challenge datasets \cite{ma2015deep} was conducted. This data consists of 15 datasets from real-world production environments with time-split training and test sets. As the compound structures are only provided in the form of anonymized atom-pair descriptors, it is not possible to use a uniform subsample of a large chemical database as a context point distribution.
Instead, the evaluation focused on the three most covariate- and label-shifted datasets, see~\Cref{fig:covariate_label_shift_scatterplot}, using a uniform distribution over molecules from the remaining datasets as a context point distribution. To select these datasets, the multiset version of the standard Jaccard/Tanimoto index
$$
k_{\text{jac-multiset}}(\mathbf{x}, \mathbf{y}) = \frac{\sum_{i} \min(\mathbf{x}_i, \mathbf{y}_i)}{\sum_{i} \max(\mathbf{x}_i, \mathbf{y}_i)}
$$
was used to evaluate the \textsc{MMD} statistic between two sets of count vectors and quantify covariate shift. Label shift between the regression targets of every training and test set was quantified through the two-sample Kolmogorov-Smirnov test statistic.

% Figure environment removed

\vspace*{10pt}

\begin{wrapfigure}{r}{0.5\linewidth}
    \centering
    \vspace*{-20pt}
    % Figure removed %
    \vspace*{-30pt}
    \caption{Heatmap illustrating the pairwise overlap between different datasets from the Merck Molecular Activity Challenge, defined as the proportion of molecules from the smaller dataset that are found in the larger dataset, i.e., ${|\mathbf{X}_1\cup\mathbf{X}_2|}/{\min(|\mathbf{X}_1|,|\mathbf{X}_2|)}$} %
    \label{fig:merck_dataset_overlap} %
    %
\end{wrapfigure}

As shown in~\Cref{fig:merck_dataset_overlap}, the direct overlap between the \textsc{HIVPROT}, \textsc{NK1}, and \textsc{DPP4} datasets with the remaining data is minimal, warranting its use as a general and diverse context point distribution.
Using this evaluation setup, 10\% of the training sets was randomly split off as a validation set for hyperparameter optimization and, where applicable, early stopping. Model-specific details are outlined below, including implementational details and hyperparameter ranges for regularized linear regressions (\Cref{app_sec:det_lin_reg_regression}), random forest regressors (\Cref{app_sec:det_rf_regression}), and an adapted version of our probabilistic regularization scheme (\Cref{app_sec:det_ours_regression}).



\clearpage

 

\subsection{Regularized Linear Regression}
\label{app_sec:det_lin_reg_regression}

The \textbf{regularized linear regression models} were trained using the scikit-learn library \cite{scikit-learn}. The \textsc{liblinear} solver \cite{fan2008liblinear} was employed with a maximum of 1,000 iterations and a stopping tolerance of \num{1e-4}.
The models were independently fitted for all specified hyperparameter combinations presented in \Cref{tab:hyper_linreg_regression}.
The combination yielding the lowest validation set mean squared error was selected to evaluate the model on the held-out test set. 

\setlength{\tabcolsep}{18.0pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for $L-1$ and $L_2$-Regularized Linear Regression}
\label{tab:hyper_linreg_regression}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Linear Regression & regularization type & $\ell1, \ell2$ \\
& regularization strength & 100 values spaced log-linearly in [\num{1e-4}, \num{1e4}] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Random Forest Regressors}
\label{app_sec:det_rf_regression}

The \textbf{random forest regression models} were trained using the scikit-learn library \cite{scikit-learn}. The models consisted of 100 decision trees with the \textsc{gini} splitting criterion. Each model was independently fitted for all specified hyperparameter combinations shown in~\Cref{tab:rfr_hypers_regression}. The combination with the lowest validation set mean squared error was selected to evaluate the model on the held-out test set.

\setlength{\tabcolsep}{32.0pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for Random Forest Regressor}
\label{tab:rfr_hypers_regression}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Random Forest & maximum depth & 50 values spaced linearly in [5, 500] \\
& min. samples per split & 5, 15, 50, 100 \\
& min. samples per leaf & 1, 5, 10, 30, 100 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Q-SAVI}
\label{app_sec:det_ours_regression}

The regression variant of our probabilistic regularization scheme was set up identically to the classification variant described in~\Cref{app_sec:det_ours}, the only difference being the likelihood function used to evaluate~\Cref{eq:objective}. Instead of specifying $\log p(\by_{\calD} \vbar f(\bx_{\calD} ; \btheta))$ as a Bernoulli likelihood, a homoscedastic multivariate Normal likelihood with a unit diagonal covariance matrix was used. While a more expressive approach of either optimizing the covariance as a hyperparameter or letting the network predict point-wise means and variances to use in combination with a heteroscedastic likelihood function is possible, this straightforward method was found to be sufficient in this context. The full hyperparameter search space is presented in~\Cref{tab:ours_hypers_regression} and is identical to that in ~\Cref{tab:ours_hypers}.

\begin{table}[H]
\centering
\caption{Hyperparameters for Our Model}
\label{tab:ours_hypers_regression}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Ours & learning rate & \num{1e-4}, \num{1e-3} \\
 & number of layers & 2, 4, 6 \\
 & embedding dimension & 32, 64 \\
 & prior variance & \num{1e-2}, \num{1e-1}, \num{1e0}, \num{1e1}, \num{1e2} \\
 & context points per sample & 16, 128 \\
\bottomrule
\end{tabular}
\end{table}