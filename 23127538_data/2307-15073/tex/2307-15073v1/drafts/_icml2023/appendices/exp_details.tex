\section{Additional Experimental Details}
\label{appsec:training_details}

This section provides additional implementational details of our experimental setup.
\Cref{app:sec_auprcs} presents the test set performance of each model under different data splitting and featurization techniques using the \textsc{AUC-PRC} and the \textsc{ACE} scores.
Additional experimental details are provided in~\Cref{app:impl_details}, describing the implementation and hyperparameter screening ranges for each model in our empirical evaluation, namely logistic regression (\Cref{app_sec:det_log_reg}), random forest classifiers (\Cref{app_sec:det_rf}), multi-layer perceptrons (\Cref{app_sec:det_mlp}), deep ensembles (\Cref{app_sec:det_deep_ens}), pre-trained graph neural networks (\Cref{app_sec:det_gnn}), GROVER (\Cref{app_sec:det_grover}), various domain adaptation and generalization techniques (\Cref{app_sec:det_domain_adapt}), and Q-SAVI (\Cref{app_sec:det_ours}). 
Detailed ablation plots that explore the influence of different hyperparameters on the performance of Q-SAVI are presented in~\Cref{app_sec:ablation_plots}.

All experiments and analyses were performed in Python \cite{van1995python}, using a range of general-purpose packages to aid model development and analysis \cite{harris2020array, Waskom2021, mckinney-proc-scipy-2010, virtanen2020scipy}.
All code that is necessary to reproduce the results presented in this work is available in the following repository: \href{https://github.com/leojklarner/Q-SAVI}{https://github.com/leojklarner/Q-SAVI}.

\subsection{Test Set Performances in \textsc{AUC-PRC} and \textsc{ACE}}
\label{app:sec_auprcs}

Following model training and hyperparameter selection, the predictive accuracy and calibration of the estimated test-set label probabilities were assessed using the area under the ROC curve (\textsc{AUC-ROC}) and the Brier score. These metrics allow for a direct comparison of models across test sets with different label distributions (see~\Cref{tab:results}). In addition, the performance of each algorithm was evaluated using the area under the precision-recall curve (\textsc{AUC-PRC}) and the adaptive calibration error (\textsc{ACE};~\citet{nixon2019measuring}). \textsc{AUC-PRC} and \textsc{ACE} are particularly well-suited for imbalanced datasets,  and provide a characterization of model performance that closely aligns with \textsc{AUC-ROC} and \textsc{Brier score} (see~\Cref{tab:results_auprc}). Note that their performance is not comparable across data splits, as it depends on the label distribution of a given test set---while the \textsc{AUC-ROC} of a no-skill classifier is $0.5$, its \textsc{AUC-PRC} is given by the positive label probability $p(y=1)$ of the test set.
\vspace{-0.5em}
\begin{table*}[h]
\centering
\caption{
    An overview of the test set performance of each model for each data splitting and featurization technique, quantified by the \textsc{AUC-PRC} ($\uparrow$) and the \textsc{ACE} ($\downarrow$) scores. All entries indicate the mean and standard error computed over $10$ independent training runs with different random seeds.
    The best models within a margin of statistical significance are highlighted in bold.
}
\small
\label{tab:results_auprc}
%
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{cl|cc|cc|cc|cc}
\toprule
 \multicolumn{2}{c|}{\multirow{2}{*}{Model \& Featurization}} & \multicolumn{2}{c|}{Spectral Split} & \multicolumn{2}{c|}{Weight Split} & \multicolumn{2}{c|}{Scaffold Split} & \multicolumn{2}{c}{Random Split}\\
 & & ECFP & rdkitFP &  ECFP & rdkitFP&  ECFP  & rdkitFP&  ECFP  & rdkitFP \\
\bottomrule
\multicolumn{9}{c}{}\\
\toprule
\parbox[t]{1.5mm}{\multirow{14}{*}{\rotatebox[origin=c]{90}{\textsc{AUC-PRC} ($\uparrow$)}}} 
& Logistic Regression & $.211\pms{.000}$ & $.140\pms{.000}$ & $.106\pms{.000}$ &  \cellcolor[gray]{0.9}$\mathbf{.112\pms{.000}}$ & $.211\pms{.000}$ & $.211\pms{.000}$ & $.248\pms{.000}$ & $.225\pms{.000}$\\
& Random Forest & $.207\pms{.005}$ & $.141\pms{.002}$ & $.090\pms{.002}$ & $.089\pms{.003}$ & $.165\pms{.002}$ & $.200\pms{.002}$ & \cellcolor[gray]{0.9} $\mathbf{.292\pms{.002}}$ & \cellcolor[gray]{0.9} $\mathbf{.294\pms{.002}}$\\
& MLP & $.208\pms{.003}$ & $.144\pms{.001}$ & $.090\pms{.003}$ & $.089\pms{.004}$ & $.180\pms{.005}$ & $.184\pms{.007}$ & $.270\pms{.004}$ & $.233\pms{.006}$\\
& Deep Ensemble & $.217\pms{.003}$ & $.144\pms{.001}$ & $.114\pms{.001}$ & $.102\pms{.003}$ & $.209\pms{.002}$ & $.209\pms{.002}$ & $.288\pms{.002}$ & $.266\pms{.004}$\\
\cmidrule(l){2-10}
& GIN & $.183\pms{.006}$ & $.149\pms{.005}$ & \multicolumn{2}{c|}{$.082\pms{.004}$} & \multicolumn{2}{c|}{$.202\pms{.005}$} & \multicolumn{2}{c}{$.218\pms{.005}$}\\
& GIN (attr masking) & $.192\pms{.003}$ & $.152\pms{.005}$ & \multicolumn{2}{c|}{$.108\pms{.003}$} & \multicolumn{2}{c|}{\cellcolor[gray]{0.9} $\mathbf{.245\pms{.004}}$} & \multicolumn{2}{c}{$.251\pms{.003}$}\\
& GIN (context pred) & $.188\pms{.004}$ & $.152\pms{.004}$ & \multicolumn{2}{c|}{$.098\pms{.002}$} & \multicolumn{2}{c|}{$.206\pms{.005}$} & \multicolumn{2}{c}{$.254\pms{.005}$}\\
& Grover & $.199\pms{.001}$ & $.139\pms{.002}$ & \multicolumn{2}{c|}{$.106\pms{.001}$} & \multicolumn{2}{c|}{$.204\pms{.004}$} & \multicolumn{2}{c}{$.227\pms{.003}$}\\
\cmidrule(l){2-10}
& IRM & $.154\pms{.004}$ &$.145\pms{.004}$&\multicolumn{2}{c|}{$.086\pms{.004}$}&\multicolumn{2}{c|}{$.178\pms{.005}$}&\multicolumn{2}{c}{$.176\pms{.004}$}\\
& GroupDRO &$.166\pms{.003}$&$.159\pms{.002}$&\multicolumn{2}{c|}{$.102\pms{.003}$}&\multicolumn{2}{c|}{$.202\pms{.003}$}&\multicolumn{2}{c}{$.172\pms{.005}$}\\
& DANN &$.156\pms{.003}$&$.155\pms{.005}$&\multicolumn{2}{c|}{$.095\pms{.005}$}&\multicolumn{2}{c|}{$.184\pms{.004}$}&\multicolumn{2}{c}{$.202\pms{.003}$}\\
& DeepCoral &$.154\pms{.004}$&$.151\pms{.004}$&\multicolumn{2}{c|}{$.091\pms{.003}$}&\multicolumn{2}{c|}{$.194\pms{.003}$}&\multicolumn{2}{c}{$.212\pms{.003}$}\\
\cmidrule(l){2-10}
& Q-SAVI & \cellcolor[gray]{0.9}$\mathbf{.221\pms{.003}}$ &  \cellcolor[gray]{0.9}$\mathbf{.165\pms{.004}}$ &  \cellcolor[gray]{0.9}$\mathbf{.121\pms{.002}}$ &  \cellcolor[gray]{0.9}$\mathbf{.111\pms{.003}}$ & $.197\pms{.003}$ & $.216\pms{.003}$ & $.239\pms{.002}$ & $.208\pms{.004}$\\
\bottomrule
\multicolumn{9}{c}{}\\
\toprule
\parbox[t]{1.5mm}{\multirow{14}{*}{\rotatebox[origin=c]{90}{\textsc{ACE} ($\downarrow$)}}}
& Logistic Regression & $.061\pms{.000}$ & $.055\pms{.000}$ & $.041\pms{.000}$ & $.034\pms{.000}$ & $.026\pms{.000}$ &  \cellcolor[gray]{0.9}$\mathbf{.025\pms{.000}}$ & $.018\pms{.000}$ & $.024\pms{.000}$\\
& Random Forest & $.078\pms{.001}$ &  \cellcolor[gray]{0.9}$\mathbf{.033\pms{.001}}$ & $.074\pms{.001}$ & $.087\pms{.001}$ & $.029\pms{.001}$ &  \cellcolor[gray]{0.9}$\mathbf{.025\pms{.001}}$ &  \cellcolor[gray]{0.9}$\mathbf{.016\pms{.001}}$ & $.035\pms{.001}$\\
& MLP & $.079\pms{.003}$ & $.052\pms{.003}$ & $.035\pms{.003}$ & $.055\pms{.007}$ & $.029\pms{.002}$ & $.044\pms{.011}$ & $.029\pms{.001}$ & $.026\pms{.002}$\\
& Deep Ensemble & $.078\pms{.004}$ & $.050\pms{.001}$ & $.025\pms{.001}$ & $.053\pms{.005}$ &  \cellcolor[gray]{0.9}$\mathbf{.022\pms{.001}}$ &  \cellcolor[gray]{0.9}$\mathbf{.025\pms{.001}}$ & $.023\pms{.001}$ & $.019\pms{.001}$\\
\cmidrule(l){2-10}
& GIN & $.064\pms{.004}$ & $.047\pms{.007}$ & \multicolumn{2}{c|}{$.036\pms{.003}$} & \multicolumn{2}{c|}{$.033\pms{.003}$} & \multicolumn{2}{c}{$.026\pms{.003}$}\\
& GIN (attr masking) &  \cellcolor[gray]{0.9}$\mathbf{.053\pms{.002}}$ & $.057\pms{.009}$ & \multicolumn{2}{c|}{$.038\pms{.002}$} & \multicolumn{2}{c|}{$.030\pms{.001}$} & \multicolumn{2}{c}{$.020\pms{.001}$}\\
& GIN (context pred) & $.078\pms{.002}$ & $.051\pms{.005}$ & \multicolumn{2}{c|}{$.034\pms{.003}$} & \multicolumn{2}{c|}{$.028\pms{.002}$} & \multicolumn{2}{c}{\cellcolor[gray]{0.9}$\mathbf{.015\pms{.001}}$}\\
& Grover & $.074\pms{.004}$ & \cellcolor[gray]{0.9} $\mathbf{.035\pms{.002}}$ & \multicolumn{2}{c|}{$.036\pms{.002}$} & \multicolumn{2}{c|}{$.038\pms{.002}$} & \multicolumn{2}{c}{$.020\pms{.001}$}\\
\cmidrule(l){2-10}
& IRM & $.071\pms{.003}$ &$.067\pms{.002}$&\multicolumn{2}{c|}{$.044\pms{.002}$}&\multicolumn{2}{c|}{$.035\pms{.001}$}&\multicolumn{2}{c}{$.024\pms{.002}$}\\
& GroupDRO &$.060\pms{.003}$&\cellcolor[gray]{0.9}$\mathbf{.035\pms{.003}}$&\multicolumn{2}{c|}{$.039\pms{.002}$}&\multicolumn{2}{c|}{$.036\pms{.002}$}&\multicolumn{2}{c}{$.026\pms{.001}$}\\
& DANN &$.057\pms{.002}$&$.046\pms{.003}$&\multicolumn{2}{c|}{$.035\pms{.003}$}&\multicolumn{2}{c|}{$.028\pms{.001}$}&\multicolumn{2}{c}{$.030\pms{.002}$}\\
& DeepCoral &$.097\pms{.006}$&\cellcolor[gray]{0.9}$\mathbf{.035\pms{.002}}$&\multicolumn{2}{c|}{$.041\pms{.004}$}&\multicolumn{2}{c|}{$.036\pms{.002}$}&\multicolumn{2}{c}{$.026\pms{.002}$}\\
\cmidrule(l){2-10}
& Q-SAVI &  \cellcolor[gray]{0.9}$\mathbf{.052\pms{.001}}$ & $.043\pms{.013}$ &  \cellcolor[gray]{0.9}$\mathbf{.015\pms{.001}}$ &  \cellcolor[gray]{0.9}$\mathbf{.016\pms{.001}}$ & $.036\pms{0
.002}$ &  \cellcolor[gray]{0.9}$\mathbf{.025\pms{.002}}$ & $.021\pms{.001}$ & $.024\pms{.002}$\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}



\clearpage




\subsection{Model Implementations and Hyperparameter Ranges}
\label{app:impl_details}

To ensure a fair and meaningful comparison of the evaluated machine learning models, the hyperparameters of each algorithm were independently optimized for every data split and featurization technique. The following sections provide comprehensive details about the implementation and hyperparameter ranges used for each model in our empirical evaluation.

\begin{itemize}
\setlength{\itemsep}{-2pt}
\item Logistic Regression (Section~\ref{app_sec:det_log_reg})
\item Random Forest Classifiers (Section~\ref{app_sec:det_rf})
\item Multi-layer Perceptrons (Section~\ref{app_sec:det_mlp})
\item Deep Ensembles (Section~\ref{app_sec:det_deep_ens})
\item Pre-trained Graph Neural Networks (Section~\ref{app_sec:det_gnn})
\item GROVER (Section~\ref{app_sec:det_grover})
\item Domain Adaptation and Generalization Techniques (Section~\ref{app_sec:det_domain_adapt})
\item Our Probabilistic Regularization Scheme (Section~\ref{app_sec:det_ours})
\end{itemize}



\subsubsection{Logistic Regression}
\label{app_sec:det_log_reg}

The \textbf{logistic regression models} were trained with the scikit-learn library \cite{scikit-learn} using the \textsc{liblinear} solver \cite{fan2008liblinear} with a maximum of 1000 iterations and a stopping tolerance of \num{1e-4}. They were independently fit for all hyperparameter combinations specified in\Cref{tab:hyper_logreg}, using the combination with the best unweighted validation set log-likelihood to choose the best hyperparameter setting to evaluate on the held-out test set.

\setlength{\tabcolsep}{20.0pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for Logistic Regression}
\label{tab:hyper_logreg}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Linear Regression & regularization type & $\ell1$, $\ell2$ \\
 & regularization strength & \num{1.0e-4}, \num{2.6e-04} \ldots, \num{3.8e+03}, \num{1.0e+04} \\
 & class weight & none, balanced \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Random Forest Classifiers}
\label{app_sec:det_rf}
 
The \textbf{random forest models} were trained with the scikit-learn library \cite{scikit-learn} using 100 decision trees and the \textsc{gini} splitting criterion.
They were independently fit for all hyperparameter combinations specified in~\Cref{tab:rfr_hypers}, using the combination with the best unweighted validation set log-likelihood to choose the best hyperparameter setting to evaluate on the held-out test set.

\setlength{\tabcolsep}{32.0pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for Random Forest Classifiers}
\label{tab:rfr_hypers}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Random Forest & maximum depth & 5, 15, 26, 36, 47, 57, 68, 78, 89, 100 \\
 & min. samples per split & 5, 15, 50, 100 \\
 & min. samples per leaf & 1, 5, 10, 30, 100 \\
 & class weight & none, balanced \\
\bottomrule
\end{tabular}
\end{table}


\clearpage


\subsubsection{Multi-Layer Perceptrons}
\label{app_sec:det_mlp}

The \textbf{multi-layer perceptrons} were implemented with the PyTorch library \cite{paszke2019pytorch}, using rectified linear units \cite{nair2010rectified} as activation functions. Their weights were initialized using a Normal distribution $\mathcal{N}(0,1)$ truncated at $\pm2\sigma$, with biases initialized at zero. These parameters were optimized on the training set using the \textsc{AdamW} stochastic gradient descent optimizer \cite{loshchilov2017decoupled} with a batch size of 128 and the cross-entropy loss for a maximum of 500 epochs, using early stopping to terminate training if the unweighted log-likelihood on the validation set did not decrease for more than 10 epochs, reverting to the checkpoint with best validation set log-likelihood for evaluating their performance for hyperparameter optimization and the subsequent on the held-out test set. Batch normalization and dropout were applied after the ReLU non-lineary. The full hyperparameter search space is presented in~\Cref{table:mlp_hypers}.

\setlength{\tabcolsep}{28.0pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for Multi-layer Perceptrons}
\label{table:mlp_hypers}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Multi-layer Perceptron & learning rate & \num{1e-4}, \num{1e-3} \\
 & weight decay & \num{1e-3}, \num{1e-2}, \num{1e-1} \\
 & number of layers & 2, 4, 6 \\
 & embedding dimension & 32, 64 \\
 & batch normalization (BN) & yes, no \\
 & BN running statistics & yes, no \\
 & dropout & 0.0, 0.2, 0.5 \\
 & class weight & none, balanced \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Deep Ensembles}
\label{app_sec:det_deep_ens}

The \textbf{deep ensembles} were trained using an identical setup to the multi-layer perceptrons, with the distinction that $M=5$ independent networks were trained with different random seeds and evaluated with respect to their average log-likelihood on the validation set. Similarly, at inference time the class probabilities were averaged across ensembles. The full hyperparameter search space is presented in~\Cref{tab:deep_ensembles_hypers} and is identical to~\Cref{table:mlp_hypers}.

\setlength{\tabcolsep}{33.0pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for Deep Ensembles}
\label{tab:deep_ensembles_hypers}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Deep Ensemble & learning rate & \num{1e-4}, \num{1e-3} \\
 & weight decay & \num{1e-3}, \num{1e-2}, \num{1e-1} \\
 & number of layers & 2, 4, 6 \\
 & embedding dimension & 32, 64 \\
 & batch normalization (BN) & yes, no \\
 & BN running statistics & yes, no \\
 & dropout & 0.0, 0.2, 0.5 \\
 & class weight & none, balanced \\
\bottomrule
\end{tabular}
\end{table}


\clearpage


\subsubsection{Pre-trained Graph Neural Networks}
\label{app_sec:det_gnn}

The graph featurization pipeline, architectures,  and pre-trained initializations of the graph isomorphism networks presented in~\citet{hu2019strategies} were retrieved from the paper's official GitHub repository and fine-tuned on the training set using the \textsc{AdamW} optimizer \cite{loshchilov2017decoupled} with a batch size of 128 and the cross-entropy loss for a maximum of 500 epochs, using early stopping to terminate training if the unweighted log-likelihood on the validation set did not decrease for more than 10 epochs and reverting to the checkpoint with best validation set log-likelihood for evaluating their performance for hyperparameter optimization and the subsequent on the held-out test set. The full hyperparameter search space is presented in~\Cref{tab:pre_trained_hypers}. The pre-trained initializations were provided for networks with $5$ layers of $300$ hidden units, set up using batch normalization with running statistics.

\setlength{\tabcolsep}{22.5pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for Pre-trained GINs}
\label{tab:pre_trained_hypers}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Pre-trained GINs & learning rate & \num{1e-4}, \num{3e-3}, \num{1e-3}, \num{3e-3}, \num{1e-2} \\
 & weight decay & \num{1e-3}, \num{1e-2}, \num{1e-1} \\
 & dropout & 0.0, 0.2, 0.5 \\
 & class weight & none, balanced \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{GROVER}
\label{app_sec:det_grover}

All code, models, and initializations required to fine-tune the pre-trained graph transformers presented in~\citet{rong2020self} was retrieved from the paper's official GitHub repository and fine-tuned on the training set with a batch size of 128 for a maximum of 500 epochs, using early stopping to terminate training if the unweighted log-likelihood on the validation set did not decrease for more than 10 epochs and reverting to the checkpoint with best validation set log-likelihood for evaluating their performance for hyperparameter optimization and the held-out test set. The hyperparameters specifying the number of layers and their embedding dimension indicate the size of the MLP fit on top of the pre-trained molecular representations produced by the \textsc{GROVER} base model and were chosen to be identical to the other MLP-based deep learning algorithms. The full hyperparameter search space is presented in~\Cref{tab:grover_hypers}.

\setlength{\tabcolsep}{38.5pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for \textsc{GROVER}}
\label{tab:grover_hypers}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
\textsc{GROVER} & learning rate & \num{1e-4}, \num{1e-3} \\
 & weight decay & \num{1e-3}, \num{1e-2}, \num{1e-1} \\
 & dropout & 0.0, 0.2, 0.5 \\
 & number of layers & 2, 4, 6 \\
 & embedding dimension & 32, 64 \\
\bottomrule
\end{tabular}
\end{table}


\clearpage


\subsubsection{Domain Adaptation Techniques}
\label{app_sec:det_domain_adapt}

All code and featurization utilities required to run the evaluated domain adaptation and generalization techniques, namely invariant risk minimization (\textbf{IRM}), group-distributionally robust training (\textbf{GroupDRO}), domain-adversarial networks (\textbf{DANN}) and deep correlation alignment (\textbf{DeepCoral}), were adapted from \citet{ji2022drugood} and provided with data split-specific domain indicators. For this, the training set was additionally split into three domains, either using spectral clustering, molecular weight thresholds, a grouped scaffold split, or random partitions. 
All models used the default architecture choice in~\citet{ji2022drugood}---a graph isomorphism network with 4 layers and 128 hidden units---and trained according to the respective optimization procedures. The full hyperparameter range is presented in~\Cref{tab:domain_adapt_hypers}.

\setlength{\tabcolsep}{23.5pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for Domain Adaptation Techniques}
\label{tab:domain_adapt_hypers}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
IRM/GroupDRO/DANN/DeepCoral & learning rate & \num{1e-4}, \num{1e-3} \\
 & weight decay & \num{1e-3}, \num{1e-2}, \num{1e-1} \\
 & dropout & 0.0, 0.2, 0.5 \\
\bottomrule
\end{tabular}
%
\end{table}



%



\subsubsection{Q-SAVI}
\label{app_sec:det_ours}

The models based on our probabilistic regularization scheme were trained using the implementation of the local linearization scheme presented in~\citet{Rudner2022fsvi, Rudner2022sfsvi} provided by the authors and using the exact same architecture, initialization, and optimization procedures as for the multi-layer perceptrons and deep ensembles---differing only in the objective function. 
Specifically, at each gradient step iteration, a sample of $M$ molecules (where $M$ is a hyperparameter) was drawn from a uniform distribution over the ZINC database~\citep{irwin2020zinc20}, providing a set of context points on which to evaluate the objective in~\Cref{eq:objective}, using the Bernoulli likelihood to specify $\log p(\by_{\calD} \vbar f(\bx_{\calD} ; \btheta))$.
%
%
%
%
%
%
%
To construct a prior distribution over parametric function mappings $p_{f(\{ \bX, \bX_{\calC} \} ; \bTheta)}$ that maximizes predictive uncertainty away from the training data, it was defined as a distribution over functions with a logit-space mean vector of approximately zero and minimal structure in the off-diagonal entries of its covariance matrix. We refer to our code repository for further implementational details. The full hyperparameter search space is presented in~\Cref{tab:ours_hypers}.

\setlength{\tabcolsep}{31.0pt}
\begin{table}[H]
\centering
\caption{Hyperparameters for Our Model}
\label{tab:ours_hypers}
\vspace{0.3em}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Search Space} \\
\midrule
Q-SAVI & learning rate & \num{1e-4}, \num{1e-3} \\
 & number of layers & 2, 4, 6 \\
 & embedding dimension & 32, 64 \\
 & prior variance & \num{1e-2}, \num{1e-1}, \num{1e0}, \num{1e1}, \num{1e2} \\
 & context points per sample & 16, 128 \\
\bottomrule
\end{tabular}
\end{table}


\clearpage


\subsection{Ablation Studies}
\label{app_sec:ablation_plots}

To understand the impact of different hyperparameters on the performance of our proposed probabilistic regularization scheme, we conducted a series of ablation experiments. In these experiments, we systematically varied the hyperparameters relevant to evaluating the objective in~\Cref{eq:objective}---namely the prior variance and the number of sampled context points---while keeping others fixed, and measured their effects on the test set \textsc{AUC-ROC} and \textsc{Brier Score}. The resulting ablation plots are presented in~\Cref{fig:prior_cov_effect,fig:n_inducing_inputs_effect} and show that larger prior covariances are strongly correlated with more robust test-set performances across splits---while the effect of larger context point samples is less pronounced.

% Figure environment removed

% Figure environment removed

