\section{Introduction}

Discovering novel drug candidates that are able to safely and effectively treat neglected diseases or combat multidrug-resistant pathogens is a challenging biomedical research problem of considerable scientific and societal importance. 
Leveraging modern deep learning algorithms to accurately predict clinically relevant molecular properties and reduce the need for time- and resource-intensive experiments has the potential to significantly accelerate the development of promising and innovative chemical leads in drug discovery.
% Figure environment removed

A key feature of practical early-stage drug discovery research is the application of predictive models to novel compounds that are \textit{structurally or functionally dissimilar} to molecules that have already been explored (see~\Cref{fig:graphical_abstract}). 
%
%
%
%
In such an \textit{extrapolative regime}, the practical utility of machine learning systems hinges on their ability to \begin{enumerate*}[label=(\alph*)] \item robustly generalize to unexplored areas of chemical space and \item reliably indicate when they fail to do so by generating well-calibrated predictive uncertainty estimates\end{enumerate*}.
% Figure environment removed
However, standard deep learning algorithms often perform poorly under covariate shift, generating both incorrect and highly miscalibrated predictions \citep{ovadia2019uncertainty, koh2021wilds}. 
This is particularly problematic in the context of early-stage drug discovery, where experimental labels are expensive to acquire and therefore only available for a small and often highly biased subset of compounds.

To improve the predictive performance of deep learning algorithms in such resource-constrained, low-data settings, we may wish to use relevant prior knowledge about the problem domain to specify inductive biases that make some predictive functions more likely than others.
Common approaches to imbuing neural networks with useful inductive biases include \begin{enumerate*}[label=(\alph*)]
    \item pre-training them on larger, potentially unlabeled datasets \citep{finn2017model, tan2018survey, bommasani2021opportunities} and 
    \item adjusting their architectures to mirror appropriate invariances of their input domain \citep{bronstein2017geometric, satorras2021n}.
\end{enumerate*}
%
However, these approaches are only an indirect---and often insufficiently precise---way of translating explicit modeling preferences into constraints over a neural network's hypothesis space.

In this paper, we present an alternative approach.
%
%
To encode domain-informed prior knowledge of the data-generating process into neural network training, we specify a prior distribution over the space of \textbf{Q}uantitative \textbf{S}tructure-\textbf{A}ctivity mappings evaluated at a carefully selected set of context points, and perform \textbf{V}ariational \textbf{I}nference in the resulting probabilistic model (see~\Cref{fig:science_background}). 
We will refer to this method as \textbf{\textsc{q-savi}}.

To demonstrate the practical utility of this approach, we construct a robust evaluation setup based on a carefully pre-processed bioactivity dataset.
We then apply several different techniques to induce strong covariate and label shifts, resulting in challenging and practically meaningful train-test splits.
Finally, we use \textsc{q-savi} to specify explicit and problem-informed prior knowledge of drug-like chemical space and show that this substantially improves the predictive accuracy and calibration of deep learning algorithms in an out-of-distribution setting, outperforming a range of strong self-supervised pre-training, domain adaptation, and ensembling techniques.
%
\codebox{Code and datasets are provided at}{\begin{center}
    \footnotesize
    \url{https://github.com/leojklarner/Q-SAVI}.
\end{center}}
\vspace*{-10pt}


\newpage

%

%
%

%

%
%
%
%
%
%
%
%

%
%

%
%

%
%
%
%
%
%

%

%
%

%
%
%