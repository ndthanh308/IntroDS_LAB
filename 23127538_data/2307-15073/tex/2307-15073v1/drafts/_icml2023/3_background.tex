
\section{Predicting Properties to Discover Drugs}
\label{sec:background}

%

The overarching objective of small molecule drug discovery is to identify compounds that modulate a biological target of interest and elicit a therapeutically beneficial response.
Unfortunately, the process of discovering a promising candidate to take into clinical trials is difficult and often unsuccessful, as the search space of viable drug-like molecules \mbox{$\mathcal{X}=\{m_1,m_2, ...\}$} is vast, with estimates of $\vert\mathcal{X}\vert$ ranging from $10^{20}$ to $10^{60}$ \cite{bohacek1996art, ertl2003cheminformatics, polishchuk2013estimation}.
This is compounded by the inherent experimental limitations of medicinal chemistry, meaning that labels can only be acquired for a vanishingly small subset of compounds \mbox{$\calX'\subset\calX$}, with \mbox{$|\calX'|\ll|\calX|$}.
Naturally, this has generated substantial interest in training supervised machine learning algorithms on available data \mbox{$\calD = \{ (\bx_{i}, \by_{i}) | \bx_i\in \calX', \by_i\in\calY\}_{i=1}^N $} to predict the properties of compounds in \mbox{$\calX\setminus\calX'$}.

As the purpose of such models is to accelerate the discovery of novel and more effective therapeutics, predictions are usually desired most on compounds that are meaningfully dissimilar to molecules in $\calX'$.
Were $\calX'$ sampled uniformly from $\calX$, that is, \mbox{$\calX'\sim \mathcal{U}(\calX)$}, these predictions would be made in an interpolative regime, in which standard regularization techniques such as weight decay, dropout \cite{srivastava2014dropout}, and batch normalization \cite{ioffe2015batch} constitute effective approaches to minimizing the expected loss on new samples from $\mathcal{U}(\calX)$.

In practice, however, the composition of $\calX'$ is largely determined by empirical considerations such as compound availability and the preferences and intuitions of medicinal chemists, resulting in a highly biased subsample $\calX'\sim \Tilde{p}_{\calX}$.
This means that, in order to reliably predict the properties of novel and scientifically interesting compounds, it is essential for machine learning algorithms to perform well in an extrapolative regime.
As this requirement is distinct from in-distribution generalization, standard approaches to regularization are unlikely to be effective.

Instead, we propose an alternative regularization scheme---\textsc{Q-SAVI}---that builds on the fact that we are able to approximately sample from $\mathcal{U}(\calX)$ through large chemical databases such as \textsc{ZINC} \citep{irwin2020zinc20} or \textsc{GDB} \cite{polishchuk2013estimation} to specify arbitrary modeling preferences on \mbox{$\calX\setminus\calX'$}.
Specifically, we construct a probabilistic model of neural network \textit{functions} and define a tractable prior distribution over parametric function mappings evaluated at points in $\mathcal{U}(\calX)$. 
We then extend this probabilistic model to include a label-space prior over $\calX$, which encodes contextualized information on $\Tilde{p}_{\calX}$ and $\calY$, and demonstrate empirically that variational inference in this probabilistic model results in neural networks that make accurate predictions in regions of chemical space that they can reliably extrapolate to while generating well-calibrated predictive uncertainty estimates that indicate when correct predictions are unlikely.

%
%

%
%