\section{Empirical Evaluation}
\label{sec:experiments}

To demonstrate the practical utility of \textsc{q-savi}, we establish a robust evaluation setup:
In~\Cref{sec:data_curation}, we argue that many commonly-used bioactivity datasets may not be able to meaningfully assess the extrapolative power of supervised machine learning algorithms and present a carefully cleaned and pre-processed alternative dataset and in~\Cref{sec:data_shift}, we define an appropriate set of statistics to quantify covariate and label shifts in chemical space and use them to investigate the extent to which different splitting techniques induce data shift.
In~\Cref{sec:models_and_eval}, we then use this experimental setup to demonstrate that employing \textsc{q-savi} to incorporate domain-informed prior knowledge into the modeling process leads to significant gains in predictive accuracy and calibration, outperforming a range of strong self-supervised pre-training, domain adaptation, and ensembling techniques.
Finally, in~\Cref{sec:merck-datasets}, we show that these strong empirical results extend to real-world production settings by evaluating our method on the time-split data presented in \citet{ma2015deep}.


\subsection{Curating an Appropriate Dataset}
\label{sec:data_curation}

A fundamental obstacle to training and evaluating \textsc{QSAR} models in the public domain is the scarcity of sufficiently large datasets with high-quality labels~\citep{schneider2020rethinking}.
Even though collections of publicly available bioactivity data exist~\citep{wu2018moleculenet, huang2021therapeutics}, they are often sourced directly from repositories of high-throughput screening (HTS) data such as \textsc{PubChem}~\citep{kim2019pubchem}, \textsc{ChEMBL}~\citep{mendez2019chembl} or \textsc{ToxCast}~\citep{richard2016toxcast} without significant filtering or pre-processing.
While this approach maximizes the number of available data points, it may reduce the discriminative power of model performance comparisons.
For instance, a well-known problem of confirmatory dose-response screens---which make up the bulk of measurements in the above repositories---is that they usually contain a large number of reproducible false positive readouts (in many cases up to 95\% of hits~\citep{thorne2010apparent}) caused by molecular substructures that interfere with an assay's readout system \citep{baell2010new,dahlin2015pains}.
Using such data without further processing runs the risk of simply testing for the ability of algorithms to memorize these substructures instead of assessing meaningful extrapolative performance \citep{klarner2022bias}.

To curate a dataset of sufficient quality to enable an informative comparison of predictive models, we used the measurement meta-data of bioactivity and toxicity screens 
%
to prioritize certain data points for further inspection.
After surveying the publications associated with the most promising datasets, we selected a high-quality screening campaign for inhibitors of the development of liver-stage malaria parasites for further processing \citep{antonova2018open}.

Specifically, we retrieved and reprocessed the raw measurement data to remove likely false positives and other experimental artifacts, yielding a binary classification dataset with 7,301 inactive and 849 active molecules, each measured in biological duplicate and confirmed as a true positive or negative through a set of quality-assuring counter-screens (see~\Cref{appsec:dataset_processing} for full details).

% Figure environment removed


\subsection{Inducing and Quantifying Data Shift}
\label{sec:data_shift}

\paragraph{Featurization.}
Commonly used techniques to numerically represent the structural properties of a molecule include strings, graphs, and topological fingerprints. 
For the following experiments, each molecule was featurized as both an extended-connectivity fingerprint (\textsc{ECFP}; \citet{rogers2010extended}) and an \textsc{RDKit} fingerprint (\textsc{rdkitFPs}), using the respective implementations in the open-source cheminformatics package \textsc{RDKit}~\citep{landrum2013rdkit}.
An illustration of this process is presented in~\Cref{fig:featurizaton}.

\paragraph{Statistics for covariate and label shift.}
To evaluate the extent to which different train-test splits induce covariate and label shift, we identified  a set of suitable two-sample test statistics and used it to quantify the dissimilarity of the marginal covariate and label distributions of the respective training and test sets 
$\calD_\text{tr}=(\bX_\text{tr}, \by_\text{tr})$ and $\calD_\text{te}=(\bX_\text{te}, \by_\text{te})$.

%
%
%
%

Since $\by_\text{tr}$ and $\by_\text{te}$ consist of binary indicators of antimalarial activity, well-established categorical statistics such as Fisher's exact test~\citep{upton1992fisher} are applicable.
In the following, its negative logarithmic p-value is used as a scalar indicator of label shift.  %

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

Defining a corresponding statistic to quantify covariate shift between two sets of molecules is more challenging, as they constitute disjoint sets of discrete objects.
For this purpose, we used the maximum mean discrepancy (\textsc{MMD}) metric~\citep{gretton2012kernel} to quantify the difference between two samples of molecules as the distance between the embeddings of their expectations in a reproducing kernel Hilbert space (\textsc{RKHS}) defined by some mapping $\phi:\calX\to\mathcal{H}$ and an associated kernel function $k(\bx_i, \bx_j)=\langle\phi(\bx_i), \phi(\bx_j)\rangle_\mathcal{H}$.
An empirical estimator of this statistic is obtained by
\begin{align*} 
    &
    \operatorname{\textsc{MMD}}^2(\bX_\text{tr},\bX_\text{te})
    \\
    &
    =
    \left\lVert \mathbb{E}_{\bx\in\bX_\text{tr}}\left[\phi(\bx)\right] - \mathbb{E}_{\bx\in\bX_\text{te}}\left[\phi(\bx)\right] \right\rVert_\mathcal{H}^2
    \\
    &
    =
    \mathbb{E}_{\bx_i, \bx_j\in\bX_\text{tr}}\left[k(\bx_i, \bx_j)\right] + \mathbb{E}_{\bx_i, \bx_j\in\bX_\text{te}}\left[k(\bx_i, \bx_j)\right]
    \\
    &
    \qquad
    - 2\mathbb{E}_{\bx_i\in\bX_\text{tr}, \bx_j\in\bX_\text{te}}\left[k(\bx_i, \bx_j)\right],
\end{align*}
using the Jaccard/Tanimoto similarity coefficient 
\begin{align*}
k_\text{jac}(\bx_i,\bx_j)=\frac{\bx_i\cap\bx_j}{\bx_i\cup\bx_j}=\frac{\left\langle\bx_i,\bx_j\right\rangle}{\left\lVert\bx_i\right\rVert^2+\left\lVert\bx_j\right\rVert^2-\left\langle\bx_i,\bx_j\right\rangle}
\end{align*}
as an appropriate similarity metric, both due to its established use in the cheminformatics community~\citep{bajusz2015tanimoto} and the favorable properties of the \textsc{RKHS} that it induces.
The \textsc{MMD} statistic is only valid if the mean embedding $\mathbb{E}_{\bx\in\bX}\left[\phi(\bx)\right]$ is injective, which is the case for strictly positive definite kernels operating in discrete domains~\citep{borgwardt2006integrating}, such as $k_\text{jac}$~\citep{bouchard2013proof}.

% Figure environment removed

\paragraph{Random and scaffold splits.}
\label{par:data_splits}
Equipped with the appropriate statistical tools to quantify distributional similarities, we investigated the extent to which different train-test splits are able to emulate practically relevant covariate and label shifts, beginning with the two most popular approaches of splitting data either randomly or by scaffold. 
While \emph{random splits} are commonly used in many domains, they are known to produce unrealistically optimistic performance estimates in the context of molecular property prediction.
This is a consequence of the biased composition of many experimental datasets, which often contain structurally similar compounds from so-called chemical series.
As these often exhibit very similar properties, distributing them evenly across data splits leads to a de-facto overlap between training and test sets that incentivizes overfitting and memorization~\citep{wallach2018most}.
\emph{Scaffold splits} attempt to mitigate this shortcoming by mapping each molecule to an overarching compound class---usually its Bemis-Murcko scaffold~\citep{bemis1996properties, bemis1999properties}---and splitting the data so that all molecules of a given scaffold are assigned to the same partition.
However, this approach often results in a similar pathology, as even molecules with nominally different scaffolds can exhibit a high degree of structural and functional similarity, as illustrated in~\Cref{fig:scaffolds}.

% Figure environment removed
\vspace{-1em}
\paragraph{Molecular weight and spectral splits.}
To facilitate the comparison of models in an extrapolative regime, we explored two alternative approaches.
A straightforward \textit{molecular weight split} was used to induce data shift by assigning molecules into training and test sets based on a molecular weight cut-off, relying on the correlation of molecular size and binding strength to also induce strong label shift \citep{hopkins2014role}.
More rigorously, we developed a clustering-based \emph{spectral split} to generate data splits that are guaranteed to exhibit maximal covariate shift under the \textsc{MMD} statistic.
By interpreting the Jaccard kernel Gram matrix $\mathbf{W}^\text{jac}\in[0,1]^{\vert\bX_\calD\vert\times\vert\bX_\calD\vert}$ of a given set of molecules $\bX_\calD$ as the weighted adjacency matrix of a fully-connected similarity graph $S$, well-established spectral clustering algorithms~\citep{von2007tutorial} can be employed to identify an optimal partitioning of $S$ that maximizes the similarity within and minimizes the similarity between partitions.

We present a comparison of the resulting covariate and label shift statistics in~\Cref{tab:data_shift}, which shows that molecular weight and spectral clustering-based splits generate a significantly more extrapolative evaluation setup than random and scaffold splits. This is substantiated by the qualitative visualization presented in~\Cref{fig:split_umap}.
\vspace{-0.5em}
\begin{table}[H]
\centering
\caption{
    A summary of the covariate and label shifts induced by the different train-test splits presented in~\Cref{sec:data_shift}, using rdkit and extended-connectivity (EC) fingerprints. 
    Covariate shift is quantified as the Jaccard kernel-based {\textsc{MMD}} statistic, while label shift is quantified as the negative $\log p$-value of Fisher's exact test.
}
\label{tab:data_shift}
\vspace{1pt}
\small
\begin{tabularx}{\columnwidth}{lcc}
\toprule
Split & Covariate Shift \emph{(rdkit, EC)} & Label Shift \emph{(rdkit, EC)}\\
\midrule
Random & $0.00$, $0.00$ & \num{0.00}\\
Scaffold & $0.08$, $0.07$ & \num{4.23} \\
Weight & $0.14$, $0.10$ & $\mathbf{61.96}$ \\
Spectral & $\mathbf{0.34}$, $\mathbf{0.25}$ &  $17.49$, $50.05$ \\
\bottomrule
\end{tabularx}
\end{table}


\subsection{Model Construction, Baselines \&  Results}
\label{sec:models_and_eval}


\paragraph{Model construction.}
Using the increasingly data-shifted splits constructed in \Cref{sec:data_curation,sec:data_shift}, we assessed \textsc{q-savi} with respect to its ability to improve the predictive accuracy and calibration of deep learning algorithms under covariate and label shifts.
By leveraging the option to specify both an arbitrary context point distribution $p_{\bX_{\calC}}$ and a prior distribution over parametric function mappings $p_{f(\{ \bX, \bX_{\calC} \} ; \bTheta)}$, we used \textsc{q-savi} to encode relevant information about both the input domain and the label space of the problem setup into the model.
Specifically, we precomputed the featurizations of a uniform subsample of the \textsc{ZINC} database of commercially available compounds \citep{irwin2020zinc20} and used them to construct a uniform context point distribution $p_{\bX_{\calC}}=\mathcal{U}(\Bar{\calX})$ over a set of \num{2e6} synthetically accessible drug-like molecules $\Bar{\calX}$. 
Additionally, we used the prior distribution $p_{f(\{ \bX, \bX_{\calC} \}; \bTheta)}$ over parametric mappings to encode an informative function-space prior that encourages high predictive uncertainty in unexplored regions of chemical space, counteracting the likelihood term in~\Cref{eq:objective} to generate better predictive uncertainty estimates. 

%

\paragraph{Baselines.}
We compared the performance of the resulting probabilistic model to a range of standard baselines and state-of-the-art pre-training and domain adaptation techniques.
The simplest of these models is regularized \textbf{logistic regression}, which is expected to underperform in an extrapolative regime due to the linearity of its logit function.
While \textbf{random forest classifiers}~\citep{breiman2001random} represent a more flexible baseline with strong in-distribution generalization guarantees, they generally exhibit coarser decision boundaries at the fringes of the training distribution that are unlikely to perform well on covariate-shifted inputs.
Standard deep learning methods such as multi-layer perceptrons \textbf{(MLPs)} have an even higher representational capacity, yet also generally underperform under data shift, yielding both incorrect and highly overconfident predictions~\citep{ovadia2019uncertainty, koh2021wilds}.
\textbf{Deep ensembles} are an effective technique to improve the predictive performance of MLPs by averaging the predictive distributions of a set of independently trained neural networks~\citep{lakshminarayanan2017simple}.
To investigate the extent to which existing self-supervised pre-training techniques and more expressive model architectures impact the performance of deep learning algorithms in this setting, we fine-tuned graph isomorphism networks (\textbf{GINs}; \citet{xu2018powerful}) provided by \citet{hu2019strategies} both from scratch and from initializations that were pre-trained on compounds from the ZINC database using \emph{context prediction} and \emph{attribute masking} objectives.
Additionally, we fine-tuned the graph transformer (\textbf{GROVER}) proposed by \citet{rong2020self} from a pre-trained initialization that was optimized on molecules from the ZINC and ChEMBL databases using self-supervised contextual property and graph-level motif prediction techniques.
Finally, we adapted a range of domain adaptation and generalization techniques, including invariant risk minimization (\textbf{IRM}; \citet{arjovsky2019invariant}), group-distributionally robust training (\textbf{GroupDRO}; \citet{sagawa2019distributionally}), domain-adversarial networks (\textbf{DANN}; \citet{ganin2016domain}), and deep correlation alignment (\textbf{DeepCoral}; \citet{sun2016deep}) from \citet{ji2022drugood} who provided them with data split-specific domain indicators. 
%

\setlength{\tabcolsep}{5.0pt}
\begin{table*}[t]
\centering
\caption{
    An overview of the test set performance of each model for each data split and featurization technique, quantified by the \textsc{AUC-ROC} ($\uparrow$) and the \textsc{Brier score} ($\downarrow$).
    All entries indicate the mean and standard errors computed over $10$ independent training runs with different random seeds.
    The best models within a margin of statistical significance are highlighted in bold.
}
\small
\label{tab:results}
\vspace{1pt}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{cl|cc|cc|cc|cc}
\toprule
 \multicolumn{2}{c|}{\multirow{2}{*}{Model \& Featurization}} & \multicolumn{2}{c|}{Spectral Split} & \multicolumn{2}{c|}{Weight Split} & \multicolumn{2}{c|}{Scaffold Split} & \multicolumn{2}{c}{Random Split}\\
 & & ECFP & rdkitFP &  ECFP & rdkitFP&  ECFP  & rdkitFP&  ECFP  & rdkitFP \\
\bottomrule
\multicolumn{9}{c}{}\\
\toprule
\parbox[t]{1.5mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{\textsc{AUC-ROC} ($\uparrow$)}}} & Logistic Regression & $.583\pms{.000}$ & $.551\pms{.000}$ & $.626\pms{.000}$ & $.632\pms{.000}$ & $.684\pms{.000}$ & $.698\pms{.000}$ & $.704\pms{.000}$ & $.687\pms{.000}$\\
& Random Forest & $.576\pms{.009}$ & $.552\pms{.006}$ & $.592\pms{.006}$ & $.567\pms{.004}$ & $.605\pms{.004}$ & $.642\pms{.003}$ & $.696\pms{
.002}$ & $.690\pms{.002}$\\
& MLP & $.574\pms{.006}$ & $.571\pms{.003}$ & $.614\pms{.004}$ & $.577\pms{.005}$ & $.625\pms{.010}$ & $.631\pms{.014}$ & \cellcolor[gray]{0.9}$\mathbf{.720\pms{.002}}$ & $.692\pms{.004}$\\
& Deep Ensemble & $.589\pms{.006}$ & $.571\pms{
.002}$ & $.644\pms{.001}$ & $.594\pms{.002}$ & $.679\pms{.001}$ & \cellcolor[gray]{0.9} $\mathbf{.697\pms{.003}}$ & \cellcolor[gray]{0.9}$\mathbf{.720\pms{.001}}$ & \cellcolor[gray]{0.9} $\mathbf{.710\pms{.003}}$\\
\cmidrule(l){2-10}
& GIN & $.549\pms{.009}$ & $.551\pms{.007}$ & \multicolumn{2}{c|}{$.582\pms{.007}$} & \multicolumn{2}{c|}{$.664\pms{.005}$} & \multicolumn{2}{c}{$.685\pms{.004}$}\\
& GIN (attr masking) & $.588\pms{.004}$ & $.559\pms{.010}$ & \multicolumn{2}{c|}{$.625\pms{.004}$} & \multicolumn{2}{c|}{\cellcolor[gray]{0.9}$\mathbf{.700\pms{.002}}$} & \multicolumn{2}{c}{$.705\pms{.002}$}\\
& GIN (context pred) & $.541\pms{.005}$ & $.566\pms{.009}$ & \multicolumn{2}{c|}{$.621\pms{.003}$} & \multicolumn{2}{c|}{$.674\pms{.003}$} & \multicolumn{2}{c}{\cellcolor[gray]{0.9} $\mathbf{.713\pms{.003}}$}\\
& Grover & $.574\pms{.002}$ & $.544\pms{.006}$ & \multicolumn{2}{c|}{$.623\pms{.003}$} & \multicolumn{2}{c|}{$.689\pms{.003}$} & \multicolumn{2}{c}{$.701\pms{.001}$}\\
\cmidrule(l){2-10}
& \textbf{Q-SAVI} & \cellcolor[gray]{0.9}$\mathbf{.606\pms{.003}}$ & \cellcolor[gray]{0.9}$\mathbf{.603\pms{.006}}$ & \cellcolor[gray]{0.9}$\mathbf{.650\pms{.002}}$ & \cellcolor[gray]{0.9}$\mathbf{.643\pms{.003}}$ & $.657\pms{.004}$ & \cellcolor[gray]{0.9} $\mathbf{.701\pms{.002}}$ & $.708\pms{.001}$ & $.681\pms{.002}$\\
\bottomrule
\multicolumn{9}{c}{}\\
\toprule
\parbox[t]{1.5mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{\textsc{Brier score} ($\downarrow$)}}}& Logistic Regression & $.131\pms{.000}$ & $.111\pms{.000}$ & $.051\pms{.000}$ & $.049\pms{.000}$ & $.101\pms{.000}$ & $.100\pms{.000}$ & $.087\pms{.000}$ & $.088\pms{.000}$\\
& Random Forest & $.133\pms{.000}$ & \cellcolor[gray]{0.9}$\mathbf{.110\pms{.000}}$ & $.055\pms{.000}$ & $.058\pms{.000}$ & $.104\pms{.000}$ & $.102\pms{.000}$ & \cellcolor[gray]{0.9}$\mathbf{.085\pms{.000}}$ & \cellcolor[gray]{0.9}$\mathbf{.086\pms{.000}}$\\
& MLP & $.133\pms{.001}$ & $.111\pms{.000}$ & $.050\pms{.000}$ & $.055\pms{.002}$ & $.103\pms{.000}$ & $.108\pms{.003}$ & $.087\pms{.000}$ & $.088\pms{.000}$\\
& Deep Ensemble & $.133\pms{.001}$ & \cellcolor[gray]{0.9}$\mathbf{.110\pms{.000}}$ & $.048\pms{.000}$ & $.052\pms{.001}$ & $.101\pms{.000}$ & $.100\pms{.000}$ & $.086\pms{.000}$ & $.086\pms{.000}$\\
\cmidrule(l){2-10}
& GIN & $.132\pms{.001}$ & $.112\pms{.001}$ & \multicolumn{2}{c|}{$.050\pms{.000}$} & \multicolumn{2}{c|}{$.103\pms{.000}$} & \multicolumn{2}{c}{$.090\pms{.001}$}\\
& GIN (attr masking) &\cellcolor[gray]{0.9} $\mathbf{.130\pms{.000}}$ & $.114\pms{.002}$ & \multicolumn{2}{c|}{$.049\pms{.000}$} & \multicolumn{2}{c|}{\cellcolor[gray]{0.9}$\mathbf{.100\pms{.000}}$} & \multicolumn{2}{c}{$.087\pms{.000}$}\\
& GIN (context pred) & $.134\pms{.000}$ & $.113\pms{.001}$ & \multicolumn{2}{c|}{$.050\pms{.000}$} & \multicolumn{2}{c|}{$.101\pms{.000}$} & \multicolumn{2}{c}{$.087\pms{.000}$}\\
& Grover & $.134\pms{.001}$ & $.111\pms{.001}$ & \multicolumn{2}{c|}{$.049\pms{.000}$} & \multicolumn{2}{c|}{$.101\pms{.000}$} & \multicolumn{2}{c}{$.088\pms{.000}$}\\
\cmidrule(l){2-10}
& \textbf{Q-SAVI} & \cellcolor[gray]{0.9}\cellcolor[gray]{0.9}$\mathbf{.130\pms{.000}}$ & $.112\pms{.003}$ & \cellcolor[gray]{0.9}$\mathbf{.047\pms{.000}}$ & \cellcolor[gray]{0.9}$\mathbf{.048\pms{.000}}$ & $.102\pms{.000}$ & \cellcolor[gray]{0.9}$\mathbf{.099\pms{.000}}$ & $.088\pms{.000}$ & $.090\pms{.000}$\\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace*{-10pt}
\end{table*}

\paragraph{Training and evaluation.}

To facilitate a fair comparison, we carried out an extensive hyperparameter search for every model, data split, and featurization. 
After an initial division of the data into training and test sets, the same data-splitting technique was applied again to derive a representative validation set.
The hyperparameter setting with the lowest negative log-likelihood on that validation set was then used to train ten independent models using different random seeds. Full implementation details and hyperparameter ranges are provided in~\Cref{appsec:training_details}.

Following model training and hyperparameter selection, the predictive accuracy and calibration of the estimated test-set label probabilities were characterized by the area under the \textsc{ROC} curve (\textsc{AUC-ROC}) and the \textsc{Brier Score}, as these enable the direct comparison of models across test sets with different label distributions (see~\Cref{tab:results}). Additionally, each algorithm's performance was characterized by the area under the precision-recall curve (\textsc{AUC-PRC}) and the adaptive calibration error (\textsc{ACE}; \citet{nixon2019measuring}), which closely mirror the \textsc{AUC-ROC} and \textsc{Brier Score} (see~\Cref{tab:results_auprc}). 

%
\paragraph{Results.}
The predictive accuracy and calibration metrics presented in~\Cref{tab:results,tab:results_auprc} (see~\Cref{app:sec_auprcs}) demonstrate that \textsc{q-savi} achieves significant performance gains in an out-of-distribution setting.
On the spectral and molecular weight splits---the evaluation settings with the strongest covariate and label shift---\textsc{q-savi} outperformed all other algorithms by a substantial and statistically significant margin in terms of predictive accuracy.
Similarly, its predictive uncertainty estimates were significantly better calibrated than all other algorithms on the molecular weight split and most other algorithms on the ECFP-based spectral split. 

On the substantially less data-shifted scaffold and random splits, relatively simple machine learning algorithms (e.g., random forests and deep ensembles) as well as more sophisticated self-supervised pre-training-based approaches consistently achieved the best predictive performance. 

In line with the empirical observations of \citet{ji2022drugood}, IRM, GroupDRO, DANN, and DeepCoral---domain adaptation and generalization techniques originally developed for images---were found to perform worse than most other techniques across most splits and featurizations (see~\Cref{tab:results_auprc}).

%

%
\subsection{Merck Molecular Activity Challenge}
\label{sec:merck-datasets}

As a complementary assessment of the practical utility of \textsc{q-savi}, we evaluated the method on the Merck Molecular Activity Challenge \citep{ma2015deep}. Consisting of 15 datasets from real-world production settings, it provides time-split training and test sets that represent the data shift encountered throughout a molecular optimization campaign \citep{sheridan2013time}. As the compound structures are only provided in the form of anonymized atom-pair descriptors in count and bit vector form, using a uniform subsample of a large chemical database as a context point distribution is not possible.

\setlength{\tabcolsep}{8.6pt}
\begin{table}[H]
    \vspace*{-20pt}
    \centering
    \caption{
    Covariate and label shift of time-split data from the Merck Molecular Activity Challenge.
    Covariate shift is quantified as the \mbox{(multi-)set} Jaccard kernel-based {\textsc{MMD}} statistic, while label shift is quantified as the two-sample Kolmogorovâ€“Smirnov test statistic.}
    \label{tab:merck_cov_shift}
    \vspace{1pt}
    \small
    \begin{tabular}{lccc}
        \toprule
        \multirow{2}{*}{Dataset}& \multirow{2}{*}{Label Shift} & \multicolumn{2}{c}{Covariate Shift} \\
        & & Count Vector & Bit Vector\\
        \midrule
        \textsc{HIVPROT} & $0.579$ & $0.132$ & $0.162$ \\
        \textsc{DPP4} & $0.375$ & $0.112$ & $0.125$ \\
        \textsc{NK1} & $0.419$ & $0.071$ & $0.062$ \\
        \bottomrule
    \end{tabular}
    %
\end{table}


\setlength{\tabcolsep}{12pt}
\begin{table*}[t]
\small
    \centering
    \caption{
    A summary of the test set performance of each model for each of the datasets from the Merck Molecular Activity Challenge, quantified by the mean squared error ($\downarrow$). All entries indicate the mean and standard error computed over $10$ independent training runs with different random seeds.
    The best models within a margin of statistical significance are highlighted in bold.}
    \label{tab:merck_results}
    \vspace{1pt}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|cc|cc|cc}
    \toprule
    \small
    \multirow{2}{*}{Model} & \multicolumn{2}{c|}{HIVPROT} & \multicolumn{2}{c|}{\textsc{DPP4}} & \multicolumn{2}{c}{\textsc{NK1}}\\
    & count vector & bit vector & count vector & bit vector & count vector & bit vector \\
    \midrule
    $L_1$-Regression & $1.137\pms{.000}$ & $0.714\pms{.000}$ & $1.611\pms{.000}$ & $1.130\pms{.000}$ & $0.482\pms{.000}$ & $0.442\pms{.000}$ \\
    $L_2$-Regression & $0.999\pms{.000}$ & $0.723\pms{.000}$ & $1.495\pms{.000}$ & $1.143\pms{.000}$ & $0.498\pms{.000}$ & $0.436\pms{.000}$ \\
    Random Forest & $0.815\pms{.009}$ & $0.834\pms{.010}$ & $1.473\pms{.008}$ & $1.461\pms{.012}$ & $0.458\pms{.002}$ & $0.438\pms{.002}$ \\
    MLP & $0.768\pms{.014}$ & $2.118\pms{.015}$ & $1.393\pms{.024}$ & $1.094\pms{.029}$ & $0.443\pms{.007}$ & $0.399\pms{.006}$ \\
    \midrule
    \textbf{Q-SAVI} & \cellcolor[gray]{0.9}$\mathbf{0.682\pms{.019}}$ & \cellcolor[gray]{0.9}$\mathbf{0.664\pms{.028}}$ &\cellcolor[gray]{0.9} $\mathbf{1.332\pms{.017}}$ &\cellcolor[gray]{0.9} $\mathbf{1.028\pms{.027}}$ & \cellcolor[gray]{0.9}$\mathbf{0.436\pms{.007}}$ &\cellcolor[gray]{0.9} $\mathbf{0.387\pms{.012}}$ \\
    
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    %
\end{table*}


Instead, our evaluation focused on the three most covariate- and label-shifted datasets (see~\Cref{tab:merck_cov_shift}), repurposing the remaining data as an anonymized context point distribution.
All methods were evaluated following the protocol outlined in~\Cref{sec:experiments}, with full details presented in~\Cref{sec:app_merck}.
The performance metrics for our method and the baseline algorithms investigated in \citet{ma2015deep} are presented in~\Cref{tab:merck_results}, demonstrating that \textsc{q-savi} performs favorably across every setting and outperforms all other models on the strongly data-shifted \textsc{HIVPROT}, \textsc{DPP4}, and \textsc{NK1} datasets by a substantial and statistically significant margin.