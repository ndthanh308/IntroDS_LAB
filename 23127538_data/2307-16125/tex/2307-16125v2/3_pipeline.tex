
\section{SEED-Bench}
Our benchmark contains 19K multiple-choice questions with accurate human annotations spanning 12 evaluation dimensions including both the spatial and temporal understanding. In this section, we first present the evaluation dimensions of SEED-Bench in Sec.~\ref{sec:pipeline_multi_task}. We introduce the data source in Sec.~\ref{sec:source} and our pipeline for constructing multiple-choice questions in Sec.~\ref{sec:mcq}. We finally describe the evaluation strategy for MLLMs to answer multiple-choice questions in Sec.~\ref{sec:strategy}.



\begin{table}[]
    \centering
    \caption{Evaluation dimensions of SEED-Bench including both the spatial and temporal understanding. We omit the image in the sample questions.}\label{tab:multilevel}
    \vspace{6pt}
    % \vspace{-0.6em}
    {\small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cll}
         \toprule
         & Evaluation Dimensions & Sample Questions\\ 
         \midrule
         \multirow{25}{*}{Spatial Understanding} & 1. Scene Understanding & \makecell[l]{What is the weather like in the image? \\
          A. It's a sunny day 
          B. It's foggy  \\
          C. It's raining heavily 
          D. It's a cloudy day}
         \\
         \cmidrule(lr){2-3} 
         & 2. Instance Identity & \makecell[l]{What kind of animal is visible in the image? \\
         A. Horse 
         B. Cow 
         C. Sheep 
         D. Goat}\\
         \cmidrule(lr){2-3}
         & 3. Instance Attribute & \makecell[l]{What is the material of the table? \\
         A. Marble 
         B. Wood 
         C. Glass 
         D. Plastic} \\
         \cmidrule(lr){2-3} 
         & 4. Instance Location & \makecell[l]{Where is the dog located in the living room? \\
         A. On the fireplace 
         B. On the table 
         C. On the chair 
         D. On the rug }\\ 
         \cmidrule(lr){2-3} 
         &5. Instance Counting & \makecell[l]{How many people are there in the image? \\
         A. 1 
         B. 2 
         C. 4 
         D. 3}\\
        \cmidrule(lr){2-3}
         &6. Spatial Relation  & \makecell[l]{What is the tree in relateion to the house? \\
            A. In front of the house 
            B. Behind the house \\
            C. Inside the house 
            D. Left to the house 
         }\\
         \cmidrule(lr){2-3}
         &7. Instance Interaction            & \makecell[l]{
         What is the relation between a player and a referee? \\
            A. The player is shaking hands with a referee \\
            B. The player is arguing with a referee \\
            C. The player is receiving an award from a referee \\
            D. The player is shown a card by a referee 
         } \\
         \cmidrule(lr){2-3} 
         &8. Visual Reasoning & \makecell[l]{what can we infer about the situation? \\
                    A. They are admiring the engine 
                    B. They are experiencing car trouble \\
                    C. They are having a picnic 
                    D. They are washing the car }\\
        \cmidrule(lr){2-3} 
        &9. Text Recognition &  \makecell[l]{
         What is the main warning on the sign? \\
            A. Do not enter 
            B. Dead end road \\
            C. Beware of bears 
            D. Trail closed}\\
        \midrule
        \multirow{10}{*}{Temporal Understanding} &10. Action Recognition & \makecell[l]{What is the action being carried out in the video? \\
         A. Throwing something in the air and letting it fall \\
         B. Throwing something in the air and catching it \\
         C. Lifting up one end of something, then letting it drop down \\
         D. Poking something so that it falls over}\\
        \cmidrule(lr){2-3}
        &11. Action Prediction  & \makecell[l]{What action do you anticipate following the end of this video? \\
            A. Stir potatoes
            B. Wash potatoes 
            C. Add potatoes 
            D. Slice potatoes
         }\\
         \cmidrule(lr){2-3}
         &12. Procedure Understanding            & \makecell[l]{
         Can you recognize the actions in this video and list them in order? \\
            A. Cook breakfast, switch stove on, close fridge, carry milk, peel banana \\
            B. Scoop ice cream, squeeze chocolate syrup, pour sprinkles, close fridge \\
            C. Close fridge, carry milk, screw open milk cap, pour milk, screw close milk cap \\
            D. Reach for cereal box, grab bowl, pour milk, stir cereal, close fridge \\
         } \\
         \bottomrule
    \end{tabular}
    }
   }
\end{table}

\subsection{Evaluation Dimensions}
\label{sec:pipeline_multi_task}

In order to comprehensively assess the visual understanding capability of MLLMs, SEED-Bench incorporates 12 evaluation dimensions including both the spatial and temporal comprehension as shown in Table~\ref{tab:multilevel}. 

\noindent\textbf{Spatial Understanding.} For the evaluation of spatial comprehension, we consider 9 dimensions covering image-level and instance-level perception and reasoning.
\begin{itemize}
    \item Scene Understanding. This dimension focuses on the global information in the image. Questions can be answered through a holistic understanding of the image.
    \item Instance Identity. This dimension involves the identification of a certain instance in the image, including the existence or category of a certain object in the image. It evaluates a model's object recognition capability.  
    \item Instance Attributes. This dimension is related to the attributes of an instance, such as color, shape or material. It assesses a model's understanding of an object's visual appearance. 
    \item Instance Location. This dimension concerns the absolute position of one specified instance. It requires a model to correctly localize the object referred to in the question.
    \item Instances Counting. This dimension requires the model to count the number of a specific object in the image. This requires the model to understand all objects, and successfully count the referred object's instances.
    \item Spatial Relation. This dimension asks an model to ground the two mentioned objects, and recognize their relative spatial relation within the image.
    \item Instance Interaction. This dimension requires the model to recognize the state relation or interaction relations between two humans or objects.
    \item Visual Reasoning. This dimension evaluates if a model is able to reason based on the visual information. This requires the model to fully understand the image and utilize its commonsense knowledge to correctly answer the questions.
    \item Text Understanding. For this dimension, the model should answer question about the textual elements in the image.
    \end{itemize}
\noindent\textbf{Temporal Understanding.} For the evaluation of temporal comprehension, we consider 3 dimensions focusing on the recognition, prediction and procedure understanding of actions.
\begin{itemize}
    \item Action Recognition. In this dimension, the model is required to recognize the action shown in the videos. Not only the ability of capture temporal dynamics, but also the knowledge of physical motions, human actions and dynamic interaction between objects is evaluated. 
    \item Action Prediction. The target of this dimension is to predict the future action through the preceding video segment, which requires the understanding of contextual information from videos and temporal reasoning. 
    \item Procedure Understanding. This dimension requires the model to capture all the key actions and perform temporal ordering on them. We aims to evaluate the ability of temporally fine-grained understanding and procedure reasoning. 
\end{itemize}

% Figure environment removed


\subsection{Data Source}
\label{sec:source}
To create a benchmark with various evaluation dimensions, we need to collect data containing images with abundant visual information and videos with rich temporal dynamics, so that we can construct diverse challenging multiple-choice questions. In SEED-Bench, we use CC3M~\cite{sharma2018conceptual_gcc} dataset with filtered samples to build questions for spatial understanding. Specifically, considering the noisy original captions of CC3M, we generate captions for each image with Tag2Text~\cite{huang2023tag2text}. We filter out those images with no more than 5 nouns in their captions, so as to ensure the information richness in the remaining images for constructing questions.

%
We further adopt Something-Something-v2 (SSV2)~\cite{ssv2}, Epic-kitchen 100~\cite{epickitchen100} and Breakfast~\cite{breakfast} dataset to build questions for temporal understanding. SSV2 is an action recognition dataset including 174 fine-grained categories of basic actions with everyday objects and we adopt 1740 videos from its validation set. We also select 138 long videos from Epic-kitchen 100 dataset with temporally annotated action labels. Moreover, videos and fine-grained action segmentation annotations in Breakfast dataset~\cite{breakfast} are utilized for the procedure understanding task. 


\subsection{Multiple-Choice Questions} 
\label{sec:mcq}
As shown in Fig.~\ref{fig_pipeline}, our pipeline for generating multiple-choice questions involves question/answer generation and verification. For generating question/answer pairs, we first leverage various foundation models to extract visual information including image-level captions, instance-level descriptions and textual elements. Based on specially designed prompts corresponding to specific evaluation dimension, ChatGPT/GPT-4 subsequently generates questions and four candidate options with one groundtruth answer. For verifying question/answer pairs, we filter out questions that can be answered correctly by multiple LLMs without resorting to visual information. We further employ human annotators to select the correct option and classify each question into one evaluation dimension. 

\noindent\textbf{Visual Information Extraction.}
For constructing questions related to spatial understanding, we interpret the rich information in each image with texts using multiple pretrained models, so that ChatGPT/GPT-4 can understand the image and create questions accordingly. For constructing questions related to temporal understanding, considering that extracting reliable temporal information from videos (especially fine-grained actions and long-term temporal context) is extremely difficult given existing foundation models, we utilize the ground-truth annotations of video datasets. We will explore how to generate questions based on automatically extracted video information in the future. The extraction of visual information for images includes the following parts:

\begin{itemize}
\item \noindent\textbf{Image Captions.} 
Image captions contain the overall description of an image. 
We employ BLIP2~\cite{li2022blip} and Tag2Text~\cite{huang2023tag2text} to create captions for each image.
The former creates captions for the whole image while the latter generates captions based on descriptions of each instance.
The two models complement each other to depict the image content within a single sentence.

\item \noindent\textbf{Instance Descriptions.}
Besides captions which may ignore specific details in the image, we also extract visual information from images using instance-level descriptions, including object detection, attribute detection, and dense captions.
Specifically, we use SAM~\cite{kirillov2023sam} to segment each instance in the image and obtain their bounding boxes according to the segmentation results.
The object labels are obtained using Tag2Text~\cite{huang2023tag2text}.
Besides, we also utilize attribute detector~\cite{zhang2021vinvl} to obtain the attributes of each instance in the image.
Finally, we employ GRiT~\cite{wu2022grit} to generate dense captions, which describe each detected instance in the image with a short sentence.
These instance-level descriptions are complementary to the image captions, further enriching the visual information of each image. 

\item \noindent\textbf{Textual Elements.}
Besides objects, the texts in the image also contain important information describing the image. 
We employ PaddleOCR~\cite{paddleocr} for detecting textual elements.
\end{itemize}

\noindent\textbf{Question-Answer Generation.} After extracting visual information from the image and video, we task ChatGPT/GPT-4 with generating multiple-choice questions based on the extracted information or video annotations.  
For each of the spatial understanding evaluation, we carefully design prompts and ask ChatGPT/GPT-4 to create multiple choice questions with four candidate options based on the extracted visual information.
We create questions with ChatGPT for all evaluation dimensions, except for the reasoning dimension, where we use GPT-4~\cite{openai2023gpt4} due to its exceptional reasoning capability. 
For each question, we ask ChatGPT/GPT-4 to create four choices with one correct option and three distractors. We try to make the multiple-choice questions challenging by encouraging the three wrong choices to be similar to the correct one. The detailed prompts of generating multiple-choice questions for different evaluation dimensions are listed in Fig.~\ref{fig:prompt}.
For generating questions related to temporal understanding, we utilize the ground-truth annotations of selected videos as the answer of multi-choice questions and employ ChatGPT to generate three distractors.


% Figure environment removed



\noindent\textbf{Automatic Filtering.}
Our benchmark aims at evaluating the multimodal vision-language understanding capability of MLLMs. 
However, we observe that some generated questions can be correctly answered by LLMs without seeing the image.
We argue that such questions are not helpful to evaluate the visual comprehension capability of MLLMs.
To this end, we feed the generated questions (without image) into three powerful LLMs, including Vicuna-7B~\cite{vicuna}, Flan-T5-XXL~\cite{chung2022scaling_flant5} and LLaMA-7B~\cite{touvron2023llama} and ask them to answer the questions.
We empirically found that $5.52\%$ of the generated questions can be correctly answered by all of the three LLMs.
We filter out these questions from our benchmark.

\noindent\textbf{Human Annotation.}
To ensure the accuracy and objectiveness of SEED-Bench, we further employ human annotators to verify the generated question/answer pairs.  Human annotators are asked to choose the correct answer for each multiple-choice question and categorize each question into one of the evaluation dimension. If one question can not be answered based on the visual input or does not have any correct choice or has multiple correct choices, it will be discarded by human annotators. This results in a clean, high-quality and well-categorized benchmark for evaluation with a total of 19K multiple-choice questions. The statistics of the number of multiple-choice questions in each evaluation dimension is shown in Fig.~\ref{fig:seed_bench_overview}. We can observe a minimum number of questions in text recognition with 85 samples, and a maximum number in instance localization with 4649 samples. We will maintain an even distribution among multiple-choice questions associated with different evaluation dimensions in the future. 


% \subsection{Evaluating Multi-Choice Questions}
\subsection{Evaluation Strategy}
\label{sec:strategy}
Different from MMBench~\cite{liu2023mmbench} that employs ChatGPT to match a modelâ€™s prediction to one of the choices in a multiple-choice question (achieves only 87.0\% alignment rate), we adopt the answer ranking strategy~\cite{dai2023instructblip, brown2020gpt3, lin2021truthfulqa} for evaluating existing MLLMs with multiple-choice questions. Specifically, for each choice of a question, we compute the likelihood that an MLLM generates the content of this choice given the question. We select the choice with the highest likelihood as model's prediction. Our evaluation strategy does not rely on the instruction-following capabilities of models to output ``A'' or ``B'' or ``C'' or ``D''. Furthermore, this evaluation strategy eliminates the impact of the order of multiple-choice options on the model's performance.



