\begin{thebibliography}{10}

\bibitem{chung2022scaling_flant5}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{ChatGPT}
OpenAI.
\newblock Introducing chatgpt.
\newblock https://openai.com/blog/chatgpt, 2022.

\bibitem{vicuna}
FastChat.
\newblock Vicuna.
\newblock https://github.com/lm-sys/FastChat, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em ICML}, 2023.

\bibitem{zhu2023minigpt4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced
  large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\bibitem{liu2023visual_llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2304.08485}, 2023.

\bibitem{ye2023mplugowl}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
  Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et~al.
\newblock mplug-owl: Modularization empowers large language models with
  multimodality.
\newblock {\em arXiv preprint arXiv:2304.14178}, 2023.

\bibitem{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao,
  Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with
  instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.06500}, 2023.

\bibitem{li2023otter}
Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
\newblock Otter: A multi-modal model with in-context instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.03726}, 2023.

\bibitem{gong2023multimodalgpt}
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,
  Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.
\newblock Multimodal-gpt: A vision and language model for dialogue with humans,
  2023.

\bibitem{su2023pandagpt}
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.
\newblock Pandagpt: One model to instruction-follow them all.
\newblock {\em arXiv preprint arXiv:2305.16355}, 2023.

\bibitem{peng2023kosmos}
Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and
  Furu Wei.
\newblock Kosmos-2: Grounding multimodal large language models to the world.
\newblock {\em arXiv preprint arXiv:2306.14824}, 2023.

\bibitem{li2023videochat}
KunChang Li, Yinan He, Yi~Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,
  Limin Wang, and Yu~Qiao.
\newblock Videochat: Chat-centric video understanding.
\newblock {\em arXiv preprint arXiv:2305.06355}, 2023.

\bibitem{maaz2023videochatgpt}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision
  and language models.
\newblock {\em arXiv preprint arXiv:2306.05424}, 2023.

\bibitem{luo2023valley}
Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao
  Wang, and Zhongyu Wei.
\newblock Valley: Video assistant with large language model enhanced ability.
\newblock {\em arXiv preprint arXiv:2306.07207}, 2023.

\bibitem{ge2023planting}
Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan.
\newblock Planting a seed of vision in large language model.
\newblock {\em arXiv preprint arXiv:2307.08041}, 2023.

\bibitem{sun2023generative}
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang,
  Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
\newblock Generative pretraining in multimodality.
\newblock {\em arXiv preprint arXiv:2307.05222}, 2023.

\bibitem{yu2023scaling}
Yu~Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin, Golovneva Olga, Wang Tianlu,
  Babu Arun, Tang Binh, Karrer Brian, Sheynin Shelly, Ross Candace, Polyak
  Adam, Howes Russ, Sharma Vasu, Xu~Jacob, Singer Uriel, Li~(AI) Daniel, Ghosh
  Gargi, Taigman Yaniv, Fazel-Zarandi Maryam, Celikyilmaz Asli, Zettlemoyer
  Luke, and Aghajanyan Armen.
\newblock Scaling autoregressive multi-modal models: Pretraining and
  instruction tuning.
\newblock 2023.

\bibitem{koh2023gill}
Jing~Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.
\newblock Generating images with multimodal language models.
\newblock {\em arXiv preprint arXiv:2305.17216}, 2023.

\bibitem{goyal2017making}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding
  in visual question answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6904--6913, 2017.

\bibitem{fu2023mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin,
  Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke~Li, Xing Sun, and Rongrong
  Ji.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large
  language models.
\newblock {\em arXiv preprint arXiv:2306.13394}, 2023.

\bibitem{yin2023lamm}
Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li,
  Lu~Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et~al.
\newblock Lamm: Language-assisted multi-modal instruction-tuning dataset,
  framework, and benchmark.
\newblock {\em arXiv preprint arXiv:2306.06687}, 2023.

\bibitem{xu2023lvlm}
Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng,
  Siyuan Huang, Yu~Qiao, and Ping Luo.
\newblock Lvlm-ehub: A comprehensive evaluation benchmark for large
  vision-language models.
\newblock {\em arXiv preprint arXiv:2306.09265}, 2023.

\bibitem{liu2023mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike
  Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock {\em arXiv preprint arXiv:2307.06281}, 2023.

\bibitem{huang2023tag2text}
Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang,
  Yaqian Li, Yandong Guo, and Lei Zhang.
\newblock Tag2text: Guiding vision-language model via image tagging.
\newblock {\em arXiv preprint arXiv:2303.05657}, 2023.

\bibitem{wu2022grit}
Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan,
  and Lijuan Wang.
\newblock Grit: A generative region-to-text transformer for object
  understanding.
\newblock {\em arXiv preprint arXiv:2212.00280}, 2022.

\bibitem{kirillov2023sam}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C. Berg, Wan-Yen Lo, Piotr
  Doll{\'a}r, and Ross Girshick.
\newblock Segment anything.
\newblock {\em arXiv:2304.02643}, 2023.

\bibitem{zhang2021vinvl}
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
  Yejin Choi, and Jianfeng Gao.
\newblock Vinvl: Revisiting visual representations in vision-language models.
\newblock In {\em CVPR}, 2021.

\bibitem{paddleocr}
https://github.com/PaddlePaddle/PaddleOCR.
\newblock Paddleocr.

\bibitem{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{wang2023gvt}
Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan.
\newblock What makes for good visual tokenizers for large language models?
\newblock {\em arXiv preprint arXiv:2305.12223}, 2023.

\bibitem{sharma2018conceptual_gcc}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In {\em ACL}, 2018.

\bibitem{ssv2}
Raghav Goyal, Samira Ebrahimi~Kahou, Vincent Michalski, Joanna Materzynska,
  Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos,
  Moritz Mueller-Freitag, et~al.
\newblock The" something something" video database for learning and evaluating
  visual common sense.
\newblock In {\em ICCV}, 2017.

\bibitem{epickitchen100}
Dima Damen, Hazel Doughty, Giovanni~Maria Farinella, Antonino Furnari,
  Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett,
  Will Price, et~al.
\newblock Rescaling egocentric vision.
\newblock {\em arXiv preprint arXiv:2006.13256}, 2020.

\bibitem{breakfast}
Hilde Kuehne, Ali Arslan, and Thomas Serre.
\newblock The language of actions: Recovering the syntax and semantics of
  goal-directed human activities.
\newblock In {\em CVPR}, 2014.

\bibitem{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em ICML}, 2022.

\bibitem{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock {\em arXiv preprint arXiv:2109.07958}, 2021.

\bibitem{2023vpgtrans}
Ao~Zhang, Hao Fei, Yuan Yao, Wei Ji, Li~Li, Zhiyuan Liu, and Tat-Seng Chua.
\newblock Transfer visual prompt generator across llms.
\newblock abs/23045.01278, 2023.

\bibitem{openflamingo}
ml\_foundations.
\newblock Openflamingo.
\newblock https://github.com/mlfoundations/open\_flamingo, 2023.

\bibitem{gao2023llamaadapterv2}
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei
  Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu~Qiao.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock {\em arXiv preprint arXiv:2304.15010}, 2023.

\end{thebibliography}
