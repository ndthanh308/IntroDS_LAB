\section{Related Work}\label{sec:related_work}
\noindent\textbf{Multimodal Large Language Models.}
With the impressive success of Large language models (LLM)~\cite{chung2022scaling_flant5,touvron2023llama,vicuna}, recent studies work on generative Multimodal Large Language Models (MLLMs)~\cite{li2023blip2, zhu2023minigpt4, liu2023visual_llava, ye2023mplugowl, dai2023instructblip, li2023otter, gong2023multimodalgpt, su2023pandagpt, peng2023kosmos, ge2023planting, sun2023generative,yu2023scaling, koh2023gill} to improve multimodal comprehension and generation through utilizing the strong generality of LLMs. Some work~\cite{li2023videochat,maaz2023videochatgpt,luo2023valley} further considers video inputs and leverage the vast capabilities of LLMs for video understanding tasks. In SEED-Bench, we provide a comprehensive quantitative evaluations of these models to thoroughly assess and compare their performance in generative comprehension.

\noindent\textbf{Benchmarks for Multimodal Large Language Models.}
%
With the rapid development of Multimodal Large Language Models (MLLMs), some concurrent works~\cite{fu2023mme,yin2023lamm,xu2023lvlm,liu2023mmbench} propose various benchmarks for evaluating MLLMs. For example, GVT~\cite{wang2023gvt} constructs a benchmark by aggregating two semantic-level understanding tasks (VQA and Image Captioning) and two fine-grained tasks (Object Counting and Multi-class Identification). But its evaluation is constrained to limited aspects of visual understanding. LVLM-eHub~\cite{xu2023lvlm} combines multiple existing computer vision benchmarks and develops an online platform, where two models are prompted to answer a question related to an image and human annotators are employed to compare the predictions of models. The involvement of human annotators during evaluation not only introduces bias but also incurs significant costs. LAMM~\cite{yin2023lamm} evaluates image and point cloud tasks by using entity extraction to obtain key answers from open-form predictions and utilizing GPT to evaluate the answersâ€™ relevance and accuracy to the groundtruth. The reliance on entity extraction and GPT metric can impact the accuracy and reliability of the evaluation. MME~\cite{fu2023mme} and MMBench~\cite{liu2023mmbench} aim to enhance the objective evaluation of MLLMs by constructing 2914 True/False Questions and 2974 Multiple Choice Questions across a variety of ability dimensions respectively. Considering the relatively small scale of these benchmarks, their evaluation results may exhibit instability. In this work, we introduce SEED-Bench to provide objective and comprehension evaluation of MLLMs, which contains 19K multiple-choice questions covering 12 evaluation dimensions including both spatial and temporal understanding.  


