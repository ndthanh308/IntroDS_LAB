\section{Evaluation Results}
\begin{table}[]
    \centering
    \caption{Evaluation results of different models on SEED-Bench, where ``Spatial'' shows the averaged performance on nine dimensions for evaluating spatial understanding, and ``Temporal'' shows the averaged performance on three dimensions for evaluating temporal understanding.}\label{tab:performance}
    \vspace{3pt}
    % \vspace{-0.6em}
    {\small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccccc}
         \toprule
         \multirow{2}{*}{Model Type} & \multirow{2}{*}{Model} & \multirow{2}{*}{Language Model}& \multicolumn{2}{c}{Spatial} & \multicolumn{2}{c}{Temporal} & \multicolumn{2}{c}{Overall} \\
         \cmidrule(lr){4-9}
         \cmidrule(lr){4-5}
         \cmidrule(lr){6-7}
         \cmidrule(lr){8-9}
         &&& Acc & Rank & Acc & Rank & Acc & Rank \\
         \midrule
         \multirow{3}{*}{LLM} & Flan-T5~\cite{chung2022scaling_flant5} &Flan-T5-XL  &27.32 &17 &28.56 &11 & 27.65 &17 \\
         & Vicuna~\cite{vicuna} &Vicuna-7B &28.16 &16 &29.46 & 8 & 28.50 & 16\\
         & LLaMA~\cite{touvron2023llama} &LLaMA-7B &26.56 &18 &27.27 & 13 &26.75 & 18\\
         \midrule
         \multirow{12}{*}{ImageLLM} & BLIP-2~\cite{li2023blip2} &Flan-T5-XL &49.74 &3 &36.71 & 3 &46.35 &3 \\
         & InstructBLIP~\cite{dai2023instructblip} &Flan-T5-XL &57.80 &2 &\bf38.31 & 1 &52.73 &2\\
         & InstructBLIP Vicuna~\cite{dai2023instructblip} &Vicuna-7B &\bf58.76 &1 &38.05 &2 &\bf53.37 &1\\
         & LLaVA~\cite{liu2023visual_llava} &LLaMA-7B &36.96 &8 &23.75 &16 &33.52 &9\\
         & MiniGPT-4~\cite{zhu2023minigpt4} &Flan-T5-XL &47.40 &4 &29.89 &7 &42.84 &4\\
         & VPGTrans~\cite{2023vpgtrans} &LLaMA-7B &41.81 &5 &31.40 &5 &39.10 &5\\
         & MultiModal-GPT~\cite{gong2023multimodalgpt} &LLaMA-7B &34.54 &12 &29.21 &10 &33.15 &11\\
         & Otter~\cite{li2023otter} &LLaMA-7B &35.16 &11 &30.35 &6 &33.91 &8\\
         & OpenFlamingo~\cite{openflamingo} &LLaMA-7B &34.51 &13 &29.25 &9 &33.14 &12\\
         & LLaMA-Adapter V2~\cite{gao2023llamaadapterv2} &LLaMA-7B &35.19 &10 &25.75 &14 &32.73 &13\\
         & GVT~\cite{wang2023gvt} &Vicuna-7B &35.49 &9 &27.77 &12 &33.48 &10\\
         & mPLUG-Owl~\cite{ye2023mplugowl} &LLaMA-7B &37.88 &7 &23.02 &18 &34.01 &7\\
         \midrule
         \multirow{3}{*}{VideoLLM} & VideoChat~\cite{li2023videochat} &Vicuna-7B &39.02 &6 &33.68 &4 &37.63 &6\\
         & Video-ChatGPT~\cite{maaz2023videochatgpt} &LLaMA-7B &33.88 &14 &23.46 &17 &31.17 &14\\
         & Valley~\cite{luo2023valley} &LLaMA-13B &32.04 &15 &25.41 &15 &30.32 &15\\
         \bottomrule
    \end{tabular}
    }
   }
\end{table}

\subsection{Models}
Based on our SEED-Bench, we evaluate 18 models including 3 LLMs, \textit{i.e.}, Flan-T5~\cite{chung2022scaling_flant5}, Vicuna~\cite{vicuna}, LLaMA~\cite{touvron2023llama}, 12 ImageLLMs, \textit{i.e.}, OpenFlamingo~\cite{openflamingo}, BLIP-2~\cite{li2023blip2}, MiniGPT-4~\cite{zhu2023minigpt4}, LLaVa~\cite{liu2023visual_llava}, mPLUG-Owl~\cite{ye2023mplugowl}, InstructBLIP~\cite{dai2023instructblip}, Otter~\cite{li2023otter}, MultimodalGPT~\cite{gong2023multimodalgpt}, GVT~\cite{wang2023gvt}, PandaGPT~\cite{su2023pandagpt}, VPGTrans~\cite{2023vpgtrans}, LLaMA-Adapter V2~\cite{gao2023llamaadapterv2}, and 3 VideoLLMs, \textit{i.e.}, VideoChat~\cite{li2023videochat}, Video-ChatGPT~\cite{maaz2023videochatgpt} and Valley~\cite{luo2023valley}. Each model is evaluated with all the 12 dimensions including both the spatial and temporal understanding. For ImageLLMs, besides the evaluation of spatial understanding, we aim to investigate their capability to perform temporal reasoning among multiple frames. For VideoLLMs, we seek to explore whether their spatial understanding abilities have degraded by taking a single image as the input. 
 



% Figure environment removed

\subsection{Results}
The evaluation results of different models on SEED-Bench are listed in Table.~\ref{tab:benchmark_compare}, where the accuracy refers to the proportion of correctly answered multiple-choice questions relative to the total number of questions. We are surprised to observe that InstructBLIP~\cite{dai2023instructblip} not only achieves the best performance based on the averaged results across nine dimensions for evaluating spatial understanding, but also surpasses VideoLLMs in terms of the averaged results across three dimensions for evaluating temporal understanding. We display leaderboards of various evaluation dimensions on SEED-Bench in Fig.~\ref{fig:each_task} to provide a comprehensive assessment of different models. The overall leaderboard based on the averaged results across all the evaluation dimensions are shown in Fig.~\ref{fig:seed_bench_overview}. To better showcase the the capabilities of models across different evaluation dimensions, we further visualize the ranking of each model within each evaluation dimension in Fig.~\ref{fig:rank}, where darker colors represent higher ranks. We can observe that the BLIP series~\cite{li2023blip2, dai2023instructblip} model achieves competitive results in multiple evaluation dimensions, but they are not good at visual reasoning and action recognition. VideoLLM Valley~\cite{luo2023valley} achieves suboptimal performance in the majority of evaluation dimensions. LLaVa~\cite{liu2023visual_llava} exhibits unparalleled capabilities in the evaluation of text recognition compared to other evaluation dimensions. In terms of specific evaluation dimension, MiniGPT-4~\cite{zhu2023minigpt4} model and mPLUG-Owl~\cite{ye2023mplugowl} model performs better in visual reasoning, while VPGTrans~\cite{2023vpgtrans} model excels in action recognition and procedure understanding. LLaMA Adapter V2~\cite{gao2023llamaadapterv2} model shows more proficiency in action recognition. What's more, Multimodal GPT~\cite{gong2023multimodalgpt}, Otter~\cite{li2023otter}, Openflamingo~\cite{openflamingo}, GVT~\cite{wang2023gvt}, and the three VideoLLMs~\cite{li2023videochat,maaz2023videochatgpt,luo2023valley} exhibit balanced strength across various evaluation dimensions.

\subsection{Analysis}
Through the comprehension and objective evaluation of various models on SEED-Bench, we have observed  a number of findings that can bring insights for future work.


% Figure environment removed

\noindent\textbf{Most MLLMs still exhibit limited performance across all 12 evaluation dimensions.} As shown in Fig.~\ref{fig:seed_bench_overview}, ~\ref{fig:each_task}, most MLLMs (except BLIP series models) can not reach 50\% accuracy on both average performance and the performance on more than three single evaluation dimension. In some specific evaluation dimension (\textit{e.g.}, visual reasoning), it seems that most MLLMs achieve high accuracy. However, when comparing the performance of MLLMs to LLMs, we observe that the performance improvement of most MLLMs is still relatively limited. 

\noindent\textbf{MLLMs achieve relatively high performance on global image comprehension} On the evaluation of scene understanding and visual reasoning, the accuracy of most MLLMs is higher than 40\%, and all MLLMs outperforms LLMs. This shows that MLLMs are more proficient in global understanding and reasoning of images, compared with other evaluation dimensions that require fine-grained instance-level comprehension.
%is because the instruction-tuning datasets of most models consist of image captions that cover the scene descriptions, and visual reasoning QA data.

\noindent\textbf{InstructBLIP achieves top performance on 8 of 12 evaluation dimensions. } We can observe that InstructBLIP outperforms other models on 8 evaluation dimensions and the possible explanations for this superior performance are as follows. (a) The instruction-tuning data of InstructBLIP contains totally 16M samples (larger than other instruction-tuning datasets), and covers a wide range of multimodal tasks, even including QA data of OCR and temporal visual reasoning. (b) The weights of LLMs are frozen when performing instruction-tuning of InstructBLIP, which may alleviate catastrophic forgetting. However, InstructBLIP series models still perform poorly on action recognition and procedure understanding that differ significantly from the instruction-tuning data. For instance, on action recognition that requires the understanding of fine-grained actions in Something-Something-v2, InstructBLIP series models can not achieve significant performance gain compared to LLMs (\textit{i.e.}, lower than 2\%). This indicates that InstructBLIP series models may fail to generalize well on the out-of-distribution data. 

\noindent\textbf{MLLMs show weaker abilities in understanding spatial relationships between objects.} The top-ranked model InstructBLIP only achieves 40\% accuracy on the evaluation of spatial relations, which shows that recognizing relative spatial relationships between instances is challenging because there can be many possible arrangements and combinations of spatial relationships between instances. Additionally, spatial relationships between objects may cause ambiguity in some cases, making it difficult to determine their relationship. 


\noindent\textbf{Most MLLMs show poor performance for text recognition.} Apart from InstructBLIP, all other models achieve an accuracy lower than 40\% for text recognition due to the lack of textual elements in multimodal pre-training datasets. Since the ability to accurately identify and extract text from images is important, future work should develop models that are better equipped to handle text recognition by pre-training on datasets with rich textual elements in visual data.

\noindent\textbf{VideoLLMs achieve promising results on spatial understanding.} For example, VideoChat achieves 39.98\% accuracy (ranking 4-th on instance localization, surpassing LLaVa by 11.55\% and performing only 3.58\% lower than the top-1 model. It shows that VideoChat's ability of spatial understanding does not degrade by jointly training on both image and video data during the pre-training and instruction-tuning stages.

\noindent\textbf{Most MLLMs exhibit unsatisfactory performance on fine-grained temporal understanding.} It is notable that on the evaluation of procedure understanding, the top-ranked model, VPGTrans, achieves an accuracy that is only 5\% higher than that of LLaMA. The performance improvement of the following 4 MLLMs is even less than 1.2\% compared with LLaMA. This demonstrates that it is extremely difficult for both the ImageLLMs and VideoLLMs to perform fine-grained temporal reasoning so that they can recognize and sort the key actions in a video.

\noindent\textbf{VideoLLMs fail to achieve competitive performance on temporal understanding.}  Although VideoLLMs are instruction-tuned on video data, they do not exhibit a significant advantage on evaluation dimensions for temporal understanding.  Surprisingly, two VideoLLMS (Video-ChatGPT and Valley) even perform worse than most ImageLLMs on action recognition, action prediction and procedure understanding. It indicates that the capabilities of existing VideoLLMs for fine-grained action recognition, temporal relationship understanding and temporal reasoning are still limited. Similar concerns about existing VideoLLMs are also presented in recent works~\cite{li2023videochat,maaz2023videochatgpt}. 


