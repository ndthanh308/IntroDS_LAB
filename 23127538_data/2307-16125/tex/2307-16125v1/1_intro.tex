\section{Introduction}

% Figure environment removed


In recent years, Large Language Models (LLMs)~\cite{chung2022scaling_flant5, openai2023gpt4, ChatGPT, vicuna, touvron2023llama} have exhibited remarkable capabilities to understand, reason, and generate texts across a variety of open-ended tasks. Leveraging the strong generality of LLMs, generative Multimodal Large Language Models (MLLMs)~\cite{li2023blip2, zhu2023minigpt4, liu2023visual_llava, ye2023mplugowl, dai2023instructblip, li2023otter, gong2023multimodalgpt, su2023pandagpt, peng2023kosmos, li2023videochat, maaz2023videochatgpt, luo2023valley, ge2023planting, sun2023generative,yu2023scaling, koh2023gill} have demonstrate enhanced abilities for 
multimodal comprehension and generation. However, current MLLMs mainly evaluate their performance with a limited number of qualitative examples, or by employing previous benchmarks that are not tailored for evaluating MLLMs with open-form output. For example, in VQAv2~\cite{goyal2017making}, an answer is considered correct only if the model's output exactly matches the groundtruth answer, which typically consists of just one or two words. The lack of a comprehensive and objective benchmark to evaluate MLLMs poses a significant challenge for comparing and investigating the performance of various models.

%
Concurrent works~\cite{fu2023mme,yin2023lamm,xu2023lvlm,liu2023mmbench} have made efforts to develop benchmarks for specifically evaluating MLLMs as shown in Table~\ref{tab:benchmark_compare}. For example, LVLM-eHub~\cite{xu2023lvlm} and LAMM~\cite{yin2023lamm} utilize exiting public datasets across various computer vision tasks as evaluation samples, and employ human annotators or GPT to assess the quality, relevance, and usefulness of model's predictions. However, the involvement of human and GPT during evaluation not only compromises efficiency, but also leads to increased subjectivity and reduced accuracy of the assessment. MME~\cite{fu2023mme} and MMBench~\cite{liu2023mmbench} further advance objective evaluation of MLLMs by constructing True/False Questions or Multiple-Choice Questions, which cover a variety of ability dimensions. Restricting the model's output to True/False or A/B/C/D options facilitates the convenient computation of accuracy, which serves as an objective metric for evaluation. However, the relatively small scale of these benchmarks (fewer than 3K samples) introduces instability in the evaluation statistics.


%
In this work, we focus on evaluating the generative comprehension capability of MLLMs as a preliminary step towards a comprehensive assessment of generative
models, by introducing a benchmark named SEED-Bench\footnote{In pursuit of Artificial General Intelligence (AGI), LLMs have witnessed substantial progress. We have made a bold assumption that the premise for the emergence of multimodal capabilities is to unify both comprehension and generation within an autoregressive generative model, where SEED~\cite{ge2023planting} takes a modest step. Besides the exploration of models, it is essential to have appropriate evaluations that motivate research directions. Therefore, we concurrently propose SEED-Bench to evaluate the comprehension ability of generative models.}. SEED-Bench spans 12 evaluation dimensions across both image and video modalities as shown in Fig.~\ref{fig:seed_bench_overview}. SEED-Bench consists of 19K multiple choice questions with groundtruth answers derived from human annotation ($\times$9 larger than MME and $\times$6 larger than MMBench) as shown in Fig.~\ref{fig:example}. We design a sophisticated pipeline for the generation of multiple-choice questions that are tailored to evaluate specific dimensions. We further incorporate automated filtering mechanism and manual verification process to ensure the quality of questions and the accuracy of groundtruth answers.   

%
Specifically, for images, we utilize various foundation models to extract their visual information including image-level captions~\cite{li2023blip2, huang2023tag2text}, instance-level descriptions~\cite{wu2022grit,kirillov2023sam,zhang2021vinvl} and textual  elements~\cite{paddleocr}. For videos, we leverage the original human annotations to provide visual information. We then feed the visual information to ChatGPT/GPT-4 with specially designed prompts corresponding to specific evaluation dimension. ChatGPT/GPT-4 subsequently generates questions as well as four candidate options with one groundtruth answer. We further filter out questions that can be answered without the visual input through utilizing multiple LLMs. Finally, we employ human annotators to choose the correct option of each multiple-choice question and classify each question into one evaluation dimension, resulting in a clean and high-quality benchmark containing 19K multiple-choice questions. Our pipeline supports the scalability of evaluation data across multiple domains, and we will continue to expand the benchmark with more evaluation dimensions.

Based on SEED-Bench, we comprehensively evaluate 18 models including LLMs, ImageLLMs and VideoLLMs across all 12 dimensions as shown in Fig.~\ref{fig:seed_bench_overview}. Different from MMBench~\cite{liu2023mmbench} that employs ChatGPT to match a modelâ€™s prediction to one of the choices in a multiple-choice question (achieves only 87.0\% alignment rate), we follow GPT-3~\cite{brown2020gpt3} to calculate log-likelihood for each candidate option and select the one with the highest value as the final prediction, without relying on the instruction-following capabilities of models to output ``A'' or ``B'' or ``C'' or ``D''. By analyzing the results across 12 dimensions, we conduct a comprehensive comparison of existing multimodal models in both spatial and temporal understanding capabilities. We observe that the majority of MLLMs still exhibit limited performance across all 12 evaluation dimensions, and surprisingly find that VideoLLMs fail to achieve competitive performance on temporal understanding compared with ImageLLMs. Through the evaluation results, we aim for SEED-Bench to provide insights for motivating future exploration of a more advanced MLLM. We will launch an evaluation platform and consistently maintain a leaderboard for assessing and comparing model performance.




% Figure environment removed


\begin{table}[]
    \centering
    \caption{Comparisons between existing benchmarks for Multimodal LLMs. ``H/G Evaluation'' denotes whether human or GPT is used for evaluation.}\label{tab:benchmark_compare}
    % \vspace{-0.6em}
    \vspace{3pt}
    {\small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccc}
         \toprule
             Benchmark & Visual Modality & Customized Question & \#Answer Annotation& Answer Type & H/G Evaluation & \#Models \\
         \midrule
         MME~\cite{fu2023mme} & Image   & \ding{51} & 2194 &Y/N & N/A & 10\\
         LAMM~\cite{yin2023lamm} & Image \& Point cloud  & \ding{55} & - & free-form & GPT & 4\\
         LVLM-eHub~\cite{xu2023lvlm} & Image   & \ding{55} & - & free-form & Human &8\\
         MMBench~\cite{liu2023mmbench} & Image   &\ding{51} &2974 & free-form& GPT &14\\
         Ours & Image \& Video  & \ding{51} & 19242 & A/B/C/D&  N/A & 18\\
         \bottomrule
    \end{tabular}
    }
   }
\end{table}
