\subsection{Summarizing the Optimal Model}

\label{sec:summarizing}

Given a dataset $S$ and all singleton values in $ST$, the summarizing algorithm aims to find the optimal code table $CT$.
However, due to the large search space, finding the optimal $CT$ is time-consuming.
Inspired by \textsc{DITTO} \cite{bertens2016keeping}, we propose a heuristic algorithm to efficiently find an approximate optimal code table $CT$.
Algorithm \ref{alg:summarizing} demonstrates the algorithm overview.

The algorithm first initializes $CT$ with $ST$ (line 1).
Then a set of candidate patterns $Cand$ is generated (line 2) based on $S$ and $CT$.
The core idea of the algorithm is to examine whether each pattern $p$ in $Cand$ can bring information compression (line 3\textasciitilde4) and update $CT$ with $p$ that can do so (line 5\textasciitilde7).
To ensure priority access to a pattern that can bring higher information compression, the algorithm traverses the patterns $p$ in $Cand$ in the \textbf{Candidate Order}: $\downarrow support(p~|~S)$, $\downarrow |p|$, and $\uparrow$ lexicographically.

After confirming the ability of pattern $p$ to compress description length, the algorithm updates $CT$ with $p$ with three steps (line 5\textasciitilde7).
(1) The code table $CT$ is pruned to remove patterns that have become redundant with the addition of the new pattern $p$.
(2) The algorithm generates several variations of pattern $p$ to extend the code table efficiently.
(3) The algorithm updates the set of candidates for $CT$ based on the new code table $CT$.

We introduce three main functions below: the function for generating candidate patterns (Sec. \ref{sec:candidates}); the function for pruning (Sec. \ref{sec:pruning}); and the function for variations (Sec. \ref{sec:variations}).
We further introduce our method for speeding up the algorithm and the motivation for employing it (Sec. \ref{sec:lsh})

\input{algorithms/summarizing.tex}

\subsubsection{Generating Candidates}

\label{sec:candidates}

\input{figures/candidate}

A candidate pattern can be generated by combining two patterns that exist in $CT$, allowing us to construct complex patterns with simple ones.
Two patterns can be combined at different alignments.
To give the simplest example of univariate patterns, pattern $\{a, b\}$ and pattern $\{c, d\}$ can construct four candidates, namely, $\{a, b, c, d\}$, $\{a, c, b, d\}$, $\{c, a, d, b\}$, and $\{c, d, a, b\}$.
The combinations $\{a, c, d, b\}$ and $\{c, a, b, d\}$ are invalid because the number of gaps in $a, b$ and $c, d$ should be at most 1.
When constructing multivariate patterns, more candidates may arise, because the patterns may be related to different attributes such that the two patterns can be overlapped.
Furthermore, we need to consider the feature of missing values.
For example, in Fig. \ref{fig:candidate}, we list all the candidate patterns ($cp_1$ to $cp_6$) generated based on multivariate pattern $p_1$ and $p_2$ over three attributes.
When aligning the two events $e_2$ of $p_1$ and $p_2$, a conflict exists on value $x$ and $y$.
We resolve this conflict by regarding it as a missing value of one of the original patterns so that there exist two candidate patterns $cp_4$ and $cp_5$.
However, considering the limitation on the number of missing values, we regard these two candidates as invalid in practice.
We use $construct(p_i, p_j)$ to represent the set of all valid candidate patterns generated based on patterns $p_i$ and $p_j$.
We traverse each pair of patterns in $CT$ and construct candidate patterns.

% \input{algorithms/candidate}

\subsubsection{Pruning Patterns}

\label{sec:pruning}

If we add a new pattern $p^*$ into $CT$, some patterns may become redundant.
For example, if pattern $p_1={a}$ is always followed by pattern $p_2={b}$, all sequences containing $p_1$ can be covered by a new pattern $p^*={a,~b}$.
To keep the code table simple, we remove these redundant patterns.
Algorithm \ref{alg:pruning} demonstrates the process.
We first find all the patterns that have a lower usage after adding $p*$ (line 1\textasciitilde2).
Note that even if a singleton pattern has a lower support, we preserve it in $CT$ so that $CT$ always contains all singleton patterns.
Then, we traverse all the patterns to be pruned in \textbf{Prune Order}, where a pattern $p$ with a higher usage change $\Delta support(p)=usage(p~|~CT) - usage(p~|~CT_p)$ will be considered first.
If removing pattern $p$ can bring a smaller description length, we delete it from $CT$ (line 4\textasciitilde5).

\input{algorithms/pruning}

\subsubsection{Variations}

\label{sec:variations}

Constructing a large pattern is time-consuming, because we usually need to combine two patterns multiple times and examine all possible combinations.
To tackle this challenge, we follow the extremely effective solution used by \textsc{Ditto}: \textit{variations}.
The core idea of this algorithm is to quickly extend a pattern $p$ with the gap events that occur when using $p$ to cover the dataset.
More details can be found in Sec. 4.4 of \cite{bertens2016keeping}.

% \begin{algorithm}[t]
%     \caption{Function for Variations}
%     \label{alg:pruning}
%     \KwIn{The dataset $S$, a pattern $p^*$ and a code table $CT$}
%     \KwOut{A code table $CT$ with variations of $p$}
%     \tcc{the explanation can be found in Sec. \ref{sec:variations}}
%     $Cand \leftarrow p^* \times gapEvents(p^*)$\;
%     \For{{\bf each} $p \in Cand$}{
%         \If{$L(S,~CT \cup \{p\}) < L(S,~CT)$}{
%             $CT \leftarrow prune(S,~p,~CT) \cup \{p\}$\;
%             $CT \leftarrow variations(S,~p,~CT)$\;
%         }
%     }
%     \Return{$CT$}
% \end{algorithm}

\subsubsection{Speedup with Locality Sensitive Hashing (LSH)}

\label{sec:lsh}

The most time-consuming step was calculating the description length.
This is straightforward, as this was the only step that considered thousands of original sequences.
However, we found that, in line 4, Algorithm \ref{alg:summarizing}, a candidate $p$ is usually not an ideal pattern after comparing $L$, indicating that this step was wasting a lot of time examining useless candidates.
A faster algorithm would filter out those useless candidates before the description length step.

Since the candidate pattern is constructed from two patterns, when these two patterns often occur simultaneously or consecutively, the candidate pattern is more likely to be a real pattern.
Thus, we employ weighted \textit{Locality Sensitive Hashing} (LSH) \cite{ioffe2010improved} for efficiently determining whether two patterns can be combined.
If two sets of numbers have a weighted Jaccard similarity larger than a threshold $th$, the weighted LSH algorithm will generate the same hash value for them.

In our algorithm, for each pattern $p$, we record the positions where it occurs when covering the dataset.
A position is an index of a segment of original sequences, where a segment contains at most $l$ events (default $l$ is 20), and a long sequence will be cut into several segments.
If a pattern spans two segments, we record both positions.
Then we apply weighted LSH to examine whether the two patterns occur in similar sets of positions.
If the weighted LSH algorithm returns the same hash value, the candidate pattern constructed from these two patterns is indicated to be promising.

% 这里不太清楚是否需要加入一段，对应用MDL的内存消耗、时间消耗进行理论分析，还是只要最后进行实验就好了？
% Finally, we consider the runtime complexity and the memory requirements of LSH.
% Collecting the positions of a pattern in code table is not time-consuming.
% Before adding a pattern into code table, we have to computing the description length bases on the cover $C$ to examine whether the pattern brings information compression, where all the positions can be found $C$.
