The purpose of this section is to show how sampling via Langevin Monte Carlo
algorithm yields the first provable convergence guarantees for computing
barycenters in the free-support setup (cf.\ the discussion at the end of
Section~\ref{sec:doubly-entropic-barycenters}). In particular, we provide
computational guarantees for implementing Algorithm~\ref{alg:inexact}.

A measure $\mu$ is said to satisfy the logarithmic Sobolev inequality (LSI)
with constant $C$ if for all sufficiently smooth functions $f$ it holds that
\begin{equation}
  \mathbf{E}_{\mu}[f^{2}\log f^{2}] - \E_{\mu}[f^{2}]\log\E_{\mu}[g^{2}]
  \leq 2C\mathbf{E}_{\mu}[\|\nabla f\|^{2}_{2}].
\end{equation}
To sample from a measure $\mu(dx) = \exp(-f(x))dx$ supported on $\R^{d}$, the
unadjusted Langevin Monte Carlo algorithm is defined via the following
recursive update rule:
\begin{equation}
  \label{eq:langevin-monte-carlo-updates}
  x_{k+1} = x_{k} - \eta \nabla f(x_{k}) + \sqrt{2\eta}
  Z_{k},\quad\text{where}\quad Z_{k} \sim \mathcal{N}(0, I_{d}).
\end{equation}
The following Theorem is due to \citet*[Theorem 3]{vempala2019rapid}.
\begin{theorem}%[\citep[Theorem 3]{vempala2019rapid}]
  \label{thm:langevin}
  Let $\mu(dx) = \exp(-f(x))dx$ be a measure on $\R^{d}$. Suppose that $\mu$
  satisfies LSI a with constant $C$ and that $f$
  has $L$-Lipschitz gradient with respect to the Euclidean norm.
  Consider the sequence of iterates $(x_{k})_{k \geq 0}$ defined via
  \eqref{eq:langevin-monte-carlo-updates} and let
  let $\rho_{k}$ be the distribution of $x_{k}$.
  Then, for any $\varepsilon > 0$, any $\eta \leq
  \frac{1}{8L^{2}C}\min\{1,\frac{\varepsilon}{4d}\}$, and any
  $k \geq \frac{2C}{\eta}\log\frac{2\kl{\rho_{0}}{\mu}}{\varepsilon}$,
  it holds that
  \begin{equation}
    \kl{\rho_{k}}{\mu} \leq \varepsilon.
  \end{equation}
\end{theorem}
Thus, LSI on the measure $\mu$ provides convergence guarantees on
$\kl{\rho_{k}}{\mu}$. It is shown in \cite[Lemma 1]{vempala2019rapid} how to
initialize the iterate $x_{0}$ so that $\kl{\rho_{0}}{\mu}$ scales linearly
with the ambient dimension $d$ up to some additional terms.
The final condition described in Problem
Setting~\ref{setup:ball-and-squared-loss}
ensures that (by \cite[Lemma 1]{vempala2019rapid})
for any $\sigma > 0$, the initialization
scheme $x_{0} \sim \mathcal{N}(x_{\vecpsi}, I_{d})$
for the  Langevin algorithm \eqref{eq:langevin-monte-carlo-updates}
satisfies
\begin{equation}
  \label{eq:initialization-langevin-KL-bound}
  \kl{\rho_{0}}{\mu_{\vecpsi,\sigma}}
  \leq \frac{c_{\infty}(\mathcal{X})}{\tau} +
  \frac{d}{2}\log\frac{L_{\sigma}}{2\pi},
\end{equation}
where $L_{\sigma}$ is the smoothness constant of $V_{\vecpsi}/\tau +
\mathrm{dist}(x, \mathcal{X})/(2\sigma^{2})$ (see
Lemma~\ref{lemma:properties-of-mu-sigma}) and
$\mu_{\vecpsi,\sigma}$ is the probability measure defined in
\eqref{eq:mu-sigma-dfn}.


To implement the approximate Sinkhorn oracle described in
Definition~\ref{dfn:approximate-sinkhorn-oracle}, we can combine
Lemma~\ref{lemma:approximate-oracle-implementation} with approximate sampling
via Langevin Monte Carlo; note that by Pinsker's inequality, Kullback-Leibler
divergence guarantees provide total variation guarantees which are sufficient
for the application of Lemma~\ref{lemma:approximate-oracle-implementation}.
Therefore, providing provable convergence guarantees for
Algorithm~\ref{alg:inexact}
amounts to proving that we can do arbitrarily accurate approximate sampling
from distributions of the form
\begin{equation}
  \mu_{\vecpsi}(dx)
  \propto \mathbb{1}_{\mathcal{X}}(x)\exp(-V_{\vecpsi}(x)/\tau)dx,
  \quad\text{where}\quad V_{\vecpsi}(x) =
  \sum_{j=1}^{k}w_{j}\phi^{j}_{\psi^{j}}(x).
\end{equation}
Here $\mathbb{1}_{\mathcal{X}}$ is the indicator function of $\mathcal{X}$,
$\vecpsi$ is an arbitrary iterate generated by
Algorithm~\ref{alg:inexact}, and we consider the free-support setup
characterized via the choice $\piref(dx) = \mathbb{1}_{\mathcal{X}}dx$.

Notice that we cannot apply Theorem~\ref{thm:langevin} directly because the
measure $\mu_{\vecpsi}$ defined above has constrained support while
Theorem~\ref{thm:langevin} only applies for measures supported on all of
$\mathbb{R}^{d}$. Nevertheless, we will show that the compactly supported
measure $\mu_{\vecpsi}$ can be approximated by a measure $\mu_{\vecpsi,
\sigma}$, where the parameter $\sigma$ will trade-off LSI constant of
$\mu_{\vecpsi, \sigma}$ against the total variation norm between the two
measures. To this end, define
\begin{equation}
  \label{eq:mu-sigma-dfn}
  \mu_{\vecpsi, \sigma} =
  \propto
\exp(-V_{\vecpsi}(x)/\tau
-\mathrm{dist}(x, \mathcal{X})^{2}/(2\sigma^{2}))dx,
  \quad\text{where}\quad
  \mathrm{dist}(x,\mathcal{X})
  = \inf_{y \in \mathcal{X}}\|x - y\|_{2}.
\end{equation}

The following lemma, proved at the end of this section,
collects the main properties of the measure $\mu_{\vecpsi,\sigma}$.
\begin{lemma}
  \label{lemma:properties-of-mu-sigma}
  Consider the setup described in Problem
  Setting~\ref{setup:ball-and-squared-loss}.
  Let $\vecpsi$ be any iterate generated by Algorithm~\ref{alg:inexact} and
  let $\mu_{\vecpsi, \sigma}$ be the distribution defined in
  \eqref{eq:mu-sigma-dfn}. Then, the measure $\mu_{\vecpsi,\sigma}$ satisfies
  the following properties:
  \begin{enumerate}
    \item For any $\sigma \in (0,1/4]$ it holds that
     \begin{equation}
       \|\mu_{\vecpsi} - \mu_{\vecpsi, \sigma}\|_{\mathrm{TV}}
        \leq
        2\sigma
        \exp\left(\frac{8R^{2}}{\tau}\right)
        \left[
          \left(4Rd^{-1/4}\right)^{d-1}
          + 1
        \right].
    \end{equation}

   \item
     Let $V_{\sigma}(x) = \exp(-V_{\vecpsi}(x)/\tau
     -\mathrm{dist}(x, \mathcal{X})^{2}/(2\sigma^{2}))$; thus $\mu_{\vecpsi,
     \sigma}(dx) = \exp(-V_{\sigma}(x))dx$.
     The function $V_{\sigma}$ has $L_{\sigma}$-Lipschitz gradient where
       \begin{equation}
         \label{eq:L-sigma-dfn}
         L_{\sigma} =
         \frac{1}{\tau} + \frac{1}{\tau\lambda}4R^{2}\max_{j}m_{j} +
         \frac{1}{\sigma^{2}}.
       \end{equation}

     \item The measure $\mu_{\vecpsi,\sigma}$ satisfies LSI with a constant
       $C_{\sigma} = \mathrm{poly}(R, \exp(R^{2}/\tau), L_{\sigma})$.
  \end{enumerate}
\end{lemma}
Above, the notation $C = \mathrm{poly}(x,y,z)$ denotes a constant that depends
polynomially on $x,y$ and $z$.
With the above lemma at hand, we are ready to prove
Theorem~\ref{thm:inexact-algorithm-implementation}.


\begin{proof}[Proof of Theorem~\ref{thm:inexact-algorithm-implementation}]
  Let $\vecpsi$ be an arbitrary iterate generated via
  Algorithm~\ref{alg:inexact}.
  We can simulate a step of approximate Sinkhorn oracle with accuracy
  $\varepsilon$ via Lemma~\ref{lemma:approximate-oracle-implementation}
  (with $\zeta = \varepsilon/4$)
  in time $\mathrm{poly}(n,m,d)$ provided
  access to $n = \mathrm{poly}(\varepsilon^{-1}, m, \log(m/\delta))$ samples
  from any distribution $\mu_{\vecpsi}'$ such that
  \begin{equation}
    \label{eq:mu-prime-needed-approximation}
    \|\mu_{\vecpsi}' - \mu_{\vecpsi}\|_{\mathrm{TV}} \leq
    \frac{\varepsilon^{2}}{16m}.
  \end{equation}
  To find a choice of $\mu_{\vecpsi}'$ satisfying the above bound, consider
  the distribution
  \begin{equation}
    \mu_{\vecpsi, \sigma}\quad\text{with}\quad
      \sigma = \frac{\varepsilon^{2}}{32m}\cdot
      \left(
        2
        \exp\left(\frac{8R^{2}}{\tau}\right)
        \left[
          \left(4Rd^{-1/4}\right)^{d-1}
          + 1
        \right]
      \right)^{-1}.
  \end{equation}
  Let $C_{\sigma}$ and $L_{\sigma}$ be the LSI and smoothness constants of the
  distribution $\mu_{\vecpsi,\sigma}$ provided in
  Lemma~\ref{lemma:properties-of-mu-sigma}.
  By Theorem~\ref{thm:langevin}, it suffices to run the Langevin algorithm
  ~\eqref{eq:langevin-monte-carlo-updates} for $\mathrm{poly}(\varepsilon^{-1},
  m, d, C_{\sigma}, L_{\sigma})$ number of iterations to obtain a sample from
  a distribution $\widetilde{\mu}_{\vecpsi,\sigma}$ such that
  \begin{equation}
    \|\widetilde{\mu}_{\vecpsi, \sigma} - \mu_{\vecpsi, \sigma}\|_{\mathrm{TV}}
    \leq \frac{\varepsilon^{2}}{32m}.
  \end{equation}
  In particular, by the triangle inequality for the total variation norm,
  the choice $\mu_{\vecpsi}' = \widetilde{\mu}_{\vecpsi, \sigma}$ satisfies
  \eqref{eq:mu-prime-needed-approximation}. This finishes the proof.
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:properties-of-mu-sigma}}
\label{sec:proof-of-lemma-properties-of-mu-sigma}
To simplify the notation, denote $\mu = \mu_{\vecpsi}, \mu_{\sigma} =
\mu_{\vecpsi, \sigma}$, $V(x) = V_{\vecpsi}(x)/\tau$,
and
$V_{\sigma}(x) = V(x)/\tau
  +\mathrm{dist}(x, \mathcal{X})^{2}/(2\sigma^{2})$.

\paragraph{Total variation norm bound.}

With the above shorthand notation, we have
\begin{equation}
  \mu(dx) = \mathbb{1}_{\mathcal{X}}Z^{-1}\exp(-V(x))dx,\quad\text{where}\quad
  Z = \int_{\mathcal{X}}\exp(-V(x))dx
\end{equation}
and
$$
  \mu_{\sigma}(dx) = (Z+Z_{\sigma})^{-1}\exp(-V_{\sigma}(x))dx,
  \quad\text{where}\quad
  Z_{\sigma} = \int_{\mathbb{R}^{d} \backslash
  \mathcal{X}}\exp(-V_{\sigma}(x))dx.
$$
We have
\begin{align}
  \|\mu - \mu_{\sigma}\|_{\mathrm{TV}}
  &=
  \int_{\R^{d}\backslash\mathcal{X}}(Z+Z_{\sigma})^{-1}\exp(-V_{\sigma}(x))dx
  + \int_{\mathcal{X}}|(Z+Z_{\sigma})^{-1} - Z^{-1}|\exp(-V(x))dx
  \\
  &=
  \frac{2Z_{\sigma}}{Z + Z_{\sigma}}
  \leq
  \frac{2Z_{\sigma}}{Z}
  \leq
  2\exp\left(\frac{c_{\infty}(\mathcal{X})}{\tau}\right)Z_{\sigma}
  \leq
  2\exp\left(\frac{4R^{2}}{\tau}\right)Z_{\sigma}.
\end{align}
We thus need to upper bound $Z_{\sigma}$.
Let $\mathrm{Vol}(A)$ be the Lebesgue measure of the set $A$,
let $\partial A$ denote the boundary of $A$,
and let $A + B = \{a + b : a \in A, b \in B\}$ be the Minkowski sum of sets
$A$ and $B$. Using the facts that for each $j \in \{1,\dots,k\}$ we have
$\sup_{y \in \mathcal{X}} \psi^{j}(y) \leq c_{\infty}(\mathcal{X}) \leq 4R^{2}$ and
that $\mathcal{X} \subseteq \mathcal{B}_{R}
= \{x : \|x\|_{2} \leq R\}$ we have
\begin{align}
  Z_{\sigma}
  &= \int_{\mathbb{R}^{d} \backslash
  \mathcal{X}}\exp(-V_{\sigma}(x))dx
  \\
  &\leq
  \exp\left(\frac{4R^{2}}{\tau}\right) \int_{\mathbb{R}^{d} \backslash
    \mathcal{X}}\exp\left(
  -\frac{\mathrm{dist}(x, \mathcal{X})}{2\sigma^{2}}\right)dx
  \\
  &=
  \exp\left(\frac{4R^{2}}{\tau}\right)
  \int_{0}^{\infty}
  \mathrm{Vol}(\partial(\mathcal{X} + \mathcal{B}_{x}))
    \exp\left(
    -\frac{x^{2}}{2\sigma^{2}}\right)dx
  \\
  &\leq
  \exp\left(\frac{4R^{2}}{\tau}\right)
  \int_{0}^{\infty}
  \mathrm{Vol}(\partial \mathcal{B}_{R+x})
    \exp\left(
    -\frac{x^{2}}{2\sigma^{2}}\right)dx
  \\
  &=
  \exp\left(\frac{4R^{2}}{\tau}\right)
  \frac{\pi^{d/2}}{\Gamma(d/2)}
  \int_{0}^{\infty}
    (R + x)^{d-1}
    \exp\left(
    -\frac{x^{2}}{2\sigma^{2}}\right)dx.
\end{align}
Bounding $(R+x)^{d-1} \leq 2^{d-1}R^{d-1} + 2^{d-1}x^{d-1}$
and computing the integrals results in
\begin{align}
  \|\mu - \mu_{\sigma}\|_{\mathrm{TV}}
  &\leq
  2\exp\left(\frac{8R^{2}}{\tau}\right)
  \frac{\pi^{d/2}}{\Gamma(d/2)}
  2^{d-1}
  \left[
    R^{d-1}
    \sigma
    \frac{\sqrt{\pi}}{2}
    +
    2^{d/2-1}\Gamma(d/2)\sigma^{d}
  \right]
  \\
  &\leq
  2\sigma\exp\left(\frac{8R^{2}}{\tau}\right)
  \left[
    \frac{(2R)^{d-1}}{\Gamma(d/2)}
    + (4\sigma)^{d-1}
  \right].
\end{align}
Using the assumption $\sigma \leq 1/4$ and using the bound $\Gamma(d) \geq
(d/2)^{d/2}$ we can further simplify the above bound to
\begin{equation}
  \|\mu - \mu_{\sigma}\|_{\mathrm{TV}}
  \leq
  2\sigma\exp\left(\frac{8R^{2}}{\tau}\right)
  \left[
    \left(4Rd^{-1/4}\right)^{d-1}
    + 1
  \right],
\end{equation}
which completes the proof of the total variation bound.


\paragraph{Lipschitz constant of the gradient.}
Recall that for any any $j \in \{1, \dots,d\}$ we have
\begin{equation}
  \phi^{j}(x) - \frac{1}{2}\|x\|_{2}^{2} = -\lambda\log\left(
    \sum_{l=1}^{n_j}
    \exp\left(\frac{\psi^{j}(y^{j}_{l}) - \frac{\|y^{j}_{l}\|_{2}^{2}}{2} +
    \langle x, y^{j}_{l}\rangle}{\lambda}\right)
    \nu^{j}(y^{j}_{l})
    \right).
\end{equation}
Denote $\widetilde{\phi}^{j}(x) = \phi^{j}(x) - \frac{1}{2}\|x\|_{2}^{2}$.
Fix any $x, x'$ and define $g(t) = \widetilde{\phi}^{j}(x + (x'-x)t)$.
Then, for any $t \in [0,1]$ we have
\begin{align}
  \label{eq:second-derivative-phi-tilde}
  g''(s) =
  -\frac{1}{\lambda}\mathrm{Var}_{L \sim \rho_{t}}\left[
    (Y^{j}(x' - x))_{L}
  \right]
  \geq
  -\frac{1}{\lambda}\|x-x'\|_{2}^{2}m_{j}4R^{2},
\end{align}
where
\begin{align}
  \rho_{t}(l)
  \propto
  \nu(y^{j}_{l})
  \exp\left(\frac{\psi^{j}(y^{j}_{l}) - \frac{\|y^{j}_{l}\|_{2}^{2}}{2} +
  \langle x + t(x' - x), y^{j}_{l}\rangle}{\lambda}\right)
\end{align}
and $Y^{j} \in \mathbb{R}^{d \times m_{j}}$ is the matrix whose $l$-th column
is equal to the vector $y^{j}_{l}$.

Because $\widetilde{\psi}^{j}$ is concave, the bound
\eqref{eq:second-derivative-phi-tilde} shows that
$\phi^{j}$ is $1 + \frac{1}{\lambda}m_{j}4R^{2}$-smooth.

Combining the above with the fact that the convex function
$\mathrm{dist}(x,\mathcal{X})$ has $1$-Lipschitz gradient
\cite[Proposition 12.30]{bauschke2017convex} proves the desired smoothness
bound on the function $V_{\sigma}$.

\paragraph{LSI Constant bound.}
The result follows, for example, by applying the sufficient log-Sobolev
inequality criterion stated in
\cite[Corollary 2.1, Equation (2.3)]{cattiaux2010note}, combined with
the bound \eqref{eq:second-derivative-phi-tilde}.
The exact constant appearing in the log-Sobolev inequality can be traced from
\cite[Equation (3.10)]{cattiaux2010note}.







