\section{Damped Sinkhorn Scheme}
\label{sec:damped-sinkhorn}

This section introduces a damped Sinkhorn-based optimization scheme
(Algorithm~\ref{alg:exact}) and provides guarantees for its convergence
(Theorem~\ref{thm:exact-scheme-convergence}). Before describing the algorithm,
we make a quick detour to the following lemma, proved in
Appendix~\ref{sec:proof-of-suboptimality-to-kl-lemma},
which shows that the sub-optimality gap bounds on the dual objective
\eqref{eq:doubly-entropic-dual} can be transformed into corresponding bounds on
relative entropy between the $(\lambda,\tau)$-barycenter and the barycenter
associated to a given dual variable.

\begin{lemma}
  \label{lemma:suboptimality-to-kl}
  Fix any $\lambda,\tau > 0$ and $\vecnu,w$.
  Let $\vecpsi^{*}$ be the maximizer of dual problem
  $E^{\vecnu, w}_{\lambda,\tau}$ and let $\mu_{\boldsymbol{\psi}^{*}}$ be the
  corresponding minimizer of the primal objective
  \eqref{eq:doubly-regularized-barycenter-primal}.
  Then, for any $\vecpsi \in L_{1}(\vecnu)$ we have
  \begin{equation}
    \kl{\mu_{\boldsymbol{\psi}^{*}}}{\mu_{\vecpsi}}
    \leq \tau^{-1}(E^{\vecnu, w}_{\lambda, \tau}
    (\boldsymbol{\psi}^{*})
    -E^{\vecnu, w}_{\lambda, \tau}(\vecpsi)).
  \end{equation}
\end{lemma}

We now turn to describing an iterative scheme that ensures convergence of the
dual suboptimality gap to zero.
Let $\vecpsi_{t}$ be an iterate at time $t$. Then, we have
\begin{equation}
  E^{\vecnu,w}_{\lambda, \tau}(\vecpsi_{t})
  = L(\vecpsi_{t}, \vecphi_{t}, \mu_{t})
  = \sum_{j=1}^{k}w_{j}\mathbf{E}_{\nu^{j}}[\psi_{t}^{j}]
  -\mathbf{E}_{\mu_{t}}[\phi_{t}^{j}]
  +\tau\kl{\mu_{t}}{\piref},
\end{equation}
where
\begin{equation}
  \label{eq:maxi-minimization-steps}
  \phi^{j}
  = \mathrm{argmax}_{\phi}\,
  E^{\mu_{t-1},\nu^{j}}_{\lambda}(\phi,\psi^{j}_{t})
  \quad\text{and}\quad
  \mu_{t} = \mathrm{argmin}_{\mu}\, \bigg\{
    \mathbf{E}_{\mu}\big[\sum_{j}w_{j}\phi^{j}_{t}\big] +
  \tau\kl{\mu}{\piref}\bigg\}.
\end{equation}
In particular, when optimizing the dual objective $E^{\nu,w}_{\lambda, \tau}$,
every time the variable $\vecpsi_{t}$ is updated, it automatically triggers
the exact maximization/minimization steps defined in
\eqref{eq:maxi-minimization-steps}.
It is thus a natural strategy to fix $\vecphi_{t}$ and $\mu_{t}$ and perform
exact minimization on $\vecpsi$, which can be done in closed form:
\begin{equation}
  \label{eq:undamped-updates}
  \psi^{j}_{t+1} =
  \mathrm{argmax}_{\psi}\,
  E^{\mu_{t},\nu^{j}}_{\lambda}(\phi^{j}_{t},\psi)
  =
  \psi^{j}_{t} -\lambda\log\frac{d\nu_{t}^{j}}{d\nu^{j}},
\end{equation}
where $\nu_{j}^{t}$ denotes the marginal distribution $\nu^{j}_{\vecpsi_{t}}$
defined in \eqref{eq:psi-marginals}.
The update \eqref{eq:undamped-updates} performs a Sinkhorn update on each block
of variables $\psi^{j}$.
Together, the update \eqref{eq:undamped-updates} followed by
\eqref{eq:maxi-minimization-steps} results in the iterative Bregman projections
algorithm introduced in \cite{benamou2015iterative}. In
\cite{kroshnin2019complexity}, it was shown that this scheme converges for the
$(\lambda,\lambda)$-barycenters.
The analysis of \cite{kroshnin2019complexity} is built upon a
different dual formulation from the one considered in our work;
this alternative formulation is only available
when $\tau = \lambda$ \cite[Section 2.3]{chizat2023doubly}
and thus excludes the consideration of debiased barycenters $(\lambda, \lambda/2)$.

We have observed empirically that the iterates of the iterative Bregman
projections (i.e., the scheme of updates defined in \eqref{eq:undamped-updates}
and \eqref{eq:maxi-minimization-steps})
diverge whenever $\tau < \lambda/2$. Indeed, decreasing the outer
regularization parameter $\tau$ makes the minimization step in
\eqref{eq:maxi-minimization-steps} less stable. As a result, the cumulative
effect of performing the updates \eqref{eq:undamped-updates} and
\eqref{eq:maxi-minimization-steps} may result in a decrease in the value of
the optimization objective $E^{\vecnu,w}_{\lambda,\tau}$.

One of the main contributions of our work is to show that this bad behaviour
can be mitigated by damping the exact Sinkhorn updates
\eqref{eq:undamped-updates}. This leads to Algorithm~\ref{alg:exact} for which
convergence guarantees are provided in
Theorem~\ref{thm:exact-scheme-convergence} stated below.

\begin{algorithm}
  \caption{Exact Damped Sinkhorn Scheme}
  \label{alg:exact}
  \KwIn{regularization strengths $\lambda,\tau > 0$,
  reference measure $\piref$,number of iterations $T$ and
  $k$ marginal measures $\nu^{1},\dots,\nu^{k}$ with positive weights
  $w_{1}, \dots, w_{k}$ such that $\sum_{j=1}^{k}w_{j} = 1$.}
  \begin{enumerate}
    \item
      Set $\eta=\min(1, \tau/\lambda)$
      and initialize $(\psi^{j}_{0})=0$
      for $j \in \{1,\dots,k\}$.
  \item For $t=0,1\dots,T-1$ do
  \begin{enumerate}
    \item $\phi_{t}^{j}(x)
      \leftarrow
      - \lambda \log\int_{\mathcal{X}} \exp((\psi_{t}^{j}(y)-c(x,y))/\lambda)\nu^{j}(dy)$
      for $j \in \{1,\dots,k\}$
    \item $V_{t}(x) \leftarrow \sum_{j=1}^{k} w_j \phi^{j}(t)(x)$
    \item $Z_{t}  \leftarrow \int \exp(-V_{t}(x)/\tau)d \piref(dx)$
    \item $\mu_{t}(dx) \leftarrow Z_{t}^{-1}\exp(-V_{t}(x)/\tau)\piref(dx)$
    \item $
      \frac{d\nu^{j}_{t}}{d\nu^{j}}(y)
      \leftarrow
      \int\exp\left(
      \frac{\phi^{j}_{t}(x) + \psi^{j}_{t}(y) - c(x,y)}{\lambda}\right)\mu_{t}(dx)$
      for $j \in \{1,\dots,k\}$.
    \item $\psi^{j}_{t+1}(y) \leftarrow \psi^{j}_{t}(y) - \eta\lambda\log
      \frac{d\nu^{j}_{t}}{d\nu^{j}}(y)$
      for $j \in \{1,\dots,k\}$.
    \end{enumerate}
  \item Return $(\phi_{T}^{j}, \psi_{T}^{j})_{j=1}^{k}$.
  \end{enumerate}
\end{algorithm}


\begin{theorem}
  \label{thm:exact-scheme-convergence}
  Fix any $\lambda,\tau > 0$ and $\vecnu,w$.
  Let $\psi^{*}$ be the maximizer of dual problem
  $E^{\vecnu, w}_{\lambda,\tau}$.
  Let $(\vecpsi_{t})_{t \geq 0}$ be the sequence of iterates generated by
  Algorithm~\ref{alg:exact}.
  Then, for any $t \geq 1$ it holds that
  \begin{equation}
    E^{\vecnu, w}_{\lambda,\tau}(\vecpsi^{*})
    -
    E^{\vecnu, w}_{\lambda,\tau}(\vecpsi_{t})
    \leq \frac{2c_{\infty}(\mathcal{X})^{2}}{\min(\lambda,\tau)}
    \,\frac{1}{t}.
  \end{equation}
\end{theorem}

Our convergence analysis draws upon the existing analyses of Sinkhorn's
algorithm \cite{altschuler2017near, dvurechensky2018computational}, which in
turn are based on standard proof strategies in smooth convex
optimization (e.g., \cite[Theorem 2.1.14]{nesterov2018lectures}).
Concerning the proof of Theorem~\ref{thm:exact-scheme-convergence},
the main technical contribution of our work lies in the following
proposition proved in Appendix~\ref{sec:proof-of-one-step-improvement-proposition}.

\begin{proposition}
  \label{prop:one-step-improvement}
  Consider the setup of Theorem~\ref{thm:exact-scheme-convergence}.
  Then, for any integer $t \geq 0$ it holds that
  \begin{equation}
    E^{\vecnu,w}_{\lambda, \tau}(\vecpsi_{t+1})
    - E^{\vecnu,w}_{\lambda,\tau}(\vecpsi_{t})
    \geq
    \min\left(\tau, \lambda\right)
    \sum_{j=1}^{k}w_{j}\kl{\nu^{j}}{\nu^{j}_{t}}.
  \end{equation}
\end{proposition}

With Proposition~\ref{prop:one-step-improvement} at hand, we are ready to prove
Theorem~\ref{thm:exact-scheme-convergence}.

\begin{proof}[Proof of Theorem~\ref{thm:exact-scheme-convergence}]
  Denote $\delta_{t} = E^{\vecnu,w}_{\lambda,\tau}(\vecpsi^{*})
  - E^{\vecnu,w}_{\lambda,\tau}(\vecpsi_{t})$.
  We would like to relate the suboptimality gap $\delta_{t}$ to the increment
  $\delta_{t} - \delta_{t+1}$. To do this, we will first show that the iterates
  $\vecpsi_{t}$ have their oscillation norm bounded uniformly in $t$.
  Indeed, for any $j \in \{1,\dots,k\}$, any $t \geq 1$, and any $y \in
  \mathcal{X}$ we have
  \begin{align}
    \psi^{j}_{t}(y)
    = (1-\eta)\psi^{j}_{t-1}(y) + \eta\psi_{\phi^{j}_{t}}(y).
  \end{align}
  By \eqref{eq:schroedinger-potentials-bounded},
  $\psi_{\phi^{j}_{t}}$ has oscillation norm bounded by
  $c_{\infty}(\mathcal{X})$. Because $\psi^{j}_{0} = 0$ and $\eta \in (0,1]$,
  by induction on $t$ it follows that $\|\psi_{t}\|_{\mathrm{osc}} \leq
  c_{\infty}(\mathcal{X})$ for any $t \geq 0$.
  Combining the bound on the dual sub-optimality gap
  \eqref{eq:concave-suboptimality-gap} with Pinsker's inequality yields
  \begin{equation}
    \delta_{t} \leq 2c_{\infty}(\mathcal{X})\sum_{j=1}^{k}w_{j}\|\nu^{j} -
    \nu_{t}^{j}\|_{\mathrm{TV}}
    \leq \sqrt{2}c_{\infty}\sum_{j=1}^{k}w_{j}\sqrt{\kl{\nu^{j}}{\nu^{j}_{t}}}.
  \end{equation}
  Using concavity of the square root function,
  Proposition~\ref{prop:one-step-improvement}  yields for any $t \geq 0$
  \begin{equation}
    \delta_{t} - \delta_{t+1}
    \geq \min(\lambda,\tau) \sum_{j=1}^{k}
    w_{j}\kl{\nu^{j}}{\nu^{j}_{t}}
    \geq
    \frac{\min(\lambda,\tau)}{2c_{\infty}(\mathcal{X})^{2}} \delta_{t}^{2}.
  \end{equation}
  By Proposition~\ref{prop:one-step-improvement}, the sequence $\delta_{t}$ is
  non-increasing. Hence, dividing the above equality by
  $\delta_{t}\delta_{t+1}$ yields
  \begin{equation}
    \frac{1}{\delta_{t+1}} - \frac{1}{\delta_{t}} \geq
    \frac{\min(\lambda,\tau)}{2c_{\infty}(\mathcal{X})^{2}}.
  \end{equation}
  Telescoping the left hand side completes the proof.
\end{proof}
