% !TEX program = pdflatex
% !TEX root = main.tex


\section{The Model}

We represent a series of interactions between $N$ individuals as a sequence of weighted directed networks with adjacency matrix $A^t$ for $t=0,1,2,\ldots,T$. For each $t$, its entry $A_{ij}^t$ is the outcome of interactions $i \rightarrow j$ suggesting that $i$ is ranked above $j$. This allows both cardinal and ordinal inputs. For instance, in team sports, $A_{ij}^t$ could be the number of points by which team $i$ beat team $j$, or we could simply set $A_{ij}^t=1$ to indicate that $i$ won and $j$ lost. We can include the case where individuals interact multiple times at time $t$ by summing the corresponding entries.

We assume that the values of $A_{ij}^t$ are influenced by a vector of real-valued ranks $\v{s}^t=(s_{1}^t,\dots, s_{N}^t)$, where $s_i^t$ is $i$'s skill, strength or prestige at time $t$.
To model these interactions, we follow SpringRank's approach of imagining the network as a physical system~\cite{de2018physical}. Specifically, each node $i$ is embedded in $\mathbb{R}$ at position $s_i^t$, and each directed edge $i \rightarrow j$ becomes an oriented spring with a non-zero resting length and displacement $s_i^t-s_j^t$. Since we are free to rescale latent space and the energy scale, we set the spring constant and resting length to $1$. The spring corresponding to an edge $i \rightarrow j$ at time $t$ then has energy
\be\label{eqn:staticH}
H_{ij}(s_i^t,s_j^t)=\f{1}{2} \bup{s_i^t-s_j^t-1}^{2} \, .
\ee
If there were no other effects, the total energy of the system at time $t$ would then be 
\be\label{eqn:totalstaticH}
H^t(\v{s}^t) = \sum_{i,j=1}^{N} A_{ij}^t \,H_{ij}(s_i^t,s_j^t) \, .
\ee
If we determined $\v{s}^t$ by minimizing $H^t$ for each $t$ separately, we would simply be applying the static SpringRank model separately to each ``snapshot'' of the network. This would ignore all previous (and future) interactions, and ignore the hypothesis that ranks change smoothly from one time-step to the next.

% Figure environment removed

To model this smoothness, we also assume a dependence between ranks at successive time-steps. Specifically, we extend the Hamiltonian~\eqref{eqn:totalstaticH} with an extra term that models the \emph{self-interaction} between past and current ranks,
\begin{equation}\label{eqn:selfH}
\Hself^t(\v{s}^t,\v{s}^{t-1}) 
= \frac{\kself}{2} \sum_{i=1}^N (s_i^t-s_i^{t-1})^2 \, .
\end{equation}
This can be seen as a set of additional ``self-springs'' that connect the rank of each individual with its own previous rank. The spring constant $\kself$ parametrizes how smoothly we want the ranks to change from one step to the next. In inference terms, $\kself$ is a hyperparameter which we tune using cross-validation.

Summing over all time-steps $0 < t \le T$ and adding this to the pairwise interactions at each time-step then gives a total energy

\begin{align}\label{eqn:fullH}
\Htotal(\{\v{s}^t\}) = \sum_{t=0}^T H^t(\v{s}^t) + \sum_{t=1}^T \Hself^t(\v{s}^t,\v{s}^{t-1}) \, .
\end{align}
We call this the dynamical SpringRank Hamiltonian. The optimal ranks $\v{s}^0,\v{s}^1,\ldots,\v{s}^T$ are those that minimize it.


There are two ways to minimize $\Htotal$. One is to proceed in an online way, moving forward in time. In this approach, we use the static SpringRank model Eq.~\eqref{eqn:totalstaticH} to find the initial ranks $\v{s}^0$ by minimizing $H^0(\v{s}^0)$. As in Ref.~\cite{de2018physical}, the energy is unchanged if we add a constant to all the ranks; we can break this translational symmetry by setting the mean initial rank $(1/N) \sum_{i=1}^N v_i^0$ to zero.
Then, at each subsequent time-step $t \ge 1$, we update the ranks by taking into account both the new pairwise interactions and the self-springs connecting the ranks with their previous values. Namely, given $\v{s}^{t-1}$ and $A^t$, we find the ranks $\v{s}^t$ that minimize $H^t(\v{s}^t) + \Hself^t(\v{s}^t,\v{s}^{t-1})$.

Since this is a convex function of $\v{s}^t$, we can find its minimum by setting its gradient to zero, or equivalently by balancing all the forces $v_i^t$. This yields a system of linear equations:
\begin{align}\label{eqn:fullsolution}
\rup{ D^{out,t}+D^{in,t}- \bup{A^t + (A^t)^\dagger}+\kself\id} \,\v{s}^t
&=\rup{D^{out,t}-D^{in,t}}\v{1} \nonumber \\& +\kself\, \v{s}^{t-1} \, . 
\end{align}

Here 
$D^{out,t}$ and $D^{in,t}$ are diagonal matrices whose entries are the weighted out- and in-degrees $D^{out,t}_{ii}=\sum_{j}A^t_{ij}$ and $D^{in,t}_{ii}=\sum_{j}A^t_{ji}$; 
$\dagger$ denotes the transpose; 
$\id$ is the identity matrix; 
and $\v{1}$ is the all-ones vector.

The matrix on the left side of~\Cref{eqn:fullsolution} is invertible if $\kself > 0$. In particular, its eigenvector $\v{1}$ has eigenvalue $N \kself$. Thus for each $A^t$ and each $\v{s}^{t-1}$, Eq.~\eqref{eqn:fullsolution} has a unique solution $\v{s}^t$. Overall, Eq.~\eqref{eqn:fullsolution} is similar to the regularized version of SpringRank~\cite{de2018physical} with regularization parameter $\alpha= \kself$. However, unlike the static model, there is a term on the right-hand side containing the previous ranks $\v{s}^{t-1}$, creating a Markovian dependence between successive time-steps. We refer to this model as \dsrfull\ (\dsr).

Importantly the online DSR approach does not actually minimize $\Htotal$, instead solving a sequence of minimization problems, one for each time step. To minimize $\Htotal$ instead, we set $\nabla \Htotal(\v{s}^t) = 0$, solving for the minimizers $\v{s}^t$ over all $N(T+1)$ ranks simultaneously, yielding the following system of equations (SI \Cref{sec:h_total_derive}):

\begin{align}\label{eqn:h_total}
\rup{ D^{out,t}+D^{in,t} - \bup{A^t+(A^t)^\dagger} + 2\kself\id}\,\v{s}^t 
&=\rup{D^{out,t}-D^{in,t}}\v{1} \nonumber\\ 
& +\kself \,\bup{\v{s}^{t-1} + \v{s}^{t+1}} \, . 
\end{align}
This differs from \Cref{eqn:fullH} in that the right-hand side now includes both past and future ranks (which doubles the contribution of $\kself$ on the left). We remove the terms $\v{s}^{t-1}$ and $\v{s}^{t+1}$ for $t=0$ and $t=T$ respectively. This entire system has translational symmetry, since the energy Eq.~\eqref{eqn:fullH} remains the same if we add the same constant to all ranks at all times, but we can again break this symmetry by setting the mean rank to zero.

Additionally, in contrast to \Cref{eqn:fullsolution}, the ranks at $t$ now depend on both $t-1$ and $t+1$, which themselves depend on ranks at adjacent time-steps, so that ranks are affected by interactions in both the past and the future. In computer science, methods like this where the entire history is provided to the algorithm are called \emph{offline}, to distinguish them from \emph{online} approaches that update their results in real time as data becomes available. Thus we refer to this model as \nmdsrfull\ (\nmdsr).  

The cost of solving \Cref{eqn:fullsolution} for a single time-step is the same as static SpringRank with only one additional parameter to be tuned using cross-validation, and there are $T$ such $N$-dimensional equations to be solved successively. On the other hand, \Cref{eqn:h_total} requires solving a single  system of dimension $NT$, whose operator consists of $T$ blocks, each of dimension $N\times N$. While these two approaches feature numbers of non-zero entries that are fundamentally determined by the number of total edges across all time steps, the cost of solving \dsr vs \nmdsr will depend on the particular choice of linear solver~\cite{peng2021solving}.

Philosophically, Eqns.~\eqref{eqn:fullsolution} and~\eqref{eqn:h_total} are trying to do two different things. If we are given all the data $A^0,A^1,\ldots,A^T$ and we want to infer retrospectively how each individual's rank changed over time, it makes sense to include both past and future interactions as in~\eqref{eqn:h_total} so that $s_i^t$ is affected by $i$'s entire history. 

In contrast, \eqref{eqn:fullsolution} can be viewed as modeling each individual's perceived rank at the time, based only on the interactions that have occurred so far.

In principle, one could envisage other ways to formally incorporate an explicit dependence on  $\v{s}^{t-1}$ into the model, and we provide one example in SI \Cref{sec:sidynl}. However, we found that the approaches presented in this Section provide a natural interpretation, result in good prediction performance on both real and synthetic datasets (see \Cref{sec:results}) and are computationally scalable. 

We close this section with two possible extensions to these models. First, in some settings we might have timestamps $t$ that are not successive integers $0,1,\ldots,T$. In this case, if the time interval between two successive times is $\Delta t$, one could scale the spring constant of the self-springs between time-steps as $\kself/\Delta t$. This corresponds to the fact that if we have $\Delta$ identical springs in series, each of which is stretched by $(s^t-s^{t-1})/\Delta$, their total energy is $(1/2)(\kself/\Delta)(s^t-s^{t-1})^2$. The same expression applies if the timestamps are real-valued so that $\Delta$ is not an integer.

Second, if we believe that not just the ranks themselves but their rates of change behave smoothly over time, one could add a momentum term to the Hamiltonian which is quadratic in the discrete second derivative of the ranks. Since
\begin{gather*}
\left( (s^{t+1}-s^t) - (s^t-s^{t-1}) \right)^2
= \left( s^{t+1} - 2 s^t + s^{t-1} \right)^2 \\
= 2 (s^t-s^{t-1})^2 + 2 (s^{t+1}-s^t)^2 - (s^{t+1} - s^{t-1})^2 \, ,
\end{gather*}
this is equivalent to adding a repulsive force, i.e., a spring with negative spring constant, between ranks two time-steps apart. Note that the system nevertheless remains convex: this momentum term is positive semidefinite, so adding it to~\eqref{eqn:fullH} keeps the coupling matrix positive definite except for translational symmetry. Of course, these terms are second-order in time. In the online approach, one would have to determine $\v{s}^0$ from the static model, $\v{s}^1$ from the first-order model~\eqref{eqn:fullsolution}, and then use the model including this momentum term for $\v{s}^t$ for $t \ge 2$. We have not pursued this here, but it may make sense for certain datasets.


\subsection{Moving-window SpringRank}\label{subsec:mwsr}

Before we test the various versions of \dsrfull\ defined above, we consider a simpler model as a baseline. 
The simplest way to extend SpringRank to a dynamical context is to apply the static model to the interactions in a series of ``windows,'' where in each window we sum the interactions over a series of consecutive time-steps. For instance, we can compute $\v{s}^t$ for each $t$ by applying the static model to a window of width $\tau$, i.e., replacing $A^t$ with $\sum_{t'=t}^{t+\tau-1} A^{t'}$. Since these windows overlap, the resulting estimates $\v{s}^t$ will be smooth to some extent, even without imposing an explicit dependence between $\v{s}^t$ and $\v{s}^{t-1}$. We use this method, which we call \mwsrfull\ (\mwsr), as a baseline to compare with the dynamical models presented above.

Roughly speaking, a larger $\tau$ is like a larger self-spring constant $\kself$, since it induces more overlap between windows and thus a stronger correlation between the inferred ranks. However, like a decaying-history approach, \mwsr\ assumes a particular kernel for the importance of past time-steps: namely, that all $t'$ in the window are equally important. In contrast, \dsrfull\ infers the importance of past time-steps by coupling $\v{s}^t$ with $\v{s}^{t-1}$.

However, both models have a free parameter that needs to be tuned, i.e., $\kself$ and $\tau$. A shorter window $\tau$ or smaller spring constant $\kself$ allows the ranks to respond quickly to new interactions, while a longer window or larger spring constant more tightly couples nearby estimates. This trade-off suggests the existence of an optimal window length $\tau_{\opt}$. We tune $\tau$ using a cross-validation procedure as explained in SI \Cref{sisec:tuning}.


\subsection{Generative Model and Synthetic Data}
\label{sec:genmod}

Analogous to a model presented in~\cite{de2018physical}, we propose a probabilistic generative model for dynamic data. It takes as input the ranks $\v{s}^t$ and generates a sequence of weighted directed networks with adjacency matrix $A^t$ at time $t$. One can also imagine models that generate the ranks, for instance with a random walk with Gaussian steps whose log-probability is the self-spring Hamiltonian~\eqref{eqn:selfH}, but we treat $\v{s}^t$ as an input since we want the user of this model to have control over how the ground-truth ranks vary with time.  For instance, in our experiments below we generate synthetic data where the ranks vary sinusoidally.

The generative model has two real-valued parameters: a signal-to-noise ratio or inverse temperature $\beta$, and an overall density of edges $c$. Given the ranks $\v{s}^t$, it generates weighted, directed edges between each pair of nodes $i,j$ independently, as follows. The probability $P_{ij}^t(\beta)$ of $i$ ``beating'' $j$ at time $t$, giving a directed edge $i \to j$, is a logistic function as in~\cite{de2018physical} or the Bradley-Terry-Luce model~\cite{bradley1952,luce1959}:
\bea
\nonumber P_{ij}^t(\beta)=\frac{1}{1+\e^{-2\beta(s_i^t-s_j^t)}} \, .
\eea
The number of such edges, which gives the integer weight $A_{ij}^t$, is then drawn from a Poisson distribution whose mean $\lambda_{ij}^t$ is $cP^t_{ij}\,(\beta)$: 
\be
\label{generative_poiss}
A^t_{ij} \sim \Poi\left(\lambda_{ij}^t=\frac{c}{1+\e^{-2\beta(s_i^t-s_j^t)}}\right).
\ee
Since $P_{ij}^t(\beta) + P_{ji}^t(\beta)=1$, for any pair $i,j$ the total number of interactions $A_{ij}^t + A_{ji}^t$ is Poisson-distributed with mean $c$. The rank differences $s_i^t-s_j^t$ are used only to choose the directions of these edges. This  is equivalent to a model where we define a random multigraph where the number of edges between $i$ and $j$ is $\Poi(c)$, and then we choose the direction of each edge independently according to $P_{ij}^t$.

This is different from the generative model proposed in the static case in~\cite{de2018physical}. In that model the probability that $i$ and $j$ interact depends on $s_i-s_j$ so that nodes are more likely to interact if their ranks are fairly close. This is consistent with SpringRank's assumption that if $i$ beats $j$ then $j$ is below $i$, but not too far below it (since the springs have resting length $1$). This assumption makes sense for some datasets but not for others. By generating synthetic data without this dependence, our intent is to pose a greater challenge to SpringRank by modeling (for example) round-robin tournaments where every team plays each other.

\subsection{Model Evaluation}
\label{sec:testing}

Assessing a ranking model on real datasets is not straightforward since we do not know the true values of the underlying ranks. Nevertheless, we may measure the extent to which inferred ranks are accurate in the sense that they can predict the outcome of new observations. 

There are several performance metrics that can be used for prediction evaluation. From coarse-grained measures capable of predicting the likely winner to more fine-grained measures that also estimate odds, we consider four main metrics in our experiments, detailed in \Cref{sisec:evaluation}. We measure prediction performance using a cross-validation protocol where datasets are divided into training and test sets. The training set is used for hyperparameter tuning and parameter estimation while performance is evaluated on the test set. In order to preserve the chronological ordering of the data, the test set contains future observations, i.e., observations that chronologically follow those used in training. Hyperparameters for each method are tuned using grid-search in order to maximize the performance metrics as described in SI \Cref{sisec:tuning}.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
