% !TEX program = pdflatex
% !TEX root = main.tex

\section{Results}
\label{sec:results}

Having introduced \dsrfull\ and its generative counterpart, as well as discussing model selection between the dynamic and static versions of SpringRank, we now illustrate their behavior on synthetic and real data.

We compare prediction performance on held-out test data for \dsr\ and \nmdsr\ against several state-of-the-art algorithms such as the Elo Rating System (Elo)~\cite{elo1978rating}, TrueSkill (TS)~\cite{dangauthier2008trueskill}, ``win-loss'' decay-history rating (W-L)~\cite{motegi2012network} and Whole-History Rating (WHR)~\cite{coulom2008whole}. In addition, we consider two baselines: static SpringRank (SR) \cite{de2018physical} and \mwsr\ presented above in \Cref{subsec:mwsr}.  (Note that static SpringRank is the limiting case of \mwsr\ with one window covering the entire dataset.)



\subsection{Performance on Synthetic Data}
\label{sec:resultssynt}

We first consider synthetic data, generated as described in \Cref{sec:genmod}, in which ranks evolve according to periodic ground truth dynamics,
\begin{equation}
\label{eq:dyn_score}
s_{i}^{t}=b_{i}\cos(\omega_i t+\phi_i) + c_{i}\cos(\upsilon_i t+\phi_i)\ ,
\end{equation}
where $b_{i}$,$c_{i}$, $\omega_i$, $\phi_i$, $\upsilon_i$ are parameters randomly chosen for each node from a continuous uniform distribution (see \Cref{apx:synthetic_experiments} for details). This results in changes in rankings, and swaps in the order of ranks, reminiscent of real scenarios where teams and players rise and fall.

In order to assess the effect of different network structures, we vary parameters $\beta$ and $c$ from Eq.~\eqref{generative_poiss}. We tabulate the results in \Cref{tab:varying_noise} for varying values of $\beta$ and fixed $c=0.5$, and in \Cref{tab:varying_density} for varying values of $c$ and fixed $\beta=2.0$. We use $50\%$ of the data for training and 4 time-steps for testing, detailed in \Cref{sisec:tuning}.
 
Overall, \dsr\ has the largest number of top performances when considering all metrics (Tables~\ref{tab:varying_noise} and \ref{tab:varying_density}). Notably, \dsr\ outperforms its offline variant \nmdsr, even though \nmdsr\ is given the entire history. This implies that using future interactions to retrodict out-of-sample interactions is less accurate than simply using past interactions. Recall also that \dsr\ is more efficient algorithmically than \nmdsr. Overall, all algorithms perform better for higher values of $\beta$ (i.e., lower noise).

The model with the second largest number of top performances is WHR, which does well particularly for $\sigma_{L}$, the metric that accounts for the likelihood of the outcomes. Notably, static SpringRank is significantly worse than the other models, illustrating that performance can be negatively affected by choosing a static model in dynamical settings. However, for higher noise levels such as $\beta=0.1$, static SpringRank performs comparably well to the other models. This suggests that when there is less structure in the data, a static algorithm is enough: taking the chronological order of events into account does not improve performance. 

As a sanity check of our permutation test for model selection between static and dynamic models, we also considered synthetic datasets generated with static ranks $s^{t}_{i}=s_i$. 
As expected, static SpringRank performs well in comparison to the dynamic algorithms as shown in \Cref{tab:static_results}. 

Finally, we qualitatively investigate the inferred ranking in \Cref{fig:synscores} for \dsr, Elo and W-L where the hierarchy as well as predictive performance is the strongest, as can be seen in \Cref{tab:varying_noise} when $\beta=2.0$. We notice how the time-scale of the evolution of the ranks is different in all cases, with W-L having frequent and sudden jumps while \dsr\ and Elo are smoother with roughly equal performance. In all cases though, we notice small jumps indicating changes in ranks that deviate from the smoothness in the ground truth. Nevertheless performance is strong for \dsr\ and Elo, who perform roughly equally well, as the behaviors of the individual trajectories resembles that of the ground truth well in both cases.

These synthetic tests suggest that dynamical algorithms capture relevant information when the data has a hierarchical structure and chronological ordering matters (i.e low noise). In these cases,  \dsrfull\ performs the best according to several metrics. For higher noise levels or static ranks, timestamp information is no longer relevant and static SpringRank performs well.

\begin{table}[t]
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|cccccccc}
			{$\boldsymbol{\beta}$} & {\textbf{Metric}} & {\textbf{Elo}} & {\textbf{\nmdsr}} & {\textbf{\mwsr}} & {\textbf{\dsr}} & {\textbf{SR}} & {\textbf{TS}} & {\textbf{W-L}} & {\textbf{WHR}} \\
			\hline
			\multirow[t]{4}{*}{\textbf{0.1}} & accuracy & 0.545 & 0.544 & 0.533 & \sethlcolor{green}\hl{\textbf{0.549}} & 0.540 & 0.548 & 0.489 & 0.549 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & 1.568 & 1.596 & 1.658 & 1.574 & 1.646 & \sethlcolor{green}\hl{\textbf{1.551}} & 1.909 & 1.578 \\
			& $\sigma_a$ & 0.593 & \sethlcolor{green}\hl{\textbf{0.594}} & 0.576 & 0.584 & 0.594 & 0.593 & --   & 0.592 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$ & -1.426 & -1.382 & -1.389 & \sethlcolor{green}\hl{\textbf{-1.378}} & -1.382 & -1.392 & --   & -1.389 \\
			\hline
			\multirow[t]{4}{*}{\textbf{0.5}} & accuracy & 0.700 & 0.700 & 0.698 & 0.700 & 0.652 & \sethlcolor{green}\hl{\textbf{0.703}} & 0.384 & 0.701 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & 0.881 & 0.877 & 0.887 & \sethlcolor{green}\hl{\textbf{0.877}} & 1.075 & 0.885 & 2.416 & 0.882 \\
			& $\sigma_a$ & 0.666 & 0.647 & \sethlcolor{green}\hl{\textbf{0.708}} & 0.705 & 0.635 & 0.674 & -- & 0.670 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$ & -1.344 & -1.286 & -1.165 & -1.163 & -1.263 & -1.167 & -- & \sethlcolor{green}\hl{\textbf{-1.152}} \\
			\hline
			\multirow[t]{4}{*}{\textbf{1.0}} & accuracy & 0.810 & \sethlcolor{green}\hl{\textbf{0.816}} & 0.810 & 0.810 & 0.713 & 0.808 & 0.279 & 0.811 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & 0.455 & 0.436 & \sethlcolor{green}\hl{\textbf{0.429}} & 0.440 & 0.799 & 0.458 & 2.826 & 0.442 \\
			& $\sigma_a$ & 0.771 & 0.783 & 0.813 & \sethlcolor{green}\hl{\textbf{0.813}} & 0.702 & 0.767 & -- & 0.756 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$ & -1.143 & -0.988 & -0.848 & -0.853 & -1.149 & -0.863 & -- & \sethlcolor{green}\hl{\textbf{-0.846}} \\
			\hline
			\multirow[t]{4}{*}{\textbf{1.5}} & accuracy & \sethlcolor{green}\hl{\textbf{0.866}} & 0.862 & 0.863 & 0.864 & 0.752 & 0.865 & 0.232 & 0.863 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & \sethlcolor{green}\hl{\textbf{0.260}} & 0.269 & 0.269 & 0.261 & 0.655 & 0.266 & 3.085 & 0.270 \\
			& $\sigma_a$ & 0.835 & 0.823 & 0.863 & \sethlcolor{green}\hl{\textbf{0.866}} & 0.745 & 0.825 & -- & 0.815 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$ & -0.883 & -0.918 & -0.671 & -0.670 & -1.128 & -0.662 & -- & \sethlcolor{green}\hl{\textbf{-0.655}} \\
			\hline
			\multirow[t]{4}{*}{\textbf{2.0}} & accuracy & 0.898 & 0.898 & 0.898 & \sethlcolor{green}\hl{\textbf{0.903}} & 0.772 & 0.900 & 0.195 & 0.900 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & 0.172 & 0.179 & 0.171 & \sethlcolor{green}\hl{\textbf{0.163}} & 0.606 & 0.172 & 3.229 & 0.169 \\
			& $\sigma_a$ & 0.876 & 0.847 & 0.899 & \sethlcolor{green}\hl{\textbf{0.901}} & 0.769 & 0.861 & -- & 0.856 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$ & -0.673 & -0.844 & -0.492 & -0.500 & -1.088 & -0.500 & -- & \sethlcolor{green}\hl{\textbf{-0.492}} \\
	\end{tabular}}
	\caption{\textbf{Results obtained from synthetic data with varying noise levels, represented by $\boldsymbol{\beta}$.} Each value is the mean of 4 independent realizations of the noisy model. The green highlighted values are the top performances for the considered metric. Notably, some of the values in the same row appear identical but only a single value is highlighted. The reason for this is that the highlighted value is better by less than three decimal places. \Cref{tb:sem_beta} contains the standard error of the above values. $\sigma_a$ and $\sigma_L$ cannot be applied to the W-L model, so there are no values for the metrics.}
	\label{tab:varying_noise}
\end{table}


% Figure environment removed


\subsection{Performance on Real Data}
\label{sec:resultsreal}

We consider a variety of real datasets of timestamped interactions, as described in \Cref{tb:datasets}. These datasets come from competitions in well-known sports such as soccer, basketball and chess. They are both relevant and relatable sources of information for our experiments. 

In soccer, we consider the Italian Serie A and the English Premier League (EPL). The Serie A data is from the period 1993--2016 and contains the results of thousands of  games between 47 teams. Similarly, the EPL contains results of thousands of games between 39 teams in the period 2006--2018. In contrast, the NBA dataset contains roughly three times the number of  EPL matches from 2010-2018 between 30 teams. All these 3 dataset can be found on \textit{kaggle.com}. Finally, the chess dataset is obtained from matches on \textit{lichess.org}. It contains 298 matches from 2014-2017. In all cases, $A^{t}_{ij}$ is the number of times team $i$ (or for chess, player $i$) beats $j$ in a given time-step $t$. The definition of a time-step varies from sport to sport (see below). 


\begin{table}[h!]
	\caption{Descriptions of the real datasets.}
	\label{tb:datasets}
	\vskip 0.15in
	\begin{center}
		\begin{small}
				\begin{tabular}{lllll}
					\toprule
					Competition	&   Type & $N_{teams}$  &  $N_{games}$ & $T_{steps}$\\
					\midrule
					NBA &       Basketball &  30 & 9594& 218 \\
					lichess.org &       Chess & 96  & 298& 90 \\
					Serie A                      &  Soccer &  47  &  5679 & 397 \\
					English Premier League (EPL) &  Soccer &  39  &  3396 & 114 \\
					\bottomrule
				\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

As with synthetic data, we found that \dsr\ outperforms the other algorithms in terms of the most top performances across our four different metrics (\Cref{tab:real_data_results}). Elo and WHR are the next best performers: Elo does slightly better in a few cases on the accuracy or agony metric for NBA and chess, and as in the synthetic data WHR does well for the $\sigma_L$ metric, the conditional log-likelihood of generating directed edges (outcomes) given their existence.


\begin{table}[h!]
	\resizebox{\linewidth}{2.5cm}{	
		\begin{tabular}{cc|cccccccccc}
			{\textbf{Dataset}} & {\textbf{Metric}} & {\textbf{Elo}}  & {\textbf{\nmdsr}} & {\textbf{\mwsr}} & {\textbf{\dsr}} & {\textbf{SR}} & {\textbf{TS}} & {\textbf{WHR}} & {\textbf{W-L}} \\
			\hline
			\multirow[t]{4}*{\textbf{NBA}}  & accuracy & \sethlcolor{green}\hl{\textbf{0.650}} & 0.642 & 0.637 & 0.649 & 0.607 & 0.645 & 0.648 & 0.564 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & \sethlcolor{green}\hl{\textbf{2.981}} & 3.048 & 3.084 & 2.987 & 3.568 & 3.006 & 2.997 & 4.070 \\
			& $\sigma_a$ & 0.579 & 0.562 & 0.639 & \sethlcolor{green}\hl{\textbf{0.646}} & 0.596 & 0.584 & 0.580 & --   \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$ & -1.426 & -1.331 & -1.266 & -1.256 & -1.324 & -1.280 & \sethlcolor{green}\hl{\textbf{-1.255}} & --   \\
			\hline
			\multirow[t]{4}{*}{\textbf{Chess}} & accuracy & \sethlcolor{green}\hl{\textbf{0.526}} & 0.477 & 0.422 & 0.513 & 0.521 & 0.500 & 0.481 & 0.099 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & 8.253 & 11.485 & 9.325 & 8.318 & 7.925 & 7.133 & 6.946 & \sethlcolor{green}\hl{\textbf{0.641}} \\
			& $\sigma_a$ & 0.615 & 0.580 & 0.625 & \sethlcolor{green}\hl{\textbf{0.651}} & 0.581 & 0.635 & 0.628 & --   \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$ & -1.290 & -1.341 & -1.332 & -1.333 & -1.550 & -1.246 & \sethlcolor{green}\hl{\textbf{-1.200}} & --   \\
			\hline
			\multirow[t]{4}{*}{\textbf{EPL}} & accuracy & 0.678 & \sethlcolor{green}\hl{\textbf{0.681}} & 0.669 & 0.675 & 0.679 & 0.672 & 0.673 & 0.609 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & 3.239 & 4.144 & 4.141 & \sethlcolor{green}\hl{\textbf{3.147}} & 3.401 & 3.797 & 3.825 & 5.386 \\
			& $\sigma_a$ & 0.595 & 0.530 & 0.669 & \sethlcolor{green}\hl{\textbf{0.675}} & 0.662 & 0.601 & 0.598 & --   \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$ & -1.285 & -1.357 & -1.208 & \sethlcolor{green}\hl{\textbf{-1.184}} & -1.206 & -1.211 & -1.199 & --   \\
			\hline
			\multirow[t]{4}{*}{\textbf{Serie A}} & accuracy & 0.655 & 0.652 & 0.629 & 0.653 & \sethlcolor{green}\hl{\textbf{0.663}} & 0.655 & 0.653 & 0.543 \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& agony & 4.296 & 5.800 & 6.278 & \sethlcolor{green}\hl{\textbf{4.041}} & 4.241 & 5.669 & 5.653 & 8.021 \\
			& $\sigma_a$ & 0.582 & 0.530 & 0.628 & \sethlcolor{green}\hl{\textbf{0.652}} & 0.647 & 0.590 & 0.585 & --   \\
			\rowcolor[gray]{0.95}[\tabcolsep][\tabcolsep]
			& $\sigma_L$  & -1.363 & -1.357 & -1.287 & -1.240 & -1.269 & -1.257 & \sethlcolor{green}\hl{\textbf{-1.237}} & --   \\
	\end{tabular}}
	\caption{\textbf{Results obtained from real data.} The green highlighted values are the top performances for the considered metric. Notably, some of the values in the same row appear identical but only a single value is highlighted. The reason for this is that the highlighted value is better by less than three decimal places. \Cref{tb:sem_real} contains the standard error of the above values.  $\sigma_a$ and $\sigma_L$ cannot be applied to the W-L model hence there are no values for the metrics.}
	\label{tab:real_data_results}	
\end{table}

Perhaps surprisingly, static SpringRank performs well on both the Serie A and chess datasets, achieving the highest accuracy on Serie A. For Serie A, this could, in part, be explained by the fact that the dataset has larger time gaps between matches, implying less of a connection between time-steps, hence time may not play as much of a role in the Serie A as it does in the NBA dataset.

For the chess dataset, each time-step represents a day of matches, but match days are not necessarily consecutive. For example, the first day of matches is 2014-03-04 and the second is 2015-11-15. Again, this poses the problem of large gaps in time which could lessen the connection between time-steps. 

As such, in both the Serie A and chess datasets, it is understandable that the static version of SpringRank would perform fairly well as time-steps do not influence each other as much as in, for example, the NBA dataset. This is further supported by the closeness in results between the static version of SpringRank and the dynamic models on the soccer and chess datasets. In contrast, the gap of results from  the NBA  dataset between the aforementioned static and dynamic models is larger. We discuss the influence of time further in \Cref{sec:dynamicity}. (The Serie A and chess datasets might also be suitable for the model described above where time intervals between snapshots can vary; we leave this for future work.)

Overall, we observe a fairly broad distribution of values for the various metrics over the cross-validation trials, as there are matches that are more difficult to predict than others. Hence, we take a closer look by analyzing a fold-by-fold performance comparison, where we assess the number of test sets in which one algorithm outperforms the others. We find that \dsr\ performs equal to or better than the other algorithms in most cases on the NBA dataset, and in all cases when compared to Elo and WHR in terms of $\sigma_{a}$ (\Cref{fig:test_comparison}).

We observed qualitative differences of the inferred ranks in \Cref{fig:nba_scores_over_time} similar to those observed in  \Cref{fig:synscores} for synthetic data. W-L infers ranks that change with a much higher frequency than the others. While smoother, the ranks inferred by TS show more frequent variations than \dsr\ and Elo, which infer similarly behaving ranks.  

% Figure environment removed

% Figure environment removed


\subsection{The Relevance of Time}\label{sec:dynamicity}

As a final consideration, we turn to a fundamental question: given a dataset of timestamped interactions, does their chronological order matter? If the answer is positive, then we should use a dynamical ranking algorithm to analyze the data. If not, a simpler static algorithm should be enough.

One way to assess whether a given dataset is better modeled by a dynamical or static algorithm is by randomly permuting the order of interactions---but not their outcomes---and thus removing any relationship between ranks and time. If an algorithm performs significantly better on the original data than on the permuted data, this shows that the order matters and a dynamical model is justified. To be more precise, applying random permutations to the data produces a distribution of any test statistic, including any measure of the performance of an algorithm that predicts which way a given interaction will go (e.g., which of two players will win a chess match, conditioned on the event that they play). If the performance on the original data is far out in the tail of this distribution, we can reject the null hypothesis that the time-steps are simply independent draws from a static model.

We run this permutation test first on synthetic data, confirming as expected that the dynamical model performs significantly better on synthetic data generated with the time-varying model introduced in \Cref{sec:genmod}, provided that the hierarchy itself is sufficiently strong (\Cref{fig:histogram_varying_noise}). However, when the hierarchy is weak (i.e., $\beta$ is small), the ranks have little relationship to the outcomes, and treating the ranks dynamically is no longer justified by the permutation tests (\Cref{fig:histogram_varying_noise}).

For NBA data, permutation tests show that chronological order matters, and that using a dynamical model significantly improves prediction (\Cref{fig:histogram_nba}). However, for the soccer and chess datasets, we find mixed results depending on the test statistic. For instance, the ``agony'' (a measure which penalizes the model for interactions $i \to j$ if $s_j-s_i$ is large) suggests that time-order is relevant, while the accuracy (the fraction of interactions whose direction is correctly predicted) is less sensitive to this information (\Cref{fig:histogram_real_extra}, \Cref{tb:pvalues}). While the most straightforward explanation is that NBA rankings are more time-varying, while soccer and chess are less so, we also note that there are many more games in a NBA season than in a soccer season, since there are more teams and more frequent games in the NBA, therefore allowing our simple permutation test to reject the null hypothesis more easily with more available data to differentiate time-varying versus static ranks (\Cref{tb:datasets}).

% Figure environment removed


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
