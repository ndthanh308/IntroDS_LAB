\section{Introduction}

\IEEEPARstart{F}{ederated Learning} (FL) is a machine learning setting, which aims to collaboratively train machine learning models with the participation of several clients under the orchestration of a central server in a privacy-preserving and communication efficient manner~\cite{mcmahan2017communication, kairouz2021advances}. FL has seen a surge of interest in the machine learning research community in recent years, thanks to its potential to improve the performance of edge users without compromising data privacy. This innovative approach has been successfully applied to a wide range of tasks, including image classification, natural language processing, and more~\cite{kairouz2021advances, yang2018applied, sui2020feded, muhammad2020fedfast, dayan2021federated}.

The ultimate goal of standard (global) FL is to train a shared global model which uniformly performs well over almost the entire participating clients. However, the inherent diversity and not independent and identical (Non-IID) distribution of clients' local data \textcolor{red}{has} made the global FL approach very challenging~\cite{haddadpour2019convergence, li2019feddane, zhao2018federated, hsieh2020non, li2020federated, sattler2020clustered}. Indeed, clients' incentives to participate in FL can be to derive personalized models rather than learning a shared global model. This client-centric perspective along with the challenges in the global FL approach under Non-IID data distributions has motivated an alternative personalized FL approach. Personalized FL aims to learn personalized models performing well according to the distinct data distribution of each participating client.
%Personalized FL has become one of the rapidly growing topics in this community and directly driven a significant amount of work.

%With the ambitions of deriving practical insights for the broader federated machine learning research community, we surveyed 200 papers on FL with Non-IID data and implemented 40 different algorithms in this work. For example: under what experimental settings personalized FL approach is more reasonably motivated than global FL? which technique achieves the best trade-off in terms of accuracy, communication cost, fairness, and generalization to newcomers? which experimental design settings are effective for a fair comparison in FL?

%Despite the existence of several consistent notions, for example, personalized FL performs better than global FL in Non-IID settings, and local training performs the best under extreme Non-IID distributions, we primarily find the state of the existing literature is such that our motivating questions cannot be answered. Few papers provide comparison to other methods under the same settings, and the methodologies and implementations are so incompatible between papers that thorough comparisons are impossible to achieve. For example, many (XX) papers compare to others from only accuracy performance under modified settings to match their methodologies and do not compare from other aspects, e.g. communication cost, fairness, generalization to newcomers. most of the papers create their experimental setup to match their assumptions and present the superiority of their algorithm (and present their exagerated best results under the created settings). In addition, evaluation metrics, hyperparameters, and other confounders are diverse or not mentioned, and there is no consensus about a comprehensive experimental setup.

Despite the significant \textcolor{red}{number} of works that have been done in both global and personalized FL approaches under data heterogeneity, the state of progress is not well understood in the FL community. In particular, the following key questions have remained unanswered in the existing literature: what factors affect experimental results and how to control them? Which experimental design setups are effective and well-incentivized for a fair comparison in each FL approach? What are the best practices and remedies to compare different methods and avoid evaluation failure? We primarily find that the methodologies, experimental setups, and evaluation metrics are so inconsistent between papers that a comprehensive comparison is impossible. For example, some papers consider a specific FL approach, however, they use an experimental setup \textcolor{red}{that} is not well-incentivized or has been created to match their assumptions and \textcolor{red}{is} not applicable to other cases. Moreover, the existence of numerous benchmarks and inconsistent implementation environments together with different confounding variables such as data augmentation and pre-processing techniques, choice of \textcolor{red}{the} optimizer, and learning rate schedule have made such comparison even more difficult.

% Despite the significant amount of works that have been done in both global and personalized FL approaches under data heterogeneity, the effect of experimental design settings has been 

%For example, few papers provide comparison to other methods under the same settings, and many create their experimental setup to match their unique methodology and assumptions, thus failing to control the effect of other important FL specific variables in their results. This clearly hurts fair comparisons.

%Most of the mentioned issues come from the absence of a comprehensive study of different FL experimental variables in relation to each other and performance results and existence of suggestions for a unified and well-incentivized experimental setup for each FL approach. To address these issues, we provide the first comprehensive study to the best of our knowledge on FL-specific experimental variables, bring many insights and identify the best practices for a meaningful and well-incentivized FL experimental design for each FL approach. We also release FedZoo-Bench, an open-source library based on PyTorch which provides a comprehensive set of standardized and customizable features, standardized implementation and evaluation of more that 17 SOTA methods under a unified setting, different Non-IID data partitioning, and a wide variety of datasets and architectures. FedZoo-Bench facilitates a more reproducible and comparable FL research. Furthermore, we conduct a comprehensive comparison of 17 different SOTA methods in terms of performance, fairness and generalization to newcomers to better understand the promises and limitations of the exisitngs methods. 

%The current state of FL research is hindered by a lack of comprehensive studies on experimental variables and a lack of suggestions for a unified and well-incentivized experimental setup for each FL approach. 

To address the mentioned issues in the current state of FL research, we present the first comprehensive study, to the best of our knowledge, on FL-specific experimental variables and provide new insights and best practices for a meaningful and well-incentivized FL experimental design for each FL approach. We also introduce FedZoo-Bench, an open-source library based on PyTorch that provides a commonly used set of standardized and customizable features in FL, and implementation of 22 state-of-the-art (SOTA) methods under a unified setting to make FL research more reproducible and comparable. Finally, we present a comprehensive evaluation of several SOTA methods in terms of performance, fairness, and generalization to newcomers, to provide a clear understanding of the promises and limitations of existing methods.

% We also introduce FedZoo-Bench, an open-source library based on PyTorch that provides a commonly used set of standardized and customizable features in FL, implementation and evaluation of 22 state-of-the-art (SOTA) methods under a unified setting, different Non-IID data partitionings, and a wide variety of datasets and architectures to make FL research more reproducible and comparable.

% In this paper, we present a thorough examination of key variables that influence the success of
% federated learning experiments. We provide new insights and analysis of these variables in relation
% to each other and performance results, and propose recommendations and best practices for a
% meaningful and well-incentivized FL experimental design. To further aid the FL community in
% understanding the current state of the field, we have developed FedZoo-Bench, an open-source library
% based on PyTorch that provides a comprehensive set of standardized and customizable features,
% different evaluation metrics, and pre-implementation of more than 17 state-of-the-art methods. We
% also present a comprehensive evaluation of performance, fairness, and generalization to newcomers
% of these SOTA methods using FedZoo-Bench. We hope that our work will help the FL community
% to better understand the state of progress in the field.

%determine specific bottlenecks, standardize evaluation metrics, recommend best experimental settings/practices and propose FedZooBench, a comprehensive benchmark for standardized evaluation of global and personalized FL with Non-IID data. FedZooBench provides implementation of more than 40 different algorithms, 8 different Non-IID data partitioning, and a wide variety of datasets and architectures to pave the way for to adhere to the best experimental setings/practices we identify. Most of the problems and confusions come from the fact that there is a lack of comprehensive study on experimental design variables and their effects on the results in FL. To the best of our knowledge, we are the first comprehensive study on FL experimental design variables and proposing the best settings for a fair comparison that we identify. Other papers, fail to control the effect of variables in their results which hurts fair comparisons. Without recognizing effective regions and fl variables all work will be in vain. 

% Despite a significant amount of work in personalized FL, there remains a lack of consensus about which
% methods perform best in various federated settings, let alone the degree to which personalization itself is really
% necessary in practice. For example, many works target a specific form of personalization (e.g., clustering)
% in their methodology, and then artifically create or modify data to match this assumption (e.g., manually
% clustering the data) before testing the efficacy of their approach [69]. While this is a reasonable sanity check,
% it fails to validate how impactful such approaches are for real-world FL applications. In particular, a major
% issue is that while ‘heterogeneity’ is believed to be a natural occurrence in federated networks, there is a lack of benchmarks that reflect real applications of FL, making it hard to understand the prevalence and
% magnitude of heterogeneity in practice.
% To address these concerns, we propose Motley, the first comprehensive benchmark for personalized and
% heterogeneity-aware FL. The benchmark includes a suite of four cross-device and three cross-silo datasets, as
% well as baseline personalization methods evaluated on this data. The datasets themselves are drawn from
% real-world applications mirroring federated settings in an effort to better reflect naturally occurring forms
% of heterogeneity. In developing baselines for the benchmark, we pay careful attention to the evaluation of
% personalization and heterogeneity—making concrete suggestions for future work in evaluating the impact of
% heterogeneity/personalization. Finally, although we focus specifically on the application of FL, we note that
% the benchmark is also a useful tool for the areas of transfer learning, meta learning, and multi-task learning
% more generally, as techniques from these areas are commonly used for personalized FL. The remainder of the
% paper is organized as follows:

%meta-analysis of the neural network pruning litera-
% ture based on comprehensively aggregating reported re-
% sults from 81 papers.
% 2. A catalog of problems in the literature and best prac-
% tices for avoiding them. These insights derive from an-
% alyzing existing work and pruning hundreds of models.
% 3. ShrinkBench, an open-source library for evaluating
% neural network pruning methods available at

\textbf{Contributions.} Our study makes the following key contributions:
\begin{itemize}
\item We conduct the first comprehensive analysis of FL experimental design variables by running extensive experiments and provide new insights and best practices for a meaningful and well-incentivized experimental setup for each FL approach.
\item We introduce FedZoo-Bench, an open-source library for implementation and evaluation of FL algorithms under data heterogeneity, \textcolor{red}{consisting of the} implementation of 22 SOTA algorithms, available at~\url{https://github.com/MMorafah/FedZoo-Bench}.
\item Using FedZoo-Bench, we conduct a comprehensive evaluation of several SOTA algorithms in terms of performance, fairness, and generalization to newcomers to form a clear understanding of the strengths and limitations of existing methods.
\end{itemize}

% \textbf{Contributions.} We provide a summery of our contributions bellow:
% \begin{itemize}
%     \item We provide the first comprehensive study to the best of our knowledge on FL-specific experimental design variables by running more than XX different experiments.
%     \item We identify insights and recommendations for a more meaningful and well-incentivized experimental setup for each FL approach.
%     \item We develop FedZoo-Bench, an open-source library for implementation and evaluation of FL algorithms under data heterogeneity containing implementations of more than XX different algorithms available at XX.
%     \item We utilize the best practices that we identify and experimentally evaluate 17 different state-of-the-art (SOTA) algorithms under a common configurations in terms of performance, fairness and generalization to newcomers to better understand the state of progress for each FL approach.
    
%     %\item Identify and agrregate different reported methodologies and protocols in federated learning with Non-IID data literature based on our study of 200 papers. 
%     %\item We provide a list of experimental inconsistencies in the literature, insights and best protocols to avoid them.
% \end{itemize}

\textbf{Organization.} The rest of the paper is organized as follows. In Section~\ref{related-works} we bring a concise literature review. In Section~\ref{background} we provide the background for each FL approach and statistical heterogeneity. In Section~\ref{study} we provide our comprehensive study on FL-specific experimental variables. In Section~\ref{recommendation} we \textcolor{red}{discuss our recommendations.} In Section~\ref{fedzoo} we introduce FedZoo-Bench and compare 17 different algorithms. In Section~\ref{sec:conclusion} we conclude and provide future works.