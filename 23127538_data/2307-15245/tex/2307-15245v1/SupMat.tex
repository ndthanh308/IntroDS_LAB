\textbf{Organization.} We organize the supplementary materials as follow: 
\begin{itemize}
    \item In Section~\ref{app:sec-additional}, we present addition experimental results to complete our analysis in Section~\ref{study} of the main paper.
    \item In Section~\ref{app:recommendation}, we discuss an experimental checklist to facilitate an easier comparison of FL methods.
    \item In Section~\ref{sec:app-fedzoo}, we provide more details about the available algorithms, datasets, architectures and data partitionings in FedZoo-Bench.
    %\item In Section~\ref{sec:app-hyperparameters}, we brings the details of the hyperparameters we used for each method in our experiments of Section~\ref{sec:comparison}. 
\end{itemize}
\section{Additional Results}\label{app:sec-additional}
%In this section we present additional experimental results for completeness of our study.
\subsection{Globalization and Personalization Incentives} \label{appendix:incentives}
The additional results in this part complements the results discussed in Section~\ref{sec:heterogeneity-localepoch}. Comparing Figures~\ref{app:fig-incentive-dir} and~\ref{app:fig-incentive-skew} further corroborates our finding mentioned in Section~\ref{sec:heterogeneity-localepoch} that Non-IID Label Skew partitioning has higher incentive for personalization compared to the other type of heterogeneity. Moreover, increase of local epochs has incentivized personalization more for both types of heterogeneity.
% Figure environment removed

% Figure environment removed

\begin{comment}
\subsection{Experimental Settings}


\begin{table}[h]
\caption{Hyper-parameters used for each figure}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{c|cccccccccccc}
\toprule
Figure           & Dataset & Architecture & clients & sample rate & epochs & communication rounds & optimizer & learning rate & momentum  & partitioning & mics \\
\midrule
Figure 1 & XX & XX & XX & XX & XX & XX & XX & XX & XX & XX & XX \\
\midrule
Figure 1 & XX & XX & XX & XX & XX & XX & XX & XX & XX & XX & XX \\
\midrule
Figure 1 & XX & XX & XX & XX & XX & XX & XX & XX & XX & XX & XX \\
\midrule
Figure 1 & XX & XX & XX & XX & XX & XX & XX & XX & XX & XX & XX \\
\midrule

\bottomrule
\end{tabular}
}
\label{tab:hyperparameters-figs-tabs}
\end{table}
\end{comment}

\section{Experimental Checklist} \label{app:recommendation}
To facilitate an easier comparison between FL methods in future studies, we recommend the following checklist:
\begin{itemize}
    \item Make sure that the used experimental setting is meaningful and well-incentived for the considered FL approach. 
    \item State the exact setting including local epochs, sample rate, number of clients, type of data partitioning, level of heterogeneity, communication rounds, dataset, architecture, evaluation metrics, any pre-processing on the dataset, any learning rate scheduling if used, and initialization.
    \item Report the average results over at least $3$ independent and different runs.
    \item Mention the hyperparameters used to obtain the results.
\end{itemize}

\section{FedZoo-Bench}\label{sec:app-fedzoo}
We introduced FedZoo-Bench in Section~\ref{fedzoo}. In this section we provide more details about the available features in FedZoo-Bench. For more information on FedZoo-Bench's implementation and use cases for different settings, refer to the project's documentation at~\url{https://github.com/MMorafah/FedZoo-Bench}.
\subsection{Available Baselines}
\begin{itemize}
    \item \textbf{Global FL} (7 algorithms)
    \begin{itemize}
        \item FedAvg~\cite{mcmahan2017communication}
        \item FedProx~\cite{li2020federated}
        \item FedNova~\cite{wang2020tackling}
        \item Scaffold~\cite{karimireddy2020scaffold}
        \item FedDF~\cite{lin2020ensemble}
        \item MOON~\cite{li2021model}
        \item FedBN~\cite{li2021fedbn}
    \end{itemize}
    \item \textbf{Personalized FL} (15 algorithms)
    \begin{itemize}
        \item FedAvg + FT~\cite{jiang2019improving}
        \item LG-FedAVg~\cite{liang2020think}
        \item PerFedAvg~\cite{fallah2020personalized}
        \item FedPer~\cite{arivazhagan2019federated}
        \item FedRep~\cite{collins2021exploiting}
        \item Ditto~\cite{li2021ditto}
        \item APFL~\cite{deng2020adaptive}
        \item IFCA~\cite{ghosh2020efficient}
        \item SubFedAvg~\cite{vahidian2021personalized}
        \item pFedMe~\cite{t2020personalized}
        \item CFL~\cite{sattler2020clustered}
        \item PACFL~\cite{vahidian2022efficient}
        \item MTL~\cite{smith2017federated}
        \item FedEM~\cite{marfoq2021federated}
        \item FedFOMO~\cite{zhang2020personalized}
    \end{itemize}
\end{itemize}
Additionally, FedZoo-Bench can be easily used for other variations of FedAvg~\cite{reddi2020adaptive} and different choice of optimizers.

\subsection{Available Datasets}
\begin{itemize}
    \item MNIST~\cite{mnist}
    \item CIFAR-10~\cite{cifar}
    \item CFIAR-100
    \item USPS~\cite{usps}
    \item SVHN~\cite{svhn}
    \item CelebA~\cite{celeba}
    \item FMNIST~\cite{fmnist}
    \item FEMNIST~\cite{femnist}
    \item Tiny-ImageNet~\cite{tiny}
    \item STL-10~\cite{stl-10}
\end{itemize}

\subsection{Available Architectures}
\begin{itemize}
    \item MLP as in FedAvg \cite{mcmahan2017communication}
    % should we give a specific architecture of mlp?
    \item LeNet-5 \cite{lenet}
    \item ResNet Family \cite{resnet}
    \item ResNet-50
    % I think resnet-50 is more like part of resnet family than resnet-9. Maybe we should put resnet-9 here?
    \item VGG Family \cite{vgg}
\end{itemize}

\subsection{Available Data Partitionings}
\begin{itemize}
    \item IID
    \item Non-IID Label Dir~\cite{hsu2019measuring}
    \item Non-IID Label Skew~\cite{li2021federated}
    \item Non-IID Random Shard~\cite{mcmahan2017communication}
    \item Non-IID Quantity Skew~\cite{li2021federated}
\end{itemize}

\begin{comment}
\section{Hyperparameters}\label{sec:app-hyperparameters}
In this section, we detail the hyperparameters used for each algorithm for our experiments in Section~\ref{sec:comparison}.
\subsection{Architectures}
Tables \ref{tab:lenet5} and \ref{tab:resnet9} summarize LeNet-5 and ResNet-9 architectures we used in the experiments, respectively.
\subsection{Global FL}
\subsubsection{Setting (gFL \#1)}
We fix local batch size to $10$ and use SGD optimizer for all the baselines. Herein, we detail the hyperparameter used for each method:
\begin{itemize}
    \item FedAvg~\cite{mcmahan2017communication}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
    \end{itemize}
    \item FedProx~\cite{li2020federated}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\mu=0.001}$
    \end{itemize}
    \item FedNova~\cite{wang2020tackling}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
    \end{itemize}
    \item Scaffold~\cite{karimireddy2020scaffold}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.001}$
        \item $\mathtt{momentum=0.9}$
    \end{itemize}
    \item MOON~\cite{li2021model}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{use\_project\_head=false}$
        \item $\mathtt{\tau=0.5}$
        \item $\mathtt{\mu=0.1}$
    \end{itemize}
    \item FedDF~\cite{lin2020ensemble}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{distillation\_dataset=CIFAR{-}100}$
        \item $\mathtt{distillation\_T=1}$
        \item $\mathtt{distillation\_epoch=1}$
        \item $\mathtt{distillation\_batch\_size=128}$
        \item $\mathtt{distillation\_optimizer=Adam}$
        \item $\mathtt{distillation\_learning\_rate=0.0001}$
    \end{itemize}
\end{itemize}
\subsubsection{Setting (gFL \#2)}
We fix local batch size to $64$ and use SGD optimizer for all the baselines. Herein, we detail the hyperparameter we used for each method:
\begin{itemize}
    \item FedAvg~\cite{mcmahan2017communication}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
    \end{itemize}
    \item FedProx~\cite{li2020federated}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\mu=0.001}$
    \end{itemize}
    \item FedNova~\cite{wang2020tackling}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
    \end{itemize}
    \item Scaffold~\cite{karimireddy2020scaffold}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
    \end{itemize}
    \item MOON~\cite{li2021model}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{use\_project\_head=false}$
        \item $\mathtt{\tau=0.5}$
        \item $\mathtt{\mu=0.1}$
    \end{itemize}
    \item FedDF~\cite{lin2020ensemble}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{distillation\_dataset=tiny\_imagenet}$
        \item $\mathtt{distillation\_T=1}$
        \item $\mathtt{distillation\_epoch=1}$
        \item $\mathtt{distillation\_batch\_size=128}$
        \item $\mathtt{distillation\_optimizer=Adam}$
        \item $\mathtt{distillation\_learning\_rate=0.0001}$
    \end{itemize}
\end{itemize}
\subsection{Personalized FL}
\subsubsection{Setting (pFL \#1)}
We fix local batch size to $10$ and use SGD optimizer for all the baselines. Herein, we detail the hyperparameter we used for each method:
\begin{itemize}
    \item FedAvg + FT~\cite{jiang2019improving}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{ft\_epoch=20}$
    \end{itemize}
    \item LG-FedAvg~\cite{liang2020think}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{global\_layers=last\_two\_layers}$
    \end{itemize}
    \item PerFedAvg~\cite{fallah2020personalized}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\beta=0.001}$
    \end{itemize}
    \item FedPer~\cite{arivazhagan2019federated}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{global\_layers=except\_last\_two\_layers}$
    \end{itemize}
    \item FedRep~\cite{collins2021exploiting}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{global\_layers=except\_last\_two\_layers}$
    \end{itemize}
    \item Ditto~\cite{li2021ditto}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\lambda=0.8}$
    \end{itemize}
    \item APFL~\cite{deng2020adaptive}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\alpha=0.75}$
    \end{itemize}
    \item IFCA~\cite{ghosh2020efficient}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{nclusters=2}$
    \end{itemize}
    \item SubFedAvg~\cite{vahidian2021personalized}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{pruning\_percent=5}$
        \item $\mathtt{pruning\_target=50}$
        \item $\mathtt{dist\_thresh=0.0001}$
        \item $\mathtt{acc\_thresh=55}$
    \end{itemize}
    \item pFedMe~\cite{t2020personalized}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{K=5}$
        \item $\mathtt{\lambda=15}$
        \item $\mathtt{personal\_lr=0.09}$
    \end{itemize}
    \item CFL~\cite{sattler2020clustered}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\epsilon_1=0.4}$
        \item $\mathtt{\epsilon_2=1.6}$
    \end{itemize}
    \item PACFL~\cite{vahidian2022efficient}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
    \end{itemize}
\end{itemize}

\subsubsection{Setting (pFL \#2)}
We fix local batch size to $10$ and use SGD optimizer for all the baselines. Herein, we detail the hyperparameter we used for each method:
\begin{itemize}
    \item FedAvg + FT~\cite{jiang2019improving}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{ft\_epoch=20}$
    \end{itemize}
    \item LG-FedAvg~\cite{liang2020think}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{global\_layers=last\_two\_layers}$
    \end{itemize}
    \item PerFedAvg~\cite{fallah2020personalized}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\beta=0.001}$
    \end{itemize}
    \item FedPer~\cite{arivazhagan2019federated}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{global\_layers=except\_last\_two\_layers}$
    \end{itemize}
    \item FedRep~\cite{collins2021exploiting}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{global\_layers=except\_last\_two\_layers}$
    \end{itemize}
    \item Ditto~\cite{li2021ditto}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\lambda=0.8}$
    \end{itemize}
    \item APFL~\cite{deng2020adaptive}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\alpha=0.75}$
    \end{itemize}
    \item IFCA~\cite{ghosh2020efficient}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{nclusters=2}$
    \end{itemize}
    \item SubFedAvg~\cite{vahidian2021personalized}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{pruning\_percent=5}$
        \item $\mathtt{pruning\_target=50}$
        \item $\mathtt{dist\_thresh=0.0001}$
        \item $\mathtt{acc\_thresh=55}$
    \end{itemize}
    \item pFedMe~\cite{t2020personalized}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{K=5}$
        \item $\mathtt{\lambda=15}$
        \item $\mathtt{personal\_lr=0.09}$
    \end{itemize}
    \item CFL~\cite{sattler2020clustered}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
        \item $\mathtt{\epsilon_1=0.4}$
        \item $\mathtt{\epsilon_2=1.6}$
    \end{itemize}
    \item PACFL~\cite{vahidian2022efficient}
    \begin{itemize}
        \item $\mathtt{learning\_rate=0.01}$
        \item $\mathtt{momentum=0.9}$
    \end{itemize}
\end{itemize}

 \begin{table}[htbp]
\footnotesize
\centering
\caption{The details for LeNet-5 architecture.}
\begin{tabular}{l|l}
\toprule
\multicolumn{1}{l|}{\textbf{Layer}} & \multicolumn{1}{c}{\textbf{Details}} \\
\midrule
\multirow{3}{*}{layer 1} & Conv2d(i=3, o=6, k=(5, 5), s=(1, 1)) \\
                         & ReLU() \\
                         & MaxPool2d(k=(2, 2)) \\
\midrule
\multirow{3}{*}{layer 2} & Conv2d(i=6, o=16, k=(5, 5), s=(1, 1)) \\
                         & ReLU() \\
                         & MaxPool2d(k=(2, 2)) \\
\midrule
\multirow{2}{*}{layer 3} & Linear(i=400 (256 for FMNIST), o=120) \\
                         & ReLU() \\
\midrule
\multirow{2}{*}{layer 4} & Linear(i=120, o=84) \\
                         & ReLU() \\
\midrule
layer 5    & Linear(i=84, o=10 (100 for CIFAR-100, and 40 for Mix-4)) \\
\bottomrule
\end{tabular}
\label{tab:lenet5}
\end{table}

\begin{table}[htbp]
\footnotesize
\centering
\caption{The details for ResNet-9 architecture.}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{l|l|l}
\toprule
\multicolumn{1}{l|}{\textbf{Block}} & \multicolumn{1}{c|}{\textbf{Details}} & \multicolumn{1}{l}{\textbf{Input}} \\
\midrule
\multirow{3}{*}{block 1}    & Conv2d(i=3, o=64, k=(3, 3), s=(1, 1))    & \multirow{3}{*}{image} \\
                            & BatchNorm(64)                    & \\
                            & ReLU()                                   & \\
\midrule
\multirow{4}{*}{block 2}    & Conv2d(i=64, o=128, k=(3, 3), s=(1, 1))  & \multirow{4}{*}{block 1} \\
                            & BatchNorm(128)                   & \\
                            & ReLU()                                   & \\
                            & MaxPool2d(k=(2, 2))                      & \\
\midrule
\multirow{6}{*}{block 3}    & Conv2d(i=128, o=128, k=(3, 3), s=(1, 1)) & \multirow{6}{*}{block 2} \\
                            & BatchNorm(128)                   & \\
                            & ReLU()                                   & \\
                            & Conv2d(i=128, o=128, k=(3, 3), s=(1, 1)) & \\
                            & BatchNorm(128)                   & \\
                            & ReLU() \\
\midrule
\multirow{4}{*}{block 4}    & Conv2d(i=128, o=256, k=(3, 3), s=(1, 1)) & \\ %\multirow{4}{*}{block 2 + block 3} \\
                            & BatchNorm(256)                   & block 2 + \\
                            & ReLU()                                   & block 3 \\
                            & MaxPool2d(k=(2, 2))                      & \\
\midrule
\multirow{4}{*}{block 5}    & Conv2d(i=256, o=512, k=(3, 3), s=(1, 1)) & \multirow{4}{*}{block 4} \\
                            & BatchNorm(512)                   & \\
                            & ReLU()                                   & \\
                            & MaxPool2d(k=(2, 2))                      & \\
\midrule
\multirow{6}{*}{block 6}    & Conv2d(i=512, o=512, k=(3, 3), s=(1, 1)) & \multirow{6}{*}{block 5} \\
                            & BatchNorm(512)                   & \\
                            & ReLU()                                   & \\
                            & Conv2d(i=512, o=512, k=(3, 3), s=(1, 1)) & \\
                            & BatchNorm(512)                   & \\
                            & ReLU() \\
\midrule
\multirow{2}{*}{classifier} & MaxPool2d(k=(4, 4))                      & block 4 + \\ %\multirow{2}{*}{block 4 + block 5} \\
                            & Linear(i=512, o=100)                     & block 5\\
\bottomrule
\end{tabular}
}
\label{tab:resnet9}
\end{table}

\end{comment}