\section{Literature Review}\label{related-works}
%There has been a growing interest in federated learning recently and the number of works are proliferating. In this section we provide a concise review of the FL literature.

\textbf{Global FL.} McMahan et al.~\cite{mcmahan2017communication} proposed the first method for global FL called FedAvg which simply averages the local models to update the server-side model. This FL approach mainly suffers from poor convergence and degradation of the results in \textcolor{red}{the} presence of data heterogeneity~\cite{tan2022towards, zhao2018federated}. Some works attempt to address these issues by regularizing local training. FedProx~\cite{li2020federated} uses an L2 regularizer on the weight difference between the local and global model. MOON~\cite{li2021model} utilizes contrastive learning to preserve global knowledge during local training. In FedDyn~\cite{acar2021federated}, an on\textcolor{red}{-}device dynamic regularization at each round has been used to align the local and global solutions. Another set of works \textcolor{red}{studies} the optimization issues of FedAvg in the presence of data heterogeneity and \textcolor{red}{proposes} alternative optimization procedures with better convergence guarantees~\cite{wang2020tackling, karimireddy2020scaffold, karimireddy2020mime, reddi2020adaptive, haddadpour2019convergence}. FedNova~\cite{wang2020tackling} addresses the objective inconsistency caused by heterogeneity in the local updates by weighting the local models in server-side averaging to eliminate bias in the solution and achieve fast error convergence. Scaffold~\cite{karimireddy2020scaffold}  proposes control variates to correct the local updates and eliminate the ``client drift'' which happens because of data heterogeneity resulting \textcolor{red}{in} convergence rate improvement. Other approaches have focused on proposing better model fusion techniques to improve performance~\cite{lin2020ensemble, wang2020federated, liu2022deep, sattler2021fedaux, gong2021ensemble}. FedDF~\cite{lin2020ensemble} adds a server-side KL training step after averaging local models by using the average of clients' logits on a public dataset. FedMA~\cite{wang2020federated} proposes averaging on the matched neurons of the models at each layer. GAMF~\cite{liu2022deep} formulates model fusion as a graph matching task by considering neurons or channels as nodes and weights as edges. For a more detailed review of methods on global FL literature, we recommend reading the surveys~\cite{mahlool2022comprehensive, kairouz2019advances}.

% There are some studies through bayesian learnig~\cite{}, hyper-networks~\cite{}, batch-normalization~\cite{} as well.

\textbf{Personalized FL.} The fundamental challenges of \textcolor{red}{the} global FL approach, such as poor convergence in the presence of data heterogeneity and lack of personalized solutions, have motivated the development of personalized FL. Personalized FL aims to obtain personalized models for participating clients. There are various efforts to solve this problem through different techniques. Multi-Task Learning (MTL) based techniques have been proposed in a number of works by considering clients as tasks and framing the personalized FL as an MTL problem~\cite{smith2017federated, t2020personalized, hanzely2020lower, li2021ditto, marfoq2021federated, dinh2021fedu}. Another group of studies \textcolor{red}{proposes} model interpolation techniques by mixing the global and local models~\cite{mansour2020three, deng2020adaptive, marfoq2022personalized}. There are also works that utilize representation learning techniques by decoupling parameters (or layers) into global and local parameters and then averaging only the global parameters with other clients~\cite{liang2020think, arivazhagan2019federated, collins2021exploiting, pillutla2022federated}. Additionally, there are some meta-learning\textcolor{red}{-}based works that attempt to obtain a global model with the capability of getting personalized fast by further local fine-tuning~\cite{jiang2019improving, fallah2020personalized, fallah2020personalized, chen2018federated, singhal2021federated}. Clustering-based methods have been also shown to be effective in several studies by grouping clients with similar data distribution to achieve better personalized models and faster convergence~\cite{ghosh2020efficient, briggs2020federated, lyu2022personalized, duan2021flexible, cho2021personalized, sattler2020clustered, vahidian2022efficient}. More recently, personalized FL has been realized with pruning-based techniques as well~\cite{li2020lotteryfl, vahidian2021personalized, li2021fedmask, bibikar2022federated, huang2022achieving}. For a more detailed review of methods on personalized FL literature, we recommend the surveys~\cite{kulkarni2020survey, zhu2021federated}.

%FedPer~\cite{arivazhagan2019federated} is very similar to LG-FedAvg with the difference that let clients to have different initializations for the local personalized layers.
%Clustering
%Fine-Tuning/Transfer Learning
%Pruning
%\textbf{FL Benchmarks.} Recently, there has been a growing interest in using federated learning for various applications on different platforms. To support this, several FL benchmarks and codebases have been developed, including FATE~\cite{liu2021fate}, FedML~\cite{he2020fedml}, PaddleFL~\cite{paddlefl}, FedLearner~\cite{fedlearner}, TFF~\cite{tff}, FLOWER~\cite{beutel2020flower}, FLUTE~\cite{dimitriadis2022flute}, FedTree~\cite{li2022fedtree}, Motley~\cite{wu2022motley}, PFL~\cite{chen2022pfl}, and NIID-Bench~\cite{li2021federated}. Many of available benchmarks support only global FL with limited customization for research.  

\textbf{FL Benchmarks.} Current FL benchmarks primarily focus on enabling various platforms and APIs to perform FL for different applications. They often only realize the global FL approach and implement basic algorithms (e.g. FedAvg)~\cite{liu2021fate, paddlefl, tff, fedlearner}, while other benchmarks,such as FLOWER~\cite{beutel2020flower}, FedML~\cite{he2020fedml}, FLUTE~\cite{dimitriadis2022flute} and NIID-Bench~\cite{li2021federated}, offer more customizable features, and implementation of more algorithms. For a more detailed review \textcolor{red}{of} the applicability and comparison \textcolor{red}{of} the existing benchmarks, we defer the reader to UniFed~\cite{liu2022unifed}.  While the majority of existing benchmarks are for global FL, there are \textcolor{red}{a} few recently released personalized FL benchmarks, including pFL-Bench~\cite{chen2022pfl} and Motley~\cite{wu2022motley}. However, these benchmarks do not investigate the effect of experimental variables and only consider a few algorithms and their variants for comparison. In contrast, FedZoo-Bench offers support for both global and personalized FL approaches, providing implementation of 22 SOTA methods and the ability to assess generalization to newcomers. Additionally, our study on the effect of FL-specific experimental variables in relation to each other and performance results, together with \textcolor{red}{the} identification of more meaningful setups for each FL approach, is a new contribution to the field.
%~\footnote{FedZoo-Bench supports the variants of}.

% In contrast, our study make a new contribution to the field by studying the effect of FL-specific experimental variables in relation to each other and performance results ~\cite{beutel2020flower, he2020fedml, dimitriadis2022flute, li2021federated}

% Current FL benchmarks primarily focus on enabling various platforms and APIs to perform FL for different applications. They often only realize the global FL approach and implementing basic algorithms such as FedAvg, including FATE~\cite{liu2021fate}, PaddleFL~\cite{paddlefl}, TFF~\cite{tff}, FedLearner~\cite{fedlearner}. While some other benchmarks offer more customizable features, such as FLOWER~\cite{beutel2020flower}, FedML~\cite{he2020fedml}, FLUTE~\cite{dimitriadis2022flute} and NIID-Bench~\cite{li2021federated}. UniFed~\cite{liu2022unifed} provides a comprehensive study on the applicability and comparison of these FL benchmarks.  The recent personalized FL benchmarks, such as pFL-Bench~\cite{chen2022pfl} and Motley~\cite{wu2022motley}, do not investigate the effect of experimental variables and only consider a few number of algorithms and their variants for comparison. In contrast, FedZoo-Bench offers support for both global and personalized FL approaches, providing researchers with a comprehensive set of standardized and customizable features, including the ability to assess generalization to newcomers, and pre-implementation of 22 different state-of-the-art methods. Additionally, our study on the effect of FL-specific experimental variables in relation to each other and performance results, as well as identification of more meaningful setups for each FL approach, is a new contribution to the field.


%For a more detailed review and applicability comparison of existing FL benchmarks, we refer the reader to UniFed~\cite{liu2022unifed}, which provides a comprehensive evaluation and comparison of the existing FL benchmarks.

% Personalized FL: FedBench~\cite{matsuda2022empirical}
%\textbf{Comparison to existing benchmarks.} Current federated learning benchmarks primarily focus on making the technology run on various platforms and APIs for various applications, often only addressing the global FL approach and implementing basic algorithms such as FedAvg. While a few benchmarks do consider personalized FL, they do not provide support for generalization to newcomers or fairness assessment. Furthermore, existing benchmarks are not easily customizable or convenient for comparing different methods. FedZoo-Bench, on the other hand, stands out in this field by supporting both global and personalized FL approaches, providing researchers with a comprehensive set of standardized and customizable features and generalization to newcomers assessment, pre-implementing 17 state-of-the-art methods, and continuously adding more FL methods to it. This eliminates the need for researchers to spend time re-implementing methods and allows them to focus on their research promoting fairness, consistency, and reproducibility in federated learning research rather than the technicalities of implementation. FedZoo-Bench's ease of use makes it an ideal tool for researchers to test their ideas and hypotheses.
% All of the existing FL benchmarks put their focus on making different platforms to be able to perform federated learning. They mostly implement few algorithms (many just FedAvg) using their platform to realize that and their platform has less flexibility to be extended to different experimental settings for academic research purposes. However, FedZoo-Bench contains the implementation of more than XX different algorithms in PyTorch with easy extension to custom algorithm implementation and experimental setups. We defer discussions of FedZoo-Bench's implementation and use case for different settings to the project's documentation.

%Despite the extensive amount of works have been done for both FL approaches, the state of progress is not well understood. This issue comes from lack of understanding in FL experimental design setups and consistent evaluation of methods under a common experimental setup and implementation.


\begin{comment}
\begin{table}
\caption{comparison of Benchmarks. }
\label{tab:comp-benchmarks}
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{l|cccc}
            \toprule
Benchmark & Time & Accuracy & Memory & Library \\
            \midrule
FedML  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
NIID-Bench  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
Motley  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
NVIDIA-Flare  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
Intel-FL  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
FedZoo  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
\end{tabular}
}
\end{table}

\begin{table}
\caption{Comparison of Benchmarks with FedZoo  }
\label{tab:comp-benchmarks-2}
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{l|cccc}
            \toprule
Benchmark & \# Supporting Algorithms & Fairness & Generalization to newcommers & \# Supporting Non-IID \\
            \midrule
FedML  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
NIID-Bench  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
Motley  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
NVIDIA-Flare  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
Intel-FL  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
FedZoo  & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$ & $XX\pm XX$\\
            \midrule
\end{tabular}
}
\end{table}
\end{comment}