\section{Comprehensive Study on FL Experimental Variables}\label{study}
%\subsection{Overview and Setup}
\begin{comment}
In the previous sections, we discussed that there is a lack of consistent and experimental design settings in the literature and prior works tend to create the settings in a way to satisfy their assumptions and show better results. In summary, prior works tend to 
\begin{itemize}
    \item make it unclear about the exact experimental settings, and evaluation metrics
    \item do not follow a exact experimental settings universally
    \item not consider the effect of confounding variables in their results.
\end{itemize}

These obstacles have made it difficult for a fair and comprehensive evaluation of the proposed methods and identify the-state-of-the-art baselines. To identify the best experimental design settings and suggestions, we perform a comprehensive study on different experimental design factors and bring our findings in this section.
\end{comment} 
%In this section, we conduct a comprehensive study on the impact of various experimental variables in FL.
\vspace{-0.2cm}

\textbf{Overview.}  To design an effective FL experiment, it is crucial to understand how the FL-specific variables which are clients' data partitioning (type and level of statistical heterogeneity), local epochs ($E$), sample rate ($C$), and communication rounds ($T$) interact with each other and can affect the results. While communication rounds ($T$) serve as the equivalent of epochs in traditional centralized training and primarily determine the training budget, the other variables have a more direct impact on performance. Hence, we focus our analysis on these variables, in relation to each other and performance results, and evaluation metric failure and derive new insights for the FL community to design meaningful and well-incentivized FL experiments.

%Our analysis of these variables, in relation to each other and performance results, provides new insights and best practices for the FL community to design meaningful and well-incentivized FL experiments and avoid evaluation metric failure.
%We also identify the causes of evaluation metrics failure for each FL approach and bring our suggestions on how to avoid them. The study that we provide in this section derives new insights for the broader FL community. 
%We also bring our findings and suggestions on how to control the effect of each variables for a fair experimental design.

\textbf{Baselines.} We use three key baselines in our study: \emph{FedAvg}~\cite{mcmahan2017communication}, the standard FL baseline that has been widely used in the existing literature and can serve as a good representative for the global FL approach~\footnote{\label{foot1}We show in Section~\ref{sec:comparison} that the performance of this baseline is competitive to the SOTA methods.}; \emph{FedAvg + Fine-Tuning (FT)}~\cite{jiang2019improving}, a simple personalized FL baseline that has been shown to perform well in practice and can serve as a representative for the personalized FL approach~\footref{foot1}; and \emph{SOLO}, a solo (local only) training of each client on their own dataset without participation in federation, which serves as a baseline to evaluate the benefit of federation under different experimental conditions. 
%Our results show that the performance of these baselines is competitive with state-of-the-art methods, indicating that our studies can be generalized to both global and personalized FL approaches.

%We use three important baselines for our study in this section. \emph{FedAvg}~\cite{mcmahan2017communication} which is the standard FL baseline that has been compared with in almost the entire of existing works and can be used as a good representative for the global FL approach~\footnote{\label{foot1}We show in Section~\ref{sec:comparison} that the performance of this baseline is competitive to the SOTA methods and therefore, our studies can be generalized to global and personalized FL approaches.}.
%widely used in the real world applications~\cite{apple-siri}. \emph{FedAvg + Fine-Tuning (FT)}~\cite{jiang2019improving} which is the simplest personalized federated learning baseline and has been shown performing well in practice~\cite{wang2019federated,sim2021robust}. This baseline can be also served as a good representative for the personalized FL approach~\footref{foot1}. \emph{SOLO} which is a simple solo (local only) training of each client on their own dataset without participation in federation and can be competitive when clients have enough data or the statistical heterogeneity level is very high. This serves as a good comparison baseline to identify if federation has enough incentive under a specific experimental setting. 

\textbf{Setup.} We use CIFAR-10~\cite{krizhevsky2009learning} dataset and LeNet-5~\cite{lecun1998gradient} architecture which has been used in the majority of existing works. We fix number of clients ($N $), communication rounds ($R$), and sample rate ($C$) to $100$, $100$, \text{and} $0.1$ respectively; unless specified otherwise. We use SGD optimizer with \textcolor{red}{a} learning rate of $0.01$, and momentum of $0.9$~\footnote{This optimizer and learning rate have been used in some works~\cite{collins2021exploiting, li2021federated, vahidian2022efficient} under a similar setup and we also find that it works the best for our studies.}. We use this base setting for all of our experimental \textcolor{red}{studies} in this section. The reported results are the average results over 3 independent and different runs for a more fair and robust assessment.
% This is general setting which has been used in the majority of existing works as well.

% Figure environment removed


\subsection{Evaluation Metric}
\vspace{-0.1cm}
\textcolor{red}{The evaluation metric} for performance is a critical factor in making a fair and consistent assessment in FL. However, the way in which evaluation metrics are calculated in the current FL literature is often ambiguous and varies significantly between papers. In this part we focus on identifying the causes of evaluation metric failures for each FL approach and bring our suggestions for avoiding them.


\textbf{Global FL.} The evaluation metric is the performance of \textcolor{red}{the} global model on the test dataset at the server. We find that the causes for evaluation failures are (1) the round used to report the result and (2) the test data percentage used to evaluate the model. Figure~\ref{fig1:a} shows the global model accuracy over the last $10$ rounds on Non-IID Label Dir($0.1$) partitioning. We can see there is a maximum variation of $7\%$ in the results based on which round to pick for reporting the result. Also, the difference of \textcolor{red}{the} final round result with the average bar is about $4\%$. This shows that the round used to report the result is important and to have a more robust metric \textcolor{red}{for} these variations, it is better to report the average results over a number of rounds. Figure~\ref{fig1:b} shows the variations of the reported result for the same model using different test data percentages. This clearly shows that using different test data points can cause bias in the evaluation. To avoid the mentioned failures and have a more reliable evaluation metric we suggest the following definition:

\begin{tcolorbox}[colback=white!5!white,colframe=black!75!black]\label{def:gfl}
  \textbf{Definition 1} (global FL evaluation metric). We define the average performance of \textcolor{red}{the} global model on the entire test data at the server (if available) over the last $[C \cdot N]$ communication rounds as the evaluation metric for global FL approach~\footnote{\label{foot2}We use this metric definition for all of our experiments.}.
\end{tcolorbox}
\vspace{-0.2cm}
\textbf{Personalized FL.} The evaluation metric is the final average performance of all participating clients on their local test data. The factor which can cause evaluation failure is the local test data percentage used to evaluate each client's model. \textcolor{red}{Prior works have allocated different amounts of data as local test sets to individual clients.} Figure~\ref{fig1:c} shows the variability of the reported results for the same clients under different local test data percentages on Non-IID Label Dir($0.1$) partitioning. This highlights that the use of a randomly selected portion for test data can lead to inaccurate and biased evaluations based on the selected data points be easy or hard.
%\footnote{Previous works, use different percentage of test data for each client when partitioning the dataset across clients.} 
To avoid the mentioned failure in the evaluation metric we suggest the following definition:
\vspace{-0.1cm}
\begin{tcolorbox}[colback=white!5!white,colframe=black!75!black]\label{def:pfl}
  \textbf{Definition 2} (personalized FL evaluation metric). We define the average of \textcolor{red}{the} final performance of all the clients on their entire local test data (if available) as the evaluation metric for personalized FL approach~\footref{foot2}\textsuperscript{,}\footnote{\textcolor{red}{The entire local test data consists of allocating all available test samples belonging to the classes owned by the client.}}.
\end{tcolorbox}

\subsection{Statistical Heterogeneity and Local Epochs} \label{sec:heterogeneity-localepoch}
In this part, we focus our study to understand how different \textcolor{red}{levels and types} of statistical heterogeneity together with local epochs can affect the results and change the globalization and personalization incentives in FL.

% Figure environment removed

\textbf{Level of statistical heterogeneity.} Figure~\ref{fig2:a} illustrates how the performance of the baselines for a fixed label Dir type of statistical heterogeneity with 10 local epochs varies with the level of statistical heterogeneity. As the level of statistical heterogeneity decreases, the global FL approach becomes more successful than the personalized one. The vertical line in the plot indicates the approximate boundary between the incentives of the two FL perspectives. We can see that in the extreme Non-IID case ($\alpha=0.05$), neither of the FL approaches \textcolor{red}{is} motivated, as the performance of the SOLO baseline is competitive. Additionally, from $\alpha=0.8$ onwards, the global FL approach seems to perform \textcolor{red}{close} to the end of the spectrum at $\alpha=\infty$, which is IID partitioning. Furthermore, we find that the incentives for globalization and personalization can vary with changes in the number of local epochs for a fixed type of statistical heterogeneity (see Section~\ref{appendix:incentives} for more results).

\textbf{Local epochs.} Figures~\ref{fig2:b}, and~\ref{fig2:c} show how the performance of FedAvg and FedAvg + FT for a fixed label Dir type of statistical heterogeneity varies with different \textcolor{red}{levels} of statistical heterogeneity and \textcolor{red}{the} number of local epochs. Figure~\ref{fig2:b} suggests that FedAvg favors fewer local epochs to achieve higher performance. However, Figure~\ref{fig2:c} suggests that FedAvg + FT favors more local epochs for achieving better results but no more than $5$ or $10$ depending on the level of statistical heterogeneity. Our findings support the observation in~\cite{karimireddy2020scaffold} that client drift can have a significant impact on performance, and increasing the number of local epochs amplifies this effect in the results.
%Generally, for both approaches in FL it is desirable to have more local training while not hurting the results which is an aspect that researchers need to consider in their algorithm design.

%Local epoch of $\{5, 10\}$, and $\{1, 5\}$ seems to work really good for Non-IID Label Dir, and Non-IID Label Skew respectively. Another observation is that excessive fine-tuning more than $10$ local epochs can hurt the results. 
% In FL, it is desirable to have more local training and fewer communication rounds while not hurting the results. Increasing the number of local epochs can magnify the effect of client drift which is an aspect that researchers need to consider to handle in their algorithm desing.

% Figure environment removed

\textbf{Type of statistical heterogeneity.} Figure~\ref{fig3} illustrates the results of an experiment similar to that of Figure~\ref{fig2}, but with a label skew type of statistical heterogeneity. Figure~\ref{fig3:a} shows the performance of the baselines with fixed $10$ local epochs at different levels of statistical heterogeneity. Comparing this figure with Figure~\ref{fig2:a} reveals that this type of heterogeneity favors personalization over globalization across a wider range of heterogeneity levels. Figures~\ref{fig3:b} and~\ref{fig3:c} show the performance of FedAvg and FedAvg + FT with different levels of statistical heterogeneity and local epochs. Comparing these figures with Figures~\ref{fig2:b} and~\ref{fig2:c} for the label Dir type of statistical heterogeneity reveals that this type of statistical heterogeneity is less affected by an increase in local epochs at each level of heterogeneity. This highlights another finding that the effect of client drift may vary for different types of statistical heterogeneity (see Section~\ref{appendix:incentives} for more results).

%\textbf{Summary and Suggestions.} We observe that level of heterogeneity and local epochs are related all together and can affect personalization and globalization incentives. In particular, we notice that the effect of client drift phenomenon is more sever for Non-IID Label Dir partitioning compared to Non-IID Label Skew and Non-IID Label Skew partitioning favors personalized FL approach on a wider spectrum of level of heterogeneity compared to Non-IID Label Dir. Table~\ref{tab:incentivized-settings} identifies the well-incentivized settings for each FL approach.

% \begin{tcolorbox}[colback=white!5!white,colframe=black!75!black]
%   \textbf{Finding 1} (Global FL incentivized settings). We find the following (Non-IID, Epoch) combinations are well incentivized settings for global FL:
%   \begin{itemize}
%       \item \textbf{Dir:} $(\alpha >= 0.5, E \in \{1, 5, 10, 20\})$ 
%       \item \textbf{Label Skew:} $(\rm{ls} >= 8, E \in \{1, 5, 10, 20\})$, except $(\rm{ls} = 8, E = 20)$
%   \end{itemize}
% \end{tcolorbox}

% \begin{tcolorbox}[colback=white!5!white,colframe=black!75!black]
%   \textbf{Finding 2} (Personalized FL incentivized settings). We find the following (Non-IID, Epoch) combinations are well incentivized settings for personalized FL:
%   \begin{itemize}
%       \item \textbf{Dir:} $(0.05 < \alpha < 0.5, E \in \{1, 5, 10, 20\})$ 
%       \item \textbf{Label Skew:} $(2 <= \rm{ls} < 8, E \in \{1, 5, 10, 20\})$, except $(\rm{ls} = 8, E = 20)$
%   \end{itemize}
% \end{tcolorbox}

\begin{comment}
\subsection{Number of Communication Rounds}
Figure~\ref{fig4:a}, \ref{fig4:b}, and~\ref{fig4:c}, \ref{fig4:d} show the effect of number of communication rounds on Non-IID Dir(0.1) and Non-IID Label Skew (30\%) for different number of local epochs. We observe that more communication rounds can improve the results of FedAvg and FedAvg+FT for $1$ local epoch. However, more communication rounds hurt the results for higher local epochs. 

\begin{tcolorbox}[colback=white!5!white,colframe=black!75!black]
  \textbf{Finding 3} (Effect of communication rounds). We find that higher number of communication rounds indeed leads to better results for 1 local epochs. However, for more than 1 local epochs more number of communication rounds can hurt the results due to magnification of client drift effect. Also, hyper-parameter tuning becomes more difficult for more than 1 local epochs along with higher than 250 number of communication rounds.\footnote{We observe similar exists for other level of heterogeneity.}
\end{tcolorbox}

% Figure environment removed

\end{comment}

\subsection{Sample Rate}
Figures \ref{fig5:a}, \ref{fig5:b} and \ref{fig5:c},~\ref{fig5:d} demonstrate the impact of sample rate and local epochs on the performance for label Dir($0.1$) and label skew ($30$\%) types of statistical heterogeneity, respectively. Our observations show that \textbf{increasing the sample rate (i.e., averaging more models) can effectively \textcolor{red}{mitigate} the \textcolor{red}{negative} impact of statistical heterogeneity on performance}\footnote{\textcolor{red}{Model averaging is a standard component of many FL algorithms.}}. \textcolor{red}{Additionally, it is essential to consider that averaging a higher number of models (i.e., a high sample rate) reduces the approximation error associated with the averaged models across all clients\footnote{\textcolor{red}{It is also worth mentioning that generally high sample rates are not favorable, as they would increase the communication cost of the FL algorithms.}}.} We find that sample rates of $C >= 0.4$ can significantly reduce the effect of heterogeneity on the performance, while very small sample rates of $C < 0.1$ result in poor performance. Based on these findings, we suggest using a sample rate in the range of $0.1 <= C < 0.4$ for experimental design to accurately evaluate an algorithm's success in \textcolor{red}{the} presence of data heterogeneity.

% \begin{tcolorbox}[colback=white!5!white,colframe=black!75!black]
%   \textbf{Finding 4} (Effect of sample rate). We find that higher sample rate can indeed improve the results for all number of local epochs. Moreover, this improvement is more for lower than 5 local epochs. Also, hyper-parameter tuning becomes more difficult for higher than 0.5 number of local epochs.\footnote{We observe similar exists for other level of heterogeneity.}
% \end{tcolorbox}

% Figure environment removed

\begin{comment}
\subsection{Number of Clients}
Figure~\ref{fig6:a}, and~\ref{fig6:b} show the effect of number of clients on Non-IID Dir(0.1) and Non-IID Label Skew (30\%) for different number of local epochs with a fixed sample rate $C=0.1$. Increasing the number of clients with a fixed sample rate has increased the performance results. This happens due to the effect averaging more models as the number of clients increases. In Figure~\ref{fig6:b}, we have plotted the results with adaptive sample rate. In this figure we adaptively adjusted the sample rate with the number of clients so that the number of models being averaged at the server be between 8 to 15. This plot shows that the results are the same.  

% \begin{tcolorbox}[colback=white!5!white,colframe=black!75!black]
%   \textbf{Finding 5} (Effect of sample rate). We find that higher sample rate can indeed improve the results for all number of local epochs. Moreover, this improvement is more for lower than 5 local epochs. Also, hyper-parameter tuning becomes more difficult for higher than 0.5 number of local epochs.\footnote{We observe similar exists for other level of heterogeneity.}
% \end{tcolorbox}

% Figure environment removed


% Figure environment removed

\end{comment}