\section{Background: Federated Learning and Statistical Heterogeneity} \label{background}
%In this section, we provide the backgrounds for each FL approach and statistical heterogeneity.
\subsection{Overview of Federated Learning and Notations}
Consider a server and a set of clients $S$ which participate in the federation process. $f_i(\bx; \btheta_i)$ is the $L$ layers neural network model of client $i$ with parameters $\btheta_i=(\bW_i^1,\ldots,\bW_i^L)$ where $\bW^l$ stands for the $l$-th layer weights, training dataset $D_i^{train}$, and test dataset $D_i^{test}$. \textcolor{red}{At} the beginning of communication round $t$, a subset of clients $S_t$ are randomly selected with the sampling rate of $C \in (0, 1]$ out of total $N$ available clients. The selected clients receive the parameters from the server and perform local training for $E$ epochs with \textcolor{red}{the} batch size of $B$ and learning rate of $\eta$. At the end of communication round $t$, the selected clients send back their updated parameters to the server for server-side processing and model fusion. This federation process continues for $T$ total communication rounds. Algorithm~\ref{alg:FederatedLearning} shows this process in detail.

\subsection{Problem Formulation}
%We formally define the problem of global and personalized FL in this section. 

\textbf{Global Federated Learning (gFL).}
The objective is to train a shared global model at the server which uniformly performs well on each client and the problem is defined as follow: 
\begin{align} \label{eq:gfl}
    \widehat \btheta_g = \argmin {\btheta_g} \sum_i^N \E_{(\bx,y)\sim D_i^{\text{train}}}[\ell(f_i(\bx; \btheta_g), y)].
\end{align}
FedAvg~\cite{mcmahan2017communication} is the first and most popular algorithm proposed to solve \textcolor{red}{Equation}~\ref{eq:gfl}, which uses parameter averaging at the server-side for model fusion.

\textbf{Personalized Federated Learning (pFL).}
The objective is to train personalized models \textcolor{red}{to perform} well on each client's distinctive data distribution and the problem is defined as \textcolor{red}{follows}: 
\begin{align} \label{eq:pfl}
\textcolor{red}{
    \{{\widehat \btheta_i}\}_1^N = \argmin {\{\btheta_i\}_1^N} \sum_i^N
    \E_{(\bx,y)\sim D_i^{\text{test}}}[\ell(f_i(\bx; \btheta_i), y)].
    }
    %\{\btheta_1,\ldots,\btheta_N\}
\end{align}
FedAvg + Fine-Tunining (FT)~\cite{jiang2019improving} is the simplest algorithm proposed to solve \textcolor{red}{Equation}~\ref{eq:pfl}, where each client fine-tunes the global model obtained via FedAvg on their local data.

\begin{algorithm}[t]
\caption{Federated Learning}
\label{alg:FederatedLearning}

\begin{algorithmic}[1]
\REQUIRE number of clients ($N$), sampling rate ($C\in(0,1]$), number of communication rounds ($T$), local dataset of client $k$ ($D_k$), local epoch ($E$), local batch size ($B$), learning rate ($\eta$).
%\STATE \textbf{Server Executes:}
\STATE Initialize the server model with $\btheta_g^0$ \;
\FOR {each round $t = 0, 1, \ldots, T-1$}
\STATE $m \leftarrow {\rm{max}}(C \cdot N,1)$ \;
\STATE $S_t \leftarrow $(random set of m clients) \; %\tcp*{set of $m$ available clients}
\FOR {each client $k \in S_t$ \rm{\textbf{in parallel}}}
\STATE $\btheta^{t+1}_{k}\leftarrow {\mathtt{ClientUpdate}}(k; \btheta^t_{g} )$ \; 
\ENDFOR
\STATE $\btheta^{t+1}_{g}=\mathtt{ModelFusion}(\{\btheta_k \}_{k \in S_t})$ \COMMENT{FedAvg~\cite{mcmahan2017communication}: $\theta^{t+1}_{g}=\sum_{k \in S_t }{|D_{k}|\theta^{t+1}_{k}}  /\sum_{k \in S_t }{|D_{k}|}$}
%\STATE $\theta^{t+1}_{g}=\sum_{k \in S_t }{|D_{k}|\theta^{t+1}_{k}}  /\sum_{k \in S_t }{|D_{k}|}$
\ENDFOR
\vspace{1mm}
\FUNCTION {$\mathtt{ClientUpdate}(k, \btheta^t_{g})$}
\STATE $\btheta^{t}_k \leftarrow \btheta^t_{g}$ \;
\STATE $\mathcal{B} \leftarrow$ ({randomly splitting $D_k^{train}$ into batches of size $B$})\;
\FOR {each local epoch $ \in \{1, \ldots, E\}$}
\FOR {each batch $\mathbf{b} \in \mathcal{B}$}
\STATE $\btheta^{t}_k \leftarrow \btheta^{t}_k - \eta \nabla_{\btheta^{t}_k} \ell(f_k(\bx; \btheta^{t}_k), y)$\;
\ENDFOR
\ENDFOR
\STATE $\btheta^{t+1}_k \leftarrow \btheta^{t}_k$\;
\ENDFUNCTION
% \Fn{\FClient{$k, \btheta^t_{g}$}}{
%     \STATE $\btheta^{t}_k \leftarrow \btheta^t_{g}$ \;
%     \STATE $\mathcal{B} \leftarrow$ ({randomly splitting $D_k$ into batches of Size $B$})\;
%     \For {each local epoch $i = 1, \dots, E$} {
%         \For {each batch $\mathbf{b} \in \mathcal{B}$} {
%             \STATE $\btheta^{t}_k \leftarrow \btheta^{t}_k - \eta \nabla_{\btheta^{t}_k} f_k(\btheta^{t}_k; \mathbf{b})$\;
%         }
%     }
%     \STATE $\btheta^{t+1}_k \leftarrow \btheta^{t}_k$\;
%     \KwRet $\btheta^{t+1}_{k}$\;
% }
\end{algorithmic}
\end{algorithm}

\subsection{Statistical Heterogeneity}
Consider the local data distribution of clients, denoted as $\P_i(\bx, y)$. Clients can have statistical heterogeneity or Non-IID data distribution w.r.t each other when $\P_i(\bx, y) \neq \P_j(\bx, y)$. The most popular way to realize this heterogeneity in FL is label distribution skew in which clients have different label \textcolor{red}{distributions} of a dataset (i.e. $\P_i(y) \neq \P_j(y)$)\footnote{Other mechanisms to realize statistical heterogeneity do exist, but are less commonly used in the FL community.}. Two commonly used mechanisms to create label distribution skew in FL are:
\begin{itemize}
    \item \textbf{Label skew($p$)~\cite{li2021federated}:} each client receives $p\%$ of the total classes of a dataset at random. The data points of each class are uniformly partitioned amongst the clients owning that class. %\vspace{-0.5cm}
    \item \textbf{Label Dir($\alpha$)~\cite{hsu2019measuring}:} each client draws a random vector with the length of total classes of a dataset from Dirichlet distribution with a concentration parameter $\alpha$. The data points of each class are uniformly partitioned amongst clients according to each client's class proportions.
\end{itemize}
%\vspace{-0.6cm}

% Consider the local data distribution of clients $\P_i(\bx, y)$. Clients can have statistical heterogeneity or Non-IID distribution across each other under the following cases: 
% \begin{enumerate}
%     \item \textbf{Label distribution skew:} happens when the label distribution (i.e. $\P_i(y)$) is different across clients.
%     \item \textbf{Feature distribution skew:} happens when the feature distribution (i.e. $\P_i(\bx)$) is different across clients.
%     \item \textbf{Same label, different features (concept drift):} happens when the conditional feature distribution $\P_i(\bx|y)$ is different across clients. 
%     \item \textbf{Same features, different label (concept shift):} happens happens when the conditional label distribution $\P_i(y|\bx)$ is different across clients. 
%     \item \textbf{Quantity skew:} happens when clients have different amount of data.
% \end{enumerate}

% In the existing literature, the majority of works have focused on label distribution skew case. We find the following ways to simulate label distribution skew from the prior arts: 
% \begin{itemize}
%     \item \textbf{Label skew($p\%$):} in this case $p\%$ of the total classes are being randomly selected for each clients; then, each class data points are being partitioned amongst the clients who have that class.
%     \item \textbf{Label Dir($\alpha$):} in this case each client draws a random vector from $\textit{Dir}(\alpha)$ for their class data points; then, each class data points are being partitioned amongst clients based on each clients' proportions.
%     \item \textbf{Random shard($p$):} in this case, dataset is being partitioned into shards based on the number of clients, where each shard has only one class; then, each client receives $p$ random shards. In this scenario, clients have at most $p$ classes. 
%     \item \textbf{Quantity Dir($\alpha$):} in this case each client draws a random number from $\textit{Dir}(\alpha)$ for their data points; then, data points are being randomly partitioned amongst clients based on each clients' data proportions.
%     \item \textbf{Affine Distribution Shift:}
%     \item \textbf{Feature noise:} 
% \end{itemize}
% Figure~\ref{} shows the demographic percentage of Non-IID simulation ways used in our corpus. Leaf also provides a simulated Non-IID partitions based the natural inherent Non-IID.