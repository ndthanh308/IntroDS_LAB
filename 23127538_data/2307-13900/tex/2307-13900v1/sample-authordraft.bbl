\begin{thebibliography}{10}

\bibitem{kaur2023refind}
Simerjot Kaur, Charese Smiley, Akshat Gupta, Joy Sain, Dongsheng Wang, Suchetha
  Siddagangappa, Toyin Aguda, and Sameena Shah.
\newblock Refind: Relation extraction financial dataset.
\newblock In {\em In Proceedings of the 46th International ACM SIGIR Conference
  on Research and Development in Information Retrieval(SIGIR ’23), July
  23–27, 2023, Taipei, Taiwan. ACM, New York, NY, USA,}, 2023.

\bibitem{NIPS2017_3f5ee243}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{schick-schutze-2021-exploiting}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock Exploiting cloze-questions for few-shot text classification and
  natural language inference.
\newblock In {\em Proceedings of the 16th Conference of the European Chapter of
  the Association for Computational Linguistics: Main Volume}, pages 255--269,
  Online, April 2021. Association for Computational Linguistics.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{Lan2020ALBERT:}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{he2022debertav3}
Pengcheng He, Jianfeng Gao, and Weizhu Chen.
\newblock Debertav3: Improving deberta using electra-style pre-training with
  gradient-disentangled embedding sharing.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem{loukas-etal-2021-edgar}
Lefteris Loukas, Manos Fergadiotis, Ion Androutsopoulos, and Prodromos
  Malakasiotis.
\newblock {EDGAR}-{CORPUS}: Billions of tokens make the world go round.
\newblock In {\em Proceedings of the Third Workshop on Economics and Natural
  Language Processing}, pages 13--18, Punta Cana, Dominican Republic, November
  2021. Association for Computational Linguistics.

\bibitem{baldini-soares-etal-2019-matching}
Livio Baldini~Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski.
\newblock Matching the blanks: Distributional similarity for relation learning.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 2895--2905, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem{araci2019finbert}
Dogu Araci.
\newblock Finbert: Financial sentiment analysis with pre-trained language
  models.
\newblock {\em arXiv preprint arXiv:1908.10063}, 2019.

\bibitem{yamada-etal-2020-luke}
Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto.
\newblock {LUKE}: Deep contextualized entity representations with entity-aware
  self-attention.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 6442--6454, Online, November
  2020. Association for Computational Linguistics.

\bibitem{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{NEURIPS2020_1ef91c21}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 2958--2969. Curran Associates, Inc., 2020.

\end{thebibliography}
