{
  "title": "A Variance-Reduced Aggregation Based Gradient Tracking method for Distributed Optimization over Directed Networks",
  "authors": [
    "Shengchao Zhao",
    "Siyuan Song",
    "Yongchao Liu"
  ],
  "submission_date": "2023-07-27T11:12:46+00:00",
  "revised_dates": [],
  "abstract": "This paper studies the distributed optimization problem over directed networks with noisy information-sharing. To resolve the imperfect communication issue over directed networks, a series of noise-robust variants of Push-Pull/AB method have been developed. These methods improve the robustness of Push-Pull method against the information-sharing noise through adding small factors on weight matrices and replacing the global gradient tracking with the cumulative gradient tracking. Based on the two techniques, we propose a new variant of the Push-Pull method by presenting a novel mechanism of inter-agent information aggregation, named variance-reduced aggregation (VRA). VRA helps us to release some conditions on the objective function and networks. When the objective function is convex and the sharing-information noise is variance-unbounded, it can be shown that the proposed method converges to the optimal solution almost surely. When the objective function is strongly convex and the sharing-information noise is variance-bounded, the proposed method achieves the convergence rate of $\\mathcal{O}\\left(k^{-(1-ε)}\\right)$ in the mean square sense, where $ε$ could be close to 0 infinitely. Simulated experiments on ridge regression problems verify the effectiveness of the proposed method.",
  "categories": [
    "math.OC"
  ],
  "primary_category": "math.OC",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14776",
  "pdf_url": "https://arxiv.org/pdf/2307.14776v1",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 219237,
  "size_after_bytes": 25497
}