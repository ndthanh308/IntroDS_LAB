\section{Evaluation}\label{section5}

In this section, we evaluate whether \toolname can address the super producer threat without introducing significantly higher system overhead than existing approaches. Specifically, we focus on the following questions:

\begin{itemize}
    \item RQ 1: Can \toolname avoid dropping provenance data?
    \item RQ 2: Can \toolname prevent a super producer from slowing down other applications?
    \item RQ 3: What is the run-time overhead of \toolname?
    \item RQ 4: Can data reduction techniques address the super-producer threat?
    \item RQ 5: Can increasing the buffer size address the super-producer threat?
\end{itemize}

To ensure the generalizability of our evaluation, we run experiments on four hardware configurations: 1 CPU core with 2GB memory (C1 and C5), 4 CPU cores with 8GB memory (C2 and C6), 16 CPU cores with 32GB memory (C3 and C7), and 32 CPU cores with 64 GB memory (C4 and C8). We also conduct our experiments on both physical and virtual machines, resulting in eight different configurations in total. C1-C4 represent VM configurations, while C5-C8 represent PM configurations. All machines run OSes of Ubuntu 18.04. \toolname is also tested on other Linux distribution like openEuler~\cite{oe} 20.03 which shows the similar results.


\subsection{RQ 1: Event Drop}
\label{ssec:eventdrop}
To answer this research question, we first conducted a controlled measurement study on the number of provenance events dropped by \toolname and pro-performance solutions such as Sysdig, LTTng, and Linux Audit. Then, we simulated a realistic web server to evaluate how well \toolname can prevent \ac{pdos} attacks.

In the controlled experiment, we launched a bash script to mimic a super producer that generates a large amount of provenance data in a short period. The script forks $n$ processes, where $n$ is the number of CPU cores, and each process repeatedly invokes the \code{write} system call and performs the \code{count++} operation. We adjusted the proportion of the \code{write} and \code{count++} operations to control the generation speed of system call events over 30 seconds. We ran the experiment on four hardware configurations, on both virtual and physical machines. The percentage of dropped events represents the potential success rate of a \ac{pdos} attack.


% Figure environment removed

Figure~\ref{fig:droppaper}(a),~\ref{fig:droppaper}(b),~\ref{fig:droppaper}(c), and~\ref{fig:droppaper}(d) show the results for cases with 1 CPU core and 2GB memory, and 32 CPU cores with 64 GB memory, on both virtual and physical machines. The results for other configurations are similar to those shown in Figure~\ref{fig:droppaper}, but due to space limitations, we have included them in our GitHub repository. The x-axis represents the number of system call events generated by the kernel per second, while the y-axis represents the number of system call events processed per second by the user-space component. The gap between y and x represents the number of events dropped. The diagonal line in the figure is the ideal line, indicating that no events have been dropped. The blue dot-dashed line and magenta dash line represent Sysdig and LTTng, respectively. We have omitted the line for Linux Audit for clarity, as it drops nearly all events while the super producer is running.

Our evaluation shows that \toolname drops ZERO events while a super-producer is running. In Figure~\ref{fig:droppaper}, the line for \toolname overlaps with the diagonal line, indicating that no events have been dropped. In contrast, existing pro-performance solutions drop most of the generated system events, allowing for a high success rate for \ac{pdos} attacks. Specifically, on machines with 1 CPU core and 2 GB memory, Sysdig drops 31\% and 33\% of system call events on virtual and physical machines, respectively. On 32-core machines, Sysdig drops 98.5\% and 98.9\% of system call events on virtual and physical machines, respectively. Similarly, LTTng drops 71.9\% of total system call events on a 32-core virtual machine and about 60\% of total events on a 32-core physical machine. LTTng does not drop events on machines with 1 CPU core and 2 GB memory because it adopts a dynamic buffer mechanism that can hold more events than Sysdig. However, as we will show in the next section, this design also causes LTTng to introduce more system overhead than Sysdig. For Linux Audit, since it is not as well optimized as Sysdig and LTTng~\cite{lttng}, it drops nearly all system call events when the super producer reaches 1\% of its highest event generation speed.


\subsubsection{Preventing \ac{pdos} in realistic web-apps:} 
We evaluated whether \toolname can prevent \ac{pdos} attacks by simulating a production-level web-server. We used the 'web-serving' benchmark from CloudSuite as the implementation of the \textit{victim app}. This benchmark hosts a production-quality social networking engine, which we hosted with Apache 2.4, MariaDB 10.1, PHP 7.4, and Elgg 3.3. The \textit{target app} was implemented as a static website that provides a "ping lookup" service to users, but it had a command line injection vulnerability that allowed attackers to run remote code. We hosted the \textit{target app} in a different process of Apache and evaluated \ac{pdos} in two widely adopted resource isolation methods: (1) the \textit{target app} and \textit{victim app} were scheduled and isolated by the default Linux configuration, which focuses on maximizing system utilization while providing fairness and performance isolation in a best-effort way; and (2) the \textit{target app} and \textit{victim app} were in different \code{cgroups} with isolated CPU utilization. All applications were deployed on an Intel Xeon Silver server with 16 CPU cores, 128 GB memory, and a 10 TB HDD.


Our implemented \ac{pdos} attack consists of three steps. First, we simulate 20 visitors accessing the \textit{victim app} and turn it into a super producer. Second, the attacker waits for three to five minutes to ensure that the auditing frameworks are overloaded. Third, the attacker initiates a command \& control connection to the \textit{target app} by exploiting a command line injection vulnerability. We consider the \ac{pdos} attack successful if the auditing frameworks do not record any provenance data about the command \& control connection. For each auditing framework, we conduct 120 \ac{pdos} attacks and count how many of them are successful.

\noindent\textbf{Results:} Our experiment shows that \toolname can prevent \ac{pdos} attacks, while all three baseline methods are vulnerable to the \ac{pdos} attack. The attack success rates for these baseline methods are higher than 90\% in all cases, with at least 107 out of 120 \ac{pdos} attacks being successful. These results are shown in Table~\ref{tab:pdosattack} for both isolation methods. Furthermore, using \code{cgroup} does not prevent the \ac{pdos} attack because the \textit{victim app} does not exceed its quota, and the collector is unable to isolate provenance events internally.
 
\begin{table}[t]
    \caption{\small Attack success rate of the \ac{pdos} attack ({\#}successful/ {\#}attempts)}
     \vspace{-0.5em}    
    \label{tab:pdosattack}
    \centering
       \resizebox{0.49\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        & Sysdig &LTTng &Linux Audit & \cellcolor[gray]{0.8}\toolname\\ \hline
    Default    & 120/120    & 107/120   &  120/120 & \cellcolor[gray]{0.8}0/120 \\\hline
    Cgroup     & 115/120    & 107/120   &  120/120 & \cellcolor[gray]{0.8} 0/120\\
    \hline
    \end{tabular}
    }
    \vspace{-1em}    
\end{table}

% Figure environment removed

\subsection{RQ 2: Application Slowdown}
\label{ssec:slowdonw}
To answer this research question, we implemented realistic \ac{pados} attacks using realistic web applications. In the \ac{pados} attack, we assumed that the \textit{target app} and the \textit{victim app} were running in different \code{cgroups}. We implemented the \textit{target app} and the \textit{victim app} in different \code{cgroups} to allow for absolute isolation between them. An auditing framework was running in both \code{cgroups} to monitor the \textit{target app} and the \textit{victim app}. 

We implemented the \textit{victim app} using the same script as in \S\ref{ssec:eventdrop}, and we implemented the \textit{target apps} using three widely used web applications: Nginx~\cite{Nginx}, Redis~\cite{redis}, and OpenSSL~\cite{openssl}. By leveraging the script, we were able to control the generation speed of provenance data in the super producer (the \textit{victim app}). We evaluated Sysdig, Linux Audit, and LTTng to show how they affected the performance of the \textit{target app}. To adopt a pro-integrity strategy, we implemented two additional versions based on Sysdig. The first one, called Sysdig-Camflow, optimized Camflow using a more efficient kernel module from Sysdig while preserving the per-core thread user-space collector from Camflow. The second one, called Sysdig-Integrity, integrated synchronized event processing~\cite{ahmad2022hardlog} into Sysdig to ensure the integrity of provenance data. Sysdig-Integrity blocked the currently running process when its event buffer was full and woke it up once the buffer had been processed. In this way, Sysdig-Integrity ensured zero event loss and was a guaranteed pro-integrity solution.


We also reported the performance of applications on a vanilla machine as the "No Consumer" to show the baseline performance of the system without any auditing frameworks in the user space. To measure the system performance of our applications, we ran their corresponding benchmark scripts and reported the official performance scores reported by the scripts. This avoided statistical data bias that could have resulted from poorly self-implemented benchmark scripts.

Figure~\ref{fig:rq12-nginx-1+32} shows detailed results for Nginx with 1 CPU core and 32 CPU cores on virtual machines. The results for other configurations are similar, but we have included them in our GitHub repository due to space constraints. The x-axis of Figure~\ref{fig:rq12-nginx-1+32} represents the workload of the super producer, namely the speed of generated events per second, while the y-axis represents the official performance score reported by the benchmark scripts. Since the number of consumer threads in Sysdig-Camflow equals the number of CPU cores, Sysdig-Camflow and Sysdig are identical on a single-core machine. Therefore, we merged the lines for Sysdig-Camflow and Sysdig in Figure~\ref{fig:rq12-nginx-1+32}(a). Overall, our experiment showed that an attacker could paralyze the \textit{target app} by turning the \textit{victim} into a super producer in a different \code{cgroup}.


% Figure environment removed

% Figure environment removed

Our experiments showed that (1) \toolname prevents the \ac{pados} attack, (2) existing pro-integrity solutions suffer from the \ac{pados} attack across different hardware configurations, and (3) pro-performance collectors also slow down the \textit{target app}. For all three web applications on all eight hardware configurations, when the workload of the super producer increased, \toolname maintained stable application performance regardless of the increasing workload from the super producers. For example, in Figure~\ref{fig:rq12-nginx-1+32}, \toolname was at most 5.1\% slower on a single-core virtual machine and at most 0.5\% slower on a 32 CPU cores virtual machine than the ideal "No Consumer" baseline. This overhead was also lower than that of the baselines. This result proves that \toolname is robust against \ac{pados} attacks. In other words, an attacker cannot slow down applications in different \code{cgroup} by running a super producer.

We also notice that the pro-performance collectors can also be vulnerable to the \ac{pados} attack. On the single-core virtual and physical machines, Sysdig, LTTng, and Linux Audit also decrease the performance of the \textit{target app} proportionally to the event generation speed of the super producer. This is because Sysdig, LTTng, and Linux Audit limit the resource usage of their user-space component by allowing only one user-space thread. Thus, Sysdig, LTTng, and Linux Audit behave the same as Sysdig-Camflow on a single-core platform, making them de facto pro-integrity collectors. Linux Audit has the worst performance because it relies on Netlink~\cite{salim2003linux} to pass events from the kernel to the user-space component, which leads to a less efficient kernel module than other auditing frameworks.

\subsubsection{CloudSuite Setting} 
To further evaluate how well \toolname can prevent \ac{pados} attacks in a production environment, we also implemented \ac{pados} attacks using the CloudSuite setting that we used in \S\ref{ssec:eventdrop}.

Our implemented \ac{pados} attack consisted of three steps. First, similar to \ac{pdos}, the attacker used 20 visitors to generate a flood of remote requests to the victim app. We further adjusted the workload of visitors to generate increasing pressure on the victim app. Second, the attacker waited for five minutes to ensure that the auditing framework had enough resources. Third, the attacker started a normal DoS attack on the target app. In this case, the required workload for the DoS attack on the target app was substantially reduced.

In our experiment, we measured the performance reduction of the target app to evaluate the effectiveness of the \ac{pados} attack. To accurately measure the performance of the target app, we also used \textit{wrk}~\cite{wrk} to issue HTTP requests to the target app and reported the number of returned HTTP responses as its performance metric.

\textbf{Results:} Our evaluation showed that pro-integrity collectors could substantially decrease the performance of the \textit{target app} and, thus, amplify possible DoS attacks. We showed the performance of the \textit{target app} in Figure~\ref{fig:padoscase}. Note that the request sending rate on the x-axis represents the speed at which requests reach the \textit{victim app}, not the request processing speed of the app. To better understand how pro-integrity collectors decrease the performance of the \textit{target app}, we plotted the case of Sysdig and the case of no provenance collectors (No Consumer) in Figure~\ref{fig:padoscase} as baselines.

With the default Linux configuration (Figure~\ref{fig:padoscase}(a)), we observed that pro-integrity collectors accelerated the DoS attack, requiring less workload to slow down the \textit{target app}. Specifically, the curve for No Consumer remained flat when the system had sufficient spare resources to digest attack traffic. When the workload exceeded 34K packets/s, however, the whole system became overloaded, and the performance of the \textit{target app} rapidly decreased. However, when provenance collectors were present, the \textit{target app} had smaller turning points (for Sysdig, Sysdig-Camflow, and Sysdig-Integrity, these turning points were 32K, 15K, and 15K, respectively) and much lower performance than No Consumer. This was because pro-integrity collectors consumed too many system resources and competed with other applications, overloading the server more easily. In general, Figure~\ref{fig:padoscase}(a) shows that pro-integrity collectors can amplify DoS attacks since they blindly compete for resources with other applications.
 
Figure~\ref{fig:padoscase}(b) reports the results under \code{cgroup} restrictions. The \textit{target app} and the \textit{victim app} had 20\% and 80\% CPU utilization limitations, respectively. Consequently, the super producer could not use up all system resources, and the \textit{target app} was not impacted. The flat line for No Consumer in Figure~\ref{fig:padoscase}(b) indicates this, showing that the DoS attack was ineffective. However, pro-integrity collectors compromised the isolation of \code{cgroup} and greatly influenced the \textit{target app}. In the case of Sysdig, Sysdig-Camflow, and Sysdig-Integrity, the worst performance loss for the \textit{target app} reached 40.1\%, 55.6\%, and 83.7\%, respectively. Please note that placing a \code{cgroup} limitation on provenance collectors would cause a \ac{pados} attack as studied above. In contrast to existing provenance collectors, \toolname preserved \code{cgroup} isolation and successfully protected the \textit{target app} from DoS attacks. 

\subsection{RQ 3: Runtime Overhead}
\label{sec:overhead}
This research question evaluated the runtime overhead of \toolname by measuring its impact on the OS and on running applications, both with and without a super producer. We used Sysdig as a baseline, repeated measurements ten times, and employed statistical methods, including the Wilcoxon signed-rank test, to test for significant differences between Sysdig and \toolname. We used p-value~\cite{Pvalue} as our evaluation metric and concluded that results were statistically significant when $p \textless 0.05$.


\subsubsection{OS Overhead}
\label{ssec:syscallovehead}
Similar to evaluations in other auditing frameworks~\cite{paccagnella2020logging}, we used \textit{lmbench}~\cite{mcvoy1996lmbench} to measure the OS overhead of \toolname. On average, the OS overhead of \toolname was 35\% higher than vanilla Linux across eight different hardware configurations. We also showed the relative overhead percentage compared to Sysdig for four configurations in Table~\ref{tab:lmbench} and left the results for other configurations in our GitHub repository.

Our experiments show that the OS overhead of \toolname is similar to that of Sysdig. On average, the OS overhead of \toolname is 0.2\% lower than Sysdig across eight different hardware configurations. Overall, we cannot find a statistically significant difference between Sysdig and \toolname. We conclude that the differences between the OS overhead of \toolname and Sysdig are mainly due to the randomness of a dynamic system. The OS overhead measures the performance of the kernel modules of \toolname and Sysdig. Since \toolname implements its kernel module in the same way as Sysdig, their OS overhead should be the same.

\begin{table}[h]
    \centering
    \footnotesize
    \caption{\small Performance scores of \textit{lmbench}. \emph{All values are shown as percentages relative to Sysdig. The negative value means \toolname is faster than Sysdig.}}
      \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lrrrrr}
    \toprule
Configurations & 
    C1 & C4 & C5 & C8 & Ave  \\ 
    \cmidrule{1-6}
    \multicolumn{6}{c}{\small Syscall Tests} \\
    \cmidrule{0-1}\cmidrule{2-6}
NULL syscall & 
    -8.1\% & -17\% & -8.3\% & -7.9\% & -10.3\% \\ 
stat & 
    -9.0\% & +5.5\% & -1.8\% & -0.6\% & -1.5\% \\
fstat & 
    +4.2\% & -1.7\% & +1.7\% & +1.6\% & +2.3\% \\ 
open/close file & 
    -6.1\% & -2.9\% & -0.3\% & -1.8\% & -2.8\% \\ 
read file & 
    +7.4\% & +7.1\% & +4.5\% & +7.2\% & +6.6\% \\ 
write file & 
    +7.7\% & +7.2\% & +12.5\% & +12.1\% & +9.9\% \\ 
    \cmidrule{0-1}\cmidrule{2-6}
    \multicolumn{6}{c}{\small File Access} \\
    \cmidrule{0-1}\cmidrule{2-6}
file create (0K) & 
    -15.8\% & -7.1\% & -10.0\% & +2.7\% & -7.5\% \\ 
file delete (0K) & 
    +0.5\% & +3.0\% & -0.7\% & -0.9\% & +0.5\% \\ 
file create (10K) & 
    +0.1\% & +2.9\% & -3.7\% & -0.8\% & -0.4\% \\ 
file delete (10K) & 
    +4.7\% & +1.5\% & -0.9\% & -0.4\% & +1.2\% \\ 
pipe & 
    +3.0\% & +0.8\% & +6.9\% & +1.3\% & +3.0\% \\ 
AF\_UNIX & 
    +3.8\% & -10.5\% & +5.3\% & +10.1\% & +2.2\% \\ 
    \bottomrule
    \end{tabular}
    }
\label{tab:lmbench}
\vspace{-1em}
\end{table}

\subsubsection{Application Overhead}
\label{sec:appoverhead}
To answer this research question, we conducted macro-benchmarks to measure the performance of seven applications, both with and without a super producer. These applications can be divided into two categories: the first category includes I/O-intensive benchmarks such as Nginx\cite{Nginx}, Redis\cite{redis}, and Postmark\cite{katcher1997postmark}, as well as two other applications, Django~\cite{py-django} for Python and http~\cite{go-http} for Golang. The second category includes CPU-intensive benchmarks, namely OpenSSL~\cite{openssl} and 7-ZIP~\cite{7zip}.


\begin{table*}[!ht]
\centering
\label{tab:new}
\begin{subtable}{0.45\textwidth}
\centering
\caption{\textbf{Benchmark WITHOUT super producer.}}
\label{tab:benchmarkidle}
    \begin{tabular}{l|l|rrrr} 

\hline
\textbf{Application}             & \textbf{Collector}                       & \textbf{C1}                                & \textbf{C4}                               & \textbf{C5}                                & \textbf{C8}                                               \\ 
\hline\hline
\multirow{3}{*}{Nginx}           & \toolname                                   & 9.80                                                & 3.60                                               & 11.35                                               & 4.84                                                \\
                                 & Sysdig                                   & 55.30                                               & 5.50                                               & 37.64                                               & 7.20                                                \\ 
\cline{2-6}
                                 & {\cellcolor[rgb]{0.906,0.906,0.906}}DIFF & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{-29.30} & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{-1.80} & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{-19.10} & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{-2.20}  \\ 
\hline
\multirow{3}{*}{Redis}           & \toolname                                   & 8.90                                                & 3.10                                               & 8.66                                                & 2.42                                                \\
                                 & Sysdig                                   & 21.00                                               & 5.00                                               & 21.00                                               & 5.70                                                \\ 
\cline{2-6}
                                 & {\cellcolor[rgb]{0.906,0.906,0.906}}DIFF & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-11.10 }         & \textbf{ {\cellcolor[rgb]{0.906,0.906,0.906}}-2.00  }        & \textbf{ {\cellcolor[rgb]{0.906,0.906,0.906}}-10.20    }      & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-3.10      }   \\ 
\hline
\multirow{3}{*}{Postmark}        & \toolname                                   & 25.50                                               & 19.10                                              & 30.60                                               & 20.65                                               \\
                                 & Sysdig                                   & 96.30                                               & 8.60                                               & 95.80                                               & 13.50                                                \\ 
\cline{2-6}
                                 & {\cellcolor[rgb]{0.906,0.906,0.906}}DIFF & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{-36.00} & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{9.70}  & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{-33.30} & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{6.30}   \\ 
\hline
\multirow{3}{*}{\makecell{Django \\ (Python)}} & \toolname                                   & 1.30                                                & 2.00                                               & 1.30                                                & -0.20                                               \\
                                 & Sysdig                                   & 1.10                                                & 2.30                                               & 1.10                                                & 0.30                                                \\ 
\cline{2-6}
                                 & {\cellcolor[rgb]{0.906,0.906,0.906}}DIFF & {\cellcolor[rgb]{0.906,0.906,0.906}}0.30            & {\cellcolor[rgb]{0.906,0.906,0.906}}-0.30          & {\cellcolor[rgb]{0.906,0.906,0.906}}0.20            & {\cellcolor[rgb]{0.906,0.906,0.906}}-0.50           \\ 
\hline
\multirow{3}{*}{\makecell{http \\ (Golang)}}   & \toolname                                   & 14.10                                               & 2.20                                               & 14.66                                               & 2.71                                                \\
                                 & Sysdig                                   & 78.90                                               & 2.20                                               & 65.70                                               & 2.20                                                \\ 
\cline{2-6}
                                 & {\cellcolor[rgb]{0.906,0.906,0.906}}DIFF & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{-36.20} & {\cellcolor[rgb]{0.906,0.906,0.906}}0.10           & {\cellcolor[rgb]{0.906,0.906,0.906}}\textbf{-30.80} & {\cellcolor[rgb]{0.906,0.906,0.906}}0.50            \\ 
\hline
\multirow{3}{*}{OpenSSL}         & \toolname                                   & 0.60                                                & 0.10                                               & 0.20                                                & 0.70                                                \\
                                 & Sysdig                                   & 0.60                                                & 0.10                                               & 0.20                                                & 0.60                                                \\ 
\cline{2-6}
                                 & {\cellcolor[rgb]{0.906,0.906,0.906}}DIFF & {\cellcolor[rgb]{0.906,0.906,0.906}}0.10            & {\cellcolor[rgb]{0.906,0.906,0.906}}0.00           & {\cellcolor[rgb]{0.906,0.906,0.906}}0.00            & {\cellcolor[rgb]{0.906,0.906,0.906}}0.10            \\ 
\hline
\multirow{3}{*}{7-ZIP}           & \toolname                                   & 0.30                                                & 0.80                                               & 1.20                                                & 0.70                                                \\
                                 & Sysdig                                   & 0.20                                                & 0.70                                               & 1.20                                                & 0.70                                                \\ 
\cline{2-6}
                                 & {\cellcolor[rgb]{0.906,0.906,0.906}}DIFF & {\cellcolor[rgb]{0.906,0.906,0.906}}0.10            & {\cellcolor[rgb]{0.906,0.906,0.906}}0.10           & {\cellcolor[rgb]{0.906,0.906,0.906}}0.00            & {\cellcolor[rgb]{0.906,0.906,0.906}}0.00            \\ 
\hline
\multirow{3}{*}{PostgreSQL}      & \toolname                                   & 7.05                                                & 3.80                                               & 10.20                                               & 4.70                                                \\
                                 & Sysdig                                   & 15.20                                               & 4.70                                               & 17.40                                               & 4.90                                                \\ 
\cline{2-6}
                                 & {\cellcolor[rgb]{0.906,0.906,0.906}}DIFF & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-7.61 }          & {\cellcolor[rgb]{0.906,0.906,0.906}}-0.80           & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-6.50   }        & {\cellcolor[rgb]{0.906,0.906,0.906}}-0.19           \\
\hline
\end{tabular}
\end{subtable}
\hspace{1cm}
% \hfill
\begin{subtable}{0.45\textwidth}
\centering
\caption{\textbf{Benchmark WITH super producer.}}
\label{tab:benchmarkbusy}
\begin{tabular}{rrrr} 
\hline
 \textbf{C1}                                & \textbf{C4}                               & \textbf{C5}                                & \textbf{C8}                                \\ 
\hline\hline
                               10.00                                      & 3.20                                      & 13.15                                      & 1.96                                       \\
                                                         51.93                                      & 6.28                                      & 58.70                                      & 3.20                                       \\ 
\cline{1-4}
                                 \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-27.60 }& \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-2.90} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-28.70 }& \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-1.20  }\\ 
\hline
                               3.70                                       & 1.20                                      & 4.23                                       & 0.27                                       \\
                                                  56.17                                      & 7.66                                      & 61.10                                      & 3.80                                       \\ 
\cline{1-4}
                          \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-33.60} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-6.00 }& \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-35.30 }& \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-3.40  }\\ 
\hline
                                   15.10                                      & 33.50                                     & 14.30                                      & 30.20                                      \\
                                                              65.37                                      & 6.37                                      & 68.58                                      & 7.43                                       \\ 
\cline{1-4}
                                 \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-30.40} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}25.50 }& \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-32.20} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}21.20  }\\ 
\hline
                               1.20                                       & 3.30                                      & 1.50                                       & 1.40                                       \\
                                                             41.14                                      & 2.18                                      & 47.74                                      & 0.90                                       \\ 
\cline{1-4}
                             \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-28.30} & {\cellcolor[rgb]{0.906,0.906,0.906}}1.10  & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-31.30} & {\cellcolor[rgb]{0.906,0.906,0.906}}0.50   \\ 
\hline
                              5.80                                       & 4.80                                      & 9.80                                       & 3.20                                       \\
                                                        47.56                                      & 0.48                                      & 47.38                                      & 0.10                                       \\ 
\cline{1-4}
                         \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-28.30} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}4.30}  & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-25.50} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}3.10  } \\ 
\hline
                                0.80                                       & 3.20                                      & 0.17                                       & 0.08                                       \\
                                            47.37                                      & 5.09                                      & 43.30                                      & 1.40                                       \\ 
\cline{1-4}
                             \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-31.60} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-1.80} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-30.10} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-1.30 } \\ 
\hline
                                   0.40                                       & 1.50                                      & 0.20                                       & 1.20                                       \\
                                                      50.30                                      & 4.86                                      & 38.02                                      & 3.16                                       \\ 
\cline{1-4}
                              \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-33.20} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-3.20 }& \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-27.40} & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-1.90 } \\ 
\hline
                                 11.60                                      & 4.50                                      & 12.20                                      & 6.20                                       \\
                                                               22.30                                      & 4.86                                      & 25.02                                      & 6.30                                       \\ 
\cline{1-4}
                \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-9.50 } & {\cellcolor[rgb]{0.906,0.906,0.906}}-0.34 & \textbf{{\cellcolor[rgb]{0.906,0.906,0.906}}-11.40} & {\cellcolor[rgb]{0.906,0.906,0.906}}0.00   \\
\hline
\end{tabular}
\end{subtable}
\caption{\textbf{We measured the processing time per request/transaction for seven representative applications and a Kubernetes-based PostgreSQL. For each application, the first two lines show the relative runtime overhead (\%) compared to vanilla Linux, where a lower value indicates performance closer to that of vanilla Linux. The third line shows the relative overhead between \toolname and Sysdig, with values smaller than 0 indicating that \toolname outperforms Sysdig. For brevity, we denote this as DIFF. We report the mean values across 10 runs, with p-values less than 0.05 shown in bold.}}
\end{table*}



We use the official benchmark tools with their default settings for different hardware configurations to evaluate their performance. We repeat each experiment 10 times and measure the average metrics reported by the benchmarks of each application. Specifically, We use \textit{wrk}~\cite{wrk} configured with 1,000 concurrent connections to benchmark Nginx. For Redis, we use the \textit{redis-benchmark} configured to send 1,000,000 requests and measure the speed of operation \code{get}. For Postmark, we use the built-in benchmark with the configuration of manipulating 500 files concurrently and launching 100,000 transactions. We rely on the Phoronix Test Suite, one of the most comprehensive benchmark suites of web applications~\cite{phoronix,sharath-ijca-13}, to benchmark Django and http. For OpenSSL, We use the built-in \textit{speed} benchmark configured to utilize all CPU cores and measure the time to compute one rsa4096 signature. For 7-ZIP, we use the built-in benchmark configured to utilize all CPU cores and measure the compression speed in MIPS. We report the relative cost of \toolname to Sysdig for four configurations in Table~\ref{tab:benchmarkidle} (without the super producer) and Table~\ref{tab:benchmarkbusy} (with the super producer). We also measure the overhead relative to the vanilla Linux with no auditing framework running at all. We leave the results of other configurations in our GitHub repository.

On average, the overhead of \toolname is 6.58\% higher than vanilla Linux and 6.30\% lower than Sysdig across eight different configurations. Our statistical analysis confirms the validity of our data. Both Sysdig and \toolname introduce overhead compared to vanilla Linux since they record and consume provenance events. However, the overheads of \toolname to vanilla Linux are relatively less, with overheads less than 15\% for all applications except Postmark. The reasons for the difference in application overheads between \toolname and Sysdig depend on hardware configurations and application categories.

For single-core machines (C1 and C5 in Table~\ref{tab:benchmarkidle} and Table~\ref{tab:benchmarkbusy}), \toolname is more efficient because it eliminates the process scheduling overhead. With a single-core, the OS needs to periodically switch to the Sysdig process for provenance data processing, which leads to higher application overhead. 

For multi-core machines, \toolname shows relatively high performance because it not only eliminates the process switching cost of Sysdig but also avoids cross-core data transmission. With multiple cores, the kernel module collects the provenance data on the same core as the running application. However, the Sysdig process accesses the data in parallel on a different core. This introduces a notable overhead of cache coherence across the two cores due to the shared provenance data buffer. On the contrary, \toolname processes the provenance data on the same core as the in-kernel collector, avoiding the cost of cross-core data transmission.

As shown in Tables~\ref{tab:benchmarkidle} and~\ref{tab:benchmarkbusy}, \toolname offers lower runtime overhead than Sysdig for applications with many I/O-intensive processes, such as Nginx, Redis, and http. If there are more I/O-intensive processes than CPU cores, the Sysdig auditing processes will compete for computational resources with the monitored applications. In other words, the auditing processes of Sysdig will interrupt the monitored apps in the same manner as \toolname. However, \toolname employs threadlets by design, which add less scheduling overhead than Sysdig. The fewer the number of CPU cores, the greater the scheduling overhead for Sysdig. Thus, \toolname is typically more effective when there are fewer CPU cores. Moreover, we find that as the number of CPU cores increases, the application's event generation speed decreases due to changes in application architecture. As a result, \toolname offers lower runtime overhead in C4 compared to C1. 

For CPU-intensive applications such as 7-ZIP, OpenSSL, and Django, they generate almost no system call events. Therefore, when there is no super producer, the overheads of both \toolname and Sysdig compared to vanilla Linux are much smaller, averaging less than 2\% across all configurations. When a super producer is running, the overhead of \toolname remains while the overhead of Sysdig increases. This is because the centralized Sysdig auditing process needs to process a large number of events generated by the super producer and will persistently compete for computational resources with the monitored application.

Sysdig may introduce less runtime overhead to the monitored process when there are spare CPU cores available to host the centralized auditing processes. In this case, there is no resource competition between the monitored and auditing processes. This is also the case for Postmark in C4 of Tables~\ref{tab:benchmarkidle} and~\ref{tab:benchmarkbusy} since our Postmark benchmark is single-threaded. Although \toolname shows higher runtime overhead for Postmark compared to Sysdig, it spares the core that would otherwise be used to host the Sysdig auditing process and prevents event dropping.

\noindent\textbf{Kubernetes-managed application.} To monitor IO-bound applications and complex systems, we combine PostgreSQL Operator with Pgpool-II to deploy a PostgreSQL cluster with query load balancing and connection pooling capability on Kubernetes ~\cite{pgpool}. We deploy a Pgpool-II pod that contains a Pgpool-II container and a Pgpool-II Exporter container. The Pgpool-II container Docker image is built with streaming replication mode. We set the replicas to 1, so we have three pods in total. To test the replication functionality, we use a benchmark tool called pgbench ~\cite{pgbench}, which comes with the standard PostgreSQL installation, and we measure the processing time per transaction. We repeat each test 10 times for all configurations, and each test lasts for 20 seconds.

Since we implemented the privilege escalation as mentioned in \S\ref{sec:consumer}, \toolname is able to monitor system behaviors inside the Docker container. The results are shown in Tables~\ref{tab:benchmarkidle} and~\ref{tab:benchmarkbusy}.  We find that for each configuration, \toolname introduces less than 15\% overhead compared to vanilla Linux. On average, \toolname introduces 3.34\% lower overhead than Sysdig. These results show that \toolname is suitable for monitoring Kubernetes-based deployments.

\subsection{RQ 4: Effectiveness of Data Reduction}
Several log reduction and partitioning techniques, such as CPR~\cite{10.1145/2976749.2978378}, LogGC~\cite{10.1145/2508859.2516731}, ProTracer~\cite{MaZX16}, and KCAL~\cite{10.5555/3277355.3277379}, do not solve the data integrity vs. performance dilemma because they add high computation overhead, amplifying PADoS attacks. To validate their ineffectiveness, we modified Sysdig's code by inlining the CPR algorithm into the kernel and called it Sysdig-CPR.

\textbf{Design of Sysdig-CPR:} Since the CPR algorithm is an offline algorithm that depends on the global properties of graphs, we maintain a temporary graph in an extra 8M kernel buffer for each CPU core. The design of Sysdig-CPR is similar to Sysdig-Integrity, which blocks the currently running process when the event buffer (kernel and userspace shared buffer) is full and wakes it up once the buffer has been processed. Sysdig-CPR does two additional things: it wakes up a kernel thread to run the CPR algorithm when the kernel buffer is full and copies the reduced events to the shared buffer for consumption; and it notifies the userspace component to consume the buffer. Sysdig-CPR is available at: \url{https://github.com/nodropforsecurity/sysdigcpr}

\textbf{Results:} Our measurement follows the configurations of \S\ref{ssec:slowdonw}. We omit Sysdig-CPR for clarity in Figure~\ref{fig:rq12-nginx-1+32} as well as our GitHub repository because we find that the performance curves of Sysdig-CPR in all configurations are stuck on the horizontal axis. In our experiment, the kernel CPR can handle 2,000 events per second per core, which is consistent with the original paper~\cite{10.1145/2976749.2978378}. Although it can reduce most of the events (more than 70\%), the super producer can easily generate 100,000 events per second. This means that the system will take several seconds to handle the generated events, which greatly blocks the running applications.

\subsection{RQ 5: Effectiveness of Increasing Buffer Size}

One possible approach to avoid event dropping in system auditing frameworks~\cite{sysdigcve,discuss1,discuss2} is to increase the buffer size. However, this approach is ineffective. We applied this approach to Sysdig, LTTng, and Linux Audit and evaluated them as follows. For each CPU core, we set the maximum applicable buffer size, which is 768M for Sysdig and LTTng and 77,000 messages for Linux Audit. We cannot increase the buffer size even larger because the system will crash when the buffer size exceeds the threshold. In our measurement shown in Table~\ref{tab:collectorbuffer}, Sysdig, LTTng, and Linux Audit still drop events with the same super producer configuration in \S\ref{ssec:eventdrop}. The dropping rate is 88\%, 52\%, and 99\% for Sysdig, LTTng, and Linux Audit when the generation speed per core reaches 1.6 million per second. Moreover, as reported by the developers of Sysdig, increasing the size of the ring buffer may significantly slow down the whole system~\cite{sysdigcve,buffersizeslow}. According to our experiment, when increasing buffer size from 8M to 768M, the events generation speed decreases by 35\% under the same stress test.

\begin{table}[t]
\setlength{\abovecaptionskip}{5pt}
 \caption{Dropping rate of auditing frameworks with maximum buffer size.}
    \label{tab:collectorbuffer}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Configuration}  & \textbf{C1}   & \textbf{C4}   & \textbf{C5}  &\textbf{C8}  \\ \hline
Linux Audit &  99.2\% & 99.6\% & 99.1\%& 99.4\%\\ \hline
Sysdig  &  19.3\% & 86.1\% & 25.5\% & 88.1\%\\ \hline
LTTng  &  0\% & 49.5\% & 0\% & 52.1\%\\ \hline
\end{tabular}
\end{table}
