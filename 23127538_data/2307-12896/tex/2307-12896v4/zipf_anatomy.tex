\documentclass[a4paper,12pt]{article}

\usepackage{amsmath}
\interdisplaylinepenalty=200
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[authoryear,round]{natbib}
\usepackage{graphicx}
\usepackage[OT4]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage[unicode,debug,colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black,pdfusetitle=true,breaklinks=true]{hyperref}

\newcommand{\bibnamefont}[1]{\MakeUppercase{#1}}
\newcommand{\toprule}{\hline}
\newcommand{\midrule}{\hline}
\newcommand{\botrule}{\hline}
\bibliographystyle{abbrvnat}

\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left|| #1 \right||}
\newcommand{\okra}[1]{\left( #1 \right)}
\newcommand{\join}[1]{\left\langle #1 \right\rangle}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\kwad}[1]{\left[ #1 \right]}
\newcommand{\klam}[1]{\left\{ #1 \right\}}
\newcommand{\subst}[1]{\left. #1 \right|}
\newcommand{\boole}[1]{{\bf 1}{\klam{#1}}}
\DeclareMathOperator{\mean}{\mathbf{E}}
\DeclareMathOperator{\var}{Var}

\title{Corrections of Zipf's and Heaps' Laws \\ Derived from Hapax
  Rate Models}

\author{{\L}ukasz D\k{e}bowski%
  \thanks{{\L}. D\k{e}bowski is with the Institute of Computer
    Science, Polish Academy of Sciences, ul.\ Jana Kazimierza 5,
    01-248 Warszawa, Poland (e-mail: ldebowsk@ipipan.waw.pl).}}
\date{}

\begin{document}
%\layout
\maketitle

\begin{abstract}
  The article introduces corrections to Zipf's and Heaps' laws based
  on systematic models of the proportion of hapaxes, i.e., words that
  occur once.  The derivation rests on two assumptions: The first one
  is the standard urn model which predicts that marginal frequency
  distributions for shorter texts look as if word tokens were sampled
  blindly from a given longer text.  The second assumption posits that
  the hapax rate is a simple function of the text length.  Four such
  functions are discussed: the constant model, the Davis model, the
  linear model, and the logistic model. It is shown that the logistic
  model yields the best fit.
  \\[1ex]
  \textbf{Keywords:} calculus, hapax rate, Heaps' law, urn model,
  Zipf's law
\end{abstract}

\section{Introduction}
\label{secIntroduction}

Zipf's law is the most famous and the oldest known statistical law
observed for texts in natural language.  It states that the $n$-th
most frequent word in a text appears approximately $n$ times less
often than the most frequent one.  This regularity was noticed by
\citet{Estoup16} for French % in 1916
and later recognized by \citet{Condon28} % in 1928
and \citet{Zipf35} % in 1935
for English. Similar power-law distributions were later observed in
other domains of empirical research such as ecology, sociology,
economics, and physics \citep{Zipf49}. Hence Zipf's law is often
considered a hallmark of complex systems. The literature of Zipf's law
is vast but scattered over venues devoted to diverse branches of
science. Many older historical references can be found in the online
bibliography by \citet{LiWWW}, whereas a departure point for a modern
statistically informed theory is provided by the monograph of
\citet{Baayen01}, which was influenced by a technical report by
\citet{Khmaladze88}.

In spite of the sheer size of existing literature, the question why
Zipf's law is so ubiquitous has not been fully answered.  Zipf's law
can be explained by qualitatively diverse mechanisms ranging from
disappointing monkey-typing models by \citet{Mandelbrot54} and
\citet{Miller57}, through preferential attachment by \citet{Simon55}
and game-theoretic considerations by \citet{HarremoesTopsoe05}, to our
own attempts of linking Zipf's law with semantics and information
theory \citep{Debowski21} as well as with deep learning and computable
deterministic sequences \citep{Debowski23}. There is a plethora of
explanations which look plausible but, upon closer scrutiny, ask for
deeper research. Once we scratch off the surface of linguistic
phenomena, we discover fundamental questions concerning randomness and
meaningfulness of human actions. Do randomness and meaning support
each other \citep{Weaver53} or not \citep{Shannon48}?  To what degree
is human choice random \citep{TanakaIshii21}? Is meaningfulness a
specific sort of randomness resembling the famous Omega number that is
unpredictable but encodes a large portion of mathematical knowledge
\citep{Chaitin05}?  Can we write a plausible story by combining coin
tossing with computation as suggested by recent advances of large
language models \citep{RadfordOther19,BrownOthers20}? If so, how can
we put all these partial insights together?

This paper sets out a more modest goal. The aim is to derive and
verify principled corrections to Zipf's and Heaps' laws for texts of
an arbitrary size. Heaps' law is a corollary of Zipf's law that
predicts the number of distinct words as a function of the text length
\citep{Herdan64,Heaps78}.  Our derivation of corrections stems from
two modeling assumptions:
\begin{itemize}
\item The first assumption is the standard urn model
  \citep{Khmaladze88,Baayen01} which states that the marginal word
  frequency distributions for shorter texts look as if word tokens
  were sampled blindly from a longer text.  In the limit of an
  unbounded source, the word distributions look as if texts were
  generated by a memoryless stochastic process. The urn model yields a
  surprisingly good prediction of the marginal distributions although
  it obviously fails at predicting statistical dependencies among
  consecutive words.
\item Our second assumption is that the hapax rate, i.e., the
  proportion of words appearing once, can be reasonably approximated
  by a simple analytic function of the text size. We will show that a
  simple parametric model for the hapax rate is enough to derive the
  type-token and rank-frequency plots that look more plausible than
  more popular but theoretically less informed models.  In particular,
  a two-parameter model is sufficient for texts of a size of a novel,
  where the hapax rate has a decaying shape. By contrast, adding one
  more parameter accounts for larger corpora of texts, where the hapax
  rate is $U$-shaped \citep{Fengxiang10} and a second regime of Zipf's
  law emerges \citep{FerrerSole01b,FerrerSole03,MontemurroZanette02}.
\end{itemize}

The statistical theory developed in this article draws from works by
\citet{Khmaladze88} and \citet{Baayen01} with some later independent
add-ons by \citet{Milicka09,Milicka13} and \citet{Davis18}. In our
contribution, we develop a few new formulae and, most importantly, we
identify the fundamental role of the hapax rate function in deriving
the type-token and rank-frequency relationships that correct Heaps'
and Zipf's laws. Consequently, we test how all these theoretically
derived formulae fit linguistic reality. It turns out that if we
manage to model the hapax rate well enough then the standard urn model
predicts the word frequency distributions surprisingly precisely.

An important linguistic problem in this modeling quest is whether the
size of the potential vocabulary of an individual language speaker can
be reasonably upper bounded by a constant.  The fairly good linear
model of the hapax rate that we will discuss in \S \ref{secLinear}
predicts that the maximal lexicon size is upper bounded by $22\, 026$
word types for the empirically motivated parameter $\gamma=0.05$. The
problem is that for large corpora of texts, we need to correct this
unimodal model with a mixture term, which predicts a $U$-shaped hapax
rate plot \citep{Fengxiang10}.  Language evolution in time and a
minute but non-zero probability of encountering awkward strings in
written texts \citep{Fengxiang10} effectively prevent an observation
of a hard bounded lexicon in large corpora.
% Here, we exclude resorting to a costly and unethical life-long
% longitudinal study of an individual, which would return a certain
% finite number but, given no statistical theory, would not say the
% definitive word about what else could have possibly happened.

Thus the boundedness of the potential vocabulary seems an illusion
that arises for novel-sized texts. The exact nature of this illusion
is worth further investigation, in our opinion.  As we have indicated,
a simple explanation of this phenomenon may be that an individual's
working lexicon is finite but open. This working lexicon evolves at a
certain longer time scale than the typical text size of a novel---by
incorporating new word types and disposing of infrequent ones.
Supplying a simple mathematical model of this lexicon evolution should
be feasible but we leave it for future research.

The organization of the paper is as follows. 
%\ref{secIntroduction}
In \S \ref{secPreliminaries} we present preliminaries: \S
\ref{secBasic} introduces the basic concepts in the study of word
frequency distributions, whereas \S \ref{secPerspective}
describes the fundamental challenges in modeling Zipf's law. In
\S \ref{secGeneral}, we expose the standard theory of the urn
model and a relatively less developed theory of the analytic
vocabulary size function: \S \ref{secUrn} deals with the finite
urn model, \S \ref{secMemoryless} concerns the memoryless source,
\S \ref{secConvergence} treats the stochastic convergence,
whereas \S \ref{secAnalytic} develops the theory of analytic
vocabulary growth and hapax rate. In \S \ref{secModels}, we
introduce four parametric models for the hapax rate: the constant
model in \S \ref{secConstant}, the Davis model in \S
\ref{secDavis}, the linear model in \S \ref{secLinear}, the
logistic model in \S \ref{secLogistic}, and mixture models in
\S \ref{secMixture}. In \S \ref{secExperiments}, we test
these models experimentally: \S \ref{secSetup} describes our
experimental setup, \S \ref{secResults} resumes the results,
whereas \S \ref{secDiscussion} offers the discussion. The article
is concluded in \S \ref{secConclusion}.

\section{Preliminaries}
\label{secPreliminaries}

In this section, we introduce the basic concepts in the study of word
frequency distributions and we develop a preliminary discussion of
Zipf's and Heaps' laws. We begin with some necessary definitions and
then we explain why the classical Zipf law can be only a theoretical
idealization.

\subsection{Basic entities}
\label{secBasic}

Suppose that we count certain objects in empirical data.  We assume
that these objects come in many shapes but we are able to tell that
some shapes are identical or similar enough to be treated as instances
of the same type.  To fix our attention, let us call these objects
words and let the empirical data be a text, i.e., a fixed sequence of
words. For each word $w$ and a text $\mathbf{t}=(t_1,t_2,...,t_n)$ we
define the frequency of word $w$ as
\begin{align}
  f(w):=\sum_{i=1}^n \boole{t_i=w}.
\end{align}
where $\boole{\text{true}}:=1$ and $\boole{\text{false}}:=0$.  The
vocabulary of text $\mathbf{t}$ is the set of words that appear in
this text, namely,
%\begin{align}
  $\mathbf{w}:=\klam{w: f(w)>0}$.
%\end{align}
  
To distinguish two meanings of the word ``word'', the elements of text
$\mathbf{t}$ will be called \emph{tokens}, whereas the elements of
vocabulary $\mathbf{w}$ will be called \emph{types}. The number of
tokens is denoted $n$, whereas the number of types is denoted $v$. We
may write
\begin{align}
  v&=\sum_{w\in\mathbf{w}} 1,  & n&=\sum_{w\in\mathbf{w}} f(w).
\end{align}
We use lower-case symbols, reserving upper-case ones for random
variables.

Let us proceed to the next important idea, namely, the notion of
ranks.  Let us introduce a total order on types
$w_1,w_2,...,w_v\in\mathbf{w}$, where $w_i\neq w_j$ for $i\neq j$, by
sorting them according to frequencies:
%\begin{align}
$f(w_1)\ge f(w_2)\ge ... \ge f(w_v)$. 
%\end{align}
We define the \emph{rank} of a word as
% \begin{align}
$r(w_k):=k$.
% \end{align}
% That is, the most frequent word $w_1$ has rank $1$, the second most
% frequent word $w_2$ has rank $2$, and so on.
We can also define the
\emph{frequency function} as
\begin{align}
  f_r:=f(w_r).
\end{align}
We may express $n=\sum_{r=1}^v f_r$.
%
The frequency function is sometimes inconvenient to evaluate. It is
simpler to evaluate a related entity, namely, the maximal rank with a
given frequency,
\begin{align}
  \label{RankFunction}
 r_k:=\max\klam{r: f_r\ge k}=\sum_{w\in\mathbf{w}} \boole{f(w)\ge f}. 
\end{align}
Function $k\mapsto r_k$ will be called the \emph{rank function}.

Subsequently, we will define the frequency spectrum, starting with the
sets of types that appear exactly $k$ times,
%\begin{align}
  $\mathbf{w}_k:=\klam{w: f(w)=k}$.
%\end{align}
Elements of $\mathbf{w}_1$, i.e., types that appear exactly once, are
called \emph{hapax legomena}, or succinctly \emph{hapaxes}. The number
of elements of $\mathbf{w}_k$ is denoted $v_k$. In particular, $v_1$
is the number of hapaxes. We may express
\begin{align}
  v&=\sum_{k=1}^\infty v_k,  & n&=\sum_{k=1}^\infty k v_k.
\end{align}
Sequence $(v_1,v_2,...)$ is called the \emph{frequency spectrum}. The
spectrum elements $v_k$ are sometimes called the frequencies of
frequencies.
%
Can we derive the rank function $k\mapsto r_k$ from the frequency
spectrum?  Indeed, we have $v_k=r_k-r_{k+1}$ and $r_1=v$, so
\begin{align}
  \label{RankFunctionV}
  r_f=v-\sum_{k=1}^{f-1} v_k.
\end{align}
This linear formula conveniently allows to compute the expectation
of the rank function when we consider random texts. Hence, we would
like to advocate that the rank function $f\mapsto r_f$ is a simpler
behaved object than the frequency function $r\mapsto f_r$.

\subsection{Zipf's law in perspective}
\label{secPerspective}

It is obvious that the frequency function $r\mapsto f_r$ is
decreasing.  Shall we expect a particular form of this function?
Observe that
\begin{align}
  fr_f=f\sum_{w\in\mathbf{w}} \boole{f(w)\ge f}
  \le
  \sum_{w\in\mathbf{w}} f(w)=n. 
\end{align}
Hence we obtain a \emph{harmonic bound} for the frequency and rank functions
\begin{align}
  f_r&\le\frac{n}{r}, & r_f&\le\frac{n}{f}.
\end{align}

The typical shape of the frequency function for orthographic words in
an average text written in natural language is relatively close to this
upper bound. Namely, it turns out that function $r\mapsto f_r$ is not
only decreasing but equals approximately
%\begin{align}
$f_r\approx C/r$,
%\end{align}
where $C$ is a constant.  This empirical relationship is called
\emph{Zipf's law}. It was discovered by \citet{Estoup16} and
\citet{Condon28} and popularized by \citet{Zipf35,Zipf49}.

In this section, we will interpret Zipf's law quite literally. We will
assume that
\begin{align}
  \label{Zipf}
  f_r=\floor{\frac{v}{r}}
\end{align}
and we will inspect mathematical consequences of formula
(\ref{Zipf}).

Let $k\mapsto r_k$ be the rank function given by
(\ref{RankFunction}). We have $r_k\approx v/k$. Hence,
\begin{align}
  \label{Lotka}
  v_k=r_k-r_{k+1}\approx
  \frac{v}{k}-\frac{v}{k+1}=\frac{v}{k(k+1)}.
\end{align}
Formula (\ref{Lotka}) is called \emph{Lotka's law} \citep{Lotka26}. It
is approximately correct for $v_k>1$ assuming the exact Zipf law
(\ref{Zipf}). In particular, the number of hapaxes for Zipf's law
approximately equals half the total number of types,
\begin{align}
 v_1\approx \frac{v}{2},
\end{align}
which is pretty large. In the reality, the \emph{hapax rate} $v_1/v$
differs significantly from $1/2$ \citep{Baayen01,Fengxiang10}, which
will be the base for our amendments in \S \ref{secModels}.

Let frequency $f_{\bullet}=f_{r_{\bullet}}$ be the minimal frequency
$f$ such that $v_f=1$. 
By Lotka's law (\ref{Lotka}), we may approximate
%\begin{align}
$f_{\bullet}\approx \sqrt{v}$ and $r_{\bullet}\approx \sqrt{v}$.
%\end{align}
Hence we may compute
\begin{align}
  n
  \approx
  \sum_{r=1}^{r_{\bullet}} f_r + \sum_{f=1}^{f_{\bullet}} f v_f
  \approx
  \sum_{r=1}^{\sqrt{v}} \frac{v}{r}+\sum_{f=1}^{\sqrt{v}}
  \frac{v}{f+1}
  &\approx
  % 2v(\log\sqrt{v}+\gamma)-v
  % \nonumber\\
  % &=
    v(\log v+2\gamma-1),
  \label{TokensZipf}
\end{align}
where $\log x$ is the natural logarithm of $x$ and Euler's gamma is
$\gamma\approx 0.577$.  In particular, the relative frequency of the
most frequent type is
\begin{align}
  \label{RelativeFrequency}
  \frac{f_1}{n}\approx \frac{1}{\log v+2\gamma-1},
\end{align}
which tends to zero for the number of types $v$ tending to infinity.

Since, for a given language, the most frequent word is usually a fixed
functional word then the relative frequency of the most frequent word
$f_1/n$ is quite stable and it does not depend strongly on a
particular text. Hence relationship (\ref{RelativeFrequency}) suggests
some corrections to Zipf's law depending on the text
size. \citet{Orlov82}, followed by \citet{Baayen01} and
\citet{Davis18}, supposed that Zipf's law (\ref{Zipf}) holds exactly
only for a text of a certain length.  Let $n$ be this specific length.
From (\ref{RelativeFrequency}), we may estimate the corresponding
number of types
\begin{align}
  \label{TypesZipf}
  v\approx \exp\okra{\frac{n}{f_1}-2\gamma+1},
\end{align}
which yields $v\approx 18\,883$ types for the empirically motivated
value $f_1/n\approx 0.1$ for English.  Consequently, by estimate
(\ref{TokensZipf}), we obtain $n\approx 83\,653$ tokens, which is
somewhat less than the length of the average novel.  For both shorter
and longer texts we should expect deviations.

To account for these deviations, various heuristic corrections driven
by empirical observations were proposed. For example, a correction
proposed by \citet{Mandelbrot54} reads
\begin{align}
  \label{Mandelbrot}
  f_r\approx \floor{\frac{B+v}{B+r}}^\alpha, \quad \alpha>1.
\end{align}
In particular, \citet{Debowski02c} combined Mandelbrot's correction
with Orlov's idea by adjusting parameters $\alpha$ and $B$ so that
$f_1/n$ be constant. However, \citet{FerrerSole01b,FerrerSole03}
discovered that the empirical data for large collections of texts are
closer to two regimes: Zipf's law (\ref{Zipf}) for small ranks and
Mandelbrot's correction (\ref{Mandelbrot}) with $\alpha\gg 1$ for
large ranks. \citet{MontemurroZanette02} observed that the second
regime can be sometimes closer to an exponential decay. The shape of
the frequency function tail depends on the composition of the
collection of texts, being steeper for a single-author collection and
decaying slower for a mixture of texts by many authors.

The heuristic Mandelbrot correction predicts a power-law growth of the
number of types. In fact, if Mandelbrot's correction
(\ref{Mandelbrot}) holds exactly and the proportion of the most
frequent type is constant, $f_1/n = p_1$, then we have
\begin{align}
 np_1=f_1= \okra{\frac{B+v}{B+1}}^\alpha.
\end{align}
Hence we obtain \emph{Herdan-Heaps' law}
\begin{align}
  \label{HerdanHeaps}
  v= (B+1)n^{1/\alpha}p_1^{1/\alpha}-B\approx Cn^{1/\alpha},
\end{align}
proposed independently by \citet{Herdan64} and \citet{Heaps78}.  This
derivation is approximate and prone to gross errors since the best fit
of the power-law type-token relationship $v\propto n^\beta$ yields
usually $\beta\approx 0.8$ \citep{Kornai02} or even $\beta\approx 0.6$
for spoken texts \citep{HernandezFenrandezOthers19,TorreOthers19},
whereas $1/\alpha$ estimated by fitting Mandelbrot's correction
(\ref{Mandelbrot}) is often closer to $1$.  Moreover, the number of
hapaxes predicted by Mandelbrot's correction is much larger than it
follows from Herdan-Heaps' law (\ref{HerdanHeaps}) via the urn model,
as we will discus in \S \ref{secConstant}.

\section{General theory}
\label{secGeneral}

In the following, we will present a more precise account of word
frequency distributions. We will expose the standard theory of the urn
model and a less developed theory of the analytic vocabulary size
function. These two theories describe the expected frequency spectrum
in function of the text length.  The content of this section is based
on works \citep{Khmaladze88,Baayen01,Milicka09,Milicka13,Davis18} with
some extensions of ours. We try to present a concise but systematic
exposition.

\subsection{Urn model}
\label{secUrn}

In the context of quantitative linguistic research, the standard urn
model is due to \citet{Khmaladze88} and \citet{Baayen01}.  It was
independently rediscovered by \citet{Milicka09,Milicka13} and
\citet{Davis18}.  The idea of this framework is that word frequency
distributions for a given part of a finite text look as if tokens were
selected blindly without replacement from an urn that contains all
tokens from the whole text. That is, if we are interested in the
marginal distribution of types, we can ignore the sequential order of
tokens.

Formally, suppose that we know the exact shape of the rank function
for a certain text $\mathbf{t}^*=(t^*_1,t^*_2,...,t^*_{n^*})$. We can derive the
expected values of the rank function for the random text of length
$n<n^*$ that is sampled without replacement from population
$\mathbf{t}$.  We introduce random variable being text
$\mathbf{T}=(T_1,T_2,...,T_{n})$ with random tokens $T_i$ that
are sampled from $\mathbf{t}=(t_1,t_2,...,t_{n^*})$ without replacement.
The probability distribution of $\mathbf{T}$ is
\begin{align}
  P(\mathbf{T}=\mathbf{t})
  &=
    \frac{\prod_{w} f^*(w)!/[f^*(w)-f(w)]!}{n^*!/(n^*-n)!},
\end{align}
where $f^*(w)$ and $f(w)$ are the frequencies of word $w$ in texts
$\mathbf{t}^*$ and $\mathbf{t}$ respectively.

Now let us consider sequence $\mathbf{B}=(B_1,B_2,...,B_{n})$
where $B_i:=\boole{T_i=w}$ for a fixed word $w$.  For brevity, let
$f^*:=f^*(w)$ and $f:=f(w)$. We derive
\begin{align}
  P(\mathbf{B}=\mathbf{b})
  &=
    \frac{[n^*-f^*]!/[n^*-n-f^*+f]!\cdot f^*!/[f^*-f]!}{n^*!/(n^*-n)!}
    %\nonumber\\
  %&
    =\binom{n^*-n}{f^*-f}/\binom{n^*}{f^*}
    ,
\end{align}
where the binomial coefficient is denoted
%\begin{align}
  $\binom{n}{k}:=\frac{n!}{k!(n-k)!}$.
%\end{align}
Let us denote the frequency of word $w$ in text $\mathbf{T}$ as
$F:=F(w)$.  The number of distinct sequences $\mathbf{b}$ that
induce a given frequency $f$ is $\binom{n}{f}$.  Hence we evaluate
\begin{align}
  P(F=f)
  &=
    \binom{n}{f}\binom{n^*-n}{f^*-f}/\binom{n^*}{f^*}
    .
\end{align}

Let $V$ be the number of types and $V_k$ be the spectrum elements for
text $\mathbf{T}$. We have
\begin{align}
  \label{VVk}
  V&=\sum_{w} \boole{F(w)>0},
  &
    V_k&=\sum_{w} \boole{F(w)=k}.                                 
\end{align}
Hence we derive the expectations
\begin{align}
  \label{ExpVVk}
  \mean V&=v^* - \sum_{k^*=1}^\infty v^*_{k^*} \binom{n^*-n}{k^*}/\binom{n^*}{k^*},
  &
  \mean V_k&=\sum_{k^*=k}^\infty v^*_{k^*} 
                 \binom{n}{k}\binom{n^*-n}{k^*-k}/\binom{n^*}{k^*},
\end{align}
where $v^*_k$ are the spectrum elements for text $\mathbf{t}^*$.
Expectations $\mean V$ and $\mean V_k$ are close to the empirical
values for natural language if we use observed values $v^*_k$
\citep{Milicka09,Milicka13}.

Similarly, we derive the expected rank function. Let $R_f$ be the rank
function for text $\mathbf{T}$, respectively. Using formula
(\ref{RankFunctionV}), the Chu-Vandermonde identity
\begin{align}
  \sum_{k=0}^{k^*} \binom{n}{k}\binom{n^*-n}{k^*-k}/\binom{n^*}{k^*}=1,
\end{align}
and putting $v_0:=0$, we obtain the expected rank function
\begin{align}
  \mean R_f
  =\mean V-\sum_{k=1}^{f-1} \mean V_{k}
  &=v^*-\sum_{k=0}^{f-1} \sum_{k^*=k}^\infty v^*_{k^*}
    \binom{n}{k}\binom{n^*-n}{k^*-k}/\binom{n^*}{k^*}
    \nonumber\\
  &=r^*_f -\sum_{k^*=f}^\infty v_{k^*} \sum_{k=0}^{f-1}
    \binom{n}{k}\binom{n^*-n}{k^*-k}/\binom{n^*}{k^*}
    ,
    \label{ExpectedRanks}
\end{align}
% $a\wedge b:= \min(a,b)$.
Hence, to obtain the expectation of $R_f$, we have to subtract from
$r^*_f$ the number of types appearing $\ge f$ times multiplied by the
probability of observing a given type $<f$ times.

\subsection{Memoryless source}
\label{secMemoryless}

This setting was also studied by \citet{Khmaladze88} and
\citet{Baayen01}. If we let the size of the urn tend to infinity,
preserving the proportion of tokens of each type then we obtain a
memoryless source. The relative frequencies tend to probabilities and
sampling without replacement becomes indistinguishable from sampling
with replacement. Each draw from the urn becomes a probabilistically
independent copy of previous draws.

Formally, we are given a certain probability distribution of word
types $w\mapsto p(w)$, where $p(w)\in[0,1]$ and $\sum_w p(w)=1$. The
theoretical vocabulary, i.e., the domain of function $w\mapsto p(w)$
may be countably infinite.  We consider a memoryless source which
emits sequences of words according to the probability distribution
$w\mapsto p(w)$.  We introduce random text
$\mathbf{T}=(T_1,T_2,...,T_{n})$ with random tokens $T_i$.  The
probability distribution of $\mathbf{T}$ is simply
%\begin{align}
  $P(\mathbf{T}=\mathbf{t})
  =
    \prod_{w}
    p(w)^{f(w)}$,
%\end{align}
where $f(w)$ is the frequency of word $w$ in text
$\mathbf{t}$. Defining sequence $\mathbf{B}=(B_1,B_2,...,B_{n})$ where
$B_i:=\boole{T_i=w}$ for a fixed word $w$ and putting $f:=f(w)$ and
$p:=p(w)$, We derive
%\begin{align}
  $P(\mathbf{B}=\mathbf{b})
  =
    p^{f}(1-p)^{n-f}$.
%\end{align}
Let us denote the frequency of word $w$ in text $\mathbf{T}$ as
$F:=F(w)$. Since the number of distinct sequences $\mathbf{b}$ that
induce a given frequency $f$ is $\binom{n}{f}$, we obtain the binomial
distribution
\begin{align}
  P(F=f)
  &=
    \binom{n}{f}\, p^{f}(1-p)^{n-f}.
    ,
\end{align}

Let $V$ be the number of types in text $\mathbf{T}$ and let $V_l$ be
the number of types with frequency $l$ in text $\mathbf{T}$.  By
(\ref{VVk}), we derive the expectations
\begin{align}
  \label{ExpVS}
  \mean V&=\sum_w [1-(1-p(w))^n]
           \approx \sum_w [1-e^{-np(w)}],
  \\
  \label{ExpVkS}
  \mean V_k&=\sum_w \binom{n}{k} p(w)^{k}(1-p(w))^{n-k}
             \approx \sum_w \frac{[np(w)]^k}{k!} e^{-np(w)},
\end{align}
where the approximations are valid for $n\gg 1$ by the asymptotic
convergence of the binomial distribution to the Poisson distribution.
Let $R_f$ be the rank function for text $\mathbf{T}$, respectively.
Using formula (\ref{RankFunctionV}), we obtain
\begin{align}
  \mean R_f
  =\mean V-\sum_{k=1}^{f-1} \mean V_{k}
    %&=\sum_{w} \kwad{1-\sum_{k=0}^{f-1}
    %\binom{n}{k} p(w)^{k}(1-p(w))^{n-k}}
    %\nonumber\\
    &\approx \sum_w \kwad{1-\sum_{k=0}^{f-1}
    \frac{[np(w)]^k}{k!}e^{-np(w)}}.
\end{align}

\subsection{Probabilistic convergence}
\label{secConvergence}

Let us consider the memoryless source as in the previous section. We
will briefly comment on the probabilistic convergence for this
model. For this aim, we introduce the absolute total order on types
$w_1,w_2,w_3,...$, where $w_i\neq w_j$ for $i\neq j$, by sorting them
according to probabilities:
%\begin{align}
 $p(w_1)\ge p(w_2)\ge p(w_3)\ge ...$ .
%\end{align}
Consequently, we define the \emph{theoretical rank} of a word defined as
%\begin{align}
  $r(w_k):=k$.
%\end{align}
We also define the \emph{theoretical probability function} and the
\emph{theoretical rank function},
\begin{align}
  p_r&:=p(w_r),
  &
    s_t&:=\max\klam{r: p_r\ge t}. 
\end{align}
Observe that $ts_t, rp_r\le\sum_{i=1}^\infty p(w_i)=1$. Hence, we
obtain a \emph{harmonic bound} for the theoretical probability and rank
functions, namely,
\begin{align}
  p_r&\le \frac{1}{r},
  &
    s_t&\le \frac{1}{t}.
\end{align}

Analogously, for the random text $\mathbf{T}=(T_1,T_2,...,T_{n})$, we
introduce a random total order on types $W_1,W_2,W_3,...$, where
$W_i\neq W_j$ for $i\neq j$, by sorting them according to the
empirical frequencies:
%\begin{align}
 $F(W_1)\ge F(W_2)\ge F(W_3)\ge ...$ . 
%\end{align}
Consequently, we define the \emph{empirical rank} of a word defined as
%\begin{align}
  $R(W_k):=k$.
%\end{align}
We also define the \emph{empirical frequency function} and the
\emph{empirical rank function},
\begin{align}
  F_r&:=F(W_r)
  &
  R_t&:=\max\klam{r: F_r\ge t}=R_{\ceil{t}}. 
\end{align}

For the memoryless source, we have $\mean F(w)=np(w)$ and
$\var F(w)=np(w)(1-p(w))$. By the strong law of large numbers,
relative empirical frequencies converge to probabilities,
\begin{align}
  \lim_{n\to\infty} \frac{F(w)}{n}=p(w) \text{ almost surely}.
\end{align}
If $p(w_1)> p(w_2)> p(w_3)> ...$ then the empirical ranking converges
to the theoretical one, 
\begin{align}
  \lim_{n\to\infty} W_k&=w_k  \text{ almost surely},
  &
  \lim_{n\to\infty} R(w)&=r(w) \text{ almost surely}.
\end{align}
Consequently, the empirical relative frequency function converges to
the theoretical probability function,
\begin{align}
  \label{SLLN}
  \lim_{n\to\infty} \frac{F_r}{n}&=p_r \text{ almost surely},
  &
  \lim_{n\to\infty} \frac{\mean F_r}{n}&=p_r,
\end{align}
where the second equality follows by the Lebesgue dominated
convergence since $F_r/n\le 1$.

The pointwise convergence (\ref{SLLN}) implies the analogous
convergence for the rank function,
%\begin{align}
$\lim_{n\to\infty} R_{nt}=s_t \text{ almost surely}$,
%\end{align}
since
\begin{align}
  R_{nt}=\max\klam{r: \frac{F_r}{n}\ge t}\xrightarrow[n\to\infty]{}
  \max\klam{r: p_r\ge t}=s_t.
\end{align}
Moreover, for an arbitrary size of the theoretical vocabulary, we
notice an upper bound
\begin{align}
 R_f=\sum_w\boole{F(w)\ge f}\le \sum_w \frac{F(w)}{f}.
\end{align}
Since $\sum_w F(w)=n$, we obtain a uniform bound $R_{nt}\le 1/t$.
Thus, also the convergence in expectation,
%\begin{align}
  $\lim_{n\to\infty} \mean R_{nt}=s_t$,
%\end{align}
holds by the Lebesgue dominated convergence.

\subsection{Analytic functions}
\label{secAnalytic}

The idea of analytic functions in the study of word frequency
distributions is due to \citet{Baayen01}. It was later independently
discovered by \citet{Davis18}.  Suppose that the texts are long, that
is, $n\gg 1$. Then for the memoryless source, we may approximate
formulae (\ref{ExpVS})--(\ref{ExpVkS}) as
\begin{align}
  \mean V&\approx g(n):= \sum_w [1-e^{-np(w)}],
  &
  \mean V_k&\approx g(n|k):= \sum_w \frac{[np(w)]^k}{k!} e^{-np(w)},
\end{align}
where functions $g(n)$ and $g(n|k)$ are analytic functions of a real
argument $n\ge 0$.

Function $g(n)$ that approximates the expected number of types for a
given text size $n$ will be called the \emph{vocabulary size
  function}.  Similarly, functions $g(n|k)$ approximate the expected
frequency spectrum.  It was explicitly observed by \citet[Definition
2.11]{Baayen01} for $k=1$ and independently by \citet{Davis18} for any
$k\ge 1$ that functions $g(n|k)$ can be expressed via the consecutive
derivatives of function $g(n)$,
\begin{align}
  g(n|k)=-\frac{(-n)^{k}}{k!} g^{(k)}(n).
  \label{Derivatives}
\end{align}
We may also express the expected rank function as
\begin{align}
  \mean R_f
  %=\mean V-\sum_{k=1}^{f-1} \mean V_k
  \approx
  g(n||f)
  &:=g(n)-\sum_{k=1}^{f-1} g(n|k)
    %\nonumber\\
  =g(n)+\sum_{k=1}^{f-1} \frac{(-n)^{k}}{k!} g^{(k)}(n).
  \label{Taylor}
\end{align}
Formula (\ref{Taylor}) can be easily remembered as the truncated
Taylor series for $g(0)$ expanded around point $n$. The take-away is
that if we can guess a certain analytic vocabulary size function
$g(n)$ then by taking the Taylor series thereof, we may evaluate also
the expected rank function $g(n||f)$ for any text size $n$. The
conditions for the vocabulary size function are
\begin{align}
  g(1)&=1, &
  g(n|k)&\ge 0
  \text{ for }
  k\ge 1, &
  g(0)&=g(n)-\sum_{k=1}^\infty g(n|k)=0.
  \label{GConditions}
\end{align}

The vocabulary size function and the expected frequency spectrum can
be expanded as the Taylor series also around an arbitrarily chosen
point $n^*$. We obtain
\begin{align}
  \label{GN}
  g(n)&=g(n^*)-\sum_{k^*=1}^\infty g(n^*|k^*) \okra{1-\frac{n}{n^*}}^{k^*},
  \\
  \label{GNk}
  g(n|k)&=\sum_{k^*=k}^\infty \binom{k^*}{k} g(n^*|k^*)
  \okra{\frac{n}{n^*}}^{k}\okra{1-\frac{n}{n^*}}^{k^*-k}.
\end{align}
These formulae were observed by \citet{Davis18}, who derived them as
approximations of equations (\ref{ExpVVk}) for the
finite urn model. In fact, to obtain formulae
(\ref{GN})--(\ref{GNk}), it suffices to approximate
$v^*\approx g(n^*)$, $v^*_{k^*}\approx g(n^*|k^*)$, and
%\begin{align}
 $\binom{r}{k}\approx \frac{r^k}{k!}$,
%\end{align}
which is valid for $n^*,n\gg k^*,k$.

Let us make another useful observation, which seems new. This arises
from investigating the rate of hapaxes. We may define the \emph{hapax
  rate function} $h(u)$ as the expected proportion of hapaxes to the
number of types,
\begin{align}
  h(\log n):=\frac{g(n|1)}{g(n)}
  \approx 
  \frac{\mean V_{1}}{\mean V}
  .
\end{align}
Variable $u=\log n$ is a natural choice of the argument for the hapax
rate function for the reason that becomes clear in a while.

Namely, we observe that the hapax rate function $h(u)$ carries the
same information as the vocabulary size function $g(n)$ since there is
a one-to-one correspondence between $h(u)$ and $g(n)$.  It is so since
solving the differential equation
\begin{align}
  n \frac{dg(n)}{dn} : g(n) = h(\log n),
\end{align}
we obtain $d\log g(n)/dn = h(\log n)/n$
%\begin{align}
%  \frac{d\log g(n)}{dn} = \frac{h(\log n)}{n}
%\end{align}
and consequently
\begin{align}
  \label{HUGN}
  g(n)=\exp\okra{\int_0^{\log n}h(u)du}.
\end{align}
In particular, the maximal number of types is finite if
%\begin{align}
$\int_0^\infty h(u)du<\infty$.
%\end{align}
Hence a quick glance at the plot of function $h(u)$ with respect to
variable $u=\log n$ can inform a guess whether the potential
vocabulary of a given text is finite or not. For example, if function
$h(u)$ follows a decaying linear trend then we may guess that the
potential vocabulary is finite.

In consequence, to derive the vocabulary size function $g(n)$ and the
expected rank function $g(n||f)$, it suffices to assume a certain form
of the hapax rate function $h(u)$. Since function $h(\log n)$ varies
slowly as a function of the text length $n$, it seems a convenient
object for approximations, as we will discuss thoroughly in \S
\ref{secModels}.  A necessary, though not sufficient, requirement for
the hapax rate function is that it remains in the unit interval,
$0\le h(u)\le 1$.  For example, for a constant function
$h(u)=\beta\in(0,1)$, we obtain a power-law growth of the vocabulary
$g(n)=\exp(\beta\log n)=n^\beta$, known as Herdan-Heaps' law
\citep{Herdan64,Heaps78} mentioned in \S \ref{secPerspective}. This
model is a sort of a baseline to be analyzed in \S \ref{secConstant}.

Before we delve into particular examples of the hapax rate function,
let us comment on the necessary conditions for this object.  In
general, for the vocabulary size function of form (\ref{HUGN}), we
obtain the frequency spectrum and the rank function
\begin{align}
  g(n|k)&= g(n) h(\log n|k),
  &
  g(n||f)&= -g(n) \sum_{k=0}^{f-1} h(\log n|k),
\end{align}
where we define recursively $h(u|0):=-1$ and
\begin{align}
  h(u|k)
  &:=\kwad{1-\frac{1}{k}\okra{1+h(u)+\frac{d}{du}}}h(u|k-1), \quad k\ge 1.
    \label{Recursion}
\end{align}
We notice that $h(u|1)=h(u)$.  Recursion (\ref{Recursion}) was
observed by \citet[equation (3.43)]{Baayen01}. Functions $h(u|k)$ are
called the \emph{relative spectrum elements} \citep[page
90]{Baayen01}.  Conditions (\ref{GConditions}) are equivalent to
conditions
\begin{align}
  \int_{-\infty}^0 h(u)du&=\infty,
  &
  h(u|k)&\ge 0 \text{ for } k\ge 1.
  \label{HConditions}
\end{align}
We note that if function $u\mapsto h(u)$ satisfies conditions
(\ref{HConditions}) then so does function $u\mapsto h(u-\alpha)$ but
we do not know whether function $u\mapsto h(\gamma u)$ must be an
admissible hapax rate function.  This is bad news since we do not have
an easy theoretical control of the slope of the hapax rate function.

\section{Hapax rate models}
\label{secModels}

In this section, a model will be understood as a particular choice of
hapax rate function $h(u)$ and implied functions $g(n)$, $g(n|k)$, and
$g(n||f)$ that follow by the formulae derived in \S \ref{secAnalytic}.
The empirical hapax rate function for texts of the size of novels, to
be discussed in \S \ref{secExperiments}, has usually a decaying
shape. It equals $1$ for the text length $n=1$ and it decays slowly
for the growing argument.
% , see Figure \ref{figHapaxEN}.
The dominating trend is approximately linear in terms of variable
$u=\log n$ and the slope of $h(u)$ is of an approximate magnitude
$-0.05$.
% , see Table \ref{tabPars}.
By contrast, for large corpora studied in
\citep{FerrerSole01b,FerrerSole03,MontemurroZanette02,Fengxiang10},
the empirical hapax rate function is $U$-shaped.
% , see Figure \ref{figHapaxU}.

Having this in mind, we propose four models of the hapax rate:
\begin{align}
  h_\beta(u)&=\beta,
  &&\text{(constant model)}
  \\
  h_\alpha(u)&=\frac{1}{u-\alpha}-\frac{1}{e^{u-\alpha}-1},
  &&\text{(Davis model)}
  \\
  h_{\alpha\gamma}(u)
            &=
              \begin{cases}
                1, & u<\alpha,
                \\
                1-\gamma (u-\alpha), & \alpha\le u\le \gamma^{-1}+\alpha,
                \\
                0, & u>\gamma^{-1}+\alpha,
              \end{cases}
  &&\text{(linear model)}
  \\
  h_{\alpha\beta\gamma}(u)&=\frac{1-\beta}{1+e^{\gamma
                            (u-\alpha)}}+\beta.
  &&\text{(logistic model)}
\end{align}

The constant model is a crude baseline that implies the exact
Herdan-Heaps law for any text size. The Davis model implies the exact
Zipf law for a certain text size and reproduces the general decreasing
trend of the hapax rate. The linear model is a more precise model of
the hapax rate than the Davis model but it is not analytic. The
logistic model corrects on this issue and additionally it allows to
model the saturation of the hapax rate for growing text sizes, which
may be necessary for large corpora.  In \S \ref{secConstant}--\S
\ref{secLogistic}, we will derive corresponding functions $g(n)$,
$g(n|k)$, and $g(n||f)$ for these four models.  Subsequently, the
models will be empirically tested in \S \ref{secExperiments} on
selected texts from Project Gutenberg.

\subsection{Constant model}
\label{secConstant}

The constant model consists in a constant hapax rate
function,
\begin{align}
  \label{HerdanH}
  h(u)=\beta\in(0,1].
\end{align}
This model is a baseline since the argument of the hapax rate function
equals variable $u=\log n$, which varies slowly with the text size
$n$. Thus, in some relatively large region, we may assume that $h(u)$
is approximately constant.  A special case of the constant model is
the maximal model for $\beta=1$, where
\begin{align}
  h(u)&:=1,
  &
  g(n)&:=n,
  \\
  g(n|k)&:=\boole{k=1}n,
  &
  g(n||f)&:=\boole{f=1}n.
\end{align}
The maximal model arises when all tokens are of distinct types.

Now let us solve the constant model for $\beta\neq 1$.  By formula
(\ref{HUGN}), the constant model (\ref{HerdanH}) implies a power-law
growth of the vocabulary,
\begin{align}
  \label{HerdanG}
  g(n)=\exp(\beta\log n)=n^\beta.
\end{align}
As mentioned in \S \ref{secPerspective}, this is known as
Herdan-Heaps' law \citep{Herdan64,Heaps78}.  In view of the one-to-one
correspondence between functions $h(u)$ and $g(n)$, Herdan-Heaps' law
is equivalent to a constant rate of hapaxes, regardless of the text
size.  Let us define the binomial coefficients with a real upper
argument,
%\begin{align}
  $\binom{r}{k}:=r(r-1)...(r-k+1)/k!$.
%\end{align}
Differentiating function $g(n)=n^\beta$, we obtain the frequency
spectrum
\begin{align}
  g(n|k)=(-1)^{k+1}\binom{\beta}{k}\,n^\beta
  =-\binom{k-\beta-1}{k}\,n^\beta.
\end{align}
Applying the Taylor expansion (\ref{Taylor}), by induction, we derive
the expected rank function
\begin{align}
  \label{MandelbrotCorrected}
  g(n||f)
  =
  g(n)-\sum_{k=1}^{f-1} g(n|k)
  =
  \binom{f-\beta-1}{f-1}\,n^\beta.
\end{align}
We observe that for the constant model, the normalized rank function
$g(n||f)/g(n)$ is invariant with respect to the text size! It is a
sort of a scale-free distribution.
% By a scale-free distribution, we mean a distribution which does not
% distinguish any particular text length or a vocabulary size.

We may approximate the expected rank function
(\ref{MandelbrotCorrected}) as
\begin{align}
  \log g(n||f)
  % &=
  % \log \binom{f-\beta-1}{f-1}+\beta\log n
  %   \nonumber\\
  &=
  \sum_{k=1}^{f-1} \log\okra{1-\frac{\beta}{k}}+\beta\log n
  \nonumber\\
  &\approx
    -\sum_{k=1}^{f-1} \frac{\beta}{k}+\beta\log n
    \approx
  \beta\okra{\log \frac{n}{f-1}-\gamma}.
  \label{MandelbrotAsymptotic}
\end{align}
Approximation (\ref{MandelbrotAsymptotic}) resembles Mandelbrot's
formula (\ref{Mandelbrot}) in the middle-rank region.  However, the
expected rank function (\ref{MandelbrotCorrected}) is somewhat
different than Mandelbrot's heuristic correction (\ref{Mandelbrot}).
The discrepancy between formulae (\ref{Mandelbrot}) and
(\ref{MandelbrotCorrected}) arises both for very large and very small
ranks. In particular, the number of hapaxes is significantly larger
for the constant model than for Mandelbrot's formula
(\ref{MandelbrotCorrected}).

What kind of a memoryless source generates texts that follow the
constant model? Inverting approximation (\ref{MandelbrotAsymptotic}),
we obtain
\begin{align}
  \log\frac{f_r-1}{n}\approx -\frac{1}{\beta}\log r-\gamma.
\end{align}
Using the theory developed in \S \ref{secConvergence}, we see
that the theoretical probabilities satisfy
$p_r=\lim_{n\to\infty} f_r/n$. Hence we obtain the power-law
probability distribution
\begin{align}
  p_r\approx\frac{\exp(-\gamma)}{r^{1/\beta}}.
\end{align}

\subsection{Davis model}
\label{secDavis}

In \S \ref{secConstant}, we saw an example of a scale-free
word frequency distribution.  In this section, we will develop a model
described by \citet[pages 97--101]{Baayen01} and rediscovered by
\citet{Davis18}, which is not scale invariant and which reproduces the
exact Zipf law for the text size $n=1$. We call this model the Davis
model to have a more distinctive name. \citet[Chapter 3]{Baayen01}
considered many more related models based on series
(\ref{GN})--(\ref{GNk}) with a particular choice of the
frequency spectrum for an ideal text size $n$. The Davis model is a
special case of the Yule-Simon model \citep[pages 107-114]{Baayen01},
which in turn specializes the Waring-Herdan-Muller model \citep[pages
114-117]{Baayen01}.

The Davis model starts with an apparently eccentric formula for the
hapax rate function,
\begin{align}
  \label{DavisH}
  h(u)=\frac{1}{u}-\frac{1}{e^{u}-1},
\end{align}
where $\lim_{u\to -\infty} h(u)=1$, $\lim_{u\to 0} h(u)=1/2$, and
$\lim_{u\to \infty} h(u)=0$. Thus function (\ref{DavisH}) has a
decaying sigmoid shape.  To fit the Davis model to real data, we have
to rescale it using the offset operation
\begin{align}
    h_\alpha(u)&:= h(u-\alpha),
    &
    g_\alpha(n)&:=\frac{g(ne^{-\alpha})}{g(e^{-\alpha})},
    \\
    g_\alpha(n|k)&:=\frac{g(ne^{-\alpha}|k)}{g(e^{-\alpha})},
    &
    g_\alpha(n||f)&:=\frac{g(ne^{-\alpha}||f)}{g(e^{-\alpha})}.
\end{align}
where $\alpha$ is an empirically chosen real parameter.  Similar
offset operation with parameter $\alpha$ will be applied tacitly to
all subsequently discussed models. Obviously, parameter $\alpha$ makes
sense only for word frequency distributions that are not scale-free,
i.e., when $h(u)$ is not constant. Exactly, $\alpha$ is a location
parameter that selects a certain particular text length.

By (\ref{HUGN}), the Davis model (\ref{DavisH}) implies an
asymptotically logarithmic vocabulary growth,
\begin{align}
  \label{DavisG}
  g(n)
  =\frac{n\log n}{n-1}\approx \log n. 
\end{align}
To deal with the vocabulary size function $g(n)$, let
us consider the series
\begin{align}
  \sum_{m=1}^\infty \frac{x^m}{m(m+1)}
  =1+\frac{(1-x)\log(1-x)}{x},
  \quad
    \abs{x}\le 1.
\end{align}
Hence we may write the vocabulary size function and the frequency
spectrum as
\begin{align}
    g(n)&=1-\sum_{m=1}^\infty \frac{(1-n)^m}{m(m+1)},
  &
    g(n|k)&=
            \sum_{m=k}^\infty \binom{m}{k} \frac{n^{k}(1-n)^{m-k}}{m(m+1)},
            \quad
            k\ge 1.
\end{align}
Thus for the Davis model (\ref{DavisH}) and text size $n=1$, we obtain
Lotka's law (\ref{Lotka}) and Zipf's law (\ref{Zipf}) in a rescaled
form,
\begin{align}
  g(1|k)&=\frac{1}{k(k+1)},
  &
    g(1||f)&=\frac{1}{f}. 
\end{align}

Now we will evaluate the rank function $g(n||f)$ for any text size
$n$.  \citet{Davis18} evaluated the derivatives of $g(n)$ using a
computer algebra system without noticing a simple pattern that emerges
from the calculations. Let us present a brief derivation of this
regularity.  We can write the derivatives
\begin{align}
  \label{GPQ}
  g^{(k)}(n)
  &=\sum_{j=0}^k \binom{k}{j} p^{(j)}(n) q^{(k-j)}(n)
    ,
\end{align}
where
\begin{align}
  p^{(0)}(n)&=n\log n,
  & 
  p^{(1)}(n)&=1+\log n,
  \\
  p^{(j)}(n)&=\frac{(-1)^{j}(j-2)!}{n^{j-1}},
                   \quad
                   j\ge 2,
  &
  q^{(j)}(n)&=\frac{(-1)^{j}j!}{(n-1)^{j+1}}.
\end{align}

Let us define $s_{-1}:=0$, $s_{0}:=\log n$, $s_{j}:=-1/j$ for
$j\ge 1$, and $t:=1-1/n$.  Hence we may compute the rank function as
\begin{align}
  g(n||f)
  &=\sum_{k=0}^{f-1}\sum_{j=0}^k
    \frac{(-1)^jn^{j-1}p^{(j)}(n)}{j!}
    \cdot
    \frac{(-1)^{k-j}n^{k-j+1}q^{(k-j)}(n)}{(k-j)!}
  \nonumber\\
%  &=\sum_{k=0}^{f-1}\sum_{j=0}^k \kwad{s_{j}-s_{j-1}} t^{j-k-1}
%    \nonumber\\
  &=\sum_{j,i\ge 0:\,j+i\le f-1} \kwad{s_{j}-s_{j-1}} t^{-i-1}
%    \nonumber\\
  =\sum_{j,i\ge 0:\,j+i=f-1} s_{j} t^{-i-1}
    =\sum_{j=0}^{f-1} s_{j} t^{j-f}
    .
    \label{ZipfCorrectedFast}
\end{align}
Subsequently, we may rewrite formula (\ref{ZipfCorrectedFast}) as
expression
\begin{align}
  g(n||f)
  &=\frac{\log n-\sum_{j=1}^{f-1} \okra{1-1/n}^{j}/j
    }{\okra{1-1/n}^{f}}
    .
   \label{ZipfCorrected}
\end{align}
Formula (\ref{ZipfCorrected}) adjusts the ideal Zipf law for an
arbitrary text size.

As for the practical use of formula (\ref{ZipfCorrected}), we notice
the Taylor series
\begin{align}
  \log x = \sum_{j=1}^\infty \frac{\okra{1-1/x}^{j}}{j},
  \quad
  x\ge \frac{1}{2},
\end{align}
so the limit of expression (\ref{ZipfCorrected}) for $n\to 1$ is
indeed $1/f$, i.e., the ideal Zipf law. If we could compute the
logarithm function with an arbitrarily high precision then formula
(\ref{ZipfCorrected}) would imply an efficient algorithm for
computing the rank function plot for an arbitrary text length $n$. It
only takes $O(l)$ arithmetic operations to compute ranks $g(n||f)$ for
all frequencies $f\le l$. Indeed, for $n\neq 1$, we may write the
recursion
\begin{align}
  \label{ZipfRecursion}
  g(n||1)&= \frac{\log n}{1-1/n},
  &
  %\label{ZipfRecursionII}
  g(n||f+1)&= \frac{g(n||f)-1/f}{1-1/n},
       \quad
       f\ge 1,
\end{align}
whereas for $n\ge 1/2$, we may also consider summing the series
\begin{align}
  g(n||f)=\sum_{j=0}^\infty \frac{\okra{1-1/n}^{j}}{j+f},
  \label{ZipfCorrectedSeries}
\end{align}
which is considerably slower but more precise. We notice some beauty
of recursion (\ref{ZipfRecursion}), where numerator $g(n||f)-1/f$ is
the deviation of the rank function from the exact Zipf law and
denominator $1-1/n$ is the deviation of the text size from the ideal.

What kind of a memoryless source generates texts that follow the Davis
model? Formula (\ref{ZipfCorrectedSeries}) for large $n$ can be
approximated as
\begin{align}
  g(n||f)
  &\approx
  \sum_{j=0}^\infty
    \frac{\exp\okra{-{j}/{n}}}{{j}/{n}+{f}/{n}}\cdot\frac{1}{n}
    %\nonumber\\
  \approx
  \int_0^\infty \frac{\exp(-u)}{u+{f}/{n}}du
  =
  \exp\okra{\frac{f}{n}}\Gamma\okra{0,\frac{f}{n}},
\end{align}
where the incomplete gamma function is
%\begin{align}
$\Gamma(a,x):=\int_x^\infty t^{a-1}\exp(-t)dt$.
%\end{align}
Hence, for the theoretical probabilities
$p_r=\lim_{n\to\infty} f_r/n$, we obtain
%\begin{align}
  $\exp\okra{p_r}\Gamma\okra{0,p_r}\approx r$.
%\end{align}
Since we have 
\begin{align}
  \Gamma(0,x)=-\gamma-\log x-\sum_{j=0}^\infty\frac{(-x)^j}{j(j!)},
\end{align}
then for small probabilities $p_r$, we obtain
$\exp\okra{p_r}\Gamma\okra{0,p_r}\approx -\gamma-\log p_r$. Hence the
theoretical probabilities decay exponentially asymptotically,
%\begin{align}
 $p_r\approx \exp(-r-\gamma)$.
%\end{align}

\subsection{Linear model}
\label{secLinear}

To keep the theory consistent, functions $g(n)$ and $h(u)$ are assumed
to be analytic functions that satisfy conditions (\ref{GConditions})
and (\ref{HConditions}), respectively.  On the other hand, as we have
mentioned, the empirical hapax rate function has usually a decaying
shape for moderately sized texts. Moreover, as the empirical data
discussed in \S \ref{secExperiments} indicate, this trend is
approximately linear in terms of variable $u=\log
n$. %, see Figure \ref{figHapaxEN} and Table \ref{tabPars}.
Therefore, we may be tempted to propose the following piecewise linear
model of the hapax rate function,
\begin{align}
  \label{LinearH}
  h(u)&=
        \begin{cases}
          1, & u<0,
          \\
          1-\gamma u, & 0\le u\le \gamma^{-1},
          \\
          0, & u>\gamma^{-1}.
        \end{cases}
\end{align}
where $\gamma>0$. Of course, this model is not an analytic function
and we have no guarantee that conditions (\ref{GConditions}) and
(\ref{HConditions}) are satisfied even for $0\le u\le \gamma^{-1}$.

Nonetheless, let us proceed with calculations and let us derive the
corresponding ill-defined vocabulary size function $g(n)$ and the
relative spectrum elements $h(u|k)$. By (\ref{HUGN}), 
\begin{align}
  g(n)&=
        \begin{cases}
          n, & n\le 1,
          \\
          n^{1-\frac{1}{2}\gamma\log n}, & 1\le n\le \exp(\gamma^{-1}),
          \\
          \sqrt{\exp(\gamma^{-1})}, & n>\exp(\gamma^{-1}).
  \end{cases}
\end{align}
Thus the vocabulary size is bounded by a finite constant. For the
empirically motivated value $\gamma\approx 0.05$, the maximal
vocabulary size is $\sqrt{\exp(\gamma^{-1})}\approx 22\,026$ types and
the text length for which this limit is hit amounts to
$\exp(\gamma^{-1})\approx 4.85\cdot 10^8$ tokens. We note that this
maximal vocabulary size is close in the magnitude to the ideal size of
the vocabulary estimated in (\ref{TypesZipf}).

For $0\le u\le \gamma^{-1}$, (\ref{Recursion}) implies that
the relative spectrum elements are polynomials,
\begin{align}
  \label{LinearPoly}
  h(u|k)&=\frac{1}{k!} \sum_{m=0}^k a_{km}u^m,
\end{align}
where we have the recursion $a_{km}=0$ for $m<0$ or $m>k$, $a_{00}=-1$, and 
\begin{align}
  \label{LinearRecursion}
  a_{km}&=\gamma a_{k-1,m-1}+(k-2)a_{k-1,m}-(m+1)a_{k-1,m+1}
  % &=
  %   \begin{cases}
  %     0, & m<0 \text{ or } m>k,
  %     \\
  %     -1, & k=0 \text{ and } m=0,
  %     \\
  %     \gamma a_{k-1,m-1}+(k-2)a_{k-1,m}
  %     % \\
  %     % \quad
  %     -(m+1)a_{k-1,m+1}, &
  %     k\ge 1 \text{ and } 0\le m\le k.
  %   \end{cases}
\end{align}
for $k\ge 1$ and $0\le m\le k$.
%
From (\ref{LinearPoly}), we can compute the expected rank
function as explained in \S \ref{secAnalytic}.  We have not
seen a clear pattern in these polynomials that would solve the
recursion in a closed form.  We do not know what the range of $u$ is
such that $h(u|k)\ge 0$ for all $k\ge 1$.

\subsection{Logistic model}
\label{secLogistic}

The plain logistic model is unrealistic as a model of word frequency
distributions because of a too large slope but it can be analyzed
easily and, after a modification, it yields the best model that we
propose in this paper.  The plain logistic model assumes the familiar
logistic function as the hapax rate function,
\begin{align}
  \label{LogisticH}
  h(u)=\frac{1}{1+e^{u}},
\end{align}
where $\lim_{u\to -\infty} h(u)=1$, $\lim_{u\to 0} h(u)=1/2$, and
$\lim_{u\to \infty} h(u)=0$. Hence function (\ref{LogisticH}) has also
a decaying sigmoid shape.

By (\ref{HUGN}), the logistic model (\ref{LogisticH}) implies a
bounded vocabulary size function,
\begin{align}
  \label{LogisticG}
  g(n)
  =\frac{2n}{n+1}\xrightarrow[n\to\infty]{} 2.
\end{align}
Let us evaluate the corresponding rank function.  Like for the Davis
model, we can write the derivatives as (\ref{GPQ}) where
\begin{align}
  p^{(0)}(n)&=2n,
  &
  p^{(1)}(n)&=2,
  \\
  p^{(j)}(n)&=0,
                   \quad
                   j\ge 2,
  & 
    q^{(j)}(n)&=\frac{(-1)^{j}j!}{(n+1)^{j+1}}.
\end{align}
Hence we may compute the rank function as
\begin{align}
  g(n||f)
  &=g(n)+\sum_{k=1}^{f-1}\sum_{j=0}^1
    \frac{(-1)^jn^{j-1}p^{(j)}(n)}{j!}
    \cdot
    \frac{(-1)^{k-j}n^{k-j+1}q^{(k-j)}(n)}{(k-j)!}
  \nonumber\\
  &=\frac{2n}{n+1}+
    \sum_{k=1}^{f-1}\kwad{\frac{2n^{k+1}}{(n+1)^{k+1}}
    -\frac{2n^{k}}{(n+1)^{k}}}
  %\nonumber\\
  =\frac{2n^{f}}{(n+1)^{f}}.
\end{align}
Thus the rank function is a geometric series. Conditions
(\ref{GConditions}) and (\ref{HConditions}) are satisfied.

The linear model discussed in \S \ref{secLinear} suggests a
certain amendment of the logistic model, which makes it more
realistic. This modification is analytic in contrast to the linear
model. For analytic functions $h(u)$ and $g(n)$, in general, we may
try parameterizing
\begin{align}
  \label{ScalingHG}
  h_{\gamma\beta}(u)&:=(1-\beta)h(\gamma u)+\beta,
  &
  %\label{ScalingG}
  g_{\gamma\beta}(n)&:= [g(n^\gamma)]^{(1-\beta)/\gamma}n^\beta,
\end{align}  
with parameters $\gamma>0$ and $0\le\beta<1$ but it is not guaranteed
that the frequency spectrum elements still satisfy
$h_{\gamma\beta}(u|k)\ge 0$ and $g_{\gamma\beta}(u|k)\ge 0$. Anyway,
since we are in need to control the slope of the hapax rate $h(u)$ and
we wish it to be approximately linear, let us apply scaling operation
(\ref{ScalingHG}) to (\ref{LogisticH}) and try the rescaled sigmoid
function
\begin{align}
  \label{SLogisticH}
  h_{\gamma\beta}(u)=\frac{1-\beta}{1+e^{\gamma u}}+\beta.
\end{align}

By (\ref{HUGN}), the recaled model (\ref{SLogisticH}) implies an
asymptotically power-law growing vocabulary,
\begin{align}
  \label{SLogisticG}
  g_{\gamma\beta}(n)
  =\frac{2^{(1-\beta)/\gamma}n}{(n^\gamma+1)^{(1-\beta)/\gamma}}
  \approx 2^{(1-\beta)/\gamma} n^\beta.
\end{align}
Taking derivatives of function (\ref{SLogisticG}) can be a tedious
exercise. Like for the linear model, it is simpler to deal with
recursion (\ref{Recursion}) for the relative spectrum elements.
Cleverly, let us observe that the first derivative of function
(\ref{SLogisticH}) for $\beta=0$ is a polynomial of $h_{\gamma 0}(u)$,
\begin{align}
  \label{SLogisticHD}
  \frac{d}{du} h_{\gamma 0}(u)=-\gamma
  h_{\gamma 0}(u)[1-h_{\gamma 0}(u)].
\end{align}
% The value of the first derivative for $u=0$ and $\beta=0$ is
% $-\gamma/4$. It is the minimum. Since the slope of $h(u)$ for language
% is close to $-0.05$ then the optimal $\gamma$ should equal
% $0.2$ approximately.
% Table \ref{tabPars} informs us that the optimal $\gamma$ is close to
% $0.3$, however.

In view of observation (\ref{SLogisticHD}) and recursion
(\ref{Recursion}), rewritten as
\begin{align}
  h_{\gamma\beta}(u|k)
  &:=\kwad{1-\frac{1}{k}\okra{1+\beta+(1-\beta)h_{\gamma 0}(u)+
    \frac{d}{du}}}h_{\gamma\beta}(u|k-1) 
\end{align}
for $k\ge 1$, the relative spectrum elements $h_{\gamma\beta}(u|k)$
are polynomials of variable $h_{\gamma 0}(u)$,
\begin{align}
  \label{SLogisticPoly}
  h_{\gamma\beta}(u|k)&=\frac{1}{k!} \sum_{m=0}^k b_{km}[h_{\gamma 0}(u)]^m.
\end{align}
Since we can compute the derivative of a power of $h_{\gamma 0}(u)$ as
\begin{align}
  \frac{d}{du} [h_{\gamma 0}(u)]^m
  &=m [h_{\gamma 0}(u)]^{m-1} \frac{d}{du} h_{\gamma 0}(u)
  %\nonumber\\
  =-\gamma m [h_{\gamma 0}(u)]^m[1-h_{\gamma 0}(u)],
\end{align}
we obtain the recursion $b_{km}=0$ for $m<0$ or $m>k$, $b_{00}=-1$, and 
\begin{align}
  \label{SLogisticRecursion}
  b_{km}&=(-1+\beta-\gamma (m-1)) b_{k-1,m-1}+(k-1-\beta+\gamma m)b_{k-1,m}
%     &=
%     \begin{cases}
%       0, & m<0 \text{ or } m>k,
%       \\
%       -1, & k=0 \text{ and } m=0,
%       \\
%       (-1+\beta-\gamma (m-1)) b_{k-1,m-1}
%       \\
%       \quad+(k-1-\beta+\gamma m)b_{k-1,m}, &
%       k\ge 1 \text{ and } 0\le m\le k.
%     \end{cases}
\end{align}
for $k\ge 1$ and $0\le m\le k$.
%
From (\ref{SLogisticPoly}), we can compute the expected
rank function as in \S \ref{secAnalytic}.
 
\subsection{Mixture models}
\label{secMixture}

We can fit the models defined in \S \ref{secConstant}--\S
\ref{secLogistic} to empirical data.  However, simple decaying hapax
rate models that are good for moderately sized texts break down for
large text collections.  It is so since the decaying trend of the
hapax rate is inverted for sufficiently large corpora.  We observe a
$U$-shaped plot of the hapax rate \citep{Fengxiang10}. This second
regime in the hapax rate plot arises long before we can observe a hard
upper bound for the number of word types predicted by a decaying
model.

To model such a scenario, let us suppose that we have multiple
candidates for functions $h(u)$ and $g(n)$. We observe that these
functions can be combined via the mixture operation
\begin{align}
  h_\lambda(u)&:=\frac{\lambda h_1(u)g_1(e^{u})+(1-\lambda) h_2(u)g_2(e^{u})
                }{\lambda g_1(e^{u})+(1-\lambda) g_2(e^{u})},
  \\
  g_\lambda(n)&:= \lambda g_1(n)+(1-\lambda) g_2(n).
  % \\
  % g_\lambda(n|k)&:= \lambda g_1(n|l)+(1-\lambda) g_2(n|k),
  % \\
  % g_\lambda(n||f)&:= \lambda g_1(n||f)+(1-\lambda) g_2(n||f).
\end{align}
where $\lambda\in(0,1)$ is an empirically chosen parameter. 
% $g(n|k)$ and $g(n||f)$

We can easily reproduce a $U$-shaped plot of the hapax rate function,
observed by \citet{Fengxiang10}, with a mixture of a one-parameter
decaying model for the first regime and a constant model for the
second regime, see Figure \ref{figHapaxU}.  In fact, the emergence of
the second regime was explained by \citet{Fengxiang10} as an effect of
lexical trash, i.e., neologisms or other rare word types that do not
penetrate to the general vocabulary. Namely, besides typical
dictionary words, there is a non-zero probability of observing an
arbitrary random string of letters as a word token. If the pool of
such trash strings is infinite and the probability of re-occurrence of
each trash string is negligible then we may observe a $U$-shaped plot
of the hapax rate for extremely large corpora.  The constant model
with $\beta<1$ for lexical trash can be motivated by the monkey-typing
explanations of Mandelbrot's correction (\ref{Mandelbrot}) to Zipf's
law \citep{Mandelbrot54,Miller57}.  Moreover, with the constant model
using $\beta<1$ for lexical trash, we can predict the
asymptotic law (\ref{MandelbrotCorrected}) in the second regime of the
rank-frequency plot, see
\citep{FerrerSole01b,FerrerSole03,MontemurroZanette02}.

% Figure environment removed

\section{Experiments}
\label{secExperiments}

Consequently, we proceed to an experimental verification of our models
by comparing them with empirical word frequency distributions. There
is a formidable amount of textual data available in various languages
on the internet. Aware of this, we restrict ourselves to a pilot test
of our models. We are not expecting to confirm the absolute truth of
any particular model. We confine to setting up an experimental
methodology that can be extended in future research. We are fully
satisfied with a general conclusion that the urn model predicts the
word frequency distributions reasonably well if we plug in a simple
parametric hapax rate model. Our goal is to explore a few such hapax
rate models and to propose such a way of looking at word frequency
distributions that immediately suggests how to correct the ideal Zipf
and Herdan-Heaps laws as well as the Mandelbrot correction
(\ref{Mandelbrot}).

\subsection{Setup}
\label{secSetup}

While investigating particular texts, we will try to identify which of
the hapax rate models defined in \S \ref{secModels} predicts these
data most accurately. Our minimal requirement is to have the plain
frequency list for each source text.  This list should contain the
frequencies $f^*(w)$ for all word types $w$ in a given text of the
original length $n^*$.  In particular, hapaxes must not be omitted.

From the frequency list, we compute the rank function
$r^*_f:=\sum_w\boole{f^*(w)\ge f}$, the frequency spectrum elements
$v^*_k:=r^*_k-r^*_{k+1}$, the number of tokens
$n^*:=\sum_{k=1}^{\infty} k v^*_k$, and the number of types
$v^*:=\sum_{k=1}^{\infty} v^*_k$. We also consider the smoothed
vocabulary size function $g(n)$ and the smoothed frequency spectrum
function $g(n|k)$ defined via (\ref{GN})--(\ref{GNk}) for $n\le n^*$.
Functions $g(n)$ and $g(n|k)$ should be distinguished from the
incremental vocabulary size $G(n)$ and the incremental frequency
spectrum $G(n|k)$ defined as the vocabulary size and the frequency
spectrum for the first $n$ tokens of the empirical text.
Respectively, we also distinguish the smoothed hapax rate
$g(n|1)/g(n)$ and the incremental hapax rate $G(n|1)/G(n)$.

Whereas functions $G(n)$ and $G(n|k)$ fluctuate strongly, functions
$g(n)$ and $g(n|k)$ are smooth. Empirically, fitting models to the
smooth functions $g(n)$ and $g(n|k)$ yields a better prediction of the
rank function $r_f$ than fitting them to the incremental functions
$G(n)$ and $G(n|k)$. For this reason, in the following, we present the
results of fitting the models from \S \ref{secConstant}--\S
\ref{secLogistic} to the smoothed function $g(n)$.

\subsection{Results}
\label{secResults}

\begin{table}[t]
\caption{The selection of texts from Project
  Gutenberg.\label{tabTexts}}
\centering
\medskip
\begin{tabular}{|l|l|r|}
  \toprule
  Title &Author &File \\
  \midrule
  First Folio/35 Plays &W. Shakespeare & {00ws110.txt} \\
  One of Ours&W. Cather & {1ours10.txt} \\
  20,000 Leagues under the &J. Verne & {2000010.txt} \\
  \hfill Sea & & \\
  Critical \& Historical Essays &Macaulay & {2cahe10.txt} \\
  Five Weeks in a Balloon &J. Verne & {5wiab10.txt} \\
  Eight Hundred Leagues on &J. Verne & {800lg10.txt} \\
  \hfill the Amazon & & \\
  The Complete Memoirs &J. Casanova & {csnva10.txt} \\
  Memoirs &Comtesse du Barry & {dbrry10.txt} \\
  The Descent of Man &C. Darwin & {dscmn10.txt} \\
  Gulliver's Travels &J. Swift & {gltrv10.txt} \\
  The Mysterious Island &J. Verne & {milnd10.txt} \\
  Mark Twain, A Biography &A. B. Paine & {mt7bg10.txt} \\
  The Journal to Stella &J. Swift & {stlla10.txt} \\
  Life of William Carey &G. Smith & {wmcry10.txt} \\
  \botrule
\end{tabular}  
\end{table}

\begin{table*}[t]
  \caption{The parameters fitted by least squares to function
    $g(n)$.\label{tabPars}}
  \centering
  \medskip
  \footnotesize
  \begin{tabular}{|l|l|l|lll|ll|r|}
    \toprule
    File & Constant & Davis
    & \multicolumn{3}{c|}{Logistic}
    & \multicolumn{2}{c|}{Linear} & Length\\
    \cline{2-2}\cline{3-3}\cline{4-6}\cline{7-8}
    & $\beta$       & $\alpha$
    & $\gamma$      & $\beta$       & $\alpha$
    & $\gamma$      & $\alpha$      & $N$\\
\midrule
00ws110.txt     & 0.768 & 12.06 & 0.314 & 0.218 & 10.11 & 0.0509        & 2.14  & 835726\\
1ours10.txt     & 0.797 & 11.55 & 0.318 & 0.203 & 9.72  & 0.0507        & 1.7   & 128963\\
2000010.txt     & 0.801 & 11.48 & 0.323 & 0.008 & 10.62 & 0.0578        & 2.22  & 101247\\
2cahe10.txt     & 0.796 & 12.12 & 0.314 & 0     & 11.38 & 0.0576        & 2.79  & 298339\\
5wiab10.txt     & 0.808 & 11.64 & 0.315 & 0.001 & 10.86 & 0.0552        & 2.13  & 92558\\
800lg10.txt     & 0.799 & 11.43 & 0.327 & 0.162 & 9.77  & 0.0534        & 1.84  & 95493\\
csnva10.txt     & 0.732 & 11.39 & 0.308 & 0.157 & 9.94  & 0.0542        & 1.87  & 1268149\\
dbrry10.txt     & 0.787 & 11.39 & 0.325 & 0.065 & 10.31 & 0.0583        & 2.23  & 159710\\
dscmn10.txt     & 0.774 & 11.5  & 0.328 & 0     & 10.75 & 0.0629        & 2.71  & 312075\\
gltrv10.txt     & 0.796 & 11.4  & 0.322 & 0.001 & 10.62 & 0.0584        & 2.22  & 104909\\
milnd10.txt     & 0.773 & 11.14 & 0.347 & 0.127 & 9.63  & 0.0608        & 2.24  & 195064\\
mt7bg10.txt     & 0.775 & 11.91 & 0.296 & 0.001 & 11.45 & 0.0565        & 2.55  & 519886\\
stlla10.txt     & 0.757 & 10.91 & 0.333 & 0.231 & 8.87  & 0.0536        & 1.45  & 245882\\
wmcry10.txt     & 0.799 & 11.69 & 0.314 & 0     & 10.96 & 0.0567        & 2.34  & 145487\\
\midrule
Mean    & 0.783 & 11.54 & 0.32  & 0.084 & 10.36 & 0.0562        & 2.17  & 321678\\
\botrule
  \end{tabular}
\end{table*}


\begin{table}[t]
  \caption{The goodness of fit $\sqrt{\text{WSSR/ndf}}$ for
    function $g(n)$.\label{tabWSSRs}}
  \centering
  \medskip
  \begin{tabular}{|l|r|r|r|r|}
    \toprule
File    & Constant      & Davis & Logistic      & Linear\\
\midrule
00ws110.txt     & 1784.34       & 120.42        & \textbf{11.82}        & 43.71\\
1ours10.txt     & 478.53        & 74.39 & \textbf{7.02} & 16.69\\
2000010.txt     & 439.57        & 117.31        & \textbf{2.18} & 24.14\\
2cahe10.txt     & 1118.75       & 255.29        & \textbf{29.17}        & 86.71\\
5wiab10.txt     & 414   & 111.21        & \textbf{4.15} & 25.14\\
800lg10.txt     & 402.88        & 83.74 & \textbf{3.12} & 16.48\\
csnva10.txt     & 1721.89       & 107.98        & \textbf{6.86} & 34.72\\
dbrry10.txt     & 587.39        & 125.46        & \textbf{5.65} & 31.08\\
dscmn10.txt     & 982.09        & 215.02        & \textbf{19.93}        & 63.73\\
gltrv10.txt     & 463.34        & 117.35        & \textbf{6.86} & 31.39\\
milnd10.txt     & 629.47        & 125.02        & \textbf{1.88} & 23.22\\
mt7bg10.txt     & 1433.58       & 194.1 & \textbf{8.75} & 73.41\\
stlla10.txt     & 603.45        & 45.14 & 9.67  & \textbf{9.34}\\
wmcry10.txt     & 592.57        & 143.7 & \textbf{5.25} & 36.47\\
\midrule
Mean    & 832.27        & 131.15        & \textbf{8.74} & 36.87\\
\botrule
  \end{tabular}
\end{table}


Applying our experimental setup, we have processed 14 texts in English
downloaded from Project Gutenberg (\url{https://www.gutenberg.org/})
and listed in Table \ref{tabTexts}.  To normalize word types, the
texts were projected to 26 letters and a space (27 distinct characters
in total). We have fitted the models defined in \S \ref{secModels} to
the plot of function $g(n)$ defined in (\ref{GN}). The free parameters
of the corresponding hapax rate models are: $\alpha$ of the offset,
$\beta$ of the asymptote, and $\gamma$ of the scale.  The results of
fitting by least squares are presented in Table \ref{tabPars}.  The
quality of the fit can be witnessed in Table \ref{tabWSSRs}.  Figures
\ref{fig00ws110F} and \ref{fig00ws110R} depict the hapax rate, the
vocabulary size, the rank function, and the fitting residuals for the
three aforementioned functions for \emph{First Folio/35 Plays} by
William Shakespeare.  In the supplementary materials we present the
analogous graphs for the remaining texts from Table
\ref{tabTexts}. The supplementary scripts and data are available from
Github \citep{Debowski23f}.

% Figure environment removed

% Figure environment removed

\subsection{Discussion}
\label{secDiscussion}

The urn model combined with the analytic function theory provides a
good approximation method for empirical word frequency
distributions. This theoretical framework works well even when it is
combined with simple models of the hapax rate decay.  As for the four
considered hapax rate models, for moderately-sized texts from Project
Gutenberg, the constant model is the worst, the Davis model and the
ill-defined linear model seem better, whereas the logistic model is
usually the best, see Table \ref{tabWSSRs}.
  
Contrary to the usual assumption under Herdan-Heaps' law paradigm, the
hapax rate is not constant as a function of the text length. It
exhibits a prominent decaying trend that is approximately linear in
the logarithmic scale over many decades. However, the incremental
hapax rate exhibits large fluctuations about this trend. Also the
smoothed hapax rate follows three humps that recur roughly in the same
locations for all investigated texts. Are these humps significant? Do
they correspond to a decomposition of the lexicon into functional
words, common words, and proper names? This may be worth checking in
the future research.
  
The main take-away is that Herdan-Heaps' law is a crude approximation
of empirical word frequency distributions. This law implies that the
hapax rate is constant and almost twice larger than actually observed
for a given whole text. Obviously, this model is false.  Since the
decaying trend of the hapax rate is approximately linear over many
decades then the pure Davis model becomes also suboptimal for
sufficiently long texts. Although it may be necessary to consider
mixture models for larger corpora, we have not observed the $U$-shaped
plot for the selected texts from the Project Gutenberg corpus.

\section{Conclusion}
\label{secConclusion}

Zipf's and Heaps' laws are celebrated theoretical constructs but their
particular functional forms fail upon a closer scrutiny as a too crude
approximation. Borrowing the term from economics, they seem to
constitute stylized facts, i.e., broad tendencies that summarize the
data but ignore the details \citep{Kaldor61}. Thus one might doubt
whether there is a relatively simple but more accurate description of
the word frequency distributions.

Here we have advocated, however, that investigation of the hapax rate
function yields a direct insight where and why particular forms of
Zipf's and Heaps' laws do fail. Moreover, this investigation suggests
how to improve these laws systematically with relatively simple
parametric models. Thus we hope that the hapax rate plot may be a
convenient diagnostic tool for future research in quantitative
linguistics. The basic finding of this work is that the constant model
for the hapax rate is wrong and, easily, one can propose efficiently
computable but not completely obvious corrections to the idealized
Zipf-Mandelbrot and Herdan-Heaps laws. The standard urn model allows
to deduce them from a simple logistic hapax rate model and the
predictions are more precise than for the previously discussed Davis
model.

We suppose that while investigating large natural language corpora, it
may be necessary to consider mixture models where different components
of the lexicon are distinguished based on some linguistic criteria and
possibly they obey different hapax rate functions. In particular, we
may empirically research which components of the lexicon are closed or
open---or rather to what degree they are closed or open. Are they
asymptotically bounded (the logistic model with $\beta=0$), do they
grow logarithmically (the Davis model), or do they follow a
scale-invariant power-law (the constant model)? In fact,
\citet{Baayen01} tried to answer such sort of questions by
investigating productivity of affixes in particular. This work is an
extension of his theoretical ideas into the area of hapax rate
modeling. Our main goal was to provide a relatively self-contained
mathematical toolbox for future research.

\section*{Acknowledgments}

I thank Iv\'an Gonz\'alez Torre, Antoni Hern\'andez-Fern\'andez, and
Ji\v{r}\'\i{} Mili\v{c}ka for a fruitful discussion of the ideas
contained in this paper.

\section*{Prior venues}

This article is based on a presentation (abstract and slides)
\emph{Principled Analytic Corrections of Zipf's Law} delivered at the
Quantitative Linguistics Conference QUALICO 2023, Lausanne,
Switzerland, June 28-30, 2023. That presentation was co-authored by
Iv\'an Gonz\'alez Torre, who was about to contribute experimental data
for the Catalan language but in the end he did not and he opted out
from the full paper.

\section*{Funding}

This work received no funding from funding agencies.

\bibliography{0-publishers-full,0-journals-full,books,ql,nlp,ai,mine}

%\clearpage

\appendix

\section*{Supplementary materials}
  
This is the supplementary report for article ``Corrections of Zipf's
and Heaps' Laws Derived from Hapax Rate Models'', referred to as the
main article.  The supplementary scripts and data are available from
Github \citep{Debowski23f}. In this report, we present the remaining
obtained figures and an instruction how to rerun the experiment.

\subsection*{Running the experiment}

We worked on Linux Ubuntu 20.04.4 LTS applying a mixture of Bash,
Perl, and Gnuplot scripts.  We processed 14 texts in English
downloaded from Project Gutenberg (\url{https://www.gutenberg.org/})
and listed in Table 1 of the main article. These texts were projected
to 26 letters and a space (27 distinct characters in total),
compressed by \verb}gzip}, and placed into directory \verb}gutenberg/}
of the Github repository \citep{Debowski23f}.  The scripts for running
the experiment are located in directories \verb}scripts/} and
\verb}TypeToken/}. To repeat the experiment, it suffices to run:
\begin{verbatim}
cd ./scripts/
./make.bash
\end{verbatim}
Script \verb}make.bash} calls other scripts in directories
\verb}scripts/} and \verb}TypeToken/}, which apply Bash, Perl, and
Gnuplot. Prior to running the experiment, make sure that you have
installed these in your operating system.  In particular, the final
Latex arrays for Tables 1 and 3 of the main article are produced by
scripts
\begin{verbatim}
make_parameters_herdan_1.pl,
make_parameters_herdan_2.pl.
\end{verbatim}

The output files such as text tables and PDF figures are located in
the respective subdirectories of directory \verb}output/herdan/}. Each
Project Gutenberg text has its own directory, named
accordingly. Additionally, directory \verb}output/} contains the PDF
image for a plot of a $U$-shaped hapax rate function, Figure 1 of the
main article, which does not depend on empirical data.

\subsection*{Figures}

For each of the 14 texts, we produced three plots depicting: the hapax
rate function, the vocabulary size function (Heaps' law plot), and the
rank function (Zipf's law plot), and three plots depicting the fitting
residuals for each of the three aforementioned functions.  The
respective PDF images are located in the proper subdirectories of
directory \verb}output/herdan/}. We reproduce them as Figures
2 and 3 of the main article and Figures
\ref{fig1ours10F}--\ref{figwmcry10R} of the present report.


%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%

% Figure environment removed

% Figure environment removed
  
\end{document}

