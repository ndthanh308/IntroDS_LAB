\documentclass[a4paper,12pt]{article}
\usepackage{amsmath}
\interdisplaylinepenalty=200
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[dvips]{graphicx}
\usepackage[OT4]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{pgf}
%\usepackage{tikz}
%\usetikzlibrary{shapes,arrows,positioning}
\usepackage{url}
\usepackage{calc}
\usepackage{cancel}
\usepackage{bm}
\usepackage{setspace}
%\usepackage{logoipi}
%\usepackage[round]{natbib}
\usepackage[square,numbers]{natbib}
%\usepackage[round]{natbib}
% \usepackage[unicode,debug,colorlinks=true,linkcolor=red,citecolor=red,urlcolor=red,pdfusetitle=true,breaklinks=true]{hyperref}
\usepackage[unicode,debug,colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black,pdfusetitle=true,breaklinks=true]{hyperref}
%\usepackage{makeidx}
\emergencystretch=20pt

%\renewcommand{\bibsection}{\chapter*{\bibname}}
\newcommand{\bibnamefont}[1]{\MakeUppercase{#1}}
\newcommand{\toprule}{\hline}
\newcommand{\midrule}{\hline}
\newcommand{\botrule}{\hline}
\bibliographystyle{abbrvnat}

\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left|| #1 \right||}
\newcommand{\okra}[1]{\left( #1 \right)}
%\newcommand{\join}[1]{\left( #1 \right)}
\newcommand{\join}[1]{\left\langle #1 \right\rangle}
%\newcommand{\mean}[1]{\textbf{E}\kwad{#1}}
\newcommand{\mean}[1]{\mathbf{E}\, #1}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\kwad}[1]{\left[ #1 \right]}
\newcommand{\klam}[1]{\left\{ #1 \right\}}
\newcommand{\subst}[1]{\left. #1 \right|}
%\newcommand{\ind}{\mathrel{\perp\mkern-10mu\perp}}
%\newcommand{\norm}[1]{\left|\mkern-1mu\left| #1 \right|\mkern-1mu\right|}
\newcommand{\boole}[1]{{\bf 1}{\klam{#1}}}
%\DeclareMathOperator{\essinf}{ess\,inf}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\const}{const}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\Li}{Li}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\corr}{Corr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\peq}{\stackrel{+}{=}}
\newcommand{\pge}{\stackrel{+}{>}}
\newcommand{\ple}{\stackrel{+}{<}}
\newcommand{\meq}{\stackrel{*}{=}}
\newcommand{\mge}{\stackrel{*}{>}}
\newcommand{\mle}{\stackrel{*}{<}}

\title{Corrections of Zipf's and Heaps' Laws \\ Derived from Hapax
  Rate Models}

\author{{\L}ukasz D\k{e}bowski%
  \thanks{{\L}. D\k{e}bowski is with the Institute of Computer
    Science, Polish Academy of Sciences, ul.\ Jana Kazimierza 5,
    01-248 Warszawa, Poland (e-mail: ldebowsk@ipipan.waw.pl).}}

\date{}

\begin{document}
%\layout
\maketitle

\begin{abstract}
  The article introduces corrections to Zipf's and Heaps' laws based
  on systematic models of the hapax rate. The derivation rests on two
  assumptions: The first one is the standard urn model which predicts
  that marginal frequency distributions for shorter texts look as if
  word tokens were sampled blindly from a given longer text. The
  second assumption posits that the rate of hapaxes is a simple
  function of the text size. Four such functions are discussed: the
  constant model, the Davis model, the linear model, and the logistic
  model. It is shown that the logistic model yields the best fit.
  % 99 words
  \\[1ex]
  \textbf{Keywords:} calculus, hapax rate, Heaps' law, urn model, Zipf's law
\end{abstract}

\section{Introduction}
\label{secIntroduction}

Zipf's law is the most famous and the oldest known statistical law
observed for texts in natural language.  It states that the $n$-th
most frequent word in a text appears approximately $n$ times less
often than the most frequent one.  This regularity was noticed by
\citet{Estoup16} for French in 1916 and later recognised by
\citet{Condon28} in 1928 and \citet{Zipf35} in 1935 for
English. Similar power-law distributions were later observed in other
domains of empirical research, such as ecology, sociology, economics,
and physics \citep{Zipf49}. Hence Zipf's law is often considered a
hallmark of complex systems. The literature of Zipf's law is vast but
scattered over venues devoted to diverse branches of science. Many
older historical references can be found in the online bibliography by
\citet{LiWWW}, whereas a departure point for a modern statistically
informed theory is provided by the monograph of \citet{Baayen01},
which was inspired by a technical report by \citet{Khmaladze88}.

In spite of the sheer size of existing literature, the question why
Zipf's law is so ubiquitous has not been fully answered.  Zipf's law
can be explained by qualitatively diverse mechanisms ranging from
disappointing monkey-typing models by \citet{Mandelbrot54} and
\citet{Miller57}, through preferential attachment by \citet{Simon55}
and game-theoretic considerations by \citet{HarremoesTopsoe05}, to our
own attempts of linking Zipf's law with semantics and information
theory \citep{Debowski21} as well as with deep learning and computable
deterministic sequences \citep{Debowski23}. There is a plethora of
explanations which look plausible but, upon closer scrutiny, ask for
deeper research. Once we scratch off the surface of linguistic
phenomena, we discover fundamental questions concerning randomness and
meaningfulness of human actions. Do randomness and meaning support
each other \citep{Weaver53} or not \citep{Shannon48}?  To what degree
is human choice random \citep{TanakaIshii21}? Is meaningfulness a
specific sort of randomness resembling the famous Omega number that is
unpredictable but encodes a large portion of mathematical knowledge
\citep{Chaitin05}?  Can we write a plausible story by combining coin
tossing with computation as suggested by recent advances of large
language models \citep{RadfordOther19,BrownOthers20}? If so, how can
we put all these partial insights together?

This paper sets out a more modest goal. The aim is to derive and
verify principled corrections to Zipf's and Heaps' laws for texts of
an arbitrary size. Heaps' law is a corollary of Zipf's law that
predicts the number of distinct words as a function of the text length
\citep{Herdan64,Heaps78}.  Our derivation of corrections to Zipf's and
Heaps' laws stems from two main modelling assumptions:
\begin{itemize}
\item The first assumption is the standard urn model
  \citep{Khmaladze88,Baayen01} which states that word frequency
  distributions for shorter texts look as if word tokens were sampled
  at random from a given longer text.  In the limit of an unbounded
  source, marginal word frequency distributions look as if texts were
  generated by a memoryless stochastic process. The urn model yields a
  surprisingly good prediction of the marginal frequency distributions
  although it obviously fails at predicting statistical dependencies
  among consecutive word tokens.
\item Our second assumption is that the hapax rate, i.e., the
  proportion of words appearing once, can be reasonably approximated
  by a simple analytic function of the text size. We will show that a
  simple parametric model for the hapax rate is enough to derive the
  type-token and rank-frequency plots that look more plausible than
  more popular but theoretically less informed models.  In particular,
  a two-parameter model seems sufficient for texts of a size of a
  novel, where the hapax rate has a decaying shape. By contrast, a
  three-parameter model model may be good for larger corpora of texts,
  where the hapax rate is $U$-shaped \citep{Fengxiang10}, see also
  \citep{FerrerSole01b,FerrerSole03,MontemurroZanette02}.
\end{itemize}

The statistical theory developed in this article draws from works by
\citet{Khmaladze88} and \citet{Baayen01} with some later independent
add-ons by \citet{Milicka09,Milicka13} and \citet{Davis18}. In our
contribution, we develop a few new formulae and, most importantly, we
identify the fundamental role of the hapax rate function in deriving
the type-token and rank-frequency relationships that correct Heaps'
and Zipf's laws. Consequently, we test how all these theoretically
derived formulae fit the linguistic reality. It turns out that if we
manage to model the hapax rate well enough then the standard urn model
predicts the word frequency distributions surprisingly precisely.

Some linguistic problem in this quest is whether the size of the
potential vocabulary of an individual language speaker can be
reasonably upper bounded by a constant.  The logistic model that we
will discuss in \S \ref{secLogistic} predicts, for $\beta=0$, that the
vocabulary is asymptotically bounded and this model yields the best
fit for some individual novel-sized texts from Project Gutenberg. The
problem is that when we switch to whole corpora of texts then we need
to correct this model with a mixture term, $\beta>0$, that predicts a
$U$-shaped hapax rate plot \citep{Fengxiang10}.  The language
evolution in time and a minute but non-zero probability of observing
awkward arbitrary strings in written texts \citep{Fengxiang10}
effectively prevent an observation of a hard bounded lexicon for an
individual speaker. Here, we exclude resorting to a costly and
unethical life-long longitudinal study of an individual, which would
return a certain finite number but, given no statistical theory, would
not say the definitive word about what else could have possibly
happened. Thus the boundedness of the potential vocabulary is a
certain illusion that arises for novel-sized texts. The exact nature
of this illusion is worth further investigation, in our opinion. It
may be a reflection of the workings of human attention and memory.

The organisation of the paper is as follows. 
%\ref{secIntroduction}
In \S \ref{secPreliminaries} we present preliminaries: \S
\ref{secBasic} introduces the basic concepts in the study of word
frequency distributions, whereas \S \ref{secPerspective}
describes the fundamental challenges in modelling Zipf's law. In
\S \ref{secGeneral}, we expose the standard theory of the urn
model and a relatively less developed theory of the analytic
vocabulary size function: \S \ref{secUrn} deals with the finite
urn model, \S \ref{secMemoryless} concerns the memoryless source,
\S \ref{secConvergence} treats the stochastic convergence,
whereas \S \ref{secAnalytic} develops the theory of analytic
vocabulary growth and hapax rate. In \S \ref{secModels}, we
introduce four parametric models for the hapax rate: the constant
model in \S \ref{secConstant}, the Davis model in \S
\ref{secDavis}, the linear model in \S \ref{secLinear}, the
logistic model in \S \ref{secLogistic}, and mixture models in
\S \ref{secMixture}. In \S \ref{secExperiments}, we test
these models experimentally: \S \ref{secSetup} describes our
experimental setup, \S \ref{secResults} resumes the results,
whereas \S \ref{secDiscussion} offers the discussion. The article
is concluded in \S \ref{secConclusion}.

\section{Preliminaries}
\label{secPreliminaries}

In this section, we introduce the basic concepts in the study of word
frequency distributions and we develop a preliminary discussion of
Zipf's and Heaps' laws. We begin with some necessary definitions and
then we explain why the classical Zipf law can be only a theoretical
idealisation.

\subsection{Basic entities}
\label{secBasic}

Suppose that we count certain objects in empirical data.  We assume
that these objects come in many shapes but we are able to tell that
some shapes are identical or similar enough to be treated as instances
of the same type.  To fix our attention, let us call these objects
words and let the empirical data be a text, i.e., a fixed sequence of
words. For each word $w$ and a text $\mathbf{t}=(t_1,t_2,...,t_n)$ we
define the frequency of word $w$ as
\begin{align}
  f(w):=\sum_{i=1}^n \boole{t_i=w}.
\end{align}
where $\boole{\text{true}}:=1$ and $\boole{\text{false}}:=0$.  The
vocabulary of text $\mathbf{t}$ is the set of words that appear in
this text, namely,
\begin{align}
  \mathbf{w}:=\klam{w: f(w)>0}.
\end{align}
To distinguish two meanings of the word ``word'', the elements of text
$\mathbf{t}$ will be called \emph{tokens}, whereas the elements of
vocabulary $\mathbf{w}$ will be called \emph{types}. The number of
tokens is denoted $n$, whereas the number of types is denoted $v$. We
may write
\begin{align}
  v&=\sum_{w\in\mathbf{w}} 1,  & n&=\sum_{w\in\mathbf{w}} f(w).
\end{align}
We deliberately use lower-case symbols, reserving upper-case ones for
random variables to be introduced later.


Let us proceed to the next important idea, namely, the notion of
ranks.  Let us introduce a certain total order on types
$w_1,w_2,...,w_v\in\mathbf{w}$, where $w_i\neq w_j$ for $i\neq j$, by
sorting them according to frequencies:
\begin{align}
 f(w_1)\ge f(w_2)\ge ... \ge f(w_v). 
\end{align}
We will introduce a function called the \emph{rank} of a word defined as
\begin{align}
  r(w_k):=k.
\end{align}
That is, the most frequent word $w_1$ has rank $1$, the second most
frequent word $w_2$ has rank $2$, and so on. We can also define the
\emph{frequency function} as
\begin{align}
  f_r:=f(w_r).
\end{align}
We may express $n=\sum_{r=1}^v f_r$.

The frequency function is sometimes inconvenient to evaluate. There is
often a simpler formula for quite a related entity. Let us define the
maximal rank with a given frequency,
\begin{align}
  \label{RankFunction}
 r_k:=\max\klam{r: f_r\ge k}=\sum_{w\in\mathbf{w}} \boole{f(w)\ge f}. 
\end{align}
Function $k\mapsto r_k$ will be called the \emph{rank function}.

Subsequently, we will define the frequency spectrum, starting with the
sets of types that appear exactly $k$ times,
\begin{align}
  \mathbf{w}_k:=\klam{w: f(w)=k}.
\end{align}
Elements of $\mathbf{w}_1$, i.e., types that appear exactly once, are
called \emph{hapax legomena}, or succinctly \emph{hapaxes}. The number
of elements of $\mathbf{w}_k$ is denoted $v_k$. In particular, $v_1$
is the number of hapaxes. We may express
\begin{align}
  v&=\sum_{k=1}^\infty v_k,  & n&=\sum_{k=1}^\infty k v_k.
\end{align}
Sequence $(v_1,v_2,...)$ is called the \emph{frequency spectrum}. The
spectrum elements $v_k$ are sometimes called the frequencies of
frequencies.

Can we derive the rank function $k\mapsto r_k$ from the frequency
spectrum?  Indeed, we have $v_k=r_k-r_{k+1}$ and $r_1=v$, so
\begin{align}
  \label{RankFunctionV}
  r_f=v-\sum_{k=1}^{f-1} v_k.
\end{align}
This linear formula will conveniently allow to compute the expectation
of the rank function when we consider random texts. Hence, we would
like to advocate that the rank function $f\mapsto r_f$ is a better
behaved object than the plain frequency function $r\mapsto f_r$.

\subsection{Zipf's law in perspective}
\label{secPerspective}

It is obvious that the frequency function $r\mapsto f_r$ is
decreasing.  Shall we expect a particular form of this function?
Observe that
\begin{align}
  fr_f=f\sum_{w\in\mathbf{w}} \boole{f(w)\ge f}
  \le
  \sum_{w\in\mathbf{w}} f(w)=n. 
\end{align}
Hence we obtain a \emph{harmonic bound} for the frequency and rank functions
\begin{align}
  f_r&\le\frac{n}{r}, & r_f&\le\frac{n}{f}.
\end{align}
The typical shape of the frequency function for orthographic words in
an average text written in natural language is relatively close to this
upper bound. Namely, it turns out that function $r\mapsto f_r$ is not
only decreasing but equals approximately
\begin{align}
  \label{ZipfApprox}
  f_r\approx \frac{C}{r},
\end{align}
where $C$ is the normalising constant.  Relationship
(\ref{ZipfApprox}) is called \emph{Zipf's law}. It was discovered by
\citet{Estoup16} and \citet{Condon28} and popularised by
\citet{Zipf35,Zipf49}.  In this section, we will interpret Zipf's law
quite literally. We will assume that
\begin{align}
  \label{Zipf}
  f_r=\floor{\frac{v}{r}}
\end{align}
and we will inspect mathematical consequences of formula
(\ref{Zipf}).

Let $k\mapsto r_k$ be the rank function given by
(\ref{RankFunction}). We have $r_k\approx v/k$. Hence,
\begin{align}
  \label{Lotka}
  v_k=r_k-r_{k+1}\approx
  \frac{v}{k}-\frac{v}{k+1}=\frac{v}{k(k+1)}.
\end{align}
Formula (\ref{Lotka}) is called \emph{Lotka's law} \citep{Lotka26}. It
is approximately correct for $v_k>1$ assuming the exact Zipf law
(\ref{Zipf}). In particular, the number of hapaxes for Zipf's law
approximately equals half the total number of types,
\begin{align}
 v_1\approx \frac{v}{2},
\end{align}
which is pretty large. In the reality, the \emph{hapax rate} $v_1/v$
differs significantly from $1/2$ \citep{Baayen01,Fengxiang10}, which
will be the base for our amendments in \S \ref{secModels}.

Let frequency $f_{\bullet}=f_{r_{\bullet}}$ be the minimal frequency $f$
such that $v_f=1$.  Values $f_{\bullet}$ and $r_{\bullet}$ will be called
the medium frequency and the medium rank.  We may compute the number
of tokens as
\begin{align}
  n=\sum_{r=1}^{r_{\bullet}} f_r + \sum_{f=1}^{f_{\bullet}} f v_f.
\end{align}
By Lotka's law (\ref{Lotka}), we may approximate
\begin{align}
  f_{\bullet}&\approx \sqrt{v}, & r_{\bullet}&\approx \sqrt{v}.
\end{align}
Hence we may approximate pretty well
\begin{align}
  n\approx
  \sum_{r=1}^{\sqrt{v}} \frac{v}{r}+\sum_{f=1}^{\sqrt{v}}
  \frac{v}{f+1}
  &\approx 2v(\log\sqrt{v}+\gamma)-v
    \nonumber\\
  &=v(\log v+2\gamma-1),
  \label{TokensZipf}
\end{align}
where $\log x$ is the natural logarithm of $x$ and Euler's gamma is
$\gamma\approx 0.577$.  In particular, the relative frequency of the
most frequent type is
\begin{align}
  \label{RelativeFrequency}
  \frac{f_1}{n}\approx \frac{1}{\log v+2\gamma-1},
\end{align}
which tends to zero for the number of types $v$ tending to infinity.

Since, for a given language, the most frequent word is usually a fixed
functional word then the relative frequency of the most frequent word
$f_1/n$ is quite stable and it does not depend strongly on a
particular text. Hence relationship (\ref{RelativeFrequency}) suggests
some corrections to Zipf's law depending on the text
size. \citet{Orlov82}, followed by \citet{Baayen01} and
\citet{Davis18}, supposed that Zipf's law (\ref{Zipf}) holds exactly
only for a text of a specific length.  Let $n$ be such a text length
that Zipf's law holds exactly.  From (\ref{RelativeFrequency}), we may
estimate the corresponding number of types $v$ as
\begin{align}
  \label{TypesZipf}
  v\approx \exp\okra{\frac{n}{f_1}-2\gamma+1},
\end{align}
which yields $v\approx 18\,883$ types for the empirically motivated
value $f_1/n\approx 0.1$ for English.  Consequently, by estimate
(\ref{TokensZipf}), we obtain $n\approx 83\,653$ tokens, which is
somewhat less than the length of the average novel.  For both shorter
and longer texts we should expect some deviations.

To account for these deviations, various heuristic corrections driven
by empirical observations were proposed. For example, a correction
proposed by \citet{Mandelbrot54} reads
\begin{align}
  \label{Mandelbrot}
  f_r\approx \floor{\frac{B+v}{B+r}}^\alpha, \quad \alpha>1.
\end{align}
In particular, \citet{Debowski02c} combined Mandelbrot's correction
with Orlov's idea by adjusting parameters $\alpha$ and $B$ so that
$f_1/n$ be constant. However, \citet{FerrerSole01b,FerrerSole03}
discovered that the empirical data for large collections of texts are
closer to two regimes: Zipf's law (\ref{Zipf}) for small ranks and
Mandelbrot's correction (\ref{Mandelbrot}) with $\alpha\gg 1$ for
large ranks. \citet{MontemurroZanette02} observed that the second
regime can be sometimes closer to an exponential decay. The shape of
the frequency function tail depends on the composition of the
collection of texts, being steeper for a single-author collection and
decaying slower for a mixture of texts by many authors.

The heuristic Mandelbrot correction (\ref{Mandelbrot}) predicts a
power-law growth of the number of types. In fact, if Mandelbrot's
correction (\ref{Mandelbrot}) holds exactly and the proportion of the
most frequent type is constant, $f_1/n = p_1$, then we have
\begin{align}
 np_1=f_1= \okra{\frac{B+v}{B+1}}^\alpha.
\end{align}
Hence we obtain \emph{Herdan-Heaps' law}
\begin{align}
  \label{HerdanHeaps}
  v= (B+1)n^{1/\alpha}p_1^{1/\alpha}-B\approx Cn^{1/\alpha},
\end{align}
proposed independently by \citet{Herdan64} and \citet{Heaps78}.  This
derivation is approximate and prone to gross errors since the best fit
of the power-law type-token relationship $v\propto n^\beta$ yields
usually $\beta\approx 0.8$ \citep{Kornai02} or even $\beta\approx 0.6$
for spoken texts \citep{HernandezFenrandezOthers19,TorreOthers19},
whereas $1/\alpha$ estimated by fitting Mandelbrot's correction
(\ref{Mandelbrot}) is often closer to $1$.  Moreover, the number of
hapaxes predicted by Mandelbrot's correction is larger than it follows
from the Herdan-Heaps' law (\ref{HerdanHeaps}) via the urn model that
we will discuss subsequently.

\section{General theory}
\label{secGeneral}

In the following, we will present a more precise account of word
frequency distributions.  Namely, we will expose the standard theory
of the urn model and a less developed theory of the analytic
vocabulary size function. These two theories describe the expected
frequency spectrum in function of the text length.  The content of
this section is based on works
\citep{Khmaladze88,Baayen01,Milicka09,Milicka13,Davis18} with some
extensions of ours. We try to present a concise but systematic
exposition.

\subsection{Urn model}
\label{secUrn}

In the context of quantitative linguistic research, the standard urn
model is due to \citet{Khmaladze88} and \citet{Baayen01}.  It was
independently rediscovered by \citet{Milicka09,Milicka13} and
\citet{Davis18}.  The idea of this framework is that word frequency
distributions for a given part of a finite text look as if tokens were
selected blindly without replacement from an urn that contains all
tokens from the whole text. That is, if we are interested in the
marginal distribution of types, we can ignore the sequential order of
tokens.

Formally, suppose that we know the exact shape of the rank function
for a certain text $\mathbf{t}=(t_1,t_2,...,t_n)$. We can derive the
expected values of the rank function for the random text of length
$n'<n$ that is sampled without replacement from a larger population
$\mathbf{t}=(t_1,t_2,...,t_n)$ of length $n$.  We introduce random
variable being text $\mathbf{T}'=(T'_1,T'_2,...,T'_{n'})$ with random
tokens $T_i$ that are sampled from $\mathbf{t}=(t_1,t_2,...,t_n)$
without replacement.  The probability distribution of $\mathbf{T}'$ is
\begin{align}
  P(\mathbf{T}'=\mathbf{t}')
  &=
    \frac{\prod_{w\in\mathbf{w}} f(w)\cdot
    [f(w)-1]\cdot ...\cdot
    [f(w)-f'(w)+1]}{n\cdot
    [n-1]\cdot ...\cdot [n-n'+1]}
    \nonumber\\
  &=
    \frac{\prod_{w\in\mathbf{w}} f(w)!/[f(w)-f'(w)]!}{n!/(n-n')!},
\end{align}
where $f'(w)$ is the frequency of word $w$ in text $\mathbf{t}'$.

Now let us consider sequence $\mathbf{B}'=(B'_1,B'_2,...,B'_{n'})$
where $B'_i:=\boole{T'_i=w}$ for a fixed word $w$.  For brevity, let
$f:=f(w)$ and $f':=f'(w)$. We derive
\begin{align}
  P(\mathbf{B}'=\mathbf{b}')
  &=
    \frac{[n-f]!/[n-n'-f+f']!\cdot f!/[f-f']!}{n!/(n-n')!}
    \nonumber\\
  &=\binom{n-n'}{f-f'}/\binom{n}{f}
    ,
\end{align}
where the binomial coefficient is denoted
\begin{align}
  \binom{n}{k}:=\frac{n!}{k![n-k]!}.
\end{align}
Let us denote the frequency of word $w$ in text $\mathbf{T}'$ as
$F':=F'(w)$.  The number of distinct sequences $\mathbf{b}'$ that
induce a given frequency $f'$ is $\binom{n'}{f'}$.  Hence we evaluate
\begin{align}
  P(F'=f')
  &=
    \binom{n'}{f'}\binom{n-n'}{f-f'}/\binom{n}{f}
    .
\end{align}
In particular,
\begin{align}
  P(F'=0)
  &=\binom{n-n'}{f}/\binom{n}{f}.
\end{align}

Let $V'$ be the number of types in text $\mathbf{T}'$ and let $V'_l$
be the number of types with frequency $l$ in text $\mathbf{T}'$. We
have
\begin{align}
  V'&=\sum_{w\in\mathbf{w}} \boole{F'(w)>0}=v-\sum_{w\in\mathbf{w}} \boole{F'(w)=0},
  \\
    V'_{l}&=\sum_{w\in\mathbf{w}} \boole{F'(w)=l}.                                 
\end{align}
Hence we derive the expectations
\begin{align}
  \label{ExpV}
  \mean V'&=v - \sum_{k=1}^\infty v_k \binom{n-n'}{k}/\binom{n}{k},
  \\
  \label{ExpVl}
  \mean V'_{l}&=\sum_{k=l}^\infty v_k 
                 \binom{n'}{l}\binom{n-n'}{k-l}/\binom{n}{k}.
\end{align}
Expectations $\mean V'$ and $\mean V'_{l}$ are close to the empirical
size of the vocabulary for texts in natural language if we plug in
the observed values $v_k$ for a given text size, see
\citet{Milicka09,Milicka13}.

Similarly, we can derive the expectation of the rank function. Let
$F'_r$ and $R'_f$ be the frequency function and the rank function for
text $\mathbf{T}'$, respectively. Using formula (\ref{RankFunctionV})
and the Chu-Vandermonde identity
\begin{align}
  \sum_{l=0}^{k} \binom{n'}{l}\binom{n-n'}{k-l}/\binom{n}{k}=1,
\end{align}
we obtain the expected rank function
\begin{align}
  \mean R'_f
  =\mean V'-\sum_{l=1}^{f-1} \mean V'_{l}
  &=v-\sum_{l=0}^{f-1} \sum_{k=l}^\infty v_k
    \binom{n'}{l}\binom{n-n'}{k-l}/\binom{n}{k}
    \nonumber\\
  % &=v-\sum_{k=1}^\infty v_k \sum_{l=0}^{k\wedge (f-1)}
  %   \binom{n'}{l}\binom{n-n'}{k-l}/\binom{n}{k}
  %   \nonumber\\
  % &=v-\sum_{k=1}^{f-1} v_k -\sum_{k=f}^\infty v_k \sum_{l=0}^{f-1}
  %   \binom{n'}{l}\binom{n-n'}{k-l}/\binom{n}{k}
  %   \nonumber\\
  &=r_f -\sum_{k=f}^\infty v_k \sum_{l=0}^{f-1}
    \binom{n'}{l}\binom{n-n'}{k-l}/\binom{n}{k}
    ,
    \label{ExpectedRanks}
\end{align}
where $v_0=0$.
% and $a\wedge b:= \min(a,b)$.
That is, to obtain the expectation of $R'_f$, we have to subtract from
$r_f$ the number of types appearing $\ge f$ times multiplied by the
probability of observing a given type $<f$ times.

\subsection{Memoryless source}
\label{secMemoryless}

The subsequent setting was also studied by \citet{Khmaladze88} and
\citet{Baayen01}. If we let the size of the urn tend to infinity,
preserving the proportion of tokens belonging to each type, then we
obtain a memoryless source. The relative frequencies of types tend to
probabilities and sampling without replacement becomes
indistinguishable from sampling with replacement. Each draw from the
urn becomes a probabilistically independent copy of previous draws.

Formally, we suppose that we are given a certain probability
distribution of word types $w\mapsto p(w)$, where $p(w)\in[0,1]$ and
$\sum_w p(w)=1$. The theoretical vocabulary, i.e., the domain of
function $w\mapsto p(w)$ may be countably infinite.  In the following,
we will consider a memoryless source which emits sequences of words
according to the probability distribution $w\mapsto p(w)$.  We
introduce random text $\mathbf{T}=(T_1,T_2,...,T_{n})$ with random
tokens $T_i$.  The probability distribution of $\mathbf{T}$ is simply
\begin{align}
  P(\mathbf{T}=\mathbf{t})
  &=
    \prod_{w\in\mathbf{w}}
    p(w)^{f(w)},
\end{align}
where $f(w)$ is the frequency of word $w$ in text $\mathbf{t}$.

As in the urn model considerations, let us consider sequence
$\mathbf{B}=(B_1,B_2,...,B_{n})$ where $B_i:=\boole{T_i=w}$ for a
fixed word $w$.  For brevity, let $f:=f(w)$ and $p:=p(w)$. We derive
\begin{align}
  P(\mathbf{B}=\mathbf{b})
  &=
    p^{f}(1-p)^{n-f}.
\end{align}
Let us denote the frequency of word $w$ in text $\mathbf{T}$ as
$F:=F(w)$. The number of distinct sequences $\mathbf{b}$ that induce a
given frequency $f$ is $\binom{n}{f}$.  Hence we evaluate
\begin{align}
  P(F=f)
  &=
    \binom{n}{f}\, p^{f}(1-p)^{n-f}
    ,
\end{align}
which is the familiar binomial distribution.  In particular,
\begin{align}
  P(F=0)
  &=
    (1-p)^n.
\end{align}


Let $V$ be the number of types in text $\mathbf{T}$ and let $V_l$
be the number of types with frequency $l$ in text $\mathbf{T}$. We
have
\begin{align}
  V&=\sum_{w} \boole{F(w)>0}=
      \sum_{w} \kwad{1-\boole{F(w)=0}},
  \\
    V_l&=\sum_{w} \boole{F(w)=l}.                     
\end{align}
Hence we derive the expectations
\begin{align}
  \label{ExpVS}
  \mean V&=\sum_w [1-(1-p(w))^n]
           \approx \sum_w [1-e^{-np(w)}],
  \\
  \label{ExpVlS}
  \mean V_{l}&=\sum_w \binom{n}{l} p(w)^{l}(1-p(w))^{n-l}
             \approx \sum_w \frac{[np(w)]^l}{l!} e^{-np(w)},
\end{align}
where the approximations are valid for $n\gg 1$ by the asymptotic
convergence of the binomial distribution to the Poisson distribution.
Let $R_f$ be the rank function for text $\mathbf{T}$, respectively.
Using formula (\ref{RankFunctionV}), we obtain
\begin{align}
  \mean R_f
  =\mean V-\sum_{l=1}^{f-1} \mean V_{l}
    &=\sum_{w} \kwad{1-\sum_{l=0}^{f-1}
    \binom{n}{l} p(w)^{l}(1-p(w))^{n-l}}
    \nonumber\\
    &\approx \sum_w \kwad{1-\sum_{l=0}^{f-1}
    \frac{[np(w)]^l}{l!}e^{-np(w)}}.
\end{align}

\subsection{Probabilistic convergence}
\label{secConvergence}

Let us consider the memoryless source as in the previous section. We
will briefly comment on the probabilistic convergence for this model,
which has not been sufficiently discussed yet. For this aim, we
introduce the absolute total order on types $w_1,w_2,w_3,...$, where
$w_i\neq w_j$ for $i\neq j$, by sorting them according to
probabilities:
\begin{align}
 p(w_1)\ge p(w_2)\ge p(w_3)\ge ... .
\end{align}
Consequently, we define the \emph{theoretical rank} of a word defined as
\begin{align}
  r(w_k):=k.
\end{align}
We also define the \emph{theoretical probability function} 
\begin{align}
  p_r:=p(w_r)
\end{align}
and the \emph{theoretical rank function} of a real argument $t$,
\begin{align}
  s_t:=\max\klam{r: p_r\ge t}. 
\end{align}
Observe that $ts_t, rp_r\le\sum_{i=1}^\infty p(w_i)=1$. Hence, we
obtain a \emph{harmonic bound} for the theoretical probability and rank
functions, namely,
\begin{align}
  p_r\le \frac{1}{r},
  \quad
    s_t\le \frac{1}{t}.
\end{align}

Analogously, for the random text $\mathbf{T}=(T_1,T_2,...,T_{n})$, we
introduce a random total order on types $W_1,W_2,W_3,...$, where
$W_i\neq W_j$ for $i\neq j$, by sorting them according to the
empirical frequencies:
\begin{align}
 F(W_1)\ge F(W_2)\ge F(W_3)\ge ... . 
\end{align}
Consequently, we define the \emph{empirical rank} of a word defined as
\begin{align}
  R(W_k):=k.
\end{align}
We also define the \emph{empirical frequency function}
\begin{align}
  F_r:=F(W_r)
\end{align}
and the \emph{empirical rank function} of a real argument $t$,
\begin{align}
  R_t:=\max\klam{r: F_r\ge t}=R_{\ceil{t}}. 
\end{align}

For the memoryless source, we have $\mean F(w)=np(w)$ and
$\var F(w)=np(w)(1-p(w))$. By the strong law of large numbers, for
each type $w$, the relative empirical frequency converges to the
probability,
\begin{align}
  \lim_{n\to\infty} \frac{F(w)}{n}=p(w) \text{ almost surely}.
\end{align}
If $p(w_1)> p(w_2)> p(w_3)> ...$ then the empirical ranking converges
to the theoretical one, 
\begin{align}
  \lim_{n\to\infty} W_k&=w_k  \text{ almost surely},
  \\
  \lim_{n\to\infty} R(w)&=r(w) \text{ almost surely}.
\end{align}
Consequently, the empirical relative frequency function converges to
the theoretical probability function,
\begin{align}
  \label{SLLN}
  \lim_{n\to\infty} \frac{F_r}{n}&=p_r \text{ almost surely},
  \\
  \lim_{n\to\infty} \frac{\mean F_r}{n}&=p_r,
\end{align}
where the second equality follows from the first one by the Lebesgue
dominated convergence since $F_r/n\le 1$.

The pointwise convergence (\ref{SLLN}) implies the analogous
convergence for the rank function,
\begin{align}
  \lim_{n\to\infty} R_{nt}&=s_t \text{ almost surely},
\end{align}
since
\begin{align}
  R_{nt}=\max\klam{r: \frac{F_r}{n}\ge t}\xrightarrow[n\to\infty]{}
  \max\klam{r: p_r\ge t}=s_t.
\end{align}
Moreover, for an arbitrary size of the theoretical vocabulary, we
notice an upper bound
\begin{align}
 R_f=\sum_w\boole{F(w)\ge f}\le \sum_w \frac{F(w)}{f} 
\end{align}
by the Markov inequality. Since $\sum_w F(w)=n$, we obtain a uniform
bound
\begin{align}
  R_{nt}\le \frac{n}{nt}\le \frac{1}{t}.
\end{align}
Thus, also the convergence in expectation,
\begin{align}
  \label{RankFunctionE}
  \lim_{n\to\infty} \mean R_{nt}&=s_t,
\end{align}
holds by the Lebesgue dominated convergence for any memoryless source.

\subsection{Analytic functions}
\label{secAnalytic}

The idea of analytic functions in the study of word frequency
distributions is due to \citet{Baayen01}. It was later independently
discovered by \citet{Davis18}.  Suppose that the texts are long, that
is, $n\gg 1$. Then for the memoryless source, we may approximate
formulae (\ref{ExpVS})--(\ref{ExpVlS}) as
\begin{align}
  \mean V&\approx g(n):= \sum_w [1-e^{-np(w)}],
  \\
  \mean V_{l}&\approx g(n|l):= \sum_w \frac{[np(w)]^l}{l!} e^{-np(w)},
\end{align}
where functions $g(n)$ and $g_{l}(n)$ are analytic functions of a real
number $n\ge 0$.

Function $g(n)$ that approximates the expected number of types for a
given text size $n$ will be called the \emph{vocabulary size
  function}.  Similarly, functions $g(n|l)$ approximate the expected
frequency spectrum.  It was explicitly observed by \citet[Definition
2.11]{Baayen01} for $l=1$ and independently by \citet{Davis18} for any
$l\ge 1$ that functions $g(n|l)$ can be expressed via the consecutive
derivatives of function $g(n)$,
\begin{align}
  g(n|l)=-\frac{(-n)^{l}}{l!} g^{(l)}(n).
  \label{Derivatives}
\end{align}
We may also express the expected rank function as
\begin{align}
  \mean R_f
  =\mean V-\sum_{l=1}^{f-1} \mean V_{l}
  \approx
  g(n||f)
  &:=g(n)-\sum_{l=1}^{f-1} g(n|l)
    \nonumber\\
  &=g(n)+\sum_{l=1}^{f-1} \frac{(-n)^{l}}{l!} g^{(l)}(n).
  \label{Taylor}
\end{align}
Formula (\ref{Taylor}) can be easily remembered as the truncated
Taylor series for $g(0)$ expanded around point $n$. The take-away is
that if we can guess a certain analytic vocabulary size function
$g(n)$ then by taking the Taylor series thereof, we may evaluate also
the expected rank function $g(n||f)$ for any text size $n$. The
minimal requirements for the vocabulary size function are
\begin{align}
  g(1)=1, \quad
  g(n|l)\ge 0
  \text{ for }
  l\ge 1, \quad
  g(0)=g(n)-\sum_{l=1}^\infty g(n|l)=0.
  \label{GConditions}
\end{align}

The vocabulary size function and the expected frequency spectrum can
be expanded as the Taylor series also around point $n$. We obtain
\begin{align}
  \label{GNprime}
  g(n')&=g(n)-\sum_{k=1}^\infty g(n|k) \okra{1-\frac{n'}{n}}^k,
  \\
  \label{GNprimel}
  g(n'|l)&=\sum_{k=l}^\infty \binom{k}{l} g(n|k)
  \okra{\frac{n'}{n}}^l\okra{1-\frac{n'}{n}}^{k-l}.
\end{align}
These formulae were observed by \citet{Davis18}, who derived them as
approximations of equations (\ref{ExpV})--(\ref{ExpVl}) for the
finite urn model. In fact, to obtain formulae
(\ref{GNprime})--(\ref{GNprimel}), it suffices to approximate
$v\approx g(n)$, $v_k\approx g(n|k)$, and
\begin{align}
 \binom{r}{k}\approx \frac{r^k}{k!}.
\end{align}
These approximations are valid for $n,n'\ge k$.

Let us make another useful observation, which seems new. This arises
from investigating the rate of hapaxes. We may define the \emph{hapax
  rate function} $h(u)$ as the expected proportion of hapaxes to the
number of types,
\begin{align}
  \frac{\mean V_{1}}{\mean V}\approx h(\log n):=\frac{g(n|1)}{g(n)}.
\end{align}
Variable $u=\log n$ is a natural choice of the argument for the hapax
rate function for the reason that becomes clear in a while.

Namely, we observe that the hapax rate function $h(u)$ carries the
same information as the vocabulary size function $g(n)$ since there is
a one-to-one correspondence between $h(u)$ and $g(n)$.  It is so since
solving the differential equation
\begin{align}
  n \frac{dg(n)}{dn} : g(n) = h(\log n),
\end{align}
we obtain
\begin{align}
  \frac{d\log g(n)}{dn} = \frac{h(\log n)}{n}
\end{align}
and consequently
\begin{align}
  \label{HUGN}
  g(n)=\exp\okra{\int_0^{\log n}h(u)du}.
\end{align}
In particular, the maximal number of types is finite if
\begin{align}
  \int_0^\infty h(u)du<\infty.
\end{align}
Hence a quick glance at the plot of function $h(u)$ with respect to
variable $u=\log n$ can inform a guess whether the potential
vocabulary of a given text is finite or not. For example, if function
$h(u)$ follows a decaying linear trend then we may guess that the
potential vocabulary is bounded by a finite number.

In consequence, to derive the vocabulary size function $g(n)$ and the
expected rank function $g(n||f)$, it suffices to assume a certain form
of the hapax rate function $h(u)$. Since function $h(\log n)$ varies
slowly as a function of the text length $n$, it seems a convenient
object for approximations, as we will discuss thoroughly in \S
\ref{secModels}.  A necessary, though not sufficient, requirement for
the hapax rate function is that it remains in the unit interval,
$0\le h(u)\le 1$.  For example, for a constant hapax rate function
$h(u)=\beta\in(0,1)$, we obtain a power-law growth of the vocabulary
$g(n)=\exp(\beta\log n)=n^\beta$, known as the Herdan-Heaps law
\citep{Herdan64,Heaps78} mentioned in \S \ref{secPerspective}. This
model is a sort of a baseline to be analysed in \S \ref{secConstant}.

Before we delve into particular examples of the hapax rate function,
let us comment on the necessary conditions for this object.  In
general, for the vocabulary size function of form (\ref{HUGN}), we
obtain the frequency spectrum and the rank function
\begin{align}
  g(n|l)&=h(\log n|l) g(n),
  \\
  g(n||f)&=\kwad{1-\sum_{l=1}^{f-1} h(\log n|l)} g(n),
\end{align}
where we define recursively $h(u|0):=-1$ and
\begin{align}
  h(u|l)
  &:=\kwad{1-\frac{1}{l}\okra{1+h(u)+\frac{d}{du}}}h(u|l-1), \quad l\ge 1.
    \label{Recursion}
\end{align}
We notice that $h(u|1)=h(u)$.  Recursion (\ref{Recursion}) was
observed by \citet[equation (3.43)]{Baayen01}. Functions $h(u|l)$ are
called the \emph{relative spectrum elements} \citep[page
90]{Baayen01}.  Conditions (\ref{GConditions}) are equivalent to
conditions
\begin{align}
  \int_{-\infty}^0 h(u)du=\infty, \quad
  h(u|l)\ge 0 \text{ for } l\ge 1.
  \label{HConditions}
\end{align}
We note that if function $u\mapsto h(u)$ satisfies conditions
(\ref{HConditions}) then so does function $u\mapsto h(u-\alpha)$ but
we do not know whether function $u\mapsto h(\gamma u)$ must be an
admissible hapax rate function.  This is bad news since we do not have
an easy theoretical control of the slope of the hapax rate function.

\section{Hapax rate models}
\label{secModels}

In this section, a model will be understood as a particular
theoretical choice of the hapax rate function $h(u)$ and implied
functions $g(n)$, $g(n|l)$, and $g(n||f)$ that follow by the formulae
derived in \S \ref{secAnalytic}.  The empirical hapax rate function
for texts of the size of novels, to be discussed in \S
\ref{secExperiments}, has usually a decaying shape. It equals $1$ for
the text length $n=1$ and it decays slowly for the growing argument.
% , see Figure \ref{figHapaxEN}.
The dominating trend is approximately linear in terms of variable
$u=\log n$ and the slope of $h(u)$ is of an approximate magnitude
$-0.05$.
% , see Table \ref{tabPars}.
By contrast, for large corpora studied in
\citep{FerrerSole01b,FerrerSole03,MontemurroZanette02,Fengxiang10},
the empirical hapax rate function is $U$-shaped.
% , see Figure \ref{figHapaxU}.

Having this in mind, we propose four models of the hapax rate:
\begin{itemize}
\item the constant model:
  \begin{align}
    h_\beta(u)=\beta,
  \end{align}
\item the Davis model:
  \begin{align}
    h_\alpha(u)=\frac{1}{u-\alpha}-\frac{1}{e^{u-\alpha}-1},
  \end{align}
\item the linear model:
  \begin{align}
    h_{\alpha\gamma}(u)&=
        \begin{cases}
          1, & u<\alpha,
          \\
          1-\gamma (u-\alpha), & \alpha\le u\le \gamma^{-1}+\alpha,
          \\
          0, & u>\gamma^{-1}+\alpha,
        \end{cases}
  \end{align}
\item the logistic model:
  \begin{align}
    h_{\alpha\beta\gamma}(u)=\frac{1-\beta}{1+e^{\gamma (u-\alpha)}}+\beta.
  \end{align}
\end{itemize}

The constant model is a crude baseline that implies the exact
Herdan-Heaps law for any text size. The Davis model implies the exact
Zipf law for a certain text size and reproduces the general decreasing
trend of the hapax rate. The linear model is a more precise model of
the hapax rate than the Davis model but it is not analytic. The
logistic model corrects on this issue and additionally it allows to
model the saturation of the hapax rate for growing text sizes, which
may be necessary for large corpora.  In \S \ref{secConstant}--\S
\ref{secLogistic}, we will derive corresponding functions $g(n)$,
$g(n|l)$, and $g(n||f)$ for these four models.  Subsequently, the
models will be empirically tested in \S \ref{secExperiments} on
selected texts from Project Gutenberg.

\subsection{Constant model}
\label{secConstant}

The constant model consists in putting a constant hapax rate
function,
\begin{align}
  \label{HerdanH}
  h(u)=\beta\in(0,1].
\end{align}
This model is a baseline since the hapax rate function argument is
variable $u=\log n$, which varies slowly with the text size $n$. Thus,
in some relatively large region, we may assume that $h(u)$ is
approximately constant.  A special case of the constant model is the
maximal model for $\beta=1$, where
\begin{align}
  h(u)&:=1,
  \\
  g(n)&:=n,
  \\
  g(n|l)&:=\boole{l=1}n,
  \\
  g(n||f)&:=\boole{f=1}n.
\end{align}
The maximal model arises when all tokens are of distinct types.

Now let us solve the constant model for $\beta\neq 1$.  By formula
(\ref{HUGN}), the constant model (\ref{HerdanH}) implies a power-law
growth of the vocabulary,
\begin{align}
  \label{HerdanG}
  g(n)=\exp(\beta\log n)=n^\beta.
\end{align}
As mentioned in \S \ref{secPerspective}, this is known as
Herdan-Heaps' law \citep{Herdan64,Heaps78}.  In view of the one-to-one
correspondence between functions $h(u)$ and $g(n)$, Herdan-Heaps' law
is equivalent to a constant rate of hapaxes, regardless of the text
size.  Let us define the binomial coefficients with a real upper
argument,
\begin{align}
  \binom{r}{l}:=\frac{r(r-1)...(r-l+1)}{l!}.
\end{align}
Differentiating function $g(n)=n^\beta$, we obtain the frequency
spectrum
\begin{align}
  g(n|l)=(-1)^{l+1}\binom{\beta}{l}\,n^\beta
  =-\binom{l-\beta-1}{l}\,n^\beta.
\end{align}
Consequently, applying the Taylor expansion (\ref{Taylor}), by
induction, we derive the expected rank function
\begin{align}
  \label{MandelbrotCorrected}
  g(n||f)
  =
  g(n)-\sum_{l=1}^{f-1} g_{l}(n)
  =
  \binom{f-\beta-1}{f-1}\,n^\beta.
\end{align}
We observe that for the constant model, the normalised rank function
$g(n||f)/g(n)$ is invariant with respect to the text size! It is a
sort of a scale-free distribution. By a scale-free distribution, we
mean a distribution which does not distinguish any particular text
length or a a vocabulary size. 

We may approximate the expected rank function
(\ref{MandelbrotCorrected}) as
\begin{align}
  \log g(n||f)
  &=
  \log \binom{f-\beta-1}{f-1}+\beta\log n
    \nonumber\\
  &=
  \sum_{l=1}^{f-1} \log\okra{1-\frac{\beta}{l}}+\beta\log n
    \nonumber\\
  &\approx
    -\sum_{l=1}^{f-1} \frac{\beta}{l}+\beta\log n
    \approx
  \beta\okra{\log \frac{n}{f-1}-\gamma},
  \label{MandelbrotAsymptotic}
\end{align}
where $\gamma\approx 0.577$ is Euler's gamma.  Approximation
(\ref{MandelbrotAsymptotic}) resembles Mandelbrot's formula
(\ref{Mandelbrot}) in the middle-rank region.  However, the expected
rank function (\ref{MandelbrotCorrected}) is somewhat different than
Mandelbrot's heuristic correction (\ref{Mandelbrot}).  The discrepancy
between formulae (\ref{Mandelbrot}) and (\ref{MandelbrotCorrected})
arises both for very large and very small ranks. In particular, the
number of hapaxes is significantly larger for the constant model than
for Mandelbrot's formula (\ref{MandelbrotCorrected}).

What kind of a memoryless source generates texts that follow the
constant model? Inverting approximation (\ref{MandelbrotAsymptotic}),
we obtain
\begin{align}
  \log\frac{f_r-1}{n}\approx -\frac{1}{\beta}\log r-\gamma.
\end{align}
Using the theory developed in \S \ref{secConvergence}, we see
that the theoretical probabilities satisfy
$p_r=\lim_{n\to\infty} f_r/n$. Hence we obtain the power-law
probability distribution
\begin{align}
  p_r\approx\frac{\exp(-\gamma)}{r^{1/\beta}}.
\end{align}

\subsection{Davis model}
\label{secDavis}

In \S \ref{secConstant}, we saw an example of a scale-free
word frequency distribution.  In this section, we will develop a model
described by \citet[pages 97--101]{Baayen01} and rediscovered by
\citet{Davis18}, which is not scale invariant and which reproduces the
exact Zipf law for the text size $n=1$. We call this model the Davis
model to have a more distinctive name. \citet[Chapter 3]{Baayen01}
considered many more related models based on series
(\ref{GNprime})--(\ref{GNprimel}) with a particular choice of the
frequency spectrum for an ideal text size $n$. The Davis model is a
special case of the Yule-Simon model \citep[pages 107-114]{Baayen01},
which in turn specialises the Waring-Herdan-Muller model \citep[pages
114-117]{Baayen01}.

The Davis model starts with an apparently eccentric formula for the
hapax rate function. It takes form
\begin{align}
  \label{DavisH}
  h(u)=\frac{1}{u}-\frac{1}{e^{u}-1},
\end{align}
where $\lim_{u\to -\infty} h(u)=1$, $\lim_{u\to 0} h(u)=1/2$, and
$\lim_{u\to \infty} h(u)=0$. Thus function (\ref{DavisH}) has a
decaying sigmoid shape.  To fit the Davis model to real data, we have
to rescale it using the offset operation
\begin{align}
    h_\alpha(u)&:= h(u-\alpha),
    \\
    g_\alpha(n)&:=\frac{g(ne^{-\alpha})}{g(e^{-\alpha})},
    \\
    g_\alpha(n|l)&:=\frac{g(ne^{-\alpha}|l)}{g(e^{-\alpha})},
    \\
    g_\alpha(n||f)&:=\frac{g(ne^{-\alpha}||f)}{g(e^{-\alpha})}.
\end{align}
where $\alpha\in\mathbb{R}$ is an empirically chosen parameter.
Similar offset operation with parameter $\alpha$ will be applied
tacitly to all subsequently discussed models. Obviously, parameter
$\alpha$ makes sense only for word frequency distributions that are
not scale-free, i.e., when $h(u)$ is not constant. Exactly, $\alpha$
is a location parameter that selects a certain particular text length.

By formula (\ref{HUGN}), the Davis model (\ref{DavisH})
implies an asymptotically logarithmic growth of the vocabulary,
\begin{align}
  \label{DavisG}
  g(n)
  =\frac{n\log n}{n-1}\sim \log n. 
\end{align}
To deal with the vocabulary size function $g(n)$, let
us consider the series
\begin{align}
  \sum_{k=1}^\infty \frac{x^k}{k(k+1)}
  =1+\frac{(1-x)\log(1-x)}{x},
  \quad
    \abs{x}\le 1.
\end{align}
Hence we may write the vocabulary size function (\ref{DavisG}) and
the corresponding frequency spectrum as
\begin{align}
    g(n)&=1-\sum_{k=1}^\infty \frac{(1-n)^k}{k(k+1)},
  \\
    g(n|l)&=
            \sum_{k=l}^\infty \binom{k}{l} \frac{n^{l}(1-n)^{k-l}}{k(k+1)},
            \quad
            l\ge 1.
\end{align}
Comparing this with formulae (\ref{GNprime})--(\ref{GNprimel}), we
obtain
\begin{align}
  g(1)&=1,
  \\
  g(1|k)&=\frac{1}{k(k+1)},
\end{align}
which is the rescaled Lotka law (\ref{Lotka}). Thus for the Davis
model and text size $n=1$, we obtain the rescaled exact Zipf law,
\begin{align}
 g(1||f)=\frac{1}{f}. 
\end{align}

Now we will evaluate the rank function $g(n||f)$ for any text size
$n$.  \citet{Davis18} evaluated the derivatives of $g(n)$ using a
computer algebra system without noticing a simple pattern that emerges
from the calculations. Let us present a brief derivation of this
regularity.  We can write the derivatives
\begin{align}
  g^{(l)}(n)
  &=\sum_{j=0}^l \binom{l}{j} p^{(j)}(n) q^{(l-j)}(n)
    ,
\end{align}
where
\begin{align}
  p^{(0)}(n)&=n\log n,
  & 
    q^{(j)}(n)&=\frac{(-1)^{j}j!}{(n-1)^{j+1}},
  \\
  p^{(1)}(n)&=1+\log n,
  \\
  p^{(j)}(n)&=\frac{(-1)^{j}(j-2)!}{n^{j-1}},
                   \quad
                   j\ge 2.
\end{align}

Let us define $s_{-1}:=0$, $s_{0}:=\log n$, $s_{j}:=-1/j$ for
$j\ge 1$, and $t:=1-1/n$.  Hence we may compute the rank function as
\begin{align}
  g(n||f)
  &=\sum_{l=0}^{f-1}\frac{(-n)^{l}}{l!} g^{(l)}(n)
  \nonumber\\
  &=\sum_{l=0}^{f-1}\sum_{j=0}^l
    \frac{(-1)^jn^{j-1}p^{(j)}(n)}{j!}
    \cdot
    \frac{(-1)^{l-j}n^{l-j+1}q^{(l-j)}(n)}{(l-j)!}
  \nonumber\\
%  &=\sum_{l=0}^{f-1}\sum_{j=0}^l \kwad{s_{j}-s_{j-1}} t^{j-l-1}
%    \nonumber\\
  &=\sum_{j,i\ge 0:\,j+i\le f-1} \kwad{s_{j}-s_{j-1}} t^{-i-1}
    \nonumber\\
  &=\sum_{j,i\ge 0:\,j+i=f-1} s_{j} t^{-i-1}
    =\sum_{j=0}^{f-1} s_{j} t^{j-f}
    .
    \label{ZipfCorrectedFast}
\end{align}
Subsequently, we may rewrite formula (\ref{ZipfCorrectedFast}) as
expression
\begin{align}
  g(n||f)
  &=\frac{\log n-\sum_{j=1}^{f-1} \okra{1-1/n}^{j}/j
    }{\okra{1-1/n}^{f}}
    .
   \label{ZipfCorrected}
\end{align}
Formula (\ref{ZipfCorrected}) adjusts the ideal Zipf law for an
arbitrary text size.

We have to make a couple of remarks concerning the practical use of
formula (\ref{ZipfCorrected}). First, we notice the Taylor series
\begin{align}
  \log x = \sum_{j=1}^\infty \frac{\okra{1-1/x}^{j}}{j},
  \quad
  x\ge \frac{1}{2},
\end{align}
so the limit of expression (\ref{ZipfCorrected}) for $n\to 1$ is
indeed $1/f$, i.e., the ideal Zipf law. If we can compute the
logarithm function with an arbitrarily high precision then formula
(\ref{ZipfCorrected}) supplies a very efficient algorithm for
computing the rank function plot for an arbitrary text length $n$. It
only takes $O(l)$ arithmetic operations to compute ranks
$g(n||f)$ for all frequencies $f\le l$. Indeed, for $n\neq 1$, we
may write the recursion
\begin{align}
  \label{ZipfRecursionI}
  g(n||1)&= \frac{\log n}{1-1/n},
  \\
  \label{ZipfRecursionII}
  g(n||f+1)&= \frac{g(n||f)-1/f}{1-1/n},
       \quad
       f\ge 1,
\end{align}
whereas for $n\ge 1/2$, we may also consider summing the series
\begin{align}
  g(n||f)=\sum_{j=0}^\infty \frac{\okra{1-1/n}^{j}}{j+f},
  \label{ZipfCorrectedSeries}
\end{align}
which is considerably slower but more precise when implemented
numerically. We notice some beauty of recursion
(\ref{ZipfRecursionI})--(\ref{ZipfRecursionII}), where the numerator
$g(n||f)-1/f$ is the deviation of the rank function from the exact
Zipf law and the denominator $1-1/n$ is the deviation of the text size
from the ideal.

What kind of a memoryless source generates texts that follow the Davis
model? Formula (\ref{ZipfCorrectedSeries}) for large $n$ can be
approximated as
\begin{align}
  g(n||f)\approx
  \sum_{j=0}^\infty
  \frac{\exp\okra{-\frac{j}{n}}}{\frac{j}{n}+\frac{f}{n}}\cdot\frac{1}{n}
  \approx
  \int_0^\infty \frac{\exp(-u)}{u+\frac{f}{n}}du
  =
  \exp\okra{\frac{f}{n}}\Gamma\okra{0,\frac{f}{n}},
\end{align}
where the incomplete gamma function is
\begin{align}
  \Gamma(a,x)=\int_x^\infty t^{a-1}\exp(-t)dt.
\end{align}
Consequently, for the theoretical probabilities 
$p_r=\lim_{n\to\infty} f_r/n$, we obtain
\begin{align}
  \exp\okra{p_r}\Gamma\okra{0,p_r}\approx r.
\end{align}
Since we have the expansion
\begin{align}
  \Gamma(0,x)=-\gamma-\log x-\sum_{j=0}^\infty\frac{(-x)^j}{j(j!)},
\end{align}
where $\gamma\approx 0.577$ is Euler's gamma, then for small
probabilities $p_r$, we obtain
$\exp\okra{p_r}\Gamma\okra{0,p_r}\approx -\gamma-\log p_r$. Hence the
theoretical probabilities decay exponentially asymptotically,
\begin{align}
 p_r\approx \exp(-r-\gamma).
\end{align}

\subsection{Linear model}
\label{secLinear}

To keep the theory consistent, functions $g(n)$ and $h(u)$ are assumed
to be analytic functions that satisfy conditions (\ref{GConditions})
and (\ref{HConditions}), respectively.  On the other hand, as we have
mentioned, the empirical hapax rate function has usually a decaying
shape for moderately sized texts. Moreover, as the empirical data
discussed in \S \ref{secExperiments} indicate, this trend is
approximately linear in terms of variable $u=\log
n$. %, see Figure \ref{figHapaxEN} and Table \ref{tabPars}.
Therefore, we may be tempted to propose the following piecewise linear
model of the hapax rate function,
\begin{align}
  \label{LinearH}
  h(u)&=
        \begin{cases}
          1, & u<0,
          \\
          1-\gamma u, & 0\le u\le \gamma^{-1},
          \\
          0, & u>\gamma^{-1},
        \end{cases}
\end{align}
where $\gamma\in\mathbb{R}$. For empirical data, we have approximately
$\gamma\approx 0.05$. %, as it can be seen in Table \ref{tabPars}.
Of course, this model is not an analytic function and we have no
guarantee that conditions (\ref{GConditions}) and (\ref{HConditions})
are satisfied even for $0\le u\le \gamma^{-1}$.

Nonetheless, let us proceed with calculations and let us derive the
corresponding ill-defined vocabulary size function $g(n)$ and the
relative spectrum elements $h(u|l)$. By formula (\ref{HUGN}), we
obtain
\begin{align}
  g(n)&=
        \begin{cases}
          n, & n\le 1,
          \\
          n^{1-\frac{1}{2}\gamma\log n}, & 1\le n\le \exp(\gamma^{-1}),
          \\
          \sqrt{\exp(\gamma^{-1})}, & n>\exp(\gamma^{-1}).
  \end{cases}
\end{align}
Thus the vocabulary size is asymptotically bounded. For the
empirically motivated value $\gamma\approx 0.05$, the maximal
vocabulary size is $\sqrt{\exp(\gamma^{-1})}\approx 22\,026$ types and
the text length for which this limit is hit amounts to
$\exp(\gamma^{-1})\approx 4.85\cdot 10^8$ tokens. We note that this
maximal vocabulary size is close in the magnitude to the ideal size of
the vocabulary estimated in formula (\ref{TypesZipf}).

In turn, let us proceed to the frequency spectrum.  For
$0\le u\le \gamma^{-1}$, recursion (\ref{Recursion}) predicts that the
relative spectrum elements $h(u|l)$ are polynomials,
\begin{align}
  \label{LinearPoly}
  h(u|l)&=\frac{1}{l!} \sum_{m=0}^l a_{lm}u^m,
\end{align}
where we have the recursion
\begin{align}
  \label{LinearRecursion}
  a_{lm}
  &:=
    \begin{cases}
      0, & m<0 \text{ or } m>l,
      \\
      -1, & l=0 \text{ and } m=0,
      \\
      \gamma a_{l-1,m-1}+(l-2)a_{l-1,m}
      \\
      \quad -(m+1)a_{l-1,m+1}, &
      l\ge 1 \text{ and } 0\le m\le l.
    \end{cases}
\end{align}
From polynomials (\ref{LinearPoly}), we can compute the expected rank
function $g(n||f)$, as explicated in \S \ref{secAnalytic}.  So
far, we have not seen a clear pattern in these polynomials that would
solve the recursion in a closed form.  In particular, we do not know
what the range of $u$ is such that $h(u|l)\ge 0$ for all $l\ge 1$.

\subsection{Logistic model}
\label{secLogistic}

The plain logistic model is unrealistic as a model of word frequency
distributions because of a too large slope but it can be analysed
easily and, after a modification, it yields the best model that we
propose in this paper.  The plain logistic model assumes the familiar
logistic function as the hapax rate function,
\begin{align}
  \label{LogisticH}
  h(u)=\frac{1}{1+e^{u}},
\end{align}
where $\lim_{u\to -\infty} h(u)=1$, $\lim_{u\to 0} h(u)=1/2$, and
$\lim_{u\to \infty} h(u)=0$. Hence function (\ref{LogisticH}) has also
a decaying sigmoid shape.

By formula (\ref{HUGN}), the logistic model (\ref{LogisticH})
implies a bounded vocabulary size function,
\begin{align}
  \label{LogisticG}
  g(n)
  =\frac{2n}{n+1}\xrightarrow[n\to\infty]{} 2.
\end{align}
Let us evaluate the corresponding rank function $g(n||f)$.  Like for
the Davis model, we can write the derivatives
\begin{align}
  g^{(l)}(n)
  &=\sum_{j=0}^l \binom{l}{j} p^{(j)}(n) q^{(l-j)}(n)
    ,
\end{align}
where
\begin{align}
  p^{(0)}(n)&=2n,
  & 
    q^{(j)}(n)&=\frac{(-1)^{j}j!}{(n+1)^{j+1}},
  \\
  p^{(1)}(n)&=2,
  \\
  p^{(j)}(n)&=0,
                   \quad
                   j\ge 2.
\end{align}
Hence we may compute the rank function as
\begin{align}
  &g(n||f)
  \nonumber\\
  &=g(n)-\sum_{l=1}^{f-1}\frac{(-n)^{l}}{l!} g^{(l)}(n)
  \nonumber\\
  &=g(n)+\sum_{l=1}^{f-1}\sum_{j=0}^1
    \frac{(-1)^jn^{j-1}p^{(j)}(n)}{j!}
    \cdot
    \frac{(-1)^{l-j}n^{l-j+1}q^{(l-j)}(n)}{(l-j)!}
  \nonumber\\
  &=\frac{2n}{n+1}+
    \sum_{l=1}^{f-1}\kwad{\frac{2n^{l+1}}{(n+1)^{l+1}}
    -\frac{2n^{l}}{(n+1)^{l}}}
  \nonumber\\
  &=\frac{2n^{f}}{(n+1)^{f}}.
\end{align}
Thus the rank function decays like a geometric series. In consequence,
the logistic model satisfies conditions (\ref{GConditions}) and
(\ref{HConditions}).

The linear model discussed in \S \ref{secLinear} suggests a
certain amendment of the logistic model, which makes it more
realistic. This modification is analytic in contrast to the linear
model. For analytic functions $h(u)$ and $g(n)$, in general, we may
try parameterising
\begin{align}
  \label{ScalingH}
  h_{\gamma\beta}(u)&:=(1-\beta)h(\gamma u)+\beta,
  \\
  \label{ScalingG}
  g_{\gamma\beta}(n)&:= [g(n^\gamma)]^{(1-\beta)/\gamma}n^\beta,
\end{align}  
with parameters $\gamma>0$ and $0\le\beta<1$ but it is not guaranteed
that the frequency spectrum elements still satisfy
$h_{\gamma\beta}(u|l)\ge 0$ and $g_{\gamma\beta}(u|l)\ge 0$. Anyway,
since we are in need to control the slope of the hapax rate $h(u)$ and
we wish it to be approximately linear, let us try the rescaled sigmoid
function
\begin{align}
  \label{SLogisticH}
  h_{\gamma\beta}(u)=\frac{1-\beta}{1+e^{\gamma u}}+\beta.
\end{align}
Formula (\ref{SLogisticH}) applies the scaling operation
(\ref{ScalingH})--(\ref{ScalingG}) to function (\ref{LogisticH}).

By formula (\ref{HUGN}), the scaled logistic model (\ref{SLogisticH})
implies an asymptotically power-law growing vocabulary size function,
\begin{align}
  \label{SLogisticG}
  g_{\gamma\beta}(n)
  =\frac{2^{(1-\beta)/\gamma}n}{(n^\gamma+1)^{(1-\beta)/\gamma}}\sim
  2^{(1-\beta)/\gamma} n^\beta.
\end{align}
Taking derivatives of function (\ref{SLogisticG}) can be a tedious
exercise. Like for the linear model, it is simpler to deal with
recursion (\ref{Recursion}) for the relative spectrum elements
$h_{\gamma\beta}(u|l)$.  Cleverly, let us observe that the first
derivative of function (\ref{SLogisticH}) for $\beta=0$ is a
polynomial of $h_{\gamma 0}(u)$,
\begin{align}
  \label{SLogisticHD}
  \frac{d}{du} h_{\gamma 0}(u)=-\gamma
  h_{\gamma 0}(u)[1-h_{\gamma 0}(u)].
\end{align}
In particular, the value of the first derivative for $u=0$ and
$\beta=0$ is $-\gamma/4$. It is the minimum, by the way. Since the
slope of $h(u)$ for language data is close to $-0.05$ then the optimal
value of $\gamma$ should equal $0.2$ approximately.
% Table \ref{tabPars} informs us that the optimal $\gamma$ is close to
% $0.3$, however.

In view of observation (\ref{SLogisticHD}) and recursion
(\ref{Recursion}), rewritten as
\begin{align}
  &h_{\gamma\beta}(u|l)
    \nonumber\\
  &:=\kwad{1-\frac{1}{l}\okra{1+\beta+(1-\beta)h_{\gamma 0}(u)+
    \frac{d}{du}}}h_{\gamma\beta}(u|l-1) 
    \label{Recursion}
\end{align}
for $l\ge 1$, the relative spectrum elements $h_{\gamma\beta}(u|l)$
are polynomials of variable $h_{\gamma 0}(u)$,
\begin{align}
  \label{SLogisticPoly}
  h_{\gamma\beta}(u|l)&=\frac{1}{l!} \sum_{m=0}^l b_{lm}[h_{\gamma 0}(u)]^m.
\end{align}
Since we can compute the derivative of a power of $h_{\gamma 0}(u)$ as
\begin{align}
  \frac{d}{du} [h_{\gamma 0}(u)]^m
  &=m [h_{\gamma 0}(u)]^{m-1} \frac{d}{du} h_{\gamma 0}(u)
    \nonumber\\
  &=-\gamma m [h_{\gamma 0}(u)]^m[1-h_{\gamma 0}(u)],
\end{align}
we obtain the recursion
\begin{align}
  \label{SLogisticRecursion}
  b_{lm}
  &:=
    \begin{cases}
      0, & m<0 \text{ or } m>l,
      \\
      -1, & l=0 \text{ and } m=0,
      \\
      (-1+\beta-\gamma (m-1)) b_{l-1,m-1}
      \\
      \quad+(l-1-\beta+\gamma m)b_{l-1,m}, &
      l\ge 1 \text{ and } 0\le m\le l.
    \end{cases}
\end{align}
From polynomials (\ref{SLogisticPoly}), we can compute the expected
rank function $g(n||f)$, as prescribed in \S \ref{secAnalytic}.
 
\subsection{Mixture models}
\label{secMixture}

We can fit the models defined in \S \ref{secConstant}--\S
\ref{secLogistic} to empirical data.  However, simple decaying hapax
rate models that are good for moderately sized texts break down for
large text collections.  It is so since the decaying trend of the
hapax rate is inverted for sufficiently large corpora.  We observe a
$U$-shaped plot of the hapax rate \citep{Fengxiang10}. This second
regime in the hapax rate plot arises long before we can observe a hard
upper bound for the number of word types predicted by a decaying
model.

To model such a scenario, let us suppose that we have some candidates
for functions $h(u)$, $g(n)$, $g(n|l)$, and $g(n||f)$. We observe that
these functions can be simultaneously modified via the following
mixture operation
\begin{align}
  h_\lambda(u)&:=\frac{\lambda h_1(u)g_1(e^{u})+(1-\lambda) h_2(u)g_2(e^{u})
                }{\lambda g_1(e^{u})+(1-\lambda) g_2(e^{u})},
  \\
  g_\lambda(n)&:= \lambda g_1(n)+(1-\lambda) g_2(n),
  \\
  g_\lambda(n|l)&:= \lambda g_1(n|l)+(1-\lambda) g_2(n|l),
  \\
  g_\lambda(n||f)&:= \lambda g_1(n||f)+(1-\lambda) g_2(n||f).
\end{align}
where $\lambda\in(0,1)$ is an empirically chosen parameter. 

We can easily reproduce a $U$-shaped plot of the hapax rate function,
observed by \citet{Fengxiang10}, with a mixture of a one-parameter
decaying model for the first regime and a constant model for the
second regime, see Figure \ref{figHapaxU}.  In fact, the emergence of
the second regime was explained by \citet{Fengxiang10} as an effect of
lexical trash, i.e., neologisms or other rare word types that do not
penetrate to the general vocabulary. Namely, besides typical
dictionary words, there is a non-zero probability of observing an
arbitrary random string of letters as a word token. If the pool of
such trash strings is infinite and the probability of re-occurrence of
each trash string is negligible then we may observe a $U$-shaped plot
of the hapax rate for extremely large corpora.  The constant model
with $\beta<1$ for lexical trash can be motivated by the monkey-typing
explanations of Mandelbrot's correction (\ref{Mandelbrot}) to Zipf's
law \citep{Mandelbrot54,Miller57}.  Moreover, with the constant model
using $\beta<1$ for lexical trash, we may be able to predict the
asymptotic law (\ref{MandelbrotCorrected}) in the second regime of the
rank-frequency plot, see
\citep{FerrerSole01b,FerrerSole03,MontemurroZanette02}.

% Figure environment removed

\section{Experiments}
\label{secExperiments}

Consequently, we proceed to an experimental verification of our models
by comparing them with empirical word frequency distributions. There
is a formidable amount of textual data available in various languages
on the internet. Aware of this, we restrict ourselves to a pilot test
of our models. We are not expecting to confirm the absolute truth of
any particular model. We confine to setting up an experimental
methodology that can be extended in future research. We are fully
satisfied with a general conclusion that the urn model predicts the
word frequency distributions reasonably well if we plug in a simple
parametric hapax rate model. Our goal is to explore a few such hapax
rate models and to propose such a way of looking at word frequency
distributions that immediately suggests how to correct the ideal Zipf
and Herdan-Heaps laws as well as the Mandelbrot correction
(\ref{Mandelbrot}).

\subsection{Setup}
\label{secSetup}

While investigating particular texts, we will try to identify which of
the hapax rate models defined in \S \ref{secModels} predicts these
data most accurately. Our minimal requirement is to have the plain
frequency list for each source text.  This list should contain the
frequencies $F(w)$ for all word types $w$ in a given text.  In
particular, hapaxes must not be omitted.

From the frequency list, we compute the maximal frequency
$F_1:=\max_w F(w)$, the rank function
\begin{align}
  R_f:=\sum_w\boole{F(w)\ge f}, \quad 1\le f\le F_1+1,
\end{align}
the frequency spectrum elements $V_k:=R_k-R_{k+1}$ for
$1\le k\le F_1$, the number of tokens $N:=\sum_{k=1}^{F_1} k V_k$, and
the number of types $V:=\sum_{k=1}^{F_1} V_k$. We also consider the
smoothed vocabulary size function $G(n)$ and the smoothed frequency
spectrum function $G(n|l)$ defined as
\begin{align}
  \label{GNprimeE}
  G(n)&:=V-\sum_{k=1}^{F_1} V_k \okra{1-\frac{n}{N}}^k,
  \\
  \label{GNprimelE}
  G(n|l)&:=\sum_{k=l}^{F_1} \binom{k}{l} V_k
  \okra{\frac{n}{N}}^l\okra{1-\frac{n}{N}}^{k-l}.
\end{align}
These functions are motivated by the Taylor series
(\ref{GNprime})--(\ref{GNprimel}). They approximate the expected
vocabulary size and frequency spectrum for text length $n$ by assuming
that $(V_1,V_2,...)$ is the frequency spectrum for text length $N$.

Functions $G(n)$ and $G(n|l)$ should be distinguished from the
incremental vocabulary size $\mathcal{G}(n)$ and the incremental
frequency spectrum $\mathcal{G}(n|l)$ defined as the vocabulary size
and the frequency spectrum for the first $n$ tokens of the empirical
text.  Respectively, we also distinguish the smoothed hapax rate
$G(n|1)/G(n)$ and the incremental hapax rate
$\mathcal{G}(n|1)/\mathcal{G}(n)$.

Whereas functions $\mathcal{G}(n)$ and $\mathcal{G}(n|l)$ fluctuate
strongly, functions $G(n)$ and $G(n|l)$ are smooth. Empirically,
fitting models to the smooth functions $G(n)$ and $G(n|l)$ yields a
better prediction of the rank function $R_f$ than fitting them to the
incremental functions $\mathcal{G}(n)$ and $\mathcal{G}(n|l)$. For
this reason, in the following, we present the results of fitting the
models from \S \ref{secConstant}--\S \ref{secLogistic} to the
smoothed functions such as $G(n)$ and $G(n|l)$.

\subsection{Results}
\label{secResults}

\begin{table}[t]
\caption{The selection of texts from Project
  Gutenberg.\label{tabTexts}}
\centering
\medskip
\begin{tabular}{|l|l|r|}
  \toprule
  Title &Author &File \\
  \midrule
  First Folio/35 Plays &W. Shakespeare & {00ws110.txt} \\
  One of Ours&W. Cather & {1ours10.txt} \\
  20,000 Leagues under the &J. Verne & {2000010.txt} \\
  \hfill Sea & & \\
  Critical \& Historical Essays &Macaulay & {2cahe10.txt} \\
  Five Weeks in a Balloon &J. Verne & {5wiab10.txt} \\
  Eight Hundred Leagues on &J. Verne & {800lg10.txt} \\
  \hfill the Amazon & & \\
  The Complete Memoirs &J. Casanova & {csnva10.txt} \\
  Memoirs &Comtesse du Barry & {dbrry10.txt} \\
  The Descent of Man &C. Darwin & {dscmn10.txt} \\
  Gulliver's Travels &J. Swift & {gltrv10.txt} \\
  The Mysterious Island &J. Verne & {milnd10.txt} \\
  Mark Twain, A Biography &A. B. Paine & {mt7bg10.txt} \\
  The Journal to Stella &J. Swift & {stlla10.txt} \\
  Life of William Carey &G. Smith & {wmcry10.txt} \\
  \botrule
\end{tabular}  
\end{table}

\begin{table*}[t]
  \caption{The parameters fitted by least squares to function
    $G(n)$.\label{tabPars}}
  \centering
  \medskip
  \footnotesize
  \begin{tabular}{|l|l|l|lll|ll|r|}
    \toprule
    File & Constant & Davis
    & \multicolumn{3}{c|}{Logistic}
    & \multicolumn{2}{c|}{Linear} & Length\\
    \cline{2-2}\cline{3-3}\cline{4-6}\cline{7-8}
    & $\beta$       & $\alpha$
    & $\gamma$      & $\beta$       & $\alpha$
    & $\gamma$      & $\alpha$      & $N$\\
\midrule
00ws110.txt     & 0.768 & 12.06 & 0.314 & 0.218 & 10.11 & 0.0509        & 2.14  & 835726\\
1ours10.txt     & 0.797 & 11.55 & 0.318 & 0.203 & 9.72  & 0.0507        & 1.7   & 128963\\
2000010.txt     & 0.801 & 11.48 & 0.323 & 0.008 & 10.62 & 0.0578        & 2.22  & 101247\\
2cahe10.txt     & 0.796 & 12.12 & 0.314 & 0     & 11.38 & 0.0576        & 2.79  & 298339\\
5wiab10.txt     & 0.808 & 11.64 & 0.315 & 0.001 & 10.86 & 0.0552        & 2.13  & 92558\\
800lg10.txt     & 0.799 & 11.43 & 0.327 & 0.162 & 9.77  & 0.0534        & 1.84  & 95493\\
csnva10.txt     & 0.732 & 11.39 & 0.308 & 0.157 & 9.94  & 0.0542        & 1.87  & 1268149\\
dbrry10.txt     & 0.787 & 11.39 & 0.325 & 0.065 & 10.31 & 0.0583        & 2.23  & 159710\\
dscmn10.txt     & 0.774 & 11.5  & 0.328 & 0     & 10.75 & 0.0629        & 2.71  & 312075\\
gltrv10.txt     & 0.796 & 11.4  & 0.322 & 0.001 & 10.62 & 0.0584        & 2.22  & 104909\\
milnd10.txt     & 0.773 & 11.14 & 0.347 & 0.127 & 9.63  & 0.0608        & 2.24  & 195064\\
mt7bg10.txt     & 0.775 & 11.91 & 0.296 & 0.001 & 11.45 & 0.0565        & 2.55  & 519886\\
stlla10.txt     & 0.757 & 10.91 & 0.333 & 0.231 & 8.87  & 0.0536        & 1.45  & 245882\\
wmcry10.txt     & 0.799 & 11.69 & 0.314 & 0     & 10.96 & 0.0567        & 2.34  & 145487\\
\midrule
Mean    & 0.783 & 11.54 & 0.32  & 0.084 & 10.36 & 0.0562        & 2.17  & 321678\\
\botrule
  \end{tabular}
\end{table*}


\begin{table}[t]
  \caption{The goodness of fit $\sqrt{\text{WSSR/ndf}}$ for
    function $G(n)$.\label{tabWSSRs}}
  \centering
  \medskip
  \begin{tabular}{|l|r|r|r|r|}
    \toprule
File    & Constant      & Davis & Logistic      & Linear\\
\midrule
00ws110.txt     & 1784.34       & 120.42        & \textbf{11.82}        & 43.71\\
1ours10.txt     & 478.53        & 74.39 & \textbf{7.02} & 16.69\\
2000010.txt     & 439.57        & 117.31        & \textbf{2.18} & 24.14\\
2cahe10.txt     & 1118.75       & 255.29        & \textbf{29.17}        & 86.71\\
5wiab10.txt     & 414   & 111.21        & \textbf{4.15} & 25.14\\
800lg10.txt     & 402.88        & 83.74 & \textbf{3.12} & 16.48\\
csnva10.txt     & 1721.89       & 107.98        & \textbf{6.86} & 34.72\\
dbrry10.txt     & 587.39        & 125.46        & \textbf{5.65} & 31.08\\
dscmn10.txt     & 982.09        & 215.02        & \textbf{19.93}        & 63.73\\
gltrv10.txt     & 463.34        & 117.35        & \textbf{6.86} & 31.39\\
milnd10.txt     & 629.47        & 125.02        & \textbf{1.88} & 23.22\\
mt7bg10.txt     & 1433.58       & 194.1 & \textbf{8.75} & 73.41\\
stlla10.txt     & 603.45        & 45.14 & 9.67  & \textbf{9.34}\\
wmcry10.txt     & 592.57        & 143.7 & \textbf{5.25} & 36.47\\
\midrule
Mean    & 832.27        & 131.15        & \textbf{8.74} & 36.87\\
\botrule
  \end{tabular}
\end{table}

Applying our experimental setup, we have processed 14 texts in English
downloaded from Project Gutenberg \citep{Gutenberg11} and listed in
Table \ref{tabTexts}.  We have fitted the models defined in \S
\ref{secModels} to the plot of function $G(n)$ defined in
(\ref{GNprimeE}). The free parameters of the corresponding hapax rate
models are: $\alpha$ of the offset, $\beta$ of the asymptote, and
$\gamma$ of the scale.  The results of fitting by least squares are
presented in Table \ref{tabPars}.  The quality of the fit can be
witnessed in Table \ref{tabWSSRs}. In Figures
\ref{figHapaxEN}--\ref{figRankREN}, we present how the models that are
optimal for the expected vocabulary size $G(n)$ predict the hapax rate
and the rank function. The parameters are the same as for $G(n)$.  The
supplementary files (scripts, tables, and figures) are available from
Github \citep{Debowski23e}.

\subsection{Discussion}
\label{secDiscussion}

The urn model combined with the analytic function theory provides a
good approximation method for empirical word frequency
distributions. This theoretical framework works well even when it is
combined with simple models of the hapax rate decay.  As for the four
considered hapax rate models, for moderately-sized texts from Project
Gutenberg, the constant model is the worst, the Davis model and the
ill-defined linear model seem better, whereas the logistic model is
usually the best, see Table \ref{tabWSSRs}.
  
Contrary to the usual assumption under Herdan-Heaps' law paradigm, the
hapax rate is not constant as a function of the text length. It
exhibits a prominent decaying trend that is approximately linear over
many decades in the logarithmic scale. However, the incremental hapax
rate exhibits large fluctuations about this trend. Also the smoothed
hapax rate follows three humps that recur roughly in the same
locations for all investigated texts. Are these humps significant? Do
they correspond to a decomposition of the lexicon into functional
words, common words, and proper names? This may be worth checking in
the future research.
  
The main take-away is that Herdan-Heaps' law is a crude approximation
of empirical word frequency distributions. This law implies that the
hapax rate is constant and almost twice larger than actually observed
for a given whole text. Obviously, this model is false.  Since the
decaying trend of the hapax rate is approximately linear over many
decades then the pure Davis model becomes also suboptimal for
sufficiently long texts. Although it may be necessary to consider
mixture models for larger corpora, we have not observed the $U$-shaped
plot for the selected texts from the Project Gutenberg corpus.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% % Figure environment removed

% Figure environment removed

% Figure environment removed

\section{Conclusion}
\label{secConclusion}

Zipf's and Heaps' laws are celebrated theoretical constructs but their
particular functional forms fail upon a closer scrutiny as a too crude
approximation. Borrowing the term from economics, they seem to
constitute stylised facts, i.e., broad tendencies that summarize the
data but ignore the details \citep{Kaldor61}. Thus one might doubt
whether there is a relatively simple but more accurate description of
the word frequency distributions.

Here we have advocated, however, that investigation of the hapax rate
function yields a direct insight where and why particular forms of
Zipf's and Heaps' laws do fail. Moreover, this investigation suggests
how to improve these laws systematically with relatively simple
parametric models. Thus we hope that the hapax rate plot may be a
convenient diagnostic tool for future research in quantitative
linguistics. The basic finding of this work is that the constant model
for the hapax rate is wrong and, easily, one can propose efficiently
computable but not completely obvious corrections to the idealized
Zipf-Mandelbrot and Herdan-Heaps laws. The standard urn model allows
to deduce them from a simple logistic hapax rate model and the
predictions are much more precise than even for the previously
discussed Davis model.

We suppose that while investigating large natural language corpora, it
may be necessary to consider mixture models where different components
of the lexicon are distinguished based on some linguistic criteria and
possibly they obey different hapax rate functions. In particular, we
may empirically research which components of the lexicon are closed or
open---or rather to what degree they are closed or open. Are they
asymptotically bounded (the logistic model with $\beta=0$), do they
grow logarithmically (the Davis model), or do they follow a
scale-invariant power-law (the constant model)? In fact, already
\citet{Baayen01} tried to answer such sort of questions by
investigating productivity of affixes in particular. This work is an
extension of his theoretical ideas into the area of hapax rate
modelling. Our main goal was to provide a relatively self-contained
mathematical toolbox for future research.


\section*{Acknowledgements}

I thank Iv\'an Gonz\'alez Torre, Antoni Hern\'andez-Fern\'andez, and
Ji\v{r}\'\i{} Mili\v{c}ka for a fruitful discussion of the ideas
contained in this paper.

%\setlength{\bibsep}{2pt}
\bibliography{0-publishers-full,0-journals-full,books,ql,nlp,ai,mine}


\end{document}

