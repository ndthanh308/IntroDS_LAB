
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference_tinypaper,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}



% \usepackage[square]{cite}
\usepackage{booktabs}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{wrapfig,lipsum,booktabs}
% \usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{multirow}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{verbatim}

\definecolor{tb_color}{rgb}{0.851, 0.882, 0.949}


\title{Simple Parameter-free Self-attention Approximation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yuwen Zhai$^{1}$\footnotemark[1]\quad Jing Hao$^{2}$\thanks{Equal contribution.\quad$^{\dagger}$Corresponding author.} \quad Liang Gao$^{1}$ \quad Xinyu Li$^{1\dagger}$\quad Yiping Gao$^{1}$\quad Shumin Han$^{2}$ \\
$^{1}$Huazhong University of Science and Technology\quad \quad \quad \quad $^{2}$Baidu VIS\\
\texttt{\{yuwenzhai, gaoliang, lixinyu, gaoyiping\}@hust.edu.cn}\\
\texttt{\{haojing08, hanshumin\}@baidu.com}
% \\
% \centerline{\small \texttt{Code:} \url{http://www.iclr.cc/}
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle
\vspace{-3mm}
\begin{abstract}
\vspace{-1mm}
 The hybrid model of self-attention and convolution is one of the methods to lighten ViT. The quadratic computational complexity of self-attention with respect to token length limits the efficiency of ViT on edge devices. We propose a self-attention approximation without training parameters, called SPSA, which captures global spatial features with linear complexity. To verify the effectiveness of SPSA combined with convolution, we conduct extensive experiments on image classification and object detection tasks. The source code is available at SPSA.

 % The source code will be available.
 % The source code is available at \href{https://github.com/OpenNLPLab/cosFormer}{GSCA}.
\end{abstract}
\vspace{-3mm}
\section{Introduction}
\vspace{-3mm}
% Figure environment removed
% Recently, the hybrid models of convolution and self-attention~\citep{selfattention} have become one of the main approaches to applying ViT~\citep{ViT} to edge devices~\citep{mehta2022mobilevit, Maaz2022EdgeNeXt}. 
Recently, hybrid models of convolution and self-attention have emerged as an important approach to apply ViT~\citep{ViT} to edge devices~\citep{li2022next-vit, Maaz2022EdgeNeXt}.
We propose a parameter-free self-attention approximation, called SPSA, which is extremely simple and has linear computational complexity. The paper aims to present a new form of global spatial attention that inspires researchers to design new lightweight CNN-ViT hybrid networks. Based on this, we experimentally validate the feasibility of combining SPSA with convolution taking the well-known channel attention as the performance benchmark. To demonstrate the effectiveness of SPSA as spatial attention, we explore the generalizability of the fusion of SPSA and different channel attention.
\vspace{-5mm}
\section{Method}
\label{method}
\vspace{-2mm}
Fig.~\ref{figure1} illustrates SPSA. Given an input $X \in {\mathbb{R}^{H \times W \times C}}$, we implement global average pooling (GAP) to obtain the key vector $k$, i.e., $k = \frac{1}{{WH}}\sum\nolimits_{i = 1,j = 1}^{W,{\kern 1pt} H} {{X_{ij}}}$ and $k \in {\mathbb{R}^C}$. The matrices $Q$ and $V$ are generated using identical mappings, i.e., $Q=V=X$. According to Eq.~(\ref{eq3}), SPSA uses the cross-correlation coefficient to evaluate the similarity between each location in query $Q$ and key $k$. 
\begin{equation}
{C_{Qk}} = \frac{{\sum\nolimits_{i = 1}^C {[{Q_{:,:,i}} - \bar Q][{k_i} - \bar k]} }}
{{\sqrt {\sum\nolimits_{i = 1}^C {{{[{Q_{:,:,i}} - \bar Q]}^2}} \sum\nolimits_{i = 1}^C {{{[{k_i} - \bar k]}^2}} } }},
\label{eq3}
\end{equation}
The weight matrix $A$ is get by subtracting the normalized $C_{Qk}$ from 1 (reverse operation), i.e, $A = {\left( {1 - \sigma \left( {{C_{Qk}}} \right)} \right)^\alpha }$, where $\sigma ( \cdot )$ is a Sigmoid function, the exponent $\alpha$ is used to enhance feature expression. Finally, $A \in {\mathbb{R}^{W \times H \times 1}}$ is expanded along the channel to the size of $V$, and the output $X_{out}$ is the Hadamard product of $A$ and $V$, i.e., ${X_{out}} = {\rm{expand}}(A) \circ V$.  Inspired by self-attention, multi-head SPSA is calculated in sub-channels to enhance the expression of feature subspaces.

Obviously, SPSA has a linear complexity $O(N)$ to the number of pixels and has no learnable parameters. Importantly, SPSA gives different attention to the global spatial content and is competent for the approximation of self-attention. If you are interested, we explain the code, the cross-correlation coefficient, the reverse operation and the computational complexity in detail in the appendix.

\vspace{-1mm}
\begin{minipage}[t]{\textwidth}
\begin{minipage}[t]{0.5\textwidth}
\makeatletter\def\@captype{table}
\centering
\small\caption{Classification results on ImageNet-1k.}
\vspace{-2mm}
\label{table1}
\resizebox{6.8cm}{!}{
\begin{tabular}{lcccc}
\toprule[1pt]
\multicolumn{1}{l}{Method}   &\multicolumn{1}{c}{+ Param.} &\multicolumn{1}{c}{FLOPs} &\multicolumn{1}{c}{Inference} &\multicolumn{1}{c}{Top-1}   \\
\midrule[1pt]
ResNet-50                          &\bf 0   &\bf 4.11G       &\bf1879 &77.28  \\
+ SE                               &2.51M      &4.12G       &1510   &77.86  \\
+ CBAM                                   &2.51M      &4.12G       &1286  &78.24  \\
% + $A^2$-Net                            &7.44M            &6.50G           &-   &77.00  \\
+ GSoPNet1                   &2.73M      &6.39G       &1359   &\bf 79.01  \\
+ AANet                        &0.24M      &4.15G       &-  &77.70  \\
+ ECA        &80          &4.12G       &1769   &77.99  \\
+ FCA                     &2.51M      &4.12G       &1453   &78.57  \\
\rowcolor{tb_color} + SPSA                        &\bf 0          &\bf 4.11G   & 1644  & 78.08  \\
\rowcolor{tb_color}+ SPSA-SE                      & 2.51M     & 4.12G   &1410   &78.31    \\
\rowcolor{tb_color}+ SPSA-ECA                        & 80         & 4.12G   &1442 & 78.25  \\
\rowcolor{tb_color}+ SPSA-FCA                       & 2.51M         & 4.12G   &1256   &78.69  \\
\midrule[1pt]
ResNet-101       &\bf0      &\bf 7.83G       &\bf 1129  &78.72       \\
+ SE         &4.74M      &7.85G  &960     &79.19       \\
+ AANet            &0.85M      &8.05G       &- &78.70       \\
+ ECA          &165      &7.84G      &1003  &79.09       \\
+ FCA             &4.74M      &7.85G       &933 &79.63  \\
\rowcolor{tb_color}+  SPSA           &\bf 0       &\bf 7.83G       &968  &79.42  \\
\rowcolor{tb_color}+  SPSA-SE                    &4.74M       &7.85G       &896   &79.60       \\
\rowcolor{tb_color}+  SPSA-ECA                   &165       &7.84G       &934   &79.49     \\
\rowcolor{tb_color}+  SPSA-FCA                 &4.74M       &7.85G       &808   &\bf 79.65      \\
\midrule[1pt]
ResNet-152       &\bf 0      &\bf 11.56G       &\bf 805   &79.39 \\
+ SE          &6.58M      &11.58G       &758  &79.84 \\
+ AANet           &1.41M      &11.90G       &-&79.10 \\
+ ECA           &250  &11.57G   &785   &79.86 \\
+ FCA           &6.58M      &11.58G       &713  &\bf 80.02 \\
\rowcolor{tb_color}+ SPSA   &\bf 0      &\bf 11.56G       &764       & 79.99 \\
\bottomrule[1pt] 
\end{tabular}
}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\makeatletter\def\@captype{table}
\small\caption{Object detection results on COCO 2017.}
\vspace{-2mm}
\label{table2}
\centering
\resizebox{7cm}{!}{
\begin{tabular}{lccccc}
\toprule[1pt]
\multicolumn{1}{l}{Method}  &\multicolumn{1}{c}{+ Param.}  &\multicolumn{1}{c}{FLOPs} &\multicolumn{1}{c}{AP} &\multicolumn{1}{c}{$AP_{50}$}  &\multicolumn{1}{c}{$AP_{75}$}  \\
\midrule[1pt]
\multicolumn{6}{c}{Faster-RCNN}  \\
\midrule[1pt]
ResNet-50                     &\bf0      &\bf 207.07 &36.4 &58.2 &39.2 \\
+ SE                      &2.51M                 &207.18 &37.7 &60.1 &40.9 \\
+ ECA                     &80                 &207.18 &38.0 &60.6 &40.9 \\
+ FCA                     &2.51M               &207.18 &39.0 &61.1 & 42.3 \\
\rowcolor{tb_color}+ SPSA         &\bf 0     &\bf 207.07 &39.0 &60.3 &42.2 \\
\rowcolor{tb_color}+ SPSA-SE    & 2.51M            &207.18 &\bf 39.5 &\bf 61.2 &\bf 42.9 \\ 
\rowcolor{tb_color}+ SPSA-ECA   &80  &207.18     &39.3 &\bf 61.2 &42.6 \\
\rowcolor{tb_color}+ SPSA-FCA   &2.51M  &207.18     &39.4 &61.0 &42.6 \\
\midrule

ResNet-101                     &\bf 0           &\bf 283.14       &38.7 &60.6 &41.9  \\
+ SE                     & 4.74M           &283.33       &39.6 &62.0 &43.1 \\
+ ECA                     & 165           &283.32       &40.3 &\bf 62.9 &44.0 \\
+ FCA                     & 4.74M           &283.33       &41.2 &\bf 63.3 &44.6 \\
\rowcolor{tb_color}+ SPSA            &\bf 0           &\bf 283.14       &41.2 &62.5 &45.0 \\
\rowcolor{tb_color}+ SPSA-SE              &4.74M           &283.33 &41.3 &62.8 &45.2 \\
\rowcolor{tb_color}+ SPSA-ECA             &165           &283.32       &\bf 41.6 &62.7 &\bf 45.3 \\
\rowcolor{tb_color}+ SPSA-FCA             &4.74M           &283.33       &41.5 &62.8 &45.2\\
\midrule[1pt]
\multicolumn{6}{c}{Mask-RCNN}  \\
\midrule[1pt]
ResNet-50                     &\bf 0           &\bf 260.14      &37.2 &58.9 &40.3\\
+ SE                     & 2.51M           &260.25      &38.7 &60.9 &42.1 \\
+ 1NL                &8.40M           &268.54      &39.0 &61.1 &41.9 \\
+ ECA                     & 80           &260.25      &39.0 &61.3 &42.1 \\
+ FCA               & 2.51M           &260.25      &40.3 &\bf 62.0 &44.1 \\
\rowcolor{tb_color}+ SPSA     &\bf 0  &\bf 260.14      & 39.5 & 60.5 & 43.1 \\
\rowcolor{tb_color}+ SPSA-SE   & 2.51M           &260.25      &\bf 40.5 &\bf 61.6 &\bf 44.2 \\
\rowcolor{tb_color}+ SPSA-ECA   &80  &260.25      &40.0 &61.5 &43.6 \\
\rowcolor{tb_color}+ SPSA-FCA   &2.51M  &260.25      &40.4 & 61.7 &44.0 \\
\midrule[1pt]
\multicolumn{6}{c}{RetinaNet}  \\
\midrule[1pt]
ResNet-50                     &\bf0       &\bf239.32      &35.6 &55.5 &38.2 \\
+ SE                      &2.51M           &239.43      &37.1 &57.2 &39.9 \\
+ ECA            &80           &239.43      &37.3 &57.7 &39.6 \\
\rowcolor{tb_color}+ SPSA            &\bf 0   &\bf 239.32      &37.5 &56.9 &39.9 \\
\rowcolor{tb_color}+ SPSA-SE     &2.51M &239.43       &\bf38.6 &\bf 58.0 &\bf 41.2 \\
\rowcolor{tb_color}+ SPSA-ECA     &80           &239.43      &38.2 &57.8 &40.6 \\
\bottomrule[1pt]
\end{tabular}
}
\end{minipage}
\end{minipage}

\vspace{-2.5cm}
\begin{minipage}[t]{\textwidth}
% \vspace{-5mm}
\begin{minipage}[t]{0.5\textwidth}
\makeatletter\def\@captype{table}
\centering
\small\caption{SPSA application on other baselines.}
\vspace{-2mm}
\label{table3}
\resizebox{6.8cm}{!}{
\begin{tabular}{lccccc} 
\toprule[1pt]
Baselines   &Top-1    &Top-5        &Mothod  & Top-1  &Top-5 \\ 
\midrule
ResNeXt-50 &78.35 &94.11  &+ SPSA  & \bf 78.89          &\bf 94.47\\ 
\midrule
MobileNetV2 & 67.09 & 87.92 &+ SPSA  & \bf 67.89 &\bf 88.40   \\ 
\midrule
ShuffleNetV2  & 65.45 &86.54 &+ SPSA & \bf 65.87 &\bf 86.72  \\
\bottomrule[1pt]
\end{tabular}
}
% \vspace{-2mm}
\end{minipage}
\end{minipage}

\vspace{-3mm}
\section{Experiments \& Conclusion}
\vspace{-3mm}
\textbf{Image classification} On ImageNet-1k, we carry out ResNet-SPSA experiments and take other modules as performance benchmarks, including SE~\citep{hu2018squeeze}, CBAM~\citep{woo2018cbam}, GSoPNet1~\citep{gao2019globalgsopnet}, AANet~\citep{xu2020aanet}, ECA~\citep{shen2021efficientecanet}, and FCA~\citep{fcanet}. As in Table~\ref{table1}, SPSA achieves a better trade-off in accuracy-parameter and accuracy-inference speed. Admittedly, there is a gap between SPSA and SOTA method, but it decreases as the baseline increases. SPSA fused with other channel attentions also all achieve better performance, except for FcaNet which is incompatible with SPSA due to DCT. The results are sufficient to demonstrate the feasibility of SPSA combined with convolution and its effectiveness as spatial attention. Table~\ref{table3} further indicates the generalizability of SPSA to other types of convolutions.

\textbf{Object Detection} In Table~\ref{table2}, we evaluate SPSA using Faster-RCNN~\citep{ren2015fasterrcnn}, Mask-RCNN~\citep{he2017maskrcnn} and RetinaNet~\citep{lin2017retinanet} as detectors and ResNets with FPN as the backbone. Surprisingly, SPSA achieves almost the same performance as the SOTA method. Notably, SPSA surpasses NL~\citep{wang2018nonlocal}, which is also a form of self-attention.

Extensive experiments have demonstrated the feasibility of SPSA as a self-attentive approximation. We trust that this new form of self-attention will have potential in lightweight CNN-ViT hybrid models and inspire researchers to apply it to new model designs.

\subsubsection*{URM Statement}
Author Yuwen Zhai meets the URM criteria of ICLR 2023 Tiny Papers Track.

\bibliography{iclr2023_conference_tinypaper}
\bibliographystyle{iclr2023_conference_tinypaper}

\appendix
\section{Appendix}
\vspace{-2mm}
\subsection{Code of SPSA}
\vspace{-2mm}
SPSA module is extremely simple to implement. As in Figure~\ref{code}, we give a reference implementation of SPSA in PyTorch. Multi-head SPSA simply adds one dimension to the input and adjusts the dimension index of the calculation.

% Figure environment removed

\vspace{-2mm}
\label{appendix}
% You may include other additional sections here.
\subsection{Implementation Details}
\label{impdetail}

\vspace{-2mm}
\subsubsection{ImageNet-1k}
\vspace{-2mm}
Recall that we compare SPSA with other methods on ImageNet-1k taking ResNet~\citep{he2016resnet} families as the backbones. We also apply SPSA to MobileNetV2~\citep{sandler2018mobilenetv2}, ShuffleNetV2~\citep{ma2018shufflenet} and ResNeXt~\citep{xie2017resnext} to verify its generalization. For all backbone networks, we employ exactly the same data augmentation and hyperparameter settings as in \citep{he2016resnet} and \citep{hu2018squeeze}. Specifically, the input images are randomly cropped to 224×224 with random horizontal flipping. We use an SGD optimizer with a momentum of 0.9 and a weight decay of 1e-4. The initial learning rate is set to 0.1 for a batch size of 256 (using 4 GPUs with 64 images per GPU) with the linear scaling rule \citep{goyal2017accurate} and a linear warm-up of 5 epochs. All models are trained within 100 epochs with cosine learning rate decay and label smoothing following FcaNet \citep{fcanet}. We use the Nvidia APEX mixed precision training toolkit for training efficiency. For the testing on the validation set, the shorter side of an input image is first resized to 256, and a center crop of 224 × 224 is used for evaluation.

\vspace{-2mm}
\subsubsection{MS COCO}
\vspace{-2mm}

We use MMDetection toolkit~\citep{chen2019mmdetection} for experiments on MS COCO dataset with the pre-trained ResNet-50 and ResNet-101 as the backbones for the detector. Specifically, the shorter side of the input image is resized to 800. The SGD optimizer has a weight decay of 1e-4, a momentum of 0.9, and a batch size of 8 (4 GPUs with two images per GPU) within 12 epochs. The learning rate is initialized to 0.01 and is decreased by the factor of 10 at the 8th and 11th epochs, respectively. In validation, we report the standard Average Precision (AP) under IOU thresholds ranging from 0.5 to 0.95 in increments of 0.05. We also retain AP scores for small, medium and large objects.

\vspace{-2mm}
\subsection{Discussion of the cross-correlation coefficient and the cosine-similarity}
\vspace{-2mm}


In this subsection we review and discuss the cross-correlation coefficient and the cosine-similarity. Given two sets of vectors $x \in {\mathbb{R}^N}$ and $y \in {\mathbb{R}^N}$. 

\textbf{Cross-correlation coefficient} The population cross-correlation coefficient $\rho _{x,y}$ is defined as the quotient of the covariance and standard deviation between the two variables.
\begin{equation}
    {\rho _{x,y}} = \frac{{{\mathop{\rm cov}} (x,y)}}{{{\sigma _x}{\sigma _y}}} = \frac{{E[(x - {\mu _x})(y - {\mu _y})]}}{{{\sigma _x}{\sigma _y}}},
\end{equation}
where ${{\mathop{\rm cov}} (x,y)}$ is the covariance of $x$ and $y$, and ${{\sigma _x}}$, ${{\sigma _y}}$ are the standard deviations of $x$ and $y$, respectively. Estimating the covariance and standard deviation of the samples, the sample cross-correlation coefficient $C_{x,y}$ is obtained as: 
\begin{equation}
{C_{x,y}} = \frac{{\sum\nolimits_{i = 1}^n {\left( {{x_i} - \bar x} \right)\left( {y - \bar y} \right)} }}
{{\sqrt {\sum\nolimits_{i = 1}^n {{{\left( {{x_i} - \bar x} \right)}^2}\sum\nolimits_{i = 1}^n {{{\left( {y - \bar y} \right)}^2}} } } }}
\label{eq10}
\end{equation}
where $\bar x = \frac{1}{n}\sum\nolimits_{i = 1}^n {{x_i}}$, $\bar y = \frac{1}{n}\sum\nolimits_{i = 1}^n {{y_i}}$. In this paper, we use the above equation to evaluate the pairwise affinity between pixels.

\textbf{Cosine-similarity} According to Euclid's dot product formula
\begin{equation}
x \cdot y = \left\| x \right\|\left\| y \right\|\cos \theta,
\label{eq11}
\end{equation}
the cosine-similarity $Cos_{x,y}$ between the two vectors is obtained
\begin{equation}
Co{s_{x,y}} = \cos \theta  = \frac{{x \cdot y}}
{{\left\| x \right\|\left\| y \right\|}} = \frac{{\sum\nolimits_{i = 1}^n {{x_i}{y_i}} }}
{{\sqrt {\sum\nolimits_{i = 1}^n {{x_i}^2\sum\nolimits_{i = 1}^n {{y_i}^2} } } }}.
\label{eq12}
\end{equation}
Comparing Eq.~(\ref{eq10}) and Eq.~(\ref{eq12}) to obtain Eq.~(\ref{eq13}), it shows that the cross-correlation coefficient is the cosine-similarity after the data centering process. Therefore, the cross-correlation coefficient is less sensitive to fluctuations in the data than the cosine-similarity.
\begin{equation}
  {C_{x,y}} = \frac{{\sum\nolimits_{i = 1}^n {\left( {{x_i} - \bar x} \right)\left( {y - \bar y} \right)} }}
{{\sqrt {\sum\nolimits_{i = 1}^n {{{\left( {{x_i} - \bar x} \right)}^2}\sum\nolimits_{i = 1}^n {{{\left( {y - \bar y} \right)}^2}} } } }} \\ 
   = \frac{{(x - \bar x)\cdot(y - \bar y)}}
{{\left\| {x - \bar x} \right\|\left\| {y - \bar y} \right\|}} \\ 
   = Co{s_{x - \bar x,y - \bar y}} \\ 
\label{eq13}
\end{equation}
Since $\left\| x \right\|\left\| y \right\|$ in Eq.~(\ref{eq12}) would complicate the computation, $\frac{{x \cdot y}}
{{\sqrt d }}$ is used as an alternative in self-attention mechanism to evaluate the similarity between paired vectors, where $d$ points to the vectors' dimensions. Eq.~(\ref{eq11}) shows that for larger values of $d$, the larger dot product's magnitude will affect the similarity representation and push the softmax function to the regions with extremely small gradients~\citep{selfattention}. To counteract this effect, self-attention scale the dot product by $\frac{1}{{\sqrt d }}$. The dot product in self-attention is implemented by highly optimized matrix multiplication code to achieve high parallelism. In contrast, in SPSA architecture, each position in $Q$ is only required to match the similarity with a single $k$ vector, which has high parallelism. Therefore, the cross-correlation coefficient with low sensitivity to data is allowed to be applied as an indicator of pairwise affinity.
\begin{table}[h]
\small
\caption{Comparison experiments of different evaluation methods with ResNet50 as baseline.}
\vspace{-2mm}
\label{cosine}
\centering\scalebox{0.95}{
\centering
\begin{tabular}{lcc} 
\toprule[1pt]
Method                        & ImageNet-1k (Top-1) & Mini-ImageNet (Top-1)  \\ 
\midrule[1pt]
Baseline                      & 77.28              & 80.55                  \\
Dot product cosine-similarity             & 75.69              & 80.32                  \\
Cross-correlation coefficient & 78.08              & 81.59                  \\
\bottomrule[1pt]
\end{tabular}
}
\vspace{-2mm}
\end{table}

In addition, we experimentally verified the superiority of the cross-correlation coefficient over the dot product cosine-similarity in SPSA architecture. We experiment on ImageNet-1k and Mini-ImageNet datasets by replacing the cross-correlation coefficient with dot product. Table~\ref{cosine} demonstrates that dot product similarity does not work well in SPSA. It shows that using the dot product to calculate the similarity to assess the affinity between $Q$ and vector $k$ is insufficient. The cross-correlation coefficient is a better choice.


\vspace{-2mm}
\subsection{Analysis of computational complexity}
\vspace{-2mm}
Unlike self-attention, query $Q$ and value $V$ of SPSA are obtained utilizing an identical mapping of $X$, i.e., $O_Q = O_V = 0$. The computational complexity of $k$ vector obtained by GAP is
\begin{equation}
{O_k} = O\left( {HWC} \right).
\end{equation}
We estimate the correlation coefficient matrix Eq.~(\ref{eq3}) to obtain the computational complexity of generating the weight $A$
\begin{equation}
{O_{{\text{Cross}}}} = O\left( {HW\left( {C + {C^2}} \right)} \right).
\end{equation}
The computational complexity of acting $A$ on $V$ via the Hadamard product is
\begin{equation}
{O_{{\text{Act}}}} = O\left( {HWC} \right).
\end{equation}
Thus, the overall computational complexity of SPSA is
\begin{equation}
{O_{{\text{SPSA}}}} = {O_k} + {O_{{\text{Cross}}}} + {O_{{\text{Act}}}} = O\left( {3HWC + HW{C^2}} \right).
\end{equation}
Compared with self-attention, SPSA has linear complexity for the number of pixels.

\vspace{-2mm}
\subsection{Explanation of reverse operation}
\vspace{-2mm}

% Figure environment removed

As described in section~\ref{method}, SPSA gets the key vector $k$ by the feature map's global average pooling (GAP). It causes SPSA to work differently than the intuition that comes from self-attention. Specifically, GAP is challenging to capture the complex information in the feature maps and misses most of the detailed features~\citep{fcanet}. In contrast to the general features in the global scope represented by GAP, we believe that spatial attention should enhance special features, such as texture details. Intuitively, enhancing special detail features is helpful for visual recognition tasks. Therefore, we use the reverse operation to enhance the specificity features rather than features similar to the $k$ vector generated by GAP.

To intuitively discuss the necessity of the reverse operation in the system, the feature map of SPSA module is visualized in Fig.~\ref{reverse}. Fig.~\ref{reverse} (a) and (d) show the inputs of SPSA module in layer 2.3. Both are generally similar, and with the network optimized iteratively, the feature maps have the same attention to the target and the background. Fig.~\ref{reverse} (b) and (e) show the attention weights obtained from the cross-correlation calculation in SPSA, which have opposite results. The reverse operation drives SPSA to focus almost on the object itself, while SPSA without reverse focuses almost exclusively on the background region. It proves that the reverse operation directly affects the region of attention of SPSA. Naturally, in Fig.~\ref{reverse} (c) and (f), the final outputs show that the SPSA without reverse tends to focus on the background. The reversed SPSA drives the network to focus on the object, which is more beneficial for visual tasks.

\end{document}
