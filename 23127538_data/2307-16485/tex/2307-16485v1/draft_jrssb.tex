%%
%% Copyright 2022 OXFORD UNIVERSITY PRESS
%%
%% This file is part of the 'oup-authoring-template Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'oup-authoring-template Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for OXFORD UNIVERSITY PRESS's document class `oup-authoring-template'
%% with bibliographic references
%%

%%%CONTEMPORARY%%%
% \documentclass[unnumsec,webpdf,contemporary,large]{oup-authoring-template}%
%\documentclass[unnumsec,webpdf,contemporary,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
% \documentclass[unnumsec,webpdf,contemporary,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,contemporary,small]{oup-authoring-template}

%%%MODERN%%%
% \documentclass[unnumsec,webpdf,modern,large]{oup-authoring-template}
\documentclass[modern, medium]{oup-authoring-template}
% \documentclass[unnumsec,webpdf,modern,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
% \documentclass[modern,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,small]{oup-authoring-template}

%%%TRADITIONAL%%%
%\documentclass[unnumsec,webpdf,traditional,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,traditional,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
% \documentclass[unnumsec,namedate,webpdf,traditional,medium]{oup-authoring-template}
%\documentclass[namedate,webpdf,traditional,small]{oup-authoring-template}

\onecolumn % for one column layouts

%\usepackage{showframe}

\graphicspath{{Fig/}}

% line numbers
%\usepackage[mathlines, switch]{lineno}
%\usepackage[right]{lineno}

\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}

\usepackage{graphicx}  % enhanced graphics support
\usepackage{subcaption}  % sub-figure environment
\usepackage{natbib}  % extended citation commands + bib styling
% \usepackage{appendix}  % appendix
% \usepackage{enumitem}  % customisable enumerate environment
\usepackage{multicol}  % multiple column environment
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{docmute}
\input{preamble_jrssb}

\begin{document}

% \journaltitle{Journal Title Here}
% \DOI{DOI HERE}
% \copyrightyear{2022}
% \pubyear{2019}
% \access{Advance Access Publication Date: Day Month Year}
% \appnotes{Paper}

% \firstpage{1}

%\subtitle{Subject Section}


\title[Inference for Degenerate Diffusions]{Parameter Inference for Degenerate Diffusion Processes}

\author[1,$\ast$]{Yuga Iguchi}
\author[1, 2]{Alexandros Beskos}
\author[3]{Matthew Graham}
% \author[3]{Fourth Author}
% \author[4]{Fifth Author\ORCID{0000-0000-0000-0000}}

% \authormark{Author Name et al.}

\address[1]{\orgdiv{Department of Statistical Science}, \orgname{University College London}, \state{London}, \country{UK}}

\address[2]{\orgname{Alan Turing Institute}, \state{London}, \country{UK}}

\address[3]{\orgdiv{Advanced Research Computing Centre}, \orgname{University College London}, \state{London}, \country{UK}}
% \address[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \postcode{Postcode}, \state{State}, \country{Country}}}
% \address[4]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \postcode{Postcode}, \state{State}, \country{Country}}}

\corresp[$\ast$]{Corresponding author. Email: \href{email:yuga.iguchi.21@ucl.ac.uk}{yuga.iguchi.21@ucl.ac.uk}}

% \received{Date}{0}{Year}
% \revised{Date}{0}{Year}
% \accepted{Date}{0}{Year}

%\editor{Associate Editor: Name}

%\abstract{
%\textbf{Motivation:} .\\
%\textbf{Results:} .\\
%\textbf{Availability:} .\\
%\textbf{Contact:} \href{name@email.com}{name@email.com}\\
%\textbf{Supplementary information:} Supplementary data are available at \textit{Journal Name}
%online.}

\abstract{
%%%%%%%%%%%%%%%%%
% 100 words ver
% We study parametric inference for hypo-elliptic SDEs. Existing research focuses on a restricted class of processes when components dichotomise into `rough'/`smooth' ones. The literature omits classes of hypo-elliptic models critical for applications.  We aim to cover this gap, thus analyse the \emph{highly degenerate} class of SDEs, where components split into further sub-groups. Such SDEs include, e.g., the notable case of generalised Langevin equations. However, a time-discretisation scheme with accompanying theory for deduced parameter estimates has yet to be established. We develop such a scheme for high-frequency, complete observations. The results are relevant for much more general data regimes, covering ones often encountered in applications.
%%%%%%%%%%%%%%%% 
% 100-200 words ver
We study parametric inference for hypo-elliptic \mbox{Stochastic} Differential Equations (SDEs). Existing research focuses on a particular class of hypo-elliptic SDEs, with components split into `rough'/`smooth' and noise from rough components propagating directly onto smooth ones, but some critical model classes arising in applications have yet to be explored. We aim to cover this gap, thus analyse the \emph{highly degenerate} class of SDEs, where components split into further sub-groups. Such models include e.g.~the notable case of generalised Langevin equations. We propose a tailored time-discretisation scheme and provide asymptotic results supporting our scheme in the context of high-frequency, full observations. The proposed discretisation scheme is applicable in much more general data regimes and is shown to overcome biases via simulation studies also in the practical case when only a smooth component is observed. Joint consideration of our study for highly degenerate SDEs and existing research provides a general `recipe' for the development of time-discretisation schemes to be used within statistical methods for general classes of hypo-elliptic SDEs.
%%%%%%%%%%%%%%%%%
}

\keywords{
Stochastic Differential Equation; %
Hypo-elliptic Diffusion; %
H\"ormander's Condition; 
Partial Observations; 
Generalised Langevin Equation.}

% \boxedtext{
% \begin{itemize}
% \item Key boxed text here.
% \item Key boxed text here.
% \item Key boxed text here.
% \end{itemize}}

\maketitle

\section{Introduction} \label{sec:intro}
This work addresses the statistical calibration of a wide class of hypo-elliptic diffusions.  
% motivated by both theoretical foundations that have been established over a few decades and 
% important models in applications that are not captured in those analytical developments.
% and is placed within the timeline of recent theoretical and practical developments.
Stochastic Differential Equations (SDEs) are widely used as an effective tool to describe dynamics of the time evolution of phenomena of interest across a multitude of disciplines. Consider SDE models of the following general form: 
%
\begin{align} \label{eq:sde_1}
   d X_t = V_0 (X_t, \theta) dt + \sum_{j = 1}^d V_j (X_t, \theta) d B_{j,t}, \qquad  X_0 = x \in \mathbb{R}^N,
\end{align}
%
with $V_j (\cdot, \theta) : \mathbb{R}^N \to \mathbb{R}^N$, $0 \le j \le d$, for parameter $\theta$, driven by the $d$-dimensional standard Brownian motion $B = (B_{1, t}, \ldots, B_{d, t})$, $t \geq 0$, defined upon the filtered probability space $(\Omega, \mathcal{F}, \{\mathcal{F}_t\}_{t \geq 0}, \mathbb{P})$, with $d, N\ge 1$. Several theoretical results about parameter inference for SDEs have been established under positive definiteness conditions on the diffusion matrix $a = V V^\top\in \mathbb{R}^{N\times N}$, with $V = [V_1, \ldots, V_d]$. In such a case, the solution of~(\ref{eq:sde_1}) is referred to as an \emph{elliptic} diffusion. However, many important applications give rise to diffusion processes that %violate the above condition by 
allow matrix $a$ to be degenerate. We give below examples for such classes of SDEs. %in Section \ref{sec:intro_ex}.  
% We mention, e.g., the (underdamped) Langevin equation, the Synaptic-Conductance model \citep{dit:19}, the NLCAR(p) model \citep{ts:00, str:07}, the Jansen-Rit neural mass model \citep{buc:20}, the Susceptible-Infected-Recovered (SIR) epidemiological model \citep{dur:13, sir:22} and the Generalised Langevin Equation (GLE) \citep{vr:22}. 
Under the \emph{H\"ormander's condition}, discussed later in this work, 
%Section \ref{sec:hormander}), 
% a sufficient condition for the law of $[X_t|X_0=x]$ to admit a density w.r.t.~the Lebesgue measure, 
the process defined via the SDE~(\ref{eq:sde_1}) with degenerate diffusion matrix $a$ permits the existense of a density with respect to (w.r.t.)~the Lebesgue measure for its transition dynamics, and is referred to as a \emph{hypo-elliptic} diffusion. 

% To conduct statistical inference for diffusion processes, one necessitates a (log) likelihood, but in general, the solution of SDEs and its transition density are not analytically available. Thus, a time-discretisation of the diffusion processes should be applied to construct an approximate (log) likelihood. However, careful considerations are required to define a discretisation for degenerate diffusions, otherwise one suffers from asymptotically biased estimation based on the incorrect approximate likelihood as \cite{poke:09} showed via several numerical results. Recently, for a specific class of hypo-elliptic diffusions, \cite{dit:19}, \cite{glot:21} and \cite{iguchi:22} exploited different discretisation schemes to develop approximate likelihoods and showed that the corresponding contrast estimators are asymptotically unbiased. However, some of important hypo-elliptic models in applications, e.g., q-GLE and SIR model, are not covered by those works, and also general insights of how to design an appropriate discretisation scheme for degenerate diffusion processes have not yet been clearly explained in the literature. Our work aims at providing a guideline to define a correct discretisation scheme (or approximate likelihood) for robust parameter estimation of a wide class of degenerate diffusion processes including models as q-GLE and SIR model. 
% 
% 
\subsection{Classes of Diffusion Models}  \label{sec:intro_model}
We can summarise the SDE models we consider in this work via two classes of hypo-elliptic diffusions. 
%In this work we mainly consider two classes of hypo-elliptic diffusions. 
The first hypo-elliptic class is determined via % $X_t = \bigl[ X_{S,t}^\top, X_{R,t}^\top \bigr]^\top$ is 
the following degenerate SDE: 
%
% 
\begin{align}
\begin{aligned} \label{eq:hypo-I}  
d X_t & = 
 \begin{bmatrix}
 d X_{S,t} \\[0.2cm] 
 d X_{R,t} 
 \end{bmatrix}
 = 
 \begin{bmatrix}
  V_{S, 0} ( X_t,  \beta_S)  \\[0.2cm] 
  V_{R, 0} ( X_t,  \beta_R) 
 \end{bmatrix} dt
 + 
 \sum_{j = 1}^d 
 \begin{bmatrix}
 \mathbf{0}_{N_S} \\[0.2cm]
 V_{R, j} (X_t, \sigma)
 \end{bmatrix} d B_{j, t};   \\[0.2cm]
 X_0 & = x 
= 
 \bigl[ x_{S}^\top,  x_{R}^\top \bigr]^\top \in \mathbb{R}^N. 
\end{aligned} \tag{Hypo-I}  
\end{align} 
Here, the involved SDE functionals are specified as follows: 
%
\begin{gather}
 V_{S,0} : \mathbb{R}^N  \times \Theta_{\beta_S} \to \mathbb{R}^{N_S}, \qquad   
V_{R,0} : \mathbb{R}^N  \times \Theta_{\beta_R} \to \mathbb{R}^{N_R}; \nonumber \\ 
 V_{R,j} : \mathbb{R}^N  \times \Theta_{\sigma} \to \mathbb{R}^{N_R},  \quad  1 \le j \le d, 
 \label{eq:V1}
\end{gather} 
% 
for positive integers $N_S, N_R$ such that $N= N_S + N_R$, and unknown parameter vector
% 
$$
\theta = (\beta_{S}, \beta_{R}, \sigma) \in \Theta 
= \Theta_{\beta_S} \times \Theta_{\beta_R} \times \Theta_{\sigma}
 \subseteq \mathbb{R}^{N_{\beta_S}} \times \mathbb{R}^{N_{\beta_R}} \times\mathbb{R}^{N_{\sigma}},
$$ 
% 
for positive integers $N_{\beta_S}, \, N_{\beta_R}, \, N_\sigma$, and  a compact set $\Theta$. For class (\ref{eq:hypo-I}), we will later on introduce a condition upon the vector-valued functionals $\{V_{S,0},V_{R,0},\ldots, V_{R,d}\}$ that is sufficient for the law of $X_t$ to admit a Lebesgue density, and which is related to {H\"ormander's condition}. In brief, the condition stipulates that $X_{R,t}$ is indeed a rough component, and that all coordinates of the drift function $V_{S, 0} (X_t, \beta_S)$ properly relate with the rough component $X_{R ,t}$, so that randomness from $X_{R, t}$ is propagated onto all co-ordinates of vector $X_{S, t}$.

The development of a theoretical and algorithmic framework for parametric inference over class (\ref{eq:hypo-I}) has been the topic of several recent works, see e.g.~\cite{poke:09, dit:19, glot:21, iguchi:22}.
% Under this circumstance, recent analytical results for parameter inference of hypo-elliptic diffusions have been established, e.g., \cite{dit:19}, \cite{glot:21}, and \cite{iguchi:22}.  
% Roughly, we assume that a standard 1st-order stochastic Taylor expansion applied on the drift $V_{S,0}$ of the smooth component allows for randomness from the rough SDE component to be propagated onto its smooth counterpart, in a manner that
% produces a 
% non-degenerate (conditionally) Gaussian approximation for the smooth part, via: 
% %
% \begin{align*}
% \int_0^\Delta V_{S, 0} (X_u, \beta_S) du  \approx V_{S, 0} (x, \beta_S) \Delta + 
% \sum_{\substack{ 1 \le j \le  d \\ 1 \le k \le N_R}} 
%  V_{R ,j}^k  (x, \sigma) \partial_{x_k} V_{S,0} (x, \beta_S) \int_0^\Delta B_{j, u} du.  
% \end{align*}
%
% This Gaussian approximation enables one to construct an approximate tractable log-likelihood for parameter estimation. 
% Based upon this framework, the major recent works (\cite{poke:09}, \cite{dit:19}, \cite{glot:21} and (\cite{iguchi:22})) developed theoretical/numerical results for the model (\ref{eq:hypo-I}). 
% 
However, we stress that several important hypo-elliptic SDEs used in practice do not belong in class (\ref{eq:hypo-I}), and are not covered by recent investigations.   
%an important class of hypo-elliptic SDEs used in practice
% , that includes, e.g., the GLE and SIR models, 
%does not belong within the set of models (\ref{eq:hypo-I}). 
% , and also are not covered by the analytical results we have mentioned above. 
We specify a key class of practically useful but under-explored hypo-elliptic SDEs via the following equation: 
% 
\begin{align}
\begin{aligned} \label{eq:hypo-II}
d X_t &  
= 
\begin{bmatrix}
 d X_{S_1,t} \\[0.2cm] 
 d X_{S_2,t} \\[0.2cm]  
 d X_{R,t} 
 \end{bmatrix}
 =  
\begin{bmatrix}
    V_{S_1, 0} ( X_{S_1, t}, X_{S_2, t}, \beta_{S_1}) \\[0.2cm]
    V_{S_2, 0} ( X_{t} , \beta_{S_2}) \\[0.2cm] 
    V_{R, 0} ( X_{t}, \beta_{R}) 
\end{bmatrix} dt 
+ \sum_{j = 1}^d 
\begin{bmatrix}
   \mathbf{0}_{N_{S_1}} \\[0.2cm]
   \mathbf{0}_{N_{S_2}} \\[0.2cm] 
   V_{R, j} (X_t, \sigma)
\end{bmatrix} d B_{j, t}; \\[0.3cm] 
X_0 & =  x
=
\bigl[ x_{S_1}^\top, x_{S_2}^\top, x_{R}^\top  \bigr]^\top  
\in \mathbb{R}^N.
\end{aligned}  \tag{Hypo-II}  
\end{align}
% 
Thus, the drift functions of the smooth components are now specified as: 
% 
\begin{align*} 
   V_{S_1, 0} : \mathbb{R}^{N_{S_1}+N_{S_2}} \times \Theta_{{\beta_{S_1}}} 
   \to \mathbb{R}^{N_{S_1}}, \qquad  
   V_{S_2, 0} : \mathbb{R}^{N} \times \Theta_{{\beta_{S_2}}} 
   \to \mathbb{R}^{N_{S_2}},
\end{align*}
%
and the parameter vector writes as:
%
$$\theta = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma) \in \Theta =
\Theta_{\beta_{S_1}} \times \Theta_{\beta_{S_2}} \times \Theta_{\beta_{R}} \times \Theta_\sigma \subseteq 
\mathbb{R}^{N_{\beta_{S_1}}} \times
\mathbb{R}^{N_{\beta_{S_2}}} \times 
\mathbb{R}^{N_{\beta_R}} \times
\mathbb{R}^{N_{\sigma}},
$$
%
for positive integers $N_{S_1}$, $N_{S_2}$, $N_{\beta_{S_1}}$, $N_{\beta_{S_2}}$, and $N=N_{S_1}+N_{S_2}+N_R$, where $\Theta$ is again a compact set. Notice that the upper-most smooth component $X_{S_1, t}$ in (\ref{eq:hypo-II}) does not depend on the rough component $X_{R,t}$, thus noise from the latter is not directly propagated  onto $X_{S_1, t}$. We refer to (\ref{eq:hypo-II}) as the \emph{highly degenerate} class of  SDEs.

%a class of the hypo-elliptic models with a part of drift function of the smooth component being independent from the rough component. 
%To treat such a separate key class of models, we introduce a \emph{highly degenerate system} of SDEs specified as:  

For class (\ref{eq:hypo-II}) we will set-up a restriction on its constituent vector-valued functionals $\{V_{S_{1,0}},V_{S_{2,0}},V_{R,0},\ldots,V_{R,d}\}$, 
related to H\"ormander's condition. Roughly, 
such a requirement will guarantee that 
noise from the rough component $X_{R,t}$ indeed propagates onto all co-ordinates of $X_{S_2, t}$, first, before then moving onto $X_{S_1, t}$. Thus,  $X_{S_1, t}$ is `smoother' than $X_{S_2, t}$.
Importantly, a consequence of such a behaviour is that class (\ref{eq:hypo-II}) is \emph{not} included within (\ref{eq:hypo-I}), instead  the two classes, (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), are intrinsically distinct and must be treated separately in terms of theoretical and algorithmic considerations. 
% For class (\ref{eq:hypo-II}), we assume that a non-degenerate (conditional) Gaussian approximation is still available via application of a higher order stochastic Taylor expansion, 
% % for $V_{S_1, 0} (X_{S, t}, \beta_{S_1})$, 
% whence the noise of the rough component propagates first onto $X_{S_2, t}$ and then onto $X_{S_1, t}$. 
% 
\subsection{A Motivating Class of Models} \label{sec:intro_ex}
The \emph{non-Markovian Langevin equation} (or \emph{generalised Langevin equation} (GLE)) is used in a wide range of applications due to its effectiveness in describing complex stochastic systems with memory effects (thus, of non-Markovian structure).
Examples include dynamics observed in protein folding \citep{ay:21}, cancer cells \citep{mi:20}, flocks of birds \citep{fe:20}, molecules \citep{ne:15} and coarse-grained systems \citep{kal:15, li:17}. For simplicity, we consider here a one-dimensional particle with unit mass, and denote its position and momentum, respectively,  by $(q, p)$. Then, a GLE describes the particle dynamics as follows:
% 
\begin{align} \tag{GLE} \label{eq:gle}
\begin{aligned} 
  \dot{q}_t & = \, {p}_t;  \\[0.1cm]
  \dot{p}_t & =  - U'(q_t) 
  - \int_0^t K (t-s) p_s ds 
  + \eta_t, 
 \end{aligned} 
\end{align}
% 
where $U : \mathbb{R} \to \mathbb{R}$ is an appropriate potential function, $K : [0, \infty) \to \mathbb{R}$ is the \emph{memory kernel}, and  $ \eta_t $ is a zero-mean stationary Gaussian noise with auto-correlation specified via a 
fluctuation-dissipation relation in equilibrium, i.e.,   
%
$\mathbb{E} [ \eta_t  \eta_s ] =  K (t-s)$, $s, t > 0$, given a unit temperature.   
% 
Due to the presence of $K(\cdot)$, particle dynamics will depend on the full state history, with such a property being quite desirable in applications, see e.g.~the references given above. However, the cost of generating the dynamics of model (\ref{eq:gle}) can be overly expensive. Thus, a
standard approach followed in practice is to introduce a parametrisation for the memory kernel $K(\cdot)$ and represent the non-Markovian system (\ref{eq:gle}) as a Markovian one on an extended  space, the latter system being referred to as \emph{Quasi-Markovian Generalised Langevin Equation (QGLE)}. 
Such parametrisation is extremely rich, thus being able to accurately capture the behaviour of systems with general %exponential decay memory 
true kernel
$K(\cdot)$.  
%\YI{Add comments on the parametrisation of the kernel function}. 
In particular, a common parametrisation of the memory kernel is the following: 
% 
\begin{align*} %\label{eq:K_first}
K (t) = \alpha \delta (t) - \langle e^{- t A} \lambda, \lambda \rangle, 
\qquad  \alpha > 0, \quad \lambda \in \mathbb{R}^m, \quad A \in \mathbb{R}^{m \times m}, \quad m\ge 1,
\end{align*}
% 
with $\delta=\delta(\cdot)$ the Dirac function. 
In this case, the original system in (\ref{eq:gle}) can be equivalently re-written as the following Markovian one: 
% 
\begin{align} 
\tag{QGLE-I} \label{eq:qgle-I} 
\begin{aligned}
\begin{bmatrix}
d q_t  \\[0.1cm] 
d p_t  \\[0.1cm]
d s_t  
\end{bmatrix}
& = 
\begin{bmatrix} 
p_t  \\[0.1cm]
- \nabla U (q_t) - \alpha p_t - \langle \lambda, s_t \rangle \\[0.1cm]
- p_t \lambda  - A s_t 
\end{bmatrix} dt 
+ \sum_{j = 1}^{m + 1} 
\begin{bmatrix}
{0} \\[0.1cm]
%\sigma^{\, (\mrm{I})}_j  
\sigma_j  
\end{bmatrix} d B_{j, t},  \\[0.2cm]  s_0 &\sim  \mathscr{N} \, ( \mathbf{0}_{m}, \,  I_m), 
\end{aligned} 
\end{align}
% 
with $s_t \in \mathbb{R}^m$ an auxiliary component and $\sigma_j\in \mathbb{R}^{m+1}_+$, $1\le j\le m$. Another typical choice for the memory kernel is the following:
%
\begin{align*} %\label{eq:K_second}
K (t) = \langle e^{- t A} \lambda, \lambda \rangle, 
%\qquad  \lambda \in \mathbb{R}^m, \quad A \in \mathbb{R}^{m \times m}, 
\end{align*}
% 
in which case the equivalent QGLE writes as: 
% 
\begin{align} 
\tag{QGLE-II} \label{eq:qgle-II}  
\begin{aligned} 
\begin{bmatrix}
d q_t  \\[0.1cm] 
d p_t  \\[0.1cm]
d s_t  
\end{bmatrix}
& = 
\begin{bmatrix}
p_t \\[0.1cm]
-  \nabla U (q_t) + \langle \lambda,  s_t \rangle \\[0.1cm] 
- p_t \lambda - A s_t 
\end{bmatrix} dt 
+ 
\sum_{j= 1}^m  
\begin{bmatrix}
 0 \\[0.1cm]
 0 \\[0.1cm] 
 %\sigma_j^{(\mrm{II})}
 \sigma_j
\end{bmatrix} d B_{j, t}, \\[0.2cm]  s_0 &\sim  \mathscr{N} \, ( \mathbf{0}_{m}, \,  I_m), 
\end{aligned}
\end{align}
%
with $\sigma_j \in \mathbb{R}_+^m$. Class (\ref{eq:qgle-I}) is investigated, e.g., in \cite{ce:10}. Then, class (\ref{eq:qgle-II}) is popular, e.g., in thermodynamics modelling, see \cite{pav:14, lei:15}. 
% Details of the explicit connection between the choice of memory kernel $K(\cdot)$ and the obtained form of QGLE are provided, e.g., in \cite{lei:22}. 
Class (\ref{eq:qgle-I}) belongs in  (\ref{eq:hypo-I}), with the rough component comprised of $p_t$, $s_t$.  
%$\bigl[p_t, s_t^\top \bigr]^\top$ is treated as a rough component and $q_t$ is dependent on $p_t$. 
%Thus, (\ref{eq:qgle-I}) is in the class of (\ref{eq:hypo-I}). 
For class (\ref{eq:qgle-II}), the rough component consists  only of $s_t$, with~$q_t$ 
depending on the smooth component $p_t$ and not on $s_t$. Thus, (\ref{eq:qgle-II}) lies within class (\ref{eq:hypo-II}). Recently, parametric inference for GLEs within the QGLE setting, under discrete-time observations of the smooth component $q_t$, has been of interest for applications, see e.g.~\cite{fe:20, vr:22}.    
%
% 
\subsection{Related Works and Objectives}
In this paper we investigate parameter estimation for the two classes of degenerate diffusion processes,  (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), given discrete-time observations obtained at instances 
$0 \le t_0 < t_1 < \cdots < t_n$, $n \in \mathbb{N}$, with equidistant observation intervals $\Delta_n := t_{i} - t_{i-1}$, $1 \le i \le n$. In particular, we consider the following scenarios for 
the observations: 
% 
%
\begin{itemize}
\item[1.] \emph{Complete observation regime}, i.e., with all $N$ co-ordinates of 
$X_{t}$ being observed. 
%
\item[2.] \emph{Partial observation regime}, i.e., with a strict subset of co-ordinates being observed. In agreement with applications, in this setting only the upper-most smooth component is assumed to be observed. 
\end{itemize}
% 
Within class  (\ref{eq:hypo-I}), and for the complete observation regime considered in the produced asymptotics, \cite{dit:19} and \cite{glot:20} develop judicious discrete-time (conditionally) Gaussian approximations for the transition distribution of an SDE. Such a proxy provides contrast estimators proven to be  asymptotically normal in a high-frequency observation setting, i.e., \limit, with a requirement that the step-size scales as $\Delta_n = o (n^{-1/2})$. In \cite{iguchi:22}, such a condition on the step-size to obtain asymptotic normality is weakened to $\Delta_n = o (n^{-1/3})$. In the partial observation regime, with the upper-most smooth component being observed, the missing components must be carefully imputed given the available observations. For SDEs in class (\ref{eq:hypo-I}), it is often the case that the dynamics of the smooth component is determined as $d X_{S, t} = X_{R ,t} dt$. Such a remark also applies for class (\ref{eq:hypo-II}), 
with the role of $X_{S, t}$ taken up by the upper-most smooth component, and the one of $X_{R ,t}$ by the second smooth component.
%; however,  for simplicity we use class (\ref{eq:hypo-I}) in this discussion.
We keep the discussion within class (\ref{eq:hypo-I}), as this is the context typically looked at in earlier literature.  
For the described setting, it is tempting and, indeed, widely used in practice, to recover the hidden rough component via 
finite-differences, using the observations $\{X_{S, t_{i+1}} \}_i$, i.e.~via 
%
$ X_{R, t_{i}} = (X_{S, t_{i+1}} - X_{S, t_i}) / \Delta_n$, 
% 
if the step-size $\Delta_n$ is small enough. However, \cite{poke:09} and \cite{sam:12} show that, in the context of bivariate models within class (\ref{eq:hypo-I}), such an approach delivers (asymptotically) biased estimates of diffusion parameters. \cite{poke:09, dit:19} argue against applying finite-differences and, instead,  consider appropriate It\^o-Taylor schemes leading to non-degenerate conditionally Gaussian approximations for the SDE transition density. Such proxies are then embedded  within MCMC Gibbs samplers or Monte-Carlo 
Expectation-Maximisation (MC-EM) methods to impute the missing components conditionally on observations. Nevertheless, \cite{poke:09} illustrate empirically that the scheme they develop then leads to a biased estimation for the drift parameter, $\beta_R$, of the rough component. Subsequent analytical works \citep{dit:19, glot:20, iguchi:22} illustrated that the bias occurs due to omission of drift terms of size $\mathcal{O} (\Delta_n^2)$ from the It\^o-Taylor expansion of the smooth component within class (\ref{eq:hypo-I}), as such terms are needed to counterbalance the noise terms of size $\mathcal{O} (\Delta_n^{3/2})$ arising in such an expansion.

A main conclusion arising from the above discussion is that a recipe for accurate estimation of both hidden components and parameters is the development of a conditionally Gaussian approximation for the full co-ordinates (as such Gaussianity allows for access to computationally effective inference methodologies) obtained via careful inclusion of higher-order terms from the relevant It\^o-Taylor expansion. 
Such an insight for the design of a `correct' discretisation scheme for the purposes of statistical inference has, arguably, not been clearly spelled out in the literature.
\\ 

The objective of this work is to provide a comprehensive study of statistical calibration for a wide class of degenerate diffusion models. For this purpose, we review previous works for class  (\ref{eq:hypo-I}), and, then, we establish new analytical results for class (\ref{eq:hypo-II}). %i.e.~for important SDE models not covered by recent works in the statistical literature for degenerate SDEs, such as by the derivations in \cite{sam:12, dit:19, glot:21, iguchi:22}. 
The main contributions of our work can be  summarised as follows: 
%
\begin{itemize}
%
\item[(i)] 
For the highly degenerate class (\ref{eq:hypo-II}), we construct a conditionally Gaussian time-discretisation scheme. The corresponding transition density is well-defined (i.e.~non-degenerate) under a suitable assumption on functionals $\{V_{S_{1,0}},V_{S_{2,0}},V_{R,0},\ldots,V_{R,d}\}$ motivated both by modelling considerations and by adherence to H\"ormander's condition. 
We refer to the new proxy as the `locally Gaussian scheme' in agreement with the name assigned by \cite{glot:21} to a conditionally Gaussian scheme developed for class (\ref{eq:hypo-I}).
% 
\item[(ii)] 
For class (\ref{eq:hypo-II}), we define a contrast estimator based on the transition density of the locally Gaussian scheme. Then, we show that the estimator is asymptotically normal in the complete observation regime, under a high-frequency observation setting, i.e., for $n \to \infty$, $\Delta_n \to 0$, $n \Delta_n \to \infty$, with the additional condition that the step-size must scale as $ \Delta_n = o (n^{-1/2})$. 
%
\item[(iii)] Under the partial observation regime often encountered in practical applications, we show via analytical consideration of some case studies that use of a finite-difference method for estimation of hidden components leads to  asymptotically biased estimation of the diffusion parameter $\sigma$ for class (\ref{eq:hypo-II}). Thus, we put forward the developed locally Gaussian scheme for (\ref{eq:hypo-II}) as an effective tool to impute hidden components and estimate parameters. 
% 
\item[(iv)] 
% Via the models (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}),
By reviewing the methodology already produced in the literature for class (\ref{eq:hypo-I}) and examining the new one produced in this work for (\ref{eq:hypo-II}), we can provide a complete guideline for the development of a discretisation scheme for general degenerate diffusion processes so that the corresponding contrast function does not introduce bias in parameter estimation procedures.% under the high-frequency observation setting. 
\end{itemize}
% 
% 
The rest of the paper is organised as follows. Section \ref{sec:hormander} specifies the class of hypo-elliptic SDEs of relevance for this work, with reference to H\"ormander's condition. Section \ref{sec:scheme}
revisits the correct (in terms of its statistical properties) discretisation scheme for class \mbox{(\ref{eq:hypo-I})} and introduces the one for (\ref{eq:hypo-II}). 
Section \ref{sec:main} provides  our core analytical results of  asymptotic consistency and normality for the statistical estimates obtained via the new scheme, in a complete observation setting. All proofs are collected in an Appendix.  We present case studies showcasing emergence of bias when
standard alternative schemes are called upon or when finite-differences are used to impute unobserved components (a common practice in applications).
Under the correct schemes shown here for classes (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), we set up a simple Kalman filter for fitting a non-linear sub-class of models commonly arising in applications (we term these \emph{conditional Gaussian non-linear systems}) in the practical partial observation setting.
Section \ref{sec:numerical_studies} presents numerical studies, for the partial observation regime, both for simple models and ones relevant to real applications,   within class (\ref{eq:hypo-II}). The code used in the numerical studies is available at \href{https://github.com/YugaIgu/calibration-hypoSDEs}{https://github.com/YugaIgu/calibration-hypoSDEs}.
We finish with some conclusions in Section \ref{sec:end}. \\[0.2cm]

\noindent \textbf{Notation.} 
For the highly degenerate class (\ref{eq:hypo-II}), to establish a common notation with (\ref{eq:hypo-I}), we use the argument $x_S=(x_{S_1},x_{S_2})$ and also set: 
% 
\begin{gather*}
X_{S, t}
= \bigl[ X_{S_1, t}^\top, X_{S_2, t}^\top \bigr]^\top\in \mathbb{R}^{N_S}, \quad  
N_S = N_{S_1} + N_{S_2};\\[0.2cm]
\beta_S = \bigl[\beta_{S_1}^{\top}, \beta_{S_2}^{\top}\bigr]^{\top}\in 
\Theta_{\beta_S}, \quad \Theta_{\beta_S} = \Theta_{\beta_{S_1}}\times 
\Theta_{\beta_{S_2}}, \quad N_{\beta_{S}} = N_{\beta_{S_1}} + N_{\beta_{S_2}};
\\[0.2cm]
V_{S,0} (x, \beta_S) 
= 
\bigl[ V_{S_1, 0} (x_S, \beta_{S_1})^\top, \; 
V_{S_2, 0} (x, \beta_{S_2})^\top  \bigr]^\top.
\end{gather*}
For $x  \in \mathbb{R}^N$ and $\theta = (\beta_S, \beta_R, \sigma) \in \Theta$, 
we write
% 
\begin{align}
V_0 (x, \theta)  & = 
\left[ V_{S, 0} (x, \beta_S)^\top, \; 
V_{R,0}  (x, \beta_R)^\top \right]^\top;  \label{eq:V0} \\[0.1cm] 
% 
V_j (x, \theta)  & = 
\left[ \mathbf{0}_{N_S}^\top, \;  V_{R,j} (x, \sigma)^\top \right]^\top, \qquad 1 \le j \le d.  \nonumber
\end{align}  
% 
%  
%% 
For $\varphi (\cdot , \theta) : \mathbb{R}^N \to \mathbb{R}$, $\theta \in \Theta$, bounded up to 2nd order derivatives, we define the differential operators $\mathcal{L}$ and $\mathcal{L}_j, \; 1 \le  j  \le d$, as: 
%  
\begin{gather*}
\mathcal{L} \varphi (x ,  \theta) 
= \sum_{i=1}^N V_0^i (x, \theta) \frac{\partial \varphi} {\partial x_i}(x, \theta)  
+ \frac{1}{2}  \sum_{i_1, i_2 = 1}^N \sum_{k=1}^d  V_k^{i_1} (x, \theta) V_k^{i_2} (x, \theta)  \frac{\partial^2 \varphi }{\partial x_{i_1} \partial x_{i_2} } (x,\theta);  \\
\mathcal{L}_j \varphi (x , \theta)
= \sum_{i=1}^N V_j^i (x, \theta) \frac{\partial \varphi}{\partial x_i}(x , \theta), \ \ 1 \le j \le d,  
\end{gather*} 
% 
for $(x, \theta) \in \mathbb{R}^N \times \Theta$. 
Application of the above differential operators is extended to vector-valued functions in the apparent way, via separate consideration of each scalar component. Denote by $C_b^\infty (\mathbb{R}^{n_1}, \mathbb{R}^{n_2}), \, n_1, n_2 \in \mathbb{N}$, the space of smooth functions $f : \mathbb{R}^{n_1} \to \mathbb{R}^{n_2}$ with bounded partial derivatives of every order.  
We denote the probability law of the process 
$\{ X_{t} \}_{t \geq 0}$ under a parameter $\theta \in \Theta$ as $\mathbb{P}_{\theta}$, and we write
%
$$\probconv, \ \ \distconv$$
%
for convergence in probability and distribution, respectively, under the true parameter $\trueparam$.  We write the expectation under the probability law $\mathbb{P}_\theta$ as $\mathbb{E}_\theta$ to emphasise the dependence on $\theta \in \Theta$. For $u \in \mathbb{R}^n$, $n \in \mathbb{N}$ and the multi-index $\alpha \in \{1, \ldots, n \}^l$, $l \in \mathbb{N}$, we define $$\partial^u_\alpha = \frac{\partial^l}{\partial u_{\alpha_1} \cdots \partial u_{\alpha_l}},$$ i.e.~an operator acting on maps $\mathbb{R}^n \to \mathbb{R}$, and then extended, by separate application on each co-ordinate, on maps $\mathbb{R}^n \to \mathbb{R}^{m}$, $m \in \mathbb{N}$. 

\section{Hypo-Elliptic SDEs} \label{sec:hormander}
%
% Before constructing maximum likelihood estimators (MLEs), 
We fully specify the classes of SDEs of interest in (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), by providing, in each case, appropriate conditions on the collection of functionals
$\{V_{0}, V_{1},\ldots, V_{d}\}$, motivated by modelling considerations and the existence of a Lebesgue density for the SDE transition dynamics. 
%These will be expressed in a form similar to {H\"ormander's condition}, and will be stronger than the latter one.
%Thus, it always be the case that the law of $[X_t | X_0 = x]$ will admit a Lebesgue density even though  the SDE is degenerate, but some additional structure must also be assumed.
We illustrate later on that the imposed conditions suffice so that the locally Gaussian scheme for $X_t$ in (\ref{eq:hypo-II}) we put forward in this paper is non-degenerate. %i.e.~it possesses a Lebesgue density. 




%The condition is described in connection with {H\"ormander's condition}, and more importantly, leads to construction of a local Gaussian approximation with its covariance matrix being positive definite as we explain later in Section \ref{sec:lg-hypo-II}. 

%Thus, due to the condition, the approximate likelihood associated with the local Gaussian approximation for full-coordinates is well-defined, and that can be effectively exploited to estimate the parameter $\theta$ and the hidden components as well under  the partial observations regime. In this section, and then provide the main conditions for the two hypo-elliptic models (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}). Finally, we observe the condition via an example of hypo-elliptic diffusions belonging to (\ref{eq:hypo-II}). 
% 
\subsection{H\"ormander's Condition} \label{sec:hormander_review} 
%
We quickly review the definition of H\"ormander's condition.
Consider the class of SDEs with the general form in (\ref{eq:sde_1}). We define
% 
\begin{align*}
{\tilde{V}_0 (x, \theta) = V_0 (x, \theta) 
- \tfrac{1}{2} \sum_{k = 1}^d \mathcal{L}_k V_k (x, \theta)}, \quad (x, \theta) \in \mathbb{R}^N  \times \Theta.
\end{align*}
%
From standard properties of It\^o's processes,  $\tilde{V}_0$ is the drift function of (\ref{eq:sde_1}) when written as a Stratonovich-type SDE. The functionals $\{\tilde{V}_0, \ldots, V_d\}$ of the Stratonovich SDE can be corresponded to differential operators, the latter applying on mappings on $\mathbb{R}^{N}\mapsto \mathbb{R}^{N}$ and giving as outcome mappings, again, on the same spaces. In particular, we have: 
%
\begin{align*}
 \tilde{V}_0 
\mapsto  \sum_{i = 1}^N \tilde{V}_0^i (x) {\partial_{x_i}}, 
 \qquad  
 V_k \mapsto \sum_{i = 1}^N {V}_k^i (x) {\partial_{x_i}}, 
 \quad  1 \le k \le d.  
\end{align*}
% 
Without confusion, we use the same notation both for the SDE 
functionals and the corresponding differential operators.
Parameter $\theta$ is removed from the expressions for simplicity. For two functionals (equivalently, differential operators) as above, 
$\textstyle{ W = \sum_{i = 1}^N W^i (x)\partial_{x_i}}$ and $\textstyle{ Z = \sum_{i = 1}^N Z^i (x)\partial_{x_i}}$, the Lie bracket is defined as
%
\begin{align*} %\label{eq:LB}
[W, Z] = W\,Z - Z\,W, 
\end{align*} 
% 
that is, for a given $x \in \mathbb{R}^N$, 
$$
[W, Z] (x) = \sum_{i = 1}^N \bigl\{ W^i (x) \partial_{x_i} Z (x)
- Z^i (x) \partial_{x_i} W (x) \bigr\}\in \mathbb{R}^{N}
. 
$$
%
We introduce the collections of functionals
%
\begin{gather*} 
\mathscr{H}_0 = \big\{V_1, \ldots, V_d \big\},  
\qquad 
\mathscr{H}_k = \Big\{ \big\{\,[\tilde{V}_0, V], [V_r, V]\,\big\}  :  V  \in \mathscr{H}_{k-1}, \, 1  \le r \le d \Big\}, 
\quad 
k \ge 1, \\[0.1cm] 
\widetilde{\mathscr{H}}_m = \bigcup_{k = 0}^m \mathscr{H}_k, \quad m \ge 1. 
\end{gather*}
% 
Then, H\"ormander's condition is stated as follows: 
%
\begin{definition}[H\"ormander's Condition]
There exists $M \ge 1$ such that  
$$
\mrm{span} \big\{ V (x) : V \in \widetilde{\mathscr{H}}_M \big\} = \mathbb{R}^N, 
$$
for all $x \in \mathbb{R}^N$. 
\end{definition}
% 
%It is known that if H\"ormander's condition is satisfied at the initial point of SDE, $X_0 = x \in \mathbb{R}^N$, then for any $t > 0$, the law of $X_t$ is absolutely continuous with respect to Lebesgue measure. 

\noindent H\"ormander's condition implies that for any $t > 0$ and any initial condition $X_0=x\in\mathbb{R}^{N}$, the law of $X_t$ is absolutely continuous w.r.t.~the Lebesgue measure.  Also, if the coefficients of the SDE are infinitely-times differentiable, with partial derivatives of all orders being bounded, then the Lebesgue density is smooth, see, e.g., \cite{nua:06, pav:14}.
% 
\subsection{Diffusion Classes (\ref{eq:hypo-I}) and (\ref{eq:hypo-II})}
% 
We  now set up separate conditions for the  
SDEs in classes (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}). These will make use of the drift function, 
$\tilde{V}_0=\tilde{V}_0(x,\theta)$, of the Stratonovich version of the SDEs.  
%
%write the drift functions of the Stratonovich version of the It\^o-type SDEs (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), respectively, as:
% 
% 
%\begin{gather*}
%\widetilde{V}^{\mathrm{(I)}}_0 (x, \theta)
%= 
%\Bigl[
%\widetilde{V}_{S, 0} (x, \theta)^\top, 
%\widetilde{V}_{R,0} (x,  \theta)^\top
%\Bigr]^\top,
%\\
%\widetilde{V}_0^{\mathrm{(II)}} (x, \theta)  
%=
%\Bigl[
%\widetilde{V}_{S_1, 0} (x, \theta)^\top,
%\widetilde{V}_{S_2, 0} (x, \theta)^\top, 
%\widetilde{V}_{R,0} (x,  \theta)^\top
%\Bigr]^\top.
%\end{gather*}
% 
%We define the functionals, for $\theta \in \Theta$: 
% 
%\begin{align*}
% V_{\theta, 0} = \sum_{i = 1}^N \widetilde{V}_0^i (x, \theta) \partial_{x_i}, 
% \quad   
% V_{\theta, k} = \sum_{i = 1}^N {V}_{k}^i (x, \sigma) \partial_{x_i}, \quad 1 \le k \le d.
 % , \quad (x, \theta) \in \mathbb{R}^N \times \Theta.  
%\end{align*}
% 
%We also consider the sets of functionals: 
% 
%\begin{gather*} 
%\mathscr{H}_{\theta, 0} = \bigl\{ V_{\theta, 1}, \ldots, V_{\theta, d} \bigr\}, 
%\quad 
%\mathscr{H}_{\theta, k} = \bigl\{ 
%[V_{\theta, 0}, V]\, :  \,V \in \mathscr{H}_{\theta, k-1} \bigr\},  \quad  k = 1, 2;  \\ 
%\widetilde{\mathscr{H}}_{\theta, m} = \bigcup_{k = 0}^m \mathscr{H}_{\theta, k}, \quad m = 0,1,2.   
%\end{gather*}
%
For $1 \le j \leq k \le N$, we define the projection operator $\mathrm{proj}_{j,k} : \mathbb{R}^N \to \mathbb{R}^{k - j + 1}$ as 
% 
\begin{align*}
x = \bigl[x_1,\ldots, x_N\bigr]^{\top} \mapsto \mathrm{proj}_{j, k} (x)
= \bigl[ x_{j}, \ldots, x_k \bigr]^\top. 
\end{align*}
For (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), we assign the following conditions to fully specify the structure of the corresponding degenerate system of SDEs. 
% 
\begin{condition}[Classes of SDEs]
\label{assump:hypo}
\begin{enumerate}
\item[I.] For class (\ref{eq:hypo-I}), 
%consider the set of functionals 
%
%\begin{gather*}
%\widetilde{\mathscr{H}}_{\theta,0} \equiv %\mathscr{H}_{\theta, 0}^{\mathrm{(I)}} = \bigl\{ %V_{1}, \ldots, V_{d} \bigr\}, \qquad 
%\mathscr{H}_{\theta, 1}^{\mathrm{(I)}} = \bigl\{ 
%[\widetilde{V}_{0}, V]\, :  \,V \in %\mathscr{H}_{\theta, 0}^{\mathrm{(I)}} \bigr\},\\%[0.2cm] \widetilde{\mathscr{H}}_{\theta, %1}^{\mathrm{(I)}} =  \mathscr{H}_{\theta, %0}^{\mathrm{(I)}}\cup \mathscr{H}_{\theta, %1}^{\mathrm{(I)}}. 
%\end{gather*}
for any $(x, \theta) \in \mathbb{R}^N \times \Theta$, it holds that: 
%
\begin{align} 
%\mrm{span} \bigl\{ \pi_{N_S + 1, N} \bigl( V (x) \bigr) \, : \,  V \in \widetilde{\mathscr{H}}_{\theta, 0}  \bigr\} = 
&\mrm{span} \Big\{ V_{R,k} (x,\sigma),\,1\le k\le d\Big\} =
\mathbb{R}^{N_R}; 
\nonumber
\\[0.2cm]
&
%\mrm{span} \{ V(x) : V \in 
%\widetilde{\mathscr{H}}_{\theta, 1}^{\mathrm{(I)}} \} = \mathbb{R}^N. 
\mrm{span} \Big\{ \big\{\,V_{k} (x,\sigma),\,[\tilde{V}_{0}, V_k](x,\theta)\,\big\},\,1\le k\le d\Big\} = \mathbb{R}^N. 
\label{eq:span-I} 
\end{align} 
% 
\item[II.] In the case of class (\ref{eq:hypo-II}),
for any $(x, \theta) \in \mathbb{R}^N \times \Theta$, it holds that:
%   
\begin{align}
&\mrm{span} \Big\{ V_{R,k} (x,\sigma),\,1\le k\le d\Big\} = \mathbb{R}^{N_R};
\nonumber
\\[0.2cm]
&
\mrm{span} \Big\{ \mathrm{proj}_{N_{S_{1}}+1,N}\big\{  V_{k} (x,\sigma)  \bigr\}, \,\mathrm{proj}_{N_{S_{1}}+1,N} \big\{\, [\tilde{V}_{0}, V_k](x,\theta) \big\},\,1\le k\le d\Big\} = \mathbb{R}^{N_{S_2} + N_R }; 
\nonumber
\\[0.2cm] 
&
\mrm{span} \Big\{ \big\{\,V_{k} (x,\sigma),\,[\tilde{V}_{0}, V_k](x,\theta)\,,\bigl[\tilde{V}_{0},[\tilde{V}_0,V_k]\bigr](x,\theta)\,\big\},\,1\le k\le d\Big\} = \mathbb{R}^N. 
\label{eq:span-II}
\end{align}
% 
\end{enumerate}
%
\end{condition}
%
\noindent Note that H\"ormander's condition holds for (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}) under \ref{assump:hypo}-I and \ref{assump:hypo}-II (for $M=1$ and $M=2$) respectively.
%and thus for any $t >0$, the law of $X_t$ is absolutely continuous with respect to the Lebesgue measure for given an initial point $x \in \mathbb{R}^N$ and a parameter $\theta \in \Theta$.
% 
% 
\begin{remark} 
Conditions \ref{assump:hypo}-I and \ref{assump:hypo}-II 
%provide some appropriate structure to models from the two degenerate classes of SDEs under consideration. As a result, they 
 separate classes (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}).
%for the models (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), respectively. 
%However, in order to distinguish these degenerated system, we assume the rest of the equations to be held. 
In particular, the top equation in both \ref{assump:hypo}-I, \ref{assump:hypo}-II implies that the diffusion matrix of the rough component is of full rank, thus $X_{R,t}$ acquires the roughness of an elliptic SDE.
The second equation in \ref{assump:hypo}-I, \ref{assump:hypo}-II ensures that all co-ordinates of component $X_{S,t}$ and $X_{S_2,t}$, respectively, possess the same smoothness as integrals of elliptic SDEs.
Finally, the third equation in \ref{assump:hypo}-II implies that component  $X_{S_1,t}$ has the smoothness of second integrals of elliptic SDEs.
Note, e.g., that (\ref{eq:span-I}) will not hold for (\ref{eq:hypo-II}), since for the highly degenerate case, due to the drift function of the upper-most component $X_{S_1, t}$ not involving $X_{R, t}$, 
we have $$\mathrm{proj}_{1, N_{S_1}} [\tilde{V}_{0}, V_{k}]= \mathbf{0}_{N_{S_1}},\quad  1 \le k \le d.$$
To check this latter equation, notice that for both (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}) we have: i) $V_0$ and $\tilde{V}_{0}$ coincide on the smooth co-ordinates; ii) $\tilde{V}_0 V_k$ is zero on the smooth co-ordinates. Then, for (\ref{eq:hypo-II}) we additionally have that $V_k \tilde{V}_0$ is zero on the upper-most $N_{S_1}$ co-ordinates due to the particular choice of $V_0=V_0(x_{S_1},x_{S_2})$.   
\end{remark} 
% 
\begin{remark}
We introduced conditions \ref{assump:hypo}-I \& II upon functionals $\{V_{0}, V_{1},\ldots, V_{d}\}$, so that 
the two classes of SDEs, (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), possess sufficient structure to allow for their intended use for the modelling objectives in mind.
It turns out that the exact same conditions \ref{assump:hypo}-I \& II play a key role so that the locally Gaussian schemes written down later in Section \ref{sec:scheme} are 
well-defined with a positive definite covariance matrix.  
\end{remark}
% 
\subsubsection{An Example for \ref{assump:hypo}-II}
%
We provide an example for \ref{assump:hypo}-II via the following three-dimensional hypo-elliptic diffusion motivated from model class (\ref{eq:qgle-II}):
%
\begin{align} 
\begin{aligned} \label{eq:qgle} 
 d q_t & = p_t d t; \\[0.2cm] 
 d p_t & = \bigl( -  \nabla U (q_t) + \lambda  s_t \bigr) dt;  \\[0.1cm] 
 d s_t & = \bigl( - \lambda p_t  - \alpha s_t \bigr) dt +  \sigma d B_{1, t}, 
\end{aligned}
\end{align} 
%
where $U:\mathbb{R} \to \mathbb{R}$, $\alpha  > 0$, $\sigma > 0$ and $\lambda \in \mathbb{R}/\{0\}$. 
%Following the notation of vector fields introduced in Section \ref{sec:hormander_review}, 
In this case, for $ x=(p,q,s) \in \mathbb{R}^3$, $\theta = (\lambda, \alpha, \sigma) \in \Theta$, we have:
%
\begin{gather*}
\tilde{V}_{0} = p\,\partial_{q}
+ \bigl( - \nabla U (q) + \lambda s \bigr) \partial_{p} 
+ \bigl( -\lambda p - \alpha s \bigr) \partial_{s}, 
\qquad  V_{1} = \sigma \partial_{s}, \\[0.2cm]
[\tilde{V}_{0}, V_{1}]  = 
- \lambda \sigma \partial_p + \alpha \sigma \partial_{s}, \qquad  
\bigl[\tilde{V}_{0}, [\tilde{V}_{0}, V_{1}] \bigr] = \lambda \sigma \partial_{q}  
- \lambda \alpha \sigma \partial_{p} 
+ \sigma (- \lambda^2  + \alpha^2 ) \partial_{s}. 
\end{gather*}
% 
We obtain: 
% 
\begin{align*} 
\mrm{span} \bigl\{ V_R (x, \theta) \bigr\} 
&=  \mrm{span} \bigl\{ \sigma \bigr\} = \mathbb{R}; \\[0.2cm] 
\mrm{span} \Big\{ 
\mathrm{proj}_{2, 3} \big\{ V_1 (x, \theta) \bigr\}, 
\mathrm{proj}_{2, 3} \big\{
[\tilde{V}_0,V_1] (x, \theta) 
\big\} \Big\}&= \mrm{span} \left\{ 
\begin{bmatrix}
0 \\ 
\sigma
\end{bmatrix}, \, 
\begin{bmatrix}
-\lambda \sigma \\ 
\alpha \sigma 
\end{bmatrix} 
\right\}  = \mathbb{R}^2; \\[0.2cm]
%
\mrm{span} \Big\{  V_1 (x, \theta), 
[\tilde{V}_0,V_1] (x, \theta), \bigl[\tilde{V}_{0}, [\tilde{V}_{0}, V_{1}] \bigr] (x, \theta) \Big\} & = \mrm{span} \left\{ 
\begin{bmatrix}
0 \\ 
0 \\ 
\sigma
\end{bmatrix}, \, 
\begin{bmatrix}
0 \\ 
-\lambda \sigma \\ 
\alpha \sigma 
\end{bmatrix}, \, 
\begin{bmatrix}
\lambda \sigma  \\ 
-\lambda \alpha \sigma \\ 
\sigma (- \lambda^2 + \alpha^2) 
\end{bmatrix} 
\right\} \\
&= \mathbb{R}^3. 
\end{align*}
% 
Thus, SDE (\ref{eq:qgle}) satisfies condition \ref{assump:hypo}-II and lies within the framework of class (\ref{eq:hypo-II}). 
% 
\section{Time-Discretisation of Hypo-Elliptic SDEs} \label{sec:scheme}
We discuss time-discretisation schemes for hypo-elliptic diffusions  within classes (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), with the focus being on the performance of the schemes for the purposes of parametric inference. We set up the context by reviewing  schemes proposed in literature for class (\ref{eq:hypo-I}). 
We then propose a new scheme for the highly degenerate diffusion class (\ref{eq:hypo-II}) that will later be proven to possess desirable statistical properties. 
Hereafter, to distinguish among the two classes of SDEs, we use the notation $\textstyle{ X_t^{(\mrm{I})}}$ and $\textstyle{X_t^{(\mrm{II})}}$ for  processes in (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}), respectively.     
%
% 
\subsection{Time-Discretisation of (\ref{eq:hypo-I}) 
-- Brief Review}
We review relevant schemes for the hypo-elliptic class (\ref{eq:hypo-I}) used in the literature. 
% 
First, the classical \emph{Euler-Maruyama} scheme is defined as follows, for $0 \le i \le n$, 
% 
\begin{align*} 
\begin{aligned}
 X_{S, i+1}^{\mrm{EM}, (\mrm{I})}   
 & = X_{S, i}^{\mrm{EM}, (\mrm{I})} 
+ V_{S, 0} \bigl( X_{i}^{\mrm{EM}, (\mrm{I})}, \beta_S \bigr) \Delta_n ; \\ 
 X_{R, i+1}^{\mrm{EM}, (\mrm{I})} 
 & = X_{R, i}^{\mrm{EM}, (\mrm{I})} 
 + V_{R, 0} \bigl( X_{i}^{\mrm{EM}, (\mrm{I})}, \beta_R \bigr) \Delta_n 
 + \sum_{j = 1}^d V_{R, j} \bigl( X_{i}^{\mrm{EM}, (\mrm{I})}, \sigma \bigr) 
  \times \bigl( B_{j, i+1} - B_{j, i} \bigr), 
\end{aligned} %\label{eq:EM}
\end{align*}
% 
with subscript $i$ in $X_{i}^{\mrm{EM}, (\mrm{I})}$ and $B_{j, i}$ indicating the time instance $t_i=i\Delta_n$. The approximation of the smooth component does not involve noise, thus the Euler-Maruyama scheme is degenerate. \cite{poke:09} studied some example bivariate SDEs with drift function $V_{S, 0}(x, \beta_S \bigr) = x_{R}$, and showed that use of the Euler-Maruyama scheme in the high-frequency partial observation regime, where only the smooth component $X_{S,t}$ is observed, induces bias in parameter estimates. Note that in this setting the unobserved component $X_{R,t}$ is estimated via a 
finite-difference approach, i.e., 
%
\begin{align*} 
   X_{R, i}^{\, (\mrm{I})} \approx 
    \hat{X}_{R, i}^{\, (\mrm{I})} 
    = \tfrac{X_{S, i+1}^{\, (\mrm{I})} - X_{S, i}^{\, (\mrm{I})}}{\Delta_n},
\end{align*}
%  
and such an imputation is a main cause for the presence of bias in the estimation of  parameter $\sigma$. To sidestep the above issue, \cite{poke:09} proposed the following conditionally Gaussian scheme, for $0 \le i \le n$: %$\widetilde{X}_{i}^{(\mrm{I})} = %\bigl( \widetilde{X}_{S, i}^{(\mrm{I})}, \, \widetilde{X}_{R, i}^{(\mrm{I})}  \bigr), \, 0 \le i \le n$ as:
%
\begin{align} \label{eq:IS}
\begin{aligned} 
 \widetilde{X}_{S, i+1}^{\, (\mrm{I})} & = 
 \widetilde{X}_{S, i}^{\, (\mrm{I})} 
 + V_{S, 0} \bigl( \widetilde{X}_{i}^{\, (\mrm{I})} ,  \beta_S \bigr) \Delta_n  
+ \sum_{j = 1}^d \mathcal{L}_j V_{S, 0} (\widetilde{X}_{i}^{\, (\mrm{I})}, \theta) \int_{t_i}^{t_{i+1}} \int_{t_i}^{u} d B_{j,v} du;   \\ 
 \widetilde{X}_{R, i+1}^{\, (\mrm{I})} 
 & = \widetilde{X}_{R, i}^{\, (\mrm{I})} 
 + V_{R, 0} \bigl( \widetilde{X}_{i}^{\, (\mrm{I})} , \beta_R \bigr) \Delta_n 
 + \sum_{j = 1}^d V_{R, j} \bigl( \widetilde{X}_{i}^{\, (\mrm{I})} , \sigma \bigr) \times \bigl( B_{j, i+1} - B_{j, i} \bigr). 
\end{aligned} 
\end{align}
% 
% Note that $\widetilde{X}_{R, \Delta}^{\, (\mrm{I})}$ is obtained by simply applying Euler-Maruayama scheme to $X_{R, \Delta}^{\, (\mrm{I})}$, and 
Note that now the smooth component $\widetilde{X}_{S,i+1}^{\, (\mrm{I})}$ involves Gaussian noise after application of an It\^o-Taylor expansion for $\textstyle V_{S, 0} (X_t, \beta_S)$. Under condition \ref{assump:hypo}-I, $(\widetilde{X}^{\, (\mrm{I})}_{S, i+1}, \widetilde{X}_{R, i+1}^{\, (\mrm{I})})$ is conditionally Gaussian with an invertible covariance matrix. Then, \cite{poke:09} utilised the well-posed likelihood of scheme (\ref{eq:IS}) to estimate  both the hidden paths of the rough components and the parameters via a Bayesian approach, namely Gibbs sampling. Under 
a high-frequency observation setting, they empirically showed that the estimate of parameter $\sigma$ is asymptotically unbiased, 
but the estimator of the drift parameter $\beta_R$ based on scheme (\ref{eq:IS}) suffers from bias even in the complete observation regime. 

%Instead of the scheme (\ref{eq:IS}), 
\cite{glot:20} introduced the `local Gaussian' scheme, %$\bar{X}_{i}^{(\mrm{I})} = \bigl( \bar{X}_{S, i}^{(\mrm{I})}, \, \bar{X}_{R, i}^{(\mrm{I})}  \bigr) \in \mathbb{R}^{N_{S}} \times \mathbb{R}^{N_R}, \, 
where, for $0 \le i \le n$, 
%defined as: 
%
\begin{align}  \label{eq:lg_hypo_I}
\begin{aligned} 
  \bar{X}_{S, i+1}^{\, (\mrm{I})} & = \bar{X}_{S, i}^{\, (\mrm{I})}  +
  V_{S, 0} \bigl( \bar{X}_{i}^{\, (\mrm{I})},  \beta_S \bigr) \Delta_n + \tfrac{\Delta_n^2}{2} \mathcal{L} V_{S, 0} 
  \bigl(\bar{X}_{i}^{\, (\mrm{I})}, \theta \bigr)  \\ 
  & \qquad \qquad \qquad \qquad \qquad \qquad 
  + \sum_{j = 1}^d \mathcal{L}_j V_{S, 0} (\bar{X}_{i}^{\, (\mrm{I})}, \theta) \int_{t_i}^{t_{i+1}}  \int_{t_i}^{u}  d B_{j,v} du;  \\ 
  % 
  \bar{X}_{R, i+1}^{\, (\mrm{I})} & = \bar{X}_{R, i}^{\, (\mrm{I})} + V_{R, 0} \bigl( \bar{X}_{i}^{\, (\mrm{I})}, \beta_R \bigr) \Delta_n + \sum_{j = 1}^d V_{R, j} 
  \bigl( \bar{X}_{i}^{\, (\mrm{I})}, \sigma \bigr) \times \bigl( B_{j, i+1} - B_{j, i} \bigr).  
\end{aligned} \tag{LG-I}
\end{align} 
% 
Compared to (\ref{eq:IS}), scheme (\ref{eq:lg_hypo_I}) includes term $\textstyle{ \Delta_n^2 (\mathcal{L} V_{S, 0}) (\bar{X}^{\, (\mrm{I})}_i, \theta)}/2$ in the smooth component.
%$\bar{X}_{S, i+1}^{\, (\mrm{I})}$, which is the main (and only) difference from the improved scheme (\ref{eq:IS}). 
\cite{glot:20} illustrate the significance of this term for the purposes of parameter inference, 
by proving asymptotic consistency and normality for the contrast estimator derived from the likelihood of the discretisation scheme (\ref{eq:lg_hypo_I}), in the high-frequency, complete observation regime, namely, $n \to \infty$, $\Delta_n \to 0$, $n \Delta_n \to \infty$, under the step-size condition $\Delta_n = o (n^{-1/2})$. 
%Thus, by adding the $O (\Delta_n^2)$ term in the approximation of smooth component, biased estimation for the drift parameter $\beta_R$ was resolved.
% The proof also implies that the contrast estimator based on the improved scheme (\ref{eq:IS}) leads to an asymptotically biased estimation for the drift parameter $\beta_R$. 
%
%
\begin{remark}
\cite{dit:19} applied a strong 1.5 order scheme \citep{kloe:92} to construct contrast estimators for class (\ref{eq:hypo-I}) with $N_S = 1$, under strong conditions for the diffusion matrix so that the scheme becomes conditionally Gaussian. Then, they provided two separate contrast functions for estimating $\beta_S$ and $(\beta_R, \sigma)$ from the  approximate Gaussian density for $X_S$ and $X_R$, respectively, rather than the joint density. As noted in Remark 4.6 in \cite{glot:20}, the separate contrast functions results in a larger asymptotic variance for the estimation for  $\beta_S$ compared with the single contrast estimator defined via the joint density of rough and smooth components. 
\end{remark}
% 
% 
\subsection{Time-Discretisation of (\ref{eq:hypo-II})}  \label{sec:lg-hypo-II} 
We propose a time-discretisation scheme for the second hypo-elliptic class (\ref{eq:hypo-II}),
with desirable properties for the purposes of parameter inference. 
The brief review of schemes for (\ref{eq:hypo-I}) in the previous section suggests that the discretisation scheme for (\ref{eq:hypo-II}) should satisfy the following two key criteria: 
% 
% 
\begin{itemize}
    \item[I.] 
    The scheme should be conditionally non-degenerate, i.e., the law of $X_{t_{i+1}}$ given $X_{t_i}$ should admit a Lebesgue transition density for the full co-ordinates. This will allow to impute unobserved paths conditionally on observations without making use of bias-inducing finite-difference approximations. 
    \item[II.]  The scheme should involve deterministic terms 
    obtained from careful truncation of the stochastic Taylor expansion for the drift of the smooth component,  $V_{S,0} (X_{t}^{\, (\mrm{II})}, \beta_{S})$, so that the contrast estimator corresponding to the scheme is asymptotically unbiased under the high-frequency, complete observation regime.   
    % 
\end{itemize}
% 
% 
As for Criterion I, we will explain later in Section \ref{sec:case_2} that, indeed, use of a degenerate discretisation scheme or of 
finite-differences to estimate hidden components induces a bias in the estimation of parameters. Based upon the above key criteria, we propose the following discretisation scheme for (\ref{eq:hypo-II}): 
% $
% \textstyle{
% \bar{X}_{i}^{\, {(\mrm{II})}}   
% = 
% \bigl[ 
% \bigl( \bar{X}_{S_1, i}^{\, \mathrm{(II)}} \bigr)^\top, \, 
% \bigl( \bar{X}_{S_2, i}^{\, \mathrm{(II)}} \bigr)^\top, \,  
% \bigl( \bar{X}_{R, i}^{\, \mathrm{(II)}} \bigr)^\top 
% \bigr]^\top 
% } 
% $ 
% % 
% with 
% 
% 
\begin{align}
\begin{aligned} \label{eq:lg_hypo_II} 
\bar{X}_{S_1, i+1}^{\, \mathrm{(II)}} 
    & = 
    \mu_{S_1} \bigl( \Delta_n, \bar{X}_{i}^{\, \mathrm{(II)}}, \theta \bigr)  
    + \sum_{j = 1}^d \mathcal{L}_{j} \mathcal{L} V_{S_1, 0} 
    \bigl(\bar{X}_{i}^{\, \mathrm{(II)}} , \theta \bigr) \int_{t_i}^{t_{i+1}} \int_{t_i}^u \int_{t_i}^v  
    dB_{j, w} dv du;  \\[0.2cm] 
    \bar{X}_{S_2, i+1}^{\, \mathrm{(II)}} 
    & = 
    \mu_{S_2} \bigl( \Delta_n,  \bar{X}_{i}^{\, \mathrm{(II)}} , \theta \bigr) 
    + \sum_{j = 1}^d \mathcal{L}_{j} V_{S_2, 0} (\bar{X}_{i}^{\, \mathrm{(II)}} , \theta) \int_{t_i}^{t_{i+1}}  \int_{t_i}^u d B_{j, v} du;  \\[0.2cm]  
    \bar{X}_{R, i+1}^{\, \mathrm{(II)}} 
    & = 
    \mu_{R} \bigl( \Delta_n, \bar{X}_{i}^{\, \mathrm{(II)}} , \theta \bigr)  
    + \sum_{j = 1}^d V_j \bigl( \bar{X}_{i}^{\, \mathrm{(II)}} , \sigma \bigr) \times \bigl(B_{j,i+1} - B_{j, i} \bigr),   
\end{aligned} \tag{LG-II}
\end{align} 
% 
where we have set, for $(\Delta, x, \theta) \in (0, \infty) \times \mathbb{R}^N \times \Theta$, 
% 
% 
\begin{align*}  %\label{eq:mean_expansion}   
\begin{bmatrix}
    {\mu}_{S_1}  (\Delta, x, \theta)  \\[0.3cm] 
    {\mu}_{S_2}  (\Delta, x, \theta)  \\[0.3cm]  
    {\mu}_{R}  (\Delta, x, \theta)  
\end{bmatrix}
% 
=
% 
\begin{bmatrix}
    x_{S_1} + V_{S_1,0} (x_S, \beta_{S_1}) \Delta 
    + \mathcal{L} V_{S_1, 0} (x, \theta) \tfrac{\Delta^2}{2}
    + \mathcal{L}^2 V_{S_1, 0} (x, \theta) \tfrac{\Delta^3}{6}   
    \\[0.3cm]
    x_{S_2} + V_{S_2,0} (x, \beta_{S_2}) \Delta 
    + \mathcal{L} V_{S_2, 0} (x, \theta) \tfrac{\Delta^2}{2}  \\[0.3cm] 
    x_R + V_{R, 0} (x, \beta_R) \Delta
\end{bmatrix}. 
\end{align*}   
% 
%We provide a number of remarks on the local Gaussian scheme (\ref{eq:lg_hypo_II}). 
Notice that the scheme involves $3d$ Gaussian random variables: 
%
\begin{align*} 
B_{j, i+1} - B_{j, i}, \quad  
\int_{t_i}^{t_{i+1}} \int_{t_i}^u d B_{j,  v} du, \quad 
\int_{t_i}^{t_{i+1}} \int_{t_i}^u \int_{t_i}^v  d B_{j, w} dv du, \qquad  1 \le j \le d. 
\end{align*}
% 
The latter of the above integrals appears due to the  
application of a third-order stochastic Taylor expansion on $V_{S_1,0} (x_S, \beta_{S_1})$, in the smoothest component $\bar{X}_{S_1, i+1}^{\, (\mrm{II})}$.
%Due to the application of a third-order stochastic Taylor expansion on $V_{S_1,0} (x_S, \beta_{S_1})$, the smoothest component $\bar{X}_{S_1, i+1}^{\, (\mrm{II})}$ involves the Gaussian variables $\textstyle \int_{t_i}^{t_{i+1}} \int_{t_i}^u \int_{t_i}^v  d B_{j, w} dv du$, $1 \le j \le d$. The requirement of higher order stochastic Taylor expansion to induce randomness in the smoothest component is associated with H\"ormander's condition, in that we need to add a set of higher order Lie brackets, i.e., $\{ [\tilde{V}_{0}, [\tilde{V_0}, V_{j}]] \}_{1 \le j \le d}$ so that $\mathbb{R}^N$ is spanned and the transition density of the scheme is non-degenerate. Secondly, the deterministic part of $\bar{X}_{S_1, i+1}^{\, (\mrm{II})}$ involves terms up to $\mathcal{O} (\Delta_n^3)$ so that
% for any $(x, \theta) \in \mathbb{R}^N \times \Theta$, there exists a constant $C = C (x, \theta) > 0$ s.t. 
% 
%\begin{align*}
%\, \mathbb{E}_\theta 
%   \Bigl[\,\Bigl|X_{S_1, i+1}^{\, (\mrm{II})}
%    - \mu_{S_1} (\Delta_n, X_{i}^{\, (\mrm{II})} , \theta) 
%   \bigl. \,\Bigr| \bigr| \, \mathcal{F}_{t_i} \Bigr] \, \Bigr| \le C \Delta_n^4, 
%\end{align*}
%
%for some constant $C > 0$ depending on $X_{i}^{\, (\mrm{II})} $ and $\theta$. 
As we will show in Section \ref{sec:main}, the log-likelihood based on the local Gaussian scheme (\ref{eq:lg_hypo_II}) produces a contrast estimator that is asymptotically unbiased in the high-frequency, complete observation regime. In order for the deduced contrast function to provide desirable asymptotic properties, it is required to include terms up to $\mathcal{O} (\Delta_n^3)$ in the definition of $\mu_{S_1}$, otherwise estimation of the parameter $\beta_R$ in the model (\ref{eq:hypo-II}) can be asymptotically biased as \cite{poke:09} observed for some bivariate hypo-elliptic diffusions in the framework of (\ref{eq:hypo-I}). 
\\ 

% 
\noindent We denote by $\Sigma  (\Delta, x, \theta)$ the covariance matrix for one-step implementation of scheme $\bar{X}^{(\mrm{II})}$, with step-size $\Delta >0$, current state $x \in \mathbb{R}^N$ and parameter $\theta \in \Theta$. The covariance matrix is given as: 
% 
\begin{align}  \label{eq:covariance_lg}
\Sigma (\Delta, x, \theta) 
=
\begin{bmatrix}
\Sigma_{S_1 S_1} (\Delta, x, \theta) & 
\Sigma_{S_1 S_2}  (\Delta, x, \theta) & 
\Sigma_{S_1 R}  (\Delta, x, \theta) \\[0.2cm]
\Sigma_{S_2 S_1}  (\Delta, x, \theta) & 
\Sigma_{S_2 S_2} (\Delta, x, \theta) & 
\Sigma_{S_2 R } (\Delta, x, \theta) \\[0.2cm]
\Sigma_{R S_1} (\Delta, x, \theta) & 
\Sigma_{R S_2}  (\Delta, x, \theta) &
\Sigma_{R R}  (\Delta, x, \theta) 
\end{bmatrix},    
\end{align}
% 
% 
where each block matrix is specified as: for $x=(x_{S_1}, x_{S_2}, x_R) \in \mathbb{R}^N$, $\theta = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma) \in \Theta $, 
%
\begin{align*}
\begin{aligned} %\label{eq:block_Sigma}
   & \Sigma_{S_1 S_1}  (\Delta, x, \theta)
   \equiv \tfrac{\Delta^5}{20} a_{S_1} (x, \theta), \quad  
   \Sigma_{S_1 S_2}  (\Delta, x, \theta)
   \equiv \tfrac{\Delta^4}{8}
   \partial_{x_{S_2}}^\top  V_{S_1, 0} (x_{S}, \beta_{S_1}) 
   a_{S_2} (x, \theta);  \\[0.3cm]
   & \Sigma_{S_1 R}  (\Delta, x, \theta)
   \equiv \tfrac{\Delta^3}{6}
    \partial_{x_{S_2}}^\top  V_{S_1, 0} (x_{S}, \beta_{S_1}) 
   \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) 
   a_R (x, \sigma); \\[0.3cm] 
   & \Sigma_{S_2 S_1}  (\Delta, x, \theta) 
   \equiv  \Sigma_{S_1 S_2}  (\Delta, x, \theta)^\top, \quad
   \Sigma_{S_2 S_2}  (\Delta, x, \theta) 
   \equiv \tfrac{\Delta^3}{3} a_{S_2} (x, \theta); \\[0.3cm] 
   & \Sigma_{S_2 R} (\Delta, x, \theta) 
   \equiv \tfrac{\Delta^2}{2}  
    \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) 
    a_R (x, \sigma), \quad 
   \Sigma_{R S_1} (\Delta, x, \theta) \equiv  \Sigma_{S_1 R} (\Delta, x, \theta)^\top;  \\[0.3cm] 
   & \Sigma_{R S_2} (\Delta, x, \theta) 
   \equiv  \Sigma_{S_2 R} (\Delta, x, \theta)^\top,  \quad 
   \Sigma_{RR} (\Delta, x, \theta) \equiv \Delta \, 
   a_R (x, \sigma).  
\end{aligned}
\end{align*}
% 
In the above, we have set
% 
\begin{align*}
    a_R (x, \sigma) & = \sum_{k = 1}^d V_{R, k} (x, \sigma) V_{R, k} (x, \sigma)^\top;  \\ 
    a_{S_2} (x,\theta) & = \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) a_R (x, \sigma) \bigl( \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) \bigr)^\top;   \\[0.2cm] 
    a_{S_1} (x, \theta) & = 
    \partial_{x_{S_2}}^\top  V_{S_1, 0} (x_{S}, \beta_{S_1})  a_{S_2} (x, \theta) 
    \bigl( \partial_{x_{S_2}}^\top  V_{S_1, 0} (x_{S}, \beta_{S_1}) \bigr)^\top. 
\end{align*}
% 
% 
\begin{proposition} \label{prop:positive_definite}
Under condition \ref{assump:hypo}-II, the covariance matrix $\Sigma (\Delta, x, \theta)$ is positive-definite for any $(\Delta, x, \theta) \in (0, \infty) \times \mathbb{R}^N \times \Theta$. 
\end{proposition}
%  
% 
\noindent The proof is given in Appendix \ref{appendix:positive_definite}. Due to Proposition \ref{prop:positive_definite}, the covariance $\Sigma (\Delta, x, \theta)$ is invertible, thus the approximate log-likelihood based on the local Gaussian discretisation scheme (\ref{eq:lg_hypo_II}) is well-defined for the highly degenerate class (\ref{eq:hypo-II}). We note that, in brief, the above result follows from the positive definiteness of $a_R (x, \sigma)$, $a_{S_2} (x, \theta)$ and $a_{S_1} (x, \theta)$ under condition \ref{assump:hypo}-II. 
%Furthermore, the inverse matrices are effectively used to represent the asymptotic precision matrix provided later in asymptotic normality for the contrast estimator based upon the well-defined likelihood. 
% Also, under the partial observations regime, the joint approximate transition density induced scheme itself can be effectively utilised to estimate the hidden components. 
%
\section{Parameter Inference for Class (\ref{eq:hypo-II})}
\label{sec:main}
% 
We explore analytically parameter inference procedures for hypo-elliptic diffusions in class (\ref{eq:hypo-II}). We prove in Section \ref{sec:complete} that a contrast estimator constructed from the conditionally Gaussian discretisation scheme (\ref{eq:lg_hypo_II}) is asymptotically unbiased under the high-frequency, complete observation regime. We illustrate the precise impact of the drift terms involved in 
scheme (\ref{eq:lg_hypo_II}) on the asymptotic results. In Section \ref{sec:partial}, we consider the partial observation regime. As observed for the case of class (\ref{eq:hypo-I}) in the literature, we show via analytical case studies that use of finite-differences for the estimation of hidden paths leads to biased parameter estimates within (\ref{eq:hypo-II}). Also, we explain that the local Gaussian scheme can be put into  effective use within  computational approaches for filtering hidden components and estimating parameters. 
%
\subsection{Complete Observation Regime} 
\label{sec:complete}
%
\subsubsection{Contrast Estimator} 
%
Based on the proposed scheme (\ref{eq:lg_hypo_II}) and the corresponding tractable transition density, we construct a contrast estimator for the hypo-elliptic class (\ref{eq:hypo-II}). We write the transition density of the local Gaussian scheme (\ref{eq:lg_hypo_II}), for given $\Delta > 0$, current position $x \in \mathbb{R}^N$ and parameter $\theta \in \Theta$ as:
% 
% 
\begin{align*} 
y & \mapsto \bar{p}_\Delta (x, y ; \theta) \\[0.2cm]
& \quad = \frac{1}{\sqrt{ (2 \pi)^N \Delta^{5 N_{S_1} + 3 N_{S_2} + N_R}
| \Sigma  (x, \theta) | }} 
\exp \Big( -\tfrac{1}{2} m (\Delta, x, y, \theta)^\top
    \Sigma^{-1}  (x, \theta) 
   m (\Delta, x, y, \theta) \Big),  
\end{align*}
% 
where we have set, for $y = (y_{S_1}, y_{S_2}, y_R) \in \mathbb{R}^{N_{S_1}} \times \mathbb{R}^{N_{S_2}} \times \mathbb{R}^{N_{R}}$, 
% 
\begin{align*} 
   m (\Delta, x, y, \theta) 
  =
  \begin{bmatrix}
  \tfrac{1}{\sqrt{\Delta^{5}}} \bigl(y_{S_1} - {\mu}_{S_1} (\Delta, x, \theta) \bigr) \\[0.3cm]  
  \tfrac{1}{{\sqrt{\Delta^{3}}}} 
  \bigl(y_{S_2} - {\mu}_{S_2} (\Delta, x, \theta) \bigr) \\[0.3cm] 
  \tfrac{1}{\sqrt{\Delta}}  \bigl(y_{R} - {\mu}_{R} (\Delta, x, \theta) \bigr) 
  \end{bmatrix}, \qquad  
  \Sigma (x, \theta) = \Sigma (1, x, \theta).  
\end{align*}
%  
% 
% 
Note that from Proposition \ref{prop:positive_definite}, under Condition \ref{assump:hypo}-II the covariance matrix $\Sigma (x, \theta)$ is invertible for any $(x, \theta) \in \mathbb{R}^N \times \Theta$. We denote by $X^{\, (\mrm{II})}_i$ the (complete) observation of a diffusion within class (\ref{eq:hypo-II}) at time~$t_i$, $0\le i\le n$. Then, after removing some constant terms from $-2\,\textstyle{\sum_{i = 1}^n \log \bar{p}_{\Delta_n} ( X_{i-1}^{(\mrm{II})}, X_{i}^{(\mrm{II})}  ; \theta)}$, we define the following contrast function:  
%
\begin{align} 
\begin{aligned} \label{eq:contrast}  
\ell_{n}( \theta ) 
& =  \sum_{i = 1}^n  m (\Delta_n, 
\sample{i-1}^{(\mrm{II})}, \sample{i}^{(\mrm{II})} , \theta)^\top \, 
\Sigma^{-1} (\sample{i-1}^{(\mrm{II})}, \theta) \, 
m (\Delta_n, \sample{i-1}^{(\mrm{II})},  
\sample{i}^{(\mrm{II})}, \theta)
\\%[0.2cm] 
&  \qquad \qquad + \sum_{i = 1}^n \log \bigl| \Sigma 
(\sample{i-1}^{\, (\mrm{II})}, \theta) \bigr|.   
\end{aligned}
\end{align}
%
% 
%
% 
Thus, the contrast estimator for the hypo-elliptic class
(\ref{eq:hypo-II}) is defined as: 
% 
\begin{align} \label{eq:MLE}
  \hat{\theta}_{n} 
  = \bigl( 
  \hat{\beta}_{S_1, n}, \, 
  \hat{\beta}_{S_2, n}, \,  
  \hat{\beta}_{R, n}, \, 
  \hat{\sigma}_{n}  \bigr) 
  = \mathrm{argmin}_{\theta \in \Theta} \; 
  \ell_{n} \bigl( \theta \bigr).   
\end{align}
%   
%
\subsubsection{Asymptotic Results}
Before we state our main results, we introduce some conditions for class (\ref{eq:hypo-II}). 
% 
\begin{enumerate}[resume]
%
% \item \label{assump:param_space}
% $\Theta$ is a compact subset of $\mathbb{R}^{N_\theta}$.  
% 
\item \label{assump:coeff}
% 
For each $\theta \in \Theta$, $V_j (\cdot, \theta) \in C_b^{\infty} (\mathbb{R}^N ; \mathbb{R}^N)$, $0 \le j \le d$.
%
\item  \label{assump:bd_deriv}
For any $\alpha \in \{1, \ldots, N \}^l, \, 0 \le l \le 2$, and $1 \le i \le N, \, 0 \le j \le d$, the following function
% 
\begin{align*} 
% \theta \mapsto \tfrac{\partial^l}{\partial x_{\alpha_1} \cdots \partial x_{\alpha_l}} V_j^i (x , \theta)  
\theta \mapsto \partial^x_\alpha V_j^i (x , \theta)  
\end{align*}
% 
is three times differentiable for all $x \in \mathbb{R}^N$. Furthermore, derivatives of the above map up to the third order are of polynomial growth in $x \in \mathbb{R}^N$ uniformly in $\theta \in \Theta$.  
% $\theta \mapsto V_j^i (x , \theta), \, 1 \le i \le N, \, 0 \le j \le d$ and its partial derivatives w.r.t $x \in \mathbb{R}^N$ up to the second order are three times differentiable.  
% For each $x \in \mathbb{R}^N$, $\theta \mapsto V_j^i (x ,  \theta),  \, 0 \le i \le N, \, 0 \le j \le d$ is twice differentiable. For any $\alpha \in \{1, \ldots,  N_\theta\}^l$, with $l\in\{1,2\}$, the functions  
% %
% \begin{align}
%     x \mapsto  
%     \partial^{\theta}_{\alpha}
%     V_j^i (x ,  \theta), 
%     \qquad  0 \le j \le d, \quad 1 \le i \le N, 
% \end{align}
% %
% have bounded derivatives of every order uniformly in $\theta \in \Theta$. 
%
% \item \label{assump:lipschitz} 
% For each $x \in \mathbb{R}^N$, the function $V_{S, 0} (x, \cdot) : \Theta \to \mathbb{R}^{N_S}$ is Lipschitz continuous.  
%
\item \label{assump:moments}
The diffusion process $\{X_t\}_{t \geq 0}$ defined via  (\ref{eq:hypo-II}) is ergodic under $\theta = \trueparam$, with invariant distribution $\truedist$ on $\mathbb{R}^N$. Furthermore, all moments of $\truedist$ are finite. 
%
\item \label{assump:finite_moment}
It holds that for all $p \ge 1$, 
$\textstyle \sup_{t > 0} \mathbb{E}_{\trueparam} [|X_t|^p] < \infty$. 
%
\item \label{assump:ident}
If it holds that
% 
\begin{align*}
V_{S, 0} (x , \beta_S) = V_{S, 0} (x , \truebeta_S), \ \ 
V_{R, 0} (x , \beta_R) = V_{R, 0} (x , \truebeta_R), \ \  
V_R (x , \sigma) = V_R (x , \truesigma), 
\end{align*}
%. 
for $x$ in set of probability 1 under  $\nu_{\trueparam}$, then 
$\beta_S = \truebeta_S$,  $\beta_R = \truebeta_R$,  $\sigma = \truesigma$.
%
\end{enumerate}
% 
Note that under condition (\ref{assump:coeff}) and \ref{assump:hypo}-II, the law of the solution to the degenerate SDE (\ref{eq:hypo-II}) admits a smooth Lebesgue density as we explained in Section \ref{sec:hormander_review}. 
% To state our results, we introduce some notation. 
We write the true value of the parameter for a model in (\ref{eq:hypo-II}) as 
$ 
\theta^{\dagger} = (
\beta_{S_1}^{\dagger},  
\beta_{S_2}^{\dagger},     
\beta_{R}^{\dagger},  
\sigma^{\dagger} ) \in \Theta
$. 
The latter, $\trueparam$, is assumed to lie in the interior of $\Theta$. 
% 
% Define $\mathscr{D} : \mathbb{R}^N \times \Theta \to \mathbb{R}^{N \times N}$ as: 
% % 
% \begin{align*} 
% \mathscr{D} (x, \theta) = 
% \begin{bmatrix} 
% \Sigma_{S_1 S_1}^{-1} (x, \theta) & \mathbf{0}_{N_{S_1} \times N_{S_2}}  & \mathbf{0}_{N_{S_1} \times N_{R}} \\[0.2cm] 
% \mathbf{0}_{N_{S_2} \times N_{S_1}} & \Sigma_{S_2 S_2}^{-1} (x, \theta) & \mathbf{0}_{N_{S_2} \times N_{R}}  \\[0.2cm]
% \mathbf{0}_{N_{R} \times N_{S_1}} &  \mathbf{0}_{N_{R} \times N_{S_2}}  & 
% a_R^{-1} (x, \sigma) 
% \end{bmatrix}, \quad  (x, \theta) \in \mathbb{R}^N \times \Theta,  
% \end{align*}
% % 
% where we have set the block matrices as: 
% % 
% \begin{gather*}
%  \Sigma^{-1}_{S_1 S_1} (x, \theta) = [ \Sigma^{-1}_{ij} (x, \theta)]_{1 \le i,j \le N_{{S_1}}}, \quad 
%  \Sigma^{-1}_{S_2 S_2} (x, \theta) = [ \Sigma^{-1}_{ij} (x, \theta)]_{N_{S_1} + 1  \le i,j \le N_{\beta_{S}}}; \\[0.2cm]
%  a_R^{-1} (x, \sigma) = \Sigma_{RR}^{-1} (x, \sigma),  
% \end{gather*} 
% % 
% with the matrix $\textstyle \Sigma_{RR} (x, \sigma) = \sum_{k = 1}^d V_{R, k} (x, \sigma) V_{R, k} (x, \sigma)^\top \in \mathbb{R}^{N_R \times N_R}$ 
% We write for $(x, \theta) \in \mathbb{R}^N \times \Theta$, 
% \begin{align}
% \Sigma_{S_1 S_1} (x, \theta) = \tfrac{1}{20} a_{S_1} (x, \theta), 
% \quad 
% \Sigma_{S_2 S_2} (x, \theta) = \tfrac{1}{3} a_{S_2} (x, \theta),
% \quad 
% \Sigma_{R R} (x, \theta) = a_{R} (x, \theta) 
% \end{align}
% % 
% % 
% with the matrices $a_{S_1}$, $a_{S_2}$, $a_{R}$ defined in (\ref{eq:a_s1}), (\ref{eq:a_s2}), (\ref{eq:a_R}) respectively, being non-singular under condition \ref{assump:hypo}-II due to Proposiion \ref{prop:positive_definite}. 
Recall the definition of function $V_0 : \mathbb{R}^N \times \Theta \to \mathbb{R}^N$ in (\ref{eq:V0}). Then, the contrast estimator defined in (\ref{eq:MLE}) has the following asymptotic properties in the high-frequency observation setting.
%
% 
\begin{theorem}[Consistency] \label{thm:consistency}
Assume that conditions \ref{assump:hypo}-II, (\ref{assump:coeff})--(\ref{assump:ident}) hold. If \limit, then
% 
% 
\begin{align*}
\hat{\theta}_{n} \probconv \trueparam. 
\end{align*}
%
\end{theorem}
%
%
%
\begin{theorem}[Asymptotic Normality]  \label{thm:clt} 
Assume that conditions \ref{assump:hypo}-II,  (\ref{assump:coeff})--(\ref{assump:ident}) hold. If \limit \, with $\Delta_n =  o ( n^{-1/2})$, then
%. 
\begin{align*}    
  \begin{bmatrix}
    \sqrt{\tfrac{n}{\Delta_n^3}} \bigl( \hat{\beta}_{S_1, n} - \truebeta_{S_1} \bigr) \\[0.3cm]   
    \sqrt{\tfrac{n}{\Delta_n}} 
    \bigl( \hat{\beta}_{S_2, n} - \truebeta_{S_2} \bigr) \\[0.3cm]
    \sqrt{n \Delta_n} \bigl( \hat{\beta}_{R, n} - \truebeta_{R} \bigr) \\[0.3cm]
    \sqrt{n} \bigl( \hat{\sigma}_{n} - \truesigma \bigr) 
  \end{bmatrix}
  \distconv \mathscr{N} \bigl( 
   \mathbf{0}_{N_\theta}, 
    \Gamma^{-1} ( \trueparam )  
   \bigr),  
\end{align*}
%
where the asymptotic precision matrix $\Gamma (\trueparam)$ is given as:
% 
\begin{align} \label{eq:precision_matrix} 
\Gamma ( \trueparam )  
= \mrm{diag} \Bigl(
\Gamma^{\beta_{S_1}} (\trueparam), \,  
\Gamma^{\beta_{S_2}} (\trueparam), \,
\Gamma^{\beta_{R}} (\trueparam), \,
\Gamma^{\sigma} (\trueparam)
\Bigr), 
\end{align} 
% 
% 
with the involved block matrices 
$\Gamma^{\beta_{S_1}} (\trueparam)  \in \mathbb{R}^{N_{\beta_{S_1}} \times N_{\beta_{S_1}}}$, 
$\Gamma^{\beta_{S_2}} (\trueparam)  \in \mathbb{R}^{N_{\beta_{S_2}} \times N_{\beta_{S_2}}}$,
$\Gamma^{\beta_{R}} (\trueparam)  \in \mathbb{R}^{N_{\beta_{R}} \times N_{\beta_{R}}}$,
$\Gamma^{\sigma} (\trueparam)  \in \mathbb{R}^{N_{\sigma} \times N_{\sigma}}$ 
specified as: 
% 
\begin{align*}
&  \Gamma_{ij}^{\beta_{S_1}} ( \trueparam )
=  720  \int  
\partial_i^{\beta_{S_1}} V_{S_1, 0} (x_S , \truebeta_{S_1})^\top 
\, a^{-1}_{S_1} (x, \trueparam)  \, 
\partial_{{j}}^{\beta_{S_1}} 
V_{S_1, 0} (x_S , \truebeta_{S_1})  \; 
 \nu_{\trueparam} (dx); \\[0.1cm]
 &  \Gamma_{ij}^{\beta_{S_2}} ( \trueparam )
= 12 \int 
\partial^{\beta_{S_2}}_{i} 
 V_{S_2, 0} (x , \truebeta_{S_2} )^\top 
\, a_{S_2}^{-1} (x, \trueparam )  \, 
\partial^{\beta_{S_2}}_{j} V_{S_2, 0} (x , \truebeta_{S_2} ) \; 
 \nu_{\trueparam} (dx); \\[0.1cm] 
&  \Gamma_{ij}^{\beta_R} ( \trueparam )
= \int 
\partial_{{i}}^{\beta_R}
 V_{R, 0} (x , \truebeta_R )^\top 
\, a_R^{-1}  (x ,  \truesigma)  
\, 
\partial_{{j}}^{\beta_R} V_{R, 0} (x , \truebeta_R) \; 
 \nu_{\trueparam} (dx); \\[0.1cm]
& \Gamma_{ij}^\sigma (\trueparam)  
= \tfrac{1}{2} \int \mathrm{tr} 
\bigl( 
\partial_{i}^\sigma 
\Sigma (x ,  \trueparam)
\Sigma^{-1}  (x ,  \trueparam)
\partial_{j}^\sigma
\Sigma (x ,  \trueparam)
\Sigma^{-1} (x ,  \trueparam) \bigr)   
\;  \nu_{\trueparam}(dx). 
\end{align*} 
\end{theorem} 
%
\noindent The proofs of Theorems \ref{thm:consistency} \& \ref{thm:clt} are given in Appendix \ref{sec:pf}. 
%
\begin{remark} \label{rem:pf_main}
%The main results can be seen as an extension of those in 
\cite{glot:20}
%with the latter work 
prove consistency and asymptotic normality of a contrast estimator constructed via a locally Gaussian scheme for  class~(\ref{eq:hypo-I}). 
% However, our own analytical results differ from the ones in \cite{glot:20} not only in terms of the diffusion class we consider, but also   
% in the approach used in the proofs. 
%As we explain in Appendix \ref{sec:pf}, 
Our proofs follow a different approach from the one in the above work. 
Indicatively, a condition on the step-size of $\Delta_n = o (n^{-1/2})$ is not required for our proof of consistency, whereas it is needed in \cite{glot:20}. Our proofs avoid such a condition by making use of preliminary convergence rates for the estimators $\hat{\beta}_{S_1,n}$, $\hat{\beta}_{S_2,n}$   and some key identities in the involved matrix calculations to control terms arising in the expansion of the logarithm of the contrast function -- this method then provides the consistency for $\hat{\beta}_{R,n}$.
We note that we believe that a consequence of the proofs we derive in this work is that they provide a direction for further results to be obtained on the analysis of contrast functions and the estimates they deliver for general classes of hypo-elliptic SDEs, see the discussion in Section \ref{sec:end}.

%Such a scaling for $\Delta_n$, together with the convergence rate of $\hat{\beta}_{S, n}$ allows \cite{glot:20} to control some key terms in the contrast function expansion for (\ref{eq:hypo-I}), such as 
%%
%$$ \tfrac{V_{S, 0} (\sample{i-1}, \,  \hat{\beta}_{S, n}) - V_{S, 0} ( \sample{i-1}, \, {\truebeta}_{\!\!S} )  }{\sqrt{\Delta_n}}.$$
% 
%
% We mention here that 
% the strategy utilised in our proofs of consistency 
%\textcolor{red}{We note that we believe that a consequence of proving consistency without requiring $\Delta_n = o (n^{-1/2})$ is that a path is now provided to develop contrast functions for hypo-elliptic SDEs so that a CLT holds under a condition of $\Delta_n = o (n^{-1/p})$, for an arbitrary integer $p \geq 2$. Such a result has been obtained by \cite{kess:97} and \cite{uchi:12} only for elliptic SDEs, where consistency is, indeed, shown without a condition of $\Delta_n = o (n^{-1/p})$, with such a scaling required only for the proof of CLT.}
%Our proofs in this work are expected to allow for a similar derivation in the case of hypo-elliptic SDEs.
\end{remark}
%  
% 
\subsubsection{Case Study --  Bias due to Incorrect Drift Expansion} \label{sec:case_drift}
We have proven that the contrast estimator based on the proposed scheme (\ref{eq:lg_hypo_II}) is asymptotically unbiased under the high-frequency, complete observation regime. The inclusion of an appropriate number terms from the stochastic Taylor expansion of $V_{S_1, 0} (X_{S_1, t}, X_{S_2, t}, \beta_{S_1})$ and $V_{S_2, 0} (X_{t}, \beta_{S_2})$ in scheme (\ref{eq:lg_hypo_II}) is critical for obtaining desirable asymptotic properties.
Omission of such terms will typically give rise to an asymptotic bias. 
In this subsection, we briefly highlight the effect of the `drift correction' via a simple three-dimensional hypo-elliptic model from (\ref{eq:hypo-II}). 
%We show this analytically for a particular SDE in class (\ref{eq:hypo-II}).
We consider the following SDE: 
%
\begin{align}
\begin{aligned} \label{eq:toy-3}  
 d q_ t & = p_t dt; \\ 
 d p_t & = s_t dt ; \\ 
 d s_t & = - \beta s_t dt + \sigma dB_t, 
\end{aligned} 
\end{align}
%
% 
where $\theta = (\beta, \sigma)$ is the parameter vector. We assume that all components of the system are observed, and consider the following discretisation scheme for SDE (\ref{eq:toy-3}): 
%
\begin{align} \label{eq:toy_app_1}
\begin{aligned}
  \bar{x}_{i+1} = 
\begin{bmatrix}
  \bar{q}_{i+1} \\[0.2cm]
  \bar{p}_{i+1} \\[0.2cm] 
  \bar{s}_{i+1} 
\end{bmatrix}
= 
\begin{bmatrix}
\bar{q}_i + \bar{p}_i \Delta_n + \bar{s}_i \tfrac{\Delta_n^2 }{2} 
\\[0.3cm] 
\bar{p}_i + \bar{s}_i \Delta_n 
- \beta \bar{s}_i  \tfrac{\Delta^2_n}{2} \\[0.3cm] 
\bar{s}_i - \beta \bar{s}_i \Delta_n  
\end{bmatrix}
+ 
\begin{bmatrix}
\sigma \times \int_{t_i}^{t_{i+1}} \int_{t_i}^u \int_{t_i}^v dB_w dv du \\[0.3cm] 
\sigma \times \int_{t_i}^{t_{i+1}} \int_{t_i}^u d B_{v} du \\[0.3cm] 
\sigma \times ( B_{t_{i+1}} - B_{t_i} )
\end{bmatrix}. 
\end{aligned}
\end{align}
% 
Thus, terms of size $\mathcal{O} (\Delta_n^3)$ are not included in the deterministic part of the approximation $\bar{q}_{i+1}$ of the smoothest component.
%compared to the proposed discretisation scheme (\ref{eq:lg_hypo_II}). 
Based on the conditionally Gaussian scheme (\ref{eq:toy_app_1}), we define a contrast function as $(- 2) \times$ (complete log-likelihood), that is, after some constants are removed: 
%
\begin{align*}
\ell_{n} (\theta ; x_{0:n})  
= 6n \log \sigma +  \tfrac{1}{\sigma^2} \sum_{i = 1}^n m(\Delta_n, x_{i-1}, x_i, \theta)^\top 
\, 
\Sigma^{-1} \, 
m (\Delta_n, x_{i-1}, x_i, \theta), 
\end{align*}
%  
% 
where we have set $x_i = [q_i, p_i, s_i ]^\top$,  $0 \le i \le n$, and 
% 
\begin{align*}
m (\Delta_n, x_{i-1}, x_i, \theta) 
=  
\begin{bmatrix}
\tfrac{1}{\sqrt{\Delta_n^5}} \bigl(q_{i} -  q_{i-1} - p_{i-1} \Delta_n -  s_{i-1} \tfrac{\Delta_n^2 }{2} \bigr) \\[0.3cm] 
\tfrac{1}{\sqrt{\Delta_n^3}} \bigl(p_{i} - p_{i-1} - s_{i-1} \Delta_n + \beta s_{i-1} \tfrac{\Delta^2_n}{2} \bigr) \\[0.3cm]
\tfrac{1}{\sqrt{\Delta_n}} \bigl( s_{i} - s_{i-1} + \beta s_{i-1} \Delta_n  \bigr) 
\end{bmatrix}
, \quad  
{\Sigma} 
=         
\begin{bmatrix}
 \tfrac{1}{20} & \tfrac{1}{8} & \tfrac{1}{6} \\[0.2cm]
 \tfrac{1}{8} & \tfrac{1}{3} & \tfrac{1}{2} \\[0.3cm]  
 \tfrac{1}{6} & \tfrac{1}{2} & 1
\end{bmatrix}.   
\end{align*}
% 
% 
% \begin{align} 
% \mu_i (\beta) \equiv 
% \begin{bmatrix}
% q_i + p_i \Delta_n + \tfrac{\Delta_n^2 }{2} s_i \\[0.3cm] 
% p_i + s_i \Delta_n 
%     - \frac{\Delta^2_n}{2} \beta s_i \\[0.3cm]
% s_i - \beta s_i \Delta_n 
% \end{bmatrix}
% \end{align}
%
Solving $\partial_\beta \, \ell_n (\theta; x_{0:n}) = 0$, we obtain the contrast estimator for $\beta$ as $\widetilde{\beta}_n = \tfrac{\widetilde{g}_n}{\widetilde{f}_n}$, where we have defined:   
%
% 
\begin{align*}  %\label{eq:fn_1}
 \widetilde{f}_n  & = 
 \Bigl\{ \tfrac{1}{2} \Sigma^{-1}_{32}  + \Sigma^{-1}_{33}  
+ \tfrac{1}{4} \Sigma^{-1}_{22}  + \tfrac{1}{2} \Sigma^{-1}_{23} \Bigr\} \times \tfrac{1}{n}  \sum_{i = 1}^n s_{i-1}^2; \\ 
 \widetilde{g}_n & = - \tfrac{1}{n \sqrt{\Delta_n}} \sum_{i = 1}^n s_{i-1} \times \Bigl\{ 
 \Sigma^{-1}_{31} \times \tfrac{q_{i} - q_{i-1} - p_{i-1} \Delta_n - s_{i-1} \tfrac{\Delta_n^2}{2} }{\sqrt{\Delta_n^5}} 
 \; + \;  \Sigma^{-1}_{32} \times \tfrac{p_{i} - p_{i-1} - s_{i-1} \Delta_n}{\sqrt{\Delta_n^3}} \nonumber \\ 
 & \quad  + \Sigma^{-1}_{33} \times \tfrac{s_{i} - s_{i-1}}{\sqrt{\Delta_n}}   + \tfrac{1}{2} \Sigma^{-1}_{21} 
 \times \tfrac{q_{i} - q_{i-1} - p_{i-1} \Delta_n - s_{i-1} \tfrac{\Delta_n^2}{2} }{\sqrt{\Delta_n^5}} 
 + \tfrac{1}{2} \Sigma^{-1}_{22}  
 \times \tfrac{p_{i} - p_{i-1} - s_{i-1} \Delta_n}{\sqrt{\Delta_n^3}} \nonumber  \\[0.2cm]
 & \quad + \tfrac{1}{2} \Sigma^{-1}_{23} 
 \times \tfrac{s_{i} - s_{i-1}}{\sqrt{\Delta_n}} 
 \Bigr\}.  %\label{eq:gn_1} 
\end{align*}
%
From the ergodicity of the process $\{s_t\}$ and Lemma \ref{lemma:ergodic_thm} in the Appendix, we have that as \limit, 
% 
\begin{align*} 
\widetilde{f}_n \probconv c_1 \times 
\int s^2 \, \truedist (ds), 
% \mathbb{E}_{\trueparam} 
% \bigl[ s^2 \bigr], 
\end{align*}
% 
for a non-zero constant $c_1 = \tfrac{1}{2} {\Sigma}_{32}^{-1} 
+ {\Sigma}_{33}^{-1}   
+ \tfrac{1}{4} {\Sigma}_{22}^{-1}  
+ \tfrac{1}{2} {\Sigma}_{23}^{-1}$, where $\nu_\trueparam(ds)$ is the invariant distribution of $\{ s_t \}$ under the true parameter $\trueparam$. 
% 
For the numerator $\widetilde{g_n}$, we apply Lemmas \ref{lemma:ergodic_thm} \& \ref{lemma:canonical_conv} in Appendix to obtain that 
% 
\begin{align*} 
\widetilde{g}_n \probconv (c_1 + c_2) \times \truebeta \times 
\int s^2 \, \truedist (ds), 
% \mathbb{E}_{\trueparam} \bigl[ s^2 \bigr],
\end{align*}
% 
for a non-zero constant $c_2 \equiv \tfrac{1}{6} {\Sigma}_{31}^{-1} 
 + \tfrac{1}{12} {\Sigma}_{21}^{-1}$. Hence, it holds that, if \limit, then
% 
\begin{align*}  
\widetilde{\beta}_n 
\probconv \bigl( 1 + \tfrac{c_2}{c_1} \bigr) \times \truebeta. 
\end{align*}
% 
%
Thus, the drift estimation based on the discretisation scheme (\ref{eq:toy_app_1}) with inappropriate drift expansion is, in general, asymptotically biased. One can check that the above bias is removed upon use of our locally Gaussian scheme (\ref{eq:lg_hypo_II}) instead of (\ref{eq:toy_app_1}). 
% 
% 
\subsection{Partial Observation Regime} 
\label{sec:partial}
% 
% 
\subsubsection{Case Study -- Bias due to Finite-Differences} \label{sec:case_2} 
To motivate the `appropriateness' of the proposed locally Gaussian scheme (\ref{eq:lg_hypo_II}) in the context of parameter inference in a partial observation setting, we illustrate that  naive use of finite-differences to impute hidden components (a practice quite common in applications) induces a bias in the estimation of the SDE parameters. To observe this, consider the model (\ref{eq:toy-3}) again but with the drift parameter $\beta$ fixed to 1: 
% 
\begin{align}
\begin{aligned} \label{eq:toy-2}
   d q_t & = p_t dt;  \\ 
   d p_t & =  s_t dt; \\ 
   d s_t & = - s_t dt + \sigma d B_t,  
\end{aligned}
\end{align}
%
for $\sigma > 0$.  We then apply the Euler-Maruyama scheme for the first equation of (\ref{eq:toy-2}) and the locally Gaussian scheme (\ref{eq:lg_hypo_I}) for the remaining dynamics, i.e., 
% 
\begin{align}
\begin{aligned} \label{eq:toy_app_2}
\begin{bmatrix}
 \bar{q}_{i+1} \\[0.1cm]
 \bar{p}_{i+1} \\[0.1cm]
 \bar{s}_{i+1}
\end{bmatrix}
=
 \begin{bmatrix}
 \bar{q}_{i} +  \bar{p}_{i} \Delta_n \\[0.1cm]
 \bar{p}_{i} +  \bar{s}_{i} \Delta_n 
- \bar{s}_{i} \tfrac{\Delta_n^2}{2} \\[0.1cm]
 \bar{s}_{i} - \bar{s}_i \Delta_n  
 \end{bmatrix}
 + 
 \begin{bmatrix} 
 0 \\[0.3cm] 
 \sigma \times \int_{t_i}^{t_{i+1}} \int_{t_i}^{u} 
 d B_{v} du \\[0.3cm]
 \sigma \times \bigl( B_{i+1} - B_{i} \bigr)
 \end{bmatrix}. 
\end{aligned}
\end{align} 
% 
Scheme (\ref{eq:toy_app_2}) is  degenerate since the upper-most equation does not involve noise.  
% 
We now consider the estimator based on the likelihood provided by (\ref{eq:toy_app_2}), given the discrete-time observations $\{q_{0:n}, s_{0:n} \}$, and with the hidden paths $p_{0:n}$ imputed via the first equation of (\ref{eq:toy_app_2}) using the observations $q_{0:n}$. %We  that the diffusion estimation is asymptotically biased. 
% 
\begin{remark}
In practice, the rough component $s_t$ is often not observed, so one must impute the missing components $s_{0:n}$ conditionally on the observations $q_{0:n}$ by making use of the transition density (or some approximation of it) for both co-ordinates $(p,s)$ of (\ref{eq:toy-2}). 
One would reasonably expect that presence of bias will be typical in such a practical scenario, if it found to be present in the simpler case when $s_t$ is directly observed.
%As the later discussion suggests, even imputation of true signals of $\{ s_{i} \}$ leads to asymptotically biased estimation for the diffusion parameter, which implies that one would still suffer from biased estimation with appropriately estimated $\{s_i\}$ as long as the finite difference approximation for $\{ p_i \}_{0 \le i \le n}$ is used. 
\end{remark} 
%  
% We construct the approximate likelihood based on the (conditional) Gaussian density of 
% $(p_{i+1}, s_{i+1})$ in (\ref{eq:toy_app_2}). 
\noindent The complete likelihood of the discretisation scheme (\ref{eq:toy_app_2}) is given as: 
%
\begin{align*} %\label{eq:complete_toy_2}
\begin{aligned}
    & \prod_{i =1}^n \Bigl\{ \tfrac{1}{\sqrt{(2 \pi)^2 \Delta_n^4 \sigma^4 | \Sigma | }} 
    \exp \Bigl(  
    - \frac{1}{2 \sigma^2} \, 
    m (\Delta_n, y_{i-1}, y_{i})^\top \, 
    \Sigma^{-1}  \,
    m (\Delta_n, y_{i-1}, y_{i}) 
    \Bigr) \\ 
    & \qquad  \times  \delta \bigl( q_{i} - q_{i-1} - p_{i-1} \Delta_n \bigr) \Bigr\}, 
\end{aligned} 
\end{align*}
% 
% 
where we have defined $y_i = [p_i, s_i]^\top$, $0 \le i \le n$, and  
% 
% 
\begin{align*}
m (\Delta_n, y_{i-1}, y_i) = 
\begin{bmatrix} 
  \tfrac{1}{\sqrt{\Delta_n^3}} 
  \bigl( p_i - p_{i-1} - s_{i-1} \Delta_n + s_{i-1} \tfrac{\Delta_n^2}{2} \bigr) \\[0.2cm] 
  \tfrac{1}{\sqrt{\Delta_n}} \bigl( s_i - s_{i-1} + s_{i-1} \Delta_n \bigr)
\end{bmatrix}, 
\quad 
\Sigma = 
\begin{bmatrix}
 \tfrac{1}{3}  &  \tfrac{1}{2} \\[0.2cm]  
 \tfrac{1}{2} &  1
\end{bmatrix}. 
\end{align*}  
% 
Integrating out $p_{0:n}$,  we obtain the marginal likelihood $f_n (\sigma \, ; q_{0:n+1}, s_{0:n})$ as: 
% 
\begin{align*}
& f_n (\sigma \, ; q_{0: n + 1}, s_{0:n}) \\ 
&  =  \prod_{i =1}^n \Bigl\{ \tfrac{1}{\sqrt{(2 \pi)^2 \Delta_n^4 \sigma^4 | \Sigma | }} 
\exp \Bigl(  
- \frac{1}{2 \sigma^2} \, 
m (\Delta_n, \hat{y}_{i-1}, \hat{y}_{i} )^\top \, 
\Sigma^{-1}  \,
m ( \Delta_n, \hat{y}_{i-1}, \hat{y}_{i} ) 
\Bigr) \Bigr\}, 
\end{align*}
% 
where $\hat{y}_i = [\hat{p_i}, s_i]^\top$, with $\hat{p}_i = \bigl( q_{i+1} - q_i \bigr) / \Delta_n$. 
%Note that now the latent path $p_{0:n}$ is estimated from the finite difference approximation with the observations $q_{0:n+1}$. 
% 
Then, we obtain the following contrast function for $\sigma$, after removing constant terms from $(-2) \times \log f_n (\sigma ;  q_{0:n+1}, s_{0:n})$: 
% 
\begin{align} \label{eq:toy2_cont}
\ell_{n} (\theta ; q_{0: n+1}, s_{0:n}) 
= 4 n \log \sigma 
+ \tfrac{1}{\sigma^2} \sum_{i = 1}^n 
m (\Delta_n, \hat{y}_{i-1}, \hat{y}_i )^\top  \, 
{\Sigma}^{-1} \, 
m (\Delta_n, \hat{y}_{i-1}, \hat{y}_i).  
\end{align}
% 
% 
Solving $\partial_\sigma  \ell_n (\sigma ; q_{0:n+1}, s_{0:n}) = 0 $, we obtain the estimator $\hat{\sigma}_n$, such that:
%
\begin{align*} 
(\hat{\sigma}_n)^2 = \tfrac{1}{2n} \sum_{i=1}^n 
m (\Delta_n, \hat{y}_{i-1}, \hat{y}_i)^\top 
\, {\Sigma}^{-1} \, 
m (\Delta_n, \hat{y}_{i-1}, \hat{y}_i). 
\end{align*}
%
% %  
It holds that, if \limit, then 
% 
%
\begin{align} \label{eq:conv_toy_2}
 ( \hat{\sigma}_n )^2 \probconv \tfrac{8}{5} (\truesigma)^2. 
\end{align}
% 
We prove convergence (\ref{eq:conv_toy_2}) in Appendix \ref{appendix:pf_case_study}. The proof indicates that the bias arises from the  higher order stochastic Taylor expansion terms of $q_{i+1} $ which are ignored by the estimate $\hat{p}_i$. Thus, use of the finite-difference approximation 
for imputation of component $p_t$, 
induces an asymptotic bias at the estimation of $\sigma$. 
%under the partial observations regime. 
% 
\subsubsection{Filtering and Parameter Inference via the Proposed Scheme (\ref{eq:lg_hypo_II})}
\label{sec:kalman_filter}
% 
We put forward the locally Gaussian scheme (\ref{eq:lg_hypo_II}) for imputing hidden components and performing parameter inference under a partial observation regime. The scheme and its transition density on the full set of coordinates can be combined with various computational methods, e.g., Monte-Carlo Expectation Maximisation (MCEM) and Markov Chain Monte Carlo (MCMC), similarly to earlier works \citep{dit:19, poke:09} that applied some conditionally Gaussian schemes for inference of specific hypo-elliptic models within the class (\ref{eq:hypo-I}). 
% 

We now highlight the use of a relatively straightforward Kalman filter recursion for carrying out statistical inference once the locally Gaussian scheme is adopted,
for a rich sub-class of hypo-elliptic models, referred to here as  \emph{conditionally Gaussian non-linear systems}.
%a Kalman filter formula and marginal likelihood that can be constructed for a rich sub-class of hypo-elliptic models, namely, \emph{conditional Gaussian non-linear systems}. 
That is, the system is originally specified as a non-linear SDE but can be treated as a linear system given components that correspond to observations. For elliptic diffusions with such a structure, continuous-time filtering and smoothing have been investigated in engineering, see e.g.~Chapter 8 of \cite{ch:23}. Several important hypo-elliptic models used in applications fall within this sub-class, e.g., standard Langevin equations, Quasi-Markovian generalised Langevin Equations (\ref{eq:qgle-I}, \ref{eq:qgle-II}).  Here, our interest lies in the sub-class derived via the general model (\ref{eq:hypo-II}) once the constituent coefficients are specified as:  
% 
\begin{gather*}
V_{S_1, 0} (x_S, \beta_{S_1}) 
= %f_{S_1} (x_{S_1}, \beta_{S_1}) 
C_{\beta_{S_1}} x_{S_1}
+ \hat{C}_{\beta_{S_1}} x_{S_2}, \quad   
V_{S_2, 0} (x, \beta_{S_2}) 
= f_{S_2} (x_{S_1}, \beta_{S_1}) 
+ C_{\beta_{S_2}} x_H;  
\\[0.2cm]
V_{R, 0} (x, \beta_R) 
= f_{R} (x_{S_1}, \beta_{R}) 
+ C_{\beta_R} x_H, \quad 
V_{R, j} (x, \sigma) = f_{R, j} (\sigma), \quad 1 \le j \le d, 
\end{gather*} 
% 
for $x = (x_{S_1}, x_{S_2}, x_R) = (x_{S_1}, x_{H}) \in \mathbb{R}^{N_{S_1}} \times \mathbb{R}^{N_{S_2} + N_R}$ and $\theta = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma) \in \Theta$, where: (i) $f_{S_2}$, $f_R$, $f_{R,j}$ are vector-valued functions, allowed to be non-linear w.r.t. the state $x_{S_1}$; (ii) matrices 
%
\begin{gather*}
{C}_{\beta_{S_1}} \in \mathbb{R}^{N_{S_1} \times N_{S_1} },\qquad  
\hat{C}_{\beta_{S_1}} \in \mathbb{R}^{N_{S_1} \times N_{S_2} },\\ C_{\beta_{S_2}} \in \mathbb{R}^{N_{S_2} \times (N_{S_2} + N_R)},\qquad  C_{\beta_{R}} \in \mathbb{R}^{N_{R} \times (N_{S_2} + N_R)}
\end{gather*}
are independent of the state $x$. Critically, given the observable component $x_{S_1}$, the drift functions are linear functions of the hidden component $x_H$. For the model with the above choice of coefficients, the locally Gaussian scheme (\ref{eq:lg_hypo_II}) writes as:  
% 
\begin{align} \label{eq:scheme_linear}
\bar{X}_{i+1} 
=
\begin{bmatrix}
  \bar{X}_{S_1, i+1}  \\[0.2cm] 
  \bar{X}_{S_2, i+1}  \\[0.2cm] 
  \bar{X}_{R, i+1}  
\end{bmatrix}
=  b (\Delta_n, \bar{X}_{S_1, i}, \theta) 
+ A (\Delta_n, \bar{X}_{S_1, i}, \theta) 
\begin{bmatrix}
    \bar{X}_{S_2, i} \\[0.1cm]
    \bar{X}_{R, i}
\end{bmatrix}
+ w_i (\Delta_n, \theta), 
\end{align}
% 
for functions 
$b : (0, \infty) \times \mathbb{R}^{N_{S_1}} \times \Theta \to \mathbb{R}^{N} $, 
$A : (0, \infty) \times \mathbb{R}^{N_{S_1}} \times \Theta \to \mathbb{R}^{N \times (N_{S_2} + N_R)}$ and an $N$-dimensional Gaussian variate $w_i (\Delta_n, \theta)$. Since the 
right-hand-side of scheme (\ref{eq:scheme_linear}) is linear w.r.t.~the hidden components $\bar{X}_{S_2, i}, \, \bar{X}_{R, i}$ given the observed component $\bar{X}_{S_1, i}$, one can obtain Kalman filtering and smoothing recursions, and calculate the marginal likelihood for the observations $\bar{X}_{S_1, 0:n}$. We provide the closed form filtering and marginal likelihood calculations in Appendix \ref{appendix:kalman}. We use these tools  in the numerical experiments of parameter inference under the partial observation regime in Section \ref{sec:numerical_studies} that follows.  
% 
\begin{remark} 
\cite{vr:22} studied parameter inference for the QGLE of first-type in (\ref{eq:qgle-I}), %belonging in class (\ref{eq:hypo-I}), 
where they applied an Euler-Maruyama scheme to construct Kalman filtering and smoothing for the rough components $(p_t, s_t)$  given the velocity $p_t$, with values of  the latter obtained (via finite-differences) from discrete observations of the position $q_t$. Then, they used  Kalman filtering and smoothing within an Expectation-Maximisation (EM) algorithm to estimate the parameters. However, as we have seen, such a  finite-differences approach induces bias in the estimation of the  diffusion parameters. 
%For the case of QGLE of second-type in (\ref{eq:qgle-II}), an Euler-Maruyama scheme with finite-differences will provide values for the velocity $p_t$, which now has noise-less dynamics. \textcolor{red}{Ignoring the biases involved in such an approach, even the development of a parameter inference method is now extremely challenging as the $p_t$-observations will correspond to some non-linear (for general $U(\cdot)$) deterministic restrictions on $\{s_t\}$. %(e.g., a Kalman recursion is not applicable).
%cannot lead to a Kalman recursion formula, as the velocity $p_t$ is now a smooth component as Gaussian noise does not appear in its one-step dynamics. 
%In contrast, scheme (\ref{eq:scheme_linear}), which is the version of the locally Gaussian scheme (\ref{eq:lg_hypo_II}) within the sub-class of conditionally Gaussian non-linear systems, will sidestep the shortcomings of the above alternative attempts.}  
\end{remark}
%
\section{Numerical Studies} 
\label{sec:numerical_studies}
%
% 
% 
% 
\subsection{Linear SDE in a Partial Observation Regime} \label{sec:num_case_study} 
We illustrate empirically, for an example SDE model, that parameter estimation via the proposed locally Gaussian scheme (\ref{eq:lg_hypo_II}) leads to asymptotically unbiased estimation under the partial observation regime. We also highlight the effect of the drift correction in the properties of the estimators. We again consider the model studied in Section \ref{sec:case_drift}, that is,  
% 
\begin{align*} 
\begin{aligned}
 d q_t & = p_t dt; \\
 d p_t & = s_t dt; \\ 
 d s_t & = - \beta s_t dt + \sigma dB_t,
\end{aligned}
\end{align*}
% 
where $\theta = (\beta, \sigma) \in \Theta = (0, \infty) \times (0, \infty)$ is the parameter vector. In agreement with practice,  we assume that only discrete observations of the smoothest component, $q_{0:n}$, are available, with an equidistant step-size $\Delta_n$. We compute the following two estimators based on two different discretisation schemes:  
% 
\begin{align*} 
\hat{\theta}_{n, j} 
= (\hat{\beta}_{n, j}, \hat{\sigma}_{n, j})
= {\rm argmax}_{\theta \in \Theta} \log p_{j} (\theta ; q_{0:n}), \quad j = 1,2, 
\end{align*}
% 
where $p_{1} (\theta ; q_{0:n})$ is the approximate likelihood of the observations as obtained by use of Kalman filter in the setting of our locally Gaussian scheme (\ref{eq:lg_hypo_II}), and $p_{2} (\theta ; q_{0:n})$ is a different approximate likelihood obtained in the setting of the following conditionally Gaussian scheme that omits correction terms of order $\mathcal{O}(\Delta_n^2)$ (resp.~$\mathcal{O}(\Delta_n^3)$) from the stochastic Taylor expansion of the drift function of component $p$ (resp.~$q$): 
% 
% 
\begin{align*} 
\begin{aligned}
\begin{bmatrix}
\widetilde{q}_{i+1} \\[0.1cm]
\widetilde{p}_{i+1} \\[0.1cm]
\widetilde{s}_{i+1}
\end{bmatrix}
= 
\begin{bmatrix}
\widetilde{q}_{i} + \widetilde{p}_i \Delta_n \\[0.3cm]
\widetilde{p}_{i} + \widetilde{s}_{i} \Delta_n  \\[0.3cm]
\widetilde{s}_{i} - \beta \widetilde{s}_{i} \Delta_n 
\end{bmatrix}
+ 
\begin{bmatrix}
    \sigma \times \int_{t_i}^{t_{i+1}} \int_{t_i}^u \int_{t_i}^v dB_{w} dv du \\[0.3cm] 
    \sigma \times \int_{t_i}^{t_{i+1}} \int_{t_i}^v dB_{v} du
    \\[0.3cm] 
    \sigma \times \bigl( B_{i+1} - B_i \bigr)
\end{bmatrix}. 
\end{aligned} 
\end{align*}
% 
%
We generate $100$ independent realisations of the dataset $q_{0:n}$ by sub-sampling trajectories obtained from scheme (\ref{eq:lg_hypo_II}) with a small step-size $10^{-4}$. We have chosen the scheme because it is expected to have a better accuracy than other classical schemes (such as Euler-Maruyama scheme) due to the higher order stochastic Taylor expansion of drift functions. We consider the following three high-frequency scenarios for the data: 
% 
\begin{itemize}
\item[] \textbf{Set I}. $n = 5 \cdot 10^5, \, \Delta_n = 10^{-3}, \, T (= n\Delta_n) =500$. 
\item[] \textbf{Set II}. $n = 2 \cdot 10^6, \, \Delta_n = 5 \cdot10^{-4}, \, T =10^3$. 
\item[] \textbf{Set III}. $n = 10^7, \, \Delta_n = 10^{-3}, \, T = 10^4$.  
\end{itemize}
% 
% Figure environment removed 
% 
The true parameter value is set to $\trueparam = (\truebeta, \truesigma) = (2.0, 4.0)$, and the Nelder-Mead method is applied to optimise the marginal likelihoods.   
In Figure \ref{fig:mle}, we plot the 100 realisations of the two different estimators. Table \ref{table:mle} summarizes the mean and standard error of $\hat{\theta}_{n} - \trueparam$, i.e., (estimate) - (true value), from the 100 repetitions. First, we observe that the estimates of $\hat{\theta}_{n ,1}$ (using our scheme (\ref{eq:lg_hypo_II})) is centred at the true value in all scenarios, thus in this case we have an empirical illustration of an asymptotically unbiased estimation in the partial observation setting. Secondly, it is clear from the figures and the table that the mean of estimates of $\hat{\theta}_{n,2}$ (estimator based on the conditionally Gaussian scheme without appropriate drift correction) is shifted from the true value, and seems to be centred at $(\beta, \sigma) = (2.10, 3.960)$. Thus $\hat{\theta}_{n,2}$ induces an asymptotic bias in the partial observation regime, in agreement with the case study in Section \ref{sec:case_drift} in the complete observation case. Notably, there is a clear separation between the two estimators of  $\sigma$. We stress here that the bias in $\hat{\theta}_{n,2}$ is not removed with increasing $n$ or decreasing $\Delta_n$.  Also, one can still observe the bias even if the datasets are obtained with other numerical schemes, e.g., Euler-Maruyama scheme, rather than scheme (\ref{eq:lg_hypo_II}).    
%Finally, we observe that the variance of estimates for parameter $\sigma$ decreases as the number of data $n$ grows. Similarly for the drift parameter $\beta$, the variance of estimates reduces with the rate of $1/\sqrt{T}$. These behaviours imply that the convergence rates of CLT established in Theorem \ref{thm:clt} under the complete observation regime still holds under the partial observation regime.  
%
% 
\begin{table}%[H]
\centering
\caption{Mean and standard error (in parenthesis) of (maximum likelihood estimate) $-$ (true value) from $100$ trajectories of partial observations.}
\begin{tabular}{@{}lcccc@{}}
\toprule
\\[-0.3cm]
\multirow{2}{*}{Set \hspace{0.2cm}}
& \multicolumn{2}{c}{$\hat{\beta}_{n} - \truebeta$ \hspace{0.2cm}}
& \multicolumn{2}{c}{$\hat{\sigma}_n - \truesigma$} \\ \cmidrule(l){2-3} \cmidrule(l){4-5}  
& Proposed scheme 
& Incorrect scheme 
& Proposed scheme 
& Incorrect scheme 
\\[0.1cm] 
\midrule 
I.      & 0.0037 (0.0901)   &  0.1016 (0.0939)  & -0.0024 (0.0039)  &
-0.0415 (0.0040) \\
II.    &   0.0125 (0.0699)  &  0.1109 (0.0752)  &  -0.0011 (0.0019)  & -0.0381 (0.0020)  \\
III.   &  0.0028 (0.0296) &  0.0984 (0.0252) 
&  -0.0026 (0.0015)     &   -0.0418 (0.0013)  \\
\bottomrule
\end{tabular}
\label{table:mle}
\end{table}
% 
\subsection{Quasi-Markovian Generalised Langevin Equations} 
\subsubsection{Scalar Extended State} 
\label{sec:num_qgle}
We consider the QGLE describing one-dimensional positional domain: 
%
\begin{align} \label{eq:qgle_num_1} 
\begin{aligned} 
d q_t  & = p_t dt; \\[0.1cm]
d p_t  & = \bigl( - \nabla U (q_t) + \lambda s_t  \bigr) dt;   \\[0.1cm]
d s_t  & =  (- \lambda p_t - \alpha  s_t) dt   + \sigma  d B_t,  \quad (q_0,p_0,s_0) \in \mathbb{R}^3, 
\end{aligned} 
\end{align}
% 
where $\alpha > 0, \, \sigma > 0, \, \lambda \in \mathbb{R}\setminus \{0\}$ and $U : \mathbb{R} \to \mathbb{R}$. In this experiment, we consider the following two  choices of potential $U$: 
% 
\begin{align*} 
q \mapsto U_{\mrm{HO}} (q) = D \times \tfrac{q^2}{2}, 
\qquad 
q \mapsto U_{\mrm{DW}} (q) = D \times \tfrac{q^2}{2} 
+ \sin \Bigl( \tfrac{1}{4} + 2q \Bigr),
\end{align*}
% 
where $D > 0$ is a parameter. The function $U_{\mrm{DW}}$ (used in experiments in the work \cite{lei:22}) represents an uneven double well potential, under which model (\ref{eq:qgle_num_1}) is non-linear. We generate 30 independent datasets by sub-sampling trajectories produced by the discretisation scheme (\ref{eq:lg_hypo_II}) with a small step-size $10^{-4}$ so that obtained observations correspond to $n = 2 \times 10^5, \, \Delta_n = 10^{-3}, \, T = n\Delta_n= 200$. For the complete observation regime, we compute the contrast estimator (\ref{eq:MLE}) for each given dataset. For experiments of partial observations, we use the trajectories of the position $q_t$ only, extracted from the complete observations, and compute the MLE by maximising the marginal likelihood obtained from the Kalman recursion formula under the locally Gaussian scheme (\ref{eq:lg_hypo_II}), as shown in Section \ref{sec:kalman_filter}. To minimise 
the relevant target functions 
%(\ref{eq:contrast}) and the marginal likelihood under the complete and partial observation regime, 
%respectively, 
we use the adaptive moments (Adam) optimiser with the following algorithmic specifications: (step-size) = $0.1$,  (exponential decay rate for the first moment estimates) = $0.9$, 
(exponential decay rate for the first moment estimates) = $0.9$,  (exponential decay rate for the second moment estimates) = $0.999$, (additive term for numerical stability) = $10^{-8}$. The true parameters are set to: $(D^{\dagger}, \lambda^{\dagger}, \alpha^{\dagger}, \sigma^{\dagger}) = (1.0, 2.0, 4.0, 1.0)$ and $(D^{\dagger}, \lambda^{\dagger}, \alpha^{\dagger}, \sigma^{\dagger}) = (1.0, 2.0, 4.0, 4.0)$ for the QGLE with harmonic potential $U_{\mrm{HO}}$ and the double well potential $U_{\mrm{DW}}$, respectively. Also, the initial guesses for the parameter are set to: $(D_0, \lambda_0, \alpha_0, \sigma_0) = (2.0, 2.0, 2.0, 2.0)$ and $(D_0, \lambda_0, \alpha_0, \sigma_0) = (3.0, 3.0, 3.0, 3.0)$ for the case of $U_{\mrm{HO}}$ and $U_{\mrm{DW}}$, respectively. We summarise the mean and standard error of the estimates from the 30 independent trajectories in Table~\ref{table:qgle_second}. We notice that the results for the complete observation regime are in agreement with the analytical results in Theorem \ref{thm:clt}. For instance, convergence to the true values appears to be faster for parameters $(D, \lambda)$ in the smooth component $p_t$ (recall the convergence rate $\sqrt{\Delta_n / n}$ for such parameters in the CLT of Theorem \ref{thm:clt}). Besides, under the partial observation regime, the estimates seem to be centred around the true parameter as well, with standard errors that are larger than the ones in the case of complete observations (as expected). 
Thus, parameter inference carried out via the proposed locally Gaussian  scheme (\ref{eq:lg_hypo_II}) appears in this case to provide unbiased estimates in the partial observation regime.         
% 
\begin{table} %[H] 
\caption{Parameter estimation of the QGLE (\ref{eq:qgle_num_1}). 
Mean and standard error (in brackets) of maximum likelihood estimates from $30$ trajectories of observations.}
\centering
\begin{tabular}{@{}ccccc@{}}
\toprule 
\\[-0.2cm]
\multicolumn{1}{c}{\multirow{2}{*}{Potential}} 
& \multicolumn{1}{c}{\multirow{2}{*}{Parameter}} 
& \multicolumn{1}{c}{\multirow{2}{*}{True value}}
& \multicolumn{2}{c}{Mean (standard error) of estimates}   
\\[0.2cm] \cmidrule(l){4-5} 
\multicolumn{1}{c}{}   
& \multicolumn{1}{c}{}  
& \multicolumn{1}{c}{}    
& \multicolumn{1}{c}{Complete observation} 
& Partial observation 
\\[0.2cm] 
\midrule 
\multirow{4}{*}{$U_{\mrm{HO}}$} 
& $D$  & 1.0 & 1.0010 (0.0054)  &   1.0128  (0.1069)   \\
& $\lambda$ & 2.0  & 2.0011 (0.0052)  &   1.9732 (0.1131)  \\
& $\alpha$  &  4.0   & 4.0263 (0.2019) &   4.0412 (0.2070)  \\
& $\sigma$  &  1.0   &  1.0017 (0.0096) &  1.0157 (0.0572)   \\[0.4cm] 
\multirow{4}{*}{$U_{\mrm{DW}}$} & $D$  & 1.0  &  1.0000 (0.0001)  &  1.0130 (0.1166) \\
& $\lambda$  & 2.0  & 2.0000 (0.0001)  &  2.0094 (0.1240)  \\
& $\alpha$  &  4.0   & 4.0146 (0.1937) &  4.0310 (0.1969)   \\
& $\sigma$  &  4.0   & 3.9982 (0.0030) &  3.9880 (0.2417)    \\
\bottomrule
\end{tabular} \label{table:qgle_second} 
\end{table}
% 
% 
% 
% \begin{table}
% \centering
% \caption{Parameter estimation of QGLE (\ref{eq:qgle_num_1}) with the double well potential $U_{\mrm{DW}}$. 
% Mean and standard deviation (in brackets) of maximum likelihood estimates from $30$ trajectories of observations.}
% \begin{tabular}{@{}cccc@{}}
% \toprule
% \multicolumn{1}{c}{\multirow{2}{*}{Parameter}} & \multicolumn{1}{c}{\multirow{2}{*}{True value}} & \multicolumn{2}{c}{Mean (standard deviation) of estimates}     \\ \cmidrule(l){3-4} 
% \multicolumn{1}{c}{}                           & \multicolumn{1}{c}{}                            & \multicolumn{1}{c}{Complete observation} & Partial observation \\ \midrule
% $D$  & 1.0  &  1.0000 (0.0001)  &  1.0130 (0.1166) \\[0.2cm]
% $\lambda$  & 2.0  & 2.0000 (0.0001)  &  2.0094 (0.1240)  \\[0.2cm]
% $\alpha$  &  4.0   & 4.0146 (0.1937) &  4.0310 (0.1969)   \\[0.2cm]
% $\sigma$  &  4.0   & 3.9982 (0.0030) &  3.9880 (0.2417)    \\ \bottomrule
% \end{tabular} \label{table:qgle_dw}
% \end{table}
%
\subsubsection{Multivariate Extended State} 
We consider a QGLE with one-dimensional coordinates and multivariate extended variable, motivated from the work of  \citep{ay:21} that studies protein-folding kinetics via a Quasi-Markovian GLE (\ref{eq:qgle-II}) and showcases that a QGLE accurately reproduces simulations of molecular dynamics (MD) that involve memory effects in the friction. In their investigation, an one-dimensional reaction coordinate, $q_t$, given as the sum of the separations between native contacts, is modelled via the following QGLE:
%
\begin{align}
\begin{aligned} \label{eq:qgle_second}
d q_t & = \tfrac{1}{m} \times p_t \, dt; \\[0.1cm] 
d p_t & = - \nabla U (q_t) dt + \sum_{l = 1}^L s_{l, t} dt; \\[0.1cm] 
d s_{l, t} & = - \tfrac{1}{\tau_l} \times s_{l, t} \,  dt  -  \tfrac{c_l}{\tau_l} \times p_t \, dt + \tfrac{\sqrt{2 \beta^{-1} c_l}}{\tau_l} \, d B_{l, t},  \quad  s_{l, 0} \sim \mathscr{N} (0, \beta^{-1}), \quad  \, 1 \le l \le L, 
%  
\end{aligned}
\end{align}
% 
where $M, \beta > 0$ denote the mass and the inverse thermal energy respectively, $\{ c_l, \tau_l\}_{1 \le l \le L}$ are the unknown parameters taking positive values, for $L\ge 1$, and  $U : \mathbb{R} \to \mathbb{R}$, the folding free energy landscape for proteins, is specified as $q \mapsto U(q) = - \beta^{-1} \log \nu (q)$ with $\nu (\cdot)$ being the equilibrium probability density function. QGLE (\ref{eq:qgle_second}) corresponds to the non-Markovian GLE (\ref{eq:gle}) with the memory kernel given as a so-called \emph{Prony series}:
%
\begin{align} \label{eq:memory_kernel}
K(t) = \sum_{l = 1}^L \frac{c_l}{\tau_l} 
\times \exp \Bigl(- \frac{t}{\tau_l} \Bigr), \quad t \geq 0.  
\end{align}
% 
\cite{ay:21} constructed QGLE (\ref{eq:qgle_second}) with $L = 5$ by determining the parameters via a least squares method so that the memory kernel (\ref{eq:memory_kernel}) fits the one extracted numerically from the observed time-series of $q$. 

In our experiment, we estimate the unknown parameters by maximising the marginal likelihood given the partial observations $q_{0:n}$. 
%\textcolor{red}{as \cite{vr:22} studied likelihood-based construction of QGLE in class (\ref{eq:hypo-I})}. 
For simplicity, we set $m = 1$, $L = 2$, and specify the free energy function as $q \mapsto U (q) = a (q - q_{\mathrm{min}})^2 (q - q_{\mrm{max}})^2 + b q^3$ with constants $(q_{\mathrm{min}}, q_{\mathrm{max}}, a, b) = (0.30, 0.90, 1200, 0.001)$. We set $\beta^{-1} = 2.949$ and select the true parameters as $\theta^{\dagger} = (c_1^{\dagger}, \tau_1^{\dagger}, c_2^{\dagger}, \tau_2^{\dagger}) = (0.22, 0.007, 1.2, 4.6)$, as such a choice closely reproduces the shape of the memory kernel estimated in \citep{ay:21}. We generate $50$ independent trajectories of $q$ on the time inverval $[0,1500]$ by applying scheme (\ref{eq:lg_hypo_II}) to QGLE (\ref{eq:qgle_second}) with step-size $10^{-4}$. We discard the observations up to time $500$ and sub-sample the datasets $q_{0:n}$ in equilibrium, so that  $n = 10^6$, $\Delta_n = 10^{-3}$, $T = 1000$. In Figure \ref{fig:gle_model}, we plot the shape of the free energy $U$ and one trajectory of the component $q$ from the QGLE (\ref{eq:qgle_second}) in the chosen setting. Notice that QGLE (\ref{eq:qgle_second}) is a conditionally Gaussian non-linear system (given the component~$q$),  thus
upon adoption of the locally Gaussian discretisation (\ref{eq:lg_hypo_II}),
the marginal likelihood can be calculated via the Kalman filter shown in Section \ref{sec:kalman_filter}. We use the Nelder-Mead method to optimise the marginal likelihoods with the initial value $\theta_0 = (0.1, 0.01, 1.0, 10.0)$. Figure \ref{fig:qgle_second}, summarises the results from   estimation of $\theta = (c_1, \tau_1, c_2, \tau_2)$ given the partial observations. The boxplots of (approximate) maximum likelihood estimates in Figure (\ref{fig:qgle_second_mle}) indicate that parameter estimation carried out via the locally Gaussian scheme (\ref{eq:lg_hypo_II}) delivers consistent estimates of the 
parameters. We typically observe that the standard error of estimates for $(c_1, \tau_1)$ is much smaller than that for $(c_2, \tau_2)$. In Figure (\ref{fig:memory_kernel_gle}), we plot the memory kernel (\ref{eq:memory_kernel}) with the parameters set equal to the mean value of the $50$ MLEs to observe the level of  agreement between the estimated memory kernel and the reference kernel, the latter computed with the true parameter values, where the relative absolute errors between the true and estimated memory kernels are within $0.1$ across periods $t \in [0.001, 10]$.  
% 
% 
% Figure environment removed
% Figure environment removed
%
% 
%%%%%%%%%%%%%%
\section{Conclusions and Future Directions}
\label{sec:end}
We have studied parameter inference procedures for the highly degenerate class of SDEs 
%focusing on hypo-elliptic diffusions specified as the class (\ref{eq:hypo-II}) 
that includes a wide range of practical models, e.g., quasi-Markovian generalised equations (QGLEs), 
%attracting a lot of attentions recently in practice, 
epidemiological models with time-varying parameters \citep{sir:22, dur:13},  non-linear continuous-time autoregressive models \citep{ts:00} and the classical Lorentz system upon consideration of noise effects \citep{co:21}. We have introduced the locally Gaussian time-discretisation scheme (\ref{eq:lg_hypo_II}) and provided analytical/numerical results showcasing that parameter estimation based upon such scheme sidesteps biases that would arise under alternative schemes. %under the high frequency observations setting. 
The approach followed in this work for establishing our results for class  (\ref{eq:hypo-II}) are expected to also 
guide extensions to more general classes of degenerate diffusions, for which iterated Lie brackets of \emph{any} order, i.e., $[\tilde{V}_0, [\tilde{V}_0,\ldots, [\tilde{V}_0, V_k]]]$, $1 \le k \le d$, are required for Hormander's condition to hold. Here, we draw upon the understanding obtained via the study of classes (\ref{eq:hypo-I}) and (\ref{eq:hypo-II}) to summarise key arguments for carrying out unbiased parameter estimation for general hypo-elliptic systems.  
% 
First, in a partial observation regime, use of a degenerate discretisation (e.g.~Euler-Maruyama) or equivalently of finite-differences to impute latent components will induce bias at estimates of diffusion coefficient parameters (recall the case study in Section \ref{sec:case_2}). To avoid use of finite-differences, the development of a non-degenerate conditionally Gaussian scheme for the full coordinates is essential, with the Gaussian noise obtained via high-order stochastic Taylor expansion of the drift functions. 
A lot of care should be given at the deterministic terms of the expansion to be included into the scheme, to avoid emergence of biases in estimates of drift parameters (recall the analytical study in Section \ref{sec:case_drift} and the numerical results in Section \ref{sec:num_case_study}).   
% 
We summarise below the above-designated roadmap for the construction of `correct' time-discretisation schemes for general classes of hypo-elliptic diffusions. 
%(not only for the classes (\ref{eq:hypo-I}) and (\ref{eq:hypo-II})) 
%for the purpose of unbiased parameter estimation:
% 
% 
\begin{itemize}
\item[Step 1.]  
For the rough component, $X_{R}$, the Euler-Maruyama scheme is applied. 
% with the Gaussian noise $(B_{t_{i+1}} - B_{t_i})$ appearing. 
% 
\item[Step 2.]  For the smooth coordinates in the model, one recursively applies stochastic Taylor expansion to drift functions so that Gaussian variates, in the form of iterated integrals involving Brownian motions, e.g.~of the form $\textstyle{\int_{t_i}^{t_{i+1}} B_s ds}$, $\textstyle{\int_{t_i}^{t_{i+1}} \int_{t_i}^{u} B_s ds du}$, appear in all smooth coordinates. This process is completed once the covariance-variance of the Gaussian approximation is positive definite. 
\item[Step 3.] For a smooth component containing Gaussian noise of size $\mathcal{O} ( \Delta_n^{(2k - 1)/2} )$, $k \ge 2$, the scheme should include all deterministic terms from the stochastic  Taylor expansion up to size  $\mathcal{O} (\Delta_n^k)$. 
\end{itemize}
% 
% 
Indicatively, Table \ref{table:lg_II} summarises the size of determistic and noisy parts of the locally Gaussian scheme (\ref{eq:lg_hypo_II}) for class (\ref{eq:hypo-II}). 
% Also, note that a tractable transition density of the scheme for the full coordinates exists due to Lemma \ref{prop:positive_definite} under the condition (\ref{assump:hypo})-(ii) associated with H\"ormander's condition. 
% As we have established in Section \ref{sec:main}, the contrast estimator constructed from the scheme is asymptotically unbiased under the high frequency, complete observation regime (Theorem \ref{thm:consistency} \& \ref{thm:clt}). We have also empirically shown in Section \ref{sec:numerical_studies} that parameter inference employing the local Gaussian scheme is asymptotically unbiased under the partial observation regime. 
% 

Our work in this paper leads to further research in several directions. 
In the CLT of the main analytical result for the parameter estimator (Theorem \ref{thm:clt}), the step-size $\Delta_n$ is required to satisfy $\Delta_n = o (n^{-1/2})$. An open problem for hypo-elliptic diffusions is the construction of estimators giving a CLT under a weaker condition $\Delta_n = o (n^{-1/p})$, $p \ge 3$. 
% For elliptic SDEs, such generalisations have been studied in \citep{kess:97} and \citep{uchi:12}, while \citep{iguchi:22} has produced results for class (\ref{eq:hypo-I}) with $p = 3$. 
We expect that such a general estimator for degenerate diffusion models can be produced, with accompanying theory then following the strategy used in our proofs in this work,  as we discussed in Remark \ref{rem:pf_main}. In a different direction, the effectiveness of the developed locally Gaussian scheme is yet to be studied under a low-frequency observation setting, i.e.~with the step-size $\Delta$ assumed fixed and not small enough, in which case a number, say $M$, of inner sub-steps are introduced by the user. Under such a setting, the discretisation error of the true (intractable) density over the period of size $\Delta$ typically diminishes as $M$ increases. In the case of elliptic diffusions, explicit rates of convergence to zero are provided in  \cite{go:08, iguchi:21-2}. Finally, 
in this work, in the practical scenario of partial observations, we have investigated the behaviour of discretisation schemes via case studies and numerical experiments. Analytical theory would be quite instructive in this setting. Techniques used in the context of hidden Markov models (see e.g.~\cite{douc:14}) are expected to be valuable in such a pursuit.  
% 
\begin{table}%[H] 
\centering
\caption{Size (in $\Delta_n$) of the terms appearing in the locally Gaussian scheme (\ref{eq:lg_hypo_II}).}
\label{table:lg_II}
\begin{tabular}{lcc}
 \toprule
  {Component}
 & {Gaussian part}
 & {Deterministic part}
  \\ 
 \midrule
 \\[-0.4cm] 
 $\bar{X}^{(\mrm{II})}_{S_1, {i+1}}$ 
 &   $\mathcal{O} (\Delta_n^{5/2}), \quad 
 \Bigl( {\textstyle \int_{t_i}^{t_{i+1}} \int_{t_i}^u B_{s} ds du} \Bigr)$      &   $\mathcal{O} (\Delta_n^3)$      
 \\[0.4cm]
 $ \bar{X}^{(\mrm{II})}_{S_2, {i+1}}$  
 & $ \mathcal{O} (\Delta_n^{3/2}), \quad 
 \Bigl( {\textstyle \int_{t_i}^{t_{i+1}} B_{s} ds} \Bigr)$                
 & $\mathcal{O} (\Delta_n^2)$       
 \\[0.4cm]
 $\bar{X}^{(\mrm{II})}_{R, {i+1}}$                           
 & $\mathcal{O} (\Delta_n^{1/2}), \quad \bigl( B_{t_{i+1}} - B_{t_{i}} \bigr)$
 & $\mathcal{O} (\Delta_n) $ 
 \\[0.2cm] 
\bottomrule
\end{tabular}
\end{table}
% 
% 
\section*{Acknowledgements}
YI acknowledges support from the Additional Funding Programme for Mathematical Sciences, delivered by EPSRC (EP/V521917/1) and the Heilbronn Institute for Mathematical Research.
% 
% 
\begin{appendices}
\section{Preliminaries} \label{appendix:aux}
%
In Section \ref{app:notation} we present some notation  used in the Appendix. In Section \ref{app:aux} we introduce three auxiliary results needed in the proof of our main theorems (Theorems~\ref{thm:consistency},  \ref{thm:clt}) in Section~\ref{sec:main}.  
%
% 
\subsection{Notation}
\label{app:notation}
For $0 = t_0 < \cdots < t_n$, with equi-distant step-size $\Delta_n$, we write $\sample{i}$ for the observation at time $t_i$ of the solution of the hypo-elliptic SDE (\ref{eq:hypo-II}) under the true parameter value~$\trueparam$,  defined upon the filtered probability space $(\Omega, \mathcal{F}, \{\mathcal{F}_t\}_{t \geq 0}, \mathbb{P})$. We denote by $\truedist$ the invariant distribution of process (\ref{eq:hypo-II}) under $\trueparam$. In agreement with the structure of class (\ref{eq:hypo-II}), we often represent $x \in \mathbb{R}^N$ and $\theta \in \Theta \subset \mathbb{R}^{N_{\theta}}$ as
% 
\begin{gather*}
x = (x_{S_1}, x_{S_2}, x_R) \in \mathbb{R}^{N_{S_1}} \times \mathbb{R}^{N_{S_2}} \times \mathbb{R}^{N_R}, \quad  
x_S \equiv (x_{S_1}, x_{S_2}); \\[0.2cm]
\theta = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma) \in 
\Theta_{N_{\beta_{S_1}}} \times \Theta_{N_{\beta_{S_2}}} \times \Theta_{N_{\beta_{R}}} \times \Theta_{N_{\sigma}}, \quad 
\beta_S \equiv (\beta_{S_1}, \beta_{S_2}),
\end{gather*}
% 
% 
For $\varphi (\cdot , \theta) : \mathbb{R}^N \to \mathbb{R}$, $\theta \in \Theta$, bounded up to second derivatives, we define the differential operators $\mathcal{L}$ and $\mathcal{L}_j, \; 1 \le  j  \le d$: 
%  
\begin{gather*}
\mathcal{L} \varphi (x ,  \theta) 
= \sum_{i=1}^N V_0^i (x, \theta) \frac{\partial \varphi} {\partial x_i}(x, \theta)  
+ \tfrac{1}{2}  \sum_{i_1, i_2 = 1}^N \sum_{k=1}^d  V_k^{i_1} (x, \theta) V_k^{i_2} (x, \theta)  \frac{\partial^2 \varphi }{\partial x_{i_1} \partial x_{i_2} } (x,\theta);  \\
\mathcal{L}_j \varphi (x , \theta)
= \sum_{i=1}^N V_j^i (x, \theta) \frac{\partial \varphi}{\partial x_i}(x , \theta), \quad 1 \le j \le d.  
\end{gather*} 
% 
%for $(x, \theta) \in \mathbb{R}^N \times \Theta$. 
Application of the above differential operators is extended to vector-valued functions in the apparent way, via separate consideration of each scalar component. We recall some notation used in the definition of the contrast function $\ell_n (\theta)$ in (\ref{eq:contrast}).
We have that %For $(\Delta, x , \theta) \in (0, \infty) \times \mathbb{R}^N \times \Theta$, 
% 
\begin{align*} %\label{eq:drift_expansion}
\mu (\Delta, x, \theta) = \bigl[ 
\mu_{S_1} (\Delta, x, \theta)^\top , \, 
\mu_{S_2}  (\Delta, x, \theta)^\top , \, 
\mu_{R} (\Delta, x, \theta)^\top 
\bigr]^\top,  
\end{align*}
% 
where 
%
\begin{align*} 
\begin{bmatrix}
{\mu}_{S_1}  (\Delta, x, \theta) \\[0.2cm]
{\mu}_{S_2}  (\Delta, x, \theta)  \\[0.2cm]
{\mu}_{R}  (\Delta, x, \theta) 
\end{bmatrix} 
=
\begin{bmatrix}
x_{S_1} + V_{S_1,0} (x_S, \beta_{S_1}) \Delta 
+ \mathcal{L} V_{S_1, 0} (x, \theta) \tfrac{\Delta^2}{2}
+ \mathcal{L}^2 V_{S_1, 0} (x, \theta) \tfrac{\Delta^3}{6}   
\\[0.3cm]
x_{S_2} + V_{S_2,0} (x, \beta_{S_2}) \Delta 
+ \mathcal{L} V_{S_2, 0} (x, \theta) \tfrac{\Delta^2}{2}  \\[0.3cm] 
x_R + V_{R, 0} (x, \beta_R) \Delta
\end{bmatrix}. 
\end{align*} 
% 
When $\Delta = 1$, we simply write 
$$\mu (x, \theta) \equiv \mu (1, x, \theta).$$ 
% 
For $x = (x_{S_1}, x_{S_2}, x_R) \in \mathbb{R}^N \equiv \mathbb{R}^{N_{S_1}} \times \mathbb{R}^{N_{S_2}} \times \mathbb{R}^{N_{R}}, \, y = (y_{S_1}, y_{S_2}, y_R) \in \mathbb{R}^N$ , $\Delta > 0$ and $\theta \in \Theta$, we define
% 
%  
\begin{align} \label{eq:m} 
m  (\Delta, x, y , \theta)  
= \left[
\,  
\frac{y_{S_1}^\top - {\mu}_{S_1} (\Delta, x; \theta)^\top}{\sqrt{\Delta^{5}}}, \; 
\frac{y_{S_2}^\top - {\mu}_{S_2} (\Delta, x; \theta)^\top}{\sqrt{\Delta^{3}}}, \;  
\frac{y_{R}^\top - {\mu}_{R} (\Delta, x; \theta)^\top }{\sqrt{\Delta}} \,
\right]^\top. 
\end{align}
% 
% \begin{align} \label{eq:m}
% m  (\Delta, x, y ;  \theta) 
%   \equiv  
% \begin{bmatrix}
% \frac{y_{S_1} - {\mu}_{S_1} (\Delta, x; \theta)}{\sqrt{\Delta^{5}}}  \\[0.3cm]
% \frac{y_{S_2} - {\mu}_{S_2} (\Delta, x; \theta)}{\sqrt{\Delta^{3}}} \\[0.3cm]
% \frac{y_R - {\mu}_{R} (\Delta, x; \theta)}{\sqrt{\Delta}}
% \end{bmatrix}. 
% \end{align}
% 
%
We write, for $1 \le i \le n$, 
% 
\begin{gather}  \label{eq:m_simple}
  m_{i} (\Delta, \theta)  
\equiv m (\Delta, \sample{i-1}, \sample{i}, \theta). %\qquad 
%\Delta > 0, %\quad \theta \in \Theta, 
% ,  \quad 
%   \Sigma_{i-1} (\theta)  \equiv  \Sigma (\sample{i-1}, \theta), 
%   \quad  \Lambda_{i-1} (\theta) 
%   \equiv  \Sigma^{-1} (\sample{i-1}, \theta). 
\end{gather}
%  
% 
% 
We use $\Sigma (\Delta, x, \theta)$ to represent the covariance of one step of the local Gaussian scheme (\ref{eq:lg_hypo_II}) for the hypo-elliptic SDE (\ref{eq:hypo-II}), given step-size $\Delta>0$, initial point $x\in\mathbb{R}^N$ and parameter~$\theta$. 
We often write $$\Sigma (x, \theta) \equiv \Sigma (1, x, \theta).$$ We express the inverse of $\Sigma (x, \theta)$
%, $(x, \theta) \in \mathbb{R}^N \times \Theta$, 
as: 
%
\begin{align} 
   \Sigma^{-1} (x, \theta) 
   = 
   \Lambda (x, \theta)
   & =
   \begin{bmatrix}  \label{eq:inv_Sigma_2} 
    \Lambda_{S_1 S_1} (x, \theta) 
    & \Lambda_{S_1 S_2} (x, \theta)
    & \Lambda_{S_1 R} (x, \theta)  \\[0.2cm] 
    \Lambda_{S_2 S_1} (x, \theta) 
    & \Lambda_{S_2 S_2} (x, \theta)
    & \Lambda_{S_2 R} (x, \theta)  \\[0.2cm]  
    \Lambda_{R S_1} (x, \theta) 
    & \Lambda_{R S_2} (x, \theta)
    & \Lambda_{R R} (x, \theta) 
   \end{bmatrix},    
\end{align}  
% 
where each block matrix is specified as
$$
\Lambda_{\iota_1 \iota_2} (x, \theta) \in \mathbb{R}^{N_{\iota_1} \times N_{\iota_2}}, 
\quad \iota_1, \iota_2  \in \{S_1, S_2, R\}. 
$$
% 
Each block element of the matrix $\Sigma (x, \theta)$ is given later in Section \ref{appendix:positive_definite}, and we emphasise here that $\Sigma (x, \theta)$ and its inverse $\Lambda (x, \theta)$ depend on $x$ and $(\beta_{S}, \sigma)$ but not on the drift parameter $\beta_R$ in the rough component, and this is critical in the proof of consistency of $\hat{\beta}_{R, n}$. Thus, we sometimes write 
$\Sigma (x, (\beta_S, \sigma))$ and $\Lambda (x, (\beta_S, \sigma ))$ to highlight the parameter dependency.
% We also use the notation
% % 
% \begin{align*}
%     \Sigma_{i} (\theta) \equiv \Sigma (\sample{i}, \theta), \quad 
%     \Lambda_{i} (\theta) \equiv \Lambda (\sample{i},  \theta), 
%     \qquad  0 \le i \le n.
% \end{align*}
% 
We define the mappings 
\begin{align*}
\eta_{S_1}:\mathbb{R}^N \times \Theta_{\beta_{S_1}} \to \mathbb{R}^{N_{S_1}}, \quad 
\eta_{S_2}:\mathbb{R}^N \times \Theta_{\beta_{S_2}} \to \mathbb{R}^{N_{S_2}}, \quad   
\eta_{R}:\mathbb{R}^N \times \Theta_{\beta_{R}}  \to \mathbb{R}^{N_{R}} 
\end{align*}
%
as 
%  
\begin{gather*}
\eta_{S_1} (x, \beta_{S_1})  = V_{S_1, 0} (x_S, \truebeta_{S_1}) - V_{S_1, 0} (x_S, \beta_{S_1}), \quad  
\eta_{S_2} (x, \beta_{S_2}) = V_{S_2, 0} (x, \truebeta_{S_2}) - V_{S_2, 0} (x, \beta_{S_2}); \\[0.2cm] 
\eta_{R} (x, \beta_{R}) = V_{R, 0} (x, \truebeta_{R}) - V_{R, 0} (x, \beta_{R}).   
\end{gather*}
% 
We write, with a slight abuse of notation, for $0 \le i \le n$,
% 
\begin{align*}
\eta_{S_1, i} (\Delta, \beta_{S_1})
\equiv \tfrac{\eta_{S_1} (\sample{i}, \, \beta_{S_1})}{\Delta}, 
\quad \eta_{S_2, i} (\Delta, \beta_{S_2})
 \equiv \tfrac{ \eta_{S_2} (\sample{i} , {\beta}_{S_2})}{\Delta}. %\qquad \Delta > 0, 
\end{align*}
% 
We denote by $\mathcal{S}$ the space of functions $f : [0, \infty) \times \mathbb{R}^N \times \Theta \to \mathbb{R}$ so that there are  constants $C, q > 0$ such that 
$| f (\Delta, x, \theta) |  \le C \Delta\, ( 1 + |x|^q)$ for any $(\Delta, x, \theta) \in [0, \infty) \times \mathbb{R}^N \times \Theta$. For an $M_1 \times M_2$ matrix $A$, with $M_1,M_2\ge 1$, we write each matrix entry as $[A]_{ij}$ for $1 \le i \le M_1, \, 1 \le j \le M_2$.  We recall the probability law of the process 
$\{ X_{t} \}_{t \geq 0}$ under a parameter $\theta \in \Theta$ is written as $\mathbb{P}_{\theta}$, and 
%
$$\probconv, \ \ \distconv$$
%
indicate convergence in probability and distribution, respectively, under the true parameter $\trueparam$. An
expectation under the probability law $\mathbb{P}_\theta$ is written as $\mathbb{E}_\theta$. 
We write $$\textstyle \partial_{u}  = \big[ \tfrac{\partial }{\partial u_{1}}, \ldots,  \tfrac{\partial }{\partial u_{n} } \big]^{\top}, \qquad  \partial^{2}_{u}   = \partial_{u} \partial_{u}^{\top} \equiv \big(\tfrac{\partial^{2}}{\partial u_{i} \partial u_{j} } \big)_{i,j=1}^{n}$$ for the standard differential operators acting upon maps $\mathbb{R}^{n}\to \mathbb{R}$, $n\ge 1$. 
%As above with $u \in \mathbb{R}^n$, 
We also write $\partial^u_\alpha = \tfrac{\partial^l}{\partial u_{\alpha_1} \cdots \partial u_{\alpha_l}}$ for a multi-index $\alpha \in \{1, \ldots, n \}^l$, $l \in \mathbb{N}$. For a function $g : \mathbb{R}^n \to \mathbb{R}^m, \, n, m \in \mathbb{N}$, we write: 
%
\begin{gather*}
\partial_u g (u)^\top = \bigl[ \tfrac{\partial}{\partial u_i} g^j (u) \bigr]_{\substack{1 \le i \le n, \, 1 \le j \le m}}, \quad 
\partial_u^\top g (u) = \bigl( \partial_u g (u)^\top  \bigr)^\top;  \\[0.2cm]  
\partial^u_\alpha g (u)^\top = \bigl[ \partial^u_\alpha g^1 (u), \ldots, \partial^u_\alpha g^m (u) \bigr], \, \quad \partial^u_\alpha g (u) = \bigl( \partial^u_\alpha g (u)^\top \bigr)^\top.  
\end{gather*}
% the derivative operator $\partial^u_\alpha$ acts as: 
% % 
% \begin{align*}
% \partial_\alpha^u g (u) \equiv \bigl[\,  \partial_\alpha^u g^1 (u) , \ldots, \partial_\alpha^u g^m (u)  \, \bigr].   
% \end{align*}
% , and $\partial_{u_i} = \tfrac{\partial}{\partial u_i} $ for $1 \le i \le n$.  
% 
\subsection{Auxiliary Results}
\label{app:aux}
We prepare some auxiliary results to be used in the proof of Theorems \ref{thm:consistency} and \ref{thm:clt}. 
\begin{lemma} \label{lemma:aux_1}
Let $Y_{t_i}$, $U$ be random variables, with $Y_{t_i}$ being $\mathcal{F}_{t_i}$-measurable. %The following two convergences imply $\textstyle{\sum_{i=1}^n Y_{t_{i}} \probconv U}$:
If
\begin{align*}
  \sum_{i=1}^n \trueE [\,Y_{t_{i}}\,|\,\filtration{i-1}\,]  \probconv  U,  \quad 
  \sum_{i=1}^n \trueE\big[\,(Y_{t_{i}})^2\,|\,\filtration{i-1}\,\big]  \probconv  0,
\end{align*} 
then $\textstyle{\sum_{i=1}^n Y_{t_{i}} \probconv U}$.
\end{lemma} 
% 
\begin{proof}[Proof.]
See Lemma 9 in \cite{genon:93}. 
\end{proof} 
% 
% 
\begin{lemma} \label{lemma:ergodic_thm}
Let $f : \mathbb{R}^N \times \Theta \to \mathbb{R}$ be differentiable w.r.t.~$(x, \theta) \in \mathbb{R}^N \times \Theta$ with derivatives of polynomial growth in $x$ uniformly in $\theta$. Under conditions (\ref{assump:coeff})--(\ref{assump:finite_moment}), it holds that, if \limit, then
\begin{align*}
   \tfrac{1}{n} \sum_{i=1}^n f (\sample{i-1}, \theta) \probconv 
   \int f (x, \theta) \truedist (dx),
\end{align*}
uniformly in $\theta \in \Theta$. 
\end{lemma}
% 
% 
\begin{proof}[Proof.]
This is a multivariate version of Lemma 8 in \cite{kess:97}, so we omit the proof.  
\end{proof}
%
% 
% 
\begin{lemma} \label{lemma:canonical_conv}
Let $1 \leq j_1, j_2 \leq N$ and assume that $f : \mathbb{R}^N \times \Theta \to \mathbb{R}$ is as in Lemma \ref{lemma:ergodic_thm}. Under conditions (\ref{assump:coeff})--(\ref{assump:finite_moment}), it holds that, if \limit, then
%
\begin{gather}
\tfrac{1}{n} \sum_{i=1}^n 
f (\sample{i-1}, \theta) \, 
m^{j_1}_i (\Delta_n, \trueparam)  \, 
m^{j_2}_i (\Delta_n, \trueparam) \probconv \int  f (x, \theta) \bigl[ {\Sigma} (x, \trueparam) \bigr]_{j_1 j_2} \truedist (dx);  \label{eq:canonical_conv1} \\
\tfrac{1}{n  \sqrt{\Delta_n}} \sum_{i=1}^n   f (\sample{i-1}, \theta)  m^{j_1}_i  (\Delta_n, \trueparam)  \probconv 0, 
\label{eq:canonical_conv2} 
\end{gather}
%
uniformly in $\theta \in \Theta$. 
\end{lemma}
% 
\begin{proof}[Proof.]
Notice that for any $1 \le j_1, j_2 \le N$ and $0 \le i \le n$, 
% 
\begin{align} 
& \mathbb{E}_{\trueparam}
\bigl[m^{j_1}_{i} (\Delta_n, \trueparam) 
\, | \,  \mathcal{F}_{t_{i-1}} \bigr]
= R_{j_1} (\sqrt{\Delta_n^3}, \sample{i-1}, \trueparam); 
\label{eq:mean_expectation} \\[0.3cm]
& 
\mathbb{E}_{\trueparam} 
\bigl[ 
m^{j_1}_i (\Delta_n, \trueparam) \, 
m^{j_2}_i (\Delta_n, \trueparam)  
\, |  \, \mathcal{F}_{t_{i-1}} \bigr]  
= \bigl[ \Sigma (\sample{i-1}, \trueparam) \bigr]_{j_1 j_2} + R_{j_1 j_2} (\Delta_n, \sample{i-1}, \trueparam),  \label{eq:squared_mean_expectation} 
\end{align} 
% 
where $R_{j_1 j_2}, \, R_{j_1} \in \mathcal{S}$. Applying Lemmas \ref{lemma:aux_1}, \ref{lemma:ergodic_thm} with formulae (\ref{eq:mean_expectation}) and (\ref{eq:squared_mean_expectation}), we obtain (\ref{eq:canonical_conv1}) and (\ref{eq:canonical_conv2}) in the same way as in the proof of Lemma 12 in \cite{iguchi:22}. 
\end{proof}  

\section{Proof of Proposition \ref{prop:positive_definite}} \label{appendix:positive_definite}
%
% The covariance matrix of the one-step local Gaussian scheme (\ref{eq:lg_hypo_II}) is given as: 
% % 
% \begin{align}  \label{eq:covariance_lg}
% \Sigma (\Delta, x, \theta) 
% =
% \begin{bmatrix}
% \Sigma_{S_1 S_1} (\Delta, x, \theta) & 
% \Sigma_{S_1 S_2}  (\Delta, x, \theta) & 
% \Sigma_{S_1 R}  (\Delta, x, \theta) \\[0.2cm]
% \Sigma_{S_2 S_1}  (\Delta, x, \theta) & 
% \Sigma_{S_2 S_2} (\Delta, x, \theta) & 
% \Sigma_{S_2 R } (\Delta, x, \theta) \\[0.2cm]
% \Sigma_{R S_1} (\Delta, x, \theta) & 
% \Sigma_{R S_2}  (\Delta, x, \theta) &
% \Sigma_{R R}  (\Delta, x, \theta) 
% \end{bmatrix},   
% \end{align}
% % 
% % 
% for $(\Delta, x, \theta) \in  (0, \infty) \times \mathbb{R}^N \times \Theta$, where each block matrix is specified as:  
% %
% \begin{align}
% \begin{aligned} \label{eq:block_Sigma}
%    & \Sigma_{S_1 S_1}  (\Delta, x, \theta)
%    = \tfrac{\Delta^5}{20} a_{S_1} (x, \theta), \quad  
%    \Sigma_{S_1 S_2}  (\Delta, x, \theta)
%   = \tfrac{\Delta^4}{8}
%    \partial_{x_{S_2}}^\top  V_{S_1, 0} (x_{S}, \beta_{S_1}) 
%    a_{S_2} (x, \theta);  \\[0.3cm]
%    & \Sigma_{S_1 R}  (\Delta, x, \theta)
%    = \tfrac{\Delta^3}{6}
%     \partial_{x_{S_2}}^\top  V_{S_1, 0} (x_{S}, \beta_{S_1}) 
%    \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) 
%    a_R (x, \sigma); \\[0.3cm] 
%    & \Sigma_{S_2 S_1}  (\Delta, x, \theta) 
%    =  \Sigma_{S_1 S_2}  (\Delta, x, \theta)^\top, \quad
%    \Sigma_{S_2 S_2}  (\Delta, x, \theta) 
%    = \tfrac{\Delta^3}{3} a_{S_2} (x, \theta); \\[0.3cm] 
%    & \Sigma_{S_2 R} (\Delta, x, \theta) 
%    = \tfrac{\Delta^2}{2}  
%     \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) 
%     a_R (x, \sigma), \quad 
%    \Sigma_{R S_1} (\Delta, x, \theta) =  \Sigma_{S_1 R} (\Delta, x, \theta)^\top;  \\[0.3cm] 
%    & \Sigma_{R S_2} (\Delta, x, \theta) 
%    = \Sigma_{S_2 R} (\Delta, x, \theta)^\top,  \quad 
%    \Sigma_{RR} (\Delta, x, \theta) = \Delta \, a_R (x, \sigma), 
% \end{aligned}
% \end{align}
% % 
% and the matrices 
% $a_R(x, \theta) \in \mathbb{R}^{N_R \times N_R}$, \, 
% $a_{S_1}(x, \theta) \in \mathbb{R}^{N_{S_2} \times N_{S_2}}$, \, 
% $a_{S_2} (x, \theta) \in \mathbb{R}^{N_{S_1} \times N_{S_1}}$ are defined as: 
% % 
% \begin{align*}
%  a_R (x , \theta) & =  
%  \sum_{k = 1}^d  V_{R,k} (x , \sigma)  V_{R, k} (x , \sigma)^\top;  \\[0.1cm] 
% a_{S_2} (x, \theta) & =
% \sum_{k = 1}^d 
% \Bigl( \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2})  V_{R,k} (x , \sigma) 
% \Bigr) 
% \Bigl( \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) V_{R,k} (x , \sigma) \Bigr)^\top; \\[0.2cm] 
% a_{S_1} (x, \theta) & = 
% \sum_{k = 1}^d \, \Bigl( 
% \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1})
% \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) V_{R,k} (x , \sigma)  
% \Bigr) \\ %[0.2cm] 
% & \qquad\qquad\qquad  \times 
% \Bigl( 
% \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1})
% \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) V_{R,k} (x , \sigma)  
% \Bigr)^\top. 
% \end{align*} 
% 
From the expression of the covariance $\Sigma (\Delta, x, \theta)$ in (\ref{eq:covariance_lg}), its determinant is given as: 
% 
\begin{align*} 
| \Sigma (\Delta, x, \theta) | =  \tfrac{\Delta^9}{8640} \, |a_{R} (x, \sigma)| \, |a_{S_1} (x, \theta)| \, |a_{S_2} (x, \theta)|,  
\end{align*} 
%
with the matrices 
$a_R(x, \sigma) \in \mathbb{R}^{N_R \times N_R}$, \, 
$a_{S_1}(x, \theta) \in \mathbb{R}^{N_{S_2} \times N_{S_2}}$, \, 
$a_{S_2} (x, \theta) \in \mathbb{R}^{N_{S_1} \times N_{S_1}}$ defined as: 
% 
\begin{align*}
 a_R (x , \sigma) & =  
 \sum_{k = 1}^d  V_{R,k} (x , \sigma)  V_{R, k} (x , \sigma)^\top;  \\[0.1cm] 
a_{S_2} (x, \theta) & =
\sum_{k = 1}^d 
\Bigl( \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2})  V_{R,k} (x , \sigma) 
\Bigr) 
\Bigl( \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) V_{R,k} (x , \sigma) \Bigr)^\top; \\[0.2cm] 
a_{S_1} (x, \theta) & = 
\sum_{k = 1}^d \, \Bigl( 
\partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1})
\partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) V_{R,k} (x , \sigma)  
\Bigr) \\ %[0.2cm] 
& \qquad\qquad\qquad  \times 
\Bigl( 
\partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1})
\partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) V_{R,k} (x , \sigma)  
\Bigr)^\top. 
\end{align*} 
The assertion holds if we show that under condition \ref{assump:hypo}-II, the three matrices $a_R(x, \sigma)$, $a_{S_1}(x, \theta)$ and $a_{S_2} (x, \theta)$ are positive definite for any $(x, \theta) \in \mathbb{R}^N \times \Theta$. Notice that  
% 
% \begin{align*}
% \Bigl\{ \mathrm{proj}_{N_{S} + 1, N} \bigl\{ V (x, \theta) \bigr\}
% :  V \in \widetilde{\mathscr{H}}_{\theta, 0}  \Bigr\} 
% = 
% \bigl\{ 
% V_{R, k} (x, \sigma) : 1 \le k \le d \bigr\}.  
% \end{align*}
% 
the first equation within the condition leads to the matrix $a_R (x, \sigma)$ being positive definite for all $(x, \sigma) \in \mathbb{R}^N \times \Theta_{\sigma}$. Furthermore, the first and second equations of condition \ref{assump:hypo}-II yield: 
% 
% 
\begin{align*}
& \mathrm{span} \Bigl\{ \mathrm{proj}_{N_{S_1} +1, N_S} \bigl\{ [\tilde{V}_{0}, V_{k}] (x, \theta) \bigr\} : 1 \le k \le d \Bigr\}  \\[0.2cm] 
& \qquad \qquad= \mathrm{span}  \Bigl\{ \partial_{x_R}^\top V_{S_2, 0} (x, \beta_S) V_{R, k} (x, \sigma)  : 
1 \le k \le d \Bigr\} = \mathbb{R}^{N_{S_2}}, 
\end{align*} 
% 
for each $(x, \theta) \in \mathbb{R}^N \times \Theta$, thus the matrix $a_S (x, \theta)$ is positive definite. Similarly, due to condition \ref{assump:hypo}-II,  
% 
\begin{align*} 
& \mathrm{span} \Bigl\{ \mathrm{proj}_{1, N_{S_1}} \bigl\{ \bigl[ \tilde{V}_{0}, [\tilde{V}_{0}, V_{k}] \bigr] (x, \theta) \bigr\} : 1 \le k \le d \Bigr\}  \\[0.2cm] 
& \qquad\qquad 
= \mathrm{span} \bigl\{ 
\partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_S)  
\partial_{x_R}^\top V_{S_2, 0} (x, \beta_S) V_{R, k} (x, \sigma)  :  1 \le k \le d \bigr\}  = \mathbb{R}^{N_{S_1}},
\end{align*}
%
for each $(x, \theta) \in \mathbb{R}^N \times \Theta$. This implies the positive definiteness of  $a_{S_1}(x, \theta)$. The proof is complete. 
%
% Consider the first model (\ref{eq:hypo-I}). Its covariance matrix $\Sigma^{\, (\mrm{I})} (\Delta, x, \theta)$ admits the following block matrix expression: 
% % 
% \begin{align} 
% \Sigma^{\, (\mrm{I})} (\Delta, x, \theta) 
% \equiv 
% \begin{bmatrix}
% \Sigma_{SS}^{\, (\mrm{I})} (\Delta, x, \theta)  
% & \Sigma_{SR}^{\, (\mrm{I})} (\Delta, x, \theta)  \\[0.2cm] 
% \Sigma_{RS}^{\, (\mrm{I})} (\Delta, x, \theta)  
% & \Sigma_{RR}^{\, (\mrm{I})} (\Delta, x, \theta) 
% \end{bmatrix}, 
% \end{align}
% % 
% where we have set: 
% % 
% \begin{align*}
% \Sigma_{RR}^{\, (\mrm{I})} (\Delta, x, \theta) 
% & = \Delta \sum_{k = 1}^d  V_{R,k} (x , \sigma)  V_{R, k} (x , \sigma)^\top 
% \equiv 
% \Delta \, a_R (x , \sigma)  ; \\
% \Sigma_{SR}^{\, (\mrm{I})} (\Delta, x, \theta)  
% & = \tfrac{\Delta^2}{2} \sum_{k = 1}^d 
% \mathcal{L}_k V_{S,0} (x, \theta) V_{R, k} (x, \sigma)^\top = \tfrac{\Delta^2}{2} 
% \partial_{x_R}^\top V_{S,0} (x, \theta) a_R (x, \sigma); \\ 
% \Sigma_{SS}^{\, (\mrm{I})} (\Delta, x, \theta)  
% & = \tfrac{\Delta^3}{3} \sum_{k = 1}^d 
% \mathcal{L}_k V_{S,0} (x, \theta) 
% \mathcal{L}_k V_{S,0} (x, \theta)^\top = \tfrac{\Delta^3}{3} a_S (x, \theta), 
% \end{align*}
% % 
% and $\Sigma_{RS}^{\, (\mrm{I})} (\Delta, x, \theta) 
% = \Sigma_{SR}^{\, (\mrm{I})} (\Delta, x, \theta)^\top$. We have from Lemma \ref{lemma:positive_def} that
% % 
% \begin{align}
% \bigl| 
% \Sigma^{\,(\mrm{I})} (\Delta, x, \theta)  
% \bigr| = \tfrac{\Delta^4}{12} 
% |a_S (x, \theta) | | a_R (x, \theta) | > 0. 
% \end{align}
% %
% 
% 
% 
\section{Proof of Main Results} \label{sec:pf} 
In this section
we prove the main results, i.e.~Theorem \ref{thm:consistency}, \ref{thm:clt} in Section \ref{sec:main} of  the main text. The proofs make use of some technical results from Appendix \ref{appendix:pf_tech}. %Throughout the proof, we assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:ident}) hold. 
% 
\subsection{Proof of Theorem \ref{thm:consistency} -- Consistency} 
\label{sec:pf_consistency}
%
To show consistency, we study the limit of the contrast function $\ell_{n}  (\theta)$, defined in (\ref{eq:contrast}), that involves terms such as
% 
\begin{align*}
\tfrac{X_{S_1, i+1}- \mu_{S_1} (\Delta_n, X_{i}, \theta)}{\sqrt{\Delta_n^{5}}}, \ \ 
\tfrac{X_{S_2, {i+1}} - \mu_{S_2} (\Delta_n, X_{i} , \theta)}{\sqrt{\Delta_n^{3}}}, 
\ \ 1 \le i \le n-1, 
\end{align*}
% 
where $\{ X_i \}_{i = 0, \ldots, n} $ are discrete-time observations under the true model (\ref{eq:hypo-II}) with parameter $\trueparam$. Then, the stochastic Taylor expansion for $X_{S, {i+1}}$ yields  
%
%  
\begin{align} 
\begin{aligned} \label{eq:diff_terms}
\tfrac{X_{S_1, i+1} - \mu_{S_1} (\Delta_n, X_{i}, \theta)}{\sqrt{\Delta_n^{5}}} 
 & = \tfrac{ 
   V_{S_1, 0} ( X_{S, i}, \truebeta_{S_1}) 
   - V_{S_1, 0} (X_{S, i}, \beta_{S_1}) }{\sqrt{\Delta_n^3}}  
   + R_{S_1}(\Delta_n, X_{i} , \theta);  \\[0.2cm]
 \tfrac{X_{S_2, {i+1}} - \mu_{S_2} ( \Delta_n, X_{i} , \theta)}{\sqrt{\Delta_n^{3}}} 
 & = \tfrac{ 
   V_{S_2, 0} ( X_{i}, \truebeta_{S_2}) 
   - V_{S_2, 0} (X_{i}, \beta_{S_2})  }{\sqrt{\Delta_n}}  
   + R_{S_2} (\Delta_n, X_{i}, \theta), 
\end{aligned}
\end{align}
% 
where $R_{S_1}, \, R_{S_2} \in \mathcal{S}$. Careful steps are needed to control the first terms in the right-hand-sides of (\ref{eq:diff_terms}) as $\Delta_n \to 0$, within the proof of consistency. Our proof proceeds with the following strategy which extends arguments used in \cite{iguchi:22}:
%without requiring that $\Delta_n = o (n^{-1/2})$:  
% 
% 
\begin{itemize}
    \item[Step 1.] We prove consistency, along with a convergence rate, for the estimator $\hat{\beta}_{S_1, n}$. That is, if \limit, then
    $$\hat{\beta}_{S_1, n} \probconv \beta_{S_1}^\dagger.$$
    In particular, we obtain the rate: 
    %
    \begin{align} \label{eq:step1}
    \tfrac{1}{\sqrt{\Delta_n^3}} \bigl(\hat{\beta}_{S_1, n} - \beta_{S_1}^\dagger \bigr) \probconv 0. 
    \end{align}
    % 
    %
    \item[Step 2.] Making use of the convergence rate in (\ref{eq:step1}), we prove consistency, along with a convergence rate, for the estimator $\hat{\beta}_{S_2, n}$. That is,  if \limit, then $$\hat{\beta}_{S_2, n} \probconv \beta_{S_2}^\dagger.$$ In particular, we obtain the rate:
    %
    \begin{align} \label{eq:step2}
        \tfrac{1}{\sqrt{\Delta_n}} \bigl(\hat{\beta}_{S_2, n} - \beta_{S_2}^\dagger \bigr) \probconv 0. 
    \end{align} 
    % 
    \item[Step 3.] Making use of the rates in (\ref{eq:step1}) and (\ref{eq:step2}), we prove consistency for the estimators $(\hat{\beta}_{R, n}, \hat{\sigma}_{n})$. That is, if \limit, then $$(\hat{\beta}_{R, n}, \hat{\sigma}_{n}) \probconv (\truebeta_R, \truesigma).$$
\end{itemize}
% 
% % 
We emphasise that in our proof of consistency, the condition $\Delta_n = o (n^{-1/2})$ is not required 
while \cite{glot:20} assumed the condition throughout the proof of consistency in the case of the degenerate diffusion class (\ref{eq:hypo-I}). Typically, in order to show the consistency of $\hat{\beta}_{R, n}$, \cite{glot:20} exploited the rates of convergence 
%
\begin{align}
\sqrt{\tfrac{n}{\Delta_n}} ( \beta_{S, n} - \truebeta_{S}) \probconv 0, \quad 
\sqrt{n} (\sigma_n - \truesigma) \probconv 0 
\end{align}
% 
that are derived under the condition $\Delta_n = o (n^{-1/2})$. In contrast, in our strategy, the rates of convergence (\ref{eq:step1}) and (\ref{eq:step2}) are obtained without requiring $\Delta_n = o (n^{-1/2})$, and are put into effective use to avoid explosion of terms such as
% 
\begin{align*}
\tfrac{V_{S_1, 0} (X_{S, {i-1}}, \truebeta_{S_1}) 
- V_{S_1, 0} (X_{S, {i-1}}, \hat{\beta}_{S_1, n})}{\sqrt{\Delta_n^3}}, \quad 
\tfrac{V_{S_2, 0} ( X_{i}, \truebeta_{S_2}) - V_{S_2, 0} (X_{i}, \hat{\beta}_{S_2, n})}{\sqrt{\Delta_n}} 
\end{align*}
% 
as $\Delta_n \to 0$ only, with the help of some results derived from straightforward matrix calculations (Lemma \ref{lemma:matrix}, \ref{lemma:lambda} and \ref{lemma:matrix_2} shown later).
% 
% 
%
%  
\subsubsection{Step 1}  \label{sec:step1}
Consistency of the estimator $\hat{\beta}_{S_1, n}$ is deduced from the following result.%
% 
\begin{lemma} \label{lemm:step1}
Assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:finite_moment}) hold. If \limit, then,
%
\begin{align*} 
\tfrac{\Delta_n^3}{n} \ell_{n} (\theta) 
\probconv 
\int 
% % \bigl( V_{S_1, 0} (x_S, \beta_{S_1}) - V_{S_1, 0} (x_S, \truebeta_{S_1}) \bigr) 
% \Sigma^{-1}_{S_1 S_1} (x , \theta) 
% \bigl( V_{S_1, 0} (x_S , \beta_{S_1}) 
% - V_{S_1, 0} (x_S , \truebeta_{S_1}) 
\eta_{S_1} (x, \beta_{S_1})^\top 
\Lambda_{S_1 S_1} (x, \theta) \, 
\eta_{S_1} (x, \beta_{S_1}) 
\truedist (dx),             
\end{align*} 
% 
uniformly in $\theta \in \Theta$. 
\end{lemma}
% 
\noindent The proof is given in Appendix \ref{appendix:pf_step1}. Lemma \ref{lemm:step1} implies the consistency of $\hat{\beta}_{S_1, n}$ via the following discussion. From the definition of estimator, we have that for every $\varepsilon > 0$, 
% 
\begin{align*}
\mathbb{P}_{\trueparam} \bigl( | \hat{\beta}_{S_1, n}  - \truebeta_{S_1} | > \varepsilon \bigr) 
\leq 
\mathbb{P}_{\trueparam} \Bigl(
\tfrac{\Delta_n^3}{n} \ell_{n}
\bigl( \hat{\theta}_{n} \bigr) 
<  
\tfrac{\Delta_n^3}{n} \ell_{n} 
\bigl( \truebeta_{S_1}, \hat{\beta}_{S_2, n}, \hat{\beta}_{R, n}, \hat{\sigma}_{n} \bigr)  
\Bigr). 
\end{align*}
%
From the compactness of $\Theta$, the identifiability condition (\ref{assump:ident}) and the positive definiteness of $\Lambda_{S_1 S_1} (x, \theta)$ for any $(x, \theta) \in \mathbb{R}^N \times \Theta$, Lemma \ref{lemm:step1} gives
%
\begin{align*}   
  \mathbb{P}_{\trueparam} \Bigl(
    \tfrac{\Delta_n^3}{n} \ell_{n}
    \bigl( \hat{\theta}_{n} \bigr) 
    <  
    \tfrac{\Delta_n^3}{n} \ell_{n} 
    \bigl( \truebeta_{S_1}, \hat{\beta}_{S_2, n}, \hat{\beta}_{R, n}, \hat{\sigma}_{n} \bigr)  
    \Bigr)
    \to 0, 
\end{align*}
% 
as \limit, which leads to the consistency of $\hat{\beta}_{S_1, n}$.  
% 
% 
\\

We now prove the rate of convergence in (\ref{eq:step1}). Considering the Taylor expansion of $\partial_{\beta_{S_1}} \ell_{n} (\hat{\theta}_{n})$ around 
$\partial_{\beta_{S_1}} \ell_{n} ( \truebeta_{S_1}, \hat{\beta}_{S_2,n}, \hat{\beta}_{R, n}, \hat{\sigma}_{n})$ with an appropriate scaling factor, we obtain   
%
\begin{align*} 
\mathscr{A}_{S_1, n}(  
 \truebeta_{S_1}, 
 \hat{\beta}_{S_2, n}, \hat{\beta}_{R, n}, \hat{\sigma}_{n} ) 
= \mathscr{B}_{S_1, n}
( 
 \hat{\theta}_{n} ) 
 \; \times 
 \tfrac{1}{\sqrt{\Delta_n^3}} (\hat{\beta}_{S_1, n} - \truebeta_{S_1}),  
\end{align*}
% 
where we have set, for 
$\theta = (\beta_{S_1}, \theta^{- \beta_{S_1}} ) \in \Theta$ with $\theta^{- \beta_{S_1}} \equiv (\beta_{S_2}, \beta_R, \sigma)$, 
%
% 
\begin{align*} 
\mathscr{A}_{S_1, n} ( \theta )  
=
- \tfrac{\sqrt{\Delta_n^{3}}}{n} 
\partial_{\beta_{S_1}} \ell_{n} 
( 
\theta ), \quad   
\mathscr{B}_{S_1, n} ( \theta )  
= \tfrac{\Delta_n^{3}}{n} 
\int_0^1  
\partial_{\beta_{S_1}}^2 \ell_{n}  
\bigl( \truebeta_{S_1} + \lambda ( {\beta}_{S_1} - \truebeta_{S_1}),  
\theta^{- \beta_{S_1}} \bigr) \, d \lambda.  
\end{align*}
% 
% 
For the matrix $\mathscr{B}_{S_1, n}( 
\theta)$, we have the following result: 
%
\begin{lemma} \label{lemma:step1_B}
Assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:finite_moment}) hold. 
If \limit, then 
% \AB{we need to check the dimensions of matrices, vectors, etc. in the statement below. 
% $\rightarrow$ This should be alright: 
% \begin{gather*}
% \mathscr{B}_{S_1, n}  ( \hat{\beta}_{S_1,n}, \theta^{- \beta_{S_1}}  ) \in \mathbb{R}^{N_{\beta_{S_1}} \times N_{\beta_{S_1}}}, \, V_{S_1, 0} (x_S, \truebeta_{S_1}) \in \mathbb{R}^{N_{S_1}},  \\ 
% \partial_{\beta_{S_1}} \bigl( V_{S_1, 0} (x_S, \truebeta_{S_1}) \bigr)^\top \in \mathbb{R}^{N_{\beta_{S_1}} \times N_{{S_1}}}, \quad 
% \Lambda_{S_1 S_1} \bigl(x, (\truebeta_{S_1},\theta^{- \beta_{S_1}}) \bigr)  \in \mathbb{R}^{N_{S_1} \times N_{S_1}}, \\ 
% \partial_{\beta_{S_1}}^\top V_{S_1, 0} (x_S, \truebeta_{S_1}) \in \mathbb{R}^{N_{S_1} \times N_{\beta_{S_1}}}. 
% \end{gather*}
% }
%
\begin{align*}
  & \mathscr{B}_{S_1, n}
  ( \hat{\beta}_{S_1,n},
  \theta^{- \beta_{S_1}}  )  \\
 & \qquad \qquad \probconv
   \int 
   \partial_{\beta_{S_1}} \bigl( V_{S_1, 0} (x_S, \truebeta_{S_1}) \bigr)^\top 
   \Lambda_{S_1 S_1} \bigl(x, (\truebeta_{S_1},\theta^{- \beta_{S_1}}) \bigr)
   \partial_{\beta_{S_1}}^\top V_{S_1, 0} (x_S, \truebeta_{S_1}) \, \truedist (dx) ,  
\end{align*}
%
uniformly in $\theta^{- \beta_{S_1}} \equiv (\beta_{S_2}, \beta_R, \sigma)  \in \Theta_{\beta_{S_2}} \times \Theta_{\beta_{R}} \times \Theta_{\sigma}$. 
\end{lemma}
% 
\noindent We give the proof in Appendix \ref{appendix:pf_step1_B}. We now check that $\mathscr{A}_{S_1, n} ( \truebeta_{S_1}, \theta^{- \beta_{S_1}} ) \to 0$, as \limit, uniformly in $\theta^{- \beta_{S_1}} = (\beta_{S_2}, \beta_R, \sigma)$. 
We have: 
 % 
 \begin{align*}
 \mathscr{A}_{S_1, n} (  
 \truebeta_{S_1}, \theta^{- \beta_{S_1}} ) 
 =  \widetilde{\mathscr{A}}_{S_1, n} (  
 \truebeta_{S_1}, \theta^{- \beta_{S_1}} ) 
 + \tfrac{1}{n} \sum_{i = 1}^n 
R \big(\sqrt{\Delta_n}, \sample{i-1}, (\truebeta_{S_1}, \theta^{- \beta_{S_1}})\big), 
 \end{align*}
%
for $R \in \mathcal{S}$, where we have set, for $\theta = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma) \in \Theta$, 
%
\begin{align*}
 \widetilde{\mathscr{A}}_{S_1, n} (  
 \theta ) = 
 \tfrac{1}{n \sqrt{\Delta_n}} \sum_{i = 1}^n 
 \partial_{\beta_{S_1}} \bigl(  V_{S_1, 0} (
 \sample{S, i-1}, \beta_{S_1}) \bigr)^\top \, 
 \Phi (\sample{i-1}, \theta) \, 
 \eta_{S_2} (\sample{i-1}, \beta_{S_2})
 % \bigl( 
 %  V_{S_2, 0} (\sample{i-1}, \beta_{S_2}) 
 %  - V_{S_2, 0} (\sample{i-1}, \truebeta_{S_2}) 
 % \bigr),
\end{align*}
 % 
 with $\Phi : \mathbb{R}^N \times \Theta \to \mathbb{R}^{N_{S_1} \times N_{S_2}}$ defined as:
 % 
 \begin{align} \label{eq:phi_0}
    \Phi (x ,\theta) = \Lambda_{S_1 S_1} (x, \theta)  \, 
    \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) 
    + 2 \Lambda_{S_1 S_2} (x, \theta). %\quad (x , \theta) \in \mathbb{R}^N \times \Theta. 
 \end{align}
% 
From Lemmas \ref{lemma:ergodic_thm} and \ref{lemma:canonical_conv} in Appendix \ref{appendix:aux}, we immediately have that if \limit, then  
%
\begin{align*}
\tfrac{1}{n} \sum_{i = 1}^n R 
\bigl( \sqrt{\Delta_n}, \sample{i-1}, (\truebeta_{S_1},  \,  \theta^{- \beta_{S_1}}) \bigr) \probconv 0,   
\end{align*} 
% 
uniformly in $\theta^{- \beta_{S_1}}$. Furthermore, we have that, for any $\theta \in \Theta$, $\widetilde{\mathscr{A}}_{S_1, n}( \theta ) = 0$ with probability~1, from the following result: 
% 
\begin{lemma} \label{lemma:matrix}
Assume that condition \ref{assump:hypo}-II holds. We have that for any $(x, \theta) \in \mathbb{R}^N \times \Theta$: $$\Phi(x,\theta) = \mathbf{0}_{N_{S_1} \times N_{S_2}}.$$   
\end{lemma} 
% 
\noindent We give the proof in Appendix \ref{appendix:matrix}. Hence, we obtain $ \mathscr{A}_{S_1, n} ( \truebeta_{S_1}, \theta^{- \beta_{S_1}}) \probconv 0$, and now  convergence (\ref{eq:step1}) holds.  
% 
% 
\subsubsection{Step 2}
Making use of  convergence (\ref{eq:step1}), we obtain the following result whose proof is postponed to Appendix \ref{appendix:pf_step_2}.  
%
\begin{lemma} \label{lemm:step_2}
Assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:finite_moment}) hold. 
If \limit, then 
% 
\begin{align*}
 & \tfrac{\Delta_n}{n} \ell_{n} 
 ( \hat{\beta}_{S_1,n}, \beta_{S_2}, \beta_R, \sigma
)  \probconv 
 \int
  % \Bigl\{
  % \bigl(V_{S_2, 0} (x, \beta_{S_2}) - V_{S_2, 0} (x, \truebeta_{S_2}) \bigr)^\top 
  \eta_{S_2} (x, \beta_{S_2})^\top 
  \Lambda_{S_2 S_2} \bigl(x, (\truebeta_{S_1}, \beta_{S_2}, \sigma) \bigr) 
  \eta_{S_2} (x, \beta_{S_2}) 
  % \bigl(V_{S_2, 0} (x, \beta_{S_2}) - V_{S_2, 0} (x, \truebeta_{S_2}) \bigr) 
  \truedist (dx),
\end{align*}
% 
uniformly in $(\beta_{S_2}, \beta_R, \sigma) \in \Theta_{\beta_{S_2}} \times \Theta_{\beta_{R}} \times  \Theta_\sigma$. 
\end{lemma}
% 
\noindent This result leads to the consistency of $\hat{\beta}_{S_2, n}$ following an argument similar to the one used in \textbf{Step 1} to show consistency of $\hat{\beta}_{S_1, n}$. 
\\ 

To prove convergence (\ref{eq:step2}), we apply a Taylor expansion on the contrast function to get: 
% 
\begin{align*} 
\mathscr{A}_{S, n} (  
 \truebeta_{S_1}, 
 \truebeta_{S_2}, \hat{\beta}_{R, n}, \hat{\sigma}_{n} ) 
= \mathscr{B}_{S, n} ( 
 \hat{\theta}_{n} ) 
 \; \times 
 \begin{bmatrix}
 \tfrac{1}{\sqrt{\Delta_n^3}} (\hat{\beta}_{S_1, n} - \truebeta_{S_1}) \\[0.3cm]  
 \tfrac{1}{\sqrt{\Delta_n}} (\hat{\beta}_{S_2, n} - \truebeta_{S_2})
\end{bmatrix}, 
\end{align*}
% 
where we have set, for $\theta = (\beta_{S}, \beta_R, \sigma )\in \Theta$ with $\beta_S \equiv ( \beta_{S_1}, \beta_{S_2})$,  
% 
\begin{align*}
 & \mathscr{A}_{S, n} (  \theta )  
 =
 \begin{bmatrix}
  - \tfrac{\sqrt{\Delta_n^3}}{n} \partial_{\beta_{S_1}} \ell_{n} ( \theta ) \\[0.2cm]
  - \tfrac{\sqrt{\Delta_n}}{n} \partial_{\beta_{S_2}} \ell_{n} ( \theta )
 \end{bmatrix};  \\[0.2cm] 
 & \mathscr{B}_{S,n} \bigl(  \theta \bigr)  
 = \int_0^1 M_{\beta_S, n} \,  \partial_{\beta_S}^2 \ell_{n}
 \bigl( \truebeta_S + \lambda ( \beta_S - \truebeta_S ), \beta_R, \sigma \bigr) \, M_{\beta_S, n} d \lambda,
\end{align*}
%
and $M_{\beta_S, n} \in \mathbb{R}^{N_{\beta_S} \times N_{\beta_S}}$ is defined as: 
% 
\begin{align*}
M_{\beta_S, n} = \mrm{diag}
\Bigl( 
   \Bigl[ \, 
    \underbrace{ \sqrt{\tfrac{\Delta_n^3}{n}},
    \ldots, 
    \sqrt{\tfrac{\Delta_n^3}{n}}}_{N_{\beta_{S_1}}}, \ \ 
    \underbrace{\sqrt{\tfrac{\Delta_n}{n}},
    \ldots, 
    \sqrt{\tfrac{\Delta_n}{n}}}_{N_{\beta_{S_2}}} 
    \Bigr]^\top \Bigr). 
\end{align*}
% 
% \begin{align*}
% M_{\beta_S, n} =
% \begin{bmatrix}
%  \sqrt{\tfrac{\Delta_n^3}{n}} I_{N_{S_1} \times N_{S_1}} & \mbf{0}_{N_{S_1} \times N_{S_2}} \\[0.1cm] 
%  \mbf{0}_{N_{S_2} \times N_{S_1}} &  
%  \sqrt{\tfrac{\Delta_n}{n}}  I_{N_{S_2} \times N_{S_2}}
% \end{bmatrix}. 
% \end{align*}
% 
Convergence (\ref{eq:step2}) is immediately deduced from the following result.
%
%
% 
\begin{lemma} \label{lemma:step_2}
% 
Assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:finite_moment}) hold. If \limit, then 
%
\begin{align}
& \mathscr{A}_{S, n} ( \truebeta_{S}, \beta_R, \sigma)  
\probconv \mbf{0}_{N_{\beta_S}};  \label{eq:step2_A}   \\[0.2cm] 
& \mathscr{B}_{S, n} \bigl( \hat{\beta}_{S, n}, \beta_R, \sigma \bigr)  
\probconv 2 
\times 
\mrm{diag} \Bigl(\mathscr{B}_{S_1 S_1} ( \truebeta_S, \beta_R, \sigma ),
\, \mathscr{B}_{S_2 S_2} ( \truebeta_S, \beta_R, \sigma )
\Bigr),  
% \begin{bmatrix} 
% \mathscr{B}_{S_1 S_1 , n} \bigl( \truebeta_S, \beta_R, \sigma \bigr) & \mathbf{0}_{N_{\beta_{S_1}} \times N_{\beta_{S_2}}} \\[0.2cm] 
% \mathbf{0}_{N_{\beta_{S_2}} \times N_{\beta_{S_1}}} 
% & 
% \mathscr{B}_{S_2 S_2, n} \bigl( \truebeta_S, \beta_R, \sigma \bigr)  
% \end{bmatrix}, 
\label{eq:step2_B}  
% 
% 
% \int  
% \mathcal{D}^\top (x, \truebeta_S) \, 
% \Lambda_{SS} \bigl(x, (\truebeta_S, \sigma)  \bigr) \, 
% \mathcal{D} (x, \truebeta_S) \, \truedist (dx),  
\end{align}
%
uniformly in $(\beta_R, \sigma) \in \Theta_{\beta_R} \times \Theta_\sigma$, where $\truebeta_S \equiv (\truebeta_{S_1}, \truebeta_{S_2})$, $\hat{\beta}_{S, n} \equiv (\hat{\beta}_{S_1, n}, \hat{\beta}_{S_2, n})$ and we have set: 
% 
% \begin{align*}
%  \Lambda_{SS} \bigl( x, (\beta_S, \sigma)  \bigr) 
%  & \equiv 
%  \begin{bmatrix}
%  \Lambda_{S_1 S_1}  \bigl( x, (\beta_S, \sigma)  \bigr) 
%  & \Lambda_{S_1 S_2}  \bigl( x, (\beta_S, \sigma)  \bigr)  \\[0.2cm] 
%  \Lambda_{S_2 S_1} \bigl( x, (\beta_S, \sigma)  \bigr)  & 
%  \Lambda_{S_2 S_2} \bigl( x, (\beta_S, \sigma)  \bigr)  
%  \end{bmatrix}; 
%  \\[0.3cm] 
%  D (x, \beta_S) 
%  & \equiv  
%  \begin{bmatrix} 
%    \partial_{\beta_{S_1}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) & \tfrac{1}{2} 
%    \partial_{\beta_{S_2}}^\top \mathcal{L} V_{S_1, 0} (x, \beta_S) \\[0.2cm]  
%    \mbf{0}_{N_{S_2} \times N_{\beta_{S_1}}} & \partial_{\beta_{S_2}}^\top  V_{S_2, 0} (x, \beta_{S_2})
%  \end{bmatrix},
% \end{align*}
%
\begin{align*}
\mathscr{B}_{S_1 S_1} (\theta) = 
720 \int \partial_{\beta_{S_1}} \bigl( V_{S_1, 0} (x_S, \beta_{S_1 }) \bigr)^\top  
a_{S_1}^{-1} (x, \theta) 
\, \partial_{\beta_{S_1}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) \, \truedist (dx);  \\[0.1cm]
\mathscr{B}_{S_2 S_2} (\theta) = 
12 \int  \partial_{\beta_{S_2}} \bigl( V_{S_2, 0} (x, \beta_{S_2}) \bigr)^\top 
a_{S_2}^{-1} (x, \theta) 
\partial_{\beta_{S_2}}^\top V_{S_2, 0} (x, \beta_{S_2}) 
\, \truedist (dx),    
\end{align*} 
for $x \in \mathbb{R}^N, \, \theta = (\beta_S, \beta_R, \sigma) \in \Theta$, where $\beta_S = (\beta_{S_1}, \beta_{S_2})$.   
% $x \in \mathbb{R}^N, \, \sigma \in \Theta_\sigma$ and $\beta_S \equiv (\beta_{S_1}, \beta_{S_2}) \in \Theta_{\beta_S}$.
\end{lemma}
% 
% 
\noindent We give the proof in Appendix \ref{appendix:pf_step_4}. 

\subsubsection{Step 3}
Finally, we prove the consistency of estimators $(\hat{\beta}_{R, n}, \hat{\sigma}_{n})$. Working with the rates of convergence (\ref{eq:step1}) and (\ref{eq:step2}), we obtain the following result leading to the consistency of $\hat{\sigma}_{n}$: 
% 
\begin{lemma} \label{lemma:step3_1}
Assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:finite_moment}) hold. If \limit, then
%
\begin{align*}
\tfrac{1}{n} \, \ell_{n} \bigl( \hat{\beta}_{S, n},
\beta_R, \sigma \bigr) \nonumber 
\probconv 
\int \Bigl\{ 
\mrm{tr} \bigl( \Lambda (x, (\truebeta_S, \sigma)) 
\Sigma (x, (\truebeta_S, \truesigma))  \bigr)
+ \log \bigl| \Sigma (x,  (\truebeta_S, \sigma) ) \bigr| \Bigr\} \truedist (dx),  
\end{align*}
% 
uniformly in $(\beta_R, \sigma) \in \Theta_{\beta_R} \times \Theta_{\sigma}$. 
\end{lemma}
% 
\noindent We provide the proof in Appendix \ref{appendix:pf_step3_1}. 
To show the consistency of $\hat{\beta}_{R,n}$, we consider, for 
$\beta_R \in \Theta_{\beta_R}$, 
% 
\begin{align*} %\label{eq:L_beta}
\mathscr{L} ( \beta_R )  
:= \tfrac{1}{n \Delta_n}  \ell_{n} 
( \hat{\beta}_{S_1,n},  \hat{\beta}_{S_2, n},  \beta_R,  \hat{\sigma}_{n} ) 
- \tfrac{1}{n \Delta_n}  \ell_{n} 
( \hat{\beta}_{S_1, n},  \hat{\beta}_{S_2,  n},  \truebeta_R,  \hat{\sigma}_{n} ). 
\end{align*}
%
The consistency of estimator $\hat{\beta}_{R, n}$ is obtained via the following result whose proof is given in Appendix~\ref{appendix:pf_step3_2}.
% 
\begin{lemma} \label{lemma:step3_2}
Assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:finite_moment}) hold. If \limit, then
% 
\begin{align*}
\mathscr{L} (\beta_R) \probconv 
\int 
% \bigl( V_{R, 0} (x, \beta_R) - V_{R, 0} (x, \truebeta_R) \bigr)^\top 
\eta_{R} (x, \beta_R)^\top   
a_R^{-1} \bigl( x, \truesigma \bigr)
\, \eta_R (x, \beta_R) \, 
% \bigl( V_{R, 0} (x, \beta_R) - V_{R, 0} (x, \truebeta_R) \bigr)
\truedist (dx),
\end{align*} 
% 
% where we have defined: 
% % 
% \begin{align}
%  e (x, \beta_R) 
%  \equiv 
%  \begin{bmatrix}
%     \tfrac{1}{6} \mathcal{L}^2 V_{S_1, 0} (x, \trueparam) 
%     - \tfrac{1}{6} \mathcal{L}^2 V_{S_1, 0}
%      \bigl(x, (\beta_R, \truebeta_S, \truesigma) \bigr)  
%      \\[0.2cm]
%      \tfrac{1}{2} \mathcal{L} V_{S_2, 0} 
%      \bigl(x, (\truebeta_R, \truebeta_{S_2}, \truesigma) \bigr) 
%     - \tfrac{1}{2} \mathcal{L} V_{S_2, 0}
%      \bigl(x, (\beta_R, \truebeta_{S_2}, \truesigma) \bigr)  \\[0.2cm] 
%      V_{R, 0} (x, \truebeta_R) - V_{R, 0} (x, \beta_R)
%  \end{bmatrix},  
% \end{align} 
% 
uniformly in  $\beta_R \in \Theta_{\beta_R}$.
\end{lemma}
% 
\noindent The proof of consistency for the contrast estimator $\hat{\theta}_n$ is now complete.
% 
%
\subsection{Proof of Theorem \ref{thm:clt} -- Asymptotic Normality}  \label{sec:pf_asymptotic_normality}
%The proof is based on the standard argument used, e.g., in \cite{glot:20}, \cite{iguchi:22}, that is, 
We consider the Taylor expansion of the contrast function 
$\ell_{n} ( \theta )$: 
% 
\begin{align*}
 \mathscr{C}_{n} ( \trueparam ) 
 = \int_0^1 
  \mathscr{I}_{n} \bigl(  \trueparam  + \lambda 
  ( \hat{\theta}_{n} - \trueparam )  \bigr) d \lambda \times
  {M}_{n} ( \hat{\theta}_{n} -  \trueparam )
\end{align*} 
% 
where we have set, for $\theta \in \Theta$,  
% 
\begin{gather*}
  \mathscr{C}_{n} (\theta) =
  - M_n^{-1} \,  \partial_\theta 
  \ell_{n}  ( \theta ), 
 \quad  
  \mathscr{I}_{n} (\theta) =
   M_{n}^{-1} \, \partial^2_\theta \ell_{n}
   (\theta ) \, M_n^{-1}, \quad 
 M_n = \mathrm{diag} ({v}_{n}),
\end{gather*} 
%
% 
with the $N_\theta$-dimensional vector ${v}_{n}$ defined as: 
% 
\begin{align*}
    v_n = 
    \Bigl[
    \, 
    \underbrace{\sqrt{\tfrac{n}{\Delta_n^3}},
    \ldots, 
    \sqrt{\tfrac{n}{\Delta_n^3}}}_{N_{\beta_{S_1}}}, \ \ 
    \underbrace{\sqrt{\tfrac{n}{\Delta_n}},
    \ldots, 
    \sqrt{\tfrac{n}{\Delta_n}}}_{N_{\beta_{S_2}}},  \ \  
     \underbrace{
     \sqrt{n \Delta_n},
     \ldots, 
     \sqrt{n \Delta_n}}_{N_{\beta_R}}, \ \ 
     \underbrace{
     \sqrt{n},
     \ldots, 
     \sqrt{n}
     }_{N_{\sigma}} \, 
    \Bigr]^\top.  
\end{align*}
% 
% 
The asymptotic normality immediately holds from the following two results -- their proofs are shown in Appendices \ref{appendix:pf_slln} and \ref{appendix:pf_clt}. 
% 
\begin{lemma} \label{lemma:slln}
Assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:ident}) hold. 
If \limit, then 
% 
\begin{align*}
 \mathscr{I}_{n} \bigl(  \trueparam  + \lambda ( \hat{\theta}_{n} - \trueparam)  \bigr) \probconv 2 \Gamma (\trueparam), 
\end{align*} 
% 
uniformly in $\lambda \in [0,1]$, where the matrix $\Gamma(\trueparam)$ is defined as in (\ref{eq:precision_matrix}) in the main text.
\end{lemma}  
% 
\begin{lemma} \label{lemma:clt}
Assume that conditions \ref{assump:hypo}-II and (\ref{assump:coeff})--(\ref{assump:ident}) hold.
If \limit, with $\Delta_n = o (n^{-1/2})$, then 
% 
\begin{align*}
   \mathscr{C}_{n} (\trueparam) \distconv \mathscr{N} \bigl( \mathbf{0}_{N_\theta}, 4 \Gamma (\trueparam) \bigr). 
\end{align*} 
\end{lemma}
%
\noindent The proof of Theorem \ref{thm:clt} is now complete. 
%  
\section{Proof of Technical Results}  \label{appendix:pf_tech} 
% Typically, we write $\nu_{S_1, i} (\beta_{S_1}) \equiv \nu_{S_1, i} (1, \beta_{S_1})$ and $\nu_{S_2, i} (\beta_{S_2}) \equiv \nu_{S_2, i} (1, \beta_{S_2})$.  
\subsection{Proof of Lemma \ref{lemm:step1}}  
We have that $\textstyle \tfrac{\Delta_n^3}{n} \ell_n (\theta)  = \sum_{1 \le i \le 4} \mathscr{E}_{i} (\theta)$, $\theta = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma) \in \Theta$, where we have set:  
%
\begin{align*}
& 
\mathscr{E}_1 (\theta)  
= \tfrac{1}{n} \sum_{i = 1}^n 
\eta_{S_1} (X_{{i-1}}, \beta_{S_1})^\top  
% \bigl( V_{S_1, 0} (X_{S, t_{i-1}}, \truebeta_{S_1}) - V_{S_1, 0} (X_{S, t_{i-1}}, \beta_{S_1}) \bigr)^\top 
\Lambda_{S_1 S_1} (\sample{i-1}, \theta) 
\eta_{S_1} (X_{{i-1}}, \beta_{S_1}); \\  
 % & \qquad \qquad \qquad \qquad \times \bigl( V_{S_1, 0} (X_{S, t_{i-1}}, \truebeta_{S_1}) - V_{S_1, 0} (X_{S, t_{i-1}}, \beta_{S_1}) \bigr) ; \\ 
%
% 
& \mathscr{E}_2 (\theta)  
= \tfrac{1}{n} \sum_{i = 1}^n 
 \sum_{1 \le j_1, j_2 \le N} 
 R_1^{j_1 j_2} (\Delta_n^{q_1}, \sample{i-1}, \theta)
 \, 
 m_{i}^{j_1} (\Delta, \trueparam) 
 \, 
 m_{i}^{j_2} (\Delta, \trueparam); 
 \\ 
& \mathscr{E}_3 (\theta)  
 = \tfrac{1}{n} \sum_{i = 1}^n 
 \sum_{1 \le j \le N} 
 R_2^j (\Delta_n^{q_2}, \sample{i-1}, \theta)
 \, m_{i}^{j} (\Delta, \trueparam);  \\ 
& \mathscr{E}_4 (\theta) 
 =\tfrac{1}{n} \sum_{i = 1}^n 
  R_3 (\Delta_n^{q_3}, \sample{i-1}, \theta),
\end{align*}
%
for some functions $R_1^{j_1 j_2}, R_2^j, R_3 \in \mathcal{S}$ and constants $q_1, q_2, q_3 \ge 1$. From Lemmas \ref{lemma:ergodic_thm}, \ref{lemma:canonical_conv}, we immediately have that as \limit, 
% 
\begin{align*}
 \mathscr{E}_1 (\theta) 
 & \probconv 
\int 
% \bigl( V_{S_1, 0} (x_S, \truebeta_{S_1}) - V_{S_1, 0} (x_S, \beta_{S_1}) \bigr)^\top 
\eta_{S_1} (x , \beta_{S_1})^\top  
\Lambda_{S_1 S_1} (x, \theta) 
\eta_{S_1} (x , \beta_{S_1}) 
% \bigl( V_{S_1, 0} (x_S, \truebeta_{S_1}) - V_{S_1, 0} (x_S, \beta_{S_1}) \bigr)  \biggr\} 
\truedist (dx);  \\[0.2cm]
\mathscr{E}_k ( \theta ) & \probconv 0, \ \ 2 \le k \le 4, 
\end{align*}
% 
uniformly in $\theta = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma) \in \Theta$, and the proof is now complete. 
% 
% 
\label{appendix:pf_step1}

\subsection{Proof of Lemma \ref{lemma:step1_B}}  
\label{appendix:pf_step1_B}
% 
We define $\mathscr{F} : \Theta \to \mathbb{R}^{N_{\beta_{S_1}} \times N_{\beta_{S_1}}}$ as: 
% 
\begin{align*}
 \mathscr{F} (\theta ) =
 \tfrac{\Delta_n^3}{n}
  \partial_{\beta_{S_1}}^2 \ell_n 
  \left( \theta \right), \quad \theta \in \Theta.  
\end{align*} 
% 
% and consider the limit of $F \bigl( \truebeta_{S_1} + \lambda (\hat{\beta}_{S_1, p, n} - \truebeta_{S_1}), \beta_{S_2}, \beta_R, \sigma \bigr), \ (\beta_{S_2}, \beta_R, \sigma) \in \Theta_{\beta_{S_2}} \times \Theta_{\beta_R} \times \Theta_\sigma$, $\lambda \in [0,1]$.
$\mathscr{F} (\theta)$ can be expressed as $\textstyle{\mathscr{F}(\theta) = \sum_{1 \le k \le 6} \mathscr{F}_k (\theta)}$, where we have set, for $1 \le j_1, j_2 \le N_{\beta_{S_1}}$ with multi-index $\mathbf{j} = (j_1, j_2)$, 
% 
\begin{align*}
[\mathscr{F}_1 (\theta)]_{j_1 j_2} 
  & = 
 \tfrac{2}{n} \sum_{i = 1}^n 
 \bigl( \partial^{\beta_{S_1}}_{j_1} V_{S_1, 0} (\sample{S, i-1}, \beta_{S_1}) \bigr)^\top 
 \Lambda_{S_1 S_1} (\sample{i-1}, \theta) \partial^{\beta_{S_1}}_{j_2}  
 V_{S_1, 0} (\sample{S, i-1}, \beta_{S_1});  \\ 
 %
 [\mathscr{F}_2 (\theta)]_{j_1 j_2} 
 & =  \tfrac{1}{n}  
 \sum_{i=1}^n  
 \eta_{S_1} (\sample{i-1}, \beta_{S_1})^\top
 \partial_{\mathbf{j}}^{\beta_{S_1}}  
 \Lambda_{S_1 S_1}(\sample{i-1}, \theta) \, 
 \eta_{S_1 } (\sample{i-1}, \beta_{S_1});  
 \\ 
 %
 [\mathscr{F}_3 (\theta)]_{j_1 j_2} 
 & =
 - \tfrac{2}{n} \sum_{i = 1}^n 
 % \bigl( V_{S_1, 0} (\sample{i-1}, \truebeta_{S_1})
 %      - V_{S_1, 0} (\sample{i-1}, \beta_{S_1}) 
 % \bigr)^\top  \\
 \eta_{S_1} (\sample{i-1}, \beta_{S_1})^\top
\partial_{\mathbf{j}}^{\beta_{S_1}}   
\bigl\{  
\Lambda_{S_1 S_1} (\sample{i-1}, \theta) 
 V_{S_1, 0} (\sample{S, i-1}, \beta_{S_1}) 
\bigr\};  \\
 %
 %  
 [\mathscr{F}_4 (\theta)]_{j_1 j_2}
 & = \tfrac{1}{n} \sum_{i = 1}^n 
 \sum_{1 \le k_1, k_2 \le N} 
 R_{k_1 k_2}^{j_1 j_2} (\Delta_n^{q_1}, \sample{i-1}, \theta)
 \, 
 m_{i}^{k_1} (\Delta_n, \trueparam) 
 \, 
 m_{i}^{k_2} (\Delta_n, \trueparam); 
 \\ 
 [\mathscr{F}_5 (\theta)]_{j_1 j_2}
 & = \tfrac{1}{n} \sum_{i = 1}^n 
 \sum_{1 \le k \le N} R_{k}^{j_1 j_2} (\Delta_n^{q_2}, \sample{i-1}, \theta) \, m_{i}^{k} (\Delta_n, \trueparam); 
 \\ 
 [\mathscr{F}_6 (\theta)]_{j_1 j_2}
 & = \tfrac{1}{n} \sum_{i = 1}^n  R^{j_1 j_2} (\Delta_n^{q_3}, \sample{i-1}, \theta),
\end{align*} 
% 
for some functions $R^{j_1 j_2}_{k_1 k_2}, R^{j_1 j_2}_{k}, R^{j_1 j_2} \in \mathcal{S}$ and constants $q_1, q_2, q_3 > 0$. It follows from Lemmas \ref{lemma:ergodic_thm}, \ref{lemma:canonical_conv} and the consistency of estimator $\hat{\beta}_{S_1, n}$ that if \limit, 
% \AB{we should check the dimensions. $\rightarrow$ the right hand side should be scalar since
% \begin{align*}
% \partial^{\beta_{S_1}}_{j} V_{S_1, 0} (x_S, \truebeta_{S_1}) \in \mathbb{R}^{N_{S_1}}, 
% \quad \Lambda_{S_1 S_1} \bigl(x, (\truebeta_{S_1}, \beta_{S_2}, \sigma) \bigr) \in \mathbb{R}^{N_{S_1} \times N_{S_1}}. 
% \end{align*} 
% }
% 
\begin{align*}
 & \Bigl[ \mathscr{F}_1 \bigl( \truebeta_{S_1} - \lambda (\hat{\beta}_{S_1,n} - \truebeta_{S_1} ), \beta_{S_2}, \beta_R, \sigma  \bigr) \Bigr]_{j_1 j_2} \\
 & \qquad  \probconv 2 \int \bigl( \partial^{\beta_{S_1}}_{j_1} V_{S_1, 0} (x_S, \truebeta_{S_1}) \bigr)^\top 
 \Lambda_{S_1 S_1} \bigl(x, (\truebeta_{S_1}, \beta_{S_2}, \sigma) \bigr) \partial^{\beta_{S_1}}_{j_2}  
 V_{S_1, 0} (x_S, \truebeta_{S_1}) 
 \truedist (dx); \\[0.2cm] 
 %
 & \Bigl[ \mathscr{F}_k \bigl( \truebeta_{S_1} - \lambda (\hat{\beta}_{S_1,n} - \truebeta_{S_1} ), \beta_{S_2}, \beta_R, \sigma  \bigr) \Bigr]_{j_1 j_2}
 \probconv 0, \qquad  2 \le k \le 6, \\  
% 
\end{align*} 
%
uniformly in $(\beta_{S_2}, \beta_R, \sigma) \in \Theta_{\beta_{S_2}} \times \Theta_{\beta_{R}} \times\Theta_{\sigma}$ and $\lambda \in [0,1]$. The proof is now complete. 
% 
% 
\subsection{Proof of Lemma \ref{lemma:matrix}} \label{appendix:matrix}
 We write $\Sigma (x, \theta)$, $(x, \theta) \in \mathbb{R}^N \times \Theta$, in the form of the block matrix: 
%
\begin{align} \label{eq:Sigma}
\Sigma (x, \theta)
= 
\begin{bmatrix}
\Sigma_{S_1 S_1} (x, \theta)
& \widetilde{\Sigma} (x, \theta) \\[0.2cm]
\widetilde{\Sigma} (x, \theta)^\top
& \hat{\Sigma} (x, \theta)
\end{bmatrix},
\end{align}
% 
where we have set: 
%
\begin{align} \label{eq:Sigma_blocks}
\widetilde{\Sigma} (x, \theta) 
= \Bigl[  
    \Sigma_{S_1 S_2} (x, \theta), \, 
    \Sigma_{S_1 R} (x, \theta) 
    \Bigr],
\quad 
\hat{\Sigma} (x, \theta) =
\begin{bmatrix}
\Sigma_{S_2 S_2} (x, \theta) & \Sigma_{S_2 R} (x, \theta)  \\[0.2cm]
\Sigma_{R S_2} (x, \theta)  & \Sigma_{RR} (x, \theta)
\end{bmatrix}. 
\end{align}
%
Notice that under condition \ref{assump:hypo}--II, matrix $\hat{\Sigma} (x, \theta)$ is invertible for any $(x, \theta) \in 
\mathbb{R}^N \times \Theta$. We write the inverse of $\hat{\Sigma}(x, \theta)$ as: 
 % 
\begin{align*}
\hat{\Sigma}^{-1} (x, \theta)
=
\hat{\Lambda} (x, \theta) 
= 
\begin{bmatrix}
 \hat{\Lambda}_{S_2 S_2} (x, \theta)
 & \hat{\Lambda}_{S_2 R} (x, \theta) \\[0.2cm]
 \hat{\Lambda}_{R S_2} (x, \theta)
 & \hat{\Lambda}_{R R} (x, \theta)
\end{bmatrix}.  
\end{align*}
 %
 % where each block matrix is explicitly determined as:
 % % 
 % \begin{align} 
 % \end{align} 
 % % 
 Recall the notation for the inverse of $\Sigma (x, \theta)$ in (\ref{eq:inv_Sigma_2}). Using the inverse formula for a block matrix, we obtain: 
 % 
 \begin{align} \label{eq:inv_1}
     \Lambda_{S_1 S_2} (x, \theta)
     = - \Lambda_{S_1 S_1} (x, \theta) \Xi (x, \theta), 
 \end{align}
 % 
 where we have set 
%
\begin{align} \label{eq:Xi}
\Xi (x, \theta) =  \Sigma_{S_1 S_2} (x, \theta) \hat{\Lambda}_{S_2 S_2} (x, \theta) 
 + \Sigma_{S_1 R} (x, \theta) 
 \hat{\Lambda}_{R S_2} (x, \theta). 
\end{align}
% 
% and $\Sigma_{S_1 S_1}^{-1} (x, \theta)$ is the inverse of the matrix $\Sigma_{S_1 S_1} (x, \theta)$ that is positive definite for any $(x, \theta) \in \mathbb{R}^N \times \Theta$ under the condition (\ref{assump:hypo})-(ii). 
From the block matrix representation of $\Sigma (\Delta, x, \theta)$ in (\ref{eq:covariance_lg}), we obtain 
% 
\begin{gather*}
\Sigma_{S_1 S_2} (x, \theta) = \tfrac{3}{8} \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) \Sigma_{S_2 S_2} (x, \theta), 
\\[0.1cm]
\Sigma_{S_1 R} (x, \theta)  = \tfrac{1}{3} \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) \Sigma_{S_2 R} (x, \theta).  
\end{gather*}
% 
We then have    
 % 
 \begin{align}  
 \Xi (x, \theta)  
 & =  \tfrac{3}{8} \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) \Sigma_{S_2 S_2} (x, \theta) \hat{\Lambda}_{S_2 S_2} (x, \theta) \nonumber \\[0.2cm] 
 & \quad\quad \quad  + \tfrac{1}{3} \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) \Sigma_{S_2 R} (x, \theta) \hat{\Lambda}_{R S_2} (x, \theta) \nonumber \\[0.2cm]
 & = \tfrac{1}{24} \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) \Sigma_{S_2 S_2} (x, \theta) \hat{\Lambda}_{S_2 S_2} (x, \theta) 
 + \tfrac{1}{3} \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) \nonumber \\[0.2cm]  
 & = \tfrac{1}{2}  \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}).  \label{eq:inv_2}
 \end{align}
 % 
 In the above calculation we have used: 
 % 
 \begin{gather*}
{\Sigma}_{S_2 S_2} (x, \theta) 
\hat{\Lambda}_{S_2 S_2} (x, \theta) 
+ {\Sigma}_{S_2 R} (x, \theta) 
\hat{\Lambda}_{R S_2} (x, \theta)  
= I_{N_{S_2} \times N_{S_2}}; \\[0.1cm]
\hat{\Lambda}_{S_2 S_2} (x, \theta) 
= \bigl({\Sigma}_{S_2 S_2} (x, \theta) 
- \Sigma_{S_2 R} (x, \theta) 
\Sigma^{-1}_{RR} (x, \theta) 
\Sigma_{R S_2} (x, \theta)  \bigr)^{-1} = 4 {\Sigma}_{S_2 S_2}^{-1} (x, \theta), 
 \end{gather*}
 % 
 where  matrix $\Sigma_{S_2 S_2} (x, \theta)$ is invertible under  condition \ref{assump:hypo}--II as we have seen in the proof of Proposition \ref{prop:positive_definite} in Appendix \ref{appendix:positive_definite}.  Thus, from (\ref{eq:inv_1}) and (\ref{eq:inv_2}), we obtain 
 % 
 \begin{align} \label{eq:L_S1S2}
 \Lambda_{S_1 S_2} (x, \theta) 
 = - \tfrac{1}{2} \Lambda_{S_1 S_1} (x, \theta) \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) 
 \end{align} 
 % 
and the proof is now complete. 
%
% 
\subsection{Proof of Lemma \ref{lemm:step_2}} 
\label{appendix:pf_step_2}
We write $\theta^{- S_1}
\equiv \bigl( \beta_{S_2}, \beta_R, \sigma \bigr) \in \Theta_{\beta_{S_2}} \times \Theta_{\beta_R} \times \Theta_\sigma$. It holds that 
%
\begin{align*}
\tfrac{\Delta_n}{n} \, \ell_{n} 
 ( \hat{\beta}_{S_1, n}, \,  \theta^{-S_1} ) 
= \sum_{1 \le k \le 7} \mathscr{G}_k (\hat{\beta}_{S_1, n}, \,  \theta^{- S_1}), 
\end{align*} 
% 
where we have set, for $\theta \in \Theta$, 
% 
\begin{align*} 
  \mathscr{G}_1 (\theta) 
 & =  \tfrac{\Delta_n}{n} \sum_{i = 1}^n 
  \eta_{S_1, i-1} (\sqrt{\Delta_n^3}, \beta_{S_1})^\top 
  \Lambda_{S_1 S_1} (\sample{i-1}, \theta)  
  \eta_{S_1, i-1} (\sqrt{\Delta_n^3}, \beta_{S_1}); \\[0.2cm]
  %
  %   
\mathscr{G}_2  (\theta) 
& =  \tfrac{1}{n} \sum_{i = 1}^n 
\sum_{1 \le j \le N} 
R_j (\sqrt{\Delta_n},  \sample{i-1}, \theta ) 
\eta_{S_1, i-1}^{j} (\sqrt{\Delta_n^3}, \beta_{S_1});  \\[0.2cm]  
\mathscr{G}_3 (\theta) 
& = \tfrac{1}{n} \sum_{i = 1}^n 
\sum_{1 \le j_1, j_2 \le N} 
R_{j_1 j_2} ({\Delta_n},  \sample{i-1}, \theta ) \eta_{S_1, i-1}^{j_1} (\sqrt{\Delta_n^3}, \beta_{S_1})
m_{i}^{j_2} (\Delta_n, \trueparam );  \\[0.2cm] 
 %
 %
 \mathscr{G}_4 (\theta) 
& = \tfrac{1}{n} \sum_{i = 1}^n
% \bigl( {V_{S_2, 0} (\sample{i-1} ,\truebeta_{S_2}) - V_{S_2, 0} (\sample{i-1}, \beta_{S_2})}  \bigr)^\top
\eta_{S_2} (\sample{i-1}, \beta_{S_2})^\top
\Lambda_{S_2 S_2} (\sample{i-1}, \theta)  
\eta_{S_2} (\sample{i-1}, \beta_{S_2}); \\ 
%  {V_{S_2, 0} (\sample{i-1}, \truebeta_{S_2}) 
% - V_{S_2, 0} (\sample{i-1} , \beta_{S_2})} \biggr\};  
\mathscr{G}_5 (\theta) 
& = \tfrac{1}{n} \sum_{i = 1}^n 
\sum_{1 \le j \le N} 
\widetilde{R}_{j} ({\Delta_n},  \sample{i-1}, \theta ) \,
m_{i}^{j} (\Delta_n, \trueparam);  \\[0.2cm]  
 %
 % 
\mathscr{G}_6 (\theta) 
& = \tfrac{\Delta_n}{n} \sum_{i = 1}^n  
\Bigl\{ m_{i} (\Delta_n, \trueparam)^\top \Lambda (\sample{i-1}, \theta) 
\, m_{i} (\Delta_n, \trueparam )
+ \log \bigl| \Sigma (\sample{i-1}, \theta) \bigr| \Bigr\},  
% \widetilde{R} ({\Delta_n},  \sample{i-1}, \theta), 
 % 
\end{align*} 
%
for some functions $R_j, R_{j_1 j_2}, \, \widetilde{R}_{j} \in \mathcal{S}$. Note that $\eta_{S_1, i-1}^j (\sqrt{\Delta_n^3}, \hat{\beta}_{S_1, n})$ can be expressed as:  
%
\begin{align*}
 \eta_{S_1, i-1}^j (\sqrt{\Delta_n^3}, \hat{\beta}_{S_1, n})
 = \sum_{1 \le k \le N_{\beta_{S_1}}} \tfrac{\eta_{S_1}^{j, [k]} (\sample{S, i-1})}{
 | \beta_{S_1}^{\dagger, k} - \hat{\beta}_{S_1, n}^k | } 
 \times \Bigl| \tfrac{\beta_{S_1}^{\dagger, k} - \hat{\beta}_{S_1, n}^k}{\sqrt{\Delta_n^3}} \Bigr|,  
\end{align*}
%
where we have set: 
% 
\begin{align*} 
\eta_{S_1}^{j, [k]} (\sample{S, i-1}) \equiv V_{S_1, 0}^j ( \sample{S, i-1},  \bar{\beta}_{S_1, n}^{[k-1]}) 
- V_{S_1, 0}^j ( \sample{S, i-1},  \bar{\beta}_{S_1, n}^{[k]}),  \quad  1 \le k \le N_{\beta_{S_1}}, 
\end{align*}
% 
with the notation:
%
\begin{gather*}
\bar{\beta}_{S_1, n}^{[\ell]} = (\hat{\beta}_{S_1, n}^{1}, \ldots, \hat{\beta}_{S_1, n}^{\ell},
{\beta}_{S_1}^{\dagger, \ell+1}, \ldots,  {\beta}_{S_1}^{\dagger, N_{\beta_{S_1}}}), \quad 1 \le \ell \le N_{\beta_{S_1}} -1;\\ 
\bar{\beta}_{S_1, n}^{[0]} = \truebeta_{S_1}, \quad  
\bar{\beta}_{S_1, n}^{\, [N_{\beta_{S_1}}]} = \hat{\beta}_{S_1, n}.  
\end{gather*} 
% 
From  convergence (\ref{eq:step1}), Lemma \ref{lemma:ergodic_thm} and condition (\ref{assump:bd_deriv}) it follows that if \limit, then 
% 
\begin{align}
\begin{aligned}
& \tfrac{1}{n} \sum_{i = 1}^n f (\sample{i-1}, \theta) \eta_{S_1, i-1}^j (\sqrt{\Delta_n^3}, \hat{\beta}_{S_1, n})  \\
& \quad \probconv 
\sum_{1 \le k \le N_{\beta_{S_1}}}
\Bigl( \int f (x, \theta) \, \partial^{\beta_{S_1}}_k V_{S_1, 0} (x_S, \truebeta_{S_1}) 
\, \truedist (dx) \Bigr) \times 0 = 0,  \label{eq:eta_conv}
\end{aligned}
\end{align} 
% 
uniformly in $\theta \in \Theta$ for any $f: \mathbb{R}^N \times \Theta \to \mathbb{R}$ satisfying the same property in Lemma \ref{lemma:ergodic_thm}. 
% \begin{align}
%  \eta_{S_1, i-1}^j (\sqrt{\Delta_n^3}, \hat{\beta}_{S_1, n})
%  = \tfrac{V_{S_1, 0}^j (X_{i-1}, \truebeta_{S_1}) - V_{S_1, 0}^j (X_{i-1}, \hat{\beta}_{S_1, n})}{|\truebeta_{S_1} - \hat{\beta}_{S_1, n}|} \times 
%  \left| \tfrac{\truebeta_{S_1} - \hat{\beta}_{S_1, n}}{\sqrt{\Delta_n^3}} \right| 
%   \probconv 0. 
% \end{align}
%
% 
Thus, we obtain 
% 
\begin{align*} 
 \mathscr{G}_k ( \hat{\beta}_{S_1, n}, \theta^{-S_1} )  \probconv 0, \qquad   1 \le k \le 3, 
\end{align*}
% 
uniformly in $\theta^{-S_1}$. For the other terms, we immediately have from Lemmas \ref{lemma:ergodic_thm}, \ref{lemma:canonical_conv} that
% 
\begin{gather*} 
 \mathscr{G}_4 ( \hat{\beta}_{S_1, n}, \theta^{-S_1} ) \probconv 
 \int 
\eta_{S_2} (x, \beta_{S_2})^\top 
\Lambda_{S_2 S_2} \bigl( x, (\truebeta_{S_1}, \beta_{S_2}, \sigma) 
\bigr)  
\eta_{S_2} (x, \beta_{S_2}) 
\truedist (dx); \\[0.3cm]
 %  
 % 
 \mathscr{G}_k  ( \hat{\beta}_{S_1, n}, \theta^{-S_1}) 
  \probconv  0, \qquad  k = 5,6, 
\end{gather*} 
% 
as \limit, and the proof is now complete. 
% 
% 
\subsection{Proof of Lemma \ref{lemma:step_2}} 
\label{appendix:pf_step_4} 
%
\noindent 
(\textit{Proof of (\ref{eq:step2_A})}). 
Since the proof of $\tfrac{\sqrt{\Delta_n^3}}{n} \partial_{\beta_{S_1}} \ell_{n} (\truebeta_S, \beta_R, \sigma) \probconv 0$ is identical with that in Section \ref{sec:step1}, we will only show that, if \limit, then
% 
\begin{align} \label{eq:K}
  \mathscr{K} (\truebeta_S, \beta_R, \sigma)  
  := \tfrac{\sqrt{\Delta_n}}{n} \partial_{\beta_{S_2}} 
  \ell_{n} (\truebeta_S, \beta_R, \sigma) 
  \probconv 0, 
\end{align}
% 
uniformly in $(\beta_R, \sigma) \in \Theta_{\beta_R} \times \Theta_\sigma$. It holds that 
% 
$\textstyle \mathscr{K} (\truebeta_S, \beta_R, \sigma) =  \sum_{1 \le l \le 3} \mathscr{K}_l (\truebeta_S, \beta_R, \sigma)$,
% 
where we have set, for $\theta \in \Theta$, 
% 
\begin{align*}
  \mathscr{K}_1 ( \theta )  
  & = \tfrac{1}{n} 
  \sum_{i = 1}^n \sum_{1 \le j \le N} 
  R_j (1, \sample{i-1}, \theta) \, m_{i}^j (\Delta_n, \trueparam);
  \\[0.2cm]
  %
  %
  \mathscr{K}_2 ( \theta )  
  & = \tfrac{1}{n} \sum_{i = 1}^n \sum_{1 \le j_1, j_2 \le N} 
  R_{j_1 j_2} (\sqrt{\Delta_n}, \sample{i-1}, \theta) 
  \, m_{i}^{j_1} (\Delta_n, \trueparam) 
  \, m_{i}^{j_2} (\Delta_n, \trueparam);  \\[0.2cm]  
  % 
  \mathscr{K}_3 ( \theta )  
  & = \tfrac{1}{n} \sum_{i = 1}^n 
  R (\sqrt{\Delta_n}, \sample{i-1}, \theta), 
  % 
\end{align*}
% 
for $R, R_j, R_{j_1 j_2} \in \mathcal{S}$. From Lemmas \ref{lemma:ergodic_thm} and \ref{lemma:canonical_conv}, we immediately have that $ \mathscr{K}_i ( \truebeta_S, \beta_R, \sigma )  \probconv 0, \; 1 \le i \le 3$, as \limit, thus (\ref{eq:K}) holds. 
\\ 

\noindent  
(\textit{Proof of (\ref{eq:step2_B})}). 
We define 
% 
\begin{align*}
\mathcal{Q} (\theta) = M_{\beta_{S}, n} \, \partial^2_{\beta_S} 
\ell_n (\theta) \, M_{\beta_S, n} 
=
\begin{bmatrix} 
\mathcal{Q}_{S_1 S_1}  ( \theta )  
& \mathcal{Q}_{S_1 S_2}  ( \theta )  \\[0.2cm]
\mathcal{Q}_{S_2 S_1}  ( \theta )  
& \mathcal{Q}_{S_2 S_2}  ( \theta )  
\end{bmatrix}, 
\end{align*} 
%
where we have set: 
% 
\begin{align*}
\mathcal{Q}_{S_1 S_1}  ( \theta )  
& = \tfrac{\Delta_n^3}{n} \partial^2_{\beta_{S_1}}  \ell_n (\theta), \quad 
\mathcal{Q}_{S_1 S_2} ( \theta )  
= \tfrac{\Delta_n^2}{n} \partial_{\beta_{S_1}} \partial_{\beta_{S_2}}^\top \ell_n (\theta); \\[0.2cm] 
\mathcal{Q}_{S_2 S_1}  ( \theta ) 
& =  \mathcal{Q}_{S_1 S_2} ( \theta )^\top , \quad \mathcal{Q}_{S_2 S_2} ( \theta )  
= \tfrac{\Delta_n}{n} \partial^2_{\beta_{S_2}}  
\ell_n (\theta),  
\end{align*} 
% 
for $\theta = (\beta_S, \beta_R, \sigma) \in \Theta$. From the proof of Lemma \ref{lemma:step1_B} in Appendix \ref{appendix:pf_step1_B} and the consistency of the estimator $\hat{\beta}_{S,n}$, we have that if \limit, then 
% 
\begin{align} \label{eq:Q_S1S1}
 & \mathcal{Q}_{S_1 S_1}
\bigl( 
 \truebeta_S + \lambda ( \hat{\beta}_{S, n} - \truebeta_S), \beta_R, \sigma 
\bigr) \nonumber  \\[0.2cm] 
 & \qquad \probconv 2 \int 
\partial_{\beta_{S_1}} \bigl( V_{S_1, 0} (x, \truebeta_{S_1}) \bigr)^\top
\Lambda_{S_1 S_1} \bigl(x, (\truebeta_S, \sigma) \bigr) 
\partial_{\beta_{S_1}}^\top V_{S_1, 0} (x, \truebeta_{S_1}) \, \truedist (dx),
\end{align}
% 
uniformly in $(\beta_R, \sigma) \in \Theta_{\beta_R} \times \Theta_\sigma$ and $\lambda \in [0,1]$. 
% 
We will check the convergence of the two matrices $\mathcal{Q}_{S_1 S_2} (\theta)$ and $\mathcal{Q}_{S_2 S_2} (\theta)$. 
\\ 

\noindent We have 
$\textstyle{ 
\mathcal{Q}_{S_1 S_2} (\theta) 
=\sum_{1 \le k \le 4}  \mathcal{Q}_{S_1 S_2, k} (\theta) }$, where we set for $1 \le j_1 \le N_{\beta_{S_1}}$, $1 \le j_2 \le N_{\beta_{S_2}}$, 
%
\begin{align*}
% 
\bigl[ \mathcal{Q}_{S_1 S_2, 1} (\theta)  \bigr]_{j_1 j_2} 
& = \tfrac{2}{n} \sum_{i = 1}^n 
   \bigl( \partial^{\beta_{S_1}}_{j_1} 
   V_{S_1, 0} (\sample{S, i-1}, \beta_{S_1}) \bigr)^\top 
   \times 
   \biggl\{ 
   \Lambda_{S_1 S_2} \bigl(\sample{i-1}, \theta \bigr)
   \partial^{\beta_{S_2}}_{j_2} V_{S_2, 0} ( \sample{i-1}, \beta_{S_2})  \\[0.1cm]
& \qquad \qquad \qquad \qquad  
  + \tfrac{1}{2} 
   \Lambda_{S_1 S_1} \bigl(\sample{i-1}, \theta \bigr)
   \partial^{\beta_{S_2}}_{j_2} \mathcal{L} V_{S_1, 0} ( \sample{i-1}, \beta_{S}) 
   \biggr\}; \\[0.3cm] 
%
 \bigl[ \mathcal{Q}_{S_1 S_2, 2} (\theta) \bigr]_{j_1 j_2} 
 & = \tfrac{1}{n} \sum_{i = 1}^n 
  \sum_{\substack{1 \le k_1, k_2 \le N_{S_1}}} 
  R_{j_1 j_2}^{k_1 k_2} (1, \sample{i-1}, \theta) 
  \eta_{S_1, i-1}^{k_1} (\Delta_n, \beta_{S_1})
  \eta_{S_1}^{k_2} (\sample{i-1}, \beta_{S_1})
  % \\[0.2cm] 
  % & \qquad \qquad \qquad \qquad \qquad \qquad  \times 
  %  \Bigl( V_{S_1, 0}^{k_2} (\sample{i-1}, \truebeta_{S_1}) - V_{S_1, 0}^{k_2} (\sample{i-1}, \beta_{S_1}) \Bigr) 
  ; \\[0.2cm]  
% 
 \bigl[ \mathcal{Q}_{S_1 S_2, 3} (\theta) \bigr]_{j_1 j_2} 
 & = \tfrac{1}{n} \sum_{i = 1}^n 
  \sum_{\substack{1 \le k_1 \le N_{S_1} \\ 1 \le k_2 \le N_{S} }}  
  \widetilde{R}_{j_1 j_2}^{k_1 k_2} (1, \sample{i-1}, \theta) 
  \eta_{S_1}^{k_1} (\sample{i-1}, \beta_{S_1}) b^{k_2} (\sample{i-1}, \beta_S)
  % \bigl( V_{S_1, 0}^{k_1} (\sample{i-1}, \truebeta_{S_1}) - V_{S_1, 0}^{k_1} (\sample{i-1}, \beta_{S_1}) \bigr) 
  % \bigl( b^{k_2} (\sample{i-1}, \truebeta_S) 
  %      - b^{k_2} (\sample{i-1}, \beta_S) \bigr) 
  % \biggr\}
  ; \\[0.2cm] 
  % 
  % 
  \bigl[ \mathcal{Q}_{S_1 S_2, 4} (\theta) \bigr]_{j_1 j_2} 
 & = \tfrac{1}{n} \sum_{i = 1}^n \, 
 \biggl\{  \sum_{1 \le k_1, k_2 \le N} 
 \bar{R}_{j_1j_2}^{k_1 k_2} (\Delta_n, \sample{i-1}, \theta) m_{i}^{k_1} (\Delta_n, \trueparam) 
 m_{i}^{k_2} (\Delta_n, \trueparam) \\[0.2cm]
 & \qquad \qquad \quad
+ \sum_{1 \le k \le N} \bar{R}_{j_1 j_2}^k (\Delta_n, \sample{i-1}, \theta) 
 m_{i}^k (\Delta_n, \trueparam ) 
 + \bar{R} (\Delta_n, \sample{i-1}, \theta) 
 \biggr\}, 
 % 
\end{align*}
%
%
for some functions $R_{j_1 j_2}^{k_1 k_2}, \, \widetilde{R}_{j_1 j_2}^{k_1 k_2}, \, \bar{R}_{j_1 j_2}^{k_1 k_2}, \, \bar{R}_{j_1 j_2}^{k}, \,  \bar{R} \in \mathcal{S}$, and the function $b: \mathbb{R}^N \times \Theta_{\beta_S} \to \mathbb{R}^{N_S}$ is defined as:
% 
\begin{align*}
b (x, \beta_S ) =
\begin{bmatrix}  
\tfrac{1}{2} \mathcal{L} V_{S_1, 0} (x, \beta_{S})
- \tfrac{1}{2} \mathcal{L} V_{S_1, 0} (x, \truebeta_{S}) 
\\[0.2cm]
V_{S_2, 0} (x, \beta_{S_2}) - V_{S_2, 0} (x, \truebeta_{S_2}) 
\end{bmatrix}, %\quad  x \in \mathbb{R}^N, \, \beta_S  = (\beta_{S_1}, \beta_{S_2}) \in \Theta_{\beta_S}. 
\end{align*} 
%
Notice that for any $\theta \in \Theta$, 
$$[\mathcal{Q}_{S_1 S_2, 1}(\theta)]_{j_1 j_2} = 0, \, \quad 1 \le j_1 \le N_{\beta_{S_1}}, \quad 1 \le j_2 \le N_{\beta_{S_2}}  $$ because it follows from Lemma \ref{lemma:matrix} that
% 
\begin{align*}
& 
\Lambda_{S_1 S_2} \bigl(x, \theta \bigr)
\partial^{\beta_{S_2}}_{j_2} V_{S_2, 0} ( x, \beta_{S_2}) 
+ \tfrac{1}{2} 
\Lambda_{S_1 S_1} \bigl(x, \theta \bigr)
\partial^{\beta_{S_2}}_{j_2} \mathcal{L} V_{S_1, 0} (x, \beta_{S})  \\[0.1cm] 
& \qquad \qquad = \Phi (x, \theta) \, \partial^{\beta_{S_2}}_{j_2} V_{S_2, 0} ( x, \beta_{S_2})  = 0, 
\end{align*}
% 
for any $x \in \mathbb{R}^N$, $1 \le j_2 \le N_{\beta_{S_2}}$, 
where $\Phi (x, \theta)$ is defined as in (\ref{eq:phi_0}). 
From Lemmas \ref{lemma:ergodic_thm}, \ref{lemma:canonical_conv} and the consistency of $\hat{\beta}_{S, n}$ with 
convergence (\ref{eq:step1}), we obtain: 
% 
\begin{align*}
 % & \bigl[  Q_{S_1 S_2, 1} \bigl( \truebeta_S + \lambda (\hat{\beta}_{S, p, n} - \truebeta_S), \beta_R, \sigma \bigr) 
 % \bigr]_{j_1 j_2}  \probconv 2 \int_{\mathbb{R}^N}
 % \biggl\{  \partial^{\beta_{S_1}}_{j_1} V_{S_1, 0} (x, \truebeta_{S_1})^\top 
 % \\[0.2cm]
 % & \qquad  \times  \Bigl( 
 % \Sigma^{-1}_{S_1 S_2} \bigl(x, (\truebeta_S, \sigma) \bigr)
 %   \partial^{\beta_{S_2}}_{j_2} V_{S_2, 0} ( x, \truebeta_{S_2})  
 %  + \tfrac{1}{2} 
 %   \Sigma^{-1}_{S_1 S_1} \bigl(x, (\truebeta_S, \sigma) \bigr)
 %   \partial^{\beta_{S_2}}_{j_2} \mathcal{L} V_{S_1, 0} 
 %   (x, \truebeta_{S}) \Bigr) \biggr\} \truedist (dx);  \\[0.2cm] 
%
\bigl[  \mathcal{Q}_{S_1 S_2, k} \bigl( \truebeta_S + \lambda (\hat{\beta}_{S, n} - \truebeta_S), \beta_R, \sigma \bigr) 
 \bigr]_{j_1 j_2} \probconv 0, \qquad  2 \le k \le 4,
\end{align*} 
% 
uniformly in $(\beta_R, \sigma) \in \Theta_{\beta_R} \times \Theta_{\sigma}$ and $\lambda \in [0,1]$. 
% 

Finally, we consider the term $\mathcal{Q}_{S_2 S_2} (\theta)$. It holds that 
$\textstyle{\mathcal{Q}_{S_2 S_2} (\theta) = \sum_{1 \le k \le 4} \mathcal{Q}_{S_2 S_2, k}(\theta)}$, where we set, for $1 \le j_1, j_2 \le N_{\beta_{S_2}}$, 
% 
\begin{align*}    
 \bigl[ \mathcal{Q}_{S_2 S_2, 1}(\theta) \bigr]_{j_1 j_2} 
 &  = \tfrac{1}{n} \sum_{i = 1}^n \biggl\{
     \sum_{1 \le k  \le  N_{S_1}}   
     {R}_{j_1 j_2}^{k} (1, \sample{i-1}, \theta)) 
     \eta_{S_1, i-1}^{k} (\Delta_n, \beta_{S_1})  \\[0.2cm] 
     & \quad \quad 
     + \sum_{1 \le k_1, k_2  \le  N_{S_1}} R_{j_1 j_2}^{k_1 k_2} (1, \sample{i-1}, \theta)   
     \eta_{S_1, i-1}^{k_1} (\Delta_n, \beta_{S_1}) 
     \eta_{S_1, i-1}^{k_2} (\Delta_n, \beta_{S_1})
     % \tfrac{V_{S_1, 0}^{k_1} (\sample{i-1}, \truebeta_{S_1}) - V_{S_1, 0}^{k_1} (\sample{i-1}, \beta_{S_1})  }{\Delta_n}  
     % \tfrac{V_{S_1, 0}^{k_2} (\sample{i-1}, \truebeta_{S_1}) - V_{S_1, 0}^{k_2} (\sample{i-1}, \beta_{S_1})}{\Delta_n} \biggr\} 
     \\[0.3cm] 
     & \quad \quad  + \sum_{\substack{1 \le k_1 \le N_{S_1} \\ 1 \le k_2 \le N}} \widetilde{R}_{j_1 j_2}^{k_1 k_2} (1, \sample{i-1}, \theta) 
     \eta_{S_1, i-1}^{k_1} (\sqrt{\Delta_n}, \beta_{S_1}) 
     m_{i}^{k_2} (\Delta_n, \trueparam) \biggr\};
     % \tfrac{V_{S_1, 0}^{k_1} (\sample{i-1}, \truebeta_{S_1}) - V_{S_1, 0}^{k_1} (\sample{i-1}, \beta_{S_1})  }{\sqrt{\Delta_n}} 
     \\[0.2cm]
%
% 
\bigl[\mathcal{Q}_{S_2 S_2, 2} (\theta) \bigr]_{j_1 j_2} 
& = \tfrac{2}{n} \sum_{i = 1}^n  \Bigl\{
\tfrac{1}{4} \bigl( \partial_{j_1}^{\beta_{S_2}} \mathcal{L} V_{S_1, 0} (\sample{i-1}, \theta) 
\bigr)^\top 
\Lambda_{S_1 S_1} (\sample{i-1}, \theta) 
\partial_{j_2}^{\beta_{S_2}} \mathcal{L} V_{S_1, 0} (\sample{i-1}, \theta) \\
& \qquad \qquad + 
\tfrac{1}{2} 
\bigl( \partial_{j_1}^{\beta_{S_2}} \mathcal{L} V_{S_1, 0} (\sample{i-1}, \theta)  \bigr)^\top
\Lambda_{S_1 S_2} (\sample{i-1}, \theta) 
\, \partial_{j_2}^{\beta_{S_2}} V_{S_2, 0} (\sample{i-1}, \beta_{S_2}) \\[0.2cm] 
& \qquad \qquad + 
\tfrac{1}{2} 
\bigl( \partial_{j_1}^{\beta_{S_2}} V_{S_2, 0} (\sample{i-1}, \beta_{S_2})
\bigr)^\top
\Lambda_{S_2 S_1} (\sample{i-1}, \theta) 
\partial_{j_2}^{\beta_{S_2}} \,  
\mathcal{L} V_{S_1, 0} (\sample{i-1}, \theta) \\[0.2cm]  
& \qquad \qquad + 
\bigl( \partial^{\beta_{S_2}}_{j_1} V_{S_2, 0} (\sample{i-1}, \beta_{S_2}) \bigr)^\top \Lambda_{S_2 S_2} 
\bigl( \sample{i-1}, \theta \bigr) 
\, \partial^{\beta_{S_2}}_{j_2} V_{S_2, 0} (\sample{i-1}, \beta_{S_2}) 
\Bigr\};  \\[0.1cm] 
% 
\bigl[\mathcal{Q}_{S_2 S_2, 3} (\theta) \bigr]_{j_1 j_2} 
& = \tfrac{1}{n} \sum_{i = 1}^n
\biggl\{  \sum_{1 \le k_1, k_2 \le N_{S_2}} 
\overline{R}_{j_1 j_2}^{k_1 k_2} (1, \sample{i-1}, \theta) 
\eta_{S_2}^{k_1} (\sample{i-1}, \beta_{S_2}) 
\eta_{S_2}^{k_2} (\sample{i-1}, \beta_{S_2}) 
% \bigl( V_{S_2, 0}^{k_1} (\sample{i-1}, \truebeta_{S_2})  - V_{S_2, 0}^{k_1} (\sample{i-1}, \beta_{S_2}) \bigr)
% \bigl( V_{S_2, 0}^{k_2} (\sample{i-1}, \truebeta_{S_2})  - V_{S_2, 0}^{k_2} (\sample{i-1}, \beta_{S_2}) \bigr) 
\\[0.1cm] 
& \qquad \qquad + 
\sum_{1 \le k \le N_{S_2}} \widetilde{R}_{j_1 j_2}^{k} (1, \sample{i-1}, \theta))
\eta_{S_2}^{k} (\sample{i-1}, \beta_{S_2}) \biggr\};
% \bigl( V_{S_2, 0}^{k_1} (\sample{i-1}, \truebeta_{S_2})  - V_{S_2, 0}^{k_1} (\sample{i-1}, \beta_{S_2}) \bigr);
\\[0.3cm]
%
% 
\bigl[ \mathcal{Q}_{S_2 S_2, 4}(\theta) \bigr]_{j_1 j_2}
& = 
\tfrac{1}{n} \sum_{i = 1}^n \biggl\{ 
 \sum_{1 \le k_1, k_2 \le N} 
 \hat{R}_{j_1 j_2}^{k_1 k_2} (\Delta_n, \sample{i-1}, \theta) 
 m_{i}^{k_1} (\Delta_n, \trueparam)
 m_{i}^{k_2} (\Delta_n, \trueparam) 
 \\[0.2cm]
 & + \sum_{1 \le k \le N} \overline{R}_{j_1 j_2}^{k} (\sqrt{\Delta_n}, \sample{i-1}, \theta) m_{i}^k (\Delta_n, \trueparam) 
 + R_{j_1 j_2} (\sqrt{\Delta_n}, \sample{i-1}, \theta) \biggr\},  
\end{align*}
%
for some functions 
$R_{j_1 j_2}^k, \, {R}_{j_1 j_2}^{k_1 k_2}, \, \widetilde{R}_{j_1 j_2}^{k_1 k_2}, \, \overline{R}_{j_1 j_2, }^{k_1 k_2}, \, \widetilde{R}_{j_1 j_2}^k, \, \hat{R}_{j_1 j_2, }^{k_1 k_2}, \, \overline{R}_{j_1 j_2}^{k}, \, R_{j_1 j_2}  \in \mathcal{S}$. 
% 
Note that due to Lemma~\ref{lemma:matrix}, 
%
\begin{gather*}
[Q_{S_2 S_2, 2} (\theta)]_{j_1 j_2}  
= \tfrac{2}{n} \sum_{i = 1}^n 
\tfrac{1}{2} 
\bigl( \partial_{j_1}^{\beta_{S_2}} V_{S_2, 0} (\sample{i-1}, \beta_{S_2})
\bigr)^\top
\Lambda_{S_2 S_1} (\sample{i-1}, \theta) 
\, \partial_{j_2}^{\beta_{S_2}} \,  
\mathcal{L} V_{S_1, 0} (\sample{i-1}, \theta) \\[0.1cm]
+ \tfrac{2}{n} \sum_{i = 1}^n 
\bigl( \partial^{\beta_{S_2}}_{j_1} V_{S_2, 0} (\sample{i-1}, \beta_{S_2}) \bigr)^\top \Lambda_{S_2 S_2} \bigl( \sample{i-1}, \theta \bigr) \, \partial^{\beta_{S_2}}_{j_2} V_{S_2, 0} (\sample{i-1}, \beta_{S_2}).  
\end{gather*}
% 
We obtain from Lemma \ref{lemma:ergodic_thm}, (\ref{eq:eta_conv}) and consistency of $\hat{\beta}_{S, n}$ that if \limit, then 
% 
\begin{align} 
& \Bigl[\mathcal{Q}_{S_2 S_2, 2} \bigl( \truebeta_{S} + \lambda (\hat{\beta}_{S, n} - \truebeta_S), \beta_R, \sigma  \bigr) \Bigr]_{j_1 j_2}  \nonumber \\[0.2cm]
& \probconv 2 \int \tfrac{1}{2} 
\bigl( \partial_{j_1}^{\beta_{S_2}} V_{S_2, 0} (x, \truebeta_{S_2}) \bigr)^\top
 \Lambda_{S_2 S_1} \bigl(x, (\truebeta_S, \beta_R, \sigma) \bigr) \partial_{j_2}^{\beta_{S_2}} \mathcal{L} V_{S_1, 0} (x, (\truebeta_S, \beta_R, \sigma) )   \truedist (dx)  \nonumber \\[0.1cm]
& \qquad  \qquad   + 2 \int 
\bigl( 
\partial_{j_1}^{\beta_{S_2}} V_{S_2, 0} (x, \truebeta_{S_2}) \bigr)^\top
 \Lambda_{S_2 S_2} \bigl(x, (\truebeta_S, \beta_R, \sigma) \bigr) \partial_{j_2}^{\beta_{S_2}} V_{S_2, 0} (x, \truebeta_{S_2})   \truedist (dx); 
 \label{eq:Q_S2S2} \\[0.1cm] 
 & \Bigl[\mathcal{Q}_{S_2 S_2, k} \bigl( \truebeta_{S} + \lambda (\hat{\beta}_{S,  n} - \truebeta_S), \beta_R, \sigma  \bigr) \Bigr]_{j_1 j_2} 
 \probconv 0, \qquad  k = 1, 3, 4. \nonumber 
\end{align} 
% 
The proof is complete by applying the following result to (\ref{eq:Q_S1S1}) and (\ref{eq:Q_S2S2}). 
% 
\begin{lemma} \label{lemma:lambda}
Assume that condition \ref{assump:hypo}-II holds. We have  that, for any $(x, \theta) \in \mathbb{R}^N \times \Theta$: 
\begin{gather}
\Lambda_{S_1 S_1} ( x,  \theta ) = 720 \, a_{S_1}^{-1} (x, \theta); \label{eq:L_S1S1} \\[0.1cm] 
\Lambda_{S_2 S_2} (x, \theta) = 12 a_{S_2}^{-1} (x, \theta)
- \tfrac{1}{2} 
\Lambda_{S_2 S_1} (x, \theta) 
\partial_{x_{S_2}}^\top V_{S_1, 0} (x_S,  \beta_{S_1}).  \label{eq:L_S2S2} 
\end{gather}
\end{lemma}
% The proof of convergence (\ref{eq:step2_B}) is now complete. 
\subsubsection{Proof of Lemma \ref{lemma:lambda}}
%
\noindent
({\it{Proof of (\ref{eq:L_S1S1})}}). 
% 
First, we note that the matrices 
% 
$\Sigma_{S_1 S_1}(x, \theta), \,  \Sigma_{S_2 S_2}(x, \theta), \, \Sigma_{RR }(x, \theta)$
% 
are invertible for any $(x, \theta) \in \mathbb{R}^N \times \Theta$ under condition \ref{assump:hypo}-II as we have seen in proof of Proposition \ref{prop:positive_definite}. Due to the block expression of matrix $\Sigma (x, \theta)$ in (\ref{eq:Sigma}), we have: 
% 
\begin{align}
\Lambda_{S_1 S_1} (x, \theta) 
= \Bigl( \Sigma_{S_1 S_1} (x, \theta) - 
\widetilde{\Sigma} (x, \theta) \hat{\Lambda} (x, \theta) 
\widetilde{\Sigma} (x, \theta)^\top
\Bigr)^{-1}, 
\end{align}
% 
where $\hat{\Lambda} (x, \theta)$, the inverse of matrix $\hat{\Sigma}(x, \theta)$ given in (\ref{eq:Sigma_blocks}), has the following block expression:
% 
\begin{align}
    \hat{\Lambda} (x, \theta) 
    = 
    \begin{bmatrix}
        \hat{\Lambda}_{S_2 S_2} (x, \theta)  &  \hat{\Lambda}_{S_2 R}  (x, \theta)  \\[0.1cm] 
        \hat{\Lambda}_{S_2 R}  (x, \theta)^\top  &  \hat{\Lambda}_{R R}  (x, \theta)  
    \end{bmatrix}
\end{align}
% 
where we have set: 
% 
\begin{gather*}
\hat{\Lambda}_{S_2 S_2} (x, \theta) = 4 \Sigma_{S_2 S_2}^{-1} (x, \theta), 
\quad 
\hat{\Lambda}_{S_2 R} (x, \theta) = - 4 \Sigma^{-1}_{S_2 S_2} (x, \theta) \Sigma_{S_2 R} (x, \theta) \Sigma_{RR}^{-1} (x, \sigma);  \\[0.1cm] 
\hat{\Lambda}_{R R} (x, \theta) = \Sigma_{RR}^{-1} (x, \sigma) 
+ 4 \Sigma_{RR}^{-1} (x, \sigma) \Sigma_{R S_2} (x, \theta) \Sigma^{-1}_{S_2 S_2} (x, \theta) \Sigma_{S_2 R} (x, \theta) 
\Sigma_{RR}^{-1} (x, \sigma). 
\end{gather*}
% 
We then have: 
%  
\begin{align*}
\hat{\Lambda} (x, \theta) \widetilde{\Sigma} (x, \theta)^\top 
= 
\begin{bmatrix} 
\tfrac{1}{2} \bigl( \partial_{x_{S_2}}^\top V_{S_1, 0} ( x_S, \beta_{S_1} ) \bigr)^\top  \\[0.1cm]
- \tfrac{1}{12} 
\bigl( 
\partial_{x_{S_2}}^\top V_{S_1, 0} ( x_S, \beta_{S_1} )  
\partial_{x_{R}}^\top V_{S_2, 0} ( x, \beta_{S_2} )  
\bigr)^\top 
\end{bmatrix}, 
\end{align*}
% 
where we used (\ref{eq:inv_2}) for the upper block matrix, while the lower one is obtained via: 
% 
\begin{align*}
& \hat{\Lambda}_{S_2 R}  (x, \theta)^\top \Sigma_{S_2 S_1} (x, \theta) 
+ \hat{\Lambda}_{R R}  (x, \theta) \Sigma_{R S_1} (x, \theta)
\\[0.2cm]
& = - 4 \Sigma_{RR}^{-1} (x, \sigma)  \Sigma_{R S_2} (x, \theta) \Sigma^{-1}_{S_2 S_2} (x, \theta) \Sigma_{S_2 S_1} (x, \theta) + \Sigma_{RR}^{-1} (x, \sigma) \Sigma_{R S_1} (x, \theta)  \\[0.2cm] 
& \quad \quad \quad + 4 \Sigma_{RR}^{-1} (x, \sigma) \Sigma_{R S_2} (x, \theta) \Sigma^{-1}_{S_2 S_2} (x, \theta) \Sigma_{S_2 R} (x, \theta) 
\Sigma_{RR}^{-1} (x, \sigma) \Sigma_{R S_1} (x, \theta) 
\\[0.2cm]
& = - \tfrac{3}{4} \bigl( \partial_{x_{R}}^\top V_{S_2, 0} ( x, \beta_{S_2} ) \bigr)^\top 
\bigl( \partial_{x_{S_2}}^\top V_{S_1, 0} ( x_S, \beta_{S_1} ) \bigr)^\top 
\\ & \quad \quad \quad + \tfrac{1}{6} \bigl( \partial_{x_{R}}^\top V_{S_2, 0} ( x, \beta_{S_2} ) \bigr)^\top 
\bigl( \partial_{x_{S_2}}^\top V_{S_1, 0} ( x_S, \beta_{S_1} ) \bigr)^\top  \\[0.1cm]
& \quad \quad \quad \quad + \tfrac{1}{2} \bigl( \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2})  \bigr)^\top \Sigma^{-1}_{S_2 S_2} (x, \theta) \Sigma_{S_2 S_2} (x, \theta) \bigl( \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1}) \bigr )^\top \\[0.2cm] 
& = -\tfrac{1}{12} 
\bigl( 
\partial_{x_{S_2}}^\top V_{S_1, 0} ( x_S, \beta_{S_1} )
\partial_{x_{R}}^\top V_{S_2, 0} ( x, \beta_{S_2} ) 
\bigr)^\top.  
\end{align*}
% 
Thus, we obtain: 
% 
\begin{align*}
\Bigl( \Sigma_{S_1 S_1} (x, \theta) - 
\widetilde{\Sigma} (x, \theta) \hat{\Lambda} (x, \theta) 
\widetilde{\Sigma} (x, \theta)^\top \Bigr)^{-1}
= \bigl( \tfrac{1}{36} \Sigma_{S_1 S_1} (x, \theta) \bigr)^{-1} 
= 720 \, a_{S_1}^{-1} (x, \theta), 
\end{align*}
% 
and now the proof of (\ref{eq:L_S1S1}) is complete. 
\\

\noindent
({\it{Proof of (\ref{eq:L_S2S2})}}). From the block expression of the matrix $\Sigma (x, \theta)$ in (\ref{eq:Sigma}), we obtain:
%
\begin{align*}
\begin{bmatrix}
    \Lambda_{S_2 S_2} (x, \theta) &  \Lambda_{S_2 R} (x, \theta) 
    \\[0.1cm]
    \Lambda_{R S_2} (x, \theta) &  \Lambda_{R R} (x, \theta)   
\end{bmatrix}
 = \hat{\Lambda} (x, \theta)  
 + \hat{\Lambda} (x, \theta) \widetilde{\Sigma} (x, \theta)^\top
 \Lambda_{S_1 S_1} (x, \theta) 
 \widetilde{\Sigma} (x, \theta)  \hat{\Lambda} (x, \theta).  
\end{align*}
%
Thus, we have: 
% 
\begin{align*}
\Lambda_{S_2 S_2} (x, \theta)
& = \hat{\Lambda}_{S_2 S_2} (x, \theta) 
+ \Xi (x, \theta)^\top \Lambda_{S_1 S_1} (x, \theta) 
 \Xi (x, \theta)  \\[0.1cm]
& = 4 \Sigma^{-1}_{S_2 S_2} (x, \theta) 
+ \tfrac{1}{4} \bigl( \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta _{S_1}) \bigr)^\top  \Lambda_{S_1 S_1} (x, \theta) 
\partial_{x_{S_2}}^\top V_{S_1, 0} ( x_S, \beta _{S_1} )  \\[0.1cm]
& = 12 a_{S_2}^{-1} (x, \theta) - \tfrac{1}{2} \Lambda_{S_2 S_1} (x, \theta)  \partial_{x_{S_2}}^\top V_{S_1, 0} ( x_S, \beta _{S_1}), 
\end{align*}
% 
where $\Xi (x, \theta)$ is defined as in (\ref{eq:Xi}), and we made use of (\ref{eq:inv_2}), (\ref{eq:L_S1S2}). The proof of (\ref{eq:L_S2S2}) is now complete. 
% 
% 
\subsection{Proof of Lemma \ref{lemma:step3_1}} 
\label{appendix:pf_step3_1}
It holds that $\textstyle{\tfrac{1}{n} \ell_n (\theta) = \sum_{1 \le k \le 4} \mathscr{T}_k (\theta)}$, $\theta \in \Theta$, where we have set: 
%
\begin{align*}
\mathscr{T}_1 (\theta) 
& = \tfrac{1}{n} \sum_{i = 1}^n 
\biggl\{ \sum_{1 \le j_1, j_2 \le N_{S_1}} 
R_{j_1 j_2}^{S_1 S_1} (1, \sample{i-1}, \theta) 
\eta_{S_1, i-1}^{j_1} (\sqrt{\Delta_n^3}, \beta_{S_1})
\eta_{S_1, i-1}^{j_2} (\sqrt{\Delta_n^3}, \beta_{S_1})
\\[0.2cm]
& \qquad  + \sum_{\substack{ 1 \le j_1  \le N_{S_1} \\ 1 \le j_2 \le N_{S_2}} }  R_{j_1 j_2}^{S_1 S_2} (1, \sample{i-1}, \theta) 
\eta_{S_1, i-1}^{j_1} (\sqrt{\Delta_n^3}, \beta_{S_1})
\eta_{S_2, i-1}^{j_2} (\sqrt{\Delta_n}, \beta_{S_2}) \\[0.2cm] 
& \qquad + \sum_{1 \le j_1, j_2 \le N_{S_2}}  
R_{j_1 j_2}^{S_2S_2} (1, \sample{i-1}, \theta) 
\eta_{S_2, i-1}^{j_1} (\sqrt{\Delta_n}, \beta_{S_2})
\eta_{S_2, i-1}^{j_2} (\sqrt{\Delta_n}, \beta_{S_2}) 
\biggr\}; \\[0.2cm]
%   
\mathscr{T}_2 (\theta) 
& = \tfrac{1}{n} \sum_{i = 1}^n 
\biggl\{ 
\sum_{\substack{1 \le j_1 \le N_{S_1} \\ 1 \le j_2 \le N}} {R}_{j_1 j_2}^{S_1} (1, \sample{i-1}, \theta) 
\eta_{S_1, i-1}^{j_1} (\sqrt{\Delta_n^3}, \beta_{S_1} ) 
m_{i}^{j_2} (\Delta_n, \trueparam)  \\[0.2cm]
& \qquad + \sum_{\substack{1 \le j_1 \le N_{S_2} \\ 1 \le j_2 \le N}}{R}_{j_1 j_2}^{S_2} (1, \sample{i-1}, \theta) 
\eta_{S_2, i-1}^{j_1} (\sqrt{\Delta_n}, \beta_{S_2} ) m_{i}^{j_2} (\Delta_n, \trueparam)  \\[0.2cm]
& \qquad + \sum_{1 \le j \le N_{S_1}} {R}_{j}^{S_1} (\sqrt{\Delta_n}, \sample{i-1}, \theta) \eta_{S_1, i-1}^{j} (\sqrt{\Delta_n^3}, \beta_{S_1}) \\[0.2cm]
& \qquad + \sum_{1 \le j \le N_{S_2}}
{R}_{j}^{S_2} (\sqrt{\Delta_n}, \sample{i-1}, \theta) \eta_{S_2, i-1}^{j} (\sqrt{\Delta_n}, \beta_{S_2}) \biggr\}; \\[0.2cm]  
% 
\mathscr{T}_3 (\theta)  
 & = \tfrac{1}{n} \sum_{i = 1}^n
 \biggl\{ 
m_{i}(\Delta_n, \trueparam)^\top
\Lambda \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr) 
m_{i} (\Delta_n, \trueparam) + \log \bigl| \Sigma \bigl( \sample{i-1}, (\beta_S, \sigma) \bigr) \bigr| 
\biggr\};  \\[0.2cm]
%
% 
\mathscr{T}_4 (\theta)
 & = \tfrac{1}{n} \sum_{i = 1}^n \biggl\{ 
 \sum_{1 \le j \le N} R_j (\sqrt{\Delta_n}, \sample{i-1}, \theta) m_{i}^{j} (\Delta_n, \trueparam) + R (\Delta_n, \sample{i-1}, \theta)  \biggr\}, 
\end{align*} 
%
where $R_{j_1 j_2}^{S_1 S_1}, \, R_{j_1 j_2}^{S_1 S_2}, \, R_{j_1 j_2}^{S_2 S_2}, \, R_{j_1 j_2}^{S_1}, \, R_{j_1 j_2}^{S_2}, \, R_{j}^{S_1}, \, R_{j}^{S_2}, \, R_{j_1 j_2} \, R_{j}, R  \in \mathcal{S}$. 
% 
From Lemmas \ref{lemma:ergodic_thm}, ~\ref{lemma:canonical_conv} and the convergence (\ref{eq:step1}) and (\ref{eq:step2}), we have: 
% 
\begin{align*} 
& \mathscr{T}_k \bigl( \hat{\beta}_{S, n}, \beta_R, \sigma \bigr) \probconv 0, \qquad  k = 1,2,4; \\[0.1cm]
& \mathscr{T}_3 \bigl( \hat{\beta}_{S, n}, \beta_R, \sigma \bigr) \probconv 
\int \Bigl\{ 
\mrm{tr} \bigl( 
\Lambda \bigl( x, (\truebeta_S, \sigma)  \bigr)
\Sigma \bigl( x, (\truebeta_S, \truesigma) \bigr) \bigr)
+ \log 
\bigl| \Sigma \bigl(x, (\truebeta_S, \sigma) \bigr) 
\bigr| 
\Bigr\} \, \truedist (dx), 
\end{align*} 
% 
as \limit, uniformly in $(\beta_R, \sigma) \in \Theta_{\beta_R} \times \Theta_\sigma$. The proof is now complete. 
% 
% 
\subsection{Proof of Lemma \ref{lemma:step3_2}} 
\label{appendix:pf_step3_2}
% The contrast function is expressed as: 
% % 
% % 
% \begin{align}
% \ell_{n} (\theta) 
% = \ell_{n, 1}^{(\mrm{II})}  (\theta) 
%  + \ell_{p, n, 2}^{(\mrm{II})}  (\theta),  
% \end{align}
% % 
% % 
% where we have set: 
% % 
% \begin{align*}
% \ell_{p, n, 1}^{(\mrm{II})} (\theta) 
%  & \equiv \sum_{i = 1}^n \bigl( m_i (\Delta_n, \theta)  \bigr)^\top \Sigma^{-1}_{i-1} (\beta_S, \sigma) m_i (\Delta_n, \theta)
%  + \log \bigl| \Sigma_{i-1} (\beta_S, \sigma) \bigr|;  \\[0.2cm]
% % % 
% \ell_{p, n, 2}^{(\mrm{II})} (\theta)  
%  & \equiv \sum_{i = 1}^n \sum_{j = 1}^{J_p} \Delta_n^j 
%    \biggl\{ f_{j}^{\, (\mathrm{II})} (X_{t_{j-1}}, \theta)  \\ 
%    & \quad \quad + m_p^{\, (\mathrm{II})} (\Delta_n, X_{t_{i-1}}, X_{t_{i}}, \theta)^\top 
%    G_{j}^{\, (\mathrm{II})} (X_{t_{i-1}}, \theta) 
%    m_p^{\, (\mathrm{II})} (\Delta_n, X_{t_{i-1}}, X_{t_{i}}, \theta) \biggr\},    
% \end{align*}
% % 
% for $\theta \in \Theta$. 
The matrix-valued function $\Sigma (x, \theta)$ and its inverse $\Lambda (x, \theta)$ depend on $\beta_S = (\beta_{S_1}, \beta_{S_2}) \in \Theta_{\beta_S}$ and $\sigma \in \Theta_\sigma$ but not on $\beta_R \in \Theta_{\beta_R}$ in terms of the parameter $\theta \in \Theta$. We then define 
%
\begin{align*}
\mathscr{L} (\theta) = \tfrac{1}{n \Delta_n} \ell_n (\theta) 
 -  \tfrac{1}{n \Delta_n} \ell_n \bigl( \beta_S, \truebeta_R, \sigma \bigr), \quad \theta = (\beta_S, \beta_R, \sigma) \in \Theta,  
\end{align*} 
%
and $\mathscr{L}(\theta)$ is expressed as 
$\textstyle{ \mathscr{L}(\theta) = \sum_{1 \le k \le 3} \mathscr{U}_k (\theta)}$, where we have set: 
% 
\begin{align*}
 \mathscr{U}_{1} (\theta) 
 & = \tfrac{1}{n \Delta_n} 
 \sum_{i = 1}^n 
 \biggl\{ 
  \bigl( m_{i} (\Delta_n, (\beta_S, \truebeta_R, \sigma) ) 
  - m_{i} (\Delta_n, \trueparam) \bigr)^\top 
 \Lambda (\sample{i-1}, (\beta_S, \sigma)) \\[0.1cm]
 & \qquad \qquad \qquad \qquad \times \bigl( 
   m_{i} (\Delta_n, \theta) - m_{i} (\Delta_n, (\beta_S, \truebeta_R, \sigma) )  \bigr) 
 \biggr\}; \\[0.2cm]
 \mathscr{U}_2 (\theta) 
 & = \tfrac{1}{n \Delta_n} \sum_{i = 1}^n \biggl\{ 
 \bigl( m_{i} (\Delta_n, \theta) - m_{i} (\Delta_n, \trueparam) \bigr)^\top  
 \Lambda (\sample{i-1}, (\beta_S, \sigma)) \\[0.1cm]
 & \qquad \qquad \qquad \qquad \times
   \bigl( m_{i} (\Delta_n, \theta) 
    - m_{i} (\Delta_n, (\beta_S, \truebeta_R, \sigma) )
   \bigr)   
 \biggr\}; \\[0.2cm]
 %
 % 
\mathscr{U}_3 (\theta) 
& = \tfrac{2}{n \Delta_n} \sum_{i = 1}^n
m_{i} (\Delta_n, \trueparam)^\top
\Lambda (\sample{i-1}, (\beta_S, \sigma))
\bigl( m_{i} (\Delta_n, \theta) - m_{i} (\Delta_n, (\beta_S, \truebeta_R, \sigma) ) \bigr). 
 % U_4 (\theta) 
 % & \equiv \tfrac{1}{n \Delta_n} 
 %  \biggl\{
 %   \ell_{p, n, 2}^{(\mrm{II})} (\theta) 
 %  - \ell_{p, n, 2}^{(\mrm{II})} (\beta_S, \truebeta_R, \sigma) 
 %  \biggr\}. 
\end{align*} 
%
We will derive the limit of the terms $\mathscr{U}_k (\theta)$, $1 \le k \le 3$ evaluated at 
$ \theta = ( \hat{\beta}_{S, n}, \,  \beta_R, \, 
 \hat{\sigma}_{n} )$ by utilising the following result whose proof is given in Section \ref{appendix:pf_matrix_2}: 
% 
\begin{lemma} \label{lemma:matrix_2}
Assume that condition \ref{assump:hypo}-II holds. For any $\theta = (\beta_S, \beta_R, \sigma) \in \Theta$ and $1 \le i \le N$, we have that
%
\begin{align*}
 \Lambda (\sample{i-1},  (\beta_S, \sigma))
 \bigl( m_i (\Delta_n, \theta) - m_i (\Delta_n, (\beta_S, \truebeta_R, \sigma)) \bigr)
 = \sqrt{\Delta_n}
 \begin{bmatrix}
 \mathbf{0}_{N_{S_1}} \\[0.1cm]
 \mathbf{0}_{N_{S_2}} \\[0.1cm]
 b_R
 \end{bmatrix}
 + R (\sqrt{\Delta_n^3}, \sample{i-1}, \theta),
\end{align*}
%
for $\mathbb{R}^N$-valued function $R$ with $R^j \in \mathcal{S}, \, 1 \le j \le N$, where the $N_R$-dimensional vector $b_R$ is specified as: 
%
\begin{align*}
  b_R \equiv  a_R^{-1} (\sample{i-1}, \sigma) 
  \, \eta_R (\sample{i-1}, \beta_R). 
  % \Bigl( V_{R, 0} ( \sample{i-1}, \beta_R) 
  % - V_{R, 0} ( \sample{i-1}, \truebeta_R) \Bigr).
\end{align*}
% 
\end{lemma}
% 
\noindent We first consider the term $\mathscr{U}_1 (\theta)$. Making use of Lemma \ref{lemma:matrix_2}, we have
% 
% 
\begin{align} 
 \begin{aligned}  \label{eq:U_1}
 \mathscr{U}_{1} (\theta) 
 & =  \tfrac{1}{n} \sum_{i = 1}^n 
\biggl\{  \sum_{1 \le j \le N_{S_1}} 
\eta_{S_1, i-1}^j (\sqrt{\Delta_n^3}, \beta_{S_1}) 
R^j_{S_1} (\sqrt{\Delta_n}, \sample{i-1}, \theta) \\[0.2cm]
& \qquad + \sum_{1 \le j \le N_{S_2}} 
\eta_{S_2, i-1}^j (\sqrt{\Delta_n}, \beta_{S_2}) 
R^j_{S_2} (\sqrt{\Delta_n}, \sample{i-1}, \theta)
+ R (\sqrt{\Delta_n}, \sample{i-1}, \theta) \biggr\},
 \end{aligned} 
\end{align} 
% 
where $R_{S_1}^j, \, R_{S_2}^j, \, R \in \mathcal{S}$. From Lemma \ref{lemma:ergodic_thm} and the limits (\ref{eq:step1}), (\ref{eq:step2}), we obtain that if \limit, 
%
\begin{align*} 
\mathscr{U}_1 \big( \hat{\beta}_{S, n}, \, 
\beta_R, \, 
\hat{\sigma}_{n} \bigr) \probconv 0, 
\end{align*}
%
uniformly in $\beta_R \in \Theta_{\beta_R}$. For term $\mathscr{U}_2 (\theta)$, again Lemma \ref{lemma:matrix_2} yields
% 
\begin{align*}
\mathscr{U}_2 (\theta) 
 & = \tfrac{1}{n} \sum_{i = 1}^n  
 \eta_{R} (\sample{i-1}, \beta_R)^\top 
 a_R^{-1} (\sample{i-1}, \sigma)
 \eta_{R} (\sample{i-1}, \beta_R) 
 % \biggl\{ 
 % \bigl( V_{R, 0} (\sample{i-1}, \beta_R) 
 % -  V_{R, 0} (\sample{i-1}, \truebeta_R) \bigr)^\top 
 % \Sigma_{RR, i-1}^{-1} (\beta_S, \sigma) \\[0.2cm]
 % & \qquad \qquad  
 % \times \bigl( V_{R, 0} (\sample{i-1}, \beta_R)  
 %     - V_{R, 0} (\sample{i-1}, \truebeta_R) \bigr)
 % \biggr\}  
 + \widetilde{\mathscr{U}}_2 (\theta),
\end{align*} 
% 
where $\widetilde{\mathscr{U}}_2 (\theta)$ is given in the form of the right-hand-side of formula (\ref{eq:U_1}). We then obtain 
%
\begin{align*}
\mathscr{U}_1 \big( \hat{\beta}_{S,  n}, \, 
\beta_R, \, 
\hat{\sigma}_{n} \bigr)  
\probconv \int \eta_{R} (x, \beta_R)^\top \, 
a_R^{-1} (x,  \truesigma ) \, 
\eta_{R} (x, \beta_R)  \truedist(dx), 
% \bigl( V_{R,0} (x, \beta_R) - V_{R,0} (x, \truebeta_R)  \bigr)^\top
% \Sigma_{RR}^{-1} \bigl( x, (\truebeta_S, \truesigma) \bigr) 
% \bigl( V_{R,0} (x, \beta_R) - V_{R,0} (x, \truebeta_R)  \bigr) 
% \truedist(dx), 
\end{align*}
%
as \limit, uniformly in $\beta_R \in \Theta_{\beta_R}$. 
%
For the third term $\mathscr{U}_3 (\theta)$, it follows from Lemma \ref{lemma:matrix_2} that 
%
\begin{align*}
 \mathscr{U}_3 (\theta) = 
 & \tfrac{1}{n \sqrt{\Delta_n}} 
  \sum_{i = 1}^n \sum_{N_S +1  \le j \le N} 
  m_{i}^j (\Delta_n, \trueparam) R^j (1, \sample{i-1}, \theta) \\[0.2cm]
 & \quad +  \tfrac{1}{n} 
  \sum_{i = 1}^n \sum_{1 \le j \le N} 
   m_{i}^j (\Delta_n, \trueparam) 
  \widetilde{R}^j (\sqrt{\Delta_n}, \sample{i-1}, \theta), 
\end{align*}
%
where $R^j, \, \widetilde{R}^j \in \mathcal{S}$. From Lemma \ref{lemma:canonical_conv}, we have that, if \limit, then 
%
\begin{align*}
 \mathscr{U}_3 (\hat{\beta}_{S,  n}, \beta_R, \hat{\sigma}_{ n})
 \probconv 0, 
\end{align*} 
%
uniformly in  $\beta_R \in \Theta_{\beta_R}$. The proof is now complete. 
% Finally, we consider the term $U_4 (\theta)$. We have for $\theta = (\beta_S, \beta_R, \sigma ) \in \Theta$,
% %
% \begin{align*}
%     \tfrac{1}{n \Delta_n} \ell_{p, n, 2}^{(\mrm{II})}  (\theta) 
%     = \sum_{1 \le k \le 2} Y_k (\theta), 
% \end{align*}
% % 
% where we have defined: 
% % 
% \begin{align*}
%    & Y_1 (\theta) 
%    \equiv \tfrac{1}{n} \sum_{i = 1}^n  \bigg\{ f_1^{(\mrm{II})} (\sample{i-1}, \theta)
%    + \bigl( m_i (\Delta_n, \trueparam) \bigr)^\top G_1^{(\mrm{II})} (\sample{i-1}, \theta) m_i (\Delta_n, \trueparam)
%    \biggr\};  \\[0.2cm]
%    %
%    % 
%    & Y_2 (\theta)
%     \equiv 
%    \tfrac{1}{n}  \sum_{i = 1}^n 
%    \bigg\{ R (\sqrt{\Delta_n}, \sample{i-1}, \theta)
%    +  \sum_{1 \le j \le N} R^j (1, \sample{i-1}, \theta) \bigl( m_i^{j} (\Delta_n, \trueparam) 
%    - m_i^{j} (\Delta_n, \theta) \bigr)  \\[0.2cm] 
%    &  +  \sum_{1 \le j_1, j_2 \le N} 
%    R_{j_1 j_2} (1, \sample{i-1}, \theta) 
%    \bigl( m_i^{j_1} (\Delta_n, \trueparam) - m_i^{j_1} (\Delta_n, \theta) \bigr)
%    \bigl( m_i^{j_2} (\Delta_n, \trueparam) - m_i^{j_2} (\Delta_n, \theta) \bigr) \biggr\},  
% \end{align*}
% %
% where $R, R^j, R_{j_1 j_2} \in \mathcal{S}$. 
% Now, we recall 
% % 
% \begin{align*}
% f_1^{(\mrm{II})} (\sample{i-1}, \theta) 
% & \equiv \mrm{tr} \Bigl( \Sigma^{-1}_{i-1} (\beta_S, \sigma) \hat{\Sigma}_1 (\sample{i-1}, \theta) \Bigr); \\[0.2cm]
% G_1^{(\mrm{II})} (\sample{i-1}, \theta) 
% & \equiv - \Sigma^{-1}_{i-1}(\beta_S, \sigma) \hat{\Sigma}_1 (\sample{i-1}, \theta) \Sigma^{-1}_{i-1} (\beta_S, \sigma), 
% \end{align*} 
% % 
% where $\hat{\Sigma}_1 (\sample{i-1}, \theta)$ is defined via the formula (\ref{eq:Sigma_expansion}) and typically depends only on $\beta_S, \sigma$ (not on $\beta_R$). From the limits (\ref{eq:step1}), (\ref{eq:step2}) and Lemma \ref{lemma:ergodic_thm}, \ref{lemma:canonical_conv}, we  obtain that if \limit, then
% % 
% \begin{align*}
%  & Y_k (\hat{\beta}_{S,p,n}, \beta_R, \hat{\sigma}_{p, n}) 
%   \probconv 0, \ \ k  = 1,2, 
% \end{align*}
% % 
% uniformly in $\beta_R \in \Theta_{\beta_R}$, where we used
% % 
% \begin{align*}
% & \tfrac{1}{n} \sum_{i = 1}^n \bigl( m_i (\Delta_n, \trueparam) \bigr)^\top G_1^{(\mrm{II})} 
% \bigl(\sample{i-1}, (\hat{\beta}_{S, p, n}, \beta_R, \hat{\sigma}_{p,n} ) \bigr) m_i (\Delta_n, \trueparam) \\[0.2cm]
% & \probconv - \int_{\mathbb{R}^N} \mrm{tr} \Bigl(  
%   \Sigma^{-1} \bigl(x, (\truebeta_S, \truesigma)  \bigr)
%   \hat{\Sigma}_1 \bigl(x, 
%   (\truebeta_S, \beta_R, \truesigma) \bigr) 
%   \Sigma^{-1} \bigl(x, (\truebeta_S, \truesigma) \bigr)   
%   \Sigma \bigl(x, (\truebeta_S, \truesigma) \bigr) \Bigr) \truedist (dx) \\[0.2cm]
%   & \qquad = - \int_{\mathbb{R}^N} \mrm{tr} \Bigl(  
%   \Sigma^{-1} \bigl(x, (\truebeta_S, \truesigma)  \bigr)
%   \hat{\Sigma}_1 \bigl(x, 
%   (\truebeta_S, \beta_R, \truesigma) \bigr) 
%   \Bigr) \truedist (dx).   
% \end{align*}
% % 
% Thus, we obtain $U_4 (\hat{\beta}_{S, p, n}, \beta_R, \sigma_{p, n}) \probconv 0$, as \limit, uniformly in $\beta_R \in \Theta_{\beta_R}$, and the proof is now complete. 
% 
\subsection{Proof of Lemma \ref{lemma:matrix_2}} 
\label{appendix:pf_matrix_2}
% 
We have 
% 
\begin{align*} 
& m_{i} (\Delta_n, \theta) - m_{i} (\Delta_n, (\beta_S, \truebeta_R, \sigma)) \nonumber    \\[0.2cm] 
& =  
\begin{bmatrix}
\tfrac{\sqrt{\Delta_n}}{6} \, \partial_{x_{S_2}}^\top  
V_{S_1, 0} (X_{S, {i-1}}, \beta_{S_1})  
\partial_{x_R}^\top  V_{S_2,0} (\sample{i-1}, \beta_{S_2})
\, \eta_{R} (\sample{i-1}, \beta_R) \\[0.2cm]
\tfrac{\sqrt{\Delta_n}}{2} \, \partial_{x_R}^\top V_{S_2,0} (x, \beta_{S_2})  
\, \eta_{R} (\sample{i-1}, \beta_R)
\\[0.2cm]
\sqrt{\Delta_n} \,  \eta_{R} (\sample{i-1}, \beta_R) 
\end{bmatrix} + R (\sqrt{\Delta_n^3}, \sample{i-1}, \theta)
\nonumber \\[0.3cm] 
& = 
\sqrt{\Delta_n} \begin{bmatrix}
\Sigma_{S_1 R} \bigl(\sample{i-1}, \theta \bigr) 
\\[0.2cm]
\Sigma_{S_2 R} \bigl( \sample{i-1}, \theta \bigr)   \\[0.2cm] 
\Sigma_{R R} \bigl( \sample{i-1}, \sigma \bigr) 
\end{bmatrix} 
a_{R}^{-1} (\sample{i-1}, \sigma) \eta_{R} (\sample{i-1}, \beta_R) 
+ R (\sqrt{\Delta_n^3}, \sample{i-1}, \theta),
\end{align*}
%
for $\mathbb{R}^N$-valued function $R$ with $R^j \in \mathcal{S}, \, 1 \le j \le N$, where we used for $(x, \theta) \in \mathbb{R}^N \times \Theta$, 
% 
\begin{align*} 
  \mathcal{L} V_{S_2, 0} (x, \theta) 
  & = \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) V_{R, 0} (x, \beta_R) + v_{S_2} (x, \beta_S, \sigma); %\label{eq:LV_S2} 
  \\[0.2cm]
  \mathcal{L}^2 V_{S_1, 0} (x, \theta)  
  & = 
  \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1})  \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) V_{R, 0} (x, \beta_R) + v_{S_1} (x, \beta_S, \sigma), %\label{eq:LLV_S1} 
\end{align*}
% 
with some functions $v_{S_2} : \mathbb{R}^N \times \Theta_{\beta_S} \times \Theta_\sigma \to \mathbb{R}^{N_{S_2}}$ 
and $v_{S_1} : \mathbb{R}^N \times \Theta_{\beta_S} \times \Theta_\sigma \to \mathbb{R}^{N_{S_1}}$ that are independent of $\beta_R \in \Theta_{\beta_R}$.   
%
Thus, it follows that 
% 
\begin{align*}
 & \Lambda \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr)
 \bigl(  m_{i} (\Delta_n, \theta) - m_{i} (\Delta_n, (\beta_S, \truebeta_R, \sigma))  \bigr) \nonumber \\[0.2cm] 
 & \qquad\qquad = \sqrt{\Delta_n}
 \begin{bmatrix}
 b_{S_1} \\
 b_{S_2} \\
 b_{R}
 \end{bmatrix}
 a_R^{-1} (\sample{i-1}, \sigma) \eta_R (\sample{i-1}, \beta_R)
 + \widetilde{R} (\sqrt{\Delta_n^3}, \sample{i-1}, \theta), 
\end{align*}
% 
for $\widetilde{R}^j \in \mathcal{S}, \, 1 \le j \le N$,  where we have set: 
% 
\begin{align*}
\begin{bmatrix}
 b_{S_1} \\[0.1cm]
 b_{S_2} \\[0.1cm]
 b_{R}
 \end{bmatrix}
 =  
 \Lambda \big(\sample{i-1}, \theta \bigr) \cdot 
 \begin{bmatrix}
     \Sigma_{S_1 R} \bigl( \sample{i-1},  \theta \bigr) \\[0.2cm] 
     \Sigma_{S_2 R} \bigl( \sample{i-1}, \theta \bigr) \\[0.2cm]  
     \Sigma_{R R} \bigl( \sample{i-1},  \sigma \bigr) 
 \end{bmatrix}
 = 
 \begin{bmatrix}
 \mathbf{0}_{N_{S_1} \times N_{R}} \\[0.1cm] 
 \mathbf{0}_{N_{S_2} \times N_{R}} \\[0.1cm] 
 I_{N_{R} \times N_{R}} 
 \end{bmatrix}, 
\end{align*}
% 
since it holds $\Lambda (x, \theta) \Sigma (x, \theta) = I_{N \times N}$ for each $(x, \theta) \in \mathbb{R}^N \times \Theta$. 
% 
% \begin{align*}
%     b_{S_1} 
%     & \equiv 
%     \Lambda_{S_1 S_1} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr)
%     \Sigma_{S_1 R} \bigl( \sample{i-1},  (\beta_S, \sigma) \bigr)
%     + \Lambda_{S_1 S_2} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr)
%     \Sigma_{S_2 R} \bigl( \sample{i-1}, (\beta_S, \sigma) \bigr) \\[0.2cm]
%     & \quad + \Lambda_{S_1 R} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr)
%     \Sigma_{R R} \bigl( \sample{i-1}, (\beta_S, \sigma) \bigr); 
%     \\[0.2cm]
%     %
%      b_{S_2} 
%     & \equiv  
%     \Lambda_{S_2 S_1} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr) 
%     \Sigma_{S_1 R} \bigl( \sample{i-1}, (\beta_S, \sigma) \bigr)
%     + \Lambda_{S_2 S_2} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr) 
%     \Sigma_{S_2 R} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr) \\[0.2cm]
%     & \quad + \Lambda_{S_2 R} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr) \Sigma_{R R} \bigl( \sample{i-1},  (\beta_S, \sigma) \bigr);  \\[0.2cm]
%     b_{R} 
%     & \equiv  
%     \Lambda_{R S_1} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr) 
%     \Sigma_{S_1 R} \bigl(\sample{i-1},  (\beta_S, \sigma) \bigr)
%     + \Lambda_{R S_2} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr) 
%     \Sigma_{S_2 R, i-1} \bigl( \sample{i-1},  (\beta_S, \sigma) \bigr) 
%     \\[0.2cm]
%     & \quad + \Lambda_{R R} \bigl(\sample{i-1}, (\beta_S, \sigma) \bigr) 
%     \Sigma_{R R} \bigl( \sample{i-1},  (\beta_S, \sigma) \bigr).  
% \end{align*}
% % 
% Since it holds $\Lambda (\beta_S, \sigma) \Sigma (\beta_S, \sigma) = I_{N \times N}$, we have 
% %
% \begin{align*}
%     b_{S_1} = \mathbf{0}_{N_{S_1} \times N_{R}}, \quad  b_{S_2} = \mathbf{0}_{N_{S_2} \times N_{R}}, \quad  
%     b_{R} = I_{N_{R} \times N_{R}},  
% \end{align*}
% 
The proof is now complete. 
% 
% 
%
% 
\subsection{Proof of Lemma \ref{lemma:slln}}
\label{appendix:pf_slln} 
% 
%
Recall $N_\theta =  N_{\beta} + N_{\sigma}$, where $N_\beta = N_{\beta_S} + N_{\beta_R}$ with $N_{\beta_S} = N_{\beta_{S_1}} + N_{\beta_{S_2}}$. In this section we make use of the notation $\beta = (\beta_S, \beta_R) \in \Theta_{\beta_S} \times \Theta_{\beta_R}$ with $\beta_S = (\beta_{S_1}, \beta_{S_2})$. We note again that the matrices $\Sigma (x, \theta)$ and $\Lambda (x, \theta)$ do not depend on the parameter $\beta_R$. Since we have seen the convergences of the matrices $\mathcal{Q}_{S_1 S_1} (\theta), \, \mathcal{Q}_{S_1 S_2} (\theta), \, \mathcal{Q}_{S_2 S_2} (\theta)$ evaluated at $\theta = \trueparam + \lambda ( \hat{\theta}_{n} - \trueparam), \, \lambda \in [0,1]$ in Appendix \ref{appendix:pf_step_4}, we prove that as \limit, the following convergences hold uniformly in $\lambda \in [0,1]$:
%
\begin{enumerate}
\item[(a)] For $1 \le i \le N_{\beta}$, $N_{\beta_S} + 1  \le j \le N_{\beta}$, 
% 
\begin{align}
& \bigl[ \mathscr{I}_{n} \bigl( \trueparam + \lambda (\hat{\theta}_{ n} - \trueparam) \bigr) \bigr]_{ij}
\label{eq:slln_conv1}  \\[0.2cm] 
& \probconv 
\begin{cases}
2 \int \bigl( \partial^{\beta}_i V_{R, 0} (x, \truebeta_R) \bigr)^\top\, 
a_R^{-1} (x, \truesigma) \, 
\partial^{\beta}_j V_{R, 0} (x, \truebeta_R) \,  \truedist (dx), 
  & N_{\beta_S} +1 \le i, j \le N_{\theta};  \\[0.3cm]
0, &  (\mrm{otherwise}). 
\end{cases} 
\nonumber 
\end{align}
%
\item[(b)] For $1 \le i \le N_{\beta}$, $N_{\beta} + 1  \le j \le N_{\theta}$, 
% 
\begin{align}
\bigl[ \mathscr{I}_{n} \bigl( \trueparam + \lambda (\hat{\theta}_{n} - \trueparam) \bigr) \bigr]_{ij}
\probconv 0.   \label{eq:slln_conv2}
\end{align}
 % 
 \item[(c)] For $N_{\beta} + 1  \le i, j \le N_{\theta}$, 
 % 
 \begin{align}
  \bigl[ \mathscr{I}_{n} \bigl( \trueparam + \lambda (\hat{\theta}_{n} - \trueparam) \bigr) \bigr]_{ij}   %\nonumber 
%\\[0.1cm]
\probconv  \int  \mathrm{tr}  \bigl(
\partial^{\sigma}_i \Sigma  (x, \trueparam ) \, 
\Lambda  (x, \trueparam )  
\partial^{\sigma}_j \Sigma  (x, \trueparam ) \,
\Lambda  (x, \trueparam ) \bigr)  
\truedist (dx),   \label{eq:slln_conv3}  
\end{align}
\end{enumerate}
% 
% 
\subsubsection{Proof of (\ref{eq:slln_conv1})} 
%
We consider the following three cases separately: 
% 
% 
\begin{enumerate}
\item[(a1)] $1 \le i \le N_{\beta_{S_1}}, \; N_{\beta_S} + 1 \le j \le N_\beta$;
% 
\item[(a2)] $N_{\beta_{S_1}} + 1 \le i \le N_{\beta_S}, \; N_{\beta_S} + 1 \le j \le N_\beta$;
% 
\item[(a3)] $N_{\beta_S} + 1 \le i, j \le N_{\beta}$. 
\end{enumerate}
% 
% 
In  case (a1), we have for $\theta = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma ) \in \Theta$,   
% 
\begin{align}  
\bigl[ \mathscr{I}_{n} \bigl( \theta \bigr) \bigr]_{ij} 
& = \tfrac{1}{n} \sum_{m=1}^n H_{ij} (\sample{m-1}, \theta) 
+ \tfrac{1}{n} \sum_{m = 1}^n R (\Delta_n, \sample{m-1}, \theta) \label{eq:case_a1}  \\ 
& + \tfrac{1}{n}  \sum_{m = 1}^n \sum_{ 1 \le k \le N_{S_1} }
R_{k} (1, \sample{m-1}, \theta) 
\bigl( V_{S_1, 0}^{k} (\sample{m-1}, \truebeta_{S_1}) - V_{S_1, 0}^{k} (\sample{m-1}, \beta_{S_1} ) \bigr),  \nonumber 
\end{align}
%
% 
for some $R_{k}, R \in \mathcal{S}, \, 1 \le k \le N_{S_1}$, where we have defined $H_{ij} (x, \theta), \, (x, \theta) \in \mathbb{R}^N \times \Theta$ as: 
% 
\begin{align*}
H_{ij} (x, \theta) 
& =
\tfrac{1}{3} 
\bigl( \partial^{\beta}_i V_{S_1, 0} (x_{S_1}, \beta_{S_1}) \bigr)^\top 
\Lambda_{S_1 S_1} ( x, \theta ) 
\, \partial^{\beta}_{j}  \mathcal{L}^2 V_{S_1, 0} (x, \theta) 
\\[0.2cm]
& \quad + 
\bigl( \partial^{\beta}_{i} V_{S_1, 0} (x_{S_1}, \beta_{S_1}) \bigr)^\top \Lambda_{S_1 S_2} (x, \theta) 
\, \partial^{\beta}_{j} \mathcal{L} V_{S_2, 0} (x, \theta)  \\[0.2cm] 
& \quad  + 2
\bigl( \partial^{\beta}_i V_{S_1, 0} (x_{S_1}, \beta_{S_1}) \bigr)^\top 
\Lambda_{S_1 R} (x, \theta) 
\, \partial^{\beta}_{j} V_{R, 0} (x, \beta_R). 
\end{align*}
%  
Noticing that for $N_{\beta_S} +  1  \le j  \le N_{\beta}$,  
% 
\begin{gather*}
\partial^\beta_j \mathcal{L} V_{S_2, 0} (x, \theta) 
= \partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) \partial_j^\beta V_{R, 0} (x, \beta_R);  \\[0.1cm]
\partial^\beta_j \mathcal{L}^2 V_{S_1, 0} (x, \theta) 
= \partial_{x_{S_2}}^\top V_{S_1, 0} (x_S, \beta_{S_1})  
\partial_{x_R}^\top V_{S_2, 0} (x, \beta_{S_2}) 
\partial_j^\beta V_{R, 0} (x, \beta_R),
\end{gather*}
% 
we have $H_{ij} (x, \theta) = 0$ for any $(x, \theta) \in \mathbb{R}^N \times \Theta$ since 
% 
\begin{align*} 
H_{ij} (x, \theta)  
= 2 \bigl( \partial^{\beta}_{i} V_{S_1, 0} (x, \beta_{S_1}) \bigr)^\top  
\widetilde{H}_{ij} (x, \theta) \, 
\partial^{\beta}_j V_{R, 0} (x, \beta_R),   
\end{align*}
% 
where 
%
\begin{align*}
\widetilde{H}_{ij} (x, \theta)
& = 
\bigl\{ 
\Lambda_{S_1 S_1} (x, \theta) \Sigma_{S_1 R} (x, \theta) 
+ \Lambda_{S_1 S_2} (x, \theta) \Sigma_{S_2 R} (x, \theta)  
+ \Lambda_{S_1 R} (x, \theta ) \Sigma_{R R} (x, \sigma) 
\bigr\} a_R^{-1} (x, \sigma) \\ 
& = \mathbf{0}_{N_{S_1} \times N_{R}}. 
\end{align*}
% 
% 
Thus, due to the consistency of the estimator and Lemma \ref{lemma:ergodic_thm}, we immediately obtain from (\ref{eq:case_a1}) that if \limit, then 
%
\begin{align*}  
[\mathscr{I}_n \bigl( \trueparam + \lambda (\hat{\theta}_n - \trueparam)  \bigr) ]_{ij} \probconv 0, 
\end{align*} 
uniformly in $\lambda \in [0,1]$ for $1 \le i \le N_{\beta_{S_1}}, \; N_{\beta_S} + 1 \le j \le N_\beta$.  
% 
\\ 

Subsequently, we consider the case (a2). We have 
% 
\begin{align*}
\bigl[ \mathscr{I}_{n} \bigl( \theta \bigr) \bigr]_{ij} 
 & = \tfrac{1}{n} \sum_{m=1}^n \widetilde{H}_{ij, 1} (\sample{m-1}, \theta)
 + \tfrac{1}{n} \sum_{m=1}^n \widetilde{H}_{ij, 2} (\sample{m-1}, \theta) 
 \nonumber \\[0.1cm] 
 % 
 & \quad + \tfrac{1}{n} \sum_{m = 1}^n \sum_{1 \le k \le N_{S_1}} 
 R_k  (1, \sample{m-1}, \theta) \eta_{S_1, m-1}^k (\Delta_n, \theta)  \\[0.1cm] 
 & \quad  + \tfrac{1}{n} \sum_{m = 1}^n \sum_{N_{S_1} + 1 \le k \le N_{S}} 
{R}_k  (1, \sample{m-1}, \theta) \bigl( 
V_{S, 0}^{k} ( \sample{m-1}, \truebeta_{S} ) 
-  V_{S, 0}^{k} ( \sample{m-1}, \beta_{S} ) 
\bigr) \\[0.1cm]
& \quad +  \tfrac{1}{n} \sum_{m = 1}^n R (\Delta_n, \sample{m-1}, \theta),    
\end{align*}
% 
for some $R_k, R \in \mathcal{S}, \, 1 \le k \le N_S$, where we have defined $\widetilde{H}_{ij, k} (x, \theta)$, for $(x, \theta) \in \mathbb{R}^N \times \Theta$, $k = 1,2$, as: 
% 
\begin{align*}
 \widetilde{H}_{ij, 1} (x, \theta) 
& \equiv  \tfrac{1}{6} \bigl( \partial^{\beta}_i \mathcal{L} V_{S_1, 0} (x, \theta) \bigr)^\top 
\Lambda_{S_1 S_1} (x, \theta) \, 
\partial^{\beta}_j \mathcal{L}^2 V_{S_1, 0} (x, \theta) \\[0.2cm]  
& \quad + \tfrac{1}{2} \bigl( \partial^{\beta}_i \mathcal{L} V_{S_1, 0} (x, \theta) \bigr)^\top 
\Lambda_{S_1 S_2} (x, \theta) \,  
\partial^{\beta}_{j} \mathcal{L} V_{S_2, 0} (x, \theta)  \\[0.2cm] 
& \quad  + 
\bigl( \partial^{\beta}_i \mathcal{L} V_{S_1, 0} (x, \theta) \bigr)^\top
\Lambda_{S_1 R} (x, \theta) \, 
\partial^{\beta}_{j} V_{R, 0} (x, \beta_R);  \\[0.3cm] 
\widetilde{H}_{ij, 2} (x, \theta) 
& \equiv  \bigl( \tfrac{1}{3} \partial^{\beta}_i V_{S_2, 0} (x, \beta_{S_2}) \bigr)^\top \Lambda_{S_2 S_1} (x, \theta) \, \partial^{\beta}_j \mathcal{L}^2 V_{S_1, 0} (x, \theta) \\[0.2cm]  
& \quad + \bigl( \partial^{\beta}_i V_{S_2, 0}  (x, \beta_{S_2})
\bigr)^\top \Lambda_{S_2 S_2} (x, \theta) 
\, \partial^{\beta}_{j} \mathcal{L} V_{S_2, 0} (x, \theta) \\[0.2cm] 
& \quad  + 
2 \bigl( \partial^{\beta}_i V_{S_2, 0} (x, \beta_{S_2}) \bigr)^\top \Lambda_{S_2 R} (x, \theta) 
\, \partial^{\beta}_{j} V_{R, 0} ( x, \beta_R ).  
\end{align*}
% 
% 
We then have $\widetilde{H}_{ij, k} (x, \theta) = 0$ for any $(x, \theta) \in \mathbb{R}^N \times \Theta$, $k = 1, 2$, from the same argument as in case (a1). Thus, making use of Lemma \ref{lemma:ergodic_thm}, convergence (\ref{eq:step1}) with condition (\ref{assump:bd_deriv}) and the consistency of the estimator, we obtain (\ref{eq:slln_conv1}) in case (a2).
\\ 

Finally, we consider the case (a3). We have 
% 
\begin{align*}
\bigl[ \mathscr{I}_{n} \bigl( \theta \bigr) \bigr]_{ij} 
& =  
\tfrac{1}{n} \sum_{k = 1}^n \bar{H}_{ij, 1}  (\sample{k-1}, \theta)
+ \tfrac{1}{n} \sum_{k = 1}^n \bar{H}_{ij, 2}  (\sample{k-1}, \theta)
+ \tfrac{1}{n} \sum_{k = 1}^n \bar{H}_{ij, 3}  (\sample{k-1}, \theta)  \\ 
& \quad + \tfrac{1}{n \sqrt{\Delta_n}} \sum_{k  = 1}^n R (1, \sample{k - 1}, \theta) \, m_{k} (\Delta_n, \trueparam) 
\\[0.1cm]
& \quad + \tfrac{1}{n \sqrt{\Delta_n}} \sum_{k  = 1}^n 
\bigl( m_{k} (\Delta_n, \theta) - m_{k} (\Delta_n, \trueparam) \bigr)^\top 
\Lambda (\sample{k-1}, \theta) 
\, \partial^{\beta}_{(i,j)} v (\sample{k - 1}, \theta)  \\[0.1cm]
& \quad + \tfrac{1}{n} \sum_{k  = 1}^n \widetilde{R} (\sqrt{\Delta_n}, \sample{k-1}, \theta),
\end{align*}
% 
for $R, \, \widetilde{R} \in \mathcal{S}$, where we have set, for $(x, \theta) \in \mathbb{R}^N \times \Theta$,
% 
\begin{align}
& \begin{aligned} \label{eq:bar_H1} 
 \bar{H}_{ij, 1} (x, \theta)
& \equiv  \tfrac{1}{18} 
\bigl(  \partial^{\beta}_i \mathcal{L}^2 V_{S_1, 0} (x, \theta)  \bigr)^\top 
\Lambda_{S_1 S_1} (x, \theta) 
\, 
\partial^{\beta}_j \mathcal{L}^2 V_{S_1, 0} (x, \theta) \\[0.2cm] 
& \quad + \tfrac{1}{6} 
\bigl( \partial^{\beta}_i \mathcal{L}^2 V_{S_1, 0} (x, \theta) \bigr)^\top 
\Lambda_{S_1 S_2} (x, \theta) 
\, \partial^{\beta}_{j} \mathcal{L} V_{S_2, 0} (x, \theta)  \\[0.2cm] 
& \quad + 
\tfrac{1}{3} \bigl( \partial^{\beta}_i \mathcal{L}^2 V_{S_1, 0} (x, \theta) \bigr)^\top 
\Lambda_{S_1 R} (x, \theta) 
\, \partial^{\beta}_{j} V_{R, 0} (x, \beta_R);  \\[0.3cm] 
% 
\end{aligned} \\
& 
\begin{aligned} \label{eq:bar_H2}
\bar{H}_{ij, 2}  (x, \theta) 
& \equiv \tfrac{1}{6} 
\bigl( \partial^{\beta}_i 
\mathcal{L} V_{S_2, 0} (x, \theta) \bigr)^\top 
\Lambda_{S_2 S_1} (x, \theta) 
\, \partial^{\beta}_j \mathcal{L}^2 V_{S_1, 0} (x, \theta) \\[0.2cm]  
& \quad + \tfrac{1}{2} \bigl( \partial^{\beta}_i \mathcal{L} V_{S_2, 0} (x, \theta) \bigr)^\top 
\Lambda_{S_2 S_2} (x, \theta) 
\, \partial^{\beta}_{j} \mathcal{L} V_{S_2, 0} (x, \theta) \\[0.2cm] 
& \quad  +  \bigl( \partial^{\beta}_i \mathcal{L} V_{S_2, 0}  (x, \theta) \bigr)^\top 
\Lambda_{S_2 R} (x, \theta)
\, \partial^{\beta}_{j} V_{R, 0} (x, \beta_R); \\[0.3cm]
% 
\end{aligned} \\
& 
\begin{aligned} \label{eq:bar_H3} 
\bar{H}_{ij, 3} (x, \theta) 
& \equiv  \tfrac{1}{3} \bigl( \partial^{\beta}_i V_{R, 0} (x, \beta_{R}) \bigr)^\top
\Lambda_{R S_1} (x, \theta) 
\, \partial^{\beta}_j \mathcal{L}^2 V_{S_1, 0} (x, \theta) \\[0.2cm] 
& \quad + \bigl( \partial^{\beta}_i V_{R, 0} (x, \beta_{R})  \bigr)^\top 
\Lambda_{R S_2} (x, \theta) 
\, \partial^{\beta}_{j} \mathcal{L} V_{S_2, 0} (x, \theta) 
\\[0.2cm] 
& \quad  + 
2 \bigl( \partial^{\beta}_i V_{R, 0}  (x, \beta_{R}) \bigr)^\top 
\Lambda_{RR} (x, \theta) 
\, \partial^{\beta}_{j} V_{R, 0} (x, \beta_R),  
\end{aligned} 
\end{align}
% 
and
% 
\begin{align*}
v (x, \theta) \equiv \Bigl[ \tfrac{1}{6} 
\mathcal{L}^2 V_{S_1, 0} (x, \theta)^\top, \, 
\tfrac{1}{2} \mathcal{L} V_{S_2, 0} (x, 
\theta)^\top,  \, 
V_{R, 0} (x, \beta_R)^\top  \Bigr]^\top.    
\end{align*}
% 
Notice that for any $(x, \theta) \in \mathbb{R}^N \times \Theta$, 
% 
\begin{gather*}
\bar{H}_{ij, 1} (x, \theta) = 0, 
\qquad \bar{H}_{ij, 2} (x, \theta) = 0;  \\[0.1cm]  
\bar{H}_{ij, 3} (x, \theta) 
= 2 \bigl( \partial^{\beta}_i V_{R,0} (x, \beta_R) \bigr)^\top a_R^{-1} (x, \sigma) 
\, \partial^{\beta}_j V_{R,0} (x, \beta_R).  
\end{gather*} 
% 
Furthermore, it follows that
% 
\begin{align*}
 & \tfrac{1}{n \sqrt{\Delta_n}} \sum_{k  = 1}^n 
 \bigl( m_{k} (\Delta_n, \theta) - m_{k} (\Delta_n, \trueparam) \bigr)^\top
 \Lambda (\sample{k-1},  \theta)
 \partial^{\beta}_{(i,j)}  v (\sample{k - 1}, \theta)   \\[0.1cm]
 & = \tfrac{1}{n} \sum_{k = 1}^n 
 \bigl( V_{R, 0} (\sample{k -1}, \truebeta_R) 
 - V_{R, 0} (\sample{k -1}, \beta_R) 
 \bigr)^\top 
a_R^{-1} (\sample{k-1}, \sigma) \partial^{\beta}_{(i,j)} 
V_{R, 0} (\sample{k - 1}, \beta_R),  
\end{align*}
% 
% 
where we made use of similar arguments in the proof of Lemma \ref{lemma:matrix_2} (Section \ref{appendix:pf_matrix_2}) for the term $\Lambda (\sample{k-1},  \theta)
 \partial^{\beta}_{(i,j)}  v (\sample{k - 1}, \theta)$. Hence, exploiting Lemmas \ref{lemma:ergodic_thm}, \ref{lemma:canonical_conv} and the consistency of estimator $\hat{\theta}_n$, we obtain that: 
% 
\begin{align*}
\Bigl[\mathscr{I}_{n} \bigl(\trueparam + \lambda (\hat{\theta}_n - \trueparam) \bigr) \Bigr]_{ij} 
\probconv 2 \int \bigl( \partial^{\beta}_i V_{R, 0} (x, \truebeta_R) \bigr)^\top \, 
a_R^{-1} (x, \truesigma) \, 
\partial^{\beta}_j V_{R, 0} (x, \truebeta_R) \, \truedist (dx), 
\end{align*}
% 
% 
as \limit, for $N_{\beta} + 1 \le i, j \le N_{\theta}$. 
The proof of (\ref{eq:slln_conv1}) is now complete. 
% 
\subsubsection{Proof of (\ref{eq:slln_conv2})}
%
We show (\ref{eq:slln_conv2}) when $1 \le i \le N_{\beta_{S_1}}$ and $N_{\beta} + 1 \le j \le N_\theta$. The convergence for the other cases can be deduced from a similar argument used in the proof of (\ref{eq:slln_conv1}) so we omit the proof. We have
%
\begin{align*}
\bigl[ \mathscr{I}_n \bigl( \theta \bigr) \bigr]_{ij}  
 & = \tfrac{\sqrt{\Delta_n^3}}{n} 
 \sum_{k = 1}^n  \sum_{1 \le k_1, k_2 \le N} 
 R_{k_1 k_2} (1, \sample{k -1}, \theta) 
 m_{k}^{k_1} (\Delta_n, \trueparam) 
 m_{k}^{k_2} (\Delta_n, \trueparam)  \nonumber \\[0.1cm] 
 & \quad + \tfrac{\sqrt{\Delta_n^3}}{n} 
 \sum_{k = 1}^n  \sum_{1 \le k_1, k_2 \le N} 
 \biggl\{ \widetilde{R}_{k_1 k_2} (1, \sample{k -1}, \theta) 
 \bigl( m_{k}^{k_1} (\Delta_n, \theta) - m_{k}^{k_1} (\Delta_n, \trueparam) \bigr) \\[0.2cm] 
 & \qquad \qquad  \qquad  \qquad  
 \times \bigl( m_{k}^{k_2} (\Delta_n, \theta) - m_{k}^{k_2} (\Delta_n, \trueparam) \bigr) \biggr\} \nonumber \\ 
 & \quad + \tfrac{1}{n} 
 \sum_{k = 1}^n  \sum_{1 \le k_1 \le N} 
  {R}_{k_1} (1, \sample{k -1}, \theta) 
 \bigl( m_{k}^{k_1} (\Delta_n, \theta) - m_{k}^{k_1} (\Delta_n, \trueparam) \bigr) \\[0.2cm]
 & \quad + \tfrac{1}{n} 
 \sum_{k = 1}^n {R} (\sqrt{\Delta_n}, \sample{k -1}, \theta),    
\end{align*}
% 
for some $R_{k_1 k_2}, \widetilde{R}_{k_1k_2}, R_{k_1}, R \in \mathcal{S}$.  
Thus, we immediately obtain (\ref{eq:slln_conv2}) for $1 \le i \le N_{\beta_{S_1}}, \, N_\beta + 1 \le j \le N_\theta$ from Lemma \ref{lemma:ergodic_thm}, \ref{lemma:canonical_conv} and (\ref{eq:step1})-(\ref{eq:step2}).  
% 
\subsubsection{Proof of (\ref{eq:slln_conv3})} 
%
It holds that for $N_\beta + 1 \le i, j \le N_\theta$, $\theta  = (\beta_{S_1}, \beta_{S_2}, \beta_R, \sigma) \in \Theta$,
% 
\begin{align*}
\bigl[ \mathscr{I}_{n} \bigl( \theta \bigr) \bigr]_{ij}   
& = \tfrac{1}{n} \sum_{k = 1}^n  
m_{k} (\Delta_n, \trueparam)^\top \, 
\partial^{\sigma}_{(i, j)} \Lambda (\sample{k-1}, \theta) \, 
m_{k} (\Delta_n, \trueparam)  \\[0.1cm]
& \quad + \tfrac{1}{n} \sum_{k = 1}^n 
\partial^{\sigma}_{(i,j)} 
\log \bigl| \Sigma (\sample{k - 1}, \theta ) \bigr| \\[0.1cm]
& \quad + \tfrac{1}{n} \sum_{k = 1}^n \sum_{1 \le k_1, k_2 \le N}  
R_{k_1 k_2} (1 , \sample{k - 1}, \theta) m_{k}^{k_1} (\Delta_n, \trueparam) \bigl( m_{k}^{k_2} (\Delta_n, \trueparam)  - m_{k}^{k_2} (\Delta_n, \theta) \bigr)  \\[0.2cm] 
& \quad + \tfrac{1}{n} \sum_{k = 1}^n \sum_{1 \le k_1, k_2 \le N}  
\biggl\{ \widetilde{R}_{k_1 k_2} (1 , \sample{k - 1}, \theta) \\[0.1cm]
& \qquad \qquad 
\times \bigl( m_{k}^{k_1} (\Delta_n, \trueparam)  - m_{k}^{k_1} (\Delta_n, \theta)\bigr)
\bigl( m_{k}^{k_2} (\Delta_n, \trueparam)  - m_{k}^{k_2} (\Delta_n, \theta)\bigr) \biggr\}  \\[0.2cm]  
& \quad + \tfrac{1}{n} \sum_{k = 1}^n \sum_{1 \le k_1 \le N}  
R_{k_1} (\sqrt{\Delta_n}, \sample{k - 1}, \theta) 
\bigl( m_{k}^{k_1} (\Delta_n, \trueparam) 
   - m_{k}^{k_1} (\Delta_n, \theta)\bigr)  \\[0.2cm]
& \quad + \tfrac{1}{n} \sum_{k = 1}^n R (\Delta_n, \sample{k-1}, \theta),
\end{align*}
%
for some $R_{k_1 k_2}, \widetilde{R}_{k_1 k_2}, R_{k_1}, R \in \mathcal{S}$. Making use of Lemmas \ref{lemma:ergodic_thm}--\ref{lemma:canonical_conv}, (\ref{eq:step1}), (\ref{eq:step2}) and the consistency of the estimator, we obtain as \limit, 
% 
\begin{align*} 
& \bigl[ \mathscr{I}_{n} \bigl( \trueparam + \lambda (\hat{\theta}_{n} - \trueparam) \bigr) \bigr]_{ij}  %\\[0.2cm]
%& 
\probconv \int 
    \Bigl\{ \mathrm{tr} \bigl(
    \partial^{\sigma}_{(i,j)}
    \Lambda (x, \trueparam ) 
    \, \Sigma  (x, \trueparam ) \bigr) +  \partial^{\sigma}_{(i,j)} \log \bigl| \Sigma (x, \trueparam) \bigr|  \Bigr\} \truedist (dx) \\[0.2cm] 
& \quad = \int 
     \mathrm{tr}  \bigl(
    \partial^{\sigma}_i \Sigma  (x, \trueparam) \, 
    \Lambda  (x, \trueparam)  
    \partial^{\sigma}_j \Sigma  (x, \trueparam ) \,
    \Lambda  (x, \trueparam) \bigr) \truedist (dx),
\end{align*}
% 
uniformly in $\lambda \in [0,1]$, where we applied the following two formulae to the above equation: 
% 
\begin{gather*}
\partial^{\sigma}_{(i,j)}  
\log \bigl| \Sigma (x, \trueparam) \bigr| 
= - \mathrm{tr} \bigl(
    \partial^{\sigma}_{(i,j)}  
    \Lambda (x, \trueparam) 
    \, \Sigma  (x, \trueparam) \bigr) 
  - \mathrm{tr} \bigl( \partial^{\sigma}_i \Sigma (x, \trueparam ) \partial^{\sigma}_j 
\Lambda (x, \trueparam )  \bigr); \\[0.2cm]
%
\mathrm{tr} \bigl( \partial^{\sigma}_i \Sigma (x, \trueparam) \partial^{\sigma}_j 
 \Lambda (x, \trueparam )  \bigr) 
= - \mathrm{tr} \bigl( \partial^{\sigma}_i  
\Sigma (x, \trueparam ) \, 
\Lambda  (x, \trueparam )
\partial^{\sigma}_j 
\Sigma  (x, \trueparam ) \,  
\Lambda  (x, \trueparam )   \bigr). 
\end{gather*} 
% 
The proof is now complete. 
% 
\subsection{Proof of Lemma \ref{lemma:clt}} \label{appendix:pf_clt}
%%
We write
%
$ \mathscr{C}_{n}^k (\theta) = \textstyle{\sum_{i = 1}^n \zeta_{i}^k (\theta), \;  \theta \in \Theta, \;  1 \le k \le N_\theta}$,
% 
where we have set: 
% 
\begin{align*}
\zeta_{i}^k (\theta) 
\equiv \bigl[ M_{n}^{-1} \bigr]_{kk} \times
\partial^{\theta}_k  
\bigl\{   
m_{i} (\Delta_n, \theta)^\top
\Lambda (\sample{i-1}, \theta) 
m_{i} (\Delta_n, \theta) 
+ \log |\Sigma (\sample{i-1}, \theta) |  \bigr\}. 
\end{align*}
%
% 
To prove the assertion, it suffices to show, from Theorem 3.2 and 3.4 in \cite{hall:14}, that: 
% 
\begin{enumerate}
    \item[(i)] If \limit \; with $\Delta_n = o (n^{-1/2})$, then 
    %
    \begin{align}
        \sum_{i = 1}^n \mathbb{E}_{\trueparam}
        \bigl[ \zeta_{i}^k (\trueparam) | \mathcal{F}_{t_{i-1}} \bigr] \probconv 0, \quad 1 \le k \le N_{\theta}.  \label{eq:clt_conv1}
    \end{align}
    % 
    % 
    \item[(ii)] If \limit, then 
    % 
    \begin{align}
    \sum_{i = 1}^n \mathbb{E}_{\trueparam}
    \bigl[  
     \zeta_{i}^{k_1} (\trueparam) 
     \zeta_{i}^{k_2} (\trueparam) 
    | \mathcal{F}_{t_{i-1}} \bigr] \probconv 4 [\Gamma (\trueparam)]_{k_1 k_2},  
    \quad  1 \le k_1, k_2 \le N_{\theta}. \label{eq:clt_conv2}
    \end{align}
    %
    % 
    \item[(iii)] If \limit, then 
    % 
    \begin{align}
    \sum_{i = 1}^n 
    \mathbb{E}_{\trueparam}
    \bigl[  
    \zeta_{i}^{k_1} (\trueparam) 
    \zeta_{i}^{k_2} (\trueparam) 
    \zeta_{i}^{k_3} (\trueparam) 
    \zeta_{i}^{k_4} (\trueparam) 
    | \mathcal{F}_{t_{i-1}} \bigr] \probconv 0, \quad 1 \le k_1, k_2, k_3, k_4 \le N_{\theta}. \label{eq:clt_conv3} 
    \end{align}
    % 
    % 
\end{enumerate}
% 
In what follows, we will check convergences (\ref{eq:clt_conv1}) and (\ref{eq:clt_conv2}). One can prove (\ref{eq:clt_conv3}) following similar arguments and by noticing that the left-hand-side of (\ref{eq:clt_conv3}) involves $1/n^2$.   
% 
\subsubsection{Proof of (\ref{eq:clt_conv1})}
%
We recall (\ref{eq:mean_expectation}) and (\ref{eq:squared_mean_expectation}) that are immediately obtained from the definition of $m_i (\Delta_n, \trueparam)$ in (\ref{eq:m_simple}), that is, for $1 \le k_1, k_2 \le N$, 
% 
\begin{gather} 
\mathbb{E}_{\trueparam} \bigl[ m_i^{k_1} (\Delta_n, \trueparam) | \mathcal{F}_{t_{i-1}} \bigr] = R_1 (\sqrt{\Delta_n^3}, \sample{i-1}, \trueparam); \label{eq:mean_expecatation_simple} \\[0.2cm] 
\mathbb{E}_{\trueparam} \bigl[ m_i^{k_1} (\Delta_n, \trueparam)
m_i^{k_2} (\Delta_n, \trueparam) | \mathcal{F}_{t_{i-1}} \bigr] = [\Sigma (\sample{i-1}, \trueparam )]_{k_1 k_2}
+ R_2 ({\Delta_n}, \sample{i-1}, \trueparam) 
\label{eq:squared_expecatation_simple}  
\end{gather}
% 
for $R_1, R_2 \in \mathcal{S}$. We then write $\zeta_{i}^k (\theta), \, \theta \in \Theta, \, 1 \le i \le n, \, 1 \le k \le N_{\theta}$ as $\zeta_i^k (\theta) = \zeta_{i, 1}^{k} (\theta) + \zeta_{i, 2}^{k} (\theta)$, where we have set:
% 
\begin{gather*}
\zeta_{i, 1}^{k} (\theta)
= 2 \bigl[ M_{n}^{-1} \bigr]_{kk} 
\times 
\bigl\{ 
\bigl( \partial^{\theta}_k  
m_{i} (\Delta_n, \theta) \bigr)^\top 
\Lambda (\sample{i-1}, \theta) 
\, m_{i} (\Delta_n, \theta) \bigr\};  \nonumber \\[0.2cm]
% 
\zeta_{i, 2}^{k} (\theta) 
= \bigl[ M_{n}^{-1} \bigr]_{kk}  
\times
\bigl\{ 
\partial^{\theta}_k\log | \Sigma (\sample{i-1}, \theta) |
+ m_{i} (\Delta_n, \theta)^\top 
 \partial^{\theta}_k \Lambda (\sample{i-1}, \theta) 
 \, m_{i} (\Delta_n, \theta)  
\bigr\}.  
\end{gather*}
% 
Exploiting (\ref{eq:mean_expecatation_simple}), we obtain
% 
\begin{align*}
 \mathbb{E}_{\trueparam} \bigl[ \zeta_{i, 1 }^{k} (\trueparam)  | \mathcal{F}_{t_{i-1}} \bigr]
 & = \tfrac{1}{\sqrt{n}}  \sum_{j =1}^N R_k^j (1, \sample{i-1}, \trueparam) \,  
 \mathbb{E}_{\trueparam} \bigl[ m_{i}^j (\Delta_n, \trueparam) | \mathcal{F}_{t_{i-1}} \bigr] \\ 
 & = \tfrac{1}{n}  \widetilde{R}_k^j (\sqrt{n \Delta_n^{3}}, \sample{i-1}, \trueparam) 
\end{align*}
% 
for $R_k^j, \, \widetilde{R}_k^j \in \mathcal{S}$. 
% 
Thus, from Lemma \ref{lemma:ergodic_thm}, we obtain 
% 
\begin{align*}
\sum_{i = 1}^n \mathbb{E}_{\trueparam} \bigl[ \zeta_{i, 1 }^{k} (\trueparam)  | \mathcal{F}_{t_{i-1}} \bigr] 
= \tfrac{1}{n} \sum_{i=1}^n \widetilde{R}_k^j (\sqrt{n \Delta_n^{3}}, \sample{i-1}, \trueparam) \probconv 0,  
\end{align*}
% 
if \limit \; and $\Delta_n = o (n^{-1/2})$. Next, we consider the second term $\zeta_{i, 2}^{k} (\theta)$. First, notice that for $N_{\beta_{S}} + 1 \le k \le N_{\beta}$, $\zeta_{i, 2}^k ( \trueparam) =0$, since $\Sigma (\sample{i-1}, \theta)$ and $\Lambda (\sample{i-1}, \theta)$ are independent of $\beta_R \in \Theta_{\beta_R}$. For $1 \le k \le N_{\beta_S}$ and $N_{\beta} + 1 \le k \le N_{\theta}$, we apply (\ref{eq:squared_expecatation_simple}) to obtain   
% 
\begin{align*}
\mathbb{E}_{\trueparam} [\zeta^k_{i, 2} (\trueparam) | \mathcal{F}_{t_{i-1}} ]
& = [M_n^{-1}]_{kk} \times \Bigl\{
 \partial^{\theta}_k 
 \log |\Sigma (\sample{i-1}, \trueparam) |
 + \mrm{tr} \bigl(\partial^{\theta}_k  
 \Lambda (\sample{i-1}, \trueparam) 
 \Sigma (\sample{i-1}, \trueparam) \bigr) 
 \Bigr\} \\[0.2cm]
& \qquad   + \tfrac{1}{n} R_k (\sqrt{n \Delta_n^2}, \sample{i-1}, \trueparam) \\[0.2cm]
& = \tfrac{1}{n} R_k (\sqrt{n \Delta_n^2}, \sample{i-1}, \trueparam)
\end{align*}
% 
for $R_k \in \mathcal{S}$, where we used: 
% 
\begin{align} \label{eq:deriv_logdet}
\partial^{\theta}_k \log |\Sigma (x, \theta)| 
= - \mathrm{tr} \bigl( \partial^{\theta}_k \Lambda (x, \theta) \Sigma (x, \theta) \bigr), \quad (x, \theta) \in \mathbb{R}^N \times \Theta. 
\end{align}
% 
Thus, we have from Lemma \ref{lemma:ergodic_thm} that if \limit \;  with $\Delta_n = o (n^{-1/2})$, then
% 
\begin{align*}
    \sum_{i = 1}^n \mathbb{E}_{\trueparam} 
    \bigl[  
    \zeta_{i, 2}^{k} (\trueparam)  | \mathcal{F}_{t_{i-1}} 
    \bigr]
    = \tfrac{1}{n} \sum_{i = 1}^n R_k ( \sqrt{n \Delta_n^{2}}, \sample{i-1}, \trueparam) \probconv 0, 
\end{align*}
% 
and the proof of (\ref{eq:clt_conv1}) is now complete. 
% 
% 
\subsubsection{Proof of (\ref{eq:clt_conv2})} 
% 
% 
For simplicity, we write  
% 
\begin{align*}
    \mathscr{Y}_{k_1 k_2} (\trueparam) \equiv \sum_{i = 1}^n 
    \mathbb{E}_{\trueparam}
    \bigl[  
     \zeta_{i}^{k_1} (\trueparam) 
     \zeta_{i}^{k_2} (\trueparam) 
    | \mathcal{F}_{t_{i-1}} 
    \bigr].  
\end{align*} 
% 
We have that for $1 \le k_1 \le N_{\beta_S}$, $N_{\beta_S} + 1 \le k_2 \le N_{\beta}$, $N_{\beta} + 1 \le k_3 \le N_{\theta}$, % 
\begin{align}
& 
\begin{aligned}
\sqrt{n} \zeta_{i}^{k_1} (\trueparam) 
 & = - 2 \, 
 \mu_{k_1} (\sample{i-1}, \trueparam)^\top  \Lambda (\sample{i-1},  \trueparam) 
\; m_{i} (\Delta_n, \trueparam)  \\[0.2cm]
& \qquad 
+  \sum_{1 \le j_1, j_2 \le N} 
R_{k_1}^{j_1 j_2} (\sqrt{\Delta_n}, \sample{i-1}, \trueparam)
m_{i}^{j_1} (\Delta_n, \trueparam)
m_{i}^{j_2} (\Delta_n, \trueparam) \\[0.1cm] 
& \qquad 
+  \sum_{1 \le j_1 \le N} 
R_{k_1}^{j_1} (\sqrt{\Delta_n}, \sample{i-1}, \trueparam)
m_{i}^{j_1} (\Delta_n, \trueparam);  
\end{aligned}  \label{eq:partial_beta_S}  \\[0.3cm]
% 
& 
\begin{aligned}
\sqrt{n} \zeta_{i}^{k_2} (\trueparam) 
&  = - 2 \, 
\mu_{k_2} (\sample{i-1}, \trueparam)^\top
\Lambda (\sample{i-1}, \trueparam)
m_{i} (\Delta_n, \trueparam); % \\[0.2cm]
% & \qquad  + \sum_{1 \le j_1 \le N} R_{k_2}^{j_1}  (\sqrt{\Delta_n}, \sample{i-1}, \trueparam) \,
% m_{i}^{j_1} (\Delta_n; \trueparam);  
% 
\end{aligned} \label{eq:partial_beta_R} \\[0.3cm] 
% 
& 
\begin{aligned}
\sqrt{n} \zeta_{i}^{k_3} (\trueparam) 
& = m_{i} (\Delta_n, \trueparam)^\top
\, 
\bigl( 
\partial^{\theta}_{k_3}  
\Lambda (\sample{i-1}, \trueparam) \bigr)
\, m_{i} (\Delta_n, \trueparam) + \partial^{\theta}_{k_3} 
\log | \Sigma (\sample{i-1} \trueparam) | \\[0.2cm]
& \qquad 
 + \sum_{1 \le j_1, j_2 \le N} R_{k_3}^{j_1 j_2} ({\Delta_n}, \sample{i-1}, \trueparam) \, m_{i}^{j_1} (\Delta_n, \trueparam) m_{i}^{j_2} (\Delta_n, \trueparam) \\[0.1cm]
& \qquad 
 + \sum_{1 \le j_1, j_2 \le N} R_{k_3}^{j_1} (\sqrt{\Delta_n}, \sample{i-1}, \trueparam) \, m_{i}^{j_1} (\Delta_n, \trueparam), 
\end{aligned} \label{eq:partial_sigma}
\end{align}
% 
for $R_{k_1}^{j_1 j_2}, \, R_{k_1}^{j_1},  %\, R_{k_2}^{j_1},
\, R_{k_3}^{j_1 j_2}, \, R_{k_3}^{j_1} \in \mathcal{S}$, where we defined $\mu_k : \mathbb{R}^N \times \Theta \to \mathbb{R}^N, \, 1 \le k \le N_{\beta}$ as: 
% 
\begin{align*} 
& \mu_k (x, \theta) 
%\\ & 
= 
\begin{cases}
\Bigl[
\bigl( \partial^\beta_k V_{S_1, 0} (x_S, \beta_{S_1}) \bigr)^\top, 
\, \mathbf{0}_{N_{S_2}}^\top,  \, \mathbf{0}_{N_{R}}^\top 
 \Bigr]^\top,   
 &  1 \le k \le N_{\beta_{S_1}}; \\[0.2cm] 
\Bigl[ \tfrac{1}{2} \bigl( \partial^\beta_k \mathcal{L} V_{S_1, 0} (x, \theta) \bigr)^\top, 
\,  \bigl( \partial^\beta_k  V_{S_2, 0} (x, \beta_{S_2}) \bigr)^\top,  \, \mathbf{0}_{N_{R}}^\top 
 \Bigr]^\top,   &  N_{\beta_{S_1}} + 1 \le k \le N_{\beta_{S}};  \\[0.2cm] 
 \Bigl[ \tfrac{1}{6} \bigl( \partial^\beta_k \mathcal{L}^2 V_{S_1, 0} (x, \theta) \bigr)^\top, 
\,  \tfrac{1}{2} \bigl( \partial^\beta_k \mathcal{L} V_{S_2, 0} (x, \theta) \bigr)^\top,  \, \bigl( \partial^\beta_k  V_{R, 0} (x, \beta_{R} ) \bigr)^\top 
 \Bigr]^\top,   &  N_{\beta_{S}} + 1 \le k \le N_{\beta},
\end{cases}
\end{align*}
% 
for $(x, \theta) \in \mathbb{R}^N \times \Theta$. From Lemma \ref{lemma:ergodic_thm}, \ref{lemma:canonical_conv} and (\ref{eq:partial_beta_S}), we have that for $1 \le k_1, k_2 \le N_{\beta}$,  
% 
\begin{align*}
& \mathscr{Y}_{k_1 k_2} (\trueparam)  %\\
\probconv %& \, 
4 \sum_{1 \le j_1, j_2, j_3, j_4 \le N} 
\int \mu_{k_1}^{j_1} (x, \trueparam)
\bigl[ \Lambda (x, \trueparam) \bigr]_{j_1 j_2}  
\mu_{k_2}^{j_3} (x, \trueparam) 
\bigl[ \Lambda(x, \trueparam ) \bigr]_{j_3 j_4}  
\bigl[ \Sigma (x, \trueparam) \bigr]_{j_2 j_4} 
\, \truedist (dx) \\[0.2cm]
& \qquad\qquad\qquad\qquad = 4 \sum_{1 \le j_1, j_2 \le N} 
\int \mu_{k_1}^{j_1} (x, \trueparam)  
\bigl[ \Lambda (x, \trueparam) \bigr]_{j_1 j_2} 
\mu_{k_2}^{j_2} (x, \trueparam) 
\truedist (dx) \nonumber \\[0.2cm] 
& \qquad\qquad\qquad\qquad \equiv 4 \int  \widetilde{\mathscr{Y}}_{k_1 k_2} (x, \trueparam) \,  \truedist (dx). 
\end{align*}
% 
% where we have defined, for $ (x, \theta) \in \mathbb{R}^N \times \Theta$, 
% %  
% \begin{align*} 
% \widetilde{\mathscr{Y}}_{k_1 k_2} (x, \theta) \equiv
% 4 \bigl( \partial^{\theta}_{k_1} V_0  (x, \beta) \bigr)^\top \, 
% \Lambda (x, \theta) \, 
% \partial^{\theta}_{k_2} V_0 (x, \beta).  
% \end{align*} 
% 
% 
We then have, for $1 \le k_1, k_2 \le N_{\beta_{S_1}}$, 
% 
\begin{align}
\widetilde{\mathscr{Y}}_{k_1 k_2} (x, \trueparam) 
& = \bigl( \partial^\beta_{k_1} V_{S_1, 0} (x_S, \truebeta_{S_1}) \bigr)^\top
 \Lambda_{S_1 S_1} (x, \trueparam)
\, \partial^\beta_{k_2} V_{S_1, 0} (x_S, \truebeta_{S_1}) \nonumber \\[0.1cm]
 & = 720  \bigl( \partial^\beta_{k_1} V_{S_1, 0} (x_S, \truebeta_{S_1}) \bigr)^\top
 a_{S_1}^{-1} (x, \trueparam)  
\, \partial^\beta_{k_2} V_{S_1, 0} (x_S, \truebeta_{S_1}),
\label{eq:Y_1}
\end{align}
% 
for $N_{\beta_{S_1}} + 1 \le k_1, k_2 \le N_{\beta_S}$, 
% 
\begin{align}
\widetilde{\mathscr{Y}}_{k_1 k_2} (x, \trueparam) 
& = \tfrac{1}{4} \bigl( \partial^\beta_{k_1} \mathcal{L} V_{S_1,0 } (x, \trueparam) \bigr)^\top \Lambda_{S_1 S_1} (x, \trueparam) 
\, \partial^\beta_{k_2} \mathcal{L} V_{S_1,0 } (x, \trueparam) 
\nonumber \\[0.1cm] 
& \quad +  \tfrac{1}{2} \bigl( \partial^\beta_{k_1} \mathcal{L} V_{S_1,0 } (x, \trueparam) \bigr)^\top \Lambda_{S_1 S_2} (x, \trueparam) 
\, \partial^\beta_{k_2} V_{S_2,0 } (x, \truebeta_{S_2} ) 
\nonumber \\[0.1cm] 
& \quad +  \tfrac{1}{2} \bigl( 
\partial^\beta_{k_1} V_{S_2,0 } (x, \truebeta_{S_2} ) 
\bigr)^\top \Lambda_{S_2 S_1} (x, \trueparam) 
\, \partial^\beta_{k_2} \mathcal{L} V_{S_1,0 } (x, \trueparam)  
\nonumber \\[0.1cm]   
& \quad +  \bigl( 
\partial^\beta_{k_1} V_{S_2,0 } (x, \truebeta_{S_2} ) 
\bigr)^\top \Lambda_{S_2 S_2} (x, \trueparam) 
\, \partial^\beta_{k_2} V_{S_2,0 } (x, \truebeta_{S_2} ) 
\nonumber \\[0.2cm] 
& = 12 \bigl( 
\partial^\beta_{k_1} V_{S_2,0 } (x, \truebeta_{S_2} ) 
\bigr)^\top a_{S_2}^{-1} (x, \trueparam) 
\, \partial^\beta_{k_2} V_{S_2,0 } (x, \truebeta_{S_2} ),  
\label{eq:Y_2} 
\end{align}
%
and for $N_{\beta_S} + 1 \le k_1, k_2 \le N_{\beta}$, 
% 
\begin{align}
\widetilde{\mathscr{Y}}_{k_1 k_2} (x, \trueparam)  
= \tfrac{1}{2} \sum_{j = 1}^3 \bar{H}_{k_1 k_2, j} (x, \trueparam)
= \bigl( \partial^\beta_{k_1} V_{R, 0} (x, \truebeta_R) \bigr)^\top
\, a_R^{-1} (x, \truesigma) 
\, \partial^\beta_{k_2} V_{R, 0} (x, \truebeta_R),  
\end{align}
%
where $\bar{H}_{k_1 k_2, j} \, 1 \le j \le 3$ are defined as (\ref{eq:bar_H1}), (\ref{eq:bar_H2}) and (\ref{eq:bar_H3}). Note that we used Lemma \ref{lemma:matrix}, \ref{lemma:lambda} to derive (\ref{eq:Y_1})--(\ref{eq:Y_2}). 
% 
% \begin{align*} 
% \widetilde{\mathscr{Y}}_{k_1 k_2} (x, \trueparam) = 
% \bigl( \partial^\beta_{k_1} V_0 (x, \truebeta) \bigr)^\top 
% \begin{bmatrix}
%   \Lambda_{S_1 S_1} (x, \trueparam) & \mathbf{0}_{N_{S_1} \times N_{S_2}}  
%   & \mathbf{0}_{N_{S_1} \times N_{R}} \\[0.1cm]   
%    \mathbf{0}_{N_{S_2} \times N_{S_1}} &  \Lambda_{S_2 S_2} (x, \trueparam) &   \mathbf{0}_{N_{S_2} \times N_{R}} \\[0.1cm]    
%   \mathbf{0}_{N_{R} \times N_{S_1}}  
%   & \mathbf{0}_{N_{R} \times N_{S_2}} & a_R^{-1} (x, \truesigma)
% \end{bmatrix} \partial^\beta_{k_2} V_0 (x, \truebeta), 
% \end{align*}
% % 
% \begin{align*}  
% \widetilde{\mathscr{Y}}_{k_1 k_2} (x, \theta) = 
% \begin{cases}
% 4 \bigl(  \partial^{\theta}_{k_1} V_{S_1, 0} (x_S, \beta_{S_1}) \bigr)^\top \, 
% \Lambda_{S_1 S_1} (x, \theta) \, 
% \partial^{\theta}_{k_2} V_{S_1, 0} (x_S, \beta_{S_1}), 
% &  1 \le k_1 , k_2 \le N_{\beta_{S_1}}; \\[0.2cm] 
% 4 \bigl( \partial^{\theta}_{k_1} V_{S_2, 0} (x, \beta_{S_2}) \bigr)^\top \, 
% \Lambda_{S_2 S_2} (x, \theta) \, 
% \partial^{\theta}_{k_2} V_{S_2, 0} (x, \beta_{S_2}), 
% & N_{\beta_{S_1}} + 1 \le k_1 , k_2 \le N_{\beta_{S}}; \\[0.2cm]  
% 4 \bigl( \partial^{\theta}_{k_1} V_{R, 0} (x, \beta_R) \bigr)^\top \, 
% a_R^{-1} (x, \sigma) \, 
% \partial^{\theta}_{k_2} V_{R, 0} (x, \beta_R), 
% & N_{\beta_{S}} + 1 \le k_1 , k_2 \le N_{\beta}; \\[0.2cm]
% 0 & (\mrm{otherwise})
% \end{cases} 
% \end{align*}
% 
% from Lemma \ref{lemma:matrix}.  
% 
% since it holds that 
% % 
% \begin{align*}
% \partial_{\theta_k} \mu (x, \theta) =  
% \begin{bmatrix}
% \Sigma_{S_1 R} (x, \theta) \\[0.1cm]
% \Sigma_{S_2 R} (x, \theta) \\[0.1cm]  
% \Sigma_{R R} (x, \theta)  
% \end{bmatrix}
% a_R^{-1} (x, \sigma) \partial_{\theta_k} V_{R, 0} (x, \beta_R). 
% \end{align*}
% 
% 
For $1 \le k_1 \le N_{\beta}, \, N_{\beta} + 1 \le k_2 \le N_\theta$, it follows from (\ref{eq:partial_beta_S}), (\ref{eq:partial_beta_R}) and (\ref{eq:partial_sigma}) that 
% 
\begin{align*}
\mathscr{Y}_{k_1 k_2} (\trueparam) 
\probconv 0, 
\end{align*}
% 
if \limit. For $N_{\beta_R} + 1 \le k_1, k_2 \le N_{\theta}$, it follows that if \limit, then 
% 
\begin{align*} 
{\mathscr{Y}}_{k_1 k_2} (\trueparam) \probconv 
 \widetilde{\mathscr{Y}}_{k_1 k_2, 1} (\trueparam)
 + \widetilde{\mathscr{Y}}_{k_1 k_2, 2} (\trueparam) 
 + \widetilde{\mathscr{Y}}_{k_1 k_2, 3} (\trueparam),  
\end{align*}
% 
% 
where we have set:
% 
\begin{align*}
\widetilde{\mathscr{Y}}_{k_1 k_2, 1} (\trueparam)   
&\equiv  \sum_{1 \le j_1, j_2, j_3, j_4 \le N} \int   
[ \partial^{\theta}_{k_1} \Lambda(x, \trueparam) ]_{j_1 j_2} 
[ \partial^{\theta}_{k_2} \Lambda (x, \trueparam) ]_{j_3 j_4} \mathscr{W}_{j_1 j_2 j_3 j_4} (x, \trueparam) \, \truedist (dx); \\[0.2cm]
% 
% 
\widetilde{\mathscr{Y}}_{k_1 k_2, 2}  (\trueparam) 
%\\[0.2cm] & \qquad 
&\equiv \sum_{1 \le j_1, j_2 \le N} \int \Bigl\{ 
[\partial^{\theta}_{k_1} \Lambda(x, \trueparam)]_{j_1 j_2} 
[\Sigma (x, \trueparam)]_{j_1 j_2}
\partial^{\theta}_{k_2} \log |\Sigma (x, \trueparam)| \\[0.1cm]  
& \qquad \qquad \qquad \qquad \qquad \qquad  + 
[\partial^{\theta}_{k_2} \Lambda (x, \trueparam)]_{j_1 j_2} 
[\Sigma (x, \trueparam) ]_{j_1 j_2} 
\partial^{\theta}_{k_1} \log |\Sigma (x, \trueparam)| \Bigr\} \truedist (dx) \\[0.2cm]
&= - \sum_{1 \le j_1, j_2, j_3, j_4 \le N} 
\int \Bigl\{ 
[\partial^{\theta}_{k_1} \Lambda(x, \trueparam)]_{j_1 j_2} 
[ \Sigma (x, \trueparam) ]_{j_1 j_2}
[\partial^{\theta}_{k_2} \Lambda (x, \trueparam)]_{j_3 j_4}
[\Sigma (x, \trueparam)]_{j_3 j_4} \\[0.2cm] 
& \qquad \qquad \qquad 
+ [\partial^{\theta}_{k_2} \Lambda (x, \trueparam)]_{j_1 j_2} 
[ \Sigma (x, \trueparam) ]_{j_1 j_2} 
[ \partial^{\theta}_{k_1} \Lambda (x, \trueparam) ]_{j_3 j_4}
[\Sigma (x, \trueparam) ]_{j_3 j_4} \Bigr\} 
\truedist (dx);  
\\[0.3cm]  
% 
 \widetilde{\mathscr{Y}}_{k_1 k_2, 3} (\trueparam) 
&\equiv  \int 
\partial^{\theta}_{k_1}  \log |\Sigma (x, \trueparam)|  
\partial^{\theta}_{k_2}  \log |\Sigma (x, \trueparam)| \truedist (dx) \\[0.2cm]
&= \sum_{1 \le j_1, j_2, j_3, j_4 \le N} \int 
[ \partial^{\theta}_{k_1} \Lambda (x, \trueparam) ]_{j_1 j_2} 
[ \Sigma (x, \trueparam) ]_{j_1 j_2} \, 
[\partial^{\theta}_{k_2} \Lambda (x, \trueparam)]_{j_3 j_4} \, 
[ \Sigma (x, \trueparam) ]_{j_3 j_4}  \truedist (dx), 
\end{align*}
% 
with $\mathscr{W}_{j_1 j_2 j_3 j_4} : \mathbb{R}^N \times \Theta \to \mathbb{R}$ defined as follows, for 
$(x, \theta) \in \mathbb{R}^N \times \Theta$, 
% 
\begin{align*} 
\mathscr{W}_{j_1 j_2 j_3 j_4} (x, \theta) 
& = [ \Sigma (x, \theta) ]_{j_1 j_2} 
[ \Sigma (x, \theta) ]_{j_3 j_4}  
+ 
[ \Sigma (x, \theta)]_{j_1 j_3} 
[ \Sigma (x, \theta)]_{j_2 j_4} \\[0.2cm] 
& \qquad + 
[\Sigma (x, \theta)]_{j_1 j_4}
[\Sigma (x, \theta)]_{j_2 j_3}. 
\end{align*}
% 
Notice that we have used (\ref{eq:deriv_logdet}) in the computation of $\widetilde{\mathscr{Y}}_{k_1 k_2, 2} (\trueparam)$ and  $\widetilde{\mathscr{Y}}_{k_1 k_2, 3} (\trueparam)$. 
% 
Thus, we have 
% 
\begin{align*} 
 \sum_{m = 1}^3 \widetilde{\mathscr{Y}}_{k_1 k_2, m} (\trueparam)  
&  = \sum_{1 \le j_1, j_2, j_3, j_4 \le N} \int 
\Bigl\{ [\partial^{\theta}_{k_1} \Lambda (x, \trueparam)]_{j_1 j_2}
[ \partial^{\theta}_{k_2}  \Lambda  (x, \trueparam)]_{j_3 j_4} \\[0.2cm] 
& \qquad \qquad \times \bigl( 
[\Sigma (x, \trueparam)]_{j_1 j_3} 
[ \Sigma (x, \trueparam) ]_{j_2 j_4} 
+ 
[\Sigma (x, \trueparam)]_{j_1 j_4} 
[ \Sigma (x, \trueparam)]_{j_2 j_3} 
\bigr) \Bigr\}   
\truedist (dx) \\[0.2cm]
& = 2 \int \mathrm{tr} \bigl(  
\partial^{\theta}_{k_1} \Sigma(x, \trueparam) \, 
\Lambda (x, \trueparam) \, 
\partial^{\theta}_{k_2} \Sigma(x, \trueparam)  \, 
\Lambda (x, \trueparam)  \bigr) \, \truedist (dx),  
\end{align*} 
% 
where in the second equality, we have used the following formula:  
%
\begin{align*} 
 [\partial^{\theta}_k \Lambda (\trueparam)]_{j_1 j_2} 
 = - \sum_{1 \le j_3 j_4 \le N}  
 [\Lambda (x, \trueparam)]_{j_1 j_3}
 [\partial^{\theta}_k \Sigma (x, \trueparam)]_{j_3 j_4}
 [\Lambda (x, \trueparam)]_{j_4 j_2}, \ \  N_{\beta}  +  1 \le k \le N_{\theta}.    
\end{align*}
% 
Furthermore, for other cases of $1 \le k_1, k_2 \le N_{\theta}$, it holds that: 
% 
\begin{align*}
    \widetilde{\mathscr{Y}}_{k_1 k_2} (x, \trueparam) = 0, 
\end{align*}
% 
where we have used Lemma \ref{lemma:matrix} and that for $N_{\beta_S} + 1 \le k \le N_{\beta}$, 
% 
\begin{align*} 
\Lambda (x, \theta) \, \mu_k (x, \theta) 
& = \Lambda (x, \theta)  \begin{bmatrix}
\Sigma_{S_1 R} (x, \theta) \\[0.1cm]
\Sigma_{S_2 R} (x, \theta) \\[0.1cm]  
\Sigma_{R R} (x, \theta)  
\end{bmatrix}
a_R^{-1} (x, \sigma) \partial^{\beta}_k V_{R, 0} (x, \beta_R)  \\[0.2cm] 
& =  
\begin{bmatrix} 
\mathbf{0}_{N_{S_1}} \\[0.1cm]
\mathbf{0}_{N_{S_2}} \\[0.2cm] 
a_R^{-1} (x, \sigma) \partial^{\beta}_{k} V_{R, 0} (x, \beta_R) 
\end{bmatrix}, 
\quad x \in \mathbb{R}^N, \, \theta = (\beta_{S}, \beta_R, \sigma) \in \Theta. 
\end{align*}
The proof is now complete. 

\section{Proof for Case Study in Section \ref{sec:case_2}} \label{appendix:pf_case_study}
% 
From (\ref{eq:toy2_cont}),  we have $(\hat{\sigma}_n)^2 = F_{1, n} + F_{2, n} + F_{3, n}$, where  
% 
\begin{align*} 
    F_{1, n} 
    & \equiv \tfrac{6}{ \Delta_n^3} \times 
    \tfrac{1}{n} \sum_{i = 0}^{n-1} \bigl( \hat{p}_{i+1} - \hat{p}_i -  s_i \Delta_n  + s_i \tfrac{\Delta_n^2}{2}  \bigr)^2;  \nonumber \\ 
    F_{2, n} 
    & \equiv - \tfrac{6}{\Delta_n^2} \times 
    \tfrac{1}{n} \sum_{i = 0}^{n-1} 
    \bigl( \hat{p}_{i+1} - \hat{p}_i - s_i \Delta_n + s_i \tfrac{\Delta_n^2}{2} \bigr) \bigl( s_{i+1} - s_i - s_i \Delta_n  \bigr);   \nonumber \\ 
    F_{3, n}
    & \equiv \tfrac{2}{\Delta_n} \times \tfrac{1}{n} 
    \sum_{i = 0}^{n-1} 
    \bigl( s_{i+1} - s_i - s_i \Delta_n  \bigr)^2  
\end{align*}  
% 
with the hidden components $\hat{p}_i$ estimated by numerical differentiation: 
%  
\begin{align*}
   \hat{p}_i  = \frac{q_{i+1} - q_i}{\Delta_n},\quad  0 \le i \le n. 
\end{align*}
%
Since the rough component $s_t$ follows the linear SDE, the solution is explicitly given as: for $u \in [t_i, t_{i+1})$ 
% 
\begin{align*}
  s_{u} = s_{t_i}  e^{ - (u - t_i)} 
  + \truesigma \int_{t_i}^{u} e^{- (u  - v)} dB_v  
\end{align*}
% 
under the true parameter $\truesigma$. Thus, we have 
% 
\begin{align*}
\hat{p}_{i}  
 & = p_i + \tfrac{s_{i}}{\Delta_n} \int_{t_i}^{t_{i+1}} \int_{t_i}^{u} e^{- (v-t_i)} dv du    
 + \tfrac{\truesigma}{\Delta_n} \int_{t_i}^{t_{i+1}} \int_{t_i}^{u} \int_{t_i}^{v} e^{- (v-w)} d B_w dv du, 
\end{align*}
% 
%
and then  
% 
\begin{align*}
 & \hat{p}_{i+1} - \hat{p}_{i}  = p_{i+1} - p_i +  \tfrac{1}{\Delta_n} 
 \bigl( \Delta_n - ( 1  - e^{- \Delta_n}) \bigr) (s_{i+1} -s_i)  \nonumber  \\[0.2cm] 
 & \quad + \tfrac{\truesigma}{\Delta_n} \int_{t_{i+1}}^{t_{i+2}} \int_{t_{i+1}}^{u} \int_{t_{i+1}}^{v} e^{- (v-w)} d B_w dv du 
 - \tfrac{\truesigma}{\Delta_n} \int_{t_i}^{t_{i+1}} \int_{t_i}^{u} \int_{t_i}^{v} e^{- (v-w)} d B_w dv du, 
\end{align*}
% 
where we used: 
% 
\begin{align*}
\int_{t_i}^{t_{i+1}} \int_{t_i}^{u} e^{- (v- t_i)} dv du 
 = \Delta_n - (1 - e^{- \Delta_n}). 
\end{align*} 
% 
Since $\Delta_n \in [0, 1)$ is assumed to be small, we use the Taylor expansion for the terms $e^{-\Delta_n}, \, e^{-(v-w)}$ and the stochastic Taylor expansion of $(p_{i+1}, s_{i+1})$ around $(p_{i}, s_{i})$ under the true parameter $\truesigma$ to obtain  
% 
% 
\begin{align}
\begin{aligned}
& \hat{p}_{i+1} - \hat{p}_{i} 
= s_{i} \Delta_n + \truesigma B_{t_{i+1}-t_i} \tfrac{\Delta_n}{2} 
+ \truesigma \int_{t_i}^{t_{i+1}} \int_{t_i}^u d B_v du  \\[0.2cm]
& \qquad  + \tfrac{\truesigma}{\Delta_n} 
\int_{t_{i+1}}^{t_{i+2}} \int_{t_{i+1}}^u \int_{t_{i+1}}^v dB_w dv du 
 -  \tfrac{\truesigma}{\Delta_n} 
\int_{t_{i}}^{t_{i+1}} \int_{t_{i}}^u \int_{t_i}^v d B_w dv du + \Delta_n^2 \xi_i,
\end{aligned} \label{eq:diff_p}
\end{align}
% 
where the random variable $\{ \xi_i \}_i $ appearomg in the remainder term satisfies $ \mathbb{E} | \xi_i |^2 \leq C $ for some constant $C > 0$. 
% 
We express the Gaussian random variables as: 
% 
\begin{gather*}
B_{t_{i+1} - t_i} = \sqrt{\Delta_n} \times  z_i^{(1)}, \quad 
\int_{t_i}^{t_{i+1}} \int_{t_i}^u d B_v du = \sqrt{\Delta_n^3} \times \Bigl( \tfrac{z_{i}^{(1)}}{2} + \tfrac{z_{i}^{(2)}}{2 \sqrt{3}} \Bigr); \\[0.2cm]
\int_{t_{i}}^{t_{i+1}} \int_{t_{i}}^u \int_{t_i}^v d B_w dv du  = \sqrt{\Delta_n^5} \times 
\Bigl( \tfrac{z_{i}^{(1)}}{6}  
 + \tfrac{z_{i}^{(2)}}{4 \sqrt{3}} 
 + \tfrac{z_{i}^{(3)}}{12\sqrt{5}} \Bigr),  
\end{gather*}
% 
where $\{ z_{i}^{(j)} \}_{0 \le i  \le n+1, \, j = 1,2,3}$ is an i.i.d. sequence of standard normal random variables so that it holds
% 
\begin{gather*}
\mathbb{E} \bigl[ (B_{t_{i+1} - t_i})^2  \bigr] = \Delta_n, \quad
\mathbb{E} \Bigl[ B_{t_{i+1} - t_i} \times \Bigl(\int_{t_i}^{t_{i+1}} \int_{t_i}^u d B_v du \Bigr)  \Bigr] = \tfrac{\Delta_n^2}{2}; \\[0.2cm]  
\mathbb{E} \Bigl[ B_{t_{i+1} - t_i} \times \Bigl( \int_{t_{i}}^{t_{i+1}} \int_{t_{i}}^u \int_{t_i}^v d B_w dv du \Bigr)  \Bigr] = \tfrac{\Delta_n^3}{6}, 
\quad 
\mathbb{E} \Bigl[ \Bigl(\int_{t_i}^{t_{i+1}} \int_{t_i}^u d B_v du \Bigr)^2 \Bigr] = \tfrac{\Delta_n^3}{3}; \\[0.2cm]
\quad 
\mathbb{E} \Bigl[ \Bigl(\int_{t_i}^{t_{i+1}} \int_{t_i}^u d B_v du \Bigr) \times 
\Bigl( \int_{t_{i}}^{t_{i+1}} \int_{t_{i}}^u \int_{t_i}^v d B_w dv du \Bigr) \Bigr] = \tfrac{\Delta_n^4}{8}; \\[0.2cm] 
\quad 
\mathbb{E} \Bigl[ 
\Bigl( \int_{t_{i}}^{t_{i+1}} \int_{t_{i}}^u \int_{t_i}^v d B_w dv du \Bigr)^2 \Bigr] = \tfrac{\Delta_n^5}{20}.  
\end{gather*}
% 
Then,  (\ref{eq:diff_p}) is written as: 
% 
% 
\begin{align}
& \hat{p}_{i+1} - \hat{p}_{i}  
=  s_{i} \Delta_n 
+  \truesigma \sqrt{\Delta_n^3}  
\tfrac{z_{i}^{(1)}}{2} 
+  \truesigma \sqrt{\Delta_n^3}  
\Bigl( \tfrac{z_{i}^{(1)}}{2} + \tfrac{z_{i}^{(2)}}{2 \sqrt{3}}  \Bigr)  \nonumber \\
& \qquad  + \truesigma \sqrt{\Delta_n^3}  
 \Bigl( \tfrac{z_{i+1}^{(1)}}{6}  
 + \tfrac{z_{i+1}^{(2)}}{4 \sqrt{3}} 
 + \tfrac{z_{i+1}^{(3)}}{12\sqrt{5}} \Bigr)   
 - \truesigma \sqrt{\Delta_n^3} 
 \Bigl( \tfrac{z_{i}^{(1)}}{6}  
 + \tfrac{z_{i}^{(2)}}{4 \sqrt{3}} 
 + \tfrac{z_{i}^{(3)}}{12\sqrt{5}} \Bigr) 
 + \Delta_n^2 \xi_i  \nonumber  \\[0.2cm] 
& = s_{i} \Delta_n + \truesigma \sqrt{\Delta_n^3} 
\Bigl\{ \Bigl( \tfrac{z_{i+1}^{(1)}}{6}  
 + \tfrac{z_{i+1}^{(2)}}{4 \sqrt{3}} 
 + \tfrac{z_{i+1}^{(3)}}{12\sqrt{5}} \Bigr) 
 + \Bigl( \tfrac{5 z_{i}^{(1)}}{6}  
 + \tfrac{z_{i}^{(2)}}{4 \sqrt{3}} 
 - \tfrac{z_{i}^{(3)}}{12\sqrt{5}} \Bigr)  
\Bigr\} + \Delta_n^2 \xi_i.    \label{eq:diff_p_2}
\end{align} 
%
% 
From the ergodicity of the process $\{s_t\}_t$ and (\ref{eq:diff_p_2}), we have that, 
as $n \to \infty$, $\Delta_n \to 0$ and $n \Delta_n \to \infty$, 
% 
\begin{align} 
F_{1,n}  & = 
% \tfrac{6}{ \hat{\lambda}_n^2} 
% \times \left( \tfrac{\lambda^\dagger - \hat{\lambda}_n}{\sqrt{\Delta_n}} \right)^2 
% \times \tfrac{1}{n} \sum_{i = 0}^{n-1} s_i^2 \nonumber \\[0.2cm]
% & \quad + \tfrac{12}{ \hat{\lambda}_n^2} 
% \times \left( \tfrac{\lambda^\dagger - \hat{\lambda}_n}{\sqrt{\Delta_n}} \right)
% \times \tfrac{\lambda^\dagger \truesigma}{n} \sum_{i = 0}^{n-1} s_i \Bigl( \tfrac{z_{i+1}^{(1)}}{6} 
%  + \tfrac{z_{i+1}^{(2)}}{4 \sqrt{3}} 
%  + \tfrac{z_{i+1}^{(3)}}{12\sqrt{5}}  
%  + \tfrac{5}{6}z_{i}^{(1)}
%  + \tfrac{z_{i}^{(2)}}{4 \sqrt{3}} 
%  - \tfrac{z_{i}^{(3)}}{12\sqrt{5}}    
% \Bigr)  \nonumber \\[0.2cm]
 \tfrac{6 (\truesigma)^2 }{n} \sum_{i = 0}^{n-1} \Bigl( \tfrac{z_{i+1}^{(1)}}{6} 
 + \tfrac{z_{i+1}^{(2)}}{4 \sqrt{3}} 
 + \tfrac{z_{i+1}^{(3)}}{12\sqrt{5}}  
 + \tfrac{5}{6}z_{i}^{(1)}
 + \tfrac{z_{i}^{(2)}}{4 \sqrt{3}} 
 - \tfrac{z_{i}^{(3)}}{12\sqrt{5}}    
\Bigr)^2  + \tfrac{1}{n} \sum_{i = 0}^n R_i^{(1)} (\Delta_n)  \nonumber \\[0.2cm] 
& \probconv \tfrac{23}{5} (\truesigma)^2;  \label{eq:conv_F1} \\[0.2cm] 
F_{2, n} 
% & = - \tfrac{6}{\hat{\lambda}_n } \times 
% \left( \tfrac{\lambda^\dagger - \hat{\lambda}_n}{\sqrt{\Delta_n}} \right) \times \tfrac{1}{n \sqrt{\Delta_n}} \sum_{i = 0}^{n-1} s_i \bigl( s_{i+1} - s_{i} + s_i \Delta_n \bigr) \nonumber \\[0.2cm]
& = - \tfrac{6 (\truesigma)^2}{n} \sum_{i = 0}^{n-1}
 \Bigl( \tfrac{z_{i+1}^{(1)}}{6} 
 + \tfrac{z_{i+1}^{(2)}}{4 \sqrt{3}} 
 + \tfrac{z_{i+1}^{(3)}}{12\sqrt{5}}  
 + \tfrac{5}{6}z_{i}^{(1)}
 + \tfrac{z_{i}^{(2)}}{4 \sqrt{3}} 
 - \tfrac{z_{i}^{(3)}}{12\sqrt{5}}    
\Bigr) z_i^{(1)} 
+ \tfrac{1}{n} \sum_{i = 0}^n R_i^{(2)} (\Delta_n)  \nonumber \\ & \probconv - 5 (\truesigma)^2, \label{eq:conv_F2}
\end{align}
% 
where each $\{ R_i^{(1)} (\Delta_n) \}_i$ and $\{R_i^{(2)} (\Delta_n) \}_i$ is sequence of random variables such that for $0 \le i \le n$, $j = 1,2$, 
% 
$$  \{ \mathbb{E} [ |R_i^{(j)} (\Delta_n) |^2 ]  \} \leq C \Delta_n
$$
for some constant $C > 0$. Similarly, we have that 
%
\begin{align} \label{eq:conv_F3}
 F_{3, n}  \probconv  2 (\truesigma)^2.   
\end{align}
%  
From (\ref{eq:conv_F1}), (\ref{eq:conv_F2}) and (\ref{eq:conv_F3}), we immediately obtain the convergence (\ref{eq:conv_toy_2}). 
%
\section{Kalman Filter for Sub-Class of (\ref{eq:hypo-II})} \label{appendix:kalman} 
For simplicity, we write $x_i = (x_{S_1,i}, x_{S_2, i}, x_{R,i}) \in \mathbb{R}^N = \mathbb{R}^{N_{S_1}} \times  \mathbb{R}^{N_{S_2}} \times   \mathbb{R}^{N_{R}}$ for the state of scheme (\ref{eq:scheme_linear}) at time $t_i$. Component $x_{S_1,i}$ is observable and $h_{i} = (x_{S_2, i}, x_{R, i}) \in \mathbb{R}^{N_H}$, $N_H = N_{S_1} + N_R$, is the hidden component,  in agreement with applications. 
Thus, scheme (\ref{eq:scheme_linear}) is now expressed as
% 
\begin{align} \label{eq:scheme_linear_appendix}
x_{i+1} 
=  b (\Delta_n, x_{S_1, i}, \theta)  
+ A (\Delta_n, {x}_{S_1, i}, \theta) h_i
+ w (\Delta_n, \theta).  
\end{align}
% 
We set $\Sigma (\Delta_n, \theta) = \mathrm{E}\,[\,w (\Delta_n, \theta) w (\Delta_n, \theta)^\top] $ and assume that $h_0 | x_{S_1, 0} \sim  \mathscr{N} ({m}_0, {Q}_0)$ for some 
${m}_0 \in \mathbb{R}^{N_H}$ and ${Q}_0 \in \mathbb{R}^{N_H \times N_H}$. Then, the filtering formula and the marginal likelihood are obtained as follows.
% 
\begin{itemize}
\item \emph{Filtering Recursion}:  We have that
% 
\begin{align} 
\label{eq:filter}
    h_{k} | x_{S_1, 0:k} \sim \mathscr{N} ({m}_k, {Q}_k), 
    \quad 0 \le k \le n, 
\end{align}
% 
with the filter mean ${m}_k$ and covariance ${Q}_k$ given as: 
% 
\begin{align*} 
{m}_k & =  \mu_{H, k-1} + \Lambda_{H S_1, k-1} \,  \Lambda^{-1}_{S_1 S_1, k-1} \,  \bigl( x_{S_1, k} -  \mu_{S_1, k-1} \bigr); \\[0.2cm]
Q_k & = \Lambda_{HH, k-1} - \Lambda_{H S_1, k-1} \, 
\Lambda_{S_1 S_1, k-1}^{-1} \Lambda_{S_1 H, k-1},
\end{align*}
% 
where $\mu_{H, k-1} \in \mathbb{R}^{N_{H}}$, $\mu_{S_1, k-1} \in \mathbb{R}^{N_{S_1}}$, $\Lambda_{S_1 S_1, k-1} \in \mathbb{R}^{N_{S_1} \times N_{S_1} }$, $\Lambda_{S_1 H, k-1} \in \mathbb{R}^{N_{S_1} \times N_{H}}$, $\Lambda_{H S_1, k-1} \in \mathbb{R}^{N_{H} \times N_{S_1} }$, $\Lambda_{H H, k-1} \in \mathbb{R}^{N_{H} \times N_{H}}$ are found via the following equations:
% 
\begin{align*}
\mu_{k-1} & =  
\begin{bmatrix}
    \mu_{S_1, k-1} \\[0.1cm]
    \mu_{H, k-1} 
\end{bmatrix}
= 
b (\Delta_n, x_{S_1, k-1}, \theta) + A (\Delta_n, x_{S_1, k-1}, \theta) 
m_{k-1}; \\[0.2cm] 
\Lambda_{k-1} 
& = 
\begin{bmatrix}
    \Lambda_{S_1 S_1, k-1}  & \Lambda_{S_1 H, k-1}  \\ 
    \Lambda_{H S_1, k-1}  & \Lambda_{H H, k-1}
\end{bmatrix} \\[0.2cm] 
& = \Sigma (\Delta_n, \theta) + A (\Delta_n, x_{S_1, k-1}, \theta) \, Q_{k-1} \, A (\Delta_n, x_{S_1, k-1}, \theta)^\top. 
\end{align*}
% 
\item \emph{Marginal likelihood}: For a given initial distribution $p_{\theta} (x_{S_1, 0})$, we have that
%
\begin{align} \label{eq:marginal_likelihood}
 p_{\theta} (x_{S_1, 0:n}) = p_{\theta} (x_{S_1, 0}) \times 
\prod_{k = 1}^n p_{\theta} (x_{S_1, k} | x_{S_1, 0:k-1}), 
\end{align}
% 
where $p_{\theta} (x_{S_1, k} | x_{S_1, 0:k-1})$ is the density of $x_{S_1, k}$ given $x_{S_1, 0:k-1}$ whose conditional distribution is given by:  
% 
\begin{align*}
x_{S_1, k} \, | \, x_{S_1, 0:k-1} \sim \mathscr{N} (\mu_{S_1, k-1}, \, \Lambda_{S_1 S_1, k-1}). 
\end{align*}
\end{itemize}
% 
% 
\subsection{Derivation of Filter (\ref{eq:filter})} 
% 
We assume that the filter in the previous time step is obtained as: 
% 
\begin{align} \label{eq:previous_filter}
h_{k-1} | x_{S_1, 0:k-1} \sim \mathscr{N} (m_{k-1}, Q_{k-1}). 
\end{align}
% 
It follows that 
% 
\begin{align*}  
p_\theta (h_k | x_{S_1, 0:k} )
= \frac{p_\theta (x_k | x_{S_1, 0:k-1} ) }{p_\theta (x_{S_1, k} | x_{S_1, 0:k-1})}, 
\end{align*}
% 
and 
% 
\begin{align} \label{eq:conv_filter}
p_\theta (x_k | x_{S_1, 0:k-1} )  
& = \int p_\theta (x_k, h_{k-1} | x_{S_1, 0:k-1} )  d h_{k-1}  \nonumber  \\[0.1cm]
& = \int p_\theta (x_k | x_{k-1} )  p_\theta (h_{k-1} | x_{S_1, 0:k-1} )  d h_{k-1}. 
\end{align}
% 
From the definition of scheme (\ref{eq:scheme_linear_appendix}), we have that 
% 
\begin{align} \label{eq:scheme_dist}
x_k | x_{k-1} \sim \mathscr{N} \bigl( b (\Delta_n, x_{S_1, k-1}, \theta)  
+ A (\Delta_n, {x}_{S_1, k-1}, \theta) h_{k-1}, \Sigma (\Delta_n, \theta) \bigr). 
\end{align}
% 
From (\ref{eq:previous_filter}), (\ref{eq:conv_filter}) and (\ref{eq:scheme_dist}), we obtain 
% 
\begin{align} \label{eq:state_given_obs}
x_k | x_{S_1, 0:k-1} 
\sim \mathscr{N} \bigl(\mu_{k-1}, \Lambda_{k-1}\bigr), 
\end{align} 
% 
where
% 
\begin{gather*}
\mu_{k-1} 
= b (\Delta_n, x_{S_1, k-1}, \theta)  
+ A (\Delta_n, {x}_{S_1, k-1}, \theta) m_{k-1}; \\[0.2cm]
\Lambda_{k-1} = \Sigma (\Delta_n, \theta) + 
A (\Delta_n, {x}_{S_1, k-1}, \theta) 
\, Q_{k-1}  \, 
A (\Delta_n, {x}_{S_1, k-1}, \theta)^\top. 
\end{gather*}
% 
Finally, applying the conditional Gaussian distribution formula, we obtain (\ref{eq:filter}).
% 
% 
\subsection{Derivation of the Marginal Likelihood (\ref{eq:marginal_likelihood})}
% 
From the marginal of the Gaussian distribution (\ref{eq:state_given_obs}) for $x_{k} | x_{S_1, 0:k-1}$, we immediately obtain: 
% 
\begin{align*} 
x_{S_1, k} | x_{S_1, 0:k-1} \sim \mathscr{N} 
\bigl( \mu_{S_1, k-1}, \,  \Lambda_{S_1 S_1, k-1} \bigr), 
\end{align*}
% 
where $\mu_{S_1, k-1} = \mrm{proj}_{1, N_{S_1}} ( \mu_{k-1}) $, and $\Lambda_{S_1 S_1, k-1} = \bigl[\Lambda_{k-1}^{ij} \bigr]_{1 \le i,j \le N_{S_1}}$. 

\end{appendices}

\bibliographystyle{rss}
\bibliography{draft_jrssb}


%USE THE BELOW OPTIONS IN CASE YOU NEED AUTHOR YEAR FORMAT.
% \bibliographystyle{abbrvnat}
% \bibliography{reference}



%% sample for biography with author's image
% \begin{biography}{{\color{black!20}\rule{77pt}{77pt}}}{\author{Author Name.} This is sample author biography text. The values provided in the optional argument are meant for sample purposes. There is no need to include the width and height of an image in the optional argument for live articles. This is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text.}
% \end{biography}

% %% sample for biography without author's image
% \begin{biography}{}{\author{Author Name.} This is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text.}
% \end{biography}

\end{document}