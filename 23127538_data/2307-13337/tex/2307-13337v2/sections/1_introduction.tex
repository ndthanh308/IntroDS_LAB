\section{Introduction}
\label{sec:introduction}


Image super-resolution (SR) is a core low-level vision task aimed at reconstructing high-resolution (HR) images from their corresponding low-resolution (LR) counterparts.
Recent advances in deep learning~\cite{dong2015image,kim2016accurate,lim2017enhanced,zhang2018rcan,zhang2018residual,liang2021swinir,chen2023activating,park2023content,son2021toward} have led to astonishing achievements in producing high-fidelity images.
However, the remarkable performance is based on heavy network architectures that incur significant computational costs, limiting practical viability, such as mobile deployment.

To mitigate the computational complexity of neural networks, quantization has emerged as a promising avenue. 
Network quantization has proven effective in reducing computational costs without significant accuracy loss, particularly in high-level vision tasks, such as image classification~\cite{choi2018pact,hou2018loss,zhou2016dorefa}.
However, when it comes to quantizing SR networks to lower bit-widths, substantial performance degradation~\cite{ignatov2021real} often occurs, posing a persistent and challenging problem to be addressed.


\input{sections/tables/intro-first}

This degradation can be attributed to the significant variance in the feature (activation) distributions of SR networks.
The feature distribution of a layer exhibits substantial discrepancies across different channels and images, which makes it difficult to determine a single quantization range for a layer.
Early approaches to SR quantization~\cite{Li2020pams} adopt a training scheme to learn the quantization range of each layer.
However, despite careful selection, the quantization ranges do not align with the varied values within the channel and image dimensions, which we refer to as \textit{distribution mismatch} in features.


Recent approaches aim to address this issue by incorporating dynamic adaptation modules that accommodate the varying distributions.
For example, the quantization range is dynamically adjusted by directly leveraging the distribution mean and variance at test time~\cite{hong2022daq} or by employing input-adaptive dynamic modules~\cite{zhong2022ddtb}.
Although adapting the quantization function to each image during inference might handle variable distributions, the dynamic modules introduce considerable computational overhead, potentially compromising the computational benefits of quantization.


In this study, we propose a novel quantization-aware training framework that addresses the distribution mismatch problem with a loss term that regulates the mismatch in distributions.
Although directly minimizing the feature mismatch presents the potential for quantization-friendliness, whether it preserves reconstruction accuracy is questionable. 
We observe that concurrent optimization with the mismatch regularization and the original reconstruction loss can disrupt the image reconstruction process.
Therefore, we introduce a cooperative mismatch regularization strategy, where the mismatch is regulated only when it collaborates harmoniously with the reconstruction loss. 
To determine the cooperative behavior, we assess the cosine similarity of the gradients from each loss, then we weigh the gradients of mismatch regularization based on this similarity.
Consequently, we effectively update the SR network to hold both quantization-friendliness and reconstruction accuracy. 


Furthermore, we identify the distribution mismatch among the weights of different layers.
We discover that employing a fixed policy to determine the layer-wise weight quantization range~\cite{Li2020pams,hong2022daq,hong2022cadyq,tian2023cabm} can be suboptimal and that a further precise range can be obtained by considering both the current distribution of weights and the distinct tendencies of each layer.
Therefore, we additionally incorporate layer-specific variations using a correction parameter for each layer.
This strategy allows us to accurately find the quantization range for weights while incurring only a minimal overhead (0.01\% additional storage size and no additional bitOPs)--a significantly smaller impact compared to methods that use dynamic modules.
Overall, the contributions of our work include the following:
\begin{itemize}
    \item[$\bullet$] We introduce the first quantization framework to address the distribution mismatch problem in SR networks without dynamic modules, as compared in \Cref{tab:intro-first}. Our framework updates the SR network to be quantization-friendly and accurate simultaneously.
    \item[$\bullet$] Based on the observations of the distribution mismatch in SR networks, we effectively reduce the mismatch by introducing a cooperative mismatch regularization term and a weight clipping correction term.
    \item[$\bullet$] Compared to existing approaches to SR quantization, ours achieves state-of-the-art performance with similar or fewer computations. 
\end{itemize}


