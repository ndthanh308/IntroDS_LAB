\section{Experiments}
\label{sec:experiments}

\input{sections/tables/exp-edsr}
\input{sections/tables/exp-rdn}
\input{sections/tables/exp-swinir}


The efficacy and adaptability of the proposed quantization framework, ODM, are assessed through its application across several SR networks.
The experimental settings are described (\Cref{subsec:setting}), and quantitative (\Cref{subsec:quan}), qualitative (\Cref{subsec:qual}), and complexity (\Cref{subsec:complexity}) evaluations are conducted on various SR networks. 
Ablation studies are conducted to examine each component of the framework (\Cref{subsec:ablation}).

\subsection{Implementation Details}
\label{subsec:setting}

\subsubsection{Models and Training.}
The proposed framework is applied directly to existing representative SR networks that produce satisfactory SR results, but involve heavy computations: EDSR (baseline)~\cite{lim2017enhanced} and RDN~\cite{zhang2018residual}.
Furthermore, we apply our method to the Transformer-based SR model, SwinIR-S~\cite{liang2021swinir}.
Following prior works on SR quantization~\cite{Li2020pams,ma2019efficient,xin2020binarized,hong2022daq,zhong2022ddtb,hong2022cadyq}, weights and activations of the high-level feature extraction module are quantized, which is the most computationally demanding. 
Training and validation are conducted using the DIV2K~\cite{agustsson2017ntire} dataset. 
ODM trains the network for 60K iterations with a batch size of 8.
The weights are updated with an initial learning rate of $\beta^0\Equal10^{-4}$. For cooperative update, we update clipping parameters with $10\cdot\beta^0$ initial learning rate.
The learning rates are halved every 15K iteration.
The hyperparameter for the percentile is set to $j=99$, and to balance the gradient of the loss terms, we set $\lambda_{R}\Equal1$ and $\lambda_{M}\Equal10^{-5}$.
Specially, we set $\lambda_{M}\Equal10^{-6}$ for RDN whose overall mismatch is large.
Ablation studies on the hyperparameters are in the supplementary material.
All our experiments are implemented using PyTorch and run on an RTX 2080Ti GPU.


\subsubsection{Evaluation.}
We evaluate our framework on the standard benchmark (Set5~\cite{bevilacqua2012low}, Set14~\cite{ledig2017photo}, BSD100~\cite{martin2001database}, and Urban100~\cite{huang2015single}) by measuring the peak signal-to-noise ratio (PSNR) and the structural similarity index (SSIM~\cite{wang2004image}).
To assess the computational complexity of our framework, we measure bitOPs and storage size. 
BitOPs refers to the number of operations weighted by the bit-widths of the two operands.
Storage size is calculated as the number of stored parameters weighted by the precision of each parameter value.



\subsection{Quantitative Results}
\label{subsec:quan}

To evaluate the effectiveness of our proposed scheme, we compare the results with existing SR quantization works using their official codes: PAMS~\cite{Li2020pams}, DAQ~\cite{hong2022daq}, and DDTB~\cite{zhong2022ddtb}. 
For a fair comparison, we reproduce other methods using the same training iterations, 60K iterations.
The supplementary materials provide additional experiments that further demonstrate the applicability of ODM, including results on 300K iterations, scale 2, and fully quantized settings.


\noindent \textbf{EDSR. }
As shown in \Cref{tab:exp-edsr}, ODM outperforms other methods in the 4, 3, and 2-bit settings, and notably, the improvement is significant for 2-bit, achieving a gain of more than 0.49 dB for Set5.
We notice that 4-bit EDSR-ODM achieves closer accuracy to 32-bit EDSR, with a marginal difference of 0.1 dB for Set5.
This indicates that ODM can effectively bridge the gap between the quantized network and the 32-bit network.

\noindent \textbf{RDN. }
Similarly, \Cref{tab:exp-rdn} compares the results on RDN, whose computational complexity is more burdensome than EDSR.
The results show that ODM consistently achieves superior performance on 4, 3, and 2-bit quantization.
The gain over existing methods is especially large for the 2-bit setting, where it exceeds 0.66 dB for Set5.

\noindent \textbf{SwinIR. }
Furthermore, we evaluate our framework on the Transformer-based architecture, SwinIR.
The linear and convolutional layers of SwinIR are quantized.
According to \Cref{tab:exp-swinir}, ODM is also proven effective in quantizing SwinIR across all bit settings, where the improvement is most notable in the 2-bit setting (0.43 dB).




\noindent \textbf{Comparison with QuantSR. }
We also compare our method with the concurrent work, QuantSR~\cite{qin2024quantsr}.
As the training code of QuantSR has not been released, we base our comparison on the reported performance.
For a fair comparison with QuantSR's reported performance, we also train our model for 300K iterations on SRResNet~\cite{ledig2017photo} and SwinIR~\cite{liang2021swinir}.
In \Cref{tab:exp-quantsr}, the results demonstrate that our method achieves better results than QuantSR;
compared to QuantSR, ours shows a gain of 0.51 dB on 2-bit SRResNet and a gain of 0.14 dB on 2-bit SwinIR for Set5.


\input{sections/tables/exp-quantsr}
\input{sections/tables/exp-complexity}
\input{sections/figures/exp-qual}


\subsection{Complexity Analysis}
\label{subsec:complexity}
Along with the accuracy of SR, we also evaluate the computational complexity of our framework in \Cref{tab:exp-complexity}.
We measure the storage size for the model weights and the bitOPs required for generating a 1920$\times$1080 image with $\times4$ SR network.
Overall, our framework, ODM, achieves higher restoration accuracy with similar or fewer computational resources.
Specifically, our weight-clipping correction involves an additional storage size of 0.06K / 0.15K for EDSR / RDN.
Since the correction process can be predetermined before test time, no extra bitOPs are required.
Compared to existing methods, our method achieves $\times$31.7 smaller storage size overhead than DDTB and 0.5T fewer bitOPs than DAQ on EDSR.
For RDN, the gap is larger; our method's overhead is $\times$304.7 smaller in storage size than DDTB and 51.1T fewer in bitOPs than DAQ.
Moreover, we note that DAQ adopts a channel-wise dynamic quantization function and DDTB an asymmetric weight quantization function, which are not favorably supported by hardware.
Despite incurring a minor additional storage size of 0.06K / 0.15K over PAMS, the accuracy improvement over PAMS is significant (+1.99 dB / +1.64 dB).
The results prove that our method achieves significant accuracy gain with minimal or no computational overhead.



\subsection{Qualitative Results}
\label{subsec:qual}

\Cref{fig:exp-qualitative} provides qualitative results and comparisons with the output images of quantized EDSR, RDN, and SwinIR.
Our method, ODM, produces visually cleaner output images compared to existing methods.
In contrast, existing methods, especially PAMS, suffer from blurred lines or artifacts.
These qualitative results stress the importance of alleviating the distribution mismatch problem in SR networks.





\subsection{Ablation Study}
\label{subsec:ablation}


In \Cref{tab:exp-ablation}, we verify the importance of each attribute of our framework: cooperative mismatch regularization and weight clipping correction.
According to the results, each attribute individually improves baseline accuracy.
Weight clipping correction improves the baseline by {+1.18 dB} for Set5, indicating that considering both the layer-wise trend and the current weight distribution is important for weight quantization.
Also, while directly integrating mismatch regularization with the reconstruction loss (Model (\textbf{d})) rather degrades performance by -0.13 dB, our cooperative scheme (Model (\textbf{e})) improves the SR accuracy by {+0.51 dB}.
This shows that reducing the mismatch in both activations and weights is important for accurately quantizing SR networks.

\input{sections/tables/exp-ablation}
\input{sections/figures/exp-dist}

Furthermore, we visualize the feature distributions in \Cref{fig:exp-dist} to validate the importance of our cooperative regularization term.
After training only with the reconstruction loss, the outliers remain far from the quantization grid.
When mismatch regularization loss is simply added to the original reconstruction loss, the activation distribution falls into a narrower range and resembles a multi-modal distribution near the quantization grids. 
Although this distribution is quantization-friendly, it significantly deviates from the original activation of the 32-bit network and removes originally dense values (near 0).
This can lead to an accuracy drop, as demonstrated in \Cref{tab:exp-ablation} (\textbf{d}).
In contrast, our cooperative mismatch regularization results in quantization-friendly distribution while largely preserving the activations in the dense region of the original 32-bit network.




