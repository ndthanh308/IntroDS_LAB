\title{Supplementary Material for\texorpdfstring{\\}{} ``Overcoming Distribution Mismatch\texorpdfstring{\\}{} in Quantizing Image Super-Resolution Networks''}
\titlerunning{ODM}
\author{Cheeun Hong\inst{1}\orcidlink{0009-0009-3480-748X} \and
Kyoung Mu Lee\inst{1,2}\orcidlink{0000-0001-7210-1036}}
\authorrunning{C.~Hong and K.M.~Lee}
\institute{
Dept. of ECE \& ASRI, \email{\{cheeun914, kyoungmu\}@snu.ac.kr} \and
IPAI, Seoul National University
}
\maketitle

\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\theequation}{S\arabic{equation}}


In this supplementary material, we present additional experimental results in \Cref{sec:sup-experiments}; additional ablation study in \Cref{sec:sup-ablation}; additional analyses in \Cref{sec:sup-analyses}.


\section{Additional Experiments}
\label{sec:sup-experiments}
Along with the evaluations on SR networks of scale $\times4$ discussed in the main manuscript, we also assess our framework on networks of scale $\times2$.
As shown in Table~\ref{tab:sup-scale2}, our framework outperforms existing SR quantization methods in terms of both PSNR and SSIM, demonstrating its effectiveness on scale $\times2$ SR networks.
Specifically, the PSNR gain on Set5 is 0.28 dB on EDSR, 0.74 dB on RDN, and 0.25 dB on SwinIR.
These results confirm that our framework is effective for both CNN- and Transformer-based SR networks of scale 2.
\input{sections/tables_sup/sup-scale2}


Furthermore, we compare our method with existing methods by training each method for 300K iterations.
The results in \Cref{tab:sup-ep300} show that the gains achieved by our approach for 60K iterations reported in the main manuscript are maintained.
Our framework still achieves more than a 0.37 dB gain over other methods for Set5 when trained for extended iterations.
\input{sections/tables_sup/sup-ep300}


Also, to ensure a fair comparison with DAQ~\cite{hong2022daq}, we follow their settings and apply our method to EDSR of 32 residual blocks with 256 channel dimensions.
As shown in \Cref{tab:sup-fulledsr}, our method outperforms DAQ even though DAQ employs a channel-wise quantization function, whereas our method utilizes a more efficient layer-wise function.
\input{sections/tables_sup/sup-fulledsr}


Moreover, we compare our method with a fully-quantized SR network, EDSR-FQSR~\cite{Wang2021fully}, in which all layers and also the skip connections are quantized.
For a fair comparison, we also quantize all convolutional layers and the skip connections.
The results in \Cref{tab:sup-fully} show that our ODM outperforms FQSR, indicating that our approach is also effective when the network is fully quantized.
\input{sections/tables_sup/sup-fully}

Additionally, along with SwinIR, as demonstrated in the main manuscript, we also apply our method to a more recent, large Transformer-based model, HAT~\cite{chen2023activating} ($\sim$10M parameters).
The results in Table~\ref{tab:sup-hat} indicate that our method can be effectively applied to Transformer models.
\input{sections/tables_sup/sup-hat}

\section{Additional Ablation Study}
\label{sec:sup-ablation}
In this section, we present an ablation study on the hyperparameters of our framework.
First, we conduct an ablation study on the percentile $j$, which is used to initialize quantization range clipping parameters.
The results in \Cref{tab:sup-ablation-p} show that when $j$=100, meaning the quantization range is not clipped and is determined by the maximum value, accuracy severely degrades.
This hints that clipping is important for performance and that using the max function does not serve as an effective initialization policy.

Also, we analyze the impact of the gradient balance terms $\lambda_R$ and $\lambda_M$, which balances the gradient of the reconstruction loss and the mismatch regularization loss, and initial learning rate $\beta^0$.
The results in \Cref{tab:sup-ablation-m,tab:sup-ablation-r} support our choice of $\lambda_R$=1, $\lambda_M$=1e-5, and $\beta^0$=1e-4.
\input{sections/tables_sup/sup-ablation}

Furthermore, we compare our weight clipping correction scheme with the commonly used quantization scheme for weights, LSQ~\cite{esser2019learned}.
The results in \Cref{tab:sup-ablation-lsq} validate the effectiveness of our clipping correction approach.
\input{sections/tables_sup/sup-ablation-lsq}

\section{Additional Analyses}
\label{sec:sup-analyses}
\subsection{Complexity Analysis}
\label{subsec:sup-complexity}
We provide additional complexity analysis on SwinIR in \Cref{tab:sup-complexity}.
The results show that our method achieves superior SR performance with minimal or no computational overhead in terms of model storage size and bitOPs.
BitOPs for SwinIR are calculated for processing a 64$\times$64 input patch.
\input{sections/tables_sup/sup-complexity}

\subsection{Distribution mismatch}
For the generalizability of the observed distribution mismatch problem of the main manuscript, we analyze the variance of features in SR networks across a set of images.
As reported in \Cref{tab:sup-analyses-mismatch}, the classification network (ResNet20) exhibits much less image-wise and channel-wise variance compared to SR networks (EDSR, RDN).
This suggests that the distribution mismatch problem is particularly severe in SR networks. 
\input{sections/tables_sup/sup-analyses-mismatch}


Moreover, we analyze the feature mismatch after quantization-aware training (QAT) using different methods.
To track feature similarity, we compute the variance between feature distribution statistics (mean and standard deviation) within the benchmark dataset.
As shown in \Cref{tab:sup-analyses-mismatch-reduction}, the feature mismatch is effectively reduced with our framework.
\input{sections/tables_sup/sup-analyses-mismatch-reduction}



% We provide additional analysis supporting the choice of distance from the quantization grid as a measure of distribution mismatch in Eq.~(2) of the main manuscript.
We provide additional analysis supporting the choice of distance from the quantization grid as a measure of distribution mismatch in \cref{eq:mismatch} of the main manuscript.
We note that QAT is a process searching for a quantization grid that best fits the discrepant input distributions. 
If the average distance of each feature from the quantization grid is small, it implies that most features are aligned with the grid, indicating a low distribution mismatch.
According to \Cref{tab:sup-analyses-mismatch-reduction}, it is verified that using distance as the mismatch measure reduces the variance in feature distribution statistics across test images.



\subsection{Cooperative Regularization}
\label{subsec:sup-cooperative}
In the main manuscript, we emphasized the importance of \textit{cooperatively} using mismatch regularization and reconstruction losses.
For the cooperative update, we weigh the gradient of mismatch regularization using the cosine similarity with the gradient of the reconstruction loss.
The weighing term is formulated as $0.5 \cdot (cos(\vv_a, \vv_b)+1)$.
Here, we present results using the weighing term as $cos(\vv_a, \vv_b)$ following Du~\etal~\cite{du2018adapting} and that of $u(cos(\vv_a, \vv_b))$ where $u(\cdot)$ is the unit step function.
The results in \Cref{tab:sup-cooperative} show that all these functions that alleviate the conflict between the two losses achieve high reconstruction accuracy, indicating that the general cooperative property is the key to performance gain.
\input{sections/tables_sup/sup-cooperative}


\subsection{More Visualizations}
\label{subsec:sup-visualizations}
For better comprehension, we provide additional results of the effect of our loss after training in \Cref{fig:sup-dist}.
% Along with the results in Figure~{5} of the main manuscript, these results show that our loss term updates the activation distributions to a further quantization-friendly state while mostly preserving the high-density values of the original distribution.
Along with the results in \Cref{fig:exp-dist} of the main manuscript, these results show that our loss term updates the activation distributions to a further quantization-friendly state while mostly preserving the high-density values of the original distribution.
\input{sections/figures_sup/sup-dist}


Moreover, we provide visualizations of layer-wise mismatch in weights for different SR networks in \Cref{fig:sup-weight}.
According to the visual results, the weight distributions of different layers have a similar mean (\ie, near 0), but exhibit varying minimum and maximum values.
This motivates us to use a layer-wise different policy for determining the weight quantization range.
\input{sections/figures_sup/sup-weight}

\subsection{Training Time}
\label{subsec:sup-traintime}

Although our framework primarily aims to achieve an accurate quantized SR network in which the inference cost is reduced via quantization, we also provide comparisons on the training time.
According to \Cref{tab:sup-traintime}, our training scheme requires a shorter training time than DDTB and DAQ.
Although our training incurs slightly more time overhead compared to PAMS, the gains in test accuracy compensate for this additional training cost.
\input{sections/tables_sup/sup-traintime}


\subsection*{License of the Used Assets}
\begin{itemize}
    \item[$\bullet$] DIV2K~\cite{agustsson2017ntire}  dataset is publicly available for academic research purposes.
    \item[$\bullet$] Set5~\cite{bevilacqua2012low}, Set14~\cite{ledig2017photo}, BSD100~\cite{martin2001database}, Urban100~\cite{huang2015single} datasets are made available at \href{https://github.com/jbhuang0604/SelfExSR}{https://github.com/jbhuang0604/SelfExSR}.
\end{itemize}


