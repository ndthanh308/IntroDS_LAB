\section{Conclusion}
\label{sec:conclusion}
SR networks suffer accuracy loss from quantization due to the inherent mismatch in feature distributions.
Instead of employing resource-demanding dynamic modules to handle distinct distributions during test time, we introduce a new quantization-aware training technique that alleviates this mismatch through distribution optimization.
We utilize cooperative mismatch regularization to update the SR network to be quantization-friendly and accurate.
Additionally, to address the mismatch in layer-wise weights, we propose a weight-clipping correction strategy.
These straightforward solutions effectively reduce the distribution mismatch with minimal computational overhead.