\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amssymb}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%add new pacakge
\bibliographystyle{plainnat}
\usepackage{graphicx}
\usepackage{mathptmx,mathtools}
\usepackage{setspace}
\usepackage{stix}           %square symbol
\usepackage{amsmath}
\usepackage{bm}

% for table
\usepackage{multirow}
\usepackage{pifont}
\usepackage{multicol}
\usepackage{subcaption}

% packages for algorithms
\usepackage{algorithm}
\usepackage{setspace}
\usepackage[noend]{algpseudocode}


% packages for color
\usepackage{color}
\usepackage[dvipsnames]{xcolor} % changed from \usepackage{xcolor} to include named colors
\usepackage{colortbl} % for colors in table
\definecolor{MyGreen}{cmyk}{100, 0, 100, 0}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=MyGreen]{hyperref}


% et al
\newcommand{\etal}{\textit{et al.}}

\usepackage{pdflscape} 


% commands used for commenting
\newcommand{\cheeun}[1]{\textcolor{Orchid}{\textbf{Cheeun: }#1}}
\newcommand{\junghun}[1]{\textcolor{RoyalBlue}{\textbf{Junghun: }#1}}
\newcommand{\kyoungmu}[1]{\textcolor{Aquamarine}{\textbf{Kyoung Mu: }#1}}

\newcommand{\eg}{\textit{e}.\textit{g}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}

\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\theequation}{S\arabic{equation}}
\title{Supplementary Material for \\``Overcoming Distribution Mismatch \\in Quantizing Image Super-Resolution Networks''}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
    Cheeun Hong \\
    Dept. of ECE \& ASRI,\\
    Seoul National University\\
    cheeun914@snu.ac.kr \\
    \And
    Kyoung Mu Lee \\
    IPAI, Dept. of ECE \& ASRI,\\
    Seoul National University\\
    kyoungmu@snu.ac.kr \\
}



\begin{document}
\maketitle

In this supplementary material, we present the implementation details of our training framework, ODM in Section~\ref{sup:implementations}; additional experimental results in Section~\ref{sup:experiments}; additional complexity analysis in Section~\ref{sup:complexity}; additional qualitative results in Section~\ref{sup:qualitative}.


\section{Implementation details}
\label{sup:implementations}
$\lambda_V$, the balance weight between reconstruction loss and variance regularization loss, is set differently based on the backbone model.
We set $\lambda_V$, as $1\times 10^{-4}$ for EDSR and SRResNet, and $1\times 10^{-5}$ for RDN, which is a larger model with more convolutional layers than EDSR and SRResNet.
Since $\mathcal{L}_V$ is sum of the variances in features of all the convolutional layers, it is natural to leverage smaller $\lambda_V$ for larger models.


\section{Additional experiments}
\label{sup:experiments}

In addition to the comparisons of the main manuscript done on SR networks of scale $\times4$, we evaluate our framework on networks of scale $\times2$.
As shown in Table~\ref{tab:sup-scale2-tot}, our framework outperforms existing SR quantization methods in terms of both PSNR and SSIM, demonstrating the effectiveness of our approach on scale 2 SR networks.
Specifically, the PSNR gain on Set5 is $0.37$ dB on EDSR and $0.37$ dB on RDN, while it is $0.06$ dB on SRResNet, as the distribution mismatch problem is particularly trivial for SRResNet.

\input{sections/submission/tables_sup/sup-scale2-tot}


Moreover, we compare our method with fully-quantized SR networks, EDSR-FQSR~[40], which quantizes all layers and also the skip-connections.
To make a fair comparison, we also quantize all convolutional layers and the skip-connetions.
In Table~\ref{tab:sup-fully}, the results demonstrate that ODM is also effective when the network is fully quantized.


\input{sections/submission/tables_sup/sup-fully}

\section{Additional complexity comparison}
\label{sup:complexity}
Along with the EDSR backbone models analyzed in the main manuscript, we present computational complexity analyses on RDN and SRResNet-based models.
We measure BitOPs for generating a $(1920 \times 1080)$ image and storage size required to store the model weights.
As shown in Table~\ref{tab:sup-complexity}, our framework achieves higher reconstruction accuracy regarding PSNR and SSIM, with less or similar storage size and BitOPs.
In particular, RDN-ODM involves {$\times4$} smaller storage size overhead than RDN-DDTB, and {$\times800$} fewer BitOPs overhead than RDN-DAQ, while the PSNR gap is $0.41$dB or higher.
Although the PSNR gap is smaller on SRResNet, ODM still achieves higher PSNR with fewer computations than DAQ and DDTB.
Compared to PAMS, our framework incurs additional storage size on RDN ($0.7\%$) and SRResNet ($0.1\%$), but the accuracy gap with PAMS is significant ($\sim1.4$ dB). 

\input{sections/submission/tables_sup/sup-complexity}




\clearpage
\section{Additional qualitative results}
\label{sup:qualitative}
We show more visual comparisons in Figure~\ref{fig:sup-qual}.
Overall, while other methods suffer from blurred lines or damaged structures in test images (Urban100), our approach produces further clear lines and structures.
These results demonstrate that our ODM is beneficial quantitatively and qualitatively.


\input{sections/submission/figures_sup/sup-qual}

\subsection*{License of the Used Assets}

\begin{itemize}
    \item DIV2K~[1] dataset is publicly available for academic research purposes.
    \item Set5~[4], Set14~[29], BSD100~[37], Urban100~[19] datasets are made available at \url{https://github.com/jbhuang0604/SelfExSR}.
\end{itemize}








\end{document}