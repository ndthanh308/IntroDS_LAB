


\section{Conclusion}
\label{sec:conclusion}

SR networks suffer accuracy loss from quantization due to the inherent distribution mismatch of the features.
Instead of adapting resource-demanding dynamic modules to handle distinct distributions during test time, we introduce a new quantization-aware training technique that relieves the mismatch problem via distribution optimization.
% we introduce a new quantization-aware training scheme that alleviates the aforementioned problem via distribution optimization.
We leverage variance regularization loss that updates the SR network towards being quantization-friendly and also accurately super-resolving images. 
Also, through analysis of the distribution mismatch of different layers, we find that applying additional shifting offsets to layers with a large mismatch in terms of shift and scaling offsets to the layers with a large scaling mismatch can further reduce the distribution mismatch issue with minimal computational overhead.
Experimental results demonstrate that the proposed training scheme achieves superior performance in terms of accuracy and computational complexity on various SR networks.

\noindent \textbf{Limitation}
Our framework shows limited effectiveness for SR networks with batch normalization (BN) layers, such as SRResNet. 
This is because BN layers relieve the variation in feature distribution, thus the distribution mismatch problem is not as severe.

