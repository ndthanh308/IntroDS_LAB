\section{Experiments}
\label{sec:experiments}

\input{sections/submission/tables/exp-edsr}
\input{sections/submission/tables/exp-rdn}
\input{sections/submission/tables/exp-srresnet}
\input{sections/submission/tables/exp-complexity}
\input{sections/submission/figures/exp-qual}
\input{sections/submission/tables/exp-ablation}

The efficacy and adaptability of the proposed quantization framework ODM are assessed through its application to several SR networks.
The experimental settings are outlined (Sec.~\ref{subsec:setting}), and both quantitative (Sec.~\ref{subsec:quan}) and qualitative (Sec.~\ref{subsec:qual}) evaluations are conducted on various SR networks. 
Furthermore, ablation experiments are conducted to examine each component of the framework in detail (Sec.~\ref{subsec:ablation}).

\subsection{Implementation details}
\label{subsec:setting}

\noindent\textbf{Models and training.}
The proposed framework is applied directly to existing representative SR networks that produce satisfactory SR results but with heavy computations: EDSR (baseline)~\cite{lim2017enhanced}, RDN~\cite{zhang2018residual}, and SRResNet~\cite{ledig2017photo}.
Following existing works on SR quantization~\cite{Li2020pams,ma2019efficient,xin2020binarized,hong2022daq,zhong2022ddtb,hong2022cadyq}, weights and activations of the high-level feature extraction module are quantized which is the most computationally-demanding. 
Training and validation are done with DIV2K~\cite{agustsson2017ntire} dataset. 
ODM trains the network for 60 epochs, with $1\times10^{-4}$ initial learning rate that is halved every 15 epochs and with a batch size of 8.
All our experiments are implemented with PyTorch and run using a single RTX 2080Ti GPU.




\noindent\textbf{Evaluation.}
We evaluate our framework on the standard benchmark (Set5~\cite{bevilacqua2012low}, Set14~\cite{ledig2017photo}, BSD100~\cite{martin2001database}, and Urban100~\cite{huang2015single}).
We report peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM~\cite{wang2004image}) to evaluate the SR performance.
To evaluate the computational complexity of our framework, we measure the BitOPs and storage size. 
BitOPs is the number of operations that are weighted by the bit-widths of the two operands. 
Storage size is the number of stored parameters weighted by the precision of each parameter value.
Furthermore, our ablation study is conducted on EDSR of scale 4 with 2-bit setting.



\subsection{Quantitative results}
\label{subsec:quan}


To evaluate the effectiveness of our proposed scheme, we compare the results with existing SR quantization works PAMS~\cite{Li2020pams}, DAQ~\cite{hong2022daq}, and DDTB~\cite{zhong2022ddtb} using the official code.
To make a fair comparison with the existing works, we reproduce the results of other methods using the same training epochs. 
As shown in Table~\ref{tab:exp-edsr}, our framework ODM outperforms other methods largely for all 4, 3, and 2-bit, and notably, the improvement is  significant for 2-bit quantization.
Also, 4-bit EDSR-ODM achieves closer accuracy to the 32-bit EDSR, where the margin is only 0.07dB for Set5.
This indicates that ODM can effectively bridge the gap between the quantized network and the floating-point network.  
Also, Table~\ref{tab:exp-rdn} compares the results on RDN.
The results show that ODM achieves consistently superior performance on 4, 3, and 2-bit quantization.

Furthermore, we evaluate our framework also on SRResNet which is shown in Table~\ref{tab:exp-srresnet}. 
SRResNet architecture includes BN layers and thus the distribution mismatch problem is not as severe as in EDSR or RDN.
Nevertheless, ODM is also proven effective for quantizing SRResNet on all bit settings, showing 
slightly better performance than the existing quantization methods.
Additional experiments that further demonstrate the applicability of ODM are provided in the supplementary materials.


Along with the SR accuracy, we also compare the computational complexity of our framework in Table~\ref{tab:exp-complexity}.
We calculate the BitOPs for generating a $1920\times1080$ image.
Overall, our framework ODM achieves higher accuracy (PSNR/SSIM) with similar or less computational resources.
As reported in Table~\ref{tab:exp-ablation}, the distribution offsets incur minimal computational overhead.
On EDSR, distribution offsets involve an additional $0.02\%$ storage size and the offset scaling and shifting involves $0.005\%$ additional BitOPs.
Compared to existing works that utilize dynamic adaptation, the computational overhead is $\times30$ smaller in storage size than DDTB~\cite{zhong2022ddtb} and $\times100$ smaller in BitOPs than DAQ~\cite{hong2022daq}.






\subsection{Qualitative results}
\label{subsec:qual}

Figure~\ref{fig:exp-qualitative} provides qualitative results and comparisons with the output images from quantized EDSR and RDN.
Our method, ODM, produces a further visually clean output image compared to existing quantization methods.
In contrast, existing methods, especially PAMS, suffer from blurred lines or artifacts.
The qualitative results stress the importance of alleviating the distribution mismatch problem in SR networks.
More results are provided in the supplementary materials.

\subsection{Ablation study}
\label{subsec:ablation}

On EDSR 2-bit setting, we verify the importance of each attribute of our framework: cooperative variance regularization and distribution offsets.
As shown in Table~\ref{tab:exp-ablation}, both cooperative variance regularization and distribution offsets can respectively improve the baseline accuracy.
Compared to using variance regularization directly (\textbf{a}), our cooperative scheme (\textbf{b}) improves the SR accuracy ($+0.23$dB).
Although leveraging distribution offsets (\textbf{c}) incurs additional computations, it largely increases the accuracy and the computational overhead is minimal.
The two components can jointly reduce the distribution mismatch and result in a further accurate quantized SR network.

