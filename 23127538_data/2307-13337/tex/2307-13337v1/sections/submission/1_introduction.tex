\section{Introduction}
\label{sec:introduction}

Image super-resolution (SR) is a core low-level vision task that aims to reconstruct the high-resolution (HR) images from their corresponding low-resolution (LR) counterparts.
Recent advances in deep learning~\cite{dong2015image,kim2016accurate,lim2017enhanced,zhang2018rcan,zhang2018residual} have led to astonishing achievements in producing high-fidelity images.
However, the remarkable performance relies on heavy network architectures with significant computational costs, which limits the practical viability, such as mobile deployment.

To mitigate the computational complexity of neural networks, quantization has emerged as a promising avenue. 
Network quantization has proven effective in reducing computation costs without much loss in accuracy, particularly in high-level vision tasks, such as image classification~\cite{choi2018pact,hou2018loss,zhou2016dorefa}.
Nonetheless, when it comes to quantizing SR networks to lower bit-widths, a substantial performance degradation~\cite{ignatov2021real} occurs, posing a persistent and challenging problem to be addressed.




Such degradation can be attributed to the significant variance present in the activation (feature) distributions of SR networks.
The feature distribution of a layer exhibits substantial discrepancies across different channels and images, which makes it difficult to determine a single quantization range for a layer.
Early approach on SR quantization~\cite{Li2020pams} adopts quantization-aware training to learn better quantization ranges.
However, as observed in Figure~\ref{fig:method-motiv}, 
despite careful selection, the quantization ranges fail to align with the diverse values within the channel and image dimension, which we refer to as \textit{distribution mismatch}.



Recent approaches aim to address this challenge by incorporating dynamic adaptation methods to accommodate the varying distributions.
For instance, DAQ~\cite{hong2022daq} leverages distribution mean and variance to dynamically adjust quantization ranges for each channel and DDTB~\cite{zhong2022ddtb} employs input-adaptive dynamic modules to determine quantization ranges on a per-image basis. 
However, both channel-wise quantization ranges and dynamic adaptation modules introduce significant computational overhead.
While adapting the quantization function to each image during inference might handle the variable distributions, it compromises the computational benefits of quantization.



In this study, we propose a novel quantization-aware training framework that addresses the distribution mismatch problem, by introducing a new loss term that regulates the variance in distributions.
While direct regularization of distribution variance demonstrates potential in reducing quantization errors in each quantized feature, its relationship with the reconstruction loss is questionable.
We observe that concurrently optimizing the network with variance regularization and reconstruction loss can disrupt the image reconstruction process, as shown in Figure~\ref{fig:method-conflict}. 
Therefore, we introduce a cooperative variance regularization strategy, where the variance is regulated only when it collaborates harmoniously with the reconstruction loss. 
To determine the cooperative behavior, we assess whether the sign values of the gradients from each loss are the same.
Consequently, we can effectively update the SR network to optimize both quantization-friendliness and reconstruction accuracy. 


To further reduce the distribution mismatch in SR networks, we introduce the concept of distribution offsets for features that exhibit severe mismatch. 
We first observe that the distribution mismatch problem is more critical in the channel dimension compared to the image dimension (Figure~\ref{fig:method-motiv}).
Moreover, we find that the degree of channel-wise mismatch varies across different convolutional layers.
As shown in Figure~\ref{fig:method-offset}, certain layers exhibit a large mismatch between the distribution means, while others show a large mismatch between the distribution deviations.
Intuitively, the mismatch in distribution mean can be reduced by applying channel-wise shifting of the distributions and that of the deviation be reduced by scaling.
On this basis, we leverage additional offset parameters that either shift or scale the channel-wise distributions based on the specific mismatch aptitude of the layer.
While these selectively-applied offsets effectively mitigate the distribution mismatch, they only incur negligible overhead, around $\times30$ smaller storage size overhead or $\times100$ fewer BitOPs compared to existing works with dynamic modules.


The contributions of our work include:
\begin{itemize}
    \item We introduce the first quantization framework to address the distribution mismatch problem in SR networks without dynamic modules. Our framework updates the SR network to be quantization-friendly and accurate at the same time.
    \item We identify the distinct distribution mismatch among different layers and further reduce the distribution mismatch by shifting or scaling largely mismatching features. 
    \item Compared to existing approaches on SR quantization, ours achieves state-of-the-art performance with similar or less computations. 
\end{itemize}


