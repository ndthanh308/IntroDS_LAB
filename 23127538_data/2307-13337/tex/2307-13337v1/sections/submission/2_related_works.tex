\section{Related Works}
\label{sec:related_works}
\paragraph{Image super-resolution.}
Convolutional neural network (CNN) based approaches~\cite{ledig2017photo,lim2017enhanced} have exhibited remarkable advancements in image super-resolution (SR) task, but at the cost of substantial computational resources.
The massive computations of SR networks have led to a growing interest in developing lightweight SR architectures~\cite{dong2016srcnn,hui2019imdn,hui2018idn,zhang2018rcan,jo2021practical}.
Furthermore, various lightweight networks are investigated through neural architecture search~\cite{chu2021fast,kim2019fine,li2020dhp,song2020efficient,li2021heterogeneity}, knowledge distillation~\cite{hui2018idn,hui2019imdn,zhang2021data}, and pruning~\cite{Oh_2022_CVPR}.
While these methods mostly focus on reducing the network depth or the number of channels, our focus in this work is to lower the precision of floating-point operations with network quantization.



\paragraph{Network quantization.} 
By mapping 32-bit floating point values of input features and weights of convolutional layers to lower-bit values, network quantization provides a dramatic reduction in computational resources~\cite{cai2017deep,choi2018pact,esser2019learned,jung2019learning,zhou2016dorefa,zhuang2018towards}.
Recent works successfully quantize various networks with low bit-widths without much compromise in network accuracy~\cite{cai2020rethinking,dong2019hawq,habi2020hmq,jin2020adabits,lou2019autoq,wang2019haq,yang2020fracbits}.
However, these works primarily focus on high-level vision tasks, while networks for low-level vision tasks remain vulnerable to low-bit quantization.



\paragraph{Quantized super-resolution networks.} 
In contrast to high-level vision tasks, super-resolution poses different challenges due to inherently high accuracy sensitivity to quantization~\cite{ignatov2021real,ma2019efficient,xin2020binarized,Wang2021fully}.
A few works have attempted to recover the accuracy by modifying the network architecture~\cite{ayazoglu2021extremely,jiang2021training,xin2020binarized}
or by adopting different bits for each image~\cite{hong2022cadyq} or network stage~\cite{liu2021super}.
However, the key challenge of quantizing SR networks is in the vastly distinct feature distributions of SR networks.
To deal with such issue, PAMS~\cite{Li2020pams} adopts a learnable quantization range for different layers.
More recently, DAQ~\cite{hong2022daq} recognizes that the distributions are not only distinct per layer, but per channel and per input image and adopts dynamic quantization function for each channel.
Moreover, DDTB~\cite{zhong2022ddtb} employs an input-adaptive dynamic module to adapt the quantization ranges differently for each input image.
However, these dynamic adaptations of quantization functions during test-time cost non-negligible  computational overheads.
In contrast, instead of designing input-adaptive or channel-wise different quantization modules, we focus on mitigating the feature variance itself.
Our framework reduces the inherent distribution mismatch in SR networks with minimal overhead, obtaining quantized networks without the need for dynamic modules.




