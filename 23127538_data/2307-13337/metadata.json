{
  "title": "Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks",
  "authors": [
    "Cheeun Hong",
    "Kyoung Mu Lee"
  ],
  "submission_date": "2023-07-25T08:50:01+00:00",
  "revised_dates": [
    "2024-07-19T00:35:59+00:00"
  ],
  "abstract": "Although quantization has emerged as a promising approach to reducing computational complexity across various high-level vision tasks, it inevitably leads to accuracy loss in image super-resolution (SR) networks. This is due to the significantly divergent feature distributions across different channels and input images of the SR networks, which complicates the selection of a fixed quantization range. Existing works address this distribution mismatch problem by dynamically adapting quantization ranges to the varying distributions during test time. However, such a dynamic adaptation incurs additional computational costs during inference. In contrast, we propose a new quantization-aware training scheme that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, this mismatch can be mitigated by regularizing the distance between the feature and a fixed quantization range. However, we observe that such regularization can conflict with the reconstruction loss during training, negatively impacting SR accuracy. Therefore, we opt to regularize the mismatch only when the gradients of the regularization are aligned with those of the reconstruction loss. Additionally, we introduce a layer-wise weight clipping correction scheme to determine a more suitable quantization range for layer-wise weights. Experimental results demonstrate that our framework effectively reduces the distribution mismatch and achieves state-of-the-art performance with minimal computational overhead.",
  "categories": [
    "cs.CV",
    "eess.IV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13337",
  "pdf_url": null,
  "comment": "ECCV 2024",
  "num_versions": null,
  "size_before_bytes": 29040093,
  "size_after_bytes": 427668
}