{
  "title": "Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models",
  "authors": [
    "Ziyi Wang",
    "Xumin Yu",
    "Yongming Rao",
    "Jie Zhou",
    "Jiwen Lu"
  ],
  "submission_date": "2023-07-27T16:07:03+00:00",
  "revised_dates": [
    "2023-09-07T05:44:37+00:00"
  ],
  "abstract": "With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNetPart segmentation tasks. Code is available at https://github.com/wangzy22/TAP.",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14971",
  "pdf_url": "https://arxiv.org/pdf/2307.14971v2",
  "comment": "Accepted to ICCV 2023, project page: https://tap.ivg-research.xyz",
  "num_versions": null,
  "size_before_bytes": 1855736,
  "size_after_bytes": 296362
}