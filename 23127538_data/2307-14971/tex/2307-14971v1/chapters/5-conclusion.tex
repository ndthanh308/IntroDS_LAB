\vspace{6pt}
\section{Conclusions}

In this paper, we have proposed a novel 3D-to-2D generative pre-training method TAP that is adaptable to any point cloud model. We implemented the cross-attention mechanism to generate view images of point clouds from instructed camera poses. To better encode pose conditions and generate physically meaningful queries, we derived mathematical formulations of optical lines. The proposed TAP pre-training had higher preciseness in supervision and broader adaptation to different backbones, compared with directly reconstructing point clouds in previous methods. Experimental results conveyed that the TAP pre-training can help the backbone models better capture the structural knowledge and stereoscopic relations. Fine-tuning results of TAP pre-training achieve state-of-the-art performance on ScanObjectNN classification and ShapeNetPart segmentation, among methods that do not include any pre-trained image or text models. We believe the cross-modal generative pre-training paradigm will be a promising direction for future research.
