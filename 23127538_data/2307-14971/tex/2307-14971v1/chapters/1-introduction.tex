\section{Introduction}

Nowadays, pre-training fundamental models with self-supervised mechanisms has witnessed a thriving development in the computer vision community, given its low requirement in data annotation and its high transferability to downstream applications. Self-supervised pre-training aims to fully exploit the statistical and structural knowledge from abundant annotation-free data and empowers the fundamental models with robust representation ability. In 2D vision, self-supervised pre-training has shown strong potential and achieved remarkable progress on diverse backbones in various downstream tasks. Successful pre-training strategies in 2D domain such as contrastive learning~\cite{moco, SimCLR} and mask modeling~\cite{mae, beit} have also been adopted to 3D point cloud analysis~\cite{yu2022point, pang2022masked, liu2022masked} in recent research.

% Figure environment removed

However, the pre-training paradigm hasn't become the prevailing trend in 3D learning and architectural design is still the mainstream to reach a new state-of-the-art performance, which is considerably different from the dominant status of pre-training in 2D domain. In object-level analysis, generative pre-training inspired by MAE~\cite{mae} has been studied thoroughly, but their performances still lag behind architecture-based methods like PointNeXt~\cite{pointnext}. Two factors mainly contribute to the inferior status of generative pre-training in 3D learning. Since point clouds are unordered sets of point coordinates, there is no direct and precise supervision like one-to-one MSE loss between generated point clouds and their corresponding ground truth. Chamfer Distance supervision for point clouds only calculates a rough set-to-set matching and its imprecision has been widely discussed in~\cite{liu2020morphing, wu2021balanced, huang2023learning}. Additionally, existing advanced generative pre-training methods in object analysis are limited to the Transformer-based backbone, and fail to be extended to other sophisticated point cloud models.

To alleviate the aforementioned problems, we propose a 3D-to-2D generative pre-training method for point cloud analysis that has higher preciseness in supervision and broader adaptation to different backbones. Instead of reconstructing point clouds as previous literature~\cite{pang2022masked, pointm2ae}, we propose to generate view images of the input point cloud given the instructed camera poses. This is similar to taking photos of a realistic object from different perspectives to fully present its structure or internal relations. Therefore, we name our model \textit{Take-A-Photo}, in short \textit{TAP}. More specifically, we propose a pose-dependent photograph module that utilizes the cross-attention mechanism to explicitly encode pose conditions with 3D features from the backbone. Then a 2D generator decodes pose-conditioned features into view images that are supervised by rendered ground truth images. The principle illustration of TAP is shown in Figure~\ref{fig:concept}. In the pose-dependent photograph module, we do not provide detailed projection relations from 3D points to 2D pixels, thus the cross-attention layers are encouraged to learn by themselves what those view images look like conditioned on given poses. Since the projection layout, part occlusion relation, faces colors that represent light reflections are largely distinct among view images, the proposed 3D-to-2D generative pre-training is a challenging pretext task that obliges 3D backbone to gain higher representation ability of geometrical knowledge and stereoscopic relations.


We conduct extensive experiments on various backbones and downstream tasks to verify the effectiveness and superiority of our proposed 3D-to-2D generative pre-training method. When pre-trained on synthetic ShapeNet~\cite{shapenet} and transferred to real-world ScanObjectNN~\cite{uy2019revisiting} classification, TAP brings consistent improvement to different backbone models and successfully outperforms previous point cloud generative pre-training methods based on Transformers backbone. With PointMLP~\cite{pointmlp} as the backbone, TAP achieves state-of-the-art performance on ScanObjectNN classification and ShapeNetPart~\cite{shapenetpart} part segmentation among methods that do not include any pre-trained image or text model. We also conduct thorough ablation studies to discuss the architectural design of the TAP model and verify the individual contribution of each component. 

In conclusion, the contributions of our paper can be summarized as follows:
(1) We propose TAP, the first 3D-to-2D generative pre-training method that is adaptable to any point cloud model. TAP pre-training helps to exploit the potential of point cloud models on geometric structure comprehension and stereoscopic relation understanding. (2) We propose a Photograph Module where we derive mathematical formulations to encode pose conditions as query tokens in cross-attention layers. (3) TAP surpasses previous generative pre-training methods on the Transformers backbone and achieves state-of-the-art performance on ScanObjectNN classification and ShapeNetPart segmentation.

