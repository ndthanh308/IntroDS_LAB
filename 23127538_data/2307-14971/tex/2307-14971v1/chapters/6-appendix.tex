\appendix

\section{More Experimental Results}
In this section, we conduct additional experiments to evaluate the efficacy of our proposed 3D-to-2D generative pre-training method TAP. These experiments are centered around object-level few-shot classification and scene-level dense predictions.

\subsection{Object-level Few-shot Classification}
Following Point-BERT~\cite{yu2022point}, we conduct few-shot classification with Standard Transformers on ModelNet40~\cite{modelnet} dataset. As shown in Table~\ref{tab:fewshot}, we report mean overall accuracy and standard deviation (mOA$\pm$std) on 10 pre-defined data folders for each few-shot setting. The \textit{way} and \textit{shot} in Table~\ref{tab:fewshot} specify the number of categories and the number of training examples per category, respectively. 

From the results, TAP achieves the highest mean overall accuracy across all few-shot settings when compared to previous generative pre-training approaches. Furthermore, TAP exhibits significantly lower standard deviations than those reported in the existing literature for the majority of few-shot settings, which signifies its robust performance and consistent superiority. This indicates that TAP is not only capable of achieving high mean overall accuracy but also exhibits reliability and robustness across various few-shot settings. Such stability is crucial in real-world applications, where consistency and predictability are vital for practical deployment.

\subsection{Scene-level Dense Predictions}
To assess the effectiveness of TAP in handling scene-level dense prediction tasks, we carry out experiments on object detection and semantic segmentation on the ScanNetV2~\cite{dai2017scannet} dataset. For the object detection task, we adopt 3DETR~\cite{misra20213detr} and pre-train its encoder on the object-level dataset ShapeNet~\cite{shapenet} with TAP. Average precision at 0.25 and 0.5 IoU thresholds are reported. Regarding semantic segmentation, we employ the PointTransformerV2~\cite{wu2022ptv2}(PTv2) model and pre-train it on the ScanNetV2 dataset with TAP. We report mean IoU for evaluation metric. It is worth mentioning that PTv2 represents the current state-of-the-art approach with open-source code availability.

\begin{table}[t]
    \centering
    \caption{\textbf{Few-shot Classification with Standard Transformers on ModelNet40 dataset.} We report mean overall accuracy and standard deviation on 10 pre-defined data folders for each setting. Best results are marked \textbf{bold}.}
    \label{tab:fewshot}
    \setlength\tabcolsep{2pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule[0.95pt]
    \multirow{2}{*}[-0.5ex]{Method}& \multicolumn{2}{c}{5-way} & \multicolumn{2}{c}{10-way}\\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5} & 10-shot & 20-shot & 10-shot & 20-shot\\ 
    \midrule[0.6pt]
     w/o pre-training & 87.8 $\pm$ 5.2& 93.3 $\pm$ 4.3 & 84.6 $\pm$ 5.5 & 89.4 $\pm$ 6.3\\
     \midrule
     Point-BERT~\cite{yu2022point} & 94.6 $\pm$ 3.1 & 96.3 $\pm$ 2.7 &  91.0 $\pm$ 5.4 & 92.7 $\pm$ 5.1\\
     MaskPoint~\cite{liu2022masked} & 95.0 $\pm$ 3.7 & 97.2 $\pm$ \textbf{1.7} & 91.4 $\pm$ 4.0 & 93.4 $\pm$ 3.5\\
     Point-MAE~\cite{pang2022masked} & 96.3 $\pm$ 2.5 & \textbf{97.8} $\pm$ 1.8 & 92.6 $\pm$ 4.1 & 95.0 $\pm$ 3.0\\
     \midrule
     TAP   & \textbf{97.3} $\pm$ \textbf{1.8} & \textbf{97.8} $\pm$ \textbf{1.7} &  \textbf{93.1} $\pm$ \textbf{2.6} & \textbf{95.8} $\pm$ \textbf{1.0} \\
     \bottomrule[0.95pt]
     \end{tabular}
    }
\end{table}


\begin{table}[!t]
    \centering
    % \setlength\tabcolsep{8pt}
    \caption{\textbf{Scene-level object detection and semantic segmentation on ScanNetV2~\cite{dai2017scannet}}. Average precision at 0.25 IoU thresholds (AP$_\textrm{0.25}$) and 0.5 IoU thresholds (AP$_\textrm{0.5}$) of detection and mean IoU of semantic segmentation are reported.}
    \label{tab:scene}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule[0.95pt]
    \multirow{2}{*}[-0.5ex]{Method}& \multicolumn{2}{c}{Det (3DETR~\cite{misra20213detr})} & Seg (PTv2~\cite{wu2022ptv2}) \\
    \cmidrule(l){2-3}\cmidrule(l){4-4} 
    & AP$_\textrm{0.25}$ & AP$_\textrm{0.5}$ & mIoU \\
    \midrule[0.6pt]
     Baseline & 62.1 & 37.9 & 75.4 \\
     +TAP & 63.0~\cred{(+0.9)} & 41.4~\cred{(+3.5)} & 75.6~\cred{(+0.2)} \\
     \bottomrule[0.95pt]
     \end{tabular}}
\end{table}


Based on the results presented in Table~\ref{tab:scene}, TAP consistently enhances the performance of all baselines, thereby showcasing its efficacy in tackling more intricate scene-level dense prediction tasks. Remarkably, even with the encoder solely pre-trained on an object-level dataset for scene-level detection task, significant improvements are observed in both AP$_\textrm{0.25}$ and AP$_\textrm{0.5}$ metrics. This suggests that the learned representations from TAP effectively capture relevant information and generalize well to complex scenes, even when the pre-training data is limited to object-level collections. Such generalization capabilities are valuable in scenarios where obtaining large-scale fully annotated scene-level datasets may be challenging or expensive.



\section{Ablation Studies}
In this section, we conduct more ablation studies on hyperparameter choices of the proposed 3D-to-2D generative pre-training, discussing more thoroughly the insights into architectural design and objective function design. We implement PointMLP~\cite{pointmlp} as the 3D backbone model and conduct these ablation experiments on the hardest PB-T50-RS variant of the ScanObjectNN~\cite{uy2019revisiting} dataset. We report the classification accuracy of the fine-tuning results.

\subsection{Cross-Attention Hyperparameters}

In Table~\ref{tab:abl_attn}, we display the results of ablation studies on the number of layers and feature channels of the cross-attention layers in our proposed Photograph module. From the quantitative results, we can conclude that 2 layers with 128 channels is the best hyperparameter group for cross-attention layers. When we implement a shallow layer setting (2 layers in line 1 and 4 layers in line 2), lower feature channels (128 dims) achieves better performance. On the contrary, when we implement a deeper layer setting (6 layers in line 3 and 8 layers in line 4), relatively higher feature channels (256 dims) is the best choice. Additionally, if we use 1024 dims as the feature channels in cross-attention layers, which is the same as the channels of output features from the 3D backbone model, the pre-training stage totally collapses and the fine-tuning results are much lower than models of 128 dims and 256 dims, no matter how much layers are implemented. This result indicates that a bottleneck design in our proposed photograph module is essential for the successful pre-training of the proposed 3D-to-2D generation.

The overall trend is that a lightweight architectural design of the cross-attention layers is better than a heavy module design. This may be because we completely drop the photograph module and only keep the 3D backbone in the fine-tuning stage. Therefore, a lightweight photograph module in the pre-training stage will encourage the 3D backbone to exploit more representation ability and avoid information loss in the fine-tuning stage to the best extent. On the contrary, if we implement a heavy photograph module with deep cross-attention layers and high feature dimensions, the photograph module will dominate the generation process and the importance of the 3D backbone will be neglected. What's worse, in the fine-tuning stage, the rich geometry information in the heavy photograph module is totally dropped out and no longer helpful for downstream tasks.

\begin{table}[!t]
\label{tab:ablation_supp}
\caption{\textbf{Ablation Studies on Hyperparameters.} We implement PoinMLP~\cite{pointmlp} as the 3D backbone model and conduct experiments on the hardest PB-T50-RS variant of ScanObjectNN~\cite{uy2019revisiting} dataset.}
\vspace{-6pt}
\centering
\begin{subtable}{0.49\textwidth}
    \setlength\tabcolsep{3pt}
    \centering
    \caption{Cross-Attention Hyperparameters.}
    \vspace{-4pt}
    \label{tab:abl_attn}
    \adjustbox{width=0.9\textwidth}{
    \begin{tabular}{c|ccc}
    \toprule
    LayerNum $\backslash$ Channels & 128 Dims  & 256 Dims &  1024 Dims \\
    \midrule
    2 Layers    & \textbf{89.1} & 87.9 & 86.3 \\
    4 Layers    & 88.7 & 88.0 & 85.9 \\
    6 Layers    & 88.3 & 88.5 & 85.3 \\
    8 Layers    & 87.7 & 88.1 & 85.8 \\
    \bottomrule
    \end{tabular}}
\end{subtable}
\hfill
\vspace{5pt}
\begin{subtable}{0.49\textwidth}
    \centering
    \setlength\tabcolsep{3pt}
    \caption{Loss Weight Hyperparameters.}
    \vspace{-4pt}
    \label{tab:abl_lossw}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=0.9\textwidth}{
    \begin{tabular}{@{\hskip 3pt}>{\columncolor{white}[3pt][\tabcolsep]}c|cccccc|cc}
    \toprule
        Model   & G$_1$ & G$_2$ & G$_3$ & G$_4$ & G$_5$ & G$_6$ & H$_1$ & H$_2$ \\
    \midrule
        $w^\textrm{fg}$ & 2 & 5 & 10 & 20 & 30 & 50 & 0 & 20\\
        $w^\textrm{bg}$ & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1\\
        $w^\textrm{feat}$ & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 2\\
    \midrule
        Acc. (\%) & 87.1 & 87.2 & 88.0 & \textbf{88.5} & 88.0 & 86.8 & 86.3 & 87.8 \\
    \bottomrule
    \end{tabular}}
\end{subtable}
\end{table}

% Figure environment removed


\subsection{Objective Function}
In this subsection, we discuss the objective function design of our proposed 3D-to-2D generative pre-training. In our main paper, we implement pixel-level supervision with MSE loss between generative view images $I_\textrm{gen}$ and ground truth images $I_\textrm{gt}$:
\begin{equation}
    \mathcal{L}_\textrm{pix}(I_\textrm{gen}, I_\textrm{gt}) = w^\textrm{fg} \mathcal{D}(I_\textrm{gen}^\textrm{fg}, I_\textrm{gt}^\textrm{fg}) + w^\textrm{bg} \mathcal{D}(I_\textrm{gen}^\textrm{bg}, I_\textrm{gt}^\textrm{bg})
\end{equation}
where fg denotes foreground region, bg denotes background region and $\mathcal{D}$ is the MSE distance. However, in 2D generation, perceptual loss~\cite{johnson2016perceptual} is of equal importance with pixel-wise loss. While pixel-wise MSE loss focuses on low-level similarities, perceptual loss measures high-level semantic differences between feature representations of the images computed by the pre-trained loss network. Technically, perceptual loss makes use of a loss network $\phi$ pre-trained for image classification, which is typically a 16-layer VGG~\cite{simonyan2014vgg} network pre-trained on the ImageNet~\cite{russakovsky2015imagenet} dataset. If we denote $\phi_j(x)$ as the feature map with size $c_j\times h_j\times w_j$ of the $j$th layer of the network $\phi$, then the perceptual loss is defined as the Euclidean distance:
\begin{equation}
\small
    \mathcal{L}_\textrm{feat}(I_\textrm{gen}, I_\textrm{gt}) = \frac{1}{N}\sum_j\frac{1}{c_j h_j w_j}\lVert\phi_j(I_\textrm{gen})- \phi_j(I_\textrm{gt})\rVert_2^2
\end{equation}
where $N$ is the number of total layers of the VGG network and $1\leq j\leq N$. If we combine the pixel-wise loss $\mathcal{L}_\textrm{pix}$ with the perceptual loss $\mathcal{L}_\textrm{feat}$, then the final objective function of the proposed 3D-to-2D generation is:
\begin{equation}
\small
    \mathcal{L} = \mathcal{L}_\textrm{pix} + w^\textrm{feat} \mathcal{L}_\textrm{feat}
\end{equation}

In Table~\ref{tab:abl_lossw}, we conduct ablations on loss weight of foreground pixel-wise loss $w^\textrm{fg}$, background pixel-wise loss $w^\textrm{bg}$ and perceptual loss $w^\textrm{feat}$. In Model $G_1$ to $G_6$, we only implement pixel-wise loss. In Model $H_1$, we only implement perceptual loss. In Model $H_2$, we combine pixel-wise loss with perceptual loss. From the ablation results, we can conclude that $w^\textrm{fg}:w^\textrm{bg}=20:1$ is the best hyperparameter choice for pixel-wise loss. However, the perceptual loss is not effective when we compare Model $G_4$, Model $H_1$ and Model $H_2$. This is mainly due to the reason that the rendered view image of synthetic ShapeNet~\cite{shapenet} dataset is out of the distribution of the realistic ImageNet~\cite{russakovsky2015imagenet} that the loss model $\phi$ is pre-trained on. Therefore, the high-level semantic representation ability of $\phi$ on view images is relatively poor and cannot guide the optimization of the 3D-to-2D generation process. If the rendered images are more realistic with colors and background, then the perceptual loss is expected to help 3D-to-2D generative pre-training.

\section{Visualization Results}

\subsection{Generated View Images}

Figure~\ref{fig:examples_supp} displays more visualization results of our generated view images from the 3D-to-2D generative pre-training process. We take ShapeNet~\cite{shapenet} as the pre-training dataset and implement PointMLP~\cite{pointmlp} as the 3D backbone model. The first line shows the generated results from our model while the second line shows ground truth images for reference. The visualization results convey that our 3D-to-2D generative pre-training can successfully predict the shape and colors of the objects from specific projection views. There are also some unsatisfactory cases in the last three columns, where there are some vague details in our generated images. This is mainly due to the large downsample ratio ($\times32$) in our model design.

\subsection{Feature Distributions}

Figure~\ref{fig:tsne} shows feature distributions of ModelNet40~\cite{modelnet} and ScanObjectNN~\cite{uy2019revisiting} datasets in t-SNE visualization. We choose PointMLP~\cite{pointmlp} as the 3D backbone and pre-train on ShapeNet~\cite{shapenet} dataset. We can conclude that with our proposed 3D-to-2D pre-training, the 3D backbone model can extract discriminative features after fine-tuning on downstream classification datasets.

\subsection{Part Segmentation Visualizations}

Figure~\ref{fig:partseg} presents visualizations of part segmentation results on samples from the ShapeNetPart dataset. Each part is represented by a distinct color for clarity. These qualitative results serve as compelling visual evidence and provide a vivid illustration of the efficacy of our fine-tune model in achieving accurate part segmentation.

% Figure environment removed

% Figure environment removed