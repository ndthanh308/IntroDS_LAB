\section{Related Work}


% Figure environment removed

\noindent\textbf{Point Cloud Analysis.} 
Point cloud analysis is a fundamental and important task in the realm of 3D vision.
Current literature has developed point-based and voxel-based methods to extract structural representations from 3D point clouds. Point-based methods~\cite{pointnet, pointnet2, wang2019dynamic, thomas2019KPConv, pointnext, pointmlp} process unordered points directly, introducing various techniques for local information aggregation. Existing point-based methods can be categorized into three types: SetAbstraction-based~\cite{pointnet, pointnet2, pointnext}, DynamicGraph-based~\cite{wang2019dynamic}, and Attention-based~\cite{yu2021pointr, yu2022point, pang2022masked, liu2022masked, pointm2ae}, all of which focus on modeling the relationships between points. 
Voxel-based methods~\cite{maturana2015voxnet, klokov2017escape, riegler2017octnet} partition the 3D space into ordered voxels and employ 3D convolutions for feature extraction. Voxel-based methods are primarily based on SparseConvolution~\cite{choy2019minkowski, graham2018sparseconv}, which enables efficient convolution operations in 3D space through sparse convolutions. 

\vspace{5pt}
\noindent\textbf{Point Cloud Pre-training.}
Pre-training has always been a focal point of research in the field of deep learning, and it can be distinguished based on the amount of annotation required, namely full-supervised~\cite{dosovitskiy2020vit, zhai2022scaling, carreira2017quo}, weakly-supervised~\cite{tarvainen2017mean, berthelot2019mixmatch, xie2020self}, and unsupervised~\cite{moco, SimCLR, chen2020improved}. Among them, unsupervised pre-training has become the mainstream, mainly due to its excellent transferability across tasks and its notable advantage of not relying on labeled data. 
There are two prevailing pretext tasks for unsupervised pre-training. The first is contrastive learning, as exemplified by MoCo~\cite{moco} and SimCLR~\cite{SimCLR}. The other restores the data from partially or disrupted inputs, such as MAE~\cite{mae} and BEiT~\cite{beit}.

Inspired by pre-training strategies in the image domain, more and more unsupervised pre-training methods are proposed for point clouds. PointContrast~\cite{pointcontrast} embraces the principle of contrastive learning, whereas PointBERT~\cite{yu2022point} and PointMAE~\cite{pang2022masked} integrate reconstruction pretext tasks. However, existing generative-based pre-training methods for point clouds only consider a single modality. In this paper, we propose a cross-modal generative-based pre-training strategy to achieve more effective pre-training.

\vspace{5pt}
\noindent\textbf{Cross-Modal Learning.}
Recently, cross-modal learning has been a popular research topic, aiming at learning from multiple modalities and enhancing the performance of various tasks. A variety of cross modal-learning tasks have been investigated, including multi-task learning~\cite{collobert2008unified}, conditional generation~\cite{rombach2022high}, pre-training~\cite{qi2023recon, dong2022act} and tuning~\cite{wang2022p2p, xu2021image2point}.

In the realm of point cloud analysis, much previous work has explored this learning paradigm. Some literature leverages 2D data for 3D point cloud analysis, such as MVTN~\cite{MVTN}, MVCNN~\cite{su2015multi} and CrossPoint~\cite{crosspoint}, proving that the multi-view images and the correspondence between images and points can be helpful for the 3D object understanding. Another line of the research, such as Image2Point~\cite{xu2021image2point} and P2P~\cite{wang2022p2p} take effort to adapt the models from 2D vision into 3D point cloud analysis, fully exploiting the relationship of 2D and 3D understanding. In this paper, we continue this learning paradigm in 3D vision domain, and for the first time propose the cross-modal generative pre-training scheme for point cloud models.