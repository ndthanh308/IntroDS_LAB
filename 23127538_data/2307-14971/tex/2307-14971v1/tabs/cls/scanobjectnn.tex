\begin{table}[!t]
\caption{
\textbf{Classification results on the ScanObjectNN dataset}. We report the overall accuracy (\%). The results with $\dagger$ are reproduced by PointNeXt~\cite{pointnext} repository.}
\label{tab:scanobjectnn}
\begin{center}
\vspace{-10pt}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccc}
\toprule[0.95pt]
Method & OBJ\_BG & OBJ\_ONLY & PB\_T50\_RS\\
\midrule[0.6pt]
\multicolumn{4}{c}{\textit{Hierarchical Models with TAP Pre-training}}\\
\midrule[0.6pt]
$^\dagger$DGCNN~\cite{wang2019dynamic} & - & - & 86.1 \\
$\quad$ + TAP & - & - & 86.6~\cred{(+0.5)} \\
$^\dagger$PointNet++~\cite{pointnet2} & - & - & 86.2 \\
$\quad$ + TAP & - & - & 86.8~\cred{(+0.6)} \\
$^\dagger$PointMLP~\cite{pointmlp} & - & - & 87.4 \\
$\quad$ + TAP & - & - & 88.5~\cred{(+1.1)} \\
% Finetuning
\midrule[0.6pt]
\multicolumn{4}{c}{\textit{Standard Transformers with Generative Pre-training}}\\
\midrule[0.6pt]
w/o pre-training~\cite{vaswani2017attention} & 79.86 & 80.55 & 77.24 \\
OcCo~\cite{occo} & 84.85 & 85.54 & 78.79 \\
Point-BERT~\cite{yu2022point} & 87.43 & 88.12 & 83.07 \\
MaskPoint~\cite{liu2022masked} & 89.30 & 88.10 & 84.30 \\
Point-MAE~\cite{pang2022masked} & 90.02 & 88.29 & 85.18 \\
TAP (Ours) & \textbf{90.36} & \textbf{89.50} & \textbf{85.67} \\
\bottomrule[0.95pt]
\end{tabular}}
\end{center}
\vspace{-20pt}
\end{table}
