\section{Experiments}
\label{sec:exp}
In this section, we first introduce the setups of our pre-training scheme. Then we evaluate our pre-training method on various point cloud backbones by fine-tuning them on different downstream tasks, such as point cloud classification on ModelNet and ScanObjectNN datasets, and part segmentation on the ShapeNetPart dataset. Finally, we provide in-depth ablation studies for the architectural design of our proposed TAP pre-training pipeline.

\subsection{Pre-training Setups}
\noindent\textbf{Data Setups} % object level and scene level
To align with previous research practices~\cite{yu2022point, pang2022masked, pointcontrast}, we choose \textbf{ShapeNet}~\cite{shapenet} that contains more than 50 thousand CAD models as our pre-training datasets. We sampled 1024 points from each 3D CAD model to form the point clouds, consistent with previous work. Since ShapeNet does not provide images for each point cloud, we use the rendered image from 12 surrounding viewpoints generated by MVCNN~\cite{su2015multi}. During our pre-training, the models are exclusively pre-trained with the training split following the practice of previous work~\cite{yu2022point}.

\vspace{5pt}
\noindent\textbf{Architecture Setups}
We conduct experiments on various point cloud encoders, including PointNet++~\cite{pointnet2}, DGCNN~\cite{wang2019dynamic}, PointMLP~\cite{pointmlp} and Transformers~\cite{yu2022point} for point cloud object classification. During the pre-training stage, the photograph module takes encoded point cloud features and pose conditions as inputs to generate a 32$\times$ downsampled view image feature map of size $7\times 7$ from a specific viewpoint. Then the 2D generator progressively upsamples the image feature map to decode RGB view images of size $224\times 224$. We do not alter the architecture of the point cloud backbone since the photograph module and the 2D generator are exclusively used during the pre-training phase and are dropped during the fine-tuning stage. In our experiment, the photograph module is a six-layer cross-attention block, with attention layer channels limited to 256 to enhance efficiency. During the pre-training task on ShapeNet, we utilized four simple transpose convolutions to upsample the reconstructed 2D feature map and predict the RGB value for each pixel.

\input{tabs/cls/scanobjectnn.tex}

\input{tabs/cls/joint_compare.tex}

\vspace{6pt}
\noindent\textbf{Implementation Details}
The experiments of TAP pre-training and finetuning on various downstream tasks are implemented with PyTorch~\cite{paszke2019pytorch}. We utilize AdamW~\cite{adamw} optimizer and the CosineAnnealing learning rate scheduler~\cite{loshchilov2016sgdr} to pre-train the point cloud backbone for 100 epochs. We set the initial learning rate as $5e^{-4}$ and weight decay as $5e^{-2}$. In our experiment, we train various point cloud backbones with 32 batch sizes on a single Nvidia 3090Ti GPU. The drop path rate of the cross-attention layer is set to 0.1. The foreground and background loss weights $w^\textrm{fg}, w^\textrm{bg}$ are set to 20 and 1. The detailed architecture of our simple 2D generator is: $\text{TConv}(256, 128, 5, 4)\rightarrow\text{TConv}(128, 64, 3, 2)\rightarrow\text{TConv}(64, 32, 3, 2)\rightarrow\text{TConv}(32, 3, 3, 2)$, where $\text{TConv}$ stands for Transpose Convolution and the four numbers in the tuple denotes $(C_{in}, C_{out}, \text{Kernel}, \text{Stride})$ respectively.
During the fine-tuning stage, we perform a learning rate warming up for point cloud backbones with 10 epochs, and keep other settings unchanged for a fair comparison.


\input{tabs/seg/partseg.tex}

\subsection{Downstream Tasks}
In this section, we report the experimental results of various downstream tasks. We follow the previous work to conduct experiments of object classification on real-world ScanObjectNN and synthetic ModelNet40 datasets. We also verify the effectiveness of our pre-training method on the part segmentation task with the ShapeNetPart dataset. 
\vspace{-5pt}
\subsubsection{Object Classification} 

\noindent\textbf{Main Results.}
To evaluate the effectiveness of our proposed TAP, we implement it with various point cloud architectures, including classical baselines such as PointNet++, DGCNN, and PointMLP, as well as widely used Standard Transformers backbone for existing generative pre-training methods. We follow the common practice to experiment our model on three variants of the ScanObjectNN dataset: 1) OBJ-ONLY: cropping object without any background; 2) OBJ-BG: containing the background and object; 3) PB-T50-RS: adopting various augmentations to the objects. We reported the results comparing with existing pre-training methods in Table~\ref{tab:scanobjectnn}.

% Figure environment removed

As shown in the upper part of the table, we pre-train the graph-based architecture DGCNN, set-abstraction-based architecture PointNet++, and MLP-based architecture PointMLP with TAP, and observe consistent improvements across models. The results strongly convey that our proposed TAP can be successfully applied to various types of point cloud models and the proposed novel 3D-to-2D generative pre-training is effective regardless of the backbone architecture. Considering that nearly all existing generative pre-training methods are specially designed for Transformer-based architecture, our TAP is much superior in its wider adaptation and higher flexibility. Additionally, we also provide a detailed and fair comparison with previous work by implementing TAP with the Standard Transformers architecture in the lower part of the table, where no hierarchical designs or inductive bias is included. Our TAP outperforms previous pre-training methods in all three split settings, providing strong evidence that our 3D-to-2D generative pre-training strategy can also benefit attention-based architectures and surpass uni-modal generative pre-training competitors. It is worth noting that although TAP and many previous pre-training approaches have significantly improved the performance of Transformers in point cloud tasks, increasing the accuracy from 77.24 to 85.67, they still lag far behind advanced point cloud networks such as PointMLP. Therefore, TAP's applicability to various models is an important characteristic that would benefit future research.

\vspace{6pt}
\noindent\textbf{Comparisons with Previous Methods.}
To clearly demonstrate the high performance of our proposed TAP pre-training in object classification tasks, we compare TAP with existing methods on both synthetic ModelNet and real-world ScanObjectNN (the hardest PB-T50-RS variant) datasets in Table~\ref{tab:compare}. We categorize existing methods into two types: (1) architecture-oriented methods, which focus on developing novel model architectures for 3D point clouds and do not involve any pre-training techniques, and (2) pre-training methods, which pay more attention to the pre-training strategy and whose backbone model are mostly limited to Transformer-base architecture. It's worth noticing that methods marked with an asterisk ($*$) incorporate additional knowledge from pre-trained image models or pre-trained vision-language models. To ensure a fair and unbiased comparison, we refrain from directly comparing our method with these approaches. However, we include them in the listing for reference purposes, acknowledging their existence and potential relevance in related research.

From the experimental results, we can see that accompanied by PointMLP backbone model, our proposed TAP pre-training achieves the best classification accuracy on ScanObjectNN and ModelNet40 among existing models (with no pre-trained knowledge from image or language like P2P~\cite{wang2022p2p}), demonstrating the effectiveness of our approach and validating the superiority of our 3D-to-2D cross-modal generative pre-training method over previous generative pre-training methods. Moreover, we also note that our proposed method has brought higher performance improvements on the ScanObjectNN dataset than on ModelNet40. This may be attributed to the reason that the cross-modal generative pre-training has enhanced the network's ability to understand point clouds from different views, which is beneficial for a more robust understanding of the real-scan data with more noise and disturbance in the ScanObjectNN dataset.


\vspace{6pt}
\noindent\textbf{Visualization Results.} Figure~\ref{fig:examples} shows the visualization results of TAP. The first row shows the generated view images while the second row displays the ground truth images for reference. The TAP method can successfully predict the accurate shape of the object and the RGB colors that represent light reflections in rendered images. Therefore, TAP is capable of capturing the geometric structure of 3D objects and reasoning occlusion relations from specific camera poses.

% \vspace{-5pt}
\subsubsection{Part Segmentation} 
Performing dense prediction is always a more challenging task compared with classification. In this section, we evaluate the local distinguishability of our proposed TAP pre-training method, fine-tuning the pre-trained point cloud model on the ShapeNetPart dataset for the part segmentation task. Quantitative results are shown in Table~\ref{tab:partseg}. We implement PointMLP as the backbone model and compare our TAP results with two mainstreams of previous literature. The upper row displays classical architecture-oriented methods that focus on network design and are trained from scratch. The lower row shows members of the generative pre-training family that rely on Transformer-based architectures. 

According to results comparisons, our TAP pre-training significantly improves the part segmentation performance of the PointMLP backbone, increasing class mIoU by 0.6 and instance mIoU by 0.8. More importantly, our TAP pre-training achieves state-of-the-art performance on both class mIoU and instance mIoU, surpassing leading works in both tracks. Specifically, TAP exceeds the performance of Point-M2AE on instance mIoU by 0.4. This satisfactory performance serves as strong evidence to convey that our proposed TAP pre-training is superior to previous uni-model generative pre-training mechanisms in dense prediction tasks. This may be attributed to the factor that our supervision in 2D with MSE loss is more precise than the ambiguous Chamfer Distance in 3D reconstruction. Therefore, models with TAP pre-training obtain more accurate comprehension of local geometry and detail awareness, which contributes to mIoU gain in dense prediction tasks. What's more, TAP outperforms KPConv on instance mIoU by 0.5, demonstrating that the proposed 3D-to-2D generative pre-training method can fully exploit the potential of the point cloud model and help it have a better perception of objects' geometric structure. As TAP is adaptable to any architecture, future improvements in architectural design can also benefit from TAP pre-training.

\subsubsection{Few-shot Classification}
Following Point-BERT~\cite{yu2022point}, we conduct few-shot classification with Standard Transformers on ModelNet40~\cite{modelnet} dataset. As shown in Table~\ref{tab:fewshot}, we report mean overall accuracy and standard deviation (mOA$\pm$std) on 10 pre-defined data folders for each few-shot setting. The \textit{way} and \textit{shot} in Table~\ref{tab:fewshot} specify the number of categories and the number of training examples per category, respectively. 

From the results, TAP achieves the highest mean overall accuracy across all few-shot settings when compared to previous generative pre-training approaches. Furthermore, TAP exhibits significantly lower standard deviations than those reported in the existing literature for the majority of few-shot settings, which signifies its robust performance and consistent superiority. This indicates that TAP is not only capable of achieving high mean overall accuracy but also exhibits reliability and robustness across various few-shot settings. Such stability is crucial in real-world applications, where consistency and predictability are vital for practical deployment.

\subsection{Scene-level Dense Predictions}
To assess the effectiveness of TAP in handling scene-level dense prediction tasks, we carry out experiments on more complicated scene-level object detection and semantic segmentation on the ScanNetV2~\cite{dai2017scannet} dataset. For the object detection task, we adopt 3DETR~\cite{misra20213detr} and pre-train its encoder on the object-level dataset ShapeNet~\cite{shapenet} with TAP. Average precision at 0.25 and 0.5 IoU thresholds are reported. Regarding semantic segmentation, we employ the PointTransformerV2~\cite{wu2022ptv2}(PTv2) model and pre-train it on the ScanNetV2 dataset with TAP. We report mean IoU for evaluation metric. It is worth mentioning that PTv2 represents the current state-of-the-art approach with open-source code availability.

\begin{table}[t]
    \centering
    \caption{\textbf{Few-shot Classification with Standard Transformers on ModelNet40 dataset.} We report mean overall accuracy and standard deviation on 10 pre-defined data folders for each setting. Best results are marked \textbf{bold}.}
    \label{tab:fewshot}
    \setlength\tabcolsep{2pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule[0.95pt]
    \multirow{2}{*}[-0.5ex]{Method}& \multicolumn{2}{c}{5-way} & \multicolumn{2}{c}{10-way}\\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5} & 10-shot & 20-shot & 10-shot & 20-shot\\
    % Method & 5-way 10-shot & 5-way 20-shot & 10-way 10-shot & 10-way 20-shot \\ 
    \midrule[0.6pt]
     w/o pre-training & 87.8 $\pm$ 5.2& 93.3 $\pm$ 4.3 & 84.6 $\pm$ 5.5 & 89.4 $\pm$ 6.3\\
     \midrule
     % OcCo & 94.0 $\pm$ 3.6& 95.9 $\pm$ 2.3 & 89.4 $\pm$ 5.1 & 92.4 $\pm$ 4.6\\
     Point-BERT~\cite{yu2022point} & 94.6 $\pm$ 3.1 & 96.3 $\pm$ 2.7 &  91.0 $\pm$ 5.4 & 92.7 $\pm$ 5.1\\
     MaskPoint~\cite{liu2022masked} & 95.0 $\pm$ 3.7 & 97.2 $\pm$ \textbf{1.7} & 91.4 $\pm$ 4.0 & 93.4 $\pm$ 3.5\\
     Point-MAE~\cite{pang2022masked} & 96.3 $\pm$ 2.5 & \textbf{97.8} $\pm$ 1.8 & 92.6 $\pm$ 4.1 & 95.0 $\pm$ 3.0\\
     % ACT & 96.8 $\pm$ 2.3 & 98.0 $\pm$ 1.4 & 93.3 $\pm$ 4.0 & 95.6 $\pm$ 2.8 \\
     \midrule
     TAP   & \textbf{97.3} $\pm$ \textbf{1.8} & \textbf{97.8} $\pm$ \textbf{1.7} &  \textbf{93.1} $\pm$ \textbf{2.6} & \textbf{95.8} $\pm$ \textbf{1.0} \\
     \bottomrule[0.95pt]
     \end{tabular}
    }
\end{table}


\begin{table}[!t]
    \centering
    % \setlength\tabcolsep{8pt}
    \caption{\textbf{Scene-level object detection and semantic segmentation on ScanNetV2~\cite{dai2017scannet}}. Average precision at 0.25 IoU thresholds (AP$_\textrm{0.25}$) and 0.5 IoU thresholds (AP$_\textrm{0.5}$) of detection and mean Intersection-over-Union (mIoU) of semantic segmentation are reported.}
    \label{tab:scene}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule[0.95pt]
    \multirow{2}{*}[-0.5ex]{Method}& \multicolumn{2}{c}{Det (3DETR~\cite{misra20213detr})} & Seg (PTv2~\cite{wu2022ptv2}) \\
    \cmidrule(l){2-3}\cmidrule(l){4-4} 
    & AP$_\textrm{0.25}$ & AP$_\textrm{0.5}$ & mIoU \\
    \midrule[0.6pt]
     Baseline & 62.1 & 37.9 & 72.4 \\
     +TAP & 63.0~\cred{(+0.9)} & 41.4~\cred{(+3.5)} & 72.6~\cred{(+0.2)} \\
     \bottomrule[0.95pt]
     \end{tabular}}
\end{table}


Based on the results presented in Table~\ref{tab:scene}, TAP consistently enhances the performance of all baselines, thereby showcasing its efficacy in tackling more intricate scene-level dense prediction tasks. Remarkably, even with the encoder solely pre-trained on an object-level dataset for scene-level detection task, significant improvements are observed in both AP$_\textrm{0.25}$ and AP$_\textrm{0.5}$ metrics. This suggests that the learned representations from TAP effectively capture relevant information and generalize well to complex scenes, even when the pre-training data is limited to object-level collections. Such generalization capabilities are valuable in scenarios where obtaining large-scale fully annotated scene-level datasets may be challenging or expensive.

\input{tabs/ablation/ablation.tex}

\subsection{Ablation Studies}

To investigate the architectural design of our proposed \textit{Photograph Module} in TAP pre-training pipeline, we conduct extensive ablation studies on the ScanObjectNN dataset with PointMLP as the backbone model.

\vspace{6pt}
\noindent\textbf{Photograph Module Architectural Designs.}
In Photograph Module, we implement cross-attention layers to generate view image feature maps conditioned on pose instruction. We believe that letting the module learn by itself how to rearrange 3D point features in 2D grids will enhance the representation ability of the 3D backbone. Therefore, we conduct ablation studies to verify this hypothesis. As shown in Table~\ref{tab:abl_arch}, we implement Model A$_1$ with no attention layers, directly projecting 3D feature points to 2D grids based on Eq.~\ref{eq:rotate} and Eq.~\ref{eq:proj}. This results in a much simpler pre-training task, as the projection relation has been directly told. Additionally, in Model A$_2$, we add self-attention layers after explicit projection to help the model capture longer-range correlations. Pose knowledge is encoded as a pose token that is concatenated to projected grids, similar to the CLS token in classification Transformers. According to quantitative results comparison with TAP that implements cross-attention layers, fine-tuning results of pre-training methods in A$_1$ and A$_2$ version show inferiority. Therefore, the cross-attention architecture we designed to entirely LEARN the projection relation is the most suitable choice for the proposed 3D-to-2D generative pre-training.

What's more, we discuss the number of cross-attention layers, the dimension of feature channels and whether to concatenate pad token in memory builder in Model $B,C,D$. According to the results, more cross-attention layers show stronger representation ability, while too large channel number will lead to performance decrease caused by over-fitting. The performance gain from Model $D$ to TAP also verifies that the pad token design in the memory builder is essential.

\vspace{6pt}
\noindent\textbf{Query Generator Designs.} In the query generator, we derive the mathematical formulation of the optical lines passing through 2D grids. We propose to concatenate the coordinate of origin point $O$, normalized direction $\mathbf{d}^\dagger$ and position embedding $(u/h, v/w)$ as the initial state of queries. In Table~\ref{tab:abl_query}, we first compare this mathematical design with totally learnable queries that takes pose matrix $R$ as input and implements MLP layers to predict query for each grid. As shown in Model $E$, learnable queries cannot satisfactorily encode pose information, while our derived formulation for query construction is both clearer in physical meaning and more competitive in fine-tuning accuracy.

In ablation F$_1$ to F$_4$, we progressively discuss the three components of query generation. Quantitative comparison with TAP verifies that every component is indispensable for query generation, where coordinates of origin points are of the most importance. 
