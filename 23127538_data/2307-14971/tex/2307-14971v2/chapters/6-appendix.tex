\appendix

\section{Ablation Studies}
In this section, we conduct more ablation studies on hyperparameter choices of the proposed 3D-to-2D generative pre-training, discussing more thoroughly the insights into architectural design and objective function design. We implement PointMLP~\cite{pointmlp} as the 3D backbone model and conduct these ablation experiments on the hardest PB-T50-RS variant of the ScanObjectNN~\cite{uy2019revisiting} dataset. We report the classification accuracy of the fine-tuning results.

\subsection{Cross-Attention Hyperparameters}

In Table~\ref{tab:abl_attn}, we display the results of ablation studies on the number of layers and feature channels of the cross-attention layers in our proposed Photograph module. From the quantitative results, we can conclude that 2 layers with 128 channels is the best hyperparameter group for cross-attention layers. When we implement a shallow layer setting (2 layers in line 1 and 4 layers in line 2), lower feature channels (128 dims) achieves better performance. On the contrary, when we implement a deeper layer setting (6 layers in line 3 and 8 layers in line 4), relatively higher feature channels (256 dims) is the best choice. Additionally, if we use 1024 dims as the feature channels in cross-attention layers, which is the same as the channels of output features from the 3D backbone model, the pre-training stage totally collapses and the fine-tuning results are much lower than models of 128 dims and 256 dims, no matter how much layers are implemented. This result indicates that a bottleneck design in our proposed photograph module is essential for the successful pre-training of the proposed 3D-to-2D generation.

The overall trend is that a lightweight architectural design of the cross-attention layers is better than a heavy module design. This may be because we completely drop the photograph module and only keep the 3D backbone in the fine-tuning stage. Therefore, a lightweight photograph module in the pre-training stage will encourage the 3D backbone to exploit more representation ability and avoid information loss in the fine-tuning stage to the best extent. On the contrary, if we implement a heavy photograph module with deep cross-attention layers and high feature dimensions, the photograph module will dominate the generation process and the importance of the 3D backbone will be neglected. What's worse, in the fine-tuning stage, the rich geometry information in the heavy photograph module is totally dropped out and no longer helpful for downstream tasks.

\begin{table}[!t]
\label{tab:ablation_supp}
\caption{\textbf{Ablation Studies on Hyperparameters.} We implement PoinMLP~\cite{pointmlp} as the 3D backbone model and conduct experiments on the hardest PB-T50-RS variant of ScanObjectNN~\cite{uy2019revisiting} dataset.}
\vspace{-6pt}
\centering
\begin{subtable}{0.49\textwidth}
    \setlength\tabcolsep{3pt}
    \centering
    \caption{Cross-Attention Hyperparameters.}
    \vspace{-4pt}
    \label{tab:abl_attn}
    \adjustbox{width=0.9\textwidth}{
    \begin{tabular}{c|ccc}
    \toprule
    LayerNum $\backslash$ Channels & 128 Dims  & 256 Dims &  1024 Dims \\
    \midrule
    2 Layers    & \textbf{89.1} & 87.9 & 86.3 \\
    4 Layers    & 88.7 & 88.0 & 85.9 \\
    6 Layers    & 88.3 & 88.5 & 85.3 \\
    8 Layers    & 87.7 & 88.1 & 85.8 \\
    \bottomrule
    \end{tabular}}
\end{subtable}
\hfill
\vspace{5pt}
\begin{subtable}{0.49\textwidth}
    \centering
    \setlength\tabcolsep{3pt}
    \caption{Loss Weight Hyperparameters.}
    \vspace{-4pt}
    \label{tab:abl_lossw}
    \newcolumntype{g}{>{\columncolor{Gray}}c}
    \adjustbox{width=0.9\textwidth}{
    \begin{tabular}{@{\hskip 3pt}>{\columncolor{white}[3pt][\tabcolsep]}c|cccccc|cc}
    \toprule
        Model   & G$_1$ & G$_2$ & G$_3$ & G$_4$ & G$_5$ & G$_6$ & H$_1$ & H$_2$ \\
    \midrule
        $w^\textrm{fg}$ & 2 & 5 & 10 & 20 & 30 & 50 & 0 & 20\\
        $w^\textrm{bg}$ & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1\\
        $w^\textrm{feat}$ & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 2\\
    \midrule
        Acc. (\%) & 87.1 & 87.2 & 88.0 & \textbf{88.5} & 88.0 & 86.8 & 86.3 & 87.8 \\
    \bottomrule
    \end{tabular}}
\end{subtable}
\end{table}

% Figure environment removed


\subsection{Objective Function}
In this subsection, we discuss the objective function design of our proposed 3D-to-2D generative pre-training. In our main paper, we implement pixel-level supervision with MSE loss between generative view images $I_\textrm{gen}$ and ground truth images $I_\textrm{gt}$:
\begin{equation}
    \mathcal{L}_\textrm{pix}(I_\textrm{gen}, I_\textrm{gt}) = w^\textrm{fg} \mathcal{D}(I_\textrm{gen}^\textrm{fg}, I_\textrm{gt}^\textrm{fg}) + w^\textrm{bg} \mathcal{D}(I_\textrm{gen}^\textrm{bg}, I_\textrm{gt}^\textrm{bg})
\end{equation}
where fg denotes foreground region, bg denotes background region and $\mathcal{D}$ is the MSE distance. However, in 2D generation, perceptual loss~\cite{johnson2016perceptual} is of equal importance with pixel-wise loss. While pixel-wise MSE loss focuses on low-level similarities, perceptual loss measures high-level semantic differences between feature representations of the images computed by the pre-trained loss network. Technically, perceptual loss makes use of a loss network $\phi$ pre-trained for image classification, which is typically a 16-layer VGG~\cite{simonyan2014vgg} network pre-trained on the ImageNet~\cite{russakovsky2015imagenet} dataset. If we denote $\phi_j(x)$ as the feature map with size $c_j\times h_j\times w_j$ of the $j$th layer of the network $\phi$, then the perceptual loss is defined as the Euclidean distance:
\begin{equation}
\small
    \mathcal{L}_\textrm{feat}(I_\textrm{gen}, I_\textrm{gt}) = \frac{1}{N}\sum_j\frac{1}{c_j h_j w_j}\lVert\phi_j(I_\textrm{gen})- \phi_j(I_\textrm{gt})\rVert_2^2
\end{equation}
where $N$ is the number of total layers of the VGG network and $1\leq j\leq N$. If we combine the pixel-wise loss $\mathcal{L}_\textrm{pix}$ with the perceptual loss $\mathcal{L}_\textrm{feat}$, then the final objective function of the proposed 3D-to-2D generation is:
\begin{equation}
\small
    \mathcal{L} = \mathcal{L}_\textrm{pix} + w^\textrm{feat} \mathcal{L}_\textrm{feat}
\end{equation}

In Table~\ref{tab:abl_lossw}, we conduct ablations on loss weight of foreground pixel-wise loss $w^\textrm{fg}$, background pixel-wise loss $w^\textrm{bg}$ and perceptual loss $w^\textrm{feat}$. In Model $G_1$ to $G_6$, we only implement pixel-wise loss. In Model $H_1$, we only implement perceptual loss. In Model $H_2$, we combine pixel-wise loss with perceptual loss. From the ablation results, we can conclude that $w^\textrm{fg}:w^\textrm{bg}=20:1$ is the best hyperparameter choice for pixel-wise loss. However, the perceptual loss is not effective when we compare Model $G_4$, Model $H_1$ and Model $H_2$. This is mainly due to the reason that the rendered view image of synthetic ShapeNet~\cite{shapenet} dataset is out of the distribution of the realistic ImageNet~\cite{russakovsky2015imagenet} that the loss model $\phi$ is pre-trained on. Therefore, the high-level semantic representation ability of $\phi$ on view images is relatively poor and cannot guide the optimization of the 3D-to-2D generation process. If the rendered images are more realistic with colors and background, then the perceptual loss is expected to help 3D-to-2D generative pre-training.

% Figure environment removed




\section{Visualization Results}

\subsection{Generated View Images}

Figure~\ref{fig:examples_supp} displays more visualization results of our generated view images from the 3D-to-2D generative pre-training process. We take ShapeNet~\cite{shapenet} as the pre-training dataset and implement PointMLP~\cite{pointmlp} as the 3D backbone model. The first line shows the generated results from our model while the second line shows ground truth images for reference. The visualization results convey that our 3D-to-2D generative pre-training can successfully predict the shape and colors of the objects from specific projection views. There are also some unsatisfactory cases in the last three columns, where there are some vague details in our generated images. This is mainly due to the large downsample ratio ($\times32$) in our model design.

\subsection{Feature Distributions}

Figure~\ref{fig:tsne} shows feature distributions of ModelNet40~\cite{modelnet} and ScanObjectNN~\cite{uy2019revisiting} datasets in t-SNE visualization. We choose PointMLP~\cite{pointmlp} as the 3D backbone and pre-train on ShapeNet~\cite{shapenet} dataset. We can conclude that with our proposed 3D-to-2D pre-training, the 3D backbone model can extract discriminative features after fine-tuning on downstream classification datasets.

% Figure environment removed


\subsection{Part Segmentation Visualizations}

Figure~\ref{fig:partseg} presents visualizations of part segmentation results on samples from the ShapeNetPart dataset. Each part is represented by a distinct color for clarity. These qualitative results serve as compelling visual evidence and provide a vivid illustration of the efficacy of our fine-tune model in achieving accurate part segmentation.