\section{Methodology}
\label{sec:methodology}

The objective of our system (see Fig.~\ref*{fig:system_overview}) is to offer accurate and probabilistic future estimations for trajectory data. Succinctly, it comprises four sequential steps: 
(1) transformation to the displacement space, (2) clustering step, (3) deep generative proposals, and (4) ranking proposals step. 
To gain a comprehensive understanding of each component, this section provides a detailed description of the problem we address 
and an overview of our proposed solution. Subsequently, we delve into the displacement transformation and 
clustering steps. In the succeeding subsection, we outline the process of acquiring the proposals. Lastly, we introduce the ranking 
proposals step, which assigns probabilities to the set of predicted outputs.


\subsection{Problem Statement and System Overview} \label{sec:problem_satement}

Most trajectory forecasting models generate a set of plausible futures, $\{\boldsymbol{\hat{y_i}}\}^{K}$,  given an observed {\em tracklet}, 
$\boldsymbol{x}$.
The observed {\em tracklet} is an evenly spaced time series of $T_O$ observations in the  
$XY$\nobreakdash-plane. Ultimately, the forecasting models' goal is to predict the future behavior of the input time series 
for a certain horizon of $T_P$ time steps. In this paper, we go beyond the trajectory generative process and also propose a 
post-hoc method to assign probabilities, $\{\hat{p_i}\}^{K}$, to the respective $K$ proposed trajectories. 
In this way, besides future plausible trajectories we also provide the likelihood of each trajectory. We consider the system's 
starting point a dataset composed of $N_T$ trajectories.


In the first step of our system, we transform the input trajectories into the displacement space.
As a result, we also perform any prediction in the displacement space.
In doing so, we avoid dependence on the spatial context where the input data lies. Therefore,
any downstream task is more generalizable to new domains. Then, 
we cluster the displacement input vectors 
into $K$ partitions via clustering. This step is paramount
in the system as it groups akin time series of displacements that we feed into 
the generation process of future displacements. Consequently, we propose to train cDGMs, such as 
a cVAE or a Conditional Generative Adversarial Network (cGAN) to learn the future displacements given the observed displacements
and the respective cluster class, $c$ (i.e., the identifier for the 
cluster class conceived with the ground truth displacements). As we cluster the
entire set of displacements, after training the cDGM, 
there will be a link between the prediction and both the observed displacement 
{\em tracklet} and the respective cluster class. Assuming a successful clustering and cDGM's training steps,
this link allows us to claim that the prediction generated from the cluster class $c$ will be more similar to the group of trajectories
in $c$ than the trajectories from any other cluster. Thus, during inference, we propose to generate 
$K$ future displacement vectors (one per cluster class) and use a ranking method 
to assign likelihoods to the proposed future displacements. The idea behind the ranking method is 
to assign high likelihoods to displacement vectors that resemble the ones on the respective ground truth cluster 
and do the opposite for the remaining ones. For instance, assuming that the right cluster class is $c_{i}$, 
our key insight is that the produced sample for $c_i$, $\hat{\boldsymbol{dy_i}}$,   will be more 
similar to the ground truth displacements on $c_i$ than any prediction in $\{\boldsymbol{\hat{dy_j}}\}_{j=1 \setminus i}^K$
to the respective ground truth displacements in each of the clusters in 
$\{{c_j}\}_{j=1 \setminus {i}}^{K}$.

 
\subsection{Displacement Transformation and Clustering Step} \label{sec:clustering_step}

% k-means (flattened input), ts kmeans (time series shape), FP-SCGAN (deep learning based)
% again, introduction for what is that used for 
Primarily, our method transforms the raw trajectory data, $\boldsymbol{x}\oplus\boldsymbol{y}$, 
into displacement vectors, $\boldsymbol{D} \in {\rm I\!R}^{N_{T} \times (T_O + T_P)\times 2}$, 
where each displacement vector is given by $\boldsymbol{d}=\boldsymbol{dx}\oplus\boldsymbol{dy}$ or formally the finite differences of the
observed positions:
\begin{equation}
    s = \{(d_x, d_y)^1, ..., (d_x, d_y)^{T_P}\},
\label{eq:raw_s}
\end{equation}
%
being the superscript the time step. 

At the clustering step, we group $\boldsymbol{D}$ 
into sub-partitions of akin displacements, $\{ \boldsymbol{d_j} \}_{j=1}^{K}$, where $\boldsymbol{d_j}$ is a set itself with arbitrary
cardinality (less or equal than $N_{T}$). At the end of the clustering step, 
we guarantee that $\sum_{j=1}^{K} | \{\boldsymbol{d_j}\}| = N_T$, i.e., each sample belongs to only one
cluster. We evaluate three different clustering methods: \emph{k-Means}~\cite{jin10}, \emph{TS k-Means}~\cite{tslearn20}, and 
our proposed FP SC-GAN. 
To this end, we feed \emph{k-Means} with flattened displacements vectors in the form of:
%
\begin{equation}
    s_f = \{d_{x}^1, d_{y}^1, ..., d_x^{T_P}, d_y^{T_P}\}
\end{equation}


We also evaluate the extended version for time series provided by~\cite{tslearn20}, \emph{TS k-Means}. 
The major differences between these two clustering methods are that for the \emph{TS k-Means}, the input is given by Eq.~\ref{eq:raw_s} and the similarity 
measure is evaluated time-wise (we use \emph{soft-DTW}~\cite{cuturi17}).  
Finally, Fig.~\ref*{fig:self_conditioned_gan} depicts our proposed deep feature-based clustering method, FP SC-GAN. This architecture is considered a two-fold
provider: on one side, it produces the clustering space used in the later forecasting task; on the other side, it generates 
a complete set of synthetic displacements. This synthetic data can even serve as a data augmentation process, which we do not cover 
in this work. FP SC-GAN underlies the idea that the discriminator's feature space can be meaningful for downstream tasks~\cite{liu20} (in this case, the
clustering task). To that end, FP SC-GAN comprises a conditional generator ($G$) and a discriminator ($D$). We condition the generator on a cluster class
($c$) drawn from the clustering space and white noise ($\boldsymbol{z}$). Then one MLP-based decoder generates an entire displacement vector,
$\boldsymbol{\hat{d}}=\boldsymbol{\hat{dx}\oplus\hat{dy}}$, that should resemble one of the ground truth samples belonging to the conditioning cluster. The discriminator ($D$) learns 
to distinguish generated and ground truth samples according to a score ($s$).
During training, to assess the clustering 
process, we assume that the clustering is as good as the quality of the generated displacements. We consider this a 
fair \emph{proxy} as the only deterministic input signal given to the generator is the cluster id. 
Analogously to~\cite{tiago22}, we train the FP SC-GAN with the binary cross entropy loss
to optimize the discriminator and a weighted sum of the discriminator's loss and the mean squared error (MSE) loss to optimize the generator, given by:

\begin{equation}
\begin{aligned}
    L_{G} = & \lambda~MSE(\boldsymbol{y}, \boldsymbol{\hat{y}})\\
    & + (1 - \lambda)~(\frac{1}{2} \mathbb{E}[(D(\boldsymbol{d}) - 1)^2] 
    + \frac{1}{2} \mathbb{E}[D(\boldsymbol{\hat{d}})^2]),
\end{aligned}
\label{eq:gan_loss}
\end{equation}
%
being $\boldsymbol{\hat{y}}$ the entire generated $2D$ trajectory and $\boldsymbol{y}$ the respective 
ground-truth, $\boldsymbol{\hat{d}}$ the generated displacements vector and $\boldsymbol{d}$ the respective ground truth, 
and $\lambda$ the weight applied to the MSE term. For more details on the FP SC-GAN training, we refer the reader
to~\cite{tiago22,liu20}. 
%

% Finally, based on our previous work~\cite{tiago22},
% we introduce a self-conditioned GAN method for displacements generation. In~\cite{tiago22}, we investigated a GAN-based 
% approach whose goal was to generate future displacements given as the inputs the observed {\em tracklet} and a cluster class. 
% We draw
% the cluster class from the clustered discriminator's feature space. The discriminator 
% was responsible to distinguish generated and real displacement vectors. The key insight is that if the generator produces future 
% displacement vectors, the discriminator's feature space solely focuses on the unseen part of the input displacements vectors as the observed 
% part is the same. 
% From using the clustering in a downstream forecasting task 
% perspective, this design raises one crucial problem: there is no connection between the clusters' classes 
% and the observed displacement vectors. This means that, in our previous work, the cluster classes were not carrying
% sufficient information about the observed {\em tracklet}, and then the generative process would be highly biased to 
% the unseen future set of displacements. Consequently, if we wanted to use a ranking method to discover the most likely set of
% displacements, we would have faced the issue of having equally likely trajectories. To overcome this problem, in this paper,
% we propose a new architecture to generate the complete set 
% of displacements. 



% Figure environment removed


\subsection{Deep Generative Proposals} \label{sec:deep_generative_proposals}

% cGAN & cVAE
% loss: reconstruction loss (mse) + kl divergence 

To produce potential displacement vectors, we train a cDGM that learns the mapping $\{\boldsymbol{dx}, c\}\rightarrow\boldsymbol{dy}$. 
In order to strengthen the validation of our results, we evaluate two cDGMs: a cVAE and a cGAN. In these methods, we first 
concatenate the cluster class to the input displacement vector. Then, we extract features ($f_e$) from this joint representation. The next 
step --- noise sampling --- is different in cVAE and cGAN. While training the former, a recognition netowrk, $q_\phi$, learns a low-dimensional latent representation ($\boldsymbol{z}$) by 
modeling the ground truth set of future displacements ($\boldsymbol{dy}$). During inference, it works like the conditional generator
in cGAN therefore, we sample 
this latent vector from a standard Gaussian Distribution. Afterward, we concatenate the latent representation to the temporal 
hidden representation learned by the feature extractor ($f_e$). Finally, we autoregressively generate
future displacements by decoding the last displacement vector and using $f_e$. 


To optimize the cVAE, we use a weighted sum of a reconstruction loss, given by the MSE loss, and 
a regularization loss (Kullback-Liebler divergence) encouraging the learned distribution to match the prior distribution 

\begin{equation}
    \begin{aligned}
        L_V =  & \lambda~MSE(\boldsymbol{y}, \boldsymbol{\hat{y}})\\ 
    & - (1 - \lambda)~\beta~D_{KL}[q_\phi(\boldsymbol{z} | \boldsymbol{dy}, c) \rVert p(\boldsymbol{z} | \boldsymbol{dx}, c)],
    \end{aligned}
\end{equation}
%
where $\beta$ is the weight applied to the regularization loss, 
$q_\phi(\boldsymbol{z} | \boldsymbol{dy}, c)$ and $p(\boldsymbol{z} | \boldsymbol{dx}, c)$ are the recognition and predictor networks, respectively.
For the recognition network, we employ 
an LSTM followed by linear layers. The predictor, on the other hand, utilizes an MLP to embed the input, an LSTM to extract 
temporal features, and an MLP to generate the predictions.
%

To achieve optimal performance of the cGAN, we adopt Eq.~\ref{eq:gan_loss} for 
optimization. However, it should be noted that unlike the FP SC-GAN approach, the cGAN acts as a forecaster. Consequently, 
the inputs for the MSE and discriminator losses consist of future {\em tracklets}.


During inference, the trained cDGMs output a set of $K$ possible displacement vectors, $\{\boldsymbol{\hat{dy}_i}\}^K$. This set is the input of the last component in the system --- ranking proposals --- where we 
assign likelihoods to each item.

\subsection{Ranking Proposals Step} \label{sec:ranking_step}

% distance based and classification based 
% centroids and neighbors
% auxiliar network
% likelihood transformation eq



The final step of the system aims to provide the mapping $\{\boldsymbol{\hat{dy}_i}\}^K\rightarrow\{p_i\}^{K}$, which means assigning 
probabilities to the respective $K$ predicted samples from each of the existing clusters. As mentioned before, the 
mapping to the probability space should ensure that samples from the right cluster have higher probabilities when compared to samples
from the remaining clusters.
To cope with this, one could train a deep neural network to learn the mapping $\{\boldsymbol{\hat{dy_i}}\}^K\rightarrow\{p_i\}^{K}$ 
by itself~\cite{sun21,chen22}. Alternatively, we propose mechanisms that rely on distance-based similarity measures: \emph{centroids} and \emph{neighbors}.
For both methods, we consider the inverse relationship between the distance and the similarity between samples. For instance, the smaller 
the distance to a cluster's centroid, the greater the similarity to the samples of that cluster. Similarly, in \emph{neighbors}, we hypothesize 
that the smaller the distance to $N_{neig}$ neighbors from the same cluster, the greater the similarity to those samples and so a 
higher probability of belonging to that cluster. Formally, in distance-based methods, the probability of prediction $\boldsymbol{\hat{dy_i}}$ 
belonging to the respective conditioning cluster, $c_i$, is given by:

\begin{equation}
    \hat{p}_{c_i} = \frac{\exp(\frac{1}{m_{c_i}} / \tau)}{\sum_{j}^K \exp(\frac{1}{m_{c_j}} / \tau)},     
\end{equation} 
%
which is the soft-argmax function over the inverse of the distances, $m$. For the \emph{centroids} approach, $m_{c_i}$ corresponds 
to the L2-distance to the conditioning cluster's centroid, $c_i$. For the \emph{neighbors} approach, $m_{c_i}$ is the average L2-distance 
to the $N_{neig}$ closest neighbors from cluster $c_i$. Formally, it is given as follows:
\begin{equation}
    m_{c_i} = \frac{1}{N_{neig}}\sum_j^{N_{neig}} L2(\boldsymbol{\hat{dy_i}}, \boldsymbol{dy_j})   
\end{equation}
%

We also evaluate a deep neural network~\cite{sun21,chen22}.
To this end, after training the cDGM, we use it to generate a dataset of displacements following the clustering space distribution. 
Then, we train a simple MLP that learns to classify the generated samples into the respective pseudo-labels 
given by the cluster assignments with the cross-entropy loss.
