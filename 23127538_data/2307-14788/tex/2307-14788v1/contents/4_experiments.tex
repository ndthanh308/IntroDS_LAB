\section{EXPERIMENTS}
\label{sec:experiments}

In this section we compare our proposed methods and current baselines in different settings. 
Firstly, we describe the datasets, the baselines, and the assessment metrics. 
Secondly, we show and analyze quantitatively and qualitatively the results obtained in those datasets. 
Finally, we thoroughly analyze the ranking proposals methods. 


\subsection{Datasets, Baselines, and Metrics}

% Datasets:
% Train/Test (75/25)
% - THOR
% - Argoverse
% LOO
% - Benchmark (ETH, HOTEL, UNIV, ZARA1, ZARA2)


We use two settings to evaluate our methods: train-test split and leave-one-dataset-out approach. For the former, we 
use two datasets: TH\"{O}R~\cite{rudenko20} and Argoverse~\cite{ming19}. TH\"{O}R is a human trajectory dataset, where the participants' roles --- {\em visitors}, {\em workers}, and {\em inspector}  --- are scripted. We prepocess the raw data as 
in~\cite{tiago22}, so we end up with $2052$ trajectories for training, $439$ for validation, and $441$ for testing.
Moreover, following the current benchmarks~\cite{kothari21}, we create
trajectories of $8$-time steps of observation (\SI{3.2}{s}) and $12$-time steps of forecasting (\SI{4.8}{s}). Argoverse is a 
road agents trajectory dataset, where there are also supervised classes:  autonomous vehicles ({\em av}), 
regular vehicles ({\em agents}), and other road agents ({\em others}). For this dataset, analogously to 
~\cite{tiago22},  we sample $5726$, $2100$, and $1678$ trajectories for training, validation, and testing sets, respectively. Additionally, 
we perform the leave-one-dataset-out in the widely used ETH/UCY benchmark~\cite{pellegrini09,leal14}. In this benchmark, 
similarly to TH\"{O}R, the observation length is 8-time steps (\SI{3.2}{s}) and the prediction length is  $12$-time steps (\SI{4.8}{s}).
This benchmark comprises five datasets --- ETH, HOTEL, UNIV, ZARA1, and ZARA2 --- of which four are used to train the model and 
the remaining is left for testing. We use no overlapping between segments of entire trajectories. 


% Baselines:
% - Heuristic-based : CVM linear, gaussian, and constant
% - Deep Learning based:
%     - Point Estimate: LSTM
%     - Multi-modal: CF-GAN, CF-VAE 

We conduct the experiments with baselines widely used in scientific works in the trajectory prediction field:

\begin{itemize}
    \item Constant Velocity Model (\emph{CVM})~\cite{scholler20} --- heuristic model that assumes that the humans move with constant velocity and direction. In this work we also include its comparison in road agents' data 
    (Argoverse). In~\cite{scholler20}, the velocity is given by the 
    projection of the last displacement but we use a weighted sum of the previous displacements based on a Gaussian kernel provided by \cite{rudenko21}. Both methods achieve similar results.  %Therefore, \emph{GVM} can be seen as a generalization of the previous 
    %two, in which if we increase the standard deviation of the kernel, it gets closer to \emph{LIN}, and the contrary yields a similar 
    %behavior to \emph{CVM}.
    
    \item RED-LSTM predictor (\emph{RED})~\cite{becker18} --- it is a stack of an LSTM ($64$ hidden dimensions) 
    and a 
    2-layer MLP (hidden dimensions in $\{32, 16 \}$) that receives linearly embedded displacements (linear layer with $16$ hidden dimensions).
    After every layer, we use a PreLU activation function.

    \item Context-free GAN (\emph{CF-GAN}) and Context-free VAE (\emph{CF-VAE})~\cite{kothari21} --- deep generative approaches based on~\cite{kothari21} but we remove any mechanisms that aim to model social interactions since 
    we are solely interested in seeing the potential of trajectory-related additional information. \emph{CF-VAE}, \emph{CF-GAN}, and 
    our deep generative proposals methods described in Sec.~\ref{sec:deep_generative_proposals} have the same design choice: initial linear
    layer with $16$ hidden dimensions, an LSTM with $64$ hidden dimensions and a final linear layer with $32$ hidden dimensions to decode the temporal features. 
    To investigate the ability to generate a wide range of plausible trajectories, we incorporate the \emph{k-variety loss} 
    introduced in \cite{gupta18}, as a replacement for the MSE partial loss utilized in the generator's loss 
    (in the case of \emph{CF-GAN}) and the VAE's loss (in the case of \emph{CF-VAE}).

\end{itemize}

As described in Section~\ref{sec:deep_generative_proposals}, we compare two cDGMs: cVAE (\emph{OURS-VAE}) and cGAN (\emph{OURS-GAN}). 
Additionally, we evaluate three clustering algorithms: \emph{k-Means}, \emph{TS k-Means}, and the proposed \emph{FP SC-GAN}. Furthermore,
we have three ranking proposals methods: (1) based on the Euclidean distance to the centroids (\emph{cent}); (2) based on the 
Euclidean distance to the $N_{neig}$ closest neighbors (set to $20$) in the displacement space and feature space, denoted as \emph{neigh-ds} and \emph{neigh-fs}, 
respectively. It is important to note here that \emph{neigh-fs} is only used in \emph{FP SC-GAN} as it is the unique method that 
makes use of a deep feature space; (3) based on the classification provided by the auxiliary network (\emph{anet}).
As our methods rely on a clustering process, we need to determine the number of clusters in each dataset. To 
do so, we average the results of five runs of each clustering method. For the remaining experiments, we use the number 
of clusters that yields the smallest Daviesâ€“Bouldin Index (DBI)~\cite{davies79}.
% We present the results in Tab.~\ref{tab:n-clusters}, which shows the final number of clusters we used for the remaining experiments.


% Metrics:
% Top-k ADE:
% Top-k FDE:
The metrics we use to compare the different methods rely on Average Displacement Error (ADE) that measures the average Euclidean distance between the predicted positions 
and the ground truth and the Final Displacement Error (FDE) that measures the Euclidean distance between the final predicted position and the respective ground truth (at $t=T_P$).
To compare both deterministic point estimate predictors (\emph{CVM} and \emph{RED}) to stochastic multimodal models (\emph{CF-GAN}, 
\emph{CF-VAE}, \emph{VAE-OURS}, and \emph{GAN-OURS}), we provide the following metrics:
\begin{enumerate}
  \item Top-$3$ ADE/FDE (in meters) --- used to assess the multimodal estimates produced by generative models. From \emph{CF-GAN} and \emph{CF-VAE}, we sample $3$ trajectories and evaluate the closest one to the ground truth, whereas from \emph{GAN-OURS} and \emph{VAE-OURS}, we take the $3$ most likely trajectories and compare the closest one to the ground truth.
  \item Top-$1$ ADE/FDE (in meters) --- used to compare point estimate predictors to stochastic multimodal models. From \emph{CF-GAN} and \emph{CF-VAE}, we use the first prediction generated by the models, while from \emph{GAN-OURS} and \emph{VAE-OURS}, we use the most likely trajectory.
\end{enumerate} Finally, to compare the ranking proposals methods, we use accuracy (in \%). In this case we compare the output of the ranking proposals methods to the soft labels yielded by 
each clustering method.

\subsection{Results}

In this section we show the results obtained by our methods and the baselines in the datasets, where bold scores denote the best score, and the results from Deep Learning (DL)-based approaches are averaged over five runs. 
First, as the main focus of our approach is to generate a probabilistic yet accurate set of predictions, we depict in Tab.~\ref{tab:res1_adefde} 
the Top-3 ADE/FDE results for deep generative approaches. 
Our methodology exhibits superior performance compared to the baseline models across all datasets, indicating that among the 
three most probable predicted trajectories, our system produces a more accurate prediction. 
This finding reinforces the notion that generating multimodality within the conditioning cluster, as opposed to 
relying on the latent space learned through the \emph{k-variety loss}, leads to improved outcomes. 
Furthermore, it is noteworthy to mention that the various clustering methods and ranking proposal 
mechanisms yield similar results across the datasets. However, in some specific datasets, one 
can find better results in other configurations: in TH\"{O}R with \emph{GAN-OURS}, \emph{cent} yields $0.45\pm0.02$ and $0.75\pm0.03$ and in ZARA2 
where \emph{TS k-Means} followed by \emph{neig} yields $0.37\pm0.01$ and $0.62\pm0.01$, for ADE and FDE, respectively.


\begin{table}[t]
  \begin{minipage}{\columnwidth}
  \centering
  \caption{Top-3 ADE/FDE ($\downarrow$) metrics in the test sets.}
  \label{tab:res1_adefde}
  \resizebox{0.98\columnwidth}{!}{%
  \begin{tabular}{@{}c|cc|cc@{}}
  \toprule
  Datasets &
  \emph{CF-GAN} &
  \emph{GAN-OURS}$^1$ &
  \emph{CF-VAE} &
  \emph{VAE-OURS}$^1$ \\ \midrule
  TH\"{O}R &
    \begin{tabular}[c]{@{}c@{}}$0.57\pm0.01$\\ $1.04\pm0.03$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.53\pm0.01}$\\ $\mathbf{0.84\pm0.03}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.62\pm0.02$\\ $1.05\pm0.05$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.56\pm0.01}$\\ $\mathbf{0.89\pm0.02}$\end{tabular} \\ \midrule
  Argoverse &
    \begin{tabular}[c]{@{}c@{}}$1.62\pm0.07$\\ $2.81\pm0.14$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{1.56\pm0.02}$\\ $\mathbf{2.69\pm0.02}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.96\pm0.02$\\ $3.44\pm0.06$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{1.62\pm0.02}$\\ $\mathbf{2.82\pm0.04}$\end{tabular} \\ \specialrule{.2em}{.1em}{.1em}  
  ETH &
    \begin{tabular}[c]{@{}c@{}}$0.84\pm0.03$\\ $1.64\pm0.06$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.77\pm0.04}$\\ $\mathbf{1.60\pm0.11}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.94\pm0.02$\\ $1.84\pm0.04$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.82\pm0.02}$\\ $\mathbf{1.60\pm0.04}$\end{tabular} \\ \midrule
  HOTEL &
    \begin{tabular}[c]{@{}c@{}}$0.87\pm0.07$\\ $1.64\pm0.12$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.81\pm0.10}$\\ $\mathbf{1.46\pm0.11}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.08\pm0.04$\\ $1.95\pm0.06$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.97\pm0.08}$\\ $\mathbf{1.72\pm0.14}$\end{tabular} \\ \midrule
  UNIV &
    \begin{tabular}[c]{@{}c@{}}$0.56\pm0.01$\\ $1.08\pm0.02$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.51\pm0.01}$\\ $\mathbf{0.98\pm0.03}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.61\pm0.01$\\ $1.16\pm0.01$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.56\pm0.01}$\\ $\mathbf{1.06\pm0.02}$\end{tabular} \\ \midrule
  ZARA1 &
    \begin{tabular}[c]{@{}c@{}}$0.43\pm0.02$\\ $0.82\pm0.07$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.37\pm0.01}$\\ $\mathbf{0.72\pm0.03}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.48\pm0.01$\\ $0.98\pm0.04$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.44\pm0.01}$\\ $\mathbf{0.91\pm0.03}$\end{tabular} \\ \midrule
  ZARA2 &
    \begin{tabular}[c]{@{}c@{}}$0.46\pm0.01$\\ $0.81\pm0.05$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.40\pm0.01}$\\ $\mathbf{0.65\pm0.03}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.49\pm0.01$\\ $0.86\pm0.05$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.43\pm0.01}$\\ $\mathbf{0.73\pm0.01}$\end{tabular} \\ \bottomrule
  \end{tabular}%
  }
  \vspace{0.2em} % Adjust the vertical spacing between the table and footnote
  \\ \textsuperscript{1}\tiny{\emph{FP SC-GAN} + \emph{neig-fs}}
  \end{minipage}
  \end{table}



The fact that our system performs better on the leave-one-dataset-out setting suggests that the clusters are 
unbiased to the reference dataset. In particular, Tab.~\ref{tab:res_hotel_adefde} shows the results in the 
HOTEL dataset with \emph{GAN-OURS}, where the trajectories go in a different direction than most of the ones in the training 
set~\cite{scholler20}. Here we can see that the proposed \emph{FP SC-GAN} is more robust to the distribution
shift present in this dataset.

  \begin{table}[t]
  \begin{minipage}{\columnwidth}
  \centering
  \caption{Top-3 ADE/FDE ($\downarrow$) metrics in HOTEL with GAN-OURS.}
  \label{tab:res_hotel_adefde}
  \resizebox{0.8\columnwidth}{!}{%
  \begin{tabular}{c|lll}
  \toprule
  \begin{tabular}[c]{@{}c@{}}Ranking\\ proposals\end{tabular} &
    \multicolumn{1}{c}{\emph{K-means}} &
    \multicolumn{1}{c}{\emph{TS K-means}} &
    \multicolumn{1}{c}{\emph{FP SC-GAN}} \\ \hline
  \emph{cent} &
    \begin{tabular}[c]{@{}l@{}}$1.03\pm0.05$\\ $1.80\pm0.09$\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}$1.04\pm0.05$\\ $1.81\pm0.06$\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}$\mathbf{0.80\pm0.10}$\\ $\mathbf{1.44\pm0.12}$\end{tabular} \\ \hline
  \emph{neig}$^1$ &
    \begin{tabular}[c]{@{}l@{}}$0.98\pm0.04$\\ $1.74\pm0.06$\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}$0.97\pm0.11$\\ $1.71\pm0.16$\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}$\mathbf{0.81\pm0.10}$\\ $\mathbf{1.46\pm0.11}$\end{tabular} \\ \hline
  \emph{anet} &
    \begin{tabular}[c]{@{}l@{}}$1.02\pm0.06$\\ $1.78\pm0.10$\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}$0.97\pm0.04$\\ $1.70\pm0.04$\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}$\mathbf{0.90\pm0.06}$\\ $\mathbf{1.59\pm0.08}$\end{tabular} \\ \bottomrule
  \end{tabular}%
  }
  \vspace{0.2em} 
  \\ \textsuperscript{1}\tiny{\emph{neigh-ds} for \emph{k-Means} and \emph{TS K-means}; \emph{neigh-fs} for \emph{FP SC-GAN}}
\end{minipage}
\end{table}


To assess the variability of each model and the fine-grained accuracy, we also provide in 
Tab.~\ref{tab:res2_t1adefde} the Top-1 ADE/FDE scores. Apart from HOTEL, this result shows that our method yields similar Top-1 predictions 
to point estimate and multimodal baselines, where \emph{RED} stood out the most. 
Nevertheless, when considering a comparison between the generative models \emph{CF-GAN} and \emph{CF-VAE} and our proposed methods \emph{GAN-OURS} 
and \emph{VAE-OURS}, which share the same network structure, our methods demonstrate superior performance in terms of Top-3 scores across 
all datasets. Additionally, our methods achieve equal or superior Top-1 scores in all datasets. Notably, our methods also effectively alleviate the performance disparity resulting from the models' variability when comparing Top-3 and Top-1 results.
On top of that, our methods yield more information encoded in the probabilities 
assigned to the predicted trajectories, thus, creating a space of probable future locations rather than uninformative predictions.
It is also interesting to note that 
\emph{CVM} provides similar or better performance in human trajectory 
data. However, for road agents' trajectory data, this method cannot cope with the speed variation in the trajectories.


\begin{table}[t]
  \begin{minipage}{\columnwidth}
  \centering
  \caption{Top-1 ADE/FDE ($\downarrow$) metrics in the test sets.}
  \label{tab:res2_t1adefde}
  \resizebox{0.98\columnwidth}{!}{%
  \begin{tabular}{@{}c|cc|cc|cc@{}}
  \toprule
  Datasets &
    \emph{CVM} &
    \emph{RED} &
    \emph{CF-GAN} &
    \emph{GAN-OURS}$^1$ &
    \emph{VAE} &
    \emph{VAE-OURS}$^1$ \\ \midrule
  TH\"{O}R &
    \begin{tabular}[c]{@{}c@{}}$0.79$\\ $1.28$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.65\pm0.01}$\\ $\mathbf{1.06\pm0.01}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.85\pm0.09$\\ $1.68\pm0.20$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.76\pm0.02$\\ $1.31\pm0.05$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.71\pm0.02$\\ $1.30\pm0.04$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.71\pm0.02$\\ $1.21\pm0.04$\end{tabular} \\ \midrule
  Argoverse &
    \begin{tabular}[c]{@{}c@{}}$2.57$\\ $3.93$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{1.84\pm0.01}$\\ $\mathbf{3.18\pm0.02}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$2.41\pm0.09$\\ $4.53\pm0.17$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.92\pm0.02$\\ $3.27\pm0.03$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$2.69\pm0.02$\\ $4.94\pm0.02$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.95\pm0.01$\\ $3.39\pm0.04$\end{tabular} \\ \midrule
  ETH &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.95}$\\ $2.11$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.95\pm0.01}$\\ $1.94\pm0.02$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.08\pm0.08$\\ $2.18\pm0.19$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.96\pm0.03$\\ $1.99\pm0.05$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.08\pm0.03$\\ $2.15\pm0.08$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.95\pm0.02}$\\ $1.95\pm0.03$\end{tabular} \\ \midrule
  HOTEL &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.42}$\\ $\mathbf{0.74}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.03\pm0.04$\\ $1.84\pm0.06$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.03\pm0.05$\\ $1.99\pm0.11$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.95\pm0.13$\\ $1.72\pm0.19$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.18\pm0.05$\\ $2.21\pm0.08$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$1.09\pm0.12$\\ $1.96\pm0.18$\end{tabular} \\ \midrule
  UNIV &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.65}$\\ $1.29$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.65\pm0.01}$\\ $\mathbf{1.27\pm0.01}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.74\pm0.03$\\ $1.49\pm0.05$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.74\pm0.01$\\ $1.44\pm0.04$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.71\pm0.01$\\ $1.41\pm0.01$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.69\pm0.01$\\ $1.36\pm0.01$\end{tabular} \\ \midrule
  ZARA1 &
    \begin{tabular}[c]{@{}c@{}}$0.54$\\ $1.05$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.45\pm0.01}$\\ $\mathbf{0.87\pm0.01}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.60\pm0.11$\\ $1.21\pm0.24$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.47\pm0.01$\\ $0.92\pm0.03$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.64\pm0.01$\\ $1.33\pm0.03$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.53\pm0.01$\\ $1.10\pm0.02$\end{tabular} \\ \midrule
  ZARA2 &
    \begin{tabular}[c]{@{}c@{}}$0.55$\\ $0.91$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$\mathbf{0.50\pm0.01}$\\ $\mathbf{0.80\pm0.01}$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.65\pm0.02$\\ $1.24\pm0.09$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.52\pm0.01$\\ $0.86\pm0.04$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.60\pm0.02$\\ $1.13\pm0.07$\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}$0.54\pm0.01$\\ $0.93\pm0.01$\end{tabular} \\ \bottomrule
  \end{tabular}%
  }
  \vspace{0.2em} % Adjust the vertical spacing between the table and footnote
  \\ \textsuperscript{1}\tiny{\emph{FP SC-GAN} + \emph{neig-fs}}
  \end{minipage}
  \end{table}







In Fig.~\ref{fig:top3}, we show qualitative results of the Top-3 predictions from our 
methods and baselines in TH\"{O}R (left), Argoverse (center), and ZARA1 
(right) test sets. TH\"{O}R example pertains to a quite challenging scenario as the heading change 
is sharp. Still, our method could capture this uncommon behavior within 
the three most probable trajectories (with $p=0.26$), while the most probable trajectory ($p=0.48$) 
is following the movement's trend, which is reasonable due to the most common constant velocity profile in humans walking~\cite{scholler20}. In Argoverse 
example, the auxiliary network plays an important role in the hierarchical predictions: Top-1 prediction
($p_1=0.29$) is also the closest to the ground truth; the second most likely trajectory ($p_4=0.23$) 
is following the same direction but with a shorter distance; 
finally, the third most likely trajectory ($p=0.16$) goes in a different direction, but we consider it 
a still reasonable prediction. Finally, in ZARA2 example, while \emph{CF-GAN} could not capture the 
static behavior ($X_0 \sim Y_{n_{pred}}$) but still yields a broad range of 
behaviors, our method conditioned by the cluster id assigns the highest likelihood to the predicted 
static behavior ($p=0.40$). 

% Figure environment removed

\subsection{Ranking Proposals Analysis}


In this section, we analyze the ranking proposals methods with the estimates from \emph{GAN-OURS}. Tab.~\ref{tab:acc_rank_proposals} shows the accuracy of the ranking proposals methods in the test sets 
of the train test split settings (TH\"{O}R and Argoverse).
It is evident that the accuracy of the ranking proposals methods directly
affects both Top-3 and Top-1 results presented in Tab.~\ref{tab:res1_adefde} and Tab.~\ref{tab:res2_t1adefde}, respectively. 
A broad view of the results shows that the more accurate our ranking proposals methods are, the better the ADE/FDE scores, specially Top-1 ADE/FDE. 
As it is possible to observe within each clustering method and dataset, \emph{anet} is (statistically) better only in TH\"{O}R for 
all clustering methods. We speculate that this may be because the clusters in TH\"{O}R are closer to each other as people were moving 
in the same environment (with similar moving patterns) in both train and test sets. This phenomenon is less noticeable in 
Argoverse since different behaviors stem from cars ({\em agents} and {\em av}) and other road agents ({\em others}). Similarly, in the 
leave-one-dataset-out setting (see Tab.~\ref{tab:acc_rank_proposals_loo}), the diversity of behaviors come from the different datasets. In the majority of these datasets, our proposed distance-based 
methods (\emph{cent}, \emph{neigh-ds} and \emph{neigh-fs}) provide better results than an auxiliary deep neural network (\emph{anet}) across the different clustering methods.
Furthermore, while a constant-width MLP-based \emph{anet} provides accurate estimates, it computationally scales 
as $\mathcal{O}({N_L} H_U^2)$, where $N_L$ is the number of layers of the MLP network and $H_U$ is the number of hidden units per layer.
On the contrary, distance-based methods, \emph{centroids} and \emph{neighbors}, require $\mathcal{O}(K)$ and $\mathcal{O}(K N_{neig})$ computations, respectively.
Hence, besides the fact that distance-based methods do not require training an additional neural network, during inference, they provide similar performance in \emph{linear time} while auxiliary networks require \emph{quadratic time}.




\begin{table}[t]
  \centering
  \caption{Accuracy ($\uparrow$) of the ranking proposals methods in the test sets of the tran test split setting.}
  \label{tab:acc_rank_proposals}
  \resizebox{0.75\columnwidth}{!}{%
  \begin{tabular}{cc|lllllll@{}}
  \toprule
  \begin{tabular}[c]{@{}c@{}}Clustering \\ method\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Ranking\\ proposals\end{tabular} &
    \multicolumn{1}{c}{TH\"{O}R} &
    \multicolumn{1}{c}{Argoverse} \\ \hline
  \multirow{3}{*}{\emph{k-Means}} &
    \emph{cent}& $83.0\pm0.5$
     & $95.1\pm0.2$
     \\
   &
    \begin{tabular}[c]{@{}c@{}}\emph{neigh-ds}\end{tabular} & $81.1\pm0.8$
     & $94.0\pm0.2$
     \\
   &
    \emph{anet}&
    \multicolumn{1}{c}{$\mathbf{86.6\pm0.6}$} &
    \multicolumn{1}{c}{$\mathbf{95.2\pm0.2}$}  \\ \hline
  \multirow{3}{*}{\emph{TS k-Means}} &
    \emph{cent}& 
    \multicolumn{1}{c}{$66.4\pm0.4$} &
    \multicolumn{1}{c}{$93.9\pm0.9$}  \\
   &
    \begin{tabular}[c]{@{}c@{}}\emph{neigh-ds}\end{tabular} &
    \multicolumn{1}{c}{$65.8\pm0.4$} &
    \multicolumn{1}{c}{$94.6\pm0.3$} \\
   &
    \emph{anet}& $\mathbf{72.7\pm0.8}$
     & $\mathbf{96.0\pm0.1}$
     \\ \hline
  \multirow{4}{*}{\emph{FP SC-GAN}} & 
    \emph{cent}& $64.3\pm5.6$
     & $\mathbf{96.5\pm0.4}$
     \\
   &
   \begin{tabular}[c]{@{}c@{}}\emph{neigh-fs}\end{tabular} & $68.0\pm2.7$
     & $96.3\pm5.1$
     \\
   &
    \emph{anet}& $\mathbf{70.3\pm1.3}$
     & $94.9\pm0.7$
     \\ \bottomrule
    
  \end{tabular}%
  }
  \end{table}



\begin{table}[t]
  \centering
  \caption{Accuracy ($\uparrow$) of the ranking proposals methods in the test sets in the 
leave-one-dataset-out setting.}
  \label{tab:acc_rank_proposals_loo}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{cc|lllllll@{}}
  \toprule
  \begin{tabular}[c]{@{}c@{}}Clustering \\ method\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Ranking\\ proposals\end{tabular} &
    \multicolumn{1}{c}{ETH} &
    \multicolumn{1}{c}{HOTEL} &
    \multicolumn{1}{c}{UNIV} &
    \multicolumn{1}{c}{ZARA1} &
    \multicolumn{1}{c}{ZARA2} \\ \hline
  \multirow{3}{*}{\emph{k-Means}} &
    \emph{cent}
     & $94.9\pm1.0$
     & $\mathbf{100.0\pm0.0}$
     & $\mathbf{84.0\pm0.4}$
     & $92.8\pm0.2$
     & $94.7\pm0.3$
     \\
   &
    \begin{tabular}[c]{@{}c@{}}\emph{neigh-ds}\end{tabular} 
     & $\mathbf{97.7\pm0.8}$
     & $99.2\pm0.3$
     & $80.8\pm1.0$
     & $89.7\pm0.9$
     & $\mathbf{95.5\pm0.2}$
     \\
   &
    \emph{anet}&
    \multicolumn{1}{c}{$93.3\pm2.0$} &
    \multicolumn{1}{c}{$98.6\pm0.4$} &
    \multicolumn{1}{c}{$82.5\pm0.9$} &
    \multicolumn{1}{c}{$\mathbf{93.9\pm0.9}$} &
    \multicolumn{1}{c}{$95.2\pm0.4$} \\ \hline
  \multirow{3}{*}{\emph{TS k-Means}} &
    \emph{cent}& 
    \multicolumn{1}{c}{$95.3\pm1.0$} &
    \multicolumn{1}{c}{$\mathbf{100.0\pm0.0}$} &
    \multicolumn{1}{c}{$\mathbf{86.6\pm0.2}$} &
    \multicolumn{1}{c}{$92.5\pm0.2$} &
    \multicolumn{1}{c}{$94.5\pm0.1$} \\
   &
    \begin{tabular}[c]{@{}c@{}}\emph{neigh-ds}\end{tabular} &
    \multicolumn{1}{c}{$\mathbf{97.7\pm0.8}$} &
    \multicolumn{1}{c}{$99.3\pm0.0$} &
    \multicolumn{1}{c}{$82.0\pm0.9$} &
    \multicolumn{1}{c}{$90.6\pm0.6$} &
    \multicolumn{1}{c}{$94.9\pm0.1$} \\
   &
    \emph{anet}
     & $94.5\pm0.8$
     & $98.3\pm0.6$
     & $84.1\pm0.4$
     & $\mathbf{93.2\pm0.6}$
     & $\mathbf{95.0\pm0.3}$
     \\ \hline
  \multirow{4}{*}{\emph{FP SC-GAN}} & 
    \emph{cent}
     & $70.2\pm11.2$
     & $78.8\pm8.7$
     & $64.9\pm2.1$
     & $\mathbf{90.6\pm3.5}$
     & $\mathbf{91.4\pm4.0}$
     \\
   &
   \begin{tabular}[c]{@{}c@{}}\emph{neigh-fs}\end{tabular} 
     & $\mathbf{75.3\pm5.3}$
     & $\mathbf{90.6\pm6.2}$
     & $\mathbf{65.2\pm2.8}$
     & $89.7\pm5.1$
     & $89.2\pm2.2$
     \\
   &
    \emph{anet}
     & $68.6\pm8.0$
     & $83.3\pm7.9$
     & $55.8\pm2.6$
     & $\mathbf{90.6\pm2.7}$
     & $90.2\pm3.2$
     \\ \bottomrule
    
  \end{tabular}%
  }
  \end{table}