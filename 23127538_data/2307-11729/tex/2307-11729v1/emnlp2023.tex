% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{authblk}
% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{courier}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{graphicx}
\newcommand{\ctext}[1]{\raise0.2ex\hbox{\textcircled{\scriptsize{#1}}}}
\usepackage{multirow}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.8}
\usepackage{amsmath}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% If the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}}

\author{Ryuto Koike$^1$ \quad Masahiro Kaneko$^{2,1}$ \quad Naoaki Okazaki$^1$ \\
  $^1$Tokyo Institute of Technology\\$^2$MBZUAI\\
  \texttt{\{ryuto.koike, masahiro.kaneko\}@nlp.c.titech.ac.jp} \\ \texttt{okazaki@c.titech.ac.jp}
  \\}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. 
This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts.
However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts.
% , and a malicious user may be able to evade detection by using more powerful attacking methods.
Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored.
In this paper, we propose \textbf{OUTFOX}, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays.
In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect. While the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker.
Our experiments show that our proposed detector learned in-context from the attacker improves the detection performance on the attacked dataset by up to +41.3 point F1-score.
While our proposed attacker can drastically degrade the performance of the detector by up to -57.0 point F1-score compared to the paraphrasing method.\footnote{We will provide code and data to reproduce our experiments.}

\end{abstract}

% Figure environment removed

% Figure environment removed

\section{Introduction}
% 昨今のLLMの台頭
% Large Language Models (LLMs) acquire emergent language understanding and generative capabilities, including the in-context learning ability \citep{wei2022emergent}.
% With in-context learning ability, LLMs can perform an unseen task, learning from a few input-output examples without additional fine-tuning and updating model parameters \citep{brown20,ouyang22,sanh2022multitask}.

LLMs, characterized by their extensive model size and vast training data, have demonstrated emergent abilities with impressive performance across various tasks \citep{wei2022emergent}. 
These abilities include a high degree of language understanding, fluent generation, and the capacity to handle tasks unseen during training through in-context learning \citep{brown20,ouyang22,sanh2022multitask}.

% LM検出の必要性と現状
However, the remarkable capabilities of current LLMs have raised concerns about potential misuse. 
A notable example is the educational context, where students might copy and paste text generated by LLMs, such as ChatGPT \citep{chatgpt}, for their assignments. 
This concern has led to the development of detectors designed to identify LLM-generated text \citep{John23,mitchell23,aitextclassifier,tang2023science}.

% LM検出における2つの課題
Despite these efforts, current detector development faces two significant challenges. 
%% 1. 既存の検出器はパラフレーズアタックに弱く、そのディフェンスも未だ不十分
Firstly, existing detectors often perform poorly against simple attacks (e.g., paraphrasing), as highlighted by recent studies \citep{sadasivan23, krishna23}. Furthermore, there are only a few studies specifically about defending against such attacks yet. In one example, \citet{krishna23} proposed a defense that detects a text, which is similar to an LLM-generated response in an API database.
However, this approach requires needs active management by API providers, and the false positive rate can be higher with a larger database.
%% 2. 検出する取り組みにおける実験設定の不足
%%% 2-1. instruction-tunedなモデルを対象にしていない
%%% 2-2. 対象ドメインは本当に検出すべきものか不明瞭のものが多い
%%% 2-3. パラフレーズアタックは想定される人による実際の攻撃とは異なる
Secondly, many studies lack real-life scenarios. In reality, malicious users might instruct the latest instruction-tuned LLMs, such as ChatGPT \citep{chatgpt}, to generate homework assignments and fake articles. These misuses can lead to plagiarism and the spread of misinformation. In addition, they might instruct the LLMs to evade the detectors more powerfully than the paraphrasing attack.
However, many previous works tend to focus on detecting a text continuation generated by non-instruction-tuned LMs \citep{John23,mitchell23,krishna23}. They also consider generation tasks that it is unclear whether they should be detected, such as Question Answering and Commonsense Reasoning \citep{John23,mitchell23,krishna23,li2023deepfake}. Moreover, they assume the paraphrasing attack rather than the attack by user's instruction \citep{sadasivan23,krishna23}.

% In terms of the domain to be detected, some studies recently investigate LLM-generated text detection in the student homework domain \citep{liu2023argugpt,vasilatos2023howkgpt}. Specifically, \citet{liu2023argugpt} targeted the essay domain as we do, focusing on essays written by non-native English learners. However, considering the human-level generative capabilities of LLMs, it can be more difficult to distinguish LLM-generated essays from native-written essays than non-native-written ones.
% Thus, as a more difficult setting, we create a dataset to distinguish native-student-written and LLM-generated essays.

% However, it needs active actions by API providers, and based on the nature of the method, a false positive rate can be higher with a larger database.
%% 2. 検出する取り組みにおける実験設定の不足
%%% 2-1. instruction-tunedなモデルを対象にしていない
%%% 2-2. 対象ドメインは本当に検出すべきものか不明瞭のものが多い

% ; the uses for which LLMs are actually regulated.
% However, many previous works tend to focus on detecting a text continuation generated by non-instruction-tuned LMs \citep{John23,mitchell23,krishna23}.
% % unlike the latest instruction-tuned LLMs, such as OpenAI's ChatGPT \citep{chatgpt} and Google's Bard \citep{bard}. 
% Furthermore, they consider generation tasks that it is unclear whether they should be detected, such as Question Answering and Commonsense Reasoning \citep{John23,mitchell23,krishna23,li2023deepfake}.

% Furthermore, these studies often set unclear detection parameters, focusing on tasks like Question Answering and Commonsense Reasoning while overlooking more realistic scenarios such as writing essays, academic papers, and fake articles \citep{krishna23,li2023deepfake}.

% 課題に対する我々の提案
%% 1. アタックに対する検出器のロバスト性向上のためのUTFOXフレームワークの提案
To address these challenges, we propose \textbf{OUTFOX}, a novel framework designed to enhance the robustness and applicability of LLM-generated text detectors and apply this to the domain of student essays. 
As illustrated in Figure \ref{fig:outfox}, OUTFOX introduces an approach where both the detector and the attacker learn from each other's output. The attacker uses the detector's prediction labels as examples for in-context learning to generate more sophisticated attacks, while the detector uses these adversarially generated texts to improve its detection capabilities against a strong attacker. 
% (as deeply described in \S\ref{methodlogy}).
% Unlike our attacker, the recent paraphrasing attack is not necessarily very effective because it doesn't consider the features of the target detector; that is, it is not designed directly to deceive the detectors.
%% 2. Insturction-tunedなLLMを対象にした、エッセイデータセットの作成
%%% エッセイドメインを対象にした研究もあるが、不十分
To validate our approach in a more challenging and realistic scenario for the detector, we create a dataset of native-student essay writing to detect LLM-generated essays. 
% Therefore, we instructed ChatGPT to generate a pseudo-problem-statement from each essay in the dataset of \citet{feedback-prize-effectiveness} to simulate a scenario where a student would produce the given essay. 
% We then instructed an instruction-tuned LLM (as listed in \S\ref{experimental_setup}) to generate an essay based on each generated problem statement. 
Our dataset contains 15,400 triplets of essay problem statements, student-written essays, and LLM-generated essays.


% 実験結果
%% 1. in-context learningでアタックを考慮した検出器がアタックに対して性能を向上させ、ロバストになっていた
In our evaluation, we found that our proposed detector, which learns in-context from the stronger attacker, significantly improves the detection performance on the attacked dataset, with an increase of up to +21.6 point Average Recall and +41.3 point F1-score. %% 1. in-context learningを用いることで、数個の事例からLLM(特にChatGPT)はLLMと人間の文の違いを学習することが可能。そして、それはアタック前の文の検出、アタック後の文の検出、またアタック生成にも利用することが可能である。
%% 2. アタックを考慮した検出器のオリジナルデータに対する副作用（性能低下）は最小限にとどまり、それどころかエッセイ生成モデルによっては性能向上が見られた
Interestingly, our detector performs consistently well or even better on non-attacked data when considering attacks, with an average decrease of only -0.32 point Average Recall and -0.1 point F1-score. 
%% 4. 我々のアタックを考慮すると、DIPPERのアタックに対処することができるが、DIPPERを考慮しても我々のアタックに対処することはできない → （我々のアタックはパラフレーズよりも広範囲な表現を生成可能であるから？ パラフレーズは元の文から意味的にも逸脱できないが、我々のアタックは元の文の制約はなく、スクラッチでアタック生成可能）
%% 3. 検出器を考慮したアタック手法は既存のパラフレーズ手法に比べ検出器の性能をより下げたので、検出器の攻撃に最適化されていることが示唆された
Furthermore, our proposed attacker can drastically degrade the performance of the detector, with a decrease of up to -69.2 point Machine Recall and -57.0 point F1-score compared to the paraphrasing method. The details of our approach and experimental results will be discussed in the following sections.

%% 知見 %%
%% 1. in-context learningを用いることで、数個の事例からLLM(特にChatGPT)はLLMと人間の文の違いを学習することが可能。そして、それはアタック前の文の検出、アタック後の文の検出、またアタック生成にも利用することが可能である。
% 2. 検出の際は、問題文に意味的に近い事例を持ってくることが大切である。
% 3. アタックを生成する上での事例の選び方は、理想としては「問題文に意味的に近く、かつ検出器が正しく検出できる事例とできない事例が均等に含まれている状態」だが、毎回できるものではない。そこで今回は、「均等に事例が含まれているものを事前に用意した場合」と、「問題文に意味的に近い事例を毎回用意する場合」を想定した。結果としては、同じくらいの性能となったので、両方ともアタック生成の上で同程度に重要なキーでありそう。
%% 4. 我々のアタックを考慮すると、DIPPERのアタックに対処することができるが、DIPPERを考慮しても我々のアタックに対処することはできない → （我々のアタックはパラフレーズよりも広範囲な表現を生成可能であるから？ パラフレーズは元の文から意味的にも逸脱できないが、我々のアタックは元の文の制約はなく、スクラッチでアタック生成可能）

%% Limitation %%
% 1. 我々のOUTFOXフレームワークは、クエリとそのレスポンスのペアが人間と機械の文で一定数あることが前提で、事例に関係のないものを持ってきた（しか持ってこれない）場合、性能は一定低下する。
% 2. アタックを考慮した検出は、アタック→検出を全てchatgptに行わせるため、API使用コストが高い

% Figure environment removed

\section{OUTFOX Framework}
\label{methodlogy}

\subsection{The detector taking texts adversarially generated by the attacker}
\label{detector}
Figure \ref{fig:detector} demonstrates the flow of our proposed detector:
Using the problem statement $p_{\rm tgt}$ paired with a target essay $e_{\rm tgt}$ as a key, we retrieve triplets of the top-$k$ semantically closest problem statements $p'_{i}$, human-written essays $h'_{i}$, and LLM-generated essays $m'_{i}$ from train set.
Then, our proposed attacker (as detailed in \S\ref{attacker}) adversarially generates an essay $a_{i}$ from each retrieved problem statement.
Afterward, our detector employs a mixture of the retrieved human-written, LLM-generated, and attacked essays as examples with correct labels $\{(h'_{i}, l_{i}^{\rm h}), \cdots, (a_{j}, l_{j}^{\rm m})\}$ for in-context learning.
Here, $l_{i}^{\rm h}$ stands for a binary gold label for a human-written essay, and $l_{j}^{\rm m}$ indicates a binary gold label for an LLM-generated essay or attacked essay.
Finally, our detector output a prediction label $\hat{l}$ for the target essay so that the following probability is highest:

\vspace{0.5mm}
\begin{equation}
\hat{l} = \underset{l \in L}{\operatorname{argmax}} P_{\rm LLM}\left(l \mid C_{\rm det}, e_{\rm tgt}\right)
\end{equation}
\begin{equation}
C_{\rm det}=(I, \{(h'_{i}, l_{i}^{\rm h}), \cdots, (a_{j}, l_{j}^{\rm m})\})
\end{equation}
\vspace{0.3mm}

\noindent
Here, $L$ represents possible labels: ``\texttt{Human}'' or ``\texttt{LM}''.
The task instruction $I$ is ``\texttt{Please classify whether the text is generated by a Human or a Language Model(LM).}''
An example of Human-written essay with a correct label $(h'_{i}, l_{i}^{\rm h})$ is ``\texttt{Text:} $h'_{i}$ \texttt{Answer: Human}.''
An example of LLM-generated essay with a correct label $(m_{j}, l_{j}^{\rm m})$ is ``\texttt{Text:} $m_{j}$ \texttt{Answer: LM}.''
An example of attacked essay with a correct label $(a_{j}, l_{j}^{\rm m})$ is ``\texttt{Text:} $a_{j}$ \texttt{Answer: LM}.''

% Figure environment removed

\subsection{The attacker taking predictions by the detector}
\label{attacker}
Unlike the paraphrasing attack, our attacker generates an essay that fools the detector from a problem statement.
Figure \ref{fig:attacker} illustrates the flow of our proposed attacker:
Given a problem statement $p_{\rm src}$, we first retrieve triplets of the top-$k$ semantically closest problem statements $p'_{i}$, human-written $h'_{i}$ and LLM-generated essays $m'_{i}$ from our train set.
Then, our detector predicts labels $\hat{l}$ on the retrieved LLM-generated essays using other in-context examples from our train set. 
At this time, the detector doesn't consider the attacked essays because it's a single loop.
Finally, we feed a prompt into an LLM to adversarially generate an essay that is harder to detect. The prompt includes the description of the in-context examples $I_d$, the shuffled in-context examples $\{(p'_{i}, \hat{l}_{i}, m_{i}),\cdots, (p'_{j}, \hat{l}_{j}, m_{j})\}$, an essay-generation task instruction $I$, and the given problem statement.
Our attacker adversarially generates an attacked essay $a$ according to the following probability:

\begin{equation}
\hat{a_t} \approx \underset{a_t \in V}{\operatorname{argmax}} P_{\rm LLM}\left(a_t \mid C_{\rm atk}, p_{\rm src}, \hat{a}_{1:t-1}\right)
\end{equation}
\begin{equation}
C_{\rm atk}= (I_d, \{(p_{i}, \hat{l}_{i}, m_{i}),\cdots, (p_{k}, \hat{l}_{k}, m_{k})\}, I)
\end{equation}

\noindent
Here, $V$ is a vocabulary of an LLM.
The description of the in-context examples $I_{d}$ is ``\texttt{Here are the results of detecting whether each essay from each problem statement is generated by a Human or a Language Model(LM)}.'' The in-context example $(p_{i}, \hat{l}_{i}, m_{i})$ is ``\texttt{Problem Statement:}$p_{i}$ \texttt{Answer:} $\hat{l}_{i}$ \texttt{Essay:} $m_{i}$.''
The task instruction $I$ is ``\texttt{Given the following problem statement, please write an essay detected as Human in N words with a clear opinion}.''
$N$ is the number of words in the human-written essay paired with the given problem statement.

\section{Constructing a dataset to detect LLM-generated essays}
\label{dataset}
We build a dataset on student essay writing, specifically to detect LLM-generated essays. There are already some datasets for the purpose of automatically scoring student-written essays \citep{asap-aes,feedback-prize-effectiveness}, but few of them have abundant essay-problem statement pairs. To get LLM-generated essays, we need essay problem statements.
We focus on the essay dataset of \citet{feedback-prize-effectiveness}, consisting of argumentative essays written by native students from 6th grade to 12th grade in the U.S.
As illustrated in Fig \ref{fig:dataset}, we instruct ChatGPT to generate a pseudo-problem-statement to mimic a setting where a student would produce the supplied essay. Afterward, we instruct an instruction-tuned LLM (as listed in \S\ref{experimental_setup}) to generate an essay based on each generated problem statement.
For each instruction-tuned LLM, our dataset contains 15,400 triplets of essay problem statements, student-written essays, and LLM-generated essays. In our evaluation, we split the whole dataset into three parts: train/validation/test is 14400/500/500.
We will provide the data statistics of our dataset in an Appendix.

\section{Experiments}
In our experiments, we investigate the following three aspects: 
\begin{itemize}
\item How robust is our detector considering the attacker's output against attacked data? 
\item How well can our detector considering the attacks, consistently perform even on non-attacked data? 
\item Is our attacker stronger than the previous paraphrasing attack approach?
\end{itemize}

\subsection{Overall setup}
\label{experimental_setup}
\paragraph{Essay Generation Task and Models}
We consider instruction-tuned LMs to generate essays: ChatGPT (gpt-3.5-turbo-0613) and GPT-3.5 (text-davinci-003), and FLAN-T5-XXL\footnote{https://huggingface.co/google/flan-t5-xxl}.
To generate essays, we don't focus on open-ended generation, feeding the first $n$ tokens of a human essay and completing the continuous tokens. Instead, we feed an instruction (as shown in Figure \ref{dataset}), including a problem statement of an essay, into these models.
In generating essays, we set $\mathsf{temperature}$ to 1.3.

\begin{table*}[t]
\centering
\small
\begin{tabular}{cccccc}\hline
\multicolumn{6}{c}{\textbf{The detection on attacked essays considering attacks}}\\
\multirow{2}{*}{\textbf{Attacker}}& \multirow{2}{*}{\textbf{Detector}} & \multicolumn{4}{c}{\textbf{Metrics (\%) ↑}}\\\cline{3-6}
& & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1}\\\hline
\multirow{4}{*}{DIPPER}&\multicolumn{1}{l}{\cellcolor{Gray}Ours} & \cellcolor{Gray}98.6 & \cellcolor{Gray}66.2 & \cellcolor{Gray}82.4 & \cellcolor{Gray}79.0\\
&\multicolumn{1}{l}{+ DIPPER} & 98.2 & 79.6 & 88.9 & 87.8 \\
&\multicolumn{1}{l}{+ Ours (Fixed)} & 96.8 & 74.8 & 85.8 & 84.0\\
&\multicolumn{1}{l}{+ Ours (Tf-idf)} & 97.8 & 72.4 & 85.1 & 82.9\\
\hline
\multirow{4}{*}{Ours (Fixed)}&\multicolumn{1}{l}{\cellcolor{Gray}Ours} & \cellcolor{Gray}98.6 & \cellcolor{Gray}25.0 & \cellcolor{Gray}61.8 & \cellcolor{Gray}39.6\\
&\multicolumn{1}{l}{+ DIPPER} & 99.0 & 22.0 & 60.5 & 35.8 \\
&\multicolumn{1}{l}{+ Ours (Fixed)} & 96.8 & 70.0 & 83.4 & 80.8\\
&\multicolumn{1}{l}{+ Ours (Tf-idf)} & 98.4 & 67.6 & 83.0 & 79.9\\
\hline
\multirow{4}{*}{Ours (Tf-idf)}&\multicolumn{1}{l}{\cellcolor{Gray}Ours} & \cellcolor{Gray}98.8 & \cellcolor{Gray}24.8 & \cellcolor{Gray}61.8 & \cellcolor{Gray}39.4\\
&\multicolumn{1}{l}{+ DIPPER} & 98.6 & 20.8 & 59.7 & 34.0 \\
&\multicolumn{1}{l}{+ Ours (Fixed)} & 96.8 & 64.8 & 80.8 & 77.1\\
&\multicolumn{1}{l}{+ Ours (Tf-idf)} & 97.2 & 69.6 & 83.4 & 80.7\\
\hline
\end{tabular}
\caption{\label{robustness_of_our_detector}
The detection performance of our detector on attacked essays before and after considering the attack. The attackers include our fixed attacker, our tf-idf attacker, and DIPPER. 
In the gray-highlighted rows, we show the detection performance of our detector without considering each attack on each attacked essay. Our detector robustly performs better on attacked essays after considering the same attacks than before considering ones.
}
\end{table*}

\paragraph{Evaluation Metrics and Dataset}
AUROC (Area Under Receiver Operating Characteristic curve) can be applied only to the detectors which output real number prediction scores.
Thus, we employ AvgRec as our first metric for detection performance, following \citet{li2023deepfake}. AvgRec is the average of HumanRec and MachineRec.
In our evaluation, HumanRec is the recall for detecting Human-written texts, and MachineRec is the recall for detecting LLM-generated texts. 
% Additionally, MachineRec is useful for evaluating how much an attacker can degrade a detection performance.
Our second metric is the F1-score on LLM-generated essays: Harmonic mean of precision and recall, commonly used in classification tasks.
We compute a classification threshold for each baseline detector on the validation set where the Youden Index\footnote{The Youden Index is the difference between TPR (True Positive Rate) and FPR (False Positive Rate). The cut-off point in the ROC curve, where the Youden Index is maximum, is the best trade-off between TPR and FPR.} \cite{1950youden} is maximum in the ROC curve.
Our validation set consists of 500 human-written essays and 500 LLM-generated essays.
Finally, we evaluate detectors with these metrics and thresholds on our test set: the mixture of 500 human-written essays and 500 LLM-generated essays. 

\paragraph{Detection methods}
Our proposed detector is based on ChatGPT (gpt-3.5-turbo-0613). We set $\mathsf{temperature}$ and $\mathsf{top\_p}$ to 0 in order to relieve the randomness of our detection.
We employ two types of in-context examples for our detector. The first type is random examples retrieved for each essay. The second type is tf-idf retrieved examples for each essay. In both settings, the number of examples is 10, including 5 human-written and 5 LLM-generated essays.
In detection considering attacks, the in-context examples are 5 human-written, 3 attacked, and 2 LLM-generated essays.
As a comparison (\S\ref{comparison}), we compare our detector to the following detectors, divided into two groups: statistical outlier approaches and supervised classifiers.
The first group includes Rank, LogRank, Log probability, and DetectGPT (as explained in \S\ref{baselines}).
The second group includes OpenAI's RoBERTa-based GPT-2 classifiers (Base, Large).
We employ default parameters for each detection method.
% \footnote{To adopt DetectGPT, we set all hyperparameters to default except for \textit{buffer_size}. We configure \textit{buffer_size} to 2 in order to escape getting stuck with masking.}

\paragraph{Attacking methods}
Our proposed attacker is based on ChatGPT (gpt-3.5-turbo-0613).
In generating attacks, we configure $\mathsf{temperature}$ to 1.3.
We adopt two types of in-context examples for our attacker. The first type is fixed examples that are retrieved in advance to make the types of prediction labels even. The fixed examples are the same for every problem statement. The second type is tf-idf retrieved examples for each problem statement. In both settings, the number of examples is 10.
We compare our proposed attacker to the paraphrasing attack using the 11B document-level paraphraser, DIPPER, introduced by \citet{krishna23}. DIPPER allows us to control output diversity on lexical diversity and content re-ordering. To adopt DIPPER, we configure the parameter $L$ for lexical diversity of output and the parameter $O$ for content re-ordering diversity of output to 60, which is the strongest combination of the parameter in \citet{krishna23}. Other hyperparameters are set to defaults.

\begin{table*}[!]
\centering
\small
\begin{tabular}{cccccc}\hline
\multicolumn{6}{c}{\textbf{The detection on non-attacked original essays considering attacks}}\\
\multirow{2}{*}{\textbf{Essay Generator}}& \multirow{2}{*}{\textbf{Detector}} & \multicolumn{4}{c}{\textbf{Metrics (\%) ↑}}\\\cline{3-6}
& & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1}\\\hline
\multirow{4}{*}{ChatGPT}&\multicolumn{1}{l}{\cellcolor{Gray}Ours} & \cellcolor{Gray}99.0 & \cellcolor{Gray}94.0 & \cellcolor{Gray}96.5 & \cellcolor{Gray}96.4\\
&\multicolumn{1}{l}{+ DIPPER} & 99.2 & 87.8 & 93.5 & 93.1 \\
&\multicolumn{1}{l}{+ Ours (Fixed)} & 97.2 & 94.0 & 95.6 & 95.5\\
&\multicolumn{1}{l}{+ Ours (Tf-idf)} & 97.8 & 92.4 & 95.1 & 95.0\\
\hline
\multirow{4}{*}{GPT-3.5}&\multicolumn{1}{l}{\cellcolor{Gray}Ours} & \cellcolor{Gray}98.6 & \cellcolor{Gray}95.2 & \cellcolor{Gray}96.9 & \cellcolor{Gray}96.8\\
&\multicolumn{1}{l}{+ DIPPER} & 98.8 & 92.4 & 95.6 & 95.5 \\
&\multicolumn{1}{l}{+ Ours (Fixed)} & 97.6 & 95.6 & 96.6 & 96.6\\
&\multicolumn{1}{l}{+ Ours (Tf-idf)} & 97.6 & 96.2 & 96.9 & 96.9\\
\hline
\multirow{4}{*}{FLAN-T5-XXL}&\multicolumn{1}{l}{\cellcolor{Gray}Ours} & \cellcolor{Gray}98.8 & \cellcolor{Gray}68.2 & \cellcolor{Gray}83.5 & \cellcolor{Gray}80.5\\
&\multicolumn{1}{l}{+ DIPPER} & 99.2 & 72.0 & 85.6 & 83.3 \\
&\multicolumn{1}{l}{+ Ours (Fixed)} & 97.6 & 69.8 & 83.7 & 81.1\\
&\multicolumn{1}{l}{+ Ours (Tf-idf)} & 97.0 & 73.4 & 85.2 & 83.2\\
\hline
\end{tabular}
\caption{\label{side_effect}
The detection performance of our detector on non-attacked essays before and after considering the attacks: our fixed attack, our tf-idf attack, and DIPPER. The detection performance of our detector is almost consistent on non-attacked essays before and after considering attacks. Furthermore, our detector performs better after considering attacks on non-attacked essays depending on essay generation models.
}
\end{table*}

\section{Results}
\label{results}
\subsection{How robust is our detector considering the attacker's output against attacked data?}
Table \ref{robustness_of_our_detector} presents the difference in detection performance of our detector before and after considering the attack on attacked data. The attacking models include our fixed attacker, our tf-idf attacker, and DIPPER.
Through all attackers, our detector improves the detection performance on attacked data after considering the same attack.
For instance, on attacked essays by our tf-idf attacker, the detection performance of our detector improves by +21.6 point AvgRec and +41.3 point F1-score after considering our tf-idf attacks. 
From this result, we can observe that our detector can learn to detect essays from attackers via in-context examples. 
Additionally, our detector considering our attacks performs better than before considering the ones on each attacked data by all attackers.
However, the detection performance of our detector considering DIPPER improves only on the attacked data by DIPPER, not our attackers.
This suggests our attacker is not paraphrasing but can generate a lexically and semantically diverse essay from a problem statement as an attack, and our detector considering our attacks, is able to detect texts with a wider range of expressions.

\begin{table*}[t]
\centering
\small
\begin{tabular}{cccccc}\hline
\multicolumn{6}{c}{\textbf{The attacking by various attackers}}\\
\multirow{2}{*}{\textbf{Detector}}& \multirow{2}{*}{\textbf{Attacker}} & \multicolumn{4}{c}{\textbf{Metrics (\%) ↓}}\\\cline{3-6}
& & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1}\\\hline
\multirow{4}{*}{RoBERTa-base}&\multicolumn{1}{l}{\cellcolor{Gray}Non-attacked} & \cellcolor{Gray}93.8 & \cellcolor{Gray}92.2 & \cellcolor{Gray}93.0 & \cellcolor{Gray}92.9\\
&\multicolumn{1}{l}{DIPPER} & 93.8 & 89.2 & 91.5 & 91.3 \\
&\multicolumn{1}{l}{Ours (Fixed)} & 93.8 & 70.0 & 81.9 & 79.5\\
&\multicolumn{1}{l}{Ours (Tf-idf)} & 93.8 & 69.2 & 81.5 & 78.9\\
\hline
\multirow{4}{*}{RoBERTa-large}&\multicolumn{1}{l}{\cellcolor{Gray}Non-attacked} & \cellcolor{Gray}91.6 & \cellcolor{Gray}90.0 & \cellcolor{Gray}90.8 & \cellcolor{Gray}90.7\\
&\multicolumn{1}{l}{DIPPER} & 91.6 & 97.0 & 94.3 & 94.4 \\
&\multicolumn{1}{l}{Ours (Fixed)} & 91.6 & 56.4 & 74.0 & 68.4\\
&\multicolumn{1}{l}{Ours (Tf-idf)} & 91.6 & 56.2 & 73.9 & 68.3\\
\hline
\multirow{4}{*}{Ours}&\multicolumn{1}{l}{\cellcolor{Gray}Non-attacked} & \cellcolor{Gray}99.0 & \cellcolor{Gray}94.0 & \cellcolor{Gray}96.5 & \cellcolor{Gray}96.4\\
&\multicolumn{1}{l}{DIPPER} & 98.6 & 66.2 & 82.4 & 79.0 \\
&\multicolumn{1}{l}{Ours (Fixed)} & 98.6 & 25.0 & 61.8 & 39.6\\
&\multicolumn{1}{l}{Ours (Tf-idf)} & 98.8 & 24.8 & 61.8 & 39.4\\
\hline
\end{tabular}
\caption{\label{our_stronger_attacker}
The detection performance of the detectors on non-attacked essays generated by ChatGPT before and after attacking: our fixed attack, our tf-idf attack, and DIPPER. Our attacker can drastically degrade the detection performance of our detector and supervised classifiers by up to -69.2 point Machine Recall and -57.0 point F1-score. Additionally, our attacker has worse impacts on the detection performance of all detectors than DIPPER by up to -20.6 point Machine Recall and -39.6 point F1-score.
}
\end{table*}

\begin{table*}[t]
\centering
\small
\begin{tabular}{cccccc}\hline
\multicolumn{6}{c}{\textbf{Comparison with statistical approaches on LLM-generated essay detection}}\\
\multirow{2}{*}{\textbf{Essay Generator}}& \multirow{2}{*}{\textbf{Detector}} & \multicolumn{4}{c}{\textbf{Metrics (\%) ↑}}\\\cline{3-6}
& & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1}\\\hline
\multirow{6}{*}{FLAN-T5-XXL}&\multicolumn{1}{l}{$log p(x)$} & 2.0 & 97.6 & 49.8 & 66.0\\
&\multicolumn{1}{l}{Rank} & 28.6 & 86.2 & 57.4 & 66.9 \\
&\multicolumn{1}{l}{LogRank} & 12.0 & 90.6 & 51.3 & 65.0\\
&\multicolumn{1}{l}{Entropy} & 50.2 & 71.0 & 60.6 & 64.3\\
&\multicolumn{1}{l}{DetectGPT} & 29.8 & 76.2 & 53.0 & 61.9\\
&\multicolumn{1}{l}{Ours} & 98.8 & 68.2 & \textbf{83.5} & \textbf{80.5}\\
\hline
\end{tabular}
\caption{\label{comparison_statistical}
The detection performance of our detector and previous statistical outlier approaches on the essays generated by FLAN-T5-XXL. Our detector has totally superior detection performance to previous statistical approaches. 
}
\end{table*}

\begin{table*}[t]
\centering
\small
\begin{tabular}{cccccc}\hline
\multicolumn{6}{c}{\textbf{Comparison with supervised approaches on LLM-generated essay detection}}\\
\multirow{2}{*}{\textbf{Essay Generator}}& \multirow{2}{*}{\textbf{Detector}} & \multicolumn{4}{c}{\textbf{Metrics (\%) ↑}}\\\cline{3-6}
& & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1}\\\hline
\multirow{3}{*}{ChatGPT}&\multicolumn{1}{l}{RoBERTa-base} & 93.8 & 92.2 & 93.0 & 92.9\\
&\multicolumn{1}{l}{RoBERTa-large} & 91.6 & 90.0 & 90.8 & 90.7 \\
&\multicolumn{1}{l}{Ours} & 99.0 & 94.0 &\textbf{96.5} & \textbf{96.4}\\
\hline
\multirow{3}{*}{GPT-3.5}&\multicolumn{1}{l}{RoBERTa-base} & 93.8 & 92.0 & 92.9 & 92.8\\
&\multicolumn{1}{l}{RoBERTa-large} & 92.6 & 92.0 & 92.3 & 92.3 \\
&\multicolumn{1}{l}{Ours} & 98.6 & 95.2 & \textbf{96.9} & \textbf{96.8}\\
\hline
\end{tabular}
\caption{\label{comparison_supervised}
The detection performance of our detector and previous supervised approaches on the essays generated by ChatGPT (gpt-3.5-turbo-0613) and GPT-3.5 (text-davinci-003). Our detector has totally superior detection performance to previous supervised approaches. 
}
\end{table*}

\subsection{How well can our detector considering the attacks, consistently perform even on non-attacked data?}
Table \ref{side_effect} shows the difference in the detection performance of our detector before and after considering the attack on the non-attacked data.
The generation models include ChatGPT (gpt-3.5-turbo-0613), GPT-3.5 (text-davinci-003), and FLAN-T5-XXL.
Our detector considering attacks performs well consistently even in non-attacked data compared to without considering: average -0.32 point Average Recall and -0.1 point F1-score though all comparisons.
Furthermore, in non-attacked essays generated by the FLAN-T5-XXL, our detector performs better: by up to +2.1 point AvgRec and +2.8 point F1-score after considering attacks.
This result empirically shows that considering attacks has little side effects on the detection performance of our detector in the non-attacked data.

\subsection{Is our attacker stronger than the previous paraphrasing attack approach?}
Table \ref{our_stronger_attacker} provides the detection performance on attacked essays by different attacking approaches: DIPPER, our fixed attack, and our tf-idf attack.
Our attacker can drastically degrade the detection performance of our detector and supervised classifiers by up to -69.2 point Machine Recall and -57.0 point F1-score.
In addition, our attacker has worse impacts on the detection performance of all detectors than DIPPER by up to -20.6 point Machine Recall and -39.6 point F1-score.
We also find that DIPPER doesn't degrade the detection performance much and conversely improves the detection performance partially because the paraphrasing attack is not designed directly to attack detectors.

\section{Comparison with prior work}
\label{comparison}
In this section, we compare our detector to prior detectors divided into two groups: statistical outlier approaches and supervised classifiers on non-attacked essays. Here, our detector doesn't consider any attacks and takes in-context examples from ChatGPT-generated essays regardless of the essay generation models.

\paragraph{Statistical outlier approaches}
Statistical outlier approaches (as listed in \S\ref{experimental_setup}) need access to an essay generation model's logit for their detection.
Thus, we compare our detector and these statistical outlier approaches in detecting essays generated by FLAN-T5-XXL.
As shown in Table \ref{comparison_statistical}, our detector has a totally superior detection performance of 83.5 points AvgRec and 80.5 points F1-score to previous statistical approaches. 
We find that previous statistical outlier approaches tend to aggressively detect Human-written essays as LLM-generated from the low performance in HumanRec and the high performance in MachineRec.
While the HumanRec of our detector is high: 98.8 points, partially because our detector is based on ChatGPT, which is trained with human feedback, thus escaping aggressive detection.

\paragraph{Supervised classifiers}
We contrast our detector and supervised classifiers in detecting essays generated by each ChatGPT (gpt-3.5-turbo-0613) and GPT-3.5 (text-davinci-003).
Table \ref{comparison_supervised} presents that our detector has better detection performance on both essays generated by the essay generation models.
We empirically find that ChatGPT has the few-shot ability enough good to detect LLM-generated texts with labeled in-context examples.

\section{Related Work}
% 書くこと
% 1. これまで提案された検出器の種類、その特徴
% 2. アタックに対するdefenceに関する研究
% 3. 実験設定に関する話
    % non-instruction-tunedなモデルによるopen-endedに生成された文を対象にしている
In this section, we provide brief overviews in terms of LLM-generated Text Detection, Detection for assessing academic dishonesty, Attacking on LLM-generated Text Detection, and Defence against attacking LLM-generated Text Detection.

\paragraph{LLM-generated Text Detection}
\label{baselines}
Tackling the malicious uses of LLMs, such as student homework shortcuts and fake article generation, recent studies have proposed detectors to identify LLM-generated texts.
These detectors can be mainly categorized into watermarking algorithms, statistical outlier detection methods, and binary classifiers.
Watermarking algorithms use token-level secret markers in texts that humans cannot recognize for detection \citep{John23}.
In order to embed the markers into texts, the probabilities of selected tokens by a hash function are modified to be higher in text generation at each time step.
Our work focuses on not the watermark-enhanced LMs but LLMs that are openly used in our daily life, following student essay writing settings.
Statistical outlier detection methods exploit statistical differences in linguistic features between human-written and LLM-generated texts, including n-gram frequencies \citep{badaskar-etal-2008-identifying}, entropy \citep{ent08}, perplexity \citep{bere16}, token log probabilities \citep{solaiman19}, token ranks \citep{ippolito-etal-2020-automatic}, and negative curvature regions of the model’s log probability \citep{mitchell23}. These are zero-shot detectors and don't need to train a classifier.
In addition, these methods can reliably detect the texts regardless of the domains but require white-box access to the model logits.
Binary classifiers are trained to classify human-written texts and LLM-generated texts with labels. The classifiers range from classical methods \citep{ippolito-etal-2020-automatic,crothers23} to neural-based methods \citep{bakhtin19,uchendu-etal-2020-authorship,rodriguez-etal-2022-cross}. These methods are reliable for detecting texts from various LMs without the need for the white-box setting but can degrade their detection performance to texts outside of the training data.

Unlike these above methods, our proposed detection approach is a new few-shot detector, leveraging adversarially generated examples for in-context learning.

\paragraph{Detection for assessing academic dishonesty} 
ChatGPT and other LLM-based chat services have the potential to become custom tutors for each student in school and to help students with their writing according to their different linguistic skills. While it is also true that many schools recognize these services as academic dishonesty because students can misuse these tools for their homework shortcuts \citep{educatorconsiderations4chatgpt}.
Based on this situation, most recent studies work on investigating LLM-generated Text Detection for student assignments, such as argumentative essays \citep{liu2023argugpt}, university-level course problems on Cyberwarfare, Biopsychology, etc \citep{ibrahim2023perception,vasilatos2023howkgpt}.
Specifically, \citet{liu2023argugpt} targeted the essay domain as we do, focusing on essays written by non-native English learners. However, considering the human-level generative capabilities of LLMs, it can be more difficult to distinguish LLM-generated essays from native-written essays than non-native-written ones. Thus, as a more difficult setting, we create a dataset to classify native-student-written and LLM-generated essays.

\noindent
\paragraph{Attacking on LLM-generated Text Detection}
% watermarkingの話
% データベースの話
Most recent studies have reported the effectiveness of paraphrasing attacks in which existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. \citep{sadasivan23,krishna23}. 
\citet{krishna23} proposed a document-level paraphraser that takes a long-form sequence and generates a text which has almost the same meaning but is different lexically and syntactically.
However, these attacking methods are not necessarily effective because they don't consider the features of the target detector; that is, they are not designed directly to deceive the detectors.

\noindent
\paragraph{Defense against attacking LLM-generated Text Detection}
% アタックに対するディフェンスとその課題
The exposure of these above attacking methods poses a need for defending approach against attacks. There are few studies especially to defend against the attacks. \citet{krishna23} proposed a retrieval-based defense that detects a text as LLM-generated that is semantically similar to one of the LLM-generated responses in an API database, consisting of pairs of user queries and responses of an LLM. 
However, it needs active actions by API providers, and based on the nature of the method, a false positive rate can be higher with a larger database.
In addition, \citet{sadasivan23} recently shows that the retrieval-based defense method is vulnerable against recursive paraphrasing. The detection accuracy significantly drops to 25\% after 5 rounds of paraphrasing by \citet{krishna23}'s paraphraser.

\section{Conclusion}
In this paper, we propose OUTFOX, a framework that improves the robustness of the detector against attacks by allowing both the detector and the attacker to consider each other's outputs and apply this to the domain of the student essays.
From our experiments, empirical results describe that 1) Our detector can learn to detect essays from attackers via in-context examples and 2) Interestingly, considering attacks of our detector has little side effect on the detection of non-attacked data. and 3) Our attacker, which is designed directly to deceive the detector, can evade current LLM-generated text detectors more substantially than the previous paraphrasing attack.
% We believe that our OUTFOX framework can be applied to other domains, such as fake news generation and academic paper writing.
In future work, we will investigate the difference in linguistic characteristics of attacks between our attacking method and the paraphrasing attack.

% \section*{Limitations}
%% Limitation %%
% 1. 我々のOUTFOXフレームワークは、クエリとそのレスポンスのペアが人間と機械の文で一定数あることが前提で、事例に関係のないものを持ってきた（しか持ってこれない）場合、性能は一定低下する。
% 2. アタックを考慮した検出は、アタック→検出を全てchatgptに行わせるため、API使用コストが高い

% \section*{Ethics Statement}
% it's safe.

% \section*{Acknowledgements}
% aa

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\clearpage
% \appendix

% \section{The data statistics of our essay dataset}
% \label{sec:appendix_a}


\end{document}
