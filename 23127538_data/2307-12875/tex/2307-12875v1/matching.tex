To describe the efficiency of a campaign, we compute the variance and
the expectation of the response statistics, that is, the lift as in
Equation \ref{eq:lift-general}:
\begin{equation}
\label{eq:lift-general-2}
Lift = E[r(d,t) | d \in E] + E[r(d,t) | d \in C]
\end{equation} 
With the term quasi experiments, we refer our inability to choose the
exposed and the control group before the experiment as in a randomized
clinical experiment. However, we can still choose how the control
group interacts with the exposed group by an appropriate selection: in
practice, we sample the universe and we find for every device $d_i$ a
corresponding ${d'}_i$ in the control and the best we can do is to
compute the following,:
\begin{equation}
E_d\big(E(r_\gamma|d,E) - E(r_{\not\gamma}|d',C)\big)
\label{eq:2}
\end{equation}
where we represent conditional expectation as usual, $r_{\gamma}$ is
the response to exposure, and $r_{\not\gamma}$ the response to not
exposure, from the original work and notation.  We notice that the
notation infers a matching with suitable pairing for every device $d$ a
device $d'$. In the following, we shall provide an interpretation of
Equation \ref{eq:2}. Notice that Equation \ref{eq:lift-general-2}
and \ref{eq:2} are equivalent if the choice of exposure (i.e.,
$z(d_i)=1$ if $d_i$ in Exposed) is {\em strongly
ignorable}, \cite{RosenbaumR1983}. This is clear by using the original
notations:
\begin{equation}
(r|E,r|C) \orthogonal z|d  
\end{equation}
For every person in the sample $d_i$, the fact that $d_i$ is exposed
or not, is orthogonal to the value of the response.  As Dawid's
stated \cite{Dawid1979} $X \orthogonal Y$ where $X$ and $Y$ are random
variables, then
\begin{enumerate}
   \item $P(X=x,Y=y) = P(X=x)P(Y=y)$, 
   \item $P(x|y)  =P(x)$, 
   \item $P(x,y) = a(x)b(y)$, and 
   \item $P(x|y) = a(x)$.
\end{enumerate} 
The second case is the most practical for us: the distribution of the
response is independent of the distribution of the exposure
selection. In practice, we call general lift the result of
Equation \ref{eq:lift-general-2}. We do not expect targeting to be
strongly ignorable.

This is not just a mathematical tool; Equation \ref{eq:2} suggests
that exposed and control must be paired equally. This has a natural
application to the methodology described in Section \ref{sec:CMS}
where the exposed-control pair is balanced and matching will provide
an estimate of Equation \ref{eq:2} by 1-1 matching. In general, a good
control can be large, the computation of Equation \ref{eq:2}
difficult, and targeting specific and not ignorable, then we must
estimate something like the following instead:
\begin{equation}
\label{eq:lift-general-3}
E[r_\gamma(d,t) | d \in \text{ E and } M(d) \in \Omega] - 
E[r_{\not \gamma}(d,t) | d \in \text{ C and } M(d) \in \Omega ]
\end{equation} 
where $M()$ is an appropriate function and $\Omega =
[M(d):\text{E}\rightarrow \R] \cap [M(d): \text{C}\rightarrow \R]$, is the
intersection of the projection of exposed and control into the image
of $M$. This means we are changing the response statistics by
filtering, clustering, and weighting within the cluster; then we
compute expectations for the average lift. The
Equation \ref{eq:lift-general-3} provides an insight to the
Equation \ref{eq:lift-general-2} and, in general, there is no
equality.

\subsection{Matching: Previous Work}
\label{sec:previous-work}

The seminal work by Dawid \cite{Dawid1979} introduces the notation and
the formalism of {\em ignorable}, the main property to make a quasi
experiment closer to a random experiment.  

The population composing our experiments has explanation features, see
Section \ref{sec:keywords}.  The feature space is a multidimensional
space: multivariate. As the features can be used to discriminate the
exposed group from the control, we can use them to build $M()$ of
Equation \ref{eq:lift-general-3}.  The author
in \cite{Rubin1976c,Rubin1976b} introduced examples thus paving the
path for matching using propensity score. The authors in
\cite{RosenbaumR1983} developed the theory connecting the importance
of the propensity scores to matching. The propensity score transforms
a multidimensional space problem, which users have the closest set of
features, into a one-dimensional problem, which users have the closest
score number.

Assume that $\Vc{x}_i$ is a feature vector for the person $i$; we
estimate $z_i$ as $\zeta(\Vc{x}_i)$ in the following
Equation \ref{eq:linearmodel}, in turn we may use a linear model such
as in \cite{McCullaghN1989}, Chapter 4.3:
\begin{equation} 
\log\frac{\zeta(\Vc{x})}{1-\zeta(\Vc{x})} \sim \beta_0 + \sum_{i=1}^K\beta_ix_i
\label{eq:linearmodel}
\end{equation}

Rosenbaum and Rubin explain how any function based on $\zeta(\Vc{x})$
or, for that matter, its linear model in
Equation \ref{eq:linearmodel}, can be used for the computation of the
matching. We can use it in Equation
\ref{eq:2}  and we can determine $M()$ in Equation \ref{eq:lift-general-3}.  
Compute the model and determine the $\dot{\beta}$.  Take two people
from the experiment with their feature set $\Vc{x}_i$ and
$\Vc{x}_j$. If $\Vc{x}_i=\Vc{x}_j$,
$\zeta(\Vc{x}_i)=\zeta(\Vc{x}_j)$. Also, thanks to the linearity and
continuity of the estimation, $|\Vc{x}_i - \Vc{x}_j|<\epsilon $
translates to $|\zeta(\Vc{x}_i) -\zeta(\Vc{x}_j)|< \delta$ with
$\epsilon,\delta>0$ and arbitrary small, whether or not $z_i=z_j$.

The work by the authors in \cite{DehejiaW1998} is probably the most
cited attempt to provide a first evaluation of all the matching
algorithms using propensity scores. As of today, this work is a great
beginning and most of the current libraries implementing matching
provide the data from this reference (see
\cite{HansenK2006,IacusKP2012,Griffin2014,HoImaKin07,Sekhon2011,SekhonG2012}).  For a better view of the research, please consult the
references in \cite{MorganW2007,GuoF2015}.

Often the estimate of the propensity scores provides only a handful of
values, discrete, and thus classes with very large gaps. By
construction and in this scenario, any matching will be balanced and
thus it does not provide any meaningful measure of quality. Also it
means that the matching is many-to-many. Also, assuming we have to
match one exposed user with one control user, eventually we have to
sample either one. We may believe that random sampling should not
change the average behavior; however, in practice, when the events
are rare, sampling will remove them further and actually affect the
final results. This simple consideration explains why it is difficult
computing Equation \ref{eq:2} and also undermines balanced matching
based on $\zeta(\Vc{x})$, which is unrelated to the response.

In this work, we present a single algorithm that will work with both
propensity score and clustering algorithm. Also we show the danger of
perfect matching while estimating the exposed effect in
Equation \ref{eq:lift-general-3}. To overcome this problem, we
introduce a novel quality measure of the matching algorithm and an
un-balanced algorithm where the response distribution is not known a
priori and thus it is not affected by matching.


\subsection{The algorithms}
\label{sec:algorithm}
The propensity score describes a multidimensional vector by a
probability or a score with a continuity property stemming from
Equation \ref{eq:linearmodel}. The propensity model for $N$ users with
$K$ features is computed by an iterative algorithm with $J$ iterations
computing a $QR$-factorization in each step: $O(JKN^2)$. For us $K=25$
and $J<10$. Thus $O(N^2)$ is a good approximation of the
complexity but it is often executed in native and fast code.

The other property is that the scores can be sorted and a caliper can
be introduced naturally using the strong order of the scores. Sorting
$N$ scores takes $O(N\log N)$ and thus creating the matches with any
caliper takes an extra $O(N)$, a further pass. Optimal algorithms will
circumvent calipers and try to find the closest element independently.
This takes $O(\frac{N^2}{2})$ comparisons; for example, this is the
default implementation in \cite{Sekhon2011}. For large $N$ a full
comparison is not efficient \footnote{Even though the complexity
$O(N^2)$ is the same as the QR-factorization, this is not highly
optimized, the constant factor if much larger than 250 and it is often
not acceptable for $N>10^6$. } and sorting is by far the better
solution.

Complexity is one problem.  The other problem is sampling due to
matching: when exposed and control have different number of users and
we match one-exposed-user-to-one-control-user, we sample the larger
one. If the events we measure in the response have a distribution with
long (fat) tails such as in stable
distributions \cite{Mandelbrot:1960:PLL}, sampling will censor rare
but important events. In general, we do not want to use the response
for matching, to avoid bias; however, the change in the response
distribution can be used as a measure of quality: for example, we can
provide distance measure and its confidence using several
methods \cite{DAlbertoD2009}. Here, we use a simple, consistent, and
intuitive measure: entropy \cite{Shannon1948}. Small entropy changes
for exposed and control before and after matching mean a
representative matching. To achieve this entropy balance, we may opt
for unbalanced matching and get away from one-to-one matching. We
shall explain how we use weights and how they affect the computation
of Equation \ref{eq:lift-general-3}

\subsection{The Idea}
\label{sec:idea}

The propensity score defines clusters; that is, users with the same
score are in the same cluster. If we sort these scores, then we
sort/organize the clusters. By construction, consecutive scores in the
sorted list represent close clusters thanks to the continuity property
of the propensity score.

In practice, we score each user in the corpora composed of both
exposed and control. We sort the corpora by the score; then we apply
the following algorithm:
%\newpage
{ \small
\begin{verbatim} 
 1: i = 1
 2: scoreR = Corpora.scores[i]
 3: i = i +1  
 4: while (i<length(Corpora)) {
 5:  score = Corpora.scores[i]
 6:  if (scoreR != score) { # New Cluster
 7:    TE = length(tmpE) # Exposed 
 8:    TC = length(tmpC) # Control
10:    if (TE > 1 && TC>1)  { # Non Empty cluster 
11:      N = max( TE, TC)
12:      if (BALANCED) # or further random sample 
13:        U = union(tmpE[1:N],tmpC[1:N]) 
14:      else 
15:        U = union(tmpE,tmpC)
16:      matches = union(matches, U)
17:    } 
18:    tmpE =c(); te = 0
19:    tmpC = c(); tc = 0
20:    scoreR = score;
21:  } else  {
22:    if (Corpora.exposed[i]==1)    tmpE[te++] = i 
23:    else                          tmpC[tc++] = i 
24:  } 
25:  i = i+1
26: }
27: Corpora = Corpora[matches] # Matching done
\end{verbatim}
}

The pseudo code above matches the exposed and control by clusters.  We
can choose to have a balanced matching or unbalanced. Notice that
there is no matching in between clusters: exposed or control coming
from different clusters will have different scores. It is actually
easy to encompass this problem by moving the assignments at lines 18
and 19 inside the loop and at the end of the condition at line 10 (we
call this Caliper activation and it makes sense only for
propensity-score algorithms).

In practice, we could use any other means to cluster the users. For
example, if we use any $k$-mean clustering algorithms, the scores are
the cluster labels and the algorithm above will not change.  The
matching is actually decoupled by the scores: propensity score and
$k$-mean algorithms can be applied in combination. For example, we use
propensity score first with adaptive calipers to estimate the number
of clusters and, then optionally, we apply $k$-mean algorithm.

In practice, we compute the propensity score by generalized linear
model (GLM) \cite{McCullaghN1989}  and the $k$-mean by
\cite{Forgy1965,Hartigan1979}. Both methodologies are well known and
available. From our side, we would like to expose the matching
algorithm as a computational kernel and thus apply it to large
problems.

\subsection{Quality Measure and Confidence}
\label{sec:quality}

We are aware of several matching algorithms: {\em exact, cem,
  subclass, nearest, genetic, full, and optimal}. The list is
  longer. Our implementation falls among the first four.  This
  scenario begs for a simple question: how can we compare the results
  of matching. The package {\em MatchIt}, which offers an interface to
  all of the above algorithms, takes a step back and lets the user
  decide the statistics about the matched sets, even the computation
  of the lift is avoided.

Often the quality of matching is based on the estimate of the variance
of some sort.

\mypar{Sampling a normal distributed corpora} Each user has a response
$r_i$ (i.e., $\Delta_M r(d_j,e_i)$ in Equation \ref{eq:expectation}),
with $N$ users, we can measure the average
\[ \mu_N =
\frac{1}{N}\sum_{i=0}^{N-1}r_i
\]
 and the variance
\begin{equation} 
\sigma_N^2 = \frac{1}{N-1}\sum_{i=0}^{N-1}(r_i-\mu_N)^2
\label{eq:variance}
\end{equation}
If we sample, $M_s<N$, we can compute the average and variance and we
can compute $\sigma_M$. If we sample $M_s$ randomly, then
$\frac{\sigma_M}{\sigma_N} \sim \sqrt{\frac{M}{N}}$: in particular the
ratio $\frac{\sigma_M}{\sigma_N}$ should be distributed in the
vicinity of $\sqrt{\frac{M}{N}}$ as a normal standard distribution. We
can express the confidence that the sampling is in accordance with the
corpora by computing the probability
\begin{equation}
  2\Big(1 - \Phi(\frac{\sigma_M}{\sigma_N}-\sqrt{\frac{M}{N}})\Big).
\label{eq:sample}
\end{equation}
That is, the probability to be close to the expected variance, the
higher the better.

\mypar{Variance decrease} In principle, matching should decrease the
variance $\sigma_M < \sigma_N$; because we reduce $M<N$ and because we
remove outliers; if they do appear in both exposed and control, thus
they will not be outliers. We could choose the matching that reduces
the variance the most.  Unfortunately, the classic way to perform
matching will sample either exposed or control, not both; thus if the
variance is computed as in Equation \ref{eq:variance}, then this
criterion does not apply.

\mypar{ATT Response as normal distribution} One-to-one matching takes
one exposed $r^i_e$ and finds a control $r^j_c$ .  We can see that
\begin{equation}
E[r|E] - E[r|C] \sim \frac{1}{K}\sum_{i=0}^{K-1}r^i_e - r^i_c
\end{equation}
Thus we can create a distribution of the ATT response by $r^i_e -
r^i_c$. In general, the computation $r^i_e - r^i_c$ is actually $r^i_e
- w_ir^i_c$ where $w_i$ is a weight associated with the multiplicity
of the matched control and distance from exposed. For us, $w_i =1$
represents perfect match.  As such, we can compute average $\mu_E$ and
variance $\sigma_E$. If the distribution above is normal we can use
the ratio of average and variance to describe how well the matching
represents the final result: $2(1 - \Phi(|\mu_E|)/\sigma_E)$. 

In our cases, while the two moments specify completely an ideal
normal distribution, they do not give justice to the empirical
distribution. In fact, the normal distribution is {\em fatter} close
to the average and {\em thinner} at the tails.

\mypar{ATT Response as Laplace distribution} The Laplace distribution
provides a different approximation, sharper close to the average and
still symmetric.

\Doublefigure{0.5}{EachDistribution}{ClusterDistribution}{Each
  element and Cluster contribution of the response
  distribution}{fig:distributions}

If we build a cluster, each cluster has its own response: we can
estimate the distribution of the response by the contribution of each
pair or by cluster. While using cluster responses, the distribution
may not be approximated by a Normal distribution nor by Laplace, see
Figure \ref{fig:distributions}.

\mypar{Skewness} Normal and Laplace distributions are poor
approximations of the tails of the empirical distributions. The
computation of the skewness describes with a single measure whether
these assumptions are appropriate and also whether or not we can trust
the confidence levels that we measure using the Normal or Laplace
distribution assumption. By construction, if the exposure is
effective, the distribution will be skewed and thus the normality
assumption will fail.

\mypar{$\chi^2_k$ distributions of users' response} Each cluster has a
collection of users and thus responses. The responses are correlated
in the clusters, by construction (or assumption). They should be
independent otherwise, across clusters. If the cluster has $k$
components, we can take the variance of each and their addition should
be the realization of the $\chi^2_k$ with $k$ degrees of freedom. Then we can
use the difference and the known distribution of the $\chi^2_k$ to
compute a confidence level, see Figure \ref{fig:chisquare}.

\singlefigure{0.5}{Chisquare}{Approximation by
  $\chi^2_k$}{fig:chisquare}

The main goal of these quality metrics and their description as
confidence level is to provide a measure of how well the clustering
and matching work. Basically, these metrics use two (or three) moments
of the response (average and variance) and the assurance of quality is
based on variations from an average, which comes from a known
distribution. Unfortunately, the response distribution seems to have a
mixed distribution and thus two moments cannot capture
this nature.

\mypar{Exposed and Control Entropy Constancy} The clustering/matching
should be independent of the response, just to avoid systematic bias.
One way to compare the quality of the matching is by measuring the
information contained in the original corpora and in the matched one.
Actually, we expect to have different distributions for exposed and
control.

We should and can compare the response distributions before and after
matching; in \cite{DAlbertoD2009}, we provide not only distance but
also confidence. Here, we simplify the problem and compute and compare
{\em entropy}. Take the exposed distinct response or the original
corpora
\begin{equation}
{\cal E}_e^N = -\sum_{\ell=0}P[r_e^\ell]\log_2(P[r_e^\ell])
\label{eq:entropy}
\end{equation}
${\cal E}_e^N$ is the expected information of the exposed
response. We can compute ${\cal E}_c^N$, which is the entropy of
control. We then can compare the entropy after the matching. Simply
put, we compute the pair:
\begin{equation}
({\cal E}_e^M{-}{\cal E}_e^N,{\cal E}_c^M{-}{\cal E}_c^N)
\label{eq:entropy2} 
\end{equation}
if the pair is positive, it means that the matching increased the
entropy and thus it censored responses with less
information. Otherwise, the matching is sampling responses with more
information. Among the matching algorithms, we should choose the one with the
smallest entropy difference for both exposed and control. In fact,
we are after the computation of Equation \ref{eq:2} where the
distribution affects the averages.

\mypar{Boostrap}  
We can consider the lift as a statistics and as such we can estimate a
variance using bootstrap. If the response $r(d,e)$ has $lift = E[r]$
and variance $\sigma^2_r = E[(r-lift)^2]$, then by bootstrapping the
lift as average $\hat{lift} = \frac{1}{N}\sum_i r_i$ has an average,
say $\mu_{lift}$, and $\sigma_\mu$.

Larger is the number of devices used in the experiment, smaller will
be the $\sigma_\mu$. This is because the average approximates the
expectation asymptotically 
\[\lim_{N\rightarrow \infty} (E[r]
- \frac{1}{N}\sum_i r_i )\rightarrow 0.\] As such, $\sigma_\mu$ and
$\sigma_r$ are not related: the response can have stable distribution
with unbounded variance $\sigma_r$ (i.e., have undefined variance for
$\alpha < 2$) and the $\sigma_\mu$ still will converge to zero because
the distribution has a finite expectation.

If we are prepared to run enough iterations, and considering that
$\sigma_\mu$ must be bound, we can use the equation $2(1 - \Phi(|lift
- \hat{lift}|)/\sigma_\mu)$ to achieve a p-value and thus a confidence
level.  Bootstrapping is also used to determine if the $lift$ of this
sample is a good approximation of the larger
population \cite{RePEc:idb:spdwps:1005}. For matching without
replacement, 1-1 matching, this is a welcome
approach \cite{Austin2014} and suggested to fill in the missing
information due to small experiments. For matching with replacement,
bootstrapping must be used carefully \cite{abadie:imbens:2008}. For
matching with replacement and large sample it may be not needed
because the distribution is known \cite{NBERw15301,AbadieImbens2006};
the application of bootstrap for continuous distribution of the
features and continuous response is an open question.

\mypar{Campaign as sample} Any matching algorithm samples the experiment to
remove the original bias, unfortunately the lift after matching does
not generalize to the original experiment because, in advertising,
targeting is not strongly ignorable and because the original
experiment is not a sample (it contains all exposed) and we are not
trying to estimate the lift for the hypothetical exposed.

%\newpage
\section{Experiments}
\label{sec:experiments}

In this section, we shall present our experimental results.  One way
to be able to use the methods above for large corpora is to sample
randomly. We show the hidden danger of this expedient
Section \ref{sec:sampling}. We then provide examples where all methods
provide information and thus none cannot be excluded a priori,
Section \ref{sec:comparison}. We show how the matching algorithms
presented here are comparable to the current available
Section \ref{sec:feature-algorithms}. We conclude in
Section \ref{sec:alg-performance} where we compare the average lift
computations for balanced approach of Section \ref{sec:CMS} and the
more general and unbalanced approach, showing how large experiments
can be matched.

%\subsubsection{Ideal Experiment}
%\label{sec:truth}

\subsection{The danger of Sampling}
\label{sec:sampling} 
\begin{table}[hbt]
   \tbl{The different lifts by random sampling 200,000 users out of
      310,000\label{tb:sampling}}{%     
      \small
      \begin{tabular}{|r|r|r|r|r|r|r|r|r|r|r|}
      
        \hline
        iteration&sort&sort-time&k-means&k-time&subclass& sub-time&exact&e-time& cem& c-time \\ 
        \hline \hline     
        1& -122 & -145 & -132 & -150 & -124  &  -88 & -100 & -132 & -100 & -132  \\
        2&  326 &  538 &  346 &  594 &  297 &  357 &  400 &  308 &  400 &  308 \\
        3& -101 & -673 &    0 &   26 & -163 &  166 &  185 & -209 &  185 & -209 \\
        4&   -3 &   14 &    3 &   -3 &   23 &   52 &   52 &   -3 &   52 &   -3 \\
        5&  -42 &  123 &  -10 &  -22 & -154 & -104 &  -70 &  -97 &  -70 &  -97 \\
        6& -118 &  -52 & -133 &  -59 & -156 & -109 &  -82 & -113 &  -82 & -113 \\
        7& -110 &  454 & -157 &  587 & -187 & -167 & -141 &  -43 & -141 &  -43 \\
        8&  -37 &  112 &  -47 &  166 & -134 & -113 & -111 &   38 & -111 &   38 \\
        9& 1637 &  941 & 1257 & 3795 &-1660 &-1019 &-1119 & -969 &-1119 & -969 \\
        10&  -63 &   -4 &  -79 &  -48 &  -84 &  -60 &  -88 &  -45 &  -88 &  -45 \\ \hline
        all &     -96 &  -91 &  -100 &  -50 &  -131 &  -71 &  -60 &  -88 &  -60 & -88 \\ \hline
        \hline
      \end{tabular}
    }
\end{table}

When we started our research, we applied known methods such as Match()
\cite{Sekhon2011}. To  cope with the long execution time for large
experiments, we sampled the users randomly but keeping the ratio. Our
experiments may have rare responses, fat tails; for features
represented in binary format, we have large classes creating
many-to-many matching; we noticed that sampling can bring forth
inconsistent results. For example, we took an experiment using a
balanced control-exposed set and discrete classes, the experiment had
about 310,000 users and we sampled it to 200,000. We ran 10 different
sampled matching and computed the lift ($(E[r|E]-E[r|C])/E[r|C]$) as
relative measure.  Also, we added features with time information
(i.e., two extra dimensions) to help the propensity score matching. We
summarize the results in Table \ref{tb:sampling}. Considering that in
Section \ref{sec:CMS} we suggest to sample Control, thus the response,
the table results should be a warning for large and small campaigns.

What is confusing with this experiment?  First, adding features does
not help provide consistent measures; what is lost during sampling is
lost and further space investigations seem helpless. Second, each run
provides quite different lifts in absolute values and in signs
(opposite).  In three out of ten iterations all matching results show
only negative lifts, one shows only positive lifts, and for six we
have mixed results. The experiment is not robust, but as we can see
the original experiment produce consistent, although negative,
results. In our scenario, the control group has much more signal as
the exposed group, thus sampling the experiment or sampling control
must be done with care and not randomly.


\subsection{Match comparisons}
\label{sec:comparison}

Let us take the example used in Section \ref{sec:sampling}. We do not
perform sampling and also we set to compute an unbalanced
matching. The number of users is exactly 302862. For each algorithm,
we use 25 and 27 features (whether or not we use time information). If
we use 25 features, the algorithm is fully specified by its name
(i.e., sort, k-means, subclass, exact and cem), if we use 27 features
we use longer names: sort-time/s-time, k-means-time/k-time,
subclass-time/s-time, exact-time/e-time and cem-time/c-time (features
are discrete again).

Match(), full and optimal from MatchIt() results are not reported
because they will take more than 3 hrs, which is not acceptable for
our purpose. The execution time to model the propensity score and to
score the user is about 8sec.  Also further tests show that their
performance increases linearly with the number of users in the
matching tests (considering the number of features fixed).

In Table \ref{tb:entropy}, we show the performance of the same tests,
 the entropy difference (see Equation
\ref{eq:entropy2}), and the standard deviation of the users'
responses. The corpora has variance of 0.0980. Notice that our methods
tend to decrease the entropy but in absolute value, this is the minimum.

\begin{table}[hbt]
    \tbl{Unbalanced computation: difference in entropy and variance\label{tb:entropy}}{
    \centering \small
    \begin{tabular}{|l|l|l|l|}
      \hline
      algorithm& exposed & control & variance \\ \hline \hline
      sort           & {\bf -0.00016} & {\bf -5.91886e-05} & 0.0978\\
      sort-time      & -0.00056     &-0.00061     & {\bf 0.0959}\\
      k-means        & -6.45606e-05 &-6.06477e-05 & 0.0979\\
      k-means-time   &  0.00281     &-0.00028     & 0.0970\\
      subclass       &  0           & 0.00404     & 0.0975 \\
      subclass-time  &  0           & 0.00600     & 0.0965\\
      exact          &  4.52999e-05 & 0.00808     & 0.1009\\
      exact-time     &  0.00252     & 0.01500     & 0.0978\\
      cem            &  4.52999e-05 & 0.00808     & 0.1009\\ 
      cem-time       &  0.00252     & 0.01500     & 0.0978\\ 
      \hline \hline     
    \end{tabular}
}
\end{table}


If we use the moments and a few assumptions about the users' responses,
we can compute the probability such that the empirical distribution
is indistinguishable from the assumed distribution, see Table
\ref{tb:confidence}; then, all matching algorithms accept the equality
assumption. Sort and k-mean use the clusters: by using the Normality
assumption the matching will be accepted, using the Laplace assumption
the matching will be rejected. Other methods use single user response
and thus due to the number of users (300,000 users) they converge to
normality and the others follow.
\begin{table}[hbt]
  \tbl{Unbalanced computation: confidence in the matching process\label{tb:confidence}}{
  \centering \small
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    Algorithm    &Normal & Laplace & $\chi^2_k$ & Eq. \ref{eq:sample}  \\ \hline \hline  		  
    sort              &0.99911 &0.95689 &0.94941 &0.99943 \\
    sort-time         &0.99798 &0.72556 &0.99178 &0.98933 \\
    kmeans            &0.99824 &0.67636 &0.97187 &0.99940 \\
    kmeans-time       &0.99749 &0.65775 &0.98537 &0.98061 \\
    subclass          &0.99867 &1       &0.99825 &0.99581 \\
    subclass-time     &0.99927 &1       &0.99926 &0.98781 \\
    exact             &0.99941 &1       &0.99958 &0.97563 \\
    exact-time        &0.99913 &1       &0.99910 &0.97086 \\
    cem               &0.99941 &1       &0.99846 &0.97563 \\
    cem-time          &0.99913 &1       &0.99936 &0.97086 \\
    \hline \hline     
  \end{tabular}
}
\end{table}

\subsection{Audience Selection, Features, and Algorithms (Cor)Relation}
\label{sec:feature-algorithms}
\label{sec:features}

In our experience, our problem space has three basic dimensions: First
is the choice of the exposed group and thus control; Second is the
dimension number and quality of the feature space describing our
audience; Third is the set of algorithms and what they can expose for
all the above. Eventually, we would like to infer recommendations
about what works, especially at scale. In this section, the largest
campaign has ten million devices and the smallest a few hundred
thousands.

In this section, we consider a few dozen campaigns and we applied a
balanced approach, Section \ref{sec:CMS}, with discrete feature space
Section \ref{sec:user-profile} using only registration data, known as
prior. In this scenario, the user response is discrete covering a set
of discrete values. The feature space specifies a discrete space,
although possibly large, it is limited and users could be clustered
into a few thousands classes. The estimate of the targeting function
reflects the discrete space nature; thus, the matches are often
many-to-many and the exposed group has priority, that is, we sample
control. For all the matching algorithms, there is always a
many-to-many matching because the calipers will infer classes and
within any class we do not apply a {\em nearest} matching. Even for
the standard algorithms, they apply a 1-to-many matching introducing
weights.


\doublefigure{0.80}{u1}{b1}{Audience keywords: Unbalanced $k$-to-$m$ matching (above) and Balanced many-to-many matching (below))}{fig:keywords2}

In Figure \ref{fig:keywords2}, we compare our algorithms together with
more standard ones. We can appreciate that the final lift values
differ little. This simple experiment shows that our matching
algorithms are equivalent to others with the advantage that can be
applied to larger campaigns without loss of accuracy. We make sure
that the matching as we designed and developed does not loose
information for the type of our experiments. In practice, we show
experimentally that our matching are a useful contribution to the
literature especially as meaningful extension, if not the only
available extension. In the following section, we go even bigger. To
do so, we need a different framework and we need to use an unbalanced
approach.


\subsection{Experiments and analysis of stochastic hits} 
\label{sec:brownian}                       

Setting a specific radius or the contour of a parcel as a boundary for
the computation of a hit is simple to explain and to use. However, how
we can account for those impressions close by those boundaries by
users we do not see inside those same boundaries. They may have
stepped out of the location we are interested in and sent us a
signal. In this section, we discuss the application of what we called
stochastic hit, if an impression is close enough to a location we may
consider to give it a probability (of a hit) by using a known
distribution such as the IG presented in Equation \ref{eq:ig} or a
{\em lognormal} distribution.

We consider two campaigns A and B, they have more than thirty
locations of interests relatively sparse geographically. One campaign
is to advertise a car company and the other is a restaurant chain. 

We create these two experiments for both campaigns: we consider $R=30$
and $R=96$ meters, $R$ is the radius we use to consider an impression
a hit. For each experiment, we consider all the users that have a hit
at distance $d<=R$. Then we count the number of times and the
distances $0\leq d \leq 10R$ with a precision of one meter. 

\Doublefigure{0.5}{global21015m100}{global21015m300}{Campaign A: Conditional distance distribution with users who has a visit within 30 and 100 meters}{fig:global21015}

In Figure \ref{fig:global21015}, we present the two experiments when
we observe the distribution of who hit ($d<R$ for $R=30$ and $R=100$
meter). Within $0\leq d \leq R$, we have investigated a simple
polynomial curve fit and we have found that $p[x=d]\sim \alpha
d$. This simple observation means that $P[x\leq R] \sim R^2$ and,
thus, the number of hits is proportional to the area. This is not true
for each location, but for the aggregate of 30 or more locations, we
achieve such a nice property, which is intuitive.

\Doublefigure{0.5}{global20663m100}{global20663m300}{Campaign B: Conditional distance distribution with users who has a visit within 30 and 100 meters}{fig:global20663}

In Figure \ref{fig:global20663}, we present the second campaign and we
can appreciate that the experiment have similar results. The slope is
different, specific to the campaign and locations set,
$p[x=d]\sim \beta d$. In principle, we can fit multiple distribution
models: we fit the inverse Gaussian (IG) and a log-normal. We plot the
correspondent distribution for the models. 

Our goal is to estimate the probability that an impression with
distance $d>R$ could be a probability of a hit. In practice, we could
use $IG(\mu, \lambda)$ to estimate the probability for every
impression in the range $[R, R+\frac{R}{2}]$, in
Figure \ref{fig:global21015} and \ref{fig:global20663} we represent
this space by the first two vertical gray lines (from the far
left). And we could use the $LogNormal(\mu,\sigma)$ for the interval
$(R+\frac{R}{2}, 3R]$. 
 


\subsection{Large scale  lift comparison}
\label{sec:alg-performance} 

To the best of our knowledge, the system we present in
Section \ref{sec:CMS} and show results in
Section \ref{sec:feature-algorithms} is the only one capable to tackle
an experiment with 2 million users in any practical way.  In this
section, we present a different prospective in order to create and to
measure even larger experiments and show how sampling is still a
lingering issue, although for different reasons.


Let us introduce the {\bf impression space}: We formally introduced
the concept of impression and we used to specify visits as in
Equation \ref{eq:visit}. Where do these impressions come from? We are
listening to a fire-hose of streaming impressions that we can bid
through a collection of exchanges. Our budgets dictates how much we
can listen and it changes. Here, we call this fire-hose real time bid
(RTB) exchanges, the volume of RTB is a function of an allocation
budget and it will change as a function of company wide budget,
hardware allocation and hardware/software failure. The RTB is composed
of three non intersecting parts:
\begin{itemize}

\item Listening RTB (LRTB) is a random sample of the fire-hose used only for collection purpose, let us say that LRTB is 10\% of the RTB. 

\item Won RTB (ND),  we bid and we win the impression, thus we deliver our advert. The volume of impressions here is a function of campaign budget and pacing. 

\item Unwanted RTB (URTB) is the remaining impressions, the larger portion of RTB.   

\end{itemize}

Using all URTB, LRTB, and ND (i.e., urtb-lrtb-nd) impressions and our
unbalanced approach, we will not sample impressions, visits, nor
users. This is the ultimate representation of our experiment space. As
such, it puts quite a few practical and economical constraints in the
experiment measure. For example, a national campaign like Starbucks
counting 10 thousands locations and three months period, will touch
approximately 100 million devices and (hundred of) billions
impressions. This experiments will have maximum number of users and
visits.

Historically and thus in Section \ref{sec:CMS}, the balanced approach
uses the URTB and ND impressions, it intersects the exposed users to
those impressions and it samples the control, (i.e., c-urtb-nd).  For
national campaigns as above, the sheer size of impressions to manage
can be quite large. As a practical effect, we sample control, we
sample visitors, and thus we sample visits.

Assume we embrace a different sampling: sampling of impression in
time. The LRTB has the property of being an unbiased and random sample
of RTB, thus the space LRTB-ND (i.e., lrtb-nd) may have all the
information we need, a critical size to compare users and visits, and
a practical size to have the experiment measures in a more economical
way.

A long tail distribution is applicable here. There are a lot of users
with few impressions, there are a lot of users with zero visits. The
c-urtb-nd samples mostly control, even though we use the terminology of
balanced approach, we can appreciate the irony of the name, if or when
we are control. The lrtb-nd approach will sample the impressions, we
will pick users with enough impressions, but we do that without any
bias to the targeting. Of course, a critical mass has to be met by
both, otherwise the sampling curse will be visible at this level as
well. Now, we can introduce the final experiment.


%%% WE need to introduce the story ....
We took twelve campaigns of various sizes. For each we measure lift
without any matching, we use the term of general lift. These
experiments have different goals. In Figure \ref{fig:general-1}, we
show the results. 

\singlefigure{0.90}{SparkVsCMSGeneralpancake16}{General Lift using different methodology and sampling}{fig:general-1} 
To provide a clear presentation of the small differences, we opted for
a non standard box plot. For any lift in the range $[-1,1]$, we
present it as it is. For any lift in the range $[1,\infty)$, we
represent it as $1+\log_{10}(lift)$. We work out similarly the
negative lift smaller than $-1$. Thus, we use a logarithmic
scale but only for large enough lifts.

We can appreciate that the general lift for all campaigns is negative
(on an average). There is only one exceptions where c-urtb-nd is not
correlated to urtb-lrtb-nd (i.e., campaign $20592$), and only one
exception for rtb-nd (i.e., campaign $20601$). This means that
sampling can be an issues even at this scale but it is moderate. Why
is it always negative? The main reason is targeting: targeting is far
from being ignorable and the control group is not really
comparable. We need to apply matching and we show the lift comparison
in Figure \ref{fig:general-2}.

\singlefigure{0.90}{SparkVsCMSmatchedpancake16}{Effect of Matching}{fig:general-2}

There is even a stronger correlation on the sign of the lift while
applying matching: only campaign $20483$ is the exception. In general,
sampling increases the variance of the lifts (a little obscured by the
log scale) and the absolute value of the lift; that is, sampling
reduce the population of the experiments, making the visit per user
effect larger. 

The larger the experiment is, then the smaller the lift is.  This is
an important and practical consideration. Lift is basically a
comparison measure with respect to a control that it can be much
larger than the exposed group and thus the control visits can be much
larger. Eventually, we are bound to measure lift as number of relative
visits with respect to all visits, if all visits increases the lift
will decrease. This is the course of relative measures: A larger
campaign may have a larger effect (more visits) than a smaller one,
but relatively to the population it reached, the larger campaign will
have smaller lift. 

 


