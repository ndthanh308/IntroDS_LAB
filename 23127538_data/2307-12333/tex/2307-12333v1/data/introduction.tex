Deep neural networks (DNN) have achieved success in   tasks such as image classification~\cite{simonyan2015deep}, speech recognition~\cite{5740583}, video analysis~\cite{5995719}, and action recognition~\cite{10.1007/978-3-319-46484-8_2}. Among these networks, residual networks (ResNets) are important architectures, making it practical to train ultra-deep DNN, and has ability to avoid gradient vanishing~\cite{He_2016_CVPR,he2016identity}. Also, the idea of ResNets has motivated the development of many other DNNs including WideResNet~\cite{zagoruyko2016wide}, ResNeXt ~\cite{xie2017aggregated}, and DenseNet ~\cite{huang2017densely}.

In recent years, understanding the ResNets from the dynamical perspective has become a promising approach~\cite{weinan2017proposal,haber2018learning}. More specifically, assume $\vx_0\in \mathbb{R}^d$ as the input of ResNet~\cite{He_2016_CVPR} and define $\mathcal F$ to be a mapping, then the $l$-th residual block can be realized by 
\begin{equation}
\label{eq:resnet}
\vx_{l+1}=\vx_l+\mathcal F(\vx_l,\vw_l)
\end{equation}
where $\vx_l$ and $\vx_{l+1}$ are the input and output tensors of the residual mapping, and $\vw_l$ are parameters of $l$-th layer that are learned by minimizing the training error. Define $\vx_L$ as the output of a ResNet with $L$ layers, then the classification score is determined by $\vy = \mathrm{softmax}(\vw_{\text{fc}}\vx_L)$, where $\vw_{\text{fc}}$ are also learnable parameters of the final linear layer.

% \revise{We can follow the routine~\cite{huang2006large} and separate the network as a feature extractor $k: \mathbb{R}^d \rightarrow \mathbb{R}^d$, which is the composition of residual blocks, and a logistic regression classifier $f: \mathbb{R}^d \rightarrow \mathbb{R}^C$, where $C$ is the number of classes in the classification problem.:
% \begin{align}
%     k&: \vx_0 \mapsto \vx_L \label{eq:extractor}\\
%     f&: \vx_L \mapsto \mathrm{softmax}(\vw^{\rm{FC}}\vx_L) \label{eq:classifier}
% \end{align}
% Then the final prediction score is the composition of two functions $\vy = f \circ k(\vx_0)$.
% }
%  and defining $t_l=l\Delta t, \vx_l =\vx(t_l) $
For any $T>0$, introducing a temporal partition $\Delta t = T/L$, the ResNet represented by~\cref{eq:resnet} is the explicit Euler discretization with time step $\Delta t$ of the following differential equation:
\begin{equation}
\label{eq:ode}
    \frac{{\rm d}\vx(t)}{{\rm d}t}=v(\vx(t),t), \quad \vx(0) = \vx_0 \quad t\in[0,T],
\end{equation}
where $v(\vx(t),t)$ is a velocity field such that $\Delta t v(\vx(t),t)=F(\vx(t),\vw(t))$. The above ordinary differential equation (ODE) interpretation of ResNet provides new perspective and has inspired many networks. As shown in~\cite{lu2018beyond}, by applying stable and different numerical methods for~\cref{eq:ode}, it leads to PolyNet~\cite{zhang2017polynet} and FractalNet~\cite{larsson2016fractalnet}. 
Besides, the other direction is to consider the the continuous form of~\cref{eq:ode}, representing the velocity $v(\vx,t)$ by a deep neural network. One typical method is the Neural ODE~\cite{chen2018neural} in which $v$ is updated by the adjoint state method. Similar extensions along this direction include neural stochastic differential equation (SDE)~\cite{jha2021smoother} and neural jump SDE~\cite{jia2019neural}. The theoretical property of these continuous models has been analyzed in ~\cite{thorpe2018deep,avelin2021neural,zhang2019approximation}.

% \revise{Other than ODE}, the partial differential equation (PDE) formulation of ResNet is also a promising direction,

The connection between ODE and partial differential equation (PDE) through the well-known characteristics method has motivated the analysis of ResNet from PDE perspective, including theoretical analysis~\cite{sonoda2019transport}, novel training algorithms~\cite{sun2018stochastic} and improvement of adversarial robustness~\cite{wang2020enresnet} for DNNs. To be specific, from the PDE theory, the ODE~\cref{eq:ode} is the characteristic curve of the transport equation:
\begin{equation}
\label{eq:pde}
    \frac{\partial u}{\partial t}(\vx,t)= - v(\vx,t)\nabla u(\vx,t),~ (\vx,t)\in\mathbb R^{d}\times[0,T].
\end{equation}

% Figure environment removed

The method of characteristics tells us that, along the curve $(\vx,t)$ defined by~\cref{eq:ode}, the function value $u(\vx,t)$ remains unchanged. Assume at $t=T$, $u(\vx,T) =f(\vx) \coloneqq \mathrm{softmax}(\vw_{\text{fc}}\vx)$ is the linear classifier, then
\[u(\vx(0),0)=u(\vx(T),T)=f(\vx(T))=f\circ k(\vx(0)) \]
where $k$ represents the mapping from $\vx(0)$ to $\vx(T)$, which is the continuous form of feature extraction in ResNet. Thus at $t=0$, $u(\cdot,0)$ is the composition of a feature extractor and a classifier, which is analogous to ResNet. Nonetheless, since the transport equation~\cref{eq:pde} is reversible in time, and initial value problem is more common than terminal value problem in PDE, we assume $u(\vx,0) =f(\vx)$ in our paper. Consequently, the direction of solving ODE~\cref{eq:ode} needs to be reversed, but its connection to ResNet remains consistent. In one word, the transport equation~\cref{eq:pde} can describe the evolution from a linear classifier to ResNet.

% Moreover, we change the time step to $t_l=(L-l)\Delta t$, hence $\vx_L=\vx(t_L)=\vx(0),\vx_0=\vx(t_0)=\vx(T)$. Recall the definition of feature extractor $k$ in~(\cref{eq:extractor}),

% we have
% \begin{align*}
%     u(\vx&(T),T)=u(\vx(0),0)=f(\vx(0))\\
%     &=f(\vx_L)=f \circ k(\vx_0)=f \circ k(\vx(T))
% \end{align*}
% Thus at $t=T$, $u(\cdot,T)$ is analogous to ResNet, which maps the training data to the corresponding prediction score. In one word, the transport equation can describe the evolution from a linear regression classifier to ResNet.

% \begin{remark}
% Different from previous work which also connects ResNet to the transport equation~\cite{li2017deep,wang2018deep}, we reverse the direction of solving ODE~(\cref{eq:ode}) and PDE~(\cref{eq:pde}). The reason is that previous work treats the PDE formulation as a control problem, in which logistic regression classifier is the terminal condition. Training ResNet is equivalent to tuning the velocity field $v(\vx(t),t)$. In this paper, we are more interested in the evolution from a simple classifier to a complicated neural network, and thus assuming logistic regression classifier as the initial condition is more reasonable.
% \end{remark}

% for any input~$\vx_0$, $u(\vx_0,T)$, the solution of (\cref{eq:pde}) at terminal time $T$, is the prediction of $\vx_0$ by a ResNet.

Suppose we fix the initial condition $u(\cdot,0)$ as the linear classifier $f$. DNN can be seen as a map between two functions: $u(\cdot,0)$ and $u(\cdot, T)$. In ResNets, this map is formulated as a convection equation. A natural question is: 
\textit{Is convection equation is the only PDE to formulate this map? If not, can we derive a general form of PDE to formulate the map?}
%\textit{if we replace the specific transport equation with a more generalized PDE, what will the final state be like?} 
In this paper, we try to answer above questions from mathematical point of view.
First we construct a continuous flow $\mathcal T_t$ which maps a simple linear classifier to a more complicate function, as illustrated in~\cref{fig:illustration},
\begin{equation}
    \label{eq:operator_definition}
    \mathcal T_t: f \mapsto u(\cdot,t), \quad t\in[0,T],
\end{equation}

% Hopefully, we wish the final state to be a DNN, then what is the relationship between the form of PDE and the resulting DNN?
% $\mathcal T_t$ is a map between two function spaces. $\Tilde{k}_t$ is an imitator of the feature extractor $k$ defined in~(\cref{eq:extractor}), but not necessarily be a residual flow, so we add a tilde. We write the image of $\mathcal T_t$ as a composition of classifier $f$ and extractor $\Tilde{k}_t$ to ensure the resulting function simulates DNN.

% The operator $\mathcal T_t$ alone does not carry much information, so we have little knowledge in choosing the PDE type for designing the continuous flow. So far, all we have is the relationship between transport equation and ResNet, i.e. if $u(\vx,t)\coloneqq\mathcal T_t f(\vx)$ satisfies the transport equation~(\cref{eq:pde}), then $\Tilde{k}_T=k$ is the residual flow.
The idea in this paper is also classical in mathematics. First, we extract some \textit{basic properties} $\mathcal{T}_t$ should satisfies. Then based on these basis properties, a general form of $\mathcal{T}_t$ can be derived rigorously. 
%Previous analysis show that, if the sequence of operator images $u(\vx,t)=\mathcal T_t f(\vx)$ satisfies the transport equation, the continuous flow will produce ResNet. Obviously, we may test other kinds of PDE and examine the resulting network structure. However, as the number of PDEs is infinite, we prefer not to cherry pick those with good performance. Rather, we want to extract some basic assumptions we should impose on $\mathcal T_t$, such that the flow not only covers transport equation and ResNet, but also provides insight into new PDEs and DNNs. The idea is classic in mathematics, e.g. from the integer group to group theory by extracting basic properties such as associativity, identity element and inverse element. We believe this approach is more instructive and efficient than traversing numerous PDEs.
% (3) does not become too generalized. (3) is worth noting because most systematic PDE theory are based on second or lower order PDEs.
More specifically, inspired by the scale space theory, we prove that under several reasonable assumptions on $\mathcal T_t$, $u(\vx,t)=\mathcal T_t f(\vx)$ is the solution of a second order convection-diffusion equation. This theoretical result provides a unified framework which covers transport equation and some existing works including Gaussian noise injection~\cite{wang2020enresnet,liu2020does}, dropout techniques~\cite{sun2018stochastic,srivastava2014dropout} and randomized smoothing~\cite{cohen2019certified,li2019certified,salman2019provably}. It also illuminates new thinking for designing networks. In summary, we list the main contributions as follows.

% By using the operators $\{\mathcal T_t\}_{0\leq t\leq T}$ defined in (\cref{eq:operator_definition}), we prove that the solution operator of a convection-diffusion equation is sufficient for generalizing ResNet under some assumptions.
\begin{itemize}
    \item We establish several basic assumptions on operator $\mathcal T_t$, and prove the sufficiency of these assumptions for generalizing ResNet and beyond. To the best of our knowledge, this is the first theoretical attempt for establishing a sufficient condition for designing the variants of ResNet from the PDE perspective, which may provide some insights when considering the search space in neural architecture search (NAS). 
    \item Inspired by our theoretical analysis, we propose an isotropic model by adding isotropic diffusion to~\cref{eq:pde}. Compared to the linear classifier $f$, we prove that the proposed model has lower Rademacher complexity and larger region with certified robustness. Moreover, we design a training method by applying the operator splitting scheme for solving the proposed convection-diffusion equation.
    %\item We test our model on both synthetic dataset and CIFAR-10/SVHN/Fashion-MNIST datasets, which validate our theorectical results.
    % Inspired by our theoretical findings, we provide a training method for ResNet. Our experimental results show that the proposed method achieves better performance than baseline models on both the synthetic dataset and the CIFAR-10/CIFAR-100 datasets, which verify our theoretical results.
\end{itemize}

% Although the above connections between ResNets and differential equations have demonstrated numerical success in many tasks, choosing the PDE type for designing the network architecture is heuristic and lacks study. Thus, the goal of this paper is answer this question from the continuous perspective under reasonable assumptions. Define the base classifier $f_{\rm base}(\vx) = {\rm softmax}(\vw^{FC}\vx)$ and $k$ be the feature extractor that is the composition of the residual blocks given by (\cref{eq:resnet}), ResNet can be represented by $f_{\rm base}\circ k$. For any $t\in[0,T]$, define the map~$\mathcal T_t$ between two function spaces as 

% where $u(\vx,t)$ is the solution of the transport equation~(\cref{eq:pde}), the previous analysis shows that $f_{\rm base}\circ k = \mathcal T_T(f_{\rm base})$, that is ResNet is the image of the operator $\mathcal T_T$. Thus, it suffice to know the range property of the operators   $\{\mathcal T_t\}_{0\leq t\leq T}$. In general, it is impossible to fully describe the properties of $\mathcal T_t, t\in[0,T]$. In this paper, inspired by the scale space theory, we prove that under several reasonable assumptions, $\{\mathcal T_t\}_{0\leq t\leq T}$ is the solution of a convection-diffusion equation. This theoretical result provides a unified framework to designing networks which are given by the solution operator of convection-diffusion equations.

% \subsection{Organization}
% This paper is organized in the following way: In subsection \cref{section:expression}, we derive a convection-diffusion model for ResNets. In subsection \cref{several models}, we introduce several existing models which can be interpreted by our model. Then we theoretically analyze the robustness and generalizability of isotropic model (a special case of our model defined in subsection \cref{several models}) in subsection \cref{section:robustness} and \cref{section:generalization} respectively. In section \cref{practical algorithm}, we present a training method for ResNet based on our model and we
% test the performance of our training method on both Half-moon and CIFAR-10/CIFAR-100 datasets in section \cref{section:experiment}, . Our paper ends up with some concluding remarks.

\paragraph{Notations}
We denote scalars, vectors, and matrices by lowercase and uppercase letters, where vectors and matrices are bolded.  We denote the $\ell_2$   and  $\ell_\infty$ norms of the vector $\vx = (x_1, \cdots, x_d)\in \mathbb{R}^d$ by $\|\vx\|_2 = {(\sum_{i=1}^d |x_i|^2)^{1/2}}$ and $\|\vx\|_\infty = \max_{i=1}^d|x_i|$, respectively. We denote the gradient and Laplace operators by $\nabla=(\frac{\partial}{\partial x_1},\cdots,\frac{\partial}{\partial x_d})$ and $\Delta=\sum^d_{i=1}\frac{\partial^2}{\partial x^2_i}$, respectively.
For a function $f:\mathbb R^d\rightarrow\mathbb R$,   $D^{\alpha}f$   denotes its $\alpha$-order derivative,  $\|f(\vx)\|_{L^{\infty}}=\sup_{\vx\in\mathbb R^d}|f(\vx)|$ its $L^{\infty}$ norm.
$\mathcal{N}(\mathbf{0}, \sigma^2\Ib)$ denotes   Gaussian noise with mean $\mathbf{0}$ and variance $\sigma^2$. $C^{\infty}_b$ is the space of bounded functions which have bounded derivatives at any order.

% and $\|f(\vx)\|_{C^1}=\|f(\vx)\|_{L^{\infty}}+\|\nabla f(\vx)\|_{L^{\infty}}$   its $C^1$ norm