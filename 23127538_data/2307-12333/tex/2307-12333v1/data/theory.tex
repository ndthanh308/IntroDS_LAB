%Next, we focus on the case of isotropic equation (namely, $\sigma(\vx,t)$ is $\sigma^2 \Ib$)
%To mitigate this issue, based on our PDEs framework and the knowledge that the terminal value of convection-diffusion equation (whose diffusion term is isotropic, namely $\sigma(\vx,t)$ is $\sigma^2 \Ib$) is more regular than initial value, we use the DNN $f_{\vw}(\vx)$ as the base classifier and use the solution operator of the diffusion equation 
In this section, under our convection-diffusion model, we focus on isotropic-type equation.
%\begin{equation}
%\label{eq:isotropic}
%	\begin{cases}
%	\frac{\partial u(\vx,t)}{\partial t}=v(\vx,t)\cdot \nabla u(\vx,t)+\sigma^2\Delta u, \quad \vx \in \mathbb R^{d},t\in [0,T]\\
%	u(\vx,0)=f(\vx).
%	\end{cases}
%\end{equation}
%as the evolution operator $\mathcal T_t$ to smooth the DNN for improving the robustness and generalizability. In next subsection, we will illustrate the benefits of using the evolution operator $\mathcal T_t$ from a theoretical perspective.
Furthermore, for the sake of simplicity and practical computation, we consider the model with split convection and diffusion in time.
\begin{equation}
    \label{eq:split}
    \begin{cases}\frac{\partial u(\boldsymbol{x},t)}{\partial t}= v(\boldsymbol{x},t) \cdot \nabla u(\boldsymbol{x},t), &\quad \vx \in \mathbb R^{d}, \quad  t\in [0,T-1]\\
    \frac{\partial u(\boldsymbol{x},t)}{\partial t} = \sigma^2 \Delta u, &\quad \vx \in \mathbb R^{d},\quad t\in[T-1,T]\\
    u(\boldsymbol{x},0) = f(\boldsymbol{x})
    \end{cases}
\end{equation}
In the following subsections, we will theoretically illustrate its benefits for improving the robustness and reducing the Rademacher complexity of neural networks. 

\subsection{Robustness guarantee}
Assume the data point lies in a bounded domain $\mathcal D$. Consider a multi-class classification problem from $\mathcal D\subset\mathbb R^d$ to label class $\mathcal Y=\{1,\cdots,k\}$. Let $G$ be a prediction function defined by $G(\vx)=\arg\max_{i\in\mathcal Y}u^i(\vx,T)$, where $u^i(\vx,T)$ is the $i$-th element of $u(\vx,T)$. Suppose that DNN classifies $\vx$ the most probable class $c_A$ is returned with probability $p_{A}=u^{c_A}(\vx,T)$,
and the “runner-up” class is returned with probability $p_{B}$. Our main result of this subsection is to estimate the area around the data point $\vx$ in which the prediction $G(\vx)$ is robust,
% \begin{theorem}
%     \label{thm:stability}
%     Suppose the velocity $v(\vx,t)$ is Lipschitz continuous on $\mathbb R^d \times [0,T])$. Let $u(\vx,T)$ be the solution of \Cref{eq:isotropic}. Suppose $c_A\in\mathcal Y$ and 
%     \[u^{c_A}(\vx,T)=p_{A}\geq p_{B}=\max_{i\neq c_A}u^i(\vx,T),\]
%     then $G(\vx+\bm{\delta})=c_A$ for all $\|\bm{\delta}\|_2\leq R$, where
%     \[R=\frac{p_A-p_B}{2(\sqrt{\gamma/\sigma^2} + \|\vw_{\mathrm{fc}}\|_F)}\]
%     $\gamma$ is a constant depending only on $\nabla v$.
% \end{theorem}

\begin{theorem}
\label{thm:stability}
    Suppose the velocity $v(\vx,t)$ is a continuous function $\mathbb R^d \times [0,T]$ which satisfies the Lipschitz condition, i.e. there exists a given constant $L>0$, such that
    \[\|v(\vx_1,t)-v(\vx_2,t)\|\leq L\|\vx_1-\vx_2\|, \quad \forall (\vx_1,t), (\vx_2,t) \in \mathbb R^d \times [0,T]\]
    Let $u(\vx,T)$ be the bounded solution of \Cref{eq:split}. Suppose $c_A\in\mathcal Y$ and 
    \[u^{c_A}(\vx,T)=p_{A}\geq p_{B}=\max_{i\neq c_A}u^i(\vx,T),\]
    then $G(\vx+\bm{\delta})=c_A$ for all $\|\bm{\delta}\|_2\leq R$, where
    \[R=\frac{\sigma}{\sqrt{2d}}(p_A-p_B)\]
\end{theorem}

\begin{remark}
We assume $v$ as Lipschitz continuous because $v(\boldsymbol{x}, t)=\frac{1}{\Delta t} F(\boldsymbol{x}, \boldsymbol{w})$ corresponds to a residual block, so its gradient can be bounded easily using the network parameters $\boldsymbol{w}$.
\end{remark}

We provide the proof of \Cref{thm:stability} in \Cref{sec:proof_stability}. According to \Cref{thm:stability}, we can obtain the certified radius $R$ is large when the diffusion coefficient $\sigma^2$ and the probability of the top class is high. We will also consider the generalization ability of model \Cref{eq:split} in the next subsection.

\subsection{Rademacher complexity}

For simplicity, we consider binary classification problems. Assume that data points are drawn from the underlying distribution $\mathcal{D}$. The training set $S_{N} = \{\vx_i\}_{i=1}^N$ is composed of $N$ samples drawn i.i.d. from $\mathcal{D}$. Rademacher complexity is one of the classic measures of generalization error. We first recap on the definition of empirical Rademacher complexity.

% {$X \times Y \subset \mathcal D \times \{-1, +1\}$} with $X$ and $Y$ being the input data and label spaces, respectively.
% Let {$\mathcal{H}\subset V^{X}$} be the hypothesis class of the DNN model, where $V$ is another space that might be different from $Y$. Let $\ell:V\times Y\rightarrow [0,B]$ be the loss function and $B$ is a positive constant. Denote function class $\ell_{\mathcal H}:=\{(\vx,y)\rightarrow \ell(h(\vx),y):h\in\mathcal H\}$. 

\begin{definition}[\cite{ledoux2013probability}]
Let $\mathcal H: X\rightarrow \RR$ be the space of real-valued functions on the space $X$. For a given sample set $S_N =\{\vx_i\}_{i=1}^N$, the empirical Rademacher complexity of $\mathcal H$ is defined as
\[R_{S_N}(\mathcal H):= \frac{1}{N}\mathbb E_{\sigma}[\sup_{h\in \mathcal H}\sum_{i=1}^N\sigma_ih(\vx_i)]\]
where $\sigma_1,\sigma_2, \cdots, \sigma_N$ are i.i.d. Rademacher random variables with $\mathbb{P}(\sigma_i=1)=\mathbb{P}(\sigma_i=-1)=\frac{1}{2}$.
\end{definition}
    
Rademacher complexity is a tool to bound the generalization error. The smaller the generalization gap is, the less overfitting the model is. We are interested in the empirical Rademacher complexity of the following function class:
\[\mathcal{G_{\sigma}} := \Big\{g(\vx) = \mathcal T_T(f)|f\in\mathcal F\Big\}\]
where
\[\mathcal{F}:=\left\{f: \vx \mapsto \phi(\vw_{\text{fc}}\vx) | \left\|\vw_{\text{fc}}\right\|_1\leq W \right\}\]

Here $\mathcal T_t$ is the solution operator of \cref{eq:split}, $\phi$ is sigmoid activation function. The function class $\mathcal F$ represents the hypothesis linear classifiers function class, where we assume the $\ell^1$ norm of the weight of fully-connected layer is bounded by $W$. The function class $\mathcal G_{\sigma}$ includes the evolved residual neural networks from base classifiers. We can also assume the data points are bounded by $R$, which is reasonable, e.g. in CIFAR-10 dataset~\cite{krizhevsky2009learning}, the pixel values lie in $[0,1]$. Then we have the following theorem.

% \begin{equation*}
% \begin{aligned}
% \mathcal F:&=\Big\{f(\vx)=\phi(\vw^{\rm T}\vx^{M})|\vx^{i+1}=\rho\left(\Ub^{i}\vx^{i}\right),\\
% &\mbox{where}\ \vx^0=\mbox{input data},\ \vw\in\mathbb R^{d}, \ \|\vw\|_2\leq w_2 \\
% & \mbox{and},\ \|\Ub^{i}\|_F\leq u_F,\ \Ub^{i}\in\mathbb R^{d\times d}\ \mbox{for}\  i=0,\cdots,M\Big\}.
% \end{aligned}
% \end{equation*}
% and 

% Here $\mathcal T_t$ is the solution operator of Equation (\ref{diffusion}), $M$ is the number of layers of the base classifier, $\phi$ is a $L_1$-Lipschitz and positive output function with $\|\phi\|_{\infty}\leq 1$ (e.g., Sigmoid) and $\rho$ is a $L_2$-Lipschitz with $\rho(0)=0$ and monotonically increasing activation function (e.g., ReLU). The function class $\mathcal F$ represents the base classifiers, and the function class $\mathcal G_{\sigma}$ represents the evolution classifiers of the base classifiers. Then we have the following theorem.

\begin{theorem}
\label{thm:rademacher}
Given a data set $S_N = \{\vx_i\}_{i=1}^N$. Suppose the data points $\|\vx_i\|_\infty\leq R$ for $\vx_i\in S_N$, then
\[R_{S_N}(\mathcal{G_{\sigma}})\leq R_{S_N}(\mathcal{F}) \leq \inf _\epsilon \left(\sqrt{\frac{2d \log (3WR/\epsilon)}{N}}+\epsilon\right)\]
\end{theorem}

We provide the proof of \Cref{thm:rademacher} in \Cref{sec:proof_rademacher}. According to \Cref{thm:rademacher}, we can obtain that the empirical Rademacher complexity of $\mathcal G_{\sigma}$ can be upper bounded by that of a simple linear classifier. This can help bounding the generalization error of the evolved neural network. In the next section, we will present a training method for ResNet to verify our results.

% \begin{theorem}\label{rademacher}
% Given a data set $S_N = \{(\vx_i, y_i)\}_{i=1}^N$ and let data bound $R$. When $N$ is large enough, with probability at least $1-1/(2N)$,
% \begin{align*}
%     R_{S_N}(\mathcal{F})&\leq Cd\sqrt{\frac{M+1}{N}}\left(\ln (Na)\right)^{1/2}\\
%     R_{S_N}(\mathcal{G_{\sigma}})&\leq Cd\sqrt{\frac{M+1}{N}}\left(\ln (Na)-\sigma^2 +\gamma\right)^{1/2}.
% \end{align*}
% Here $a=(M+1)C_{\max}RL_1w_2(L_2u_F)^M$, $C$ and $C_{\max}$ are two positive constants only depending on dimension $d$.
% \end{theorem}
% We provide the proof of Theorem \ref{rademacher} in Appendix \ref{proof3}. According to Theorem \ref{rademacher}, we can obtain that for function class $\mathcal G_{\sigma}$, the upper bound of Rademacher complexity is low when the diffusion coefficient $\sigma^2$ is high. We next present a training method for ResNet to verify our results.

