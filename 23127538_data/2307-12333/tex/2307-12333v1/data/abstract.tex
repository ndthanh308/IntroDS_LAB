%Partial differential equations (PDEs) are an important tool for studying residual networks (ResNets). Inspired by the PDE theory, recent works construct the improved versions of ResNets. However, choosing proper model among infinite number of PDEs remains a problem when applying PDE theory in neural networks. To address this problem, we look at ResNets as a flow from a base linear classifier to final output. Under some reasonable assumptions, we prove that this flow satisfies a convection-diffusion equation. Moreover, compared to base classifier, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Using the induced convection-diffusion equation, we design a new training method for ResNets. Experiments validate the performance of the proposed method.
Inspired by the relation between deep neural network (DNN) and partial differential equations (PDEs), we study the general form of the PDE models of deep neural networks.
%Many DNN architectures can be modeled by PDEs and have been proposed in the literatures. In this paper, we investigate the ``opposite'' direction and attempt to derive the general form of PDEs which can correspond to a residual neural networks(ResNets). 
To achieve this goal, we formulate DNN as an evolution operator from a simple base model. Based on several reasonable assumptions, we prove that the evolution operator is actually determined by convection-diffusion equation. This convection-diffusion equation model gives mathematical explanation for several effective networks. Moreover, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Based on the convection-diffusion equation, we design a new training method for ResNets. Experiments validate the performance of the proposed method.
%by incorporating convection-diffusion equation in  We also design a training method motivated by PDEs theory to train DNN models for better robustness and less chance of overfitting. Theoretically, we prove that the robustness of DNN trained with our method is certifiable and our training method reduces the generalization gap for DNN. Furthermore, we demonstrate that DNN trained with our method can achieve better generalizability and be more resistant to adversarial perturbations than the baseline model.