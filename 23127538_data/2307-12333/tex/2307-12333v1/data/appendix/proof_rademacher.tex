\section{\texorpdfstring{Proof of \Cref{thm:rademacher}}{Proof of Theorem 3.4}}
\label{sec:proof_rademacher}
% Let {$\mathcal{H}\subset V^{X}$} be the hypothesis class of the DNN model, where $V$ is another space that might be different from $Y$. Denote the ball centered at $\bm{0}$ with radius $R$ by $B(\bm{0},R)$. Assume $p(\vx)$ is the underlying distribution of $X$, which is defined in $\mathcal D\subset B(\bm{0},R)$. Let $\ell:V\times Y\rightarrow [0,B]$ be the loss function and $B$ is a positive constant. Denote function class $\ell_{\mathcal H}:=\{(\vx,y)\rightarrow \ell(h(\vx),y):h\in\mathcal H\}$. The goal of the learning problem is to find $h\in \mathcal H$ such that \textbf{population risk} $E(h):=\mathbb E_{(\vx,y)}[\ell(h(\vx),y)]$ is minimized.
% We consider the supervised learning setting where one has access to $n$ i.i.d. training examples $S_N$. A learning algorithm maps
% the $N$ training examples to a hypothesis $h\in\mathcal H$. In this section, we are interested in the gap between the \textbf{empirical risk} $E_N(h) := \frac{1}{N}\sum^N_{i=1}\ell(h(\vx_i), y_i)$ and the population risk $E(h)$, known as the generalization error. We start with the following theorem which connects the population and empirical risks via Rademacher complexity.

Assume that data points are drawn from the underlying distribution $\mathcal{D}$ with probability measure $\mu(\vx)$. The training set $S_{N} = \{\vx_i\}_{i=1}^N$ is composed of $N$ samples drawn i.i.d. from $\mathcal{D}$.  Let $\mathcal{H}$ be the hypothesis function space of the DNN model.

% We begin by introducing a widely used theorem that bounds the difference between empirical and actual mean with the \textbf{covering number} of a function class.

% First of all, we want to emphasize that (empirical) Rademacher complexity is an important tool to bound the generalization error. Here $L\left(h\right)=\underset{x \sim \mathcal{D}}{\mathbb{E}}\left[1_{h(x) \neq c(x)}\right]$ is the generalization error, $L_{S_{N}}\left(h\right)=\frac{1}{N} \sum_{i=1}^N 1_{h\left(x_i\right) \neq c\left(x_i\right)}$ is the empirical error.

% \begin{theorem}(Theorem 3.5 in \cite{mohri2018foundations})
%     Let $S_{N} = \{\vx_i\}_{i=1}^N$ be samples chosen i.i.d. from distribution $\mathcal{D}$. If the loss function $\ell$ is bounded by $B>0$. Then for any $\delta\in (0, 1)$, with probability at least $1-\delta$, the following holds for all $h\in \mathcal H$,
%     \begin{equation*}
%         L\left(h\right) \leq L_{S_{N}}\left(h\right) + 2R_{S_{N}}\left(\mathcal H\right)+3B\sqrt{\frac{\log\frac{2}{\delta}}{2N}}.
%     \end{equation*}
%         % In addition,  according to the Ledoux-Talagrand contraction inequality \citep{maurer2016vector} and assume loss function is $L$-Lipschitz, we have
%         % \begin{equation*}
%         %     R_{S_{N}}\left(\ell_{\mathcal H}\right)\leq LR_{S_{N}}\left(\mathcal H\right).
%         % \end{equation*}
% \end{theorem} 

% \begin{theorem}\label{thm:rademacher-empirical-actual}(Theorem 2.3 in \cite{mendelson2003few})
% Let $\mathcal H$ be a class of functions from $\mathcal D$ to $[-1,1]$ and set $\mu$ to be a probability measure on $\mathcal D$. Let $\{\bx_i\}_{i=1}^N$ be independent
% random variables distributed according to $\mu$. For every $\epsilon>0$ and any $N\ge 8/\epsilon^2$, we have
%   \begin{eqnarray}
%     \mathbb{P}\left(\sup_{f\in \mathcal H}|\frac{1}{n}\sum_{i=1}^N f(\bx_i)-\mathbb E_{\mu}f|>\epsilon\right)
%     \le 8\mathbb{E}_\mu\left[\mathbb N(\frac{\epsilon}{8},\mathcal H,L_1(\mu_N))\right]\exp(-\frac{N\epsilon^2}{128})
%   \end{eqnarray}
%   where $\mu_N$ is the empirical measure supported on the sample set $\{\bx_i\}_{i=1}^N$
% \end{theorem}

In the following proof, $\mathbb N(\epsilon, \mathcal H, L_p (\mu_N))$ denotes the covering number of function class $\mathcal H$ at scale $\epsilon$ with respect to the $L_p(\mu_N)$ norm. Here $L_p(\mu_N)$ is the metric endowed by samples: for $1 \le p <\infty$, $\|f\|_{L_p (\mu_N )} =\left(\frac{1}{N}\sum_{i=1}^N  |f (\vx_i )|^p\right)^{1/p}$, and $\|f\|_{L_\infty (\mu_N )} = \max_{1\le i\le N} |f (\vx_i )|$. Notice its difference from general $L_p$ norm. Then the covering number $\mathbb N(\epsilon, \mathcal H, d)$ is defined as the minimal number of open balls with radius $\epsilon$ (with respect to metric $d$) needed to cover $\mathcal{H}$. In other words, it is the minimal cardinality of the set $\{h_1 , \cdots, h_m\}\subset \mathcal{H}$ with the property that for every $h \in \mathcal{H}$, there exists some $h_j\in \{h_1 , \cdots, h_m\}$ such that $d(h, h_j) < \epsilon$. The set $\{h_1, \cdots, h_m\}$ is called an $\epsilon$-cover for $\mathcal{H}$. Notice that for every empirical measure $\mu_N$ and every $1 \leq p \leq \infty,\|f\|_{L_1\left(\mu_N\right)} \leq\|f\|_{L_p\left(\mu_N\right)} \leq$ $\|f\|_{L_{\infty}\left(\mu_N\right)}$. Hence,
\[\mathbb N\left(\varepsilon, \mathcal H, L_1\left(\mu_N\right)\right) \leq \mathbb N\left(\varepsilon, \mathcal H , L_p\left(\mu_N\right)\right) \leq \mathbb N\left(\varepsilon, \mathcal H, L_{\infty}\left(\mu_N\right)\right)\]

% To define the covering number of a function class, we should first define the covering number of a subset in Euclidean space. The covering number of $Y\subset \mathbb{R}^N$, denoted as $\mathbb N(\epsilon, Y, d)$, is defined as the minimal number of open balls with radius $\epsilon$ (with respect to metric $d$) needed to cover $Y$. In other words, it is the minimal cardinality of the set $\{y_1 , \cdots, y_m\}\subset Y$ with the property that for every $y \in Y$, there exists some $y_i$ such that $d(y, y_i ) < \epsilon$. The set $\{y_1, \cdots, y_m\}$ is called an $\epsilon$-cover for $Y$. Then, the covering number $\mathbb N(\epsilon, \mathcal H, d(\mu_N))$ of function class $\mathcal{H}$ is defined as the covering number for the image under realization $S_N=\{\bx_i\}_{i=1}^N$, i.e.
% \[\mathbb N (\epsilon, \mathcal H, d(\mu_N))=\mathbb N(\epsilon, \mathcal H|_{S_N}, d) \]
% where $\mathcal H|_{S_N}=\left\{\left(f\left(x_1\right), f\left(x_2\right), \ldots, f\left(x_N\right)\right): f \in \mathcal{H}\right\}\subset \mathbb{R}^N$. 

% The logarithm of the covering numbers is called the entropy of the set. For every sample $\{\vx_1 , \cdots, \vx_n \}$ let $\mu_N$ be the empirical measure supported on that sample. For $1 \le p <\infty$ and a
% function $f$, $\|f\|_{L_p (\mu_N )} =\left(\frac{1}{n}\sum_{i=1}^n  |f (\vx_i )|^p\right)^{1/p}$
% and $\|f\|_\infty = \max_{1\le i\le n} |f (\vx_i )|$.

% \begin{align*}
%     L_1(\mu_N)\le L_\infty(\mu_N)\le L_\infty.
% \end{align*}
% \ref{thm:entropy}.
% \begin{corollary}\label{cor:entropy-0}
% Let $\mathcal H$ be a class of functions from $\mathcal D$ to $[-1,1]$ and set $p(\bx)$ to be a probability measure on $\mathcal D$.
% Let $\{\bx_i\}_{i=1}^N$ be independent
% random variables distributed according to $p(\bx)$. For every $\epsilon>0$ and any $n\ge 8/\epsilon^2$,
%   \begin{eqnarray}
%     \mathbb{P}\left(\sup_{f\in \mathcal H}|\frac{1}{N}\sum_{i=1}^n f(\bx_i)-\int_{\mathcal D} f(\bx)p(\bx)\mathd \bx|>\epsilon\right)
% \le 8 \mathbb N(\epsilon/8,\mathcal H,L_\infty)\exp(-N\epsilon^2/128)
%   \end{eqnarray}
% where $\mathbb N(\epsilon , \mathcal H, L_\infty)$ be the covering numbers of $\mathcal H$ at scale $\epsilon$ with respect to the $L_\infty$ norm
% \end{corollary}
% Then, we get an upper bound of $\sup_{f\in \mathcal H}|\frac{1}{N}\sum_{i=1}^N f(\bx_i)-\int_{\mathcal D} f(\bx)p(\bx)\mathd \bx|$.
% \begin{corollary}\label{cor:entropy}
% Let $\mathcal H$ be a class of functions from $\mathcal D$ to $[-1,1]$. Let $\{\bx_i\}_{i=1}^N$ be independent
% random variables distributed according to $p$, where $p$ is the probability distribution whose $C^1$ norm is bounded. Then
% with probability at least $1-\delta$,
%   \begin{eqnarray}
%     \sup_{f\in \mathcal H}|p(f)-p_N(f)|\le  \sqrt{\frac{128}{N}\left(\ln \mathbb N(\sqrt{\frac{2}{N}},\mathcal H,L_\infty)
% +\ln \frac{8}{\delta}\right)},\nonumber
%   \end{eqnarray}
% where
% \begin{align}
%   \label{eq:note-mc}
%   p(f)=\int_{\mathcal D} f(\bx)p(\bx)\mathd \bx,\quad p_N(f)=\frac{1}{N}\sum_{i=1}^N f(\bx_i).
% \end{align}
% \end{corollary}

% \begin{proof}
%   Using Corollary \ref{cor:entropy-0}, with probability at least $1-\delta$,
% \begin{eqnarray}
%     \sup_{f\in \mathcal H}|p(f)-p_N(f)|\le  \epsilon_\delta,\nonumber
%   \end{eqnarray}
% where $\epsilon_\delta$ is determined by
% \begin{eqnarray}
%   \epsilon_\delta=\sqrt{\frac{128}{N}\left(\ln \mathbb N(\epsilon_\delta/8,\mathcal H,L_\infty)+\ln \frac{8}{\delta}\right)}.\nonumber
% \end{eqnarray}
% Obviously,
% \begin{align*}
%   \epsilon_\delta\ge \sqrt{\frac{128}{N}}=8\sqrt{\frac{2}{N}}
% \end{align*}
% which gives that
% \begin{align*}
%   \mathbb N(\epsilon_\delta/8,\mathcal H,L_\infty)\le \mathbb N(\sqrt{\frac{2}{N}},\mathcal H,L_\infty)
% \end{align*}
% Then, we have
% \begin{eqnarray}
%   \epsilon_\delta \le \sqrt{\frac{128}{N}\left(\ln \mathbb N(\sqrt{\frac{2}{N}},\mathcal H,L_\infty)
% +\ln \frac{8}{\delta}\right)}\nonumber
% \end{eqnarray}
% which proves the corollary.
% \end{proof}
% If the entropy bound of $\mathcal H$ is known, the upper bound of $\sup_{f\in \mathcal H}|p(f)-p_N(f)|$ follows from Corollary \ref{cor:entropy}.

Now, the key point is bounding the covering number of a given function class $\mathcal H$. Specifically, we are interested in two function classes $\mathcal F$ and $\mathcal G_{\sigma}$ defined as
% \begin{equation*}
% \begin{aligned}
% \mathcal F:&=\Big\{f(\vx)=\phi(\vw^{\rm T}\vx^{M})|\vx^{i+1}=\rho\left(\Ub^{i}\vx^{i}\right),\mbox{where}\ \vx^0=\mbox{input data},\ \vw\in\mathbb R^{d}, \\
% & \mbox{and},\ \|\Ub^{i}\|_F\leq u_F,\ \Ub^{i}\in\mathbb R^{d\times d}\ \mbox{for}\  i=0,\cdots,M-1,\ \|\vw\|_2\leq w_2 \Big\}.
% \end{aligned}
% \end{equation*}
\[\mathcal{F}:=\left\{f: \vx \mapsto \phi(\vw_{\text{fc}}\vx) | \left\|\vw_{\text{fc}}\right\|_1\leq W \right\}\]
and
\[\mathcal{G}_{\sigma} := \left\{g: \vx \mapsto \mathcal{T}_T(f(\vx)| f\in\mathcal F\right\}\]
%Let us start from the function class $\mathcal F$. To apply above corollary, we need to normalize $\mathcal F$ to make it lie in $[-1,1]$. Here we also use $\mathcal F$ to denote the normalized function class and absorb the bound of $\mathcal F$ into the generic constant $C$. 
where the image of operator $\mathcal T_t$ is the solution of the convection-diffusion equation \cref{eq:split}. Recall the proof of robustness in \cref{sec:proof_stability}. According to the conservation of $L^\infty$ norm of transport equation and the maximum principle of diffusion equation, we have
\[ \|\mathcal T_T f(\vx)\|_{L^\infty}= \|u(\vx,T)\|_{L^\infty} \leq \|u(\vx,T-1)\|_{L^\infty}= \|f(\vx)\|_{L^\infty}\]
% which implies for any $f_1,f_2\in\mathcal F$, we can find $g_1,g_2 \in\mathcal G_{\sigma}$ such that
% $$\|g_1-g_2\|_{\infty}=\|\mathcal T_t\big{|}_{t=1}(f_1-f_2)\|_{\infty}\leq e^{-\sigma^2}e^{\gamma}\|f_1-f_2\|_{\infty},$$
% where $\gamma$ is a constant depending on $\nabla v$.
If we assume $\{f_i\}^{N}_{i=1}$ is an $\epsilon$-cover of function class $\mathcal F$, which means $\{f_i\}^{N}_{i=1}\subset\mathcal F$ and for any $f\in\mathcal F$ there exist $f_k\in\{f_i\}^{N}_{i=1}$ such that
\[\|f-f_k\|_{L^\infty} < \epsilon\]
For any $g\in\mathcal G_{\sigma}$, write $g=\mathcal{T}_T(f)$, then there exists $f_k\in\{f_i\}^{N}_{i=1}$ such that
\[\|g-\mathcal T_T(f_k)\|_{L^\infty} = \|\mathcal T_T(f-f_k)\|_{L^\infty} \leq \|f-f_k\|_{L^\infty} < \epsilon\]
% Then $\{\mathcal T_1(f_i)\}^{N}_{i=1}\subset\mathcal G_{\sigma}$. For any $g\in\mathcal G_{\sigma}$ 
% So $\{\mathcal T_t\big{|}_{t=1}(f_i)\}^{N}_{i=1}$ is a $\epsilon$-cover set of function class $\mathcal G_{\sigma}$.
It follows that
\[\mathbb N(\epsilon,\mathcal G_{\sigma},L_\infty\left(\mu_N\right))\leq \mathbb N(\epsilon,\mathcal F,L_\infty\left(\mu_N\right))\]

% Now following the techniques in \cite{bartlett2017spectrally}, we turn to consider the covering number of the function class corresponding to two layers neural networks, i.e.
%     \begin{equation*}
% \begin{aligned}
% \mathcal F:=\Big\{f(\vx)=\phi(\vw^{\rm T}\rho\left(\Ub^{0}\vx\right))|\|\vw\|_2\leq w,\ \vw\in\mathbb R^{d},  \mbox{and},\ \|\Ub^{0}\|_2\leq u_2,\ \Ub^{0}\in\mathbb R^{d\times d}\Big\}.
% \end{aligned}
% \end{equation*}
% To calculate the covering number of $\mathcal F$, we first introduce several function classes defined as \\
% \begin{itemize}
% \item $\mathcal R:=\Big\{f(\vx)=\Ub^{0}\vx|\|\Ub^{0}\|_F\leq u_F,\ \Ub^{0}\in\mathbb R^{d\times d}\Big\}.$
% \item $\mathcal R_{\rho}:=\Big\{f(\vx)=\rho(\Ub^{0}\vx)|\|\Ub^{0}\|_F\leq u_F,\ \Ub^{0}\in\mathbb R^{d\times d}\Big\}.$
% \item $\mathcal K:=\Big\{f(\vx)=\vw^T\vx^1|\|\vw\|_2\leq w_2,\ \vw\in\mathbb R^{d}\Big\}.$
% \item $\mathcal K_{\phi}:=\Big\{f(\vx)=\phi(\vw^T\vx^1)|\|\vw\|_2\leq w_2,\ \vw\in\mathbb R^{d}\Big\}.$\\
% \end{itemize}

% \begin{remark}
% The function classes $\mathcal R_{\rho}$ and $\mathcal K_{\phi}$ are corresponding to the first and second layer of the neural network. And the input space of function classes $\mathcal K$ and $\mathcal K_{\phi}$ is the output space of function class $\mathcal R_{\rho}$, namely 
% $$\vx_1\in \{\rho(\Ub^{0}\vx)|\|\Ub^{0}\|_F\leq u_F,\ \Ub^{0}\in\mathbb R^{d\times d},\ \vx\in\mathcal D\subset B(\bm{0},R)\}.$$
% \end{remark}
% Suppose that there exist a $\epsilon_1$-cover set $\tilde{\mathcal R}=\{r_i=\Ub_i^{0}\vx\}^{N_1}_{i=1}$ of function class $\mathcal R$, which means $\forall \Ub^{0}\vx\in\mathcal R$, there exist $r_i\in\tilde{\mathcal R}$ such that $\|r_i-\Ub^{0}\vx\|_2\leq\epsilon_1$. So $\tilde{\mathcal R}_{\rho}=\{r_i=\rho(\Ub_i^{0}\vx)\}^{N_1}_{i=1}$ is a $L_1\epsilon_1$-cover set of $\mathcal R_{\rho}$. Then we assume that there exist a $\epsilon_2$-cover set $\{r_i=\vw^T\vx^1\}^{N_2}_{i=1}$ of function class $\mathcal K$ and let
% $$\tilde{\mathcal K}(i)=\{k_j=\vw^T_j\rho(r_i)\}^{N_2}_{j=1},\,\,\,\tilde{\mathcal K}_{\phi}(i)=\{\phi(k),k\in\tilde{\mathcal K(i)}\}^{N_2}_{j=1}.$$
% In fact, $N_1$ and $N_2$ equal to $\mathbb N(\epsilon_2,\mathcal K,l_2)$ and $\mathbb N(\epsilon_1,\mathcal R,l_2)$ respectively.
% Hence, for any given 
% $$\vw\in\{\vw|\|\vw\|_2\leq w_2,\ \vw\in\mathbb R^{d}\}$$ 
% and 
% $$\Ub^0\in\{\Ub^0|\|\Ub^{0}\|_F\leq u_F,\ \Ub^{0}\in\mathbb R^{d\times d}\},$$
% there exist $i\in\{1,\cdots,N_1\}$ and $j\in\{1,\cdots,N_2\}$ such that
% \begin{align}
% \label{number}
%     &\left\|\phi(\vw^T\rho(\Ub^0\vx))-\phi(\vw_j^T\rho(\Ub_i^0\vx))\right\|_{\infty}\nonumber\\
%     \leq&L_1\left\|\vw^T\rho(\Ub^0\vx)-\vw_j^T\rho(\Ub^0\vx)\right\|_2+L_1\left\|\vw_j^T\rho(\Ub^0\vx)-\vw_j^T\rho(\Ub_i^0\vx)\right\|_2\nonumber\\
%     \leq&L_1\epsilon_2+L_1w_2L_2\left\|\Ub^0\vx-\Ub_i^0\vx\right\|_2\nonumber\\
%     \leq&L_1\epsilon_2+L_1w_2L_2\epsilon_1.
% \end{align}
% According to (\ref{number}), let $\tau=L_1\epsilon_2+L_1w_2L_1\epsilon_1$, then $\tilde{\mathcal K}_{\phi}=\cup^{N_1}_{i=1}\tilde{\mathcal K}_{\phi}(i)$ is a $\tau$-cover set of function class $\mathcal F$. And the number of element in $\tilde{\mathcal K}_{\phi}$ equals to $N_1N_2$, which implies
% $$\mathbb N(\tau,\mathcal F,L_{\infty})\leq N_1N_2=\mathbb N(\epsilon_2,\mathcal K,l_2)\mathbb N(\epsilon_1,\mathcal R,l_2).$$

% Notice that $\forall f_1=\Ub\vx\in\mathcal R$ and $\forall f_2=\Wb\vx\in\mathcal R$, we have
% \begin{align*}
%     \|\Ub\vx-\Wb\vx\|_2&\leq\|\Ub-\Wb\|_2\|\vx\|_2\leq R\|\Ub-\Wb\|_F,
% \end{align*}
% which implies that there exist a positive constant $C_1$ only depending on the dimension $d$ such that covering numbers $\mathbb N(\epsilon_1,\mathcal R,l_2)$ can be bounded by
% \begin{align*}
% \mathbb N(\epsilon_1,\mathcal R,l_2)\leq \left(\frac{C_1u_F\|\vx\|_2}{\epsilon_1}\right)^{d^2}\leq \left(\frac{C_1u_FR}{\epsilon_1}\right)^{d^2}.
% \end{align*}
% Obviously, 
% \begin{align}
% \|\vx^1\|_2=\|\rho(\Ub^{0}\vx)-\rho(\bm{0})\|_2\leq L_2\|\Ub^{0}\vx\|_2\leq L_2u_FR.\nonumber
% \end{align}
% Similarly, there exist positive constant $C_2$ only depending on the dimension $d$ such that covering numbers $\mathbb N(\epsilon_2,\mathcal K,l_2)$ can be bounded by
% \begin{align*}
% \mathbb N(\epsilon_2,\mathcal K,l_2)\leq \left(\frac{C_2w_2\|\vx^1\|_2}{\epsilon_2}\right)^d\leq\left(\frac{C_2w_2L_2u_FR}{\epsilon_2}\right)^{d^2}.
% \end{align*}
% If we choose 
% $\epsilon_2=\epsilon/(2L_1),\ \epsilon_1=\epsilon/(2L_1L_2w_2)$, and
% $$\tau=L_1\epsilon_2+L_1w_2L_2\epsilon_1=\epsilon.$$
% Let $C_{\max}=\max\{C_1,C_2\}$, So we have
% \begin{align*}
%     \mathbb N(\epsilon_2,\mathcal K,l_2)\leq\left(\frac{2C_{\max}L_1w_2L_2u_F}{\epsilon}\right)^{d^2},\ \mathbb N(\epsilon_1,\mathcal R,l_2)\leq \left(\frac{2C_{\max}L_1w_2L_2u_F}{\epsilon}\right)^{d^2},
% \end{align*}
% which implies
% $$\mathbb N(\epsilon,\mathcal F,L_{\infty})\leq \left(\frac{2C_{\max}RL_1w_2L_2u_F}{\epsilon}\right)^{2d^2}.$$
% In the similar way, for $(M+1)$-layers neural networks, we have
% \begin{align}\label{F-coveringnumber}
%     \mathbb N(\epsilon,\mathcal F,L_{\infty})\leq \left(\frac{(M+1)C_{\max}RL_1w_2(L_2u_F)^M}{\epsilon}\right)^{(M+1)d^2}
% \end{align}
% Combining (\ref{coveringnumber-inequal}) and (\ref{F-coveringnumber}), we have
% \begin{align}\label{G-coveringnumber}
%     \mathbb N(\epsilon,\mathcal G_{\sigma},L_{\infty})\leq \left(\frac{(M+1)C_{\max}RL_1w_2(L_2u_F)^M}{e^{\sigma^2}e^{-\gamma}\epsilon}\right)^{(M+1)d^2}
% \end{align}

% We can get the following theorem to bound the generalization error of function classes $\mathcal F$ and $\mathcal G_{\sigma}$.

% \begin{theorem}
% \label{thm:entropy-bound}
%  Assume that input of data points $\mathcal D\subset B(\textbf{0},R)\subset\mathbb R^d$. With probability at least $1-1/(2N)$,
% \begin{align*}
%   \sup_{f\in \mathcal F}|p(f)-p_N(f)|\le Cd\sqrt{\frac{M+1}{N}}\left(\ln N+\ln a\right)^{1/2},\\
%   \sup_{f\in \mathcal G_{\sigma}}|p(f)-p_N(f)|\le Cd\sqrt{\frac{M+1}{N}}\left(\ln N+\ln a-\sigma^2 +\gamma\right)^{1/2},
% \end{align*}
% where
% \begin{align*}
% p(f)=\int_{\mathcal D} f(\vx)p(\vx)\mathd\vx,\quad p_N(f)=\frac{1}{N}\sum_{\vx_i\in S_N}f(\vx_i),
% \end{align*}
% $C$ is a positive constant only depending on dimension $d$, $$a=(M+1)C_{\max}RL_1w_2(L_2u_F)^M,$$
%  $\gamma$ is a constant depending on $\nabla v$ and $\mathcal F$ and $\mathcal G_{\sigma}$ are two function classes defined as
%     \begin{equation*}
% \begin{aligned}
% \mathcal F:&=\Big\{f(\vx)=\phi(\vw^{\rm T}\vx^{M})|\vx^{i+1}=\rho\left(\Ub^{i}\vx^{i}\right),\mbox{where}\ \vx^0=\mbox{input data},\ \vw\in\mathbb R^{d}, \\
% & \mbox{and},\ \|\Ub^{i}\|_F\leq u_F,\ \Ub^{i}\in\mathbb R^{d\times d}\ \mbox{for}\  i=0,\cdots,M-1,\ \|\vw\|_2\leq w_2 \Big\}.
% \end{aligned}
% \end{equation*}
% and
% \begin{equation*}
% \begin{aligned}
% \mathcal{G}_{\sigma} &:= \Big\{g(\vx) = u(\vx,1)|
% \frac{\partial u(\vx,t)}{\partial t}=v(\vx,t)\cdot \nabla u(\vx,t)+1/2\sigma^2\Delta u(\vx,t),u(\vx,0)=f(\vx)\,\,\,{\rm where}\,\,\, f\in\mathcal F\Big\}.
% \end{aligned}
% \end{equation*}
% \end{theorem}

The bound on the covering number of $\mathcal{F}$ is straightforward to get. The covering number of a $\ell^1$ ball with radius $W$ in $\mathbb{R}^d$, $N(\epsilon,B_1(W), \ell^1)$, is bounded by $m=\left(\frac{3W}{\epsilon}\right)^d$~\cite{lecture_unit}. For every $\vw$ that satisfy $\|\vw\|_1 \leq W$, there exists some $\vw_j$ from an $\epsilon$-cover $\{\vw_1, \cdots, \vw_m\}$, such that $\|\vw-\vw_j\|_1 < \epsilon$. Moreover, we have $\|\vx_i\|_\infty \leq R$, thus
\begin{eqnarray*}
    \max_{1\le i\le N} |f (\vx_i )-f_j(\vx_i)|&=&\max_{1\le i\le N} |\phi(\vw\vx_i )-\phi(\vw_j \vx_i)|\\
    &\leq& \max_{1\le i\le N} |\vw\vx_i-\vw_j \vx_i|\\
    &\leq& \max_{1\le i\le N} \|\vw-\vw_j\|_1 \|\vx_i\|_\infty \\
    &<& \epsilon R
\end{eqnarray*}
Since $\|\vw_{\text{fc}}\|_1 \leq W$ by assumption, we have $N(\epsilon,\mathcal F,L_\infty(\mu_N))\leq \left(\frac{3WR}{\epsilon}\right)^d$. 

Finally, we can bound the empirical Rademacher complexity using the covering number,
\begin{theorem}(Theorem 3.1 in \cite{lecture_upper})
Assume that all $f \in \mathcal{H}$ make predictions in $[-1,1]$. We have:
\[R_{S_N}(\mathcal H) \leq \inf _\alpha \sqrt{\frac{2 \log N(\alpha,\mathcal H,L_1(\mu_N))}{N}}+\alpha\]
\end{theorem}

Using the theorem above,
\begin{eqnarray*}
    R_{S_N}(\mathcal G_\sigma)&\leq& \inf _\epsilon \sqrt{\frac{2 \log N(\epsilon,\mathcal G_\sigma,L_1(\mu_N))}{N}}+\epsilon\\
    & \leq& \inf _\epsilon \sqrt{\frac{2 \log N(\epsilon,\mathcal G_\sigma,L_\infty(\mu_N))}{N}}+\epsilon\\
    &\leq& \inf _\epsilon \sqrt{\frac{2 \log N(\epsilon,\mathcal F,L_\infty(\mu_N))}{N}}+\epsilon\\
    &\leq& \inf _\epsilon \sqrt{\frac{2 \log \left(\frac{3WR}{\epsilon}\right)^d}{N}}+\epsilon\\
    &=& \inf _\epsilon \sqrt{\frac{2d \log (3WR/\epsilon)}{N}}+\epsilon
\end{eqnarray*}

% For the purpose of proving Theorem \ref{rademacher}, we give an inequality about Rademacher complexity as following.
% \begin{theorem}(Theorem 2.23 in \cite{mendelson2003few})\label{thm:rademacher-upper}
% Let $\mu(\vx)$ be a probability distribution function and  set $\mathcal H$ to be a class of functions on $\mathcal D$. Denote 
% \begin{align*}
%     Z=\sup_{f\in \mathcal H}|\frac{1}{N}\sum^N_{i=1}f(\vx_i)-\mathbb E_{\mu}f|,
% \end{align*}
% where $\{\vx_i\}^N_{i=1}$ are independent random variables distributed according to $\mu$. Then we have
% \[\frac{1}{2}\mathbb E_{\mu}Z\leq R_{S_N}(\mathcal H)\leq 2\mathbb E_{\mu}Z+|\sup_{f \in \mathcal H}\mathbb E_{\mu}f|\cdot\mathbb{E}_{\sigma_i}\left|\frac{1}{N} \sum_{i=1}^N \sigma_i\right|.\]
% \end{theorem}

% Finally, we can prove \cref{rademacher}. \revise{Rademacher complexity not empirical, $b \sim N$, remark on norm and measure}
% \begin{proof}
% Choose $\mathcal{H}=\mathcal G_{\sigma}$ in \cref{thm:rademacher-empirical-actual} and \cref{thm:rademacher-upper}. Since \revise{$g\leq f$}, according to \cref{thm:rademacher-empirical-actual},
%     \begin{equation}
%         \mathbb P (Z>\epsilon)\leq 8\mathbb{E}_\mu\left[ \mathbb N(\frac{\epsilon}{8},\mathcal G_{\sigma},L_1(\mu_N))\right]\exp(-\frac{N\epsilon^2}{128})
%         \label{eq:rademacher-prob}
%     \end{equation}
%     The covering number \revise{ref}
%     \[\mathbb N(\frac{\epsilon}{8},\mathcal G_{\sigma},L_1(\mu_N))\leq \mathbb N(\frac{\epsilon}{8},\mathcal G_{\sigma},L_\infty(\mu_N)) \leq \mathbb N(\frac{\epsilon}{8},\mathcal F,L_\infty(\mu_N))\]
%     The covering number of $F$ can be easily bounded, using the fact that any $f\in \mathcal{F}$ is contained within the $L_\infty$ unit ball. Indeed, for any $f\in \mathcal{F}$,
%     \[\left\|f\right\|_{L_\infty(\mu_N)}= \max_{1\le i\le N} |f (\vx_i )|\leq 1 \]
%     % The covering number of a unit ball in $\mathbb{R}^d$ by open balls with radius $r$ is upper bounded by $\left(\frac{3}{r}\right)^d$~\cite{wu2016packing}. In our setting, $d=N$ and $r=\epsilon/8$, thus
%     % \[\mathbb N(\frac{\epsilon}{8},\mathcal G_{\sigma},L_1(\mu_N)) \leq \left(\frac{24}{\epsilon}\right)^N\]
%     % The inequality holds for any realization $S_N=\{\vx_i\}_{i=1}^N$. 
%     \revise{
%     Denote $k=\left\lceil\frac{N a^2 b^2}{\epsilon^2}\right\rceil\leq \frac{N a^2 b^2}{\epsilon^2}+1$. Let
%     \[\delta=8(2d)^k\exp(-\frac{N\epsilon^2}{128})\]
%     }
%     \[\delta=8\left(\frac{24}{\epsilon}\right)^N\exp(-\frac{N\epsilon^2}{128})\]
    
%     % then
%     % \[\epsilon=\sqrt{\frac{128}{N}\left(N\log\frac{24}{\epsilon}+\log \frac{8}{\delta}\right)}\]
%     % Obviously $\epsilon\geq \sqrt{\frac{128}{N}}$, then
%     % \begin{align*}
%     %     \epsilon&\leq\sqrt{\frac{128}{N}\left(N\log\frac{24}{\sqrt{128/\epsilon}}+\log \frac{8}{\delta}\right)}\\
%     %     &=\sqrt{64\log N+64\log\frac{9}{2}+\frac{128}{N}\log \frac{8}{\delta}}
%     % \end{align*}
%     \revise{
%     \[ \]

%     }
    
    
%     Moreover, 
%     \[|\sup_{f \in \mathcal \mathcal G_{\sigma}}\mathbb E_{\mu}f|\leq |\sup_{f \in \mathcal \mathcal F}\mathbb E_{\mu}f| \leq 1\]
%     and
%     \[\mathbb{E}_{\sigma_i}\left|\frac{1}{N} \sum_{i=1}^N \sigma_i\right|=\frac{1}{N2^{N-1}}\left\lceil\frac{N}{2}\right\rceil\left(\begin{array}{c}
%     N \\
%     \lceil N / 2\rceil
%     \end{array}\right) \sim \sqrt{\frac{2}{N\pi}}\]
%     In conclusion, according to \cref{thm:rademacher-upper}, with probability at least $1-\delta$,
%     \begin{align*}
%         R_{S_N}(\mathcal G_\sigma)&\leq 2\mathbb E_{\mu}Z+|\sup_{f \in \mathcal G_\sigma}\mathbb E_{\mu}f|\cdot\mathbb{E}_{\sigma_i}\left|\frac{1}{N} \sum_{i=1}^N \sigma_i\right|\\
%         &\leq 2\sqrt{64\log N+64\log\frac{9}{2}+\frac{128}{N}\log \frac{8}{\delta}}+\frac{1}{N2^{N-1}}\left\lceil\frac{N}{2}\right\rceil\left(\begin{array}{c}
%     N \\
%     \lceil N / 2\rceil
%     \end{array}\right)\\
%     &= C_1\sqrt{\log N}+ C_2 \sqrt{\frac{\log \frac{1}{\delta}}{N}}+C_3 \sqrt{\frac{1}{N}}
%     \end{align*}
    
%     % \begin{align*}
%     % R_{S_N}(\mathcal G_{\sigma})&\leq 2\mathbb E_\mu Z+\frac{1}{\sqrt{n}}|\sup_{f\in \mathcal G_{\sigma}}\mathbb E_{\vx\sim p(\vx)}f|.\\
%     % & \leq Cd\sqrt{\frac{M+1}{N}}\left(\ln N+\ln a-\sigma^2 +\gamma\right)^{1/2}+\frac{1}{\sqrt{N}}|\sup_{f\in \mathcal G_{\sigma}}\mathbb E_{\vx\sim p(\vx)}f|\\
%     % & \leq Cd\sqrt{\frac{M+1}{N}}\left(\ln N+\ln a-\sigma^2 +\gamma\right)^{1/2}+\frac{1}{\sqrt{N}}\sup_{f\in \mathcal G_{\sigma}}\|f\|_{\infty}\\
%     % &\leq Cd\sqrt{\frac{M+1}{N}}[\left(\ln N+\ln a-\sigma^2 +\gamma\right)^{1/2} +\exp(-\sigma^2)].
%     % \end{align*}
% \end{proof}