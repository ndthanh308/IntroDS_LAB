\section{\texorpdfstring{Proof of \Cref{thm:main}}{Proof of Theorem 2.2}}
\label{sec:proof_main}

\begin{proof}
Following the techniques in~\cite{alvarez1993axioms}, we set
\[\delta_{t,s}(f)=\frac{\mathcal T_t(f)-\mathcal T_s(f)}{t-s}, \quad \delta_t(f)=\delta_{t,0}(f)\]
The proof of \cref{thm:main} mainly consists of two steps. First we will prove that $\delta_{t}(f)$ converges to a limit as $t\rightarrow 0$, which we call an infinitesimal generator. Then, we verify that the generator satisfy a second-order convection-diffusion equation.

% Using \textbf{[Temporal Regularity]}, for all $h$, $t\in[0,T]$ and all $f,g\in Q$, we can get
% \begin{align}
%     \|\delta_t(f+hg)-\delta_t(f)\|_{L^{\infty}}\leq Ch.
% \end{align}
% \vspace{0.3cm}

First of all, we describe some basic properties of $\delta_t(f)$. From \textbf{[Temporal Regularity]}, we know $\delta_t(f)$ is uniformly bounded,
\[ \|\delta_t(f)\|_{L^{\infty}} =\left\|\frac{\mathcal T_t(f)-f}{t}\right\|_{L^{\infty}} \leq C \]
Also, it is obvious that \textbf{[Linearity]} is preserved for $\delta_t(f)$,
\[\delta_t(\beta_1 f+\beta_2 g) = \beta_1 \delta_t(f)+\beta_2 \delta_t(g)\]
Additionally, $\delta_t(f)$ is Lipschitz continuous on $\mathbb{R}^d$, uniformly for $t\in(0,1]$ and $f\in C^{\infty}_b$. Indeed, let $\vh \in \mathbb R^d, \|\vh\|_2=h$,
\[\|\tau_{\vh}(\delta_t(f))-\delta_t(f)\|_{L^{\infty}} \leq \|\tau_{\vh}(\delta_t(f))-\delta_t(\tau_{\vh}f)\|_{L^{\infty}} + \|\delta_t(\tau_{\vh}f)-\delta_t(f)\|_{L^{\infty}}\]
The first term can be bounded using \textbf{[Spatial Regularity]},
\[\|\tau_{\vh}(\delta_t(f))-\delta_t(\tau_{\vh}f)\|_{L^{\infty}} = \left\|\frac{\tau_{\vh} (\mathcal T_t f)-\mathcal T_t(\tau_{\vh} f)}{t}\right\|_{L^{\infty}}\leq Ch\]
The second term can be bounded using the fact that $f\in C^{\infty}_b$. We may write $\tau_{\vh}f=f+hg$ for some $g\in C^{\infty}_b$ depending on $\vh$, then using linearity and uniform boundedness,
\[\|\delta_t(\tau_{\vh}f)-\delta_t(f)\|_{L^{\infty}}=\|\delta_t(f+hg)-\delta_t(f)\|_{L^{\infty}}=h\|\delta_t(g)\|_{L^{\infty}}\leq Ch\]
Lastly, since $f\leq g + \|f-g\|_{L^{\infty}}$, we can use \textbf{[Comparison Principle]} and \textbf{[Linearity]} to get
\[\mathcal T_{t}(f)\leq \mathcal T_{t}(g+\|f-g\|_{L^{\infty}}) = \mathcal T_{t}(g) +\|f-g\|_{L^{\infty}}\]
Thus for any $f,g \in C^{\infty}_b$\footnote{By continuity, $T_{t}$ can be extended as a mapping from $B U C\left(\mathbb{R}^{d}\right)$, the space of bounded, uniformly continuous functions on $\mathbb{R}^{d}$ into itself. By density, \cref{eq:a1_contraction} still hold for $f, g$ in $B U C\left(\mathbb{R}^{d}\right)$. The extension will be used in \cref{eq:a1_estimate_2}.} and $t\geq 0$,
\begin{equation}
	\label{eq:a1_contraction}
    \|\mathcal T_{t}(f)-\mathcal T_{t}(g)\|_{L^{\infty}}\leq \|f-g\|_{L^{\infty}}
\end{equation}
Now we are ready to prove the main theorem.

\subsection{Existence of infinitesimal generator}
First we want to prove: 
\begin{equation}
	\label{eq:a1_estimate}
    \|\delta_{t+s,t}(f)-\delta_s(f)\|_{L^{\infty}}\leq m(t)
\end{equation}
where $m(t)$ is some continuous, nonnegative, nondecreasing function such that $m(0)=0$, and $m(t)$ depends only on the bounds of derivatives of $f$.

Since $\delta_s(f)$ not necessarily belongs to $C^{\infty}_b$, we mollify $\delta_s(f)$ by introducing a standard mollifier $K\geq 0$ satisfying $\int_{\mathbb R^d}K\mathd\vy=1$, $K\in C_0^{\infty}(\mathbb R^d)$ and $K_{\varepsilon}=\varepsilon^{-d}K(\cdot/\varepsilon)$. Using the Lipschitz continuity of $\delta_s(f)$, we can obtain that for all $\varepsilon>0$, there exist a positive constant $C_1$ depending only on the derivatives of $f$, such that
\begin{equation}
	\label{eq:a1_estimate_1}
    \|\delta_s(f)\ast K_{\varepsilon}-\delta_s(f)\|_{L^\infty}\leq C_1\varepsilon.
\end{equation}
where $\ast$ denote the convolution. Because of \textbf{[Markov Property]}, \textbf{[Temporal Regularity]} and \cref{eq:a1_contraction}, we have
\begin{eqnarray}
	&&\left\|\mathcal T_{t+s}(f)-\mathcal T_t\circ \mathcal T_s(f)\right\|_{L^{\infty}} = \left\|\mathcal T_t\circ \mathcal T_{t+s,t}(f)-\mathcal T_t\circ \mathcal T_s(f)\right\|_{L^{\infty}} \label{eq:a1_estimate_2}\\
	&\leq& \left\|\mathcal T_{t+s,t}(f)-\mathcal T_s(f)\right\|_{L^{\infty}}\leq Cst \nonumber
\end{eqnarray}

By \cref{eq:a1_contraction} and \cref{eq:a1_estimate_1}, we have
\begin{eqnarray}
	&&\left\|\mathcal T_t\circ \mathcal T_s(f)-\mathcal T_t(f+s\delta_s(f)*K_{\varepsilon})\right\|_{L^{\infty}} \leq \left\|\mathcal T_s(f) \label{eq:a1_estimate_3} -(f+s\delta_s(f)*K_{\varepsilon})\right\|_{L^{\infty}}\\
	&=&s\|\delta_s(f)\ast K_{\varepsilon}-\delta_s(f)\|_{L^\infty}\leq C_1\varepsilon s \nonumber
\end{eqnarray}
By \textbf{[Linearity]} and \textbf{[Temporal Regularity]}, since $\delta_s(f)*K_{\varepsilon}\in C^{\infty}_b$, we have
\begin{eqnarray}
	&& \left\|\mathcal T_t(f+s\delta_s*K_{\varepsilon})-(\mathcal T_t(f)+s\delta_s(f)*K_{\varepsilon})\right\|_{L^{\infty}} \label{eq:a1_estimate_4} \\
	&=& s\left\|\mathcal T_t(\delta_s*K_{\varepsilon})-\delta_s(f)*K_{\varepsilon}\right\|_{L^{\infty}} \leq C_{\varepsilon}st \nonumber
\end{eqnarray}

for some positive constant $C_{\varepsilon}$ depending only on $\varepsilon$. Combining \cref{eq:a1_estimate_1}, \cref{eq:a1_estimate_2}, \cref{eq:a1_estimate_3} and \cref{eq:a1_estimate_4}, we finally deduce that
\[\|\delta_{s+t,t}(f)-\delta_s(f)\|_{L^\infty}\leq 2C_1\varepsilon+C_{\varepsilon}t+Ct\]
By setting $m(t)=\inf_{\varepsilon\in(0,1]}(2C_1\varepsilon+C_{\varepsilon}t)+Ct$ we can get desired estimate \cref{eq:a1_estimate}.

% \begin{eqnarray}\label{A5}
% &&\left\|\frac{\mathcal T_{t+s}(f)-\mathcal T_t(f+s\delta_s*K_{\varepsilon})}{s}\right\|_{L^{\infty}}\nonumber\\
% &\leq&\left\|\frac{\mathcal T_{t+s,s}\circ \mathcal T_s(f)-\mathcal T_t\circ \mathcal T_s(f)}{s}\right\|_{L^{\infty}} +\left\|\frac{\mathcal T_t\circ \mathcal T_s(f)-\mathcal T_t(f+s\delta_s*K_{\varepsilon})}{s}\right\|_{L^{\infty}}\nonumber\\
% &\leq& Ct+\left\|\frac{\mathcal T_s(f)-f-s\delta_s*K_{\varepsilon}}{s}\right\|_{L^{\infty}}\nonumber\\
% &\leq& Ct+C_2\varepsilon
% \end{eqnarray}
% On the other hand, since $\delta_s(f)\ast K_{\varepsilon}\in C^{\infty}_b$,  \textbf{[Temporal Regularity]} implies that
% \begin{align}\label{A6}
%     \|\mathcal T_t(f+s\delta_s(f)\ast  K_{\varepsilon})-\mathcal T_t(f)-s\delta_s(f)\ast K_{\varepsilon}\|_{L^{\infty}}\leq C_{\varepsilon}st
% \end{align}

Now we give a Cauchy estimate for $\delta_s(f)$. We will prove that
\[\|\delta_t(f)-\delta_h(f)\|_{L^{\infty}}\leq 2\frac{C_0r}{t}+m(t)\quad {\rm{where}}\ r = t - Nh, N=\left[\frac{t}{h}\right]\]
Notice that
\[\delta_t(f)=\frac{Nh}{t}\delta_{Nh}(f)+\frac{r}{t}\delta_{Nh+r,Nh}(f)\]
Using \cref{eq:a1_estimate} with $s = r$, $t = Nh$ we have
\begin{equation}
	\label{eq:a1_use_estimate_1}
	\|\delta_{Nh+r}(f) - \frac{Nh}{Nh+r}\delta_{Nh}(f)-\frac{r}{Nh+r}\delta_r(f)\|_{L^{\infty}}\leq \frac{r}{Nh+r}m(Nh)
\end{equation}
Again, notice that
\[\delta_{Nh}(f)=\frac{(N-1)h}{Nh}\delta_{(N-1)h}(f)+\frac{h}{Nh}\delta_{Nh,(N-1)h}(f)\]
Using \cref{eq:a1_estimate} with $s = h$, $t = (N-1)h$, we have
\begin{equation}
	\label{eq:a1_use_estimate_2}
	\|\delta_{Nh}(f)-\frac{N-1}{N}\delta_{(N-1)h}(f)-\frac{1}{N}\delta_h(f)\|_{L^{\infty}}\leq \frac{1}{N}m((N-1)h)
\end{equation}
Combining \cref{eq:a1_use_estimate_1} and \cref{eq:a1_use_estimate_2}, we obtain
\[\|\delta_t(f)-(N-1)\frac{h}{t}\delta_{(N-1)h}(f)-\frac{h}{t}\delta_h(f)-\frac{r}{t}\delta_r(f)\|_{L^{\infty}}\leq \frac{r}{t}m(Nh)+\frac{h}{t}m((N-1)h)\]
Reiterating the procedure, we obtain that after $(N-1)$ steps,
\[\|\delta_t(f)-\frac{Nh}{t}\delta_h(f)-\frac{r}{t}\delta_r(f)\|_{\infty}\leq \frac{r}{t}m(Nh)+\frac{h}{t}\sum_{j=1}^{N-1}m(jh)\]
Since $m(t)$ is nondecreasing and $\delta_t(f)$ is uniformly bounded, we have
\begin{eqnarray*}
	\|\delta_t(f)-\delta_h(f)\|_{L^{\infty}} & \leq &\|\delta_t(f)-\frac{Nh}{t}\delta_h(f)-\frac{r}{t}\delta_r(f)\|_{L^{\infty}}+\frac{r}{t}\|\delta_h(f)-\delta_r(f)\|_{L^{\infty}}\\
	& \leq &\frac{r}{t}m(Nh) + \frac{(N-1)h}{t}m(t) + \frac{r}{t}\left(\|\delta_h(f)\|_{L^{\infty}} + \|\delta_r(f)\|_{L^{\infty}}\right)\\
	&\leq &m(t)+2\frac{C_0 r}{t}
\end{eqnarray*}
Since $\delta_t(f)$ is uniformly bounded and Lipschitz continuous, We can pick $h_n$ going to $0$ and $\delta_{h_n}(f)$ converges uniformly on compact sets to a bounded Lipschitz function on $\mathbb{R}^d$, which we denote by $A[f]$ (the infinitesimal generator). Then using the Cauchy estimate we have derived, we have
\[\lim_{n\to\infty}\|\delta_t(f)-\delta_{h_n}(f)\|_{L^{\infty}}\leq \lim_{n\to\infty}m(t)+2\frac{C_0r}{t}\]
which implies
\[\|\delta_t(f)-A[f]\|_{L^{\infty}} \leq m(t)\]
So $\delta_t(f)$ converges uniformly to $A[f]$ when $t$ goes to $0$. Similarly, there exist an operator $A_t$ such that $\delta_{s,t}(f)$ converges uniformly to $A_t[f]$ when $s$ goes to $t$.

\subsection{Second-order convection-diffusion equation}
Let $f,g\in C^{\infty}_{b}$ and satisfy
$f(\bm{0})=g(\bm{0})=0$ (if not equal to 0, we replace $f(\vx),g(\vx)$ by $f(\vx)-f(\bm{0}),g(\vx)-g(\bm{0})$), $Df(\bm{0})=Dg(\bm{0})=\vp\in \mathbb R^{d}$, $D^{2}f(\bm{0})=D^{2}g(\bm{0})=\Ab\in \mathbb R^{d\times d}$. We are first going to show that $A[f](\bm{0})=A[g](\bm{0})$. 

Introduce $f^{\varepsilon}=f+\varepsilon \|\vx\|_2^{2} \in C_b^{\infty}$. Using Taylor formula, there exist a positive constant $c$ such that for $\|\vx\|_2\leq c\varepsilon$ we have $f^{\varepsilon}\geq g$. Let $w\in C^{\infty}_{b}(\mathbb R^{d})$ be a bump function satisfying 
\begin{equation*}
\begin{cases}
w(\vx)=1\,\,\,&\|\vx\|_2\leq c/2\\
0 \leq w(\vx) \leq 1\,\,\,&c/2<\|\vx\|_2<c \\
w(\vx)=0\,\,\,&\|\vx\|_2\geq c
\end{cases}
\end{equation*}
and $w_{\varepsilon}(\vx)=w(\vx/\varepsilon)$. Finally we introduce $\bar{f}^{\varepsilon}=w_{\varepsilon}f^{\varepsilon}+(1-w_{\varepsilon})g$ so that $f^{\varepsilon}_{0}\geq g$ on the whole domain $\mathbb R^{d}$. Then because of \textbf{[Comparison Principle]}, $\mathcal T_{t}(\bar{f}^{\varepsilon})\geq \mathcal T_{t}(g)$. Since $\bar{f}^{\varepsilon}(\bm{0})=f^{\varepsilon}(\bm{0})=f(\bm{0})=g(\bm{0})$, we can get $A[\bar{f}](\bm{0})\geq A[g](\bm{0})$.\\
Because there exists a neighborhood of $\bm{0}$ that $\bar{f}^{\varepsilon}=f^{\varepsilon}$, we have $D^{\alpha}\bar{f}^{\varepsilon}(\bm{0}) = D^{\alpha}f^{\varepsilon}(\bm{0})$ for $\forall \left|\alpha\right|\geq 0$. In view of \textbf{[Locality]} we have $A[\bar{f}^{\varepsilon}](\bm{0})=A[f^{\varepsilon}](\bm{0})$. And considering the continuity of $A$, we can deduce $A[f^{\varepsilon}](\bm{0})$ converges to $A[f](\bm{0})$ in $L^{\infty}$ when $\varepsilon$ goes to $0$. This means $A[f](\bm{0}) \geq A[g](\bm{0})$. By symmetry, we can get $A[f](\bm{0}) \leq A[g](\bm{0})$, which means $A[f](\bm{0}) = A[g](\bm{0})$. Also, in our proof $\bm{0}$ can be replaced by any $\vx\in \mathbb R^d$. So the value of $A[f](\vx)$ only depends on $\vx,f,Df,D^2f$. Observe that from \textbf{[Linearity]}, $A[f+C]=A[f]$ for any constant $C$, so $A[f](\vx)$ only depends on $\vx,Df,D^2f$. At last, we prove that there exists a continuous function $F$ such that
\[A[f]=F(Df,D^{2}f,\vx)\]
From \textbf{[Comparison Principle]} of $\mathcal T_t$, we can derive a similar argument for $F$. Let $\bm{A} \succeq \bm{B}$ and set
\[f(\vx)=\left[(\vp, \vx)+\frac{1}{2}(\bm{A} \vx, \vx)\right] w(\vx), \qquad g(\vx)=\left[(\vp, \vx)+\frac{1}{2}(\bm{B} \vx, \vx)\right] w(\vx)\]
Indeed, $f \geq g$ on $\mathbb{R}^{d}$ while $f(\bm{0})=g(\bm{0})$. Using \textbf{[Comparison Principle]},
\begin{eqnarray*}
	&& F(\vp,\bm{A}, \bm{0}) =A[f](\bm{0}) =\lim _{t \rightarrow 0^{+}}\frac{T_{t}(f)(\bm{0})-f(\bm{0})}{t} \\
	& \geq &\lim _{t \rightarrow 0^{+}}\frac{T_{t}(g)(\bm{0})-g(\bm{0})}{t}= A[g](\bm{0}) = F(\vp,\bm{B}, \bm{0})
\end{eqnarray*}
$\bm{0}$ can be replaced by any $\vx\in \mathbb R^d$. Thus
\begin{equation}
	\label{eq:a1_compare}
    F(\vp,\bm{A},\vx)\geq F(\vp,\bm{B},\vx) \qquad \text{for any} \quad \bm{A} \succeq \bm{B}
\end{equation}

In the same way, we can get
\[A_{t}[f]=F(D (\mathcal T_t(f)),D^{2} (\mathcal T_t(f)),\vx,t)\]
which implies $u(\vx,t)=\mathcal T_t(f)$ satisfies
\[\begin{cases}
\frac{\partial u(\vx,t)}{\partial t}=F(Du, D^{2}u,\vx,t),\vx\in \mathbb R^{d},t\in [0,T]\\
u(\vx,0)=f(\vx).
\end{cases}\]

According to \textbf{[Linearity]},
$F$ therefore satisfies
\[F(rDf+sDg,rD^{2}f + sD^{2}g,\vx,t) = rF(Df,D^{2}f,\vx,t) + sF(Dg,D^{2}g,\vx,t)\]
for any real numbers $r$ and $s$ and any functions $f$ and $g$ and at any point $(\vx,t)$. Since the values of $Df,Dg,D^2f,D^2g$ are arbitrary and can be independently taken to be 0, we obtain for any vectors $\bm{v}_1,\bm{v}_2$ and symmetric
matrices $\bm{A}_1,\bm{A}_2$ and any fixed point $(\vx_0,t_0)$ that
\begin{eqnarray*}
F(r\bm{v}_1+s\bm{v}_2,r\bm{A}_1+s\bm{A}_2,\vx_0,t_0)&=&rF(\bm{v}_1,\bm{A}_1,\vx_0,t_0)+sF(\bm{v}_2,\bm{A}_2,\vx_0,t_0) \\
F(\bm{v}_1,\bm{A}_1,\vx_0,t_0)&=&F(\bm{v}_1,0,\vx_0,t_0)+F(0,\bm{A}_1,\vx_0,t_0).
\end{eqnarray*}
Let
\[F(\bm{v},0,\vx_0,t_0)= F_1(\bm{v},\vx_0,t_0), F(0,\bm{A},\vx_0,t_0)=F_2(\bm{A},\vx_0,t_0)\]
Then $F_1$ and $F_2$ are both linear, i.e., there exists a function $v:\mathbb R^d\times [0,T]\rightarrow \mathbb R^d$ and a function $\sigma:\mathbb R^d\times [0,T]\rightarrow \mathbb R^{d\times d}$ such that,
\begin{eqnarray*}
 F_1(\bm{v},\vx_0,t_0)&=&v(\vx_0,t_0)\cdot\bm{v},\\
 F_2(\bm{A},\vx_0,t_0)&=&\sum_{i,j}\sigma_{i,j}(\vx,t)A_{i,j},
\end{eqnarray*}
where $A_{i,j}$ is the $i,j$-th element of matrix $\Ab$ and $\sigma_{i,j}$ is the $i,j$-th element of matrix function $\sigma$.

If we choose $\Ab=\bm{\xi}\bm{\xi}^T\succeq \bm{0}$, where $\bm{\xi}=(\xi_1,\cdots,\xi_d)^T$ is a $d$-dimension vector, then according to \cref{eq:a1_compare},
\[\bm{\xi}^T\sigma(\vx_0,t_0)\bm{\xi}= \sum_{i,j}\sigma_{i,j}(\vx_0,t_0)\xi_i\xi_j = F_2(\bm{A},\vx_0,t_0)\geq F_2(\bm{0},\vx_0,t_0) = 0\]
which implies matrix function $\sigma$ is a positive semi-definite function.

Thus we can finally get there exist a Lipschitz continuous function $v:\mathbb R^d\times [0,T]\rightarrow \mathbb R^d$ and a Lipschitz continuous positive semi-definite function $\sigma:\mathbb R^d\times [0,T]\rightarrow \mathbb R^{d\times d}$ such that $u(\vx,t)=\mathcal T_{t}(f)$ is the solution of the equation
\begin{equation*}
\begin{cases}
\frac{\partial u(\vx,t)}{\partial t}=v(\vx,t)\cdot \nabla u(\vx,t)+\sum_{i,j}\sigma_{i,j}(\vx,t)\frac{\partial u}{\partial x_i\partial x_j} (\vx,t),\vx\in \mathbb R^{d},t\in [0,1]\\
u(\vx,0)=f(\vx),
\end{cases}
\end{equation*}
where $\sigma_{i,j}(\vx,t)$ is the $i,j$-th element of matrix function $\sigma(\vx,t)$.
\end{proof}

The existence and uniqueness of solution of \cref{eq:main} are guaranteed by Theorem 8.1.1 in \cite{oksendal2013stochastic},
\begin{theorem}[Kolmogorovâ€™s backward equation~\cite{oksendal2013stochastic}]
Assume I$\hat t$o process $\vx(t)$ satisfies SDE
\begin{equation*}
d\vx(t)=v(\vx(t),t)dt+\sigma(\vx(t),t)d\Bb(t), \vx(0)=\vx
\end{equation*}
where $\Bb(t)$ is Brownian motion and $v:\mathbb R^d\times\mathbb R^+\rightarrow\mathbb R^d$, $\sigma:\mathbb R^d\times\mathbb R^+\rightarrow\mathbb R^{d\times d}$ are all Lipschitz continuous function. Let $f_{\vw}(\vx)\in C^2_0(\mathbb R^d)$. Then
\[u(\vx,t)=\mathbb E[f_{\vw}(\vx(t)|\vx(0)=\vx]\]
is the unique solution of \cref{eq:main}.
\end{theorem}