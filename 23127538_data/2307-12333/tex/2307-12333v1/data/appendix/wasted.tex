The appendices are structured as follows. In Sections~\ref{proof1}, \ref{proof2}, and \ref{proof3}, we prove Theorems~\ref{expression}, \ref{stability}, and \ref{rademacher}, respectively. More experimental details on Half-moon dataset are provided in Section~\ref{sec:appendix:exp:details}. Finally, in Section~\ref{arch}, we summarize the architecture of deep neural networks used in our experiments.


\section{More experimental details on Half-moon dataset}\label{sec:appendix:exp:details}
In this subsection, we plot the decision boundary of ResNet trained with different hyperparameters in Figure \ref{appendix:decisionboundary}. As drawn in Figure \ref{decision boundary}, we can see as the hyperparameters $\lambda$ and $\sigma^2$ increasing, the decision boundary becomes more smooth, which implies the overfitting risk reduce. These experimental results are also consistent with the conclusion of Theorem \ref{rademacher}.

% Figure environment removed

\section{Architectures of the Used DNNs}\label{arch}
%Figure \ref{archmoon} shows the architectures of ResNets used for Half-Moon data set, and we plot the architectures of ResNets used for CIFAR-10 and CIFAR-100 in Figure \ref{archcifar}.
 We plot the inputs for the ResNet used on CIFAR-10/CIFAR-100 datasets in Figure \ref{inputs} and every element of the matrix $tt$ equals to $t$. And Figure~\ref{fig:main-framework} shows the architectures of ResNets used in this paper. %We summarize our training method for ResNet20/56/110 in Algorithm \ref{trick}. 
% Figure environment removed


% Figure environment removed