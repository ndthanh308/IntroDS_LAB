%% Figure environment removed

%% Figure environment removed

In this section, we will numerically verify the performance of our training method for ResNets.
\subsection{Preliminaries}

\paragraph{Datasets} In our experiments, we consider both synthetic half-moon dataset and real-world CIFAR-10~\cite{krizhevsky2009learning}, Fashion-MNIST~\cite{xiao2017fashion} and SVHN~\cite{netzer2011reading} datasets. Half-moon dataset is a randomly generated 2d synthetic dataset in which we randomly generate 500 points and 1000 points with a standard deviation of 0.3 as training set and testing set, respectively. CIFAR-10 contains 60K $3 \times 32 \times 32$ color images from 10 different classes with 50K and 10K of them used for training and testing, respectively. SVHN is a 10-class house number classification dataset which contains 73257 training images and 26032 testing images, each of size $3 \times 32 \times 32$. Fashion-MNIST is a 10-class greyscale dataset which contains 60K training images and 10K testing images. The size of each image is $28 \times 28$.

%\paragraph{Project gradient descent attack (PGD).} To verify the efficiency of our method for improving the robustness of DNN, we consider the project gradient descent (PGD) \cite{madry2018towards} in all the experiments below. Before introducing PGD, we first introduce a fast gradient sign method attack (FGSM). The FGSM attack searches the adversarial image $\vx'$ by maximizing the linearized loss function function $\mathcal L(\vx',y)\approx\mathcal L(\vx,y)+\nabla\mathcal L(\vx,y)^T(\vx'-\vx)$ with constraint $\|\vx'-\vx\|_p\leq\epsilon$, where $\epsilon$ is the maximum perturbation and $p=2,\infty$. If we add uniform noise in the range $[-\epsilon,+\epsilon]$ to clean data $\vx$, denote the resulted data as $\vx^{(0)}$. Then PGD iterates FGSM with step size $\alpha$ and clips the perturbed image to generate
%the enhanced adversarial attack. Mathematically, $\ell^{\infty}$ and $\ell^2$ PGD attack can be expressed as
%$$\ell^{\infty}:\vx^{(m)}={\rm Clip}_{\vx,\epsilon}\{\vx^{(m-1)}+\alpha{\rm sign}(\nabla\mathcal L(\vx^{(m-1)},y))\},$$
%$$\ell^{2}:\vx^{(m)}={\rm Clip}_{\vx,\epsilon}\left\{\vx^{(m-1)}+\alpha\left(\frac{\nabla\mathcal L(\vx^{(m-1)},y)}{\|\nabla\mathcal L(\vx^{(m-1)},y)\|_2}\right)\right\}$$
%respectively, where $m = 1,\cdots,K$ and the adversarial data be $\vx'=\vx^{(K)}$ with $K$ being the total number of iterations. For the Half-moon dataset, we apply 20 iterations PGD attack (PGD$^{20}$) with step size $0.01$. The perturbation is constrained to be $0.2$ under $l^{\infty}$ norm. For the CIFAR-10/CIFAR-100 dataset, when under $\ell^{\infty}$ norm attack, we apply 20 iterations PGD attack with step size $1/255$ and when under $\ell^{2}$ norm attack and we apply 50 iterations PGD attack (PGD$^{50}$) with step size $0.1$. 

\paragraph{Performance evaluations} 
We evaluate the performance by both natural accuracy on original test samples and robust accuracy on adversarial test samples within a perturbation range $\epsilon$. To craft adversarial samples, we use Project Gradient Descent~(PGD) and AutoAttack~\cite{croce2020reliable}. PGD first adds random uniform noise from $(-\epsilon,\epsilon)$ to clean samples, and iterates Fast Gradient Sign Method~(FGSM) with fixed step size $\alpha$ for $m$ steps,
\[\vx^{(m)}={\rm Clip}_{\vx,\epsilon}\{\vx^{(m-1)}+\alpha{\rm sign}(\nabla\mathcal L(\vx^{(m-1)},y))\}\]

AutoAttack~\cite{croce2020reliable} is a more reliable benchmark for evaluating robustness, which ensembles four strong parameter-free attacks. It is reported that some defense methods which claim to be robust against PGD attack are destroyed by AutoAttack. Thus, we report classification accuracy on adversarial examples crafted by AutoAttack to evaluate the robustness of our method.

For half-moon dataset, we apply PGD$^{20}$ (step number $m=20$) attack with $\alpha=0.01$ and $\epsilon=0.2$ under $l^{\infty}$ norm. For CIFAR-10 and SVHN, we apply PGD$^{20}$ attack with $\alpha=2/255$ and $\epsilon=8/255$. For Fashion-MNIST, we apply PGD$^{20}$ attack with $\alpha=0.01$ and $\epsilon=0.1$. AutoAttack is parameter-free and thus we only set $\epsilon$ as same as that of PGD attack.

%For the Half-moon dataset, we consider three indicators to evaluate the performance of the model: natural testing accuracy ($\mathcal A^{test}_{nat}$), robust testing accuracy ($\mathcal A^{test}_{rob}$), and the gap between training accuracy and testing accuracy($\mathcal A^{train}_{nat}-\mathcal A^{test}_{nat}$). The natural accuracy and robust accuracy are measured on original and adversarial examples,
%respectively. The robust accuracy is used to evaluate the robustness of the models, and the difference between training and testing accuracy is used to evaluate the overfitting risk of the models. For the CIFAR-10 and CIFAR-100 datasets, we consider $\mathcal A^{test}_{nat}$ and $\mathcal A^{test}_{rob}$, where the robust accuracy is measured on adversarial images generated by using $\ell^{\infty}$ and $\ell^2$ PGD attack.

\subsection{Experiments on synthetic dataset}

In this subsection, we numerically verify the efficacy of our training method in improving the robustness of ResNet on the half-moon dataset.

% Figure environment removed   

We first vary hyperparameters $\lambda$ and $\sigma^2$ and present the classification accuracy on adversarial examples in \Cref{fig:synthetic}. The robust accuracy increase when $\lambda$ and $\sigma^2$ increase, which indicates that the introduced regularizer is helpful for improving the performance of ResNet. Moreover, we plot the decision boundary of naturally trained ResNet and ResNet trained with different $\lambda,\sigma^2$ in \Cref{fig:decision_boundary}. We can obvserve that the decision boundary of natural training is irregular, while that of our models is smoother. These experimental results are consistent with our theory.

% Figure environment removed

%final results in Figure \ref{moonresult}. The three indicators (natural accuracy, robust accuracy, and difference between training accuracy and testing accuracy) for ResNet with natural training were $88.6\%$, $62.3\%$, $10.4\%$ and for EnResNet (the standard deviation of
%Gaussian noise injected is $0.3$ and the number of average times in both training and testing phase are $3$) with natural training are $89.1\%$, $68\%$, $5.6\%$. Comparing with naturally trained ResNet, the robustness and generalizability of ResNet trained with our method were improved. And when $\lambda$ and $\sigma^2$ increased, the natural and robust accuracy increased, while the gap between training and testing accuracy decreased. Thus we can conclude that increasing $\lambda$ and $\sigma^2$ are helpful for improving the performance of the models. 

\subsection{Experiment on benchmarks}
\begin{table}[hbtp]
    \centering
    \caption{Natural accuracy and robust accuracy under PGD$^{20}$ and AutoAttack on different datasets (\%).}
    \label{tab:main}
    \begin{tabular}{ccccc}
    \toprule
    Dataset & Methods & Natural & PGD$^{20}$ & AutoAttack\\
    \midrule
    \multirow{2}{*}{CIFAR-10} & ResNet18 & 95.18 & 0.0 & 0.0 \\
    & Ours & 88.54 & 20.02 & 17.64  \\
    \midrule
    \multirow{2}{*}{SVHN} & ResNet18 & 96.58 & 0.40 & 0.02 \\
    & Ours & 94.16 & 19.05 & 15.48 \\
    \midrule
    \multirow{2}{*}{Fashion-MNIST} & ResNet18 & 93.95 & 0.0 & 0.0 \\
    & Ours & 92.01 & 35.27 & 23.48 \\
    \bottomrule
    \end{tabular}
\end{table}

We further test the performance of our method on CIFAR-10, SVHN and Fashion-MNIST datasets. We choose ResNet18 as the backbone model, where our method will add an additional channel in the input convolutional layer. During training, we apply standard data augmentation techniques including random crops and horizontal flips. The batch size is 128. We run 200 epochs with initial learning rate of 0.1, which decays by a factor of 10 at the 80th, 120th and 160th epochs. We use stochastic gradient descent optimizer with momentum of 0.9 and weight decay of $5\times 10^{-4}$.

\begin{table}[hbtp]
    \centering
    \caption{Natural accuracy and robust accuracy under PGD$^{20}$ and AutoAttack on CIFAR-10, with varying parameter $\lambda$ and $\sigma^2$ (\%).}
    \label{tab:ablation}
    \begin{tabular}{ccccc}
    \toprule
    $\lambda$  &  $\sigma^2$ & Natural & PGD$^{20}$ & AutoAttack \\
    \midrule
    0.2	    &	0.2	&   80.33	& 24.91	& 23.89 \\
    0.1	    &	0.2	&   82.64	& 24.04	& 22.97 \\
    0.05	&	0.2	&   84.51	& 23.58	& 22.53 \\
    0.01	&	0.2	&   87.45	& 20.63	& 18.37 \\
    0.005	&	0.2	&   88.54	& 20.02	& 17.64 \\
    \midrule
    0.2	    &   0.1	&   81.79	& 20.56	& 19.74 \\
    0.1	    &   0.1	&   84.73	& 20.02	& 18.87 \\
    0.05	&	0.1	&   85.95	& 19.07	& 17.76 \\
    0.01	&	0.1	&   89.52	& 18.60	& 14.12 \\
    0.005	&	0.1	&   89.95	& 15.22	& 11.11 \\
    \bottomrule
    \end{tabular}
\end{table}
There are several hyperparameters in our algorithm that need to be determined. $h$ and $\tau$ are spatial and temporal discretization parameters. We choose $\tau=10^{-4}$ and $h=0.1$. We choose a fairly large $h$ because neural networks are highly unstable on the spatial dimension and cannot converge when $h$ is too small. To reduce the computation cost, we choose $K=1$ when computing the Laplacian $\Delta$, and only consider starting and final time step $t_1=0, t_2=1$ when computing the regularization loss. The uniform noise $\epsilon$ which we add to training set $\mathcal{S}$ to fit the underlying domain $\mathcal{D}$ is chosen the same as the attack range of each dataset, i.e. $8/255$ for CIFAR-10 and SVHN, and $0.1$ for Fashion-MNIST. $\lambda$ and $\sigma^2$ both affect the trade-off between natural accuracy and robust accuracy. With larger $\lambda$ or $\sigma^2$, the natural accuracy decreases, while robust accuracy increases. We report the result of $\lambda=0.005$ and $\sigma^2=0.2$ in \cref{tab:main}, and an ablation study on the two parameters can be found in \cref{tab:ablation}. From \cref{tab:main}, we can see that our method, which introduces a regularization term based on convection-diffusion differential equation, raises the classification accuracy of ResNet against adversarial samples by a large margin. We want to emphasize that our method does not include any adversarial training techniques, which trains the model using adversarial samples crafted by PGD attacks. From \cref{tab:ablation}, we observe that with fixed parameter $\sigma^2$, the natural accuracy decreases and robust accuracy increases as the increasing of parameters $\lambda$ and vice versa.

% In order to demonstrate the influence of the two hyperparameters, we train ResNet by using our algorithm with different $\lambda,\sigma^2$, and list the result in \Cref{tab:main}. On one hand, we can see with fixed parameter $\sigma^2$, the natural accuracy decreases and robust accuracy increases as the increasing of parameters $\lambda$ and vice versa. On the other hand, comparing to the result of ResNet20 and ResNet56, we see that the performance of our proposed method is better in deeper networks.

% In order to quantitatively measure the performance of our proposed method, we compare the proposed method with natural trained ResNet and ResNet Ensemble method(EnResNet) \cite{wang2020enresnet}. In fairness, both the proposed method and EnResNet are trained without adversarial training. The activation function used for natural trained ResNet, EnResNet and the proposed method is ELU activation \cite{clevert2015fast}. The standard deviation of Gaussian noise injected to each residual mapping equals to $0.5$ for EnResNet. And the number of average times in training phase and testing phase are 1 and 20 respectively for EnResNet. The result of the comparison is shown in \cref{tab:ablation}. The experiment result shows that our method is more resistant to adversarial attack than other methods.

% \begin{table*}[t!]
% \caption{\small Natural and robust accuracies of ResNets trained using our method with different hyperparameters on the CIFAR-10/CIFAR-100.}
% \label{cifar}
% \vskip 0.15in
% \centering
% \begin{sc}
% \begin{tabular}{c|c|c|c|c|ccc}
% \hline 
% \multirow{2}{*}{Models} &
% \multirow{2}{*}{Hyperparameters} &
% %\multirow{2}{*}{Dataset} &
% %\multirow{2}{*}{Under which attack} &
% \multirow{2}{*}{$\mathcal A^{test}_{nat}$} &
% \multicolumn{3}{|c}{$\mathcal A^{test}_{rob}$} \\
% \cline{4-6}  
%  &  &  & $\epsilon=4/255\,(l^{\infty})$  & $\epsilon=8/255\,(l^{\infty})$  & $\epsilon=0.5\,(l^{2})$\\
% \hline
% \multicolumn{6}{c}{CIFAR-10 Dataset} \\
% \cline{1-6}  
% & $\lambda=0.01,\sigma^2=1$ & 86.72\% & 40.13\% & 7.8\% & 45.33\%\\
% ResNet20& $\lambda=0.05,\sigma^2=1$ & 84.78\% & 45.25\% & 13.03\% & 50.57\%\\
% & $\lambda=0.05,\sigma^2=2$ & 82.49\% & 48.29\% & 17.79\% & 53.87\%\\
% \hline
% & $\lambda=0.01,\sigma^2=1$ & 87.76\% & 41.85\% & 8.1\% & 46.17\%\\
% ResNet56& $\lambda=0.05,\sigma^2=1$ & 86.01\% & 47.42\% & 14.73\% & 53.05\%\\
% & $\lambda=0.05,\sigma^2=2$ & 83.03\% & 48.73\% & 18.02\% & 55\%\\
% \hline
% \multicolumn{6}{c}{CIFAR-100 Dataset} \\
% \cline{1-6}
% & $\lambda=0.01,\sigma^2=1$ & 58.77\% & 19.56\% & 3.91\% & 23.98\%\\
% ResNet20& $\lambda=0.05,\sigma^2=1$ & 55.7\% & 21.79\% & 5.56\% & 26.55\%\\
% & $\lambda=0.05,\sigma^2=2$ & 52.73\% & 23.33\% & 8.27\% & 28.42\%\\
% \hline
% & $\lambda=0.01,\sigma^2=1$ & 60.2\% & 19.75\% & 4.49\% & 24.93\%\\
% ResNet56& $\lambda=0.05,\sigma^2=1$ & 57.82\% & 23.26\% & 6.8\% & 27.82\%\\
% & $\lambda=0.05,\sigma^2=2$ & 53.99\% & 25.19\% & 9.41\% & 29.55\%\\
% \hline
% \end{tabular}
% \end{sc}
% \vskip -0.1in
% \end{table*}

% \begin{table*}[t!]
% \caption{\small Comparing the robustness against $\ell^{\infty}$-norm and $\ell^{2}$-norm constrained adversarial perturbations on CIFAR-10.}
% \label{comparison}
% \vskip 0.15in
% \centering
% \begin{sc}
% \begin{tabular}{c|c|c|c|ccc}
% \hline
% \multirow{2}{*}{Models} &
% \multirow{2}{*}{$\mathcal A^{test}_{nat}$\ } &
% \multicolumn{3}{|c}{ $\mathcal A^{test}_{rob}$} \\
% \cline{3-5}  
%  &  &  $\epsilon=4/255\,(l^{\infty})$  &  $\epsilon=8/255\,(l^{\infty})$ &  $\epsilon=0.5\,(l^{2})$ \\
% \hline\hline
% ResNet110 & \textbf{93.58\%} & 0\% & 0\% & 0.01\% \\
% EnResNet110 & 89.9\% & 22.81\% & 0\% & 0.1\% \\
% %Randomized Smoothing & 69.1\% & 0\% & 0\% & 45.32\% \\
% Our ResNet110 $\lambda=0.01,\sigma^2=1$ & 88.42\% & 42.13\% & 9.34\% & 46.3\%\\
% Our ResNet110 $\lambda=0.05,\sigma^2=1$ & 86.35\% & 47.47\% & 15.18\% & 54\%\\
% Our ResNet110 $\lambda=0.05,\sigma^2=2$ & 84.35\% & \textbf{50.76}\% & \textbf{19.4}\% & \textbf{56.38}\%\\
% \hline
% \end{tabular}
% \end{sc}
% \end{table*}