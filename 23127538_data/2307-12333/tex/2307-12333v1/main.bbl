\begin{thebibliography}{10}

\bibitem{alvarez1993axioms}
{\sc L.~Alvarez, F.~Guichard, P.-L. Lions, and J.-M. Morel}, {\em Axioms and
  fundamental equations of image processing}, Archive for rational mechanics
  and analysis, 123 (1993), pp.~199--257.

\bibitem{avelin2021neural}
{\sc B.~Avelin and K.~Nystr{\"o}m}, {\em Neural odes as the deep limit of
  resnets with constant weights}, Analysis and Applications, 19 (2021),
  pp.~397--437.

\bibitem{5995719}
{\sc L.~Bo, K.~Lai, X.~Ren, and D.~Fox}, {\em Object recognition with
  hierarchical kernel descriptors}, in CVPR 2011, 2011, pp.~1729--1736,
  \url{https://doi.org/10.1109/CVPR.2011.5995719}.

\bibitem{chen2018neural}
{\sc R.~T.~Q. Chen, Y.~Rubanova, J.~Bettencourt, and D.~Duvenaud}, {\em Neural
  ordinary differential equations}, Advances in Neural Information Processing
  Systems,  (2018).

\bibitem{cohen2019certified}
{\sc J.~Cohen, E.~Rosenfeld, and Z.~Kolter}, {\em Certified adversarial
  robustness via randomized smoothing}, in International Conference on Machine
  Learning, PMLR, 2019, pp.~1310--1320.

\bibitem{croce2020reliable}
{\sc F.~Croce and M.~Hein}, {\em Reliable evaluation of adversarial robustness
  with an ensemble of diverse parameter-free attacks}, in ICML, 2020.

\bibitem{5740583}
{\sc G.~E. Dahl, D.~Yu, L.~Deng, and A.~Acero}, {\em Context-dependent
  pre-trained deep neural networks for large-vocabulary speech recognition},
  IEEE Transactions on Audio, Speech, and Language Processing, 20 (2012),
  pp.~30--42, \url{https://doi.org/10.1109/TASL.2011.2134090}.

\bibitem{weinan2017proposal}
{\sc W.~E}, {\em A proposal on machine learning via dynamical systems},
  Communications in Mathematics and Statistics, 5 (2017), pp.~1--11.

\bibitem{gastaldi2017shake}
{\sc X.~Gastaldi}, {\em Shake-shake regularization}, arXiv preprint
  arXiv:1705.07485,  (2017).

\bibitem{goodfellow2014explaining}
{\sc I.~J. Goodfellow, J.~Shlens, and C.~Szegedy}, {\em Explaining and
  harnessing adversarial examples}, arXiv preprint arXiv:1412.6572,  (2014).

\bibitem{haber2018learning}
{\sc E.~Haber, L.~Ruthotto, E.~Holtham, and S.-H. Jun}, {\em Learning across
  scales---multiscale methods for convolution neural networks}, in
  Thirty-Second AAAI Conference on Artificial Intelligence, 2018.

\bibitem{He_2016_CVPR}
{\sc K.~He, X.~Zhang, S.~Ren, and J.~Sun}, {\em Deep residual learning for
  image recognition}, in Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR), June 2016.

\bibitem{he2016identity}
{\sc K.~He, X.~Zhang, S.~Ren, and J.~Sun}, {\em Identity mappings in deep
  residual networks}, in European conference on computer vision, Springer,
  2016, pp.~630--645.

\bibitem{huang2018stochastic}
{\sc C.-W. Huang and S.~S. Narayanan}, {\em Stochastic shake-shake
  regularization for affective learning from speech.}, in INTERSPEECH, 2018,
  pp.~3658--3662.

\bibitem{huang2017densely}
{\sc G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger}, {\em Densely
  connected convolutional networks}, in Proceedings of the IEEE conference on
  computer vision and pattern recognition, 2017, pp.~4700--4708.

\bibitem{huang2016deep}
{\sc G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger}, {\em Deep
  networks with stochastic depth}, in European conference on computer vision,
  Springer, 2016, pp.~646--661.

\bibitem{jha2021smoother}
{\sc S.~Jha, R.~Ewetz, A.~Velasquez, S.~Jha, and Z.-H. Zhou}, {\em On smoother
  attributions using neural stochastic differential equations}, in Proceedings
  of the Thirtieth International Joint Conference on Artificial Intelligence,
  IJCAI-21, 2021, pp.~522--528.

\bibitem{jia2019neural}
{\sc J.~Jia and A.~R. Benson}, {\em Neural jump stochastic differential
  equations}, Advances in Neural Information Processing Systems, 32 (2019),
  pp.~9847--9858.

\bibitem{krizhevsky2009learning}
{\sc A.~Krizhevsky and G.~Hinton}, {\em Learning multiple layers of features
  from tiny images}, Master's thesis, Department of Computer Science,
  University of Toronto,  (2009).

\bibitem{kurakin2016adversarial}
{\sc A.~Kurakin, I.~Goodfellow, and S.~Bengio}, {\em Adversarial examples in
  the physical world}, arXiv preprint arXiv:1607.02533,  (2016).

\bibitem{larsson2016fractalnet}
{\sc G.~Larsson, M.~Maire, and G.~Shakhnarovich}, {\em Fractalnet: Ultra-deep
  neural networks without residuals}, arXiv preprint arXiv:1605.07648,  (2016).

\bibitem{ledoux2013probability}
{\sc M.~Ledoux and M.~Talagrand}, {\em Probability in Banach Spaces:
  isoperimetry and processes}, Springer Science \& Business Media, 2013.

\bibitem{li2019certified}
{\sc B.~Li, C.~Chen, W.~Wang, and L.~Carin}, {\em Certified adversarial
  robustness with additive noise}, in Advances in Neural Information Processing
  Systems, Neural information processing systems foundation, 2019.

\bibitem{liu2020does}
{\sc X.~Liu, S.~Si, Q.~Cao, S.~Kumar, and C.-J. Hsieh}, {\em How does noise
  help robustness? explanation and exploration under the neural sde framework},
  in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition, 2020, pp.~282--290.

\bibitem{lu2018beyond}
{\sc Y.~Lu, A.~Zhong, Q.~Li, and B.~Dong}, {\em Beyond finite layer neural
  networks: Bridging deep architectures and numerical differential equations},
  in International Conference on Machine Learning, PMLR, 2018, pp.~3276--3285.

\bibitem{mao2007stochastic}
{\sc X.~Mao}, {\em Stochastic differential equations and applications},
  Elsevier, 2007.

\bibitem{martens2012estimating}
{\sc J.~Martens, I.~Sutskever, and K.~Swersky}, {\em Estimating the hessian by
  back-propagating curvature}, arXiv preprint arXiv:1206.6464,  (2012).

\bibitem{netzer2011reading}
{\sc Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng}, {\em
  Reading digits in natural images with unsupervised feature learning}, in NIPS
  Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011,
  \url{http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf}.

\bibitem{oksendal2013stochastic}
{\sc B.~Oksendal}, {\em Stochastic differential equations: an introduction with
  applications}, Springer Science \& Business Media, 2013.

\bibitem{raissi2019physics}
{\sc M.~Raissi, P.~Perdikaris, and G.~E. Karniadakis}, {\em Physics-informed
  neural networks: A deep learning framework for solving forward and inverse
  problems involving nonlinear partial differential equations}, Journal of
  Computational physics, 378 (2019), pp.~686--707.

\bibitem{salman2019provably}
{\sc H.~Salman, G.~Yang, J.~Li, P.~Zhang, H.~Zhang, I.~Razenshteyn, and
  S.~Bubeck}, {\em Provably robust deep learning via adversarially trained
  smoothed classifiers}, in Proceedings of the 33rd International Conference on
  Neural Information Processing Systems, 2019, pp.~11292--11303.

\bibitem{lecture_upper}
{\sc A.~T. Sham~Kakade}, {\em Lecture in learning theory: Covering numbers}.
\newblock \url{https://home.ttic.edu/~tewari/lectures/lecture14.pdf}, 2008.

\bibitem{simonyan2015deep}
{\sc K.~Simonyan and A.~Zisserman}, {\em Very deep convolutional networks for
  large-scale image recognition}, 2015, \url{https://arxiv.org/abs/1409.1556}.

\bibitem{sonoda2019transport}
{\sc S.~Sonoda and N.~Murata}, {\em Transport analysis of infinitely deep
  neural network}, The Journal of Machine Learning Research, 20 (2019),
  pp.~31--82.

\bibitem{srivastava2014dropout}
{\sc N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and
  R.~Salakhutdinov}, {\em Dropout: a simple way to prevent neural networks from
  overfitting}, The journal of machine learning research, 15 (2014),
  pp.~1929--1958.

\bibitem{sun2018stochastic}
{\sc Q.~Sun, Y.~Tao, and Q.~Du}, {\em Stochastic training of residual networks:
  a differential equation viewpoint}, arXiv preprint arXiv:1812.00174,  (2018).

\bibitem{thorpe2018deep}
{\sc M.~Thorpe and Y.~van Gennip}, {\em Deep limits of residual neural
  networks}, arXiv preprint arXiv:1810.11741,  (2018).

\bibitem{wang2020enresnet}
{\sc B.~Wang, B.~Yuan, Z.~Shi, and S.~J. Osher}, {\em Enresnet: Resnets
  ensemble via the feynman--kac formalism for adversarial defense and beyond},
  SIAM Journal on Mathematics of Data Science, 2 (2020), pp.~559--582.

\bibitem{10.1007/978-3-319-46484-8_2}
{\sc L.~Wang, Y.~Xiong, Z.~Wang, Y.~Qiao, D.~Lin, X.~Tang, and L.~Van~Gool},
  {\em Temporal segment networks: Towards good practices for deep action
  recognition}, in Computer Vision -- ECCV 2016, B.~Leibe, J.~Matas, N.~Sebe,
  and M.~Welling, eds., Cham, 2016, Springer International Publishing,
  pp.~20--36.

\bibitem{xiao2017fashion}
{\sc H.~Xiao, K.~Rasul, and R.~Vollgraf}, {\em Fashion-mnist: a novel image
  dataset for benchmarking machine learning algorithms}, arXiv preprint
  arXiv:1708.07747,  (2017).

\bibitem{xie2017aggregated}
{\sc S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He}, {\em Aggregated
  residual transformations for deep neural networks}, in Proceedings of the
  IEEE conference on computer vision and pattern recognition, 2017,
  pp.~1492--1500.

\bibitem{lecture_unit}
{\sc Y.~Y. Yihong~Wu}, {\em Lecture in information-theoretic methods in
  high-dimensional statistics: Packing, covering, and consequences on minimax
  risk}.
\newblock \url{http://www.stat.yale.edu/~yw562/teaching/598/lec14.pdf}, 2016.

\bibitem{zagoruyko2016wide}
{\sc S.~Zagoruyko and N.~Komodakis}, {\em Wide residual networks}, in British
  Machine Vision Conference 2016, British Machine Vision Association, 2016.

\bibitem{zhang2019approximation}
{\sc H.~Zhang, X.~Gao, J.~Unterman, and T.~Arodz}, {\em Approximation
  capabilities of neural ordinary differential equations}, arXiv preprint
  arXiv:1907.12998, 2 (2019), pp.~3--1.

\bibitem{zhang2017polynet}
{\sc X.~Zhang, Z.~Li, C.~Change~Loy, and D.~Lin}, {\em Polynet: A pursuit of
  structural diversity in very deep networks}, in Proceedings of the IEEE
  Conference on Computer Vision and Pattern Recognition, 2017, pp.~718--726.

\end{thebibliography}
