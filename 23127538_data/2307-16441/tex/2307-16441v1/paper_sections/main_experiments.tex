\section{Experiments}
\label{sec_experiments}

In this section, we perform an experimental evaluation of the proposed method for INP. Sec.~\ref{sec:dataset} describes the adopted datasets, Sec.~\ref{sec:metrics} describes the adopted metrics, Sec.~\ref{sec:ablation} shows ablation results on our method, Sec.~\ref{sec:baselines} performs a quantitative comparison against baselines and Sec.~\ref{sec:qualitatives} shows qualitative results.

\subsection{Datasets}
\label{sec:dataset}

To train our architecture, we assume a dataset of images with an associated sequence of stroke parameters, representing the painting process used to produce the corresponding painting. To produce realistic stroke suggestions, our model captures the characteristics of the painting process represented in the dataset.
To overcome the cost associated with collecting human painting demonstrations, \elia{we follow recent work of \cite{cheng2022adaptively, singh2022paint2pix} and} choose to demonstrate that our framework is capable of modeling a painting process considering a synthetic dataset of stroke sequences that mimic a human painting process. Importantly, our method is general and learns the characteristics of the strokes provided as a demonstration, thus can be readily applied to a human-collected dataset if available.

We consider two existing image datasets and associate a sequence of strokes to each image, producing our INP datasets:

\begin{itemize}
    \item \textit{ADE 20K Outdoor INP}: we employ a subset of 5000 images of the ADE 20K dataset (\cite{zhou2017ade20k}) consisting of the set of original images depicting outdoor scenes. We split the dataset into a set of 4750 training images and 250 images for evaluation. 
    \item \textit{Oxford-IIIT Pet INP}: the dataset consists of 7349 images depicting different cat and dog breeds from \textit{Oxford-IIIT Pet} (\cite{parkhi2012cats}), both in indoor and outdoor scenarios. The dataset is split into 6980 training images and 369 images for evaluation.

\end{itemize}

Each image is decomposed into a sequence of strokes, parameterized as Sec.~\ref{sec:method}, using the NP method of \cite{zou2021stylized}. \new{Similar to the human painting process, the obtained sequence is organized in different levels of detail, with large strokes depicting the outline of the image first and fine-grained detail later in the sequence.} \st{where each stroke $s$ is translated to the RGB space using a pretrained differentiable renderer.} However, such sequences of strokes do not contain the sequential patterns typically produced by humans. Painters, in fact, due to the constraints imposed by physical brushes which discourage changes in color and brush, tend to produce sequences of strokes where the same color and brush sizes are maintained across several subsequent strokes. In addition, it is common for humans to produce paintings on an object-by-object basis (\cite{Singh2021IntelliPaintTD,zhao2020painting}) and to produce strokes in contiguous regions. To replicate these patterns \new{in our synthetic dataset and produce sequences with characteristics closer to real ones}, we perform a reordering of the stroke sequence produced by \cite{zou2021stylized} by optimizing a cost function that penalizes sequences with large differences in size, position or color between adjacent strokes or where adjacent strokes are placed on different subjects. Specifically, we  perform a reordering of the sequences by minimizing the following cost function, computed along the complete stroke sequence:
 
 \begin{align}
\mathrm{cost} &= \sum_{t=2}^{T} (\lambda^{\mathrm{ord}}_x \left\|x^{t} - x^{t-1}\right\|_2^2 + \lambda^{\mathrm{ord}}_\rho \left\|\rho^{t} - \rho^{t-1}\right\|_2^2\notag\\ &+ \lambda^{\mathrm{ord}}_\sigma(\sigma^{t} - \sigma^{t-1})^2 + \lambda_{\mathrm{obj}} \chi(x^t, x^{t-1}))
\end{align}

where $\lambda^{\mathrm{ord}}_x$, $\lambda^{\mathrm{ord}}_\rho$, $\lambda^{\mathrm{ord}}_\sigma$, $\lambda_{\mathrm{obj}}$ are positive weighting parameters. The function $\chi(x^t, x^{t-1})$ is equal to 1 if the input strokes are located on different subjects and 0 otherwise, and it is computed using the dataset segmentation masks.
\st{Importantly,}We ensure that the ordering relation between overlapping strokes is preserved, guaranteeing that both the original and reordered sequences of strokes produce the same visual output when rendered. Such a problem is an instance of the Sequential Ordering Problem which we optimize following \cite{helsgaun2017lkh3}. 
\input{tables/ablation_losses_complete}
\input{tables/ablation_architecture_complete}
\input{tables/baselines}

\subsection{Evaluation Metrics}
\label{sec:metrics}
\new{As outlined in Sec.~\ref{sec:problem_formulation}, the key three factors that we expect in the INP setting are:}
\st{To evaluate the effectiveness of our approach on the novel INP task, i.e. if it proposes diverse and plausible stroke continuations, we devise a set of metrics that capture the following key factors:} (i) the method is painting the reference image, (ii) the produced strokes parameters have characteristics that match the ones of the stroke dataset, (iii) diverse stroke continuations can be produced for the same context. We devise a set of quantitative metrics to capture these desiderata, and describe them in the following:
\st{In practice, we evaluate the method with the following quantitative metrics:}

\begin{itemize}
    \item Stroke Color L2 (L2) (i): we measure the L2 difference between the color of the predicted strokes and the color of the underlying reference image region. To avoid big strokes from dominating the metric, the L2 distance associated with each stroke is normalized by the stroke area before averaging. 
    \item Fr\'echet Stroke Distance (FSD) (ii): inspired by FID \cite{heusel2017advances}, we introduce a metric measuring the similarity between ground truth and predicted stroke sequences. For each sequence of context and target strokes, we compute stroke features $\psi$ as in Eq. \eqref{eq:stroke_features}. We report the Frechet Distance (\cite{frechet1957distance}) between the distribution of features derived from ground truth sequences and predicted ones.
    \item Fr\'echet Video Distance (FVD) (\cite{unterthiner2018towards}) (ii): given a sequence of context and target strokes, we generate videos of the corresponding canvas rendered up to each stroke in the sequence. We use FVD between videos produced with ground truth and predicted strokes as a metric for capturing the similarity between sequences.
    \item Wasserstein Distance (WD) (\cite{lantorovich1939wasserstein}) (ii): following \cite{liu2021painttransformer}, we adopt the Wasserstein Distance between Gaussian distributions fitted on the ground truth and predicted strokes as a stroke reconstruction quality metric. 
    \item Dynamic Time Warping (DTW) (\cite{muller2007dynamictimewarping}) (ii): we employ DTW between the ground truth and inferred target strokes to measure the quality of matching.
    \item LPIPS (\cite{zhang2018unreasonable}) (iii): following \cite{zhu2017toward}, we use LPIPS as a metric to compute the diversity of the produced outputs. For each reference image and context, we produce 5 stroke predictions and measure the average LPIPS diversity between all pairs of rendered results.
\end{itemize}

For each image in the test set, we extract 5 sequences of corresponding context and target strokes and compute the metrics on these samples. We note that WD and DTW require paired sequences of ground truth and reconstructed target sequences, while at inference time our method generates plausible stroke sequences that may not match the ground truth. \willi{For these metrics, we adopt a top-1 sampling strategy (\cite{yu2021diverse}) and generate 100 plausible stroke sequences, reporting the metric obtained for the best one.}

\subsection{Ablation Study}
\label{sec:ablation}
In this section, we ablate the main losses and architectural components of the proposed method. To improve the number of analyzed model configurations, in this section we reduce the number of training epochs to 1000.  
We start our analysis by ablating the contribution of the proposed losses (see Tab.~\ref{tab:ablation_losses}). 
Training the model only with the $\beta\mhyphen$VAE loss produces stroke outputs with high diversity but whose Stroke Color L2 is the highest in all the configurations, suggesting that the model is predicting strokes that are not consistent with the reference image.
Introducing $\mathcal{L}_{\mathrm{col}}$ promotes the model to take $I_\mathrm{ref}$ into account, resulting in a consistent reduction of the Stroke Color L2.
To further improve the performance, we introduce our two training regularization losses, which are aimed at improving quality when the latent code $z$ is sampled from the prior distribution at inference time.
Introducing $\mathcal{L}^{reg}_{\mathrm{col}}$ improves color accuracy as demonstrated by the best Stroke Color L2, but prevents the model from learning the users' painting style resulting in the highest FSD. 
Vice-versa, with only our proposed $\mathcal{L}_{\mathrm{dist}}^{reg}$ we can effectively learn the distribution of strokes, achieving the best performances in terms of FSD, WD, and LPIPS, but performance decreases in terms of the Stroke Color L2.
Only when combining all the proposed losses in our full model we obtain good performance under \textit{all} metrics. \\

Next, to ablate the contribution of each proposed architectural component, we produce the following modified versions of our method: (i) remove the context information provided by $s_c$ and $I_c$, the only context information comes from the reference image $I_{\mathrm{ref}}$; (ii) remove $C_e$; (iii) remove the two-step decoding procedure and replace it with a single transformer decoder that directly predicts $\hat{s}_t$; 
We show the ablation results in Tab.~\ref{tab:ablation_architecture}.
As expected, removing $s_c$ and $I_c$ from the context information significantly degrades the performance. Interestingly, LPIPS is the highest among the configurations, probably because, without conditioning from the context, the predictions can vary more freely. Likewise, removing the transformer encoder block $C_e$ consistently reduces the metrics, showing the importance of the module that provides richer context information to the decoder $D$ by combining visual and strokes features.
Configuration (iii) shows the impact of the two-step decoding procedure, designed to provide the model with richer visual information. This version of the method results in a degraded Stroke Color L2, FSD and FVD, showing the importance of detailed visual features in the prediction of strokes. 

\subsection{Comparison against Baselines}
\label{sec:baselines}
In this section, we compare our method against the state-of-the-art NP methods of \cite{zou2021stylized} and of \cite{liu2021painttransformer}. Due to the novelty of the task, these works are not directly comparable with the proposed one since they do not consider interaction, and need to be adapted to the INP setting.
A key component of the selected methods that makes them unsuitable for INP is their hierarchical rendering pipeline that iteratively divides the reference image and the canvas into smaller regions and operates on each in separation. This procedure allows the models to progressively focus on finer details and accurately reconstruct the reference image, but produces poor performance in INP since at each iteration the method may be forced to output strokes in a region far from the area the user is painting.
To avoid this limitation and make the model aware of the context, instead of operating on hardcoded regions, at each iteration we consider as the current region a portion of the image centered on the last context stroke. We call such a region the context region.
In this way, we encourage the method to output a sequence of strokes in the neighborhood of the context. In addition, we make the size of the region proportional to the area of the context strokes $s_c$, fostering the models to produce strokes with a level of detail compatible with the one currently adopted by the user.
We apply this modification to both methods and produce the following baselines: 

\begin{itemize}
    \item Paint Transformer (PT) \cite{liu2021painttransformer}: the model makes a prediction in the context region, but no explicit knowledge about the distribution of the dataset stroke sequences can be leveraged.
    \item Stylized Neural Painting (SNP) \cite{zou2021stylized}: we randomly initialize the sequence of predicted strokes to lie in the context region and optimize their parameters with the original SNP objective. Note that the method is not expressly conditioned on the context strokes and does not consider the characteristics of the dataset stroke sequences.
    \item Stylized Neural Painting+ (SNP+) \cite{zou2021stylized}: we modify SNP to explicitly take into consideration the characteristics of the dataset stroke sequence. In detail, we modify the SNP optimization objective by introducing a term similar to the distribution matching loss $\mathcal{L}_{\mathrm{dist}}^{\mathrm{reg}}$ to produce stroke sequences whose features $\hat{\psi}$ match those of the training dataset $\psi$. We extract stroke features from the dataset using Eq. \eqref{eq:stroke_features} and fit a multivariate Gaussian distribution with independent components on $\psi$. We improve the realism of inferred strokes by maximizing the likelihood that the inferred features $\hat{\psi}$ match the fitted distribution.
\end{itemize}

Tab.~\ref{tab:baseline} shows comparison results of our method against the baselines. To ensure a fair comparison, we follow \cite{liu2021painttransformer} and set the length of the predicted sequence to $k\!=\!8$ for all the methods.
While baseline methods are designed with the main objective of producing strokes that closely match the reference image, we notice that our method presents a Stroke Color L2 metric similar to SNP and SNP+, and lower with respect to PT.
Moreover, Paint Transformer and Stylized Neural Painting have, by design, no way to leverage information about the characteristics of the dataset strokes and tend to produce stroke sequences whose characteristics do not match the ones in the dataset. This is highly reflected in the metrics that capture the ability to paint like the demonstration; our method strongly outperforms PT and SNP in terms of the WD, DTW, FVD, and FSD. On the other side, SNP+ can exploit such information. As expected, this greatly reduces the FSD compared to the naive SNP, but comes at the cost of increasing the Stroke Color L2. Interestingly, our method can outperform SNP+ in this metric, suggesting that the better performance of our model is due not only to distribution matching objectives but also to the architecture design.
Finally, we evaluate the capacity of the method to produce varied plausible outputs for a fixed context. We observe that, while PT is deterministic and no variability can be produced, diverse predictions for a given context can be obtained for SNP and SNP+ by starting the optimization from different randomly initialized stroke parameters. Our method instead is probabilistic by nature, with a conditional VAE designed to generate different plausible continuations for a fixed context. Despite the non-determinism of some baselines, the diversity of their predictions is inferior to the ones obtained with our method, which achieves the highest LPIPS diversity score.  

\paragraph{\new{User Study}} We complement our quantitative results with a user study. We show the users a set of reference videos with rendered stroke sequences from the dataset followed by two videos, one with rendered strokes produced by one of the baselines, and one produced by \methodname. \new{To ease the evaluation, we produce sequences with a length of
24 strokes and render the obtained strokes in a short video.} We ask the users to express which of the two videos has strokes whose characteristics resemble the reference videos the most. We gather a total of 960 votes from 8 unique users. \new{We report the results in Tab.~\ref{tab:user_study},} \st{Users express a preference of 97.9\%, 97.1\%, and 95.0\% for our method respectively against PT, SNP, and SNP+,} showing a clear preference for \methodname~when evaluated on the INP task (see \emph{Supp. Mat.} for additional details).
\input{tables/user_study}

\subsection{Qualitative Results}

\input{tables/qualitative_baselines}
\input{tables/qualitative_heatmap}
\input{tables/qualitative_diversity}
\input{tables/qualitative_interpolation}

We provide qualitative results comparing our method with the baselines. \elia{Visualizing results as still images provide limited understanding of the INP task, we refer the reader to \emph{Supp.~Mat.}~for video results along with a working demo. When comparing different methods, the predicted stroke sequences should satisfy three criteria: (i) make the canvas more similar to the reference image, (ii) present similar characteristics as the demonstrations, (iii) provide diverse continuations (Sec.~\ref{sec:problem_formulation}, \ref{sec:metrics})}. \elia{While all the methods perform similarly when assessing (i), \methodname~strongly outperform
the competitors according to (ii) and (iii) which are peculiar to the \textit{interactive} component of the task.} \\


First, we analyze the ability of \methodname~to model the characteristics of the dataset strokes \elia{(ii)}. Given the reference image and the context fixed, we sample 100 possible stroke continuations. A method that correctly captures the original stroke distribution is expected to yield at least a stroke continuation close to the ground truth. In Fig.~\ref{fig:qualitative_baseline} we plot qualitative results showing the sampled sequence that better matches the ground truth in terms of L2 distance. In all examples, our method is able to produce a sequence close to the ground truth, while PT, SNP and SNP+ struggle to generate a matching sequence, indicating that our method is able to better capture the characteristics of the dataset stroke sequences. 
\elia{In Fig.~\ref{fig:qualitative_heatmap} we show the heatmap depicting the probability, obtained with the aforementioned sampling procedure, that a certain pixel will be covered by one of the next $k$ strokes. 
We notice that only \methodname~is able to produce different plausible suggestions (iii) while predicting strokes on the same object as the last context strokes (ii)}.
Second, we further evaluate the ability of \methodname~to predict different stroke continuations given a fixed context \elia{(iii)}.
Note that, as shown in Fig.~\ref{fig:teaser} (b), this is an important feature of an INP method since at each iteration of the painting process the method should be able to propose at least a stroke sequence that matches the user painting intentions.
In Fig.~\ref{fig:qualitative_diversity} we show different stroke suggestions for a fixed context obtained by sampling different latent codes $z$ from the unit normal prior distribution $\mathcal{N}(0, 1)$. Our method is capable of generating diverse stroke continuations, each of which focuses on a similar region, color and subject with respect to the given context. \elia{Finally, we show the structure of the learned latent representation (Fig. \ref{fig:interpolation}). Specifically, we sample two different latent codes from the prior distribution, $z_{\mathrm{start}}, z_{\mathrm{end}} \sim \mathcal{N}(0,1)$ and we linearly interpolate between the two samples, plotting the predicted results along the interpolation path (see \emph{Supp. Mat.} for video animation). 
It is possible to notice that the strokes smoothly transition between the two samples $z_{\mathrm{start}}, z_{\mathrm{end}}$, changing their position but focusing on the same subject suggesting that the learned latent space is well structured.}

\label{sec:qualitatives}