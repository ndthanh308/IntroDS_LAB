
In this appendix, we provide additional details about parametrization and rendering of the strokes, a description of the architecture and the losses, and user study details. 

This appendix is complemented by a supplementary website where we show additional qualitative results in the form of videos. In particular, we showcase a demo of \methodname~in an Interactive Neural Painting (INP) scenario, where a user interacts with our model to paint a reference image. \new{The material is available at the \href{https://helia95.github.io/inp-website}{\textcolor{magenta}{project page}}.}


\subsection*{Dataset}
\label{sec:supp_dataset}
Due to the cost associated with the acquisition of a real stroke dataset, we evaluate our model on two synthetic stroke datasets, built to mimic human painting style (see Sec.~4.1 of the main paper for a discussion). Here we detail the dataset acquisition procedure. Examples from the dataset are provided on the supplementary website. \\% (See \href{run:website/datasets.html}{datasets.html}).\\

\noindent \emph{Images.} We rely on two publicly available datasets, each containing images and associated segmentation masks.
The \textit{Oxford-IIIT Pet} dataset (\cite{parkhi2012cats}), contains 7349 images of cats and dogs of different species for a total of 37 different classes. 
The \textit{ADE 20K} dataset (\cite{zhou2017ade20k}) is a large-scale dataset containing $20,\!100$ images from more than $3K$ classes. The dataset is filtered to contain images of outdoor scenes, resulting in a subset of 5000 images. 
\\

\noindent \emph{Strokes Parametrization.} Following the work of \cite{zou2021stylized}, we parametrize the strokes as $s\!=\!(x, \rho, \sigma, \omega)$. The center of the stroke is represented by $x\!=\!(x_x, x_y)$. 
The height and width of the strokes are represented by $\sigma\!=\!(\sigma_h, \sigma_w)$, while $\omega$ represents the orientation, which is the counter-clock wise angle in the range $[0, \pi]$. 
Lastly, the color of the stroke is represented by $\rho\!=\!(\rho_r, \rho_g, \rho_b)$.
All the parameters are normalized to lie in the interval $[0, 1]$. \\

\noindent \emph{Decomposition.}~We make use of Stylized Neural Painting (SNP) \cite{zou2021stylized}, to extract a sequence of brushstrokes from a given image. We notice that SNP tends to produce very large strokes in the first iterations of the method, which cover a wide area of the canvas. This practice is unrealistic since the size limitations of physical brushstrokes would prevent a human painter from doing this. To circumvent such behavior, we clamp the parameter $\sigma$ to a maximum value of $0.4$.
As described in \cite{zou2021stylized}, we employ a progressive rendering pipeline with a total of 4 iterations, dividing the image in a grid with 4, 9, 16, and 25 regions. We allocate a different number of strokes during the progressive rendering process, respectively 30, 20, 15, and 10 to each region, which results in a total of 790 strokes per image. 
Lastly, SNP represents the color of each stroke using two triples of $(\mathrm{r}, \mathrm{g}, \mathrm{b})$ values that are interpolated to obtain a smooth color. For simplicity, we use the average of the two, and represent the stroke with a uniform color $\rho$. \\

\noindent \emph{Reordering.} We  perform a reordering of the sequences, by minimizing the cost function described in  Sec.\ref{sec:dataset} of the main paper. \\

\noindent \emph{Rendering.} To render the strokes on the canvas, we follow \cite{liu2021painttransformer} and use a \emph{parameter free} renderer. Starting from a primitive brushstroke, affine transformations are applied to obtain the foreground $I_{s^t}$ and the alpha matte $\alpha_{s^t}$ associated to $s^t\!=\!(x, \rho, \sigma, \omega)$. The canvas can be updated computing $I_c^{t}\!=\alpha_{s^t} \cdot I_{s^t} + (1 - \alpha_{s^t}) \cdot \!I_c^{t-1}$. We refer the reader to \cite{liu2021painttransformer} for additional details.

\subsection*{Method}
\label{sec:supp_method}
\noindent \emph{Architecture.} We report more details of the architecture, depicted in Fig. 2 of the main paper. 
Our model relies on the Transformer architecture of \cite{vaswani2017attention}, where we set the embedding dimensionality $\mathrm{d_{emb}}\!=\!256$, the number of heads in multi-head attention to 4, the dimension of the intermediate linear layer to 1024, and the dropout rate to 0.
The CNN encoder $F$ is composed of 4 convolutional blocks with residual connection and receives as input an image of size $256\times256$. The spatial resolution of the features is reduced by a factor of 2 in each block, resulting in a $16\times16$ output feature map. Following \cite{liu2021painttransformer}, we use two distinct image encoders for $I_\mathrm{ref}$ and $I_c$. The features obtained by the two input images are concatenated and projected to the embedding dimensionality $\mathrm{d_{emb}}$. %, we refer to these features as \emph{visual features}.
Similarly, the context strokes $s_c$ and the target strokes $s_t$ are projected to $\mathrm{d_{emb}}$ using a linear layer. 
The remaining components are implemented as standard transformers blocks. In particular, $C_e$ is transformer encoder with number of layers equal to 8, while $E$, $D_1$, $D_2$ are transformer decoders with number of layers equal to 6. \\


\noindent \emph{Losses.}
We provide additional details about the computation of the losses.
The reconstruction loss component of $\mathcal{L}_{\beta\mhyphen\mathrm{VAE}}$ is computed by weighting the reconstruction error differently for each component of the stroke parameter:

\begin{equation}
\begin{split}
    \left\|s_t - \hat{s}_t\right\|_2^2 & = \lambda_x \left\|x_t - \hat{x}_t\right\|_2^2 + \lambda_\rho \left\|\rho_t - \hat{\rho}_t\right\|_2^2 \\
     & + \lambda_\sigma \left\|\sigma_t - \hat{\sigma}_t\right\|_2^2 + \lambda_\omega \left\|\omega_t - \hat{\omega}_t\right\|_2^2
\end{split}
\end{equation}
with $\lambda_{x}\!=\!1$, $\lambda_{\rho}\!=\!2.5\mathrm{e}{-1}$, $\lambda_{\sigma}\!=\!1$ and $\lambda_{\omega} \!=\!1$. In early experiments, we noticed that the component corresponding to the predicted color $\rho$, \emph{i.e.} $\left\|\rho_t - \hat{\rho}_t\right\|_2^2$, was difficult to jointly optimize with $\mathcal{L}_{\mathrm{col}}$, hence we reduced its weights until convergences of the two.
The last component of our objective is the distribution matching loss $\mathcal{L}^{reg}_{\mathrm{dist}}$. We noticed that, when this loss is used, the predicted strokes may present a distorted height/width ratio. To avoid this issue, when computing this loss we exclude the size $\sigma$ and the orientation $\omega$ from the computation of features $\psi$. The same modification is applied to the SNP+ baseline for fairness of comparison.

\subsection*{Experiments}
\label{sec:supp_experiments}
\noindent \emph{User study.}  We now provide details on the user study presented in the main paper. Each task of the user study consists of an HTML page divided into two sections. In the first, called the demonstration section, we show a collection of stroke sequences taken from the training set. In the second section, called the evaluation section,  we show the users two stroke sequences produced from the same reference images and stroke context, one produced with \methodname~and the other with one of the baselines. We asked the participants to select which of the two sequences of strokes presents characteristics (in terms of stoke positions, colors, and subject consistency) that are most similar to the ones of the strokes in the demonstration section.
To ease the evaluation, we produce sequences with a length of 24 strokes and render the obtained strokes in a short video.
The user study was conducted on 40 images taken from the test set of \textit{Oxford-IIIT Pet INP} dataset, from which a total of 120 tasks was generated. We collected a total of 960 votes from 8 unique users. Examples of the user study are provided on the supplementary website.