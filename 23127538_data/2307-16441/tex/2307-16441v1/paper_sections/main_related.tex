\section{Related Work}
\label{sec_related}
In this section, we discuss the most related works in the field of NP and interactive image generation.


\subsection{Neural Painting}
Neural Painting techniques are derived from the intriguing idea of teaching machines how to paint. % like human painters. 
NP is typically formalized as the process of artistically recreating a given image using a neural network which generates a series of strokes. Several approaches are present in the literature that address this task.
Some of them make use of reinforcement learning (RL) \cite{Huang2019LearningTP,SchaldenbrandO21,Singh2021IntelliPaintTD,Singh_2021_CVPR,Xie2012ArtistAA}, where, given the current environment represented by the present status of the canvas and a reference image, an agent is trained to predict the parameters of the next strokes. The training objective is formulated as the maximization of the cumulative rewards of the whole painting process, typically expressed as the increase in similarity between the new canvas state and the reference image. Since no gradients need to be directly backpropagated from the reward function, RL-based methods do not require a differentiable stroke rendering procedure. \\ 

Other methods, instead, make use of a differentiable stroke renderer that allows direct optimization of a loss objective. Among these methods, \cite{zou2021stylized} and \cite{Kotovenko_2021_CVPR} directly optimize a set of parameters describing the stroke sequence, producing high-quality results at the cost of long inference times. Other works overcome this limitation by using a model to predict stroke parameters rather than directly optimizing them. In this context, the state of the art is represented by Paint Transformer (PT,  \cite{liu2021painttransformer}), where NP is expressed as a set prediction problem and a transformer-based architecture is proposed that predicts the parameters of a stroke set with a feedforward network. Our method shares similarities with \cite{liu2021painttransformer} as we also assume a differentiable stroke renderer and predict stroke parameters with a feedforward network. However, we address a different (and new) task, by focusing on an \emph{interactive} setting \willi{that requires seamless integration between model predictions and user inputs}. \\ %neural painting framework.

Finally, a different class of methods focuses on the closely related task of sketch generation which consists in the generation of \willi{abstract} sketches. Sketch-RNN (\cite{Ha_sketchRNN}) and Sketch-BERT (\cite{lin2020sketchbert}) represent sketches as sequences of points and are based respectively on RNN and transformer models. While these methods can be employed in interactive tasks such as sketch completion, they are not able to reproduce natural images and do not model realistic painting effects.


\subsection{Interactive Image Generation}
Interactive image generation refers to the task of automatically generating photo-realistic images, conditioned on user inputs. 
%\elia{say that with this term we include: image manipulation, image editing, conditional image generation (others?)}.
Early works fall into two directions: (1) image-to-image translation, which investigates the problem of translating an input image to a target domain, allowing to synthesize photos from label maps or reconstruct objects from edge maps (\cite{isola2017image,tang2019multi,zhu2017unpaired,zhu2017toward}); (2) learning a human-interpretable latent space (\cite{chen2016infogan}), projecting a natural image into it, manipulating the latent code to achieve an edit, and synthesizing a new image accordingly (\cite{abdal2019image2stylegan,abdal2020image2stylegan++,BrockLRW17,lin2021anycost,zhu2016generative}). To provide a more compelling experience, recent works on interactive image generation allow more user-friendly interaction, e.g. by means of sketches (\cite{ghosh2019interactive,liu2021deflocnet}), semantic maps (\cite{lee2020maskgan,ling2021editgan,park2019gaugan,tang2020local,zhu2020sean}), \elia{paint strokes (\cite{cheng2022adaptively, singh2022paint2pix})}, and text (\cite{bau2021paint,nichol2021glide,ramesh2021zero,xu2021predict}).  \cite{ghosh2019interactive} introduced iSketchNFill, an interactive GAN-based sketch-to-image translation method that helps novice users to easily create images of simple objects with a sparse sketch and the desired object category.
Differently, GauGAN (\cite{park2019gaugan}) converts a semantic segmentation mask to a photo-realistic image with a spatially-adaptive normalization layer. To flexibly manipulate an existing image, \cite{Bau:Ganpaint:2019} allows the user to perform a localized edit of an image by selecting a specific region, while \cite{liu2021deflocnet} empowers the user to edit low-level details by sketching the desired modifications. 
\elia{\cite{singh2022paint2pix} condition the image generation on strokes painted by the user, to provide a more intuitive way compared to segmentation maps, while \cite{cheng2022adaptively} rely on sketches and paint strokes to guide the generation process, allowing both flexibility and precise control.} Fueled by the success of text-to-image generation (\cite{ramesh2021zero}), very recent works proposed to control image manipulations with natural language, creating an intuitive way of interaction for the user. Notably, \cite{jiang2021language,shi2021learning} focused on the problem of global image editing, while \cite{xia2021tedigan} proposed a unique framework to both generate and manipulate images using text inputs.  \\

However, all the aforementioned methods are evaluated by the quality of the generated results, the diversity of the suggestions, and how closely they match the usersâ€™ input. The proposed task of INP adds an additional level of complexity. Since INP gives the users complete stroke-by-stroke control over the final artwork, it is necessary to represent the \emph{process} that leads to the final result.To ensure smooth interaction with the user, the method should follow a paint-like-demonstration behavior (see Sec.~\ref{sec:method} for a discussion). This requirement, and the level of control over the final output, is peculiar to the task of INP, and differentiates it from the existing literature in interactive image generation.


