\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.


\input{Components/Metafiles/packages.tex}
\input{Components/Metafiles/commands.tex}

\input{./Components/Metafiles/Lists}

\begin{document}
\begin{NoHyper}
\bstctlcite{IEEEexample:BSTcontrol}	
%\title{Trade-offs in Coexistence of Distributed AI and URLLC Traffic}
%\title{Improving the Performance of URLLC on Factory Automation using }
\title{Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning} % Let's try to find a better title

\author{
\IEEEauthorblockN{Wei Shi\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, Milad Ganjalizadeh\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, Hossein Shokri Ghadikolaei\IEEEauthorrefmark{1}, and Marina Petrova\IEEEauthorrefmark{2}\IEEEauthorrefmark{3}
\thanks{This work was partly supported by Swedish Foundation for Strategic Research (SSF) under Grant iPhD:ID17-0079.}
\thanks{\copyright 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.}
}
%
%\\
% \\
\vspace{3mm}
\IEEEauthorblockA{
% \\
\IEEEauthorrefmark{1}Ericsson Research, Sweden\\
\IEEEauthorrefmark{2}School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden\\
\IEEEauthorrefmark{3}Mobile Communications and Computing, RWTH Aachen University, Germany
\\
Email:
\{wei.b.shi, milad.ganjalizadeh, hossein.shokri.ghadikolaei\}@ericsson.com,
petrovam@kth.se}
%\\
%\and
\vspace{-1cm}
%\Mark{$\ddagger$} \IEEEauthorblockA{University of California Davis\\
%Email: xbwang@ucdavis.edu }
}
\markboth{}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Communications Society Journals}

\maketitle
	
\begin{abstract}
Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. 
One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources.
However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption.
In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL framework, we optimized the maximum number of retransmissions and transmission power of industrial devices.
Our extensive simulation results on the factory automation scenario show that the HRL framework achieves better performance as the baseline single-agent RL method, with significantly less overhead of signal transmissions and delay compared to the one-agent RL methods.
% For the upcoming \gls{6g} network, \gls{ml} also stands an important role that introduces intelligence and further enhances the system performance.
%We develop sample-efficient \gls{rl} to optimize the application-layer availability and reliability of \gls{urllc} service in factory automation scenarios. 
%However, wireless systems' parameters can be optimized either on a cell level or globally, depending on the inter-cell dynamics' impact on their optimal value. Although global optimizations can provide a better performance, such optimizations introduce major practical limitations on the control loop's delay, also excessive signalings.
\end{abstract}
	
% Note that keywords are not normally used for peer review papers.
\begin{IEEEkeywords}
	6G, availability, factory automation, hierarchical reinforcement learning (HRL), reliability, URLLC.
\end{IEEEkeywords}
\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%
%%%% New Section %%%%
%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
Nowadays, the development of \gls{5g} technology has achieved a mature technical standard aiming to provide wireless communication services to multiple vertical industrial areas \cite{ETRservice}. According to \gls{3gpp}~\cite{3gpp.22.261}, \gls{urllc} stands as one of the three main services for \gls{5g} standards, and beyond the standardization, it has shown significant improvements in the efficiency and performance of communication systems \cite{li20185g, popovski2019wireless, ren2020joint}. 
%\Gls{cps} is considered as the prospective foundation for industrial application and could stand for the main promotion for 6G development because of its focus on enabling high-quality real-time interactive communication and control for physical and engineering systems \cite{monostori2016cyber}. 
% The potential of \gls{cps} has been proved by various practical scenarios, such as smart manufacturing, autonomous vehicles, and smart electric grids. 
The main requirements for \gls{urllc} (especially in the context of \glspl{cps}) are high reliability (e.g., 10 years without failure), high availability (e.g., 99.9999\%), and low latency (below some tens of milliseconds). \Gls{ml} has proven effective in meeting these stringent requirements over resource-limited and faulty wireless channels~\cite{azari2019risk,kasgari2019model,lillicrap2015continuous,ganjalizadeh2021rl}.

\subsection{Literature Review}\label{se:1.1.1}
Various \gls{ml}-based optimization schemes have been proposed for \gls{urllc}. For example, reference \cite{azari2019risk} proposes a distributed risk-sensitive \gls{ml} solution for hybrid orthogonal/non-orthogonal radio resource slicing, regulating the spectrum to satisfy the \gls{urllc} requirements. 
Reference \cite{kasgari2019model} implements an \gls{rl} framework with the \gls{ddpg} algorithm \cite{lillicrap2015continuous} into an \gls{ofdma} system, minimizing the transmission power. Reference \cite{ganjalizadeh2021rl} optimizes both power and \gls{harq} retransmission scheme, leading to further improvements in terms of reliability and availability in factory automation use cases. 
However, adopting these strategies in real-life applications could be impractical:

\begin{itemize}[leftmargin=*]
    \item  Conventional (flat) RL methods only have one action vector that forces all decision variables to be designed at the same control loop (e.g., resource blocks and power allocation in \cite{kasgari2019model} and power and \gls{harq} retransmission scheme in \cite{ganjalizadeh2021rl}). However, \gls{5g} services usually require various decision variables to be tuned on different control loops. For example, we prefer to have one fixed slicing decision for many coherence intervals while we can constantly adjust the transmit power at every coherence interval. 
    \item To make a decision in a flat RL, we need to collect the information required to determine the state including the ones corresponding to variables with a much slower control loop (slicing in our example). The interval of such data collection is determined by the faster control loop. Such unnecessary data collection results in more energy consumption, network congestion, and extra latency, which could be detrimental to the sustainable operation of \gls{urllc} service. 
\end{itemize}

\noindent To address these limitations, we develop a \gls{hrl} framework to optimize the operation of URLLC. 

\subsection{Hierarchical Reinforcement Learning}\label{se:1.1.3}

Different from one-level \gls{rl} methods, \gls{hrl} decomposes one \gls{rl} problem into a hierarchy of sub-problems, which allows optimizing different tasks independently with different algorithms, timescales, models, and multiple agents \cite{pateria2021hierarchical}. This usually brings a reduction in exploration complexity, computation, signaling, and time required for the training and inference processes. Reference \cite{shah2021joint} proposes a hierarchical deep actor-critic method for the resource allocation problems of the 6G massive \gls{iot} scenarios. Reference \cite{liu2021dynamic} introduces a hierarchical \gls{dqn} model with one main controller and multiple sub-controllers to partition a dynamic spectrum access problem into separate sub-problems, reducing the complexity of band selection. In \cite{he2022meta}, the authors deploy a Meta-HRL framework for resource allocation in vehicular networks to enable faster learning on newly discovered sub-tasks.


\subsection{Our Contributions}
In this paper, we propose novel methods to orchestrate parameters of \gls{urllc} services.  
Reference \cite{ganjalizadeh2023tii} developed a single-agent \gls{rl} method on a factory automation scenario to jointly optimize the \gls{dl} transmission power and \gls{harq} retransmission control for the communication service performance (availability and reliability) in \gls{urllc} services. Here, we address the same problem with a novel HRL framework that supports better performance but with a more flexible structure that enables the system to allocate multiple agents and execute different operations with multi-level policies in different timescales. Our solution substantially reduces the signaling requirements for training and inference. In particular, two of the agents are located at the \glspl{gnb}, which significantly reduces the data exchange between \glspl{gnb} and the centralized remote \gls{hrl} agent. This efficiency results in time and energy savings in decision-making, thus simplifying the adaptation of our framework to the complex network requirements of real-world wireless systems and 6G.

\noindent \textbf{Notations:} Normal font $x$ and $X$, bold font \boldsymbol{$x$}, and uppercase calligraphic font $\mathcal{X}$ denote scalars, vectors, and sets respectively. Besides, $[X]$ denotes the set $\{1, 2, \ldots, X\}$, and $|\mathcal{X}|$ is the cardinality of set $\mathcal{X}$.

%%%%%%%%%%%%%%%%%%%%%
%%%% New Section %%%%
%%%%%%%%%%%%%%%%%%%%%
\section{System Model and Performance Metrics}\label{sec:PR}
\subsection{System Model}
% % Figure environment removed


We consider a factory automation scenario where a set of $\calB {\coloneqq}[B]$ \glspl{gnb} are present, each serving a set of $\calU_b {\coloneqq} [U_b]$ industrial devices, where $\calU_b\subset\calU$, and $\calU{\coloneqq}[U]$ is the set of all devices. These devices are responsible for executing various functions that facilitate automated production. 
In this scenario, the communication system must be capable of delivering monitoring data to \glspl{gnb} and computed or emergency control commands to the actuators in a timely and reliable manner. 
For the channel model, we assume \gls{inf-dh} from 3GPP in~\cite{3gpp.38.901}. Nevertheless, our problem formulation and approach, described in Section~\ref{sec:hrl} and Section~\ref{sec:simulationresults}, are not limited to this channel model.
To enable \gls{urllc} efficiently, we consider the orchestration of a set of reliability enhancement features, such as the transmission power to industrial devices and the maximum number of diversity transmissions (i.e., transmitting multiple instances of a packet or its segments in space, time, and/or frequency).
In this paper, we focus on the orchestration of transmission power and \gls{harq} retransmissions in \gls{dl} direction. However, our framework can easily be extended for other reliability enhancement features and \gls{ul} transmissions.

From the network management perspective, in the hierarchical multi-tier architecture of cellular networks, we assume two levels of control for global and local optimizations. Although there is only one top-level controller, we assume a set of low-level controllers co-located with the \glspl{gnb}, managing the transmissions towards $\calU_b$ together.

\begin{comment}

The illustration of the system model is as shown in \figurename\,\ref{fig:systemmodel}, where the factory with a \gls{cps} contains multiple \glspl{ue} executing various functions jointly for automated production. The communication condition is considered within a 5G deployment given several \glspl{gnb} responsible for the data transmission. An orchestrator is set up to manage the transmission power and techniques configuration individually for every \gls{ue} in the system. Through the northbound interface, the data from the \glspl{ue} are transmitted to the orchestrator, the network-level decisions from which are sent back through the southbound. This orchestrator can consist of multiple sub-orchestrators (i.e., \gls{hrl} agents) distributed in the network, which depends on the scale of \glspl{gnb} and devices, or other network requirements.

Ignoring the effect of rarely occurred packet loss, we define survival time $T_{s}$ as the maximum time that the application layer tolerates the consecutive losses during communication. Focusing on the \gls{dl} transmission, we define a set of \glspl{gnb} in the network as ${\mathcal{B}}\;:=\;[B]$, and each \gls{gnb} $b \in \mathcal{B}$ provides services to a set of \glspl{ue} ${\mathcal{U}_{b}} := [U_{b}]$, where $U_{b}$ denotes the number of devices in the \gls{gnb}. Besides, to describe the RL process of the system, we define a set of \gls{hrl} agents $\mathcal{M}:= [M]$, and each of them $m$ controls the operations of several corresponding \glspl{gnb}. Therefore, we have \glspl{gnb} controlled by agent $ \forall m \in \mathcal{M}$ as $\mathcal{B}_{m} := [B_{m}]$ and the set of \glspl{ue} ${\mathcal{U}_{m,b}} := [U_{m,b}]$. To fulfill the requirements from the various network operations, we enable each \gls{hrl} agent to handle one network operation, the respective time step of agent $m$ set as $\Delta t_{m}$.
% Therefore, we have $N_{U}\;:=\;\textstyle\sum_{i}{U_{i}}$ denotes the total number of devices. 
We assume that both power control and other various techniques (e.g., \gls{harq} retransmission, and packet duplication) can be deployed to support \gls{urllc} for \gls{cps}. Define $u \in \mathcal{U}_{b}$ as a \gls{ue} getting services from \gls{gnb} $b$, $N_{g}$ as the number of techniques planned to adopt, we have a diversity of techniques setting for $u$ as 
\begin{equation}
    g_{b,u}:=\left(g_{b,u}^{(1)},g_{b,u}^{(2)}, \cdots, g_{b,u}^{(k)},\cdots,g_{b,u}^{(N_{g})}\right),
\end{equation}

\noindent where $g_{b,u}^{(k)}$ represents the setting for one of the techniques and $g_{b,u}^{(k)} \in \{1, 2, 3, ..., N_{k}\}$, and $N_{k}$ is the maximum number of transmissions for the $k$th technique. 
% For example, if we set 3 techniques \textit{tech1}, \textit{tech2}, and \textit{tech3} for the device $u_{b}$, and respectively their transmissions are 3, 2, 1, then we have $ g_{b,u}:= \left(g_{b,u}^{(1)},g_{b,u}^{(2)},g_{b,u}^{(3)}\right)$, where $N_{g} = 3$ (i.e., the system contains three techniques), and $N_{1} = 3, N_{2} = 2, N_{3} = 1$. 
In addition, we define the power control of $u$ as $p_{b,u}$. 

For the infrastructure implementation, these orchestrators can be deployed as \glspl{vnf} in the cloud allocated in \gls{urllc} slice for any network node (e.g., edge or remote cloud). If the orchestrator is implemented in the edge cloud or locally with \glspl{gnb}, the added propagation delay would be low because of the close distance and the available computing resources at the edge \cite{mao2017survey}.
% where the overhead of signalings could be dismissed in some cases.
\end{comment}

\subsection{From Network to Communication Service Performance}\label{sec:nwstate}
The key element in characterizing service performance is defining service failures accurately. In~\cite{3gpp.22.104}, \gls{3gpp} defines survival time denoted as $T_{\mathrm{s}}$, as the duration for which an application can continue to function without receiving an expected packet. Therefore, a communication service failure occurs if no packets have been received by the reception entity for the duration of survival time. 

We can define the network layer state variable $Y_{b,u}{(t)}$ for industrial device $u$ associated to \gls{gnb} $b$ at time $t$, where $Y_{b,u}{(t)}$ is considered $0$ if the last packet fails to reach the communication interface within a specified delay bound due to decoding issues at lower layers, excessive retransmissions, or queuing delays, and $1$ otherwise. 
We can define the network state variable $Y_{b,u}{(t)}$ for the $u$th industrial device at time $t$, where $Y_{b,u}{(t)}$ is considered $0$ if the last packet fails to reach the communication interface within a specified delay bound due to decoding issues at lower layers, excessive retransmissions, or queuing delays, and $1$ if the packet is received successfully and timely.
Since sporadic packet losses\footnote{Here, packet loss refers to all packets that fail to reach their intended recipient within their delay bound.} within $T_{\mathrm{s}}$ do not impact the end-to-end service performance, the application layer state variable can be defined as~\cite{ganjalizadeh2023tii}:
\begin{equation}\label{eq:Z}
    Z_{b,u}{(t)}:=\max_{t-{T}_{\mathrm{s}}\leq\tau\leq t}Y_{b,u}{(\tau)}.
\end{equation}
\begin{comment}
As mentioned, this paper focuses on optimizing the application-layer performance for \gls{urllc}, which takes availability and reliability as the \glspl{kpi}. In the aspect of the network layer, the communication to a \gls{ue} is considered effective when the packets are successfully received on time by the \gls{pdcp} layer of the receiver. Therefore, we can use Bernoulli state variable $X_{b,u}^{(n)}\sim \mathrm{Ber}(1-e_{b,u}^{(n)})$ describe the channel state, where $e_{b,u}^{(n)}$ denotes the probability of packet error for $n$ \gls{dl} transmission to $u$th user in $b$th \gls{gnb}, which is also a function of \gls{sinr}, coding rate of packets, and techniques setting of the user $g_{b,u}$. Considering the network delay, the network state variable can be defined as 
\begin{equation}
    Y_{b,u}^{(n)}:=\mathbbm{1} \left\{D_{b,u}^{(n)}\leq D_{b,u}^{\mathrm{max}}, X_{b,u}^{(n)}=1\right\},
\end{equation}

\noindent where $D_{b,u}^{(n)}$ denotes the delay of $n$th packet transmission and $D_{b,u}^\mathrm{max}$ represents the maximum delay requirement for \gls{ue} $u_{b,u}$. Assuming at time $t_{n}$ the packet $n$ is received by $u_{b,u}$, the continuous network state can be defined as: 

\begin{equation}
    Y_{b,u}\left(t\right):=\sum_{n=1}^{\infty}Y_{b,u}^{\left(n\right)}\left[u(t-t_{n})-u(t-t_{n+1})\right], 
\end{equation}

\noindent where $u(t) = 1$ when $t \geq 0$ and 0 otherwise. Since sporadic packet failures within the $T_{s}$ are acceptable, the application layer state variable can be defined as:


\subsection{Communication Service Reliability Metrics}\label{sec:urllc}
\end{comment}
The application state variable $Z_{b,u}$ enables us to define and formulate our two reliability \glspl{kpi}, communication service availability and communication service reliability.

\subsubsection*{Communication Service Availability} It refers to the ability of an end-to-end communication service to perform its intended function without failure at a given point in time and is commonly expressed as a probability or as a percentage of time that the system is operational within a specified time period~\cite{3gpp.22.261}.
Considering the failure definition and $Z_{b,u}{(t)}$ in \eqref{eq:Z}, the communication service availability, $\alpha_{b,u}$, is~\cite{ganjalizadeh2023tii}
\begin{equation}\label{eq:avail}
    \alpha_{b,u}\coloneqq\lim_{t\to\infty}\Pr(Z_{b,u}{\left(t\right)}=1)=\lim_{T\to\infty}\frac{1}{T}\int_{0}^{T}Z_{b,u}(t) dt. 
\end{equation}
However, the communication service availability for $u$th device can be approximated in a short time , $\Delta t_k$, via~\cite{ganjalizadeh2023tii}
\begin{equation}\label{eq:eavail}
        \overline{\alpha}_{b,u}(\Delta t_k)\coloneqq\frac{1}{\Delta t_k}\,\int^{t_k}_{t_k-\Delta t_k} Z_{b,u}{(t)} dt.
\end{equation}
\subsubsection*{Communication Service Reliability}  It refers to the ability of an end-to-end communication service to operate without failures over a specific period, given certain environmental and operational conditions~\cite{3gpp.22.104}. It can be expressed as the meantime that the service is operational, that is $Z_{b,u}{(t)} = 1$. Therefore, reliability $\rho_{b,u}$ is formulated as~\cite{ganjalizadeh2023tii}

\begin{equation}\label{eq:relia}
    \rho_{b,u}\coloneqq\lim_{T\to\infty}\frac{1}{F_{b,u}{\left(T\right)}}\int_{0}^{T}Z_{b,u}(t) dt,
\end{equation}
\noindent where $F_{b,u}{\left(T\right)}$ denotes the number of crossings from $Z_{b,u}{(t)} = 1$ to $Z_{b,u}{(t)} = 0$ within $[0,T]$.
Since communication service reliability's unit is time, we can alternatively approximate it via the crossing rate, $\psi_{b,u}$, representing the crossings of $Z_{b,u}(t)$ from $1$ to $0$ during $\Delta t_k$; defined as $\psi_{b,u} {\coloneqq} \lim\limits_{T\to\infty}F_{b,u}{(T)}/T$. Note that $\psi_{b,u}$ is inversely proportional to $\rho_{b,u}$ in \eqref{eq:relia}. Then, the crossing rate can be approximated by~\cite{hoyland2009system}
\begin{equation}\label{eq:erelia}
        \overline{\psi}_{b,u}(\Delta t_k)\coloneqq\frac{F_{b,u}(\Delta t_k)}{\Delta t_k}.
\end{equation}
In the following section, we introduce our \gls{hrl} solution based on the formulation of the \glspl{kpi}.
%%%%%%%%%%%%%%%%%%%%%
%%%% New Section %%%%
%%%%%%%%%%%%%%%%%%%%%
\section{Optimization with HRL Framework}\label{sec:hrl}
In this paper, the objective is to maximize the communication service availability, in \eqref{eq:avail}, and communication service reliability, in \eqref{eq:relia}, of a \gls{cps} by optimizing the configuration of transmission power levels and the number of retransmissions. Nevertheless, our framework can easily be extended for more decision variables.
We propose to solve this problem using a \gls{hrl} framework, where a high-level agent collaborates with low-level agents to manage the bi-level control of the communication system. The high-level agent is responsible for inter-agent coordination and, therefore, we assign the task of mitigating inter-cell interference globally to it by placing the transmission power under its control. 
Hence, we model the problem as a twin timescale \gls{mdp} and then apply the \gls{sac} algorithm, as presented in~\cite{haarnoja2018soft}, to solve it. Additionally, we use the branching technique described in~\cite{ganjalizadeh2023tii} to enhance the performance of the algorithm.
For simplicity, we assume that the timescales of the low-level agents are identical and denote it with $\Delta t_k$, for an iteration starting from $t_k$, and represent the high-level agent's timescale with $\Delta t_{\kappa}^{\mathrm{h}}\coloneqq t_{\kappa}-t_{\kappa{-}1}$, where $\Delta t_{\kappa}^{\mathrm{h}}=c\Delta t_k$, $\forall c, k,\kappa\in\mathbb{N}$.


%The \gls{hrl} framework should contain common elements of \gls{hrl} algorithms, such as high-level and low-level agents managing the bi-level control of the communication system. The timescale of the low-level agents is denoted by (i.e., agents, sub-tasks, policy levels) that can be implemented into various systems using different \gls{rl} algorithms according to the network characteristics. Combining these  elements, we formulate our framework based on the general components of \gls{rl} including state space, action space with multi-level policy, and reward functions.

\subsection{State Space}\label{sec:state}
The state represents the set of various measurements from the environment that affects the performance of our main \glspl{kpi}, namely communication service availability and reliability. The state of $u$th device associated with the $b$th low-level agent, $\bm{s}_{b,u}{(\Delta t_k)}$, is measured within $[t_k-\Delta t_k, t_k]$, and consists of various factors that can be classified as direct and indirect, based on their effects on the two \glspl{kpi}.
As mentioned, individual availability $\alpha_{b,u}$ and reliability $\rho_{b,u}$ are the functions of the probability of packet loss and average operational time. Therefore, we add packet loss rate and mean downtime of the network layer as the direct factors to the state space. Apart from these two variables, we also consider the various factors that are not included in the \gls{kpi} functions but importantly affect the communication quality indirectly, as \gls{sinr}, packet transmission delays, the status of the \gls{rlc} layer buffers, path gain, the number of \gls{harq} transmissions, and the number of used resource blocks.
To enable a more concrete description of the communication environment, we include mean, median values, $95$th, and $5$th percentile of \gls{sinr}, path gain, and \gls{rlc} buffer status. However, for the rest of the factors, we incorporate the mean of the samples in the state.
Thus, the state of $b$th low-level agent for its $k$th iteration is defined as $\bm{s}_{b}{(\Delta t_k)} {\coloneqq} \left\{\bm{s}_{b,u}{(\Delta t_k)}| \forall u\in \calU_b\right\}$.

As for the high-level agent, we define the global state consisting of all elements in $\bm{s}_{b,u}$, but  measured in a much larger time scale, $\Delta t_{\kappa}^{\mathrm{h}}$, and including all devices. Then, the high-level agent's state for $\kappa$th iteration can be defined as $\bm{s}{(\Delta t_{\kappa}^{\mathrm{h}})} {\coloneqq} \left\{\bm{s}_{b,u}{(\Delta t_{\kappa}^{\mathrm{h}})}| \forall u\in \calU_b, \forall b\in \calB\right\}$.


\subsection{Action and HRL Policy}\label{sec:action}

The action space consists of a series of decision parameters for the agents to interact with environments. Similar to reference~\cite{ganjalizadeh2023tii}, we consider the quantized transmission power levels and the number of retransmissions in action set for each device.
\begin{comment}
Therefore, we define transmission power as $\mathcal{P} := \{p_{\min},p_{1},p_{2},\ \cdot\cdot\cdot, p_{\max} \}$, and action space as 

\begin{equation}
    \mathcal{A} := [N_{1}] \times [N_{2}] \times  \cdot\cdot\cdot \times [N_{N_{g}}] \times \mathcal{P},
\end{equation}

\noindent where $[N] = \{n|n \in \mathbbm{N}, n < N\}$.
\end{comment}
However, we decompose the joint action into a multi-level policy that enables the different \gls{hrl} agents to learn one or a combination of different network functions with different timescales based on network requirements. Specifically for our scenario, we set up a two-level policy that includes a global optimization of transmission powers (via a high-level agent), and local optimization of the maximum number of retransmissions (via several low-level agents). Hence, the high-level action in $\kappa$th iteration is defined as  $\boldsymbol{
a}_{\kappa}^{\mathrm{h}}\coloneqq(p^1_{\kappa},\ldots,p^{u}_{\kappa},\ldots,p^{U}_{\kappa})$, where $p^u_{\kappa} \in\{p_{\min},p_{1},p_{2},\ldots, p_{\max}\}$, and the action for $b$th low-level agent in $k$th iteration, $\boldsymbol{a}_{b, k}$, is a vector where each element represents the configured maximum numbers of retransmissions for devices in $\calU_b$. Compared to flat (or single-level) \gls{rl}, \gls{hrl} decomposes the action space into layers, resulting in higher scalability for handling complex orchestrations in cellular systems.
\begin{comment}
Therefore, for the \gls{hrl} agent $\forall m \in \mathcal{M}$ on time $t_{m}$, we have the action $A(t_{m}) \in \{A_{h}(t_{m}), A_{l}(t_{m})\}$, the high-level action $A_{h}(t_{m}) \in \mathcal{P}$ and low-level actions as

\begin{equation}
    A_{l}(t_{m}) \in [N_{1}] \times [N_{2}] \times  \cdot\cdot\cdot \times [N_{N_{g}}],
\end{equation}

\noindent Additionally, to fulfill the requirements of our two-level policy implementation, the agent $\forall m \in \mathcal{M}$ should learn following either the high-level step period $\Delta t_{h}$ or the low-level step period $\Delta t_{l}$ according to their actions (i.e., $\Delta t_{m} \in \{\Delta t_{l}, \Delta t_{h}\})$.

% \noindent Since every agent can schedule the learning process separately, we add different learning time steps to the two levels of actions as $\Delta t_{h}$ and $\Delta t_{l}$.
\end{comment}
% Figure environment removed

\subsection{Reward Functions}\label{sec:reward}
The \gls{rl} agents follow an explicit objective to maximize the sum of discounted rewards. 
Considering the estimations on communication service availability and crossing rate in \eqref{eq:eavail} and \eqref{eq:erelia}, respectively, we introduce two different reward functions. The first one, inspired by~\cite{ganjalizadeh2023tii}, targets the maximization of the average of reliability \glspl{kpi}, and is defined as
\begin{equation}\label{eq:rewAvg}
    r_{b}{(\Delta t_k)}\coloneqq \frac{1}{\omega U_b}\sum_{u=1}^{U_b} \left(\omega\overline{\alpha}_{b, u}(\Delta t_k)-(1{-}\omega)\overline{\psi}_{b, u}(\Delta t_k)\right),
\end{equation}
\noindent where $ 0{<}\omega{<}1$ decides the importance of $\overline{\alpha}_{b, u}(\Delta t_k)$ and $\overline{\psi}_{b, u}(\Delta t)$,  and its placement in the denominator helps to bound the reward function by $1$. Consequently, the high-level reward is defined as $r{(\Delta t_{\kappa}^{\mathrm{h}})}{\coloneqq}\frac{1}{B}\sum_{b=1}^{B}r_b{(\Delta t_{\kappa}^{\mathrm{h}})}$, where $r_b{(\cdot)}$ is derived as in \eqref{eq:rewAvg}, but within $(t_{\kappa{-}1},t_{\kappa}]$.
For the second function, we adopt a risk-sensitive reward that replaces the average value of users with the extremum within $\calU_b$, where agents can be more aggressive in exploration. Enabling agent $m$ to maximize the availability and minimize the crossing rate, the $k$th iteration reward is defined as
\begin{equation}\label{eq:rewRisk}
    r_b{(\Delta t_k)}\coloneqq \exp\left(\frac{\eta}{\omega}\left(r'_b{(\Delta t_k)} - \omega\right)\right),
\end{equation}
where
\begin{equation}\label{eq:rPrime}
   r'_b{(\Delta t_k)}\coloneqq\omega\min_{u \in \mathcal{U}_{b}}\!\!\left(\overline{\alpha}_{b, u}{(\Delta t_{k})}\right)-(1{-}\omega)\max_{u \in \mathcal{U}_{b}}\!\!\left(\overline{\psi}_{b,u}{(\Delta t_{k})}\right),
\end{equation}
and $\eta$ represents a fixed coefficient predefined to adjust the reward reduction with the application's sensitivity to reliability \glspl{kpi}.
The high-level reward, $r{(\Delta t_{\kappa}^{\mathrm{h}})}$, can then be calculated
as \eqref{eq:rewRisk}, but within $(t_{\kappa{-}1},t_{\kappa}]$ where the $\min$ and $\max$ functions in \eqref{eq:rPrime} are calculated for $\forall u\in\calU$.


\subsection{Learning Procedure}\label{sec:algo}
The learning procedure of our two-level policy \gls{hrl} framework is as shown in \figurename \ref{fig:learningprocedure}, where two network operations, global power control and maximum \gls{harq} retransmission number are decided by a high-level agent and low-level agents, parallelly following two timescales ($\Delta t_{k}$, $\Delta t^{\mathrm{h}}_{\kappa}$).
At every time step $\Delta t_{k}$, the low-level agents collect the states of the users they control, also the calculated rewards. Then the generated actions for \gls{harq} retransmission number are sent to the communication system. The high-level agent issues power values to all the devices every $c\Delta t_{k}$. Similar to the design in \cite{ganjalizadeh2021rl}, all the agents are model-free and can individually implement any \gls{rl} algorithms according to the task requirements. For the sake of fair comparison to the single-agent RL in~\cite{ganjalizadeh2023tii}, we deploy the same \gls{sac} algorithm \cite{haarnoja2018soft} to all the \gls{hrl} agents, and adopt the same branching technique that enables the continuous actions to describe our discrete actions in our factory automation scenario.


% For our second reward function, to make agents more aggressive in exploration, we adopt a risk-raware reward that replaces the average value of users with the extremum within a \gls{gnb}. In this case, the reward function is defined as
% Our optimization target is to maximize the availability and reliability of the system
% Since all the agents have independent learning iterations, the reward functions should be formulated at the agent level. 
% and the second one is the risk-sensitive reward. 

% \noindent Since every agent can schedule the learning process separately, we add different learning time steps to the two levels of actions as $\Delta t_{h}$ and $\Delta t_{l}$.


%%%%%%%%%%%%%%%%%%%%%
%%%% New Section %%%%
%%%%%%%%%%%%%%%%%%%%%
\section{Simulation Methodology and Result Analysis}\label{sec:simulationresults}

In this section, we present the simulation methodology and evaluate the performance of our \gls{hrl} framework.

\subsection{Simulation Configuration and Methodology}\label{sec:simulation}

% Figure environment removed

\figurename\,\ref{fig:simulator} shows our simulation architecture, which consists of a link-level Matlab simulator, a network-level Java simulator, and the \gls{rl} agents implemented using an open source library, RLlib. With path gain matrices, 3D channel data \cite{3gpp.38.901}, and nodes allocation provided by the link-level simulator, the network-level simulator is able to simulate the network with multiple \glspl{gnb} and users in \gls{phy}, \gls{mac}, and other higher layers of the \gls{5g}-NR network. As for the \gls{rl}/\gls{hrl} deployment, our network-level simulator supports interacting with external agents using the pipelines based on ZeroMQ protocol.
%Following the patterns by RLlib, our simulator and the \gls{hrl} agents take episode as the time unit to run the simulations and perform the learning process. Each episode has the same execution time equally divided into a number of learning steps, which varies from the agents and network requirements. 

We considered a $40 \times 25 \times 11$\,m$^{3}$ factory with two $10$\,m height \glspl{gnb} providing communication service to $10$ industrial devices. Moreover, we assumed that the devices are in a high interference condition, and they move with the speed of $30$\,km/h, while staying in close proximity to
the original position. We considered periodic control traffic with periodicity $2$\,ms, and a delay bound $2.5$\,ms. On the transmitter side, packets are queued in the \gls{rlc} buffers and then sent via transport blocks based on the selected \gls{mcs}.
On the receiver side, the successfully decoded packets that were received after the delay bound were discarded by \gls{pdcp}. In the end, the reliability and availability are calculated in the application layer based on survival time, $T_{\mathrm{s}}$. The network and learning parameters of our simulations are presented in \tablename\,\ref{tab:netpara}. For more details on our simulation setup, you can refer to \cite[\S VI.A, \S VI.B]{deviceSelection}.
\begin{table}[t]
\centering
\caption{Simulation Parameters.}
\label{tab:netpara}
\scalebox{0.75}{\begin{tabular}{  l|l  }
	\hline
        \multicolumn{2}{c}{\textbf{Network Parameters}} \\
        \hline
	\textbf{Parameter}& \textbf{Value}\\
	\hline
	Deployment   & $2$ \glspl{gnb}, $1$ cell each\\
	\glspl{gnb} antenna height&   $8$\,m \\
	Devices' height &$1.5$\,m \\
	Carrier frequency & $2.6$\,GHz \\
	Bandwidth&   $20$\,MHz\,\\
	TTI length/Subcarrier spacing& $0.5$\,ms/$30$\,KHz  \\
	DL transmit power $(p_{min}/p_{max})$ &   $0.2$\,W/$0.5$\,W  \\
        Number of \gls{gnb}/Devices' antennas & $2/2$ \\
	\gls{dl}\,\gls{urllc}\,delay bound& $2.5$\,ms \\
	\gls{dl} \gls{urllc} Survival time ($T_{\mathrm{s}}$)& $5$\,ms \\
	Simulation time &  $10$\,s$/$Episode\\
	\hline
        \multicolumn{2}{c}{\textbf{Learning Parameters}} \\
        \hline
        \textbf{Parameter}& \textbf{Value}\\
	\hline
	Neural network hidden layers & $128 \times 128$  \\
        Activation function & ReLU  \\
        Loss function & MSE  \\
        Optimizer & mini-batch SGD\\
	Discount factor & 0.1 \\
        Batch size & 200 \\
%	Blocker width   & $[0.5, 2]$\,m\\
%	Blocker height   & $[1, 3]$\,m\\
	Learning rate & 0.0003 \\
	% Smooth update coefficient & 0.002 \\
	% Prioritized replay buffer & $0.4/0.6$ \\
        \gls{rl} step period ($\Delta t$) & $0.1$\,s \\
	\gls{hrl} step period low/high-level ($\Delta t_k$ / $\Delta t_{\kappa}^{\mathrm{h}}$) & $0.1$/$0.5$\,s, $\forall k,\kappa\in\mathbb{N}$ \\
	\hline
\end{tabular}}
 \vspace{-4.5mm}
\end{table}

% Figure environment removed
% % Figure environment removed


The goal of our \gls{hrl} solution is to find the optimal solution for power control and \gls{harq} retransmissions to maximize communication service availability and reliability. We considered the following baselines in our evaluations:
\begin{itemize}
    \item \texttt{MaxRetPwr}: Similiar to~\cite{ganjalizadeh2023tii}, all resource blocks are configured with $0.02$\,W, and the maximum number of transmissions is set to 2.
    
    \item \texttt{RLAvg}/\texttt{RLRiskSen}: There exist only one \gls{rl} agent (in a remote server) interacting with the two \glspl{gnb} (the blue agent in \figurename\,\ref{fig:simulator}). The step period is set to $0.1$\,s. The agent selects from the two possible levels of $0.008$\,W, and $0.02$\,W, set for all resource blocks allocated to a specific device, and the maximum number of transmissions (which can be either 1 or 2). While \texttt{RLAvg} used the average reward from \cite{ganjalizadeh2023tii} and presented in \eqref{eq:rewAvg}, \texttt{RLRiskSen} was implemented with the risk-sensitive reward in \eqref{eq:rewRisk}.
\end{itemize}
Furthermore, for the \gls{hrl} solution, we implemented one top-level agent and two low-level agents (the red agents in \figurename\,\ref{fig:simulator}). The former agent's objective was to optimize the \gls{dl} transmission powers globally, while the numbers of \gls{harq} retransmissions were optimized per \gls{gnb} by the latter agents. For the sake of fair comparison, the action space was set similar to \texttt{RLAvg} and \texttt{RLRiskSen}. Besides, we considered two setups as \texttt{HRLAvg}, where we incorporated the average reward in \eqref{eq:rewAvg}, and \texttt{HRLRiskSen}, where we incorporated the risk-sensitive reward, in \eqref{eq:rewRisk}.

\subsection{Result and Analysis}\label{sec:results}
%In this section, we list and analyze the results from our simulations using the single-agent \gls{rl} solution and our \gls{hrl} framework. 

\figurename\,\ref{fig:ava} and \figurename\,\ref{fig:ns} present the \gls{cdf} of the communication service availability and the \gls{ccdf} of the device crossing rate, determined by \eqref{eq:eavail} and \eqref{eq:erelia}, respectively. 
In these figures, each data point represents the device availability (in \figurename\,\ref{fig:ava}) or crossing rate (in \figurename\,\ref{fig:ns}) in one simulation round. 
Assuming an availability requirement of $0.999$, \figurename\,\ref{fig:ava} shows that both \texttt{HRLRiskSen} and \texttt{RLRiskSen} achieve a similar violation probability of $0.083$, while \texttt{MaxRetPwr} can only reach violation probability of $0.35$.
Similarly, assuming a crossing rate requirement of $0.001$, \figurename\,\ref{fig:ns} indicates that \texttt{HRLRiskSen} and \texttt{RLRiskSen} can obtain violation probability of $0.058$, closely followed by \texttt{HRLAvg}, and they all significantly outperform \texttt{MaxRetPwr}. Furthermore, our \gls{hrl} framework shows a lower violation probability for availability $>$ 0.999 and crossing rate $<$ 0.001, outperforming the \gls{rl}.

It is surprising that our \gls{hrl} framework achieves better performance than the single-agent \gls{rl} method, which learns the states from all the devices within one agent. In comparison, the \gls{hrl} agents have three different actions, where two of them only learn partial states of the devices from one \gls{gnb}. Since we run both \gls{rl} and \gls{hrl} training for a fixed number of iterations, such improvement can be contributed by the reduction in the action space (as a result of action decomposition), leading to faster convergence.  


% Figure environment removed

The error bar plot in \figurename\,\ref{fig:bar_twobs} demonstrates the mean (shown by square) and 5th percentile (shown by line) device availability. The average and 5th percentile availability of single-agent \gls{rl} and \gls{hrl} in both rewards achieve similar performance that is much higher than \texttt{MaxRetPwr}. Although the baseline simulation adopts the maximum power and 2 \gls{harq} retransmissions, its poor performance indicates that the lack of freedom in operations could reduce interference management capability. Same as in \figurename\,\ref{fig:ava_ns_twobs}, the risk-sensitive reward shows improvement in performance than the average reward for the \gls{hrl} and \gls{rl}.

% Figure environment removed
 
\figurename\,\ref{fig:sig_num} presents the number of signal exchanges (communication rounds) the learning frameworks require to converge (i.e., all reward values tend to stabilize and converge, and they no longer increase with further exploration). One signal exchange in \figurename\,\ref{fig:sig_num}  represents a signal transmission between the remote agent and one of the \glspl{gnb}. According to the allocation of agents in \figurename\,\ref{fig:simulator}, there are two learning agents assembled locally with the \glspl{gnb}, the transmission overhead of which can be ignored compared to the remote server. In the case of the single-agent \gls{rl} solution, the agent transmits one message of action to each \gls{gnb} and receives a reply message with states and rewards from them. Therefore, there are four signal transmissions at every step. Similarly, for \gls{hrl} framework, there are four signal transmissions at every high-level step. We performed two \gls{rl} simulations and two \gls{hrl} simulations and logged the total number of exchanged signals till convergence. 
As \figurename\,\ref{fig:sig_num} confirms, compared to \gls{hrl}, \gls{rl} simulations required over triple the number of communication rounds to converge.
Such reduction in signal exchanges can significantly improve the energy efficiency of large-scale communication systems with many remote devices and simultaneous operations.
Beyond signal exchanges, decomposing the \gls{rl} action allows the \gls{hrl}'s average learning time per iteration to be 33\% less than that of the single-agent \gls{rl} (i.e., $6.288$\,ms vs $8.386$\,ms).
This translates into massive gains in terms of latency and energy saving to run the training, especially in dynamic environments where we may need to retrain the models regularly.

% Look how you should reference 3gpp \cite{3GPP22104} and arxiv \cite{mobileNets}.



%%%%%%%%%%%%%%%%%%%%%
% NEW SECTION
%%%%%%%%%%%%%%%%%%%%%
% \section{Results and Discussion}\label{sec:preformance}

%%%%%%%%%%%%%%%%%%%%%
% NEW SECTION
%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conculsions}

In this paper, we propose a \gls{hrl} framework and implement that into a simulated factory automation model to optimize the operations of power control and \gls{harq} retransmissions in \gls{5g} communication, aiming to achieve the optimal availability and reliability according to the standard of \gls{urllc}. We design and compare five simulations that separately deploy the single-agent \gls{rl} strategy, our \gls{hrl} framework with average and risk-sensitive reward functions, and one with fixed operations as the baseline. Our \gls{hrl} framework achieves better performance on availability and reliability to the ideal single-agent \gls{rl} solution and significantly outperforms the baseline simulation. Besides, our \gls{hrl} framework enables better flexibility that allows the operations to be executed in different timescales. Furthermore, due to the flexible allocation of agents, the \gls{hrl} solution can save signal consumption by at least
triple less than that of the \gls{rl} %125\%
in the scenario of our factory model.

\end{NoHyper}
\ifCLASSOPTIONcaptionsoff
\newpage
\fi


%\begin{thebibliography}{1}
%	
%	\bibitem{IEEEhowto:kopka}
%	H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%	0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%	
%\end{thebibliography}
\Urlmuskip=0mu plus 1mu\relax
\bibliographystyle{IEEEtran}

% argument is your BibTeX string definitions ref and bibliography database(s)

\bibliography{Components/Metafiles/IEEEabrv,Components/Metafiles/ref}
\clearpage
% \input{sections/Appendix}
%\printglossaries
\end{document}