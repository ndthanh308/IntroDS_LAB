\section{Discussion and Future Work}
\label{sec:discussion}

\paragraph{Optimal weights beyond Gaussianity.} 
$\Ep \brk{x\iplus  \param\imin^\top z\iplus  z\iplus^\top  \param\imin x\iplus^\top}$ has a nice closed form in Gaussian setting because $z\iplus$ and $x\iplus$ are independent---which is in general not true without Gaussianity. If we can directly sample from the feature distribution $\mc{P}$ (e.g., unlabeled data), then we can empirically estimate $\Ep \brk{x\iplus  \param\imin^\top z\iplus  z\iplus^\top  \param\imin x\iplus^\top}$ by sampling from $\mc{P}$ and using any consistent plug-in estimate $\hparam$ (e.g., run \textsc{Collab} with weights $W_i = I_{d_i}$). This will return a good estimate of the optimal weights. An interesting future direction is to prove lower bounds without the Gaussianity assumption. 

\paragraph{Generalization to non-linear models.} Recall in the Gaussian setting, the optimal weights in \textsc{Collab} are 
$
	W_i\gauss 
    = \scov\iplus /(\E_{x, y}[(\< x\iplus, \hparam_i\> - y)^2]).
$
Then, the optimal loss function in Eq.~\eqref{eq:defn-weighted-avg-est-param} becomes
\begin{align*}
	\sum_{i=1}^m \norm{\param\iplus + \scov\iplus^{-1} \scov\ipm \param\imin - \hparam_i }_{W_i\gauss}^2  =  \sum_{i=1}^m \frac{\E_{x\iplus} [ (\<x \iplus, \hparam_i\> - \<x \iplus, T_i \param\>)^2]}{\E_{x, y}[(\< x\iplus, \hparam_i\> - y)^2]}.
\end{align*}

This hints at a generalization to non-linear models. Suppose the local agents train on models $f^i(x\iplus; \param_i), \R^{d_i} \times \R^{d_i} \mapsto \mc{Y}$ and the global model $f(x; \param), \R^{d} \times \R^{d} \mapsto \mc{Y}$ satisfies for some mapping $T_i : \R^d \to \R^{d_i}$, $f(x; T_i \param) = f^i(x\iplus; \param_i)$. Consider a loss function $\ell(\cdot, \cdot): \mc{Y} \times \mc{Y} \to [0, \infty)$. Then we can consider the following way of aggregation inspired by \textsc{Collab} for linear models
\begin{align*}
	\est{\param} := \argmin_\param \sum_{i=1}^m \frac{\E_{x\iplus} \ell(f^i(x\iplus; \hparam_i), f(x\iplus; T_i \param))}{\E_{x\iplus, y}\ell(f^i(x\iplus; \hparam_i), y)}.
\end{align*}
We can consistently estimate the denominators (weights) using training time loss. An interesting future direction is to investigate the performance of this general approach for non-linear problems. 