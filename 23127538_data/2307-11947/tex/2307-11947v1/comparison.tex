\section{Comparison with other methods} \label{sec:comparison}





\begin{table}[t]
    \centering
	\renewcommand{\arraystretch}{1.5} 
    \begin{tabular}{|C{3.8cm}|C{3.1cm}|C{3.7cm}|C{2.1cm}|}
    \hline
    Method & Full-feature asymptotic covariance & Missing-feature asymptotic covariance & Communication cost for agent $i$ \\
    \hline
    Local OLS - $\hparam_i$ &  - & $(W_i\gauss)\inv$ & 0 \\
    \hline
    Local imputation w/ collaboration - $\est{\param}\impute_i $ &$ \prn{\sum_{i=1}^m T_i^\top  W_i\gauss   T_i}^{-1}$& $T_i \prn{\sum_{i=1}^m T_i^\top  W_i\gauss   T_i}^{-1}T_i^\top$ & $\Theta(d^2)$ \\
    \hline
    Global imputation - $\est{\param}\imputeglb_i$  & $ \prn{\sum_{i=1}^m T_i^\top  W_i\gauss   T_i}^{-1}$& $T_i \prn{\sum_{i=1}^m T_i^\top  W_i\gauss   T_i}^{-1}T_i^\top$ & $\Theta(nd_i)$ \\
    \hline
    \textsc{Collab} - $\est{\param}\collab_i$& $ \prn{\sum_{i=1}^m T_i^\top  W_i\gauss   T_i}^{-1}$& $T_i \prn{\sum_{i=1}^m T_i^\top  W_i\gauss   T_i}^{-1}T_i^\top$ & $\Theta(d_i^2)$ \\
    \hline
    \end{tabular}
    \caption{Full and Missing feature asymptotic covariance and communication cost for agent $i$. Communication cost is measured by how many real numbers are received and sent from agent $i$. }
    \label{table:Comparison}
\end{table}


In this section, we compare our collaborative learning procedure with other popular least squares techniques based on imputation and comment on the statistical efficacy and communication cost differences. We summarize our analysis in \Cref{table:Comparison}. The proofs of the theorems are in Appendix~\ref{proof:upper-bound-comparison}.


\paragraph{Local imputation w/ collaboration.} Suppose a coordinating server collected covariance information $\scov_i$ from each agent and then distributed $\scov$ back to each of them.
Then one intuitive strategy is to use this information to impute each agent's local data by replacing $X\iplus$ with $\E[X_i \mid X\iplus] = X\iplus T_i$, before performing local linear regression. In other words, instead of computing $\hparam_i$, compute
\begin{align*}
	\est{\param}\impute_i = (T_i^\top X\iplus^\top X\iplus T_i)^\dagger T_i^\top X\iplus^\top y_i
\end{align*}
to send back to the coordinating server.
Note that we use Mooreâ€“Penrose inverse here as $T_i^\top X\iplus^\top X\iplus T_i$ is in general of rank $d_i$, and $\est{\param}\impute_i$ is then the min-norm interpolant of agent $i$'s data. Similar to \textsc{Collab}, we can use weighted empirical risk minimization parameterized by $W_i \in \R^{d \times d}$ and to aggregate $\est{\theta}\impute$ via
\begin{align*} 
	\est{\theta}\impute = \est{\param}(W_1, \cdots, W_m) \defeq \argmin_\param \sum_{i=1}^m  \norm{T_i^\top (T_i T_i^\top)^{-1} T_i  \param - \est{\param}_i\impute  }_{W_i}^2.
\end{align*}

The next theorem, in conjunction with Theorem~\ref{thm:upper-bound}, implies that under the WERM optimization scheme, aggregation of least squares estimators on imputed local data does not bring additional statistical benefit. In fact, the local imputation estimator is a linearly transformed on local OLS $\est{\param}_i$.



\begin{theorem} \label{thm:upper-bound-imputed}
	For  $\est{\param}_i\impute$ from agent $i$, we have $\est{\param}\impute_i = T_i^\top (T_i T_i^\top)^{-1} \est{\param}_i$. Given any weighting matrices $W_i \in \R^{d \times d}$, the aggregated imputation estimator $\est{\param}\impute $ is consistent and asymptotically normal
	\begin{align*}
		\sqrt{n} \prn{\est{\param}\impute - \param} = \normal(0, C\impute(W_1, \cdots, W_m)).
	\end{align*}
	Using the same weights $W_i\opt \in \R^{d_i \times d_i}$ as in Theorem~\ref{thm:upper-bound} for aggregated $\est{\param}\impute$, we have
	under p.s.d.\ cone order, for weights $W_i$, 
	$C\impute(W_1, \cdots, W_m) \succeq C\opt$, where $C\opt = (\sum_{i=1}^m T_i^\top  W_i\opt   T_i)^{-1}$. In addition, the equality holds when $W_i =  T_i^\top W_i\opt T_i$.
\end{theorem}

As we will see in Sec.~\ref{sec:lower-bounds} where we provide minimax lower bound for weak observation models, the fact that the weighted imputation does not outperform our \textsc{Collab} approach is because the WERM on local OLS without imputation is already optimal. In fact, having access to the features will not achieve better estimation rate for both the global parameter $\param$ and local parameters $T_i \param$.

In terms of communication cost, this local imputation method requires more communication than \textsc{Collab}, as a central server needs to communicate $\scov$ to all the hospitals. This amounts to a total of $ \Theta(m d^2)$ communication cost instead of $\Theta(\sum_{i\in[m]} d_i^2)$ communication cost for \textsc{Collab}.

\paragraph{Global imputation.} 
Finally, we analyze the setting where 
we allow each agent to send the central server all of their data $(X\iplus, y_i)$ for $i=1,\cdots, m$ instead of their local estimators, $\est{\param}_i$ or $\est{\param}_i\impute$. Having all the data with structured missingness available, a natural idea is to first impute the data, replacing $X\iplus$ with $\E[X_i \mid X\iplus] = X\iplus T_i$, and then performing weighted OLS on \emph{all} of the $nm$ data points. Namely for scalars $\alpha_1, \cdots, \alpha_m > 0$, we take
\begin{align*}
	\est{\param}\imputeglb = \est{\param}\imputeglb(\alpha_1, \cdots, \alpha_m) := \prn{\sum_{i=1}^m \alpha_i T_i^\top X\iplus^\top X\iplus T_i}^{-1} \prn{\sum_{i=1}^m \alpha_i T_i^\top X\iplus^\top y_i}.
\end{align*}
Surprisingly, in spite of the additional power,
$\est{\param}\imputeglb$ still does not beat $\est{\param}$ in Theorem~\ref{thm:upper-bound}.

\begin{theorem} \label{thm:upper-bound-imputed-glb}
	For any scalars $\alpha_1, \cdots, \alpha_m > 0$, $\est{\param}\imputeglb$ is  consistent and asymptotically normal
	\begin{align*}
		\sqrt{n} \prn{\est{\param}\imputeglb - \param} = \normal(0, C\imputeglb(\alpha_1, \cdots, \alpha_m)). 
	\end{align*}
	Recall the lower bound matrix $C\opt \defeq (\sum_{i=1}^m T_i^\top  W_i\opt   T_i)^{-1}$ in Theorem~\ref{thm:upper-bound}. If $X_i \sim \normal(0, \scov)$, we have
	under p.s.d.\ cone order and any $\alpha_i > 0$, $C\imputeglb(\alpha_1, \cdots, \alpha_m) \succeq C\opt$. In addition, the equality holds when $\alpha_i = 1/(\|\param\imin\|_{\Gamma\imin}^2 + \sigma^2)$.
\end{theorem}

The communication cost for this method is significantly larger. Having each agent send all of its data to a coordinating server requires $\Theta(\sum_{i\in[m]} d_i n)$ communication cost, as opposed to the $\Theta(\sum_{i\in[m]} d_i^2)$ communication cost for \textsc{Collab}. The fact that communication cost for this method scales with $n$ is a significant disadvantage for the reasons we outlined in the introduction.