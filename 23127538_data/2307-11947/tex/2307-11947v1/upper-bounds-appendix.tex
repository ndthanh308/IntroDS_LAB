\section{Proofs for Section~\ref{sec:upper-bounds}}

\begin{lemma} \label{lem:est-param-consistency}
	For any positive definite matrices $W_i \in \R^{d_i \times d_i}$, $i=1,2,\dots, m$, the aggregated estimator $\est{\param}$ in Eq.~\eqref{eq:defn-weighted-avg-est-param} is consistent $\est{\param} \cp \param$. In addition, if $X_i \sim \normal(0, \scov)$, we have unbiasedness $\Ep [\est{\param}] = \param$ 	where $\Ep$ is over the random data $X_i$ and noise $\noise_i$.
\end{lemma}

\subsection{Proof of Lemma~\ref{lem:est-param-consistency}}
\label{sec:proof-est-param-consistency}
	For the general case, identify for $\est{\param}_i$, we can write
\begin{align*}
	& \est{\param}_i  = (X\iplus^\top X\iplus)^{-1} X\iplus y_i   = (X\iplus^\top X\iplus)^{-1} X\iplus^\top (X\iplus \param\iplus + X\imin \param\imin + \noise_i) \nonumber \\
	& = \param\iplus +  (X\iplus^\top X\iplus)^{-1}  (X\iplus^\top X\imin \param\imin + X\iplus^\top \noise_i) \nonumber \\
	& =  \param\iplus + \prn{\frac{1}{n}X\iplus^\top X\iplus}^{-1} \prn{\frac{1}{n} X\iplus^\top X\imin \param\imin  + \frac{1}{n} X\iplus^\top \noise_i}.
\end{align*}
The weak law of large numbers implies that $X\iplus^\top X\iplus/n \cp \scov\iplus$, $X\iplus^\top X\imin/n \cp \scov\ipm$ and $ \frac{1}{n} X\iplus^\top \noise_i \cp 0$. Then Slutsky's theorem gives the consistency guarantee
\begin{align*}
	\est{\param}_i \cd \param\iplus + \scov\iplus^{-1} \prn{\scov \ipm \param\imin + 0} = \param\iplus + \scov\iplus^{-1} \scov \ipm \param\imin = T_i \param,
\end{align*}
which is equivalent to $\est{\param}_i \cp T_i \param$. Substituting back into $\est{\param}$, we can obtain again from continuous mapping theorem that
\begin{align*}
	\est{\param} & = \prn{\sum_{i=1}^m T_i^\top W_i T_i}^{-1} \prn{\sum_{i=1}^m T_i^\top W_i \est{\param}_i}\cp \prn{\sum_{i=1}^m T_i^\top W_i T_i}^{-1} \prn{\sum_{i=1}^m T_i^\top W_i T_i \param} = \param.
\end{align*}

Next, we specialize to Gaussian features and show $\est{\param}$ is indeed unbiased in this case. By the tower property, we can write for each local OLS estimator,
\begin{align*}
	& \Ep [\est{\param}_i]  = \Ep [(X_{i+}^\top X_{i+})^{-1} X_{i+} y_i]  = \Ep \brk{\Ep [(X\iplus^\top X\iplus)^{-1} X\iplus^\top (X\iplus \param\iplus + X\imin \param\imin + \noise_i) \mid X\iplus]} \nonumber \\
	& = \param\iplus + \Ep \brk{(X\iplus^\top X\iplus)^{-1} X\iplus^\top  \Ep [ X\imin \mid X\iplus]} \param\imin.
\end{align*}
We want to compute $\Ep [ X\imin \mid X\iplus]$ and the key observation is that with Gaussianity in $X_i$, we have
\begin{align*}
	&\cov (x\imin - \scov\imp \scov\iplus^{-1} x\iplus, x\iplus) = \cov (x\imin, x\iplus) - \scov\imp \scov\iplus^{-1}  \cov (x\iplus, x\iplus) \nonumber \\
	& = \scov\imp - \scov\imp \scov\iplus^{-1} \cdot \scov\iplus = 0,
\end{align*}
and therefore $x\iplus$ is independent of $x\imin - \scov\imp \scov\iplus^{-1} x\iplus$, which further implies that
\begin{align*}
	& \Ep \brk{ X\imin  \mid X\iplus}  = \Ep \brk{ X\iplus \scov\iplus^{-1} \scov\ipm  \mid X\iplus}  + \Ep \brk{X \imin - X\iplus \scov\iplus^{-1} \scov\ipm \mid X\iplus}  =  X\iplus \scov\iplus^{-1} \scov\ipm.
\end{align*} 
Substituting the above property into computing the expectation of local estimates $\est{\param}_i$, it then holds
\begin{align*}
	\Ep [\est{\param}_i] & = \param\iplus + \Ep [(X\iplus^\top X\iplus)^{-1} X\iplus^\top X\iplus \scov\iplus^{-1} \scov\ipm  ] \param\imin  = \param\iplus +  \scov\iplus^{-1} \scov\ipm \param\imin = T_i \param.
\end{align*}
We can then conclude the proof as
\begin{align*}
	\Ep [\est{\param}] = \prn{\sum_{i=1}^m T_i^\top W_i T_i}^{-1} \prn{\sum_{i=1}^m T_i^\top W_i T_i \param} = \param.
\end{align*}

\subsection{Proof of Theorem~\ref{thm:upper-bound}}
\label{sec:proof-upper-bound}
	We first study the central limit theorem for local OLS estimators $\est{\param}_i$. Let the data matrices $X\iplus = [x\iplus^1, \dots, x\iplus^n]^\top$ and $X\imin = [x\imin^1, \dots, x\imin^n]$ and the noise vector $\noise_i = [\noise_i^1, \dots, \noise_i^n]^\top$, we can write out for $\est{\param}_i$ that
\begin{align}
	& \sqrt{n} \prn{\est{\param}_i - T_i \param} = \underbrace{\prn{X\iplus^\top X\iplus/n}^{-1}}_{\mathrm{(I)}}  \cdot \underbrace{\frac{1}{\sqrt{n}} X\ipm^\top \brc{(X \imin - X\iplus \scov\iplus^{-1} \scov\ipm)\param\imin  + \noise_i}}_{\mathrm{(II)}}. \label{eq:local-estimate-clt}
\end{align}
For (II), note that
\begin{align*}
	&  \frac{1}{\sqrt{n}} X\ipm^\top \brc{(X \imin - X\iplus \scov\iplus^{-1} \scov\ipm)\param\imin  + \noise_i}  = \frac{1}{\sqrt{n}} \sum_{k=1}^n x\iplus^j \brc{(x\imin^j - \scov\imp \scov\iplus^{-1} x\iplus^j)^\top \param\imin + \noise_i^j }.
\end{align*}
The summands are independent mean zero random vectors, since 
\begin{align*}
	& \E \brk{x\iplus^j \brc{(x\imin^j - \scov\imp \scov\iplus^{-1} x\iplus^j)^\top \param\imin}}  = \prn{\E \brk{x\iplus^j {x\imin^j}^\top} - \E \brk{x\iplus^j {x\iplus^j}^\top} \scov\iplus^{-1} \scov\ipm } \param\imin \nonumber \\
	& = \prn{\scov\ipm - \scov\iplus \scov\iplus^{-1} \scov\ipm} \param\imin = 0,
\end{align*}
and $\E [x\iplus^j \noise_i^j] = \E [x\iplus^j] \cdot \E [\noise_i^j] = 0$. Denote by $z\iplus^j := x\imin^j - \scov\imp \scov\iplus^{-1} x\iplus^j$ and we can infer from the above display that $x\iplus$ and $z\iplus$ are uncorrelated.
(II) is then asymptotically normal by CLT with limiting covariance (suppressing the superscript $j$ below)
\begin{align}
	& \cov \prn{x\iplus \brc{(x\imin - \scov\imp \scov\iplus^{-1} x\iplus)^\top \param\imin + \noise_i }} = \Ep \brk{x\iplus  \param\imin^\top z\iplus  z\iplus^\top  \param\imin x\iplus^\top} + \Ep \brk{\noise_i^2 x\iplus  x\iplus^\top} \nonumber \\
	& = \Ep \brk{x\iplus  \param\imin^\top z\iplus  z\iplus^\top  \param\imin x\iplus^\top} + \sigma^2 \scov\iplus := Q_i. \label{eq:def-Q-i}
\end{align}

If $X_i$ are Gaussian random vectors, we can additionally have independence between $z\iplus$ and $x\iplus$ by zero correlation. Therefore
\begin{align*}
	& \Ep \brk{x\iplus  \param\imin^\top z\iplus  z\iplus^\top  \param\imin x\iplus^\top}  = \Ep \brk{x\iplus  \param\imin^\top \Ep \brk{z\iplus z\iplus^\top}  \param\imin x\iplus^\top} \nonumber \\
	& = \param\imin^\top  \cov \prn{ x\imin- \scov\imp \scov\iplus^{-1} x\iplus} \param\imin \cdot \Ep \brk{x\iplus x\iplus^\top} = \param\imin^\top  \prn{\scov\imin - \scov\imp \scov\iplus^{-1} \scov\ipm} \param\imin \cdot \scov\iplus   = \norm{\param\imin}_{\Gamma\imin}^2 \scov\iplus,
\end{align*}
and $Q_i =  (\norm{\param\imin}_{\Gamma\imin}^2 + \sigma^2)\scov\iplus$.

We proceed to show $C(W_1, \cdots, W_n) \succeq C\opt$ under general feature distribution $\mc{P}$ and $W_i\opt := \scov\iplus Q_i^{-1} \scov\iplus$. By Slutsky theorem, (I) converges to $\scov\iplus^{-1}$ in probability and we can conclude from Eq.~\eqref{eq:local-estimate-clt} that
\begin{align} \label{eq:asymptotic-normality-est-i}
	\sqrt{n} \prn{\est{\param}_i - T_i \param} \cd \normal\prn{0, \scov\iplus^{-1} Q_i \scov\iplus^{-1}}.
\end{align}
Further from $\est{\param} = \prnbig{\sum_{i=1}^m T_i^\top W_i T_i}^{-1} \prnbig{\sum_{i=1}^m T_i^\top W_i \est{\param}_i}$, it follows that
\begin{align*}
	\sqrt{n} \prn{\est{\param}_i - \param} = \normal(0, C(W_1, \cdots, W_n))
\end{align*}
where
\begin{align}
	& C(W_1, \cdots, W_n) = \prn{\sum_{i=1}^m T_i^\top W_i T_i}^{-1} \cdot \prn{\sum_{i=1}^m T_i^\top W_i {W_i\opt}^{-1} W_i T_i}   \cdot \prn{\sum_{i=1}^m T_i^\top W_i T_i}^{-1}. \label{eq:def-C-function}
\end{align}
With the choice of $W_i = W_i\opt$, we achieve the claimed lower bound for asymptotic covariance as in this case $C(W_1, \cdots, W_m) = \prnbig{\sum_{i=1}^m T_i^\top W_i\opt  T_i }^{-1}$. It thus remains to show
\begin{align*}
	C(W_1, \cdots, W_n) \succeq \prn{\sum_{i=1}^m T_i^\top  W_i\opt T_i}^{-1} = C\opt.
\end{align*}
To prove the above claim, we construct auxiliary matrices $M_i$ as
\begin{align*}
	M_i & = \begin{bmatrix}
		T_i^\top W_i\opt T_i  & T_i^\top W_i T_i \\
		T_i^\top W_i T_i &   T_i^\top W_i  {W_i\opt}^{-1} W_i T_i
	\end{bmatrix}  = \begin{bmatrix} T_i^\top {W_i\opt}^\half  \\  T_i^\top W_i {W_i\opt}^{-\half} \end{bmatrix} \begin{bmatrix} T_i^\top {W_i\opt}^\half  \\  T_i^\top W_i {W_i\opt}^{-\half} \end{bmatrix}^\top \succeq 0.
\end{align*}
Therefore
\begin{align*}
	\sum_{i=1}^m M_i & = \begin{bmatrix}
		{C\opt}^{-1} & \sum_{i=1}^m T_i^\top W_i T_i \\
		\sum_{i=1}^m T_i^\top W_i T_i  & \sum_{i=1}^m   T_i^\top W_i  {W_i\opt}^{-1} W_i T_i
	\end{bmatrix} \succeq 0.
\end{align*}
As the Schur complement is also p.s.d.\, we can conclude with
\begin{align*}
	0 &  \preceq {C\opt}^{-1} - \prn{ \sum_{i=1}^m T_i^\top W_i T_i} \cdot \prn{\sum_{i=1}^m   T_i^\top W_i   {W_i\opt}^{-1} W_i T_i}^{-1} \nonumber \\ & \qquad \cdot \prn{ \sum_{i=1}^m T_i^\top W_i T_i} =  {C\opt}^{-1}  - C(W_1, \cdots, W_n)^{-1}.
\end{align*}


\subsection{Proof of \Cref{cor:upper-bound-local}}
\label{sec:proof-upper-bound-local}
	We first prove (i) and asymptotic normality of $\sqrt{n} (\est{\param}\collab - \param) \cd \normal\left(0, C\gauss \right)$. We point out that Theorem~\ref{thm:upper-bound} is not directly applicable as we use estimated weights that reuse the training data. We claim consistency for $\est{W}\gauss_i \cp W\gauss$, and under this premise, the proof is rather straightforward since we can write
	\begin{align*}
		\sqrt{n} \prn{\est{\param}\collab - \param} = \prn{\sum_{i=1}^m T_i^\top \est{W}_i\gauss T_i}^{-1} \prn{\sum_{i=1}^m T_i^\top \est{W}_i\gauss (\est{\param}_i - T_i\param)}.
	\end{align*}
	With the asymptotic normality established for $\sqrt{n} (\est{\param}_i - T_i \param)$ in Eq.~\eqref{eq:asymptotic-normality-est-i}, Slutsky's theorem and continuous mapping theorem, we can conclude that $\sqrt{n} (\est{\param}\collab - \param) \cd \normal\left(0, C\gauss \right)$. Now it remains to showing $\est{W}\gauss_i \cp W\gauss$, this is from Slutksy's theorem applied to $\est{W}\gauss_i = \est{\Sigma}\iplus / \est{R}_i$ and the weak law of large numbers as follows
	\begin{align*}
		\est{\Sigma}\iplus = \frac{X\iplus^\top X\iplus}{n} \cp \Sigma\iplus, \qquad \est{R}_i = \frac{1}{n}\|X\iplus  \hparam_i - y\|_2^2 \cp \Ep [\norm{x\iplus^\top T_i \param - y_i}_2^2],
	\end{align*}
	where
	\begin{align*}
		& \Ep [\norm{x\iplus^\top T_i \param - y_i}_2^2] = \Ep [\norm{x\iplus^\top \scov\iplus^{-1} \scov\ipm \param\imin - x \imin^\top \param \imin }_2^2] + \sigma^2 \nonumber \\
		& = \norm{\param\imin}_{\cov \prn{x \imin - \scov\imp \scov\iplus^{-1} x\iplus}}^2 + \sigma^2 = \norm{\param\imin}_{\Gamma\imin}^2 + \sigma^2.
	\end{align*}

	We proceed to prove (ii). Applying delta method to the mapping $\param \mapsto T_i \param, \R^d \to \R^{d_i}$ on $\est{\param}(W_1\opt, \cdots, W_m\opt)$ immediately yields the asymptotic normality for $\est{\param}_i\collab$. It only remains to show $T_i C\opt T_i^\top \preceq {W_i\opt}^{-1}$.
	
	Identify ${W_i\opt}^{-1} - T_i C\opt T_i^\top$ as the Schur complement for the block matrix
	\begin{align*}
		M = \begin{bmatrix}
			{W_i\opt}^{-1} & T_i \\
			T_i^\top & {C\opt}^{-1}
		\end{bmatrix},
	\end{align*}
	and it suffices to show $M \succeq 0$. This follows from ${C\opt} = (\sum_{i=1}^m T_i^\top W_i\opt T_i)^{-1}$ and thus
	\begin{align*}
		M & = \begin{bmatrix}
			{W_i\opt}^{-1} & T_i \\
			T_i^\top & \sum_{j=1}^m T_j^\top W_j\opt T_j
		\end{bmatrix} \succeq \begin{bmatrix}
		{W_i\opt}^{-1} & T_i \\
		T_i^\top & T_i^\top W_i\opt T_i 
	\end{bmatrix}   = \begin{bmatrix}
	{W_i\opt}^{-\half} \\
	T_i^\top {W_i\opt}^{\half}
\end{bmatrix} \begin{bmatrix}
	{W_i\opt}^{-\half} \\
	T_i^\top {W_i\opt}^{\half}
\end{bmatrix}^\top \succeq 0.
	\end{align*}