\section{Experimental Details}
\subsection{Census Experimental Details}\label{sec:experiment-census-details}
% Figure environment removed


We use the 15 of the 17 features in the ACSTravelTime dataset---which include Age, Educational Attainment, Marital Status, Sex, Disability record, Mobility status, Relationship, etc. More specifically, using the notation from \cite{Ding2021RetiringAN}, we choose to keep the 'AGEP', 'SCHL', 'MAR', 'SEX', 'DIS',
'MIG', 'RELP', 'RAC1P', 'PUMA', 'CIT', 'OCCP', 'JWTR', 'POWPUMA', and 'POVPIP' features. We choose to exclude the State code (ST) and Employment Status of Parents (ESP) as a quick way to bypass low-rank covariance matrix issues. We turn the columns 'MAR', 'SEX', 'DIS', 'MIG', 'RAC1P', 'CIT', 'JWTR' into one-hot vectors. We make use commute time 'JWMNP' as the target variable. We clean our data by making sure AGEP (Age) must be greater than 16, PWGTP (Person weight) must be greater than or equal to 1, ESR (Employment status recode) must be equal to 1 (employed), and JWMNP (Travel time to work) is greater than 0. We normalize our features and targets by centering and dividing by the standard deviation computed from the training data. The California datacenter has access to all of the features. The New York datacenter has access to all categories except 'AGEP'. The Texas datacenter has access to all but 'AGEP', 'SCHL'. The Florida datacenter has access to all but  'AGEP', 'SCHL', 'MAR', 'SEX', and the Illinois datacenter has access to all but 'AGEP', 'SCHL', 'MAR', 'SEX', 'DIS', 'MIG'.


\subsection{Synthetic Experiments}
\label{sec:experiment-synthetic}
% Figure environment removed

We start with a synthetic experiment where we generate $m=30$ agents observing some subset of $d=30$ features. Ten of the agents will have access to random subsets of $20$ of the features. The other twenty agents will have access to random subsets of $15$ of the features. Each agent will have $n$ samples which we vary in this experiment. We sample the features from a $\normal(0, \Sigma)$ distribution. We generate $\Sigma$ by first generating $d$ eigenvalues by sampling $d$ times from a uniform $[0, 1]$ distribution. We randomly select $3$ eigenvalues to multiply by $10$ and use these eigenvalues to populate the diagonal of a diagonal matrix $\Lambda$. Then we use a randomly generated orthogonal matrix $W$ to form $\Sigma \defeq W\Lambda W^T$. We plot a heatmap of $\Sigma$ in \Cref{fig:gauss-cov-heatmap}. For each method that we test, we run $20$ trials to form $95\%$ confidence intervals.

We compare our method \textsc{Collab}, against the Imputation and RW-Imputation methods we outlined in \Cref{sec:experiment-census}. 
After we train each of these methods using the data on our $30$ agents, we measure how well these methods perform in using the features of a test-agent with access to $20$ of the total $30$ features to predict outputs. 
We will also compare our methods against Naive-Local, where we only use the $n$ training datapoints of the $20$ features our test-agent has access to, also described in \Cref{sec:experiment-census}. 
We plot this result in \Cref{fig:gauss-local-pred}.

We also compare our methods in an alternative setting where the test-center of interest has access to all $30$ features. This setup models the setting where we are interested making the best possible predictions from all of the features available. In this experiment, we compare against Naive-Collab, Optimized-Naive-Collab, described in \Cref{sec:experiment-census}. We note that Optimized-Naive-Collab uses fresh labeled samples without any missing features during gradient descent, so in this sense, Optimized-Naive-Collab is more powerful than our method. We plot this result in \Cref{fig:gauss-full-pred}.

We see that reweighting is important; this is why \textsc{Collab} and RW-Imputation outperform the unweighted Imputation method. Our \textsc{Collab} method improves over the Naive-Local approach, meaning that the agents are benefiting from sharing information.
\textsc{Collab} also matches the performance of the RW-Imputation method, despite only needing to communicate the learned parameters of each agent's model, as opposed to all of the data on each agent. The Naive-Collab approaches level out very quickly, likely reflecting the fact that these methods are biased, as the covariance of our underlying data is far from isotropic.
