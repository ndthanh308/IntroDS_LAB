\section{Proofs for \Cref{sec:lower-bounds}}
\label{sec:proofs-lower-bounds}


We will use the van Trees inequality to prove our lower bound shown. In particular, we will use a slight modification to Theorem 4 of \cite{Gassiat2014RevisitingTV}, which we state as a corollary below here. 
Throughout this section, we let $\psi: \R^d \to \R^s$ be an absolutely continuous function. The distribution $P_\param$ in the family $\{P_\param\}_{\param \in \R^d}$ is assumed to have density $p_\param$ which satisfies $\int_{\R^d} \ltwo{\nabla p_\param(x)}^2 dx < \infty$.
Let $P_\param^j$ for $j \in [m]$ denote the distribution over either $\tparam_j^n$ or $y_j \in \R^n$.
Let $\finfo_i^n(\param)$ denote the Fisher Information of $P_\param^i$, and let $\finfo^n(\param) = \sum_{i = 1}^m \finfo_i^n(\param)$ denote the Fisher Information of $P_\param$. We note that $P_\param$ is allowed to depend on $n$.


\begin{corollary}[\citet{Gassiat2014RevisitingTV}]\label{thm:van-trees}
 Let $\psi: \R^d \to \R^s$ be an absolutely continuous function such that $\nabla \psi(\param)$ is continuous at $\param_0$. For all $n$, let all distributions $P_\param$ in the family $\{P_\param\}_{\param \in \R^d}$ have density $p_\param$ which satisfies $\int_{\R^d} \ltwo{\nabla p_\param(x)}^2 dx < \infty$. 
 If $\lim_{c \to \infty}\lim_{n \to \infty} \sup_{\ltwo{h} < 1}\finfo^n(\param_0 + ch / \sqrt{n}) / n$ exists almost surely and is positive definite, denote it by $\rho$. Then
 for all sequences $(\hparam_n)_{n \geq 1}$ of statistics $S_n: \mc{X}^n \to \R^s$ and for all $u \in \R^s$
 \begin{align*}
    \liminf_{c \to \infty} \liminf_{n \to \infty} \sup_{\norm{h} < 1}
    \E^n_{\param_0 + \frac{ch}{\sqrt{n}}} \left[ 
        \left\<\sqrt{n} \left(\hparam_n - \psi\left(\param_0 + \frac{ch}{\sqrt{n}}\right)\right), u \right\>^2
    \right]
    \geq 
        u^\top \nabla \psi(\param_0)^\top \rho\inv \nabla \psi (\param_0) u
\end{align*}
\end{corollary}
\begin{proof}
    The main difference between our version of the proof and the one presented in Theorem 4 of \citet{Gassiat2014RevisitingTV} is that we do not assume $\finfo^n = n \finfo$. We also select $\ell(x) = \< u, x\>^2$ in particular. All the steps and notation remain the same except with $n\finfo$ replaced with $\finfo^n$ up until equation (13), which we define with a modified choice of $\Gamma_{c,n}$
    \begin{align*}
        \Gamma_{c,n} \defeq 
        \left(\int_{\mc{B}_{p}([0], 1)}  \nabla\psi(\param_0 + ch /\sqrt{n}) q(h) dh \right)^\top
        \left(
            \frac{1}{c^2} \finfo_q + \frac{1}{n}\int_{\mc{B}_{p}([0], 1)}  \finfo^n(\param_0 + ch /\sqrt{n}) q(h) dh
        \right)\inv\\
        \times \left(\int_{\mc{B}_{p}([0], 1)}  \nabla\psi(\param_0 + ch /\sqrt{n}) q(h) dh \right).
    \end{align*}
   By definition of $\rho$, with probability 1,
    \begin{align*}
        \lim_{c \to \infty} \lim_{n \to \infty} \Gamma_{c, n} = \nabla \psi(\param_0)^\top \rho\inv \nabla \psi (\param_0)
    \end{align*}
\end{proof}


\subsection{Proof of \Cref{thm:weak-global-lb}}
\label{sec:proof-weak-global-lb}
    We will apply \Cref{thm:van-trees} and apply it to two different choices of $\psi$ to get the full feature minimax bound and missing feature minimax bound respectively. 
    For notational simplicty, let $P_\param$ denote the distribution over $\{\tparam_i^n\}_{i \in [m]}$ induced by $\param$. 
    $P_\param$ is in the exponential family, so the conditions of \Cref{thm:van-trees} are satisfied.

    We begin by computing the Fisher Information.
    Let $P_\param^j$ for $j \in [m]$ denote the distribution over
     $\tparam_j^n \in \R^{d_j}$.
    Let $\finfo_i^n(\param)$ denote the Fisher Information of $P_\param^i$, and 
    let $\finfo^n(\param) = \sum_{i=1}^m \finfo_i^n$ denote the Fisher Information of $P_\param$.
    Let $x\iplus$ denote an arbitrary row of $X\iplus$. Let $x\imin$ be drawn from $\normal(\mu\imin(x\iplus), \Gamma\imin)$. 
    Some straightforward calculations tell us 
    $\mu\imin(x\iplus) = \scov\imp \scov\iplus\inv x\iplus$ and $\Gamma\imin = \scov\imin - \scov\imp \scov\iplus\inv\scov\ipm$. From this we can deduce that $\param\imin^T x\imin$ is distributed as $\normal(\mu\imin^T \param\imin, \param\imin^T \Gamma\imin \param\imin)$; we use $\mu\imin$ in place of $\mu\imin(x\imin)$ for simplicity. And $y_i$ is distributed as $P_\param^i$ which is $\normal(\param^T\gamma, \param\imin^T \Gamma\imin \param\imin + \sigma^2 )$ where $\gamma\defeq [ x\iplus^T\projm\iplus, \mu\imin^T \projm\imin]^T$. From this we can deduce that $P_\param^i$ is $\normal\left(J_i \projm_i\param, \beta_i\inv \hscov\iplus\inv\right)$, where $\beta_i\inv \defeq \frac{\param\imin \Gamma\imin \param\imin + \sigma^2}{n}$; let $p_\param^i$ denote its density. 
    We know $\finfo^n(\param) = \sum_{i=1}^m \finfo_i^n(\param)$ due to independence. All that remains is to compute $\finfo_i^n(\param)$.
\begin{align*}
    \finfo_i^n(\param) = \int \nabla_\param \log p_\param^i(z) [\nabla_\param \log p_\param^i(z) ]^T p_\param^i(z) dz.
\end{align*}

We know that for some constant $C$,
\begin{align*}
    \log p_i^\param(z) &= 
    C + \frac{d_i}{2}\log(\beta_i) - \frac{\beta_i}{2} \ltwo{\hscov^\half\iplus\param\iplus + \hscov\iplus^\half \scov\iplus\inv \scov\ipm \param\imin - \hscov\iplus^\half z}^2 .
\end{align*}
Taking derivaties we get that 
\begin{align*}
    \nabla_{\param\iplus}  \log p_i^\param(z) &= 
    -\beta_i \left[ \hscov\iplus \param\iplus + \hscov\iplus \scov\iplus\inv \scov\ipm \param\imin - \hscov\iplus z \right]\\
    \nabla_{\param\imin}  \log p_i^\param(z) &= \left[ -\frac{d_i}{n} + \ltwo{\hscov^\half\iplus\param\iplus + \hscov\iplus^\half \scov\iplus\inv \scov\ipm \param\imin - \hscov\iplus^\half z}^2 \right]\beta_i \Gamma\imin\param\imin\\
    &\quad  + \left[\scov\imp \scov\iplus\inv \hscov\iplus \param\iplus + \scov\imp \scov\iplus\inv \hscov\iplus \scov\iplus\inv \scov\ipm \param\imin- \scov\imp \scov\iplus\inv \hscov\iplus z \right]\beta_i
\end{align*}
Let $b^2 = \ltwo{\hscov^\half\iplus\param\iplus + \hscov\iplus^\half \scov\iplus\inv \scov\ipm \param\imin - \hscov\iplus^\half z}^2$.
Now we compute the expectation over outer products:
\begin{align*}
    &\E[\nabla_{\param\iplus}  \log p_i^\param(z) \nabla_{\param\iplus}  \log p_i^\param(z)^T] = \beta_i \hscov\iplus \\
    &\E[\nabla_{\param\iplus}  \log p_i^\param(z) \nabla_{\param\imin}  \log p_i^\param(z)^T] = \beta_i^2 \hscov\iplus \beta_i\inv \hscov\iplus\inv \hscov\iplus \scov\iplus\inv \scov\ipm
    = \beta_i \hscov\iplus \scov\iplus\inv \scov\ipm\\
    &\E[\nabla_{\param\imin}  \log p_i^\param(z) \nabla_{\param\imin}  \log p_i^\param(z)^T] =\beta_i \scov\imp\scov\iplus\inv \hscov\iplus \scov\iplus\inv \scov\ipm\\
    &\qquad\qquad + \left( 
        \frac{d_i^2}{n^2} + \E[b^2] \frac{2d_i}{n}+ \E[b^4]
    \right)\beta_i^2 \Gamma\imin \param\imin \param\imin^T \Gamma\imin\\
    &\qquad= \beta_i \scov\imp\scov\iplus\inv \hscov\iplus \scov\iplus\inv \scov\ipm + \left( 
        \frac{d_i^2}{n^2} +  \frac{2\beta_i\inv d_i^2}{n}+ \beta_i^{-2}(2d_i + d_i^2)
    \right)\beta_i^2 \Gamma\imin \param\imin \param\imin^T \Gamma\imin
\end{align*}

\begin{align*}
    \finfo_i^n(\param) &= \int \nabla_\param \log p_\param^i(z) [\nabla_\param \log p_\param^i(z) ]^T p_\param^i(z) dz\\
    &= \int \projm_i^T \begin{bmatrix}
        \nabla_{\param\iplus} \log p_\param^i(z)\\
        \nabla_{\param\imin} \log p_\param^i(z)
    \end{bmatrix}
    \begin{bmatrix}
        \nabla_{\param\iplus} \log p_\param^i(z)^T &
        \nabla_{\param\imin} \log p_\param^i(z)^T
    \end{bmatrix}
    \projm_i p_\param^i(z)dz\\
    &=  \frac{n}{\sigma^2 + \param\imin^T\Gamma\param\imin}
    \projm_i^T\begin{bmatrix}
        \hscov\iplus & \hscov\iplus\scov\iplus\inv\scov\ipm\\
        \scov\imp \scov\iplus\inv \hscov\iplus  & \scov\imp\scov\iplus\inv \hscov\iplus \scov\iplus\inv \scov\ipm 
    \end{bmatrix} \projm_i 
    \\
    &\qquad\qquad +
    \projm_i^T
    \begin{bmatrix}
        0 & 0\\
        0 & \left( 
            \frac{d_i^2 \beta_i^2}{n^2} +  \frac{2\beta_i d_i^2}{n}+ 2d_i + d_i^2
        \right)\Gamma\imin \param\imin \param\imin^T \Gamma\imin
    \end{bmatrix}
    \projm_i\\
    &= \frac{n}{\sigma^2 + \param\imin^T\Gamma\param\imin} \left(Q_i + o_n(1)\right)
\end{align*}
The $o_n(1)$ term is due to strong law of large numbers.
From this we know that, with probability 1,
\begin{align*}
    \lim_{c\to \infty}\lim_{n \to \infty} \sup_{\ltwo{h} < 1}\frac{\finfo^n(\param_0 + ch / \sqrt{n})}{n} &= \sum_{i=1}^m  \frac{1}{\sigma^2 + \param\imin^T\Gamma\param\imin} Q_i =: \rho
\end{align*}

Applying \Cref{thm:van-trees} with $\psi \R^d \to \R^d$ as the identity function $\psi(x) = x$ gives the full-feature minimax lower bound. Applying \Cref{thm:van-trees} with $\psi\R^d \to \R^{d_i}$ as $\psi(x) = T_i x$ gives the missing-feature minimax lower bound.





\subsection{Proof of \Cref{thm:strong-global-lb}}
\label{sec:proof-strong-global-lb}
We will apply \Cref{thm:van-trees} and apply it to two different choices of $\psi$ to get the full feature minimax bound and missing feature minimax bound respectively. 
    For notational simplicity, we will use $P_\param$ in place of $P_\param^{y}$.
    $P_\param$ is in the exponential family, so the conditions of \Cref{thm:van-trees} are satisfied.

We begin by computing the Fisher Information.
Let $P_\param^j$ for $j \in [m]$ denote the distribution over $y_j \in \R^n$.
    Let $\finfo_i^n(\param)$ denote the Fisher Information of $\P_\param^i$, and let $\finfo^n(\param) = \sum_{i=1}^m \finfo_i^n(\param)$ denote the Fisher Information of $P_\param$.

    Let $x\kth_i, y\kth_i$ be the $k$th sample from agent $i$.
    We will let $\finfo_i\kth(\param)$ be the fisher information of $y\kth_i$. We know that $\finfo_i^n(\param) = \sum_{k=1}^n\finfo_i\kth(\param)$ by independence. 
    Some straightforward calculations tell us that $x\kth\imin$ is distributed as $\normal(\mu, \Gamma)$ where $\mu = \scov\imp \scov\iplus\inv x\kth\iplus$ and $\Gamma = \scov\imin - \scov\imp \scov\iplus\inv\scov\ipm$. From this we can deduce that $\param\imin^T x\kth\imin$ is distributed as $\normal(\mu^T \param\imin, \param\imin^T \Gamma \param\imin)$. And $y_i\kth$ is distributed as 
     $\normal(\param^T\gamma, \param\imin^T \Gamma \param\imin + \sigma^2 )$ 
    where $\gamma\defeq \projm\iplus^T x\iplus\kth + \projm\imin^T \mu$.
    
    
    Let $\phi \defeq \frac{z - \gamma^T \param}{\sigma^2 +\param\imin^T \Gamma\param\imin}$ and $\Delta \defeq \phi^2 - \frac{1}{\sigma^2 + \param\imin^T \Gamma \param\imin}$. Using $p_\param^{ik}$ denote the density of $x\kth\imin, y\kth_i$, we can calculate the derivative of the log density
    \begin{align*}
        \nabla_{\param\iplus} \log p_\param^{ik}(z) &= \frac{z - \param\iplus^T x\kth\iplus - \param\imin^T \mu}{\sigma^2 + \param\imin \Gamma \param\imin}x\kth\iplus = \phi x\kth\iplus\\
        \nabla_{\param\imin} \log p_\param^{ik}(z) &= \Delta \Gamma \param\imin + \phi \mu.
    \end{align*}

    Using the facts that $\E[\phi]=0$, $\E[\phi^2]=\frac{1}{\sigma^2 + \param\imin\Gamma\param\imin}$, $\E[\phi\Delta]=0$, and $\E[\Delta^2]= \frac{2}{(\sigma^2 + \param\imin\Gamma\param\imin)^2}$, where the expectation is an integral over $z$, we have that 
    \begin{align*}
        &\finfo_i\kth(\param) = \int \nabla_\param \log p_\param^{ik}(z) [\nabla_\param \log p_\param^{ik}(z) ]^T p_\param^{ik}(z) dz\\
        &= \int \projm_i^T \begin{bmatrix}
            \nabla_{\param\iplus} \log p_\param^{ik}(z)\\
            \nabla_{\param\imin} \log p_\param^{ik}(z)
        \end{bmatrix}
        \begin{bmatrix}
            \nabla_{\param\iplus} \log p_\param^{ik}(z)^T &
            \nabla_{\param\imin} \log p_\param^{ik}(z)^T
        \end{bmatrix}
        \projm_i p_\param^{ik}(z)dz\\
        &= \projm_i^T 
        \begin{bmatrix}
            \E[\phi^2] x\iplus\kth (x\iplus\kth)^T & \E[\phi x\iplus\kth (\Delta \Gamma \theta\imin + \phi \mu)^T]\\
            \E[(\Delta \Gamma \theta\imin + \phi \mu) (\phi x\iplus\kth)^T]
            & \E[(\Delta \Gamma \theta\imin + \phi \mu)(\Delta \Gamma \theta\imin + \phi \mu)^T] 
        \end{bmatrix}\projm_i\\
        &= 
        \frac{1}{\sigma^2 + \param\imin\Gamma\param\imin}
        \projm_i^T 
        \begin{bmatrix}
             x\iplus\kth (x\iplus\kth)^T &  x\iplus\kth \mu^T\\
            \mu(x\iplus\kth)^T
            & \mu\mu^T +  \frac{2}{\sigma^2 + \param\imin^T\Gamma\param\imin} \Gamma \param\imin \param\imin^T\Gamma
        \end{bmatrix}\projm_i\\
        &= 
        \frac{1}{\sigma^2 + \param\imin\Gamma\param\imin}
        \projm_i^T
        \begin{bmatrix}
             x\iplus\kth (x\iplus\kth)^T &  x\iplus\kth (x\iplus\kth)^T \scov\iplus\inv \scov\ipm\\
             \scov\imp \scov\iplus\inv x\kth\iplus(x\iplus\kth)^T
            & \scov\imp \scov\iplus\inv x\kth\iplus(x\iplus\kth)^T\scov\iplus\inv \scov\ipm + \frac{2}{\sigma^2 + \param\imin^T\Gamma\param\imin} \Gamma \param\imin \param\imin^T\Gamma
        \end{bmatrix}\projm_i.
    \end{align*}
    From this we can sum over 
    \begin{align*}
       \finfo_i^n(\param)&= \sum_{k=1}^n\finfo_i\kth(\param)\\ 
&= \frac{n}{\sigma^2 + \param\imin^T\Gamma\param\imin}\projm_i^T\begin{bmatrix}
    \hscov\iplus &  \hscov\iplus \scov\iplus\inv \scov\ipm\\
    \scov\imp \scov\iplus\inv \hscov\iplus
   & \scov\imp \scov\iplus\inv\hscov\iplus\scov\iplus\inv \scov\ipm + \frac{2}{\sigma^2 + \param\imin^T\Gamma\param\imin} \Gamma \param\imin \param\imin^T\Gamma
\end{bmatrix} 
    \projm_i\\
        &= \frac{n}{\sigma^2 + \param\imin^T\Gamma\param\imin}\left( Q_i + o_n(1) + \projm_i^T\begin{bmatrix}
                0 & 0\\
                0 & \frac{2}{\sigma^2 + \param\imin^T\Gamma\param\imin} \Gamma \param\imin \param\imin^T\Gamma
            \end{bmatrix}\projm_i
            \right)
    \end{align*}
The $o_n(1)$ term is due to strong law of large numbers. From this we know that, with probability 1
    \begin{align*}
        &\lim_{c\to \infty}\lim_{n \to \infty} \sup_{\ltwo{h} < 1}\frac{\finfo^n(\param_0 + ch / \sqrt{n})}{n} \\&\qquad\qquad= \sum_{i=1}^m \frac{1}{\sigma^2 + \param\imin^T\Gamma\param\imin}\left( Q_i + \projm_i^T\begin{bmatrix}
            0 & 0\\
            0 & \frac{2}{\sigma^2 + \param\imin^T\Gamma\param\imin} \Gamma \param\imin \param\imin^T\Gamma
        \end{bmatrix}\projm_i
        \right) =: \rho
    \end{align*}
    Applying \Cref{thm:van-trees} with $\psi \R^d \to \R^d$ as the identity function $\psi(x) = x$ gives the full-feature minimax lower bound. Applying \Cref{thm:van-trees} with $\psi\R^d \to \R^{d_i}$ as $\psi(x) = T_i x$ gives the missing-feature minimax lower bound. 

    One final transformation remains to get the form of this lower bound to match the one in the theorem statement. We know that from Cauchy-Schwartz that for all $u\in\R^{d - d_i}$
    \begin{align*}
        \frac{u^T \Gamma \param\imin \param\imin^T \Gamma u}{\param\imin^T \Gamma \param\imin} = \frac{(u^T \Gamma^\half \Gamma^\half \param\imin)^2}{\param\imin^T \Gamma \param\imin} \leq u^T \Gamma u.
    \end{align*}
    Using this fact and the definition of $\Gamma$ and $Q_i$ we have that 
    \begin{align*}
        \frac{1}{\sigma^2 + \param\imin^T\Gamma\param\imin}\left( Q_i + \projm_i^T\begin{bmatrix}
            0 & 0\\
            0 & \frac{2}{\sigma^2 + \param\imin^T\Gamma\param\imin} \Gamma \param\imin \param\imin^T\Gamma
        \end{bmatrix}\projm_i \right) \preceq  \frac{2n}{\sigma^2 + \param\imin^T\Gamma\param\imin} \Sigma.
    \end{align*}
    Using this bound gives our final result.





\subsection{Proof of \Cref{cor:lower-bounds-comparable}}
The existence of the limits is a consequence of strong law of large numbers. To further show the inequality in the limit, we note that
\begin{align*}
	& \frac{1}{m} \sum_{i=1}^m \prn{\frac{\scov }{\sigma^2 + \param\imin^T\Gamma\imin \param\imin}  - \frac{T_i^\top \Sigma\iplus T_i}{\sigma^2 + \param\imin^T\Gamma\imin \param\imin}} = \frac{1}{m} \sum_{i=1}^m \frac{\Pi_i^\top \begin{bmatrix}
			0 & 0\\
			0 & \Gamma\imin
		\end{bmatrix} \Pi_i}{\sigma^2 + \param\imin^T\Gamma\imin \param\imin} \nonumber \\
	& \preceq \frac{1}{m} \sum_{i=1}^m \frac{\Pi_i^\top \begin{bmatrix}
			0 & 0\\
			0 & \Gamma\imin
		\end{bmatrix} \Pi_i}{\sigma^2} \preceq \frac{1}{m} \sum_{i=1}^m \frac{\Pi_i^\top \begin{bmatrix}
			0 & 0\\
			0 & \scov\imin
		\end{bmatrix} \Pi_i}{\sigma^2}  \to \frac{p \mathrm{diag}(\scov) + p^2 (\scov - \mathrm{diag}(\scov))}{\sigma^2},
\end{align*}
where the last step holds with probability one by strong law of large numbers. This is true as by our random missing model, $\scov_{ij}$ is not observed with probability $p$ if $i=j$, and $p^2$ if $i \neq j$. We can further derive that
\begin{align*}
	& \frac{p \mathrm{diag}(\scov) + p^2 (\scov - \mathrm{diag}(\scov))}{\sigma^2} \preceq \frac{p\lambda_1(\Sigma) I}{\sigma^2} \preceq \frac{p \kappa \scov}{\sigma^2} \preceq \frac{p\lambda_1(\Sigma) I}{\sigma^2} \nonumber \\
	& \stackrel{\mathrm{(i)}}{\preceq} \frac{p \kappa (\sigma^2 + \norm{\param}_\scov^2)}{\sigma^2} \frac{1}{m} \sum_{i=1}^m \frac{\scov }{\sigma^2 + \param\imin^T\Gamma\imin \param\imin}.
\end{align*} 
In (i), we make use of the fact that
\begin{align*}
	\scov \succeq \Pi_i^\top \begin{bmatrix}
		0 & 0\\
		0 & \Gamma\imin
	\end{bmatrix} \Pi_i
\end{align*} 
and therefore $\norm{\param}_\scov^2 \geq \norm{\param\imin}_{\Gamma\imin}^2$. By our choice of $p \leq \half\kappa^{-1} (1 + \norm{\param}_\scov^2 / \sigma^2)^{-1}$, we can conclude that
\begin{align*}
	\lim_{m \to \infty} \frac{1}{m} \sum_{i=1}^m \frac{T_i^\top \Sigma\iplus T_i}{\sigma^2 + \param\imin^T\Gamma\imin \param\imin} \succeq \lim_{m \to\infty} \frac{1}{m} \sum_{i=1}^m \frac{\scov/2}{\sigma^2 + \param\imin^T\Gamma\imin \param\imin}.
\end{align*}