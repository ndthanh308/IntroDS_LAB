\newcommand{\nth}{^{(n)}}
\section{Asymptotic Local Minimax Lower Bounds}
\label{sec:lower-bounds}
In this section, we prove asymptotic local minimax lower bounds that show \textsc{Collab} is (nearly) optimal. We work in the partially-fixed-design regime. 
For every sample $x \in \R^d$, $x\iplus\in \R^{d_i}$ is a fixed vector.  We draw $x\imin$ from $\normal(\mu\imin, \Gamma\imin)$ where $\mu\imin$ and $\Gamma\imin$ is the conditional mean and variance of $x\imin$ given $x\iplus$.
Here $\Gamma\imin$ is also the Schur complement. We draw $x\imin$ from $\normal(\mu\imin, \Gamma\imin)$.
The samples $x\iplus \in \R^{d_i}$ comprise the matrices $X\iplus\in \R^{n \times d_i}$. For all $i\in [m]$, we will assume we have an infinite sequence (w.r.t. $n$) of matrices $X\iplus$.
This partially-fixed-design scheme gives the estimators knowledge of the observed features and the distribution of the unobserved features, which is consistent with knowledge that \textsc{Collab} has access to. In this section we fix $\param \in \R^d$.
The corresponding label $y = x\iplus \param\iplus + x\imin \param\imin + \noise$, where $\noise \in \R$ is drawn from i.i.d.\ $\normal(0, \sigma^2 )$. We use $y_j \in \R^n$ to denote its vector form for the agent $j$.
To model the estimator's knowledge about the labels, we will have two observation models---one weaker and one stronger---which we will specify later  when we present our results.
 
For each observation model, we will have two types of results. The first type of result is a minimax lower bound for full-featured data; i.e., how well can estimator perform on a fresh sample without missing features. 
This type of result will concern the full-feature asymptotic local minimax risk
\begin{align*}
    \liminf_{n \to \infty}\minimax_{m, \varepsilon}(\{X\iplus\}_{i\in[m]};\statfamily_n, u) \defeq \liminf_{n \to \infty} \inf_{\bar{\param}} \sup_{\statdist \in \statfamily_n} n
\E_{Z \sim \statdist}  \<u, \bar{\param}(Z, \{X\iplus\}_{i\in[m]}) - \param\>^2.
\end{align*}
We will show that there exists a $B \in \R^{d \times d}$ such that the local minimax risk in the previous display is lower bounded by $u^T B u$ for all $u\in\R^d$. In other words, we have lower bounded the asymptotic covariance of our estimator with $B$ (with respect to the p.s.d.\ cone order). 
The second type of result is an agent specific minimax lower bound; i.e., what is the best prediction error an estimator (for the given observation model) can possibly have on a fresh sample for a given agent.
This type of result will deal with the missing-feature asymptotic local minimax risk, defined as
\begin{align*}
    \liminf_{n \to \infty}\minimax_{m, \varepsilon}^{i+}(\{X\iplus\}_{i\in[m]}; \statfamily_n, u) \defeq \liminf_{n \to \infty}\inf_{\bar{\param}} \sup_{\statdist \in \statfamily_n}
    n \E_{Z \sim \statdist}  \<u, \bar{\param}(Z, \{X\iplus\}_{i\in[m]}) - T_i \param\>^2.
\end{align*}
Similar to the first minimax error definition, we will again show that there exists a $B_i \in \R^{d_i \times d_i}$ such that the local minimax risk we just defined is lower bounded by $u^T B_i u$ for all $u\in\R^{d_i}$.
Recall \eqref{eqn:local-test-loss} for discussion surrounding why $T_i \param$ 
is the right object to compare against. 




\subsection{Weak Observation Model: Access only to local models and features} \label{sec:weak-observation}
Recall the local least squares estimator 
$\est{\param}_i = (X\iplus^\top X\iplus)^{-1} X\iplus^\top y_i$.
Let $P_{\param}^{\est{\param}}$ be a distribution over $\est{\param}_1, \ldots , \est{\param}_m$ induced by $\param$ and $(\noise_1, \ldots, \noise_m) \simiid \normal(0, \sigma^2 I_n)$.
We define the following family of distributions
    $\mc{P}^{\est{\param}}_{n,c} \defeq \{P_{\param'}^{\est{\param}} : \ltwo{\param' - \param} \leq c n^{- 1/2}\}$ 
which defines our observation model. Intuitively, in this observation model, we are constructing a lower bound for estimators which have access to the features $X_{1+}, \ldots, X_{m+}$, the population covariance $\scov$, and access to $\est{\param}_1, \ldots, \est{\param}_m$. In comparison, our estimator \textsc{Collab} only uses $\scov$ and $\est{\param}_1, \ldots \est{\param}_m$.
We present our first asymptotic local minimax lower bound result here. The proof of this result can be found in \Cref{sec:proof-weak-global-lb}.



\begin{theorem}\label{thm:weak-global-lb}
    Recall that $C\gauss \defeq (\sum_{i=1}^m T_i^\top  W_i\gauss   T_i)^{-1}$.
    For all $\in [m]$ and $n$ let the rows of $X\iplus$ be drawn i.i.d.\ from $\normal(0, \scov\iplus)$. Then for all $u\in\R^d$, with probability 1, the full-feature asymptotic local minimax risk for $\mc{P}^{\est{\param}}_{n,c}$ is bounded below as,
    \begin{align*}
        \liminf_{c \to \infty} \liminf_{n \to \infty}\minimax_{m, \varepsilon}(\{X\iplus
        \}_{i\in[m]};\mc{P}^{\est{\param}}_{n,c}, u)  \geq u^\top C\gauss u.
    \end{align*}
    For all $u\in\R^{\di}$, with probability 1, the missing-feature asymptotic local minimax risk for $\mc{P}^{\est{\param}}_{n,c}$ is bounded below as
    \begin{align*}
        \liminf_{c \to \infty} \liminf_{n \to \infty}\minimax_{m, \varepsilon}^{i+}(\{X\iplus
        \}_{i\in[m]};\mc{P}^{\est{\param}}_{n,c}, u) \geq u^\top T_i C\gauss T_i^\top u.
    \end{align*}
\end{theorem}
This exactly matches the upper bound for \textsc{Collab} we presented in \Cref{cor:upper-bound-local}.




\subsection{Strong Observation Model: Access to features and labels}


Define the family of distributions 
    $\mc{P}^y_{n,c} \defeq \{P_{\param'}^y : \ltwo{\param' - \param} \leq c n^{- 1/2}\}$ as the observation model.
Intuitively, in this model, we are constructing a lower bound for estimators having access to all of the features $X_{1+}, \ldots, X_{m+}$ and access to $y_1, \ldots y_m$. This observation model is stronger than the previous observation model because estimators now have access to the labels $y$. We note again that our estimator \textsc{Collab} only uses $\scov$ and $\est{\param}_1, \ldots \est{\param}_m$. The quantities our estimator rely on do not scale with $n$, making our estimator much weaker than other potential estimators in this observation model, as estimators are allowed to depend on $y_i$, which grows in size with $n$.
We present our second asymptotic local minimax lower bound result here, starting with defining the strong local lower bound matrix $
	C\strong := (\sum_{i=1}^m 2\scov/(\norm{\param\imin}_{\Gamma\imin}^2 + \sigma^2))\inv$. 
The proof of this result is in \Cref{sec:proof-strong-global-lb}.
\begin{theorem}\label{thm:strong-global-lb}
    For all $i\in [m]$ and $n$ let the rows of $X\iplus$ be drawn i.i.d.\ from $\normal(0, \scov\iplus)$. Then for all $u\in\R^d$, with probability 1, the full-feature asymptotic local minimax risk for $\mc{P}^{y}_{n,c}$ is bounded below as
    \begin{align*}
        \liminf_{c \to \infty} \liminf_{n \to \infty}\minimax_{m, \varepsilon}(\{X\iplus\}_{i\in[m]};\mc{P}^{y}_{n,c}, u)  \geq u^\top C\strong u .
    \end{align*}
    For all $u\in\R^{\di}$, with probability 1, the missing-feature asymptotic local minimax risk for $\mc{P}^{y}_{n,c}$ is bounded below as
    \begin{align*}
        \liminf_{c \to \infty} \liminf_{n \to \infty}\minimax_{m, \varepsilon}^{i+}(\{X\iplus\}_{i\in[m]};\mc{P}^{y}_{n,c}, u) \geq u^\top T_i C\strong T_i^\top u.
    \end{align*}
\end{theorem}

In view of the lower bound in the strong observation model and that of the weak observation model in Theorem~\ref{thm:weak-global-lb}, it is clear that the lower bound in the strong observation setting is in general smaller as 
\begin{align*}
	\scov - T_i^\top \Sigma\iplus T_i = \Pi_i^\top \begin{bmatrix}
		0 & 0\\
		0 & \Gamma\imin
	\end{bmatrix} \Pi_i \succeq 0,
\end{align*}
which further implies $C\gauss \succeq (\sum_{i=1}^m \scov/(\norm{\param\imin}_{\Gamma\imin}^2 + \sigma^2) )\inv \succeq C\strong$.

We argue that the two lower bounds are comparable in the missing completely at random \cite{Little2019StatisticalAW}. Consider for every agent $i$, each coordinate is missing independently with probability $p$. In this case, $(d_i, \scov\iplus, T_i)$ are i.i.d.\ random triplets parameterized by $p$.

\begin{corollary} \label{cor:lower-bounds-comparable}
	Under the random missingness setup with missing probability $p$, let the eigenvalue of $\Sigma$ be $\lambda_1(\Sigma) \geq \cdots \geq \lambda_d(\Sigma) > 0$ and define its condition number $\kappa = \lambda_1(\Sigma) / \lambda_d(\Sigma)$. Suppose $p \leq \half\kappa^{-1} (1 + \|\param\|_\scov^2 / \sigma^2)^{-1}$, we have the limits $\lim_{m \to \infty} mC\gauss$ and $\lim_{m \to \infty} mC\strong$ exist and
	\begin{align*}
		4\lim_{m \to \infty} mC\strong \succeq \lim_{m \to \infty} m C\gauss \succeq \lim_{m \to \infty} mC\strong.
	\end{align*}
\end{corollary}

