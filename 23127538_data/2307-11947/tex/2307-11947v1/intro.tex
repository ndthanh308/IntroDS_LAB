\section{Introduction}


\looseness=-1
Consider a set of agents that collect data to make predictions, where different agents may collect different features---because of different sensor availability or specialization---but wish to leverage shared structure to achieve better accuracy.
Concretely, suppose we have $m$ agents, where each agent $i\in[m]$ observes $n$ samples of $(x\iplus, y)$ where $x\iplus \in \R^{d_i}$ is some subset of $x\in \R^d$. We set this as a regression problem where the data $(x, y)$ has the linear relationship $y = \<x, \param\> + \noise$ for some noise variable $\noise$.
For example, these agents could be a network of satellites, each collecting data with a distinct set of sensors of varying resolution and specialization, with the purpose of estimating quantities like crop-yields \cite{Sahajpal2020UsingMM}, biomass \cite{Mansaray2020EvaluationOM}, and solar-flare intensity \cite{Jiao2019SolarFI}. Or these agents could be a group of seismic sensors, using acoustic modalities or accelerometers to predict whether an earthquake will occur \cite{Bolton2019CharacterizingAS}. 
Other examples may include networks of hospitals or phones \cite{Kairouz2019AdvancesAO}.
In these settings, the agents can share information to collaboratively train a model; however, they are limited by communication bandwidth constraints, a situation satellites and seismic sensors often face due to radio frequency spectrum scarcity and interference \cite{Curzi2020LargeCO, Physics2015ASF}. 
Without being too rigorous, we will define a communication efficient algorithm as one with communication cost that is sublinear in $n$; this definition is suited for applications with significant data volume but limited communication resources.
Can we construct a statistically optimal and communication efficient procedure to estimate $\param$?

We answer in the affirmative and introduce our estimator \textsc{Collab}. \textsc{Collab} consists of three steps: local training on all agents, aggregation on a coordinating server, and distribution back to all agents. Our algorithm is communication-efficient: each agent $i\in[m]$ syncs twice with a coordinating server and incurs communication cost scaling like $\Theta(d_i^2)$. We prove local minimax lower bounds which prove that \textsc{Collab} is (nearly) instance-optimal. 
We choose to study this problem in a stylized linear setting so that we can provide stronger guarantees for the algorithms we make. Indeed, our results which pair the exact asymptotic covariance of our estimator \textsc{Collab} with matching asymptotic local minimax lower bounds heavily rely on the linearity of our problem and would not be possible without strong structural assumptions. Having said this, the theory we develop for linear models does hint at potential methods for non-linear settings, which we discuss in \Cref{sec:discussion}.
We also acknowledge privacy considerations are important for real world systems such as hospitals. We choose to focus instead on sensor settings where privacy is less of a concern. We leave adapting our results to privacy-sensitive settings to future work.

We compare our methods to single-imputation methods theoretically and empirically. We choose to baseline against imputation methods for three reasons. First, if we ignore communication constraints, our problem is a missing data problem, where formally the data is ``missing at random'' (MAR) \cite{Little2019StatisticalAW}. MAR problems are well studied, so we know that imputation methods work well theoretically and in practice \cite{Schafer2000Inference, Kyureghian2011AMV}. 
Second,
because we have instance-optimal lower bounds, we know that imputation methods are also optimal for our problem. Finally, because imputation methods use more information than the method we propose, imputation will serve as a ``oracle'' baseline of sorts.











\paragraph{Contributions.} We briefly summarize our contributions.
\begin{enumerate}
    \item We design a communication-efficient, distributed learning algorithm \textsc{Collab} which performs a weighted de-biasing procedure on the ordinary least squares estimator of each agent's data. 
    \item We show \textsc{Collab} is asymptotically locally minimax optimal among estimators which have access to the ordinary least squares estimator of each agent's data. We also show that with some additional assumptions, \textsc{Collab} is also asymptotically locally minimax optimal among estimators that have access to \textit{all} of the training data of all agents.
    \item We propose and develop theory for various baseline methods based on imputation. 
    We compare the statistical error and communication cost of \textsc{Collab} against these baseline methods both theoretically and empirically on real and synthetic data.
    \item We discuss generalizations of \textsc{Collab} for non-Gaussian feature settings and non-linear settings. We highlight open problems and identify possible directions for future work.
\end{enumerate}






\subsection{Related Work}

\paragraph{Missing data.}
If we ignore the communication and computational aspects of our problem, the problem we study reduces to one of estimation with missing data. There has been a lot of work on this topic; please see \cite{Little2019StatisticalAW} for an overview. The data in our problem is missing at random (MAR)---the missing pattern does not depend on the value of the data and is known given agent $i$.
There are many approaches to handling missing data such as weighting and model-based methods \cite{Schafer2002MissingDO}.
Most related to our work are methods on single imputation. \citet{Schafer2000Inference} shows imputation with conditional mean is nearly optimal with special corrections applied. More recently, \citet{Chandrasekher2020Imputation} show that single imputation is minimax optimal in the high dimensional setting. Another closely related popular approach is multiple imputation \cite{Schafer1999Multiple, Azur2011MultipleIB}.
Previous work \cite{Tsiatis2006SemiparametricTA, Wang1998LargesampleTF} has shown that multiple imputation in low dimensional settings produces correct confidence intervals under a more general set of assumptions compared to single imputation settings. However, we choose to focus on single imputation methods for two reasons. First, we are interested in estimation error and not confidence intervals, and our lower bounds show that single imputation has optimal estimation error for our setting. Second, in our problem context, multiple imputation would require more rounds of communication and consequently higher communication cost. 
Other methods for missing data include weighting and model-based methods.

\paragraph{Distributed learning.}
Learning with communication constraints is a well studied practical problem. We provide a couple of examples. \citet{Suresh2022CorrelatedQF} study how to perform mean estimation with communication constraints.
\citet{Duchi2014OptimalityGF} develop communication-constrained minimax lower bounds. Distributed convex optimization methods like Hogwild \cite{Recht2011HogwildAL} have also been well studied. However, the works mentioned all concern the no-missing-data regime.
A more relevant subfield of distributed learning is federated learning.
In federated learning, a central server coordinates a collection of client devices to train a machine learning model. Training data is stored on client devices, and due to communication and privacy constraints, clients are not allowed to share their training data with each other or the central server \cite{Kairouz2019AdvancesAO}. 
In the no-missing-features regime, optimization algorithms for federated optimization are well studied.
There is also more theoretical work, which focus on characterizing communication, statistical, and privacy tradeoffs, albeit for a more narrow set of problems such as mean and frequency estimation \cite{Chen2020BreakingTC}.
More related to the missing data regime we consider is cross-silo federated learning \cite{Kairouz2019AdvancesAO} or vertical federated learning \cite{Yang2019FederatedML}. In this paradigm, the datasets on client machines are not only partitioned by samples but also by features. Researchers have studied this problem in the context of trees \cite{Cheng2019SecureBoostAL}, calculating covariance matrices \cite{Karr2009PrivacyPreservingAO}, k-means clustering \cite{Vaidya2003PrivacypreservingKC}, support vector machines \cite{Yu2006PrivacyPreservingSC}, and neural nets \cite{Liu2020ASF}. Most related to our work is \citet{Gascn2017PrivacyPreservingDL, Hardy2017PrivateFL}; they study how to privately perform linear regression in a distributed manner. However, unlike our work, these works focus more on developing algorithms with privacy guarantees rather than statistical ones.
