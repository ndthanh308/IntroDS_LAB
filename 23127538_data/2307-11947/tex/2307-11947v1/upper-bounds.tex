
\section{Mathematical model}
\label{sec:linear-model}
We assume we have $m$ agents that observes a subset of the dimensions of the input data $x \in \R^d$. Each agent $i$ has a ``view'' permutation matrix $\projm_i^\top := \begin{bmatrix} \projm\iplus^\top & \projm\imin^\top \end{bmatrix} \in \R^{d \times d}$. $\projm\iplus \in \R^{d_i\times d}$ describes which feature dimensions the agent sees, and $\projm\imin \in \R^{(d-d_i) \times d}$ describes the dimensions the agent does not see. 
For a feature, label pair $(x, y)$, the $i$-th agent has data $(x\iplus, y)$ where $x\iplus \defeq \projm\iplus x\in \R^\di$.
Each agent has $n$ such observations (independent across agents) denoted as a matrix $X\iplus \in R^{n \times \di}$ and vector $y_i\in\R^n$.
We let $X\imin \in \R^{n \times (d- \di)}$ denote the unobserved dimensions of the input data $x$ drawn for the $i$-th agent, and we let $X_i \in \R^{n \times d}$ denote the matrix of input data $x$ drawn for the $i$-th agent, including the dimensions of $x$ unobserved by the $i$-th agent. To simplify discussions in the following sections, for any vector $v \in \R^d$ we use the shorthand $v\iplus = \projm\iplus v$ and $v\imin = \projm\imin v$. Similarly for any matrix $A \in \R^{d \times d}$ we denote by
\begin{align*}
	A\iplus & = \projm\iplus A \projm\iplus^\top ,
	&  A\imin & =  \projm\imin A \projm\imin^\top ,  \\
	A\ipm & = \projm\iplus A \projm\imin^\top ,
	&  A\imp & =  \projm\imin A \projm\iplus^\top .
\end{align*}
For a p.s.d.\ matrix $A$, we let $\norm{x}_A = \< x, Ax\>$. 

We assume the data from the $m$ agents follow the same linear model. The features vectors $x$ comprising the data matrices $X_1, \ldots, X_m$ are i.i.d.\ with zero mean and covariance $\scov \succ 0$. 
We will assume that each agent has knowledge of $\scov\iplus$---e.g., they have a lot of unlabeled data to use to estimate this quantity. The labels generated follow the linear model
\begin{align*}
	y_i = X_i \param + \noise_i, \qquad \noise_i \simiid\ \normal(0, \sigma^2 I_n).
\end{align*} 
Throughout this work we consider a  fixed ground truth parameter $\param$.


\paragraph{Objectives.} We are interested in proposing a method of using the data of the agents to form an estimate $\hparam$ which minimizes the full-feature prediction error on a fresh sample $x \in \R^d$
\begin{align}\label{eqn:global-test-loss}
	\E_{x}[( \< x, \hparam\> - \<x, \param\>)^2] = \|\hparam - \param
    \|_\scov^2.
\end{align}
We are also interested in forming an estimate $\hparam_i$ which minimizes the missing-feature prediction error of a fresh sample $x\iplus \in \R^{d_i}$ for agent $i$---i.e., $x\iplus = \projm\iplus x$ where $x \in \R^d$ is fresh. Define $T_i \defeq \begin{bmatrix} I_{d_i} & \scov\iplus^{-1} \scov\ipm \end{bmatrix} \projm_i$ and the Schur complement $\Gamma\imin \defeq \scov \backslash \scov\iplus \defeq \scov\imin - \scov\imp \scov\iplus^{-1} \scov\ipm$. The local test error is then
\begin{align}\label{eqn:local-test-loss}
	\E_{x}[( \<x\iplus, \hparam_i\> - \<x, \param\>)^2]= \|\hparam_i - T_i \param \|_{\scov\iplus}^2 +  \norm{\param\imin}_{\Gamma\imin}^2
\end{align}

Here, $\norm{\param\imin}_{\Gamma\imin}^2$ is irreducible error. The role of the operator $T_i$ is significant as $T_i \param$ is the best possible estimator for the $i$th agent\footnote{Maybe surprisingly, $T_i \param$ is better than naively selecting the subset of $\param$ corresponding to the features observed by agent $i$---i.e., $\projm_i \param$. This is because $T_i \param$ leverages the correlations between features.}.
Through this paper, we will also highlight the communication costs of the methods we consider. Recall that we would like our methods to have $o(n)$ communication cost.



\section{Our approach} \label{sec:upper-bounds}


We begin by outlining an approach of solving this problem for general feature distributions. 
The general approach is not immediately usable because it requires some knowledge of $\param$, so we need to do some massaging. 
In \Cref{sec:collab}, we show how to circumvent this issue in the Gaussian feature setting and introduce our method \textsc{Collab}.
Adapting the general approach to other non-Gaussian settings is an open problem, but we discuss some potential approaches in \Cref{sec:discussion}.

\subsection{General approach}
Our solution begins with each agent $i$ performing ordinary least squares on their local data
\begin{align*}
	\est{\param}_i = X_{i+}^\dagger y_i \stackrel{\mathrm{(i)}}{=} (X_{i+}^\top X_{i+})^{-1} X_{i+}^\top y_i,
\end{align*} 
where $A^\dagger$ denotes the Mooreâ€“Penrose inverse for a general matrix $A$, and (i) holds whenever $\rank(X_{i+}) \geq d_i$. Because we focus on the large sample asymptotics regime ($n \gg d_i$), (i) will hold with probability $1$.

Then, we aggregate $\hat{\theta}$ using a form of weighted empirical risk minimization parameterized by the positive definite matrices $W_i \in \R^{d_i \times d_i}$
\begin{align} \label{eq:defn-weighted-avg-est-param}
	\est{\param} = \est{\param}(W_1, \cdots, W_m) \defeq \argmin_\param \sum_{i=1}^m \norm{\param\iplus + \scov\iplus^{-1} \scov\ipm \param\imin - \est{\param}_i }_{W_i}^2.
\end{align}
We know by first order stationarity that 
$\est{\param} = \prnbig{\sum_{i=1}^m T_i^\top W_i T_i}^{-1} \prnbig{\sum_{i=1}^m T_i^\top W_i \est{\param}_i}$. 
$\est{\param}$ is a consistent estimate of $\param$ regardless the choice of weighting matrices $W_i$. Furthermore, if the features $X_i$, $\est{\param}$ are Gaussian, $\est{\param}$ is also unbiased. 
We show this result in the Appendix in \Cref{lem:est-param-consistency}.
While \Cref{lem:est-param-consistency} shows that the choice of weighting matrices $W_i$ does not affect consistency, the choice of weighting matrices $W_i$ does dictate the asymptotic convergence rate of the estimator. In the next theorem, we show what the best performing choice of weighting matrices are. 
The proof is in \Cref{sec:proof-upper-bound}.


\newcommand{\gauss}{^{\textup{g}}}
\begin{theorem} \label{thm:upper-bound}
	For any weighting matrices $W_i$, the aggregated estimator $\est{\param} = \est{\param}(W_1, \cdots, W_m)$ is asymptotically normal
	\begin{align*}
		\sqrt{n} \prn{\est{\param} - \param} = \normal(0, C(W_1, \cdots, W_m)),
	\end{align*}
	with some covariance matrix $C(W_1, \cdots, W_m)$. The optimal choice of weighting matrices is
	$$W_i\opt \defeq \scov\iplus (\Ep \brk{x\iplus  \param\imin^\top z\iplus  z\iplus^\top  \param\imin x\iplus^\top} + \sigma^2 \scov\iplus)^{-1} \scov\iplus,$$ where $z\iplus = x\imin - \scov\imp \scov\iplus^{-1} x\iplus$. In particular, 
	for all $W_i$, 
		$C(W_1, \cdots, W_m) \succeq C(W_1\opt, \cdots, W_m\opt) =\prn{\sum_{i=1}^m T_i^\top  W_i\opt   T_i}^{-1}$.
		
\end{theorem}
The main challenge of using \Cref{thm:upper-bound} is in constructing the optimal weights $W_i\opt$, as at face value, they depend on knowledge of $\param$. While we will discuss high level strategies of bypassing this issue in non-Gaussian data settings in \Cref{sec:discussion}, we will currently focus our attention on how we can make use of Gaussianity to construct our estimator \textsc{Collab}.

\subsection{\textsc{Collab} Estimator - Gaussian feature setting}\label{sec:collab}

\SetKwComment{Comment}{/* }{ */}
\begin{algorithm}[t]
\caption{\textsc{Collab} algorithm}\label{alg:collab}
\KwData{$m$ agents with training data $(X_{1+}, y_1), \ldots, (X_{m+}, y_m)$ each with $n$ datapoints}
\For{Each agent $i=1, \ldots, m$ in parallel}{
  Compute $\hparam_i = (X_{i+}^\top X_{i+})^{-1} X_{i+}^\top y_i$\;
  Compute $\hscov_i = \frac{1}{n} X\iplus^\top X\iplus$ or with additional unlabeled data\;
  Compute $R_i = \frac{1}{n}\|X\iplus  \hparam_i - y\|_2^2$\;
  Send $\hparam_i, \hscov_i, R_i$ to central server\;
}
Central server constructs $\hat{W}_i\gauss \defeq \hscov\iplus / R_i$\;
Central server computes $\est{\param}_i\collab = T_i \est{\param}(\hat{W}_1\gauss, \cdots, \hat{W}_m\gauss)$ and distributes them to respective agents\;
\end{algorithm}


If $X_i$ are distributed as $\normal(0, \scov)$, $W_i\opt$ has an explicit closed form as
\begin{align*}
	W_i \opt =W_i\gauss \defeq \frac{\scov\iplus}{\norm{\param\imin}_{\Gamma\imin}^2 + \sigma^2} = \frac{\scov\iplus}{\E_{x, y}[(\< x\iplus, \hparam_i\> - y)^2]},
\end{align*}
where $\Gamma\imin =  \scov\imin - \scov\imp \scov\iplus^{-1} \scov\ipm$ is the Schur complement. 
Recall we assume that each agent has enough unlabeled data to estimate $\scov\iplus$. Furthermore, $\frac{1}{n}\|X\iplus  \hparam_i - y\|_2^2$ is a consistent estimator of $\E_{x, y}[(\< x\iplus, \hparam_i\> - y)^2]$.
Thus, each agent is able to construct estimates of $W_i\gauss$ by computing
\begin{align*}
	\hat{W}_i\gauss \defeq \frac{\scov\iplus}{\frac{1}{n}\|X\iplus  \hparam_i - y\|_2^2}
\end{align*}

Now we construct our global and local \textsc{Collab} estimators defined respectively as
\begin{align}\label{eqn:collab-est}
	\begin{split}
		\est{\param}\collab \defeq \est{\param}(\hat{W}_1\gauss, \cdots, \hat{W}_m\gauss), \qquad\quad
		\est{\param}_i\collab \defeq T_i \est{\param}(\hat{W}_1\gauss, \cdots, \hat{W}_m\gauss).
	\end{split}
\end{align}
We summarize the \textsc{Collab} algorithm in Algorithm~\ref{alg:collab}.
At a high level, $\est{\param}\collab$ is an estimate of $\param$ which also minimizes the full-feature prediction error \eqref{eqn:global-test-loss} and $\est{\param}_i\collab$ minimizes the missing-feature prediction error for agent $i$ \eqref{eqn:local-test-loss}.
Now we show that using the collective ``biased wisdom'' of local estimates $\est{\param}_i$, our collaborative learning approach returns an improved local estimator. The proof is in \Cref{sec:proof-upper-bound-local}.

\begin{corollary}  \label{cor:upper-bound-local}
	Let $X_i \sim \normal(0, \scov)$ and define $C\gauss \defeq (\sum_{i=1}^m T_i^\top  W_i\gauss   T_i)^{-1}$. The global \textsc{Collab} estimator $\est{\param}_i\collab$ and the local $\est{\param}_i\collab$ on agent $i$ are asymptotically normal
	\begin{align*}
		\sqrt{n} \prn{\est{\param}\collab - \param} \cd \normal\left(0, C\gauss \right) \quad \text{and}\quad 
		\sqrt{n} \prn{\est{\param}_i\collab -  T_i \param} \cd \normal\left(0, T_i C\gauss T_i^\top\right).
	\end{align*}
	The following are true
	\begin{itemize}
		\item[(i)] $W_i\gauss$ are the optimal choice of weighting matrices i.e.,particular, $C(W_1, \cdots, W_m) \succeq C(W_1\gauss, \cdots, W_m\gauss) =C\gauss$.
		\item[(ii)] On agent $i$, we have $\sqrt{n}(\est{\param}_i - T_i \param) \cd \normal(0, (W_i\gauss)^{-1})$. The asymptotic variance of $\est{\param}_i$ is larger than that of the \textsc{Collab} estimator $\est{\param}_i\collab$---i.e., $(W_i\gauss)^{-1} \succeq T_i C\gauss T_i^\top$.
	\end{itemize}
\end{corollary}




