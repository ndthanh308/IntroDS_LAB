\section{Experiments}
We perform experiments to empirically test and compare the methods we have discussed in this paper. Our first experiment is on real data with potential distribution shift between agents and models a potentially real setting concerning the US Census. This experiment is meant to show how our methods would perform in practice. The setup of the synthetic experiment is similar to the setup of our theory; due to space, we defer this to \Cref{sec:experiment-synthetic}. 

\subsection{US Census Experiments}
\label{sec:experiment-census}


% Figure environment removed
\looseness=-1
We experiment on real US census data modified from the ACSTravelTime dataset from the \texttt{folktables} package \cite{Ding2021RetiringAN} to test how our methods work on real data, which may contain covariate shift across agents. After dataset preprocessing, described in the \Cref{sec:experiment-census-details}, we have $d=37$ features. We plot the covariance matrix of the features in \Cref{fig:folk-cov-heatmap}. We compute the covariance from training data across all of the datacenters. We assume we are able to do this because this computation can be done in a distributed manner, without communicating training data points or labels.

We (artificially) construct $m=5$ datacenters (agents), each containing data from one of California, New York, Texas, Florida, and Illinois. The goal is to collaboratively learn a model for each datacenter in a communication efficient way.
This setup models potentially real settings where state governments are interested in similar prediction tasks but may not be allowed to directly transfer data about their constituents directly to one another due to privacy or communication constraints.
The California datacenter will have access to $37$ features, New York to $36$, Texas to $35$, Florida to $30$, and Illinois to $27$. This models the feature heterogeneity which varies across geography. 
Each datacenter will have $n$ datapoints, which we vary in this experiment. The objective to predict people from Illinois's travel time to work given all $37$ features. This task models the setting where the datacenter of interest does not have access to labeled full-featured, data to use to predict on full-featured test data.

We compare our method \textsc{Collab} 
against methods we call Naive-Local, Naive-Collab, Optimized-Naive-Colllab, Imputation, and RW-Imputation. 
We briefly describe each method here; \Cref{sec:experiment-census-details} contains a more detailed description of each method.
Naive-Local refers to each agent locally perform OLS to construct $\hparam_i$.
Naive-Collab does an equal-weighted average of the agent OLS models---$\sum_{i \in [m]} \projm\iplus^\top \hparam_i / m$. Optimized-Naive-Collab uses gradient descent to optimize the choices of weights of Naive-Collab. Optimized-Naive-Collab uses fresh labeled samples without any missing features during gradient descent, so in this sense, Optimized-Naive-Collab is more powerful than our method. Imputation refers to the global imputation estimator $\hparam\imputeglb$ with $\alpha_i = 1/m$. RW-Imputation is Imputation but with the optimal choice of weights $\alpha_i$. We also compare against Naive-Local trained with $5n$ datapoints. We choose $5n$ to model the hypothetical scenario setting where all of the other datacenters available contain data (albeit with missing features) from Illinois. For each method that we test, we run $80$ trials to form $95\%$ confidence intervals. We see that for $n \leq 800$ in \Cref{fig:folk-small-n,fig:folk-small-n-2}, \textsc{Collab} performs the best; the imputation methods do the worst, and have much higher variance. In this small $n$ regime, even the Naive-Local method with $5$ times the data does worse than \textsc{Collab}. For $n \geq 2000$ in \Cref{fig:folk-large-n}, the aggregation methods do worse than the imputation methods, and Naive-Local method with $5$ times the data is the best performing method. However, \textsc{Collab} remains better than Optimized-Naive-Collab and Naive-Collab. 
The fact that the performance of the Naive-Collab approaches in much closer to the performance of \textsc{Collab} than in the Synthetic experiment in \Cref{sec:experiment-synthetic} is not surprising, as the covariance of the features is much more isotropic, meaning that the naive aggregation methods will not incur nearly as much bias.


