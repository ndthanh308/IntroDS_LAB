Video analytics are becoming increasingly important at the edge with the widespread integration of camera sensors in many scenarios, such as autonomous systems and IoT devices, to benefit from the visual awareness of the surroundings. For example, authorities are ramping up the deployment of surveillance systems to assist in law enforcement and crime prevention \cite{one_billion_cameras}; transportation systems are moving towards partial or full automation \cite{hancock2019future}; and unmanned aerial vehicles are starting to revolutionize a number of industries and remote military operations \cite{drone_and_future}. 

In such systems, the videos captured by the cameras are often transferred to remote cloud servers for analytics
%It is often desirable to send the data collected by by the end devices in these systems to remote cloud servers for analytics 
\cite{chen2015glimpse, chin2019adascale, wang2019bridging, zhang2018awstream, du2020server, li2020reducto}, but this incurs expensive video transportation. Several considerations drive this model including: (1) the analytics algorithm may require more resources than available on the end devices; or (2) the analytics consumers may not be located on the device itself ~\cite{qiu-18} or may require access to multiple streams of data ~\cite{kolar2016ctcv}. %; and finally, (3) the server may desire to keep the data to facilitate longer-term forensic analysis or for continued training and refinement of the analytics algorithm. 
Transporting high-quality video to support accurate analytics, consumes significant bandwidth. %Moreover, attempting to degrade the video quality in order to save bandwidth can be potentially detrimental to the downstream analytics accuracy. 
Thus, a key research question regarding streaming video analytics is: \emph{how to reduce the volume of the video data without sacrificing accuracy with respect to the downstream analytics?}    

Conventional video compression algorithms, such as MPEG~\cite{grois2013performance} prioritize maximizing the perceived visual quality of the video over preserving the essential features for analytics~\cite{pakha2018reinventing, du2020server}. For example, these algorithms degrade the video quality uniformly across both regions that are salient for analytics (e.g., object regions) and those that are not (e.g., background regions). As a result, we are left with the unsatisfying choice of either losing accuracy by degrading the quality of the object regions along with the background regions, or losing bandwidth by keeping everything in sufficient quality to preserve the analytics accuracy~\cite{pakha2018reinventing}. 


An alternative approach is semantic compression: such algorithms preserve necessary features while aggressively degrading unimportant regions, achieving high compression while preserving analytics accuracy.  A challenge in this approach is how to identify what regions are salient without running the full analytics at the source. %Compressing the video with this knowledge can keep important regions while aggressively compressing unimportant ones.  Since we are trying to avoid the associated overhead with carrying out the full analytics pipeline at the source, we need a more efficient implementation to identify semantically relevant regions of the video. 
Our insight is that differentiating the relevant regions from the background regions in a video is easier than the full analytics task.  In the context of object detection and classification, identifying which areas may have objects is much simpler than the full classification problem.  % identifying the location of specific objects and their classes in an object detection task). % and a \underline{D}eep \underline{N}eural \underline{N}etwork (DNN) based object detector as our analytics algorithm, but we believe this approach equally applies to other tasks and algorithms). 
Another observation that simplifies the problem is that, given a specific video context, we typically encounter a small subset of the possible objects -- for example, in a highway surveillance context, we expect different types of vehicles, but not animals, or furniture.  

LtC leverages these two observations by using a light-weight student neural network at the video source, which is trained by a Deep Neural Network (DNN) based teacher network at the server. The student network identifies the salient regions within video frames, allowing the source to differentially compress the video before sending over the network. This training method is suitable for the analytics tasks where the teacher network infers the locations of the object regions in addition to the primary task, such as, object detection and segmentation.  %To reiterate: the student network is simpler for two reasons: (1) it solves the much simpler problem of identifying likely object regions compared to object detection and classification; and (2) it is trained on and specialized for the scenarios encountered by the camera. 
We also contribute a novel temporal filtering algorithm that uses a feature based discriminator to identify successive video frames where semantic information has not changed significantly, allowing us to avoid their transmission entirely.

Since LtC operates by specializing the student network to the encountered scenarios, it is important to detect when these scenarios have changed, as it is likely to be the case in dynamic environments or with mobile cameras.  LtC also reacts to this occurrence of \emph{concept drift} when the server discovers that the information relayed by the source is not effective (e.g., the higher resolution regions do not yield objects).  We observe that the layers of the student network closer to the input (which we collectively call \emph{the encoder}) are likely to remain unchanged with the scenario since they embed general features (i.e., shapes, patterns, etc.). Instead, we train only the layers closer to the output (which we call \emph{the extension}) mapping features to posteriors, which significantly reduces the update overhead.  The student network consists of merely 0.5\% of the teacher network's total parameters, and in the event of concept drift we only need to update as little as 2.2\% of the student's parameters.

In order to ensure fair comparison, we have implemented LtC and other baselines by extending an open-source emulator ~\cite{du2020server}. The video dataset was obtained from live-streaming public surveillance cameras and videos from public streaming platforms. Additionally, we regulated the end-to-end traffic conditions using an open-source network emulator ~\cite{netem}.  When compared to the closest spatial compression baseline (DDS from SIGCOMM 2020)~\cite{du2020server}, LtC uses 23\% less bandwidth and has 21-45\% shorter response time. Moreover, it is able to filter 8-14\% more frames than the closes temporal compression baseline (Reducto, also from SIGCOMM 2020) \cite{li2020reducto}, and as a result, reduces bandwidth comsumption by 8\%. Combining both spatial and temporal compression, LtC uses 28-35\% less bandwidth while having 14-45\% shorter response time than these state-of-the-art alternatives. Also, LtC substantially outpuerforms commercial video compression standards including AWStream ~\cite{zhang2018awstream}, Cloudseg ~\cite{wang2019bridging} and Glimpse ~\cite{chen2015glimpse} both in terms of F1-score and bandwidth use.

In summary, the contributions of this paper are as follows:
% \vspace{-0.05in}
\begin{itemize}[leftmargin=10pt]
    \item \textbf{One-shot semantic spatial compression using lightweight student network.} We use a student network to implement semantic compression without frequent feedback from the server.
    \item \textbf{Temporal filtering through efficient deep feature differencing.} We develop a new approach for filtering redundant frames by measuring changes in the feature space. % This approach also leverages the student network, consolidating overheads: a constant-sized feature vector is extracted from an intermediate layer of the student network to quantify this change with little extra computation.
    We show that this approach is more stable to unimportant changes in the video (e.g., due to wind, illumination, etc.) and therefore more effective than manually crafted features % such as  (e.g., \underline{S}cale-\underline{I}nvariant \underline{F}eature \underline{T}ransform (SIFT), \underline{H}istogram of \underline{O}riented \underline{G}radients (HOG), etc.) that do not have 
    because it uses context-specific knowledge \cite{li2020reducto}.
    \item \textbf{Efficient adaption to concept drift.} We introduce an efficient concept drift detection and update mechanism, significantly lowering the cost of updating the detector. %Specifically, the input-adjacent layers of the student network remain relatively stable when the environment changes \cite{zeiler2014visualizing, liu2019fusing, kataoka2015feature}, enabling us to send low-cost updates only for the layers adjacent to the output.
   % \item \textbf{Unified spatial-temporal compression framework.} To the best of our knowledge, our approach is the first framework that combine spatial and temporal compression in a complementary fashion.
\end{itemize}


%To describe \emph{LtC}'s workflow more concretely: the server hosts a Deep Neural Network (DNN) for video analytics.  We use a limited number of frames from the client to enable the server to train a small student model whose goal is to identify object regions (rather than classify the objects, which is a significantly harder problem).  Moreover, the student's task is simpler because it is specialized to the video context of the specific client on which it will be deployed.  The student model is transferred to the client, where it is used to identify regions with objects, allowing us to more aggressively compress background regions while retaining sufficient precision for object regions to maintain accuracy.  Moreover, \emph{LtC} also uses a novel temporal compression algorithm that recognizes the presence of new information through comparison in the feature space.  Finally, since \emph{LtC} is specialized to the context of the camera, if this context changes (for example, when the camera moves or the environment changes), we develop an efficient approach to update the model, recognizing that only the later layers in the model have to be updated.  

% \textcolor{red}{usually, we do two types of evaluations: 1) simulations with real traffic to check the correctness and scalability and 2) a testbed based evaluation to check the real deployment feasibility. if you have done both, we can categorize them accordingly. I start the evaluation paragraph like "we have implemented a prototype of \emph{LtC} (you can mention lines of code and point to the Git repo here) and evaluated it over simulation and real deployment." -- then can talk about both aspects with key results.} \textcolor{cyan}{sorry, this is legacy writing, have controlled the end to end network b/w and delay with NetEm, no real network data used, i'll fix this}

%In order to save network bandwidth, the camera sensors utilize a compression logic on the outgoing video frames, which can be either i) spatial, or ii) temporal in nature. Spatial compression downsamples or even omits the background regions that are irrelevant for the DNN inference, whereas temporal compression filters out successive video frames that are similar in terms of object features.   

%\nael{Commented out the next paragraph -- can leave to related work, or background.}
%In the previous literature, the compression at the camera sensor is realized as either independent heuristic \cite{chen2015glimpse, zhang2015design, canel2019scaling, chin2019adascale, wang2019bridging, emmons2019cracking}, or driven by the server-side DNN \cite{pakha2018reinventing, du2020server, zhang2018awstream, li2020reducto}. In dynamic scenarios where the object definitions change frequently, server-driven compression has been proven to be more effective than independent heuristics \cite{du2020server}. For example, the state-of-the-art spatial compression technique DDS \cite{du2020server} uses a multi-feedback loop to acquire the whereabouts of the object windows at the source directly from the DNN at the server. \textcolor{black}{Whereas, the state-of-the-art temporal compression technique Reducto\cite{li2020reducto} profiles constant-sized segments of consecutive video frames against optimal compression metrics at the server to be shared with the source for future use.} Although, these server-driven mechanisms are able to achieve high accuracy during the DNN inference, in both cases either the multi-feedback loop or the profiling mechanism hinders the agility of the pipeline. Moreover, it remains to be seen whether applying both spatial and temporal compression concurrently can improve over any individual approach. 

%In this paper, we propose Learning to Compress (\emph{LtC}), a learning-based server-driven compression framework targeted for efficient streaming video analytics. In addition to optimizing network bandwidth against analytics accuracy, we also consider related device and application specific factors, such as, camera resource usage and server response delay. In order to perform in real-time on compute and power limited camera sensors, the compression logic needs to be light-weight than the server-side DNN. Moreover, for critical applications, such as, autonomous driving, the responses from the server need to arrive in a timely fashion so that the actions are not delayed. In order to achieve these goals, our solution consists of three complementary mechanisms.
% \textcolor{purple}{can we write somewhere in the intro that \emph{LtC} is based on a novel idea?}
% This paper makes the following contributions:
%\begin{itemize}[leftmargin=10pt]
%    \item We propose \emph{LtC}, a novel framework for analytics oriented video compression based on student-teacher knowledge distillation. 
%    \item We contribute new learning-based spatial and temporal compression techniques.
 %   \item We implement \emph{LtC} and demonstrate that it significantly outperforms state-of-the art solutions, resulting in upto \textcolor{red}{30\%} less bandwidth usage, and \textcolor{red}{55\%} shorter response time than the closest baselines. 
%\end{itemize}