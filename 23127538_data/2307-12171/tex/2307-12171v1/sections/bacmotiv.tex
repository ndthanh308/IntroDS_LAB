%There are established video compression algorithms (e.g., HVEC, MPEG, etc.) that are used to compress consumer-oriented videos. 
The pertinent question that arises is, \emph{why do we need different compression algorithms for video analytics?}   Conventional video compression algorithms (e.g., HVEC or MPEG) are designed to maintain human-perceived visual quality; these compression algorithms uniformly degrade the quality across the video. In contrast, the goal of semantic compression is to maintain the accuracy of downstream analytics, and therefore, by using differential compression techniques it is able to achieve a better bandwidth-accuracy tradeoffs. The process of spatial and temporal compression with respect to object classification is presented in Figure \ref{fig:spatial_temporal}.
% Figure environment removed

To demonstrate the potential of semantic compression in comparison to conventional video compression in the context of video analytics, we carry out a series of experiments. Our results indicate that semantic compression offers better bandwidth-accuracy tradeoffs than conventional codecs, such as MPEG. To evaluate the performance of these compression algorithms, we calculate their F1-scores by comparing the outputs of the full-sized DNN on both the compressed and the original video data. Alternatively, this approach can be thought of as the full-sized DNN being placed at the source to act as an optimal semantic compression, which provides perfect estimates of the positions and sizes of the required regions for analytics (by the same DNN) at the server. This optimal algorithm, however, is not feasible in resource-constrained source devices, and serves only to illustrate the size of the opportunity.



% To highlight the potential of semantic compression in the context of video analytics compared , we carry out a series of experiments. Specifically, we show that semantic compression is able to achieve better bandwidth-accuracy tradeoff than conventional compression (such as MPEG).  % terms of both bandwidth efficiency and F1-score of the downstream analytics. 



% We determine the F1-score by comparing the outputs of the full-sized DNN on both the compressed and the original video data. Alternatively, 

% this can be thought of as the full-sized DNN placed at the source to act as an optimal semantic compression algorithm, which provides perfect estimates of the positions and sizes of the required regions for analytics (by the same) at the server. This optimal algorithm, however, is not feasible in resource-constrained source devices, and serves only to illustrate the magnitude of the opportunity.


% using the output of the full-sized object detector DNN on both the compressed and the original video data. We assume an optimal semantic compression algorithm that uses the full DNN at the source, which provides perfect estimates of the positions and the sizes of the regions that are needed (by the same DNN) at the server. 
% In this section, we motivate the need for a semantic compression algorithm by comparing it to traditional video codecs (specifically, MPEG) with respect to both bandwidth efficiency and accuracy of the downstream analytics. Specifically, when evaluating the accuracy, 
% Specifically, when evaluating the accuracy, we consider the output at \emph{the full-sized object detector DNN after the compressed data is delivered to it and used as input}. To estimate the potential of semantic compression, we assume we have access to the full-sized object detector DNN at the source cameras, which provides perfect estimates of the positions and the sizes of the regions that are needed (by the same DNN) at the server. 
% This optimal algorithm yields no loss in F1-score, and uses the least possible bandwidth within our scheme, and only serves to illustrate the size of the opportunity.  %We end the section by showing the amount of spatial and temporal redundancies present in the videos of our dataset.
% Also, we discuss the semantic compression opportunities that typically lie in a video. Lastly, we also show the possible bandwidth saving opportunities using spatial and temporal compressions separately, and in cascade. 


% Figure environment removed

% We use a video dataset collected from a range of real world scenarios (details presented later in Table \ref{tab:dataset}).
  Figure \ref{fig:mpeg_comparison} presents the performance of MPEG against the optimal versions of spatial, temporal, and spatial-temporal algorithms. Each MPEG bar is configured such that it consumes either similar or higher bandwidth than the semantic compression. In all cases, the MPEG suffers a 5-13\% F1-score drop for similar bandwidth use. This is a substantial loss in terms of F1-score: over the past 5 years, the improvement in the state of the art performance in ImageNet classification is less than 10\% \cite{beyer2020we, stock2018convnets}. Also, to reach a 95\% F1-score, MPEG is able to compress only 20\% of the highest quality video (e.g., the MPEG bar with 94.7\% F1-score), while spatio-temporal compression (albeit under optimal conditions) is able to compress over 80\%.

% Figure environment removed

We also use the optimal semantic compression algorithm to characterize the redundancies present along the spatial and temporal dimensions in real videos from our dataset listed in Table \ref{tab:dataset}. Generally, in these videos the object regions constitute a small fraction of the size of the field of view. Across all the videos, around 80\% of the video frames have 33\% or fewer object regions (Figure \ref{fig:spatial_cdf}). Also, approximately in 80\% of the 1-second batches sampled from these videos, only 20 frames in 30 frames-per-second (FPS) videos and 5 frames in 15 FPS videos are necessary for analytics (Figure \ref{fig:temporal_cdf}). 


% Moreover, in approximately 80\% of 1-second segments sampled from 30 FPS videos, only 20 or fewer frames suffice to reach an optimal F1-score (Figure \ref{fig:temporal_cdf}).%\nael{20 out of 30?  Earlier you say frame rate is 15 or 30, so 20 useful frames is not surprising.  Better to use percentage like you did with spatial.  Also definition of useful from a temporal perspective is not given precisely like you do for spatial.} 



% Lastly, in Figure \ref{fig:spatial_temporal_opportunity} we present the bandwidth savings opportunities using the optimal spatial, temporal, and spatial-temporal algorithms. We can see that for the drone, traffic cam, and dash cam dataset, the optimal spatial compression is able to reduce more than 20\% of the full size of the video. However, the traffic cam dataset is less sparse, so optimal spatial compression has less gain. We can also observe that the optimal temporal compression is able to reduce more than 80\% of the full size of the videos across all the dataset. Also, it is evident that it is possible to reduce the videos even more by applying spatial and temporal compressions in cascade.
% % Figure environment removed










% There are highly efficient video compression algorithms, such as MPEG, HEVC, etc.~\cite{grois2013performance} that are used to compress consumer-oriented videos. Naturally the question arises, \emph{why do we need different compression algorithms for video analytics?} In consumer-oriented videos, a human viewer may look at any part of the video, and as such, the aforementioned compression algorithms uniformly compress the entire video in order to maximize the visual appeal. However, for video analytics, compression quality should be measured in terms of the accuracy achieved by the downstream analytics.  Semantic compression recognizes that different spatio-temporal regions within the field of view contribute differently to the features needed by the analytics: for example, a background region may not be important and can be compressed aggressively without affecting accuracy (Spatial compression as can be seen in Figure~\ref{fig:i_spatial_compression}).  %As a result, any regions within the spatial-temporal volume of the video that do not contribute to the accuracy may be omitted. By leveraging this opportunity, it is possible to design a more aggressive compression algorithm for the purposes of video analytics. This algorithm will spatially compress the video frames by catering to the need of the DNN, preserving the features of the object regions in high-resolution, while degrading or even omitting the background regions (Figure \ref{fig:i_spatial_compression}). Also, 
% In addition, temporal redundancy may exist and enable additional temporal compression opportunities targeted towards preserving features rather than visual perception quality.  We may, for example, omit frames that do not have sufficient feature entropy as can be seen in Figure \ref{fig:i_temporal_compression}. 




% In this section, we motivate the use of semantic compression.   Specifically, we characterize the available compression opportunity from the perspective of semantic compression (preserving features rather than visual quality).  %In the second subsection, we discuss the tradeoffs in terms of how to structure the compression between the camera sources (clients) and the edge/cloud servers.  %We structure the presentation around a number of questions that underlie the main design decisions.

%\nael{I think this question/paragraph is boring at this point.  We already made this point twice, in abstract and intro and I dont think the current paragraph adds much that is new.  We should see if we can introduce the compression figure without this paragraph.}

% \nael{This section now is focused on characterizing the opportunity for semantic compression.  What would be ideal is if we show an experiment where regular video compression is ineffective, but I know we struggled to do that in the past and it may not be possible.}

% % Figure environment removed

%\subsection{Semantic Compression and size of the opportunity}

% There are highly efficient video compression algorithms, such as MPEG, HEVC, etc.~\cite{grois2013performance} that are used to compress consumer-oriented videos. Naturally the question arises, \emph{why do we need different compression algorithms for video analytics?} In consumer-oriented videos, a human viewer may look at any part of the video, and as such, the aforementioned compression algorithms uniformly compress the entire video in order to maximize the visual appeal. However, for video analytics, compression quality should be measured in terms of the accuracy achieved by the downstream analytics.  Semantic compression recognizes that different spatio-temporal regions within the field of view contribute differently to the features needed by the analytics: for example, a background region may not be important and can be compressed aggressively without affecting accuracy (Spatial compression as can be seen in Figure~\ref{fig:i_spatial_compression}).  %As a result, any regions within the spatial-temporal volume of the video that do not contribute to the accuracy may be omitted. By leveraging this opportunity, it is possible to design a more aggressive compression algorithm for the purposes of video analytics. This algorithm will spatially compress the video frames by catering to the need of the DNN, preserving the features of the object regions in high-resolution, while degrading or even omitting the background regions (Figure \ref{fig:i_spatial_compression}). Also, 
% In addition, temporal redundancy may exist and enable additional temporal compression opportunities targeted towards preserving features rather than visual perception quality.  We may, for example, omit frames that do not have sufficient feature entropy as can be seen in Figure \ref{fig:i_temporal_compression}. 

%\subsection{What is the size of the opportunity?}
%\textcolor{red}{can we replace the above with -- How much can we compress?}
%\textcolor{purple}{we can start with the section like "to answer the above question, we perform some investigation (please see results in Fig.{}). The results reveal that"}
% % Figure environment removed

% We conduct experiments to characterize the amount of redundancy available in real videos with respect to semantic compression.  Our video dataset is collected from a wide range of real-world scenarios: traffic cameras deployed in highways and intersections, drone footage, dash cam recordings, and surveillance videos from a parking lot, as presented in Table \ref{table:dataset}. In all of these videos, the object regions are sparser than the background regions, and constitute only a small percentage of the total frame. Throughout the four scenarios, approximately 80\% of the video frames have 33\% or fewer object regions, as can be seen in Figure \ref{fig:i_spatial_cdf}. Moreover, all the videos are captured at a high frame-rate, e.g., 15 or 30 frames-per-second (FPS). In approximately 80\% of 1-second segments sampled from these videos, 20 or fewer frames are sufficient for analytics, as can be seen in Figure \ref{fig:i_temporal_cdf}.\nael{20 out of 30?  Earlier you say frame rate is 15 or 30, so 20 useful frames is not surprising.  Better to use percentage like you did with spatial.  Also definition of useful from a temporal perspective is not given precisely like you do for spatial.} 


%\textcolor{purple}{the definition of optimal compression algorithm is not clear to me.}
% If an exact copy of the full-sized DNN were somehow placed at the source, we could use the bounding-boxes generated by this network to perform spatial and temporal compression, and the resulting compression algorithm would be completely lossless. We use this lossless compression in order to demonstrate the spatial and temporal compression opportunities in different videos in our dataset, as seen in Figure \ref{fig:i_spatial_temporal_opportunity}. Finally, by pursuing both spatial and temporal compression, we are able to take advantage of both of these opportunities.
%it is possible to achieve  gains over any single approach. 

% There are highly efficient video compression standards such as H.265 and MPEG, etc.~\cite{grois2013performance} that are used to reduce the size of consumer-oriented videos. The question naturally arises: why do we need specialized compression for video analytics?  In consumer-oriented videos, the target is to maximize the video quality as perceived by a human viewer within the constraint of the resource budget both in terms of networking or computation. Since a human viewer may be looking at any part of the video, traditional compression algorithms generally compress the field of view uniformly, for example, not distinguishing between regions that have objects from background regions~\cite{du2020server}. Optimizing beyond uniform compression requires tracking of human perception, e.g., in some application spaces such as Augmented/Virtual Reality, if the human focus can be tracked within the rendered field of view, compression may be tailored to where the user is looking catering for human perception--a type of rendering called \emph{foveated rendering}~\cite{patney2016perceptually}.  

% Similarly, in the case of analytics-oriented videos a different type of compression is required that will track the regions-of-interest to the server-side DNN, and preserve the semantics within those regions~\cite{patwa-20}. Traditional compression algorithms yield the following unsatisfying tradeoff between bandwidth and accuracy: as we compress more aggressively to reduce bandwidth consumption, equal amount of degradation is incurred at both important object regions along with background regions causing a drop in the accuracy. Therefore, a more effective semantic compression will cater to the needs of the server-side object detection DNN by non-uniformly compressing the video regions spatially: keeping the features of the object regions intact while aggressively compressing the background regions that are immaterial to the task (Figure ~\ref{fig:i_spatial_compression}). %We call this type of compression spatial compression, as illustrated in Figure . 

% A similar argument can be applied to videos that are captured in a high frame-rate. More frames in a video results in smoother viewing experience to the viewer, but do not contribute to the downstream analytics as object positions and sizes remain relatively constant between successive frames. As MPEG only reduces this temporal redundancy up to the point where it does not hamper the visual appeal, there is an opportunity to filter additional frames in video analytics \cite{li2020reducto} (Figure \ref{fig:i_temporal_compression}. Instead of relying on the pixel values of in the image, we use a novel approach that compares differences in the feature space which are more discriminative of the values of the frames. % meaningful features from these frames to determine if there are any redundancies. We call this type of compression temporal compression, as illustrated in Figure .


% We pursue this opportunity as well  Therefore, instead of applying the compression evenly across the spatial-temporal volume of the videos, in analytics, we aim to compress the regions that are not essential for the downstream analytics. It is noteworthy that, aforementioned conventional video codecs are still applicable after analytics-oriented compression to further reduce the outgoing video stream.



% \subsection{Why a learning-based compression?}
% % \subsection{How should we structure compression responsibilities between the client and the server?}
% % \nael{How is this different from the previous question? -- help me understand if my suggested question is ok.  I left the original question commented out.}
% % Decoupling the operation of video collection from the analytics renders the source-side compression logic oblivious to the server-side DNN accuracy. Therefore, independent heuristics perform poorly in streaming video analytics where the scene changes rapidly \cite{du2020server, li2020reducto, zhang2018awstream}. For example, the same heuristic that can successfully compress a video in a room may not work effectively when it is pointed towards a window facing a street. Also, depending on the query, some heuristics are proven be more effective than others \cite{li2020reducto}. In order to incorporate the server-side DNN into the compression logic, the server needs to send feedback to the source concerning how it requires the video to be compressed. 

% In literature, this feedback process is performed at different granularity. One approach is to utilize short-lived, but frequent feedback \cite{du2020server}. Although this results in a highly contextual compression algorithm, it causes a high network delay due to recurring communication between the source and the server. Another approach is to summarize the compression strategy into longer-lasting feedback, so that it can be updated only when needed \cite{li2020reducto, zhang2018awstream}. In order to prepare this feedback the server will need a sizable amount of unfiltered raw video from the source, which ramps up bandwidth usage. To overcome these issues, several properties of learning-based models come in handy.

% % Based on the previous discussion, there is a knowledge gap between the source and the server that is the root cause behind the aforementioned issues. 
% We suggest use a neural network as feedback which learns the shifting requirements of the server-side DNN in reaction to the changes in the source-side scenario. In our design this neural network is called the student network, and is prepared by distilling the knowledge \cite{zeiler2014visualizing} of the server-side teacher DNN. This student network inherits the generalization capabilities of learning-based models, which make the feedback tolerant to minor environment changes and effective through longer periods of time. Additionally, the knowledge-transfer capabilities of the neural networks \cite{pan2009survey, zhuang2020comprehensive} facilitates the student network to be updated with only a small amount of new data. Thus, a learning-based compression logic is able to reduce the network delay as well as save bandwidth usage. 

% The input-adjacent layers of the student network capture 2D patterns (e.g., corners, edges, etc.) in videos and remain relatively unchanged as the environment changes \cite{zeiler2014visualizing, liu2019fusing, kataoka2015feature}. This allows us to send feedback as low-cost updates to only the layers adjacent to the output of the student network. Finally, the size of the neural network is constant and small, a negligible fraction of the streamed video data. 
%As a result, \emph{LtC} allows the feedback to be both long-lasting and infrequent; thus, it does not adversely affect bandwidth savings or response delay.
% \nael{This section is confusing and does not make clear points.  I would rather talk about alternatives: everything at source, everything at server (with and without feedback).  Note that at this point we have not introduced our system, so the discussion has to be general.}


%\noindent\textbf{Why not tiny models?} 
% \subsection{Why not tiny models?}

% Recently there have been developments of tiny neural network models \cite{tiny} that can perform video analytics tasks on resource-limited devices. One could argue that why not use these models at the camera to avoid streaming video to remote cloud servers altogether? Although these tiny models can achieve high FPS in IoT and embedded devices, their accuracy is significantly lower than their full-sized counterparts~\cite{incremental_improv}. While these models can be useful for certain applications (e.g., face recognition, gesture detection, etc.), in this paper, we focus on critical applications (e.g., autonomous driving, unmanned flights, etc.) that require near-optimal accuracy.  
% Moreover, the server may have analytics that apply to multiple video streams or may require the video data for other purposes such as training.  

% \nael{This raises expectations that you will compare to the tiny models.}

% \textcolor{purple}{should not following definitions go to the evaluation section?}
% \subsection{What are the performance metrics?} \label{section:performance_metrics}
% We evaluate LtC as well as other baselines based on the following performance metrics: (i) accuracy, (ii) bandwidth use, and (iii) response delay. 
% \begin{itemize}[leftmargin=10pt]
%     \item \textbf{Accuracy:} We use F1 score (the harmonic mean of precision and recall) to measure the downstream analytics accuracy. We use F1 score for two reasons: i) the object and background regions are highly imbalanced; and ii) we want to penalize the false negatives. Consistent with previous approach \cite{du2020server}, we run the pipeline using raw video frames without any spatial or temporal compression (only MPEG is applied) to attain the \emph{ground truth} results. In this way, when we actually apply spatial and temporal compression, the ground truth results reflect any loss that was caused by the compression.  
%     \item \textbf{Bandwidth use:} The bandwidth use of compressed data is normalized with the size of the ground truth data, and represented as a fractional number in the range [0, 1]. This allows us to compare results run on different dataset, and also across arbitrary runtimes. 
%     \item \textbf{Response delay:} The response delay is measured in seconds, and implies the freshness of the results. It consists of the the network delay and the processing delay incurred between the video stream leaving the source and the results leaving the server. 
% \nael{This is inconsistent with our results where temporal did not save a lot (I guess probably it does not account for mpegs temporal compression).  It would be good to go back and relate to the optimal opportunity to see how close we got.}







