%\subsection{Methodology}

\noindent 
{\bf Methodology:} We used Tensorflow and Python to implement LtC within an open-source video streaming emulator for DDS~\cite{du2020server}. We extended this emulator with trace-driven emulation models for CloudSeg \cite{wang2019bridging}, Reducto \cite{li2020reducto}, and Glimpse \cite{chen2015glimpse}. This approach enables us to carry out fair comparisons within the same environment, and using similar assumptions for all the baselines. AWStream \cite{zhang2018awstream} was evaluated separately because it was not amenable to trace-driven emulation.

To the best of our knowledge LtC is the first streaming video analytics framework that uses both spatial and temporal semantic compression. While comparing with spatial (or alternatively, temporal) baselines, we turned off the temporal (and respectively, spatial) part of LtC. Consistent with prior work~\cite{du2020server, li2020reducto} we use object detection as our primary video analytics task; although it can be specialized to any analytics task that also infers the location of the object regions, such as segmentation, tagging, activity recognition, counting, etc. with alternative student training. %\textcolor{red}{Our approach is independent of the object detector model used, and unless otherwise stated we use FasterRCNN-ResNet101 \cite{ren2015faster} to be consistent with other baselines, such as, DDS.}
We evaluate LtC using a dataset consisting of publicly available traffic surveillance videos collected from cameras deployed across various intersection of North America ~\cite{vid1, vid2, vid3, vid4, vid5, vid6, vid7, vid8}, drone videos from the VisDrone2021 dataset ~\cite{visdrone}, dashcam ~\cite{dash1, dash2} and parking lot surveillance videos from public video sharing platforms ~\cite{plot}. Details are in Table ~\ref{tab:dataset}.

\begin{table}[t]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lllll} 
\hline
Name        & \# Videos & Total length & FPS       & Resolution    \\ 
\hline\hline
Traffic cam & 8         & live         & 15 and 30 & 4K and 1080p  \\ 
\hline
Drone       & 8         & 3 mins      & 30        & 4K and 1080p  \\ 
\hline
Dash cam    & 2         & 3 hours      & 30        & 4K            \\ 
\hline
Parking lot & 1         & 5 mins       & 15        & 1080p         \\
\hline
\end{tabular}
}
\caption{Details of the video dataset}
\vspace{-0.3in}
\label{tab:dataset}
\end{table}

We also evaluate LtC both in a resource-constrained network (1.2Mbps 100ms), and a more resource rich (100Mbps 20ms) network. We use Linux NetEm \cite{netem} to control network bandwidth and delay. %For both the server and the source we machines with Intel Core i7-10750H and a Nvidia GeForce GTX 1650 Ti GPU.  
Lastly, MPEG is used as a post-processing step to provide additional compression beyond the spatial and temporal compression, although any standard compression is equally valid for this purpose. We add this step to ensure fair comparison of LtC, against a number of baselines, such as, DDS ~\cite{du2020server}, AWStream ~\cite{zhang2018awstream}, and Cloudseg ~\cite{wang2019bridging} that integrally use MPEG in their implementations.

% Please note that MPEG, beyond compressing through spatial and temporal techniques, uses a standard compression algorithm to reduce the size of the images. Thus, we use MPEG as a post-processing step in order to further compress the video. We could have replaced this step with any standard compression algorithm, but using MPEG enables us to equalize this compression algorithm since many baselines such as DDS, AWStream and CloudSeg already use MPEG. % It is noteworthy that, all the arguments made in this paper are valid without this step, and added to ensure fair comparison between LtC and a number of baselines that integrally uses MPEG, such as, DDS, AWStream, and CloudSeg. 

% \subsection{Optimization using MPEG}
% As we have discussed in previous sections, traditional video codecs (e.g. MPEG) uniformly compresses the entire video, and therefore, suffers in performance compared to the selective compression of semantic codecs during video analytics. While this is true, we have found that applying MPEG as a post-process to semantic compression enables us to further reduce the video size. The reason behind this is, after spatially and temporally compressing the video volume using semantic compression, MPEG is still the best way to degrade the quality of the unimportant background regions, or to rule out any remaining spatial or temporal redundancies. In order to leverage MPEG, we send the video in constant length batches (usually of 30 frames), instead of sending individually as single frames. It is noteworthy that, all the arguments made in the previous sections and the experiments shown in evaluations in future sections are still valid without this MPEG post-processing step, and it was applied throughout all the baselines to ensure fairness with algorithms that integrally uses MPEG, such as, DDS\cite{du2020server}, and AWStream\cite{zhang2018awstream}.

%\subsection{Performance metrics}
We evaluate LtC on the following performance metrics: 

\begin{itemize}[leftmargin=10pt]
\setlength{\itemsep}{0in}
    \item \textbf{F1-score:} We use F1-score to measure the performance of the downstream analytics. The results from using an uncompressed image is considered as the ground truth, which is consistent with the previous literature \cite{du2020server, li2020reducto}.
    % We use F1 score (the harmonic mean of precision and recall) to measure the accuracy of the downstream analytics. F1 score is appropriate in the context of video analytics for two reasons: i) the object and background regions are highly imbalanced; and ii) we want to penalize the false negatives. The ground truth results are achieved in the hypothetical scenario where the full-sized object detector DNN can be run on the source. Consistent with previous approaches \cite{du2020server, li2020reducto}, this is alternatively calculated by sending uncompressed frames (perhaps only MPEG is applied) to the server. In this way, when we actually apply semantic spatial and temporal compression on the video frames, the ground truth results reflect any loss that was caused by the compression.  
    \item \textbf{Bandwidth use:} The bandwidth use of transported video data is normalized with the size of the MPEG compressed image at the highest resolution (no loss in quality). % This allows us to compare the results across different dataset, and arbitrary runtime. 
    \item \textbf{Response delay:} This delay is the time from when a video frame is sent until the output of the analysis is ready at the server consisting of both network and processing delays. %consists of the network delay and the processing delay incurred between a video frame leaving the source and the results leaving the server. 
\end{itemize}

\subsection{Evaluating LtC's Spatial Compression}

We compare LtC with a state-of-the-art spatial compression framework DDS~\cite{du2020server}, and a few other spatial baselines, such as, AWStream \cite{zhang2018awstream} and CloudSeg \cite{wang2019bridging}. During these comparisons we disable the temporal filtering module of LtC, since the other baselines do not support temporal filtering (beyond that provided by MPEG).

%since both LtC and DDS use semantic compression by identifying regions that need to be compressed differentially.  

% In this section, we compare LtC with the state-of-the-art spatial compression algorithm DDS~\cite{du2020server}, and other spatial baselines, such as, AWStream \cite{zhang2018awstream} and CloudSeg \cite{wang2019bridging}. Since none of these algorithms use temporal compression, we disable the temporal compression module of LtC to ensure a fair comparison of just the spatial compression module.  We are particularly interested in the comparison with DDS~\cite{du2020server} since both LtC and DDS uses a spatial compression that selectively cuts out the object regions. DDS achieves this by sending first a low-quality version of the video frames to the server in order to generate probable bounding boxes around object regions, and then feeds that back to source. Whereas, LtC uses the trained student network to identify such regions without requiring frequent feedback.

%\noindent \textbf{Performance in different operating points.} Unlike LtC, 


We compare the F1-scores and total bandwidth consumption (broken down into bandwidth used for both low and high resolution phases) of LtC and DDS~\cite{du2020server} against the normalized resolutions of the low-quality regions. As presented in Figure ~\ref{fig:accuracy_dds_ltc}, LtC outperforms DDS across the entire range, specially in the low resolution range. This is because, DDS sends the low-quality regions in order to generate feedback at the server, and decreasing the resolution of these regions results in diminished feedback. Whereas, LtC has access to high-quality frames at the source, and the low-quality regions are sent to compensate for any misclassified regions.
% Whereas, LtC has the privilege of having access to the high-quality frames at the source, and low-quality regions are used to compensate for missing features. 
As a result, at lower resolutions of the low-quality regions, DDS struggles to find out the locations of the high-quality regions accurately.
% LtC is able to find out the locations of the high-quality regions more accurately. 
Also, as presented in Figure \ref{fig:bw_dds_ltc}, the sizes of the low-quality regions are less in LtC than DDS. This is because, DDS needs to send all the low-quality regions for feedback, however, LtC can exclude the low-quality regions where it sends them in high-quality, resulting in less or equivalent bandwidth use at high F1-scores.   




% We compare the F1-score and total bandwidth consumption (broken down into bandwidth used for low and high resolution phases) (Figure \ref{fig:bw_dds_ltc}) of LtC and DDS~\cite{du2020server} against the normalized resolution of the low-quality regions. As we can see in Figure ~\ref{fig:accuracy_dds_ltc}, LtC outperforms DDS across the entire range, specially in the low resolutions. This is because, the accuracy of the generated feedback in DDS is dependent on the quality of these low-resolution regions.



% This is because, LtC uses the low-resolution patches only for the background regions., whereas DDS uses them for generating feedback. 


% We are particularly interested in the comparison of LtC against DDS~\cite{du2020server}, since both use semantic spatial compression with similar overall properties. Specifically, DDS sends low quality frames containing both object and background regions, and uses the server to identify regions that need to be sent at higher quality. These regions are provided back to the source to be sent again in high quality to complete the input.  Whereas, LtC identifies these regions directly at the source using the small student network. One of the advantages of LtC is that it has access to the full quality video frames at the source. This allows the student network in LtC to be highly accurate, where in DDS, the teacher network at the server must operate on lower quality baseline video. On the other hand, DDS benefits from the more accurate teacher network in providing the feedback, which may resolve some of the objects even at low quality allowing them to be omitted from the feedback.
% in DDS, the server benefits from the more accurate teacher network in providing the feedback, and may also be able to resolve some objects even at low quality, omitting them in the feedback.  %In turn, LtC sends more high-quality regions than DDS because it is conservative in identifying areas of possible presence of objects.%sends high-quality patches only for the feedback regions (which is only a fraction of the object regions, as some of the objects are captured by the object detector DNN using the low-quality frames), where LtC sends high-quality regions containing all the objects. 

% We can see the effect of increasing the resolution of the low quality regions on the F1-score and bandwidth efficiency in Figure ~\ref{fig:accuracy_dds_ltc}. The bandwidth used to transport the low quality regions in DDS increases more rapidly, as it has to send low quality regions for both object and background regions in the baseline video. Whereas, LtC sends low quality regions only for the backgrounds. Moreover, for a fixed resolution of the low quality regions, the F1-score of LtC is higher, as DDS depends highly on these regions for accurate feedback. Improving the resolution of the low quality regions also has some effect on LtC, since they might compensate for any missing object regions caused by misclassification of the student network. We also show the update size of LtC in Figure \ref{fig:bw_dds_ltc}.
% As can be seen in Figure~\ref{fig:accuracy_dds_ltc}, increasing the low quality video has substantial effect on the accuracy of DDS, since it is necessary for accurate feedback.   In this experiment, we increase the resolution of the low  quality regions.   Improving the low quality video resolution also has some effect on LtC since the student network may miss some parts of objects that are classified as background, improving performance slightly when the background patches are sent at higher resolution.  The resulting bandwidth usage is shown in Figure \ref{fig:bw_dds_ltc} (also broken down into low quality and high quality regions) for both LtC and DDS.  % Notably, the sizes of the high-quality regions remain same as the resolution of the low-quality regions increase. This is because the total amount of object regions in a video remains constant, and as the resolution of the low-quality regions increase, the object detector DNN generates more accurate feedback, replacing the false feedback regions with the true ones.


%  As can be seen in Figure \ref{fig:accuracy_dds_ltc}, the accuracy of DDS is significantlyfor the same F-1 score, LtC is able to significantly improve the bandwidth, with an advantage that increases if higher accuracy is desired.  

%LtC still benefits from the low-quality regions as they compensate for any missing features caused by misclassifaction of the student network. However, LtC is far less dependent on these low-quality regions than DDS, because it does not need these regions to generate feedback. As a result, LtC achieves a higher F1 score over the whole range, as can be seen in Figure \ref{fig:accuracy_dds_ltc}.

% Figure environment removed


% DDS has a number of operating points based on the quality of the video frames that are initially sent by default. The higher this quality is, the better are the feedback regions provided by the server, but in turn lowers the compression ratio. Thus, in this experiment, we vary this resolution from 1\% upto 90\% of the highest-quality; this is referred to as the low-resolution phase. Based on the feedback, the source sends high-quality patches for the feedback regions; we refer to it as high-resolution phase.

% Intuitively, LtC do not rely on the low-resolution phase because it generates the equivalent of the feedback regions using the student network directly at the source. Moreover, the student network has access to the full-quality video frames available at the source, and use that to more accurately find the object regions. In reality, the low-resolution patches in the background regions still benefit the overall accuracy, because they compensate for any missing features caused by misclassification of the student network. However, LtC is far less dependent on these low-resolution patches than DDS, because it does not need these patches in order to generate feedback regions. As a result, LtC achieves higher F1 score over the whole range, even when the low-resolution phase is below 10\% of the highest-quality, as seen in Figure (the x-axis represents the normalized resolution).

% Based on the previous paragraph, LtC sends high-resolution patches for the object regions and low-resolution patches for the background regions, in a single phase. Whereas, DDS sends low-resolution patches for both object and background regions in the initial phase, but sends high-resolution patches only for the feedback regions in the following phase. One thing to note here, the feedback regions in DDS do not encompass all the object regions in a frame. This is because DDS is able to detect a fraction of the objects using the low-resolution frames, and these are excluded from the feedback. As a result, the low-resolution size of LtC is smaller than DDS, and the high-resolution size of LtC is larger than DDS, as seen in Figure . However, for the same accuracy target, the total size of LtC remains lower that that of DDS.

\noindent \textbf{Performance against target F1-scores.} For a given target F1-score, LtC can significantly outperform DDS in terms of bandwidth usage even without considering its temporal filtering. In order to generate accurate feedback regions to reach an target F1-score, DDS has to send more low quality regions (almost double) than LtC, as can be seen in Figure \ref{fig:accuracy_dds_ltc_alt}. In turn, LtC sends more high quality regions, as it has to send all the regions containing objects in high resolution. For an increasing F1-score demand, the gain of LtC over DDS also increases, as the size of the high quality regions remains the same, but the size of the low quality regions are larger and increases more rapidly for DDS. 

% Figure environment removed

\noindent \textbf{Performance against different video types.} LtC and DDS perform similarly on different videos. In general, in a video with sparse frames (in terms of object presence), the bandwidth savings are higher. However, in denser videos, LtC sends comparatively more high-quality regions and uses more bandwidth. %In the parking lot video, the camera is set in a way that the frames are filled with cars, and compared to other videos in the dataset, LtC has the smallest gain on it than DDS, as can be seen in Figure \ref{fig:bw_dds_ltc_alt}. 
Therefore, LtC performs better in sparser videos, and DDS in denser videos.
% Figure environment removed

 
% The performance of both LtC and DDS varies with different attributes, such as, content, viewing angle, etc. of the videos. In denser videos, LtC sends more high-resolution patches, and less low-resolution patches. Again, this is becuase LtC does not use server feedback, and sends high-resolution patches for all the object regions. In DDS, the server detects some of the objects during the low resolution phase, and removes them from the feedback. As a result, LtC performs better in sparser videos. Among the four kind of videos in our dataset, the parking lot video is relatively more dense than the other videos (the camera is set in a way that the frame is filled with cars). We can see in Figure that, compared to the other videos in the dataset, LtC has smaller gains in parkiing lot video than DDS.

\noindent \textbf{Performance against other spatial compression baselines.} Figure \ref{fig:i_spat_baselines} presents the F1-scores against the normalized bandwidth. Overall, LtC compresses 42\% of the highest-quality video and uses 23\% less bandwidth than DDS for the same F1-score. Other spatial compression baselines such as AWStream, and CloudSeg perform worse because they do not use semantic compression. Additionally, CloudSeg has a low F1-score because it uses downsampling and super-resolution based upsampling which appear to harm the semantic features of the video.

The response delay of LtC and DDS is presented in Figure \ref{fig:i_ltc_dds_rd}. LtC has lower network delay than DDS as it does not require feedback from the server. Also, LtC has a lower server processing delay then DDS as it only needs to process the video one time. As a result, LtC achieves an overall 21-45\% shorter response delay than DDS in our two network configurations. This advantage will likely be higher in networks with higher latency and lower bandwidth.

% Figure environment removed


% % Figure environment removed

\subsection{Evaluating LtC's Temporal Filtering} 

In this section, we compare LtC with a state-of-the-art temporal filtering algorithm Reducto~\cite{du2020server}, and an object tracing and filtering based algorithm Glimpse \cite{chen2015glimpse}. Like before, we disable the spatial compression module if LtC to enable a fair comparison.  % We emphasize on the comparison of LtC with Reducto, as they follow a similar feature-differencing based frame filtering method. 
% Specifically, for a particular video and query, Reducto finds out the most potent feature among several candidates (e.g., Pixel, SIFT, HOG, etc.) and also provides a mapping of feature difference and the threshold to use. Whereas, in LtC, we extract features from an intermediate layer of the student network, and use feature difference against an optimal constant threshold.

\noindent\textbf{Performance against target F1-score.} For a given target F1-score, LtC  outperforms Reducto in terms of number of frames filtered. In order to reach a high target F1-score, Reducto becomes conservative about using its existing profiles, and sends out more frames for profiling, as can be seen in Figure \ref{fig:acc_wise_red}. In turn, in LtC, the student network learns about object features implicitly while training. LtC also gains because the filtering based on its features is more accurate, leading to higher precision in identifying the correct frames to discard. 
% \nael{Is it possible to get some kind of direct evidence that we are filtering frames more precisely?}% For higher accuracy, the gain of LtC over Reducto also increases because of the additional frames that Reducto sends to the server for profiling. 
% Moreover, instead of finding out the best possible feature to use in a certain scenario, the student network implicitly learns the appropriate abstract features that are instrumental for detecting an object region. Thus, the features extracted from the student network is also more contextual than any manually crafted features.
% Figure environment removed

\noindent\textbf{Performance against different video types.} Both LtC and Reducto are sensitive to the frame-rate and the content of the video, which determines the available opportunity for temporal filtering. Intuitively, if the frame-rate of a video is high then we have higher redundancy. In addition, there are more redundancies in a static or slowly changing video. Among the four kinds of videos in our dataset, the parking lot dataset is relatively static compared to the other videos (cars are only going in and out of the parking lot occasionally). In Figure \ref{fig:vwf}, we can see that for the parking lot videos both DDS and LtC is able to filter more frames compared to the other videos in the dataset.
% Figure environment removed

\noindent\textbf{Performance against other temporal compression baselines.}  Figure \ref{fig:i_tmp_baselines} shows that LtC is able to filter 8-14\% more frames, and uses 8\% less bandwidth than Reducto for the same F1-score. The other baseline, Glimpse, falls behind in F1-score, because it filters more aggressively with a suboptimal policy, leading to a loss in analytics performance.
%many frames from the outgoing stream.
The response delay of LtC and Reducto is similar, since hey filter at the source.
% Figure environment removed

\subsection{End to end evaluation of LtC}  In Figure \ref{fig:i_e2e_all_baselines_acc_bw}, we present the accuracy-bandwidth tradeoff of LtC against all other baselines. Overall, LtC is uses 28-35\% less bandwidth than the closest baselines DDS and Reducto. %Other systems fall behind for the same reasons discussed before.
% \noindent\textbf{Bandwidth use.} Figure \ref{fig:i_e2e_all_baselines_acc_bw} demonstrates the tradeoff between F1 score and bandwidth use of LtC with regards to other baselines in an end-to-end setup. Overall, LtC is able to compress 48\% of the full-sized video, reduce up to 40-53\% bandwidth use than the closest baselines DDS and Reducto for the same F1 score. Other compression fall behind for the same reasons discussed previously. 
% Compared to the highest-quality video, LtC is able to reduce 55\% bandwidth usage for a high F1 score of 94\%. Moreover, LtC improves 30\% in bandwidth savings over the closest baseline DDS for a comparable accuracy. Other baselines, such as, AWStream and Reducto does not apply a non-uniform semantic compression that discriminates between object and background regions, and therefore, falls behind is bandwidth savings. Cloudseg applies a feature obnoxious downsampling, and subsequent upsampling using super-resolution, and therefore suffers in accuracy. Lastly, Glimpse utilizes a suboptimal object tracking and frame caching method that filters out significant amount of frames in order to save bandwidth, however, at the cost of a large reduction in accuracy. \\
Moreover, for both the resource-constrained and the resource-rich network, LtC has shorter response delay than both DDS and Reducto. Specifically, LtC achieves 14-45\% shorter response delay than these baselines. Interestingly, for resource-rich network, the server processing delay of DDS exceeds the network delay, resulting in a significant advantage for LtC, as can be seen in Figure \ref{fig:i_e2e_all_baselines_rd}.

% Figure environment removed


% % Figure environment removed

%\subsection{Performance in different configuration}
\noindent \textbf{Granularity of identifying objects.} %\naell{Patches are a new term-- what are they?  Is it the number of equal size segments to characterize objectness?} 
The student network operates on fixed-size non-overlapping patches of each frame, to identify if they contain objects. As the number of patches along a single axis (if this number is 10, then there are 100 patches in total) increases, the performance of the student network (measured in \underline{F}rames \underline{P}er \underline{S}econd, FPS) decreases rapidly. However, increasing the number of patches results in little added benefit  beyond 20 patches. Even at a smaller number, such as 10, the bandwidth increases only slightly as the patches becomes larger. We use 16 patches along a single axis, which can run at up to 65 FPS on our machine.

\noindent \textbf{Number of layers.} The performance of LtC also varies with the number of convolution layers used in the student network. As the number increases the FPS value of the student network decreases. However, there is no added benefit when increasing beyond 2 layers (the configuration we use).
The teacher network runs at 2 FPS, while the student network runs at 65 FPS (32x faster), due to its much simpler architecture. %This means that the student is 32 times faster than the teacher, and has significantly less resource requirements. Lastly, the number of patches and convolution layers can be configured differently based on the available resources in the devices.
% Figure environment removed
% Figure environment removed


































% We extend an existing open source video streaming emulator \cite{du2020server, li2020reducto} to model \emph{LtC}. The emulator already had a model of DDS only~\cite{du2020server}.  We modeled Reducto~\cite{li2020reducto}.  We also implemented a trace based emulation of the other baselines (Glimpse~\cite{chen2015glimpse}, CloudSeg~\cite{...}) within the same environment.   AWStream was  evaluated outside this environment using direct execution.  %\nael{What can we say about this emulator?  What does it model?  Do you have a link for the source?  Does it model everything else or did you have to model them? } \textcolor{orange}{this is basically saying I implemented it over DDS without naming names...} \nael{So, DDS already had models for everything and you added LtC, or did you have to implement others?} \textcolor{orange}{Reducto had models for everything, I ran in reducto, and then imported the result in json (list of filtreed frames), and then ran again on DDS using the json. AWstream is different, had to run it separately...}
% To enable a fair comparison and equalize assumptions, we implemented \emph{LtC} within the same environment. 

%  To the best of our knowledge, \emph{LtC} is the first semantic compression algorithm for video analytics that combines both spatial and temporal compression --prior algorithms use either spatial or temporal compression but not both.    \emph{LtC} is configurable to enable only one of spatial or temporal compression, to enable us to evaluate the value of each of them in isolation.   %To the best of our knowledge, \emph{LtC} is the first video streaming compression mechanism that applies compresses along both spatial and temporal direction in a unified manner.
%In order to compare the performance of \emph{LtC} against previous baselines that are either spatial or temporal, we have kept only the respective module of \emph{LtC} active. Therefore, 
% This configurability also allows us to compare each of the spatial and temporal algorithms on its own to the baselines that do compression of the same type.  More specifically, we compare\emph{LtC}'s spatial compression against the state-of-the art spatial compression DDS \cite{du2020server}. Similarly, for temporal performance, we compare \emph{LtC} against Reducto \cite{li2020reducto}. Finally, we compare \emph{LtC} overall (with both spatial and temporal compression active) against a number of previous baselines. 

% We use object detection as our main vision task, consistent with prior works in this area; \emph{LtC} is applicable with some fine-tuning to other tasks such as segmentation, tagging, activity recognition, and counting. Unless otherwise indicated, we use FasterRCNN-ResNet101 \cite{ren2015faster} for the server-side DNN, which serves both as the teacher network as well as the consumer of the video where we evaluate the accuracy.  We evaluate \emph{LtC} in a number of environments: we collect video footage from a range of real-word scenarios with both static and mobile cameras, and varying properties in terms of the recorded activities.   Specifically, the video dataset consists of traffic footage collected from live streaming public videos from surveillance cameras deployed across North America \cite{vid1, vid2, vid3, vid4, vid5, vid6, vid7, vid8}, drone videos from the VisDrone2021 dataset \cite{visdrone}, and Dashcam \cite{dash1, dash2} and Parking-lot \cite{plot} footage from public video sharing platforms. Table \ref{table:dataset} provides an overall summary of our dataset.  


% We also evaluate \emph{LtC} both in a resource-constrained network (1.2Mbps 350ms), perhaps representative of a wireless scenario, and a more resource rich (80Mbps 100ms) network.  We use Linux NetEm \cite{netem} to control the network bandwidth and delay for our experiments.  For our client and server machines we have used machines powered by a Intel Core i7-10750H and a NVIDIA GeForce GTX 1650 Ti GPU. 

%\nael{Lets delay discussion of metrics until the experiments.}
%We evaluate the performance of \emph{LtC} based on three metrics: i) accuracy, ii) bandwidth usage, and iii) network delay. In order to quantify accuracy we need a method that will award the true values and penalize the false values. As a result, F1 score is used to present accuracy. Moreover, in order to compare bandwidth usage among various run-time and content we normalize the size of the spatially and temporallly compressed video using the size of the raw uncompressed video.

% \nael{This is unusual here (the takeaways).  My preference is that we should have these in the intro, but wait until the results are presented and then summarize in discussion or conclusions.}
% \subsection{Key takeaways}
% We list the key takeaways from the evaluation of \emph{LtC} below:
% \begin{itemize}[leftmargin=10pt]
%     \item In spatial compression, \emph{LtC} reduces the per-frame network delay by a 52\% comparing with the highest accuracy baseline, while reducing the bandwidth usage by upto 22\%. 
%     \item In temporal compression, \emph{LtC} is able to filter 5-28\% more frames comparing with the highest accuracy baseline.
%     \item In an end-to-end setup, \emph{LtC} is able to reduce the bandwidth usage by upto 31\% and reduce the per-frame network delay by upto 52\% percent.
%     \item Lastly, \emph{LtC} is specially effective in the high accuracy regime, achieving substantial gains than the closest baselines.
% \end{itemize}








% DDS implements this feedback by sending low quality video to the server, and having the server send feedback on areas where it suspects there were unidentifiable objects.  Conversely, \emph{LtC} uses the trained student network to identify such areas at the source, without requiring feedback.  
% As a result, we expect \emph{LtC} to have lower latency (no feedback is necessary from the server).  
% this allows us to perform in-depth comparison of different aspects between \emph{LtC} and DDS. The F1 score is presented as a percentage, and the bandwidth usage is normalized with the size of the full-resolution video frames.



% Intuitively, LtC should not rely on the low resolution image sent to the server to enable accurate feedback because it operates at the source. The student network in \emph{LtC} has access to the full-resolution video available at the camera. As a result, it can accurately detect the regions that contain objects using the full-resolution frames. The low-resolution regions still benefit the overall accuracy because they compensate for any missing features in the high-resolution regions during the DNN inference. However, \emph{LtC} is less dependent on the low-resolution regions, whereas DDS needs better quality for all low-resolution regions in order to generate high-quality feedback.  As a result, \emph{LtC} achieves higher overall accuracy (F1-score) over the whole range, even when the low resolution component is aggressively compressed as can be seen in Figure~\ref{fig:accuracy_dds_ltc} (the x-axis shows the fraction of the resolution).  % However, DDS benefits from more accurate feedback from the deeper network on the server especially as the resolution of the baseline video sent to the server increases. 

% Figure~\ref{fig:bw_dds_ltc} shows the bandwidth consumed by both DDS and \emph{LtC} broken into the low-resolution and high-resolution components.   \emph{LtC} consumes more bandwidth for the high resolution regions because it sends high resolution for all areas that the student network suspects include an object.  In contrast, DDS only asks for high resolution for a subset of those regions where it was not able to identify the object using the low resolution video.  %However, for the same accuracy (F1 score), \emph{LtC} uses significantly less bandwidth than DDS at higher F1 scores as can be seen in Figure~\ref{fig:accuracy_dds_ltc}.  Note that \emph{LtC} has other advantages that we will show later in this section: it achieves significantly lower latency by avoiding the feedback process.  It is also able to carry out temporal compression for additional efficiencies, which is not possible for DDS where the image is not analyzed at the source. 
  %Notably, unlike DDS, LtC doesn't have to send all the low-resolution regions for accurate analytics, and saves some bandwidth usage there. On the other hand, DDS enjoys direct communication with the DNN at the server. 
  %Consequently, it has to send high-resolution patches for only the patches that the DNN fails to classify, which are only a fraction of all the object patches. Thus, DDS can work with smaller amount of high-resolution patches than LtC. \\

% A significant advantage for \emph{LtC} over DDS is that \emph{LtC} does not require feedback from the server to identify high-resolution patches, resulting in much lower latency.  Our next experiment shows that, as a result, \emph{LtC} is able to achieve per-frame network delay around half that of DDS (52\% of DDS for the high bandwidth network).  This is the time from the generation of a frame, until the classifier at the server has the full information to carry out its inference task.  It includes the time to carry out the student network inference (\emph{LtC}) as well as the server inference time to generate the feedback regions (DDS).  \emph{LtC} requires a single transfer from the source to the server, while \emph{DDS} requires a first transfer of the low resolution video, then a relay of the feedback regions back to the source, and then transfer of the high resolution patches.  This is a crucial factor in real-time systems where the video analytics drives control decisions.  We expect the advantage to be higher if the network bandwidth is higher (making the transfer delay lower) and the latency is higher; this could be the case where the server is a remote cloud server.  % advantage is likely to be higher IoT and embedded systems, where there is usually a lot of contention, and the network resources are a premium commodity. 
%\nael{The point about IoT and embedded systems is contradicted by your results where the constrained network has lower advantage.  I rephrased} 
% Moreover, recall that for a given F1 score target, \emph{LtC} can save significant bandwidth compared to the closest baseline. 

%\nael{Overall, there is a pattern of the results and experiments being unclear.  Our job is to explain exactly and honestly what the experiment are to the reviewers (if we are hiding something to try to make ourselves look better or because we are taking experimental shortcuts, then this is not honest.  We should not just give a result with a lot of missing context and let them guess what we did. "Per-frame network delay" is not defined.  The results dont make sense (e.g., you'd think like you said that the advantage is higher for the higher delay network.  I'll try to revise}


% \nael{For figure 12 (currently titled performance in target accuracy) and 14 (performance in video types), its hard to see the takehome number of how much we save.  Perhaps you can put the number on top as you did in 14.  For 14, we can omit the accuracy and just say the target F1 score.  Why did you pick 90 percent instead of higher where LtC saves even more?  If you cannot do this, maybe we can explain slowly and clearly in the caption.  I'll try to do that.}

% \noindent \textbf{Performance in different video types.} The next experiment evaluates the performance for different video workloads.  The performance of both LtC and DDS is sensitive to the video content.  In videos with fewer objects, both LtC and DDS consume less bandwidth: LtC will need to send less high-resolution patches, and similarly DDS identifies fewer feedback regions, and therefore less high-resolution patches.  Moreover, highly dynamic scenarios may trigger retraining of the student network in LtC.    The video set includes both sparse traffic cam videos and denser videos, as well as viedos with varying degrees of mobility from mobile drone and dashcam videos to static traffic cam videos. In all video categories, LtC was able to perform better than DDS, again due to being able to achieve high accuracy without sending a higher resolution baseline video to the server to generate accurate feedback. %This is directly a result of DDS needing larger low-resolution images to generate accurate feedback regions.
%\nael{Why are the F1 scores all over the place?  It makes it seem like you picked the setting for each video that makes you look best.  Why dont we remove the F1 scores from the top, and replace perhaps with the percentage saved}


% For a given target F1 score, LtC can significantly outperform DDS because it can more aggressively compress the low resolution components of the video. In contrast, in order to reach higher accuracy, DDS needs sufficient resolution in the baseline low-resolution video to generate quality feedback regions. In Figure \ref{fig:accuracy_dds_ltc} we  observe that, DDS needs higher quality of low-resolution video to reach a target accuracy than LtC, making it less efficient in terms of the low resolution component. Moreover, we see that the amount of high-resolution patches required for LtC remains fairly constant among various target accuracy goals, as the fraction of objects present in the frames remain the same. %With higher baseline accuracy, LtC still gains some incremental improvement in accuracy.



% We compare LtC against Reducto~\cite{li2020reducto}, a recent state-of-the-art frame filtering temporal compression algorithm that leverages frame-differencing to determine  whether a frame should be transmitted.  We also compare \emph{LtC} against Glimpse, which is an object tracking and filtering system\cite{chen2015glimpse}. Similar to how we evaluated \emph{LtC}'s, while comparing with these temporal compression techniques, we disable spatial compression.  %We present F1 score for these individual approaches, along with percentage of frames that were kept in a one second segment, used for profiling, and omitted.


