{
  "title": "SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems",
  "authors": [
    "Amir Samadi",
    "Amir Shirian",
    "Konstantinos Koufos",
    "Kurt Debattista",
    "Mehrdad Dianati"
  ],
  "submission_date": "2023-07-28T19:56:01+00:00",
  "revised_dates": [],
  "abstract": "A CF explainer identifies the minimum modifications in the input that would alter the model's output to its complement. In other words, a CF explainer computes the minimum modifications required to cross the model's decision boundary. Current deep generative CF models often work with user-selected features rather than focusing on the discriminative features of the black-box model. Consequently, such CF examples may not necessarily lie near the decision boundary, thereby contradicting the definition of CFs. To address this issue, we propose in this paper a novel approach that leverages saliency maps to generate more informative CF explanations. Source codes are available at: https://github.com/Amir-Samadi//Saliency_Aware_CF.",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.LO"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15786",
  "pdf_url": "https://arxiv.org/pdf/2307.15786v1",
  "comment": "This paper is accepted at the 26th IEEE International Conference on Intelligent Transportation Systems (ITSC 2023)",
  "num_versions": null,
  "size_before_bytes": 17177462,
  "size_after_bytes": 2113044
}