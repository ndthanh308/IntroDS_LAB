
Deep Neural Networks (DNNs) have achieved remarkable success in solving complex tasks, ranging from image recognition \cite{zou2023object} to natural language processing \cite{hirschberg2015advances}. However, their black-box nature has impeded their widespread adoption in safety-critical applications, such as healthcare \cite{mahmud2018applications} and automated driving systems \cite{tampuu2020survey}, where interpretability and transparency are paramount. The advent of Interpretable Artificial Intelligence (IAI) has offered a potential solution to these challenges, and CounterFactual (CF) explanations have gained prominence as a promising IAI approach for revealing in human-understandable terms the underlying rationale behind the decisions of black-box AI systems. A CF example identifies the minimum required changes to the model's input that would alter the model output to its complement. % Such explanations illustrate the minimum modifications that are able to surpass the modelâ€™s decision boundary.
% Figure environment removed

% The variations of existing models and their drawbacks
In recent years, a number of approaches have been proposed for generating CF explanations. Perturbation-based techniques modify the input data by adding or removing specific features~\cite{ivanovs2021perturbation}, while gradient-based methods use the model gradients to identify and alter the most influential features of the input data~\cite{wachter2017counterfactual}. Unfortunately, despite their promising performance with low-dimensional tabular data, these methods suffer from various limitations. On the one hand, perturbation-based methods can be computationally expensive and often generate non-realistic or irrelevant CF instances~\cite{yang2021generative, samadi2023counterfactual}. On the other hand, gradient-based techniques may not be suitable for DNNs due to their non-linearity, particularly when processing high-dimensional image input data, resulting in adversarial examples~\cite{wachter2017counterfactual, goodfellow2014explaining, moosavi2016deepfool}. 

To overcome the above shortcomings, deep generative models, such as Generative Adversarial Networks (GANs)~\cite{goodfellow2020generative}, Variational Auto Encoders (VAEs)~\cite{kim2021counterfactual} and Diffusion Models \cite{jeanneret2022diffusion} have emerged in the literature as promising solutions. Generative models can learn to map the distribution of (high-dimensional) input images between different domains. Transferring a query image from a source domain to another (target) domain is essentially the objective of the CF explanation. Therefore, deep generative models can produce diverse and realistic CF examples for high-dimensional input data~\cite{chou2022counterfactuals}. However, for complex domains, e.g., driving scenarios, generative models tend to generate implausible or semantically invalid examples~\cite{jacob2022steex}. One way to remedy this issue is to limit the potential solutions by encoding the query images into a disentangled latent state and applying their sparse modification~\cite{jacob2022steex, zemni2022octet, rodriguez2021beyond}. This approach involves the manual selection of the input features, which may lead to sub-optimal and biased explanations nonetheless. For example, the Steex method developed in~\cite{jacob2022steex} generates CF examples by modifying groups of pixels specified in the segmented semantic layer. Although this can provide informative explanations, it actually deviates from the definition of CFs; recall that a CF seeks minimal changes in the input. An alternative solution may exist within another semantic feature class that is closer to the decision boundary of the model, see Fig. \ref{fig:teaser}(a) for an example illustration. %Therefore, such a biased explanation could result in an inexact comprehension of the black-box model. 
This research gap in generating relevant and unbiased CF explanations near the decision boundary of the model serves as the key motivation for the present study.

% why saliency mapes
To this end, instead of altering user-defined input features, we propose a novel approach that leverages saliency maps to explicitly guide the generative model to focus on the most influential features of the input and generate more effective and unbiased CF explanations. Saliency maps constitute a significant development in advancing the realm of IAI, as they provide insights into the regions of the input that receive the most attention from the black-box model during its decision-making process~\cite{selvaraju2017grad,wang2020score}. That can enhance the quality of the generated CF explanations and achieve a more accurate estimation of the decision boundary, which is crucial for improving the interpretability of DNN models. 
%By incorporating saliency maps into our generative CF approach, we aim to address the limitation of previous methods that only consider altering user-defined input features. Instead, our approach uses saliency maps to guide the generation of CFs that focus on the most influential region of the input for the model's decision. 
% Consequently, we aim to achieve a more accurate estimation of the decision boundary, which can be crucial for improving the trustworthiness and reliability of DNN models. 
The connection between the saliency map and the decision boundary has been explored in previous studies~\cite{etmann2019connection,mangla2021saliency} and an intuitive illustration of that can be found in Fig.~\ref{fig:teaser}(b). % and explored in previous studies~\cite{etmann2019connection, mangla2021saliency}, serves as a guide for the generative model to traverse the decision boundary more closely.
% what we are going to do in this work
%In this paper, we propose a deep generative CF explainer model to IAI, namely Saliency Aware CF Explanation (SAFE), which aims to address the limitations of existing CF methods in generating diverse, realistic and unbiased explanations. 

The main contribution of the present article lies in integrating saliency maps, extracted from pre-trained off-the-shelf models, into the GAN training process to better understand the discriminative features in the input space. 
%Specifically, we demonstrate that by guiding the generative model to focus on the most significant features in the input space through the utilization of saliency maps, we can generate CF instances that are more accurate sparse, and close to the decision boundary. This will enable our proposed method to provide more comprehensive and accurate explanations of the black-box models.
The proposed deep generative CF explainer is hereafter referred to as Saliency-Aware counterFactual Explainer (SAFE) and the associated deep generative model as SAcleGAN. The key contributions of this article to the state-of-art are summarised next:

\begin{itemize}
\item Devising a framework for improving the interpretability of DNNs in complex real-world applications by introducing the use of saliency maps encompassing decision boundary information.
\item Introduction of a novel term in the loss function that ensures the proper fusion of saliency information in the SAcleGAN.
\item A comprehensive performance analysis of the SAcleGAN using a complex driving scene dataset, i.e., the BDD. The evaluation demonstrates the superiority of SAcleGAN over several baseline methods in terms of validity, sparsity, and proximity of generated CF examples.
\item A qualitative analysis of the results to gain insights into the strengths and limitations of SAFE. The results provide strong empirical evidence of the effectiveness and reliability of our approach.
\end{itemize} 

The rest of this paper is organised as follows. In Section~\ref{section:related}, we briefly review the related work on IAI and generating CF  explanations. In Section~\ref{section:method}, we describe how to leverage saliency maps to guide the generation of CF explanations. 
Section~\ref{section:results} presents the evaluation results and discusses their implications. Finally, we conclude this work in Section~\ref{section: conlusion}.