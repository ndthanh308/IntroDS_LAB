


% In this paper, we proposed a novel method  using saliency maps. 

This paper designed a novel method named after SAFE for generating counterfactual (CF) explanations leveraging a cycle-GAN guided by saliency maps. 
The latter was adopted to highlight the most salient regions of the input image that play a decisive role in determining the output of the machine learning model and subsequently help alter only those regions given the target (complement) label. The proposed method significantly advanced the state-of-art models in terms of  validity (the higher, the better) and sparsity (the lower, the better) of the generated CF examples using the Berkeley DeepDrive dataset. At the same time, the generated CFs were more interpretable and clear to understand by visual inspection. We believe that this method has the potential to strengthen our trust and facilitate the adoption of machine learning models in real-world applications, such as autonomous vehicles and automated driving systems in general, by providing more transparent and interpretable explanations for their  predictions and decision-making. In the future, it is worth validating the performance of SAFE with other datasets too. 