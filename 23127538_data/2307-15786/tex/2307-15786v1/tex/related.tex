This section provides an overview of the related work in the area of interpretability with a focus on CF explanations. More specifically, we review generative CF methods as they have shown to be more effective on real-world data.

% sub
\subsection{Interpretability in Machine Learning}
Interpretability has been a topic of growing interest in the machine-learning community over the past few years. Various methods can increase the interpretability of models, ranging from simple rule-based models and decision trees to more complex techniques such as sensitivity analysis~\cite{samek2016evaluating} and deep learning models~\cite{kim2019grounding}.
These methods become particularly important in safety-critical situations such as autonomous driving applications, where interpretability is essential for reliability, regulatory compliance, and end-user trust~\cite{wachter2017counterfactual}.

%The lack of interpretability has prompted the development of 
Interpretable methods include both post-hoc and intrinsic models. % enabling insights into black-box model behaviour and decision-making processes.
Intrinsic methods refer to the models that involve designing and training DNN models with built-in interpretability and post-hoc methods employ auxiliary models to extract explanations from already trained models.
% Post-hoc interpretable algorithms can be classified as either model-agnostic or model-specific~\cite{}. Model-specific interpretability methods view the model as a grey box, allowing access to internal model features. Conversely, model-agnostic methods consider the model as a black box, where only input and output can be examined. Using saliency maps calculated from internal model parameters aligned this study to post-hoc and model-specific paradigm.
Post-hoc explanations can be global or local, depending on the level of detail they provide. The former provides a holistic view of the main factors driving the decision-making process of the model, and the latter targets to understand the model's behaviour on a specific input~\cite{ribeiro2016should}. 
Various methods, such as saliency maps, LIME~\cite{ribeiro2018anchors}, SHAP~\cite{lundberg2017unified}, partial dependence plots~\cite{ziegel2003elements}, and feature importance rankings~\cite{breiman2001random}, facilitate the generation of both local and global explanations, allowing users to gain insights into model decisions, identify biases or errors, and understand the model's behaviour on individual instances as well as across the entire dataset.
% Local explanations can be generated through a variety of methods, including saliency maps \cite{SimonyanVZ13}, LIME (Local Interpretable Model-Agnostic Explanations) \cite{ribeiro2018anchors}, and SHAP (SHapley Additive exPlanations) \cite{lundberg2017unified}. These methods, including our proposed method, allow users to better understand how a model arrived at a particular decision by highlighting the most important features or inputs that contributed to the decision. Such explanations for individual predictions can be useful in understanding specific instances and identifying potential biases or errors. Another spectrum to interpretability is through the use of global explanations, which aim to explain the overall behaviour of a model. Global explanations can be generated through methods such as partial dependence plots \cite{ziegel2003elements} and feature importance rankings \cite{breiman2001random}. These methods provide insights into how the model makes decisions and which features are most important for making those decisions, resulting in a broader understanding of the model's behaviour and uncovering general patterns and insights across the entire dataset. 


Despite the recent progresses in IAI research, there is still a strong need for better models. The CF explanations are a subset approach to post-hoc explanations and are gaining momentum due to their ability to provide human-understandable insights into the model's behaviour~\cite{wachter2017counterfactual}. Recent works have focused not only on   explaining the model decisions but also on generating CF examples, which are minimally modified versions of the input data that would change the model's output~\cite{wachter2017counterfactual}. 
%CF explanations can provide valuable insights into why a model made a certain decision and can be used to identify biases in the data or model.
In the context of automated driving systems (ADS), CF explanations can improve our understanding of why an automated driving model chose a particular action and what features of the input were the most crucial during decision-making~\cite{kim2017interpretable}. For example, CF explanations can indicate whether the colour of a traffic light or the orientation of a pedestrian's body influenced the model's decision to stop or proceed.
Conventional CF explanation techniques employ optimisation methods, such as gradient-based~\cite{wachter2017counterfactual}, genetic~\cite{guidotti2018local, sharma2019certifai, dandl2020multi} and game-theory\cite{ramon2020comparison, rathi2019generating} approaches and perform well with simple tabular data~\cite{wachter2017counterfactual}. However, they struggle to generate visually appealing outputs when dealing with  high-dimensional image data. To address this issue, deep generative CF models have been introduced. % as a means of mitigating the aforementioned problem.

% sub
\subsection{Deep Generative Counterfactual Models}
Deep generative CF explanations are a promising approach to increasing the interpretability of complex decision-making systems, such as DNNs~\cite{AugustinBC022,jacob2022steex,khorram2022cycle}. 
%For a given decision model and query input, a CF explanation is defined as a minimally modified version of the query that changes the decision of the model. 
For instance, in an initial work\cite{liu2019generative}, a trained classifier named as AttGAN provides labels for a cycleGAN~\cite{he2019attgan}. This deep generative model then transfers the input image to the target domain serving as a CF explanation example. In a follow-up work~\cite{huber2023ganterfactual}, a starGAN~\cite{choi2018stargan} is employed to generate CF examples for a  trained reinforcement learning-based model. The authors in~\cite{kothiyalutilization} conducted a comparison of different cycleGAN models for generating CF examples focusing on image quality metrics. The majority of works can explain classifiers working on simple images, such as face portraits or featuring a single and centered object.
Nevertheless, generating CF explanations in complex environments, such as those encountered in ADS, can be challenging due to the high diversity and scattered distribution of images in the available driving datasets~\cite{jacob2022steex}. To this end, several works tend to limit the solution space for guiding the learning of generative models.

% \st{Initially, counterfactuals were developed for models operating on tabular data \cite{mothilal2020dice}, but they have recently been applied to explain deep computer vision models as well \cite{goyal2019counterfactual,moosavi2016deepfool,vandenhende2022making,jacob2022steex}. One of the challenges is to ensure that the explanations are both minimal and meaningful. Adversarial attacks can be minimal, but they are not informative for the end user \cite{GoodfellowSS14,moosavi2016deepfool}. Retrieval-based methods have been used to mine the dataset for counterfactual explanations, but they can only provide explanations as minimal as the dataset granularity allows \cite{goyal2019counterfactual,hu2022x,vandenhende2022making}.}

% \st{More recently, generator-based counterfactual explanations have been proposed \cite{AugustinBC022,jacob2022steex,khorram2022cycle}. These methods leverage}

One way to restrict the solution space utilises an encoder-decoder pair in deep generative models, which effectively maps the input data distribution into a disentangled latent space~\cite{jacob2022steex, zemni2022octet}. This learned feature space ensures that each dimension captures a specific attribute of the data independently of other dimensions, enabling control and understanding of the underlying features of the data that contribute to the generation of the CF example. Deep generative methods can provide minimal and meaningful CF examples by applying optimised manipulations into the latent space. 
However, such CF explanations are obtained from a user-selected feature space rather than being optimised within the entire input feature space. For instance, the STEEX model in~\cite{jacob2022steex} generates CF examples within the driving environment from a  semantic layer selected by the end-user, such as traffic light segmentation, which might end up being suboptimal as the feature space is restricted. Unlike previous methods, in this study, we use saliency map features which lead the generative model to search near the black-box model's decision boundary, resulting in a global minimum solution. 
%Moreover, STEEX \cite{jacob2022steex} requires annotated semantic segmentation masks of images to work, which is not always available or is costly for the user to produce \cite{zemni2022octet}. Several works have proposed different generator-based approaches to generate CF explanations \cite{kothiyalutilization}. For instance, DIVE \cite{rodriguez2021beyond} exploits the disentangled latent space to generate diverse CF explanations. However, the visual quality of its CFs hinders interpretability. %DVCE  \cite{AugustinBC022} builds on denoising diffusion probabilistic models to achieve better-looking results but lose the more curated control on diversity. 


% \sam{On the other line of work, as an alternative to using disentangled latent state, conditional cycle-GAN methods have been proposed to translate unpaired image-to-image directly. Avoiding intermediate transferring (to the disentangled latent state) enables the deep generative models to extract more comprehensive features (unknown latent state) and decode them to the queried domain. Meanwhile, those extracted features are tied to the given conditional constraints.}

%\st{In the case of complex input data such as perception images collected by ADS, a shift in the input space cannot be expressed by an interpretable mapping function because an interpretable latent space is not known. In practice, conditional cycle-GAN methods have emerged as a solution for unpaired image-to-image translation. They determine the common invariants and changes within a set of CF pairs, effectively explaining the shift from the source to the target distribution. }
% \sam{coppied from the paper: \cite{kulinski2022towards} In some cases, a shift cannot be expressed by an interpretable mapping function because an interpretable latent space is not known. Thus, we can remove the interpretability constraint, and leverage methods from the unpaired Image-to-Image translation (I2I) literature to translate between the source and target domain while preserving the content. For a comprehensive summary of the recent I2I works and methods, please see [14]. Once a mapping is found, to serve as an explanation, we can provide an operator with a set of CF pairs. Then, by determining what commonly stays invariant and what commonly changes across the set of CF pairs, this can serve as an explanation of how the source distribution shifted to the target distribution. While more broadly applicable, this approach could put a higher load on the operator than the interpretable mapping approach.}
%\st{For instance, in an initial work\cite{liu2019generative}, a trained classifier named as AttGAN provides labels for a cycle-GAN, ~\cite{he2019attgan}. This deep generative model then transfers the input image to the target domain, serving as a CF explanation example. In a follow-up work~\cite{huber2023ganterfactual}, a starGAN~\cite{choi2018stargan} is employed to generate CF examples for trained reinforcement learning-based model. The authors in~\cite{kothiyalutilization} conducted a comparison of different cycleGAN models for generating CF examples, focusing on image quality metrics. The majority of works can explain classifiers working on simple images, such as face portraits or featuring a single and centered object. However, scaling up to classifiers operating over more complex images such as crowded urban scenes remains a major challenge.} 

In this study, avoiding intermediate transferring (to the disentangled latent state) enables our approach to extract more comprehensive features (unknown latent state) and decode them to the queried domain. Meanwhile, for limiting the potential solutions, we use a novel loss function that forces the deep generative model to apply changes merely in salient pixels. Considering that saliency maps encompass decision boundaries~\cite{etmann2019connection, mangla2021saliency}, we have in practice limited the SAcleGAN to apply modifications near the decision boundary regions.

% \sam{we need to select a name for the proposed GAN model, so that mention that name instead of overusing cyleGAN. It also distinguishes our works from those used prepared cyleGAN model only.}\ASh{What about SAcycleGAN (Saliency-aware cycle gan)}


% % past
% {\color{red}

% The field of interpretability is rapidly growing with various research approaches. One approach is post-hoc explanation methods that explain why a model made a specific prediction. These methods can be divided into gradient-based \cite{baehrens2010explain, sundararajan2017axiomatic, smilkov2017smoothgrad, shrikumar2017learning, lundberg2017unified, selvaraju2017grad} that can be reformulated as computing backpropagation for a modified gradient function and perturbation-based approaches \cite{zeiler2014visualizing, suresh2017clinical, ribeiro2016should, tonekaboni2020went} that perturb areas of the input and measure how much this changes the model output. Other approaches aim to measure the reliability of interpretability methods by creating standardized benchmarks \cite{hooker2019benchmark, ismail2020benchmarking, deyoung2020benchmark, tomsett2020sanity, samek2016evaluating,
% petsiuk2018rise} or debugging explanations \cite{ba2014deep, frosst2017distilling, ross2017right, wu2018beyond, ismail2019input}. Some researchers focus on modifying neural architectures for better interpretability. Two previous works \cite{ghaeini2019saliency,ross2017right} incorporate explanations into the learning process but rely on ground truth explanations or annotations, which may not always be available. Our proposed approach does not rely on such annotations and is more practical for datasets with only ground truth labels.
% }
% The problem of generating CF explanations for machine learning models has been the subject of significant research in recent years. In this section, we review some of the related work in the literature.

% One approach to generating CF explanations is through perturbation-based methods. These methods involve systematically perturbing the input data until a CF is found that satisfies certain constraints, such as being close to the original input or adhering to some desired outcome. Examples of perturbation-based methods include the LIME algorithm [1], which generates local linear models to approximate the behavior of the underlying machine learning model, and the CF risk minimization (CRM) method [2], which formulates the CF generation problem as an optimization problem.

% Another approach to generating CF explanations is through gradient-based methods. These methods involve computing gradients with respect to the input data and using them to guide the search for CFs. Examples of gradient-based methods include the influential CF instances (ICI) method [3], which computes gradients with respect to a set of influential instances, and the causal CF learning (CCL) method [4], which uses a causal graph to guide the generation of CFs.

% Recently, several GAN-based methods have also been proposed for generating CF explanations. These methods involve training a GAN to generate CF examples that are similar to the original input but result in a different prediction from the machine learning model. Examples of GAN-based methods include the STEEX method [5], which uses a semantic segmentation network to guide the GAN in generating CFs, and the MAD-GAN method [6], which incorporates a memory module to store past CF examples and encourage diversity in the generated CFs.

% In contrast to the above approaches, our proposed method for generating CF explanations incorporates saliency maps into the CF generator. Saliency maps have been used in previous work for various tasks such as visual attention, object recognition, and image segmentation. By using saliency maps to guide the generation of counterfactual explanations, we aim to generate more interpretable and effective explanations while also reducing the computational cost of the generation process.

% \subsection{saliency generation methods}
% {\color{red}
% five different saliency methods to understand the importance of different input features in determining the output of a model. These methods are:





% Gradient (GRAD) \cite{baehrens2010explain}: This method computes the gradient of the output with respect to the input. It gives a measure of how much each input feature affects the output.

% Integrated Gradients (IG) \cite{sundararajan2017axiomatic}: This method calculates a path integral of the model gradient from a non-informative reference point to the input. It helps to understand the contribution of each input feature in the overall output.

% DeepLIFT (DL)\cite{shrikumar2017learning}: This method compares the activation of each neuron to a reference activation and computes the relevance as the difference between the two activations. It helps to understand the importance of each neuron in determining the output.

% SmoothGrad (SG)\cite{smilkov2017smoothgrad}: This method adds noise to the input and takes the average of the resulting sensitivity maps for each sample. It helps to reduce the noise in the sensitivity maps and get a clearer understanding of the importance of each input feature.

% Gradient SHAP (GS)\cite{lundberg2017unified}: This method adds noise to the input, selects a point along the path between a reference point and the input, and computes the gradient of outputs with respect to those points. It helps to understand the importance of each input feature and how it interacts with other input features.
% }

% \subsection{counterfactual}
% {\color{red}
% }

% \subsection{generative counterfactual}
% {\color{red}

% this paper \cite{huber2023ganterfactual} uses adversarial learning techniques to generate counterfactual explanations for RL agents by formulating the problem as a domain transfer problem, so that they can use starGAN \cite{choi2018stargan}.

% This paper \cite{yang2021generative} proposes a framework called Attribute-Informed Perturbation (AIP) for generating counterfactuals for raw data instances such as text and images. AIP utilizes generative models conditioned with different attributes to obtain counterfactuals with desired labels. The framework optimizes the attribute-informed latent space instead of directly modifying instances in the data space, which improves effectiveness and efficiency. Experimental results demonstrate the effectiveness and sample quality of AIP and its potential for practical applications beyond model interpretability.
% This paper \cite{zhao2020fast} presents a Fast ReAl-time Counterfactual Explanation (FRACE) algorithm that uses Generative Adversarial Networks (GANs) to generate perturbations on a query image that causes it to be classified as a counterfactual. The algorithm uses a residual generator trained on starGAN and a perturbation loss, as well as a classifier, to ensure the generated images are conditional on the classifier and have a minimum change from the query. The FRACE algorithm produces more realistic images compared to previous methods and is faster during inference time. Experimental results show the effectiveness and efficiency of FRACE compared to state-of-the-art methods.
% }
% \subsection{attention guided GAN}
% {\color{red}
% starGAN \cite{choi2018stargan}StarGAN, a scalable approach for image-to-image translation for multiple domains using a single model. This approach leads to superior quality of translated images and the capability to flexibly translate an input image to any desired target domain. StarGAN is demonstrated to be effective on facial attribute transfer and facial expression synthesis tasks.
% AttGAN \cite{he2019attgan} is a framework for high-quality facial attribute editing that uses an attribute classification constraint on the generated image, reconstruction learning, and adversarial learning to ensure visually realistic editing. It outperforms state-of-the-art methods and is extended for unsupervised attribute style manipulation. 
% AttentionGAN \cite{tang2021attentiongan} proposes Attention-Guided Generative Adversarial Networks (AttentionGAN) for the task of unpaired image-to-image translation. The proposed method aims to improve the quality of generated images by identifying the most discriminative foreground objects and minimizing the change of the background. AttentionGAN achieves this by using attention-guided generators that produce attention masks, which are then fused with the generation output to obtain high-quality target images. Additionally, a novel attention-guided discriminator is designed which only considers attended regions. Experimental results on several generative tasks with eight public datasets show that AttentionGAN generates sharper and more realistic images compared to existing competitive models.
% }

% \subsection{generating saliency with GAN}
% {\color{red}
% This paper proposes SalGAN \cite{pan2017salgan}, a data-driven metric based saliency prediction method that uses an adversarial loss function. SalGAN consists of two networks that predict and discriminate saliency maps, respectively. The adversarial training enables SalGAN to achieve state-of-the-art performance across various saliency metrics.

% The authors \cite{sun2020implicit} propose an unsupervised method for extracting implicit saliency in deep neural networks, which performs comparably to state-of-the-art supervised algorithms and is more robust to input noise. The method uses semantic features, lowers the threshold for saliency detection in terms of data, and bridges the gap between human and model saliency.

% This paper \cite{liu2021saliency} proposes a saliency-guided remote sensing image super-resolution method (SG-GAN) that utilizes salient maps to guide the recovery process and focus more on salient objects. Experimental results show that SG-GAN produces competitive results and superior structural restoration compared to advanced methods.
% }
% \subsection{saliency-guided GAN}
% {\color{red}
% This paper \cite{jiang2021saliency} proposes a new task for image-to-image translation, where the translation is conditioned on a user-specified saliency map. The authors propose a Generative Adversarial Network (GAN) called SalG-GAN, which can generate a translated image that satisfies the target saliency map. The model uses a disentangled representation framework to encourage diverse translations for the same target saliency condition. The authors also introduce a saliency-based attention module to help facilitate the structures of the generator, cue encoder, and global and local discriminators. The authors provide a synthetic dataset and a real-world dataset with labeled visual attention to train and evaluate their model. The results of experiments on both datasets show the effectiveness of the proposed model for saliency-guided image translation.


% This paper \cite{sun2023tsrnet} proposes an unsupervised two-stage approach called TsrNet for object-level image style transfer on fashion images. The first stage uses Mask-GAN to identify the clothing texture region, and the second stage utilizes an improved CycleGAN with a Drf-module and GTS to perform texture transfer. The approach is effective and demonstrated through qualitative and quantitative experimental results.
% The paper proposes SGFusion \cite{liu2023sgfusion}, a saliency guided deep-learning framework for pixel-level image fusion. It uses dual-guided encoding, image reconstruction decoding, and saliency detection decoding to extract feature and saliency maps from images. The saliency maps are used as fusion weights to generate the fusion image. SGFusion achieves state-of-the-art performance in different types of image fusion tasks, including infrared and visible image fusion, multi-exposure image fusion, and medical image fusion.

% }
% \subsection{counterfactual using att-GAN}
% {\color{red}
% \cite{liu2019generative} trained classifiers then used attgan\cite{he2019attgan} but they did not used saliency which we did.

% \cite{kothiyalutilization} related works
% The framework described in \cite{singla2023explaining}, generates counterfactual visual explanations for the classifier while using a coditional GAN for transparent decision-making process in healthcare applications. The methodology revolves around perturbation of the original input image such that an explanation function is designed using cGAN keeping in mind the three properties of valid transformation namely: data consistency, classification model consistency and context-aware self-consistency. They used metrics like FID (Frechet Inception Distance), CV (Counterfactual Validity) \cite{mothilal2020explaining} and FOP (Foreign Object Preservation) to support the above three properties for valid transformations. FOP is a metric that they devised which helps in measuring if the patient-specific properties (foreign objects) are retained in the image like pacemaker etc. The intent of the task is similar to our experiment with a difference that in their method, counterfactuals are already images. However, in our experiment the counterfactuals are a human-readable text, that is easier for humans to interpret but difficult for testing and validating. Additionally, their work is specifically for healthcare service line and has been tested on datasets: celebA, MNIST and simulated data in comparison to ours.

% The other model STEEX (STEering counterfactual EXplanations using semantics) implemented in \cite{jacob2022steex} makes use of the latest advancements made in the area of semantic-to-real image synthesis in order to achieve “region-targeted counterfactual explanations” (a concept introduced by the authors), which is the highlight of the paper. Their prime attention is to spotlight the how content-based image classification is vital than only region-based classification which is absolutely true when considering scenarios where safety is of utmost importance like self-driving cars. The metrics used for evaluation in their paper include: FID, Face Verification Accuracy (FVA) and Mean Number of Attributes Changed (MNAC), where the model has been trained for CelebA, CelebAMask-HQ and BDD100k datasets. Similar to the implementation in the previous paper, even this model updates the query image to produce the counterfactual image. Their method involves no textual explanations for producing a counterfactual image which is relatively easier to construe by a human for better understanding as used in our model.

% Another methodology discussed in \cite{goyal2019explaining} talks about Causal Concept Effect (CaCE) measure for interpretability and to get rid of the errors arising from confounding. Their model is based on Variational AutoEncoder (VAE), to estimate VAECaCE metric, which as proposed in the paper can estimate the true concept causal effect. In order to showcase the effectiveness of the CaCE metric, they have tested their model on different datasets like MNIST, COCO Miniplaces and CelebA which clearly exhibits the generalizability of their implemented framework. But however, no GitHub repository links have been shared to support the reproducibility of the code for further research. This paper implements StarGAN for producing the counterfactual images, which is similar to our experiment where we have also used StarGAN to produce counterfactual images for celebA and CUB-200-2011 datasets. However, their experiment does not mention about semantic explanations used for producing counterfactual images but is limited to the causal concept effect.

% PIECE (PlausIble Exceptionality-based Contrastive Explanations) algorithm is one more illustration, which combines a GAN that creates counterfactual and semi-factual explanatory images with a CNN that makes predictions that need to be explained on datasets \cite{kenny2021generating}. This model PIECE uses semantic explanations as a base to generate their counterfactual images just like our model. However, PIECE model has been tested only for datasets: CIFAR-10 and MNIST. Furthermore, Figure 7 from \cite{kenny2021generating} showcases the generated image of a counterfactual explanation for a bird using PIECE and Min-Edit as an image generation model for comparison. It is easily visible to the human eye that the images have a very low resolution, and additionally there is no mention of the resolution of images employed for training the classifier and the counterfactual image generating GAN. Considering the results showcased in the paper, their model PIECE performs better in comparison to other methods like Min-Edit, C-Min-Edit, Proto-CF (Interpretable Counterfactual Explanations Guided by Prototypes) and CEM (Contrastive Explanations Method) for the five metrics presented in Table 1 of the report and scores low only for the optimization time taken for each image, which as per the authors can be compensated by utilizing a GPU or reducing the number of epochs.


% As it is clear from the above-mentioned models that research in this specific domain of adding interpretability to counterfactuals is either limited to a specific domain like healthcare or have been tested on simpler datasets like CelebA, MNIST, CIFAR etc. Therefore, we had to take a step further while implementing our model by parsing the complex dataset of CUB-200-2011, which contains 312 attributes for each bird with a total collection of 11,788 images belonging to 200 class labels. Additionally, we needed latest metrics which could measure the feature vector distance in the images and also indicate their perceptual similarity like FID and LPIPS scores.

% In this paper \cite{kang2022fair}, the authors propose a method to address concerns about racial and gender disparities in facial attribute classification by generating synthetic images that consider both factual and counterfactual assumptions about sensitive attributes. This is done using a causal graph-based approach with an encoder-decoder framework, and the attribute classifier is trained using counterfactual regularization. The proposed method is different from previous works, as it uses a causal graph to generate counterfactual images that preserve the identity of the given image. The approach is tested on the CelebA dataset and shows effectiveness and interpretability in classifying multiple face attributes.
% }
% \subsection{counterfactual using sal-GAN}
% {\color{red}
% }