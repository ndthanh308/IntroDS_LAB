

This section provides insights into the SAFE approach that leverages saliency maps to generate CFs using an unpaired dataset in the SAcleGAN. We formulate the problem of generating CFs for a given grey-box model in Section~\ref{subsec: problem_def}, present an overview of the proposed approach in Section~\ref{subsec: SAFE_overview} and finally, describe the SAFE training phase in Section~\ref{subsec: learning}. 

\subsection{Problem Definition}
\label{subsec: problem_def}
Let $x \in \mathcal{R}^{H \times W \times C}$ be an input instance with $H, W$ and $C$ being the height, width and  channel. Also, let $M(\cdot)$ be our grey-box model (having access to some internal hyperparameters), classifying $x$ as $y \in \mathcal{Z}^d$, where $d$ is the number of classes in the grey-box model. Given a target class $y' \neq 
 y $, we shall generate a CF $x' \in \mathcal{R}^{H \times W \times C}$ such that $M(x')=y'$ and $x'$ is as similar as possible to $x$. To this end, the problem in this paper is formulated as leveraging saliency maps to guide the generation of $x'$, i.e., the changes made to $x$ in the search of $x'$ are concentrated in the most salient regions yielding minimal and effective substitutions.

\subsection{Overview of SAFE}
\label{subsec: SAFE_overview}
In this section, we first discuss the construction of the saliency map, and then detail the procedure of CF generation using the SAcleGAN model, see Fig.~\ref{fig: method0}. Previous studies utilised different GAN-based models to create CFs~\cite{liu2019generative, huber2023ganterfactual,kothiyalutilization,jacob2022steex, zemni2022octet}, but they proved inadequate in uncovering those  associated with the minimal and effective changes. To address this, we use  AttentionGAN~\cite{tang2019attention} as backbone model and introduce a novel loss function term to ensure that the applied changes are minimal within an effective area. Following the conventional GAN architecture~\cite{GoodfellowPMXWOCB14}, AttentionGAN is composed of two competing modules, i.e., the generator $G(\cdot)$ and the discriminator $D(\cdot)$,  iteratively trained to compete against each other in the manner of two-player minimax games. %In a more formal manner, 

% Figure environment removed

\subsubsection{Saliency Map Generation}
% Saliency maps are a visualization technique used to understand the behaviour of deep learning models, particularly in image classification tasks~\cite{ selvaraju2017grad, jacobgilpytorchcam}. Saliency maps aim to highlight the regions of an input image that are most important in determining the output class. In other words, they show which parts of the query image attracted more attention to the model during the prediction of such images.
% Saliency maps can be generated using various methods, but the most commonly used method is called the gradient-based approach \cite{selvaraju2017grad}. The gradient-based approach computes the gradient of the output class with respect to the input image. The magnitude of the gradient indicates how much the output class changes when the corresponding input pixel changes. By computing the gradient for each pixel in the image, a saliency map can be obtained that highlights the most important regions.

 Saliency maps are visualisations used to understand the behaviour of deep learning models in image classification tasks by highlighting the parts of an image used to determine the output class. Recently, gradient-based approaches have been used to compute saliency maps by measuring the impact of each pixel on the output. For an input image $x \in \mathcal{R}^{H \times W \times C}$ classified as $y$, the saliency map generator returns a 2D map $s=S(x,y) \in \mathcal{R}^{H \times W}$ representing the salient parts of the input image with respect to the model's output. In order to generate a saliency map, we employ the Gradient-weighted Class Activation Mapping (Grad-CAM)~\cite{WangWDYZDMH20} method. 
% Grad-cam computes the gradients of the predicted class score with respect to the feature maps of the last convolutional layer. These gradients capture the importance of each feature map in the final prediction. Next, applying a global average pooling to the computed gradients will then produce the weights that represent the significance of each feature map. Positive weighted regions are highlighted by element-wise multiplying the obtained weights to the corresponding feature maps and then applying a ReLU activation. This process highlights the discriminative regions that strongly influence the network's decision.

\subsubsection{Generating CF Explanation}
The saliency map along with the original image and the target label enables the generator to extract important features about the grey-box model and alter them to obtain the desired CF. 
% That is achieved by defining novel loss functions to let the SAcleGAN accept the new source of information. 
Fig.~\ref{fig: method0} illustrates the SAcleGAN training phase, where for a query  image $x$, the grey-box model $M(\cdot)$ provides a label $y$. Accessing the gradient signals of the model (hyperparameters), the Grad-CAM provides the saliency map $s$ of the real image. The Generator network~$G(\cdot)$ then transfers these images into the target domain/label $y'$, i.e., $G(x, s, y') = (x', s')$, where $x'$ and $s'$ denote the CF example (fake image) and its predicted saliency map, respectively. Since paired ground-truth images are not available, the SAcleGAN transfers the provided images back to the initial domain to evaluate the generator's performance, i.e., $G(x', s', y) = (x_{rec}, s_{rec})$ where $x_{rec}, s_{rec}$ are the reconstructed image and saliency map, respectively that are used to train the model. Meanwhile, the discriminator network~$D(\cdot)$ learns to predict image labels, denoted by $D_{cls}(\cdot)$, and distinguish real images from fake ones, showed by $D_{src}(\cdot)$. In Fig.~\ref{fig: method0} one can also find attention and content layers, following the architecture of AttentionGAN in~\cite{tang2021attentiongan}. The attention matrix indicates the pixels that need to be varied in one channel, while the content matrices provide the (target) RGB values of the indicated pixels. The 'Op' component in the figure implements the operation specified in the legend, resulting in the generation of the desired image.  


% In Figure \ref{fig: method0}, attention and content layers are depicted, inspired by AttentionGAN [48]. The attention matrix highlights the specific pixels in a single channel that require variation, whereas the content matrices provide the RGB values for the highlighted pixels. The "Op" component performs the necessary operations on these layers to produce the resulting image, denoted as $x'$.


%Using several loss functions, the generator and discriminator networks can be trained adversarially (see Algorithm~\ref{alg: alg}). To ensure that the provided saliency maps are fused into the generator network we have defined novel loss functions alongside the common one presented in the following section.

\subsection{Training phase}
\label{subsec: learning}
Let us assume that the real image $x$ is labeled as $y \in Y$ during training, and the generator provides a fake image (CF example) $x'$ that can be classified with the target label $y' \neq y$. The salience maps of the input $x$ and the CF $x'$ are $s$ and $s'$ respectively. The generator's objective is to deceive the discriminator $D$ by generating images that appear to be real. Conversely, the discriminator $D$ aims to enhance its ability to distinguish between generated samples and real data samples. This dynamic interplay encourages both the generator and discriminator to improve their respective performances. Thus, the adversarial loss function is designed in a way to encourage the aforementioned behaviours~\cite{huber2023ganterfactual} :
\begin{equation}
\label{eq: loss_adv}
\begin{gathered}
    \mathcal{L}^D_{{adv}} = \mathbb{E}_{x} \Bigr[D_{src}(x)\Bigr] - \mathbb{E}_{x'} \Bigr[D_{src}(x') \Bigr] \quad  + \\
    \lambda_{gp} \, \mathbb{E}_{\hat{x}}\Bigr[(\lVert \nabla_{\hat{x}} D_{src}(\hat{x}) \rVert_2-1 )^2\Bigr].
\end{gathered}
\end{equation}
\begin{equation}
\label{eq: x_hat}
    \hat{x} = \alpha  x + (1 - \alpha) x', 
\end{equation}
where $\alpha$ is a uniform random variable in $(0, 1)$, $||\cdot||_2$ the Euclidean norm of a vector, $\nabla_{\hat{x}}$ is the gradient of $\hat{x}$ and $\hat{x}$ is a linear combination of $x$ and $x'$. %$s$ is the attention mask of image $x$, enabling generator $G$ to only focus on the masks' pixels during the training phase.

While the first two terms in Eq.~\ref{eq: loss_adv} guide the discriminator to distinguish real images from fake ones, the gradient penalty loss term 
\begin{equation}
\mathcal{L}_{gp} = \mathbb{E}_{\hat{x}}\Bigr[(\lVert \nabla_{\hat{x}} D_{src}(\hat{x}) \rVert_2-1 )^2\Bigr]
\end{equation}
prevents the discriminator from being trapped near trivial solutions~\cite{gulrajani2017improved}. It  ensures that the discriminator's gradients are bounded for better convergence calculated by choosing a random point $\alpha$ along the direction connecting a real image sample and a generated one, see Eq.~\ref{eq: x_hat}, and then taking the  gradient w.r.t. that point. Intuitively, minimising the loss $\mathcal{L}_{gp}$ for a random point between real and fake images, ensures that the discriminator has smooth gradients along the data manifold, preventing it from ignoring or focusing too much on individual samples and improving the overall stability of the GAN training process.

To evaluate whether the CF examples can be classified as $y'$ by the grey-box model $M$, the discriminator should learn the mapping of images to corresponding labels. This can be achieved through the following loss function:
\begin{equation}
\begin{gathered}
    \mathcal{L}^y_{{cls}} = \mathbb{E}_{x,y} \Bigr[(-\log D_{cls}(y|x)\Bigr].
\end{gathered}
\end{equation}

A linear combination of the two loss functions is used to train the discriminator network:
\begin{equation}
\label{eq: L_d}
\begin{gathered}
    \mathcal{L}_D = -\mathcal{L}_{adv}^D + \lambda_{{cls}}\mathcal{L}_{{cls}}^y.
\end{gathered}
\end{equation}


To further encourage the generator, the following loss functions are utilised to generate real-like images. To achieve this objective, the generator is penalised if it fails to deceive the discriminator in differentiating real images from generated ones and mis-classifies generated images into the target labels. 
\begin{equation}
\begin{gathered}
    \mathcal{L}^G_{{adv}} = \mathbb{E}_{x'} \Bigr[D_{src}(x) \Bigr].\\
    \mathcal{L}^{y'}_{{cls}} = \mathbb{E}_{x',y'}\Bigr[-\text{log}D_{cls}(y'|x')\Bigr].
\end{gathered}
\end{equation}

Moreover, since the ground truth of CF examples is unavailable, following the SAcleGAN approach, we transfer back the generated CF example (fake image) into the initial domain and compare it with the query image. The following loss penalises the generator model for adding excessive artifacts to enforce forward and backward consistency:
\begin{equation}
\begin{gathered}
%\binom{x'}{s'} = G_{Y' \to Y}(x, s,y').\\
\mathcal{L}_{rec} = \mathbb{E}_{x,y,y'}\Bigr[ \lVert{ (x,s)^T -  G(G(x, s,y'),y)^T} \lVert_1 \Bigr], 
\end{gathered}    
\end{equation}
where the transpose operator $(\cdot,\cdot)^T$ turns the two matrices into a column vector. 

% The proposed GAN model is further trained with the cycle consistency loss~\cite{zhu2017unpaired} to enforce forward and backward consistency. The cycle loss can be formulated as follows:

% \begin{equation}
% \begin{gathered}
%     \mathcal{L}_{\text{cycle}} = \mathbb{E}_{y \sim p_{\text{data}(y)}} \| G_{Y \to Y'} ( G_{Y' \to Y}(x)) - x \|_1 + \\
%     \mathbb{E}_{y' \sim p_{\text{data}(y')}} \| G_{Y \to Y'} ( G_{Y' \to Y}(y')) - y' \|_1
% \end{gathered}
% \end{equation}

% In addition to the previous losses, two regularization terms have been added to avoid trivial solutions in attention masks and reduce changes between source and target domains with $\mathcal{L}_{\text{tv}}$ and $\mathcal{L}_{pixel}$ losses, respectively:

% \begin{equation}
% \begin{gathered}
%     \mathcal{L}_{pixel} = \mathbb{E}_{y \sim p_{\text{data}(y)}} \|  G_{Y \to Y'}(y) - y \|_1 + \\
%     \mathbb{E}_{y' \sim p_{\text{data}(y')}} \|  G_{Y' \to Y}(y') - y' \|_1\\
%     \mathcal{L}_{\text{tv}}(M_y) = \displaystyle\sum_{w,h=1}^{W,H} \big| M_y(w+1,h)-M_y(w,h) \big| + \\
%     \big| M_y(w,h+1)-M_y(w,h) \big|
% \end{gathered}
% \end{equation}



To incorporate the saliency maps into the learning phase, we propose a novel term, $\mathcal{L_\text{fuse}}$, into the loss function. This  term ensures that the saliency map information is fused into the generator model properly. Penalising the generator for modifying the  non-salient features, $(1-s)$, leads the model to perceive the salient region and alter only that region's pixels towards the generation of the CF example:
\begin{equation}
\label{eq:lfuse}
    \mathcal{L}_{fuse} = \mathbb{E}_{x,y,y'}\Bigr[ || (x-x')\odot (1-s) ||_1\Bigr], 
\end{equation}
where $\odot$ denotes point-wise multiplication of matrices.

Finally, a linear combination of the three introduced loss functions shapes the generator model's objective $\mathcal{L_\text{G}}$:
\begin{equation}
\label{eq: L_G}
    {\mathcal{L}_{G}} = -\mathcal{L}^G_{adv} + \lambda_{{cls}} \mathcal{L}_{{cls}}^{y'} + \lambda_{{gp}} \mathcal{L}_{{gp}} + \lambda_{{fuse}} \mathcal{L}_{{fuse}}.
\end{equation}

The pseudocode under Algorithm~\ref{alg: alg} summarises the SAFE approach  generating the CF examples.  


\RestyleAlgo{ruled}

%% This is needed if you want to add comments in
%% your algorithm with \Comment
\SetKwComment{Comment}{\textcolor{teal}{/* }}{ \textcolor{teal}{/* }}
\SetKwInput{kwReturn}{return}
% \vspace{1mm}
\begin{algorithm}[hbt!]
\caption{Pseudo-code of SAFE 
% toward generating CF $x'$ for a query image $x$, classfied as $y$ by the decision Model $M$. $G$ and $D$ represent Generator and discreminator models, $l_r$ stands for learning rate. Hyperparameters $\lambda_{clc}, \lambda_{gp},\lambda_{fuse},\lambda_{rec}$ balance the contribution between corresponding loss terms
}\label{alg: alg}
\Comment{~~~~~~~~\textcolor{teal}{Training Procedure}~~~}
\KwData{Input image $x$ labeled as $y$, target label $y'$}
\KwResult{CF instance $x'$}
$N \gets 0$\\ 
\For{$x$ in data}{
    ${l}_{class} \gets \mathcal{L}_{cls}(D_{cls}(x), M(x))$;\\
    ${l}_{real} \gets \mathcal{L}^D_{adv}(D_{src}(x))$;\\
    $s \gets \text{Grad-cam}(x, y, y')$;\\
    $x', s' \gets G(x, s, y')$;\\
    ${l}_{fake} \gets \mathcal{L}^D_{adv}(D_{src}(x'))$;\\
    $\alpha \gets rand(0,1)$;\\  
    $\hat{x} \gets (\alpha  x + (1 - \alpha)  x')$;\\
    $l_{gp} \gets \mathcal{L}_{gp}(D_{src}(\hat{x}))$;\\
    $l_{D} \gets  -l_{real} + l_{fake} + \lambda_{cls}  l_{class} + \lambda_{gp}  l_{gp}$;\\
    $D \gets ADAM(D, l_{D}, \mathit{learning\mbox{-}rate});$\\
    $N \gets N+1$;\\
    \If{$N~\text{is}~5$}{
        $x', s' \gets G(x, s, y')$;\\
        $l_{fake} \gets D_{src}(x')$;\\
        ${l}_{class} \gets \mathcal{L}_{class}(D_{cls}(x'), M(x'))$;\\
        $x_{rec}, s_{rec} \gets G(x', s', M(x))$;\\  
        $l_{rec} \gets \mathcal{L}_{rec}(x,s,x_{rec}, s_{rec})$;\\
        $l_{fuse} \gets \mathcal{L}_{fuse}(x, x', s)$;\\
        $l_{G} \gets -l_{fake} + \lambda_{cls} l_{class} + \lambda_{gp} l_{gp} + \lambda_{fuse} l_{fuse}$;\\      
        $G \gets ADAM(G,l_{G}, \mathit{learning\mbox{-}rate});$\\
        $N \gets 0$\\ 
    }
}
% \EndFor
\end{algorithm}