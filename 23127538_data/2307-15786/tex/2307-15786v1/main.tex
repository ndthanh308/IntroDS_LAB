\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}



\usepackage{float}
% \documentclass[lettersize,journal]{IEEEtran}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{soul}
\usepackage{algpseudocode}
\usepackage{algorithm2e}
\usepackage{graphics}
\usepackage{adjustbox}
\usepackage{hyperref}
\DeclareMathOperator{\mean}{mean}
\usepackage{cuted} 
\usepackage{tabularray}
\usepackage{caption}
% \usepackage{subcaption}


\usepackage{booktabs}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage{cite}
\usepackage{lipsum, babel}
\begin{document}
\bibliographystyle{IEEEtran}
\newcommand{\RNum}[1]{\lowecase\expandafter{\romannumeral #1\relax}}
\newcommand{\mybluecolor}[1]{{\color{blue}#1}}
\newcommand{\myredcolor}[1]{{\color{red}#1}}

\newcommand{\ASh}[1]{\textcolor{red}{(ASh: #1)}}
\newcommand{\sam}[1]{\textcolor{blue}{(sam: #1)}}

%Saliency-Aware Counterfactual Explanations for Deep Neural Networks
\title{SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems}
% Counterfactual Explanation for Black-Box Deep Reinforcement Learning Agent Based on Policy Distillation
\author{Amir Samadi, Amir Shirian, Konstantinos Koufos, Kurt Debattista and Mehrdad Dianati
\thanks{The authors are with the Warwick Manufacturing Group,
The University of Warwick, Coventry CV4 7AL. (e-mail:amir.samadi, amir.shirian, konstantinos.koufos, k.debattista and m.dianati@warwick.ac.uk). This research is sponsored by Centre for Doctoral Training to Advance the Deployment of Future Mobility Technologies
(CDT FMT) at the University of Warwick.}}


\maketitle
\thispagestyle{plain}
\pagestyle{plain}
\begin{abstract} 
% Generating counterfactual explanations for complex machine learning models is a challenging problem in the field of explainable AI. In this paper, we propose a novel approach for generating counterfactual explanations based on saliency maps. Our approach integrates saliency maps into the counterfactual generator to guide the generation of explanations based on the most important features of the input data. We evaluate our proposed approach on a real-world dataset and compare it against several baseline methods. Our experimental results show that our proposed approach outperforms the baselines in terms of both the quality of the generated counterfactuals and the interpretability of the explanations. We also conduct a qualitative analysis of our results to gain insights into the strengths and limitations of our approach. Our work contributes to the growing body of research on explainable AI and provides a promising direction for generating more interpretable and effective counterfactual explanations for machine learning models. Source codes of this are available at: \url{https://github.com/Amir-Samadi/Counterfactual-Explanation}. 

% Recent advancements in Deep Neural Networks (DNN) light a spotlight on the need for interpretability. Counterfactual explanation as an effective method in the realm of interpretability offers a range of changes that can be applied to the input results in changing the output of the model. In practice gathering these changes leads to depicting of models' decision boundaries. However recent generative counterfactual models alters the end-user selected features, rather than changing important features for the black-box model. Here using saliency maps, yeilded from most important features for the model, would contribute to generate CFs near to decision boundary, results in more accurate decision boundary estimation.        Despite recent advances in the field of explainable AI, generating effective counterfactual explanations remains a challenging task. One key limitation of existing approaches to generating counterfactual explanations is that they often fail to explicitly focus on the most important features of the input that have the greatest impact on the model's output. As a result, these methods may generate explanations that are not aligned with human intuition and provide limited insights into the model's decision-making process. To overcome this limitation, we propose a novel approach that integrates saliency maps into our counterfactual generator, which guides the generation of explanations based on the most important features of the input data. This allows us to provide more informative and intuitive explanations that are closely aligned with human intuition and provide a better understanding of the model's behavior. Our approach addresses this missing point in the literature and has significant implications for improving the transparency and interpretability of deep learning models. 

% The interpretability of Deep Neural Networks (DNNs) has gained significant importance in recent years due to the increasing complexity and prevalence of these models. Counterfactual explanations have emerged as an effective approach for interpreting black-box DNNs by identifying necessary changes to the input that would result in altering the model's output. These explanations offer the minimum modifications required to surpass the decision boundary of the model. However, current generative counterfactual models often modify user-selected features, rather than focusing on the crucial features of the black-box model. Consequently, the generated counterfactuals obtained through offline feature selection raise concerns as they may not necessarily be in proximity to boundary decisions, thereby compromising the integrity of the counterfactual definitions. To address this issue, we propose a novel approach that utilizes saliency maps to generate more accurate counterfactual explanations closer to the decision boundary. Our approach guides the generation of counterfactuals based on the most significant features for the black-box model, leading to more informative and interpretable explanations. We assess the effectiveness of our method using a real-world dataset and demonstrate its superiority over several baseline methods in terms of decision boundary estimation accuracy and counterfactual metrics, including validity and sparsity. Our work contributes to the ongoing endeavors to enhance the interpretability of DNNs and offers a promising direction for generating precise and informative counterfactual explanations. The source code for our approach can be accessed at: https://github.com/Amir-Samadi/Counterfactual-Explanation.


The explainability of Deep Neural Networks~(DNNs) has recently gained significant importance especially in safety-critical applications such as automated/autonomous vehicles, a.k.a. automated driving systems. CounterFactual~(CF) explanations have emerged as a promising approach for interpreting the behaviour of black-box DNNs. 
A CF explainer identifies the minimum modifications in the input that would alter the model's output to its complement. In other words, a CF explainer computes the minimum modifications required to cross the model's decision boundary. Current deep generative CF models often work with user-selected features rather than focusing on the discriminative features of the black-box model. Consequently, such CF examples may not necessarily lie near the decision boundary, thereby contradicting the definition of CFs. To address this issue, we propose in this paper a novel approach that leverages saliency maps to generate more informative CF explanations. % which lie closer to the decision boundary. 
Our approach guides a Generative Adversarial Network based on the most influential features of the input of the black-box model to produce CFs near the decision boundary. We evaluate the performance of this approach using a real-world dataset of driving scenes, BDD100k, and demonstrate its superiority over several baseline methods in terms of well-known CF metrics, including proximity, sparsity and validity. Our work contributes to the ongoing efforts to improve the interpretability of DNNs and provides a promising direction for generating more accurate and informative CF explanations\footnote{Source codes are available at: \url{https://github.com/Amir-Samadi//Saliency_Aware_CF}}.
\end{abstract}

% \begin{IEEEkeywords}
% Counterfactual explanation, saliency GAN, interpretable automated driving systems, interpretable DNN, attention GAN.
% \end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
\input{tex/intro}

\section{Background and Related work}
\label{section:related}
\input{tex/related}

\section{Saliency-Aware counterFctual Explainer}
\label{section:method}
\input{tex/method}

\section{Evaluations}
\label{section:results}
\input{tex/results}

% \section{Discussion}
% \label{section: discussion}
% \input{tex/discussion}

\section{Conclusions}
\label{section: conlusion}
\input{tex/conclusion}

%\section{Acknowledgment}
%This research is sponsored by Centre for Doctoral Training to Advance the Deployment of Future Mobility Technologies (CDT FMT) at the University of Warwick.
% \clearpage
\bibliography{ref}
\end{document}