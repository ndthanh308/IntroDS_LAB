\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{alemi2016deep}
Alexander~A Alemi, Ian Fischer, Joshua~V Dillon, and Kevin Murphy.
\newblock Deep variational information bottleneck.
\newblock {\em ICLR}, 2017.

\bibitem{an1996effects}
Guozhong An.
\newblock The effects of adding noise during backpropagation training on a
  generalization performance.
\newblock {\em Neural computation}, 8(3):643--674, 1996.

\bibitem{andreas2016neural}
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
\newblock Neural module networks.
\newblock In {\em CVPR}, 2016.

\bibitem{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock {\em Machine learning}, 79(1):151--175, 2010.

\bibitem{bishop1995training}
Chris~M Bishop.
\newblock Training with noise is equivalent to tikhonov regularization.
\newblock {\em Neural computation}, 7(1):108--116, 1995.

\bibitem{borji2019empirical}
Ali Borji and Seyed~Mehdi Iranmanesh.
\newblock Empirical upper bound in object detection and more.
\newblock {\em arXiv preprint arXiv:1911.12451}, 2019.

\bibitem{buda2018systematic}
Mateusz Buda, Atsuto Maki, and Maciej~A Mazurowski.
\newblock A systematic study of the class imbalance problem in convolutional
  neural networks.
\newblock {\em Neural networks}, 2018.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em ICML}, 2020.

\bibitem{chen2021dual}
Tung-I Chen, Yueh-Cheng Liu, Hung-Ting Su, Yu-Cheng Chang, Yu-Hsiang Lin,
  Jia-Fong Yeh, Wen-Chin Chen, and Winston Hsu.
\newblock Dual-awareness attention for few-shot object detection.
\newblock {\em IEEE TMM}, 2021.

\bibitem{chen2021exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In {\em CVPR}, 2021.

\bibitem{chu2020feature}
Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling.
\newblock Feature space augmentation for long-tailed data.
\newblock In {\em ECCV}, 2020.

\bibitem{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock In {\em CVPR}, 2019.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em CVPRW}, 2020.

\bibitem{cui2019class}
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
\newblock Class-balanced loss based on effective number of samples.
\newblock In {\em CVPR}, 2019.

\bibitem{dai2022enabling}
Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung.
\newblock Enabling multimodal generation on clip via vision-language knowledge
  distillation.
\newblock {\em ACL}, 2022.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, 2009.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em ACL}, 2018.

\bibitem{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2021.

\bibitem{everingham2010pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew
  Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em IJCV}, 2010.

\bibitem{frans2021clipdraw}
Kevin Frans, Lisa~B Soros, and Olaf Witkowski.
\newblock Clipdraw: Exploring text-to-drawing synthesis through language-image
  encoders.
\newblock {\em arXiv preprint arXiv:2106.14843}, 2021.

\bibitem{gal2022image}
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit~H Bermano, Gal
  Chechik, and Daniel Cohen-Or.
\newblock An image is worth one word: Personalizing text-to-image generation
  using textual inversion.
\newblock {\em ICLR}, 2023.

\bibitem{gal2022stylegan}
Rinon Gal, Or Patashnik, Haggai Maron, Amit~H Bermano, Gal Chechik, and Daniel
  Cohen-Or.
\newblock Stylegan-nada: Clip-guided domain adaptation of image generators.
\newblock {\em ACM TOG}, 2022.

\bibitem{gulcehre2016noisy}
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio.
\newblock Noisy activation functions.
\newblock In {\em ICML}, pages 3059--3068. PMLR, 2016.

\bibitem{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em ICML}, 2017.

\bibitem{gupta2023visual}
Tanmay Gupta and Aniruddha Kembhavi.
\newblock Visual programming: Compositional visual reasoning without training.
\newblock In {\em CVPR}, 2023.

\bibitem{happel1994design}
Bart~LM Happel and Jacob~MJ Murre.
\newblock Design and evolution of modular neural network architectures.
\newblock {\em Neural networks}, 7(6-7):985--1004, 1994.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{holmstrom1992using}
Lasse Holmstrom and Petri Koistinen.
\newblock Using additive noise in back-propagation training.
\newblock {\em IEEE transactions on neural networks}, 3(1):24--38, 1992.

\bibitem{hu2021dense}
Hanzhe Hu, Shuai Bai, Aoxue Li, Jinshi Cui, and Liwei Wang.
\newblock Dense relation distillation with context-aware aggregation for
  few-shot object detection.
\newblock In {\em CVPR}, 2021.

\bibitem{jain2022zero}
Ajay Jain, Ben Mildenhall, Jonathan~T Barron, Pieter Abbeel, and Ben Poole.
\newblock Zero-shot text-guided object generation with dream fields.
\newblock In {\em CVPR}, 2022.

\bibitem{jia2021scaling}
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em ICML}, 2021.

\bibitem{kang2019decoupling}
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi
  Feng, and Yannis Kalantidis.
\newblock Decoupling representation and classifier for long-tailed recognition.
\newblock In {\em ICLR}, 2020.

\bibitem{kim2022diffusionclip}
Gwanghyun Kim, Taesung Kwon, and Jong~Chul Ye.
\newblock Diffusionclip: Text-guided diffusion models for robust image
  manipulation.
\newblock In {\em CVPR}, 2022.

\bibitem{kim2021distilling}
Youmin Kim, Jinbae Park, YounHo Jang, Muhammad Ali, Tae-Hyun Oh, and Sung-Ho
  Bae.
\newblock Distilling global and local logits with densely connected relations.
\newblock In {\em ICCV}, 2021.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{kumar2021rma}
Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik.
\newblock Rma: Rapid motor adaptation for legged robots.
\newblock In {\em Robotics: Science and Systems}, 2021.

\bibitem{kwon2022clipstyler}
Gihyun Kwon and Jong~Chul Ye.
\newblock Clipstyler: Image style transfer with a single text condition.
\newblock In {\em CVPR}, 2022.

\bibitem{le2015tiny}
Ya Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock {\em CS 231N}, 2015.

\bibitem{li2021feature}
Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, and Kilian~Q Weinberger.
\newblock On feature normalization and data augmentation.
\newblock In {\em CVPR}, 2021.

\bibitem{li2021beyond}
Bohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, and Qixiang Ye.
\newblock Beyond max-margin: Class margin equilibrium for few-shot object
  detection.
\newblock In {\em CVPR}, 2021.

\bibitem{li2022grounded}
Liunian~Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
  Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et~al.
\newblock Grounded language-image pre-training.
\newblock In {\em CVPR}, 2022.

\bibitem{li2021simple}
Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy~M Hospedales.
\newblock A simple feature augmentation for domain generalization.
\newblock In {\em ICCV}, 2021.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}, 2014.

\bibitem{liu2019large}
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella~X
  Yu.
\newblock Large-scale long-tailed recognition in an open world.
\newblock In {\em CVPR}, 2019.

\bibitem{michel2022text2mesh}
Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka.
\newblock Text2mesh: Text-driven neural stylization for meshes.
\newblock In {\em CVPR}, 2022.

\bibitem{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock In {\em ICLR}, 2013.

\bibitem{nichol2021glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
  Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock In {\em ICML}, 2022.

\bibitem{oh2019speech2face}
Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William~T Freeman, Michael
  Rubinstein, and Wojciech Matusik.
\newblock Speech2face: Learning the face behind a voice.
\newblock In {\em CVPR}, 2019.

\bibitem{patashnik2021styleclip}
Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski.
\newblock Styleclip: Text-driven manipulation of stylegan imagery.
\newblock In {\em ICCV}, 2021.

\bibitem{Perarnau2016icgan}
Guim Perarnau, Joost van~de Weijer, Bogdan Raducanu, and Jose~M. Álvarez.
\newblock Invertible conditional gans for image editing.
\newblock In {\em CVPR}, 2016.

\bibitem{qiao2021defrcn}
Limeng Qiao, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, Jianan Wu, and Chi Zhang.
\newblock Defrcn: Decoupled faster r-cnn for few-shot object detection.
\newblock In {\em ICCV}, 2021.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em ICML}, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In {\em ICML}, 2021.

\bibitem{reed1999neural}
Russell Reed and Robert~J MarksII.
\newblock {\em Neural smithing: supervised learning in feedforward artificial
  neural networks}.
\newblock Mit Press, 1999.

\bibitem{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em ICML}, 2018.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In {\em NeurIPS}, 2015.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em CVPR}, 2022.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em IJCV}, 2015.

\bibitem{sammani2022nlx}
Fawaz Sammani, Tanmoy Mukherjee, and Nikos Deligiannis.
\newblock Nlx-gpt: A model for natural language explanations in vision and
  vision-language tasks.
\newblock In {\em CVPR}, 2022.

\bibitem{shin2022namedmask}
Gyungin Shin, Weidi Xie, and Samuel Albanie.
\newblock Namedmask: Distilling segmenters from complementary foundation
  models.
\newblock {\em arXiv preprint arXiv:2209.11228}, 2022.

\bibitem{simard1998transformation}
Patrice~Y Simard, Yann~A LeCun, John~S Denker, and Bernard Victorri.
\newblock Transformation invariance in pattern recognition—tangent distance
  and tangent propagation.
\newblock In {\em Neural networks: tricks of the trade}, pages 239--274.
  Springer, 1998.

\bibitem{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang,
  Colin~A Raffel, Ekin~Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In {\em NeurIPS}, 2020.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 2014.

\bibitem{sun2021fsce}
Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, and Chi Zhang.
\newblock Fsce: Few-shot object detection via contrastive proposal encoding.
\newblock In {\em CVPR}, 2021.

\bibitem{sung2023sound}
Kim Sung-Bin, Arda Senocak, Hyunwoo Ha, Andrew Owens, and Tae-Hyun Oh.
\newblock Sound to visual scene generation by audio-to-visual latent alignment.
\newblock In {\em CVPR}, 2023.

\bibitem{szegedy2014intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In {\em ICLR}, 2014.

\bibitem{tian2022vl}
Changyao Tian, Wenhai Wang, Xizhou Zhu, Jifeng Dai, and Yu Qiao.
\newblock Vl-ltr: Learning class-wise visual-linguistic representation for
  long-tailed visual recognition.
\newblock In {\em ECCV}, 2022.

\bibitem{ulyanov2018deep}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Deep image prior.
\newblock In {\em CVPR}, 2018.

\bibitem{van2008visualizing}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em JMLR}, 9(11), 2008.

\bibitem{verma2019manifold}
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,
  David Lopez-Paz, and Yoshua Bengio.
\newblock Manifold mixup: Better representations by interpolating hidden
  states.
\newblock In {\em ICML}, 2019.

\bibitem{vincent2010stacked}
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine
  Manzagol, and L{\'e}on Bottou.
\newblock Stacked denoising autoencoders: Learning useful representations in a
  deep network with a local denoising criterion.
\newblock {\em Journal of machine learning research}, 11(12), 2010.

\bibitem{wang2020frustratingly}
Xin Wang, Thomas~E Huang, Trevor Darrell, Joseph~E Gonzalez, and Fisher Yu.
\newblock Frustratingly simple few-shot object detection.
\newblock In {\em ICML}, 2020.

\bibitem{wang2022multimodal}
Zhecan Wang, Noel Codella, Yen-Chun Chen, Luowei Zhou, Xiyang Dai, Bin Xiao,
  Jianwei Yang, Haoxuan You, Kai-Wei Chang, Shih-fu Chang, et~al.
\newblock Multimodal adaptive distillation for leveraging unimodal encoders for
  vision-language tasks.
\newblock {\em arXiv preprint arXiv:2204.10496}, 2022.

\bibitem{wortsman2022robust}
Mitchell Wortsman, Gabriel Ilharco, Jong~Wook Kim, Mike Li, Simon Kornblith,
  Rebecca Roelofs, Raphael~Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi,
  Hongseok Namkoong, et~al.
\newblock Robust fine-tuning of zero-shot models.
\newblock In {\em CVPR}, 2022.

\bibitem{wu2020tod}
Chien-Sheng Wu, Steven Hoi, Richard Socher, and Caiming Xiong.
\newblock Tod-bert: Pre-trained natural language understanding for
  task-oriented dialogue.
\newblock {\em EMNLP}, 2020.

\bibitem{xiao2020few}
Yang Xiao and Renaud Marlet.
\newblock Few-shot object detection and viewpoint estimation for objects in the
  wild.
\newblock In {\em ECCV}, 2020.

\bibitem{yan2019meta}
Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xiaodan Liang, and Liang Lin.
\newblock Meta r-cnn: Towards general solver for instance-level low-shot
  learning.
\newblock In {\em ICCV}, 2019.

\bibitem{yang2022survey}
Lu Yang, He Jiang, Qing Song, and Jun Guo.
\newblock A survey on long-tailed visual recognition.
\newblock {\em IJCV}, 2022.

\bibitem{ye2023eninst}
Moon Ye-Bin, Dongmin Choi, Yongjin Kwon, Junsik Kim, and Tae-Hyun Oh.
\newblock Eninst: Enhancing weakly-supervised low-shot instance segmentation.
\newblock {\em arXiv preprint arXiv:2302.09765}, 2023.

\bibitem{youwang2022clip}
Kim Youwang, Kim Ji-Yeon, and Tae-Hyun Oh.
\newblock Clip-actor: Text-driven recommendation and stylization for animating
  human meshes.
\newblock {\em ECCV}, 2022.

\bibitem{yuan2021florence}
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
  Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu,
  Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen
  Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang.
\newblock Florence: A new foundation model for computer vision.
\newblock {\em arXiv preprint arXiv:2111.11432}, 2021.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em ICCV}, 2019.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em ICLR}, 2018.

\bibitem{zhang2023diagnosing}
Yuhui Zhang, Jeff~Z HaoChen, Shih-Cheng Huang, Kuan-Chieh Wang, James Zou, and
  Serena Yeung.
\newblock Diagnosing and rectifying vision models using language.
\newblock In {\em ICLR}, 2023.

\bibitem{zhu2020domain}
Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou.
\newblock In-domain gan inversion for real image editing.
\newblock In {\em ECCV}, 2020.

\end{thebibliography}
