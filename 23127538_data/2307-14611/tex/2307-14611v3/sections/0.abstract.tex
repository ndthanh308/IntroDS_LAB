We propose $\TextMani$, a text-driven manifold augmentation method that semantically enriches visual feature spaces, regardless of class distribution.
$\TextMani$ augments visual data with intra-class semantic perturbation by exploiting easy-to-understand visually mimetic words, i.e., attributes.
This work is built on an interesting hypothesis that general language models, \eg, BERT and GPT, encompass visual information to some extent, even without training on visual training data.
Given the hypothesis, $\TextMani$ transfers pre-trained text representation obtained from a well-established large language encoder to a target visual feature space being learned.
Our extensive analysis hints that the language encoder indeed encompasses visual information at least useful to augment visual representation.
Our experiments demonstrate that $\TextMani$ is particularly powerful in scarce samples with class imbalance as well as even distribution.
We also show compatibility with the label mix-based approaches in evenly distributed scarce data.
