% In this section, we demonstrate the effectiveness of $\TextMani$ in scarce data regimes.
We evaluate $\TextMani$ in various cases presenting sparse data with 
% two
different tasks: long-tail classification in \Sref{sec4.1}, evenly distributed scarce data classification in \Sref{sec4.2}, and few-shot object detection in \Sref{sec4.3}.
We also conduct additional studies demonstrating the effectiveness of the design of our method and the versatility of $\TextMani$
% and ablation study on attributes
% regarding the sampling distribution and the minimum of the
% number of attribute samples for feature manipulation and
% mixing weight $\alpha$ range 
in \Sref{sec4.4}.
Additional experimental results and details can be found in the supplementary material.



\begin{table}
\centering
\resizebox{1.0\linewidth}{!}{
    \footnotesize
    \begin{tabular}{@{}l C{16mm}C{16mm}C{16mm}@{}}
        \toprule
        \multirow{2}[2]{*}{\textbf{(a) Augmentation}} & \multicolumn{3}{c}{\textbf{Imbalance Factor (IF)}} \\ 
        \cmidrule{2-4}
        & \textbf{100} & \textbf{50} & \textbf{10} \\ 
        \midrule
        Baseline           & 38.39 & 43.33 & 59.29 \\
        $\TextMani$ (CLIP) & 40.65 (\blue{+2.26}) & 46.48 (\blue{+3.15}) & 60.17 (\blue{+0.88}) \\ 
        $\TextMani$ (BERT) & 41.10 (\blue{+2.71}) & \textbf{47.17 (\blue{+3.84})} & 60.67 (\blue{+1.38}) \\
        $\TextMani$ (GPT-2) & \textbf{41.20 (\blue{+2.81})} & 46.93 (\blue{+3.60}) & 60.94 (\blue{+1.65}) \\
        \midrule
        Cutout~\cite{devries2017improved}      & 37.51 & 42.28 & 59.26 \\
         + $\TextMani$    & 40.35 (\blue{+2.84}) & 45.48 (\blue{+3.20}) & \textbf{61.31 (\blue{+2.05})} \\
        \cmidrule{1-4}
        Cutmix~\cite{yun2019cutmix}      & 37.93 & 43.34 & 59.30\\
         + $\TextMani$    & 40.22 (\blue{+2.29}) & 45.36 (\blue{+2.02}) & 61.30 (\blue{+2.00})\\
        \cmidrule{1-4}
        Mixup~\cite{zhang2017mixup}       & 36.75 & 40.77 & 57.50 \\
         + $\TextMani$     & 38.40 (\blue{+1.65}) & 43.33 (\blue{+2.56}) & 59.80 (\blue{+2.30})\\
        \cmidrule{1-4}
        ManiMixup~\cite{verma2019manifold}   & 35.72 & 40.51 & 55.26 \\ 
         + $\TextMani$ & 38.60 (\blue{+2.88}) & 43.22 (\blue{+2.71}) & 59.35 (\blue{+4.09})\\
        \midrule
        \midrule
        \multirow{2}[2]{*}{\textbf{(b) Augmentation}} & \multicolumn{3}{c}{\textbf{Set of Classes (IF=100)}} \\
        \cmidrule{2-4}
        & \textbf{Many} & \textbf{Medium} & \textbf{Few}  \\ 
        \midrule
        Baseline           & 71.11 & 38.42 & 3.00 \\
        $\TextMani$ (CLIP) & 71.14 (\blue{+0.03}) & 40.28 (\blue{+1.86}) & 7.53 (\blue{+4.53}) \\
        $\TextMani$ (BERT) & 70.22 (\orange{-0.89}) & 40.73 (\blue{+2.31}) & 9.41 (\blue{+6.41}) \\
        $\TextMani$ (GPT-2) & 70.60 (\orange{-0.51}) & 40.61 (\blue{+2.19}) & \textbf{9.93 (\blue{+6.93})} \\
        \midrule
        Cutout               & 71.54 & 35.94 & 1.06 \\
        + $\TextMani$ & 71.94 (\blue{+0.83}) & \textbf{40.97 (\blue{+2.55})} & 4.03 (\blue{+3.03}) \\
        \cmidrule{1-4}
        Cutmix                & 72.02 & 37.17 & 0.90 \\
         + $\TextMani$  & 72.37 (\blue{+0.35}) & 40.80 (\blue{+3.63}) & 3.90 (\blue{+3.00}) \\
        \cmidrule{1-4}
        Mixup               & 71.97 & 33.62 & 0.36 \\
         + $\TextMani$ & 71.97 (+0.00) & 36.77 (\blue{+3.15}) & 1.83 (\blue{+1.47})\\
        \cmidrule{1-4}
        ManiMixup               & 72.97 & 29.51 & 0.70 \\
         + $\TextMani$ & \textbf{73.20 (\blue{+0.23})} & 36.80 (\blue{+7.29})& 0.76 (\blue{+0.06})\\
        \bottomrule
    \end{tabular}
    }
    \caption{Long-tail classification results (\%) on CIFAR-100-LT with ResNet18.
    (a) The accuracy with respect to the different imbalance factors, \ie, IF=$\{100, 50, 10\}$.
    (b) The accuracy of each class set with IF=$100$.
    Baseline contains random horizontal flip, random crop and rotation, and normalization, applied in all experiments.
    $\TextMani$ without parenthesis uses CLIP for the text encoder.
    }
    \label{tab:CIFAR-100-LT}
    \vspace{4mm}
% \end{table}
% \begin{table}
    \centering
    \resizebox{1.0\linewidth}{!}{\scriptsize
    \begin{tabular}{@{\,\,}l@{\quad}C{5mm}C{9mm}C{9mm}C{9mm}c@{\,\,}}
         \toprule
         \textbf{Aug.} & \textbf{CBS} & \textbf{All} & \textbf{Many} & \textbf{Medium} & \textbf{Few}\\
         \midrule
         Baseline  &            & 38.39 & 71.11 & 38.42 & 3.00 \\
         Cutmix    & \checkmark & 38.23 & \textbf{71.77} & 37.79 & 1.90 \\
         Mixup     & \checkmark & 38.73 & 71.60 & 37.64 & 3.16 \\ 
         ManiMixup & \checkmark & 38.56 & 71.25 & 37.88 & 2.80 \\
         \TextMani & & \textbf{40.65} & 71.14 & \textbf{40.28} & \textbf{7.53} \\
         \bottomrule
    \end{tabular}
    }
    % \vspace{-3mm}
    \caption{Comparison 
    % of Top-1 accuracy (\%) 
    to label mix-based augmentations with class-balanced sampling (CBS) on CIFAR-100-LT with IF=100. 
    % with ResNet18.
    CBS samples two classes first and then samples data in each classes.
    }
    \label{tab:balanced_sampling}
    % \vspace{-3mm}
\end{table}


% textmani for all(Many,mid,few) groups, 30 sampling
% \begin{table}
% \centering
% \resizebox{1.0\linewidth}{!}{
%     \footnotesize
%     \begin{tabular}{@{}ll C{11mm}C{11mm}C{11mm}C{11mm}@{}}
%         \toprule
%         \multirow{2}[2]{*}{\textbf{Method}} &\multirow{2}[2]{*}{\textbf{Aug.}} & \multicolumn{3}{c}{\textbf{Set of Classes}} & \multirow{2}[2]{*}{\textbf{All}} \\ 
%         \cmidrule{3-5}
%         && \textbf{Many} & \textbf{Medium} & \textbf{Few} & \\ 
%         \midrule
%         \multirow{2}{*}{LWS~\cite{kang2019decoupling}}
%           & Baseline    & \textbf{67.65} & 37.52 & 6.03 & 44.84\\
%           & $\TextMani$ & 67.00 & \textbf{39.32} & \textbf{9.49} & \textbf{45.92}\\
%         \midrule 
%         \multirow{2}{*}{cRT~\cite{kang2019decoupling}}
%           & Baseline    & \textbf{67.58} & 38.74 & 9.30 & 45.84 \\ 
%           & $\TextMani$ & 66.91 & \textbf{40.86} & \textbf{13.01} & \textbf{47.10} \\
%           % & $\TextMani$ & 63.61 & \textbf{47.30} & \textbf{24.71} & \textbf{50.50} \\
%         \bottomrule
%     \end{tabular}
% }
% \caption{Long-tail classification accuracy (\%) on ImageNet-LT with ResNext50.
% % LWS represents weight scale learning method, and cRT is a decoupled classifier learning method.
% Baseline stands for the basic augmentation with random horizontal flip and random resized crop.
% % The results show the impact of each augmentation in terms of class sets.
% % \textbf{Bold} indicates the best results in each class set.
% % ResNet-18  
% The models are trained with the batch size of 128.
% }
% \label{tab:ImageNet-LT}
% \end{table}




\subsection{Long-tail Classification}\label{sec4.1}
\paragraph{Experimental Setting}
We compare $\TextMani$ with the mix-based augmentations on the CIFAR-100-LT~\cite{cui2019class} and ImageNet-LT~\cite{liu2019large} datasets, where LT stands for long-tailed distribution.
% The long-tail datasets
They are artificially truncated to have a long-tail from each original dataset, CIFAR-100~\cite{krizhevsky2009learning} and ImageNet-2012~\cite{deng2009imagenet}.
Long-tail datasets usually have three sets of classes: Many-shot (more than 100 images), Medium-shot (20-100 images), and Few-shot (less than 20 images).

For CIFAR-100-LT, we 
% can
control the imbalance factor (IF) \cite{chu2020feature} computed as the ratio of samples in the head to tail class, $N_1/N_K$, where $N_k=\left| \mathcal{D}_k\right|$, and $\mathcal{D}_k$ is the set of samples belonging to the class $k\in\{1,\cdots,K\}$.
% $IF = \max(\{N_i\})/\min(\{N_i\})$, where $N_i$ is the number of training samples of the $i$-th class.
A larger value of the IF represents a more severe imbalance in data, which is more challenging.
We evaluate the performance according to different IFs of 100, 50, and 10.
% We use 100, 50, and 10 for the IF, confirming the performance along the IF value.

We utilize ResNet18 as the baseline on CIFAR-100-LT and ResNext50 on ImageNet-LT.
We use the validation set of the original datasets to measure the Top-1 accuracy.
Note that we apply each augmentation on all the samples without carefully selecting a set of classes in \Tref{tab:CIFAR-100-LT}.




\paragraph{Results}
\moon{\Tref{tab:CIFAR-100-LT} presents the long-tail classification results on CIFAR-100-LT,
% of $\TextMani$, mix-based augmentations, and their combination with $\TextMani$
% are in \Tref{tab:CIFAR-100-LT}.
which show consistent improvement with
% when additionally applying
$\TextMani$.
Also, $\TextMani$ with various text encoders achieves analogous improvement trend regardless of the imbalance factor but marginal degradation on Many class of IF=100 when using general language model, BERT and GPT-2.
While the performance gain is from leaking pre-trained language information, it is surprising and a virtue that the language models never exposed to any image can improve the visual recognition performance.
% It implies that while extracted information from the general language model is helpful for Medium or Few classes due to their scarcity, there is 
% In \Tref{tab:CIFAR-100-LT} for CIFAR-100-LT, we compare the effect of $\TextMani$, mix-based augmentations, and their combination with $\TextMani$.
% Our method achieves the largest improvement among the single usage of augmentation methods in all the imbalance factor conditions regardless of the text encoder type.
% The results show that $\TextMani$ is more effective than other mix-based augmentation when the data distribution has a long-tail, 
In comparison to single usage of mix-based augmentations, our method shows higher accuracy
because of uniform effects of $\TextMani$ on samples regardless of class imbalance.
The mix-based methods, on the other hand, sample two data points from the total dataset, where the probability that a tail class sample contributes to a resulting augmented sample is very low. 
Even with class-balanced sampling on mixed-based augmentation in \Tref{tab:balanced_sampling}, $\TextMani$ performs better, further demonstrating our effectiveness.
}
% The result of \Tref{tab:balanced_sampling} shows that our method performs better even with advantage on mix-based augmentation, and verifies our effectiveness.

\begin{table}
\centering
\resizebox{1.0\linewidth}{!}{
    \footnotesize
    \begin{tabular}{@{}l C{11mm}C{11mm}C{11mm}C{11mm}@{}}
        \toprule
        \multirow{2}[2]{*}{\textbf{Augmentation}} & \multicolumn{3}{c}{\textbf{Set of Classes}} &\multirow{2}[2]{*}{\textbf{Total}}\\
        % & \multirow{2}[2]{*}{\textbf{All}} \\ 
        \cmidrule{2-4}
        & \textbf{Many} & \textbf{Medium} & \textbf{Few}  \\ 
        \midrule
        Baseline & 85.34 & 70.47 & 42.80 & 72.24 \\
        $\TextMani$ & \textcolor{CarnationPink}{85.40} & 71.75 & \textcolor{CarnationPink}{48.49}& \textbf{\textcolor{RedViolet}{73.68}}\\ 
        \cmidrule{1-5}
        Cutout~\cite{devries2017improved} & 85.02 & 70.32 & 42.91 & 72.07 \\ 
         + $\TextMani$ & 85.33 & 71.70 & \textbf{\textcolor{RedViolet}{48.54}}& \textcolor{CarnationPink}{73.65}\\
         \cmidrule{1-5}
        Cutmix~\cite{yun2019cutmix} & 84.85 & 69.90 & 35.82 & 70.77 \\
         + $\TextMani$  & 85.30 & \textcolor{CarnationPink}{71.93} & 47.04 & 73.52 \\
        \cmidrule{1-5}
        Mixup~\cite{zhang2017mixup} & 84.96 & 70.27 & 40.20 & 71.63 \\
         + $\TextMani$ & 84.55 & 69.95 & 35.72 & 70.66 \\
        \cmidrule{1-5}
        ManiMixup~\cite{verma2019manifold} & 84.84 & 69.97 & 38.83 & 71.24 \\
         + $\TextMani$ & \textbf{\textcolor{RedViolet}{85.42}} & \textbf{\textcolor{RedViolet}{71.98}} & 46.65 & 73.54 \\
        \bottomrule
    \end{tabular}
}
\caption{
Long-tail classification results (\%) on ImageNet-LT with ViT, and color the value as \textbf{\textcolor{RedViolet}{best}} and \textcolor{CarnationPink}{second best}.
Baseline contains random horizontal flip, random resize crop, color jitter, and normalization, applied in all experiments.
% \textbf{Bold} indicates the best results in each set of classes, and the value in the parentheses for the improvement or degradation compared to the Baseline.
}
\label{tab:ImageNet-LT_vit}
\end{table}

\begin{table}
    \centering
    \resizebox{1.0\linewidth}{!}{\scriptsize
    \begin{tabular}{@{\,}l@{\quad\quad}C{9mm}C{9mm}C{9mm}C{9mm}@{\,}}
         \toprule
         \textbf{Method} & \textbf{Many} & \textbf{Medium} & \textbf{Few} & \textbf{All}\\
         \midrule
         LWS~\cite{kang2019decoupling} & \textbf{\textcolor{RedViolet}{63.34}} & \textcolor{CarnationPink}{48.08} & \textcolor{CarnationPink}{27.19}& \textcolor{CarnationPink}{51.14}  \\
         cRT~\cite{kang2019decoupling} & 61.80 & 46.20 & 27.40 & 49.60 \\
         cRT+\TextMani & \textcolor{CarnationPink}{62.74} & \textbf{\textcolor{RedViolet}{48.60}} & \textbf{\textcolor{RedViolet}{29.67}} & \textbf{\textcolor{RedViolet}{51.47}} \\
         \bottomrule
    \end{tabular}
    }
    % \vspace{-3mm}
    \caption{Long-tail classification results (\%) on ImageNet-LT with ResNext50. 
    We compare with LWS, cRT, and $\TextMani$ on cRT, and color the value as \textbf{\textcolor{RedViolet}{best}} and \textcolor{CarnationPink}{second best}.
    % The highest accuracy is colored with \textcolor{RedViolet}{purple}, the second one is \textcolor{CarnationPink}{pink}, and the third one is \textcolor{gray}{gray}.
    % The models are trained with a batch size of 512.
    }\vspace{3mm}
    \label{tab:ImageNetLT_512}
\end{table}


Particularly in \Tref{tab:CIFAR-100-LT}(b), the mix-based methods have degraded performance in the Medium and Few-shot classes, while our $\TextMani$ improves performance.
% Although $\TextMani$ with BERT and GPT-2 have marginally degraded accuracy on the Many class, increments on other classes are larger, especially in the Few-shot class.
Combining the mix-based methods with $\TextMani$ improves overall performance, but the tendency to sacrifice the Medium and Few-shot classes is the same as before combining.
Additionally, while Cutout has performance degradation due to the information loss~\cite{yun2019cutmix}, it is not affected by skewness due to no mix between inter-classes; thus, the performance is higher than the mix-based one in the long-tailed distribution.


We also evaluate our $\TextMani$ on the large-scale dataset ImageNet-LT. 
In \Tref{tab:ImageNet-LT_vit}, the best and second best results are with $\TextMani$, which demonstrate that our augmentation method is also effective in the large-scale long-tailed data distribution, consistent with the CIFAR-100-LT results in \Tref{tab:CIFAR-100-LT}.
The improvement with $\TextMani$ implies the importance of intra-class perturbation, which can uniformly affect the samples regardless of the skewness of the class distribution.
% While the probability of getting a tail class sample is getting lower when using the mix-based methods, $\TextMani$ uniformly affects the samples regardless of the skewness of the class distribution.

In \Tref{tab:ImageNetLT_512}, we compare with LWS~\cite{kang2019decoupling}, cRT~\cite{kang2019decoupling}, and $\TextMani$ on cRT. 
LWS and cRT are one of effective methods in recent long-tailed recognition.
% This is the same experiment with Table~\textcolor{blue}{3} in the main paper but with a different batch size of 512.
The result shows that $\TextMani$ on cRT achieves the best results compared to the counterparts in all classes except for the Many class, wherefrom ours achieves second best.
% This is consistent with the result in Table~\textcolor{blue}{3} in the main paper regardless of different batch sizes.
Overall, $\TextMani$ improves well-established works, \eg, LWS, and cRT, and it demonstrates the compatibility of our method.
% The overall results show that our $\TextMani$ improves 
% % is capable of boosting
% well-established works, \eg, LWS, and cRT.


% \Tref{tab:ImageNet-LT} shows the results on ImageNet-LT. 
% We adopt two variants with top accuracy, Learnable Weight Scaling (LWS) and classifier Re-Training (cRT), from a decoupling methodology~\cite{kang2019decoupling} as the baselines.\footnote{The baseline results are reproduced with the authors' code, and the reproduced value would differ from the one in the original paper due to different experimental environments, \eg, the number of GPUs and batch size.}
% % Decoupled representation trained with
% % LWS learns the scaling factor for classifier weights, and representations and classifier weights are fixed with the same weights in both experiments for a fair comparison.
% Compared to the LWS baseline, $\TextMani$ achieves gain in performance, which indicates that $\TextMani$ is a scalable method not only effective in neural network training with skewed class distribution but also in scaling factor learning.
% % cRT fine-tunes the classifier with class-balanced sampling. 
% \moon{Compared to the cRT baseline, $\TextMani$ also shows an improvement.
% The overall results show that our $\TextMani$ is capable of boosting well-established works, and is also very flexible yet favorable since it can be attached to any rich representations along with a noticeable gain in performance.
% }
% The cRT baseline scores were reproduced following the best configuration and the one with $\TextMani$ shows improvement. 
% The overall results show that our method is very flexible yet favorable 
% % accurate
% since it can be attached to any rich representations along with a noticeable
% % significant
% gain in performance.
% The results show that $\TextMani$ is capable of boosting well-established work.







\begin{table}
\centering
\resizebox{1.0\linewidth}{!}{
\footnotesize
        \begin{tabular}{@{\,}l C{16mm}C{16mm} @{\,}} 
        \toprule
        % \multirow{2}[2]{*}{\textbf{Model}} & 
        \multirow{2}[2]{*}{\textbf{Augmentation}} & \multicolumn{2}{c}{\textbf{Acc.}} \\
        \cmidrule{2-3}
        &  \textbf{Top-1} & \textbf{Top-5}\\
        \midrule
        % \multirow{8}{*}{ResNet18}
          Baseline    & 31.10 & 59.14 \\
          Cutout      & 32.03 & 60.53 \\
          Cutmix      & 32.43 & 61.04 \\
          Mixup       & 32.72 & 62.47 \\
          ManiMixup   & 33.74 & 63.29 \\
        %   $\TextMani$ 30sample & 32.89 & 60.77 \\
        %   $\TextMani$ maxsample & \textbf{34.52} & \textbf{65.74} \\
          $\TextMani$ & \textbf{34.52 (\blue{+3.42})} & \textbf{65.74 (\blue{+6.60})} \\
          \midrule
        %  \cmidrule{2-4}
          Cutout + $\TextMani$    & 33.91 (\blue{+2.81}) & 61.58 (\blue{+2.44}) \\
          Cutmix + $\TextMani$    & 35.61 (\blue{+4.51}) & 63.82 (\blue{+4.68}) \\
          Mixup + $\TextMani$     & 37.97 (\blue{+6.87}) & 66.75 (\blue{+7.61}) \\
          ManiMixup + $\TextMani$ & \textbf{38.02 (\blue{+6.92})} & \textbf{67.28 (\blue{+8.14})} \\

        % conditions
        % 10% cifar100
        % vit-tiny-patch16-224
        % randn fuction
        % common
            % attr: color, size
            % epochs 200, schedule=50,100, 150
            % init_lr=0.1, gamma 0.1, 0.1, 0.1
            % momentum=0.9, decay=5e-5
            % base_aug
                % random horizontal flip
                % random crop size=32, padding=2
                % normalize
        % mixup
            % mixup_alpha 1.0
        % cutout
            % cutout 16
        % cutmix
            % mixup_alpha 1.0
            % cutmix prob 0.5
        % textmani
            % 30, max sample
            % 0.1 scale
            % nsample true
        % textmani + alpha
            % 30 sample
            % nsample
            % 0.1 scale

        \bottomrule
    \end{tabular}
    }
    \caption{
    Classification results (\%) on CIFAR-100-10\% with ResNet18.
    Baseline represents random horizontal flip, random crop, and normalization, basically applied in all experiments.
    % \textbf{Bold} stands for the best results among those with the same number of added augmentations except for Basic, and 
    The parentheses stands for the improvement compared to the Baseline.
    }
    \label{tab:cifar100_10}
    \vspace{3mm}
% \end{table}
% \begin{table}
% \centering
\resizebox{0.95\linewidth}{!}{
\footnotesize
        \begin{tabular}{@{\,}l C{16mm}C{16mm} @{\,}} 
        \toprule
        % \multirow{2}[2]{*}{\textbf{Model}} & 
        \multirow{2}[2]{*}{\textbf{Augmentation}} & \multicolumn{2}{c}{\textbf{Acc.}} \\
        \cmidrule{2-3}
        &  \textbf{Top-1} & \textbf{Top-5}\\
        \midrule
          Baseline    & 65.37 & 89.82 \\
          Cutout      & 69.17 & 91.12 \\
          Cutmix      & 69.82 & 91.76 \\
          Mixup       & 67.54 & 90.23 \\
          $\TextMani$ & \textbf{70.81 (\blue{+5.44})} & \textbf{92.37 (\blue{+2.55})} \\
        %   $\TextMani-30sample$ & 70.81 & 92.37 \\
        %   $\TextMani-maxsample$ & \textbf{71.21} & 92.40 \\
          \midrule
          Cutout + $\TextMani$    & 69.71 (\blue{+4.34}) & 91.32 (\blue{+1.50}) \\
          Cutmix + $\TextMani$    & \textbf{71.05 (\blue{+5.68})} & \textbf{92.22 (\blue{+2.40})} \\
          Mixup + $\TextMani$     & 70.56 (\blue{+5.19}) & 91.58 (\blue{+1.76}) \\
        
        % conditions
        % 10% cifar100
        % resnet50
        % randn fuction
        % common
            % pretrained
            % interpolation 224x224
            % attr: color, size
            % epochs 200, schedule=50,100, 150
            % init_lr=0.1, gamma 0.1, 0.1, 0.1
            % momentum=0.9, decay=5e-4
            % base_aug
                % random horizontal flip
                % random crop size=32, padding=2
                % normalize
        % manimixup
            % mixup_alpha 2.0
        % mixup
            % mixup_alpha 1.0
        % cutout
            % cutout 16
        % cutmix
            % cutmix prob 0.5
            % mixup alpha 1.0
        % textmani
            % 30, max samples
        % textmani + alpha
            % 30 samples
            % nsample True
            % scale min 0.1
            
            
        % \midrule
        % \textbf{ViT-T} & \\
        % \midrule
        %   Vanilla & & \\
        %   TextMani & & \\
        % \midrule
        % \textbf{ViT-S} & \\
        % \midrule
        %   Vanilla & & \\
        %   TextMani & & \\
        \bottomrule
    \end{tabular}
    }
    \caption{Classification results (\%) on CIFAR-100-10\% with VIT-Tiny. %pretrain
    % Input image is interpolated to 224x224, 
    % Basic augmentation contains random horizontal flip, random crop, and normalization, which are Basically applied in all experiments.
    The configuration follows \Tref{tab:cifar100_10}.
    % \textbf{Bold} stands for the best results,
    % among those with the same number of added augmentations except for Basic, 
    The parentheses stands for the improvement compared to the Baseline.
    }
    \label{tab:cifar100-10_preVIT}
\end{table}


\subsection{Evenly Distributed Scarce Data Classification}\label{sec4.2}
\paragraph{Experimental Setting}
For evaluating the effectiveness of $\TextMani$ on the scarce dataset, we use 10\% data of the CIFAR-100~\cite{krizhevsky2009learning} and Tiny-ImageNet~\cite{le2015tiny} datasets, named CIFAR-100-10\% and Tiny-ImageNet-10\%, respectively.
CIFAR-100 has 100 classes with 500 training images per class, but we only use randomly sampled 50 images per class.
% The evaluation set is the same as the CIFAR-100 test set containing 10k images. 
Tiny-ImageNet is a subset of ImageNet-1k~\cite{russakovsky2015imagenet} with 100k images and 200 classes, but we use 10k images (50 images per class) for simulating 
% constructing the 
a small dataset.
\moon{Note that the evaluation set is same with those of the original datasets.}
% The evaluation set is the same as the original Tiny-ImageNet one having 10k images.
% , and the validation metrics are Top-1 and 5 accuracies.


The baseline models of scarce data classification are ResNet18~\cite{he2016deep} and ViT-Tiny~\cite{dosovitskiy2020image}.
Due to the space limit, 
% we present the ResNet18 and Vit-Tiny results in this section. 
% the results of other models and 
% the 
details of training can be found in the supplementary material.


\paragraph{Results}
\moon{We demonstrate the effectiveness of $\TextMani$ compared to mix-based augmentations on evenly distributed scarce datasets.}
% We compare the effectiveness of $\TextMani$ and other augmentation methods on the evenly distributed scarce datasets.
As in \Tref{tab:cifar100_10} for CIFAR-100-10\%, $\TextMani$ \moon{outperforms}
% is more effective than 
other methods when a single augmentation is used.
Furthermore, the effect is amplified when our method and mix-based methods are combined, \moon{with particularly good compatibility with Manifold Mixup.}
% and the compatibility with ManiMixup is particularly good.
The results demonstrate the importance of intra-class semantic perturbation along with inter-class in scarce data settings.
\moon{This tendency is also observed with another baseline architecture in \Tref{tab:cifar100-10_preVIT}, and datasets in \Tref{tab:TinyImagenet_10}, implying that
% which implies
$\TextMani$ is model-agnostic to be applied.}
% This tendency is also observed in both the ViT-Tiny (\Tref{tab:cifar100-10_preVIT}) and ResNet18 (\Tref{tab:TinyImagenet_10}) cases.
% evaluated on the Tiny-ImageNet-10\% dataset.
% The results also imply that $\TextMani$ is model-agnostic to be applied.
\moon{The overall results demonstrate the potential of $\TextMani$ to enrich the visual feature space using text modalities and develop more accurate and robust models in scarce data regimes.}










\begin{table}
\centering
% \resizebox{0.8\linewidth}{!}{
% epoch 500 : max sample 10, lr 0.2, attribute color+size
% textmani + other augmentation 은 max sample 10보다 30이 훨씬 좋아서 일단 table에는 30개 sampling으로 작성헀습니다.
\footnotesize
\resizebox{1.0\linewidth}{!}{%
        \begin{tabular}{@{\,}l C{16mm}C{16mm} @{\,}} 
        \toprule
        % \multirow{2}[2]{*}{\textbf{Model}} & 
        \multirow{2}[2]{*}{\textbf{Augmentation}} & \multicolumn{2}{c}{\textbf{Acc.}} \\
        \cmidrule{2-3}
         & \textbf{Top-1} & \textbf{Top-5}\\
        \midrule
        % \multirow{8}{*}{ResNet18}
            Baseline    & 25.94 & 50.53\\
            Cutout      & 26.41 & 50.28\\
            Cutmix      & 25.94 & 49.67\\
            Mixup       & 29.34 & \textbf{54.10}\\
            ManiMixup   & 28.43 & 53.25\\
            $\TextMani$ & \textbf{29.37 (\blue{+3.43})} & 52.37 (\blue{+1.84})\\ 
        %   \cmidrule{2-6}
            \midrule
            Cutout + $\TextMani$    & 29.14 (\blue{+3.20})	& 52.60 (\blue{+2.07}) \\
            Cutmix + $\TextMani$    & 29.86 (\blue{+3.92}) & 54.31 (\blue{+3.78}) \\
            Mixup + $\TextMani$     & 31.15 (\blue{+5.21}) & 56.71 (\blue{+6.18}) \\
            ManiMixup + $\TextMani$ & \textbf{32.39 (\blue{+6.35})} & \textbf{58.25 (\blue{+7.72})} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Classification results on Tiny-ImageNet-10\% with ResNet18. 
    % The results imply Top-1 and 5 accuracies (\%) on the test set.
    The configuration follows \Tref{tab:cifar100_10}.
    % \textbf{Bold} represents the best results, and 
    The parentheses represents the improvement compared to the Baseline.
    }
    \label{tab:TinyImagenet_10}
\end{table}




\subsection{Few-shot Object Detection}\label{sec4.3}
\paragraph{Experimental Setting}
We evaluate $\TextMani$ on the PASCAL VOC~\cite{everingham2010pascal} and MS-COCO~\cite{lin2014microsoft} datasets with a few-shot divison following Wang~\etal~\cite{wang2020frustratingly}.
% ~\cite{qiao2021defrcn}.
For VOC, we have three random splits, which have different divisions into 15 base classes and 5 novel classes among the 20 total classes, and $K=1, 2, 3, 5, 10$ objects are sampled from the novel classes.
We utilize the VOC2007 test set for evaluation with AP50 metrics and train with the combination of the VOC2007 and VOC2012 train/val set.
For COCO, the base classes are disjoint with VOC classes while the remaining classes are used as novel classes, and $K=1, 3, 5, 10, 30$ objects are sampled from the novel classes for few-shot fine-tuning.
We use 5k images from the validation set in COCO for evaluation with mAP metrics and the rest for training.

% The model is trained with the base classes first, and then fine-tuned with the novel classes.
% (few-shot object detection; FSOD setting) or with both base and novel classes (Generalized few-shot object detection; G-FSOD setting).
\moon{The baseline~\cite{yan2019meta} is the Faster R-CNN~\cite{ren2015faster} trained with the base classes first and then fine-tuned 
% the model 
with the novel classes.}
% TFA~\cite{wang2020frustratingly} using
% of the FSOD and G-FSOD 
% , denoted as FRCN, 
% which is the standard model for the object detection task.
% and Decoupled Faster R-CNN~\cite{qiao2021defrcn}, which are denoted as FRCN and DeFRCN, respectively.
\moon{$\TextMani$ is applied to the novel class samples during the fine-tuning stage.}
% at each baseline.
Following the 
% As following 
prior studies, all the reported results are averaged over 10 repeated runs.
% All the results are reproduced based on the DeFRCN~\cite{qiao2021defrcn} code.

\begin{table}
\centering
\resizebox{1.0\linewidth}{!}{
\footnotesize
        \begin{tabular}{@{\,}cl ccccc @{\,}} 
        \toprule
        \multirow{2}[2]{*}{\textbf{Split}} & \multirow{2}[2]{*}{\textbf{Aug.}} & \multicolumn{5}{c}{\textbf{$K$- shot}} \\
        \cmidrule{3-7}
        & & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{5} & \textbf{10} \\
        \midrule
        \multirow{3}{*}{All}
        & Baseline     & 12.82 & 16.65 & 20.04 & 20.64 & 23.19 \\
        & \multirow{2}{*}{$\TextMani$} & \textbf{17.74} & \textbf{22.40} & \textbf{23.37} & \textbf{25.09} & \textbf{24.22} \\
        & & \textbf{(\blue{+4.92})} & \textbf{(\blue{+5.75})} & \textbf{(\blue{+3.33})} & \textbf{(\blue{+4.45})} & \textbf{(\blue{+1.03})} \\
        \midrule
        \midrule
        \multirow{3}{*}{1}
        & Baseline     & 15.11 & 18.82 & 22.61 & 21.97 & 23.74 \\
        & \multirow{2}{*}{$\TextMani$} & \textbf{21.94} & \textbf{26.44} & \textbf{23.66} & \textbf{25.88} & \textbf{25.14} \\
        & & \textbf{(\blue{+6.83})} & \textbf{(\blue{+7.62})} & \textbf{(\blue{+1.05})} & \textbf{(\blue{+3.91})} & \textbf{(\blue{+1.40})} \\
        \midrule
        \multirow{3}{*}{2}
        & Baseline     & 10.86 & 14.22 & 18.67 & 19.34 & 22.49 \\
        & \multirow{2}{*}{$\TextMani$} & \textbf{14.64} & \textbf{18.49} & \textbf{23.28} & \textbf{23.06} & \textbf{24.44} \\
        & & \textbf{(\blue{+3.78})} & \textbf{(\blue{+4.27})} & \textbf{(\blue{+4.61})} & \textbf{(\blue{+3.72})} & \textbf{(\blue{+1.95})} \\
        \midrule
        \multirow{3}{*}{3}
        & Baseline     & 12.49 & 16.90 & 18.84 & 20.61 & 23.35 \\
        & \multirow{2}{*}{$\TextMani$} & \textbf{16.65} & \textbf{22.26} & \textbf{23.16} & \textbf{26.33} & \textbf{25.08} \\
        & & \textbf{(\blue{+4.16})} & \textbf{(\blue{+5.36})} & \textbf{(\blue{+4.32})} & \textbf{(\blue{+5.72})} & \textbf{(\blue{+1.73})} \\
        \bottomrule
        
    % gfsod
    % novel AP50 
    % textmani 30 samples
    % scale 0.1
    
    % defrcn vanilla take only 2 repeat yet
    \end{tabular}
    }
    \caption{Few-shot object detection results (AP50) on VOC. % FSOD setting.
    % The second column represents the types of augmentation methods applied.
    % Baseline stands for no augmentation methods applied.
    % \textbf{Bold} indicates the best results in each model, and 
    The value in the parentheses indicates the improvement compared to the Baseline of each split set.
    }
    \label{tab:voc}
\end{table}


\begin{table}
\centering
\resizebox{1.0\linewidth}{!}{
\footnotesize
        \begin{tabular}{@{\,} l C{10mm}C{10mm}C{10mm}C{10mm}C{10mm} @{\,}} 
        \toprule
        % \multirow{2}[2]{*}{\textbf{Model}} &
        \multirow{2}[2]{*}{\textbf{Aug.}} & \multicolumn{5}{c}{\textbf{$K$- shot}} \\
        \cmidrule{2-6}
        & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{10} & \textbf{30} \\
        \midrule
        % \multirow{2}{*}{FRCN}
         Baseline      & 3.43 & 4.66 & 6.10 &  9.11 & 12.78 \\
         \multirow{2}{*}{$\TextMani$}
         & \textbf{5.39} & \textbf{6.47} & \textbf{7.80} & \textbf{10.03} & \textbf{13.60} \\
         & \textbf{(\blue{+1.96})} & \textbf{(\blue{+1.81})} & \textbf{(\blue{+1.70})} & \textbf{(\blue{+0.92})} & \textbf{(\blue{+0.82})} \\
        % \midrule
        % \multirow{2}{*}{DeFRCN}
        % & Vanilla     & 4.63 & 12.31 & 16.06 & 18.56 & 22.43 \\
        % & $\TextMani$ & & & & & \\
        \bottomrule
    \end{tabular}
    }
    \caption{Few-shot object detection results (mAP) on COCO. % FSOD setting. 
    The configuration follows \Tref{tab:voc}.
    % \textbf{Bold} indicates the best results in each model, and 
    % The parentheses indicates the improvement compared to the Baseline.
    }
    \label{tab:coco}
\end{table}

% \begin{table}
% \centering
% \caption{GFSOD experimental results (mAP) on the COCO dataset.
% }
% \resizebox{1.0\linewidth}{!}{
% \footnotesize
%         \begin{tabular}{@{}ll ccccc @{}} 
%         \toprule
%         \multirow{2}[2]{*}{\textbf{Model}} & \multirow{2}[2]{*}{\textbf{Aug.}} & \multicolumn{5}{c}{\textbf{$K$- shot mAP}} \\
%         \cmidrule{3-7}
%         & & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{10} & \textbf{30} \\
%         \midrule
%         \multirow{2}{*}{FRCN}
%         & Vanilla        & 1.17 (15.51) & 2.72 (14.90) & 3.96 (14.74) & 5.26 (15.15) & 7.32 (15.19) \\
%         & $\TextMani$  & & & & & \\
%         \midrule
%         \multirow{2}{*}{DeFRCN}
%         & Vanilla       & 4.49 (23.73) & 10.30 (26.46) & 13.29 (27.67) & 16.42 (29.51) & 20.70 (31.23) \\
%         & $\TextMani$ & & & & & \\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:coco_g}
% \end{table}


\paragraph{Results}
% Our evaluation on the FSOD task validates that $\TextMani$ can also be applied in the detection task.
% , we evaluate our method on 
Note that we apply $\TextMani$ only on the classification head; thus, the quality of the regressed bounding boxes will remain 
% be the
similar 
% the same
as before applying $\TextMani$.
As shown in \Tref{tab:voc} for VOC and \Tref{tab:coco} for COCO, $\TextMani$ improves the AP by improving only the classification accuracy, where the result 
% follows
is in a similar line to
the analysis~\cite{borji2019empirical} that classification error weighs more than localization error.
The improvement is clearer when $K$ is low.
\moon{The results demonstrate the applicability of $\TextMani$ to enhance the classification accuracy of detection models.}






\begin{table}
    \centering
    \resizebox{1.0\linewidth}{!}{\scriptsize
    \begin{tabular}{@{\,}c@{\,\,\,}l@{\,\,}c@{\quad}c@{\quad}c@{\quad}c@{\quad}c@{\quad}c@{\,}}
         \toprule
         & \textbf{Aug.} & \textbf{Many} & \textbf{Medium} & \textbf{Few}& \textbf{IF=100} & \textbf{IF=50} & \textbf{IF=10}\\
         \midrule
         \multirow{3}{*}{(a)}
         & Baseline   & 71.11 & 38.42 & 3.00 & 38.39 & 43.33 & 59.29  \\ 
         & Random     & \textbf{71.37} & 38.55 & 2.90 & 38.43 & 43.28 & 60.39 \\
         & \TextMani  & 70.22 & \textbf{40.73} & \textbf{9.41} & \textbf{41.10} & \textbf{47.17} & \textbf{60.67} \\
         \midrule
         \multirow{2}{*}{(b)}
         & Direct.    & 71.34 & 38.64 & 4.32 & 38.66 & 43.44 & 59.82 \\
         & Concat.    & 68.02 & 35.82 & 5.35 & 36.98 & 42.68 & 59.44 \\
         \bottomrule
    \end{tabular}
    }
    \caption{Comparison to (a) random perturbation, and
    (b) direct text and concatenated embeddings on CIFAR-100-LT. 
    }
    \label{tab:random}
\end{table}




\subsection{Further Analyses}\label{sec4.4}
% In this section, we further demonstrate the compatibility of our $\TextMani$ with the linear-probed model and conduct an ablation study on attributes.

\paragraph{Random Baseline}
In \Tref{tab:random}-{\color{blue}(a)}, we compare our method with the Random baseline.
We randomly sample a vector from a Normal distribution $\mathcal{N}(0,1)$ and use it instead of the difference vector, \ie, augmenting visual features with random perturbations on the same manifold of visual features.

The result shows that the Random baseline improves performance by serving as intra-perturb, but marginal compared to our method considering semantics additionally, which implies that semantic information embedded in the difference vector guides the augmentation more effective direction rather than random.


\paragraph{Effectiveness of Difference Vectors}
While we use the difference vectors by subtracting the embeddings with and without attribute words for \TextMani, there could be another way to extract the attribute information.
In \Tref{tab:random}-{\color{blue}(b)}, we compare with counterparts, direct text embedding (Direct.) and concatenated embeddings (Concat.).
For the Direct method, we use the text embedding computed from the attribute word directly instead of the difference vector.
For the Concat method, we concatenate the text embeddings from with and without attribute words, \eg, [``bull''$\|$``red bull''], and use it instead of the difference vector.

The results show that using difference vector (\TextMani) outperforms using direct text embedding or concatenated embeddings, and imply that remaining contextual information after subtraction plays an important role in doing intra-perturbation in a semantic way.
% We further discuss the reason for the better performance in the following questions.
Although the word ``blue'' can function as both an adjective and a noun, its exact role in a sentence cannot be determined solely based on the word itself.
Our intention of subtraction is for attribute words to act as a modifier in the sentence motivated by word analogy.
When we computed the cosine similarity, embeddings derived directly from ``red'' and those obtained from the difference exhibited low similarity because they \emph{contain different contextual information} despite the same origin of a word.
% Also, the surrounding meaning can be left even after subtraction to some extent; the class label information would be reflected in the difference vector, which makes the subtle differences (L464-465) in Fig.~{\color{blue}4}.
% Also, the results imply that remaining contextual information after subtraction plays an important role in doing intra-perturbation in a semantic way.



% \begin{table}
\begin{wraptable}{r}{0.46\linewidth}
    \centering
    \vspace{-3mm}
    \resizebox{0.85\linewidth}{!}{\scriptsize
    \begin{tabular}{@{\,}l@{\,\,\,}c@{\,}}
         \toprule
         \textbf{Model} & \textbf{LP-Full}\\
         \midrule
         VL-LTR     & 61.04 \\
         +\TextMani & \textbf{61.82} \\
         \bottomrule
    \end{tabular}
    }
    % \vspace{-3mm}
    \caption{\moon{Comparison between the SOTA model with and without $\TextMani$ during linear probing on CIFAR-100.}
    % Comparison between linear-probed SOTA method and applying our method to it on CIFAR-100.
    % with ResNet18.
    }
    \label{tab:sota}
    \vspace{-2mm}
\end{wraptable}
% \end{table}
\paragraph{Linear Probing with Advanced Models}
\moon{Further demonstrating the compatibility of $\TextMani$, we apply our method during linear probing of the model.}
% when linear-probe the model.
In \Tref{tab:sota}, we test VL-LTR~\cite{tian2022vl}, the state-of-the-art model in long-tail classification, on CIFAR-100.
In \Tref{tab:CLIP_baseline}, we use a CLIP image encoder~\cite{radford2021learning} with various architectures as the baseline model and linear-probe the model on both 10\% and full data of CIFAR-100.
The results demonstrate that $\TextMani$ is compatible with linear-probed CLIP and VL-LTR models.


\begin{table}
    \centering
    \resizebox{0.9\linewidth}{!}{\scriptsize
    \begin{tabular}{@{\ \ }l@{\quad}l@{\quad}c@{\quad}c@{\quad}c@{\ \ }}
         \toprule
         \textbf{CLIP Arch.} & \textbf{Aug.} & \textbf{ZS} & \textbf{LP-10\%} & \textbf{LP-Full}\\
         \midrule
         \multirow{2}{*}{ResNet50} 
         & Baseline  & 39.47 & 50.18 & 63.64 \\
         & \TextMani &   -   & \textbf{52.83} & \textbf{64.17} \\
         \midrule
         \multirow{2}{*}{ResNet101} 
         & Baseline  & 45.17 & 57.37 & 68.60 \\
         & \TextMani &   -   & \textbf{59.49} & \textbf{69.12} \\
         \midrule
         \multirow{2}{*}{ViT-B} 
         & Baseline  & 58.21 & 73.30 & \textbf{79.99} \\
         & \TextMani &   -   & \textbf{73.35} & 79.58 \\
         \bottomrule
    \end{tabular}
    }
    % \vspace{-3mm}
    \caption{Classification results (\%) of CLIP with zero-shot (ZS) and linear-probe (LP) on Full and 10\% CIFAR-100. 
    We apply our $\TextMani$ to the linear-probed CLIP.
    % with ResNet18.
    }
    \label{tab:CLIP_baseline}
    % \vspace{-6mm}
\end{table}






\begin{wraptable}{r}{0.37\linewidth}
    \centering
    % \vspace{-2mm}
    \resizebox{0.75\linewidth}{!}{\scriptsize
    \begin{tabular}{@{\,}c@{\,\,\,}c@{\,\,\,}c@{\,}}
         \toprule
         \textbf{Color} & \textbf{Size} & \textbf{Acc.}\\
         \midrule
                    & & 31.10 \\
         \checkmark & & 33.48 \\
         & \checkmark & 33.89 \\ 
         \checkmark & \checkmark & \textbf{34.52} \\
         \bottomrule
    \end{tabular}
    }
    % \vspace{-3mm}
    \caption{Ablation study on the attributes with CIFAR-100-10\%.
    % with ResNet18.
    }
    \label{tab:ablation_attr}
    \vspace{-2mm}
\end{wraptable}
\paragraph{Ablation Study on Attributes}
\moon{In $\TextMani$, we have considered color and size attributes.}
% We consider color and size attributes in experiments of $\TextMani$.
To confirm the effect of each attribute, we conduct an ablation study on attributes in \Tref{tab:ablation_attr}.
The result shows that while each attribute brings non-trivial gain, using both brings more gain.
We believe that there are additional attributes we could use and a more effective method for selecting appropriate attributes, 
% such as prompt suggestion~\cite{pratt2022does}, 
but leave it for future work.










