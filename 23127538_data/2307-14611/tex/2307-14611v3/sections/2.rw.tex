We brief the related work in the following three perspectives:
% from the perspective of methods, \ie, 
image data augmentation, foundation models, and target application tasks.
In this work, our $\TextMani$ augments data by leveraging 
% using
% text features obtained from 
the text encoder of CLIP~\cite{radford2021learning}, BERT~\cite{devlin2018bert}, or GPT-2\footnote{GTP-2 is the decoder-only architecture, but we use it as a text embedding extractor, so we call it text-encoder.}~\cite{radford2019language}.
% , and whereby we tackle data scarce regimes including
% To validate that $\TextMani$ is model-agnostic and suited for scarce data cases, we evaluate on 
% long-tail class distribution, small dataset, and few-shot problems.
For main target applications, 
% mainly dealt within this work, 
we focus on long-tail and small data classification and few-shot object detection tasks in the data-scarce regimes.



\paragraph{Image Data Augmentation}
% \cite{simard1998transformation}
% Data augmentation is the simple and effective technique that makes models train on similar but different augmented samples to given training samples.
% These augmented samples can be obtained by applying perturbation to the original training samples, which enlarges the support of the training distribution defined by samples and produces the effect of increasing the amount of data. 
% From the viewpoint of an embedding space, data augmentation densifies
% % can fill the 
% empty spaces among 
% % between the 
% original data points without 
% % the
% additional laborious data collection.
% It is beneficial in practice, where only sparse data samples are available.
% This has shown to be surprisingly effective to generalization. 
% augment the amount of data by applying various transformations to 
% the
% original images.
% Through augmented data, a model can be more generalized and improve performance.
% Image data augmentation can be largely divided into whether semantic perturbation exists along label mixing.
Image data augmentation can be largely divided into whether semantic perturbation exists.
Semantic perturbation, in specific, can be further split into methods with or without label mixing.
% image-level and feature-level augmentations.
% In image data augmentation methods, basic perturbations applied to original training images are 
Methods~\cite{devries2017improved,srivastava2014dropout,reed1999neural,an1996effects, gulcehre2016noisy, bishop1995training, holmstrom1992using, vincent2010stacked,li2021simple,alemi2016deep,li2021feature,simard1998transformation,cubuk2020randaugment,cubuk2018autoaugment} without semantic perturbation, which have no label change, contain
% The image-level augmentation directly manipulates a training image without changing its label, 
% such as 
primitive image processing and transformation operations.
This includes photometric (\eg, color jitter, contrast, blur, noise, \etc) and geometric (\eg, horizontal reflection, rotation, \etc) operations, and
% color jitter, blur, 
% add
% noise, 
% random
% cropping, flipping, rotation, 
advanced augmentations, including Cutout~\cite{devries2017improved} and adaptive combinations~\cite{cubuk2020randaugment,cubuk2018autoaugment}.
% RandAugment~\cite{cubuk2020randaugment}, and AutoAugment~\cite{cubuk2018autoaugment}, 
% or with label manipulation, such as 

In contrast, Mixup~\cite{zhang2017mixup}, CutMix~\cite{yun2019cutmix}, and manifold Mixup~\cite{verma2019manifold} execute semantic perturbation along with label mixing.
Mixup interpolates two whole input images pixel-wisely, 
% in a pixel level,
% within the overall region of an image, 
CutMix interpolates a partial region of an image with another, and manifold Mixup mixes features from the images.
These mix-based methods also augment labels of samples by an inter-class semantic perturbation, where labels of two different class samples are mixed. 
While the mixed label is known to be effective for generalization and model calibration effects~\cite{guo2017calibration}, we found that 
% \moon{which would be consistent with Cha~\etal~\cite{cha2021swad}.}
% They include the process of sampling two targets to be mixed from the dataset.
% If the dataset with a non-uniform distribution has long-tail classes, the probability of being sampled between classes would be different, which can degrade the performance.
the mix-based methods are heavily affected by class distribution due to sampling from two sources; thus, their effect is restricted to evenly distributed datasets.
For datasets with skewed
% non-uniform 
class distributions with tails,
% classes, 
the sampling probabilities between major and minor classes would significantly differ, which can exaggerate biased sampling to major classes and makes minor classes more minor. \\
% can lead to accuracy degradation.
% of the performance.


Our $\TextMani$, on the other hand, is applied to all of the given samples uniformly regardless of class distribution.
$\TextMani$ densifies around the sample features by perturbing and enriching the semantic meaning of them
% the samples 
at an intra-class level, which does not change the label.
Moreover, because of the different semantic granularity of perturbation between $\TextMani$ (intra-class) and mix-based methods (inter-class), two methods can be used complementarily when class imbalance does not exist.

% augment both inputs and labels.
% While the image-level augmentation is often visually intuitive, it should be designed carefully, considering the characteristics of the image data, target task, and the model architecture.

% The feature-level augmentation manipulates input at a feature level in 
% % extracted from
% the model.
% Given features extracted from an intermediate layer of a neural network for corresponding input samples, 
% feature augmentation methods have been suggested, such as
% % such as 
% interpolation and extrapolation between two data points~\cite{devries2017dataset}, adding noise~\cite{li2021simple,li2021feature,srivastava2014dropout,alemi2016deep}, which preserve the original labels unchanged.
% % corruption, 
% Manifold Mixup~\cite{verma2019manifold} mixes (interpolates) two features of different classes with label mix.
% The feature contains the information effective to do the target task from the image data, and implicitly reflect the characteristics of the model;
% thus, the feature-level augmentation can be designed in more loose constraints than the image-level one.

% However, the existing feature-level augmentations do not consider semantic awareness. 
% Usually, they utilize arithmetic characteristics such as interpolation, which is not intuitive to understand the meaning of the augmented feature.
% $\TextMani$ augments the data at the feature-level, but we can interpret the meaning of the augmented feature through the embedding
% % feature
% from the text modality.
% Our method, also, can
% % could
% be used together 
% % combined
% with
% % the
% % without the label corruption 
% other image-level augmentation that does not change labels or Manifold Mixup for feature-level augmentation, which are complementary.
% $\TextMani$ has the advantages of both level augmentations and also could additionally leverage the benefit of other methods.




\paragraph{Foundation Models}
Recent foundation models~\cite{yuan2021florence, radford2021learning, li2022grounded, jia2021scaling, ramesh2021zero, devlin2018bert, radford2019language} have shown a successful case of reflecting human nuances with visually imitated word composition.
% Especially, 
Particularly, language models, \eg, BERT~\cite{devlin2018bert} and GPT~\cite{radford2019language}, show their ability not only in language tasks~\cite{wu2020tod} but also in vision-language multi-modal tasks~\cite{sammani2022nlx,gal2022image}.
Contrastive Language-Image Pretraining (CLIP)~\cite{radford2021learning} also achieves huge success in various tasks even in zero-shot recognition.
% CLIP contains the image and text encoders and is trained with contrastive loss on the large-scale image and text pairs.
% The multi-modal joint embedding space of the CLIP is learned for zero-shot image and language-related recognition tasks.
Follow-up studies show that CLIP representation is effective in conducting other visual tasks by bridging vision and language, \eg, 2D image generation~\cite{gal2022stylegan, kwon2022clipstyler, kim2022diffusionclip, nichol2021glide}, image manipulation~\cite{patashnik2021styleclip, kim2022diffusionclip} and synthesis~\cite{frans2021clipdraw}, and even 3D domain tasks~\cite{youwang2022clip, jain2022zero, michel2022text2mesh}.
% Prior works optimize the features with CLIP loss to output manipulated images, pose or textures.

In $\TextMani$, we focus on estimating attribute features by exploiting BERT, GPT-2, or CLIP text encoder alone. 
Distinctively, we only transfer the estimated attribute feature to augment visual features in a different space, which makes our work different from knowledge distillation~\cite{hinton2015distilling} of foundation models~\cite{dai2022enabling,wang2022multimodal,shin2022namedmask}.
Rather, our design is an instance of the module neural network structure~\cite{happel1994design,andreas2016neural}, where recent module-based designs procedurally train the whole model module-by-module with the guidance of the well pre-trained module, \eg, \cite{kumar2021rma,rombach2022high,oh2019speech2face,sung2023sound,gupta2023visual}.
Also, our work is applicable agnostically to architectures; thus, 
% which is
more flexibly applicable than fine-tuning of foundation models~\cite{wortsman2022robust}.

% $\TextMani$, however, focused on feature manipulation exploiting CLIP embedding to extract the residual information and improve target task performance.


\paragraph{Long-tail Classification}
In real world, visual data follow a long-tailed distribution which induces class imbalance and leads to
% Model performance heavily relies
% has a heavy reliance 
% on data quality, which is affected by the imbalance factor~\cite{yang2022survey};
% thus, model 
performance degrading~\cite{yang2022survey}.
% on heavy tail data distribution.
A representative line of the methods for long-tail classification is rebalancing~\cite{buda2018systematic, cui2019class, ren2018learning}, which resamples data or reweights the loss for tail classes.
However, improvement in performance of the tail classes comes with the sacrifice of head class performance.
Note that $\TextMani$ densifies all the given samples regardless of the class imbalance, and whereby 
% So 
the model is trained with reasonable variations of training samples for every class at least,
% has a lower bound in number of samples for every class, 
which improves the performance while minimizing sacrifice of the head class.
% Note that $\TextMani$ densifies all the given samples regardless of the class imbalance so the model can see a certain number of samples for all classes, which improves the performance with minimum sacrifice of the head class.


% The prior work for long-tail classification can be classified into class rebalancing~\cite{buda2018systematic, cui2019class, ren2018learning}, multi-stage training~\cite{cao2019learning, kang2019decoupling}, and multi-expert~\cite{cai2021ace, xiang2020learning, zhou2020bbn, wang2020long} methods.
% Class rebalancing methods resample the data or reweight the loss for the tail classes, which improves the performance of the tail classes with the sacrifice of the performance of the head classes.
% Multi-stage training methods consist of representation learning and classifier training stages to learn richer representations that can encode even tail distribution.
% Multi-expert methods engage multiple models to learn different aspects of information and ensemble or distill the knowledge of each other.
% Note that $\TextMani$ augments all data so that the model can see a certain number of samples for all classes.
% $\TextMani$ could be used in combination with existing long-tail classification methods because it is orthogonal with existing methods.



\paragraph{Few-Shot Object Detection (FSOD)}
% In this work, w
We 
% specifically
tackle FSOD, one of the sparse sample problems, to demonstrate the effectiveness of $\TextMani$ and its model architecture agnostic property.
% of our proposed method.
FSOD handles novel object classes after the base training for object detection tasks.
The model rapidly adapts to novel classes using few data by matching-based~\cite{li2021beyond, chen2021dual, xiao2020few} or fine-tuning based~\cite{hu2021dense, sun2021fsce, wang2020frustratingly, qiao2021defrcn, ye2023eninst} methods.
% Matching-based methods train the object detection model by measuring the similarity between the query objects and the given small number of novel objects.
% Fine-tuning based methods force the model to adapt its parameters fast to novel object classes with few examples.
$\TextMani$ is evaluated with the fine-tuning-based FSOD approach~\cite{wang2020frustratingly},
% task in a 
% % with
% fine-tuning approach, 
% based methods, 
which facilitates to use 
% have
general model architectures.
% Note that we apply $\TextMani$ only on the classification head; thus, the quality of the regressed bounding boxes will be the same as before applying $\TextMani$.

% There are several data augmentation methods~\cite{hariharan2017low, wang2018low} in the few-shot classification, which generates additional images of novel classes using generative models.
% They are similar to $\TextMani$ in the sense of enriching the data, but different in augmentation level and target task.
% $\TextMani$ augments the data by manipulating the feature, which is more simple than image generation.






