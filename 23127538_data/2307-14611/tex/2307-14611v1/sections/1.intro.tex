% Neural networks have been a strong backbone of many industries as well as research fields. One of crucial success-requirements of the \emph{data-driven} neural networks is \emph{relevant} and \emph{sufficient} data. Neural networks normally degenerate their performance on target tasks if the train and test data present even slightly different distributions~\cite{ben2010theory,szegedy2014intriguing,verma2019manifold}. 
% Neural networks often present high accuracy on train data, whereas suffer from under-performance on test set
% if data is scarce. 
% In practice, acquiring data
% with those properties is uneasy or expensive.
% Medical data is expensive and uneasy to acquire but synthetic or other data can not be a complete replacement due to the domain shift.
% In open world recognition, where the target classes
% % concepts (or classes) 
% are almost unlimited, collecting sufficient data with ground-truth on all the classes is almost impossible.

% Deep neural networks have been the backbone model of the state-of-the-art systems and have enabled breakthroughs in diverse fields including computer vision~\cite{lecun2015deep}.


% Figure environment removed

% Neural networks 
Learning models, \eg, neural networks, are known to perform well on visual recognition tasks when training and testing datasets 
% that
present similar distributions~\cite{ben2010theory}.
However, their performance often degrades considerably when evaluated in subtly different distributions
% The performance, however, often degrades considerably when evaluated in subtly different distributions
% shifted even slightly
\cite{szegedy2014intriguing}.
One effective way to enhance the generalization ability of 
% make
a model 
% generalized 
against such data distribution shifts would be
% is
data augmentation~\cite{devries2017improved,zhang2017mixup,yun2019cutmix,li2021simple,li2021feature,verma2019manifold}.
Augmenting data enlarges the support of the training distribution formed
% defined
by given samples and yields the effect of increasing the amount of data even without additional laborious data collection.
By training on augmented data, decision boundaries 
% \oh{can secure margins,}
are smoothed, 
and the generalization ability of the model is improved~\cite{verma2019manifold}.
% ability is increased



% Visual data augmentation should need careful design founded on understanding visual data generation process, models, and target tasks.
There has been a distinctive and successful line of research for label mix-based data augmentation, such as Mixup~\cite{zhang2017mixup}, CutMix~\cite{yun2019cutmix}, and manifold Mixup~\cite{verma2019manifold}, which are
% is
effective for model generalization and calibration~\cite{guo2017calibration}.
% There are simple but effective mix-based visual data augmentations, such as Mixup~\cite{zhang2017mixup}, CutMix~\cite{yun2019cutmix}, and manifold Mixup~\cite{verma2019manifold}.
The effectiveness of those label mix-based approaches is attributed to semantic perturbation by label mixing~\cite{zhang2017mixup,verma2019manifold,yun2019cutmix}.
This is a distinctive property from other lines of data augmentation methods, \eg, \cite{vincent2010stacked,li2021simple,li2021feature,simard1998transformation,cubuk2020randaugment}, where they synthesize diverse virtual data that appear differently but retain class semantics of original contents.
% They augment data by interpolating the input images or those features as well as mixing those labels. 
% with the mixing ratio corresponding to the input mix.
% which additionally have a model calibration effect.
However, we found that the performance of mix-based augmentation methods is
% are
noticeably degraded when 
% applied to
training with 
% the effect of mix-based augmentation is reduced when we have only 
skewed class distribution having scarce samples for non-major classes, \ie, long-tailed distribution.
% ~\cite{yang2022survey,cui2019class,cao2019learning}.
In 
% the
real-world, data often exhibit long-tailed class distribution (\eg, 
% e.g.,
Pareto distribution), which cannot be dealt with the prevalent mix-based approaches.
% Frustratingly, real-world data often exhibit long-tailed class distribution.
This motivates us to seek a semantically rich data augmentation 
% that is 
effective for limited data regimes, including long-tailed distribution, scarce data, and few-shot cases.




% The performance, however, often degrades considerable when evaluated in distributions even slightly-shifted~\cite{szegedy2014intriguing} or long-tailed~\cite{yang2022survey,cui2019class,cao2019learning} where we only have limited scarce samples for non-major classes.



% This behavior is attributed to the sharp decision boundary close to the training data~\cite{verma2019manifold}.
% This flaw becomes severer with sparser training data support, which easily leads large neural networks to memorize the training data even with strong regularization~\cite{zhang2017mixup}, \ie, overfitting.
% % This is a practical issue for developing many practical systems exploiting real world data.
% Frustratingly, real world data often exhibit a long-tailed class distribution, where we only have limited scarce samples for non-major classes.
% % This is a more challenging regime and a practical issue for both training and inference.
% Real world often have long-tail distributed sparse samples besides major classes.
% As real world data often exhibit a long-tailed class distribution, we typically exploit data samples of such distribution as a training dataset, in which we only have limited scarce samples for non-major classes.
% A practical recognition model should consider the tail classes during or after training.

% These tail classes often act as a bottleneck to accuracy of recognition models during both training and inference in practice.
% To mitigate these challenges, 
% Prior researchers~\cite{buda2018systematic,cui2019class,ren2018learning} attempt to expose the sparse samples more frequently by resampling or reweighting the sparse
% tail class 
% samples.
% While the performance on tail class is increased, they sacrifice major class performance due to they do not avoid overfitting.
% While these approaches improve accuracy of tail classes, it often accompanies sacrificing major class performance.
% due to overfitting.
% because it is challenging to avoid overfitting.
% Another work~\cite{hariharan2017low, wang2018low} in few-shot setting densify the sparse samples by generating novel class data.
% However, they need to filter out whether the generated image is semantically appropriate as the data of the class.
% With simple resampling and reweighting, 
% However, simply repeating the same data during training does not help and rather increases chances of memorization and sharp decision boundaries.
% One way to alleviate the problem caused by sparse samples is data augmentation~\cite{devries2017improved,zhang2017mixup,yun2019cutmix,li2021simple,li2021feature,verma2019manifold}.
% Augmenting data enlarges the support of the training distribution defined by samples and yields the effect of increasing the amount of data even without additional laborious data collection.
% Decision boundaries can be smoothed by training on the augmented data, which is simple and effective.
% Data augmentation is a simple and effective method to address the issues by smoothing decision boundaries with training on
% make model train on 
% augmented data, \ie, synthesized diverse virtual data that appear differently but preserve class semantics of original contents.
% similar but diverse augmented samples different to given training samples.

% Thereby, it densifies and fills up empty space among feature points of original training data 

% efforts.
% From the viewpoint of an embedding space
% , 
% It is beneficial in practice, where only sparse data samples are available.
% It has been shown 
% to be 
% surprisingly
% effective for generalization.



% These 
% Augmented samples can be obtained by applying sorts of perturbation to original training samples.
% Mainly three types of perturbation have been exploited: 1) simple random perturbation~\cite{devries2017improved,srivastava2014dropout,reed1999neural,an1996effects, gulcehre2016noisy, bishop1995training, holmstrom1992using, vincent2010stacked,li2021simple,alemi2016deep} including uninformative noise,
% % \eg, , 
% 2) jittering each component of the dataset-specific data generation pipeline~\cite{li2021feature,simard1998transformation,cubuk2020randaugment,cubuk2018autoaugment}, \eg, image warping, scaling,
% % , 
% and
% 3) semantic interpolation by label mix~\cite{yun2019cutmix,zhang2017mixup,verma2019manifold} having model calibration effect.
% % , \eg, 
% While most augmentation methods conserve the semantic contexts of data, label mix-based augmentations change the semantic meaning during augmentation.
% In other words, augmenting in a semantically meaningful way has barely been explored.
% Interestingly, most augmentation methods conserve the semantic contexts of data, 
% % most of data augmentation methods augment data such that semantic contexts of data are unchanged, 
% except the label mix-based augmentation.
% The label mix-based methods corrupt semantic meaning during augmentation.
% Thus, augmenting in a semantically meaningful way has been barely explored.
% % the role and effect of semantic meaning changes after augmentation have been barely explored.
% % is not intuitive and interpretable. 




% Our work is built on a hypothesis that, if we have an oracle image manipulator that can edit some attributes (\eg, size, color, pattern, style, \etc) of contents in an image easily, 
% % text-based image editing model. 
% then we can augment input images on input domain by randomly editing their attributes. 
% These augmented images would result in semantically meaningful and plausible feature representation, and the decision boundary trained by those would be better shaped according to semantic relations. 
% However, while generative models have been advanced, realization of this naive way would be intractable mainly due to expensive computation to be used during training a target model and lack of data to train the oracle image manipulator that implies the chicken-or-egg dilemma.





% By avoiding such explicit image-level manipulation as augmentation, 
In this work, we propose $\TextMani$, a text-driven manifold augmentation, which is effective for 
% takes account of
long-tail classes and scarce data.
% \oh{Our hypothesis 
% We hypothesize that 
% visual attribute words represented by general language models, \eg, BERT~\cite{devlin2018bert} and GPT~\cite{radford2019language}, may already be learned to encode some visual concept information to some extent, which may be transferred to the visual embedding space.
We hypothesize that general language models, \eg, BERT~\cite{devlin2018bert} and GPT~\cite{radford2019language}, have learned visual information to some extent that can be transferred to visual feature spaces.
% With this hypothesis, we develop our method enriching the visual feature space in a semantically meaningful way by leveraging visually mimetic text.
% \son{With this hypothesis, we leverage visually mimetic texts, encoded with general language models and transferred to target visual feature space, to semantically enrich the target space.}
With this hypothesis, we semantically enrich the target visual feature space by leveraging visually mimetic texts, encoded with general language models and transferred to the target space.
% in a semantically rich meaningful way.
% Based on the hypothesis that the relationship between visual attributes in general language models would be related to the visual embedding space, our method enriches the visual feature space in a semantically meaningful way
% % latent augmentation method 
% by leveraging visually mimetic text.
% , called $\TextMani$, to tackle limited data challenges.
% including long-tailed data, few-shot, and scarce data problems.
% $\TextMani$ is designed to mimic attribute changes of input on the feature space by adding meaningful attribute vectors to input features.
Specifically, $\TextMani$ encodes meaningful attributes such as ``red'' and ``large'' to vectors by computing the difference between text embeddings 
% of text 
with and without attributes.
We add the attribute embeddings to target visual features to mimic those attributes on the target visual feature space.
% \son{
% Specifically, $\TextMani$ encodes meaningful attribute texts such as "red" and "large" to vectors with general language models and adds them to target visual features to mimic those attributes on the target feature space. 
% Due to the difference between language feature spaces and the target visual feature spaces, we transform gradient of vectors: difference between text embeddings with vs without attributes, before adding them to the target feature space.
% }
%Figure~\ref{fig:teaser2} illustrates the augmentation process of $\TextMani$ on feature level,
% (latent representation), 
% where
% Specifically, $\TextMani$ is illustrated in .
%the input feature (\eg, the visual feature of ``bull'') is manipulated by adding the attribute vector induced by the attribute text (\eg, ``red''), which yields the augmented visual feature (\eg, ``red bull'').
Figure~\ref{fig:teaser2} illustrates the augmentation process of $\TextMani$. 
The input feature (\eg, the visual feature of ``bull'') is manipulated by adding the attribute vector induced by the attribute text (\eg, ``red''), which yields the augmented visual feature (\eg, ``red bull'').
% mimics attribute changes of input
% on the feature space (\eg, to the visual feature of ``red bull'').
% We note that $\TextMani$ uses a controlled text. 
Thanks to the text modality properties, the augmentations generated by $\TextMani$ are symbolic, human-interpretable, 
% physically relevant 
and easily controllable.
% We induce attribute vectors for augmentation from texts due to its modality benefits that are symbolic, human interpretable, physically relevant and easily controllable by its symbolic property.
% \moon{For example, $\TextMani$ uses modifier words, such as ``red,'' ``blue,'' ``large,'' and ``small,''  and returns the augmented visual feature related to the words.}
% For example, modifier words such as ``red'', ``blue'', ``large'' and ``small'' are used to control $\TextMani$ and it returns augmentations visually related to the words.


Our approach applies semantic perturbation on a different level to that of the label mix-based methods~\cite{zhang2017mixup,verma2019manifold,yun2019cutmix}.
The mix-based methods augment a sample from a combination of two different class samples, \ie, applying semantic perturbation in an \emph{inter-class} way.
% by mixing two different source classes.
This further aggravates the class imbalance problem in the long-tailed (skewed) class distribution cases.\footnote{For example, if data size of major classes is 10 times larger than 
% one in 
minor classes, the probability of choosing a pair of source samples from the major classes is approximately 100 times more than that of 
% ones from
minor classes.}
% In contrast, we perturb in an \emph{intra-class} way which does not alter the class label of a sample but enriches semantic granularity of the class.
Our $\TextMani$, whereas, perturbs data in an \emph{intra-class} way. 
A sample per each class is selected, and we enrich the semantic granularity of the class using the sample, thus enabling us to better maintain the amount of augmentation balances in the long-tailed class distribution cases.
Moreover, $\TextMani$ can densify around the given training samples by extrapolating the class semantics along augmented semantic attribute axes.
Our method, further, can be combined with the label mix-based methods to improve performance in evenly distributed sparse data cases because they are complementary.
% \moon{Further, since our method is complementary to the label mix-based methods, fusions with ours
% % those fusion with ours
% % using both methods 
% are effective in evenly distributed sparse data cases.}








% Compared to the label mix-based augmentation, $\TextMani$ has advantages in imbalanced data distribution.
% As depicted in \Fref{fig:vs_mix}, 
% % let us
% suppose the scenario that two training samples are given for two tail classes.
% The augmented points by a mix-based method would be located only on the interpolation lines between the given samples, which limits the augmentation effects.
% If we apply a mix-based method in the long-tail class distribution cases, \ie, notably skewed distribution, 
% the class imbalance problem is further 
% % exaggerated 
% aggravated.
% This is because the mix-based methods augment a sample from a combination of two different class samples, where the probability of the tail classes to be sampled is decreased by square. 
% % less sampled in a batch.
% % selected.
% In contrast, $\TextMani$ applies 
% % achieves
% % the
% uniform
% % same
% effects regardless of class imbalance.
% $\TextMani$ can densify around the given samples by extrapolating the class semantic along augmented semantic attribute axes.
% On the other hand, in another scenario with small training data but with mild uniform class distribution, 
% % When only small data are considered, 
% both methods would increase diverse combinations of samples by augmentation in respective aspects, which leads to complementary performance improvement; thus, this may hint that 
% combining
% % We can combine 
% the two complementary methods is favorable 
%  for mild uniform class distribution cases.
% % can be obtained.


% naive
% one, this
% Our approach would be 
% % far more
% efficient and tractable.
% % while hallucinating similar effects. 
% A trade-off is to use a text encoder to estimate visually plausible attribute vectors from texts, where we use the pretrained CLIP text encoder~\cite{radford2021learning} to define a visual-language joint embedding space.
% Note that, different from knowledge distillation~\cite{hinton2015distilling}, we do not transfer-learn the CLIP embeddings directly, but a difference vector between a class name and its modified class name embeddings, \ie, attribute vector, with learned projection to fit to target model's dimension.
% This allows the target model's feature space to be independent to the CLIP embedding space and its dimension.


% We define it by using the text encoder part of a recently successful foundation model, Contrastive Language-Image Pre-training (CLIP)~\cite{radford2021learning}, which maps free-form natural language as input and can reflect the nuances with the composition of visually mimetic words.




% for synthesizing the perturbing vectors, 

% , and enrich the sparse sample points semantically plausibly.


% In this way, with CLIP text encoder, we can achieve a generic zero-shot feature augmentation.
% drift the original class information to another.
% which manipulates the zero-shot attribute on a target feature space.


% which are interpretable and do not 
% harm the essence of the original classes.

% exploiting the advantages of the visually mimetic words

% Recent foundation models~\cite{radford2021learning, li2022grounded, jia2021scaling, ramesh2021zero} have shown successful cases that 

% The advantages of visually mimetic words are human interpretable, physically relevant, and controllable or manipulatable.
% We are motivated to exploit these advantages and 
% agonize whether the visually mimetic words can be used as 

% To mitigate the challenging sparse data problems by exploiting the advantages of the visually mimetic words, we propose $\TextMani$, which manipulates the zero-shot attribute on a target feature space.


% We extract the residual information between original and variant class name texts on the Contrastive Language-Image Pre-training (CLIP)~\cite{radford2021learning} embedding, 
% which has multi-modal joint representation trained with enormous text-image data pairs.
% The extracted perturbing vector is then projected 
% % transferred 
% to the feature space of the target task model, and enrich the sparse sample points semantically plausibly.




% % Figure environment removed

\moon{To empirically support that our attribute vector estimation with text embedding is reasonably designed, we devise two visualization-based analyses: with t-SNE~\cite{van2008visualizing} and a latent inversion technique.}
% When designing $\TextMani$, 
% \oh{we compute and use a difference vector between text embeddings of a pure class text and its modified text with visual attributes as an attribute vector.
% This is built on our hypothesis that}
% % we hypothesize that 
% the relationship between visual attributes 
% in text embedding space would be largely relevant to the visual embedding space.
% % and difference vectors have the meaning of attribute words.
% % the residual information 
% % represent attribute words.
% To validate these hypotheses, we devise two visualizations: with t-SNE~\cite{van2008visualizing} and a latent inversion technique.
\begin{comment}
First, we visualize the CLIP text embedding with the class name and its variant points by t-SNE~\cite{van2008visualizing}, which shows fairly clustered distribution of text-level augmentation.
Second, we devise a feature inversion technique of the CLIP texture encoder from a text embedding to an image, which enables visualization of attribute effects on input images.
\end{comment}
\begin{comment}
Given a target model to be trained for a target task and an input training sample, inspired by the word vector analogy~\cite{mikolov2013efficient}\footnote{It was shown that simple algebraic operations can be performed on the word vectors, \eg, king - man + woman $\approx$ queen on the embedding space.
}, 
\end{comment}
\begin{comment}
However, the text and target visual feature spaces are not directly compatible; thus, 
we project the attribute vector to the visual feature space of the target task model, which is used as an augmentation vector to the input feature.
For estimating the attribute vector, we need to define a text embedding space that represents texts. 
We use the class name of input as a reference input text and variant texts by adding random visual attribute words. By subtracting two text embeddings of the reference input text and a variant text, we can estimate a vector corresponds to the added attribute, \ie, an attribute vector on the text embedding space.
\end{comment}
% It
\moon{These demonstrate that attribute vectors lead to visually interpretable manifold augmentation of input.}
% images.
% , which validates our design.
% These results show that the difference vector between the original and variant text embeddings embeds visual attribute information, and we may use it to inject the attribute signals to the target feature space.
We also evaluate our method with two different tasks in scarce data regimes: few-shot object detection and image classification with deficient datasets and long-tail datasets. 
% The considerable 
Our experiments demonstrate that $\TextMani$ is an effective and model-agnostic data augmentation method, especially in 
% the
scarce data cases, by exploiting the favors of zero- shot attributes.
Our key contributions are summarized as:
% \vspace{-1mm}
\begin{itemize}
    \item We propose $\TextMani$, which enriches the visual features
    % is a generic zero-shot feature augmentation 
    % for densifying sparse samples 
    by conveying \moon{attribute information from the text embedding to the target visual feature space.}
    % text attribute information to the target feature space.
    % The residual information is from the class name as text input and its variant with attribute words, which are explainable and controllable.
    % \vspace{-1.5mm}
    % \item We devise two visualization-based analyses to empirically support that our attribute vector estimation with text embedding is reasonably designed.
    \item We demonstrate that $\TextMani$ is especially helpful in augmenting sparse samples in long-tail class cases.
    \item We show that our $\TextMani$ is complementary to other augmentation methods, and in particular, the combination of our $\TextMani$ and manifold Mixup~\cite{verma2019manifold} remarkably improves the performance in deficient data cases.
    % We demonstrate adding attribute words produces scattered embedding vectors around the original points by visualizing the CLIP text embedding. 
    % We also give evidence that we can utilize the difference vector to transfer the attribute signal to the target feature space through simple image manipulation examples.
\end{itemize}
