% Figure environment removed



% A lack of information occurs in various cases. 
% Data size can be imbalanced among the classes (long-tailed data distribution); total data is 
% % often
% too scarce to reach 
% % target
% goals; or the number of novel class examples for adaptation is small in practice (few-shot case).
% % The problem we are addressing, the lack of information, occurs in various cases; data imbalance among the classes (long-tail data distribution);low volume data or insufficient example for novel class adaptation (few-shot case).
% % To deal with the information scarcity in such cases, we propose $\TextMani$, Text-driven Manipulation and Augmentation, which densifies the given data by exploiting the advantages of visually mimetic words from the foundation models.
% % Our $\TextMani$, Text-driven Manifold Augmentation deals with the information scarcity in such cases, which densifies the given data by exploiting the advantages of visually mimetic words.
% \moon{We propose $\TextMani$, Text-driven Manifold Augmentation that deals with information scarcity by
% exploiting the advantages of visually mimetic text to densify the target visual feature space with insufficient data.}
% % from the CLIP text encoder.

% derived from attribute words.
% utilizes the class as text input.

% We are interested in leveraging the class name as means of extracting additional information to densify the sparse image samples.

% The class label is usually utilized only as a supervision for measuring the loss, but we treat it as text and manipulate it.
% In classification tasks, the class label is typically utilized only as a supervision for measuring the loss, 
% but we treat the class label as text and derive semantic information from it.
% However, the class label as text is too coarse to represent rich semantics within a class,
% compact and semantic to be used for training, 
%\eg, the small size of brown colored dog and the large size of black colored dog are involved in the same class ``dog.''
In image classification, the class label is typically utilized only as a supervision for measuring the loss. 
We, instead, propose to treat the class label as additional information, the text describing the class, and derive semantic information from it. 
% We, instead, propose to treat the class label as one more information, the text describing the class, and derive semantic information from it. 
% However, only a class label as a text description is too coarse to represent rich semantics within a class.
However, class label as a text description itself is too coarse to represent rich semantics within a class.
For example, a class label ``dog'' does not represent all the details of the description such as ``small size of the brown colored dog.''
% and the large size of the black colored dog.
To enrich
% make the model understand the
% diverse levels of 
the detailed semantics over
% from
the given coarse class texts,
% To unpack the information of compact class text and augment the sparse image data, 
we leverage the attribute words, such as ``small size'' and ``brown colored,'' that can visually modify objects in images at the semantic level.



% The most of the image data augmentation deforms the image directly, which needs to understand the characteristics of the image used and the target task, or interpolates the features, which meaning is not intuitive.
% Image-level data augmentation deforms the image directly, which needs a careful understanding of the image and target task's characteristics.
% Feature-level data augmentation usually utilizes feature interpolation, which is more comfortable to manipulate but whose semantic meaning is not intuitive.
% % While most of the image data augmentation methods deform the image-level directly or interpolate in the feature-level, our 
% While our proposition, $\TextMani$, belongs to the feature-level augmentation, the augmented features can be easily interpreted and manipulated by adding attribute words at the text-level. 
% changes the text, which is more casual to handle.
% To manipulate the image effectively for training, we need to understand the characteristics of the image used and the target task.
% After careful investigation, it is also necessary to devise an efficient method to apply it to numerous 2D images.
% Text is more comfortable to manipulate because it is more intuitive and structured than images.


% The class information is usually exploited only as a supervision label for measuring the loss.
% We considered that class information could also be used as input data at the same time as the image.
% However, the class information as text is too compact and semantic to be used for training, \eg, the small size of the brown colored dog and the large size of black colored dog have the same class ``dog''.
% To unpack the information of compact class text, we leverage attribute text that can modify objects.

% A commonly used text caption has enriched information but harder to obtain and has a higher probability of containing useless or intrusive information than class text.


\subsection{Main Idea}\label{sec3.1}
The main idea of $\TextMani$ is to densify distribution around sparse training samples on the target
% model's
feature space, making it semantically rich through the difference vectors having plausible attribute information,
% in a semantically meaningful way 
% through
% % by using semantically 
% plausible zero-shot attributes,
% to improve the performance of the target model,
% 
as depicted in \Fref{fig:teaser2}.

Figure~\ref{fig:textmani} illustrates how $\TextMani$ augment data.
% the way to augment data with $\TextMani$.
Suppose we have an image $\bI_0$ and 
% its
corresponding class label $T_0$.
The model generally learns the target task using the image $\bI_0$ as an input and the class label $T_0$ as supervision.
In this work, we also consider the class label $T_0$ as text information and extract the embedding vector $\bolde_{T_0} \in \mathbb{R}^{d_c}$ using text encoder, \eg, CLIP~\cite{radford2021learning}, BERT~\cite{devlin2018bert}, or GPT-2~\cite{radford2019language}, where $d_c$ is the text embedding dimension.
For obtaining an embedding vector $\bolde_{T_0}$, we use the text embedding of the encoder output directly when using CLIP text encoder, or use the average vector of 
% computed from 
all the embeddings of the sentence when using other language models such as BERT or GPT-2.
% When using BERT or GPT-2, we average all the embeddings from the sentence and use it as an embedding vector $\bolde_{T_0}$.
% In this way, we can leverage the visual-language joint embedding space to represent text as a visually relevant vector and obtain visual attributes from given texts.


Specifically, text input $T_0$ is formed with class name and pre-defined prompts, such as ``a photo of,'' ``a picture of,'' and ``a sketch of.''
We also synthesize another text input variant $T_1$ by adding color or size attribute words, such as ``red'' and ``big,'' and compute the embedding vector $\bolde_{T_1} \in \mathbb{R}^{d_c}$.
Numerous variants can be created with various attribute words and their combinations, but we explain the case of one variant for convenience.
Based on the word vector analogy\footnote{It was shown that simple algebraic operations can be performed on the word vectors, \eg, king - man + woman $\approx$ queen on the embedding space.}~\cite{mikolov2013efficient},
we hypothesize that the relationship between $T_0$ and $T_1$ is maintained in the text embedding space, \ie, 
% the difference by attribute in the text input can be regarded as the difference of feature vectors $\bolde_{T_0}$ and $\bolde_{T_1}$ in the text embedding space; 
% thus, 
the difference vector $\mathbf{\Delta}_{0\to1}=\bolde_{T_1} - \bolde_{T_0}$ would contain the information of added attributes (this hypothesis will be validated 
% later
in \Sref{sec3.2}).
To exploit the difference vector from text embeddings, we design our method on the manifold.
% motivated by manifold augmentation~\cite{verma2019manifold}.


% While
We can obtain such diverse attribute vectors from various attribute text templates; however,
% samples, 
their representation space is not directly related
% completely irrelevant 
to the visual feature space of the target model we are interested in.
To bridge the gap, we project the difference vector 
% on the target dimension
%using a learnable linear projection layer $\texttt{proj}(\cdot)$ to the target feature space, 
to the target feature space with a learnable linear projection layer $\texttt{proj}(\cdot)$.
Then, we
% and 
add the projected difference vector to the target image feature $\boldf_{I_0} \in \mathbb{R}^{d_t}$ obtained from the target task encoder with the input image $\bI_0$, where $d_t$ is the target feature space dimension.
% the dimension of the target feature space.

% To exploit the attribute vector on the target feature space, we add the difference vector on the image feature  from the target task encoder, 


% When the dimension of CLIP text embedding and target feature space are the same, \ie, $d_c = d_t$, we can add the difference vector on the target image feature directly.
% When they are different, \ie, $d_c \not = d_t$, 






To inject the stochasticity, 
a mixing weight $\alpha \in \mathbb{R}$ is introduced and randomly sampled from the clamped Normal distribution in the range over $0.1$. %$[0.1, \infty)$.
Then, we have the augmented feature vector $\hat\boldf_{I_0}$ as,
\begin{equation}
    \label{eq:aug_vec}
    \begin{aligned}
    % \begin{dcases*}
        \hat\boldf_{I_0} = \boldf_{I_0} + \alpha\cdot \mathtt{proj}(\mathbf{\Delta}_{0\to1}).
        % & if $d_c \not = d_t$,\\
        % \boldf_{I_0} + \alpha\cdot \mathbf{\Delta}_{0\to1}, & if $d_c = d_t$,
    % \end{dcases*}
    \end{aligned}
\end{equation}
For the cases having $d_t = d_c$, we can set $\mathtt{proj}(\cdot)$ operation to be an identity mapping without any learnable parameter.

% where $\mathtt{proj}(\cdot)$ stands for projection layer.
We train the target task model with this augmented feature vector, whose class label is still $T_0$. %\ie, intra-class semantic perturbation.
We note that computing difference attribute vectors with text encoder is computationally expensive. 
For efficient training, we pre-compute all possible combinations of difference vectors $\{\mathbf{\Delta}\}$ and store them in a look-up table because class names and attributes can be
% texts are
pre-determined and unchanged during training.
% Note that 






Different from knowledge distillation~\cite{dai2022enabling,wang2022multimodal,shin2022namedmask,hinton2015distilling}, $\TextMani$ does not transfer-learn the text embeddings directly. 
Instead, the difference
% attribute
vector projected onto the target domain is injected into the target model, allowing $\TextMani$ to be applied to 
% which enables us to apply $\TextMani$ to
arbitrary target models.
Since the visual feature augmentation is solely controlled by text, $\TextMani$ is human-interpretable and easily controllable.
% tractable. 
% while hallucinating similar effects. 
% A trade-off is to use a text encoder to define a visual-language joint embedding space and to estimate visually plausible attribute vectors from texts.
% , where we use the pre-trained CLIP text encoder~\cite{radford2021learning} 




% By adding the difference vector from the CLIP text embedding, $\TextMani$ induces the margin of difference vector size around the target image feature $\boldf_{I_0}$ through the interpretable residual information and densifies all the given samples in a uniform probability.
% % the sparse distribution.
% % thus, the performance of the target task model is improved due to the margin.
% Our $\TextMani$ augments the image data at the feature-level in a simple and interpretable way.

% Figure environment removed

% In comparison 
Compared to label mix-based augmentations~\cite{zhang2017mixup,verma2019manifold,yun2019cutmix}, $\TextMani$ has advantages in imbalanced data distribution.
% As depicted in \Fref{fig:vs_mix}, 
% let us
We suppose a scenario where few samples are in one class and many samples are in another class.
% two training samples are given for each of the two tail classes.
The augmented points by a mix-based method would be located only on the interpolation lines between the given samples, which limits the augmentation effects, as depicted in \Fref{fig:vs_mix}.
If we apply a mix-based method in the long-tailed class distribution cases, \ie, notably skewed distribution, 
the class imbalance problem is further aggravated, and augmentation is more biased toward major classes.
% This is because the mix-based methods augment a sample from a combination of two different class samples, where the probability of the tail classes to be sampled is decreased by square. 
% less sampled in a batch.
% selected.
% In contrast, $\TextMani$ can equally densify all the given samples.
In contrast, $\TextMani$ can equally densify all the given samples since it augments each sample independently.
% in a uniform probability.
% applies uniform effects regardless of class imbalance.
% By adding the difference vector from the CLIP text embedding, $\TextMani$ induces the margin of difference vector size around the target image feature $\boldf_{I_0}$ through the interpretable residual information and densifies all the given samples in a uniform probability.
Thus, $\TextMani$ can be used in general regardless of the imbalance factor of class distribution.



On the other hand, 
% \Fref{fig:vs_mix} suggests
in another scenario with small training data but with uniform class distribution,
% When only small data are considered, 
% In this case, 
both $\TextMani$ and mix-based methods would increase diverse combinations of samples by augmentation in respective aspects, which leads to complementary performance improvement. 
% This may hint that combining
% % We can combine 
% the two complementary methods is favorable for mild uniform class distribution. 
This will be empirically demonstrated in \Sref{sec:4}.% cases.
% can be obtained.






\subsection{Characteristics of Attribute Embedding}\label{sec3.2}
To scrutinize the relationship between the text $T_0$ and text variant $T_1$ and the attribute embedding $\mathbf{\Delta}_{0\to1}$, we visualize their distribution and discuss the characteristics.
We also visualize the difference vector to verify the hypothesis that the difference vector embeds its corresponding attribute.

% $\TextMani$ is designed upon two hypotheses: (1) the embedding vectors of variants of the class names are distributed near the original point in the text embedding, \ie, vicinal samples, and (2) the difference vector embeds the attributes.
% To verify the hypotheses, we visualize the text embedding and difference vector in this section.



% Figure environment removed


%\paragraph{Attribute Embedding vs. Text Embedding}
\paragraph{Embedding Difference vs.~Direct Text Embedding}
When guessing the difference between two
% of
texts, \eg, ``brown X'' -- ``X,'' it would be ``brown.''
Someone may think of using the text embedding directly obtained from ``brown'' instead of our attribute embedding from ``brown X'' -- ``X.''
%To verify the necessity of the difference vector, we visualize the difference  and text embeddings in \Fref{fig:tsne_color}.
To understand the difference between the two representations, we visualize the difference vectors and text embeddings with BERT and CLIP text encoder in \Fref{fig:tsne_color}.
% \oh{(the visualization of other language models can be found in the supplementary material).}
%The t-SNE plot implies that our difference vector is non-trivial in that distinctive color attributes are not represented by the embeddings directly obtained from noun color words, which are in the red circle.
While the direct text embeddings in the red circle of \Fref{fig:tsne_color} are clustered no matter with different color-texts, the difference vectors are well clustered dependent on the color.
This observation indicates that the difference vector is more effective in augmenting the visual feature space than text embedding.
% \son{The text embeddings in the red circle in \Fref{fig:tsne_color} are clustered no matter with different color-texts. 
% The difference vectors, whereas, are clustered dependent on the color.
% used to compute the difference vectors. 
% This observation strongly indicates that the difference vector is more effective way to augment the visual feature space than text embedding.}
In addition, the difference vectors obtained from the same attribute word are similarly clustered regardless of the class ``X'' but slightly different. 
It may imply our attribute embedding has subtle difference awareness on granularity according to class.
% Note that the visualized embeddings are projected ones, and the distributions of the projected and non-projected difference vectors appear similarly on feature space, which means the relationship of texts is properly transferred on the target visual feature space.

Note that \Fref{fig:tsne_color} presents difference vectors in the visual feature space, and we also observe similar distributions of difference vectors in the original text embedding space. 
This observation supports our hypothesis that general language models, \eg, BERT or GPT, have learned visual information to some extent.
It, also, demonstrates
% and
the visual information is properly transferred to the target visual feature space.

% To verify the first assumption that the embedding vectors $\bolde_{T_0}$ and $\bolde_{T_1}$, corresponding to the class names and the variant of class names, are distributed nearby in the CLIP text embedding, we
% % project data points on the 2D space and 
% visualize them with the t-SNE plot~\cite{van2008visualizing}, as shown in \Fref{fig:tsne_color}.
% The 100 class names of the CIFAR-100 dataset are used as original text inputs, and the variants are constructed by color attributes.
% We first compute all the embedding vectors of original and variant points, and then project them on the 2D space through t-SNE at once to visualize them.

% The plot shows that the variant embedding vectors $\bolde_{T_i}$ (brighter points) are distributed in the neighborhood of the original feature $\bolde_{T_0}$ (darker points), as we assumed.
% This may imply that the difference vectors induce the margin around the target image feature by extrapolating the class semantic along augmented semantic attribute axes.
% Also, we can check the distribution according to each color attribute at the enlarged part of the ``orchids'' class.
% The variant embedding vectors are distributed around the original point, and similar colors are also grouped within each class.
% % This may imply that $\TextMani$ induces the margin with difference vectors
% This may hint about the semantic granularity and its hierarchical relation between classes and attributes injected by our method, \ie, semantically meaningful margins.
% The results confirm the relationship between the variant and the original embedding vectors.


% \paragraph
\noindent\textbf{Do We Need to Rule out Unrealistic Attributes?}
% \moon{for Some Classes}}
One can be curious about how $\TextMani$ handles the unrealistic attribute, such as ``blue cow.''
We intentionally include such unrealistic attributes, motivated by other contexts in 
self- and semi-supervised learning~\cite{chen2021exploring,sohn2020fixmatch,chen2020simple}, where they showed the strong benefit of unnatural strong augmentations to train neural networks.
\moon{This observation regarding strong augmentation is consistent with the design of $\TextMani$ containing unrealistic attributes.}
% In our empirical study, this observation was consistently applied to our case.
% It 
% \son {The observation in the prior works} is consistent with our report of $\TextMani$ containing unrealistic attributes.


% \begin{wraptable}{r}{0.38\linewidth}
%     \centering
%     \vspace{-3mm}
%     \resizebox{0.8\linewidth}{!}{\scriptsize
%     \begin{tabular}{@{\,}l@{\,\,\,}c@{\,}}
%          \toprule
%          \textbf{Aug.} & \textbf{Acc.}\\
%          \midrule
%          Basic       & 31.10\\
%          Random  & 34.04\\
%          \TextMani   & \textbf{34.52}\\
%          \bottomrule
%     \end{tabular}
%     }
%     \caption{Comparison 
%     % of Top-1 accuracy (\%) 
%     to random vector addition on CIFAR-100-10\%.
%     % and class-balanced (CB) one, \ie, CIFAR-100-LT with IF=1. 
%     % with ResNet18.
%     }
%     \label{tab:noise_vec}
%     \vspace{-3mm}
% \end{wraptable}
% \paragraph{Random Words instead of Attributes}
% Our \TextMani\ is an intra-class perturbation, which seems it could be replaced with arbitrary words or noise addition.
% However, \Tref{tab:noise_vec} shows that our difference vector performs favorably to that of random attributes.


% Figure environment removed


\noindent\textbf{Does Difference Vector Embed Attribute?}
To visually understand whether attribute editing is reflected while maintaining class information, we attempt to manipulate images by changing their features with the difference vectors $\mathbf{\Delta}_{0\to1} = \bolde_{T_1} {-} \bolde_{T_0}$, \ie, we want to visualize the change effect between $\boldf_{\bI_0}$ and $\bolde_{a_1}= \boldf_{\bI_0} {+} \alpha\mathbf{\Delta}_{0\to1}$ in image domain.
% , so that we can intuitively visualize its effects.
% images manipulated 
To see the effect in image domain, we need to invert the change from $\bolde_{\bI_0}$ to $\bolde_{a_1}$ in image domain, which can be formulated as the following optimization problem, 
\begin{equation}
    \label{eq:l1_loss_naive}
    \begin{aligned}
    % \mathop{\arg\min}_{\theta}\  \| \bolde_{\bI'_0} - \bolde_{a_1} \|_1 = 
    \mathop{\arg\min}\nolimits_{\bI}\  \| E_i(\bI) - \bolde_{a_1} \|_1,
    \end{aligned}
\end{equation}
where $E_i(\cdot): \bI \rightarrow \boldf$ denotes the image encoder in \Fref{fig:textmani}.
Direct optimization in \Eref{eq:l1_loss_naive} is known to be difficult~\cite{zhu2020domain}; thus,
% a difficult optimization~\cite{zhu2020domain}.
we parameterize a given image with an image generator $G_{\theta}$ with a latent code $\bz$, \ie, $\bI(\theta) = G_{\theta}(\bz)$, which is known to ease the optimization~\cite{ulyanov2018deep}. 
Then, we can obtain the visualization by the following optimization over $\theta$
\begin{equation}
    \label{eq:l1_loss}
    \begin{aligned}
    % \mathop{\arg\min}_{\theta}\  \| \bolde_{\bI'_0} - \bolde_{a_1} \|_1 = 
    \mathop{\arg\min}\nolimits_{\theta}\  \| E_i(\bI(\theta)) - \bolde_{a_1} \|_1.
    \end{aligned}
\end{equation}
Since the goal is to see the move from $\boldf_{\bI_0} = E_i(\bI_0)$ to  $\bolde_{a_1}$, 
we initialize $\theta$ and $\bz$ such that $G_{\theta}(\bz) = \bI'_0 \simeq \bI_0$ by the GAN inversion technique~\cite{zhu2020domain}.
Note that the latent vector $\bz$, the encoders $E_i(\cdot)$, and the augmented visual embedding vector $\bolde_{a_1}$ are frozen during the optimization.
In this work, we use IC-GAN~\cite{Perarnau2016icgan} for the image generator and the text embeddings are obtained from the CLIP text encoders. Details can be found in the supplementary material.

\begin{comment}
% For this, we devise a GAN latent inversion technique motivated by Zhu~\etal\cite{zhu2020domain}.
% of such vectors through image manipulation using IC-GAN~\cite{Perarnau2016icgan}. 
% generative models. 
We test whether difference vectors are meaningful among both user-given real-world images and generated images, as shown in \Fref{fig:img_manipulation}.
% To test whether difference vectors are meaningful among real-world images (user-given) as well as generated images, we use a trained generator(ic-gan)~\cite{Perarnau2016icgan} and over-fit ic-gan for each experiment. 

% For preparation in visualizing 
To set a starting point for a user-given real-world image, 
the generator $G_{\theta}$ is trained with a given image $\bI_0$ and a fixed latent vector $\bz$, until $G_{\theta}$ is overfitted to the point where $G_{\theta}(\bz) = \bI'_0 \simeq \bI_0$.
% Given an image $\bI_0$ and a fixed latent vector $\bz$, the generator $G_{\theta}$ is trained until overfitted to the point where $G_{\theta}(\bz) = \bI'_0 \simeq \bI_0$.
When we use a generated image, we give a certain class as a condition to the generator $G_{\theta}$.
Then, we can get a visual embedding vector $\bolde_{\bI'_0}=E_i(\bI'_0)$
% to give an initial latent point
and the text embedding vectors $\bolde_{T_0}=E_t(T_0)$ and $\bolde_{T_1}=E_t(T_1)$, where $E_i(\cdot)$ and $E_t(\cdot)$ are the frozen image and text encoders, respectively.
The augmented visual embedding vector can be computed as $\bolde_{a_1} = \bolde_{\bI'_0} + \alpha\cdot \mathbf{\Delta}_{0\to1}$, where $\mathbf{\Delta}_{0\to1} = \bolde_{T_1} - \bolde_{T_0}$.
% stands for the difference vector and $\alpha$ is the weight value.
Then, we optimize the parameters of the generator $\theta$ with minimization of $L_1$ loss between the visual embedding vector $\bolde_{\bI'_0}$ and the augmented visual embedding vector $\bolde_{a_1}$ as follows:
\begin{equation}
    \label{eq:l1_loss}
    \begin{aligned}
    % \mathop{\arg\min}_{\theta}\  \| \bolde_{\bI'_0} - \bolde_{a_1} \|_1 = 
    \mathop{\arg\min}_{\theta}\  \| E_i(G_{\theta}(\bz)) - \bolde_{a_1} \|_1.
    \end{aligned}
\end{equation}
That is, we parameterize the initial image $\bolde_{\bI'_0}$ with GAN and attempt to manipulate
% by driving 
with the attribute embeded in the difference vector.
% Note that the latent vector $\bz$, the encoders $E_i(\cdot)$ and $E_t(\cdot)$, and the augmented visual embedding vector $\bolde_{a_1}$ are frozen during the optimization.
\end{comment}


Figure~\ref{fig:img_manipulation}
% of both real-world and generated images 
shows that the manipulated image reflects the added attribute, \ie,
the size of the dog is reduced by the size attribute ``small,'' and the bird becomes yellow by injecting the color attribute ``yellow.''
% \moon{Also, a single butterfly image becomes various manipulated images reflecting one of the attributes.}
% In addition, there is a result of various attributes reflected in a single butterfly image.
The manipulated results imply that 1) the difference vector indeed embeds the attributes while preserving its semantics, and 2) our augmentation on the feature space may have analogous effects to an image-level augmentation but without implementing complicated image perturbation operations.
Note that these visualizations are for analysis purposes but not for competing with any existing image manipulation methods.

% Difference vector is then measured between the feature vector of $T_0$ and the feature vector of augmented text $T_1$. 
% Using L1 loss, $\bG_{\theta}$ is guided towards generating image that has a feature similar to the augmented vector $\boldf_{a_1}$.The augmented images are shown in \Fref{fig:butterfly_diff}. 
% With pretrained $\bG_{\theta}$ and latent vector $l$, we follow the same scheme as above. The augmented images are shown in \Fref{fig:generator_diff}. 
% The augmented images show a reasonable editing result corresponding to its attribute text. This indicates the residency of semantic meanings in difference vectors.



% Given image As shown in Fig


% % Figure environment removed


