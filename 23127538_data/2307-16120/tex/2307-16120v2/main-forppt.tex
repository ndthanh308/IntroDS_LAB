% SIAM Article Template
% \documentclass[review,onefignum,onetabnum]{siamonline220329}
\documentclass[final,onefignum,onetabnum]{siamonline220329}

\usepackage{amsmath}  
\usepackage{mathrsfs}
\newtheorem{rem}{Remark}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{url}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{setspace}
% \linespread{3.0}  
\linespread{1.15}  
\usepackage{booktabs}
\usepackage{tablefootnote} % for table footnotes
\usepackage{lineno}
\usepackage{soul}
% \usepackage{graphicx}
% \usepackage{subfigure}
%\usepackage[pagewise]{lineno}


% \DeclareMathAlphabet\mathpzc{OT1}{pzc}{m}{it}
% \let\mathcal=\mathpzc
\def\E{{\mathbb E}}
\def\P{{\mathbb P}}
\def\dis{{\mathcal{D}}}
\def\pr{{\mathrm{pr}}}
\def\erf{{\mathrm{erf}}}
\let \ra = \rightarrow
\let \la = \leftarrow
%\numberwithin{equation}{section}
\newcommand\deriv[3][]{\frac{d^{#1}#2}{d{#3}^{#1}}}
\newcommand\partialderiv[3][]{\frac{\partial^{#1}#2}{\partial{#3}^{#1}}}
\def\Real{{\mathbb R}}
\def\Complex{{\mathbb C}}
\def\F{{\cal F}}
\let\<=\langle
\let\>=\rangle
%\let\^=\hat
%\def\~#1{{\-ox{\sf#1}}}
\def\~#1{{\mbox{#1}}}
\def\N{{\mathbb N}}
\def\R{{\mathbb R}}
\let\-=\mathbf
\def\sech{\mathop{\rm sech}\nolimits}
\def\Re{\mathop{\rm Re}\nolimits}
\def\Im{\mathop{\rm Im}\nolimits}
\def\bolde{{\boldsymbol\epsilon}}
\def\o#1{^{(#1)}}
\def\txtfrac#1#2{{\textstyle\frac{#1}{#2}}}
\def\circ{\ifmmode\mathchar"220E\else$\mathchar"220E$\fi}
%\catcode`\@ 13
\def\@#1{{\mathcal #1}}
\usepackage[inline]{enumitem}
\setlength{\fboxsep}{1pt}
\newcommand{\colorline}[1]{\hspace{-0.03\linewidth}\colorbox{xkcdWine!30}{\makebox[0.99\linewidth][l]{#1}}}
%\algnewcommand{\LineComment}[1]{\Statex \(\triangleright\) #1}
\newcommand \red [1]{\textcolor{red}{#1}} 

\newcommand{\billy}[1]{{\color{purple}{Billy: #1}}}
%\newcommand{\ok}[1]{{\color{blue}{#1}}}
\newcommand{\ok}[1]{{{#1}}}
\newcommand{\zqp}[1]{{\color{blue}{#1}}}

\usepackage{threeparttable}
\usepackage{diagbox}
% Packages and macros go here
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}
%\usepackage{algorithm2e}
\usepackage{algorithm}
\usepackage{algpseudocode,float}
\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi

% Prevent itemized lists from running into the left margin inside theorems and proofs
\usepackage{enumitem}
\setlist[enumerate]{leftmargin=.5in}
\setlist[itemize]{leftmargin=.5in}

% Add a serial/Oxford comma by default.
%\newcommand{\creflastconjunction}{, and~}
% Used for creating new theorem and remark environments
\newsiamremark{remark}{Remark}
\newsiamremark{hypothesis}{Hypothesis}
\crefname{hypothesis}{Hypothesis}{Hypotheses}
\newsiamthm{claim}{Claim}
\usepackage{amsmath}               
  {
      \theoremstyle{plain}
      \newtheorem{assumption}{Assumption}
  }

% Sets running headers as well as PDF title and authors
\headers{DuNets-RMA}{}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems
\thanks{Submitted to the editors 2023-06-xx.
\funding{The work of the authors was supported by the NSF of China (12101614) and NSF of Hunan (2021JJ40715). 
% xxx founder id
}
}
}

% Authors: full names plus addresses.
 \author{Qingping Zhou\thanks{School of Mathematics and Statistics, HNP-LAMA, Central South University, 932 South Lushan Rd, Hunan 410083, China, (\email{qpzhou@csu.edu.cn, jyqian@csu.edu.cn})}.
 \and Jiayu Qian\footnotemark[2]
 \and Junqi Tang\thanks{School of Mathematics, University of Birmingham, Edgbaston, Birmingham B15 2TT, UK, (\email{j.tang.2@bham.ac.uk})}.
 \and Jinglai Li\thanks{Corresponding author. School of Mathematics, University of Birmingham, Edgbaston, Birmingham B15 2TT, UK, (\email{j.li.10@bham.ac.uk})}.
 }



% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={DuNets-RMA for NIP},
  pdfauthor={Qingping Zhou, Jiayu Qian, Junqi Tang, Jinglai Li}
}
\fi
% \let\nofiles\relax  

\begin{document}
\maketitle
% \tableofcontents

% REQUIRED
\begin{abstract}
Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems.
While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. 
Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a  recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. 
We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, 
resulting in LPGD-RMA and LPD-RMA  respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, and an electrical impedance tomography problem with limited boundary measurements. In the first experiment we have observed that the improvement due to RMA largely increases with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes  can significantly improve the performance of DuNets in  strongly ill-posed  problems. 
\end{abstract}

% REQUIREDFan 
\begin{keywords}
inverse problems, deep unrolling networks, momentum acceleration, learned primal-dual, learned proximal gradient descent, recurrent neural network 
\end{keywords}

% REQUIRED
\begin{MSCcodes}
68Q25, 68R10, 68U05
\end{MSCcodes}

% \linenumbers
% --------------------------------------------------
\newpage
\section{Introduction}\label{sec:intro}
Many image processing tasks can be formulated as an invers˛˛e problem, i.e., to recover an unknown image  $x$ from indirect measurements $y$
\begin{equation}\label{eq:forward}
y=\mathcal{A} (x)+\epsilon, 
\end{equation}
where $\mathcal{A}$ represents a forward measurement operator and $\epsilon$ is the observation noise. %denotes a sample drawn from a noise distribution that may depend on the ideal measurement $\mathcal{A}(x)$.
Problems that can be formulated with Eq.~\eqref{eq:forward} include denoising~\cite{houdard2018high}, compressive sensing~\cite{zhang2018ista}, computed tomography reconstruction~\cite{adler2018learned,baguer2020computed}, phase retrieval~\cite{shechtman2015phase}, optical diffraction tomography~\cite{sung2009optical}, 
electrical impedance tomography~\cite{seo2019learning,colibazzi2022learning} and so on.

A common challenge in these problems is their ill-posedness due to measurement noise and undersampling, which indicates that finding a unique solution stably ranges from difficult to impossible without using \ok{regularization techniques}  about the problem. 
\ok{Classical regularization techniques} could obtain accurate and stable solutions by
formulating variational regularization problems and then solving them with iterative algorithms, such as the alternating direction method of multipliers (ADMM)~\cite{boyd2011distributed} and the primal-dual hybrid gradient (PDHG) method~\cite{chambolle2011first}. 
\ok{Although these methods can provide stable estimations of the true solutions, they have a number of limitations}: slow convergence, the requirement of parameter tuning, and mathematical inflexibility. To compensate for these deficiencies,~\cite{gregor2010learning,adler2017solving,adler2018learned,monga2021algorithm} modified the classical iterative scheme by replacing certain modules in the iterative procedure for variational regularization problems with deep neural networks.

Over the past few years, deep unrolling networks (DuNets), which was pioneered by Gregor and LeCun in~\cite{gregor2010learning},
have obtained great empirical success in the field of inverse problems and image processing~\cite{zhang2018ista,adler2018learned,zhang2019dynamically,hosseini2020dense,xiang2021fista,monga2021algorithm,colibazzi2022learning,tang2022accelerating}. 
Intuitively, \ok{an unrolling model} is obtained by replacing certain modules in the iterative procedure for variational regularization problems with deep neural networks.
DuNets combines traditional model-based optimization algorithms with learning-based deep neural networks, yielding an interpretable and efficient deep learning framework for 
solving inverse imaging problems. 
It should be noted that most aforementioned studies have focused on solving problem~\eqref{eq:forward} with linear or linearized forward operators. 
On the other hand, there has been relatively little research on applying DuNets to nonlinear imaging problems, such as optical diffraction tomography and electrical impedance tomography.
This paper attempts to bridge this gap 
by investigating DuNets for solving nonlinear  problems.
In such problems nonlinearity typically poses 
additional difficulty for the DuNets methods and makes their performance less effective. 
In this work we will draw on momentum acceleration (MA), a efficiency-improving strategy commonly used in optimization algorithms, and the recurrent neural networks (RNN) techniques to \ok{improve the performance} of the deep unrolling networks. 
Specifically, we propose a recurrent momentum acceleration (RMA) module that utilizes a long short-term memory recurrent neural network (LSTM-RNN) to simulate momentum acceleration accurately. The RMA module exploits the LSTM-RNN's capacity  to remember previous inputs over extended periods and learn from them, thereby providing 
information from the previous gradients. 
Compared to the classical momentum acceleration, RMA allows a more flexible formulation to incorporate the gradient history, 
%relationship between the previous gradients and the current one,
and therefore, as will be illustrated by the numerical experiments, can provide better performance than the classical methods. 
As is illustrated by the numerical experiments, RMA can substantially outperform the classical momentum acceleration models in DuNets,
which is mainly due to the fact that both the relationship between the previous update and the new gradient term and the momentum coefficients are learned from the data.
In light of this, we apply the RMA approach to the two popular DuNets: the learned proximal gradient descent (LPGD)~\cite{mardani2018neural} based upon the proximal gradient descent algorithm and the learned primal-dual (LPD)~\cite{adler2018learned} upon the hybrid gradient primal-dual algorithm, resulting in the formulation of our new LPGD-RMA and LPD-RMA approaches, respectively. %\ok{, which we will introduce in the next sections}.
It should be noted that several existing works propose to use the iteration history to improve the performance of the unrolling algorithms.
 For example,  \cite{adler2018learned} proposes to extend the state space to allow the
algorithm some ``memory'' between the iterations; 
\cite{hosseini2020dense} develops a history-cognizant unrolling of the proximal gradient descent where the outputs of all the previous regularization units
are used  for improved performance. 
As a comparison, our RMA method employs the previous gradients that are combined via a flexible RNN model. 


%becomes increasingly popular in  
%recent years~\cite{gregor2010learning,zhang2018ista,adler2018learned,zhang2019dynamically,liu2021physical,hosseini2020dense,xiang2021fista,monga2021algorithm,colibazzi2022learning,tang2022accelerating}. 
% Adler and Öktem~\cite{adler2018learned} proposed a method to create neural networks based on proximal primal-dual optimization used for computed tomographic reconstruction, which replaces proximal operators with convolutional neural networks and is trained end-to-end from raw data.
%The popularity of such method can be illustrated by the following notable examples. 
%Particularly, Gregor and LeCun~\cite{gregor2010learning} developed the learned iterative soft-thresholding algorithm, a pioneering method of unrolling algorithms, which has recently been recognized as a promising technique for %inverse problems.

%Liu et al.~\cite{liu2021physical} developed a physical model-inspired deep unrolling network for solving nonlinear inverse scattering problems.

%In addition, DuNets involve unrolling and optimizing a huge number of parameters in the deep %neural network architecture. Consequently, their efficiency, accuracy, robustness and %stability cannot be fully ensured and require further developments and studies.
 

The remainder of this paper is organized as follows. In section~\ref{sec:DuNets}, we review two widely used types of deep unrolling models: the LPGD and LPD methods. In section~\ref{sec:DuNets-RMA}, we present our RMA formulation, and incorporate it with both LPGD and LPD, yielding LPGD-RMA and LPD-RMA. 
Numerical experiments performed on two nonlinear inverse problems are reported in section~\ref{sec:exp}. 
Finally section~\ref{sec:conclusion} gives some concluding remarks.

\section{Deep unrolling networks}\label{sec:DuNets}
\ok{We start by introducing the variational optimization methods for solving inverse problems of the form~\eqref{eq:forward}}. 
\ok {These methods seek to solve the following minimization problem} by balancing a data consistency term $\mathcal{D}(\cdot,\cdot): Y \times Y \rightarrow \mathbb{R}$ against a regularization term $\@R(\cdot): X \rightarrow \mathbb{R}$:
\begin{equation}\label{eq:optim}
\underset{x \in X}{\arg \min }~
\mathcal{D}(\mathcal{A}(x), y)
+
\lambda \mathcal{R}(x),
\end{equation}
where $x\in X$, $y\in Y$ and $\lambda$ is a positive parameter balancing $\mathcal{R}$ against $\mathcal{D}$.
This regularizer often encodes the prior information on $x$ representing desired solution properties. Common regularization functions include Tikhonov regularization, total variation, wavelets, and sparsity promoting dictionary~\cite{benning2018modern}, to name a few.  Besides Tikhonov regularization, all the others are non-smooth,
and as a result  Eq.~\eqref{eq:optim} is typically solved by the first-order algorithms, such as proximal gradient descent algorithm~\cite{combettes2011proximal}, variable splitting scheme~\cite{boyd2011distributed} and primal-dual hybrid gradient (PDHG) method~\cite{chambolle2011first}. 
In order to deal with the nonsmoothness, all the aforementioned algorithms involve a rather expensive and complex updating procedure in each iteration. 
Loosely speaking, DuNets  use a learned operator represented by a deep neural network to model the iterative update of these optimization algorithms. We here focus on two archetypes of the deep unrolling models: the LPGD method with both sharing \cite{mardani2018neural,lohit2019unrolled} and  independent weights~\cite{gupta2018cnn,cherkaoui2020learning}, and the LPD method~\cite{adler2018learned}. 
%We also emphasize that although various DuNets may use the same underlying algorithm \ok{as the foundation}, they can vary greatly in their performance due to choices regarding what parameters are %learned and what safeguard precautions are used to ensure convergence. 

\subsection{Learned proximal gradient descent method}
%\billy{We should be consistent in how we address the name of the proximal gradient descent algorithm here}
%\ok{I have changed the learned proximal operator (LPO) to learned gradient descent (LPGD) throughout the entire paper.}

The proximal gradient descent (PGD) algorithm contains two steps, \textit{i.e.} gradient descent~\eqref{eq:gd} and proximal mapping~\eqref{eq:proximal}. The proximal gradient descent
starts from an initial value $x_0$ and performs the following iterates until convergence:
\begin{subequations}\label{eq:pgd}
\begin{align}
s_{t} &= x_{t-1}-\alpha_t \nabla_{x_{t-1}} \mathcal{D}(\mathcal{A}(x_{t-1}), y), \label{eq:gd} \\ 
x_{t} &= \mathcal{P}_{\lambda \@R}\left(s_{t} \right), \label{eq:proximal}
\end{align}
\end{subequations}
where $\alpha_t$ is the step size  and the proximal gradient descent $\mathcal{P}_{\lambda \@R}(\cdot)$ is defined  by
\begin{equation*}
\@P_{\lambda \@R}(x)=\underset{x^{\prime} \in X}{\arg \min }~\frac{1}{2}\left\|x^{\prime}-x\right\|_{X}^{2}+\lambda \mathcal{R} \left(x^{\prime}\right).
\end{equation*}

We follow~\cite{gupta2018cnn,cherkaoui2020learning,yang2022dynamic} to use independent parameterized neural networks $\Psi_{\theta_t}$ to replace the proximal operators $\@P_{\lambda \@R}$. %It is here a reconstruction operator 
%$\mathcal{A}_{\theta}^{\dagger}: Y \rightarrow X$ with $\theta=(\theta_1,\cdots,\theta_T)$ that is defined as $\mathcal{A}_{\theta}^{\dagger}(y)=x_{t}$ where 
Guided by recent advances in inverse problems~\cite{adler2018learned}, we allow the network to learn how to combine the previous update with the gradient update direction instead of enforcing the updates of the form 
$x_{t-1}-\alpha_t g_t$ where throughout the paper we let $g_t=\nabla_{x_{t-1}} \mathcal{D}(\mathcal{A}(x_{t-1}), y)$
for conciseness. Therefore, $x_{t}$ is given by the following
finite recursive scheme initialised by $x_{0}\in X:$
\begin{equation}\label{eq:LPGD}
x_t = \Psi_{\theta_t}(x_{t-1},g_{t-1}, y))
\qquad \text{for~~~}  t=1,\ldots,T,
%\tag{LPGD}
\end{equation}
mimicing the iteration of PGD.  The learned proximal gradient descent (LPGD) method is outlined in algorithm~\ref{alg:lpgd}. Note that the LPGD method with sharing weights, {i.e.}, restricting $\theta_1=...=\theta_T$, is another commonly used strategy
and we refer to~\cite{mardani2018neural,lohit2019unrolled} for more details.
    \begin{algorithm}[H]
          \small
          \caption{LPGD algorithm}
          \label{alg:lpgd}
            \begin{algorithmic}[1]
               \item[] \textbf{Input:} $x_0 \in X$
               \item[] \textbf{Output:} $x_T$  
               \For{$t=1,\ldots,T$}
                 \State{$g_{t-1} = \nabla_{x_{t-1}} \mathcal{D}(\mathcal{A}(x_{t-1}), y)$}
                 \State{$x_t = \Psi_{\theta_t}(x_{t-1}, g_{t-1})$}
               \EndFor
            \end{algorithmic}
    \end{algorithm}



    
\subsection{Learned primal-dual method}\label{sec:lpd} 
Adler and $\ddot{\mathrm{O}}$ktem first introduced the partially learned primal-dual approach as an extension of iterative deep neural networks in~\cite{adler2017solving} and further elaborate it into the LPD approach~\cite{adler2018learned}. %When the objective functions \eqref{eq:optim} is non-differentiable, 
The method is based on PDHG, another popular algorithm for solving the non-differentiable optimization problem \eqref{eq:optim}.
%the PDHG method is better suited for this setting since the proximal gradient descent~\eqref{eq:pgd} is sub-optimal. 
%For a given number of iterates, 
The PDHG  iteration is given by
\begin{equation}\label{eq:PDHG}
\left\{\begin{array}{l}
u_{t+1} =\operatorname{prox}_{\sigma \mathcal{A}^*}\left(u_t+\sigma \mathcal{A}\left(\bar{x}_t\right)\right) \\
x_{t+1} =\operatorname{prox}_{\tau \mathcal{R}}\left(x_t-\tau\left[\partial \mathcal{A}\left(x_t\right)\right]^*\left(u_{t+1}\right)\right) \\
\bar{x}_{t+1} =x_{t+1}+\gamma\left(x_{t+1}-x_t\right)
,\end{array} %\quad \text {for}~~~t=1, \ldots, T.
\right.
\end{equation}
where $\sigma, \tau, \gamma$ are predefined parameters, $\mathcal{A}^*$ denotes the Fenchel conjugate of $\mathcal{A}$,  and $\left[\partial \mathcal{A}\left(x_i\right)\right]^*$ is the adjoint of the (Fréchet) derivative of $\mathcal{A}$ at point $x_i$. 
The LPD method is built upon Eq.~\eqref{eq:PDHG},
and loosely the main idea is to replace 
$\operatorname{prox}_{\sigma \mathcal{A}^*}$ and $\operatorname{prox}_{\tau \mathcal{R}}$ with the parametrized neural network models, 
which are then learned from data. 
%then extend the primal and dual space, and finally allow the network %to learn how to combine the previous update with the result of the %operator evaluation. 
%The LPD (Algorithm~\ref{alg:lpd}) approach here is denoted as a reconstruction operator
%$\mathcal{A}_{\theta}^{\dagger}: Y \rightarrow X$ with $\theta=\left(\theta_{1}^{d}, \theta_{1}^{p}, \ldots, \theta_{T}^{d}, \theta_{T}^{p}\right)$ that is defined as $\mathcal{A}_{\theta}^{\dagger}%(y)=x_{T}$ where $x_{T}$ is given by the following
%finite recursive scheme initialised by $\left(x_{0}, u_{0}\right) \in X^{N_{primal}} \times Y^{N_{dual}}:$
Once the models are learned, the reconstruction proceeds via the following iterations: 
\begin{equation}\label{eq:LPD}
\left\{\begin{array}{l}
u_{t} =\Gamma_{\theta_{t}^{d}}\left(u_{t-1}, \mathcal{A}\left(x_{t-1}^{}\right), g\right) \\ x_{t} =\Lambda_{\theta_{t}^{p}}\left(x_{t-1},\left[\partial \mathcal{A}\left(x_{t-1}\right)\right]^{*}\left(u_{t}^{}\right)\right)
,\end{array} \quad \text {for}~~~ t=1, \ldots, T.\right.
\end{equation}
The complete LPD algorithm is given in Algorithm~\ref{alg:lpd}.
It is important to note that the LPD method often enlarges the state space to allow some
''memory'' between the iterations, which is omitted here and interested readers may consult \cite{adler2018learned}. 
%Here superscripts (1) and (2) denote the 1st and the 2nd channels of assigned variables. The functions $\Gamma_{\theta_{t}^{d}}: Y \times Y \times Y \rightarrow Y$ and
Also note that $\Lambda_{\theta_{t}^{p}}: X \times X \rightarrow X$ corresponds to dual and primal networks with different learned parameters but with the same architecture for each iteration. A typical initialisation is $x_{0}=0$ and $u_{0}=g$. We refer to~\cite{adler2018learned} for details on the LPD method.
    \begin{algorithm}[H]
            \small
           \caption{LPD algorithm}
           \label{alg:lpd}
            \begin{algorithmic}[1]
               \item[] \textbf{Input:} $x_0 \in X^{N_{\text{primal}}}, u_0 \in U^{N_{\text{dual}}}$  
               \item[] \textbf{Output:} $x_T^{(1)}$  
               % \item[]
               
               \For {$t=1,\ldots,T$}
                    \State $u_{t} =\Gamma_{\theta_{t}^{d}}\left(u_{t-1}, \mathcal{A}\left(x_{t-1}^{(2)}\right), y\right)$
                    \State $g_t = \left[\partial \mathcal{A}\left(x_{t-1}\right)\right]^{*}u_{t}^{(1)}$
                    \State $x_{t} =\Lambda_{\theta_{t}^{p}}\left(x_{t-1}, g_t\right)$
              \EndFor
            \end{algorithmic}
    \end{algorithm}
    
\section{Deep unrolling networks with  momentum acceleration}\label{sec:DuNets-RMA}
The conventional deep unrolling methods, exemplified by LPGD and LPD, only use the current gradient, ignoring a large amount of historical gradient data. 
As has been discussed, the methods can be improved by adopting the momentum acceleration (MA) strategies that are frequently used in optimization methods.
In this section we will provide such momentum accelerated DuNet methods. 

%In this section, we first introduce the explicit momentum acceleration (MA) and the recurrent momentum acceleration (RMA) algorithms. We then incorporate 
%the MA and RMA schemes into the LPGD and the LPD unrolling frameworks.

%\section{Momentum acceleration}\label{sec:ema}
%In this section we will provide a momentum based approach to accelerate the unrolling methods with the historical gradient information. 
%for example, for LPGD with data-fidelity $ \mathcal{D}(\mathcal{A}(x), y)=\frac12 \|\mathcal{A}(x)-y\|_{Y}^2$, the iterates are formulated as $x_t = \Psi_{\theta}(x_{t-1}-\alpha_t  g_t)$ with $g_t=\nabla_{x_{t-1}}\mathcal{D}(\mathcal{A}(x_{t-1}),y) = (\mathcal{A}(x_{t-1})-y)\nabla_{x_{t-1}}\mathcal{A}(x_{t-1}))$, where $\nabla_{x_{t-1}} \mathcal{A}(x_{t-1})$ is reevaluated at every step. 
%The reconstruction model uses only current gradient information, ignoring a large amount of valuable historical gradient data. 
%To address this issue, we propose an efficient nonlinear momentum acceleration module called RMA, depicted in Figure~\ref{fig:model} (c). In essence, RMA generates a %new direction term by utilizing historical gradient data via the LSTM-RNN model, as shown in Eq.~\eqref{eq:rma}.
% $$
% g_t' = \Xi_{\theta}
% \left(
% \nabla_{x_{0}}\mathcal{D}(\mathcal{A}(x),y),
% \ldots,
% \nabla_{x_{t-1}}\mathcal{D}(\mathcal{A}(x),y)
% \right)),
% $$
% where $\Xi_{\theta}$ is the LSTM-RNN model.

\subsection{Momentum acceleration methods}\label{sec:momentum}
We here discuss the conventional explicit MA scheme and the one based on RNN. 
\subsubsection{Explicit momentum acceleration}
Momentum-based acceleration methods, like Nesterov’s accelerated gradient~\cite{nesterov1983method} and adaptive moment estimation~\cite{adam2015}, are well-established  algorithms for speeding up the optimization procedure and have vast applications in machine learning~\cite{sutskever2013importance}. 
The classical gradient descent with MA utilizes the previous ``velocity'' $v_{t-1}$ at each iteration to perform extrapolation and generates the new update via:
\begin{subequations}
 %   \begin{equation}
    \label{eq:momentum}
\begin{align}
v_t &= \gamma v_{t-1} - \eta 
 g_{t-1} \label{eq:vtma}\\%\mathcal{D}(\mathcal{A}(x_{t-1}), y)   \\
x_t &= x_{t-1} + v_t, 
\end{align}
%\end{equation}
\end{subequations}
where $g_{t-1}$ is the gradient of the objective function evaluated at $x_{t-1}$, %=\nabla_{x_{t-1}} \mathcal{D}(\mathcal{A}(x_{t-1}), y)$, 
$\eta$ is the step size,
and $\gamma \in[0,1)$ is the momentum coefficient controlling the relative contribution of the current gradient and the previous velocity.  
%Intuitively, it first takes a step along the velocity $v_{t}$, which is the direction of the previous parameter update, and then calculate the gradient update. 
%\zqp{
%For simplicity, we denote $\nabla_{x_{t}} \mathcal{D}(\mathcal{A}(x_{t}), y)$ as $g_{t}$.
%}
Eq~\eqref{eq:momentum} can be rewritten as:
\begin{equation}\label{eq:vt}
\begin{aligned}
    v_t &=\gamma v_{t-1} -\eta g_{t-1} 
    = \gamma (\gamma v_{t-2}+\eta g_{t-2} +\eta g_{t-1})\\
    &= \ldots 
    =\gamma^t v_0 
    -\gamma^{t-1}\eta g_0
    %-\gamma^{t-2}\eta  \nabla_{x_{1}} \mathcal{D}(\mathcal{A}(x_{1}), y)
    -\ldots
    -\eta g_{t-1},
\end{aligned}
\end{equation}
which shows that the current velocity is essentially a weighted average of all the gradients (assuming $v_0=0$). 
As mentioned above, the momentum coefficient $\gamma$ controls how much information from previous iterations is used to compute the new velocity $v_t$,
and therefore needs to be chosen carefully for good performance of the method. 
However, the optimal value for the parameter is problem-specific and typically requires manual tuning~\cite{sutskever2013importance,liu2020improved}.
%It should be clear that the MA method can be directly used in DuNets:
%namely one just replaces the gradient in the standard DuNets by $v_t$ in Eq.~\eqref{eq:momentum}.  

 
\subsubsection{Momentum acceleration via RNN}
As one can see the conventional momentum method utilizes a fixed formula (a linear combination of all the gradients) to calculate the present velocity $v_t$.
In this section we introduce a more flexible scheme that uses the recurrent neural networks to learn the velocity term~\cite{hochreiter1997long}, which  we refer to as the recurrent momentum acceleration (RMA) method. 
In particular we use the long short-term memory (LSTM) based RNN, which is briefly described as follows. 
%Figure~\ref{fig:rma-lstm} outlines the schematic structure of RMA and the LSTM cell.
%RMA provides more flexibility compared to momentum acceleration, thanks to its powerful  ability to learn the temporal %dependencies in historical iterations. 
At each time step $t$ we employ a neural network to compute the ``velocity'' $v_t$.
The neural network has three inputs and three outputs:
the inputs include the current gradient input $g_{t-1}$, the cell-state $c_{t-1}$ (carrying long-memory information) and the hidden state $h_{t-1}$ (carrying short-memory information), 
where the latter two are both inherited from the previous steps,
and the outputs are the velocity $v_t$,  
$h_{t}$ and $c_t$. 
We formally write this neural network model as,  
\begin{equation}
(v_{t}, h_{t}, c_t) =\Xi_{\vartheta_t}\left(g_{t-1}, h_{t-1}, c_{-1}\right), \label{eq:rmaXi}
\end{equation}
where $\vartheta^t$ are the neural network parameters,
and leave the details of it in Appendix \ref{sec:lstm}. 
As one can see this network integrates the current gradient $g_{t-1}$ and the information from previous step $h_{t-1}$ and $c_{t-1}$
to produce the velocity $v_t$ that can be used in DuNets.  
Finally we note that other RNN models such as the Gated Recurrent Unit (GRU)~\cite{cho2014learning} can also be used here.
%is another popular RNN architecture. It couples the input and the forget gate into an update gate and merges the cell %state and the hidden state as well as some other changes. We refer to~\cite{cho2014learning} for more discussions on GRU. We also applied the GRU-RNN based momentum acceleration to the DuNets models. 
In our numerical experiments 
we have tested both LSTM and GRU, and found no significant difference between the performances of the two approaches, which is consistent with some existing works, e.g.,~\cite {chung2014empirical,greff2016lstm}. 
As such, we focus on LSTM-RNN in this work.

\subsection{LPGD and LPD with MA}
Incorporating the momentum acceleration schemes in the
DuNets algorithms is rather straightforward. In this section we use LPGD and LPD as an example,
while noting that the MA methods can be implemented in other DuNets in a similar manner. 
%This subsection provides three DuNets-RMA approaches instantiated by LPGD-RMA, LPGDSW-RMA, and LPD-RMA methods. 
%We use the term LPGD/LPGDSW/LPD-type to refer to all LPGD/LPGDSW/LPD techniques with explicit momentum acceleration (MA) and implicit momentum acceleration strategy( \textit{i.e.} recurrent momentum acceleration, RMA),~\ok{while the term ``SW" stands for shared-weights}. 
%Specifically, the classical DuNets involve LPGD/LPGDSW/LPD methods, the DuNets with explicit MA composed of %LPGD/LPGDSW/LPD-MA methods, and the DuNets with implicit MA refer to LPGD/LPGDSW/LPD-RMA methods.

% Figure environment removed

\paragraph{LPGD}
The explicit MA can be easily incorporated with LPGD.
A notable difference is that in the unrolling methods, $g_t$ is not the gradient of the objective function which 
may be  nondifferentiable or not explicitly available. 
In LPGD, $g_t$ is taken to be the gradient of the data fidelity term only, i.e.,
 $g_t = \nabla_{x_{t-1}} \mathcal{D}(\mathcal{A}(x_{t-1}), y)$. 
The main idea here is to replace $g_{t-1}$ %in $\Phi_{\theta^t}(x_{t-1},g_{t-1})$ 
in Eq.~\eqref{eq:LPGD} by $v_t$ calculated via Eq.~\eqref{eq:vtma}, yielding the LPGD-MA method~(Alg.~\ref{alg:lpgd-ma}).
Similarly by inserting the LSTM model into Alg.~\eqref{alg:lpgd}, we obtain the LPGD-RMA algorithm which is summarised in Alg.~\ref{alg:lpgd-rma}.
Finally recall that, LPGD also has a sharing weights version (referred to as LPGDSW), and correspondingly we  have LPGDSW, LPGDSW-MA, and LPGDSW-RMA,
which will also be tested in our numerical experiments. 

%\zqp{We also offer a sharing weights version, dubbed as LPGDSW, in which all stages have the same parameter, \textit{i.e.}, $\theta_{1}=\ldots=\theta_{T}$.  In this %setting, we get three counterparts of LPGD: LPGDSW, LPGDSW-MA, and LPGDSW-RMA methods.}

    \begin{algorithm}[H]
          \small
          \caption{LPGD-MA algorithm}
          \label{alg:lpgd-ma}
            \begin{algorithmic}[1]
               \item[] \textbf{Input:} $x_0 \in X, \, v_0=0$
               \item[] \textbf{Output:} $x_T$  
               \For{$t=1,\ldots,T$}
                 \State{$g_{t-1} = \nabla_{x_{t-1}} \mathcal{D}(\mathcal{A}(x_{t-1}), y)$}
                 \State{$v_t = \gamma v_{t-1} - \eta g_{t-1}$}%\nabla_{x_{t-1}} \mathcal{D}(\mathcal{A}(x_{t-1}), y)$}
                 \State{$x_t = \Psi_{\theta_t}(x_{t-1}, v_{t})$}
               \EndFor
            \end{algorithmic}
    \end{algorithm}
    
\begin{algorithm}[H]
            \small
          \caption{LPGD-RMA algorithm}
          \label{alg:lpgd-rma}
           \begin{algorithmic}[1]
               \item[] \textbf{Input:} $x_0 \,\in X, h_0=0$
               \item[] \textbf{Output:} $x_T$  
               \For{$t=1,\ldots,T$}
               \State{$g_{t-1} = \nabla_{x_{t-1}} \mathcal{D}(\mathcal{A}(x_{t-1}), y)$}
                 \State{$(v_{t},h_t) = \Xi_{\vartheta_t}( h_{t-1},g_{t-1})$}
                 \State{$x_t = \Psi_{\theta_t}(x_{t-1}, {\color{black}{{v}_{t}}})$
                 }
               \EndFor
            \end{algorithmic}
    \end{algorithm}
    

\paragraph{LPD}
The integration of the MA schemes and LPD is a bit different.
Namely in LPGD, the velocity is constructed based on the gradient of the data fidelity term,
while in LPD, we build it upon $g_{t-1} = \left[\partial \mathcal{A}\left(x_{t-1}\right)\right]^{*}u_{t}^{(1)}$. 
By inserting  the explicit MA formula~\eqref{eq:vtma} into  Alg.~\ref{alg:lpd} we obtain the LPD-MA method (Alg.~\ref{alg:lpd-ma}).
The LPD-RMA method can be constructed similarly: one simply 
replaces the explicit MA formula in Alg.~\ref{alg:lpd} by the RMA module Eq.~\eqref{eq:rmaXi},
and the complete algorithm is outlined in Alg.~\ref{alg:lpd-rma}.


   \begin{algorithm}[H]
            \small
           \caption{LPD-MA algorithm}
           \label{alg:lpd-ma}
            \begin{algorithmic}[1]
               \item[] \textbf{Input:} $x_0 \in X^{N_{\text{primal}}}, u_0 \in U^{N_{\text{dual}}}$  
               \item[] \textbf{Output:} $x_T^{(1)}$  
               % \item[]
               
               \For {$t=1,\ldots,T$}
                    \State $u_{t} =\Gamma_{\theta_{t}^{d}}\left(u_{t-1}, \mathcal{A}\left(x_{t-1}^{(2)}\right), y\right)$
                    \State $g_{t-1} = \left[\partial \mathcal{A}\left(x_{t-1}\right)\right]^{*}u_{t}^{(1)}$
                    \State $v_t = \gamma v_{t-1} - \eta g_{t-1}$
                    \State $x_{t} =\Lambda_{\theta_{t}^{p}}\left(x_{t-1}, v_t\right)$
              \EndFor
            \end{algorithmic}
    \end{algorithm}

%\begin{equation}\label{eq:LPD-momentum}
%\begin{aligned}
%v_t &= \gamma v_{t-1} - \eta \partial \mathcal{A}\left(x_{t-1}\right)^{*}u_{t}^{(1)}, \\
%x_{t}&=\Lambda_{\theta_{t}^{p}}\left(x_{t-1},v_t\right). 
%\end{aligned} 
%\tag{LPD-MA}
%\end{equation}



%Loosely speaking, one replaces the primal update step in~\eqref{eq:LPD} by the RMA module~\eqref{eq:rma}, yielding the proposed LPD-RMA method,
%\begin{equation}\label{eq:LMPD}
%x_{t}=\Lambda_{\theta_{t}^{p}}\left(x_{t-1},
%                 \Xi_{\vartheta}\left(
%                 \left[\partial\mathcal{A}\left(x_{0}\right)\right]^* u_{1}^{(1)},\ldots,\left[\partial\mathcal{A}\left(x_{t-1}\right)\right]^*u_t^{(1)}\right)\right).
%\tag{LPD-RMA}
%\end{equation}
%It is essential to mention that, instead of utilizing only the derivative, we employ the product of derivative $\partial \mathcal{A}\left(x_{t-1}\right)$ and dual %variable $u_i$.  This is because, $\partial\mathcal{A}\left(x_{t-1}\right)$ is a high-dimensional sparse function, and training such an RNN model can be %computationally expensive. To address the issue, we propose to simplify the derivatives term and reduce its dimension using the term $\left[\partial\mathcal{A}\left(x_{t-1}\right)\right]^*u_t^{(1)}$. The LPD-RMA is outlined in Algorithm~\ref{alg:lpd-rma}.

    \begin{algorithm}[H]
            \small
           \caption{LPD-RMA algorithm}
           \label{alg:lpd-rma}
            \begin{algorithmic}[1]
               \item[] \textbf{Input:} $x_0 \in X^{N_{\text{primal}}}, u_0 \in U^{N_{\text{dual}}}$ 
               \item[] \textbf{Output:} $x_T^{(1)}$  
               % \item[]
               \For{$t=1,\ldots,T$}
                 \State $u_{t} =\Gamma_{\theta_{t}^{d}}\left(u_{t-1}, \mathcal{A}\left(x_{t-1}^{(2)}\right), y\right)$
                 \State $g_{t-1} = \left[\partial \mathcal{A}\left(x_{t-1}\right)\right]^{*}u_{t}^{(1)}$
                 \State $(v_{t}, h_{t}) =\Xi_{\vartheta_t}\left(h_{t-1}, g_{t-1}\right)$
               %  {$g'_{t-1} =  \Xi_{\vartheta}(
                % \left[\partial\mathcal{A}\left(x_{0}\right)\right]^* u_{1}^{(1)},\ldots,\left[\partial\mathcal{A}\left(x_{t-1}\right)\right]^*u_t^{(1)})$}
                 
                 
                 \State $x_{t} =\Lambda_{\theta_{t}^{p}}\left(x_{t-1}, {{g'_{t-1}}}\right)$
               \EndFor
            \end{algorithmic}
    \end{algorithm}



\section{Experiments and Results}\label{sec:exp}
In this section we present our  numerical experiments on two nonlinear inverse problems: 
a nonlinear deconvolution and an electrical impedance tomography (EIT) image reconstruction.
\subsection{Implementation details}
For the LPGD-type methods, we choose the number of unrolling iterations to be $I=20$.
The outputs of the proximal operator unit are first concatenated with the estimated direction from the RMA module and then combined using convolutional layers with a $3 \times 3$ kernel size and $32$ output channels before being fed to the subsequent block. 
The learned primal subnetwork consists of two convolutional layers of kernel size $3\times3$ and output channels 32. 
The convolutional layers are followed by a parametric rectified linear units (PReLU) activation function.  \ok{The output convolutional layer is designed to match a desired number of channels and does not include any nonlinear activation functions.}
For the LPD-type methods, we let the number of unrolling iterations be $T=10$, and the number of data that persists between the iterates be $N_{primal}=5, N_{dual}=5$. The primal subnetwork $\Gamma_{\theta_i^d}$ is the same as that used in LPGD-based methods. The dual subnetwork consists of one convolutional layer of kernel size $3\times3$ and output channels 32, and the other setting is the same as the primal subnetwork. 

All networks were trained end-to-end using Adam optimizer~\cite{adam2015} to minimize the empirical loss~\eqref{eq:mseloss}. We use a learning rate schedule according to the cosine annealing, \textit{i.e.}, the learning rate in step \(t\) was
$$
\eta_{t}=\frac{\eta_{0}}{2}\left(1+\cos \left(\pi \frac{t}{t_{\max }}\right)\right),
$$
where the initial learning rate \(\eta_{0}\) was set to \(10^{-3}\). We also let the parameter \(\beta_{2}\) of the ADAM optimizer to \(0.99\) and keep all other parameters as default. We performed global gradient norm clipping~\cite{zhanggradient}, limiting the gradient norms to 1 to improve training stability. We used a batch size of 32 for the nonlinear convolution example and 1 for the electrical impedance tomography. 
\zqp{For the DuNets-MA methods, the value of $\gamma$ is initially set to 0.5 and gradually increased to 0.9 over several epochs.}
We train all models with 20 epochs and keep a set of trainable parameters that achieve minimal validation losses.
We do not enforce any constraint on the trainable parameters during training.

All experiments were run on an Intel Xeon Golden 6248 CPU and an NVIDIA Tesla V100 GPU. The nonlinear deconvolution example was run entirely on the GPU. The forward and adjoint operators in electrical impedance tomography experiments were run on the CPU rather than the GPU as the \texttt{pyEIT} toolbox required is not computationally parallelizable and runs faster on the CPU. The training duration for a single epoch was approximately 4 minutes in the case of the nonlinear convolution example utilizing 10000 training samples, and 60 minutes for the electrical impedance tomography example with 400 training samples. The code for implementing all the experiments is available at \url{https://github.com/zhouqp631/DuNets-RMA.git}.

%\paragraph{Loss function}
We use the $\ell_{2}$ loss function on the outputs from all the stages. Specifically, given the paired samples $\{x_i,y_i\}, i=1,\ldots,N$, the training objective is defined as:
\begin{equation}\label{eq:mseloss}
\mathrm{L}(\Theta)=\frac{1}{N} \sum_{i=1}^{N} \left\|\hat{x}_i-x_{i}\right\|_{l^2}^{2}.
\end{equation}
Here, $\hat{x}_i$ is the reconstruction, $\Theta$ presents the set of trainable parameters. 
%For the vanilla LPGD and its explicit momentum version, $\Theta = \{\theta_1,\ldots,\theta_T\}$; for the LPGD-RMA method, $\Theta = \{\theta_1,\ldots,\theta_T, \vartheta\}$, where $\vartheta$ denotes the trainable parameters of RMA model.
%In LPGDSW-based approaches, we have $\theta_1=\ldots=\theta_T$. Similarly, for the vanilla LPD and its explicit momentum version, $\Theta=\{\theta_{1}^{d}, \theta_{1}^{p}, \ldots, \theta_{T}^{d}, \theta_{T}^{p}\}$; for the LPD-RMA method, $\Theta=\{\theta_{1}^{d}, \theta_{1}^{p}, \ldots, \theta_{T}^{d}, \theta_{N}^{p}, \vartheta\}$, where $\vartheta$ also denotes the trainable parameters of RMA model.

\subsection{A nonlinear deconvolution problem}\label{sec:exp-deconv}
\subsubsection{Problem setting}
 We here consider a nonlinear deconvolution problem %with a nonlinear quadratic convolution through the Volterra kernels,
which is constructed largely following~\cite{zoumpourlis2017non}. %We consider a deconvolution problem, which involves recovering the parameters $x$ from the observations $y$:
For each input $\mathbf{x}=[x_{1}, x_{2}, \cdots, x_{n}]'$ consisting of $n$ elements, the forward problem is defined as
\begin{equation}\label{eq:ndp_forward}
y(\mathbf{x}) = a\cdot\mathbf{x}' \mathbf{W}_{2} \mathbf{x}+\mathbf{w}_{1}' \mathbf{x}+b.
\end{equation}
Here $\-w_1=[w_{1}^{1}, w_{1}^{2}, \cdots, w_{1}^{n}]'$ is the first-order Volterra kernel, which contains the coefficients of the Volterra series' linear part. The second-order Volterra kernel, denoted as {$\-W_2$}, is structured as follows:
\begin{equation*}
\mathbf{W}_{2}=\left[\begin{array}{cccc}w_{2}^{1,1} & w_{2}^{1,2} & \cdots & w_{2}^{1, n} \\ 0 & w_{2}^{2,2} & \cdots & w_{2}^{2, n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & w_{2}^{n, n}\end{array}\right]
\end{equation*}
The superscripts $(i,j)$ to weights $w_{2}^{i,j}$ denote correspondence to the spatial positions of the input elements $x_{i}$ and $x_{j}$ that interact.
It is important to note that the parameter $a$ controls the degree of nonlinearity in the deconvolution problem.
% Specifically the forward problem is defined as 
% \begin{equation}\label{eq:deconv}
% y(u)=\int_{\Omega} k_a\left(u-v\right) x\left(v\right) d v \stackrel{\text { def }}{=}(\mathcal{K}_a x)(u), u \in \mathbb{R},
% \end{equation}
% where $k_a$ is a second-order Volterra kernel in which $a$ controls the degree of nonlinearity. For each input patch $\mathbf{x}=[x_{1}, x_{2}, \cdots, x_{n}]'\in \mathbb{R}^{n}$,
% the Volterra kernel is defined as:
% \begin{equation}\label{eq:volterra_kernel}
% g_a(\mathbf{x}) =(\mathcal{K}_a x)(t)= a\cdot\mathbf{x}' \mathbf{W}_{2} \mathbf{x}+\mathbf{w}_{1}' \mathbf{x}+b,
% \end{equation}
% where \ok{$\-W_2$} contains the coefficients $w_{2}^{i, j}$ of the quadratic term:
% \begin{equation*}
% \mathbf{W}_{2}=\left[\begin{array}{cccc}w_{2}^{1,1} & w_{2}^{1,2} & \cdots & w_{2}^{1, n} \\ 0 & w_{2}^{2,2} & \cdots & w_{2}^{2, n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & w_{2}^{n, n}\end{array}\right]
% \end{equation*}
% and $\-w_1=[w_{1}^{1}, w_{1}^{2}, \cdots, w_{1}^{n}]'$.
% Note that superscripts $(i,j)$ to weights $w_{2}^{i,j}$ denote correspondence to the spatial positions of the input elements $x_{i}$ and $x_{j}$ that interact.

\subsubsection{Training and testing datasets}
We assume that the unknown $x$ is on 53 mesh grid points, and meanwhile we choose the nonlinear kernel with size 9 and stride 4, and the dimension of the observed data $y$ is 12. {The first-order and second-order Volterra kernels in Eq.~\eqref{eq:ndp_forward} are derived using the methods described in Section 3 of~\cite{kumar2009volterrafaces}.}
We consider four sets of experiments with different coefficients in~\eqref{eq:ndp_forward}: $a=0, 1, 2, 4$. We generate the ground truth via sampling from a total variation prior (see Chapter 3.3 in~\cite{kaipio2006statistical}), and then obtain the observation data by~\eqref{eq:ndp_forward}. We employ 12000 randomly generated paired samples, where 10000 pairs were used as training sets, and the remaining 1000/1000 pairs were used as validation/test sets.

\subsubsection{Results and discussion}
\paragraph{Benefit of the RMA scheme}
We assess the performance of RMA with the three popular unrolling methods LPGDSW~\cite{lohit2019unrolled}, LPGD~\cite{yang2022dynamic} and LPD~\cite{adler2018learned},
and in each method we implement the three different cases: without acceleration, with the conventional MA module and with the RMA module;
as such there are 9 schemes implemented in total. To keep the comparison fair, all parameters involved in the competing methods are manually tuned for optimal performance or automatically chosen as described in the references. Table~\ref{tab:toy-results} demonstrates the performance of each method in terms of the mean-square error (MSE) on four different settings: $a=0,1,2,4$.  The visual comparison can be found in Figure~\ref{fig:toy_reconstructs}. 
We summarize our findings as the following: 
\begin{enumerate}[label=(\roman*)]
  \item when $a=0$, the MSE values of each type of DuNets method are almost the same,
  which is not surprising as  RMA degenerates with constant derivatives information;
  \item when $a > 0$, DuNets-RMA methods significantly outperform the state-of-the-art methods by a large margin (\textit{e.g.}, LPD-RMA outperforms the LPD method 8.0\%, 12.0\%, and 16.0\% in terms of MSE for $a=1, 2, 4$ respectively), suggesting that including the RMA module significantly improves the performance of the deep unrolling networks, especially for the inverse problems 
  that are highly nonlinear;
  \item the connventional MA module can also improve the performance, but RMA clearly outperforms it in all the nonlinear cases; 
  \item LPD-RMA method consistently achieves the highest-quality results that closely resemble the ground truth, conforming with the quantitative assessment via MSE.
\end{enumerate}

\begin{table}[]
\centering
\caption{Quantitative results (MSE) of different DuNets methods under different $a$ values. We do not report the MSE values of the LPGD model since it does not converge. The best results are indicated in \textcolor{orange}{orange} color.}
\label{tab:toy-results}
\begin{tabular}{lclclcll}
\hline
          & $a=0$        &  & $a=1$        &  & $a=2$        &  & \multicolumn{1}{c}{$a=4$} \\ \hline \specialrule{0em}{0pt}{3pt}
LPGD       &   ---      &  &   ---      &  &     ---    & &   ---                 \\ \specialrule{0em}{3pt}{3pt}
LPGD-MA    &  3.21E-02        &  &  5.37E-02        &  &  6.49E-02        &  &  7.76E-02                     \\ \specialrule{0em}{3pt}{3pt}
LPGD-RMA   &  3.23E-02        &  &  \textcolor{orange}{3.56E-02}        &  &  \textcolor{orange}{4.43E-02}        &  &  \textcolor{orange}{4.97E-02}                     \\ \hline  
\specialrule{0em}{0pt}{3pt}

LPGDSW     &  3.01E-02        &  &   4.61E-02       &  &  5.88E-02        &  & 6.85E-02                      \\ \specialrule{0em}{3pt}{3pt}
LPGDSW-MA  &  3.02E-02        &  &   4.54E-02       &  &  5.32E-02        &  & 5.78E-02                      \\ \specialrule{0em}{3pt}{3pt}
LPGDSW-RMA &  3.01E-02        &  &   \textcolor{orange}{3.89E-02}        &  &  \textcolor{orange}{4.68E-02} &  &   \textcolor{orange}{5.24E-02}                       \\ \hline  \specialrule{0em}{0pt}{3pt}

LPD       & 2.69E-02   &  & 3.65E-02 &  & 4.61E-02 &  & 5.17E-02              \\ \specialrule{0em}{3pt}{3pt}
LPD-MA    & 2.68E-02 &  &  3.71E-02 & &4.65E-02 & &5.22E-02                              \\ \specialrule{0em}{3pt}{3pt}
LPD-RMA   & 2.67E-02 &  & \textcolor{orange}{3.35E-02} &  & \textcolor{orange}{4.04E-02}  &  & \textcolor{orange}{4.33E-02}             \\ \hline  \specialrule{0em}{0pt}{3pt}
\end{tabular}
\end{table}

% Figure environment removed



{Since the LPGDSW-type and LPGD-type approaches were significantly outperformed by the LPD methods in this example, for the following experiments in this section we only use LPD-type methods for illustration.}

\paragraph{Sensitivity of the RMA structure}
We discuss here the choice for the architecture of the RMA modules, \textit{i.e.}, the hidden layers $L$ and the hidden size $n$ of the LSTM layer.  
Since the DuNets-RMA approach only requires knowledge of the direction of the new estimation, not its magnitude, as it learns the relationship between the previous update and the new derivative term. Thus, building an extremely accurate RMA module is unnecessary.
On the other hand, the RMA may suffer from overfitting when trained using a small number of training samples which is common in nonlinear inverse problems. 
Hence, we limit the ranges of the hidden layers as $L \in \{1,2,3\}$ and the hidden size as $n \in \{30, 50, 70\}$. 
Table~\ref{tab:different-L-toy} demonstrates the results of LPD-RMA trained with different network structures in the setting where $a=1$. We observe that in all these settings the LPD-RMA yields similar results, suggesting that the algorithm is rather robust provided that the parameter values are in a reasonably range. 
%It is up to the users’ preference in choosing $L,n$, where larger $L$ or $n$, leads to more trainable parameters. 
%To reduce the risk of overfitting and, at the same time, improve the training speed, 
With extensive numerical tests, we have found that a reasonable choice of the hidden layer of RMA may be $L\in\{1, 2\}$ in moderate dimensions and 
the hidden size $n$ can be chosen to be about the same as the dimensionality of the unknown variables. We have also examined MSE for $a=2, 4$, where the results are qualitatively similar to those shown in Table~\ref{tab:different-L-toy}, 
and hence we omit those results here.

\begin{table}[]
\centering
\caption{Mean MSE values of the LPD-RMA models with $L=1,2,3$ and $n=30, 50, 70$. \ok{Evaluation is done via repeating the experiment 10 times}. The number of trainable parameters is also reported below the MSE value in parentheses.}
\label{tab:different-L-toy}
\begin{tabular}{c|lll}
\hline
\multicolumn{1}{l|}{\diagbox{$n$}{$L$}} & \multicolumn{1}{c}{1}        & \multicolumn{1}{c}{2}        & \multicolumn{1}{c}{3}        \\ \hline
30      
& \begin{tabular}[c]{@{}c@{}}3.37E-02\\(94193)\end{tabular}  
& \begin{tabular}[c]{@{}c@{}}3.36E-02\\(101363)\end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3.34E-02\\(108533)\end{tabular}   \\ \specialrule{0em}{3pt}{3pt}
50      
& \begin{tabular}[c]{@{}c@{}}3.35E-02\\(103353)\end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3.34E-02\\(121303)\end{tabular}   
& \begin{tabular}[c]{@{}c@{}}3.33E-02\\(139253)\end{tabular}  \\ \specialrule{0em}{3pt}{3pt}
70     
& \begin{tabular}[c]{@{}c@{}}3.34E-02\\(114913)\end{tabular}   
& \begin{tabular}[c]{@{}c@{}}3.35E-02\\(148443)\end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3.32E-02\\(181973)\end{tabular}     \\ 
%\specialrule{0em}{0pt}{3pt}
\hline
\end{tabular}
\end{table}

\paragraph{Sensitivity to data size}

To reveal the data efficiency of the proposed methods, we train the proposed model on different data sizes.
The data size is measured as the percentage of the total available training data, and 
the MSE results are plotted against the data size in Figure~\ref{fig:toy-differnt-datasize}. 
%which demonstrate that our LPD-RMA consistently yields the best numerical performance. Meanwhile, 
The figure shows that LPD-RMA is considerably more data efficient than LPD and LPD-MA. 
Interestingly after the data size increases to over 5\%, the use of the conventional MA module
can not improve the performance of LPD, while our LPD-RMA consistently achieves the best accuracy across the whole range.
%For example, 1\% means we trained the model with 1\% (100 data pairs) of the available training data and 1\% (10 data pairs) of the validation data. Afterwards, we tested the performance of the method on all 1000 samples of the test dataset. 
%For each data size, we chose different hyper-parameters, namely the momentum coefficient $\gamma$ and the step-size $\eta$, based on the available validation dataset. The results are depicted in Figure~\ref{fig:toy-differnt-datasize}, 

% Figure environment removed

\subsection{Electrical impedance tomography}\label{sec:exp-eit}
\subsubsection{Problem setting}
EIT is a nondestructive imaging technique that aims at reconstructing the inner conductivity distribution of a medium from a set of voltages registered on the boundary of the domain by a series of electrodes~\cite{zou2003review}. 

In this example, we consider a bounded domain $\Omega \subseteq \mathbb{R}^{2}$ with a boundary $\delta\Omega$ containing certain conducting materials whose electrical conductivity is defined by a positive spatial function $\sigma(x) \in L^{\infty}(\Omega)$.
%Suppose the homogeneous background material has a conductivity of $\sigma_0$, and then the support of %$\sigma-\sigma_0$ indicates the inhomogeneous inclusions indicated by $D$.
Next we assume that $L$ different electrical currents are injected into the boundary of $\partial \Omega$, and the resulting electrical potential should satisfy the following  governing equations with the same coefficient but different boundary conditions:
\begin{equation}\label{eq:eit_pde}
\left\{\begin{array}{ll}\nabla \cdot(\sigma \nabla u)=0 & \text { in } \Omega \\ 
u+z_{l} \sigma \frac{\partial u}{\partial e}=V_{l} & \text { on } E_{l},~~ l=1, \ldots, L \\ 
\int_{E_{l}} \sigma \frac{\partial u}{\partial e} \mathrm{~d} s=I_{l} & \text { on } \Gamma \\ 
\sigma \frac{\partial u}{\partial e}=0 & \text { on } \tilde{\Gamma}\end{array}\right.
\end{equation}
where \(\Gamma(\tilde{\Gamma})\) is the boundary \(\partial \Omega\) with (without) electrodes, $e$ is the outer normal direction at the boundary, \(V_{l}\) is the voltage to be
measured by \(l\)-th electrode \(E_{l}\) when the currents \(I_{l}\) are applied, \(z_{l}\) are the contact impedance.

 Numerically we consider the object domain $\Omega$ discretized into $n_{S}$ subdomains $\left\{\tau_{j}\right\}_{j=1}^{n_{S}}$ and
$\sigma$ is constant over each of them. 
\zqp{One injects a current at a fixed frequency through a pair of electrodes attached to the boundary and measure the voltage differences on the remaining electrode pairs. This process is repeated over all electrodes, and the resulting data is represented as a vector denoted by $Y \in \mathbb{R}^{n_{Y}}$
where $n_Y$ is the number of measurements.}
% In particular, we discretize $\Omega$ with a triangular mesh. Given an EIT medium's finite element model (FEM), we calculate the vector of voltages, $V_m$, for each FEM degree of freedom. 
We can define a mapping $F: \mathbb{R}^{n_{S}} \rightarrow \mathbb{R}^{n_{M}}$
representing the discrete version of the forward operator:
\begin{equation}\label{eq:eit_nonlinear}
V_{m}=F(\sigma)+\eta \text {, }
\end{equation}
where $\eta \in \mathbb{R}^{n_{Y}}$ is a zero-mean
Gaussian distributed measurement noise vector.

% A frame of data, denoted by v ∈ RM , is formed by rotating and repeating this process iteratively over all 16 electrodes.
Assuming the nonlinear degradation model~\eqref{eq:eit_nonlinear} and the given measurements $V_{m}$, the so-called absolute imaging problem aims to estimate the (static) conductivity $\sigma$ by solving the
following the nonlinear least squares problem
\begin{equation*}
\min~\int_{\sigma}\left(F(\sigma)-Y\right)^{2} \mathrm{~d} \Omega. 
\end{equation*}
The EIT problem is widely recognized as an exceedingly challenging problem due to its severe ill-posedness, caused by the highly non-linear dependence of the boundary currents on the conductivity.


\subsubsection{Training and Testing Datasets}\label{sec:eit-dataset}

We run numerical tests on a set of synthetic $2 \mathrm{D}$ experiments to evaluate the performance of the various methods. 
In the circular boundary ring, $L=16$ electrodes are equally spaced and located. The conductivity of the background liquid is set to be $\sigma_{0}=1.0 \Omega m^{-1}$. Measurements are simulated through opposite injection-adjacent measurement protocol via \texttt{pyEIT}~\cite{liu2018pyeit}, a Python-based framework for EIT. 
For each simulated conductivity phantom, the forward EIT problem~\eqref{eq:eit_pde} is solved using FEM with approximately 1342 triangular elements. 
 We explore the following two typical cases to test the methods:
\begin{itemize}
\item Case 1: the anomalies consist of two random circles with radii
generated from the uniform distribution ${U}(-0.6,0.6)$ and the conductivity values are 0.5 and 2 respectively in each circle; 

\item Case 2: the anomalies consist of four random circles with  radii generated according to ${U}(-0.55, 0.55)$ 
and the  conductivity values are 0.3, 0.5, 1.5, and 2.0.
\end{itemize}
We perform the unrolling methods with three different training sample sizes 50, 200 and 400, and 20 testing samples to evaluate the performance of the methods.
\subsubsection{Results and discussion}
In the numerical experiments, we use the same set of  unrolling schemes as in Section~\ref{sec:exp-deconv},
and in addition we also implement 
the regularized Gauss-Newton (GN) method   with optimally tuned parameters. 
We calculate the mean and the standard deviation of MSE of ten independent runs with different training data sizes, 
and provide the results for case 1 in Table~\ref{tab:eit2targets} and those for case 2 in Table~\ref{tab:eit-4-targets}.
%(see the 3rd column starts with 200). 
In what follows our discussion is focused on three aspects of the experimental results, 
\begin{enumerate*}[label=(\roman*)]
  \item benefit of the RMA module,
  \item behaviors in low-data regimes, %data efficient
  and
  \item robustness to the number of inclusions in the conductivity area. 
\end{enumerate*}

\paragraph{Benefit of RMA scheme} 
First we report that  that five of the ten runs of the LPGD approach fail to converge within 20 epochs in both cases and as such we omit the results of the method
in the tables. 
All the other algorithms can capture the inclusions' shape and position in all the ten runs. 
We highlight that the methods with the RMA module achieve the best performance 
in all but one case (LPGDSW method with 50 samples) where the standard MA has the best results. 
%We highlight that our RMA module demonstrates great potential to stabilize the proposed method and %deliver a better usage efficiency of the past derivatives information than the explicit momentum %acceleration. 
% Furthermore, for LPGDSW and LPD methods, if we look at the SSIM values, we observe that this improvement does not translate that well to the structural similarity. The reason is that the networks were optimized to reduce the MSE in the conductivity rather than the image structure error.
%Finally, the LPD-RMA method achieved the lowest MSE of $2.17\times 10^{-3}$, followed by the LPGD-RMA %with MSE of $2.48 \times 10^{-3}$ and LPGDSW-RMA with MSE of $2.65 \times 10^{-3}$. 
\begin{table}[!htb]
\centering
\caption{Case 1: The average MSE values of the DuNets methods under different training samples, with the associated standard deviations in parentheses. %We first compute the average MSE values of the testing samples. Then, we repeat the experiment 10 times and report its mean with one standard deviation in parentheses. 
%We excluded the LPGD model from the results because it did not converge, \zqp{i.e. the loss value increased when the number of iterations exceeded a certain threshold}, in 3, 5 and 7 out of 10 tests for models trained with 50, 200 and 400 samples, respectively.
The best MSE results are indicated in \textcolor{orange}{orange} color.
}\label{tab:eit2targets}
\begin{tabular}{llll}
\hline
   &
  \multicolumn{1}{c}{50} &
  \multicolumn{1}{c}{200} &
  \multicolumn{1}{c}{400} \\ \hline
LPGD &
  \multicolumn{1}{c}{---} &
  \multicolumn{1}{c}{---} &
  \multicolumn{1}{c}{---} \\  \specialrule{0em}{2pt}{2pt}
LPGD-MA &
  \begin{tabular}[c]{@{}c@{}}6.13E-03\\ ($\pm$ 16.1E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}5.17E-03\\ ($\pm$ 14.1E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}4.18E-03\\ ($\pm$ 10.5E-04)\end{tabular} \\  \specialrule{0em}{2pt}{2pt}
LPGD-RMA &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{3.02E-03}\\ ($\pm$ 1.34E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{2.48E-03}\\ ($\pm$ 1.08E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{2.25E-03}\\ ($\pm$ 1.41E-04)\end{tabular}  \\  \hline 

LPGDSW &
  \begin{tabular}[c]{@{}c@{}}4.33E-03\\ ($\pm$ 13.7E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2.87E-03\\ ($\pm$ 1.15E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}3.07E-03\\ ($\pm$ 1.86E-04)\end{tabular} \\  \specialrule{0em}{2pt}{2pt}
LPGDSW-MA &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{3.81E-03}\\ ($\pm$ 3.15E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2.92E-03\\ ($\pm$ 1.40E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2.95E-03\\ ($\pm$ 1.55E-04)\end{tabular}  \\ \specialrule{0em}{2pt}{2pt}
LPGDSW-RMA &
  \begin{tabular}[c]{@{}c@{}}3.92E-03\\ ($\pm$ 4.79E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{2.65E-03}\\ ($\pm$ 1.99E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{2.63E-03}\\ ($\pm$ 1.14E-04)\end{tabular} \\ \hline
LPD &
  \begin{tabular}[c]{@{}c@{}}3.25E-03\\ ($\pm$ 1.87E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2.55E-03\\ ($\pm$ 1.34E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2.35E-03\\ ($\pm$ 2.13E-04)\end{tabular}  \\ \specialrule{0em}{2pt}{2pt}
LPD-MA &
  \begin{tabular}[c]{@{}c@{}}3.29E-03\\ ($\pm$ 1.09E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2.71E-03\\ ($\pm$ 1.46E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2.44E-03\\ ($\pm$ 1.64E-04)\end{tabular}  \\ \specialrule{0em}{2pt}{2pt}
LPD-RMA &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{3.11E-03}\\ ($\pm$ 1.52E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{2.17E-03}\\ ($\pm$ 1.36E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{2.04E-03}\\ ($\pm$ 1.89E-04)\end{tabular}  \\ \hline
\end{tabular}%
\end{table}

Moreover, we present the reconstruction results of four testing samples in Figure~\ref{fig:eit-2-reconst}. It can be seen that all  approaches with the RMA module can yield rather accurate reconstruction for all the inclusions having different geometry and topology. Furthermore, the inclusions near the boundary are better recovered than those near the center,
which confirms that inclusions  far away from the boundary are more difficult to
reconstruct since the boundary data are not sensitive to them in the EIT problem~\cite{yang2020gauss,guo2021construct}.
% Figure environment removed



We then report the performance improvements of the MA and the RMA modules  in Fig.~\ref{fig:eit-2-MSE}. 
We can see that the LPGDSW-RMA/LPD-RMA approaches significantly improved over the LPGDSW/LPD and LPGDSW-MA/LPD-MA approaches. 
The proposed RMA structure provides a 7.67\% and 14.91\% improvement over the baseline in terms of MSE, respectively. However, in this test, the use of MA reduces the performance of the LPGD and LPD methods. Interestingly, we observe that MA has a varying impact on the performance of models with different numbers of training samples. For example, it improves the performance of LPGDSW but reduces the performance when there are 50 training samples.
%It is worth noting that, despite fine-tuning the parameters $\gamma$ and $\eta$ in Eq.~\eqref{eq:momentum}, it cannot be guaranteed that the optimal direction $v_t$ lies within the subspace spanned by past iterations. 
The ability of the RMA module to utilize past derivatives through nonlinear mapping may contribute to the better performance observed in the LPGDSW-RMA/LPD-RMA methods compared to the LPGDSW-MA/LPD-MA methods.
% Figure environment removed

\paragraph{Performance comparisons of the different number of training samples}
To examine the data efficiency of the proposed methods, we train the proposed network on different data sizes (see the results in Table~\ref{tab:eit2targets} and Fig.~\ref{fig:eit-2-differnt-training-number}). We manually tune the hyper-parameters ($\alpha$, $\gamma$, $K$)
for the optimal performance, where  $\alpha$ denotes the step size, $\gamma$ is the momentum weight, and $K$ denotes the number of iterations.
Table~\ref{tab:eit2targets}  shows that the LPD-type approaches perform better than LPGDSW-type and LPGD-type methods for all different numbers of training samples. 
We believe this approach is more helpful in cases where we have little training data. Besides LPGD and LPGD-MA methods, all other approaches yield higher-quality reconstructions in MSE when trained using more training data. 
It can be observed from Fig.~\ref{fig:eit-2-differnt-training-number} that the average and standard deviation of MSE of the DuNets-RMA (LPGD-RMA/LPGDSW-RMA/LPD-RMA) is reasonably smaller than that of both DuNets-MA (LPGD-MA/LPGDSW-MA/LPD-MA) and DuNets (LPGDSW/LPD) methods, 
indicating that the DuNets-RMA scheme has better stability and data efficiency.

% Figure environment removed

\paragraph{Robustness to the number of the inclusions}
To demonstrate the robustness of our methods against different numbers of inclusions, we provide the numerical and visual results in Table~\ref{tab:eit-4-targets} and Fig.~\ref{fig:eit-4-reconst} respectively. 
We conjecture that a similar result holds for Case 1 setting as well, where RMA could serve as a better learned direction term than MA when plugging into the DuNets framework, leading to a more efficient and effective unrolling model than the vallina one.

\begin{table}[!htb]
\centering
\caption{Case 2: The average MSE values of the DuNets methods under different training samples,
 with the associated standard deviations in parentheses. 
%We first compute the average MSE values of the testing samples. Then, we repeat the experiment 10 times and report its mean with one standard deviation in parentheses. We excluded the LPGD model from the results because it did not converge, \zqp{i.e. the loss value increased when the number of iterations exceeded a certain threshold}, in 5, 7, and 7 out of 10 tests for the models trained with 50, 200, and 400 samples, respectively.
The best results are indicated in \textcolor{orange}{orange} color.
}\label{tab:eit-4-targets}
\begin{tabular}{llll}
\hline
&
  \multicolumn{1}{c}{50} &
  \multicolumn{1}{c}{200} &
  \multicolumn{1}{c}{400} \\  \hline 
LPGD &
  \multicolumn{1}{c}{---} &
  \multicolumn{1}{c}{---} &
  \multicolumn{1}{c}{---} \\ \specialrule{0em}{2pt}{2pt}
LPGD-MA &
  \begin{tabular}[c]{@{}c@{}}10.3E-03\\ ($\pm$ 9.11E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}8.60E-03\\ ($\pm$ 12.5E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}8.57E-03\\ ($\pm$ 9.79E-04)\end{tabular}  \\  \specialrule{0em}{2pt}{2pt}
LPGD-RMA &
  \begin{tabular}[c]{@{}c@{}}{\textcolor{orange}{7.43E-03}}\\ ($\pm$ 1.16E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}{\textcolor{orange}{6.66E-03}}\\ ($\pm$ 1.37E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}{\textcolor{orange}{6.31E-03}}\\ ($\pm$ 1.98E-04)\end{tabular} \\ \hline

LPGDSW &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{7.81E-03}\\ ($\pm$ 3.70E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}6.88E-03\\ ($\pm$ 4.13E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}6.69E-03\\ ($\pm$ 5.59E-04)\end{tabular} \\ \specialrule{0em}{2pt}{2pt}
LPGDSW-MA &
  \begin{tabular}[c]{@{}c@{}}{7.86E-03}\\ ($\pm$ 5.04E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\textcolor{orange}{6.86E-03}\\ ($\pm$ 1.01E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}6.50E-03\\ ($\pm$ 0.96E-04)\end{tabular}  \\ \specialrule{0em}{2pt}{2pt}
LPGDSW-RMA &
  \begin{tabular}[c]{@{}c@{}}7.83E-03\\ ($\pm$ 3.42E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}{6.86E-03}\\ ($\pm$ 1.36E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}{\textcolor{orange}{6.40E-03}}\\ ($\pm$ 1.35E-04)\end{tabular}  \\ \hline \specialrule{0em}{2pt}{2pt}
  
LPD &
  \begin{tabular}[c]{@{}c@{}}7.66E-03\\ ($\pm$ 7.72E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}6.66-03\\ ($\pm$ 1.90E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}6.20E-03\\ ($\pm$ 1.71E-04)\end{tabular}\\ \specialrule{0em}{2pt}{2pt}
LPD-MA &
  \begin{tabular}[c]{@{}c@{}}7.32E-03\\ ($\pm$ 0.95E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}6.49E-03\\ ($\pm$ 5.63E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}6.18E-03\\ ($\pm$ 5.86E-04)\end{tabular}\\ \specialrule{0em}{2pt}{2pt}
LPD-RMA &
  \begin{tabular}[c]{@{}c@{}}{\textcolor{orange}{7.21E-03}}\\ ($\pm$ 1.60E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}{\textcolor{orange}{6.33E-03}}\\ ($\pm$ 1.91E-04)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}{\textcolor{orange}{6.10E-03}}\\ ($\pm$ 1.29E-04)\end{tabular}  \\  \hline
\end{tabular}%
\end{table}

% The conductivities obtained by the learned momentum method are illustrated in Fig.~\ref{fig:eit-4-reconst}.
The results in Fig.~\ref{fig:eit-4-reconst} show the robustness of the proposed methods to more inclusions. From a visual comparison of the  results, we can observe how the number of inclusions affects the quality of the reconstructions, slightly corrupting the identification of the different anomalies. 
% Figure environment removed
% --------------------------------------------------
\clearpage
\section{Conclusion}\label{sec:conclusion}
In this work, we propose the recurrent momentum acceleration (RMA) strategy for boosting the performance of the deep unrolling networks (DuNets) for nonlinear inverse problems. 
Thanks to the RMA module, which is capable of learning  from 
the historical information of the gradient of the forward model, 
the proposed method can considerably improve the performance of DuNets.
We present two numerical experiments to test 
the performance of the proposed method. 
With the numerical examples we have found that the RMA module are effective with both the LPGD and the LPD methods. 
We expect that the proposed method can be extended to other unrolling algorithms,
and useful in a wide range of nonlinear inverse problems. 

%convergence rate of the LPGD method, as \ok{10 independent runs} of LPGD-RMA consistently converge while many of the 10 %runs of LPGD fail to do so. 
%Moreover, we have extensively demonstrated the accuracy and data efficiency of the proposed method through various analyses, including the comparison against the state-of-art deep unfolding methods and traditional regularisation methods, %the data efficiency under different training samples, and the sensitivity analysis of the RMA structures.    
%Overall, our approach takes a big step towards solving the issue of stability and data efficiency, \ok{which are well-known weaknesses of DuNets}. We expect that the presented approach extends directly to, and bears great promise for, other classes of deep unrolling models for solving nonlinear inverse problems.
\appendix 
\section{The LSTM network}\label{sec:lstm}
Here we will provide a detailed description of the LSTM network used in RMA. 
Recall that in RMA, at each time $t$, a network model 
$(v_{t}, h_{t}) =\Xi_{t}\left(g_{t-1}, h_{t-1}, c_{t-1}\right)$ (where network coefficients are omitted for simplicity)
is used, and the structure of this network is specified as follows:
\begin{itemize}
\item The model $\Xi_{\vartheta_t}(\cdot)$ is a $L$-layer network with a LSTM-cell (denoted as $\mathrm{LSTM}^l(\cdot)$ for $l=1,...,L$) at each layer. 
\item Both the cell state $c_{t}$
and the hidden state $h_{t}$ have $L$ components: $c_t=(c^1_t\,,...,\,c^L_t)$ 
and $h_t=(h^1_t\,,...,\,h^L_t)$ with each component being a vector of a prescribed dimension,
and the initial states ${h}_{0}$ and $c_0$ are set to be zero.
\item For the $l$-th layer, the inputs of the LSTM cell are $g_{t}$, $c^1_{t-1}$ and $h^1_{t-1}$, 
and the output of it are  $h^1_{t}$ and an intermediate state $z^1_t$ that will be inputted into the next layer:
\begin{equation*}
 (z^{l}_t,h_{t}^{l},c_{t}^{l}) =\mathrm{LSTM}^l\left(z_t^{l-1}, h_{t-1}^{l}, c^l_{t-1}\right),
\end{equation*}
where $z_t^0$ is initialized as $g_{t}$ for the first layer $l=1$. In the final layer $l=L$, the output $h^L_{t}$ is set as the velocity $v_t$. 
% \item At the first layer $l=1$, the inputs of the LSTM cell are $g_{t}$, $c^1_{t-1}$ and $h^1_{t-1}$, 
% and the output of it are  $h^1_{t}$ and an intermediate state $z^1_t$ that will be inputted into the next layer:
% \begin{equation*}
%  (z^{l}_t,h_{t}^{l},c_{t}^{l}) =\mathrm{LSTM}^l\left(z_t^{l-1}, h_{t-1}^{l}, c^l_{t-1}\right),
% \end{equation*}
% with $z_t^0=g_{t}$;
% \item For all the intermediate layers (i.e. $1<l<L$), the inputs are $h^l_{t-1}$ and $z^{l-1}_{t}$ and the outputs are $h^l_{t}$ and $z_t^l$:
% \begin{equation*}
% ( h_{t}^{\ell},z_{t}^{\ell}) =\mathrm{LSTM}^{\ell}\left(h_{t-1}^{\ell}, z_{t}^{\ell-1}\right).
% \end{equation*}
% \item At the final layer ($l=L$), 
% the inputs are $h^L_{t-1}$ and $z^{L-1}_{t}$ and the outputs are $h^L_{t}$ and the velocity $v_t$:
% \begin{equation*}
% ( h_{t}^{L},v_{t}) =\mathrm{LSTM}^{L}\left(h_{t-1}^{L}, z_{t}^{L-1}\right).
% \end{equation*}
\end{itemize}
The structure of the LSTM network is summarised in Fig.~\ref{fig:rma-lstm} (a). 
%The input is consists of a sequence $\left(g_{0}, g_{1}, \ldots, g_{t}\right)$, which is fed into the first layer.
%The first layer takes the input sample $g_{t}$ at time $t$, and the previous hidden state $h_{t-1}^{1}$ to produce the output $z_{t}^{1}$, given its parameter $\vartheta$, as follows:
%\begin{equation*}
%z_{t}^{1}, h_{t}^{1} =\Xi_{\vartheta^{1}}\left(h_{t-1}^{1}, g_{t}\right),
%\end{equation*}
%where $\vartheta^{\ell}$ denotes the parameters of the LSTM cells for layer $\ell$, as shown in Equation~\eqref{eq:rma}.
%Any layer $\ell$ above the first layers takes the output of the layer below $y_{t}^{\ell-1}$ as its input:
%\begin{equation*}
%z_{t}^{\ell}, h_{t}^{\ell} =\Xi_{\vartheta^{\ell}}\left(h_{t-1}^{\ell}, z_{t}^{\ell-1}\right).
%\end{equation*}
%The final LSTM cell in the top layer $L$ generates the new velocity $\tilde{g}_t$. 
% Figure environment removed


We now discuss the details of the LSTM cell that  combines the input features $g_{t}$ at each time step and the inherited information from previous time steps.
%Each LSTM cell contains a cell state $c_{t}$, which serves
%as a memory  reserving information from the
%past. 
{In what follows we often omit the layer index $l$ %of $z_t^\ell, h_t^\ell, \vartheta^\ell$ and simply use $z_t, h_t, \vartheta$, 
when not causing ambiguity.} 
At each layer, the LSTM cell proceeds as follows.  
First LSTM  computes a candidate
cell state $\tilde{c}_{t}$ by combining $h^l_{t-1}$ and $z^{l-1}_{t}$ (with $z^1_t=g_{t-1}$), as:
\begin{equation*}
\tilde{c}_{t}=\tanh \left(W_{hc} h^l_{t-1}+W_{gc} z^{l-1}_{t}+b_c\right),
\end{equation*}
and it then generates a forget gate $x_{t}$, an input
gate $i_{t}$, and an output gate $o_{t}$ via the sigmoid function $\sigma(\cdot)$:
\begin{equation*}
\begin{aligned} f_{t} &=\sigma\left(W_{hx} h^l_{t-1}+W_{gx} z_t^{l-1}+b_x\right), 
\\ i_{t} &=\sigma\left(W_{hi} h^l_{t-1}+W_{gi} z_t^{l-1}+b_i\right), \\ o_{t} &=\sigma\left(W_{ho} h^l_{t-1}+W_{go} 
z_t^{l-1}+b_o\right). \end{aligned}
\end{equation*}
The forget gate is used to filter the information
inherited from $c_{t-1}$, and the input gate is used to filter
the candidate cell state at $t$. Then we compute
the cell state  $c_t$  via,
\[c_{t}=f_{t} \otimes c_{t-1}+i_{t} \otimes \tilde{c}_{t},\]
which serves as a memory reserving information from the previous iterations,
and the hidden representation $h^l_t$ as, 
%\[c_{t}=x_{t} \otimes c_{t-1}+i_{t} \otimes \tilde{c}_{t}\]
\begin{equation*}
%\begin{array}{l}
%c_{t}=x_{t} \otimes c_{t-1}+i_{t} \otimes \tilde{c}_{t}, \\ 
\quad h_{t}=o_{t} \otimes \tanh \left(c_{t}\right),
%\tilde{g}_{t-1} = W_{hg}h_t+b_g,
%\end{array}
\end{equation*}
where $\otimes$ denotes the element-wise product. 
Finally the  output of the LSTM model $v_t$ at time $t$ is calculated as:
\begin{equation*}
%\begin{array}{l}
%c_{t}=x_{t} \otimes c_{t-1}+i_{t} \otimes \tilde{c}_{t}, \\ 
%\quad h_{t}=o_{t} \otimes \tanh \left(c_{t}\right),\\
v_{t} = W_{hg}h_t+b_g,
%\end{array}
\end{equation*}
which is used to replace the gradient in the standard DuNets methods. 
%We use all historical derivatives $g_0,g_1,\ldots,g_{t-1}$ as inputs, and then obtain a new direction term:
%We can write the whole LSTM model as 
%\begin{equation}\label{eq:rma}
%    \tilde{g}_{t} = \text{LSTM}_\vartheta(g_0,g_1,\ldots,g_{t}),
%\end{equation}
%where $\vartheta=\{W_{hc}, W_{hx}, W_{hi}, W_{ho}, W_{hg}, W_{gc}, W_{gx}, W_{gi}, W_{go}, b_c, b_x, b_i, b_o, b_g\}$ are the trainable parameters. 
Note that in what is above we only provide a brief introduction to  LSTM  tailored for our purposes, and readers who are interested in more details of the method may consult~\cite{hochreiter1997long,greff2016lstm}.
% \section{The structural similarity index measure}\label{sec:ssim}
% %\subsubsection{Evaluation metrics}\label{sec:eit-metrics}
% %To fully evaluate the methods` performance, MSE and structural similarity (SSIM) are used to describe the quality of the reconstructions.
% %We denote the reconstructed conductivity as \(\sigma^{*}\) and its corresponding ground truth as \(\sigma^{G T}\).
% %MSE is defined as
% %$$
% %\mathrm{MSE}=\frac{\left\|\sigma^{G T}-\sigma^{*}\right\|_{2}^{2}}{n_{T}},
% %$$
% %  RE is given as
% % $$
% % \mathrm{RE}=\frac{\|\sigma^{GT}-{\sigma}^{*}\|_{2}}{\|\sigma^{GT}\|_{2}}.
% % $$
% Here we define SSIM following~\cite{colibazzi2022learning}.
% First the local \(S S I M_{\tau_{i}}\) measures are computed on local neighborhoods \(\mathcal{N}\left(\tau_{i}\right)\),
% for each mesh element on the entire mesh \(\Omega\). 
% In other words, for each triangle \(\tau_{i}\), the corresponding \(\mathrm{S S I M}_{\tau_{i}}\)
% is calculated on its neighborhood as
% $$
% \mathrm{S S I M}_{\tau_{i}}\left(\sigma_{i}^{G T}, \sigma_{i}^{*}\right)=\frac{\left(2 \mu_{i}^{G T} \mu_{i}^{*}+c_{1}\right)\left(2 s_{i}^{G T *}+c_{2}\right)}{\left(\left(\mu_{i}^{G T}\right)^{2}+\left(\mu_{i}^{*}\right)^{2}+c_{1}\right)\left(\left(s_{i}^{G T}\right)^{2}+\left(s_{i}^{*}\right)^{2}\right)},
% $$
% {where \(\mu_{i}^{GT}\) and  \(\mu_i^*\) denote the average values of $\sigma^{GT}$ and $\sigma^*$ in  \(\mathcal{N}\left(\tau_{i}\right)\), $s_{i}^{GT}$ and $s_i^*$ represent the associated standard deviations,  $s_{i}^{G T *}$ is defined as
% $$
% s_{i}^{G T *}=\frac{1}{\left|\mathcal{N}\left(\tau_{i}\right)\right|} \sum_{k \in \mathcal{N}\left(\tau_{i}\right)}\left(\sigma_{k}^{G T}-\mu_{k}^{G T}\right)\left(\sigma_{k}^{*}-\mu_{k}^{*}\right),
% $$
% and finally, \(c_{1}=\max(\sigma^{G T})\times0.01^{2}\) and \(c_{2}=\max(\sigma^{G T})\times0.03^{2}\).} 
% The global SSIM is then defined as:
% $$
% \mathrm{S S I M}\left(\sigma^{G T}, \sigma^{*}\right)=\frac{1}{n_{T}} \sum_{i=1}^{n_{T}} \mathrm{S S I M}_{\tau_{i}}\left(\sigma_{i}^{G T}, \sigma_{i}^{*}\right).
% $$
% %The SSIM value  varies between 0 and 1. 
% Loosely speaking, a higher SSIM value indicates a higher similarity between the two images under comparison, and it equals to 1 if they are identical.  
%and \(S S I M\) tends toward 0
%when \(\mathrm{X}\) and \(\mathrm{Y}\) are very different. 
\section*{Acknowledgments}
We are grateful to the High-Performance Computing Center of Central South University for assistance with the computations.

\bibliographystyle{siamplain}
\bibliography{main}


\end{document}








