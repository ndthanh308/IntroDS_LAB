{
  "title": "SR-R$^2$KAC: Improving Single Image Defocus Deblurring",
  "authors": [
    "Peng Tang",
    "Zhiqiang Xu",
    "Pengfei Wei",
    "Xiaobin Hu",
    "Peilin Zhao",
    "Xin Cao",
    "Chunlai Zhou",
    "Tobias Lasser"
  ],
  "submission_date": "2023-07-30T14:29:13+00:00",
  "revised_dates": [],
  "abstract": "We propose an efficient deep learning method for single image defocus deblurring (SIDD) by further exploring inverse kernel properties.  Although the current inverse kernel method, i.e., kernel-sharing parallel atrous convolution (KPAC), can address spatially varying defocus blurs, it has difficulty in handling large blurs of this kind.  To tackle this issue, we propose a Residual and Recursive Kernel-sharing Atrous Convolution (R$^2$KAC).  R$^2$KAC builds on a significant observation of inverse kernels, that is, successive use of inverse-kernel-based deconvolutions with fixed size helps remove unexpected large blurs but produces ringing artifacts.  Specifically, on top of kernel-sharing atrous convolutions used to simulate multi-scale inverse kernels, R$^2$KAC applies atrous convolutions recursively to simulate a large inverse kernel.  Specifically, on top of kernel-sharing atrous convolutions, R$^2$KAC stacks atrous convolutions recursively to simulate a large inverse kernel.  To further alleviate the contingent effect of recursive stacking, i.e., ringing artifacts, we add identity shortcuts between atrous convolutions to simulate residual deconvolutions.  Lastly, a scale recurrent module is embedded in the R$^2$KAC network, leading to SR-R$^2$KAC, so that multi-scale information from coarse to fine is exploited to progressively remove the spatially varying defocus blurs.  Extensive experimental results show that our method achieves the state-of-the-art performance.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.16242",
  "pdf_url": "https://arxiv.org/pdf/2307.16242v1",
  "comment": "Submitted to IEEE Transactions on Cybernetics on 2023-July-24",
  "num_versions": null,
  "size_before_bytes": 39610986,
  "size_after_bytes": 63732
}