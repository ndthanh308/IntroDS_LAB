% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@INPROCEEDINGS{9623288,  author={Vaidya, Sahaj and Cai, Jie and Basu, Soumyadeep and Naderi, Azadeh and Wohn, Donghee Yvette and Dasgupta, Aritra},  booktitle={2021 IEEE Visualization Conference (VIS)},   title={Conceptualizing Visual Analytic Interventions for Content Moderation},   year={2021},  volume={},  number={},  pages={191-195},  doi={10.1109/VIS49827.2021.9623288}}

 @misc{wagner_bloomberg_2021, title={Facebook says it has spent \$13 billion on safety and security efforts since 2016}, url={https://fortune.com/2021/09/21/facebook-says-it-has-spent-13-billion-on-safety-and-security-efforts-since-2016/}, journal={Fortune}, publisher={Fortune}, author={Wagner, Kurt and Bloomberg}, year={2021}, month={Sep}} 

 @article{wharton,
author = {Liu, Yi and Yildirim, Pinar and Zhang, Z. John},
title = {Implications of Revenue Models and Technology for Content Moderation Strategies},
year = {2022},
issue_date = {July-August 2022},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {41},
number = {4},
issn = {1526-548X},
url = {https://doi-org.proxy.lib.umich.edu/10.1287/mksc.2022.1361},
doi = {10.1287/mksc.2022.1361},
abstract = {This paper develops a theoretical model to study how the revenue model and technology of a social media platform affect its content-moderation strategy.This paper develops a theoretical model to study the economic incentives for a social media platform to moderate user-generated content. We show that a self-interested platform can use content moderation as an effective marketing tool to expand its installed user base, to increase the utility of its users, and to achieve its positioning as a moderate or extreme content platform. For the purpose of maximizing its own profit, a platform balances pruning some extreme content, thus losing some users, with gaining new users because of more moderate content on the platform. This balancing act plays out differently depending on whether users have to pay to join (subscription versus advertising revenue models) and on whether the technology for content moderation is perfect. We show that, when conducting content moderation optimally, a platform under advertising is more likely to moderate its content than one under subscription but does it less aggressively compared with the latter when it does. This is because a platform under advertising is more concerned about expanding its user base, whereas a platform under subscription is also concerned with users’ willingness to pay. We also show a platform’s optimal content moderation strategy depends on its technical sophistication. Because of imperfect technology, a platform may optimally throw away the moderate content more than the extreme content. Therefore, one cannot judge how extreme a platform is by just looking at its content moderation strategy. Furthermore, we show that a platform under advertising does not necessarily benefit from a better technology for content moderation, but one under subscription does as the latter can always internalize the benefits of a better technology. This means that platforms under different revenue models can have different incentives to improve their content moderation technology. Finally, we draw managerial and policy implications from our insights.},
journal = {Marketing Science},
month = {jul},
pages = {831–847},
numpages = {17},
keywords = {revenue models, content moderation, technology, social media platforms}
}

@book{gillespie2018custodians,
  title={Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media},
  author={Gillespie, Tarleton},
  year={2018},
  publisher={Yale University Press}
}

 @misc{zhang_2019, title={Stop bad content before it's posted, and build better communities}, url={https://blog.sift.com/stop-bad-content-build-better-communities/}, journal={Sift Blog}, publisher={Sift Blog}, author={Zhang, Yuchen}, year={2019}, month={Jul}} 

 @misc{davidson_2017,
  doi = {10.48550/ARXIV.1703.04009},
  
  url = {https://arxiv.org/abs/1703.04009},
  
  author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Automated Hate Speech Detection and the Problem of Offensive Language},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DADA2019e01802,
title = {Machine learning for email spam filtering: review, approaches and open research problems},
journal = {Heliyon},
volume = {5},
number = {6},
pages = {e01802},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e01802},
url = {https://www.sciencedirect.com/science/article/pii/S2405844018353404},
author = {Emmanuel Gbenga Dada and Joseph Stephen Bassi and Haruna Chiroma and Shafi'i Muhammad Abdulhamid and Adebayo Olusola Adetunmbi and Opeyemi Emmanuel Ajibuwa},
keywords = {Computer science, Computer security, Computer privacy, Analysis of algorithms, Machine learning, Spam filtering, Deep learning, Neural networks, Support vector machines, Naïve Bayes},
abstract = {The upsurge in the volume of unwanted emails called spam has created an intense need for the development of more dependable and robust antispam filters. Machine learning methods of recent are being used to successfully detect and filter spam emails. We present a systematic review of some of the popular machine learning based email spam filtering approaches. Our review covers survey of the important concepts, attempts, efficiency, and the research trend in spam filtering. The preliminary discussion in the study background examines the applications of machine learning techniques to the email spam filtering process of the leading internet service providers (ISPs) like Gmail, Yahoo and Outlook emails spam filters. Discussion on general email spam filtering process, and the various efforts by different researchers in combating spam through the use machine learning techniques was done. Our review compares the strengths and drawbacks of existing machine learning approaches and the open research problems in spam filtering. We recommended deep leaning and deep adversarial learning as the future techniques that can effectively handle the menace of spam emails.}
}

 @misc{lee_2022, title={Rules vs. machine learning: Why you need both to win: SIFT}, url={https://blog.sift.com/rules-vs-machine-learning-need-win/}, journal={Sift Blog}, publisher={Sift Blog}, author={Lee, Kevin}, year={2022}, month={Jul}} 

 @misc{deep_learning_hate_2022,
  doi = {10.48550/ARXIV.2202.09517},
  
  url = {https://arxiv.org/abs/2202.09517},
  
  author = {Malik, Jitendra Singh and Pang, Guansong and Hengel, Anton van den},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Learning for Hate Speech Detection: A Comparative Study},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{shido-etal-2022-textual,
    title = "Textual Content Moderation in {C}2{C} Marketplace",
    author = "Shido, Yusuke  and
      Liu, Hsien-Chi  and
      Umezawa, Keisuke",
    booktitle = "Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.ecnlp-1.8",
    doi = "10.18653/v1/2022.ecnlp-1.8",
    pages = "58--62",
    abstract = "Automatic monitoring systems for inappropriate user-generated messages have been found to be effective in reducing human operation costs in Consumer to Consumer (C2C) marketplace services, in which customers send messages directly to other customers.We propose a lightweight neural network that takes a conversation as input, which we deployed to a production service.Our results show that the system reduced the human operation costs to less than one-sixth compared to the conventional rule-based monitoring at Mercari.",
}

@inproceedings{lai_2022,
author = {Lai, Vivian and Carton, Samuel and Bhatnagar, Rajat and Liao, Q. Vera and Zhang, Yunfeng and Tan, Chenhao},
title = {Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3491102.3501999},
doi = {10.1145/3491102.3501999},
abstract = {Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, e.g., moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for human-AI collaboration where humans create rules to indicate trustworthy regions of a model. Using content moderation as a testbed, we develop novel interfaces to assist humans in creating conditional delegation rules and conduct a randomized experiment with two datasets to simulate in-distribution and out-of-distribution scenarios. Our study demonstrates the promise of conditional delegation in improving model performance and provides insights into design for this novel paradigm, including the effect of AI explanations.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {54},
numpages = {18},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

 @misc{kemp_ekins_2021, title={Poll: 75\% Don’t Trust Social Media to Make Fair Content Moderation Decisions, 60\% Want More Control over Posts They See}, url={https://www.cato.org/survey-reports/poll-75-dont-trust-social-media-make-fair-content-moderation-decisions-60-want-more}, journal={Cato.org}, author={Kemp, David and Ekins, Emily}, year={2021}, month={Dec}} 

  @misc{hate_alert_das_mathew_saha_2022, title={Hate speech: Detection, mitigation and beyond @aaai}, url={https://hate-alert.github.io/talk/aaai_tutorial/}, journal={Hate Alert}, author={Animesh  Mukherjee and Das, Mithun and Mathew, Binny and Saha, Punyajoy}, year={2022}, month={Feb}} 

  @misc{awasthi_2020,
  doi = {10.48550/ARXIV.2004.06025},
  
  url = {https://arxiv.org/abs/2004.06025},
  
  author = {Awasthi, Abhijeet and Ghosh, Sabyasachi and Goyal, Rasna and Sarawagi, Sunita},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning from Rules Generalizing Labeled Exemplars},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{sunyong_2021,
  doi = {10.48550/ARXIV.2106.07804},
  
  url = {https://arxiv.org/abs/2106.07804},
  
  author = {Seo, Sungyong and Arik, Sercan O. and Yoon, Jinsung and Zhang, Xiang and Sohn, Kihyuk and Pfister, Tomas},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Controlling Neural Networks with Rule Representations},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{reid_2022,
  doi = {10.48550/ARXIV.2205.09067},
  
  url = {https://arxiv.org/abs/2205.09067},
  
  author = {Pryzant, Reid and Yang, Ziyi and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Automatic Rule Induction for Interpretable Semi-Supervised Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

 @misc{tarasov_2021, title={Why content moderation costs billions and is so tricky for Facebook, Twitter, YouTube and others}, url={https://www.cnbc.com/2021/02/27/content-moderation-on-social-media.html}, journal={CNBC}, publisher={CNBC}, author={Tarasov, Katie}, year={2021}, month={Feb}} 

 @article{haimson_2021,
author = {Haimson, Oliver L. and Delmonaco, Daniel and Nie, Peipei and Wegner, Andrea},
title = {Disproportionate Removals and Differing Content Moderation Experiences for Conservative, Transgender, and Black Social Media Users: Marginalization and Moderation Gray Areas},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3479610},
doi = {10.1145/3479610},
abstract = {Social media sites use content moderation to attempt to cultivate safe spaces with accurate information for their users. However, content moderation decisions may not be applied equally for all types of users, and may lead to disproportionate censorship related to people's genders, races, or political orientations. We conducted a mixed methods study involving qualitative and quantitative analysis of survey data to understand which types of social media users have content and accounts removed more frequently than others, what types of content and accounts are removed, and how content removed may differ between groups. We found that three groups of social media users in our dataset experienced content and account removals more often than others: political conservatives, transgender people, and Black people. However, the types of content removed from each group varied substantially. Conservative participants' removed content included content that was offensive or allegedly so, misinformation, Covid-related, adult, or hate speech. Transgender participants' content was often removed as adult despite following site guidelines, critical of a dominant group (e.g., men, white people), or specifically related to transgender or queer issues. Black participants' removed content was frequently related to racial justice or racism. More broadly, conservative participants' removals often involved harmful content removed according to site guidelines to create safe spaces with accurate information, while transgender and Black participants' removals often involved content related to expressing their marginalized identities that was removed despite following site policies or fell into content moderation gray areas. We discuss potential ways forward to make content moderation more equitable for marginalized social media users, such as embracing and designing specifically for content moderation gray areas.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {466},
numpages = {35},
keywords = {misinformation, black, conservative, social media, race, content moderation, gender, marginalization, transgender, LGBTQ, hate speech, political orientation}
}

@article{juneja_2020,
author = {Juneja, Prerna and Rama Subramanian, Deepika and Mitra, Tanushree},
title = {Through the Looking Glass: Study of Transparency in Reddit's Moderation Practices},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {GROUP},
url = {https://doi-org.proxy.lib.umich.edu/10.1145/3375197},
doi = {10.1145/3375197},
abstract = {Transparency in moderation practices is crucial to the success of an online community. To meet the growing demands of transparency and accountability, several academics came together and proposed the Santa Clara Principles on Transparency and Accountability in Content Moderation (SCP). In 2018, Reddit, home to uniquely moderated communities called subreddits, announced in its transparency report that the company is aligning its content moderation practices with the SCP. But do the moderators of subreddit communities follow these guidelines too? In this paper, we answer this question by employing a mixed-methods approach on public moderation logs collected from 204 subreddits over a period of five months, containing more than 0.5M instances of removals by both human moderators and AutoModerator. Our results reveal a lack of transparency in moderation practices. We find that while subreddits often rely on AutoModerator to sanction newcomer posts based on karma requirements and moderate uncivil content based on automated keyword lists, users are neither notified of these sanctions, nor are these practices formally stated in any of the subreddits' rules. We interviewed 13 Reddit moderators to hear their views on different facets of transparency and to determine why a lack of transparency is a widespread phenomenon. The interviews reveal that moderators' stance on transparency is divided, there is a lack of standardized process to appeal against content removal and Reddit's app and platform design often impede moderators' ability to be transparent in their moderation practices.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {17},
numpages = {35},
keywords = {rules, mixed methods, content moderation, transparency, online communities, norms}
}

@inproceedings{clarke-etal-2022-one,
    title = "One Agent To Rule Them All: Towards Multi-agent Conversational {AI}",
    author = "Clarke, Christopher  and
      Peper, Joseph  and
      Krishnamurthy, Karthik  and
      Talamonti, Walter  and
      Leach, Kevin  and
      Lasecki, Walter  and
      Kang, Yiping  and
      Tang, Lingjia  and
      Mars, Jason",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.257",
    doi = "10.18653/v1/2022.findings-acl.257",
    pages = "3258--3267",
    abstract = "The increasing volume of commercially available conversational agents (CAs) on the market has resulted in users being burdened with learning and adopting multiple agents to accomplish their tasks. Though prior work has explored supporting a multitude of domains within the design of a single agent, the interaction experience suffers due to the large action space of desired capabilities. To address these problems, we introduce a new task BBAI: Black-Box Agent Integration, focusing on combining the capabilities of multiple black-box CAs at scale. We explore two techniques: question agent pairing and question response pairing aimed at resolving this task. Leveraging these techniques, we design One For All (OFA), a scalable system that provides a unified interface to interact with multiple CAs. Additionally, we introduce MARS: Multi-Agent Response Selection, a new encoder model for question response pairing that jointly encodes user question and agent response pairs. We demonstrate that OFA is able to automatically and accurately integrate an ensemble of commercially available CAs spanning disparate domains. Specifically, using the MARS encoder we achieve the highest accuracy on our BBAI task, outperforming strong baselines.",
}

@misc{sbert,
  doi = {10.48550/ARXIV.1908.10084},
  
  url = {https://arxiv.org/abs/1908.10084},
  
  author = {Reimers, Nils and Gurevych, Iryna},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{laprador,
  doi = {10.48550/ARXIV.2203.06169},
  
  url = {https://arxiv.org/abs/2203.06169},
  
  author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{bert,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{hall,
  doi = {10.48550/ARXIV.2201.11706},
  
  url = {https://arxiv.org/abs/2201.11706},
  
  author = {Hall, Melissa and van der Maaten, Laurens and Gustafson, Laura and Jones, Maxwell and Adcock, Aaron},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Systematic Study of Bias Amplification},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{contrastive,
    author={Hadsell, R. and Chopra, S. and LeCun, Y.},
    booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
    title={Dimensionality Reduction by Learning an Invariant Mapping}, 
    year={2006},
    volume={2},
    number={},
    pages={1735-1742},
    doi={10.1109/CVPR.2006.100}
}

@misc{hatexplain,
  doi = {10.48550/ARXIV.2012.10289},
  
  url = {https://arxiv.org/abs/2012.10289},
  
  author = {Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@inproceedings{cad_2021,
    title = "Introducing {CAD}: the Contextual Abuse Dataset",
    author = "Vidgen, Bertie  and
      Nguyen, Dong  and
      Margetts, Helen  and
      Rossini, Patricia  and
      Tromble, Rebekah",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.182",
    doi = "10.18653/v1/2021.naacl-main.182",
    pages = "2289--2303",
    abstract = "Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets.We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.",
}

@misc{mpnet,
  doi = {10.48550/ARXIV.2004.09297},
  
  url = {https://arxiv.org/abs/2004.09297},
  
  author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MPNet: Masked and Permuted Pre-training for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{simcse,
  doi = {10.48550/ARXIV.2104.08821},
  
  url = {https://arxiv.org/abs/2104.08821},
  
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{poletto2021resources,
  title={Resources and benchmark corpora for hate speech detection: a systematic review},
  author={Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
  journal={Language Resources and Evaluation},
  volume={55},
  number={2},
  pages={477--523},
  year={2021},
  publisher={Springer}
}

@article{al2020automatic,
  title={Automatic hate speech detection using killer natural language processing optimizing ensemble deep learning approach},
  author={Al-Makhadmeh, Zafer and Tolba, Amr},
  journal={Computing},
  volume={102},
  number={2},
  pages={501--522},
  year={2020},
  publisher={Springer}
}

@inproceedings{schmidt-wiegand-2017-survey,
    title = "A Survey on Hate Speech Detection using Natural Language Processing",
    author = "Schmidt, Anna  and
      Wiegand, Michael",
    booktitle = "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1101",
    doi = "10.18653/v1/W17-1101",
    pages = "1--10",
    abstract = "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",
}

@article{MODY2023108832,
title = {A curated dataset for hate speech detection on social media text},
journal = {Data in Brief},
volume = {46},
pages = {108832},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108832},
url = {https://www.sciencedirect.com/science/article/pii/S2352340922010356},
author = {Devansh Mody and YiDong Huang and Thiago Eustaquio {Alves de Oliveira}},
keywords = {Hate speech, Cyberhate, Cyberbullying, Natural language processing, Online Hate},
abstract = {Social media platforms have become the most prominent medium for spreading hate speech, primarily through hateful textual content. An extensive dataset containing emoticons, emojis, hashtags, slang, and contractions is required to detect hate speech on social media based on current trends. Therefore, our dataset is curated from various sources like Kaggle, GitHub, and other websites. This dataset contains hate speech sentences in English and is confined into two classes, one representing hateful content and the other representing non-hateful content. It has 451,709 sentences in total. 371,452 of these are hate speech, and 80,250 are non-hate speech. An augmented balanced dataset with 726,120 samples is also generated to create a custom vocabulary of 145,046 words. The total number of contractions considered in the dataset is 6403. The total number of bad words usually used in hateful content is 377. The text in each sentence of the final dataset, which is utilized for training and cross-validation, is limited to 180 words. The generated contractions dataset can be used for any projects in the area of NLP for data preprocessing. The augmented dataset can help to reduce the number of out-of-vocabulary words, and the hate speech dataset can be used as a classifier to detect hate or no hate on social media platforms.}
}

