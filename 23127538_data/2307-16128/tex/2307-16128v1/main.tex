\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algcompatible}
\usepackage{algorithm}
\usepackage{graphicx}
%\usepackage{algorithm,algorithmic}
\usepackage{hyperref}
\hypersetup{hidelinks=true}
\usepackage{textcomp}
\usepackage{bm}



\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normy}[1]{\left\lVert#1\right\rVert_\yy}
\newcommand{\normx}[1]{\left\lVert#1\right\rVert_\xx}

%\usepackage{amsthm}
% thm:
% \theoremstyle{plain}
\newtheorem{theorem}{\bf{Theorem}}
\newtheorem{lemma}{\bf{Lemma}}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%\renewcommand{\QED}{$\blacksquare$}
% \DeclareMathOperator{\Pr}{Pr}


% Math macros:

% \newcommand{\Pr}{\mathtext{Pr}}
\newcommand{\RR}{\mathbb{R}}
\renewcommand{\SS}{\mathcal{S}}
\newcommand{\Rt}{\mathcal{R}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\cc}{\mathbf{c}}
\newcommand{\ww}{\mathbf{w}}
\newcommand{\zz}{\mathbf{z}}
\renewcommand{\AA}{\mathbf{A}}
\newcommand{\AAt}{\mathbf{A}^\top}
\newcommand{\FF}{\mathbf{F}}
\newcommand{\FFo}{\overline{\FF}_t}
\newcommand{\vv}{\bm{\nu}}
\newcommand{\vt}{\mathbf{v}_t}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\rr}{\mathbf{r}}
\renewcommand{\gg}{\mathbf{g}}
\newcommand{\DD}{\mathbf{D}}
\newcommand{\DDt}{\tilde{\mathbf{D}}}
\newcommand{\hf}{\hat{f}}
\newcommand{\ft}{\tilde{f}}
\newcommand{\ftg}{\ft^\text{g}}
\newcommand{\xt}{\mathbf{\tilde{x}}}
\newcommand{\xh}{\mathbf{\hat{x}}}
\newcommand{\tSs}{\tilde{S}^\star}
\newcommand{\ddi}{\nabla^2d(\xx)^{-1}}
\newcommand{\nn}{\eta}

\newcommand{\R}{\texttt{Regret}}



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
%\markboth{\hskip25pc IEEE TRANSACTIONS AND JOURNALS TEMPLATE}
%{Lupien \MakeLowercase{\textit{et al.}}: Online Interior-point Methods for TIme-Varying Equality-constrained Optimization}
\begin{document}


\title{Online Interior-point Methods for Time-varying Equality-constrained Optimization}


\author{Jean-Luc Lupien, \IEEEmembership{Student Member, IEEE}, Iman Shames \IEEEmembership{Member, IEEE}, and Antoine Lesage-Landry, \IEEEmembership{Member, IEEE}
\thanks{Submitted July 2023. This work was funded by NSERC and IVADO.}
\thanks{J-L. Lupien and A. Lesage-Landry are with the Department of Electrical Engineering, Polytechnique Montréal, MILA \& GERAD, Montréal, QC, Canada, H3T 1J4. e-mail: \texttt{jean-luc.lupien@polymtl.ca}, \texttt{antoine.lesage-landry@polymtl.ca}}
\thanks{I. Shames is with CIICADA Lab in the School of Engineering at the Australian National University, Canberra, ACT, Australia. e-mail: \texttt{iman.shames@anu.edu.au}}}

\maketitle

\begin{abstract}
An important challenge in the online convex optimization (OCO) setting is to incorporate generalized inequalities and time-varying constraints. The inclusion of constraints in OCO widens the applicability of such algorithms to dynamic but safety-critical settings such as the online optimal power flow (OPF) problem. In this work, we propose the first projection-free OCO algorithm admitting time-varying linear constraints and convex generalized inequalities: the online interior-point method for time-varying equality constraints (\texttt{OIPM-TEC}). We derive simultaneous sublinear dynamic regret and constraint violation bounds for \texttt{OIPM-TEC} under standard assumptions. For applications where a given tolerance around optima is accepted, we propose a new OCO performance metric -- the epsilon-regret -- and a more computationally efficient algorithm, the epsilon \texttt{OIPM-TEC}, that possesses sublinear bounds under this metric. Finally, we showcase the performance of these two algorithms on an online OPF problem and compare them to another OCO algorithm from the literature.
\end{abstract}

\begin{IEEEkeywords}
Online convex optimization, Optimization algorithms, Time-varying systems, Machine learning
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{O}{nline} convex optimization (OCO) with time-varying constraints, henceforth referred to as constrained OCO, is an emerging field with many practical applications including network resource allocation \cite{NetworkExample}, portfolio selection~\cite{hazanOCO}, and control of electric grids \cite{OnlinePowerReview} or of multi-energy systems \cite{L1penalty}. In OCO, an agent makes sequential decisions to minimize an environment-determined loss function~\cite{hazanOCO, Shalev, zinkevich2003online}. Constrained OCO adds the requirement that the decision sequence is subject to time-varying constraints. Integrating online or time-varying constraints is essential to ensure effective implementation of OCO algorithms in safety-critical applications. The performance of a constrained OCO algorithm is measured both in terms of its regret and its constraint violation~\cite{DynamicPolyak, Shalev}. These represent the cumulative distance from optimality and from feasibility, respectively. Regret can be calculated either by comparing the decision sequence to the best fixed decision in hindsight (static regret) or to the round-optimal decision (dynamic regret) \cite{hall2015online}. The dynamic regret is a tighter metric than the static regret as a bounded dynamic regret guarantees a bounded static regret~\cite{MOSP}. When designing an OCO algorithm, the objective is to obtain sublinear regret and constraint violation bounds which imply the time-averaged regret and constraint violation will go to zero as the time-horizon goes to infinity~\cite{hazanOCO, Shalev, MOSP}. This means implemented decisions will be optimal and feasible, on average, over a long time horizon.

A promising application of constrained OCO is the online optimal power flow (OPF) problem. The OPF problem consists of finding the optimal power generation profile that minimizes costs for the operator of the electric grid while ensuring that all loads are adequately supplied and that physical and safety limits are respected~\cite{ConvexRelaxations}. This problem is non-convex and thus NP-hard to solve. For this reason, multiple convex relaxations of the OPF have been formulated to compute an approximated solution in polynomial time \cite{ConvexRelaxations, InteriorforOPF, taylor2015convex}. However, the tightest relaxations are conic relaxations that leverage generalized inequalities~\cite{taylor2015convex}. This makes solving these problems tedious and time-consuming in large-scale systems. In modern power grids, the integration of renewable energy sources introduce power fluctuations on a significantly faster timescale when compared to conventional generators~\cite{taylor2016power}. This makes the traditional approach of solving the OPF every 15-30 minutes \cite{OnlinePowerReview, ConicSurvey} unable to conform to the renewable timescale. The efficiency, adaptability, and resiliency of constrained OCO algorithms can overcome this limitation.

However, solving a conic relaxation of the OPF problem in an OCO context is challenging for multiple reasons. First and foremost is the integration of time-varying equality constraints linked to changes in power demand. Addressing the generalized inequalities associated with the conic constraints is also an obstacle. Many OCO algorithms use a projection step~\cite{MOSP, LagrangianTime-Varying, QueueConstrained} onto the feasible space defined by these constraints. This is problematic because when the feasible space is the cone of positive semi-definite (PSD) matrices or a second-order cone -- as is the case in the relaxed OPF -- the projection can be as costly as solving the offline problem. Additionally, the less stringent static regret is ill-adapted to an online OPF setting. A fixed solution satisfying the constraints at all timesteps may incur arbitrarily large losses or might not exist at all \cite{MOSP}. Therefore, a candidate OCO algorithm for the online OPF \emph{is one admitting time-varying equality constraints, being projection-free, and possessing a sublinear dynamic regret and constraint violation}. We propose an online optimization approach based on a primal interior-point method that possesses these three qualities. 



\paragraph*{Related work}

Although many constrained OCO approaches can be found in the literature, none possesses all three required properties necessary to effectively solve the online OPF. The majority of OCO algorithms that admit time-varying constraints use static regret as a performance metric. This metric uses the best fixed decision in hindsight as the comparator sequence. A common approach in this setting is to use a virtual queue to ensure a bounded static regret while satisfying long-term or slowly varying constraints. This approach is equivalent to solving a modified Lagrangian system. Virtual queues are used in \cite{JMLR, FTRLTime-Varying, OnlineConvexBandit,OnlineEpsilonProof} to achieve sublinear static regret and constraint violation bounds. The model-based augmented Lagrangian method (\texttt{MALM}) presented in \cite{LagrangianTime-Varying} which possesses sublinear regret and constraint violation bounds has been shown to experimentally outperform many of these previous methods. A projection-free OCO algorithm utilising barrier functions as regularizers is presented in \cite{HazanInterior} which achieves sublinear static regret. This approach does not admit time-varying equality constraints or a dynamic regret bound. 

Algorithms with dynamic regret bounds have also been developed to handle time-varying constraints. An online saddle-point method (\texttt{MOSP}) proposed in \cite{MOSP} and an approach using virtual queues presented in \cite{QueueConstrained} possess sublinear dynamic regret and constraint violation bounds. These bounds are, however, predicated on strict limits on objective and constraint function variations as well as time-varying stepsizes. Both these algorithms rely on a costly projection step for conic constraints. Sublinear dynamic regret and constraint violation is obtained in \cite{DynamicPolyak} even for non-convex functions. The bounds are looser than those of \cite{MOSP, QueueConstrained} and the algorithm requires a projection at every iteration. A primal-dual mirror descent approach was proposed in \cite{DistributedOnline2} for distributed OCO and analyzed using dynamic regret but the authors only obtained constraint violation bounds that grow linearly with time. A second-order method was presented in \cite{OnlineNewton} for unconstrained optimization which was extended in \cite{OPENM} to admit time-varying equality constraints. Both these approaches have tight dynamic regret bounds but are incompatible with inequality constraints.

Approaches to iteratively solve the OPF have also been presented. An algorithm based on a quasi-Newton update was used to solve the OPF in real-time in \cite{realtimeOPF}. Although this algorithm has good tracking performance under strict conditions, the authors did not derive a bound on constraint violation. A distributed feedback controller is designed in \cite{OptimalPowerFlowPursuit} but uses a linear approximation of the OPF rather than a conic relaxation. A model-free online feedback optimization algorithm is presented in~\cite{OFO}. This approach is shown to converge to optimal solutions of the OPF problem but does not have theoretical guarantees on tracking or constraint violation.

In this paper, we present the first constrained OCO algorithm capable of satisfying all three necessary conditions for effective resolution of the online OPF. This is achieved by extending the \texttt{OPEN-M} algorithm from \cite{OPENM} to integrate generalized inequalities using an interior-point method-like update. Our contributions can be summarized as follows:

\begin{itemize}
	\item We present the first projection-free OCO algorithm that admits convex generalized inequalities and time-varying linear equality constraints while achieving tight sublinear dynamic regret and constraint violation bounds.
 \item We introduce the notion of epsilon-regret in which one accepts being in an epsilon-neighbourhood of the optimal solution and propose an efficient algorithm possessing tight epsilon-regret and constraint violation bounds.
	\item We illustrate the performance of our approaches by solving an online OPF based on the 33-bus Matpower case.
	
\end{itemize}



\section{Preliminaries}

In this section, we define useful terms, lay down important assumptions, and introduce the Newton's step which will be used in the analysis of  our main results.
\subsection{Definitions}


In this work, our goal is to solve time-varying equality constrained OCO problems subject to generalized inequality constraints described in~\eqref{prob1}. The generalized inequality constraints can, for example, be defined with respect to the cone of positive semidefinite matrices or a second-order cone. Formally, let $\cc \in \RR^N$ be our objective vector with $N\in\mathbb{N}$ the dimension of the decision variable. Let $\xx_t \in \RR^N$  be the decision vector at any time $t = 0,1,2,...,T$ where $T\in\mathbb{N}$ is the time horizon. All functions $g_i(\xx) : \RR^N\mapsto \RR^{K_i}$, $i=1,2,...,I$, $I\in\mathbb{N}$, $K_i \in \mathbb{N}$ are assumed to be convex and self-concordant. Self-concordancy is defined in \eqref{def:selfconcordance} and is a common assumption in the interior-point literature \cite{boyd, renegar}. Let the matrix $\AA \in \RR^{P\times N}$ be a full row-rank matrix with $P\in \mathbb{N}$, $P<N$. Let the vector $\bb_t \in \RR^P$ be time-varying. In the sequel, the norm $\norm{\cdot}$ denotes the Euclidean norm. In summary, we tackle the following online optimization problem:

\begin{align}\label{prob1}
\begin{split}
    \min_{\xx_t}&\quad \cc^\top\xx_t\\
    \text{s.t.} \quad & g_i(\xx_t)\preceq 0, \quad \text{for } i=1,2,3,...,I\\
    & \AA \xx_t - \bb_t = 0.
    \end{split}
\end{align}
We define the optimal decision, $\xx_t^*$, at every timestep as:
\begin{align*}
    \xx_t^* \in \argmin_\xx &\quad \cc^\top\xx\\
     \text{s.t.} \quad g_i&(\xx) \preceq 0,\quad \text{for } i=1,2,3,...,I\\
     &\AA\xx-\bb_t=0.
\end{align*}

To measure the performance of constrained OCO algorithms, two metrics are employed: regret and constraint violation. These represent the cumulative distance from optimality and from feasibility, respectively. In this work, dynamic regret is used which uses the round-optimal solution $\xx_t^*$ as a comparator. This can be formally defined as:
\begin{equation}\label{def:regret}
    R_{\text{d}}(T) = \sum_{t=1}^T \cc^\top\xx_t-\cc^\top\xx_t^*.
\end{equation}
The constraint violation is null if every decision is feasible and positive otherwise. It is defined as:
\begin{equation}
    \text{Vio}(T) = \sum_{t=1}^T\left[ \sum_i\left[ \text{Tr}\Big(g_i(\xx_t)\Big)\right]^++\norm{\AA\xx_t-\bb_t}\right],
\end{equation}
where $[x]^+$ denotes the $\max \{x,0\}$ operator and Tr denotes the trace operator. We define the cumulative variation between optimal decisions $V_T$ as:
\[V_T = \sum_{t=1}^{T} \norm{\xx_t^*-\xx_{t-1}^*}.\]
Similarly, we define the cumulative variation in the right-hand term of the time-varying  equality constraint $\bb$ as: 
\[V_\bb = \sum_{t=1}^{T} \norm{\bb_t-\bb_{t-1}}.\]
Let $\norm{\cdot}_{\xx}$ denote the Hessian norm as in \cite{renegar}. For any vector $\zz \in \RR^N$ in the domain of a function $f(\xx)\in\RR^N\mapsto \RR$, the Hessian norm is:
\[\norm{\zz}_{\xx}^2 = \zz^\top \nabla^2 f(\xx)\zz.\]
The Hessian norm will be used extensively in the analysis of our proposed constrained OCO algorithms. 

An important property of functions that guarantee convergence of interior-point methods is self-concordance~\cite{boyd, renegar}. Self-concordance can be defined as all functionals for which any $\xx_1$, $\xx_2$ $\in\RR^N$ such that $\norm{\xx_2-\xx_1}_{\xx_1} < 1$, satisfy the following condition:
\begin{align}
\label{def:selfconcordance}
    \frac{\norm{\mathbf{v}}_{\xx_2}}{\norm{\mathbf{v}}_{\xx_1}} \le \frac{1}{1-\norm{\xx_2-\xx_1}_{\xx_1}},
\end{align}
where $\mathbf{v}\in\RR^N$ is a non-zero vector.
Next, we define the barrier functional $\phi(\xx)\in \mathbb{R}^N\mapsto\mathbb{R}$ as follows:
\[\phi(\xx) = \sum_{i=1}^I -\log\big(-g_i(\xx)\big).\]
This barrier functional is convex and self-concordant over its domain~\cite{boyd}.


Interior-point methods do not attempt to solve \eqref{prob1} directly. Instead, a sequence of convex optimization problems related to the original problem are solved. This is achieved in the following way. Let $\nn\in \mathbb{R}^+$ be a positive barrier parameter. This barrier parameter is useful in defining the following equality-constrained problem:

\begin{align}\label{prob2}\begin{split}\min_{\xx_t}\quad &\nn\cc^\top\xx_t+\phi(\xx_t)\\
\text{s.t.} \quad& \AA \xx_t - \bb_t = 0.
\end{split}
\end{align} Problem \eqref{prob2} is of interest because when $\nn \rightarrow \infty$, the solutions of \eqref{prob1} and \eqref{prob2} coincide. The strategy is, therefore, to incrementally increase $\nn$, iteratively solving the original problem.

In the sequel, we use the following notation:
\begin{equation}\label{defd}d(\xx_t, \nn) \stackrel{\text{def}}{=} \nn \cc^\top\xx_t+\phi(\xx_t).\end{equation}
This represents the objective function we seek to minimize at every timestep $t$ given a barrier parameter $\nn$.

Finally, we denote the feasible set by $D_f \stackrel{\text{def}}{=} \left\{\xx \in \RR^N | \sum_i g_i(\xx) \preceq 0, \AA\xx-\bb_t=0\right\}$.



\subsection{Assumptions}
Certain sufficient conditions need to be met to ensure the regret and constraint violation suffered by the algorithms are sublinear. These are standard in the interior-point literature \cite{InteriorforOPF} and will hold for any convex relaxation of the OPF problem \cite{ConvexRelaxations}.

\begin{enumerate}
    

    \item The relative interior of the feasible set $D_f$ forms a closed and convex set.
    \item The feasible set $D_f$ is non-empty. This implies there exists at least one feasible point and that Slater's condition holds.

    \item The barrier functional $\phi(\xx)$ is a self-concordant barrier functional of complexity $v_f\in\RR^+$. That is,
    \begin{align}\label{def:sccomplexity}\begin{split}
        \sup_\xx\quad \norm{\nabla^2\phi^{-1}(\xx)\nabla\phi(\xx)}^2_\xx&\le v_f.
        \end{split}
    \end{align}

    The complexity of standard barrier functionals can be reasonably bounded by the dimension of the decision space, i.e., $v_f=O(N)$ \cite{boyd}. For example, the barrier functionals used for the space of positive semi-definite (PSD) matrices, second-order cones, quadratic functions, and linear functions all satisfy \eqref{def:sccomplexity}~\cite[Section 2.3]{renegar}. 

\end{enumerate}

\subsection{Newton Step}

To define the Newton's step, we use the following formalism:
\begin{align*}\yy_t &= \begin{bmatrix}\xx_t\\ \vv_t\end{bmatrix}\\
r_t(\yy,\nn) &= \begin{bmatrix}
    \nabla d(\xx_t, \nn) + \AA^\top \vv\\
    \AA \xx_t -\bb_t
\end{bmatrix}\\
\DD(\yy_t) &= \begin{bmatrix}
    \nabla^2 d(\xx_t) & \AA^\top\\
    \AA& 0
\end{bmatrix} = \begin{bmatrix} \nabla^2 \phi(\xx_t) & \AA^\top\\ \AA& 0\end{bmatrix}.\end{align*}
We notice that $\DD$ is the gradient of $r$ and is independent of both time $t$ and barrier parameter $\nn$.

The infeasible Newton's step, $n_t(\yy_t,\nn)$, at round $t$ associated to a barrier parameter $\nn$ is defined as:
\[n_t(\yy_t,\nn)=\Delta \yy_t = -\DD(\yy_t)^{-1}r_t(\yy_t,\nn),\]
where $t$ denotes the current round.

We now present a sequence of lemmas that will later be used to to prove our main results in Section \ref{algos}.

\begin{lemma}
    Consider the following functional: 
    \[\Gamma(\yy) = \Gamma(\xx, \vv) = d(\xx)+\vv^\top\AA\xx - \vv^\top\bb_t.\]
    Then $\Gamma$ is self-concordant and $\nabla \Gamma(\yy) = r(\yy)$ and $\nabla^2\Gamma(\yy) = \DD(\yy)$.
\end{lemma}
\begin{proof}
    The third derivative of $\Gamma(\yy)$ coincides with the third derivative of $d(\xx)$ having no dependency in $\vv$. Because the third derivative of $d(\xx)$ is bounded by assumption, this provides a bound on the third derivative of $\Gamma(\yy)$. By \cite[Section~2.5]{renegar}, we have that $\Gamma(\yy)$ is self-concordant.
\end{proof}


\begin{lemma}\label{invHess}
    Let $\Gamma$ be a self-concordant function. The following holds:
    \[\norm{\DD^{-1}(\yy_1)\DD(\yy_2)}_{\yy_1}\le \frac{1}{(1-\norm{\yy_1-\yy_2}_{\yy_1})^2}.\]
\end{lemma}

\begin{proof}
    The inequalities follow directly from the definition of self-concordance \cite{renegar}. We have:
    \begin{align*}
        \norm{\DD^{-1}(\yy_1)\DD(\yy_2)}_{\yy_1} &= \max_{\mathbf{v}}\frac{\mathbf{v}^\top\DD(\yy_2)\DD^{-1}(\yy_1)\DD(\yy_2)\mathbf{v}}{\norm{\mathbf{v}}_{\yy_1}^2}\\
        =& \max_{\mathbf{v}} \frac{\norm{\mathbf{v}}^2_{\yy_2}}{\norm{\mathbf{v}}^2_{\yy_1}} \le \left(\frac{1}{1-\norm{\yy_2-\yy_1}_{\yy_1}}\right)^2,
    \end{align*}
    and we have established the inequality.
\end{proof}

\begin{lemma}
    If $\exists m \in \mathbb{R}$ such that $\norm{\nabla^2\phi(\xx)}\ge m$, for all $\xx\in D_f$ then,
    \[\norm{\DD^{-1}(\yy)} \le \frac{1}{m}\quad \forall \yy \quad\text{\normalfont s.t. } \xx\in D_f.\]
\end{lemma}
\begin{proof}
    By construction we have:
    \begin{align*}
    \norm{\DD(\yy)} \ge &\norm{\nabla^2 \phi(\xx)} \ge m\quad \forall \yy,\xx,\\
\end{align*}
and the result follows from \cite[Lemma 1]{OnlineNewton}.
\end{proof}


The dual-feasible nature of every point on the central path provides an estimation of the error for each iteration. Defining the next iterate as $\yy^+ =\yy + n_t(\yy,\nn)$, Lemma~\ref{lem:nred} formalizes this concept. 
\begin{lemma}
\label{lem:nred}
    For any pair $\{\yy,\nn\}$ such that $\normy{n_t(\yy,\nn)}\le 1$, we have:
    \begin{equation}\label{normreduction}
\norm{n_t(\yy^+,\nn)}_{\yy^+} \le \frac{\norm{n_t(\yy,\nn)}^2_\yy}{(1-\norm{n_t(\yy,\nn)}_\yy)^2}.\end{equation}
    
\end{lemma}

\begin{proof}
From the definition of the Newton's step, we have:
\[\norm{n_t(\yy, \nn)}_{\yy}^2 = r_t(\yy,\nn)^\top\DD(\yy)^{-1}r_t(\yy,\nn).\]
We can now express $\norm{n_t(\yy^+,\nn)}_{\yy^+}$ as:
\begin{align*}
    &\norm{n_t(\yy^+,\nn)}^2_{\yy^+}\\ &\quad= r_t(\yy^+,\nn)^\top\DD(\yy^+)^{-1}r_t(\yy^+,\nn)\\
    &\quad= r_t(\yy^+,\nn)^\top\DD(\yy)^{-1}\DD(\yy)\DD(\yy^+)^{-1}r_t(\yy^+,\nn)\\
    &\quad\le \norm{\DD(\yy)\DD(\yy^+)^{-1}}_{\yy}r_t(\yy^+,\nn)^\top\DD(\yy)^{-1}r_t(\yy^+,\nn)\\
    &\quad= \norm{\DD(\yy)\DD(\yy^+)^{-1}}_{\yy} \norm{\DD(\yy)^{-1}r_t(\yy^+,\nn)}^2_{\yy},
\end{align*}
where $\DD(\yy)^\top = \DD(\yy)$ because $\DD$ is symmetric.
Using Lemma \ref{invHess}, we can bound the first term by:
\begin{align*}
    \norm{\DD(\yy)\DD(\yy^+)^{-1}}_{\yy} &\le \frac{1}{(1-\norm{\yy^+-\yy}_\yy)^2}\\
    &= \frac{1}{(1-\norm{n_t(\yy,\nn)}_\yy)^2.}
\end{align*}
Therefore, we have that:
\begin{equation}\label{eq:2}\norm{n_t(\yy^+,\nn)}_{\yy^+}\le \frac{\norm{\DD(\yy)^{-1}r_t(\yy^+,\nn)}_\yy}{1-\norm{n_t(\yy,\nn)}_\yy}.\end{equation}
We now bound the numerator of \eqref{eq:2}. We have:
\begin{align*}
    &\norm{\DD(\yy)^{-1}r_t(\yy^+,\nn)}_\yy\nonumber\\
    &\quad=\norm{\DD(\yy)^{-1}r_t(\yy^+,\nn)-\DD(\yy)^{-1}r_t(\yy,\nn)-n_t(\yy,\nn)}_\yy\nonumber\\ %\label{eq1}
    &\quad\le \int_0^1 \normy{\DD(\yy)^{-1}\DD\big(\yy-\tau n_t(\yy,\nn)\big)}\normy{n_t(\yy,\nn)}\text{d}\tau \nonumber\\ \nonumber&\quad\quad\quad\quad+ \int_0^1\normy{n_t(\yy,\nn)}\text{d}\tau.\end{align*} Using Lemma 2 yields: \begin{align} %\label{eq2}
    \norm{\DD(\yy)^{-1}r_t(\yy^+,\nn)}_\yy&\le \int_0^1 \frac{\normy{n_t(\yy,\nn)}}{(1-\tau\normy{n_t(\yy,\nn)})^2}\text{d}\tau\nonumber\\ &\quad\quad\quad\quad\quad\quad\quad\quad
    + \normy{n_t(\yy,\nn)}\nonumber\\
    &= \left[\frac{-1}{1-\tau\normy{n_t(\yy,\nn)}}\right]_0^1+\normy{n_t(\yy,\nn)}\nonumber\\
    &= \frac{\norm{n_t(\yy,\nn)}^2_\yy}{(1-\norm{n_t(\yy,\nn)}_\yy)},\label{eq:3}
\end{align}
Substituting \eqref{eq:3} into \eqref{eq:2} completes the proof.
\end{proof}

An important consequence of Lemma \ref{lem:nred} is if we have an initial pair $\{\yy,\nn\}$ such that $\norm{n_t(\yy,\nn)}_\yy \le \frac{1}{4}$, the next iterate satisfies the following inequality:
\[\norm{n_t(\yy^+,\nn)}_{\yy^+}\le \frac{1}{9}.\]


It is possible to determine a point's proximity to the optimum using the norm of the Newton's step computed from this point. This result is presented in Lemma~\ref{lem:closeness}.

\begin{lemma}
\label{lem:closeness}
    For any pair $\{\yy, \nn\}$ such that $\normy{n_t(\yy,\nn)}\le 1$ we have:
    \[\norm{\yy^\nn-\yy}_\yy \le \normy{n_t(\yy,\nn)} + \frac{3\normy{n_t(\yy,\nn)}^2}{(1-\normy{n_t(\yy,\nn)})^3},\]
where $\yy^\nn=\begin{bmatrix}\xx^\nn\\ \vv^\nn\end{bmatrix}$ denotes the optimal primal and dual variables such that:
\begin{align*}
    \xx^\nn &\in \argmin_{\xx} \nn \cc^\top \xx-\phi(\xx)\\
    &\quad\quad\text{\normalfont s.t.}\quad\nn\cc-\nabla\phi(\xx^\nn)+\AA^\top\vv^\nn = 0.
\end{align*}
\end{lemma}

\begin{proof}
   The proof follows from Lemma \ref{lem:nred} and the analysis from \cite[Section 2.2.5]{renegar}.
\end{proof}
In the specific case where $\norm{n_t(\yy^+,\nn)}_{\yy^+}\le \frac{1}{9}$, this implies:
\[\norm{\yy^\nn-\yy^+}_{\yy^+} \le \frac{1}{6}.\]




\begin{lemma}\label{lem:close2} If $\norm{\yy^\nn-\yy}_{\yy} \le \frac{1}{6}$, where $\yy$ is a primal-dual feasible point, then:
\[\norm{\yy^\nn-\yy}_{\yy^\nn} \le \frac{1}{5}.\]
\end{lemma}
\begin{proof}
From the definition of self-concordance we have:
\begin{align*}
    \norm{\yy^\nn-\yy}_{\yy^\nn} &\le \frac{\norm{\yy-\yy^*}_\yy}{1-\norm{\yy-\yy^\nn}_\yy}= \frac{\frac{1}{6}}{\frac{5}{6}}= \frac{1}{5}.
\end{align*}
\end{proof}



\begin{lemma}
\label{lem:b}
    Consider problem \eqref{prob2}. Let $\Delta\bb_t \stackrel{\text{def}}{=}\bb_{t+1}-\bb_t$ be the change in the online parameter $\bb_t$ between rounds.
    If $\norm{n_t(\yy,\nn)}\le\frac{1}{9}$ and $\norm{\Delta\bb_t}\le \sqrt{\frac{3m}{160}}$ for all $t$, then:
    \[\norm{n_{t+1}(\yy,\nn)} \le \frac{1}{4}.\]
    
\end{lemma}

\begin{proof}
First, we remark that the only change between rounds is the parameter $\bb_t$. Therefore,
\[r_{t+1}(\yy,\nn)=r_t(\yy,\nn)+\begin{bmatrix}\mathbf{0}\\ \Delta\bb_t\end{bmatrix}.\]
Let $\Delta \mathbf{r}_t = \begin{bmatrix}\mathbf{0}\\ \Delta\bb\end{bmatrix}$. Thus, $\norm{\Delta \mathbf{r}_t}=\norm{\Delta \bb_t}$. The Hessian norm of the Newton's step becomes:
\begin{align*}
    \normy{n_{t+1}(\yy,\nn)}^2 &= (r_t(\yy,\nn)+\Delta \mathbf{r})^\top\DD(\yy)^{-1}(r_t(\yy,\nn)+\Delta \mathbf{r}_t)\\
    \le 2 &r_t(\yy,\nn)^\top\DD(\yy)^{-1}r_t(\yy,\nn) %\\&\quad\quad\quad\quad\quad\quad
    + 2 \Delta \mathbf{r}_t^\top \DD(\yy)^{-1} \Delta \mathbf{r}_t\\
    =& \frac{2}{81} + 2\norm{\Delta \bb_t}^2\norm{\DD(\yy)^{-1}}\\
    =& \frac{2}{81} + \frac{2\norm{\Delta \bb_t}^2}{m}\le \frac{1}{16},
\end{align*}
where we used the bound on $\norm{\Delta\bb}$ to obtain the final inequality.
\end{proof}
%
% The following is an interesting property of the Hessian norm of the constrained Newton step for feasible points.

% \begin{align*}
%     \norm{n(\yy)}_\yy^2 &= \begin{bmatrix}
%         \nabla d(\xx) +\AA^\top\vv\\ \mathbf{0}
%     \end{bmatrix}^\top\begin{bmatrix}\nabla^2 d(\xx) & \AA^\top\\ \AA& \mathbf{0}\end{bmatrix}^{-1}\begin{bmatrix} \nabla d(\xx) +\AA^\top\vv\\ \mathbf{0}\end{bmatrix}\\
%     &= \begin{bmatrix}
%         \nabla d(\xx) +\AA^\top\vv\\ \mathbf{0}
%     \end{bmatrix}^\top\begin{bmatrix}\ddi - \ddi\AAt(\AA\ddi\AAt)^{-1}\AA\ddi & \ddi\AAt(\AA\ddi\AAt)^{-1}\\
%     (\AA\ddi\AAt)^{-1}\AA\ddi & -(\AA\ddi\AAt)^{-1}\end{bmatrix}\begin{bmatrix}
%         \nabla d(\xx) +\AA^\top\vv\\ \mathbf{0}
%     \end{bmatrix}
% \end{align*}
% The structure of the problem is such that the following is true. Here, we use the fact that the inverse of any symmetric matrix is symmetric and our Hessian matrix is always symmetric.
% \begin{align*}
%     \norm{n(\yy)}_\yy^2 &= (\nabla d(\xx)^\top + \vv^\top\AA)(\ddi-\ddi\AAt(\AA\ddi\AAt)^{-1}\AA\ddi)(\nabla d(\xx)+\AA^\top\vv)\\
%     &= \nabla d(\xx)^\top\ddi\nabla d(\xx) - \nabla d(\xx)^\top \ddi\AAt(\AA\ddi\AAt)^{-1}\AA\ddi\nabla d(\xx)
% \end{align*}
% Although this formulation is quite heavy, the important element is that the Hessian norm of the Newton step is completely independent of the dual variable $\vv$ when the current iterate is feasible. Furthermore, we can prove that this is the same quantity as would be obtained from applying Newton's method to the equivalent projected problem. 


From \cite[Section 2.3.1]{renegar}, we have that the restriction of any barrier functional to a subspace or translation of a subspace results in a barrier functional whose complexity barrier is smaller than that of the original functional. This implies that if $\norm{\phi(\xx)}_\xx^2\le v_f$ any feasible pair $\xx, \vv$ satisfy:
\[\begin{bmatrix}
        \nabla \phi(\xx) +\AA^\top\vv\\ \mathbf{0}
    \end{bmatrix}^\top\DD(\xx)^{-1}\begin{bmatrix} \nabla \phi(\xx) +\AA^\top\vv\\ \mathbf{0}\end{bmatrix} \le v_f.\]
Defining  \[h(\yy) \stackrel{\text{def}}{=} \DD(\xx)^{-1}\begin{bmatrix} \nabla \phi(\xx) +\AA^\top\vv\\ \mathbf{0}\end{bmatrix},\] we notice that $\norm{h(\yy)}_\yy \le \sqrt{v_f}$. We will use this result in Lemma \ref{lem:eta}.

\begin{lemma}
    \label{lem:eta}
    For a feasible pair $\{\yy,\nn\}$ where $\normy{n_t(\yy,\nn)}\le\frac{1}{9}$, there exists a parameter $1<\beta\le 1+\frac{1}{8\sqrt{v_f}}$ such that for $\nn^+ = \beta\nn$, we have:
    \[\normy{n_t(\yy,\nn^+)} \le \frac{1}{4}.\]
\end{lemma}

\begin{proof}

We can decompose the Newton's step taken at any feasible point in the following way: 
\begin{align*}n_t(\yy, \nn) &= -\DD(\xx)^{-1}\begin{bmatrix}\nn \cc + \nabla\phi(\xx) + \AAt\vv\\ \mathbf{0}\end{bmatrix}\\
&= -\DD(\xx)^{-1}\left(\begin{bmatrix}\nn \cc \\ \mathbf{0}\end{bmatrix}+\begin{bmatrix}\nabla\phi(\xx) + \AA^\top\vv\\ \mathbf{0}\end{bmatrix}\right). \end{align*}
This allows us to express $n_t(\yy, \nn^+)$ as a function of $n_t(\yy, \nn)$:
\[n(\yy,\nn^+) = \frac{\nn^+}{\nn}n_t(\yy,\nn) + \left(\frac{\nn^+}{\nn}-1\right)h(\yy).\]
Taking the Hessian norm on both sides leads to:
\begin{align*}
    \norm{n_t(\yy,\nn^+)}_\yy &\le \frac{\nn^+}{\nn}\norm{n_t(\yy,\nn)}_\yy+\left |1-\frac{\nn^+}{\nn}\right |\normy{h(\yy)}\\
    & \le  \frac{\nn^+}{\nn}\norm{n_t(\yy,\nn)}_\yy+\left |1-\frac{\nn^+}{\nn}\right |\sqrt{v_f}.
\end{align*}
By assumption, $\normy{n_t(\yy,\nn)}\le\frac{1}{9}$. Letting $\frac{\nn^+}{\nn}=\beta\le1+\frac{1}{8\sqrt{v_f}}$ we obtain:
\begin{align*}
    \norm{n_t(\yy,\nn^+)}_\yy &\le \frac{1}{9}\left(1+\frac{1}{8\sqrt{v_f}}\right)+\frac{\sqrt{v_f}}{8\sqrt{v_f}}\le \frac{1}{4},
\end{align*}
where we use the fact that $\sqrt{v_f}\ge 1$ \cite[Section 2.2]{renegar} to complete the proof.
\end{proof}




\begin{lemma}
\label{lem:renmyst}
    Let $\xx^\nn_t\in \argmin_{\xx} \{ \nn \cc^\top\xx + \phi(\xx)$ {\normalfont s.t.} $\AA\xx-\bb_t = 0\}$. If $\bar{\xx}$ is a feasible point then,
    \begin{align}
\cc^\top (\bar{\xx}-\xx^\nn_t) &\le   \frac{v_f}{\nn_t}(1+\norm{\bar{\xx}-\xx^\nn_t}_{\xx^\nn_t}).\end{align}
\end{lemma}
\begin{proof}
    The above identity follows from \cite[Section 2.4.1]{renegar} for a linear objective.
\end{proof}

Finally, we relate $\normy{\yy_t-\yy_t^\nn}$ to $\norm{\xx_t-\xx_t^\nn}_{\xx}$ in Lemma~\ref{lem:yx}.
\begin{lemma}
    \label{lem:yx}
    For any primal-dual feasible point $\yy$, we have:
    \[\normy{\yy_t-\yy_t^\nn}\ge\norm{\xx_t-\xx_t^\nn}_\xx.\]
\end{lemma}
\begin{proof}
    Recall from the definition of the Hessian norm:
    \begin{align*}
        \normy{\yy_t-\yy_t^\nn}^2 &= (\yy_t-\yy_t^\nn)^\top \DD(\yy)(\yy_t-\yy_t^\nn)\\
        &\ge \begin{bmatrix}
            \xx_t-\xx_t^\nn\\ \mathbf{0}
        \end{bmatrix}^\top \DD(\yy)\begin{bmatrix}\xx_t-\xx_t^\nn\\ \mathbf{0}\end{bmatrix}\\
        &= (\xx_t-\xx^\nn_t)^\top\nabla^2\phi(\xx_t)(\xx_t-\xx^\nn_t)\\
        &= \norm{\xx_t-\xx_t^\nn}_{\xx_t}^2.
    \end{align*}
    Taking the square root on both sides, we obtain the desired result completing the proof.
\end{proof}

\section{Algorithms} \label{algos}

Suppose we have an initial primal-dual point $\xx_0$, $\vv_0$ and an initial barrier parameter $\nn_0$ such that $\norm{n_0(\yy_0, \nn_0)}_{\yy_0} \le \frac{1}{9}$ and $\AA\xx_0=\bb_0$. We propose the online interior-point method for time-varying equality constraints (\texttt{OIPM-TEC}) presented in Algorithm~\ref{alg:TIP2}.




\begin{algorithm}[h!]
\caption{\texttt{OIPM-TEC}}\label{alg:TIP2}
\begin{algorithmic}
\State\textbf{Parameters:} $\AA$, $\beta$
\State\textbf{Initialization}: Receive $\xx_0 \in\mathbb{R}^n$, $\vv_0\in\mathbb{R}^p$, $\nn_0 \in\mathbb{R}^+$ such that $\norm{n_0(\yy_0, \nn_0)}_{\yy_0}\le \frac{1}{9}$.
\FOR{$t=0,1,2...T$}
\State Implement the decision $\xx_t$.
\State Observe the outcome $\cc^\top\xx_t$ and the new constraint $\bb_t$.
\State \underline{Perform $t$-step:}
\State   $\tilde{\yy}_{t+1}$ $=\yy_{t}-\begin{bmatrix}\nabla^2 \phi(\xx_t) & \AA^\top\\ \AA& \mathbf{0}\end{bmatrix}^{-1}\begin{bmatrix} \nabla d(\xx_t,\nn_t) +\AA^\top\vv\\ \AA\xx-\bb_t\end{bmatrix}$.
\State \underline{Perform $\nn$-step:}
\State $\nn_{t+1} = \nn_t\beta$,
\State $\yy_{t+1}$ $=\tilde{\yy}_{t+1}-\begin{bmatrix}\nabla^2 \phi(\tilde{\xx}_t) & \AA^\top\\ \AA& \mathbf{0}\end{bmatrix}^{-1}\begin{bmatrix} \nabla d(\tilde{\xx}_t,\nn_{t+1}) +\AA^\top\vv\\ \mathbf{0}\end{bmatrix}$.
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    If Assumptions 1-3 hold and supposing an initial point $\xx_0$, $\vv_0$ and an initial barrier parameter $\nn_0$ such that $\norm{n_0(\yy_0, \nn_0)}_{\yy_0} \le \frac{1}{9}$, then the regret and constraint violation of Algorithm \ref{alg:TIP2} are bounded by:
    \begin{align}
        R_{\text{{\normalfont d}}}(T)&\le \frac{11 v_f\beta}{5\nn_0(\beta-1)} + cV_T\\
        \text{{\normalfont Vio}}(T)&\le V_\bb.
    \end{align}
    The dynamic regret and constraint violation are, therefore, $O(V_T+1)$ and $O(V_\bb)$, respectively.
\end{theorem}


\begin{proof} We first derive some properties of the algorithm. 
Suppose at each round we have $\yy_{t-1}$ such that $\norm{n_{t-1}(\yy_{t-1}, \eta_{t-1})}_{\yy_{t-1}}\le \frac{1}{9}$. Then, when observing the new vector $\bb_t$, Lemma \ref{lem:b} yields $\norm{n_t(\yy_{t-1}, \eta_{t-1})}_{\yy_{t-1}}\le \frac{1}{4}$. By Lemma \ref{lem:nred}, this implies that the point $\tilde{\yy}$ obtained after the $t$-step will respect:
\[\norm{n_t(\tilde{\yy}_t, \eta_{t-1})}_{\tilde{\yy}_t}\le \frac{1}{9}.\]
After updating $\nn$, Lemma \ref{lem:eta} yields:
\[\norm{n_t(\tilde{\yy}_t, \eta_t)}_{\tilde{\yy}_t}\le \frac{1}{4}.\]
After the $\nn$-step from Lemma \ref{lem:nred} we have:
\[\norm{n_t(\yy_t, \eta_t)}_{\yy_t}\le \frac{1}{9}.\]
Lemma \ref{lem:closeness} leads to:
\[\norm{\yy_t-\yy_{t-1}^{\nn_t}}_{\yy_t}\le \frac{1}{6}.\]
Now, applying Lemma \ref{lem:close2}, we get:
\[\norm{\yy_t-\yy_{t-1}^{\nn_t}}_{\yy_{t-1}^{\nn_t}}\le \frac{1}{5}.\]
Finally, Lemma \ref{lem:yx} yields,
\begin{equation}\label{result}\norm{\xx_t-\xx_{t-1}^{\nn_t}}_{\xx_{t-1}^{\nn_t}}\le \frac{1}{5}.\end{equation}
It follows that because we initially have $\norm{n_0(\yy_0, \nn_0)}_{\yy_0} \le \frac{1}{9}$ by assumption, then $\norm{n_{t-1}(\yy_{t-1}, \eta_{t-1})}_{\yy_{t-1}}\le \frac{1}{9}$ for all~$t$.

Defining $c=\norm{\cc}$, we now re-express the regret as:
\noindent \begin{align}\label{eq:regret2}
    R_{\text{d}}(T) &= \sum_{t=1}^{T} f(\xx_t)-f(\xx^*_t)\nonumber\\
    \le& \sum_{t=1}^{T}\cc^\top\left(\xx_t - \xx^{\nn_t}_{t-1} +\xx^{\nn_t}_{t-1}-\xx^*_{t-1}+\xx^*_{t-1}-\xx^*_t \right)\nonumber\\
    \le& \sum_{t=1}^{T}\cc^\top\left(\xx_t - \xx^{\nn_t}_{t-1} +\xx^{\nn_t}_{t-1}-\xx^*_{t-1}\right)+c\norm{\xx^*_{t-1}-\xx^*_t }\nonumber\\
    =& \sum_{t=1}^T\cc^\top\big(\xx_t - \xx^{\nn_t}_{t-1}\big)+\sum_{t=1}^T\cc^\top\big(\xx^{\nn_t}_{t-1}-\xx^*_{t-1}\big)+cV_T.
\end{align}
Next, we bound the first terms of \eqref{eq:regret2}. The first sum represents the estimation error between the current decision and the optimum at a given $\nn$. Using Lemma \ref{lem:renmyst},
\begin{align*}
    \sum_{t=1}^T\cc^\top\big(\xx_t - \xx^{\nn_t}_{t-1}\big) &\le \sum_{t=1}^T \frac{v_f}{\nn_t}\left(1+\norm{\xx_t-\xx^{\nn_t}_{t-1}}_{\xx^{\nn_t}_{t-1}} \right)\\
    &\le v_f\sum_{t=1}^T \frac{1}{\nn_t}\left(1+\frac{1}{5}\right)\\
    &= \frac{6v_f}{5\nn_0}\sum_{t=1}^T\left(\frac{1}{\beta}\right)^{t}\le \frac{6 v_f\beta}{5\nn_0(\beta-1)},
\end{align*}
where we used \eqref{result} to obtain the second inequality.


From \cite[Chapter 9]{boyd}, we have that any optimizer $\xx_t^\nn$ of the functional $d_t(\xx_t, \nn)$ for a particular $\nn$ has an optimality gap of at most $\frac{v_f}{\nn}$, i.e., $\cc^\top(\xx_t^\nn-\xx_t^*)\le\frac{v_f}{\nn}$. Therefore,
\begin{align*}
    \sum_{t=1}^T\cc^\top(\xx^{\nn_t}_{t-1}-\xx^*_{t-1}) &\le  \sum_{t=1}^T \frac{v_f}{\nn_t}\\
    & = \frac{v_f}{\nn_0}\sum_{t=1}^T\left(\frac{1}{\beta} \right)^t\le \frac{v_f\beta}{\nn_0(\beta-1)}.
\end{align*}
Thus, the regret bound is : 
\[R_{\text{d}}(T) \le cV_T+\frac{11v_f\beta}{5\nn_0(\beta-1)}.\]


As for constraint violation, we have:
\begin{align*}
    \text{Vio}(T) = \sum_{t=1}^T \left[\sum_i\left[\text{Tr}\Big(g_i(\xx_t)\Big)\right]^++\norm{\AA\xx_t-\bb_t}\right].
\end{align*}
We note that iterates are always in the feasible space defined by the inequality constraints $g_i(\xx)$. This is because, at each round, a unit-step is taken in the Newton direction which lies in the feasible space of the self-concordant functional \cite{renegar}. It follows that, $g_i(\xx_t)\preceq 0\forall t,i$. We also observe that iterates are always feasible with respect to the previous round's equality constraints. This is again because of the unit-step taken during the $t$-step \cite{boyd}. Using this fact, we can write:
\begin{align*}
    \text{Vio}(T) &= \sum_{t=1}^T\big[0 + \norm{\AA\xx_t-\bb_t}\big]\\
    &= \sum_{t=1}^T \norm{\bb_{t-1}-\bb_t}= V_{\bb},
\end{align*}
which completes the proof.
\end{proof}

When operating power grids, the exact optimal solution is not always required. Indeed there is a non-zero tolerance-region around the optimum that the electric grid operator can accept. For example, if the objective is cost-minimization, a solution to the OPF within a few cost units would fall within the tolerance of the grid operator. To translate this to a more traditional OCO framework, we can define an epsilon-regret $R_\epsilon(T)$ as:
\begin{equation}
    \label{def:epregret}
    R_\epsilon(T)=\sum_{t=1}^T \left[f_t(\xx_t)-f_t(\xx_t^*)-\epsilon\right]^+,
\end{equation} 
where $0\le\epsilon $ is the desired tolerance. We remark that the epsilon-regret is null if all decisions are within our tolerance and positive otherwise. Note that epsilon-regret is a weaker metric than dynamic regret. A bounded dynamic regret implies a bounded epsilon-regret.

If instead of requiring sublinear dynamic regret, we are only interested in sublinear epsilon-regret, an algorithm can be used which only requires a $t$-step, the epsilon online interior-point method for time-varying equality constraints (\texttt{$\epsilon$OIPM-TEC}). This algorithm is presented in Algorithm \ref{alg:EpsilonTIP}. 
\begin{algorithm}[h!]
\caption{\texttt{$\epsilon$OIPM-TEC}}\label{alg:EpsilonTIP}
\begin{algorithmic}
\State\textbf{Parameters:} $\AA$, $\cc$
\State\textbf{Initialization}: Receive $\xx_0 \in\mathbb{R}^n$, $\vv_0\in\mathbb{R}^p$, $\nn \in\mathbb{R}^+$ such that $\norm{n_0(\yy_0, \nn)}_{\yy_0}\le \frac{1}{9}$.
\FOR{$t=0,1,2...T$}
\State Implement the decision $\xx_t$.
\State Observe the outcome $\cc^\top\xx_t$ and the new constraint $\bb_t$.
\State   $\yy_{t+1}$ $=\yy_{t}-\begin{bmatrix}\nabla^2 \phi(\xx_t) & \AA^\top\\ \AA& \mathbf{0}\end{bmatrix}^{-1}\begin{bmatrix} \nabla d(\xx_t,\nn_t) +\AA^\top\vv\\ \AA\xx-\bb_t\end{bmatrix}$.
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    Let $\epsilon\ge 0$ be the tolerance. If Assumptions 1-3 hold and we have the initial point $\xx_0$, $\vv_0$, and $\nn$ such that $\nn \ge \frac{11v_f}{5\epsilon}$ and $\norm{n_0(\yy_0,\nn)}_{\yy_0}<\frac{1}{9}$, then the epsilon-regret and constraint violation of Algorithm \ref{alg:EpsilonTIP} are bounded by:
    \begin{align}
        R_\epsilon(T) &\le cV_T\\
        \text{{\normalfont Vio}}(T) &\le V_\bb.
    \end{align}
\end{theorem}

\begin{proof}
    The constraint violation is bounded by $V_\bb$ following the same reasoning as for Algorithm \ref{alg:TIP2}.
    
    Similarly to Algorithm \ref{alg:TIP2}, the point $\yy_{t+1}$ obtained after the Newton's step will respect:
    $\norm{n_t(\yy_t, \eta_{t-1})}_{\tilde{\yy}_t}\le \frac{1}{9}.$
    Applying Lemmas \ref{lem:closeness}, \ref{lem:close2}, and \ref{lem:yx} successively, we have:
    $\norm{\xx_{t+1}-\xx_t^\nn}_{\xx_t^\nn}\le \frac{1}{5}$.
    We can therefore re-express the epsilon-regret as:
    \begin{align}
    \label{eq:afs}
        R_\epsilon(T) &= \sum_{t=1}^T\left[f_t(\xx_t)-f_t(\xx_t^*)-\epsilon\right]^+\nonumber\\ 
         %\le  \sum&_{t=1}^T\Big[\cc^\top\big(\xx_t - \xx^{\nn_t}_{t-1}+\xx^{\nn_t}_{t-1}-\xx^*_{t-1}%\nonumber\\&\quad\quad\quad\quad\quad\quad\quad\quad
        %+\xx^*_{t}-\xx^*_{t-1}\big)-\epsilon\Big]^+\nonumber\\
        &\le \sum_{t=1}^T\left[\frac{v_f}{\nn}\left(1+\norm{\xx_{t+1}-\xx_t^\nn}_{\xx_t^\nn}\right)+\frac{v_f}{\nn}-\epsilon\right]^+ +cV_T\nonumber\\
        &\le \sum_{t=1}^T\left[\frac{11v_f}{5\nn}-\epsilon\right]^++cV_T.
    \end{align}
    Because $\nn \ge \frac{11v_f}{5\epsilon}$ by assumption, $\epsilon \ge \frac{11v_f}{5\nn}$. Therefore the leftmost sum of \eqref{eq:afs} is zero. This implies that $R_\epsilon(T)\le cV_T$, thus completing the proof.
\end{proof}

\section{Numerical Experiment}

We now illustrate the performance of our algorithms in an online OPF problem. In the online OPF, the grid operator seeks to serve time-varying loads within a fixed network while minimizing the generation costs. Consider a set of buses $\mathcal{B}$ on which lies a set of generators $\mathcal{G}$ connected to a set of loads $\mathcal{D}$ by power lines $\mathcal{L}$. The admittance of every line $ij\in\mathcal{L}$ is given by $\gamma_{ij}\in\mathbb{C}$.  

At time $t$, this problem's decision variables are the real and reactive power injections at every bus given by $p_{i,t}$ and $q_{i,t}$, respectively. The dependant variables are the voltages, $v_{i,t}$ at each bus. The parameters are the minimum and maximum power limits on active and reactive generation ($\underline{p},\underline{q},\overline{p},\overline{q})$, the apparent power limits in the power lines ($\overline{k}_{ij}$) and the voltage limits $(\underline{v}, \overline{v})$. The active and reactive demand at every bus at time $t$ is given by $p_{d,t}$ and $q_{d,t}$, respectively. Finally, in order to obtain a second-order cone relaxation of the problem, the matrix $W\in \mathbb{C}^{\text{card} \mathcal{B}^2}$ is formed where every element $W_{ij}$ for $ij\in\mathcal{L}$ represents the product $v_iv_j^*$, with $^*$ denoting the complex conjugate \cite{taylor2015convex}. At time $t$, the online OPF problem takes the following form:
\begin{align*}
    \min& \quad s_t\\
        \text{s.t.}\quad &\sum_{i\in\mathcal{G}} a_i p_{i,t}^2+b_i p_{i,t}-s_t \le 0 
\end{align*}
\begin{align}
\label{OPF}
    & \underline{p_i} \le p_{i,t} \le \overline{p_i}\quad \forall i\in\mathcal{G}\nonumber\\
        & \underline{q_i} \le q_{i,t} \le \overline{q_i}\quad \forall i\in\mathcal{G}\\
        &\underline{v}^2 \le W_{ii,t}\le \overline{v}^2\quad\forall i\in\mathcal{N}\nonumber\\
        &\norm{(W_{ii,t}-W_{ij,t})\gamma^*_{ij}} \le \overline{k}_{ij}\quad \forall ij\in\mathcal{L}\nonumber\\
        &\norm{\begin{matrix}2W_{ij,t}\\ W_{ii,t}-W_{jj,t}\end{matrix}}\le W_{ii,t} + W_{jj,t}\nonumber\\
        & p_{i,t} +p_{d,t} + \jmath q_{i,t}+\jmath q_d = \sum_{j\in\mathcal{N}} (W_{ii}-W_{ij})\gamma^*_{ij}\nonumber
        %&p_{d,t} = q_{d,t} = 0 \quad\forall d\in\mathcal{G}\\
        %&p_{i,t} = q_{i,t} = 0 \quad \forall i \notin\mathcal{G},
\end{align}
where $\jmath$ is the imaginary number.
We consider a time-varying extension of the second-order cone relaxation of the Matpower 33-bus case network presented in \cite{Matpower}. To represent demand fluctuation, a random variation $\Delta p_{d,t}$ is added to every load at every timestep. For this experiment, every variation is independent and taken as: $\Delta p_{d,t} = \frac{0.01\zeta}{\sqrt{t}}$ where $\zeta$ is uniformly sampled in $[0,1]$ at every round. The load variation diminishes with the square root of $t$ ensuring that the variation in optima ($V_T$) and in the online parameter $\bb_t$ ($V_\bb$) are sublinear. If the offline problem, at a given timestep, is unsolvable, new random variables $\zeta$ are selected. The time horizon is set at $T=2000$.
For \texttt{OIPM-TEC}, the initial barrier parameter is set to $\eta_0=1$ and the parameter $\beta$ is set to $1.02$. For \texttt{$\epsilon$OIPM-TEC}, the barrier parameter is set to $\eta = 10^4$. The initial primal-dual point $\xx_0, \vv_0$ for both algorithms is determined from the offline optimal solution of the Matpower 33-bus network. The acceptable tolerance $\epsilon=0.015$ is selected for the epsilon-regret which represents less than $0.001\%$ of the offline optimal cost. This value is equivalent to $0.015\$$/hr in this case. As a comparison, the \texttt{MOSP} algorithm \cite{MOSP} is also applied to problem \eqref{OPF} using the same initial primal point as \texttt{OIPM-TEC}. In the \texttt{MOSP} implementation, the time-varying equality constraints are relaxed to inequality constraints such that $\AA\xx_t-\bb_t\le\mathbf{0}$. This enables the operator to over-serve loads and should be strictly respected at the optimum. The other linear inequality constraints are implemented directly while a projection is used to satisfy the second-order cone constraints. The parameters of the algorithm are set to : $\alpha=\mu=t^{-\frac{1}{3}}$, where $t$ denotes the current round.

The resulting regret (dynamic regret and epsilon regret are considered) and constraint violation suffered by the three algorithms are presented in Figures \ref{fig:regret} and \ref{fig:const}. We remark that all algorithms achieve sublinear epsilon-regret and constraint violation. However, the epsilon-regret suffered by \texttt{OIPM-TEC}, while higher than that of \texttt{$\epsilon$OIPM-TEC}, is much lower than for \texttt{MOSP}. Both \texttt{OIPM-TEC} and \texttt{$\epsilon$OIPM-TEC} exhibit sublinear constraint violation terms that outperform \texttt{MOSP}.

% Figure environment removed

% Figure environment removed

\section{Conclusion}
In this work, an online optimization algorithm using an interior-point method-like update capable of solving the online OPF is presented. This algorithm, the \texttt{OIPM-TEC}, admits time-varying equality constraints and generalized inequalities while possessing a $O(V_T+1)$ dynamic regret bound and a $O(V_\bb)$ constraint violation bound. Additionally, if there exists an acceptable tolerance when solving an online optimization problem, the \texttt{$\epsilon$OIPM-TEC} algorithm is capable of ensuring the tightest possible bound with respect to $T$ on regret and constraint violation. These two algorithms are applied to an online OPF example showcasing their performance compared to other methods from the literature. 

The possibility of using OCO approaches to solve the OPF bridges the gap between traditional OCO algorithms with performance guarantees and real-time OPF approaches like \cite{realtimeOPF} and \cite{OptimalPowerFlowPursuit} that enjoy good real-world performance. A possible future direction is the development of online primal-dual interior-point methods which could increase numerical stability. 


\section*{References}
\bibliographystyle{IEEEtran}
\bibliography{ref.bib}


%\begin{IEEEbiography}[{% Figure removed}]{First A. Author} (Fellow, IEEE) and all authors may include 
%biographies. Biographies are
%often not included in conference-related papers.
%This author is an IEEE Fellow. The first paragraph
%may contain a place and/or date of birth (list
%place, then date). Next, the author’s educational
%background is listed. The degrees should be listed
%with type of degree in what field, which institution,
%city, state, and country, and year the degree was
%earned. The author’s major field of study should
%be lower-cased.

%The second paragraph uses the pronoun of the person (he or she) and
%not the author’s last name. It lists military and work experience, including
%summer and fellowship jobs. Job titles are capitalized. The current job must
%have a location; previous positions may be listed without one. Information
%concerning previous publications may be included. Try not to list more than
%three books or published articles. The format for listing publishers of a book
%within the biography is: title of book (publisher name, year) similar to a
%reference. Current and previous research interests end the paragraph.

%The third paragraph begins with the author’s title and last name (e.g.,
%Dr. Smith, Prof. Jones, Mr. Kajor, Ms. Hunter). List any memberships in
%professional societies other than the IEEE. Finally, list any awards and work
%for IEEE committees and publications. If a photograph is provided, it should
%be of good quality, and professional-looking.
%\end{IEEEbiography}

%\begin{IEEEbiographynophoto}{Second B. Author,} photograph and biography not available at the
%time of publication.
%\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Third C. Author Jr.} (Member, IEEE), photograph and biography not available at the
%time of publication.
%\end{IEEEbiographynophoto}

\end{document}
