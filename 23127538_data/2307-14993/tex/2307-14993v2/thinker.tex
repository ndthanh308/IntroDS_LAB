\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final, nonatbib]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Additional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{graphicx} 
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{caption}
\usepackage{makecell}

\usepackage{xargs}                      % Use more than one optional parameter in a new commands
% \usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
% \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
% \newcommandx{\im}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\usepackage{todonotes}
\usepackage{hyperref}


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\pr}{\text{Pr}}
\DeclareMathOperator*{\ex}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}
\newcommand{\defeq}{\vcentcolon=}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}

\hypersetup{
	pdftitle={Thinker: Learning to Plan and Act},
	pdfauthor={Stephen Chung, Ivan Anokhin, David Krueger},
	pdfsubject={Deep Reinforcement Learning},
	pdfkeywords={Reinforcement Learning, Deep Learning, Planning Algorithm, Monte-Carlo Tree Search},
	pdfproducer={LaTeX with hyperref},
	pdfcreator={pdflatex}
}


\title{Thinker: Learning to Plan and Act}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
	Stephen Chung \\
	University of Cambridge\\
	\texttt{mhc48@cam.ac.uk} \\
	\And
	Ivan Anokhin \\
	Mila, Université de Montréal \\
	\texttt{ivan.anokhin@mila.quebec} \\
	\And
	David Krueger \\
	University of Cambridge\\
	\texttt{dsk30@cam.ac.uk} \\  
}

\begin{document}
	
	
	\maketitle
	\begin{abstract}
		We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for handcrafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. Thinker is the first work showing that an RL agent can learn to plan with a learned world model in complex environments. 
	\end{abstract}
	
	\section{Introduction}
	
	% intro
	Model-based reinforcement learning (RL) has significantly enhanced sample efficiency and performance by employing world models, or simply models, to generate additional training data \cite{hafner2021mastering, sutton1990integrated}, provide better estimates of the gradient \cite{heess2015learning, deisenroth2011pilco}, and facilitate planning \cite{schrittwieser2020mastering}. Here, \emph{planning} refers to the process of interacting with the world model to inform the subsequent selection of actions. Handcrafted planning algorithms, such as Monte Carlo Tree Search (MCTS) \cite{coulom2007efficient}, have achieved remarkable success in recent years \cite{silver2017mastering, silver2018general, schrittwieser2020mastering}, underscoring the importance of planning.
	
	However, a significant research gap persists in creating methods that enable an RL agent to \emph{learn to plan} \cite{luo2022survey}, meaning to interact autonomously with a model, eliminating the need for handcrafted planning algorithms. We suggest this shortfall stems from the inherent complexities of mastering several skills concurrently required by planning: searching (exploring new potential plans), evaluating (assessing plan quality), summarizing (contrasting different plans), and executing (implementing the optimal plan). While a handcrafted planning algorithm can handle these tasks, they pose a significant learning challenge for an RL agent that learns to plan. Introducing a learned world model further complicates the issue, as predictions from the model might not be accurate.
	
	To address these challenges, we introduce the \emph{Thinker} algorithm, a novel approach that enables the agent to interact with a learned model as an integral part of the environment. The Thinker algorithm wraps a Markov Decision Process (MDP) with a learned model and introduces a new set of actions that enable agents to interact with the model.\footnote{Full code is available at \url{https://github.com/stephen-chung-mh/thinker}, which allows for using the Thinker-augmented MDP with the same interface as OpenAI Gym~\cite{brockman2016openai}.} The manner in which an agent uses a model is not predetermined. In principle, an agent can learn various common planning algorithms, such as $n$-step exhaustive search or MCTS, or ignore the model if planning is not beneficial. Importantly, we have designed the augmented MDP in a way that simplifies the processes of searching, evaluating, summarizing, and executing, making it easier for agents to learn to use the model.
	
	% neuroscience
	Drawing from neuroscience, our work is inspired by the influential hypothesis that the brain conducts planning via \emph{internalized actions}—actions processed within our experience-based internal world model rather than externally \cite{buzsaki2014emergence, gyorgy2019brain}. According to this hypothesis, both model-free and model-based behaviors utilize a similar neural mechanism. The main distinction lies in whether actions are directed toward the external world or the internal world model. This perspective is consistent with the structural similarities observed in brain regions governing model-free and model-based behaviors \cite{yin2006role}. This hypothesis offers insights into how an RL agent might \emph{learn to plan}—by unifying both imaginary and real actions and leveraging existing RL algorithms. 
	
	% experiment
	Experimental results show that actor-critic algorithms, when applied to the Thinker-augmented MDP, yield state-of-the-art performance in Sokoban. Notably, this approach attains a solving rate of 94.5\% within 5e7 frames, a significant improvement over the 56.7\% solving rate achieved when the same algorithm is applied to the raw MDP. On the Atari 2600 benchmark, actor-critic algorithms using the Thinker-augmented MDP show a significant improvement over those using the raw MDP. Visualization results, as shown in Fig \ref{fig:vis}, reveal the agent's effective use of the model. To summarize, the Thinker algorithm provides the following advantages:
	
	% Figure environment removed
	
	\begin{itemize}
		\item \textbf{Flexibility:} The agent learns to plan on its own, without any handcrafted planning algorithm, allowing it to adapt to different states, environments, and models.
		\item \textbf{Generality:} The Thinker algorithm only dictates how an MDP is transformed, making it compatible with any RL algorithm. Specifically, one can convert any model-free RL algorithm into a model-based RL algorithm by switching to the Thinker-augmented MDP.
		\item \textbf{Interpretability:} We can visualize the agent's plan prior to its execution, as depicted in Fig \ref{fig:vis}. This provides greater insight compared to scenarios where the neural network internalizes the model and plans in a black box fashion \cite{guez2019investigation}.
		%\item \textbf{Biological plausibility:} We argue that the Thinker algorithm more closely resembles how a model is used in the brain, in contrast to handcrafted planning algorithms like MCTS. % remove
		\item \textbf{Aligned objective:} Both the real and imaginary actions are trained using the same rewards from the environment, ensuring that the objectives of planning and acting align.
		\item \textbf{Improved learned model:} We introduce a novel combination of architecture and feature loss for the model, which is designed to prioritize the learning of task-relevant features and enable visualization at the same time.
	\end{itemize}
	
	
	\section{Related Work}
	
	Various approaches exist for incorporating planning into RL algorithms, with the majority relying on handcrafted planning algorithms. In other words, the policy underlying the interaction with the world model, termed here as the \emph{imaginary policy}, is typically handcrafted. For example, MuZero~\cite{schrittwieser2020mastering}, AlphaGo~\cite{silver2016mastering}, and AlphaZero~\cite{silver2017mastering} employ MCTS. VPN~\cite{oh2017value}, TreeQN and ATreeC~\cite{farquhar2017treeqn} explore all possible action sequences up to a predefined depth. I2A~\cite{weber2017imagination} generates one rollout per action, with the first action iterating over the action set and subsequent actions following the current policy. 
	
	Learning to plan is a challenging domain, and a few works exist in this area.  MCTSnets~\cite{guez2018learning} is a supervised learning algorithm that learns to imitate actions from expert trajectories generated from MCTS. VIN~\cite{tamar2016value} and DRC~\cite{guez2019investigation} are model-free approaches that utilize a special neural network architecture. VIN’s architecture aims to facilitate value iteration, while DRC’s architecture aims to facilitate general planning. IBP~\cite{pascanu2017learning}, which is closely related to our work, allows the agent to engage with both the model and the environment. However, several critical differences distinguish IBP from Thinker. First, in IBP, the imaginary policy is trained by backpropagating through the model, a method that is primarily suitable for continuous action sets, whereas our approach focuses on environments with discrete action sets and treats the model as a black-box tool external to the agent. Second, unlike our method, IBP does not predict future values and policies, which we have identified as crucial for complex environments (see Appendix \ref{app:ab}). Third, we have designed an augmented state representation that significantly simplifies the process of using the model by providing more than just the predicted states to the agent, as is the case with IBP.
	
	Another research direction employs gradient updates to refine rollouts \cite{anthony2019policy, henaff2017model, fickinger2021scalable} prior to taking each real action. These methods start with a preliminary imaginary policy and use this policy to gather rollouts from the model. Subsequently, these rollouts are used to calculate gradient updates for the imaginary policy by maximizing the imaginary rewards, thus incrementally refining the rollouts' quality. In the final step, a handcrafted function is used to select a real action. For instance, previous works such as \cite{anthony2019policy}, \cite{henaff2017model}, and \cite{fickinger2021scalable} have proposed to maximize the rollout return using REINFORCE \cite{williams1992simple}, PPO \cite{schulman2017proximal}, and backpropagation through the model, respectively. In contrast to these works, Thinker does not employ such gradient updates to refine rollouts, as the agent has to learn how to refine rollouts and select a real action on its own. Moreover, unlike these works, Thinker trains the imaginary policy to maximize the real rewards, thus preventing model exploitation when the model is learned.
	
	Thinker is the first work showing that an RL agent \emph{can learn to plan with a learned world model in complex environments.} Prior related works either do not satisfy (i) learning to plan, i.e., a learned imaginary policy, (ii) using a learned model or (iii) being evaluated in a complex environment. For example,  MuZero~\cite{schrittwieser2020mastering}, VPN~\cite{oh2017value}, TreeQN~\cite{farquhar2017treeqn}, and I2A~\cite{weber2017imagination} do not satisfy (i). VIN~\cite{tamar2016value}, DRC~\cite{guez2019investigation} and \cite{anthony2019policy, fickinger2021scalable} do not satisfy (ii). IBP~\cite{pascanu2017learning} and \cite{henaff2017model} do not satisfy (iii). 
	
	We describe the connection between our algorithm and Meta-RL \cite{wang2016learning, duan2016rl}, as well as generalized policy iteration \cite{bellman1957dynamic, sutton2018reinforcement}, in Appendix \ref{app:pi}.
	
	\section{Background and Notation}
	
	We consider a Markov Decision Process (MDP) defined by a tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma, d_0)$, where $\mathcal{S}$ is a set of states, $\mathcal{A}$ is a finite set of actions, $P:\mathcal{S}\times \mathcal{A}\times \mathcal{S} \rightarrow [0,1]$ is a transition function representing the dynamics of the environment, $R: \mathcal{S}\times \mathcal{A} \rightarrow \mathbb{R}$ is a reward function, $\gamma \in [0,1]$ is a discount factor, and $d_0: \mathcal{S} \rightarrow [0,1]$ is an initial state distribution. Denoting the state, action, and reward at time $t$ by $s_t$, $a_t$, and $r_t$ respectively, $P(s, a, s') = \pr(s_{t+1}=s'|s_t=s, a_t=a)$, $R(s, a) = \ex[r_{t+1}|s_t=s, a_t=a]$, and $d_0(s) = \pr(s_{1}=s)$, where $P$ and $d_0$ are valid probability mass functions. An episode is a sequence of $(s_t, a_t, r_{t+1})$, starting from $t=1$ and continuing until reaching the terminal state, a special state where the environment ends. Letting $g_t = \sum_{k=t}^{\infty} \gamma^{k-t} r_k$ denote the infinite-horizon discounted return accrued after acting at time $t$, we are interested in finding, or approximating, a \emph{policy} $\pi:  \mathcal{S}\times \mathcal{A} \rightarrow [0,1]$, such that for any time $t \geq 1$, selecting actions according to $\pi(s,a)=\pr(a_t=a|s_t=s)$ maximizes the expected return $\ex[g_{t+1}|\pi]$. The value function for policy $\pi$ is $v^{\pi}$ where for all $s \in  \mathcal{S}$, $v^{\pi}(s) = \ex[g_{t+1}|s_t=s, \pi]$. We adopt the notation $(s_t, a_t, r_{t+1}, s_{t+1})$ for representing transitions, as opposed to $(s_t, a_t, r_{t}, s_{t+1})$, which facilitates a clearer description of the algorithm. In this work, we only consider an MDP with a discrete action set.
	
	%In this work, we only consider an MDP with a discrete action set. We use the subscript $t$ to denote the time step in a raw MDP and the subscript $k$ to denote the time step in the augmented MDP. 
	
	\section{Algorithm}
	
	The Thinker algorithm transforms a given MDP, denoted as $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma, d_0)$, into an augmented MDP, denoted as $\mathcal{M}^\text{aug} = (\mathcal{S}^\text{aug}, \mathcal{A}^\text{aug}, P^\text{aug}, R^\text{aug}, \gamma^\text{aug}, d_0^\text{aug})$. We call $\mathcal{M}$ the real MDP, and  $\mathcal{M}^\text{aug}$ the augmented MDP. Besides the real MDP, we assume that we are given (i) a model in the form of $(\hat{P}, \hat{R})$ that is an approximate version of $(P, R)$ and is deterministic, and (ii) a \emph{base policy} $\hat{\pi}:(\mathcal{S}, \mathcal{A}) \rightarrow [0, 1]$ and its value function $\hat{v}: \mathcal{S} \rightarrow \mathbb{R}$. The base policy can be any policy, which helps to simplify the search and evaluation of rollouts for the agent. We will show how both the model and base policy can be learned from scratch in Section \ref{sec:dual}, but for now we assume that both are given.
	
	\subsection{Augmented Transitions}
	
	% Figure environment removed
	
	To facilitate interaction with the model, we add $K-1$ steps before each step in the real MDP, where $K \geq 1$ and denotes the \emph{stage length}. We typically use $K=20$ in the experiments. The first $K-1$ steps are called imaginary steps, as actions in these steps are passed to the model instead of the real MDP. The step that follows the imaginary step is called the real step, as the action selected is passed to the real MDP. These $K$ steps in the augmented MDP, including $K-1$ imaginary steps and one real step, constitute a single \emph{stage} in the augmented MDP. An illustration of a stage is shown in Fig \ref{fig:planning}. Each episode in the augmented MDP is composed of multiple stages, with a single stage corresponding to one step in the real MDP. The augmented MDP terminates if and only if the underlying real MDP terminates. We use $k \in \{1, 2, ..., K\}$ to denote the augmented step within a stage, and $t  \in \{1, 2, ... \}$ to denote the current stage. We omit the subscript $t$ where it is not pertinent.
	
	%The tuple $(t, k)$ thus denotes the $k^{th}$ step in the $t^{th}$ stage of an episode. Denoting the augmented state, augmented action, and augmented reward by $s^{\text{aug}}_{t,k}, a^{\text{aug}}_{t,k}, r^{\text{aug}}_{t,k+1}$ respectively, a stage can be represented by $(s^{\text{aug}}_{t,1}, a^{\text{aug}}_{t,1}, r^{\text{aug}}_{t,2}, s^{\text{aug}}_{t,2}, ...,  s^{\text{aug}}_{t,K}, a^{\text{aug}}_{t,K}, r^{\text{aug}}_{t,K+1})$.
	
	
	Our next step is to determine the form of the augmented transition. We can view the model search as tree traversal, with the tree's root node corresponding to the current real state $s$. We consider a simple method of traversing in a tree. At each imaginary step, the agent decides which child node to visit by imaginary action $\tilde{a} \in {\mathcal{A}}$ and also whether to reset the model to the root node by reset action $\tilde{a}^r \in \{0, 1\}$. In other words, the imaginary action unrolls the model by one step while the reset action sets the model back to the root node. We also impose a maximum search depth, $L$, in the algorithm. This ensures that the agent is forced to reset once its search depth exceeds $L$, as a learned model may not be accurate for a large search depth. An illustration of the tree traversal is shown in Figure \ref{fig:tree}.
	
	After the imaginary steps, the agent chooses a real action $\tilde{a} \in {\mathcal{A}}$ in a real step, where this real action is passed to the real MDP. We can merge both the imaginary and real actions, while interpreting the reset action as an additional one, thereby defining the augmented action space by ${\mathcal{A}}^\text{aug} \defeq \mathcal{A} \times \{0, 1\}$, and an augmented action by the tuple $a^{\text{aug}} \defeq (\tilde{a}, \tilde{a}^r)$. During imaginary steps, $\tilde{a}$ becomes the imaginary action, while $\tilde{a}^r$ decides whether to reset the current node. During real steps, $\tilde{a}$ becomes the real action, while $\tilde{a}^r$ is not used. The pseudocode can be found in Algorithm \ref{alg:1}. %Note that both the augmented transition ${P}^\text{aug}$ and the augmented initial distribution $d_0^\text{aug}$ are implicitly defined by the algorithm. %The remaining questions are how to design the augmented state $s^{\text{aug}}$ and the augmented reward $r^{\text{aug}}$ in the augmented MDP. 
	
	% Figure environment removed
	
	\subsection{Augmented State} \label{sec:aug_state}
	
	We then consider the design of the augmented state that is passed to the agent. One straightforward method involves concatenating all predicted states in the tree. At each real step, the $K-1$ predicted states serve as input, informing the selection of the real action. However, this method presents two significant challenges: (i) the high dimensionality of the predicted states makes learning to search and evaluate rollouts difficult, and (ii) the concatenation effectively multiplies the state's dimension by $K$, further complicating the learning of rollout summarization. To mitigate these issues, we propose a compact way of representing the tree in a low-dimensional vector. First, we consider how to encode a node compactly, followed by how to use these node representations to represent the tree. 
	
	To encode a node, we include the reward, the base policy and its value at the node. Moreover, we provide certain \emph{hints} that summarize the rollouts. Taking inspiration from MCTS, we consider three hints: mean rollout return, maximum rollout return, and visit count.
	
	For a node $i_0$ and its $n$-level descendant node $i_n$, we define the \emph{rollout return} from node $i_0$ to node $i_n$ through the path $(i_0, i_1, i_2, ..., i_n)$ as:
	\begin{equation}
		g_{i_0, i_n} \defeq \hat{r}_{i_0} + \gamma \hat{r}_{i_1} + \gamma^2 \hat{r}_{i_2} + ... \gamma^{n} \hat{r}_{i_{n}} + \gamma^{n+1} \hat{v}_{i_{n}}, \label{eq:rollout_return}
	\end{equation}
	where $\hat{v}_{i}$ denotes the value function applied on the node $i$ and $\hat{r}_i$ denotes the predicted reward upon reaching node $i$. That is, $\hat{v}_{i} \defeq \hat{v}(\hat{s}_i)$, and $\hat{r}_i \defeq \hat{R}(\hat{s}_{\text{parent\_node}(i)}, a_{\text{parent\_edge}(i)})$.
	
	For a given node $i$, we can compute $g_{i, j}$ for all its descendant nodes $j$ that are visited in the tree. We also define $g_{i, i} \defeq \hat{r}_{i} + \gamma \hat{v}_{i}$. We define the mean rollout return $g_{i}^{\text{mean}}$ and the maximum rollout return $g_{i}^{\text{max}}$ as the mean of $g_{i, j}$ and the maximum of $g_{i, j}$ over all descendant nodes $j$ (including itself) that are visited respectively. The mean and maximum rollout returns of a node's children are useful in guiding actions. For example, the mean rollout return of the child $i$ of a node $k$ gives a rough estimate of the average estimated return that can be collected following action $i$ from node $k$. The mean rollout return is also equivalent to $Q(s,a)$ used in the UCB formula of MCTS. We use $g_{\text{child}(i)}^{\text{mean}}$ and $g_{\text{child}(i)}^{\text{max}}$ to denote the vector of mean and maximum rollout returns for each child of a node $i$. %If the child node has not been visited, then the corresponding mean or maximum rollout return is set to $0$.
	
	The visit count of a node, which represents the number of times the agent traverses that node, may also be useful. For example, a child node that has never been visited may be worth exploring more. We use $n_{\text{child}(i)}  \in  \mathbb{R}^{|\mathcal{A}|}$ to denote the vector of visit count for each possible child of a node $i$.
	
	Altogether, we represent a node $i$ by a vector $u_i \in \mathbb{R}^{5|\mathcal{A}|+2}$, called \emph{node representation}, that contains:
	\begin{enumerate}
		\item $\hat{r}_i, \hat{v}_{i} \in \mathbb{R}, \hat{\pi}_i \in \mathbb{R}^{|\mathcal{A}|}$: The core statistics, composed of the reward, the value, and the base policy of the node $i$. Here, $\hat{\pi}_i \defeq (\hat{\pi}(s_i, a_1), \hat{\pi}(s_i, a_2), ..., \hat{\pi}(s_i, a_{|\mathcal{A}|}))$.
		\item $\text{one\_hot}(a_i) \in \mathbb{R}^{|\mathcal{A}|}$: The one-hot encoded action that leads to node $i$.
		\item $g_{\text{child}(i)}^{\text{mean}}, g_{\text{child}(i)}^{\text{max}},  \frac{1}{K} n_{\text{child}(i)} \in  \mathbb{R}^{|\mathcal{A}|}$: The hints, composed of mean rollout return, maximum rollout return and visit counts of the child nodes normalized by $K$.
	\end{enumerate}
	We only include the root node's representation $u_\text{root}$ and the current node's representation $u_\text{cur}$ to represent the tree (we use the subscript $_\text{root}$ to denote the root node, which is unchanged throughout a stage, and the subscript $_\text{cur}$ to denote the current node \emph{before reset}). The former guides the selection of real actions, while the latter guides the selection of imaginary actions. Besides $u_\text{root}$ and $u_\text{cur}$, we also add auxiliary statistics, such as the current step in a stage, in the representation. Further details can be found in Appendix \ref{app:alg}. Combined, we represent the tree with a fixed-size vector $u_\text{tree} \in \mathbb{R}^{10|\mathcal{A}|+K+9}$, called \emph{tree representation}. We may also include the real state $s$ or the current node's predicted state $\hat{s}_\text{cur}$ in the augmented state, providing a richer context for the agent.
	
	The tree representation is designed to simplify various planning procedures. In searching, the base policy and visit count help the identification of promising, yet unvisited rollouts. In evaluation, rather than simulating until a terminal state, value estimates can be utilized to assess the potential of a given state. In summarization, mean and maximum rollout returns consolidate the outcomes for the current node. Finally, in execution, the hints at the root node provide a compact summary of all rollouts. Compared to the concatenated predicted states, which have a dimension of $K|\mathcal{S}|$, the tree representation, typically with fewer than 100 dimensions, is much easier to learn from.
	
	The tree representation enables an agent to execute various common planning algorithms, such as $n$-step exhaustive search and MCTS, without requiring memory. Employing an agent with memory, such as an actor-critic algorithm with an RNN, may facilitate learning a broader class of planning algorithms, as the agent can access previous node representations. In principle, an agent with memory can learn to compute the hints or other ways of summarizing rollouts by itself.
	
	%Interestingly, experiments in Appendix \ref{app:ab} demonstrate that without providing hints, an RNN agent can achieve the same final performance but with slower initial learning, suggesting that agents can also learn to summarize rollouts independently.
	
	
	\subsection{Augmented Reward} \label{sec:aug_reward}
	
	To align the agent's goals in the augmented MDP with those in the real MDP, we adopt the following augmented reward function: no reward is given for imaginary steps, and the reward for real steps is identical to the real reward from the real MDP. In other words, ${r}^\text{aug}_k = 0$ for $k \leq K$ and ${r}^\text{aug}_{K+1} = r$, where $r$ denotes the real reward. We set the augmented discount rate $\gamma^{\text{aug}} \defeq \gamma^{\frac{1}{K}}$ to account for the $K-1$ imaginary steps preceding one real step, ensuring that the return in the augmented MDP equals the return in the real MDP.
	
	We also experimented with the use of an auxiliary reward, called the \emph{planning reward}, to mitigate the sparse reward in the augmented MDP. The core idea is to encourage the agent to maximize the maximum rollout return $g^\text{max}_\text{root}$, but we later found that it only provided a minimal increase in initial learning speed, indicating that the real rewards are sufficient for learning to plan. The details of the planning reward can be found in Appendix \ref{app:planning}.
	
	This completes the description of the augmented MDP. One may view an RL agent acting on this augmented MDP with a fixed base policy as performing one step of policy improvement in generalized policy iteration, but the policy improvement operator is learned instead of handcrafted. Experiments show that RL agents can indeed learn to improve the base policy (see Appendix \ref{app:pi}). As such, we only need to project the improved base policy back to the base policy space to complete the full cycle of generalized policy iteration, which will be discussed next.
	
	\subsection{Learning the World Model and the Base Policy} \label{sec:dual}
	
	Until now, we assume that we are provided (i) the model ($\hat{P}, \hat{R}$) and (ii) a base policy $\hat{\pi}$ and its value $\hat{v}$. To remove both assumptions, we propose employing a single, unified model that assumes the roles of $\hat{P}, \hat{R}, \hat{\pi}$, and $\hat{v}$, learning directly by fitting the real transitions. Specifically, we introduce a novel model, the \emph{dual network}, designed for this purpose.
	
	%Nonetheless, if the predicted state is inaccurate, the learned base policy and value function applied on it may also yield inaccurate outputs. As such, one may instead directly learn to predict the real action and values based on the root state and the action sequence, which is done in MuZero. However, this approach is data-inefficient, as the future states, which contains valuable information, are discarded when training the base policy. To solve this dilemna, we propose a novel model, called the \emph{dual network}, that unifies $\hat{P}, \hat{R}, \hat{\pi}, \hat{v}$.
	
	% Figure environment removed
	
	The dual network consists of two RNNs which we refer to as the \emph{state-reward network} and the \emph{value-policy network}. The state-reward network's input consists of the root state $s_t$ and an action sequence $a_{t+1:t+L}$, and the network predicts future states $\hat{s}_{t+1:t+L}$, rewards $\hat{r}_{t+1:t+L}$, and termination probability $\hat{p}^d_{t+1:t+L}$. Meanwhile, the value-policy network takes both the state-reward network's input and its predicted states $\hat{s}_{t+1:t+L}$ as inputs, and the network predicts future policies $\hat{\pi}_{t:t+L}$ and values $\hat{v}_{t:t+L}$.  We can also view the dual network as an RNN with $h=(h^{sr}, h^{vp})$ being the hidden state, where $h^{sr}$ and $h^{vp}$ denote the hidden states of the two sub-RNNs. The relevant statistics required in the augmented state can be computed by unrolling the dual network using the root node and the action sequence leading to the current node.
	
	To train the model, we first store all real transitions $(s_t, a_t, r_{t+1}, d_{t+1})$ in a replay buffer, which corresponds to the state, action, reward, and termination indicator. We then sample a sequence of real transitions with length $L+1$ from the buffer. The two sub-RNNs in the dual network are trained separately. For the state-reward network, the following loss is used:
	\begin{equation}
		\mathcal{L}^{sr} \defeq \sum_{l=1}^L \left( c^r (\hat{r}_{t+l} - r_{t+l})^2 + c^d d_{t+l} \log \hat{p}^d_{t+l}  +  c^s \mathcal{L}^{\text{state}}(\hat{s}_{t+l}, s_{t+l})  \right), \label{eq:sr}
	\end{equation}
	where $c^r, c^d, c^s \geq 0$ are hyperparameters that modulate the loss strength. While a straightforward choice for the state prediction loss metric, $\mathcal{L}^{\text{state}}$, is the L2 loss – defined as $\mathcal{L}^{\text{state}}(s', s) = \vert \vert s' - s \vert \vert^2_2$ – this approach could cause the model to allocate capacity towards predicting non-essential features. To address this issue, we suggest using the feature loss instead of the L2 loss. Specifically, $\mathcal{L}^{\text{state}}(s', s) = \vert \vert g(s') - g(s) \vert \vert^2_2$, where $g$ represents the encoding function of the value-policy network. As this encoding function concentrates on features relevant to action, the feature loss encourages the model to prioritize predicting the most crucial features.%\footnote{Since $g$ is part of the value-policy network, its parameters are excluded from the state-reward network's optimizer. Consequently, $g$ remains unoptimized during the minimization of the feature loss.}
	
	For the value-policy network, the following loss is used:
	\begin{equation}
		\mathcal{L}^{vp}  \defeq \sum_{l=0}^L \left( c^v (\hat{v}_{t+l} - \hat{v}^{\text{target}}_{t+l})^2 + c^{\pi} \text{one\_hot}(a_{t+l})^T \log \hat{\pi}_{t+l}  \right), \label{eq:vp}
	\end{equation}
	where $c^v, c^\pi \geq 0$ are hyperparameters and $\hat{v}^{\text{target}}_{t+l}$ denotes the multi-step return. One may also use the real action distribution $\pi_t$ place of $\text{one\_hot}(a_{t+l})$ for a better learning signal, though this requires passing the real action distribution instead of only the actions to the augmented MDP.
	% \footnote{Since the optimizer of the value-policy network does not include the parameters of the state-reward network, the gradient cannot propagate through $\hat{s}_{t+l}$ to the state-reward network, effectively meaning that we apply a stop gradient to all $\hat{s}_{t+l}$.}
	
	%It should be noted that the training of the dual network takes place within the augmented MDP. Whenever an agent executes a real transition in the augmented MDP, this transition is stored in a buffer. Subsequently, the world model is trained for a fixed number of steps before the augmented MDP returning the next state and rewards.
	
	The design choice of the dual network aims to improve data efficiency while also ensuring inaccurately predicted states do not negatively affect the learning of the base policy and values. If the predicted states are inaccurate, the value-policy network, being an RNN, can learn to ignore these states, relying solely on the root state and action sequence. Conversely, if the predicted states are accurate, the value-policy network can effectively learn to make predictions based on these states. Consider, for instance, the task of predicting the value after executing several actions in Sokoban. If the predicted state suggests the level is close to completion, the predicted value should naturally be higher. Although, in theory, a standalone RNN might autonomously learn this intermediary step, in practice, it would likely require much more data. We explore the relationship of this dual network with models from other model-based RL algorithms in Appendix \ref{app:misc}.
	
	With the dual network, the augmented state can also include the hidden state of the model at either the current node, $h_\text{cur}$, or the root node, $h_\text{root}$. These hidden states, usually in much lower dimensions, might be easier to process than the real state $s$ or the current node's predicted state $\hat{s}_\text{cur}$. In our experiments, we use $(u_\text{tree}, h_\text{cur})$ as the augmented state, based on the intuition that the current hidden state is likely the most informative among the others.\footnote{Strictly speaking, using $(u_\text{tree}, h_\text{cur})$ as the augmented state leads to a POMDP instead of an MDP. One can recover an MDP by omitting the auxiliary hints in $u_\text{tree}$ and adding $s_\text{root}$ to the augmented state, but both do not help performance in our experiments.}  This completes the specification of the Thinker algorithm, which takes an MDP as input and transforms it into another MDP.
	
	\section{Experiments}
	For all our experiments, we train a standard actor-critic algorithm, specifically the IMPALA algorithm \cite{espeholt2018impala}, on the Thinker-augmented MDP. Although other RL algorithms could be employed, we opted for the IMPALA due to its computational efficiency and simplicity. The actor-critic's network uses an RNN for encoding the tree representation and a convolutional neural network for encoding the model's hidden state. These encoded representations are concatenated, then passed through a linear layer to generate actions and predicted values. We set the stage length, $K$, to $20$, and the maximum search depth or model unroll length, $L$, to $5$. More experiment details can be found in Appendix \ref{app:exp}.
	
	\subsection{Sokoban}
	
	% Figure environment removed
	
	We selected the game of Sokoban \cite{botea2003using, weber2017imagination}, a classic puzzle problem, as our primary testing environment due to its inherent complexity and requirement for extensive planning. In Sokoban, the objective is to push all four boxes onto the designated red target squares, as depicted in Figure \ref{fig:vis}. We used the unfiltered dataset comprising 900,000 Sokoban levels from \cite{guez2019investigation}. We train our algorithm for 5e7 frames, while baselines in the literature typically train for at least 5e8 frames. Despite this discrepancy, we present learning curves plotted against frame count for direct comparison.
	
	%A particular challenge in Sokoban is the possibility of rendering a game state irreversible by pushing a box against a wall, given the agent's inability to pull boxes.
	
	The learning curve for the actor-critic algorithm applied to the Thinker-augmented MDP is depicted in Fig. \ref{fig:baseline_sokoban}, with results averaged across five seeds. We include seven baselines: DRC \cite{guez2019investigation}, Dreamer-v3 \cite{hafner2023mastering}, MuZero \cite{schrittwieser2020mastering}, I2A \cite{weber2017imagination}, ATreeC \cite{farquhar2017treeqn}, VIN \cite{tamar2016value}, and IMPALA with ResNet \cite{espeholt2018impala}. In Sokoban, DRC stands as the current state-of-the-art algorithm, outperforming others by a significant margin, likely due to its well-suited prior for the game. The `DRC (original paper)' result is sourced directly from the paper, whereas `DRC' denotes our replicated results.
	
	Thinker surpasses the other baselines in terms of performance in the first 5e7 frames. At 2e7 frames, Thinker solves 88\% of the levels, while DRC (original paper) solves 80\% and the other baselines solve at most 21\%. At 5e7 frames, Thinker solves 95 \% of the levels, while DRC (original paper) solves 93\% and the other baselines solve at most 45\%. These results underscore the enhanced planning capabilities afforded by the Thinker-augmented MDP to an RL agent.
	
	To understand the benefits of planning, we evaluate a policy trained with $K=20$ on the Thinker-Augmented MDP, by testing its performance with different $K$ values in the same augmented environment during the testing phase. This variability allows us to control the degree of planning and the result is shown in Figure \ref{fig:main_sokoban_infK}. The figure also depicts the performance of the base policy, which can be regarded as a distilled policy devoid of planning. We observe a significant performance improvement attributable to planning, even at the end of training.
	
	The agent's behavior, visualized in Figure \ref{fig:vis}, illustrates that it learns to use the model for selecting better actions. Interestingly, it appears that the agents learn a planning algorithm that diverges from traditional $n$-step-exhaustive search and MCTS. For instance, the agent chooses real actions based on both visit counts and rollout returns, and the agent learns to reset upon entering an irreversible state, contrasting the MCTS strategy of resetting at a leaf node. Further analysis on the trained agent's behavior can be found in Appendix \ref{app:beh}.
	%To facilitate further exploration and understanding of the planning algorithm learned by the agent, we will be releasing the trained network for public access. Future studies could delve deeper into the analysis of the learned planning algorithm.
	
	% Figure environment removed
	
	% Figure environment removed
	
	We conducted an ablation analysis to examine the impact of planning duration, varying the stage length $K$ across $\{1, 2, 5, 10, 20\}$ during both training and testing. When $K=1$, the agent does not interact with the model at all. The outcomes of this ablation analysis are presented in Figure \ref{fig:main_sokoban_K}. The results suggest that $K=10$ already gives the optimal performance. This is in stark contrast to the 2000 simulations required by MCTS to achieve good performance in Sokoban \cite{guez2018learning}. 
	
	As a comparison, we train the same actor-critic algorithm on the raw MDP, employing a similar network architecture used by the model to encode the raw state. Surprisingly, the result for $K=1$ is worse than the raw MDP. A detailed explanation of the reasons behind this phenomenon can be found in Appendix \ref{app:exp}. However, employing a marginally larger $K$, such as 2, greatly improves the performance on the Thinker-augmented MDP, surpassing that of the raw MDP.
	
	%We conjecture the reason is that the model's hidden state is included in the augmented state instead of the real state. While this hidden state may be more effective for predicting future states and rewards, it can be more difficult for learning a good policy. 
	
	We use the dual network as the world model, trained with feature loss in state prediction. We consider several variants of the model, including (i) a dual network that uses L2 loss in state prediction, (ii) a dual network that has no loss in state prediction, (iii) a single network that uses L2 loss in state prediction, (iv) a single network that does not predict state. The single network refers to a single RNN that predicts the relevant quantities, and (iv) is equivalent to MuZero's model. The results of these four variants are shown in Figure \ref{fig:ab_model}. We observe that both the dual network architecture and the state loss are critical to performance. However, the use of feature loss instead of L2 loss gives marginal performance improvement, likely because the L2 loss' prior that each pixel is equally important suits well in Sokoban. Nonetheless, this prior may not be suitable for other games like Breakout.
	
	Further ablation studies in Appendix \ref{app:ab} offer insights into the various components of Thinker, including the role of hints in tree representation, the agent's memory, base policy, planning reward, etc. For example, either the hints \emph{or} an RNN agent is sufficient to achieve good performance, as the hints summarize the rollouts, and an agent with memory can learn to summarize rollouts on its own. However, if both hints and memory are omitted, the agent fails to learn to plan, as it cannot recall past rollouts. In addition, if the base policy and its value are also omitted, then an RNN agent still fails to learn, suggesting the necessity of a heuristic in learning to plan.
	
	\subsection{Atari 2600}
	
	Finally, we test our algorithm on the Atari 2600 benchmark \cite{bellemare2013arcade} using a 200M frames setting. The performance of the actor-critic algorithm on both the Thinker-augmented MDP and raw MDP is evaluated using the same hyperparameters as in the Sokoban experiment, but with a deeper neural network for the model and an increased discount rate from 0.97 to 0.99.
	
	The learning curve, measured in the median human-normalized score across 57 games, is displayed in Figure \ref{fig:main_atari}. We evaluate the agent in the no-ops starts regime at the end of the training, and the learning curve is shown in Fig \ref{fig:main_atari}. The median (mean) normalized score for the actor-critic algorithm applied on the Thinker-augmented MDP and the raw MDP are 261\% (1372\%) and 102\% (514\%), respectively, underscoring the advantages of the Thinker-augmented MDP over the raw MDP. For context, the median (mean) normalized score of Rainbow \cite{hessel2018rainbow}, a robust baseline, is 223\% (868\%). %It is important to note, however, that any RL algorithms, including Rainbow, could be applied to the Thinker-augmented MDP, so this comparison may not be entirely indicative. As the goal of the experiments is to investigate the benefits of the augmented MDP, we do not include other common baselines here.
	%\footnote{The Rainbow results presented here are taken from the original Rainbow paper \cite{hessel2018rainbow}, where the reported score is obtained from the best agent snapshots throughout training.}. 
	%\footnote{The difference between the median normalized score for the raw MDP and the result reported in \cite{guez2019investigation} can be mainly attributed to two factors: (i) we did not use greyscale in the environment pre-processing, and (ii) we used a deeper neural network. Both of these factors increase the difficulty of learning. }
	
	The advantages of the Thinker algorithm are particularly pronounced in certain types of games, especially shooting games such as \texttt{chopper command}, \texttt{seaquest}, and \texttt{phoenix}. It is plausible that the agents benefit from predicting the trajectories of fast-moving objects in these shooting games. However, in some games, particularly those where a single action does not produce significant effects, the Thinker algorithm appears to offer no advantages. For instance, in the game \texttt{qbert}, the decision to move to a different box is made only once every five or more actions. This effectively transforms any 5-step plan into a single-step plan, diminishing the algorithm's benefits. Detailed experiment results on the Atari 2600 are available in Appendix \ref{app:exp}.
	
	Many interesting behaviors learned by the agent are visualized in \url{https://youtu.be/0AfZh5SR7Fk}. We also observe that the predicted frames in the video are of high quality, showing that the feature loss can lead to the emergence of interpretable and visualizable predicted frames, despite the absence of an explicit loss in the state space. We attribute this capability to using a convolutional mapping $g$ in the feature loss, as convolutions can preserve local features.
	
	\section{Future Work and Conclusion}
	While the Thinker algorithm offers many advantages, it also presents several limitations that provide potential areas for future research. Firstly, the Thinker algorithm carries a large computational cost. A single step in the original MDP corresponds to $K$ steps in the augmented MDP, in addition to the overhead of model training. Secondly, the algorithm currently enforces rigid planning, requiring the agent to roll out from the root state and restricts it to planning for a fixed number of steps. Thirdly, the algorithm employs a deterministic model to facilitate tree traversal, which may not work well for a stochastic environment.
	
	Future work could focus on addressing these limitations, such as enabling more flexible planning steps and integrating uncertainty into the model. Additionally, exploring the application of the algorithm in multi-environment settings is a promising direction. We hypothesize that the advantages of a learned planning algorithm over a handcrafted one will become more pronounced in such scenarios. Exploring how other RL algorithms perform within the Thinker-augmented MDP is an additional direction for future work. 
	
	The history of machine learning research tells us that learned approaches often prevail over handcrafted ones. This transition becomes especially pronounced when large amounts of data and computational power are available. In the same vein, we surmise that learned planning algorithms will eventually surpass handcrafted planning algorithms in the future.
	
	\clearpage
	\section*{Acknowledgement}
	This work was performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (www.csd3.cam.ac.uk), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital grant EP/T022159/1), and DiRAC funding from the Science and Technology Facilities Council (www.dirac.ac.uk).
	
	We note that author Ivan Anokhin has since moved to the Mila, Université de Montréal, after completing the primary research reflected in this paper at the University of Cambridge.
	
	\bibliographystyle{unsrt}
	\bibliography{citation}
	
	\clearpage
	\appendix
	\section{Algorithm} \label{app:alg}
	
	\begin{algorithm}
		\caption{Transition in Thinker-Augmented MDP}
		\label{alg:1}
		\begin{algorithmic}[1]
			\State \textbf{Input:} real MDP: \textit{env}, model: $\hat{P}, \hat{R}$, base policy and its value function: $\hat{\pi}, \hat{v}$.;
			\State \textbf{Algorithm Parameter:} stage length $K \geq 1$, maximum search depth $L \geq 1$.
			\State \textbf{Initialize $k \leftarrow K$}    \Comment {Ensures that the first step is a real step}
			\Procedure{Step}{$\tilde{a}, \tilde{a}^r$} \Comment{$\tilde{a}$ denotes imaginary/real action, $\tilde{a}^r$ denotes reset action}	
			\If{$k = K$}   \Comment{Real step}	
			\State $k, l \leftarrow 1, 0$ \Comment {Initialize step number and search depth}
			\State $r, s, d \leftarrow env.step(\tilde{a})$ \Comment{Compute reward, state and termination for the real MDP}	 \label{line:rstep}   
			\State $s_\text{root} \leftarrow s$ \Comment{Record the root node}		\label{line:root}
			\State $s_\text{cur} \leftarrow s_\text{root}$ \Comment{Set the current node to be the root node}			
			\State $\hat{r}_\text{root} \leftarrow 0$ \Comment{No predicted rewards for the root node}		
			\Else \Comment {Imaginary step}
			\State $k, l \leftarrow k + 1, l + 1$	
			\If{$l \geq L$}   \Comment{Check if search depth exceeds maximum search depth $L$}	
			\State $\tilde{a}^r \leftarrow 1$   \Comment{Override reset}	
			\EndIf				
			\State $\hat{r}_\text{cur} \leftarrow \hat{R}(s_\text{cur}, \tilde{a})$	 \Comment {Compute the predicted reward}  \label{line:r}
			\State $s_\text{cur} \leftarrow \hat{P}(s_\text{cur}, \tilde{a})$	 \Comment {Compute the new current node by unrolling the model}             \label{line:unroll_p}
			\label{line:unroll_r}		
			\State $r \leftarrow 0$ \Comment{No rewards for imaginary steps}
			\State $d \leftarrow False$ \Comment{No termination for imaginary steps}
			\EndIf		
			\State $\hat{\pi}_\text{cur}, \hat{v}_\text{cur} \leftarrow \hat{\pi}(s_\text{cur}), \hat{v}(s_\text{cur})$	 \Comment {Compute the base policy and its value} \label{line:vp}
			\State Compute $s^\text{aug}$ using $s_i, \hat{r}_i, \hat{\pi}_i, \hat{v}_i$ where $i \in \{\text{root}, \text{cur}\}$ as described in Section \ref{sec:aug_state}            
			\If{$\tilde{a}^r = 1$}   \Comment{Reset to the root node if reset is triggered}
			\State $l \leftarrow 0$				
			\State $s_\text{cur} \leftarrow s_\text{root}$
			\EndIf		
			\State \textbf{return} $r, s^\text{aug}, d$ \Comment {Return rewards, state, and termination for the augmented MDP}
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	
	Algorithm \ref{alg:1} implements the step function of the Thinker-augmented MDP, assuming we have access to a given model $(\hat{P}, \hat{R})$, a base policy $\hat{\pi}$, and its value $\hat{v}$. We assume $\hat{P}$ is deterministic and use $s' = \hat{P}(s,a)$ to denote the predicted next state $s'$ from state $s$ and action $a$.
	
	To compute the hints (i.e.\ mean rollout return, max rollout return, and visit counts) in the tree representation, we need to maintain a tree with each node storing the predicted reward $\hat{r}_i$, value $\hat{v}_i$, and visit count $n_i$, but the details of updating the tree are omitted in the pseudocode for clarity.
	
	It is important to note that since $\hat{P}$ is deterministic, one can store the computed node in a buffer to avoid re-computing it when the agent traverses the same node again within the same stage.
	
	In the full Thinker-augmented MDP, both the model and the base policy with its values are learned by an RNN model. The code only needs slight modifications as follows:
	
	\begin{enumerate}
		\item After line \ref{line:rstep}, we insert the real transition in the form of $(s_\text{old}, \tilde{a}, r, d)$, where $s_\text{old}$ denotes the previous real state, into a replay buffer. A parallel thread trains the model by sampling from the replay buffer.
		\item A node is represented by the hidden state of the RNN model, not the real state. Consequently, line \ref{line:root} should initialize the root node with the first hidden state $h$ of the RNN model. Line \ref{line:unroll_p} updates the new current node to the new hidden state by unrolling the RNN model with the current node and the latest action. Lines \ref{line:r} and \ref{line:vp} compute the reward, policy, and value as predicted by the RNN model.
	\end{enumerate}
	
	\paragraph{Implementation details} Several key implementation details are worth noting. First, when transitioning into a new stage, we carry forward the tree from the previous stage and prune the non-descendant nodes of the new root node. This approach ensures the continuity of the previous plan into the subsequent stages. However, our experiments show that carrying the tree has minimal impact on performance. Second, for clarity, the planning reward is not shown in the pseudocode, but details can be found in Appendix \ref{app:planning}. Lastly, if the model predicts episode termination at node $i$, defined by $\hat{p}^d_i > 0.5$, we manually set the content of all its descendant nodes $j$ to $\hat{r}_j=0$, $\hat{\pi}_j=\hat{\pi}_i$, $\hat{v}_j=0$, $\hat{s}_j = \mathbf{0}$, and ${h}_j = {h}_i$, which represents the terminal state that is absorbing and has zero rewards and values.
	
	\subsection{Augmented State}
	As discussed in Section \label{sec:aug_state}, both the current node representation $u_\text{cur}$ and the root node representation $u_\text{root}$ are included in the tree representation. Denoting the current search depth by $d \in \{0, 1,..., L\}$, the full tree representation $u_\text{tree} \in \mathbb{R}^{10|\mathcal{A}|+K+9}$ contains:
	
	\begin{enumerate}
		\item $u_\text{root}, u_\text{cur} \in \mathbb{R}^{5|\mathcal{A}|+2}$: The root node representation and the current node representation.
		\item $g_{\text{root}, \text{cur}} \in  \mathbb{R}$: Current rollout return.
		\item $g_{\text{root}, \text{cur}} - \gamma^{d+1} \hat{v}_{\text{cur}} \in  \mathbb{R}$: Sum of discounted rewards collected in the current rollout.
		\item $g_{\text{root}}^{\text{max}} \in \mathbb{R}$: Maximum rollout return of the root node.
		\item $\gamma^{d+1} \in \mathbb{R}$: Trailing discount factor.
		\item $\text{one\_hot}(k) \in [0,1]^K$: Current step in a stage, represented in one-hot encoding.
		\item $i^{\text{reset}}  \in [0,1]$: Indicator of whether the current node has just been reset to the root node.
	\end{enumerate}
	
	%As an agent can learn to ignore non-useful inputs, it is better to include all information that is deemed relevant. This underscores the advantage of learning to plan. Unlike handcrafted planning algorithms, there is no need to meticulously design which statistics to use or how they should be applied.
	
	And the following five inputs can be included in the augmented state $s^\text{aug}_{t,k}$: 
	\begin{enumerate}
		\item $u_\text{tree} \in \mathbb{R}^{10|\mathcal{A}|+K+9}$: The tree representation.
		\item $s \in \mathcal{S}$: The real state underlying the stage.
		\item $\hat{s}_\text{cur} \in \mathcal{S}$: The predicted real state of the current node.
		\item $h_\text{root} \in \mathcal{H}$: The hidden state of the RNN model at the root node.
		\item $h_\text{cur} \in \mathcal{H}$: The hidden state of the RNN model at the current node.
	\end{enumerate}
	We used $s^\text{aug}_{t,k} = (u_\text{tree}, h_\text{cur})$ in our main experiments. Our code enables users to decide which inputs to include as part of the augmented state in the Thinker-augmented MDP.
	
	\subsection{Code} 
	The complete code is accessible at \url{https://github.com/stephen-chung-mh/thinker}. We implemented the Thinker-augmented MDP in batches using Cython \cite{behnel2011cython}, a Python language extension that optimizes performance by interfacing with C. This ensures that the primary computational burden is attributed to model training and the RL algorithm, rather than the logic flow within the augmented MDP. The code supports multi-thread processing, allowing multiple threads of Thinker-augmented MDP to share the same model.
	
	Our implementation of the Thinker-augmented MDP aligns with the OpenAI Gym interface \cite{brockman2016openai}, facilitating easy integration with existing RL frameworks. The code allows customization of various parameters such as stage depth $K$, maximum search depth $L$, and the model's learning rate, among others. Included in the repository is the code for the IMPALA algorithm applied to the Thinker-augmented MDP, reproducing the experimental results presented in the paper. 
	
	\clearpage
	\section{Model Training Procedure} \label{app:model}
	
	\subsection{Collection of Training Samples}
	
	A transition tuple $x_t \defeq (s_t, a_t, r_{t+1}, d_{t+1})$ is stored in a replay buffer during each real step, which corresponds to the state, action, rewards, and termination indicator. The replay buffer is a first-in-first-out buffer with a capacity of 200,000 transitions. To train the model, a batch of sequences $x_{i:i+2L}$ is randomly sampled from the buffer, where $L$ represents the model rollout length. We use a batch size of 128 and a model unroll length $L=5$.
	
	Consistent with MuZero \cite{schrittwieser2020mastering}, prioritized sampling \cite{horgan2018distributed} is used for sampling the sequence $x_{i:i+2L}$. Each transition tuple is assigned a priority $p_i$. Newly added transition tuples are given the maximum priority value in the buffer. During training, the priority $p_i$ of the first transition in a sampled sequence $x_{i:i+2L}$ is updated to the L1 loss of the value, $\vert \hat{v}_i - v^{\text{target}}_i \vert$. Transitions are sampled with a probability of $P(i) = \frac{p_i^\alpha}{\sum_j p_j^\alpha}$, where $\alpha=0.6$. Each sampled transition sequence's loss is weighted by the importance weight $w_i = ( \frac{1}{N P(i)} )^\beta$, where $N$ is the total number of transition tuples in the buffer, and $\beta$ is annealed from $0.4$ to $1.0$ throughout training.
	
	We employ a replay ratio of $6$, implying that model training is paused if the ratio of the total number of transitions trained on the model to the total collected transitions exceeds $6$.
	
	\subsection{Additional Details for Training the Model} 
	
	The state-reward network and the value-policy network are trained separately, which leads to the following implications. First, when training the state-reward network, the $g$ within feature loss for state prediction is not optimized since it belongs to the value-policy network. This prevents $g$ from degeneration. Second, when training the value-policy network, gradients cannot flow through $\hat{s}_{t+l}$ towards the state-reward network since  $\hat{s}_{t+l}$ is considered as the input to the value-policy network. This ensures that the predicted state aligns with the actual future state.
	
	The multi-step return $\hat{v}^{\text{target}}_{t+l}$ in Equation \ref{eq:vp} is defined by:
	\begin{equation}
		\hat{v}^{\text{target}}_{t+l} \defeq r_{t+l+1} + \gamma r_{t+l+2} + ... + \gamma^{2L-l} r_{t+2L} + \gamma^{2L-l} g^{\text{mean}}_{\text{root}, t+2L}.
	\end{equation}
	Instead of using the predicted value $\hat{v}_{t+2L}$, we opted for the mean rollout return (at the end of the stage) $g^{\text{mean}}_{\text{root}, t+2L}$ as the bootstrap target\footnote{$g^{\text{mean}}_{\text{root}}$ does not contain the reward upon arriving the root node, as $\hat{r}_\text{root}$ is manually set to $0$ in Algorithm \ref{alg:1}.}. We found that it yields more stable training in our experiments. This stability likely stems from averaging across multiple rollouts starting from $s_{t+2L}$, rather than relying directly on the predicted value.
	
	\clearpage
	\section{Network Architecture} \label{app:nn}
	
	In this section, we describe the neural networks that we use for the model and the actor-critic network.
	
	\subsection{Model} 
	
	For Sokoban, the shape of the real state, $s_t$, is (3, 80, 80), while for Atari 2600, the real state's shape is (12, 80, 80) due to the stacking of four frames together. Before inputting the real state $s_t$ to the encoder, we concatenate it with the one-hot encoding of the action $a_{t-1}$, tiled to (80, 80), channel-wise. The encoder architecture is the same for both the state-reward and value-policy networks, structured as follows:
	
	\begin{itemize}
		\item Convolution with 64 output channels and stride 2, followed by a ReLu activation.
		\item $D$ residual blocks, each with 64 output channels.
		\item Convolution with 128 output channels and stride 2, followed by a ReLu activation.
		\item $D$ residual blocks, each with 128 output channels.
		\item Average pooling operation with stride 2.
		\item $D$ residual blocks, each with 128 output channels.
		\item Average pooling operation with stride 2.
	\end{itemize}
	All convolutions use a kernel size of 3. The resulting output shape for both Sokoban and Atari 2600 is (128, 6, 6). We use $D=1$ for Sokoban and $D=2$ for Atari 2600. 
	
	We denote $g^{sr}(s_t)$ and $g(s_t)$ as the encoder's output of $s_t$ for the state-reward network and the value-policy network respectively, and the dependency on $a_{t-1}$ is omitted for clarity. 
	
	For the state-reward network, we define the hidden state $h^{sr}_t \defeq g^{sr}(s_t)$. To obtain $h^{sr}_{t+l+1}$ from $(h^{sr}_{t+l}, a_{t+l})$ with $l \geq 0$, we first concatenate $h^{sr}_{t+l}$ with the one-hot encoding of the action $a_{t+l}$, tiled to (6, 6), channel-wise. This concatenated output is passed to the unrolling function, which consists of $4D+1$ residual blocks, each having an output channel of 128. The output of the unroll function is $h^{sr}_{t+l+1}$.
	
	For the value-policy network, in addition to the previous hidden state, the unrolling function should also take the encoded predicted frame $g(\hat{s}_{t+l})$ as input, so the unrolling procedure is slightly modified. First, we define $h^{vp}_{t-1}$ as a zero vector of shape (128, 6, 6). To obtain $h^{vp}_{t+l}$ from $(h^{vp}_{t+l-1}, a_{t+l-1}, g(\hat{s}_{t+l}))$ with $l \geq 0$ (replace $\hat{s}_{t+l}$ with $s_{t+l}$ when $l=0$ here), we first concatenate all three together channel-wise, with $a_{t+l-1}$ tiled to (6, 6). This concatenated output is passed to the unrolling function, which consists of $4D+1$ residual blocks, each having an output channel of 128. The output of the unroll function is $h^{vp}_{t+l}$. %Note that the computation of $h^{vp}_{t}$ requires the use of unrolling function, which differs from that of  $h^{sr}_t$.
	
	To compute the predicted quantities, we apply a prediction function to $h_{t+l}$ for both state-reward and value-policy networks. The predicted quantities for the state-reward network are rewards $\hat{r}_{t+l}$ and termination probability $\hat{p}^{d}_{t+l}$, while the predicted quantities for the value-policy network are values $\hat{v}_{l}$ and policies $\hat{\pi}_{l}$. The architecture of this prediction function is the same for both networks and is structured as follows:
	
	\begin{itemize}
		\item Convolution with 64 output channels and stride 1, followed by a ReLu activation.
		\item Convolution with 32 output channels and stride 1, followed by a ReLu activation.
		\item Flatten the 3D vector to a 1D vector.
		\item Separate linear layers for each predicted quantity.
	\end{itemize}
	
	In the case of probability outputs, namely the predicted termination probability $\hat{p}^d_{t+l}$ and the predicted policy $\hat{\pi}_{t+l}$, we pass the output of the linear layer through a softmax layer to ensure a valid probability distribution. In addition, the weights and biases of the final linear layers are initialized to zero to stabilize initial training.
	
	For the state-reward network, we compute the predicted frame, $\hat{s}_{t+l}$ for $l \geq 1$, by applying a decoder on $h^{sr}_{t+l}$ for $l \geq 1$. The decoder's structure is as follows:
	
	\begin{itemize}
		\item $D$ residual blocks, each with 128 output channels.
		\item ReLu activation followed by a transpose convolution with 128 output channels and stride 2.
		\item $D$ residual blocks, each with 128 output channels.
		\item ReLu activation followed by a transpose convolution with 128 output channels and stride 2.
		\item ReLu activation followed by a transpose convolution with 64 output channels and stride 2.
		\item $D$ residual blocks, each with 64 output channels.
		\item ReLu activation followed by a transpose convolution with 3 output channels and stride 2.
	\end{itemize}
	All convolutions use a kernel size of 4. The decoder outputs a tensor of shape $(3, 84, 84)$. In Atari, we concatenate this output with the three most recent true or predicted frames in $s_{t+l-1}$ for $l=1$ or $\hat{s}_{t+l-1}$ for $l > 1$, resulting in a final decoded predicted state with shape $(12, 84, 84)$. This approach is adopted to prevent the model from redundantly replicating previous inputs, as the prior three frames are already present in the input or have been predicted in preceding steps.
	
	\subsection{Actor-critic} 
	
	The augmented state consists of the model's hidden states $h = [h^{sr}_k,  h^{vp}_k]$, a 3D vector of shape $(256, 6, 6)$, and the tree representation $u_\text{tree}$, a flat vector of shape $(10  \vert \mathcal{A} \vert + K + 9)$. The model's hidden state is processed by a convolutional neural network (CNN), structured as:
	
	\begin{itemize}
		\item Convolution with 64 output channels and stride 1, followed by a ReLu activation.
		\item $2$ residual blocks, each with 64 output channels.
		\item Convolution with 64 output channels and stride 1, followed by a ReLu activation.
		\item $2$ residual blocks, each with 64 output channels.
		\item Convolution with 32 output channels and stride 1, followed by a ReLu activation.
		\item $2$ residual blocks, each with 32 output channels.
		\item Flatten the 3D vector to a 1D vector.
		\item A linear layer with output size 256, followed by a ReLu activation.
	\end{itemize}
	
	The tree representation is processed by a special recurrent neural network (RNN) architecture, which combines the Long Short-Term Memory (LSTM) \cite{hochreiter1997long} with the attention module of the Transformer \cite{vaswani2017attention}. This RNN architecture is necessitated by the requirements of the Thinker-augmented MDP, which relies on long-term memory capabilities for plan execution. For instance, a 5-step plan in the augmented MDP requires the agent to maintain the plan in memory for $5K=100$ steps, a demand that exceeds the capabilities of a traditional LSTM. However, using Transformers solely could lead to instability in RL \cite{parisotto2020stabilizing} due to their disregard of the Markov property in MDP as they process inputs across all steps in the same manner. Therefore, our approach merges the LSTM with the attention module of the Transformer to allow for extended memory access while preserving the stability of the training process.
	
	In detail, the RNN processes the tree representation as follows:
	
	\begin{itemize}
		\item A linear layer with output size 128, followed by a ReLu activation.
		\item A LSTM-Attention network (explained in detail below) with output size 128.
		\item A linear layer with output size 128, followed by a ReLu activation.
	\end{itemize}
	
	Finally, the input for the final layer is obtained by concatenating the following:
	
	\begin{enumerate}
		\item The output from the CNN processing the model's hidden state.
		\item The output from the RNN processing the tree representation.
		\item The one-hot encoding of the last actions (real action, imaginary action, and reset action).
		\item The last rewards (both the real reward and the planning reward).    
	\end{enumerate}
	
	This input is passed to separate linear layers to generate the final output of the actor-critic network, which includes predicted values and logits of the policies for real actions, imaginary actions, and reset actions. We use a separate head for computing the policies for real and imaginary actions, and select the respective logits based on whether the current step is a real step or an imaginary step. To generate valid probabilities, the logits predicted by the linear layers are fed into a softmax layer. Additionally, an L2 regularization cost of 1e-9 is added for the final layer's input and 1e-6 for the real action's logits.
	
	\paragraph{LSTM-Attention Cell} This section details the LSTM-Attention cell, which serves as the building block of the RNN. The notation here is distinct and should not be conflated with the notation in other sections. The LSTM-Attention cell, while akin to a conventional LSTM, incorporates an additional input from the attention module, multiplied by a gate variable. The LSTM-Attention cell is denoted by:
	\begin{equation}
		(h_{t}, c_{t}, K_{t}, V_{t}) = \text{LSTM\_Attn}(x_{t}, h_{t-1}, c_{t-1}, K_{t-1}, V_{t-1}),
	\end{equation}
	where $x_t$ denotes the input to the cell, $h_t$ denotes the hidden state, $c_t$ denotes the input activation, $K_{t}$ and $V_{t}$ denotes the key and value vector for the attention module, respectively. The function $\text{LSTM\_Attn}$ is defined as:
	\begin{align}
		f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
		i^c_t &= \sigma(W_{ic} x_t + U_{ic} h_{t-1} + b_{ic}), \\
		i^a_t &= \sigma(W_{ia} x_t + U_{ia} h_{t-1} + b_{ia}), \label{eq:lstm_ia}\\
		o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o), \\
		a_t, K_{t}, V_{t} &= \text{attn}(x_t, , K_{t-1}, V_{t-1}), \label{eq:lstm_a} \\
		\tilde{c}_t &=  \text{tanh}(W_c x_t + U_c h_{t-1} + b_c), \\
		c_t &= f_t \odot c_{t-1} + i^c_t \odot \tilde{c}_t + i^a_t \odot \text{tanh}(a_t)  \label{eq:lstm_c} \\
		h_t &= o_t \odot \text{tanh}(c_t), 
	\end{align}
	where $\{W_f, W_{ic}, W_{ia}, W_o, W_c, U_f, U_{ic}, U_{ia}, U_o,  U_c, b_f, b_{ic}, b_{ia}, b_o, b_c\}$ are parameters with output shape $d_m=128$ and $\odot$ denotes element-wise product. The only differences compared to a standard LSTM cell are Equations \ref{eq:lstm_ia}, \ref{eq:lstm_a}, and \ref{eq:lstm_c}. Equation \ref{eq:lstm_ia} computes the input gate for the attention module outputs, Equation \ref{eq:lstm_a} represents the attention module which will be described below, and Equation \ref{eq:lstm_c} adds the gated output from the attention module to the input activation.
	
	The attention function $\text{attn}(x_t, K_{t-1}, V_{t-1})$ is based on the multi-head attention module used in Transformer, with the following modifications: (i) Attention is restricted to the last $d_t=40$ steps, equivalent to two stages. This is because maintaining an excessively long memory might complicate learning due to the Markov assumption of MDP. (ii) A learned relative positional encoding, akin to Transformer XL \cite{dai2019transformer}, is used to facilitate attention to steps relative to the current step.
	
	For clarity, we first describe a single-head attention module. We denote $q_t \in \mathbb{R}^{d_e}$, $k_t \in \mathbb{R}^{d_e}$, $v_t \in \mathbb{R}^{d_e}$ as the query, key, and value at step $t$, respectively, where $d_e=16$ is the dimension of the embedding. The input $K_{t-1} \in \mathbb{R}^{d_t, d_e}$ and $V_{t-1} \in \mathbb{R}^{d_t, d_e}$ represent the preceding $d_t$ keys and values, defined as follows:
	\begin{align}
		K_{t-1} \defeq [k_{t-d_t}, k_{t-d_t+1} ..., k_{t-1}]^{T},   \\
		V_{t-1} \defeq [v_{t-d_t}, v_{t-d_t+1} ..., v_{t-1}]^{T}.
	\end{align}
	We calculate the current $q_{t}, k_{t}, v_{t}$ via a linear projection on input $x_t$:
	\begin{align}
		q_{t} &= W_q x_t,   \\
		k_{t} &= W_k x_t,    \\
		v_{t} &= W_v x_t,
	\end{align}
	where $\{W_q, W_k, W_v\}$ are parameters with output shape $d_e$. This enables us to compute $K_{t}$ and $V_{t}$ by stacking the corresponding $k$ and $v$. Subsequently, we compute the attention vector $p_{t} \in \mathbb{R}^{d_t}$ as:
	\begin{align}
		p_{t} &= \text{softmax} \left(\frac{(K_{t} + U_r)q_{t}}{\sqrt{d_e}} + b_r \right),
	\end{align}
	where $U_r \in \mathbb{R}^{d_t, d_e}$ and $b_r \in \mathbb{R}^{d_t}$ are learnable parameters representing the relative position encoding. This differs subtly from the relative position encoding used in Transformer XL, as the fixed attention window in our case allows us to utilize fixed-size parameters for encoding relative positions. We initialize $U_r$ and $b$ to be zero vectors, with the last entry of $b$ set to 5, reflecting the prior that the current step should be attended to more. In addition, the \text{softmax} here should be masked for steps in the previous episode.
	
	The final output of a single-head attention module, denoted as $\tilde{a}_t \in \mathbb{R}^{d_e}$, is computed as follows:
	\begin{align}
		\tilde{a}_t &= V_{t}^{T} p_{t}.
	\end{align}
	For the multi-head attention module, we repeat the above single-head attention module $d_h=8$ times and stack the outputs to yield $\tilde{a}^{\text{stack}}_t = \text{stack}([\tilde{a}_{t, 1}, \tilde{a}_{t, 2}, ..., \tilde{a}_{t, d_h}])$, where $\tilde{a}^{\text{stack}}_t \in \mathbb{R}^{d_m}$ and $\tilde{a}_{t, i}$ denotes the output of the $i^\text{th}$ single-head attention module. The final output of the attention module $a_t$ is given by applying a linear layer and layer normalization \cite{ba2016layer} on this stacked output:
	\begin{align}
		{a}_t &= \text{LayerNorm}(W_a \tilde{a}^{\text{stack}}_t + b_a + x_t),
	\end{align}
	where $x_t$ represents the skip connection, and $\{W_a, b_a\}$ are parameters with output size $d_m$. This concludes the description of the LSTM-Attention cell.
	
	\paragraph{LSTM-Attention Network}
	An LSTM-Attention Network is constructed by stacking $d_n=3$ LSTM-Attention Cells together. We denote the network's input as $x_t$, and use the superscript $1 \leq n \leq d_n$ to represent the $n^\text{th}$ cell in the network. We denote the LSTM-Attention network as:
	\begin{equation}
		(h_{t}^{1:d_n}, c_{t}^{1:d_n}, K_{t}^{1:d_n}, V_{t}^{1:d_n}) = \text{LSTM\_Attn\_Net}(x_{t-1}, h^{1:d_n}_{t-1}, c^{1:d_n}_{t-1}, K^{1:d_n}_{t-1}, V^{1:d_n}_{t-1}),
	\end{equation}
	defined by the iterative equation for $1 \leq n \leq d_n$:
	\begin{equation}
		(h_{t}^{n}, c_{t}^{n}, K_{t}^{n}, V_{t}^{n}) = \text{LSTM\_Attn}( \text{stack}([x_{t-1}, h_{t}^{n-1}]), h^{n}_{t-1}, c^{n}_{t-1}, K^{n}_{t-1}, V^{n}_{t-1}),
	\end{equation}
	and we define $h^{0}_{t} \defeq h^{n}_{t-1}$, the top LSTM-Attention cell's output from the previous step. The final output of the network is the top LSTM-Attention cell's output, $h^{n}_t$. The input to each LSTM-Attention cell comprises both the network's input and the output from the previous LSTM-Attention cell. This design choice was inspired by DRC \cite{guez2019investigation}, which suggests that this recurrence method may facilitate better planning.
	
	The rationale for integrating LSTM with the attention module in this manner stems from the following reasoning. The LSTM's hidden states can be interpreted as a form of short-term memory, primarily focusing on the current step. In contrast, the attention module acts as a long-term memory unit, as it maintains an equal computational path from all past steps. By fusing these two forms of memory, the LSTM is granted the capability to control access to long-term memory, including the ability to disregard it entirely by setting the gate variable to zero. This flexibility enables a focus on the most recent few steps while still preserving access to long-term memory, thus combining the advantages of both types of RNN. Using gradient clipping when applying the LSTM-Attention network is also recommended to stabilize training further.
	
	A significant drawback of the proposed LSTM-Attention network is the sequential computation it requires. Unlike the Transformer, which can compute attention across all steps simultaneously, the proposed network has to compute each step one at a time due to the recurrent computation of LSTM. Consequently, this leads to a much higher computational cost. Nonetheless, this sequential computation limitation may not be as significant in an RL setting. An RL task inherently requires the agent to generate action sequentially, and this sequential computation cannot be avoided even for Transformer. Future studies could explore the potential of the proposed LSTM-Attention network in contexts beyond the Thinker-augmented MDP, such as RL tasks that require long-term memory.
	
	\clearpage
	\section{Experiment Details} \label{app:exp}
	
	\subsection{Hyperparameters}
	The hyperparameters used in all our experiments are shown in Table \ref{table:hp}. We tune our hyperparameters exclusively on Sokoban based on the final solving rate. The specific hyperparameters we tune are: learning rates, model batch size, model loss scaling, maximum search depth or model unroll length, planning reward scaling, actor-critic clip global gradient norm, and actor-critic unroll length. In terms of preprocessing for Atari, we adhere to the same procedure as outlined in the IMPALA paper \cite{espeholt2018impala}, with the exception that we do not apply grayscaling.
	
	For the actor-critic baseline implemented on the raw MDP, we maintain the same hyperparameters, with two exceptions: (i) we adjust the actor-critic unroll length to 20 to ensure the same number of real transitions in an unrolled trajectory; (ii) we reduce the learning rate by half to 3e-4, after observing that the original learning rate led to unstable learning. We found that this set of hyperparameters provides optimal performance for the baseline in Sokoban, based on the final solving rate. Regarding the network architecture, we employ an encoder that mirrors the model's encoder architecture in Thinker, followed by a CNN that mirrors the actor-critic's CNN in Thinker.
	
	\subsection{Detailed Results on Sokoban}
	
	The performance of Thinker and the baselines on Sokoban can be found in Table \ref{table:baseline_sokoban}, corresponding to the learning curves shown in Figure \ref{fig:baseline_sokoban}. We use our own implementation for DRC~\cite{guez2019investigation} and open-source implementation for Dreamer-v3~\cite{hafner2023mastering}, with the result being averaged over three seeds. MuZero’s result is taken from \cite{hamrick2021on}. Results of DRC (original paper), I2A, ATreeC, VIN and IMPALA with Resnet are taken from \cite{guez2019investigation}. 
	
	As Thinker augments the MDP by adding $K-1$ steps for every step in the real MDP, there is an increased computational cost due to the extra K-1 imaginary steps, in addition to the added computational cost of training the model. On our workstation equipped with two A100s, the training durations for the raw MDP, DRC, and Thinker on Sokoban are around 1, 2, and 7 days, respectively.
	
	However, when benchmarked against other model-based RL baselines, Thinker's training time is similar or even shorter. For instance, Dreamer-v3 (the open-sourced implementation) requires around 8 days for a single Sokoban run on our hardware, primarily due to its high replay ratio. MuZero, when conducting a standard number of simulations (e.g., 100), requires 10 training days. Notably, despite the Thinker's additional training overhead for RL agents, the algorithm compensates by requiring significantly fewer simulations. Furthermore, if one replaces the RNN in the actor-critic network with an MLP (as detailed in Appendix \ref{app:ab}), without compromising the performance, the computational time for a single Thinker run can be reduced to 3 days.
	
	In Figure \ref{fig:main_sokoban_K}, we observed that the result for $K=1$, which does not use any imaginary transitions, underperforms compared to the result on the raw MDP. Initially, we hypothesized that this was because the model's hidden state, rather than the real state, was included in the augmented state. Although this hidden state might be more effective for predicting future states and rewards, it potentially complicates the learning of an optimal policy. However, subsequent experiments revealed that substituting the real state for the model's hidden state did not impact performance. This outcome suggests that the additional information given by the tree representation is, in fact, \emph{detrimental} to performance. We speculate that this counterintuitive effect is due to the tree representation providing information, such as value estimation, that are so insightful that the critic in the actor-critic algorithm overly relies on this estimation, thereby neglecting the real state and impeding the learning of a useful state representation for the actor. To investigate this speculation, we experimented with masking the tree representation at a random rate (with a probability $p=0.20$), which restored the performance to the level of the raw MDP. This propensity of the agent to depend excessively on a single type of input shown in these experiments points to a potential avenue for subsequent research.
	
	\subsection{Detailed Results on Atari 2600}
	
	% Figure environment removed
	
	% Figure environment removed
	
	\begin{table}
		\caption[Atari scores after 200M frames of training for the five selected games with different seeds.]{Atari scores after 200M frames of training for the five selected games with different seeds. Up to 30 no-ops at the beginning of each episode.}
		\label{table:atari5_score}
		\begin{center}	
			\begin{tabular}{lrrrr}
				\toprule[0.1ex]
				&\textbf{Seed 1} & \textbf{Seed 2} & \textbf{Seed 3}  & \textbf{Mean}   \\
				\midrule
				Battle Zone & 78740.00 & 31110.00 & 34460.00 & 48103.33 \\
				Double Dunk & 12.20 & 14.54 & 21.54 & 16.09 \\
				Name This Game & 34079.60 & 33400.20 & 31775.80 & 33085.20 \\
				Phoenix & 543987.70 & 535685.10 & 578485.30 & 552719.37 \\
				Qbert & 30734.50 & 36810.25 & 37003.25 & 34849.33 \\                               
				\bottomrule[0.25ex]
			\end{tabular}
		\end{center}
	\end{table}
	
	
	The performance of Thinker and the baselines on Atari 2600 can be found in Table \ref{table:baseline_atari}, corresponding to the learning curves shown in Figure \ref{fig:main_atari}. We include two additional baselines, UNREAL \cite{jaderberg2017reinforcement} and LASER \cite{schmitt2020off}, for comparison. The results of the baselines are taken from their original papers.
	
	We note that Thinker's performance on Atari 2600 lags behind that of state-of-the-art algorithms. There are several possible reasons: (i) we did not tune hyper-parameters on Atari 2600; (ii) learning to plan may require a much larger number of frames to show significance, similar to how MuZero requires 20 billion frames. As the goal of the experiments on Atari 2600 is to demonstrate the general advantage of the Thinker-augmented MDP over the raw MDP, we leave the problem of mastering Atari 2600 using Thinker for future study. In principle, more advanced RL algorithms, including the baselines, could be applied to the Thinker-augmented MDP.
	
	The final scores and the corresponding learning curves for each individual Atari game are presented in Table \ref{table:atari_score} and Figure \ref{fig:each_atari}, respectively. The Rainbow results are taken from the original paper \cite{hessel2018rainbow}, where the reported score is obtained from the best agent snapshots throughout training. The learning curves of Rainbow in Figure \ref{fig:each_atari} are also taken from the original paper.
	
	To validate the reproducibility of our results, we carried out multiple runs of five selected games with three different seeds. These games include \texttt{battle zone}, \texttt{double dank}, \texttt{name this game}, \texttt{phoenix}, and \texttt{qbert}. The selection of these games was influenced by the recommendations of \cite{aitchison2022atari}, which suggests that these five games are pivotal in predicting the median human-normalized score. The final scores for these five games are displayed in Table \ref{table:atari5_score}, and their individual learning curves can be found in Figure \ref{fig:each_atari5}. The results for these five games, as presented in other figures and tables in this paper, are derived from the average of these three seed values. %Given that each run requires approximately a week of computational time on a workstation equipped with two A100 GPUs, and considering that Sokoban is our primary testing ground, we find it adequate to conduct three seeds for each of the five selected games.
	
	\subsection{Discussion on Atari 2600's results}
	
	As stated in the main paper, Thinker demonstrates superior performance in shooting games like \texttt{chopper command}, \texttt{seaquest}, \texttt{phoenix}, \texttt{beam rider}, and \texttt{space invader}. A more general observation is that Thinker excels in environments with fast-moving objects, such as \texttt{breakout}. As depicted in \url{https://youtu.be/0AfZh5SR7Fk}, Thinker's model is capable of predicting the trajectories of these swift objects with near-perfect precision. We hypothesize that such predictions, essential for planning future actions, pose a challenge for an agent in the raw MDP to learn. For example, in \texttt{Breakout}, the video illustrates the agent exploring different methods of deflecting the ball at various angles, sometimes missing the ball in its imagination but performing adeptly in the actual game. Upon identifying a sequence of actions that might result in missing the ball, the agent can opt to avoid it. This adaptive decision-making and the significant performance gap between the agent policy and the base policy in these games underscore the benefits of planning in games with fast-moving objects.
	
	It is important to highlight that applying a direct L2 loss on future states instead of a feature loss can undermine these advantages. For example, we observed that in \texttt{Breakout}, a model trained with direct L2 loss is able to predict frames of high quality, except that the ball, arguably the most important object in the game, is missing in the predicted frame. Given that these fast-moving objects typically inhabit only a few pixels and are challenging to predict, the model is prone to neglect these elements when trained using a direct L2 loss. In contrast, by using feature loss, these important objects, such as the ball in \texttt{Breakout}, are usually learned early in training.
	
	We also observe that the agent's performance falls short in more demanding exploration games, such as \texttt{montezuma revenge} and \texttt{venture}. This is because the Thinker-augmented MDP lacks any mechanism to encourage exploration in the real environment, which results in challenges when dealing with these complex exploration games, much like an actor-critic applied directly to a raw MDP. Future research could explore methods to utilize Thinker to foster exploration, such as using model prediction loss as a curiosity reward. As exploration is a well-studied area in RL \cite{LADOSZ20221}, many existing exploration methods could also be directly applied to the Thinker-augmented MDP.
	
	Thinker also performs poorly in games where each action carries little impact, such as in \texttt{qbert}. Games in which the agent moves slowly, such as \texttt{bank heist} and \texttt{ms pacman}, also share this characteristic. These games are in stark contrast to games like Sokoban, where a single action can move the agent to a different grid. These games highlight a fundamental limitation of the Thinker-augmented MDP that cannot be avoided by switching RL algorithms—planning is conducted in the primitive action space rather than the abstract action space, thereby necessitating the significance of each primitive action. Future research could investigate modifications to Thinker that would allow model interaction through abstract actions, thereby allowing high-level and long-term planning.
	
	Interestingly, the actor-critic applied to the raw MDP occasionally outperforms the one applied to the Thinker-augmented MDP. This could be attributed to the fact that we provide the agent with the model's hidden state instead of the true or predicted frames. It is conceivable that the actual frame is simpler to learn from, given that the model learns the encoding of frames by minimizing supervised learning loss rather than maximizing the agent's returns. Future studies could consider allowing the agent to directly observe the true and predicted frame, thus ensuring that the agent possesses a strictly greater capability than an agent on the raw MDP. Another factor could be the impact of unoptimized hyperparameters. As we increased the network's depth while maintaining the same learning rate during the transition from Sokoban to the Atari environment, the training process became more unstable, as demonstrated in some learning curves. It is plausible that the poor performance in certain games stems from these unoptimized hyperparameters.
	
	Finally, as evidenced by the individual learning curves illustrated in Figure \ref{fig:each_atari}, and the median human-normalized score in Figure \ref{fig:main_atari}, we observe that the slope of the learning curve is generally steeper at the end of training compared to other baselines. We posit that mastering the use of a model represents an additional skill to be acquired, as opposed to an agent acting on a raw MDP. This necessitates extended training to learn this skill, however, once this skill is acquired, the final performance generally surpasses that of an agent on the raw MDP. As such, future work could experiment with a larger training budget than 200 million frames.
	
	\clearpage
	\begin{table}[h]
		\caption[Hyperparameters used in experiments]{Hyperparameters used in experiments.}
		\label{table:hp}
		\begin{center}	
			%\footnotesize
			\begin{tabular}{lllll}
				\toprule[0.1ex]
				\textbf{Parameter} & \textbf{Value} \\
				\midrule
				\textbf{Thinker-augmented MDP} \\
				Stage length $K$ & 20\\	 
				Maximum search depth $L$ & 5 \\
				Augmented discount rate $\tilde{\gamma}$ & $\gamma^{\frac{1}{K}} $ \\
				Planning reward scaling $c^p$ & Anneal linearly from 1 to 0\\
				\\
				\textbf{Model} \\
				Learning rate & Anneal linearly from 1e-4 to 0 \\
				Optimizer & Adam \\                
				Adam beta & (0.9, 0.999) \\        
				Adam epsilon & 1e-8 \\       
				Replay buffer size & 200,000 \\
				Minimum replay buffer size for sampling & 200,000 \\
				Replay ratio & 6 \\
				Batch size & 128 \\
				Model unroll length $L$ & 5 \\
				Reward loss scaling $c^r$ & 1 \\
				Termination indicator loss scaling $c^d$ & 1\\
				Feature loss scaling $c^s$ & 10 \\
				Value loss scaling $c^v$ & 0.25 \\
				Policy loss scaling $c^\pi$ & 0.5 \\
				Target for policy & Action distribution \\
				\\
				\textbf{Actor-critic} \\
				Learning rate & Anneal linearly from 6e-4 to 0 \\
				Optimizer & Adam \\                
				Adam beta & (0.9, 0.999) \\        
				Adam epsilon & 1e-8 \\       
				Batch size & 16 \\
				Actor-critic unroll length & 20$K$ + 1 \\
				Baseline scaling & 0.5 \\
				Clip global gradient norm & 1200 \\
				Entropy regularizer for real actions & 1e-3 \\
				Entropy regularizer for imaginary and reset actions & 5e-5 \\
				\\
				\textbf{Environment specific—Sokoban} \\
				Discount rate $\gamma$ & 0.97 \\
				Input shape & (3, 80, 80) \\ 
				\\
				\textbf{Environment specific—Atari 2600} \\
				Discount rate $\gamma$ & 0.99 \\
				Grayscaling & No \\
				Action repetitions & 4 \\
				Max-pool over last N action repeat frames & 2 \\
				Input shape & (12, 80, 80) \\ 
				Frame stacking & 4 \\
				End of the episode when life lost & Yes \\
				Reward clipping & [-1, 1] \\
				Sticky action & No \\
				\bottomrule[0.25ex]
			\end{tabular}
		\end{center}
	\end{table}
	
	\clearpage
	% Figure environment removed
	
	\clearpage
	\begin{table}[h]
		\caption[Atari scores after 200M frames of training]{Atari scores after 200M frames of training. Up to 30 no-ops at the beginning of each episode.}
		\small
		\label{table:atari_score}
		\begin{center}	
			\begin{tabular}{lrrrr}
				\toprule[0.1ex]
				&\textbf{Rainbow} & \textbf{Raw MDP} & \textbf{\thead{\scriptsize Thinker \\ \scriptsize  Base Policy}}  & \textbf{\thead{\scriptsize Thinker \\ \scriptsize  Agent Policy}}   \\
				\midrule
				Alien & 9491.70 & \textbf{13868.80} & 229.20 & 230.10\\
				Amidar & \textbf{5131.20} & 1121.56 & 1338.75 & 3591.84\\
				Assault & 14198.50 & 6998.29 & 18856.51 & \textbf{24262.47}\\
				Asterix & \textbf{428200.30} & 279440.00 & 238992.50 & 85254.00\\
				Asteroids & 2712.80 & 12570.40 & 40030.60 & \textbf{212805.50}\\
				Atlantis & 826659.50 & \textbf{896017.00} & 808384.00 & 809160.00\\
				Bank Heist & 1358.00 & 378.50 & \textbf{1531.20} & 1528.90\\
				Battle Zone & \textbf{62010.00} & 2320.00 & 47553.33 & 48103.33\\
				Beam Rider & 16850.20 & 24202.46 & 15213.50 & \textbf{59533.14}\\
				Berzerk & 2545.60 & \textbf{2664.00} & 1024.60 & 1038.30\\
				Bowling & 30.00 & 23.78 & \textbf{31.26} & 31.14\\
				Boxing & 99.60 & 8.17 & 99.51 & \textbf{99.86}\\
				Breakout & 417.50 & 593.83 & 660.84 & \textbf{840.14}\\
				Centipede & 8167.30 & \textbf{8201.71} & 5169.38 & 6507.05\\
				Chopper Command & 16654.00 & 1046.00 & 43691.00 & \textbf{843973.00}\\
				Crazy Climber & \textbf{168788.50} & 97824.00 & 98755.00 & 116193.00\\
				Defender & 55105.00 & \textbf{385397.50} & 71378.50 & 123064.50\\
				Demon Attack & 111185.20 & 502.30 & 133410.20 & \textbf{135040.10}\\
				Double Dunk & -0.30 & -1.72 & 15.27 & \textbf{16.09}\\
				Enduro & \textbf{2125.90} & 26.03 & 0.00 & 0.00\\
				Fishing Derby & 31.30 & 32.46 & 38.58 & \textbf{43.68}\\
				Freeway & \textbf{34.00} & 0.00 & 28.04 & 28.65\\
				Frostbite & \textbf{9590.50} & 269.70 & 303.30 & 303.10\\
				Gopher & \textbf{70354.60} & 5049.40 & 15871.40 & 55911.00\\
				Gravitar & 1419.30 & 267.00 & 3766.00 & \textbf{4226.00}\\
				Hero & \textbf{55887.40} & 13117.30 & 2984.30 & 2987.80\\
				Ice Hockey & 1.10 & -5.27 & 20.62 & \textbf{22.99}\\
				Jamesbond& N.A. & 1523.00 & 4171.50 & \textbf{7612.50}\\
				Kangaroo & \textbf{14637.50} & 390.00 & 12424.00 & 13218.00\\
				Krull & \textbf{8741.50} & 5767.00 & 4663.10 & 4557.60\\
				Kung Fu Master & \textbf{52181.00} & 35209.00 & 32639.00 & 36391.00\\
				Montezuma Revenge & \textbf{384.00} & 0.00 & 12.00 & 14.00\\
				Ms Pacman & 5380.40 & \textbf{7117.66} & 2685.40 & 4206.10\\
				Name This Game & 13136.00 & 13391.00 & 25131.43 & \textbf{33085.20}\\
				Phoenix & 108528.60 & 18201.70 & 156647.23 & \textbf{552719.37}\\
				Pitfall & \textbf{0.00} & -38.17 & -2.95 & -2.51\\
				Pong & 20.90 & \textbf{20.97} & 20.96 & 20.84\\
				Private Eye & \textbf{4234.00} & 48.41 & 93.61 & 94.00\\
				Qbert & 33817.50 & 18491.75 & 20184.58 & \textbf{34849.33}\\
				Riverraid& N.A. & \textbf{23983.60} & 11664.30 & 11696.00\\
				Road Runner & \textbf{62041.00} & 2988.00 & 37967.00 & 41950.00\\
				Robotank & \textbf{61.40} & 31.73 & 40.89 & 40.53\\
				Seaquest & 15898.90 & 1501.40 & 32992.90 & \textbf{432855.00}\\
				Skiing & \textbf{-12957.80} & -16076.10 & -21515.92 & -24218.14\\
				Solaris & \textbf{3560.30} & 2136.60 & 2427.00 & 2323.40\\
				Space Invaders & 18789.00 & 8423.85 & 12880.40 & \textbf{42486.95}\\
				Star Gunner & 127029.00 & 183274.00 & 290441.00 & \textbf{692283.00}\\
				Surround & \textbf{9.70} & -9.86 & 1.89 & 4.28\\
				Tennis & 0.00 & \textbf{20.51} & -1.25 & -1.19\\
				Time Pilot & 12926.00 & 42851.00 & 37310.00 & \textbf{50767.00}\\
				Tutankham & 241.00 & 51.60 & 262.00 & \textbf{268.14}\\
				Up N Down&  N.A.& 319317.80 & 337508.10 & \textbf{404120.10}\\
				Venture & \textbf{5.50} & 0.00 & 0.00 & 0.00\\
				Video Pinball & \textbf{533936.50} & 281210.04 & 444212.40 & 512567.38\\
				Wizard Of Wor & 17862.50 & 6010.00 & 19304.00 & \textbf{31691.00}\\
				Yars Revenge & 102557.00 & 72845.97 & 70000.15 & \textbf{124342.12}\\
				Zaxxon & \textbf{22209.50} & 15.00 & 17716.00 & 19979.00\\
				
				\bottomrule[0.25ex]
			\end{tabular}
		\end{center}
	\end{table}
	
	\clearpage
	\section{Ablation Analysis} \label{app:ab}
	In this section, we investigate the significance of various elements of the Thinker-augmented MDP. All ablation experiments are performed in Sokoban under identical settings as those of the primary experiments, with results shown as the solving rate over the last 200 episodes.
	
	\paragraph{Hints and memory} 
	
	We examine the effect of removing all hints and other auxiliary statistics from the tree representation. Specifically, we remove the following statistics:
	\begin{small}{$$\small  {\{g_{\text{child(root)}}^{\text{max}},g_{\text{child(root)}}^{\text{mean}}, n_{\text{child(root)}}, g_{\text{child(cur)}}^{\text{max}}, g_{\text{child(cur)}}^{\text{mean}},  n_{\text{child(cur)}}, g_{\text{root}, \text{cur}}, g_{\text{root}, \text{cur}} - \gamma^{d+1} \hat{v}_{\text{cur}}, g_{\text{root}}^{\text{max}}\}}$$}\end{small}
	from the augmented state. These statistics are deemed auxiliary since, theoretically, the agent can compute these from the remaining statistics in the previous and current steps. The result of removing the hints is shown in Figure \ref{fig:ab_mem}.
	
	The removal of hints causes the agent to learn at a slower pace while slightly reducing the final performance. We theorize that the agent requires more training to learn to summarize the rollouts across multiple steps, leading to slower learning. With prolonged training, it could be feasible for the agent to learn this summarization and achieve the same final performance, thereby rendering the provision of these handcrafted hints unnecessary.
	
	A specialized RNN architecture is employed to endow the agents with long-term memories while preserving training stability, as elaborated in Appendix \ref{app:nn}. We then consider the impact of replacing this RNN with a three-layer MLP with 200 hidden units, effectively removing the agents' memory. In the MLP, we add a skip connection from the input to each hidden layer, to ensure easier flow of information. The result of this adjustment is presented in the same figure. Interestingly, the removal of memory does not hamper performance in any notable way.
	
	We hypothesize that as we have already summarized the rollouts for the agent by providing the hints, the agent is not required to perform this summarization itself and hence, does not require memory. To verify this hypothesis, we consider the scenario of removing both memory and hint, and the outcome is demonstrated in the same figure. The effect of eliminating memory proves to be highly significant when the hints are not provided. Consequently, we conclude that the presence of either the hints \textit{or} memory is necessary for learning to plan.
	
	\paragraph{Base policy and imaginary actions} 
	
	We evaluate the effects of removing the base policy and its value. This leads to the tree representation being mostly empty as a number of statistics, such as the hints, are computed based on the node's value. Nonetheless, the agent still receives the model's hidden state as input. The result of this removal is shown in Figure \ref{fig:ab_vp}.
	
	The result shows that the agent fails to learn in such a scenario, indicating that the base policy is a critical component for learning to plan. We offer two potential explanations for this observation. Firstly, predicted values provide an agent with the ability to evaluate a leaf node more easily. This node evaluation allows an agent to evaluate a rollout without simulating the rollout till the end of the episode, thus allowing more shallow search. Even though the predicted value could theoretically be computed based on the hidden state of the node, which is provided to the agent, it is likely that this prediction is too difficult for the agent to learn by itself. Secondly, providing the base policy allows the agent to continuously improve upon the current policy. The base policy acts as a `distilled' policy without planning, and the search corresponds to a learned policy improvement operator applied to this distilled policy. With the base policy and its value as inputs, the agent can learn different search algorithms as this policy improvement operator, completing the cycle of generalized policy iteration.
	
	In addition, we consider the scenario where all imaginary actions are replaced with random actions, and we do not train the RL agent on these imaginary actions. The results are shown in Figure \ref{fig:ab_vp}. This is intended to evaluate the benefits of learning imaginary actions. We find that performance significantly deteriorates, suggesting that the imaginary actions play a pivotal role in guiding real actions, consistent with the behavioural analysis in Appendix \ref{app:beh}.
	
	\paragraph{Maximum search depth} 
	
	We evaluate the effect of increasing the maximum search depth and the model unroll length $L$ from 5 to 10, thereby allowing the agents to search deeper. The result is shown in Figure \ref{fig:ab_depth}. The initial learning speed is significantly slower, presumably due to the added time required for the model to learn to unroll over an increased number of steps. However, the final performance remains largely unaffected. This suggests that a deeper search does not necessarily lead to improved final performance. This finding aligns with previous studies indicating that a shallow search suffices to obtain satisfactory outcomes in MuZero \cite{hamrick2021on}.
	
	\paragraph{Planning rewards and action target} 
	
	Lastly, we discuss some non-essential components of the algorithm. To mitigate the sparse rewards in the augmented MDP, we added a heuristic reward, which we refer to as the planning reward, during the imaginary steps. The details of the planning reward are discussed in Appendix \ref{app:planning}. Figure \ref{fig:ab_vp} shows the results of excluding planning rewards. The removal of the planning rewards does not have any significant impact on the performance. This suggests that the planning reward may not be a critical component for learning to plan. Additional behavior analysis of a trained agent in Appendix \ref{app:beh} demonstrates that an agent trained without planning rewards seems to learn a similar planning behavior to that of an agent trained with planning rewards.
	
	We used the action distribution instead of the sampled action for training the base policy in the model. However, this approach necessitates that the augmented MDP receive the action distribution from the agent, a procedure that is not consistent with the definition of an MDP. We alternatively experimented with using the sampled action as the target for training the base policy in the model, with the results depicted in Figure \ref{fig:ab_vp}. This modification did not significantly impact overall performance, though it led to a slightly slower initial learning. Nonetheless, we recommend employing the action distribution in lieu of the sampled action, as the former encompasses more information and has a lower variance. The adherence to the conventional definition of an MDP, while theoretically pertinent, is of marginal consequence in practical applications.
	
	% Figure environment removed
	
	\clearpage
	\section{Agent Behavior Analysis} \label{app:beh}
	
	In this section, we delve into the behavior learned by the trained agents in the Thinker-augmented MDP of Sokoban. Our inquiry aims to understand the nature of the search policies acquired by the agents. Due to the use of the dual network, it is easy to visualize what the agent is `planning' by plotting the predicted states from the model on each imaginary step. Figure \ref{fig:vis} shows a typical stage, where the agent plans different ways of solving the task. Videos of the whole game and other Atari games are available at \url{https://youtu.be/0AfZh5SR7Fk}.
	
	Though the video is valuable in discerning the plan of an agent, a more quantitative analysis is warranted. Does the agent adhere to handcrafted planning algorithms, such as the $n$-step exhaustive search or MCTS, or does it learn novel planning algorithms? Given that the agents learn three types of actions—reset, imaginary, and real actions—we will analyze these actions individually. 
	
	\paragraph{Reset Action}
	
	% Figure environment removed
	
	In a $n$-step exhaustive search, which involves iterating through every possible multi-step action sequence and choosing the one with the highest rollout return, a reset is triggered when the search depth hits $n$. Similarly, MCTS initiates a reset at a leaf node. However, the trained agent seems to learn different behavior of resetting.
	
	As visualized in the video, the agents often chose to reset when envisioning a poor rollout, for instance, when imagining a box being moved to an irreversible state. To illustrate this further, we can examine the distribution of Temporal Difference (TD) errors of the rollout from the root node to a node $i_n$, defined as:
	\begin{equation}
		\delta_{i_n} \defeq \hat{r}_{i_1} + \gamma^2 \hat{r}_{i_2} + ... \gamma^{n} \hat{r}_{i_{n}} + \gamma^{n+1} \hat{v}_{i_n} - \hat{v}_{\text{root}},
	\end{equation}
	where $\{\text{root}, i_1, i_2, ..., i_n\}$ denotes the path from the root node to the node $i_n$. This TD error can be interpreted as the quality of the rollout relative to the current policy. We compare the distribution of TD errors of the current rollout $\delta_{\text{cur}}$ where the agent decided to reset with those where it did not, and the result is visualized in the left and middle panels of Figure \ref{fig:reset}.
	
	We observe that the TD error distribution when agents opt to reset tends to lean more towards the left as compared to when they choose not to reset. This indicates a higher propensity for the agent to reset when the quality of the rollout is inferior. The thin tail on the left side of the distribution represents instances where the agent envisions a poor plan, such as reaching an irreversible state. Under such circumstances, the agent exhibits a high probability of resetting. This suggests that the agents learned to factor in the current rollout's quality in their decision to reset. This approach diverges from planning strategies like MCTS, which mandates a reset upon reaching a leaf node.
	
	An additional dimension worth investigating is the \emph{expansion length}, defined as the number of steps the agent executes beyond the last expanded node. In the case of MCTS, this length is consistently one as the algorithm requires a reset at every leaf node. However, as shown in the right panel of Figure \ref{fig:reset}, the trained agents generally exhibit a longer expansion length. In fact, when the maximum search depth $L$ is increased to 10, the agent learns to search much deeper, as depicted in the same figure.
	
	Coupled with the aforementioned insights into the TD error distribution, we see that the agents prefer to persist with their existing plan unless they encounter an unfavorable leaf node or reach the set maximum search depth. These observations suggest that the agents tend to formulate plans that are both deep and narrow. Arguably, these learned behaviors mimic our planning processes more closely—we typically continue to think about a good plan, rather than simultaneously considering multiple plans and expanding each by a single step. 
	
	\paragraph{Imaginary Action}
	In a $n$-step exhaustive search, imaginary action is chosen sequentially, while in MCTS, imaginary action is chosen according to the upper confidence bound (UCB). As for the trained agents, it is difficult to deduce the formula used to select the imaginary action as it is the output of a deep neural network. However, the maximum TD errors in a stage, defined as $\delta^{\text{max}} \defeq \max_k \delta_k$ where $1 \leq k < K$ denotes the step within a stage, could be used as a proxy to evaluate the effectiveness of the search\footnote{This maximum TD error is proportional to the undiscounted sum of planning rewards in a stage.}. The distribution of maximum TD errors of search using the trained agent, UCB, and random search is shown in Figure \ref{fig:im1}. Note that we use the learned model of the trained agent when applying UCB.
	
	We observe that the distribution of maximum TD errors has a slightly heavier tail on the right side when searching with a trained agent. This can be better visualized in log scale as shown in Figure \ref{fig:im2}. This tail mostly corresponds to the case of solving or being closer to solving a level in the imagination where the current policy has a high chance of not solving it. This scenario is relatively rare given that the current policy is capable of solving approximately 95\% of levels, which explains the thin tails in all cases. Thus, the heavier tail suggests that the agents learn to search more effectively than MCTS within the same search budgets. This finding is consistent with previous studies on replacing MCTS with neural networks in supervised learning settings \cite{guez2018learning}. 
	
	We conjecture that the difference between an RL agent and MCTS in this distribution would be much larger if the model were also trained with MCTS, given the large number of simulations required for MCTS to achieve good performance in Sokoban \cite{guez2018learning}. It is also noteworthy that even without planning rewards, the agent is still able to collect large maximum TD errors, which implies that the agent can learn to search effectively without the guidance of heuristics.
	
	\paragraph{Real Action}
	
	In a $n$-step exhaustive search, the real action is taken as the child node with the highest rollout return, while in MCTS, the real action is usually selected based on the visit count of each child node. In contrast, our trained agents appear to learn to use a combination of both.
	
	To further investigate how real action is selected given the same search result, we used the imaginary action and reset action outputs from the trained agent and considered two different ways of selecting the real action: (i) the real action output by the trained agent, (ii) the real action output by MCTS, which is sampled based on visit counts with different temperatures $T \in \{0.25, 1\}$ as in MuZero \cite{schrittwieser2020mastering}. For each method, we calculated the portion of the real action that corresponded to the highest visit count, the second highest visit count, and so on. The result is shown in the left panel of Figure \ref{fig:real}. Additionally, we computed the same for the mean rollout return and the maximum rollout return of child nodes, as shown in the middle and right panels of the figure.
	
	The figure shows that most of the real action selected by the agents corresponds to the child node with the highest visit counts or rollout returns. The figures also suggest that the maximum rollout return is the most indicative of the agent's real action among the three statistics. In contrast, MCTS tends to select the real action with the highest visit count. Notably, the action with the highest visit count does not necessarily correspond to the action with the highest rollout return, as evidenced by MCTS with $T=0.25$, which chooses the action with the highest visit count with over 80\% probability, yet only around half of these actions correspond to the one with the highest rollout return. The simultaneous consideration of both rollout returns and visit counts by the trained agent seems more intuitively reasonable—it is unwise to follow a poor plan even if it is simulated numerous times. 
	
	% Figure environment removed
	
	% Figure environment removed
	
	% Figure environment removed
	
	\paragraph{Comparison with MCTS}
	
	Lastly, to compare the efficiency of the trained agents across all three action sets with MCTS, we assess the solving rates of both the trained agents and MCTS. In MCTS, we utilize the model learned by the agent at the end of training, and we test a range of simulation steps\footnote{Note that the stage length can be smaller than the number of simulations, as the former includes traversing expanded nodes while the latter does not.} spanning over \{5,10,20,40,80,160,320,640,1280,2560\}. The results are depicted in Figure \ref{fig:mcts}. We observe that MCTS needs 1280 simulation steps to match the performance of a trained agent, which achieves the same results with only 20 stage length. This notable difference highlights the potential benefits of trained agents compared to handcrafted algorithms—they exhibit the capacity to conduct searches with increased efficiency, particularly within limited computational resources.
	
	In conclusion, the analysis in this section suggests that the trained agents appear to learn a planning algorithm that differs from common handcrafted planning algorithms. These agents learn to plan deeply and reset upon encountering a poor node, while being able to search for a good plan within a limited number of model simulations. It is likely that the agents also take advantage of the model's hidden state, which contains rich information about the root and predicted states, when deciding upon the three types of actions. However, a comprehensive investigation of this particular dynamic lies beyond the scope of our current analysis.
	
	% Figure environment removed
	
	\clearpage
	\section{Planning Rewards} \label{app:planning}
	
	In the main paper, we use the extended real reward as the augmented reward:
	\begin{equation}
		\tilde{r}_k \defeq 
		\begin{cases}
			0 & \text{if } k \leq K, \\
			r, & \text{if } k = K + 1,
		\end{cases} \label{eq:real_reward}
	\end{equation}
	where $r$ denotes the real reward obtained from the real step. However, this sparse reward may make learning difficult. We explore adding an auxiliary reward, which we call the \emph{planning reward}, to help guide the learning of imaginary and reset actions. We define the planning reward $\tilde{r}^{p}_k$ on step $k$ to be:
	
	\begin{equation}
		\tilde{r}^{p}_k \defeq 
		\begin{cases}
			c^{p} \left( g_{\text{root},k}^{\text{max}} - g_{\text{root},k-1}^{\text{max}} \right), & \text{if } k \leq K \\
			0, & \text{if } k = K + 1,
		\end{cases} \label{eq:plan}
	\end{equation}
	
	where $g^\text{max}_{\text{root}, k}$ denotes the maximum rollout return of the root node on step $k$ within a stage, as defined after Equation \ref{eq:rollout_return}, and $c^{p} \geq 0$ is a scalar hyperparameter that controls the strength of the planning reward.
	
	Note that the total undiscounted sum of planning rewards in a stage is:
	\begin{align}
		\sum_{k=2}^{K+1} \tilde{r}^{p}_k &= c^p(g_{\text{root},K}^{\text{max}} - g_{\text{root},0}^{\text{max}}) \\
		&\propto \hat{r}_{i^*_1} + \gamma^2 \hat{r}_{i^*_2} + ... \gamma^{n} \hat{r}_{i^*_{n}} + \gamma^{n+1} \hat{v}_{i^*_{n}} -  \hat{v}_{\text{root}}, 
	\end{align}
	where $\{\text{root}, i^*_1, i^*_2, ..., i^*_{n}\}$ refers to the rollout with the highest return, $g_{\text{root}, i}$, in the entire stage. As $\hat{v}_{\text{root}}$ is not influenced by imaginary and reset actions, the return of a stage is equivalent to $ \hat{r}_{i^*_1} + \gamma^2 \hat{r}_{i^*_2} + ... \gamma^{n} \hat{r}_{i^*_{n}} + \gamma^{n+1} \hat{v}_{i^*_{n}} $, the maximum rollout return. Consequently, the planning reward is a heuristic directing the agent to maximize the maximum rollout return, while not penalizing for searching paths with a low rollout return to encourage exploration in imagination.
	
	However, this planning reward should not be given to real actions to prevent incentive misalignment. For example, collecting a large amount of planning rewards is possible by continually doing nothing in real steps but imagining a good plan. As such, the planning reward signal has to be separated from the real reward signal $\tilde{r}_k$ instead of being added together. We also decay $c^{p}$ from $1$ to $0$ throughout training to further ensure that the imaginary action is aligned with the real reward function. Combined, the augmented reward contains both the real reward and the planning reward: $r^{\text{aug}} \defeq (\tilde{r}, \tilde{r}^{p}).$
	
	The incorporation of planning rewards necessitates certain modifications to the actor-critic algorithm in our experiments. Specifically, we separately learn the value of these two distinct rewards when training the critic, resulting in two different TD errors corresponding to the real and the planning rewards. In training the critic for the planning reward, we treat each stage as a single episode. This approach ensures that the planning reward incentives the agent to optimize the maximum rollout returns exclusively for the current stage. Furthermore, when using the TD errors to train the actor, we impose a mask on the planning rewards' TD error, setting it to zero during a real step.
	
	Experiments in Appendix \ref{app:ab} show that planning rewards only provided a minimal increase in initial learning speed, indicating that the real rewards are sufficient for learning to plan. We conjecture that sparse real rewards are not a problem, as the TD errors induced by real rewards may not be sparse. For example, if the critic learns to use the maximum rollout return $g_{\text{root},k}^{\text{max}}$ as the estimated value, which is provided in the tree representation, then the TD errors in an imaginary step equal the planning rewards, except for the adjustment due to the discount rate.
	
	\clearpage
	\section{Relationship with Generalized Policy Iteration and Meta-Reinforcement Learning} \label{app:pi}
	
	\subsection{Generalized Policy Iteration Algorithm}
	Applying an RL algorithm to the Thinker-augmented MDP can be viewed as an instance of a generalized policy iteration algorithm \cite{bellman1957dynamic, sutton2018reinforcement}. Policy improvements are carried out by an agent that conducts searches within the model. Given the base policy, these searches yield an improved policy. Simultaneously, the policy evaluation is done by the learning of the value-policy network. This involves training the network on trajectories generated by the improved policy, so as to learn the values of this improved policy and project the improved policy back to the base policy. A noteworthy divergence from a typical generalized policy iteration algorithm is the learning of the policy improvement operator, instead of employing a handcrafted one such as MCTS.
	
	Given this perspective, a crucial question arises: how effective is a learned policy improvement operator in comparison to a handcrafted one? To address this question, we evaluate the efficiency of different policy improvement operators, such as MCTS and an RL agent, during a single step of policy improvement. 
	
	Specifically, we consider the Thinker-augmented MDP with a fixed base policy $\hat{\pi}$ and its value $\hat{v}$. We do not update the base policy that is described in Section \ref{sec:dual}. The base policy is a given policy that performs slightly better than a random policy. For simplicity, we employ the true environment dynamics, represented as $(P, R)$, as the model in the augmented MDP and do not impose a limit on the maximum search depth, given that we are using the true environmental dynamics. Sokoban serves as the real environment in our experiments.
	
	We experiment with three different types of policy improvement operators: (i) $n$-step exhaustive search, (ii) MCTS, and (iii) RL agents. In (iii), we trained the agent from scratch in the Thinker-augmented MDP. Only the tree representation $u_\text{tree}$ is provided as input to the agent, so the agent cannot learn to act based on the real states directly. The results are shown in Figure \ref{fig:pi}. Due to the low solving rate, we report the return of an episode here for better illustration. The maximum return in Sokoban is around 14, and a return of 4 gives at most 30\% solving rate.
	
	From the figure, we see that the base policy achieves a return of 0.12, and all policy improvement operators are able to improve on this policy and achieve a better return. We observe that the policy improvement gap for RL agents widens as the value of $K$ increases. With $K$=10, the agent's performance is similar to that of  a 3-step exhaustive search, which requires 155 model simulations. Increasing $K$ to 20, the performance is slightly worse than a 4-step exhaustive search, which requires 780 model simulations. At $K$=40, the agent performance surpasses a 5-step exhaustive search, which requires 3905 model simulations. In addition, the agent at $K$=10 also shows comparable performance to MCTS that uses 60 model simulations. It is also worth noting that the number of model simulations used by the agent is typically less than $K$, given that imaginary steps also include the traversal through expanded nodes.
	
	The results here demonstrate that increasing $K$ to 40 can still enhance performance, whereas in the main paper, the performance improvement gained by increasing $K$ above 10 is insignificant. We conjecture that once the value-policy network is trained, the model can learn to predict the values of leaf nodes more accurately, reflecting the value of the current policy instead of the base policy. This eliminates the need for a deep search to evaluate a rollout more precisely, rendering a large $K$ unnecessary.
	
	The results also underscore the importance of a learned base policy. The agent achieves a maximum solving rate of less than 30\%, whereas a 95\% solving rate is attainable when the base policy undergoes training as well. This outcome is intuitive, considering that the agent gives an improved version of the base policy, and thus the agent policy will not significantly outperform it. If a fixed poor base policy is used, the agent policy will not perform well.
	
	Nonetheless, these findings suggest that an agent can learn to perform policy improvement and achieve similar performance to handcrafted algorithms while requiring significantly fewer simulations. However, it requires a moderate number of transitions for an agent to learn this policy improvement operator.
	
	% Figure environment removed
	
	\clearpage
	\subsection{Meta-Reinforcement Learning}
	In this section, we explore the relationship between meta-reinforcement learning (meta-RL) algorithms \cite{beck2023survey} and the Thinker algorithm. Meta-RL is a method designed to learn how to perform reinforcement learning. Its objective is to develop a policy that can quickly adapt to a variety of new tasks derived from a distribution of tasks, using minimal data. While meta-RL is often used in scenarios involving a task distribution, it is also applicable to single-task scenarios \cite{zahavy2020self, xu2020meta}, which is our primary focus here.
	
	% meta-RL basics
	In the meta-RL framework, a trial typically comprises $L$ episodes. During a trial, the agents initially encounter $K$ episodes—termed \emph{burn-in episodes}. Notably, the return within these episodes does not contribute to the main objective. Instead, the primary goal is to maximize the return of the remaining $L-K$ episodes, known as \emph{evaluation episodes}. Consequently, the purpose of a meta-RL algorithm is to use the knowledge gained throughout the entire trial to maximize returns during the evaluation episodes. One common class of meta-RL algorithm applies an RL algorithm with an RNN that can access previous actions and rewards, and the hidden state is not reset across episodes. As such, the knowledge of the trial is embedded in the RNN’s hidden states \cite{wang2016learning, duan2016rl}.
	
	% key similarity
	If the model perfectly represents the true environment dynamics, we can align the Thinker-augmented MDP with meta-RL by considering imaginary rollouts as (truncated) burn-in episodes, and real transitions as the evaluation episode in meta-RL. As agents only aim to maximize the return in real transitions, the objectives of meta-RL and RL agents on the Thinker-augmented MDP coincide. Moreover, the purpose of burn-in episodes in meta-RL or imaginary rollouts in planning is the same – to guide better actions in the evaluation episodes or real transitions, possibly through risk-taking behavior in them.
	
	% interleave and reset
	However, unlike conventional meta-RL, which uses a fixed number of burn-in episodes followed by evaluation episodes, the Thinker-augmented MDP interleaves the burn-in episodes within a single evaluation episode. In other words, for each step in the evaluation episode with state $s_t$, we allow agents to have a few burn-in episodes starting from $s_t$.
	%, before executing one action $a_t$ in the same evaluation episode. 
	% Additionally, in the Thinker-augmented MDP, the burn-in trajectories are truncated as agents have the option to reset and there is a maximum search depth.
	
	% learned model
	In meta-RL, it is generally assumed that the burn-in episodes and evaluation episodes are drawn from the same MDP, but this assumption does not hold when the burn-in episodes are generated by a learned model. But if the MDP underlying the burn-in episodes, $\hat{\mathcal{M}}$, is similar to that of the evaluation episode, $\mathcal{M}$, then the meta-RL algorithm may also learn how to utilize the collected data while accounting for the difference. This is analogous to how the RL agent in the Thinker-augmented MDP learns to use the imaginary rollout for better action selection, despite the possible inaccuracies of the model.
	
	% Some general closing or after-thought
	In summary, there is a close relationship between learning to plan and learning to learn. The former employs a model on each step to decide better actions at that state, while the latter uses episodes within a trial to determine better actions in the evaluation episodes. This highlights the converging ideas in meta-learning and planning, both aiming to make efficient use of available information for decision-making.
	
	\clearpage
	\section{Miscellaneous} \label{app:misc}
	\subsection{Related Work on the Dual Network}
	The dual network is similar to the model in MuZero \cite{schrittwieser2020mastering}, except for (i) the use of the dual network architecture and (ii) the prediction of future states trained by feature loss. Improvements to MuZero's model have been proposed. EfficientZero~\cite{ye2021mastering} deviates from MuZero by adding SimSiam loss \cite{chen2021exploring} to match future representations. We briefly experimented with it, but the result was only slightly better than that of MuZero's model, possibly because this loss works better in a limited data regime where EfficientZero was designed and evaluated. \cite{hamrick2021on} also proposes to predict future states as an auxiliary loss but does not use the dual network architecture. In our experiments, this addition led to better performance, as shown in Figure \ref{fig:dual}, but the result was still significantly worse than that of the dual network.
	
	Most other types of models used in model-based RL do not predict values and policies, as future values and policies are often not required in the algorithm. Among these models, various loss functions have been proposed for training the model to acquire meaningful representations for the task. \cite{ha2018recurrent} employs a VAE~\cite{KingmaW13} to encode the state while Dreamer~\cite{hafner2023mastering} utilizes a sequential VAE. Given the extensive body of research in model-based RL, we direct readers to the review~\cite{moerland2023model} for a detailed exploration of this field.
	
	\subsection{Planning Capacity}
	In this section, we discuss the planning algorithms that can be implemented in the Thinker-augmented MDP. Uninformed search strategies \cite[Ch~3.4]{stuart2010artificial} that do not require backward search, including Breadth-first search (BFS), Depth-first search (DFS), MCTS can be implemented by the RL agent in the Thinker-augmented MDP. For example, to implement BFS with two actions available, the agent can select ($a_1$, reset), ($a_2$, reset), ($a_1$, not reset), ($a_1$, reset), ($a_1$, not reset), ($a_2$, reset), … as the imaginary actions. If we treat the values as heuristics, then informed search strategies \cite[Ch~3.5]{stuart2010artificial} that do not require backward search, can also be implemented. For example, A* could be implemented by expanding the unexpanded node that has the highest rollout return in the tree, which can be achieved by keeping a record of unexpanded nodes along with the rollout return in the memories.
	
	Nonetheless, as an MDP does not necessarily involve a goal state as in planning problems and our trained model does not support backward unrolling, any backward search from a goal state, such as bidirectional search \cite[Ch~3.4]{stuart2010artificial} and partial-order planning \cite[Ch~10.4]{stuart2010artificial}, cannot be implemented by the agent. Agents are also unable to implement planning algorithms that involve heuristics beyond value function, such as GRAPHPLAN \cite[Ch~10.3]{stuart2010artificial} that employs heuristics based on planning graphs. In summary, an RL agent under the Thinker-augmented MDP can implement any forward-based planning algorithms that do not involve heuristics beyond the value functions.
	
	In practice, the learned planning algorithm is very different from the aforementioned handcrafted planning algorithms, as illustrated in Appendix \ref{app:beh} and the video visualization. This is because, unlike traditional planning algorithms whose goal is to solve the task in the search, the RL agent's primary incentive is to provide useful information for selecting the next immediate real action.
	
\end{document}