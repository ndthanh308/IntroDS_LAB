\begin{thebibliography}{10}

\bibitem{hafner2021mastering}
Danijar Hafner, Timothy~P Lillicrap, Mohammad Norouzi, and Jimmy Ba.
\newblock Mastering atari with discrete world models.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{sutton1990integrated}
Richard~S Sutton.
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In {\em Machine learning proceedings 1990}, pages 216--224. Elsevier,
  1990.

\bibitem{heess2015learning}
Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and
  Yuval Tassa.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{deisenroth2011pilco}
Marc Deisenroth and Carl~E Rasmussen.
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In {\em Proceedings of the 28th International Conference on machine
  learning (ICML-11)}, pages 465--472, 2011.

\bibitem{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock {\em Nature}, 588(7839):604--609, 2020.

\bibitem{coulom2007efficient}
R{\'e}mi Coulom.
\newblock Efficient selectivity and backup operators in monte-carlo tree
  search.
\newblock In {\em Computers and Games: 5th International Conference, CG 2006,
  Turin, Italy, May 29-31, 2006. Revised Papers 5}, pages 72--83. Springer,
  2007.

\bibitem{buzsaki2014emergence}
Gy{\"o}rgy Buzs{\'a}ki, Adrien Peyrache, and John Kubie.
\newblock Emergence of cognition from action.
\newblock In {\em Cold Spring Harbor symposia on quantitative biology},
  volume~79, pages 41--50. Cold Spring Harbor Laboratory Press, 2014.

\bibitem{gyorgy2019brain}
MD~Gy{\"o}rgy~Buzs{\'a}ki.
\newblock {\em The brain from inside out}.
\newblock Oxford University Press, 2019.

\bibitem{yin2006role}
Henry~H Yin and Barbara~J Knowlton.
\newblock The role of the basal ganglia in habit formation.
\newblock {\em Nature Reviews Neuroscience}, 7(6):464--476, 2006.

\bibitem{guez2019investigation}
Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, S{\'e}bastien
  Racani{\`e}re, Th{\'e}ophane Weber, David Raposo, Adam Santoro, Laurent
  Orseau, Tom Eccles, et~al.
\newblock An investigation of model-free planning.
\newblock In {\em International Conference on Machine Learning}, pages
  2464--2473. PMLR, 2019.

\bibitem{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484--489, 2016.

\bibitem{silver2017mastering}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock {\em arXiv preprint arXiv:1712.01815}, 2017.

\bibitem{oh2017value}
Junhyuk Oh, Satinder Singh, and Honglak Lee.
\newblock Value prediction network.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{farquhar2017treeqn}
Gregory Farquhar, Tim Rockt{\"a}schel, Maximilian Igl, and Shimon Whiteson.
\newblock Treeqn and atreec: Differentiable tree-structured models for deep
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1710.11417}, 2017.

\bibitem{weber2017imagination}
Th{\'e}ophane Weber, S{\'e}bastien Racaniere, David~P Reichert, Lars Buesing,
  Arthur Guez, Danilo~Jimenez Rezende, Adria~Puigdomenech Badia, Oriol Vinyals,
  Nicolas Heess, Yujia Li, et~al.
\newblock Imagination-augmented agents for deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1707.06203}, 2017.

\bibitem{macfarlane2022training}
Matthew Macfarlane, Diederik~M Roijers, and Herke van Hoof.
\newblock Training graph neural networks with policy gradients to perform tree
  search.
\newblock In {\em Deep Reinforcement Learning Workshop NeurIPS 2022}.

\bibitem{danihelka2022policy}
Ivo Danihelka, Arthur Guez, Julian Schrittwieser, and David Silver.
\newblock Policy improvement by planning with gumbel.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{guez2018learning}
Arthur Guez, Th{\'e}ophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol
  Vinyals, Daan Wierstra, R{\'e}mi Munos, and David Silver.
\newblock Learning to search with mctsnets.
\newblock In {\em International conference on machine learning}, pages
  1822--1831. PMLR, 2018.

\bibitem{tamar2016value}
Aviv Tamar, Yi~Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel.
\newblock Value iteration networks.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{pascanu2017learning}
Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien
  Racani{\`e}re, David Reichert, Th{\'e}ophane Weber, Daan Wierstra, and Peter
  Battaglia.
\newblock Learning model-based planning from scratch.
\newblock {\em arXiv preprint arXiv:1707.06170}, 2017.

\bibitem{anthony2019policy}
Thomas Anthony, Robert Nishihara, Philipp Moritz, Tim Salimans, and John
  Schulman.
\newblock Policy gradient search: Online planning and expert iteration without
  search trees.
\newblock {\em arXiv preprint arXiv:1904.03646}, 2019.

\bibitem{srinivas2018universal}
Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn.
\newblock Universal planning networks: Learning generalizable representations
  for visuomotor control.
\newblock In {\em International Conference on Machine Learning}, pages
  4732--4741. PMLR, 2018.

\bibitem{henaff2017model}
Mikael Henaff, William~F Whitney, and Yann LeCun.
\newblock Model-based planning with discrete and continuous actions.
\newblock {\em arXiv preprint arXiv:1705.07177}, 2017.

\bibitem{fickinger2021scalable}
Arnaud Fickinger, Hengyuan Hu, Brandon Amos, Stuart Russell, and Noam Brown.
\newblock Scalable online planning via reinforcement learning fine-tuning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:16951--16963, 2021.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256, 1992.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{ha2018recurrent}
David Ha and J{\"u}rgen Schmidhuber.
\newblock Recurrent world models facilitate policy evolution.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{KingmaW13}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In {\em 2nd International Conference on Learning Representations,
  {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
  Proceedings}, 2014.

\bibitem{ye2021mastering}
Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao.
\newblock Mastering atari games with limited data.
\newblock {\em Advances in Neural Information Processing Systems},
  34:25476--25488, 2021.

\bibitem{chen2021exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 15750--15758, 2021.

\bibitem{wang2016learning}
Jane~X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel~Z Leibo,
  Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.
\newblock Learning to reinforcement learn.
\newblock {\em arXiv preprint arXiv:1611.05763}, 2016.

\bibitem{duan2016rl}
Yan Duan, John Schulman, Xi~Chen, Peter~L Bartlett, Ilya Sutskever, and Pieter
  Abbeel.
\newblock Rl2: Fast reinforcement learning via slow reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.02779}, 2016.

\bibitem{bellman1957dynamic}
Richard Bellman.
\newblock {\em Dynamic Programming}.
\newblock Princeton University Press, Princeton, NJ, USA, 1957.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{espeholt2018impala}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward,
  Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In {\em International conference on machine learning}, pages
  1407--1416. PMLR, 2018.

\bibitem{botea2003using}
Adi Botea, Martin M{\"u}ller, and Jonathan Schaeffer.
\newblock Using abstraction for planning in sokoban.
\newblock In {\em Computers and Games: Third International Conference, CG 2002,
  Edmonton, Canada, July 25-27, 2002. Revised Papers 3}, pages 360--375.
  Springer, 2003.

\bibitem{bellemare2013arcade}
The arcade learning environment: An evaluation platform for general agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279, 2013.

\bibitem{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem{behnel2011cython}
Stefan Behnel, Robert Bradshaw, Craig Citro, Lisandro Dalcin, Dag~Sverre
  Seljebotn, and Kurt Smith.
\newblock Cython: The best of both worlds.
\newblock {\em Computing in Science \& Engineering}, 13(2):31--39, 2011.

\bibitem{horgan2018distributed}
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado
  van Hasselt, and David Silver.
\newblock Distributed prioritized experience replay.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{parisotto2020stabilizing}
Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre,
  Siddhant Jayakumar, Max Jaderberg, Raphael~Lopez Kaufman, Aidan Clark, Seb
  Noury, et~al.
\newblock Stabilizing transformers for reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  7487--7498. PMLR, 2020.

\bibitem{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock {\em arXiv preprint arXiv:1901.02860}, 2019.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{aitchison2022atari}
Matthew Aitchison, Penny Sweetser, and Marcus Hutter.
\newblock Atari-5: Distilling the arcade learning environment down to five
  games.
\newblock {\em arXiv preprint arXiv:2210.02019}, 2022.

\bibitem{LADOSZ20221}
Pawel Ladosz, Lilian Weng, Minwoo Kim, and Hyondong Oh.
\newblock Exploration in deep reinforcement learning: A survey.
\newblock {\em Information Fusion}, 85:1--22, 2022.

\bibitem{hamrick2021on}
Jessica~B Hamrick, Abram~L. Friesen, Feryal Behbahani, Arthur Guez, Fabio
  Viola, Sims Witherspoon, Thomas Anthony, Lars~Holger Buesing, Petar
  Veli{\v{c}}kovi{\'c}, and Theophane Weber.
\newblock On the role of planning in model-based deep reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{beck2023survey}
Jacob Beck, Risto Vuorio, Evan~Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea
  Finn, and Shimon Whiteson.
\newblock A survey of meta-reinforcement learning.
\newblock {\em arXiv preprint arXiv:2301.08028}, 2023.

\bibitem{zahavy2020self}
Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado~P van
  Hasselt, David Silver, and Satinder Singh.
\newblock A self-tuning actor-critic algorithm.
\newblock {\em Advances in neural information processing systems},
  33:20913--20924, 2020.

\bibitem{xu2020meta}
Zhongwen Xu, Hado~P van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, and
  David Silver.
\newblock Meta-gradient reinforcement learning with an objective discovered
  online.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15254--15264, 2020.

\end{thebibliography}
