\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint, nonatbib]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Additional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{graphicx} 
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{makecell}
\usepackage{chngcntr}

\usepackage{xargs}                      % Use more than one optional parameter in a new commands
% \usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
% \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
% \newcommandx{\im}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\usepackage{todonotes}


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\pr}{\text{Pr}}
\DeclareMathOperator*{\ex}{\mathbb{E}}
\DeclareMathOperator*{\var}{\text{Var}}
\newcommand{\defeq}{\vcentcolon=}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}

\title{Thinker: Learning to Plan and Act}

\author{%
  Stephen Chung \\
  University of Cambridge\\
  \texttt{mhc48@cam.ac.uk} \\
  \And
  Ivan Anokhin \\
  University of Cambridge\\
  \texttt{i.anokhin.mm@gmail.com} \\
  \And
  David Krueger \\
  University of Cambridge\\
  \texttt{dsk30@cam.ac.uk} \\  
}


\begin{document}


\maketitle
\begin{abstract}
We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. The algorithm's generality opens a new research direction on how a world model can be used in reinforcement learning and how planning can be seamlessly integrated into an agent's decision-making process.
\end{abstract}

\section{Introduction}

% update figure
% Figure environment removed

% intro
Model-based reinforcement learning (RL) has shown significant success in enhancing sample efficiency and performance by employing world models, or simply models, for generating additional training data \cite{hafner2021mastering, sutton1990integrated}, providing better estimates of the gradient \cite{heess2015learning, deisenroth2011pilco}, and planning \cite{schrittwieser2020mastering}. Despite these advances, a notable gap in existing research lies in the development of methods that allow an RL agent to autonomously interact with and utilize a model without relying on hand-crafted planning algorithms. To address this gap, we introduce the \emph{Thinker} algorithm, a novel approach that enables the agent to interact with a \emph{learned} model as an integral part of the environment, allowing the agent to adapt its planning strategies dynamically based on the situation.

% high-level description
The Thinker algorithm wraps a Markov Decision Process (MDP) with a learned model and introduces a new set of actions that enable agents to interact with the model. The agent can engage with the model to perform planning, such as proposing alternative plans to the model before selecting the final action. Importantly, the manner in which an agent uses a model is not fixed. In principle, an agent can learn any planning algorithm, such as a $n$-step exhaustive search or even Monte Carlo Tree Search (MCTS) \cite{coulom2007efficient}, or ignore the model if the state does not require planning. As the algorithm only dictates how an MDP is transformed, the Thinker can be used in conjunction with any RL algorithm.

% neuroscience
Drawing from neuroscience, our work is inspired by the hypothesis that the brain conducts imagination and planning via \emph{internalized actions}—actions processed within our experience-based virtual world rather than externally \cite{buzsaki2014emergence, gyorgy2019brain}. Under the hypothesis, both model-free and model-based behaviors share the same learning system, differing only in whether actions are passed to the external world or the experience-based virtual world. This idea aligns with the structural similarities found in brain regions governing model-free and model-based behaviors \cite{yin2006role}.

% experiment
Experimental results show that actor-critic algorithms, when applied to the Thinker-augmented MDP, yield state-of-the-art performance in Sokoban. Notably, this approach attains a solving rate of 94.5\% within 5e7 frames, a significant improvement over the 56.7\% solving rate achieved when the same algorithm is applied to the raw MDP. On the Atari 2600 benchmark, actor-critic algorithms using the Thinker-augmented MDP show a significant improvement over those using the raw MDP. The median human-normalized score on the Thinker-augmented MDP is 261\%, in contrast to the 102\% on the raw MDP. Visualization results, as shown in Fig \ref{fig:0}, reveal the agent's effective use of the model. To summarize, the Thinker algorithm provides the following advantages:

\begin{itemize}
\item \textbf{Flexibility:} The agent learns to plan on its own, without any hand-crafted planning algorithm, allowing it to adapt to different states, environments, and models.
\item \textbf{Generality:} The Thinker algorithm only dictates how an MDP is transformed, making it compatible with any RL algorithm. Specifically, one can convert any model-free RL algorithm into a model-based RL algorithm by switching to the Thinker-augmented MDP.
\item \textbf{Interpretability:} We can visualize the agent's plan prior to its execution, as depicted in Fig \ref{fig:0}. This provides greater insight compared to scenarios where the neural network internalizes the model and plan in a black box fashion \cite{guez2019investigation}. %Furthermore, we can control the degree of planning of a trained agent by adjusting the number of planning steps.
%\item \textbf{Biological plausibility:} We argue that the Thinker algorithm more closely resembles how a model is used in the brain, in contrast to hand-crafted planning algorithms like MCTS. % remove
\item \textbf{Aligned objective:} Both the real and imaginary actions are trained using the same rewards from the environment, ensuring that the objectives of planning and acting align.
\item \textbf{Improved learned model:} We introduce a novel combination of architecture and feature loss for the model, which is designed to prioritize the learning of task-relevant features and enable visualization at the same time.
%We introduce a novel combination of architecture and feature loss for the model, which is designed to prioritize the learning of useful features even during the prediction of raw frames.


%We propose a novel architecture and training approach for the model, designed to focus on learning useful features, even while predicting raw frames.
\end{itemize}

\section{Background and Notation}

We consider a Markov Decision Process (MDP) defined by a tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma, d_0)$, where $\mathcal{S}$ is a set of states, $\mathcal{A}$ is a finite set of actions, $P:\mathcal{S}\times \mathcal{A}\times \mathcal{S} \rightarrow [0,1]$ is a transition function representing the dynamics of the environment, $R: \mathcal{S}\times \mathcal{A} \rightarrow \mathbb{R}$ is a reward function, $\gamma \in [0,1]$ is a discount factor, and $d_0: \mathcal{S} \rightarrow [0,1]$ is an initial state distribution. Denoting the state, action, and reward at time $t$ by $S_t$, $A_t$, and $R_t$ respectively, $P(s, a, s') = \pr(S_{t+1}=s'|S_t=s, A_t=a)$, $R(s, a) = \ex[R_{t+1}|S_t=s, A_t=a]$, and $d_0(s) = \pr(S_{1}=s)$, where $P$ and $d_0$ are valid probability mass functions. An episode is a sequence of $(S_t, A_t, R_{t+1})$, starting from $t=1$ and continuing until reaching the terminal state, a special state where the environment ends. Letting $G_t = \sum_{k=t}^{\infty} \gamma^{k-t} R_k$ denote the infinite-horizon discounted return accrued after acting at time $t$, we are interested in finding, or approximating, a \emph{policy} $\pi:  \mathcal{S}\times \mathcal{A} \rightarrow [0,1]$, such that for any time $t \geq 1$, selecting actions according to $\pi(s,a)=\pr(A_t=a|S_t=s)$ maximizes the expected return $\ex[G_{t+1}|\pi]$. The value function for policy $\pi$ is $V^{\pi}$ where for all $s \in  \mathcal{S}$, $V^{\pi}(s) = \ex[G_{t+1}|S_t=s, \pi]$. We adopt the notation $(S_t, A_t, R_{t+1}, S_{t+1})$ for representing transitions, as opposed to $(S_t, A_t, R_{t}, S_{t+1})$, which facilitates a clearer description of the algorithm. In this paper, we define planning as the general process of interacting with an approximate transition function, or model, to guide action selection.

In this work, we only consider an MDP with a discrete action set. We use the subscript $t$ to denote the time step in a raw MDP and the subscript $k$ to denote the time step in the augmented MDP. 
\section{Algorithm}

% High-level description of the algorithm
The Thinker algorithm transforms a raw MDP, denoted as $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma, d_0)$, into an MDP augmented with a model, denoted as $\tilde{\mathcal{M}} = (\tilde{\mathcal{S}}, \tilde{\mathcal{A}}, \tilde{P}, \tilde{R}, \tilde{\gamma}, \tilde{d_0})$. This augmentation allows an RL agent to interact with the model and extract useful information from it while acting in the augmented MDP $\tilde{\mathcal{M}}$. The transformation process involves constructing an augmented action set, $\tilde {\mathcal{A}}$, and an augmented state space, $\tilde {\mathcal{S}}$. The augmented action set is defined as $\tilde {\mathcal{A}} \defeq \mathcal{A}^r \times \mathcal{A}^i \times \mathcal{A}^{reset}$, where $\mathcal{A}^r$ denotes the real action set, $\mathcal{A}^i$ denotes the imaginary action set, and $\mathcal{A}^{reset}$ denotes the reset action set. We define $\mathcal{A}^r \defeq \mathcal{A}$, $\mathcal{A}^i \defeq \mathcal{A}$, and $\mathcal{A}^{reset} \defeq \{0, 1\}$. The augmented state space is constructed as $\tilde {\mathcal{S}} \defeq \mathcal{S} \times \mathcal{Z}$, where $\mathcal{S}$ is the raw state space (or the real state space), and $\mathcal{Z}$ denotes the internal state space that contains the model's outputs.

The augmented MDP's transitions are divided into two types: real transitions and imaginary transitions. Together, these transitions form a \emph{planning stage}. A planning stage consists of $K$ transitions, where $K \geq 1$, and $K=20$ is used throughout the paper. Each planning stage begins with a root state that corresponds to the underlying raw state. The first $K-1$ transitions are imaginary transitions, during which the agent interacts with the model through imaginary and reset actions. The imaginary action unrolls the model by one step, while the reset action resets the model back to the root state. The last transition in the planning stage is a real transition, in which the real action is passed to the underlying raw MDP, and the raw MDP advances by one step. This completes a planning stage, and the next planning stage begins with the root state set to the new real state. In each of the $K$ steps, the agent sees a summary of the model's outputs, including the most recent hidden state and predicted quantities of the model. Thus, the augmented MDP allows the agent to `play' in the imagined world generated by the model during the $K-1$ imaginary transitions and to use this information to guide the selection of a real action. An illustration of this process is shown in Fig \ref{fig:1}. The details of the transitions, state representation, and rewards in the augmented MDP will be discussed in the following sections. The pseudo-code of the algorithm can be found in Appendix A. 

%if k=K, set new root state to s'
% Figure environment removed

\subsection{Augmented Transition} \label{sec:tran}

First, we describe the model used to augment the MDP, with its training details provided in Section \ref{sec:model}. The model consists of three components: an encoder, an unrolling function, and a predicting function. The encoder, denoted as $m^{\text{enc}}:\mathcal{S} \rightarrow \mathcal{H}$, transforms a real state into a hidden state, which has a significantly lower dimension than the real state. The unrolling function, denoted as $m^{\text{unroll}}:\mathcal{H} \times \mathcal{A} \rightarrow \mathcal{H}$, takes a hidden state and an imaginary action as input and produces the next hidden state as output. Finally, the predicting function, denoted as $m^{\text{pred}}: \mathcal{H} \rightarrow \mathcal{O}$, generates output statistics $o_k \in \mathcal{O}$ based on the hidden state.

The output statistics are chosen as $o_k = (\hat{r}_k, \hat{p}^{d}_k, \hat{v}_k, \hat{\pi}_k)$, where $\hat{r}_k \in \mathbb{R}$ denotes the predicted reward, $\hat{p}^{d}_k \in [0, 1]$ denotes the predicted probability of episode termination, $\hat{v}_k \in \mathbb{R}$ denotes the predicted value of the current policy, and $\hat{\pi}_k \in [0,1]^{|\mathcal{A}|}$ denotes the predicted action's distribution of the current policy. To avoid ambiguity, these statistics are associated with the raw MDP rather than the augmented MDP.

Given a model, we construct a planning stage as follows. We denote the root state as $s_{\text{root}} \in {\mathcal{S}}$, which remains unchanged during the planning stage. The internal state $z \in \mathcal{Z}$ is initialized to contain a tree with a single root node, $n_{\text{root}} = (h_{\text{root}}, o_{\text{root}})$, where $h_{\text{root}} \defeq m^{\text{enc}}(s_{\text{root}})$ and $o_{\text{root}} \defeq m^{\text{pred}} (h_{\text{root}})$. In addition to the tree, the initialized internal state $z$ also includes the current search depth, the number of steps passed, the visit count of each edge in the tree, and the current node pointer $n_{\text{cur}}$, which is initialized as the root node.

During each of the first $K-1$ imaginary transitions, the imaginary action at step $k \in \{1, 2, ..., K-1\}$, denoted as $a^{i}_k$, is used to expand a node from the current node. This process creates a new node, $n_{\text{new}} = (h_{\text{new}}, o_{\text{new}})$, where $h_{\text{new}} \defeq m^{\text{unroll}}(h_{\text{cur}}, a^{i}_k)$ and $o_{\text{new}} \defeq m^{\text{pred}} (h_{\text{new}})$. The new node is added to the tree and set as the current node. If the node has already been expanded, the previous computation can be reused since the model is deterministic. Afterwards, the current node pointer is reset to the root node if the reset action $a^{\text{reset}}_k=1$ or if the search depth exceeds $L$, the maximum search depth. We use a maximum search depth of $L=5$ throughout the paper. The reason for adding this restriction is that the model is trained by unrolling $L=5$ steps, and thus unrolling for more steps may not provide an accurate prediction. This completes a single imaginary transition. Finally, in the last real transition, the real action $a^{r}_{K}$ is passed to the raw MDP. The new raw state returned by executing action $a^{r}_{K}$ is set as the new root state. It is important to note that the real action at the first $K-1$ steps, the imaginary action, and the reset action at the last step $K$ are not used. 

\subsection{Augmented State Representation} \label{sec:tran}

As the internal state $z$ contains a tree with a variable number of nodes, $z$ cannot be directly fed into a neural network. Moreover, we can compute certain quantities to help the agent utilize the model. In particular, the rollout return over a path in a tree is useful for guiding action. That is, for a node $i_0$ and its $n$-level descendant node $i_n$, we can compute the rollout return from node $i_0$ to node $i_n$ through the path $(i_0, i_1, i_2, ..., i_n)$ as:
\begin{equation}
    g_{i_0, i_n} \defeq \hat{r}_{i_0} + \gamma \hat{r}_{i_1} + \gamma^2 \hat{r}_{i_2} + ... \gamma^{n} \hat{r}_{i_{n}} + \gamma^{n+1} \hat{v}_{i_{n}}. \label{eq:1}
\end{equation}
For a given node $i$, we can compute $g_{i, j}$ for all its descendant nodes $j$. We also define $g_{i, i} \defeq \hat{r}_{i} + \gamma \hat{v}_{i}$. We define the mean rollout return $g_{i}^{\text{mean}}$ and the maximum rollout return $g_{i}^{\text{max}}$ as the mean of $g_{i, j}$ and the maximum of $g_{i, j}$ over all descendant nodes $j$ (including itself) respectively. It should be noted that the mean and maximum rollout returns may change after each imaginary action as the tree may be expanded. However, the subscript $k$ for step $k$ is omitted here for clarity.

Altogether, we represent a node $i$ by a vector $u_i$ that contains: (a) the node output statistics, $o_i$, (b) the one-hot encoded action that leads to node $i$, (c) the mean rollout return of its child nodes, (d) the maximum rollout return of its child nodes, (e) the visit count of its child nodes. It should be noted that the auxiliary statistics (c) to (e) can be computed by the agent (assuming the agent has memory) using the (a) and (b) observed during the planning stage, but we compute them manually to make using the model easier for the agent. These auxiliary statistics, which are also used in the action selection process of MCTS (the mean rollout return here corresponds to the `mean value' in MCTS), are inspired by MCTS \cite{coulom2007efficient}. The agent can also ignore these statistics if they are not useful.

Finally, the augmented state representation is composed of (a) the root node's representation $u_\text{root}$, (b) the current node's representation $u_\text{cur}$, (c) the hidden state of the current node $h_\text{cur}$, (d) supplementary information, such as the number of steps passed in the planning stage.  The root node's representation is essential for selecting the real action in a real transition, while the current node's representation is important for determining the next node to be expanded in an imaginary transition. This fixed-size augmented state representation is passed to the agent in both imaginary and real transitions.

Even without memory, the agent can perform various common planning algorithms, such as $n$-step exhaustive search and MCTS, due to the auxiliary statistics provided by the augmented MDP. However, the inclusion of only two nodes' statistics turns the problem into a partially observable MDP. Therefore, using an agent with memory, such as an actor-critic algorithm with an RNN, may allow learning planning algorithms beyond these basic approaches.

\subsection{Augmented Reward} \label{sec:plr}

A simple choice for the reward function in the augmented MDP is to assign zero rewards for all imaginary transitions and the reward from the raw MDP for all real transitions. In other words, $\tilde{r}_k = 0$ for $k \leq K$ and $\tilde{r}_{K+1} = r$, where $r$ represents the raw reward obtained from the last real transition. We also set the new discount rate $\tilde{\gamma} \defeq \gamma^{\frac{1}{K}}$ to account for the $K-1$ imaginary transitions preceding one real transition.

However, we observe that the sparse reward may make learning difficult. We suggest adding an auxiliary reward, which we call the \emph{planning reward}, to help guide the learning of imaginary and reset actions. We define the planning reward $\tilde{r}^{p}_k$ in step $2 \leq k \leq K+1$ of a planning stage to be:

\begin{equation}
\tilde{r}^{p}_k \defeq 
\begin{cases}
c^{p} \left( g_{\text{root},k}^{\text{max}} - g_{\text{root},k-1}^{\text{max}} \right), & \text{if } k \leq K \\
0, & \text{if } k = K + 1,
\end{cases}
\end{equation}

where $g^\text{max}_{\text{root}, k}$ denotes the maximum rollout return of the root node on step $k$, as defined after Equation \ref{eq:1}, and $c^{p} \geq 0$ is a scalar hyperparameter that controls the strength of the planning reward. The sum of planning rewards in a planning stage thus equals the maximum rollout return from the root node subtracting the root's initial value. The intuition is that imaginary actions should search for a path that leads to the largest rollout return, while not penalizing for searching paths with a low rollout return to encourage exploration in imagination.

However, this planning reward should not be given to real actions to prevent incentive misalignment. For example, collecting a large amount of planning reward is possible by continually doing nothing in real transitions but imagining a good plan. As such, the planning reward signal has to be separated from the raw reward signal $\tilde{r}_k$ instead of being added together. We also decay $c^{p}$ from $1$ to $0$ throughout training to further ensure that the imaginary action is aligned with the raw reward function. Analysis of the planning reward are described in Appendix H. Experiments show that planning reward only provides a moderate increase in the initial learning performance, suggesting that the planning reward may not be a critical element of learning to plan.

\subsection{The World Model} \label{sec:model}

% Figure environment removed

As outlined in Section \ref{sec:tran}, our model comprises three main components: an encoder $m^{\text{enc}}$, an unrolling function $m^{\text{unroll}}$, and a predicting function $m^{\text{pred}}$. Given the similarities to MuZero \cite{hafner2021mastering}, one could adopt the same architecture and training methodology used by MuZero, which involves storing all transitions with the raw MDP in a replay buffer and training the network to predict output statistics by unrolling the model for $L$ steps using samples from the replay buffer. Nonetheless, this approach suffers from sample inefficiency since it discards information from the future state, which carries a rich supervised learning signal. Moreover, interpreting the model becomes challenging, making it difficult to discern the agent's planning process. To address these issues, we propose a novel architecture called the \emph{dual network}, which utilizes the future state as a learning signal. %This dual network not only facilitates easy visualization of the model but also allows the model to focus on learning important features for control.

The dual network consists of two sub-networks, which we refer to as the stationary network and the non-stationary network. For clarity, we use subscript $t$ here to refer to a real step instead of the step number $k$ in a planning stage. The stationary network's input consists of the current state $s_{t}$ and a sequence of raw actions $a_{t+1:t+L}$. Based on an RNN, the stationary network predicts future states $\hat{s}_{t+1:t+L}$, future rewards $\hat{r}_{t+1:t+L}$, and future episode termination probability $\hat{p}^d_{t+1:t+L}$. Meanwhile, the non-stationary network takes both the stationary network's input and its predicted states $\hat{s}_{t+1:t+L}$ as inputs. Also based on an RNN, the non-stationary network predicts future action's probability $\hat{\pi}_{t:t+L}$ and the value of the current policy $\hat{v}_{t:t+L}$ on the future states. The architecture of the dual network is shown in Figure \ref{fig:2}.

In addition to the supervised training loss on the four predicted quantities, we propose adding a supervised learning signal for the predicted state $\hat{s}_{t+l}$ for $1 \leq l \leq L$:
\begin{equation}
    \label{feature_loss}
    \mathcal{L}^{s} (\hat{s}_{t+l}, s_{t+1}) \defeq \vert\vert g(\hat{s}_{t+l}) - g(s_{t+l}) \vert\vert^2_2,
\end{equation} 
where $g$ represents the encoder of the non-stationary network. Rather than directly minimizing the L2 loss between the predicted state and the actual state, we minimize the L2 loss in the feature space. This encourages the model to focus on learning task-relevant features. 

It is important to note that $g$ is not being optimized when minimizing this loss, and $\hat{s}_{t+l}$ does not receive gradients from the non-stationary network, as the two sub-networks are separately optimized. Further details regarding the model's training can be found in Appendix B.

The dual network leverages both stationary and non-stationary aspects of the environment for a comprehensive state dynamics representation. The stationary network focuses on predicting state and reward features that are consistent across policies, while the non-stationary network predicts policy-dependent features that change as the agent's policy evolves. This separation facilitates effective adaptation to policy evolution. The dual network also encourages efficient learning through feature loss and enables easier visualization and interpretation of the agent's plan.


\section{Related Work}

Various approaches exist for incorporating planning into RL algorithms, with the majority relying on hand-crafted planning algorithms. MuZero~\cite{schrittwieser2020mastering}, AlphaGo~\cite{silver2016mastering}, and AlphaZero~\cite{silver2017mastering} employ MCTS. VPN~\cite{oh2017value} simulates the real policy to a particular depth, considering only a select number of the best actions from each imaginary state. Alternatively, TreeQN and ATreeC~\cite{farquhar2017treeqn} explore all possible action sequences up to a predefined depth. I2A~\cite{weber2017imagination} generates one rollout per action, with the first action iterating over the action set. More recently, ~\cite{macfarlane2022training, danihelka2022policy} introduced improvement for MCTS. 

Previous studies have also proposed methods that learn to plan. MCTSnets~\cite{guez2018learning} is a supervised learning algorithm that learns to imitate actions from expert trajectories. VIN \cite{tamar2016value} and DRC ~\cite{guez2019investigation} are model-free approaches that utilize a special neural network architecture. VIN’s architecture aims to facilitate value iteration, while DRC’s architecture aims to facilitate general planning. Lastly, IBP~\cite{pascanu2017learning} allows the agent to interact with the model and the environment. However, the imaginary action is trained by backpropagating through the model,  which only works well for continuous action sets. Moreover, IBP does not predict values and policies, which are important in evaluating rollouts.

Another line of research utilizes gradient updates to plan within a planning stage \cite{anthony2019policy, srinivas2018universal, henaff2017model, fickinger2021scalable}. These methods start with a preliminary imaginary policy and use this policy to gather rollouts from the model. Subsequently, these rollouts are used to calculate gradient updates for the imaginary policy, thus incrementally refining the rollouts' quality. In the final step, a hand-crafted function is used to select a real action from the results of the rollouts. For instance, \cite{anthony2019policy}, \cite{henaff2017model} and \cite{fickinger2021scalable} propose maximizing the rollout return using REINFORCE \cite{williams1992simple}, PPO \cite{schulman2017proximal}, and backpropagation through the model, respectively.

Various loss functions have been proposed for training the model to acquire meaningful representations. \cite{ha2018recurrent}  employs a VAE \cite{KingmaW13} to encode the state, while Dreamer \cite{hafner2021mastering} utilizes a sequential VAE. MuZero \cite{schrittwieser2020mastering} focuses solely on fitting task-relevant predictions comprising future rewards, policies, and values. EfficientZero\cite{ye2021mastering} deviates from MuZero by adding SimSiam loss \cite{chen2021exploring} to match future representations. In contrast to these methods, our proposed dual network fits the next-state prediction with a feature loss that enables visualization and prioritizes the learning of task-relevant features.

We describe the connection between our algorithm and Meta-RL \cite{wang2016learning, duan2016rl}, as well as policy iteration \cite{bellman1957dynamic, sutton2018reinforcement}, in Appendix I.

\section{Experiments}
For all our experiments, we train a standard actor-critic algorithm, specifically the IMPALA algorithm \cite{espeholt2018impala}, on the Thinker-augmented MDP. Although other RL algorithms could be employed, we opted for the IMPALA due to its computational efficiency and simplicity. The actor-critic's network uses an RNN for encoding the node representations and a convolutional neural network for encoding the model's hidden state. These encoded representations are concatenated, then passed through a linear layer to generate actions and predicted values. We set the number of planning steps, $K$, to $20$, and the maximum search depth or model unroll length, $L$, to $5$. As a comparison baseline, we train the same actor-critic algorithm on the raw MDP, employing a similar network architecture used by the model to encode the raw state. More experiment details can be found in Appendix E.

\subsection{Sokoban}
% Figure environment removed

% Figure environment removed

We selected the game of Sokoban \cite{botea2003using, weber2017imagination}, a classic puzzle problem, as our primary testing environment due to its inherent complexity and requirement for extensive planning. In Sokoban, the objective is to push all four boxes onto the designated red target squares, as depicted in Figure \ref{fig:0}. We used the unfiltered dataset comprising 900,000 Sokoban levels from \cite{guez2019investigation}. The training of all algorithms was conducted with 5e7 frames.

The learning curves of the actor-critic algorithm applied to the raw MDP and the Thinker-augmented MDP is shown in Fig \ref{fig:main_sokoban}, with results averaged over three seeds. We have also reproduced the results of DRC \cite{guez2019investigation}, the current state-of-the-art for Sokoban. In terms of initial learning speed and final performance, Thinker surpasses the other baselines. Thinker solves more than 80\% of the Sokoban games at 1e7 frames, whereas DRC requires over 2e7 frames to reach the same level. The trained agents' performance, assessed on the unfiltered test dataset from \cite{guez2019investigation}, is presented in Table \ref{table:1}. %, further underscores the algorithm's strength.

To understand the benefits of planning, we evaluate a policy trained with $K=20$ on the Thinker-Augmented MDP, by testing its performance with different $K$ values in the same augmented environment during the testing phase. This variability allows us to control the degree of planning and the result is shown in Figure \ref{fig:main_sokoban_infK}. The figure also depicts the performance of the policy predicted by the model, which can be regarded as a distilled policy devoid of planning. We observe a significant performance improvement attributable to planning, even at the end of training.

The agent's behavior, visualized in Figure \ref{fig:0}, illustrates that it learns to utilize the model for selecting better actions. Interestingly, it appears that the agents learn a planning algorithm that diverges from traditional $n$-step-exhaustive search and MCTS. For instance, the agent chooses real actions based on both visit counts and rollout returns, and the agent learns to reset upon entering an irreversible state, contrasting the MCTS strategy of resetting at a leaf node. Further analysis on the trained agent's behavior can be found in Appendix G.

We conducted an ablation analysis to examine the impact of planning duration, varying the number of planning steps $K$ across $\{1, 2, 5, 10, 20\}$ during both training and testing. When $K=1$, the agent does not interact with the model at all. The outcomes of this ablation analysis are presented in Figure \ref{fig:main_sokoban_K}. A shift from $K=1$ to $K=2$ resulted in a significant performance improvement, indicating that even a single interaction with the model can provide substantial benefits. Furthermore, the results suggest that $K=10$ already gives the optimal performance. This is in stark contrast to the 2000 simulations required by MCTS to achieve good performance in Sokoban \cite{guez2018learning}.

Additional ablation studies, addressing aspects such as the agent's memory, the planning reward, and $L$, are available in Appendix F. In summary, we found that neither the removal of the agent's memory nor the planning reward significantly affects the agent's performance in Sokoban. We hypothesize that the chosen node's representation already provides a good and sufficient summary for planning and acting, rendering additional memory and reward signals unnecessary. 

\subsection{Atari 2600}

Finally, we test our algorithm on the Atari 2600 benchmark \cite{bellemare2013arcade} using a 200M frames setting. The performance of the actor-critic algorithm on both the Thinker-augmented MDP and raw MDP is evaluated using the same hyperparameters as in the Sokoban experiment, but with a deeper neural network for the model and an increased discount rate from 0.97 to 0.99.

The learning curve, measured in the median human-normalized score across 57 games, is displayed in Figure \ref{fig:main_atari}. We evaluate the agent in the no-ops starts regime at the end of the training, and the result is shown in Table \ref{table:2}\footnote{The Rainbow results presented here are taken from the original Rainbow paper \cite{hessel2018rainbow}, where the reported score is obtained from the best agent snapshots throughout training.}. The median normalized score for the actor-critic algorithm applied on the Thinker-augmented MDP and the raw MDP are 261\% and 102\%, respectively, underscoring the advantages of the Thinker-augmented MDP\footnote{The difference between the median normalized score for the raw MDP and the result reported in \cite{guez2019investigation} can be mainly attributed to two factors: (i) we did not use greyscale in the environment pre-processing, and (ii) we used a deeper neural network. Both of these factors increase the difficulty of learning. }. For context, the median normalized score of Rainbow \cite{hessel2018rainbow}, a robust baseline, is 223\%. It is important to note, however, that any RL algorithms, including Rainbow, could be applied to the Thinker-augmented MDP, so this comparison may not be entirely indicative. As the goal of the experiments is to investigate the benefits of the augmented MDP, we do not include other common baselines here.

The advantages of the Thinker algorithm are particularly pronounced in certain types of games, especially shooting games such as \texttt{chopper command}, \texttt{seaquest}, and \texttt{phoenix}. It is plausible that the agents benefit from predicting the trajectories of fast-moving objects in these shooting games. However, in some games, particularly those where a single action does not produce significant effects, the Thinker algorithm appears to offer no advantages. For instance, in the game \texttt{qbert}, the decision to move to a different box is made only once every five or more actions. This effectively transforms any 5-step plan into a single-step plan, diminishing the algorithm's benefits. Detailed experiment results on the Atari 2600 are available in Appendix E.

Many interesting behaviors learned by the agent are visualized in \url{https://youtu.be/0AfZh5SR7Fk}. We also observe that the predicted frames in the video are of high quality, showing that the feature loss can lead to the emergence of interpretable and visualizable predicted frames, despite the absence of an explicit loss in the state space. We attribute this capability to using a convolutional mapping $g$ in the feature loss (\ref{feature_loss}), as convolutions can preserve local features. 

\section{Future Work and Conclusion}
While the Thinker algorithm offers many advantages, it also presents several limitations that provide potential areas for future research. Firstly, the Thinker algorithm carries a large computational cost. A single step in the original MDP corresponds to $K$ steps in the augmented MDP, in addition to the overhead of model training. Secondly, the algorithm currently enforces rigid planning, requiring the agent to roll out from the root state and restricts it to planning for a fixed number of steps. Thirdly, the algorithm assumes a deterministic environment, with the model not predicting stochastic transitions.

Future work could focus on addressing these limitations, such as enabling more flexible planning steps and integrating uncertainty into the model. Additionally, exploring the application of the algorithm in multi-environment settings is a promising direction. We hypothesize that the advantages of a learned planning algorithm over a hand-crafted one will become more pronounced in such scenarios, given that planning is a universally applicable skill across many environments. Exploring how other RL algorithms perform within the Thinker-augmented MDP is an additional direction for future work.
% model-free and model-based in mammal here

In conclusion, we introduce the Thinker algorithm, an MDP transformation approach that enables agents to learn planning and acting jointly. We posit that the universality of this algorithm paves the way for new research exploring how models can be used.%, potentially in a more biologically plausible manner.


% Figure environment removed

\clearpage

\bibliographystyle{unsrt}
\bibliography{citation}

\clearpage
\appendix
\counterwithin{figure}{section} % Reset figure counter and append section number
\counterwithin{table}{section}

\section{Algorithm} \label{sec:A}
\begin{algorithm}
\caption{Thinker-Augmented MDP}
\label{alg:1}
\begin{algorithmic}[1]
\State \textbf{Input:} raw environment: \textit{env}, model: \textit{model}
\State \textbf{Algorithm Parameter:} number of planning step $K \geq 1$; maximum search depth $L \geq 1$
\State \textbf{Initialize $k \leftarrow K$}   \Comment {Ensure that the first step is a real step}
\Procedure{Step}{$real\_action, im\_action, reset\_action, real\_action\_prob$}
%\State \textbf{Procedure} Step(real\_action, im\_action, reset\_action, real\_action\_prob):
\If{$k < K$} \Comment{Imaginary step}
\State $h\_new \leftarrow model.unroll(node\_cur.h, im\_action)$ \Comment{Compute hidden states} \label{code:ref0}
\State $o\_new \leftarrow model.pred(h\_new)$  \Comment{Compute output statistics} \label{code:ref1}
\State $node\_new \leftarrow Node(h\_new, o\_new)$
\If{end node of $(node\_cur, im\_action)$ exists in $tree$} \Comment {Update visited node}
\State Update contents of $(node\_cur, im\_action)$'s end node to that of $node\_new$
\State Increase the visit count of the end node of $(node\_cur, im\_action)$ by $1$
\Else \Comment {Expand new node}
\State Add $node\_new$ as the end node of $(node\_cur, im\_action)$ in $tree$ 
\EndIf
\State $node\_cur \leftarrow node\_new$
\State $search\_depth \leftarrow search\_depth + 1$
\State $reward \leftarrow [0, max(tree.max\_root\_return - last\_max\_root\_return, 0)]$  \label{code:planrew}
\State $state\_rep \leftarrow $ state representation computed as described below
\State $done \leftarrow False$
\If{$reset\_action == 1$ OR $search\_depth \geq L$}
\State $node\_cur \leftarrow tree.node\_root$  \Comment{Reset $n_{\text{cur}}$ to $n_{\text{root}}$ if needed}
\State Increase the visit count of the root node by $1$,  $search\_depth \leftarrow 0$
\EndIf
\State $k \leftarrow k + 1$
\Else \Comment{Real step}
\State $real\_reward, state, done \leftarrow env.step(real\_action)$ \Comment{Step in the raw environment}
\If{$tree$ exists}  \Comment{$tree$ exists except for the first step}
\State $send\_buffer(real\_action, real\_action\_prob, reward, state, done,$ 
\Statex \hspace{2.3cm} $tree.mean\_root\_return)$ \Comment{Send raw transitions to buffer to train model}
\EndIf
\State $h\_root \leftarrow model.enc(state)$ \Comment{Compute hidden states}
\State $o\_root \leftarrow model.pred(h\_root)$ \Comment{Compute output statistics}
\State $o\_root.r \leftarrow 0$ \Comment{Set root's predicted reward $o\_root.r$ to $0$ as it has been collected}
\State $node\_cur \leftarrow Node(h\_root, o\_root)$
\If{NOT $done$ AND $tree$ exists AND end node of $(tree.node\_root, real\_action)$ exists in $tree$} \Comment {Retain the tree} \label{code:retain}
\State Update contents of $(tree.node\_root, real\_action)$'s end node to that of $node\_cur$
\State $tree.node\_root \leftarrow$ end node of $(tree.node\_root, real\_action)$
\State Remove non-descendent nodes of the root node in $tree$
\Else \Comment{Create a new tree with a root node}
\State $tree \leftarrow Tree(node\_cur)$  
\EndIf
\State $k \leftarrow 1$  \Comment{Reset number of planning steps}
\State $search\_depth \leftarrow 0$  \Comment{Reset current search depth}
\State $reward \leftarrow [real\_reward, 0]$  \Comment{Real reward and planning reward}
\State $state\_rep \leftarrow $ state representation computed as described below
\EndIf
\State $last\_max\_root\_return \leftarrow tree.max\_root\_return$ \Comment{Record last $g_{\text{root}}^{\text{max}}$}
\State \textbf{return} $reward, state\_rep, done$
\EndProcedure
\end{algorithmic}
\end{algorithm}

In the algorithm above\footnote{The full code is accessible at \url{https://github.com/anonymous-scrl/thinker}.}, the term `end node of $(node, action)$' refers to the node in the tree that is reached by executing the specified $action$ at $node$. Updating the contents of a node $i$ to node $j$ means that we set the hidden state and output statistics of node $i$ to those of node $j$. All nodes are initialized to have a visit count of $1$.

The algorithm defines a step function, representing the transition in the Thinker-augmented MDP. This function takes as input a real action $real\_action$, an imaginary action $im\_action$, a reset action $reset\_action$, and the distribution of the real action $real\_action\_prob$. The function operates in two modes: an imaginary step mode, where it unrolls the model using the imaginary action and updates the internal tree representation, and a real step mode, where it interacts with the actual environment. During an imaginary step, the function updates the current node, tree statistics, and planning reward based on model predictions. If a reset action is triggered or the search depth exceeds a threshold, the current node is reset to the root. During a real step, the function performs the real action in the raw environment, updates the root node of the tree based on the new state, and passes the real reward. The function outputs a reward $reward$, which is a vector that consists of the real reward and the planning reward, an augmented state representation $state\_rep$, and an indicator of episode end $done$.

\paragraph{Implementation details} Several key implementation details are worth noting. First, we do not prune the entire tree when updating the root node, as indicated in Line \ref{code:retain}. This approach ensures the continuity of the previous plan into the subsequent planning stage. Secondly, given the deterministic nature of the model, we can skip Line \ref{code:ref0} and \ref{code:ref1} if the new node has already been visited during the current planning stage. However, it is important to note that if the new node was only visited in the preceding planning stage, we still need to calculate the new hidden state and output statistics since the root node's hidden state is updated. Thirdly, in Line \ref{code:planrew}, we calculate the planning reward as $\max(g_{\text{root},k}^{\text{max}} - g_{\text{root},k-1}^{\text{max}},0)$. We add $\max(\cdot, 0)$ because the maximum rollout return can diminish if we carry the tree. The reason being, the value of a node visited in earlier planning stages may decrease as a result of the updated hidden state of the root node. Nevertheless, the agent should not be penalized for revisiting nodes that were overly optimistic in previous planning stages. Finally, in Line \ref{code:ref0} and \ref{code:ref1}, if the model predicts an episode ending at a node $i$, defined by $\hat{p}^d_i > 0.5$, we maintain the same hidden state and output statistics when unrolling the model from node $i$. The only exception is that the predicted reward $\hat{r}$ and predicted value $\hat{v}$ in the new output statistics are set to 0.

\paragraph{Augmented State Representation} 
The augmented state representation is described in Section 3.2. Here, we provide additional details about the augmented state representation. The augmented state is divided into two parts: the first part is the tree statistics as a flat vector, and the second part is the hidden state of the model as a 3D vector. Denoting the current search depth by $d$ and the current step in a planning stage by $k$, the tree statistics is a vector of size $10|\mathcal{A}| + K + 9$, composed of:

\begin{itemize}
    \item $\hat{r}_\text{root} \in \mathbb{R}$: Predicted reward of the root node.
    \item $\hat{v}_\text{root} \in \mathbb{R}$: Predicted value of the root node.
    \item $\hat{\pi}_\text{root} \in \mathbb{R}^{|\mathcal{A}|}$: Predicted policy of the root node.
    \item $\text{one\_hot}(a_\text{root}) \in [0,1]^{|\mathcal{A}|}$: Action leading to the root node, represented in one-hot encoding.
    \item $g_{\text{root\_child}}^{\text{max}} \in  \mathbb{R}^{|\mathcal{A}|}$: Maximum rollout return of the root's child nodes.
    \item $g_{\text{root\_child}}^{\text{mean}} \in  \mathbb{R}^{|\mathcal{A}|}$: Mean rollout return of the root's child nodes.
    \item $\frac{1}{K} n_{\text{root\_child}} \in  \mathbb{R}^{|\mathcal{A}|}$: Visit count of the root's child nodes.    
    \item $\hat{r}_\text{cur} \in \mathbb{R}$: Predicted reward of the current node.
    \item $\hat{v}_\text{cur} \in \mathbb{R}$: Predicted value of the current node.
    \item $\hat{\pi}_\text{cur} \in \mathbb{R}^{|\mathcal{A}|}$: Predicted policy of the current node.
    \item $\text{one\_hot}(a_\text{cur}) \in [0,1]^{|\mathcal{A}|}$: Action leading to the current node, represented in one-hot encoding.
    \item $g_{\text{cur\_child}}^{\text{max}} \in  \mathbb{R}^{|\mathcal{A}|}$: Maximum rollout return of the current node's child nodes.    
    \item $g_{\text{cur\_child}}^{\text{mean}} \in  \mathbb{R}^{|\mathcal{A}|}$: Mean rollout return of the current node's child nodes.
    \item $\frac{1}{K} n_{\text{cur\_child}} \in  \mathbb{R}^{|\mathcal{A}|}$: Visit count of the current node's child nodes.    
    \item $g_{\text{root}, \text{cur}} \in  \mathbb{R}$: Current rollout return.
    \item $g_{\text{root}, \text{cur}} - \gamma^{d+1} \hat{v}_{\text{cur}} \in  \mathbb{R}$: Sum of discounted rewards collected in the current rollout.
    \item $g_{\text{root}}^{\text{max}} \in \mathbb{R}$: Maximum rollout return of the root node.
    \item $\gamma^{d+1} \in \mathbb{R}$: Trailing discount factor.
    \item $\text{one\_hot}(k) \in [0,1]^K$: Current step in a planning stage, represented in one-hot encoding.
    \item $i^{\text{reset}}  \in [0,1]$: Indicator of whether the current node has just been reset to the root node.
\end{itemize}

The visit count is divided by $K$ to normalize the magnitude of the input. While calculating the maximum and mean rollout returns, a simple approach would be to iterate over all descendant nodes and compute the rollout return for each one. However, a more efficient method involves maintaining a buffer for each node's rollout return. When a new descendant node is added to the tree, a new rollout return is inserted into this buffer. With this method, special attention must be paid when the predicted reward and value of a node change, a situation that can occur due to carrying the tree over planning stages.

As for the hidden state branch, it consists of the hidden state of the current node $h_\text{cur}$. In the context of the dual network outlined in Section 3.4, $h_\text{cur}$ is defined as the concatenated hidden states of both the stationary network, $h^\text{stat.}_\text{cur}$, and the non-stationary network, $h^\text{non-stat.}_\text{cur}$.

\paragraph{Code} 
The full code is accessible at \url{https://github.com/anonymous-scrl/thinker}. We have implemented the Thinker-augmented MDP in batches using Cython \cite{behnel2011cython}, a C extension for Python, to enhance efficiency. By leveraging the high-performance capabilities of C, we ensure that the primary computational cost lie with the model training and RL algorithm instead of the augmented MDP's logic flow and tree processing.

\section{Model Training Procedure} \label{sec:B}

This section outlines the specifics of model training. The model's training process closely follows the MuZero approach, but also incorporates the feature loss on future states as per Equation (3).

\subsection{Collection of Training Samples}

Algorithm \ref{alg:1} illustrates that a transition tuple $x_t \defeq (a_t, \pi_{t}, g^{\text{mean}}_{\text{root}, t}, r_{t+1}, s_{t+1}, d_{t+1})$ is stored in a replay buffer during each real step. Here, $a_t \in \mathcal{A}$ denotes the action, $\pi_{t} \in \mathbb{R}^{|\mathcal{A}|}$ denotes the distribution of action $a_t$, while  $g^{\text{mean}}_{\text{root}, t} \in \mathbb{R}$ denotes the mean rollout return at the root node, $r_{t+1} \in \mathbb{R}$ and $s_{t+1} \in \mathcal{S}$ denote the reward and state following the execution of action $a_t$, and $d_{t+1} \in [0,1]$ denotes a binary indicator denoting if the episode has ended post action $a_t$. This tuple is added to a first-in-first-out buffer with a capacity of 200,000 transitions. To train the model, a batch of sequences $x_{i:i+2L}$ is randomly sampled from the buffer, where $L$ represents the model rollout length. We use a batch size of 128 and a model unroll length $L=5$.

Consistent with MuZero \cite{schrittwieser2020mastering}, prioritized sampling \cite{horgan2018distributed} is used for sampling the sequence $x_{i:i+2L}$. Each transition tuple is assigned a priority $p_i$. Newly added transition tuples are given the maximum priority value in the buffer. During training, the priority $p_i$ of the first transition in a sampled sequence $x_{i:i+2L}$ is updated to the L1 loss of the value, $\vert \hat{v}_i - v^{\text{target}}_i \vert$. Transitions are sampled with a probability of $P(i) = \frac{p_i^\alpha}{\sum_j p_j^\alpha}$, where $\alpha=0.6$. Each sampled transition sequence's loss is weighted by the importance weight $w_i = ( \frac{1}{N P(i)} )^\beta$, where $N$ is the total number of transition tuples in the buffer, and $\beta$ is annealed from $0.4$ to $1.0$ throughout training.

We employ a replay ratio of $6$, implying that model training is paused if the ratio of the total number of transitions trained on the model to the total collected transitions exceeds $6$.

\subsection{Model Loss} 

We described in Section 3.4 that the dual network comprises two separate networks, the stationary and non-stationary networks, each subject to individual optimization, thereby necessitating two distinct optimizers. For any given training sequence $x_{t:t+2L}$, the stationary network loss is calculated as an aggregate of the future rewards' L2 loss, future termination indicator's cross-entropy loss, and future feature loss:

\begin{equation}
        \mathcal{L}^{\text{stationary}} \defeq \sum_{l=1}^L \left( c^r (\hat{r}_{t+l} - r_{t+l})^2 + c^d d_{t+l} \log \hat{p}^d_{t+l}  +  c^s \vert\vert  g(\hat{s}_{t+l}) - g(s_{t+l}) \vert\vert^2_2  \right),
\end{equation}
where $g$ denotes the non-stationary network's encoding function and $c \geq 0$ are scalar hyperparameters that modulate the loss strength. As $g$ belongs to the non-stationary network, its parameters are not included in the stationary network's optimizer, thus $g$ remains unoptimized during this loss minimization.

For the non-stationary network, we employ the mean rollout return $g^{\text{mean}}_{\text{root}, t+2L}$ for the bootstrapped value in target value computation. Thus, the target value for $\hat{v}_{t+l}$, where $0 \leq l \leq L$, is:

\begin{equation}
    \hat{v}^{\text{target}}_{t+l} \defeq r_{t+l+1} + \gamma r_{t+l+2} + ... + \gamma^{2L-l} r_{t+2L} + \gamma^{2L-l+1} g^{\text{mean}}_{\text{root}, t+2L}.
\end{equation}

The non-stationary network loss is then calculated as the sum of the value's L2 loss and the policy's cross-entropy loss:

\begin{equation}
\mathcal{L}^{\text{non-stationary}}  \defeq \sum_{l=0}^L \left( c^v (\hat{v}_{t+l} - \hat{v}^{\text{target}}_{t+l})^2 + c^{\pi} \pi_{t+l}^T \log \hat{\pi}_{t+l}  \right),
\end{equation}
where $c \geq 0$ are scalar hyperparameters that modulate the loss strength. Since the optimizer of the non-stationary network doe not include the parameters of the stationary network, the gradient cannot propagate through $\hat{s}_{t+l}$ to the stationary network, effectively meaning that we apply a stop gradient to all $\hat{s}_{t+l}$.

\section{Actor and Critic Training Procedure} \label{sec:C}
We trained the RL agents using IMPALA, an actor-critic algorithm that supports distributed training. This algorithm can be directly applied to the Thinker-augmented MDP with three minor modifications.

Firstly, we prevent gradient updates for the policies associated with unused actions. Specifically, in a planning stage, the policies corresponding to real actions at steps $1$ to $K-1$, denoted as $a^r_{1:K-1}$, as well as the imaginary action $a^i_K$ and reset action $a^\text{reset}_K$ at step $K$, do not receive any gradients. In principle, it is not necessary to mask the gradient update, as the expected gradient update is zero, but doing so helps reduce the variance of the update.

Secondly, due to the planning reward, the critic outputs a vector of size $2$ at each step. The first item corresponds to the value with respect to the raw reward, and the second item corresponds to the value with respect to the planning reward. For the planning reward, we treat a planning stage as one episode. This implies that when computing the target value for planning rewards, we set all rewards and bootstrap values in the next and subsequent planning stages as $0$. 

Thirdly, the real policy only receives the raw reward, whereas the imaginary and reset policies receive the sum of the raw and imaginary rewards. The rationale for this is explained in Section 3.3.

In summary, denoting the advantage function at step $k$ in a planning stage with respect to the raw reward and the planning reward as $\tilde{\delta}_k$ and $\tilde{\delta}_k^{p}$ respectively, the loss for the actor in an actor-critic network is computed as follows:

\begin{align}
    \mathcal{L}^{\text{actor}}  \defeq  - \Big( \sum_{k=1}^{K-1} & (\tilde{\delta}_k + \tilde{\delta}^{p}_k) (\log P(A^{i}_k=a^{i}_k | \tilde{s}_k) + \log P(A^{\text{reset}}_k=a^{\text{reset}}_k | \tilde{s}_k) ) \nonumber \\ & + \tilde{\delta}_K \log P(A^{r}_K=a^r_K | \tilde{s}_K) \Big).
\end{align}

The entropy loss and the importance weight are omitted here for clarity. Readers are referred to the IMPALA paper \cite{espeholt2018impala} for detailed explanations on these aspects.

The second and third modifications can be conceptualized as the agent playing two MDPs simultaneously. The first one is tied to the raw reward and the raw termination indicator and is played by all three sets of actions. The second one is tied to the planning reward and the planning stage termination indicator, and is only played by the imaginary and reset actions. From this perspective, generalizing to other RL algorithms, such as Q-learning, is straightforward.

If one chooses not to use planning rewards and does not apply variance reduction for unused actions, all three of these modifications become unnecessary.

\section{Network Architecture} \label{sec:D}

In this section, we describe the neural networks that we use for the model and the actor-critic network.

\subsection{Model} 

For Sokoban, the shape of the raw state, $s_t$, is (3, 84, 84), while for Atari 2600, the raw state's shape is (12, 84, 84) due to the stacking of four frames together. Before inputting the raw state $s_t$ to the encoder, we concatenate it with the one-hot encoding of the action $a_{t-1}$, tiled to (84, 84), channel-wise. The encoder architecture is the same for both the stationary and non-stationary networks, structured as follows:

\begin{itemize}
    \item Convolution with 64 output channels and stride 2, followed by a ReLu activation.
    \item $D$ residual blocks, each with 64 output channels.
    \item Convolution with 128 output channels and stride 2, followed by a ReLu activation.
    \item $D$ residual blocks, each with 128 output channels.
    \item Average pooling operation with stride 2.
    \item $D$ residual blocks, each with 128 output channels.
    \item Average pooling operation with stride 2.
\end{itemize}
All convolutions use a kernel size of 3. The resulting output shape for both Sokoban and Atari 2600 is (128, 6, 6). We use $D=1$ for Sokoban and $D=2$ for Atari 2600. 

We denote $g^{\text{stat.}}(s_t)$ and $g(s_t)$ as the encoder's output of $s_t$ for the stationary network and the non-stationary network respectively, and the dependency on $a_{t-1}$ is omitted for clarity. 

For the stationary network, we define the hidden state $h^\text{stat.}_t \defeq g^{\text{stat.}}(s_t)$. To obtain $h^\text{stat.}_{t+l+1}$ from $(h^\text{stat.}_{t+l}, a_{t+l})$ with $l \geq 0$, we first concatenate $h^\text{stat.}_{t+l}$ with the one-hot encoding of the action $a_{t+l}$, tiled to (6, 6), channel-wise. This concatenated output is passed to the unrolling function, which consists of $4D+1$ residual blocks, each having an output channel of 128. The output of the unroll function is $h^\text{stat.}_{t+l+1}$.

For the non-stationary network, in addition to the previous hidden state, the unrolling function should also take the encoded predicted frame $g(\hat{s}_{t+l})$ as input, so the unrolling procedure is slightly modified. First, we define $h^\text{non-stat.}_{t-1}$ as a zero vector of shape (128, 6, 6). To obtain $h^\text{non-stat.}_{t+l}$ from $(h^\text{non-stat.}_{t+l-1}, a_{t+l-1}, g(\hat{s}_{t+l}))$ with $l \geq 0$ (replace $\hat{s}_{t+l}$ with $s_{t+l}$ when $l=0$ here), we first concatenate all three together channel-wise, with $a_{t+l-1}$ tiled to (6, 6). This concatenated output is passed to the unrolling function, which consists of $4D+1$ residual blocks, each having an output channel of 128. The output of the unroll function is $h^\text{non-stat.}_{t+l}$. 

To compute the output statistics $o_{t+l}$, we apply a prediction function to $h_{t+l}$ for both stationary and non-stationary networks. The architecture of this prediction function is the same for both networks and is structured as follows:

\begin{itemize}
    \item Convolution with 64 output channels and stride 1, followed by a ReLu activation.
    \item Convolution with 32 output channels and stride 1, followed by a ReLu activation.
    \item Flatten the 3D vector to a 1D vector.
    \item Separate linear layers for each output statistic.
\end{itemize}

In the case of probability outputs, namely the predicted termination probability $\hat{p}^d_{l}$ and the predicted policy $\hat{\pi}_{l}$, we pass the output of the linear layer through a softmax layer to ensure a valid probability distribution. In addition, the weights and biases of the final linear layers are initialized to zero to stabilize initial training.

For the stationary network, we compute the predicted frame, $\hat{s}_{t+l}$ for $l \geq 1$, by applying a decoder on $h^\text{stat.}_{t+l}$ for $l \geq 1$. The decoder's structure is as follows:

\begin{itemize}
    \item $D$ residual blocks, each with 128 output channels.
    \item ReLu activation followed by a transpose convolution with 128 output channels and stride 2.
    \item $D$ residual blocks, each with 128 output channels.
    \item ReLu activation followed by a transpose convolution with 128 output channels and stride 2.
    \item ReLu activation followed by a transpose convolution with 64 output channels and stride 2.
    \item $D$ residual blocks, each with 64 output channels.
    \item ReLu activation followed by a transpose convolution with 3 output channels and stride 2.
\end{itemize}
All convolutions use a kernel size of 4. The decoder outputs a tensor of shape $(3, 84, 84)$. In Atari, we concatenate this output with the three most recent true or predicted frames in $s_{t+l-1}$ for $l=1$ or $\hat{s}_{t+l-1}$ for $l > 1$, resulting in a final decoded predicted state with shape $(12, 84, 84)$. This approach is adopted to prevent the model from redundantly replicating previous inputs, as the prior three frames are already present in the input or have been predicted in preceding steps.

\subsection{Actor-critic} 

As described in Appendix \ref{sec:A}, the augmented state consists of the model's hidden states $[h^{\text{stat}}_k,  h^{\text{non-stat}}_k]$, a 3D vector of shape $(256, 6, 6)$, and the tree statistics, a flat vector of shape $(10  \vert \mathcal{A} \vert + K + 9)$. The model's hidden state is processed by a convolutional neural network (CNN), structured as:

\begin{itemize}
    \item Convolution with 64 output channels and stride 1, followed by a ReLu activation.
    \item $2$ residual blocks, each with 64 output channels.
    \item Convolution with 64 output channels and stride 1, followed by a ReLu activation.
    \item $2$ residual blocks, each with 64 output channels.
    \item Convolution with 32 output channels and stride 1, followed by a ReLu activation.
    \item $2$ residual blocks, each with 32 output channels.
    \item Flatten the 3D vector to a 1D vector.
    \item A linear layer with output size 256, followed by a ReLu activation.
\end{itemize}

The tree statistics are processed by a special recurrent neural network (RNN) architecture, which combines the Long Short-Term Memory (LSTM) \cite{hochreiter1997long} with the attention module of the Transformer \cite{vaswani2017attention}. This RNN architecture is necessitated by the requirements of the Thinker-augmented MDP, which relies on long-term memory capabilities for plan execution. For instance, a 5-step plan in the augmented MDP requires the agent to maintain the plan in memory for $5K=100$ steps, a demand that exceeds the capabilities of a traditional LSTM. However, using Transformers solely could lead to instability in RL \cite{parisotto2020stabilizing} due to their disregard of the Markov property in MDP as they process inputs across all steps in the same manner. Therefore, our approach merges the LSTM with the attention module of the Transformer to allow for extended memory access while preserving the stability of the training process.

The proposed RNN architecture is critical for learning when the auxiliary statistics are not part of the augmented state representation. However, once these statistics are integrated into the state representation, a simple Multi-Layer Perceptron (MLP) can replace the proposed RNN without performance degradation. A more comprehensive ablation analysis is available in Appendix \ref{sec:F}.

In detail, the RNN processes the tree statistics as follows:

\begin{itemize}
    \item A linear layer with output size 128, followed by a ReLu activation.
    \item A LSTM-Attention network (explained in detail below) with output size 128.
    \item A linear layer with output size 128, followed by a ReLu activation.
\end{itemize}

Finally, the input for the final layer is obtained by concatenating the following:

\begin{enumerate}
    \item The output from the CNN processing the model's hidden state.
    \item The output from the RNN processing the tree statistics.
    \item The one-hot encoding of the last actions (real action, imaginary action, and reset action).
    \item The last rewards (both raw reward and planning reward).    
\end{enumerate}

This input is passed to separate linear layers to generate the final output of the actor-critic network, which includes predicted values and policies for the real action, imaginary action, and reset action. To generate valid probabilities, the logits predicted by the linear layers are fed into a softmax layer. Additionally, an L2 regularization cost of 1e-9 is added for the final layer's input and 1e-6 for the real action's logits.

\paragraph{LSTM-Attention Cell} This section details the LSTM-Attention cell, which serves as the building block of the RNN. The notation here is distinct and should not be conflated with the notation in other sections. The LSTM-Attention cell, while akin to a conventional LSTM, incorporates an additional input from the attention module, multiplied by a gate variable. The LSTM-Attention cell is denoted by:
\begin{equation}
 (h_{t}, c_{t}, K_{t}, V_{t}) = \text{LSTM\_Attn}(x_{t}, h_{t-1}, c_{t-1}, K_{t-1}, V_{t-1}),
\end{equation}
where $x_t$ denotes the input to the cell, $h_t$ denotes the hidden state, $c_t$ denotes the input activation, $K_{t}$ and $V_{t}$ denotes the key and value vector for the attention module, respectively. The function $\text{LSTM\_Attn}$ is defined as:
\begin{align}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
i^c_t &= \sigma(W_{ic} x_t + U_{ic} h_{t-1} + b_{ic}), \\
i^a_t &= \sigma(W_{ia} x_t + U_{ia} h_{t-1} + b_{ia}), \label{eq:lstm_ia}\\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o), \\
a_t, K_{t}, V_{t} &= \text{attn}(x_t, , K_{t-1}, V_{t-1}), \label{eq:lstm_a} \\
\tilde{c}_t &=  \text{tanh}(W_c x_t + U_c h_{t-1} + b_c), \\
c_t &= f_t \odot c_{t-1} + i^c_t \odot \tilde{c}_t + i^a_t \odot \text{tanh}(a_t)  \label{eq:lstm_c} \\
h_t &= o_t \odot \text{tanh}(c_t), 
\end{align}
where $\{W_f, W_{ic}, W_{ia}, W_o, W_c, U_f, U_{ic}, U_{ia}, U_o,  U_c, b_f, b_{ic}, b_{ia}, b_o, b_c\}$ are parameters with output shape $d_m=128$ and $\odot$ denotes element-wise product. The only differences compared to a standard LSTM cell are Equations \ref{eq:lstm_ia}, \ref{eq:lstm_a}, and \ref{eq:lstm_c}. Equation \ref{eq:lstm_ia} computes the input gate for the attention module outputs, Equation \ref{eq:lstm_a} represents the attention module which will be described below, and Equation \ref{eq:lstm_c} adds the gated output from the attention module to the input activation.

The attention function $\text{attn}(x_t, K_{t-1}, V_{t-1})$ is based on the multi-head attention module used in Transformer, with the following modifications: (i) Attention is restricted to the last $d_t=40$ steps, equivalent to two planning stages. This is because maintaining a too long memory might complicate learning due to the Markov assumption of MDP. (ii) A learned relative positional encoding, akin to Transformer XL \cite{dai2019transformer}, is used to facilitate attention to steps relative to the current step.

For clarity, we first describe a single-head attention module. We denote $q_t \in \mathbb{R}^{d_e}$, $k_t \in \mathbb{R}^{d_e}$, $v_t \in \mathbb{R}^{d_e}$ as the query, key, and value at step $t$, respectively, where $d_e=16$ is the dimension of the embedding. The input $K_{t-1} \in \mathbb{R}^{d_t, d_e}$ and $V_{t-1} \in \mathbb{R}^{d_t, d_e}$ represent the preceding $d_t$ keys and values, defined as follows:
\begin{align}
  K_{t-1} \defeq [k_{t-d_t}, k_{t-d_t+1} ..., k_{t-1}]^{T},   \\
  V_{t-1} \defeq [v_{t-d_t}, v_{t-d_t+1} ..., v_{t-1}]^{T}.
\end{align}
We calculate the current $q_{t}, k_{t}, v_{t}$ via a linear projection on input $x_t$:
\begin{align}
  q_{t} &= W_q x_t,   \\
  k_{t} &= W_k x_t,    \\
  v_{t} &= W_v x_t,
\end{align}
where $\{W_q, W_k, W_v\}$ are parameters with output shape $d_e$. This enables us to compute $K_{t}$ and $V_{t}$ by stacking the corresponding $k$ and $v$. Subsequently, we compute the attention vector $p_{t} \in \mathbb{R}^{d_t}$ as:
\begin{align}
    p_{t} &= \text{softmax} \left(\frac{(K_{t} + U_r)q_{t}}{\sqrt{d_e}} + b_r \right),
\end{align}
where $U_r \in \mathbb{R}^{d_t, d_e}$ and $b_r \in \mathbb{R}^{d_t}$ are learnable parameters representing the relative position encoding. This differs subtly from the relative position encoding used in Transformer XL, as the fixed attention window in our case allows us to utilize fixed-size parameters for encoding relative positions. We initialize $U_r$ and $b$ to be zero vectors, with the last entry of $b$ set to 5, reflecting the prior that the current step should be attended to more. In addition, the \text{softmax} here should be masked for steps in the previous episode.

The final output of a single-head attention module, denoted as $\tilde{a}_t \in \mathbb{R}^{d_e}$, is computed as follows:
\begin{align}
    \tilde{a}_t &= V_{t}^{T} p_{t}.
\end{align}
For the multi-head attention module, we repeat the above single-head attention module $d_h=8$ times and stack the outputs to yield $\tilde{a}^{\text{stack}}_t = \text{stack}([\tilde{a}_{t, 1}, \tilde{a}_{t, 2}, ..., \tilde{a}_{t, d_h}])$, where $\tilde{a}^{\text{stack}}_t \in \mathbb{R}^{d_m}$ and $\tilde{a}_{t, i}$ denotes the output of the $i^\text{th}$ single-head attention module. The final output of the attention module $a_t$ is given by applying a linear layer and layer normalization \cite{ba2016layer} on this stacked output:
\begin{align}
    {a}_t &= \text{LayerNorm}(W_a \tilde{a}^{\text{stack}}_t + b_a + x_t),
\end{align}
where $x_t$ represents the skip connection, and $\{W_a, b_a\}$ are parameters with output size $d_m$. This concludes the description of the LSTM-Attention cell.

\paragraph{LSTM-Attention Network}
An LSTM-Attention Network is constructed by stacking $d_n=3$ LSTM-Attention Cells together. We denote the network's input as $x_t$, and use the superscript $1 \leq n \leq d_n$ to represent the $n^\text{th}$ cell in the network. We denote the LSTM-Attention network as:
\begin{equation}
 (h_{t}^{1:d_n}, c_{t}^{1:d_n}, K_{t}^{1:d_n}, V_{t}^{1:d_n}) = \text{LSTM\_Attn\_Net}(x_{t-1}, h^{1:d_n}_{t-1}, c^{1:d_n}_{t-1}, K^{1:d_n}_{t-1}, V^{1:d_n}_{t-1}),
\end{equation}
defined by the iterative equation for $1 \leq n \leq d_n$:
\begin{equation}
 (h_{t}^{n}, c_{t}^{n}, K_{t}^{n}, V_{t}^{n}) = \text{LSTM\_Attn}( \text{stack}([x_{t-1}, h_{t}^{n-1}]), h^{n}_{t-1}, c^{n}_{t-1}, K^{n}_{t-1}, V^{n}_{t-1}),
\end{equation}
and we define $h^{0}_{t} \defeq h^{n}_{t-1}$, the top LSTM-Attention cell's output from the previous step. The final output of the network is the top LSTM-Attention cell's output, $h^{n}_t$. The input to each LSTM-Attention cell comprises both the network's input and the output from the previous LSTM-Attention cell. This design choice was inspired by DRC \cite{guez2019investigation}, which suggests that this recurrence method may facilitate better planning.

The rationale for integrating LSTM with the attention module in this manner stems from the following reasoning. The LSTM's hidden states can be interpreted as a form of short-term memory, primarily focusing on the current step. In contrast, the attention module acts as a long-term memory unit, as it maintains an equal computational path from all past steps. By fusing these two forms of memory, the LSTM is granted the capability to control access to the long-term memory, including the ability to disregard it entirely by setting the gate variable to zero. This flexibility enables a focus on the most recent few steps while still preserving access to long-term memory, thus combining the advantages of both types of RNN. Using gradient clipping when applying the LSTM-Attention network is also recommended to stabilize training further.

A significant drawback of the proposed LSTM-Attention network is the sequential computation it requires. Unlike the Transformer, which can compute attention across all steps simultaneously, the proposed network has to compute each step one at a time due to the recurrent computation of LSTM. Consequently, this leads to a much higher computational cost. Nonetheless, this sequential computation limitation may not be as significant in an RL setting. An RL task inherently requires the agent to generate action sequentially, and this sequential computation cannot be avoided even for Transformer. Future studies could explore the potential of the proposed LSTM-Attention network in contexts beyond the Thinker-augmented MDP, such as RL tasks that require long-term memory.


\clearpage
\begin{table}[h]
	\caption{Hyperparameters used in experiments.}
	\label{table:hp}
	\begin{center}	
		\begin{tabular}{lllll}
			\toprule[0.1ex]
			\textbf{Parameter} & \textbf{Value} \\
			\midrule
                \textbf{Thinker-augmented MDP} \\
                Number of planning step $K$ & 20\\	 
                Maximum search depth $L$ & 5 \\
                Augmented discount rate $\tilde{\gamma}$ & $\gamma^{\frac{1}{K}} $ \\
                Planning reward scaling $c^p$ & Anneal linearly from 1 to 0\\
                \\
                \textbf{Model} \\
                Learning rate & Anneal linearly from 1e-4 to 0 \\
                Optimizer & Adam \\                
                Adam beta & (0.9, 0.999) \\        
                Adam epsilon & 1e-8 \\       
                Replay buffer size & 200,000 \\
                Minimum replay buffer size for sampling & 200,000 \\
                Replay ratio & 6 \\
                Batch size & 128 \\
                Model unroll length $L$ & 5 \\
                Reward loss scaling $c^r$ & 1 \\
                Termination indicator loss scaling $c^d$ & 1\\
                Feature loss scaling $c^s$ & 10 \\
                Value loss scaling $c^v$ & 0.25 \\
                Policy loss scaling $c^\pi$ & 0.5 \\
                \\
                \textbf{Actor-critic} \\
                Learning rate & Anneal linearly from 6e-4 to 0 \\
                Optimizer & Adam \\                
                Adam beta & (0.9, 0.999) \\        
                Adam epsilon & 1e-8 \\       
                Batch size & 16 \\
                Actor-critic unroll length & 20$K$ + 1 \\
                Baseline scaling & 0.5 \\
                Clip global gradient norm & 1200 \\
                Entropy regularizer for real actions & 1e-3 \\
                Entropy regularizer for imaginary and reset actions & 5e-5 \\
                \\
                \textbf{Environment specific - Sokoban} \\
                Discount rate $\gamma$ & 0.97 \\
                Input shape & (3, 84, 84) \\ 
                \\
                \textbf{Environment specific - Atari 2600} \\
                Discount rate $\gamma$ & 0.99 \\
                Grayscaling & No \\
                Action repetitions & 4 \\
                Max-pool over last N action repeat frames & 2 \\
                Input shape & (12, 84, 84) \\ 
                Frame stacking & 4 \\
                End of the episode when life lost & Yes \\
                Reward clipping & [-1, 1] \\
                Sticky action & No \\
			\bottomrule[0.25ex]
		\end{tabular}
	\end{center}
\end{table}
\clearpage


\section{Experiment Details} \label{sec:E}
\subsection{Hyperparameters}
The architecture used in the experiment is described in Appendix \ref{sec:D}. The hyperparameters used in all our experiments are shown in Table \ref{table:hp}. We tune our hyperparameters exclusively on Sokoban based on the final solving rate. The specific hyperparameters we tune are: learning rates, model batch size, model loss scaling, maximum search depth or model unroll length, planning reward scaling, actor-critic clip global gradient norm, and actor-critic unroll length. In terms of preprocessing for Atari, we adhere to the same procedure as outlined in the IMPALA paper \cite{espeholt2018impala}, with the exception that we do not apply grayscaling.

For the actor-critic baseline implemented on the raw MDP, we maintain the same hyperparameters, with two exceptions: (i) we adjust the actor-critic unroll length to 20 to ensure the same number of real transitions in an unrolled trajectory; (ii) we reduce the learning rate by half to 3e-4, after observing that the original learning rate led to unstable learning. We found that this set of hyperparameters provides optimal performance for the baseline in Sokoban, based on the final solving rate. Regarding the network architecture, we employ an encoder that mirrors the model's encoder architecture in Thinker, followed by a CNN that mirrors the actor-critic's CNN in Thinker, thus ensuring consistent network capacity.

\subsection{Detailed Results on Atari 2600}

The final scores and the corresponding learning curves for each individual Atari game are presented in Table \ref{table:atari_score} and Figure \ref{fig:each_atari}, respectively. The Rainbow results presented in Table \ref{table:atari_score} are taken from the original paper \cite{hessel2018rainbow}, where the reported score is obtained from the best agent snapshots throughout training. The learning curves of Rainbow in Figure \ref{fig:each_atari} are also taken from the original paper.

To validate the reproducibility of our results, we carried out multiple runs of five selected games with three different seeds. These games include \texttt{battle zone}, \texttt{double dank}, \texttt{name this game}, \texttt{phoenix}, and \texttt{qbert}. The selection of these games was influenced by the recommendations of \cite{aitchison2022atari}, which suggests that these five games are pivotal in predicting the median human-normalized score. The final scores for these five games are displayed in Table \ref{table:atari5_score}, and their individual learning curves can be found in Figure \ref{fig:each_atari5}. The results for these five games, as presented in other figures and tables in this paper, are derived from the average of these three seed values. Given that each run requires approximately a week of computational time on a workstation equipped with two A100 GPUs, and considering that Sokoban is our primary testing ground, we find it adequate to conduct three seeds for each of the five selected games.

\subsection{Discussion on Atari's Results}

As stated in the main text, Thinker demonstrates superior performance in shooting games like \texttt{chopper command}, \texttt{seaquest}, \texttt{phoenix}, \texttt{beam rider}, and \texttt{space invader}. A more general observation is that Thinker excels in environments with fast-moving objects, such as \texttt{breakout}. As depicted in \url{https://youtu.be/0AfZh5SR7Fk}, Thinker's model is capable of predicting the trajectories of these swift objects with near-perfect precision. We hypothesize that such predictions, essential for planning future actions, pose a challenge for an agent in the raw MDP to learn. For example, in \texttt{Breakout}, the video illustrates the agent exploring different methods of deflecting the ball at various angles, sometimes missing the ball in its imagination but performing adeptly in the actual game. Upon identifying a sequence of actions that might result in missing the ball, the agent can opt to avoid it. This adaptive decision-making and the significant performance gap between the model's and the agent's policies in these games underscore the benefits of planning in games with fast-moving objects.

It is important to highlight that applying a direct L2 loss on future states instead of a feature loss can undermine these advantages. For example, we observed that in \texttt{Breakout}, a model trained with direct L2 loss is able to predict frames of high quality, except that the ball, arguably the most important object in the game, is missing in the predicted frame. Given that these fast-moving objects typically inhabit only a few pixels and are challenging to predict, the model is prone to neglect these elements when trained using a direct L2 loss. In contrast, by using feature loss, these important objects, such as the ball in \texttt{Breakout}, are usually learned early in training.


\clearpage
\begin{table}[h]
     \small
	\caption{Atari scores after 200M frames of training. Up to 30 no-ops at the beginning of each episode.}
	\label{table:atari_score}
	\begin{center}	
		\begin{tabular}{lrrrr}
			\toprule[0.1ex]
		   &\textbf{Rainbow} & \textbf{Raw MDP} & \textbf{\thead{Thinker \\ Model's policy}}  & \textbf{\thead{Thinker \\ Agent's policy}}   \\
			\midrule
                    Alien & 9491.70 & \textbf{13868.80} & 229.20 & 230.10\\
                    Amidar & \textbf{5131.20} & 1121.56 & 1338.75 & 3591.84\\
                    Assault & 14198.50 & 6998.29 & 18856.51 & \textbf{24262.47}\\
                    Asterix & \textbf{428200.30} & 279440.00 & 238992.50 & 85254.00\\
                    Asteroids & 2712.80 & 12570.40 & 40030.60 & \textbf{212805.50}\\
                    Atlantis & 826659.50 & \textbf{896017.00} & 808384.00 & 809160.00\\
                    Bank Heist & 1358.00 & 378.50 & \textbf{1531.20} & 1528.90\\
                    Battle Zone & \textbf{62010.00} & 2320.00 & 47553.33 & 48103.33\\
                    Beam Rider & 16850.20 & 24202.46 & 15213.50 & \textbf{59533.14}\\
                    Berzerk & 2545.60 & \textbf{2664.00} & 1024.60 & 1038.30\\
                    Bowling & 30.00 & 23.78 & \textbf{31.26} & 31.14\\
                    Boxing & 99.60 & 8.17 & 99.51 & \textbf{99.86}\\
                    Breakout & 417.50 & 593.83 & 660.84 & \textbf{840.14}\\
                    Centipede & 8167.30 & \textbf{8201.71} & 5169.38 & 6507.05\\
                    Chopper Command & 16654.00 & 1046.00 & 43691.00 & \textbf{843973.00}\\
                    Crazy Climber & \textbf{168788.50} & 97824.00 & 98755.00 & 116193.00\\
                    Defender & 55105.00 & \textbf{385397.50} & 71378.50 & 123064.50\\
                    Demon Attack & 111185.20 & 502.30 & 133410.20 & \textbf{135040.10}\\
                    Double Dunk & -0.30 & -1.72 & 15.27 & \textbf{16.09}\\
                    Enduro & \textbf{2125.90} & 26.03 & 0.00 & 0.00\\
                    Fishing Derby & 31.30 & 32.46 & 38.58 & \textbf{43.68}\\
                    Freeway & \textbf{34.00} & 0.00 & 28.04 & 28.65\\
                    Frostbite & \textbf{9590.50} & 269.70 & 303.30 & 303.10\\
                    Gopher & \textbf{70354.60} & 5049.40 & 15871.40 & 55911.00\\
                    Gravitar & 1419.30 & 267.00 & 3766.00 & \textbf{4226.00}\\
                    Hero & \textbf{55887.40} & 13117.30 & 2984.30 & 2987.80\\
                    Ice Hockey & 1.10 & -5.27 & 20.62 & \textbf{22.99}\\
                    Jamesbond& N.A. & 1523.00 & 4171.50 & \textbf{7612.50}\\
                    Kangaroo & \textbf{14637.50} & 390.00 & 12424.00 & 13218.00\\
                    Krull & \textbf{8741.50} & 5767.00 & 4663.10 & 4557.60\\
                    Kung Fu Master & \textbf{52181.00} & 35209.00 & 32639.00 & 36391.00\\
                    Montezuma Revenge & \textbf{384.00} & 0.00 & 12.00 & 14.00\\
                    Ms Pacman & 5380.40 & \textbf{7117.66} & 2685.40 & 4206.10\\
                    Name This Game & 13136.00 & 13391.00 & 25131.43 & \textbf{33085.20}\\
                    Phoenix & 108528.60 & 18201.70 & 156647.23 & \textbf{552719.37}\\
                    Pitfall & \textbf{0.00} & -38.17 & -2.95 & -2.51\\
                    Pong & 20.90 & \textbf{20.97} & 20.96 & 20.84\\
                    Private Eye & \textbf{4234.00} & 48.41 & 93.61 & 94.00\\
                    Qbert & 33817.50 & 18491.75 & 20184.58 & \textbf{34849.33}\\
                    Riverraid& N.A. & \textbf{23983.60} & 11664.30 & 11696.00\\
                    Road Runner & \textbf{62041.00} & 2988.00 & 37967.00 & 41950.00\\
                    Robotank & \textbf{61.40} & 31.73 & 40.89 & 40.53\\
                    Seaquest & 15898.90 & 1501.40 & 32992.90 & \textbf{432855.00}\\
                    Skiing & \textbf{-12957.80} & -16076.10 & -21515.92 & -24218.14\\
                    Solaris & \textbf{3560.30} & 2136.60 & 2427.00 & 2323.40\\
                    Space Invaders & 18789.00 & 8423.85 & 12880.40 & \textbf{42486.95}\\
                    Star Gunner & 127029.00 & 183274.00 & 290441.00 & \textbf{692283.00}\\
                    Surround & \textbf{9.70} & -9.86 & 1.89 & 4.28\\
                    Tennis & 0.00 & \textbf{20.51} & -1.25 & -1.19\\
                    Time Pilot & 12926.00 & 42851.00 & 37310.00 & \textbf{50767.00}\\
                    Tutankham & 241.00 & 51.60 & 262.00 & \textbf{268.14}\\
                    Up N Down&  N.A.& 319317.80 & 337508.10 & \textbf{404120.10}\\
                    Venture & \textbf{5.50} & 0.00 & 0.00 & 0.00\\
                    Video Pinball & \textbf{533936.50} & 281210.04 & 444212.40 & 512567.38\\
                    Wizard Of Wor & 17862.50 & 6010.00 & 19304.00 & \textbf{31691.00}\\
                    Yars Revenge & 102557.00 & 72845.97 & 70000.15 & \textbf{124342.12}\\
                    Zaxxon & \textbf{22209.50} & 15.00 & 17716.00 & 19979.00\\
                               
			\bottomrule[0.25ex]
		\end{tabular}
	\end{center}
\end{table}

\clearpage
% Figure environment removed
\clearpage


\begin{table}[h]
	\caption{Atari scores after 200M frames of training for the five selected games with different seeds. Up to 30 no-ops at the beginning of each episode.}
	\label{table:atari5_score}
	\begin{center}	
		\begin{tabular}{lrrrr}
			\toprule[0.1ex]
		   &\textbf{Seed 1} & \textbf{Seed 2} & \textbf{Seed 3}  & \textbf{Mean}   \\
			\midrule
                  Battle Zone & 78740.00 & 31110.00 & 34460.00 & 48103.33 \\
                  Double Dunk & 12.20 & 14.54 & 21.54 & 16.09 \\
                  Name This Game & 34079.60 & 33400.20 & 31775.80 & 33085.20 \\
                  Phoenix & 543987.70 & 535685.10 & 578485.30 & 552719.37 \\
                  Qbert & 30734.50 & 36810.25 & 37003.25 & 34849.33 \\                               
			\bottomrule[0.25ex]
		\end{tabular}
	\end{center}
\end{table}


% Figure environment removed

We also observe that the agent's performance falls short in more demanding exploration games, such as \texttt{montezuma revenge} and \texttt{venture}. This is because the Thinker-augmented MDP lacks any mechanism to promote exploration in the real game, which results in challenges when dealing with these complex exploration games, much like an actor-critic applied directly to a raw MDP. Future research could explore methods to utilize Thinker to foster exploration, such as using model prediction loss as a curiosity reward. As exploration is a well-studied area in RL \cite{LADOSZ20221}, many existing exploration methods could also be directly applied to the Thinker-augmented MDP.

Thinker also performs poorly in games where each action carries little significant impact, such as in \texttt{qbert}, as outlined in the main text. Games in which the agent moves slowly, such as \texttt{bank heist} and \texttt{ms pacman}, also share this characteristic. This is in stark contrast to games like Sokoban, where a single action can move the agent to a different grid. These games highlight a fundamental limitation of the Thinker-augmented MDP that cannot be avoided by switching RL algorithms - planning is conducted in the primitive action space rather than the abstract action space, thereby necessitating the significance of each primitive action. Future research could investigate modifications to Thinker that would allow model interaction through abstract actions, thereby allowing high-level and long-term planning.

Interestingly, the actor-critic applied to the raw MDP occasionally outperforms the one applied to the Thinker-augmented MDP. This could be attributed to the fact that we provide the agent with the model's hidden state instead of the true or predicted frames. It is conceivable that the actual frame is simpler to learn from, given that the model learns the encoding of frames by minimizing supervised learning loss rather than maximizing the agent's returns. Future studies could consider allowing the agent to directly observe the true and predicted frame, thus ensuring that the agent possesses a strictly greater capability than an agent on the raw MDP. Another factor could be the impact of unoptimized hyperparameters. As we increased the network's depth while maintaining the same learning rate during the transition from Sokoban to the Atari environment, the training process became more unstable, as demonstrated in some learning curves. It is plausible that the poor performance in certain games stems from these unoptimized hyperparameters.

Finally, as evidenced by the individual learning curves illustrated in Figure \ref{fig:each_atari}, and the median human-normalized score in Figure 7, we observe that the slope of the learning curve is generally steeper at the end of training compared to other baselines. We posit that mastering the use of a model represents an additional skill to be acquired, as opposed to an agent acting on a raw MDP. This necessitates extended training to learn this skill, however, once this skill is acquired, the final performance generally surpasses that of an agent on the raw MDP. As such, future work could experiment with a larger training budget than 200M frames.

\clearpage
\section{Ablation Analysis} \label{sec:F}
In this section, we investigate the significance of various elements of the Thinker-augmented MDP. The components under consideration include (i) planning rewards, (ii) auxiliary statistics and memory, (iii) maximum search depth, (iv) the prediction of values and policies, and (v) the feature loss of the model. All ablation experiments are performed in Sokoban under identical settings as those of the primary experiments, with results shown as the solving rate over the last 200 episodes.

\paragraph{Planning Rewards} Planning rewards function as a heuristic, incentivizing the agents to search for a rollout with the maximum rollout return. The result of excluding planning rewards is shown in Figure \ref{fig:ab_1}. Surprisingly, the removal of the planning rewards results in only a marginal decrease in the initial learning speed, without having any significant impact on the final performance. This suggests that the planning reward may not be a critical component for learning to plan. Additional behavior analysis of a trained agent in Appendix \ref{sec:G} demonstrates that an agent trained without planning rewards seems to learn a similar planning behavior to that of an agent trained with planning rewards.

In our initial experiments, where the augmented representation and the model were in a more rudimentary version, planning rewards were found to considerably impact the initial learning speed. This leads us to conjecture that devising the MDP so that the agent can utilize the model more effortlessly may obviate the requirement for heuristic planning rewards.

\paragraph{Auxiliary Statistics and Memory} We examine the scenario where all auxiliary statistics are removed from the augmented state representation. Specifically, we remove the following auxiliary statistics:
$${\{g_{\text{root\_child}}^{\text{max}},g_{\text{root\_child}}^{\text{mean}}, n_{\text{root\_child}}, g_{\text{cur\_child}}^{\text{max}}, g_{\text{cur\_child}}^{\text{mean}},  n_{\text{cur\_child}}, g_{\text{root}, \text{cur}}, g_{\text{root}, \text{cur}} - \gamma^{d+1} \hat{v}_{\text{cur}}, g_{\text{root}}^{\text{max}}\}}$$
from the augmented state representation as described in Appendix \ref{sec:A}. These statistics are deemed auxiliary since, theoretically, the agent can compute these from the remaining tree statistics in the previous and current steps. The result of removing auxiliary statistics is shown in Figure \ref{fig:ab_2}.

The removal of auxiliary statistics causes the agent to learn at a slower pace while slightly reducing the final performance. We theorize that the agent requires more training to learn to summarize the tree statistics across multiple steps, leading to slower learning. With prolonged training, it could be feasible for the agent to learn this summarization and achieve the same final performance, thereby rendering the provision of these handcrafted `hints' unnecessary.

A specialized RNN architecture is employed to endow the agents with long-term memories while preserving training stability, as elaborated in Appendix \ref{sec:D}. We then consider the impact of replacing this RNN with a three-layer MLP with 200 hidden units, effectively removing the agents' memory. In the MLP, we add a skip connection from the input to each hidden layer, to ensure easier flow of information. The result of this adjustment is presented in the same figure. Interestingly, the removal of memory does not hamper performance in any notable way.

We hypothesize that as we have already summarized the tree statistics for the agent, the agent is not required to perform this summarization itself and hence, does not require memory. To verify this hypothesis, we consider the scenario of removing both memory and auxiliary statistics, and the outcome is demonstrated in the same figure. The effect of eliminating memory proves to be highly significant when auxiliary statistics are not provided. Consequently, we conclude that the presence of either auxiliary statistics \textit{or} memory is necessary for learning to plan.

\paragraph{Maximum Search Depth} We investigate the scenario in which we set the maximum search depth and the model unroll length $L$ to 10, thereby allowing the agents to search deeper. We did not utilize planning rewards in this case to avoid the influence of heuristic rewards. The result of this modification is shown in Figure \ref{fig:ab_3}. The initial learning speed is significantly slower, presumably due to the added time required for the model to learn to unroll over an increased number of steps. However, the final performance remains largely unaffected. This suggests that a deeper search does not necessarily lead to improved final performance. This finding aligns with previous studies indicating that a shallow search suffices to obtain satisfactory outcomes in MuZero \cite{hamrick2021on}.

\paragraph{Predicted Values and Policies} We evaluate the effects of omitting the non-stationary network within the dual network, thereby eliminating the prediction of values and policies. This removal also leads to the exclusion of planning rewards and auxiliary statistics, both of which are computed based on predicted values. The result is displayed in Figure \ref{fig:ab_4}. 

The result shows that the agent fails to learn in such a scenario, indicating that predicted values and policies are indispensable components for learning. We offer two potential explanations for this observation. Firstly, predicted values provide an agent with the ability to evaluate a leaf node more easily. This node evaluation allows an agent to evaluate a rollout without simulating the rollout till the end of the episode, thus allowing more shallow search. Even though the predicted value could theoretically be computed based on the hidden state of the node, which is provided to the agent, it is likely that this prediction is too difficult for the agent to learn by itself. Secondly, predicted policies and values allow the agent to continuously improve upon the existing policy. The learned policy of the model acts as a `distilled' policy without planning, and the search corresponds to a learned policy improvement operator applied to this distilled policy. With predicted policies and values as inputs, the agent can learn to apply various search algorithms, such as MCTS \cite{coulom2007efficient} and $n$-step exhaustive search, as this policy improvement operator.

\paragraph{Model's Feature Loss} Lastly, we evaluate the impact of excluding the model's feature loss when training the model, thereby yielding a model akin to MuZero albeit with a distinct architecture. The results are demonstrated in Figure \ref{fig:ab_4}. We observe that the agent fails to learn in the absence of feature loss. This is likely attributable to the slow rate of model learning when feature loss is not included. When relying solely on output statistics as supervised learning signals for the model, a large amount of training samples is necessary. This is evidenced by the 20 billion frames utilized by MuZero. Given that we train our models on only 50 million frames, the model without feature loss learns slowly, resulting in the agent's inability to use this slow-learning model effectively.

% Figure environment removed

% Figure environment removed

\clearpage
\section{Agent Behavior Analysis} \label{sec:G}

In this section, we delve into the behavior learned by the trained agents in the Thinker-augmented MDP of Sokoban. Our inquiry aims to understand the nature of the search policies acquired by the agents. Does the agent adhere to hand-crafted planning algorithms, such as the $n$-step exhaustive search or MCTS, or does it learn novel planning algorithms? Given that the agents learn three types of actions - reset, imaginary, and real actions - we will analyze these actions individually.\footnote{The trained agents' networks are available at  \url{https://github.com/anonymous-scrl/thinker}.}

\subsection{Reset Action}
In an $n$-step exhaustive search, which involves iterating through every possible $n$-step action sequence and choosing the one with the highest rollout return, a reset is triggered when the search depth hits $n$. Similarly, MCTS initiates a reset at a leaf node. However, the trained agent seems to learn different behavior of resetting.

As visualized in \url{https://youtu.be/0AfZh5SR7Fk}, the agents often chose to reset when envisioning a poor rollout, for instance, when imagining a box being moved to an irreversible state. To illustrate this further, we can examine the distribution of Temporal Difference (TD) errors of the rollout from the root node to a node $i_n$, defined as:
\begin{equation}
    \delta_{i_n} \defeq \hat{r}_{i_1} + \gamma^2 \hat{r}_{i_2} + ... \gamma^{n} \hat{r}_{i_{n}} + \gamma^{n+1} \hat{v}_{i_n} - \hat{v}_{\text{root}},
\end{equation}
where $\{\text{root}, i_1, i_2, ..., i_n\}$ denotes the path from the root node to the node $i_n$. This TD error can be interpreted as the quality of the rollout relative to the current policy. We compare the distribution of TD errors of the current rollout $\delta_{\text{cur}}$ where the agent decided to reset with those where it did not, and the result is visualized in the left and middle panels of Figure \ref{fig:reset}.

We observe that the TD error distribution when agents opt to reset tends to lean more towards the left as compared to when they choose not to reset. This indicates a higher propensity for the agent to reset when the quality of the rollout is inferior. The thin tail on the left side of the distribution represents instances where the agent envisions a poor plan, such as reaching an irreversible state. Under such circumstances, the agent exhibits a high probability of resetting. This suggests that the agents learned to factor in the current rollout's quality in their decision to reset. This approach diverges from planning strategies like MCTS, which mandates a reset upon reaching a leaf node.

An additional dimension worth investigating is the \emph{expansion length}, defined as the number of steps the agent executes beyond the last expanded node. In the case of MCTS, this length is consistently one as the algorithm requires a reset at every leaf node. However, as shown in the right panel of Figure \ref{fig:reset}, the trained agents generally exhibit a longer expansion length. In fact, when the maximum search depth $L$ is increased to 10, the agent learns to search much deeper, as depicted in the same figure.

Coupled with the aforementioned insights into the TD error distribution, we see that the agents prefer to persist with their existing plan unless they encounter an unfavorable leaf node or reach the set maximum search depth. These observations suggest that the agents tend to formulate plans that are both deep and narrow. 

\subsection{Imaginary Action}
In an $n$-step exhaustive search, imaginary action is chosen sequentially, while in MCTS, imaginary action is chosen according to the upper confidence bound (UCB). As for the trained agents, it is difficult to deduce the formula used to select the imaginary action as it is the output of a deep neural network. However, the maximum TD errors in a planning stage, defined as $\delta^{\text{max}} \defeq \max_k \delta_k$ where $1 \leq k < K$ denotes the planning step, could be used as a proxy to evaluate the effectiveness of the search\footnote{This maximum TD error is proportional to the undiscounted sum of planning rewards in a planning stage.}. The distribution of maximum TD errors of search using the trained agent, UCB, and random search is shown in Figure \ref{fig:im1}. Note that we use the learned model of the trained agent when applying UCB.

We observe that the distribution of maximum TD errors has a slightly heavier tail on the right side when searching with a trained agent. This can be better visualized in log scale as shown in Figure \ref{fig:im2}. This tail mostly corresponds to the case of solving or being closer to solving a level in the imagination where the current policy has a high chance of not solving it. This scenario is relatively rare given that the current policy is capable of solving approximately 95\% of levels, which explains the thin tails in all cases. Thus, the heavier tail suggests that the agents learn to search more effectively than MCTS within the same search budgets. This finding is consistent with previous studies on replacing MCTS with neural networks in supervised learning settings \cite{guez2018learning}. We conjecture that the difference in this distribution would be much larger if the model were also trained with MCTS, given the large number of simulations required for MCTS to achieve good performance in Sokoban \cite{guez2018learning}. It is also noteworthy that even without planning rewards, the agent is still able to collect large maximum TD errors, which implies that the agent can learn to search effectively without the guidance of heuristics.

\subsection{Real Action}
In an $n$-step exhaustive search, the real action is taken as the child node with the highest rollout return, while in MCTS, the real action is usually selected based on the visit count of each child node. In contrast, our trained agents appear to learn to use a combination of both.

To further investigate how real action is selected given the same search result, we used the imaginary action and reset action outputs from the trained agent and considered two different ways of selecting the real action: (i) the real action output by the trained agent, (ii) the real action output by MCTS, which is sampled based on visit counts with different temperatures $T \in \{0.25, 1\}$ as in MuZero \cite{schrittwieser2020mastering}. For each method, we calculated the portion of the real action that corresponded to the highest visit count, the second highest visit count, and so on. The result is shown in the left panel of Figure \ref{fig:real}. Additionally, we computed the same for the mean rollout return and the maximum rollout return of child nodes, as shown in the middle and right panels of the figure.

The figure shows that most of the real action selected by the agents corresponds to the child node with the highest visit counts or rollout returns. The figures also suggest that the maximum rollout return is the most indicative of the agent's real action among the three statistics. In contrast, MCTS tends to select the real action with the highest visit count. Notably, the action with the highest visit count does not necessarily correspond to the action with the highest rollout return, as evidenced by MCTS with $T=0.25$, which chooses the action with the highest visit count with over 80\% probability, yet only around half of these actions correspond to the one with the highest rollout return. The simultaneous consideration of both rollout returns and visit counts by the trained agent seems more intuitively reasonable - it is unwise to follow a poor plan even if it is simulated numerous times. 

% Figure environment removed

\subsection{Comparison with MCTS}

Lastly, to compare the efficiency of the trained agents across all three action sets with MCTS, we assess the solving rates of both the trained agents and MCTS. In MCTS, we utilize the model learned by the agent at the end of training, and we test a range of planning steps\footnote{Note that the number of planning steps differs from the number of simulations, as the former includes traversing expanded nodes while the latter does not.} spanning over \{5,10,20,40,80,160,320,640,1280,2560\}. The results are depicted in Figure \ref{fig:mcts}. We observe that MCTS needs 1280 planning steps to match the performance of a trained agent, which achieves the same results with only 20 planning steps. This notable difference highlights the potential benefits of trained agents compared to hand-crafted algorithms — they exhibit the capacity to conduct searches with increased efficiency, particularly within limited computational resources.

In conclusion, the analysis in this section suggests that the trained agents appear to learn a planning algorithm that differs from common hand-crafted planning algorithms. These agents learn to plan deeply and reset upon encountering a poor node, while being able to search for a good plan within limited number of planning steps. It is likely that the agents also take advantage of the model's hidden state, which contains rich information about the root and predicted state, when deciding upon the three types of actions. However, a comprehensive investigation of this particular dynamic lies beyond the scope of our current analysis.

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed
\clearpage


\section{Discussion on Planning Rewards} \label{sec:H}

In the main text, we assign the following planning rewards to both imaginary and reset actions:

\begin{equation}
\tilde{r}^{p}_k \defeq 
\begin{cases}
c^{p} \left( g_{\text{root},k}^{\text{max}} - g_{\text{root},k-1}^{\text{max}} \right), & \text{if } k \leq K \\
0, & \text{if } k = K + 1. \tag{2} 
\end{cases}
\end{equation}

We consider a planning stage as a standalone episode in terms of these planning rewards. In other words, the imaginary and reset actions receive raw rewards, tied to the raw episode termination, and concurrently, they receive planning rewards, tied to the planning stage termination. Appendix \ref{sec:C} provides a detailed implementation of this process. In the following, we will discuss the rationale of planning rewards, under certain simplifying assumptions\footnote{For simplification, we make the following three assumptions in this section: (i) $\tilde{\gamma}=\gamma=1$, (ii) we do not retain the tree statistics across the planning stage, hence, $g^{\text{max}}_{\text{root},0} = g_{\text{root}, \text{root}} = \hat{r}_{\text{root}} + \gamma \hat{v}_{\text{root}}$, and (iii) $\hat{r}_\text{root}=0$, since the reward at the root node has already been collected during the last real action.}.

We notice that the total undiscounted sum of planning rewards in a planning stage is:
\begin{align}
\sum_{k=2}^{K+1} \tilde{r}^{p}_k &= c^p(g_{\text{root},K}^{\text{max}} - g_{\text{root},0}^{\text{max}}) \\
&\propto \hat{r}_{i^*_1} + \gamma^2 \hat{r}_{i^*_2} + ... \gamma^{n} \hat{r}_{i^*_{n}} + \gamma^{n+1} \hat{v}_{i^*_{n}} -  \hat{v}_{\text{root}}, 
\end{align}
where $\{\text{root}, i^*_1, i^*_2, ..., i^*_{n}\}$ refers to the rollout with the highest return, $g_{\text{root}, i}$, in the entire planning stage. As $\hat{v}_{\text{root}}$ is not influenced by imaginary and reset actions, the return of a planning stage is equivalent to $ \hat{r}_{i^*_1} + \gamma^2 \hat{r}_{i^*_2} + ... \gamma^{n} \hat{r}_{i^*_{n}} + \gamma^{n+1} \hat{v}_{i^*_{n}} $, the maximum rollout return. Consequently, the planning reward is a heuristic directing the agent to maximize the maximum rollout return.

Many search algorithms use the maximum rollout return as a heuristic to guide searching \cite{anthony2019policy,srinivas2018universal,henaff2017model,fickinger2021scalable}. When (i) the model perfectly predicts future rewards and values, and (ii) the real action is selected to match the rollout with the highest return, the raw return and maximum rollout return are equivalent in expectation. Under these conditions, the rollout with the highest return is repeated in the raw MDP, providing the same return as the modeled MDP in expectation. However, these conditions may not hold in practice. For instance, during the early stages of training, the model's prediction of future rewards and values might be inaccurate. Furthermore, in the agent behavior analysis (refer to Appendix \ref{sec:G}), we observe that trained agents only select the rollout with the highest return approximately 90\% of the time.

It is worth noting that when an actor-critic network is applied to the Thinker-augmented MDP and the critic learns to use the maximum rollout return of the root node, $g^{\text{max}}_{\text{root},k}$, as the predicted raw values $\hat{v}^\text{critic, raw}(s, z_k)$, then the raw TD error (i.e., the TD error with respect to the raw reward) in an imaginary transition $k < K$, denoted by $\tilde{\delta}_k$, is:
\begin{align}
\tilde{\delta}_k &\defeq \tilde{r}_{k+1} + \tilde{\gamma} \hat{v}^\text{critic, raw}(s, z_{k+1})  - \hat{v}^\text{critic, raw}(s, z_{k}) \\
&= g_{\text{root},{k+1}}^{\text{max}} - g_{\text{root},k}^{\text{max}}, \\
&\propto \tilde{r}^{p}_{k+1}.
\end{align}
Here, the raw reward signal $\tilde{r}_{k+1}$ in the imaginary transition is zero. We observe that under such prediction, the TD error is proportional to the planning reward, thereby making the planning reward redundant. Given that $g^{\text{max}}_{\text{root},k}$ is part of the augmented state and provides a reasonable estimation of the current raw value under the aforementioned two conditions, the critic might learn to use $g^{\text{max}}_{\text{root},k}$ as the predicted raw value. We conjecture that this may explain why the planning behavior learned by the agent seems to be similar without planning rewards, as illustrated in Appendix \ref{sec:G}, and also why the planning reward appears not to impact the final performance; the critic can learn to achieve similar results. Still, this heuristic reward can be beneficial for initially directing how to utilize a model, and it may lead to a modest to moderate boost in the initial learning speed.

\clearpage

\section{Relationship with Policy Iteration and Meta-Reinforcement Learning} \label{sec:I}

\subsection{Policy Iteration Algorithm}
Applying an RL algorithm to the Thinker-augmented MDP can be viewed as an instance of a policy iteration algorithm \cite{bellman1957dynamic, sutton2018reinforcement}. Policy improvements are carried out by an agent that conducts searches within the model. Given the current policy, these searches yield an improved policy. Simultaneously, the  policy evaluation is done by the learning of the non-stationary network. This involves training the network on trajectories generated by the improved policy, so as to learn the values of this improved policy. A noteworthy divergence from a typical policy iteration algorithm is the learning of the policy improvement operator, instead of employing a hand-crafted one such as MCTS.

Given this perspective, a crucial question arises: how effective is a learned policy improvement operator in comparison to a hand-crafted one? To address this question, we `freeze' the policy evaluation step (i.e., the training of the non-station network) and evaluate the efficiency of different policy improvement operators, such as MCTS and an RL agent, during a single step of policy improvement. 

Specifically, we start with a base policy that marginally outperforms a random policy in Sokoban. We then conduct a policy evaluation step on this policy by training the non-stationary network to estimate the value of this policy. Finally, we apply different policy improvement operators to the trained non-stationary network to estimate the policy improvement gap, while freezing the non-stationary network. For the sake of simplicity, we substitute the stationary network with the true environment dynamics, so a $n$-step exhaustive search could solve the task perfectly given sufficiently large $n$. As we are using the true environment dynamics, we did not limit the maximum search depth.

We experiment with three different types of policy improvement operators: (i) $n$-step exhaustive search, (ii) MCTS, and (iii) RL agents. In (iii), we trained the agents from scratch in the Thinker-augmented MDP, employing different planning steps $K$ and using the frozen model. The model's hidden state is not provided to the agents to ensure that they cannot directly learn the task. The results are shown in Figure \ref{fig:pi}.\footnote{Due to the low solving rate, we report the return of an episode here for better illustration, rather than the solving rate.} From the figure, we see that the base policy achieves a return of 0.12, and all policy improvement operators are able to improve on this policy and achieve a better return.

We observe that the policy improvement gap widens as the value of $K$ increases. With $K$=10, the agent's performance is similar to that of  a 3-step exhaustive search, which requires 155 model simulations. Increasing $K$ to 20, the performance is slightly worse than a 4-step exhaustive search, which requires 780 model simulations. At $K$=40, the agent performance surpasses a 5-step exhaustive search, which requires 3905 model simulations. In addition, the agent at $K$=10 also shows comparable performance to MCTS that uses 60 model simulations. It is also worth noting that the number of model simulations used by the agent is typically less than $K$, given that planning steps also include the traversal through expanded nodes.

The results here demonstrate that increasing $K$ to 40 can still enhance performance, whereas in the main text, the performance improvement gained by increasing $K$ above 10 is insignificant. We conjecture that once policy evaluation is unfrozen, the model can learn to predict the values of leaf nodes with much higher accuracy. This eliminates the need for a deep search to evaluate a rollout more precisely, rendering a large $K$ unnecessary.

These findings suggest that an agent can learn to perform policy improvement and achieve similar performance to hand-crafted algorithms while requiring significantly fewer simulations. However, it requires a moderate number of transitions for an agent to learn this policy improvement operator.

% Figure environment removed

\subsection{Meta-Reinforcement Learning}

In this section, we explore the relationship between meta-reinforcement learning (meta-RL) algorithms \cite{beck2023survey} and the Thinker algorithm. Meta-RL is a method designed to learn how to perform reinforcement learning. Its objective is to develop a policy that can quickly adapt to a variety of new tasks derived from a distribution of tasks, using minimal data. While meta-RL is often used in scenarios involving a task distribution, it is also applicable to single-task scenarios \cite{zahavy2020self, xu2020meta}, which is our primary focus here.

% meta-RL basics
In the meta-RL framework, a trial typically comprises $L$ episodes. During a trial, the agents initially encounter $K$ episodes—termed \emph{burn-in episodes}. Notably, the return within these episodes does not contribute to the main objective. Instead, the primary goal is to maximize the return of the remaining $L-K$ episodes, known as \emph{evaluation episodes}. Consequently, the purpose of a meta-RL algorithm is to use the knowledge gained throughout the entire trial to maximize returns during the evaluation episodes. One common class of meta-RL algorithm applies an RL algorithm with an RNN that can access previous actions and rewards, and the hidden state is not reset across episodes. As such, the knowledge of the trial is embedded in the RNN’s hidden states \cite{wang2016learning, duan2016rl}.

% key similarity
If the model perfectly represents the true environment dynamics and there are no planning rewards, we can align the Thinker-augmented MDP with meta-RL by considering the imaginary rollouts as (truncated) burn-in episodes, and the raw episode as the evaluation episode in meta-RL. This is because, without planning rewards, agents only aim to maximize the return in the raw episode. Thus, the objectives of meta-RL and RL agents on the Thinker-augmented MDP coincide. Moreover, the purpose of burn-in episodes in meta-RL or imaginary rollouts in planning is the same – to guide better actions in the evaluation or raw episodes, possibly through risk-taking behavior in those episodes.

% interleave and reset
However, unlike conventional meta-RL, which uses a fixed number of burn-in episodes followed by $L-K$ evaluation episodes, the Thinker-augmented MDP interleaves the burn-in episodes within a single evaluation episode. In other words, for each step in the evaluation episode with state $s_t$, we allow agents to have a few burn-in episodes starting from $s_t$.
%, before executing one action $a_t$ in the same evaluation episode. 
% Additionally, in the Thinker-augmented MDP, the burn-in trajectories are truncated as agents have the option to reset and there is a maximum search depth.

% learned model
In meta-RL, it is generally assumed that the burn-in episodes and evaluation episodes are drawn from the same MDP, but this assumption does not hold when the burn-in episodes are generated by a learned model. But if the MDP underlying the burn-in episodes, $\hat{\mathcal{M}}$, is similar to that of the evaluation episode, $\mathcal{M}$, then the meta-RL algorithm may also learn how to utilize the collected data while accounting for the difference. This is analogous to how the RL agent in the Thinker-augmented MDP learns to use the imaginary rollout for better action selection, despite the possible inaccuracies of the model.

% Some general closing or after-thought
In summary, there is a close relationship between learning to plan and learning to learn. The former employs a model on each step to decide better actions at that state, while the latter uses episodes within a trial to determine better actions in the evaluation episodes. This highlights the converging ideas in meta-learning and planning, both aiming to make efficient use of available information for decision-making.

\clearpage
\bibliographystyle{unsrt}
\bibliography{citation}

\end{document}