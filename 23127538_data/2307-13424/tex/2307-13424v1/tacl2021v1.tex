% File tacl2021v1.tex
% Dec. 15, 2021

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2021v1}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2021v1}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2021v1}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

%\usepackage{tacl2021v1}
\usepackage[acceptedWithA]{tacl2021v1}
% \setlength\titlebox{10cm} % <- for Option 2 below

%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author information supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block

\usepackage{paralist}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath,cases}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{epstopdf}
\usepackage{comment}
\usepackage{url}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{color}
\usepackage{soul}
\newcommand{\colorb}[1]{\textcolor[RGB]{10,30,91}{#1}}
\newcommand{\colorr}[1]{\textcolor[RGB]{165,42,42}{#1}}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

%%%%

\title{Holistic Exploration on Universal Decompositional Semantic Parsing: Architecture, Data Augmentation, and LLM Paradigm}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

% The author block may be formatted in one of two ways:

% Option 1. Author’s address is underneath each name, centered.

% \author{
%   Template Author1 
%   \\
%   Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \And
%   Template Author2 
%   \\
%   Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
%   \And
%   Template Author2 
%   \\
%   Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
%     \And
%   Template Author2 
%   \\
%   Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
%     \And
%   Template Author2 
%   \\
%   Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

% % Option 2.  Author’s address is linked with superscript
% % characters to its name, author names are grouped, centered.

\author{
  Hexuan Deng\textsuperscript{\rm 1},
  Xin Zhang\textsuperscript{\rm 1},
  Meishan Zhang\textsuperscript{\rm 1},
  Xuebo Liu\textsuperscript{\rm 1},
  Min Zhang\textsuperscript{\rm 1}
  \\
  \textsuperscript{\rm 1} Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China
  \\
  \texttt{\{22s051030,zhangxin2023\}@stu.hit.edu.cn}
  \\
  \texttt{\{zhangmeishan,liuxuebo,zhangmin2021\}@hit.edu.cn}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
In this paper, we conduct a holistic exploration of the Universal Decompositional Semantic (UDS) Parsing. We first introduce a cascade model for UDS parsing that decomposes the complex parsing task into semantically appropriate subtasks. Our approach outperforms the prior models, while significantly reducing inference time. We also incorporate syntactic information and further optimized the architecture. Besides, different ways for data augmentation are explored, which further improve the UDS Parsing. Lastly, we conduct experiments to investigate the efficacy of ChatGPT in handling the UDS task, revealing that it excels in attribute parsing but struggles in relation parsing, and using ChatGPT for data augmentation yields suboptimal results. Our code is available at~\url{https://github.com/hexuandeng/HExp4UDS}.
\end{abstract}

\section{Introduction}

A long-standing objective in the fields of natural language understanding and computational semantics is to create a structured graph of linguistic meaning. Various efforts have been made to encode semantic relations and attributes into a semantic graph---e.g., Abstract Meaning Representation (AMR; \citealp{AbstractMeaningRepresentation_2013}), Universal Conceptual Cognitive Annotation (UCCA; \citealp{UniversalConceptualCognitive_2013}), and Semantic Dependency Parsing formalisms (SDP; \citealp{SemEval2014Task_2014a, ComparabilityLinguisticGraph_2016a}). Recently, Universal Decompositional Semantics (UDS; \citealp{UniversalDecompositionalSemantics_2020}) introduce an alternative approach, as shown in Figure~\ref{fig:dataset}. It constructs semantic relations from syntactic annotations \citep{EvaluationPredPattOpen_2017a}, and annotates semantic attributes following the decompositional semantics \cite{SemanticProtoRoles_2015a}, which takes the form of many simple questions about words or phrases, thus significantly lowering the annotation cost.

% Parsing is a fundamental task in natural language processing(NLP), which acts as the basic part of multiple upstream tasks, e.g., question answering, text summarization, sentiment analysis, et al. Parsing has three components that involve the analysis of syntactic, semantic structures as well as the attributes of nodes and edges. Previous attempts have tried to jointly parse all the components in a single model \citep{MultitaskParsingSemantic_2018, BroadCoverageSemanticParsing_2019, JointUniversalSyntactic_2021}, which shows that the three components of parsing do not stand alone, but rather help the parsing of other parts. However, it seems impossible to achieve better results on all the three parsing objectives simultaneously. To the best of our knowledge, there is no systematic study on whether the three components help each other in all situations, and what methods are more beneficial when given a certain goal. 

% Many parsing datasets have been devised, e.g., Abstract Meaning Representation (AMR; \citealp{AbstractMeaningRepresentation_2013a}), Universal Conceptual Cognitive Annotation (UCCA; \citealp{UniversalConceptualCognitive_2013}), and Semantic Dependency Parsing formalisms (SDP; \citealp{SemEval2014Task_2014, ComparabilityLinguisticGraph_2016}). These semantic representations have varying degrees of abstraction from the input and syntax, ranging from being directly tied to the input tokens (e.g. SDP formalisms) to being heavily abstracted away from it (e.g. AMR, UCCA). Universal Decompositional Semantics (UDS; \citealp{UniversalDecompositionalSemantics_2020}) falls between these extremes, with a semantic graph that is closely tied to the syntax while not being constrained to match the input tokens. Furthermore, UDS graphs also host scalar-valued crowdsourced annotations encoding a variety of semantic inferences. The high-quanlity and closely-related syntactic, semantic, and attribute annotation is perfectly suited for outr puropse, and the proper degree that tied to the input tokens makes it making it a good representative dataset for parsing, and an example is shown in Figure~\label{fig:dataset}.

Previous parsing models for UDS dataset are mainly under the Seq2Seq transduction framework \citep{UniversalDecompositionalSemantic_2020}, which suffer from poor parallelism and long inference time that increase with the sentence length. In this paper, we propose a cascade architecture that decomposes the complex parsing task into multiple subtasks in a semantically appropriate manner. Within each subtask, our model predicts all corresponding sentence elements simultaneously, enhancing parallelism and substantially reducing inference time. Experimental results demonstrate that our approach outperforms previous models while maintaining high efficacy during inference.

To further improve our proposed model, we introduce enhancements from two perspectives. Firstly, we try to incorporate syntactic information, which has been proven beneficial to many downstream tasks \citep{AdaptiveKnowledgeSharing_2018b, SyntaxAwareOpinionRole_2020a, JointUniversalSyntactic_2021, Deng_Ding_Liu_Zhang_Tao_Zhang_2023}. We use multi-task training \cite{MultitaskLearning_1997a} as the default setting and propose several approaches for better utilizing syntactic information. Secondly, while \citet{UniversalDecompositionalSemantic_2020} have tried to utilize the external tool PredPatt \citep{EvaluationPredPattOpen_2017a}, which contains the relationship between syntax and semantic information, they do not achieve any improvements. In contrast, we propose a data augmentation method that effectively exploits the capabilities of PredPatt, leading to significant performance gains in relation parsing. Moreover, we have explored various approaches for these enhancements, providing guidance for the design of similar systems.

Large language models (LLMs), such as ChatGPT and GPT-4 \citep{SparksArtificialGeneral_2023}, have attracted considerable attention due to their impressive ability. These models engage in conversational interactions with users, accepting natural language prompts and producing textual responses. Their applications cover a broad spectrum, including machine translation \citep{ChatGPTGoodTranslator_2023}, grammar error correction \cite{ChatGPTGrammarlyEvaluating_2023}, information extraction \cite{EvaluatingChatGPTInformation_2023}, among others. We investigate the performance of LLMs on the UDS task. Our experiments involve either directly applying the LLMs for parsing or using LLMs to generate data to enhance downstream models. Results demonstrate that LLMs excel in attribute parsing but struggle in relation parsing, which appears to be too complex for LLMs.
% Our contributions are summarized as follows:
% \begin{compactitem}
% \item We built a cascade model on top of the UDS dataset, which improve the parsing quantity, while shorten the inference time compared to the previous state-of-the-art model.
% \item We proposed several methods for better incorporating syntactic information, which outperforms the basic multi-task training method, and verify the necessity of syntactic information for semantic parsing.
% \item We propose several data augmentation methods to argument the sementic parsing without introduce any labeled data, which achieve considerable improvement, and is orthogonal to the improvement achieved above.
% \end{compactitem}


\section{Background and Related Work}
\label{sec:related}

\paragraph{UDS Datasets} \citet{GoldStandardDependency_2014a} create a standard set of Stanford dependency annotations for the English Web Treebank (EWT, \citealp{GoldStandardDependency_2014a}) corpus. Subsequently, \citet{UniversalDecompositionalSemantics_2016} proposed a framework aimed at constructing and deploying cross-linguistically robust semantic annotation protocols and proposed annotations on top of the EWT corpus using PredPatt \citep{UniversalDecompositionalSemantics_2016, EvaluationPredPattOpen_2017a}. Several works have then been proposed to provide semantic annotations within this framework, including annotations for semantic roles \citep{SemanticProtoRoles_2015a}, entity types \citep{UniversalDecompositionalSemantics_2016}, event factuality \citep{NeuralModelsFactuality_2018}, linguistic expressions of generalizations about entities and events \citep{DecomposingGeneralizationModels_2019a}, and temporal properties of relations between events \citep{FineGrainedTemporalRelation_2019a}. All of these efforts culminated in \citet{UniversalDecompositionalSemantics_2020}, which presents the first unified decompositional semantics-aligned dataset, namely, Universal Decompositional Semantics (UDS).

\paragraph{UDS Parser} UDS parsing has been conducted using transition-based parser \citep{FastAccurateDependency_2014a}, deep biaffine attention parser \citep{DeepBiaffineAttention_2017a}, and sequence-to-graph transductive parser \citep{UniversalDecompositionalSemantic_2020}. The latter significantly outperforms the others by employing an efficient Seq2Seq transduction framework \citep{SequenceSequenceLearning_2014, NeuralMachineTranslation_2015}. This approach is initially used in AMR parsing \citep{AMRParsingSequencetoGraph_2019} and later extended to cover other semantic frameworks, such as UCCA and SDP, by \citet{BroadCoverageSemanticParsing_2019} in a unified transduction framework, which predicted nodes and corresponding edges simultaneously in a Seq2Seq manner. For UDS, an attribute module is added by \citet{UniversalDecompositionalSemantic_2020}. Syntactic information is incorporated into the model by \citet{JointUniversalSyntactic_2021}, yielding further improvements. Despite these attempts, cascade models with better parallelism and shorter inference time have not yet been explored.

\paragraph{Incorporating Syntactic Information} Syntactic information has been shown to improve the performance of downstream tasks. Multi-task learning is widely used to incorporate syntactic information. \citet{MultitaskParsingSemantic_2018a} improve the performance of semantic parsing by using multi-task learning, with syntactic and other semantic parsing tasks serving as auxiliary tasks. \citet{AdaptiveKnowledgeSharing_2018b} use syntactic and semantic information to improve the efficacy of two low-resource translation tasks. \citet{JointUniversalSyntactic_2021} employ a single model to parse syntactic and semantic information simultaneously to improve semantic parsing. Graph convolutional networks (GCN, \citealp{SemiSupervisedClassificationGraph_2017a}) have also been widely used. \citet{EncodingSentencesGraph_2017a} use GCNs to incorporate syntactic information in neural models and construct a syntax-aware semantic role labeling model. \citet{GraphConvolutionPruned_2018a} propose an extension of GCNs to help relation extraction models capture long-range relations between words. \citet{SyntaxAwareOpinionRole_2020a} present a syntax-aware approach based on dependency GCNs to improve opinion role labeling tasks.


\section{Preliminaries}
\label{sec:prelim}

The UDS dataset comprises three layers of annotations: syntactic annotations, semantic relation annotations, and decompositional semantic attribute annotations at the edge and node levels.

\paragraph{Syntactic Annotation} is derived from the EWT dataset, which provides consistent annotation of grammar, including part-of-speech (\emph{POS}) tag, morphological features, and syntactic dependencies, for human languages. These annotations are used to construct the \emph{syntactic tree}, where each word is tied to a node. As shown in Figure~\ref{fig:dataset}, the headword of "we" is "get", and the headword of the root "Sounds" is defined as itself.

% In Figure~\ref{fig:dataset}, the syntactic tree consists of the words (gray) and syntactic edges (purple), with each word having exactly one head node except the root word. For instance, the headword of "we" is "get", and the headword of the root "Sounds" is defined as itself.

\paragraph{Semantic Relations} consist of predicates, arguments, and edges between them, which forms the \emph{semantic relations}. It is generated by Predpatt tool \citep{EvaluationPredPattOpen_2017a} automatically, using the POS tag and the syntactic tree as input. Each semantic node explicitly corresponds to one word in the sentence called the center word, demonstrated by the instance edge. Additionally, each semantic node is also tied with several non-repetitive words with the non-head edge, which forms a multi-word span. As shown in Figure~\ref{fig:dataset}, the leftmost predict node has a span "Sounds like" with the center word "Sounds". Note that two semantic nodes may correspond to the same word in the case of clausal embedding. Then, an extra argument node "SOMETHING" is introduced as the the root of clause, e.g., "executed" is corresponding to an extra argument node.

% In Figure~\ref{fig:dataset}, the semantic relation is corresponding to the argument (blue) and predicate (red) nodes, and edges (yellow) between them. For each node, the instance edge (solid) shows the center word, and the non-head edge (dash) shows the span of the semantic node, e.g., the left most predict node has a span "Sounds like" with the center word "Sounds". Note that two semantic nodes may corresponding to a same word in the case of clausal embedding. Then, an extra argument node "SOMETHING" is introduced as the the root of clause, e.g., "executed" is corresponding to an extra argument node.

% Figure environment removed

\paragraph{Semantic Attributes} consist of crowdsourced decompositional annotations tied to the semantic relations, detailed in \S\ref{sec:related}. These annotations can be further categorized into node-level and edge-level attributes, corresponding to the table on the left and right in Figure~\ref{fig:dataset}, respectively. For each node or edge, all attributes have a value in range $[-3, 3]$. Besides, each attribute also has a confidence in range $[0, 1]$, which shows how likely it is to have the property. Following \citet{UniversalDecompositionalSemantic_2020}, we discretized it into $\{0, 1\}$ by setting every non-zero confidence to one.


\section{Methodology}

In this section, we introduce our cascade model, methods to improve its performance, and the exploration of LLMs.
% Specifically, We introduce our cascade model for UDS parsing in \S\ref{sec:models}, techniques to incorpoate syntactic information into our model in \S\ref{sec:addsyn}, the data augmentation method with the help of external toolkit in \S\ref{sec:data-centric}, and the performance exploration of the large language model on the current task in \S\ref{sec:llm}, seperately.

% Figure environment removed

\subsection{Efficient Cascade Model}
\label{sec:models}

As discussed in \S\ref{sec:prelim}, our goal is to predict syntactic information (POS tags and syntactic tree), semantic relations (semantic nodes, edges, and spans), and semantic attributes (node- and edge-level) using a single model. To this end, we propose a cascade model to predict all of these information step by step, as illustrated in Figure~\ref{fig:model}. In the following paragraphs, we discuss each component of our model in detail. The sentence is represented as $x_1, x_2, \dots, x_K$, where $x_i$ represents the $i$-th word. The properties of the node are represented as $t$, the properties of the edge as $e$, the $\operatorname{softmax}$ function as $\sigma$, and the $\operatorname{ReLU}$ function as $R$.
% We use the same representation $w_1, w_2, \dots, w_M$ for the word and syntactic node in the sentence because they are one-on-one, and the semantic nodes in the sentence are representant as $n_1, n_2, \dots, n_N$.

\paragraph{Encoder Module} embeds each word $x_i$ into a corresponding context-aware representation $h_i$. We utilize three types of encoders: multi-layer BiLSTM, transformer encoder, and Pre-trained Language Model (BERT, \citealp{BERTPretrainingDeep_2019}). For BiLSTM and transformer encoder, we employ a similar embedding layer with \citet{JointUniversalSyntactic_2021} to ensure comparability, concatenating GloVe word embeddings \citep{GloveGlobalVectors_2014}, character CNN embeddings, and BERT contextual embeddings. For BERT encoder, we use the default subword embeddings layer and mean-pool over all subwords of a word to obtain the word-level representations.

% \paragraph{POS model} generates the part-of-speech tag $t_i^{p}$ of the word $w_i$. A simple multi-layer perceptron ($\operatorname{MLP}$) is used, formally:
% \begin{equation}
%     \hat{t}_i^{p} = p(t_p | w_i) = \sigma(\operatorname{MLP}(h_i))
%     \label{eq:pos}
% \end{equation}

\paragraph{Syntactic Module} predicts the part-of-speech (POS) tag and the syntactic tree. For POS, we use a simple multi-layer perceptron ($\operatorname{MLP}$) over each word representation $h_i$. For the syntactic tree, each word has exactly one syntactic head, so we predict the headword $x_i^y$ and the corresponding edge type $t_i^y$ for each word $x_i$. We follow the approach of \citet{DeepBiaffineAttention_2017a} and \citet{BroadCoverageSemanticParsing_2019} to use a biaffine parser, formally:
\begin{equation}
\begin{aligned}
    \hat{x}_i^y = & p(x|x_i) \\
    = & \sigma(\operatorname{Biaffine}(R(w_l^yh_i), R(w_r^yh_{1:i-1}))) \\
    \hat{t}_i^y = & p(t_y|x_i, \hat{x}^h_i) \\
    = & \sigma(\operatorname{Bilinear}(R(w_l^th_i), R(w_r^t\hat{h}_{1:i-1}^y)))
    \label{eq:synedge}
\end{aligned}
\end{equation}
where $x \in \{x_1, x_2, \dots, x_K\}$, and $\hat{h}_i^y$ is the representation of the predicted head $\hat{x}_i^y$.

\paragraph{Word Classification Module} predicts the semantic edge directly connected to each word (instance, non-head) and the type of the parent node. We simplify this edge prediction problem into a classification problem. We define:
\begin{compactitem}
\item Type ``$\Phi$'': Words with no connecting edge;
\item Type ``Syn'': Words connect with a non-head edge;
\item Type ``Pre'': Words connect with an instance edge, and its parent is a predicate node;
\item Type ``Arg'': Words connect with an instance edge, and its parent is an argument node;
\item Type ``Pre + Arg'': Words connect with two instance edge, and its parents are a predicate node and an argument node;
\end{compactitem}
We use a simple $\operatorname{MLP}$ for classification, formally:
\begin{equation}
    \hat{t}_i^m = p(t_m | x_i) = \sigma(\operatorname{MLP}(h_i))
    \label{eq:semnode}
\end{equation}

%, non-head edges as "Syn", instance edges connected with predicate nodes as "Pre", and instance edges connected with argument nodes as "Arg". Besides, each word may be connected to two semantic nodes, and we simply design a new label. For example, we define like as "Syn", we as "Arg", and executed as "Pre + Arg" in Figure~\ref{fig:model}. 

\paragraph{Node Generation} predicts the syntactic nodes, semantic nodes, and their corresponding node embeddings. Syntactic nodes $n_1,$ $n_2, \dots, n_N$ have label ``Syn'', and we define the embedding $g^n_i$ of the syntactic node $n_i$ the same as its word embeddings. semantic nodes $m_1,$ $m_2, \dots, m_M$ have label ``Pre'', ``Arg'', or ``Pre + Arg''. We generate will two nodes for ``Pre + Arg''. So for node embeddings, we first concatenate a node type embedding with its word embedding to distinguish whether it is an argument or predicate node. Then we project it back to the previous dimension with a linear layer to generate the embedding $g^m_i$ of the semantic node $m_i$. Furthermore, we generate a virtual root node for every sentence with the same trainable embeddings.

\paragraph{Semantic Span Module} predicts the semantic span by separating each syntactic node to the semantic nodes. Each syntactic node belongs to exactly one semantic node. So we use the same model as Eq.~\ref{eq:synedge} to predicted which semantic node $m_i^h$ is the syntactic node $n_i$ belongs to, formally:
\begin{equation}
\begin{aligned}
    \hat{m}_i^h = & p(m|n_i)\\
    = & \sigma(\operatorname{Biaffine}(R(w_l^mg^n_i), R(w_r^mg_{1:i-1}^m)))
    \label{eq:semspan}
\end{aligned}
\end{equation}
where $m \in \{m_1, m_2,\dots,$ $m_M\}$. The new span level embedding $g^s_i$ for the semantic node $m_i$ is the same as $g^m_i$ by default. Besides, we have also tried to refine $g^s_i$ with the syntactic node embedding, which does not achieve obvious effects.

\paragraph{Semantic Edge Module} predicts the edge and the corresponding type $e_{i,j}^m$ between any two semantic nodes. We consider the case where there is no edge between two semantic nodes as a special type $\Phi$. For prediction, we consider the span level embedding for each pair of nodes, formalized as:
\begin{equation}
\begin{aligned}
    \hat{e}_{i,j}^m = & p(e_m|m_i, m_j)\\
    = & \sigma(\operatorname{Biaffine}(R(w_l^eg^s_i), R(w_r^eg^s_j)))
    \label{eq:semedge}
\end{aligned}
\end{equation}

\paragraph{Attribute Module} predicts the node-level attributes $\hat{t}_i^{a}$ for node $m_i$, and edge-level attributes $\hat{e}_{i,j}^{a}$ for edge between $m_i$ and $m_j$. We use the $\operatorname{MLP}$ model as the main part, formalized as follows:
\begin{equation}
\begin{aligned}
    \hat{t}_i^{a} = & \operatorname{MLP}(g^s_i) \\
    v_i= & R(w_l^vg^s_i), \; v_j=R(w_r^vg^s_j) \\
    \hat{e}_{i,j}^{a} = & \operatorname{MLP}([v_i^TWv_j, v_i, v_j])
    \label{eq:attr}
\end{aligned}
\end{equation}
Here, $W \in \mathbb{R}^{d_v \times d_v \times d_o}$, where $d_v$ is the dimension of $v_i$ and $v_j$, and $d_o$ is the output dimension. $i,j$ must satisfy $\hat{e}_{i,j}^m \neq \phi$ for $\hat{e}_{i,j}^{a}$ (edge exists). Note that attributes may not exist, and we use the same model as above to predict the mask of attributes.

% \paragraph{Semantic Node Classification} generates all the semantic nodes of a sentence and its corresponding span, while one of the syntactic node in the span is regarded as the center node, detailed in \S\ref{sec:prelim}. Each syntactic node may be the center of 0, 1, or 2 semantic nodes at the same time, which makes this problem much complex. We used a cascade model to solve the problem.

% We first generate the semantic node by finding out all the center words, identified by the instance edge in Figure~\ref{fig:dataset}. To figure out the number and the corresponding type of the semantic nodes which the syntactic node is corresponding to, we simply design a different label $t_i^m$ for the word $w_i$ corresponding to the number of semantic nodes, e.g., "$\phi$" for like (0 semantic nodes), "arg" for we (1 semantic nodes), and "arg-pred" for executed (2 semantic nodes) in Figure~\ref{fig:dataset}. A simple $\operatorname{MLP}$ is used, formally:
% \begin{equation}
%     \hat{t}_i^m = p(t_m | w_i) = \sigma(\operatorname{MLP}(h_i))
%     \label{eq:semnode}
% \end{equation}
% Then all the semantic nodes $n_1, n_2, \dots, n_N$ can be easily extracted. To get the embedding vector $g_i$ of node $n_i$, we first use the embedding $h_j$ of the syntactic node $w_j$ it generate from. Since two nodes may corresponding to the same syntactic word, causing a same embedding for two different node, we contact a node type embedding with $h_j$ as the embedding $g_i$ to show that it is an argument node or a predicate node, corresponding to the ``Semantic Embedding'' in Figure~\ref{fig:model}.

% We then predicte which semantic span is the syntactic node belongs to, corresponding to the non-head edge in Figure~\ref{fig:dataset} and ``Semantic Span'' part in Figure~\ref{fig:model}. Since there is no multiple edges problem, we use the same model as Eq.~\ref{eq:synedge} to predicted the semantic head node of the syntactic nodes unused in the first model, formalized as: 
% \begin{equation}
% \begin{aligned}
%     \hat{n}_i^h = & p(n|w_i)\\
%     = & \sigma(\operatorname{Biaffair}(\delta_{e1}^y(h_i), \delta_{e2}^y(g_{1:i-1}^h)))
%     \label{eq:semspan}
% \end{aligned}
% \end{equation}
% where $\delta$ is defined in Eq.~\ref{eq:synedge}, $n \in \{n_1, n_2, \dots, n_N\}$, $g^h_i$ is the representation of the predicted head $\hat{n}_i^h$, and $i$ satisfies $\hat{t}_i^m = \phi$ (no corresponding semantic node).

% \paragraph{semantic edge model} generates the edge and the corresponding type $t_{i,j}^e$ between any two semantic nodes, corresponding to the ``Semantic Edge'' part in Figure~\ref{fig:model}. We regard "no edge" as a type "$\phi$", and use similar model as Eq.~\ref{eq:semspan}, formalized as:
% \begin{equation}
% \begin{aligned}
%     \hat{e}_{i,j}^m = & p(e_m|n_i, n_j)\\
%     = & \sigma(\operatorname{Biaffine}(\delta_{t1}^y(g_i), \delta_{t2}^y(g_j)))
%     \label{eq:semedge}
% \end{aligned}
% \end{equation}
% where $\delta$ is defined in Eq.~\ref{eq:synedge}.

% \paragraph{Attribute model} generates the attribute of nodes and edges shown in the table in Figure~\ref{fig:dataset}, corresponding to the ``Node Attribute'' and `Edge Attribute'' parts in Figure~\ref{fig:model}. Attributes may not exist, and we regard it as an additional attribute and both use the regressive $\operatorname{MLP}$ model to generate, formalized as follows:
% \begin{equation}
% \begin{aligned}
%     v_{i,j}=\operatorname{Biaffine}(\delta_{e1}^y(g_i), \delta_{e2}^y(g_j)) \\
%     \hat{t}_i^{n} = \operatorname{MLP}(g_i),\hat{t}_{i,j}^{e} = \operatorname{MLP}(v_{i,j})
%     \label{eq:attr}
% \end{aligned}
% \end{equation}
% where $\delta$ defined in Eq.~\ref{eq:synedge}, and $i,j$ satisfy $\hat{e}_{i,j}^m \neq \phi$ for $\hat{t}_{i,j}^{e}$ (edge exists).

\paragraph{Loss} To train our models, we use different loss functions depending on the task. For word classification, semantic span, and semantic edge modules, we use cross-entropy loss. For attribute module, when predicting the mask, we use binary cross-entropy loss. When predicting the attribute, we follow \citet{UniversalDecompositionalSemantic_2020} to use a composite loss function $\mathcal{L}$ for the values, formally:
% \begin{equation}
% \small
% \begin{aligned}
%     \mathcal{L}_{\operatorname{MSE}}\left(\hat{t}, t\right)= & \frac{1}{NK} \sum_{i=1}^N \sum_{j=1}^K \mathbbm{1}_i\left(\hat{t_i^j}-t_i^j\right)^2 \\
%     \mathcal{L}_{\mathrm{BCE}}\left(\hat{t}, t\right)= & \frac{1}{NK} \sum_{i=1}^N \sum_{j=1}^K \left(\tau(\hat{t_i^j}) \log (\tau(t_i^j)) \right. \\
%     & +\left.(1-\tau(\hat{t_i^j})) \log (1-\tau(t_i^j))\right) \\
%     \mathcal{L}\left(\hat{t}, t\right)= & \frac{2 \cdot \mathcal{L}_{\mathrm{MSE}}\left(\hat{t}, t\right) \cdot \mathcal{L}_{\mathrm{BCE}}\left(\hat{t}, t\right)}{\mathcal{L}_{\mathrm{MSE}}\left(\hat{t}, t\right)+\mathcal{L}_{\mathrm{BCE}}\left(\hat{t}, t\right)}
%     \label{eq:attrloss}
% \end{aligned}
% \end{equation}
\begin{equation}
\small
    \mathcal{L}_{attr}^{value}\left(\hat{t}, t\right)= \frac{2 \cdot \mathcal{L}_{\mathrm{MSE}}\left(\hat{t}, t\right) \cdot \mathcal{L}_{\mathrm{BCE}}\left(\hat{t}, t\right)}{\mathcal{L}_{\mathrm{MSE}}\left(\hat{t}, t\right)+\mathcal{L}_{\mathrm{BCE}}\left(\hat{t}, t\right)}
    \label{eq:attrloss}
\end{equation}
where $\mathcal{L}_{\mathrm{MSE}}$ is the mean squared loss, $\mathcal{L}_{\mathrm{BCE}}$ is the binary cross-entropy loss, $t$ is the gold attribute, and $\hat{t}$ is our prediction. $\mathcal{L}_{\operatorname{MSE}}$ encourages the predicted attribute value to be close to the true value, while $\mathcal{L}_{\operatorname{BCE}}$ encourages the predicted and reference values to share the same sign. 

Finally, to handle this multi-task problem, we use a weighted sum of all the loss functions mentioned above for our model:
\begin{equation}
\small
\mathcal{L}=a_1\mathcal{L}_{cls} + a_2\mathcal{L}_{span} + a_3\mathcal{L}_{edge} + a_4\mathcal{L}_{attr}^{mask} + a_5\mathcal{L}_{attr}^{value}
    \label{eq:totloss}
\end{equation}
where $a_i = 1$ for $i \in [1, .., 5]$, except $a_2 = 2$.

\subsection{Incorporating Syntactic Information}
\label{sec:addsyn}

By default, we incorporate syntactic information by \emph{multi-task training}. Additionally, we propose \emph{GCN} and \emph{attention} approaches for a more profound incorporation of syntactic information. Specifically, we utilize the syntactic information to update the word embeddings generated by the encoder. The strategies are as follows:

\paragraph{Multi-task Training}
We add the loss of the syntactic module to term $\mathcal{L}$, which incorporates syntactic information into the shared encoder through back-propagation. We use cross-entropy loss for POS and syntactic tree parsing, formally:
\vspace{-3pt}
\begin{equation}
\small
\mathcal{L_{\text{+}SYN}}=\mathcal{L} + a_6\mathcal{L}_{pos} + a_7\mathcal{L}_{tree}
\vspace{-3pt}
\label{eq:synloss}
\end{equation}
where $a_6 = a_7 = 1$.

\paragraph{GCN} Inspired by the idea of GCN \citep{SemiSupervisedClassificationGraph_2017a}, we try to encode the predicted adjacency matrix information into the embedding. In the syntactic tree, we consider two types of edges: directed edges from parent nodes (top) to child nodes (bottom), and those with reverse directions. Then we employ a bidirectional GCN consisting of 1) top-down GCN to convey sentence-level information to local words, and 2) bottom-up GCN to convey phrase-level information to the center word. Additionally, to further convey the edge type information corresponding to the current word, we 3) consider the probability distribution of its edge type, and use a GCN-like method to convey this information. With the word embedding matrix $\mathbf{H}$ being the input $\mathbf{H}^{(0)}$, we use a $l$ layer model (with $l=2$ in practice), formally:
\begin{equation}
\begin{aligned}
    \mathbf{V}^{(i)} = & [\mathbf{A}_h\mathbf{H}^{(i)}\mathbf{W}_1^{(i)}, \mathbf{A}_h^T\mathbf{H}^{(i)}\mathbf{W}_2^{(i)}, \mathbf{A}_t\mathbf{T}_{e}\mathbf{W}_3^{(i)}] \\
    & \mathbf{H}^{(i+1)} = R(\mathbf{W}_4^{(i)}R(\mathbf{V}^{(i)})) \\
    & \mathbf{H}_o = \mathbf{W}_o[\mathbf{H}^{(0)}, \mathbf{H}^{(l)}]
    \label{eq:udgcn}
\end{aligned}
\end{equation}
where $\mathbf{A}_h$ is the top-down adjency matrix prediction, $\mathbf{A}_h^T$ is the bottom-up ones, $\mathbf{A}_t$ is the edge type probability distribution, and $\mathbf{T}_{e}$ is the trainable edge type embedding matrix. Note that the adjacency matrix does not self-loop, so GCN does not convey information about the words themselves. We then combine the original word embeddings $\mathbf{H}^{(0)}$ with the output $\mathbf{H}^{(l)}$ to get the new word embeddings $\mathbf{H}_o$. Under such a design, good results can be achieved with a relatively shallow network.

% We use the word embedding matrix in the sentence $\mathbf{H}^{(0)}$ as input, and output the new word embedding $\mathbf{H}_o$. 
% In order to explore the extent to which syntactic information is beneficial to semantic and attribute prediction, we propose a combination of several different dimensions. We have used three combination strategies of different degrees, from shallow to deep: using the same encoder word vector, but the model structure and parameters of each part are different; on the basis of the above, the grammatical prediction information is fused in some form Enter the word vector and use it for subsequent prediction; bind some parameters of syntax, semantics, and attribute prediction. These three methods and the strategies we actually use will be described in detail later.

% \paragraph{Share Encoder} As described in sec4.1, we use different output model for different prediction target, but using the same encoder model to share the basic word embeddings. Two strategies have we used: 1) pretraining encoder using syntactic information; 2) joint training all the information using the same encoder.

% \paragraph{Join syntactic information} in the cotext of joint training all the information using the same encoder, we try to join the syntactic information into the context-aware word embeddings. Two strategies have we used: 1) gcn. Then we dopt a two-layer gcn to encode the syntactic into the syntactic model. In order to add the node and edge type we predicted, and get a relatively good result using a relatively shallow network, wo use the $l$ layer model shown as follows:

\paragraph{Attention} Word representation after dimension reduction used in syntactic edge and type prediction contains basic information of the syntactic tree \citep{JointUniversalSyntactic_2021}. So we directly use the representations in Eq.~\ref{eq:synedge}, which are used in the $\operatorname{Biaffine}$ and $\operatorname{Bilinear}$ model. Formally:
\begin{equation}
\begin{aligned}
    \mathbf{V} = & R([w_l^w\mathbf{H}, w_r^w\mathbf{H}, w_l^t\mathbf{H}, w_r^t\mathbf{H}]) \\
    & \mathbf{H}_o = \mathbf{W}_o[\mathbf{H}, \mathbf{A}_h \mathbf{V}]
    \label{eq:udctat}
\end{aligned}
\end{equation}
where all the $w_*^*\mathbf{H}$ come from Eq.~\ref{eq:synedge} without recalculation. Comparing to the GCN approach, this method uses fewer new parameters and requires less additional calculation, while still preserving the performance improvements achieved by the GCN approach to some extent.

% \paragraph{Combine model prameters} note that they all dist relation between two nodes, wo doubt that we can use a single model to describe all the relation in syntactic, semantic and attribute. we try to share some of the pram and keep most of the flexiblity. insly, wo combine biaffair in Eq. 2,3,and5,and using different proj func, . we use the same model dimen in Equation5 (the largest) and add an output proj func for eq2and3.

% Figure environment removed

\subsection{Data Augmentation}
\label{sec:data-centric}

One of the features of UDS dataset is the strong correlation with external tools PredPatt. \citet{UniversalDecompositionalSemantic_2020} attempt to use an external model to predict the POS tags and syntactic tree on the test dataset, which are then fed directly to PredPatt to obtain the semantic relations. However, the effectiveness of this method is relatively poor, likely due to two issues: 1) the error transmission problem comes from the prediction of the syntactic model, and 2) the rule-based tools are not as robust as neural networks towards noisy inputs.

To address these issues, we propose a data augmentation method. Instead of using it during inference, we use it to augment the data, with only the help of external unlabeled data. Specifically, we first train a model to predict the syntactic tree and POS tag, using the above syntactic model. Next, we use PredPatt to generate pseudo labels (i.e., semantic relations) for the unlabeled data. Finally, we use these data to pre-train our model, and then fine-tune it with a smaller learning rate using the labeled UDS dataset, which achieves significant improvements in relation parsing.
%We also explore using a trained external model to generate pseudo-labels. Compare to \citet{UniversalDecompositionalSemantic_2020} and our basic model, this method 

\subsection{Large Language Models}
\label{sec:llm}
% With the release of ChatGPT and GPT-4, large language models (LLMs), which trained on massive datasets and with massive parameters, have gained significant attention, and may push the boundaries of what is possible in NLP. It can interact in a conversational style that align with the user, accepts both natural language as prompts, and returns textual responses. These models have a wide range of applications, including machine translation, conversation, grammar error correction, and natural language understanding tasks, e.g., information extraction, sentiment analysis.
We explore the direct use of large language models (LLMs) for parsing tasks. First, for semantic relation parsing, we try two types of prompts shown in Figure~\ref{fig:prompt}: 1) The Predpatt prompt guides the LLM to first generate the syntactic parsing, then follow the instruction of the PredPatt tool step by step to generate the semantic relations. 2) Cascade approach follows the idea of our model to decompose the UDS parsing, which first selects the center phrase of the semantic node, then expands every phrase into a span. To make sure that the center word has only one word, we select it at the last step. Second, for semantic attribute parsing, we provide the sentence and the corresponding node/edge as input, definitions of attribute types as instruction, and conduct experiments under the oracle setting defined in \S\ref{sec:metric}. As the scale of attribute scoring may vary across conversation rounds, we only let it predict positive or negative.
%Since this is a relatively small and new datasets, directly let LLM to do UDS parsing is difficult. 

Additionally, we investigate the generation of new data for downstream model training, which is widely used. We use the random token lists as input rather than the unlabeled data, and let LLM generate the POS tag and syntax tree, which are further used to generate pseudo-labels, following \S\ref{sec:data-centric}. Since Universal Dependencies is a widely used dataset and contains both of the required information, a simple prompt can be used.

% , expecially under zero-shot settings, since describe the full information included in a graph is complex and inefficient

% Besides, we are also curious about how our model do under zero-shot data-centric approach. we also alleviate a zero-shot approach using chatgpt, which take random selected word list and then output a semantic ok sent with its corresponding SYN, further tune the model trained above, and get significantly better results. This method also can explore how well can the LLM do with the help of the external tool.

\begin{table*}[t]
\small
\centering
\begin{tabular}{clC{1.15cm}C{1.15cm}C{1.15cm}C{1.15cm}cC{1.15cm}C{1.15cm}C{1.15cm}}
\toprule
~ & \bf Strategy & \bf S-P & \bf S-R & \bf S-F1 & \bf Attr. $\mathbf{\rho}$ & \bf Attr. F1 & \bf UAS & \bf LAS & \bf POS \\ \midrule
\multirow{4}*{\rotatebox{90}{\small \textbf{Baseline}}} & LSTM & 89.90 & 85.85 & 87.83 & 0.46 & 60.41 & - & - & - \\
~ & \enspace + SYN & 88.58 & 87.67 & 88.12 & 0.46 & 61.28 & 91.44 & 88.80 & - \\
~ & TFMR  & 90.04 & 87.98 & 89.19 & 0.56 & 67.89 & - & - & - \\
~ & \enspace + SYN  & \bf 91.09 & 89.01 & 90.04 & 0.56 & 66.85 & 92.40 & 89.96 & - \\ \midrule
\multirow{9}*{\rotatebox{90}{\small \textbf{Mine}}} & LSTM & 87.75 & 91.12\dag & 89.79\dag & 0.47\dag & 57.93 & - & - & - \\
~ & \enspace + SYN & 88.82\dag & 92.50\dag & 90.62\dag & 0.46 & 57.34 & 91.71 & 89.10 & 96.29 \\
~ & \enspace + SYN + DA & 90.00 & 93.37 & 91.65 & 0.33 & 49.66 & 92.65 & 90.51 & 96.67 \\
~ & TFMR & 88.34 & 92.90\dag & 90.56\dag & 0.49 & 59.68 & - & - & - \\
~ & \enspace + SYN & 89.28 & 93.56\dag & 91.37\dag & 0.49 & 58.45 & 92.07 & 89.65 & 96.85 \\
~ & \enspace + SYN + DA & 90.15 & 93.49 & 91.79 & 0.42 & 54.27 & \bf 93.03 & 90.91 & 97.10 \\
~ & BERT & 88.90 & 92.77\dag & 90.79\dag & \bf 0.60\dag & 67.02 & - & - & - \\
~ & \enspace + SYN & 89.51 & 94.18\dag & 91.79\dag & 0.59\dag & 65.78 & 92.81\dag & 90.73\dag & \bf 97.18 \\
~ & \enspace + SYN + DA & 90.27 & \bf 94.23 & \bf 92.20 & 0.54 & 63.91 & 92.98 & \bf 90.93 & 97.08 \\ \midrule
\multirow{3}*{\rotatebox{90}{\small \textbf{LLM}}} & PRED & 35.50 & 51.28 & 41.96 & - & - & - & - & - \\
& CASC & 38.13 & 53.26 & 44.44 & - & - & - & - & - \\
& ATTR & - & - & - & - & \bf 80.69 & - & - & - \\
\bottomrule
\end{tabular}
\caption{\label{tab:casmodel}
Main results. ``LSTM'', ``TFMR''(Transformer), ``BERT'' stands for different encoder. We run t-test against the corresponding baseline, and $\dag$ means significantly higher with $>95\%$ confidence. ``+SYN'' means GCN approach in \S\ref{sec:addsyn}, and ``DA'' means the data augmentation method in \S\ref{sec:data-centric}. ``PRED'' and ``CASC'' are Predpatt and Cascade Instruction for semantic relation parsing, respectively, and ``ATTR'' is the semantic attribute parsing, detailed in \S\ref{sec:llm}. Metric abbreviation are detailed in \S\ref{sec:metric}.}
\end{table*}

\section{Experiment and Analysis}

\subsection{Experimental Setup}
\label{sec:expsetup}

\paragraph{Datasets} We conduct experiments on the UDS dataset \citep{UniversalDecompositionalSemantics_2020}, with 10k valid training sentences. For English monolingual data, we use publicly available News Crawl 2021 corpus \citep{ExploitingSourcesideMonolingual_2016, ExploitingMonolingualData_2019a}. In the experiment of the data augmentation method, we first generate the pseudo-targets for all the monolingual data, then filter out the ones that have invalid syntactic and semantic graphs. Finally we randomly select a 100k corpus subset. For ChatGPT generation, we select a 10k corpus subset.
% For random word list, we randomly select words from the distribution of the training source sentences, and randomly select the length of the word list from 5 to 64. 

\paragraph{Model Training} Our model is trained on one NVIDIA A30 Tensor Core GPU with a batch size of 16 and a dropout rate of 0.3. We fix BERT parameters for LSTM and transformer encoders and keep them trainable when BERT itself is the encoder. For the majority of the training process, we set the learning-rate to 2e-4, while for BERT encoder, we set it to 1e-5. For a fair comparison, we use a linear projection of the output of all the encoders to unify the output dimension to 1024. We run each model five times under different seeds in the main table and show the average score. For ChatGPT, we build experiments in the dialog box, using the ChatGPT Mar 23 Version.
% and the fine-turning of the model trained using monolingual data with pseudo-targets, and use cosine schedule with warmup as the learning-rate schedular. 

\paragraph{Baseline} We use \citet{JointUniversalSyntactic_2021} as the baseline. It first employs GloVe word embeddings \citep{GloveGlobalVectors_2014}, character CNN embeddings, and BERT \citep{BERTPretrainingDeep_2019} to generate the context-aware representations of the input sentence. Then, it generates each edge with a decoder in an autoregressive way, following the idea of a pointer-generator network \citep{GetPointSummarization_2017}. After that, it uses a deep biaffine \citep{DeepBiaffineAttention_2017a} graph-based parser to create edges. Node- and edge-level attributes are then predicted after every step, with a multi-layer perception for node attributes and a deep biaffine for edge attributes. Besides, the introduction of syntactic information is preliminarily tried, and we only report their optimal results for each metric.
% where "Encoder-side" and "Concat-after" achieve the best results. 

\subsection{Metrics}
\label{sec:metric}

We follow the setting given by \citet{JointUniversalSyntactic_2021}, detailed as follows.

\paragraph{S-score} 
% Because the output UDS graphs are not lexicalized, evaluating their quality w.r.t. a reference graph becomes an NP-hard graph-alignment problem.
This metric measures performance on the semantic relation prediction task. Following the Smatch metric \citep{SmatchEvaluationMetric_2013a}, which uses a hill-climbing approach to find an approximate graph matching between a reference and predicted graph, S-score \citep{EvaluationPredPattOpen_2017a} provides precision (S-P), recall (S-R), and F1 score (S-F1) for nodes, edges, and attributes. We follow \citet{JointUniversalSyntactic_2021} and evaluate the S-score for nodes and edges only, which evaluates against full UDS arborescences with linearized syntactic subtrees included as children of semantic heads.

\paragraph{Attribute $\mathbf{\rho}$ \& F1} For UDS attributes, we use the pearson correlation $\rho$ (Attr. $\rho$) between the predicted attributes at each node and the gold annotations in the UDS corpus. We also use F1-score (Attr. F1) to measure whether the direction of the attributes matches that of the gold annotations. We binarized the attribute with threshold value $\theta=0$ for gold attributes, and tune $\theta$ for predicted ones per attribute type on validation data. Both of them are obtained under an ``oracle'' setting, where the gold graph structure is provided.

\paragraph{Syntactic Metric} We follow \citet{DependencyParsingMetrics_2015a} to use Unlabeled Attachment Score (UAS) to compute the fraction of words with correctly assigned heads, and Labeled Attachment Score (LAS) to compute the fraction with correct heads and edge types. While for part-of-speech (POS), we simply use the accuracy of prediction.
% We binarize the predicted attributes with a threshold that is tuned on the development set. 


\subsection{Main Results}
\label{sec:expmain}

We conduct experiments on three types of encoders, as demonstrated in Table~\ref{tab:casmodel}.

% Figure environment removed

% Figure environment removed

\paragraph{Our cascade model outperforms the baseline model.} Under basic settings, our best setting (BERT) significantly improves the baseline (TFMR) in S-F1 (+1.60) and Attr. $\rho$ (+0.04), and slightly worse in Attr. F1. The above results are also preserved under +SYN settings (+1.75 and +0.03, respectively). Furthermore, we calculated the total inference time for forward propagation of the two models, averaging on validation and test datasets (about 1.3k sentences). The results are shown in Figure~\ref{fig:time} under logarithmic coordinates. Our model significantly reduces the inference time for all batch sizes (9.56 times faster on average). Finally, using additional data augmentation methods, the S-F1 can be further improved (+2.16), which is also held in LSTM and Transformer (+3.53 and +1.75, respectively). The above results show that our model significantly outperforms the baseline.

% Baseline model introduce pretrained-model information in embedding layer, which we closely follow in our LSTM and Transformer model. And the hyperprameters are mostly keep the same, e.g., encoder layer, dropout, ect. While in our BERT model, we use pretrained-model directly as encoder, and use the default embedding layer, which use less parameters. We conducted experiments on basic setting (only parse semantic and attribute) and SYN setting (add syntactic information). Experiments shows that our model achieve significant better results on LSTM (+2.87 for basic and +2.80 for SYN setting) and best (+1.67 for basic and +1.00 for SYN setting). While due to the small amount of the available corpus, transformer does dot achieve good results in our setting.

\paragraph{Syntactic information and data augmentation methods enhance semantic relation parsing.} Our model primarily focuses on improving the semantic relation parsing, which LLMs are not good at. We summarize the corresponding result in Figure~\ref{fig:semsf}. We can see that both the two approaches can significantly improve relation parsing, with +0.88 for syntactic information and +0.62 for the data augmentation method on average. Besides, the improvements are orthogonal to each other and can be used simultaneously, pushing the results of different models towards a similar limit, since lower-performing models experience greater improvements.

\paragraph{The same methods do not benefit attribute prediction.} However, our proposed methods for further improvements do not consistently improve the attribute parsing. Attributes derive from crowdsourced annotation, which is not closely related to the syntactic or semantic information. Thus, syntactic information cannot provide useful information for attribute prediction, and using more data to pre-train a better model for semantic relation parsing is harmful to the performance of attribute parsing.

% Figure environment removed

\begin{table*}[t]
\small
\centering
\begin{tabular}{clC{1.1cm}C{1.1cm}C{1.1cm}C{1.1cm}cC{1.1cm}C{1.1cm}C{1.1cm}}
\toprule
~ & \bf Strategy & \bf S-P & \bf S-R & \bf S-F1 & \bf Attr. $\mathbf{\rho}$ & \bf Attr. F1 & \bf UAS & \bf LAS & \bf POS \\ \midrule
\multirow{5}*{\rotatebox{90}{\small \textbf{LSTM}}} & Naive & 87.75 & 91.12 & 89.79 & 0.47 & 57.93 & - & - & - \\
& \enspace + Joint & 88.21 & 92.51 & 90.31 & 0.45 & 55.42 & 91.51 & 89.07 & 96.23 \\
~ & \enspace + Attn. & 88.62 & 92.55 & 90.54 & 0.45 & 57.65 & 91.95 & 89.41 & 96.26 \\
~ & \enspace + GCN & 88.82 & 92.50 & 90.62 & 0.46 & 57.34 & 91.71 & 89.10 & 96.29 \\
~ & \enspace + Span & 88.45 & 92.31 & 90.34 & 0.47 & 57.85 & 91.58 & 89.08 & 96.37 \\ \midrule
\multirow{5}*{\rotatebox{90}{\small \textbf{TFMR}}} & Naive & 88.34 & 92.90 & 90.56 & 0.49 & 59.68 & - & - & - \\
~ & \enspace + Joint & 88.64 & 93.53 & 91.02 & 0.51 & 59.56 & 91.99 & 89.42 & 96.60 \\
~ & \enspace + Attn. & 88.82 & 93.46 & 91.08 & 0.49 & 58.86 & 91.84 & 89.35 & 96.77 \\
~ & \enspace + GCN & 89.28 & 93.56 & 91.37 & 0.49 & 58.45 & 92.07 & 89.65 & 96.85 \\
~ & \enspace + Span & 88.85 & 93.19 & 90.97 & 0.50 & 59.46 & 91.60 & 89.29 & 96.71 \\ \midrule
\multirow{5}*{\rotatebox{90}{\small \textbf{BERT}}} & Naive & 88.90 & 92.77 & 90.79 & 0.60 & 67.02 & - & - & - \\
& \enspace + Joint & 88.87 & 93.75 & 91.25 & 0.60 & 67.63 & 92.95 & 90.79 & 97.12 \\
~ & \enspace + Attn. & 89.25 & 94.05 & 91.59 & 0.58 & 66.60 & 92.94 & 90.77 & 97.02 \\
~ & \enspace + GCN & 89.51 & 94.18 & 91.79 & 0.59 & 65.78 & 92.81 & 90.73 & 97.18 \\
~ & \enspace + Span & 88.95 & 93.64 & 91.23 & 0.59 & 65.97 & 92.92 & 90.74 & 97.23 \\
\bottomrule
\end{tabular}
\caption{\label{tab:combinsyn}
The effect of different strategies to incorporate syntactic information. ``Naive'' means no additional syntactic information. ``+Joint'', ``+Attn'', and ``+GCN'' means incorporating syntactic information using joint training, GCN, and attention in \S\ref{sec:addsyn}, separately. ``+Span'' means refine span embeddings using syntactic nodes.}
\end{table*}

\paragraph{ChatGPT performs poorly on relation parsing.} For semantic relation parsing, we use the prompt given in \S\ref{sec:llm} 3 times, which generates 9 different results. We filter out invalid output (no table or table with incorrect headers) and select the best result for each sentence. There are still 11.04\% and 0.37\% of the sentences that do not have correct results for PRED and CASC, respectively, which are filtered out. Despite this favorable setting, it still achieved poor results. Under our observation, the generated relations of LLMs are typically semantically compliant. However, they struggle to follow the instruction step by step, leading to outputs that often do not meet our requirements, and repetitions and incorrect summarizations in the table also commonly occur. As a result, LLMs perform poorly on relation parsing, especially in precision, and complex post-processing constructed by professionals is highly required.
% In our cascade prompt, the results were relatively better (+2.48) and produced more valid responses (+10.67), which proves that our ways to decompose the parsing task consists with the semantics.
% In addition, we try to correct some problems in the post-processing section in Figure~\ref{fig:prompt}, but there is still no guarantee that we can correct these errors.

\paragraph{ChatGPT performs perfectly on attribute parsing.} For semantic attribute parsing, we only run ChatGPT once. 3.03\% of the sentences do not have correct results and are filtered out. Results show that ChatGPT significantly outperforms the small models, achieving a +12.80 increase in Attribute F1 scores compared to the best model. We think that for ChatGPT, which is well-aligned with humans, it is easier to predict the attributes given by human annotators rather than the long logical chain reasoning task. In addition, only need to predict positive and negative without considering the pearson correlation is also one of its advantages. For further verification, we calculate the Attribute F1 scores for all attributes in Figure~\ref{fig:chatgpt}. We can observe that ChatGPT performs well on most of the attributes when compared to our model, with 60.34\% and 25.86\% of the attributes respectively having F1 scores above 85\%. Furthermore, ChatGPT performs perfectly on word-sense attributes, achieving an F1 score of 86.99. In contrast, our models do not display significantly superior results, with an F1 score of 68.49. We believe that with more detailed guidance and rigorous post-processing, LLMs have the potential to replace humans in annotation tasks.
% Here, we use the most simple way to add syntactic information, i.e., share encoder without other combination, and for baseline, we use their best setting. Experiments shows that for S-F1, adding syntactic information gives +1.02 (baseline) and +0.60 (Mine) improvement on average, which shows its efficacy. While for attribute, no consistent improvements are found. The detailed relation of those information will be explored in the next section.

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccc}
% \toprule
% \bf Model & In domain & Predpatt & \bf S-P & \bf S-R & \bf S-F1 & $\bf \Delta$ & UAS & LAS & POS \\ \midrule
% SYN Teacher & - & - & - & & 93.24 & 91.14 & 97.54 \\
% UDS Teacher & 88.87 & 93.75 & 91.25 & - & 92.95 & 90.79 & 97.12 \\ \midrule
% \enspace + Mono. with Distill & 89.34 & 94.05 & 91.63 & +0.38 & 93.05 & 91.01 & 97.10 \\
% \enspace + Mono. with Predpatt & 90.15 & 94.28 & 92.17 & +0.92 & 93.26 & 91.28 & 97.17 \\
% \enspace + Selec. with Distill & 89.08 & 94.03 & 91.49 & +0.24 & 92.62 & 90.52 & 97.19 \\
% \enspace + Selec. with Predpatt & 89.56 & 94.37 & 91.90 & +0.65 & 92.88 & 90.91 & 97.22 \\
% \enspace + Mono. with Stanza & 89.24 & 94.12 & 91.62 & +0.37 & 92.96 & 90.86 & 97.27 \\
% \enspace + Selec. with Stanza & 89.69 & 94.24 & 91.90 & +0.65 & 92.76 & 90.74 & 97.10 \\
% \enspace + Rand. with ChatGPT & 88.25 & 93.28 & 90.70 & -0.55 & 92.38 & 90.28 & 96.99 \\
% \bottomrule
% \end{tabular}
% \caption{\label{tab:datacentric}
% The effect of different data augmentation approaches. Since our proposed strategy and baseline belong to the same policy, there is almost no difference in latency. Therefore, we display the results in the form of table to highlight the details of the improvement in translation quality. Improvements against random sampling ``Random'' are in column \textbf{$\mathbf{\Delta}$}. Here, ``Distill'' demonstrate the baseline of data-centric approach, and ``Rand.'' demonstrate the zero-shot settings.}
% \end{table*}

\subsection{Exploration on Syntactic Information}
\label{sec:expsyn}

We conduct experiments on different ways to join syntactic information into the model, and the results are shown in Table~\ref{tab:combinsyn}.

\paragraph{Syntactic information enhances semantic relation parsing.} Our experiments show consistent improvements in S-F1 scores across different methods of integrating syntactic information, with +0.48 for +SYN, +0.69 for Contact, and +0.88 for GCN, which is also used as our default settings. However, because of the different syntactic foundations arising from different annotation methods, we do not observe a consistent trend of attribute parsing results, aligned with the findings in \citet{JointUniversalSyntactic_2021}. 

\paragraph{Incorporating child syntactic information has less impact on the results.} We tried to use a better span representation, which uses a self-attention over all words in the span, instead of using only the representation of the center word. However, the attribute prediction does not achieve consistent improvements. This shows that the center word can well represent the semantics of the whole span, and is the default setting in our experiments.

\begin{table*}[t]
\small
\centering
\begin{tabular}{lcccC{1.05cm}C{1.05cm}C{1.05cm}C{1.0cm}C{1.05cm}C{1.05cm}C{1.05cm}}
\toprule
\bf Model & \bf In domain & \bf Predpatt & \bf S-P & \bf S-R & \bf S-F1 & $\bf \Delta$ & \bf UAS & \bf LAS & \bf POS \\ \midrule
Ours & $\times$ & $\times$ & 89.34 & 94.05 & 91.63 & +0.38 & 93.05 & 91.01 & 97.10 \\
Ours & $\times$ & $\checkmark$ & 90.15 & 94.28 & 92.17 & +0.92 & 93.26 & 91.28 & 97.17 \\
Ours & $\checkmark$ & $\times$ & 89.08 & 94.03 & 91.49 & +0.24 & 92.62 & 90.52 & 97.19 \\
Ours & $\checkmark$ & $\checkmark$ & 89.56 & 94.37 & 91.90 & +0.65 & 92.88 & 90.91 & 97.22 \\
Stanza & $\times$ & $\checkmark$ & 89.24 & 94.12 & 91.62 & +0.37 & 92.96 & 90.86 & 97.27 \\
Stanza & $\checkmark$ & $\checkmark$ & 89.69 & 94.24 & 91.90 & +0.65 & 92.76 & 90.74 & 97.10 \\
ChatGPT & $\times$ & $\checkmark$ & 88.25 & 93.28 & 90.70 & -0.55 & 92.38 & 90.28 & 96.99 \\ \midrule
\multicolumn{3}{l}{Syntactic Teacher} & - & - & - & & 93.24 & 91.14 & 97.54 \\
\multicolumn{3}{l}{Semantic Relation Teacher} & 88.87 & 93.75 & 91.25 & - & 92.95 & 90.79 & 97.12 \\
\bottomrule
\end{tabular}
\vspace{-2pt}
\caption{\label{tab:datacentric}
The effect of different data augmentation approaches. ``Model'' means which teacher model to use, ``In domain'' means whether to select data with closer domain, and ``Predpatt'' means whether to use an external tool or simply use the distillation method. ``Syntactic Teacher'' is trained only on syntactic targets, while ``Semantic Relation Teacher'' on syntactic and semantic relation targets. Both only use multi-task learning methods.}
\vspace{-8pt}
\end{table*}
% For comparison with other external models,  and  are trained with the basic multi-task learning method with no additional tricks, with the former 

\subsection{Exploration on Data Augmentation}
\label{sec:expdata}

We conduct experiments using the data augmentation method under the basic multi-task training method to incorporate syntactic information, and the results are shown in Table~\ref{tab:datacentric}.

\paragraph{Data augmentation significantly improves the semantic parsing.} Under different ways to incorporate syntactic information, the S-F1 consistently improves, with +0.54 on average and +0.92 for best settings (ours w/o in domain w/ predpatt), which is used as the default data augmentation method. Besides, our proposed ways to better utilized the external tool also significantly outperforms the basic distillation settings, i.e., +0.48 on average, which shows the efficacy of our methods.
% In this experiment, we test the efficacy of the combination approach in \S\ref{sec:addsyn} and data-centric approach in \S\ref{sec:data-centric}. While the former do not see a consistent improvement, data-centric approach with the help of predpatt and ChatGPT both shows a significant improvement, compared with the strong baseline of using distillation (+0.64 for predpatt and +1w for ChatGPT). But for predpatt, restrictions still remains: 1) suitable external tools that express the relation between semantic and syntactic information is highly needed, 2) monolingual data is needed, and act poorly under the zero-shot settings (through better than the distillation setting), and 3) external syntactic model is needed, which is a cost of time and computional resources.

\paragraph{How does the in-domain unlabeled-data act?} We are also curious about how the domain of the datasets influences the results. We follow the idea of \citet{IntelligentSelectionLanguage_2010a} to score the unlabeled data by the difference between the score of the in-domain language model and the language model trained from which the unlabeled data is drawn. We refer the reader to the original paper for further details. Results have shown that for our larger models with better generalization, the in-domain data hurt the performance (-0.27). For smaller model given in Stanza, the in-domain data performs better (+0.28), while both are worse than the results with our models. This shows that the performance of the teacher model is important, and for models with good generalization, always using in-domain data is not a good choice.
\vspace{12pt}
% Figure environment removed

\paragraph{How does zero-shot setting with ChatGPT act?} LLMs do not perform well in semantic relation generation, and directly using external tools to assist the test set is not effective \citep{UniversalDecompositionalSemantic_2020}, so it is natural to think of using LLM to augment the data in a similar way. We used the zero-shot settings, detailed in \S\ref{sec:llm}. However, the performance has declined. For further analysis, we propose the training loss for the first 3k updates for different models in Figure~\ref{fig:loss}. We can see that our data augmentation method can significantly lower the initial training loss, which shows that similar data distribution is shared between our proposed pseudo-labeled data and the training data. However, the initial loss of ChatGPT argumentation is even higher than random initializing (Ours +SYN). This shows significant distribution shifts for the data generated by ChatGPT, which shows earge need for more detailed prompts and ways to select properly generated data.

% ChatGPT has suddenly become a research hotspot in the field of natural language generation (NLG) recently. Here we try to use the generation ability of ChatGPT to improve the effect of natural language understanding (NLU) tasks. In response to the three problems of the ``Mono. with Predpatt'', ChatGPT can be easily utilized to avoid them. Instead of directly using random word list as source sentence, we use it as a part of the prompt for ChatGPT generation, which ensure the diversity of generation. Then, we let ChatGPT to directly generate the syntactic information, which liberate the training and inference costs of external syntactic models, while fully utilize the reasoning ability and the large-scale multilingual syntactic information included in ChatGPT.
% Detailly, we use the spicific prompt as shown in Figure~\ref{fig:prompt}, and since the prompt is entirely in natural language, our experiment setup can also be easily read out from it. We want ChatGPT to generate a fluent sentence with its corresponding syntactic information (Universal Dependencies), where ChatGPT demonstrate amazing strength. We generate using the default GPT-3.5, then filter out those with illigal dependency trees, finally get a dataset with 20k sentences. We use this dataset for pretraining, and finetune with the same setting with other data-centric approach. The experiments shown in Table~\ref{tab:datacentric} prove that we can achieve appealing improvements in the zero-shot settings.



% \subsection{Is Semantic and Attribute Helpful?}

% In the sections before, experiments shows different trend for semantic graph parsing and attribute parsing. Here, we construct additional experiments to discuss whether these informations are helpful or harmful to each others.

% \paragraph{downstream task do not help much to syntactic parsing.}

% \paragraph{using finetune format seems more helpful than multi-task training.} we use the model without SYN and finetune SYN afterwards.

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{clC{1.15cm}C{1.15cm}C{1.15cm}C{1.15cm}cC{1.15cm}C{1.15cm}C{1.15cm}}
% \toprule
% ~ & \bf Strategy & S-P & S-R & S-F1 & Attr. $\rho$ & Attr. F1 & \bf UAS & \bf LAS & \bf POS \\ \midrule
% \multirow{5}*{\rotatebox{90}{\small \textbf{LSTM}}} & Baseline & 88.21 & 92.51 & 90.31 & 0.45 & 55.42 & 91.51 & 89.07 & 96.23 \\
% ~ & \enspace - POS & 88.04 & 92.71 & 90.32 & 0.45 & 57.00 & 91.26 & 88.60 & - \\
% ~ & \enspace - SYN & 87.97 & 92.15 & 90.01 & 0.47 & 56.44 & - & - & 96.25 \\
% ~ & \enspace - UDS & - & - & - & 0.50 & 61.22 & 91.34 & 88.62 & 96.71 \\
% ~ & \enspace - Attr. & 87.06 & 92.14 & 89.53 & - & - & 91.71 & 89.11 & 96.29 \\ \midrule
% \multirow{5}*{\rotatebox{90}{\small \textbf{TFMR}}} & Baseline & 88.63 & 93.53 & 91.02 & 0.51 & 59.56 & 91.99 & 89.42 & 96.60 \\
% ~ & \enspace - POS & 88.26 & 93.08 & 90.79 & 0.50 & 58.79 & 91.95 & 89.47 & - \\
% ~ & \enspace - SYN & 88.11 & 92.66 & 90.33 & 0.52 & 60.87 & - & - & 96.47 \\
% ~ & \enspace - UDS & - & - & - & 0.49 & 60.07 & 91.91 & 89.36 & 96.90 \\
% ~ & \enspace - Attr. & 87.28 & 91.77 & 89.47 & - & - & 91.89 & 89.36 & 96.64 \\ \midrule
% \multirow{5}*{\rotatebox{90}{\small \textbf{BERT}}} & Baseline & 88.87 & 93.75 & 91.25 & 0.60 & 67.63 & 92.95 & 90.79 & 97.12 \\
% ~ & \enspace - POS & 88.98 & 93.84 & 91.35 & 0.59 & 67.81 & 93.03 & 90.91 & - \\
% ~ & \enspace - SYN & 88.78 & 93.38 & 91.02 & 0.60 & 66.35 & - & - & 97.04 \\
% ~ & \enspace - UDS & - & - & - & 0.61 & 67.23 & 92.79 & 90.60 & 97.51 \\
% ~ & \enspace - Attr. & 88.72 & 93.49 & 91.04 & - & - & 92.96 & 90.86 & 97.21 \\ 
% \bottomrule
% \end{tabular}
% \caption{\label{tab:sampling_strategies}
% The effect of different sampling strategies. Since our proposed strategy and baseline belong to the same policy, there is almost no difference in latency. Therefore, we display the results in the form of table to highlight the details of the improvement in translation quality. Improvements against random sampling ``Random'' are in column \textbf{$\mathbf{\Delta}$}.}
% \end{table*}

\section{Conclusion}

In this paper, we conduct a holistic exploration of the Universal Decompositional Semantic (UDS) Parsing. We propose a cascade model and ways to better incorporate syntactic information, which both outperform the baseline, while significantly improving the parallelism and reducing inference time. Data augmentation methods are also detailly explored. Finally, ChatGPT performs poorly in relation parsing and data augmentation, but well in attribute parsing, which shows potential for dataset annotation in place of humans. % We hope to inspire further research in the field of natural language understanding and computational semantics, especially under LLMs.
% In this paper, we proposed a cascade model for Universal Decompositional Semantics (UDS) parsing that decomposes the parsing task into multiple subtasks in a semantically appropriate manner. We show that our approach improve the performance over previous models, while enhances parallelism and significantly reduces inference time. Additionally, we incorporated syntactic information by multi-task training and proposed methods for deeper incorporation, including the GCN approach and attention mechanism. Finally, our data augmentation method, which further exploits the capabilities of external tool PredPatt, also achieved considerable improvements without introducing any labeled data. Besides, we also investigated the performance of large language models (LLMs) on the UDS task under zero-shot settings. We found that LLMs performed well on semantic attribute parsing but poorly on semantic relation parsing. In summary, we propose a significantly better baseline for UDS parsing, and make a primilary step towards solving this problem with the help of LLMs. We hope to inspire further research in the field of natural language understanding and computational semantics.

\bibliography{tacl2021v1}
\bibliographystyle{acl_natbib}

\end{document}
