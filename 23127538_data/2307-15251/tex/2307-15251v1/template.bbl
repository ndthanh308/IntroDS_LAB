% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{ref1}
J.~Li, L.~Deng, Y.~Gong, and R.~Haeb-Umbach, ``An overview of noise-robust
  automatic speech recognition,'' \emph{IEEE/ACM Transactions on Audio, Speech,
  and Language Processing}, vol.~22, no.~4, pp. 745--777, 2014.

\bibitem{ref2}
K.~A~Al-Karawi, A.~H~Al-Noori, F.~F. Li, T.~Ritchings \emph{et~al.},
  ``Automatic speaker recognition system in adverse conditionsâ€”implication of
  noise and reverberation on system performance,'' \emph{International Journal
  of Information and Electronics Engineering}, vol.~5, no.~6, pp. 423--427,
  2015.

\bibitem{ref3}
B.~Edwards, ``The future of hearing aid technology,'' \emph{Trends in
  amplification}, vol.~11, no.~1, pp. 31--45, 2007.

\bibitem{ref4}
Y.~Xu, J.~Du, L.-R. Dai, and C.-H. Lee, ``An experimental study on speech
  enhancement based on deep neural networks,'' \emph{IEEE Signal processing
  letters}, vol.~21, no.~1, pp. 65--68, 2013.

\bibitem{ref5}
X.~Xu, W.~Tu, and Y.~Yang, ``{Selector-Enhancer}: Learning dynamic selection of
  local and non-local attention operation for speech enhancement,''
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 2023.

\bibitem{ref6}
Y.~Wang, A.~Narayanan, and D.~Wang, ``On training targets for supervised speech
  separation,'' \emph{IEEE/ACM transactions on audio, speech, and language
  processing}, vol.~22, no.~12, pp. 1849--1858, 2014.

\bibitem{ref7}
S.~Routray and Q.~Mao, ``Phase sensitive masking-based single channel speech
  enhancement using conditional generative adversarial network,''
  \emph{Computer Speech \& Language}, vol.~71, p. 101270, 2022.

\bibitem{ref8}
S.~R. Park and J.~W. Lee, ``A fully convolutional neural network for speech
  enhancement,'' \emph{Proc. Interspeech 2017}, pp. 1993--1997, 2017.

\bibitem{ref9}
Y.~Bengio, P.~Simard, and P.~Frasconi, ``Learning long-term dependencies with
  gradient descent is difficult,'' \emph{IEEE transactions on neural networks},
  vol.~5, no.~2, pp. 157--166, 1994.

\bibitem{ref10}
D.~B. Pisoni, ``Long-term memory in speech perception: Some new findings on
  talker variability, speaking rate and perceptual learning,'' \emph{Speech
  communication}, vol.~13, no. 1-2, pp. 109--125, 1993.

\bibitem{ref11}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{ref12}
X.~Wang, R.~Girshick, A.~Gupta, and K.~He, ``Non-local neural networks,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2018, pp. 7794--7803.

\bibitem{ref13}
S.~Khan, M.~Naseer, M.~Hayat, S.~W. Zamir, F.~S. Khan, and M.~Shah,
  ``Transformers in vision: A survey,'' \emph{ACM Computing Surveys (CSUR)},
  2021.

\bibitem{ref14}
X.~Xu and J.~Hao, ``{U-Former}: Improving monaural speech enhancement with
  multi-head self and cross attention,'' in \emph{2022 26th International
  Conference on Pattern Recognition (ICPR)}, 2022, pp. 663--369.

\bibitem{ref15}
X.~Xu, Y.~Wang, J.~Jia, B.~Chen, and J.~Hao, ``{GLD-Net: Improving Monaural
  Speech Enhancement by Learning Global and Local Dependency Features with GLD
  Block},'' in \emph{Proc. Interspeech 2022}, 2022, pp. 966--970.

\bibitem{ref16}
G.~Yu, A.~Li, C.~Zheng, Y.~Guo, Y.~Wang, and H.~Wang, ``Dual-branch
  attention-in-attention transformer for single-channel speech enhancement,''
  in \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2022, pp. 7847--7851.

\bibitem{ref17}
Y.~Luo and N.~Mesgarani, ``Conv-tasnet: Surpassing ideal time-frequency
  magnitude masking for speech separation,'' \emph{IEEE/ACM transactions on
  audio, speech, and language processing}, vol.~27, no.~8, pp. 1256--1266,
  2019.

\bibitem{ref18}
A.~Pandey and D.~Wang, ``Densely connected neural network with dilated
  convolutions for real-time speech enhancement in the time domain,'' in
  \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2020, pp. 6629--6633.

\bibitem{ref19}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Delving deep into rectifiers: Surpassing
  human-level performance on imagenet classification,'' in \emph{Proceedings of
  the IEEE international conference on computer vision}, 2015, pp. 1026--1034.

\bibitem{ref20}
W.~Shi, J.~Caballero, F.~Husz{\'a}r, J.~Totz, A.~P. Aitken, R.~Bishop,
  D.~Rueckert, and Z.~Wang, ``Real-time single image and video super-resolution
  using an efficient sub-pixel convolutional neural network,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2016, pp. 1874--1883.

\bibitem{ref21}
M.~Sperber, J.~Niehues, G.~Neubig, S.~St{\"u}ker, and A.~Waibel,
  ``Self-attentional acoustic models,'' \emph{Proc. Interspeech 2018}, pp.
  3723--3727, 2018.

\bibitem{ref22}
L.~Zhang, Z.~Shi, J.~Han, A.~Shi, and D.~Ma, ``Furcanext: End-to-end monaural
  speech separation with dynamic gated dilated temporal convolutional
  networks,'' in \emph{MultiMedia Modeling: 26th International Conference, MMM
  2020, Daejeon, South Korea, January 5--8, 2020, Proceedings, Part I
  26}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020, pp. 653--665.

\bibitem{ref23}
K.~Tan and D.~Wang, ``Learning complex spectral mapping with gated
  convolutional recurrent networks for monaural speech enhancement,''
  \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  vol.~28, pp. 380--390, 2019.

\bibitem{ref24}
J.~Hu, L.~Shen, and G.~Sun, ``Squeeze-and-excitation networks,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2018, pp. 7132--7141.

\bibitem{ref25}
X.~Li, W.~Wang, X.~Hu, and J.~Yang, ``Selective kernel networks,'' in
  \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern
  recognition}, 2019, pp. 510--519.

\bibitem{ref26}
X.~Ding, Y.~Guo, G.~Ding, and J.~Han, ``Acnet: Strengthening the kernel
  skeletons for powerful cnn via asymmetric convolution blocks,'' in
  \emph{Proceedings of the IEEE/CVF international conference on computer
  vision}, 2019, pp. 1911--1920.

\bibitem{ref26.5}
K.~Jiang, Z.~Wang, C.~Chen, Z.~Wang, L.~Cui, and C.-W. Lin, ``{Magic ELF: Image
  Deraining Meets Association Learning and Transformer},'' in \emph{Proceedings
  of the 30th ACM International Conference on Multimedia}, 2022, pp. 827--836.

\bibitem{ref27}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``Librispeech: an asr corpus
  based on public domain audio books,'' in \emph{2015 IEEE international
  conference on acoustics, speech and signal processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2015, pp. 5206--5210.

\bibitem{ref28}
J.~Thiemann, N.~Ito, and E.~Vincent, ``The diverse environments multi-channel
  acoustic noise database ({DEMAND}): A database of multichannel environmental
  noise recordings,'' in \emph{Proceedings of Meetings on Acoustics ICA2013},
  vol.~19, no.~1.\hskip 1em plus 0.5em minus 0.4em\relax Acoustical Society of
  America, 2013, p. 035081.

\bibitem{ref29}
K.~Wang, B.~He, and W.-P. Zhu, ``{TSTNN: Two-stage transformer based neural
  network for speech enhancement in the time domain},'' in \emph{ICASSP
  2021-2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  7098--7102.

\end{thebibliography}
