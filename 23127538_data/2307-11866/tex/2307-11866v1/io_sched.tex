\section{\as{io} Scheduling}\label{sec:io_sched}
Given that flash storage has the capabilities to achieve single digit $\mu$-second latency, whereas overheads in the software stack, such as context switching in the Kernel caused by system calls, can already require $\mu$-seconds to complete~\cite{soares2010flexsc}, making software the dominating factor in overheads~\cite{2010-Caulfield-Moneta,2012-Caulfield-Fast_User_Space_Access,foong2010towards,seppanen2010high,vasudevan2012using}. In addition, \textit{interrupts} cause significant overheads for systems. Aimed at slow storage devices, the \as{io} request is submitted to the device, the context is switched, such that the process can continue with other work, and upon completion of the \as{io}, the host is interrupted, and the context is switched again. Any added interrupt on the \as{io} path can cause significant delays~\cite{2014-Shin-OS_IO_Path}. Cache effects are another drawback of context switching, since other work is continued, replacing data in the caches, it requires bringing the replaced data back into the caches after the interrupt and resuming of the prior context. Similarly, it also causes \af{tlb} pollution on the host system. A different approach to submitting \as{io} requests is with \textit{polling}, which eliminates the need for context switches. With polling, the \as{io} request is submitted and instead of continuing other work, the process regularly checks the \as{io} for completion. Using polling for \as{io} requests has been shown to be a favored method of building application for fast storage devices~\cite{kourtis2019reaping,yang2012poll,didona2022understanding}.

Modern systems largely rely on \textit{asynchronous \as{io}} over \textit{synchronous \as{io}}. With synchronous \as{io}, a process submits a single \as{io} request and waits for its completion. In order to saturate the storage device performance, additional threads are required, which submit \as{io} requests to the numerous \as{io} queues in the of the storage device. However, this mechanism does not scale efficiently, where each thread must wait until the \as{io} request is completed. Therefore, with asynchronous \as{io} the threads do not wait for completion, but instead submit a larger number of \as{io} requests each, allowing to fill the device \as{io} queues more effectively. The \as{io} requests for which a thread as submitted a request, but have not completed, are referred to as \textit{outstanding} or \textit{in-flight} \as{io} requests. \cref{tab:io_sched} shows the mechanisms for host systems to better leverage the flash storage performance and minimize overheads. In addition to file systems implementing particular mechanisms, we discuss more general methods applicable to all applications for benefiting from flash storage and enhancing performance.

\begin{table}[!t]
    \centering
    % Update reference format to fit table into column
    \crefformat{section}{\S#2#1#3}
    \crefformat{subsection}{\S#2#1#3}
    \crefformat{subsubsection}{\S#2#1#3}
    \begin{tabular}{||p{40mm}|p{35mm}||}
        \hline 
        Mechanism & File Systems \\
        \hline
        \hline
        \as{io} Operations (\cref{sec:io_ops}) & \cite{2020-Tu-URFS} \\
        \hline
        \as{io} Scheduler (\cref{sec:io_scheduler}) & \cite{2021-Qin-Atomic_Writes} \\
        \hline
        \as{io} Path - User-Space File Systems & \cite{2020-Tu-URFS,2019-Yoshimura-EvFS,2018-Kannan-DevFS,2019-Liu-fs_as_process} \\
        \hline
    \end{tabular}
    % Reset reference format to before
    \crefformat{section}{Section #2#1#3}
    \crefformat{subsection}{Section #2#1#3}
    \crefformat{subsubsection}{Section #2#1#3}
    \caption{\as{io} scheduling mechanisms to exploit performance capabilities of flash storage, and the respective file systems that implement a particular mechanism.}
    \label{tab:io_sched}
\end{table}

\subsection{\as{io} Operations}\label{sec:io_ops}
Given that particular \as{io} patterns can have degrading affects on the \as{ssd} performance, such as mixing read and write operations, as they share resources on the device, including the mapping table and \as{ecc} engine, and furthermore possibly invalidating the cached data in the \as{ssd} \as{ram}. Similarly, mixing \as{io} operations with different block sizes can result in increased fragmentation~\cite{2020-Tu-URFS}. As the Linux Kernel relies on a submission and completion queue for \as{io}, user-space frameworks such as \as{spdk} and NVMeDirect provide more flexibility for user-space file systems to design different queues, depending on the requirements. URFS~\cite{2020-Tu-URFS} utilizes this possibility to create adaptive queues that can better optimize \as{io} submissions to the device. Based on the workload characteristics URFS dynamically creates flexible \as{io} queues (e.g., group by size, read/write operation) to increase \as{ssd} performance. Similarly, Borge et al.~\cite{2019-Borge-SSD_read_variability} show with a case study on HDFS performance with \as{ssd}, that in order to leverage the capabilities of flash \as{ssd}, direct \as{io}, and increased parallel requests with buffered \as{io} are needed. 

\subsection{\as{io} Scheduler}\label{sec:io_scheduler}
With the possibility for asynchronous \as{io} to merge and reorder requests, the Linux Kernel implements several schedulers, such as \textit{NOOP}, \textit{deadline}, and \textit{CFQ}~\cite{sun2014exploring,pratt2004workload,moallem2008study,heger2010linux}. NOOP being the least intrusive scheduler only merges \as{io} request, but does not reorder them, which is beneficial on devices such as \as{ocssd}, that require consecutive LBAs. Son et al.~\cite{2015-Son-Optimizing_FS} showcase the benefits of merging random write requests, regardless of contiguity of the LBAs, in order to better enhance performance with fast storage devices. The deadline scheduler adds to NOOP by utilizing merging and reordering, however also applies a deadline for each \as{io} request to ensure requests are submitted to the device eventually. Two separate queues, one for read requests and an additional one for write requests are utilized, which are both ordered by the deadline of the request. Another scheduler variant of deadline exists, called \textit{mq-deadline}, which is aimed at multi-queue devices, such as \as{nvme} \as{ssd}. \af{cfq} implements a round-robin based queue that assigns time slices to each in order to prevent starvation and provide fairness. While these are the common schedulers in the Linux Kernel, to see details on all schedulers present in the Linux Kernel consult~\cite{2019-ubuntu_wiki_io_schedulers}. Such scheduling configuration begs the question on which scheduler is best suited for file systems on flash storage. Several studies into performance of the schedulers exist~\cite{sun2014exploring,yu2014optimizing}, showcasing that merging of read \as{io} requests in synchronous \as{io} provides beneficial performance gains, and similarly the merging of write \as{io}s in asynchronous \as{io} shows performance gains.

Qin et al.~\cite{2021-Qin-Atomic_Writes} argue that \as{io} ordering limits exploiting the parallelism of flash devices. Especially as the Linux block layer does not guarantee particular ordering, flags such as \af{fua}, indicating that \as{io} completion is signaled upon arrival of data on the storage, and \textit{PREFLUSH}, which before completing the request flushes the volatile caches, have to be set in order to ensure a specific ordering~\cite{2021-Qin-Atomic_Writes}. With file systems, the \as{io} of metadata and data has a particular ordering, such that metadata can only point to data that exists, needing to ensure that data is written prior to metadata. Removing of \as{io} ordering allows eliminating this need and better utilize the flash parallelism. Utilizing the \as{oob} area on flash pages, the file system developed by Qin et al., called NBFS, maintains versioning in order to identify out of order updates. Furthermore, updates are done using atomic writes (discussed in Section~\ref{sec:failure_consistency}). The issuing of \as{fua} requests further implies that its \as{io}s cannot be merged in the scheduler~\cite{2021-Qin-Atomic_Writes}, implying that if a smaller than flash page size \as{fua} \as{io} request is issued, it is padded to the page size, causing \as{wa}. NBFS solves this with its atomic writes that imply that the \as{fua} request does not immediately have to be written to the flash, but instead wait for all data blocks to arrive, which are then used to fill the pages, allowing to reduce the \as{wa} (solving \textbf{\as{fic}3}).

\subsection{\as{io} Path - User-Space File Systems}\label{sec:user_space_fs}
A mechanism that is gaining significant attention in the research community is the utilization of user-space file systems, bypassing the Kernel layers and avoid its associated overheads. These file systems run only in the user-space, as opposed to commonly used file systems (e.g., F2FS) running in Kernel space. In addition to the benefit of avoiding Kernel overheads, user-space file systems are easier to develop, have increased reliability and security by avoiding Kernel bugs and vulnerabilities, and provide increased portability~\cite{2015-Tarasov-User_space_fs_practicality}. A widely adopted framework for building user-space storage applications is \af{fuse}~\cite{szeredi2010fuse}. It is implemented over a Kernel module with which it exports a virtual file system from the Kernel, where data and metadata are provided by a user-space process, hence allowing user-space applications to interact with it. Since \as{fuse} is implemented with a Kernel module, \as{fuse} based file systems suffer significant performance penalties, requiring more \as{cpu} cycles than file systems in Kernel space. Particularly contributing to overheads is the need to copy memory between user-space and kernel-space, caused by the way \as{fuse} handles \as{io} requests~\cite{2019-Vangoor-Fuse_performance,vangoor2017fuse}. Furthermore, \as{fuse} still suffers from context switching~\cite{vangoor2017fuse,2019-Vangoor-Fuse_performance,rajgarhia2010performance} overheads and \af{ipc} between the \as{fuse} kernel module and \as{fuse} user-space daemon~\cite{zhu2018direct}. 

A similar framework for building user-space applications with direct storage access is \as{nvme}Direct~\cite{2016-Kim-NVMe_Direct}. However, it also relies on a Kernel driver to provide enhanced \as{io} policies. \as{spdk}~\cite{2017-Yang-SPDK} is another framework for building user-space storage applications, however it provides the mechanisms to bypass the Kernel and submit \as{io} directly to the device, by implementing a user-space driver for the storage device. Such a framework allows building high performance storage applications in user-space, which eliminate the overheads coming from the Kernel \as{io} stack. 

URFS~\cite{2020-Tu-URFS} provides increased concurrency performance by implementing a multi-process shared cache in memory, in order to avoid the Kernel overheads of copying data as is present in \as{fuse}. It furthermore helps avoid contention on the storage device. Eliminating of data copy is also addressed in ZUFS (zero-copy user-mode file system)~\cite{2019-Harrosh-zufs}, which is a
user-space file system for persistent memory, which completes \as{io} requests by requesting exact data locations instead of copying data into the own address space. A similar user-space file system that implements a shared cache for process is EvFS~\cite{2019-Yoshimura-EvFS}, which is \as{spdk}-based. While this file system can also support multiple readers/writers in the page cache, it only supports these for a single user process. User-space frameworks often provide capabilities to either expose the storage device as a block device, which the user-space application then accesses, or build a custom block device module (e.g., with \as{spdk}, which also has default driver modules such as \as{nvme}). For \as{nvme} devices that support \as{nvme} controller memory buffer management, the file system can manage parts of the device memory. DevFS~\cite{2018-Kannan-DevFS} utilize such an integration to manage the device memory for file metadata and \as{io} queues.

Different from prior discussed development frameworks, \af{fsp}~\cite{2019-Liu-fs_as_process} provides a storage architecture design for user-space file systems. The emphasis of \as{fsp} is to scale with the arrival of faster storage, and similarly to other user-space frameworks, minimize the software stack. For this it bases development on running file systems as processes, providing safer metadata integrity, data sharing coordination, and consistency control, as the process running the file system is in control of everything, instead of trusting libraries. Furthermore, \as{fsp} relies on \as{ipc} for fast communication, which unlike FUSE has a low overhead since it does not require context switching. Inter-core message passing comes at a low overhead and cache-to-cache transfers on multi-core systems can complete in double-digit cycles~\cite{soares2010flexsc}. DashFS~\cite{2019-Liu-fs_as_process} is built with \as{fsp}, providing a safe user-space file system with isolation of different user processes, and efficient \as{ipc}.
