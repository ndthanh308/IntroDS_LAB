\section{Challenges of Flash Storage Integration}\label{sec:flash_challenges} 
While \as{ssd} uses the same block interface that is used with \as{hdd}, flash has different characteristics that software must account for to better integrate flash storage. This section details the challenges that arise from integrating flash storage into systems, providing the guidelines along which we dictate the bottom up view of changes in the host storage software stack, up to the file system. We define the challenges to account for the characteristics of flash storage, as well as enhance its integration into host systems. In the case of \as{ssd} devices, these challenges often largely depend on the underlying \as{ftl}, as it is making the final decision, independent of what data placement the host implements, however aiding the \as{ftl} can increase the performance. Embedded devices provide a higher level of host data placement by eliminating the \as{ftl} and directly attaching flash chips to the motherboard. Each challenge is assigned a specific identifier with the \af{fic}, in order to refer back to the specific challenge throughout this literature review.

\noindent \textbf{\as{fic}1: Asymmetric Read and Write Performance.} On flash storage write operations require more time than read operations~\cite{stoica2009evaluating,2022-intel-p,chen2009understanding}, making it important for software to limit write operations. Particularly, frequent small writes that are smaller than the allocation unit, referred to as \textit{microwrites} incur significant performance penalties, and should be avoided where possible.

\noindent \textbf{\as{fic}2: Garbage Collection (GC).} While the \as{ftl} hides the flash access constraints from host applications, providing seemingly in-place data updates, it adds the cost of performing garbage collection to free up space. \as{gc} overheads have unpredictable performance penalties for the host system~\cite{2015-Kim-SLO_complying_ssds,2014-yang-dont_stack_log_on_log}, resulting in large tail latency~\cite{2013-Dean-tail_at_scale}. Dealing with, and aiming to minimize required garbage collection for the flash device is a key challenge in integrating flash storage.

\noindent \textbf{\as{fic}3: \as{io} Amplification.} Due to the characteristics of flash avoiding in-place updates of flash pages, writes often encounter \af{wa}. With this the amount of data that is written on the flash storage is larger than the write that is issued by the host system. For example a 4KiB issued write may increase to 16KiB being written on the device, due to possible garbage collection requiring to copy data, resulting in a \as{wa} factor of 4x. \as{wa} furthermore adds to an increase in wear on the flash cells~\cite{2013-Lu-SSD_WA_Lifetime}. \af{ra} similarly is caused by requiring to read a larger amount of data than is issued in the read \as{io} request. \as{ra} most commonly happens when reading metadata in order to locate data, thus requiring an additional read of metadata on top of the request read \as{io} request for the data. This is most often inevitable, as all data requires metadata for management, however this should be kept to a minimum at application-level. Furthermore, minimization of \as{wa} is more important than \as{ra}, since write requests have a higher latency than read requests, and writing has a more significant impact on the flash storage, resulting in increased flash wear. While read requests also incur wear on the flash cell, called read disturbance~\cite{2015-Liu-Read_leveling}, it is not as significant as for write requests. 

% \textbf{\as{fic}2.} \textit{Auxiliary Write Amplification (AWA):} Similar to on-device write amplification, applications can also encounter write amplification, which is referred to as auxiliary write amplification~\cite{2014-Leonardo-NVMKV}. An example of this happens when the application is writing more data than was intended, when for instance required to run application-level garbage collection.

% \textbf{\as{fic}4.} \textit{Auxiliary Read Amplification (ARA):} Similar to read amplification and auxiliary write amplification, when an application is required to read more data than the read size issued. Such an amplification is not caused by the FTL but rather represents application introduced amplification. An example of this is an application required to issue more read requests to locate data, in for instance a file system required to read inodes prior to locating valid data blocks. Therefore, a 4KiB read for a data block requires multiple inode (data directory and file inode) to be located.

\noindent \textbf{\as{fic}4: Flash Parallelism.} With the various possible levels of parallelism on flash storage devices (discussed in \cref{sec:bg_flash_ssds}), exploiting of the various possibilities requires software design consideration to aligning with these. Although the \as{io} scheduling of on-device parallelism, such for channel-level parallelism, is responsibility of the \as{ftl} (on devices at \as{ssd} integration level), the \as{ftl} implements particular parallelism, given that the host \as{io} requests aligning with the possibility of parallelizing the request, such as with large enough \as{io}s to stripe across channels and dies. Embedded device and custom flash integrations have more possibility to manage flash device parallelism at the host software level.

\noindent \textbf{\as{fic}5: Wear Leveling (WL).} Given limited program/erase cycles for flash cells, even wear over the entire device is required to ensures that no specific areas of the device are burnt out faster than others. Similar to flash parallelism, this largely depends on the flash integration level, as the \as{ftl} at the \as{ssd} integration level ensures \as{wl}, however embedded flash integration and custom flash integration is required to place more significance on ensuring even wear across the flash cells. Strongly related to prior flash integration challenges, wear is commonly a result of \as{gc}, which in turn increases the \as{io} amplification, and particularly the  \as{wa} and \as{ra}~\cite{2014-Desnoyers-Analytic_Models_SSD,2009-Hu-WA_SSD}. 

% Figure environment removed

\noindent \textbf{\as{fic}6: \as{io} Scheduling.} As \as{ssd} ships with integrated firmware to expose the flash storage as a block/sector addressable storage device, integration into the current software stack is seamless. \cref{fig:storage_stack} shows the integration of a flash \as{ssd} into the Linux Kernel storage stack. Since flash storage devices are significantly faster than prior storage technologies, such as \as{hdd}, the storage software stack becomes the dominating factor in \as{io} latency~\cite{2010-Caulfield-Moneta,2012-Caulfield-Fast_User_Space_Access}. As is visible in \cref{fig:storage_stack}, the block \as{io} layer implements various schedulers with different functionality. Providing a ranging degree of optimizations for \as{io} requests, such as varying scheduling policies and merging of \as{io} requests, or possible reordering, specific configurations are favorable to increase performance with flash storage. Particularly the utilization of multiple queues, with multiple software and hardware dispatch queues (visible in the blk-mq configuration of the block \as{io} layer), allows better exploitation of flash storage capabilities, and avoids certain Linux Kernel overheads. Furthermore, evaluating mechanisms that reduce the latency of \as{io} operations, and particularly write \as{io} operations.

\subsection{Log-Structured File System - F2FS}\label{sec:bg_lfs}
A ubiquitous approach of managing persistent storage devices is with file systems, providing the familiar file and directory interface for organizing storage. The lack of in-place updates on flash pages, enforcing sequential writes, makes \af{lfs}~\cite{1992-Rosenblum-LFS,2006-Konishi-Linux_LFS,seltzer1993implementation} a suitable file system design. The design of \as{lfs} revolves around writing data as a log, appending new data sequentially on the storage device. Similar to the \as{ftl}, as data is being updated in a \as{lfs}, the log contains an increasing amount of no longer needed data, requiring \as{gc} to free occupied space. In this section, we describe the de-facto standard \as{lfs} for flash-based storage devices, \af{f2fs}~\cite{2015-Changman-f2fs}. A plethora of additional flash-based file systems exists, however these are commonly based on top of \as{f2fs} or adapt \as{f2fs} mechanisms.

% Figure environment removed

With \as{f2fs} being a \as{lfs}, it writes data sequentially, in several different logs. \as{f2fs} maintains six concurrently writable logs, of which three are for data of different lifetime classifications (hot/warm/cold), and the remaining three are for node data of different lifetime classifications. Internally, \as{f2fs} utilizes a data allocation unit referred to as a \textit{block}, of 4KiB, in which blocks are allocated in the different logs. Consecutive blocks are collected into a 2MiB \textit{segment}, of which one or multiple segments are further grouped into a \textit{section}, and sections are combined into a \textit{zone}. The goal of sections is to align with the \as{gc} unit of the underlying \as{ftl}, whereas a zone is utilized in order to avoid sections in different zones to be mapped into the same on-device erase unit by the \as{ftl}, which would defeat the purpose of section grouping. Such mapping is illustrated in \cref{fig:f2fs-zone-blind-allocation}, which depicts an example of \textit{zone-blind allocation}, where data for different files, with different lifetime classification, are mapped into different sections, however align to the same zone. As a result the data mapping may not be far enough, resulting in the \as{ftl} to write the data to flash pages within the same erase unit. With \textit{zone-aware allocation}, illustrated in \cref{fig:f2fs-zone-aware-allocation}, the zone serves the purpose to provide a large enough separation between particular sections, such that the \as{ftl} similarly separates the written flash pages for the different file data into different erase units. 

Such a mapping is provided by \as{f2fs}, with the option of configuring the number of sections in a zone, which depending on the erase unit of the device ensures adequate physical data separation. However, by default \as{f2fs} is configured to contain a single segment in each section and a single section in a zone. As \as{f2fs} is log-structured, over time it contains valid and invalid blocks, similar to the \as{ftl}, and must therefore also run \as{gc}. In \as{f2fs} the process of \as{gc} is done at the unit of a section, where valid blocks in the containing segments are read and written to a free space, prior to the segments in the section being freed. \cref{fig:f2fs-layout} shows the layout of segments, sections, and zones (on the right half of the figure), alongside the metadata structures which \as{f2fs} requires. We discuss each of the essential structures throughout this section, except the superblock which is similar to other file system superblocks and contains essential file system metadata for the host system to be able to mount the file system.

% Figure environment removed

Similar to other file systems, a file in \as{f2fs} is managed through an \af{inode}, and contains all the file information, including the file specific metadata on creation time, access permissions, file name, and more. \cref{fig:f2fs-inode} shows the inode of F2FS. For identifying the data blocks of the associated file, inodes contain a fixed number of direct pointers to the addresses of the file data. Since inodes are allocated in blocks, they can often contain inline data of the file, if the file data fits in the available space of the inode. To be able to locate larger files, inodes utilize \textit{multi-level indexing}, where indirect pointers point to blocks containing further pointers to more data blocks. There are several levels of indirection, where a \textit{single indirect} pointer points to one block containing pointers to the blocks which contain the data. For example, with a block size of 4KiB and 4B addresses, a single indirect pointer can store 1024 addresses, which each point to 4KiB of data, giving an addressable space of $4KiB*1024=4MiB$ (plus the number of direct pointers in the inode). Increasing the level of indirection further increases the maximum file size of the file system, where nowadays file systems most commonly provide up to three levels of indirect pointers, allowing in the example of 4KiB block size and 4B addresses, a maximum file size of $4KiB*(1024+1024^2+1024^3)=4TiB$ (plus the number of direct pointers). While multi-level indexing is a popular implementation in file systems, other solutions such as \textit{extents} exist, utilizing a single pointer to a contiguous number of blocks for the file. This approach decreases the metadata size by requiring only a single pointer, however it is more difficult of finding a contiguous number of blocks for allocating space for the file.

% Figure environment removed

File systems commonly utilize tree-based indexing data structures in order to achieve a worst case time complexity of $\mathcal{O}(\log{}n)$ for locating of inodes or other metadata, compared to a log based approach requiring to scan the entire log, corresponding to $\mathcal{O}(n)$ worst case time complexity. However, a tree-based data structure introduces the Wandering Tree Problem~\cite{bityutskiy2005jffs3}, where if a leaf node is modified, its new block is written at the end of the log, giving it a new block address. Its parent node must also be modified to point to this new address, requiring a new block to be written for it at the end of the log, giving it a new block address. These changes to the parent node are propagated all the way up the tree to the root node. Therefore, F2FS manages metadata for segment allocations in a \af{nat}. It divides allocated blocks into three types; \textit{inode blocks} that contain the metadata for the file from the \as{inode}, \textit{direct pointer node blocks} containing the \as{lba} of the data, and \textit{indirect node blocks} holding unique identifiers to the direct node blocks. Therefore, a change in a data block requires the direct node block to be updated, to point to the new \as{lba} of the data, and a change in the \as{nat} to point the identifiers of the direct node blocks to the new direct node block address. The indirect node block does not require any changes, as it stores identifies of the direct node blocks, which are modified in the \as{nat}, therefore solving the wandering tree problem. To avoid metadata updates being written directly after a change, F2FS uses the \af{lmu} to buffer metadata and node block updates in memory before being flushed to the flash storage. Additional data structures for metadata in F2FS include the \af{sit} for information on the segment usage and the bitmap to identify valid and invalid blocks, and a \af{ssa} which includes further information on segments such as the information on owners of data and node blocks. Note that F2FS requires randomly writable space for metadata, as it is not written in the log, delegating data management to the \as{ftl}. 

Given that file systems are written in units of blocks, smaller \as{io}s arising from minor file challenges, such as microwrites, are undesirable due to the added \as{wa}. Therefore, file systems employ \textit{write buffering} by caching the changes in memory, and writing to storage when needed. However, this comes at the drawback that if the system crashes, the data cached in memory is lost. For this file systems provide the possibility to directly write to storage with \textit{direct \as{io}} or to explicitly flush caches to the storage device with an \textit{fsync()} call. To provide failure recovery in the case of system crashes or power loss, file systems commonly employ methods such as \textit{journaling}, in which operations are logged in a journal in addition to being written to the storage. Therefore, if the system crashes replaying the journal will result in reaching the prior valid file system before the system failure. File systems with this journaling feature are referred to as \textit{journaling file systems}. 

A similar approach with fewer overheads is using \af{cow}, commonly also referred to as \textit{shadow paging}, in which a copy of the original data is made on writing, and the copy is modified. Only upon successful completion is the metadata, pointing to the valid data block, changed to present the new copy. This can additionally be done atomically and simplifies the crash recovery, as the metadata state will always be valid, such that if the copy was written but the metadata points to the old data, the write did not complete successfully and thus the original data is the valid data. While there exist more implementations for ensuring crash recovery and consistency, journaling and \as{cow} are among the most utilized approaches. F2FS however utilizes \textit{checkpointing}, in which metadata including the file system status, bitmaps, inode lists, and the segment summary, are periodically written to the \af{cp} region. During a checkpoint, all data in page cache is flushed to storage and a \textit{checkpoint pack} is written, containing the metadata (\as{nat} and segment information bitmaps). In the case of a system crash or power loss, the file system can recover the state from the latest checkpoint, referred to as \textit{roll-back recovery} since the latest changes which are not in the checkpoint are reverted. In order to recover the latest changes, the host must call \textit{fsync()} to ensure that metadata and data are flushed from memory to the device. This recovery is referred \textit{roll-forward recovery} since it recovers the state past the latest stable checkpoint. F2FS can only guarantee roll-forward recovery with \textit{fsync()}.

To minimize the file system \as{gc} overheads, \as{f2fs} groups data based on the modification frequency. It achieves this by assigning a temperature (hold/warm/cold), and pre-defines classifications for direct node blocks, as these are frequently modified, and indirect node blocks are classified as cold, as these are not frequently modified. To further extend this mechanism, F2FS maintains six different logging zones for the different temperature assignments (hot/warm/cold for nodes and file data). In addition to node block classifications, data blocks are classified in several ways. Firstly, the host can provide a hint on the lifetime of a file, upon which the data blocks in the file are grouped according to this hint. The Linux Kernel provides five different lifetime hints, ranging from \textit{short} to \textit{extreme}, which \as{f2fs} reduces to the three groups it has. Secondly, \as{f2fs} by default assigns files with particular extensions, such as media data (e.g., .mp3, .mp4, .gif), with cold lifetime classification, as media is commonly written just once and then only read again. The default extensions to classify are written in the \as{f2fs} superblock as formatting time. \as{f2fs} reclassifies the lifetime based on the access frequency, where file data which is being moved during \as{gc}, meaning it has not been modified up to the time of \as{gc} and is therefore likely to not be modified frequently in the future, is classified as cold. Similarly, if the number of dirty blocks in a segment exceeds a threshold (16 by default), the corresponding inode is marked as hot. 

In F2FS \as{gc} is referred to as \textit{cleaning}, and is run periodically (called \textit{background cleaning}) or when space is needed (called \textit{foreground cleaning}). The cleaning policies implemented in F2FS are greedy~\cite{2010-Bux-Greedy_GC,kawaguchi1995flash}, \af{cb}~\cite{kawaguchi1995flash,1992-Rosenblum-LFS,wu1994envy}, which are used for foreground and background cleaning respectively, and \af{at}~\cite{2020-f2fs-age_threshold,menon1998age}. Greedy victim selection selects the segment with the lowest utilization, whereas the \as{cb} policy selects the segment with the best cost/benefit ratio, calculated as~\cite{1992-Rosenblum-LFS}, with $u$ being the utilization of the segment
\begin{equation*}
    \text{cost-benefit}=\frac{\text{free space generated}}{\text{cost}}=\frac{(1-u)*\text{age}}{1+u}
\end{equation*}
where the cost is based on reading the segment (depicted by the 1), plus writing the data in the segment (depicted by the utilization). \as{at} based \as{gc} is disabled by default and aims at avoiding that the same section is continuously being selected during \as{gc} if it always provides the best victim. To achieve this, a minimum lifetime is set (by default 7 days), which must be exceeded in order to be selected as the victim.

While F2FS utilizes a log-structured approach, under increased concurrency such a log becomes a bottleneck. Therefore, to provide increased performance, F2FS can switch between a single sequentially written log (\textit{normal logging}) to \textit{threaded logging}, where data is written in dirty segments instead of the log. However, while this approach provides increased logging capacity for concurrent access and avoids garbage collection, it incurs random writing. Similarly, \as{f2fs} provides the option for \af{ssr}, in which it uses invalid blocks in a segments instead of running foreground \as{gc}. While this approach avoid running \as{gc} it generates random write patterns. Given that during file system \as{gc}, block addresses are modified, as the cleaning moves still valid blocks to free space, however the metadata is not directly updated due to the \as{lmu} policy, F2FS is required to create a checkpoint after each segment cleaning.

\subsection{Flash Integration Organization}
With the different possible levels for integration of flash storage (recall \cref{fig:flash_integrations}), and while mechanisms for solving flash integration challenges are often applicable at various integration levels, several of the mechanisms we present require deeper integration of flash storage, levering increased control, in order to be implemented. For instance, the incorporation of the various levels of on-device parallelism is not directly possible at the \as{ssd} integration level, as the \as{ftl} controls the parallelism on the physical device. The custom and embedded level flash integration provide the host with more possibility to manage these. In order to separate the possibility of mechanisms to be implemented with a particular flash integration level, \cref{tab:integration_levels} provides a classification of each evaluated file system in this literature study to the respective integration level. During the evaluation we present the different mechanisms to solve a \as{fic}, and indicate which file systems utilize these. Therefore, when considering the feasibility of a mechanism for a particular flash integration consult this table to see its applicability. Note that file systems are not limited to the classification we provide, as for instance file systems designed for \as{ssd} flash integration also work on some embedded flash integration. However, we utilize only a single classification for each file system to avoid confusion. Exceptions are made only in specific cases where an existing file system is adapted for a different flash integration level.

\begin{table}[!t]
    \centering
    \begin{tabular}{||p{35mm}|p{35mm}||}
        \hline 
        Integration Level & File Systems \\
        \hline
        \hline
        \as{ssd} Flash Integration & \cite{2015-Changman-f2fs,2021-Gwak-SCJ,2012-Min-SFS,2014-Lu-ReconFS,2020-Tu-URFS,2019-Yoshimura-EvFS,2022-Jiao-BetrFS,jannen2015betrfs,huang2013improve,2019-Liu-fs_as_process,2018-Kannan-DevFS,2021-Liao-Max,2015-Kang-SpanFS,2022-Oh-exF2FS,rodeh2013btrfs} \\
        \hline
        Custom Flash Integration & \cite{kawaguchi1995flash,josephson2011direct,2020-Wu-DualFS,2018-Rho-Fstream,dubeyko2019ssdfs,lee2014refactored,2019-Lu-Sync_IO_OCSSD,2016-Lee-AMF,2018-Yoo-OrcFS,2016-Zhang-ParaFS,2021-Qin-Atomic_Writes} \\
        \hline
        Embedded Flash \newline Integration & \cite{2006-Lim-NAND_fs,hunter2008brief,2009-Sungjin-FlexFS,2008-Jung-ScaleFFS,woodhouse2001jffs,park2013enffis,nahill2015flogfs,schildt2012contiki,2015-Park-Suspend_Aware_cleaning-F2FS,2009-Zuck-NANDFS,2009-Park-Multimedia_NAND_Fs,2007-Hyun-LeCramFS,2021-Ji-F2FS_compression,2011-Park-Multi_NAND,manning2010yaffs,aleph2001yaffs,manning2002yaffs,engel2005logfs,2008-Kim-DFFS,park2006flash,2020-Yang-F2FS_Framentation,2022-Lee-F2FS_Address_Remapping,2011-Lim-DeFFS,2014-Hyunchan-O1FS,2022-Zhang-ELOFS,2020-Zhang-LOFFS,kim2006mnfs,2009-Tsiftes-Coffee_FS} \\
        \hline
    \end{tabular}
    \caption{Classification on the flash integration level utilised for the file systems evaluated in this literature study.}
    \label{tab:integration_levels}
\end{table}

We divide the discussion of mechanism by the \as{fic} for which the evaluated study presents a novel solution. This implies that mechanisms that solve multiple \as{fic} are discussed in detail in the first section they appear in, however are also mentioned in all latter sections for the \as{fic} that the mechanism solves. Therefore, each \as{fic} section contains a table of the respective mechanisms presented to solve that particular \as{fic}, along with a reference to the corresponding section of its detailed discussion.

