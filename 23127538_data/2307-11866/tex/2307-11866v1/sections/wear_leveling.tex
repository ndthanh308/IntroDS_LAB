\section{FIC-5: Wear Leveling}\label{sec:wear_leveling}
As flash cells wear out over time, it is important to utilize the flash evenly to avoid burning out particular flash cells faster than others. The possibility for ensuring even wear at the different levels of integration is limited, as at the \as{ssd} flash integration level, the \as{ftl} handles all wear leveling, without host considerations. However, similar to prior flash integration challenges, several mechanisms are nonetheless applicable. In particular, reducing the write traffic to the flash device, as less writing incurs less flash wear, and particularly flash-specific data structures inherently provide a degree of wear leveling. Based on the sequential write requirement, data structures, such as the \as{lfs} must write sequentially in an append-only fashion, which evenly writes the space. At closer to flash integrations, where host systems have more control over the flash management, there are particular mechanisms to ensure better flash wear. \cref{tab:wl} depicts the methods we discuss in this section for enabling increased wear leveling for file systems.

\begin{table}[!t]
    \centering
    % Update reference format to fit table into column
    \crefformat{section}{\S#2#1#3}
    \crefformat{subsection}{\S#2#1#3}
    \crefformat{subsubsection}{\S#2#1#3}
    \begin{tabular}{||p{50mm}|p{25mm}||}
        \hline 
        Mechanism & File Systems \\
        \hline
        \hline
        \cellcolor{lightgreen!75} Write Optimised Data Structures (\cref{sec:wods}) & \cellcolor{lightgreen!75} \cite{2020-Tu-URFS,hunter2008brief,rodeh2013btrfs,2022-Jiao-BetrFS,dubeyko2019ssdfs} \\
        \hline
        \cellcolor{lightgreen!75} Reducing Write Traffic (\cref{sec:reduce_write_traffic}) & \cellcolor{lightgreen!75} \cite{huang2013improve,2011-Lim-DeFFS,woodhouse2001jffs,dubeyko2019ssdfs,2021-Ji-F2FS_compression,2007-Hyun-LeCramFS,ning2011design,2009-Zuck-NANDFS,2022-Lee-F2FS_Address_Remapping} \\
        \hline
        \cellcolor{lightgreen!75} \as{gc} Policies (\cref{sec:gc_policy}) & \cellcolor{lightgreen!75} \cite{2012-Min-SFS,2018-Yoo-OrcFS,2021-Gwak-SCJ,2015-Park-Suspend_Aware_cleaning-F2FS} \\
        \hline
        Write \& Read Leveling (\cref{sec:write_leveling,sec:read_leveling}) & \cite{2006-Lim-NAND_fs,2016-Lee-AMF,2020-Wu-DualFS,2009-Sungjin-FlexFS} \\
        \hline
    \end{tabular}
    % Reset reference format to before
    \crefformat{section}{Section #2#1#3}
    \crefformat{subsection}{Section #2#1#3}
    \crefformat{subsubsection}{Section #2#1#3}
    \caption{Mechanisms for file systems to deal with wear leveling of the flash storage, and the respective file systems that implement a particular mechanism. Green highlighted table cells depict previously discussed mechanisms with their respective section.}
    \label{tab:wl}
\end{table}

\subsection{Write Leveling}\label{sec:write_leveling}
Several flash integration challenges proved data grouping to be an effective method for dealing with \as{gc} overheads and \as{io} amplification. However, this can have an effect on the flash storage. In particular the hot data, such as file system metadata which is more frequently updated and written, must be moved across the flash space more than cold data. CFFS~\cite{2006-Lim-NAND_fs} therefore switches the allocation areas for metadata and data blocks, such that an erased metadata block becomes a free data block. Therefore, cold data should be placed in blocks that have been written more frequently, whereas hot data should be placed in blocks with a lower write count history. The principle of migrating cold data from less written blocks, which are also referred to as \textit{younger blocks}, to more frequently written, \textit{older blocks}, is referred to as \textit{cold-data migration}, and similarly moving hot data from old blocks to younger blocks is known as \textit{hot-data migration}~\cite{chang2007efficient}. These methods are commonly used in \as{ftl} implementations due to their simplicity and effectiveness, and similarly in file systems such as ALFS~\cite{2016-Lee-AMF}.

Wear leveling is an increasingly vital concern on file systems that utilize flash dual mode~\cite{2020-Wu-DualFS,2009-Sungjin-FlexFS}, where it switches the flash level to increase performance for critical \as{io} requests. Due to the lowering in flash cell level, the same amount of written data requires a larger amount of space, where a switch from \as{mlc} to \as{slc} divides the capacity in half, requiring double the space for the same \as{io} request. Therefore, these file systems include a \textit{write budget} that is maintained for the areas, and dynamically resizes the available lower cell level area, such as decreasing the space if the wear is reaching a threshold. This switches the cell level back to a higher number, allowing to write more with less wear. Additionally, the file systems utilize the wear budget in order to identify if an \as{io} request should be redirected to the larger cell area, instead of being written to the lower cell level area.

\subsection{Read Leveling}\label{sec:read_leveling}
While write operations are the major cause of flash cells burning out, read operations also pay a toll on flash cells, as the current flash cell technology utilizes flash cells that are only capable of holding very few electrons (determining the charge of the gate) due to their size~\cite{lu2009future,shin2005non}. This makes the cells increasingly susceptible to \textit{read disturbance}~\cite{2015-Liu-Read_leveling}, where reading of a page results in shifting of voltages in nearby cells (typically in the same block), requiring frequent rewriting to ensure the charge stays consistent. In order to control read disturbance, Liu et al.~\cite{2015-Liu-Read_leveling} propose to read-leveling mechanisms in the FTL. While their proposal is aimed at \as{ftl} implementations, the ideas are applicable to file systems for flash storage devices. With the proposed read-leveling, the read-hot data, that is read more frequently, is isolated from other data pages by placing the hot pages into \textit{shadow blocks}, which contain no valid data, in order to avoid disturbing that data. However, this requires to identify the read-hot pages, where a tracking of read counters for each page would require significant resources. Therefore, a \textit{second-chance monitoring strategy} is proposed, which initially tracks the reads for each block, therefore requiring counters at a higher block granularity, and upon reaching a threshold indicating the block contains read-hot pages, the individual pages in the block are tracked on their read counters. Finally, the pages in these blocks that reach a certain threshold are copied to the shadow blocks. Therefore, this avoids the tracking of read counters for individual pages and only copies read-hot pages into the shadow blocks. While this strategy requires copying of read-hot pages to shadow blocks, it minimizes read disturbance which in turn minimizes the \as{wa} it causes.

\subsection{Summary}
In addition to previously discussed mechanisms to reduce \as{io} amplification and \as{gc}, resulting in decreased wear of the flash storage, write leveling, ensuring that write requests are spread across the available storage space, and read leveling are important mechanisms for ensuring the longevity of the flash storage.
