\section{Background}\label{chp:background}
The increasing adoption of flash-based \as{ssd}~\cite{daim2008forecasting,2008-Lee-Flash_in_enterprise_db}, due to its
higher performance capabilities compared to conventional \as{hdd}, and decreasing cost is making it a ubiquitous storage
media for storage systems. In this section we explain the high-level concepts of the construction of flash-based \as{ssd} (\cref{sec:bg_flash_storage,sec:building-ssd,sec:ssd-performance,sec:ssd-ftl}), technical details of the newly standardizes \as{zns} \as{ssd} (\cref{sec:zns-bg}), and how the de facto standard file system \as{f2fs} is designed for flash storage (\cref{sec:f2fs-bg}).

\subsection{Flash Storage Building Block: Flash Cells}\label{sec:bg_flash_storage}
Flash storage is based on \textit{flash cells} providing the persistent storage capabilities through programming of the cell and erasing it to clear its content. Each flash cell is constructed with a floating gate, which can hold an electrical charge, and a control gate, inside a \af{cmos} transistor~\cite{parat2015floating}. Using the electrical charge present inside  the flash cell, each cell is capable of representing a logical value (0 or 1). Such flash cells, capable of representing a single bit, are called \af{slc}. Data is written to flash cells by \textit{programming} the cell. Updating existing data in flash cells requires the cell to be \textit{erased}, followed by being programmed. In addition to \as{slc}, other types of flash cells are available, ranging from \af{mlc} (2 bits per flash cell) to \af{plc} (5 bits per cell)~\cite{2020-Ma-Flash_Memory_Bit_Errors}. To represent more bits with a single flash cell, the charge inside the flash cell is divided into a respective number of ranges in order to represent the number of bit combinations (e.g., 4 ranges to represent all combinations of 2 bits in \as{mlc}). 

Increasing the number of levels in flash cells however results in slower access time~\cite{arpaci2018operating}, increased wear on the flash cells and a lower lifetime~\cite{mohan2010learned,2016-Schroeder-Flash_Reliability}, as the cells erode over time with it being programmed. The wear on flash cells is accelerated with more bits being stored in the cell (i.e., increasing the cell level). For \as{slc} the number of program/erase cycles is typically 100K, and decreases to 10K or less for \as{mlc}~\cite{2016-Schroeder-Flash_Reliability}. As a result enterprise flash \as{ssd} most commonly employ \as{slc} for increased reliability and lifetime.

Similarly, the organization of flash cells dictates the resulting type of storage device. Flash cells can be organized
in the form of \textit{NOR} or \textit{NAND}, representing the connection architecture of the cells. With NOR flash, the
resulting flash is byte-addressable, whereas NAND flash provides a page-addressable structure, with a page representing
the unit of read and write. As a result NOR flash is commonly applied in \af{bios}~\cite{2012-Lai-NOR_BIOS}, however
NAND architecture is suitable for mass data storage, making it the primary architecture for flash \as{ssd}. Throughout
this literature study we focus purely on NAND flash, however for more information on flash cell architecture consult reports such as~\cite{bez2003introduction} and~\cite{compagnoni2017reviewing}.

\subsection{Building Mass Storage SSD with Flash Cells}\label{sec:building-ssd}
Utilizing the building block of flash cells, building mass storage devices is achieved by grouping together several flash cells into \textit{pages} (typically 8-16KiB in size), depicting the unit of access (read/write). A flash page additional has a small extra space, called the \af{oob} area, which is utilized for \af{ecc} and small metadata. As the operation for erasing of a flash page is costly~\cite{2009-Gupta-DFTL}, multiple pages are grouped into a \textit{block} (several MiB in size), which is the unit of erase. Increasing the erase unit from individual pages to a block helps amortize the erase overheads. Pages within a block have to be written sequentially and prior to a page being overwritten, the entire block must be erased. A number of blocks are then grouped into a \textit{plane}, of which multiple planes are combined into a \textit{die}~\footnote{\textit{Dies} are also referred to as \textit{chips}, we use the terms interchangeably and mean the same entity.}. Lastly, numerous chips are combined into a single flash package. \cref{fig:ssd_architecture} shows a visual representation of a flash-based \as{ssd} architecture.

\input{figs/ssd_architecture.tex}

In order to build full storage devices, multiple flash packages are packed together into a \as{ssd}. While a \as{ssd} can be constructed with any solid state technology such as Optane~\cite{2019-Wu-Unwritten_Contract_Optane,2020-Yang-Optane_3DXpoint}, we focus purely on NAND flash based \as{ssd}. The \as{ssd} contains a controller, which is responsible for processing requests, managing data buffers, and contains the NAND flash controller that manages the flash packages. An additional \af{ram} buffer, most commonly in the form of \af{dram}, is present on the device for maintaining address mappings. Lastly, a \as{ssd} contains the host interface that provides the means to connect the storage device to the host system over connection interfaces such as \af{sata} and \af{pcie}, and defining standards such as \af{ahci} and \af{nvme}~\cite{landsman2013ahci}. \as{nvme} is the interface specification particularly designed for fast \as{ssd} devices, capable of outperforming legacy protocols such as \as{sata} with 8x higher performance~\cite{xu2015performance}, due to its increased number of \as{io} queues to which requests can be issued in parallel. Similarly, \as{pcie} protocol achieves the highest throughput and lowest latency compared to \as{sata}~\cite{2013-Eshghi-SSD}. As a result, flash \as{ssd} are commonly connected to systems through \as{nvme} over \as{pci}.

\subsection{Increasing Flash SSD Performance}\label{sec:ssd-performance}
Given the architecture of flash \as{ssd}, the it contains a large degree of possible parallelism (i.e., multiple channels, flash packages, dies, and planes). Furthermore, based on a study on a Samsung \as{ssd}, the bandwidth of NAND flash \af{io} buses, connecting the flash to the flash controller, is limited to 32MB/s because of physical restrictions, and 40MB/s with interleaving in dies~\cite{2008-Agrawal-Design-Tradeoff-SSD}. A write operation to flash storage firstly writes the data to a data register, from which the data in the register is then programmed into the flash page. Because the programming operation takes longer than loading of data, these operations can be interleaved, such that while a page is being programmed, data of another write operation is loaded~\cite{kim2011parameter}. This avoids stalling whenever a page programming finishes and data needs to be loaded again, which is fundamentally similar to the concept of CPU pipelining~\cite{tanenbaum2016structured-pipelinining}. 

In order to increase performance, parallelism is utilized on the different flash entities in the \as{ssd}~\cite{chen2011essential}. Depending on the hardware configurations, the different types of parallelism are referred to as \textit{channel-level parallelism}, where requests are distributed individually across the flash channels, \textit{package-level parallelism} where flash packages on the same channel are accessed in parallel, \textit{chip-level parallelism} where flash packages with multiple chips access these in parallel, and \textit{plane-level parallelism} where the same operation can be run on multiple planes, on the same chip, in parallel. A commonly applied technique for enhanced parallelism is the utilization of \textit{clustered blocks}, where requests are issued to blocks in parallel to different chips and planes.

There are several additional performance optimizations for accessing flash storage. Other than accessing a flash entity in parallel and operation interleaving, flash packages can be accessed synchronously with a single request queue through \textit{ganging}~\cite{2008-Agrawal-Design-Tradeoff-SSD}. Flash packages within a gang share the same control bus from the flash controller, however can utilize different data buses, thus allowing a single command request queue with multiple data buses to provide the resulting data. Therefore, requests that require data from multiple pages can be split among a gang from a single request queue, and provide the data on different buses, avoiding the bottleneck of a single data bus. 

\subsection{Hiding Flash Management Idiosyncrasies on the SSD}\label{sec:ssd-ftl}
With the flash characteristic requiring sequential writing within blocks, and lacking support for in-place updates, flash \as{ssd} employs a \af{ftl} to hide these management idiosyncrasies, and provide seemingly in-place updates. As a result of the sequential write constraint, if data inside a flash page is updated, the page is simply marked as invalid and the new data is appended to a new a flash page, possibly in the same block. The \as{ftl} is responsible for managing flash page information on their validity and its mappings to \af{lba}, which is the mechanism for storage software to address the storage device. 

 Different implementations of a \as{ftl} can utilize different mapping levels, such as block- and page-level mappings. A
 naive \as{ftl} design is to maintain a fully associative mapping of each \af{lba} to every possible
 \af{pba}~\cite{2009-Gupta-DFTL}, referred to as \af{l2p} mapping, and inversely \af{p2l} mapping. These mappings are
 maintained in a \textit{mapping table}, which is kept in the \as{ram} of the flash device for faster accesses. For
 consistency the mapping table is also maintained in the persistent flash storage, where on startup time of the devices
 the mapping table is reconstructed in the device
 \as{ram}~\cite{chen2009understanding,2008-Agrawal-Design-Tradeoff-SSD}. With this, the \as{ssd} can provide seemingly
 in-place updates by simply writing data to a free page and invalidating the overwritten data. Further details on
 \as{ftl} mappings are not required for the remainder of this literature study, however for a detailed explanation of \as{ftl} mapping algorithms consult \cite{chung2009FTL_survey} and \cite{2009-Gupta-DFTL}.

As over time an increasing number of flash pages become invalid due to data updates, blocks contain valid and invalid data, requiring the \as{ftl} to run \af{gc}. During \as{gc}, the \as{ftl} selects a block from which it reads the still valid flash pages, writes these to an empty block, followed by erasing of the original block. The process of \as{gc} requires the device to provide an empty space that cannot be used as storage, but is only used for the \as{ftl} to move valid pages, referred to as the \af{ops}. If a device has fully utilized all its capacity, the \as{ftl} must be able to write out valid pages, which the overprovisioning space serves at. Therefore, \as{ssd} commonly ship with an overprovisioning space of 10-28\%, which is only usable by the \as{ftl}. In addition to \as{gc}, the \as{ftl} is responsible to ensure even wear across flash cells, as a flash cell has a limited number of program/erase cycles. This process is referred to as \af{wl}, where the \as{ftl} ensures the flash wears out evenly across the entire device, and no particular parts are burnt out faster than others.

\subsection{Zoned Namespace SSD}\label{sec:zns-bg}
While the \as{ftl} provides the seamless integration of flash \as{ssd} into storage systems, details about the flash
characteristics, such as the garbage collection unit, are commonly hidden from users, making reverse engineering of such information increasingly complex~\cite{2023-cluster-zns-study,2022-systor-ssd-study}. Furthermore, its unpredictable
performance as a result of \as{gc}~\cite{2015-Kim-SLO_complying_ssds,2014-yang-dont_stack_log_on_log} has shifted the
research community to new flash interfaces that expose flash management to the host system. Such interfaces allow for
better coordination between the storage device and the host software by decreasing \as{ftl} responsibility, and
increasing control for host software of the flash storage. Efforts for open flash \as{ssd} interfaces include
\af{sdf}~\cite{2014-Ouyang-SDF} and \af{ocssd}~\cite{bjorling2017lightnvm,picoli2020open}, which however failed to gain
large scale adoption due to the lack of standardization, resulting in device-specific implementations, and complex
interfaces, requiring software developers to have extensive knowledge of flash in order to be able to build
flash-specific software. 

The newest addition to opening flash \as{ssd} interfaces comes with arrival of \af{zns} \as{ssd}. The 2.0 base specification of \as{nvme}~\cite{2022-nvme-spec} (published in June 2021), establishes the standardization of \as{zns} with the concept of splitting the address space of the storage device into a number of \textit{zones}, which are independently addressable. This concept of representing the storage space with zones has previously been introduced with the addition of \as{smr} \as{hdd}s~\cite{Feldman2013ShingledMR,Gibson2011PrinciplesOO,Suresh2012ShingledMR}. These are a particular type of \as{hdd} that increase the storage density. Its concept of zones was included in the Linux kernel through the \af{zac} and \af{zbc} specifications~\cite{2014-ZBC,2015-ZAC}. Similar to \as{smr}, with \as{zns} zones have to be written sequentially, and must be reset prior to being overwritten, matching the internal characteristics of flash. 

\textbf{\as{zns} Interface.} \cref{fig:ZNS_HW} depicts a simplified layout of zones on a \as{zns} device, illustrating the management of the sequential write requirement within zones with a \af{wp} for each zone. The \as{wp} indicates the next \af{lba} to be written in the particular zone. The starting \as{lba} of a zone is represented by a \af{zslba}, identifying the first \as{lba} of each zone. For zone management, each zone has an associated state, such as \textit{FULL} and \textit{EMPTY}. 

In addition to the zone management, the \as{nvme} standardization of \as{zns} introduces three new concepts. Firstly, the specification details the zone capacity, which limits the addressable region within a zone. In order to integrate into the Linux kernel, the zone size must be a power of two value, since kernel operations rely on bit shifts for addressing. However, the addressable space in a zone may not be a power of two value, and can therefore be less than or equal to the zone size. Any \as{lba} beyond the zone capacity is not addressable. 

% Figure environment removed

Secondly, the specification adds a limit on the number of concurrently active zones. Zone states indicate the state of the zone, where zones that are currently in an \textit{OPEN} or \textit{CLOSED} state are considered to be active. As the device must allocate resources for active zones, such as write buffers, it enforces a limit on concurrently active resources. Lastly, \as{zns} introduces a new \textit{zone-append} command, which instead of requiring the host to manage I/Os, such that they adhere to the sequential write constraint within a zone, allows the host to issue write I/Os to a zone without specifying the \as{lba}. The device handles the write and returns the address at which the data is written. 

The \textit{zone append} command is particularly beneficial with large queue depths (submitting numerous asynchronous
write \as{io} requests), which is not possible with write commands, as these must be issued at consecutive addresses and
writes can be reordered in the block layer of the Linux kernel or inside the storage device. In order to adhere to the
write constraint in a zone without relying on the zone append command, the \texttt{mq-deadline} scheduler within the
Linux kernel must be enabled. The \texttt{mq-deadline} scheduler holds back \as{io}s and only submits a single \as{io}
at a time to the \as{zns} device. Furthermore, this allows to merge \as{io} requests in the scheduler, enhancing
performance by issuing a smaller number of larger \as{io} requests. Evaluations on the performance of the different
schedulers show the benefits of merging \as{io} requests with larger \as{io}s of $\ge16$KiB being required to saturate
the device bandwidth~\cite{2022-Tehrany-Understanding_ZNS,2020-nvmsa-zns-implications}. 

\subsection{F2FS: Flash-Friendly File System}\label{sec:f2fs-bg}
A ubiquitous approach of managing persistent storage devices is with file systems, providing the familiar file and
directory interface for organizing storage. The lack of in-place updates on flash pages, enforcing sequential writes,
makes \af{lfs}~\cite{1992-Rosenblum-LFS,2006-Konishi-Linux_LFS,seltzer1993implementation} a suitable file system design.
\as{lfs} revolves around writing data as a log, appending new data sequentially on the storage device. In this section,
we describe the de facto standard \as{lfs} for flash-based storage devices, \af{f2fs}~\cite{2015-Changman-f2fs}, a
plethora of file systems base their design on the foundations presented by \as{f2fs}. 

\subsubsection{F2FS Data Layout}
Internally, \as{f2fs} utilizes a data allocation unit referred to as a \textit{block}, of 4KiB, in which blocks are allocated on the log. Consecutive blocks are collected into a 2MiB \textit{segment}, of which one or multiple segments are further grouped into a \textit{section}, that are combined into a \textit{zone}. \cref{fig:f2fs-layout} shows the layout of segments, sections, and zones for the data logs (on the right half of the figure). The left half of figure \cref{fig:f2fs-layout} shows the metadata structures in \as{f2fs} to manage the file system, consisting of a \af{cp}, \af{sit}, \af{nat}, and \af{ssa}. We now explain each of these data structures.

% Figure environment removed

\textbf{Checkpoint.} \as{f2fs} utilizes \textit{checkpointing}, in which all essential metadata for the file system is stored to provide recovery in the case of system failure. A checkpoint is periodically generated, or explicitly triggered, and persists all metadata information from memory. In the case of a system crash or power loss, the file system can recover the state from the latest checkpoint, referred to as \textit{roll-back recovery}, since the latest changes which are not in the checkpoint are reverted. In order to recover the latest changes, the host must call \textit{fsync()} to ensure that metadata and data are flushed from memory to the device. This recovery is referred \textit{roll-forward recovery} since it recovers the state past the latest checkpoint. F2FS can only guarantee roll-forward recovery with \textit{fsync()}.

\textbf{Segment Information Table.} With the data allocation in \as{f2fs} being organized in 2MiB segments on the log, the \as{sit} maintains information on each of the segments. It maintains bitmaps for each segment to indicate valid and invalid blocks (blocks that have been overwritten).

\textbf{Node Address Table.} Similar to other file systems, a file in \as{f2fs} is managed through an \af{inode}, and contains all the file information, including the file specific metadata on creation time, access permissions, file name, and more. \cref{fig:f2fs-inode} shows the inode of F2FS. For identifying the data blocks of the associated file, \as{inode}s contain a fixed number of \textit{pointers} to the addresses of the file data. Since an \as{inode} is allocated in a block (4KiB), they can often contain inline data of the file, if the file data fits in the available space of the \as{inode}. If data for a file is updated, a new block is allocated on the log for the new data, followed by an update to the \as{inode}, in order to modify the pointer to the data to point to the newly written data. However, this allocates a new block for the \as{inode} of the file, requiring the metadata to track the inode locations to similarly be updated. These changes continue propagating to the parent nodes, resulting in a high increase of required metadata updates. Therefore, \as{f2fs} utilizes the \as{nat}, to maintain an identifier of each node and its corresponding block address. Upon an update of a node, only the block address in the \as{nat} is modified to depict the new block address of the node. Finding a node address then checks the \as{nat} entry for the respective node identifier.

% Therefore, \as{f2fs} manages metadata for segment allocations in a \af{nat}. It divides allocated blocks into three types; \textit{inode blocks} that contain the metadata for the file from the \as{inode}, \textit{direct pointer node blocks} containing the \as{lba} of the data, and \textit{indirect node blocks} holding unique identifiers to the direct node blocks. Therefore, a change in a data block requires the direct node block to be updated, to point to the new \as{lba} of the data, and a change in the \as{nat} to point the identifiers of the direct node block to the new direct node block address. The indirect node block does not require any changes, as it stores identifies of the direct node blocks, which are modified in the \as{nat}, therefore solving the wandering tree problem. 

% Figure environment removed

\textbf{Segment Summary Area.} In addition to the segment information in the \as{sit}, \as{f2fs} tracks information such as the owner of segments in the \as{ssa}. Furthermore, the \as{ssa} provides a cache for frequently accessed \as{nat} and \as{sit} information. 

\textbf{Main Area Logs.} The right half of \cref{fig:f2fs-layout} shows the layout of the main area logs, to which data and \as{inode}s are written. \as{f2fs} utilizes multiple concurrently writable logs to enhance data grouping and performance, with 3 logs to which nodes are written, and 3 logs for data. An essential mechanism for limiting the required \as{gc}, which \as{f2fs} must run to free space on the logs, comes from efficient \textit{data grouping}. With data grouping, data that has a similar lifetime is grouped together. Given that data has a similar lifetime, it is likely to be updated within close proximity. Therefore, when \as{gc} is run, fewer valid blocks are present, as the data with the same lifetime has likely been updated, reducing the amount of valid data that must be moved by the \as{gc} process. 

To support data grouping \as{f2fs} utilizes the three types of lifetime classes (hot/warm/cold), which are separated into the three different logs for node and data. The lifetime of data can be explicitly set for each file by an application through the passing of a \textit{lifetime hint} with the \texttt{fcntl()} function. The Linux kernel provides a total of 5 different lifetime hints, which \as{f2fs} reduces to the three lifetime hints it utilizes. If a lifetime hint for a file is not set, \as{f2fs} either assigns the default warm lifetime classification, or assigns a lifetime classification based on the file type. With the extension of a file (e.g., \texttt{.txt}, \texttt{.pdf}), \as{f2fs} identifies which files are likely to be updated in the future. Multi-media files (e.g., \texttt{.mp4}, \texttt{.gif}, \texttt{.png}) are less likely to be updated and are directly classified as cold data.

\subsubsection{F2FS Garbage Collection}\label{sec:f2fs-gc-bg}
As \as{f2fs} is log-structured, over time it contains valid and invalid blocks, similar to the \as{ftl}, and must therefore also run \as{gc}. In \as{f2fs} the process of \as{gc} is done at the unit of a section, where valid blocks in all the segments of the section are read and written to a free space, prior to all the segments in the section being freed. In \as{f2fs} \as{gc} is referred to as \textit{cleaning}, and is run periodically (called \textit{background cleaning}) or when free space for writing is needed (called \textit{foreground cleaning}). The foreground cleaning utilizes a \textit{greedy} approach for finding the section to garbage collect, which results in the largest amount of space being freed by erasing the section. Background cleaning on the other hand utilizes a \textit{cost-benefit} method that considers the required data blocks to be moved during the \as{gc} of a section, and the resulting free space that is generated by the erasing of the section.

% The cleaning policies implemented in F2FS are greedy~\cite{2010-Bux-Greedy_GC,kawaguchi1995flash}, \af{cb}~\cite{kawaguchi1995flash,1992-Rosenblum-LFS,wu1994envy}, which are used for foreground and background cleaning respectively, and \af{at}~\cite{2020-f2fs-age_threshold,menon1998age}. Greedy \as{gc} selects a \textit{victim section}, from which valid blocks are copied followed by the section being erased, with the lowest utilization of valid data. Therefore, the result of erasing the section provides the largest gain in available space, with the minimum amount of moved data blocks. The \as{cb} policy on the other hand selects the section with the best cost/benefit ratio, calculated as~\cite{1992-Rosenblum-LFS}, with $u$ being the utilization of the section

% \begin{equation*}
%     \text{cost-benefit}=\frac{\text{free space generated}}{\text{cost}}=\frac{(1-u)*\text{age}}{1+u}
% \end{equation*}

% where the cost is based on reading the section (depicted by the 1), plus writing the data in the section (depicted by the utilization). \as{at} based \as{gc} is disabled by default and aims at avoiding that the same section is continuously being selected during \as{gc} if it always provides the best victim. To achieve this, a minimum lifetime is set (by default 7 days), which must be exceeded in order to be selected as the victim. File data that is being moved during \as{gc}, meaning it has not been modified up to the time of \as{gc} and is therefore likely to not be modified frequently in the future, is reclassified as cold data. 

% While \as{f2fs} utilizes a log-structured approach, under increased concurrency such a log becomes a bottleneck. Therefore, to provide increased performance, \as{f2fs} can switch between a single sequentially written log (\textit{normal logging}) to \textit{threaded logging}, where data is written in dirty segments instead of the log. While this approach provides increased logging capacity for concurrent access and avoids garbage collection, it incurs random writing and therefore generates device-level \as{gc}. Similarly, \as{f2fs} provides the option for \af{ssr}, in which it uses invalid blocks in segments, instead of running foreground \as{gc}. While this approach avoid running \as{gc} it similarly generates random write patterns, resulting in device-level \as{gc}.

Given that during \as{f2fs} \as{gc} block addresses are modified, as the cleaning moves still valid blocks to free space, the metadata is not directly updated to depict these changes, in order to provide recovery. Therefore, \as{f2fs} is required to create a checkpoint after each \as{gc} call. Similarly, discard commands, issued after \as{gc} calls to delete data from the flash \as{ssd}, can only be issued after a checkpoint, such that in the case of a necessary recovery, the prior checkpoint still points to existing data that has not been discarded. Once a new checkpoint has been written a discard command can be issued.

\subsubsection{Aligning F2FS Data Layout with the FTL}
To ensure the data grouping of \as{f2fs} is similarly depicted by the mapping of data to flash pages in the \as{ftl}, \as{f2fs} utilizes the separation provided by sections and zones. The goal of sections is to align the \as{f2fs} allocation with the \as{gc} unit of the underlying \as{ftl}. Therefore, the \as{f2fs} \as{gc} occurring at the unit of a section, matches the \as{ftl} \as{gc}. Zones are utilized to avoid sections in different zones to be mapped into the same on-device erase unit by the \as{ftl}. With a mapping that results in different sections of different lifetimes (e.g., hot and cold data) being in written to the same erase unit on the flash \as{ssd}, as is illustrated in \cref{fig:f2fs-zone-blind-allocation}, data grouping is violated, furthermore resulting in possible \as{gc} overheads if only the hot data is updated while the cold data remains valid. This allocation of inadequately separating sections is referred to as \textit{zone-blind allocation}. 

% Figure environment removed

With \textit{zone-aware allocation}, illustrated in \cref{fig:f2fs-zone-aware-allocation}, the zone serves the purpose to provide a large enough separation between particular sections, such that the \as{ftl} similarly separates the written flash pages for the different file data into different erase units. As a result, only data of similar lifetime is within the same erase unit, resulting in reduction of \as{gc} overheads. As the erase unit of flash \as{ssd} is commonly hidden from users, the default \as{f2fs} configuration utilizes a single segment in each section, and a single section in a zone. However, if the flash storage device characteristics are known, these values can be configured.

\subsection{Summary}
The foundational building block of flash \as{ssd} is based on the \textit{flash cell}. However, the characteristics of flash cells, and their utilization to construct mass storage flash \as{ssd} introduce flash management idiosyncrasies, such as having to erase flash prior to updating the data. Due to the resulting lack of in-place updates for flash storage, flash \as{ssd} employ firmware called the \as{ftl}, that hides the complexity of managing the flash, which however introduces garbage collection overheads. File system design, particularly \as{f2fs}, has therefore focused on utilizing flash-friendly data structures and mechanisms to integrate with flash storage. The hiding of flash management idiosyncrasies, specifically the process of \as{gc}, however results in unpredictable performance and high tail latency~\cite{2013-Dean-tail_at_scale,2015-Kim-SLO_complying_ssds,2014-yang-dont_stack_log_on_log}. Therefore, interfaces that expose flash storage characteristics are appearing, with \as{zns} being the first standardized effort. The \textit{zone} interface of \as{zns} eliminates the flash \as{ssd} \as{gc}, moving the responsibility of \as{gc} to the storage software on the host (e.g., the file system).
