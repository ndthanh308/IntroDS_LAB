\section{FIC-4: Flash Parallelism}\label{sec:flash_parallelism}
With the capabilities of flash storage relying largely on increased parallelism, several existing mechanisms are leveraging these. Depending on the level of flash integration, different mechanisms are possible, where at the \as{ssd} integration the host has not control over the possible physical parallel utilization of flash, as the \as{ftl} controls this, however deeper flash integration at the custom and embedded levels provide more possibility. \cref{tab:flash_parallelism} depicts the various mechanisms to aid the utilization of flash parallelism and exploit the physical characteristics of flash.

\begin{table}[!t]
    \centering
    % Update reference format to fit table into column
    \crefformat{section}{\S#2#1#3}
    \crefformat{subsection}{\S#2#1#3}
    \crefformat{subsubsection}{\S#2#1#3}
    \begin{tabular}{||p{40mm}|p{35mm}||}
        \hline 
        Mechanism & File Systems \\
        \hline
        \hline
        \cellcolor{lightgreen!75}Aligning the Allocation Unit (\cref{sec:align_alloc}) & \cellcolor{lightgreen!75}\cite{2020-Tu-URFS,2009-Park-Multimedia_NAND_Fs} \\
        \hline
        Clustered Allocation \& Striping (\cref{sec:clustered_alloc}) & \cite{2012-Min-SFS,dubeyko2019ssdfs,2018-Yoo-OrcFS,2016-Zhang-ParaFS,2011-Park-Multi_NAND,manning2010yaffs,aleph2001yaffs,manning2002yaffs} \\
        \hline
        Concurrency (\cref{sec:concurrency}) & \cite{2021-Liao-Max,2018-Kannan-DevFS,2015-Kang-SpanFS,2019-Lee-Parallel_LFS,2016-Lee-AMF,2016-Zhang-ParaFS} \\
        \hline
    \end{tabular}
    % Reset reference format to before
    \crefformat{section}{Section #2#1#3}
    \crefformat{subsection}{Section #2#1#3}
    \crefformat{subsubsection}{Section #2#1#3}
    \caption{Mechanisms for file systems to exploit flash parallelism capabilities, and the respective file systems that implement a particular mechanism. Green highlighted table cells depict previously discussed mechanisms with their respective section.}
    \label{tab:flash_parallelism}
\end{table}

\subsection{Clustered Allocation \& Striping}\label{sec:clustered_alloc}
As host software has no direct access to flash storage with \as{ssd}, the \as{ftl} implements and manages all device-level parallelism. The possibility for the host to utilize flash parallelism comes from aiding the \as{ftl} in providing large enough \as{io}s such that the \as{ftl} can stripe data across flash chips and channels. This is achieved with \textit{clustered blocks/pages}~\cite{2012-Kim-clustered_blocks}, where blocks or pages on different units (such as blocks on different planes) are accessed in parallel. The \as{ftl} can possibly stripe data across these clustered blocks, given that the \as{io} request is large enough to fill the clustered unit. Such a mechanism aligns with prior discussed aligning of the allocation unit (\cref{sec:align_alloc}) to a physical unit to reduce \as{io} amplification, such as making the file system segment unit a multiple of the flash allocation unit. SFS~\cite{2012-Min-SFS} takes advantage of the achieved device-level parallelism with clustered blocks by aligning segments to a multiple of the clustered block size. During garbage collection SFS ensures that cleaning of segments, that do not have enough blocks to fill the clustered block size, is delayed until enough data is present.

SSDFS~\cite{dubeyko2019ssdfs} utilizes the custom flash integration to map data allocation of segments to the unit of a \af{peb}, where the \as{peb} is split over the parallel unit on the device, such as previously mentioned parallel erasing of flash blocks over channels. Therefore, utilizing \as{peb}s over the parallel unit allows striping writes into a segment over the varying channels, increasing the device parallelism. In order to achieve this, \as{io} requests have to be large enough such that they can be striped across the channel and fill the \as{peb}s mapped to the segment. For this, SSDFS utilizes aggressive merging of \as{io} requests to achieve the larger \as{io}s that can be striped across the parallel units (solving \textbf{\as{fic}7}). Instead of merging \as{io} requests, OrcFS~\cite{2018-Yoo-OrcFS} increases its file system allocation unit to a \textit{superblock} (not to be confused with the file system superblock), which represents a set of flash blocks. These flash blocks are then split over the parallel units of the flash storage by the file system for increased parallelism. The file system utilizes a custom flash integration, and hence is capable of managing the parallelism of the flash storage.

The large allocation unit however introduces increased \as{gc} overheads, and block-level striping has lower performance than page-level striping~\cite{2016-Zhang-ParaFS}. Therefore, ParaFS~\cite{2016-Zhang-ParaFS} implements a 2-D allocation scheme, with page-level striping over the flash channels, where striping is also based on the data hotness, hence having a 2-dimensional allocation scheme. Different groups are assigned for the hotness levels, where writes are issued to the corresponding hotness group striped over the flash channels. Several other file systems implement variations of striping across different parallel units on flash storage~\cite{2011-Park-Multi_NAND,manning2010yaffs,aleph2001yaffs,manning2002yaffs}. These mechanisms are also present in the design of storage applications, such as key-value stores~\cite{zhang2017flashkv,wang2014efficient}.

\subsection{Concurrency}\label{sec:concurrency}
Similar to increasing the data allocation unit, concurrency is a mechanism to exploit the flash parallelism. \as{lfs} design relies on a single append point at the head of the log, in the simplistic implementations, depending on methods such as locking to ensure only one write is issued at the log head. This has a significant impact on the performance where other \as{io}s are idle while a single \as{io} completes. F2FS however suffers from severe lock contention overheads, where the performance of the multi-headed logging is nearly fully deprecated due to the serialization of data updates~\cite{2021-Liao-Max}. In particular, as data has to be written persistently before \as{inode} and other metadata can be written. Furthermore, F2FS suffers from contention of the in-memory data structures, for which it uses reader-writer semaphores for read and write operations from the user (termed \textit{external \as{io} operations}), and reader-writer locks for writing of checkpoints and other metadata (termed \textit{internal operations})~\cite{2021-Liao-Max}. As lock counters are shared among all cores, cache coherence adds a significant overhead that increases with more cores. Max~\cite{2021-Liao-Max} extends F2FS to increase the concurrency scalability with three main modifications.

Firstly, in order to eliminate cache coherence overheads, it introduces a \af{rps} that uses a per-process counter. Secondly, the shared data structures in memory are partitioned by the \as{inode}, such that concurrent accessing does not require locking on parts of the radix tree, but instead on an \as{inode} basis. Lastly, it utilizes multiple independent logs, called a \af{mlog}, which are accessed concurrently. The different between \as{mlog} and multi-headed logging in F2FS is that atomic data blocks are mapped to the same \as{mlog}, eliminating the need to ensure concurrency control across different logs. Ordering for persistence, ensuring data blocks are written before metadata, is delegated to the recovery mechanism using a global versioning number in each inode to identify ordering across \as{mlog}s, and recover the most recent version number in case of a system crash. These mechanisms eliminate much of the needed concurrency control, which sequentialized major parts of operations and hindered multicore scalability.

With this increased concurrency capabilities, the file system can issue more \as{io} requests to the device, allowing to leverage a higher degree of on-device parallelism. Similarly, DevFS~\cite{2018-Kannan-DevFS} utilizes the parallel capabilities by exploiting the high number of \as{io} queues supported by \as{nvme}. It maps \as{io} queues to individual files, allowing single file operations to submit \as{io}s concurrently without interfering on the \as{io} queue, therefore increasing the per-file concurrency as well. Likewise to the concept of \as{mlog}, SpanFS~\cite{2015-Kang-SpanFS} maps files and directories to different \textit{domains}, such that individual domains can be accessed in parallel. Such methods have a higher lock granularity, where concurrency below the lock granularity is not possible, as a single process is holding the lock. Therefore, instead of holding locks for individual inodes or files, preventing concurrent writing to the same inode or file, Lee et al.~\cite{2019-Lee-Parallel_LFS} extend F2FS to utilize \af{rl}, in which ranges of a file are locked, and different ranges can be written concurrently. Therefore, it provides the possibility for intra-file parallelism. 

ALFS~\cite{2016-Lee-AMF}, exploits the flash parallelism by mapping consecutive file system segments to the flash channels and utilizing different \as{io} queues for each flash channel. Similarly, ParaFS~\cite{2016-Zhang-ParaFS} implements \textit{parallelism-aware scheduling}, which also maintains different \as{io} queues for each channel. However, it extends this concept by using a \textit{dispatching phase} and a \textit{request scheduling phase}. The dispatching phase optimizes write \as{io}s by scheduling \as{io} requests to the respective channels based on the utilization of the channel, such that the least utilized channels receives more requests. All requests are assigned a weight, which indicates their priority in the queue, where read requests weight is lower than that of write requests, because of the asymmetric performance of flash storage. During the request scheduling phase the scheduler assigns slices to the read and write/erase operations in the individual queues, such that if the time from the slice of a read operation is up and the queue contains no other read requests, a write or erase is scheduled, based on a fairness formula that incorporates the amount of free space in the block and concurrently active erase operations on other channels. This allows to minimize the erase operations on the flash, giving always free channels to utilize and maintain a fair schedule between write and erase operations.

\subsection{Summary}
Due to the architecture of flash storage providing a high degree of parallelism, numerous methods are employed to leverage these parallel units in order to maximize the performance. Depending on the level of flash integration, particular design choices can be made, such as clustered allocation and striping can be achieved with flash \as{ssd} integration, by providing large write \as{io} requests such that the \as{ftl} can stripe the data across parallel units. With a higher degree of control over the flash storage, file systems can directly rely on utilizing concurrency to leverage the parallelism of flash storage.
