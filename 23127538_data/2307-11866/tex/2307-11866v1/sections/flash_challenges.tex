\section{Challenges of Flash Storage Integration}\label{sec:flash_challenges} 
While \as{ssd} uses the same block interface that is used with \as{hdd}, flash has different characteristics that software must account for to better integrate flash storage. This section details the challenges that arise from integrating flash storage into systems, providing the guidelines along which we dictate the bottom up view of changes in the host storage software stack, up to the file system. We define the challenges to account for the characteristics of flash storage, as well as enhance its integration into host systems. In the case of \as{ssd} devices, these challenges often largely depend on the underlying \as{ftl}, as it is making the final decision, independent of what data placement the host implements, however aiding the \as{ftl} can increase the performance. Embedded devices provide a higher level of host data placement by eliminating the \as{ftl} and directly attaching flash chips to the motherboard. Each challenge is assigned a specific identifier with the \af{fic}, in order to refer back to the specific challenge throughout this literature review. \cref{tab:flash-challenges} summarizes the 6 key challenges arising from flash storage devices.

\begin{table*}[!t]
    \centering
    \begin{tabular}{||p{1cm}|p{35mm}|p{95mm}||}
        \hline 
        ID & Flash Integration Challenge & Description \\
        \hline
        \hline
        \textbf{\as{fic} 1} & Asymmetric Read and Write Performance & Write operations require more time than read operations~\cite{stoica2009evaluating,2022-intel-p,chen2009understanding,2021-osdi-zns+,2017-parity-stream}\\
        \hline
        \textbf{\as{fic} 2} & Garbage Collection & The lack of in-place updates results in flash storage running garbage collection to free space and clear invalid pages. \\
        \hline
        \textbf{\as{fic} 3} & I/O Amplification & The lack of in-place and the required garbage collection introduce write amplification, writing more flash pages than the size of the I/O issued by the host. \\
        \hline
        \textbf{\as{fic} 4} & Flash Parallelism & The architecture of flash utilizes a high degree of parallelism (channels/chips/planes) to be utilized to enhance performance. \\
        \hline
        \textbf{\as{fic} 5} & Wear Leveling & Limited lifetime of flash cells requires careful consideration during writes to ensure flash is worn out evenly across the storage space. \\
        \hline
        \textbf{\as{fic} 6} & I/O Management & Optimizations on the I/O requests, such as merging, aims at leveraging the flash storage capabilities and reducing I/O latency. \\
        \hline
    \end{tabular}
    \caption{Overview of the challenges arising from integrating flash storage. The identifier corresponds to the respective \af{fic} referred to throughout this literature study.}
    \label{tab:flash-challenges}
\end{table*}

\noindent \textbf{\as{fic}1: Asymmetric Read and Write Performance.} On flash storage write operations require more time than read operations~\cite{stoica2009evaluating,2022-intel-p,chen2009understanding,2017-parity-stream,2021-osdi-zns+}, making it important for software to limit write operations. Particularly, frequent small writes that are smaller than the allocation unit, referred to as \textit{microwrites} incur significant performance penalties, and should be avoided where possible. Similarly, methods for enhancing the write performance are important to account for the lower write performance, compared to read performance.

\noindent \textbf{\as{fic}2: Garbage Collection (GC).} While the \as{ftl} hides the flash access constraints from host applications, providing seemingly in-place data updates, it adds the cost of performing garbage collection to free up space. \as{gc} overheads have unpredictable performance penalties for the host system~\cite{2015-Kim-SLO_complying_ssds,2014-yang-dont_stack_log_on_log}, resulting in large tail latency~\cite{2013-Dean-tail_at_scale}. Dealing with, and aiming to minimize required garbage collection for the flash device is a key challenge in integrating flash storage.

\noindent \textbf{\as{fic}3: \as{io} Amplification.} Due to the characteristics of flash avoiding in-place updates of flash pages, writes often encounter \af{wa}. With this the amount of data that is written on the flash storage is larger than the write that is issued by the host system. For example a 4KiB issued write may increase to 16KiB being written on the device, due to possible garbage collection requiring to copy data, resulting in a \as{wa} factor of 4x. \as{wa} furthermore adds to an increase in wear on the flash cells~\cite{2013-Lu-SSD_WA_Lifetime}. \af{ra} similarly is caused by requiring to read a larger amount of data than is issued in the read \as{io} request. \as{ra} most commonly happens when reading metadata in order to locate data, thus requiring an additional read of metadata on top of the request read \as{io} request for the data. This is most often inevitable, as all data requires metadata for management, however this should be kept to a minimum at application-level. Furthermore, minimization of \as{wa} is more important than \as{ra}, since write requests have a higher latency than read requests, and writing has a more significant impact on the flash storage, resulting in increased flash wear. While read requests also incur wear on the flash cell, called read disturbance~\cite{2015-Liu-Read_leveling}, it is not as significant as for write requests. 

% \textbf{\as{fic}2.} \textit{Auxiliary Write Amplification (AWA):} Similar to on-device write amplification, applications can also encounter write amplification, which is referred to as auxiliary write amplification~\cite{2014-Leonardo-NVMKV}. An example of this happens when the application is writing more data than was intended, when for instance required to run application-level garbage collection.

% \textbf{\as{fic}4.} \textit{Auxiliary Read Amplification (ARA):} Similar to read amplification and auxiliary write amplification, when an application is required to read more data than the read size issued. Such an amplification is not caused by the FTL but rather represents application introduced amplification. An example of this is an application required to issue more read requests to locate data, in for instance a file system required to read inodes prior to locating valid data blocks. Therefore, a 4KiB read for a data block requires multiple inode (data directory and file inode) to be located.

\noindent \textbf{\as{fic}4: Flash Parallelism.} With the various possible levels of parallelism on flash storage devices (discussed in \cref{sec:ssd-performance}), exploiting of the various possibilities requires software design consideration to aligning with these. Although the \as{io} scheduling of on-device parallelism, such for channel-level parallelism, is responsibility of the \as{ftl} (on devices at \as{ssd} integration level), the \as{ftl} implements particular parallelism, given that the host \as{io} requests aligning with the possibility of parallelizing the request, such as with large enough \as{io}s to stripe across channels and dies. Embedded device and custom flash integrations have more possibility to manage flash device parallelism at the host software level.

\noindent \textbf{\as{fic}5: Wear Leveling (WL).} Given limited program/erase cycles for flash cells, even wear over the entire device is required to ensures that no specific areas of the device are burnt out faster than others. Similar to flash parallelism, this largely depends on the flash integration level, as the \as{ftl} at the \as{ssd} integration level ensures \as{wl}, however embedded flash integration and custom flash integration is required to place more significance on ensuring even wear across the flash cells. Strongly related to prior flash integration challenges, wear is commonly a result of \as{gc}, which in turn increases the \as{io} amplification, and particularly the  \as{wa} and \as{ra}~\cite{2014-Desnoyers-Analytic_Models_SSD,2009-Hu-WA_SSD}. 

% Figure environment removed

\noindent \textbf{\as{fic}6: \as{io} Management.} As \as{ssd} ships with integrated firmware to expose the flash storage as a block addressable storage device, integration into the current software stack is seamless. \cref{fig:storage_stack} shows the integration of a flash \as{ssd} into the Linux kernel storage stack. Since flash storage devices are significantly faster than prior storage technologies, such as \as{hdd}, the storage software stack becomes the dominating factor in \as{io} latency~\cite{2010-Caulfield-Moneta,2012-Caulfield-Fast_User_Space_Access}. One particular optimization for performance of \as{io} requests to flash storage devices is provided by an \as{io} scheduler, deciding when to issue \as{io} requests to the storage device. As is visible in \cref{fig:storage_stack}, the block \as{io} layer implements various schedulers with different functionality. Providing a ranging degree of optimizations for \as{io} requests, such as varying scheduling policies and merging of \as{io} requests, or possible reordering, specific configurations are favorable to increase performance with flash storage. Particularly the utilization of multiple queues, with multiple software and hardware dispatch queues (visible in the \texttt{blk-mq} configuration of the block \as{io} layer), allows better exploitation of flash storage capabilities, and avoids certain Linux kernel overheads. Furthermore, evaluating mechanisms that reduce the latency of \as{io} operations, and particularly write \as{io} operations.

\subsection{Flash Integration Organization}
\begin{table}[!t]
    \centering
    \begin{tabular}{||p{35mm}|p{38mm}||}
        \hline 
        Integration Level & File Systems \\
        \hline
        \hline
        \as{ssd} Flash Integration & \cite{2015-Changman-f2fs,2021-Gwak-SCJ,2012-Min-SFS,2014-Lu-ReconFS,2020-Tu-URFS,2019-Yoshimura-EvFS,2022-Jiao-BetrFS,jannen2015betrfs,huang2013improve,2019-Liu-fs_as_process,2018-Kannan-DevFS,2021-Liao-Max,2015-Kang-SpanFS,2022-Oh-exF2FS,rodeh2013btrfs} \\
        \hline
        Custom Flash Integration & \cite{kawaguchi1995flash,josephson2011direct,2020-Wu-DualFS,2018-Rho-Fstream,dubeyko2019ssdfs,lee2014refactored,2019-Lu-Sync_IO_OCSSD,2016-Lee-AMF,2018-Yoo-OrcFS,2016-Zhang-ParaFS,2021-Qin-Atomic_Writes} \\
        \hline
        Embedded Flash \newline Integration & \cite{2006-Lim-NAND_fs,hunter2008brief,2009-Sungjin-FlexFS,2008-Jung-ScaleFFS,woodhouse2001jffs,park2013enffis,nahill2015flogfs,schildt2012contiki,2015-Park-Suspend_Aware_cleaning-F2FS,2009-Zuck-NANDFS,2009-Park-Multimedia_NAND_Fs,2007-Hyun-LeCramFS,2021-Ji-F2FS_compression,2011-Park-Multi_NAND,manning2010yaffs,aleph2001yaffs,manning2002yaffs,engel2005logfs,2008-Kim-DFFS,park2006flash,2020-Yang-F2FS_Framentation,2022-Lee-F2FS_Address_Remapping,2011-Lim-DeFFS,2014-Hyunchan-O1FS,2022-Zhang-ELOFS,2020-Zhang-LOFFS,kim2006mnfs,2009-Tsiftes-Coffee_FS} \\
        \hline
    \end{tabular}
    \caption{Classification on the flash integration level utilised for the file systems evaluated in this literature study.}
    \label{tab:integration_levels}
\end{table}

With the different possible levels for integration of flash storage (recall \cref{fig:flash_integrations}), and while mechanisms for solving flash integration challenges are frequently applicable at various integration levels, several of the mechanisms we present require deeper integration of flash storage, levering increased control, in order to be implemented. For instance, the incorporation of the various levels of on-device parallelism is not directly possible at the \as{ssd} integration level, as the \as{ftl} hides the parallelism on the physical device from the host system. The custom and embedded level flash integration provide the host with more possibility to manage these. In order to separate the possibility of mechanisms to be implemented with a particular flash integration level, \cref{tab:integration_levels} provides a classification of each evaluated file system in this literature study to the respective integration level. During the evaluation we present the different mechanisms to solve a \as{fic}, and indicate which file systems utilize these. Therefore, when considering the feasibility of a mechanism for a particular flash integration consult this table to see its applicability. Note that file systems are not limited to the classification we provide, as for instance file systems designed for \as{ssd} flash integration also work on some embedded flash integration. However, we utilize only a single classification for each file system to avoid confusion. Exceptions are made only in specific cases where an existing file system is adapted for a different flash integration level.

We divide the discussion of mechanism by the \as{fic} for which the evaluated study presents a novel solution. This implies that mechanisms that solve multiple \as{fic} are discussed in detail in the first section they appear in, however are also mentioned in all latter sections for the \as{fic} that the mechanism solves. Therefore, each \as{fic} section contains a table of the respective mechanisms presented to solve that particular \as{fic}, along with a reference to the corresponding section of its detailed discussion.
