\section{Failure Consistent Operations}\label{sec:failure_consistency}
An important aspect of file systems is to ensure that in the case of power failure or system crashes, the file system state and its data remain in a consistent state, and can be recovered upon a subsequent mount. While we do not classify it as a challenge of flash storage, we discuss the methods for ensuring failure consistent operations for flash. Most commonly used mechanisms for ensuring this are journaling, CoW, and checkpointing. However, journaling suffers from having to write data twice, once for the metadata log and again for the data log, to ensure full consistency~\cite{pillai2017application,xu2016nova}, also referred to as the double-write problem~\cite{2018-Kannan-DevFS}. Therefore, file systems commonly only provide metadata consistency by logging only the metadata. Furthermore, power failure is a concern with flash storage, as it has capacitors that can flush parts of the on-device memory buffers, unless more expensive capacitors are used that can flush the entirety of the memory buffers. In the case of power failure, partially written data or metadata updates can result in the file system being in an inconsistent state.

While checkpointing, \as{cow}, and other consistency mechanisms aim to handle these failures and provide recovery after failure, other methods exist for providing interfaces that eliminate partial operations. Particularly, the design of flash storage, such as the available \as{oob} space of pages, provides beneficial options for such implementations. \cref{tab:consistency} depicts the methods for ensuring failure consistency with flash storage, and the file systems implementing such methods.

\begin{table}[!t]
    \centering
    % Update reference format to fit table into column
    \crefformat{section}{\S#2#1#3}
    \crefformat{subsection}{\S#2#1#3}
    \crefformat{subsubsection}{\S#2#1#3}
    \begin{tabular}{||p{40mm}|p{35mm}||}
        \hline 
        Mechanism & File Systems \\
        \hline
        \hline
        Atomic Writes (\cref{sec:atomic_writes}) & \cite{2021-Qin-Atomic_Writes,cheon2017exploiting,f2fs-atomic-write} \\
        \hline
        Transactions (\cref{sec:transactions}) & \cite{2022-Oh-exF2FS} \\
        \hline
    \end{tabular}
    % Reset reference format to before
    \crefformat{section}{Section #2#1#3}
    \crefformat{subsection}{Section #2#1#3}
    \crefformat{subsubsection}{Section #2#1#3}
    \caption{Failure consistency mechanisms to for flash storage, and the respective file systems that implement a particular mechanism.}
    \label{tab:consistency}
\end{table}


% Nowhere else are these mentioned, removed prior section on mount time
% O1FS~\cite{2014-Hyunchan-O1FS} decreases the crash recovery time, when remounting a file system, by dividing the storage space into working areas, where all file system data is going into one working area at a time. This implies that superblock and additional metadata is duplicated for each working area, however on a system crash it requires only to check the latest working area and recover the file system from it. This allows to significantly decrease the required complexity for recovering from the system crash, due to the smaller region that is required to be scanned. A similar decrease of recovery time at mount time for LFS is achieved by recording file system states for faster metadata checks, which is presented by Wu et al.~\cite{wu2006efficient}. It relies on maintaining an in-memory record, called the \textit{log record manager}, of the file system records, including the important data structures to the file system. When persisting the log records onto flash, the file system can upon remounting more quickly check the log records to reconstruct the file system state, with the present data structures in the log record, instead of reconstructing them.

\subsection{Atomic Writes}\label{sec:atomic_writes}
A possible method for ensuring that operations are not completed partially is through atomic writes. This is important to file systems as an update of data requires its respective metadata to also be updated. Failure of one should result in the other not being completed. A variety of flash devices support atomic writes through mechanisms such as in \as{ftl}~\cite{ouyang2011beyond} and through user-programmable \as{ssd}~\cite{Seshadri-2014-Willow}, however it can also be implemented in the file system itself. F2FS supports multi-block atomic writing, which allows updating multiple file system blocks in a single \textit{ioctl} command~\cite{2021-Qin-Atomic_Writes,cheon2017exploiting,f2fs-atomic-write}. ReconFS~\cite{2014-Lu-ReconFS} provides multi-page atomicity by using a flag in each page to indicate if it is valid. This is achieved by writing a 1 in the head of the last flash page of the metadata updated, and all other pages have a 0 in the head. Therefore, in the case of power failure, when the file system is reconstructed, any metadata pages in between two pages with a 1 in the head (depicting two ending page updates) are valid pages, and in the case when the log has no page with the flag set to 1, the update failed and all pages after the last 1 flag are invalid. 

Similarly, Qin et al.~\cite{2021-Qin-Atomic_Writes} showcase the utilization of the \as{oob} area on flash with \as{ocssd} in order to store metadata for ensuring data integrity in the No-Barrier File System (NBFS), which is extending F2FS. The file system uses \af{dnchain} to ensure consistency measurements. \as{dnchain} is a linked list of all the blocks in an atomic operation, which are stored through a pointer in the \as{oob} space, thus representing a linked list of pointers in an atomic operation. The last block in the \as{dnchain} points to itself to indicate the end of the linked list and the atomic operation. Furthermore, each block contains a checkpoint version, allowing the recovery after a failure to traverse the \as{dnchain} and identify if an invalid checkpoint version number is present, which indicates a failed atomic operation. As the initial writing of journal or \as{cow} is done in memory and later flushed to the flash storage, mechanisms such as failure-atomic \textit{msync()}~\cite{park2013failure}, provide atomicity in such operations.

\subsection{Transactions}\label{sec:transactions}
An additional approach for ensuring consistency is to use transactions, enforcing an ``all-or-nothing`` mechanism that either writes all data or no data at all if there are any failures during the transaction. Transactional support can stem from transactional block devices, such as TxFlash (Transactional Flash)~\cite{prabhakaran2008transactional} LightTX with embedded transaction support in the flash storage FTL~\cite{2013-Lu-LightTX,lu2015high}, and similar block device implementations that expose transaction support~\cite{huang2012bvssd,kang2013x,2013-Coburn-SSD_Txs}. Additions to file systems can similarly implement transactions for block devices without transaction support. exF2FS~\cite{2022-Oh-exF2FS} is an extension on F2FS that provides it with support for transactions. In order to support transactions, exF2FS implements several features. Firstly, with transactions relying on either all updates being present or no updates at all, the \as{gc} policy is adapted such that the \as{gc} module cannot reclaim flash pages that contain data from a transaction that is in progress. If such a page is garbage collected without the transaction having finished, it can be recovered and thus violates the all-or-nothing mechanism. 

For this, exF2FS implements \textit{shadow garbage collection}, which prohibits the \as{gc} module from using pages involved in pending transactions. Secondly, in order to provide large scale transactions with multiple exF2FS maintains \textit{transaction file groups}, which are kernel objects that identify the set of files involved in a transaction. Lastly, it implements \textit{stealing}, which allows dirty pages of pending transactions to be committed to the flash storage, however do not allow it to be garbage collected until the transaction completed. This mechanism is referred to as \textit{delayed invalidation and relocation record}, and the process of allowing dirty pages to be evicted in uncommited transactions is referred to as \textit{stealing}~\cite{2022-Oh-exF2FS}.

\subsection{Flash Failures}
Flash failures are another important aspect of file systems on how they manage and recover from these failures. A study into file system reliability showed that 16\% of injected faults resulted in an unmountable file system, which furthermore was not fixable by a file system checker~\cite{2020-Jaffer-SSD_Errors}. Especially, as the density of flash is increasing, where more layers of NAND is stacked on top of each other, resulting in a decrease in the flash reliability~\cite{meza2015large}. While flash employs \as{ecc} to handle correcting of data, there are errors that are not fixable such as the \textit{uncorrectable bit corruption}~\cite{boboila2010write,belgal2002new,brand1993novel}, which can be caused by flash wear, read disturbance, and other factors~\cite{2020-Jaffer-SSD_Errors}. We do not go into detail of flash induced errors, for a detailed evaluation on flash errors recovery mechanisms consult \cite{2020-Jaffer-SSD_Errors,gatla2018towards}. Jaffer et al.~\cite{2020-Jaffer-SSD_Errors} provide several guidelines for file systems to enhance reliability with flash errors, including adding more sanity checks, especially of metadata, and finding better tradeoffs between checksums and the checksum granularity, where a large checksum granularity can result in significant data loss if it has an unrecoverable invalid checksum, and a small checksum adds overheads on the needed space and performance. While file system checkers aim to solve a degree of faults, they are no panacea to fixing corrupted file systems~\cite{gatla2018towards} and require better failure handling in the file system itself.

\subsection{Summary}
The importance of failure consistent operations for storage systems has resulted mechanisms, such as atomic writes, to utilize the available \as{oob} area of flash pages for metadata to ensure consistency in the case of failures. Similarly, in order to avoid flash errors, checksum methods have been introduced to alleviate errors as a result of flash failures.
