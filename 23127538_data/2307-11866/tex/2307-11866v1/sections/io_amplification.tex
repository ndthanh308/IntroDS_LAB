\section{FIC-3: \as{io} Amplification}\label{sec:io_amplification}
Several of the applied mechanisms for dealing with asymmetric flash performance and garbage collection are key mechanisms to eliminate \as{io} amplification. \cref{tab:io_amplification} shows the different mechanisms that can be applied to lower the various types of \as{io} amplification, including \af{wa}, \af{ra}, and \af{sa}. These include the benefit of write buffering, which in the case of write requests that are smaller than the flash allocation unit, avoids the unnecessary \as{wa} to fill the flash allocation unit. Similar buffering as is employed in \as{wods}, such as the $\text{B}^\varepsilon$-trees, allows decreasing the \as{wa}. Another key mechanism is reducing the generated write traffic, which minimizes \as{wa}, \as{ra}, and \as{sa} through deduplication, compression, delta-encoding, and virtualization. Similarly, the discussed methods of grouping data, avoiding fragmentation, allows reducing the \as{ra} to locate data, and furthermore limits \as{gc} overheads and \as{gc} caused \as{wa}. Throughout this section we evaluate the additional methods for limiting the various types of \as{io} amplification.

\begin{table}[!t]
    \centering
    % Update reference format to fit table into column
    \crefformat{section}{\S#2#1#3}
    \crefformat{subsection}{\S#2#1#3}
    \crefformat{subsubsection}{\S#2#1#3}
    \begin{tabular}{||p{30mm}|p{20mm}|p{20mm}||}
        \hline 
        Mechanism & Amplification Type & File Systems \\
        \hline
        \cellcolor{lightgreen!75} Write Optimized Data Structures (\cref{sec:wods}) & \cellcolor{lightgreen!75}\as{wa} & \cellcolor{lightgreen!75}\cite{2020-Tu-URFS,hunter2008brief,rodeh2013btrfs,2022-Jiao-BetrFS,dubeyko2019ssdfs} \\
        \hline
        \cellcolor{lightgreen!75} Write Buffering (\cref{sec:write_buffering}) & \cellcolor{lightgreen!75}\as{wa} & \cellcolor{lightgreen!75}\cite{park2006cflru,jo2006fab,park2013enffis,2018-Kannan-DevFS,josephson2011direct,2019-Lu-Sync_IO_OCSSD,2010-Josephson-DFS,2011-Park-Multi_NAND} \\
        \hline
        \cellcolor{lightgreen!75} Reducing Write Traffic (\cref{sec:reduce_write_traffic}) & \cellcolor{lightgreen!75}\as{wa}, \as{ra}, \as{sa} & \cellcolor{lightgreen!75}\cite{huang2013improve,2011-Lim-DeFFS,woodhouse2001jffs,dubeyko2019ssdfs,2021-Ji-F2FS_compression,2007-Hyun-LeCramFS,ning2011design,2009-Zuck-NANDFS,2022-Lee-F2FS_Address_Remapping} \\
        \hline
        \cellcolor{lightgreen!75} Data Grouping (\cref{sec:data_grouping}) & \cellcolor{lightgreen!75}\as{wa}, \as{ra} & \cellcolor{lightgreen!75}\cite{2006-Lim-NAND_fs,2022-Zhang-ELOFS,2020-Zhang-LOFFS,2015-Changman-f2fs,2018-Rho-Fstream} \\
        \hline
        \cellcolor{lightgreen!75} \as{gc} Policies (\cref{sec:gc_policy}) & \cellcolor{lightgreen!75}\as{wa}, \as{ra} & \cellcolor{lightgreen!75}\cite{2012-Min-SFS,2018-Yoo-OrcFS,2021-Gwak-SCJ,2015-Park-Suspend_Aware_cleaning-F2FS} \\
        \hline
        Space Optimized Data Structure (\cref{sec:space_optimized_ds}) & SA & \cite{2014-Lu-ReconFS,2015-Changman-f2fs,2021-Liao-Max} \\
        \hline
        \as{wa} with Coarse Granularity Flash Mappings (\cref{sec:coarse_gran}) & \as{wa} & \cite{2018-Yoo-OrcFS,kim2006mnfs,2018-Yoo-OrcFS,lee2007log} \\
        \hline
        Reverse Indexing (\cref{sec:reverse_indexing}) & \as{wa} & \cite{2014-Lu-ReconFS}\\
        \hline
    \end{tabular}
    % Reset reference format to before
    \crefformat{section}{Section #2#1#3}
    \crefformat{subsection}{Section #2#1#3}
    \crefformat{subsubsection}{Section #2#1#3}
    \caption{Mechanisms for file systems to deal with \as{io} amplification caused by flash storage integration, and the respective file systems that implement a particular mechanism. Green highlighted table cells depict previously discussed mechanisms with their respective section.}
    \label{tab:io_amplification}
\end{table}

\subsection{Space Optimized Data Structures}\label{sec:space_optimized_ds}
Similar to the design of a \as{wods}, data structures with particular focus on optimally utilizing available space are a mechanism to deal with \as{sa}. A commonly applied method for file systems to optimize space utilization is to possibly embed file data in the \as{inode}. Commonly the allocation of the file system \as{inode} occupies at least a block, such as 4KiB in F2FS, which however is more space than file metadata requires. Therefore, several bytes ($\sim$3.4KB in F2FS) are free, which are used to inline file data in the inode. This particularly allows for small files to entirely fit into the \as{inode}, avoiding writing the \as{inode} and leave the unused space empty, and additionally write an additional data block, which also has free space. Different to inline data, ReconFS~\cite{2014-Lu-ReconFS} uses an \as{inode} with a size of 128B, allowing to place numerous inodes in a single flash page. With such an \as{inode} size, writing each \as{inode} change directly requires filling the flash page with unnecessary data. Therefore, ReconFS implements a \textit{metadata persistent log}, in which metadata changes are logged and compacted to align with pages, and are only written back to the storage when evicted or checkpointed, in order for the file system to remain consistent.

Similar to effective tree-based \as{wods}, the radix tree is a space optimized tree variant, that is commonly used as the directory and inode tree for file systems~\cite{2015-Changman-f2fs,2021-Liao-Max}. The directory tree is commonly constructed and maintained in memory and written to the persistent flash storage. Its space optimization revolves around merging of nodes that have a single child with that child node. This eliminates the need for an individual node that is assigned to each child, lowering the space requirement. As a result, the radix tree is also referred to as a compressed tree, due to the compression of single child nodes.

\subsection{Coarse Granularity Flash Mappings}\label{sec:coarse_gran}
A similar goal of file systems is to reduce the amount of memory that is required for the mapping table to maintain the \as{l2p} mappings. \as{l2p} mappings are commonly persisted periodically, from the storage device \as{ram} to the flash storage, such that in the case of system shutdown or the device is unplugged, upon reconnection the mapping information can be recovered. Hence, the mapping information similarly requires flash pages to be stored. A common solution is to increase the granularity of the mapping table (e.g., block-level mapping instead of page-level mapping), requiring fewer mappings. MNFS~\cite{kim2006mnfs} manages flash storage with page-level and block-level mapping, depending on the update frequency. Metadata is updated more frequently and therefore utilizes a page-level mapping compared to larger mapping granularity for data. OrcFS~\cite{2018-Yoo-OrcFS} similarly utilizes a page-level for metadata, and a superblock-level mapping for data, which represents several flash blocks. The allocation unit is called a superblock as it consists of multiple blocks (not to be confused with the file system superblock). Furthermore, logical addresses are mapped to the same physical addresses in the data partition, requiring no mapping table, and file system sections are aligned to the superblock unit. Therefore, OrcFS only requires a block allocation information for each file in the superblock, which are stored in the inode block in the metadata area. 

However, this comes at the cost of having a larger allocation unit, and if a host write is smaller than the allocation unit it causes \as{wa}, due to the partial flash page write when the flash page size and the allocation unit are not aligned~\cite{2018-Yoo-OrcFS,lee2007log}. OrcFS~\cite{2018-Yoo-OrcFS,lee2007log} implements \textit{block patching} to solve this issue. It takes write requests that are smaller than the flash page size and pads the remaining space with dummy data to align the write request to flash page size. This mechanism avoids copying data if a flash page is partially written, and the next \as{lba} in the same flash page is written, which triggers a copy of all \as{lba}s in the flash page followed by writing the \as{lba} for the new write. For instance, for a flash page containing 4 \as{lba}s, if \as{lba}s 1-3 are written by one request, the first 3 \as{lba}s are mapped to the data and the fourth holds dummy data, such that the page is fully filled. If a second request to \as{lba} 4 is issued, it cannot fill the flash page as it has already been written. Therefore, to write the newly written data after the already written \as{lba}s, it must copy \as{lba}s 1-3, append the new write to \as{lba} 4, and write the 4 \as{lba}s to a new flash page. Adding of dummy data to fill pages reduces the \as{wa}, which would be caused by copying of all data in the flash page, as it now avoids copying the added dummy data on consecutive writes. While reduction in \as{wa} are presented, the adding of dummy data nonetheless adds \as{wa} to fill the flash page. However, as latter updates require less data written, and the importance of data grouping indicates, maintaining related data in the same flash page is more beneficial and possibly decreases future \as{wa}. Related data remains in the same flash page, as only the valid data in flash pages is copied on writes, as opposed to copying the entire flash page, introducing copied dummy data.

\subsection{Reverse Indexing}\label{sec:reverse_indexing}
As file system metadata is commonly maintained in a tree-based data structure, updates to metadata in the leaf nodes can propagate changes to the root node, known as the Wandering Tree Problem~\cite{bityutskiy2005jffs3}. Due to the update of leaf metadata, such that when file data is modified, the metadata points to the new location of the file data, causing new metadata to be written, which in turn requires its parent to be updated to point to the new location of the metadata. This propagates up to the root note, causing significant \as{wa}. F2FS utilizes a table based indexing, with the \as{nat}, such that only a table entry is required to change to update the data location, and metadata points to the table entry to locate the data. ReconFS~\cite{2014-Lu-ReconFS} utilizes an inverted indexing tree, which also avoids the wandering tree problem. With such a tree, each node points to its parent node, instead of the parent node pointing to a child node. Therefore, upon address change of a child node, the parent does not need to be modified, since the child node points upwards to the parent node. Similar mechanisms are utilized in \as{ftl} design~\cite{2013-Lu-Flash_Lifetime_Reduce_With_WA}, where indexing data is written in the \as{oob} space of the flash page from the data, in order to locate its metadata. In order to avoid increased scan times on failure recovery, which can no longer traverse the tree from the root, the updated pages are tracked to locate the most recently updated valid page, which is then periodically included in the checkpointing to ensure consistency. 

\subsection{Summary}
The introduced \as{io} amplification of flash storage, particularly a result of \as{gc}, requires careful consideration to reduce the write requests, such that the flash device lifetime can be extended. Several of the previously discussed mechanisms, such as reducing write traffic and utilizing effective \as{gc} policies, aid in reducing the \as{io} amplification, however furthermore particular data structures optimized for space utilization similarly provide efficient methods for reducing \as{io} amplification.
