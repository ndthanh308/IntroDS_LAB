\section{SSD Flash Integration}\label{sec:ssd_integration_evaluation}
With conventional SSDs, the FTL has full control over the underlying flash media. As a result, the possibility for file systems to fully incorporate each flash challenge is limited. However, particular design decisions in the file system aid the performance of the FTL and help leverage the flash characteristics. Therefore, this section covers the file systems that are particularly built on conventional flash-based SSDs (recall Figure~\ref{fig:flash_integration_ssd}), with an FTL exposing a conventional block interface. Table~\ref{tab:ssd_integration_comparison} shows a comparison of all file systems meeting this criteria, evaluating if the file system provides novel methods for handling of certain flash characteristics. As all file systems are log-structured file systems, which is a write optimized data structure due to its append only nature, which further inherently provides basic wear leveling by writing everything sequentially, the WL column is only marked on file systems that further extend this mechanism with additional novel methods.

\begin{table*}[!t]
    \centering
    \begin{tabular}{||p{15mm}||p{8mm}|p{15mm}|p{5mm}|p{20mm}|p{15mm}|p{5mm}|p{15mm}|p{15mm}||}
        \hline 
        & & \multicolumn{7}{|c||}{Flash Challenge} \\
        \hline
        File \newline System & Year & Read/Write \newline Asymmetry & GC & I/O \newline Amplification & Flash \newline Parallelism & WL & Software \newline Stack & I/O \newline Scheduling \\
        \hline
        \hline
        F2FS\cite{2015-Changman-f2fs} & 2015 & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark \\
        \hline
        F2FS + SCJ\cite{2021-Gwak-SCJ} & 2021 & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark \\
        \hline
        SFS\cite{2012-Min-SFS} & 2014 & \xmark & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark \\
        \hline
        ReconFS\cite{2014-Lu-ReconFS} & 2014 & \xmark & \xmark & WA \& SA & \cmark & \xmark & \xmark & \xmark \\
        \hline
        URFS\cite{2020-Tu-URFS} & 2020 & \cmark & \cmark & WA & \xmark & \xmark & \cmark & \cmark \\
        \hline
        EvFS\cite{2019-Yoshimura-EvFS} & 2020 & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \cmark \\
        \hline
        BetrFS\cite{2022-Jiao-BetrFS} & 2022 & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
        \hline
        CSA-FS\cite{huang2013improve} & 2013 & \cmark & \xmark & SA & \xmark & \xmark & \xmark & \xmark \\
        \hline
        DashFS\cite{2019-Liu-fs_as_process} & 2019 & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \cmark \\
        \hline
        DevFS\cite{2018-Kannan-DevFS} & 2018 & \xmark & \xmark & \xmark & \cmark & \xmark & \cmark & \cmark \\
        \hline
        % From here on papers added from original FS Design Considerations classification
        Max\cite{2021-Liao-Max} & 2021 & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark & \cmark \\
        \hline
        SpanFS\cite{2015-Kang-SpanFS} & 2015 & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark & \cmark \\
        \hline
        exF2FS\cite{2022-Oh-exF2FS} & 2022 & \cmark & \cmark & WA & \cmark & \xmark & \xmark & \xmark \\
        \hline
    \end{tabular}
    \caption{Evaluated file systems for flash-based SSD integration (with conventional FTL) and the flash challenges (Section~\ref{sec:opt_flash}) the file system solves. A \cmark\hspace{0.5mm} indicates that a file system implements a mechanism to handle the particular flash challenge, however it does not indicate to what extent it is handled or how successful it is handled. If a file system does not have a name and/or it is an extension on an existing file system, the authors' names are used as the file system name. As the majority of file systems are LFS-based designs, the table indicates if the file system employs a novel method in addition to what is required of LFS. For instance, the GC column contains a \xmark\hspace{0.5mm} if the file system simply implements greedy or cost-benefit GC, and a \cmark\hspace{0.5mm} if a new GC policy or new mechanisms of managing GC are presented.}
    \label{tab:ssd_integration_comparison}
\end{table*}

\subsection{Read/Write Asymmetry}\label{sec:ssd_rw_asym}
The characteristic of flash having lower write than read performance requires that data structures for flash are write optimized. Such data structures which are optimized for write operations are referred to as \textit{write optimized indexes} (WOI). With the addition of the missing support for in-place updates on flash, the best suiting data structure for flash-based file systems is a log-based structure. As a result, all file systems discussed in this section are LFS. The nature of a LFS being append only writes accounts for the lower write performance on flash. 

However, file systems should further avoid small writes, referred to as microwrites. Particularly, as file systems write in units of blocks, which commonly are 4KiB, small writes require a full unit to be filled. The majority of file systems provide the possibility for inline data in inodes, such that the inode data, which is written regardless, has a small capacity to include data. However, this still requires that for small files inodes are directly written to flash. Therefore, several schemes that involve buffering and caching of data in memory, before flushing to the flash storage, provide increased write performance, and additionally increase overall performance.

While buffering of I/O prior to flushing to flash storage provides performance gains, buffers are often limited in size and are much smaller than the persistent storage, therefore numerous buffering and eviction algorithms exists, in particular algorithms designed for flash storage characteristics. Clean first least recently used (CFLRU)~\cite{park2006cflru} is an extension on the LRU algorithm. It splits the buffer into two segments, one as the working region in which recently accessed pages reside, which are managed in most recently used (MRU) fashion, therefore depicting the frequently accessed pages. The second region, called the clean-first region contains the less commonly accessed pages in LRU fashion. On eviction (e.g., when writing a new page and freeing space in the buffer), it first attempts to evict a clean page, rather than a dirty page from the clean-first region, as this does not require a flush of the dirty data to the flash storage, and only resorts to evicting dirty pages as last resort. Flash aware buffer (FAB)~\cite{jo2006fab} is another buffering mechanism particular to flash. It organizes pages in blocks and upon eviction flushes entire blocks to the flash storage, issuing larger I/Os and aligning better to the flash erase unit. 

Similar to write optimized indexes, \textit{write optimized dictionaries} (WODs) provide increased write performance. These are implemented through data structures such as LSM trees~\cite{sears2012blsm} and $\text{B}^\varepsilon$-Trees (a write optimized version of the B-Tree). BetrFS~\cite{2022-Jiao-BetrFS} (not the original BetrFS file system but an updated, flash-optimized version) implements such a $\text{B}^\varepsilon$ as the metadata storage for indexing data. It is implemented in the Linux Kernel as a key-value store, based on TokuDB~\cite{tokudb}, which exposes a key-value addressable interface. The benefit of $\text{B}^\varepsilon$-Trees is that the nodes have a larger (2-4MiB) sequentially written log, batching updates into larger units and thus avoiding small updates. Updates are placed in the leaf nodes. Initially all updates go into the root node message log, which when full gets flushed to its child nodes, similar to the internals of LSM tree compaction. Updates are cached in the VFS page cache , and are flushed to the $\text{B}^\varepsilon$-Tree. While WODs provide optimized write performance by batching updates, read requests require reading an entire node (2-4MiB), causing small read requests to suffer from significant read amplification (\textbf{FC2}).

With the lower write performance on flash storage, CSA-FS~\cite{huang2013improve} aims to reduce the write traffic on the device by utilizing deduplication (in detail discussed in Section~\ref{sec:deduplication}). Storage often contains duplicate data (e.g., archival copies of data, or backup copies). However, this requires additional space for the same data. Therefore, CSA-FS implements deduplicaiton by calculating the MD5 digest (MD5 is a hash function applied to the input) and looks up the result in a hash table which provides the corresponding LBA of the block. If it already exists and the user requested a new file with it, the new inode simply uses the given LBA from the hash table, instead of duplicating the data again. Furthermore, for reducing the writes of semantic data (superblock, descriptors, bitmaps, and tables), CSA-FS uses delta-encoding (discussed in detail in Section~\ref{sec:compression_encoding}) for updates. Delta-encoding finds the difference of the data and its original data, storing only the changes that are made instead of the entire new data. Both of these mechanisms, deduplication and delta-encoding reduce the amount of data that is written, which in turn provides a longer lifetime of the flash storage (\textbf{FC5}) and enhances space amplification by minimizing data that is written (\textbf{FC3}).

\subsection{Garbage Collection}
A significant challenge of flash storage is the garbage collection overheads. While the FTL is running garbage collection, the file system can experience significant performance drops and suffer from high and unpredictable tail latency. While the FTL is in control of managing the garbage collection, there are methods for improving the GC required by minimizing the amount of data that is being invalidated, and thus lowering the required erasing. URFS~\cite{2020-Tu-URFS} aligns the data allocation unit for large files to the erase unit, allowing files to be erased as a single unit, limiting required GC. However, as the erase unit of flash can be several MBs, this is only applicable to large files.

\subsubsection{Data Grouping}
Similar to F2FS, an effective method for this is the grouping of data based on the access frequency. This grouping allows to separate the hot data, which is frequently accessed and overwritten from cold data, which is infrequently modified. In the scenario where hot and cold data is not separated and resident in the same block, the frequently modified hot data will repeatedly invalidate the block, which will cause the garbage collection to have to move the valid data to a new free space. This cold data is then again resident in a block with hot data, which will be invalidated, requiring to move the cold data again. This constitutes to significant garbage collection, where cold data is constantly moved. Jung et al.~\cite{jung2010process} propose a novel hot/cold data grouping, which unlike simplistic data grouping that classifies based on the access frequency, that further periodically classifies based on the process ID.

SFS~\cite{2012-Min-SFS} implements such a data grouping to separate hot and cold data into different segments, by utilizing an eager data grouping on writing during segment cleaning. Emphasizing that lazily grouping data on segment cleaning fails to account for the asymmetric write performance of SSDs, SFS utilizes a comprehensive grouping classification mechanism. SFS assigns a hotness to blocks and to groups, which represent a collection of blocks with similar temperature. Using an \textit{iterative segment quantization} algorithm, based on k-means clustering~\cite{hartigan1979algorithm}, SFS partitions the segments into a number of groups to which blocks are assigned based on the quantization model, that finds the center of groups through iterative search as the data in segments and their hotness changes. Such a model allows to provide a better distribution of hotness in the particular data.

Shafaei et al.~\cite{2016-Shafaei-WA_extent_temp} identify that the majority of hot/cold data grouping methods fail to account for the accuracy in the hot/cold grouping mechanism, as well as relying on an individual classification of each LBA, making the management of increasingly larger flash storage difficult. Therefore, an extent-based temperature identification mechanism is proposed, based on the density-based stream clustering problem~\cite{jia2008grid,chen2007density,forestiero2013single,isaksson2012sostream}, which is a common approach of classification in artificial intelligence and stream processing, however has not been applied to storage before. The density-based stream clustering groups/clusters data in a one dimensional space. Applying this method to storage, the one-dimensional space is the range of LBAs, and extent-based clustering splits the available space into a number of extents for a group cluster. Initially, the entire space is a single extent and is expanded and merged (merging of extents with the same group classification) as writes occur. Such a grouping allows a more detailed grouping, compared to hot/cold grouping, reducing possible garbage collection penalties. 

Numerous additional hot/cold grouping schemes exist, such as request size-based prediction scheme~\cite{chang2008hybrid}, which bases its classification on the sector size of the request. This classification is based on the assumption that write requests follow a bi-modal distribution, large number of small requests and a large number of very large requests, incorporating the tendency of file systems to incur a large number of metadata updates. A table-based classification model~\cite{hsieh2005efficient} is another mechanism that maintains a history of accesses for each LBA, allowing precise access frequency modeling for individual LBAs. This however comes at a high overhead cost, as an entry for each LBA is needed, which becomes increasingly expensive as flash storage grows. Another mechanism is the two-level LRU classification~\cite{chang2002adaptive}, which maintains two LRU lists, of which an initial LBA access stores the LBA in the first list, and a subsequent access moves it to the next list, which is referred to as the hot list. Therefore, only LBAs in the hot list are considered to be frequently accessed. 

Multiple bloom filters (MBF)~\cite{jagmohan2010write,park2011hot} uses bloom filters to identify if an LBA is hot. Bloom filters rely on a hash function that, given an input such as the LBA, provide an output which is mapped to a bit array, and sets the bit to true. Therefore, if an LBA is accessed applying the hash function sets the respective bit in the array to true, and checking if an LBA is hot simply applies the hash function and checks if the bit is set. However, depending on the length of the array, multiple LBAs can map to the same bit location, as proven by the pigeonhole principle~\cite{ajtai1994complexity}, resulting in false positive hotness classifications for LBAs. To avoid frequent false positive classifications, MBF utilizes multiple bloom filters. Similar to MBF, Kuo et al.~\cite{kuo2006configurability} present a hot data identification method by using multiple hash functions and a hash table. Upon a write, the LBA is hashed by multiple hash function and a counter for each has function is incremented in a hash table. To check if an LBA is hot, the LBA is hashed and a configured $H$ most significant bits of the resulting hash table indicate if that the LBA is hot if they are non-zero. Multiple hash functions are used for the same reason multiple bloom filters are used in MBF, to avoid false positive classifications. Lee and Kim~\cite{2013-Lee-data_grouping_empirical_study} provide a study into comparing performance of two-level LRU, MBF, and dynamic data clustering (DAC, which are similar to the density-based stream clustering by Shafaei et al.~\cite{2016-Shafaei-WA_extent_temp}). The authors show that on the evaluated synthetic workloads DAC clustering provides the highest reduction in write amplification factor.

Instead of grouping data based on an assigned temperature, indicating the access frequency, Chakraborttii and Litz~\cite{2021-Chakraborttii-DT_LBA} propose a temporal convolutional network that predicts the \textit{death-time} of LBAs. This allows to more optimally group data based on the death-time of individual LBAs. Grouping related death-time LBA reduces the required garbage collection, as blocks containing LBAs with similar death-times are erased together, which in turn reduces the write amplification (\textbf{FC2}).

\subsubsection{Garbage Collection Policies}
While data grouping provides benefits of co-locating data based on their update likelihood, an additional essential part of garbage collection is the policy of victim selection for segments to clean. Since during garbage collection a segment to clean is required to be selected, where all still valid data in the segment is moved to a free space, selecting a victim becomes non-trivial. The common existing policies for garbage collection are greedy~\cite{2010-Bux-Greedy_GC,kawaguchi1995flash} and cost-benefit~\cite{kawaguchi1995flash,1992-Rosenblum-LFS,wu1994envy}. However, with the prior established data grouping, both of these policies fail to fully account for the hotness of groups in the victim selection.

Greedy victim selection selects the segment with the lowest utilization, whereas the cost-benefit policy selects the segment with the best cost/benefit ratio, calculated as, with $u$ being the utilization of the segment~\cite{1992-Rosenblum-LFS}
\begin{equation}
    \text{cost-benefit}=\frac{\text{free space generated}}{\text{cost}}=\frac{(1-u)*\text{age}}{1+u}
\end{equation}
where the cost is based on reading the segment (equivalent to cost 1) + writing the data in the segment (equivalent to its utilization). SFS~\cite{2012-Min-SFS} proposes the \textit{cost-hotness} policy to account for the hotness of segments instead of the segment age, better incorporating the data grouping into victim selection. Similar to the cost-benefit policy, the cost-hotness is calculated as
\begin{equation}
    \text{cost-hotness}=\frac{\text{free space generated}}{\text{cost}*\text{segment hotness}}=\frac{(1-u)*\text{age}}{2u*h}
\end{equation}
where the cost considers reading and writing the valid blocks (equivalent to $2u$) with the segment hotness. A further garbage collection policy that is based on the cost-benefit policy, is the \textit{cost-age-time} (CAT) policy~\cite{chiang1999cleaning}. It extends cost-benefit by including an erase count for each block, improving wear leveling (\textbf{FC5}).

Customizable victim selection algorithms, such as the \textit{$d$-Choice} algorithm~\cite{van2013mean}, which combines greedy selection with random selection. The tunable parameter $d$ defines the number of blocks to be selected randomly out of the $N$ total blocks. Therefore, configuring $d=1$ results in fully random victim selection, as a single block is randomly selected from the total blocks, providing effective wear leveling through randomness~\cite{2015-Yang-algebric_WA_modeling}. Configuring $d\rightarrow \infty$ selects a larger subsets of blocks to use greedy selection on, such that $d=N$ is equivalent to fully greedy victim selection, allowing to provide the lowest cleaning latency. Configuration of $d$ thus allows to define the tradeoff between wear leveling and performance. An evaluation by Yang and Zhu~\cite{2015-Yang-algebric_WA_modeling} of the algorithm shows the significance of the number of hotness tiers which are utilized and the configuration of the $d$ parameter, where a large number of hotness tiers can significantly increase the write amplification during garbage collection. 

A further drawback of segment cleaning with LFS, in particular F2FS, is that the modification of metadata during segment cleaning requires a checkpoint to be created after each segment clean. This constitutes to a significant overhead for the segment cleaning process. To avoid the excessive checkpointing after segment cleaning, \textit{segment cleaning journaling} SCJ~\cite{2021-Gwak-SCJ} adds support to F2FS to journal metadata updates made during segment cleaning, instead of creating a checkpoint. This journal is stored in a journal area, which delays the updating of the original metadata until the journal becomes large enough or the checkpointing time interval is reached. However, metadata still points to old invalid data blocks (referred to as pre-invalid blocks), which requires that data only be invalidated once the metadata is updated by the segment cleaning journal. Therefore, SCJ implements an adaptive checkpointing that evaluates the cost of checkpointing to flush the metadata updates and the accumulation of pre-invalid blocks, and checkpoints if its cost is lower. 

\subsection{I/O Amplification}
As the main cause of write amplification is the incurred garbage collection, mechanisms to reduce garbage collection consequently reduce the write amplification. However, additional mechanisms exist to reduce the write amplification. In particular, as the smallest unit of write in flash storage is a single page, smaller writes still require to write a full page, filling up unused space and writing more than was intended, which causes space and write amplification. While the majority of file systems aim to align inodes to the size of flash pages, such that space amplification can be avoided. File systems, such as ReconFS~\cite{2014-Lu-ReconFS} have much smaller inode size, equal to 128B, allowing to place numerous inodes in a single flash page. With such an inode size, writing each inode change directly would require to fill the flash page with unnecessary data, therefore, ReconFS implements a \textit{metadata persistent log}, in which metadata changes are logged and compacted to align with pages, and are only written back to the storage when evicted or checkpointed, in order for the file system to remain consistent.

\subsection{Flash Parallelism}
As host software has no direct access to flash storage with SSDs, the FTL implements and manages all device-level parallelism. The possibility for the host to utilize flash parallelism comes from aiding the FTL in providing large enough I/Os such that the FTL can stripe data across flash chips and channels. This combining of resources for striping of data is referred to as \textit{clustered blocks/pages}~\cite{2012-Kim-clustered_blocks}. Furthermore, clustered blocks are also erased as one, allowing for write I/Os that are aligned to a multiple of the clustered block size to invalidate blocks in parallel, helping to amortize overheads.

SFS~\cite{2012-Min-SFS} takes advantage of the achieved device-level parallelism with clustered blocks by aligning segments to a multiple of the clustered block size. During garbage collection SFS ensures that cleaning of segments which do not have enough blocks to fill the clustered block size is delayed until enough data is present. Increasing device parallelism can further be achieved by mapping operations to individual resources. DevFS~\cite{2018-Kannan-DevFS}, a direct access user-space file system, maps I/O queues to individual files, allowing concurrent file operations to submit I/Os concurrently without interfering on the I/O queue.

\subsection{Wear Leveling}
Garbage collection and the selection of garbage collection policy are major contributing factors to burning out of flash cells~\cite{2016-Verschoren-GC_impact_WL}. While file systems commonly aim to provide even wear over the flash device, especially during file system garbage collection with LFS, at the SSD integration level there is limited possibility of ensuring even wear, since the FTL makes the final decision on physical write addresses, and it also attempts to ensure even wear. However, as seen with earlier discussed file systems such as CSA-FS~\cite{huang2013improve}, flash lifetime can further be prolonged by minimizing the data write traffic through mechanisms such as deduplication and delta-encoding.

% per process stream pattern recognition to improve wear on ssd\cite{wang2013exploit} - NO ACCESS TO PAPER ON IEEE

\subsection{Minimizing the Software Stack with User-Space File Systems}
With SSDs providing significantly lower access latency, the overheads of the I/O stack on the storage becomes a major contributing factor in the performance~\cite{foong2010towards,seppanen2010high,vasudevan2012using}. Any added interrupt on the I/O path can cause significant delays~\cite{2014-Shin-OS_IO_Path}. Especially with the Kernel involvement requiring to possibly do context switches, the majority of time can be spent in the I/O stack rather than the storage retrieving the data. Therefore, research has aimed at minimizing the I/O stack and avoiding costly mechanisms.

A major contribution of this is the design of user-space file systems. These file systems run only in the user-space, as opposed to commonly used file systems (e.g., F2FS) running in Kernel space. Further benefiting from easier development, increased reliability and security by avoiding Kernel bugs and vulnerabilities, and increased portability~\cite{2015-Tarasov-User_space_fs_practicality}. The most commonly framework for building user-space libraries is FUSE~\cite{szeredi2010fuse}, however it is implemented over a Kernel module, allowing to export virtual file systems into the Kernel. However, FUSE based file systems suffer significant performance penalties, requiring more CPU cycles that file systems in Kernel space, because of it need to copy memory between user space and kernel space and overheads from the way FUSE handles requests~\cite{2019-Vangoor-Fuse_performance,vangoor2017fuse}. Furthermore, FUSE suffers from context switching~\cite{vangoor2017fuse,2019-Vangoor-Fuse_performance,rajgarhia2010performance} and inter-process communication (IPC) with the FUSE kernel module~\cite{zhu2018direct}. An existing file system in the Linux Kernel called ZUFS (zero-copy user-mode file system)~\cite{2019-Harrosh-zufs}, which is aimed at persistent memories but applicable to other devices, provides a lockless, zero-copy, user-space file system, which completes I/O requests by requesting exact data locations instead of copying data into the own address space.

A similar framework for building user-space applications with direct access to storage devices is NVMeDirect~\cite{2016-Kim-NVMe_Direct}. However, it also relies on a Kernel driver to provide enhanced I/O policies. SPDK~\cite{2017-Yang-SPDK} is another framework for building user-space storage applications, however it provides the mechanisms to bypass the Kernel and submit I/O directly to the device, by implementing a user-space driver for the storage device. Such a framework allows to build high performance storage applications in user-space, which eliminate the overheads coming from the Kernel I/O stack. Borge et al.~\cite{2019-Borge-SSD_read_variability} show with a case study on HDFS performance with SSD, that in order to leverage the capabilities of flash SSDs, direct I/O and increased parallel requests with buffered I/O are needed. 

File system as processes (FSP)~\cite{2019-Liu-fs_as_process} provides a storage architecture design for user-space file systems. The emphasis of FSP is to scale with the arrival of faster storage, and similarly to other user-space frameworks, minimize the software stack. For this it bases development on running file systems as processes, providing safer metadata integrity, data sharing coordination, and consistency control, as the process running the file system is in control of everything, instead of trusting libraries. Furthermore, FSP relies on IPC for fast communication, which unlike FUSE has a low overhead since it does not require context switching. Inter-core message passing comes at a low overhead and cache-to-cache transfers on multi-core systems can complete in double digit cycles~\cite{soares2010flexsc}. DashFS~\cite{2019-Liu-fs_as_process} is built with FSP, providing a safe user-space file system with isolation of different user processes, and efficient IPC.

URFS~\cite{2020-Tu-URFS} provides increased concurrency performance by implementing a multi-process shared cache in memory. This mechanism helps avoid contention on the storage device, and further avoids Kernel interaction. In order to guarantee permissions are valid, URFS implements an \textit{access control list} (ACL). Through the shared memory, it avoids the Kernel having to copy memory for every process, limiting Kernel overheads. A similar user-space file system that implements a shared cache for process is EvFS~\cite{2019-Yoshimura-EvFS}, which is SPDK-based. While this file system can also support multiple readers/writers in the page cache, it only supports these for a single user process. User-space frameworks often provide capabilities to either expose the storage device as a block device, which the user-space application then accesses, or build a custom block device module (e.g., with SPDK which also has default driver modules such as NVMe). For NVMe devices that support NVMe controller memory buffer management, the file system can manage parts of the device memory. DevFS~\cite{2018-Kannan-DevFS} utilize such an integration to manage the device memory for file metadata and I/O queues.

Lastly, specific operating systems, such as Arrakis~\cite{peter2014towards,peter2015arrakis}, which exposes storage devices as virtualized storage and allows user-space I/O requests to bypass their kernel. However, it requires the Kernel to be updated to enforce storage protection, which makes detailed discussion of its methods out of the scope of this literature study.  

\subsection{I/O Scheduling}
Given that particular I/O patterns can have degrading affects on the SSD performance, such as mixing read and write operations, as they share resources on the device, including the mapping table and ECC engine, and furthermore possibility invalidating the cached data in the SSD RAM. Similarly, mixing I/O operations with different block sizes can result in increased fragmentation~\cite{2020-Tu-URFS}. As the Linux Kernel relies on a submission and completion queue for I/O, user-space frameworks such as SPDK and NVMeDirect provide more flexibility for user-space file systems to design different queues, depending on the requirements. URFS~\cite{2020-Tu-URFS} utilizes this possibility to create adaptive queues that can better optimize I/O submissions to the device. Based on the workload characteristics URFS dynamically creates flexible I/O queues (e.g., group by size, read/write operation) to increase SSD performance. In addition to better utilizing SSD performance, this mechanism also enhances dealing with the read/write asymmetry of flash (\textbf{FC1}).

Conventionally I/O requests are submitted asynchronously, where the application submits the I/O to the Kernel, which then submits it to the device and waits for an interrupt from the storage device when the I/O has completed. While this allows the application to continue different work and frees the CPU, it was originally designed for slow storage devices, where I/O completion takes significant time. Especially, as the Kernel asynchronous block I/O layer adds a substantial overhead, with fast storage devices, such as NVMe SSD, polling provides better performance. With polling, the I/O is submitted synchronously, avoiding the Kernel asynchronous block I/O layer, and spin-waits for the I/O to complete. Faster storage devices can complete I/O requests significantly faster, where the overheads of spin-waiting are lower than the overheads of asynchronous I/O. Furthermore, a large penalty of asynchronous I/O is the need for context switching, when interrupts happen, as well as causing cache and translation lookaside buffer (TLB) pollution~\cite{yang2012poll}. Yang et al.~\cite{yang2012poll} provide a quantitative comparison of asynchronous and synchronous, interrupt and polling, I/O scheduling.

With the possibility for asynchronous I/O to merge and reorder requests, the Linux Kernel implements several schedulers, such as \textit{NOOP}, \textit{deadline}, and \textit{CFQ}~\cite{sun2014exploring,pratt2004workload,moallem2008study,heger2010linux}. NOOP being the least intrusive scheduler only merges I/O request, but does not reorder them, which is beneficial on devices such as OCSSD and ZNS that require consecutive LBAs. Son et al.~\cite{2015-Son-Optimizing_FS} showcase the benefits of merging random write requests, regardless of contiguity of the LBAs, in order to better enhance performance with fast storage devices. Deadline adds to NOOP by utilizing merging and reordering, however also applies a deadline for each I/O request to ensure requests are submitted to the device eventually. Two separate queues, one for read requests and an additional one for write requests are utilized, which are both ordered by the deadline of the request. Another scheduler variant of deadline exists, called \textit{mq-deadline}, which is aimed at multi-queue devices, such as NVMe SSD. CFQ (completely fair queueing) implements a round-robin based queue that assigns time slices to each in order to prevent starvation and provide fairness. While these are some of the common schedulers in the Linux Kernel, various more schedulers exist~\cite{2019-ubuntu_wiki_io_schedulers}.

Such scheduling configuration begs the question on which scheduler is best suited for file systems on flash storage. Sun et al.~\cite{sun2014exploring} provide a study into several possible scheduler configurations for file systems on SSD, with an emphasis on incorporating the storage energy consumption, however we do not discuss energy consumption aspects in this literature review. Yu et al.~\cite{yu2014optimizing} present similar study of the I/O path and block I/O system optimizations, however over SCSI and SATA instead of NVMe, which is why we do not include its details here. Nonetheless, several of the findings are applicable to all applications submitting I/O to storage devices. The merging of read I/O requests in synchronous I/O provides beneficial performance gains, and similarly the merging of write I/Os in asynchronous I/O shows performance gains. However, optimal I/O scheduling for storage devices is an obstacle that has yet to be overcome.
