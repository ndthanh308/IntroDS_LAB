\subsection{Log-Structured File Systems}
A ubiquitous approach of managing persistent storage devices is with file systems, providing the familiar file and directory interface for organizing storage. While there exist a plethora of different file systems, they largely rely on similar concepts, albeit often using different data structures and naming. The lack of in-place updates on flash pages, making sequential writes a requirement, makes \af{lfs}~\cite{1992-Rosenblum-LFS,2006-Konishi-Linux_LFS,seltzer1993implementation} a suitable file system design. Originally designed to minimize random writes on HDDs, and thus minimize arm movements on the disk, LFS have been around for several years. The design of log-structured file systems revolves around writing data as a log, appending new data sequentially on the storage device. Similar to the \as{ftl}, as data is being updated in a \as{lfs}, the log contains an increasing amount of no longer needed data, requiring \as{gc} to be freed and regain the used space.

In \as{lfs}, due to the convention coming from the block interface for \as{hdd} integration, storage is addressed in \textit{sectors}, typically 512B or 4KiB, representing the smallest unit of I/O on the device. File systems then divide the storage space in a number of \textit{blocks}, commonly 4KiB, which consists of at least one up to multiple sectors. Blocks are the unit in which file systems allocate space for files. Next, we discuss the design of the de facto standard \as{lfs} for flash storage devices, F2FS~\cite{2015-Changman-f2fs}.

\subsection{F2FS - Flash Friendly File System}
The most widely adopted \as{lfs} for flash storage is F2FS~\cite{2015-Changman-f2fs}. With it being the de facto standard of flash file systems, a large number of file systems are based on its design. In F2FS, the data space is divided into \textit{segments} (typically 4KiB), presenting the smallest writable unit in the file system in which data space is allocated. Segments are then grouped into \textit{sections}, representing the erase unit of F2FS at which it runs victim selection during \as{gc}. Furthermore, the section size is aligned to the flash erase unit, to amortize overheads and avoid unnecessary copying data with \as{ftl} garbage collection.

To minimize the garbage collection overheads, F2FS groups data based on the modification frequency. It achieves this by assigning a temperature (hold/warm/cold) to the data blocks, and pre-defines classifications for direct node blocks, as these are frequently modified, and indirect node blocks are classified as cold, as these are not frequently modified. To further extend this mechanism, F2FS maintains six different logging zones for the different temperature assignments (hot/warm/cold for nodes and file data). In F2FS \as{gc} is commonly referred to as \textit{cleaning}, and is run periodically (called \textit{background cleaning}) or when space is needed (called \textit{foreground cleaning}). The cleaning policies implemented in F2FS are greedy~\cite{2010-Bux-Greedy_GC,kawaguchi1995flash} and cost-benefit~\cite{kawaguchi1995flash,1992-Rosenblum-LFS,wu1994envy}, which are used for foreground and background cleaning respectively.

% Figure environment removed

Data in file systems is managed through an \af{inode}. \cref{fig:f2fs-inode} shows the inode of F2FS. It contains all information of a file, including the file specific metadata on creation time, access permissions, file name, and more. For identifying the data blocks of the associated file, inodes contain a fixed number of direct pointers to the addresses of the file data. Since inodes are allocated in blocks, they can often contain inline data of the file, if the file data fits in the available space of the inode. To be able to locate larger files, inodes utilize \textit{multi-level indexing}, where indirect pointers point to blocks containing further pointers to more data blocks. There are several levels of indirection, where a \textit{single indirect} pointer points to one block containing pointers to the blocks which contain the data. For example, with a block size of 4KiB and 4B addresses, a single indirect pointer can store 1024 addresses, which each point to 4KiB of data, giving an addressable space of $4KiB*1024=4MiB$ (plus the number of direct pointers in the inode). Increasing the level of indirection further increases the maximum file size of the file system, where nowadays file systems most commonly provide up to three levels of indirect pointers, allowing in the example of 4KiB block size and 4B addresses, a maximum file size of $4KiB*(1024+1024^2+1024^3)=4TiB$ (plus the number of direct pointers). While multi-level indexing is a popular implementation in file systems, other solutions such as \textit{extents} exist, utilizing a single pointer to a contiguous number of blocks for the file. This approach decreases the metadata size by requiring only a single pointer, however it is more difficult of finding contiguous number of blocks for allocating space for the file.

File systems commonly utilize tree-based indexing data structures in order to achieve a worst case time complexity of $\mathcal{O}(\log{}n)$ for locating of inodes or other metadata, compared to a log based approach requiring to scan the entire log, corresponding to $\mathcal{O}(n)$ worst case time complexity. However, a tree-based data structure introduces the Wandering Tree Problem~\cite{bityutskiy2005jffs3}, where if a leaf node is modified by changing it to point to new data, thus requiring a new entry at the end of the log, its parent node also requires to change the pointer to the changed node. This parent node then also requires to be updated, propagating the changes all the way up the tree to the root node. Therefore, F2FS manages metadata for segment allocations in a \af{nat}. It divides allocated blocks into three types; \textit{inode blocks} that contain the metadata for the file from the \as{inode}, \textit{direct pointer node blocks} containing the \as{lba} of the data, and \textit{indirect node blocks} containing the identifier of the associated direct pointer node block, which is unique for each direct node. Therefore, a change in a data block requires the direct node block to be updated, to point to the new \as{lba} of the data, and a change in the NAT to point the indirect node block to the new direct node block, solving the wandering tree problem. 

To avoid metadata updates being written directly after a change, F2FS uses the \af{lmu} to buffer metadata and node block updates in memory before being flushed to the flash storage. A similar policy is the \af{libu}~\cite{oh2010optimizations}, in which only the block node updates are buffered in memory before being flushed. Additional data structures for metadata in F2FS include the \af{sit} for information on the segment usage and the bitmap to identify valid and invalid blocks, and a \as{ssa} which includes further information on segments such as the information on owners of data and node blocks. Note that F2FS requires randomly writable space for metadata, as it is not written in the log, delegating data management to the \as{ftl}. 

Given that file systems are written in units of blocks, smaller I/Os arising from minor file challenges, such as microwrites, are undesirable due to the added \as{wa}. Therefore, file systems employ \textit{write buffering} by caching the changes in memory, and writing to storage when needed. However, this comes at the drawback that if the system crashes, the data cached in memory is lost. For this file systems provide the possibility to directly write to storage with \textit{direct I/O} or to explicitly flush caches to the storage device with an \textit{fsync()} call. To provide failure recovery in the case of system crashes or power loss, file systems commonly employ methods such as \textit{journaling}, in which operations are logged in a journal in addition to being written to the storage. Therefore, if the system crashes replaying the journal will result in reaching the prior valid file system before the system failure. File systems with this journaling feature are referred to as \textit{journaling file systems}. 

A similar approach with less overheads is using \af{cow}, commonly also referred to as \textit{shadow paging}, in which a copy of the original data is made on writing, and the copy is modified. Only upon successful completion is the metadata, pointing to the valid data block, changed to present the new copy. This can additionally be done atomically and simplifies the crash recovery, as the metadata state will always be valid, such that if the copy was written but the metadata points to the old data, the write did not complete successfully and thus the original data is the valid data. While there exist more implementations for ensuring crash recovery and consistency, journaling and \as{cow} are among the most utilized approaches. F2FS utilizes \textit{checkpointing}, in which metadata including the file system status, bitmaps, inode lists, and the segment summary, are written to the \as{cp} region periodically. During a checkpoint, all data in page cache is flushed to storage and a \textit{checkpoint pack} is written, containing the metadata (\as{nat} and segment information bitmaps). 

In the case of a system crash or power loss, the file system can recover the state from the latest checkpoint, referred to as \textit{roll-back recovery} since the latest changes which are not in the checkpoint are reverted. In order to recover the latest changes, the host must call \textit{fsync()} to ensure that metadata and data are flushed from memory to the device. This recovery is referred \textit{roll-forward recovery} since it recovers the state past the latest stable checkpoint. F2FS can only guarantee roll-forward recovery with \textit{fsync()}.

While F2FS utilizes a log-structured approach, under increased concurrency such a log becomes a bottleneck. Therefore, to provide increased performance, F2FS can switch between a single sequentially written log (\textit{normal logging}) to \textit{threaded logging}, where data is written in dirty segments instead of the log. However, while this approach provides increased logging capacity for concurrent access and avoids garbage collection, it incurs random writing. Given that during file system \as{gc}, block addresses are modified, as the cleaning moves still valid blocks to free space, however the metadata is not directly updated due to the \as{lmu} policy, F2FS is required to create a checkpoint after each segment cleaning. 
