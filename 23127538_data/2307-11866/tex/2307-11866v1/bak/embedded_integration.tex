\section{Embedded Flash Integration}
The last group of flash-based storage systems is the embedded systems, which give the file system full control over the underlying flash storage. This allows the file system to fully manage the storage, however comes at the cost of increased complexity. This section evaluates the file systems for flash-based embedded systems, analyzing how different file systems solve the particular design challenges of flash, leveraging the direct management of flash media. Again, we only discuss concepts that are not mentioned in the prior integration levels or methods of which further optimization is possible as a result of the embedded flash integration. Table~\ref{tab:embedded_integration_comparison} shows a comparison of the various file systems discussed in this section, evaluating the flash challenge(s) that a file system addresses. Note, also not all file systems in the table are discussed in this section if they do not contribute any new methods not already discussed.

\begin{table*}[!ht]
    \centering
    \begin{tabular}{||p{15mm}||p{8mm}|p{15mm}|p{5mm}|p{20mm}|p{15mm}|p{5mm}|p{15mm}|p{15mm}||}
        \hline 
        & & \multicolumn{7}{|c||}{Flash Challenge} \\
        \hline
        File \newline System & Year & Read/Write \newline Asymmetry & GC & I/O \newline Amplification & Flash \newline Parallelism & WL & Software \newline Stack & I/O \newline Scheduling \\
        \hline
        \hline
        CFFS\cite{2006-Lim-NAND_fs} & 2006 & \cmark & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark \\
        \hline
        UBIFS\cite{hunter2008brief} & 2008 & \cmark & \cmark & SA & \xmark & \cmark & \xmark & \xmark \\
        \hline
        FlexFS\cite{2009-Sungjin-FlexFS} & 2009 & \cmark & \cmark & WA & \xmark & \cmark & \xmark & \xmark \\
        \hline
        ScaleFFS\cite{2008-Jung-ScaleFFS} & 2008 & \cmark & \xmark & WA & \xmark & \xmark & \xmark & \xmark \\
        \hline
        JFFS\footnote{focusing on JFFS V2}\cite{woodhouse2001jffs} & 2001 & \xmark & \cmark & \xmark & \xmark & \cmark & \cmark & \cmark \\
        \hline
        EnFFiS\cite{park2013enffis} & 2013 & \cmark & \xmark & \xmark & \cmark & \cmark & \xmark & \xmark \\
        \hline
        FlogFS\cite{nahill2015flogfs} & 2015 & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
        \hline
        RFS\cite{schildt2012contiki} & 2012 & \cmark & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark \\
        \hline
        F2FS + SAC\cite{2015-Park-Suspend_Aware_cleaning-F2FS} & 2015 & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark \\
        \hline
        NANDFS\cite{2009-Zuck-NANDFS} & 2009 & \cmark & \cmark & WA & \xmark & \cmark & \xmark & \xmark \\
        \hline
        NAMU\cite{2009-Park-Multimedia_NAND_Fs} & 2009 & \cmark & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark \\
        \hline
        LeCramFS\cite{2007-Hyun-LeCramFS} & 2007 & \cmark & \xmark & SA & \xmark & \xmark & \xmark & \xmark \\
        \hline
        F2FS + FPC\cite{2021-Ji-F2FS_compression} & 2021 & \cmark & \cmark & SA & \cmark & \xmark & \xmark & \xmark \\
        \hline
        NAFS\cite{2011-Park-Multi_NAND} & 2011 & \cmark & \xmark & WA & \cmark & \cmark & \xmark & \xmark \\
        \hline
        YAFFS\cite{manning2010yaffs,aleph2001yaffs,manning2002yaffs} & 2010\footnote{references include all YAFFS publications, however we focus on YAFFS2} & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark \\
        \hline
        % From here on papers added from original FS Design Considerations classification
        LogFS\cite{engel2005logfs} & 2005 & \xmark & \cmark & SA & \xmark & \xmark & \xmark & \xmark \\
        \hline
        DFFS\cite{2008-Kim-DFFS} & 2008 & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
        \hline
        RFFS\cite{park2006flash} & 2006 & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
        \hline
        F2FS + ARS\cite{2020-Yang-F2FS_Framentation} & 2020 & \cmark & \cmark & WA \& RA & \cmark & \xmark & \xmark & \xmark \\
        \hline
        F2FS + RM-IPU\cite{2022-Lee-F2FS_Address_Remapping} & 2022 & \cmark & \cmark & WA \& RA & \cmark & \xmark & \xmark & \xmark \\
        \hline
        DeFFS\cite{2011-Lim-DeFFS} & 2011 & \cmark & \xmark & WA & \xmark & \xmark & \xmark & \xmark \\
        \hline
    \end{tabular}
    \caption{Evaluated file systems for embedded systems with flash storage under the flash challenges (Section~\ref{sec:opt_flash}) the file system solves. A \cmark\hspace{0.5mm} indicates that a file system implements a mechanism to handle the particular flash challenge, however it does not indicate to what extent it is handled or how successful it is handled. If a file system does not have a name and/or it is an extension on an existing file system, the authors' names are used as the file system name. For instance, the GC column contains a \xmark\hspace{0.5mm} if the file system simply implements greedy or cost-benefit GC, and a \cmark\hspace{0.5mm} if a new GC policy or new mechanisms of managing GC are presented. Continued in Table~\ref{tab:embedded_integration_comparison_cont}.}
    \label{tab:embedded_integration_comparison}
\end{table*}

\begin{table*}[!ht]
    \centering
    \begin{tabular}{||p{15mm}||p{8mm}|p{15mm}|p{5mm}|p{20mm}|p{15mm}|p{5mm}|p{15mm}|p{15mm}||}
        \hline 
        & & \multicolumn{7}{|c||}{Flash Challenge} \\
        \hline
        File \newline System & Year & Read/Write \newline Asymmetry & GC & I/O \newline Amplification & Flash \newline Parallelism & WL & Software \newline Stack & I/O \newline Scheduling \\
        \hline
        \hline
        O1FS\cite{2014-Hyunchan-O1FS} & 2014 & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
        \hline
        ELOFS\cite{2022-Zhang-ELOFS,2020-Zhang-LOFFS} & 2022 & \xmark & \cmark & WA & \cmark & \cmark & \xmark & \xmark \\
        \hline
        MNFS\cite{kim2006mnfs} & 2006 & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
        \hline
        Coffee FS\cite{2009-Tsiftes-Coffee_FS} & 2009 & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
        \hline
    \end{tabular}
    \caption{Table~\ref{tab:embedded_integration_comparison} continued.}
    \label{tab:embedded_integration_comparison_cont}
\end{table*}

\subsection{Read/Write Asymmetry}
With the lower integration level and increased flash management responsibility comes increased complexity. Similar to DualFS~\cite{2020-Wu-DualFS}, which implemented a dynamically reconfigurable file system for dual-flash mode (switching of the flash cell level such as MLC to SLC), flexfs\cite{2009-Sungjin-FlexFS}, it implements a similar mechanism for increased performance on the SLC area. The drawback of such a design is that data is required to be moved from the SLC area to the MLC area as it has a lower write lifetime, and is aimed at the for critical I/O requests that require lower completion latency, which in turn causes \textit{data migration overheads}. To solve this, FlexFS implements several migration techniques that aim to hide the overheads for the data migration from the host. The first technique revolves around \textit{background migration}, which pushes the migration to happen when the system is idle. The second technique is \textit{dynamic allocation}, which writes non-critical requests to the MLC area, saving on flash degradation in the SLC area, as well as avoiding future data migration. The dynamic allocator functions based on measurements of prior system idle times from I/O requests, to predict current idle time, and if sufficient idle time is predicted in order to complete data migration, the data is written to the SLC, and otherwise part of the data, depending on required migration and idle time, is written to the MLC area. The last technique, \textit{locality-aware data management}, takes into account the hotness of data, and the dynamic allocator attempts to migrate only cold data from the SLC area.

NAFS~\cite{2011-Park-Multi_NAND} implements a file system which utilizes multiple NAND chips, by representing them as a storage array to the file system in order to simplify the management of the different chips, where parallelism is utilized by striping I/O across the different chips (\textbf{FC4}). In order to incorporate the asymmetric flash performance, it further implements a \textit{double list cache}, containing a clean list with only clean pages for caching of data, and a second dirty list for writing of modified pages and to prefetch of pages based on the access patters, only containing dirty pages. Prefetching is striped across the NAND chips, increasing read performance, and minimizing cache misses from the lists. The benefit of two separate lists is that firstly it allows to cache write operations and avoid small writes to the flash device, and secondly it prefetches data into the clean list in order to minimize cache misses.

\subsection{Garbage Collection}
A major challenge of flash-based storage is the performance impacts of garbage collection. At points where the device has to move valid pages in blocks to a free space, and erase the old block, application performance becomes unpredictable and can encounter significantly higher tail latency. There are several approaches for file systems to utilize in order to minimize the required garbage collection. While we discuss these methods in the context of file systems, they are applicable to the majority of applications utilizing flash storage.

Similar to prior discussed file systems, UBIFS~\cite{hunter2008brief}, which is built on the unsorted block images (UBI)~\cite{gleixner2006ubi}. UBI implements the management of the flash storage, including allocation and wear leveling. In order for UBIFS to decrease the garbage collection overheads, it firstly utilizes inline metadata where file data and metadata are stored together. Secondly similar to existing file systems, it utilizes a log where updates are initially written to, before writing to the on-flash index structure, avoiding small writes.

A different approach to reducing the garbage collection overheads for the file system is by adding a level of indirection. As Butler Lampson mentioned in his famous quote from 1972 (which was originally stated by David Wheeler), ``Any problem in computer science can be solved with another level of indirection``~\cite{lampson1993principles}. For this, NANDFS~\cite{2009-Zuck-NANDFS} implements a \textit{sequencing layer} on top of which the file system is built, which handles block allocation, garbage collection, wear leveling, and the logical-to-physical flash mapping. The sequencing layer implements the block allocation as immutable storage, therefore when file system data is updated, it cannot overwrite, and simply marks the data as obsolete. The new updated data is written in a newly allocated block, however the sequencing layer implements the logical-to-physical mapping, with a mapping table, such that the logical block address to the new data is the same as the old logical block address, avoiding the updating of data addresses in the file system metadata. This implies that the file system does less writing by eliminating metadata updates, but it also does not run any garbage collection.

\subsubsection{Data Grouping}
Similar to discussed file systems, grouping of data is also beneficial for embedded devices to minimize garbage collection. Similar to how F2FS assigns a temperature to the data, the Core Flash File System (CFFS)~\cite{2006-Lim-NAND_fs}, which is an earlier flash-based file system, differentiates between hotness of metadata to data. Given that metadata is more likely to be updated frequently, separating metadata from data improves on required garbage collection. Specifically, since metadata is often updated even if the data is not updated, for example in scenarios where the file attributes (access time, permissions, etc.) are updated or the file is moved. With CFFS being a file system for embedded devices, there is more control over the flash storage, as it has direct flash access without an FTL, it separates metadata and data in distinct blocks (the flash erase unit). However, this can quickly lead to uneven wear across the flash, since metadata blocks are erased more frequently, which CFFS solves by switching metadata and data blocks after an erase, such that an erased metadata block becomes a free data block.

\subsubsection{Garbage Collection Policies}
On embedded devices, and in particular mobile devices, in order to save energy the device suspends all threads when not needed (e.g., when the mobile screen is turned off). This implies that all file system threads are also suspended, which means the file system cannot run the background garbage collection during these inactive sessions. Therefore, \textit{suspend aware cleaning}~\cite{2015-Park-Suspend_Aware_cleaning-F2FS} (we abbreviate it as SAC) is an addition to LFS, which utilizes the slack time, which is the time between the suspend initiation (e.g., turning off the mobile device screen) and the suspending of the file system threads. It uses this slack time to run background garbage collection, however it does not write any data on the flash storage, but instead selects a victim block and brings the still valid pages in the page cache and marks these as dirty, resulting in all pages in the victim block to be dirty, which allows it to be erased. This process is referred to as \textit{virtual segment cleaning}, which however is not run every time the screen is turned off, but rather based on the device utilization, which is called \textit{utilization based segment cleaning}. Similarly, file-aware garbage collection (FaGC)~\cite{yan2014efficient} is an optimization to garbage collection policies, which maintains an update frequency table (UTF) for each logical block address of a file, in order to to group valid pages in the victim block based on the access frequency when these are copied to a new block during garbage collection. This is similar to grouping hot and cold data, but at the level of garbage collection for LBAs in a file, and therefore allows to minimize the GC overheads.

The application domain of embedded devices is often particular to specific workloads and files. This allows file systems to optimize for a particular domain, such as multimedia content. NAMU~\cite{2009-Park-Multimedia_NAND_Fs} showcases a file system that, similar to other file systems, also manages flash in a number of segments, however these are aligned to match the characteristics of large files, and in particular multimedia content. The main benefit of large file mappings, aside of lower memory usage from fewer address mappings, the possible optimization in garbage collection. Given that multimedia files are rarely modified, and if removed, all file segments are erased in one unit, garbage collection in NAMU is done at the granularity of files. During garbage collection, NAMU selects a victim file with the smallest erase count, which is maintained for each file, and the erasure of large files allows freeing up significantly more space. 

\subsection{I/O Amplification}
While write operations are the major cause of flash cells burning out, read operations also pay a toll on the flash. The assumption that read operations can be issued indefinitely without interfering with the flash cells is flawed, as the current flash technology utilizes flash gates that are only capable of holding very few electrons (determining the charge of the gate) due to their size~\cite{lu2009future,shin2005non}. This makes the cells increasingly susceptible to \textit{read disturbance}~\cite{2015-Liu-Read_leveling}, requiring frequent rewriting to ensure the charge stays consistent. In order to control read disturbance, Liu et al.~\cite{2015-Liu-Read_leveling} propose to include in the FTL read-leveling mechanisms. While the discussion is aimed at FTL implementations, the ideas are applicable to file systems in embedded flash integration, which control physical mappings. Read-hot data is isolated from other data pages by placing the hot pages into \textit{shadow blocks}, which contain no valid data (i.e., they can be invalid blocks). However, this requires to identify the read-hot pages, where a tracking of read counters for each page would require significant resources. Therefore, a \textit{second-chance monitoring strategy} is used, which initially tracks the reads for a particular block, and upon reaching a threshold indicating the block contains read-hot pages, the individual pages in the block are tracked on read counters. Finally, pages reaching a threshold are then copied to the shadow blocks, thus avoiding the tracking of read counters for individual pages and only copying hot pages into the shadow blocks. While this strategy requires copying of read-hot pages to shadow blocks, it minimizes read disturbance which in turn minimizes the write amplification it causes.

Similar to SSD integration, an effective method to reduce space amplification is to utilize compression. LeCramFS~\cite{2007-Hyun-LeCramFS} (short for less crammed FS) is such a read-only file system with compression for embedded devices. However, unlike other existing file systems, which utilize a memory buffer for decompression, to read the data, decompress it, followed by writing the uncompressed data to a memory space, LeCramFS does not utilize a decompression buffer, avoiding copying overheads. It achieves this by using \textit{page boundary alignment} (PBA), in which the block with the compressed data is aligned to the page, such that compressed data does not share pages. Conventionally, compressed data blocks are allocated directly after prior blocks, even if the prior page is not fully utilized, resulting in blocks sharing the same pages. This requires the reading of all the compressed data into the device cache, separating the compressed data, followed by decompressing the desired part. Aligning compressed data to the page unit allows to directly bring the desired data into the device cache and decompressing it, avoiding an extra copy to separate the requested part. 

In order to avoid wasting space if the page is not fully utilized from a single compression, LeCramFS implements a partial compression which fits part of data as a separate compressed part, and the remaining data is compressed in the next free region. The difference from this and conventional compression is that partial compression splits the data and compresses it separately into the available space, avoiding that the entire compressed data spans across page boundaries. Similar compression is applied to F2FS~\cite{2021-Ji-F2FS_compression} for mobile devices, where \textit{file pattern-guided compression} (FPC) selects the compression method for files based on how compressible they are. For instance SQLite journals are rarely read and can be easily compressed, generic files can be compressed with inline metadata, and binaries can be compressed using specific compression methods.

\subsection{Flash Parallelism}
While IoT devices such as microcontrollers commonly only contain a small amount of flash, such as a single flash chip, limiting the possible parallelism that can be exploited, parallelism on more resource available embedded devices can be utilized. NAFS~\cite{2011-Park-Multi_NAND} utilizes the flash parallelism by striping I/O across multiple flash chips. Similarly, YAFFS~\cite{manning2010yaffs,aleph2001yaffs,manning2002yaffs} allocates data in units of \textit{chunks}, which can be a single page, however in the presence of multiple NAND chips, a chunk can be mapped to multiple pages, with a single page from each chip being in a chunk. This results in I/Os being striped across the flash chips.

\subsection{Wear Leveling}
\ntytodo{add papers here? several that have checkmark on WL in table}

\subsection{Software Stack}
The largest benefit of embedded flash is the possible reduction of layers in the software stack. The direct control over the flash storage allows to remove the FTL, and optimize flash management at the file system level, which in turn allows to incorporate the file system knowledge of data into the data placement and flash management. JFFS~\cite{woodhouse2001jffs} is a well known file system for embedded devices, which particularly argues for removing the redundant layers of the FTL and file system, and include all flash management in the file system level.

\ntytodo{can we extend this? it's mostly embedded so directly on flash anyways, also JFFS doesn't really minimize the stack, just argue for it, maybe drop this section during refactor?}
