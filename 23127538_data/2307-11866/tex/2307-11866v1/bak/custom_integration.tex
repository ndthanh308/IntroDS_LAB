\section{Custom Flash Integration}
Different types of flash-based storage devices allow for varying levels of flash management for the file system. With OCSSD, the device geometry of the underlying flash storage is exposed to the file system, allowing for specific data placement and optimizations. Multi-streamed SSDs can leverage the stream hint for the FTL to leverage increased parallelism and better data placement. Custom FTLs further extend this concept by giving the file system developers advanced interfaces and exposing distinct flash characteristics, allowing the file system to better manage data and optimize for the flash characteristics. Custom device drivers similarly allow increased device management. 

This section evaluates such storage devices, giving the host file system more flash management responsibility. We analyze the various file system implementations at this integration level, evaluating the additional flash integration modifications, which are not possible at the SSD level integration. Thus, we evaluate what additional benefits the deeper integration of custom flash devices can provide, and how file systems deal with these. We do not repeat the concepts that are discussed in the SSD integration, as they are all also applicable at this integration, instead we look only at the enhancement made by this integration on the concepts and new concepts made possible by the integration level. Table~\ref{tab:custom_integration_comparison} shows a comparison of the file systems evaluated in this section, indicating which flash challenges the particular proposed file system incorporates. Note, not all file systems in the table are discussed in the text, if they did not contribute a mechanism that is not already discussed.

\begin{table*}[!ht]
    \centering
     \begin{tabular}{||p{19mm}||p{8mm}|p{12mm}|p{15mm}|p{5mm}|p{20mm}|p{15mm}|p{5mm}|p{15mm}|p{15mm}||} \hline & & & \multicolumn{7}{|c||}{Flash Challenge} \\
        \hline
         File \newline System & Year & Type & Read/Write \newline Asymmetry & GC & I/O \newline Amplification & Flash \newline Parallelism & WL & Software \newline Stack & I/O \newline Scheduling \\
        \hline
        \hline
        Kawaguchi et al.\cite{kawaguchi1995flash} & 1995 & CD & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
        \hline
        DFS\cite{josephson2011direct} & 2011 & CD & \xmark & \xmark & \xmark & \cmark & \xmark & \cmark & \cmark \\
        \hline
        DualFS\cite{2020-Wu-DualFS} & 2020 & OCSSD & \cmark & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark \\
        \hline
         Ext4Stream\cite{2018-Rho-Fstream} & 2018 & MS-SSD & \xmark & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark \\
        \hline
         XFStream\cite{2018-Rho-Fstream} & 2018 & MS-SSD & \xmark & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark \\
        \hline
         SSDFS\cite{dubeyko2019ssdfs} & 2019 & OCSSD & \xmark & \cmark & WA \& SA & \cmark & \xmark & \xmark & \xmark \\
        \hline
         RFS\cite{lee2014refactored} & 2014 & CSC & \xmark & \xmark & \xmark & \cmark & \xmark & \cmark & \xmark \\
        \hline
         StageFS\cite{2019-Lu-Sync_IO_OCSSD} & 2019 & OCSSD & \cmark & \cmark & WA & \cmark & \xmark & \xmark & \cmark \\
        \hline
         ALFS\cite{2016-Lee-AMF} & 2016 & CFTL & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
        \hline
         OrcFS\cite{2018-Yoo-OrcFS} & 2018 & CFW & \cmark & \cmark & \cmark & \cmark & \xmark & \cmark & \xmark \\
        \hline
        ParaFS~\cite{2016-Zhang-ParaFS} & 2016 & CFTL & \cmark & \cmark & \xmark & \cmark & \xmark & \cmark & \cmark \\
        \hline
        % From here on papers added from original FS Design Considerations classification
         NBFS\cite{2021-Qin-Atomic_Writes} (based on F2FS) & 2021 & OCSSD & \cmark & \cmark & WA & \cmark & \xmark & \xmark & \cmark \\
        \hline
    \end{tabular}
    \caption{Evaluated file systems for flash-based storage with custom FTL, OCSSD, custom device driver, and multi-streamed SSD under the flash challenges (Section~\ref{sec:opt_flash}) the file system solves. A \cmark\hspace{0.5mm} indicates that a file system implements a mechanism to handle the particular flash challenge, however it does not indicate to what extent it is handled or how successful it is handled. The type column indicates if the file system is built on OCSSD, custom FTL (abbreviation C-FTL), custom driver (abbreviation CD), custom storage controller (abbreviation CSC), custom FTL (CFTL), custom firmware (CFW), or multi-streamed SSD (abbreviation MS-SSD). If a file system does not have a name and/or it is an extension on an existing file system, the authors' names are used as the file system name. For instance, the GC column contains a \xmark\hspace{0.5mm} if the file system simply implements greedy or cost-benefit GC, and a \cmark\hspace{0.5mm} if a new GC policy or new mechanisms of managing GC are presented.}
    \label{tab:custom_integration_comparison}
\end{table*}

\subsection{Read/Write Asymmetry}
The close integration of flash storage to the host software, compared to SSD integration, provides the host with more knowledge of the underlying flash characteristics, allowing for more tailored data placement and algorithm integration. The close integration further allows to integrate and possible eliminate the FTL. Kawaguchi et al.~\cite{kawaguchi1995flash} provide one of the first file systems for flash, which is integrated into the flash storage driver, taking full control over flash. Similarly, integrating the file system at an enhanced level, DFS~\cite{josephson2011direct} implements a direct file system on an optimized virtual flash storage (VFS). By combining the FTL with a flash virtualization layer, which similarly to virtual memory exposes the flash as a large addressable space as a block device. Therefore, it still relies on logical addressing, which the virtual layer translates, making it backward compatible, however exposing a 64-bit logical address space. Such a layer allows to match the physical flash characteristics such as the flash erase unit to the virtual addressing, better matching flash properties compared to exposing flash as a linear address space. 

The enhanced interface of flash management allows DFS to leave block allocation for file system data and metadata, hiding latency for flash erasure, and providing consistency operations. To account for read/write asymmetry, DFS implements a lazy LRU caching policy, which implements a bloom filter to identify if blocks are present in cache (similar to MLB caching, see Section~\ref{sec:ssd_rw_asym}). However, on a cache miss is the data block only copied into the user memory, and not the cache. Only on the second miss is the data cached in LRU policy, allowing the cache to only contain the frequently accessed data.

Some modern flash devices allow the host to switch the cell level of underlying flash blocks (e.g., switching MLC to SLC)~\cite{2020-Wu-DualFS,li2019accelerating}. This provides the benefit that a lower cell level, representing less bits, has a lower read, write, and erase latency, thus providing the flash with a higher performance than with a larger cell level configuration~\cite{2020-Wu-DualFS}. This switching between flash modes is referred to as \textit{flash dual mode}, which however comes at the cost of being able to store less data in the same amount of flash as the block is only able to store half the data, in the example of switching from MLC to SLC. DualFS~\cite{2020-Wu-DualFS} utilizes the flash dual mode feature to provide a dynamically sized SLC area, alongside the remaining MLC area, to accelerate the performance of critical I/O requests. By evaluating the I/O queue depth of I/O requests, DualFS determines the criticality of the incoming request, and maps it to the SLC area for increased performance. It further profiles incoming request based on the hotness and allocates hot data into the SLC mode for lower request latency.

StageFS~\cite{2019-Lu-Sync_IO_OCSSD} overcomes the lower write performance of flash by utilizing multiple stages for file system updates. Firstly, writes are written into the first stage, called the \textit{file system staging area}. Writes to the staging area are completed in a \textit{persistence-efficient} way, which matches the write updates to the operations more optimal for flash, which are smaller fine-fine grained updates~\cite{2013-Lu-Flash_Lifetime_Reduce_With_WA} in a log structured fashion~\cite{2015-Changman-f2fs}. Secondly, writes are then regrouped, based on the file system structure and hot/cold identification, and are written to the \textit{file system image} second stage based on the group assignment in parallel over the flash parallel units (solving \textbf{FC4}). The staging area allows to write synchronous I/O directly to the staging log with optimal flash write characteristics, lowering completion latency (solving \textbf{FC7}), followed by better grouping of data when writing from the staging to the file system area.

\subsection{Garbage Collection}
Similar to the SSD integration, grouping of data based on its access frequency is a commonly used approach to mitigate GC overheads. However, the deeper integration allows to better map the hot/cold data area to specific regions on the flash, independent of the FTL intervention as with the SSD integration. Such a separation is implemented by Fstream~\cite{2018-Rho-Fstream}, which modify ext4~\cite{cao2007ext4} and xfs~\cite{sweeney1996scalability} to map different operations to different streams on a stream SSD. Ext4Stream, the modified ext4 to support streams, maps different metadata operations to different streams, including the journal writes for consistency, the inode writes, as well as different streams for the directory blocks and the bitmaps (inode and block). Furthermore, it utilizes different streams that can be created for different files and for different file extensions. The goal of such streams is to map particular files, such as LOG files for key-value stores for example to a particular stream, separating its access patterns from that of other files and file system data. Similarly, the modified XFStream utilizes different streams for the log, inodes, and specific files.

The close integration to flash allows the file system to maintain closer control, allowing specific garbage collection mechanisms to enhance the GC overhead. For this, SSDFS~\cite{dubeyko2019ssdfs} manages the storage with segments, which are mapping to a number of \textit{physical erase blocks} (PEB). Each PEB is a multiple of the flash erase unit, aligning data writing and GC to the physical unit. In order to improve on GC overheads, SSDFS employs PEB self-migration, which relies on each PEB having an associated clean PEB that upon updates can be migrated to. Furthermore, incorporating hot/cold data grouping into this scheme allows to that groups into PEBs, such that during an update all hot data can be moved to the associated PEB, invalidating the prior original PEB which can then be erased without requiring to run GC.

A different approach to mitigate GC overheads is to design the GC procedure such that accesses do not suffer from high tail latency when GC is running. TTFlash~\cite{2017-Yan-Tiny_tail} achieves this through several mechanisms. It implements \textit{plane-blocking GC}, which limits any resources that are blocking I/Os to only the affected planes on the flash. However, this leads to blocking of requests to the GC affected planes. Therefore, TTFlash implements \textit{rotating GC} that only runs at most one GC operation on a \textit{plane group}, which are planes in parallel channels allowing planes to be accessed in parallel. This ensures that a plane group is never blocked for more than a single GC operation, implying that any request will not be blocked by more than one GC operation. Additionally, with the presence of capacitors on the flash device, which can flush the memory buffers (at least parts of the buffers if not using expensive capacitors that can flush the entire device memory), GC affected write operations buffers blocked writes in memory and flushes at a later point, allowing to complete the I/O without waiting for GC to complete. TTFlash refers to this mechanism as \textit{GC-tolerant flush}.

The garbage collection cost of foreground cleaning in F2FS can take up to several seconds~\cite{2018-Yoo-OrcFS}, as it is not a preemptive task. Implementing preemptive scheduling in Kernel file systems is challenging as during the preemption the Kernel has to store the file system state to continue after higher priority tasks have finished, however there can be modified metadata from the garbage collection. OrcFS~\cite{2018-Yoo-OrcFS} implements \textit{quasi preemptive segment cleaning} (QPSC), which sets a maximum time interval $T_{max}$ (default of 100ms). After cleaning a segment it checks if the timer has expired, and if so it checks if outstanding writes are present from the host. If there are outstanding writes the locks are released and the write is executed, and if there are no outstanding writes the next segment can be cleaned and the timer is reset. This allows any host write command to not encounter a segment cleaning overhead higher than $T_{max}$.

ParaFS~\cite{2016-Zhang-ParaFS} coordinates the file system garbage collection with the garbage collection in the FTL, reducing the migration times and Overprovisioning space for GC. The file system runs greedy garbage collection on its log structure to free segments, and the FTL runs cost-benefit cleaning on the space that is being erased by the file system. Therefore, space management and garbage collection are completed by the file system, and the erasing of physical blocks is completed by the FTL. This allows to better manage GC overheads at the file system level, and avoids duplicate operations from the file system and FTL GC.

\subsection{I/O Amplification}
While write amplification can already be significantly reduced by avoiding unnecessary garbage collection through the prior discussed mechanisms, a major contributing factor in write amplification is the modification of pointers, which only require several bytes, however on an update at least a full flash page is required to be written, or in the cases of LFS an inode block needs to be written, which can comprise of multiple flash pages. One approach at solving this is employed in OFTL~\cite{2013-Lu-Flash_Lifetime_Reduce_With_WA}, an object-based FTL implementation, that utilizes \textit{backpointer-assisted lazy indexing}. It relies on inversely indexing the associated metadata to objects in the data page itself. The metadata page then contains a reverse index to the object, thus on a data update avoiding to rewrite the object metadata page, reducing write amplification. While the implementation focuses on object-based mapping, the concept can be applied to file system management. However, in order to avoid increased scan times on failure recover, after power failure, where only one page is updated, thus requiring to scan the entire region, the updated pages are tracked to indicate the most recently updated valid page. The tracking is then periodically included in the checkpointing to ensure consistency, which OFTL refers to as the \textit{updating window}.

Similar to file systems at SSD integration, file system at the custom flash integration also employ on data compression, which allows to decrease the space amplification, as well as using inline data for small files. Inline data writes the content into the inode, utilizing the inode space which would be otherwise written at the allocation unit without data, and avoiding the extra allocation to write the data. SSDFS~\cite{dubeyko2019ssdfs} incorporates all these concepts, and additionally includes delta-encoding to only write changes on data updates, instead of writing the entire new data, as well as deduplication.

A similar goal of file systems is to reduce the amount of memory that is required for the mapping table to maintain the logical-to-physical mappings. A common solution is to increase the granularity of the mapping table (e.g., block-level mapping instead of page-level mapping), requiring less mappings. However, this comes at the cost of having a larger allocation unit, and if a host write is smaller than the allocation unit it causes write amplification due to the partial flash page write when the flash page size and the file system allocation unit (block size) are not identical~\cite{2018-Yoo-OrcFS,lee2007log}. OrcFS~\cite{2018-Yoo-OrcFS,lee2007log} implements \textit{block patching} to solve this issue, which takes write requests that are smaller than the flash page size and pads the remaining space with dummy data to align the write request to flash page size. This mechanism avoids copying data if a flash page is partially written and the next LBA (in the same flash page) is written, which would trigger a copy of all LBAs in the flash page followed by writing the LBA of the new write. For instance, a flash page contains 4 LBAs, if LBAs 1-3 are written by one request and a second request to LBA 4 is issued, it cannot fill the flash page as it has already been written, therefore requiring to copy LBAs 1-3, add the new write to LBA 4 to it, and write the 4 LBAs to a new flash page. Adding of dummy data to fill pages reduces the write amplification of copying LBAs, however it still adds write amplification to fill the flash page.

\subsection{Flash Parallelism}
The hardware organization of flash allows to address flash dies and channels in parallel, which FTLs commonly do to increase performance. Architectures utilizing multiple flash chips also allow these to be addressed in parallel. At the SSD integration, the FTL commonly stripes data across the parallel units, which is hidden from the host, as the FTL controls the physical data mapping. SSDFS~\cite{dubeyko2019ssdfs} utilizes the closer integration to map data allocation of segments to PEBs, which are aligned to the physical erase unit, and additionally the allocated PEBS can be split over the available parallel unit. Therefore, utilizing PEBs over the parallel unit (e.g., flash channels) allows to stripe writes into a segment over the varying channels, increasing the device parallelism. In order to achieve this, I/O requests have to be large enough such that they can be striped across the channel and fill the PEBs mapped to the segment. SSDFS, utilizes aggressive merging of I/O requests to achieve the larger I/Os that can be striped across the parallel units.

Co-designing the FTL and the file system allows to remove uncoordinated duplicate work, and coordinate the flash management. To this end, Lee et al.~\cite{lee2014refactored} present a redesigned I/O architecture, called REDO, which avoids the duplicate operations from file system and FTL by implemented the new framework directly as the storage controller, and building the refactored file system (RFS) on top of the new controller interface. By combining the file system operations with the storage controller, the file system is responsible for running GC and managing the storage, which is made possible by REDO giving it direct access to the storage the file system managing logical-to-physical mappings. REDO implements the bad block management and wear leveling. The file system is based on LFS, and thus is also organized in segments. However, logical segments are mapped directly to physical segments, where pages in the logical segment are mapped to the physical pages across the flash channels. This firstly allows to utilize the device parallelism with the flash channels and secondly avoids to maintain a logical-to-physical mapping table for pages, as it only requires segment mappings which contain numerous pages. Similarly, OrcFS~\cite{2018-Yoo-OrcFS} implements its file system allocation unit as a \textit{superblock} (not to be confused with the file system superblock), which represents a set of flash blocks. These flash blocks are then split over the parallel units of the flash storage, such as the flash channels, allowing increased parallelism.

Exploiting the flash parallelism relies on utilizing the parallel unit, such as the flash channels. However, there are several methods that these can be utilized, as mentioned earlier in OrcFS, striping superblocks across the flash channels can utilize the channels and increases parallelism. The large allocation unit however introduces increased garbage collection overheads. Furthermore, block-level striping has lower performance than page-level striping~\cite{2016-Zhang-ParaFS}. Therefore, ParaFS~\cite{2016-Zhang-ParaFS} implements a 2-D allocation scheme, with page-level striping over the flash channels, where striping is also based on the data hotness, hence having a 2-dimensional allocation scheme. Different groups are assigned for the hotness levels, where writes are issued to the corresponding hotness group.

\subsection{Wear Leveling}
With the importance of ensuring that flash cells are utilized evenly, and no particular area burns out faster than others, devices ensure even wear across the device. At the SSD integration this is responsibility of the FTL, where the file system has little control over the physical mapping of data. With custom integration that can provide more possibility for higher software levels to manage physical mappings, there is an increased responsibility to manage wear leveling. Particular to the file system implementations that utilize the flash dual mode, which causes the lower cell level area (e.g., SLC area) to burn out quicker than the higher cell level area, due to writing the same amount of data, but not being able to storage the same amount. Therefore, DualFS~\cite{2020-Wu-DualFS} implements a \textit{write budget} in order to ensure that writes to the SLC area are not burning out, incorporating the write budget in the SLC area admission which decides if data should be written to the SLC or MLC area, resorting to writing data to the MLC area if necessary to avoid burning out the flash in the SLC area.

\subsection{Software Stack}
Similar to prior discussed file systems at SSD integration level, several file systems with close flash integration also minimize the software stack through various mechanisms. In particular, closer integration to the flash storage allows to remove parts of the redundancy present in the conventional software stack, where the FTL does independent data management and operations, uncoordinated with the file system and software layers above the storage device. At the SSD integration level a significant amount of duplicate work is done by the file system and the flash storage. LFS requires to run garbage collection to free space in its log, and the FTL similarly requires to run garbage collection on the flash to free erase blocks. Stacking multiple layers with uncoordinated operations on top of each other, such as a log-based file system and a log-based FTL, generates duplicate work, which in turn leads to significant performance impacts~\cite{2014-yang-dont_stack_log_on_log}. Co-designing of the FTL and file system has shown benefits of reduced memory requirements for logical-to-physical mappings, and increased channel parallelism through file system information~\cite{2013-Qiu-Codesign_FTL_FS}.

Therefore, merging operations of the file system with the FTL can provide performance gains, which are not possible at the SSD integration level, but at the custom flash integration level. DualFS~\cite{2020-Wu-DualFS} utilizes the custom integration with OCSSD to merge the garbage collection of the file system with that of the FTL, and present this scheme as \textit{global garbage collection}. Similar to file systems at SSD integration, avoiding Kernel intervention in the I/O path can minimize I/O overheads. Direct accesses file systems, including DFS~\cite{josephson2011direct}, avoid such an I/O path and provide direct access to the underlying storage.

A different approach taken by Lee at al.~\cite{2016-Lee-AMF} is to modify the block interface with the flash characteristics, moving responsibility directly to the file system, or other application built on top of it. The resulting interface called \textit{application managed flash} (AMF) exposes a block interface that does not allow overwriting unless an explicit erase is issued for the blocks. This matches the flash requirement that prior to overwriting data it has to be erased. The interface is implemented as a custom FTL, called AFTL, and the authors built a file system (ALFS) with this new interface. This avoids the duplicate garbage collection of the FTL and the file system, as the garbage collection of the ALFS erases blocks during garbage collection, informing the FTL to erase the physical block.

\subsection{I/O Scheduling}
Similar to the scheduler merging I/O requests in the I/O path for SSDs, custom flash integration incorporates the concept of merging similarly. The VFS layer on top of which DFS~\cite{josephson2011direct} is built, can merge all incoming requests, as virtualized storage can map logical to physical addresses similar to virtual memory, merging incoming block writes is achieved when the VFS decides on the physical mapping. A further issue of the I/O path are the overheads of the operating system and file system. Moneta-D~\cite{caulfield2012providing}, based on the original Moneta project~\cite{caulfield2010moneta}, implements this by bypassing the file and operating system, and providing \textit{channels} to the storage for each process. Channels are virtual interfaces to the storage area for the accessed region, and ensure that correct permissions are enforced for each process, which allows bypassing the file and operating system permission checks. Permission checks are instead enforced at the hardware level, only giving access to valid processes. Once permissions are established for a channel, an emulated in memory register is written to execute commands on the storage area. Commands are executed over DMA on the channel. With a user-space library, against which application can be linked, are the library calls replaced to go to the Moneta-D driver, instead of through the file system and Kernel, thus allowing to minimize the software stack and avoid overheads (solving \textbf{FC6}).

ALFS~\cite{2016-Lee-AMF}, which is built on a custom FTL which does not allow overwrites, but rather requires the application to issue an erase before blocks can be overwritten, exploits the flash parallelism by mapping consecutive segments to the flash channels and utilizing different I/O queues for each flash channel. Each flash channel has a separate I/O queue, upon which I/O requests are sorted when issued to the flash controller. This striping of segment data allows exploiting the flash parallelism, resulting in higher performance, and furthermore handles I/Os arriving in random order, due to possible rescheduling in the Kernel block layer. As I/Os are sorted per channel, such that for instance writes at page 0 go to channel 1, writes at page 1 go to channel 2, and so forth, they can be assigned to the correct channel independent of the arrival order. Furthermore, AFTL utilizes multiple segments in parallel, where data grouping and checkpointing can use different channels. 

Similarly, ParaFS~\cite{2016-Zhang-ParaFS} implements \textit{parallelism-aware scheduling}, which also maintains different I/O queues for each channel. However, it extends this concept by using a \textit{dispatching phase} and a \textit{request scheduling phase}. The dispatching phase optimizes write I/Os with a scheduling to the respective channels based on the utilization of the channel, such that the least utilized channels receives more requests. All requests are assigned a weight, which indicates their priority in the queue, where read requests weight is lower than that of write requests, because of the asymmetric performance of flash storage. During the request scheduling phase the scheduler assigns slices the read and write/erase operations in the individual queues, such that if the time from the slice of a read operation is up and the queue contains no other read requests, a write or erase is scheduled, based on a fairness formula that incorporates the amount of free space in the block and concurrently active erase operations on other channels. This allows to minimize the erase operations on the flash, giving always free channels to utilize and maintain a fair schedule between write and erase operations.

Qin et al.~\cite{2021-Qin-Atomic_Writes} argue that I/O ordering limits from utilizing the parallelism of flash devices. Especially as the Linux block layer does not guarantee particular ordering, flags such as \textit{forced unit access (FUA)}, indicating that I/O completion is signaled upon arrival of data on the storage, and \textit{PREFLUSH}, which before completing the request flushes the volatile caches, have to be set in order to ensure a specific ordering~\cite{2021-Qin-Atomic_Writes}. With file systems, the I/O of metadata and data has a particular ordering, such that metadata can only point to data that exists, needing to ensure that data is written prior to metadata. Removing of I/O ordering allows to eliminate this need and better utilize the flash parallelism. Utilizing the OOB area on flash pages, the file system developed by Qin et al., called NBFS, maintains versioning in order to identify out of order updates. Furthermore, updates are done using atomic writes (discussed in Section~\ref{sec:failure_consistency}). The issuing of \textit{FUA} requests further implies that its I/Os cannot be merged in the scheduler~\cite{2021-Qin-Atomic_Writes}, implying that if a smaller than flash page size \textit{FUA} I/O request is issued, it is padded to the page size, causing write amplification. NBFS solves this with its atomic writes that imply that the \textit{FUA} request does not immediately have to be written to the flash, but instead wait for all data blocks to arrive, which are then used to fill the pages, allowing to reduce the write amplification (\textbf{FC3}).
