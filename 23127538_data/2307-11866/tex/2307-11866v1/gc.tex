\section{Garbage Collection}\label{sec:gc}
A significant challenge of flash storage is \as{gc} overheads having unpredictable performance penalties for the host system~\cite{2015-Kim-SLO_complying_ssds,2014-yang-dont_stack_log_on_log}, resulting in large tail latency~\cite{2013-Dean-tail_at_scale}. Dealing with, and aiming to minimize required garbage collection for the flash device is a key challenge in integrating flash storage. Naturally, as flash storage does not provide in-place updates, data is written in a log-based fashion, sequentially in the flash blocks. Therefore, over time as data is overwritten, the blocks contain an increasing number of invalid pages that must be erased to free space. However, as the block also contains valid data, and the erase unit is a block, the still valid pages are moved to a new block, such that the old black can be erased. 

% Figure environment removed

In a \as{lfs} updates to file data are written at the head of the log, resulting in the parts of the file that are updated to be located in a different flash block than the parts of the original file data. Furthermore, \as{gc} causes file data to moved around the storage space as well, resulting in scattering of parts of files, referred to as \textit{fragmentation}. Figure~\ref{fig:fragmentation} shows the resulting fragmentation for several files that occurs over time from file updates and \as{gc}. Fragmentation results in increased read time, due to files being in non-contiguous regions, and introduces increased garbage collection overheads due to the failed grouping of data. Ji et al.~\cite{2016-Ji-Framgentation_empirical_study} show an empirical study on fragmentation in mobile devices, identifying that fragmentation introduces performance degradation due to increased \as{io} requests, and it further produces increased pressure on caching. The effects on caching are due to increased difficulty of prefetching data, since it no longer is in contiguous physical ranges, but scattered throughout the physical space. Therefore, prefetching cannot bring correct data into the caches, resulting in an increase in cache misses. 

In addition to increased \as{io} requests for reading and rising cache pressure, increased garbage collection is caused when frequently modified files or file fragments, referred to as the \textit{hot data}, are in the same block as rarely accessed files, referred to as the \textit{cold data}. When hot data is modified, its flash pages are invalidated. Once enough flash pages in a block are invalidated, it can be erased. However, if it is co-located with cold data, the cold must be copied to a free space, as it is still valid. If this cold data is then again co-located with hot data, the modifications of the hot data cause the block to be cleared during \as{gc}, which requires the cold data to be moved again. Therefore, co-locating hot and cold data in the same physical erase unit results in significant \as{gc} increase due to the unnecessary moving of the cold data.

\begin{table}[!t]
    \centering
    % Update reference format to fit table into column
    \crefformat{section}{\S#2#1#3}
    \crefformat{subsection}{\S#2#1#3}
    \crefformat{subsubsection}{\S#2#1#3}
    \begin{tabular}{||p{40mm}|p{35mm}||}
        \hline 
        Mechanism & File Systems \\
        \hline
        \hline
        Reducing Write Traffic (\cref{sec:reduce_write_traffic}) & \cite{huang2013improve,2011-Lim-DeFFS,woodhouse2001jffs,dubeyko2019ssdfs,2021-Ji-F2FS_compression,2007-Hyun-LeCramFS,ning2011design,2009-Zuck-NANDFS,2022-Lee-F2FS_Address_Remapping} \\
        \hline
        Aligning the Allocation Unit (\cref{sec:align_alloc}) & \cite{2020-Tu-URFS,2009-Park-Multimedia_NAND_Fs} \\
        \hline
        Data Grouping (\cref{sec:data_grouping}) & \cite{2006-Lim-NAND_fs,2022-Zhang-ELOFS,2020-Zhang-LOFFS,2015-Changman-f2fs,2018-Rho-Fstream} \\
        \hline
        \as{gc} Policies (\cref{sec:gc_policy}) & \cite{2012-Min-SFS,2018-Yoo-OrcFS,2021-Gwak-SCJ,2015-Park-Suspend_Aware_cleaning-F2FS} \\
        \hline
        Coordinating the Software Stack Layers (\cref{sec:software_stack}) & \cite{2020-Wu-DualFS,2016-Zhang-ParaFS,2016-Lee-AMF,lee2014refactored} \\
        \hline
    \end{tabular}
    % Reset reference format to before
    \crefformat{section}{Section #2#1#3}
    \crefformat{subsection}{Section #2#1#3}
    \crefformat{subsubsection}{Section #2#1#3}
    \caption{Mechanisms for file systems to deal with \as{gc} overheads from flash storage, and the respective file systems that implement a particular mechanism.}
    \label{tab:gc}
\end{table}

Reducing the write traffic to the storage device, an effective method to handle the asymmetric flash performance (see \cref{sec:reduce_write_traffic}), is a solution to minimize fragmentation and reduce possible future \as{gc} overheads. However, additional mechanisms are required for effective \as{gc} management. With fragmentation being a significant contribution to increased garbage collection, avoiding it is a key objective to reducing garbage collection overheads. Fragmentation is classified into three different types~\cite{2007-Sato-defrag}. Firstly, \textit{single file fragmentation}, where data in a single file is dispersed over the storage (as is shown in Figure~\ref{fig:fragmentation}). Secondly, \textit{relevant file fragmentation}, where files that are relevant to each other and should be grouped together are split over the storage, such as co-locating hot and cold data in the same erase unit. Lastly, \textit{free space fragmentation}, where the file system has a large amount of small free space, because of deletion of dispersed small files. The cause of fragmentation occurring over time is referred to as \textit{file system aging}~\cite{smith1997file}. While several tools exist that implement \textit{defragmentation}~\cite{hahn2017improving,f2fs_defrag_tool,park2021fragpicker}, additional mechanism can be utilized to avoid fragmentation and \as{gc} overheads. \cref{tab:gc} depicts the mechanisms for file systems to deal with and minimize garbage collection overheads. The process of countering the different types of fragmentation is commonly referred to as \textit{storage gardening}~\cite{2020-Kesavan-Fragmentation,2019-Kesavan-Storage_Gardening}, and for file system development various aging tools exist in order to generate real-world file system workloads and simulate file system aging~\cite{2019-Conway-FS_aging,2018-Kadekodi-Geriatrix}. 

\subsection{Aligning the Allocation Unit}\label{sec:align_alloc}
\as{gc} is a result of having to move valid blocks in the erase unit to a free space, in order erase the flash block. While data grouping allows to align the validity inside the block, such that blocks are likely to be updated within close proximity, multiple files may be co-located in the same block. Therefore, a similar method is to align the allocation unit of data blocks for a single file to the erase unit, resulting in only a single file being located in a block. Such a mechanism is implemented in URFS~\cite{2020-Tu-URFS}, which aligns the data allocation unit for large files to the flash erase unit, allowing files to be erased as a single unit, limiting required \as{gc}. However, as the erase unit of flash can be several hundreds MBs, resulting in significant over-allocation for small files, it makes such a mechanism only beneficial with large files. NAMU~\cite{2009-Park-Multimedia_NAND_Fs} similarly showcases a file system that aligns its content with the requirement of large files. Focused on the multimedia domain, where files have the particular characteristics of rarely being modified, and if removed all file data blocks are erased in one unit, \as{gc} in NAMU is done at the granularity of a file. In addition to improving on \as{gc}, the memory requirements for mapping tables are also minimized. For generic file systems that vary in file characteristics, \af{ars}~\cite{2020-Yang-F2FS_Framentation} minimizes the issue of over-allocating space by allocating in a smaller unit of 2MB (a single segment). File data is written to the space until it is exhausted, upon which a new segment is allocated. While this does not map entire files to the flash erase unit, it allows writing of file data sequentially for each segment, eliminating fragmentation to a degree. 

\subsection{Data Grouping}\label{sec:data_grouping}
A key circumvention method for fragmentation relies on grouping of related data. Most commonly this is applied in the type grouping data by its access and modification frequency into hot and cold data, however other less commonly used groupings based on \textit{death-time prediction} exist~\cite{2021-Chakraborttii-DT_LBA}. Commonly more classifications than simply hot and cold are utilized for more effective grouping. A plethora of methods for grouping data in such a way have been proposed, which we split by its application type into several groups.

\subsubsection{Data Type Grouping}
Common write patterns in storage systems follow a bi-modal distribution, where many very small write requests and a numerous very large write requests are issued~\cite{chang2008hybrid}. This stems from the fact that small changes are caused by metadata updates, which occur the most frequently, whereas large changes are file updates. Given that metadata is more likely to be updated frequently, separating metadata from data improves on required garbage collection. Specifically, since metadata is often updated even if the data is not updated, for example in scenarios where the file attributes (access time, permissions, etc.) are updated, or the file is moved. Therefore, based on the request size, the data can be classified to be metadata and be grouped accordingly. F2FS also groups based on the data type, where metadata is considered to be always hot data, which is implemented by similar file systems~\cite{2006-Lim-NAND_fs}.

Dividing of file system data is similarly applied in ELOFS~\cite{2022-Zhang-ELOFS,2020-Zhang-LOFFS}, which splits the flash storage two partitions, where a directory partition contains the data of directory entries, and a data partition contains the file system data, which is compacted with the inodes. Jung et al.~\cite{jung2010process} propose the addition of classifying data based on the \af{pid}, as a process is likely to generate similar access patterns and data types throughout its lifetime, classifying by the \as{pid} allows to indirectly infer a data type. A similar data type separation is implemented by Fstream~\cite{2018-Rho-Fstream}, for which the authors modify ext4~\cite{cao2007ext4} and xfs~\cite{sweeney1996scalability} to map different operations to different streams on a stream \as{ssd}. Ext4Stream, the modified ext4 to support streams, maps different metadata operations to different streams, including the journal writes for consistency, the inode writes, as well as different streams for the directory blocks and the bitmaps (inode and block). Furthermore, it utilizes different streams that can be created for different files and for different file extensions. The goal of such streams is to map particular files, such as LOG files for key-value stores for example to a particular stream, separating its access patterns from that of other files and file system data. Similarly, the modified XFStream utilizes different streams for the log, inodes, and specific files.

\subsubsection{Dynamic Grouping}\label{sec:dynamic_grouping}
While grouping is an effective method for minimizing \as{gc}, they rely on static definition on classification targets for the number of hot/cold degrees to classify to. Shafaei et al.~\cite{2016-Shafaei-WA_extent_temp} identify that the majority of hot/cold data grouping methods fail to account for the accuracy in the hot/cold grouping mechanism, as well as relying on an individual classification of each LBA, making the management of increasingly larger flash storage difficult. Therefore, Shafaei et al.~\cite{2016-Shafaei-WA_extent_temp} propose an extent-based temperature identification mechanism. It is based on the density stream clustering problem~\cite{jia2008grid,chen2007density,forestiero2013single,isaksson2012sostream}, which is a common approach of classification in artificial intelligence and stream processing, however has not been applied to storage before. The density-based stream clustering groups data in a one dimensional space as the data arrives, hence its applicability for stream processing. 

Applying this method to storage, the one-dimensional space is the range of \as{lba}s, and extent-based clustering splits the available space into a number of extents to group by. Initially, the entire space is a single extent and as writes occur the extent is split into smaller extents with different classifications. Over time as more writes are issued, extents are expanded and merged (merging of extents with the same classification). Such a grouping allows a more detailed grouping due to the increase in classification targets, compared to binary hot/cold grouping. However, an evaluation by Yang and Zhu~\cite{2015-Yang-algebric_WA_modeling} on a configurable garbage collection policy, where the number of hotness classification targets is evaluated, shows that various hotness classification targets can significantly increase the write amplification during garbage collection. 

\subsubsection{\as{lba} Hotness Classification}\label{sec:lba_classification}
Different to grouping data based on its type, hotness can be classified on the \as{lba} based on its access frequency. The naive approach at modeling hotness for each \as{lba} is with table-based classification model~\cite{hsieh2005efficient}. This however comes at a high overhead cost, as an entry for each LBA is needed, which becomes increasingly expensive as flash storage grows.  Therefore, a more non-trivial method is based on two-level \as{lru} classification~\cite{chang2002adaptive}, with two LRU lists. Upon an initial \as{lba} access, the \as{lba} is stored in the first list, and a subsequent access moves it to the next list, which is referred to as the \textit{hot list}. Therefore, if an \as{lba} is in the hot list, it is considered to be frequently accessed. 

A different approach is implemented by \af{mbf}~\cite{jagmohan2010write,park2011hot}, which uses bloom filters to identify if an \as{lba} is hot. Bloom filters rely on a hash function that, given an input such as the \as{lba}, provide an output which is mapped to a bit array, and sets the bit to true. Therefore, if an \as{lba} is accessed, applying the hash function sets the respective bit in the array to true, and checking if an \as{lba} is hot simply applies the hash function and checks if the bit is set. However, depending on the length of the array, multiple \as{lba}s can map to the same bit location, as proven by the pigeonhole principle~\cite{ajtai1994complexity}, resulting in false positive hotness classifications for an \as{lba}. To avoid frequent false positive classifications, \as{mbf} utilizes multiple bloom filters at the same time. With multiple bloom filters, the same amount of arrays exist, applying all bloom filter hash functions to the \as{lba}, and setting the respective bit in each of the arrays. As a result, collisions on all bloom filter hash functions are less likely, minimizing the possibility for false positives.

Similar to \as{mbf}, Kuo et al.~\cite{kuo2006configurability} present a hot data identification method by using multiple hash functions and a hash table. Upon a write, the \as{lba} is hashed by multiple hash functions, and a counter for each hash function is incremented in a hash table. To check if an \as{lba} is hot, the \as{lba} is hashed and a configured $H$ most significant bits of the resulting hash table indicate if the \as{lba} is hot if they are non-zero, as the counter is increased on accesses the most significant bits are only non-zero if the \as{lba} is frequently accessed. Multiple hash functions are used for the same reason multiple bloom filters are used in \as{mbf}, to avoid false positive classifications. Lee and Kim~\cite{2013-Lee-data_grouping_empirical_study} provide a study into comparing performance of two-level \as{lru}, \as{mbf}, and \af{dac}, which are similar to the density-based stream clustering by Shafaei et al.~\cite{2016-Shafaei-WA_extent_temp} (discussed in the prior \cref{sec:dynamic_grouping}). The authors show that on the evaluated synthetic workloads \as{dac} provides the highest reduction in write amplification factor, which in turn leads to a decrease in \as{gc} overheads.

Unlike all prior approaches basing classification on the access frequency directly to identify hotness, Chakraborttii and Litz~\cite{2021-Chakraborttii-DT_LBA} propose a temporal convolutional network that predicts the \textit{death-time} of an \as{lba}, based on modification history. This allows to more optimally group data based on the death-time of individual \as{lba}, which has been shown to be an effective grouping mechanism~\cite{2017-He-SSD-Unwritten-Contract}. Grouping related death-time LBA reduces the required garbage collection, as blocks containing \as{lba}s with similar death-times are erased together, which in turn reduces the write amplification (solving \textbf{\as{fic}3}).

\subsection{Garbage Collection Policies}\label{sec:gc_policy}
While data grouping provides benefits of co-locating data based on their update likelihood, an additional essential part of garbage collection is the policy of victim selection for segments to clean. Since during garbage collection a segment to clean is required to be selected, where all still valid data in the segment is moved to a free space, selecting a victim becomes non-trivial. With the importance of data grouping with hotness for effective \as{gc}, conventional \as{gc} policies, such as greedy and cost-benefit lack their inclusion. SFS~\cite{2012-Min-SFS} proposes the \textit{cost-hotness} policy to account for the hotness of segments instead of the segment age, better incorporating the data grouping into victim selection. Similar to the cost-benefit policy (recall \cref{sec:bg_lfs}), the cost-hotness is calculated as
\begin{equation*}
    \text{cost-hotness}=\frac{\text{free space generated}}{\text{cost}*\text{segment hotness}}=\frac{(1-u)*\text{age}}{2u*h}
\end{equation*}
where the cost considers reading and writing the valid blocks (equivalent to $2u$) with the segment hotness. A further \as{gc} policy that is based on the cost-benefit policy, is the \af{cat} policy~\cite{chiang1999cleaning}. It extends cost-benefit by including an erase count for each block, improving wear leveling (solving \textbf{\as{fic}5}).

The majority of \as{gc} policies have a fixed algorithm, limiting configuration possibility. The \textit{$d$-Choice} algorithm~\cite{van2013mean} is a configurable \as{gc} policy that combines greedy selection with random selection. The tunable parameter $d$ defines the number of blocks to be selected randomly out of the $N$ total blocks. Therefore, configuring $d=1$ results in fully random victim selection, as a single block is randomly selected from the total blocks, providing effective wear leveling through randomness~\cite{2015-Yang-algebric_WA_modeling} (solving \textbf{\as{fic}5}). Configuring $d\rightarrow \infty$ selects a larger subset of blocks to use greedy selection on, such that $d=N$ is equivalent to fully greedy victim selection, allowing to provide the lowest cleaning latency. Configuration of $d$ thus allows to define the tradeoff between wear leveling and performance. 

An evaluation by Yang and Zhu~\cite{2015-Yang-algebric_WA_modeling} of the algorithm shows the significance of the number of hotness classification targets that are utilized, and the configuration of the $d$ parameter, where various hotness classification targets can significantly increase the \as{wa} during \as{gc}. Similarly, \af{fagc}~\cite{yan2014efficient} is a \as{gc} policy that maintains an \af{uft} for each \as{lba} of a file, in order to group valid pages in the victim block based on the access frequency when these are copied to a new block during \as{gc}. This is similar to grouping hot and cold data, but at the level of \as{gc} for each \as{lba} in a file. SFS~\cite{2012-Min-SFS} implements a \as{gc} policy that accounts for data grouping, with a lower overhead of having maintaining a \as{uft} for each file. It maintains a hotness classification for each block, and combines blocks with k-means clustering~\cite{hartigan1979algorithm}, into groups with similar hotness classification. 

The \as{gc} cost of foreground cleaning in F2FS can take up to several seconds~\cite{2018-Yoo-OrcFS}, as it is not a preemptive task. Implementing preemptive scheduling in Kernel file systems is challenging, as during the preemption the Kernel has to store the file system state to continue after higher priority tasks have finished, and the state being restored when returning depicts possibly outdated data. Due to the continued writing on the file system, restoring the prior state may no longer be valid, as ongoing writes may have changed file system metadata. OrcFS~\cite{2018-Yoo-OrcFS} implements \af{qpsc}, which sets a maximum time interval $T_{max}$ (default of 100ms). After cleaning a segment it checks if the timer has expired, and if so it checks if outstanding writes are present from the host. If there are outstanding writes, the locks are released and the write is executed, and if there are no outstanding writes the next segment can be cleaned and the timer is reset. This allows any host write command to not encounter a segment cleaning overhead higher than $T_{max}$.

A further drawback of segment cleaning with \as{lfs}, in particular F2FS, is that the modification of metadata during segment cleaning requires a checkpoint to be created after each segment clean. This constitutes to a significant overhead for the segment cleaning process. To avoid the excessive checkpointing after segment cleaning, \af{scj}~\cite{2021-Gwak-SCJ} adds support to F2FS to journal metadata updates made during segment cleaning, instead of creating a checkpoint. This journal is stored in a journal area, which delays the updating of the original metadata until the journal becomes large enough or the checkpointing time interval is reached. However, metadata still points to old invalid data blocks (referred to as \textit{pre-invalid blocks}), which requires that data only be invalidated once the metadata is updated by the \as{scj}. Therefore, \as{scj} implements an adaptive checkpointing that evaluates the cost of checkpointing to flush the metadata updates and the accumulation of pre-invalid blocks, and checkpoints if its cost is lower. 

In addition to utilizing \as{gc} policies to reduce its overheads, the policy can further be used to incorporate management of fragmentation. Park el at.~\cite{2016-Park-LFS_Defragmentation} propose to use a \af{vbq} in which valid blocks are sorted during garbage collection. Typically, a victim segment in the \as{lfs} is selected for cleaning, valid data is copied to the free space at the log head, and the old segment is erased. The \as{vbq} is added such that after a victim segment is selected, it is copied into the \as{vbq}, where the blocks it contains are sorted by the \as{inode} number. Then are the valid blocks written to a new segment and the old one is erased. This sorting allows to maintain file associated blocks together based on their \as{inode} number. 

A different approach to mitigate \as{gc} overheads is to design the \as{gc} procedure such that accesses do not suffer from high tail latency when \as{gc} is running. TTFlash~\cite{2017-Yan-Tiny_tail} achieves this through several mechanisms. It implements \textit{plane-blocking GC}, which limits any resources that are blocking \as{io}s to only the affected planes on the flash. However, this leads to blocking of requests to the \as{gc} affected planes. Therefore, TTFlash implements \textit{rotating \as{gc}} that only runs at most one \as{gc} operation on a \textit{plane group}. The plane group assignment is based on the possible parallelism of the device, such as plane- and channel-level parallelism. This ensures that a plane group is never blocked for more than a single \as{gc} operation, implying that any request will not be blocked for more than one \as{gc} operation. 

On embedded devices, and in particular mobile devices, in order to save energy the device suspends all threads when not needed (e.g., when the mobile screen is turned off). This implies that all file system threads are also suspended, which means the file system cannot run the background \as{gc} during these inactive sessions. Therefore, \af{sac}~\cite{2015-Park-Suspend_Aware_cleaning-F2FS} is an addition to \as{lfs}, which is the time between the suspend initiation and the suspending of the file system threads, also referred to as the \textit{slack time}. It uses this slack time to run background \as{gc}, however it does not write any data on the flash storage, but instead selects a victim block and brings the still valid pages in the page cache and marks these as dirty. As a result, all pages in the victim block to be dirty, which allows it to be erased. This process is referred to as \textit{virtual segment cleaning}, which however is not run every time the screen is turned off, but rather based on the device utilization, which is called \textit{utilization based segment cleaning}. 

\subsection{Coordinating the Software Stack Layers}\label{sec:software_stack}
% Figure environment removed

The various layers in the storage software stack introduces several redundant operations, such as \as{gc} that is run in the \as{lfs} and \as{gc} run on the storage device. This duplicate work leads to significant performance impacts~\cite{2014-yang-dont_stack_log_on_log}, requiring a co-design of the storage device software and the file system. Qiu et al.~\cite{2013-Qiu-Codesign_FTL_FS} show that the co-design of \as{ftl} and the file system show benefits of reduced memory requirements for \as{l2p} mappings, and increased device parallelism that can be exploited with better file system knowledge of the storage characteristics. In addition to duplicated work, information about data characteristics are lost across the various, since in order to integrate communication across the layers, each utilizes an interface for the other layers to communicate with. However, the expressiveness of the interfaces limits the capability to communicate data characteristics across the layers, as \cref{fig:software_stack} illustrates. Applications have the highest knowledge of data characteristics, how it is best allocated on the flash and managed in order to reduce \as{gc}, however cannot forward all of this information to the file system. Similarly, the file system may group specific data together, however cannot forward this information to the block layer, and similarly the \as{ftl} may take the submitted \as{io} requests and organize these different on the flash storage. This \textit{semantic gap} between the storage device and storage software is a result of the device integrating into the existing block \as{io} interface~\cite{zhang2017flashkv}, however failing to represent the flash-specific characteristics. Such mismatch between storage device and its accessing interface, requires storage software to enhance capabilities to pass information across the layers to avoid increasing the semantic gap across the layers.

DualFS~\cite{2020-Wu-DualFS} utilizes the custom integration with \as{ocssd} to merge the garbage collection of the file system with that of the \as{ftl}, and present this scheme as \textit{global garbage collection}. ParaFS~\cite{2016-Zhang-ParaFS} implements a similar coordination of file system \as{gc} and \as{ftl} \as{gc}. A different approach taken by Lee at al.~\cite{2016-Lee-AMF} is to modify the block interface with the flash characteristics, moving responsibility directly to the file system, or other application built on top of it. The resulting interface called \af{amf} exposes a block interface that does not allow overwriting unless an explicit erase is issued for the blocks. This matches the flash requirement that prior to overwriting data it has to be erased. The interface is implemented as a custom \as{ftl}, called AFTL, on top of which the ALFS file system is built. This avoids the duplicate garbage collection of the \as{ftl} and the file system, as the garbage collection of the ALFS erases blocks during garbage collection, informing the \as{ftl} to erase the physical block.

Co-designing the FTL and the file system allows removing uncoordinated duplicate work, and coordinate the flash management. To this end, Lee et al.~\cite{lee2014refactored} present a redesigned \as{io} architecture, called REDO, which avoids the duplicate operations from file system and \as{ftl} by implemented the new framework directly as the storage controller, and building the \af{rfs} on top of the new controller interface. By combining the file system operations with the storage controller, the file system is responsible for running \as{gc} and managing the storage by maintaining the \as{l2p} mappings.

