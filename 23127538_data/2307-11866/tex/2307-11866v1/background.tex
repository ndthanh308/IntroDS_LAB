\section{Background}\label{sec:background}
Flash storage has become ubiquitous with its increased performance capabilities over conventional storage, such as \as{hdd}, and with its decreasing cost it is increasing in adoption~\cite{daim2008forecasting,2008-Lee-Flash_in_enterprise_db}. This background section explains the high level concepts of how flash storage is constructed (\cref{sec:bg_flash_storage}), how performant storage devices are built with flash storage (\cref{sec:bg_flash_ssds}), and how these devices are managed to be integrated into host systems (\cref{sec:bg_ssd_internals,sec:flash_integration_separation,sec:ssd_host_interfaces}).

\subsection{Flash Storage}\label{sec:bg_flash_storage}
Flash storage is based on flash cells providing the persistent storage capabilities through programming of the cell and erasing to clear its content. These are constructed with a floating gate, which can hold an electrical charge, and a control gate, inside a complementary metal-oxide semiconductor (CMOS) transistor~\cite{parat2015floating}. \cref{fig:flash_cell} shows an overview of the organization of the floating and control gate in a flash cell. Oxide layers are used around the floating gate to provide insulation, resulting in trapping of electrons in the floating gate. Charging the floating gate with an electrical charge results in it dampening the electric field of the control gate, which requires a larger voltage to be applied to the control gate in order to become conductive~\cite{pavan1997flash}. Therefore, if the control gate does not become conductive at a certain voltage threshold, it means the floating gate is charged, which represents a logical 0. When the floating gate is not charged, the control gate becomes conductive at a lower voltage, indicating a logical 1. The current in the flash cell flows between the source and drain terminals. A single flash cell can therefore represent a single bit, with a logical 0 or 1. The current flowing between the terminals is managed by programming and erasing the floating gate. The charge of the floating gate after an erase operation is equivalent to a logical 1, and programming the flash cell can modify the charge to logical 0~\cite{jagmohan2010write}. A subsequent erase operation brings the charge back to a logical 1. 

% Figure environment removed

Flash that stores a single bit value in a flash cell is called \af{slc}. The storage capacity of a flash cell can be increased by increasing the number of layers, by modifying the variability in the charging state of the floating. In order for a floating gate to represent two bits, its voltage must be able to represent 4 distinct states (as it is two bits $2^2=4$), based on which its logical value can be interpreted. Therefore, the voltage threshold is divided into regions to represent the values, such as no charge, low charge, medium charge, and a high charge. Available flash cell levels are \af{mlc}, \af{tlc}, \af{qlc}, and \af{plc}~\cite{2020-Ma-Flash_Memory_Bit_Errors}, which respectively increase the number of bits stored per flash cell by one. However, increasing number of cell levels results in slower access time~\cite{arpaci2018operating}, lower flash cell lifetime~\cite{mohan2010learned}, and introduced noise between cells due to the voltage differences, which in turn leads to increased bit errors~\cite{2015TaranalliFlashError,2020-Ma-Flash_Memory_Bit_Errors}. As the flash cell is programmed over time, the oxide layers erode, causing the flash cell to no longer be usable. For \as{slc} this value is typically 100K and decreases to 10K or less for \as{mlc}~\cite{2016-Schroeder-Flash_Reliability}.

There are various ways of packing flash cells in full storage devices. Firstly, organization of flash cells representing NOR gates. In NOR flash each individual flash cell is connected directly to the source line and the bit line, responsible for providing the connection to the source terminal of the cell and the drain terminal, respectively. Over the source terminal the charge for programming of reading the flash cell is applied, where the source drain of the flash cells is connected to the source line. The bit line is connected to the drain terminal in order to read the voltage that is applied from the source terminal. The connection to the source and bit line for each flash cell in NOR flash allows it to be byte addressable, however incurs a high erase latency, requires more space, and is therefore also more costly. Secondly, organization that depicts NAND gates, where several (typically 8) flash cells are connected in series from the source line to the bit line. This allows for a significantly higher density of flash cells to be packed, however makes the flash no longer byte addressable, but instead needs to be written and erased in a larger unit, referred to as a \textit{flash page}, which are typically 16KiB in modern flash storage. As NOR flash provides byte addressability and lower access latency, it is suitable for small persistent storage such as for system BIOS~\cite{2012-Lai-NOR_BIOS}, however for mass data storage NOR flash becomes costly compared to NAND flash. In addition to a higher cost, NOR flash has significantly higher erase latency than NAND flash~\cite{toshiba2006vs}, making it undesirable for large data storage.

NAND can also be stacked vertically in what is called 3-D NAND~\cite{micheloni2017array}, for a higher density, however such specifics are out of the scope of this survey. For this survey only a basic understanding of flash cells is required, as was explained. For more detailed information on functionality of flash cells, reports such as~\cite{bez2003introduction} and~\cite{compagnoni2017reviewing} can be consulted. We only focus on NAND based flash storage due to its increased benefits and adoption for large scale data storage systems, on which the file systems we evaluate are built.

\subsection{Flash SSDs}\label{sec:bg_flash_ssds}
Flash cells are grouped into \textit{pages}, depicting the unit of access (typically 16KiB). A flash page additional has a small extra space, called the \af{oob} area, which is utilized for \af{ecc} and small metadata. As the operation for erasing of a flash page is costly, multiple pages are grouped into a \textit{block} (typically several 256MiB), which is the unit of erase. Increasing the erase unit from individual pages to a block helps amortize the erase overheads. Pages within a block have to be written sequentially and prior to a page being overwritten, the entire block needs to be erased. A number of blocks are then grouped into a \textit{plane}, of which multiple planes are combined into a \textit{die}~\footnote{\textit{Dies} are also referred to as \textit{chips}, we use the terms interchangeably and mean the same entity.}. Lastly, numerous dies are combined into a single flash package. \cref{fig:ssd_architecture} shows a visual representation of a flash-based SSD architecture. The flash storage is organized a number of \textit{flash packages}.

\input{figs/ssd_architecture.tex}

In order to build fast storage devices, multiple flash packages are packed together into a \as{ssd}. While an \as{ssd} can be constructed with any solid state technology such as Optane~\cite{2019-Wu-Unwritten_Contract_Optane,2020-Yang-Optane_3DXpoint}, we focus purely on NAND flash based SSDs. The SSD contains a controller, which is responsible for processing requests, managing data buffers, and contains the NAND flash controller that manages the flash packages. An additional \af{ram} buffer, most commonly in the form of \af{dram}, is present on the device for maintaining address mappings (discussed in \cref{sec:bg_ssd_internals}). Lastly, an \as{ssd} contain the host interface that provides the means to connect to the storage to the host system over connection interfaces such as \af{sata} and \af{pcie}, and defining standards such as \af{ahci} and \af{nvme}~\cite{landsman2013ahci} (discussed in \cref{sec:ssd_host_interfaces}). Depending on the chosen connection protocol for the storage media, throughput and achievable performance is limited by it, where the newest \as{pcie} protocol achieves the highest throughput and lower latency compared to SATA~\cite{2013-Eshghi-SSD}.

However, based on a study on a Samsung \as{ssd}, the bandwidth of NAND flash \af{io} buses, connecting the flash to the flash controller, is limited to 32MB/s because of physical restrictions, and 40MB/s with interleaving in dies~\cite{2008-Agrawal-Design-Tradeoff-SSD}. A write operation to flash storage firstly writes the data to a data register, from which the data in the register is then programmed into the flash page. Because the programming operation takes longer than loading of data, these operations can be interleaved, such that while a page is being programmed, data of another write operation is loaded~\cite{kim2011parameter}. This avoids stalling whenever a page programming finishes and data needs to be loaded again. Interleaving is similar to the concept of CPU pipelining~\cite{tanenbaum2016structured-pipelinining}. In order to increase performance, parallelism is utilized on the different flash entities in the \as{ssd}~\cite{chen2011essential}. Depending on the hardware configurations, the different types of parallelism are referred to as \textit{channel-level parallelism}, where requests are distributed individually across the flash channels, \textit{package-level parallelism} where flash packages on the same channel are accessed in parallel, \textit{chip-level parallelism} where flash packages with multiple dies access the dies in parallel, and \textit{plane-level parallelism} where the same operation can be run on multiple planes, in the same die, in parallel. 

There are several additional performance optimizations for accessing flash storage. Other than accessing a flash entity in parallel and interleaving, flash packages can be accessed synchronously with a single request queue through \textit{ganging}~\cite{2008-Agrawal-Design-Tradeoff-SSD}. Flash packages within a gang share the same control bus from the flash controller, however can utilize different data buses, thus allowing a single command request queue with multiple data buses to provide the resulting data. Therefore, requests that require data from multiple pages can be split among a gang from a single request queue, and provide the data on different buses, avoiding the bottleneck of a single data bus. With the increased parallelism on a flash \as{ssd}, the process of accessing multiple blocks simultaneously across different chips and planes is referred to as \textit{clustered blocks}~\cite{kim2011parameter}. Accessing flash storage in units of clustered blocks allows exploiting a higher level of the flash performance.

\subsection{SSD Internals}\label{sec:bg_ssd_internals}
With the complexity of flash-based storage devices, an \as{ssd} ships with firmware, containing the \as{ftl}, that hides the flash management idiosyncrasies from the host system, exposing a conventional sector/page addressable storage device (just as a \as{hdd}). The \as{ftl} is responsible for aligning \as{io}s to the device constraints, such that pages are written sequentially and a block is erased prior to a page being overwritten. For achieving this, the \as{ftl} maintains a mapping of pages and their state, such that it can identify which pages in a block are valid and which have been overwritten. Different implementations of an \as{ftl} can utilize different mapping levels, such as block mappings or custom units. For a detailed explanation of FTL mapping algorithms consult \cite{chung2009FTL_survey} and \cite{2009-Gupta-DFTL}.

A naive FTL design is to maintain a fully associative mapping of each \af{lba} to \af{pba}~\cite{2009-Gupta-DFTL}, referred to as \af{l2p} mapping, and inversely \af{p2l} mapping. These mappings are maintained in a \textit{mapping table}, which is kept in the \as{ram} of the flash device for faster accesses. For consistency the mapping table is also maintained in the persistent flash storage, where on startup time of the devices the mapping table is reconstructed in the device \as{ram}~\cite{chen2009understanding,2008-Agrawal-Design-Tradeoff-SSD}. With this, the SSD can provide seemingly in-place updates by simply writing data to a free page and invalidating the overwritten data. Over time this generates an increasing number of invalid pages within blocks, requiring to run \af{gc} to read out still valid pages in the block and move these pages to a free block on the SSD, followed by erasing the original block. An additional requirement for performing \as{gc} is the need for extra space, such that the \as{ftl} can move valid pages to a free space before erasing the block. This extra space is called the \af{ops}, and typically ranges between 10-28\% of the device capacity. Due to the limited life of flash cells~\cite{mohan2010learned,2016-Schroeder-Flash_Reliability}, the FTL has to additionally ensure even wear across the device, such that no particular region of the flash storage is burnt out quicker than other regions. This process is referred to as \af{wl}. While SSDs are not the only method for utilizing flash storage, embedded devices, such as mobile and IoT devices, directly attached flash chips on the motherboard. This avoids the internal management from the \as{ftl} (discussed in detail in \cref{sec:bg_ssd_internals}) which is required with SSDs, allowing embedded devices to have direct control over the flash storage.

\subsection{Additional Flash Integrations}\label{sec:flash_integration_separation}
% Figure environment removed

\as{ssd} is not the only integration of flash storage into full storage devices. \cref{fig:flash_integrations} shows three integration levels for flash, where \cref{fig:flash_integration_ssd} depicts the conventional integration with an \as{ssd}. \cref{fig:flash_integration_custom} shows a custom integration of flash storage, under which classification several types of devices fall. Particularly in this literature study we classify custom flash integration as either \af{ocssd}~\cite{2019-Lu-Sync_IO_OCSSD,bjorling2017lightnvm}, devices with custom firmware or \as{ftl}, and multi-stream SSD~\cite{bhimani2017enhancing,kang2014multi}. The main benefit of such a setup is that the flash characteristics are no longer hidden behind the device, giving the host an increasing level of possible management for the flash. \as{ocssd} is a type of \as{ssd} that exposes the device geometry to the host, allowing the host to manage device parallelism and allocation. Similarly, with custom firmware and \as{ftl} it is commonly the case that the device exposes more information and control of the underlying flash storage to the host. While such a device allows increased data management for the host software, it comes at increased complexity for managing the device constraints. Lastly, \cref{fig:flash_integration_embedded} shows flash integration at the embedded level, such as is commonly used in mobile devices and \af{iot} devices. In embedded flash configuration the flash chip is commonly directly attached to the motherboard, giving the host system full control over the underlying flash storage. Throughout this literature study we group file system design and mechanisms based on these three integration levels, as different levels of integration allow different degrees of flash management and ranging possibility for flash integration.

\subsection{Host Interfaces and Storage Protocols}\label{sec:ssd_host_interfaces}
Storage systems were largely relying on slow storage media, such as \as{hdd}, for which many of the legacy protocols and interfaces such as \af{scsi}, \af{sas}, \af{ata}, \af{sata}, and \af{ahci} were design for. However, the arrival of faster storage devices, including flash based storage, made the legacy protocols the bottleneck for performance~\cite{walker2012comparison}. \as{scsi} provides the standards for the physical connection of host systems to peripheral components, such as external storage including disk and tape, allowing to connect multiple devices at the same time. \as{sas} allows to increase the number of attached devices, compared to SCSI, up to 65535. \as{ata} is the interface for connecting the host system bus to the attached storage device. \as{sata} is the successor of ATA, which provides higher transfer speeds for attached devices. \as{ahci} is the standard defining the interface to communicate with \as{sata} attached devices. 

In order to integrate storage devices with \as{ahci} and \as{sata}, such that the host can communicate to the device, a \af{hba} or integrated chip on the storage device is required. These serve the purpose to provide the translation between the host bus and the storage subsystem interconnect~\cite{walker2012comparison}. As an \as{ssd} provides significantly higher speeds, and the storage interconnect becomes the bottleneck, efforts led to the development of the \af{pcie} interconnect, an extension of on \af{pci}, specifically designed to provide lower latency and increased throughput compared to \as{sata}. However, \as{ahci} storage controller connected over the \as{pcie} interconnect is still a bottleneck for the entire system with the required interconnect translation. Therefore, \af{nvme} was designed, specifically for fast SSD devices. It allows to directly connect storage devices over \as{pcie}, without the need for a \as{hba}.

An additional benefit of \as{nvme} is that the storage device can be partitioned into multiple logical extents, called namespaces, of which each can have its own \as{io} channels, allowing to exploit the internal device parallelism~\cite{walker2012comparison}. Contrary to \as{sata}, which had a single request queue in which \as{io} requests is inserted and sequentialized, \as{nvme} utilizes a \af{sq} and a \af{cq}. Requests are placed in the SQ, which allows up to 64K outstanding requests and up to 64K different \as{io} queues, and upon completion of a request the \as{nvme} controller places the result in the \as{cq} and writes to a doorbell register (called the completion queue head doorbell) to notify the host of completion for the request. Additionally, on submission of a command by the host the submission queue tail doorbell is written to announce a new request. The increased parallelism of \as{nvme} allows to better exploit \as{ssd} performance, and removes the single point of failure and contention in storage systems~\cite{xu2015performance}. Performance comparison of \as{sata} and \as{nvme} show that \as{nvme} \as{ssd} can achieve 8x higher performance than \as{sata} \as{ssd}~\cite{xu2015performance}.
