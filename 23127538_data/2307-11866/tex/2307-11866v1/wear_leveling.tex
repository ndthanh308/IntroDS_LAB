\section{Asymmetric Read and Write Performance}\label{sec:ssd_rw_asym}
Given that read and write performance on flash storage is asymmetric~\cite{stoica2009evaluating,2022-intel-p,chen2009understanding}, this section provides the various mechanisms to handle the asymmetric performance. \cref{tab:rw_asym} shows the various methods for dealing with asymmetric performance, and how to increase file system performance. A particularly important mechanism for improving lower write performance of flash is to utilize write-optimized data structures, such as log-based data structures, where write operations are always written to the end of the log. Further mechanisms for improving write performance involve efficient methods of data caching, and effective organization of data on the storage device.

\begin{table}[!t]
    \centering
    % Update reference format to fit table into column
    \crefformat{section}{\S#2#1#3}
    \crefformat{subsection}{\S#2#1#3}
    \crefformat{subsubsection}{\S#2#1#3}
    \begin{tabular}{||p{40mm}|p{35mm}||}
        \hline 
        Mechanism & File Systems \\
        \hline
        \hline
        Write Optimized Data Structures (\cref{sec:wods}) & \cite{2020-Tu-URFS,hunter2008brief,rodeh2013btrfs,2022-Jiao-BetrFS,dubeyko2019ssdfs} \\
        \hline
        Write Buffering (\cref{sec:write_buffering}) & \cite{park2006cflru,jo2006fab,park2013enffis,2018-Kannan-DevFS,josephson2011direct,2019-Lu-Sync_IO_OCSSD,2010-Josephson-DFS,2011-Park-Multi_NAND} \\
        \hline
        Deduplication (\cref{sec:deduplication}) & \cite{huang2013improve,dubeyko2019ssdfs,2011-Lim-DeFFS} \\
        \hline
        Compression (\cref{sec:compression}) & \cite{woodhouse2001jffs,dubeyko2019ssdfs,2021-Ji-F2FS_compression,2007-Hyun-LeCramFS,ning2011design} \\
        \hline
        Delta-Encoding (\cref{sec:delta_encoding}) & \cite{huang2013improve,dubeyko2019ssdfs} \\
        \hline
        Virtualization (\cref{sec:virtualization}) & \cite{2009-Zuck-NANDFS,2022-Lee-F2FS_Address_Remapping} \\
        \hline
        Flash Dual Mode Switching (\cref{sec:flash_dual_mode}) & \cite{2020-Wu-DualFS,2009-Sungjin-FlexFS} \\
        \hline
    \end{tabular}
    % Reset reference format to before
    \crefformat{section}{Section #2#1#3}
    \crefformat{subsection}{Section #2#1#3}
    \crefformat{subsubsection}{Section #2#1#3}
    \caption{Mechanisms for file systems to deal with \textbf{\as{fic}1}, asymmetric read and write performance of flash storage, and the respective file systems that implement a particular mechanism.}
    \label{tab:rw_asym}
\end{table}

\subsection{Write Optimized Data Structures}\label{sec:wods}
The characteristic of flash having lower write than read performance requires that data structures for flash are write optimized. Such data structures which are optimized for write operations are referred to as \af{wods}~\cite{bender2015introduction}. With the addition of the missing support for in-place updates on flash, the best suiting data structure for flash-based file systems is a log-based structure. As a result, all file systems discussed in this section are \as{lfs}. The nature of a \as{lfs} being append only writes accounts for the lower write performance on flash, which matches the write updates to the operations more optimal for flash, which are smaller fine-fine grained updates~\cite{2013-Lu-Flash_Lifetime_Reduce_With_WA} in a log structured fashion~\cite{2015-Changman-f2fs}. 

However, while the log provides increased write performance, metadata is scattered throughout the log, requiring a full scan of the log to locate metadata. Therefore, file systems commonly employ tree-based data structures for metadata, decreasing the worst case time complexity from $\mathcal{O}(n)$ to $\mathcal{O}(\log{}n)$. B-tree is a commonly used data structure for storage systems, including databases~\cite{comer1979ubiquitous,graefe2011modern} and file systems~\cite{rodeh2013btrfs}. Nodes in a B-tree can be larger than in conventional binary search trees, allowing the node to align to a unit of the underlying storage. B-trees furthermore are maximizing the breadth of the tree, instead of its height, in order to minimize the \as{io} requests in order to locate data, since each traversal to a child node requires an \as{io} request. The tree itself is sorted and self-balancing, making the worst case complexity for search relative to the tree height, however also requiring to balance the tree. The B+tree further reduce required \as{io} by only having data in the leaf nodes, such that higher nodes only contain keys, increasing the number of keys that fit inside a block. Leaf nodes are linked in a linked list, for faster node traversal, which in turn speeds up searching. 

In order to write optimize these trees, $\text{B}^\varepsilon$-trees~\cite{bender2015introduction,brodal2003lower} adapt the node structure of the B-tree to include a buffer to which updates to its children nodes are written. As node updates are initially written in memory, this allows to gather a larger number of small writes, encode these in the added buffer of the respective nodes, and write these as a larger unit, avoiding frequent small updates to nodes. The most recent version of BetrFS~\cite{2022-Jiao-BetrFS} (published in 2022) implements such a $\text{B}^\varepsilon$ as the metadata storage for indexing data. It is implemented in the Linux Kernel as a key-value store, based on TokuDB~\cite{tokudb}, which exposes a key-value addressable interface. The benefit of this $\text{B}^\varepsilon$-tree is that the nodes have a larger (2-4MiB) sequentially written log, batching updates into larger units and thus avoiding small updates. Initially all updates go into the root node message log, which when full gets flushed to its child nodes. While \as{wods} provide optimized write performance by batching updates, read requests require reading an entire node (2-4MiB), causing small read requests to suffer from significant read amplification.

SSDFS~\cite{dubeyko2019ssdfs} adapts the tree design to utilize hybrid nodes, since the node allocation uses blocks, which are several KB in size, and may not directly be filled directly. Therefore, hybrid nodes in the tree adapt the size of the node, such that if a hybrid node is allocated and filled, it allocates an additional hybrid node, which upon being filled is merged with the first hybrid node and becomes a leaf node. This allows to reduce write amplification (solving \textbf{\as{fic}3}) if a node is not filled enough. An additional optimization to B-trees is the \af{piobt}~\cite{2011-Roh-Bp_tree_optimizations}, which implements a parallel flash-optimized B-Tree variant (solving \textbf{\as{fic}4}), utilizing a larger \as{io} granularity to exploit package-level parallelism, maintain a high number of outstanding \as{io} requests to utilize the channel-level parallelism, and avoid mixed read and write operations.

\subsection{Write Buffering}\label{sec:write_buffering}
In addition to \as{wods} optimizing write operations file systems need to also avoid small writes, referred to as \textit{microwrites}. Particularly, as file systems write in units of blocks, which commonly are 4KiB, small writes require a full unit to be filled. The majority of file systems provide the possibility for inline data in inodes, such that the inode data, which is written regardless, has a small capacity to include data. However, this still requires that for small files inodes are directly written to flash. Therefore, several schemes that involve buffering and caching of data in memory, before flushing to the flash storage, provide increased write performance, and additionally increase overall performance.

While buffering of \as{io} prior to flushing to flash storage provides performance gains, buffers are often limited in size and are much smaller than the persistent storage. Furthermore, in addition to buffering write requests for new data, accessing or updating existing data is also cached in the buffer. Therefore, the caches utilize effective methods that minimize the cache misses by optimizing the eviction policy. \af{lru}~\cite{tanenbaum2016structured-LRU} is a common caching policy that maintains the items in the cache in the order of usage, where the least recently used item is selected for eviction. This mechanism is extended by DFS~\cite{2010-Josephson-DFS,josephson2011direct}, which utilizes \textit{lazy \as{lru}}, which does not insert accessed data into the buffer upon a cache miss, but rather inserts the data into the buffer only on an additional cache. Requiring of two cache misses implies that the cache only contains data that is frequently accessed, instead of caching all data from a cache miss.

\af{cflru}~\cite{park2006cflru} is another extension on the \as{lru} algorithm, which splits the buffer into two segments, one as the working region in which recently accessed pages reside, which are managed in \af{mru} fashion, therefore depicting the frequently accessed pages. The second region, called the clean-first region contains the less commonly accessed pages in \as{lru} fashion. On eviction (e.g., when writing a new page and freeing space in the buffer), it first attempts to evict a clean page, rather than a dirty page from the clean-first region, as this does not require a flush of the dirty data to the flash storage, and only resorts to evicting dirty pages as last resort. \af{fab}~\cite{jo2006fab} optimizes the caching policy to align with the flash characteristics by organizing pages in the cache in a larger unit, called \textit{block}, and upon eviction flushes entire blocks to the flash storage, issuing larger \as{io}s and aligning better to the flash erase unit. 

Similarly, NAFS~\cite{2011-Park-Multi_NAND} implements a \textit{double list cache}, containing a clean list with only clean pages for caching of data, and a second dirty list for writing of modified pages and to prefetch of pages based on the access patters, only containing dirty pages. The benefit of two separate lists is that firstly it allows caching write operations and avoid small writes to the flash device, and secondly it prefetches data into the clean list in order to minimize cache misses. EnFFiS~\cite{park2013enffis} presents the \textit{dirty-last cache} policy, which considers the flash characteristics by utilizing a \textit{delay region} and a \textit{replacement region}, which correspond to a multiple of the flash memory blocks. By using a multiple of the flash blocks, the data written is sequentialized and written to the flash as a larger unit, where dirty pages are initially moved from the replacement region to the delay region, before being flushed to the flash. This buffering in the delay region allows collecting more dirty pages, improving performance and avoiding smaller writes to flash. 

StageFS~\cite{2019-Lu-Sync_IO_OCSSD} likewise utilizes two stages, where write operations are initially written into the first stage, called the \textit{file system staging area}. The issued writes to the staging area are completed in a \textit{persistence-efficient} way, which utilizes a log to account for asymmetric flash performance for the staging area. Subsequently, writes are then regrouped, based on the file system structure and hot/cold identification, and are written to the second stage, based on the group assignment. The staging area allows writing synchronous \as{io} directly to the staging log with optimal flash write characteristics, lowering completion latency, followed by better grouping of data when writing from the staging to the second stage file system area.

In an effort to limit the required memory for data caching, DevFS~\cite{2018-Kannan-DevFS} proposes the \textit{reverse caching} mechanism for effectively managing host memory and device memory. Reverse caching aims at keeping only active files in the device memory, which is limited in size, and upon closing of a file migrates the metadata to the host memory. Reopening a file migrates it back to the device memory, and consistency of metadata is not violated as any actively modified metadata is in the device memory, and only inactive metadata is in the host memory. To efficiently utilize reverse caching, DevFS uses a host-memory cache that is able to use \af{dma} to move metadata between itself and the device memory, additionally minimizing host overheads.

\subsection{Reducing Write Traffic}\label{sec:reduce_write_traffic}
In order to avoid the slower write performance of flash storage another mechanisms aims at minimizing the amount of data that is written to the storage device. Whenever data is updated on the flash storage, updating all metadata and writing the new data incurs a significant amount of write traffic, in addition to causing \as{wa}. Therefore, reducing the amount of data being written through mechanisms such as deduplication, compression, and delta-encoding helps at avoiding the increased \as{wa}.

\subsubsection{Deduplication}\label{sec:deduplication}
File systems commonly contain duplicate data, from backups or archival copies made and general work. Therefore, file systems often aim to avoid creating duplicates of existing data, which is referred to as \textit{deduplication}. Evaluations show that in high-performance computing centers on average about 20-30\%, with peaks of 70\%, of the stored data can be removed through deduplication~\cite{meister2012study}. Deduplication avoids writing the same data multiple times, which in turn helps reduce the write traffic and minimizes write and space amplification (solving \textbf{\as{fic}2}). Effective deduplication relies on hash functions, which provide a deterministic output, called the \textit{digest} or commonly referred to as the \textit{fingerprint} of the data, based on which duplicates can be identified. The utilized hash functions for deduplication are \textit{one way hash functions}~\cite{merkle1989one}, which calculate a fingerprint of the data, or digest, but given the digest or fingerprint, the data cannot be generated. Given data from a file for instance, hashing the data provides a digest, and if the same data is to be written again, the same digest is generated. By identifying if a digest already exists in the file system, can it identify if the data is being duplicated, and avoid writing the duplicate by making the metadata of the newly written data point to the existing data. 

In order to apply deduplication on file system data, two methods exits. \textbf{(1)} Deduplication with fixed-sized chunks, where hashes are generated based on the entire file or a pre-determined chunk size, and \textbf{(2)} variable-sized chunks where the chunk size that is hashed depends on the file and content. The effectiveness of deduplication with fixed-sized chunks is limited and highly dependent on the chunk size and modification sequences in the chunks~\cite{2011-Lim-DeFFS}, as for example a small change in a large file, where the chunk size is the entire file, results in a completely different hash, even if much of the data is duplicated. While variable-sized chunks reduces the configuration dependability it is significantly more complex to implement. DeFFS~\cite{2011-Lim-DeFFS} implements a duplicate elimination algorithm that reduces the complexity of identifying duplicates. As comparing duplicates of every byte is not possible, the algorithm finds the smallest modified region in a file adapting the chunk size, which is initially assigned the default value that corresponds to the flash page size.

The idea of eliminating duplication through fingerprints is not new for \as{ssd}, as existing \as{ftl} implementations implement such mechanism. \af{caftl}~\cite{chen2011caftl} generates collision-free fingerprints of write requests and maintains fingerprints of all flash resident data in order to avoid writing the same data twice. Flash Saver~\cite{li2012flash} is a similar architecture running between the SSD and the file system, which manages the file system \as{io} requests and ensures deduplication using SHA-1 fingerprints~\cite{wang2005finding}. A plethora of similar hash-based deduplication mechanisms exist~\cite{fu2011aa,ha2013deduplication,kim2012deduplication,xia2014combining}, which may not be particular to file systems, but can be adopted by a wide range of storage systems. File systems with deduplication are CSA-FS~\cite{huang2013improve}, which implements deduplication by calculating the MD5 digest~\cite{rivest1992md5} (MD5 is a hash function applied to the input) and looks up the result in a hash table which provides the corresponding LBA of the block. If it already exists and the user requested a new file with it, the new inode simply uses the given LBA from the hash table, instead of duplicating the data again. Similarly, SSDFS~\cite{dubeyko2019ssdfs} maintains fingerprints with its metadata B-Tree, and DeFFS~\cite{2011-Lim-DeFFS} stores hash keys with all inodes for their data to avoid duplicates. 

\subsubsection{Compression}\label{sec:compression}
An effective method for reducing the required storage space for data is to utilize \textit{compression}. While there exist lossy compression algorithms~\cite{iverson2012fast,kavitha2016survey}, which discard parts of the data, and lossless compression algorithms~\cite{kavitha2016survey,sharma2010compression}, which maintain all data, lossy compression is very application dependent, and in the case of file systems we assume a lossless data storage and therefore focus on lossless compression algorithms. Lossless compression methods rely on three methods. \textbf{(1)} \af{rle} which minimizes size by taking the repeating characters, referred to as the run, and replacing them with a 2 byte sequence of the number of repetitions of the character, called the run count, followed by the replaced character. For example the sequence of ``aaabbccc`` is represented as ``a3b2c3`` with \as{rle}. \textbf{(2)} \af{lz}~\cite{ziv1977universal,ziv1978compression} which utilizes a dictionary to replace strings with their dictionary value. Variations of the \as{lz} algorithm exist, such as LZ77~\cite{ziv1977universal}, LZ78~\cite{ziv1978compression}, and LZW~\cite{welch1984technique}. The dictionary is constructed at the compression time and used a decompression time~\cite{kapoor2013review}. A well-known compression algorithm GZIP~\cite{deutsch1996gzip} is based on two \as{lz} algorithms. \textbf{(3)} Huffman Coding~\cite{huffman1952method} which creates a full binary tree based the frequency of occurring characters and generating code of the character and the frequency. 

JFFS~\cite{woodhouse2001jffs} and several other file systems~\cite{woodhouse2001jffs,dubeyko2019ssdfs,2021-Ji-F2FS_compression,2007-Hyun-LeCramFS,goyal2005energy,cramfs,ning2011design} provide a data compression, however metadata compression has shown to cause decompression overheads~\cite{kang2022pr}. Applying compression at the file system level allows reducing the utilized storage space, especially on flash this reduces the space and write amplification factors (solving \textbf{\as{fic}3}), and in turn prolonging the flash lifetime as shown by Li et al.~\cite{2015-Li-SSD_lifetime_compression} on the benefits of different compression algorithms for NAND flash lifetime. SSDFS~\cite{dubeyko2019ssdfs} utilizes LZO compression~\cite{oberhumer2008lzo}, a library for data compression with LZ algorithms, for data and metadata. Given that different data benefits from different compression mechanisms, where for instance data is less frequently read can be compressed more efficiently than frequently read data, which may require more simplistic decompression to minimize overheads, or embed metadata in the compressed data to avoid decrypting data and metadata separately. Adaptive compression selection is employed in an extension of F2FS~\cite{2021-Ji-F2FS_compression}, where \af{fpc} selects the compression method for files based on how compressible they are.

% Figure environment removed

An issue of compression is that it must be aligned to the flash unit, in order to avoid unnecessary read amplification and required data copying. \cref{fig:pba_decompression} shows that if the compressed data is not aligned to the flash unit of a page, such that compressed data can span across pages, the decompression process becomes more complex. This is due to first having to read all the associated flash pages into the device memory. However, if compressed data can span across flash pages, this implies that non-associated data can be included. Therefore, only the associated data must be copied to a \textit{decompression buffer} (step 2a), from which the data can be decompressed (step 3). LeCramFS~\cite{2007-Hyun-LeCramFS} (short for less crammed FS) implements a read-only file system with compression for embedded devices, that avoids this additional copy of data by using \as{pbal}. Any compressed data will not span across page boundaries, implying that no additional pages with non-associated data are read, and the data can be decompressed directly (step 2). Therefore, it avoids the additional copy of data and reduces the read amplification (solving \textbf{\as{fic}}3). In order to avoid wasting space if the page is not fully utilized from a single compression, LeCramFS extends the compression implementation with a \textit{partial compression}, which splits the data into parts that fit into the available space. These parts are then compressed separately, allowing to utilize all available space.

\subsubsection{Delta-Encoding}\label{sec:delta_encoding}
Similar to deduplication, delta-encoding lowers the write traffic, further limiting the space and write amplification (solving \textbf{\as{fic}2}) by instead of writing out entire data when updated, with delta-encoding the new data is compared to the old data and only the differences, called the \textit{delta}, are written. This is especially beneficial in the case of small changes in large files, where it avoids writing the entire file again and writes only the changes in the data. While delta-encoding provides a similar space reduction to compression, particular file characteristics of what is being encoded can affect resulting performance. For instance, data sets that have a significant redundancy across them, such as email data sets, require significantly less space than being compressed~\cite{douglis2003application}. However, non-textual data, such as pdf documents, will have large deltas for even small changes, where compression is likely to be a better choice. Therefore, the application of delta-encoding depends on the data characteristics and what type of content is being encoded, in order to achieve better utilization of delta-encoding compared to simpler compression. CSA-FS~\cite{huang2013improve} implements delta encoding on its file system metadata (superblock, descriptor, bitmaps, and tables), allowing minor updates such as the access time of metadata and bitmaps to only write the new changes. Similarly, SSDFS~\cite{dubeyko2019ssdfs} uses delta-encoding for user data.

\subsubsection{Virtualization}\label{sec:virtualization}
As Butler Lampson mentioned in his famous quote from 1972 (which was originally stated by David Wheeler), ``Any problem in computer science can be solved with another level of indirection``~\cite{lampson1993principles}. By adding an additional layer on flash storage the write traffic can be reduced as a result of avoiding metadata update propagation (as with the wandering tree problem~\cite{bityutskiy2005jffs3}). Adding a virtual layer is conceptually identical to \af{lba} on top of \as{pba}. \cref{fig:virtualization} shows an example of how virtualization maintains the same \af{vba} when file data is updated, eliminating a need for metadata updates. Note, for simplicity we assume the virtual layer to be on top of the physical layer, providing \as{pba}s, however it can be directly on the physical layer of the flash storage, depending on the flash integration level, and would therefore be using \af{l2p} mappings. In the illustrated scenario, three blocks are mapped to \as{pba}0, \as{pba}1, and \as{pba}2, respectively. Assuming these are part of a file, metadata points to these respective virtual block addresses. If the first block of the file is modified, a new data block is written at \as{pba}3. With virtualization, the \as{vba} remains unchanged, only the \as{v2p} mapping table is modified to depict the new mapping of \as{pba}3. As a result, file metadata remains unchanged, avoiding write amplification for updating file metadata. 

% Figure environment removed

A similar mechanism is implemented in NANDFS~\cite{2009-Zuck-NANDFS}, using a \textit{sequencing layer} that implements the block allocation as immutable storage on the logical layer. Therefore, when file system data is updated, it cannot overwrite and simply marks the data as obsolete, and the new updated data is written in a newly allocated block. The sequencing layer implements the \as{l2p} mapping,  such that the \as{lba} of the new data is the same as the old \as{lba}, avoiding the updating of data addresses in the file system metadata. This implies that the file system does less writing by eliminating metadata updates, which in turn also reduces the required \as{gc}.

The concept of virtualization can similarly be applied through \af{ar}. With \as{ar} only the \as{l2p} mapping table of the storage device is modified, which similar to virtualization avoids rewriting of metadata. \as{ar} is an effective mechanism for solving additional overheads in file systems, such as file system garbage collection~\cite{kang2018address}, journaling~\cite{choi2009jftl,kang2018address,weiss2015anvil}, and data duplication~\cite{2021-Zhou-Remap_SSD}. Lee et al~\cite{2022-Lee-F2FS_Address_Remapping} propose \af{rmipu}, an address remapping scheme implemented in F2FS to solve issues of out-of-place updates. Instead of applying \as{ar} into a single file system operation, \as{rmipu} includes all write operations to utilize \as{ar}. All updated data is first stored in the log, followed by \as{ar} to update data pointers, thus avoiding metadata updates for file overwrite operations. File write operations to new files are appended to the log as usual, and the contiguity in \as{lba}s for files is maintained.

\subsection{Flash Dual Mode Switching}\label{sec:flash_dual_mode}
Some modern flash devices allow the host to switch the cell level of underlying flash blocks (e.g., switching \as{mlc} to \as{slc})~\cite{2020-Wu-DualFS,li2019accelerating}. This provides the benefit that a lower cell level, representing fewer bits, has a lower read, write, and erase latency, thus providing the flash with a higher performance than with a larger cell level configuration~\cite{2020-Wu-DualFS}. This switching between flash modes is referred to as \textit{flash dual mode}, which however comes at the cost of being able to store less data in the same amount of flash, as the block is only able to store half the data in the example of switching from \as{mlc} to \as{slc}. DualFS~\cite{2020-Wu-DualFS} utilizes the flash dual mode feature to provide a dynamically sized \as{slc} area, alongside the remaining \as{mlc} area, to accelerate the performance of critical \as{io} requests. By evaluating the \as{io} queue depth of \as{io} requests, DualFS determines the criticality of the incoming request, and maps it to the \as{slc} area for increased performance. It further profiles incoming request based on the hotness and allocates hot data into the \as{slc} mode for lower request latency.

FlexFS\cite{2009-Sungjin-FlexFS} implements a similar mechanism for increased performance on the \as{slc} area. The drawback of such a design is that data is required to be moved from the \as{slc} area to the \as{mlc} area as it has a lower write lifetime, and is aimed at the for critical \as{io} requests that require lower completion latency, which in turn causes \textit{data migration overheads}. To solve this, FlexFS implements several migration techniques that aim to hide the overheads for the data migration from the host. The first technique revolves around \textit{background migration}, which pushes the migration to happen when the system is idle. The second technique is \textit{dynamic allocation}, which writes non-critical requests to the \as{mlc} area, saving on flash degradation in the \as{slc} area, as well as avoiding future data migration. The dynamic allocator functions based on measurements of prior system idle times from \as{io} requests, to predict current idle time, and if sufficient idle time is predicted in order to complete data migration, the data is written to the \as{slc}, and otherwise part of the data, depending on required migration and idle time, is written to the \as{mlc} area. The last technique, \textit{locality-aware data management}, takes into account the hotness of data, and the dynamic allocator attempts to migrate only cold data from the \as{slc} area.
