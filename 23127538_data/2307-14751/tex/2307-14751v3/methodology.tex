%auto-ignore
\section{Methodology}
\label{sec:Methodology}
%In this section, we construct our adversary model and explain the proposed method, \fingerprint, to verify the ownership of DRL policies. 
\subsection{Adversary Model}
\label{ssec:adversarymodel}
The adversary \adversary's goal is to obtain an illegal copy of the victim agent's (\victim) policy $\pi_{\mathcal{V}}$ without being detected. \adversary has economic incentives and aims to illegally monetize stolen policy $\pi_{\mathcal{A}}$ using a surrogate DRL agent. $\pi_{\mathcal{V}}$ can be leaked by exploiting hardware/software vulnerabilities~\cite{yan2020cache} of different components within \victim. Furthermore, \adversary seeks to prevent traceback. Therefore, \adversary attempts to degrade the effectiveness of possible ownership verification methods by modifying $\pi_{\mathcal{A}}$, without incurring any substantial drop in $\pi_{\mathcal{A}}$'s return. 

\subsubsection{Adversary's capabilities.}\adversary has computational capabilities and access to the similar environment that $\pi_{\mathcal{V}}$ was trained on, but it cannot reproduce the same training episodes. %due to the innate randomness of the environment. 
One can argue that \adversary can also train its own policy, but we assume that it cannot obtain a policy as good as $\pi_{\mathcal{V}}$ due to nondeterminism (e.g., network architecture, difference in environment dynamics, DRL algorithm, hyperparameter selection, difference in computational resources, etc.). \adversary presumes that there might be an ownership verification mechanism, but does not know the exact algorithm. Based on this assumption, we also consider the existence of \emph{well-informed} adversaries\footnote{\newtext{ML literature commonly uses the term ``adaptive'' to refer to adversaries who are aware of deployed defenses. In security literature, it is customary to assume that \emph{all} adversaries are aware of the defenses, and the term ``adaptive'' is used for adversaries who are able to \emph{dynamically modify} their attack strategy based on what they learn about the defenses \emph{during} the attack. We use the term ``well-informed'' to refer to such adversaries so that our usage does not conflict with either ML or security literature.}} knowing that ownership verification is performed by fingerprinting and adversarial examples. If well-informed \adversary knows the complete procedure of the fingerprinting process, then it can forge its own fingerprints to create ambiguity in verification. However, this could be prevented with \fingerprint if $\pi_{\mathcal{V}}$ and the corresponding fingerprints are securely time-stamped and registered in a bulletin or provided to a trusted third party~\cite{szyller2021dawn}.


\subsubsection{Verifier's Capabilities.} A verifier (judge, \verifier) is a trusted third party independent of both \victim and \adversary. Given a suspected DRL agent $\mathcal{S} $ with policy $\pi_{\mathcal{S}}$ and fingerprints provided by \victim, the duty of \verifier is to determine whether $\pi_{\mathcal{S}}$ can be traced back to $\pi_{\mathcal{V}}$ and demonstrate the true ownership. \verifier has black-box access to $\mathcal{S}$, i.e., it does not know the algorithm and parameters of $\pi_{\mathcal{S}}$. \verifier can modify the environment without introducing any temporal latency or suspending the task. If the verification uses time stamps, it provides anteriority to \verifier to resolve any ambiguity. We also give \verifier computational capabilities to train and search for independent policies used for the same task if there is a need to validate that fingerprints are unique to the original model and do not transfer to independent models. We also define that a good fingerprinting mechanism should satisfy the following requirements: 
\begin{enumerate}[leftmargin=*]
\item \textbf{Effectiveness}: Successful ownership verification of stolen policies, \newtext{i.e., maximizing true positives.}
\item \textbf{Integrity}: Avoiding accidental accusations of independently trained policies, \newtext{i.e., minimizing false positives}.
\item \textbf{Robustness}: Withstanding model modification and evasion attacks. This is achieved if either the ownership of the modified policy is still successfully verified or the modification results in a substantial decrease in utility measured by the agent performance. 
%when successful verification still persists after the policy is modified or a failed verification along with a noticeable decrease in agent performance after the attack. 
\end{enumerate}
Fingerprinting algorithms do not necessarily aim for \emph{utility} (i.e., maintaining the quality of the suspected model on fingerprints), as they typically use adversarial examples during verification~\cite{lukas2021deep,peng2022fingerprinting} and the desired outcomes for fingerprints contain incorrect predictions. Therefore, we did not include utility as a requirement. However, we still restrict \fingerprint based on the utility concept, so that agents can still maintain their overall performance and complete the task without a significant performance degradation in episodes that include the verification phase. 

\subsection{Universal Adversarial Masks as Fingerprints}
\label{ssec:fingerprint-design}

\fingerprint aims to find a set of adversarial masks that can fool the original agent in any input state to which it is added, but cannot transfer to independently trained agents. Lukas et al.~\cite{lukas2021deep} define a similar property for classifiers called ``conferrability''. Conferrable adversarial examples can transfer from the original classifier to its derivatives but not to independently trained classifiers. In contrast, \fingerprint does not generate individual adversarial examples but instead searches for universal adversarial masks that can be used to generate conferrable adversarial examples.

\subsubsection{Fingerprint Generation.} 

During fingerprint generation, \fingerprint first computes the universal adversarial mask using the original policy $\pi_{\mathcal{V}}$ and independently trained models $\pi_{i}, (i \in \mathcal{I})$ that have the same DNN architecture. \fingerprint aims to find a universal mask $\vect{r}$ that maximizes the loss function in Equation~\ref{eq:lossfunction} and is bounded by $\epsilon$ in $l_{\infty}$-norm.
\begin{equation}\label{eq:lossfunction}
    \mathcal{L}(\pi_{\mathcal{V}}(\vect{s}_t + \vect{r}), \hat{\pi}_{\mathcal{V}}(\vect{s}_t)) - \mathbbm{1}_{(\hat{\pi}_\mathcal{V}(\vect{s}_t) = \hat{\pi}_{i}(\vect{s}_t))}
    \mathcal{L}(\pi_{i}(\vect{s}_t + \vect{r}), \hat{\pi}_{i}(\vect{s}_t))
\end{equation}

The first part of Equation~\ref{eq:lossfunction} maximizes the categorical cross-entropy loss between $\pi_{\mathcal{V}}$'s predictions for clean and adversarial states \newtext{using the log-probability vector for all actions $\pi_{\mathcal{V}}$ in adversarial state and the performed action $\hat{\pi}_{\mathcal{V}}$ in the clean version of that state. The second part minimizes the categorical cross-entropy loss between adversarial states $\vect{s}_{t} + \vect{r}$ computed for $\pi_i$ and their clean counterparts $\vect{s}_t$ only if the predicted action for $\vect{s}_t$ is the same for both $\pi_{\mathcal{V}}$ and $\pi_{i}$. The modified loss function ensures that the same $\vect{s}_t + \vect{r}$ cannot mislead $\pi_{\mathcal{V}}$ and $\pi_{i}$ in the same way, even if $\hat{\pi}_{i}$ produces a suboptimal action.} \newtext{\fingerprint uses untargeted adversarial examples as fingerprints (see Section~\ref{ssec:adversarial}), so the solution of Equation~\ref{eq:lossfunction} forces $\pi_{\mathcal{V}}$ into the incorrect action in $\vect{s}_{t} + \vect{r}$, but has a minimum effect on $\pi_i$.} Multiple independently trained policies are used to calculate the second part of Equation~\ref{eq:lossfunction} for each $i \in \mathcal{I}$ by taking the average of individual losses. A universal adversarial mask should also achieve a high fooling rate $\delta_{\vect{r}}$ as presented in Equation~\ref{eq:foolingrate}.

%minimizes the categorical cross-entropy loss of independent policies in the same clean and adversarial states only if the predicted action for $\vect{s}_t$ is the same for both $\pi_{\mathcal{V}}$ and $\pi_{i}$. } 

\begin{algorithm}[t]
\caption{Fingerprint generation}
\label{alg:fingerprint_generation}
\begin{algorithmic}[1]
\Require{$\mathcal{D}_{flare}$: Fingerprint generation set}
\Ensure{FRL: Fingerprint list}
\State \text{parameters:} $\tau_{nts}, \tau_{\delta}, 
n_{\text{episodes}}, n_{\text{FRL}}$
\State $\text{FRL} = [\;]$.
\For{$eps \leq$ $n_{\text{episodes}}$}
    %\State $\mathcal{D}_{e}= [\;]$
    \State \text{Generate} $\vect{r}_{candidate}$ \text{from}  $\mathcal{D}_{flare}$
    %\For{$\vect{s}_t \in$ episode}
    %    \State $\mathcal{D}_{e}.\text{append}(\hat{\pi}_{\mathcal{V}}(\vect{a} | \vect{s}_t + \vect{r}_{candidate}) , \vect{s}_t)$
    %\EndFor
    \State Compute $nts(\vect{r}_{candidate})$ using $\mathcal{I}$ and $\forall \vect{s}_t \in eps$
    \If {$nts \geq \tau_{nts} \;\textbf{and}\; \delta_{\vect{r}_{candidate}} \geq \tau_{\delta}$}
        \State Add $\vect{r}_{candidate}$ into FRL
    \EndIf
    \If {$len(\text{FRL}) == n_{\text{FRL}}$}
        \State return FRL
    \EndIf
    %\State {Run next episode, $e=e+1$} 
\EndFor
\State \textbf{return} \text{FRL}
\end{algorithmic}
\end{algorithm}

To ensure universality, \fingerprint uses an approach similar to~\cite{pan2022characterizing} when solving Equation~\ref{eq:lossfunction}. First, \victim completes one episode and the observed states are saved in a training set $\mathcal{D}_{flare}$. Then \fingerprint computes the average gradient of the loss function in Equation~\ref{eq:lossfunction} w.r.t. $k$ states randomly sampled from $\mathcal{D}_{flare}$. This enables \fingerprint to generate $len(\mathcal{D}_{flare})\choose{k}$ different universal adversarial masks as a fingerprint candidate. After generating the fingerprint candidate, \fingerprint checks its \emph{non-transferability score}. We compute the non-transferability score $(nts)$ for a universal adversarial mask $\vect{r}$ on an episode $eps$ (that $\pi_{\mathcal{V}}$ follows) as 
\begin{equation}\label{eq:nts}
    nts(\vect{r}, eps) = \delta_{\vect{r}, eps}\times max_{i \in \mathcal{I}}(1-AA(\pi_{\mathcal{V}}, \pi_{i}, \vect{s}, \vect{r})),
\end{equation}
where $\delta_{\vect{r}, eps}$ refers to the fooling rate measured for $\pi_{\mathcal{V}}$ using all $\vect{s}$ observed in $eps$. $AA$ denotes \emph{action agreement} and is calculated as
\begin{equation}
    AA(\pi_i, \pi_j, \vect{s}, \vect{r}) = \frac{1}{N}\sum_{\substack{t=0}}^{t=N} 
    \mathbbm{1}_{(\hat{\pi}_i(\vect{s}_t + \vect{r}) = \hat{\pi}_j(\vect{s}_t + \vect{r}))},
\end{equation} 
where $N$ refers to the length of one full episode $eps$ that $\pi_{\mathcal{V}}$ follows.

\fingerprint only accepts the candidate $\vect{r}_{candidate}$ as a valid fingerprint if $nts(\vect{r}_{candidate})$ is greater than a threshold value $\tau_{nts}$ and achieves a fooling rate $\delta_{\vect{r}_{candidate}}$ higher than $\tau_{\delta}$ over a single $eps$. How \fingerprint decides whether to include a universal adversarial mask in a fingerprint list FLR is presented in Algorithm \ref{alg:fingerprint_generation}.

\subsubsection{Fingerprint Verification.} For fingerprint verification, the verifier \verifier has given a fingerprint set FLR. \verifier first observes the interactions between the suspected agent $\mathcal{S}$ and the environment to estimate the total number of states $N$ that occur during a single episode. Then, for each subsequent episode, \verifier adds one fingerprint starting from a random state at time $t_{start}$ over a short time window of length $M$ to preserve the return in an acceptable range. \victim is also queried with the adversarial states $\vect{s}_t + \vect{r}$ that the suspected agent receives. For each fingerprint, $AA$ is calculated as $1/M\sum_{\substack{t=t_{start}}}^{t_{start}+M-1} AA(\pi_{\mathcal{V}}, \pi_{\mathcal{S}}, \vect{s}_t, \vect{r})$. If $AA$ for a single fingerprint exceeds a decision threshold $AA \geq 0.5$, that fingerprint produces supporting evidence to verify that the suspected model is the stolen copy. The final verdict (stolen vs. independent) is made based on the \emph{majority vote}. \fingerprint also returns $AA$ averaged on all fingerprints to quantify the confidence in the final decision. The verification procedure is summarized by Algorithm~\ref{alg:fingerprint_verification}.

\begin{algorithm}[t]
\caption{Fingerprint verification}
\label{alg:fingerprint_verification}
\begin{algorithmic}[1]
\Require{FRL, $\pi_{\mathcal{V}}$, $\pi_{\mathcal{S}}$: Fingerprint list, victim and suspected policies}
\Ensure{$AA$, $Mvote$: action agreement, majority vote}
%\State \text{parameters:} $M$
\State $AA = [\;], Mvote=0, Tvote=0.$
\State{Run a single episode with $\pi_{\mathcal{S}}$, save total number of states $N$}
%\State{Save number of states $N$ observed by $\pi_{\mathcal{S}}$ to finish episode}
\For{$i,\vect{r}_i\; \text{in}\; (range(\text{FRL}), \text{FRL})$}
    \State{$AA_i = 0.0$}
    \State {Generate random $t_{start} \in [0,min(N, N-M)]$}
    \State {Run a test episode with $\pi_{\mathcal{S}}$}
    \While {test episode of $\pi_{\mathcal{S}}$ not finished}
    \State {Calculate $AA_i$ over time steps $t \in  [t_{start}, t_{start}+M)$}
    %\If {time step $t \in  [t_{start}, t_{start}+M)$}
    %\State{$ AA_i = AA_i + 
    %\mathbbm{1}_{(\hat{\pi}_\mathcal{S}(\vect{s}_t + \vect{r}) = \hat{\pi}_\mathcal{V}(\vect{s}_t + \vect{r}))}$ }
    %\EndIf
    \EndWhile
    %\State{$AA_i = AA_i/M$} 
    \State{$Mvote \,+= 1$ \textbf{if} $(AA_i \geq 0.5)$, $Tvote\,+=1$}
    \State{Add $AA_i$ into $AA$}
    \State{Decision: Stolen \textbf{if} $Mvote > (Tvote-Mvote)$}
\EndFor
\State \textbf{return} Decision, $Mvote$, mean and std of $AA$
\end{algorithmic}
\end{algorithm}
