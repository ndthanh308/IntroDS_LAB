%auto-ignore
\section{Conclusion}
\label{sec:Conclusion}

In this paper, we propose \fingerprint, the first fingerprint method that can be used for ownership verification of DRL policies, and show the existence of non-transferable universal adversarial masks in DRL settings. We empirically demonstrate that our fingerprints are efficient and do not accidentally accuse independently trained models. Adversarial training is the only method that evades verification by making policies robust to adversarial examples. However, our experiments show that the fingerprints obtained by \fingerprint for robust policies are persistent. We hypothesize that \fingerprint can be extended to continuous tasks, where the verifier can check how much the suspected agent deviates from the original action value, and we leave this for future work.

A promising direction for future work related to DRL fingerprinting is to study whether an intentional change in environment conditions can be useful candidates for fingerprints. DRL policies show decreased robustness when deployed in a different environment and include high-sensitivity directions due to natural causes. This vulnerability leads to model evasion attacks via natural adversarial examples, but can also be leveraged to learn natural (and non-transferable) fingerprints for ownership verification. We believe that our study can create more interest in securing DRL agents using novel ownership verification methods against possible model piracy and extraction attacks.   

\subsubsection*{Acknowledgements} \newtext{This research was partially supported by Intel. We thank Dr. Samuel Marchal and Shelly Wang for initial discussions on this problem and for collaborating on an alternative approach to fingerprinting DRLs that we explored prior to the solution presented in this paper. We also thank Aalto Science-IT for computational resources.}