%auto-ignore
\section{Appendix}
\label{sec:appendixA}
\subsection{Training DRL Agents}
\label{ssec:appendixA1}

\begin{table*}
\centering
\caption{Return (averaged over 10 test episodes) of the victim and independent policies trained for MsPacman. The best and worst agents for the same DRL algorithm are highlighted in green and red, respectively.}\label{tab:mspacman}
\begin{tabular}{c|c|ccccc}
\multicolumn{1}{c}{\bf DRL Method} & \multicolumn{1}{c}{\bf Victim Agent}  & \multicolumn{5}{c}{\bf Independent Agents} \\
\hline
A2C & \textcolor{green}{$3316.00 \pm  512.72$} & \textcolor{red}{$1670.00 \pm 537.27$} & $2552.00 \pm 595.66$ & $2144.00 \pm 816.58$ & $2246.00 \pm 4.90$ &  $1750.00 \pm 72.66$ 
\\
DQN & \textcolor{green}{$2620.00 \pm 80.62$} & $2363.00 \pm 269.26$ & $2484.00 \pm 389.67$ & $2218.00 \pm 347.84$ & \textcolor{red}{$2211.00 \pm 154.24$} & $2472.00 \pm 412.74$ 
\\
PPO & \textcolor{green}{$2731.00 \pm 545.50$} & $ 2019.00 \pm 77.13$ &  $2198.00\pm536.35$ & $2040.00 \pm 161.43$ & \textcolor{red}{$2017.00 \pm 397.57$} & $2167.00 \pm 268.52$ 
\\
\hline
\end{tabular}
\end{table*}

To facilitate the comparison, we used the same setup to implement all DRL policies and attacks: PyTorch (version 1.4.0), NumPy (version 1.18.1), Gym (a toolkit for developing reinforcement learning algorithms, version 0.15.7) and Atari-Py (a Python interface for the Arcade Learning Environment,
version 0.2.6). All experiments were carried out on a computer with 2x12 core Intel(R) Xeon(R) CPUs (32GB RAM) and NVIDIA Quadro P5000 with 16GB memory. To train DQN agents, we used a dueling Q-network architecture proposed in~\cite{wang2016dueling}. For training A2C and PPO agents, we choosed to implement the convolutional neural networks suggested in OpenAI Baselines\footnote{\url{https://github.com/openai/baselines}}. In both A2C and PPO, actors and critics use the same architecture, except for the penultimate later. The hyperparameter values of each victim agent are set the same as OpenAI baselines, while slightly differ for training independent agents.

All victim and independent DRL agents trained to play Pong reach the highest score +21. The summary of agents trained to play MsPacman is presented in Table~\ref{tab:mspacman}. In MsPacman, we deliberately chose agents with the best performance as the victim, since they have a clear business advantage over other models, thus incentivizing adversaries to apply piracy attacks against them.

\begin{table}
\centering
\caption{Hyperparameters used in fingerprint generation}\label{tab:hyperparameters}
\resizebox{1.0\columnwidth}{!}{%
\begin{tblr}{ccc}
\bf Parameter & \bf Value  & \bf Definition \\
\hline
$\epsilon$ & $0.05$ & $l_{\infty}$ constraint on the perturbation $\vect{r}$ \\
$\tau_{nts}$ & $0.7$& minimum non-transferability score of $\vect{r}$\\
$\tau_{\delta}$ & $0.8$& minimum fooling rate of $\vect{r}$ on a dataset\\
\SetCell[r=2]{m} $n_{\text{episodes}}$ & \SetCell[r=2]{m} $1000$ & maximum number of training episodes \\
& & to generate/collect fingerprints \\
%$n_{\text{FRL}}$ & $10$& maximum number of fingerprints\\
\hline
\end{tblr}%
}
\end{table}

\subsection{Hyperparameter Selection in \fingerprint}
\label{ssec:appendixA3}

The perturbation constraint $\epsilon$ directly affects the trade-off between the success of an adversarial example and its non-transferability. Therefore, we performed a grid search for $\epsilon$ and set it to an optimal value $0.05$. We set the minimum fooling rate $\tau_{\delta}$ at a high value $0.8$ to ensure the universality of the adversarial mask and set the non-transferability score at $0.7$. Based on these values and Equation~\ref{eq:nts}, for a candidate universal adversarial mask $\vect{r}$, the minimum action agreement of independent agents $ min_{i \in \mathcal{I}}(AA(\pi_{\mathcal{V}}, \pi_{i}, \vect{s}, \vect{r}))$ should be lower than $0.125$ to be chosen as a valid fingerprint. For both the Pong and MsPacman agents, we used a reduced set of actions (4 discrete actions in total). The minimum $ min_{i \in \mathcal{I}}(AA(\pi_{\mathcal{V}}, \pi_{i}, \vect{s}, \vect{r})) = 0.125$ is much lower than $0.25$ ($AA$, if actions are randomly chosen) and satisfies the non-transferability requirement. Finally, we set $n_{\text{episodes}}$ at a high value to guarantee that a sufficient number of fingerprints is generated for efficient verification. The prescribed hyperparameter values during fingerprint generation are listed in Table~\ref{tab:hyperparameters}.  

\subsubsection{Selection of the Number of Fingerprints}

The number of fingerprints generated and used for verification affects integrity and robustness. An insufficient number of fingerprints could result in a high action agreement $AA$ between the independent and victim (original) policies and ultimately falsely verify the ownership of the independent policies as explained in Section~\ref{ssec:reliability}. On the contrary, a high number of fingerprints could give low $AA$ between the victim policy and its modified versions, since some of the fingerprints could give a lower fooling rate in the modified policies. For that, we performed verification by changing the maximum number of fingerprints used for fingerprint generation. As demonstrated in Figure~\ref{fig:fingerprints_onPong}, the number of fingerprints does not affect the return during verification, but a sufficient number of fingerprints (around 5) are needed to achieve high $AA$ to provide high confidence for the final decision. We set the number of fingerprints at $10$ to satisfy the effectiveness and robustness requirements simultaneously. 

\subsubsection{Selection of the Window Size.} In addition to the number of fingerprints, the decision on window size is important. If the window size is large, then the return during verification decreases and the agent can perform poorly. Figure~\ref{fig:windowsize_onPong} illustrates the effect of window size on return and $AA$ during verification for Pong DQN agents. Although $AA$ does not change significantly with larger window sizes, there is a steady decline in return. Based on this result, the window size can be set to $40$ or even less, but we set it to $40$ after performing the same analysis for all agents and observing the change in return.   

% Figure environment removed

% Figure environment removed

\subsubsection{Computation costs of \fingerprint}

\begin{table}
\centering
\caption{\newtext{Total number of trials (i.e., episodes) required for obtaining the fingerprint list during the fingerprint generation phase (2nd column), and the average ratio of adversarial states that includes the fingerprint to the total number of states observed for the same episode (3rd column) during verification. The ratio is averaged over 10 verification episodes for each victim agent.}}\label{tab:costoffingrprint}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{ccc}
 & \bf $\#$ of trials & \bf ($\#$ of adversarial states)/($\#$ of states) \\
 & \bf in generation & \bf in verification \\ 
\hline
Pong, A2C & $34$ & $0.02 \pm 0.00$\\
Pong, DQN & $46$ & $0.02 \pm 0.00$\\
Pong, PPO & $14$ & $0.02 \pm 0.00$\\
MsPacman, A2C & $110$ & $0.04 \pm 0.01$\\
MsPacman, DQN & $38$ &  $0.05 \pm 0.01$\\
MsPacman, PPO & $10$ &  $0.05 \pm 0.01$\\
\hline
\end{tabular}%
}
\end{table}

\newtext{Based on the hyperparameters chosen in our experimental setup, we computed the number of trials (i.e. epiosodes) required to generate the fingerprint list and presented them in Table~\ref{tab:costoffingrprint}. The required number of trials is less than 50 in almost all cases, except MsPacman. We found that this exception occurs due to the high $AA$ generated for some independent policies used during the fingerprint generation phase. To generate each $\vect{r}_{candidate}$ (see Algorithm~\ref{alg:fingerprint_generation}, line 4), \fingerprint randomly selects 100 states and computes the average gradient using those states. During verification, based on the window size ($40$) and the number of fingerprints ($10$), the suspected models are queried $400$ times in total with the additional fingerprint. Table~\ref{tab:costoffingrprint} also shows the average ratio of states with an additional fingerprint to the total number of states observed during verification episodes. Based on these results, we confirm that verification episodes include only a small number of states (up to $5\%$) with the additional fingerprint.}

\subsection{Receiver Operating Characteristic of \fingerprint}
\label{ssec:appendixA4}

\newtext{Figure~\ref{fig:roc_curve} shows the receiver operation characteristic (ROC) curve produced by the verification results of individual fingerprints over multiple thresholds $\tau_{AA}$, where the $i$-th fingerprint votes ``stolen'' when $AA_{i} \geq \tau_{AA}$. For each victim policy, we calculated true positive and false positive rates (TPR and FPR) on 10 fingerprints that are used to verify the victim policy itself, 3 randomly selected independent policies, and 3 fine-tuned versions of the victim policy incurring a small impact on utility. We found the optimal $\tau_{AA}$ that maximizes TPR and minimizes FPR to be $0.5$ and $0.68$ in Pong and MsPacman, respectively. We set the threshold value at $0.5$ in all our experiments, but it would be beneficial to analyze the ROC for each environment, as the choice of $\tau_{AA}$ affects the overall effectiveness and integrity of \fingerprint.} 

% Figure environment removed

\subsection{Impact of Verification}
\label{ssec:appendixA5}

As discussed in Section~\ref{ssec:adversarymodel}, utility is not a necessary requirement in fingerprinting methods, as fingerprints typically trigger abnormal behavior. However, the impact on agent performance is still important, since verification might also be carried out in a stealthy way to avoid raising any suspicion. Moreover, if the agent fails to perform the task quickly during verification, then the collected information may not be sufficient to correctly calculate the action agreement $AA$. Therefore, we computed the impact of verification on agent performance and summarized the results in Table~\ref{tab:vimpact}. In Pong, the impact is almost zero, while we experienced an average impact of $0.22$ in MsPacman agents due to the high complexity of the game. The impact in MsPacman can be further improved by adding fingerprints in non-critical states that do not affect the return if the agent replaces one action with another.

\begin{table}
\centering
\caption{Impact of verification (averaged over 10 verification episodes) on victim agent performance.}\label{tab:vimpact}
\begin{tabular}{c|c|cc}
\multicolumn{1}{c}{\bf DRL Method} & \multicolumn{1}{c}{\bf Pong}  & \multicolumn{1}{c}{\bf MsPacman} \\
\hline
A2C & $0.02 \pm 0.02$ & $0.20 \pm 0.21$ \\
DQN & $0.01 \pm 0.02$ & $0.28 \pm 0.19$ \\
PPO & $0.02 \pm 0.02$ & $0.18 \pm 0.28$ \\
\hline
\end{tabular}
\end{table}