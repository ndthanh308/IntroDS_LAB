%auto-ignore%auto-ignore
\section{Background}
\label{sec:Background}
\subsection{Deep Reinforcement Learning}
\subsubsection{Reinforcement Learning} A typical reinforcement learning (RL) problem is modeled as a 5-tuple Markov Decision Process (MDP) $(S,A,P,R,\gamma)$, where $S$ denotes the state space, $A$ is the action space, $P$ symbolizes the state transition probability (i.e., environment dynamics), $R$ is the reward function, and $\gamma \in [0,1]$ denotes the discount factor used to calculate the discounted cumulative reward, i.e., \emph{return}. In this setting, the RL agent receives a state $\vect{s}_t \in S$ at the time step $t$, performs an action $a_t \in A$, and then subsequently receives a reward $r_{t+1}$ as well as the next state $\vect{s}_{t+1}$ based on $P(\vect{s}_{t+1}|\vect{s}_t,a_t)$. The objective of an RL agent is to maximize its expected return by interacting with the environment and to obtain an optimal policy $\pi(a | \vect{s}): S \rightarrow A$ that outputs an optimal action (the action that gives the maximum expected return over all actions) for any given state. During training, the policy is optimized recursively by calculating the expected return over states using the Bellman equation~\cite{SuttonBarto1998}. In this work, we consider states to be fully observable and finite-horizon tasks (i.e., an episode is completed when a stopping criterion is reached). Therefore, the discounted return at a time step $t$ is calculated as $R_{t} = \sum_{k=t}^{T}\gamma^{k-t}r_k$ where T is the final time step in a single episode. We also focus on tasks with a discrete action space, where one-hot vectors can be used to distinguish one action from every other action. 

%At the time step $t$, the RL agent receives a state $\vect{s}_t \in S$, performs an action $a_t \in A$, and then subsequently obtains a reward $r_t$ as well as the next state $\vect{s}_{t+1}$. 

%In reinforcement learning (RL), the goal of the RL agent is to maximize its expected total reward (i.e., return) $\text{R}$ by interacting with the environment and finding the optimal strategy. In this work, we adhere to the typical RL setting where the environment is modeled as a 5-tuple Markov Decision Process (MDP) $(S,A,P,R,\gamma)$, where $S$ denotes the state space and $A$ is the set of discrete actions. The states are numerical representations of the environment received by the agent. 

%At the time step $t$, the agent receives a state $\vect{s}_t \in S$, performs an action $a_t \in A$, and then subsequently obtains a reward $r_t$ as well as the next state $\vect{s}_{t+1}$. In the 5-tuple MDP, the remaining $P$ denotes the environment dynamics (state transition probabilities) and $\gamma \in [0,1]$ is the discount factor used to calculate the return. 
%We consider states to be fully observable, and the task is finite horizon (i.e., an episode is completed when a stopping criterion is reached), and therefore the return at $t$ is calculated as $R_{t} = \sum_{t'}^{T}\gamma^{t'-t}r_t$ where T is the final time step in a single episode. 

%The agent aims to obtain an optimized policy $\pi(a | \vect{s}_t): S \rightarrow A$, which is a function that maps states into actions. The optimal policy maximizes the expected return for each state $\vect{s}_t$. 

%The action-value (q-value) function $Q^{\pi}(\vect{s},a)$ computes the estimated return of state $\vect{s}_t$ where the agent chooses the action $a_{t}$ and then follows the policy $\pi$. $Q^{\pi}(\vect{s},a)$ is optimized recursively with the Bellman equation~\cite{SuttonBarto1998}:
%\begin{equation}
%    Q^{\pi}(\vect{s},a) = \mathbb{E}[r_t + \gamma \argmax_{a'}Q^{\pi}(\vect{s}_t,a)]
%\end{equation}

\subsubsection{Deep Reinforcement Learning (DRL)} When the state space $S$ is too complex and high-dimensional, deep neural networks (DNNs) can be useful to approximate policy $\pi(a | \vect{s})$. In this work, we assume that the environment is dynamic, as in real-world applications. Model-free DRL methods are the preferred approach in this setting, since these methods do not require estimating the dynamics of the environment. Two typical model-free DRL methods approximate $\pi$: value-based and policy-based methods. Value-based~\cite{mnih2015human} methods approximate the action value function $Q^{\pi}(\vect{s},a)$ which computes the estimated return of state $\vect{s}_t$ if the agent chooses the action $a_{t}$ and then follows the current policy. The optimal policy is implicitly obtained once $Q^{\pi}$ is optimized. Policy-based methods~\cite{mnih2016asynchronous, schulman2017proximal} first parameterize the policy $\pi(a|\vect{s}, \theta)$ and then optimize it by updating the parameters $\theta$ through the gradient ascent. 

In this paper, we use $\pi$ to symbolize the optimal policy obtained during training and $\hat{\pi}$ to denote the optimal action $a_t$ decided by $\pi$ for the input state $\vect{s}_t$, where $a_t = \hat{\pi}(\vect{s}_t)$. 

\subsection{Adversarial Examples}
\label{ssec:adversarial}

\subsubsection{Adversarial Examples in DNN} An adversarial example $\vect{x}'$ is an intentionally modified input sample $\vect{x} \in X$ with an imperceptible amount of noise $\vect{r}$ to force a DNN model $f: X \rightarrow Y$ into producing incorrect predictions $\hat{f}$. Targeted adversarial examples are labeled with $y'$, the intended (incorrect) prediction, in advance to satisfy $y' = \hat{f}(\vect{x}')$ and $ y' \neq \hat{f}(\vect{x})$, while untargeted adversarial examples aim to evade the correct prediction, i.e., $\hat{f}(\vect{x}') \neq \hat{f}(\vect{x})$. Untargeted adversarial examples against a victim DNN model $f$ are computed by solving an optimization problem, 
\begin{gather}\label{eq:advexamples}
      \argmax_{\vect{x}'}\mathcal{L}(f(\vect{x}'),\hat{f}(\vect{x}))\;
    \text{\small s.t.: } \lVert{\vect{x}'- \vect{x}}\rVert_{p} = \lVert{\vect{r}}\rVert_p \leq \epsilon,
\end{gather}
where $\mathcal{L}$ denotes the prediction loss of $f$. 
%The most prevalent adversarial example generation techniques are gradient-based, and iteratively update $\vect{x}'$ along the gradient of $\ell$. 
This formulation is used by \emph{maximum-confidence} adversarial example generation methods~\cite{demontis2019adversarial} that maximize $\mathcal{L}$ while constraining the amount of perturbation with $\epsilon$. On the contrary, the \emph{minimum-distance} methods aim to minimize the sufficient amount of perturbation that changes the prediction~\cite{moosavi2017universal}. 

An adversarial example $\vect{x}'$ calculated against one model $f$ and successfully misleads it can \emph{transfer} across other models, i.e., fools $f^{*}$ that are trained for the same task. The transferability of an adversarial example increases when the source model $f$ and the target models $f^{*}$ learn similar decision boundaries~\cite{tramer2017space}. Since maximum-confidence adversarial examples are misclassified with higher confidence, they have a higher transferability rate than minimum-confidence adversarial examples~\cite{demontis2019adversarial}.


The definition of an adversarial example in DRL differs according to the target component of the victim agent and the overall goal~\cite{huang2017adversarial,gleave2019adversarial, weng2020toward,MankowitzL2020Robust,tekgul2022real,pan2022characterizing}.
In this work, we consider adversarial states $\vect{s}'$ that mislead the policy $\pi$: $\hat{\pi}(\vect{s}') \neq \hat{\pi}(\vect{s})$, $\lVert{\vect{s}'-\vect{s}}\rVert_{p} = \lVert{\vect{r}}\rVert_p$, and set the norm $p$ to $\infty$.

\subsubsection{Universal Adversarial Perturbations} 
Instead of computing individual adversarial examples, Moosavi et al.~\cite{moosavi2017universal} propose finding a perturbation vector $\vect{r}$ that fools the DNN model $\hat{f}(\vect{x} + \vect{r}) \neq \hat{f}(\vect{x})$ on almost all data points $\vect{x}$ sampled from the same distribution as the dataset $\mathcal{D}_{train}$ used for training $f$. The optimization problem in Equation~\ref{eq:advexamples} is modified to find universal perturbations as 

\begin{gather}\label{eq:foolingrate}
\mathbb{P}_{\vect{x} \sim \mu}(\hat{f}(\vect{x} + \vect{r}) \neq \hat{f}(\vect{x})) \geq \delta_{\vect{r}}\;
\text{\small s.t.: } \lVert{\vect{r}}\rVert_p \leq \epsilon,
\end{gather}
where $\delta_{\vect{r}}$ denotes the desired \emph{fooling rate} of $\vect{r}$ for all $\vect{x}$ sampled from a dataset $D$ with distribution $\mu$.

Following Moosavi et al.'s initial work~\cite{moosavi2017universal}, several different techniques are proposed to generate universal adversarial perturbations. For example, Mopuri et al.~\cite{mopuri2018NAG} train a generative adversarial network to model the distribution of universal adversarial perturbations for a target DNN classification model and produce diverse perturbations that achieve a high $\delta_{\vect{r}}$. Liu et al.~\cite{Liu2019Universal} generate a universal adversarial perturbation that does not require training data and exploits the uncertainty of the model at each DNN layer. 

%\subsubsection{Adversarial Examples in DRL} Recent work has shown that DRL policies are vulnerable to adversarial examples generated for agents' states~\cite{huang2017adversarial,tekgul2022real} or actions~\cite{weng2020toward} in single-agent environments, or produce natural adversarial states by 
%exploiting other agents in multi-agent settings~\cite{gleave2019adversarial}. Other studies focus on perturbing the dynamics of the environment by changing the environment conditions~\cite{MankowitzL2020Robust,pan2022characterizing}. In this work, we consider $\ell_{p}$ norm adversarial states $\vect{s}'$ that mislead the policy $\pi$: $\hat{\pi}(\vect{s}') \neq \hat{\pi}(\vect{s})$ and $\lVert{\vect{s}'-\vect{s}}\rVert_{p} = \lVert{\vect{r}}\rVert_p$.


\subsection{Ownership Verification via Fingerprinting}
%\subsubsection{Ownership verification in DRL}
Ownership verification in machine learning refers to a type of defense against model theft and extraction attacks by deterrence. Model owners can reduce the incentive for such attacks by identifying and verifying the true ownership of stolen models. DNN model fingerprinting is a well-known ownership verification technique. DNN fingerprinting methods identify unique knowledge that characterizes the victim model (fingerprint generation) and later use this information to verify whether the suspected model is derived from the victim model (fingerprint verification). For example, Ciao et al.~\cite{Cao2021IPGuard} use adversarial example generation methods to extract data points near the decision boundary of DNN classifiers, label them as fingerprints, and utilize them along with their labels to detect piracy models. Lukas et al.~\cite{lukas2021deep} fingerprint DNN models through conferrable adversarial examples (CAE) that can successfully transfer from the source model to its modified versions, but not to other DNN models independently trained for the same classification task. To verify the fingerprint in a suspected model, CAE measures the error rate between the predictions of victim and suspected models, and the verdict is delivered based on a decision threshold. Despite its effectiveness, CAE has a high computational cost, since it requires training multiple modified and independent models to extract conferrable adversarial examples. Peng et al.~\cite{peng2022fingerprinting} propose using universal adversarial perturbations (UAP) as fingerprints. During verification, previously computed UAPs for both victim and suspected models are mapped to a joint representation space, and contrastive learning is used to measure a similarity score in this projected space.
%Unlike watermarking, fingerprinting methods insert no additional information while training the model, so they do not affect overall utility. 

Adopting both UAP and CAE in DRL settings faces similar challenges. First, the verification episodes should include adversarial states that are completely different from each other, and also from a normal test episode during deployment. Second, CAEs employ predictions of different independently trained models and modified versions of the victim model to compute fingerprints. The predictions of well-trained DNN models are close to each other for the same input samples, since these models are trained over a labeled dataset. However, there is no single predefined optimal action for input states in DRL. When agents receive the same state, they might act differently to perform the task due to their unique and different policies. %Similar challenges appears when attempting to implement UAP fingerprinting~\cite{peng2022fingerprinting} in DRL. 
Third, UAP fingerprinting uniformly selects data samples that are from different source classes and moves them towards different target classes in DNNs, but there is no one-to-one mapping between input states and corresponding optimal actions to obtain useful fingerprints in DRL settings.

%Previous DRL ownership verification methods adopt well-known watermarking techniques used for DNN~\cite{adi2018turning,zhang2018protecting,lounici2021yes} in DRL settings. Behzadan et al.~\cite{behzadan2019sequential} propose the embedding of sequential states that are separate from the main environment as watermarks during training. However, watermark verification also requires a disjoint environment, and there is no guarantee that watermarks are retained while learning complex tasks. Chen et al.~\cite{chen2021temporal} obtain sequence of states as watermarks that are sampled from the same environment and do not damage the performance of the agent. During verification, the authors compare the action probability distributions given by both the victim and suspected agents over these sequential states. However, this watermarking method requires modifying both the training process and the reward function. 

%\subsubsection{Fingerprinting in DNNs}
%DNN fingerprinting methods identify unique knowledge that characterizes the trained model (fingerprint generation) and later use this information to verify whether the suspected model is derived from the victim model (fingerprint verification). Unlike watermarking, fingerprinting methods insert no additional information while training the model, so they do not affect overall utility. Ciao et al.~\cite{Cao2021IPGuard} use adversarial example generation methods to extract data points near the decision boundary of DNN classifiers, label them as fingerprints, and utilize them along with their labels to detect piracy models. Lukas et al.~\cite{lukas2021deep} fingerprint DNN models via conferrable adversarial examples (CAE). CAE are adversarial examples that can successfully transfer from the source model to its modified versions, but not to other DNN models independently trained for the same classification task. To verify the fingerprint in a suspected model, the error rate between the predictions of this model and the victim model, and the verdict is delivered based on a decision threshold. CAE has a high computational cost, since it requires training multiple modified and independent models to extract adversarial examples.
%Peng et al.~\cite{peng2022fingerprinting} propose using universal adversarial perturbations (UAP) as fingerprints. During verification, previously computed UAPs for both victim and suspected models are mapped to a joint representation space, and contrastive learning is used to measure a similarity score in this projected space. Compared to watermarking, fingerprinting methods show improved robustness to model modification and extraction attacks with well-informed adversaries. However, currently there is no work that suggests fingerprinting DRL agents for ownership verification. 
