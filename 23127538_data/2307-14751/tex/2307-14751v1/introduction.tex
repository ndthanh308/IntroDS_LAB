%auto-ignore
%auto-ignore\section{Introduction}
\label{sec:Introduction}

Deep reinforcement learning (DRL) has emerged as a promising technique for building intelligent agents due to its ability to learn from and interact with high-dimensional input data.
Following the work of Mnih et al.~\cite{mnih2015human}, which shows that DRL has exceeded human-level performance in Atari games, it has been successfully used in many real-world applications, including green data centers~\cite{li2019transformingcooling}, autonomous driving~\cite{kiran2021deep} and robotic manipulation~\cite{pmlr-v87-kalashnikov18a}. 

The commercial success and continuous improvement of DRL methods attract adversaries, leading them to look for and exploit vulnerabilities in DRL agents. DRL agents leverage the power of deep neural networks (DNNs) to improve agents' decision-making strategy, i.e.,\emph{ policy}. Therefore, known vulnerabilities of DNNs might also be valid for agents' policies. For example, considerable research has been devoted to evasion attacks against DRL policies using adversarial examples~\cite{huang2017adversarial,gleave2019adversarial,Inkawhich2020snooping,pan2022characterizing,tekgul2022real}, which are generally computed for input states that are representations of the environment received by the agent. 
%When the input state is adversarial, the agent's policy is expected to yield a sub-optimal action, and consequently the agent shows a poor performance or even fails the task. %Proposed defenses~\cite{lin2017detecting,zhang2020robust,oikarinen2021robust} aim to preserve the performance during the attack and increase the robustness of agents to adversarial examples. 
Unlike evasion attacks and related defenses~\cite{lin2017detecting,zhang2020robust,oikarinen2021robust}, few studies investigated the ownership of DRL models~\cite{behzadan2019sequential,chen2021temporal,lounici2021yes} against model piracy attacks. The high training costs of DRLs and their business advantage lead adversaries to steal models and redistribute them unauthorized ways. To deter adversaries, it is crucial to have the technical means to identify the true ownership of illegitimate copies of DRL agents. %The same problem is relevant in DRL, and additional measures must be taken to preserve the ownership rights of DRL agents. 

%Model watermarking~\cite{adi2018turning, zhang2018protecting,li2019prove} has become a widely known ownership verification procedure for DNNs. Model watermarking embeds traceable information (i.e. watermark) to the DNN model by either directly inserting it into model parameters or adding unique knowledge into a small subset of the training set. During ownership verification, the existence of the watermark is proved on illegitimate copies. %without wrongly accusing models independently trained on a similar task. %Behzadan et. al~\cite{behzadan2019sequential} and Chen et. al~\cite{chen2021temporal} design a watermarking scheme suitable for DRL agents. 
%Although model watermarking is considered a practical solution to protect the ownership of DNNs, many studies have shown~\cite{lukas2022sok, yan2022cracking} that they cannot withstand well-informed adversaries and model modification attacks. 

%Recently, DNN fingerprinting has been proposed as an alternative ownership verification method~\cite{lukas2021deep, peng2022fingerprinting}. to overcome the issues of model watermarking. 
Recently, DNN fingerprinting has been proposed as an ownership verification method~\cite{lukas2021deep, peng2022fingerprinting}. DNN fingerprinting aims to identify the inherent properties of the victim (original) model and use this information during verification. Current DNN fingerprinting methods leverage \emph{conferrable adversarial examples} (CAE)~\cite{lukas2021deep} or \emph{universal adversarial perturbations} (UAP fingerprinting)~\cite{peng2022fingerprinting}, as adversarial examples can characterize the DNN decision boundary. Conferrable adversarial examples~\cite{lukas2021deep} are a subclass of transferable adversarial examples that can successfully force the victim DNN model and its modified versions into the same wrong predictions, but are not transferable to other independently trained models. Unlike CAE, UAP~\cite{peng2022fingerprinting} obtains universal adversarial perturbations from both victim and suspected models, and produces a similarity score using contrastive learning. Both methods are shown to be effective and robust ownership verification approaches, but adapting them in DRL has challenges. First, both methods query suspected DNN models with adversarial version of input samples with different labels that are selected from the training set. However, this is not possible in DRL due to dynamic environments and continuous agent-environment interaction. Therefore, these methods require constructing a special verification setup that has unconventional environment dynamics and completely changes the trajectory of the agent regardless of the task. Second, there is no one-to-one mapping between the input states and the corresponding actions in DRL. This makes fingerprint generation challenging, since there is no single optimal action for any clean state, and no desired incorrect action for any adversarial state either. 


%CAE has been shown to be a very effective and robust ownership verification approach, but adapting CAE in DRL has challenges. First, a suspected DNN model is queried with CAEs that are independently sampled from the training set. However, this is not possible in DRL due to dynamic environments and continuous agent-environment interaction. %where agents constantly interact with their surroundings, and constantly changing the environment might be laborious. 
%Therefore, CAE requires an unconventional verification setup different from the agent's course. Second, CAEs employ predictions of different independently trained models and modified versions of the victim model to compute fingerprints. The predictions of well-trained DNN models are close to each other for the same input samples, since these models are trained over a labeled dataset. However, there is no single predefined optimal action for input states in DRL. When agents receive the same state, they might act differently to perform the task due to their unique, different policies. The same challenge appears when attempting to implement UAP fingerprinting~\cite{peng2022fingerprinting} in DRL. UAP fingerprinting uniformly selects data samples that are from different source classes and moves them towards different target classes in DNNs, but there is no 1-1 mapping between input states and corresponding actions in DRL settings. 

In this paper, we propose \fingerprint, \emph{the first DRL fingerprinting scheme} designed for discrete reinforcement learning tasks and combines the idea behind CAE and UAP. %We first point out that the effectiveness of adversarial perturbations decreases as they transfer across DRL algorithms and policies~\cite{huang2017adversarial}, thus increasing the number of non-transferable adversarial examples between DRL agents. 
Effectiveness of adversarial perturbations decreases as they transfer from one DRL policy or algorithm to another~\cite{huang2017adversarial}. This implies that it would be possible to find adversarial examples that are not transferable across DRL agents. However, using individual non-transferable adversarial examples for ownership verification might be impractical due to the problems mentioned above. Therefore, \fingerprint aims to generate \emph{non-transferable universal masks} as fingerprints, which are independent of input states, source actions, or target actions. The fingerprints computed by \fingerprint are instances of weaknesses that are inherent in the victim's DRL policy. \fingerprint leverages these weak points to verify the true ownership of \emph{suspected} policies. During verification, suspected agents receive states modified by applying a universal mask from the victim's fingerprint in a small time window while trying to complete their task. \fingerprint verifies the true ownership if the similarity between the actions of the suspected agent and the victim agent in the same fingerprinted states is greater than a threshold value. %Unlike watermarking, 
\fingerprint does not change the training procedure, and verification can be implemented at any time during deployment. %without significantly affecting the trajectory of the suspected agent.
Our main contributions are as follows:
\begin{enumerate}
    \item We propose \fingerprint, the first fingerprinting method to verify the ownership of DRL agents used in discrete tasks by leveraging non-transferable universal adversarial masks (Section~\ref{sec:Methodology}). We show that \fingerprint is an effective ownership verification method with no false positives (Section~\ref{ssec:reliability}).%\footnote{The code will be released upon publication\label{footnoteref}.}
    \item We verify the robustness of \fingerprint against model modification attacks (e.g., fine-tuning and pruning) on 6 different DRL agents trained using two different games of the Arcade Learning Environment~\cite{bellemare2013arcade}. We also show that well-informed adversaries cannot easily evade verification without sacrificing agent performance, and \fingerprint is robust against false claims made by malicious accusers. (Section~\ref{ssec:robustness}).
    \item We empirically demonstrate that universal adversarial perturbations generated by minimum-distance methods~\cite{moosavi2017universal,peng2022fingerprinting} are not good candidates for DRL fingerprinting. These perturbations are not unique weaknesses of DRL policies by design and fail against model modification attacks (Section~\ref{sec:Discussion}).
\end{enumerate}