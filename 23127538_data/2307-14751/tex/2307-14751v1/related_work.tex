%auto-ignore
\section{Related Work}
\label{sec:Related}

\textit{Adversarial Examples in DRL :} Recent work has shown that DRL policies are vulnerable to adversarial examples generated for agents' states~\cite{huang2017adversarial,tekgul2022real} or actions~\cite{weng2020toward} in single-agent environments, or produce natural adversarial states by exploiting other agents in multi-agent settings~\cite{gleave2019adversarial}. %When the input state is adversarial, the agent's policy is expected to yield a sub-optimal action, and consequently the agent shows a poor performance or even fails the task. 
Other studies focus on perturbing the dynamics of the environment by modifying the environment conditions~\cite{MankowitzL2020Robust,pan2022characterizing}. DRL adversarial training ~\cite{oikarinen2021robust, zhang2018protecting} has been considered as a mitigation, but adversarially robust policies were found to be more vulnerable to high-sensitivity directions caused by a natural change in the environment~\cite{korkmaz2022deep}.    

\noindent\textit{Ownership Verification via Model Watermarking :} Model watermarking has become a widely known ownership verification procedure for DNNs~\cite{adi2018turning,zhang2018protecting,lounici2021yes}. Model watermarking embeds traceable information (i.e. watermark) into the DNN by either directly inserting it into model parameters or adding unique knowledge into a small subset of the training set. During ownership verification, the existence of the watermark is proven on illegitimate copies. % Although model watermarking is considered a practical solution to protect DNN ownership, many studies have shown~\cite{lukas2022sok, yan2022cracking} that they cannot withstand well-informed adversaries and model modification attacks.
Previous DRL ownership verification methods adapt model watermarking techniques. For example, Behzadan et al.~\cite{behzadan2019sequential} propose the embedding of sequential states that are separate from the main environment as watermarks during training. However, watermark verification also requires a different environment, and there is no guarantee that watermarks will be retained while learning complex tasks. Chen et al.~\cite{chen2021temporal} obtain a sequence of damage-free states as watermarks that are sampled from the same environment and do not impact agent performance. During verification, the authors compare the action probability distributions given by both the victim and the suspected agents over these sequential states. However, this watermarking method requires modifying both the training process and the reward function. 

Although model watermarking is considered a practical solution to protect DNN ownership, many studies have shown~\cite{lukas2022sok, yan2022cracking} that they cannot withstand well-informed adversaries and model modification attacks. Compared to watermarking, DNN fingerprinting methods show improved robustness to model modification and extraction attacks~\cite{lukas2021deep,peng2022fingerprinting}. Furthermore, fingerprinting does not change the training procedure unlike watermarking. However, there is no prior work applying fingerprinting as an ownership verification method in DRL.