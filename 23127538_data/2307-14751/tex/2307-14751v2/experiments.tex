%auto-ignore
\section{Empirical Analysis}
\label{sec:Experiments}

\newcommand{\greenline}{\raisebox{0.0pt}{\tikz{\draw[-, green!40,solid,line width = 6pt](0,0) -- (5mm,0);}}}

\newcommand{\blueline}{\raisebox{0.0pt}{\tikz{\draw[-, blue!30,solid,line width = 6pt](0,0) -- (5mm,0);}}}

\newcommand{\yellowline}{\raisebox{0.0pt}{\tikz{\draw[-, yellow!40,solid,line width = 6pt](0,0) -- (5mm,0);}}}

\newcommand{\redline}{\raisebox{0.0pt}{\tikz{\draw[-, red!40,solid,line width = 6pt](0,0) -- (5mm,0);}}}

\subsection{Experimental Setup}

We evaluated \fingerprint using the Arcade Learning Environment (ALE)~\cite{bellemare2013arcade}. We selected two different games, Pong and MsPacman, from ALE to train agents with three different model-free DRL algorithms: A2C~\cite{mnih2016asynchronous}, DQN~\cite{mnih2015human}, and PPO~\cite{schulman2017proximal}). Pong is a two-player game in which agents are trained to win against the computer, while MsPacman is a single-player game with the goal of achieving the highest score without clashing enemies. When constructing the state information, we applied the same pre-processing methods proposed in~\cite{mnih2015human}. Furthermore, for each victim \victim, we independently trained five additional policies $\pi_{i}\, (i \in \mathcal{I})$ that have the same DNN architecture and the DRL algorithm as \victim, and we use them during fingerprint generation. In total, we obtained six victim policies and thirty independent policies. In Pong, the victim and the independent policies win the game with the highest score (+21). In MsPacman, it was harder to achieve similar high scores since states are more complex than Pong and depend on the position of multiple enemies. In both games, the score is used to quantify the agent's return. Appendix~\ref{ssec:appendixA1} presents software/hardware requirements for reproduction, as well as the average performance of all agents. 

% Figure environment removed

During implementation of \fingerprint, we set the maximum number of fingerprints $len(FRL)$ at 10, and the window size $M$ at 40. The discussion on the choice of $len(FRL)$ and $M$ is included in Appendix~\ref{ssec:appendixA3}. Other hyperparameters used in fingerprint generation are also listed in Appendix~\ref{ssec:appendixA3}. In our experimental setup, we used different random initializations for episodes used in training, fingerprint generation, verification, estimation of agent performance, modification attacks, and evasion attacks in order to ensure randomness in dynamic (and uncontrollable) environments.


\subsection{Effectiveness and Integrity}\label{ssec:reliability}

Figure~\ref{fig:fooling_rate} summarizes various \fingerprint metrics (fooling rate $\delta$, non-transferability score $nts$ and action agreement $AA$ calculated on different policies) for three different DRL algorithms.
$AA_{orig}$ denotes $AA$ of the adversary's policy $\pi_{\mathcal{A}}$ which is identical to the victim policy $\pi_{\mathcal{V}}$. $AA_{ind}\,\text{(verification)}$ refers to $AA$ values of independent policies $\pi_{i}$ that share the same DRL algorithm as $\pi_{\mathcal{V}}$ and used in Algorithm~\ref{alg:fingerprint_generation} (5 policies for each $\mathcal{V}$). We use the remaining 10 independent policies (having a different DRL algorithm from $\mathcal{V}$) trained for the same task to calculate $AA_{others}\,\text{(verification)}$. $AA_{orig}$ is much higher than the threshold value $0.5$, almost equal to $1.0$ in most cases. Furthermore, the average fooling rate of fingerprints is high, which proves that fingerprints successfully mislead $\pi_{\mathcal{A}}$. Both the average of $AA_{ind}\,\text{(verification)}$ and $AA_{others}\,\text{(verification)}$ are lower than $0.5$ in all cases, and the majority vote is always ```not stolen (independent)'' for any other $\pi_{i}$ that is not $\pi_{\mathcal{A}}$. Therefore, we conclude that \fingerprint satisfies the effectiveness and integrity requirements. 

As shown in Figure~\ref{fig:fooling_rate}, $AA_{ind}$ and $AA_{others}$ show different variances for three DRL algorithms. We found that one or two fingerprints seldom produce $AA \geq 0.5$ for $\pi_{\mathcal{V}}$ and $\pi_{i}$, although they behave differently in the same clean states. This reveals that a single fingerprint rarely represents the same weakness of two separate policies, and the number of fingerprints should be high enough to satisfy integrity considering this phenomenon. 


\begin{table*}[t]
\centering
\caption{Average impact, $AA$ and voting results (\cmark:Stolen, \xmark: Independent) for piracy policies that are 1) fine-tuned over a different number of episodes and 2) pruned and then fine-tuned over 200 episodes. $AA$ is averaged over 10 verification episodes, while impact is averaged over 10 test episodes. (\protect\greenline : Successful verification with $AA\geq 0.75$, \protect\blueline : Successful verification with $0.75 \geq AA \geq 0.50$, \protect\yellowline : Failed verification with high impact $\geq 0.4$, \protect\redline : Failed verification with low impact $ < 0.4$)}\label{tab:fine_tuning_pruning}
\begin{tblr}{colspec={ccccc|cccc},
cell{5}{3-7} = {green!40},
cell{5}{8} = {blue!30},
cell{5}{9} = {yellow!40},
cell{8}{3-6} = {green!40},
cell{8}{7} = {blue!30},
cell{8}{8-9} = {yellow!40},
cell{11}{3-6} = {green!40},
cell{11}{7-9} = {blue!30},
cell{11}{9} = {green!40},
cell{14}{3-4} = {green!40},
cell{14}{5} = {blue!30},
cell{14}{6} = {green!40},
cell{14}{7} = {blue!30},
cell{14}{8} = {green!40},
cell{14}{9} = {blue!30},
cell{14}{3} = {green!40},
cell{17}{3-9} = {yellow!40},
cell{20}{3-9} = {yellow!40},}
\SetCell[r=2]{m,5em}\bf Game, DRL method & \SetCell[r=2]{b}\bf Stats &
\SetCell[c=3]{c}\bf Fine-tuning, \# of episodes & & & \SetCell[c=4]{c}\bf Pruning and fine-tuning, pruning levels (\%) & & & &\\ 
&  & \bf 50 & \bf 100  & \bf 200 & \bf 25 & \bf 50 & \bf 75 & \bf 90 \\
\hline
\SetCell[r=3]{m,4em}\bf Pong, A2C& Impact & $0.0 \pm 0.0$  & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ 
& $0.0 \pm 0.0$  & $0.0 \pm 0.0$ & $0.0 \pm 0.0$  & $1.0 \pm 0.0$ \\
& $AA$ & $0.95 \pm 0.14$ & $0.95 \pm 0.14$& $0.94 \pm 0.10$ & $0.94 \pm 0.14$ & $0.91 \pm 0.25$& $0.67 \pm 0.42$ & $0.28 \pm 0.42$\\
& Votes & 10 \cmark / 0 \xmark  & 10 \cmark / 0 \xmark &  10 \cmark / 0 \xmark & 
10 \cmark / 0 \xmark & 9 \cmark / 1 \xmark & 6 \cmark / 4 \xmark  & 3 \cmark / 7 \xmark \\
\hline[dashed]
\SetCell[r=3]{m,4em} \bf Pong, DQN& Impact & $0.0 \pm 0.0$  & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ 
& $0.0 \pm 0.0$  & $0.8 \pm 0.0$ & $1.0 \pm 0.0$  & $1.0 \pm 0.0$ \\
& $AA$ &  $0.94 \pm 0.05$ & $0.89\pm 0.14$ &  $0.90 \pm 0.17$ & $0.88 \pm 0.16$ & $0.66 \pm 0.38$& 
$0.09 \pm 0.17$& $0.27 \pm 0.4$\\
& Votes & 10 \cmark / 0 \xmark & 10 \cmark / 0 \xmark& 9 \cmark /1 \xmark & 
10 \cmark / 0 \xmark & 7 \cmark / 3 \xmark & 1 \cmark / 9 \xmark &3 \cmark / 7 \xmark\\
\hline[dashed]
\SetCell[r=3]{m,4em}\bf Pong, PPO& Impact & $0.0 \pm 0.0$  & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ 
& $0.0 \pm 0.0$  & $0.0 \pm 0.0$ & $1.0 \pm 0.0$  & $1.0 \pm 0.0$ \\
& $AA$ & $0.88 \pm 0.23$ & $0.89\pm 0.25$ & $0.88\pm 0.30$ &  
$0.78 \pm 0.35$ &  $0.67 \pm 0.35$&  $0.65 \pm 0.41$ &  $0.71 \pm 0.39$ \\
& Votes & 9 \cmark /1 \xmark  &9 \cmark /1 \xmark  & 9 \cmark /1 \xmark  & 
7 \cmark / 3 \xmark &  7 \cmark / 3 \xmark  &  6 \cmark / 4 \xmark & 7 \cmark / 3 \xmark \\
\hline[dashed]
\SetCell[r=3]{m,5em}\bf MsPacman, A2C& Impact & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ & $0.0 \pm 0.0$
& $0.39\pm0.19$ & $0.03\pm0.10$& $0.30\pm0.15$ & $0.73\pm0.11$\\
& $AA$ & $0.82 \pm 0.16$ & $0.75 \pm 0.29$ & $0.62 \pm 0.35$ &  
$0.71 \pm 0.28$  & $0.65 \pm 0.39$ &  $0.72 \pm 0.26$ & $0.59 \pm 0.23$  \\
& Votes & 9 \cmark /1 \xmark  & 8 \cmark /2 \xmark & 6 \cmark /4 \xmark & 
6 \cmark /4 \xmark & 7 \cmark /3 \xmark &  8 \cmark /2 \xmark &  6 \cmark /4 \xmark  \\
\hline[dashed]
\SetCell[r=3]{m,5em}\bf MsPacman, DQN& Impact & $0.79 \pm 0.11$ & $0.83 \pm 0.02$ & $0.87 \pm 0.03$
&  $0.79 \pm 0.11$ &  $0.74 \pm 0.09$ &  $0.86 \pm 0.01$ &  $0.71 \pm 0.43$ \\
& $AA$ & $0.23 \pm 0.34$ & $0.15 \pm 0.28$& $0.16 \pm 0.31$ & $0.38 \pm 0.44$ 
& $0.00 \pm 0.01$  & $0.59 \pm 0.46$ & $0.42 \pm 0.42$ \\
& Votes & 2 \cmark /8 \xmark & 1 \cmark /9 \xmark& 2 \cmark /8 \xmark & 4 \cmark /6 \xmark
& 0 \cmark /10 \xmark& 6 \cmark /4 \xmark & 4 \cmark /6 \xmark\\
\hline[dashed]
\SetCell[r=3]{m,5em} \bf MsPacman, PPO& Impact & $0.85 \pm 0.11$ & $0.40 \pm 0.26$ & $0.51 \pm 0.08$
& $0.52 \pm 0.15$ & $0.57 \pm 0.04$ &  $0.62 \pm 0.05$ & $0.66 \pm 0.19$\\
& $AA$ & $0.43 \pm 0.36$ & $0.11 \pm 0.16$ &  $0.25 \pm 0.32$& 
$0.26 \pm 0.36$ & $0.33 \pm 0.38$ &  $0.31 \pm 0.32$ & $0.13\pm 0.20$ \\
& Votes & 4 \cmark /6 \xmark & 0 \cmark /10 \xmark&  3 \cmark /7 \xmark& 
3 \cmark / 7 \xmark& 3 \cmark / 7 \xmark&  4 \cmark / 6 \xmark& 1 \cmark / 9 \xmark \\
\hline 
\end{tblr}
\end{table*}

\textit{Utility} : As stated in Section~\ref{ssec:adversarymodel}, we do not consider utility a requirement for \fingerprint. However, we measure the \emph{impact}~\cite{korkmaz2022deep} of the verification on the victim agent to ensure that it does not fail the task during verification. We measure the impact as 
\begin{equation}\label{eqn:impact}
    Impact = \frac{Return_{\mathcal{V}(test)} - Return_{\mathcal{V}(verification)}}{Return_{\mathcal{V}(test)} - Return_{\mathcal{V}_{min}(test)}},
\end{equation}
where $Return_{\mathcal{V}(test)}$ and $Return_{\mathcal{V}(verification)}$ are the average return of \victim in an episode initialized with the same start state (and with the same environment dynamics) with or without the verification. $Return_{\mathcal{V}_{min}(test)}$ is the return of $\mathcal{V}$ if it chooses the worst possible actions for each state in the same episode. The results presented in Appendix~\ref{ssec:appendixA4} show that the average impact on agent performance is $0.02$ and $0.22$ in MsPacman and Pong, respectively. We also found that the return never drops to $Return_{\mathcal{V}_{min}(test)}$ during verification. Thus, we conclude that agents continue their task without a significant impact after the verification phase ends.

\subsection{Robustness}\label{ssec:robustness}

\subsubsection{Robustness Against Model Modification Attacks.} Adversary \adversary could modify the stolen model $\pi_{\mathcal{A}}$ by carefully retraining it to preserve agent performance while trying to suppress evidence used for verification. We consider two common types of model modification attacks, fine-tuning~\cite{sharif2014cnn} and weight pruning~\cite{han2015learning}, where \adversary is aware of the existence of an ownership verification technique but does not know the type of it. We implemented fine-tuning by retraining $\pi_{\mathcal{A}}$ in an additional 200 episodes and decreasing the learning rate by $100$ to maintain agent performance. For pruning, we first performed global pruning, i.e., removed a percentage of the lowest connections across the DNN model. After pruning, we fine-tuned the pruned model over 200 episodes. 

To evaluate the robustness requirement, we computed the majority vote and $AA$ values of the stolen $\pi_{\mathcal{A}}$ and modified policies $\pi_{\mathcal{A}^*}$ on the verification episodes. We also measured the impact of the modification on model utility (agent performance) by changing Equation~\ref{eqn:impact} to:
\begin{equation}\label{eqn:impact2}
    Impact = \frac{Return_{\mathcal{A}(test)} - Return_{\mathcal{A}^*(test)}}{Return_{\mathcal{A}(test)} - Return_{A_{min}(test)}},
\end{equation}
where $Return_{\mathcal{A}^*}$ is the return of stolen and modified policy, and $Return_{\mathcal{A}}$ denotes the return of stolen (unmodified) policy over the same test episodes. Based on the results on the impact of verification on utility, we generously set the maximum allowable impact as $0.4$ for modification attacks, indicating that $Return_{\mathcal{A}^*(test)}$ would fall a little more than halfway between $Return_{\mathcal{A}(test)}$ and $Return_{\mathcal{A}_{min}(test)}$.

%where $Return_{\mathcal{V}}$ - $Return_{\mathcal{A}}$ are the average returns of \victim and modified \adversary in 10 test episodes (each episode is initialized with the same start state for both \victim and \adversary), and $Return_{min}$ is the return of the agents if they chose the worst possible actions for each state. 

% Figure environment removed

Table~\ref{tab:fine_tuning_pruning} shows the robustness evaluation of \fingerprint against model modification attacks. As shown in the table, \fingerprint successfully verifies fine-tuned Pong agents with high $AA$ values. \fingerprint usually results in a failed verification of fine-tuned MsPacman agents. However, the impact of modification is exceptionally high for these cases. A similar conclusion can be drawn from the pruning results. An increase in the pruning level negatively affects the verification by decreasing its $AA$ values. However, the impact of pruning is too high in three cases in Pong and most of the cases in MsPacman, despite failed verification. Based on our robustness definition in Section~\ref{ssec:adversarymodel}, we conclude that \fingerprint is robust against model modification attacks.    


\subsubsection{Robustness Against Evasion Attacks and Well-informed Adversaries.}\label{ssec:robustnessevasion} \adversary can evade verification by discovering individual inputs used for verification or adapt the agent's behavior to avoid a successful verification. For evasion, \adversary should have more information about the ownership verification procedure. In our setup, \adversary knows that the ownership verification is done via \fingerprint but is unaware of the exact adversarial mask used during verification. Based on this information, the simplest evasion attack is performing sub-optimal actions with a pre-defined random action ratio on each episode. Figure~\ref{fig:evasion_results} confirms that the increase in the random action ratio causes a decrease in agent performance (lower return) despite successful evasion. Therefore, \fingerprint is robust against evasion via sub-optimal action return.

Evasion attacks can combine detecting adversarial examples (i.e. fingerprints used for verification) and then performing either suboptimal actions or restoring original actions. We employ Visual Foresight (VF)~\cite{lin2017detecting} to carry out this attack. VF predicts the next states and the associated probability distribution of actions by looking at a history of previous states and observed actions. If the distance between the predicted and current action distribution is large, VF detects that state as adversarial, and %which might lead to the worst possible actions when the input state is adversarial, VF 
performs the predicted action instead of the current one as a recovery mechanism. Figure~\ref{fig:evasion_results} shows that the use of VF does not affect the agent performance. However, the high values of $AA$ shown in the figure justifies that VF cannot recover agent performance when states are perturbed with non-transferable, universal adversarial masks and fail to evade verification. This is because the collected history of previous states consists of adversarial inputs, which might lead to the original (incorrect) action even if the adversarial state is detected correctly~\cite{tekgul2022real}. For this reason, we also evaluated the case where VF chooses a suboptimal action (VF + suboptimal action) instead of the one predicted during recovery. Figure~\ref{fig:evasion_results} shows that it decreases $AA$ more than VF, but $AA$ is not too low to evade verification and change the final verdict.     

Finally, we evaluated \fingerprint against the most well-informed adversaries that can improve the robustness of the agent against $l_{\infty}$ norm adversarial perturbations by adversarial training. For adversarial training, we implemented one of the recent state-of-the-art methods, RADIAL-RL~\cite{oikarinen2021robust}. We choose to implement RADIAL-RL for DQN agents, because these are shared cases between our and the authors' experiments. RADIAL-DQN (RADIAL-RL designed for DQN) first obtains a policy without adversarial training and then fine-tunes the policy by incorporating an adversarial loss term into the loss function that is minimized during training. In our setting, \adversary performs RADIAL-DQN by skipping the first step and fine-tunes the stolen policy $\pi_{\mathcal{A}}$ using adversarial loss. We adopted the open source repository of the authors~\footnote{\url{https://github.com/tuomaso/radial_rl_v2}} in our framework, did not change the hyper-parameters used in RADIAL-DQN, and saved both agents with the best performance and the final agent after RADIAL-DQN was completed. 

The first two rows of Table~\ref{tab:radial-rl} summarize the impact, $AA$ values, and the votes for agents modified through RADIAL-DQN. The results indicate that \adversary can evade verification by making $\pi_{\mathcal{A}}$ more robust to adversarial states in Pong. \adversary obtains an improved policy for MsPacman (3rd column, negative impact: higher reward), but cannot evade verification. This outcome is not surprising, as DNN fingerprinting has limitations against adaptive adversaries that perform adversarial training~\cite{lukas2021deep}. Then, we considered an alternative scenario where \victim fine-tunes its policy with RADIAL-DQN, saves the best agent, and generates fingerprints for this agent (RDQN). The last two rows of Table~\ref{tab:radial-rl} show the verification results when \adversary implements RADIAL-DQN against adversarially robust victim agents. In this case, \adversary cannot evade verification without affecting the agent's performance. Therefore, although \fingerprint is limited against adversarial training, it satisfies the robustness requirement when fingerprinting adversarially robust victim agents.  

\begin{table}[t]
\centering
\caption{Average impact, $AA$ and voting results for stolen policies modified by RADIAL-DQN. Results are reported for both the agent with a best performance during RADIAL-DQN (3rd column) and the final agent obtained after RADIAL-DQN finishes (4th column). $AA$ is averaged on 10 verification episodes and impact is averaged over 10 test episodes. (*: improved policy, \protect\greenline : Successful verification with $AA\geq 0.75$, \protect\blueline : Successful verification with $0.75 \geq AA \geq 0.50$, \protect\yellowline : Failed verification with high impact $\geq 0.4$, \protect\redline : Failed verification with low impact $ < 0.4$)}\label{tab:radial-rl}
\begin{tblr}{colspec={cccc},
cell{5}{3-4} = {red!40},
cell{8}{3} = {blue!30},
cell{8}{4} = {red!40},
cell{11}{3-4} = {green!40},
cell{14}{3} = {blue!30},
cell{14}{4} = {yellow!40}
}
\SetCell[r=2]{m,5em}\bf Game, DRL method & \SetCell[r=2]{m}\bf Stats & \SetCell[r=2]{m}\bf Best Agent&  \SetCell[r=2]{m}\bf Final Agent \\
 & &  & \\
\hline
\SetCell[r=3]{m,5em}\bf Pong, RADIAL-DQN & Impact & $0.0 \pm  0.0$ & $0.0 \pm  0.0$\\
                                        & $AA$ & $0.04 \pm 0.06$ & $0.04 \pm 0.06$\\ 
                                        & Votes &  0 \cmark / 10 \xmark &  0 \cmark / 10 \xmark\\
\hline[dashed]
\SetCell[r=3]{m,5em}\bf MsPacman, RADIAL-DQN & Impact & $-0.16 \pm  0.03^{*}$  & $ 0.39 \pm 0.03$\\
                                        & $AA$ &  $0.59 \pm 0.40$ &  $0.29 \pm 0.31$\\ 
                                        & Votes & 6 \cmark / 4 \xmark & 4 \cmark / 6 \xmark\\
\hline[dashed]
\SetCell[r=3]{m,5em}\bf Pong, RADIAL-RDQN & Impact & $0.0 \pm 0.0$ & $0.0 \pm  0.0$\\
                                        & $AA$ &$0.84 \pm 0.21$ & $0.89 \pm 0.17$\\ 
                                        & Votes & 8 \cmark / 2 \xmark & 9 \cmark / 1 \xmark  \\
\hline[dashed]
\SetCell[r=3]{m,5em}\bf MsPacman, RADIAL-RDQN & Impact & $0.15 \pm 0.04$ & $0.55 \pm 0.06$\\
                                        & $AA$ & $0.61 \pm 0.34$& $0.09 \pm 0.18$\\ 
                                        & Votes & 7 \cmark / 3 \xmark & 1 \cmark / 9 \xmark \\
\hline
\end{tblr}
\end{table}

\begin{table*}[t]
\centering
\caption{$AA$ values (averaged over 10 verification episodes) and voting results for false claims against victim \victim, and independent $\mathcal{I}$ policies with different perturbation constraint $\epsilon$ values. (The cases where a false claim succeeds are shown as follows: \protect\greenline : False claim with $AA\geq 0.75$, \protect\blueline : False claim with $0.75 \geq AA \geq 0.50$)}\label{tab:moc}
\begin{tblr}{colspec={cccccc},
cell{7}{3-4} = {blue!30},
cell{7}{5-6} = {green!40},
cell{8}{3-6} = {blue!30},
cell{14}{4-5}= {blue!30},
cell{14}{6}  = {green!40},
}
 &  & \SetCell[c=4]{c}\bf $\bm{\epsilon}$ vs. $\bm{AA}$ (Votes) & & &\\
\cline{3-6}
\SetCell[c=2]{c} \bf Game, DRL method & & \bf 0.05 & \bf 0.1 & \bf 0.2 & \bf 0.5 \\
\hline
\bf Pong, & \victim & $0.45\pm 0.47$ (5 \cmark / 5 \xmark)& $0.49\pm 0.49$ (5 \cmark / 5 \xmark) & 
$0.40\pm 0.49$ (4 \cmark / 6 \xmark)& $0.40\pm 0.49$ (4 \cmark / 6 \xmark)\\
\bf A2C & $\bm{\mathcal{I}}$, avg. & $0.32 \pm 0.36$ (3 \cmark / 7 \xmark)& $0.38\pm 0.45$ (3 \cmark / 7 \xmark) & $0.30 \pm 0.41$ (3 \cmark / 7 \xmark) & $0.28 \pm 0.43$ (3 \cmark / 7 \xmark)\\ 
\hline[dashed]
\bf Pong, & \victim & $0.37 \pm 0.42$ (4 \cmark / 6 \xmark)& $0.37 \pm 0.45$ (3 \cmark / 7 \xmark) & 
$0.33 \pm 0.45$ (3 \cmark / 7 \xmark)  & $0.40 \pm 0.49$ (4 \cmark / 6 \xmark)  \\
\bf DQN &  $\bm{\mathcal{I}}$, avg.& $0.01 \pm 0.18$ (1 \cmark / 9 \xmark)& $0.07 \pm 0.22$ (1 \cmark / 9 \xmark)& $0.05 \pm 0.19$ (1 \cmark / 9 \xmark) & $0.05 \pm 0.19$ (1 \cmark / 9 \xmark)  \\ 
\hline[dashed]
\bf Pong, & \victim &  $0.56 \pm 0.39$ (5 \cmark / 5 \xmark)& $0.68 \pm 0.42$ (7 \cmark / 3 \xmark)& $0.76 \pm 0.38$ (8 \cmark / 2 \xmark) & $0.78 \pm 0.39$ (8 \cmark / 2 \xmark)  \\
\bf PPO &  $\bm{\mathcal{I}}$, avg. & $0.56 \pm 0.36$ (6 \cmark / 4 \xmark)& $0.59 \pm 0.38$ (6 \cmark / 4 \xmark)& $0.59 \pm 0.38$ (6 \cmark / 4 \xmark) & $0.52 \pm 0.41$ (6 \cmark / 4 \xmark) \\ 
\hline[dashed]
\bf MsPacman, & \victim & $0.00\pm 0.00$ (0 \cmark /10 \xmark) & $0.03\pm 0.05$ (0 \cmark /10 \xmark) &  $0.14\pm 0.29$ (1 \cmark /9 \xmark)  & $0.09\pm 0.22$ (1 \cmark /9 \xmark)  \\
\bf A2C &  $\bm{\mathcal{I}}$, avg.& $0.15\pm 0.56$  (1 \cmark /9 \xmark)  & $0.14\pm 0.21$ (1 \cmark /9 \xmark)  & $0.13\pm 0.30$ (2 \cmark /8 \xmark) & $0.21\pm 0.36$ (2 \cmark /8 \xmark) \\ 
\hline[dashed]
\bf MsPacman, & \victim & $0.23\pm 0.36$ (2 \cmark /8 \xmark)& $0.0\pm 0.0$ (0 \cmark /10 \xmark) & $0.0\pm 0.0$ (0 \cmark /10 \xmark) & $0.0\pm 0.0$ (0 \cmark /10 \xmark) \\
\bf DQN &  $\bm{\mathcal{I}}$, avg.& $0.26\pm 0.24$ (2\cmark /8 \xmark)  & $0.19\pm 0.26$ (2\cmark /8 \xmark)& $0.15 \pm 0.29$ (1\cmark /9\xmark) & $0.24 \pm 0.26$ (3\cmark /7\xmark)\\ 
\hline[dashed]
\bf MsPacman, & \victim & $0.19 \pm 0.18$ (1 \cmark / 9 \xmark) & $0.26 \pm 0.31$ (3 \cmark / 7 \xmark) & $0.38 \pm 0.37$ (4 \cmark / 6 \xmark) & $0.07 \pm 0.21$ (1 \cmark / 9 \xmark)  \\
\bf PPO &  $\bm{\mathcal{I}}$, avg.& $0.10\pm 0.11$  (0 \cmark /10 \xmark) & $0.50\pm 0.39$ (5 \cmark /5 \xmark) & $0.74\pm 0.40$ (8 \cmark /2 \xmark) & $0.80\pm 0.20$ (8 \cmark /2 \xmark)\\ 
\hline[dashed]
\end{tblr}
\end{table*}
     
\subsubsection{Robustness Against False Claims.}\label{ssec:robustnessfalse} %Liu et al.~\cite{liu2023false} raise an overlooked robustness concern for ownership verification methods: A malicious accuser can falsely claim the ownership of an independently trained model (victim model) using its own model. 
Liu et al.~\cite{liu2023false} show that malicious accusers can produce fake fingerprints that can pass the ownership verification test against independent models in many ownership verification schemes, including CAE~\cite{lukas2021deep}. Therefore, we also evaluated the robustness of \fingerprint against malicious accusers by generating fingerprints for the accuser's policy without maximizing the loss for independent policies (Equation~\ref{eq:lossfunction}) and not measuring the non-transferability score (Algorithm~\ref{alg:fingerprint_generation}, line 6), which is similar to the setup proposed in~\cite{liu2023false} to evaluate CAE. We selected one of the five independent policies $\pi_{i}, i\in \mathcal{I}$ that behaves the closest to $\pi_{\mathcal{V}}$ in the test episodes and has the same DRL algorithm as the accuser policy and other independent policies as \verifier's control set. As shown in Table~\ref{tab:moc}, the malicious accuser cannot falsely claim ownership of \victim for the perturbation constraint set used in \fingerprint ($\epsilon=0.05$), except the PPO agent trained for Pong. If the perturbation constraint becomes larger ($\epsilon \geq 0.1$), then the accuser's false fingerprints transfer to other models in those cases. Having \verifier perform an additional check that the size of the adversarial mask does not exceed a prescribed bound can mitigate against false claims attacks for \fingerprint. Table~\ref{tab:moc} also indicates that adversarial states have a higher transferability rate between PPO algorithms compared to others. In these cases, \verifier can train or search for other independent PPO policies for the same task as suggested in~\cite{liu2023false}, and it can reject the claim if the accuser's fingerprints falsely verify all independent models. Therefore, we conclude that \fingerprint is not susceptible to false claims with a simple additional countermeasure on $\epsilon$ and non-transferability check based on the DRL algorithm. 

\textit{Model extraction attacks in DRL}: In this work, we limit the scope to the adversary model described in Section~\ref{ssec:adversarymodel} and do not consider model extraction attacks against DRL policies through imitation learning~\cite{chen2021stealing}. Nevertheless, we tried to implement the model extraction attack proposed by Chen et al.~\cite{chen2021stealing}, but were unable to obtain good stolen policies, which could be due to the simpler tasks chosen in the setup of the original work. Chen et al.~\cite{chen2021stealing} experimentally show that adversarial examples can successfully transfer from stolen policies to the victim policy if they share the same DRL algorithm. Their preliminary results provide insight into the possibility of preserving fingerprints during the extraction of the DRL model. Thus, we leave the construction of effective DRL model extraction attacks and evaluate the robustness of \fingerprint against these attacks for future work.


