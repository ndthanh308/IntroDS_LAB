%auto-ignore

\section{Appendix}
\label{sec:appendixA}
\subsection{Training DRL Agents}
\label{ssec:appendixA1}

\begin{table*}
\centering
\caption{Return (averaged over 10 test episodes) of the victim and independent policies trained for MsPacman. The best and worst agents for the same DRL algorithm are highlighted in green and red, respectively.}\label{tab:mspacman}
\begin{tabular}{c|c|ccccc}
\multicolumn{1}{c}{\bf DRL Method} & \multicolumn{1}{c}{\bf Victim Agent}  & \multicolumn{5}{c}{\bf Independent Agents} \\
\hline
A2C & \textcolor{green}{$3316.00 \pm  512.72$} & \textcolor{red}{$1670.00 \pm 537.27$} & $2552.00 \pm 595.66$ & $2144.00 \pm 816.58$ & $2246.00 \pm 4.90$ &  $1750.00 \pm 72.66$ 
\\
DQN & \textcolor{green}{$2620.00 \pm 80.62$} & $2363.00 \pm 269.26$ & $2484.00 \pm 389.67$ & $2218.00 \pm 347.84$ & \textcolor{red}{$2211.00 \pm 154.24$} & $2472.00 \pm 412.74$ 
\\
PPO & \textcolor{green}{$2731.00 \pm 545.50$} & $ 2019.00 \pm 77.13$ &  $2198.00\pm536.35$ & $2040.00 \pm 161.43$ & \textcolor{red}{$2017.00 \pm 397.57$} & $2167.00 \pm 268.52$ 
\\
\hline
\end{tabular}
\end{table*}

To facilitate the comparison, we used the same setup to implement all DRL policies and attacks: PyTorch (version 1.4.0), NumPy (version 1.18.1), Gym (a toolkit for developing reinforcement learning algorithms, version 0.15.7) and Atari-Py (a Python interface for the Arcade Learning Environment,
version 0.2.6). All experiments were carried out on a computer with 2x12 core Intel(R) Xeon(R) CPUs (32GB RAM) and NVIDIA Quadro P5000 with 16GB memory. To train DQN agents, we used a dueling Q-network architecture proposed in~\cite{wang2016dueling}. For training A2C and PPO agents, we choosed to implement the convolutional neural networks suggested in OpenAI Baselines\footnote{\url{https://github.com/openai/baselines}}. In both A2C and PPO, actors and critics use the same architecture, except for the penultimate later. The hyperparameter values of each victim agent are set the same as OpenAI baselines, while slightly differ for training independent agents.

All victim and independent DRL agents trained to play Pong reach the highest score +21. The summary of agents trained to play MsPacman is presented in Table~\ref{tab:mspacman}. In MsPacman, we deliberately chose agents with the best performance as the victim, since they have a clear business advantage over other models, thus incentivizing adversaries to apply piracy attacks against them.

\begin{table}
\centering
\caption{Hyperparameters used in fingerprint generation}\label{tab:hyperparameters}
\begin{tblr}{ccc}
\bf Parameter & \bf Value  & \bf Definition \\
\hline
$\epsilon$ & $0.05$ & $l_{\infty}$ constraint on the perturbation $\vect{r}$ \\
$\tau_{nts}$ & $0.7$& minimum non-transferability score of $\vect{r}$\\
$\tau_{\delta}$ & $0.8$& minimum fooling rate of $\vect{r}$ on a dataset\\
\SetCell[r=2]{m} $n_{\text{episodes}}$ & \SetCell[r=2]{m} $1000$ & maximum number of training episodes \\
& & to generate/collect fingerprints \\
%$n_{\text{FRL}}$ & $10$& maximum number of fingerprints\\
\hline
\end{tblr}
\end{table}

\subsection{Hyperparameter Selection in \fingerprint}
\label{ssec:appendixA3}

The perturbation constraint $\epsilon$ directly affects the trade-off between the success of an adversarial example and its non-transferability. Therefore, we performed a grid search for $\epsilon$ and set it to an optimal value $0.05$. We set the minimum fooling rate $\tau_{\delta}$ at a high value $0.8$ to ensure the universality of the adversarial mask and set the non-transferability score at $0.7$. Based on these values and Equation~\ref{eq:nts}, for a candidate universal adversarial mask $\vect{r}$, the minimum action agreement of independent agents $ min_{i \in \mathcal{I}}(AA(\pi_{\mathcal{V}}, \pi_{i}, \vect{s}, \vect{r}))$ should be lower than $0.125$ to be chosen as a valid fingerprint. For both the Pong and MsPacman agents, we used a reduced set of actions (4 discrete actions in total). The minimum $ min_{i \in \mathcal{I}}(AA(\pi_{\mathcal{V}}, \pi_{i}, \vect{s}, \vect{r})) = 0.125$ is much lower than $0.25$ ($AA$, if actions are randomly chosen) and satisfies the non-transferability requirement. Finally, we set $n_{\text{episodes}}$ at a high value to guarantee that a sufficient number of fingerprints are generated for an efficient verification. The prescribed hyperparameter values during fingerprint generation are listed in Table~\ref{tab:hyperparameters}.  

% Figure environment removed

% Figure environment removed

\subsubsection{Selection of the Number of Fingerprints}

The number of fingerprints generated and used for verification affects integrity and robustness. An insufficient number of fingerprints could result in a high action agreement $AA$ between the independent and victim (original) policies and ultimately falsely verify the ownership of the independent policies as explained in Section~\ref{ssec:reliability}. On the contrary, a high number of fingerprints could give low $AA$ between the victim policy and its modified versions, since some of the fingerprints could have a lower fooling rate on the modified policies. For that, we performed verification by changing the maximum number of fingerprints used for fingerprint generation. As demonstrated in Figure~\ref{fig:fingerprints_onPong}, the number of fingerprints does not affect the return during verification, but a sufficient number of fingerprints (around 5) are needed to achieve high $AA$ to provide high confidence for the final decision. We set the number of fingerprints at $10$ to satisfy the effectiveness and robustness requirements simultaneously. 

\subsubsection{Selection of the Window Size.} In addition to the number of fingerprints, the decision on window size is important. If the window size is large, then the return during verification decreases and the agent can perform poorly. Figure~\ref{fig:windowsize_onPong} illustrates the effect of window size on return and $AA$ during verification for Pong DQN agents. Although $AA$ does not change significantly with larger window sizes, there is a steady decline in return. Based on this result, the window size can be set to $40$ or even less, but we set it to $40$ after performing the same analysis for all agents and observing the change in return.   

\subsection{Impact of Verification}
\label{ssec:appendixA4}

As discussed in Section~\ref{ssec:adversarymodel}, utility is not a necessary requirement in fingerprinting methods, as fingerprints typically trigger abnormal behavior. However, the impact on agent performance is still important, since verification might also be carried out in a stealthy way to avoid raising any suspicion. Moreover, if the agent fails to perform the task quickly during verification, then the collected information may not be sufficient to correctly calculate the action agreement $AA$. Therefore, we computed the impact of verification on agent performance and summarized the results in Table~\ref{tab:vimpact}. In Pong, the impact is almost zero, while we experienced an average impact of $0.22$ in MsPacman agents due to the high complexity of the game. The impact in MsPacman can be further improved by adding fingerprints in non-critical states that do not affect the return if the agent replaces one action with another.

\begin{table}
\centering
\caption{Impact of verification (averaged over 10 verification episodes) on victim agent performance.}\label{tab:vimpact}
\begin{tabular}{c|c|cc}
\multicolumn{1}{c}{\bf DRL Method} & \multicolumn{1}{c}{\bf Pong}  & \multicolumn{1}{c}{\bf MsPacman} \\
\hline
A2C & $0.02 \pm 0.02$ & $0.20 \pm 0.21$ \\
DQN & $0.01 \pm 0.02$ & $0.28 \pm 0.19$ \\
PPO & $0.02 \pm 0.02$ & $0.18 \pm 0.28$ \\
\hline
\end{tabular}
\end{table}