%auto-ignore
\section{Transferability of Universal Masks}
\label{sec:Discussion}

The fingerprint generation process in \fingerprint is based on maximum-confidence adversarial example generation techniques and it is similar to Fast Gradient Sign Method (FGSM)~\cite{huang2017adversarial}, since \fingerprint averages the gradient of Equation~\ref{eq:lossfunction} w.r.t. randomly selected states. As presented in Section~\ref{ssec:adversarial}, maximum-confidence adversarial examples have a higher transferability rate than minimum-confidence examples. FGSM is a maximum-confidence method itself; however, during the computation of universal, non-transferable adversarial masks, the effect of the high-sensitivity directions obtained from the most confident adversarial examples is diminished by others. Nevertheless, we analyzed whether minimum-confidence adversarial masks in DNNs can be useful for DRL fingerprinting. %have an effect on the degree of effectiveness, integrity, and robustness. 
For that reason, we changed the universal mask generation $\vect{r}$, (Algorithm~\ref{alg:fingerprint_generation}, line 4) with the universal adversarial perturbation (UAP)~\cite{moosavi2017universal} by implementing the method proposed for DRL settings~\cite{tekgul2022real}.

We found that \fingerprint with UAP satisfies the effectiveness and integrity requirements for all agents, except the DQN agent trained for MsPacman. It was impossible to obtain an adversarial example with the perturbation constraint used in \fingerprint ($\epsilon = 0.05$) against this agent, but increasing it leads to transferable adversarial examples and false positives. The real issue with UAP emerges when the adversary \adversary modifies the stolen policy $\pi_{\mathcal{A}}$ with model modification attacks. Due to its minimum-distance property, UAP finds the smallest high-sensitivity directions belonging to the closest incorrect class (or discrete actions in DRL), and generally the resulting $\vect{r}$ is smaller than $\epsilon$. Therefore, a small change in $\pi_{\mathcal{A}}$ negatively affects the robustness of UAP. Contrary to UAP, \fingerprint shifts the source sample using the maximum amount of perturbation $\epsilon$, forces $\pi_{\mathcal{A}}$ to perform the same incorrect action and is more robust against model modification attacks. We illustrate this problem in Figure~\ref{fig:discussion}.   

% Figure environment removed

One of the main reasons why \fingerprint has better robustness stems from the fact that the input space embeddings in DRL are not as separable as in DNN~\cite{annasamy2019towards}. In DRL, although the input states are spatially similar, they often result in different actions. %This behavior differs from DNNs that are trained to solve classification problems by exploiting local generalization. 
DRL agents optimize policies using both input state and environment dynamics and act upon spatio-temporal abstractions~\cite{zahavy2016graying}. \fingerprint identifies discontinuities in the optimal policy and computes an adversarial state that is spatially similar to the source state but far from it in temporal dimension. %and require a large number of time steps to reach each other. 
UAP typically explores adversarial pockets that are closer in the spatial domain due to its minimum-distance strategy. Therefore, it cannot withstand model modification attacks that preserve the spatio-temporal abstractions and slightly change the sequential strategy. 

\begin{table*}[t]
\centering
\caption{Comparison of UAP and \fingerprint based on fooling rate (measured for the victim policy) and action agreement $AA$. Both the fooling rate and $AA$ are averaged using adversarial states (fingerprints) in 10 verification episodes. The higher fooling rate and $AA$ values are highlighted in green. Matched actions: Cases where victim and modified policies perform the same action for the same state. Different actions: Cases where victim and modified policies perform different actions for the same state.}\label{tab:uap}
\begin{tabular}{c |cccc |cccc}
\multicolumn{1}{c}{\bf Game, DRL Method} & \multicolumn{4}{c}{\bf UAP} & \multicolumn{4}{c}{\bf \fingerprint} \\
\hline
\bf (Fine-tuned & \multicolumn{2}{c}{\bf Matched actions} &  \multicolumn{2}{c|}{\bf Different actions} & \multicolumn{2}{c}{\bf Matched actions} &  \multicolumn{2}{c}{\bf Different actions} \\
\bf over 200 eps.) & Fooling rate &  $AA$ & Fooling rate &  $AA$ & Fooling rate &  $AA$ & Fooling rate &  $AA$ \\
\hline
\bf Pong, A2C  & $0.79 \pm 0.09 $& $0.78 \pm 0.08$&  $\cellcolor{green!40}{0.89 \pm 0.07}$& $0.69 \pm 0.13$& $\cellcolor{green!40}{0.95 \pm 0.10}$& $\cellcolor{green!40}{0.94 \pm 0.09}$ & $0.85\pm 0.12$& $\cellcolor{green!40}{0.92 \pm 0.16}$\\
\bf Pong, DQN  & $0.69 \pm 0.11 $& $0.12 \pm 0.10$& $0.55\pm 0.18$& $0.24 \pm 0.14$& 
$\cellcolor{green!40}{0.89 \pm 0.13}$& $\cellcolor{green!40}{0.92 \pm 0.18}$& $\cellcolor{green!40}{0.93 \pm 0.07}$& $\cellcolor{green!40}{0.94\pm 0.09}$\\
\bf Pong, PPO  & $0.86 \pm 0.05$& $0.40 \pm 0.21$& $0.82 \pm 0.14$& $0.41 \pm 0.13$& 
 $\cellcolor{green!40}{0.91\pm 0.03}$& $\cellcolor{green!40}{0.98 \pm 0.08}$ & $\cellcolor{green!40}{0.90 \pm 0.07}$ & $\cellcolor{green!40}{0.87 \pm 0.3}$\\
\bf MsPacman, A2C  &  $\cellcolor{green!40}{0.76 \pm 0.32}$ & $0.53 \pm 0.42 $& $\cellcolor{green!40}{0.91 \pm 0.13}$& $\cellcolor{green!40}{0.58 \pm 0.42}$& $0.68 \pm 0.36$& $\cellcolor{green!40}{0.64 \pm 0.36}$& $0.64 \pm 0.42$ &$0.55 \pm 0.43$\\
\hline
\end{tabular}
\end{table*}

We provide experimental results for our discussion in Table~\ref{tab:uap}. This table compares the fooling rate and action agreement $AA$ for adversarial states used in the verification of fine-tuned policies. We chose to report the results for fine-tuned policies from Table~\ref{tab:fine_tuning_pruning} considering the acceptable impact range $(< 0.4)$ on the modified agent's performance. Matched actions refers to situations where both victim and modified policies perform the same action for the same input state without any added fingerprints. In contrast, different actions refer to cases where victim and modified policies behave differently for the same input state. Table~\ref{tab:uap} shows that the fooling rate of UAP is lower than \fingerprint in almost all cases. This supports our first claim regarding the robustness of UAP and \fingerprint. The columns labeled with $AA$ show the action agreement where the fingerprint successfully misleads the victim policy. In this case, the ideal $AA$ value for matched actions would be $1.0$. As can be seen from Table~\ref{tab:uap}, \fingerprint reaches much higher $AA$ values for matched actions than UAP. Surprisingly, \fingerprint attains higher $AA$ values for different actions as well. This shows that, even if the fine-tuned policy successfully changes the agent's behavior, the adversarial states generated by \fingerprint force the policy to perform the same incorrect action. The same conclusion cannot be drawn from the UAP results, as the $AA$ values reported for the same actions are lower than \fingerprint even in the case with the higher fooling rate.

Based on this discussion, we conjecture that using minimum-distance adversarial examples to fingerprint DRL agents requires either adding modified policies to the loss function in Equation~\ref{eq:lossfunction}, or considering the temporal structure of the policy while finding the high-sensitivity directions. The latter option also opens a new space of adversarial examples that can exploit temporal abstractions learned by DRL policies.  

%Correlation between states generates local gradients leads similar local gradient directions for the same action. The average of gradients would give the direction that pushes a big fraction of input states out of the decision boundary. Because of the averaging, the effect of the gradient coming from the most confident input state would diminish, and maximum-confidence methods will transform into minimum-distance adversarial example generation method.
%One can also use minimum-distance methods such as DeepFool(cite) or even UAP(cite) can be used to generate universal masks. However, we have found that with UAP, we reach a higher action agreement with independently trained policies, so we chose OSFW as a suitable method. 

%he effectiveness of UAP is directly dependent on the source policy's randomization during training and the source policy's actions in the training set used to generate UAP.This constrains the transferability of UAP across policies. As a result, two agents that have two independently trained policies would act differently in an episode where UAP are applied because the effectiveness of UAP differs between policies.

% Explain high and low positives
%From our preliminary experiments using Pong games, we found that in a normal task with no adversarial perturbation attacks, agents with A2C and PPO policies have low \saa (around $0.2$) between agents that have the same policies.
%When we generate \osfwu against a source policy and apply it to an episode, the \saa between agents with the source policy and its copies can go up to as high as $0.9$.
%Following this observation, high \saa between two agents in an episode where \osfwu is applied is an indication that the two agents have the same policies. 

%We also found that compared to \uaps and \uapo, \osfwu is the most effective in leading agents with independently trained policies to have low \saa (as low as $0.1$ in some cases) between the agents. 
%However, there are some instances where applying \osfwu in an episode leads to a high \saa (higher than $0.8$) between two agents that follow two independently trained policies.
%To minimize false positives, we generate multiple adversarial perturbations using \osfwu and apply each of them to different episodes of a task.
%A policy is only identified as a copy of the source policy if the \saa is high for all episodes.
%We use \osfwu adversarial perturbations to reliably identify copies of the source policy, and we use multiple \osfwu adversarial perturbations to minimize false negatives of \fingerprint and increase the confidence of detecting stolen policies. 

% However, while \osfwu is  the most effective UAP to cause this type of behaviours between different policies, we found that in some cases, it also leads to an independent policy into behaving more similarly to the source policy that \osfwu is generated on. Therefore, we decide to use multiple \osfwu masks as fingerprints to lower the probability of misidentifying an independently trained policy as the source policy in accordance with requirement VR-1.

%%\caption{Example of non-transferable universal masks as fingerprints in binary classification. The effect of the gradient coming from the most confident adversarial direction is diminished by others, while generating the universal mask. In addition, \fingerprint selects universal masks as fingerprints if they move a large percentage of test samples into the non-transferable (shaded) region.}
