\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multirow}
\usepackage{diagbox}
\usepackage{amsfonts} 

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{multirow}
\usepackage{bm}
\usepackage{algorithm, algpseudocode}
\usepackage{color}
\usepackage{colortbl}
\usepackage{textcomp}
\usepackage[outercaption]{sidecap}
\usepackage{makecell}
\usepackage{stmaryrd}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subfigure}
\usepackage[table]{xcolor}
\usepackage[figuresleft]{rotating}
\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother


\newcommand{\dec}[1]{\ensuremath{_{\text{\textcolor{blue}{($\downarrow$)}}}}}
\newcommand{\inc}[1]{\ensuremath{_{\text{\textcolor{magenta}{($\uparrow$)}}}}}
\newcommand{\grayback}[1]{\ensuremath{_{\text{\textcolor{gray! 20}{($\uparrow$)}}}}}
\newcommand{\wpic}[1]{\ensuremath{_{\text{\textcolor{white}{($\uparrow$)}}}}}

\newcommand{\mvar}[1]{\ensuremath{_{\text{($\pm$#1)}}}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\author{Dong Lu${}^{1}$\thanks{$~$Equal contribution. ${}^\dagger$ Corresponding author.}, Zhiqiang Wang${}^1$\samethanks, Teng Wang${}^{1,2}$, Weili Guan${}^3$, Hongchang Gao${}^4$, Feng Zheng${}^{1,5\dagger}$\\
{\small ${}^1$Southern University of Science and Technology ${}^2$The University of Hong Kong}\\
{\small ${}^3$Monash University ${}^4$Temple University ${}^5$Peng Cheng Laboratory}\\
{\tt\small sammylu\_@outlook.com wangzq\_2021@outlook.com tengwang@connect.hku.hk}\\
{\tt\small honeyguan@gmail.com  hongchang.gao@temple.edu f.zheng@ieee.org}\\
}

% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Vision-language pre-training (VLP) models have shown vulnerability to adversarial examples in multimodal tasks. 
Furthermore, malicious adversaries can be deliberately transferred to attack other black-box models.
However, existing work has mainly focused on investigating white-box attacks.
In this paper, we present the first study to investigate the adversarial transferability of recent VLP models.
We observe that existing methods exhibit much lower transferability, compared to the strong attack performance in white-box settings.
The transferability degradation is partly caused by the under-utilization of cross-modal interactions.
Particularly, unlike unimodal learning, VLP models rely heavily on cross-modal interactions and the multimodal alignments are many-to-many, \textit{e.g.}, an image can be described in various natural languages.
To this end, we propose a highly transferable Set-level Guidance Attack (SGA) that thoroughly leverages modality interactions and incorporates alignment-preserving augmentation with cross-modal guidance.
Experimental results demonstrate that SGA could generate adversarial examples that can strongly transfer across different VLP models on multiple downstream vision-language tasks. 
On image-text retrieval, SGA significantly enhances the attack success rate for transfer attacks from ALBEF to TCL by a large margin (at least 9.78\% and up to 30.21\%), compared to the state-of-the-art.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{Sections/S1_Introduction}
\input{Sections/S2_RelatedWork}
\input{Sections/S3_Analysis}
\input{Sections/S4_Method}
\input{Sections/S5_Experiment}
\input{Sections/S6_Conclusion}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

%%%%%%%%% APPENDIX
\clearpage
\appendix
\input{Appendices/A}
\input{Appendices/B}
\input{Appendices/C}

\end{document}