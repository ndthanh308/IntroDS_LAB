\section{Motivation}
\label{sec:supp_a}

The analysis and discussion presented in Section \ref{sec:analysis} shed light on the keys to improving adversarial transferability among VLP models: multimodal interaction and diverse data.
To further figure out a practicable solution, we delve into the cases of transfer failure of existing attack methods.
We find that half of the failure cases are raised by the existence of multiple paired captions.

Considering an image-text pair $(v,t)$, the corresponding adversarial image $v'$ generated on model $f_{wb}$ in white-box manner (note that only $(v,t)$ and $f_{wb}$ are utilized in the process of crafting $v'$), a black-box model $f_{bb}$ and another several matched captions $\mathbf{t}=\{t_1,...,t_n\}$, we define two events:
\begin{itemize}
    \item \textit{Event A}: adversarial image $v'$ cannot match anyone of the captions $t \cup \{t_1,...,t_n\}$  in model $f_{wb}$. 
    \item \textit{Event B}: adversarial image $v'$ can match one of captions $\{t_1,...,t_n\}$ in model $f_{bb}$.
\end{itemize}
\textit{Event A} indicates that the adversarial image successfully fools model $f_{wb}$, successful case of white-box attack.
\textit{Event B} indicates that the adversarial image cannot fully fool the target model $f_{bb}$ in a transferring manner, failure case of transfer-based black-box attack.
We present the statistic figures of $p($\textit{Event A}$)$ and $p($\textit{Event B} $|$ \textit{Event A}$)$ in Table \ref{tab:supp_t6_black_box_failures}.
As shown in the table, even though the adversarial images have high attack ability in the white-box model (about 71\% - 80\% error rate), around half of them fail due to matching other paired captions when transferring to a black-box model (about 46\%-57\%).

In detail, existing attacks tend to restrict the generated adversarial image $v'$ far away (Euclidean distance or cosine distance in the embedding space in most of the cases) from the original image $v$ or the caption $t$.
These methods only utilize the information of a single image-caption pair $(v,t)$ in their processes of crafting adversarial examples.
As a result, although in most of the cases $v'$ is far away from $t$ and other paired captions $ \{t_1,...,t_n\}$ in the embedding space of the white-box model, it is prone to approaching $ \{t_1,...,t_n\}$ when transferred to a black-box model and the embedding space changes, which means the failure of transfer-based black-box attack.

We attribute the failure of the transfer attack to the lack of cross-modal interaction (corresponding to the first two rows in Table \ref{tab:supp_t6_black_box_failures}). 
The adversarial image $v'$ generated merely based on image $v$ or single image-text pair $(v,t)$ can have strong attack ability to the caption $t$ and always weak attack ability to $\{t_1,...,t_n\}$.
When transferred to a black-box model, the adversarial image $v'$ may still maintain satisfactory attack ability to caption $t$ but most likely to lose the attack ability to captions $\{t_1,...,t_n\}$.
Note that $v'$ has the attack ability to $t$ in model $f$ means $v'$ cannot successfully match $t$ in the embedding space of model $f$.
To validate the claim, in Figure \ref{fig:supp_f6_attack_ability}, we use the ranking to measure the adversarial image's attack ability to the caption.
Higher ranking, stronger attack ability.
Since an adversarial image has several paired captions in the gallery, we present the lowest, average, and highest ranking of these captions.
As shown in Figure \ref{fig:supp_f6_attack_ability}, for the attack method with no cross-modal interaction (Sep-Attack) and the attack method with single-pair cross-modal interaction (Co-Attack), though the generated adversarial image can have a high attack ability to some captions, there always exists a caption that the adversarial image has weak attack ability to it (the lowest rankings of Sep-attack and Co-Attack are both around 600, which means weak attack ability compared the highest rankings of them, around 2,200 and 2,400).

An implicit assumption in the previous statement is that high attack ability in the white-box model means high adversarial transferability in the black-box model, which can also be verified among existing attack methods.
Considering a image-caption pair $(v,t)$, the corresponding adversarial image $v'$ generated on the white-box model $f_{wb}$ , a black-model $f_{bb}$ and several matched captions of $v$, $\{t_1,...,t_n\}$, we define two events:
\begin{itemize}
    \item \textit{Event C}: adversarial image $v'$ cannot match $t$ in white-box model $f_{wb}$. 
    \item \textit{Event D}: adversarial image $v'$ cannot match $t$ in black-box model $f_{bb}$.
\end{itemize}
We present the statistic figures of $p($\textit{Event C}$)$ and $p($\textit{Event D} $|$ \textit{Event C}$)$ in Table \ref{tab:supp_t7_attack_preservation}.
If the adversarial image $v'$ has a high attack ability to caption $t$ in the white-box model, it is very likely that it also maintains the attack ability towards caption $t$ when transferred to a black-box model.
For example, if the adversarial image $v'$ generated on model ALBEF succeeds in attacking caption $t$ in model ALBEF, there is a high probability that it can succeed in attacking caption $t$ in model TCL, 40.51\%, compared to the overall adversarial transferability from ALBEF to TCL, 15.21\%.

\input{Figures/supp_f6_attack_ability}

According to the analysis above, to boost the adversarial transferability of the generated adversarial image, it is crucial to consider multiple paired captions and push the adversarial image away from all the paired captions, thus preserving the attack ability when transferring to other black-box models.
Crafting adversarial captions for high transferability follows a similar approach, which can also benefit from more paired images.

\input{Tables/supp_t6_black_box_failures}
\input{Tables/supp_t7_attack_preservation}