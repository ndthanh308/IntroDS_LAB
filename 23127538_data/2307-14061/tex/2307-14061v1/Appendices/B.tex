\section{Experiments \& Analysis}
\label{sec:supp_b}

\subsection{Experimental Settings} 
\label{sec:supp_b_exp_setting}
Since fused VLP models contain both multimodal encoder and unimodal encoder, two types of embedding can be perturbed, \textit{i.e.}, multimodal embedding, and unimodal embedding. 
The embeddings can be further divided into the full embedding (denoted as {\texttt{Multi$\rm_{full}$}} or {\texttt{Uni$\rm_{full}$}}) and [CLS] of embedding (denoted as {\texttt{Multi$\rm_{CLS}$}} or {\texttt{Uni$\rm_{CLS}$}}).
For aligned VLP models (\textit{e.g.}, CLIP \cite{Radford2021CLIP}), since the image encoder can be ViT or CNN, only [CLS] of embedding for CLIP$_{\rm ViT}$  is discussed and consider the embedding of CLIP$_{\rm CNN}$ as [CLS] of embedding \cite{Zhang2022Co-attack}.

\input{Tables/supp_t8_exp_sep_attack_albef}
\input{Tables/supp_t9_exp_sep_attack_tcl}
\input{Tables/supp_t10_exp_sep_attack_clip}
\input{Tables/supp_t11_exp_co_attack}
\input{Tables/supp_t12_SGA_flickr}
\input{Tables/supp_t13_SGA_coco}


\subsection{Transferability Analysis}
\label{sec:supp_b_ana}
Table \ref{tab:supp_t8_exp_sep_attack_albef}, Table \ref{tab:supp_t9_exp_sep_attack_tcl}, Table \ref{tab:supp_t10_exp_sep_attack_clip}, and Table \ref{tab:supp_t11_exp_co_attack} show adversarial transferability among different VLP models and configurations under Sep-Attack and Co-Attack.
We report the attack success rates of the adversarial examples generated by the source model to attack the target models. 

Some observations on adversarial transferability are summarized below:
\begin{itemize}
  \item For all VLP models, attacking two modalities simultaneously shows better adversarial transferability than only attacking a single modality. This is consistent with the observation in \cite{Zhang2022Co-attack} for the white-box setting.
  \item Even though models with exact same architectures but with different pretrain objectives (\textit{e.g.}, ALBEF and TCL), the adversarial examples cannot directly pass through another model with a similar success attack rate. 
  \item The adversarial transferability from fused VLP models to aligned VLP models is higher than that from backward (\textit{e.g.}, from ALBEF or TCL to CLIP-ViT and CLIP-CNN). 
  \item Although ALBEF, TCL, and CLIP-ViT are using ViT as image-encoders, the adversarial transferability from ALBEF or TCL to CLIP-CNN will be higher than that of CLIP-ViT; similarly, the adversarial transferability of CLIP-ViT to CLIP-CNN is higher than that of CLIP-CNN to CLIP-ViT. 
\end{itemize}

\input{Tables/supp_t14_exp_unimodal}


\subsection{Main Results}
\label{sec:supp_b_main_res}
We present a thorough analysis of the performance of our proposed high transferable multimodal attack method, SGA, on the popular benchmark datasets Flickr30K and MSCOCO. 
The experimental results are summarized in Table \ref{tab:supp_t12_SGA_flickr} and Table \ref{tab:supp_t13_SGA_coco}, providing a clear comparison between the performance of our SGA and existing multimodal attack methods across different attack scenarios. 
As we can see, our proposed SGA outperforms the state-of-the-art in all white-box and black-box settings.
Moreover, as illustrated in Table \ref{tab:supp_t14_exp_unimodal}, we conduct extensive experiments on Flickr30K under a unimodal scenario, with perturbed input in either the image or text modality. 
Empirical evidence suggests that even in scenarios where only query data are accessible, the performance of SGA consistently surpasses that of existing methods.

Our results suggest that the proposed SGA can serve as a promising method for evaluating the robustness of multimodal models and improving their security in real-world applications.


\subsection{Ablation Study}
\label{sec:supp_b_ablation}

This section presents the ablation experiments on the augmented multimodal data and the iterative strategy sequence of SGA. To provide a thorough analysis, detailed experimental results are presented and discussed.

\paragraph{Iterative Strategy.}
In this study, we generate adversarial examples through cross-modal guidance. 
This allows for the disruption of multimodal interactions through the collaborative generation of perturbations. 
Notably, our process follows a ``text-image-text" (t-i-t) pipeline.

We have conducted additional experiments to evaluate the effectiveness of our attack strategy. 
As shown in Table \ref{tab:supp_t15_ablation_reverse_multi_alter}, an interesting observation is that reversing the ``t-i-t" pipeline does not significantly impact the results. 
Furthermore, although adding one iteration (t-i-t-i-t) slightly enhances performance, it also doubles the computational cost. 
This suggests that our SGA is not sensitive to the exact order of the pipeline, but rather benefits from cross-modal guidance.


\paragraph{Multi-scale Image Set.}
In SGA, an augmented image set is used to generate adversarial data based on the scale-invariant property of deep learning models.
To verify the effectiveness of the augmented image set, we choose different scale ranges to build the image sets and evaluate the adversarial transferability.
As presented in Table \ref{tab:supp_t16_ablation_img}, there exists a positive correlation between transferability and the scale range, with the highest transferability observed at a scale range of $[0.50,1.50]$ with a step size of 0.25.
The experimental results show that the augmented image set plays a crucial role in increasing the transferability of the generated adversarial data.

\input{Tables/supp_t15_ablation_reverse_multi_alter}
\input{Tables/supp_t16_ablation_img}
\input{Tables/supp_t17_ablation_txt}

\paragraph{Multi-pair Caption Set.}
The proposed SGA involves augmenting the original caption into a caption set for the purpose of generating adversarial data. 
To determine the effectiveness of the augmented caption set, various numbers of captions are utilized to construct the caption sets, and the transferability of the resulting adversarial data is evaluated. 
As illustrated in Table \ref{tab:supp_t17_ablation_txt}, the use of multiple captions in the process of crafting adversarial data is observed to have a significant positive impact on adversarial transferability. 
Experimental results demonstrate that the augmented caption set also helps enhance the transferability of the generated adversarial data.


\subsection{Visualization}
\label{sec:supp_b_vis}
Figure \ref{fig:supp_f7_visualization} depicts randomly selected original clean images and the corresponding adversarial examples, and such small perturbations are hard to be perceived.
We magnified the imperceptible perturbation by a factor of 50 for visualization.

