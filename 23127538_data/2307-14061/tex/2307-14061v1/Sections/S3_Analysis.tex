\section{Analysis of Adversarial Transferability}
\label{sec:analysis}

\input{Figures/f3_ana_perturb_input}
\input{Figures/f4_ana_transfer}

In this section, we conduct an empirical study on VLP models to evaluate adversarial transferability using existing methods.
A common approach to attack multimodal tasks is combining unimodal adversarial attacks \cite{Madry2018PGD, Dong2018BoostingAA, Xie2018ImprovingTransDiverse, Li2020BERTATTACK} of each modalities together. 
For instance, the separate unimodal attack (Sep-Attack) includes PGD \cite{Madry2018PGD} and BERT-Attack \cite{Li2020BERTATTACK} for attacking image modality and text modality, respectively.
Other recent multimodal adversarial attacks, such as Co-Attack \cite{Zhang2022Co-attack}, consider cross-modal interactions by perturbing image modalities and text modalities collectively.

We first present the observations regarding the adversarial transferability of VLP models.
Then we discuss the limitations of the existing methods.
By conducting this study, we aim to provide insights into the robustness of VLP models against adversarial attacks and the effectiveness of different attack strategies.


\subsection{Observations}
To investigate the adversarial transferability of perturbed inputs with respect to different modalities (\textit{i.e.}, image, text, and image \& text) and the effect of different VLP models on transferability, we conduct experiments and present the attack success rates of the adversarial examples generated by the source model to attack the target models in Figure \ref{fig:f3_ana_perturb_input} and Figure \ref{fig:f4_ana_transfer}.
The observations are summarized below:
\begin{itemize}
  \item The adversarial transferability of attacking both modalities (image \& text) is consistently more effective than attacking any unimodal data alone (image or text).
  As shown in Figure \ref{fig:f3_ana_perturb_input}, transferring both adversarial image and text from ALBEF to TCL leads to a much higher attack success rate than transferring adversarial examples of any single modality. Notably, ALBEF and TCL are both fused VLP models.
  Similar observations also exist in the following settings: (1) The source and target models are different types of VLP models but have the same basic architectures (\textit{e.g.}, TCL and CLIP$_{\rm ViT}$). (2) The source and target models are the same types of VLP models, but with different basic architectures (\textit{e.g.}, CLIP$_{\rm ViT}$ and CLIP$_{\rm CNN}$).
  \item Adversarial multimodal data (\textit{i.e.}, adversarial image \& text), which have strong attack performance on the source model, can hardly maintain the same capability when transferring to target models. 
  For example, as illustrated in Figure \ref{fig:f4_ana_transfer}, even though ALBEF and TCL have the same model architecture, the attack success rate sees a significant drop when transferring adversarial examples generated on ALBEF to TCL.
  The phenomenon exists in both Sep-Attack and Co-Attack.
\end{itemize}

In summary, the attack methods can have stronger transferability in black-box settings if all the modalities are attacked simultaneously.
However, even though two modalities are allowed to be perturbed, existing methods still exhibit much lower transferability. 
This suggests that attacks with higher transferability should be specifically designed, instead of directly utilizing the existing white-box attack methods.


\subsection{Discussions}\label{sec:sec3_dis}
We posit that the degradation in transferability of adversarial examples is mainly due to the limitations of the existing attack methods:
\begin{itemize}
  \item One major limitation of Sep-Attack is that it does not take into account the interactions between different modalities. 
  As a combined independent attack method for each modality, it fails to model the inter-modal correspondence that is crucial for successful attacks in multimodal learning.
  This is particularly evident in multimodal tasks such as image-text retrieval, where the ground truth is not discrete labels (\textit{e.g.}, image classification) but another modality data that corresponds to the input modality.
  The complete lack of cross-modal interactions in Sep-Attack severely limits the generalization of adversarial examples and reduces their transferability among different VLP models. 
  \item While Co-Attack is designed to leverage the collaboration between modalities to generate adversarial examples, it still suffers from a key drawback that hinders its transferability to other VLP models.
  Unlike unimodal learning, multimodal learning involves multiple complementary modalities with many-to-many cross-modal alignments, which pose unique challenges to achieving sufficient adversarial transferability.
  However, Co-Attack only uses single image-text pairs to generate adversarial data, limiting the diversity of guidance from multiple labels in other modalities.
  This lack of diversity in cross-modal guidance makes adversarial samples highly correlated with the alignment pattern of the white-box model.
  Therefore, the generality of adversarial examples is restricted, and their effectiveness in transferring to other models drops.
\end{itemize}

In conclusion, the analysis motivates our investigation into the adversarial transferability of VLP models.
Moreover, it highlights the pressing need to explore transferable multimodal attacks for generating adversarial examples that can be effectively transferred across different VLP models.