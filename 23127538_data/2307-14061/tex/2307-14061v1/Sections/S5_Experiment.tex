\section{Experiments}
\label{sec:experiment}

In this section, we present experimental evidence for the advantages of our proposed SGA. 
We conduct experiments on a diverse set of datasets and popular VLP models.
First, we describe the experimental settings in Section \ref{sec: exp_setting}. 
Then, in Section \ref{sec: exp_ana}, we validate the immediate integration of transfer-based unimodal attacks into multimodal learning. 
Next, we provide the main evaluation results compared to the state-of-the-art method in Section \ref{sec: exp_res}. 
In Section \ref{sec:cross_task}, we analyze the cross-task transferability between different V+L tasks. 
Finally, we present ablation studies in Section \ref{sec: exp_ablation}.


\subsection{Experimental Settings}\label{sec: exp_setting}
\paragraph{Datasets.} We consider two widely used multimodal datasets, Flickr30K \cite{Plummer2015Flickr30k} and MSCOCO \cite{Lin2014COCO}.
Flickr30K consists of 31,783 images, each with five corresponding captions.
Similarly, MSCOCO comprises 123,287 images, and each image is annotated with around five captions.
We adopt the Karpathy split \cite{Karpathy2017KarpathySplit} for experimental evaluation. 

\paragraph{VLP Models.} We evaluate two popular VLP models, the fused VLP and aligned VLP models. 
For the fused VLP, we consider ALBEF \cite{Li2021ALBEF} and TCL \cite{Yang2022TCL}.
ALBEF contains a 12-layer visual transformer ViT-B/16 \cite{Dosovitskiy2021ViT} and two 6-layer transformers for the image encoder and both the text encoder and the multimodal encoder, respectively. 
TCL uses the same model architecture as ALBEF but with different pre-train objectives.
For the aligned VLP model, we choose to evaluate CLIP \cite{Radford2021CLIP}.
CLIP has two different image encoder choices, namely, $\rm CLIP_{ViT}$ and $\rm CLIP_{CNN}$, that use ViT-B/16 and ResNet-101 \cite{He2016ResNet} as the base architectures for the image encoder, respectively.

\paragraph{Adversarial Attack Settings.} 
To craft adversarial images, we employ PGD \cite{Madry2018PGD} with perturbation bound $\epsilon_{v}=2/255$, step size $\alpha=0.5/255$, and iteration steps $T=10$.
For attacking text modality, we adopt BERT-Attack \cite{Li2020BERTATTACK} with perturbation bound $\epsilon_{t}=1$ and length of word list $W=10$.
Furthermore, we enlarge the image set by resizing the original image into five scales, $\{0.50, 0.75, 1.00, 1.25, 1.50\}$, using bicubic interpolation.
Similarly, the caption set is enlarged by augmenting the most matching caption pairs for each image in the dataset, with the size of approximately five.

\paragraph{Metrics.}
We employ Attack Success Rate (ASR) as the metric for evaluating the adversarial robustness and transferability in both white-box and black-box settings. 
Specifically, ASR evaluates the percentage of attacks that only produce successful adversarial examples. 
A higher ASR indicates better adversarial transferability.

\input{Tables/t3_main_f30k}


\subsection{Transferability Analysis}\label{sec: exp_ana}
In this paper, we present a systematic study of the adversarial transferability of VLP models, which has not been explored. 
As demonstrated in Section \ref{sec:analysis}, existing methods, including the separate unimodal adversarial attack (Sep-Attack) and the multimodal adversarial attack (Co-Attack), exhibit limited transferability to other VLP models.

To improve transferability in multimodal learning, we intuitively investigate the adoption of transfer-based attacks from unimodal learning such as image classification. 
Specifically, we consider MI \cite{Dong2018BoostingAA}, DIM \cite{Xie2018ImprovingTransDiverse}, and PNA\_PO \cite{wei2022PNA_PO}. 
However, this approach can be problematic if cross-modal interactions and the unique many-to-many alignments in multimodal learning are not taken into account.

Table \ref{tab:t2_exp_unimodal} illustrates that multimodal attack methods that incorporate transfer-based image attacks exhibit minimal improvement in transferability while compromising white-box performance.
Specifically, when integrated with MI, Co-Attack drops significantly by 12.3\% in white-box settings, while only maintaining 25.40\% ASR in transferability (ALBEF to TCL).
However, our SGA shows superior performance in both white-box and black-box settings.
Notably, Sep-Attack combined with transfer-based attacks not only reduces the effectiveness of white-box attacks but also fails to improve adversarial transferability in almost all black-box settings.
The results provide empirical evidence that directly combining unimodal adversarial attacks in multimodal learning without considering cross-modal interactions and alignments can be problematic, even when using transfer-based unimodal attacks.
Additional discussion is provided in Appendix \ref{sec:supp_b_ana}.
\input{Tables/t4_exp_caption}


\subsection{Experimental Results}\label{sec: exp_res}
\paragraph{Multimodal Fusion Modules.}
First, we investigate VLP models with different fusion modules, namely, fused VLP models and aligned VLP models. 
We generate adversarial examples on both types of models and evaluate their attack performance when transferred to other VLP models while ensuring consistency in the input size of images. 
For example, adversarial images generated by ALBEF or TCL are resized to $224\times224$ before performing transfer attacks on CLIP, and adversarial examples generated on CLIP are resized to $384\times384$ before being transferred to ALBEF or TCL.

As shown in Table~\ref{tab:t3_main_f30k}, experimental results demonstrate the superiority of our proposed SGA over existing multimodal attack methods in all black-box settings.
Specifically, our SGA achieves significant improvements in adversarial transferability when the source and target models are of the same type. 
For instance, SGA outperforms Co-Attack by approximately 30$\%$ in terms of attack success rate when transferring adversarial data from ALBEF to TCL. 
Moreover, in the more challenging scenario where the source and target models are of different types, SGA also surpasses Co-Attack with higher attack success rates. 
More experiments on the MSCOCO dataset are provided in Appendix \ref{sec:supp_b_main_res}.

\paragraph{Model Architectures.}
Then, we explore VLP models with respect to different model architectures.
Many VLP models commonly use ViTs as the vision encoder, where images are segmented into patches before being processed by the transformer model. 
However, in the case of CLIP, the image encoder consists of two distinct architectures: conventional Convolutional Neural Networks (CNNs) and ViTs.
The transferability between CNNs and ViTs has been well-studied in unimodal learning. Therefore, we also investigate the adversarial transferability of CNN-based and ViT-based CLIP models in multimodal learning.

As shown in Table~\ref{tab:t3_main_f30k}, we observe a similar phenomenon as unimodal learning that compared to CNNs, ViTs show better robustness against adversarial perturbations \cite{Naseer2021IntriguingViT}.
Specifically, for all attack methods, the same adversarial multimodal data have a stronger white-box attack effect on $\rm CLIP_{CNN}$ compared to $\rm CLIP_{ViT}$.
Moreover, the adversarial examples generated on $\rm CLIP_{ViT}$ are found to be more transferable to $\rm CLIP_{CNN}$ than vice versa (38.76\% vs. 31.24\%).

Furthermore, our proposed SGA consistently improves transferability on both CNN-based CLIP and ViT-based CLIP compared to other attacks. 
For instance, SGA increases the adversarial transferability for $\rm CLIP_{ViT}$ by 5.83\% and 6.24\% compared to Co-Attack under the white-box setting and black-box setting, respectively.

\subsection{Cross-Task Transferability}\label{sec:cross_task}
Cross-modal interactions and alignments are the core components of multimodal learning regardless of the task.
Therefore, we conduct extensive experiments to explore the effectiveness of our proposed SGA on two additional V+L tasks: Image Captioning (IC) and Visual Grounding (VG).

\paragraph{Image Captioning.}
Image captioning is a generation-based task, where an input image is encoded into a feature vector and then decoded into a natural language sentence.
In our experiments, we craft adversarial images using the source model (ALBEF) with an image-text retrieval objective and then directly attack the target model (BLIP \cite{Li2022BLIP}) on image captioning.
We employ the MSCOCO dataset, which is suitable for both two tasks, and utilize various evaluation metrics to measure the quality of the generated captions, including BLEU \cite{Papineni2002BLEU}, METEOR \cite{Banerjee2005METEORAA}, ROUGE \cite{Lin2004ROUGEAP}, CIDEr \cite{Vedantam2015CIDEr}, and SPICE \cite{Anderson2016SPICE}.

We present the performance on image captioning of BLIP after being attacked in Table~\ref{tab:t4_exp_caption}. 
Experimental results demonstrate clear improvements in the adversarial transferability of the proposed SGA compared to Co-Attack.
Specifically, our SGA improves the BLEU score by up to 2.6\% and the CIDEr score by up to 9.5\%.

\input{Tables/t5_exp_grounding}

\paragraph{Visual Grounding.}
Visual grounding is another V+L task that aims to localize the region in an image based on the corresponding specific textual description.
Similarly, we generate adversarial images using the source model (ALBEF) from image-text to attack the target model (ALBEF) on visual grounding.
Table~\ref{tab:t5_exp_grounding} shows the results on RefCOCO+ \cite{Yu2016RefCOCO}, where our SGA still outperforms Co-Attack.


\subsection{Ablation Study}\label{sec: exp_ablation}
To systematically investigate the impact of our set-level alignment-preserving augmentations, we conducted ablation experiments on image-text retrieval to evaluate the effect of varying the number of augmented image sets $N$ with multi-scale transformation and the number $M$ of augmented caption sets. 
Specifically, we employed ALBEF as the source model and $\rm CLIP_{ViT}$ as the target model on Flickr30K. 
More details are provided in Appendix \ref{sec:supp_b_ablation}.

\input{Figures/f5_ablation}

\paragraph{Multi-scale Image Set.}
We propose the use of multiple scale-invariant images to generate diverse adversarial data in SGA. 
Results in the left of Figure \ref{fig:f5_ablation} reveal that the transferability significantly increases as we introduce more diverse images with different scales, peaking when the scale range is set to $[0.50, 1.50]$ with a step of $0.25$. 
We set the scale range $S=\{0.50, 0.75, 1.00, 1.25, 1.50\}$ for optimal performance.

\paragraph{Multi-pair Caption Set.}
We also conduct experiments to investigate the impact of an enlarged caption set on adversarial transferability. 
The number of additional captions ranged from $1$ to $M$, where $M$ represents the most matching caption pairs from the dataset for each image. 
Results presented in the right panel of Figure \ref{fig:f5_ablation} indicate that if $M > 1$, the black-box performance increases significantly but eventually plateaus.
These results demonstrate the effectiveness of using multiple alignment-preserving inter-modal information to enhance adversarial transferability. 
Furthermore, we observed that the performance is relatively insensitive to the number of extra captions, but adding more captions can improve the overall adversarial transferability.
