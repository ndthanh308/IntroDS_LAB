\section{Conclusion}
\label{sec:conclusion}    
In this paper, we make the first attempt to investigate the adversarial transferability of typical VLP models.
We systematically evaluate the existing attack methods and reveal that they still exhibit lower transferability, despite their impressive performance in white-box settings.
Our investigation highlights the need for specially designed transferable attacks in multimodal learning that can model the many-to-many cross-modal alignments and interactions. 
We propose SGA, a highly transferable multimodal attack, which leverages set-level alignment-preserving augmentations through cross-modal guidance to thoroughly exploit multimodal interactions. 
We hope that this work could inspire further research to evaluate and enhance the adversarial robustness of VLP models.

\paragraph{Acknowledgments.}
This work was supported by the National Key R\&D Program of China (Grant NO. 2022YFF1202903) and the National Natural Science Foundation of China (Grant NO. 62122035).       