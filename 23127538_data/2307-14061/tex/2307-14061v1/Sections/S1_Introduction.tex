\section{Introduction}
\label{sec:introduction}

Recent work has shown that vision-language pre-training (VLP) models are still vulnerable to adversarial examples \cite{Zhang2022Co-attack}, even though they have achieved remarkable performance on a wide range of multimodal tasks \cite{Shi2021DenseCV, Khan2021ExploitingBF, Lei2021UnderstandingCV}.
Existing work mainly focuses on white-box attacks, where information about the victim model is accessible.
However, the transferability of adversarial examples across VLP models has not been investigated, which is a more practical setting. 
It is still unknown whether the adversarial data generated on the source model can successfully attack another model, which poses a serious security risk to the deployment of VLP models in real-world applications. 

\input{Figures/f1_cover}

This paper makes the first step to investigate the transferability of adversarial samples within VLP models. 
Without loss of generality, most of our experiments are based on image-text retrieval tasks. 
We first empirically evaluate this attack performance with respect to different modalities on multimodal tasks across multiple datasets. 
Our results show that the adversarial transferability of attacking both modalities (image \& text) consistently beats attacking unimodal data (image or text). 
Unfortunately, even though two modalities are allowed to be perturbed simultaneously, the attack success rates of existing methods \cite{Madry2018PGD, Li2020BERTATTACK, Zhang2022Co-attack} still significantly drops when transferring from the white-box to black-box settings, as shown in Figure \ref{fig:f1_cover}.

Different from recent studies focusing on separate attacks on unimodal data \cite{Madry2018PGD, Li2020BERTATTACK}, multimodal pairs exhibit intrinsic alignment and complementarity to each other.
The modeling of inter-modal correspondence turns out to be a critical problem for transferability. 
Considering that the alignments between image and text are many-to-many, for example, an image could be described to be with various human perspectives and language styles, a reasonable perturbation direction may be determined with diverse guidance from multiple labels in the other modality. 
However, recent adversarial attack methods for VLP models \cite{Zhang2022Co-attack} usually employ a single image-text pair to generate adversarial samples. 
Although they exhibit strong performance in white-box settings, the poor diversity of guidance makes adversarial samples highly correlated with the alignment pattern of the white-box model, and therefore impedes generalization to black-box settings.

\input{Figures/f2_SGA}

To address the weak transferability problem, we propose Set-level Guidance Attack (SGA), which leverages diverse cross-modal interactions among multiple image-text pairs (Figure \ref{fig:f2_SGA}). 
Specifically, we introduce alignment-preserving augmentation which enriches image-text pairs while keeping their alignments intact. 
The image augmentation is based on the scale-invariant property of deep learning models \cite{Lin2019NesterovScaleInva}, thus we can construct multi-scale images to increase the diversity. For text augmentation, we select the most matching caption pairs from the dataset.
More importantly, SGA generates adversarial examples on multimodal augmented input data with carefully designed cross-modal guidance. 
In detail, SGA iteratively pushes supplemental information away between two modalities with another modality as supervision to disrupt the interactions for better harmonious perturbations. 
Note that resultant adversarial samples could perceive the gradients originated from multiple guidance.

We conduct experiments on two well-established multimodal datasets, Flickr30K \cite{Plummer2015Flickr30k} and MSCOCO \cite{Lin2014COCO}, to evaluate the performance of our proposed SGA across various Vision-and-Language (V+L) downstream tasks.
The experimental results demonstrate the high effectiveness of SGA in generating adversarial examples that can be strongly transferred across VLP models, surpassing the current state-of-the-art attack methods in multimodal learning. 
In particular, SGA achieves notable improvements in image-text retrieval under black-box settings and also exhibits superior performance in white-box attack settings.
Moreover, SGA also outperforms the state-of-the-art methods in image captioning and yields higher fooling rates on visual grounding. 

We summarize our contributions as follows. 
\textbf{1)} We make the first attempt to explore the transferability of adversarial examples on popular VLP models with a systematical evaluation; 
\textbf{2)} We provide SGA, a novel transferable multimodal attack that enhances adversarial transferability through the effective use of set-level alignment-preserving augmentations and well-designed cross-modal guidance;
\textbf{3)} Extensive experiments show that SGA consistently boosts adversarial transferability across different VLP models than the state-of-the-art methods.