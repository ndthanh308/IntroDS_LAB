\section{Related Work}
\subsection{Vision-Language Pre-training Models}
Vision-Language Pre-training (VLP) aims to improve the performance of downstream multimodal tasks by pre-training large-scale image-to-text pairs \cite{Li2022BLIP}.
Most works are developed upon the pre-trained object detectors with region features to learn the vision-language representations \cite{Chen2020UNITER,Li2020Oscar,Zhang2021VinVL,wang2022vlmixer}. 
Recently, with the increasing popularity of Vision Transformer (ViT) \cite{Dosovitskiy2021ViT, Touvron2021TrainingDI_forBLIP_arhc,  Yuan2021TokenstoTokenVT}, some other works propose to use ViT as an image encoder and transform the input into patches in an end-to-end manner \cite{Li2021ALBEF,Yang2022TCL,Li2022BLIP,Dou2021METER,wang2023accelerating}.

According to the VLP architectures, VLP models can be classified into two typical types: fused VLP models and aligned VLP models \cite{Zhang2022Co-attack}.  
Specifically, fused VLP models (\textit{e.g.}, ALBEF \cite{Li2021ALBEF}, TCL \cite{Yang2022TCL}) first utilize separate unimodal encoders to process token embeddings and visual features, and further use a multimodal encoder to process image and text embeddings to output fused multimodal embeddings.
Alternatively, aligned VLP models (\textit{e.g.}, CLIP \cite{Radford2021CLIP}) have only unimodal encoders with independent image and text modality embeddings.
In this paper, we focus on popular architectures with fused and aligned VLP models.


\subsection{Image-Text Retrieval Task}
Image-Text Retrieval (ITR) aims to retrieve the relevant top-ranked instances from a gallery database with one modality, given an input query from another modality \cite{Wang2019CAMP, Chen2020IMRAM, Zhang2020ContextAwareAN, Cheng2022ViSTAVA}.
This task can be divided into two subtasks, image-to-text retrieval (TR) and text-to-image retrieval (IR).

For ALBEF \cite{Li2021ALBEF} and TCL \cite{Yang2022TCL}, the semantic similarity score in the unimodal embedding space will be calculated for all image-text pairs to select top-$k$ candidates. Then the multimodal encoder takes the top-$k$ candidates and computes the image-text matching score for ranking.
For CLIP \cite{Radford2021CLIP}, without the multimodal encoder, the final rank list can be obtained based on the similarity in the embedding space between image and text modalities. 


\subsection{Adversarial Transferability}
Existing adversarial attacks can be categorized into two settings: white-box attacks and black-box attacks. 
In a white-box setting, the target model is fully accessible, but not in a black-box setting.
In computer vision, many methods employ gradient information for adversarial attacks in white-box settings, such as FGSM \cite{Goodfellow2015FGSM}, PGD \cite{Madry2018PGD}, C\&W \cite{Carlini2017CW}, and MIM \cite{Dong2018BoostingAA}.
In contrast, in the field of natural language processing (NLP), current attack methods mainly modify or replace some tokens of the input text \cite{Li2020BERTATTACK, Ren2019NLP_Adv_Saliency, Gao2018BlackBoxGO, Jin2020IsBR}.
In the multimodal vision-language domain, Zhang \textit{et al.} \cite{Zhang2022Co-attack} proposed a white-box multimodal attack method with respect to popular VLP models on downstream tasks. 

However, white-box attacks are unrealistic due to the inaccessibility of model information in practical applications.
In addition, there is no related work that systematically analyzes the adversarial transferability of multimodal attack methods on VLP models.
Therefore, in this work, we mainly focus on generating highly transferable adversaries across different VLP models.



