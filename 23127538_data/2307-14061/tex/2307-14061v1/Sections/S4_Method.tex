\section{Methodology}
\label{sec:method}
In this section, we propose a transferable multimodal adversarial attack, termed Set-level Guidance Attack (SGA).
SGA is designed to enhance adversarial transferability across VLP models by leveraging multimodal interactions and incorporating diverse alignment-preserving inter-modal information with carefully designed cross-modal guidance.
We provide our motivation first, followed by relevant notations, and finally present SGA in detail.


\subsection{Motivation}
To improve the transferability of multimodal attacks, we first conduct an investigation into the shortcomings of existing methods in black-box settings. 
Through a systematic analysis of failure cases, we observe that around half of these cases arise due to the presence of multiple matched captions of the image, as shown in Table \ref{tab:t1_black_box_failures}. 
More specifically, our findings indicate that while the generated adversarial image may be significantly distant from the single supervised caption in the source model, it is prone to approaching other matched captions in the target model, where the alignments can be modeled and ultimately lead to attack failure.

Therefore, to maintain the attack ability of adversarial images when transferring to other models, it is crucial to consider multiple paired captions and push the adversarial image away from all the paired captions, thus preserving the attack ability when transferred to other black-box models.
Crafting adversarial text for high transferability follows a similar approach, which can also benefit from more paired images.
More discussions can be found in Appendix \ref{sec:supp_a}.


\input{Tables/t1_black_box_failures}


\subsection{Notations}
Let $(v,t)$ denote an image-text pair sampled from a multimodal dataset $D$. 
For VLP models, we denote $f_I$ as the image encoder and $f_T$ as the text encoder. 
The multimodal fusion module in fused VLP models is denoted by $f_M$.
Specifically, $f_I(v)$ represents the image representation $e_v$ encoded by $f_I$ taking an image $v$ as input, $f_T(t)$ denotes the text representation $e_t$ encoded by $f_T$ taking a text $t$ as input, and $f_M(e_v,e_t)$ denotes the multimodal representation encoded by $f_M$ taking image and text representations as inputs.

We use $B[v,\epsilon_v]$ and $B[t,\epsilon_t]$ to represent the legal searching spaces for optimizing adversarial image and text, respectively.
Specifically, $\epsilon_v$ denotes the maximal perturbation bound for the image, and $\epsilon_t$ denotes the maximal number of changeable words in the caption.


\subsection{Transferable Set-Level Guidance Attack}
\paragraph{Alignment-preserving Augmentation.}
The analysis presented in Section \ref{sec:analysis} highlights the key limitation of existing methods: the inter-modal information used to generate adversarial examples lacks diversity.
The limitation will make the generated adversarial examples fail to generalize to other black-box models with strong attack performance, resulting in limited adversarial transferability.


To inject more diversity in the generation of generalizable adversarial examples, we propose using set-level alignment-preserving augmentation to expand multimodal input spaces while maintaining cross-modal alignments intact.
Unlike previous methods that only consider a single image-text paired example $(v,t)$ to generate adversarial data, we enlarge the input to a set level of images and captions.
Specifically, we select the most matching caption pairs from the dataset of each image $v$ to form an augmented caption set $\boldsymbol{t}=\{t_1, t_2,...,t_M\}$, and resize each image $v$ into different scales $S=\{s_1,s_2,...,s_N\}$ and then add Gaussian noise to obtain a multi-scale image set $\boldsymbol{v} = \{v_1,v_2,...,v_N\}$ based on the scale-invariant property. 
The enlarged input set $(\boldsymbol{v},\boldsymbol{t})$ is then used to generate the adversarial data $(v',t')$.

\paragraph{Cross-modal Guidance.}
Cross-modal interactions play a crucial role in multimodal tasks. 
For example, in image-text retrieval, the paired information from another modality provides unique annotation supervision for each sample. Similarly, in adversarial attacks, supervisory information is essential in guiding the search for adversarial examples.

To fully utilize the enlarged alignment-preserving multimodal input set $(\boldsymbol{v},\boldsymbol{t})$ and further improve the transferability of the generated adversarial data, we propose cross-modal guidance to utilize interactions from different modalities. Specifically, we use the paired information from another modality as the supervision to guide the direction of optimizing the adversarial data.
This guidance iteratively pushed away the multimodal information and disrupt the cross-modal interaction for better harmonious perturbations. 
Notably, the resultant adversarial examples can perceive the gradients originated from multiple guidance.

First, we generate corresponding adversarial captions for all captions in the text set $\boldsymbol{t}$, forming an adversarial caption set $\boldsymbol{t'}=\{t_{1}', t_{2}'...,t_{M}'\}$. 
The process can be formulated as,
\begin{equation}\label{eq:CMAG_t_group}
  t_{i}' = \mathop{\arg\max}\limits_{t_{i}'\in {B}[t_i,\epsilon_t]}-\frac{f_T(t_{i}')\cdot f_I(v)}{\Vert f_T(t_{i}') \Vert \Vert f_I(v) \Vert}.
\end{equation}
The adversarial caption $t_{i}'$ is constrained  to be dissimilar to the original image $v$ in the embedding space. 
Next, the adversarial image $v'$ is generated by solving
\begin{equation}\label{eq:CMAG_x_adv}
\small
  v'=\mathop{\arg\max}\limits_{v'\in {B}[v,\epsilon_v]} -\sum_{i=1}^{M}\frac{f_T(t_{i}')}{\Vert f_T(t_{i}')\Vert}\sum_{s_i\in S}\frac{f_I(g(v', s_i))}{\Vert f_I(g(v', s_i))\Vert},
\end{equation}
where $g(v',s_i)$ denotes the resizing function that takes the image $v'$ and the scale coefficient $s_i$ as inputs. 
All the scaled images derived from $v'$ are encouraged to be far away from all the adversarial captions $t_{i}'$ in the embedding space.
Finally, the adversarial caption $t'$ is generated as follows,
\begin{equation}\label{eq:CMAG_t_adv}
\small
  t' = \mathop{\arg\max}\limits_{t'\in {B}[t,\epsilon_t]}-\frac{f_T(t')\cdot f_I(v')}{\Vert f_T(t') \Vert \Vert f_I(v') \Vert},
\end{equation}
in which $t'$ is encouraged to be far away from the adversarial image $v'$ in the embedding space. 
The detailed algorithm can be found in Appendix \ref{sec:supp_c}.

\input{Tables/t2_exp_unimodal}


