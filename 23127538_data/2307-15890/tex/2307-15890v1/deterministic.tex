%!TEX root = ./robust_pe.tex
%\newpage

\section{Deterministic Robust Policy Evaluation}\label{sec_deterministic}
The deterministic setting we consider in this section assumes that $\overline{\PP}$ is known. 
%As a consequence,  $\PP^\pi$ defined in \eqref{kernel_defined_by_nature_policy} is available to the planner, for any fixed policy $\pi$ of nature.
We separate our discussions into two parts. 
The first part deals with the tabular setting, where the state space is relatively small, and exact computation is affordable.
We then discuss the extension to handling large state space in the second part, which involves inexact computation. 


\subsection{Tabular Setting}


The proposed method, first-order robust policy evaluation (FRPE),  assumes the access to an oracle with the following capability:
for any $\pi_k$ of the nature, it returns the value function $V^{\vartheta}_{\PP^{\pi_k}}$ of $\vartheta$ within MDP $\cM_{\PP^{\pi_k}}$. 
Equipped with this oracle, FRPE then updates the policy of the nature according to 
\begin{align*}
	\pi_{k+1}(s) = \gamma \zeta \argmin_{\DD \in \cD_s}  \tsum_{t=0}^{k}  \beta_t \inner{ \DD}{{\cV}^{\pi_t}_{\vartheta, s}} + \lambda_k w(\DD), ~ \forall s \in \cS,
\end{align*}
where $w(\cdot)$ is a strictly convex function over $\Delta_{\cS}^{\abs{\cA}}$.
%where ${\cV}^{\pi_t}_{\vartheta, s} = \vartheta(\cdot|s) \otimes {\cV}^{\pi_t}$.
For tabular setting this evaluation oracle can be easily implemented. 
Given that $\overline{\PP}$ and $\pi_k$ are both known,  one can directly compute $\PP^{\pi_k}$ defined in \eqref{kernel_defined_by_nature_policy}. 
Then evaluating $V^{\vartheta}_{\PP^{\pi_k}}$ reduces to a standard policy evaluation problem with known transition kernel $\PP^{\pi_k}$, which can be computed by either solving a linear system or fixed point iteration. 


\begin{algorithm}[t!]
  \caption{First-order Robust Policy Evaluation (FRPE)}
  \begin{algorithmic}
%    \REQUIRE Input
%    \ENSURE Output
    \STATE {\bf Input:} $\cbr{(\beta_k, \lambda_k)}$.
    \STATE {\bf Initialize:} arbitrary initial policy $\pi_0 \in \Pi$.
 \FOR{$k = 0, 1, \ldots$}
 	\STATE Evaluate ${\cV}^{\pi_k} = - {V}^{\vartheta}_{\PP^{\pi_k}}$.
	\STATE Update:
	\begin{align}\label{raw_update_deterministic_rpe}
	\pi_{k+1}(s) = \gamma \zeta \argmin_{\DD \in \cD_s}  \tsum_{t=0}^{k}  \beta_t \inner{ \DD}{{\cV}^{\pi_t}_{\vartheta, s}} + \lambda_k w(\DD), ~ \forall s \in \cS.
	\end{align}
	\STATE	 where 
	$
		{\cV}^{\pi_t}_{\vartheta, s} \coloneqq \vartheta(\cdot|s) \otimes {\cV}^{\pi_t} .
	$
    \ENDFOR
%    \RETURN $\cV^{\pi_k}$
     \end{algorithmic}
\end{algorithm}

It should be noted that to perform \eqref{raw_update_deterministic_rpe} one does not necessarily need to store the historical $\{\cV^{\pi_t}\}_{t=0}^k$.
Instead one can maintain a proper running average of these historical values.
From the definition of $\phi^{\pi}$ in Lemma \ref{lemma_perf_diff}, it is also clear that \eqref{raw_update_deterministic_rpe} is equivalent to the following update: 
\begin{align}
\pi_{k+1}(s) & = \argmin_{\DD \in \cD_s} \tsum_{t= 0}^k \beta_t \phi_t(s, \DD)  + \lambda_k w(\DD) \nonumber \\
& = \argmin_{\DD \in \cD_s}  \Phi_k(s, \DD)  + \lambda_k w(\DD) , \label{pda_nabular_deterministic_update}
\end{align}
where we have defined 
\begin{align*}
\phi_t \coloneqq \phi^{\pi_t}, ~ 
\Phi_k \coloneqq \tsum_{t=0}^k \beta_t \phi_t.
\end{align*} 
Going forward we will often make use of the  Bregman divergence associated with $w(\cdot)$, defined as 
\begin{align*}
\cB(\DD, \DD') \coloneqq w(\DD') - w(\DD) - \inner{\nabla  w(\DD)}{\DD' - \DD}.
\end{align*}
%We assume $w(\cdot)$ is a strictly convex function over $\Delta_{\cS}^{\abs{\cA}}$ and its associated Bregman divergence is defined as 
%\begin{align*}
%\cB(\DD, \DD') \coloneqq w(\DD) - w(\DD') - \inner{\nabla  w(\DD')}{\DD - \DD'}.
%\end{align*}
%Many of our ensuing discussions consider the case where the distance-generating function is defined as 
%\begin{align}\label{dgf_negative_entropy}
%w(\DD) =  \tsum_{a \in \cA} \tsum_{s' \in \cS} \DD_{a}(s') \log \DD_{a}(s').
%\end{align}
%In this case, it can be readily verify that  
%\begin{align}\label{bregman_divergence_negative_entropy}
%\cB(\DD, \DD') & =  \tsum_{a \in \cA} \tsum_{s' \in \cS} \DD_a(s') \log \rbr{ 
%\frac{\DD_a(s')}{\DD'_a(s)}
%} 
%% \nonumber \\ 
%% & = \tsum_{a \in \cA} \mathrm{KL}(\DD_{a} \Vert \DD'_{a})   \nonumber \\
%%& \geq \frac{1}{2\abs{\cA}} \norm{\DD - \DD'}_1^2, 
% \geq \frac{1}{2} \tsum_{a \in \cA} \norm{\DD_a - \DD'_a}_1^2,
%%~ \forall \DD, \DD' \in \Delta_{\cS}^{\abs{\cA}}.
%\end{align}
%where the last inequality follows from the Pinsker's inequality.
As \eqref{raw_update_deterministic_rpe} is invariant when shifting $w$ by a constant, without loss of generality we assume $\inf_{\DD \in \Delta_{\cS}^{\abs{\cA}}} w(\DD) \geq 0$.
We also require  $\overline{w} = \sup_{\DD \in \Delta_{\cS}^{\abs{\cA}}} w(\DD) < \infty$.




We start by  providing a simple characterization on each step of FRPE.


\begin{lemma}\label{lemma_pda_determinsitic_step_characterization}
Let $\Phi_{-1} \equiv 0$, $\lambda_{-1} = 0$, and $\lambda_k \geq \lambda_{k-1}$ for every $k \geq 1$.
Then for any $k \geq 0$, we have 
\begin{align}
& \Phi_k(s, \pi_{k+1}(s)) + \lambda_k \cB( \pi_{k+1}(s), \DD) \leq \Phi_k(s, \DD), ~ \forall \DD \in \cD_s, \label{pda_nhree_point} \\
& \beta_k \phi_k(s, \pi_{k+1}(s)) 
 \leq \Phi_k(s, \pi_{k+1}(s))  - \Phi_{k-1}(s, \pi_k(s)) - \lambda_{k-1} \cB(\pi_k(s), \pi_{k+1}(s))  \label{raw_progress_ineq_pda} .
\end{align}
%where $\cB(\DD, \DD') \coloneqq w(\DD) - w(\DD') - \inner{\nabla  w(\DD')}{\DD - \DD'}$ denotes the Bregman divergence between actions $\DD$ and $\DD'$.
\end{lemma}

\begin{proof}
Clearly, from the optimality condition of \eqref{pda_nabular_deterministic_update}, we obtain \eqref{pda_nhree_point}.
Given the definition of $\Phi_k$, we have 
\begin{align*}
\beta_k \phi_k(s, \DD) & = \Phi_k(s, \DD) - \Phi_{k-1}(s, \DD) - (\lambda_k - \lambda_{k-1}) w(\DD) \\
& \leq \Phi_k(s, \DD) - \Phi_{k-1}(s, \DD) \\
& \leq \Phi_k(s, \DD) - \Phi_{k-1}(s, \pi_k(s)) - \lambda_{k-1} \cB(\pi_k(s), \DD), ~ \forall k \geq 0.
\end{align*}
Further taking $\DD = \pi_{k+1}(s)$ in the above inequality yields \eqref{raw_progress_ineq_pda}.
\end{proof}

With Lemma \ref{lemma_pda_determinsitic_step_characterization} in place, we  proceed to establish the generic convergence properties of FRPE.

\begin{lemma}\label{lemma_pda_deterministic_generic}
Suppose $\lambda_k \geq \lambda_{k-1}$ for every $k \geq 1$, then 
\begin{align*}
\tsum_{t=0}^k 
\beta_t \rbr{f(\pi_{t+1}) - f(\pi^*)}
 \leq 
\tsum_{t=0}^k 
\beta_t \rbr{
1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
}
\rbr{f(\pi_t) - f(\pi^*)} 
+  \frac{2 \lambda_k\overline{w}}{1-\gamma}.
\end{align*}
%where $\overline{w} = \sup_{\DD \in \Delta_{\cS}^{\abs{\cA}}} w(\DD) $. 
\end{lemma}

\begin{proof}
%Clearly, from the optimality condition of \eqref{pda_nabular_deterministic_update}, we obtain
%\begin{align}
%\label{pda_nhree_point}
%\Phi_k(s, \pi_{k+1}(s)) + \lambda_k \cB(\DD, \pi_{k+1}(s)) \leq \Phi_k(s, \DD), ~ \forall \DD \in \cD_s,
%\end{align} 
%where $\cB(\DD, \DD') = w(\DD) - w(\DD') - \inner{\nabla  w(\DD')}{\DD - \DD'}$ denotes the Bregman divergence between actions $\DD$ and $\DD'$.
%Given the definition of $\Phi_k$, we have 
%\begin{align*}
%\beta_k \phi_k(s, \DD) & = \Phi_k(s, \DD) - \Phi_{k-1}(s, \DD) - (\lambda_k - \lambda_{k-1}) w(\DD) \\
%& \leq \Phi_k(s, \DD) - \Phi_{k-1}(s, \DD) \\
%& \leq \Phi_k(s, \DD) - \Phi_{k-1}(s, \pi_k(s)) - \lambda_{k-1} \cB(\DD, \pi_k(s)), ~ \forall k \geq 0,
%\end{align*}
%where we have defined $\Phi_{-1} \equiv 0$ and $\lambda_{-1} = 0$.
%Further taking $\DD = \pi_{k+1}(s)$ in the above inequality yields 
From \eqref{raw_progress_ineq_pda} in Lemma \ref{lemma_pda_determinsitic_step_characterization} we have
\begin{align}
\beta_k \phi_k(s, \pi_{k+1}(s)) 
& \leq \Phi_k(s, \pi_{k+1}(s))  - \Phi_{k-1}(s, \pi_k(s)) - \lambda_{k-1} \cB( \pi_k(s), \pi_{k+1}(s)) \nonumber \\
& \overset{(a)}{\leq} \Phi_k(s, \pi_{k}(s)) - \Phi_{k-1}(s, \pi_k(s)) - \lambda_{k-1} \cB(\pi_k(s), \pi_{k+1}(s))  - \lambda_k \cB( \pi_{k+1}(s), \pi_k(s))  \nonumber  \\
& \overset{(b)}\leq \beta_k \phi_k(s, \pi_k(s)) + (\lambda_k - \lambda_{k-1}) w(\pi_k(s)) \nonumber \\
&\overset{(c)} = (\lambda_k - \lambda_{k-1}) w(\pi_k(s)), \nonumber \\
& \leq (\lambda_k - \lambda_{k-1}) \overline{w} \label{subgrad_inner_ub}, 
\end{align}
where $(a)$ follows from applying \eqref{pda_nhree_point} again;
$(b)$ follows from the definition of $\Phi_k$;
 $(c)$ follows from the trivial identity $\phi_k(s, \pi_k(s)) = 0$ given the definition of $\phi_k$.
 Combing \eqref{subgrad_inner_ub} with Lemma \ref{lemma_perf_diff}, we have
\begin{align}
\cV^{\pi_{k+1}}(s) - \cV^{\pi_k}(s) 
& = \frac{1}{1-\gamma} \EE_{d_s^{\pi_{k+1}}} \sbr{
\phi_k(s', \pi_{k+1}(s') )- \frac{(\lambda_k - \lambda_{k-1}) \overline{w}}{\beta_k} 
} 
+  \frac{(\lambda_k - \lambda_{k-1} ) \overline{w}}{ (1-\gamma) \beta_k } 
 \nonumber \\
& \leq 
\frac{1}{1-\gamma} d_s^{\pi_{k+1}}(s) \sbr{
\phi_k(s', \pi_{k+1}(s') )- \frac{(\lambda_k - \lambda_{k-1}) \overline{w}}{\beta_k} 
}
+ \frac{(\lambda_k - \lambda_{k-1} ) \overline{w}}{ (1-\gamma) \beta_k } \nonumber  \\
& = 
\phi_k(s, \pi_{k+1}(s) ) +  \frac{ \gamma(\lambda_k - \lambda_{k-1}) \overline{w}}{(1-\gamma) \beta_k} . \label{eq_pda_approx_progress}
\end{align}
It is also clear from the first equality above and \eqref{subgrad_inner_ub} that 
\begin{align}
\cV^{\pi_{k+1}}(s) - \cV^{\pi_k}(s) \leq  \frac{ (\lambda_k - \lambda_{k-1}) \overline{w}}{(1-\gamma) \beta_k} . \label{eq_pda_approx_monotone}
\end{align}
Now taking the telescopic sum of \eqref{raw_progress_ineq_pda}, we obtain 
\begin{align*}
\tsum_{t=0}^k \beta_t \phi_t(s, \pi_{t+1}(s))
& \leq \Phi_k(s, \pi_{k+1}(s)) - \Phi_{-1}(s, \pi_0(s)) \\
%- \lambda_{-1} \cB(\pi_0(s), \pi_1(s)) \\
& \overset{(d)}{=} \Phi_k(s, \pi_{k+1}(s)) \\
& \overset{(e)}{\leq} \Phi_k(s, \DD) = \tsum_{t=0}^k \beta_t \phi_t(s, \DD) + \lambda_k w(\DD) .
\end{align*}
where $(d)$ applies the definition of $\Phi_{-1} \equiv 0$,
and $(e)$ applies \eqref{pda_nhree_point}.
Combining the above inequality with \eqref{eq_pda_approx_progress}, 
we have 
\begin{align*}
\tsum_{t=0}^k \beta_t \rbr{ \cV^{\pi_{t+1}}(s) - \cV^{\pi_t}(s) -  \frac{(\lambda_t - \lambda_{t-1} ) \overline{w}}{\beta_t (1-\gamma)}}
\leq \tsum_{t=0}^k \beta_t \phi_t(s, \DD)  +  \lambda_k \overline{w}.
\end{align*}
Substituting  $\DD = \pi^*(s)$ into the above for every $s\in \cS$,  and aggregating the resulting inequalities by weight $d_{s }^{\pi^*}$, we obtain 
\begin{align*}
& \tsum_{t=0}^k \EE_{s' \sim d_{s}^{\pi^*}} \big[   \beta_t ( \cV^{\pi_{t+1}}(s) - \cV^{\pi_t}(s) -  \frac{(\lambda_t - \lambda_{t-1} ) \overline{w}}{\beta_t (1-\gamma)}) \big] \\
 \leq &  \tsum_{t=0}^k \beta_t \EE_{s' \sim d_s^{\pi^*}}\sbr{ \phi_t(s, \pi^*(s))} +  \lambda_k \overline{w} \\
 \overset{(f)}{=} & 
(1-\gamma) \tsum_{t=0}^k \beta_t \rbr{\cV^{\pi^*}(s) - \cV^{\pi_t}(s)} + \lambda_k \overline{w},
\end{align*} 
where $(f)$ applies Lemma \ref{lemma_perf_diff}.
It remains to take expectation with respect to $s \sim \rho$ of the above inequality and obtain
%Combining the above inequality and  \eqref{eq_pda_approx_monotone}, we can further obtain 
\begin{align*}
&  \tsum_{t=0}^k  \big\lVert\frac{d_{\rho}^{\pi^*}}{\rho}\big\rVert_\infty \EE_{s \sim \rho}
\sbr{ \beta_t ( \cV^{\pi_{t+1}}(s) - \cV^{\pi_t}(s) -  \frac{(\lambda_t - \lambda_{t-1} ) \overline{w}}{\beta_t (1-\gamma)})} \\
\overset{(h)}{\leq} & 
\tsum_{t=0}^k \EE_{s \sim d_{\rho}^{\pi^*}} \sbr{  \beta_t ( \cV^{\pi_{t+1}}(s) - \cV^{\pi_t}(s) -  \frac{(\lambda_t - \lambda_{t-1} ) \overline{w}}{\beta_t (1-\gamma)})} \\
 \leq & 
 (1-\gamma) \tsum_{t=0}^k \beta_t \EE_{s \sim \rho} \sbr{\cV^{\pi^*}(s) - \cV^{\pi_t}(s)} + \lambda_k \overline{w},
\end{align*} 
where $(h)$ utilizes \eqref{eq_pda_approx_monotone}, from which the term inside expectation is non-positive.
Simple arrangement of the above relation yields 
\begin{align*}
\tsum_{t=0}^k 
\beta_t \rbr{f(\pi_{t+1}) - f(\pi^*)}
& \leq 
\tsum_{t=0}^k 
\beta_t \rbr{
1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
}
\rbr{f(\pi_t) - f(\pi^*)} 
+  \rbr{ \frac{1}{1-\gamma} + {\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}^{-1} } \lambda_k\overline{w}  \\
& \leq 
\tsum_{t=0}^k 
\beta_t \rbr{
1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
}
\rbr{f(\pi_t) - f(\pi^*)} 
+ \frac{ 2 \lambda_k \overline{w}}{1-\gamma}  .
\end{align*}
The proof is then completed.
\end{proof}


We are ready to establish the linear convergence of FRPE with proper specification of $\cbr{(\beta_k, \lambda_k)}$.


\begin{theorem}\label{thrm_linear_convergence_frpe}
Take $\beta_k = \rbr{
1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
}^{-k}$ and $\lambda_k = \lambda \zeta > 0 $,  then 
\begin{align*}
f(\pi_{k+1}) - f(\pi^*) \leq 
\rbr{
1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
}^{k} \sbr{ f(\pi_0) - f(\pi^*) + \frac{2 \lambda \zeta \overline{w}}{1-\gamma} }.
\end{align*}
\end{theorem}

\begin{proof}
The  claim follows from a direct application of Lemma \ref{lemma_pda_deterministic_generic}, combined with the choice of $\cbr{(\beta_k, \lambda_k)}$.
\end{proof}

%\yan{remark on comparison to standard approach using fixed point iteration, mention the failure of the latter approach to function approximation, and then motivate the discussion of linear function approximation}

In view of Theorem \ref{thrm_linear_convergence_frpe}, FRPE in the deterministic setting attains linear convergence.
In particular, the performance of applying FRPE to the offline robust MDP problem described in Example \ref{example_offline_rmdp}  matches the classical robust policy evaluation methods based on fixed point iteration \cite{nilim2005robust, iyengar2005robust}. 
Notably both methods requires computing the robust value for every state. 
In the following section we proceed to discuss the setting where approximate computation is required, and demonstrate the clear advantage of FRPE. 


%Now suppose 
%\begin{align*}
%\beta_t \rbr{
%1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
%}
%\leq \beta_{n-1}, ~ \beta_0 = 1,
%\end{align*}
%we then obtain 
%\begin{align*}
%\beta_k \rbr{f(\pi_{k+1}) - f(\pi^*)}
%\leq   f(\pi_0) - f(\pi^*) + 2 \lambda_k \overline{w},
%\end{align*}
%which implies 
%\begin{align*}
%f(\pi_{k+1}) - f(\pi^*) \leq 
%\rbr{
%1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
%}^{k} \sbr{ f(\pi_0) - f(\pi^*) + 2 \lambda \overline{w} }
%\end{align*}



%
%\yan{add the constant stepsize version, show its iteration complexity scales with $\zeta$, hence more favorable when the ambiguity set's size $\zeta$ becomes small
%
%Wait, it seems that the iteration complexity above already implicitly scales with $\zeta$, to see this, simply take $\lambda = \zeta$, and the right hand side both scales as $\cO(\zeta)$.
%}








\subsection{FRPE with Function Approximation}

We now discuss the extension of FRPE to handle large state space.
In such a setting both parameterizing the policy $\pi$ of the nature and its value function $\cV^{\pi}$ can be costly.
We start by observing the that FRPE does not require explicit parameterization  of $\pi$.
Namely, the latest policy $\pi_{t+1}(s)$ can be generated incrementally according to \eqref{raw_update_deterministic_rpe} whenever its value is needed. 
On the other hand, parameterizing $\cV^{\pi}$ can be efficiently done by employing function approximation with relatively few parameters. 
We now discuss an example of such approach, and establish the convergence of FRPE in the presence of potential approximation error.

\begin{example}
Suppose a feature map $\psi: \cS \to \RR^d$ is given, we consider the problem of approximating $\cV^{\pi}(\cdot)$ by $\psi(\cdot)^\top \theta^\pi$ so the difference of these two functions can be properly controlled. 
In view of Lemma \ref{lemma_value_correspondence}, this is equivalent to approximating $V^{\vartheta}_{\PP^\pi}(s)$ by $-\psi(\cdot)^\top \theta^\pi$, a long studied problem with numerous off-the-shelf methods.
%which by itself is simply approximating the value function of $\vartheta$ with respect to MDP $\cM_{\PP^\pi}$.
%Such a problem has been long studied in the reinforcement learning and dynamic programming literature. 
In particular, 
as $\PP^\pi$ is known,
one can consider solving the so-called projected Bellman equation
\begin{align}\label{projected_bellman_equation}
\Psi \theta^\pi = \Pi_{\Psi, \nu^{\vartheta}_{\PP^\pi}} \cT (\Psi \theta^\pi),
\end{align}
where $\Psi \in \RR^{\abs{\cS} \times d}$ denotes the feature matrix constructed by applying $\psi(\cdot)$ to every state,
$\Pi_{\Psi, \nu^{\vartheta}_{\PP^\pi}}$ denotes the projection onto $\mathrm{span}(\Psi)$ in $\norm{\cdot}_{\nu^{\vartheta}_{\PP^\pi}}$ norm,
$ \nu^{\vartheta}_{\PP^\pi}$ denotes the stationary state distribution induced by $\vartheta$ within MDP $\cM_{\PP^\pi}$,
and $\cT$ denotes the Bellman operator of $\vartheta$ within MDP $\cM_{\PP^\pi}$.
A feature to note here is that solution $\theta^\pi$ exists for \eqref{projected_bellman_equation} \cite{580874}, and the latter  being a linear system of $\theta^\pi$ implies efficient methods for solving it \cite{karczmarz1937angenaherte, li2023accelerated}.
The approximation error is captured by $\norm{\Psi \theta^\pi - \cV^\pi}_\infty$, which depends on the expressiveness of $\psi$. 
An alternative to solving \eqref{projected_bellman_equation} is to minimize the mean-squared Bellman error, which also yields a linear system and does not require information of $\nu^{\vartheta}_{\PP^{\pi}}$.
In Section \ref{sec_stoch_linear_approx} we study this approach in the stochastic~setting.
\end{example}

%At this point it is worth mentioning a few prior studies that take a direct approach towards robust policy evaluation, by extending  the projected Bellman equation \eqref{projected_bellman_equation} to the robust setting with linear function approximation \cite{badrinath2021robust, tamar2013scaling}. 
%In this case, the objective is to solve 
%\begin{align}\label{robust_projected_bellman_equation}
%\Psi \theta^* = \Pi_{\Psi,  \nu} \cT_{\mathrm{r}} (\Psi \theta^*)
%\end{align}
% to obtain estimate $\Psi \theta^* \approx V^{\vartheta}_r$. 
%Compared to \eqref{projected_bellman_equation}, the operator $\cT_{\mathrm{r}}$ corresponds to the so-called robust Bellman operator defined in \eqref{def_robust_ballmen_op}, and $\nu$ is the stationary distribution for certain exploration policy.
%The most critical limitation of such approach is that \eqref{robust_projected_bellman_equation} does not necessarily admit a solution, as the operator $\Pi_{\Psi,  \nu} \cT_{\mathrm{r}}$ is no longer a contraction. 
%Indeed it is shown in \cite{tamar2013scaling} that a fairly restrictive yet necessary condition is required to certify the existence of $\theta^*$ satisfying \eqref{robust_projected_bellman_equation}.
%This assumption also seems difficult to verify even if the model is known to the planner.
%On the other hand, if one seeks the approximate solution of \eqref{robust_projected_bellman_equation} by seeking to minimize its mean-squared error $\norm{\Psi \theta^* - \Pi_{\Psi,  \nu} \cT_{\mathrm{r}} (\Psi \theta^*)}^2_2$, the resulting objective can be easily non-convex due to the non-linearity of $\cT_{\mathrm{r}}$. 
 

In the remainder of this section we proceed to establish the convergence of FRPE with arbitrary approximation scheme, as long as the error can be controlled. 
%We now turn our attention to the convergence of FRPE where potential function approximation error exists in the evaluated $\cV^\pi$. 
Namely, instead of using exact $\cV^{\pi_t}$ in the update \eqref{raw_update_deterministic_rpe} of FRPE, one only has an approximation  $\tilde{\cV}^{\pi_t}$ such that 
\begin{align}\label{funx_approx_deterministic}
\norm{\cV^{\pi_t} - \tilde{\cV}^{\pi_t} }_\infty \leq \varepsilon_{\mathrm{approx}} 
\end{align}
for some $\varepsilon_{\mathrm{approx}}  > 0$.
%This substantially generalizes prior approaches mentioned above, by lifting any technical conditions and being applicable to broader approximation schemes.
The FRPE method in this setting takes the following update:
\begin{align}\label{raw_update_deterministic_rpe_func_approx}
\textstyle
	\pi_{k+1}(s) = \gamma \zeta \argmin_{\DD \in \cD_s}  \tsum_{t=0}^{k}  \beta_t \inner{ \DD}{\tilde{\cV}^{\pi_t}_{\vartheta, s}} + \lambda_k w(\DD), ~ \forall s \in \cS,
\end{align}
where $\tilde{\cV}^{\pi_t}_{\vartheta, s} \coloneqq \vartheta(\cdot|s) \otimes \tilde{\cV}^{\pi_t}$.
Clearly, \eqref{raw_update_deterministic_rpe_func_approx} is similar to \eqref{raw_update_deterministic_rpe} except that we replace the exact value function $\tilde{\cV}^{\pi_t}$ with its approximation $\tilde{\cV}^{\pi_t}$.
%We proceed to show that FRPE remains stable in the presence of approximation error.
It is clear that  \eqref{raw_update_deterministic_rpe_func_approx} is equivalent to:
\begin{align}
\pi_{k+1}(s) & = \argmin_{\DD \in \cD_s} \tsum_{t= 0}^k \beta_t \tilde{\phi}_t(s, \DD)  + \lambda_k w_s(\DD) \nonumber \\
& = \argmin_{\DD \in \cD_s}  \tilde{\Phi}_k(s, \DD)  + \lambda_k w_s(\DD) , \label{pda_nabular_func_approx_update}
\end{align}
where  
$
\tilde{\phi}_t(s, \DD)  = \gamma \zeta \inner{\DD - \pi_t(s)}{\tilde{\cV}^{\pi_t}_{\vartheta, s}}$ and  $\tilde{\Phi}_k  = \tsum_{t=0}^k \beta_t \tilde{\phi}_t .
$
%Let us also define 
%\begin{align}\label{def_noise_in_phi}
%\delta_n(s, \DD)  \coloneqq \tilde{\phi}_t(s, \DD)  - \phi_t (s, \DD) = \gamma \zeta \inner{\DD - \pi_t(s)}{\tilde{\cV}^{\pi}_{\vartheta, s} - {\cV}^{\pi}_{\vartheta, s}},
%\end{align}
%where the last equality follows from the definition of $\tilde{\phi}_t$ and $\phi_t$.




The following lemma follows from similar lines as in Lemma \ref{lemma_pda_determinsitic_step_characterization}.


\begin{lemma}\label{lemma_pda_determinsitic_step_characterization_func_approx}
Let $\Phi_{-1} \equiv 0$, $\lambda_{-1} = 0$, and $\lambda_k \geq \lambda_{k-1}$ for every $k \geq 1$.
Then for any $k \geq 0$, we have 
\begin{align}
& \tilde{\Phi}_k(s, \pi_{k+1}(s)) + \lambda_k \cB(\pi_{k+1}(s), \DD) \leq \tilde{\Phi}_k(s, \DD), ~ \forall \DD \in \cD_s, \label{pda_nhree_point_func_approx} \\
& \beta_k \tilde{\phi}_k(s, \pi_{k+1}(s)) 
 \leq \tilde{\Phi}_k(s, \pi_{k+1}(s))  - \tilde{\Phi}_{k-1}(s, \pi_k(s)) - \lambda_{k-1} \cB(\pi_k(s), \pi_{k+1}(s))  \label{raw_progress_ineq_pda_func_approx} .
\end{align}
%where $\cB(\DD, \DD') \coloneqq w(\DD) - w(\DD') - \inner{\nabla  w(\DD')}{\DD - \DD'}$ denotes the Bregman divergence between actions $\DD$ and $\DD'$.
\end{lemma}

We now proceed to establish the convergence properties of FRPE in the presence of approximation error. 


\begin{theorem}
Suppose \eqref{funx_approx_deterministic} holds, 
then taking $\beta_k = \rbr{
1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
}^{-k}$ and $\lambda_k = \lambda \zeta > 0 $ yields 
\begin{align*}
f(\pi_{k+1}) - f(\pi^*) \leq 
\rbr{
1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
}^{k} \sbr{ f(\pi_0) - f(\pi^*) + \frac{2 \lambda \zeta \overline{w}}{1-\gamma} }
+ \frac{2 (1+ \norm{{d_\rho^{\pi^*}}/{\rho}}_\infty) \gamma \zeta  \varepsilon_{\mathrm{approx}} }{(1-\gamma)^2}.
\end{align*}
\end{theorem}

\begin{proof}
We start by noting that following the same lines as we show \eqref{subgrad_inner_ub}, one has 
\begin{align}
\beta_k \tilde{\phi}_k(s, \pi_{k+1}(s)) 
 \leq (\lambda_k - \lambda_{k-1}) \overline{w} \label{subgrad_inner_ub_func_approx}.
\end{align}
Similar to \eqref{eq_pda_approx_progress}, we obtain 
\begin{align}
& \cV^{\pi_{k+1}} (s) - \cV^{\pi_k} (s) 
-  \frac{1}{1-\gamma} \EE_{s' \sim d_s^{\pi_{k+1}}} \sbr{
\phi_k(s', \pi_{k+1}(s')) - \tilde{\phi}_k(s', \pi_{k+1}(s')) 
} \nonumber \\
\overset{(a)}{ =} & \frac{1}{1-\gamma} \EE_{s' \sim d_s^{\pi_{k+1}}} \sbr{
\tilde{\phi}_k(s', \pi_{k+1}(s')) 
}  \nonumber \\
= & \frac{1}{1-\gamma} \EE_{s' \sim d_s^{\pi_{k+1}}} \sbr{
\tilde{\phi}_k(s', \pi_{k+1}(s')) - \frac{(\lambda_k - \lambda_{k-1})\overline{w}}{\beta_k}
}
+ \frac{(\lambda_k - \lambda_{k-1}) \overline{w}}{(1-\gamma)\beta_k}  \label{approx_progress_func_approx_raw_1} \\
\overset{(b)}{\leq} & 
\tilde{\phi}_k(s, \pi_{k+1}(s)) + \frac{(\lambda_k - \lambda_{k-1} )\overline{w}}{\beta_k (1-\gamma)}, \label{inner_product_lb_func_approx}
\end{align}
where $(a)$ follows from Lemma \ref{lemma_perf_diff}, and $(b)$ follows from \eqref{subgrad_inner_ub_func_approx}.
In addition, from the definition of $\phi_k$ and $\tilde{\phi}_k$, one has 
\begin{align}\label{func_approx_error_effect_on_q}
\abs{\phi_k(s, \DD) - \tilde{\phi}_k(s,\DD)}
& = \abs{
\tsum_{a \in \cA} \gamma \zeta \inner{\DD_a - \DD^{\pi_k(s)}_a}{ \cV^{\pi_k} - \hat{\cV}^{\pi_k} } \vartheta(a|s)
}  \nonumber \\
& \leq 2 \gamma \zeta \norm{\cV^{\pi_k} - \hat{\cV}^{\pi_k}}_\infty \leq 2  \gamma \zeta \varepsilon_{\mathrm{approx}}.
\end{align}
where the last inequality follows from H\"{o}lder's inequality.
Combining \eqref{subgrad_inner_ub_func_approx},  \eqref{approx_progress_func_approx_raw_1},  and \eqref{func_approx_error_effect_on_q} also yields 
\begin{align}\label{approx_progress_func_approx_raw_3}
 \cV^{\pi_{k+1}} (s) - \cV^{\pi_k} (s)  \leq \frac{(\lambda_k - \lambda_{k-1}) \overline{w}}{(1-\gamma) \beta_k} + \frac{2 \gamma \zeta \varepsilon_{\mathrm{approx}}}{1-\gamma}.
\end{align}
Repeating the same lines after \eqref{eq_pda_approx_monotone} in the proof of Lemma \ref{lemma_pda_deterministic_generic},
with \eqref{raw_progress_ineq_pda_func_approx}, and  \eqref{inner_product_lb_func_approx}, \eqref{approx_progress_func_approx_raw_3} replaced by \eqref{raw_progress_ineq_pda}, 
\eqref{eq_pda_approx_progress}, and \eqref{eq_pda_approx_monotone},
we obtain 
\begin{align*}
\tsum_{t=0}^k 
\beta_t \rbr{f(\pi_{t+1}) - f(\pi^*)}
& \leq 
\tsum_{t=0}^k 
\beta_t \rbr{
1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
}
\rbr{f(\pi_t) - f(\pi^*)} 
+  \frac{2 \lambda_k\overline{w}}{1-\gamma} \\
& ~~~~~~
+ \tsum_{t=0}^k \beta_t \frac{2 \gamma \zeta  \varepsilon_{\mathrm{approx}}}{1-\gamma} \rbr{1 + \norm{\frac{d_\rho^{\pi^*}}{\rho}}_\infty^{-1}}.
\end{align*}
Plugging the choice of $\cbr{(\beta_k, \lambda_k)}$ yields the desired result.
%\begin{align*}
%f(\pi_{k+1}) - f(\pi^*) \leq 
%\rbr{
%1  - \frac{1-\gamma}{\norm{{d_{\rho}^{\pi^*}}/{\rho}}_\infty}
%}^{k} \sbr{ f(\pi_0) - f(\pi^*) + \frac{2 \lambda \zeta \overline{w}}{1-\gamma} }
%+ \frac{2 (1+ \norm{{d_\rho^{\pi^*}}/{\rho}}_\infty) \gamma \zeta  \varepsilon_{\mathrm{approx}} }{(1-\gamma)^2}.
%\end{align*}
\end{proof}



%\yan{comparison to Mannor's prior work after technical discussion}
%An immediate observation is that FRPE does not necessarily require parameterizing the policy. 
%Instead, the updated policy $\pi_{t+1}(s)$ at any given state $s$ can be computed whenever it is need within the evaluation procedure. 




At this point it is worth mentioning a few prior studies that take a direct approach towards robust policy evaluation with linear function approximation, by extending the projected Bellman equation \eqref{projected_bellman_equation} to the robust setting \cite{roy2017reinforcement, tamar2014scaling}.
In this case, the objective is to solve 
\begin{align}\label{robust_projected_bellman_equation}
\Psi \theta^* = \Pi_{\Psi,  \nu} \cT_{\mathrm{r}} (\Psi \theta^*)
\end{align}
 to obtain estimate $\Psi \theta^* \approx V^{\vartheta}_r$. 
Compared to \eqref{projected_bellman_equation}, the operator $\cT_{\mathrm{r}}$ corresponds to the so-called robust Bellman operator defined in \eqref{def_robust_ballmen_op}, and $\nu$ is the stationary distribution for certain exploration policy.
An important limitation of such approach is that \eqref{robust_projected_bellman_equation} does not necessarily admit a solution, as the operator $\Pi_{\Psi,  \nu} \cT_{\mathrm{r}}$ is no longer a contraction. 
Indeed it is shown in \cite{tamar2014scaling} that an restrictive yet necessary condition is required to certify the existence of $\theta^*$ satisfying \eqref{robust_projected_bellman_equation}.
This assumption also seems difficult to verify even if the model is known to the planner, and only asymptotic convergence has been established depsite its restrictive nature.


On the other hand, if one seeks the approximate solution of \eqref{robust_projected_bellman_equation} by minimizing its mean-squared error $\norm{\Psi \theta^* - \Pi_{\Psi,  \nu} \cT_{\mathrm{r}} (\Psi \theta^*)}^2_2$, the resulting objective can be easily non-convex due to the non-linearity of $\cT_{\mathrm{r}}$. 
 It should be clear that FRPE substantially improves the aforementioned approaches, by removing unrealistic assumptions while being applicable to broader approximation schemes.


It should also be noted that for large-scale offline robust MDPs, both the model $\overline{\PP}$ and ambiguity set $\cD$ can be difficult to store. 
Consequently, in solving the linear system defined by \eqref{projected_bellman_equation} or the mean-squared Bellman error, one might proceed with an incremental manner.   
This can be done, for instance, by Kaczmarz method \cite{karczmarz1937angenaherte} and its randomized variants \cite{strohmer2009randomized, gower2015randomized}.
One can also utilize the stochastic variant of FRPE to be discussed in the next section.



%\yan{should mention that only **asymptotic results** are obtained with this approach!!!}
%
%\yan{should we mention that stochasticity is also allowed even when $\overline{\PP}$ is known }















