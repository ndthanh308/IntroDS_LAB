%!TEX root = ./robust_pe.tex


\section{Introduction}

A robust Markov decision process (MDP)  $\cM_\cP \coloneqq \cbr{\cM_{\PP} : \PP \in \cP}$ consists of a collection of MDPs.
Each MDP $\cM_{\PP} = (\cS, \cA, c, \PP, \gamma)$ is a quintuple, where  
  $\cS$ and $\cA$ denote the finite state and action space, respectively;  
 $c: \cS \times \cA \to \RR$ denotes the cost function, which we assume without loss of generality that $c(s,a) \in [0,1]$ for all $(s,a)$; $\gamma$ denotes the discount factor;
and $\PP \in \Delta_{\cS}^{\abs{\cS}  \abs{\cA}}$ denotes the transition kernel,
where $\Delta_{\cS}$ corresponds to the probability simplex over $\cS$.
Notably MDPs within $\cM_{\cP}$ differ from each other only in their respective transition kernels. 
The set of potential transitional kernels $\cP$ is referred to as the ambiguity set.
%We adopt the following notation going forward. 


For $\PP \in \Delta_{\cS}^{\abs{\cS} \abs{\cA}}$, we use 
$\PP_s \in \Delta_{\cS}^{\abs{\cA}}$ to denote its component corresponding to $s \in \cS$, 
and $\PP_{s,a} \in \Delta_{\cS}$ to denote its component of $(s,a) \in \cS \times \cA$.
Similarly, for $\DD \in \Delta_{\cS}^{\abs{\cA}}$,  $\DD_{a} \in \Delta_{\cS}$ denotes its component of $a \in \cA$.
%In addition,  $\PP \in (\Delta_{\cS})^{\abs{\cS} \times \abs{\cA}}$ denotes the transition kernel, with $\Delta_{\cS}$ being the probability simplex over $\cS$. 
%Accordingly, $\PP_{s,a} \in \Delta_{\cS}$ corresponds to the conditional distribution of the next state upon making an action $a \in \cA$ at state $s \in \cS$,
%and we let $\PP_s = [\PP_{s,a_1}, \ldots, \PP_{s, a_{\abs{\cA}}}]$.
%The set of MDPs differ from each other only in their respective transition kernels. 
Then the standard value function $V^{\vartheta}_{\PP}: \cS \to \RR$ of a randomized stationary policy $\vartheta$ with respect to $\cM_{\PP}$  is defined as 
\begin{align}\label{eq_standard_value_function}
V^{\vartheta}_{\PP}(s) \coloneqq \EE \sbr{\tsum_{t=0}^\infty \gamma^t c(s_t, a_t) \big| s_0 = s, a_t \sim \vartheta(\cdot|s_t), s_{t+1} \sim \PP_{s_t, a_t} }, ~~ \forall s \in \cS.
\end{align} 


We consider policy evaluation for robust Markov decision process with $\mathrm{s}$-rectangular ambiguity sets \cite{wiesemann2013robust}.
That is, for a given policy $\vartheta$, we aim to find worst-case value function  defined as
\begin{align}\label{eq_def_robust_value}
\textstyle
V^{\vartheta}_r (s) \coloneqq \max_{\PP \in \cP} V^{\vartheta}_{\PP}(s), ~ \forall s \in \cS,
\end{align}
where the set of potential transition kernels $\cP \in \Delta_{\cS}^{\abs{\cS}  \abs{\cA}}$ decomposes as 
\begin{align}\label{eq_def_s_rectangular}
\cP = \times_{s \in \cS}  \cP_{s},  ~ \text{where}~ \cP_{s} \subseteq \Delta_{\cS}^{\abs{\cA}}.
\end{align}
%Ambiguity sets with above decomposition are first introduce in \cite{wiesemann2013robust} and referred to as the $\mathrm{s}$-rectangular set.
%We refer as the marginal ambiguity set in the ensuing discussion.
We assume $\cP_{s}$ takes the form of 
\begin{align}\label{def_ambiguity_set_structure}
\cP_{s} = \cbr{
(1-\zeta) \overline{\PP}_{s} + \zeta \DD: ~ \DD \in \cD_{s}
},
\end{align}
where $\overline{\PP}$ is a fixed but possibly unknown transition kernel,   $\cD_{s}$ is  a pre-specified convex subset of $\Delta_{\cS}^{\abs{\cA}}$, and $\zeta \in [0,1]$ is a parameter of user choice. 
One can also specify $\zeta$ in a potentially state-dependent manner.
%We also denote 
%$\cD = \times_{s \in \cS} \cD_{s}$.
%\begin{align}
%\label{eq_cD_set}
%\cD = \times_{s \in \cS} \cD_{s}.
%\end{align}
We next discuss two examples of robust MDPs that can be modeled by \eqref{def_ambiguity_set_structure}.
%We do not assume any information on $\PP$ except that we can sample from it.


%\begin{example}[Robust MDP with Offline Data]\label{example_offline_rmdp}
\begin{example}[Offline Robust MDP]\label{example_offline_rmdp}
One of the most classical applications of robust MDP deals with planning with a historical dataset, collected by prior interactions with an environment of unknown transition kernel $\PP^{\mathrm{N}}$.
The goal is to perform robust planning in response to the inherent uncertainty associated with data collection. 
In this setting, one would first construct for each state a set of plausible models $\cD_{s}$, such that  $ \PP^{\mathrm{N}}_{s} \in \cD_{s}$ with sufficient confidence.
Then the planner solves the robust MDP 
\begin{align*}
\textstyle
\min_{\vartheta} V^{\vartheta}_r(s), ~ \forall s \in \cS,
\end{align*}
where the ambiguity set is defined by \eqref{eq_def_s_rectangular}, with $\zeta = 1$ in \eqref{def_ambiguity_set_structure}, and $\overline{\PP}$ being an arbitrary transition kernel.
% and solves the robust MDP problem with ambiguity set $\cP$. 
Such an approach can be found in early studies of robust MDPs \cite{nilim2005robust, iyengar2005robust, wiesemann2013robust}.
\end{example}


%\begin{example}[Robust MDP without Offline Data]
\begin{example}[Online/Hybrid Robust MDP]\label{online_robust_mdp}
Another emerging class of applications of robust MDP seeks robustness against environment shift \cite{roy2017reinforcement, liu2022distributionally, wang2022policy}.
Namely, the goal is to learn a robust policy that performs well in both training and potentially altered testing environment. 
Notably both training and testing environment can be unknown, except that one can access samples from the training environment, and the testing environment should be close to the training one. 
A typical example of this scenario is the so-called sim-to-real process, where the target is to learn a robust policy within a simulated environment that behaves reasonably well in reality despite the modeling errors associated with simulation.
It can be seen that \eqref{def_ambiguity_set_structure} creates a mechanism of robustness after an immediate reformulation
\begin{align*}
\cP_{s} = \cbr{
 \overline{\PP}_{s} + \zeta (\DD - \overline{\PP}_{s}): ~ \DD \in \cD_{s}
}.
\end{align*}
Here $\overline{\PP}$ denotes the transition kernel of the training environment, while $\cU_\zeta = \cbr{\zeta (\DD - \overline{\PP}_{s}): ~ \DD \in \cD_{s}}$ corresponds to the potential differences between testing and training environment.
Clearly $\zeta$ provides a way to adjust the size of the ambiguity set (and consequently how much environment change one can tolerate) based on our robustness preference.
The flexibility of choosing $\cD_{s}$ also provides a convenient way to utilize partial offline data or domain knowledge.
Namely, for a particular state, if offline data or domain knowledge is available, one can construct $\cD_{s}$ by exploiting this knowledge.
Otherwise one may set $\cD_{s}$ as $\Delta_{\cS}^{\abs{\cA}}$. 
\end{example}


\begin{remark}
Our discussions focus on the robust value function with $\mathrm{s}$-rectangular sets. 
Yet it can be readily seen that the developed methods in this manuscript extend naturally to evaluating the robust Q-function with $(\mathrm{s}, \mathrm{a})$-rectangular sets.
Note that for $\mathrm{s}$-rectangular sets, there is no natural notion of robust Q-function. 
%, and the value function for $\mathrm{s}$-rectangular sets.
%For the latter class of ambiguity sets, there is no natural notion of robust Q-function.
\end{remark}


Going forward we refer to the setting where $\overline{\PP}$ is known as the deterministic setting, and 
where it is unknown as the stochastic setting.
Clearly, offline robust MDP can be categorized into deterministic setting while online robust MDP belongs to the stochastic setting.




The problem of robust policy evaluation \eqref{eq_def_robust_value} serves as the essential step of policy optimization for robust MDPs. 
Both classical methods of robust policy iteration and recently developed first-order policy optimization (i.e., policy gradient) methods \cite{wang2022policy, li2022first} hinge upon an accurate estimation of the robust value function. 
For deterministic setting with known ambiguity sets, 
robust policy evaluation can be efficiently done for small-sized problems. 
In particular,  recursive applications of the robust Bellman operator  yields linear convergence to the robust value function \cite{nilim2005robust, iyengar2005robust} as the robust Bellman operator is a contraction.
%much similar to how non-robust Bellman operator can be used for evaluating standard value function \eqref{eq_standard_value_function}.
% Both methods  depend critically on the contraction property of the Bellman operators. 
This property is further exploited in the stochastic setting, 
where an $\cO(1/\epsilon^2)$ sample complexity is obtained with a robust temporal difference learning method \cite{li2022first}.
Even for this simple tabular problem, it remains unclear whether robust policy evaluation requires additional samples compared to its non-robust counterpart.


For large-scale problems,  robust policy evaluation becomes challenging even for the deterministic setting.
For computational consideration one needs to introduce certain level of approximation to handle large state space.
One prior approach focuses on linear function approximation \cite{tamar2014scaling}, where the robust Bellman operator is replaced by its robust projected counterpart  applied to the linearly-parameterized robust value estimation. 
Unfortunately, this modification destroys the contraction property of the resulting fixed point update and leads to divergence.
Indeed a rather unrealistic assumption on the discount factor and the ambiguity set is necessary to retain  contraction  \cite{tamar2014scaling}.
% Yet modification destroys the contraction properties of the resulting fixed point update and leads to divergence \cite{tamar2014scaling}. 
% A necessary and sufficient condition is introduced in \cite{tamar2014scaling}  to recover contraction. Unfortunately, this condition places unrealistic requirement on the discount factor and the ambiguity set, and is generally not verifiable. 
Another natural approach seeks to directly minimize Bellman residual. 
This leads to a linear system for standard policy evaluation problem \eqref{eq_standard_value_function} but becomes easily non-convex for robust policy evaluation \eqref{eq_def_robust_value},  due to the non-linearity of the robust Bellman operator \cite{roy2017reinforcement}. 
For the stochastic setting, there seems to be no evaluation method without a similar assumption \cite{roy2017reinforcement}.
%It appears that there is no stable and globally convergent method of robust policy evaluation for large-scale problems. 


An alternative approach, yet largely dismissed, is to consider robust policy evaluation as a policy optimization problem in its own right.
%The connection between robust policy evaluation and policy optimization appears to be known.
The connection of these two problems appears in various prior development.
It is discussed in \cite{ruszczynski2010risk, shapiro2021distributionally} that certain nested formulations of robust MDP can be viewed as a stochastic game between the planner and the nature, with the nature's optimal policy defining the robust value of the planner. 
From our perspective the omission of this viewpoint may be attributed to two reasons.
First,  early applications of offline robust MDPs (Example \ref{example_offline_rmdp}) can already be solved for small-sized problems. 
This is further compounded by the fact that in the minimax game the nature has a continuous action space, and parameterizing the nature's policy seems to already require a memory of the same size as the transition~kernel. 


In this manuscript we develop the first unified and globally convergent framework of robust policy evaluation for both deterministic and stochastic settings, with  either tabular representation or generic function approximation. 
Enabled by this framework we establish a set of computational results that appear to be completely new for robust policy evaluation problems, and greatly enhance the applicability of robust MDPs to large-scale problems. 
Our main contributions are summarized into the following aspects. 


%In this manuscript we develop the first stable and unified framework of robust policy evaluation for both deterministic and stochastic settings, with  either tabular representation or generic function approximation. 
%Enabled by this framework we establish a set of computational results that appear to be completely new for robust policy evaluation problems, and greatly enhance the applicability of robust MDPs to large-scale problems. 
%Notably we adopt a policy optimization viewpoint towards robust policy evaluation, and consequently our method development does not require the contraction property.
%The connection between robust policy evaluation and policy optimization appears to be known.
%It is shown in \cite{ruszczynski2010risk, shapiro2021distributionally} that certain nested formulations of robust MDP can be viewed as a minimax stochastic game between the planner and the nature, with the nature's optimal policy defining the robust value of the planner. 
%Nevertheless, this viewpoint has largely been dismissed for algorithmic development.
%From our perspective this might be attributed to two reasons.
%First,  early applications of robust MDPs (Example \ref{example_offline_rmdp}) can already be solved for small-sized problems by using exact fixed point iterations. 
%This is further compounded by the fact that in the minimax  game the nature has a continuous action space, and parameterizing the nature's policy seems to already require a memory of the same size as the transition kernel. 
%Our main contributions are summarized into the following aspects. 

First, we formulate the robust policy evaluation problem \eqref{eq_def_robust_value} into a Markov decision process of the nature, whose action space corresponds to the ambiguity set of the robust MDP. 
We then develop a method named first-order robust policy evaluation (FRPE), which only requires solving a standard policy evaluation problem \eqref{eq_standard_value_function} to improve the policy of the nature.
In a nutshell, FRPE operates much similarly to the dual averaging method \cite{nesterov2009primal} and its recent adaptation to policy optimization for general state and action spaces \cite{lan2022policy}. Accordingly, despite being a policy based method, FRPE can be implemented in a fully value-based manner and avoid explicit policy parameterization. 
%Consequently, no policy parameterization is required for FRPE.


In the deterministic setting, we show that FRPE converges linearly to the robust value function for tabular problems with small state spaces. 
Each step of FRPE only requires a simple subroutine for standard policy evaluation with a known transition kernel.
%Notably, the iteration complexity is independent of the size of the action space for the planner, despite the nature's MDP having a continuous action space.
We then extend FRPE to settings where function approximation is employed to handle large state space. 
In this case FRPE converges at the same linear rate, up to the approximation error produced by the standard evaluation subroutine. 
Consequently, FRPE substantially improves existing methods of robust policy evaluation for large-scale problems as it does not require any restrictive assumption, and can be combined with general function approximation schemes.
%as no assumption is required for convergence, and is applicable to general function approximation schemes. 

In the stochastic setting, we develop a stochastic variant named SFRPE.
For tabular problems, we establish an $\cO\rbr{\frac{\zeta^2 \abs{\cS}}{(1-\gamma)^5 \epsilon^2} + \frac{\abs{\cS}}{1-\gamma} \log (\frac{1}{\epsilon})}$ sample complexity for SFRPE to return an estimated robust value function with its bias bounded by $\epsilon$.
%is $\epsilon$-accurate in expectation (Definition \ref{def_acc_certificate}). 
Notably, the established sample complexity is independent of the continuous action space of the nature. 
We then establish an $\cO\rbr{\frac{\zeta^2 \abs{\cS}}{(1-\gamma)^5 \epsilon^2} + \frac{\abs{\cS}}{(1-\gamma)^3} \log (\frac{1}{\epsilon})}$ sample complexity for 
returning $\epsilon$-accurate estimator with high probability.
As a consequence the robust policy evaluation does not require additional samples compared to standard policy evaluation, provided the size $\zeta$ of the ambiguity set is bounded by $\cO(1-\gamma)$. 
For large-scale problems, we further incorporate SFRPE with linear function approximation and establish a similar $\cO\rbr{{\zeta^2}/{\epsilon^2} + \log({1}/{\epsilon})}$ (resp. $\cO\rbr{{\zeta^2}/{\epsilon^2} + {1}/{\epsilon^2}}$) sample complexity for returning an $\epsilon$-accurate estimator in expectation (resp. in high probability).
The dependence on the size of the ambiguity set clearly delineates the price of robustness for robust policy evaluation. 
In particular, setting $\zeta =0$ recovers a tight sample complexity for standard policy evaluation. 
To the best of our knowledge, all the obtained sample complexities appear to be new.
Importantly, with linear function approximation we have established the first known sample complexity of robust policy evaluation beyond the tabular setting.
SFRPE is flexible enough and can be combined even with nonlinear function approximation, provided certain off-policy evaluation problem can be solved to a prescribed target precision.


Finally, we demonstrate a direct application of SFRPE for solving large-scale robust MDPs.
 Combining a recently developed stochastic policy optimization method for robust MDPs \cite{li2022first} and SFRPE, we establish an $\tilde{\cO}(1/\epsilon^2)$ sample complexity for both tabular problems, and for large-scale problems with log-linear policy class. 
Notably the latter sample complexity has not been reported for this problem class. 


The rest of the manuscript is organized as follows.
Section \ref{sec_pe_as_po}  formulates the MDP for robust policy evaluation and studies a few of its useful structural properties. 
Section \ref{sec_deterministic} then introduces FRPE for the deterministic setting and establishes its iteration complexities.
Section \ref{sec_stochastic} develops the stochastic FRPE and its sample complexities. 
 Finally, we conclude in section \ref{sec_conclusion}, and discuss a few future directions in robust policy evaluation and optimization  enabled by FRPE.

%\yan{Key points to make:
%
%
%
%\begin{itemize}
%\item need to mention one of the biggest strength --
%one does not need to store the nature's policy -- all we need is store weighted value and function approximation can be used -- making it truly scalable
%\item the first term of the sample complexity corresponds to the price we pay for robustness -- notably when $\zeta = \cO(1-\gamma)$ there is no price to pay for robustness!
%\end{itemize}
%
%}






