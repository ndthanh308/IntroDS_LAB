\begin{thebibliography}{10}

\bibitem{gower2015randomized}
Robert~M Gower and Peter Richt{\'a}rik.
\newblock Randomized iterative methods for linear systems.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  36(4):1660--1690, 2015.

\bibitem{goyal2023robust}
Vineet Goyal and Julien Grand-Clement.
\newblock Robust markov decision processes: Beyond rectangularity.
\newblock {\em Mathematics of Operations Research}, 48(1):203--226, 2023.

\bibitem{iyengar2005robust}
Garud~N Iyengar.
\newblock Robust dynamic programming.
\newblock {\em Mathematics of Operations Research}, 30(2):257--280, 2005.

\bibitem{karczmarz1937angenaherte}
Stefan Karczmarz.
\newblock Angenaherte auflosung von systemen linearer glei-chungen.
\newblock {\em Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat.}, pages
  355--357, 1937.

\bibitem{kotsalis2020simple}
Georgios Kotsalis, Guanghui Lan, and Tianjiao Li.
\newblock Simple and optimal methods for stochastic variational inequalities,
  ii: Markovian noise and policy evaluation in reinforcement learning.
\newblock {\em arXiv preprint arXiv:2011.08434}, 2020.

\bibitem{lan2022policy}
Guanghui Lan.
\newblock Policy optimization over general state and action spaces.
\newblock {\em arXiv preprint arXiv:2211.16715}, 2022.

\bibitem{li2023accelerated}
Tianjiao Li, Guanghui Lan, and Ashwin Pananjady.
\newblock Accelerated and instance-optimal policy evaluation with linear
  function approximation.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 5(1):174--200,
  2023.

\bibitem{li2023policy}
Yan Li and Guanghui Lan.
\newblock Policy mirror descent inherently explores action space.
\newblock {\em arXiv preprint arXiv:2303.04386}, 2023.

\bibitem{li2022first}
Yan Li, Tuo Zhao, and Guanghui Lan.
\newblock First-order policy optimization for robust markov decision process.
\newblock {\em arXiv preprint arXiv:2209.10579}, 2022.

\bibitem{liu2022distributionally}
Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing Zhou, and
  Zhengyuan Zhou.
\newblock Distributionally robust $ q $-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  13623--13643. PMLR, 2022.

\bibitem{mannor2016robust}
Shie Mannor, Ofir Mebel, and Huan Xu.
\newblock Robust mdps with k-rectangular uncertainty.
\newblock {\em Mathematics of Operations Research}, 41(4):1484--1509, 2016.

\bibitem{nesterov2009primal}
Yurii Nesterov.
\newblock Primal-dual subgradient methods for convex problems.
\newblock {\em Mathematical programming}, 120(1):221--259, 2009.

\bibitem{nilim2005robust}
Arnab Nilim and Laurent El~Ghaoui.
\newblock Robust control of markov decision processes with uncertain transition
  matrices.
\newblock {\em Operations Research}, 53(5):780--798, 2005.

\bibitem{roy2017reinforcement}
Aurko Roy, Huan Xu, and Sebastian Pokutta.
\newblock Reinforcement learning under model mismatch.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{ruszczynski2010risk}
Andrzej Ruszczy{\'n}ski.
\newblock Risk-averse dynamic programming for markov decision processes.
\newblock {\em Mathematical programming}, 125(2):235--261, 2010.

\bibitem{shapiro2021distributionally}
Alexander Shapiro.
\newblock Distributionally robust optimal control and mdp modeling.
\newblock {\em Operations Research Letters}, 49(5):809--814, 2021.

\bibitem{strohmer2009randomized}
Thomas Strohmer and Roman Vershynin.
\newblock A randomized kaczmarz algorithm with exponential convergence.
\newblock {\em Journal of Fourier Analysis and Applications}, 15(2):262, 2009.

\bibitem{sutton2009fast}
Richard~S Sutton, Hamid~Reza Maei, Doina Precup, Shalabh Bhatnagar, David
  Silver, Csaba Szepesv{\'a}ri, and Eric Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 993--1000, 2009.

\bibitem{sutton2008convergent}
Richard~S Sutton, Csaba Szepesv{\'a}ri, and Hamid~Reza Maei.
\newblock A convergent o (n) algorithm for off-policy temporal-difference
  learning with linear function approximation.
\newblock {\em Advances in neural information processing systems},
  21(21):1609--1616, 2008.

\bibitem{tamar2014scaling}
Aviv Tamar, Shie Mannor, and Huan Xu.
\newblock Scaling up robust mdps using function approximation.
\newblock In {\em International conference on machine learning}, pages
  181--189. PMLR, 2014.

\bibitem{tao2015random}
Terence Tao and Van Vu.
\newblock Random matrices: universality of local spectral statistics of
  non-hermitian matrices.
\newblock 2015.

\bibitem{580874}
J.N. Tsitsiklis and B.~Van~Roy.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock {\em IEEE Transactions on Automatic Control}, 42(5):674--690, 1997.

\bibitem{wang2022policy}
Yue Wang and Shaofeng Zou.
\newblock Policy gradient method for robust reinforcement learning.
\newblock {\em arXiv preprint arXiv:2205.07344}, 2022.

\bibitem{wiesemann2013robust}
Wolfram Wiesemann, Daniel Kuhn, and Ber{\c{c}} Rustem.
\newblock Robust markov decision processes.
\newblock {\em Mathematics of Operations Research}, 38(1):153--183, 2013.

\end{thebibliography}
