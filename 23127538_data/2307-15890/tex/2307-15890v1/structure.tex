%!TEX root = ./robust_pe.tex
%\newpage

\section{Robust Policy Evaluation as Policy Optimization}\label{sec_pe_as_po}
This section adopts a policy optimization viewpoint towards policy evaluation, by first formulating the robust policy evaluation problem as a Markov decision problem of the nature.
We then identify a few key structures of the formulated MDP that will prove useful for our subsequent development. 

Consider a MDP of nature, denoted by $\mathfrak{M}$,  defined as follows.
The state space is given by $\cS$, and
the set of possible actions  at any state $s \in \cS$ is given by $ \cD_s \subseteq \Delta_{\cS}^{\abs{\cA}}$ (cf. \eqref{def_ambiguity_set_structure}).
%We will write $\DD = [\DD_{a_1}, \ldots, \DD_{a_{\abs{\cA}}}]$ for any $\DD \in \Delta_{\cS}^{\abs{\cA}}$.
%Equivalently, any possible action $\DD \in \cD_s$ specifies $\abs{\cA}$ elements in $\Delta_{\cS}$,  each denoted as $\DD_{a}$ for every $a \in \cA$.
At state $s \in \cS$, upon making an action $\DD \in \cD_s$, the conditional distribution of the next state $s' \in \cS$ is given by 
\begin{align}\label{transit_kernel_of_nature_mdp}
\mathfrak{P}(s' | s, \DD) \coloneqq  \tsum_{a \in \cA} \sbr{(1-\zeta) \overline{\PP}_{s,a}(s')  + \zeta \DD_{a}(s')} \vartheta(a|s).
\end{align}
%Clearly, the above transition kernel $\mathfrak{P}$ is affine with respect to the action of the nature  $\DD$. 
Finally, the cost function associated with $(s, \DD)$ for any $\DD \in \cD_s$ is given by 
\begin{align*}
\mathfrak{C}(s) \coloneqq - \tsum_{a \in \cA} \vartheta(a|s) c(s,a),
\end{align*}
 and the discount factor is set as $\gamma$.

A non-randomized policy of the nature is denoted as $\pi: \cS \to \cD_s$. 
%It is clear that $\pi$ uniquely determines $\DD^{\pi} \in \cD$ defined in \eqref{eq_cD_set}.
%Let us denote $\DD^{\pi(s)} = \pi(s)$, and 
%Let us define $\DD^{\pi(s)}  = \pi(s)$ for any policy $\pi$.
For any policy $\pi$ and $s \in \cS$, let us define $\DD^{\pi(s)} \equiv \pi(s) \in \cD_s$.
For notational clarity we will sometimes use these two quantities interchangeably. 
We then define
\begin{align}\label{kernel_defined_by_nature_policy}
\PP^{\pi}_{s,a} \coloneqq (1-\zeta) \overline{\PP}_{s,a}  + \zeta \DD^{\pi(s)}_{a}, ~ (s,a) \in \cS \times \cA.
\end{align}
%as the transition kernel of the original planner when the nature's policy is $\pi$.
Consequently, from \eqref{transit_kernel_of_nature_mdp} it holds that 
\begin{align}\label{state_transit_of_nature_given_policy}
\mathfrak{P}(s'|s,  \pi(s)) =  \tsum_{a \in \cA} \sbr{(1-\zeta) \overline{\PP}_{s,a}(s')  + \zeta \DD^{\pi(s)}_{a}(s')} \vartheta(a|s)
= \tsum_{a \in \cA} \PP^{\pi}_{s,a}(s') \vartheta(a|s).
\end{align}
We define the value function of policy $\pi$ as 
\begin{align*}
%\label{eq_def_value_func_nature}
\cV^{\pi} (s) \coloneqq 
\EE \sbr{\tsum_{t=0}^\infty \gamma^t \mathfrak{C}(s_t) \big| s_0 = s, s_{t+1} \sim \mathfrak{P}(\cdot| s_t, \pi(s_t) ) , t \geq 0}, ~~ \forall s \in \cS,
\end{align*}
and the goal of the nature is to find the optimal policy of 
\begin{align}\label{eq_def_optmal_value_nature}
\textstyle
\min_{\pi \in \Pi} \cV^{\pi} (s),
\end{align}
where $\Pi: s \mapsto \cD_s$ is the set of non-randomized stationary policies of the nature.

\begin{lemma}\label{lemma_value_correspondence}
For any $\pi \in \Pi$, we have 
\begin{align}\label{eq_nature_value_as_player_value}
\cV^{\pi}(s) = - V^{\vartheta}_{\PP^{\pi}}(s), ~ \forall s \in \cS,
\end{align}
with $V^{\vartheta}_{\PP^{\pi}}$ defined in \eqref{eq_standard_value_function}.
In addition, let $\cV^*$ denote the optimal value function of \eqref{eq_def_optmal_value_nature}.
Then 
\begin{align}\label{nature_opt_as_robust_value}
\cV^*(s) = - V^{\vartheta}_r(s), ~ \forall s \in \cS,
\end{align}
where $V^{\vartheta}_r$ is defined as in \eqref{eq_def_robust_value}.
\end{lemma}

\begin{proof}
It is clear that the $\cV^{\pi}$ satisfies the following dynamic programming equation 
\begin{align*}
\cV^{\pi} (s) & = \mathfrak{C}(s) + \gamma \tsum_{s' \in \cS} \mathfrak{P}(s'|s,  \pi(s)) \cV^{\pi}(s') \\
 & = - \tsum_{a \in \cA} \vartheta(a|s) c(s,a)
 + \gamma  \tsum_{s' \in \cS} \tsum_{a \in \cA} \PP^{\pi}_{s,a}(s') \vartheta(a|s) \cV^{\pi}(s'), ~ \forall s \in \cS.
\end{align*}
where the last equality follows from \eqref{state_transit_of_nature_given_policy}.
The above relation implies that $- \cV^{\pi}$ is the fixed point of operator 
\begin{align*}
(\cT^{\pi} V)(s) = 
\tsum_{a \in \cA} \vartheta(a|s) c(s,a)
 + \gamma  \tsum_{s' \in \cS} \tsum_{a \in \cA} \PP^{\pi}_{s,a}(s') \vartheta(a|s) V(s'), ~ \forall s \in \cS.
\end{align*}
On the other hand, it is known that $V^{\vartheta}_{\PP^{\pi}}$ is the unique fixed point of $\cT^{\pi}$, from which
we obtain \eqref{eq_nature_value_as_player_value}.
% That is, the value function of the nature's policy $\cV^{\pi}$ corresponds to the negative value function of the policy $\pi$ within $\cM_{\PP^{\pi}}$.
In addition,  Bellman optimality condition of MDP \eqref{eq_def_optmal_value_nature} yields  
\begin{align*}
\cV^*(s) & = \min_{\DD \in \cD_s} \mathfrak{C}(s)  + \gamma \tsum_{s' \in \cS} \mathfrak{P}(s' |s, \DD) \cV^*(s') \\ 
& = \min_{\DD \in \cD_s} -  \tsum_{a \in \cA} \vartheta(a|s) c(s,a) 
+ \gamma \tsum_{s' \in \cS} \tsum_{a \in \cA}\sbr{(1-\zeta) \overline{\PP}_{s,a}(s')  + \zeta \DD_{a}(s')}\vartheta(a|s) \cV^*(s') \\
& = \min_{\PP \in \cP_s} -  \tsum_{a \in \cA} \vartheta(a|s) c(s,a) 
+ \gamma \tsum_{s' \in \cS} \tsum_{a \in \cA} \PP_{a}(s') \vartheta(a|s) \cV^*(s') , ~ \forall s \in \cS.
%\\
%& =  - \tsum_{a \in \cA} \vartheta(a|s) c(s,a) 
%+ \gamma  \tsum_{a \in \cA}   \vartheta(a|s)  \tsum_{s' \in \cS} \min_{\PP_{s,a} \in \cP_{s,a}} \PP_{s,a}(s') \cV^*(s'), ~ \forall s \in \cS.
\end{align*}
Clearly, $-\cV^*$ is the fixed point of operator 
\begin{align}\label{def_robust_ballmen_op}
(\cT V)(s) = \max_{\PP \in \cP_s} \tsum_{a \in \cA} \vartheta(a|s) c(s,a) 
+ \gamma \tsum_{s' \in \cS} \tsum_{a \in \cA} \PP_{a}(s') \vartheta(a|s) V(s'), ~ \forall s \in \cS. 
\end{align}
On the other hand, it is well known that $V^{\vartheta}_{r}$ is the unique fixed point of $\cT V$ \cite{wiesemann2013robust}. 
Consequently we obtain \eqref{nature_opt_as_robust_value}.
%.
%That is, the optimal value function of the nature \eqref{eq_def_value_func_nature} corresponds to the negative robust value function $V^{\pi}_r$ of the policy,
%as both are the (unique) solution of the above dynamic programming equation.
\end{proof}


In view of the above observations, the robust policy evaluation problem \eqref{eq_def_robust_value} can be equivalently solved by solving a Markov decision process \eqref{eq_def_optmal_value_nature}  of nature with finite state space and continuous action space. 
To this end, let us define the following problem:
\begin{align}\label{policy_opt_obj_nature}
\textstyle
\min_{\pi \in \Pi} \cbr{f(\pi) \coloneqq \EE_{s \sim \rho} \sbr{\cV^{\pi}(s)}},
\end{align}
where $\rho$ is any distribution with full support over $\cS$.
Our end goal is to develop efficient methods that can be applied to solve \eqref{policy_opt_obj_nature}.


The state-action value function of the nature, also know as the Q-function, is defined by
\begin{align}\label{def_q_func_nature}
\cQ^{\pi}(s, \DD) & \coloneqq 
\EE \sbr{\tsum_{t=0}^\infty \gamma^t \mathfrak{C}(s_t) \big| s_0 = s, s_1 \sim   \mathfrak{P}(\cdot| s, \DD ), s_{t+1} \sim \mathfrak{P}(\cdot| s_t, \pi(s_t) ), t \geq 1 } . 
%\\
%& =  \mathfrak{C}(s) + \gamma \EE_{s' \sim   \mathfrak{P}(\cdot| s, \DD )} \sbr{\cV^{\pi}(s')} \\
%& = \mathfrak{C}(s) +  
% \gamma \tsum_{s' \in \cS} \tsum_{a \in \cA}\sbr{(1-\zeta) \overline{\PP}_{s,a}(s')  + \zeta \DD_{s,a}(s')}\vartheta(a|s) \cV^{\pi}(s')  \\
%& =  \mathfrak{C}(s) +  
% \gamma  \tsum_{a \in \cA} \vartheta(a|s) 
% \inner{(1-\zeta) \overline{\PP}_{s,a} + \zeta \DD_{s,a}}{\cV^{\pi}} \\
% & = 
% \mathfrak{C}(s) + \gamma \inner{(1-\zeta) \overline{\PP}_s + \zeta \DD}{\cV^{\pi}_{\vartheta, s}}, 
\end{align}
Clearly one also has 
\begin{align}\label{relation_q_and_v}
\cQ^\pi(s,\DD) = \mathfrak{C}(s) + \gamma \EE_{s' \sim \mathfrak{P}(\cdot | s,\DD)} \sbr{\cV^{\pi}(s')}.
\end{align}
%for any $ \DD \in \cD_s$, 
%where in the last equality we define $\cV^{\pi}_{\vartheta, s} = \vartheta(\cdot|s) \otimes \cV^{\pi} \in \RR^{\abs{\cA} \abs{\cS}}$.
We next show that $Q^{\pi}(s, \cdot)$ is indeed an affine function over $\cD_s$, an immediate yet important property that we will exploit in the ensuing development. 

\begin{proposition}\label{prop_q_structure}
For any $\DD \in \cD_s$, we have 
\begin{align*}
\cQ^{\pi}(s, \DD)
=  \mathfrak{C}(s) + \gamma \inner{(1-\zeta) \overline{\PP}_s + \zeta \DD}{\cV^{\pi}_{\vartheta, s}}, 
\end{align*}
where $\cV^{\pi}_{\vartheta, s} \coloneqq \vartheta(\cdot|s) \otimes \cV^{\pi} \in \RR^{ \abs{\cS} \abs{\cA}}$.
%and $\overline{\PP}_s \coloneqq [\overline{\PP}_{s, a_1}, \ldots, \overline{\PP}_{s, a_{\abs{\cA}}}] \in \Delta_{\cS}^{\abs{\cA}}$.
\end{proposition}

\begin{proof}
We have 
\begin{align*}
\cQ^{\pi}(s, \DD)
& =  \mathfrak{C}(s) + \gamma \EE_{s' \sim   \mathfrak{P}(\cdot| s, \DD )} \sbr{\cV^{\pi}(s')} \\
& = \mathfrak{C}(s) +  
 \gamma \tsum_{s' \in \cS} \tsum_{a \in \cA}\sbr{(1-\zeta) \overline{\PP}_{s,a}(s')  + \zeta \DD_{a}(s')}\vartheta(a|s) \cV^{\pi}(s')  \\
& =  \mathfrak{C}(s) +  
 \gamma  \tsum_{a \in \cA} \vartheta(a|s) 
 \inner{(1-\zeta) \overline{\PP}_{s,a} + \zeta \DD_{a}}{\cV^{\pi}} \\
 & = 
 \mathfrak{C}(s) + \gamma \inner{(1-\zeta) \overline{\PP}_s + \zeta \DD}{\cV^{\pi}_{\vartheta, s}},
\end{align*}
which completes the proof.
\end{proof}

%\yan{need to define $d_{\rho}^{\pi}$ within the perf diff lemma}
Our ensuing discussions repeatedly make use of the discounted visitation measure, defined as $d_{\rho}^{\pi}(s) = (1-\gamma) \tsum_{s' \in \cS} \rho(s') \tsum_{t=0}^\infty \gamma^t \mathtt{P}^{\pi}(s_t = s| s_0=s')$ for every $s \in \cS$, where $\mathtt{P}^{\pi}(s_t = s| s_0=s')$ denotes the probability of reaching state $s$, if running $\vartheta$ starting from $s'$ within MDP $\cM_{\PP^\pi}$.
In particular, we write $d_{s}^{\pi}$ when $\rho$ has support $\cbr{s}$. 
We next establish the difference of values for two policies of nature. 

\begin{lemma}\label{lemma_perf_diff}
For a pair of policies $\pi, \pi'$, and any $s\in \cS$,  we have
\begin{align}\label{eq_perf_diff}
\cV^{\pi'}(s) - \cV^{\pi}(s) = \frac{1}{1-\gamma}
\EE_{s' \sim d_{s}^{\pi'}} \sbr{
\cQ^{\pi}(s', \pi'(s')) - 
\cQ^{\pi}(s', \pi(s'))
}
\end{align}
Equivalently, by defining $\phi^{\pi}( s, \pi'(s)) 
%\coloneqq \cQ^{\pi}(s, \pi'(s)) - 
%\cQ^{\pi}(s, \pi(s)) 
\coloneqq \gamma \zeta \inner{\pi'(s) - \pi(s)}{\cV^{\pi}_{\vartheta, s}}$, then 
\begin{align}\label{eq_perf_diff_linearized}
\cV^{\pi'}(s) - \cV^{\pi}(s) = \frac{1}{1-\gamma}
\EE_{s' \sim d_{s}^{\pi'}} \sbr{\phi^{\pi}(s', \pi'(s'))}
\end{align}
\end{lemma}


\begin{proof}
%The proof of \eqref{eq_perf_diff} follows standard steps of performance difference lemma for finite MDPs \cite{lan2021policy, kakade2002approximately} and hence is omitted here.
%\yan{need to expand on this one}
%Let $\xi_(s)$ denote the 
Let $\xi'(s) = \cbr{s_0 = s, \pi'(s_0), s_1, \pi'(s_1), \ldots} $ denote the trajectory generated by $\pi'$ within $\mathfrak{M}$. 
That is 
\begin{align*}
s_{t+1} \sim \mathfrak{P}(\cdot|s_t, \pi'(s_t)),
\end{align*}
or equivalently, in view of \eqref{state_transit_of_nature_given_policy}, that
\begin{align}\label{state_transition_distribution_equivalence}
s_{t+1} \sim \tsum_{a \in \cA} \vartheta(a|s_t) \PP^{\pi'}_{s_t,a} (\cdot) .
\end{align}
We then obtain 
\begin{align*}
\cV^{\pi'}(s) - \cV^{\pi}(s)
& \overset{(a)}{=} \EE_{\xi'(s)} \sbr{\tsum_{t=0}^\infty \gamma^t \rbr{ \mathfrak{C}(s_t) + \gamma \cV^{\pi}(s_{t+1}) - \cV^{\pi}(s_t)}  + \cV^\pi(s_0) }   - \cV^\pi(s)  \\
& \overset{(b)}{=} \EE_{\xi'(s)} \sbr{\tsum_{t=0}^\infty \gamma^t \rbr{ \cQ^{\pi}(s_t, \pi'(s_t)) - \cV^{\pi}(s_t)}  } \\
& \overset{(c)}{=} \frac{1}{1-\gamma} \EE_{s' \sim d_s^{\pi'}} \sbr{\cQ^{\pi}(s', \pi'(s')) - \cV^\pi(s')},
\end{align*}
where $(a)$ follows from the definition of $\cV^{\pi'}(s)$, 
 $(b)$  follows from $s_0 = s$ and \eqref{relation_q_and_v},
and $(c)$ follows from \eqref{state_transition_distribution_equivalence} and the definition of $d_s^{\pi'}$.
Then \eqref{eq_perf_diff} follows  by noting that $\cV^{\pi}(s) = \cQ^\pi(s, \pi(s))$. 
Finally, \eqref{eq_perf_diff_linearized} follows from \eqref{eq_perf_diff} and Proposition \ref{prop_q_structure}.
\end{proof}

Interested readers might find the formulated MDP of nature challenging upon initial examination. 
In particular, as nature has a continuous action space, even evaluating the state-action value function \eqref{def_q_func_nature} seems to be challenging, a crucial quantity for policy improvement. 
It is also unclear whether one should and how to parameterize the policy of nature. 
In the next section, we proceed to develop the first-order robust policy evaluation (FRPE) method that exploits the structural properties established in this section and overcomes the aforementioned difficulties.






















